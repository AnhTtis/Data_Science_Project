\section{Network Details}
\textbf{Voxel Feature Encoder} We adopt the same design as ~\cite{Zhang_2020_CVPR} to encode the point cloud into a voxel feature map. First, we group points within each voxel together and append 6 additional features to the point features $P\in \mathbb{R}^{3+c+6}$, i.e. the center of corresponding voxel $(x_{v},y_{v},z_{v})$ and the offset to the center $(x-x_{v},y-y_{v},z-z_{v})$. Next, we use 4 stacked layers of MLP to transform the point feature to a high dimensional space, followed by a sparse max pooling layer to extract voxel feature representation in each valid voxel. The channel size is $[64,128,256,256]$ in each MLP.

\textbf{XSF structure} We apply 2 stacked transformer blocks, each with 4 heads of deformable self-attention. We use a channel size of 64 in each head of Dense-to-Sparse XSF and a channel size of 32 in each head of Sparse-to-Dense XSF. The channel size of the FFN is 256 in both XSFs. We use pre-norm rather than post-norm in each layer.

\textbf{XTF structure} We use 3 stacked transformer decoder layers, each with 4 heads of self-attention and cross-attention. We use a channel size of 32 in each head and channel size of 64 in the FFN.


\section{More Discussions}
\textbf{More Analysis of XSF} 
In Table~\ref{table:query}, we show the results of adopting different types of query in the dense-to-sparse XSF. The features in the BEV feature map at valid voxels serve as queries in our LiDARFormer. Voxel query refers to taking the features from a sparse feature map at valid voxels while embedding query means treating the embeddings of the valid coordinates as queries. The performance slightly drops by $0.3\%$ and $0.4\%$ respectively, which may be due to the BEV features from the 2D multi-scale feature extractor containing more contextual information.

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=\linewidth]{figs/runtime.pdf}
  \end{center}
  \caption{\textbf{Inference Latency Comparison.} The notation ``Det + Seg'' refers to the combination of latencies from the previous SOTA detection and segmentation methods. Specifically, we chose CenterFormer~\cite{Zhou_centerformer} and Cylinder3D~\cite{zhu2021cylindrical} because they share similar backbone network structures with our approach. All methods were evaluated on Nvidia A100 GPU.}
  \label{fig:runtime}
\end{figure}

\begin{table}[t]
\centering
    \caption{The ablation of different query types adopted in the dense-to-sparse XSF on the nuScenes \texttt{val} split.}
    \label{table:query}
    \resizebox{0.8\linewidth}{!}{%
    \begin{tabular}{l|c}
    \Xhline{4\arrayrulewidth}
    LiDARFormer seg only result (mIoU) & 81.7\phantom{ (-1.7)} \\
    \Xhline{2\arrayrulewidth}
    Voxel Query & 81.4 (\textcolor{green}{-0.3}) \\
    Embedding Query  & 81.3 (\textcolor{green}{-0.4}) \\
    \Xhline{4\arrayrulewidth}
    \end{tabular}
    }
\end{table}
 
 \textbf{Runtime and Model Size}
We evaluated the runtime and model size of LiDARFormer on Nvidia A100 GPU. Figure~\ref{fig:runtime} demonstrates that a multi-task network can significantly reduce latency by sharing backbone networks. Our approach has similar latency to the previous 2-stage multi-task network but outperforms it in an end-to-end 1-stage network design. Additionally, LiDARFormer employs fewer parameters (77M) than the LidarMultiNet (131M).
 
 \begin{table}[t]
\centering
    \caption{Comparison of different class feature embedding initialization methods.}
    \label{table:init}
    \resizebox{0.8\linewidth}{!}{%
    
    \begin{tabular}{l|ccc}
    \Xhline{4\arrayrulewidth}
    Initialization Method & mAP & NDS & mIoU\\
    \Xhline{2\arrayrulewidth}
    BEV & \underline{66.6} & \underline{70.8} & 82.7\\
    Voxel & 66.4 & 70.5 & \underline{82.9}\\
    \Xhline{4\arrayrulewidth}
    \end{tabular}
    }
\end{table}

\begin{figure*}[t]
  \begin{center}
    \includegraphics[width=0.9\textwidth]{figs/cmp_baseline_ours.pdf}
  \end{center}
  \caption{\textbf{Visualization of the improvement of our model on the nuScenes \texttt{val} split.} With our proposed cross-space and cross-task transformer module, our approach generates more accurate labels in the previously uncertain area. Best viewed in color.}
  \label{fig:cmp_baseline_ours}
\end{figure*}

\begin{table*}[t]
	\centering
	\caption{Segmentation results of each class on the \texttt{test} split of nuScenes. We underline the best performance in each category.}
	\label{tab:nusc_seg_test_full}
	\resizebox{\linewidth}{!}{%
	\begin{tabular}{l|c|*{16}{c}}
		\Xhline{4\arrayrulewidth}
		Model & mIoU & \rotatebox{60}{barrier} &	\rotatebox{60}{bicycle} &	\rotatebox{60}{bus} &	\rotatebox{60}{car}&	\rotatebox{60}{\makecell{construction\\vehicle}} &	\rotatebox{60}{motorcycle} &	\rotatebox{60}{pedestrian} &	\rotatebox{60}{traffic cone} &	\rotatebox{60}{trailer} &	\rotatebox{60}{truck} & \rotatebox{60}{\makecell{driveable\\ surface}} &	\rotatebox{60}{other flat} &	\rotatebox{60}{sidewalk} &	\rotatebox{60}{terrain} &	\rotatebox{60}{manmade} &	\rotatebox{60}{vegetation} \\
		\Xhline{4\arrayrulewidth}
		PolarNet~\cite{Zhang_2020_CVPR} &  69.8 & 80.1 & 19.9 & 78.6 & 84.1 & 53.2 & 47.9 &  70.5 & 66.9 &  70.0 & 56.7 & 96.7 & 68.7 & 77.7 &  72.0 &  88.5 &  85.4 \\
		PolarStream~\cite{chen2021polarstream} &  73.4 & 71.4 & 27.8 & 78.1 & 82.0 & 61.3 &  77.8 & 75.1 &  72.4 & 79.6 & 63.7 & 96.0 & 66.5 &  76.9 &  73.0 &  88.5 & 84.8 \\
		JS3C-Net~\cite{yan2020sparse} &  73.6 & 80.1 & 26.2 & 87.8 & 84.5 & 55.2 &  72.6 & 71.3 &  66.3 & 76.8 & 71.2 & 96.8 & 64.5 &  76.9 &  74.1 &  87.5 & 86.1 \\
		Cylinder3D~\cite{zhu2021cylindrical} &  77.2 & 82.8 & 29.8 & 84.3 & 89.4 & 63.0 &  79.3 & 77.2 &  73.4 & 84.6 & 69.1 & 97.7 & 70.2 &  80.3 &  75.5 &  90.4 & 87.6 \\
		AMVNet~\cite{liong2020amvnet} &  77.3 & 80.6 & 32.0 & 81.7 & 88.9 & 67.1 &  84.3 & 76.1 &  73.5 & 84.9 & 67.3 & 97.5 & 67.4 &  79.4 &  75.5 &  91.5 & 88.7 \\
		SPVNAS~\cite{tang2020searching} &  77.4 & 80.0 & 30.0 & 91.9 & 90.8 & 64.7 &  79.0 & 75.6 &  70.9 & 81.0 & 74.6 & 97.4 & 69.2 &  80.0 &  76.1 &  89.3 & 87.1 \\
		Cylinder3D++~\cite{zhu2021cylindrical} &  77.9 & 82.8 & 33.9 & 84.3 & 89.4 & 69.6 &  79.4 & 77.3 &  73.4 & 84.6 & 69.4 & 97.7 & 70.2 &  80.3 &  75.5 &  90.4 & 87.6 \\
		AF2S3Net~\cite{Cheng_2021_CVPR} &  78.3 & 78.9 & \underline{52.2} & 89.9 & 84.2 & \underline{77.4} &  74.3 & 77.3 &  72.0 & 83.9 & 73.8 & 97.1 & 66.5 &  77.5 &  74.0 &  87.7 & 86.8 \\
		GASN~\cite{ye2022efficient} &  80.4 & 85.5 & 43.2 & 90.5 & 92.1 & 64.7 &  86.0 & 83.0 &  73.3 & 83.9 & 75.8 & 97.0 & 71.0 &  81.0 &  \underline{77.7} & 91.6 & \underline{90.2} \\
		SPVCNN++~\cite{tang2020searching} &  81.1 & \underline{86.4} & 43.1 & 91.9 & 92.2 & 75.9 &  75.7 & 83.4 &  77.3 & 86.8 & \underline{77.4} & 97.7 & \underline{71.2} &  81.1 &  77.2 & 91.7 & 89.0 \\
		LidarMultiNet~\cite{ye2022lidarmultinet} & 81.4 & 80.4 & 48.4 & \underline{94.3} & 90.0 & 71.5 &  87.2 & \underline{85.2} &  80.4 & 86.9 & 74.8 & 97.8 & 67.3 &  80.7 &  76.5 & 92.1 & 89.6 \\
		\Xhline{2\arrayrulewidth}
		LiDARFormer &  81.0 & 83.5 & 39.8 & 85.7 & 92.4 & 70.8 &  \underline{91.0} & 84.0 &  80.7 & \underline{88.6} & 73.7 & 97.8 & 69.0 & 80.9 & 76.9 & 91.9 & 89.0 \\
		LiDARFormer-TTA &  \underline{81.5} & 84.4 & 40.8 & 84.7 & \underline{92.6} & 72.7 &  \underline{91.0} & 84.9 &  \underline{81.7} & \underline{88.6} & 73.8 & \underline{97.9} & 69.3 & \underline{81.4} & 77.4 & \underline{92.4} & 89.6 \\
		
		\Xhline{4\arrayrulewidth}
	\end{tabular}
	}
\end{table*}
 
\textbf{Class Feature Embedding Initialization}
In our cross-task transformer decoder, we initialize the class feature embedding using a coarse prediction and its BEV features. As shown in Table~\ref{table:init}, if we change the initialization to use voxel features, the performance of LiDARFormer will increase in the segmentation task but will decrease in the detection task.

\textbf{Analysis of Cross-task Transformer}
We compare the segmentation prediction of our model to the baseline model without our proposed transformer module. As shown in Figure~\ref{fig:cmp_baseline_ours}, we notice that the improvement usually takes place in the discontinuous area, e.g. points from one object are predicted to have labels of different classes. 

\textbf{Polar Coordinate} PolarNet~\cite{Zhang_2020_CVPR} and Cylinder3D\cite{zhu2021cylindrical} have shown the potential of polar feature representation on the LiDAR segmentation problem. It mimics the scan pattern of the LiDAR sensor to balance the point distribution across different ranges of voxels. Contrary to their finding, the experiment on Waymo Open Dataset shows the performance on mIoU has a 1.4\% drop when we transform our baseline model to the polar coordinate with a similar voxel size. We conjecture that it is because we already use a relatively small voxel size, which does not induce a huge imbalance of points accumulation in the close-range voxel. Conversely, polar coordinates suffer more distortion in the distant voxels, leading to inferior performance.

\textbf{Class-wise Segmentation Results on nuScenes}
In Table~\ref{tab:nusc_seg_test_full}, we show the class-wise performance of LiDARFormer on the test set of nuScenes. The best segmentation results of each class are scattered among the top five methods. This is due to the learning competition among different classes. For example, better performance in ``motorcycle'' class will cause a drop in ``bicycle'' class. How to deal with the competition between similar classes is still an unsolved problem.

\begin{figure*}[th]
  \begin{center}
    \includegraphics[width=0.99\textwidth]{figs/nusc_vis.pdf}
  \end{center}
  \caption{Visualization of the detection and segmentation results on nuScenes.}
  \label{fig:nusc_vis}
\end{figure*}

\begin{figure*}[th]
  \begin{center}
    \includegraphics[width=0.99\textwidth]{figs/waymo_vis.pdf}
  \end{center}
  \caption{Visualization of the detection and segmentation results on Waymo Open Dataset.}
  \label{fig:waymo_vis}
\end{figure*}

\section{Qualitative Results}
We illustrate the qualitative results of LiDARFormer on nuScenes and WOD in Figure~\ref{fig:nusc_vis},~\ref{fig:waymo_vis}. Our method can generate accurate semantic predictions in diverse environments. 
