% SIAM Article Template
\documentclass[letter,onetabnum]{siamart171218}
%\documentclass{amsart}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.
\usepackage{booktabs}
\usepackage[disable,obeyFinal,textsize=footnotesize]{todonotes}
\newcommand{\alan}[2][]{\todo[color=yellow, #1]{\textbf{Alan}: #2}}
\newcommand{\bw}[2][]{\todo[color=red!20, #1]{\textbf{Bernie}: #2}}
\newcommand{\ekin}[2][]{\todo[color=green, #1]{\textbf{Ekin}: #2}}
\input{siam_shared}


\usepackage{amsmath}

\usepackage{array}
\usepackage{fancyvrb}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{listings}
%\usepackage{fontspec}
\usepackage{mdframed}
\usepackage{graphicx,grffile}
\usepackage{amsmath,amssymb}
\usepackage{resizegather}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{subcaption}



\definecolor{ekinblue}{RGB}{75,172,198}
\definecolor{ekinorange}{RGB}{247,150,70}
\definecolor{ekinpurple}{RGB}{112,48,160}
\usepackage{xcolor}
\usetikzlibrary{arrows}
\newcommand{\matr}[1]{\mathbf{#1}}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

%  \makeatletter
% \def\l@subsection{\@tocline{2}{0pt}{2.5pc}{5pc}{}}
% \def\l@subsubsection{\@tocline{2}{0pt}{3.5pc}{5pc}{}}
%  \makeatletter

% % \tiny and \scriptsize are too small and too big respectively for the Julia listings.
% % This should be adjusted if further font size changes should be made. 
% \makeatletter
% \newcommand\notsotiny{\@setfontsize\notsotiny\@vipt\@viipt}
% \makeatother

\begin{document}
%\input{julia_font}
%\input{julia_listings}



\author{Alan Edelman\thanks{Department of Mathematics and CSAIL, MIT, Cambridge, MA 
  (\email{edelman@mit.edu}).}
  \and Ekin Aky\"urek\thanks{Department of EECS and CSAIL, MIT, Cambridge, MA  \email{(akyurek@mit.edu)}}
\and Yuyang Wang\thanks{AWS AI Labs, Santa Clara, CA 
  (\email{yuyawang@amazon.com}). Work done prior joining Amazon.}}
  

\maketitle


% REQUIRED
\begin{abstract}
% Automatic differentiation can be implemented by tracking underlying computations, storing intermediate values and backpropagating gradients through these tracked values. Graph representations are commonly used to represent the computation.

 We present a linear algebra formulation 
 of backpropagation which allows the calculation of gradients by using a generically written ``backslash'' or Gaussian elimination on triangular systems of equations.  Generally the matrix elements are  operators.
 
 This paper has three   contributions: 
\begin{enumerate}
\item
 It is of intellectual value to replace traditional treatments of automatic differentiation with a (left acting) operator theoretic, graph-based approach.

\item
 Operators can be readily placed in matrices in software in
programming languages such as Ju lia as an implementation option.

\item We introduce a novel notation, ``transpose dot'' operator ``$\{\}^{T_\bullet}$''
that  allows the reversal of operators.

\end{enumerate}
 
    We demonstrate  the elegance of the operators approach in a suitable programming language
consisting of
generic linear algebra operators
such as Julia
 \cite{bezanson2017julia}, and that it is possible
 to realize this abstraction in code.
 Our implementation shows how generic linear algebra can allow operators as elements of matrices, and without rewriting  any code, the software carries through to completion giving the correct answer.
 

\end{abstract}

% % REQUIRED
% \begin{keywords}
%   backpropagation, gradients, neural network, backslash, forward-mode, reverse-mode, Julia
% \end{keywords}

% % REQUIRED
% \begin{AMS}
%   62M45, 15A06, 15A09
% \end{AMS}









\section{Preface: Summary and the Challenge}
\label{preface}

This paper provides the mathematics to show how an operator theoretic, graph-based approach can realize 
backpropogation using backsubstitution of a matrix whose elements are
operators. 

As a showecase result, one can backpropogate to compute the gradient
on feed-forward neural networks with
$$\nabla J = M^T*((I-L)^T \backslash g).$$



We then set up a challenge to ourselves. 
Could we vivify the linear algebra math 
expressed in the above equation
 by simply typing
 the command (after basic setup)
 \begin{center}
    \includegraphics[width=2in]{goal.png} \raisebox{.1in}{\large ?}
\end{center}
We demonstrate that indeed the 
backpropogated gradient 
can be computed, almost by magic, in a programming language
that allows for generic programming as operators as elements of matrices.

The software in
Section \ref{challenge} 
is by itself interesting not for the usual reasons
of what it does, but in this case how it does it:
how a powerful language 
with generic programming and multiple dispatch can allow this abstract mathematical formulation to be realized. 




\section{Introduction: Linear Algebra, Graphs, Autodiff, Operators, and Julia}





Automatic differentiation(AD)\ekin{I think we could clarify what we work on isn't AD but the differentiation part of the AD? Because we manually create the matrices that corresponds to the computation graph} is fundamental to gradient-based 
optimization of neural networks and is used throughout scientific computing.
There are two popular approaches to AD namely forward and reverse modes~\cite{JMLR:v18:17-468, revels2016forward}.
A common high level description of AD is that 
it is really ``only''  the chain-rule.
The centuries old technology of taking derivatives
 is taking on a modern twist in the form of Differentiable Programming~\cite{Wik:DifProg,li2018differentiable}.
 Who would have thought that one of the most 
 routine college course material would now be the
 subject of  much renewed interest both in applied mathematics and computer science?


This paper introduces the notion that AD is best understood with a matrix based approach.  The chain-rule explanation takes on the role of
a distraction or
extra baggage.
We suspect that while the chain-rule is well known,
it is understood mechanically rather than deeply
by most of us.
We argue that a linear algebra based framework for AD, while mathematically equivalent to other approaches,
provides a simplicity of understanding, and equally importantly a
viable approach to  implementation worthy of further study.


Regarding software, while most high level languages allow for matrices whose elements are scalars, the ability to work with matrices
whose elements might be operators without major changes to the elementwise software is an intriguing abstraction for software.
We discuss a Julia implementation that makes this step particularly mathematically natural.

  It is our view that a linear algebraic approach sheds light on how backpropagation works in its essence. 
For example, we theoretically connect backpropagation to the back substitution method for triangular systems of equations. Similarly, 
forward substitution corresponds to forward mode calculation of automatic differentiation.
As is well documented in the preface to the book Graph Algorithms in the Language of Linear Algebra \cite{kepner2011graph} there have been many known benefits to formulate mathematically a graph algorithm in linear algebraic terms. One of the known benefits is cognitive complexity. 
%In particular we show that with the correct abstractions, backpropagation can be viewed simply as a ``backslash'' operation on triangular matrices, i.e. backpropagation equals backward substitution.  

The ability to implement these abstractions and retain performance is demonstrated in Julia,  a language that facilitates abstractions, multiple dispatch, the type system, and which offers generic operators.

%\todo[inline]{A summary of the three parts.}


% The rest of the paper is organized as follows. In Section 2, we introduce the basics of the matrix methods for weighted paths with several concrete examples, and discuss its connection to adjoint method in scientific computing. In Section 3, using multi-layer perceptron (MLP) as an example, we show  how it can be derived using a simple `backlash' with the tool developed in Section 2. 

% More importantly, we show that such a technique allows us to immediately generalize to a much broader spectrum of Neural Networks, where the ``weights'' could lie in any Riemannian manifold. The proposed framework enables us to easily derive the methods in (Inderjit's paper). Lastly, we provide numerical experiments, followed by a concluding remark.
% \todo{inderjit paper stuff}

\section{A Matrix Method for Weighted Paths}

\subsection{``Forward and Back'' through Graphs and Linear Algebra}
\label{frg}

In the spirit of reaching the simple mathematical core,
let us strip away the derivatives, gradients, Jacobians,  the computational graphs, and the ``chain rule''  that clutter the story of how is it possible
to compute the same thing forwards and backwards.
%\footnote{We will add back the details in Section 3 of this paper.}
We set ourselves
the goal of explaining the essence of forward mode vs backwards (reverse) mode in AutoDiff with a single figure. Figure \ref{hw} is the result. Note that
``forward mode'' differentiation
is not to be confused with the forward computation
of the desired quantity.

\subsubsection{Path Weights on Directed Graph} Consider a directed acyclic graph (DAG) with edge weights as 
in Figure \ref{hw} where nodes 1 and 2 are sources (starting nodes), and node 5 is a sink (end node). The problem is to compute the {\bf path weights}, which we define as the products of the weights from every start node to every sink node.

% \alan[inline]{make ``path weight'' a formal definition, where is the ``edge weights''
% matrix defined, labelled graph?}

Evidently, the path weights that we seek in Figure \ref{hw} may be obtained by calculating
\begin{equation}
\label{assoc}
\mbox{path weights} =
\underbrace{\begin{pmatrix} 1 & 0  \\  0 & 1 \\ 0 & 0  \\  0 & 0 \\ 0 & 0 \end{pmatrix}^{\! T}}_{\mbox{sources}}
  (I-L^T)^{-1} 
  \underbrace{\begin{pmatrix} 0 \\  0 \\ 0 \\  0 \\ 1 \end{pmatrix}}_{\mbox{sink}} ,
\end{equation}
where $L^T$ is displayed in the lower left of Figure \ref{hw}.
One explanation of why
(\ref{assoc})
works for calculating the path weights is that $(L^T)^k_{ij}$ sums the path weights of length $k$ from node $i$ to node $j$
and  $(I-L^T)^{-1} = I + L^T + \ldots + (L^T)^{n-1}$\ekin{I guess this is true for this specific matrix, should we mention that?}  then counts  path weights of all lengths.
%\alan[inline]{maybe a bit more explanation}
\definecolor{dg}{rgb}{0,0.5,0}
\begin{figure}[H]
\begin{center}
\includegraphics[width=.9\textwidth]{weights.pdf}
\caption{
\label{hw} 
(Upper Left:) Multiply the weights along the paths from source node 1  to sink node 5 and also source node 2 to sink node 5 to obtain
{\color{ekinpurple}{acd}} and {\color{ekinpurple}{bcd}}.
{\color{ekinblue} (Right Blue:) The obvious forward method.}
{\color{ekinorange} (Right Orange:) A backward method that requires one fewer multiplication. } 
(Below:) A matrix method: if
$L^T_{ij}=$ 
the weight on  edge $ij$, then $(I-L^T)^{-1}$ simultaneously exhibits the forward and backward methods. \newline
{\bf Color Coding} : {\color{ekinpurple} Purple: target weights}, {\color{ekinblue} Blue:forward computation}, {\color{ekinorange} Orange: backwards computation.}
}
\end{center}
\end{figure}


Moreover, if one follows step by step the linear algebra methods of forward substitution for lower triangular matrices or back substitution for
upper triangular matrices one obtains exactly the same algorithms on the graph which we summarize in Figure \ref{forbac}.

\begin{figure}[h]
$$\begin{array}{|lc|}
\hline
\multicolumn{2}{|l|}{\mbox{Two Equivalent Ways to Compute the Path Weights in Figure \ref{hw}:}} \\
 \hline  &  \\[.03in]
\mbox{Forward Substitution: }  &  \begin{pmatrix} 0 \\  0 \\ 0 \\  0 \\ 1 \end{pmatrix}^{\! \! \! \!  T} \! 
\ \left[ (I-L)^{-1} \begin{pmatrix} 1 & 0  \\  0 & 1 \\ 0 & 0  \\  0 & 0 \\ 0 & 0 \end{pmatrix}  \right]  \! \! \! \\[.5in]
\mbox{Back  Substitution: }  &
\begin{pmatrix} 1 & 0  \\  0 & 1 \\ 0 & 0  \\  0 & 0 \\ 0 & 0 \end{pmatrix}^{\! \! \! \!  T}     \! \! 
\left[ (I-L^T)^{-1}    \begin{pmatrix} 0 \\  0 \\ 0 \\  0 \\ 1 \end{pmatrix} \ \   \right]   \\[.5in]   \hline
 \end{array}$$
\caption{\label{forbac}The forward and backward approaches 
compared: Both are seen equivalently as a choice of parenthesizing Equation \ref{assoc}
or as forward substitution vs.\ back substitution.  Generally speaking, when the number of sources is larger than the number of sinks,
one might expect the backward  method to have less complexity.}
\end{figure}

%\newpage

\subsubsection{Generalizing ``Forward and Back'' to a Catalan number of possibilities}


% We recall from elementary linear algebra that we can  factor $(I-L^T)^{-1}$ with elimination matrices.
% Following our example,   $(I-L^T)^{-1}$ ,
% \[
% \begin{array}{c}
% (I-L^T)^{-1} = %E_1 E_2 E_3 E_4= 
% \begin{pmatrix}
% 1 & . & a & . & . \\
% . & 1 & .  & . & . \\
% . & .& 1 & .  & .  \\
% . & .& . & 1 & .  \\
% . & .& . & .  & 1  \\
% \end{pmatrix}
% \begin{pmatrix}
% 1 & . & .& . & . \\
% . & 1 & b  & . & . \\
% . & .& 1 & .  & .  \\
% . & .& . & 1 & .  \\
% . & .& . & .  & 1  \\
% \end{pmatrix}
% \begin{pmatrix}
% 1 & . & .& . & . \\
% . & 1 & .  & . & . \\
% . & .& 1 & c  & .  \\
% . & .& . & 1 & .  \\
% . & .& . & .  & 1  \\
% \end{pmatrix}
% \begin{pmatrix}
% 1 & . & .& . & . \\
% . & 1 & .  & . & . \\
% . & .& 1 & . & .  \\
% . & .& . & 1 & d \\
% . & .& . & .  & 1  \\
% \end{pmatrix}
% \end{array} .
% \]

% \bw[inline]{not sure what's the role of the following equation, which seems to be a straightforward substitution of $(I -L^T)^{-1}.$}


% It follows that




Continuing with the same $L$ matrix from Section \ref{frg},  we can
begin to understand all of the possibilities including the forward approach,
the backwards approach, the mixed-modes approaches, and even more possibilities:
%\vspace{-2in}
\[ 
 \! \!  \! \! \!
\begin{array}{c}
\begin{pmatrix} 1 & 0  \\  0 & 1 \\ 0 & 0  \\  0 & 0 \\ 0 & 0 \end{pmatrix}^{\! T}  (I-L^T)^{-1} \begin{pmatrix} 0 \\  0 \\ 0 \\  0 \\ 1 \end{pmatrix} = \\ \\
\begin{pmatrix} 1 & 0  \\  0 & 1 \\ 0 & 0  \\  0 & 0 \\ 0 & 0 \end{pmatrix}^{\! \!  \!  \!T} \! \!  \! \!
\begin{pmatrix}
1 & . & a & . & . \\
. & 1 & .  & . & . \\
. & .& 1 & .  & .  \\
. & .& . & 1 & .  \\
. & .& . & .  & 1  \\
\end{pmatrix}\! \!  \! \!
\begin{pmatrix}
1 & . & .& . & . \\
. & 1 & b  & . & . \\
. & .& 1 & .  & .  \\
. & .& . & 1 & .  \\
. & .& . & .  & 1  \\
\end{pmatrix} \! \!  \! \!
\begin{pmatrix}
1 & . & .& . & . \\
. & 1 & .  & . & . \\
. & .& 1 & c  & .  \\
. & .& . & 1 & .  \\
. & .& . & .  & 1  \\
\end{pmatrix}\! \!  \! \!
\begin{pmatrix}
1 & . & .& . & . \\
. & 1 & .  & . & . \\
. & .& 1 & . & .  \\
. & .& . & 1 & d \\
. & .& . & .  & 1  \\
\end{pmatrix}\! \!  \! \!
\begin{pmatrix} 0 \\  0 \\ 0 \\  0 \\ 1 \end{pmatrix}
 \!  \!   .
\end{array} 
\]

%\vspace{-2in}




It is well known \cite{stanley2015catalan}, that there are a Catalan number, $C_5=42$, ways to parenthesize the above expression.
One of which evaluates left to right; this is forward substitution  which computes the graph weights forward.
Another evaluates from right to left,  backward substitution.   There are three other ``mixed-modes''  \cite{1810.08297}
which combine forward and backward methods.  The remaining 37 methods require matrix-matrix multiplication as a first step.
We encourage the reader to work out some of these on the graph. Partial products correspond to working through subgraphs.
Perhaps readers might find cases where working from the middle outward can be useful.
For example it would be possible to go from the
middle outward using the Example of Figure \ref{hw}: we would go from  $c$ to $cd$ then
compute $acd$ and $bcd$.

\subsubsection{Edge Elimination}

It is possible to eliminate an edge
(and preserve the path weights)
by moving the weight of the edge to the
weights of the incoming edges.   We illustrate this in Figure \ref{elim}
by eliminating the edge from node 3 to node 4, moving the weight $c$ to the incoming edges by multiplication.
The corresponding linear algebra operation on $L^T$ is the deletion of column 3 and row 3
and the rank 1 update based on this column and row with the (3,3) element deleted.
The corresponding linear algebra operation on $(I-L^T)^{-1}$ is merely the deletion of column 3 and row 3.
This example  is representative of the general case.
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.65\textwidth]{elimination.pdf}
\end{center}
\caption{\label{elim} Elimination of the edge from node 3 to node 4 
on the graphs and with matrices.  The matrix versions involve
a rank one update in the case of the Edge Weights Matrix and a
deletion of a row and column in the Path Weights Matrix.
 }
\end{figure}



\subsubsection{Edge addition at the Sink Node}

For reasons that shall become clear when we introduce
loss functions, we will be  interested in the case where the edge weight graph is modified by adding one edge to the sink node. 
Continuing our example from Figure \ref{elim}, we add  
an edge ``$e$'': 
\begin{equation}
\label{nowloss}
\mbox{original path weights} =
\underbrace{\begin{pmatrix} 1 & 0  \\  0 & 1 \\ 0 & 0  \\  0 & 0 \\ 0 & 0 \end{pmatrix}^{\! T}}_{\mbox{sources}}
\underbrace{  (I-L^T)^{-1}}_{   \tiny  \mbox{ path weights}  \mbox{ matrix} } 
  \underbrace{\begin{pmatrix} 0 \\  0 \\ 0 \\  0 \\ 1 \end{pmatrix}}_{\mbox{sink}} 
\end{equation}
which updates by augmenting by one node to become
\begin{equation}
\label{nowloss2}
\mbox{updated path weights } =
\underbrace{\begin{pmatrix} 1 & 0  \\  0 & 1 \\ 0 & 0  \\  0 & 0 \\ 0 & 0  \\ 0 & 0 \end{pmatrix}^{\! T}}_{\mbox{sources}}
\underbrace{
\begin{pmatrix}
  (I-L^T)^{-1}  &. \\
   . & 1
  \end{pmatrix}
  \begin{pmatrix}
  1& . & .&  . & . & .   \\
    . & 1 & .&  . & . & . \\
    . & . & 1&  . & . & .  \\
      . & . & .&  1 & . & . \\
         . & . & .&  . & 1 &  \! \! e  \! \! \\
 . & . & .&  . & . & 1\\ 
  \end{pmatrix}}_{\tiny \mbox {updated} \mbox{ path weights}  \mbox{ matrix}}
  \underbrace{\begin{pmatrix} 0 \\  0 \\ 0 \\  0 \\ 0 \\ 1 \end{pmatrix}}_{\mbox{sink}} . 
\end{equation}

The update from the path weights matrix in Equation \ref{nowloss} to the updated
path weights matrix in Equation \ref{nowloss2} 
can probably be verified in many ways.
One simple way is to look at the explicit elements of the path weights matrix
before and after  noticing that the new matrix has a column with one more factor
of $e$ augmented with a $1$.
Another way, is to update the edge weights matrix $L$ and compute the block inverse.

It is an easy exercise in linear algebra to show that Equation \ref{nowloss2}  is the same as Equation \ref{newloss3} which
folds the added edge $e$  multiplicatively into the sink vector.
\begin{equation}
\label{newloss3}
\mbox{ path weights (with loss) } =
\underbrace{\begin{pmatrix} 1 & 0  \\  0 & 1 \\ 0 & 0  \\  0 & 0 \\ 0 & 0 \end{pmatrix}^{\! T}}_{\mbox{sources}}
  (I-L^T)^{-1} 
  \underbrace{\begin{pmatrix} . \\  . \\ . \\  . \\ e \end{pmatrix}}_{\mbox{sink}} .
\end{equation}

\subsection{Examples of DAGs and Weighted Paths}

\subsubsection{The ``Complete DAG'' and Weighted Paths}

Consider as an example (Figure~\ref{fulldag})  the complete DAG on four nodes with graph weights evaluated through the forward and reverse mechanism.
There is one source and one sink.  We find that 
this complete DAG example reveals most clearly the equivalence between path weights and the inverse matrix.

We see that  the forward mode folds in the edges labelled ``a,'' then ``b,'' then ``c.''  This works through the matrix $L^T$ by columns.
The backward mode folds in the edges with subscript ``3,'' then ``2,'' then ``1.''  This works through the matrix $L^T$ by rows from bottom to top.

\begin{figure}[H]
\begin{center}
\includegraphics[width=.9\textwidth]{figure2.6ekin2.png}
\caption{\label{fulldag} 
The complete DAG on four nodes illustrates a symmetric situation where forward and reverse have the same complexity but arrive at the same answer through different operations.
 }
\end{center}
\end{figure}

\subsubsection{The ``multi-layer perceptron DAG''  and Weighted Paths}




Figure~\ref{mlp} is the DAG for the derivatives in a deep neural network. It may be thought of as a spine with feeds for 
parameters (nodes 1,2,3, and 4 in the figure).  

% \alan[inline]{
% We concentrate here on the weighted paths corresponding to the
% dependency structure of a multi-layer perceptron
% completely ignoring the computational graph which we will
% full develop in Section ???3?? of this paper.
% }

% \alan[inline]{need to connect this
% to the later section}

\begin{figure}[h]
\begin{center}
\includegraphics[width=\textwidth]{fig2.5.png}
\caption{ This diagram contains most of what is needed to understand forward and reverse propagation 
of derivatives through a deep neural network.
The details of what the weights look like will come later. 
If we take 
%$n=3$
$n=4$
for the pictured network, the sources are labeled $1:(n+1)$ and the sink is labeled 
%$2(n+1)$.
$2n$.
Forward mode requires 
%$n(n+1)/2$
$n(n-1)/2$
multiplications while reverse mode requires
%$2n-1$
$2n-3$
multiplications.
 }
 \label{mlp}
\end{center}
\end{figure}


% In the examples in this paper, no two sources have been connected by an edge.  
% Being connected by
% an edge is not precluded by the theory.
If sources are labeled $1,\ldots,s$
(in Figure \ref{mlp}, $s=4$),
then  the top left $s$ by $s$ matrix in $L^T$ is the zero matrix
as there are no connections.
We can then write
\begin{equation}
L^T  = \begin{pmatrix} 0 & M \\ 0 & \tilde{L}^T \end{pmatrix},    
\label{eqn:lt}
\end{equation}
and therefore
\[
(I-L^T)  = \begin{pmatrix} I & -M \\ 0 & I-\tilde{L}^T \end{pmatrix},
\mbox{ and } (I-L^T)^{-1} = \begin{pmatrix}  I &  M(I-\tilde{L}^T)^{-1}\\ 0 & (I-\tilde{L}^T)^{-1} \end{pmatrix}.
\]
Notice that the matrix $M$ corresponds to connections between the sources and internal nodes. In this example
$M$ is diagonal corresponding to a bipartite matching between nodes $1,2,3,4$ and $5,6,7,8$. The $\tilde{L}^T$ matrix represents internal connections, in this case it is the ``spine'' linearly connecting nodes $5,6,7,8$. 

If the last node is  the one  unique sink, then we obtain the useful formula
\begin{equation}
\label{likeadjoint3}
 \mbox{Path weights} =  M(I-\tilde{L}^T)^{-1} \begin{pmatrix} 0 \\ \vdots \\0 \\ 1 \end{pmatrix},
  \end{equation}
  

We can now take a close look at Figure \ref{mlp} and fully grasp the path weight structure.
The spine consisting of $a,b,c,1$ requires the computation of the cumulative suffix product  $1,c,bc,abc$. What follows is an element-wise multiplication by $z,y,x,w$, and
$$\tilde{L}^T = \begin{pmatrix} 
. & a & . & . \\
. & . & b & . \\
. & . & . & c \\
. & . & . & . \\
\end{pmatrix},
M = \begin{pmatrix}
w & . & .& . \\
. & x & . & . \\
. & .& y & . \\
. & . & . & z
\end{pmatrix}
$$
from which we can calculate the last column of $M(I-\tilde{L}^T)^{-1}$ 
or
\begin{equation}
\label{likeadjoint}
 M(I-\tilde{L}^T)^{-1} \begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix}=
 \begin{pmatrix} wabc \\ xbc \\ yc \\ z \end{pmatrix}.
  \end{equation}

\subsection{Computational Graphs, Derivative Graphs, and their superposition}
% \alan[inline]{this got deleted, i undeleted, let's rethink}


Many treatments of automatic differentiation introduce
computational graphs at the start of the 
discussion.  Our treatment shows that this is not necessary. However, in the end the key application of edge weights will be as derivatives of computed quantities.  To this end, we define

\begin{defn}
A computational graph is
a node labelled DAG, where leaf nodes
consist of variable names, and 
non-leaf nodes contain variable names and
formulas that depend on incoming variables.
\end{defn}

We remark that there are variations on where the variable names and formulas live on a computational graph, but we believe
the definition here is the cleanest when wishing to incorporate derivative information.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=.4]{comp_deriv}
\caption{Computational (node) graph, derivative (edge) graph, and their ``superposition.''
}
\label{figcomp}
\end{center}
\end{figure}

\begin{center}
\begin{tabular}{ m{3cm}| m{8cm} } 
\toprule
Player & Description \\
\midrule
Edge weight from node $i$ and $j$ & These are the derivatives of  one step of
a computation.  These can be scalars
but in general these are Jacobian matrices. \\
\midrule
Path weight from node $i$ to  $j$ & These are the derivatives that
reach back into a chain of computations.
The chain rule states that if you multiply
(``chain together'') the  derivatives ate each step
you get the  dependence of one variable on an earlier variable.\\
\midrule
Source & The sources in the graph are typically
parameters in real computations, as many modern applications
are interested in the derivatives with respect to the input
parameters.\\
\midrule
Sink & The sink is usually what's known as a loss function
in modern applications.\\
 \bottomrule
\end{tabular}
\end{center}

\subsubsection{The chain rule, derivatives, and Jacobians}

Here we say explicitly how the edge weights and path weights
relate to derivatives in a computation. 

Consider the computation from \ref{figcomp}.  The next three
algorithms show the computation, the derivatives of each
line of code, and the overall derivatives.  We see that
the one step derivatives are edge weights and the overall derivatives
are path weights.

If the final output is a scalar, we immediately have that the gradient with respect to $p$ is exactly the path weight defined in (\ref{likeadjoint3}), 
\begin{equation}
\label{likeadjoint2}
\text{gradient = the last column of }\ M(I-\tilde{L}^T)^{-1}.
\end{equation}

That's it! Equation~(\ref{likeadjoint2}) is all that is about adjoint method (discussed in the subsequent section) in scientific computation and backpropagation in deep learning.





\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H] 
\caption{Simple Algorithm Example from Figure \ref{figcomp}}\label{gen1}
\begin{algorithmic}[1]
%\State { input constants  \color{ekinorange} $x,y$} 
  \State $ p \gets \text{multiply}(x,y)$  
  \State $q \gets \text{square}(p)$
  \State $r \gets  \text{exp\_neg}(q)$
    \State {\color{ekinorange} output $r$}  \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H] 
\caption{Edge weights 
(derivatives of one line of code)}
\label{gen2}
\begin{algorithmic}[1]
  \State $d\{\text{multiply}(x,y)\}/d x = y$ ($\frac{\partial p}{\partial x} $)
    \State $d\{\text{multiply}(x,y)\}/d y  = x$ ($\frac{\partial p}{\partial y} $)
  \State $d\{\text{square}(p)\}/d p = 2p$ ($\frac{\partial q}{\partial p} $)
  \State $d\{\text{exp\_neg}(q)\}/d r =-e^{-q}$ ($\frac{\partial r}{\partial q} $)
\end{algorithmic}
\end{algorithm}
\end{minipage}

\begin{center}
\begin{minipage}{0.46\textwidth}
\begin{center}
\begin{algorithm}[H] 
\caption{Path weights 
(Chained derivatives)
}\label{gen3}
\begin{algorithmic}[1]
  \State $d r/d x = y \times 2p \times (-e^{-q})$
  (Chain  lines 1,3, and 4 of Algorithm 2.2) 
  \State $d r/d y = x \times 2p \times (-e^{-q})$
  (Chain lines 2,3, and 4 of Algorithm 2.2) 
\end{algorithmic}
\end{algorithm}
\end{center}
\end{minipage}
\end{center}

% \begin{figure}[H]
% \begin{center}
% \includegraphics[width=\textwidth]{references/bipartite.pdf}
% \caption{Two specific graph structures of concern in this paper. There are two types of nodes: one corresponds to the intermediate and final results $x'$s coming from the computational flow (blue), and the others are parameters $p$ (green). A bipartite graph is formed from the blue nodes to the green ones.}
% \label{fig:bipartite}
% \end{center}
% \end{figure}

% \todo[inline]{delete?}
% In this paper we are primarily interested in the structures shown above in Figure~\ref{fig:bipartite}, whose $L$ bears the same form as in (\ref{eqn:lt}). Note that the gradient of the green nodes, \emph{the parameters}, is what we are after in the applications.

\section{Key Idea: Linear Operators as elements of Matrices}
\label{sec:matrix}


\label{bigops}

We will illustrate in Section \ref{challenge} the value of software that allows
linear operators as elements of matrices.  This section will set
the mathematical stage.

Consider a matrix operation of $X$ such as
$T_{A,B}: X \mapsto BXA^T,$ how should we represent the Jacobian $\partial T/\partial X$?

 


%Let us assume $X$ is $m \times n$ and $A$ and $B$ ar
%$n_1 \times n$ and $m_1 \times m$ respectively so that $T$ is $m_1 \times n_1$.



Before we answer, we remind the reader how the Kronecker product works.  One view of the Kronecker product
$A \otimes B$ of two matrices is that it multiplies every element in $A$ times every element of $B$ placing the elements in such
a way that we have the identity
$$ (A \otimes B) (\mbox{vec}(X)) = \mbox{vec}(BXA^T),$$
where  $vec$ denotes the flattening of a matrix $X$ into a vector by stacking its  columns. We may abuse notation when there is no confusion and write
$$ (A \otimes B) (X) = BXA^T,$$
for the linear operator that sends $X$ to $BXA^T$.


Identifying the matrix $A \otimes B$ with the operator is more than a handy convenience, it makes computations practical in a software
language that allows for this. Table~\ref{tab:operators2} indicates some operators of interest.


\begin{table}[h]
\centering
\begin{tabular}{m{3cm}|c|l|lc}


\toprule
 & Symbol & Definition & \multicolumn{2}{c}{ Dense Representation} \\ 
 \midrule
Kronecker Product of $A,B$  &  $A \otimes B$ &  $X \mapsto  BXA^T\rule{0pt}{3ex} $ &  $A \otimes B$ & $m_1n_1 \times mn$ \\
\midrule
Left Multiplication by $B$ & $B_L$ & $ X \mapsto BX$  & $I \otimes B$ & $m_1n \times mn$\\
\midrule
Right Multiplication by $A$ & $A_R $ & $X \mapsto XA$  & $A^T \otimes I$ & $m n_1 \times mn$ \\
\midrule
Hadamard Product with $M$  &  $M_H$ & $X \mapsto M .*X $  &  diag(vec($M$))& $m n \times mn$ \\
\midrule
Matrix inner product with $G$ & $G^{T_\bullet}$ & $X \mapsto \mbox{trace}(G^TX) $&
vec($G$)$^T$ & $1 \times mn$ \\
\midrule
\multicolumn{4}{l} {\tiny (We overload $A \otimes B$ to be both the operator and the matrix.)} \vspace{-.1in} \\
%\multicolumn{5}{c}{\tiny (We overload $A \otimes B$ to be both the operator and the matrix.)}
%\bottomrule
\end{tabular}
\caption{ Matrix Operators  and the size of their  dense representations
assuming \newline \hspace*{.2in}{ $X: m \times n,$ \ \  $A:n_1 \times n,$  \  \   $B:m_1 \times m,$ \ \  $M:m \times n,$ \ \  $G:m \times n$.} 
}
\label{tab:operators2}
\end{table}





Consider the inner product $\langle X,Y \rangle =\mbox{trace}(X^TY)$.  
The identity $$\langle X,AY \rangle=\langle A^TX,Y \rangle$$
implies $$(A_L)^T=(A^T)_L,$$
in words, the  operator adjoint with respect to the left multiplication by $A$ operator is left multiplication by $A^T$.


The  operator transposes are

{ \setlength{\tabcolsep}{2pt} 
\begin{tabular}{rcl}
$(A_L)^T$ &= & $(A^T)_L$  \\
$(B_R)^T$& = &$(B^T)_R$ \\
$(M_H)^T$& = &$M_H$ (symmetric)
\end{tabular}
}


\begin{defn}
We wish to propose a very carefully
thought out notation for another useful operator, the matrix inner (or dot) product with $G$.
We will denote $G^{T_\bullet}$ (``$G$ transpose dot.'')  This operator takes a matrix $X$ of the same size as $G$ and returns the scalar, $G^{T_\bullet}X \equiv$ Tr$(G^TX)$= vec($G$)$^T$vec(X) = $\sum G_{ij}X_{ij}$. 
\end{defn}

Many communities choose a notation where small Roman letters
denote a column vector, so that $x \mapsto g^Tx$ denotes
a linear function of $x$.  Those use to this notation
no longer ``see'' the transpose so much as turning a column into
a row, but rather  they see the linear function $g^T$
as an object that ``eats'' vectors and returns scalars.
In the same way we propose that one might denote a linear
function of a matrix $X\mapsto tr(G^TX)$ with the operator
notation $X \mapsto G^{T_\bullet}X$, an operator that ``eats'' 
matrices and returns scalars.





\begin{lemma}
If  the superscript ``$()^T$''  is overloaded to denote real operator adjoint or matrix transpose
as appropriate, ${\cal L}$ is a linear
operator and $G$ is a matrix, then we have the operator identity:
$({\cal L}^TG)^{T_\bullet}=G^{T_\bullet}{\cal L}.$
Notice that if we pretend all letters are just matrices and
you ignore the dot, the notation has the appearance of the familiar transpose rule.

\end{lemma}

\noindent {\bf Proof:} \newline
We have that for all $X$,
$$
({\cal L}^TG)^{T_\bullet}X = \langle {\cal L}^TG,X \rangle  = \langle G, {\cal L}X \rangle =
G^{T_\bullet} {\cal L}X,$$
showing that as operators
$({\cal L}^TG)^{T_\bullet}=
G^{T_\bullet} {\cal L}.$

As an example,
$$(A^T_LG)^{T_\bullet} = X \mapsto tr( (A^TG)^TX)$$
and
$$G^{T_\bullet}A_L = X \mapsto  tr(G^TAX) ,$$
which shows that $(A^T_LG)^{T_\bullet} = G^{T_{\bullet}}A_L$.
We encourage the reader to follow the matrices
$A,G,A^T$ and the operators 
$A_L^T,A_L,(A^T_LG)^{T_\bullet},G^{T_{\bullet}}.$
(See Section \ref{notation} for why this notation can be valuable.)

\section{Operator Methodology}

\begin{table}
    \centering
    \begin{tabular}{m{1.8cm}|l llll}
    \toprule
    % Graph
    &  \multicolumn{5}{c}{\includegraphics[width=.4\textwidth]{mlp4.png}}\\
    \midrule
    Gradient  &  $\nabla_p\mathcal{L}= $  \hspace{.2in} \textcolor{red}{$M^T$} & $\times$ & \hspace{.3in}   \textcolor{blue}{$(I - L)^{-1}$} & $\times$ &\hspace{.1in}  \textcolor{cyan}{$g$}\\ \\
    %    & \multicolumn{3}{c}{$\nabla_p\mathcal{L} = \textcolor{red}{M}\textcolor{blue}{(I - L)^{-1}}\textcolor{cyan}{g_n}$}\\
    %\midrule
    & = $\color{red}{\begin{pmatrix}
m_1  &  &&  \\
 & m_2  &  &  \\
& & m_3  &  \\
 &  &  & m_4
\end{pmatrix}}^T$ & $\times$ & 
$\color{blue}{\begin{pmatrix} 
I &  & & \\
 -l_2 & I &    & \\
 & -l_3  & I &    \\
 &  & -l_4 & I\\
\end{pmatrix}}^{-T}$
& $\times$ & $\color{cyan}{\begin{pmatrix} . \\ . \\ . \\ g_4 \end{pmatrix} }$\\
    \midrule
(i) Scalar & & & & \\
   $p = w$ & \multicolumn{2}{l}{$m_i = \delta_ix_{i-1}$} & \multicolumn{2}{l}{$l_i = \delta_{i}w_i$} & $g_4 = {\mathcal L}^\prime(x_4)$\\ \\
(ii) Vector  & & & & \\ 
$p = [w, b]$ &  \multicolumn{2}{l}{$m_i = [\delta_ix_{i-1} \  \delta_i]$} &  \multicolumn{2}{l}{\hspace*{.1in} \textquotesingle \textquotesingle \hspace*{.1in} \textquotesingle \textquotesingle} & \hspace*{.1in} \textquotesingle \textquotesingle \hspace*{.1in} \textquotesingle \textquotesingle\\ \\
(iii) Matrices  & & & & \\
$p=[W, B]$ & \multicolumn{2}{l}{$m_i = 
[{\Delta_i}_H  \circ {X_{i-1}}_R \,\, \ {\Delta_i}_H]$} & \multicolumn{2}{l}{$l_i =  {\Delta_{i}}_H \circ {W_{i}}_L$} & $g_4 = \nabla_{X_4} {\cal L}$\\
%\bottomrule
& \multicolumn{4}{c}{${}^\uparrow \! \! \! - {\rm Operators}{} - \! \! \! ^\uparrow$} & \\
\bottomrule
    \end{tabular}
    \caption{Algebraic Structure for a basic neural network when the parameters are (i) only scalar weights (ii) a weight/bias vector, (iii) a vector of weight/bias matrices.  We emphasize the common algebraic structure and the benefit of software that can represent matrices of vectors and matrices of operators.}
    \label{tab:my_label}
\end{table}















\subsection{Matrices of scalars}

\begin{center}
\begin{minipage}{.6\linewidth}
\begin{algorithm}[H] 
\caption{Scalar Neural Net Without Bias}\label{nnalg}
\begin{algorithmic}[1]
  \State {\color{ekinorange} input $x_0$} 
    \For{i = 1 to $N$}
     \State {\color{ekinorange} input $w_i$}
           \State $x_{i}  \gets h_i(w_{i}  x_{i-1} $)

          \State {\color{ekinblue}$(\delta_i \gets h_i^\prime(w_{i}  x_{i-1}$)) }
    \EndFor
    \State {\color{ekinorange} output $x_{N}$}  \end{algorithmic}
\end{algorithm}
\end{minipage}
\end{center}

The simple case of 
scalar neural networks without bias shows the power of
the graph approach.  However, the full power is revealed
in the coming sections.  Here we remind the reader
of the algorithm, draw the graphs, and instantly
write down the linear algebra that provides the gradients
through backpropogation. (The graphs and matrices
are illustrated for $N=4$ for ease of presentation.)

$$\tilde{L}^T = \begin{pmatrix} 
. & \delta_2 w_1  & . & . \\
. & . & \delta_3 w_2   & . \\
. & . & .& \delta_4 w_3   \\
. & . & . & .\\
\end{pmatrix},
M = \begin{pmatrix}
\delta_1 x_0  & . & .& . \\
. & \delta_2 x_1  & . & . \\
. & .& \delta_3 x_2  & . \\
. & . & . & \delta_4  x_3
\end{pmatrix}
$$


\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{mlp2.png}
\caption{Top left: computation graph of a scalar MLP. This computation, which has nothing to do with derivatives, is often referred to as forward propagation because of its direction. Evaluation must generally necessarily go from left to right. Top right: derivative edge weights. Since derivatives are linear, multiple directions are possible to evaluate the products. Bottom: the superimposed graph showing both the forward computation and backward derivative propagation.} 
\label{dnn}
\end{center}
\end{figure}




It is an immediate consequence of our graph theory
methodology which concluded with Equations
(\ref{newloss3})
and
(\ref{likeadjoint})
that the backpropogated gradient is
computed by evaluating efficiently

$$\nabla_w {\cal L} =
\begin{pmatrix}
\delta_1 x_0  &  &&  \\
 & \delta_2 x_1  &  &  \\
& & \delta_3 x_2  &  \\
 &  &  & \delta_4  x_3
\end{pmatrix}
\begin{pmatrix} 
1 &   & & \\
 -\delta_2 w_1 & 1 &    & \\
 & -\delta_3 w_2  & 1&    \\
 &  & -\delta_4 w_3 & 1\\
\end{pmatrix}^{-T}
\begin{pmatrix} . \\ . \\ . \\ {\mathcal L}^\prime(x_4) \end{pmatrix} 
$$






% $\nabla_w x_4 = M(I-\tilde{L}^T)^{-1}
% \begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix}= $
% $$
% \begin{pmatrix}
% \delta_1 x_0  & . & .& . \\
% . & \delta_2 x_1  & . & . \\
% . & .& \delta_3 x_2  & . \\
% . & . & . & \delta_4  x_3
% \end{pmatrix}
% \begin{pmatrix} 
% 1 & -\delta_2 w_1  & . & . \\
% . & 1 & -\delta_3 w_2   & . \\
% . & . & 1& -\delta_4 w_3   \\
% . & . & . & 1\\
% \end{pmatrix}^{-1}
% \begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix} 
% $$
% If one has a loss function ${\mathcal L}(x)$ then using
% Equation (\ref{newloss3}) we have immediately

%%%   Make this minus transpose




\subsection{Matrices of vectors}

As a baby step towards the matrices of operators approach,
we show how one can (optionally) group weights and biases 
that appear in a neuron.  Algorithm \ref{nnalg} is modified so that
$w_i x_{i-1}$ is replaced with $w_i x_{i-1}+b_i$.
In the interest of space, we 
will simply write the answer and discuss its format.

$
\nabla_{[w,b]} {\cal L} =
$
$$ 
  \begin{pmatrix}
        [{\delta_1}  {x_0} \hspace*{.05in} \delta_1] \\
        & \hspace{-.2in}[{\delta_2}  {x_1}  \hspace*{.05in}  \delta_2] \\
        &&\hspace{-.2in}\hspace{.05in} \  [{\delta_{3}}  {x_{2}} \hspace*{.05in}  {\delta_{3}}] \\ 
        &&&\hspace{-.2in} \  [{\delta_{4}}  {x_{3}}  \hspace*{.05in}  {\! \delta_{4}}]
    \end{pmatrix}^T \! \! \!
    \begin{pmatrix} 
1 &   &  & \\
-\delta_2 w_1 & 1 &   &  \\
 &  -\delta_3 w_2 & 1&    \\
 &  & -\delta_4 w_3 & 1\\
\end{pmatrix}^{\! -T} \! \! \!
\begin{pmatrix} .\\ . \\ . \\ {\mathcal L}^\prime(x_4) \end{pmatrix}  \! \! .
    $$

We see we have an ordinary matrix backsubstitution followed by multiplication
by a diagonal matrix of  row vectors of length 2
 so that the result is
a vector of column vectors of length 2 which nicely packages the gradients
with respect to the weight and bias in each neuron.
We remark that the transpose applies recursively
in the diagonal matrix.  The transpose is overkill
in this case but is critical in the next section.

\subsection{Matrices of operators} \

Letting ${\cal I}$ denote the identity operator and
empty space the zero operator:

$
\nabla_{[W,B]} {\cal L} =
$

\resizebox{.9\linewidth}{!}{
 \begin{minipage}{\linewidth}
\begin{align*} 
\label{matrixform2}
\begin{pmatrix}
        [{\Delta_1}_H  \circ {X_0}_R \,\, {\Delta_1}_H] \\
        & [{\Delta_2}_H  \circ {X_1}_R \,\, {\Delta_2}_H] \\
        &&&\hspace{-.2in} \  [{{\Delta_{3}}_H}  \circ {{X_{2}}_R} \,\, {{\Delta_{3}}_H}] \\ 
        &&&&\hspace{-.2in} \  [{{\Delta_{4}}_H}  \circ {{X_{3}}_R} \,\, {{\Delta_{4}}_H}]
    \end{pmatrix}^T
     \begin{pmatrix}
        {\cal I} &   &   \\
        -{\Delta_2}_H \circ {W_2}_L &    {\cal I} &  \\
        & -{{\Delta_{3}}_H} \circ {{W_{3}}_L} &   {\cal I}& \\
        &&- {{\Delta_{4}}_H} \circ {{W_{4}}_L}&   {\cal I} \\
    \end{pmatrix}^{-T}
      \begin{pmatrix}
       . \\. \\ . \\ 
        \nabla_{X_4} {\cal L}
        \end{pmatrix}
\end{align*}
\end{minipage}
}\\


for the matrix neural network in Algorithm
\ref{matrixneuralnet}

\begin{center}
\begin{minipage}{.5\linewidth}
\begin{algorithm}[H]
\caption{Matrix Neural Network}\label{matrixneuralnet}
\begin{algorithmic}[1]
 \State {\color{ekinorange} input $X_0$ ($n_0 \times k$)} 
    \For{i := 1 to N}
      \State {\color{ekinorange} input $W_i (n_i \times n_{i-1}), B_i (n_i \times k)$} 
          \State $X_{i} \gets h_i(W_i*X_{i-1} + B_i)$.
          \State (${\color{ekinblue} \Delta_i \gets h_i^\prime(W_i*X_{i-1} + B_i)}$)
    \EndFor
    \State $J \gets \mathcal{L}(X_{N},y)$
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{center}



The entries of our matrix of operators may be read immediately from
the differential of line 4 of Algorithm \ref{matrixneuralnet}:
\begin{equation}
dX_{i} =   ({\Delta_i}_H  \circ {{}X_{i-1}}_R) dW_i \, +  {\Delta_i}_H dB_i
+ ({\Delta_i}_H \circ {W_i}_L )dX_{i-1}.
\end{equation}


% \section{
% The multi-layer perceptron Neural-Network: A Timely Example}
% \label{thedag}


% \label{cdag}


% \hfill
% \begin{minipage}{.45\linewidth}
% \begin{algorithm}[H] 
% \caption{Backpropagation of Algorithm \ref{nnalg} }\label{bp}
% \begin{algorithmic}[1]
%   \State  $\frac{\partial X_N}{\partial X_N} \gets 1$
%   \For{$i = N-1$ to 1 }
%      \State $\dfrac{\partial x_N}{\partial x_i} \gets w_{i+1}\delta_{i+1} \dfrac{\partial x_N}{\partial x_{i+1} }
%      % \   ( \frac{\partial x_N}{\partial \mbox{temp}_i}  \gets \delta_{i+1}\frac{\partial x_N}{\partial x_i})
%      $
%   \State $ \dfrac{\partial x_N}{\partial w_i} \gets x_{i-1} \delta_i \dfrac{\partial x_N}{\partial x_i}$ 
%      \EndFor
% \end{algorithmic}
% \end{algorithm}
% \end{minipage}
% \end{center}



%\input{bayesnets.tex}


% \begin{figure}[H]
% \begin{center}
% \includegraphics[width=\textwidth]{sup_mlp.pdf}
% \caption{TODO.} 
% \label{dnn}
% \end{center}
% \end{figure}



% Figure   \ref{dnn} labels  each edge with the immediate partial derivative:
% \begin{equation}
% \label{pd}
%   \frac{\partial  x_{i}}{\partial\mbox{temp}_i} = \delta_i  , \ \
%   \frac{\partial\mbox{temp}_i}{\partial  x_{i-1}} = w_i, 
% \ \
% \frac{\partial\mbox{temp}_i}{\partial  w_{i}} = x_{i-1}.
% \end{equation}

% From  Figure \ref{dnn} it is easy to read off from the graph path weights the long range partial derivative

% \begin{displaymath}
% \begin{split}
%     \frac{\partial x_N}{\partial w_i} &= \frac{\partial x_i}{\partial w_i}\cdot \prod_{j=i+1}^N \frac{\partial x_j}{\partial x_{j-1}}\\
%     &= x_{i-1}\delta_i\cdot \prod_{j=i+1}^N w_j\delta_j,
% \end{split}
% \end{displaymath}

% which may be computed through backpropagation by the recurrence in algorithm \ref{bp}.

% As most evident from Figure \ref{dnn}, line 4 in Algorithm \ref{bp} can be moved out of the inner loop into
% its own loop or in any order.  Line 3, by contrast should be computed in the  order exactly as specified in Algorithm \ref{bp}.


%\alan[inline]{Do we have a $w_0$? BW: the index in this section is wrong, and needs to be fixed.}

% The same algorithm expressed with linear algebra as in Equation \ref{likeadjoint2}  illustrated with $N=3$ is
% \begin{displaymath}
% \begin{split}
% \nabla_w x_3 &:= \left[\frac{\partial x_3}{\partial w_1}, \frac{\partial x_3}{\partial w_2}, \frac{\partial x_3}{\partial w_3}\right]^T\\
% &= 
% \left( \begin{array}{rrr}x_{1}&.&.\\.&.&.\\.&x_{2}&.\\.&.&.\\.&.&x_{3}\\.&.&.\end{array}\right)^T
% \left( \begin{array}{rrrrrr}1&.&.&.&.&.\\
% -\delta_{1}&1&.&.&.&.\\.&-w_{2}&1&.&.&.\\.&.&-\delta_{2}&1&.&.\\.&.&.&-w_{3}&1&.\\.&.&.&.&-\delta_{3}&1\end{array}\right)^{-T}
% \begin{pmatrix}
% . \\ .  \\ . \\ .  \\ . \\ 1
% \end{pmatrix}
% =
% \left( \begin{array}{r}w_{2} w_{3} x_{1} \delta_{1} \delta_{2} \delta_{3}\\w_{3} x_{2} \delta_{2} \delta_{3}\\x_{3} \delta_{3}\end{array} \right) .
% \end{split}
% \end{displaymath}

% It is worthwhile to do this computation once explicitly, but more conveniently
% one may group the nodes into ``neurons''  following the edge elimination procedure of Figure \ref{elim}.  Rather than six nodes for $N=3$, we have three neurons.
% The linear algebra becomes:

% \begin{equation}
% \label{threeneurons}
% \nabla_w x_3 = 
% \left( \begin{array}{rrr}\delta_1 x_{1}&.&.\\
% .&\delta_2 x_{2}&.\\.&.&\delta_3 x_{3}\end{array}\right)
% \left( \begin{array}{rrr}
% 1&-\delta_{1}w_2 &.\\
% .&1&-\delta_2 w_{3}\\
% .&.&1\end{array}\right)^{-1}
% \begin{pmatrix}
%  .  \\ . \\ 1
% \end{pmatrix}
% =
% \left( \begin{array}{r}w_{2} w_{3} x_{1} \delta_{1} \delta_{2} \delta_{3}\\w_{3} x_{2} \delta_{2} \delta_{3}\\x_{3} \delta_{3}\end{array} \right) .
% \end{equation}

% \subsubsection{Add in Loss functions}

% Algorithm \ref{dnn} did not include the customary ``loss'' function which represents the last step of a neural network computation.
% Figure \ref{loss} tacks on the loss function and its derivative. The derivative of the loss function certainly appears multiplicatively in
% an edge weight from any source to this last step. By Section ???, we immediately get the complete gradient is simply a re-scaling by $\partial{L}/\partial{x_N}.$

%\input{bayesnets2.tex}

\subsection{The Power of Notation}
\label{notation}

% We now consider  the analogs of equations \ref{eq1} and \ref{eq2} considering $W_i \in \mathbb{R}^{n_{i} \times n_{i-1}}$, $X_i \in \mathbb{R}^{n_{i} \times k}$ matrices and $B_i  \in \mathbb{R}^{n_{i} \times k}$ as the bias matrix. 
% We use broadcasting instead of summation in the Algorithm \ref{neuralnet}. Every column of the $x_i$ corresponds to a vector instance from the data and $b_i$ is summed with each column. 
%For broadcast operator we will use \mathrel{.+} symbol.








% The differential for line 4 is
% \begin{align}
% dX_{i} &= \Delta_i  \ .\! * (W_i dX_{i-1} + dW_iX_{i-1} \, \mathrel{+}  dB_i)  , \ \ i=1,\ldots,N \label{eq2m}.
% \end{align}
% where .* denotes elementwise multiplication.
% Here, we will treat right and left multiplication as mappings on vector spaces of matrices.
% Right multiplication $R_W$ and $L_W$ operates on an  $X$ matrix as in the equation   below:
% \begin{align}
%     L_W X = W * X \\
%     R_W X = X * W
% \end{align}
% \alan[inline]{Kronecker should be BXA' by some convention}
% \ekin[inline]{done}
% Secondly, we will define a conjugate mapping [see \ref{sec:appendix2}] to the Kronecker product:
% \begin{equation}
% \label{kroneckerconj}
%     (A \otimes_c B) * X = (A * X) * B^\prime
% \end{equation}
% Then, we use $H_W$ to represent the elementwise or hadamard product:
% \begin{equation}
%  H_W X=W .*  X
% \end{equation}
% And it  has following properties:
% \begin{equation}
% \Delta_i * X = \delta_i \odot X  = X * \Delta_i  
% \end{equation}
% With the new conventions, equation \ref{eq2m} may be rewritten as:

% \alan[inline]{in principle We don't need another Kronecker product 
%  we
% are good with $I \otimes w_i$ and $x_i^\prime \otimes I $
% but yes we need an operator that would be diagonal
% in the big space that does the hadamard (elemenwise) product
% }
% \ekin[inline]{How would you write first term in equation 2.18 with only using Kronecker by keepÄ±ng $dx_i$ outside of the paranthesis?}

% \begin{equation}
% dX_{i} = H_{\Delta_i} \circ L_{W_i} dX_{i-1} +  H_{\Delta_i}  \circ R_{X_i} dW_i \, +  H_{\Delta_i}dB_i .
% \end{equation}f
% Then we can immediately write the generalized version of equation \ref{matrixform1}:

% \resizebox{.95\linewidth}{!}{
%  \begin{minipage}{\linewidth}
% \begin{align*} 
% \label{matrixform2}
%     \begin{pmatrix}
%         dX_1 \\
%         dX_3 \\
%         \vdots \\
%         dX_{N-1} \\
%         dX_{N}
%     \end{pmatrix} =
%     \begin{pmatrix}
%         [H_{\Delta_1}  \circ R_{X_0} \,\, H_{\Delta_1}] \\
%         & [H_{\Delta_2}  \circ R_{X_1} \,\, H_{\Delta_2}] \\
%         && \hspace{-.2in} \ \ddots \\
%         &&&\hspace{-.2in} \  [H_{\Delta_{N-1}}  \circ R_{X_{N-2}} \,\, H_{\Delta_{N-1}}] \\ 
%         &&&&\hspace{-.2in} \  [H_{\Delta_{N}}  \circ R_{X_{N-1}} \,\, H_{\Delta_{N}}]
%     \end{pmatrix}
%     \begin{pmatrix}
%         [dW_1; dB_1]\\
%         [dW_2; dB_2]\\
%         \vdots \\
%         [dW_{N-1};dB_{N-1}] \\ 
%         [dW_N; dB_N]
%     \end{pmatrix} \\  +  \begin{pmatrix}
%         0 & \ldots & 0 & 0 & 0 \\
%         H_{\Delta_2} \circ L_{W_2} & \ldots & 0 & 0 & 0 \\
%         & \ddots \\
%         && H_{\Delta_{N-1}} \circ L_{W_{N-1}} & 0 & 0\\
%         &&& H_{\Delta_{N}} \circ L_{W_{N}}& 0 \\
%     \end{pmatrix}
%     \begin{pmatrix}
%         dX_1 \\
%         dX_2 \\
%         \vdots \\
%         dX_{N-1} \\
%         dX_{N}
%     \end{pmatrix}
% \end{align*}
%   \end{minipage}
% }

% Here, $g = (\bold{0},\bold{0},..., \mathcal{L}(X_{N},Y)^\prime)$ and $\nabla J$ can be calculated with equation \ref{gradient}.



% \begin{figure}[h]
% \begin{center}
% \includegraphics[scale=.5]{matrixdnn.png}
% \caption{
% \label{matrixdnn} 
% Matrix neural network with edge weight operators
% }
% \end{center}
% \end{figure}


% Figure \ref{matrixdnn} illustrates the operators on the computational graph.  It is easy to see that forward or backward passes with operators
% will always be evaluated in reverse mode.  One could imagine doing forward mode with huge matrices, but this is unlikely to be useful.

% We recall the scalar Jacobians Equations \ref{solg1} and \ref{solg2}:

% \begin{equation}
% \frac{\partial \mathcal{L}}{\partial w_i} = x_{i-1}\delta_i w_{i+1} \delta_{i+1} w_{i+2} \ldots w_N \delta_N  g, \\
% \frac{\partial \mathcal{L}}{\partial b_i} =  \ \delta_i w_{i+1} \delta_{i+1} w_{i+2} \ldots w_N \delta_N g\ .
% \end{equation}


% \alan[inline]{refer back to the notation that i'm introducing in this paper
% the ``transpose dot'' not mentioned in Section 1}


We read directly off the edge weight graph in Figure~\ref{mat_dnn} 

\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{op_mlp.png}
\caption{Computation, derivative graph of a matrix MLP with their superimposed version. Comparing to Figure~\ref{dnn} of the scalar MLP, all remains the same except that the players in computational graph become matrices while the edges in the derivative graph being replaced by operators.}
\label{mat_dnn}
\end{center}
\end{figure}

that for a matrix neural network we have

\begin{itemize}
    \item FORWARD Mode Operators (right to left)
    \begin{equation}
\frac{\partial \mathcal{L}}{\partial W_i} =    G^{T_\bullet}
(\Delta_N)_H(W_{N})_L\ldots(W_{i+2})_L(\Delta_{i+1})_H(W_{i+1})_L(\Delta_i)_H(X_{i-1})_R  \label{fmo1} \\
\frac{\partial \mathcal{L}}{\partial B_i} =  \ 
G^{T_\bullet}
(\Delta_N)_H(W_{N})_L\ldots(W_{i+2})_L(\Delta_{i+1})_H(W_{i+1})_L(\Delta_i)_H
\label{fmo2}
\end{equation}
or going the other way we have
\item REVERSE Mode Operators (right to left)
\begin{equation}
\left( \frac{\partial \mathcal{L}}{\partial W_i}\right)^{T_\bullet} \!=   
\left\{ (X_{i-1}^T)_R (\Delta_i)_H (W_{i+1}^T)_L(\Delta_{i+1})_H(W_{i+2}^T)_L \ldots (W_{N}^T)_L(\Delta_N)_H G \right \}^{T_\bullet} \\
\left( \frac{\partial \mathcal{L}}{\partial B_i}\right)^{T_\bullet} =  \ 
\left\{  (\Delta_i)_H (W_{i+1}^T)_L(\Delta_{i+1})_H(W_{i+2}^T)_L \ldots (W_{N}^T)_L(\Delta_N)_H G \right \}^{T_\bullet} 
\end{equation}
\end{itemize}








% \todo[inline]{[1.3.21] mention that in the scalar's case, the order doesn't matter since things commute. BW: shall we also add the standard vector case as an example? we don't need the derivation, but just put there for comparison. }



\textbf{Understanding these operators.} The forward operators in Equations (\ref{fmo1}) and (\ref{fmo2})
may be thought of as sensitivity operators or as a means of computing
the full gradient.  As a sensitivity operator, one can 
state that the directional derivative of $\mathcal{L}$
in the direction 
 $ \Delta W_i$
is $\frac{\partial \mathcal{L}}{\partial W_i}( \Delta W_i$).
Alternative each operator can be written out as a (large) matrix,
and ultimately a gradient can be computed.


The reverse operator is intended to be evaluated from right to left
inside the braces.  Doing so computes the gradient directly.

We hope the reader appreciates the power of the ``$T_\bullet$''
notation, whereby one feels we are taking transposes of matrices
and reversing order, but in fact we are transposing the operators.
Either way the operators can be read right off the graphs.




\subsection{Relationship to the Adjoint Method of scientific computing}

% Notes.
% \alan[inline]{
% What's important to me:

% 0. note there really is now a computational graph -- maybe
% no longer can be avoided.  This is now the first time i'm
% taking derivatives.

% 1.In  2.3 there is a dense matrix (rectangular) corresponding
% to a complete bipartite graph.  In 2.2 specializes to a square
% diagonal matrix.

% 2. In 2.3 there is a complete DAG like in Figure 2.4.
% though in 2.2 it is just a line corresponding to the spine.

% 3. However, the linear algebra that we did in 2.3 represents
% the general case, i.e. the M and the L correspond to 
% parameters $-> x$  and $x-->x$ respectively.

% 4. There is a cute trick in the adjoint method that i didn't
% see in the literature (maybe it's there) that defines f
% to correspond to an explicit program.

% 5. We can derive what we derived in 2.4 by simply taking
% jacobians and getting the ansewr.

% 6. alternatively, we can pull the ''adjoint equation'' out of a hat,
% and show that it gives the same answer.
% }


We will show how to derive  equations (\ref{likeadjoint2}) and (\ref{likeadjoint}) using the adjoint method
so-named because of its focus on the transpose (the adjoint) of the Jacobian. We encourage interested readers to  see
\cite{johnson2007notes} and  \cite{bradley2010pde}.

We find it satisfying that the graph theoretic interpretation of reverse mode autodifferentiation and the adjoint method of scientific computing can be seen to give the same answer from two very different viewpoints.

%\verb+https://cs.stanford.edu/~ambrad/adjoint_tutorial.pdf+

%\verb+https://math.mit.edu/~stevenj/18.336/adjoint.pdf +



Consider a  general computation with  known constant  input $x_0 \in \mathbb{R}$ and parameters $p = [p_1,\ldots,p_k]$:

\begin{center}
\begin{algorithm}[H] 
\caption{General Computation}\label{gen}
\begin{algorithmic}[1]
  \State { input constant  \color{ekinorange} $x_0$} 
  \State { input parameters \color{ekinorange} $p_1,\ldots,p_k$} 
   \State $ x_{1}  \gets  \Phi_1(; p_1,\ldots,p_k; x_0)$
   \State $ x_{2}  \gets  \Phi_2(x_1; p_1,\ldots,p_k; x_0)$
   \State \hspace{.3in} $\vdots$\hspace{.3in} $\vdots$
    \State $ x_{N}  \gets  \Phi_N(x_1,\ldots,x_{N-1}; p_1,\ldots,p_k; x_0)$
    
    \State {\color{ekinorange} output $x_{N}$}  \end{algorithmic}
\end{algorithm}
\end{center}

Algorithm \ref{gen} is an explicit computation.
The {\it function} $\phi_i$ computes the
value of the {\it variable} $x_i$.
The notation
$\frac{d\Phi_i}{dx_j}$ or
$\frac{d\Phi_i}{dp_j}$
gives the partial derivatives of one step
of the algorithm.  By contrast, the
notation $\frac{dx_i}{dp_j}$
gives the partial derivatives across multiple
steps of the algorithm.
Algorithm \ref{gen}  is the general case of Algorithm
2.1, the $\frac{d\Phi_i}{dx_j}$ and
$\frac{d\Phi_i}{dp_j}$ are general cases of what is seen
in Algorithm 2.2, and the $\frac{dx_i}{dp_j}$ generalize
what is seen in Algorithm 2.3.

We note that the adjoint method literature tends to consider a yet more general implicit approach. Placing Section 3 of  \cite{johnson2007notes} in an explicit setting, we define a function $f$ such that $f(x, p)=0$.
To this end let
\begin{equation}
\label{explicit}
f(x,p)= x - \Phi(x, p) := \begin{pmatrix} x_1 \\  x_2 \\ \vdots \\ x_N  \end{pmatrix}-  
\begin{pmatrix}
\Phi_1(; p; x_0) \\
\Phi_2(x_1; p; x_0) \\
\vdots \\
\Phi_N(x_1,\ldots,x_{N-1}; p; x_0)
\end{pmatrix}
.
\end{equation}

Clearly given $p$ the  computed  $x=(x_1,\ldots,x_N)$ from Algorithm \ref{gen} is a solution  to $f(x,p)=0$. 
Our goal is to reproduce Equation (\ref{likeadjoint2}), which is the derivative of $x_N$ wrt to the parameter $p$. % and (\ref{likeadjoint}).


Let us first consider the derivation for $x_p$, which is the derivative of $x$, implicitly defined by $f(x, p) =  0$, wrt to $p$. To connect the viewpoints a table of notation for various Jacobians is helpful:

\vspace{.2in}

\begin{center}
\begin{tabular}{llll} 
\toprule
Adjoint Method & Nabla Notation & Matrix & Size  \\ 
\midrule
$f_x$  &  $\nabla_x f $   &  $  I- \tilde{L} $  & $N\times N$  \\[.1in]
$f_p$ & $\nabla_p f$ &  $-M^T $ & $N \times k$ \\[.1in]
$x_p$ & $\nabla_p x$ & $(I-\tilde{L})^{-1}M^T$ & $N \times k$\\
\bottomrule
\end{tabular} 
\end{center}

\vspace{.1in}
The  matrices  themselves are explicitly:

\begin{displaymath}
	\tilde{L} = \left[\dfrac{\partial \Phi_i}{\partial x_j}\right]_{i,j}, \quad i > j,\ j = 1, \ldots, N-1,
\end{displaymath}
and 

$$
M^T = \left[\dfrac{\partial \Phi_i}{\partial p_j}\right]_{i,j}, \quad \nabla_p x =  \left[\dfrac{\partial x_i}{\partial p_j}\right]_{i,j}, \quad i \in 1,\ldots,N, j \in 1,\ldots,k.
$$


% $$\tilde{L}=\begin{pmatrix}
% 0 & 0 &  \ldots & 0 \\
% \dfrac{\partial \Phi_2}{\partial x_1} & 0 & \ldots  & 0 \\
% \vdots & \ddots & \vdots  & \vdots \\
% \dfrac{\partial \Phi_N}{\partial x_1} & \ldots & \dfrac{\partial \Phi_N}{\partial x_{N-1 }} & 0 
% \end{pmatrix},
% \
% M^T=
% \begin{pmatrix}
% \frac{\partial \Phi_1}{\partial dp_1} & \ldots & \frac{\partial \Phi_1}{\partial dp_k } \\
% \vdots & \vdots & \vdots \\
% \frac{\partial \Phi_N}{\partial dp_1} & \ldots & \frac{\partial \Phi_N}{\partial dp_k } 
% \end{pmatrix},
% \
% \nabla_p x =
% \begin{pmatrix}
% \frac{\partial x_1}{\partial p_1} & \ldots & \frac{\partial x_1}{\partial p_k} \\
% \ldots & \ldots & \dots \\
% \frac{\partial x_N}{\partial p_1} & \ldots & \frac{\partial x_N}{\partial p_k} \\
% \end{pmatrix} ,
% $$
% where 

The matrix $\tilde{L}$ that contains the partials $\partial \Phi_j / \partial x_j$ is strictly lower triangular exactly because 
Algorithm \ref{gen} is an explicit computation, whereas an implicit function would generally have a dense jacobian.
Since $f(x,p) = x - \Phi(x,p)$, the Jacobian $\nabla_x f= I - \tilde{L}$.
Differentiating $0=f(x,p)$ with respect to $p$ we get $0 = f_x x_p + f_p$ or 
$x_p = - f_x^{-1}f_p$ which is $(I-\tilde{L})^{-1}M^T$ in matrix notation explaining the bottom row of the above table.


If $g(x)$ is any scalar function of $x$, then the key adjoint equation is
\[
\nabla_p g = g_xx_p = -g_x f_x^{-1} f_p := -\lambda^T f_p,
\]
where $\lambda$ satisfies the so-called adjoint equation $f_x^T \lambda =g_x^T$. Since $g_x$ is an $1$ by $k$ vector, by computing the adjoint $\lambda$ first, we reduce the computation of a matrix-matrix multiplication and a matrix-vector multiplication to two matrix-vector multiplications. 

If we take $g(x)=x_N$ then $g_x = [0,\ldots,0,1]$.
The gradient is then
$$ \nabla_p g(x) =  [0,\ldots,0,1]  (I-\tilde{L})^{-1} M^T,$$
achieving our goal of reproducing Equation (\ref{likeadjoint}). % and \ref{likeadjoint2}.

So much is happening here that it is worth repeating with other notation.
We can use the Jacobian of $f$ with respect to $x$ and $p$ to  differentiate Equation \ref{explicit}:
\[
0=
\begin{pmatrix}
 dx_1   \\ dx_2 \\  \vdots  \\ dx_N 
\end{pmatrix}
 -
\begin{pmatrix}
0 & 0 &  \ldots & 0 \\
\dfrac{\partial \Phi_2}{\partial x_1} & 0 & \ldots  & 0 \\
\vdots & \ddots & \vdots  & \vdots \\
\dfrac{\partial \Phi_N}{\partial x_1} & \ldots & \dfrac{\partial \Phi_N}{\partial x_{N-1 }} & 0 
\end{pmatrix}
\begin{pmatrix}
dx_1   \\ dx_2 \\  \vdots \\ dx_N 
\end{pmatrix}
-
\begin{pmatrix}
\dfrac{\partial \Phi_1}{\partial p_1} & \ldots & \dfrac{\partial \Phi_1}{\partial p_k } \\
\vdots & \vdots & \vdots \\
\dfrac{\partial \Phi_N}{\partial p_1} & \ldots & \dfrac{\partial \Phi_N}{\partial p_k } 
\end{pmatrix}
\begin{pmatrix}
dp_1   \\ dp_2 \\  \vdots \\ dp_k 
\end{pmatrix},
\]
which can be solved to obtain
\[
\begin{pmatrix}
dx_1   \\ dx_2 \\  \vdots \\ dx_N 
\end{pmatrix}
=
\left(
I -
\begin{pmatrix}
0 & 0 &  \ldots & 0 \\
\dfrac{\partial \Phi_2}{\partial x_1} & 0 & \ldots  & 0 \\
\vdots & \ddots & \vdots  & \vdots \\
\dfrac{\partial \Phi_N}{\partial x_1} & \ldots & \dfrac{\partial \Phi_N}{\partial x_{N-1 }} & 0 
\end{pmatrix}
\right)^{-1}
\begin{pmatrix}
\dfrac{\partial \Phi_1}{\partial p_1} & \ldots & \dfrac{\partial \Phi_1}{\partial p_k } \\
\vdots & \vdots & \vdots \\
\dfrac{\partial \Phi_N}{\partial p_1} & \ldots & \dfrac{\partial \Phi_N}{\partial p_k } 
\end{pmatrix} 
\begin{pmatrix}
dp_1   \\ dp_2 \\  \vdots \\ dp_k 
\end{pmatrix} .
\]



Some readers unfamiliar with the notation of differentials might prefer what amounts to a notational change, but avoids
the notation of differentials:
\begin{displaymath}
\begin{pmatrix}
\dfrac{\partial x_1}{\partial p_1} & \ldots & \dfrac{\partial x_1}{\partial p_k} \\
\vdots & \vdots & \vdots \\
\dfrac{\partial x_N}{\partial p_1} & \ldots & \dfrac{\partial x_N}{\partial p_k} \\
\end{pmatrix}
=
\left(
I -
\begin{pmatrix}
0 & 0 &  \ldots & 0 \\
\dfrac{\partial \Phi_2}{\partial dx_1} & 0 & \ldots  & 0 \\
\vdots & \ddots & \vdots  & \vdots \\
\dfrac{\partial \Phi_N}{\partial dx_1} & \ldots & \dfrac{\partial \Phi_N}{\partial x_{N-1}} & 0 
\end{pmatrix}
\right)^{-1}
\begin{pmatrix}
\dfrac{\partial \Phi_1}{\partial dp_1} & \ldots & \dfrac{\partial \Phi_1}{\partial p_k } \\
\vdots & \vdots & \vdots \\
\dfrac{\partial \Phi_N}{\partial dp_1} & \ldots & \dfrac{\partial \Phi_N}{\partial p_k } 
\end{pmatrix} .
\end{displaymath}

%\newpage

\section{Julia, the power of language}

\subsection{The challenge}
\label{challenge}

Here is a complete realization of the challenge of the preface
(Section \ref{preface}).
The question we asked is whether we
could  vivify the linear algebra math
expressed in
 $$\nabla J = M^T*((I-L)^T \backslash g)$$
 by typing
 the command
 \begin{center}
    \includegraphics[width=2in]{goal.png} %\raisebox{.1in}{\Large ?}
\end{center}
and  compute the 
backpropogated gradient of a matrix neural network almost by magic?




We remark that it is common to see code or pointers to code in papers.
Code can serve the purpose of specifying details, facilitating
reproducibility, and verifiability.  Code can also allow users
to adapt methods to their own situations.
In addition to all of the above, we have a further purpose.
We believe the code example we provide shows 
the power, elegance, and utility of the Julia programming language
in ways that may be difficult or impossible to imagine in 
other languages.

At the risk of showing the end of the code before the start,
63  lines of setup culminate in exactly what we wanted:
code which looks just like the math of matrices with operators
that correctly calculates the gradient fulfilling our 
title goal of backpropogating through back substitution with a backslash:


% \begin{center}
% \begin{mdframed}[backgroundcolor=backgroundcolor, rightline=false]
%     \lstinputlisting[language=Julia, style=julia, frame={}]{julia1.jl}
% \end{mdframed}
% \end{center}

\begin{center}
  \includegraphics[width=\textwidth]{code_snippet/code1.png}
\end{center}


The first 28 lines elegantly set up
the mathematics very much like a mathematician defining
operators and algebraic axioms:



\begin{center}
  \includegraphics[width=\textwidth]{code_snippet/code2.png}
\end{center}

% \caption{Lines 10-14 define matrix operators and their adjoints.
% Lines 16-28 define various math operations, su
% as the negative operator on line 21, or the composition of operators
% on line 25.
% }
% \end{figure}

Lines 10-14  above define matrix operators and their adjoints.
Lines 16-28 define various math operations, such
 as the negative operator on line 21, or the composition of operators
 on line 25.



For completeness  we list   lines 29 through 63 which
constitute the setup of
 a basic forward pass through a matrix neural  net.  We remark th    at lines 30 and  38 allow
an index origin of 0.


For reference, we present the matrices with operators below:



$$
M = \begin{pmatrix}
        [{\Delta_1}_H  \circ {X_0}_R \,\, {\Delta_1}_H] \\
        & [{\Delta_2}_H  \circ {X_1}_R \,\, {\Delta_2}_H] \\
        &&&\hspace{-.2in} \  [{{\Delta_{3}}_H}  \circ {{X_{2}}_R} \,\, {{\Delta_{3}}_H}] \\ 
        &&&&\hspace{-.2in} \  [{{\Delta_{4}}_H}  \circ {{X_{3}}_R} \,\, {{\Delta_{4}}_H}]
    \end{pmatrix}
   $$
   
   
   $$ 
    (I-L)=
     \begin{pmatrix}
        {\cal I} &   &   \\
        -{\Delta_2}_H \circ {W_2}_L &    {\cal I} &  \\
        & -{{\Delta_{3}}_H} \circ {{W_{3}}_L} &   {\cal I}& \\
        &&- {{\Delta_{4}}_H} \circ {{W_{4}}_L}&   {\cal I} \\
    \end{pmatrix} $$
    $$
    g=
      \begin{pmatrix}
       . \\. \\ . \\ 
        \nabla_{X_4} {\cal L}
        \end{pmatrix} .
$$
 
\begin{center}
  \includegraphics[width=\textwidth]{code_snippet/code3.png}
\end{center}



\subsection{Modern Computer Science meets Linear Algebra}

The venerable position of numerical linear algebra libraries
can not be undersold.  Years of rigorous mathematical and
algorithmic research culminating in the modern LAPACK library 
\cite{anderson1999lapack}
have lead to a crowning achievement of value to a huge number
of users calling LAPACK perhaps from, for example, Julia, NumPy, or MATLAB
in most cases unaware of the scientific bedrock of which they
are beneficiaries.

Continuing this grand tradition, we wish to highlight some
of the computer science innovations that allow for the code
in Section \ref{challenge} to look so deceptively simple.  

 

 {\bf Generic Programming or how can the backslash just work? }
We invite the reader to consider how the innocent backslash on line 75
of the code in Section \ref{challenge} could possibly perform a backpropogation of derivative.
We believe this would be impossible in, say, NumPy or MATLAB
as these packages currently exist.  
  From a computer science point of view, Julia's multiple dispatch
mechanism and generic programming features allow the 
generic backslash to work with matrices and vectors whose elements
are operators and compositions of operators.  We remind the reader
that the operators are not language constructs, but are created
in the software on the first 28 lines of code.  The backslash,
however, is not LAPACK's backlash, as the LAPACK library
is constrained to floating point real and complex numbers.  
Julia's backslash currently runs LAPACK when dispatched by
matrices of floats, but, as is the case here, the generic algorithm
is called.  
   We are fascinated by the fact that the author of the generic algorithm
 would not have imagined how it might be used.  We are aware of 
 backslash being run on quaternion matrices, block matrices, matrices over finite fields,  and now matrices with operators.  Such is the mathematical power
 of abstraction and what becomes possible if software is allowed to be generic.
    In the context of back propagation, replacing the ``for loops'' with
the backlash helps us see backpropogation from a higher viewpoint.


  {\bf The significance of transpose all the way down: }
Not without controversy, Julia implements transpose recursively.  We believe this
is the preferred behavior.  This means a block matrix of block matrices of matrices (etc.) will transpose in the expected manner.  Similarly matrices
of complex number or quaternions will perform conjugate transposes as expected.
In this work the $M$ as seen in Line 66 of the code in Section
\ref{challenge} is diagonal, but is not symmetric.
In line 75 we are transposing a diagonal matrix of 1x2 matrices 
of composed operators \verb+M'+  while
in that same line we are also transposing a bidiagonal matrix  of  operators.
Because the operator adjoint is defined on lines 10-14 of the code
and the adjoint for a composed operator is defined on line 25,
Julia's generic implementation, again, just works.  We are not
aware of any other linear algebra system whereby the transpose
would just work this readily.


{\bf A quick  word about performance:}
There is nothing in the backslash formulation that would impede performance.

{\bf A quick word about the example code  in Section \ref{challenge}:}
We deliberately only used as an example the matrix neural network.
We also have implemented a fully connected neural network where the
matrix $I-L$ is  a Julia triangular type, whereas the reference example
was bidiagonal.   We also implemented a square case where the $W$ parameter
was constant from one iteration to the next.   
We also conceived of the case of being restricted to a manifold.
We thus stress that
we did not build a fully functional package at this time, and
thus emphasize that this could be future research, but we have not 
yet seen any roadblock to this methodology.

\small{\noindent
\verb+https://discourse.julialang.org/t/why-is-transpose-recursive/2550+} documents some of the controversy. We are extremely grateful that the recursive
definition won the day.

{\bf Summary:}
  For software to efficiently achieve the goals set out of 
  this paper, modern elements not found in older languages that are important
  include:

\begin{itemize}
\item Generic Programming (generic operators)
\item Abstract Representations
\item Fast Performance without waste
\item Multiple dispatch
\item Aggressive type system
\end{itemize}
 


\section{Acknowledgments} \small{We wish to thank David Sanders and Jeremy Kepner for  helpful conversations. This material is based upon work supported by the National Science Foundation under grant no. OAC-1835443, grant no. SII-2029670, grant no. ECCS-2029670, grant no. OAC-2103804, and grant no. PHY-2021825. We also gratefully acknowledge the U.S. Agency for International Development through Penn State for grant no. S002283-USAID. The information, data, or work presented herein was funded in part by the Advanced Research Projects Agency-Energy (ARPA-E), U.S. Department of Energy, under Award Number DE-AR0001211 and DE-AR0001222. This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Agreement No HR00112290091. We also gratefully acknowledge the U.S. Agency for International Development through Penn State for grant no. S002283-USAID. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof. This material was supported by The Research Council of Norway and Equinor ASA through Research Council project "308817 - Digital wells for optimal production and drainage". Research was sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.}

\bibliographystyle{siamplain}
\bibliography{references}
\end{document}
