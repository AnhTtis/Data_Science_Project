
@misc{Authors14,
 author = {Full Author Name},
 title = {The Frobnicatable Foo Filter},
 note = {Face and Gesture  submission ID 324. Supplied as additional material {\tt fg324.pdf}},
 year = 2014
}
@inproceedings{davies2012population,
  title={Population-based routing in the SpiNNaker neuromorphic architecture},
  author={Davies, Sergio and Navaridas, Javier and Galluppi, Francesco and Furber, Steve},
  booktitle={The 2012 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2012},
  organization={IEEE}
}

@article{hao2023bridging,
  title={Bridging the Gap between ANNs and SNNs by Calibrating Offset Spikes},
  author={Hao, Zecheng and Ding, Jianhao and Bu, Tong and Huang, Tiejun and Yu, Zhaofei},
  journal={arXiv preprint arXiv:2302.10685},
  year={2023}
}

@article{fawcett2006introduction,
  title={An introduction to ROC analysis},
  author={Fawcett, Tom},
  journal={Pattern recognition letters},
  volume={27},
  number={8},
  pages={861--874},
  year={2006},
  publisher={Elsevier}
}

@article{neftci2017event,
  title={Event-driven random back-propagation: Enabling neuromorphic deep learning machines},
  author={Neftci, Emre O and Augustine, Charles and Paul, Somnath and Detorakis, Georgios},
  journal={Frontiers in neuroscience},
  volume={11},
  pages={324},
  year={2017},
  publisher={Frontiers Media SA}
}

@inproceedings{fang2021incorporating,
  title={Incorporating learnable membrane time constant to enhance learning of spiking neural networks},
  author={Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Masquelier, Timoth{\'e}e and Huang, Tiejun and Tian, Yonghong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2661--2671},
  year={2021}
}

@article{o2013real,
  title={Real-time classification and sensor fusion with a spiking deep belief network},
  author={O'Connor, Peter and Neil, Daniel and Liu, Shih-Chii and Delbruck, Tobi and Pfeiffer, Michael},
  journal={Frontiers in neuroscience},
  volume={7},
  pages={178},
  year={2013},
  publisher={Frontiers Media SA}
}

@article{cornford2021learning,
  title={Learning to live with Dale’s principle: ANNs with separate excitatory and inhibitory units},
  author={Cornford, Jonathan and Kalajdzievski, Damjan and Leite, Marco and Lamarquette, Am{\'e}lie and Kullmann, Dimitri M and Richards, Blake},
  journal={bioRxiv},
  pages={2020--11},
  year={2021},
  publisher={Cold Spring Harbor Laboratory}
}

@article{tripp2016function,
  title={Function approximation in inhibitory networks},
  author={Tripp, Bryan and Eliasmith, Chris},
  journal={Neural Networks},
  volume={77},
  pages={95--106},
  year={2016},
  publisher={Elsevier}
}

@article{parisien2008solving,
  title={Solving the problem of negative synaptic weights in cortical models},
  author={Parisien, Christopher and Anderson, Charles H and Eliasmith, Chris},
  journal={Neural computation},
  volume={20},
  number={6},
  pages={1473--1494},
  year={2008},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{liu2016noisy,
  title={Noisy softplus: A biology inspired activation function},
  author={Liu, Qian and Furber, Steve},
  booktitle={International Conference on Neural Information Processing},
  pages={405--412},
  year={2016},
  organization={Springer}
}

@article{ding2021optimal,
  title={Optimal ann-snn conversion for fast and accurate inference in deep spiking neural networks},
  author={Ding, Jianhao and Yu, Zhaofei and Tian, Yonghong and Huang, Tiejun},
  journal={arXiv preprint arXiv:2105.11654},
  year={2021}
}

@inproceedings{mueller2021minimizing,
  title={Minimizing Inference Time: Optimization Methods for Converted Deep Spiking Neural Networks},
  author={Mueller, Etienne and Hansjakob, Julius and Auge, Daniel and Knoll, Alois},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}

@article{wu2021progressive,
  title={Progressive tandem learning for pattern recognition with deep spiking neural networks},
  author={Wu, Jibin and Xu, Chenglin and Han, Xiao and Zhou, Daquan and Zhang, Malu and Li, Haizhou and Tan, Kay Chen},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  publisher={IEEE}
}


@article{maass1997networks,
  title={Networks of spiking neurons: the third generation of neural network models},
  author={Maass, Wolfgang},
  journal={Neural networks},
  volume={10},
  number={9},
  pages={1659--1671},
  year={1997},
  publisher={Elsevier}
}


@article{eliasmith2012large,
  title={A large-scale model of the functioning brain},
  author={Eliasmith, Chris and Stewart, Terrence C and Choo, Xuan and Bekolay, Trevor and DeWolf, Travis and Tang, Yichuan and Rasmussen, Daniel},
  journal={science},
  volume={338},
  number={6111},
  pages={1202--1205},
  year={2012},
  publisher={American Association for the Advancement of Science}
}


@article{maass2007computational,
  title={Computational aspects of feedback in neural circuits},
  author={Maass, Wolfgang and Joshi, Prashant and Sontag, Eduardo D},
  journal={PLoS Comput Biol},
  volume={3},
  number={1},
  pages={e165},
  year={2007},
  publisher={Public Library of Science}
}


@article{cao2015spiking,
  title={Spiking deep convolutional neural networks for energy-efficient object recognition},
  author={Cao, Yongqiang and Chen, Yang and Khosla, Deepak},
  journal={International Journal of Computer Vision},
  volume={113},
  number={1},
  pages={54--66},
  year={2015},
  publisher={Springer}
}

@article{ho2020tcl,
  title={TCL: an ANN-to-SNN Conversion with Trainable Clipping Layers},
  author={Ho, Nguyen-Dong and Chang, Ik-Joon},
  journal={arXiv preprint arXiv:2008.04509},
  year={2020}
}


@article{hwang2021low,
  title={Low-Latency Spiking Neural Networks Using Pre-Charged Membrane Potential and Delayed Evaluation},
  author={Hwang, Sungmin and Chang, Jeesoo and Oh, Min-Hye and Min, Kyung Kyu and Jang, Taejin and Park, Kyungchul and Yu, Junsu and Lee, Jong-Ho and Park, Byung-Gook},
  journal={Frontiers in Neuroscience},
  volume={15},
  pages={135},
  year={2021},
  publisher={Frontiers}
}

@article{qin2020binary,
  title={Binary neural networks: A survey},
  author={Qin, Haotong and Gong, Ruihao and Liu, Xianglong and Bai, Xiao and Song, Jingkuan and Sebe, Nicu},
  journal={Pattern Recognition},
  volume={105},
  pages={107281},
  year={2020},
  publisher={Elsevier}
}

@article{elsken2019neural,
  title={Neural architecture search: A survey},
  author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1997--2017},
  year={2019},
  publisher={JMLR. org}
}

@article{esser2019learned,
  title={Learned step size quantization},
  author={Esser, Steven K and McKinstry, Jeffrey L and Bablani, Deepika and Appuswamy, Rathinakumar and Modha, Dharmendra S},
  journal={arXiv preprint arXiv:1902.08153},
  year={2019}
}

@article{chen2020nanoscale,
  title={Nanoscale room-temperature multilayer skyrmionic synapse for deep spiking neural Networks},
  author={Chen, Runze and Li, Chen and Li, Yu and Miles, James J and Indiveri, Giacomo and Furber, Steve and Pavlidis, Vasilis F and Moutafis, Christoforos},
  journal={Physical Review Applied},
  volume={14},
  number={1},
  pages={014096},
  year={2020},
  publisher={APS}
}

@book{warden2019tinyml,
  title={Tinyml: Machine learning with tensorflow lite on arduino and ultra-low-power microcontrollers},
  author={Warden, Pete and Situnayake, Daniel},
  year={2019},
  publisher={O'Reilly Media}
}

@article{krishnamoorthi2018quantizing,
  title={Quantizing deep convolutional networks for efficient inference: A whitepaper},
  author={Krishnamoorthi, Raghuraman},
  journal={arXiv preprint arXiv:1806.08342},
  year={2018}
}


@article{lu2020exploring,
  title={Exploring the connection between binary and spiking neural networks},
  author={Lu, Sen and Sengupta, Abhronil},
  journal={Frontiers in Neuroscience},
  volume={14},
  pages={535},
  year={2020},
  publisher={Frontiers}
}

@inproceedings{beyeler2015carlsim,
  title={CARLsim 3: A user-friendly and highly optimized library for the creation of neurobiologically detailed spiking neural networks},
  author={Beyeler, Michael and Carlson, Kristofor D and Chou, Ting-Shuo and Dutt, Nikil and Krichmar, Jeffrey L},
  booktitle={2015 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2015},
  organization={IEEE}
}

@article{huang2017multi,
  title={Multi-scale dense networks for resource efficient image classification},
  author={Huang, Gao and Chen, Danlu and Li, Tianhong and Wu, Felix and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  journal={arXiv preprint arXiv:1703.09844},
  year={2017}
}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}

@article{ovadia2019can,
  title={Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift},
  author={Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, David and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{kim2022neural,
  title={Neural architecture search for spiking neural networks},
  author={Kim, Youngeun and Li, Yuhang and Park, Hyoungseob and Venkatesha, Yeshwanth and Panda, Priyadarshini},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXIV},
  pages={36--56},
  year={2022},
  organization={Springer}
}

@article{li2022converting,
  title={Converting artificial neural networks to spiking neural networks via parameter calibration},
  author={Li, Yuhang and Deng, Shikuang and Dong, Xin and Gu, Shi},
  journal={arXiv preprint arXiv:2205.10121},
  year={2022}
}

@inproceedings{wu2019direct,
  title={Direct training for spiking neural networks: Faster, larger, better},
  author={Wu, Yujie and Deng, Lei and Li, Guoqi and Zhu, Jun and Xie, Yuan and Shi, Luping},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={1311--1318},
  year={2019}
}

@article{li2022quantization,
  title={Quantization Framework for Fast Spiking Neural Networks},
  author={Li, Chen and Ma, Lei and Furber, Steve B},
  journal={Frontiers in Neuroscience},
  pages={1055},
  year={2022},
  publisher={Frontiers}
}

@article{mozafari2019spyketorch,
  title={Spyketorch: Efficient simulation of convolutional spiking neural networks with at most one spike per neuron},
  author={Mozafari, Milad and Ganjtabesh, Mohammad and Nowzari-Dalini, Abbas and Masquelier, Timoth{\'e}e},
  journal={Frontiers in neuroscience},
  pages={625},
  year={2019},
  publisher={Frontiers}
}

@inproceedings{bu2021optimal,
  title={Optimal ANN-SNN Conversion for High-accuracy and Ultra-low-latency Spiking Neural Networks},
  author={Bu, Tong and Fang, Wei and Ding, Jianhao and Dai, PengLin and Yu, Zhaofei and Huang, Tiejun},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{adrian1926impulses,
  title={The impulses produced by sensory nerve-endings: Part II. The response of a Single End-Organ},
  author={Adrian, Edgar D and Zotterman, Yngve},
  journal={The Journal of physiology},
  volume={61},
  number={2},
  pages={151},
  year={1926},
  publisher={Wiley-Blackwell}
}

@article{bekolay2014nengo,
  title={Nengo: a Python tool for building large-scale functional brain models},
  author={Bekolay, Trevor and Bergstra, James and Hunsberger, Eric and DeWolf, Travis and Stewart, Terrence C and Rasmussen, Daniel and Choo, Xuan and Voelker, Aaron Russell and Eliasmith, Chris},
  journal={Frontiers in neuroinformatics},
  volume={7},
  pages={48},
  year={2014},
  publisher={Frontiers Media SA}
}

@article{paredes2020unsupervised,
  title={Unsupervised Learning of a Hierarchical Spiking Neural Network for Optical Flow Estimation: From Events to Global Motion Perception},
  author={Paredes-Valles, Federico and Scheper, Kirk Yannick Willehm and De Croon, Guido Cornelis Henricus Eugene},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  volume={42},
  number={8},
  pages={2051-2064},
  publisher={IEEE}
}


@article{hazan2018bindsnet,
  title={Bindsnet: A machine learning-oriented spiking neural networks library in python},
  author={Hazan, Hananel and Saunders, Daniel J and Khan, Hassaan and Patel, Devdhar and Sanghavi, Darpan T and Siegelmann, Hava T and Kozma, Robert},
  journal={Frontiers in neuroinformatics},
  volume={12},
  pages={89},
  year={2018},
  publisher={Frontiers Media SA}
}

@software{norse2021,
  author       = {Pehle, Christian and
                  Pedersen, Jens Egholm},
  title        = {{Norse -  A deep learning library for spiking 
                   neural networks}},
  month        = jan,
  year         = 2021,
  note         = {Documentation: https://norse.ai/docs/},
  publisher    = {Zenodo},
  version      = {0.0.7},
  doi          = {10.5281/zenodo.4422025},
  url          = {https://doi.org/10.5281/zenodo.4422025}
}

@article{kaiser2020synaptic,
  title={Synaptic plasticity dynamics for deep continuous local learning (DECOLLE)},
  author={Kaiser, Jacques and Mostafa, Hesham and Neftci, Emre},
  journal={Frontiers in Neuroscience},
  volume={14},
  pages={424},
  year={2020},
  publisher={Frontiers}
}

@article{eshraghian2021training,
  title={Training spiking neural networks using lessons from deep learning},
  author={Eshraghian, Jason K and Ward, Max and Neftci, Emre and Wang, Xinxin and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and Jeong, Doo Seok and Lu, Wei D},
  journal={arXiv preprint arXiv:2109.12894},
  year={2021}
}

@article{davison2009pynn,
  title={PyNN: a common interface for neuronal network simulators},
  author={Davison, Andrew P and Br{\"u}derle, Daniel and Eppler, Jochen M and Kremkow, Jens and Muller, Eilif and Pecevski, Dejan and Perrinet, Laurent and Yger, Pierre},
  journal={Frontiers in neuroinformatics},
  volume={2},
  pages={11},
  year={2009},
  publisher={Frontiers}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@misc{SpikingJelly,
	title = {SpikingJelly},
	author = {Fang, Wei and Chen, Yanqi and Ding, Jianhao and Chen, Ding and Yu, Zhaofei and Zhou, Huihui and Tian, Yonghong and other contributors},
	year = {2020},
	howpublished = {\url{https://github.com/fangwei123456/spikingjelly}},
	note = {Accessed: YYYY-MM-DD},
}

@article{neftci2019surrogate,
  title={Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks},
  author={Neftci, Emre O and Mostafa, Hesham and Zenke, Friedemann},
  journal={IEEE Signal Processing Magazine},
  volume={36},
  number={6},
  pages={51--63},
  year={2019},
  publisher={IEEE}
}

@article{wu2018spatio,
  title={Spatio-temporal backpropagation for training high-performance spiking neural networks},
  author={Wu, Yujie and Deng, Lei and Li, Guoqi and Zhu, Jun and Shi, Luping},
  journal={Frontiers in neuroscience},
  volume={12},
  pages={331},
  year={2018},
  publisher={Frontiers}
}

@article{chowdhury2021spatio,
  title={Spatio-Temporal Pruning and Quantization for Low-latency Spiking Neural Networks},
  author={Chowdhury, Sayeed Shafayet and Garg, Isha and Roy, Kaushik},
  journal={arXiv preprint arXiv:2104.12528},
  year={2021}
}

@inproceedings{schaefer2020quantizing,
  title={Quantizing spiking neural networks with integers},
  author={Schaefer, Clemens JS and Joshi, Siddharth},
  booktitle={International Conference on Neuromorphic Systems 2020},
  pages={1--8},
  year={2020}
}

@article{lui2021hessian,
  title={Hessian Aware Quantization of Spiking Neural Networks},
  author={Lui, Hin Wai and Neftci, Emre},
  journal={arXiv preprint arXiv:2104.14117},
  year={2021}
}

@article{fang2021deep,
  title={Deep Residual Learning in Spiking Neural Networks},
  author={Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Huang, Tiejun and Masquelier, Timoth{\'e}e and Tian, Yonghong},
  journal={arXiv preprint arXiv:2102.04159},
  year={2021}
}

@article{liu2013asynchronous,
  title={Asynchronous Binaural Spatial Audition Sensor With 4 x 64 x 4 Channel Output},
  author={Liu, Shih-Chii and van Schaik, Andr{\'e} and Minch, Bradley A and Delbruck, Tobi},
  journal={IEEE transactions on biomedical circuits and systems},
  volume={8},
  number={4},
  pages={453--464},
  year={2013},
  publisher={IEEE}
}

@article{mahowald1992vlsi,
  title={VLSI analogs of neuronal visual processing: a synthesis of form and function},
  author={Mahowald, Misha},
  year={1992},
  publisher={California Institute of Technology}
}

@article{murthy1998synaptic,
  title={Synaptic plasticity: step-wise strengthening},
  author={Murthy, Venkatesh N},
  journal={Current biology},
  volume={8},
  number={18},
  pages={R650--R653},
  year={1998},
  publisher={Elsevier}
}

@article{sengupta2014power,
  title={Power consumption during neuronal computation},
  author={Sengupta, Biswa and Stemmler, Martin B},
  journal={Proceedings of the IEEE},
  volume={102},
  number={5},
  pages={738--750},
  year={2014},
  publisher={IEEE}
}

@article{hodgkin1952quantitative,
  title={A quantitative description of membrane current and its application to conduction and excitation in nerve},
  author={Hodgkin, Alan L and Huxley, Andrew F},
  journal={The Journal of physiology},
  volume={117},
  number={4},
  pages={500},
  year={1952},
  publisher={Wiley-Blackwell}
}

@article{izhikevich2003simple,
  title={Simple model of spiking neurons},
  author={Izhikevich, Eugene M},
  journal={IEEE Transactions on neural networks},
  volume={14},
  number={6},
  pages={1569--1572},
  year={2003},
  publisher={IEEE}
}

@article{izhikevich2004model,
  title={Which model to use for cortical spiking neurons?},
  author={Izhikevich, Eugene M},
  journal={IEEE transactions on neural networks},
  volume={15},
  number={5},
  pages={1063--1070},
  year={2004},
  publisher={Ieee}
}

@article{abbott1999lapicque,
  title={Lapicque’s introduction of the integrate-and-fire model neuron (1907)},
  author={Abbott, Larry F},
  journal={Brain research bulletin},
  volume={50},
  number={5-6},
  pages={303--304},
  year={1999},
  publisher={Citeseer}
}

@inproceedings{pan2019neural,
  title={Neural population coding for effective temporal classification},
  author={Pan, Zihan and Wu, Jibin and Zhang, Malu and Li, Haizhou and Chua, Yansong},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}

@article{guo2021neural,
  title={Neural coding in spiking neural networks: A comparative study for robust neuromorphic systems},
  author={Guo, Wenzhe and Fouda, Mohammed E and Eltawil, Ahmed M and Salama, Khaled Nabil},
  journal={Frontiers in Neuroscience},
  volume={15},
  pages={638474},
  year={2021},
  publisher={Frontiers Media SA}
}

@inproceedings{park2020t2fsnn,
  title={T2FSNN: Deep spiking neural networks with time-to-first-spike coding},
  author={Park, Seongsik and Kim, Seijoon and Na, Byunggook and Yoon, Sungroh},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@article{garcia2007evolution,
  title={The evolution of robotics research},
  author={Garcia, Elena and Jimenez, Maria Antonia and De Santos, Pablo Gonzalez and Armada, Manuel},
  journal={IEEE Robotics \& Automation Magazine},
  volume={14},
  number={1},
  pages={90--103},
  year={2007},
  publisher={IEEE}
}

@article{furber2007sparse,
  title={Sparse distributed memory using rank-order neural codes},
  author={Furber, Stephen B and Brown, Gavin and Bose, Joy and Cumpstey, John Michael and Marshall, Peter and Shapiro, Jonathan L},
  journal={IEEE Transactions on neural networks},
  volume={18},
  number={3},
  pages={648--659},
  year={2007},
  publisher={IEEE}
}

@article{gallego2020event,
  title={Event-based vision: A survey},
  author={Gallego, Guillermo and Delbr{\"u}ck, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J and Conradt, J{\"o}rg and Daniilidis, Kostas and others},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={1},
  pages={154--180},
  year={2020},
  publisher={IEEE}
}

@inproceedings{li2021towards,
  title={Towards Biologically-Plausible Neuron Models and Firing Rates in High-Performance Deep Spiking Neural Networks},
  author={Li, Chen and Furber, Steve},
  booktitle={International Conference on Neuromorphic Systems 2021},
  pages={1--7},
  year={2021}
}

@inproceedings{li2020robustness,
  title={Robustness to Noisy Synaptic Weights in Spiking Neural Networks},
  author={Li, Chen and Chen, Runze and Moutafis, Christoforos and Furber, Steve},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2020},
  organization={IEEE}
}


@article{zue1990speech,
  title={Speech database development at MIT: TIMIT and beyond},
  author={Zue, Victor and Seneff, Stephanie and Glass, James},
  journal={Speech communication},
  volume={9},
  number={4},
  pages={351--356},
  year={1990},
  publisher={Elsevier}
}

@article{brunel2001effects,
  title={Effects of synaptic noise and filtering on the frequency response of spiking neurons},
  author={Brunel, Nicolas and Chance, Frances S and Fourcaud, Nicolas and Abbott, Larry F},
  journal={Physical Review Letters},
  volume={86},
  number={10},
  pages={2186},
  year={2001},
  publisher={APS}
}

@article{kheradpisheh2018stdp,
  title={STDP-based spiking deep convolutional neural networks for object recognition},
  author={Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J and Masquelier, Timoth{\'e}e},
  journal={Neural Networks},
  volume={99},
  pages={56--67},
  year={2018},
  publisher={Elsevier}
}

@article{li2021differentiable,
  title={Differentiable Spike: Rethinking Gradient-Descent for Training Spiking Neural Networks},
  author={Li, Yuhang and Guo, Yufei and Zhang, Shanghang and Deng, Shikuang and Hai, Yongqing and Gu, Shi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{rathi2020diet,
  title={Diet-snn: Direct input encoding with leakage and threshold optimization in deep spiking neural networks},
  author={Rathi, Nitin and Roy, Kaushik},
  journal={arXiv preprint arXiv:2008.03658},
  year={2020}
}

@article{lee2018training,
  title={Training deep spiking convolutional neural networks with stdp-based unsupervised pre-training followed by supervised fine-tuning},
  author={Lee, Chankyu and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  journal={Frontiers in neuroscience},
  volume={12},
  pages={435},
  year={2018},
  publisher={Frontiers Media SA}
}

@article{iandola2016squeezenet,
  title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size},
  author={Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1602.07360},
  year={2016}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}

@article{diehl2015unsupervised,
  title={Unsupervised learning of digit recognition using spike-timing-dependent plasticity},
  author={Diehl, Peter U and Cook, Matthew},
  journal={Frontiers in computational neuroscience},
  volume={9},
  pages={99},
  year={2015},
  publisher={Frontiers Media SA}
}

@article{lin2017runtime,
  title={Runtime neural pruning},
  author={Lin, Ji and Rao, Yongming and Lu, Jiwen and Zhou, Jie},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{hua2019channel,
  title={Channel gating neural networks},
  author={Hua, Weizhe and Zhou, Yuan and De Sa, Christopher M and Zhang, Zhiru and Suh, G Edward},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{gao2018dynamic,
  title={Dynamic channel pruning: Feature boosting and suppression},
  author={Gao, Xitong and Zhao, Yiren and Dudziak, {\L}ukasz and Mullins, Robert and Xu, Cheng-zhong},
  journal={arXiv preprint arXiv:1810.05331},
  year={2018}
}

@inproceedings{almahairi2016dynamic,
  title={Dynamic capacity networks},
  author={Almahairi, Amjad and Ballas, Nicolas and Cooijmans, Tim and Zheng, Yin and Larochelle, Hugo and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={2549--2558},
  year={2016},
  organization={PMLR}
}

@inproceedings{chen2020dynamic,
  title={Dynamic convolution: Attention over convolution kernels},
  author={Chen, Yinpeng and Dai, Xiyang and Liu, Mengchen and Chen, Dongdong and Yuan, Lu and Liu, Zicheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11030--11039},
  year={2020}
}

@article{faisal2008noise,
  title={Noise in the nervous system},
  author={Faisal, A Aldo and Selen, Luc PJ and Wolpert, Daniel M},
  journal={Nature reviews neuroscience},
  volume={9},
  number={4},
  pages={292--303},
  year={2008},
  publisher={Nature Publishing Group}
}

@inproceedings{stromatias2015scalable,
  title={Scalable energy-efficient, low-latency implementations of trained spiking deep belief networks on spinnaker},
  author={Stromatias, Evangelos and Neil, Daniel and Galluppi, Francesco and Pfeiffer, Michael and Liu, Shih-Chii and Furber, Steve},
  booktitle={2015 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2015},
  organization={IEEE}
}

@phdthesis{han2017efficient,
  title={Efficient methods and hardware for deep learning},
  author={Han, Song},
  year={2017},
  school={Stanford University}
}

@inproceedings{kim2020spiking,
  title={Spiking-yolo: spiking neural network for energy-efficient object detection},
  author={Kim, Seijoon and Park, Seongsik and Na, Byunggook and Yoon, Sungroh},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={07},
  pages={11270--11277},
  year={2020}
}

@inproceedings{han2020rmp,
  title={Rmp-snn: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural network},
  author={Han, Bing and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={13558--13567},
  year={2020}
}

@inproceedings{wu2018blockdrop,
  title={Blockdrop: Dynamic inference paths in residual networks},
  author={Wu, Zuxuan and Nagarajan, Tushar and Kumar, Abhishek and Rennie, Steven and Davis, Larry S and Grauman, Kristen and Feris, Rogerio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8817--8826},
  year={2018}
}

@inproceedings{liu2022spikeconverter,
  title={Spikeconverter: An efficient conversion framework zipping the gap between artificial neural networks and spiking neural networks},
  author={Liu, Fangxin and Zhao, Wenbo and Chen, Yongbiao and Wang, Zongwu and Jiang, Li},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2022}
}

@inproceedings{he2017channel,
  title={Channel pruning for accelerating very deep neural networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1389--1397},
  year={2017}
}

@article{lin2022device,
  title={On-Device Training Under 256KB Memory},
  author={Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
  journal={arXiv preprint arXiv:2206.15472},
  year={2022}
}

@inproceedings{liu2018progressive,
  title={Progressive neural architecture search},
  author={Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={19--34},
  year={2018}
}


@article{courbariaux2015binaryconnect,
  title={Binaryconnect: Training deep neural networks with binary weights during propagations},
  author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@inproceedings{long2015fully,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3431--3440},
  year={2015}
}

@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={779--788},
  year={2016}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{hinton2006fast,
  title={A fast learning algorithm for deep belief nets},
  author={Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  journal={Neural computation},
  volume={18},
  number={7},
  pages={1527--1554},
  year={2006},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{sorbaro2020optimizing,
  title={Optimizing the energy consumption of spiking neural networks for neuromorphic applications},
  author={Sorbaro, Martino and Liu, Qian and Bortone, Massimo and Sheik, Sadique},
  journal={Frontiers in neuroscience},
  volume={14},
  pages={662},
  year={2020},
  publisher={Frontiers Media SA}
}

@inproceedings{narduzzi2022optimizing,
  title={Optimizing The Consumption Of Spiking Neural Networks With Activity Regularization},
  author={Narduzzi, Simon and Bigdeli, Siavash A and Liu, Shih-Chii and Dunbar, L Andrea},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={61--65},
  year={2022},
  organization={IEEE}
}

@book{raschka2019python,
  title={Python machine learning: Machine learning and deep learning with Python, scikit-learn, and TensorFlow 2},
  author={Raschka, Sebastian and Mirjalili, Vahid},
  year={2019},
  publisher={Packt Publishing Ltd}
}

@article{zhang2018generalized,
  title={Generalized cross entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@article{rhodes2018spynnaker,
  title={sPyNNaker: a software package for running PyNN simulations on SpiNNaker},
  author={Rhodes, Oliver and Bogdan, Petru{\c{t}} A and Brenninkmeijer, Christian and Davidson, Simon and Fellows, Donal and Gait, Andrew and Lester, David R and Mikaitis, Mantas and Plana, Luis A and Rowley, Andrew GD and others},
  journal={Frontiers in neuroscience},
  volume={12},
  pages={816},
  year={2018},
  publisher={Frontiers Media SA}
}

@article{patino2020event,
  title={Event-driven implementation of deep spiking convolutional neural networks for supervised classification using the SpiNNaker neuromorphic platform},
  author={Pati{\~n}o-Saucedo, Alberto and Rostro-Gonzalez, Horacio and Serrano-Gotarredona, Teresa and Linares-Barranco, Bernab{\'e}},
  journal={Neural Networks},
  volume={121},
  pages={319--328},
  year={2020},
  publisher={Elsevier}
}

@article{shrestha2018slayer,
  title={Slayer: Spike layer error reassignment in time},
  author={Shrestha, Sumit B and Orchard, Garrick},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{stockl2021optimized,
  title={Optimized spiking neurons can classify images with high accuracy through temporal coding with two spikes},
  author={St{\"o}ckl, Christoph and Maass, Wolfgang},
  journal={Nature Machine Intelligence},
  volume={3},
  number={3},
  pages={230--238},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{stockl2019recognizing,
  title={Recognizing images with at most one spike per neuron},
  author={St{\"o}ckl, Christoph and Maass, Wolfgang},
  journal={arXiv preprint arXiv:2001.01682},
  year={2019}
}



@inproceedings{diehl2015fast,
  title={Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing},
  author={Diehl, Peter U and Neil, Daniel and Binas, Jonathan and Cook, Matthew and Liu, Shih-Chii and Pfeiffer, Michael},
  booktitle={2015 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2015},
  organization={ieee}
}



@article{hunsberger2015spiking,
  title={Spiking deep networks with LIF neurons},
  author={Hunsberger, Eric and Eliasmith, Chris},
  journal={arXiv preprint arXiv:1510.08829},
  year={2015}
}


@inproceedings{esser2015backpropagation,
  title={Backpropagation for energy-efficient neuromorphic computing},
  author={Esser, Steve K and Appuswamy, Rathinakumar and Merolla, Paul and Arthur, John V and Modha, Dharmendra S},
  booktitle={Advances in neural information processing systems},
  pages={1117--1125},
  year={2015}
}



@article{lee2016training,
  title={Training deep spiking neural networks using backpropagation},
  author={Lee, Jun Haeng and Delbruck, Tobi and Pfeiffer, Michael},
  journal={Frontiers in neuroscience},
  volume={10},
  pages={508},
  year={2016},
  publisher={Frontiers}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}


@article{stromatias2015robustness,
  title={Robustness of spiking deep belief networks to noise and reduced bit precision of neuro-inspired hardware platforms},
  author={Stromatias, Evangelos and Neil, Daniel and Pfeiffer, Michael and Galluppi, Francesco and Furber, Steve B and Liu, Shih-Chii},
  journal={Frontiers in neuroscience},
  volume={9},
  pages={222},
  year={2015},
  publisher={Frontiers}
}



@article{o2016deep,
  title={Deep spiking networks},
  author={O'Connor, Peter and Welling, Max},
  journal={arXiv preprint arXiv:1602.08323},
  year={2016}
}



@article{liu2017noisy,
  title={Noisy softplus: an activation function that enables snns to be trained as anns},
  author={Liu, Qian and Chen, Yunhua and Furber, Steve},
  journal={arXiv preprint arXiv:1706.03609},
  year={2017}
}

@article{deng2021optimal,
  title={Optimal Conversion of Conventional Artificial Neural Networks to Spiking Neural Networks},
  author={Deng, Shikuang and Gu, Shi},
  journal={arXiv preprint arXiv:2103.00476},
  year={2021}
}

@article{li2021bsnn,
  title={BSNN: Towards Faster and Better Conversion of Artificial Neural Networks to Spiking Neural Networks with Bistable Neurons},
  author={Li, Yang and Zeng, Yi and Zhao, Dongcheng},
  journal={arXiv preprint arXiv:2105.12917},
  year={2021}
}


@article{rueckauer2017conversion,
  title={Conversion of continuous-valued deep networks to efficient event-driven networks for image classification},
  author={Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael and Liu, Shih-Chii},
  journal={Frontiers in neuroscience},
  volume={11},
  pages={682},
  year={2017},
  publisher={Frontiers}
}




@article{stockl2020optimized,
  title={Optimized spiking neurons can classify images with high accuracy through temporal coding with two spikes},
  author={St{\"o}ckl, Christoph and Maass, Wolfgang},
  journal={arXiv preprint arXiv:2002.00860},
  year={2020}
}

@article{sengupta2019going,
  title={Going deeper in spiking neural networks: Vgg and residual architectures},
  author={Sengupta, Abhronil and Ye, Yuting and Wang, Robert and Liu, Chiao and Roy, Kaushik},
  journal={Frontiers in neuroscience},
  volume={13},
  pages={95},
  year={2019},
  publisher={Frontiers}
}


@article{lennie2003cost,
  title={The cost of cortical computation},
  author={Lennie, Peter},
  journal={Current biology},
  volume={13},
  number={6},
  pages={493--497},
  year={2003},
  publisher={Elsevier}
}

@article{zhou2018synaptic,
  title={Synaptic EI balance underlies efficient neural coding},
  author={Zhou, Shanglin and Yu, Yuguo},
  journal={Frontiers in Neuroscience},
  volume={12},
  pages={46},
  year={2018},
  publisher={Frontiers Media SA}
}

@article{lillicrap2016random,
  title={Random synaptic feedback weights support error backpropagation for deep learning},
  author={Lillicrap, Timothy P and Cownden, Daniel and Tweed, Douglas B and Akerman, Colin J},
  journal={Nature communications},
  volume={7},
  number={1},
  pages={1--10},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{zhou2022spikformer,
  title={Spikformer: When Spiking Neural Network Meets Transformer},
  author={Zhou, Zhaokun and Zhu, Yuesheng and He, Chao and Wang, Yaowei and Yan, Shuicheng and Tian, Yonghong and Yuan, Li},
  journal={arXiv preprint arXiv:2209.15425},
  year={2022}
}

@article{hao2023reducing,
  title={Reducing ANN-SNN Conversion Error through Residual Membrane Potential},
  author={Hao, Zecheng and Bu, Tong and Ding, Jianhao and Huang, Tiejun and Yu, Zhaofei},
  journal={arXiv preprint arXiv:2302.02091},
  year={2023}
}

@article{bellec2020solution,
  title={A solution to the learning dilemma for recurrent networks of spiking neurons},
  author={Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={1--15},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{jug2012spiking,
  title={Spiking networks and their rate-based equivalents: does it make sense to use Siegert neurons?},
  author={Jug, Florian and Lengler, Johannes and Krautz, Christoph and Steger, Angelika},
  journal={Swiss Society for Neuroscience},
  year={2012}
}

@inproceedings{glorot2011deep,
  title={Deep sparse rectifier neural networks},
  author={Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={315--323},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{ostojic2011spiking,
  title={From spiking neuron models to linear-nonlinear models},
  author={Ostojic, Srdjan and Brunel, Nicolas},
  journal={PLoS computational biology},
  volume={7},
  number={1},
  pages={e1001056},
  year={2011},
  publisher={Public Library of Science San Francisco, USA}
}

@article{imam2020rapid,
  title={Rapid online learning and robust recall in a neuromorphic olfactory circuit},
  author={Imam, Nabil and Cleland, Thomas A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={3},
  pages={181--191},
  year={2020},
  publisher={Nature Publishing Group}
}
article{platt1999probabilistic,
  title={Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods},
  author={Platt, John and others},
  journal={Advances in large margin classifiers},
  volume={10},
  number={3},
  pages={61--74},
  year={1999},
  publisher={Cambridge, MA}
}
}

@inproceedings{zadrozny2002transforming,
  title={Transforming classifier scores into accurate multiclass probability estimates},
  author={Zadrozny, Bianca and Elkan, Charles},
  booktitle={Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={694--699},
  year={2002}
}

@article{kull2019beyond,
  title={Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration},
  author={Kull, Meelis and Perello Nieto, Miquel and K{\"a}ngsepp, Markus and Silva Filho, Telmo and Song, Hao and Flach, Peter},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{pfeiffer2018deep,
  title={Deep learning with spiking neurons: opportunities and challenges},
  author={Pfeiffer, Michael and Pfeil, Thomas},
  journal={Frontiers in neuroscience},
  pages={774},
  year={2018},
  publisher={Frontiers}
}

@article{davidson2021comparison,
  title={Comparison of artificial and spiking neural networks on digital hardware},
  author={Davidson, Simon and Furber, Steve B},
  journal={Frontiers in Neuroscience},
  volume={15},
  pages={651141},
  year={2021},
  publisher={Frontiers Media SA}
}

@inproceedings{blouw2019benchmarking,
  title={Benchmarking keyword spotting efficiency on neuromorphic hardware},
  author={Blouw, Peter and Choo, Xuan and Hunsberger, Eric and Eliasmith, Chris},
  booktitle={Proceedings of the 7th annual neuro-inspired computational elements workshop},
  pages={1--8},
  year={2019}
}

@inproceedings{hinton1993keeping,
  title={Keeping the neural networks simple by minimizing the description length of the weights},
  author={Hinton, Geoffrey E and Van Camp, Drew},
  booktitle={Proceedings of the sixth annual conference on Computational learning theory},
  pages={5--13},
  year={1993}
}

@article{strukov2008missing,
  title={The missing memristor found},
  author={Strukov, Dmitri B and Snider, Gregory S and Stewart, Duncan R and Williams, R Stanley},
  journal={nature},
  volume={453},
  number={7191},
  pages={80--83},
  year={2008},
  publisher={Nature Publishing Group}
}

@article{jiang2015blowing,
  title={Blowing magnetic skyrmion bubbles},
  author={Jiang, Wanjun and Upadhyaya, Pramey and Zhang, Wei and Yu, Guoqiang and Jungfleisch, M Benjamin and Fradin, Frank Y and Pearson, John E and Tserkovnyak, Yaroslav and Wang, Kang L and Heinonen, Olle and others},
  journal={Science},
  volume={349},
  number={6245},
  pages={283--286},
  year={2015},
  publisher={American Association for the Advancement of Science}
}

@article{zheng2018learning,
  title={Learning in memristor crossbar-based spiking neural networks through modulation of weight-dependent spike-timing-dependent plasticity},
  author={Zheng, Nan and Mazumder, Pinaki},
  journal={IEEE Transactions on Nanotechnology},
  volume={17},
  number={3},
  pages={520--532},
  year={2018},
  publisher={IEEE}
}

@article{sung2018perspective,
  title={Perspective: A review on memristive hardware for neuromorphic computation},
  author={Sung, Changhyuck and Hwang, Hyunsang and Yoo, In Kyeong},
  journal={Journal of Applied Physics},
  volume={124},
  number={15},
  pages={151903},
  year={2018},
  publisher={AIP Publishing LLC}
}

@article{chen2018magnetic,
  title={Magnetic skyrmion as a spintronic deep learning spiking neuron processor},
  author={Chen, Mei-Chin and Sengupta, Abhronil and Roy, Kaushik},
  journal={IEEE Transactions on Magnetics},
  volume={54},
  number={8},
  pages={1--7},
  year={2018},
  publisher={IEEE}
}

@article{buesing2011neural,
  title={Neural dynamics as sampling: a model for stochastic computation in recurrent networks of spiking neurons},
  author={Buesing, Lars and Bill, Johannes and Nessler, Bernhard and Maass, Wolfgang},
  journal={PLoS computational biology},
  volume={7},
  number={11},
  pages={e1002211},
  year={2011},
  publisher={Public Library of Science San Francisco, USA}
}

@article{rolls2011neuronal,
  title={The neuronal encoding of information in the brain},
  author={Rolls, Edmund T and Treves, Alessandro},
  journal={Progress in neurobiology},
  volume={95},
  number={3},
  pages={448--490},
  year={2011},
  publisher={Elsevier}
}

@article{tavanaei2019deep,
  title={Deep learning in spiking neural networks},
  author={Tavanaei, Amirhossein and Ghodrati, Masoud and Kheradpisheh, Saeed Reza and Masquelier, Timoth{\'e}e and Maida, Anthony},
  journal={Neural networks},
  volume={111},
  pages={47--63},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{wu2019deep,
  title={Deep spiking neural network with spike count based learning rule},
  author={Wu, Jibin and Chua, Yansong and Zhang, Malu and Yang, Qu and Li, Guoqi and Li, Haizhou},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--6},
  year={2019},
  organization={IEEE}
}

@inproceedings{mostafa2017fast,
  title={Fast classification using sparsely active spiking networks},
  author={Mostafa, Hesham and Pedroni, Bruno U and Sheik, Sadique and Cauwenberghs, Gert},
  booktitle={2017 IEEE International Symposium on Circuits and Systems (ISCAS)},
  pages={1--4},
  year={2017},
  organization={IEEE}
}



@inproceedings{cohen2017emnist,
  title={EMNIST: Extending MNIST to handwritten letters},
  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
  booktitle={2017 international joint conference on neural networks (IJCNN)},
  pages={2921--2926},
  year={2017},
  organization={IEEE}
}

@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

@inproceedings{fei2004learning,
  title={Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories},
  author={Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
  booktitle={2004 conference on computer vision and pattern recognition workshop},
  pages={178--178},
  year={2004},
  organization={IEEE}
}

@article{lin2021imagenet,
  title={ES-ImageNet: A Million Event-Stream Classification Dataset for Spiking Neural Networks},
  author={Lin, Yihan and Ding, Wei and Qiang, Shaohua and Deng, Lei and Li, Guoqi},
  journal={Frontiers in Neuroscience},
  pages={1546},
  year={2021},
  publisher={Frontiers}
}

@article{li2017cifar10,
  title={Cifar10-dvs: an event-stream dataset for object classification},
  author={Li, Hongmin and Liu, Hanchao and Ji, Xiangyang and Li, Guoqi and Shi, Luping},
  journal={Frontiers in neuroscience},
  volume={11},
  pages={309},
  year={2017},
  publisher={Frontiers}
}


@inproceedings{sironi2018hats,
  title={HATS: Histograms of averaged time surfaces for robust event-based object classification},
  author={Sironi, Amos and Brambilla, Manuele and Bourdis, Nicolas and Lagorce, Xavier and Benosman, Ryad},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1731--1740},
  year={2018}
}

@inproceedings{kriener2022yin,
  title={The yin-yang dataset},
  author={Kriener, Laura and G{\"o}ltz, Julian and Petrovici, Mihai A},
  booktitle={Neuro-Inspired Computational Elements Conference},
  pages={107--111},
  year={2022}
}

@inproceedings{amir2017low,
  title={A low power, fully event-based gesture recognition system},
  author={Amir, Arnon and Taba, Brian and Berg, David and Melano, Timothy and McKinstry, Jeffrey and Di Nolfo, Carmelo and Nayak, Tapan and Andreopoulos, Alexander and Garreau, Guillaume and Mendoza, Marcela and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7243--7252},
  year={2017}
}

@article{cramer2020heidelberg,
  title={The heidelberg spiking data sets for the systematic evaluation of spiking neural networks},
  author={Cramer, Benjamin and Stradmann, Yannik and Schemmel, Johannes and Zenke, Friedemann},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2020},
  publisher={IEEE}
}

@article{warden2018speech,
  title={Speech commands: A dataset for limited-vocabulary speech recognition},
  author={Warden, Pete},
  journal={arXiv preprint arXiv:1804.03209},
  year={2018}
}

@article{anumula2018feature,
  title={Feature representations for neuromorphic audio spike streams},
  author={Anumula, Jithendar and Neil, Daniel and Delbruck, Tobi and Liu, Shih-Chii},
  journal={Frontiers in neuroscience},
  volume={12},
  pages={23},
  year={2018},
  publisher={Frontiers}
}


@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{leonard1993tidigits,
  title={Tidigits speech corpus},
  author={Leonard, R Gary and Doddington, George},
  journal={Texas Instruments, Inc},
  year={1993}
}

@article{stimberg2019brian,
  title={Brian 2, an intuitive and efficient neural simulator},
  author={Stimberg, Marcel and Brette, Romain and Goodman, Dan FM},
  journal={Elife},
  volume={8},
  pages={e47314},
  year={2019},
  publisher={eLife Sciences Publications Limited}
}

@article{stimberg2020brian2genn,
  title={Brian2GeNN: accelerating spiking neural network simulations with graphics hardware},
  author={Stimberg, Marcel and Goodman, Dan FM and Nowotny, Thomas},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--12},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{goodman2008brian,
  title={Brian: a simulator for spiking neural networks in python},
  author={Goodman, Dan FM and Brette, Romain},
  journal={Frontiers in neuroinformatics},
  volume={2},
  pages={5},
  year={2008},
  publisher={Frontiers}
}

@article{gewaltig2007nest,
  title={Nest (neural simulation tool)},
  author={Gewaltig, Marc-Oliver and Diesmann, Markus},
  journal={Scholarpedia},
  volume={2},
  number={4},
  pages={1430},
  year={2007}
}

@article{garofolo1993darpa,
  title={DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1},
  author={Garofolo, John S and Lamel, Lori F and Fisher, William M and Fiscus, Jonathan G and Pallett, David S},
  journal={NASA STI/Recon technical report n},
  volume={93},
  pages={27403},
  year={1993}
}

@article{nakamura2000acoustical,
  title={Acoustical sound database in real environments for sound scene understanding and hands-free speech recognition},
  author={Nakamura, Satoshi and Hiyane, Kazuo and Asano, Futoshi and Nishiura, Takanobu and Yamada, Takeshi},
  year={2000}
}


@article{everingham2015pascal,
  title={The pascal visual object classes challenge: A retrospective},
  author={Everingham, Mark and Eslami, SM and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={International journal of computer vision},
  volume={111},
  number={1},
  pages={98--136},
  year={2015},
  publisher={Springer}
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}


@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}
%krizhevsky2009learning wasn't actually published
@article{li2021free,
  title={A Free Lunch From ANN: Towards Efficient, Accurate Spiking Neural Networks Calibration},
  author={Li, Yuhang and Deng, Shikuang and Dong, Xin and Gong, Ruihao and Gu, Shi},
  journal={arXiv preprint arXiv:2106.06984},
  year={2021}
}

@article{hasanpour2016lets,
  title={Lets keep it simple, using simple architectures to outperform deeper and more complex architectures},
  author={Hasanpour, Seyyed Hossein and Rouhani, Mohammad and Fayyaz, Mohsen and Sabokrou, Mohammad},
  journal={arXiv preprint arXiv:1608.06037},
  year={2016}
}

@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}


@article{serrano2015poker,
  title={Poker-DVS and MNIST-DVS. Their history, how they were made, and other details},
  author={Serrano-Gotarredona, Teresa and Linares-Barranco, Bernab{\'e}},
  journal={Frontiers in neuroscience},
  volume={9},
  pages={481},
  year={2015},
  publisher={Frontiers}
}

@article{orchard2015converting,
  title={Converting static image datasets to spiking neuromorphic datasets using saccades},
  author={Orchard, Garrick and Jayawant, Ajinkya and Cohen, Gregory K and Thakor, Nitish},
  journal={Frontiers in neuroscience},
  volume={9},
  pages={437},
  year={2015},
  publisher={Frontiers}
}


@inproceedings{schemmel2010wafer,
  title={A wafer-scale neuromorphic hardware system for large-scale neural modeling},
  author={Schemmel, Johannes and Br{\"u}derle, Daniel and Gr{\"u}bl, Andreas and Hock, Matthias and Meier, Karlheinz and Millner, Sebastian},
  booktitle={2010 IEEE International Symposium on Circuits and Systems (ISCAS)},
  pages={1947--1950},
  year={2010},
  organization={IEEE}
}

@article{khodagholy2015neurogrid,
  title={NeuroGrid: recording action potentials from the surface of the brain},
  author={Khodagholy, Dion and Gelinas, Jennifer N and Thesen, Thomas and Doyle, Werner and Devinsky, Orrin and Malliaras, George G and Buzs{\'a}ki, Gy{\"o}rgy},
  journal={Nature neuroscience},
  volume={18},
  number={2},
  pages={310--315},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{juarez2022r,
  title={R-STDP Spiking Neural Network Architecture for Motion Control on a Changing Friction Joint Robotic Arm},
  author={Juarez-Lora, Alejandro and Ponce-Ponce, Victor H and Sossa, Humberto and Rubio-Espino, Elsa},
  journal={Frontiers in Neurorobotics},
  volume={16},
  year={2022},
  publisher={Frontiers Media SA}
}

@book{gerstner2014neuronal,
  title={Neuronal dynamics: From single neurons to networks and models of cognition},
  author={Gerstner, Wulfram and Kistler, Werner M and Naud, Richard and Paninski, Liam},
  year={2014},
  publisher={Cambridge University Press}
}

@book{towle1991modern,
  title={Modern biology},
  author={Towle, Albert},
  year={1991},
  publisher={Holt McDougal}
}

@article{brett2011neuron,
  title={Neuron Anatomy},
  author={brett Szymik, Khodagholy},
  publisher={Arizona State University School of Life Sciences Ask A Biologist}
}


@article{davies2018loihi,
  title={Loihi: A neuromorphic manycore processor with on-chip learning},
  author={Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and others},
  journal={Ieee Micro},
  volume={38},
  number={1},
  pages={82--99},
  year={2018},
  publisher={IEEE}
}

@article{merolla2014million,
  title={A million spiking-neuron integrated circuit with a scalable communication network and interface},
  author={Merolla, Paul A and Arthur, John V and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and others},
  journal={Science},
  volume={345},
  number={6197},
  pages={668--673},
  year={2014},
  publisher={American Association for the Advancement of Science}
}

@article{mead1990neuromorphic,
  title={Neuromorphic electronic systems},
  author={Mead, Carver},
  journal={Proceedings of the IEEE},
  volume={78},
  number={10},
  pages={1629--1636},
  year={1990},
  publisher={IEEE}
}

@article{shainline2017superconducting,
  title={Superconducting optoelectronic circuits for neuromorphic computing},
  author={Shainline, Jeffrey M and Buckley, Sonia M and Mirin, Richard P and Nam, Sae Woo},
  journal={Physical Review Applied},
  volume={7},
  number={3},
  pages={034013},
  year={2017},
  publisher={APS}
}

@article{song2020skyrmion,
  title={Skyrmion-based artificial synapses for neuromorphic computing},
  author={Song, Kyung Mee and Jeong, Jae-Seung and Pan, Biao and Zhang, Xichao and Xia, Jing and Cha, Sunkyung and Park, Tae-Eon and Kim, Kwangsu and Finizio, Simone and Raabe, J{\"o}rg and others},
  journal={Nature Electronics},
  volume={3},
  number={3},
  pages={148--155},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{li2018review,
  title={Review of memristor devices in neuromorphic computing: materials sciences and device challenges},
  author={Li, Yibo and Wang, Zhongrui and Midya, Rivu and Xia, Qiangfei and Yang, J Joshua},
  journal={Journal of Physics D: Applied Physics},
  volume={51},
  number={50},
  pages={503002},
  year={2018},
  publisher={IOP Publishing}
}

@article{furber2012overview,
  title={Overview of the SpiNNaker system architecture},
  author={Furber, Steve B and Lester, David R and Plana, Luis A and Garside, Jim D and Painkras, Eustace and Temple, Steve and Brown, Andrew D},
  journal={IEEE transactions on computers},
  volume={62},
  number={12},
  pages={2454--2467},
  year={2012},
  publisher={IEEE}
}

@article{furber2014spinnaker,
  title={The spinnaker project},
  author={Furber, Steve B and Galluppi, Francesco and Temple, Steve and Plana, Luis A},
  journal={Proceedings of the IEEE},
  volume={102},
  number={5},
  pages={652--665},
  year={2014},
  publisher={IEEE}
}
@article{dampfhofferAreSNNsReally2022,
  ids = {dampfhofferAreSNNsReally2022a},
  title = {Are {{SNNs Really More Energy-Efficient Than ANNs}}? An {{In-Depth Hardware-Aware Study}}},
  shorttitle = {Are {{SNNs Really More Energy-Efficient Than ANNs}}?},
  author = {Dampfhoffer, Manon and Mesquida, Thomas and Valentian, Alexandre and Anghel, Lorena},
  date = {2022},
  year = {2022},
  journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  pages = {1--11},
  issn = {2471-285X},
  doi = {10.1109/TETCI.2022.3214509},
  abstract = {Spiking Neural Networks (SNNs) hold the promise of lower energy consumption in embedded hardware due to their spike-based computations compared to traditional Artificial Neural Networks (ANNs). The relative energy efficiency of this emerging technology compared to traditional digital hardware has not been fully explored. Many studies do not consider memory accesses, which account for an important fraction of the energy consumption, use naive ANN hardware implementations, or lack generality. In this paper, we compare the relative energy efficiency of classical digital implementations of ANNs with novel event-based SNN implementations based on variants of the Integrate and Fire (IF) model. We provide a theoretical upper bound on the relative energy efficiency of ANNs, by computing the maximum possible benefit from ANN data reuse and sparsity. We also use the Eyeriss ANN accelerator as a case study. We show that the simpler IF model is more energy-efficient than the Leaky IF and temporal continuous synapse models. Moreover, SNNs with the IF model can compete with efficient ANN implementations when there is a very high spike sparsity, i.e. between 0.15 and 1.38 spikes per synapse per inference, depending on the ANN implementation. Our analysis shows that hybrid ANN-SNN architectures, leveraging a SNN event-based approach in layers with high sparsity and ANN parallel processing for the others, are a promising new path for further energy savings.},
  eventtitle = {{{IEEE Transactions}} on {{Emerging Topics}} in {{Computational Intelligence}}},
  keywords = {Computational modeling,Computer architecture,deep neural network accelerators,Energy consumption,energy efficiency,Hardware,Membrane potentials,neuromorphic hardware,Neurons,Spiking neural networks,Synapses},
  file = {/home/edwardjones/Zotero/storage/EMBNBI4F/Dampfhoffer et al_2022_Are SNNs Really More Energy-Efficient Than ANNs.pdf;/home/edwardjones/Zotero/storage/83JDWLV7/9927729.html}
}

@article{juFPGAImplementationDeep2020,
  ids = {juFPGAImplementationDeep2020a},
  title = {An {{FPGA Implementation}} of {{Deep Spiking Neural Networks}} for {{Low-Power}} and {{Fast Classification}}},
  author = {Ju, Xiping and Fang, Biao and Yan, Rui and Xu, Xiaoliang and Tang, Huajin},
  year = {2020},
  journal= {Neural Computation},
  volume = {32},
  number = {1},
  pages = {182--204},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01245},
  abstract = {A spiking neural network (SNN) is a type of biological plausibility model that performs information processing based on spikes. Training a deep SNN effectively is challenging due to the nondifferention of spike signals. Recent advances have shown that high-performance SNNs can be obtained by converting convolutional neural networks (CNNs). However, the large-scale SNNs are poorly served by conventional architectures due to the dynamic nature of spiking neurons. In this letter, we propose a hardware architecture to enable efficient implementation of SNNs. All layers in the network are mapped on one chip so that the computation of different time steps can be done in parallel to reduce latency. We propose new spiking max-pooling method to reduce computation complexity. In addition, we apply approaches based on shift register and coarsely grained parallels to accelerate convolution operation. We also investigate the effect of different encoding methods on SNN accuracy. Finally, we validate the hardware architecture on the Xilinx Zynq ZCU102. The experimental results on the MNIST data set show that it can achieve an accuracy of 98.94\% with eight-bit quantized weights. Furthermore, it achieves 164 frames per second (FPS) under 150 MHz clock frequency and obtains 41× speed-up compared to CPU implementation and 22 times lower power than GPU implementation.},
  eventtitle = {Neural {{Computation}}},
  file = {/home/edwardjones/Zotero/storage/PWC77DI5/8937120.html}
}

@article{kimSpikingYOLOSpikingNeural2020,
  title = {Spiking-{{YOLO}}: {{Spiking Neural Network}} for {{Energy-Efficient Object Detection}}},
  shorttitle = {Spiking-{{YOLO}}},
  author = {Kim, Seijoon and Park, Seongsik and Na, Byunggook and Yoon, Sungroh},
  year = {2020},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {07},
  pages = {11270--11277},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i07.6787},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/6787},
  urldate = {2022-11-08},
  abstract = {Over the past decade, deep neural networks (DNNs) have demonstrated remarkable performance in a variety of applications. As we try to solve more advanced problems, increasing demands for computing and power resources has become inevitable. Spiking neural networks (SNNs) have attracted widespread interest as the third-generation of neural networks due to their event-driven and low-powered nature. SNNs, however, are difficult to train, mainly owing to their complex dynamics of neurons and non-differentiable spike operations. Furthermore, their applications have been limited to relatively simple tasks such as image classification. In this study, we investigate the performance degradation of SNNs in a more challenging regression problem (i.e., object detection). Through our in-depth analysis, we introduce two novel methods: channel-wise normalization and signed neuron with imbalanced threshold, both of which provide fast and accurate information transmission for deep SNNs. Consequently, we present a first spiked-based object detection model, called Spiking-YOLO. Our experiments show that Spiking-YOLO achieves remarkable results that are comparable (up to 98\%) to those of Tiny YOLO on non-trivial datasets, PASCAL VOC and MS COCO. Furthermore, Spiking-YOLO on a neuromorphic chip consumes approximately 280 times less energy than Tiny YOLO and converges 2.3 to 4 times faster than previous SNN conversion methods.},
  issue = {07},
  langid = {english},
  file = {/home/edwardjones/Zotero/storage/BR6LYKII/Kim et al_2020_Spiking-YOLO.pdf}
}

@unpublished{dengOptimalConversionConventional2021,
  title = {Optimal {{Conversion}} of {{Conventional Artificial Neural Networks}} to {{Spiking Neural Networks}}},
  author = {Deng, Shikuang and Gu, Shi},
  date = {2021-02-28},
  eprint = {2103.00476},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2103.00476},
  urldate = {2021-03-06},
  abstract = {Spiking neural networks (SNNs) are biology-inspired artificial neural networks (ANNs) that comprise of spiking neurons to process asynchronous discrete signals. While more efficient in power consumption and inference speed on the neuromorphic hardware, SNNs are usually difficult to train directly from scratch with spikes due to the discreteness. As an alternative, many efforts have been devoted to converting conventional ANNs into SNNs by copying the weights from ANNs and adjusting the spiking threshold potential of neurons in SNNs. Researchers have designed new SNN architectures and conversion algorithms to diminish the conversion error. However, an effective conversion should address the difference between the SNN and ANN architectures with an efficient approximation of the loss function, which is missing in the field. In this work, we analyze the conversion error by recursive reduction to layer-wise summation and propose a novel strategic pipeline that transfers the weights to the target SNN by combining threshold balance and soft-reset mechanisms. This pipeline enables almost no accuracy loss between the converted SNNs and conventional ANNs with only ∼ 1/10 of the typical SNN simulation time. Our method is promising to get implanted onto embedded platforms with better support of SNNs with limited energy and memory. Codes are available at https://github.com/Jackn0/snn optimal conversion pipeline.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/edwardjones/Zotero/storage/X9SRY9N4/Deng and Gu - 2021 - Optimal Conversion of Conventional Artificial Neur.pdf}
}

@article{huangStructuredDynamicPrecision2022,
  title = {Structured {{Dynamic Precision}} for {{Deep Neural Networks Quantization}}},
  author = {Huang, Kai and Li, Bowen and Xiong, Dongliang and Jiang, Haitian and Jiang, Xiaowen and Yan, Xiaolang and Claesen, Luc and Liu, Dehong and Chen, Junjian and Liu, Zhili},
  year = {2022},
  month = jul,
  journal = {ACM Transactions on Design Automation of Electronic Systems},
  pages = {3549535},
  issn = {1084-4309, 1557-7309},
  doi = {10.1145/3549535},
  abstract = {Deep Neural Networks (DNNs) have achieved remarkable success in various Artiicial Intelligence (AI) applications. Quantization is a critical step in DNNs compression and acceleration for deployment. To further boost DNN execution eiciency, many works explore to leverage the input-dependent redundancy with dynamic quantization for diferent regions. However, the sensitive regions in the feature map are irregularly distributed, which restricts the real speed up for existing accelerators. To this end, we propose an algorithm-architecture co-design, named Structured Dynamic Precision (SDP). In speciic, we propose a quantization scheme in which the high-order bit part and the low-order bit part of data can be masked independently. And a ixed number of term parts are dynamically selected for computation based on the importance of each term in the group. We also present a hardware design to enable the algorithm eiciently with small overheads, whose inference time mainly scales with the precision proportionally. Evaluation experiments on extensive networks demonstrate that compared to the state-of-the-art dynamic quantization accelerator DRQ, our SDP can achieve 29\% performance gain and 51\% energy reduction for the same level of model accuracy. CCS Concepts: {$\cdot$} Hardware \textrightarrow{} Power and energy; {$\cdot$} Computing methodologies \textrightarrow{} Computer vision; {$\cdot$} Computer systems organization \textrightarrow{} Neural networks.},
  langid = {english},
  file = {/home/edwardjones/Zotero/storage/P46J65LA/Huang et al. - 2022 - Structured Dynamic Precision for Deep Neural Netwo.pdf}
}

@inproceedings{songDRQDynamicRegionbased2020,
  ids = {songDRQDynamicRegionbased2020a},
  title = {{{DRQ}}: {{Dynamic Region-based Quantization}} for {{Deep Neural Network Acceleration}}},
  shorttitle = {{{DRQ}}},
  booktitle = {2020 {{ACM}}/{{IEEE}} 47th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Song, Zhuoran and Fu, Bangqi and Wu, Feiyang and Jiang, Zhaoming and Jiang, Li and Jing, Naifeng and Liang, Xiaoyao},
  year = {2020},
  month = may,
  pages = {1010--1021},
  doi = {10.1109/ISCA45697.2020.00086},
  abstract = {Quantization is an effective technique for Deep Neural Network (DNN) inference acceleration. However, conventional quantization techniques are either applied at network or layer level that may fail to exploit fine-grained quantization for further speedup, or only applied on kernel weights without paying attention to the feature map dynamics that may lead to lower NN accuracy. In this paper, we propose a dynamic region-based quantization, namely DRQ, which can change the precision of a DNN model dynamically based on the sensitive regions in the feature map to achieve greater acceleration while reserving better NN accuracy. We propose an algorithm to identify the sensitive regions and an architecture that utilizes a variable-speed mixed-precision convolution array to enable the algorithm with better performance and energy efficiency. Our experiments on a wide variety of networks show that compared to a coarse-grained quantization accelerator like ``Eyeriss'', DRQ can achieve 92\% performance gain and 72\% energy reduction with less then 1\% accuracy loss. Compared to the state-of-the-art mixed-precision quantization accelerator ``OLAccel'', DRQ can also achieve 21\% performance gain and 33\% energy reduction with 3\% prediction accuracy improvement which is quite impressive for inference.},
  file = {/home/edwardjones/Zotero/storage/JV42R63I/Song et al_2020_DRQ.pdf;/home/edwardjones/Zotero/storage/ETVDGR7J/9138970.html}
}

@article{goltz2021fast,
  title={Fast and energy-efficient neuromorphic deep learning with first-spike times},
  author={G{\"o}ltz, Julian and Kriener, Laura and Baumbach, Andreas and Billaudelle, Sebastian and Breitwieser, Oliver and Cramer, Benjamin and Dold, Dominik and Kungl, Akos Ferenc and Senn, Walter and Schemmel, Johannes and others},
  journal={Nature machine intelligence},
  volume={3},
  number={9},
  pages={823--835},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{bagheri2018training,
  title={Training probabilistic spiking neural networks with first-to-spike decoding},
  author={Bagheri, Alireza and Simeone, Osvaldo and Rajendran, Bipin},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2986--2990},
  year={2018},
  organization={IEEE}
}



@article{luExploringConnectionBinary2020,
  title = {Exploring the {{Connection Between Binary}} and {{Spiking Neural Networks}}},
  author = {Lu, Sen and Sengupta, Abhronil},
  year = {2020},
  journal = {Frontiers in Neuroscience},
  volume = {14},
  issn = {1662-453X},
  abstract = {On-chip edge intelligence has necessitated the exploration of algorithmic techniques to reduce the compute requirements of current machine learning frameworks. This work aims to bridge the recent algorithmic progress in training Binary Neural Networks and Spiking Neural Networks\textemdash both of which are driven by the same motivation and yet synergies between the two have not been fully explored. We show that training Spiking Neural Networks in the extreme quantization regime results in near full precision accuracies on large-scale datasets like CIFAR-100 and ImageNet. An important implication of this work is that Binary Spiking Neural Networks can be enabled by ``In-Memory'' hardware accelerators catered for Binary Neural Networks without suffering any accuracy degradation due to binarization. We utilize standard training techniques for non-spiking networks to generate our spiking networks by conversion process and also perform an extensive empirical analysis and explore simple design-time and run-time optimization techniques for reducing inference latency of spiking networks (both for binary and full-precision models) by an order of magnitude over prior work. Our implementation source code and trained models are available at https://github.com/NeuroCompLab-psu/SNN-Conversion.}
}

@article{davidsonComparisonArtificialSpiking2021,
  title = {Comparison of {{Artificial}} and {{Spiking Neural Networks}} on {{Digital Hardware}}},
  author = {Davidson, Simon and Furber, Steve B.},
  year = {2021},
  journal = {Frontiers in Neuroscience},
  volume = {0},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2021.651141},
  abstract = {Despite the success of Deep Neural Networks - a type of Artificial Neural Network (ANN) - in problem domains such as image recognition and speech processing, the energy and processing demands during both training and deployment are growing at an unsustainable rate in the push for greater accuracy. There is a temptation to look for radical new approaches to these applications, and one such approach is the notion that replacing the abstract neuron used in most deep networks with a more biologically-plausible spiking neuron might lead to savings in both energy and resource cost. The most common spiking networks use rate-coded neurons for which a simple translation from a pretrained ANN to an equivalent spike-based network (SNN) is readily achievable. But does the spike-based network offer an improvement of energy efficiency over the original deep network? In this work, we consider the digital implementations of the core steps in an ANN and the equivalent steps in a rate-coded spiking neural network. We establish a simple method of assessing the relative advantages of rate-based spike encoding over a conventional ANN model\vphantom\{\}. Assuming identical underlying silicon technology we show that most rate-coded spiking network implementations will not be more energy or resource efficient than the original ANN, concluding that more imaginative uses of spikes are required to displace conventional ANNs as the dominant computing framework for neural computation.},
  langid = {english},
  keywords = {artificial neural network,Deep neural network,neuromorphic hardware,rate-based encoding,Spiking Neural network},
  file = {/home/edwardjones/Zotero/storage/VTSLNEZE/Davidson_Furber_2021_Comparison of Artificial and Spiking Neural Networks on Digital Hardware.pdf}
}

@misc{Authors14b,
 author = {Full Author Name},
 title = {Frobnication Tutorial},
 note = {Supplied as additional material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {Alvin Alpher},
title = {Frobnication},
journal = {Journal of Foo},
volume = 12, 
number = 1, 
pages = {234--778}, 
year = 2002
}

@article{Alpher03,
author = {Alvin Alpher and Ferris P.~N. Fotheringham-Smythe},
title = {Frobnication Revisited},
journal = {Journal of Foo},
volume = 13, 
number = 1, 
pages = {234--778}, 
year = 2003
}

@article{Alpher04,
author = {Alvin Alpher and Ferris P.~N. Fotheringham-Smythe and Gavin Gamow},
title = {Can a Machine Frobnicate?},
journal = {Journal of Foo},
volume = 14, 
number = 1, 
pages = {234--778}, 
year = 2004
}
