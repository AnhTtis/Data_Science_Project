%%
%% Copyright 2021 OXFORD UNIVERSITY PRESS
%%
%% This file is part of the 'ima-authoring-template Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'ima-authoring-template Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for OXFORD UNIVERSITY PRESS's document class `ima-authoring-template'
%% with bibliographic references
%%

\documentclass[a4paper]{article}

%% Language and font encodings
%\usepackage[dutch]{babel}
%\usepackage[utf8x]{inputenc}
%\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=3cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
%\usepackage{fullpage}
%% Useful packages
\usepackage{amsmath}
\usepackage{cite}
\usepackage{tikz}
%\usepackage{tikzscale}
%\usetikzlibrary{external}
%\tikzexternalize[prefix=tikz/]
\usetikzlibrary{matrix}
\newlength\figureheight
\newlength\figurewidth
\usepackage{pgfplots}
\usepackage{amssymb}
\usepackage{mathdots}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepgfplotslibrary{patchplots}
\pgfplotsset{compat=1.14}

\usepackage{booktabs}
%\usepackage[font=small,labelfont=bf]{caption}
\usepackage{subcaption}

\usepackage{url}

\usepackage{amsthm}

\usepackage{epstopdf}


\usepackage{mathtools}

%\theoremstyle{thmstyletwo}%
%%\newtheorem{theorem}{Theorem}%  meant for continuous numbers
\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}%
\newtheorem{lemma}[theorem]{Lemma}%
\newtheorem{property}[theorem]{Property}%
\newtheorem{corollary}[theorem]{Corollary}%
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\newtheorem{example}{Example}[section]%
\newtheorem{problem}{Problem}[section]%

\newtheorem{definition}{Definition}[section]

\numberwithin{equation}{section}

\newcommand{\block}[1]{
	\underbrace{\begin{matrix}0 & \cdots & 0\end{matrix}}_{#1}
}


\begin{document}
	
	
	%\subtitle{Subject Section}
	
	\title{A new Legendre polynomial-based approach for non-autonomous linear ODEs}
		\providecommand{\keywords}[1]{\textit{Keywords: } #1}
	
	\author{Stefano Pozza\footnotemark[1] and Niel Van Buggenhout\footnotemark[1]}

	\renewcommand{\thefootnote}{\fnsymbol{footnote}}
	\footnotetext[1]{Charles University, Sokolovsk√° 83, 186 75 Praha 8, Czech Republic. (email: pozza@karlin.mff.cuni.cz; buggenhout@karlin.mff.cuni.cz )}
%	\author{Stefano Pozza\ORCID{0000-0003-1529-8420} and Niel Van Buggenhout*\ORCID{0000-0003-0856-0670}
%		\address{\orgdiv{Department of Numerical Mathematics}, \orgname{Charles University}, \orgaddress{\street{Sokolovsk\'a 83}, \postcode{186 75}, \state{Praha 8}, \country{Czech Republic}}}}
%	\authormark{S. Pozza and N. Van Buggenhout}
	
%	\corresp[*]{Corresponding author: \href{email:buggenhout@karlin.mff.cuni.cz}{buggenhout@karlin.mff.cuni.cz}}
%	
%	\received{Date}{0}{Year}
%	\revised{Date}{0}{Year}
%	\accepted{Date}{0}{Year}
	
	%\editor{Associate Editor: Name}
		\maketitle
	\begin{abstract}
		We introduce a new method with spectral accuracy to solve linear non-autonomous ordinary differential equations (ODEs) of the kind $ \frac{d}{dt}\tilde{u}(t) = \tilde{f}(t) \tilde{u}(t)$, $\tilde{u}(-1)=1$, with $\tilde{f}(t)$ an analytic function.
		The method is based on a new expression for the solution $\tilde{u}(t)$ given in terms of a convolution-like operation, the $\star$-product. This expression is represented in a finite Legendre polynomial basis translating the initial problem into a matrix problem. An efficient procedure is proposed to approximate the Legendre coefficients of $\tilde{u}(t)$ and its truncation error is analyzed. We show the effectiveness of the proposed procedure through some numerical experiments. The method can be easily generalized to solve systems of linear ODEs.
	\end{abstract}
	\indent \keywords{Legendre polynomials; spectral accuracy; ordinary differential equations.}
	
	% \boxedtext{
	% \begin{itemize}
	% \item Key boxed text here.
	% \item Key boxed text here.
	% \item Key boxed text here.
	% \end{itemize}}
	

	
	
	\section{Introduction}
	Let $\tilde{A}(t)$ be an $N \times N$ analytic matrix-valued function over the interval $[-1, 1]$. Then the unique solution of the ordinary differential equation (ODE) 
	\begin{equation}\label{eq:ode:intro}
		\frac{d}{dt} \tilde{U}(t) = \tilde{A}(t) \tilde{U}(t), \quad \tilde{U}(-1) = I_N, \quad t\in\left[-1,1\right],
	\end{equation}
    is also an analytic $N \times N$ matrix-valued function $\tilde{U}(t)$, where $I_N$ is the identity matrix of size $N\times N$.
	Systems of non-autonomous linear ODEs are ubiquitous in mathematics and related applications. 
	An application of particular interest is nuclear magnetic resonance spectroscopy (NMR).
   	In NMR, the given matrix-valued function is of the form $\tilde{A}(t) = - 2\pi \imath \tilde{H}(t)$, where $\tilde{H}(t)$ is the Hamiltonian of the system \cite{HaSp98,Le08}.
	The Hamiltonian describes the dynamics of the nuclear spins in some sample that is placed in a varying magnetic field. 
	For $\ell$ spins the Hamiltonian is of size $2^\ell \times 2^\ell$.
	So, for a moderate amount of spins, the Hamiltonian becomes extremely large. Thankfully, this matrix is usually sparse since the dominant interactions are those between neighboring spins.  While such large problems will not be considered in this paper, the numerical approach presented here has the ambition to provide a new framework for tackling these challenging problems. This numerical framework arises from a recently developed analytical framework \cite{GiLuThJa15,GiPo20,BonGis2020} in which the solution of \eqref{eq:ode:intro} is given by a simple expression.
 
When $\tilde{A}(\tau_1)\tilde{A}(\tau_2)=\tilde{A}(\tau_2)\tilde{A}(\tau_1)$ for every $\tau_1,\tau_2 \in \left[-1,1\right]$, $\tilde{U}(t)$ can be expressed in the explicit form:
$$\tilde{U}(t)=\exp\left(\int_s^{t} \tilde{A}(\tau)\, \text{d}\tau\right).$$ 
Unfortunately, in general, $\tilde{U}(t)$ has no known simple expression in terms of $\tilde{A}(t)$. 
Nevertheless, a closed form for $\tilde{U}(t)$ exists in a specific algebraic structure of distributions, as shown in \cite{GiLuThJa15} (another possible expression for the solution is the Magnus expansion; see \cite{Blanes2009}).
Let $\tilde{F}(t,s)$ be a matrix-valued function analytic in both variables over $[-1,1]$.
In the previously appeared works on the $\star$-product, the functions are usually assumed to be smooth. Here, we restrict the assumption to analytic for the sake of simplicity since the application we are considering deals with analytic functions.
Moreover, let $\Theta(t-s)$ be the Heaviside step function
    \begin{equation*}
		\Theta(t-s)  = \begin{cases}
			0, \quad \text{if } t<s\\
			1, \quad \text{otherwise}
		\end{cases}.
	\end{equation*}
Let us define  $\mathcal{A}^N_{\Theta}$ as the set of all the distributions of the kind $F(t,s):=\tilde{F}(t,s)\Theta(t-s)$ with size $N \times N$. 
Given $F, G \in \mathcal{A}^N_{\Theta}$, the $\star$-product \cite{GisPozInv19}, denoted by $\star$, is defined as
	\begin{equation*}
		F(t,s) \star G(t,s) := \int_{-1}^{1} G(t,\tau) F(\tau,s) d\tau, \quad \text{with }F,G \in \mathcal{A}^N_{\Theta}.
	\end{equation*}
    From the definition above, by replacing the matrix-matrix product in the integrand with an appropriate one, the $\star$-product can be easily extended to the matrix-vector, matrix-scalar, vector-scalar, and scalar-scalar products. 
    The $\star$-product identity is the distribution $I_\star(t-s) = \delta(t-s)I_N$, where $\delta(t-s)$ is the Dirac delta distribution, and $I_N$ the identity matrix of size $N$ \cite{schwartz1978}.
    Overall, we have defined an algebraic structure (a module) composed of the set of distributions $\mathcal{D}_0^N : = \mathcal{A}_\Theta^N \cup \{I_\star\}$, the $\star$-product (interpreted both as a matrix-matrix and a scalar product), and the usual addition. 
    Table \ref{table:starAlgebra} summarizes the $\star$-product properties and other useful related definitions.
    \begin{table}[ht]
		\centering
		\begin{tabular}{l|l}
			 Operations and related objects & Description \\				\hline  
			  $F(t,s) \star G(t,s)$                            &  matrix-matrix product $\mathcal{D}_0^N \times \mathcal{D}_0^N \rightarrow \mathcal{D}_0^N$ \\
            $f(t,s) \star G(t,s) $      &  (left) scalar product $\mathcal{D}_0^1 \times \mathcal{D}_0^N \rightarrow \mathcal{D}_0^N$ \\
            $G(t,s) \star f(t,s) $      &  (right) scalar product $\mathcal{D}_0^N \times \mathcal{D}_0^1 \rightarrow \mathcal{D}_0^N$ \\
			$f + g$                                          &  usual addition  \\
			$I_{\star} = \delta(t-s) I_N$                    &  identity   (Dirac delta)       \\
			$R_\star(F)(t,s) = I_\star + \sum_{k\geq 1} F^{\star k}(t,s)$   &  $\star$-resolvent
		\end{tabular}
		\caption{Operations and related objects in the $\star$-framework.}	
		\label{table:starAlgebra}
	\end{table}	
 
 The ODE \eqref{eq:ode:intro} can be formulated in the $\star$-framework in terms of $A(t,s) := \tilde{A}(t) \Theta(t-s)$:
	\begin{equation*}
		\frac{d}{dt} U(t,s) = A(t,s) U(t,s), \quad U(s,s) = I_N, \quad t,s\in\left[-1,1\right].
	\end{equation*}
	By Theorem 3.1 in \cite{GiLuThJa15}, the solution to this ODE can be expressed in the form
 \begin{equation}\label{eq:ODE:star:sol}
     U(t,s) = \Theta(t-s) \star R_\star(A)(t,s),
 \end{equation}
 where $R_\star(\cdot)$ is the $\star$-resolvent, i.e., $R_\star(A)(t,s) = I_\star + \sum_{k\geq 1} A^{\star k}(t,s)$, with $A^{\star k}(t,s)$ the $k$th $\star$-power of $A$.
 Once $U(t,s)$ is known, the solution to the original ODE can be obtained by evaluation in $s=-1$, i.e., $\tilde{U}(t) = U(t,-1)$. The symbolic computation of \eqref{eq:ODE:star:sol} leads to complicated expressions involving special functions, even for problems of moderate size.
  
  This paper introduces a new numerical strategy for the computation of $U(t,-1)$ by discretizing the $\star$-product.
	For the sake of simplicity, we will restrict the discussion to the scalar case
	\begin{equation}\label{eq:ODE_univar}
		\frac{d}{dt} \tilde{u}(t) = \tilde{f}(t) \tilde{u}(t),\quad \tilde{u}(-1) = 1,\quad t\in\left[-1,1\right],
	\end{equation}
	where $\tilde{f}(t)$ is an analytical function.
	It is important to remark that the numerical approximation scheme presented in this paper can be easily extended to the matrix case \cite{PoVB22}.
	We also rewrite Equation~\eqref{eq:ODE_univar} in the $\star$-framework as
	\begin{equation}\label{eq:ODE_bivar}
		\frac{d}{dt} u(t,s) = \underbrace{\tilde{f}(t)\Theta(t-s)}_{=:f(t,s)} u(t,s),\quad u(s,s) = 1,\quad t,s\in\left[-1,1\right].
	\end{equation}
    In the following, we denote $\mathcal{A}_\Theta^1$ with the simpler notation $\mathcal{A}_\Theta$.
	In order to devise a numerical approach, we will transform the computation of $\Theta(t-s) \star R_\star(f)(t,s)$ into a linear algebra problem.
	To this end, the operations and objects in the $\star$-algebraic structure are rewritten as equivalent operations and objects in the usual matrix algebra.
	The distributions in $\mathcal{A}_{\Theta}$ are expanded as an infinite double series of Legendre polynomials, and the coefficients in this series are grouped in a matrix.
	The resulting infinite \textit{coefficient matrix} then represents an element in $\mathcal{A}_{\Theta}$, and the operations in Table \ref{table:starAlgebra} have a meaningful equivalent in the matrix algebra of these coefficient matrices.
 
	Section \ref{sec:Legendre} describes the expansion in Legendre basis and shows how the problem of solving ODE \eqref{eq:ODE_univar} can be reformulated as an infinite matrix problem with the coefficient matrix of $f(t,s)\in \mathcal{A}_{\Theta}$.
	The properties of the coefficient matrix are analyzed in detail in Section \ref{sec:coeffMatrix}, and an analytical formula is provided for its entries.
	In Section \ref{sec:practicalComp}, these properties are used to show that the infinite problem can be approximated by a finite problem, and techniques to solve this finite problem efficiently are proposed.
	Section \ref{sec:NumExp} formulates a finite matrix problem corresponding to approximating $\tilde{u}(t)$ and proposes a numerical procedure to solve it.
	The effectiveness of this procedure is illustrated by numerical examples.
	
	\section{From the $\star$-product to the matrix algebra}\label{sec:Legendre}
	The proposed numerical method for the approximation of $\tilde{u}(t)$ is based on the expansion of the distribution $f(t,s)\in \mathcal{A}_{\Theta}$ in a basis of orthonormal Legendre polynomials.
	The distribution $f(t,s)$ can be represented by its \textit{coefficient matrix}, which contains the expansion (Fourier) coefficients of $f(t,s)$.
	The solution $U(t,-1)$ is obtained by exploiting the connection between the $\star$-product and the usual matrix algebra.
	Section \ref{sec:LegendrePoly} discusses the expansion of functions and distributions in the basis of orthonormal Legendre polynomials and defines the coefficient matrix.
	In Section \ref{sec:star_to_matrix}, the connection between the $\star$-product and the matrix algebra is used to reformulate the problem in \eqref{eq:ODE_univar} as an infinite matrix problem.
	
	
	
	\subsection{Legendre polynomials}\label{sec:LegendrePoly}
	The sequence of orthonormal Legendre polynomials $\{p_k\}_{k\geq 0}$ consists of polynomials $p_k$ of exact degree $k$ that satisfy the orthonormality conditions
	\begin{equation*}
		\int_{-1}^1 p_k(t) p_{\ell}(t) dt= \begin{cases}
			0,\quad \text{if } k\neq \ell\\
			1,\quad \text{if } k=\ell
		\end{cases}.
	\end{equation*}
	For a univariate function $\tilde{f}(t)$, its expansion into the Legendre basis is given by
	\begin{equation*}
		\tilde{f}(t) := \sum_{d=0}^\infty \alpha_d p_d(t),\quad \text{with } \alpha_d = \int_{-1}^1 \tilde{f}(t) p_d(t) dt.
	\end{equation*}
	The Fourier coefficients $\{\alpha_d\}_{d\geq 0}$ decay at a rate depending on the smoothness of $\tilde{f}(t)$.
	% In NMR and many other important applications {\color{red}XXX citations??XXX} the functions are entire functions.
	% Let us assume in this paper that the functions of interest are at least analytic.
	 Any analytic function over $[-1, 1]$ allows an analytic continuation to a Bernstein ellipse $E_\rho$ for a $\rho>1$ small enough. Therefore, for some constant $C>0$, the Fourier coefficients satisfy
	\begin{equation}\label{eq:coeffDecayRate}
		\vert a_d\vert \leq C \rho^{-d-1};
	\end{equation}
	for details we refer to \cite{Tr13,WaXi12}.
	Moreover, the orthonormal Legendre polynomials can be bounded by
	\begin{equation*}
		\vert p_d(t)\vert \leq \sqrt{\frac{2d+1}{2}} \quad \text{for } t\in\left[-1,1\right],
	\end{equation*}
	and therefore the truncated expansion $\hat{f}_N(t) := \sum_{d=0}^N \alpha_d p_d(t)$ has the error, measured in the maximum norm for $t\in\left[-1,1\right]$,
	\begin{equation}\label{eq:boundLeg}
		\Vert \tilde{f}(t) - \hat{f}_N(t)  \Vert_\infty = \sum_{d=N+1}^\infty \alpha_d p_k(t) \leq \sum_{d=N+1}^\infty \vert \alpha_d \vert \sqrt{\frac{2d+1}{2}}.
	\end{equation}
	Hence, if the (decaying) coefficients $\alpha_N,\alpha_{N+1},\dots$ are smaller than a given threshold, the truncation $\hat{f}_N(t)$ can provide a good approximation to $\tilde{f}(t)$.\\	
	Consider a distribution $f\in \mathcal{A}_\Theta$.
	Its Legendre expansion is
	\begin{equation*}
		f(t,s) = \sum_{k=0}^{\infty} \sum_{\ell=0}^{\infty} f_{k,\ell} p_k(t) p_\ell(s), \quad \text{for every }t\neq s, \quad t,s\in \left[-1,1\right],
	\end{equation*}	
	with Fourier coefficients given by
	\begin{equation}\label{eq:FourCoeffs}
		f_{k,\ell} = \int_{-1}^{1} \int_{-1}^{1} f(\tau,\rho) p_k(\tau) p_\ell(\rho) d\rho d\tau.
	\end{equation}
	We define the \emph{coefficient matrix} $F$ of the distribution $f(t,s)$, which is the infinite matrix composed of the Fourier coefficients $\eqref{eq:FourCoeffs}$
	\begin{equation}\label{eq:coeffMatrix}
		F := \begin{bmatrix}
			f_{k,\ell}
		\end{bmatrix}_{k,\ell=0}^\infty = \begin{bmatrix}
			f_{0,0} & f_{0,1} & f_{0,2} & \dots \\
			f_{1,0} & f_{1,1} & f_{1,2} & \dots \\
			f_{2,0} & f_{2,1} & f_{2,2} & \dots \\
			\vdots & \vdots & \vdots & \ddots
		\end{bmatrix}.
	\end{equation}
	The distribution $f(t,s)$ is only piecewise smooth, because the Heaviside function $\Theta(t-s)$ introduces discontinuities.
	Thus, its Fourier coefficients $f_{k,\ell}$ do not decay at a geometric rate.
	Due to these discontinuities, there is essentially no decay in the coefficients, and the Gibbs phenomenon arises \cite{GoSh97}.
	This means that the reconstruction of  $f(t,s)$ over the entire domain $[-1,1] \times [-1,1]$ by using only the coefficients $f_{k,\ell}$ is not possible.
	For such a reconstruction, there is no convergence at the discontinuities $t=s$, and away from the discontinuities, it converges only linearly to the actual values. 
	There are techniques to resolve the Gibbs phenomenon; see, for example, \cite{GoSh97,GeTa06}.
	In our setting, such techniques are not needed since we only need accurate coefficients $f_{k,\ell}$ representing $f(t,s)$ in the Legendre basis, and we will not use these to reconstruct the function values on the entire domain $(t,s)\in \left[-1,1\right]\times\left[-1,1\right]$, but only on $t\in\left[-1,1\right]$ for $s=-1$, i.e., where the function is analytic in $t$.
 
	
	
	
	\subsection{A matrix formulation}\label{sec:star_to_matrix}
	The operations of addition and $\star$-multiplication for distributions in $\mathcal{A}_\Theta$ have equivalent operations in the matrix algebra of the associated coefficient matrices, namely the usual matrix addition and matrix-matrix multiplication.
	\begin{lemma}\label{lemma:matrixProduct}
		Consider $f,g\in \mathcal{A}_{\Theta}$ and their respective coefficient matrices $F,G$ in Legendre basis.
		Then:
		\begin{itemize}
			\item $f+g = h \in \mathcal{A}_{\Theta}$ and its coefficient matrix is $H = F+G$.
			\item $f\star g =h \in \mathcal{A}_{\Theta}$ and, assuming the matrix product is well-defined, its coefficient matrix is $H = F G$.
		\end{itemize}
	\end{lemma}
	\begin{proof}
		Addition: from the Legendre expansion of $f$ and $g$ it follows that
		\begin{align*}
			h = f+g &= \begin{bmatrix}
				p_0(t) & p_1(t) & \cdots
			\end{bmatrix} F \begin{bmatrix}
				p_0(s)\\
				p_1(s)\\
				\vdots
			\end{bmatrix} + \begin{bmatrix}
				p_0(t) & p_1(t) & \cdots
			\end{bmatrix} G \begin{bmatrix}
				p_0(s)\\
				p_1(s)\\
				\vdots
			\end{bmatrix}\\
			&= \begin{bmatrix}
				p_0(t) & p_1(t) & \cdots
			\end{bmatrix} \underbrace{(F+G)}_{=H} \begin{bmatrix}
				p_0(s)\\
				p_1(s)\\
				\vdots
			\end{bmatrix}.
		\end{align*}
		Multiplication: plugging in the double series and using the definition of the $\star$-product provides
		\begin{align*}
			h &= f\star g = \int_{-1}^1 \begin{bmatrix}
				p_0(t) & p_1(t) & \cdots
			\end{bmatrix} F \begin{bmatrix}
				p_0(\tau)\\
				p_1(\tau)\\
				\vdots
			\end{bmatrix} \begin{bmatrix}
				p_0(\tau) & p_1(\tau) & \cdots
			\end{bmatrix} G \begin{bmatrix}
				p_0(s)\\
				p_1(s)\\
				\vdots
			\end{bmatrix} d\tau\\
			&= \begin{bmatrix}
				p_0(t) & p_1(t) & \cdots
			\end{bmatrix} F \left(\int_{-1}^1 \begin{bmatrix}
				p_0(\tau) p_0(\tau) & p_0(\tau) p_1(\tau) & \dots\\
				p_1(\tau) p_0(\tau) & p_1(\tau) p_1(\tau) & \dots\\
				\vdots & \vdots & \ddots
			\end{bmatrix} d\tau\right) G \begin{bmatrix}
				p_0(s)\\
				p_1(s)\\
				\vdots
			\end{bmatrix}.
		\end{align*}
		Thanks to the orthonormality of $\{p_k(t)\}_{k\geq 0}$, the matrix in the middle equals the identity matrix
		\begin{equation*}
			\begin{bmatrix}
				\int_{-1}^1 p_0(\tau) p_0(\tau)d\tau & \int_{-1}^1 p_0(\tau) p_1(\tau)d\tau & \dots\\
				\int_{-1}^1 p_1(\tau) p_0(\tau)d\tau & \int_{-1}^1 p_1(\tau) p_1(\tau)d\tau & \dots\\
				\vdots & \vdots & \ddots
			\end{bmatrix}  = \begin{bmatrix}
				1 & 0 & \dots\\
				0 & 1 & \ddots\\
				\vdots & \ddots & \ddots 
			\end{bmatrix}.
		\end{equation*}
		Thus we get
		\begin{equation*}
			h = f\star g = \begin{bmatrix}
				p_0(t) & p_1(t) & \cdots
			\end{bmatrix} \underbrace{FG}_{=H} \begin{bmatrix}
				p_0(s)\\
				p_1(s)\\
				\vdots
			\end{bmatrix},
		\end{equation*}
		that is, the coefficient matrix for $h$ is $H=FG$, under the assumption that this matrix product is well-defined.
	\end{proof}
	In section \ref{sec:completeProof}, the infinite matrix product is discussed and we show that the matrix product between coefficient matrices of distributions $f\in\mathcal{A}_\Theta$ is always well-defined.
	If $f(t,s)$ is bounded for $t,s\in\left[-1,1\right]$, then the $\star$-resolvent of $f(t,s)$, 
	$R_{\star}(f) = 1_\star + \sum_{k\geq 1} f^{\star k}$,
	exists, since the series $\sum_{k\geq 1} f^{\star k}$ uniformly converges in $\mathcal{A}_\Theta$ for every $t,s \in [-1,1]$; see \cite{GiLuThJa15}. 
	Since,
	\begin{equation*}
		R_{\star}(f) \star (1_\star - f) = \left(1_\star + \sum_{k\geq 1} f^{\star k}\right) \star (1_\star - f) = 1_\star,
	\end{equation*}
	 the $\star$-resolvent is the $\star$-inverse of $(1_\star -f)$, i.e., $R_{\star}(f) = (1_\star - f)^{-\star}$.
	Let $g := \sum_{k\geq 1} f^{\star k}$, then $g \in\mathcal{A}_\Theta$ and, hence, we can define its coefficient matrix $G$. 
	Therefore, we have
	\begin{align*}
		R_{\star}(f) &= 1_\star + \sum_{k\geq 1} f^{\star k} = \phi(t)^T\left(I + G \right) \phi(s), \text{ with  } \phi(\tau):= \begin{bmatrix}
			p_0(\tau)\\
			p_1(\tau)\\
			\vdots
		\end{bmatrix},
	\end{align*}
	which allows us to derive the following relation between $(I+G)$ and $(I-F)$,
	\begin{align}\label{eq:reso:mtx:1}
		1_\star =(R_{\star}(f)\star (1_\star - f))(t,s) &= 
		\left(\phi(t)^T 
		\left(I + G \right) 
		\phi(s)\right)
		\star
		\left(\phi(t)^T 
		\left(I - F \right) 
		\phi(s)\right), \\ \label{eq:reso:mtx:2}
		&= 
		\phi(t)^T 
		\left(I + G \right) 
		\left(I - F \right) 
		\phi(s) =  \phi(t)^T I\, \phi(s).
	\end{align}
	As a consequence, we have the following result.
	\begin{lemma}\label{cor:resolvent}
		Consider $f\in \mathcal{A}_{\Theta}$ and its corresponding coefficient matrix $F$.
		If the inverse of the infinite matrix $(I-F)$ exists, then
		\begin{align*}
			R_{\star}(f) &= \begin{bmatrix}
				p_0(t) & p_1(t) & \dots
			\end{bmatrix}(I-F)^{-1} \begin{bmatrix}
				p_0(s)\\
				p_1(s)\\
				\vdots
			\end{bmatrix}.
		\end{align*}
	\end{lemma}
	\begin{proof}
		Let us define $R_{\star}(f) := \phi(t)^T (I-F)^{-1} \phi(s)$, i.e., set $(I+G) = (I-F)^{-1}$. Then, by Equations~\eqref{eq:reso:mtx:1} and \eqref{eq:reso:mtx:2}, we get $R_{\star}(f)\star (1_\star - f) = 1_\star$.
	\end{proof}
	Combining Lemmas~\ref{lemma:matrixProduct} and \ref{cor:resolvent} allows us to obtain an expression for the Legendre coefficients of $\tilde{u}(t)$ in terms of coefficient matrices.
	This expression is the matrix counterpart to the expression for $\tilde{u}(t)$ in the $\star$-framework: $\tilde{u}(t) = u(t,s)\vert_{s=-1} = \Theta(t-s) \star R_\star(f)\vert_{s=-1}$, see \eqref{eq:ODE:star:sol}, and is stated in the following theorem.
	\begin{theorem}\label{theorem:coeffs_smoothu}
		Consider $f\in \mathcal{A}_{\Theta}$ and its corresponding coefficient matrix $F$.
		Let $T$ denote the coefficient matrix of $\Theta(t-s)$, $I$ the identity matrix and $\{p_k\}_{k\geq 0}$ the sequence of orthonormal Legendre polynomials.
		Assume that $(I-F)$ is invertible.
		Then, the Legendre coefficients $\{c_k\}_{k\geq 0}$ of the solution $\tilde{u}(t)$ of the ODE \eqref{eq:ODE_univar} are given by
		\begin{equation}\label{eq:sysEq}
			\begin{bmatrix}
				c_0\\
				c_1\\
				c_2\\
				\vdots
			\end{bmatrix} = T (I-F)^{-1} \begin{bmatrix}
				p_0(-1)\\
				p_1(-1)\\
				p_2(-1)\\
				\vdots
			\end{bmatrix}.
		\end{equation}
	\end{theorem}
	Based on Theorem \ref{theorem:coeffs_smoothu}, we can formulate a matrix problem that is equivalent to the problem of solving ODE \eqref{eq:ODE_univar}.
	
	\begin{problem}[Infinite matrix problem]\label{prob:matrixProblem}
		Given a smooth function $\tilde{f}(t)$, compute the Legendre coefficients $\{c_k\}_{k=0}^\infty$ of the solution $\tilde{u}(t)$ to the ODE \eqref{eq:ODE_univar}.
		By \eqref{eq:sysEq} this corresponds to three matrix problems:
		\begin{enumerate}
			\item Construct the infinite coefficient matrix $F=\begin{bmatrix}
				f_{k,\ell}
			\end{bmatrix}_{k,\ell=0}^\infty$ of Fourier coefficients in Legendre basis $f_{k,\ell} = \int_{-1}^1\int_{-1}^1 \tilde{f}(\tau)\Theta(\tau-\rho) p_k(\tau)p_{\ell}(\rho) d\rho d\tau$.
			\item Solve the infinite linear system of equations $(I-F)x = \phi(-1)$ for $x$. The right hand side is the column vector $\phi(-1) = \begin{bmatrix}
				p_k(-1)
			\end{bmatrix}_{k=0}^{\infty}$ and $I$ is the infinite identity matrix.
			\item Compute the matrix-vector product $T x = \begin{bmatrix}
				c_0& c_1& c_2& \cdots
			\end{bmatrix}^\top$, where $T$ is the coefficient matrix of $\Theta(t-s)$.
		\end{enumerate}
	\end{problem}
	Problem \ref{prob:matrixProblem} is the main problem to solve.
	In the remainder of this paper, we develop a numerical scheme to approximate its solution and investigate the conditions under which this approximation is expected to converge.
 	
	\section{The coefficient matrix and its properties}\label{sec:coeffMatrix}
	Since the coefficient matrix is central to our analysis and to the proposed procedure, we study its structure.
	In Section \ref{sec:coeffMatrix_decay}, an analytical expression for the entries of the coefficient matrices is presented and we show that the entries decay along the diagonals.
	Section \ref{sec:coeffMatrix_band} proves that the coefficient matrices can be approximated by a banded matrix.
	These results allow us to show, in Section \ref{sec:completeProof}, that the matrix-matrix product between two coefficient matrices of distributions in $\mathcal{A}_{\Theta}$ is well-defined, see Lemma~\ref{lemma:matrixProduct}.
	
	\subsection{Formula for the Fourier coefficients}\label{sec:coeffMatrix_decay}
	The Fourier coefficients $f_{k,\ell}$ \eqref{eq:FourCoeffs} of $f(t,s)= \tilde{f}(t) \Theta(t-s)\in \mathcal{A}_{\Theta}$ are studied by relying on the Legendre expansion of the analytical function $\tilde{f}(t) = \sum_{d=0}^{\infty} \alpha_d p_d(t)$, its fast decaying coefficients $\{\alpha_d\}_{d\geq 0}$ and the coefficient matrices of $p_d(t)\Theta(t-s)\in \mathcal{A}_{\Theta}$.
	The Fourier coefficients of $p_d(t)\Theta(t-s)$ can be computed via an analytical formula, see Theorem~\ref{theorem:FormulaLegCoeffs}.
	This formula follows from combining the two known properties of Legendre polynomials below.
	\begin{property}[Integral of a Legendre polynomial on a subinterval {\cite[p.178]{Sa77}}]\label{prop:int_shift_Legendre}
		Let $p_\ell(t)$ denote the orthonormal Legendre polynomial of degree $\ell$.
		For $\ell = 0$ it holds that
		\begin{equation*}
			\int_{-1}^{\tau} p_0(\rho)d\rho = \frac{1}{\sqrt{3}} p_1(\tau) + p_0(\tau),
		\end{equation*}
		and for $\ell>0$
		\begin{equation*}
			\int_{-1}^{\tau} p_\ell(\rho)d\rho = \frac{1}{\sqrt{2\ell+1}} \left(\frac{1}{\sqrt{2\ell+3}}p_{\ell+1}(\tau) - \frac{1}{\sqrt{2\ell-1}} p_{\ell-1}(\tau)  \right).
		\end{equation*}
	\end{property}
	
	\begin{property}[Integral of the triple product of Legendre polynomials \cite{GiJeZe88}]\label{prop:intLeg}
		Let $p_\ell$ be the orthonormal Legendre polynomial of degree $\ell$.
		Consider integers $a,b,c\geq 0$ and set $s:=\frac{a+b+c}{2}$ and $\alpha:=\vert b-c\vert$.
		The integral of the product of three orthonormal Legendre polynomials is
		\begin{align*}
			\mathcal{F}_{a,b,c} &:= \int_{-1}^{1} p_a(\rho) p_b(\rho) p_c(\rho) d\rho \\
			&= 
			\begin{cases}
				0,\quad \text{if } a+b+c \text{ odd},\\
				0, \quad \text{if } s<\max(a,b,c),\\
				0, \quad \text{if } a<\vert b-c \vert,\\
				\frac{\sqrt{(2a+1)(2b+1)(2c+1)}}{\sqrt{2}(a+b+c+1)}
				\begin{psmallmatrix}
					2s-2a\\
					s-a
				\end{psmallmatrix} \begin{psmallmatrix}
					2s-2b\\
					s-b
				\end{psmallmatrix} \begin{psmallmatrix}
					2s-2c\\
					s-c
				\end{psmallmatrix} \begin{psmallmatrix}
					2s\\
					s
				\end{psmallmatrix}^{-1}, \quad \text{else}
			\end{cases}\\
			&= \begin{cases}
				0,\quad \text{if } a+b+c \text{ odd},\\
				0, \quad \text{if } b+c<a\\%s<\max(a,b,c),\\
				0, \quad \text{if } a<\alpha,\\
				\frac{\sqrt{(2a+1)(2b+1)(2c+1)}}{2^{(2a+1/2)}} \frac{\prod_{j=1}^{a}\frac{-a+b+c+2j}{-a+b+c+2j-1}}{(a+b+c+1)}  {\frac{\prod_{j=(\frac{a+\alpha}{2}+1)}^{a+\alpha}j^2}{\prod_{j=1}^{\frac{a-\alpha}{2}}j^2 \prod_{j=(a-\alpha+1)}^{a+\alpha} j}} ,\quad \text{else}.
			\end{cases}
		\end{align*}
	\end{property}
	
	\begin{theorem}[Coefficients of a Legendre polynomial in $\mathcal{A}_\Theta$]\label{theorem:FormulaLegCoeffs}
		Let $p_d(t),p_k(t),p_\ell(s)$ be the orthonormal Legendre polynomials of degree $d,k,\ell$, respectively, and $\mathcal{F}_{a,b,c}$ as in Property \ref{prop:intLeg}.
		Then the coefficients $b_{k,\ell}^{(d)}$ of the Legendre expansion of $p_d(t)\Theta(t-s)$ are given, for $\ell=0$, by		
		\begin{equation*}
			b^{(d)}_{k,0}= \frac{1}{\sqrt{3}}\mathcal{F}_{d,k,1}+\mathcal{F}_{d,k,0},
		\end{equation*}
		and, for $\ell>0$, by
		\begin{equation}\label{eq:LegBasisCoeffs}
			b^{(d)}_{k,\ell}=\frac{1}{\sqrt{2\ell+1}} \left(\frac{1}{\sqrt{2\ell+3}}\mathcal{F}_{d,k,\ell+1}-\frac{1}{\sqrt{2\ell-1}}\mathcal{F}_{d,k,\ell-1}\right).
		\end{equation}
	\end{theorem}
	\begin{proof}
		By orthonormality of the Legendre polynomials, the Fourier coefficients for $\ell>0$ are given by
		\begin{align*}
			b^{(d)}_{k,\ell} &:= \int_{-1}^1 \int_{-1}^1 p_d(\tau) \Theta(\tau-\rho) p_k(\tau) p_\ell(\rho) d\rho d\tau\\
			&= \int_{-1}^1  p_d(\tau) p_k(\tau) \left(\int_{-1}^1\Theta(\tau-\rho)  p_\ell(\rho) d\rho\right) d\tau
			= \int_{-1}^1  p_d(\tau) p_k(\tau) \underbrace{\left(\int_{-1}^\tau p_\ell(\rho) d\rho\right)}_{\textrm{Apply Property \ref{prop:int_shift_Legendre}}} d\tau \\
			&=  \frac{1}{\sqrt{2\ell+1}} \left[ \frac{1}{\sqrt{2\ell+3}}\int_{-1}^1  p_d(\tau) p_k(\tau) p_{\ell+1}(\tau) d\tau - \frac{1}{\sqrt{2\ell-1}}\int_{-1}^1  p_d(\tau) p_k(\tau)  p_{\ell-1}(\tau)  d\tau\right]\\
			&= \frac{1}{\sqrt{2\ell+1}} \left[\frac{1}{\sqrt{2\ell+3}} \mathcal{F}_{d,k,\ell+1} -\frac{1}{\sqrt{2\ell-1}} \mathcal{F}_{d,k,\ell-1} \right].
		\end{align*}
		For $\ell=0$ the same derivation holds, by using the formula for this case stated in Property \ref{prop:int_shift_Legendre}.
	\end{proof}
	Denote the coefficient matrix of $p_d(t)\Theta(t-s)$ by $B^{(d)} := \left[b_{k,\ell}^{(d)}\right]_{k,\ell=0}^{\infty}$, with $b_{k,\ell}^{(d)}$ as in Theorem \ref{theorem:FormulaLegCoeffs}.
	We will call such a matrix the \textit{Legendre basis matrix of degree $d$}.
	Along a diagonal of $B^{(d)}$ the entries decay linearly, this is formally stated in Lemma \ref{lemma:LegCoeffsDecay}.
	
	\begin{lemma}[Decay of Legendre basis coefficients]\label{lemma:LegCoeffsDecay}
		For $b_{k,l}^{(d)}$, as in Theorem~\ref{theorem:FormulaLegCoeffs}, it holds that
		\begin{equation*}
			\lim_{\substack{k,\ell\rightarrow\infty\\ \vert k-\ell\vert \text{ constant}}} \vert b^{(d)}_{k,\ell} \vert \sim \mathcal{O}(1/\ell).
		\end{equation*}
	\end{lemma}
	\begin{proof}
		In the last equality in the formula in Property \ref{prop:intLeg}, the last fraction is constant since $\alpha = \vert b-c\vert$ is constant and $a=d$ is fixed.
		Then, it is straightforward to see that there is no decay in the expression of the integral over the triple product,
		\begin{equation*}
			\lim_{\substack{k,\ell\rightarrow\infty\\ \vert k-\ell\vert \text{ constant}}} \mathcal{F}_{d,k,\ell} \sim \mathcal{O}(1).
		\end{equation*}
		Since 
		\begin{equation*}
			\lim_{\substack{k,\ell\rightarrow\infty\\ \vert k-\ell\vert \text{ constant}}} \frac{1}{\sqrt{(2\ell+1)(2\ell-1)}} \sim \mathcal{O}(1/\ell),
		\end{equation*}
		the statement follows from \eqref{eq:LegBasisCoeffs}.
	\end{proof}
	The coefficient matrix $F := \left[f_{k,\ell}\right]_{k,\ell=0}^{\infty}$ of $f(t,s) = \tilde{f}(t)\Theta(t-s)\in \mathcal{A}_\Theta$ can be written as $F = \sum_{d=0}^{\infty} \alpha_d B^{(d)}$, where $\{\alpha_d\}_{d\geq 0}$ are the Legendre coefficients of $\tilde{f}(t)$.
	Thus, we can relate properties of $B^{(d)}$ to properties of $F$.
	Namely, the fact that along a diagonal of $F$ the entries decay linearly follows from Lemma \ref{lemma:LegCoeffsDecay} and is illustrated in Example \ref{example:decay}.
	\begin{corollary}[Decay of expansion coefficients]\label{cor:expCoeffsDecay}
		Let $\alpha=\vert k-\ell\vert$ be constant as $k,\ell$ go to infinity. 
		Then the coefficients $f_{k,\ell}$ of the Legendre expansion of $f(t,s)\in \mathcal{A}_{\Theta}$ decay asymptotically at the rate $\frac{1}{\ell}$.
	\end{corollary}
	\begin{example}\label{example:decay}
		The polynomial of degree one $\tilde{f}(t) = -\imath \tau (t+1)$, with $\tau>0$, can be written as a linear combination of $p_0(t)$ and $p_1(t)$, namely $-\imath \tau (t+1) = -2\imath \tau p_0(t) - \sqrt{\frac{2}{3}} \imath \tau p_1(t)$.
		Thus, its coefficient matrix is $F = -2\imath \tau B^{(0)} - \sqrt{\frac{2}{3}} \imath \tau B^{(1)}$, which is a pentadiagonal matrix.
		The order of magnitude of the entries of $F$ for $\tau=4$ are shown in Figure \ref{fig:band_ITVOLT}.
		A linear decay is observed in this figure.
		
		\begin{figure}[!ht]
%			\captionsetup{width=.85\linewidth}
			\begin{subfigure}{.49\textwidth}
				\centering
				\setlength\figureheight{.6\textwidth}
				\setlength\figurewidth{.6\textwidth}
%				\captionsetup{width=.85\linewidth}
				\input{figs/ITVOLT_contour_tend=4.tikz}
			\end{subfigure}
%		\hspace{-1cm}
			\begin{subfigure}{.49\textwidth}
				\centering
				\setlength\figureheight{2cm}
				\setlength\figurewidth{.85\textwidth}
%				\captionsetup{width=.85\linewidth}
				\input{figs/diagdecay_ITVOLT.tikz}
			\end{subfigure}
			\caption{Left: the order of magnitude of the entries $f_{k,\ell}$ of $F$, the coefficient matrix of $f(t,s) = \left[-\imath 4(t+1)\right]\Theta(t-s)$. Right: the magnitude of the entries on the first superdiagonal $\vert f_{\ell,\ell+1}\vert$ together with the predicted decay rate $\mathcal{O}(\frac{1}{\ell})$.}
			\label{fig:band_ITVOLT}
		\end{figure}
	\end{example}
	
	
	\subsection{Banded coefficient matrix}\label{sec:coeffMatrix_band}
	A key property of the coefficient matrices of distributions in $\mathcal{A}_{\Theta}$ is that they are numerically banded.
	That is, they can be approximated by a banded matrix for any given threshold, e.g., machine precision.
	A matrix $A$ is said to be an $N$-banded matrix, or, to have bandwidth $N$, if $a_{k,\ell} = 0$, for $\vert k-\ell \vert>N$.
	In this convention a diagonal matrix is a $0$-banded matrix.
	The following corollary follows from Property~\ref{prop:intLeg} and Theorem \ref{theorem:FormulaLegCoeffs}.
	\begin{corollary}[Bandedness of Legendre basis matrix $B^{(d)}$]\label{cor:banded_Bd}
		Consider the coefficient matrix $B^{(d)}$ of $p_d(t)\Theta(t-s)$, where $p_d(t)$ is the orthonormal Legendre polynomial of degree $d$.
		Then $B^{(d)}$ is a $(d+1)$-banded matrix, i.e.,
		\begin{equation*}
			b_{k,\ell}^{(d)} = 0, \quad \text{for } \vert k-\ell\vert >d+1.
		\end{equation*}
	\end{corollary}	
	We would like to truncate the infinite series ${F}:= \sum_{d=0}^{\infty} \alpha_d B^{(d)}$ to a finite series ${F}^{(N)}:= \sum_{d=0}^{N} \alpha_d B^{(d)}$, which is justified if $\hat{F}^{(N)}$ is in some sense close to $F$.
	Closeness will be expressed in terms of the maximum norm $\Vert F-F^{(N)} \Vert_\infty$.
	In order to bound this quantity we state an upper bound on the maximum norm of the Legendre basis matrices $B^{(d)}$ in the following lemma.
  \begin{lemma}\label{lemma:infNormBd}
		Consider the Legendre basis matrix $B^{(d)}$, its maximum norm can be bounded by
  \begin{align*}
        \Vert B^{(d)} \Vert_\infty \leq 3d+2.
  \end{align*}
 \end{lemma}
	The proof of this lemma is technical and lengthy and is, therefore, postponed to Appendix~\ref{app:A}.
    We remark that we have observed, by numerical computations, that the infinity norm $\Vert B^{(d)} \Vert_\infty$ can be bounded by a constant.
	So the bound is too pessimistic. However, it is sufficient to prove the following result.
	\begin{theorem}\label{theorem:bandedMatrix}
		Consider the coefficient matrix $F$ of $\tilde{f}(t)\Theta(t-s) =f(t,s)\in \mathcal{A}_{\Theta}$.
		For any given tolerance $\delta_{\textrm{tol}}>0$ the matrix $F$ can be approximated by an $(N+1)$-banded matrix $F^{(N)}$ satisfying
		\begin{equation*}
			\Vert F-F^{(N)}\Vert_{\infty} \leq \delta_{\textrm{tol}}.
		\end{equation*}
		In other words, $F$ is a numerically banded matrix.
	\end{theorem}
	\begin{proof}
		Let $\tilde{f}(t) = \sum_{d=0}^{\infty} \alpha_d p_d(t)$ be the Legendre series of the function $\tilde{f}(t)$, analytic in the Bernstein ellipse $E_\rho$, $\rho>1$.
		Its coefficient matrix is $F = \sum_{d=0}^{\infty} \alpha_d B^{(d)}$.
		Using the bound~\eqref{eq:coeffDecayRate} and Lemma~\ref{lemma:infNormBd}, we have, for $F^{(N)} := \sum_{d=0}^{N} \alpha_d B^{(d)}$, that
		\begin{equation}\label{eq:infNormF}
			\Vert F-F^{(N)}\Vert_\infty = \left\Vert \sum_{d=N+1}^{\infty} \alpha_d B^{(d)} \right\Vert_\infty \leq \sum_{d=N+1}^{\infty} \vert \alpha_d\vert \Vert B^{(d)} \Vert_\infty \leq \sum_{d=N+1}^{\infty} C \rho^{-d-1}(3d+2).
		\end{equation}
		Therefore, there exists an $N$ for which $\sum_{d=N+1}^{\infty} C \rho^{-d-1}(3d+2)\leq \delta_{\textrm{tol}}$. This proves the statement.
	\end{proof}
	In the proof above, note that the truncated series
	\begin{equation}\label{eq:bandMatrix}
		F^{(N)} = \sum_{d=0}^{N} \alpha_d B^{(d)},
	\end{equation}
	defines an $(N+1)$-banded matrix sufficiently close to $F$ for $N$ large enough.
	The numerical bandedness of $F$ and the bound in Equation \eqref{eq:infNormF} for $F^{(N)}$ are illustrated in the following example.
	\begin{example}\label{example:bandedness}
		Consider the function $\tilde{f}(t) = -\imath \omega \sin(\omega t)$, where $\omega$ controls the oscillation of the function.
		This function is not a polynomial, so we cannot expect a banded coefficient matrix, however, it is numerically banded.
		For $\omega = 1$, Figure \ref{fig:numBand_nu1} shows (left) the norm $\Vert F-F^{(N)}\Vert_\infty$ and the upper bound \eqref{eq:infNormF} for increasing $N$, and (right) the order of magnitude of the entries of $F$, i.e., $\log_{10}(\vert f_{k,\ell}\vert)$.
		We see a clear numerical band structure of the matrix $F$ and that the upper bound holds.
		With the given threshold chosen equal to machine precision $\delta_{\textrm{tol}}=\epsilon_{\textrm{mach}}$, the bandwidth is $N=14$.
		\begin{figure}[!ht]
			\centering
%			\captionsetup{width=.85\linewidth}
			\begin{subfigure}{.49\textwidth}
				\centering
				\setlength\figureheight{2cm}
				\setlength\figurewidth{\textwidth}
%				\captionsetup{width=.85\linewidth}
				\input{figs/maxnorm_DE_m=50nu=1.tikz}
			\end{subfigure}
			\begin{subfigure}{.49\textwidth}
				\centering
				\setlength\figureheight{.6\textwidth}
				\setlength\figurewidth{.6\textwidth}
%				\captionsetup{width=.85\linewidth}
				\input{figs/contour_DE_m=50nu=1.tikz}
			\end{subfigure}
			\caption{Coefficient matrix $F$ of $\left[-\imath \omega \sin(\omega(t+1))\right] \Theta(t-s)$ with $\omega=1$.
				Left: maximum norm $\Vert F-F^{(N)}\Vert_\infty$ (${\color{blue} \ast}$) and the upper bound $\sum_{d=N+1}^{\infty} \vert \alpha_d\vert (3d+2)$ ($\color{red} \circ$).
				Right: order of magnitude of the entries $f_{k,\ell}$.}
			\label{fig:numBand_nu1}
		\end{figure}
		
		For $\omega=5$, Figure \ref{fig:numBand_nu5} shows the same.
		We notice that $\Vert F-F^{(N)}\Vert_\infty$ reaches machine precision at $N= 24$ and again this corresponds to the numerical bandwidth of $F$.
		The bandwidth has increased compared to the less oscillatory function with $\omega=1$, since now we require more Legendre coefficients to represent the function accurately.
		\begin{figure}[!ht]
			\centering
%			\captionsetup{width=.85\linewidth}
			\begin{subfigure}{.49\textwidth}
				\centering
				\setlength\figureheight{2cm}
				\setlength\figurewidth{\textwidth}
%				\captionsetup{width=.85\linewidth}
				\input{figs/maxnorm_DE_m=50nu=5.tikz}
			\end{subfigure}
			\begin{subfigure}{.49\textwidth}
				\centering
				\setlength\figureheight{.6\textwidth}
				\setlength\figurewidth{.6\textwidth}
%				\captionsetup{width=.85\linewidth}
				\input{figs/contour_DE_m=50nu=5.tikz}
			\end{subfigure}
			\caption{Coefficient matrix $F$ of $\left[-\imath \omega \sin(\omega(t+1))\right] \Theta(t-s)$ with $\omega=5$.
				Left: maximum norm $\Vert F-F^{(N)}\Vert_\infty$ (${\color{blue} \ast}$) and the upper bound $\sum_{d=N+1}^{\infty} \vert \alpha_d\vert (3d+2)$ ($\color{red} \circ$).
				Right: order of magnitude of the entries $f_{k,\ell}$.}
			\label{fig:numBand_nu5}
		\end{figure}
	\end{example}
	
	
	\subsection{Well-defined matrix product}\label{sec:completeProof}
	Given the distributions $f, g \in \mathcal{A}_\Theta$ and their coefficient matrices $F, G$, until now, and in particular in Lemma~\ref{lemma:matrixProduct}, we have assumed that the matrix product $FG$ is well-defined.
	Thanks to the properties above, we can show that each element of $FG$ exists, i.e., the matrix product is well-defined.
	Consider the Legendre series 
	\begin{equation*}
		\tilde{f}(t) = \sum_{d=0}^\infty \alpha_d p_d(t), \quad \tilde{g}(t) = \sum_{d=0}^\infty \beta_d p_d(t),
	\end{equation*}
	and the related matrix expansions
	\begin{equation*}
		F = \sum_{d=0}^\infty \alpha_d B^{(d)}, \quad G = \sum_{d=0}^\infty \beta_d B^{(d)}.
	\end{equation*}
	Using Corollary~\ref{cor:banded_Bd} and Lemma \ref{lemma:infNormBd}, for $k,\ell=0, 1, \dots$ we get bounds on the $(k,\ell)$-entry of these coefficient matrices
	\begin{equation*}
		|f_{k,\ell}| \leq \sum_{d=|k-\ell|-1}^\infty |\alpha_d| (3d+2) , \quad |g_{k,\ell}| \leq \sum_{d=|k-\ell|-1}^\infty |\beta_d| (3d+2),
	\end{equation*}
	(by convention, when $k=\ell$, $d$ is set to start from $0$).
	As recalled in Equation \eqref{eq:coeffDecayRate}, $\vert \alpha_d\vert \leq C_f \rho_f^{-d-1}$, with $C_f>0$, $\rho_f>1$.
	Therefore, there exists $K_f>0$ such that
	\begin{align*}
		|f_{k,\ell}| &\leq C_f \sum_{d=|k-\ell|-1}^\infty \rho_f^{-d-1} (3d+2) \leq C_f \rho_f^{-|k-\ell|} \sum_{d=0}^\infty \rho_f^{-d-2} (3d + 3|k-\ell|-1) \leq K_{f} \rho_f^{-|k-\ell|}.
	\end{align*}
	The same applies to $|g_{k,\ell}|$ for some $K_g>0$ and $\rho_g>1$. Note that this shows that $F$ is characterized by an off-diagonal exponential decay.
	As a consequence, there exist constants $\rho > 1$ and $C>0$ such that
	\begin{align*}
		\left|(FG)_{k,\ell}\right| &= \left| \sum_{j=0}^{\infty} F_{k,j} G_{j, \ell}\right| \leq \sum_{j=0}^{\infty} |F_{k,j}| |G_{j, \ell}| \leq \sum_{j=0}^\infty C \rho^{-(|j-k| + |j-\ell|)} < \infty,
	\end{align*}
	proving that, for every $k,\ell = 0, 1, \dots$, $(FG)_{k,\ell}$ is well-defined.
	Hence, Lemma~\ref{lemma:matrixProduct} holds for all the distributions in $\mathcal{A}_{\Theta}$.
	This allows us to state that there is a correspondence between the $\star$-product algebraic structure and the matrix (sub)algebra of Legendre coefficient matrices.
	This is summarized in Table \ref{table:matrixAlgebra}.
	\begin{table}[!ht]
		\centering
%		\captionsetup{width=.85\linewidth}
		\begin{tabular}{l|l}
			$\star$-framework & matrix algebra\\				\hline
			$f(t,s) \star g(t,s)$             &  $F G$ \\
			$f + g$           & $F+G$  \\
			$1_{\star} = \delta(t-s)$ &     $I$       \\
			$R_\star(f)(t,s) = 1_\star +  \sum_{k\geq 1} f^{\star k}(t,s)$   &  $(I-F)^{-1}$
		\end{tabular}
		\caption{The $\star$-operations for distributions $f,g\in \mathcal{A}_{\Theta}$ and the associated matrix algebra operations for the respective coefficient matrices $F,G$.}	
		\label{table:matrixAlgebra}
	\end{table}	

\section{Practical computation in the matrix algebra}\label{sec:practicalComp}
	The second matrix problem in Problem \ref{prob:matrixProblem} is to find the solution $x$ to the infinite system of equations
	\begin{equation*}
		(I-F)x = \phi(-1).
	\end{equation*}
	From Section \ref{sec:coeffMatrix_band} we know that we can accurately represent the infinite numerically banded coefficient matrix $F$ by an infinite banded matrix $F^{(N)}$.
	In Section \ref{sec:field_of_values}, we will discuss the existence of $(I-F^{(N)})^{-1}$.
	That is, the banded infinite system of equations
	\begin{equation*}
		(I-F^{(N)}) x = \phi(-1)
	\end{equation*}
	has a unique solution $x$.
	This solution can be arbitrarily close to the solution of the original system by choosing $N$ appropriately and, therefore, we make a slight abuse of notation by using the same variable $x$ for both these solutions in favor of an easier notation.
 
	To be able to use standard linear algebra techniques we would like to work with a finite system of equations instead of an infinite system.
	In Section \ref{sec:truncErr}, we discuss how an accurate approximation $\dot{x}$ to the first entries of $x$ can be obtained by solving the finite system
	\begin{equation*}
		(I_M - F_M^{(N)}) \dot{x} = \phi_M(-1),
	\end{equation*}
	with $\phi_M(-1)= \begin{bmatrix}
		p_0(-1) & p_1(-1) &	\dots & p_{M-1}(-1)
	\end{bmatrix}^\top$ and  $F_M^{(N)}$ the $M\times M$ leading principal submatrix of $F^{(N)}$.
	In Section \ref{sec:LegCoeffs}, we elaborate on choosing an appropriate value for $M$ and how Legendre coefficients can be obtained from $\dot{x}$.
	Section \ref{sec:fastComp} explores further improvements to compute the solution $\dot{x}$ more efficiently based on exploiting hidden matrix structures.

 	
	\subsection{Resolvent existence and decay phenomenon}\label{sec:field_of_values}
	In this section, we deal with the existence of the resolvent $(I-F^{(N)})^{-1}$. Addressing this problem means discussing the invertibility of an operator. More precisely, consider the Hilbert space $\mathcal{H}$ with orthonormal basis $\{\dot e_0, \dot e_1, \dot e_2, \dots\}$. The coefficients $f_{k,\ell}$ of the (banded) matrix $F^{(N)}$ define the operator $\mathcal{R}: \mathcal{H} \rightarrow \mathcal{H}$ as follows
	\begin{align}\label{eq:operator}
		\mathcal{R} \, \dot e_\ell = \sum_{k=\max\{\ell - N,0\}}^{\ell+N} f_{k,\ell} \, \dot e_k.
	\end{align}
	Denoting by $\mathcal{H}_M$ the linear span of $\{\dot e_0, \dots, \dot e_{M-1} \}$ and with $\mathcal{P}_M : \mathcal{H} \rightarrow \mathcal{H}_M$ the related orthogonal projection, we can define the finite-dimensional operator $\mathcal{R}_M = \mathcal{P}_M \mathcal{R} \mathcal{P}_M$. The operator $\mathcal{R}_M$ is then represented by the matrix $F_M^{(N)}$.
	Theorem 3.1 in \cite{KulRadSar22} shows that the operator $\mathcal{R}$ is invertible under the following conditions:
	\begin{enumerate}  
		\item $F^{(N)}$ is banded;
		\item For every $M=1,2,\dots$, and for $j=1, \dots, (N+1)$, there exist positive constants $K_j, L_j$, such that 
		$$\left\| \left(I_M-F_M^{(N)}\right)^{-1} e_{M-j} \right\|_2 \leq K_j, \quad \left\| \left(I_M-\left(F_M^{(N)}\right)^{H}\right)^{-1} e_{M-j} \right\|_2 \leq L_j.$$
	\end{enumerate}
	In the following, we demonstrate that Condition~2 is satisfied under certain assumptions on the \emph{field of values} of the matrices $F_M^{(N)}$, i.e., the convex set in $\mathbb{C}$ defined as
	$$ W(F_M^{(N)}) := \{ v^H F_M^{(N)} \, v, \|v\|_2 = 1 \}. $$
	Under these assumptions, we show that the matrix $(I_{M}-F_M^{(N)})^{-1}$ is characterized by the so-called \emph{decay phenomenon} (e.g., \cite{BenCetraro16,BenUMI21,BenBoi14}), i.e., the magnitude of its elements decay exponentially as we move away from the band of $F_M^{(N)}$.
	\begin{lemma}\label{lemma:decay}
		Let $A$ be a matrix with bandwidth $N+1$ and so that $W(A)$ is contained in $D(0,r)$, a disk with radius $r<1$ centered at the origin. Then 
		$$ \left| (I-A)^{-1}_{k,\ell} \right| \leq  C \mu^{d(k,\ell)}, \quad d(k,\ell) := \frac{|k-\ell|}{(N+1)}, $$
		for every $r<\mu<1$ and $C$ a constant determined by $\mu$.
	\end{lemma}
	The proof follows immediately from Theorem 2.3 in \cite{PozSim19}; see also \cite{BenBoi14}.
	Assume that $W(F_M^{(N)}) \subset D(0,r)$ for a fixed $r<1$ for $M \geq M_0$, with $M_0$ large enough. Then, for every $\ell=0,1\dots, M-1$, we get
	\begin{align*}
		\left\| \left(I_M-F_M^{(N)}\right)^{-1} e_{\ell} \right\|_2^2 &=
		\sum_{k=0}^M \left| \left(I_M-F_M^{(N)}\right)^{-1}_{k,\ell} \right|^2 \\
		& \leq C^2 \sum_{k=0}^M  \mu^{2 d(k,\ell)} 
		\leq C^2 \sum_{k=0}^M  \tau^{|k-\ell|}, \quad \tau = \mu^{2/(N+1)} < 1, \\
		&\leq C^2 \sum_{k=0}^\infty  \tau^{|k-\ell|} =: K_\ell< \infty.
	\end{align*}
	Note that $K_\ell$ is independent from $M$, proving that Condition~2 above holds (a similar argument holds for the Hermitian transposed case).
	Therefore, by Theorem 3.1 in \cite{KulRadSar22}, we proved the following result.
	\begin{theorem}
		Assume that $W(F_M^{(N)}) \subset D(0,r)$, with $r<1$, for every $M>M_0$, with $M_0$ large enough. Then the operator $\mathcal{R}$ defined in \eqref{eq:operator} is invertible. 
		Moreover, consider the operator equations $\mathcal{R} x = y$ and $\mathcal{R}_M x_M = \mathcal{P}_M y$. If $y$ is in the range of $\mathcal{R}$, then
		$x_M \rightarrow x$ in the norm topology.
	\end{theorem}
	
	As a final step, we need to determine conditions on the function $\Tilde{f}(t)$ so that the coefficient matrix $F_M^{(N)}$ of $\tilde{f}(t)\Theta(t-s)$ satisfies
	$W(F_M^{(N)}) \subset D(0,r)$ for every $M$ large enough.
	Since these matrices are usually characterized by a field of values with a disk shape, we estimate it by using the so-called
	\emph{numerical radius}, which is defined, for a given matrix $A$, as
	$$ \nu(A) := \sup\{|\lambda|, \lambda \in W(A) \}. $$
	Note that $W(A) \subseteq D(0,\nu(A))$.
	Moreover, the numerical radius can also be expressed via the formula
	\begin{equation}
		\nu(A) \leq \max_{k} \left( \sum_{\ell} \frac{|a_{k,\ell}| + |a_{\ell,k}| }{2} \right);
	\end{equation}
	see, e.g., \cite[Corollary 5.2-3]{GusRaoDug97}.
	Unfortunately, obtaining analytic bounds for the numerical radius has proved to be difficult. Therefore, for the moment, we rely on arguments based on numerical observations.
	
	First, consider $B^{(d)}$, the Legendre basis matrix of degree $d$, and the related truncated matrix $B_M^{(d)}$. For $M=2000$ and $d=0,\dots,500$, we observed numerically that
	$$ \max_{k} \left( \sum_{\ell} \frac{|(B_{2000}^{(d)})_{k,\ell}| + |(B_{2000}^{(d)})_{\ell,k}| }{2} \right) \leq 0.87. $$
	Moreover, for all the tested matrices, the maximum was obtained for $k=0$, the first row.
	As the magnitude of the elements of $B_M^{(d)}$ tends to decay along the diagonal (see Lemma \ref{lemma:LegCoeffsDecay}), we can bound the numerical radius by 
	$$\nu(B^{(d)}) \leq 0.87, \quad d=0,\dots,500.$$ 
	
	Finally, by the Legendre expansion of $\tilde{f}(t) = \sum_{d=0}^{\infty} \alpha_d p_d(t)$ we get the bound
	$$ \nu(F^{(N)}) \leq \sum_{d=0}^N |\alpha_d| \nu(B^{(d)}). $$
	For the reasons given above, we conjecture that as long as 
	\begin{equation}\label{eq:f:cond:decay}
		\sum_{d=0}^N |\alpha_d| \leq 1.1494,     
	\end{equation}
	we get the inclusion:
	\begin{equation*}
		W(F_M^{(N)}) \subseteq D(0, 0.87), \quad M=0, 1, \dots  .
	\end{equation*}
	Nevertheless, we have often observed that the solution $x_M$ can still converge even when $\nu(F_M^{(N)})>1$.
	The condition in \eqref{eq:f:cond:decay} is very restrictive, it gives the impression that the techniques presented in this paper are applicable only to slow oscillating, low amplitude functions $\tilde{f}(t)$.
	However, in numerical experiments (Section~\ref{sec:NumExp}) we observe that even when the sum of coefficients is orders of magnitude larger than $1.1494$, or $\nu(F_M^{(N)})>1$, the matrix $(I-F^{(N)}_M)$ is still invertible and its inverse still has an off-diagonal decay.
	Hence, both conditions are nondescriptive in our context, they are too pessimistic.
	More descriptive conditions might~ be obtained by looking at the pseudospectrum of $F^{(N)}$.
	Exploring this path is part of ongoing research.


 
	\subsection{Truncation error}\label{sec:truncErr}
	The finite banded system is obtained by taking the $M\times M$ leading principal submatrix of the matrix $(I-F^{(N)})$:
	\begin{equation}\label{eq:sys_fin}
		(I_M-F^{(N)}_M) \dot{x} = \phi_M(-1).
	\end{equation}
	We analyze the error $\vert \dot{x}-x_M\vert $, where $x_M$ denotes the vector containing the first $M$ entries of the infinite solution $x$.\\
	First, we derive an analytical expression for $x_M$ in terms of submatrices of the infinite matrix $(I-F^{(N)})$.
	Set, for a more compact notation, $A:=(I_M-F^{(N)}_M)\in\mathbb{C}^{M\times M}$, and the matrices $B,C,D$ as in Figure \ref{fig:Schur}.
	In this figure, the colored region contains generic nonzeros, white indicates zeros, and arrows are used to emphasize the size of a block or region.
	Note that $N+2$ is equal to the bandwidth of the matrix plus one.
	\begin{figure}[!ht]
		\centering
%		\captionsetup{width=.85\linewidth}
		\input{figs/SchurComplement.tikz}
		\caption{Block subdivision of an infinite banded matrix $(I-F^{(N)})$, the colored region shows the band of the matrix. The left upper block is $A:=(I_M-F^{(N)}_M)\in\mathbb{C}^{M\times M}$, the dimensions of the other blocks follow immediately from this. Open lines indicate that the row and/or column index goes to infinity.}
		\label{fig:Schur}			
	\end{figure}
	
	This block subdivision allows us to rewrite the infinite system of equations as
	\begin{equation*}
		(I-F^{(N)}) x = \phi(-1) \Leftrightarrow \begin{bmatrix}
			A & B\\
			C & D
		\end{bmatrix} \begin{bmatrix}
			x_M\\
			z
		\end{bmatrix} = \begin{bmatrix}
			\dot{y}\\
			v
		\end{bmatrix},
	\end{equation*}
	with $x_M\in\mathbb{C}^M$, $\dot{y} = \phi_M(-1)\in\mathbb{C}^M$ and $v=\begin{bmatrix}
		p_M(-1) & p_{M+1}(-1) & \dots
	\end{bmatrix}^{\top}$.
	Assume that $A$ and $D$ are invertible, then the first $M$ entries of the solution are
	\begin{equation*}
		x_M =(I_M-A^{-1}BD^{-1}C)^{-1} A^{-1}\dot{y} - (I_M-A^{-1}BD^{-1}C)^{-1}A^{-1}B D^{-1} v.
	\end{equation*}
	The object under study is the error $\vert \dot{x}-x_M\vert$, where $\dot{x} = A^{-1}\dot{y} $, which is given by
	\begin{equation}\label{eq:error}
		\left\vert \left[(I_M-A^{-1}BD^{-1}C)^{-1} - I_M\right] \dot{x} - (I_M-A^{-1}BD^{-1}C)^{-1}A^{-1} B D^{-1} v\right\vert.
	\end{equation}
	To study this error we look at the matrix structures of the matrices appearing in Equation \eqref{eq:error}.
	Assume that $A^{-1}=(I_M-F^{(N)}_M)^{-1}$ is a numerically banded matrix, see Section \ref{sec:field_of_values}.
	In the following, matrix entries with magnitude below a given threshold are  truncated. As a consequence, the matrix $A^{-1}$ is a $K$-banded matrix.
	Since $W(D)\subseteq W(I-F^{(N)})$, if $F^{(N)}$ shows a decay then, by Lemma \ref{lemma:decay}, $D^{-1}$ also shows a decay and can be approximated accurately by an $L$-banded matrix.
	The values $K$ and $L$ can be estimated a priori by using spectral information of $F^{(N)}$ using, e.g., Lemma~\ref{lemma:decay}.
	In Figure \ref{fig:ABD}, the structure of the matrix products $A^{-1}BD^{-1}$ and $A^{-1}B D^{-1}C$ appearing in \eqref{eq:error} is derived.
	\begin{figure}[!ht]
%		\captionsetup{width=.85\linewidth}
		\begin{subfigure}{\textwidth}
			\centering
		\input{figs/BD.tikz}
			\caption{Structure of $BD^{-1}$.}
		\end{subfigure}
		\begin{subfigure}{\textwidth}
			\centering
		\input{figs/ABD.tikz}
			\caption{Structure of $A^{-1}BD^{-1}$.}
		\end{subfigure}
		\begin{subfigure}{\textwidth}
			\centering
		\input{figs/ABDC2.tikz}
			\caption{Structure of $A^{-1}BD^{-1}C$.}
		\end{subfigure}
		\caption{Structure of the matrices appearing in the error analysis of $\dot{x}$. Colored regions indicate generic nonzeros and dashed lines indicate the boundary of the matrix, the lack of a dashed line indicates that the row and/or column index goes to infinity.}
		\label{fig:ABD}
	\end{figure}
	
	
	The structure of the error is now easily determined from the structure of these matrices. 
	Namely, plug the matrices into the formula $\vert x_M-\dot{x}\vert$ and we obtain that (at least) the first $M-N-K-2$ entries are computed accurately, see Figure~\ref{fig:vectorError}.
	\begin{figure}[!ht]
		\centering
%		\captionsetup{width=.85\linewidth}
	\input{figs/structure_xdotError.tikz}
%			\includegraphics{tikz/TextInTemplate2-figure12.pdf}
		\caption{Structure of the truncation error $\vert x_M - \dot{x} \vert =\vert \left[(I-A^{-1}BD^{-1}C)^{-1} - I \right] \dot{x} - (I-A^{-1}B D^{-1} C)^{-1} A^{-1} B D^{-1} v \vert$.
			Colored regions indicate generic nonzeros and dashed lines indicate the boundary of the matrix, the lack of a dashed line indicates that the row and/or column index goes to infinity.}
		\label{fig:vectorError}			
	\end{figure}		
	In practice, thanks to the decay phenomenon, more than $M-N-K-2$ entries might be computed accurately.
	This is illustrated in Example \ref{example:error}.
	\begin{example}\label{example:error}
		Consider $\tilde{f}(t) = -\imath \sin(t+1)$.
	 For $M=50$, Figure \ref{fig:matrixStructure_contours} shows the corresponding matrix $(I_M-F_M)$, its inverse and the $50\times 50$ leading principal submatrix of $D^{-1}$.
	All these matrices are numerically banded and for $\delta_{\textrm{tol}} = \epsilon_{\textrm{mach}}$ they have the bandwidth $N+2=16$, $K=22$ and $L=16$, respectively.
		\begin{figure}[!ht]
			\centering
			\setlength\figureheight{.28\textwidth}
			\setlength\figurewidth{.28\textwidth}
%			\captionsetup{width=.82\linewidth}
	\input{figs/Structure_contour_DE_m=100nu=1.tikz}			
%			\includegraphics{tikz/TextInTemplate2-figure13.pdf}
			\caption{Order of magnitude of the entries of the matrices $(I_M-F_M)$, $(I_M-F_M)^{-1}$ and $D^{-1}$, respectively, for the function $\tilde{f}(t) = -\imath \sin(t+1)$ and $M=50$. On the far right the colorbar indicates the colors corresponding to the orders of magnitude.}	
			\label{fig:matrixStructure_contours}	
		\end{figure}
		
		From the above error analysis, we expect that the first $M-N-K-2$ entries of $\dot{x}$ are close to those of the exact (infinite) solution $x$.
		In our example, this would be $50-14-22-2 = 12$ entries.
		The predicted structure in Figure \ref{fig:vectorError} is verified by computing these matrices numerically.
		This is shown in Figure \ref{fig:truncError}, where we have chosen to set elements smaller than $10^{-19}$ to zero (i.e., the color white in the colobar).
		The observed matrices adhere to the predicted structure, but thanks to the decay of the entries it does not fill the whole predicted submatrix with large elements.
		As a consequence, more than the predicted 12 entries of $\dot{x}$ are computed accurately; we observe that 30 entries are computed up to machine precision.
		\begin{figure}[!ht]
			\centering
			\setlength\figureheight{.28\textwidth}
			\setlength\figurewidth{.28\textwidth}
%			\captionsetup{width=.85\linewidth}
		\input{figs/truncError.tikz}
%			\includegraphics{tikz/TextInTemplate2-figure14.pdf}
			\caption{Order of magnitude of the entries of the matrices $(I-A^{-1}BD^{-1}C)^{-1} -I$ and $(I-A^{-1}BD^{-1})^{-1} A^{-1}B D^{-1}$ and of the error vector $\vert x_M-\dot{x}\vert$ for the function $\tilde{f}(t) = -\imath \sin(t+1)$, from left to right, respectively. On the far right the colorbar indicates the colors corresponding to the orders of magnitude.
				Dashed red lines indicate the predicted structure as shown in Figure \ref{fig:vectorError}.}	
			\label{fig:truncError}	
		\end{figure}		
	\end{example}
	
	
	The above example numerically validates the error analysis in this section.
	Thus, the truncation to the leading principal submatrix is both theoretically and numerically justified.
	We would like to stress that the inverse of $A=(I_M-F_M)$ is not computed explicitly in our proposed procedure, instead the system of equations $(I_M-F_M^{(N)})\dot{x} = \phi_M(-1)$ is solved to obtain $\dot{x}$.
	Once $\dot{x}$ is available, the approximate Legendre coefficients are obtained as $\dot{c} = T_M \dot{x}$.
	Since $T_M$ is a tridiagonal matrix, it follows immediately that at least $M-N-K-3$ coefficients of $\dot{c}$ are computed accurately.

 
	
 
	\subsection{Finding the accurate Legendre coefficients}\label{sec:LegCoeffs}
	From the analysis in the above section two questions arise
	\begin{enumerate}
		\item What is an optimal choice for $M$, large enough to accurately compute a sufficient amount of Legendre coefficients and as small as possible, to reduce computational cost?
		\item After computing the Legendre coefficients $\dot{c}$, how many should we keep?
	\end{enumerate}

	The first question is the most challenging and might not have a precise answer.
	We sketch two possible strategies.
	The first strategy can be based on Lemma~\ref{lemma:decay}, which makes it possible to obtain an estimate for the bandwidth $K$ and thus to obtain an estimate for $M-N-K-3$, i.e., the number of accurately computed Legendre coefficients for a system of size $M$.
	For a good estimation of $K$ we require spectral information of $F^{(N)}$, but the bound from Section \ref{sec:field_of_values} is too pessimistic for practical use.
	Moreover, it is difficult to predict how many Legendre coefficients would suffice to accurately represent the unknown solution $\tilde{u}(t)$.\\
	The second strategy follows the strategy employed in chebfun \cite{AuTr17} and it exploits the bound in Equation~\eqref{eq:boundLeg}.
	The idea behind this approach is to first compute the Legendre coefficients for some chosen $M$, and check whether the Legendre coefficients have reached a given tolerance, if not, try $2M$ and repeat until the tolerance is reached.
	The reason such an approach works well for chebfun is because computing the Chebyshev coefficients can be done at a computational complexity of $\mathcal{O}(N\log N)$.
	Thus, in order to make such a method feasible for our procedure we must have an efficient algorithm to solve Problem~\ref{prob:matrixProblem}.
	The next section proposes some directions which can lead to an efficient procedure to compute the Legendre coefficients. \\
	In the sequel we assume that $M$ is chosen appropriately.
	
	The second question is answered by a particular truncation combined with a numerical method that chooses the right amount of coefficients automatically.
	We describe the truncation using an example.
	Consider the function $\tilde{f}(t)= -\imath \omega \sin(\omega(t+1))$, for which we approximate the solution to the ODE \eqref{eq:ODE_univar}, that is $\tilde{u}(t)=\exp\left(-\imath(1-\cos(\omega t + \omega))\right)$.
	For $\omega=1$ this function is the one considered in Example~\ref{example:error}, from which we know that for $M=50$ we compute about $30$ Legendre coefficients up to machine precision, which suffices to represent $\tilde{u}(t)$.
	In Figure \ref{fig:coeffs_DE}, we show the Legendre coefficients obtained in the following two ways:
	\begin{enumerate}
		\item Solve $(I-F_M^{(N)}) \dot{x}=\phi_M(-1)$ and compute $\dot{c} = T_M \dot{x}$.
		\item Solve $(I-\underline{F}_M^{(N)})\underline{\dot{x}}=\phi_M(-1)$ and compute $\underline{\dot{c}} = \underline{T}_M \underline{\dot{x}}$, where the entries of the coefficient matrices are set to zero in the last $K$ rows, where $K$ is the bandwidth of the matrix.
		This means that the first $M-N-1$ rows of $\underline{F}_M^{(N)}$ equal those of $F_M^{(N)}$ and the last $N+1$ rows are all zeros.
		Since $T_M$ has bandwidth equal to one, omitting its last row gives us $\underline{T}_M$.
	\end{enumerate}
	For $\omega=1$ and $M=50$, the Legendre coefficients $\dot{c}$ without truncation lead to the first 30 coefficients being accurately computed and after this the coefficients increase in amplitude, resulting in a \textit{u}-shaped curve for the coefficients.
	This \textit{u}-shape does not correspond to the actual Legendre coefficients and it makes it more difficult to determine how many coefficients one would keep to obtain an accurate approximation to $\tilde{u}(t)$.
	Because, using more than 30 coefficients will cause the approximation to deteriorate.
	
	The Legendre coefficients $\underline{\dot{c}}$ with truncation of the coefficient matrices is equally accurate as $\dot{c}$ for the first $30$ coefficients and does not have an increase after this, the truncation in fact pushes these last coefficients to zero.
	Thus $\underline{\dot{c}}$ is easier to use, since it has a simpler shape allowing chopping of the series by, e.g., the procedure in \cite{AuTr17}, moreover, using more coefficients than 30 will not deteriorate the approximation to $\tilde{u}(t)$.
	
	\begin{figure}[!ht]
%		\captionsetup{width=.85\linewidth}
		\begin{subfigure}{\textwidth}
			\centering
			\setlength\figureheight{1.5cm}
			\setlength\figurewidth{11cm}
			\input{figs/coeffs_DE_M=50nu=1.tikz}
%			\includegraphics{tikz/TextInTemplate2-figure15.pdf}
			\vspace{-0.2cm}
			\caption{$\omega=1$ and $M=50$}
		\end{subfigure}
		\begin{subfigure}{\textwidth}
			\centering
			\setlength\figureheight{1.5cm}
			\setlength\figurewidth{11cm}
			\input{figs/coeffs_DE_M=100nu=5.tikz}
%			\includegraphics{tikz/TextInTemplate2-figure16.pdf}
			\vspace{-0.2cm}
			\caption{$\omega=5$ and $M=100$}
		\end{subfigure}
		\begin{subfigure}{\textwidth}
			\centering
			\setlength\figureheight{1.5cm}
			\setlength\figurewidth{11cm}
			\input{figs/coeffs_DE_M=50nu=5.tikz}
%			\includegraphics{tikz/TextInTemplate2-figure17.pdf}
			\vspace{-0.2cm}
			\caption{$\omega=5$ and $M=50$}
		\end{subfigure}
		\caption{Legendre coefficients obtained by system solve for the coefficient matrix $F^{(N)}_M$ for $\tilde{f}(t) = -\imath \omega \sin(\omega (t+1))$. Left: Legendre coefficients, exact $c$ (${\color{blue} \ast}$), approximation $\dot{c}$ (${\color{red} \circ}$), approximation with truncation $\underline{\dot{c}}$ (${\color{green} \triangle}$).
			Right: the error on the computed coefficients, $\vert c-\dot{c}\vert$ ({\color{red}$\circ$}) and $\vert c - \underline{\dot{c}}\vert$ ({\color{green} $\triangle$}).}
		\label{fig:coeffs_DE}
	\end{figure}

	In Figure \ref{fig:coeffs_DE}, we also show the Legendre coefficients and their error for two other choices of parameters.
	For $\omega=5$ and $M = 100$, we observe similar behavior as for $\omega=1$.	
	For $\omega=5$ and $M = 50$, $M$ is chosen smaller than the minimal required size to represent $\tilde{u}(t)$ up to machine precision.
	Now the first coefficients in $\underline{\dot{c}}$ are computed less accurately than $\dot{c}$ due to the truncation.
	This might be because the last $N=24$ rows of the coefficient matrix $F^{(N)}_M$ of size $M=60$ are truncated, thereby discarding too many entries.
	
		
	
	\subsection{Fast computation of Fourier coefficients}\label{sec:fastComp}
	The first subproblem in Problem \ref{prob:matrixProblem} is to construct the coefficient matrix $F$.
	We have shown that in fact it suffices to approximate $F$ by the finite banded matrix
	\begin{equation*}
		F_M^{(N)}=\sum_{d=0}^{N} \alpha_d B^{(d)}_M.
	\end{equation*}
	The efficient construction of the coefficient matrix thus requires efficient computation of $\{\alpha_d\}_{d\geq 0}$ and $\{B^{(d)}_M\}_{d\geq 0}$.
	Since $B_M^{(d)}$ are the coefficient matrices of the basis, they must be computed only once for each $d$ and can then be reused for different functions.\\
	First an efficient algorithm to approximate the Legendre coefficients $\{\alpha_d\}_{d=0}^{N}$ of $\tilde{f}(t)$ is discussed.
	We use functions that are available in the MATLAB package \verb|chebfun| \cite{DrHaTr14} .
	The Legendre coefficients for the smooth function $\tilde{f}(t)$ are given by $\alpha_d= \int_{-1}^{1} \tilde{f}(t) p_d(t)$ and will be approximated by $\{\hat{\alpha}_d\}_{d=0}^N$ as follows:
	\begin{enumerate}
		\item Using \verb|chebfun|, we compute the coefficients of the interpolating Chebyshev series $\sum_{k=0}^N \hat{c}_d T_d(t) \approx \tilde{f}(t)$. Given a required accuracy, an appropriate truncation value $N$ is chosen automatically \cite{AuTr17}. 
		The coefficients $\{\hat{c}_d\}_{d=0}^N$ are obtained at complexity $\mathcal{O}(N \log(N))$.
		For details on the error incurred by interpolation, instead of by computing the integral $c_d = \int_{-1}^{1} \frac{\tilde{f}(t) T_d(t)}{\sqrt{1-t^2}}dt$, we refer to the book by Trefethen \cite[Chapter 4]{Tr13}.
		\item The Chebyshev coefficients $\{\hat{c}_d\}_{d=0}^N$ can be transformed into Legendre coefficients $\{\hat{\alpha}_d\}_{d=0}^N$ at complexity $\mathcal{O}(N \log^2(N))$ by using the method proposed by Townsend, et al. \cite{ToWeOl18}.
		In \verb|chebfun|, this method is available under the name \verb|cheb2leg|.
		This transformation from Chebyshev to Legendre coefficients is expected to have a worst case error growth of $\mathcal{O}(\sqrt{N}\log(N))$, and for a fast decaying set of coefficients $\{\hat{c}_d\}_{d=0}^{N}$ Townsend and collaborators have observed numerically that there is no error growth with $N$.
	\end{enumerate}
	Thus, at an overall complexity of $\mathcal{O}(N\log^2 (N))$ we are able to compute the coefficients $\{\hat{\alpha}_d\}_{d=0}^N$ representing $\tilde{f}(t)$ in the Legendre basis.
	The coefficient matrix $F^{(N)}_M = \sum_{d=0}^{N} \alpha_d B^{(d)}_M $ can be accurately approximated by $\hat{F}_{M}^{(N)} = \sum_{d=0}^N \hat{\alpha}_d B^{(d)}_M$.
	
	Second, we want to compute and store the Legendre basis matrices $B^{(d)}_M$ efficiently.
	Equation~\eqref{eq:LegBasisCoeffs} provides an expression for the entries $b_{k,\ell}^{(d)}$ in terms of integrals of the triple product of Legendre polynomials $\mathcal{F}_{a,b,c}$.
	Our approach is based on expressing the matrix $\left[\mathcal{F}_{a,b,c}\right]_{b,c=0}^{M-1}$ as
	\begin{equation*}
		\left[\mathcal{F}_{a,b,c}\right]_{b,c=0}^{M-1} = \sqrt{2a+1} \left(C_M\circ H_M\circ T_M\right),
	\end{equation*}
	with $C_M = \left[\sqrt{(2b+1)}\sqrt{(2c+1)}\right]_{b,c=0}^{M-1}$, $H_M$ a Hankel matrix and $T_M$ a Toeplitz matrix.
	The Hankel matrix depends on the value $\gamma := b+c$ and is characterized completely by its last row $r_H^\top$ and first column $c_H$.
	Let us define a function that generates the entries in $H_M$,
	\begin{equation*}
		h(a,\gamma) :=  \frac{1}{(a+\gamma+1)} \left(\prod_{j=1}^{a}\frac{-a+\gamma+2j}{-d+\gamma+2j-1}\right),
	\end{equation*}
	then, for $(m+a)$ even, the last row and first column are:
	\begin{align*}
		r_H^\top &= \begin{bmatrix}
			h(a,m) & 0 & h(a,m+2) & 0 & \dots & h(a,2m-2) & 0 & h(a,2m) 
		\end{bmatrix}^\top,\\
		c_H &= \begin{bmatrix}
			\smash[b]{\block{a}} & h(a,a) & 0 & h(a,a+2) & 0 & \dots & h(a,m-2) & 0 & h(a,m) 
		\end{bmatrix}^\top \\
		&\quad
	\end{align*}
	and, for $(m+a)$ odd:
	\begin{align*}
		r_H^\top &= \begin{bmatrix}
			h(a,m) & 0 & h(a,m+2) & 0 & \dots & 0 & h(a,2m-1) & 0 
		\end{bmatrix}^\top,\\
		c_H &= \begin{bmatrix}
			\smash[b]{\block{a}} & h(a,a) & 0 & h(a,a+2) & 0 & \dots & 0 & h(a,m-1) & 0 
		\end{bmatrix}^\top.\\
		&\quad
	\end{align*}
	The Toeplitz matrix depends on $\alpha := \vert b-c\vert$ and since the Toeplitz matrix $T_M$ is symmetric, it is characterized its first column $c_T$.
	Using the following function generating the entries of $T_M$,
	\begin{equation*}
		t(a,\alpha) := \frac{1}{2^{(2a+1/2)}} \frac{\prod_{j=(\frac{a+\alpha}{2}+1)}^{a+\alpha}j^2}{\prod_{j=1}^{\frac{a-\alpha}{2}}j^2 \prod_{j=(a-\alpha+1)}^{a+\alpha} j},
	\end{equation*}
	the first column of the Toeplitz matrix is given, for $a$ odd, by:
	\begin{equation*}
		c_T =\begin{bmatrix}
			t(a,0) & 0 & t(a,2) & 0 & \dots & t(a,a)  &	\smash[b]{\block{m-a}}\\
		\end{bmatrix}^\top
	\end{equation*}
	and, for $a$ even, by:
	\begin{align*}
		c_T = \begin{bmatrix}
			0 & t(a,1) & 0 & t(a,3) & \dots & 0 & t(a,a)  & 	\smash[b]{\block{m-a}}\\
		\end{bmatrix}^\top.\\
		\quad
	\end{align*}
	Using this expression, the matrices $\left[\mathcal{F}_{d,b,c}\right]_{b,c=0}^{M-1}$ for $d=0,1,\dots,N$ can be stored by $3(N+1)M+M^2$ numbers, instead of $(N+1)M^2$ if each matrix is stored naively.
	Further reduction of memory cost can be obtained by exploiting the zero structure of the Hankel and Toeplitz matrix.	\\
	The Legendre basis matrix of degree $d$ can now be written as
	\begin{equation}\label{eq:HadBasisMatrix}
		B_M^{(d)} = \left[b_{k,l}^{(d)}\right]_{k,l=0}^{M-1} = \sqrt{2d+1} \left(\tilde{C}_M\circ \left((\underline{H}_M\circ \underline{T}_M) Z_M\right)\right),
	\end{equation}
	where $\tilde{C}_M := \left[\frac{\sqrt{2k+1}}{\sqrt{2\ell+1}}\right]_{k,\ell=0}^{M-1}\in\mathbb{R}^{M\times M}$, $\underline{H}_M$ and $\underline{T}_M$ are, respectively, $H_{M+1}$ and $T_{M+1}$ with the last row removed and
	\begin{align*}
		Z_M &:= \begin{bmatrix}
			1 & -1 & \\
			1 & 0 & -1 &\\
			& 1 & 0 & -1 & &  \\
			&  & \ddots & \ddots & \ddots & \\
			& & & 1 & 0 & -1 \\
			&  & &  & 1 & 0\\
			& &  & & &  1
		\end{bmatrix}\in\mathbb{R}^{(M+1)\times M}.
	\end{align*}	
	The representation of the Legendre basis matrices $B_M^{(d)}$ given by Equation \eqref{eq:HadBasisMatrix} creates possibilities for the development of efficient methods for storing and computing the coefficient matrix $\hat{F}^{(N)}_M$; see, for example, the procedure proposed in \cite{ToWeOl18} for the positive semidefinite case.
	Exploring and implementing efficient memory and computational schemes further is subject of ongoing research and is out of the scope of this paper.

 
	
	\section{Proposed numerical procedure}\label{sec:NumExp}
	Using the presented results, we can replace the infinite matrix problem formulated in Problem \ref{prob:matrixProblem} by the following finite matrix problem.
	\begin{problem}[Finite matrix problem]\label{prob:matrixProblem_finite}
		Given a smooth function $\tilde{f}(t)$ and a tolerance $\delta_{\textrm{sol}}$, compute the approximate Legendre coefficients $\{\hat{c}_k\}_{k=0}^{\hat{M}}$ of the solution $\tilde{u}(t)$ to the ODE \eqref{eq:ODE_univar} such that they satisfy $\Vert \tilde{u}(t) - \sum_{d=0}^{\hat{M}}\hat{c}_d p_d(t)\Vert_{\infty} \lesssim  \delta_{\textrm{sol}}$ on the interval $t\in\left[-1,1\right]$.
		This corresponds to solving five subproblems:
		\begin{enumerate}
			\item Compute the interpolating Legendre coefficients $\{\hat{\alpha}_k\}_{k=0}^{N}$ of $\tilde{f}(t)$ for an appropriate value of $N$.
			\item Determine an appropriate value for the size $M$ of the coefficient matrix $F$ such that enough Legendre coefficients, $\hat{M}$, are computed accurately in order to reach the given tolerance, see Section \ref{sec:truncErr}.
			\item Construct the finite banded coefficient matrix $\hat{F}^{(N)}_M=\sum_{d=0}^{N} \hat{\alpha}_d B^{(d)}_M$.
			\item Solve the finite linear system of equations $(I_M-\hat{F}^{(N)}_M)\dot{x} = \phi_M(-1)$ for $\dot{x}$. The right hand side is the column vector $\phi_M(-1) = \begin{bmatrix}
				p_k(-1)
			\end{bmatrix}_{k=0}^{M-1}$ and $I_M$ is the identity matrix.
			\item Compute the finite matrix vector product $T_M \dot{x}=\hat{c}$, thereby obtaining the approximate Legendre coefficients $\{\hat{c}_k\}_{k=0}^{M}$.
		\end{enumerate}
	\end{problem}
	The procedure developed in this paper proposes to solve the subproblems in Problem \ref{prob:matrixProblem_finite} as follows:
	\begin{enumerate}
		\item Using \verb|chebfun|, the coefficients $\{\hat{\alpha}_k\}_{k=0}^{N}$ can be computed at complexity $\mathcal{O}(N\log^2(N))$, as described in Section \ref{sec:fastComp}. Moreover, for a given tolerance the value for $N$ is chosen automatically.
		\item This remains an open problem for which two possible strategies are proposed in Section \ref{sec:LegCoeffs}.
		\item Compute the sum $\hat{F}^{(N)}_M=\sum_{d=0}^{N} \hat{\alpha}_d B^{(d)}_M$. An analytical formula for the entries of $B^{(d)}_M$ is stated in Property \ref{prop:intLeg}.
		Equation \eqref{eq:HadBasisMatrix} provides a formulation for the construction of $B^{(d)}_M$ that is more memory and computationally efficient.
		\item Solve the finite system of equations $(I_M-\underline{\hat{F}}^{(N)}_M)\underline{\dot{x}} = \phi_M(-1)$, where $\underline{F}^{(N)}_M$ equals $F^{(N)}_M$ with its last $N+1$ rows set equal to zero. See Section \ref{sec:truncErr} and Section \ref{sec:LegCoeffs} for details.
		The system is solved in MATLAB by the \verb|backslash| function.
		\item Form the product $\underline{T}_{M}\underline{\dot{x}}=\hat{c}$, where $\underline{T}_M$ equals $T_M$ with its last row set to zero. 
		If requested, the Legendre series can be chopped by applying the procedure proposed by Aurentz and Trefethen \cite{AuTr17}.
		See also Section \ref{sec:LegCoeffs}.
	\end{enumerate}
	In the following, we present numerical experiments which will confirm the validity of this procedure.
	The invertibility of $(I_M-F_M^{(N)})$ is studied by its spectral properties.
	We report the numerical radius $\nu(F_M^{(N)})$ and the upper bound \eqref{eq:f:cond:decay}.
	However, these quantities are not descriptive of the observed numerical behavior, therefore we also report the pseudospectra of $(I_M-F_M^{(N)})$ which might provide a better description.
	We denote by $\sigma(A)$ the spectrum of $A$, then, for $\epsilon>0$, the $\epsilon$-pseudospectrum of $(I_M-F_M^{(N)})$ is defined by
	\begin{equation*}
		\sigma_\epsilon(I_M-F_M^{(N)}) = \{z\in\mathbb{C}\vert z\in\sigma(I_M-F_M^{(N)}+E) \text{ for some }E \text{ with }\Vert E\Vert \leq \epsilon \}.
	\end{equation*}
	See, for example, the book by Trefethen and Embree \cite{TrEm05} for details.
	
	Since, for the scalar ODEs the analytical solution $\tilde{u}(t)$ is available, we can compute the error of our approximation $\hat{u}(t):=\sum_{d=0}^M \hat{c}_d p_d(t)$. 
	This error is expressed in the maximum norm 
	\begin{equation*}
		\textrm{err}_\textrm{f} := \frac{\Vert \tilde{u}(t) - \hat{u}(t)\Vert_\infty}{\Vert \tilde{u}(t)\Vert_\infty}.
	\end{equation*}
	This error is estimated by evaluating the solution and approximation in $10M$ equidistant nodes in $t\in \left[-1,1\right]$.
	Denote by $c$ the vector of the first $M$ Legendre coefficients of $\tilde{u}(t)$, then the error on the computed Legendre coefficients $\hat{c}$ is quantified by
	\begin{equation*}
		\textrm{err}_c = \frac{\vert c-\hat{c}\vert }{\Vert c\Vert_\infty}.
	\end{equation*}

	Timings are not performed since in this paper we focus on the validity and accuracy of the procedure.
	For the scalar case studied here we do not expect to outperform state-of-the-art methods.
	However, thanks to the straightforward generalization of our procedure to the matrix case we aim to develop a procedure for the matrix ODE competitive with the state-of-the-art.
	Understanding the applicability and accuracy of the scalar case is a fundamental step towards developing a competitive procedure for the matrix case.	
	
	\subsection{Toy problem}
	The following function is constructed so that we have control over its behavior
	\begin{equation*}
		\tilde{f}(t) = -\imath \frac{\omega}{\beta} \sin(\omega (t+1)).
	\end{equation*}
	Parameter $\omega$ controls the oscillation of the function and $\beta$ controls its amplitude.
	The solution to the ODE
	\begin{equation*}
		\frac{d}{dt} \tilde{u}(t) = -\imath \frac{\omega}{\beta} \sin(\omega (t+1)) \tilde{u}(t), \quad \tilde{u}(-1) = 1,\quad \text{on } t\in \left[-1,1\right]
	\end{equation*}
	is
	\begin{equation*}
		\tilde{u}(t) = \exp\left(-\frac{\imath}{\beta} (1-\cos(\omega t+\omega))\right).
	\end{equation*}
	We report results for three different choices of parameters.
	The first choice is $\omega=5$, $\beta=10$ and we take $M=100$.
	The condition \eqref{eq:f:cond:decay} is satisfied, namely $\sum_{d=0}^N \vert \alpha_d\vert = 1.0909$ and the numerical radius is $\nu(F_{100}) = 0.2151$.
	Thus, the matrix $(I_M-F_M^{(N)})$ is nonsingular and subproblem 4 in Problem \ref{prob:matrixProblem_finite} has a unique solution.
	The approximation has accurate Legendre coefficients, $\max(\textrm{err}_c) =  1.7828\textrm{e}-15$, and a function error of $\textrm{err}_\textrm{f} = 1.3345e-15$.\\
	The second choice is $\omega=5$, $\beta=1$ for $M=100$.
	Condition \eqref{eq:f:cond:decay} is not satisfied, $\sum_{d=0}^{N} \vert \alpha_d \vert = 10.909$ and the numerical radius of the coefficient matrix is $\nu(F_{100}^{(24)}) = 2.151$, thus the existence of $(I_{100}-F_{100}^{(24)})^{-1}$ cannot be guaranteed by looking at these quantities.
	Nevertheless, it exists and the approximation obtained is accurate, $\textrm{err}_\textrm{f} = 1.8621\textrm{e}-15$ and $\max(\textrm{err}_c) =  2.5823\textrm{e}-15$.\\
	As a final choice we take a more oscillatory function $\nu=100$ and $\beta=1$ and compute an approximation for $M=1500$.
	The numerical radius $\nu(F^{(148)}_{1500}) = 45.11$ is much smaller than the upper bound $\sum_{d=0}^{N}\vert \alpha_d \vert = 796.7$.
	The approximation is accurate, $\textrm{err}_\textrm{f} = 9.9812\mathrm{e}-14$ and $\max(\textrm{err}_c) =  3.6107\mathrm{e}-14$.
	The numerical radius for the second and third choice of parameters does not guarantee the existence of $(I_{M}-F_{M}^{(N)})^{-1}$.
	Therefore, in Figure \ref{fig:pseudoSpectrum_DE}, we show the pseudospectra for several levels for $(I_M-F_M^{(N)})$ for these choices with $M=1500$.
	They indicate that even for perturbations $E$ that are relatively large in norm (up to $10^{-5}$) the spectrum of $(I_{1500}-F_{1500}^{(N)}+E)$, for both functions, remains contained in a disk centered at $(1,0)$ with a radius equal to one.
	\begin{figure}[!ht]
		\centering
		\setlength\figureheight{4.5cm}
		\setlength\figurewidth{5.4cm}
%		\captionsetup{width=.85\linewidth}
		\input{figs/Spectrum_DE_M=1500nu=100beta=1__M=1500nu=5beta=1.tikz}
%			\includegraphics{tikz/TextInTemplate2-figure16.pdf}
		\caption{Spectrum ({\color{black} $\cdot$}) and pseudospectra of $(I-F_M)$ for the coefficient matrix $F_M$ of $\tilde{f}(t) = -\imath \frac{\nu}{\beta} \sin(\nu (t+1))$.
			Left: $\nu=5$, $\beta=1$ and $M=1500$. Right: $\nu=100$, $\beta=1$ and $M=1500$.}
		\label{fig:pseudoSpectrum_DE}
	\end{figure}	
	
	\subsection{A polynomial problem}
	Consider the following ODE, which appeared, e.g., in \cite{ScGhSc22},
	\begin{equation*}
		\frac{d}{d\tau} \tilde{u}(\tau) = -\imath \tau \tilde{u}(\tau), \quad  \tilde{u}(0) = 1, \quad \text{on }\tau\in\left[0,\tau_{\textrm{end}} \right]
	\end{equation*}
	the function $\tilde{f}(\tau) = -\imath \tau$ is a degree one polynomial and the solution is $\tilde{u}(\tau) = \exp(-\imath \tau^2)$.
	Since the Legendre polynomials are defined on $\left[-1,1\right]$, we perform a transformation on the ODE, mapping $\tau\in\left[0,\tau_{\textrm{end}}\right]$ onto $t\in \left[-1,1\right]$.
	This transformation is $t = \frac{2\tau}{\tau_{\textrm{end}}}-1$ and its inverse $\tau = (t+1)\frac{\tau_{\textrm{end}}}{2}$, and leads to
	\begin{equation*}
		\frac{d}{d t} \tilde{u}(t) = -\imath \left(\frac{\tau_{\textrm{end}}}{2}\right)^2 (t+1) \tilde{u}(t) ,
	\end{equation*}
	with solution $\tilde{u}(t) = \exp\left(-\frac{\imath}{2} \left(\frac{\tau_{\textrm{end}}}{2}\right)^2 (t+1) \right)$.
	The coefficient matrix $F$ of $f(t,s)=\tilde{f}(t)\Theta(t-s)$ with $\tilde{f}(t) = -\imath \left(\frac{\tau_{\textrm{end}}}{2}\right)^2 (t+1)$ is pentadiagonal and is discussed in Example \ref{example:decay}.\\
	For a fixed size of the coefficient matrix, $M=1000$, we run our procedure for $\tau_{\textrm{end}} = 25$ and $\tau_{\textrm{end}} = 50$.
	Table \ref{table:ITVOLT} shows the metrics for these cases; a good approximation is obtained in both cases even though the numerical radius is much larger than one.
	
	\begin{table}[ht]
		\centering
%		\captionsetup{width=.85\linewidth}
		\begin{tabular}{l|llll}
			$\tau_{\textrm{end}}$ & $\vert \alpha_0\vert+\vert \alpha_1\vert$ & $\nu(F_{1000}^{(1)})$ & $\mathrm{err}_c$      & $\textrm{err}_\textrm{f}$ \\ \hline
			25                    & 348.5                               & 147.6                & $5.228\mathrm{e}-14$  & $1.067\mathrm{e}-13$      \\
			50                    & 1394                                & 590.6                & $3.210\mathrm{e}-13$ & $3.008\mathrm{e}-13$    
		\end{tabular}
	\caption{Metrics for the function $\tilde{f}(t)=-\imath \left(\frac{\tau_{\textrm{end}}}{2}\right)^2 (t+1)$ and for the approximation to $\tilde{u}(t)$ obtained for $M=1000$.}
	\label{table:ITVOLT}
	\end{table}
	In Figure \ref{fig:pseudoSpectrum_ITVOLT}, the spectrum and pseudospectra for the two choices are shown.
	The eigenvalues of $(I-F_{1000}^{(N)})$ lie on a circle and, as $\tau_{\textrm{end}}$ increases, the radius of this circle increases; however, the center of the circle also shifts and the circle does not cross the real line. Thus $(I-F_{1000}^{(N)})^{-1}$ exists in both cases.
	The pseudospectra that contain the origin are those associated with large perturbations of the coefficient matrix.	
	In Figure \ref{fig:decay_ITVOLT}, the order of magnitude of the entries of the inverse for both cases is shown.
	It shows that the inverse is characterized by the decay phenomenon.
	\begin{figure}[!ht]
		\centering
		\setlength\figureheight{4.5cm}
		\setlength\figurewidth{5.4cm}
%		\captionsetup{width=.85\linewidth}
		\input{figs/Spectrum_ITVOLT_M=1000tend=25__M=1000tend=50.tikz}
%			\includegraphics{tikz/TextInTemplate2-figure17.pdf}
		\caption{Spectrum ({\color{black} $\cdot$}) and pseudospectra of $(I_M-F_M^{(N)})$ for the coefficient matrix $F_M^{(N)}$ of $\tilde{f}(t) =  -\imath \left(\frac{\tau_{\textrm{end}}}{2}\right)^2 (t+1)$.
			Left: $\tau_{\textrm{end}}= 25$ and $M=1000$. Right: $\tau_{\textrm{end}}=50$ and $M=1000$.}
		\label{fig:pseudoSpectrum_ITVOLT}
	\end{figure}

	\begin{figure}[!ht]
		\centering
%		\captionsetup{width=.85\linewidth}
		\includegraphics[width=0.4\textwidth]{figs/M=1000_tend=25.eps}
		\includegraphics[width=0.4\textwidth]{figs/M=1000_tend=50.eps}
		\caption{Order of magnitude of the entries of $(I_M-F_M^{(N)})^{-1}$ for $M=1000$, where $F_M^{(N)}$ is the coefficient matrix of $\tilde{f}(t) =  -\imath \left(\frac{\tau_{\textrm{end}}}{2}\right)^2 (t+1)$.
			Left: $\tau_{\textrm{end}}= 25$. Right: $\tau_{\textrm{end}}=50$.}
		\label{fig:decay_ITVOLT}
	\end{figure}

	Tables \ref{table:conv25} and \ref{table:conv50} show for both the choices $\tau_{\textrm{end}}=25$ and $\tau_{\textrm{end}}=50$ the function error $\textrm{err}_\textrm{f}$ and the amplitude of the last accurately computed Legendre coefficient, respectively.
	The last accurately computed Legendre coefficient for both choices is $\hat{c}_{M-1}$.
	From the tables we can conclude that, for $M$ large enough, an estimate for $\textrm{err}_\textrm{f}$ can be obtained by looking only at the Legendre coefficients, which is the expected behavior in function approximation with Legendre polynomials \cite{Tr13}.
	\begin{table}[!ht]
		\begin{minipage}[t]{0.48\textwidth}
		\centering
		\captionsetup{width=.8\linewidth}
		\begin{tabular}{l|ll}
			$M$ & $\textrm{err}_{\textrm{f}}$ & $\vert \hat{c}_{M-1}\vert$  \\ \hline
			200 & $1.8\textrm{e+}00$                & $2.7\textrm{e}-02$                            \\
			210 & $3.3\textrm{e}-01$                & $1.3\textrm{e}-02$                       \\
			220 & $1.6\textrm{e}-02$                & $1.9\textrm{e}-03$                \\
			230 & $4.6\textrm{e}-04$                & $9.0\textrm{e}-05$                                 \\
			240 & $8.0\textrm{e}-06$                & $2.2\textrm{e}-06$                                  \\
			250 & $8.5\textrm{e}-08$                & $2.9\textrm{e}-08$                        \\
			260 & $5.9\textrm{e}-10$                & $2.4\textrm{e}-10$                   \\
			270 & $2.8\textrm{e}-12$                & $1.2\textrm{e}-12$                      \\
			280 & $9.9\textrm{e}-14$                & $2.1\textrm{e}-14$                      \\
			290 & $8.4\textrm{e}-14$                & $1.2\textrm{e}-14$                     \\
			300 & $8.7\textrm{e}-14$                & $1.2\textrm{e}-14$                    
		\end{tabular}
		\caption{Error on the Legendre coefficients and the magnitude of the last accurate Legendre coefficient $\hat{c}_{M-N}$ for increasing $M$ for $\tau_{\textrm{end}} = 25$.}
		\label{table:conv25}
		\end{minipage}
		\hspace{-0.3cm}
		\begin{minipage}[t]{0.48\textwidth}
		\centering
		\captionsetup{width=.75\linewidth}
			\begin{tabular}{l|ll}
				$M$ & $\textrm{err}_{\textrm{f}}$ & $\vert \hat{c}_{M-1}\vert$ \\ \hline
				830 & $8.3\textrm{e}-02$                & $2.4\textrm{e}-03$                \\
				840 & $1.1\textrm{e}-02$                & $5.5\textrm{e}-04$                \\
				850 & $1.1\textrm{e}-03$                & $8.4\textrm{e}-05$                \\
				860 & $9.9\textrm{e}-05$                & $9.3\textrm{e}-06$                \\
				870 & $7.0\textrm{e}-06$                & $8.0\textrm{e}-07$                \\
				880 & $4.0\textrm{e}-07$                & $5.4\textrm{e}-08$                \\
				890 & $1.9\textrm{e}-08$                & $3.0\textrm{e}-09$                \\
				900 & $7.7\textrm{e}-10$                & $1.3\textrm{e}-10$                \\
				910 & $2.6\textrm{e}-11$                & $5.0\textrm{e}-12$                \\
				920 & $9.6\textrm{e}-13$                & $1.6\textrm{e}-13$                \\
				930 & $3.1\textrm{e}-13$                & $1.6\textrm{e}-14$               
			\end{tabular}
		\caption{Error on the Legendre coefficients and the magnitude of the last accurate Legendre coefficient $\hat{c}_{M-1}$ for increasing $M$ for $\tau_{\textrm{end}} = 50$.}
		\label{table:conv50}
		\end{minipage}
	\end{table}


	\subsection{NMR-inspired problem}
	The following experiment is inspired by a problem in nuclear magnetic resonance spectroscopy (NMR) where the matrix ODE
	\begin{equation*}
		\frac{d}{dt} \tilde{A}(t) = -2\imath \pi \tilde{H}(t) \tilde{A}(t), \quad \left[0,t_{\textrm{end}}\right],
	\end{equation*}
	governs the dynamics of, e.g., a magic angle spinning experiment.
	The matrix valued function $\tilde{H}(t)$ is the Hamiltonian and is of size $2^\ell\times 2^\ell$, where $\ell$ is the number of spins in the sample \cite{Le08}.
	The functions appearing in $\tilde{H}(t)$ are of the form
	\begin{equation}\label{eq:ftil_NMR}
		\tilde{f}(t) = -2 \imath \pi (\alpha + \beta \cos(2\pi \nu t) + \gamma \cos(4\pi \nu t)),
	\end{equation}
	where $\alpha\in \left[-1,1\right]$ and $\beta,\gamma\in \left[100,5000\right]$ are typical ranges for these parameters.
	In a magic angle spinning experiment \cite{HaSp98}, the sample spins at an angular velocity $\nu\in \left[5000, 120 000\right]$ chosen by the user. The experiment typically runs for about $t_{\textrm{end}}=10^{-2}$ seconds.
	Here, we consider the simpler problem of a scalar ODE 
	\begin{equation*}
		\frac{d}{dt} \tilde{u}(t) = \tilde{f}(t) \tilde{u}(t), \quad \left[0,t_{\textrm{end}}\right].
	\end{equation*}
	We lose the connection to NMR, but studying this problem provides insight into the capabilities of our proposed procedure to tackle the physically relevant matrix case.
	For $\alpha = 0.05$, $\beta=\gamma=3450$, $\nu=5000$ and $M=1500$ we compute an approximation to $\tilde{u}(t)$.
	The function approximation error is $\textrm{err}_{\textrm{f}} = 1.5994\textrm{e}-04$, increasing $M$ will improve on this error.
	However, an accuracy of the order $1\textrm{e}-4$ suffices for NMR experiments, where one is limited by the accuracy of the measurements.\\
	If the frequency is increased to $\nu=120 000$ the function and solution require more coefficients to be represented accurately, i.e., we require a larger $M$.
	Alternatively, the interval $\left[0,t_{\textrm{end}}\right]$ can be split into smaller subintervals and the ODE is then solved on each of these subintervals separately.
	Suppose we split the interval into 20 equal subintervals, then the first subproblem is the ODE on the interval $\left[0,\frac{t_{\textrm{end}}}{20}\right] = \left[0, 5 \textrm{e}{-04}\right]$.
	The approximate solution to this subproblem obtained for $M=1500$ has $\textrm{err}_\textrm{f} = 1.4101\textrm{e}-07$ and $\textrm{err}_c= 1.4087\textrm{e}-08$.
	Thus, by splitting the interval into 20 subintervals the ODE can be solved accurately for the highest frequency of interest.
	
	The spectrum and pseudospectra for both functions are shown in Figure \ref{fig:spectrum_NMR}.
	\begin{figure}[!ht]
		\centering
		\setlength\figureheight{4.5cm}
		\setlength\figurewidth{5.4cm}
%		\captionsetup{width=.85\linewidth}
		\input{figs/Spectrum_NMR_M=1500nu=5000tend=10ms__nu=120000tend=10over20ms.tikz}
%			\includegraphics{tikz/TextInTemplate2-figure18.pdf}
		\caption{Spectrum ({\color{black} $\cdot$}) and pseudospectrum of $(I-F_M)$ for the coefficient matrix $F_M$ of $\tilde{f}(t)$ \eqref{eq:ftil_NMR}.
			Left: $\nu=5000$, $t_\textrm{end}=10^{-2}$ and $M=1500$. Right: $\nu=120000$, $t_\textrm{end}=\frac{1}{20}10^{-2}$ and $M=1500$.}
		\label{fig:spectrum_NMR}
	\end{figure}	
\newpage
	\section{Conclusion}
	We presented a new approach for the solution of linear non-autonomous scalar ODEs based on the discretization of the $\star$-product by using expansions in series of Legendre polynomials. This approach effectively transforms operations defined on bivariate distributions to operations in a (sub)algebra of infinite matrices.
	We studied the properties of such matrices and used them to prove the existence of the ODE solution in the infinite matrix algebra. Once the Legendre polynomial series is truncated, the ODE solution is accessible by solving a (finite) linear system. We studied the truncation error, proving that obtaining accurate approximations from this finite system is possible. We also presented effective methods to compute the discretization and tested the method on several numerical examples.
	
	The new method was numerically analyzed in the scalar case. The scalar analysis is a fundamental step toward understanding the more general case of systems of non-autonomous linear ODEs \cite{GiPo20,PoVB22}. In fact, the authors are developing a method for this more general case whose analysis and understanding will be built on the crucial results presented here. 
	
	\appendix
	\section{Proof of Lemma \ref{lemma:infNormBd}}\label{app:A}
	In this proof, it is easier to work with the formula based on Legendre polynomials $\dot{p}_k(t)$, which are normalized such that $\dot{p}_k(1)=1$.
	Then, setting $s=(a+b+c)/2$, we have
	\begin{align*}
		\dot{\mathcal{F}}_{a,b,c} &:= \int_{-1}^{1} \dot{p}_a(\tau) \dot{p}_b(\tau) \dot{p}_c(\tau) d\tau\\
		&= 
		\begin{cases}
			0\quad \text{if } a+b+c \text{ odd},\\
			0 \quad \text{if } s<\max(a,b,c),\\
			0 \quad \text{if } a<\vert b-c \vert,\\
			\frac{2}{a+b+c+1}\begin{pmatrix}
				2(s-a)\\
				s-a
			\end{pmatrix} \begin{pmatrix}
				2(s-b)\\
				s-b
			\end{pmatrix} \begin{pmatrix}
				2(s-c)\\
				s-c
			\end{pmatrix} \begin{pmatrix}
				2s\\
				s
			\end{pmatrix}^{-1} \quad \text{else}.
		\end{cases}\\
		&= \begin{cases}
			0\quad \text{if } a+b+c \text{ odd},\\
			0 \quad \text{if } b+c<a\\%s<\max(a,b,c),\\
			0 \quad \text{if } a<\alpha:=\vert b-c\vert,\\
			\frac{1}{2^{(2a-1)}}  {\frac{1}{(a+b+c+1)} \left(\prod_{j=1}^{a}\frac{-a+b+c+2j}{-a+b+c+2j-1}\right)} {\frac{\prod_{j=(\frac{a+\alpha}{2}+1)}^{a+\alpha}j^2}{\prod_{j=1}^{\frac{a-\alpha}{2}}j^2 \prod_{j=(a-\alpha+1)}^{a+\alpha} j}}\quad \text{else}.
		\end{cases}
	\end{align*}
	First, we need the following property.
	\begin{property}\label{prop:derivative}
		The following equality holds for $ x \in \mathbb{R}$ and $d=1,2,\dots$,
		\begin{equation*}
			\frac{\partial}{\partial x}\left( \prod_{j=1}^{d}\frac{2x+2j}{2x+2j-1}\right)
			= -2  \prod_{j=1}^{d}\frac{2x+2j}{2x+2j-1} \sum_{j=1}^{d} \frac{1}{(2x+2j)(2x+2j-1)}.
		\end{equation*}
	\end{property}
	\begin{proof}
		Follows by induction on $d$.
		For $d=1$:
		\begin{equation*}
			\frac{\partial}{\partial x} \frac{2x+2}{2x+1} = -2 \frac{2x+2}{2x+1} \frac{1}{(2x+1)(2x+2)}.
		\end{equation*}
		Assume the equality holds for $d$, consider $d+1$:
		\begin{align*}
			\frac{\partial}{\partial x} \left(\prod_{j=1}^{d+1} \frac{2x+2j}{2x+2j-1}\right) &= \frac{\partial}{\partial x} \left(\frac{2x+2d+2}{2x+2d+1} \prod_{j=1}^{d} \frac{2x+2j}{2x+2j-1}\right)\\
			&= -2 \prod_{j=1}^{d+1} \frac{2x+2j}{2x+2j-1} \sum_{j=1}^{d+1} \frac{1}{(2x+2j)(2x+2j-1)}.
		\end{align*}
		This proves the statement.
	\end{proof}
	
		\begin{lemma}[Monotonous decay along diagonals]\label{lemma:monoDecay}
		For given integers $d\geq 0$ and $k\leq d$ , the following equality is satisfied for $i=1,2,\dots$,
		\begin{equation*}
			\dot{\mathcal{F}}_{d,k,d-k} > \dot{\mathcal{F}}_{d,k+i,d-k+i} > 0.
		\end{equation*}
	\end{lemma}
	\begin{proof}
		For $d=0$ the integral of the triple product is
		\begin{equation*}
			\dot{\mathcal{F}}_{0,b,c}\int_{-1}^1 \dot{p}_0(\tau) \dot{p}_b(\tau) \dot{p}_c(\tau) d\tau = \int_{-1}^1 \dot{p}_b(\tau) \dot{p}_c(\tau) d\tau = \frac{2}{b+c+1},
		\end{equation*}
		which clearly satisfies $\dot{\mathcal{F}}_{0,0,0} > \dot{\mathcal{F}}_{0,i,i} > 0$.
		Next, we prove the statement for $d\geq 1$.
		The elements $\dot{\mathcal{F}}_{d,k+i,d-k+i}$ are clearly positive and nonzero.
		Set $\alpha =  \vert 2k-d\vert$, then
		\begin{equation*}
			\dot{\mathcal{F}}_{d,k+i,d-k+i} = \frac{1}{2^{(2d-1)}} \frac{1}{d+2i+1} \prod_{j=1}^{d}\frac{2i+2j}{2i+2j-1} \frac{\prod_{j=\frac{d+\alpha}{2}+1}^{d+\alpha}j^2}{\prod_{j=1}^{\frac{d-\alpha}{2}}j^2 \prod_{j=d-\alpha+1}^{d+\alpha}j}.
		\end{equation*}
		The term $C(d,\alpha):= \frac{1}{2^{(2d-1)}}\frac{\prod_{j=\frac{d+\alpha}{2}+1}^{d+\alpha}j^2}{\prod_{j=1}^{\frac{d-\alpha}{2}}j^2 \prod_{j=d-\alpha+1}^{d+\alpha}j}$ is independent of $i$.
		To show that the expression decreases as $i$ increases, we take the derivative with respect to $0\leq x\in\mathbb{R}$ and use Property \ref{prop:derivative}
		\begin{align*}
			&\frac{\partial}{\partial x}\dot{\mathcal{F}}_{d,k+x,d-k+x} = C(d,\alpha)  \frac{\partial}{\partial x}\left(\frac{1}{d+2x+1} \prod_{j=1}^{d}\frac{2x+2j}{2x+2j-1}\right)\\
			& = -\frac{2 C(d,\alpha)}{d+2x+1}   \prod_{j=1}^{d}\frac{2x+2j}{2x+2j-1} \left(\frac{1}{d+2x+1}+\sum_{j=1}^{d} \frac{1}{(2x+2j)(2x+2j-1)}\right)\\
			&< 0.
		\end{align*}
		Since $\dot{\mathcal{F}}> 0$ for $i=0,1,\dots$, replacing $x$ with integers $i\geq 0$ proves the statement.
	\end{proof}

 \begin{lemma}\label{lemma:max:F}
 For given integers $d\geq 0$ and $k \leq d$, 
 \begin{equation*}
     \dot{\mathcal{F}}_{d,k,d-k} \leq \frac{2}{2d + 1}.
 \end{equation*}
 \end{lemma}
 \begin{proof}
   In order to prove this lemma it is sufficient to show that
   \begin{equation*}
   \max_{k=0,\dots,d}
       \begin{pmatrix}
				2(d-k)\\
				d-k
			\end{pmatrix} \begin{pmatrix}
				2k\\
				k
			\end{pmatrix} 
   = \begin{pmatrix}
				2d\\
				d
			\end{pmatrix}.
   \end{equation*}
   Note that, because of symmetry, this is equivalent to showing that, for $d/2 \leq k < d$ and $d>0$, it holds that
      \begin{equation*}
          \begin{pmatrix}
				2(d-k)\\
				d-k
			\end{pmatrix} \begin{pmatrix}
				2k\\
				k
			\end{pmatrix} 
   \geq        \begin{pmatrix}
				2(d-k) -2\\
				d-k -1
			\end{pmatrix} \begin{pmatrix}
				2k-2\\
				k-1
			\end{pmatrix}.
   \end{equation*}
   The equation above can be reformulated as the quadratic problem
   \begin{equation*}
       4(2d-2k-1)(2k-1) \geq k(d-k).
   \end{equation*}
   For $d\geq2$, the inequality above is satisfied for every $d/2 \leq k < d$. The proof is then concluded since the cases $d=0, 1$ are trivial.
 \end{proof}
 
	
	\begin{proof}[Proof of Lemma \ref{lemma:infNormBd}]
		The coefficients of $B^{(d)} = \left[b_{k,\ell}^{(d)} \right]_{k,\ell=0}^\infty$ are given by the formula
		\begin{equation*}
			b_{k,\ell}^{(d)} = \frac{\sqrt{(2d+1)(2k+1)}}{\sqrt{8} \sqrt{2l+1}} \left( \dot{\mathcal{F}}_{d,k,\ell+1} - \dot{\mathcal{F}}_{d,k,\ell-1}\right).
		\end{equation*}
		Then, by the definition of the infinity norm, we have
		\begin{align*}
			\Vert B^{(d)} \Vert_\infty &= \max_{k\geq 0} \sum_{\ell = 0}^{\infty} \vert b_{k,\ell}^{(d)} \vert = \frac{\sqrt{2d+1}}{\sqrt{8}} \max_{k\geq 0} \sum_{\ell=0}^{\infty} \left( \frac{\sqrt{2k+1}}{\sqrt{2\ell+1}} \left\vert \dot{\mathcal{F}}_{d,k,\ell+1} - \dot{\mathcal{F}}_{d,k,\ell-1} \right\vert \right) \\
			&\leq \frac{\sqrt{2d+1}}{\sqrt{8}} \max_{k\geq 0} \sum_{\ell=0}^{\infty} \left( \frac{\sqrt{2k+1}}{\sqrt{2\ell+1}} \left\vert \dot{\mathcal{F}}_{d,k,\ell+1} \right\vert +  \left\vert \dot{\mathcal{F}}_{d,k,\ell-1} \right\vert \right) \\
			&= \frac{\sqrt{2d+1}}{\sqrt{8}} \left(\max_{k\geq 0} \sum_{\ell=0}^{\infty}  \frac{\sqrt{2k+1}}{\sqrt{2\ell+1}} \left\vert  \dot{\mathcal{F}}_{d,k,\ell+1} \right\vert + \max_{k\geq 0} \sum_{\ell=0}^{\infty}  \frac{\sqrt{2k+1}}{\sqrt{2\ell+1}}  \left\vert \dot{\mathcal{F}}_{d,k,\ell-1} \right\vert \right).
		\end{align*}
		Consider the first term and use Lemma \ref{lemma:monoDecay}, which implies that $\dot{\mathcal{F}}_{d,k+i,d-k+i}$ for any $i\geq 0$ can be bounded from above by $\dot{\mathcal{F}}_{d,k,d-k}$.
		We can also bound the term $\frac{\sqrt{2k+1}}{\sqrt{2\ell+1}}$:
		\begin{align*}
			\underset{\ell = d-k}{\max_{0\leq k\leq d}} \frac{2k+1}{2\ell +1} = \max_{0\leq k\leq d} \left(\frac{2d+2}{2d-(2k-1)}\right) -1 = \frac{2d+2}{2d -(2d-1)} -1 = 2d+1.
		\end{align*}
		Since $\dot{\mathcal{F}}_{d,k,\ell+1} = 0$ for $\vert k-\ell-1\vert >d$, the infinite sum can be bounded by a finite sum:
		\begin{equation*}
			\max_{k\geq 0} \sum_{\ell=0}^{\infty}  \frac{\sqrt{2k+1}}{\sqrt{2\ell+1}} \left\vert  \dot{\mathcal{F}}_{d,k,\ell+1} \right\vert \leq \sqrt{2d+1}  \sum_{k=0}^{d} \dot{\mathcal{F}}_{d,k,d-k}.
		\end{equation*}
		Since $\vert \dot{\mathcal{F}}_{d,d,-1}\vert = \vert \dot{\mathcal{F}}_{d,d,0}\vert$, similar arguments lead to the following bound for the second term,
		\begin{equation*}
			\max_{k\geq 0} \sum_{\ell= 0}^{\infty} \frac{\sqrt{2k+1}}{\sqrt{2\ell+1}} \vert \dot{\mathcal{F}}_{d,k,\ell-1} \vert \leq \sqrt{2d+1} \dot{\mathcal{F}}_{d,d,0} + \sqrt{2d+1} \sum_{k=0}^{d} \dot{\mathcal{F}}_{d,k,d-k}.
		\end{equation*}
		Hence, we obtain the bound
		\begin{align*}
			\Vert B^{(d)} \Vert_\infty  \leq \frac{2d+1}{\sqrt{8}} \left( \dot{\mathcal{F}}_{d,d,0} + 2 \sum_{k=0}^{d} \dot{\mathcal{F}}_{d,k,d-k} \right).
		\end{align*}
		Now we plug in the formula for $\dot{\mathcal{F}}_{d,k,d-k}$, note that $d+k+d-k = 2d$ and $-d+k+d-k = 0$ and let $\alpha(k) := \vert 2k-d\vert$, then 
 \begin{align*}
  			\Vert B^{(d)} \Vert_\infty  &\leq \frac{2d+1}{\sqrt{8}} \left( \dot{\mathcal{F}}_{d,d,0} + 2 \sum_{k=0}^{d} \dot{\mathcal{F}}_{d,k,d-k} \right) \leq \frac{2d+1}{\sqrt{8}} \left( 1 + 2 \sum_{k=0}^{d} \frac{2}{2d+1} \right)\\
			&\leq \frac{2d+1}{\sqrt{8}} \left( 1 + \frac{4(d+1)}{2d+1} \right)  \leq \frac{6d+5}{\sqrt{8}} < 3d+2,
 \end{align*} 
 which proves Lemma~\ref{lemma:infNormBd}. 
	\end{proof}

	\section*{Acknowledgments}

	The second author thanks Marcus Webb for discussion in an early stage of this project.

	
	\section*{Funding}

	This  work  was  supported  by  Charles  University  Research programs PRIMUS/21/SCI/009 and UNCE/SCI/023 and by the Magica project ANR-20-CE29-0007 funded by the French National Research Agency.
	
	
	\bibliographystyle{siam}
%	\bibliography{/home/buggenhout/Research/references}  % name your BibTeX data base
	\input{references.bbl}
	
\end{document}
