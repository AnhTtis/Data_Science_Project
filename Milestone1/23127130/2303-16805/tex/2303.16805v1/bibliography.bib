@inproceedings{Gunther.2018,
 author = {G{\"u}nther, Sebastian and M{\"u}ller, Florian and Funk, Markus and Kirchner, Jan and Dezfuli, Niloofar and M{\"u}hlh{\"a}user, Max},
 title = {TactileGlove: TactileGlove: Assistive Spatial Guidance in 3D Space through Vibrotactile Navigation},
 pages = {273--280},
 publisher = {ACM},
 isbn = {9781450363907 },
 booktitle = {Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference},
 year = {2018},
 address = {New York, NY, USA},
 doi = {10.1145/3197768.3197785 },
 file = {3197768.3197785:Attachments/3197768.3197785.pdf:application/pdf}
}

@BOOK{Girden1992,
	AUTHOR = {Girden, Ellen R.},
	YEAR = {1992},
	TITLE = {ANOVA - Repeated Measures},
	EDITION = {},
	ISBN = {978-0-803-94257-8},
	PUBLISHER = {SAGE},
	ADDRESS = {London},
}

@ARTICLE{4081935,
author={Alles, David S.},
journal={IEEE Transactions on Man-Machine Systems}, 
title={Information Transmission by Phantom Sensations}, 
year={1970},
volume={11},
number={1},
pages={85-91},
doi={10.1109/TMMS.1970.299967}}

@inproceedings{Pascher.2023robotMotionIntent,
author = {Pascher, Max and Gruenefeld, Uwe and Schneegass, Stefan and Gerken, Jens},
title = {How to Communicate Robot Motion Intent: A Scoping Review},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems  - CHI '23},
doi = {10.1145/3544548.3580857},
year = {2023}
}

@article{Wu.2013,
  doi = {10.18494/sam.2013.855},
  year = {2013},
  publisher = {MYU},
  author = {Juan Wu and Yu Ding and Dejing Ni and Guangming Song and Wei Liu},
  pages = {79-97},
  title = {Vibrotactile Representation of Three-Dimensional Shape and Implementation on a Vibrotactile Pad},
  journal = {Sensors and Materials}
}
@inproceedings{Gruenefeld2017,
  doi = {10.1145/3131277.3132175},
  url = {https://doi.org/10.1145/3131277.3132175},
  year = {2017},
  month = oct,
  publisher = {{ACM}},
  author = {Uwe Gruenefeld and Dag Ennenga and Abdallah El Ali and Wilko Heuten and Susanne Boll},
  title = {{EyeSee}360},
  booktitle = {Proceedings of the 5th Symposium on Spatial User Interaction}
}

@inproceedings{10.1145/3173574.3173832,
author = {Park, Gunhyuk and Choi, Seungmoon},
title = {Tactile Information Transmission by 2D Stationary Phantom Sensations},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173832},
doi = {10.1145/3173574.3173832},
abstract = {A phantom sensation refers to an illusory tactile sensation perceived midway between multiple distant stimulations on the skin. Phantom sensations have been used intensively in tactile interfaces owing to their simplicity and effectiveness. Despite that, the perceptual performance of phantom sensations is not completely understood, especially for 2D cases. This work is concerned with 2D stationary phantom sensations and their fundamental value as a means for information display. In User Study 1, we quantified the information transmission capacity using an absolute identification task of 2D phantom sensations. In User Study 2, we probed the distributions of the actual perceived positions of 2D phantom sensations. The investigations included both types of phantom sensations-within and out of the body. Our results provide general guidelines as to leveraging 2D phantom sensations in the design of spatial tactile display.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {phantom sensation, illusion, funneling, information transmission, haptics, perception, vibrotactile},
location = {Montreal QC, Canada},
series = {CHI '18}
}


@article{Tan2003,
 author = {Hong Z. Tan and Robert Gray and J. Jay Young and Ryan Traylor},
 title = {A Haptic Back Display for Attentional and Directional Cueing},
 volume = {Vol.3.No.1.,Haptics-e, 11-June-2003},
 file = {he-v3n1:Attachments/he-v3n1.pdf:application/pdf},
 year= {2003}
}

@article{Chouvardas.2008,
 author = {Chouvardas, V. G. and Miliou, A. N. and Hatalis, M. K.},
 year = {2008},
 title = {Tactile displays: Overview and recent advances},
 pages = {185--194},
 volume = {29},
 number = {3},
 issn = {01419382},
 journal = {Displays},
 doi = {10.1016/j.displa.2007.07.003},
 file = {chouvardas2008:Attachments/chouvardas2008.pdf:application/pdf}
}

@inbook{Lehtinen2012,
author = {Lehtinen, Ville and Oulasvirta, Antti and Salovaara, Antti and Nurmi, Petteri},
title = {Dynamic Tactile Guidance for Visual Search Tasks},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380173},
abstract = {Visual search in large real-world scenes is both time consuming and frustrating, because the search becomes serial when items are visually similar. Tactile guidance techniques can facilitate search by allowing visual attention to focus on a subregion of the scene. We present a technique for dynamic tactile cueing that couples hand position with a scene position and uses tactile feedback to guide the hand actively toward the target. We demonstrate substantial improvements in task performance over a baseline of visual search only, when the scene's complexity increases. Analyzing task performance, we demonstrate that the effect of visual complexity can be practically eliminated through improved spatial precision of the guidance.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {445–452},
numpages = {8}
}

@inproceedings{Lee_Starner_2010,
author = {Lee, Seungyon Claire and Starner, Thad},
title = {BuzzWear: Alert Perception in Wearable Tactile Displays on the Wrist},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753392},
doi = {10.1145/1753326.1753392},
abstract = {We present two experiments to evaluate wrist-worn wearable tactile displays (WTDs) that provide easy to perceive alerts for on-the-go users. The first experiment (2304 trials, 12 participants) focuses on the perception sensitivity of tactile patterns and reveals that people discriminate our 24 tactile patterns with up to 99\% accuracy after 40 minutes of training. Among the four parameters (intensity, starting point, temporal pattern, and direction) that vary in the 24 patterns, intensity is the most difficult parameter to distinguish and temporal pattern is the easiest. The second experiment (9900 trials, 15 participants) focuses on dual task performance, exploring users' abilities to perceive three incoming alerts from two mobile devices (WTD and mobile phone) with and without visual distraction. The second experiment reveals that, when visually distracted, users' reactions to incoming alerts become slower for the mobile phone but not for the WTD.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {433–442},
numpages = {10},
keywords = {tactile display, wearable computing, attention},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@article{Bland1995MultipleST,
  title={Multiple significance tests: the Bonferroni method},
  author={J Martin Bland and Douglas G. Altman},
  journal={BMJ},
  year={1995},
  volume={310},
  pages={170}
}

@article{Cholewiak.2000,
 abstract = {In order to provide information regarding orientation or direction, a convenient code employs vectors (lines) because they have both length and direction. Potential users of such information, encoded tactually, could include persons who are blind, as well as pilots, astronauts, and scuba divers, all of whom need to maintain spatial awareness in their respective unusual environments. In these situations, a tactile display can enhance environmental awareness. In this study, optimal parameters were explored for lines presented dynamically to the skin with vibrotactile arrays on three body sites, with veridical and saltatory presentation modes. Perceived length, straightness, spatial distribution, and smoothness were judged while the durations of the discrete taps making up the {\textquotedbl}drawn{\textquotedbl} dotted lines and the times between them were varied. The results indicate that the two modes produce equivalent sensations and that similar sets of timing parameters, within the ranges tested, result in {\textquotedbl}good{\textquotedbl} lines at each site.},
 author = {Cholewiak, R. W. and Collins, A. A.},
 year = {2000},
 title = {The generation of vibrotactile patterns on a linear array: influences of body site, time, and presentation mode},
 keywords = {Adult;Female;Humans;Male;Perception/physiology;Random Allocation;Spatial Behavior/physiology;Time Factors;Touch/physiology;Vibration},
 pages = {1220--1235},
 volume = {62},
 number = {6},
 issn = {1532-5962},
 journal = {Perception {\&} Psychophysics},
 doi = {10.3758/bf03212124},
 file = {CholewiakCollins00w:Attachments/CholewiakCollins00w.pdf:application/pdf}
}


@article{Hirsh.1961,
  title={Perceived order in different sense modalities.},
  author={Ira J. Hirsh and Carl E. Sherrick},
  journal={Journal of experimental psychology},
  year={1961},
  volume={62},
  pages={
          423-32
        }
}

@article{Petrosino.1989,
  title={Temporal Resolution of the Aging Tactile Sensory System},
  author={Linda Petrosino and Donald Fucci},
  journal={Perceptual and Motor Skills},
  year={1989},
  volume={68},
  pages={288 - 290}
}

@article{Friedman1937TheUO,
  title={The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance},
  author={Milton Friedman},
  journal={Journal of the American Statistical Association},
  year={1937},
  volume={32},
  pages={675-701}
}

@article{
Geldard.1960,
author = {Frank A. Geldard },
title = {Some Neglected Possibilities of Communication},
journal = {Science},
volume = {131},
number = {3413},
pages = {1583-1588},
year = {1960},
doi = {10.1126/science.131.3413.1583},
URL = {https://www.science.org/doi/abs/10.1126/science.131.3413.1583},
eprint = {https://www.science.org/doi/pdf/10.1126/science.131.3413.1583}}

@article{Sherrick.1966,
  title={Apparent haptic movement},
  author={Carl E. Sherrick and Ronald Rogers},
  journal={Perception \& Psychophysics},
  year={1966},
  volume={1},
  pages={175-180}
}

@inproceedings{Israr.2011,
author = {Israr, Ali and Poupyrev, Ivan},
year = {2011},
month = {05},
pages = {2019-2028},
title = {Tactile Brush: Drawing on skin with a tactile grid display},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/1978942.1979235}
}

@article{
Geldard.1972,
author = {Frank A. Geldard  and Carl E. Sherrick },
title = {The Cutaneous "Rabbit": A Perceptual Illusion},
journal = {Science},
volume = {178},
number = {4057},
pages = {178-179},
year = {1972},
doi = {10.1126/science.178.4057.178},
URL = {https://www.science.org/doi/abs/10.1126/science.178.4057.178},
eprint = {https://www.science.org/doi/pdf/10.1126/science.178.4057.178},
abstract = {Anomalous localizations of mechanical and electrical cutaneous pulses are produced when widely separated bodily points are successively stimulated with trains of taps. The observer experiences a manifold of discrete "phantom" impressions connecting the points actually touched. The theoretical basis for this perceptual phenomenon is not understood, but some boundary conditions are specified.}}

@article{Eimer.2005,
  title={Cutaneous saltation within and across arms: A new measure of the saltation illusion in somatosensation},
  author={Martin Eimer and Bettina Forster and Jonas F. Vibell},
  journal={Perception \& Psychophysics},
  year={2005},
  volume={67},
  pages={458-468}
}

@Book{Geldard.1977,
author = { Geldard, Frank A. },
title = { Sensory saltation : metastability in the perceptual world / Frank A. Geldard },
isbn = { 0470295716 },
publisher = { Lawrence Erlbaum Associates ; distributed by the Halsted Press Division of Wiley Hillsdale, N.J. : New York },
pages = { 133 p. : },
year = { 1975 },
type = { Book },
language = { English },
subjects = { Psychophysiology.; Perception.; Sensation.; Neural transmission. },
}

@article{Kirman.1974,
 abstract = {The effects of variations in stimulus duration and interstimulus onset interval on ratings of tactile apparent movement were determined for seven Ss with stimulators of very small diameter. Judgments of successiveness and simultaneity were also obtained. It was found that apparent movement increased as a power function of increases in stimulus duration. The function relating tactile apparent movement and stimulus duration was shown to be similar to that obtained by Kolers (1964) for visual apparent movement, lnterstimulus onset interval also had a marked effect on apparent movement, and the optimal interval was influenced by stimulus duration in a manner similar to that reported by Sherrick and Rogers (1966).},
 author = {Kirman, Jacob H.},
 year = {1974},
 title = {Tactile apparent movement: The effects of interstimulus onset interval and stimulus duration},
 pages = {1--6},
 volume = {15},
 number = {1},
 issn = {1532-5962},
 journal = {Perception {\&} Psychophysics},
 doi = {10.3758/BF03205819}
}

@article{Burtt.1917,
  title={Tactual illusions of movement},
  author={Harold Ernest Burtt},
  journal={Journal of Experimental Psychology},
  volume={2},
  year= {1917},
  month= {10},
  pages={371-385}
}

@article{cohen_1992, title={A power primer.}, volume={112}, DOI={10.1037/0033-2909.112.1.155}, number={1}, journal={Psychological Bulletin}, author={Cohen, Jacob}, year={1992}, pages={155–159}} 

@inproceedings{Chen2018,
author = {Chen, Taizhou and Wu, Yi-Shiun and Zhu, Kening},
title = {Investigating Different Modalities of Directional Cues for Multi-Task Visual-Searching Scenario in Virtual Reality},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281516},
doi = {10.1145/3281505.3281516},
abstract = {In this study, we investigated and compared the effectiveness of visual, auditory, and vibrotactile directional cues on multiple simultaneous visual-searching tasks in an immersive virtual environment. Effectiveness was determined by the task-completion time, the range of head movement, the accuracy of the identification task, and the perceived workload. Our experiment showed that the on-head vibrotactile display can effectively guide users towards virtual visual targets, without affecting their performance on the other simultaneous tasks, in the immersive VR environment. These results can be applied to numerous applications (e.g. gaming, driving, and piloting) in which there are usually multiple simultaneous tasks, and the user experience and performance could be vulnerable.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {41},
numpages = {5},
keywords = {multi-task, visual, directional cue, virtual reality, vibration, auditory},
location = {Tokyo, Japan},
series = {VRST '18}
}

@article{Kaul2016HapticHead3G,
  title={HapticHead: 3D Guidance and Target Acquisition through a Vibrotactile Grid},
  author={Oliver Beren Kaul and Michael Rohs},
  journal={Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
  year={2016}
}

@inproceedings{metaanalysis,
title = "Comparing the effects of visual-auditory and visual-tactile feedback on user performance: A meta-analysis",
abstract = "In a meta-analysis of 43 studies, we examined the effects of multimodal feedback on user performance, comparing visual-auditory and visual-tactile feedback to visual feedback alone. Results indicate that adding an additional modality to visual feedback improves performance overall. Both visual-auditory feedback and visual-tactile feedback provided advantages in reducing reaction times and improving performance scores, but were not effective in reducing error rates. Effects are moderated by task type, workload, and number of tasks. Visual-auditory feedback is most effective when a single task is being performed (g = .87), and under normal workload conditions (g = .71). Visual-tactile feedback is more effective when multiple tasks are begin performed (g = .77) and workload conditions are high (g = .84). Both types of multimodal feedback are effective for target acquisition tasks; but vary in effectiveness for other task types. Implications for practice and research are discussed.",
keywords = "Meta-analysis, Multimodal interface, Visual-auditory feedback, Visual-tactile feedback",
author = "Burke, {Jennifer L.} and Prewett, {Matthew S.} and Gray, {Ashley A.} and Liuquin Yang and Stilson, {Frederick R.B.} and Coovert, {Michael D.} and Elliot, {Linda R.} and Elizabeth Redden",
year = "2006",
doi = "10.1145/1180995.1181017",
language = "English",
isbn = "159593541X",
series = "ICMI'06: 8th International Conference on Multimodal Interfaces, Conference Proceeding",
pages = "108--117",
booktitle = "ICMI'06",
note = "null ; Conference date: 02-11-2006 Through 04-11-2006",
}

@inproceedings{Goldau.2021petra,
author = {Goldau, Felix Ferdinand and Frese, Udo},
title = {Learning to Map Degrees of Freedom for Assistive User Control: Towards an Adaptive DoF-Mapping Control for Assistive Robots},
year = {2021},
isbn = {9781450387927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453892.3453895},
doi = {10.1145/3453892.3453895},
abstract = { This paper presents a novel approach to shared control for an assistive robot by
adaptively mapping the degrees of freedom (DoFs) for the user to control with a low-dimensional
input device. For this, a convolutional neural network interprets camera data of the
current situation and outputs a probabilistic description of possible robot motion
the user might command. Applying a novel representation of control modes, the network’s
output is used to generate individual degrees of freedom of robot motion to be controlled
by single DoF of the user’s input device. These DoFs are not necessarily equal to
the cardinal DoFs of the robot but are instead superimpositions of those, thus allowing
motions like diagonal directions or orbiting around a point. This enables the user
to perform robot motions previously impossible with such a low-dimensional input device.
The shared control is implemented for a proof-of-concept 2D simulation and evaluated
with an initial user study by comparing it to a standard control approach. The results
show a functional control which is both subjectively and objectively significantly
faster, but subjectively more complex.},
booktitle = {The 14th PErvasive Technologies Related to Assistive Environments Conference},
pages = {132–139},
numpages = {8},
keywords = {Deep Learning (DL), Assistive Robotics, Convolutional Neural Network (CNN), Shared User Control, Human Robot Interface (HRI), Human Machine Interface (HMI)},
location = {Corfu, Greece},
series = {PETRA 2021}
}
@article{Fong.2003,
title = {A survey of socially interactive robots},
journal = {Robotics and Autonomous Systems},
volume = {42},
number = {3},
pages = {143-166},
year = {2003},
note = {Socially Interactive Robots},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(02)00372-X},
url = {https://www.sciencedirect.com/science/article/pii/S092188900200372X},
author = {Terrence Fong and Illah Nourbakhsh and Kerstin Dautenhahn},
keywords = {Human–robot interaction, Interaction aware robot, Sociable robot, Social robot, Socially interactive robot},
abstract = {This paper reviews “socially interactive robots”: robots for which social human–robot interaction is important. We begin by discussing the context for socially interactive robots, emphasizing the relationship to other research fields and the different forms of “social robots”. We then present a taxonomy of design methods and system components used to build socially interactive robots. Finally, we describe the impact of these robots on humans and discuss open issues. An expanded version of this paper, which contains a survey and taxonomy of current applications, is available as a technical report [T. Fong, I. Nourbakhsh, K. Dautenhahn, A survey of socially interactive robots: concepts, design and applications, Technical Report No. CMU-RI-TR-02-29, Robotics Institute, Carnegie Mellon University, 2002].}
}

@article{Bauer.2008,
author = {Bauer, Andrea and Wollherr, Dirk and Buss, Martin},
title = {Human–Robot Collaboration: A Survey},
journal = {International Journal of Humanoid Robotics},
volume = {05},
number = {01},
pages = {47-66},
year = {2008},
doi = {10.1142/S0219843608001303},

URL = { 
        
    
},
eprint = { 
        
    
}
,
    abstract = { As robots are gradually leaving highly structured factory environments and moving into human populated environments, they need to possess more complex cognitive abilities. They do not only have to operate efficiently and safely in natural, populated environments, but also be able to achieve higher levels of cooperation and communication with humans. Human–robot collaboration (HRC) is a research field with a wide range of applications, future scenarios, and potentially a high economic impact. HRC is an interdisciplinary research area comprising classical robotics, cognitive sciences, and psychology. This paper gives a survey of the state of the art of HRC. Established methods for intention estimation, action planning, joint action, and machine learning are presented together with existing guidelines to hardware design. This paper is meant to provide the reader with a good overview of technologies and methods for HRC. }
}

@article{Kerr.1987,
  doi = {10.1037/h0091572},
  url = {https://doi.org/10.1037/h0091572},
  year = {1987},
  publisher = {American Psychological Association ({APA})},
  volume = {32},
  number = {3},
  pages = {173--180},
  author = {Nancy Kerr and Lee Meyerson},
  title = {Independence as a goal and a value of people with physical disabilities: Some caveats.},
  journal = {Rehabilitation Psychology}
}

@misc{Pflegestatistik,
title = "{Pflegestatistik - Pflege im Rahmen der Pflegeversicherung - Deutschlandergebnisse - 2019}",
author = "{Statistisches Bundesamt (Destatis)}",
howpublished = "\url{https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Gesundheit/Pflege/Publikationen/Downloads-Pflege/pflege-deutschlandergebnisse-5224001199004.pdf}",
year = 2020
}

@INPROCEEDINGS{Goldau.2019drinking,  
author={Goldau, Felix Ferdinand and Shastha, Tejas Kumar and Kyrarini, Maria and Gräser, Axel},  
booktitle={2019 IEEE 16th International Conference on Rehabilitation Robotics (ICORR)},   
title={Autonomous Multi-Sensory Robotic Assistant for a Drinking Task},  
year={2019},  
volume={},  
number={},  
pages={210-216},  
doi={10.1109/ICORR.2019.8779521}
}

@inproceedings{Herlant.2016modeswitch,
author = {Herlant, Laura V. and Holladay, Rachel M. and Srinivasa, Siddhartha S.},
title = {Assistive Teleoperation of Robot Arms via Automatic Time-Optimal Mode Switching},
year = {2016},
isbn = {9781467383707},
publisher = {IEEE Press},
abstract = {Assistive robotic arms are increasingly enabling users with upper extremity disabilities
to perform activities of daily living on their own. However, the increased capability
and dexterity of the arms also makes them harder to control with simple, low-dimensional
interfaces like joysticks and sip-and-puff interfaces. A common technique to control
a high-dimensional system like an arm with a low-dimensional input like a joystick
is through switching between multiple control modes. However, our interviews with
daily users of the Kinova JACO arm identified mode switching as a key problem, both
in terms of time and cognitive load. We further confirmed objectively that mode switching
consumes about 17.4\% of execution time even for able-bodied users controlling the
JACO. Our key insight is that using even a simple model of mode switching, like time
optimality, and a simple intervention, like automatically switching modes, significantly
improves user satisfaction.},
booktitle = {The Eleventh ACM/IEEE International Conference on Human Robot Interaction},
pages = {35–42},
numpages = {8},
keywords = {assistive robotics, teleoperation, mode switching, modal control, shared control},
location = {Christchurch, New Zealand},
series = {HRI '16}
}

@inproceedings{Pascher.2021recommendations,
title = {Recommendations for the Development of a Robotic Drinking and Eating Aid - An Ethnographic Study },
author = {Max Pascher and Annalies Baumeister and Stefan Schneegass and Barbara Klein and Jens Gerken},
editor = {Carmelo Ardito and  Rosa Lanzilotti and Alessio Malizia and Helen Petrie and Antonio Piccinno and Giuseppe Desolda and Kori Inkpen},
url = {https://hci.w-hs.de/pub_recommendations_for_the_development_of_a_robotic_drinking_and_eating_aid___an_ethnographic_study/, PDF Download},
doi = {10.1007/978-3-030-85623-6_21},
year = {2021},
date = {2021-09-01},
booktitle = {Human-Computer Interaction – INTERACT 2021},
publisher = {Springer, Cham},
abstract = {Being able to live independently and self-determined in one's own home is a crucial factor for human dignity and preservation of self-worth. For people with severe physical impairments who cannot use their limbs for every day tasks, living in their own home is only possible with assistance from others. The inability to move arms and hands makes it hard to take care of oneself, e. g. drinking and eating independently. In this paper, we investigate how 15 participants with disabilities consume food and drinks. We report on interviews, participatory observations, and analyzed the aids they currently use. Based on our findings, we derive a set of recommendations that supports researchers and practitioners in designing future robotic drinking and eating aids for people with disabilities.},
keywords = {assisted living technologies, human-centered computing, meal assistance, participation design, people with disabilities, robot assistive drinking, robot assistive feeding, user acceptance, user participation, user-centered design},
}
@article{Hoaglin1987,
author = { David C.   Hoaglin  and  Boris   Iglewicz },
title = {Fine-Tuning Some Resistant Rules for Outlier Labeling},
journal = {Journal of the American Statistical Association},
volume = {82},
number = {400},
pages = {1147-1149},
year  = {1987},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1987.10478551},

URL = { 
        
    
},
eprint = { 
        
    
}

}

@inproceedings{Pascher2019littlehelper,
title = {Little Helper: A Multi-Robot System in Home Health Care Environments},
author = {Max Pascher and Annalies Baumeister and Barbara Klein and Stefan Schneegass and Jens Gerken},
url = {https://hci.w-hs.de/pub_little_helper/, PDF Download
https://hal.archives-ouvertes.fr/hal-02128382},
year = {2019},
date = {2019-05-04},
booktitle = {Proceedings of the 2019 International workshop on Human-Drone Interaction (iHDI) as part of the ACM Conference on Human Factors in Computing Systems},
organization = {ACM},
abstract = {Being able to live independently and self-determined in once own home is a crucial factor for social participation. For people with severe physical impairments, such as tetraplegia, who cannot use their hands to manipulate materials or operate devices, life in their own home is only possible with assistance from others. The inability to operate buttons and other interfaces results also in not being able to utilize most assistive technologies on their own. In this paper, we present an ethnographic field study with 15 tetraplegics to better understand their living environments and needs. Results show the potential for robotic solutions but emphasize the need to support activities of daily living (ADL), such as grabbing and manipulating objects or opening doors. Based on this, we propose Little Helper, a tele-operated pack of robot drones, collaborating in a divide and conquer paradigm to fulfill several tasks using a unique interaction method. The drones can be tele-operated by a user through gaze-based selection and head motions and gestures manipulating materials and applications.},
keywords = {activities of daily living, ethnographic study, healthcare environment, multi-robot system, tetraplegics}
}

@article{Pollak.2020autonomystress,
title = {Stress in manual and autonomous modes of collaboration with a cobot},
journal = {Computers in Human Behavior},
volume = {112},
pages = {106469},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106469},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220302211},
author = {Anita Pollak and Mateusz Paliga and Matias M. Pulopulos and Barbara Kozusznik and Malgorzata W. Kozusznik},
keywords = {Collaborative robot, Cobot, Stress, Heart rate, Primary appraisal, Secondary appraisal},
abstract = {Working with collaborative robots (cobots) can be a potential source of stress for their operators. However, research on specific factors that affect users’ stress levels when working with a cobot is still scarce. This study is the first to investigate the levels of psychological (primary and secondary stress appraisal) and physiological (heart rate) stress in human operators working in two different cobot modes (i.e., manual and autonomous). We applied an experimental within-subject repeated-measures design to 45 healthy adults (26 women, 19 men). The results show that the levels of secondary stress appraisal were lower and the heart rate levels were higher in the autonomous cobot mode. The results suggest that, when working with a cobot, control plays a key role in the emotional, cognitive, and physiological reactions during the human-robot collaboration. Implications for organizational practice are discussed.}
}

@inproceedings{Andersen.projectingintention,
 author = {Andersen, Rasmus S. and Madsen, Ole and Moeslund, Thomas B. and Amor, Heni Ben},
 title = {{Projecting robot intentions into human environments}},
 keywords = {Augmented Reality;Collaborating;Intention;Projection},
 pages = {294--301},
 publisher = {IEEE},
 isbn = {978-1-5090-3929-6},
 booktitle = {{2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)}},
 year = {26.08.2016 - 31.08.2016},
 doi = {10.1109/ROMAN.2016.7745145},
 file = {http://ieeexplore.ieee.org/document/7745145/},
 file = {e199f4e5-614b-4194-b383-80275ec0d36d:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\e199f4e5-614b-4194-b383-80275ec0d36d.pdf:pdf}
}

@inproceedings{walker_2018_armotionintent,
author = {Walker, Michael and Hedayati, Hooman and Lee, Jennifer and Szafir, Daniel},
title = {Communicating Robot Motion Intent with Augmented Reality},
year = {2018},
isbn = {9781450349536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3171221.3171253},
doi = {10.1145/3171221.3171253},
abstract = {Humans coordinate teamwork by conveying intent through social cues, such as gestures and gaze behaviors. However, these methods may not be possible for appearance-constrained robots that lack anthropomorphic or zoomorphic features, such as aerial robots. We explore a new design space for communicating robot motion intent by investigating how augmented reality (AR) might mediate human-robot interactions. We develop a series of explicit and implicit designs for visually signaling robot motion intent using AR, which we evaluate in a user study. We found that several of our AR designs significantly improved objective task efficiency over a baseline in which users only received physically-embodied orientation cues. In addition, our designs offer several trade-offs in terms of intent clarity and user perceptions of the robot as a teammate.},
booktitle = {Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {316–324},
numpages = {9},
keywords = {mixed reality, robot intent, drones, interface design, virtuality continuum, arhmd, augmented reality, aerial robots, robots},
location = {Chicago, IL, USA},
series = {HRI '18}
}

@inproceedings{shindev_2012_intentexpression,
author = {Shindev, Ivan and Sun, Yu and Coovert, Michael and Pavlova, Jenny and Lee, Tiffany},
title = {Exploration of Intention Expression for Robots},
year = {2012},
isbn = {9781450310635},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2157689.2157778},
doi = {10.1145/2157689.2157778},
abstract = {This paper presents a novel exploration on how to enable a robot to express its intention so that the humans and robot can form a synergic relationship. A systematic design approach is proposed to obtain a set of possible intentions for a given robot from three levels of intentions. A visual intention expression system approach is developed to visualize the intentions and implemented on a mobile robot and a manipulator to demonstrate the intention expression concept.},
booktitle = {Proceedings of the Seventh Annual ACM/IEEE International Conference on Human-Robot Interaction},
pages = {247–248},
numpages = {2},
keywords = {augmented reality, robot, intention expression},
location = {Boston, Massachusetts, USA},
series = {HRI '12}
}

@INPROCEEDINGS{shrestha_2016_motionintent,  author={Shrestha, Moondeep C. and Kobayashi, Ayano and Onishi, Tomoya and Uno, Erika and Yanagawa, Hayato and Yokoyama, Yuta and Kamezaki, Mitsuhiro and Schmitz, Alexander and Sugano, Shigeki},  booktitle={2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},   title={Intent communication in navigation through the use of light and screen indicators},   year={2016},  volume={},  number={},  pages={523-524},  doi={10.1109/HRI.2016.7451837}}

@inproceedings{Chadalavada.2015floorprojection,
 author = {Chadalavada, Ravi Teja and Andreasson, Henrik and Krug, Robert and Lilienthal, Achim J.},
 title = {{That's on my mind! robot to human intention communication through on-board projection on shared floor space}},
 keywords = {Human Response;Intention;Trajectory},
 pages = {1--6},
 publisher = {IEEE},
 isbn = {978-1-4673-9163-4},
 booktitle = {{2015 European Conference on Mobile Robots (ECMR)}},
 year = {02.09.2015 - 04.09.2015},
 doi = {10.1109/ECMR.2015.7403771},
 file = {http://ieeexplore.ieee.org/document/7403771/},
 file = {aaf2df2a-ac45-4218-b9d6-a356cea9fc82:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\aaf2df2a-ac45-4218-b9d6-a356cea9fc82.pdf:pdf}
}

@inproceedings{Stulp.2015prediction,
 author = {Stulp, Freek and Grizou, Jonathan and Busch, Baptiste and Lopes, Manuel},
 title = {{Facilitating intention prediction for humans by optimizing robot motions}},
 keywords = {Intention},
 pages = {1249--1255},
 publisher = {IEEE},
 isbn = {978-1-4799-9994-1},
 booktitle = {{2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}},
 year = {28.09.2015 - 02.10.2015},
 doi = {10.1109/IROS.2015.7353529},
 file = {http://ieeexplore.ieee.org/document/7353529/},
 file = {e1f004f6-e80f-49ff-a0f8-de8be40e5c91:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\e1f004f6-e80f-49ff-a0f8-de8be40e5c91.pdf:pdf}
}

@inproceedings{Watanabe.2015navigationintent,
 author = {Watanabe, Atsushi and Ikeda, Tetsushi and Morales, Yoichi and Shinozawa, Kazuhiko and Miyashita, Takahiro and Hagita, Norihiro},
 title = {{Communicating robotic navigational intentions}},
 keywords = {Communication;Human Response;Intention;Interpredictability;Motion},
 pages = {5763--5769},
 publisher = {IEEE},
 isbn = {978-1-4799-9994-1},
 booktitle = {{2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}},
 year = {28.09.2015 - 02.10.2015},
 doi = {10.1109/IROS.2015.7354195},
 file = {http://ieeexplore.ieee.org/document/7354195/},
 file = {b69db6df-9ce4-49a4-a351-b0ac2240225f:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\b69db6df-9ce4-49a4-a351-b0ac2240225f.pdf:pdf}
}

@inproceedings{gruenefeld2020mind,
author = {Gruenefeld, Uwe and Pr\"{a}del, Lars and Illing, Jannike and Stratmann, Tim and Drolshagen, Sandra and Pfingsthorn, Max},
title = {Mind the ARm: Realtime Visualization of Robot Motion Intent in Head-Mounted Augmented Reality},
year = {2020},
isbn = {9781450375405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404983.3405509},
doi = {10.1145/3404983.3405509},
abstract = {Established safety sensor technology shuts down industrial robots when a collision
is detected, causing preventable loss of productivity. To minimize downtime, we implemented
three Augmented Reality (AR) visualizations (Path, Preview, and Volume) which allow
users to understand robot motion intent and give way to the robot. We compare the
different visualizations in a user study in which a small cognitive task is performed
in a shared workspace. We found that Preview and Path required significantly longer
head rotations to perceive robot motion intent. Volume, however, required the shortest
head rotation and was perceived as most safe, enabling closer proximity of the robot
arm before one left the shared workspace without causing shutdowns.},
booktitle = {Proceedings of the Conference on Mensch Und Computer},
pages = {259–266},
numpages = {8},
keywords = {hazard warning, robot motion intent, augmented reality, collaborative robots, visualization},
location = {Magdeburg, Germany},
series = {MuC '20}
}
@ARTICLE{kim2021,  author={Kim, Dae-Jin and Hazlett-Knudsen, Rebekah and Culver-Godfrey, Heather and Rucks, Greta and Cunningham, Tara and Portee, David and Bricout, John and Wang, Zhao and Behal, Aman},  journal={IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},   title={How Autonomy Impacts Performance and Satisfaction: Results From a Study With Spinal Cord Injured Subjects Using an Assistive Robot},   year={2012},  volume={42},  number={1},  pages={2-14},  doi={10.1109/TSMCA.2011.2159589}}

@article{braun_using_2006,
	title = {Using thematic analysis in psychology},
	volume = {3},
	issn = {1478-0887, 1478-0895},
	url = {http://www.tandfonline.com/doi/abs/10.1191/1478088706qp063oa},
	doi = {10.1191/14780887 06qp063oa},
	pages = {77--101},
	number = {2},
	journal = {Qualitative Research in Psychology},
	shortjournal = {Qualitative Research in Psychology},
	author = {Braun, Virginia and Clarke, Victoria},
	urldate = {2021-12-18},
	date = {2006-01},
	year = {2006},
	langid = {english}
}

@ARTICLE{graeser_2013,  author={Gräser, Axel and Heyer, Torsten and Fotoohi, Leila and Lange, Uwe and Kampe, Henning and Enjarini, Bashar and Heyer, Stefan and Fragkopoulos, Christos and Ristic-Durrant, Danijela},  journal={IEEE Robotics   Automation Magazine},   title={A Supportive FRIEND at Work: Robotic Workplace Assistance for the Disabled},   year={2013},  volume={20},  number={4},  pages={148-159},  doi={10.1109/MRA.2013.2275695}}

@incollection{hart1988,
  doi = {10.1016/s0166-4115(08)62386-9},
  url = {https://doi.org/10.1016/s0166-4115(08)62386-9},
  year = {1988},
  publisher = {Elsevier},
  pages = {139--183},
  author = {Sandra G. Hart and Lowell E. Staveland},
  title = {Development of {NASA}-{TLX} (Task Load Index): Results of Empirical and Theoretical Research},
  booktitle = {Advances in Psychology},
  address={Amsterdam, The Netherlands}
}

@article{Hart.2006,
 author = {Hart, Sandra G.},
 year = {2006},
 title = {{Nasa-Task Load Index (NASA-TLX); 20 Years Later}},
 keywords = {Analysis;Workload},
 pages = {904--908},
 volume = {50},
 number = {9},
 issn = {1541-9312},
 journal = {{Proceedings of the Human Factors and Ergonomics Society Annual Meeting}},
 doi = {10.1177/154193120605000909},
 file = {e239320f-7ea3-48dc-b1ab-934b21fce568:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\e239320f-7ea3-48dc-b1ab-934b21fce568.pdf:pdf}
}

@inproceedings{cleaver_2021,
author = {Cleaver, Andre and Tang, Darren Vincent and Chen, Victoria and Short, Elaine Schaertl and Sinapov, Jivko},
title = {Dynamic Path Visualization for Human-Robot Collaboration},
year = {2021},
isbn = {9781450382908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434074.3447188},
doi = {10.1145/3434074.3447188},
abstract = {Augmented reality technology can enable robots to visualize their future actions giving users crucial information to avoid collisions and other conflicting actions. Although a robot's entire action plan could be visualized (such as the output of a navigational planner), how far into the future it is appropriate to display the robot's plan is unknown. We developed a dynamic path visualizer that projects the robot's motion intent at varying lengths depending on the complexity of the upcoming path. We tested our approach in a virtual game where participants were tasked to collect and deliver gems to a robot that moves randomly towards a grid of markers in a confined area. Preliminary results on a small sample size indicate no significant effect on task performance; however, open-ended responses reveal participants preference towards visuals that show longer path projections.},
booktitle = {Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {339–343},
numpages = {5},
keywords = {mixed-reality, navigation, augmented-reality},
location = {Boulder, CO, USA},
series = {HRI '21 Companion}
}

@article{kim2018,
title = {Virtual reality sickness questionnaire (VRSQ): Motion sickness measurement index in a virtual reality environment},
journal = {Applied Ergonomics},
volume = {69},
pages = {66-73},
year = {2018},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2017.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S000368701730282X},
author = {Hyun K. Kim and Jaehyun Park and Yeongcheol Choi and Mungyeong Choe},
keywords = {Virtual reality, Motion sickness, Simulator sickness questionnaire},
abstract = {This study aims to develop a motion sickness measurement index in a virtual reality (VR) environment. The VR market is in an early stage of market formation and technological development, and thus, research on the side effects of VR devices such as simulator motion sickness is lacking. In this study, we used the simulator sickness questionnaire (SSQ), which has been traditionally used for simulator motion sickness measurement. To measure the motion sickness in a VR environment, 24 users performed target selection tasks using a VR device. The SSQ was administered immediately after each task, and the order of work was determined using the Latin square design. The existing SSQ was revised to develop a VR sickness questionnaire, which is used as the measurement index in a VR environment. In addition, the target selection method and button size were found to be significant factors that affect motion sickness in a VR environment. The results of this study are expected to be used for measuring and designing simulator sickness using VR devices in future studies.}
}

@inproceedings{10.1145/1518701.1519044,
author = {Spelmezan, Daniel and Jacobs, Mareike and Hilgers, Anke and Borchers, Jan},
title = {Tactile Motion Instructions for Physical Activities},
year = {2009},
isbn = {9781605582467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1518701.1519044},
doi = {10.1145/1518701.1519044},
abstract = {While learning new motor skills, we often rely on feedback from a trainer. Auditive feedback and demonstrations are used most frequently, but in many domains they are inappropriate or impractical. We introduce tactile instructions as an alternative to assist in correcting wrong posture during physical activities, and present a set of full-body vibrotactile patterns. An initial study informed the design of our tactile patterns, and determined appropriate locations for feedback on the body. A second experiment showed that users perceived and correctly classified our tactile instruction patterns in a relaxed setting and during a cognitively and physically demanding task. In a final experiment, snowboarders on the slope compared their perception of tactile instructions with audio instructions under real-world conditions. Tactile instructions achieved overall high recognition accuracy similar to audio instructions. Moreover, participants responded quicker to instructions delivered over the tactile channel than to instructions presented over the audio channel. Our findings suggest that these full-body tactile feedback patterns can replace audio instructions during physical activities.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2243–2252},
numpages = {10},
keywords = {motor skills, physical activities, real-time instructions, sports training, vibrotactile feedack},
location = {Boston, MA, USA},
series = {CHI '09}
}

@article{Goldreich.2007,
    doi = {10.1371/journal.pone.0000333},
    author = {Goldreich, Daniel},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {A Bayesian Perceptual Model Replicates the Cutaneous Rabbit and Other Tactile Spatiotemporal Illusions},
    year = {2007},
    month = {03},
    volume = {2},
    url = {https://doi.org/10.1371/journal.pone.0000333},
    pages = {1-11},
    abstract = {BackgroundWhen brief stimuli contact the skin in rapid succession at two or more locations, perception strikingly shrinks the intervening distance, and expands the elapsed time, between consecutive events. The origins of these perceptual space-time distortions are unknown.Methodology/Principal FindingsHere I show that these illusory effects, which I term perceptual length contraction and time dilation, are emergent properties of a Bayesian observer model that incorporates prior expectation for speed. Rapidly moving stimuli violate expectation, provoking perceptual length contraction and time dilation. The Bayesian observer replicates the cutaneous rabbit illusion, the tau effect, the kappa effect, and other spatiotemporal illusions. Additionally, it shows realistic tactile temporal order judgment and spatial attention effects.Conclusions/SignificanceThe remarkable explanatory power of this simple model supports the hypothesis, first proposed by Helmholtz, that the brain biases perception in favor of expectation. Specifically, the results suggest that the brain automatically incorporates prior expectation for speed in order to overcome spatial and temporal imprecision inherent in the sensorineural signal.},
    number = {3},

}

@inproceedings{McDaniel2011,

  year = {2011},
  month = nov,
  publisher = {{ACM}},
  author = {Troy McDaniel and Morris Goldberg and Daniel Villanueva and Lakshmie Narayan Viswanathan and Sethuraman Panchanathan},
  title = {Motor learning using a kinematic-vibrotactile mapping targeting fundamental movements},
  booktitle = {Proceedings of the 19th {ACM} international conference on Multimedia}
}
@inproceedings{Raisamo2009,
  doi = {10.1145/1647314.1647381},
  url = {https://doi.org/10.1145/1647314.1647381},
  year = {2009},
  month = nov,
  publisher = {{ACM}},
  author = {Jukka Raisamo and Roope Raisamo and Veikko Surakka},
  title = {Evaluating the effect of temporal parameters for vibrotactile saltatory patterns},
  booktitle = {Proceedings of the 2009 international conference on Multimodal interfaces}
}

@INPROCEEDINGS{Foresi2018,  author={Foresi, Gabriele and Freddi, Alessandro and Monteriù, Andrea and Ortenzi, Davide and Pagnotta, Daniele Proietti},  booktitle={2018 IEEE International Conference on Consumer Electronics (ICCE)},   title={Improving mobility and autonomy of disabled users via cooperation of assistive robots},   year={2018},  volume={},  number={},  pages={1-2},  doi={10.1109/ICCE.2018.8326291}}

@article{tsui2011want,
  title={“I want that”: Human-in-the-loop control of a wheelchair-mounted robotic arm},
  author={Tsui, Katherine M and Kim, Dae-Jin and Behal, Aman and Kontak, David and Yanco, Holly A},
  journal={Applied Bionics and Biomechanics},
  volume={8},
  number={1},
  pages={127--147},
  year={2011},
  publisher={IOS Press}
}


@inproceedings{Arevalo2021b,
title = {Reflecting upon Participatory Design in Human-Robot Collaboration for People with Motor Disabilities: Challenges and Lessons Learned from Three Multiyear Projects},
author = {Ar{\'e}valo Arboleda, Stephanie and Pascher, Max and Baumeister, Annalies and Klein, Barbara and Gerken, Jens},
url = {https://hci.w-hs.de/pub_petra_2021_cameraready/, PDF Download},
doi = {10.1145/3453892.3458044},
isbn = {978-1-4503-8792-7/21/06},
year = {2021},
date = {2021-06-29},
booktitle = {The 14th PErvasive Technologies Related to Assistive Environments Conference - PETRA 2021},
organization = {ACM},
abstract = {Human-robot technology has the potential to positively impact the lives of people with motor disabilities. However, current efforts have mostly been oriented towards technology (sensors, devices, modalities, interaction techniques), thus relegating the user and their valuable input to the wayside. In this paper, we aim to present a holistic perspective of the role of participatory design in Human-Robot Collaboration (HRC) for People with Motor Disabilities (PWMD). We have been involved in several multiyear projects related to HRC for PWMD, where we encountered different challenges related to planning and participation, preferences of stakeholders, using certain participatory design techniques, technology exposure, as well as ethical, legal, and social implications. These challenges helped us provide five lessons learned that could serve as a guideline to researchers when using participatory design with vulnerable groups. In particular, young researchers who are starting to explore HRC research for people with disabilities.},
keywords = {accessibility design, human-robot collaboration, lessons learned, participatory design},
pubstate = {published},
tppubtype = {inproceedings}
}

@inproceedings{goldau2019autonomous,
  title={Autonomous multi-sensory robotic assistant for a drinking task},
  author={Goldau, Felix Ferdinand and Shastha, Tejas Kumar and Kyrarini, Maria and Gr{\"a}ser, Axel},
  booktitle={2019 IEEE 16th International Conference on Rehabilitation Robotics (ICORR)},
  pages={210--216},
  year={2019},
  organization={IEEE}
}

@article{tanaka2014meal,
  title={Meal-assistance robot using ultrasonic motor with eye interface},
  author={Tanaka, Kanya and Mu, Shenglin and Nakashima, Shota},
  journal={International Journal of Automation Technology},
  volume={8},
  number={2},
  pages={186--192},
  year={2014},
  publisher={Fuji Technology Press Ltd.}
}

@article{mccoll2013brian,
  title={Brian 2.1: A socially assistive robot for the elderly and cognitively impaired},
  author={McColl, Derek and Louie, Wing-Yue Geoffrey and Nejat, Goldie},
  journal={IEEE Robotics \& Automation Magazine},
  volume={20},
  number={1},
  pages={74--83},
  year={2013},
  publisher={IEEE}
}
@inproceedings{guerreiro2019cabot,
  title={Cabot: Designing and evaluating an autonomous navigation robot for blind people},
  author={Guerreiro, Jo{\~a}o and Sato, Daisuke and Asakawa, Saki and Dong, Huixu and Kitani, Kris M and Asakawa, Chieko},
  booktitle={The 21st International ACM SIGACCESS conference on computers and accessibility},
  pages={68--82},
  year={2019}
}

@article{kyrarini2021survey,
  title={A survey of robots in healthcare},
  author={Kyrarini, Maria and Lygerakis, Fotios and Rajavenkatanarayanan, Akilesh and Sevastopoulos, Christos and Nambiappan, Harish Ram and Chaitanya, Kodur Krishna and Babu, Ashwin Ramesh and Mathew, Joanne and Makedon, Fillia},
  journal={Technologies},
  volume={9},
  number={1},
  pages={8},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{canal2016personalization,
  title={Personalization framework for adaptive robotic feeding assistance},
  author={Canal, Gerard and Aleny{\`a}, Guillem and Torras, Carme},
  booktitle={International conference on social robotics},
  pages={22--31},
  year={2016},
  organization={Springer}
}
@inproceedings{hashimoto2013field,
  title={A field study of the human support robot in the home environment},
  author={Hashimoto, Kunimatsu and Saito, Fuminori and Yamamoto, Takashi and Ikeda, Koichi},
  booktitle={2013 IEEE Workshop on Advanced Robotics and its Social Impacts},
  pages={143--150},
  year={2013},
  organization={IEEE}
}

@inproceedings{maheu2011evaluation,
  title={Evaluation of the JACO robotic arm: Clinico-economic study for powered wheelchair users with upper-extremity disabilities},
  author={Maheu, Veronique and Archambault, Philippe S and Frappier, Julie and Routhier, Fran{\c{c}}ois},
  booktitle={2011 IEEE International Conference on Rehabilitation Robotics},
  pages={1--5},
  year={2011},
  organization={IEEE}
}

@article{fattal2019sam,
  title={Sam, an assistive robotic device dedicated to helping persons with quadriplegia: Usability study},
  author={Fattal, Charles and Leynaert, Violaine and Laffont, Isabelle and Baillet, Axelle and Enjalbert, Michel and Leroux, Christophe},
  journal={International Journal of Social Robotics},
  volume={11},
  number={1},
  pages={89--103},
  year={2019},
  publisher={Springer}
}

@article{drolshagen2021acceptance,
  title={Acceptance of industrial collaborative robots by people with disabilities in sheltered workshops},
  author={Drolshagen, Sandra and Pfingsthorn, Max and Gliesche, Pascal and Hein, Andreas},
  journal={Frontiers in Robotics and AI},
  pages={173},
  year={2021},
  publisher={Frontiers}
}

@article{park2021hands,
  title={Hands-free human--robot interaction using multimodal gestures and deep learning in wearable mixed reality},
  author={Park, Kyeong-Beom and Choi, Sung Ho and Lee, Jae Yeol and Ghasemi, Yalda and Mohammed, Mustafa and Jeong, Heejin},
  journal={IEEE Access},
  volume={9},
  pages={55448--55464},
  year={2021},
  publisher={IEEE}
}

@inproceedings{kyrarini2019robot,
  title={Robot learning of assistive manipulation tasks by demonstration via head gesture-based interface},
  author={Kyrarini, Maria and Zheng, Quan and Haseeb, Muhammad Abdul and Gr{\"a}ser, Axel},
  booktitle={2019 IEEE 16th International Conference on Rehabilitation Robotics (ICORR)},
  pages={1139--1146},
  year={2019},
  organization={IEEE}
}

@article{zhang2019probabilistic,
  title={Probabilistic real-time user posture tracking for personalized robot-assisted dressing},
  author={Zhang, Fan and Cully, Antoine and Demiris, Yiannis},
  journal={IEEE Transactions on Robotics},
  volume={35},
  number={4},
  pages={873--888},
  year={2019},
  publisher={IEEE}
}

@article{zlatintsi2020support,
  title={I-Support: A robotic platform of an assistive bathing robot for the elderly population},
  author={Zlatintsi, Athanasia and Dometios, AC and Kardaris, Nikolaos and Rodomagoulakis, Isidoros and Koutras, Petros and Papageorgiou, X and Maragos, Petros and Tzafestas, Costas S and Vartholomeos, Panagiotis and Hauer, Klaus and others},
  journal={Robotics and Autonomous Systems},
  volume={126},
  pages={103451},
  year={2020},
  publisher={Elsevier}
}

@article{pascher2022my,
  title={My Caregiver the Cobot: Comparing Visualization Techniques to Effectively Communicate Cobot Perception to People with Physical Impairments},
  author={Pascher, Max and Kronhardt, Kirill and Franzen, Til and Gruenefeld, Uwe and Schneegass, Stefan and Gerken, Jens},
  journal={Sensors},
  volume={22},
  number={3},
  pages={755},
  year={2022},
  publisher={Multidisciplinary Digital Publishing Institute},
  URL = {https://www.mdpi.com/1424-8220/22/3/755},
  ISSN = {1424-8220},
  ABSTRACT = {Nowadays, robots are found in a growing number of areas where they collaborate closely with humans. Enabled by lightweight materials and safety sensors, these cobots are gaining increasing popularity in domestic care, where they support people with physical impairments in their everyday lives. However, when cobots perform actions autonomously, it remains challenging for human collaborators to understand and predict their behavior, which is crucial for achieving trust and user acceptance. One significant aspect of predicting cobot behavior is understanding their perception and comprehending how they &ldquo;see&rdquo; the world. To tackle this challenge, we compared three different visualization techniques for Spatial Augmented Reality. All of these communicate cobot perception by visually indicating which objects in the cobot&rsquo;s surrounding have been identified by their sensors. We compared the well-established visualizations Wedge and Halo against our proposed visualization Line in a remote user experiment with participants suffering from physical impairments. In a second remote experiment, we validated these findings with a broader non-specific user base. Our findings show that Line, a lower complexity visualization, results in significantly faster reaction times compared to Halo, and lower task load compared to both Wedge and Halo. Overall, users prefer Line as a more straightforward visualization. In Spatial Augmented Reality, with its known disadvantage of limited projection area size, established off-screen visualizations are not effective in communicating cobot perception and Line presents an easy-to-understand alternative.},
DOI = {10.3390/s22030755}
}

@inproceedings{arboleda2020understanding,
  title={Understanding human-robot collaboration for people with mobility Impairments at the Workplace, a thematic analysis},
  author={Ar{\'e}valo Arboleda, Stephanie and Pascher, Max and Lakhnati, Younes and Gerken, Jens},
  booktitle={2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
  pages={561--566},
  year={2020},
  organization={IEEE}
}

@article{arevalo2022does,
  title={Does One Size Fit All? A Case Study to Discuss Findings of an Augmented Hands-Free Robot Teleoperation Concept for People with and without Motor Disabilities},
  author={Ar{\'e}valo Arboleda, Stephanie and Becker, Marvin and Gerken, Jens},
  journal={Technologies},
  volume={10},
  number={1},
  pages={4},
  year={2022},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@Article{Grushko.2021tactile,
AUTHOR = {Grushko, Stefan and Vysocký, Aleš and Heczko, Dominik and Bobovský, Zdenko},
TITLE = {Intuitive Spatial Tactile Feedback for Better Awareness about Robot Trajectory during Human–Robot Collaboration},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5748},
URL = {https://www.mdpi.com/1424-8220/21/17/5748},
PubMedID = {34502639},
ISSN = {1424-8220},
ABSTRACT = {In this work, we extend the previously proposed approach of improving mutual perception during human–robot collaboration by communicating the robot’s motion intentions and status to a human worker using hand-worn haptic feedback devices. The improvement is presented by introducing spatial tactile feedback, which provides the human worker with more intuitive information about the currently planned robot’s trajectory, given its spatial configuration. The enhanced feedback devices communicate directional information through activation of six tactors spatially organised to represent an orthogonal coordinate frame: the vibration activates on the side of the feedback device that is closest to the future path of the robot. To test the effectiveness of the improved human–machine interface, two user studies were prepared and conducted. The first study aimed to quantitatively evaluate the ease of differentiating activation of individual tactors of the notification devices. The second user study aimed to assess the overall usability of the enhanced notification mode for improving human awareness about the planned trajectory of a robot. The results of the first experiment allowed to identify the tactors for which vibration intensity was most often confused by users. The results of the second experiment showed that the enhanced notification system allowed the participants to complete the task faster and, in general, improved user awareness of the robot’s movement plan, according to both objective and subjective data. Moreover, the majority of participants (82%) favoured the improved notification system over its previous non-directional version and vision-based inspection.},
DOI = {10.3390/s21175748}
}

@Article{Grushko.2021haptic,
AUTHOR = {Grushko, Stefan and Vysocký, Aleš and Oščádal, Petr and Vocetka, Michal and Novák, Petr and Bobovský, Zdenko},
TITLE = {Improved Mutual Understanding for Human-Robot Collaboration: Combining Human-Aware Motion Planning with Haptic Feedback Devices for Communicating Planned Trajectory},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3673},
URL = {https://www.mdpi.com/1424-8220/21/11/3673},
PubMedID = {34070528},
ISSN = {1424-8220},
ABSTRACT = {In a collaborative scenario, the communication between humans and robots is a fundamental aspect to achieve good efficiency and ergonomics in the task execution. A lot of research has been made related to enabling a robot system to understand and predict human behaviour, allowing the robot to adapt its motion to avoid collisions with human workers. Assuming the production task has a high degree of variability, the robot’s movements can be difficult to predict, leading to a feeling of anxiety in the worker when the robot changes its trajectory and approaches since the worker has no information about the planned movement of the robot. Additionally, without information about the robot’s movement, the human worker cannot effectively plan own activity without forcing the robot to constantly replan its movement. We propose a novel approach to communicating the robot’s intentions to a human worker. The improvement to the collaboration is presented by introducing haptic feedback devices, whose task is to notify the human worker about the currently planned robot’s trajectory and changes in its status. In order to verify the effectiveness of the developed human-machine interface in the conditions of a shared collaborative workspace, a user study was designed and conducted among 16 participants, whose objective was to accurately recognise the goal position of the robot during its movement. Data collected during the experiment included both objective and subjective parameters. Statistically significant results of the experiment indicated that all the participants could improve their task completion time by over 45\% and generally were more subjectively satisfied when completing the task with equipped haptic feedback devices. The results also suggest the usefulness of the developed notification system since it improved users’ awareness about the motion plan of the robot.},
DOI = {10.3390/s21113673}
}

%=================================================================
% This file was created with Citavi 6.5.0.0

@proceedings{.2327May1988,
 year = {23-27 May 1988},
 title = {{Proceedings of the IEEE 1988 National Aerospace and Electronics Conference}},
 publisher = {IEEE}
}

@misc{SchwerbehindertenStatistik,
title = "{{Disability Facts and Figures – Brief Report 2019 (nach "Statistik der schwerbehinderten Menschen -- Kurzbericht 2019")}}",
author = "{Statistisches Bundesamt (Destatis)}",
howpublished = "\url{https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Gesundheit/Behinderte-Menschen/Publikationen/Downloads-Behinderte-Menschen/sozial-schwerbehinderte-kb-5227101199004.html}",
year = 2020
}


@inproceedings{Mainprice.2010,
 author = {Mainprice, Jim and Sisbot, E. Akin and Sim{\'e}on, Thierry and Alam, Rachid},
 title = {{Planning safe and legible hand-over motions for human-robot interaction}},
 keywords = {Accessibility;Handover;Manipulation;Motion;Planning;Pose;Safety},
 booktitle = {{IARP/IEEE-RAS/EURON  workshop on technical challenges for dependable robots in human environments}},
 year = {2010},
 file = {https://hal.laas.fr/hal-01976223},
 file = {7a82c449-6b95-4898-9de6-84f5a3f04a94:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\7a82c449-6b95-4898-9de6-84f5a3f04a94.pdf:pdf}
}


@article{Li.2008,
 author = {Li, Xin and Hess, Traci J. and Valacich, Joseph S.},
 year = {2008},
 title = {{Why do we trust new technology? A study of initial trust formation with organizational information systems}},
 pages = {39--71},
 volume = {17},
 number = {1},
 issn = {09638687},
 journal = {{The Journal of Strategic Information Systems}},
 doi = {10.1016/j.jsis.2008.01.001},
 file = {8d135c6a-d036-4970-b4be-d12825344b96:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\8d135c6a-d036-4970-b4be-d12825344b96.pdf:pdf}
}


@inproceedings{Leutert.03.03.201306.03.2013,
 author = {Leutert, Florian and Herrmann, Christian and Schilling, Klaus},
 title = {{A Spatial Augmented Reality system for intuitive display of robotic data}},
 pages = {179--180},
 publisher = {IEEE},
 isbn = {978-1-4673-3101-2},
 booktitle = {{2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}},
 year = {03.03.2013 - 06.03.2013},
 doi = {10.1109/HRI.2013.6483560},
 file = {http://ieeexplore.ieee.org/document/6483560/},
 file = {0ae040f8-fe34-4221-9b7c-a54f07ed6f38:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\0ae040f8-fe34-4221-9b7c-a54f07ed6f38.pdf:pdf}
}


@article{Lee.2004,
 abstract = {Automation is often problematic because people fail to rely upon it appropriately. Because people respond to technology socially, trust influences reliance on automation. In particular, trust guides reliance when complexity and unanticipated situations make a complete understanding of the automation impractical. This review considers trust from the organizational, sociological, interpersonal, psychological, and neurological perspectives. It considers how the context, automation characteristics, and cognitive processes affect the appropriateness of trust. The context in which the automation is used influences automation performance and provides a goal-oriented perspective to assess automation characteristics along a dimension of attributional abstraction. These characteristics can influence trust through analytic, analogical, and affective processes. The challenges of extrapolating the concept of trust in people to trust in automation are discussed. A conceptual model integrates research regarding trust in automation and describes the dynamics of trust, the role of context, and the influence of display characteristics. Actual or potential applications of this research include improved designs of systems that require people to manage imperfect automation.},
 author = {Lee, John D. and See, Katrina A.},
 year = {2004},
 title = {{Trust in automation: designing for appropriate reliance}},
 pages = {50--80},
 volume = {46},
 number = {1},
 issn = {0018-7208},
 journal = {{Human factors}},
 doi = {10.1518/hfes.46.1.50{\textunderscore }30392},
 file = {39fc8379-4ed2-4b44-a04d-06adddbcfdf2:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\39fc8379-4ed2-4b44-a04d-06adddbcfdf2.pdf:pdf}
}


@inproceedings{Kulesza.15.09.201319.09.2013,
 author = {Kulesza, Todd and Stumpf, Simone and Burnett, Margaret and Yang, Sherry and Kwan, Irwin and Wong, Weng-Keen},
 title = {{Too much, too little, or just right? Ways explanations impact end users' mental models}},
 pages = {3--10},
 publisher = {IEEE},
 isbn = {978-1-4799-0369-6},
 booktitle = {{2013 IEEE Symposium on Visual Languages and Human Centric Computing}},
 year = {15.09.2013 - 19.09.2013},
 doi = {10.1109/VLHCC.2013.6645235},
 file = {2436b20e-a49a-49b7-83a2-062fe0b63e66:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\2436b20e-a49a-49b7-83a2-062fe0b63e66.pdf:pdf}
}


@article{Kim.2012,
 author = {Kim, Dae-Jin and Hazlett-Knudsen, Rebekah and Culver-Godfrey, Heather and Rucks, Greta and Cunningham, Tara and Portee, David and Bricout, John and Wang, Zhao and Behal, Aman},
 year = {2012},
 title = {{How Autonomy Impacts Performance and Satisfaction: Results From a Study With Spinal Cord Injured Subjects Using an Assistive Robot}},
 pages = {2--14},
 volume = {42},
 number = {1},
 issn = {1083-4427},
 journal = {{IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans}},
 doi = {10.1109/TSMCA.2011.2159589},
 file = {ae2d5c72-9623-44a7-a2b4-36c0683c6178:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\ae2d5c72-9623-44a7-a2b4-36c0683c6178.pdf:pdf}
}


@inproceedings{Malinverni.2017,
 author = {Malinverni, Laura and Maya, Julian and Schaper, Marie-Monique and Pares, Narcis},
 title = {{The World-as-Support}},
 keywords = {Augmented Reality;Cost;Embodied;HMD;Interaction;Portable;Projection;Recognize;SAR;Smartphone;Tablet;Window-on-the World;World-as-Support},
 pages = {5132--5144},
 publisher = {{ACM Press}},
 isbn = {9781450346559},
 editor = {Mark, Gloria and Fussell, Susan and Lampe, Cliff and schraefel, m.c and Hourcade, Juan Pablo and Appert, Caroline and Wigdor, Daniel},
 booktitle = {{Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems  - CHI '17}},
 year = {2017},
 address = {New York, New York, USA},
 doi = {10.1145/3025453.3025955},
 file = {0c9339fd-cd98-494e-b4b4-5ca36eb13558:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\0c9339fd-cd98-494e-b4b4-5ca36eb13558.pdf:pdf}
}


@proceedings{Kanda.2018,
 year = {2018},
 title = {{Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction  - HRI '18}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450349536},
 editor = {Kanda, Takayuki and Ŝabanovi{\'c}, Selma and Hoffman, Guy and Tapus, Adriana},
 doi = {10.1145/3171221}
}


@inproceedings{Hashimoto.2013,
 abstract = {As concern for Japan's aging population and shrinking workforce continues to mount, many researchers are focusing on nursing and healthcare-related robots to help ease the burden on society. In order to successfully integrate robots into the home environment to coexist with family members, three features are considered essential: safe interaction, a compact and lightweight body, and a simple interface. In this paper the authors introduce a robot that possesses these qualities: the human support robot (HSR). The current prototype HSR is designed to support independent living of persons with limited limb mobility. This paper discusses the development of the HSR and the results of user testing conducted in the homes of two persons with disabilities.},
 author = {Hashimoto, Kunimatsu and Saito, Fuminori and Yamamoto, Takashi and Ikeda, Koichi},
 title = {{A Field Study of the Human Support Robot in the Home Environment}},
 keywords = {Field Study;Social sciences;Support Robot},
 pages = {143--150},
 publisher = {IEEE},
 isbn = {978-1-4799-2368-7},
 booktitle = {{2013 IEEE Workshop on Advanced Robotics and Its Social Impacts (ARSO)}},
 year = {2013},
 address = {Piscataway, NJ},
 doi = {10.1109/ARSO.2013.6705520},
 file = {881537f2-987a-44b7-bbd0-19ec9dbe05af:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\881537f2-987a-44b7-bbd0-19ec9dbe05af.pdf:pdf}
}


@inproceedings{Harris.02.03.201005.03.2010,
 author = {Harris, John and Sharlin, Ehud},
 title = {{Exploring Emotive Actuation and its role in human-robot interaction}},
 keywords = {Human Response;Motion;Productive;Rapid;Repeating;Trust},
 pages = {95--96},
 publisher = {IEEE},
 isbn = {978-1-4244-4892-0},
 booktitle = {{2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}},
 year = {02.03.2010 - 05.03.2010},
 doi = {10.1109/HRI.2010.5453252},
 file = {f20f6db0-36ff-488c-b655-b41ea5843d55:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\f20f6db0-36ff-488c-b655-b41ea5843d55.pdf:pdf}
}


@article{Green.2008,
 author = {Green, Scott A. and Billinghurst, Mark and Chen, XiaoQi and Chase, J. Geoffrey},
 year = {2008},
 title = {{Human-Robot Collaboration: A Literature Review and Augmented Reality Approach in Design}},
 pages = {1},
 volume = {5},
 number = {1},
 issn = {1729-8814},
 journal = {{International Journal of Advanced Robotic Systems}},
 doi = {10.5772/5664},
 file = {251a12e8-c779-456b-8459-7cdf473cd9ee:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\251a12e8-c779-456b-8459-7cdf473cd9ee.pdf:pdf}
}


@inproceedings{Gong.2019,
 author = {Gong, L. L. and Ong, S. K. and Nee, A. Y. C.},
 title = {{Projection-based Augmented Reality Interface for Robot Grasping Tasks}},
 keywords = {Augmented Reality;Ease;Feedback;Pick-and-Place;Projection;Undesirable;Visualization},
 pages = {100--104},
 publisher = {{ACM Press}},
 isbn = {9781450371834},
 editor = {Unknown},
 booktitle = {{Proceedings of the 2019 4th International Conference on Robotics, Control and Automation  - ICRCA 2019}},
 year = {2019},
 address = {New York, New York, USA},
 doi = {10.1145/3351180.3351204},
 file = {http://dl.acm.org/citation.cfm?doid=3351180},
 file = {ac2312b7-6b5d-4da1-9e1d-b85b337a05d1:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\ac2312b7-6b5d-4da1-9e1d-b85b337a05d1.pdf:pdf}
}


@inproceedings{Ghiringhelli.14.09.201418.09.2014,
 author = {Ghiringhelli, Fabrizio and Guzzi, Jerome and {Di Caro}, Gianni A. and Caglioti, Vincenzo and Gambardella, Luca M. and Giusti, Alessandro},
 title = {{Interactive Augmented Reality for understanding and analyzing multi-robot systems}},
 pages = {1195--1201},
 publisher = {IEEE},
 isbn = {978-1-4799-6934-0},
 booktitle = {{2014 IEEE/RSJ International Conference on Intelligent Robots and Systems}},
 year = {14.09.2014 - 18.09.2014},
 doi = {10.1109/IROS.2014.6942709},
 file = {http://ieeexplore.ieee.org/document/6942709/},
 file = {2f5b0995-d746-41d5-b1d7-f0a8abad7e89:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\2f5b0995-d746-41d5-b1d7-f0a8abad7e89.pdf:pdf}
}


@article{Hoffman.2013,
 author = {Hoffman, Robert R. and Johnson, Matthew and Bradshaw, Jeffrey M. and Underbrink, Al},
 year = {2013},
 title = {{Trust in Automation}},
 pages = {84--88},
 volume = {28},
 number = {1},
 issn = {1541-1672},
 journal = {{IEEE Intelligent Systems}},
 doi = {10.1109/MIS.2013.24},
 file = {dc497211-fb43-4bb9-8fd2-6f1ff8ef6f78:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\dc497211-fb43-4bb9-8fd2-6f1ff8ef6f78.pdf:pdf}
}


@proceedings{Mark.2017,
 year = {2017},
 title = {{Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems  - CHI '17}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450346559},
 editor = {Mark, Gloria and Fussell, Susan and Lampe, Cliff and schraefel, m.c and Hourcade, Juan Pablo and Appert, Caroline and Wigdor, Daniel},
 doi = {10.1145/3025453}
}


@inproceedings{Matsumaru.06.09.200608.09.2006,
 author = {Matsumaru, Takafumi},
 title = {{Mobile Robot with Preliminary-announcement and Display Function of Forthcoming Motion using Projection Equipment}},
 pages = {443--450},
 publisher = {IEEE},
 isbn = {1-4244-0564-5},
 booktitle = {{ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication}},
 year = {06.09.2006 - 08.09.2006},
 doi = {10.1109/ROMAN.2006.314368},
 file = {http://ieeexplore.ieee.org/document/4107847/},
 file = {e56fc8b1-e768-4984-b408-7140d0718da1:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\e56fc8b1-e768-4984-b408-7140d0718da1.pdf:pdf}
}


@incollection{Mead.2010,
 author = {Mead, Ross and Mataric, Maja J.},
 title = {{Automated caricature of robot expressions in socially assistive human-robot interaction}},
 booktitle = {{The 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI2010) Workshop on What Do Collaborations with the Arts Have to Say about HRI}},
 year = {2010},
 file = {dd251165-57d1-4c5d-9d9a-2e9ce65baae7:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\dd251165-57d1-4c5d-9d9a-2e9ce65baae7.pdf:pdf}
}


@inproceedings{Watanabe.28.09.201502.10.2015,
 author = {Watanabe, Atsushi and Ikeda, Tetsushi and Morales, Yoichi and Shinozawa, Kazuhiko and Miyashita, Takahiro and Hagita, Norihiro},
 title = {{Communicating robotic navigational intentions}},
 keywords = {Communication;Human Response;Intention;Interpredictability;Motion},
 pages = {5763--5769},
 publisher = {IEEE},
 isbn = {978-1-4799-9994-1},
 booktitle = {{2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}},
 year = {28.09.2015 - 02.10.2015},
 doi = {10.1109/IROS.2015.7354195},
 file = {http://ieeexplore.ieee.org/document/7354195/},
 file = {b69db6df-9ce4-49a4-a351-b0ac2240225f:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\b69db6df-9ce4-49a4-a351-b0ac2240225f.pdf:pdf}
}


@inproceedings{Walker.2018,
 author = {Walker, Michael and Hedayati, Hooman and Lee, Jennifer and Szafir, Daniel},
 title = {{Communicating Robot Motion Intent with Augmented Reality}},
 keywords = {Augmented Reality;Framework;Human-AI Interaction;Interpredictability},
 pages = {316--324},
 publisher = {{ACM Press}},
 isbn = {9781450349536},
 editor = {Kanda, Takayuki and Ŝabanovi{\'c}, Selma and Hoffman, Guy and Tapus, Adriana},
 booktitle = {{Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction  - HRI '18}},
 year = {2018},
 address = {New York, New York, USA},
 doi = {10.1145/3171221.3171253},
 file = {http://dl.acm.org/citation.cfm?doid=3171221},
 file = {cd3f0421-bcdd-46fe-b224-b29c39598acf:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\cd3f0421-bcdd-46fe-b224-b29c39598acf.pdf:pdf}
}


@proceedings{Unknown.2019,
 year = {2019},
 title = {{Proceedings of the 2019 4th International Conference on Robotics, Control and Automation  - ICRCA 2019}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450371834},
 editor = {Unknown},
 doi = {10.1145/3351180}
}


@inproceedings{Takayama.2011,
 author = {Takayama, Leila and Dooley, Doug and Ju, Wendy},
 title = {{Expressing thought}},
 keywords = {Communication;Human Response;Implicit;Interpredictability;Planning;Simultaenously;Thinking},
 pages = {69},
 publisher = {{ACM Press}},
 isbn = {9781450305617},
 editor = {Billard, Aude and Kahn, Peter and Adams, Julie A. and Trafton, Greg},
 booktitle = {{Proceedings of the 6th international conference on Human-robot interaction - HRI '11}},
 year = {2011},
 address = {New York, New York, USA},
 doi = {10.1145/1957656.1957674},
 file = {http://portal.acm.org/citation.cfm?doid=1957656},
 file = {89108c5a-b4a2-4f0a-9fe0-61786b3d31e5:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\89108c5a-b4a2-4f0a-9fe0-61786b3d31e5.pdf:pdf}
}


@article{Szafir.2017,
 author = {Szafir, Daniel and Mutlu, Bilge and Fong, Terrence},
 year = {2017},
 title = {{Designing planning and control interfaces to support user collaboration with flying robots}},
 pages = {514--542},
 volume = {36},
 number = {5-7},
 issn = {0278-3649},
 journal = {{The International Journal of Robotics Research}},
 doi = {10.1177/0278364916688256},
 file = {2d48fd85-1314-4459-8c49-da1b57aa78f9:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\2d48fd85-1314-4459-8c49-da1b57aa78f9.pdf:pdf}
}


@inproceedings{Stulp.28.09.201502.10.2015,
 author = {Stulp, Freek and Grizou, Jonathan and Busch, Baptiste and Lopes, Manuel},
 title = {{Facilitating intention prediction for humans by optimizing robot motions}},
 keywords = {Intention},
 pages = {1249--1255},
 publisher = {IEEE},
 isbn = {978-1-4799-9994-1},
 booktitle = {{2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}},
 year = {28.09.2015 - 02.10.2015},
 doi = {10.1109/IROS.2015.7353529},
 file = {http://ieeexplore.ieee.org/document/7353529/},
 file = {e1f004f6-e80f-49ff-a0f8-de8be40e5c91:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\e1f004f6-e80f-49ff-a0f8-de8be40e5c91.pdf:pdf}
}


@inproceedings{Stadler.26.08.201631.08.2016,
 author = {Stadler, Susanne and Kain, Kevin and Giuliani, Manuel and Mirnig, Nicole and Stollnberger, Gerald and Tscheligi, Manfred},
 title = {{Augmented reality for industrial robot programmers: Workload analysis for task-based, augmented reality-supported robot control}},
 pages = {179--184},
 publisher = {IEEE},
 isbn = {978-1-5090-3929-6},
 booktitle = {{2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)}},
 year = {26.08.2016 - 31.08.2016},
 doi = {10.1109/ROMAN.2016.7745108},
 file = {f00b2764-3b9a-4d2b-a77b-43abc1f451f0:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\f00b2764-3b9a-4d2b-a77b-43abc1f451f0.pdf:pdf}
}


@article{Shinozawa.2005,
 author = {Shinozawa, Kazuhiko and Naya, Futoshi and Yamato, Junji and Kogure, Kiyoshi},
 year = {2005},
 title = {{Differences in effect of robot and screen agent recommendations on human decision-making}},
 pages = {267--279},
 volume = {62},
 number = {2},
 issn = {10715819},
 journal = {{International Journal of Human-Computer Studies}},
 doi = {10.1016/j.ijhcs.2004.11.003},
 file = {0c496084-dcda-4bc5-9bb1-b09016d3ef69:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\0c496084-dcda-4bc5-9bb1-b09016d3ef69.pdf:pdf}
}


@article{SeverinsonEklundh.2003,
 author = {Severinson-Eklundh, Kerstin and Green, Anders and H{\"u}ttenrauch, Helge},
 year = {2003},
 title = {{Social and collaborative aspects of interaction with a service robot}},
 pages = {223--234},
 volume = {42},
 number = {3-4},
 issn = {09218890},
 journal = {{Robotics and Autonomous Systems}},
 doi = {10.1016/S0921-8890(02)00377-9},
 file = {608bbb4e-09bf-432a-8ecc-6bb03a674042:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\608bbb4e-09bf-432a-8ecc-6bb03a674042.pdf:pdf}
}


@article{Romann.2014,
 author = {Ro{\ss}mann, J{\"u}rgen and {Guiffo Kaigom}, Eric and Atorf, Linus and Rast, Malte and Grinshpun, Georgij and Schlette, Christian},
 year = {2014},
 title = {{Mental Models for Intelligent Systems: eRobotics Enables New Approaches to Simulation-Based AI}},
 pages = {101--110},
 volume = {28},
 number = {2},
 issn = {0933-1875},
 journal = {{KI - K{\"u}nstliche Intelligenz}},
 doi = {10.1007/s13218-014-0298-z},
 file = {473acbb6-3b7d-4ee9-95a5-23e1f26a67fb:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\473acbb6-3b7d-4ee9-95a5-23e1f26a67fb.pdf:pdf}
}


@inproceedings{Rea.2017,
 author = {Rea, Daniel J. and Seo, Stela H. and Bruce, Neil and Young, James E.},
 title = {{Movers, Shakers, and Those Who Stand Still}},
 pages = {398--407},
 publisher = {{ACM Press}},
 isbn = {9781450343367},
 editor = {Mutlu, Bilge and Tscheligi, Manfred and Weiss, Astrid and Young, James E.},
 booktitle = {{Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction - HRI '17}},
 year = {2017},
 address = {New York, New York, USA},
 doi = {10.1145/2909824.3020246},
 file = {http://dl.acm.org/citation.cfm?doid=2909824},
 file = {f83c75fb-8116-4a56-a6d8-dfb4abae6eb3:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\f83c75fb-8116-4a56-a6d8-dfb4abae6eb3.pdf:pdf}
}


@inproceedings{Pascher.2019,
 author = {Pascher, Max and Baumeister, Annalies and Klein, Barbara and Schneegass, Stefan and Gerken, Jens},
 title = {{Little Helper: A Multi-Robot System in Home Health Care Environments}},
 url = {https://hal.archives-ouvertes.fr/hal-02128382},
 keywords = {activities of daily living;ethnographic study;healthcare environment;multi robot system;people with disabilities},
 booktitle = {{1st International Workshop on Human-Drone Interaction}},
 year = {2019},
 address = {Glasgow, United Kingdom},
 file = {f0d081e3-af42-4b61-99ee-45e68e685d5f:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\f0d081e3-af42-4b61-99ee-45e68e685d5f.pdf:pdf}
}


@proceedings{Mutlu.2017,
 year = {2017},
 title = {{Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction - HRI '17}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450343367},
 editor = {Mutlu, Bilge and Tscheligi, Manfred and Weiss, Astrid and Young, James E.},
 doi = {10.1145/2909824}
}


@inproceedings{Milgram.2630July1993,
 author = {Milgram, P. and Zhai, S. and Drascic, D. and Grodski, J.},
 title = {{Applications of augmented reality for human-robot communication}},
 pages = {1467--1472},
 publisher = {IEEE},
 isbn = {0-7803-0823-9},
 booktitle = {{Proceedings of 1993 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS '93)}},
 year = {26-30 July 1993},
 doi = {10.1109/IROS.1993.583833},
 file = {http://ieeexplore.ieee.org/document/583833/},
 file = {2beadf54-ff5c-40d3-9912-8b8729d09158:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\2beadf54-ff5c-40d3-9912-8b8729d09158.pdf:pdf}
}



@article{Fong.2003b,
 author = {Fong, T. and Thorpe, C. and Baur, C.},
 year = {2003},
 title = {{Multi-robot remote driving with collaborative control}},
 pages = {699--704},
 volume = {50},
 number = {4},
 issn = {0278-0046},
 journal = {{IEEE Transactions on Industrial Electronics}},
 doi = {10.1109/TIE.2003.814768},
 file = {51b39923-a1b4-4812-ae41-a0bc0bb93203:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\51b39923-a1b4-4812-ae41-a0bc0bb93203.pdf:pdf}
}


@article{Fattal.2019,
 author = {Fattal, Charles and Leynaert, Violaine and Laffont, Isabelle and Baillet, Axelle and Enjalbert, Michel and Leroux, Christophe},
 year = {2019},
 title = {{SAM, an Assistive Robotic Device Dedicated to Helping Persons with Quadriplegia: Usability Study}},
 pages = {89--103},
 volume = {11},
 number = {1},
 issn = {1875-4791},
 journal = {{International Journal of Social Robotics}},
 doi = {10.1007/s12369-018-0482-7},
 file = {3e30b478-2f79-4e32-b355-bc3ed681cfb4:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\3e30b478-2f79-4e32-b355-bc3ed681cfb4.pdf:pdf}
}


@article{Fang.2012,
 author = {Fang, H. C. and Ong, S. K. and Nee, A.Y.C.},
 year = {2012},
 title = {{Interactive robot trajectory planning and simulation using Augmented Reality}},
 keywords = {Augmented Reality;Planning;Trajectory},
 pages = {227--237},
 volume = {28},
 number = {2},
 issn = {07365845},
 journal = {{Robotics and Computer-Integrated Manufacturing}},
 doi = {10.1016/j.rcim.2011.09.003},
 file = {e4e69fb3-8073-46be-9d8d-1c3506cc5ecf:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\e4e69fb3-8073-46be-9d8d-1c3506cc5ecf.pdf:pdf}
}


@proceedings{.26.08.201631.08.2016,
 year = {26.08.2016 - 31.08.2016},
 title = {{2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)}},
 publisher = {IEEE},
 isbn = {978-1-5090-3929-6}
}


@proceedings{.28.09.201502.10.2015,
 year = {28.09.2015 - 02.10.2015},
 title = {{2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}},
 publisher = {IEEE},
 isbn = {978-1-4799-9994-1}
}


@proceedings{.02.09.201504.09.2015,
 year = {02.09.2015 - 04.09.2015},
 title = {{2015 European Conference on Mobile Robots (ECMR)}},
 publisher = {IEEE},
 isbn = {978-1-4673-9163-4}
}


@proceedings{.14.09.201418.09.2014,
 year = {14.09.2014 - 18.09.2014},
 title = {{2014 IEEE/RSJ International Conference on Intelligent Robots and Systems}},
 publisher = {IEEE},
 isbn = {978-1-4799-6934-0}
}


@proceedings{.15.09.201319.09.2013,
 year = {15.09.2013 - 19.09.2013},
 title = {{2013 IEEE Symposium on Visual Languages and Human Centric Computing}},
 publisher = {IEEE},
 isbn = {978-1-4799-0369-6}
}


@proceedings{.June24282013,
 year = {June 24-28, 2013},
 title = {{Robotics: Science and Systems IX}},
 publisher = {{Robotics: Science and Systems Foundation}},
 isbn = {9789810739379},
 doi = {10.15607/RSS.2013.IX}
}


@proceedings{.03.03.201306.03.2013,
 year = {03.03.2013 - 06.03.2013},
 title = {{2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}},
 publisher = {IEEE},
 isbn = {978-1-4673-3101-2}
}


@proceedings{.2013,
 year = {2013},
 title = {{2013 IEEE Workshop on Advanced Robotics and Its Social Impacts (ARSO)}},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4799-2368-7}
}


@proceedings{.02.03.201005.03.2010,
 year = {02.03.2010 - 05.03.2010},
 title = {{2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}},
 publisher = {IEEE},
 isbn = {978-1-4244-4892-0}
}


@book{.2010,
 year = {2010},
 title = {{The 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI2010) Workshop on What Do Collaborations with the Arts Have to Say about HRI}}
}


@proceedings{.2010b,
 year = {2010},
 title = {{IARP/IEEE-RAS/EURON  workshop on technical challenges for dependable robots in human environments}}
}


@proceedings{.06.09.200608.09.2006,
 year = {06.09.2006 - 08.09.2006},
 title = {{ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication}},
 publisher = {IEEE},
 isbn = {1-4244-0564-5}
}


@proceedings{.March2006,
 year = {March 2006},
 title = {{9th IEEE International Workshop on Advanced Motion Control, 2006}},
 publisher = {IEEE},
 isbn = {0-7803-9511-1}
}


@proceedings{.02.08.200502.08.2005,
 year = {02.08.2005 - 02.08.2005},
 title = {{2005 IEEE/RSJ International Conference on Intelligent Robots and Systems}},
 publisher = {IEEE},
 isbn = {0-7803-8912-3}
}


@proceedings{.2630July1993,
 year = {26-30 July 1993},
 title = {{Proceedings of 1993 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS '93)}},
 publisher = {IEEE},
 isbn = {0-7803-0823-9}
}


@proceedings{Adams.2015,
 year = {2015},
 title = {{Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction - HRI '15}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450328838},
 editor = {Adams, Julie A. and Smart, William and Mutlu, Bilge and Takayama, Leila},
 doi = {10.1145/2696454}
}


@inproceedings{Woods.06.09.200608.09.2006,
 author = {Woods, Sarah and Walters, Michael and Koay, Kheng and Dautenhahn, Kerstin},
 title = {{Methodological Issues in HRI: A Comparison of Live and Video-Based Methods in Robot to Human Approach Direction Trials}},
 pages = {51--58},
 publisher = {IEEE},
 isbn = {1-4244-0564-5},
 booktitle = {{ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication}},
 year = {06.09.2006 - 08.09.2006},
 doi = {10.1109/ROMAN.2006.314394},
 file = {8424bd14-2ff4-42ad-84aa-0da649b710e6:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\8424bd14-2ff4-42ad-84aa-0da649b710e6.pdf:pdf}
}


@inproceedings{Amershi.2019,
 author = {Amershi, Saleema and Inkpen, Kori and Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N.},
 title = {{Guidelines for Human-AI Interaction}},
 keywords = {Accuracy;Correction;Explanation;Guidelines;Support},
 pages = {1--13},
 publisher = {{ACM Press}},
 isbn = {9781450359702},
 editor = {Brewster, Stephen and Fitzpatrick, Geraldine and Cox, Anna and Kostakos, Vassilis},
 booktitle = {{Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems  - CHI '19}},
 year = {2019},
 address = {New York, New York, USA},
 doi = {10.1145/3290605.3300233},
 file = {http://dl.acm.org/citation.cfm?doid=3290605},
 file = {b5ec8b28-e1e9-444a-b579-9fdf3f77580b:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\b5ec8b28-e1e9-444a-b579-9fdf3f77580b.pdf:pdf}
}


@book{Anderson.2007,
 author = {Anderson, Rosemarie},
 year = {2007},
 title = {{Thematic content analysis (TCA): Descriptive presentation of qualitative data}},
 file = {e8d62d84-3599-446f-89e7-bf1259fbcb5c:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\e8d62d84-3599-446f-89e7-bf1259fbcb5c.pdf:pdf}
}


@misc{Ertel.2016,
 author = {Ertel, Wolfgang and Winter, Maik H.-J. and Rau, Harald},
 year = {2016},
 title = {{Assistenzroboter f{\"u}r Menschen mit k{\"o}rperlicher Behinderung}},
 url = {http://asrobe.hs-weingarten.de/},
 urldate = {16.09.2019}
}


@inproceedings{Endsley.2327May1988,
 author = {Endsley, M. R.},
 title = {{Situation awareness global assessment technique (SAGAT)}},
 pages = {789--795},
 publisher = {IEEE},
 booktitle = {{Proceedings of the IEEE 1988 National Aerospace and Electronics Conference}},
 year = {23-27 May 1988},
 doi = {10.1109/NAECON.1988.195097},
 file = {c9bad115-a23c-407e-b84f-65572f54ae59:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\c9bad115-a23c-407e-b84f-65572f54ae59.pdf:pdf}
}


@proceedings{EcoleNationaledelAviationCivileENAC.2019,
 year = {2019},
 title = {{1st International Workshop on Human-Drone Interaction}},
 address = {Glasgow, United Kingdom},
 institution = {{Ecole Nationale de l'Aviation Civile [ENAC]}}
}


@inproceedings{Dragan.2015,
 author = {Dragan, Anca D. and Bauman, Shira and Forlizzi, Jodi and Srinivasa, Siddhartha S.},
 title = {{Effects of Robot Motion on Human-Robot Collaboration}},
 keywords = {Collaborating;Human Response;Interpredictability;Motion;Planning},
 pages = {51--58},
 publisher = {{ACM Press}},
 isbn = {9781450328838},
 editor = {Adams, Julie A. and Smart, William and Mutlu, Bilge and Takayama, Leila},
 booktitle = {{Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction - HRI '15}},
 year = {2015},
 address = {New York, New York, USA},
 doi = {10.1145/2696454.2696473},
 file = {754b4fd1-cc0e-4e94-90e2-4ea792d688ca:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\754b4fd1-cc0e-4e94-90e2-4ea792d688ca.pdf:pdf;0ec707b8-a566-47b9-b661-60e6ded446fb:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\0ec707b8-a566-47b9-b661-60e6ded446fb.pdf:pdf}
}


@inproceedings{Dragan.June24282013,
 author = {Dragan, Anca and Srinivasa, Siddhartha},
 title = {{Generating Legible Motion}},
 publisher = {{Robotics: Science and Systems Foundation}},
 isbn = {9789810739379},
 booktitle = {{Robotics: Science and Systems IX}},
 year = {June 24-28, 2013},
 doi = {10.15607/RSS.2013.IX.024},
 file = {http://www.roboticsproceedings.org/rss09/index.html},
 file = {8142326d-b593-4bb4-8ebe-97481be1aa8c:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\8142326d-b593-4bb4-8ebe-97481be1aa8c.pdf:pdf}
}


@article{Dragan.2015b,
 author = {Dragan, Anca and Holladay, Rachel and Srinivasa, Siddhartha},
 year = {2015},
 title = {{Deceptive robot motion: synthesis, analysis and experiments}},
 pages = {331--345},
 volume = {39},
 number = {3},
 issn = {0929-5593},
 journal = {{Autonomous Robots}},
 doi = {10.1007/s10514-015-9458-8},
 file = {d10bec77-bb14-4470-8afc-0b6fae431913:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\d10bec77-bb14-4470-8afc-0b6fae431913.pdf:pdf;95f7b346-5630-4230-bd4e-cfd9fe2b2bf9:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\95f7b346-5630-4230-bd4e-cfd9fe2b2bf9.pdf:pdf}
}


@article{Dietz.2011,
 author = {Dietz, Graham},
 year = {2011},
 title = {{Going back to the source: Why do people trust each other?}},
 pages = {215--222},
 volume = {1},
 number = {2},
 issn = {2151-5581},
 journal = {{Journal of Trust Research}},
 doi = {10.1080/21515581.2011.603514},
 file = {6365e71d-2516-47dd-9f90-7423db3727ee:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\6365e71d-2516-47dd-9f90-7423db3727ee.pdf:pdf}
}


@article{Dautenhahn.2007,
 abstract = {Social intelligence in robots has a quite recent history in artificial intelligence and robotics. However, it has become increasingly apparent that social and interactive skills are necessary requirements in many application areas and contexts where robots need to interact and collaborate with other robots or humans. Research on human-robot interaction (HRI) poses many challenges regarding the nature of interactivity and 'social behaviour' in robot and humans. The first part of this paper addresses dimensions of HRI, discussing requirements on social skills for robots and introducing the conceptual space of HRI studies. In order to illustrate these concepts, two examples of HRI research are presented. First, research is surveyed which investigates the development of a cognitive robot companion. The aim of this work is to develop social rules for robot behaviour (a 'robotiquette') that is comfortable and acceptable to humans. Second, robots are discussed as possible educational or therapeutic toys for children with autism. The concept of interactive emergence in human-child interactions is highlighted. Different types of play among children are discussed in the light of their potential investigation in human-robot experiments. The paper concludes by examining different paradigms regarding 'social relationships' of robots and people interacting with them.},
 author = {Dautenhahn, Kerstin},
 year = {2007},
 title = {{Socially intelligent robots: dimensions of human-robot interaction}},
 pages = {679--704},
 volume = {362},
 number = {1480},
 issn = {0962-8436},
 journal = {{Philosophical transactions of the Royal Society of London. Series B, Biological sciences}},
 doi = {10.1098/rstb.2006.2004},
 file = {35b922f4-af87-46d6-b94d-4b22ebcb1d9d:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\35b922f4-af87-46d6-b94d-4b22ebcb1d9d.pdf:pdf}
}


@article{Chen.2013,
 author = {Chen, Tiffany L. and Ciocarlie, Matei and Cousins, Steve and Grice, Phillip M. and Hawkins, Kelsey and Hsiao, Kaijen and Kemp, Charles C. and King, Chih-Hung and Lazewatsky, Daniel A. and Leeper, Adam E. and Nguyen, Hai and Paepcke, Andreas and Pantofaru, Caroline and Smart, William D. and Takayama, Leila},
 year = {2013},
 title = {{Robots for humanity: using assistive robotics to empower people with disabilities}},
 pages = {30--39},
 volume = {20},
 number = {1},
 issn = {1070-9932},
 journal = {{IEEE Robotics {\&} Automation Magazine}},
 doi = {10.1109/MRA.2012.2229950},
 file = {e8b4f7a6-f6c3-41e6-b8d7-bea58da59953:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\e8b4f7a6-f6c3-41e6-b8d7-bea58da59953.pdf:pdf}
}


@inproceedings{Chadalavada.2015,
 author = {Chadalavada, Ravi Teja and Andreasson, Henrik and Krug, Robert and Lilienthal, Achim J.},
 title = {{That's on my mind! robot to human intention communication through on-board projection on shared floor space}},
 keywords = {Human Response;Intention;Trajectory},
 pages = {1--6},
 publisher = {IEEE},
 isbn = {978-1-4673-9163-4},
 booktitle = {{2015 European Conference on Mobile Robots (ECMR)}},
 year = {02.09.2015 - 04.09.2015},
 doi = {10.1109/ECMR.2015.7403771},
 file = {http://ieeexplore.ieee.org/document/7403771/},
 file = {aaf2df2a-ac45-4218-b9d6-a356cea9fc82:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\aaf2df2a-ac45-4218-b9d6-a356cea9fc82.pdf:pdf}
}


@proceedings{Brewster.2019,
 year = {2019},
 title = {{Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems  - CHI '19}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450359702},
 editor = {Brewster, Stephen and Fitzpatrick, Geraldine and Cox, Anna and Kostakos, Vassilis},
 doi = {10.1145/3290605}
}


@inproceedings{Breazeal.02.08.200502.08.2005,
 author = {Breazeal, Cynthia and Kidd, Cory D. and Thomaz, Andrea Lockerd and Hoffman, Guy and Berlin, Matt},
 title = {{Effects of nonverbal communication on efficiency and robustness in human-robot teamwork}},
 keywords = {Collaborating;Communication;Coordinating;Explicit;Human Response;Implicit;Information},
 pages = {708--713},
 publisher = {IEEE},
 isbn = {0-7803-8912-3},
 booktitle = {{2005 IEEE/RSJ International Conference on Intelligent Robots and Systems}},
 year = {02.08.2005 - 02.08.2005},
 doi = {10.1109/IROS.2005.1545011},
 file = {99fca1af-2fce-44db-9a95-55af16557947:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\99fca1af-2fce-44db-9a95-55af16557947.pdf:pdf;aa396920-c3fc-469d-bced-cb0a4285b854:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\aa396920-c3fc-469d-bced-cb0a4285b854.pdf:pdf}
}

@article{Coovert.2014,
 author = {Coovert, Michael D. and Lee, Tiffany and Shindev, Ivan and Sun, Yu},
 year = {2014},
 title = {{Spatial augmented reality as a method for a mobile robot to communicate intended movement}},
 keywords = {Intention;Projection},
 pages = {241--248},
 volume = {34},
 issn = {07475632},
 journal = {{Computers in Human Behavior}},
 doi = {10.1016/j.chb.2014.02.001},
 file = {3de00430-c980-44b8-a6a1-bd52d9586ac3:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\3de00430-c980-44b8-a6a1-bd52d9586ac3.pdf:pdf}
}

@article{Rosen.2019,
author = {Eric Rosen and David Whitney and Elizabeth Phillips and Gary Chien and James Tompkin and George Konidaris and Stefanie Tellex},
title ={Communicating and controlling robot arm motion intent through mixed-reality head-mounted displays},
journal = {The International Journal of Robotics Research},
volume = {38},
number = {12-13},
pages = {1513-1526},
year = {2019},
doi = {10.1177/0278364919842925},

URL = {},
eprint = {}
,
    abstract = { Efficient motion intent communication is necessary for safe and collaborative work environments with co-located humans and robots. Humans efficiently communicate their motion intent to other humans through gestures, gaze, and other non-verbal cues, and can replan their motions in response. However, robots often have difficulty using these methods. Many existing methods for robot motion intent communication rely on 2D displays, which require the human to continually pause their work to check a visualization. We propose a mixed-reality head-mounted display (HMD) visualization of the intended robot motion over the wearer’s real-world view of the robot and its environment. In addition, our interface allows users to adjust the intended goal pose of the end effector using hand gestures. We describe its implementation, which connects a ROS-enabled robot to the HoloLens using ROS Reality, using MoveIt for motion planning, and using Unity to render the visualization. To evaluate the effectiveness of this system against a 2D display visualization and against no visualization, we asked 32 participants to label various arm trajectories as either colliding or non-colliding with blocks arranged on a table. We found a 15\% increase in accuracy with a 38\% decrease in the time it took to complete the task compared with the next best system. These results demonstrate that a mixed-reality HMD allows a human to determine where the robot is going to move more quickly and accurately than existing baselines. }
}

@book{Bimber.2005,
 abstract = {Like virtual reality, augmented reality is becoming an emerging platform in new application areas for museums, edutainment, home entertainment, research, industry, and the art communities using novel approaches which have taken augmented reality beyond traditional eye-worn or hand-held displays. In this book, the authors discuss spatial augmented reality approaches that exploit optical elements, video projectors, holograms, radio frequency tags, and tracking technology, as well as interactive rendering algorithms and calibration techniques in order to embed synthetic supplements into the real},
 author = {Bimber, Oliver and Raskar, Ramesh},
 year = {2005},
 title = {{Spatial augmented reality: Merging real and virtual worlds}},
 url = {},
 address = {Wellesley, Mass},
 publisher = {{A K Peters}},
 isbn = {1568812302},
 institution = {{ebrary, Inc}},
 file = {https://ebookcentral.proquest.com/lib/gbv/detail.action?docID=1633390},
 file = {db2af8d6-c22d-4265-8877-8b5580ea0a8a:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\db2af8d6-c22d-4265-8877-8b5580ea0a8a.pdf:pdf}
}


@proceedings{Billard.2011,
 year = {2011},
 title = {{Proceedings of the 6th international conference on Human-robot interaction - HRI '11}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450305617},
 editor = {Billard, Aude and Kahn, Peter and Adams, Julie A. and Trafton, Greg},
 doi = {10.1145/1957656}
}


@article{Bemelmans.2012,
 abstract = {The ongoing development of robotics on the one hand and, on the other hand, the foreseen relative growth in number of elderly individuals suffering from dementia, raises the question of which contribution robotics could have to rationalize and maintain, or even improve the quality of care. The objective of this review was to assess the published effects and effectiveness of robot interventions aiming at social assistance in elderly care. We searched, using Medical Subject Headings terms and free words, in the CINAHL, MEDLINE, Cochrane, BIOMED, PUBMED, PsycINFO, and EMBASE databases. Also the IEEE Digital Library was searched. No limitations were applied for the date of publication. Only articles written in English were taken into account. Collected publications went through a selection process. In the first step, publications were collected from major databases using a search query. In the second step, 3 reviewers independently selected publications on their title, using predefined selection criteria. In the third step, publications were judged based on their abstracts by the same reviewers, using the same selection criteria. In the fourth step, one reviewer made the final selection of publications based on complete content. Finally, 41 publications were included in the review, describing 17 studies involving 4 robot systems. Most studies reported positive effects of companion-type robots on (socio)psychological (eg, mood, loneliness, and social connections and communication) and physiological (eg, stress reduction) parameters. The methodological quality of the studies was, mostly, low. Although positive effects were reported, the scientific value of the evidence was limited. The positive results described, however, prompt further effectiveness research in this field.},
 author = {Bemelmans, Roger and Gelderblom, Gert Jan and Jonker, Pieter and de Witte, Luc},
 year = {2012},
 title = {{Socially assistive robots in elderly care: a systematic review into effects and effectiveness}},
 pages = {114-120},
 volume = {13},
 number = {2},
 journal = {{Journal of the American Medical Directors Association}},
 doi = {10.1016/j.jamda.2010.10.002}
}


@inproceedings{Andersen.26.08.201631.08.2016,
 author = {Andersen, Rasmus S. and Madsen, Ole and Moeslund, Thomas B. and Amor, Heni Ben},
 title = {{Projecting robot intentions into human environments}},
 keywords = {Augmented Reality;Collaborating;Intention;Projection},
 pages = {294--301},
 publisher = {IEEE},
 isbn = {978-1-5090-3929-6},
 booktitle = {{2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)}},
 year = {26.08.2016 - 31.08.2016},
 doi = {10.1109/ROMAN.2016.7745145},
 file = {http://ieeexplore.ieee.org/document/7745145/},
 file = {e199f4e5-614b-4194-b383-80275ec0d36d:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\e199f4e5-614b-4194-b383-80275ec0d36d.pdf:pdf}
}


@inproceedings{Woods.March2006,
 author = {Woods, Sarah N. and Walters, Michael L. and Koay, Kheng Lee and Dautenhahn, Kerstin},
 title = {{Comparing human robot interaction scenarios using live and video based methods: towards a novel methodological approach}},
 keywords = {Prototyping;Testing},
 pages = {750--755},

 isbn = {0-7803-9511-1},
 booktitle = {{9th IEEE International Workshop on Advanced Motion Control, 2006}},
 year = {March 2006},
 doi = {10.1109/AMC.2006.1631754},
 file = {5d4e2d58-6634-4074-ad82-0022d5b03462:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\5d4e2d58-6634-4074-ad82-0022d5b03462.pdf:pdf}
}

@inproceedings{Pascher2019b,
title = {SwipeBuddy: A Teleoperated Tablet and Ebook-Reader Holder for a Hands-Free Interaction},
author = {Max Pascher and Stefan Schneegass and Jens Gerken},
editor = {David Lamas and Fernando Loizides and Lennart Nacke and Helen Petrie and Marco Winckler and Panayiotis Zaphiris},
url = {https://hci.w-hs.de/pub_SwipeBuddy/, PDF Download},
doi = {10.1007/978-3-030-29390-1_39},
isbn = {978-3-030-29390-1},
year = {2019},
date = {2019-08-23},
booktitle = {Human-Computer Interaction – INTERACT 2019},
issuetitle = {Lecture Notes in Computer Science},
volume = {11749},
pages = {568-571},
publisher = {Springer, Cham},
abstract = {Mobile devices are the core computing platform we use in our everyday life to communicate with friends, watch movies, or read books. For people with severe physical disabilities, such as tetraplegics, who cannot use their hands to operate such devices, these devices are barely usable. Tackling this challenge, we propose SwipeBuddy, a teleoperated robot allowing for touch interaction with a smartphone, tablet, or ebook-reader. The mobile device is mounted on top of the robot and can be teleoperated by a user through head motions and gestures controlling a stylus simulating touch input. Further, the user can control the position and orientation of the mobile device. We demonstrate the SwipeBuddy robot device and its different interaction capabilities.},
keywords = {hands-free, head gestures, head-based interaction, human-robot collaboration, human-robot interaction, robot control, self-determined life},
pubstate = {published},
tppubtype = {inproceedings}
}

@inproceedings{Gustafson.2008,
author = {Gustafson, Sean and Baudisch, Patrick and Gutwin, Carl and Irani, Pourang},
title = {Wedge: Clutter-Free Visualization of off-Screen Locations},
year = {2008},
isbn = {9781605580111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1357054.1357179},
doi = {10.1145/1357054.1357179},
abstract = {To overcome display limitations of small-screen devices, researchers have proposed techniques that point users to objects located off-screen. Arrow-based techniques such as City Lights convey only direction. Halo conveys direction and distance, but is susceptible to clutter resulting from overlapping halos. We present Wedge, a visualization technique that conveys direction and distance, yet avoids overlap and clutter. Wedge represents each off-screen location using an acute isosceles triangle: the tip coincides with the off-screen locations, and the two corners are located on-screen. A wedge conveys location awareness primarily by means of its two legs pointing towards the target. Wedges avoid overlap programmatically by repelling each other, causing them to rotate until overlap is resolved. As a result, wedges can be applied to numbers and configurations of targets that would lead to clutter if visualized using halos. We report on a user study comparing Wedge and Halo for three off-screen tasks. Participants were significantly more accurate when using Wedge than when using Halo.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {787–796},
numpages = {10},
keywords = {spatial cognition, peripheral awareness, maps, off-screen, visualization, small screens},
location = {Florence, Italy},
series = {CHI '08}
}


@misc{Anoymous,
title = "{ANONYMOUS FOR REVIEW}",
}

@inbook{Pascher2020,
title = {Praxisbeispiel Digitalisierung konkret: Wenn der Stromzähler weiß, ob es Oma gut geht. Beschreibung des minimalinvasiven Frühwarnsystems „ZELIA“},
author = {Max Pascher},
editor = {Michael Vilain},
doi = {10.5771/9783748907008-137},
isbn = {78-3-8487-6621-5},
year = {2020},
date = {2020-10-06},
booktitle = {Wege in die digitale Zukunft - Was bedeuten Smart Living, Big Data, Robotik & Co für die Sozialwirtschaft?},
pages = {137-148},
publisher = {Nomos Verlagsgesellschaft mbH & Co. KG},
keywords = {AAL, consumption, data science, elderly, monitoring, wellbeing},
pubstate = {published},
tppubtype = {inbook}
}

@inproceedings{Pascher.2021,
title = {Recommendations for the Development of a Robotic Drinking and Eating Aid - An Ethnographic Study },
author = {Max Pascher and Annalies Baumeister and Stefan Schneegass and Barbara Klein and Jens Gerken},
editor = {Carmelo Ardito and Rosa Lanzilotti and Alessio Malizia and Helen Petrie and Antonio Piccinno and Giuseppe Desolda and Kori Inkpen},
url = {https://hci.w-hs.de/pub_recommendations_for_the_development_of_a_robotic_drinking_and_eating_aid___an_ethnographic_study/, PDF Download},
doi = {10.1007/978-3-030-85623-6_21},
year = {2021},
date = {2021-09-01},
booktitle = {Human-Computer Interaction – INTERACT 2021},
publisher = {Springer, Cham},
abstract = {Being able to live independently and self-determined in one's own home is a crucial factor for human dignity and preservation of self-worth. For people with severe physical impairments who cannot use their limbs for every day tasks, living in their own home is only possible with assistance from others. The inability to move arms and hands makes it hard to take care of oneself, e. g. drinking and eating independently. In this paper, we investigate how 15 participants with disabilities consume food and drinks. We report on interviews, participatory observations, and analyzed the aids they currently use. Based on our findings, we derive a set of recommendations that supports researchers and practitioners in designing future robotic drinking and eating aids for people with disabilities.},
keywords = {assisted living technologies, human-centered computing, meal assistance, participation design, people with disabilities, robot assistive drinking, robot assistive feeding, user acceptance, user participation, user-centered design}
}

@inproceedings{Arevalo-Arboleda2021b,
title = {Reflecting upon Participatory Design in Human-Robot Collaboration for People with Motor Disabilities: Challenges and Lessons Learned from Three Multiyear Projects},
author = {Stephanie Arévalo-Arboleda and Max Pascher and Annalies Baumeister and Barbara Klein and Jens Gerken},
doi = {10.1145/3453892.3458044},
isbn = {978-1-4503-8792-7/21/06},
year = {2021},
date = {2021-06-29},
booktitle = {The 14th PErvasive Technologies Related to Assistive Environments Conference - PETRA 2021},
organization = {ACM},
abstract = {Human-robot technology has the potential to positively impact the lives of people with motor disabilities. However, current efforts have mostly been oriented towards technology (sensors, devices, modalities, interaction techniques), thus relegating the user and their valuable input to the wayside. In this paper, we aim to present a holistic perspective of the role of participatory design in Human-Robot Collaboration (HRC) for People with Motor Disabilities (PWMD). We have been involved in several multiyear projects related to HRC for PWMD, where we encountered different challenges related to planning and participation, preferences of stakeholders, using certain participatory design techniques, technology exposure, as well as ethical, legal, and social implications. These challenges helped us provide five lessons learned that could serve as a guideline to researchers when using participatory design with vulnerable groups. In particular, young researchers who are starting to explore HRC research for people with disabilities.},
keywords = {accessibility design, human-robot collaboration, lessons learned, participatory design},
pubstate = {forthcoming},
tppubtype = {inproceedings}
}
@inproceedings{Burigat.2012,
author = {Burigat, Stefano and Chittaro, Luca and Vianello, Andrea},
title = {Dynamic Visualization of Large Numbers of Off-Screen Objects on Mobile Devices: An Experimental Comparison of Wedge and Overview+detail},
year = {2012},
isbn = {9781450311052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2371574.2371590},
doi = {10.1145/2371574.2371590},
abstract = {Overview+Detail [25] and Wedge [16] have been proposed in the literature as effective approaches to resolve the off-screen objects problem on mobile devices. However, they have been studied with a small number of off-screen objects and (in most studies) with static scenarios, in which users did not have to perform any navigation activity. In this paper, we propose improvements to Wedge and Overview+Detail which are specifically aimed at simplifying their use in dynamic scenarios that involve large numbers of off-screen objects. We compare the effectiveness of the two approaches in the considered scenario with a user study, whose results show that Overview+Detail allows users to be faster in searching for off-screen objects and more accurate in estimating their location.},
booktitle = {Proceedings of the 14th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {93–102},
numpages = {10},
keywords = {mobile devices, peripheral awareness, wedge, off-screen objects, visualization, map navigation, overview+detail},
location = {San Francisco, California, USA},
series = {MobileHCI '12}
}

@INPROCEEDINGS{Voilque.2019,  author={Voilqué, A. and Masood, J. and Fauroux, JC. and Sabourin, L. and Guezet, O.},  booktitle={2019 Wearable Robotics Association Conference (WearRAcon)},   title={Industrial Exoskeleton Technology: Classification, Structural Analysis, and Structural Complexity Indicator},   year={2019},  volume={},  number={},  pages={13-20},  doi={10.1109/WEARRACON.2019.8719395}}

@article{Wang.2019,
title = {Symbiotic human-robot collaborative assembly},
journal = {CIRP Annals},
volume = {68},
number = {2},
pages = {701-726},
year = {2019},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0007850619301593},
author = {L. Wang and R. Gao and J. Váncza and J. Krüger and X.V. Wang and S. Makris and G. Chryssolouris},
keywords = {Assembly, Robot, Human-robot collaboration},
abstract = {In human-robot collaborative assembly, robots are often required to dynamically change their pre-planned tasks to collaborate with human operators in a shared workspace. However, the robots used today are controlled by pre-generated rigid codes that cannot support effective human-robot collaboration. In response to this need, multi-modal yet symbiotic communication and control methods have been a focus in recent years. These methods include voice processing, gesture recognition, haptic interaction, and brainwave perception. Deep learning is used for classification, recognition and context awareness identification. Within this context, this keynote provides an overview of symbiotic human-robot collaborative assembly and highlights future research directions.}
}

@article{Dianatfar.2021,
title = {Review on existing VR/AR solutions in human–robot collaboration},
journal = {Procedia CIRP},
volume = {97},
pages = {407-411},
year = {2021},
note = {8th CIRP Conference of Assembly Technology and Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.259},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120314815},
author = {Morteza Dianatfar and Jyrki Latokartano and Minna Lanz},
keywords = {Human–robot collaboration, Virtual reality, Augmented reality},
abstract = {During the last decades, there has been wide interest towards creating more agile and reconfigurable automation systems. This includes the research interest towards human–robot collaboration (HRC) solutions, where semi or fully automated process could be combined with human dexterity and flexibility without complexity and inflexibility. However, the communication between the human and the robot is not intuitive, fast or flexible yet. The emerging technologies such as Augmented Reality (AR), Virtual Reality (VR) and Mixed Reality (XR) are seen as good solution candidates for increasing the communication between human and the machine during the design, commission and operation phases. The design of the HRC system includes layout design evaluation, task scenario analysis, robot programming, cycle time calculations, and safety analysis. Virtual Reality and Augmented Reality technologies provide immersive experiences to visualize and analyze these procedures. This paper aims to review the current status of virtual and augmented reality solutions in HRC with meta-analysis and highlight missing elements. Finally, future research directions and requirements are presented.}
}

@article{Biocca.2007,
author = { Frank   Biocca  and  Charles   Owen  and  Arthur   Tang  and  Corey   Bohil },
title = {Attention Issues in Spatial Information Systems: Directing Mobile Users' Visual Attention Using Augmented Reality},
journal = {Journal of Management Information Systems},
volume = {23},
number = {4},
pages = {163-184},
year  = {2007},
publisher = {Routledge},
doi = {10.2753/MIS0742-1222230408},

URL = {},
eprint = {}

}

@inproceedings{Adcock.2013,
author = {Adcock, Matt and Feng, David and Thomas, Bruce},
title = {Visualization of Off-Surface 3D Viewpoint Locations in Spatial Augmented Reality},
year = {2013},
isbn = {9781450321419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491367.2491378},
doi = {10.1145/2491367.2491378},
abstract = {Spatial Augmented Reality (SAR) systems can be used to convey guidance in a physical task from a remote expert. Sometimes that remote expert is provided with a single camera view of the workspace but, if they are given a live captured 3D model and can freely control their point of view, the local worker needs to know what the remote expert can see. We present three new SAR techniques, Composite Wedge, Vector Boxes, and Eyelight, for visualizing off-surface 3D viewpoints and supporting the required workspace awareness. Our study showed that the Composite Wedge cue was best for providing location awareness, and the Eyelight cue was best for providing visibility map awareness.},
booktitle = {Proceedings of the 1st Symposium on Spatial User Interaction},
pages = {1–8},
numpages = {8},
keywords = {spatial augmented reality, spatial user interfaces, remote guidance, off-surface rendering, visualization},
location = {Los Angeles, California, USA},
series = {SUI '13}
}

@inproceedings{Baudisch.2003,
author = {Baudisch, Patrick and Rosenholtz, Ruth},
title = {Halo: A Technique for Visualizing off-Screen Objects},
year = {2003},
isbn = {1581136307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/642611.642695},
doi = {10.1145/642611.642695},
abstract = {As users pan and zoom, display content can disappear into off-screen space, particularly on small-screen devices. The clipping of locations, such as relevant places on a map, can make spatial cognition tasks harder. Halo is a visualization technique that supports spatial cognition by showing users the location of off-screen objects. Halo accomplishes this by surrounding off-screen objects with rings that are just large enough to reach into the border region of the display window. From the portion of the ring that is visible on-screen, users can infer the off-screen location of the object at the center of the ring. We report the results of a user study comparing Halo with an arrow-based visualization technique with respect to four types of map-based route planning tasks. When using the Halo interface, users completed tasks 16-33\% faster, while there were no significant differences in error rate for three out of four tasks in our study.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {481–488},
numpages = {8},
keywords = {navigation, peripheral awareness indicators, visualization, Halo, maps},
location = {Ft. Lauderdale, Florida, USA},
series = {CHI '03}
}

@inproceedings{Gruenefeld.2018,
author = {Gruenefeld, Uwe and Ali, Abdallah El and Boll, Susanne and Heuten, Wilko},
title = {Beyond Halo and Wedge: Visualizing out-of-View Objects on Head-Mounted Virtual and Augmented Reality Devices},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229438},
doi = {10.1145/3229434.3229438},
abstract = {Head-mounted devices (HMDs) for Virtual and Augmented Reality (VR/AR) enable us to alter our visual perception of the world. However, current devices suffer from a limited field of view (FOV), which becomes problematic when users need to locate out of view objects (e.g., locating points-of-interest during sightseeing). To address this, we developed and evaluated in two studies HaloVR, WedgeVR, HaloAR and WedgeAR, which are inspired by usable 2D off-screen object visualization techniques (Halo, Wedge). While our techniques resulted in overall high usability, we found the choice of AR or VR impacts mean search time (VR: 2.25s, AR: 3.92s) and mean direction estimation error (VR: 21.85°, AR: 32.91°). Moreover, while adding more out-of-view objects significantly affects search time across VR and AR, direction estimation performance remains unaffected. We provide implications and discuss the challenges of designing for VR and AR HMDs.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {40},
numpages = {11},
keywords = {off-screen, out-of-view, Halo, virtual reality, Wedge, visualization technique, head-mounted, augmented reality},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}



@article{Kerr.1987,
  doi = {10.1037/h0091572},
  url = {https://doi.org/10.1037/h0091572},
  year = {1987},
  publisher = {American Psychological Association ({APA})},
  volume = {32},
  number = {3},
  pages = {173--180},
  author = {Nancy Kerr and Lee Meyerson},
  title = {Independence as a goal and a value of people with physical disabilities: Some caveats.},
  journal = {Rehabilitation Psychology}
}

@article{Ajoudani.2017,
  doi = {10.1007/s10514-017-9677-2},
  url = {https://doi.org/10.1007/s10514-017-9677-2},
  year = {2017},
  month = oct,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {42},
  number = {5},
  pages = {957--975},
  author = {Arash Ajoudani and Andrea Maria Zanchettin and Serena Ivaldi and Alin Albu-Sch\"{a}ffer and Kazuhiro Kosuge and Oussama Khatib},
  title = {Progress and prospects of the human{\textendash}robot collaboration},
  journal = {Autonomous Robots}
}

@article{Mcguckin.1998,
author = { Robert H.   Mcguckin  and  Mary L.   Streitwieser  and  Mark   Doms },
title = {The Effect Of Technology Use On Productivity Growth},
journal = {Economics of Innovation and New Technology},
volume = {7},
number = {1},
pages = {1-26},
year  = {1998},
publisher = {Routledge},
doi = {10.1080/10438599800000026},

URL = {},
eprint = { }

}
@Article{Castelli.2019,
AUTHOR = {Castelli, Kevin and Zaki, Ahmed Magdy Ahmed and Giberti, Hermes},
TITLE = {Development of a Practical Tool for Designing Multi-Robot Systems in Pick-and-Place Applications},
JOURNAL = {Robotics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {71},
URL = {https://www.mdpi.com/2218-6581/8/3/71},
ISSN = {2218-6581},
ABSTRACT = {Pick-and-place manipulators have become one of the principal components of almost all manufacturing plants. The process of sizing the number of manipulators required to efficiently carry out pick-and-place tasks depends on the complexities of such plants, the characteristics of the production line and the particular requirements. These aspects tend to make the sizing procedure rather complex and time consuming. Moreover, the results are closely linked to the accuracy of the input data that is usually, especially in the initial stages, unreliable and haphazard. To face these issues, the simulation tools currently available in the market are not always suitable. In this paper, a practical procedure to size the number of manipulators required in any particular plant to perform pick-and-place tasks is presented. This procedure results in a relatively simple tool capable of calculating the number of robots required in a line knowing the layout, type of robot to be used, and production characteristics. This tool is able to simulate the different distribution of goods on the line as well as the required strategies for picking in a multi-robot environment to test several production situations and assess the accuracy of the sizing.},
DOI = {10.3390/robotics8030071}
}

@ARTICLE{Honig.2018,
AUTHOR={Honig, Shanee and Oron-Gilad, Tal},   	 
TITLE={Understanding and Resolving Failures in Human-Robot Interaction: Literature Review and Model Development},      	
JOURNAL={Frontiers in Psychology},      	
VOLUME={9},      
PAGES={861},     	
YEAR={2018},      	  
URL={https://www.frontiersin.org/article/10.3389/fpsyg.2018.00861},       	
DOI={10.3389/fpsyg.2018.00861},      	
ISSN={1664-1078},    
ABSTRACT={While substantial effort has been invested in making robots more reliable, experience demonstrates that robots operating in unstructured environments are often challenged by frequent failures. Despite this, robots have not yet reached a level of design that allows effective management of faulty or unexpected behavior by untrained users. To understand why this may be the case, an in-depth literature review was done to explore when people perceive and resolve robot failures, how robots communicate failure, how failures influence people's perceptions and feelings toward robots, and how these effects can be mitigated. Fifty-two studies were identified relating to communicating failures and their causes, the influence of failures on human-robot interaction (HRI), and mitigating failures. Since little research has been done on these topics within the HRI community, insights from the fields of human computer interaction (HCI), human factors engineering, cognitive engineering and experimental psychology are presented and discussed. Based on the literature, we developed a model of information processing for robotic failures (Robot Failure Human Information Processing, RF-HIP), that guides the discussion of our findings. The model describes the way people perceive, process, and act on failures in human robot interaction. The model includes three main parts: (1) communicating failures, (2) perception and comprehension of failures, and (3) solving failures. Each part contains several stages, all influenced by contextual considerations and mitigation strategies. Several gaps in the literature have become evident as a result of this evaluation. More focus has been given to technical failures than interaction failures. Few studies focused on human errors, on communicating failures, or the cognitive, psychological, and social determinants that impact the design of mitigation strategies. By providing the stages of human information processing, RF-HIP can be used as a tool to promote the development of user-centered failure-handling strategies for HRIs.}
}

@InProceedings{Steinbauer.2018,
author="Steinbauer, Gerald",
editor="Chen, Xiaoping
and Stone, Peter
and Sucar, Luis Enrique
and van der Zant, Tijn",
title="A Survey about Faults of Robots Used in RoboCup",
booktitle="RoboCup 2012: Robot Soccer World Cup XVI",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="344--355",
abstract="Faults that occur in an autonomous robot system negatively affect its dependability. The aim of truly dependable and autonomous systems requires that one has to deal with these faults in some way. In order to be able to do this efficiently one has to have information on the nature of these faults. Very few studies on this topic have been conducted so far. In this paper we present results of a survey on faults of autonomous robots we conducted in the context of RoboCup. The major contribution of this paper is twofold. First we present an adapted fault taxonomy suitable for autonomous robots. Second we give information on the nature, the relevance and impact of faults in robot systems that are beneficial for researcher dealing with fault mitigation and management in autonomous systems.",
isbn="978-3-642-39250-4"
}

@inproceedings{Bhattacharjee.2020,
author = {Bhattacharjee, Tapomayukh and Gordon, Ethan K. and Scalise, Rosario and Cabrera, Maria E. and Caspi, Anat and Cakmak, Maya and Srinivasa, Siddhartha S.},
title = {Is More Autonomy Always Better? Exploring Preferences of Users with Mobility Impairments in Robot-Assisted Feeding},
year = {2020},
isbn = {9781450367462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319502.3374818},
doi = {10.1145/3319502.3374818},
abstract = {A robot-assisted feeding system can potentially help a user with upper-body mobility
impairments eat independently. However, autonomous assistance in the real world is
challenging because of varying user preferences, impairment constraints, and possibility
of errors in uncertain and unstructured environments. An autonomous robot-assisted
feeding system needs to decide the appropriate strategy to acquire a bite of hard-to-model
deformable food items, the right time to bring the bite close to the mouth, and the
appropriate strategy to transfer the bite easily. Our key insight is that a system
should be designed based on a user's preference about these various challenging aspects
of the task. In this work, we explore user preferences for different modes of autonomy
given perceived error risks and also analyze the effect of input modalities on technology
acceptance. We found that more autonomy is not always better, as participants did
not have a preference to use a robot with partial autonomy over a robot with low autonomy.
In addition, participants' user interface preference changes from voice control during
individual dining to web-based during social dining. Finally, we found differences
on average ratings when grouping the participants based on their mobility limitations
(lower vs. higher) that suggests that ratings from participants with lower mobility
limitations are correlated with higher expectations of robot performance.},
booktitle = {Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {181–190},
numpages = {10},
keywords = {assistive feeding, assistive robotics},
location = {Cambridge, United Kingdom},
series = {HRI '20}
}

@inproceedings{Kragic.2018,
author = {Kragic, Danica and Gustafson, Joakim and Karaoguz, Hakan and Jensfelt, Patric and Krug, Robert},
title = {Interactive, Collaborative Robots: Challenges and Opportunities},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Robotic technology has transformed manufacturing industry ever since the first industrial robot was put in use in the beginning of the 60s. The challenge of developing flexible solutions where production lines can be quickly re-planned, adapted and structured for new or slightly changed products is still an important open problem. Industrial robots today are still largely preprogrammed for their tasks, not able to detect errors in their own performance or to robustly interact with a complex environment and a human worker. The challenges are even more serious when it comes to various types of service robots. Full robot autonomy, including natural interaction, learning from and with human, safe and flexible performance for challenging tasks in unstructured environments will remain out of reach for the foreseeable future. In the envisioned future factory setups, home and office environments, humans and robots will share the same workspace and perform different object manipulation tasks in a collaborative manner. We discuss some of the major challenges of developing such systems and provide examples of the current state of the art.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {18–25},
numpages = {8},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@INPROCEEDINGS{Trepkowski.2019,  author={Trepkowski, Christina and Eibich, David and Maiero, Jens and Marquardt, Alexander and Kruijff, Ernst and Feiner, Steven},  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},   title={The Effect of Narrow Field of View and Information Density on Visual Search Performance in Augmented Reality},   year={2019},  volume={},  number={},  pages={575-584},  doi={10.1109/VR.2019.8798312}}


@ARTICLE{Chatzopoulos.2017,  author={Chatzopoulos, Dimitris and Bermejo, Carlos and Huang, Zhanpeng and Hui, Pan},  journal={IEEE Access},   title={Mobile Augmented Reality Survey: From Where We Are to Where We Go},   year={2017},  volume={5},  number={},  pages={6917-6950},  doi={10.1109/ACCESS.2017.2698164}}

% This file was created with Citavi 6.5.0.0

@proceedings{.2327May1988,
 year = {23-27 May 1988},
 title = {{Proceedings of the IEEE 1988 National Aerospace and Electronics Conference}},
 publisher = {IEEE}
}


@inproceedings{Mainprice.2010,
 author = {Mainprice, Jim and Sisbot, E. Akin and Sim{\'e}on, Thierry and Alam, Rachid},
 title = {{Planning safe and legible hand-over motions for human-robot interaction}},
 keywords = {Accessibility;Handover;Manipulation;Motion;Planning;Pose;Safety},
 booktitle = {{IARP/IEEE-RAS/EURON  workshop on technical challenges for dependable robots in human environments}},
 year = {2010},
 file = {https://hal.laas.fr/hal-01976223},
 file = {7a82c449-6b95-4898-9de6-84f5a3f04a94:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\7a82c449-6b95-4898-9de6-84f5a3f04a94.pdf:pdf}
}


@article{Li.2008,
 author = {Li, Xin and Hess, Traci J. and Valacich, Joseph S.},
 year = {2008},
 title = {{Why do we trust new technology? A study of initial trust formation with organizational information systems}},
 pages = {39--71},
 volume = {17},
 number = {1},
 issn = {09638687},
 journal = {{The Journal of Strategic Information Systems}},
 doi = {{10.1016/j.jsis.2008.01.001}},
 file = {8d135c6a-d036-4970-b4be-d12825344b96:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\8d135c6a-d036-4970-b4be-d12825344b96.pdf:pdf}
}


@inproceedings{Leutert.03.03.201306.03.2013,
 author = {Leutert, Florian and Herrmann, Christian and Schilling, Klaus},
 title = {{A Spatial Augmented Reality system for intuitive display of robotic data}},
 pages = {179--180},
 publisher = {IEEE},
 isbn = {978-1-4673-3101-2},
 booktitle = {{2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}},
 year = {03.03.2013 - 06.03.2013},
 doi = {{10.1109/HRI.2013.6483560}},
 file = {http://ieeexplore.ieee.org/document/6483560/},
 file = {0ae040f8-fe34-4221-9b7c-a54f07ed6f38:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\0ae040f8-fe34-4221-9b7c-a54f07ed6f38.pdf:pdf}
}


@article{Lee.2004,
 abstract = {Automation is often problematic because people fail to rely upon it appropriately. Because people respond to technology socially, trust influences reliance on automation. In particular, trust guides reliance when complexity and unanticipated situations make a complete understanding of the automation impractical. This review considers trust from the organizational, sociological, interpersonal, psychological, and neurological perspectives. It considers how the context, automation characteristics, and cognitive processes affect the appropriateness of trust. The context in which the automation is used influences automation performance and provides a goal-oriented perspective to assess automation characteristics along a dimension of attributional abstraction. These characteristics can influence trust through analytic, analogical, and affective processes. The challenges of extrapolating the concept of trust in people to trust in automation are discussed. A conceptual model integrates research regarding trust in automation and describes the dynamics of trust, the role of context, and the influence of display characteristics. Actual or potential applications of this research include improved designs of systems that require people to manage imperfect automation.},
 author = {Lee, John D. and See, Katrina A.},
 year = {2004},
 title = {{Trust in automation: designing for appropriate reliance}},
 pages = {50--80},
 volume = {46},
 number = {1},
 issn = {0018-7208},
 journal = {{Human factors}},
 doi = {{10.1518/hfes.46.1.50{\textunderscore }30392}},
 file = {39fc8379-4ed2-4b44-a04d-06adddbcfdf2:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\39fc8379-4ed2-4b44-a04d-06adddbcfdf2.pdf:pdf}
}


@inproceedings{Kulesza.15.09.201319.09.2013,
 author = {Kulesza, Todd and Stumpf, Simone and Burnett, Margaret and Yang, Sherry and Kwan, Irwin and Wong, Weng-Keen},
 title = {{Too much, too little, or just right? Ways explanations impact end users' mental models}},
 pages = {3--10},
 publisher = {IEEE},
 isbn = {978-1-4799-0369-6},
 booktitle = {{2013 IEEE Symposium on Visual Languages and Human Centric Computing}},
 year = {15.09.2013 - 19.09.2013},
 doi = {{10.1109/VLHCC.2013.6645235}},
 file = {2436b20e-a49a-49b7-83a2-062fe0b63e66:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\2436b20e-a49a-49b7-83a2-062fe0b63e66.pdf:pdf}
}








@proceedings{Kanda.2018,
 year = {2018},
 title = {{Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction  - HRI '18}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450349536},
 editor = {Kanda, Takayuki and Ŝabanovi{\'c}, Selma and Hoffman, Guy and Tapus, Adriana},
 doi = {{10.1145/3171221}}
}

@ARTICLE{drolshagen2021,
  AUTHOR={Drolshagen, Sandra and Pfingsthorn, Max and Gliesche, Pascal and Hein, Andreas},   
TITLE={Acceptance of Industrial Collaborative Robots by People With Disabilities in Sheltered Workshops},
JOURNAL={Frontiers in Robotics and AI},      
VOLUME={7},      
PAGES={173},     
YEAR={2021},      
URL={https://www.frontiersin.org/article/10.3389/frobt.2020.541741},      
DOI={10.3389/frobt.2020.541741},     
ISSN={2296-9144}
}

@inproceedings{10.1145/3098279.3122124,
author = {Gruenefeld, Uwe and Ali, Abdallah El and Heuten, Wilko and Boll, Susanne},
title = {Visualizing Out-of-View Objects in Head-Mounted Augmented Reality},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122124},
doi = {10.1145/3098279.3122124},
abstract = {Various off-screen visualization techniques that point to off-screen objects have
been developed for small screen devices. A similar problem arises with head-mounted
Augmented Reality (AR) with respect to the human field-of-view, where objects may
be out of view. Being able to detect so-called out-of-view objects is useful for certain
scenarios (e.g., situation monitoring during ship docking). To augment existing AR
with this capability, we adapted and tested well-known 2D off-screen object visualization
techniques (Arrow, Halo, Wedge) for head-mounted AR. We found that Halo resulted in
the lowest error for direction estimation while Wedge was subjectively perceived as
best. We discuss future directions of how to best visualize out-of-view objects in
head-mounted AR.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {81},
numpages = {7},
keywords = {out-of-view objects, peripheral awareness, visualization techniques, augmented reality, head-mounted, off-screen},
location = {Vienna, Austria},
series = {MobileHCI '17}
}


@Article{Bonci.2021perception,
AUTHOR = {Bonci, Andrea and Cen Cheng, Pangcheng  David and Indri, Marina and Nabissi, Giacomo and Sibona, Fiorella},
TITLE = {Human-Robot Perception in Industrial Environments: A Survey},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1571},
URL = {https://www.mdpi.com/1424-8220/21/5/1571},
PubMedID = {33668162},
ISSN = {1424-8220},
ABSTRACT = {Perception capability assumes significant importance for human–robot interaction. The forthcoming industrial environments will require a high level of automation to be flexible and adaptive enough to comply with the increasingly faster and low-cost market demands. Autonomous and collaborative robots able to adapt to varying and dynamic conditions of the environment, including the presence of human beings, will have an ever-greater role in this context. However, if the robot is not aware of the human position and intention, a shared workspace between robots and humans may decrease productivity and lead to human safety issues. This paper presents a survey on sensory equipment useful for human detection and action recognition in industrial environments. An overview of different sensors and perception techniques is presented. Various types of robotic systems commonly used in industry, such as fixed-base manipulators, collaborative robots, mobile robots and mobile manipulators, are considered, analyzing the most useful sensors and methods to perceive and react to the presence of human operators in industrial cooperative and collaborative applications. The paper also introduces two proofs of concept, developed by the authors for future collaborative robotic applications that benefit from enhanced capabilities of human perception and interaction. The first one concerns fixed-base collaborative robots, and proposes a solution for human safety in tasks requiring human collision avoidance or moving obstacles detection. The second one proposes a collaborative behavior implementable upon autonomous mobile robots, pursuing assigned tasks within an industrial space shared with human operators.},
DOI = {10.3390/s21051571}
}

@INPROCEEDINGS{Kohli,
  author={Kohli, L. and Niwa, M. and Noma, H. and Susami, K. and Yanagida, Y. and Lindeman, R.W. and Hosaka, K. and Kume, Y.},
  booktitle={2006 14th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems}, 
  title={Towards Effective Information Display Using Vibrotactile Apparent Motion}, 
  year={2006},
  volume={},
  number={},
  pages={445-451},
  doi={10.1109/HAPTIC.2006.1627115  }}

@article{Gescheider1990VibrotactileID,
  title={Vibrotactile intensity discrimination measured by three methods.},
  author={George A. Gescheider and Stanley J. Bolanowski and Ronald T. Verrillo and D J Arpajian and T F Ryan},
  journal={The Journal of the Acoustical Society of America},
  year={1990},
  volume={87 1},
  pages={
          330-8
        }
}

@article{Gescheider1996a,
  title={Effects of stimulus duration on the amplitude difference limen for vibrotaction.},
  author={George A. Gescheider and Josef J. Zwislocki and Andrea L. Rasmussen},
  journal={The Journal of the Acoustical Society of America},
  year={1996},
  volume={100 4 Pt 1},
  pages={
          2312-9
        }
}

@InProceedings{harboe2015realworld,
    author    = {Harboe, Gunnar and Huang, Elaine M.},
    title     = {Real-World Affinity Diagramming Practices: Bridging the Paper-Digital Gap},
    booktitle = {Proc. 33rd Annual ACM Conf. Human Factors in Computing Systems},
    year      = {2015},
    pages     = {95--104},
    publisher = {ACM},
    acmid     = {2702561},
    doi       = {10.1145/2702123.2702561},
    isbn      = {978-1-4503-3145-6},
    keywords  = {affinity diagrams, augmented paper, cooperative data analysis, interview study, paper clustering, sticky notes},
    numpages  = {10},
}

@inproceedings{Harris.02.03.201005.03.2010,
 author = {Harris, John and Sharlin, Ehud},
 title = {{Exploring Emotive Actuation and its role in human-robot interaction}},
 keywords = {Human Response;Motion;Productive;Rapid;Repeating;Trust},
 pages = {95--96},
 publisher = {IEEE},
 isbn = {978-1-4244-4892-0},
 booktitle = {{2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}},
 year = {02.03.2010 - 05.03.2010},
 doi = {{10.1109/HRI.2010.5453252}},
 file = {f20f6db0-36ff-488c-b655-b41ea5843d55:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\f20f6db0-36ff-488c-b655-b41ea5843d55.pdf:pdf}
}


@article{Green.2008,
 author = {Green, Scott A. and Billinghurst, Mark and Chen, XiaoQi and Chase, J. Geoffrey},
 year = {2008},
 title = {{Human-Robot Collaboration: A Literature Review and Augmented Reality Approach in Design}},
 pages = {1},
 volume = {5},
 number = {1},
 issn = {1729-8814},
 journal = {{International Journal of Advanced Robotic Systems}},
 doi = {{10.5772/5664}},
 file = {251a12e8-c779-456b-8459-7cdf473cd9ee:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\251a12e8-c779-456b-8459-7cdf473cd9ee.pdf:pdf}
}


@inproceedings{Gong.2019,
 author = {Gong, L. L. and Ong, S. K. and Nee, A. Y. C.},
 title = {{Projection-based Augmented Reality Interface for Robot Grasping Tasks}},
 keywords = {Augmented Reality;Ease;Feedback;Pick-and-Place;Projection;Undesirable;Visualization},
 pages = {100--104},
 publisher = {{ACM Press}},
 isbn = {9781450371834},
 editor = {Unknown},
 booktitle = {{Proceedings of the 2019 4th International Conference on Robotics, Control and Automation  - ICRCA 2019}},
 year = {2019},
 address = {New York, New York, USA},
 doi = {{10.1145/3351180.3351204}},
 file = {http://dl.acm.org/citation.cfm?doid=3351180},
 file = {ac2312b7-6b5d-4da1-9e1d-b85b337a05d1:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\ac2312b7-6b5d-4da1-9e1d-b85b337a05d1.pdf:pdf}
}


@inproceedings{Ghiringhelli.14.09.201418.09.2014,
 author = {Ghiringhelli, Fabrizio and Guzzi, Jerome and {Di Caro}, Gianni A. and Caglioti, Vincenzo and Gambardella, Luca M. and Giusti, Alessandro},
 title = {{Interactive Augmented Reality for understanding and analyzing multi-robot systems}},
 pages = {1195--1201},
 publisher = {IEEE},
 isbn = {978-1-4799-6934-0},
 booktitle = {{2014 IEEE/RSJ International Conference on Intelligent Robots and Systems}},
 year = {14.09.2014 - 18.09.2014},
 doi = {{10.1109/IROS.2014.6942709}},
 file = {http://ieeexplore.ieee.org/document/6942709/},
 file = {2f5b0995-d746-41d5-b1d7-f0a8abad7e89:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\2f5b0995-d746-41d5-b1d7-f0a8abad7e89.pdf:pdf}
}


@article{Hoffman.2013,
 author = {Hoffman, Robert R. and Johnson, Matthew and Bradshaw, Jeffrey M. and Underbrink, Al},
 year = {2013},
 title = {{Trust in Automation}},
 pages = {84--88},
 volume = {28},
 number = {1},
 issn = {1541-1672},
 journal = {{IEEE Intelligent Systems}},
 doi = {{10.1109/MIS.2013.24}},
 file = {dc497211-fb43-4bb9-8fd2-6f1ff8ef6f78:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\dc497211-fb43-4bb9-8fd2-6f1ff8ef6f78.pdf:pdf}
}


@inproceedings{Matsumaru.06.09.200608.09.2006,
 author = {Matsumaru, Takafumi},
 title = {{Mobile Robot with Preliminary-announcement and Display Function of Forthcoming Motion using Projection Equipment}},
 pages = {443--450},
 publisher = {IEEE},
 isbn = {1-4244-0564-5},
 booktitle = {{ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication}},
 year = {06.09.2006 - 08.09.2006},
 doi = {{10.1109/ROMAN.2006.314368}},
 file = {http://ieeexplore.ieee.org/document/4107847/},
 file = {e56fc8b1-e768-4984-b408-7140d0718da1:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\e56fc8b1-e768-4984-b408-7140d0718da1.pdf:pdf}
}


@incollection{Mead.2010,
 author = {Mead, Ross and Mataric, Maja J.},
 title = {{Automated caricature of robot expressions in socially assistive human-robot interaction}},
 booktitle = {{The 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI2010) Workshop on What Do Collaborations with the Arts Have to Say about HRI}},
 year = {2010},
 file = {dd251165-57d1-4c5d-9d9a-2e9ce65baae7:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\dd251165-57d1-4c5d-9d9a-2e9ce65baae7.pdf:pdf}
}


@proceedings{Unknown.2019,
 year = {2019},
 title = {{Proceedings of the 2019 4th International Conference on Robotics, Control and Automation  - ICRCA 2019}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450371834},
 editor = {Unknown},
 doi = {{10.1145/3351180}}
}


@inproceedings{Takayama.2011,
 author = {Takayama, Leila and Dooley, Doug and Ju, Wendy},
 title = {{Expressing thought}},
 keywords = {Communication;Human Response;Implicit;Interpredictability;Planning;Simultaenously;Thinking},
 pages = {69},
 publisher = {{ACM Press}},
 isbn = {9781450305617},
 editor = {Billard, Aude and Kahn, Peter and Adams, Julie A. and Trafton, Greg},
 booktitle = {{Proceedings of the 6th international conference on Human-robot interaction - HRI '11}},
 year = {2011},
 address = {New York, New York, USA},
 doi = {{10.1145/1957656.1957674}},
 file = {http://portal.acm.org/citation.cfm?doid=1957656},
 file = {89108c5a-b4a2-4f0a-9fe0-61786b3d31e5:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\89108c5a-b4a2-4f0a-9fe0-61786b3d31e5.pdf:pdf}
}


@article{Szafir.2017,
 author = {Szafir, Daniel and Mutlu, Bilge and Fong, Terrence},
 year = {2017},
 title = {{Designing planning and control interfaces to support user collaboration with flying robots}},
 pages = {514--542},
 volume = {36},
 number = {5-7},
 issn = {0278-3649},
 journal = {{The International Journal of Robotics Research}},
 doi = {{10.1177/0278364916688256}},
 file = {2d48fd85-1314-4459-8c49-da1b57aa78f9:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\2d48fd85-1314-4459-8c49-da1b57aa78f9.pdf:pdf}
}


@inproceedings{Stadler.26.08.201631.08.2016,
 author = {Stadler, Susanne and Kain, Kevin and Giuliani, Manuel and Mirnig, Nicole and Stollnberger, Gerald and Tscheligi, Manfred},
 title = {{Augmented reality for industrial robot programmers: Workload analysis for task-based, augmented reality-supported robot control}},
 pages = {179--184},
 publisher = {IEEE},
 isbn = {978-1-5090-3929-6},
 booktitle = {{2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)}},
 year = {26.08.2016 - 31.08.2016},
 doi = {{10.1109/ROMAN.2016.7745108}},
 file = {f00b2764-3b9a-4d2b-a77b-43abc1f451f0:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\f00b2764-3b9a-4d2b-a77b-43abc1f451f0.pdf:pdf}
}


@article{Shinozawa.2005,
 author = {Shinozawa, Kazuhiko and Naya, Futoshi and Yamato, Junji and Kogure, Kiyoshi},
 year = {2005},
 title = {{Differences in effect of robot and screen agent recommendations on human decision-making}},
 pages = {267--279},
 volume = {62},
 number = {2},
 issn = {10715819},
 journal = {{International Journal of Human-Computer Studies}},
 doi = {{10.1016/j.ijhcs.2004.11.003}},
 file = {0c496084-dcda-4bc5-9bb1-b09016d3ef69:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\0c496084-dcda-4bc5-9bb1-b09016d3ef69.pdf:pdf}
}


@article{SeverinsonEklundh.2003,
 author = {Severinson-Eklundh, Kerstin and Green, Anders and H{\"u}ttenrauch, Helge},
 year = {2003},
 title = {{Social and collaborative aspects of interaction with a service robot}},
 pages = {223--234},
 volume = {42},
 number = {3-4},
 issn = {09218890},
 journal = {{Robotics and Autonomous Systems}},
 doi = {{10.1016/S0921-8890(02)00377-9}},
 file = {608bbb4e-09bf-432a-8ecc-6bb03a674042:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\608bbb4e-09bf-432a-8ecc-6bb03a674042.pdf:pdf}
}


@article{Romann.2014,
 author = {Ro{\ss}mann, J{\"u}rgen and {Guiffo Kaigom}, Eric and Atorf, Linus and Rast, Malte and Grinshpun, Georgij and Schlette, Christian},
 year = {2014},
 title = {{Mental Models for Intelligent Systems: eRobotics Enables New Approaches to Simulation-Based AI}},
 pages = {101--110},
 volume = {28},
 number = {2},
 issn = {0933-1875},
 journal = {{KI - K{\"u}nstliche Intelligenz}},
 doi = {{10.1007/s13218-014-0298-z}},
 file = {473acbb6-3b7d-4ee9-95a5-23e1f26a67fb:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\473acbb6-3b7d-4ee9-95a5-23e1f26a67fb.pdf:pdf}
}



}


@inproceedings{Rea.2017,
 author = {Rea, Daniel J. and Seo, Stela H. and Bruce, Neil and Young, James E.},
 title = {{Movers, Shakers, and Those Who Stand Still}},
 pages = {398--407},
 publisher = {{ACM Press}},
 isbn = {9781450343367},
 editor = {Mutlu, Bilge and Tscheligi, Manfred and Weiss, Astrid and Young, James E.},
 booktitle = {{Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction - HRI '17}},
 year = {2017},
 address = {New York, New York, USA},
 doi = {{10.1145/2909824.3020246}},
 file = {http://dl.acm.org/citation.cfm?doid=2909824},
 file = {f83c75fb-8116-4a56-a6d8-dfb4abae6eb3:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\f83c75fb-8116-4a56-a6d8-dfb4abae6eb3.pdf:pdf}
}


@proceedings{Mutlu.2017,
 year = {2017},
 title = {{Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction - HRI '17}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450343367},
 editor = {Mutlu, Bilge and Tscheligi, Manfred and Weiss, Astrid and Young, James E.},
 doi = {{10.1145/2909824}}
}


@inproceedings{Milgram.2630July1993,
 author = {Milgram, P. and Zhai, S. and Drascic, D. and Grodski, J.},
 title = {{Applications of augmented reality for human-robot communication}},
 pages = {1467--1472},
 publisher = {IEEE},
 isbn = {0-7803-0823-9},
 booktitle = {{Proceedings of 1993 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS '93)}},
 year = {26-30 July 1993},
 doi = {{10.1109/IROS.1993.583833}},
 file = {http://ieeexplore.ieee.org/document/583833/},
 file = {2beadf54-ff5c-40d3-9912-8b8729d09158:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\2beadf54-ff5c-40d3-9912-8b8729d09158.pdf:pdf}
}


@article{Fong.2003b,
 author = {Fong, T. and Thorpe, C. and Baur, C.},
 year = {2003},
 title = {{Multi-robot remote driving with collaborative control}},
 pages = {699--704},
 volume = {50},
 number = {4},
 issn = {0278-0046},
 journal = {{IEEE Transactions on Industrial Electronics}},
 doi = {{10.1109/TIE.2003.814768}},
 file = {51b39923-a1b4-4812-ae41-a0bc0bb93203:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\51b39923-a1b4-4812-ae41-a0bc0bb93203.pdf:pdf}
}

@article{Fang.2012,
 author = {Fang, H. C. and Ong, S. K. and Nee, A.Y.C.},
 year = {2012},
 title = {{Interactive robot trajectory planning and simulation using Augmented Reality}},
 keywords = {Augmented Reality;Planning;Trajectory},
 pages = {227--237},
 volume = {28},
 number = {2},
 issn = {07365845},
 journal = {{Robotics and Computer-Integrated Manufacturing}},
 doi = {{10.1016/j.rcim.2011.09.003}},
 file = {e4e69fb3-8073-46be-9d8d-1c3506cc5ecf:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\e4e69fb3-8073-46be-9d8d-1c3506cc5ecf.pdf:pdf}
}


@proceedings{.26.08.201631.08.2016,
 year = {26.08.2016 - 31.08.2016},
 title = {{2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)}},
 publisher = {IEEE},
 isbn = {978-1-5090-3929-6}
}


@proceedings{.28.09.201502.10.2015,
 year = {28.09.2015 - 02.10.2015},
 title = {{2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}},
 publisher = {IEEE},
 isbn = {978-1-4799-9994-1}
}


@proceedings{.02.09.201504.09.2015,
 year = {02.09.2015 - 04.09.2015},
 title = {{2015 European Conference on Mobile Robots (ECMR)}},
 publisher = {IEEE},
 isbn = {978-1-4673-9163-4}
}


@proceedings{.14.09.201418.09.2014,
 year = {14.09.2014 - 18.09.2014},
 title = {{2014 IEEE/RSJ International Conference on Intelligent Robots and Systems}},
 publisher = {IEEE},
 isbn = {978-1-4799-6934-0}
}


@proceedings{.15.09.201319.09.2013,
 year = {15.09.2013 - 19.09.2013},
 title = {{2013 IEEE Symposium on Visual Languages and Human Centric Computing}},
 publisher = {IEEE},
 isbn = {978-1-4799-0369-6}
}


@proceedings{.June24282013,
 year = {June 24-28, 2013},
 title = {{Robotics: Science and Systems IX}},
 publisher = {{Robotics: Science and Systems Foundation}},
 isbn = {9789810739379},
 doi = {{10.15607/RSS.2013.IX}}
}


@proceedings{.03.03.201306.03.2013,
 year = {03.03.2013 - 06.03.2013},
 title = {{2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}},
 publisher = {IEEE},
 isbn = {978-1-4673-3101-2}
}


@proceedings{.2013,
 year = {2013},
 title = {{2013 IEEE Workshop on Advanced Robotics and Its Social Impacts (ARSO)}},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4799-2368-7}
}


@proceedings{.02.03.201005.03.2010,
 year = {02.03.2010 - 05.03.2010},
 title = {{2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}},
 publisher = {IEEE},
 isbn = {978-1-4244-4892-0}
}


@book{.2010,
 year = {2010},
 title = {{The 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI2010) Workshop on What Do Collaborations with the Arts Have to Say about HRI}}
}


@proceedings{.2010b,
 year = {2010},
 title = {{IARP/IEEE-RAS/EURON  workshop on technical challenges for dependable robots in human environments}}
}


@proceedings{.06.09.200608.09.2006,
 year = {06.09.2006 - 08.09.2006},
 title = {{ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication}},
 publisher = {IEEE},
 isbn = {1-4244-0564-5}
}


@proceedings{.March2006,
 year = {March 2006},
 title = {{9th IEEE International Workshop on Advanced Motion Control, 2006}},
 publisher = {IEEE},
 isbn = {0-7803-9511-1}
}


@proceedings{.02.08.200502.08.2005,
 year = {02.08.2005 - 02.08.2005},
 title = {{2005 IEEE/RSJ International Conference on Intelligent Robots and Systems}},
 publisher = {IEEE},
 isbn = {0-7803-8912-3}
}


@proceedings{.2630July1993,
 year = {26-30 July 1993},
 title = {{Proceedings of 1993 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS '93)}},
 publisher = {IEEE},
 isbn = {0-7803-0823-9}
}


@proceedings{Adams.2015,
 year = {2015},
 title = {{Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction - HRI '15}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450328838},
 editor = {Adams, Julie A. and Smart, William and Mutlu, Bilge and Takayama, Leila},
 doi = {{10.1145/2696454}}
}


@inproceedings{Woods.06.09.200608.09.2006,
 author = {Woods, Sarah and Walters, Michael and Koay, Kheng and Dautenhahn, Kerstin},
 title = {{Methodological Issues in HRI: A Comparison of Live and Video-Based Methods in Robot to Human Approach Direction Trials}},
 pages = {51--58},
 publisher = {IEEE},
 isbn = {1-4244-0564-5},
 booktitle = {{ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication}},
 year = {06.09.2006 - 08.09.2006},
 doi = {{10.1109/ROMAN.2006.314394}},
 file = {8424bd14-2ff4-42ad-84aa-0da649b710e6:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\8424bd14-2ff4-42ad-84aa-0da649b710e6.pdf:pdf}
}


@inproceedings{Amershi.2019,
 author = {Amershi, Saleema and Inkpen, Kori and Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N.},
 title = {{Guidelines for Human-AI Interaction}},
 keywords = {Accuracy;Correction;Explanation;Guidelines;Support},
 pages = {1--13},
 publisher = {{ACM Press}},
 isbn = {9781450359702},
 editor = {Brewster, Stephen and Fitzpatrick, Geraldine and Cox, Anna and Kostakos, Vassilis},
 booktitle = {{Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems  - CHI '19}},
 year = {2019},
 address = {New York, New York, USA},
 doi = {{10.1145/3290605.3300233}},
 file = {http://dl.acm.org/citation.cfm?doid=3290605},
 file = {b5ec8b28-e1e9-444a-b579-9fdf3f77580b:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\b5ec8b28-e1e9-444a-b579-9fdf3f77580b.pdf:pdf}
}


@book{Anderson.2007,
 author = {Anderson, Rosemarie},
 year = {2007},
 title = {{Thematic content analysis (TCA): Descriptive presentation of qualitative data}},
 file = {e8d62d84-3599-446f-89e7-bf1259fbcb5c:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\e8d62d84-3599-446f-89e7-bf1259fbcb5c.pdf:pdf}
}


@misc{Ertel.2016,
 author = {Ertel, Wolfgang and Winter, Maik H.-J. and Rau, Harald},
 year = {2016},
 title = {{Assistenzroboter f{\"u}r Menschen mit k{\"o}rperlicher Behinderung}},
 url = {\url{http://asrobe.hs-weingarten.de/}},
 urldate = {16.09.2019}
}


@inproceedings{Endsley.2327May1988,
 author = {Endsley, M. R.},
 title = {{Situation awareness global assessment technique (SAGAT)}},
 pages = {789--795},
 publisher = {IEEE},
 booktitle = {{Proceedings of the IEEE 1988 National Aerospace and Electronics Conference}},
 year = {23-27 May 1988},
 doi = {{10.1109/NAECON.1988.195097}},
 file = {c9bad115-a23c-407e-b84f-65572f54ae59:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\c9bad115-a23c-407e-b84f-65572f54ae59.pdf:pdf}
}


@proceedings{EcoleNationaledelAviationCivileENAC.2019,
 year = {2019},
 title = {{1st International Workshop on Human-Drone Interaction}},
 address = {Glasgow, United Kingdom},
 institution = {{Ecole Nationale de l'Aviation Civile [ENAC]}}
}


@inproceedings{Dragan.2015,
 author = {Dragan, Anca D. and Bauman, Shira and Forlizzi, Jodi and Srinivasa, Siddhartha S.},
 title = {{Effects of Robot Motion on Human-Robot Collaboration}},
 keywords = {Collaborating;Human Response;Interpredictability;Motion;Planning},
 pages = {51--58},
 publisher = {{ACM Press}},
 isbn = {9781450328838},
 editor = {Adams, Julie A. and Smart, William and Mutlu, Bilge and Takayama, Leila},
 booktitle = {{Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction - HRI '15}},
 year = {2015},
 address = {New York, New York, USA},
 doi = {{10.1145/2696454.2696473}},
 file = {754b4fd1-cc0e-4e94-90e2-4ea792d688ca:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\754b4fd1-cc0e-4e94-90e2-4ea792d688ca.pdf:pdf;0ec707b8-a566-47b9-b661-60e6ded446fb:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\0ec707b8-a566-47b9-b661-60e6ded446fb.pdf:pdf}
}


@inproceedings{Dragan.June24282013,
 author = {Dragan, Anca and Srinivasa, Siddhartha},
 title = {{Generating Legible Motion}},
 publisher = {{Robotics: Science and Systems Foundation}},
 isbn = {9789810739379},
 booktitle = {{Robotics: Science and Systems IX}},
 year = {June 24-28, 2013},
 doi = {{10.15607/RSS.2013.IX.024}},
 file = {http://www.roboticsproceedings.org/rss09/index.html},
 file = {8142326d-b593-4bb4-8ebe-97481be1aa8c:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\8142326d-b593-4bb4-8ebe-97481be1aa8c.pdf:pdf}
}


@article{Dragan.2015b,
 author = {Dragan, Anca and Holladay, Rachel and Srinivasa, Siddhartha},
 year = {2015},
 title = {{Deceptive robot motion: synthesis, analysis and experiments}},
 pages = {331--345},
 volume = {39},
 number = {3},
 issn = {0929-5593},
 journal = {{Autonomous Robots}},
 doi = {{10.1007/s10514-015-9458-8}},
 file = {d10bec77-bb14-4470-8afc-0b6fae431913:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\d10bec77-bb14-4470-8afc-0b6fae431913.pdf:pdf;95f7b346-5630-4230-bd4e-cfd9fe2b2bf9:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\95f7b346-5630-4230-bd4e-cfd9fe2b2bf9.pdf:pdf}
}


@article{Dietz.2011,
 author = {Dietz, Graham},
 year = {2011},
 title = {{Going back to the source: Why do people trust each other?}},
 pages = {215--222},
 volume = {1},
 number = {2},
 issn = {2151-5581},
 journal = {{Journal of Trust Research}},
 doi = {{10.1080/21515581.2011.603514}},
 file = {6365e71d-2516-47dd-9f90-7423db3727ee:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\6365e71d-2516-47dd-9f90-7423db3727ee.pdf:pdf}
}


@article{Dautenhahn.2007,
 abstract = {Social intelligence in robots has a quite recent history in artificial intelligence and robotics. However, it has become increasingly apparent that social and interactive skills are necessary requirements in many application areas and contexts where robots need to interact and collaborate with other robots or humans. Research on human-robot interaction (HRI) poses many challenges regarding the nature of interactivity and 'social behaviour' in robot and humans. The first part of this paper addresses dimensions of HRI, discussing requirements on social skills for robots and introducing the conceptual space of HRI studies. In order to illustrate these concepts, two examples of HRI research are presented. First, research is surveyed which investigates the development of a cognitive robot companion. The aim of this work is to develop social rules for robot behaviour (a 'robotiquette') that is comfortable and acceptable to humans. Second, robots are discussed as possible educational or therapeutic toys for children with autism. The concept of interactive emergence in human-child interactions is highlighted. Different types of play among children are discussed in the light of their potential investigation in human-robot experiments. The paper concludes by examining different paradigms regarding 'social relationships' of robots and people interacting with them.},
 author = {Dautenhahn, Kerstin},
 year = {2007},
 title = {{Socially intelligent robots: dimensions of human-robot interaction}},
 pages = {679--704},
 volume = {362},
 number = {1480},
 issn = {0962-8436},
 journal = {{Philosophical transactions of the Royal Society of London. Series B, Biological sciences}},
 doi = {{10.1098/rstb.2006.2004}},
 file = {35b922f4-af87-46d6-b94d-4b22ebcb1d9d:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\35b922f4-af87-46d6-b94d-4b22ebcb1d9d.pdf:pdf}
}

@proceedings{Brewster.2019,
 year = {2019},
 title = {{Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems  - CHI '19}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450359702},
 editor = {Brewster, Stephen and Fitzpatrick, Geraldine and Cox, Anna and Kostakos, Vassilis},
 doi = {{10.1145/3290605}}
}


@inproceedings{Breazeal.02.08.200502.08.2005,
 author = {Breazeal, Cynthia and Kidd, Cory D. and Thomaz, Andrea Lockerd and Hoffman, Guy and Berlin, Matt},
 title = {{Effects of nonverbal communication on efficiency and robustness in human-robot teamwork}},
 keywords = {Collaborating;Communication;Coordinating;Explicit;Human Response;Implicit;Information},
 pages = {708--713},
 publisher = {IEEE},
 isbn = {0-7803-8912-3},
 booktitle = {{2005 IEEE/RSJ International Conference on Intelligent Robots and Systems}},
 year = {02.08.2005 - 02.08.2005},
 doi = {{10.1109/IROS.2005.1545011}},
 file = {99fca1af-2fce-44db-9a95-55af16557947:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\99fca1af-2fce-44db-9a95-55af16557947.pdf:pdf;aa396920-c3fc-469d-bced-cb0a4285b854:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\aa396920-c3fc-469d-bced-cb0a4285b854.pdf:pdf}
}



@proceedings{Billard.2011,
 year = {2011},
 title = {{Proceedings of the 6th international conference on Human-robot interaction - HRI '11}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450305617},
 editor = {Billard, Aude and Kahn, Peter and Adams, Julie A. and Trafton, Greg},
 doi = {{10.1145/1957656}}
}





@inproceedings{Woods.March2006,
 author = {Woods, Sarah N. and Walters, Michael L. and Koay, Kheng Lee and Dautenhahn, Kerstin},
 title = {{Comparing human robot interaction scenarios using live and video based methods: towards a novel methodological approach}},
 keywords = {Prototyping;Testing},
 pages = {750--755},

 isbn = {0-7803-9511-1},
 booktitle = {{9th IEEE International Workshop on Advanced Motion Control, 2006}},
 year = {March 2006},
 doi = {{10.1109/AMC.2006.1631754}},
 file = {5d4e2d58-6634-4074-ad82-0022d5b03462:C\:\\Users\\Max\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\b2fxu7fdpi4b8bpq11x7slb3keh4txwjhfiyfvr8\\Citavi Attachments\\5d4e2d58-6634-4074-ad82-0022d5b03462.pdf:pdf}
}

@inproceedings{Pascher2019b,
title = {SwipeBuddy: A Teleoperated Tablet and Ebook-Reader Holder for a Hands-Free Interaction},
author = {Max Pascher and Stefan Schneegass and Jens Gerken},
editor = {David Lamas and Fernando Loizides and Lennart Nacke and Helen Petrie and Marco Winckler and Panayiotis Zaphiris},
url = {https://hci.w-hs.de/pub_SwipeBuddy/, PDF Download},
doi = {10.1007/978-3-030-29390-1_39},
isbn = {978-3-030-29390-1},
year = {2019},
date = {2019-08-23},
booktitle = {Human-Computer Interaction – INTERACT 2019},
issuetitle = {Lecture Notes in Computer Science},
volume = {11749},
pages = {568-571},
publisher = {Springer, Cham},
abstract = {Mobile devices are the core computing platform we use in our everyday life to communicate with friends, watch movies, or read books. For people with severe physical disabilities, such as tetraplegics, who cannot use their hands to operate such devices, these devices are barely usable. Tackling this challenge, we propose SwipeBuddy, a teleoperated robot allowing for touch interaction with a smartphone, tablet, or ebook-reader. The mobile device is mounted on top of the robot and can be teleoperated by a user through head motions and gestures controlling a stylus simulating touch input. Further, the user can control the position and orientation of the mobile device. We demonstrate the SwipeBuddy robot device and its different interaction capabilities.},
keywords = {hands-free, head gestures, head-based interaction, human-robot collaboration, human-robot interaction, robot control, self-determined life},
pubstate = {published},
tppubtype = {inproceedings}
}

@misc{Anoymous,
title = "{ANONYMOUS FOR REVIEW}",
}

@inbook{Pascher2020,
title = {Praxisbeispiel Digitalisierung konkret: Wenn der Stromzähler weiß, ob es Oma gut geht. Beschreibung des minimalinvasiven Frühwarnsystems „ZELIA“},
author = {Max Pascher},
editor = {Michael Vilain},
doi = {10.5771/9783748907008-137},
isbn = {78-3-8487-6621-5},
year = {2020},
date = {2020-10-06},
booktitle = {Wege in die digitale Zukunft - Was bedeuten Smart Living, Big Data, Robotik & Co für die Sozialwirtschaft?},
pages = {137-148},
publisher = {Nomos Verlagsgesellschaft mbH & Co. KG},
keywords = {AAL, consumption, data science, elderly, monitoring, wellbeing},
pubstate = {published},
tppubtype = {inbook}
}



@article{Kerr.1987,
  doi = {10.1037/h0091572},
  url = {https://doi.org/10.1037/h0091572},
  year = {1987},
  publisher = {American Psychological Association ({APA})},
  volume = {32},
  number = {3},
  pages = {173--180},
  author = {Nancy Kerr and Lee Meyerson},
  title = {Independence as a goal and a value of people with physical disabilities: Some caveats.},
  journal = {Rehabilitation Psychology}
}

@article{Ajoudani.2017,
  doi = {10.1007/s10514-017-9677-2},
  url = {https://doi.org/10.1007/s10514-017-9677-2},
  year = {2017},
  month = oct,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {42},
  number = {5},
  pages = {957--975},
  author = {Arash Ajoudani and Andrea Maria Zanchettin and Serena Ivaldi and Alin Albu-Sch\"{a}ffer and Kazuhiro Kosuge and Oussama Khatib},
  title = {Progress and prospects of the human{\textendash}robot collaboration},
  journal = {Autonomous Robots}
}





@misc{Colgate.1996,
  title={Cobots: Robots for collaboration with human operators},
  author={Colgate, J Edward and Edward, J and Peshkin, Michael A and Wannasuphoprasit, Witaya},
  year={1996},
  publisher={Citeseer}
}
@article{Pollak.2020,
title = {Stress in manual and autonomous modes of collaboration with a cobot},
journal = {Computers in Human Behavior},
volume = {112},
pages = {106469},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106469},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220302211},
author = {Anita Pollak and Mateusz Paliga and Matias M. Pulopulos and Barbara Kozusznik and Malgorzata W. Kozusznik},
keywords = {Collaborative robot, Cobot, Stress, Heart rate, Primary appraisal, Secondary appraisal},
abstract = {Working with collaborative robots (cobots) can be a potential source of stress for their operators. However, research on specific factors that affect users’ stress levels when working with a cobot is still scarce. This study is the first to investigate the levels of psychological (primary and secondary stress appraisal) and physiological (heart rate) stress in human operators working in two different cobot modes (i.e., manual and autonomous). We applied an experimental within-subject repeated-measures design to 45 healthy adults (26 women, 19 men). The results show that the levels of secondary stress appraisal were lower and the heart rate levels were higher in the autonomous cobot mode. The results suggest that, when working with a cobot, control plays a key role in the emotional, cognitive, and physiological reactions during the human-robot collaboration. Implications for organizational practice are discussed.}
}



@inproceedings{Pascher2021,
title = {Recommendations for the Development of a Robotic Drinking and Eating Aid - An Ethnographic Study },
author = {Max Pascher and Annalies Baumeister and Stefan Schneegass and Barbara Klein and Jens Gerken},
editor = {Carmelo Ardito and  Rosa Lanzilotti and Alessio Malizia and Helen Petrie and Antonio Piccinno and Giuseppe Desolda and Kori Inkpen},
url = {https://hci.w-hs.de/pub_recommendations_for_the_development_of_a_robotic_drinking_and_eating_aid___an_ethnographic_study/, PDF Download},
doi = {10.1007/978-3-030-85623-6_21},
year = {2021},
date = {2021-09-01},
booktitle = {Human-Computer Interaction – INTERACT 2021},
publisher = {Springer, Cham},
abstract = {Being able to live independently and self-determined in one's own home is a crucial factor for human dignity and preservation of self-worth. For people with severe physical impairments who cannot use their limbs for every day tasks, living in their own home is only possible with assistance from others. The inability to move arms and hands makes it hard to take care of oneself, e. g. drinking and eating independently. In this paper, we investigate how 15 participants with disabilities consume food and drinks. We report on interviews, participatory observations, and analyzed the aids they currently use. Based on our findings, we derive a set of recommendations that supports researchers and practitioners in designing future robotic drinking and eating aids for people with disabilities.},
keywords = {assisted living technologies, human-centered computing, meal assistance, participation design, people with disabilities, robot assistive drinking, robot assistive feeding, user acceptance, user participation, user-centered design},
}





@INPROCEEDINGS{Biocca.2006,  author={Biocca, F. and Tang, A. and Owen, C. and Xiao Fan},  booktitle={Proceedings of the 39th Annual Hawaii International Conference on System Sciences (HICSS'06)},   title={The Omnidirectional Attention Funnel: A Dynamic 3D Cursor for Mobile Augmented Reality Systems},   year={2006},  volume={1},  number={},  pages={22c-22c},  doi={10.1109/HICSS.2006.476}}



% This file was created with Citavi 6.10.0.0

@incollection{Canal.2016,
 abstract = {The deployment of robots at home must involve robots with pre-defined skills and the capability of personalizing their behavior by non-expert users. A framework to tackle this personalization is presented and applied to an automatic feeding task. The personalization involves the caregiver providing several examples of feeding using Learning-by-Demostration, and a ProMP formalism to compute an overall trajectory and the variance along the path. Experiments show the validity of the approach in generating different feeding motions to adapt to user's preferences, automatically extracting the relevant task parameters. The importance of the nature of the demonstrations is also assessed, and two training strategies are compared.},
 author = {Canal, Gerard and Aleny{\`a}, Guillem and Torras, Carme},
 title = {{Personalization Framework for Adaptive Robotic Feeding Assistance}},
 keywords = {Assistive robotics;Feeding;Personalized Human-Robot Interaction;Trajectory adaptation},
 pages = {22--31},
 volume = {9979},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-47436-6 978-3-319-47437-3},
 booktitle = {{Social Robotics}},
 year = {2016},
 address = {Cham},
 doi = {10.1007/978-3-319-47437-3{\textunderscore }3},
 file = {Canal, Aleny{\`a} et al. 2016 - Personalization Framework for Adaptive Robotic:Attachments/Canal, Aleny{\`a} et al. 2016 - Personalization Framework for Adaptive Robotic.pdf:application/pdf}
}

@article{McColl.2013,
 abstract = {As the world's elderly population continues to grow, so does the number of individuals diagnosed with cognitive impairments. It is estimated that 115 million people will have age-related memory loss by 2050 [1]. The number of older adults who have difficulties performing self-care and independent-living activities increases significantly with the prevalence of cognitive impairment. This is especially true for the population over 70 years of age [2]. Cognitive impairment, as a result of dementia, severely affects a person's ability to independently initiate and perform daily activities, as cognitive abilities can be diminished [3]. If a person is incapable of performing these activities, continuous assistance from others is necessary. In 2010, the total worldwide cost of dementia (including medical, social, and informal care costs) was estimated to be US604 billion [1].},
 author = {McColl, D. and Louie, Wing-Yue Geoffrey and Nejat, G.},
 year = {2013},
 title = {{Brian 2.1: A Socially Assistive Robot for the Elderly and Cognitively Impaired}},
 keywords = {Social sciences},
 pages = {74--83},
 volume = {20},
 number = {1},
 issn = {1070-9932},
 journal = {{IEEE Robotics {\&} Automation Magazine}},
 doi = {10.1109/MRA.2012.2229939},
 file = {McColl, Louie et al. 2013 - Brian 2.1 A Socially Assistive:Attachments/McColl, Louie et al. 2013 - Brian 2.1 A Socially Assistive.pdf:application/pdf}
}


@article{Tanaka.2014,
 author = {Tanaka, Kanya and Mu, Shenglin and Nakashima, Shota},
 year = {2014},
 title = {{Meal-Assistance Robot Using Ultrasonic Motor with Eye Interface}},
 pages = {186--192},
 volume = {8},
 number = {2},
 issn = {1883-8022},
 journal = {{International Journal of Automation Technology}},
 doi = {10.20965/ijat.2014.p0186},
 file = {Tanaka, Mu et al. 2014 - Meal-Assistance Robot Using Ultrasonic Motor:Attachments/Tanaka, Mu et al. 2014 - Meal-Assistance Robot Using Ultrasonic Motor.pdf:application/pdf}
}

@INPROCEEDINGS{Ren.2016,  author={Ren, Donghao and Goldschwendt, Tibor and Chang, YunSuk and Höllerer, Tobias},  booktitle={2016 IEEE Virtual Reality (VR)},   title={Evaluating wide-field-of-view augmented reality with mixed reality simulation},   year={2016},  volume={},  number={},  pages={93-102},  doi={10.1109/VR.2016.7504692}}





% This file was created with Citavi 6.11.0.0
@inbook{Lehtinen.2012,
author = {Lehtinen, Ville and Oulasvirta, Antti and Salovaara, Antti and Nurmi, Petteri},
title = {Dynamic Tactile Guidance for Visual Search Tasks},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380173},
abstract = {Visual search in large real-world scenes is both time consuming and frustrating, because the search becomes serial when items are visually similar. Tactile guidance techniques can facilitate search by allowing visual attention to focus on a subregion of the scene. We present a technique for dynamic tactile cueing that couples hand position with a scene position and uses tactile feedback to guide the hand actively toward the target. We demonstrate substantial improvements in task performance over a baseline of visual search only, when the scene's complexity increases. Analyzing task performance, we demonstrate that the effect of visual complexity can be practically eliminated through improved spatial precision of the guidance.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {445–452},
numpages = {8}
}

@inproceedings{Regenbrecht.2005,
author = {Regenbrecht, Holger and Hauber, Joerg and Schoenfelder, Ralph and Maegerlein, Andreas},
title = {Virtual Reality Aided Assembly with Directional Vibro-Tactile Feedback},
year = {2005},
isbn = {1595932011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1101389.1101464},
doi = {10.1145/1101389.1101464},
abstract = {We introduce different approaches to user interface devices that provide directed tactile feedback to the user's hand. The basic idea is to enhance the user's six degrees of freedom of interaction within virtual or augmented environments by offering an additional three-dimensional tactile feedback as an immediate, directed response from the virtual world. We also describe the prototype systems TactilePointer and TACTool, which utilize vibro-motors, alarm buzzers, and piezo bend elements as actuators in combination with magnetic and optical tracking. The prototypes have been informally tested within collision sensitive virtual environments.},
booktitle = {Proceedings of the 3rd International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia},
pages = {381–387},
numpages = {7},
keywords = {virtual reality, haptics, vibro-tactile feedback, interaction},
location = {Dunedin, New Zealand},
series = {GRAPHITE '05}
}

@inproceedings{mci/Kniewel2008,
author = {Kniewel, Romy AND Hipp, Cornelia},
title = {Gestaltung einer haptischen Navigations hilfe für den mobilen Bereich},
booktitle = {Tagungsband UP08},
year = {2008},
editor = {Brau, Henning AND Diefenbach, Sarah AND Hassenzahl, Marc AND Koller, Franz AND Peissner, Matthias AND Röse, Kerstin} ,
pages = { 264-269 },
publisher = {Fraunhofer Verlag},
address = {Stuttgart}
} 

@ARTICLE{Barralon.2009,  author={Barralon, Pierre and Ng, Ginna and Dumont, Guy A. and Schwarz, Stephan K. W. and Ansermino, J. Mark},  journal={IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},   title={Design of Rhythm-Based Vibrotactile Stimuli Around the Waist: Evaluation of Two Encoding Parameters},   year={2009},  volume={39},  number={5},  pages={1062-1073},  doi={10.1109/TSMCA.2009.2025026}}

@book{Bernhaupt.2017,
 author = {Bernhaupt, Regina and Dalvi, Girish and Joshi, Anirudha and {K. Balkrishan}, Devanuj and O'Neill, Jacki and Winckler, Marco},
 year = {2017},
 title = {{Human-Computer Interaction -- INTERACT 2017}},
 address = {Cham},
 volume = {10516},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-68058-3},
 doi = {10.1007/978-3-319-68059-0},
 file = {Kaul2017{\_}Chapter{\_}IncreasingPresenceInVirtualRea:Attachments/Kaul2017{\_}Chapter{\_}IncreasingPresenceInVirtualRea.pdf:application/pdf}
}


@inproceedings{Chen.2018,
 abstract = {-  Human-centered computing  -{\textgreater}  Virtual reality; Haptic devices;},
 author = {Chen, Taizhou and Wu, Yi-Shiun and Zhu, Kening},
 title = {{Investigating different modalities of directional cues for multi-task visual-searching scenario in virtual reality}},
 keywords = {Auditory;Directional cue;Multi-task;Vibration;virtual reality;Visual},
 pages = {1--5},
 publisher = {ACM},
 isbn = {9781450360869},
 editor = {Spencer, Stephen N. and Morishima, Shigeo and Itoh, Yuichi and Shiratori, Takaaki and Yue, Yonghao and Lindeman, Rob},
 booktitle = {{Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology}},
 year = {2018},
 address = {New York, NY, USA},
 doi = {10.1145/3281505.3281516},
 file = {3281505.3281516:Attachments/3281505.3281516.pdf:application/pdf}
}


@article{Grushko.2021,
 abstract = {In this work, we extend the previously proposed approach of improving mutual perception during human-robot collaboration by communicating the robot's motion intentions and status to a human worker using hand-worn haptic feedback devices. The improvement is presented by introducing spatial tactile feedback, which provides the human worker with more intuitive information about the currently planned robot's trajectory, given its spatial configuration. The enhanced feedback devices communicate directional information through activation of six tactors spatially organised to represent an orthogonal coordinate frame: the vibration activates on the side of the feedback device that is closest to the future path of the robot. To test the effectiveness of the improved human-machine interface, two user studies were prepared and conducted. The first study aimed to quantitatively evaluate the ease of differentiating activation of individual tactors of the notification devices. The second user study aimed to assess the overall usability of the enhanced notification mode for improving human awareness about the planned trajectory of a robot. The results of the first experiment allowed to identify the tactors for which vibration intensity was most often confused by users. The results of the second experiment showed that the enhanced notification system allowed the participants to complete the task faster and, in general, improved user awareness of the robot's movement plan, according to both objective and subjective data. Moreover, the majority of participants (82{\%}) favoured the improved notification system over its previous non-directional version and vision-based inspection.},
 author = {Grushko, Stefan and Vysock{\'y}, Ale{\v{s}} and Heczko, Dominik and Bobovsk{\'y}, Zdenko},
 year = {2021},
 title = {{Intuitive Spatial Tactile Feedback for Better Awareness about Robot Trajectory during Human-Robot Collaboration}},
 volume = {21},
 number = {17},
 journal = {{Sensors (Basel, Switzerland)}},
 doi = {10.3390/s21175748},
 file = {Grushko, Vysock{\'y} et al. 2021 - Intuitive Spatial Tactile Feedback:Attachments/Grushko, Vysock{\'y} et al. 2021 - Intuitive Spatial Tactile Feedback.pdf:application/pdf}
}


@inproceedings{Hong.2017,
 author = {Hong, Jonggi and Pradhan, Alisha and Froehlich, Jon E. and Findlater, Leah},
 title = {{Evaluating Wrist-Based Haptic Feedback for Non-Visual Target Finding and Path Tracing on a 2D Surface}},
 pages = {210--219},
 publisher = {ACM},
 isbn = {9781450349260},
 editor = {Hurst, Amy and Findlater, Leah and Morris, Meredith Ringel},
 booktitle = {{Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility}},
 year = {2017},
 address = {New York, NY, USA},
 doi = {10.1145/3132525.3132538},
 file = {3132525.3132538:Attachments/3132525.3132538.pdf:application/pdf}
}


@inproceedings{Nakamura.2021,
 abstract = {-  Human-centered computing  -{\textgreater}  Virtual reality.Haptic devices.},
 author = {Nakamura, Fumihiko and Verhulst, Adrien and Sakurada, Kuniharu and Fukuoka, Masaaki and Sugimoto, Maki},
 title = {{Virtual Whiskers: Cheek Haptic-Based Spatial Directional Guidance in a Virtual Space}},
 keywords = {cheek haptics;robot arm;spatial guidance;virtual reality},
 pages = {1--2},
 publisher = {ACM},
 isbn = {9781450390750},
 editor = {Shiota, Shuzo John and Kimura, Ayumi and Sandor, Christian and Sugimoto, Maki},
 booktitle = {{SIGGRAPH Asia 2021 XR}},
 year = {2021},
 address = {New York, NY, USA},
 doi = {10.1145/3478514.3487625},
 file = {3478514.3487625:Attachments/3478514.3487625.pdf:application/pdf}
}


@inproceedings{Uchiyama.2008,
 author = {Uchiyama, H. and Covington, M. A. and Potter, W. D.},
 title = {{Vibrotactile Glove guidance for semi-autonomous wheelchair operations}},
 keywords = {semi-autonomous wheelchair;tactile display;vibrotactile glove},
 pages = {336},
 publisher = {{ACM Press}},
 isbn = {9781605581057},
 editor = {Unknown},
 booktitle = {{Proceedings of the 46th Annual Southeast Regional Conference on XX - ACM-SE 46}},
 year = {2008},
 address = {New York, New York, USA},
 doi = {10.1145/1593105.1593195},
 file = {1593105.1593195:Attachments/1593105.1593195.pdf:application/pdf}
}


@inproceedings{Weber.2011,
 abstract = {IEEE World Haptics Conference 2011; 21-24 June, Istanbul, Turkey; 978-1-4577-0297-6/11/{\$}26.00 {\copyright}2011 IEEE},
 author = {Weber, B. and Schatzle, S. and Hulin, T. and Preusche, C. and Deml, B.},
 title = {{Evaluation of a vibrotactile feedback device for spatial guidance}},
 keywords = {Human factors and ergonomics;Human-computer interaction;Tactile devices and display},
 pages = {349--354},
 publisher = {IEEE},
 isbn = {978-1-4577-0299-0},
 booktitle = {{2011 IEEE World Haptics Conference}},
 year = {2011},
 doi = {10.1109/WHC.2011.5945511},
 file = {weber:Attachments/weber.pdf:application/pdf}
}

@article{Kronhardt.2022,
title = {Adapt or Perish? Exploring the Effectiveness of Adaptive DoF Control Interaction Methods for Assistive Robot Arms},
author = {Kirill Kronhardt and Stephan Rübner and Max Pascher and Felix Goldau and Udo Frese and Jens Gerken},
url = {https://hci.w-hs.de/pub_technologies-10-00030/, PDF Download
https://youtu.be/AEK4AOQKz1k},
doi = {10.3390/technologies10010030},
issn = {2227-7080},
year = {2022},
date = {2022-02-14},
urldate = {2022-02-14},
journal = {Technologies},
volume = {10},
number = {1},
abstract = {Robot arms are one of many assistive technologies used by people with motor impairments.Assistive robot arms can allow people to perform activities of daily living (ADL) involving graspingand manipulating objects in their environment without the assistance of caregivers. Suitable inputdevices (e.g., joysticks) mostly have two Degrees of Freedom (DoF), while most assistive robot armshave six or more. This results in time-consuming and cognitively demanding mode switches tochange the mapping of DoFs to control the robot. One option to decrease the difficulty of controllinga high-DoF assistive robot arm using a low-DoF input device is to assign different combinations ofmovement-DoFs to the device’s input DoFs depending on the current situation (adaptive control). Toexplore this method of control, we designed two adaptive control methods for a realistic virtual 3Denvironment. We evaluated our methods against a commonly used non-adaptive control method thatrequires the user to switch controls manually. This was conducted in a simulated remote study thatused Virtual Reality and involved 39 non-disabled participants. Our results show that the numberof mode switches necessary to complete a simple pick-and-place task decreases significantly whenusing an adaptive control type. In contrast, the task completion time and workload stay the same. Athematic analysis of qualitative feedback of our participants suggests that a longer period of trainingcould further improve the performance of adaptive control methods.},
keywords = {assistive robotics, augmented reality, human-robot interaction, shared user control, virtual reality, visual cues}
}

@Article{Pascher.2022,
AUTHOR = {Pascher, Max and Kronhardt, Kirill and Franzen, Til and Gruenefeld, Uwe and Schneegass, Stefan and Gerken, Jens},
TITLE = {My Caregiver the Cobot: Comparing Visualization Techniques to Effectively Communicate Cobot Perception to People with Physical Impairments},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {755},
URL = {https://www.mdpi.com/1424-8220/22/3/755},
PubMedID = {35161503},
ISSN = {1424-8220},
ABSTRACT = {Nowadays, robots are found in a growing number of areas where they collaborate closely with humans. Enabled by lightweight materials and safety sensors, these cobots are gaining increasing popularity in domestic care, where they support people with physical impairments in their everyday lives. However, when cobots perform actions autonomously, it remains challenging for human collaborators to understand and predict their behavior, which is crucial for achieving trust and user acceptance. One significant aspect of predicting cobot behavior is understanding their perception and comprehending how they &ldquo;see&rdquo; the world. To tackle this challenge, we compared three different visualization techniques for Spatial Augmented Reality. All of these communicate cobot perception by visually indicating which objects in the cobot&rsquo;s surrounding have been identified by their sensors. We compared the well-established visualizations Wedge and Halo against our proposed visualization Line in a remote user experiment with participants suffering from physical impairments. In a second remote experiment, we validated these findings with a broader non-specific user base. Our findings show that Line, a lower complexity visualization, results in significantly faster reaction times compared to Halo, and lower task load compared to both Wedge and Halo. Overall, users prefer Line as a more straightforward visualization. In Spatial Augmented Reality, with its known disadvantage of limited projection area size, established off-screen visualizations are not effective in communicating cobot perception and Line presents an easy-to-understand alternative.},
DOI = {10.3390/s22030755}
}

@inproceedings{Arevalo-Arboleda2021,
title = {Assisting Manipulation and Grasping in Robot Teleoperation with Augmented Reality Visual Cues},
author = {Stephanie Arévalo-Arboleda and Franziska Ruecker and Tim Dierks and Jens Gerken},
url = {https://hci.w-hs.de/pub_VisualCuesCHI_compressed/, PDF Download},
doi = {10.1145/3411764.3445398},
isbn = {978-1-4503-8096-6/21/05},
year = {2021},
date = {2021-05-03},
booktitle = {CHI Conference on Human Factors in Computing Systems (CHI '21)},
publisher = {ACM},
abstract = {Teleoperating industrial manipulators in co-located spaces can be challenging. Facilitating robot teleoperation by providing additional visual information about the environment and the robot affordances using augmented reality (AR), can improve task performance in manipulation and grasping. In this paper, we present two designs of augmented visual cues, that aim to enhance the visual space of the robot operator through hints about the position of the robot gripper in the workspace and in relation to the target. These visual cues aim to improve the distance perception and thus, the task performance. We evaluate both designs against a baseline in an experiment where participants teleoperate a robotic arm to perform pick-and-place tasks. Our results show performance improvements in different levels, reflecting in objective and subjective measures with trade-offs in terms of time, accuracy, and participants’ views of teleoperation. These findings show the potential of AR not only in teleoperation, but in understanding the human-robot workspace.},
keywords = {augmented reality, depth perception, hands-free interaction, human-robot interaction, teleoperation, visual cues}
}

@Article{Rakkolainen2021,
AUTHOR = {Rakkolainen, Ismo and Farooq, Ahmed and Kangas, Jari and Hakulinen, Jaakko and Rantala, Jussi and Turunen, Markku and Raisamo, Roope},
TITLE = {Technologies for Multimodal Interaction in Extended Reality - A Scoping Review},
JOURNAL = {Multimodal Technologies and Interaction},
VOLUME = {5},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {81},
URL = {https://www.mdpi.com/2414-4088/5/12/81},
ISSN = {2414-4088},
ABSTRACT = {When designing extended reality (XR) applications, it is important to consider multimodal interaction techniques, which employ several human senses simultaneously. Multimodal interaction can transform how people communicate remotely, practice for tasks, entertain themselves, process information visualizations, and make decisions based on the provided information. This scoping review summarized recent advances in multimodal interaction technologies for head-mounted display-based (HMD) XR systems. Our purpose was to provide a succinct, yet clear, insightful, and structured overview of emerging, underused multimodal technologies beyond standard video and audio for XR interaction, and to find research gaps. The review aimed to help XR practitioners to apply multimodal interaction techniques and interaction researchers to direct future efforts towards relevant issues on multimodal XR. We conclude with our perspective on promising research avenues for multimodal interaction technologies.},
DOI = {10.3390/mti5120081}
}

