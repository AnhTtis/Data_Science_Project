\label{sec:rela}

\noindent \textbf{Cytology Instance Segmentation:}
Challenges such as cell clustering, hard mimics, and semi-transparent overlapping cytoplasm regions pose in the segmentation of cellular instance (e.g., cell, nuclei, cytoplasm) from cytology images. These challenges inspired numerous insightful works, especially after the publication of ISBI2014\cite{ushizima2015segmentation}. Two mainstream approaches are proposed to tackle the overlapping cell segmentation challenge: the segment-then-refine stream and the end-to-end training stream.


Early approaches tackling this challenge are primarily the combination of pixel-level segmentation models and additional post-processing techniques, such as seeded watershed algorithm \cite{kowal2020cell}, random walk \cite{zhang2020polar}, conditional random field algorithms \cite{wan2019accurate}, and star-convex parameterization \cite{walter2021multistar}. To further improve the segmentation performance, some other methods introduce morphological prior. Song et al. design a dynamic multi-template deformation model, which leverages case-specific shape constraints to guide the inference of overlapping cell boundaries\cite{song2016accurate}. Tareef et al. propose to refine cytoplasm boundary via a learnable shape prior, which is dynamically modeled as the fusion of shape templates \cite{tareef2017optimizing}. 

Escaping from the complex post-processing procedure, many later works turn to end-to-end training. These approaches typically adopt Mask R-CNN as their baseline. The first attempt following this approach was \cite{sompawong2019automated}, a model for nuclear instance segmentation from liquid-based cytology smears. Based on the appearance similarity between different cells, Zhou et al. propose the IRNet to explore instance relation during overlapping cervical cell segmentation \cite{zhou2019irnet}. To leverage information from unlabeled data, Zhou et al. provide a mean-teacher-based semi-supervised learning scheme, MMT-PSM \cite{zhou2020deep}. 

The above two methods have achieved notable performance improvements, yet still lack the necessary perceptual capability for overlapping regions, leading to sub-optimal results, i.e., ambiguous cell boundaries. In this study, we propose a de-overlapping network, named DoNet, to implicitly and explicitly model the interaction between the integral instance and its sub-regions. 

\noindent \textbf{Occluded Instance Segmentation:}
Instance segmentation, a typical task in computer vision, refers to the inference of bounding boxes and instance segmentation masks. Most notable approaches follow the detect-then-segment paradigm, such as Mask R-CNN \cite{he2017mask} and its following variants \cite{huang2019mask,cai2018cascade}. However, these approaches could not handle the occlusion problem, common in visual application scenarios like robotic manipulations, scene parsing, and autonomous driving \cite{back2022unseen,geiger2012we}. Due to the inconsistencies within a Regions of Interest (RoI), these models suffer from the limited perceptual capability, leading to ambiguous segmentation boundaries. To tackle this issue, a specific task, occluded instance segmentation aims at leveraging visible regions to  perceive the entirety of the occluded instance accurately. This capability of inferring and reasoning occluded objects is defined as amodal perception \cite{follmann2019learning}, thus occluded instance segmentation is also called amodal instance segmentation.

Research on amodal instance segmentation starts from \cite{li2016amodal}, which poses this challenging task and gives a synthesis-based solution. In this work, the first amodal instance segmentation dataset, the COCOA dataset, is established based on the COCO dataset. Afterward, some notable solutions accomplish this task and achieve state-of-the-art performance. Previous approaches tackle this challenge by leveraging information from visible instance segmentation, then inferring the amodal mask \cite{follmann2019learning,back2022unseen}. For example, Occlusion R-CNN (ORCNN) is built on R-CNN with two mask heads, the visible mask head, and the amodal mask head, which directly predict the amodal mask and the visible mask, inferring the visible mask through the subtraction of these two masks \cite{follmann2019learning}. Qi et al. \cite{qi2019amodal} perform amodal instance segmentation with multi-level coding and establish a large-scale amodal instance segmentation dataset. Xiao et al. \cite{xiao2021amodal} leverage shape prior knowledge to infer amodal mask using memory codebook. Some other models, like BCNet \cite{ke2021deep} abandon the visible instance prediction based manner and directly models the relationship between the occluder and the occludee. To address the lack of amodal mask ground truth, ASBU \cite{nguyen2021weakly} introduces a weakly supervised amodal segmenter, which generates pseudo-ground truth by boundary uncertainty estimation.

Essentially, the above occluded instance segmentation methods aim to enhance the amodal perception and reasoning capability, inspiring us to explore the interaction between instance sub-regions and the integral instance. Compared to amodal segmentation, whose focus on inferring the invisible region from the partially visible regions, our focus is to resolve the inconsistency between the intersection and complement regions.
