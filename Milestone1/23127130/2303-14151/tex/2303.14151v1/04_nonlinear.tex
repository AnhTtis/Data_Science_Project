\section{Intuition Extends to Nonlinear Models}

Although we mathematically studied ordinary linear regression, the intuition for why double descent occurs extends to nonlinear models, including deep neural networks.
For instance, we can conduct the same analysis we did for linear regression but instead for polynomial regression by considering the training covariates $X$ projected into a (typically much higher dimensional) polynomial feature space: 
%
\begin{equation*}
    \Phi_P(X) \, \defeq \, \begin{bmatrix}
    -\Vec{\phi}_P(\vec{x}_1)-\\
    -\Vec{\phi}_P(\vec{x}_2)-\\
    \vdots\\
    -\Vec{\phi}_P(\vec{x}_N)-\\
    \end{bmatrix}
\end{equation*}

For certain classes of infinitely wide deep neural networks (e.g., as studied \cite{jacot2018neural, lee2017deep, bordelon2020spectrum}), the output predictions of the network are equivalent to a linear regression performed on a particular set of features. Different infinite width limits correspond to different sets of features; for instance, one such limit (known as the Neural Network Gaussian Process limit \cite{lee2017deep}), studies the training covariates $X$ represented in the penultimate layer of the deep neural network $\vec{f}_{\theta}(x)$:
%
\begin{equation*}
    F_{\theta}(X) \, \defeq \, \begin{bmatrix}
    -\Vec{f}_{\theta}(\vec{x}_1)-\\
    -\Vec{f}_{\theta}(\vec{x}_2)-\\
    \vdots\\
    -\Vec{f}_{\theta}(\vec{x}_N)-\\
    \end{bmatrix}
\end{equation*}

For a concrete example about how our intuition can shed light on the behavior of nonlinear models, \cite{henighan2023superposition} recently discovered interesting properties of shallow nonlinear autoencoders: depending on the number of training data, (1) autoencoders either store data points or features, and (2) double descent occurs between these two regimes. Our tutorial helps explains the results, and also sheds light on two comments the authors make:

\begin{enumerate}
    \item \cite{henighan2023superposition} write, ``[Our work] suggests a naive mechanistic theory of overfitting and memorization: memorization and overfitting occur when models operate on ``data point features" instead of "generalizing features". We expect this naive theory to be overly simplistic, but it seems possible that it's gesturing at useful principles!" Our tutorial hopefully clarifies that this choice of terminology (``data point features" vs. "generalizing features") can be made more precise. When overparameterized, the ``data point features" are akin to the data-by-data Gram matrix $X X^T \in \mathbb{R}^{N \times N}$ and when underparameterized, the ``generalizing features" are akin to the feature-by-feature second moment matrix $X^T X \in \mathbb{R}^{D \times D}$. Our tutorial hopefully shows that ``data point features" can (and very often do) generalize, and that there is a deep connection between the two, e.g., their shared spectra. %For these reasons, we suggest the use of alternative terminology: ``data representations" and ``feature representations".
    % XXX You should be able to not just state the above, but directly show it on the data used in the paper. 
    % Rylan 2023-03-23: Agreed, but I'm not sure whether doing so is worth the time. Could you please advise?
    \item \cite{henighan2023superposition} write, ``It’s interesting to note that we’re observing double-descent in the absence of label noise. That is to say: the inputs and targets are exactly the same. Here, the “noise” arises from the lossy compression happening in the down projection." Our tutorial clarifies that noise in the sense of a random unpredictable quantity is \textit{not} necessary to produce double descent. Rather, what is necessary is \textit{residual errors from the perspective of the model class}. Those residual errors could be entirely deterministic, such as a nonlinear model attempting to fit a noiseless linear relationship.
\end{enumerate}





