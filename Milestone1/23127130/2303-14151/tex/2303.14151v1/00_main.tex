\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage[mathscr]{euscript}
\usepackage{graphicx}
\usepackage[numbers]{natbib} % has a nice set of citation styles and commands
\usepackage{mathtools} % amsmath with fixes and additions
\usepackage{bbm}
\usepackage{boxedminipage}
\usepackage{alltt}
\usepackage{bm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage{lineno}         % Enables line numbers
\geometry{
% a4paper,
 total={150mm,200mm},
 left=30mm,right = 30mm,
 top=30mm,bottom =30mm,
}
\usepackage{lipsum}
\usepackage{amssymb}

\setlength{\parindent}{0pt}

\DeclareMathOperator{\defeq}{\stackrel{\text{def}}{=}}
\DeclareMathOperator{\Tr}{\text{Tr}}
\newcommand{\rs}[1]{\textcolor{red}{Rylan: #1}}


% \title{What Double Descent Is and Why It Occurs:\\An Intuitive, Interpretable Understanding with Causal Ablations}
% \title{An Approachable Introduction to Double Descent}
% \title{Why Does Double Descent Occur?\\An Intuitive \& Interpretable Explanation with Ablations}
\title{Double Descent Demystified: Identifying, Interpreting \& Ablating the Sources of a Deep Learning Puzzle}
\author[1]{Rylan Schaeffer}
\author[2]{Mikail Khona}
\author[1]{Zachary Robertson}
\author[3]{Akhilan Boopathy}
\author[4]{Kateryna Pistunova}
\author[5]{Jason W. Rocks}
\author[6]{Ila Rani Fiete}
\author[1]{Oluwasanmi Koyejo}


\affil[1]{Computer Science, Stanford University}
\affil[2]{Physics, Massachusetts Institute of Technology}
\affil[3]{EECS, Massachusetts Institute of Technology}
\affil[4]{Physics, Stanford University}
\affil[5]{Physics, Boston University}
\affil[6]{Brain \& Cognitive Sciences, Massachusetts Institute of Technology}

\date{March 2023}

\begin{document}

\maketitle

\begin{abstract}
    Double descent is a surprising phenomenon in machine learning, in which as the number of model parameters grows relative to the number of data, test error drops as models grow ever larger into the highly overparameterized (data undersampled) regime. This drop in test error flies against classical learning theory on overfitting and has arguably underpinned the success of large models in machine learning. This non-monotonic behavior of test loss depends on the number of data, the dimensionality of the data and the number of model parameters.
    Here, we briefly describe double descent, then provide an explanation of why double descent occurs in an informal and approachable manner, requiring only familiarity with linear algebra and introductory probability.
    We provide visual intuition using polynomial regression, then mathematically analyze double descent with ordinary linear regression and identify three interpretable factors that, when simultaneously all present, together create double descent.
    We demonstrate that double descent occurs on real data when using ordinary linear regression, then demonstrate that double descent does not occur when any of the three factors are ablated.
    We use this understanding to shed light on recent observations in nonlinear models concerning superposition and double descent.
    Code is \href{https://github.com/RylanSchaeffer/Stanford-AI-Alignment-Double-Descent-Tutorial}{publicly available}.
\end{abstract}


%XXX suggestion to add before you get to regression on real datasets:
% 1) within linear regression framework, add visualizations showing the teasing apart of the 3 criteria for double descent: i.e., show also numerically that when any of the 3 criteria is removed, DD is eliminated. 
% Rylan 2023-03-08: This is a good idea and I will do this.
% 2) do the same as 1) above but for a teacher-student framework with random teacher network and added noise. Show that 
%1) if all the data generated are directly from a random teacher network, and the student network is initialized as the teacher network was, you should see no double descent. 
% 2) if add some perturbations to the training+test data from the teacher network that act as the noise, so that it's not possible for student network to exactly fit these, then get double descent. 
% ... and so on. 
% I don't think Jason's paper pulls apart all three factors that you do. You would explicitly say that you're re-doing exactly what was in Jason's paper, but teasing it apart into the 3 factors, and also connecting that result to the very simple linear regression analytical framework that you present here, without relying on replica etc. calcs. In other words, even if you didn't show a single new thing in the teacher-student setting relative to Jason's (i.e. not even a teasing apart of the 3 factors necessary for double descent), but simply related his non-linear teacher-student framework results to those you get from the linear framework, it would be useful. The more you connect disparate dots the better. 

\input{01_introduction}
\input{02_polynomial_regression}
\input{03_linear_regression}
\input{04_nonlinear}
\input{05_discussion}

\clearpage

% \bibliographystyle{abbrv}
% \bibliographystyle{authordate1}
\bibliographystyle{plain}
\bibliography{references}

\clearpage

\appendix

\input{Implicit_Bias_GD}


\end{document}
