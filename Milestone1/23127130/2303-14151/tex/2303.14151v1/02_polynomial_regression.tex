\section{Visual Intuition from Polynomial Regression}

% TODO Make interpolation threshold line more obvious.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/double_descent_polynomial_regression_horizontal.pdf}
    \caption{\textbf{Intuition for double descent from polynomial regression.} Top: Polynomial regression displays double descent.
    Bottom: When \textit{underparameterized}, the model is unable to capture finer-grained features in the training data, meaning bias is large but variance is small. As the interpolation threshold is approached, the training data can be fit exactly, meaning bias is small; however, the particular realization of the training data significantly affects the learnt function, meaning variance is large. When \textit{overparameterized}, the model can exactly fit the training data, meaning bias is again small, but the model is also regularized towards a small-norm solution, making variance small.}
    \label{fig:polynomial_regression}
\end{figure}

To offer visual intuition for the cause of double descent, we turn to polynomial regression.
Concretely, suppose we wish to predict $y \in \mathbb{R}$ from $x \in [-1, 1]$, where the true (unknown) relationship is:
%
\begin{equation*}
    y(x) = 2 x + \cos(25 x)
\end{equation*}

In polynomial regression, we take the approach of mapping each datum $x$ to a $P$-dimensional space (corresponding to the $P$ parameters) by using the following ``feature" map $\vec{\phi}_P: \mathbb{R}^1 \rightarrow \mathbb{R}^P$:
%
\begin{equation*}
    \vec{\phi}_P: x \rightarrow \begin{bmatrix}
        \phi_1(x)\\
        \phi_2(x)\\
        \vdots\\
        \phi_P(x)
    \end{bmatrix} \in \mathbb{R}^P
\end{equation*}

where $\phi_i$ denotes some polynomial\footnote{In our simulations, we choose $\phi_i$ to be the $i$-th Legendre polynomial}. We'll then fit a linear model using parameters $\vec{\beta}_P \in \mathbb{R}^P$:
%
\begin{equation*}
    y \approx \vec{\phi}_P(x) \cdot \vec{\beta}_P    
\end{equation*}

We show the results of sweeping the number of parameters $P$ (= the number of polynomials = the number of features) from 1 to 200 (Fig. \ref{fig:polynomial_regression}). 
% Recall that the test mean-squared error can be decomposed into three terms:
% %
% \begin{equation*}
%     \text{Squared Error} = \text{Bias}^2 + \text{Variance} + \text{Noise}
% \end{equation*}
When $P=1$, we can fit a line to our data, which is insufficiently expressive to capture the true relationship between $x$ and $y$; thus, the model has bias, but low variance.
As the number of parameters $P$ increases towards the number of training data $N$, the model can more accurately fit the training data but at the cost of inducing  ``wiggles" that depend on the particular realization of the training data and that incur a high test mean squared error; the bias decreases but the variance diverges.
As the number of parameters $P$ increases beyond the number of training data $N$, the regression remains sufficiently expressive to exactly fit the training data, meaning it has low bias, but the model fitting process prefers solutions with smaller norm that ``wiggle`` less, meaning it has low variance as well. Next, we mathematically analyze under what conditions this double-descent phenomenon occurs. 

