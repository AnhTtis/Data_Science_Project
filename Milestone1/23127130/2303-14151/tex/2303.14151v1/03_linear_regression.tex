\section{Mathematical Intuition from Ordinary Linear Regression}


To offer an intuitive yet quantitative understanding of double descent, we turn to ordinary linear regression. Recall that in linear regression, the number of fit parameters $P$ must equal the dimension $D$ of the covariates; consequently, rather than thinking about changing the number of parameters $P$, we'll instead think about changing the number of data $N$. \textit{Because double descent is fundamentally about the ratio of number of parameters $P$ to number of data $N$}, varying the number of data is as valid an approach as varying the number of parameters is. To understand where and why double descent occurs in linear regression, we'll study how linear regression behaves in the two parameterization regimes. \newline
%We say that a linear model is \textit{overparameterized} (a.k.a. underconstrained) if $N < P = D$ and \textit{underparameterized} (a.k.a. overconstrained) if $N > P = D$, and the \textit{interpolation threshold} refers to where $N=P=D$.

% \subsection{Succinct Mathematical Explanation}
% \label{sec:succinct_math}

% For the time-limited reader, this section contains a $1$-page explanation of why double descent occurs, using ordinary linear regression. A more detailed step-by-step explanation is provided in Sec \ref{sec:detailed_math}.  When performing ordinary linear regression on $N$ data in $D$ dimensions using training features $X \in \mathbb{R}^{N \times D}$ and training regression targets $Y \in \mathbb{R}^{N}$ , we fit the linear parameters $\hat{\vec{\beta}} \in \mathbb{R}^D$ using the (pseudo)inverse of the training data features $X^+ \defeq V \Sigma^+ U^T \in \mathbb{R}^{D \times N}$ and the training regression targets $Y$:
% %
% \begin{equation*}
%     \hat{\vec{\beta}} = X^+ Y = V \Sigma^+ U^T Y
% \end{equation*}

% There are (unknown) optimal linear parameters $\vec{\beta}^* \in \mathbb{R}^D$, that have some residual errors $E \in \mathbb{R}^N$ (possibly $=0$) on the training data:
% %
% \begin{equation*}
%     Y = X \vec{\beta}^* + E
% \end{equation*}

% To be clear, we are not making assumptions; we are introducing notation to express that, from the perspective of the function class, there may be uncapturable ``errors". Combining the two above equations, when given a new test point $\vec{x}_{test}$, our model's prediction $\hat{y}_{test}$ will differ from the ideal prediction $y_{test}^* \defeq \vec{x}_{test} \cdot \vec{\beta}*$ as:
% %
% \begin{equation*}
%     \hat{y}_{test} - y_{test}^* = \vec{x}_{test} \cdot V \Sigma^+ U^T E
% \end{equation*}

% We can rewrite the above equation by splitting the total error into the individual error contributed by each singular mode. Letting $R \defeq rank(X)$:
% %
% \begin{equation*}
%     \hat{y}_{test} - y_{test}^* = \sum_{r=1}^R  \frac{1}{\sigma_r} (\Vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E)
%     \label{eq:key_eqn}
% \end{equation*}

% The discrepancy between the model's prediction and the ideal prediction will be largest when three quantities simultaneously grow extreme:

% \begin{enumerate}
%     \item The \textit{training features} $X$ have at least one mode with a small-but-nonzero singular value i.e. $\sigma_r$ is small, causing $1/\sigma_r$ to explode
%     \item The \textit{training data} has large residual errors $E$ along that mode i.e. $\vec{u}_r \cdot E$ is large
%     \item The \textit{test datum features} $\vec{x}_{test}$ vary significantly along this mode i.e. $\vec{x}_{test} \cdot \vec{v}_r$ is large
% \end{enumerate}

% These three quantities grow extreme as one approaches the interpolation threshold (i.e. where the number of parameters $P$ = the number of data $N$) and less extreme as one becomes overparameterized ($P > N$) or underparameterized ($P < N$). Intuitively, double descent is because as one approaches the interpolation threshold ($P = N$), at least one singular mode in the training features becomes probabilistically likely to have a small amount of non-zero variance, whose relation to the true training regression targets is error-prone, such that when new test data varies greatly along this direction, the model is forced to extrapolate distantly along a mode it doesn't understand well.
% Small-but-non-zero singular values of the training features $X$ become probabilistically more likely as one approaches the interpolation threshold because 

% \subsection{Detailed Mathematical Explanation}
% \label{sec:detailed_math}

% Suppose we are given a supervised dataset of $N$ points for regression:
% %
% \begin{equation*}
%     \mathcal{D} \, \defeq \, \Big\{ (\vec{x}_n, y_n) \Big\}_{n=1}^N
% \end{equation*}

% with covariates $\vec{x}_n \in \mathbb{R}^D$ and targets $y_n \in \mathbb{R}$.
% We'll sometimes use matrix-vector notation to refer to our training data, treating the features $\vec{x}_n$ as row vectors:
% %
% \begin{equation*}
%     X \, \defeq \, \begin{bmatrix} - \vec{x}_1 - \\ 
%     \vdots\\
%     - \vec{x}_N - \end{bmatrix} \in \mathbb{R}^{N \times D}
%     \quad \quad \quad \quad 
%     Y \, \defeq \, \begin{bmatrix} y_1\\  \vdots \\ y_N \end{bmatrix} \in \mathbb{R}^{N \times 1}   
% \end{equation*}


% \subsubsection{Two Key Spectral Properties of the Features $X$} 

% Before we study the different parameterization regimes, it is helpful to define two derived objects from the data matrix $X \in \mathbb{R}^{N \times D}$ and study two key facts about their spectral properties:
% \begin{enumerate}
%     \item The Gram matrix (a.k.a. the outer product matrix) $\defeq XX^T \in \mathbb{R}^{N \times N}$.
%     \item The second moment matrix (sometimes called the scatter matrix or more commonly the correlation matrix, although we needn't center $X$) $\defeq X^TX \in \mathbb{R}^{D \times D}$.
% \end{enumerate}

% \textit{The first key fact for understanding double descent will be that the Gram matrix and the second-moment matrix share all non-zero eigenvalues} (and each might have a few more zero eigenvalues). To see why, consider the singular value decomposition for $X \defeq USV^T$. Let $R \defeq rank(X)$. Then $U \in \mathbb{R}^{N \times N}$ and $V \in \mathbb{R}^{D \times D}$ are orthogonal matrices and $S \in \mathbb{R}^{N \times D} $ is the diagonal matrix of $X$'s $R$ singular values.
% %
% \begin{align}
%     XX^T &= (USV^T)(USV^T)^T = US\underbrace{(V^T V)}_{I_N} SU^T = US^2U^T\\
%     X^TX &= (USV^T)^T(USV^T) = VS \underbrace{(U^TU)}_{I_D} SV^T = VS^2V^T
% \end{align}

% The above expressions show that the Gram matrix and second-moment matrix have the same $R$ non-zero eigenvalues (and one or both might have some additional 0 eigenvalues). \textit{The second key fact is that for any positive semi-definite matrix, the eigenvalues of its inverse are its eigenvalues inverted}:
% %
% \begin{align}
%     (X X^T)^{-1} = (U S^2 U^T)^{-1} = U S^{-2} U^T\\
%     (X^T X)^{-1} = (V S^2 V)^{-1} = V S^{-2} V^T
% \end{align}

% Intuitively, this means that if the Gram matrix $X X^T$ (or equivalently the second moment matrix $X^T X$) has smallest eigenvalue $\lambda_R \geq 0$, its inverse $(X X^T)^{-1}$ (or equivalently, $(X^T X)^{-1}$ will have largest eigenvalues $1/\lambda_R \geq 0$. These eigenvalues will play a key role in double descent.

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/double_descent_dataset=WHO-Life-Expectancy.png}%
    \includegraphics[width=0.49\textwidth]{figures/least_informative_singular_value_dataset=WHO-Life-Expectancy.png}
    \caption{Left: Double descent occurs in ordinary linear regression on the World Health Organization's Life Expectancy dataset. Right: Double descent occurs when three quantities simultaneously grow extreme. One of the three is small-but-nonzero singular values in the training features $X$. The smallest non-zero singular value of $X$ (probabilistically) obtains its lowest value when the number of parameters $P$ equals the number of data $N$ (the \textit{interpolation threshold}), approaching from either the \textit{overparameterized regime} ($P > N$; left) or the \textit{underparameterized regime} ($P < N$; right).}
    \label{fig:WHO_life_expectancy}
\end{figure}

If the regression is \textit{underparameterized}, we estimate the linear relationship between the covariates $\vec{x}_n$ and the target $y_n$ by solving the classical least-squares minimization problem:
%
\begin{equation*}
    \hat{\vec{\beta}}_{under} \, \defeq \,  \arg \min_{\vec{\beta}} \frac{1}{N} \sum_n ||\vec{x}_n \cdot \vec{\beta} - y_n||_2^2 \, = \, \arg \min_{\vec{\beta}} ||X \vec{\beta} - Y ||_F^2
\end{equation*}

The solution to this underparameterized optimization problem is the well-known ordinary least squares estimator that uses the second moment matrix $X^T X$:
%
\begin{equation*}
    \hat{\vec{\beta}}_{under} = (X^T X)^{-1} X^T Y
\end{equation*}

If the model is \textit{overparameterized}, the above optimization problem is ill-posed since there are infinitely many solutions; this is because we have fewer constraints than parameters. Consequently, we need to choose a different (constrained) optimization problem:
%
\begin{equation*}
    \hat{\vec{\beta}}_{over} \, \defeq \, \arg \min_{\vec{\beta}} ||\vec{\beta}||_2^2 \quad \quad \text{s.t.} \quad \quad \forall \, n \in \{1, ..., N\} \quad \vec{x}_n \cdot \vec{\beta} = y_n
\end{equation*}

This constrained optimization problem asks for the smallest parameters $\vec{\beta}$ that guarantee $\vec{x}_n \cdot \vec{\beta} = y_n$ for all training data. One reason why we choose this optimization problem is that it is the optimization problem that gradient descent implicitly minimizes (App. \ref{app:why_sgd_regularizes}). The solution to this optimization problem is less well-known and instead uses the so-called \href{https://en.wikipedia.org/wiki/Gram_matrix}{Gram matrix} $X X^T \in \mathbb{R}^{N \times N}$:
%
\begin{equation*}
    \hat{\vec{\beta}}_{over} = X^T (X X^T)^{-1} Y
\end{equation*}

One way to see why the Gram matrix appears is via constrained optimization. Define the Lagrangian with Lagrange multipliers $\vec{\lambda} \in \mathbb{R}^N$:
%
\begin{equation*}
    \mathcal{L}(\vec{\beta}, \vec{\lambda}) \, \defeq \, ||\vec{\beta}||_2^2 + \vec{\lambda}^T (Y - X \vec{\beta})
\end{equation*}

Differentiating with respect to both the parameters and the Lagrange multipliers yields:
%
\begin{align*}
    \nabla_{\vec{\beta}}\,  \mathcal{L}(\vec{\beta}, \vec{\lambda}) = \vec{0} = 2\hat{\vec{\beta}} - X^T \vec{\lambda} &\Rightarrow \hat{\vec{\beta}}_{over} = \frac{1}{2} X^T \vec{\lambda}\\
    \nabla_{\vec{\lambda}} \,\mathcal{L}(\beta, \lambda) = \vec{0} = Y - X \hat{\vec{\beta}}_{over} &\Rightarrow Y = \frac{1}{2} X X^T \vec{\lambda}\\
    &\Rightarrow \vec{\lambda} = 2 (X X^T)^{-1} Y\\
    &\Rightarrow \hat{\vec{\beta}}_{over} = X^T (X X^T)^{-1} Y
\end{align*}

Here, we are able to invert the Gram matrix because it is full rank in the overparametrized regime.
After fitting its parameters, the model will make the following predictions for given test point $\vec{x}_{test}$:
%
\begin{align*}
    \hat{y}_{test, under} &= \vec{x}_{test} \cdot \hat{\Vec{\beta}}_{under} = \vec{x}_{test} \cdot (X^T X)^{-1} X^T Y\\
    \hat{y}_{test, over} &= \vec{x}_{test} \cdot \hat{\Vec{\beta}}_{over} 
    = \vec{x}_{test} \cdot X^T (X X^T)^{-1} Y
\end{align*}

\textit{Hidden in the above equations is an interaction between three quantities that can, when all grow extreme, create double descent.} To reveal the three quantities, we'll rewrite the regression targets by introducing a slightly more detailed notation. Unknown to us, there are some ideal linear parameters $\vec{\beta}^* \in \mathbb{R}^P = \mathbb{R}^D$ that truly minimize the test mean squared error. We can write any regression target as the inner product of the data $\vec{x}_n$ and the ideal parameters $\beta^*$, plus an additional error term $e_n$ that is an ``uncapturable" residual from the ``perspective" of the model class:
%
\begin{equation*}
    y_n = \vec{x}_n \cdot \vec{\beta}^* + e_n
\end{equation*}

In matrix-vector form, we will equivalently write:
%
\begin{equation*}
    Y = X \vec{\beta}^* + E
\end{equation*}

with $E \in \mathbb{R}^{N \times 1}$. To be clear, we are \textit{not} imposing assumptions on the model or data. Rather, we are introducing notation to express that there are (unknown) ideal linear parameters, and possibly residuals that even the ideal model might be unable to capture; these residuals could be random noise or could be fully deterministic patterns that this particular model class cannot capture. Using this new notation, we rewrite the model's predictions to show how the test datum's features $\vec{x}_{test}$, training data's features $X$ and training data's regression targets $Y$ interact. In the underparameterized regime:
%
\begin{align*}
    \hat{y}_{test,under} &= \Vec{x}_{test} \cdot (X^T X)^{-1} X^T Y\\
    &= \Vec{x}_{test} \cdot (X^T X)^{-1} X^T (X \beta^* + E)\\
    &= \Vec{x}_{test} \cdot (X^T X)^{-1} X^T X \beta^* + \vec{x}_{test} \cdot (X^T X)^{-1} X^T E\\
    &= \underbrace{\Vec{x}_{test} \cdot \beta^*}_{\defeq y_{test}^*} + \, \vec{x}_{test} \cdot (X^T X)^{-1} X^T E\\
    \hat{y}_{test,under} - y_{test}^* &= \vec{x}_{test} \cdot (X^T X)^{-1} X^T E
    % \hat{y}_{test,over} &= \underbrace{\vec{x}_{test} \cdot \vec{\beta}^*}_{\defeq y_{test}^*} \quad + \quad \Vec{x}_{test} \cdot \underbrace{X^T (X X^T)^{-1}}_{\defeq X^+} E
\end{align*}

This equation is important, but opaque. To extract the intuition, we will replace $X$ with its \href{https://en.wikipedia.org/wiki/Singular_value_decomposition}{singular value decomposition}\footnote{For those unfamiliar with the SVD, any real-valued $X$ can be decomposed into the product of three matrices $X = U \Sigma V^T$ where $U$ and $V$ are both orthonormal matrices and $\Sigma$ is diagonal; intuitively, any linear transformation can be viewed as composition of first a rotoreflection, then a scaling, then another rotoreflection. Because $\Sigma$ is diagonal and because $U, V$ are orthonormal matrices, we can equivalently write $X = U \Sigma V^T$ in vector-summation notation as a sum of rank-1 outer products $X = \sum_{r=1}^{rank(X)} \sigma_r u_r v_r^T$. Each term in the sum is referred to as a ``singular mode", akin to eigenmodes.} $X = U \Sigma V^T$ to reveal how different quantities interact. Let $R \, \defeq \, rank(X)$ and let $\sigma_1 > \sigma_2 > ... > \sigma_R > 0$ be $X$'s (non-zero) singular values. Recalling $E \in \mathbb{R}^{N \times 1}$, we can decompose the (underparameterized) prediction error $\hat{y}_{test, under} - y_{test}^*$ along the orthogonal singular modes:
%
\begin{align*}
    \hat{y}_{test, under} - y_{test}^* &= \Vec{x}_{test} \cdot V \Sigma^{+} U^T E = \sum_{r=1}^R  \frac{1}{\sigma_r} (\Vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E)
\end{align*}

In the overparameterized regime, our calculations change slightly:
%
\begin{align*}
    \hat{y}_{test,over} &= \Vec{x}_{test} \cdot X^T (X X^T)^{-1}  Y\\
    &= \vec{x}_{test} \cdot X^T (X X^T)^{-1} (X \beta^* + E)\\
    &= \vec{x}_{test} \cdot X^T (X X^T)^{-1} X \beta^* + \vec{x}_{test} \cdot X^T (X X^T)^{-1} E\\
    \hat{y}_{test,over} - \underbrace{\vec{x}_{test} \cdot \beta^*}_{\defeq y_{test}^*} &= \vec{x}_{test} \cdot X^T (X X^T)^{-1} X \beta^*  - \vec{x}_{test} \cdot I_{D} \beta^* + \vec{x}_{test} \cdot (X^T X)^{-1} X^T E\\
    \hat{y}_{test,over} - y_{test}^* &= \vec{x}_{test} \cdot (X^T (X X^T)^{-1} X - I_D) \beta^*  + \vec{x}_{test} \cdot (X^T X)^{-1} X^T E
\end{align*}

If we again replace $X$ with its SVD $U S V^T$, we can again simplify $\vec{x}_{test} \cdot (X^T X)^{-1} X^T E$. This yields our final equations for the prediction errors.
%
\begin{align*}
\hat{y}_{test,over} - y_{test}^* &= \vec{x}_{test} \cdot (X^T (X X^T)^{-1} X - I_D) \beta^* \quad \quad \quad \quad + && \sum_{r=1}^R  \frac{1}{\sigma_r} (\Vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E)\\
    \hat{y}_{test,under} - y_{test}^* &= &&\sum_{r=1}^R  \frac{1}{\sigma_r} (\Vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E)
\end{align*}

What is the discrepancy between the underparameterized prediction error and the overparameterized prediction error, and from where does the discrepancy originate? The overparameterized prediction error $\hat{y}_{test,over} - y_{test}^*$ has the extra term $\vec{x}_{test} \cdot (X^T (X X^T)^{-1} X - I_D) \beta^*$. To understand where this term originates, recall that our goal is to understand how fluctuations in the features $\vec{x}$ correlate with fluctuations in the targets $y$. In the overparameterized regime, there are more parameters than there are data. Consequently, for $N$ data points in $D=P$ dimensions, the model can ``see" fluctuations in at most $N$ dimensions, but has no ``visibility" into fluctuations in the remaining $P-N$ dimensions. This causes information about the optimal linear relationship $\vec{\beta}^*$ to be lost, which in turn increases the overparameterized prediction error $\hat{y}_{test, over} - y_{test}^*$. Statisticians call this term $\vec{x}_{test} \cdot (X^T (X X^T)^{-1} X - I_D) \beta^*$ the ``bias". The other term (the ``variance") is what causes double descent:
%
\begin{equation}
    \sum_{r=1}^R  \frac{1}{\sigma_r} (\Vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E)
    \label{eqn:variance}
\end{equation}

\textit{Eqn. \ref{eqn:variance} is critical}. It reveals that our test prediction error (and thus, our test squared error!) will depend on an interaction between 3 quantities:
%
\begin{enumerate}
    \item How much the \textit{training features} $X$ vary in each direction; more formally, the inverse (non-zero) singular values of the \textit{training features} $X$:
    %
    $$\frac{1}{\sigma_r}$$
    
    \item How much, and in which directions, the \textit{test features} $\vec{x}_{test}$ vary relative to the \textit{training features} $X$; more formally: how $\vec{x}_{test}$ projects onto $X$'s right singular vectors $V$:
    %
    $$\Vec{x}_{test} \cdot \Vec{v}_r$$
    
    \item How well the \textit{best possible model} can correlate the variance in the \textit{training features} $X$ with the \textit{training regression targets} $Y$; more formally: how the residuals $E$ of the best possible model in the model class (i.e. insurmountable ``errors" from the ``perspective" of the model class) project onto $X$'s left singular vectors $U$:
    %
    $$\Vec{u}_r \cdot E$$
    
\end{enumerate}

Here, we use the terminology ``vary" and ``variance", suggesting a connection to \href{https://en.wikipedia.org/wiki/Covariance_matrix}{the statistical notion of variance}. There is indeed one\footnote{If we had centered the training features $X$ to form $\bar{X}$, with corresponding SVD $\bar{X} = \bar{U} \bar{\Sigma} \bar{V}^T$, then $\bar{X}^T \bar{X}$ would be the empirical covariance matrix, and its eigendecomposition would be:
%
\begin{equation*}
    \bar{X}^T \bar{X} = \bar{V} \bar{\Sigma}^2 \bar{V}^T   
\end{equation*}

Each right singular vector of $\bar{X}$ would be an orthogonal axis of variation, with the variance along each direction given by the squared singular values of $\bar{X}$. However, because we don't center the features, $X^T X$ is the empirical second moment matrix, not the empirical covariance matrix. Thus, when we refer to ``vary" and ``variance", we are slightly abusing terminology, but the intuition - how much the features are wiggling, and whether the wiggling is correlated with the regression targets - is the right way to understand the concepts. We could center our data features to refer to the variance, but we felt doing so might be pedagogically confusing since centering is not related to double descent.}, although we slightly abuse terminology! These three quantities, multiplied, determine how much the $r$-th singular mode contributes to the prediction error. \newline

Double descent occurs when these three quantities grow extreme: (i) the \textit{training features} contain small-but-nonzero variance in some singular direction(s), (ii) from the ``perspective" of the model class, residual errors in the \textit{training features and targets} have large projection along this singular mode, and (iii) the \textit{test features} vary significantly along this singular mode. When (i) and (ii) co-occur, this means the model's parameters along this mode are likely incorrect. Then, when (iii) is added to the mix by a test datum $\vec{x}_{test}$ with a large projection along this mode, the model is forced to extrapolate significantly beyond what it saw in the training data, in a direction where the training data had an error-prone relationship between its training predictions and the training regression targets, using parameters that are likely wrong. As a consequence, the test squared error explodes! \newline

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.55\textwidth]{figures/doubledescentfigure.pdf}%
%     \includegraphics[width=0.44\textwidth]{figures/random_data_eigenvalue_distribution.png}
%     \caption{Left: Overparameterized and underparameterized linear regression respectively use the Gram matrix $X X^T$ and the second moment matrix $X^T X$, respectively. When $ X \in \mathbb{R}^{N \times D}$ is non-square, computing either matrix averages over many entries; the fewest entries are averaged over when $X$ is square, at the interpolation threshold. Right: As a consequence, the smallest non-zero eigenvalue becomes smallest as one approaches the the interpolation threshold, harming the test error; the smallest non-zero eigenvalue is likely to be larger as the model becomes more overparameterized or more underparameterized. For more information, see the \href{https://en.wikipedia.org/wiki/Marchenko\%E2\%80\%93Pastur\_distribution}{Marchenko–Pastur distribution}.}
%     \label{fig:marchenko_pastur}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.32\textwidth]{figures/data_distribution.png}%
    \includegraphics[width=0.32\textwidth]{figures/data_distribution_num_data=1.png}%
    \includegraphics[width=0.32\textwidth]{figures/data_distribution_num_data=2.png}
    \includegraphics[width=0.32\textwidth]{figures/data_distribution_num_data=3.png}%
    \includegraphics[width=0.32\textwidth]{figures/data_distribution_num_data=8.png}%
    \includegraphics[width=0.32\textwidth]{figures/data_distribution_num_data=100.png}%
    \caption{\textbf{Geometric intuition for why the smallest non-zero singular value reaches its lowest value when approaching the interpolation threshold.} As one nears the interpolation threshold (here, $D=P=N=3$), $N$ training data are unlikely to vary substantially in $P$ orthogonal directions, meaning at least one orthogonal direction is likely to have small variance. If fewer data are observed, then the trailing directions have exactly zero variance, and if more data are observed, then the additional data reveal variance along these trailing directions.}
    \label{fig:geometric_viewpoint}
\end{figure}


Why does this explosion happen near the interpolation threshold? The answer is that the first factor becomes more likely to occur when approaching the interpolation threshold from either parameterization regime. The reason why the smallest non-zero singular value is (probabilistically) likely to reach its lowest value at the interpolation threshold is a probabilistic one, based on the \href{https://en.wikipedia.org/wiki/Marchenko\%E2\%80\%93Pastur\_distribution}{Marchenko–Pastur distribution} from random matrix theory. Because the Marchenko–Pastur distribution is rather technical, we instead focus on gaining intuition by thinking about how much variance we've seen along each orthogonal direction in the data feature space (Fig. \ref{fig:geometric_viewpoint}).\newline



\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/double_descent_dataset=California-Housing.png}%
    \includegraphics[width=0.49\textwidth]{figures/least_informative_singular_value_dataset=California-Housing.png}
    \caption{Double Descent in Ordinary Linear Regression on California Housing Dataset.}
    \label{fig:California_housing}
\end{figure}

Suppose we're given a single training datum $\vec{x}_1$. So long as this datum isn't exactly zero, that datum varies in a single direction, meaning we gain information about the data distribution's variance in that direction. Of course, the variance in all orthogonal directions is exactly 0, which the linear regression fit will ignore. Now, suppose we're given a second training datum $\vec{x}_2$. Again, so long as this datum isn't exactly zero, that datum varies, but now, some fraction of $\vec{x}_2$ might have a positive projection along $\vec{x}_1$; if this happens (and it likely will, since the two vectors are unlikely to be exactly orthogonal), the shared direction of the two vectors gives us \textit{more} information about the variance in this shared direction, but gives us \textit{less} information about the second orthogonal direction of variation. This means that the training data's smallest non-zero singular value after 2 samples is probabilistically smaller than after 1 sample. As we gain more training data, thereby approaching the interpolation threshold, the probability that each additional datum has large variance in a new direction orthogonal to all previously seen directions grows increasingly unlikely. At the interpolation threshold, where $N = P = D$, in order for the $N$-th datum to avoid adding a small-but-nonzero singular value to the training data, two properties must hold: (1) there must be one dimension that none of the preceding $N-1$ training data varied in, and (2) the $N$-th datum needs to vary significantly in this single dimension. That's pretty unlikely! As we move beyond the interpolation threshold, the variance in each covariate dimension becomes increasingly clear, and the smallest non-zero singular values moves away from 0. This is displayed visually in Fig. \ref{fig:geometric_viewpoint}.


% To summarize in a single sentence: \textit{Double descent can occur as one approaches the interpolation threshold ($P = N$) because at least one singular mode in the training features becomes probabilistically likely to have a small amount of non-zero variance, whose relation to the true training regression targets is error-prone, such that when new test data varies greatly along this direction, the model is forced to extrapolate distantly along a mode it doesn't understand well.}




% \subsubsection{Why does test error (sometimes) diverge at the interpolation threshold?}


\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/double_descent_dataset=Diabetes.png}
    \includegraphics[width=0.49\textwidth]{figures/least_informative_singular_value_dataset=Diabetes.png}
    \caption{Double Descent in Ordinary Linear Regression on Diabetes Dataset.}
    \label{fig:Diabetes}
\end{figure}



\section{Empirical Evidence on Real Data with Linear Regression}

Does our claim that ordinary linear regression can exhibit double descent hold empirically? We show that it indeed does, using three datasets: WHO Life Expectancy, California Housing, Diabetes. These three datasets were randomly selected on the basis of being easily accessible, e.g., through sklearn \cite{scikit-learn}. All display a sharp spike in test mean squared error at the interpolation threshold (Left panels of Figs. \ref{fig:WHO_life_expectancy}, \ref{fig:California_housing}, \ref{fig:Diabetes}). We additionally substantiate our previous claim that the smallest non-zero singular value of the training features $X$ obtains its lowest value as one nears the interpolation threshold (Right panels of Figs. \ref{fig:WHO_life_expectancy}, \ref{fig:California_housing}, \ref{fig:Diabetes}).
Code is \href{https://github.com/RylanSchaeffer/Stanford-AI-Alignment-Double-Descent-Tutorial}{publicly available}. 


\section{When Does Double Descent Not Occur?}
\label{sec:double_descent_ablation}


Double descent will not occur if any of the three factors are absent. What could cause that?

\begin{itemize}
    \item \textit{Small-but-nonzero singular values do not appear in the training data features}. One way to accomplish this is by switching from ordinary linear regression to ridge regression, which effectively adds a gap separating the smallest non-zero singular value from $0$.
    \item \textit{The test datum does not vary in different directions than the training features}. If the test datum lies entirely in the subspace of just a few of the leading singular directions, then double descent is unlikely to occur.
    \item \textit{The best possible model in the model class makes no errors on the training data.} For instance, suppose we use a linear model class on data where the true relationship is a noiseless linear one. Then, at the interpolation threshold, we will have $D=P$ data, $P=D$ parameters, our line of best fit will exactly match the true relationship, and no double descent will occur.
\end{itemize}


To confirm our understanding, we causally test the predictions of when double descent will not occur by ablating each of the three factors individually. Specifically, we do the following:

\begin{enumerate}
    \item No Small Singular Values in Training Features: As we run the ordinary linear regression fitting process, as we sweep the number of training data, we also sweep different singular value cutoffs and remove all singular values of the training features $X$ below the cutoff.
    \item Test Features Lie in the Training Features Subspace: As we run the ordinary linear regression fitting process, as we sweep the number of training data, we project the test features $\vec{x}_{test}$ onto the subspace spanned by the training features $X$ singular modes.
    \item No Residual Errors in the Optimal Model: We first use the entire dataset to fit a linear model $\vec{\beta}^*$, then replace $Y$ with $X \vec{\beta}^*$ and $y_{test}^*$ with $\vec{x}_{test} \cdot \vec{\beta}^*$ to ensure the true relationship is linear. We then rerun our typical fitting process, sweeping the number of training data.
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/double_descent_ablations_dataset=Student-Teacher.png}
    \includegraphics[width=0.99\textwidth]{figures/double_descent_ablations_dataset=California-Housing.png}
    \includegraphics[width=0.99\textwidth]{figures/double_descent_ablations_dataset=Diabetes.png}
    \includegraphics[width=0.99\textwidth]{figures/double_descent_ablations_dataset=WHO-Life-Expectancy.png}
    \caption{\textbf{Double descent will not occur if any of the three critical quantities are absent.} We demonstrate this via ablations (Sec. \ref{sec:double_descent_ablation}). Left to Right: Double descent appears in ordinary linear regression. Removing small singular values in the training features $X$ prevents double descent. Preventing the test features $\vec{x}_{test}$ from varying in the trailing singular modes of the training features $X$ prevents double descent. Ensuring that the optimal model in the model class has zero residual prediction errors $E$ prevents double descent. Top to Bottom: Synthetic data generated in a Student-Teacher framework, California Housing dataset, Diabetes dataset, WHO Life Expectancy dataset}
    \label{fig:double_descent_ablation}
\end{figure}

We first conduct experiments on a synthetic dataset in a student-teacher setup, and find that causally ablating each of the three factors prevents double descent from occurring (Fig. \ref{fig:double_descent_ablation}, top row). Next, we apply the same ablations to real world datasets (California Housing, Diabetes, WHO Life Expectancy) and find in all three that removing any of the three factors prevents double descent (Fig. \ref{fig:double_descent_ablation}, rows 2-5).