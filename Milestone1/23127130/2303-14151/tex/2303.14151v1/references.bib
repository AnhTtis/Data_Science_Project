@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2021},
  number={12},
  pages={124003},
  year={2021},
  publisher={IOP Publishing}
}

@article{advani2020high,
  title={High-dimensional dynamics of generalization error in neural networks},
  author={Advani, Madhu S and Saxe, Andrew M and Sompolinsky, Haim},
  journal={Neural Networks},
  volume={132},
  pages={428--446},
  year={2020},
  publisher={Elsevier}
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@article{spigler2018jamming,
  title={A jamming transition from under-to over-parametrization affects loss landscape and generalization},
  author={Spigler, Stefano and Geiger, Mario and d'Ascoli, St{\'e}phane and Sagun, Levent and Biroli, Giulio and Wyart, Matthieu},
  journal={arXiv preprint arXiv:1810.09665},
  year={2018}
}

@article{opper1995statistical,
  title={Statistical mechanics of learning: Generalization},
  author={Opper, Manfred},
  journal={The handbook of brain theory and neural networks},
  pages={922--925},
  year={1995},
  publisher={MIT Pres) (Cambridge, MA}
}


@article{rocks2022memorizing,
  title={Memorizing without overfitting: Bias, variance, and interpolation in overparameterized models},
  author={Rocks, Jason W and Mehta, Pankaj},
  journal={Physical Review Research},
  volume={4},
  number={1},
  pages={013201},
  year={2022},
  publisher={APS}
}

@article{rocks2022bias,
  title={Bias-variance decomposition of overparameterized regression with random linear features},
  author={Rocks, Jason W and Mehta, Pankaj},
  journal={Physical Review E},
  volume={106},
  number={2},
  pages={025304},
  year={2022},
  publisher={APS}
}

@article{rocks2021geometry,
  title={The Geometry of Over-parameterized Regression and Adversarial Perturbations},
  author={Rocks, Jason W and Mehta, Pankaj},
  journal={arXiv preprint arXiv:2103.14108},
  year={2021}
}

@article{mei2022generalization,
  title={The generalization error of random features regression: Precise asymptotics and the double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={4},
  pages={667--766},
  year={2022},
  publisher={Wiley Online Library}
}

@article{adlam2020understanding,
  title={Understanding double descent requires a fine-grained bias-variance decomposition},
  author={Adlam, Ben and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={11022--11032},
  year={2020}
}


@article{poggio2019double,
  title={Double descent in the condition number},
  author={Poggio, Tomaso and Kur, Gil and Banburski, Andrzej},
  journal={arXiv preprint arXiv:1912.06190},
  year={2019}
}

@article{henighan2023superposition,
  title={Double descent in the condition number},
  author={Henighan, Tom and Carter, Shan and Hume, Tristan and Elhage, Nelson and Lasenby, Robert and Fort, Stanislav and Schiefer, Nicholas and Olah, Christopher},
  journal={Transformer Circuits Thread},
  url={https://transformer-circuits.pub/2023/toy-double-descent/index.html},
  year={2023}
}

@article{hastie2022surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={The Annals of Statistics},
  volume={50},
  number={2},
  pages={949--986},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}

@inproceedings{bordelon2020spectrum,
  title={Spectrum dependent learning curves in kernel regression and wide neural networks},
  author={Bordelon, Blake and Canatar, Abdulkadir and Pehlevan, Cengiz},
  booktitle={International Conference on Machine Learning},
  pages={1024--1034},
  year={2020},
  organization={PMLR}
}


@article{chen2021multiple,
  title={Multiple descent: Design your own generalization curve},
  author={Chen, Lin and Min, Yifei and Belkin, Mikhail and Karbasi, Amin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8898--8912},
  year={2021}
}

@article{nakkiran2020optimal,
  title={Optimal regularization can mitigate double descent},
  author={Nakkiran, Preetum and Venkat, Prayaag and Kakade, Sham and Ma, Tengyu},
  journal={arXiv preprint arXiv:2003.01897},
  year={2020}
}
