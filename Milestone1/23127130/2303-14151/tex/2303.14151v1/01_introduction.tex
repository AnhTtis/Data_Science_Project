\section{What is double descent?}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/rocks_2022_memorizing.png}
    \caption{Double descent: test error falls, rises, then falls as a ratio of parameters to data. Fig. 3A from \cite{rocks2022memorizing}.}
    \label{fig:rocks2022memorizing}
\end{figure}

Double descent is a phenomenon in machine learning describing the key observation that many classes of models can, under relatively broad conditions, exhibit seemingly perplexing changes in test loss as a function of three parameters: the number of data, the dimensionality of the data and the number of parameters in the model.
For instance, as the number of model parameters increases, the test loss can fall, then rise, then fall again (Fig. \ref{fig:rocks2022memorizing}).
What causes such behavior? \newline

Double descent has a rich history, both empirically and analytically; for a non-exhaustive list, see \citep{opper1995statistical, advani2020high, spigler2018jamming, nakkiran2021deep, poggio2019double, advani2020high, adlam2020understanding, mei2022generalization, hastie2022surprises}.
The term ``double descent" was coined by \cite{belkin2019reconciling}, and some of our favorite papers on the topic include \cite{rocks2021geometry, rocks2022bias, rocks2022memorizing}.
One important note is that not every dataset and model pair exhibits \textit{two} descents; under different settings, there can be one, three or more descents \cite{nakkiran2020optimal,chen2021multiple}.
% the reason why is well understood but beyond our scope.

Our goal in this tutorial is to explain why double descent occurs in an approachable manner, without resorting to advanced tools oftentimes employed to analyze double descent such as random matrix theory or statistical physics.
To accomplish this, we provide (1) visual intuition via polynomial regression, (2) mathematical analysis using ordinary linear regression, (3) empirical evidence from ordinary linear regression on real (tabular) data and (4) novel insights for nonlinear neural networks.
To the best of our knowledge, we are the first to take this approach.
Although we focus on regression tasks, our insights hold more generally.


\section{Notation and Terminology}

Consider a supervised dataset of $N$ training data for regression:
%
\begin{equation*}
    \mathcal{D} \, \defeq \, \Big\{ (\vec{x}_n, y_n) \Big\}_{n=1}^N
\end{equation*}

with covariates $\vec{x}_n \in \mathbb{R}^D$ and targets $y_n \in \mathbb{R}$.
We'll sometimes use matrix-vector notation to refer to our training data, treating the features $\vec{x}_n$ as row vectors:
%
\begin{equation*}
    X \, \defeq \, \begin{bmatrix} - \vec{x}_1 - \\ 
    \vdots\\
    - \vec{x}_N - \end{bmatrix} \in \mathbb{R}^{N \times D}
    \quad \quad \quad \quad 
    Y \, \defeq \, \begin{bmatrix} y_1\\  \vdots \\ y_N \end{bmatrix} \in \mathbb{R}^{N \times 1}   
\end{equation*}

In general, our goal is to use our training dataset $\mathcal{D}$ find a function $f: \mathcal{X} \rightarrow \mathcal{Y}$ that makes:
%
\begin{equation*}
    f(x) \approx y   
\end{equation*}

In the setting of ordinary linear regression, we assume that $f$ is a linear function i.e. $f(\vec{x}) = \vec{x} \cdot \vec{\beta}$, meaning our goal is to find (estimate) linear parameters $\hat{\vec{\beta}} \in \mathbb{R}^{D}$ that make:
%
\begin{equation*}
    \vec{x} \cdot \vec{\beta} \approx y
\end{equation*}

Of course, our real goal is to hopefully find a function that generalizes well to new data. As a matter of terminology, there are typically three key parameters:

\begin{enumerate}
    \item The number of model parameters $P$
    \item The number of training data $N$
    \item The dimensionality of the data $D$
\end{enumerate}


We say that a model is \textit{overparameterized} (a.k.a. underconstrained) if $N < P$ and \textit{underparameterized} (a.k.a. overconstrained) if $N > P$.
The \textit{interpolation threshold} refers to where $N=P$, because when $P\geq N$, the model can perfectly interpolate the training points.