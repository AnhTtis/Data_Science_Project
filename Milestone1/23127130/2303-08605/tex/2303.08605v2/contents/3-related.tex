\section{Related Work}
\label{related}

\begin{figure*}[htbp]
\begin{center}
\includegraphics[width=0.95\linewidth]{contents/figs/fig3_new.pdf}
\end{center}
% \vspace{-0.5em}
   \caption{\textbf{Overview.} In this work, we propose two different regularizations. We first regularize the geometry smoothness of the unobserved background regions in a sampled patch. Then, we exploit the background surface as the prior to constrain the objects' surface. In detail, a per-point SDF loss and a reversed depth loss are introduced to regularize the manifold of objects' SDF functions. Combined with other reconstruction losses, our method reaches a neat and disentangled compositional reconstruction in indoor scenes.}
   % \vspace{-1em}
\label{fig:method}
\end{figure*}

\boldparagraph{Neural Implicit Representation for Reconstruction}
\label{related-inr-recon}
Recently,
neural implicit representations have emerged as a popular representation for 3D reconstruction.
While early approaches~\cite{mescheder2019occupancy,park2019deepsdf} rely on 3D supervision, a few works~\cite{niemeyer2020differentiable,yariv2020multiview} exploit surface rendering to use multi-view image supervision only, but also suffer from the unstable training. Neural radiance field~(NeRF)~\cite{mildenhall2021nerf} adopts volume rendering technique to achieve photorealistic scene representation and stable optimization. However, NeRF's formulation can not guarantee accurate underlying geometry. 
Therefore, \cite{oechsle2021unisurf,wang2021neus,yariv2021volume} combine the geometry representation with iso-surface ~(\eg occupancy~\cite{mescheder2019occupancy}, SDF~\cite{wang2021neus,yariv2021volume}) and volume density to accurately reconstruct object-level scenes from RGB images.
\cite{guo2022neural} further applies the planar regularization for scene-level reconstruction. To tackle the problem in texture-less regions, \cite{wang2022neuris,yu2022monosdf} utilize results from pretrained normal and depth estimation networks to guide the SDF training and boost the reconstruction performance.

However, despite the promising reconstruction performance, the aforementioned methods all consider the whole scene as an entirety. Our method focuses on decomposing the scene reconstruction into the background and different foreground objects, which can be regarded as compositional scene reconstruction.

\boldparagraph{Compositional Scene Reconstruction}
\label{related-composite-recon}
Decomposing a scene into its different components could benefit downstream applications like scene editing. Many works have been proposed to recover the scene in a compositional manner from different perspectives. \cite{nie2020total3dunderstanding,zhang2021holistic,irshad2022shapo,nie2022learning} detect and reconstruct different objects in the given monocular image, and predict the scene's layout at the same time. But most of these methods require large-scale datasets with 3D ground truth for training. \cite{kobayashi2022decomposing,tschernezki2022neural,mazur2022feature} optimize a feature field from a large pretrained model~\cite{caron2021emerging,li2022language}, which enables deep feature based decomposition and manipulation.
\cite{wu2022d,sharma2022seeing} exploit the self-supervise paradigm to decompose the scenes into static and dynamic parts. 
More works~\cite{guo2020object,zhi2021place,yang2021learning,yang2022neural,wang2022dm,fu2022panoptic,kundu2022panoptic,wallingford2023neural} focus on recovering the object-compositional scenes given semantic masks with images. However, this line of works concerns the rendering outputs rather than the geometry, \ie the reconstruction results are sub-optimal.

Recently, ObjSDF~\cite{wu2022object} proposes a transform function between semantic logits and SDFs of different objects, which enables optimizing SDF fields with image and semantic mask supervision, and decomposing the whole scene with accurate reconstruction. However, in the indoor scenes where many objects are in partial observation, its reconstruction results are far from satisfactory and can not be used for further applications~(see Fig.~\ref{fig:intro-objsdf-edit}). On the contrary, our method introduces geometry prior to unobserved regions, yielding better compositional scene reconstruction.

\boldparagraph{Prior Regularization in Neural Implicit Reconstruction}
\label{related-prior}
In addition to the commonly used RGB image supervision, many different priors have been proposed to benefit the neural implicit representation. 
For example, \cite{deng2022depth,roessle2022dense} use explicit point cloud as the depth prior, \cite{niemeyer2022regnerf} employs regularization in unseen views and \cite{jain2021putting} introduces pretrained model's feature consistency.
\cite{jiang2022neuman,pavlakos2022one} adopt explicit human model as the structural prior for the human-centric scenes. 
As for surface reconstruction, \cite{guo2022neural} proposes a manhattan-world assumption for the planar regions, \cite{yu2022monosdf} utilizes the normal and depth prediction from off-the-shelf model~\cite{eftekhar2021omnidata} as prior 
to regularize the texture-less regions.

Our method exploits the geometry prior in the unobserved region for compositional scene reconstruction. The proposed regularization can effectively reconstruct the objects and disentangle them from the background, even if the object itself is partially observed.
