\section{Experiments}
\label{experiments}
 
We first conduct ablation study on each proposed component. 
Then we provide quantitative and qualitative results of object-compositional reconstruction on real-world and synthetic scenes.
Finally, we show some possible object manipulation with our compositional reconstruction.

% \boldparagraph{Datasets~\&~Metrics}
% We consider two types of indoor datasets with multi-view RGB images and masks: 1)~ScanNet~\cite{dai2017scannet}, a real-world dataset widely used in previous works~\cite{guo2022neural,wang2022neuris,wu2022object,yu2022monosdf}; 2)~a hand-crafted synthetic dataset with five scenes, each containing $5\sim 10$ objects. The synthetic dataset is considered such that the ground truth geometry of both, occluded and non-occluded regions are available.
\boldparagraph{Datasets}
We consider three types of indoor datasets with multi-view RGB images and masks: 
1)~ScanNet~\cite{dai2017scannet}, a real-world dataset widely used in previous works~\cite{guo2022neural,wang2022neuris,wu2022object,yu2022monosdf}; 
2)~ToyDesk dataset, a real-world dataset with two scenes and each has four objects placed on the table plane. This dataset has also been widely used in previous compositional works~\cite{yang2021learning,wu2022object}
3)~a hand-crafted synthetic dataset with five scenes, each containing $5\sim 10$ objects. The synthetic dataset is considered such that the ground truth geometry of both, occluded and non-occluded regions are available.

\boldparagraph{Metrics}
For reconstruction performance, we report Chamfer Distance, F-score for evaluation on ScanNet, following \cite{guo2022neural,yu2022monosdf}. On synthetic scenes we divide metrics for two aspects: objects and background. For objects we report the reconstruction performance compared to the complete ground truth object mesh, and the final results are averaged across all the objects in all the scenes. For background we report the reconstruction metrics and rendered depth errors of the occluded regions only to highlight the effectiveness of our regularization. The results are also averaged across all the scenes. See supplementary for a detailed introduction to datasets and metrics.
We also report two 2D metrics, PSNR and mIoU, on the ToyDesk and ScanNet scenes. Here we omit the synthetic dataset considering its relatively simple texture. For the test image set, on ToyDesk dataset we use the official split, and on ScanNet we sample frames from their original camera trajectories. The metrics are averaged across different images of different scenes.

\begin{figure}
\begin{center}
\includegraphics[width=0.9\linewidth]{contents/figs/ablation.pdf}
\end{center}
% \vspace{-0.5em}
   \caption{\textbf{Effects of Regularizations.} The first row shows two backgrounds, the $\cL_{\text{bs}}$ clearly mitigates most of the artifacts. The last row shows that with $\cL_{\text{op}}$ we can get the watertight object and $\cL_{\text{rd}}$ further constrains the unobserved surface.}
% \vspace{-0.5em}
\label{fig:ablation}
\end{figure}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{contents/figs/fig_synthetic.pdf}
\end{center}
% \vspace{-1em}
   \caption{\textbf{Qualitative Results on Synthetic Scenes.} Above the blue line we show the comparison of different methods on two background scenes. Below we provide results of two scenes where only the object results are shown. In red rectangles at the bottom of each picture, we show the back~(left part) and front~(right part) views of an example object. Detailed descriptions in Section~\ref{exp:synthetic-reconstruction}.}
\label{fig:synthetic}
% \vspace{-1em}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{contents/figs/fig_scannet.pdf}
\end{center}
% \vspace{-0.5em}
   \caption{\textbf{Qualitative Results on ScanNet~\cite{dai2017scannet}.} On the left of blue line we show the overall reconstruction from \cite{yu2022monosdf} as the reference. On the right we show the comparison between our method and two baselines. Similarly, the back and front views of objects with partial observations are provided in red rectangles. Detailed descriptions in Section~\ref{exp-scannet}.}
\label{fig:scannet}
% \vspace{-1em}
\end{figure*}


\boldparagraph{Baselines}
We mainly compare with \textbf{ObjSDF}~\cite{wu2022object} in this work, as it is the only method that focuses on the same task, a specific discussion is provided in the supplementary. Since we focus on the indoor scene where monocular cues can benefit a lot~\cite{yu2022monosdf}, we add losses proposed in \cite{yu2022monosdf} to our method for better performance. For a fair comparison, we also combine \cite{wu2022object} and \cite{yu2022monosdf} for a stronger baseline, named \textbf{ObjSDF*}. 
Moreover, since ObjSDF*'s object reconstructions inevitably have artifacts, we develop a post-process method to cull the parts outside the background range and name this baseline \textbf{ObjSDF*-C}. We provide the details of how to set the range in the supplementary. Note that our method directly generates clean watertight meshes and doesn't need such post-processing. In ScanNet experiments, we also report results for MonoSDF~\cite{yu2022monosdf} as a reference since we can only evaluate the overall reconstruction~(details in Section~\ref{exp-scannet}).

\boldparagraph{Implementation Details}
We implement our method in PyTorch~\cite{paszke2019pytorch}. We use the Adam~\cite{kingma2014adam} optimizer with a learning rate of 5e-4 for 50k iterations and sample 1024 rays per iteration. The weight initialization scheme for SDF MLP is identical to \cite{yariv2021volume,wang2021neus,wu2022object}. The $u$ is initialized as $0.05$ and we set $\gamma$=$20$ as proposed in \cite{wu2022object}. $\cP$, $\cT_{\cP}$ and $\epsilon$ are set as $32$, $10$ and $0.05$ respectively. All the reconstructions are acquired by using marching cube~\cite{lorensen1987marching} at the resolution of $512$. 
% More implementation details can be found in the supplementary.

Moreover, we adopt the geometric initialization~\cite{gropp2020implicit} for the geometry network, which initializes the reconstruction with a unit sphere and the surface normals are facing inside at the beginning of the optimization. For the ScanNet dataset, we follow the protocol in \cite{yu2022monosdf} to crop the input image to $384\times 384$ and adjust the intrinsics accordingly. For the synthetic dataset, we directly render the image at the same resolution. Since we focus on the indoor scenes in this paper, we adopt the common practice to process the ``bounded'' scene. For each scene, we normalize the camera poses so that all the cameras lie in a unit sphere. The rays intersection, \ie the furthest sampling location, is computed based on this sphere and we also conduct Marching Cubes~\cite{lorensen1987marching} within the same area for the final reconstruction.

\subsection{Ablation Study}
\label{section-ablation}

\begin{table}[t]
	% \setlength{\tabcolsep}{0.5cm}
	\centering
	\resizebox{0.99\linewidth}{!}{%
		\input{contents/tabs/ablation}
	}
	\caption{
	\textbf{Ablation Study.} The D. denotes the depth error of the occluded background. The C. and F. mean the Chamfer-$L_1$ and F-score respectively, for the reconstruction of occluded background regions and full complete objects. Details in Section~\ref{section-ablation}.
}
\label{tab:ablation}
% \vspace{-0.8em}
\end{table}

We first quantitatively analyze the effectiveness of the proposed regularizations on synthetic scenes by comparing our full method to four variants in Tab.~\ref{tab:ablation}. 
First, adding the smoothness loss~(V2) significantly decreases the depth error of occluded background regions. 
Next, the $\cL_{\text{op}}$ makes the object reconstruction watertight and the object-level metrics are greatly improved~(V3). The $\cL_{\text{rd}}$ further improves the object reconstruction performance~(Full). Results of V4 prove that without $\cL_{\text{bs}}$ to improve the background, the object reconstruction quality also drops. Note that $\cL_{\text {rd}}$ needs to be applied jointly with $\cL_{\text{op}}$ to guarantee its second condition.


\begin{table}[t]
	% \setlength{\tabcolsep}{0.4cm}
	\centering
	\resizebox{0.99\linewidth}{!}{%
		\input{contents/tabs/synthetic}
	}
    \vspace{1em}
	\caption{
	\textbf{Synthetic Scenes Reconstruction Quantitative Results.} The D. C. and F. have the same meanings as in Tab.~\ref{tab:ablation}.
}
\label{tab:synthetic}
% \vspace{-1em}
\end{table}

A qualitative case is shown in Fig.~\ref{fig:ablation} for better illustration.
In the first row, the occluded part of the background can have many artifacts because of occlusion. 
With the smoothness regularization, the reconstructed backgrounds are much cleaner. Since the $\cL_{\text{bs}}$ only applies to the occluded region, the other parts won't be affected. In the second row, the sofa is pushed against the wall and reconstruction from \cite{wu2022object} contains only visible open surface. The $\cL_{\text{op}}$ first regularizes the SDF field to be a watertight mesh. The $\cL_{\text{rd}}$ further mitigates the flaws at the back by regularizing the object to be confined to the background.

\subsection{Reconstruction in Synthetic Scenes}
\label{exp:synthetic-reconstruction}

Since the synthetic scenes are rendered with objects that have accurate object-level 3D ground truth geometry, we compare our methods with previous object-compositional reconstruction method~\cite{wu2022object} on the $5$ generated scenes and report the quantitative results in Tab.~\ref{tab:synthetic}.

In Tab.~\ref{tab:synthetic} we provide detailed metrics on object-level reconstruction. A qualitative comparison is shown in Fig.~\ref{fig:synthetic}. The computation procedure of all the metrics is available in the supplementary. 
With monocular cues, ObjSDF* can achieve better performance than the original ObjSDF. However, their performance on object reconstruction is far from satisfactory, since they can only accurately obtain the visible surface with large parts of irrelevant structures in the indoor scenes as pointed out in the analysis before. 
Though ObjSDF*-C can eliminate most of the outliers and get improved performance, it can only reconstruct visible regions as an open surface as shown in Fig.~\ref{fig:synthetic}.
On the contrary, our regularizations help to smoothen the background and recover the object as a watertight mesh, which promotes performance by a large margin and leads to broader downstream applications. As shown in Fig.~\ref{fig:synthetic}, RICO can reconstruct the unobservable~(back view as in the caption) regions of the objects where the previous methods fail.

\begin{table}[t]
	% \setlength{\tabcolsep}{0.5cm}
	\centering
	\resizebox{0.95\linewidth}{!}{%
		\input{contents/tabs/scannet}
	}
	\caption{
	\textbf{ScanNet Reconstruction Quantitative Results.}
}
\label{tab:scannet}
% \vspace{-1em}
\end{table}

\subsection{Reconstruction in Real-world Scenes}
\label{exp-scannet}

We conduct experiments on 7 scenes of ScanNet~\cite{dai2017scannet} to show the effectiveness of our method on real-world data. Since ScanNet only provides ground truth for the visible surface, here we follow the protocol in \cite{guo2022neural} and report the reconstruction performance of the entire scene in Tab.~\ref{tab:scannet}. By utilizing the rendering formulation in \cite{wang2021neus} which explicitly models the angle between surface normal and ray, our method can achieve slightly better performance compared to ObjSDF* and \cite{yu2022monosdf} on visible surfaces.

In Tab.~\ref{tab:psnr-miou} we evaluate the 2D rendering and segmentation performance and provide the PSNR and mIoU results on two real-world datasets. As our method constrains mainly the unobservable, the PSNR and mIoU metrics are not with significant difference.

\begin{table}[t]
	% \setlength{\tabcolsep}{0.5cm}
	\centering
	\resizebox{0.95\linewidth}{!}{%
		\input{contents/tabs/psnr-miou}
	}
	\caption{
	\textbf{PSNR and mIoU Results} of different methods on scenes from ScanNet and ToyDesk datasets.
}
\label{tab:psnr-miou}
% \vspace{-1em}
\end{table}

In Fig.~\ref{fig:scannet} we also provide the comparisons over two scenes. Note that the ScanNet images are blurry and noisy, and the masks are sometimes inaccurate~(\eg, legs of chairs missing), leading to less accurate reconstructions. However, it can still be seen that our method can successfully regularize the unobservable regions to get the watertight mesh, while the baselines always result in open surface. For example, the piano in the first row and the sofas in the second row can only be reconstructed on visible surfaces. 

And in Fig.~\ref{fig:toydesk} we provide the qualitative results of our method and two baselines on two ToyDesk scenes. It can be seen that the baseline methods generate non-watertight surface with undesired artifacts~(using table plane to cut the mesh will result in holes on the cut face), while our method can directly get clean results.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{contents/figs/toydesk_vis.pdf}
\end{center}
% \vspace{-1em}
   \caption{\textbf{Qualitative Results on ToyDesk.} Here we show the reconstruction results both with and without the background for better visualization.}
\label{fig:toydesk}
% \vspace{-1em}
\end{figure}

\subsection{Object Manipulation}

As aforementioned, the reconstruction results from ObjSDF* are sub-optimal for downstream applications like object manipulation. On the contrary, since RICO can get a watertight mesh, it can be easily used for such applications.
In Fig.~\ref{fig:manipulation} we show the volume rendered normal maps and segmentation masks before and after moving an object in the scene. An illustration of how to manipulate one object in our framework and conduct volume rendering accordingly is applicable in the supplementary. As illustrated, after manipulation, the rendered segmentation and normal map are still clean for RICO. In contrast, the results of ObjSDF* are messy because their reconstruction is connected with artifacts that were originally outside of the scenes.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{contents/figs/fig_manipulation_new.pdf}
\end{center}
% \vspace{-1em}
   \caption{\textbf{Object Manipulation Cases.} Here we show the volume rendered normal maps and segmentation masks in two scenes, before~(top row) and after~(bottom row) the manipulation.}
\label{fig:manipulation}
% \vspace{-1em}
\end{figure}