\section{Introduction}
\label{intro}

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{contents/figs/fig1_new.pdf}
\end{center}
% \vspace{-1em}
\caption{\textbf{Comparison} of~\cite{wu2022object}~(Top) and ours~(Bottom). Different objects are visualized in different colors. Previous reconstructions are connected with large artifacts. Taking the partially observed cubic object's front and back views~(in red and green rectangles respectively) as an example, \cite{wu2022object} can only get the visible surface, while ours can complement the complete body.}
% \vspace{-1em}
\label{fig:intro}
\end{figure}

Reconstructing 3D geometry from images is a fundamental problem in computer vision and has many downstream applications like VR/AR and game assets creation. With the advance of neural implicit representations~\cite{mildenhall2021nerf}, recent reconstruction methods~\cite{oechsle2021unisurf,wang2021neus,yariv2021volume,yu2022monosdf} can recover accurate geometry from multi-view images.
However, existing methods typically regard the whole scene as an entirety and reconstruct everything altogether, thus preventing applications like scene editing. In indoor scenes with plenty of reconfigurable objects, a disentangled object-compositional reconstruction, \ie decomposing the geometry into different instantiated objects and background, can be more suitable for further applications like moving the sofa in the scene.


In this paper, we aim to recover the room-level indoor scenes with decomposed geometry of individual objects and background.
We assume that multi-view posed images and semantic masks that assign different labels to each instantiated object and the background are given as input.
Existing object-compositional methods~\cite{guo2020object,zhi2021place,yang2021learning} concentrate more on the rendering performance rather than the underlying geometry, and thus can not be directly used for reconstruction. 
The most recent work ObjSDF~\cite{wu2022object} learns an object-compositional signed distance function~(SDF) field by proposing a transform function between SDF values and semantic logits. Specifically speaking, ObjSDF predicts multiple SDF values at each 3D point for different semantic classes, and converts them to semantic logits, allowing for separating object SDF values from the background when supervised by semantic masks. 
Although achieving plausible shape disentanglement, it suffers from a common problem in indoor scenes: objects and background can only be \textit{partially observed due to occlusions}. When the object is partially observed, \eg a cubic object against the wall, ObjSDF can not properly reconstruct the geometry between them~(see Fig.~\ref{fig:intro} top-right). The reason is that the existing works~\cite{zhi2021place,wu2022object} can only effectively regularize semantic labels and geometry of observed regions, and have little impact on the unobserved regions. 
When processing the indoor scenes where a large portion of objects are partially observed, the reconstruction results of these objects will be visible surface connected with the unconstrained structures~(as shown in Fig.~\ref{fig:intro}, see Section~\ref{method-obj-scene} for a detailed analysis).
Fig.~\ref{fig:intro-objsdf-edit} shows that even with reasonable reconstruction when composing all the objects together, each object's result in the unobserved region is far from satisfactory and can hinder further applications like manipulating the object.

We propose \netname, which realizes the proper geometry disentanglement for indoor scenes~(see Fig.~\ref{fig:intro} bottom) by explicitly regularizing the unobserved regions. To be more specific, when the object is partially observed, recovering its geometry is an ill-posed problem even with corresponding masks. Thus, introducing prior regularization for unobserved regions is necessary. We exploit two types of prior knowledge for indoor scenes in this work: 1)~background smoothness and 2)~object-background relations. First, when one ray hits the object surface, the existing method~\cite{wu2022object} can properly regularize the geometry and appearance on the hitting point, but can not account for the background surface behind this object. This drawback leads to artifacts and holes on the unobserved background surface~(see Fig.~\ref{fig:intro-objsdf-edit}). We propose a patch-based smoothness loss to regularize the SDF values of unobserved background regions. Then, since the background reconstruction is improved, we can leverage another strong prior: \textit{the objects are all within the room}, \ie using the background surface to regularize the SDF field of objects. We design two regularization terms: an object point-SDF loss for sampled points behind the background surface and a reversed depth loss to regularize the SDF distribution of the entire ray. Both terms aim to bound the object within the background surface's range, thus preventing the aforementioned unmeaning structure, making the object reconstruction a \textit{watertight and plausible} shape instead of an open surface with severe artifacts.

In summary, we propose RICO to realize compositional reconstruction in indoor scenes where a large portion of objects are partially observed. Our main contributions are: 
i)~A patch-based background smoothness regularizer for unobserved background surface geometry. ii)~Guided by the improved background surface, we exploit the object-background relation as prior knowledge and design objectives that effectively regularize objects' unobserved regions. iii)~Extensive experiments on both real-world and synthetic datasets prove our superior reconstruction performance compared to previous works, especially for the partially observed objects.

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{contents/figs/fig2.pdf}
\end{center}
   \caption{\textbf{ObjSDF results.} Interested objects are dyed in blue. Despite of the plausible composition, the disentangled backgrounds have artifacts and sunk holes, and partially observed objects can only get the visible surface~(illustrated in `Object Backward').}
% \vspace{-1em}
\label{fig:intro-objsdf-edit}
\end{figure}
