\section{Experiments}
\label{experiments}
 
We first conduct ablation study on each proposed component. 
Then we provide quantitative and qualitative results of object-compositional reconstruction on real-world and synthetic scenes.
Finally, we show some possible object manipulation with our compositional reconstruction.

\boldparagraph{Datasets~\&~Metrics}
We consider two types of indoor datasets with multi-view RGB images and masks: 1)~ScanNet~\cite{dai2017scannet}, a real-world dataset widely used in previous works~\cite{guo2022neural,wang2022neuris,wu2022object,yu2022monosdf}; 2)~a hand-crafted synthetic dataset with five scenes, each containing $5\sim 10$ objects. The synthetic dataset is considered such that the ground truth geometry of both, occluded and non-occluded regions are available.
On ScanNet we report Chamfer Distance, F-score for evaluation, following \cite{guo2022neural,yu2022monosdf}. On synthetic scenes we divide metrics for two aspects: objects and background. For objects we report the reconstruction performance compared to the complete ground truth object mesh, and the final results are averaged across all the objects in all the scenes. For background we report the reconstruction metrics and rendered depth errors of the occluded regions only to highlight the effectiveness of our regularization. The results are also averaged across all the scenes. See supplementary for a detailed introduction to datasets and metrics.

\begin{figure}
\begin{center}
\includegraphics[width=0.9\linewidth]{contents/figs/ablation.pdf}
\end{center}
\vspace{-0.5em}
   \caption{\textbf{Effects of Regularizations.} The first row shows two backgrounds, the $\cL_{\text{bs}}$ clearly mitigates most of the artifacts. The last row shows that with $\cL_{\text{op}}$ we can get the watertight object and $\cL_{\text{rd}}$ further constrains the unobserved surface.}
\vspace{-0.5em}
\label{fig:ablation}
\end{figure}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{contents/figs/fig_synthetic.pdf}
\end{center}
\vspace{-1em}
   \caption{\textbf{Qualitative Results on Synthetic Scenes.} Above the blue line we show the comparison of different methods on two background scenes. Below we provide results of two scenes where only the object results are shown. In red rectangles at the bottom of each picture, we show the back~(left part) and front~(right part) views of an example object. Detailed descriptions in Section~\ref{exp:synthetic-reconstruction}.}
\label{fig:synthetic}
\vspace{-1em}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{contents/figs/fig_scannet.pdf}
\end{center}
\vspace{-0.5em}
   \caption{\textbf{Qualitative Results on ScanNet~\cite{dai2017scannet}.} On the left of blue line we show the overall reconstruction from \cite{yu2022monosdf} as the reference. On the right we show the comparison between our method and two baselines. Similarly, the back and front views of objects with partial observations are provided in red rectangles. Detailed descriptions in Section~\ref{exp-scannet}.}
\label{fig:scannet}
\vspace{-1em}
\end{figure*}


\boldparagraph{Baselines}
We mainly compare with \textbf{ObjSDF}~\cite{wu2022object} in this work, as it is the only method that focuses on the same task, a specific discussion is provided in the supplementary. Since we focus on the indoor scene where monocular cues can benefit a lot~\cite{yu2022monosdf}, we add losses proposed in \cite{yu2022monosdf} to our method for better performance. For a fair comparison, we also combine \cite{wu2022object} and \cite{yu2022monosdf} for a stronger baseline, named \textbf{ObjSDF*}. 
Moreover, since ObjSDF*'s object reconstructions inevitably have artifacts, we develop a post-process method to cull the parts outside the background range and name this baseline \textbf{ObjSDF*-C}. We provide the details of how to set the range in the supplementary. Note that our method directly generates clean watertight meshes and doesn't need such post-processing. In ScanNet experiments, we also report results for MonoSDF~\cite{yu2022monosdf} as a reference since we can only evaluate the overall reconstruction~(details in Section~\ref{exp-scannet}).

\subsection{Ablation Study}
\label{section-ablation}

\begin{table}[t]
	% \setlength{\tabcolsep}{0.5cm}
	\centering
	\resizebox{0.99\linewidth}{!}{%
		\input{contents/tabs/ablation}
	}
	\caption{
	\textbf{Ablation Study.} The D. denotes the depth error of the occluded background. The C. and F. mean the Chamfer-$L_1$ and F-score respectively, for the reconstruction of occluded background regions and full complete objects. Details in Section~\ref{section-ablation}.
}
\label{tab:ablation}
\vspace{-0.8em}
\end{table}

We first quantitatively analyze the effectiveness of the proposed regularizations on synthetic scenes by comparing our full method to four variants in Tab.~\ref{tab:ablation}. 
First, adding the smoothness loss~(V2) significantly decreases the depth error of occluded background regions. 
Next, the $\cL_{\text{op}}$ makes the object reconstruction watertight and the object-level metrics are greatly improved~(V3). The $\cL_{\text{rd}}$ further improves the object reconstruction performance~(Full). Results of V4 prove that without $\cL_{\text{bs}}$ to improve the background, the object reconstruction quality also drops. Note that $\cL_{\text {rd}}$ needs to be applied jointly with $\cL_{\text{op}}$ to guarantee its second condition.


\begin{table}[t]
	% \setlength{\tabcolsep}{0.4cm}
	\centering
	\resizebox{0.99\linewidth}{!}{%
		\input{contents/tabs/synthetic}
	}
    \vspace{0.5em}
	\caption{
	\textbf{Synthetic Scenes Reconstruction Quantitative Results.} The D. C. and F. have the same meanings as in Tab.~\ref{tab:ablation}.
}
\label{tab:synthetic}
\vspace{-1em}
\end{table}

A qualitative case is shown in Fig.~\ref{fig:ablation} for better illustration.
In the first row, the occluded part of the background can have many artifacts because of occlusion. 
With the smoothness regularization, the reconstructed backgrounds are much cleaner. Since the $\cL_{\text{bs}}$ only applies to the occluded region, the other parts won't be affected. In the second row, the sofa is pushed against the wall and reconstruction from \cite{wu2022object} contains only visible open surface. The $\cL_{\text{op}}$ first regularizes the SDF field to be a watertight mesh. The $\cL_{\text{rd}}$ further mitigates the flaws at the back by regularizing the object to be confined to the background.

\subsection{Reconstruction in Synthetic Scenes}
\label{exp:synthetic-reconstruction}

Since the synthetic scenes are rendered with objects that have accurate object-level 3D ground truth geometry, we compare our methods with previous object-compositional reconstruction method~\cite{wu2022object} on the $5$ generated scenes and report the quantitative results in Tab.~\ref{tab:synthetic}.

In Tab.~\ref{tab:synthetic} we provide detailed metrics on object-level reconstruction. A qualitative comparison is shown in Fig.~\ref{fig:synthetic}. The computation procedure of all the metrics is available in the supplementary. 
With monocular cues, ObjSDF* can achieve better performance than the original ObjSDF. However, their performance on object reconstruction is far from satisfactory, since they can only accurately obtain the visible surface with large parts of irrelevant structures in the indoor scenes as pointed out in the analysis before. 
Though ObjSDF*-C can eliminate most of the outliers and get improved performance, it can only reconstruct visible regions as an open surface as shown in Fig.~\ref{fig:synthetic}.
On the contrary, our regularizations help to smoothen the background and recover the object as a watertight mesh, which promotes performance by a large margin and leads to broader downstream applications. As shown in Fig.~\ref{fig:synthetic}, RICO can reconstruct the unobservable~(back view as in the caption) regions of the objects where the previous methods fail.

\begin{table}[t]
	% \setlength{\tabcolsep}{0.5cm}
	\centering
	\resizebox{0.95\linewidth}{!}{%
		\input{contents/tabs/scannet}
	}
	\caption{
	\textbf{ScanNet Reconstruction Quantitative Results.}
}
\label{tab:scannet}
\vspace{-1em}
\end{table}

\subsection{Reconstruction in Real-world Scenes}
\label{exp-scannet}

We conduct experiments on 7 scenes of ScanNet~\cite{dai2017scannet} to show the effectiveness of our method on real-world data. Since ScanNet only provides ground truth for the visible surface, here we follow the protocol in \cite{guo2022neural} and report the reconstruction performance of the entire scene in Tab.~\ref{tab:scannet}. By utilizing the rendering formulation in \cite{wang2021neus} which explicitly models the angle between surface normal and ray, our method can achieve slightly better performance compared to ObjSDF* and \cite{yu2022monosdf} on visible surfaces.

In Fig.~\ref{fig:scannet} we also provide the comparisons over two scenes. Note that the ScanNet images are blurry and noisy, and the masks are sometimes inaccurate~(\eg, legs of chairs missing), leading to less accurate reconstructions. However, it can still be seen that our method can successfully regularize the unobservable regions to get the watertight mesh, while the baselines always result in open surface. For example, the piano in the first row and the sofas in the second row can only be reconstructed on visible surfaces. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{contents/figs/fig_manipulation_new.pdf}
\end{center}
\vspace{-1em}
   \caption{\textbf{Object Manipulation Cases.} Here we show the volume rendered normal maps and segmentation masks in two scenes, before~(top row) and after~(bottom row) the manipulation.}
\label{fig:manipulation}
\vspace{-1em}
\end{figure}

\subsection{Object Manipulation}

As aforementioned, the reconstruction results from ObjSDF* are sub-optimal for downstream applications like object manipulation. On the contrary, since RICO can get a watertight mesh, it can be easily used for such applications.
In Fig.~\ref{fig:manipulation} we show the volume rendered normal maps and segmentation masks before and after moving an object in the scene. An illustration of how to manipulate one object in our framework and conduct volume rendering accordingly is applicable in the supplementary. As illustrated, after manipulation, the rendered segmentation and normal map are still clean for RICO. In contrast, the results of ObjSDF* are messy because their reconstruction is connected with artifacts that were originally outside of the scenes.
