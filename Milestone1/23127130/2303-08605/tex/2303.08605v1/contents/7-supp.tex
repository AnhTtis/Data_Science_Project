% \section{xxx}
% \section{yyy}

\section{Network Architecture}
\label{supp-architecture}

We adopt the similar network structure as in \cite{wu2022object} and add the appearance code utilized in \cite{yu2022monosdf} for all the baseline models and ours. Fig.~\ref{fig:network} shows our detailed network architecture.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\linewidth]{contents/figs/supp/architecture.pdf}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\end{center}
% \vspace{-1em}
   \caption{\textbf{Network Architecture.} The geometry network corresponds to the SDF function $f$ and the appearance network denotes the appearance $g$ in the main paper. }
\label{fig:network}
\vspace{-1em}
\end{figure}

The geometry network is a 8-layer MLP with hidden dimension 256, and one skip connection at the fourth layer. The input of this network is the point coordinate mapped by a fixed positional encoding~\cite{mildenhall2021nerf}. The output consists of a 256 dimensional geometry feature and $k$ SDF values for each object. These SDF values can be transformed to semantic logits using the function proposed in \cite{wu2022object}.

For the input of the appearance network, we adopt the design in \cite{yu2022monosdf}. We optimize a per-frame appearance code during the training and use this per-frame code to model the varying light and blurry condition in each image for better reconstruction. The appearance code is concatenated with the viewing direction~(also mapped by positional encoding), the normal of the scene SDF, the geometry feature and the point coordinate. The appearance network consists of two layers with hidden dimension 256, and outputs a 3-channel RGB color.

We use Softplus activation for the geometry network and use ReLU
activation for the appearance network, the RGB color is obtained after passing the network's output through the Sigmoid activation.

\section{Implementation}
\subsection{Loss Functions Details}

We elaborate all the losses and the weight choices used in the optimization in this section.

\boldparagraph{RGB Reconstruction Loss}
To learn the surface from images input, we need to minimize the difference between ground-truth pixel color and the rendered color. We follow the previous works~\cite{yu2022monosdf,wu2022object} here for the RGB reconstruction loss:

\begin{equation}
    \cL_{\text{RGB}} = \sum_{\br}||\hat{\bC}(\br) - \bC(\br)||_1.
\end{equation}
Here $\hat{\bC}(\br)$ is the rendered color from volume rendering and $\bC(\br)$ denotes the ground truth. The weight for this loss function is set to 1.

\boldparagraph{Depth Consistency Loss}
Monocular depth and normal cues~\cite{yu2022monosdf} can greatly benefit indoor scene reconstruction. For the depth consistency, we minimize the difference between rendered depth $\hat{D}(\br)$ and the depth estimation $\bar{D}(\br)$ from the Omnidata~\cite{eftekhar2021omnidata} model:

\begin{equation}
    \cL_{\text{D}} =  \sum_{\br}|| (w\hat{D}(\br) + q) - \bar{D}(\br) ||^2,
\end{equation}
where $w$ and $q$ are the scale and shift values to match the different scales. We solve $w$ and $q$ with a least-squares criterion, which has the closed-form solution. Please refer to the supplementary of \cite{yu2022monosdf} for a detailed computation process. The loss weight $\lambda_{\text{D}}$ is set as $0.1$.

\boldparagraph{Normal Consistency Loss}
Similar to the depth consistency loss, we also use the normal cues $\bar{N}$ from Omnidata model to supervise the rendered normal. Specifically, the normal consistency loss consists of L1 and the angular losses:
\begin{equation}
    \cL_{\text{N}} = \sum_{\br} ||\hat{N}(\br) - \bar{N}(\br)||_1 + ||1-  \hat{N}(\br)^{\text{T}}\bar{N}(\br)||_1.
\end{equation}
Here the volume-rendered normal and normal estimation will be transformed into the same coordinate system by the camera pose. The loss weight $\lambda_{\text{N}}$ is $0.05$.


\boldparagraph{Semantic Loss}
We minimize the semantic loss between volume-rendered semantic logits of each pixel and the ground-truth pixel semantic class. Here the semantic objective is implemented as a cross-entropy loss:
\begin{equation}
    \cL_{\text{S}} = \sum_{\br}\sum_{j=1}^k-\hat{h}_j(\br)\log h_j(\br).
\end{equation}
The $\hat{h}_j(\br)$ is the ground-truth semantic probability for $j$-th object, which is $1$ or $0$. We follow the weight design in \cite{wu2022object} and set $\lambda_{\text{S}} = 0.1$.

\boldparagraph{Eikonal Loss}
Following common practice, we also add an Eikonal term on the sampled points to regularize SDF values in 3D space:
\begin{equation}
    \cL_{\text{E}} = \sum_{i}^{n}(|| \nabla \min_{1\leq j\leq k}~s_j(\bp_i)||_2 - 1)
\end{equation}
Here the eikonal loss is applied to the gradient of the scene SDF, which is the minimum of all the SDFs. The loss weight $\lambda_{\text{E}}$ is set to $0.1$, which is identical to \cite{wu2022object}.

\subsection{Additional Implementation Details}
% \textbf{normalize the pose and use cube/sphere to get intersection}
We adopt the geometric initialization~\cite{gropp2020implicit} for the geometry network, which initializes the reconstruction with a unit sphere and the surface normals are facing inside at the beginning of the optimization. For the ScanNet dataset, we follow the protocol in \cite{yu2022monosdf} to crop the input image to $384\times 384$ and adjust the intrinsics accordingly. For the synthetic dataset, we directly render the image at the same resolution. Since we focus on the indoor scenes in this paper, we adopt the common practice to process the ``bounded'' scene. For each scene, we normalize the camera poses so that all the cameras lie in a unit sphere. The rays intersection, \ie the furthest sampling location, is computed based on this sphere and we also conduct Marching Cubes~\cite{lorensen1987marching} within the same area for the final reconstruction.

\section{Evaluation Metrics}

To evaluate the reconstruction performance, we use the Chamfer Distance and F-score with a threshold of 5cm in this paper. In detail, Chamfer Distance comes from \textit{Accuracy} and \textit{Completeness}, and F-score is derived from \textit{Precision} and \textit{Recall}. For point clouds $P$ and $P^*$ sampled from the predicted and the ground-truth mesh, we show the detailed computation procedure here:

\small
\begin{equation}
    \begin{aligned}
        \texttt{Accuracy} &= \mathop{\texttt{mean}}_{\bp \in P}\left( \min_{\bp^*\in P*} ||\bp-\bp^*||_1 \right), \\
        \texttt{Completeness} &= \mathop{\texttt{mean}}_{\bp^* \in P^*}\left( \min_{\bp\in P} ||\bp-\bp^*||_1 \right), \\
        \texttt{Chamfer-}L_1 &= \frac{\texttt{Accuracy} + \texttt{Completeness}}{2}.
    \end{aligned}
\end{equation}

\begin{equation}
    \begin{aligned}
        \texttt{Precision} &= \mathop{\text{mean}}_{\bp \in P}\left( \min_{\bp^*\in P*} ||\bp-\bp^*||_1 < 0.05 \right), \\
        \texttt{Recall} &= \mathop{\text{mean}}_{\bp^* \in P^*}\left( \min_{\bp\in P} ||\bp-\bp^*||_1 < 0.05 \right), \\
        \texttt{F-score} &= \frac{2\times \texttt{Precision}\times \texttt{Recall}}{\texttt{Precision}+ \texttt{Recall}}.
    \end{aligned}
\end{equation}
\normalsize

\section{Synthetic Dataset Construction}

In order to quantitatively evaluate the object-level reconstruction performance in the object-compositional indoor scenes, we create a synthetic dataset with object ground-truth geometry. In this part we elaborate on how to construct the Synthetic Dataset used in this paper. Despite that the dataset is not a major contribution of this paper, we will release it for future comparisons.

We use Blender~\cite{blender} and an add-on BlenderNeRF~\cite{Raafat_BlenderNeRF_2023} to construct the scenes~(assign different object locations, lighting conditions and camera trajectories) and render the RGB images together with the camera poses. The Blender's camera coordinate system is different from the coordinate system in ScanNet, which requires an extra $180^\circ$ rotation along with the x-axis on the recorded extrinsic matrix. 

To render semantic masks, we switch each object's surface texture to a certain value and render again with the identical camera trajectories. We create $5$ scenes and three of them contain $5$ objects while other two contain $10$ objects~(background not included). For each scene we render $200$ images and use the Omnidata~\cite{eftekhar2021omnidata} model to obtain the corresponding monocular depth and normal cues.

\section{Additional Ablation Experiments}

\subsection{Parameter Ablation Study}
In this part we provide the ablation experiment for the $\epsilon$ in proposed object point-SDF loss $\cL_{\text{op}}$. Particularly, $\epsilon$ is a non-negative number as a threshold, that the objects' SDFs outside of the background should be larger than this value. We provide an ablation study on different $\epsilon$ values on synthetic scenes in Tab.~\ref{tab:epsilon}.

\begin{table}[htbp]
	% \setlength{\tabcolsep}{0.5cm}
	\centering
	\resizebox{0.95\linewidth}{!}{%
		\input{contents/tabs/supp/epsilon}
	}
	\caption{
	\textbf{Ablation Study on $\epsilon$.} Metrics are evaluated and averaged on all the objects of all the synthetic scenes.
}
\label{tab:epsilon}
\vspace{-1em}
\end{table}

Intuitively, $\epsilon$ should be larger than 0 because the points behind the background are outside of each object's surface, \ie the object SDFs of these points should be positive. Empirically we find setting $\epsilon=0.05$ yields slightly better performance than $0.1$ and $0.2$. When setting $\epsilon=0$, the object reconstruction performance drops significantly. We found the reason is that, when $\epsilon$ is $0$, the SDFs of the sampled points can not all be effectively optimized to positive, yielding some negative SDF vaules, which results in the flaws in the empty space.

\subsection{Backbone Ablation Study}

When utilizing SDF as the surface geometry representation, there are typically two choices to combine SDF and volume rendering as proposed in\cite{yariv2021volume,wang2021neus}. In the main paper we adopt the scheme proposed in NeuS~\cite{wang2021neus} for RICO. We provide the comparison of reconstruction results on ScanNet~(evaluated on whole scene) and Synthetic scenes~(evaluated on each object), and report the results in Tab.~\ref{tab:backbone}. Since \cite{wang2021neus} explicitly models the angle difference of ray direction and the surface normal, it can have slightly better performance by better reconstructing the visible surface.

\begin{table}[htbp]
    \centering
    \resizebox{0.95\linewidth}{!}{%
		\input{contents/tabs/supp/backbone}
	}
    \caption{\textbf{Ablation Study on Backbone}. We show the reconstruction comparison of our methods using the volume rendering scheme proposed in \cite{yariv2021volume}~(RICO-VolSDF) and using the scheme in \cite{wang2021neus}~(RICO, which represents the method proposed in main paper).}
    \label{tab:backbone}
    \vspace{-1em}
\end{table}

\section{Construct the ObjSDF*-C Baseline}

As stated in the main paper, we construct an improved baseline over ObjSDF*, named ObjSDF*-C, to provide better visualization and quantitative results. The main procedure is to use the reconstructed background surface to eliminate the parts of object reconstruction that are outside of the background range. The construction procedure is just a post-process method on the object meshes and do not change the original nature of the ObjSDF* that only the visible surfaces are reconstructed.

For Synthetic scenes, since we set the background as a cubic room with a range of $[-2m, 2m]$ in three dimensions, \ie the background is an axis-aligned box, we can directly use this range to segment the object reconstruction meshes. For ScanNet scenes, we use the ground-truth scene meshes to get a coarse range and manually finetune the range of each scene~(ObjSDF*-C on ScanNet is only used for visualization, not for quantitative evaluation), then segment the object meshes based on the finetuned range.

\section{Object Manipulation Implementation}
\label{manipulation}

To manipulate the reconstructed objects, a straightforward way is to directly manipulate the meshes. In the main paper we show the volume rendered normal maps and semantic masks before and after manipulation. In Fig.~\ref{fig:manipulation-implementation} we show how to implement the volume rendering in current framework. The core is to query the SDF value of manipulated object at the destined point, and combine it with other objects' SDF values. 
Notably, the color is decided by not only the coordinate but also the geometry feature~(illustrated in Fig.~\ref{fig:network}).
However, now the original point for other object SDFs and the manipulated point for the desired object SDF will result in two geometry feature vectors. 
In contrast to use minimum value to get the scene SDF from all SDFs, it's hard to decide how to fuse these two geometry features together in current framework. Now we only show the volume rendering results that are decided by SDF values, \ie geometry, like the normal map and semantic mask.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\linewidth]{contents/figs/supp/manipulation.pdf}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\end{center}
% \vspace{-1em}
   \caption{\textbf{Object Manipulation Implementation.} In this figure we show when moving the object $j$ with $\left[\delta x, \delta y, \delta z\right]$ in each direction, how to implement volume rendering in the current network.}
\label{fig:manipulation-implementation}
\vspace{-1em}
\end{figure}

\section{Qualitative Results on Individual Scenes}

In Fig.~\ref{fig:vis_scannet} and Fig.~\ref{fig:vis_synthetic}, we provide the RICO's object-compositional reconstruction on ScanNet scenes and Synthetic scenes~(with the object ground-truth) respectively.

\section{Limitations} 
In this work, we assume the indoor scene as convex room, that the ray shot inside of the room penetrates the background surface once. 
However when processing the more complex indoor scenes where one ray can go through multiple rooms, our object regularizations may require extra conditions to decide which points to be applied to.
Additionally, the object-scene relation prior regularized the completeness from only the geometry perspective. The framework can be extended to utilizing more complex category-level prior for better reconstruction.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{contents/figs/supp/vis_scannet.pdf}
    \caption{\textbf{Qualitative Visulization on ScanNet.} We show the object-compositional reconstruction results from RICO on seven ScanNet~\cite{dai2017scannet} scenes.}
    \label{fig:vis_scannet}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.85\linewidth]{contents/figs/supp/vis_synthetic.pdf}
    \caption{\textbf{Qualitative Visualization on Synthetic Scenes.} In the left column we show the ground-truth object geometry of the five synthetic scenes, in the right column we provide the qualitative object-compositional reconstruction results of our proposed RICO.}
    \label{fig:vis_synthetic}
\end{figure*}
