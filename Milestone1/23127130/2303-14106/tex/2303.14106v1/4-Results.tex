%\documentclass[main.tex]{subfiles}
\label{sec:intro}
%\begin{document}

\subsection{Well-defined executions}

We start with a basic result, on the consistency of an execution as defined by the algorithm.

\begin{lem}\label{lem:welldefined}
Any signal trace of an execution has at most finitely many value-changes within a finite time interval; it is thus a well-defined signal trace.
\end{lem}
\begin{proof}
Assume by contradiction that a signal trace has infinitely many value-changes within a finite interval $[t,t'] \subset \IR$.
By consistency of prefixes of executions, this implies that the algorithm returns
  an execution with infinitely many value-changes when setting $T = t'$.

In the algorithm, at any point in time $\tau$ there is at most one action per non-input signal in the set of scheduled actions and at most bounded many actions per input signal until time $T$.
Observing that there is a minimum propagation delay $d_{\min} > 0$ for signal values $0$, $1$, and $\meta$, any newly scheduled action must occur at earliest at time $\tau + d_{\min}$.
Thus only bounded many actions occur within $[\tau,\tau + d_{\min}]$.
The statement follows.
\end{proof}

\subsection{A transient-fault insertion tool}

To study the effect of short transient faults on the behavior of circuits we extend the algorithm from Section~\ref{sec:model} to allow the insertion of external events: signal transitions from a set of \emph{external events} are applied at the end of step~3.
Step~5 is changed to include external events when updating time~$t$ to the time of the next event.
A transient fault then corresponds to two subsequent signal transitions of the same signal in the set of external events.

We have implemented the algorithm in Python.
% add when available: and made it available via (link).

\paragraph{Linear pipeline}
To study the susceptibility of QDI circuits to transient faults, we used the tool to insert short pulses (glitches) at different times.
As a prototypical QDI circuit, we used the linear 3-stage Muller pipeline shown in Figure~\ref{fig:muller3_linear}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/muller3_linear.pdf}
    \caption{Linear Muller pipeline with 3 stages. The delays are set to 1 ({\tt INV2, INV3}), 5 (C gate), 4 (source delay = {\tt INV1}), and 4 (sink delay = {\tt INV4}).}
    \label{fig:muller3_linear}
\end{figure}

Delays have been uniformly set to 1 for the two pipeline inverters {\tt INV2} and {\tt INV3}, to 5 for all Muller C-elements ({\tt MCE1} to {\tt MCE3}), and 4 for the left most inverter {\tt INV1} and the rightmost inverter {\tt INV4}, which model the source and sink of the pipeline, respectively. 

Figure~\ref{fig:trace} shows an execution prefix until time $T=32$ in absence of transient faults, generated by our tool.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/trace-muller-linear.pdf}
    \caption{Execution prefix of linear 3-stage pipeline until time $T=32$.}
    \label{fig:trace}
\end{figure}

Figures~\ref{fig:trace-glitch} and \ref{fig:trace-glitch-ii} show execution prefixes of the same circuit until time $T=32$ when a glitch is inserted at the same signal, {\tt c2}, at different points in time:
  the intervals during which a signal has value $\meta$ are marked in red.
One observes that the behavior in presence of the glitch is different as detailed in the following.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/trace-muller-linear-glitch.pdf}
    \caption{Execution prefix of linear 3-stage pipeline until time $T=32$ with glitch of width $0.1$ inserted at time $10$ at signal {\tt c2}.}
    \label{fig:trace-glitch}
\end{figure}

\emph{Non-masked glitch.} In Figure~\ref{fig:trace-glitch} the glitch occurs at the input of the MCE while it is in storage mode, i.e., non-matching inputs. Since the other stable input, {\tt en3}, is at different logic level than the MCE output, {\tt c3}, the $\meta$ value is generated in the latter which later propagates through other signals of the circuit.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/trace-muller-linear-glitch-ii.pdf}
    \caption{Execution prefix of linear 3-stage pipeline until time $T=32$ with glitch of width $0.1$ inserted at time $22$ at signal {\tt c2}.}
    \label{fig:trace-glitch-ii}
\end{figure}

\emph{Masked glitch.} The glitch in Figure~\ref{fig:trace-glitch-ii}, however, occurs at the input of the MCE while in combinational mode, i.e. matching inputs. The glitch is %(\emph{logically}) 
masked at the output {\tt c3}, but the $\meta$ value appears for a short period of time at {\tt en1} (since an inverter will always propagate any value it is fed). During this time span, the $\meta$ value appeared and disappeared while the other MCE was also in combinational mode, hence was prevented from propagating the unstable value further in the circuit. 


\paragraph{Susceptibility to transient faults}
The two different behaviors raise the question of when a QDI circuit like the linear pipeline can mask glitches successfully, and when it is susceptible to them.
To address that, we relate susceptibility to the occurrence of glitches at signals of particular interest (typically the output signals to the environment).
We call these signals of interest the \emph{monitored signals}.
For example, in the linear pipeline, signals {\tt c1} and {\tt c3} are the outputs to the environment represented by the source on the left and the sink on the right.

In general, let $C$ be a circuit, and $i$
  input signal traces.
Let $M \subseteq \calL \cup \calO$ be the set of monitored signals.
Then, $(C,i)$ is \emph{susceptible to a glitch (of width $w$) at signal $s \in \calI \cup \calL \cup \calO$ at time $t$}, if there exists a signal $m \in M$ and a time $t'$ such that in the execution, induced by the circuit $C$ and the input signal traces $i$ and with a glitch (of width $w$) at signal $s$ and time $t$, it is $m(t') = \meta$.

Revisiting the example of the linear pipeline, and letting $M = \{{\tt c1}, {\tt c3}\}$ be the set of monitored signals,
we have that the pipeline with its input are susceptible to a glitch (of width $0.1$) at signal {\tt c2} at time $10$, but not at time $22$ (see Figures \ref{fig:trace-glitch} and \ref{fig:trace-glitch-ii}).

This directly leads to the question of the sensitivity windows, i.e., the times when a circuit with an input is susceptible and when not.
Related, if combined with a probability measure on faults occurring at these times, one may ask how likely a transient fault is to cause a glitch at a monitored signal.
We address both questions in the following.


\subsection{Equivalence of transient faults}

While the previous tool allows one to sample the susceptibility at particular times, such an approach has several drawbacks: (i) it is time consuming to generate such sweeps and (ii) small susceptible windows may be overlooked.

In the following we present an alternative approach that relies on showing the equivalence between certain transient faults.
We begin the analysis with a definition.

We say \emph{signal $s$ has a pulse at time $t$ of width $w > 0$} if $s$ changes value at time $t$, remains constant within time $[t,t+w)$, and changes value at time $t+w$.
A $v$-pulse, with $v \in \BX$, is a pulse that has value $v$ during $[t,t+w)$.
We speak of a \emph{transient fault} as an $\meta$-pulse that has width of at most some predefined small $\gamma > 0$.

We are now ready to show a monotonicity property of the value $\meta$ in executions: If transient faults are added to a circuit's execution, the resulting execution differs from the original one at most by turning Boolean values into $\meta$.
For example, it cannot differ by a shifted 0-1 transition.

\begin{thm}[Monotonicity of $\meta$]\label{thm:monotonicity}
Let $C$ be a circuit and $i$ be input traces.
Let $e$ be the execution induced by circuit $C$
  and input traces $i$,
  and $e'$ the execution induced by circuit $C$
  and input $i$ in presence of transient faults.
Then for all signals $s$ and times $t$,
if $s(t) \in \IB$ in~$e'$, then~$s(t)$
  is identical in~$e$ and~$e'$.
\end{thm}
\begin{proof}
Assume by means of contradiction that the statement does not hold and let $t$ be the smallest time at
which executions~$e$ and~$e'$ do not fulfill the theorem's statement.
Then there is a signal $s$ such that $s(t) = b \in \IB$
  in execution $e'$ and $s(t) = v \neq b$ in execution~$e$.
We distinguish between two cases for value~$v$:

\medskip

Case $v = \meta$.
If so, in execution $e$, signal $s$ was set to $\meta$
  at some time $\tau \leq t$, and not set again
  within $[\tau,t]$.
By minimality of $t$, and the theorem's statement,
  $s$ was also set to $\meta$ in $e'$ at time $\tau$ (or earlier).
It follows that in execution $e'$,
  signal $s$ was set to $b$ within $(\tau,t]$.
This implies that a rule with guard $G$ and action $s = b$ was triggered at a time before $t$, and thus
$G$ was true in execution $e'$.
By minimality of $t$ and the theorem's statement,
  $G$ must have been also true in $e$, resulting in the
  same action being scheduled also in $e$;
  a contradiction to the assumption that $v = \meta$.

\medskip

Case $v = \neg b$.
If so, $s$ was set via two different rules in~$e$ and~$e'$ and not set to another value until time $t$.
This implies that mutually exclusive guards have evaluated to $1$ in $e$ and $e'$ before time $t$; a contradiction to the minimality of $t$ in combination with the theorem's statement.

\medskip

The theorem's statement follows in both cases.
\end{proof}

We next define time intervals that play a central role in a circuit's behavior in presence of transient faults.

Given a circuit $C$ and an execution $e$ of $C$,
the \emph{set of value switching times}, $V_{C}(e)$, is the set of times $\tau_0 = 0, \tau_1, \dots$ at which a signal in execution $e$ switches value.
A \emph{value region of execution $e$} is an interval $[t,t') \subset \IR$, where $t,t'$ are consecutive value switching times of execution $e$.
A \emph{postfix of a value region $[t,t')$} is a (potentially empty) interval $[t'',t') \subseteq [t,t')$.


\begin{thm}\label{thm:main}
Let $C$ be a circuit,
$i$ be input traces,
$\gamma > 0$ the width of a transient fault,
and $\varepsilon > 0$ the propagation delay of
  value $\meta$.
Let $e$ be the execution induced by circuit $C$
  and input traces~$i$.
  
Then, for a signal~$s$ of the circuit, 
  and a value region $R$ of execution $e$,
  the set $\Sigma_s(R)$ of times $t \in R$ such that $(C,i)$ is susceptible to a transient fault (of width $\gamma$) at signal $s$ at time $t \in R$ converges to a postfix of $R$ as $\varepsilon \to 0$ and $\gamma \to 0$.
\end{thm}

Informally this means that every value region can be split into two intervals per signal: the left part of the region that contains the times at which the circuit
is not susceptible, and the right part where it is susceptible to faults.
Both parts/intervals can be empty.

\paragraph{Proof}
In the following fix $C$, $i$, $\gamma$, $\varepsilon$, execution $e$, signal $s$, and value region $R$ of execution $e$.
We first show a monotonicity property within a value region.

\begin{lem}\label{lem:tau1M-to-tau2M}
Let $R = [t,t')$ be a value region of execution $e$ and
  $s$ a signal.
Further, let $e_1$ and $e_2$ be executions of $C$
  with the same input traces as $e$, but with
  $e_1$ additionally having transient faults within $R$ at $s$ up to some time $\tau_1 \in R$ and $e_2$
  having at least one transient at a time $\tau_2 \in R$ at $s$, where $\tau_1 \leq \tau_2 \leq t' - |C| \varepsilon - \gamma$.

Then for all value regions $R'$ of execution $e$ and all signals $s'$, if~$s'$ has value $\meta$ at some time within $R'$ in execution $e_1$, then it does so at some time within $R'$ in execution $e_2$.
\end{lem}
\begin{proof}
Within the same value region, both transient faults
  cause the same signals to become $\meta$, given that $\tau_1,\tau_2$ are sufficiently far from the value region's boundary $t'$ to allow for propagation of $\meta$ with delay $\varepsilon$ (at most $|C|\varepsilon$ time):
  this follows from the fact that the circuit's signal
  values and set of scheduled actions are identical at the start of the first transient in $e_1$ and in $e_2$.

Further, a signal with value $\meta$ remains so unless it
  is set again to a Boolean value by a production rule.
This can only happen by its guard becoming true right after a transient fault.
Since, $\tau_1 \leq \tau_2$, and both times are in the same
  value region, any event scheduled (and not canceled) after the transient fault at $\tau_2$ must also be scheduled (and not canceled) after the transient faults that occur until time $\tau_1$:
  signals have the same Boolean values and remain stable for a longer time in $e_1$ than in $e_2$.

The argument is inductively repeated for each subsequent value region of execution $e$.
\end{proof}

We are now in the position to show the section's main result.

\begin{proof}[Proof of Theorem \ref{thm:main}]
Letting $\varepsilon \to 0$ and $\gamma \to 0$, we have from Lemma \ref{lem:tau1M-to-tau2M} that if a transient fault at a signal $s$ at a time $\tau_1 \in R$
  that causes $\meta$ at a signal $s'$ then a transient fault at a signal $s$ at a time $\tau_2 \in R$, where $\tau_1 \leq \tau_2$, also causes $s'$ to become $\meta$ at some point in time.
The theorem's statement then follows from the definition of a postfix of $R$.
\end{proof}

\subsection{Automated computation of susceptible regions}\label{sec:tool}

Theorem \ref{thm:main} directly leads to an algorithm that marks all
  sensitivity windows, i.e., susceptible times, within an execution prefix:
  for each non-output signal $s$, and for each value region $R$, it finds
  per bisection of repeatedly inserting transient faults
  the boundary of non-susceptible times (on the left within $R$)
  and susceptible times (on the right within $R$).
We have implemented the algorithm in Python:
  %(add link when available)
  given a circuit, input traces, the set of monitored signals,
  as well as a time $T$ until which an execution
  prefix is to be generated, it outputs a figure with all
  susceptible windows highlighted in blue as well as the percentage of
  the length of the susceptible windows in the execution prefix
  (by default excluding the monitored signals, but with the possibility to include them).
This value corresponds to the probability of a transient fault causing
  an $\meta$ value at a monitored signal, i.e., \emph{the probability to fail} $P(\text{fail})$, given a uniform distribution
  of a single transient on all times in the execution prefix and
  on all signals that are not monitored signals (by default; alternatively on all signals).
Clearly, though, the uniform distribution can be easily
  replaced by a more involved distribution.
Towards this direction, the tool also outputs the probability per signal.
This allows one to compute a weighted average, e.g., depending on driver strength
  or shielding at certain signals.
Figure \ref{fig:trace-check} shows the tool's output for the previous example
  of the 3-stage linear pipeline with sensitivity windows marked in blue.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/trace-muller-linear-check.pdf}
    \caption{Execution prefix of linear 3-stage pipeline until time $T=32$ with sensitivity windows marked in blue. Monitored signals are {\tt c1} and {\tt c3}.
    Probability to fail $P(\text{fail}) = 0.54375$.}
    \label{fig:trace-check}
\end{figure}

A fault occurring at any point of the blue sensitivity windows will drive one (or more) of the circuit's monitored signals to~$\meta$. A fault hitting any other region (excluding the monitored signals) will be masked and will not reach the monitored signals of the circuits.

\subsection{Comparison of fault-tolerance depending on speed}\label{sec:linear}

We next illustrate the use of our tool at several example studies.
To start with, let us investigate the fault masking potential of the example linear pipeline while varying the source and sink latencies.
Inverter delays are symmetric and normalized to~1 and Muller C-element latencies are set to~5 inverter delays.
The results are shown in Figure~\ref{fig:muller3_sweep_3dplot}, with detail sweeps (cuts) in Figures~\ref{fig:muller3-sweepSink} and~\ref{fig:muller3-sweepSource}. 
The length of the execution prefix has been chosen sufficiently high, to account for a sufficiently long time for $P(\text{fail})$ to be dominated by the periodic operation of the circuit rather than the initial transient phase: $T=500$ in the overview plot and $T=1000$ in the detailed sweeps.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/muller3_sweep_3dplot.png}
    \caption{Influence of source and sink speed on the probability to fail $P(\text{fail})$. Linear 3-stage pipeline with delays as follows: 1 (INV), 5 (MCE), varying source and sink delays. $T = 500$.}
    \label{fig:muller3_sweep_3dplot}
\end{figure}
\vspace{-0.7cm}

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/muller3_sweepSink.png}
    \caption{Influence of sink speed on the probability to fail $P(\text{fail})$. Linear 3-stage pipeline with delays as follows: 1 (INV), 5 (MCE), 4 different source delays, varying sink delay. $T=1000$.}
    \label{fig:muller3-sweepSink}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/muller3_sweepSource.png}
    \caption{Influence of source speed on the probability to fail $P(\text{fail})$. Linear 3-stage pipeline with delays as follows: 1 (INV), 5 (MCE), 4 different sink delays, varying source delay. $T=1000$.}
    \label{fig:muller3-sweepSource}
\end{figure}


Figure~\ref{fig:muller3_sweep_3dplot} shows an overview of the behavior of the circuit under a stable environment, be it fast or slow, and how the circuit reacts when there is an unbalance between the speeds of source and sink. The $z$-axis displays the probability of an~$\meta$ value presence at any of the monitored signals. The $x$ and $y$-axes represent the speeds (latencies) of sink and source, respectively, in time units. Note that the pattern of the plot is best visualized when having the latter axis inverted.
The diagonal of the frame where both sink and source latencies are equal to 1 (fast) to where they are both 25 (slow) represents the stable/balanced environment, i.e., the source provides \emph{tokens} with the same speed as the sink provides the \emph{acknowledgment}. The figure indicates that $P(\text{fail})$ is high when the environment is stable and fast, and decreases as it gets stable and slow. When the environment is balanced, the MCEs in the circuit are not waiting for either the \emph{data} or the \emph{ack} signals; both are supplied within short intervals of time of each other. Recall that the waiting phases are those where the MCE operates in the vulnerable storage mode (inputs mismatching), so reducing the waiting period decreases $P(\text{fail})$.

The environment imbalance is divided further into 2 modes of operation. On the right side (for relatively low source delay) of the figure, the circuit is operating in \emph{bubble-limited} mode, where the sink's response to the source's new tokens is slow. On the left half of the figure, the sink's activity is faster than the source's, driving it in \emph{token-limited} mode.

 The vulnerability of the bubble-limited mode can be seen more clearly in Figure~\ref{fig:muller3-sweepSource}; this is where the system is most prone to failure. The probability $P(\text{fail})$ varies from around 60-80\%, where it reaches the maximum when the sink delay is equal to 22 while source delay is 1 (maximum imbalance).
Similarly, the token-limited mode falls near the sink latency of~1 in Figure~\ref{fig:muller3-sweepSink}, varying from around 40-60\%.
The latter figures show several cross-sections of the 3D plot from Figure~\ref{fig:muller3_sweep_3dplot}. In addition to mapping the token-limited and the bubble-limited areas to these 2 graphs, we can also spot the points belonging to the \emph{balanced environment} diagonal in the frame in Figure~\ref{fig:muller3_sweep_3dplot}.
These points are where the abrupt changes of behavior of each line occur, and consequently we can pinpoint where one region of the mode of operation ends and the other starts.

Finally, Figure \ref{fig:p_per_sig} shows the fault probabilities per signal of the
  linear pipeline as reported by our tool for varying source and sink delays (fast, normal, and slow). It allows us to give a more detailed interpretation of our observations in Figure~\ref{fig:muller3-sweepSink}. 
The probabilities for the monitored signals {\tt c1} and {\tt c3} are always~$1.0$ as
  by definition of the fault probabilities.

Interestingly, {\tt c2} has a high fault probability, too. For fast sink this can be explained as follows: {\tt MCE3} spends most of the time in the vulnerable storage mode, waiting for a transition on {\tt c2}. As soon as one occurs, it triggers a transition on {\tt c3} which, after the short sink delay, puts {\tt MCE3} back to storage mode. This only leaves a very short time window where {\tt MCE3} is masking faults at {\tt c2}. The enable signals, in turn, are only vulnerable during those short windows, thus showing low fault probability, especially when the source delay is high, see subplot ``source:25, sink:1''.

Recall that we have chosen a relatively large switching delay for the MCE, and our pessimistic model assumes the MCE to be vulnerable during the whole switching duration. This explains why in general $P(\text{fail})$ increases for faster operation speed: the proportion of the sensitive MCE switching phase increases. This can be most directly observed for the balanced cases.

For the other imbalanced extreme with ``source:1, sink:25'' we observe high fault probability for the enable signals. This is not surprising, since now the MCEs spend most time waiting for transitions on these signals. A fault probability of $1.0$ for {\tt c1} and {\tt c3} is also unsurprising, due to our definitions, as mentioned already. Quite unexpected, on the first glance, is the fact that {\tt c2} again shows high fault probability, even though we can assume good masking by {\tt MCE3} for that input. The reason here is that, via {\tt INV2}, faults from {\tt c2} directly propagate to {\tt en1} which is known to have low protection by masking. As a result, we see a generally high fault probability in this mode.
 
\begin{figure}
    \centering
    \includegraphics[width=0.32\columnwidth]{figs/muller3-bar-1-1.pdf}
    \includegraphics[width=0.32\columnwidth]{figs/muller3-bar-1-4.pdf}
    \includegraphics[width=0.32\columnwidth]{figs/muller3-bar-1-25.pdf}\\
    \vspace{0.2cm}
    \includegraphics[width=0.32\columnwidth]{figs/muller3-bar-4-1.pdf}
    \includegraphics[width=0.32\columnwidth]{figs/muller3-bar-4-4.pdf}
    \includegraphics[width=0.32\columnwidth]{figs/muller3-bar-4-25.pdf}\\
    \vspace{0.2cm}
    \includegraphics[width=0.32\columnwidth]{figs/muller3-bar-25-1.pdf}
    \includegraphics[width=0.32\columnwidth]{figs/muller3-bar-25-4.pdf}
    \includegraphics[width=0.32\columnwidth]{figs/muller3-bar-25-25.pdf}
    \caption{Circuit fault probability per signal of the linear 3-stage pipeline with varying source and sink delays. Delays: 1 (INV), 5 (MCE), and 3 different source and sink delays indicated in the figure. $T=1000$.}
    \label{fig:p_per_sig}
\end{figure}

\subsection{Comparison of fault-tolerance for Muller pipeline rings}\label{sec:ring}

Another very common asynchronous pipeline
construct is rings. We interpret the pipeline operation to implement a 4-phase QDI protocol in the following. Shown in Figure~\ref{fig:muller3_ring} is a 3-stage Muller pipeline where one \emph{data token}, one \emph{spacer}, and one \emph{bubble} keep oscillating. Note that when using the term \emph{token} on its own, it encompasses a data token along with a spacer, so 1 token means one of each.
It is possible to also interpret a token to follow the 2-phase communication protocol, and in this case we would double this count.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/muller3_ring.pdf}
    \caption{Ring 3-stage pipeline. The delays are set to 1 (INV) and 5 (MCE).}
    \label{fig:muller3_ring}
\end{figure}

In order to study the resilience of a Muller ring w.r.t. the activity inside the pipeline 
we need to vary its \emph{occupancy}, i.e., the number of data items revolving in it \cite{gill2008performance}. We need at least one bubble in the ring, regardless of how many stages constitute it. When the number of data items in the ring is small, the other stages of the pipeline will be filled with \emph{holes}. When there is more holes than data, the pipeline is said to be \emph{data-limited}; when there is more data than holes, it is said to be \emph{hole-limited} \cite{gill2008performance}. These operation modes correspond to the token-limited and bubble-limited modes, respectively, in the linear Muller pipeline.

We use our tool to study the effect of varying the ring occupancy, by building the ring with a different number of stages and changing the token count. As this is a Muller pipeline, each C-element needs alternating input sequences to be able to transition from $0$ to $1$ every stage.

In order to keep the pipeline running and avoid deadlock, the process of correctly initializing the stages of the ring is crucial. As previously mentioned, there must be at least one bubble in the pipeline. For each combination of tokens and stages, we calculate the number of bubbles needed and we fill the pipeline in the following manner:
\begin{itemize}
    \item If the number of bubbles is much larger than the number of tokens, we start by filling the pipeline with bubbles, and insert tokens equally paced from one another.
    \item If the number of bubbles is much lower than the number of tokens, we start by inserting tokens, and spread the bubbles in between.
    \item If there is only one bubble, it doesn't matter where it is inserted. Same if there is only one token.
    \item A token is always inserted as a data token and a spacer that are not separated by a bubble.
\end{itemize}

The results for these settings are shown in Figure~\ref{fig:muller-ring-sweepTokens}. The first point of each line (from the left) represents the maximum number of tokens allowed for the corresponding number of stages (recall that this count represents, in fact, a data token and a spacer). The top left region represents the bubble-limited operation mode, where one can clearly see that $P(\text{fail})$ gets higher. 
The increasing number of stages also seems to play a role in this trend. From what we have previously observed 
 we can conjecture that this is because an idle (waiting) stage (MCE) has the highest fault probability, while one that processes a token/transition is more resilient. By adding stages while keeping the number of tokens constant, we add idle stages -- consequently $P(\text{fail})$ increases.

As we move to the edges of the token-limited region where the number of bubbles largely exceeds the number of tokens,  $P(\text{fail})$ converges to a steady percentage of approximately 45\%. 


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/muller-ring-sweepTokens.png}
    \caption{Influence of number of tokens and stages on the probability to fail $P(\text{fail})$. Ring pipeline with varying number of stages and tokens. Delays as follows: 1 (INV), 5 (MCE). $T=500$. A token encompasses a data token along with a spacer, so 1 token means one of each, following the 4-phase communication protocol. In the case of a 2-phase protocol, this token count is doubled.}
    \label{fig:muller-ring-sweepTokens}
\end{figure}

Finally, we compare throughput and probability to fail as a function of the same ring pipeline with a varying number of (4-phase) tokens.
It has been previously observed \cite{williams1987self,gill2008performance} that the throughput as a function of tokens behaves as a canopy plot: it is low for few number of tokens (token-limited), high in the middle, and low for high numbers of tokens (bubble-limited).
Figure \ref{fig:canopy} compares this behavior with the failure probability as determined by our tool for execution prefixes of length $T = 200$.
While the canopy diagram suggests that 4 and 5 tokens yield optimum throughput, the failure probability favors a lower token count. So for maximum performance the better choice would be 4 tokens.
This result is not general and seems to depend on the design choices, but the general strategy should be to also consider $P(\text{fail})$ in the system design.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/canopy.png}
    \caption{Influence of number of tokens on the probability to fail $P(\text{fail})$ and the throughput (in 4-phase tokens per {\tt INV} delay passing {\tt MCE1}). Ring pipeline with 20 stages and varying number of tokens. Delays as follows: 1 (INV), 5 (MCE). $T=200$. A token encompasses a data token along with a spacer (4-phase; for 2-phase interpretation multiply by factor 2).}
    \label{fig:canopy}
\end{figure}


\subsection{Multi-bit QDI designs}

To demonstrate the ability of our tool to handle larger, multi-bit designs,
  we ran the algorithm to determine susceptible windows on execution prefixes
  of duration $T=500$.
As circuits under test we used 2-bit and 4-bit versions of the previously analyzed
  linear and ring pipeline.
Our tool reported the following values for $P(\text{fail})$:
(i) the 3-stage linear pipeline with 2-bit resulted in $0.22$ and with 4-bit in $0.10$.
(ii) the 3-stage ring pipeline with 2-bit resulted in $0.22$ and with 4-bit in $0.16$.
The observed decrease of $P(\text{fail})$ for higher pipeline width is as expected from literature: while the last rail to switch is the most critical one, the faster bits are less critical and hence contribute to lowering the overall fault probability -- with growing impact for increasing bit number.

All results were obtained within minutes on a MacBook Pro (M2, 2022) with 24~GB RAM.

%\end{document}