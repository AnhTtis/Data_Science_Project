

\section{Review of ML-based ride-hailing planning}
\label{sec:review}
\revise{In this section, we review matching, repositioning, and joint matching and repositioning in Sec.~\ref{sec:review-matching} Sec.~\ref{sec:review-repositioning}, and Sec.~\ref{sec:review-joint}, respectively.}
In each part, we discuss the collective and the distributed strategy separately.
Fig.~\ref{fig:review-outline} gives an outline of the review.

\begin{figure*}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{figs/survey-taxonomy.pdf}
	\caption{\revise{A taxonomy of the ride-hailing planning literature. %is summarized.
	In each category, we discuss three works as representative examples.}}
	\label{fig:review-outline}
\end{figure*}

\subsection{Matching}
\label{sec:review-matching}

\subsubsection{Collective Matching}
\revise{RL is a promising technique for solving the matching problem.
Chen et al.~\cite{chen2020order} propose an RL-based solution in which
a deep evaluation network, which is a plain feed-forward neural network, is used to calculate a score for each pair of driver and rider based on the predicted detour distance, vehicle's seat utilization rate, and profit achieved if they get matched.
For each new ride request, the vehicle with the highest score will be assigned to serve the rider.
When the trip of the ride request is finished, the observed reward, i.e., the sum of the increased profit of the driver and, if any, the reduced cost of the rider through sharing the ride with others, is used to guide the learning process of the deep evaluation network.
Agussurja et al.~\cite{agussurja2019state} formulate the matching problem as a two-stage planning process.
In the first stage, ride requests to be scheduled are selected from all the unserved ones, the problem of which is modeled as a Markov Decision Process.
An approximated value iteration algorithm is used to learn the value function for the matching actions.
In the second stage, the final matching decision is made between the selected ride requests and all vehicles based on the learned value function.
\revise{Kullman et al.~\cite{kullman2022dynamic} apply deep RL to develop matching policies whose decisions leverage the Q-value approximations learned by deep neural networks.}
Multi-hop ride-hailing can improve the efficiency of a ride-hailing system.
To find the transfer points for each transferring trip in the multi-hop ride-hailing service, Xu et al.~\cite{xu2020highly} use a multi-layer feed-forward network to predict the reachable areas of vehicles, based on which the search space of possible vehicle pairs and transfer points for transferring riders is pruned.
In this way, the transfer points searching process can be more efficient.
	Wang et al.~\cite{wang2023optimization} also consider the scenario where riders are allowed to transfer between vehicles.
	They leverage RL to learn a policy that estimates the values of all the vehicles, which are then used to compute the optimal matching decisions by integer-linear programming.
The lengths of the time-intervals between the matching decisions can have critical impact in the matching outcomes.
	Specifically, the efficiency of matching may be improved substantially if the matching is delayed by adaptively adjusting the matching time-intervals according to the real-time situation of the riders and drivers.
	Wang et al.~\cite{wang2019adaptive} find that, if riders are willing to wait for a certain amount of time even if there are available vehicles that can serve them right away, the ride-hailing system can achieve better results, for example, in terms of the total vehicle miles traveled.
	In their solution, 
	 they propose to use an RL policy to decide for each rider, at each time step, whether to conduct matching for her/him, or %leave her/him alone and
	wait for the next time step. 
	Similarly, Qin et al.~\cite{qin2021optimizing} leverage RL in solving the ride-hailing matching problem with dynamic matching time-intervals.}

\revise{Clustering techniques are frequently used in ride-hailing planning.
Hong et al.~\cite{hong2017commuter} propose to use a density-based clustering algorithm, specifically DBSCAN \cite{parimala2011survey}, to identify riders that share similar itineraries based on their historical traveling trajectories. 
To alleviate the computational overhead caused by the large number of distance queries in the matching process, Zhang et al.~\cite{zheng2018order} propose a new clustering algorithm that groups the geographical locations in the road network into different clusters.
Then, the distance between any two nodes is approximated by the distance between the centers of the clusters they belong to. 
Shen et al.~\cite{shen2019roo} propose a spatial-temporal distance metric that measures the similarity of each pair of ride requests.
The ride requests are grouped by a clustering process based on the proposed distance metric.
Then, shared-rides are computed within each group of ride requests.
Another clustering algorithm is proposed by \cite{trasarti2011mining} to extract the mobility profiles from riders' and drivers' historical itineraries.
The matching between riders and drivers is determined based on the similarities between their profiles.}


\revise{An increasing number of collective matching solutions leverage various other ML techniques in planning.
Most of them take social factors of drivers and riders into consideration \cite{mitropoulos2021systematic}.
To mitigate the social barriers in the ride-hailing process, especially in shared-rides, Yatnalkar et al.~\cite{yatnalkar2020enhanced} and Narman et al.~\cite{narman2021enhanced} use Support Vector Machine (SVM) to predict the user social types, e.g., chatty, safety, or punctuality, based on their registered user characteristics.
Riders with similar social characteristics %are more likely
would be more willing to share a trip.
%on their closest available vehicle.
Levinger et al.~\cite{levinger2020human} use a feed-forward neural network to predict rider satisfaction levels according to their profile and trip information.
They proposed a stochastic algorithm to compute the matching decision with rider satisfaction level maximization as the objective.
Montazery and Wilson \cite{montazery2016learning, montazery2018new} propose to take into account the user preference in evaluating the weight (benefit) of the matching between each pair of rider and driver, which is given by their proposed support vector machine-based score function.
With the value calculated, the final matching can be obtained by solving an optimization problem in which the sum of the weights of those matched pairs is maximized.
Tang et al.~\cite{tang2020efficient} model various types of information (e.g., driver, rider, travel time, and activity) and their relationships within a ride-hailing system using a Heterogeneous Information Network (HIN) \cite{sun2012mining}.
Each driver or rider is projected to a multi-dimensional embedding (vector) using the skip-gram model \cite{mikolov2013efficient}.
Moreover, the skip-gram is conducted on node sequences obtained by meta path-based random walks originating from the corresponding node within the HIN \cite{dong2017metapath2vec}.
The cosine similarity between the embeddings of each driver-rider pair is then used to identify possible matching.
Zhang et al.~\cite{zhang2017taxi} consider a scenario where each rider is assigned to multiple drivers (to improve the order answer rate), and riders are free from having to enter the details of destinations (to improve the user experience).
They first leverage historical data to model the probability distribution of destinations of each rider based on his/her departure time and location with Bayesian rules, which is followed by predicting the acceptance probability between the rider and available drivers with logistic regression \cite{friedman2001elements} and gradient boosted decision tree \cite{mason1999boosting}.
They propose a hill climbing-based algorithm to solve the matching problem, which is formulated as an NP-hard combinatorial optimization with maximizing the success rate of matching as the objective.
Schleibaum and M{\"u}ller \cite{schleibaum2020human} advocate taking the determinants of user satisfaction and explainable matching decisions into consideration.
One of their future studies is to find out whether increasing the explainability can improve user satisfaction level or not.}

\revise{It is worth mentioning that many ML-based collective matching strategies take advantage of the Kuhn-Munkres (KM) bipartite matching algorithm as a component of their decision-making pipelines \cite{jonker1986improving}.
Drivers and riders are usually regarded as the two sets of vertices in the target bipartite graph.
To guide the matching between ride requests and ride offers,  Guo et al.~\cite{guo2020spatiotemporal} propose spatial-temporal Thermo, which is used to reflect the demand density of different places and times.
They use Random Forest Regression \cite{breiman2001random} to map multiple features of spatial, temporal, and meteorological dimensions to Thermo.
The weight of each pair of driver and rider in the bipartite graph is estimated by Thermo.
A KM algorithm is then used to calculate the final matching decisions according to the constructed bipartite graph.
Similarly, Xu et al.~\cite{xu2018large} derive their matching decisions using the KM algorithm.
In contrast to \cite{guo2020spatiotemporal}, Xu et al.~\cite{xu2018large} leverage a policy evaluation algorithm to learn a value function which maps each pair of driver and rider to a score.
The KM algorithm calculates the final matching between drivers and riders based on the scores.
Guo and Xu \cite{guo2020deep} also conduct the matching planning using the KM algorithm.
The weight between each pair of driver and rider is obtained from a value function learned by a convolutional neural network-based Double Q-learning (Double DQN) algorithm \cite{van2016deep}.}

\subsubsection{Distributed Matching}
\revise{RL is also a powerful technique for distributed matching \cite{sutton1999reinforcement}.
Gu{\'e}riau and Dusparic \cite{gueriau2018samod} use the Q-learning algorithm to train a policy for each agent (driver) to choose the pickup or rebalancing action based on the environment state, including the status of itself and current distribution of supply and demand.
If pickup action is chosen, then the vehicle will go and pick up the nearest rider.
In their follow-up work \cite{gueriau2020shared}, they extend the method to consider traffic congestion when agents are making decisions.
Wang et al.~\cite{wang2018deep} propose to use the DQN \cite{mnih2015human}, in which a deep neural network is employed to estimate the state-action value function from a single driver's perspective.
Many methods of distributed matching allow the decisions to be determined individually while the matching policy is trained collectively.
For example, De Lima et al.~\cite{de2020efficient} follow the QMIX framework proposed in \cite{rashid2018qmix}, in which the coordinated planning policies are trained by learning a joint action-value function for multiple vehicles and riders aiming at optimizing a global objective.
In the execution process, the matching decision of each vehicle is made in a distributed  manner following its own component in the learned action-value function.
By ensuring the monotonicity of the relationship between the global action-value and the action-value of each passenger, the objectives of distributed planning decisions are ensured to coincide with the centralized decisions during the training process.
Similar to \cite{de2020efficient}, Li et al.~\cite{li2019efficient} adopt the framework where the matching policy is trained in a centralized manner and executed in a distributed manner.
Specifically, they adopt the actor-critic RL framework, where actor and critic are two different networks used to decide and evaluate the action for each driver, respectively.
The coordination among drivers in the matching policy is enabled by the critic network.
It adopts the mean field approximation to model the interactions of drivers by calculating an average on the actions taken by their neighborhoods, which is then considered in the process of evaluating each driver's action.
\revise{In \cite{zhou2019multi}, another centralized training process is proposed, in which a Kullbackâ€“Leibler divergence optimization is used to balance the supply and demand and to enable coordination among the vehicles.}
In the execution phase, each driver chooses an action based on their own action-value functions.}

\revise{Some distributed matching strategies leverage other ML techniques.
They mostly determine the matching decisions based on the similarities between the riders and drivers in ride-hailing.
For example, Bicocchi and Mamei \cite{bicocchi2014investigating} use the bag-of-words model to summarize users' frequently visited places as vector representations, which are then fed to the Latent Dirichlet Allocation (LDA) \cite{blei2003latent} model to identify their patterns of daily travel routine behaviors.
Given a rider or a driver, his/her potential participants of shared-rides can be found by calculating the similarities between his/her daily travel routine and those of the other riders and drivers.
Lasmar et al.~\cite{lasmar2019rsrs} propose to leverage a multi-layer Perceptron model to learn user preferences based on their responses to the questionnaires.
For each rider, a ranking list of potential partners for shared-rides is generated according to the similarities between the predicted preferences of her/him and other riders.}

    
\subsection{Repositioning}
\label{sec:review-repositioning}
\subsubsection{Collective Repositioning}
\revise{Some collective repositioning methods leverage RL techniques.
Ride-hailing repositioning for electric vehicles is studied in \cite{liang2020mobility, tang2020online}, in which the state of charge of the electric vehicles is an important factor to be considered.
Liang et al.~\cite{liang2020mobility} develop a solution method utilizing deep RL combined with binary linear programming to obtain a regional joint planning policy for electric vehicles with their state of charge considered.
Using binary linear programming, each vehicle repositioning action is modeled as a binary decision variable, and its weight in the objective is obtained by the value function learned by the policy iteration method.
Similarly, Tang et al.~\cite{tang2020online} also combine RL with combinatorial optimization, in which the RL learned policy is used to advise decision making in the optimization step.
Liang et al.~\cite{liang2021integrated} adopt temporal-difference (TD) learning to obtain action-value function.
Different from \cite{de2020efficient}, the settings in \cite{liang2021integrated} do not allow factorization of the joint action-value function into individual ones while guaranteeing global maximization.
Thus, they formulate two linear programming instances to collectively find the decisions for the vehicles.
To improve the stability of the training process in RL, Fluri et al.~\cite{fluri2019learning} propose a cascading multi-level learning model.
In this model, the area concerned is split in halves as the number of levels of learning increases.
The policy training process proceeds in a top-down manner, i.e., from less to more fine-grained area partitioning. 
The motivation behind is that the policy trained from a coarse level can serve as guidance to the finer levels, which avoids the instability caused by directly training a policy with a large state size (w.r.t. the number of regions).
Fluri et al.~\cite{fluri2019learning} propose to leverage the Lloyd K-means algorithm \cite{lloyd1982least} to partition the area concerned into multiple smaller regions.
Deng et al.~\cite{deng2020multi} leverage the Proximal Policy Optimization algorithm (PPO) \cite{schulman2017proximal} to learn the joint repositioning policy for vehicles, in which the value- and policy-function are approximated by neural networks.
Shi et al.~\cite{shi2019optimal} use Deep Deterministic Policy Gradient (DDPG) \cite{silver2014deterministic} to learn the grid-based multiple vehicles repositioning policy with the objective of total profits maximization.
\revise{In \cite{shou2020reward}, a mean-filed multi-agent RL approach is leveraged to collectively relocate the vehicles in ride-hailing.}}


%In collective repositioning, 
\revise{Some other collective repositioning solutions leverage various ML techniques to predict future information of a ride-hailing system, which plays an important role in guiding the platforms to make better repositioning decisions \cite{chen2022h}.
Riley et al.~\cite{riley2020real} leverage Vector autoregression to forecast the future demand from region to region.
The predicted demand and current system status are then fed into two mixed-integer programming instances to find the desired distribution of vehicles and the assignment of vehicles to regions, respectively.
Iglesias et al.~\cite{iglesias2018data} use a Long Short-Term Memory (LSTM) neural network to predict the future ride requests for each pair of origin and destination within a certain time period.
The predicted information is then used as input to their proposed mixed-integer linear programming instance, which is solved to find the optimal rebalancing actions.
Xu et al.~\cite{xu2018taxi} use two LSTM-based and Mixture Density Network (MDN)-based models to predict the distributions of origins and destinations of future requests, respectively.
With a prediction on the distributions, the repositioning decisions are then obtained by solving a mixed-integer programming problem with total idle driving distance minimization as the objective.
Cheng et al.~\cite{cheng18taxis} leverage a multilevel logistic regression model to predict the likelihood of ride requests occurring at different times and places.
The online repositioning planning decisions of drivers are obtained by leveraging a centralized multi-period stochastic optimization model with both the real-time and predicted demand considered.
Li et al.~\cite{li2020data} and Gao et al.~\cite{gao2020learning} formulate the repositioning task as a two-stage stochastic programming problem.
The source of the stochasticity is the underlying uncertainty of the future demands, the probability distribution of which is obtained by kernel density estimation and a deep learning model combining the LSTM and MDN in \cite{li2020data} and \cite{gao2020learning}, respectively.
Pouls et al.~\cite{pouls2020idle} propose a forecast-driven repositioning solution framework, the core of which is a mixed-integer programming problem with the demand predictions as inputs.
Moreover, it is solved by an off-the-shelf solver called Gurobi \cite{gurobi}.
Note that, in practice, not all planning decisions can be successfully executed by the drivers at the end.
Xu et al.~\cite{xu2020recommender} take the first step to predict the failure possibility of repositioning tasks in the decision-making process, including situations where drivers disobey the planning or end up being unmatched for an unexpectedly long time even though they follow the repositioning planning decisions accordingly.
In the latter case, drivers will be compensated.
The failure rate of each repositioning task is predicted by XGBoost \cite{chen2016xgboost} with both driver- and environment-related features as inputs.
\revise{The problem of multi-vehicle collaboration optimization aiming at maximizing the platform's profit is converted into a minimum cost flow problem, which is solved by an off-the-shelf method called GNU Linear Programming Kit (GLPK) \cite{makhorin2008glpk}. }}


\subsubsection{Distributed Repositioning}
Geographical regions or grids (i.e., abstracts of individual locations) are usually used to model the road networks in the problem of ride-hailing repositioning.
Different from most of the repositioning methods (e.g., \cite{lin2018efficient, riley2020real, ke2019optimizing, li2019efficient, zhou2019multi}) in which the region of interest is divided into predefined and static geographic zones, Castagna et al.~\cite{castagna2020demand, castagna2021demand} leverage the Expectation-Maximization clustering algorithm to derive zones for rebalancing vehicles in an online manner.
They leverage the Proximal Policy Optimization algorithm (PPO) \cite{schulman2017proximal} to train a policy for each vehicle to decide whether to make a pick-up, drop-off, or repositioning action.
Specifically, similar to \cite{tang2021value}, the repositioning destination is also sampled from a probability distribution over all potential positions, which is determined by the number of unserved requests.
Different from \cite{castagna2020demand, castagna2021demand}, Verma et al.~\cite{verma2017augmenting} propose an iterative method to dynamically split the zones based on their expected revenue (Q-values).
The iterative splitting process does not terminate until the historical data is exhausted for the Q-values learning.
\revise{Different from most of the works that model the drivers as agents, Jin et al.~\cite{jin2019coride} regard each geographical region as an agent.}
By hierarchically partitioning the target areas into regions with different granularities, they perform hierarchical RL where the multi-head attention mechanism is used to capture the impacts among the neighboring agents.
Guo et al.~\cite{guo2021multi} try various methods (e.g., Support Vector Regression, Random Forest Regression, and k-Nearest Neighbors regression) to predict future demand density, which is then used to evaluate each region for their spatial-temporal value.
Each available vehicle chooses to stay still or relocate to a neighbor region in a probabilistic manner based on their spatial-temporal values, which can help avoid over-saturation of supply.
In \cite{provoostdemandprop}, the region of interest is represented as a graph.
They build two neural networks to predict the demand on vertices and the passenger flows on edges, respectively.
The proposed repositioning algorithm aims at satisfying the demand on edges in the decreasing order with the nearest vehicles found by backward traversing.

\revise{However, in spite of the various grid-based methods as discussed in most of the related works mentioned above, e.g., \cite{lin2018efficient, guo2021multi}, Jiao et al.~\cite{jiao20deep, jiao2021real} argue that grid-based repositioning policies are not satisfactory in practice because of the excessively-simplified and overlooked non-stationarity in the environment caused by the dynamic environment and the large number of vehicles when coarse-grained region-wise decisions are considered.}
They put forward the process of carrying out repositioning %algorithms
in industrial production by combining offline learning, i.e., batch RL, and online planning stages, i.e., decision-time planning \cite{sutton1999reinforcement}.
To counter the issues of coarse-grained decisions, Kim and Kim \cite{kim2020optimizing} uses a graph to model the road networks which is more realistic.
They build a Graph Neural Network to predict the future demands.
The repositioning destination of each driver is decided greedily based on a function of the predicted demand, the number of excessive vehicles, and the distance information to each candidate position.

\revise{Some other works also spend special effort on tackling the non-stationarity.
With the observation that the actions of drivers are independent (based on self interests), Chaudhari et al.~\cite{chaudhari2020learn} propose a vanilla RL framework where each driver, based on a probabilistic value denoting the extent to which coordination is needed, stochastically chooses to perform an action guided by the independent or coordinated policy.
Note that, although vehicles execute repositioning decisions sequentially in this solution framework, coordination in the latter policy is explicitly considered by solving a minimum cost flow problem for the optimal rebalancing flow of vehicles among all the regions (which is similar to \cite{xu2020recommender}).
In addition, the independence between different repositioning policies learned by the drivers concurrently also contributes to the non-stationarity of the environment.
In this regard, Verma et al.~\cite{verma2019entropy} propose a method for each driver to learn the information of other vehicles in order to make a better planning decision.}
%Concretely,
The principle of maximum entropy \cite{jaynes1957information} is leveraged to improve the predictability of the distribution of drivers even with only limited knowledge available, e.g., the local density of supply.
To tackle the non-stationary challenge in online ride-hailing as well as the catastrophic forgetting of RL \cite{kemker2018measuring}, Haliem et al.~\cite{haliem2020adapool, haliem2021adapool} propose to learn multiple repositioning policies to deal with different contexts of environments (e.g., peak/non-peak hours and weekends/weekdays).
When changes in the distribution of experiences are identified by their proposed change point detection algorithm, switching among those different policies is enabled so as to enhance adaptability to the dynamic environment.
Lei et al.~\cite{lei2019optimal} define the concept of stochastic relocation matrix.
The element in the $i$-th row and the $j$-th column within the matrix represents the probability that an empty vehicle located in the $i$-th region should relocate to the $j$-th region.
\revise{To circumvent the curse of dimensionality, they leverage low-rank approximation to project the original matrix onto a low-dimensional vector.}
They propose a deep convolution-LSTM model to learn how to predict the approximation vector based on the system status.
To alleviate the instability of the state-value function approximator caused by the large scale of its states, Tang et al.~\cite{tang2019deep} propose to bound its outputs by regularizing its worst-case variation w.r.t. %any
changes in its inputs (i.e., states).
Transfer learning proposed in \cite{wang2018deep} is applied to increase the adaptability of the trained model across different cities.

\revise{Besides the traditional ML techniques discussed above, RL, being another well-known technique for decision making in non-stationary environments, has been a key technology in distributed repositioning \cite{khetarpal2022towards,xie2021deep,mao2021near}.}
\revise{Liu et al.~\cite{liu2022deep} propose a single-agent deep RL approach which relocates vacant vehicles to regions with a large demand gap in advance.}
Nguyen et al.~\cite{nguyen2018policy} propose to use the RL framework to train a homogeneous repositioning policy for all agents, i.e., vehicles.
\revise{The policy is trained in a centralized manner with collective behaviors of drivers considered while executing in a distributed manner.}
He and Shin \cite{he2019spatio} leverage Double DQN with their proposed spatial-temporal capsule-based neural network as the state-action value approximator.
The inputs of the network proposed include the location of the vehicle to be relocated, distribution of other vehicles and riders, ride preferences, and some external factors that have impacts on supply and demand, e.g., weather conditions and holiday events.
With all those information processed, the estimated value for each candidate position given the current state of the target vehicle is obtained, and the final decision can be decided in a probabilistic manner.
A more elaborate analysis is presented in their follow-up study \cite{he2020spatio}.
Yu et al.~\cite{yu2019markov} formulate the single-vehicle repositioning planning problem as a Markov Decision Process.
They propose to leverage parallelized matrix operations to re-formulate the Bellman equation \cite{sutton1999reinforcement}, thus reducing the computational complexity in finding optimal planning policy.
Multi-hop ride-hailing repositioning is considered in \cite{singh2019reinforcement, singh2021distributed}.
Similar to \cite{al2019deeppool}, they predict the number of vehicles in each region for certain time slots ahead of time using an estimated time of arrival (ETA) model.
Double DQN is adopted for each vehicle to choose the best neighbor region to move forward based on the current status of all the vehicles and the predicted demand and supply.
    
\subsection{Joint Matching and Repositioning}
\label{sec:review-joint}
In this part, we review methods that jointly optimize matching and repositioning with ML techniques. 
Note that all of them belong to the category of distributed planning. The research works in this part leverage RL to guide the decision making process.
Different from the review given by Qin et al.~\cite{qin2021reinforcement}, we focus on the works that jointly decide matching and repositioning.

Haliem et al.~\cite{haliem2020distributed-a, haliem2021distributed} propose to consider both the matching and repositioning in the ride-hailing planning process.
In their ride-hailing systems, each vehicle conduct initial matching by greedily searching the nearest requests, after which an insertion-based method is used to finalize the potential request list.
Then each driver, based on the value function learned by the DQN, weighs the requests in the final list.
The riders who receive those proposed ride offers can decide whether to accept the offers and join the trips where shared-rides are allowed.
The trips can be solo-ride or shared-ride.
Drivers are repositioned in parallel with the matching process.
Each driver takes actions indicated by his/her trained RL agent, i.e., the decision-making policy, independently.
Their proposed solution framework learns an optimal policy for each driver as opposed to those RL-based methods with collective planning scheme where a central policy is used, e.g., \cite{oda2018movi}.
Note that, in some works, although each driver makes decisions independently (e.g., \cite{haliem2020distributed-a, haliem2021distributed}), all drivers share one trained policy (e.g., \cite{manchella2020passgoodpool, manchella2021flexpool}).
Manchella et al.~\cite{manchella2020passgoodpool, manchella2021flexpool} propose to collectively optimize the system objectives, e.g., minimizing the waiting times and routing times.
Nevertheless, they allow distributed inference at the level of individual drivers. 
Their proposed model can be used by each vehicle independently.
It helps decrease computational costs associated with the growth of distributed systems. 
Specifically, they utilize a Double DQN with the experience relay mechanism.
Their model learns a probabilistic dependence between drivers' actions and the reward function.
The trained policy indicates a destination for each driver if s/he is not matched with any rider according to their proposed heuristic matching algorithm. 
Similar to \cite{xu2020highly, singh2019reinforcement, singh2021distributed}, multi-hop transit is enabled in their solutions.
Wang et al.~\cite{wang2018deep} model the matching and repositioning problems as a Markov Decision Process and propose learning solutions based on DQNs to optimize the trained policy for the drivers.
\revise{Their solution uses a temporal and spatial expanded action search strategy to accommodate the scenarios where there is only sparse training data, e.g., certain remote regions in the middle of the night.}
Besides, to increase the learning adaptability and
efficiency, they propose to use a transfer learning method to leverage the knowledge across both spatial and temporal spaces.

Besides \cite{haliem2020distributed-a, haliem2021distributed, manchella2020passgoodpool, manchella2021flexpool, wang2018deep},
DQN is used in other works as well, e.g., \cite{al2019deeppool, guo2022deep, tang2021value, li2020balancing}.
In \cite{al2019deeppool}, each vehicle decides its action by learning the impact of its action on the reward using a DQN model without coordinating with other vehicles.
In \cite{guo2022deep}, the vehicle repositioning procedure is formulated as a Markov Decision Process.
By sampling the future riders based on the historical probability distribution, the proactive relocation of vehicles is realized via a deep RL framework, which is composed of a Convolutional Neural Network and a Double DQN module. \revise{Then a request-vehicle assignment scheme is presented based on the value function attained from the vehicle repositioning process.}
\revise{Similarly, Tang et al.~\cite{tang2021value} propose a planning framework for tackling both the matching and repositioning tasks, the core of which is a unified value function which is trained offline using abundant historical data and is updated during the online phase.}
With the value function learned, the matching problem is then solved by the method proposed in \cite{xu2018large}, while the reposition destination of each idle vehicle is determined in a probabilistic manner following the distribution given by the discounted long-term values of all the candidate positions.
Li and Allan \cite{li2020balancing} also leverage a global value function for both the tasks of matching and repositioning, which is learned by the value iteration algorithm with historical data of ride requests.


