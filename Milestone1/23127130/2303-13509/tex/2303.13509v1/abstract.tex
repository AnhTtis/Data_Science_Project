% !TEX root = ../iccv_review.tex

% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/teaser_v4.pdf}
%   \caption{(a) Classical two-branch panoptic segmentation framework and (b) our unified panoptic segmentation framework. Previous methods split panoptic segmentation into two sub-problems and then fuse their results together. The instance predictions rely on the ambiguous center predictions. In contrast, we tackle the problem in a unified manner via learnable kernels.}
%   \label{fig:teaser}
% \end{figure*}
% dense pixels in the image vs. sparse points in 3d
% boundary is more discriminative in 3d
% and positions / points coordinates are good indicators

% segmentation by feature vs. position (high-level vs. low-level, long-range vs. local)

% 2D粘连 3D区分
% 变化过程，物体很小，kernel怎么样聚焦在一个instance，怎么样设计。

% panoptic方法
% 柱状图

% \twocolumn[{
%     \renewcommand\twocolumn[1][]{#1}%
%     \maketitle
%     \vspace{-36pt}
%     \begin{center}
%         \centering
%         \includegraphics[width=0.96\textwidth]{figures/teaser_crop.pdf}
%         \vspace{-6pt}
%         \captionof{figure}{\centering (a) Left. Hard cases in images are common in point cloud scenes: geometric-alike and relative small objects.\\
%         (b) Right. Promotion of P3Former with position guidance on SemanticKITTI~\cite{SemanticKITTI} validation set, especially for $\mathrm{PQ}^{\mathrm{Th}}$.}
%         \label{fig:teaser}
%     \end{center}
%     % \vspace{10pt}
% }]

\begin{abstract}
\vspace{-6pt}
% LiDAR-based panoptic segmentation desires a comprehensive perception of things and stuff categories in point clouds. 
% % Recent solutions treat it as a sequential of semantic and instance segmentation tasks, which are inefficient due to the cumbersome framework. 
% % Recent solutions mainly segment point clouds into semantics, cluster , and 
% In contrast to previous methods that tackle the problem in a cumbersome three-stage process, we propose Point K-Net that one-shot segment all kinds of groups of coherent point clouds. 
% to unify the segmentation frameworks in LiDAR point clouds. It uses a group of learnable kernels to cluster all groups of coherent point clouds unanimously for.
% Unlike segmentation in 2D images, the lack of appearance information poses challenges for kernels to separate instances in the 3D point clouds, making positional differences a critical factor for instance segmentation.
% Therefore, we enhance the discriminability of instances by Mixed-format Positional Encoding from Cartesian and polar spaces. To distinguish instances at close locations, we propose a Positional Segmentation Strategy that makes the kernels specialize more on concrete regions.   % zww: uncertain, need rethinking
% Point K-Net establishes new records on SemanticKITTI and nuScenes panoptic segmentation benchmarks, outperforming previous best results by 3.4\% and 1.2\% PQ, respectively. We hope its simplicity and effectiveness can benefit future research in LiDAR-based segmentation. The source code and models will be released.
% %\footnote{The source code and models will be released.}

% 1.

% LiDAR-based panoptic segmentation desires a comprehensive perception of things and stuff categories in point clouds. 
% In contrast to previous methods that tackle the problem in a cumbersome three-stage process, we propose Point K-Net that groups all kinds of coherent point clouds unanimously for either a potential object or a stuff region. 
% The framework is inspired by recent revolutions in image segmentation such as K-Net but composes of components tailored for LiDAR point clouds. 
% In specific, due to the distinct properties between point clouds and pixels such as the absence of appearance, we emphasize the role of position difference to distinguish different points in the end-to-end learning framework.
% Two ingredients, Mixed-parameterized Positional Encoding and Positional Segmentation Strategy, are proposed respectively for a comprehensive positional representation and optimizing the kernels' specialization on positions.
% Point K-Net establishes new records on SemanticKITTI and nuScenes panoptic segmentation benchmarks, outperforming previous best results by 3.4\% and 1.2\% PQ, respectively. We hope its simplicity and effectiveness can benefit future research in LiDAR-based segmentation. The source code and models will be released.

% We present Point K-Net for LiDAR-based panoptic segmentation in a fully end-to-end manner. 
% In contrast to previous three-stage methods that are cumbersome, error-prone, and parameter sensitive, Point K-Net benefits from a group of unified learnable kernels that is simple yet effective.
% The idea is inspired by recent developments in the 2D domain such as K-Net that use Transformer layers for kernel-feature interaction and bipartite matching for end-to-end training, so that each kernel can attend to different regions of interest.
% However, unlike images that assume complete appearance information for recognition, LiDAR-based point clouds have distinct properties such as the absence of appearance that raise different challenges.
% To make the idea feasible, we emphasize the role of positional differences in distinguishing different points. We design two specialized ingredients, Mixed-parameterized Positional Encoding and Segmentation by Position, respectively to extract better positional representation and optimize kernels to concentrate on specific regions.
% Point K-Net establishes new records on SemanticKITTI and nuScenes panoptic segmentation benchmarks, outperforming previous best results by 3.4\% and 1.2\% PQ, respectively.
% The source code and models will be released.

% Recent developments in the 2D domain use a group of learnable kernels for unified segmentation, which can significantly simplifies the typical separated-segmentation frameworks in LiDAR-based panoptic segmentation.
% However, this unified framework faces challenges in segmenting ambiguous instances with similar features. While these cases are corner cases in the 2D domain, they are common in LiDAR-based scenes since instances belonging to the same category are generally geometrically alike. Furthermore, small-scale objects add to the difficulty of locating and segmenting instances.
% To address these challenges, we present P3Former, Position-guided Point cloud Panoptic segmentation transFormer, that leverages a robust positional embedding to guide the whole segmentation process. Specifically, the positional embedding is designed in mixed parameterization. We not only embed it into features to enhance their discrimination, but also involve it in the mask prediction and kernel update, leading to Position-Aware Segmentation (PA-Seg) and Masked Focal Attention (MFA). Both designs 
% allow the kernel to concentrate in specific positions and predict small masks in a particular region.
% Extensive experiments on both SemanticKITTI and nuScenes demonstrate the effectiveness of P3Former, with 3.4\% and 1.2\% PQ above previous SOTA, respectively.
% The source code and models will be released.

% Recent developments in the 2D domain use a group of learnable kernels for unified segmentation, inspiring new frameworks to simplify the typical multi-stage methods in LiDAR-based panoptic segmentation.
% However, such a unified approach struggles in segmenting small objects and ambiguous instances with similar features, which are corner cases in 2D images but are common in point clouds, thus having inferior performance in 3D instance segmentation.
% % especially for close instances belonging to the same category,
% To tackle this challenge, we propose P3Former, Position-guided Point cloud Panoptic segmentation transFormer, that leverages a robust positional embedding to guide the segmentation process.
% Specifically, we devise a Mixed-parameterized Positional Embedding (MPE) that encodes Polar and Cartesian coordinate differences for stronger instance discriminative capability. Apart from embedding it into features, we further involve it in mask prediction and kernel update, leading to Position-Aware Segmentation (PA-Seg) and Masked Focal Attention (MFA). Both designs allow the kernel to concentrate in specific positions and predict small masks in a particular region.
% Extensive experiments on both SemanticKITTI and nuScenes demonstrate the effectiveness of P3Former, with 3.4\% and 1.2\% PQ above the previous SoTA, respectively.
% The source code and models will be released.

DEtection TRansformer (DETR) started a trend that uses a group of learnable queries for unified visual perception.
This work begins by applying this appealing paradigm to LiDAR-based point cloud segmentation and obtains a simple yet effective baseline.
Although the naive adaptation obtains fair results, the instance segmentation performance is noticeably inferior to previous works. 
By diving into the details, we observe that instances in the sparse point clouds are relatively small to the whole scene and often have similar geometry but lack distinctive appearance for segmentation, which are rare in the image domain. 
Considering instances in 3D are more featured by their positional information, we emphasize their roles during the modeling and design a robust Mixed-parameterized Positional Embedding (MPE) to guide the segmentation process. 
It is embedded into backbone features and later guides the mask prediction and query update processes iteratively, leading to Position-Aware Segmentation (PA-Seg) and Masked Focal Attention (MFA).
All these designs impel the queries to attend to specific regions and identify various instances. 
The method, named Position-guided Point cloud Panoptic segmentation transFormer (P3Former), outperforms previous state-of-the-art methods by 3.4\% and 1.2\% PQ on SemanticKITTI and nuScenes benchmark, respectively.
The source code and models are available at \href{https://github.com/SmartBot-PJLab/P3Former}{https://github.com/SmartBot-PJLab/P3Former}.
%\url{https://github.com/SmartBot-PJLab/P3Former}.

% 2.

% LiDAR-based panoptic segmentation desires a comprehensive perception of things and stuff categories in point clouds. 
% Existing methods tackle the problem in a three-stage, cumbersome and error-prone process, leaving the end-to-end framework underestimated. Learnable kernels unify panoptic segmentation in images but can't achieve satisfying results in point clouds due to the absence of appearance. To this end, we propose two ingredients to emphasize the role of position difference in distinguishing different points in 3D: (1) Mixed-parameterized Positional Encoding for a comprehensive positional representation and (2) Positional Segmentation Strategy for optimizing the kernels concentrate to specific regions. 
% With these tailored components, we propose Point K-Net that groups all kinds of coherent point clouds unanimously for either a potential object or a stuff region.
% It establishes new records on SemanticKITTI and nuScenes panoptic segmentation benchmarks, outperforming previous best results by 3.4\% and 1.2\% PQ, respectively. We hope its simplicity and effectiveness can benefit future research in LiDAR-based segmentation. The source code and models will be released.

\end{abstract}

\vspace{-12pt}