% !TEX root = ../iccv_review.tex

\vspace{-1mm}\section{Conclusion}\vspace{-1mm}

We present \methodname~for unified LiDAR-based panoptic segmentation. 
It builds on the DETR paradigm and addresses the challenges of distinguishing small and geometrically similar instances. 
We introduce a robust Mixed-parameterized Positional Embedding which is embedded into voxel features and further guides the segmentation process, leading to Position-Aware Segmentation and Masked Focal Attention. All these designs enable queries to concentrate on specific positions and predict small masks in a particular region.
State-of-the-art performance on SemanticKITTI and nuScenes demonstrate the effectiveness of our framework and reveal the potential of \methodname~with more robust 3D representations in future research.

% The proposed ingredients, Mixed-parameterized Positional Encoding and Positional Segmentation Strategy, make learnable kernels effectively separate instances by their positional differences, remedying the disadvantage of the lack of appearance information in 3D space.
% State-of-the-art performance on SemanticKITTI and nuScenes demonstrate the effectiveness of our framework and reveal the potential of Point K-Net with more robust 3D representations in future research.