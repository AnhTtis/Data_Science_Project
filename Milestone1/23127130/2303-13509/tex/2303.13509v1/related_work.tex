% !TEX root = ../iccv_review.tex

\vspace{-1mm}\section{Related Work}\vspace{-1mm}


\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{main_figure_crop.pdf}
  \caption{\methodname~ overview. After voxelizing points into voxels, \methodname~uses a backbone to extract voxel features and an MPE module to generate the MPE (Sec. \ref{sec:positional_encoding}). Apart from being embedded into features, MPE further serves as position guidance in the entire segmentation head. In the segmentation head, we use several PA-Seg (Sec. \ref{sec:positional_aware_seg}) and transformer decoder layers to update queries according to features and MPE, then another PA-Seg layer to predict panoptic results with updated queries. The PA-seg consists of two parallel branches to predict masks from features and MPE, with separate supervision. The integrated masks serve as the cross-attention map in Masked Focal Attention (Sec. \ref{sec:mfa}).}
  \label{fig:Overview}
\end{figure*}

\myparagraph{Point Cloud Segmentation.}\quad
Point cloud segmentation aims to map the points into multiple homogeneous groups. Previous works typically resort to different paradigms for indoor~\cite{VV-Net,KPConv,PointConv,LtS,3D-MPA} and outdoor~\cite{Pointnet,Pointnet++, RandLA-Net,Rangenet++,Point-to-Voxel,3D-MiniNet,Cylinder3D,AF2S3Net} scenes, given their difference in the point cloud distribution. Our work focuses on the outdoor case, where the point clouds are obtained by LiDAR and exhibit sparse and non-uniform distribution. The corresponding segmentation problem is called LiDAR-based 3D segmentation.

\noindent\emph{LiDAR-Based 3D semantic segmentation.}\quad
% The goal of LiDAR-based 3D semantic segmentation is to classify point clouds into different groups according to their semantic classes.
LiDAR-based 3D semantic segmentation categorizes point clouds according to their semantic properties.
Based on different representations of point clouds, we divide existing approaches into three streams, point-based~\cite{Pointnet,Pointnet++,Interpolated}, voxel/grid-based~\cite{RandLA-Net,salsa,Rangenet++,3D-MiniNet,Cylinder3D,AF2S3Net} and multi-modal~\cite{spvnas,rpvnet}. Point-based methods focus on processing individual points, while voxel/grid-based methods quantize point clouds into 3D voxels or 2D grids and apply convolution. Current LiDAR-based segmentation frameworks~\cite{Cylinder3D,AF2S3Net,spvnas} typically adopt voxel-based backbones due to their ability to capture large-scale spatial structures and acceptable computational cost thanks to sparse convolution~\cite{Spconv}. 

\noindent\emph{LiDAR-Based 3D panoptic segmentation.}\quad
Compared with LiDAR-based 3D semantic segmentation, LiDAR-based panoptic segmentation further segments foreground point clouds into different instances.
Most previous top-tier works~\cite{Panoptic-PolarNet,DS-Net,EfficientLPS,GPS3Net,SCAN,PHNet} start from this difference and produce panoptic predictions following a three-stage paradigm, \emph{i.e.}, first predict semantic results, then separate instances based on semantic predictions, and finally fuse the two results.
This paradigm makes the panoptic segmentation performance inevitably bounded by semantic predictions and requires cumbersome post-processing. In contrast, this paper proposes a unified framework with learnable queries for LiDAR segmentation, eliminating all of these problems and predicting panoptic results unanimously.

\myparagraph{Unified Panoptic Segmentation.}\quad
Unified panoptic segmentation was first proposed in the context of 2D image segmentation. Early works built their methods based on either semantic segmentation frameworks~\cite{PanopticPyramid, UPSNet, unifying_CVPR2020, solov2} or instance segmentation frameworks~\cite{panoptic_deeplab, deeplab} to perform panoptic segmentation separately. More recently, DETR~\cite{DETR} simplified the framework for 2D panoptic segmentation, but it still requires two-stage processing. ~\cite{K-Net,maskformer,mask2former} take this a step further by unifying 2D segmentation in one stage. These methods provide excellent solutions for making queries conditional on specific objects, but they require objects to have distinctive features. As a result, these frameworks may be inferior when dealing with objects that have similar texture and geometry. However, such situations are common in LiDAR-based scenes, resulting in ambiguous instance segmentation.