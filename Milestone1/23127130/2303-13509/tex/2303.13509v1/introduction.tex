% !TEX root = ../iccv_review.tex

\section{Introduction}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{teaser_crop.pdf}
  \caption{(a) Corner cases in images are common in point cloud scenes: geometrically alike and relatively small objects. (b) Promotion of P3Former with position guidance on SemanticKITTI~\cite{SemanticKITTI} test set, especially for $\mathrm{PQ}^{\mathrm{Th}}$.}
  \label{fig:teaser}
\end{figure}

DEtection TRansformer (DETR)~\cite{DETR} started a trend that uses a group of learnable queries for unified visual perception.
Various methods, such as K-Net~\cite{K-Net} and MaskFormer~\cite{maskformer}, are proposed to unify and simplify the frameworks of image segmentation. 
This work begins by applying this appealing paradigm to LiDAR-based point cloud segmentation and obtains a simple yet effective baseline.
It relies on transformer decoder and bipartite matching for end-to-end training so that each query can attend to different regions of interest and predict the segmentation mask of either a potential object or a stuff class.


This initial attempt unifies and significantly simplifies the framework for LiDAR-based panoptic segmentation.
However, although the naive implementation obtains fair results, the instance segmentation performance is noticeably inferior to previous works~(Fig.~\ref{fig:teaser}-(b)).
Our investigation into this issue reveals two notable challenges in this 3D case: 1) \emph{Geometry ambiguity} (Fig.~\ref{fig:teaser}-(a) left). Instances with similar geometry are much more common in the point clouds than in 2D images and are even harder to be separated due to the lack of texture and colors. 2) \emph{Relatively small objects} (Fig.~\ref{fig:teaser}-(a) right). Instances are typically much smaller with respect to the whole 3D scene, while queries empirically learn to respond to a relatively large area for high recall, making the segmentation of multiple close instances particularly difficult.

Further revisiting the difference between segmentation in images and LiDAR-based point clouds, we observe that instances in images, a dense and structured representation, always have their masks overlapped and their boundaries adhered to each other. In contrast, instances in the 3D space can be clearly separated according to the \emph{positional} information included in their point clouds.
Motivated by this observation, to tackle the aforementioned problems, we fully leverage such properties and propose a Position-guided Point cloud Panoptic Transformer, \emph{P3Former}, which uses a specialized positional embedding to \emph{guide} the whole segmentation procedure.

Specifically, we first devise a \emph{Mixed-parameterized Positional Embedding (MPE)}, which combines Polar and Cartesian spaces. MPE incorporates the Polar pattern prior to the point distribution with the Cartesian embedding, resulting in a robust embedding that serves as the foundation of position-guided segmentation.
It is first embedded into backbone features as the positional discriminator to separate geometrically alike instances.
Furthermore, we also involve it in the mask prediction and masked cross-attention, leading to \emph{Position-Aware Segmentation (PA-Seg)} and \emph{Masked Focal Attention (MFA)}.
% \emph{PA-Seg} introduces a branch for MPE-based mask prediction parallel with the original feature-based one to compensate for the lack of absolute positional information in high-level features.
\emph{PA-Seg} introduces a parallel branch for position-based mask prediction alongside the original feature-based one. It compensates for the lack of absolute positional information in high-level features.
\emph{MFA} simplifies the masked attention operation by replacing the cross-attention map with our integrated mask prediction.
All these designs enable queries to concentrate on specific positions and predict small masks in a particular region.

We validate the effectiveness of our method on SemanticKITTI~\cite{SemanticKITTI} and nuScenes~\cite{nuScenes} panoptic segmentation datasets.
\emph{P3Former} finally performs even better than previous methods on instance segmentation, resulting in new records with a PQ of 64.9\% on SemanticKITTI and a PQ of 75.9\% on nuScenes, surpassing previous best results by 3.4\% and 1.2\% PQ, respectively (Fig.~\ref{fig:teaser}-(b)).
The simplicity and effectiveness of \emph{P3Former} with our exploration along this pathway shall benefit future research in LiDAR-based panoptic segmentation.

