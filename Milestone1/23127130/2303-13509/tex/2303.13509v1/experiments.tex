% !TEX root = ../iccv_review.tex

\section{Experiments}\label{sec:Experiments}
In this section, we present the experimental setting and benchmark results on two popular LiDAR-based panoptic segmentation datasets: SemanticKITTI~\cite{SemanticKITTI} and nuScenes~\cite{nuScenes}. We also ablate the detailed designs of our proposed methods and provide representative visualizations for qualitative analysis.

\subsection{Experimental Setting}\label{sec:setting}
% We conduct our experiments on two mainstream datasets: SemanticKITTI~\cite{SemanticKITTI} and nuScenes~\cite{nuScenes}.

\myparagraph{SemanticKITTI. }
SemanticKITTI~\cite{SemanticKITTI,semantickittipan} is the first large-scale dataset on LiDAR-based panoptic segmentation. 
It contains 43552 frames of outdoor scenes, of which 23201 frames with panoptic labels are used for training and validation, and the remaining 20351 frames without labels are used for testing. 
% SemanticKITTI contains 28 semantic classes, which become 19 after mapping. Nine categories are things classes, and 11 classes are stuff classes.
The annotations include 8 things classes and 11 stuff classes out of 19 semantic classes.

\myparagraph{nuScenes.}
nuScenes~\cite{fong2021panoptic} is a public autonomous driving dataset. It provides a total of 1000 scenes, including 850 scenes (34149 frames) for training and validation and 150 scenes (6008 frames) for testing. Among the 16 semantic classes, there are 10 things classes and 6 stuff classes.

\myparagraph{Evaluation Metrics.}
We use the panoptic quality (PQ)~\cite{panoptic} as our main metric to evaluate the performance of panoptic segmentation. PQ can be seen as the multiplication of segmentation quality (SQ) and recognition quality (RQ), which is formulated as

\eqnsm{psq-seg-det}{\small{\text{PQ}} = \underbrace{\frac{\sum_{\TP} \text{IoU}}
{|\TP|}}_{\text{SQ}} \times \underbrace{\frac{|\TP|}{|\TP| + \frac{1}{2} |\FP| + \frac{1}{2} |\FN|}}_{\text{RQ}}.}

%The $\text{IoU}$ threshold for a prediction to be concerned as $\TP$ is 0.5. 
These three metrics can be extended to things and stuff classes, denoted as $\mathrm{PQ^{Th}}$, $\mathrm{PQ^{St}}$, $\mathrm{RQ^{Th}}$, $\mathrm{RQ^{St}}$, $\mathrm{SQ^{Th}}$, $\mathrm{SQ^{St}}$, respectively. We also report PQ$^{\dagger}$ proposed by~\cite{PQdagger}, which replaces PQ with $\text{IoU}$ for stuff classes.


\myparagraph{Implementation Details.}
We implement \methodname~with MMDetection3D~\cite{mmdet3d}. We follow Cylinder3D's~\cite{Cylinder3D} procedures for sparse feature extraction and data augmentation on both SemanticKITTI and nuScenes. For sparse feature extraction, we discretize the 3D space to $ 480\times360\times32$ voxels.
% We clip the 3D space range into $r\in [0, 50], \theta\in [-\pi, \pi], z\in [-4, 2]$ for SemanticKITTI, and $r\in [0, 50], \theta\in [-\pi, \pi], z\in [-5, 3]$ for nuScenes.
For data augmentation, we adopt rotation, flip, scale, and noise augmentation. We choose AdamW~\cite{ADAMW} as the optimizer with a default weight decay of 0.01. In the ablation study, we train the model for 40 epochs with a batch size of 4 on 4 NVIDIA A100 GPUs. The initial learning rate is 0.005 and will decay to 0.001 in epoch 26. More details are provided in the appendix.

\subsection{Benchmark Results}

\myparagraph{SemanticKITTI.}
We compare our method with RangeNet++~\cite{Rangenet++} + PointPillars~\cite{PointPillars}, LPSAD~\cite{LPSAD}, KPConv~\cite{KPConv} + PointPillars~\cite{PointPillars}, Panoster~\cite{Panoster}, Panoptic-PolarNet~\cite{Panoptic-PolarNet}, DS-Net~\cite{DS-Net}, EfficientLPS~\cite{EfficientLPS}, GP-S3Net~\cite{GPS3Net}, SCAN~\cite{SCAN} and Panoptic-PHNet\cite{PHNet}. Table~\ref{tab:benchmark:SemanticKITTI:test} shows comparisons of LiDAR panoptic segmentation performance on the SemanticKITTI test split. Our method surpasses the best baseline method by 3.4\% in terms of PQ. Notably, we outperform the second method ~\cite{PHNet} by 3.3\% and 3.4\% in $\mathrm{PQ^{Th}}$ and $\mathrm{PQ^{St}}$, respectively, suggesting that our unified framework gains in both things and stuff classes.

\myparagraph{nuScenes.}
% To verify our approach's effectiveness and generalization ability, 
We also provide the comparison results of LiDAR panoptic segmentation performance on nuScenes validation split. Compared with SemanticKITTI, the point clouds contained in scenes of nuScenes are more sparse, so it can better highlight the model's ability to model long-distance information. We compare our method with DS-Net~\cite{DS-Net}, EfficientLPS~\cite{EfficientLPS}, GP-S3Net~\cite{GPS3Net}, SCAN~\cite{SCAN} and Panoptic-PHNet~\cite{PHNet}. As shown in Table~\ref{tab:benchmark:nuScenes}, our method outperforms existing methods on most of the metrics, surpassing the runner-up method by 1.2\% and 2.9\% in terms of PQ and $\mathrm{PQ^{Th}}$. It shows that our method has a strong modeling ability for long-distance information and maintains a strong segmentation ability for sparse instances.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{4error.pdf}
  \caption{Comparisons of different designs in our model. ``Oracle $\mathrm{RQ^{Th}}$'' takes all instances included in things prediction as $\TP$. ``Error distances" measures the one-to-one distances of instances being wrongly predicted as one.}
  \label{fig:comparison_of_many}
\end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{attn_update_crop.pdf}
  \caption{Comparisons of update process with and without PA-Seg. (a) Darker red indicates a high attention level.  (b) We construct a crowed scene including 9 similar cars and visualize one query's mask prediction. Dark blue indicates high prediction confidence.}
  \vspace{-12pt}
  \label{fig:attn_update}
\end{figure*}


\subsection{Ablation Studies}
We conduct several groups of ablations studies in this section to demonstrate the effectiveness of~\methodname~and detailed information about the framework. All experiments are based on SemanticKITTI~\cite{SemanticKITTI} validation split.

\myparagraph{Effects of Components.}
We ablate each component that improves the performance of~\methodname~in Table~\ref{tab:ablation:components}. Our proposed MPE and PA-Seg promote 1.8\% PQ (2.8\% $\mathrm{PQ^{Th}}$) and 1.1\% PQ (2.7\% $\mathrm{PQ^{Th}}$) above baseline, respectively. MFA further improve 0.5\% PQ and 1.4\% $\mathrm{PQ^{Th}}$.
%Heavier network settings and more augmentations can also improve the performance by 1.4\% PQ. Here we adopt~\cite{polarmix} as the additional augmentation.

\begin{table}[h]
  \begin{center}
  \caption{Ablation studies of~\methodname~on SemanticKITTI validation set}
  \label{tab:ablation:components}
  \scalebox{0.90}{\tablestyle{8pt}{1.0}
    \begin{tabular}{cccc|p{0.6cm}<{\centering}p{0.6cm}<{\centering}p{0.9cm}<{\centering}p{0.6cm}<{\centering}p{0.6cm}<{\centering}p{0.6cm}}
    
    Baseline & MPE & PS & MFA & PQ & $\mathrm{PQ^{Th}}$ & $\mathrm{PQ^{St}}$\\
    \hline
    \specialrule{0.05em}{3pt}{3pt}
      \checkmark  &                &            &            &            58.0 & 59.6 & 57.0\\
      \checkmark  &   \checkmark   &            &            &            59.8 & 62.4 & 57.9\\
      \checkmark  &   \checkmark   & \checkmark &            &            60.9 & 65.1 & 57.9\\
      \checkmark  &   \checkmark   & \checkmark & \checkmark &            61.4 & 66.5 & 57.7\\
    %   \checkmark  &   \checkmark   & \checkmark & \checkmark & \checkmark & 62.8 & 72.5 & 76.4 & 69.5 & 58.2 & 66.6\\
    \end{tabular}}
  \end{center}
  \vspace{-12pt}
\end{table}

\myparagraph{Ambiguous Instance Segmentation.}
\emph{Ambiguous Instance Segmentation} (AIS) refers to the case that one query segments multiple instances due to the close feature representation.
We conduct a series of experiments to prove that AIS is one of the salient bottlenecks in this task.
Specifically, we start from the metric $\mathrm{RQ^{Th}}$ since it is a key component of $\mathrm{PQ^{Th}}$ and a direct indicator of the model's capability to locate instances. 
As shown in Fig.~\ref{fig:comparison_of_many}-(a), there is a huge gap between Oracle $\mathrm{RQ^{Th}}$ and $\mathrm{RQ^{Th}}$ for baseline setting. Since Oracle $\mathrm{RQ^{Th}}$ ignores the AIS cases, it indicates that there are considerable numbers of AIS cases due to the lack of appearance information. Our proposed ingredients effectively remedy the gap, accompanied by the promotion of performance. Here the formulation of Oracle $\mathrm{RQ^{Th}}$ is the same as $\mathrm{RQ^{Th}}$, while we reformulate $\text{IoU}$ as $\text{Oracle IoU}$:
\begin{equation}
    \text{Oracle IoU} = \frac{P\cap T}{T},
\end{equation}
where $P$ indicates prediction areas and $T$ indicates target areas. $\text{Oracle IoU}$ measures the percentage of areas that are covered by predictions.

\vspace{12pt}

\myparagraph{Mixed-parameterized Positional Encoding (MPE). }\label{sec:MPE}
We compare different types of positional encoding in Fig.~\ref{fig:comparison_of_many}. Fig.~\ref{fig:comparison_of_many}-(a) shows that Cartesian parameterization (Cart) and Polar parameterization (Polar) are both effective in terms of PQ. The combination of these two can further bring improvements. In Fig.~\ref{fig:comparison_of_many}-(b) we gather the statistics of the average distances of instances that are wrongly segmented as one on four dimensions $(x,y,\theta,\rho)$. Since instances with smaller distances are harder to separate, methods that achieve smaller average distances on a specific dimension are more accurate on that dimension. It proves that single-parameterized positional encoding can partially improve the model's accuracy on some dimensions but is deficient on others. A mixed-parameterized one can remedy this drawback.

\begin{table}[h]
    \caption{Ablation of Segmentation by Position (SP). ``RS" stands for ``Relative Scale", measuring the relative scale of mask predictions.}
    \vspace{-12pt}
    \begin{center}
    \small{
    \begin{tabular}{c|c|ccc}
    Metrics             & PA-Seg        & Layer 0 & Layer 1 & Layer 3\\
    \hline
    \specialrule{0.05em}{3pt}{3pt}
    RS (\%)             &            & 0.146 & 0.062 & 0.036\\
                        & \checkmark & 0.136 & 0.045 & 0.030\\
    \specialrule{0.05em}{3pt}{3pt}
    PQ (\%)             &            & 55.8 & 58.3 & 59.8 \\
                        & \checkmark & 56.0 & 59.8 & 60.9 \\
    \specialrule{0.05em}{3pt}{3pt}
    $\mathrm{PQ^{Th}}$ (\%) &        & 54.5 & 60.1 & 62.9\\
                        & \checkmark & 54.6 & 63.0 & 65.1 \\
    \end{tabular}
        }
    \end{center}
    \label{tab:pf_loss}
    \vspace{-12pt}
\end{table}

\myparagraph{Position-Aware Segmentation (PA-Seg).}
We validate the effectiveness of PA-Seg in many aspects. Fig.~\ref{fig:comparison_of_many}-(a) shows that PA-Seg reduces the AIS cases and closes the gap of Oracle $\mathrm{RQ^{Th}}$ and $\mathrm{RQ^{Th}}$. With queries specialize in focused positions, their mask predictions shrink to a more precise area, resulting in smaller relative scale (Table~\ref{tab:pf_loss}) of mask prediction and smaller average error distances (Fig.~\ref{fig:comparison_of_many}-(b)). PA-Seg leads the queries to quickly be adaptive to their corresponding objects, with larger $\mathrm{PQ}$ and $\mathrm{PQ^{Th}}$ promotion during iterative refinements (Table~\ref{tab:pf_loss}).


% \myparagraph{Model Comparisons.}
% We compare panoptic segmentation models based on the Cylinder3D~\cite{Cylinder3D} backbone. As shown in Table~\ref{sec:setting}. We outperform previous methods~\cite{Cylinder3D, DS-Net} by a large margin both in performance and speed. The comparisons of~\methodname~with different settings demonstrate that while heavier network settings promote performance, they also add on computational cost and lower the inference speed.
