\section{Experiments attacking AI-detection algorithms with \model}
\label{sec:attacks}

In this section, we describe experiments attacking existing AI-generated text detectors with our \model\ paraphraser. We  first detail our experimental setup (metrics, models, and detectors) in \sectionref{sec:eval-metrics-attacks} and \ref{sec:expt-setup} before moving on to discuss our results (\sectionref{sec:attack-expts}). Overall, we find that paraphrasing easily evades all detectors across three language models (GPT2-XL, OPT-13B, GPT3.5-davinci-003) and two tasks (open-ended generation and long-form question answering).
% Finally, we demonstrate the effectiveness of our semantic retrieval defense against paraphrase attacks in \sectionref{sec:defense-results}.
% Later, we evaluate intrinsic paraphrase quality in \sectionref{sec:intrinsic-evaluation}.

\subsection{Evaluation metrics}
\label{sec:eval-metrics-attacks}

We use two metrics to automatically evaluate attack success rates after paraphrasing: (1) detection accuracy, and (2) semantic similarity between the original LM generations and \model~paraphrases.\footnote{Additional aspects of paraphrase quality like continuity to prompt, fluency, and coherence are evaluated in \sectionref{sec:intrinsic-evaluation} using GPT-3.5 perplexity~\citep{brown2020language} and RankGen~\citep{krishna-etal-2022-rankgen}.}

\vspace{0.1in}

\noindent \textbf{Detection accuracy}: Our first metric measures how often the input text is correctly detected as AI-generated. Since detection rates are heavily dependent on the chosen detection threshold, the AUC-ROC metric is commonly used to measure detector performance~\citep{mitchell2023detectgpt}, which considers the range of all possible thresholds. However, in this application, it is critical that the \emph{false positive rate} is low; in other words, human-written text must almost never be classified as machine-generated~\citep{AITextClassifier, kirchenbauer2023watermark}. Hence, we fix the false positive rate to 1\% for all detection algorithms (although even 1\% is likely too high in practice), and adjust the detection threshold accordingly while reporting detection accuracies. Additionally, we also plot ROC curves focusing on the region between FPR=0\% and FPR=1\%. Overall, at a constant false positive rate of 1\%, we expect detection rates to plummet on paraphrased text.

\begin{table*}[t!]
\small
\begin{center}
\begin{tabular}{ lrrrrrr } 
 \toprule
  \multicolumn{7}{c}{\textbf{Paraphrase attacks on open-ended generation} (Wikipedia prompts, 300 generated tokens)}\vspace{0.1in} \\ 
  Metric\hspace{0.09in} $\rightarrow$ & Sim $\uparrow$ & \multicolumn{5}{c}{Detection Accuracy $\downarrow$} \\
  \cmidrule{3-7}
 Detector $\rightarrow$ & & Watermarks & DetectGPT  & OpenAI & GPTZero & RankGen  \\
 & & \shortcite{kirchenbauer2023watermark} & \shortcite{mitchell2023detectgpt}  & \shortcite{AITextClassifier} & \shortcite{GPTZero} & \shortcite{krishna-etal-2022-rankgen}\\

% Metric $\rightarrow$ & Sim $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ \\
 
 \midrule
 GPT2-1.5B  & - & 100.0 & 70.3 & 21.6 & 13.9  & \textbf{13.5} \\
 % sim scores = using detectgpt sim, since it's the worst SIM and good to have a conservative estimate here
 + \model\ 20L  & 99.2 & 97.1 & 28.7 &  19.2  & 9.1 & 15.8 \\
 % sim scores = 99.1, 98.6, 99.5
 + \model\ 40L & 98.4 & 85.8 & 15.4 & 17.8 & 7.3 & 18.0 \\
  % sim scores = 98.2, 97.4
 + \model\ 60L & 96.9 & 68.9 & 8.7 & \textbf{13.3} & 7.1 & 19.8  \\
 % sim scores = 97.2, 95.3
 + \model\ 60L, 60O & 94.3 & \textbf{57.2} & \textbf{4.6} & 14.8 & \textbf{1.2} & 28.5 \\
 \midrule
 OPT-13B & - &  99.9 & 14.3 & 11.3 & 8.7 & \textbf{3.2}  \\
 % sim = 99.6
 + \model\ 20L & 99.1 & 96.2 & 3.3 & 11.8  & 5.4 & 5.2 \\
 + \model\ 40L & 98.6  & 84.8 &  1.2  & 11.6 & 3.8 & 6.6  \\
 + \model\ 60L & 97.1 & 63.7 & 0.8 & \textbf{9.1} & 6.3 & 9.3   \\
 % sim = 96.9
 + \model\ 60L, 60O & 94.6 & \textbf{52.8}  & \textbf{0.3} & 10.0 & \textbf{1.0} & 13.5\\
 \midrule 
 GPT-3.5-175B, davinci-003  & - & - & 26.5* & 30.0  & 7.1  & \textbf{1.2} \\
 + \model\ 20L & 97.6 & -& 12.5*  & 20.6  & 4.3 & 1.7  \\
 + \model\ 40L & 96.7 & - & 8.0*  & 22.4  & 4.8  &  2.0 \\
 + \model\ 60L & 94.2 & - & 7.0*  & \textbf{15.6} & 6.1  & 3.9  \\
 + \model\ 60L, 60O & 88.4 & - & \textbf{4.5}*  & \textbf{15.6} & \textbf{1.8} & 7.3\\
 \midrule
 % 1.7
 Human Text & - & 1.0 &  1.0\phantom{*} & 1.0 & 1.0 & 1.0 \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-0.1in}
\caption{Performance of various AI detection algorithms (at 1\% false positive rate or FPR) before and after \model\ paraphrasing on open-ended generation using Wikipedia prompts. As the lexical diversity (L) and re-ordering (O) increases, detection accuracy decreases across algorithms, with nearly perfect semantic similarity (Sim). *GPT3.5 DetectGPT scores computed using 200 samples at a 20\% FPR since it scores 0\% at a 1\% FPR.}
\vspace{-0.1in}
\label{tab:watermark-attacks}
\end{table*}


\vspace{0.1in}

\noindent \textbf{Semantic similarity (Sim)}: Detection accuracy is an insufficient evaluation of our attack's success. We also need to measure whether the original and paraphrased generations share approximately the same semantics. We measure semantic similarity using the \spavg model from~\citet{wieting-etal-2022-paraphrastic}, which is a simple embedding averaging model trained on a large corpus of filtered paraphrase data~\cite{wieting-gimpel-2018-paranmt}.\footnote{This is an improved version, trained on more data, from models proposed in prior work~\cite{wieting-etal-2019-beyond,wieting-etal-2019-simple}.} Despite its simplicity, \spavg achieves state-of-the-art performance on unsupervised semantic similarity tasks, while being four orders of magnitude faster on CPU than methods using pretrained Transformers. We consider semantics to be approximately preserved if the \spavg score is higher than 0.76, which is the median \spavg score of human-written paraphrase pairs on the \booktranslate\ dataset~\citep{thai2022booktranslate}. Finally, since automatic metrics for text generation are limited~\citep{celikyilmaz2020evaluation}, we additionally conduct a human evaluation of \model's paraphrase quality in \sectionref{sec:human-evaluation}.

% an order of magnitude faster on GPU and

% \noindent \textbf{Attack success rate (S\&A)}: The ideal paraphrase attack should \emph{simultaneously} evade detection while also preserving semantics, which necessitates combining the above two metrics. We denote overall attack success rate as the fraction of paraphrases which both evade detection as well as have a \spavg score of at least 0.75. This metric is directly inspired by the $J$ aggregate metric used previously to evaluate style transfer quality~\citep{style20,krishna2022few}.

\subsection{Models, datasets \& detection algorithms}
\label{sec:expt-setup}

\noindent \textbf{Base language models}: We conduct attacks on three language models of varying sizes that belong to different model families. We consider the GPT2-XL model which has 1.5B parameters~\citep{radford2019language}, the OPT-13B model~\citep{zhang2022opt}, and the \texttt{text-davinci-003} variant from the GPT-3.5 family of models~\citep{brown2020language}, which has 175B parameters and has additionally been instruction tuned using reinforcement learning from human feedback~\citep{ouyang2022training}. For all language models, we sample generations that are 300 tokens long before passing them through \model\ for the attack experiments.\footnote{For GPT2-XL and OPT-13B, we generate text using nucleus sampling~\citep{holtzman2020curious} with $p=0.9$. For \texttt{davinci-003}, we use the default hyperparameter settings on the API Playground of temperature = 0.7 to sample tokens.}

\vspace{0.1in}

\noindent \textbf{Natural language generation tasks}: We experiment with two text generation tasks whose outputs are long-form text since most malicious applications (e.g., fake article creation) are associated with long-form outputs. First, we consider \emph{open-ended generation}, in which an LM must generate a continuation to a passage given a two-sentence prompt. To obtain our prompts, we sample 3K contiguous two-sentence chunks from the validation split of the WikiText-103 dataset~\citep{meritypointer}, and use the next 300 tokens as the ``human-written'' continuation. Second, we evaluate long-form question answering~\citep{fan2019eli5}, in which an LM must answer a how/why question (e.g., \emph{Why are almost all boats painted white?}) with a 250-350 word answer. To build a long-form question answering dataset, we scrape questions from the \href{https://www.reddit.com/r/explainlikeimfive/}{r/explainlikeimfive} subreddit posted between July to December 2021.\footnote{We choose this period since current language models have been trained on internet data available before June 2021~\citep{GPT35}, this prevents verbatim copying from training data.} We randomly sample 500 questions from each of six popular domains on the subreddit (biology, physics, chemistry, economics, law, and technology) and pair each question with its longest human-written answer, which yields 3K long-form QA pairs. Note that in both tasks, the human references (continuation for the first task and answer for the second) are only used to adjust the detection thresholds of our studied detectors to maintain a 1\% FPR.\footnote{Evaluating how well the studied LMs perform on these two tasks is a challenging problem in its own right that could make additional use of the human references, but this is irrelevant to our paper.}



\vspace{0.1in}

\noindent \textbf{AI-text detection algorithms}: Our main experiments use five AI detection algorithms: (1) watermarking (\sectionref{sec:watermark}); (2) DetectGPT (\sectionref{sec:outlier-detection}); (3) GPTZero (\sectionref{sec:outlier-detection}); (4) OpenAI's text classifier (\sectionref{sec:classifiers}); and (5) RankGen-XL-all~\citep{krishna-etal-2022-rankgen}.\footnote{While RankGen was not explicitly optimized for AI text detection, it was trained to identify well-written continuations to a prompt to improve generation quality, so we hypothesize that it could also act as a reasonable AI-generated text detector.}  We respect the minimum length specifications of each detector, discarding instances where any of the machine-generated text, human-written text, or paraphrased text is shorter than the minimum length. We use the default hyperparameters for each detection algorithm.

\vspace{0.1in}



\noindent \textbf{Paraphrasing AI-generated text}: We pass the prompts for each task and AI-generated responses to those prompts through \model. Specifically, we feed the model input of the form 
\begin{quote}
lex = L, order = O prompt
\texttt{<p>} generated-text \texttt{</p>},
\end{quote}
 where $L$ and $O$ represent the paraphraser diversity control codes and the \texttt{<p>} and \texttt{</p>} special tokens mark the boundaries of the text to be paraphrased. We use $L = 20, 40, 60$ and $O = 0, 60$ in our main attack experiments. After paraphrasing, we ensure that the machine-generated sequence, paraphrased sequence, and human-written sequence have an equal number of words by truncating them to the length of the shortest among the three. To ensure higher semantic preservation, we iteratively paraphrase long sequences three sentences at a time, keeping already paraphrased text in the context of the generation. In our experiments, we only \textbf{paraphrase each generation once}, while a sophisticated attacker may sample multiple paraphrases and use the sample that best evades detection while preserving semantics; we discuss this further in \sectionref{sec:multiple-samples}.




\subsection{Attacking AI-generated text detectors}
\label{sec:attack-expts}

We present our attack results in \tableref{tab:watermark-attacks} for open-ended generation and in \tableref{tab:attacks-lfqa} for long-form question answering. Overall, we observe the following:

\begin{table}[t!]
\small
\begin{center}
\begin{tabular}{ lrrrr } 
 \toprule
  \multicolumn{5}{c}{\textbf{Long-form Question Answering}, 300 generated words}\vspace{0.1in} \\ 
 Metric $\rightarrow$ & Sim $\uparrow$ & \multicolumn{3}{c}{Detection Accuracy $\downarrow$} \\
 \cmidrule{3-5}
& & W.Mark & D.GPT  & OpenAI \\
 %& & \multicolumn{2}{c}{\shortcite{kirchenbauer2023watermark}} & \multicolumn{2}{c}{\shortcite{mitchell2023detectgpt}}  & \multicolumn{2}{c}{\shortcite{AITextClassifier}} \\
 
 \midrule
% \multicolumn{4}{l}{\emph{Long-form QA (300 generated tokens)}}\vspace{0.15cm} \\
 GPT2-1.5B  & - & 100.0  & 74.9\phantom{*} &  59.2 \\
 % sim scores = 99.4, 99.6, 100.0
 + \model\ 20L & 99.5 & 98.9 & 45.7\phantom{*} & 35.3   \\
 % sim scores = 99.2, 98.8, 99.4
 + \model\ 40L  & 99.0 & 90.7 & 28.0\phantom{*} & 34.4 \\
  % sim scores = 97.9, 97.7, 99.3 
 + \model\ 60L & 97.5 & 71.1 & 15.8\phantom{*} & \textbf{31.3} \\
 % sim scores = , , , 98.6
 ~~~+ 60L, 60O & 96.2 & \textbf{55.8} & \textbf{7.6}\phantom{*} & 32.7 \\
 \midrule
 OPT-13B & - & 100.0 & 29.8\phantom{*} & 33.5 \\
  % sim scores = 100.0, 99.6, 99.8
 + \model\ 20L & 99.6 & 98.3 & 15.0\phantom{*} & 24.5 \\
  % sim scores = 98.6, 99.4, 100.0
 + \model\ 40L & 99.4 & 87.3 & 6.4\phantom{*} & 24.1 \\
  % sim scores = 97.2, 97.3, 99.4
 + \model\ 60L & 96.5 & 65.5 & 3.2\phantom{*} & \textbf{21.6}   \\
 % sim scores = 94.7
 ~~~+ 60L, 60O & 92.9 & \textbf{51.4} & \textbf{1.5}\phantom{*} & \textbf{21.6} \\
 \midrule 
 GPT-3.5-175B \\
 davinci-003  & - &- & 67.0* & 40.5 \\
 + \model\ 20L & 99.9 &- & 54.0* & 43.1 \\
 + \model\ 40L & 99.8 &- & 36.0* & 43.1 \\
 + \model\ 60L & 99.5 & -& 23.0* & 40.1  \\
 ~~~+ 60L, 60O & 98.3 & - & \textbf{14.0}* & \textbf{38.1} \\
 \midrule
 % 1.7
 Human Text & - & 1.0 & 1.0\phantom{*} & 1.0 \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-0.1in}
\caption{Performance of AI-text detection algorithms (at 1\% false positive rate) before and after \model\ paraphrasing on long-form question answering. As the lexical (L) and order (O) diversity increases, detection accuracy decreases across algorithms, with nearly perfect semantic similarity (Sim). *GPT3.5 DetectGPT scores computed using 100 samples at a 20\% FPR, since it scores 0\% at a 1\% FPR.}
\vspace{-0.1in}
\label{tab:attacks-lfqa}
\end{table}


\vspace{0.1in}

\noindent \textbf{Paraphrasing significantly lowers detection accuracy while preserving input semantics}. Across all language models, detectors (except for RankGen\footnote{RankGen scores paraphrased text as AI-generated more often than non-paraphrased text. We attribute this to paraphrases being poorer continuations to the prompt compared to the original (\sectionref{sec:intrinsic-evaluation}), an aspect RankGen bases its score on. Unfortunately, low overall detection rates with or without paraphrasing make it a poor AI-generated text detector.}), and tasks, paraphrasing significantly lowers detection accuracy across all diversity control codes. For instance, paraphrasing GPT2-XL open-ended generations reduces watermark detection accuracy from 100\% to 57.2\%, and DetectGPT accuracy from 70.3\% to just 4.6\%. Trends are similar even for large language models like GPT-3.5, for which paraphrasing reduces OpenAI's text classifier accuracy from 30.0\% to 15.6\%. Additionally, \model\ preserves semantics effectively, as 88.4\%-99.9\% paraphrases achieve a \spavg\ score higher than the median \spavg\ score of human-written paraphrases in \booktranslate. Among the different detection algorithms, we find that watermarking is the most resilient to paraphrasing.




\begin{figure}[t!]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/gpt2_xl_fpr_1}
    \caption{ROC curves (with X-axis clipped at 1\% false positive rate or FPR) for GPT2-XL using different AI-detection algorithms. Curves are shown before (solid lines) and after paraphrasing (dashed). Paraphrasing reduces detection accuracy across FPRs. Our proposed defense (retrieval) has the highest accuracy after paraphrasing. See \appendixref{appendix:more-roc-curves} for unclipped ROC curves.}
    \label{fig:auc-plots}
\end{figure}




\vspace{0.1in}

\noindent \textbf{Non-watermarking detectors are generally ineffective.} We observe that all AI-generated text detectors apart from watermarking struggle with text generated by larger models like OPT-13B and GPT-3.5 davinci-003, achieving detection accuracies lower than 50\%.
For example, while DetectGPT is effective on the smaller GPT2-XL model with a detection accuracy of 74.9\% on long-form QA, its accuracy drops to just 29.8\% on OPT-13B. Furthermore, GPTZero and RankGen perform the worst among the five detectors on all tested language models (\tableref{tab:watermark-attacks}), as they are only able to detect fewer than 15\% of non-paraphrased AI-generated text in all settings. Thus, we recommend against using these models for detecting AI-generated text.

\vspace{0.1in}

\noindent \textbf{ROC plots confirm the trends at different false positive rates}. In \figureref{fig:auc-plots}, we plot the detection accuracy (true positive rate) at different values of false positive rates between 0\% and 1\% for GPT2-XL. Overall, paraphrasing significantly drops detection rates across all false positive rate thresholds. ROC plots for GPT2-XL at larger false positive rates are available in \appendixref{appendix:more-roc-curves}.


\subsection{Alternative paraphrasing attacks} 
\label{sec:alt-paraphrasers}
\label{sec:multiple-samples}
Here, we discuss two other (untested) ways to attack AI-generated text detectors via paraphrasing, which further showcase the brittleness of existing detectors.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/defense-idea}
    \caption{An illustration of AI-generated text detection with retrieval. Several users (including the attacker, shown as the purple emoji) feed prompts to the API which are collectively added to a private API-side database. Candidate queries are compared against this database using a retriever like BM25 or \spavg. Empirically, we find that this defense is quite effective at detecting paraphrases from an attacker (as shown in the figure).}
    \vspace{-0.05in}
    \label{fig:defense-idea}
\end{figure*}

\noindent\textbf{Paraphrasing multiple times:} Our presented attacks use just a single paraphrase generated by \model\ to evade detection. An simple way to further improve the effectiveness of a paraphrase attack is to sample multiple times\footnote{Precisely, compute $f_\text{dipper}(x)$ for different random seeds during nucleus or temperature sampling. Alternatively, an attacker could also compute $f_\text{dipper}(f_\text{dipper}(...f_\text{dipper}(x)))$, but this will lead to excessive semantic drift from $x$.} from \model\ and choose a paraphrase that evades the detector while also preserving semantics. We do not perform this attack since it can only be done if the attacker has access to a detector, which may be a strong assumption (more in \sectionref{sec:limitations-retrieval}). That said, we believe leveraging multiple paraphrase samples can make these kinds of paraphrase attacks even more potent against publicly available detectors.

\vspace{0.05in}

\noindent\textbf{Non-\model\ paraphrasers:} A second alternative is to use non-\model\ paraphrasers that operate at the sentence level. These models can be deployed for long-form text inputs by paraphrasing the inputs sentence by sentence, ignoring prompt context. While the concurrent work of~\citet{sadasivan2023aigenerated} shows that this method can also evade detection, our ablations in \sectionref{sec:intrinsic-evaluation} show that these paraphrasers have lower quality and are less compatible with the prompt as \model\ paraphrasers. A more interesting option is to use a large language model served by an API to perform few-shot contextual paraphrasing. While this method is likely to provide accurate paraphrases, they may be detectable by strategies like watermarking (whether using the same API provider as the original LLM or a different provider). We thus expect a sophisticated adversary to use their own private paraphraser to evade detection (\model\ in our simulated attacks).



