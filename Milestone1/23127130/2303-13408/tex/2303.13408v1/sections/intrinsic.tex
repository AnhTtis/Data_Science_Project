

\begin{table*}[t]
\small
\centering
\resizebox{0.97\textwidth}{!}{%
\begin{tabular}[b]{@{}p{0.5cm}p{6.2cm}p{4.5cm}p{4cm}@{}}
\toprule
  \textbf{L} &
  \textbf{Original} &
  \textbf{Paraphrase} &
  \textbf{Annotator Comment}\\\midrule
  40 &
  \textbf{When tailoring his 1970 composition "I'm the Greatest" for Starr to record on the 1973 album Ringo, Lennon referenced the song title with the lines [$\ldots$]} Lennon's original \textcolor{OrangeRed}{line} was changed to "Back off Boogaloo" to \textcolor{OrangeRed}{reference the song's title, creating} the line \textcolor{OrangeRed}{that} Starr sang \textcolor{OrangeRed}{in} the finished \textcolor{OrangeRed}{version.} &
  Lennon's original \textcolor{ForestGreen}{reference to "I'm the Greatest"} was changed to "Back off Boogaloo" to \textcolor{ForestGreen}{create} the line Starr sang \textcolor{ForestGreen}{on} the finished \textcolor{ForestGreen}{recording.} &
  \textbf{4---Nearly equivalent}\newline The original does not specify that the line in question was "I'm the Greatest." However, this can be inferred from the rest of the passage. Otherwise, the paraphrase and the original are quite similar.\\\midrule
  60 &
  \textbf{Heaving wildly, the Hunyani began to descend rapidly. [\ldots]} The crew \textcolor{OrangeRed}{was unable to} regain \textcolor{OrangeRed}{control} and the Hunyani \textcolor{OrangeRed}{crashed} into the ground \textcolor{OrangeRed}{at 17:14, killing all onboard.}  The \textcolor{OrangeRed}{crash} was investigated by the Civil Aviation Authority of \textcolor{OrangeRed}{Zimbabwe and the investigation concluded} that the crash was caused by a combination of pilot error, \textcolor{OrangeRed}{fatigue}, and \textcolor{OrangeRed}{weather conditions.} The \textcolor{OrangeRed}{pilot} had \textcolor{OrangeRed}{failed to} properly \textcolor{OrangeRed}{assess} the weather \textcolor{OrangeRed}{conditions before takeoff, instead relying} on a forecast from a private \textcolor{OrangeRed}{weather} service. The weather \textcolor{OrangeRed}{conditions worsened as the plane flew and with the pilots already fatigued, they were unable to appropriately react to the changing conditions. The combination of these factors led to the plane losing control and crashing.} &
  The crew \textcolor{ForestGreen}{could not} regain \textcolor{ForestGreen}{control,} and \textcolor{ForestGreen}{at 17:14} the Hunyani \textcolor{ForestGreen}{plunged} into the ground \textcolor{ForestGreen}{and all those on board were killed.} The \textcolor{ForestGreen}{accident} was investigated by the Civil Aviation Authority of \textcolor{ForestGreen}{Zimbabwe, which found} that the crash was caused by a combination of pilot error, \textcolor{ForestGreen}{fatigue} and \textcolor{ForestGreen}{the weather.} The \textcolor{ForestGreen}{pilots} had \textcolor{ForestGreen}{not} properly \textcolor{ForestGreen}{examined} the weather \textcolor{ForestGreen}{before take-off, and instead had relied} on a forecast from a private \textcolor{ForestGreen}{meteorological} service. The weather \textcolor{ForestGreen}{deteriorated during the flight, and the exhausted pilots were unable to react properly to the deteriorating conditions.} & \textbf{5---Approx. equivalent}\newline Well chosen words make the alternate version different but retain the original meaning well. The only difference is that the paraphrased version does not have the concluding sentence, but it does not alter the meaning in any way as the facts have already been included previously.\\\midrule
  60 &
  \textbf{Sonic's main means of attack is the Spin Attack, in which he curls into a ball and rapidly spins his body, damaging enemies and certain obstacles upon collision. This may be performed by jumping or by rolling on the ground.} \textcolor{OrangeRed}{Sonic can also} use a variety of other \textcolor{OrangeRed}{moves}, such as the \textcolor{OrangeRed}{Homing Attack, Slide Attack, and Wall Jump. He also has access to power-ups like the Speed Shoes, Invincibility, and the Super Peel Out. In some games, Sonic can also use} special items or vehicles to traverse \textcolor{OrangeRed}{levels.} &
  \textcolor{ForestGreen}{Also, the character can} use a variety of other \textcolor{ForestGreen}{attacks,} such as the \textcolor{ForestGreen}{homing attack, the slide attack, and the wall jump. In addition, he can acquire power-ups such as the Speed Boost, Super Speed, and the invincibility meter, and in certain games, he may make use of} special items or vehicles to traverse \textcolor{ForestGreen}{the world.} &
  \textbf{2---Topically related}\newline In the second part of the paraphrase, the writer ignores the actual moves and skills of Sonic and invents some entirely different ones that not mentioned in the original text. The method of performing the attack moves has also been missed out.\\\bottomrule
\end{tabular}%
}
\caption{Representative model outputs of each lexical code (L), along with Likert ratings and justifications from the annotators (native English teachers and editors). The bolded text in the original paragraphs are the context. \textcolor{OrangeRed}{Red words} represent the content being changed in the original text (manually marked by the authors), and \textcolor{ForestGreen}{green words} show the corresponding text in \model's paraphrases.}
\label{tab:cherry-pickedexamples}
\end{table*}
%https://docs.google.com/spreadsheets/d/10LtBZLLuTut2kDdyiCkCkYWovnmgTESOABqse-P30EU/edit?usp=sharing
\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rrrrrr@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{L}}} &
  \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Sum of 4 and 5}}} &
  \multicolumn{1}{c}{\textbf{5}} &
  \multicolumn{1}{c}{\textbf{4}} &
  \multicolumn{1}{c}{\textbf{3}} &
  \multicolumn{1}{c}{\textbf{2}} \\
\multicolumn{1}{c}{} &
  \multicolumn{1}{c}{} &
  \multicolumn{1}{c}{\textbf{Approx. equivalent}} &
  \multicolumn{1}{c}{\textbf{Nearly equivalent}} &
  \multicolumn{1}{c}{\textbf{Somewhat equivalent}} &
  \multicolumn{1}{c}{\textbf{Topically related}} \\
  \midrule
20                              & 95.0\% & 63.3\% & 31.7\% & 5.0\%    & 0.0\% \\
40                              & 78.3\% & 45.0\%   & 33.3\% & 21.7\% & 0.0\% \\
60                              & 70.0\% & 28.3\% & 41.7\% & 28.3\% & 1.7\% \\\midrule
\textbf{Total} & 81.1\% & 45.6\% & 35.6\% & 18.3\% & 0.6\%\\\bottomrule
\end{tabular}%
}
\caption{This table shows how often each point in the Likert scale was chosen by 3 annotators for the pairs of original and paraphrased texts. Twenty text pairs are randomly selected for each lexical code (L). 81.8\% of the time, our model \model\ provides a paraphrase which is nearly equivalent to the input in terms of semantic meaning.}
\label{tab:humanevalpercentage}
\end{table*}

\section{Experiments measuring intrinsic paraphrase generation quality}
\label{sec:intrinsic-evaluation}

Our experiments in \sectionref{sec:attacks} and \sectionref{sec:defense-results} focused on attacking AI-generated text detectors with paraphrases and defending against these paraphrase attacks. We used \model\ as the underlying paraphrase generation model for all of these experiments. Are \model's paraphrases actually good enough to make the attack worthwhile, and can simpler paraphrasers be just as effective as \model? In this section, we conduct careful ablation experiments (\sectionref{sec:ablation-dipper}) and human evaluations (\sectionref{sec:human-evaluation}) to validate the effectiveness of \model\ at preserving the semantics of the input generation. Our results show that \model\ effectively leverages surrounding context to paraphrase multiple sentences while preserving input semantics.

\subsection{Ablation studies on \model}
\label{sec:ablation-dipper}

In this section, we perform automatic evaluations to confirm the efficacy of \model\ as a paraphraser. From a survey of existing paraphrasers that we carry out in \appendixref{sec:appendix:papersurvey}, \model\ possess two unique features that differentiate it from other paraphrasers: (1) its ability to leverage context from \emph{outside} of the text to be paraphrased (such as the prompt); and (2) its ability to paraphrase multiple sentences at once. How useful are these features while paraphrasing long sequences of text?

To answer this question, we first train an ablated version of \model\ by constructing a training dataset (\sectionref{sec:paraphrase-data-build}) without any left or right context, and then fine-tuning T5-XXL using the same hyperparameters as in \sectionref{sec:model}. We call this model \model-no-ctx. We paraphrase 1K open-ended generations from GPT2-XL using both \model\ and \model-no-ctx, using each of the four configurations of diversity control codes studied in this paper. We then evaluate the quality of the paraphrased text using three metrics: (1) GPT3.5-davinci-003 perplexity~\citep{brown2020language} of the prompt concatenated with the paraphrased continuation; (2) \textsc{RankGen} compatibility between the prompt and the paraphrased continuation~\citep{krishna-etal-2022-rankgen}; and (3) unigram token overlap between the paraphrased continuation and the prompt.

\vspace{0.05in}

\noindent \textbf{Contextual paraphrasing leads to higher quality paraphrases}. In \tableref{tab:paraphrase-ablations} (Experiment 1), we observe that across all four control code configurations and all three metrics, paraphrases from \model\ are preferred over paraphrases from \model-no-ctx. Specifically, with the lexical and order control codes set to 60\% (most diverse), \model\ paraphrases are preferred by GPT3.5 perplexity 75\% of the time compared to non-contextual paraphrases (average perplexity drop of 12.9 vs 15.0).

\vspace{0.05in}




\noindent \textbf{Paraphrasing multiple sentences at a time is better than paraphrasing individual sentences.} Next, we use our \model-no-ctx model to compare two settings: paraphrasing 3 sentences at a time vs paraphrasing 1 sentence at a time before concatenating. We hypothesize that the former will produce higher quality paraphrases since we expect it to better connect discourse elements across the text. Indeed, in \tableref{tab:paraphrase-ablations} (Experiment 2) across all control codes, GPT3.5 and \textsc{RankGen} usually prefer multi-sentence paraphrases over the single-sentence baseline. This preference is 79\% or higher for all control codes when evaluating with GPT-3.5 perplexity, reaching 85\% for L60,O60.

\vspace{0.1in}

\noindent \textbf{\model\ paraphrases are close to the unperturbed GPT-2 XL generations}. Finally, we compare \model\ with the original GPT2-XL generations (without paraphrasing) on the same three metrics. While we expect metrics to prefer non-paraphrased text, a strong paraphraser will produce text that is close to the original in terms of these metrics.
\tableref{tab:paraphrase-ablations} (Experiment 3) confirms our hypothesis: at L20, \textsc{RankGen} has a 50-50 preference between the two outputs, while GPT3.5 prefers the non-paraphrased generations just 61\% of the time, with an average perplexity gain of just 0.4 (11.1 to 11.5). At more diverse control codes, preference for GPT2-XL generations does go up (58\% \textsc{RankGen}, 73\% GPT3.5 for L60), but absolute scores continue to be close (11.1 vs 12.3 GPT-3.5 perplexity). Note that while all of these ablations use just a single paraphrase sample, it is easy for an attacker to obtain multiple samples from \model\ and choose the sample that maximizes these metrics (as discussed in \sectionref{sec:attack-expts}). 

\subsection{Human evaluation of semantic preservation using \model}
\label{sec:human-evaluation}

The automatic semantic similarity scores in \tableref{tab:watermark-attacks} and \ref{tab:attacks-lfqa} indicate that \model~generates paraphrases that are faithful to the original input paragraphs. To confirm this result with human evaluation, we hire three native English teachers and/or editors on Upwork\footnote{\url{https://www.upwork.com}} to evaluate the semantic fidelity of the paraphrases.
As human evaluation is expensive, we focus on the impact of the lexical diversity. We evaluate paraphrases with the lexical codes $L20$, $L40$, and $L60$, corresponding to moderate, medium, and high lexical diversity. For each code, we randomly select 20 original and paraphrased text pairs for our annotators to evaluate semantic similarity on a five-point Likert scale (see \appendixref{appendix:HumanEval} for more details of the scale and interface).\footnote{The original texts are preceded by their context. The annotators see the same amount of text as \model.} The rate at which each point on the scale is chosen is reported in \tableref{tab:humanevalpercentage}. Annotators also must provide free-form comments justifying their ratings. Over 80\% of the time, our annotators rate \model's paraphrases as nearly equivalent (4 out of 5) or approximately equivalent (5 out of 5). Details about inter-annotator agreement are in \appendixref{appendix:HumanEval}.

A qualitative analysis of the free-form annotator comments reveals systemic strengths and shortcomings of \model. We provide a brief analysis below and include three representative examples in \tableref{tab:cherry-pickedexamples}. A  comprehensive discussion with more examples can be found in \appendixref{appendix:HumanEval}. 

\vspace{0.05in}

\noindent\textbf{Strengths:} The first case in \tableref{tab:cherry-pickedexamples} exemplifies \model's ability to leverage information from context to increase diversity while maintaining coherence. No prior paraphraser (see \tableref{tab:prior-eval-metric} for a list) is capable of doing so. The paraphrase in the second row highlights \model's ability to make significant changes to original texts with a high lexical diversity code while preserving their semantic meaning.

\vspace{0.05in}

\noindent\textbf{Shortcomings:} One shortcoming of \model~is that, when the original text contains unique proper nouns, such as \textit{Homing Attack} and \textit{Slide Attack} in row 3, applying \model\ with a high lexical code can modify such nouns. This causes an undesirable semantic drift, as the original proper nouns must be maintained in any valid paraphrase. However, this shortcoming can be mitigated by decreasing the lexical code as shown in \appendixref{appendix:HumanEval}. 

% Overall, our human study confirms that \model~is good at meaning preservation and introducing diversity. With help of the controllable lexical and reordering codes, \model\ produces paraphrases of high quality, which makes 





% \begin{table*}[t!]
% \small
% \begin{center}
% \begin{tabular}{ lrrrrrrrr } 
%  \toprule
%   & \multicolumn{2}{c}{\textsc{RankGen-XL-PG19}} & \multicolumn{3}{c}{\textsc{GPT2-XL} perplexity} & \multicolumn{3}{c}{\textsc{1-gram}} (with context tokens only)\\
%  \cmidrule(lr){2-3}  \cmidrule(lr){4-6} \cmidrule(lr){7-9}
%  Model & prefix-ctx & suffix-ctx & prefix-ctx & suffix-ctx & both-ctx & prefix-ctx & suffix-ctx & any-ctx \\
%  \midrule
%  \multicolumn{2}{l}{(\emph{Decode $p$ = 0.75})} \vspace{0.15cm}\\
%  \model-ctx & \textbf{68}\% {\scriptsize (9.0)} & \textbf{70}\% {\scriptsize (8.7)} & \textbf{73}\% {\scriptsize (23.7)} & \textbf{79}\% {\scriptsize (29.8)} & \textbf{77}\% {\scriptsize (30.0)} & \textbf{41}\% {\scriptsize{(3.8)}} & \textbf{40}\% {\scriptsize{(3.8)}} & \textbf{52}\% {\scriptsize{(6.4)}} \\
%  \model-no-ctx & 31\% {\scriptsize (6.4)} & 29\% {\scriptsize (6.6)} & 26\% {\scriptsize (37.0)} & 20\% {\scriptsize (35.0)} & 22\% {\scriptsize (34.5)} & 23\% {\scriptsize (2.4)} & 24\% {\scriptsize{(2.4)}} & 27\% {\scriptsize{(4.1)}} \\
%  tie & 1\% \phantom{{\scriptsize (4.4)}} & 1\% \phantom{{\scriptsize (4.4)}} & 1\% \phantom{{\scriptsize (14.4)}} & 1\% \phantom{{\scriptsize (14.4)}} & 1\% \phantom{{\scriptsize (14.4)}} & 36\% \phantom{{\scriptsize (4.4)}} & 36\% \phantom{{\scriptsize (4.4)}} & 21\% \phantom{{\scriptsize (4.4)}}  \\ 
%  \midrule
%  \multicolumn{2}{l}{(\emph{Decode $p$ = 0.90})} \vspace{0.15cm} \\
%  \model-ctx & \textbf{69}\% {\scriptsize (8.8)} & \textbf{70}\% {\scriptsize (8.5)} & \textbf{73}\% {\scriptsize (26.4)} & \textbf{80}\% {\scriptsize (30.0)} & \textbf{78}\% {\scriptsize (30.9)} & \textbf{46}\% {\scriptsize (4.4)} & \textbf{44}\% {\scriptsize (4.3)} & \textbf{55}\% {\scriptsize (7.4)} \\
%  \model-no-ctx & 31\% {\scriptsize (6.1)} & 30\% {\scriptsize (6.3)} & 27\% {\scriptsize (40.8)} & 20\% {\scriptsize (35.3)} & 22\% {\scriptsize (35.7)} & 27\% {\scriptsize (2.9)} & 27\% {\scriptsize (3.0)} & 30\% (\scriptsize{4.9}) \\
%  tie & 0\% \phantom{{\scriptsize (8.8)}} & 0\% \phantom{{\scriptsize (8.8)}} & 0\% \phantom{{\scriptsize (18.8)}} & 0\% \phantom{{\scriptsize (18.8)}} & 0\% \phantom{{\scriptsize (18.8)}} & 27\% \phantom{{\scriptsize (2.9)}} & 29\% \phantom{{\scriptsize (4.4)}}  & 15\% \phantom{{\scriptsize (4.4)}} \\
% \bottomrule
% \end{tabular}
% \end{center}
% \vspace{-0.1in}
% \caption{Intrinsic paraphrase generation quality. Results show that paraphrasing with context is more effective than paraphrasing without context.\kkcomment{todo, measuring usage of context, add discussion on higher code giving higher performance} \kkcomment{use GPT-3 perplexity} \kkcomment{does rankgen prefer original vs paraphrase?}}
% \vspace{-0.1in}
% \label{tab:context-vs-no-context}
% \end{table*}

% \begin{table*}[t!]
% \small
% \begin{center}
% \begin{tabular}{ lrrrrrrrrrrr } 
%  \toprule
%  Detector $\rightarrow$ & & \multicolumn{2}{c}{Watermarks} & \multicolumn{2}{c}{DetectGPT}  & \multicolumn{2}{c}{OpenAI} & \multicolumn{2}{c}{GPTZero} & \multicolumn{2}{c}{RankGen}  \\
%  & & \multicolumn{2}{c}{\shortcite{kirchenbauer2023watermark}} & \multicolumn{2}{c}{\shortcite{mitchell2023detectgpt}}  & \multicolumn{2}{c}{\shortcite{AITextClassifier}} & \multicolumn{2}{c}{\shortcite{GPTZero}} & \multicolumn{2}{c}{\shortcite{krishna-etal-2022-rankgen}}\\

%   \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
%  Metric $\rightarrow$ & S $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ \\
 
%  \midrule
%  \multicolumn{9}{l}{\emph{Wikipedia prompts (300 tokens generated with $p=0.9$)}}\vspace{0.15cm} \\
%  GPT2-1.5B  & 100.0 & 100.0 & 0.0 & 53.1 & 46.9 & 18.9 & 81.1 & 13.9 & \textbf{86.1} & 14.1 & 85.9 \\
%  % sim scores = , , 99.5
%  + \model\ 20L &  96.3 & 96.1 & 3.4 & 15.4 & 81.2 & 12.0  & 87.6 & 9.1 & 85.5 \\
%  % sim scores = , , 97.4
%  + \model\ 40L  & 92.3 & 80.3 & 17.9 & 6.9 & \textbf{85.7} & 9.6 & \textbf{88.1}  & 7.3 & 83.6  \\
%   % sim scores = 89.6, , 90.5, 80.9
%  + \model\ 60L & 82.6 & \textbf{64.3} & \textbf{33.6} & \textbf{4.7} & 78.2 & \textbf{6.6} & 84.2 & \textbf{7.1} & 74.2 & 24.3 & 58.4 \\
%  \midrule
%  OPT-13B & 100.0 &  &  & 9.4 & 90.6 & 11.6 & 88.4 & 8.7 & 91.3 & \textbf{3.6} & \textbf{96.4} \\
%  + \model\ 20L & 96.2 &  &  & 1.7 & \textbf{94.7} & 8.6 & \textbf{91.0} & 5.4 & \textbf{91.4} & 7.0 & 87.5 \\
%  + \model\ 40L & 92.1  &  &  & 0.6 & 91.4 & 6.7 & 90.2 & \textbf{3.8} & 89.6 & 9.8 & 79.6 \\
%  + \model\ 60L & 82.1 &  &  & \textbf{0.3} & 81.7 & \textbf{5.2} & 84.5 & 6.3 & 75.5 & 12.6 & 66.9  \\
%  \midrule 
%  GPT-3.5-175B \\
%  davinci-003  & 100.0 & -&- & 2.0* & \textbf{98.0}* & 39.1 & 60.9 & \textbf{5.6} & \textbf{94.4} & \textbf{1.4} & \textbf{98.6}\\
%  + \model\ 20L & 95.6 & -&- & 0.5* & 93.5* & 27.7 & 70.1 & 2.0 & 93.2 & 2.3 & 93.9 \\
%  + \model\ 40L & 93.2 & -&- & 0.0* & 92.5* & 22.1 & 74.6 & 2.0 & 90.0 & 3.4 & 89.1 \\
%  + \model\ 60L & 88.1 & -&- & 0.5* & 88.0* & \textbf{15.9} & \textbf{75.4}& 3.6  & 82.8 & 4.8 & 80.6  \\
%  \midrule
%  % 1.7
%  Human Text & - & 1.0 & - & 1.0 & - & 1.0 & - & 1.0 & - & 1.0 & - \\
% \bottomrule
% \end{tabular}
% \end{center}
% \caption{Performance of various AI detection algorithms at a 1\% false positive rate before and after \model\ paraphrasing. As the lexical diversity (L) increases, detection rate (Acc) decreases across algorithms, at a small cost of semantic similarity (S).\kkcomment{todo - get more data points for GPTZero}}
% \label{tab:watermark-attacks}
% \end{table*}