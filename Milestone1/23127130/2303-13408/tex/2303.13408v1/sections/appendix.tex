\section*{Appendix}

\section{Related work for discourse paraphrasing}





\subsection{Survey of paraphrase generation papers}
\label{sec:appendix:papersurvey}

As an important NLP task, paraphrasing has attracted much attention. Many models have been proposed to improve the quality of paraphrases. To position our model~\model~and highlight its strengths, we conduct a survey of paraphrase generation papers from 2018 to 2022 (\tableref{tab:prior-eval-metric}) and focus on the following four aspects:

\begin{enumerate}[label=(\arabic*)]
    \item Whether a model can paraphrase a paragraph at once,
    \vspace{-5pt}
    
    \item whether a model can merge or split consecutive sentences when appropriate,
    \vspace{-5pt}
    
    \item whether a model leverages context surrounding an input sentence when paraphrasing,
    \vspace{-5pt}
    
    \item whether a model provides control knobs for users to customize the output diversity.
    \vspace{-5pt}
\end{enumerate}

The survey shows that only three out of 25 papers mentioned that their model can paraphrase more than one sentence (but not necessarily at once). None of them enables their model to merge or split sentences when paraphrasing. No model uses information from context surrounding an input sentence during inference time. Finally, 14 papers offer ways for users to customize the diversity of paraphrases. However, most diversity control methods such as constituency parses or exemplars may not be straightforward and intuitive to end-users as the scalar control knobs in \model. 

In contrast to the papers in the survey, \model~nicely combines all desiderata into one model and offers intuitive control knobs for lexical and syntactic diversity. Automatic and human evaluation show that \model~can efficiently leverage context information and reorganize sentences while having high fidelity in meaning (\sectionref{sec:intrinsic-evaluation}). 

\subsection{Other related work}

In this section we discuss a few additional less related papers which were not included in our survey in \sectionref{sec:appendix:papersurvey}. Our discourse paraphraser is closely related to work on contextual machine translation, where source/target context is used to improve sentence-level machine translation~\citep{house2006text,jean2017does,wang-etal-2017-exploiting-cross,tiedemann-scherrer-2017-neural,kuang-etal-2018-modeling,agrawal2018contextual,miculicich-etal-2018-document,zhang-etal-2018-improving,xiong2019modeling,jean2019fill,voita-etal-2019-context,yin2021does,mansimov-etal-2021-capturing}. Prior work has shown that context helps with anaphora resolution~\citep{voita-etal-2018-context}, deixis, ellipsis, and lexical cohesion~\citep{voita-etal-2019-good}. Efforts to make paraphrase generation more contextual have been quite limited. A few efforts have attempted to use sentence level context to paraphrase phrases~\citep{connor2007context,max2009sub}, and dialogue context to paraphrase individual dialogues in a chat~\citep{garg2021unsupervised}.

Our work is also related to efforts in text simplification to go beyond a sentence, by collecting relevant datasets~\citep{xu2015problems,devaraj-etal-2021-paragraph} and building unsupervised algorithms~\citep{laban-etal-2021-keep}. Note that our work focuses on a general-purpose paraphrasing algorithm and is not tied to any particular style, but could be utilized for document-level style transfer using techniques like~\citet{style20,krishna2022few}. Similar efforts have also been undertaken in machine translation,~\citep{emnlp-2019-discourse,junczys-dowmunt-2019-microsoft,maruf2021survey}, attempting to translate paragraphs / documents at once.



\section{Human evaluation details}\label{sec:appendix:likert}\label{appendix:HumanEval}

In this appendix section, we provide further information of our human evaluation study.

The evaluation is conducted on the platform Label Studio~\citep{LabelStudio}.\footnote{\url{https://labelstud.io/}} \figureref{fig:label_studio_interface} shows the interface of our annotation platform. We estimated that the evaluation of each paraphrase takes 1.5 to 2 minutes. As such, we pay \$15 as a base rate with a bonus for the reasonable extra time that the annotators spend on the tasks. For the evaluation task, we randomly select 20 paraphrases for each of the three lexical codes. Among the 60 original text and paraphrase pairs, the three annotators agreed on their choice 28.3\% of the time, and 60\% of the time the point they chose on the scale differs by 1. 


\tableref{tab:cherry-pickedexamples-appendix} provides two representative examples for each lexical code that is evaluated in our human study. Qualitative analysis of the annotator comments reveals strengths and shortcomings of \model.

\noindent\textbf{Strengths} First, looking at the fifth example in \tableref{tab:cherry-pickedexamples-appendix} with a high lexical diversity code 60, we see that \model~is able to preserve the semantic meaning of the original text while making significant changes. Second, row 3 shows us that \model~is able to leverage the information from the context to increase the diversity of the paraphrase and keep it coherent. The same is observed in row 2 where \model~uses the context to interchange \textit{he} and \textit{Churchill}. A paraphrase model without looking into context will have great difficulty in doing this.

\noindent\textbf{Qualitative shortcomings}: The first shortcoming is that, when the original text contains new created proper names (unlike common people and country names), such as the ones in row 6 (\textit{Homing Attack} and \textit{Slide Attack}), a high lexical code has a tendency to change such nouns, leading to the result that one of our annotators deems it to be only topically related to the original. However, this shortcoming can be overcome by decreasing the lexical code, which a user can choose from a continuous range (from 0 to 100). For instance, in row 1 with \texttt{lex=20}, the songs' names \textit{Mâ€™s Confession} and \textit{Gone Fishing} are kept intact. Another shortcoming is that \model~occasionally omits content from an original text. While in some cases such removal is acceptable (see row 6), in other cases it causes significant change in the meaning of the text (see row 4). However, the former case can be overcome by paraphrasing a shorter paragraph at a time.

Overall, the human study shows that \model~performs well at preserving the semantic meaning of original texts while introducing both semantic and syntactic diversity. Because \model~provides user-friendly controllabilty of output diversity, a user can adjust the control code to find the most suitable paraphrase for their need.


\begin{table*}[h!]
\begin{center}
\begin{tabular}{@{}lcccc@{}} 
 \toprule
Paper & Multi-sentence & Merge / Splits & Contextual & Diversity Control \\ 
\midrule
\citet{iyyer-etal-2018-adversarial} & \xmark & \xmark & \xmark & \scalebox{0.95}{Constituency parse} \\
\citet{li-etal-2018-paraphrase} & \xmark & \xmark & \xmark & \xmark \\
\citet{roy-grangier-2019-unsupervised} & \xmark & \xmark & \xmark & \xmark \\
\citet{witteveen-andrews-2019-paraphrasing} & \textcolor{ForestGreen}{\cmark} & \textbf{?} & \xmark  & \xmark \\
\citet{kumar-etal-2019-submodular} & \xmark & \xmark & \xmark & \xmark \\
\citet{hu2019parabank} & \xmark & \xmark & \xmark & \scalebox{0.95}{Decoding constraints}\\
\citet{chen-etal-2019-controllable} & \xmark & \xmark & \xmark & \scalebox{0.95}{Exemplar}\\
\citet{li-etal-2019-decomposable} & \xmark & \xmark & \xmark & \scalebox{0.95}{Granularity control}$^1$\\
\citet{goyal2020neural} & \xmark & \xmark & \xmark & \scalebox{0.95}{Exemplar} \\
\citet{lewis2020pre} & \textcolor{ForestGreen}{\cmark} & \textbf{?} & \xmark & \xmark \\
\citet{thompson-post-2020-paraphrase} &  \xmark &  \xmark & \xmark & \scalebox{0.95}{$n$-gram overlap} \\
\citet{kumar2020syntax} &  \xmark &\xmark & \xmark & \scalebox{0.95}{Exemplar}\\
\citet{kazemnejad-etal-2020-paraphrase} & \xmark & \textbf{?} & \xmark & \xmark \\
\citet{style20} & \xmark & \xmark & \xmark & \xmark \\
\citet{rajauria2020paraphrase} & \xmark & \xmark & \xmark & \xmark \\
\citet{meng-etal-2021-conrpg} & \xmark & \xmark & { } \xmark$^2$ & \scalebox{0.95}{Diversity score}$^3$\\
\citet{huang-chang-2021-generating} & \xmark & \xmark & \xmark & \scalebox{0.95}{Constituency parse}\\
\citet{lin-etal-2021-towards-document-level} & \textcolor{ForestGreen}{\cmark} & \xmark & \xmark & \xmark \\
\citet{goutham2021paraphrase} & \xmark & \xmark & \xmark & \xmark \\
\citet{prithivida2021parrot} & \xmark & \xmark & \xmark & \scalebox{0.95}{Binary} \\
\citet{dopierre-etal-2021-protaugment} & \xmark & \xmark & \xmark & \scalebox{0.95}{$n$-gram} \\
\citet{bandel-etal-2022-quality} & \xmark & \xmark & \xmark & Control code$^4$\\
\citet{hosking-etal-2022-hierarchical} & \xmark & \xmark & \xmark &  Syntactic sketch\\
\citet{yang2022gcpg} & \xmark & \xmark & \xmark & Examplar$+$Keywords\\
\citet{xie-etal-2022-multi} & \xmark & \xmark & \xmark & \xmark \\
\midrule
\scalebox{1.1}{\textbf{\model~(ours)}} & \scalebox{1.15}{\textcolor{ForestGreen}{\cmark}} & \scalebox{1.11}{\textcolor{ForestGreen}{\cmark}} & \scalebox{1.11}{\textcolor{ForestGreen}{\cmark}} & \scalebox{1.11}{\textcolor{ForestGreen}{\cmark}} \\
\bottomrule
\end{tabular}
\end{center}
\caption{The table shows the result of our survey of paraphrase generation papers from 2018 to 2022. We focus on four aspects: (1) whether a model can paraphrase multiple sentences at once, (2) whether a model is able to merge or split an input sentence when appropriate, (3) whether a model takes context surrounding the input sentence into consideration when paraphrasing, and (4) whether a model enables users to control the semantic and syntactic diversity of paraphrases. $^1$Granularity levels are \textit{word}, \textit{phrase}, and \textit{sentence}. $^2$\citet{meng-etal-2021-conrpg} use context for their dataset construction, but do not leverage it during training/inference. $^3$The diversity score is a combination of the unigram Jaccard distance and the relative position change for unigrams. $^4$The code is represented by a three dimensional vector corresponding to semantic
similarity as well as syntactic and lexical distances between the input and output sentences.}
\label{tab:prior-eval-metric}
\end{table*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[scale=0.5]{figures/label_studio_screen_shot.png}
    \caption{The interface of the annotation platform used in our human study}
    \label{fig:label_studio_interface}
\end{figure*}

\begin{table*}[!ht]
\footnotesize
\centering
\resizebox{0.98\textwidth}{!}{%
\begin{tabular}[b]{@{}p{0.5cm}p{7cm}p{5.5cm}p{4cm}@{}}
\toprule
  \textbf{L} &
  \textbf{Original} &
  \textbf{Paraphrase} &
  \textbf{Annotator Comment}\\\midrule
  20 &
  \textbf{The first, originally used as Renard's theme, is heard during the mammoth "Antonov" cue on the recording, and is written for piano. The second new theme, used in the "Christmas in Turkey" track of The World Is Not Enough, is reused in the "Going Down Together" track.} Both themes were \textcolor{OrangeRed}{later} adapted by David Arnold and \textcolor{OrangeRed}{featured} in the film\textcolor{OrangeRed}{'s} score. The first theme \textcolor{OrangeRed}{appears} in the \textcolor{OrangeRed}{cue} "M's Confession" and the second \textcolor{OrangeRed}{theme appears} in the \textcolor{OrangeRed}{cue} "Gone Fishing". &
  Both themes were \textcolor{ForestGreen}{subsequently} adapted by David Arnold and \textcolor{ForestGreen}{used} in the film score. The first theme \textcolor{ForestGreen}{is used} in the "M's Confession" \textcolor{ForestGreen}{cue}, and the second in the "Gone Fishing" \textcolor{ForestGreen}{cue.} &
  \textbf{5---Approx. equivalent}\newline While the paraphrase is worded in a more simplistic way than the original and has shortened clauses, the information presented in both is the same.\\\midrule
  20 &
  \textbf{Churchill argues that the United States instituted blood quantum laws based upon rules of descendancy [\ldots]} He has argued that the United States has used them to strip \textcolor{OrangeRed}{Indigenous} people of their rights, \textcolor{OrangeRed}{land} and culture in order to benefit those in power. \textcolor{OrangeRed}{He contends} that these laws were designed to create \textcolor{OrangeRed}{a divide} among \textcolor{OrangeRed}{Indigenous people, making} them easier to control and manipulate. \textcolor{OrangeRed}{Churchill} has \textcolor{OrangeRed}{further} argued that this manipulation continues today, \textcolor{OrangeRed}{as Indigenous} people are often forced to deny their heritage in order to have access to resources and opportunities that are denied to them \textcolor{OrangeRed}{as a result} of their heritage. &
  He has argued that the United States has used them to strip \textcolor{ForestGreen}{indigenous} people of their rights, \textcolor{ForestGreen}{land,} and culture in order to benefit those in power. \textcolor{ForestGreen}{Churchill also argues} that these laws were designed to create \textcolor{ForestGreen}{divisions} among \textcolor{ForestGreen}{indigenous people in order to make} them easier to control and manipulate. \textcolor{ForestGreen}{He} has \textcolor{ForestGreen}{also} argued that this manipulation continues today, \textcolor{ForestGreen}{in that indigenous} people are often forced to deny their heritage in order to have access to resources and opportunities that are denied to them \textcolor{ForestGreen}{because} of their heritage. &
  \textbf{5---Approx. equivalent}\newline The paraphrase retains all the information of the original, but uses synonymous language in several places. Also, the paraphrase replaces the pronoun "he" with "Churchill" and vice versa.\\\midrule
  40 &
  \textbf{When tailoring his 1970 composition "I'm the Greatest" for Starr to record on the 1973 album Ringo, Lennon referenced the song title with the lines [$\ldots$]} Lennon's original \textcolor{OrangeRed}{line} was changed to "Back off Boogaloo" to \textcolor{OrangeRed}{reference the song's title, creating} the line \textcolor{OrangeRed}{that} Starr sang \textcolor{OrangeRed}{in} the finished \textcolor{OrangeRed}{version.} &
  Lennon's original \textcolor{ForestGreen}{reference to "I'm the Greatest"} was changed to "Back off Boogaloo" to \textcolor{ForestGreen}{create} the line Starr sang \textcolor{ForestGreen}{on} the finished \textcolor{ForestGreen}{recording.} &
  \textbf{4---Nearly equivalent}\newline The original does not specify that the line in question was "I'm the Greatest." However, this can be inferred from the rest of the passage. Otherwise, the paraphrase and the original are quite similar.\\\midrule
  40 &
  \textbf{Some monuments were vandalized and some were moved to new locations. [\ldots]} In some cases, \textcolor{OrangeRed}{the} monuments were moved to new locations. \textcolor{OrangeRed}{For example, the stairway of the} Great Pyramid of \textcolor{OrangeRed}{Cholula} in \textcolor{OrangeRed}{Mexico} was \textcolor{OrangeRed}{relocated} in the 19th century and then returned to its original \textcolor{OrangeRed}{location} in the \textcolor{OrangeRed}{20th century. Other monuments} were destroyed during wars or religious \textcolor{OrangeRed}{conflicts,} such as the Buddhas of Bamiyan in Afghanistan, which were destroyed by the Taliban in 2001. &
  In some cases, monuments were moved to new locations. \textcolor{ForestGreen}{The} Great Pyramid of \textcolor{ForestGreen}{Cholula,} in \textcolor{ForestGreen}{Mexico,} was \textcolor{ForestGreen}{moved} in the 19th century and then returned to its original \textcolor{ForestGreen}{place} in the \textcolor{ForestGreen}{20th. Others} were destroyed during wars or religious \textcolor{ForestGreen}{controversies,} such as the Buddhas of Bamiyan in Afghanistan, which were destroyed by the Taliban in 2001. &
  \textbf{3---Somewhat equivalent}\newline Only one major change in this short text, but I feel it's significant. The original text speaks of the moving of a pyramid's staircase, but the paraphrased text implies that the entire pyramid was moved and then replaced. This massively changes the meaning of this text. \\\midrule
  60 &
  \textbf{Heaving wildly, the Hunyani began to descend rapidly. [\ldots]} The crew \textcolor{OrangeRed}{was unable to} regain \textcolor{OrangeRed}{control} and the Hunyani \textcolor{OrangeRed}{crashed} into the ground \textcolor{OrangeRed}{at 17:14, killing all onboard.}  The \textcolor{OrangeRed}{crash} was investigated by the Civil Aviation Authority of \textcolor{OrangeRed}{Zimbabwe and the investigation concluded} that the crash was caused by a combination of pilot error, \textcolor{OrangeRed}{fatigue}, and \textcolor{OrangeRed}{weather conditions.} The \textcolor{OrangeRed}{pilot} had \textcolor{OrangeRed}{failed to} properly \textcolor{OrangeRed}{assess} the weather \textcolor{OrangeRed}{conditions before takeoff, instead relying} on a forecast from a private \textcolor{OrangeRed}{weather} service. The weather \textcolor{OrangeRed}{conditions worsened as the plane flew and with the pilots already fatigued, they were unable to appropriately react to the changing conditions. The combination of these factors led to the plane losing control and crashing.} &
  The crew \textcolor{ForestGreen}{could not} regain \textcolor{ForestGreen}{control,} and \textcolor{ForestGreen}{at 17:14} the Hunyani \textcolor{ForestGreen}{plunged} into the ground \textcolor{ForestGreen}{and all those on board were killed.} The \textcolor{ForestGreen}{accident} was investigated by the Civil Aviation Authority of \textcolor{ForestGreen}{Zimbabwe, which found} that the crash was caused by a combination of pilot error, \textcolor{ForestGreen}{fatigue} and \textcolor{ForestGreen}{the weather.} The \textcolor{ForestGreen}{pilots} had \textcolor{ForestGreen}{not} properly \textcolor{ForestGreen}{examined} the weather \textcolor{ForestGreen}{before take-off, and instead had relied} on a forecast from a private \textcolor{ForestGreen}{meteorological} service. The weather \textcolor{ForestGreen}{deteriorated during the flight, and the exhausted pilots were unable to react properly to the deteriorating conditions.} & \textbf{5---Approx. equivalent}\newline Well chosen words make the alternate version different but retain the original meaning well. The only difference is that the paraphrased version does not have the concluding sentence, but it does not alter the meaning in any way as the facts have already been included previously.\\\midrule
  60 &
  \textbf{Sonic's main means of attack is the Spin Attack, in which he curls into a ball and rapidly spins his body, damaging enemies and certain obstacles upon collision. This may be performed by jumping or by rolling on the ground.} \textcolor{OrangeRed}{Sonic can also} use a variety of other \textcolor{OrangeRed}{moves}, such as the \textcolor{OrangeRed}{Homing Attack, Slide Attack, and Wall Jump. He also has access to power-ups like the Speed Shoes, Invincibility, and the Super Peel Out. In some games, Sonic can also use} special items or vehicles to traverse \textcolor{OrangeRed}{levels.} &
  \textcolor{ForestGreen}{Also, the character can} use a variety of other \textcolor{ForestGreen}{attacks,} such as the \textcolor{ForestGreen}{homing attack, the slide attack, and the wall jump. In addition, he can acquire power-ups such as the Speed Boost, Super Speed, and the invincibility meter, and in certain games, he may make use of} special items or vehicles to traverse \textcolor{ForestGreen}{the world.} &
  \textbf{2---Topically related}\newline In the second part of the paraphrase, the writer ignores the actual moves and skills of Sonic and invents some entirely different ones that not mentioned in the original text. The method of performing the attack moves has also been missed out.\\\bottomrule
\end{tabular}%
}
\caption{Representative model outputs of each lexical code with comments from the annotators (native English teachers/editors). The texts in bold in the original texts are the context. \textcolor{OrangeRed}{Red words} are the content being changed in the original text and \textcolor{ForestGreen}{green words} are the changed content in the paraphrases.}
\label{tab:cherry-pickedexamples-appendix}
\end{table*}
%https://docs.google.com/spreadsheets/d/10LtBZLLuTut2kDdyiCkCkYWovnmgTESOABqse-P30EU/edit?usp=sharing


\section{ROC curves at different FPR}
\label{appendix:more-roc-curves}

See \figureref{fig:auc-plots-2}.

% See \tableref{tab:likertscale}.

% \begin{table*}[h]
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}l@{}}
% \toprule
% \multicolumn{1}{c}{\textbf{Which best describes the quality of the paraphrase?}} \\\midrule
% 5. Approximately equivalent: the paraphrase preserves the meaning of the source but differs in words and/or structure.       \\
% 4. Nearly equivalent: the paraphrase preserves most information in the source but differs in some minor factual details.     \\
% 3. Somewhat equivalent: the paraphrase preserves some information in the source but differs in certain significant ways. \\
% 2. Topically related: the paraphrase is topically related to the source but most information in the source is not preserved. \\
% 1. Not topically related: the paraphrase is not topically related to the source and preserves no information.\\\bottomrule
% \end{tabular}%
% }
% \caption{Description of each point of the five-point Likert scale used in our human evaluation study}
% \label{tab:likertscale}
% \end{table*}


% \section{Metrics to evaluate inherent paraphrase quality}

% \kkcomment{Marzena: discourse deixis, ellipsis, cataphora / anaphora?.. Robert-Alain de Beaugrande (intro to text linguistics): Compacting patterns: pro-forms; anaphora and cataphora; ellipsis; trade-off between compactness and clarity. Signalling relations: tense and aspect; updating; junction: conjunction, disjunction, contrajunction, and subordination; modality. Functional sentence perspective.a lot of these seems relevantish and may pose a problem when paraphrasing (including major ordering changes) is done without the regard to the context... "definition of paraphrasing"}

% \subsection{Modeling paraphrase distribution (\ppl)}

% with / without context perplexity evaluation (1.63 loss without context, 1.234 loss with context)

% \subsection{Semantic Similarity (\simmetric)}

% A valid paraphrase should approximately convey the same meaning as the input. We measure semantic similarity between the input and output using the embedding-based SIM model from~\citet{wieting-etal-2019-beyond}. SIM is trained on backtranslated paraphrase data from ParaNMT-50M~\citep{wieting-gimpel-2018-paranmt}, and has performed well on semantic textual similarity (STS) benchmarks in SemEval workshops~\citep{agirre-etal-2016-semeval} \kkcomment{John, could you confirm the last line?}.

% \subsection{Lexical and Syntactic Diversity (adherence to control values)}

% A unique aspect of our paraphraser is the ability to control the lexical diversity and content ordering using input control codes ($L, K$ in \sectionref{sec:model}). How often does the model adhere to these control codes? We measure the lexical diversity and content reordering of our generated paraphrases compared to the input, and then measure the Kendall Tau correlation~\citep{kendall1938new} with the input control codes at an instance level.

% \subsection{Coherence}
% \label{sec:coherence}

% Our paraphraser can not only perform lexical substitutions, but also re-order the content in the input. However, a trivial alternative is to simply perform sentence-by-sentence paraphrasing, and change the order of sentences.\kkcomment{also talk about merges / splits here, maybe measure \% of cases where number of sentences change?}\kkcomment{metric could be some kind of ppl test against baseline which simply does sentence-by-sentence non-contextual paraphrasing and then}

% \subsection{Measuring context utilization}
% \label{sec:eval-context}

% Our paraphraser is contextual --- it uses the text surrounding a given sequence to help paraphrase generation. How much does this context assist in paraphrase generation? We conduct an ablation study where we train a model on the same dataset, but without any context. \kkcomment{measure perplexity using same model, using independent LM, ROUGE scores with reference, fine-grained analysis, A/B tests (quality / fit to context [MASK])}

% \subsection{Human evaluation}\label{sec:human-evaluation}

% 3 A/B tests
% \begin{itemize}
%     \item contextual paraphrase vs no-context paraphrase (measures context)

%     \item end-to-end paragraph paraphrase vs (sentence-level paraphrase and then reorder). (use paragraphs that are 2-3 sentences) (do alignment between our model output and original paragraph)
    
%     \item diverse vs not diverse (measures control codes) (maybe not necessary, instead we can do sem. similarity eval and then use auto eval to measure diversity)
% \end{itemize}

% 1 absolute similarity eval (3 pt scale): paraphrase vs ungrammatical paraphrase vs not paraphrase

% \kkcomment{probability of input given output}

% \section{Details of Human Evaluation Experiments}\label{appendix:HumanEvalDetails}

% To evaluate whether the paraphraser benefits from having access to context, we conducted two human evaluation experiments. For each of them, we hired three experienced native English editors/proofreaders. We paid \$$1.8$ for each pair of paraphrases.\footnote{On average, evaluating one pair of paraphrases took 5 minutes. The instructions and the format of the annotation platform can be found \href{https://docs.google.com/presentation/d/10kRW4Lt7ZiF1EZygs4wuMwvIWKBFrlapMXr1gjSyBZI/edit?usp=sharing}{in this document}.} 
% We closely monitored the quality of the annotations and gave immediate feedback when an annotator does not follow the instructions. 

% Given a paragraph with a highlighted span to be paraphrased, an annotator was provided with two paraphrases, one was generated considering the context, and the other was not. The annotators' task was to compare the two paraphrases and choose the better one according to the following criteria:

% \begin{itemize}
%     \item how \textbf{diverse} is the \textbf{vocabulary}/\textbf{grammar} of the paraphrase compared to the original,
%     \vspace{-6pt}
    
%     \item how \textbf{well} does the paraphrase \textbf{fit} in the paragraph,
%     \vspace{-6pt}
    
%     \item whether the paraphrase \textbf{roughly preserves the meaning} of the original text.
% \end{itemize}

% \noindent Annotators were instructed to justify their choice using 3-4 sentences. Additionally, they were asked to indicate whether their selection was a difficult choice.

% In the first experiment, we fixed the \texttt{order} code to be $100$ (i.e., no sentence reordering) and used the  \texttt{lexical} code $20$, $40$, and $60$. For each \texttt{lex}--\texttt{order} code combination, we randomly selected twenty-one paragraphs and their corresponding paraphrases. In the second experiment, we used \texttt{order} = 40 to encourage the model to reorganize the sentences in a paraphrase, keeping the same \texttt{lexical} codes. The results of the two experiments can be found in Table \ref{tab:order100}.


% \begin{table}[]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}lrrrrrr@{}}
% \toprule
% \textbf{Code} & \multicolumn{2}{c}{\textbf{Rater 1}} & \multicolumn{2}{c}{\textbf{Rater 2}} & \multicolumn{2}{c}{\textbf{Rater 3}} \\ 
% (\texttt{lex}, \texttt{order}) & ctx & no ctx & ctx & no ctx & ctx & no ctx \\\midrule
% (20, 100) & 11 & 10 & 11 & 10 & 10 & 11 \\
% (40, 100) & 11 & 10 & 10 & 11 & 6 & 15 \\
% (60, 100) & 11 & 10 & 8 & 13 & 9 & 12 \\\midrule
% \textbf{Total} & 33 & 30 & 29 & 34 & 25 & 38 \\ 
% \textbf{Agreement} & \multicolumn{2}{l}{41.27\%} & \multicolumn{2}{l}{\textbf{Fleiss' $\kappa$}} & \multicolumn{2}{l}{0.212} \\\midrule\midrule
% % \textbf{Code} & \multicolumn{2}{c}{\textbf{Rater 1}} & \multicolumn{2}{c}{\textbf{Rater 2}} & \multicolumn{2}{c}{\textbf{Rater 3}} \\ 
% (\texttt{lex}, \texttt{order}) & ctx & no ctx & ctx & no ctx & ctx & no ctx \\\midrule
% (20, 40) & 11 & 10 & 15 & 6 & 10 & 11 \\
% (40, 40) & 12 & 9 & 11 & 10 & 8 & 13 \\
% (60, 40) & 17 & 4 & 7 & 14 & 11 & 10 \\\midrule
% \textbf{Total} & 40 & 23 & 33 & 30 & 29 & 34 \\ 
% \textbf{Agreement} & \multicolumn{2}{l}{33.33\%} & \multicolumn{2}{l}{\textbf{Fleiss' $\kappa$}} & \multicolumn{2}{l}{0.106} \\ \bottomrule
% \end{tabular}%
% }
% \caption{The table shows the counts of how many times the context and no context paraphrases are chosen in each code combination by each annotator. Fleiss' $\kappa$ = 0.212 indicates fair agreement. Fleiss' $\kappa$ = 0.106 is slight agreement.}
% \label{tab:order100}
% \end{table}

% \textbf{The results show that the task is subjective.} In the first experiment, the annotators agreed on the better paraphrase for only 26 out of 63 paragraphs. This number further decreased to 21 in the second experiment where sentence reordering is forced. Fleiss' $\kappa$ shows that the experiments reached only fair and slight agreement (0.212 and 0.106). The subjectivity is also reflected in the comments. We observe that, in the two experiments, 65\% of the time the paraphrases are shorter than the original text. Paraphrases with context omitted content more often than the ones without context. While content omitting was mostly not an issue for two annotators, the third annotator's comments showed that they strictly preferred the paraphrases that preserved more content, hence preferred no context paraphrases more often. 

% \textbf{Context boosts the paraphrase quality when sentence rearrangement is required.} We calculated how many times each type of paraphrases were chosen when: (1) the choice was difficult, (2) the choice was not difficult, (3) all three annotators agreed on the better paraphrase of a paragraph, and (4) using majority vote. Results are reported in Table \ref{tab:other}. Comparing the results of the two order codes, we see that the context ones were chosen more often when \texttt{order} $= 40$. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/gpt2_xl_fpr_1}
    \includegraphics[width=0.48\textwidth]{figures/gpt2_xl_fpr_3}
    \includegraphics[width=0.48\textwidth]{figures/gpt2_xl_fpr_10}
    \includegraphics[width=0.48\textwidth]{figures/gpt2_xl_fpr_100}
    \caption{ROC curves for text generated by GPT2-XL, before paraphrasing (solid lines) and after paraphrasing (dashed lines, pp). Different plots represent different clipping thresholds on the X-axis.}
    \label{fig:auc-plots-2}
\end{figure*}

% \begin{table}[]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}llrrrr@{}}
% \toprule
% \multirow{2}{*}{order} & type & hard choice & not hard choice & all agree & majority vote \\
%  & (total \#) & (73) & (116) & (26 out of 63) & (63) \\ \midrule
% \multirow{2}{*}{100} & ctx & 34 & 53 & 11 & 28 \\
%  & no ctx & \textbf{39} & \textbf{63} & \textbf{15} & \textbf{35} \\ \midrule\midrule
% \multirow{2}{*}{order} & type & hard choice & not hard choice & all agree & majority vote \\
%  & (total \#) & (68) & (121) & (21 out of 63) & (63) \\ \midrule
% \multirow{2}{*}{40} & ctx & 29 & \textbf{73} & \textbf{13} & \textbf{34} \\
%  & no ctx & \textbf{39} & 48 & 8 & 29 \\ \bottomrule 
% \end{tabular}%
% }
% \caption{The table shows the counts of how many times the context and no context paraphrases are chosen when (1) the choice was difficult, (2) the choice was not difficult, (3) all three annotators agreed on the choice, and (4) using majority vote. It turns out that when sentence rearrangement is forced, the context paraphrases are chosen more often except when it was hard to decide between two options.}
% \label{tab:other}
% \end{table}

% We manually analyzed 16 comments\footnote{Eight comments for when they chose the context paraphrase as the better paraphrase and eight for when they chose the no context paraphrase.} from each of the three annotators in the second experiment (\texttt{order} = $40$) (see Table \ref{tab:manual-comment-analysis}). We categorized the aspects mentioned in the comments and found that there are common merits and demerits in both types of paraphrases but no aspect is particularly outstanding. For example, while the annotators mentioned 9 times that the no context paraphrases used good word substitutions, there were also 9 times that they found the word choices in the no context paraphrases were bad. However, we did find that \textbf{the no context paraphrases were especially bad at sentence rearrangement}, which coincides with the results reported in Table \ref{tab:other}.

% \begin{table*}[]
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}p{5.6cm}lp{4.4cm}lp{4.5cm}lp{4.5cm}l@{}}
% \toprule
% \textbf{ctx good} & \textbf{\#} & \textbf{no ctx good} & \textbf{\#} & \textbf{ctx bad} & \textbf{\#} & \textbf{no ctx bad} & \textbf{\#} \\\midrule
% good word choice/diversity & 3 & good word choice/diversity & 9 & bad word choice/low diversity & 3 & bad word choice/low diversity & 9 \\
% closer to the original & 8 & closer to the original & 4 & missing/wrong info & 9 & missing/wrong info & 2 \\
% good fit in the context & 3 & good fit in the context & 0 & repetition & 4 & repetition & 1 \\
% good summary of the original & 3 & good summary of the original & 3 & confusing & 3 & confusing & 2 \\
% good sentence arrangement & 2 & good sentence arrangement & 3 & bad sentence arrangement & 3 & bad sentence arrangement & 9 \\
% perfect & 1 & perfect & 2 & bad punctuation & 2 & bad punctuation & 1 \\
% simplicity & 1 & feasible & 1 & less concise & 1 & lengthy & 3 \\
% states things perfectly and differently & 1 & concise & 1 &  &  & not fit in the context & 1 \\
% clear & 1 & consistent in names & 1 &  &  & awkward phrase & 1 \\
% more accurate & 1 &  &  &  &  & bad grammar & 1 \\
% vivid & 3 &  &  &  &  &  & \\ \bottomrule
% \end{tabular}%
% }
% \caption{The table lists the good and bad aspects that are mentioned in the 16 comments from each annotator in the second experiment (\texttt{order} = 40). The numbers are how many times each aspect was mentioned in the comments.}
% \label{tab:manual-comment-analysis}
% \end{table*}


% \begin{table*}[t!]
% \small
% \begin{center}
% \begin{tabular}{ lrrrrr } 
%  \toprule
%  Model & \lexical & \syntax & \simmetric & $J$(\textsc{l,s}) & $J$(\textsc{o,s}) \\
%  \midrule
% \textsc{copy} & 0.0 & 0.0 & 100.0 & 0.0 & 0.0 \\
% \textsc{ref} & 52.6 & 11.4 & 82.9 & 39.5 & 9.1 \\
% \midrule
% \textsc{dips}~\citep{kumar-etal-2019-submodular} & 28.5 & 3.9 & 98.5 & 27.3 & 3.8 \\
% \textsc{sow-reap}~\citep{goyal2020neural} & 35.1 & 30.7 & 95.9 & 32.3 & 29.7 \\
% \textsc{pegasus-pp}~\citep{rajauria2020paraphrase} & 30.7 & 18.5 & 95.9 & 28.3 & 18.0 \\
% \textsc{strap}~\citep{style20} \\
% ~~~~$p = 0.0$ & 48.1 & 20.6 & 95.5 & 44.7 & 20.0 \\
% ~~~~$p = 0.9$ & 60.3 & 25.4 & 84.3 & 48.4 & 22.1 \\
% \textsc{protaugment}~\citep{dopierre-etal-2021-protaugment} & \\
% ~~~~unigram penalty & 80.1 & 12.9 & 32.7 & 23.9 & 4.8 \\
% ~~~~bigram penalty & 58.4 & 24.8 & 79.6 & 43.5 & 19.6 \\
% \textsc{T5-large-div}~\citep{goutham2021paraphrase} & 27.3 & 11.8 & 91.9 & 23.3 & 10.9 \\
% \textsc{parrot}~\citep{prithivida2021parrot} & 21.9 & 6.7 & 97.2 & 20.0 & 6.7 \\
% \midrule
% \model~(ours) & \\
% \bottomrule
% \end{tabular}
% \end{center}
% \vspace{-0.1in}
% \caption{Automatic evaluation of single sentence paraphrasing.}
% \vspace{-0.1in}
% \label{tab:prior-auto-eval}
% \end{table*}