

\section{Building a controllable discourse paraphraser}
\label{sec:model}
Having outlined existing methods to detect machine-generated text, we now focus on a simple attack against all detection techniques: \emph{paraphrasing} the generated text. Intuitively, paraphrasing can alter the statistical properties of model generated text thus fooling outlier detection or classification methods. It can also substantially change the number of ``green'' tokens present in the watermarking approaches (\figureref{fig:watermark-evade}). To evade such defenses, a paraphraser must be able to handle \emph{context} in the form of prompts or multi-sentence inputs. Their behavior should also be \emph{controllable} in order to make as many lexical or syntactic modifications as needed to evade a given defense. In all cases, they should not appreciably change the semantics of their inputs. Finally, to evade watermarking, the paraphraser must be implemented with a model that is different from the watermarked model, as otherwise the paraphrased text will also be watermarked. In this section, we detail how we construct a paraphraser (\model) with all of these properties. To gain a better understanding of \model's ability compared to other existing paraphrase models, we survey related work in \appendixref{sec:appendix:papersurvey}.\footnote{We also note that we observe that \model\ performs competitively with the much larger and more powerful GPT-3.5 davinci-003 model in terms of paraphrase quality, and significantly better at controlling diversity. This finding shows that specialized smaller models can outperform large language models in paraphrasing tasks. We will provide experiments detailing this comparison in the next version.}

\subsection{Constructing paraphrase data}
\label{sec:paraphrase-data-build}
Our process involves fine-tuning a large language model on a parallel dataset of paragraph-level paraphrases. Our ultimate objective is to develop a model capable of paraphrasing any given sequence of sentences, demarcated by a delimiter and potentially embedded within a broader context, while offering control over the extent of lexical and word-order alterations in the original text.
We leverage the \booktranslate~dataset publicly released by~\citet{thai2022booktranslate} to train \model. This dataset contains multiple translations of non-English novels into English aligned at a paragraph level (e.g., it contains both the Henry Morley and Robert Adams translations of Voltaire's \emph{Candide}), which we treat as paragraph-level paraphrases and use to train our paraphraser. More formally, let $p$ and $q$ be aligned paragraphs that come from different English translations of the same non-English novel, where $p_1, p_2 ... p_N$ denote sentences of $p$ and $q_1, q_2, ... q_M$ denote sentences of $q$. Note that $M$ may not always equal $N$ when two translators disagree on when to merge and split source sentences. Then, we perform the following steps (see \figureref{fig:dipper-training} for an overview):

\vspace{0.05in}

\noindent 1. \textbf{Align sentences} of $p$ to sentences of $q$ by using semantic similarity scores from paraphrase similarity metric in ~\citet{wieting-etal-2019-beyond} to run the sequence alignment algorithm designed by \citet{needleman1970general} which uses dynamic programming. More details about the semantic similarity metric are discussed in~\sectionref{sec:eval-metrics-attacks}.

\vspace{0.05in}

\noindent 2. \textbf{Choose a subset of sentences} $p_i ... p_j$ from the first paragraph. Let $q_{i'} ... q_{j'}$ be the corresponding alignment in the second paragraph. In \figureref{fig:dipper-training}, $i=2, j=3, i'=2, j'=4$.

\vspace{0.05in}

\noindent 3. \textbf{Re-order} the sentences $q_{i'} ... q_{j'}$ randomly, and compute the \textbf{diversity control codes} between $p_i ... p_j$ and $\text{shuffle}(q_{i'} ... q_{j'})$. We shuffle the sentences to allow for the model to learn sentence re-ordering. We compute lexical diversity ($L$) using
unigram token overlap and the order diversity ($O$) using the Kendall-Tau correlation of tokens of overlapping words between $p_i ... p_j$ and $\text{shuffle}(q_{i'} ... q_{j'})$, which is also used in \citet{style20}. These values are normalized and rounded to a 6-point scale with values $\{0, 20, 40, 60, 80, 100\}$. Here $L=20$ roughly corresponds to a 20\% lexical modification of the input.

\vspace{0.05in}

\noindent 4. \textbf{Map} the shuffled $q_{i'} ... q_{j'}$ to $p_i ... p_j$, leveraging context from the rest of $p$ and control codes using string concatenation as follows,
\begin{align*}
    \text{codes} &= ``\text{lexical} = L, \text{order} = O''\\
    \text{left} &= p_1 ... p_{i-1} \\
    \text{input} &= \texttt{<p>}~ \text{shuffle}(q_{i'} ... q_{j'}) ~\texttt{</p>} \\
    \text{right} &= p_{j+1} ... p_N \\
    \text{target} &= p_i ... p_j \\
\text{codes} \oplus \text{left} &\oplus \text{input} \oplus \text{right} \longrightarrow \text{target}
\end{align*}
\noindent where $\oplus$ is the string concatenation operation. During inference time, we can paraphrase an arbitrary sequence of sentences by marking it with \texttt{<p>} tags, assigning values to the control codes $L$ and $O$ depending on the desired diversity.\footnote{To make our paper presentation more intuitive, we have slightly modified the notation that our actual pretrained paraphraser uses. Our pretrained model uses control codes $100-L$ and $100-O$, denoting lexical/order \emph{similarity} rather than diversity. Also, \texttt{<sent>} is used instead of \texttt{<p>}. We will clearly document this in the API and code release.}

\subsection{Model and training details}

Our paraphrase generation model is a sequence-to-sequence Transformer neural network~\citep{vaswani2017attention}. We initialize our model with the T5-XXL checkpoint~\citep{raffel2020exploring} and fine-tune it on our paraphrase generation data, using early stopping on validation loss for held-out novels. We found it helpful to paraphrase a maximum of 3 consecutive sentences at time, as it led to better adherence to control codes. Our models are implemented in JAX~\citep{jax2018github} using the T5X library~\citep{roberts2022scaling} with the default fine-tuning hyperparameters for T5-XXL v1.1. At inference time, we sample from our model using nucleus sampling~\citep{holtzman2020curious} with $p=0.75$ and a variety of control codes.


% \subsection{Leveraging \model\ for paraphrase attacks on AI-detection}
% \label{sec:attack-overview}

% \kkcomment{todo}
