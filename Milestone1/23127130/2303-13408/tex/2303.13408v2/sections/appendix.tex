\section*{Appendix}



\section{Limitations of retrieval-based detection and ideas for scaling it further}
\label{sec:limitations}

This section extensively discusses the scalability, limitations, future work and and an LLM API provider's incentive structures for retrieval-based detection. First, in \appendixref{sec:retrieval-scale}, we discuss the scalability of retrieval-based detection in terms of compute requirements, storage space, and accuracy on larger databases. Next, in \appendixref{sec:limitations-retrieval}, we first point out some limitations of using retrieval for AI-generated text detection (\sectionref{sec:defenses}), some of which potentially apply to all existing detectors. Along with limitations, we provide several possible workarounds. In \appendixref{sec:suggestions-retrieval-scale}, we then discuss ideas that can make the proposed retrieval detection work well at an even larger scale than the one we discussed in \sectionref{sec:defenses}. Finally, in \appendixref{sec:incentives} we briefly discuss why an LLM provider may be incentivized to implement retrieval-based detection, and the relationship of this detector with GDPR's right to be forgotten.

%\label{sec:defenses}\label{sec:defense-results}


\subsection{Scalability of retrieval-based detection}
\label{sec:retrieval-scale}

Retrieval-based detection requires the storage of a large database of LLM-generated responses, and querying this database to find matches for previously generated responses. How scalable is this approach in terms of storage space, compute, and accuracy? In this section we perform approximate calculations of these requirements on OpenAI's ChatGPT~\citep{schulman2022chatgpt}.

\noindent \textbf{Storage space requirements}: We estimate ChatGPT’s outputs to take 5TB space monthly (similar to a personal portable hard-disk) via the following calculations. ChatGPT currently gets about 2B monthly visits~\citep{nypost2023chatgpt}. Assuming an average response length of 500 tokens per session, this corresponds to 1 trillion tokens. Similar in size to LLaMA’s training data~\citep{touvron2023llama}, this needs 5TB space. However, 5TB is a small amount of storage compared to the industrial scale of information retrieval. For example, the Google Search index is over 100,000TB and has 100B+ pages~\citep{googlesearch}. Major LLM service providers already have complex storage infrastructure to facilitate this defense. Additionally, OpenAI already stores conversations for at least 30 days (to monitor abuse, and potentially RLHF), even after a user chooses to opt-out~\citep{openai2023data}.

\noindent \textbf{Compute requirements}: Our retrieval experiments, conducted on a 14-core CPU (similar to a Macbook Pro), took 1 second per retrieval on a 15M sized corpus. Extrapolating to a corpus of ChatGPT’s monthly usage (2B visits) would need 130 seconds/retrieval on a Macbook. However, this is fully parallelizable, and can make use of GPUs (Google searches 100B+ entries in < 1 sec). Moreover, efficient similarity search has powerful libraries like FAISS available~\citep{johnson2019billion}. For comparison, ChatGPT itself takes 10 seconds/response, possibly using a powerful 8-GPU A100 server.\footnote{\url{https://twitter.com/tomgoldsteincs/status/1600196981955100694}} Major LLM providers have massive compute clusters, and we believe the computational requirement of retrieval is much lower than hosting LLMs in the first place, which these providers are already adept at. Moreover, our proposed ideas in \appendixref{sec:suggestions-retrieval-scale} can further reduce compute costs.

\noindent \textbf{Accuracy on larger databases}: Our experiments were conducted on the RankGen training set~\citep{krishna-etal-2022-rankgen}, which is the largest publicly available database of AI-generated text that we are aware of (15M generations each in four domains). Besides this, in \sectionref{sec:defense-results} we also conducted experiments on a the ShareGPT corpus with 47K ChatGPT-generated responses. We note that it is extremely expensive and time consuming to create a corpus of AI-generated text from scratch: at a cost of \$0.001 per 500-word response, collecting a billion ChatGPT outputs would cost \$1M and take a long time to collect due to rate limits. Hence, a billion-scale experiment is likely only possible with an LLM provider's private database.

One of the concerns with a larger database is that of \emph{semantic collisions}: a database will saturate with entries having similar semantics, especially for popular topics, and subsequently harm detection at scale. However, we note that:

\begin{enumerate}[leftmargin=*]
\setlength\itemsep{0.0em}

\item Like other detectors, retrieval works best on longer sequences (\figureref{fig:retrieval-length}). Long generations exponentially increase the likelihood of semantic divergences between pairs of entries.
\item Retrieval compares the candidate response against the top-1 entry in the database, not the top-k. For false-positive candidates on popular topics, the top-k entries \emph{together} are more likely to cover input semantics (recall) rather than top-1 (precision).
\item The most effective retrievers use a combination of neural semantic encoders and token overlap scores~\citep{thakur2021beir}. We also see this our experiments (\sectionref{sec:defense-results}), BM25 beats P-SP at detection. BM25 is not purely semantic driven: it uses TF-IDF token overlap.
\item The retrieval accuracy for unperturbed AI-generated text is always 100\%, just like exact match searches in a modern search engine. Retrieval-based detection is also effective on substrings of unperturbed text (as shown in \appendixref{appendix:mixing-attacks}).

\end{enumerate}

Overall, we are optimistic about our scaling plots (\figureref{fig:semantic-search}), and see just a 0.8\% drop moving from a 1M to 10M database (PG19-BM25). We emphasize that BM25 is a basic retriever, and is not optimized on our task. Information retrieval literature has many powerful retrievers and has shown success at billion-scale corpus sizes~\citep{gomes2013creating, lakshman2021embracing}. Google Search currently effectively indexes over 100B webpages~\citep{googlesearch}. We have also suggested a dense retrieval mechanism in \appendixref{sec:suggestions-retrieval-scale} which can be optimized on the underlying retrieval corpus.

Retrieval can easily be used in tandem with other detectors like watermarking. Finally, we note that our paper is the first proof-of-concept that shows a retrieval-based detector could work, and we anticipate future work to build upon it.


\subsection{Limitations of retrieval for detection}
\label{sec:limitations-retrieval}

While retrieval over previously-generated sequences is an effective defense against paraphrase attacks, it also suffers from key limitations, some of which apply broadly to all existing detectors. We discuss these limitations below and discuss possible solutions:




\begin{enumerate}[leftmargin=*]
\setlength\itemsep{0.2em}
    \item \textbf{Detection is specific to an API}. Unlike other general-purpose AI detection algorithms e.g. OpenAI's classifier~\citep{AITextClassifier}, retrieval can only detect generations from the API over which the database is built. API \#1 has no access to the database of generations from API \#2, and thus will not be able to detect generations produced by API \#2. 
    
    \item \textbf{Retrieval is limited to closed-source LLMs}. Users of open-source LLMs like LLAMA~\citep{touvron2023llama} can freely generate outputs without the outputs being stored in a central database. However, currently most major LLM providers operate their LLMs behind closed APIs. It is also important to note that watermarking~\citep{kirchenbauer2023watermark}, the most promising alternative to retrieval, also has this limitation. Since watermarks are added during decoding rather than into the model weights, users of open LLMs are free to generate text without watermarks. While other alternatives like DetectGPT~\citep{mitchell2023detectgpt} or classifiers do not suffer from this issue, we show that they either have low accuracy, or are extremely vulnerable to paraphrasing (\sectionref{sec:attack-expts}).
    
    \item \textbf{The API provider needs to provide a retrieval infrastructure}. After the release of ChatGPT~\citep{schulman2022chatgpt}, AI chatbots are getting widespread adoption. At a conservative rate of 5M queries a day, the database will have almost two billion entries in a year. Complex retrieval infrastructure (like modern search engines) will be necessary to retrieve over these large databases with low latency.
    
    \item \textbf{False positives due to training data memorization}. Language models have been shown to memorize sequences verbatim from their training data~\citep{carlini2021}, such as the Gettysburg Address~\citep{radford2019language}. Despite being originally written by humans, these sequences will be classified as model-generated by our detector. To tackle this issue, we suggest API providers additionally perform retrieval over the training data used to train the model. If a sequence is found in the training set as well as the generation database, it is likely to be an instance of training set memorization.

    \item \textbf{Privacy concerns.} Providing a retrieval detection service partially exposes the database of previously generated text for \emph{all} users. This raises concerns of membership inference attacks~\citep{shokri2017membership} on private user data which may appear in the generated text (if present in user prompt). To mitigate this, we suggest: (1) the detection service should be provided only to trusted users under an agreement to not misuse the system, such as college teachers trying to detect cheating; (2) users should be encouraged not to enter any sensitive private data in their prompts to APIs, a practice already followed by ChatGPT\footnote{\url{https://chat.openai.com}}; (3) API providers only provide a binary output from this detector (AI-generated or not), rather than actual search results; (4) API providers rate-limit queries from IP addresses; and (5) differential privacy mechanisms or scrubbing private attributes to make it difficult / impossible to reconstruct the user prompt from just detector access.

    \item \textbf{Slight reduction in accuracy with large databases.} As we observed in \sectionref{sec:scale-defense-results}, the accuracy of detecting paraphrased text slightly degrades as the database of retrievals gets larger. However, we found this decrease to be quite small (only 1\% on PG19 scaling 1M generations to 15M), despite using fairly primitive retrievers like BM25. Moreover, unperturbed AI-generated text will always be detected with 100\% accuracy using our method, irrespective of corpus size. We discuss this topic more in \appendixref{sec:retrieval-scale} under ``Accuracy on larger databases''.
    
    \item \textbf{Tasks with constrained output space or short outputs}. Similar to all other detection algorithms, it may be hard or even impossible to distinguish AI-generated outputs for tasks with a constrained output space (like sentence-level translation, classification) or very short outputs (as shown in \sectionref{sec:detect-length-effect}). Thus, we believe the main utility of AI-generated text detection is for longer-form generated text, and hence we focus on tasks like long-form QA and open-ended text generation with relatively lengthy outputs. Note that to avoid detection, a sophisticated attacker may try to generate long-form text in smaller chunks using multiple API calls, where each newly-generated chunk is incrementally concatenated to the prompt. This is not a concern for our method if retrieval is done over the corpus of prompts concatenated with generations.

    \item \textbf{Iterative attacks with access to detector.} Another concern is that attackers with access to detection algorithms will iteratively modify their perturbations until they avoid detection (as shown by~\citet{sadasivan2023aigenerated}). While this is a valid concern for all detectors, we believe retrieval has an important advantage over the alternatives. Since the corpus of previously-generated text is proprietary, only the API provider can provide access to this detection service - it is impossible for attackers to locally reproduce this detector. This allows API providers to adopt several mitigation strategies such as (1) rate-limiting queries to avoid iterative attacks; (2) providing retrieval access only to verified users (e.g., teachers); and (3) detecting possible iterative attacks by analyzing previously queries to the retriever.
    
    \item \textbf{Lack of formal guarantees between threshold $T$, response length and detection rate.} Unlike watermarking, we believe it is harder to establish formal relationships between the chosen threshold $T$, response length, and the corresponding TPR / FPR rates. Similar to DetectGPT / classifiers, we believe the threshold needs to be estimated empirically on the underlying corpus, retriever and candidate response distribution. A formal relationship between FPR and the threshold (like in watermarking) may be possible using information about the density of the retrieval database in the semantic vector space. We leave this exploration for future work.

\end{enumerate}


\subsection{Ideas to make retrieval detection work well at an even larger scale}
\label{sec:suggestions-retrieval-scale}

In \sectionref{sec:scale-defense-results}, we observed that our proposed retrieval detector is effective even with a large corpus of 15M previously-generated sequences. While we do not have access to a larger corpus of generations (billion-scale), in this section we describe some ideas to improve retrieval detection at such a scale.


\begin{enumerate}[leftmargin=*]
    \item \textbf{Timestamp filtering in retrieval corpus.} To reduce the large search space, the detector interface could provide users with an option to restrict retrieval to only a fixed time period during which the text was likely to be generated. For instance, a common use-case of AI-generated text detection might be when teachers attempt to catch plagiarism in college essays. Teachers could restrict retrieval to only those generations  created during the assignment window.
    % \item \textbf{Recommend users to input longer queries.} Like all AI-generated text detectors, retrieval works best with longer queries (\sectionref{sec:detect-length-effect}). Clients should use as long a query as possible to maximize the chances for detecting paraphrases.
    \item \textbf{More sophisticated retrieval strategies.} In our work, we only explore simple retrieval strategies like BM25. However, several more sophisticated retrieval strategies exist, which are known to boost performance~\citep{thakur2021beir} and could be useful here. These include methods like re-ranking of top-$k$ retrievals~\citep{khattab2020colbert} or dense retrieval~\citep{karpukhin2020dense}. We do note that these more complex methods are also slower, and latency is likely to be a pressing concern for API providers.
    \item \textbf{Fine-tuning dense retrievers for the detection task.} The retrievers in our work are not fine-tuned for the task of AI-generated text detection. However, we hypothesize that fine-tuning retrievers on this task can help retrievers adapt better to the retrieval corpus and detection task. Specifically, a contrastive learning approach could be adopted here: positive pairs are paraphrased or otherwise noised sequences paired with their generations, while negative pairs are human-written continuations paired with the machine-generated text.
\end{enumerate}


\subsection{Incentives for LLM providers to implement retrieval-based detection}
\label{sec:incentives}

Finally, in this section we discuss the incentive structures for LLM API providers, and why they might want to implement retrieval-based detection.

\begin{enumerate}[leftmargin=*]

\item There is a substantial push from both the US and European governments to regulate companies to make their AI-generated text/images detectable. For instance, several major LLM providers made voluntary commitments to watermark their AI-generated content~\citep{reuterswatermark}. Similarly, the European government recently pushed for rulings to make AI-generated content detectable~\citep{reuterswatermark2}.

\item In a hypothetical scenario where there is a lawsuit about the origin of some malicious AI-generated content, maintaining a database of previously generated responses could be a reliable method to prove innocence, given its strong performance over competing AI-generated text detectors. Keeping this dataset private, also protects LLM providers from the privacy risks of retrieval-based detection (\appendixref{sec:limitations-retrieval}).


\item Major LLM providers are already storing their model-generated outputs to monitor abuse, as well as possibly help improve their products with RLHF preference-based training. For instance, OpenAI's ChatGPT stores the history for 1 month even if users choose to opt-out~\citep{openai2023data}. Hence, implementing a retrieval-based detection service on top of this database does not entail a resource overhead in terms of storage, and reduces the engineering effort needed to implement this detector.

\end{enumerate}

\textbf{Is retrieval-based detection at odds with GDPR's right-to-be-forgotten?} One of the hurdles for LLM providers to implement retrieval-based detection is GDPR's right to be forgotten,\footnote{\url{https://gdpr.eu/right-to-be-forgotten}} which allows attackers to request their data to be deleted by the LLM provider in order to avoid detection. However, we believe that AI-generated text detection is a sufficient cause to temporarily override GDPR deletion requests. We believe that AI-generated text detection could fall under the following GDPR guidelines for overrides: (1) “freedom of expression and information”, (2) “establishment of a legal defense or in the exercise of other legal claims”, and possibly (3) “comply with a legal ruling or obligation” in the future. As an example of this, while OpenAI allows users to delete their chat history~\citep{openai2023data}, they retain it for 30 days, and can review it if required to monitor for abuse.


\input{sections/ethics}




\section{Experiments measuring intrinsic paraphrase generation quality}
\label{sec:intrinsic-evaluation}

Our experiments in \sectionref{sec:attacks} and \sectionref{sec:defense-results} focused on attacking AI-generated text detectors with paraphrases and defending against these paraphrase attacks. We used \model\ as the underlying paraphrase generation model for all of these experiments. Are \model's paraphrases actually good enough to make the attack worthwhile, and can simpler paraphrasers be just as effective as \model? In this section, we conduct careful ablation experiments (\appendixref{sec:ablation-dipper}) and human evaluations (\appendixref{appendix:HumanEval}) to validate the effectiveness of \model\ at preserving the semantics of the input generation. Our results show that \model\ effectively leverages surrounding context to paraphrase multiple sentences while preserving input semantics.


\subsection{Ablation studies on \model}
\label{sec:ablation-dipper}

In this section, we perform automatic evaluations to confirm the efficacy of \model\ as a paraphraser. From a survey of existing paraphrasers that we carry out in \appendixref{sec:appendix:papersurvey}, \model\ possess two unique features that differentiate it from other paraphrasers: (1) its ability to leverage context from \emph{outside} of the text to be paraphrased (such as the prompt); and (2) its ability to paraphrase multiple sentences at once. How useful are these features while paraphrasing long sequences of text?

To answer this question, we first train an ablated version of \model\ by constructing a training dataset (\sectionref{sec:paraphrase-data-build}) without any left or right context, and then fine-tuning T5-XXL using the same hyperparameters as in \sectionref{sec:model}. We call this model \model-no-ctx. We paraphrase 1K open-ended generations from GPT2-XL using both \model\ and \model-no-ctx, using each of the four configurations of diversity control codes studied in this paper. We then evaluate the quality of the paraphrased text using three metrics: (1) GPT3.5-davinci-003 perplexity~\citep{brown2020language} of the prompt concatenated with the paraphrased continuation; (2) \textsc{RankGen} compatibility between the prompt and the paraphrased continuation~\citep{krishna-etal-2022-rankgen}; and (3) unigram token overlap between the paraphrased continuation and the prompt.

\begin{table}[t!]
\caption{Ablation experiments demonstrate the high quality of \model's paraphrases compared to alternatives. Displayed scores are the percentage of cases in which rewrite A is preferred over B by one of the three metrics, with subscripts showing absolute average scores on each metric across the dataset. Overall, \model\ benefits from context outside the input (Experiment 1), multi-sentence paraphrasing (Experiment 2), and is not too far behind non-paraphrased text in terms of quality (Experiment 3).}
\label{tab:paraphrase-ablations}
\vspace{\baselineskip}

\centering
\small
\begin{tabular}{@{}lrrrrrr@{}} 
 \toprule
   \multicolumn{7}{c}{\textbf{Open-ended generation with GPT2-XL on Wikipedia prompts}}\vspace{0.1in} \\ 
   & \multicolumn{2}{c}{\textsc{RankGen-XL}} & \multicolumn{2}{c}{GPT3.5 davinci-003 perplexity} & \multicolumn{2}{c}{unigram overlap with prompt} \\
 \cmidrule(lr){2-3}  \cmidrule(lr){4-5} \cmidrule(lr){6-7}
 Control & rewrite A &  rewrite B  & rewrite A &  rewrite B  &  rewrite A &  rewrite B \\
 \midrule
 \multicolumn{7}{l}{\textbf{Experiment 1}: \emph{Is context helpful for paraphrasing?}} \vspace{0.05in} \\
 \multicolumn{7}{l}{rewrite A = \model\ with  context}\\
 \multicolumn{7}{l}{rewrite B = \model\ no context} \vspace{0.05in}\\
 20L & \textbf{62}\% {\scriptsize 10.2} & 38\% {\scriptsize 9.4} & \textbf{58}\% {\scriptsize 11.5} & 42\% {\scriptsize 11.7} & \textbf{55}\% {\scriptsize 41.3} & 45\% {\scriptsize 40.7}\\
 40L & \textbf{62}\% {\scriptsize \phantom{0}9.8} & 38\% {\scriptsize 8.7} & \textbf{64}\% {\scriptsize 11.9} & 36\% {\scriptsize 12.5} & \textbf{57}\% {\scriptsize 40.7} & 43\% {\scriptsize 39.8} \\
 60L & \textbf{64}\% {\scriptsize \phantom{0}9.6} & 36\% {\scriptsize 7.9} & \textbf{66}\% {\scriptsize 12.3} & 34\% {\scriptsize 13.3} & \textbf{55}\% {\scriptsize 39.9} & 45\% {\scriptsize 39.2}\\
 60L,60O & \textbf{66}\% {\scriptsize \phantom{0}8.3} & 34\% {\scriptsize 6.3} & \textbf{71}\% {\scriptsize 12.9} & 29\% {\scriptsize 14.2} & \textbf{56}\% {\scriptsize 39.4} & 44\% {\scriptsize 38.4}\\
 \midrule
  \multicolumn{7}{l}{\textbf{Experiment 2}: \emph{Is it helpful to paraphrase multiple sentences at a time?}} \vspace{0.05in} \\
 \multicolumn{7}{l}{rewrite A = \model\ 3 sentences at a time} \\
 \multicolumn{7}{l}{rewrite B = \model\ 1 sentence at a time} \vspace{0.05in}\\
 20L & \textbf{55}\% {\scriptsize 9.4} & 45\% {\scriptsize 8.9} & \textbf{73}\% {\scriptsize 11.7} & 27\% {\scriptsize 12.5} & \textbf{51}\% {\scriptsize 40.7} & 49\% {\scriptsize 40.7}  \\
 40L & \textbf{55}\% {\scriptsize 8.7} & 45\% {\scriptsize 8.5} & \textbf{71}\% {\scriptsize 12.5} & 29\% {\scriptsize 13.3} & 46\% {\scriptsize 39.8} & \textbf{54}\% {\scriptsize 40.3} \\
 60L & \textbf{53}\% {\scriptsize 7.9} & 47\% {\scriptsize 7.6} & \textbf{71}\% {\scriptsize 13.3} & 29\% {\scriptsize 14.3} & 46\% {\scriptsize 39.2} & \textbf{54}\% {\scriptsize 39.8}\\
 60L,60O & \textbf{57}\% {\scriptsize 6.3} & 43\% {\scriptsize 5.3} & \textbf{83}\% {\scriptsize 14.2} & 17\% {\scriptsize 16.7} & 47\% {\scriptsize 38.4} & \textbf{53}\% {\scriptsize 38.9}\\
 \midrule
  \multicolumn{7}{l}{\textbf{Experiment 3}: \emph{Does paraphrasing preserve the quality of the original text?}} \vspace{0.05in}\\
 \multicolumn{7}{l}{rewrite A = no paraphrasing} \\
 \multicolumn{7}{l}{rewrite B = \model} \vspace{0.05in}\\
  20L & \textbf{50}\% {\scriptsize 10.4} & \textbf{50}\% {\scriptsize 10.2} & \textbf{61}\% {\scriptsize 11.1} & 39\% {\scriptsize 11.5} & \textbf{51}\% {\scriptsize 41.6} & 49\% {\scriptsize 41.3}\\
 40L & \textbf{57}\% {\scriptsize 10.4} & 43\% {\scriptsize \phantom{0}9.8} & \textbf{67}\% {\scriptsize 11.1} & 33\% {\scriptsize 11.9} & \textbf{55}\% {\scriptsize 41.6} & 45\% {\scriptsize 40.7}\\
 60L & \textbf{58}\% {\scriptsize 10.4} & 42\% {\scriptsize \phantom{0}9.6} & \textbf{73}\% {\scriptsize 11.1} & 27\% {\scriptsize 12.3} & \textbf{58}\% {\scriptsize 41.6} & 42\% {\scriptsize 39.9}\\
 60L,60O & \textbf{68}\% {\scriptsize 10.4} & 32\% {\scriptsize \phantom{0}8.3} & \textbf{79}\% {\scriptsize 11.1} & 21\% {\scriptsize 12.9} & \textbf{61}\% {\scriptsize 41.6} & 39\% {\scriptsize 39.4} \\
\bottomrule
\end{tabular}

\end{table}


\textbf{Contextual paraphrasing leads to higher quality paraphrases}. In \tableref{tab:paraphrase-ablations} (Experiment 1), we observe that across all four control code configurations and all three metrics, paraphrases from \model\ are preferred over paraphrases from \model-no-ctx. Specifically, with the lexical and order control codes set to 60\% (most diverse), \model\ paraphrases are preferred by GPT3.5 perplexity 71\% of the time compared to non-contextual paraphrases (average perplexity drop of 12.9 vs 14.2).



\textbf{Paraphrasing multiple sentences at a time is better than paraphrasing individual sentences.} Next, we use our \model-no-ctx model to compare two settings: paraphrasing 3 sentences at a time vs paraphrasing 1 sentence at a time before concatenating. We hypothesize that the former will produce higher quality paraphrases since we expect it to better connect discourse elements across the text. Indeed, in \tableref{tab:paraphrase-ablations} (Experiment 2) across all control codes, GPT3.5 and \textsc{RankGen} usually prefer multi-sentence paraphrases over the single-sentence baseline. This preference is 71\% or higher for all control codes when evaluating with GPT-3.5 perplexity, reaching 83\% for L60,O60.


\textbf{\model\ paraphrases are close to the unperturbed GPT-2 XL generations}. Finally, we compare \model\ with the original GPT2-XL generations (without paraphrasing) on the same three metrics. While we expect metrics to prefer non-paraphrased text, a strong paraphraser will produce text that is close to the original in terms of these metrics.
\tableref{tab:paraphrase-ablations} (Experiment 3) confirms our hypothesis: at L20, \textsc{RankGen} has a 50-50 preference between the two outputs, while GPT3.5 prefers the non-paraphrased generations just 61\% of the time, with an average perplexity gain of just 0.4 (11.1 to 11.5). At more diverse control codes, preference for GPT2-XL generations does go up (58\% \textsc{RankGen}, 73\% GPT3.5 for L60), but absolute scores continue to be close (11.1 vs 12.3 GPT-3.5 perplexity). Note that while all of these ablations use just a single paraphrase sample, it is easy for an attacker to obtain multiple samples from \model\ and choose the sample that maximizes these metrics (as discussed in \sectionref{sec:attack-expts}). 

\subsection{Human evaluation of semantic preservation using~\model}\label{appendix:HumanEval}%\label{sec:appendix:likert}

The automatic semantic similarity scores in \tableref{tab:watermark-attacks} and \ref{tab:attacks-lfqa} indicate that \model~generates paraphrases that are faithful to the original input paragraphs. To confirm this result with human evaluation, we hire three native English teachers and/or editors on Upwork\footnote{\url{https://www.upwork.com}} to evaluate the semantic fidelity of the paraphrases. As human evaluation is expensive, we fix the order diversity ($O$) to be $0$ and focus on the impact of the lexical diversity. We evaluate paraphrases with the lexical codes $L20$, $L40$, and $L60$, corresponding to moderate, medium, and high lexical diversity. Twenty paraphrases are sampled randomly for each lexical code, resulting in 60 original text and paraphrase pairs. 

The evaluation is conducted on the platform Label Studio~\citep{LabelStudio}.\footnote{\url{https://labelstud.io/}} As shown in the interface of our annotation platform \figureref{fig:label_studio_interface}, the text to be paraphrased (highlighted in yellow) are preceded by its context. The annotators see the same amount of text as \model. They need to first read the texts, select one point on the Likert scale, then provide free-form comments justifying their ratings. We estimated that the evaluation of each paraphrase takes 1.5 to 2 minutes. As such, we pay \$15 as a base rate with a bonus for the reasonable extra time that the annotators spend on the tasks. 



\begin{figure*}[!ht]
    \centering
    \includegraphics[scale=0.4]{figures/label_studio_screen_shot.png}
    \caption{The interface of the annotation platform used in our human study}
    \label{fig:label_studio_interface}
\end{figure*}


%https://docs.google.com/spreadsheets/d/10LtBZLLuTut2kDdyiCkCkYWovnmgTESOABqse-P30EU/edit?usp=sharing
\begin{table}
\caption{This table shows how often each point in the Likert scale was chosen by 3 annotators for the pairs of original and paraphrased texts. Twenty text pairs are randomly selected for each lexical code (L). 81.8\% of the time, our model \model\ provides a paraphrase which is nearly equivalent to the input in terms of semantic meaning.}
\label{tab:humanevalpercentage}
\vspace{1pt}

\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rrrrrr@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{L}}} &
  \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Sum of 4 and 5}}} &
  \multicolumn{1}{c}{\textbf{5}} &
  \multicolumn{1}{c}{\textbf{4}} &
  \multicolumn{1}{c}{\textbf{3}} &
  \multicolumn{1}{c}{\textbf{2}} \\
\multicolumn{1}{c}{} &
  \multicolumn{1}{c}{} &
  \multicolumn{1}{c}{\textbf{Approx. equivalent}} &
  \multicolumn{1}{c}{\textbf{Nearly equivalent}} &
  \multicolumn{1}{c}{\textbf{Somewhat equivalent}} &
  \multicolumn{1}{c}{\textbf{Topically related}} \\
  \midrule
20                              & 95.0\% & 63.3\% & 31.7\% & 5.0\%    & 0.0\% \\
40                              & 78.3\% & 45.0\%   & 33.3\% & 21.7\% & 0.0\% \\
60                              & 70.0\% & 28.3\% & 41.7\% & 28.3\% & 1.7\% \\\midrule
\textbf{Total} & 81.1\% & 45.6\% & 35.6\% & 18.3\% & 0.6\%\\\bottomrule
\end{tabular}%
}
\end{table}


Among the 60 original text and paraphrase pairs, the three annotators agreed on their choice 28.3\% of the time, and 60\% of the time the point they chose on the scale differs by 1. \tableref{tab:humanevalpercentage} reports how often each point on the Likert scale is chosen. Over 80\% of the time, our annotators rate \model's paraphrases as nearly equivalent (4 out of 5) or approximately equivalent (5 out of 5). 

A qualitative analysis of the free-form annotator comments reveals systemic strengths and shortcomings of \model. \tableref{tab:cherry-pickedexamples-appendix} provides two representative examples for each lexical code that is evaluated in our human study. 

\noindent\textbf{Strengths} First, the third example in \tableref{tab:cherry-pickedexamples-appendix} exemplifies \model's ability to leverage information from context to increase diversity while maintaining coherence (i.e., from \textit{line\ldots reference the song's title} to \textit{reference to ``I'm the Greatest"}). The same is observed in row 2 where \model~uses the context to interchange \textit{he} and \textit{Churchill}. A paraphrase model without looking into context will have great difficulty in doing this and no prior paraphraser (see \tableref{tab:prior-eval-metric} for a list) is capable of that. Second, the example in the fifth row highlights~\model's ability to make significant changes to original texts with a high lexical diversity code ($L60$) (see the color coding) while preserving their semantic meaning as rated by the annotators.


\noindent\textbf{Qualitative shortcomings}: The first shortcoming is that, when the original text contains new created proper names (unlike common people and country names), such as the ones in row 6 (\textit{Homing Attack} and \textit{Slide Attack}), a high lexical code has a tendency to change such nouns, leading to the result that one of our annotators deems it to be only topically related to the original. However, this shortcoming can be overcome by decreasing the lexical code, which a user can choose from a continuous range (from 0 to 100). For instance, in row 1 with \texttt{lex=20}, the songs' names \textit{M’s Confession} and \textit{Gone Fishing} are kept intact. Another shortcoming is that \model~occasionally omits content from an original text. While in some cases such removal is acceptable (see row 6), in other cases it causes significant change in the meaning of the text (see row 4). However, the former case can be overcome by paraphrasing a shorter paragraph at a time.

Overall, the human study shows that \model~performs well at preserving the semantic meaning of original texts while introducing both semantic and syntactic diversity. Because \model~provides user-friendly controllabilty of output diversity, a user can adjust the control code to find the most suitable paraphrase for their need.








\input{sections/paraphrase_related_work}
\input{sections/background} %original one in the arxiv version

\section{More experimental details of our attack experiments}

\subsection{Details for training our paraphraser \model}
\label{appendix:dipper-training}

 Our paraphraser \model\ is a sequence-to-sequence Transformer neural network~\citep{vaswani2017attention}, initialized with the T5-XXL 1.1 checkpoint~\citep{raffel2020exploring} and fine-tuned on our paraphrase generation data, using early stopping on validation loss for held-out novels. During training, we find it helpful to paraphrase a maximum of 3 consecutive sentences at time, which leads to better adherence to control codes. Our models are implemented in JAX~\citep{jax2018github} using the T5X library~\citep{roberts2022scaling} with the default fine-tuning hyperparameters. Our final dataset contains 6.3M paraphrase pairs. Training was done on 64 cloud TPUv3 chips, and took 6-12 hours to complete. At inference time, we use nucleus sampling~\citep{holtzman2020curious} with $p=0.75$ and a variety of control codes.

To make our paper more intuitive, we have slightly modified the notation that our actual pretrained model uses. Our pretrained model uses control codes $100-L$ and $100-O$, denoting lexical/order \emph{similarity} rather than diversity. Also, \texttt{<sent>} is used instead of \texttt{<p>}. We will clearly document this in the code release.

\subsection{Long-form question answering data processing}
\label{appendix:attack-details}

In \sectionref{sec:attacks} evaluate long-form question answering~\citep{fan2019eli5}, in which an LM must answer a how/why question (e.g., \emph{Why are almost all boats painted white?}) with a 250-350 word answer. To build a long-form question answering dataset, we scrape questions from the \href{https://www.reddit.com/r/explainlikeimfive/}{r/explainlikeimfive} subreddit posted between July to December 2021.\footnote{We choose this period since current language models have been trained on internet data available before June 2021~\citep{GPT35}, this prevents verbatim copying from training data.} We randomly sample 500 questions from each of six popular domains on the subreddit (biology, physics, chemistry, economics, law, and technology) and pair each question with its longest human-written answer, which yields 3K long-form QA pairs.

\subsection{Are successful attacks against model-specific detectors really an attack success?}
\label{appendix:why-study-model-specific}

In our experiments, we attack two model-specific detectors (watermarking, DetectGPT) in addition to model-agnostic detectors (OpenAI classifier, GPTZero, RankGen). Model-specific detectors have been specifically designed to judge whether a response was generated by a \emph{particular} model. After paraphrasing, a response is not truly generated by that specific model anymore---it has been generated by the \emph{paraphrasing} model. Hence, in a sense, the inability of these detectors to detect paraphrased text denotes their robustness in model-specific detection.

While the argument above has merit, we argue that it is critical for model-specific detectors to be robust against paraphrasing attacks. Given their strong performance over model-agnostic methods (\sectionref{sec:attack-expts}), and the large risk of perturbation attacks (human-edited or automatically-edited), we think it is important for model-specific detectors to bake perturbation robustness in their design for better downstream usability. Currently, the low performance of model-agnostic detectors (\sectionref{sec:attack-expts}) makes them quite unusable, and OpenAI even took down their text classifier due to low performance~\citep{AITextClassifier}. Currently, model-specific detectors (including watermarking and retrieval) seem to be the only reliable path towards robust AI-generated text detection.

\section{More retrieval-based detection experiments}

\subsection{Controlled comparisons of retrieval with other AI-generated text detectors on open-ended text generation}\label{sec:appendix:controlled_comparison}

We conduct a controlled comparisons of retrieval on the open-ended text generation task with Wikipedia prompts (see \sectionref{sec:control-defense-results}). The result of the experiment is presented in \tableref{tab:watermark-defense-wiki}.

\subsection{Robustness against text shortening and mixing attacks}
\label{appendix:mixing-attacks}

Our experiments in \sectionref{sec:defense-results} assumed that an attacker would paraphrase the entire LLM output to evade detection. However, an alternative attack strategy that might be adopted in practice is \emph{text shortening}, or \emph{text mixing}~\citep{kirchenbauer2023reliability}. Here, attackers only use a substring of an LLM generated output (instead of the full output), and optionally mix it with text from other sources (like other LLM-generated outputs or human-written text).

In this section, we conduct some preliminary experiments to show the robustness of retrieval-based detection to text shortening and mixing attacks. We adopt the same experimental setup as \sectionref{sec:scale-defense-results}, utilizing a BM25 retrieval on a PG19 machine-generated corpus of 2M responses.

In \tableref{tab:retrieval-text-mixing} we see that retrieval is a promising defense strategy even against text shortening and mixing attacks. Overall, we see that unperturbed random substrings (from single or multiple generations) can still be detected quite easily (86.2\% to 94.7\% detection rate at 1\% FPR). However, adding DIPPER paraphrasing on top of that reduces accuracy (56.1\% to 68.8\% detection rate).

\begin{table}[h]
\caption{Preliminary experiments measuring the robustness of retrieval-based detection to LLM response shortening and mixing attacks. Overall, we find that retrieval is a strong detector for truncated as well as mixed responses. Experiments are conducted with BM25 retrieval in the PG19 data setup with a retrieval corpus of size 2M responses (from \sectionref{sec:scale-defense-results}). Results shown are true positive rates at the 1\% FPR threshold.}
\label{tab:retrieval-text-mixing}
\vspace{\baselineskip}

\small
\centering
\begin{tabular}{lc}
\toprule
Candidate Text & Retrieval detection rate\\
&  (1\% FPR) \\
\midrule
unperturbed LLM response & 100.0 \\
\model-paraphrased response & \phantom{0}98.2 \\
50\% truncated \model-paraphrased response & \phantom{0}72.6 \\
\midrule
50\% truncated LLM response & \phantom{0}86.2 \\
50\% LLM response 1 + 50\% LLM response 2 & \phantom{0}94.7 \\
50\% \model-paraphrased response & \phantom{0}68.8 \\
50\% \model-paraphrased response 1 + \model-paraphrased response 2 & \phantom{0}56.1 \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[h!]
\caption{Our retrieval defense significantly improves AI-generated text detection accuracy (at 1\% FPR) over baselines on all settings, including our most diverse paraphrase attacks (+60L and +60L,60O).}
\label{tab:watermark-defense-wiki}
\vspace{\baselineskip}

\small
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lrrrrrrrrr@{}} 
\toprule
 \multicolumn{10}{c}{\textbf{Open-ended text generation with Wikipedia prompts} (300 generated tokens)}\vspace{0.1in} \\ 
 & \multicolumn{3}{c}{GPT2-XL} & \multicolumn{3}{c}{OPT-13B} & \multicolumn{3}{c}{GPT-3.5 (davinci-003)}\\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} 
& Original & + 60L & + 60L,60O & Original & + 60L &  + 60L,60O & Original & + 60L & + 60L,60O \\
\midrule
\multicolumn{5}{l}{\emph{Baseline methods}:} \vspace{0.15cm} \\
Watermark & 100.0 & 68.9 & 57.2 & 99.9 & 63.7 & 52.8 & - & - & -\\
DetectGPT &  70.3 & 8.7 & 4.6 & 14.3 & 0.8 & 0.3 & 2.0 & 0.5 & 0.0 \\
OpenAI & 21.6 & 13.3 & 14.8 & 11.3 & 9.1 & 10.0 & 30.0 & 15.6 & 15.6 \\
\midrule
\multicolumn{10}{l}{\emph{(Ours)} Retrieval over corpus of 3K generations from model itself, with retriever:} \vspace{0.15cm} \\
%~~~SP & 100.0 & 83.0 & 80.8 & 100.0 & 81.8 & 80.0 & 100.0 & 63.4 & 47.4 \\
%~~~BM25 & 100.0 & 99.1 & 98.0 & 100.0 & 97.2 & 95.3 & 100.0 & 58.8 & 37.4 \\
~~~SP & 100.0 & 86.4 & 81.5 & 100.0 & 84.4 & 77.7 & 100.0 & 65.9 & 49.5 \\
~~~BM25 & 100.0 & 99.0 & 98.0 & 100.0 & 97.2 & 95.3 & 100.0 & 58.8 & 37.4\\
\midrule
\multicolumn{10}{l}{\emph{(Ours)} Retrieval over corpus of 9K generations pooled from all three models, with retriever:} \vspace{0.15cm} \\
%~~~SP & 100.0 & 82.8 & 80.0 & 100.0 & 81.8 & 80.0 & 100.0 & 76.6 & 60.0 \\
%~~~BM25 & 100.0 & 98.9 & 97.8 & 100.0 & 97.1 & 95.3 & 100.0 & 58.8 & 37.4 \\
~~~SP & 100.0 & 72.1 & 63.2 & 100.0 & 74.6 & 65.6 & 100.0 & 63.1 & 45.6 \\
~~~BM25 & 100.0 & 85.0 & 78.7 & 100.0 & 87.2 & 79.1 & 100.0 & 58.8 & 37.4 \\
\bottomrule
\end{tabular}%
}
\end{table}

\section{ROC curves at different FPR}
\label{appendix:more-roc-curves}

See \figureref{fig:auc-plots-2}.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/gpt2_xl_fpr_1}
    \includegraphics[width=0.48\textwidth]{figures/gpt2_xl_fpr_3}
    \includegraphics[width=0.48\textwidth]{figures/gpt2_xl_fpr_10}
    \includegraphics[width=0.48\textwidth]{figures/gpt2_xl_fpr_100}
    \caption{ROC curves for text generated by GPT2-XL, before paraphrasing (solid lines) and after paraphrasing (dashed lines, pp). Different plots represent different clipping thresholds on the X-axis.}
    \label{fig:auc-plots-2}
\end{figure*}

% See \tableref{tab:likertscale}.

% \begin{table*}[h]
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}l@{}}
% \toprule
% \multicolumn{1}{c}{\textbf{Which best describes the quality of the paraphrase?}} \\\midrule
% 5. Approximately equivalent: the paraphrase preserves the meaning of the source but differs in words and/or structure.       \\
% 4. Nearly equivalent: the paraphrase preserves most information in the source but differs in some minor factual details.     \\
% 3. Somewhat equivalent: the paraphrase preserves some information in the source but differs in certain significant ways. \\
% 2. Topically related: the paraphrase is topically related to the source but most information in the source is not preserved. \\
% 1. Not topically related: the paraphrase is not topically related to the source and preserves no information.\\\bottomrule
% \end{tabular}%
% }
% \caption{Description of each point of the five-point Likert scale used in our human evaluation study}
% \label{tab:likertscale}
% \end{table*}


% \section{Metrics to evaluate inherent paraphrase quality}

% \kkcomment{Marzena: discourse deixis, ellipsis, cataphora / anaphora?.. Robert-Alain de Beaugrande (intro to text linguistics): Compacting patterns: pro-forms; anaphora and cataphora; ellipsis; trade-off between compactness and clarity. Signalling relations: tense and aspect; updating; junction: conjunction, disjunction, contrajunction, and subordination; modality. Functional sentence perspective.a lot of these seems relevantish and may pose a problem when paraphrasing (including major ordering changes) is done without the regard to the context... "definition of paraphrasing"}

% \subsection{Modeling paraphrase distribution (\ppl)}

% with / without context perplexity evaluation (1.63 loss without context, 1.234 loss with context)

% \subsection{Semantic Similarity (\simmetric)}

% A valid paraphrase should approximately convey the same meaning as the input. We measure semantic similarity between the input and output using the embedding-based SIM model from~\citet{wieting-etal-2019-beyond}. SIM is trained on backtranslated paraphrase data from ParaNMT-50M~\citep{wieting-gimpel-2018-paranmt}, and has performed well on semantic textual similarity (STS) benchmarks in SemEval workshops~\citep{agirre-etal-2016-semeval} \kkcomment{John, could you confirm the last line?}.

% \subsection{Lexical and Syntactic Diversity (adherence to control values)}

% A unique aspect of our paraphraser is the ability to control the lexical diversity and content ordering using input control codes ($L, K$ in \sectionref{sec:model}). How often does the model adhere to these control codes? We measure the lexical diversity and content reordering of our generated paraphrases compared to the input, and then measure the Kendall Tau correlation~\citep{kendall1938new} with the input control codes at an instance level.

% \subsection{Coherence}
% \label{sec:coherence}

% Our paraphraser can not only perform lexical substitutions, but also re-order the content in the input. However, a trivial alternative is to simply perform sentence-by-sentence paraphrasing, and change the order of sentences.\kkcomment{also talk about merges / splits here, maybe measure \% of cases where number of sentences change?}\kkcomment{metric could be some kind of ppl test against baseline which simply does sentence-by-sentence non-contextual paraphrasing and then}

% \subsection{Measuring context utilization}
% \label{sec:eval-context}

% Our paraphraser is contextual --- it uses the text surrounding a given sequence to help paraphrase generation. How much does this context assist in paraphrase generation? We conduct an ablation study where we train a model on the same dataset, but without any context. \kkcomment{measure perplexity using same model, using independent LM, ROUGE scores with reference, fine-grained analysis, A/B tests (quality / fit to context [MASK])}

% \subsection{Human evaluation}\label{sec:human-evaluation}

% 3 A/B tests
% \begin{itemize}
%     \item contextual paraphrase vs no-context paraphrase (measures context)

%     \item end-to-end paragraph paraphrase vs (sentence-level paraphrase and then reorder). (use paragraphs that are 2-3 sentences) (do alignment between our model output and original paragraph)
    
%     \item diverse vs not diverse (measures control codes) (maybe not necessary, instead we can do sem. similarity eval and then use auto eval to measure diversity)
% \end{itemize}

% 1 absolute similarity eval (3 pt scale): paraphrase vs ungrammatical paraphrase vs not paraphrase

% \kkcomment{probability of input given output}

% \section{Details of Human Evaluation Experiments}\label{appendix:HumanEvalDetails}

% To evaluate whether the paraphraser benefits from having access to context, we conducted two human evaluation experiments. For each of them, we hired three experienced native English editors/proofreaders. We paid \$$1.8$ for each pair of paraphrases.\footnote{On average, evaluating one pair of paraphrases took 5 minutes. The instructions and the format of the annotation platform can be found \href{https://docs.google.com/presentation/d/10kRW4Lt7ZiF1EZygs4wuMwvIWKBFrlapMXr1gjSyBZI/edit?usp=sharing}{in this document}.} 
% We closely monitored the quality of the annotations and gave immediate feedback when an annotator does not follow the instructions. 

% Given a paragraph with a highlighted span to be paraphrased, an annotator was provided with two paraphrases, one was generated considering the context, and the other was not. The annotators' task was to compare the two paraphrases and choose the better one according to the following criteria:

% \begin{itemize}
%     \item how \textbf{diverse} is the \textbf{vocabulary}/\textbf{grammar} of the paraphrase compared to the original,
%     \vspace{-6pt}
    
%     \item how \textbf{well} does the paraphrase \textbf{fit} in the paragraph,
%     \vspace{-6pt}
    
%     \item whether the paraphrase \textbf{roughly preserves the meaning} of the original text.
% \end{itemize}

% \noindent Annotators were instructed to justify their choice using 3-4 sentences. Additionally, they were asked to indicate whether their selection was a difficult choice.

% In the first experiment, we fixed the \texttt{order} code to be $100$ (i.e., no sentence reordering) and used the  \texttt{lexical} code $20$, $40$, and $60$. For each \texttt{lex}--\texttt{order} code combination, we randomly selected twenty-one paragraphs and their corresponding paraphrases. In the second experiment, we used \texttt{order} = 40 to encourage the model to reorganize the sentences in a paraphrase, keeping the same \texttt{lexical} codes. The results of the two experiments can be found in Table \ref{tab:order100}.


% \begin{table}[]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}lrrrrrr@{}}
% \toprule
% \textbf{Code} & \multicolumn{2}{c}{\textbf{Rater 1}} & \multicolumn{2}{c}{\textbf{Rater 2}} & \multicolumn{2}{c}{\textbf{Rater 3}} \\ 
% (\texttt{lex}, \texttt{order}) & ctx & no ctx & ctx & no ctx & ctx & no ctx \\\midrule
% (20, 100) & 11 & 10 & 11 & 10 & 10 & 11 \\
% (40, 100) & 11 & 10 & 10 & 11 & 6 & 15 \\
% (60, 100) & 11 & 10 & 8 & 13 & 9 & 12 \\\midrule
% \textbf{Total} & 33 & 30 & 29 & 34 & 25 & 38 \\ 
% \textbf{Agreement} & \multicolumn{2}{l}{41.27\%} & \multicolumn{2}{l}{\textbf{Fleiss' $\kappa$}} & \multicolumn{2}{l}{0.212} \\\midrule\midrule
% % \textbf{Code} & \multicolumn{2}{c}{\textbf{Rater 1}} & \multicolumn{2}{c}{\textbf{Rater 2}} & \multicolumn{2}{c}{\textbf{Rater 3}} \\ 
% (\texttt{lex}, \texttt{order}) & ctx & no ctx & ctx & no ctx & ctx & no ctx \\\midrule
% (20, 40) & 11 & 10 & 15 & 6 & 10 & 11 \\
% (40, 40) & 12 & 9 & 11 & 10 & 8 & 13 \\
% (60, 40) & 17 & 4 & 7 & 14 & 11 & 10 \\\midrule
% \textbf{Total} & 40 & 23 & 33 & 30 & 29 & 34 \\ 
% \textbf{Agreement} & \multicolumn{2}{l}{33.33\%} & \multicolumn{2}{l}{\textbf{Fleiss' $\kappa$}} & \multicolumn{2}{l}{0.106} \\ \bottomrule
% \end{tabular}%
% }
% \caption{The table shows the counts of how many times the context and no context paraphrases are chosen in each code combination by each annotator. Fleiss' $\kappa$ = 0.212 indicates fair agreement. Fleiss' $\kappa$ = 0.106 is slight agreement.}
% \label{tab:order100}
% \end{table}

% \textbf{The results show that the task is subjective.} In the first experiment, the annotators agreed on the better paraphrase for only 26 out of 63 paragraphs. This number further decreased to 21 in the second experiment where sentence reordering is forced. Fleiss' $\kappa$ shows that the experiments reached only fair and slight agreement (0.212 and 0.106). The subjectivity is also reflected in the comments. We observe that, in the two experiments, 65\% of the time the paraphrases are shorter than the original text. Paraphrases with context omitted content more often than the ones without context. While content omitting was mostly not an issue for two annotators, the third annotator's comments showed that they strictly preferred the paraphrases that preserved more content, hence preferred no context paraphrases more often. 

% \textbf{Context boosts the paraphrase quality when sentence rearrangement is required.} We calculated how many times each type of paraphrases were chosen when: (1) the choice was difficult, (2) the choice was not difficult, (3) all three annotators agreed on the better paraphrase of a paragraph, and (4) using majority vote. Results are reported in Table \ref{tab:other}. Comparing the results of the two order codes, we see that the context ones were chosen more often when \texttt{order} $= 40$. 


% \begin{table}[]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}llrrrr@{}}
% \toprule
% \multirow{2}{*}{order} & type & hard choice & not hard choice & all agree & majority vote \\
%  & (total \#) & (73) & (116) & (26 out of 63) & (63) \\ \midrule
% \multirow{2}{*}{100} & ctx & 34 & 53 & 11 & 28 \\
%  & no ctx & \textbf{39} & \textbf{63} & \textbf{15} & \textbf{35} \\ \midrule\midrule
% \multirow{2}{*}{order} & type & hard choice & not hard choice & all agree & majority vote \\
%  & (total \#) & (68) & (121) & (21 out of 63) & (63) \\ \midrule
% \multirow{2}{*}{40} & ctx & 29 & \textbf{73} & \textbf{13} & \textbf{34} \\
%  & no ctx & \textbf{39} & 48 & 8 & 29 \\ \bottomrule 
% \end{tabular}%
% }
% \caption{The table shows the counts of how many times the context and no context paraphrases are chosen when (1) the choice was difficult, (2) the choice was not difficult, (3) all three annotators agreed on the choice, and (4) using majority vote. It turns out that when sentence rearrangement is forced, the context paraphrases are chosen more often except when it was hard to decide between two options.}
% \label{tab:other}
% \end{table}

% We manually analyzed 16 comments\footnote{Eight comments for when they chose the context paraphrase as the better paraphrase and eight for when they chose the no context paraphrase.} from each of the three annotators in the second experiment (\texttt{order} = $40$) (see Table \ref{tab:manual-comment-analysis}). We categorized the aspects mentioned in the comments and found that there are common merits and demerits in both types of paraphrases but no aspect is particularly outstanding. For example, while the annotators mentioned 9 times that the no context paraphrases used good word substitutions, there were also 9 times that they found the word choices in the no context paraphrases were bad. However, we did find that \textbf{the no context paraphrases were especially bad at sentence rearrangement}, which coincides with the results reported in Table \ref{tab:other}.

% \begin{table*}[]
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}p{5.6cm}lp{4.4cm}lp{4.5cm}lp{4.5cm}l@{}}
% \toprule
% \textbf{ctx good} & \textbf{\#} & \textbf{no ctx good} & \textbf{\#} & \textbf{ctx bad} & \textbf{\#} & \textbf{no ctx bad} & \textbf{\#} \\\midrule
% good word choice/diversity & 3 & good word choice/diversity & 9 & bad word choice/low diversity & 3 & bad word choice/low diversity & 9 \\
% closer to the original & 8 & closer to the original & 4 & missing/wrong info & 9 & missing/wrong info & 2 \\
% good fit in the context & 3 & good fit in the context & 0 & repetition & 4 & repetition & 1 \\
% good summary of the original & 3 & good summary of the original & 3 & confusing & 3 & confusing & 2 \\
% good sentence arrangement & 2 & good sentence arrangement & 3 & bad sentence arrangement & 3 & bad sentence arrangement & 9 \\
% perfect & 1 & perfect & 2 & bad punctuation & 2 & bad punctuation & 1 \\
% simplicity & 1 & feasible & 1 & less concise & 1 & lengthy & 3 \\
% states things perfectly and differently & 1 & concise & 1 &  &  & not fit in the context & 1 \\
% clear & 1 & consistent in names & 1 &  &  & awkward phrase & 1 \\
% more accurate & 1 &  &  &  &  & bad grammar & 1 \\
% vivid & 3 &  &  &  &  &  & \\ \bottomrule
% \end{tabular}%
% }
% \caption{The table lists the good and bad aspects that are mentioned in the 16 comments from each annotator in the second experiment (\texttt{order} = 40). The numbers are how many times each aspect was mentioned in the comments.}
% \label{tab:manual-comment-analysis}
% \end{table*}


% \begin{table*}[t!]
% \small
% \begin{center}
% \begin{tabular}{ lrrrrr } 
%  \toprule
%  Model & \lexical & \syntax & \simmetric & $J$(\textsc{l,s}) & $J$(\textsc{o,s}) \\
%  \midrule
% \textsc{copy} & 0.0 & 0.0 & 100.0 & 0.0 & 0.0 \\
% \textsc{ref} & 52.6 & 11.4 & 82.9 & 39.5 & 9.1 \\
% \midrule
% \textsc{dips}~\citep{kumar-etal-2019-submodular} & 28.5 & 3.9 & 98.5 & 27.3 & 3.8 \\
% \textsc{sow-reap}~\citep{goyal2020neural} & 35.1 & 30.7 & 95.9 & 32.3 & 29.7 \\
% \textsc{pegasus-pp}~\citep{rajauria2020paraphrase} & 30.7 & 18.5 & 95.9 & 28.3 & 18.0 \\
% \textsc{strap}~\citep{style20} \\
% ~~~~$p = 0.0$ & 48.1 & 20.6 & 95.5 & 44.7 & 20.0 \\
% ~~~~$p = 0.9$ & 60.3 & 25.4 & 84.3 & 48.4 & 22.1 \\
% \textsc{protaugment}~\citep{dopierre-etal-2021-protaugment} & \\
% ~~~~unigram penalty & 80.1 & 12.9 & 32.7 & 23.9 & 4.8 \\
% ~~~~bigram penalty & 58.4 & 24.8 & 79.6 & 43.5 & 19.6 \\
% \textsc{T5-large-div}~\citep{goutham2021paraphrase} & 27.3 & 11.8 & 91.9 & 23.3 & 10.9 \\
% \textsc{parrot}~\citep{prithivida2021parrot} & 21.9 & 6.7 & 97.2 & 20.0 & 6.7 \\
% \midrule
% \model~(ours) & \\
% \bottomrule
% \end{tabular}
% \end{center}
% \vspace{-0.1in}
% \caption{Automatic evaluation of single sentence paraphrasing.}
% \vspace{-0.1in}
% \label{tab:prior-auto-eval}
% \end{table*}


\begin{table}[ht]
\caption{Representative model outputs of each lexical code with Likert ratings and comments from the annotators. The texts in bold in the original texts are the context. \textcolor{OrangeRed}{Red words} are the content being changed in the original text and \textcolor{ForestGreen}{green words} are the changed content in the paraphrases.}
\label{tab:cherry-pickedexamples-appendix}
\vspace{\baselineskip}

\centering
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}[b]{@{}p{0.5cm}p{7cm}p{5.5cm}p{4cm}@{}}
\toprule
  \textbf{L} &
  \textbf{Original} &
  \textbf{Paraphrase} &
  \textbf{Annotator Comment}\\\midrule
  20 &
  \textbf{The first, originally used as Renard's theme, is heard during the mammoth "Antonov" cue on the recording, and is written for piano. The second new theme, used in the "Christmas in Turkey" track of The World Is Not Enough, is reused in the "Going Down Together" track.} Both themes were \textcolor{OrangeRed}{later} adapted by David Arnold and \textcolor{OrangeRed}{featured} in the film\textcolor{OrangeRed}{'s} score. The first theme \textcolor{OrangeRed}{appears} in the \textcolor{OrangeRed}{cue} "M's Confession" and the second \textcolor{OrangeRed}{theme appears} in the \textcolor{OrangeRed}{cue} "Gone Fishing". &
  Both themes were \textcolor{ForestGreen}{subsequently} adapted by David Arnold and \textcolor{ForestGreen}{used} in the film score. The first theme \textcolor{ForestGreen}{is used} in the "M's Confession" \textcolor{ForestGreen}{cue}, and the second in the "Gone Fishing" \textcolor{ForestGreen}{cue.} &
  \textbf{5---Approx. equivalent}\newline While the paraphrase is worded in a more simplistic way than the original and has shortened clauses, the information presented in both is the same.\\\midrule
  20 &
  \textbf{Churchill argues that the United States instituted blood quantum laws based upon rules of descendancy [\ldots]} He has argued that the United States has used them to strip \textcolor{OrangeRed}{Indigenous} people of their rights, \textcolor{OrangeRed}{land} and culture in order to benefit those in power. \textcolor{OrangeRed}{He contends} that these laws were designed to create \textcolor{OrangeRed}{a divide} among \textcolor{OrangeRed}{Indigenous people, making} them easier to control and manipulate. \textcolor{OrangeRed}{Churchill} has \textcolor{OrangeRed}{further} argued that this manipulation continues today, \textcolor{OrangeRed}{as Indigenous} people are often forced to deny their heritage in order to have access to resources and opportunities that are denied to them \textcolor{OrangeRed}{as a result} of their heritage. &
  He has argued that the United States has used them to strip \textcolor{ForestGreen}{indigenous} people of their rights, \textcolor{ForestGreen}{land,} and culture in order to benefit those in power. \textcolor{ForestGreen}{Churchill also argues} that these laws were designed to create \textcolor{ForestGreen}{divisions} among \textcolor{ForestGreen}{indigenous people in order to make} them easier to control and manipulate. \textcolor{ForestGreen}{He} has \textcolor{ForestGreen}{also} argued that this manipulation continues today, \textcolor{ForestGreen}{in that indigenous} people are often forced to deny their heritage in order to have access to resources and opportunities that are denied to them \textcolor{ForestGreen}{because} of their heritage. &
  \textbf{5---Approx. equivalent}\newline The paraphrase retains all the information of the original, but uses synonymous language in several places. Also, the paraphrase replaces the pronoun "he" with "Churchill" and vice versa.\\\midrule
  40 &
  \textbf{When tailoring his 1970 composition "I'm the Greatest" for Starr to record on the 1973 album Ringo, Lennon referenced the song title with the lines [$\ldots$]} Lennon's original \textcolor{OrangeRed}{line} was changed to "Back off Boogaloo" to \textcolor{OrangeRed}{reference the song's title, creating} the line \textcolor{OrangeRed}{that} Starr sang \textcolor{OrangeRed}{in} the finished \textcolor{OrangeRed}{version.} &
  Lennon's original \textcolor{ForestGreen}{reference to "I'm the Greatest"} was changed to "Back off Boogaloo" to \textcolor{ForestGreen}{create} the line Starr sang \textcolor{ForestGreen}{on} the finished \textcolor{ForestGreen}{recording.} &
  \textbf{4---Nearly equivalent}\newline The original does not specify that the line in question was "I'm the Greatest." However, this can be inferred from the rest of the passage. Otherwise, the paraphrase and the original are quite similar.\\\midrule
  40 &
  \textbf{Some monuments were vandalized and some were moved to new locations. [\ldots]} In some cases, \textcolor{OrangeRed}{the} monuments were moved to new locations. \textcolor{OrangeRed}{For example, the stairway of the} Great Pyramid of \textcolor{OrangeRed}{Cholula} in \textcolor{OrangeRed}{Mexico} was \textcolor{OrangeRed}{relocated} in the 19th century and then returned to its original \textcolor{OrangeRed}{location} in the \textcolor{OrangeRed}{20th century. Other monuments} were destroyed during wars or religious \textcolor{OrangeRed}{conflicts,} such as the Buddhas of Bamiyan in Afghanistan, which were destroyed by the Taliban in 2001. &
  In some cases, monuments were moved to new locations. \textcolor{ForestGreen}{The} Great Pyramid of \textcolor{ForestGreen}{Cholula,} in \textcolor{ForestGreen}{Mexico,} was \textcolor{ForestGreen}{moved} in the 19th century and then returned to its original \textcolor{ForestGreen}{place} in the \textcolor{ForestGreen}{20th. Others} were destroyed during wars or religious \textcolor{ForestGreen}{controversies,} such as the Buddhas of Bamiyan in Afghanistan, which were destroyed by the Taliban in 2001. &
  \textbf{3---Somewhat equivalent}\newline Only one major change in this short text, but I feel it's significant. The original text speaks of the moving of a pyramid's staircase, but the paraphrased text implies that the entire pyramid was moved and then replaced. This massively changes the meaning of this text. \\\midrule
  60 &
  \textbf{Heaving wildly, the Hunyani began to descend rapidly. [\ldots]} The crew \textcolor{OrangeRed}{was unable to} regain \textcolor{OrangeRed}{control} and the Hunyani \textcolor{OrangeRed}{crashed} into the ground \textcolor{OrangeRed}{at 17:14, killing all onboard.}  The \textcolor{OrangeRed}{crash} was investigated by the Civil Aviation Authority of \textcolor{OrangeRed}{Zimbabwe and the investigation concluded} that the crash was caused by a combination of pilot error, \textcolor{OrangeRed}{fatigue}, and \textcolor{OrangeRed}{weather conditions.} The \textcolor{OrangeRed}{pilot} had \textcolor{OrangeRed}{failed to} properly \textcolor{OrangeRed}{assess} the weather \textcolor{OrangeRed}{conditions before takeoff, instead relying} on a forecast from a private \textcolor{OrangeRed}{weather} service. The weather \textcolor{OrangeRed}{conditions worsened as the plane flew and with the pilots already fatigued, they were unable to appropriately react to the changing conditions. The combination of these factors led to the plane losing control and crashing.} &
  The crew \textcolor{ForestGreen}{could not} regain \textcolor{ForestGreen}{control,} and \textcolor{ForestGreen}{at 17:14} the Hunyani \textcolor{ForestGreen}{plunged} into the ground \textcolor{ForestGreen}{and all those on board were killed.} The \textcolor{ForestGreen}{accident} was investigated by the Civil Aviation Authority of \textcolor{ForestGreen}{Zimbabwe, which found} that the crash was caused by a combination of pilot error, \textcolor{ForestGreen}{fatigue} and \textcolor{ForestGreen}{the weather.} The \textcolor{ForestGreen}{pilots} had \textcolor{ForestGreen}{not} properly \textcolor{ForestGreen}{examined} the weather \textcolor{ForestGreen}{before take-off, and instead had relied} on a forecast from a private \textcolor{ForestGreen}{meteorological} service. The weather \textcolor{ForestGreen}{deteriorated during the flight, and the exhausted pilots were unable to react properly to the deteriorating conditions.} & \textbf{5---Approx. equivalent}\newline Well chosen words make the alternate version different but retain the original meaning well. The only difference is that the paraphrased version does not have the concluding sentence, but it does not alter the meaning in any way as the facts have already been included previously.\\\midrule
  60 &
  \textbf{Sonic's main means of attack is the Spin Attack, in which he curls into a ball and rapidly spins his body, damaging enemies and certain obstacles upon collision. This may be performed by jumping or by rolling on the ground.} \textcolor{OrangeRed}{Sonic can also} use a variety of other \textcolor{OrangeRed}{moves}, such as the \textcolor{OrangeRed}{Homing Attack, Slide Attack, and Wall Jump. He also has access to power-ups like the Speed Shoes, Invincibility, and the Super Peel Out. In some games, Sonic can also use} special items or vehicles to traverse \textcolor{OrangeRed}{levels.} &
  \textcolor{ForestGreen}{Also, the character can} use a variety of other \textcolor{ForestGreen}{attacks,} such as the \textcolor{ForestGreen}{homing attack, the slide attack, and the wall jump. In addition, he can acquire power-ups such as the Speed Boost, Super Speed, and the invincibility meter, and in certain games, he may make use of} special items or vehicles to traverse \textcolor{ForestGreen}{the world.} &
  \textbf{2---Topically related}\newline In the second part of the paraphrase, the writer ignores the actual moves and skills of Sonic and invents some entirely different ones that not mentioned in the original text. The method of performing the attack moves has also been missed out.\\\bottomrule
\end{tabular}%
}
\end{table}
%https://docs.google.com/spreadsheets/d/10LtBZLLuTut2kDdyiCkCkYWovnmgTESOABqse-P30EU/edit?usp=sharing