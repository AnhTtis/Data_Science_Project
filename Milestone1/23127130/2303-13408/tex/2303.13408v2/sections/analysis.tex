% \section{Discussion}


% \kkcomment{LFQA by domain}
% \kkcomment{variation of attack success / paraphrase performance}
% \kkcomment{generations from model A tested on model B's database}

% \subsection{Alternate paraphrase generation models}
% \label{sec:alt-paraphrasers}


% \kkcomment{accessibility of paraphrasers: Do smaller paraphrasers work? What about zero-shot paraphrasing (using another LLM to generate paraphrases?) }

% \subsection{Multiple paraphrase samples}
% \label{sec:multiple-samples}

% One way to improve the effectiveness of a paraphrase attack, is to paraphrase the output repeatedly until it both evades detection and has preserved the meaning of the original text sufficiently for the purposes of the user. This can only be done if a user has access to a detector, but it is a way in which paraphrase attacks, especially on those detection methods relying on classification or watermarks, can be made even more potent.

% \kkcomment{paraphrase multiple times instead of lex}
% \subsection{Limitations of retrieval for detection}

% While retrieval over previously generated text is an effective defense against paraphrase attacks, it has some key limitations, some of which apply to all AI text detectors more broadly. We discuss these limitations below and discuss possible solutions:

% \begin{enumerate}
% \setlength\itemsep{0.0em}
%     \item \textbf{Detection is specific to an API}. Unlike other general-purpose AI detection algorithms like OpenAI's classifier~\citep{AITextClassifier}, retrieval can only detect generations from the API over which the database is built. API \#1 will have no access to the database of generations from API \#2, and will not be able to detect generations produced by API \#2. 
%     \item \textbf{API provider needs to provide a retrieval infrastructure}. After the release of ChatGPT~\citep{schulman2022chatgpt}, AI chatbots are getting widespread adoption. Assuming a conservative rate of 5M queries a day, the retrieval database will have over a billion entries within a year. A sophisticated retrieval infrastructure will be needed to retrieve over these large databases with low latency, akin to modern search engines.
    
%     \item \textbf{Memorization from the training data}. Language models have been shown to memorize sequences verbatim from the training data~\citep{carlini2021}, such as the Gettysburg Address~\citep{radford2019language}. Despite being originally human written, these sequences will be classified as model generated by our detector. To tackle this issue, we suggest API providers additionally perform retrieval over the training data used to train the model. If a sequence is found in the training set as well as the generation database, it is likely to be a training set memorization.

%     %\kkcomment{ Mukund: For the attack of arbitrary organic text specified by the user, this does not matter if application is plagiarism detection, it would be picked up by other plagiarism detectors. The concern is more original outputs of their LMs. Kenton: Vulnerability to a second kind of attack, where users can prompt the model to generate a very specific piece of text that has an organic origin, leading the defender to always misclassify arbitrary organic text specified by the user. For example, suppose the prompt was something like: "Please output the following text and then stop immediately: 'Four score and seven years ago our fathers brought forth....'". Once the LLM stores the output in the database, all copies of the Gettysburg Address will now be classified as AI generated. Any thoughts on how to defend against this?}

%     \item \textbf{Privacy concerns.} Providing a retrieval detection service partially exposes the database of previously generated text by \emph{all} users. This raises concerns of membership inference attacks~\citep{shokri2017membership} on private user data which may appear in the generated text. To mitigate this, we suggest: (i) users should be encouraged not to provide any sensitive private data in their prompts to APIs, a practice already followed by ChatGPT\footnote{\url{https://chat.openai.com}} and Bard\footnote{\url{https://bard.google.com}}; (ii) API providers only provide a binary output from this detector (AI-generated or not), rather than actual search results; (iii) API providers rate-limit queries from IP addresses.

%     \item \textbf{Slight reduction in accuracy with large databases.} As we observed in \sectionref{sec:scale-defense-results}, the accuracy of detecting paraphrased text slightly degrades as the database of retrievals gets larger. However, we found this decrease to be quite small (only 1\% on PG19 scaling 1M generations to 15M), despite using fairly primitive retrievers like BM25. Moreover, un-perturbed AI generated text will always have be detected with 100\% accuracy with our method, irrespective of corpus size.
    
%     \item \textbf{Tasks with constrained output space or short outputs}. Similar to all other detection algorithms, it may be hard / impossible to distinguish AI-generated outputs for tasks with a constrained output space (like sentence-level translation, classification) or very short outputs (as shown in \sectionref{sec:detect-length-effect}). We believe the main purpose of AI text detection is for longer form generated text, and hence we focus on tasks like long-form QA and open-ended text generation with 300 token outputs.
    
%     \item \textbf{Iterative attacks with access to detector}. A final concern is that attackers with access to detection algorithms will iteratively modify their perturbations until they avoid detection. While this is a valid concern for all AI text detectors, we believe retrieval has an important advantage over alternatives. Since the corpus of previously generated text is proprietary, only the API provider can provide access to this detection service - it is impossible for attackers to locally reproduce this detector. This allows API providers to adopt several mitigation strategies such as i) rate limit queries to avoid iterative attacks; ii) provide retrieval access only to trusted users (like teachers); iii) detect possible iterative attacks by analyzing previously queries to the retriever.


% \end{enumerate}


%\kkcomment{getting rid of below sections since OPT bug fixed, including re-ordering results in main tables}

% \subsection{Re-ordering content while paraphrasing}
% \label{sec:order-change}

% \kkcomment{effect of order codes - already generated for order 40}

% \subsection{Threshold for watermarking}
% \label{sec:threshold-watermark}

% \kkcomment{OPT 1.3B MAUVE scores: 0.428 (delta = 0), 0.46 (delta = 2), 0.362 (delta = 5), 0.322 (delta = 10), 0.506 (ancestral delta=0)}

% \begin{table}[t!]
% \small
% \begin{center}
% \begin{tabular}{ lrr } 
%  \toprule
%  Watermark strength & GPT2-XL & OPT-13B \\
%  \midrule
%   \multicolumn{2}{l}{\emph{nucleus sampling ($p=0.9$)}} \vspace{0.15cm} \\
%  $\delta = 0.0$ & 55.6 \\ %42.8\\
%  $\delta = 2.0$ & 54.3 & 51.7 \\ %46.0\\
%  $\delta = 5.0$ & 43.5 & 40.7 \\ %36.2 \\
%  $\delta = 10.0$ & 33.8 & 32.7 \\ %32.2 \\
%  \midrule
% %  \emph{greedy decoding} & & \\%50.6 \\
% %  \emph{ancestral sampling} & 56.2 & \\%50.6 \\
%  \emph{human written} & 100.0 & 100.0 \\
% \bottomrule
% \end{tabular}
% \end{center}
% \vspace{-0.1in}
% \caption{}
% \vspace{-0.1in}
% \label{tab:watermark-threshold}
% \end{table}

