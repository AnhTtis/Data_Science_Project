\section{More background on detectors of AI-generated text}
\label{sec:detection-background}

In this section, we provide an overview of existing algorithms that have been developed for the purpose of detecting machine-generated text. Such algorithms fall into three main categories: (1) watermarking algorithms, which modify the generative algorithm to encode hidden information unique to the API (\appendixref{sec:watermark}); (2) statistical outlier detection methods, which do not modify the generative algorithm but look for inherent artifacts in generated text (\appendixref{sec:outlier-detection}); and  (3) classifiers trained to discriminate machine-generated text from human-written text (\appendixref{sec:classifiers}). Finally, in~\appendixref{sec:sadasivan}, we compare and contrast our work to ~\citet{sadasivan2023aigenerated}, who also note the efficacy of paraphrasing attacks but do not consider a retrieval-based defense in their pessimistic conclusion about the fate of AI-generated text detection.

%Given the recent success of large language models (LLMs) like ChatGPT~\citep{schulman2022chatgpt} in producing long-form human-like text, there's an increasing fear that LLMs will be used for malicious purposes like fake news generation and cheating in academic writing assignments. This has led to the development of several mitigation techniques, which detect if a long-form text was human or machine generated. Methods to detect machine-generated text roughly fall under two categories: watermarking \& outlier detection.\\

\subsection{Watermarking language model outputs}
\label{sec:watermark}

A ``watermark'' is a modification to the generated text that can be detected by a statistical algorithm while remaining imperceptible to human readers. Effective watermarks are difficult to remove and have little effect on the quality of generated text. Prior work attempted to watermark natural language using syntax tree manipulations~\citep{topkara2005natural, meral2009natural}, and this area has gotten renewed interest with large language models generating human-like text~\citep{abdelnabi2021adversarial, grinbaum2022ethical}. Most recently,~\citet{kirchenbauer2023watermark} propose a simple algorithm that only requires access to the LLM's logits at each time step to add watermarks. The watermark can then be verified with only blackbox access to the LM and knowledge of a specific hash function. This algorithm operates in three steps:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Mark a random subset of the vocabulary} as ``green tokens'' (or tokens representing the watermark, as shown in \figureref{fig:watermark-evade}) using the hash of the previously generated token as a random seed. A total of $\gamma |V|$ tokens are marked green where $\gamma$ is the fraction of the tokens that are watermarked with default $\gamma = 0.5$.


    \item \textbf{Increase the logit value} for every green token by a constant $\delta$ ($= 2$ by default), which denotes the watermark strength. This raises the probability of sampling green watermarked tokens, especially for high-entropy distributions.


    \item \textbf{Sample sequences} using decoding algorithms such as nucleus sampling~\citep{holtzman2020curious}, leveraging the modified probability distribution at each timestep before truncation.

\end{enumerate}

\noindent \textbf{Detecting the watermark}: Verifying whether a text is generated by a watermarked LM is possible with just knowledge of the hash function and tokenizer. Specifically, the verifier tokenizes the text and counts the number of green tokens it contains. This is used to calculate the standard normal score ($z$-score) for the hypothesis test. If the sequence with $T$ tokens contains a certain number of the green token (denoted as $|s|_G$), the $z$-score can be computed by:
\begin{align*}
    z &= (|s|_G - \gamma T) / \sqrt{T\gamma (1 - \gamma)}
\end{align*}

Intuitively, a higher $z$-score implies it is less likely for a human to have written the text (null hypothesis) since it contains a higher than expected number of green tokens.~\citet{kirchenbauer2023watermark} recommend using a high $z$ value ($z > 4$, or $p < 3 \times 10^{-5}$) to reduce the risk of false positives (human-written text classified as AI-generated). Low false positive rates are critical in AI-generated text detection algorithms~\citep{AITextClassifier}---we discuss this in \sectionref{sec:eval-metrics-attacks}.

\subsection{Statistical outlier detection methods}
\label{sec:outlier-detection}

 Unlike the watermarking algorithms, outlier detection algorithms make no modification to the generative algorithm. Instead, they attempt to distinguish between human-written and machine-generated text based on the presence of artifacts in generated text~\citep{see-etal-2019-massively, holtzman2020curious}. Early methods detect statistical irregularities in measures such as entropy~\citep{lavergne2008detecting}, perplexity~\citep{beresneva2016computer}, and $n$-gram frequencies~\citep{grechnikov2009detection, badaskar-etal-2008-identifying}. After the release of GPT-2,~\citet{gehrmann-etal-2019-gltr} introduced the GLTR visualization tool to assist human verifiers in detecting machine-generated text. Most recently, the release of ChatGPT has prompted the development of two new tools, namely a closed-source tool called GPTZero~\citep{GPTZero}, and open-source DetectGPT~\citep{mitchell2023detectgpt}. DetectGPT uses an observation that model-generated text lies in the negative curvature regions of the model's log probability function. It constructs multiple perturbations of the model generated text (using a mask-and-fill strategy), and compares the log probability of the perturbations with the unperturbed generation. Text is considered model generated if the log probability of the unperturbed text is significantly higher than the log probability of perturbations.




\subsection{Classifiers}
\label{sec:classifiers}

The third class of detection methods relies on classifiers that are fine-tuned to distinguish human-written text from machine-generated text. Early efforts in this vein use classifiers to detect fake reviews~\citep{hovy-2016-enemy} and fake news~\citep{zellers2019defending}. Other related studies examine classification performance across domains~\citep{bakhtin2019real} and decoding strategies~\citep{ ippolito-etal-2020-automatic}. Such studies inspired others to use their insights to improve generative performance~\citep{dengresidual,krishna-etal-2022-rankgen}. Most recently, OpenAI fine-tuned a GPT model to perform this discrimination task and released it as a web interface~\citep{AITextClassifier}. They fine-tuned this classifier using generations from 34 language models, with text sourced from Wikipedia, WebText~\citep{radford2019language}, and their internal human demonstration data.

%~\citet{sadasivan2023aigenerated}
\subsection{Comparison to Sadasivan et al. (2023)}
\label{sec:sadasivan}
In very recent concurrent work, ~\citet{sadasivan2023aigenerated} also demonstrate the utility of paraphrasing attacks against AI-generated text detectors. While their work makes use of off-the-shelf sentence-level paraphrase models, \model\ possesses advanced discourse-level rewriting capabilities as well as fine-grained diversity control, which allows us to thoroughly analyze the effectiveness of various paraphrasing strategies. Our experiments also encompass more tasks, datasets, and detection algorithms. Moreover, we evaluate larger language models like GPT3.5-davinci-003. Finally and most importantly, our retrieval-based defense \emph{directly contradicts} the ``impossibility result'' of~\citet{sadasivan2023aigenerated} and its associated proof, which states that even an optimal detector will approach the performance of a random classifier 
%as the quality of LLM-generated text reaches that of human text
as the distance between the distributions of LLM-generated text and human generated text goes to zero. Since our detector does not rely on properties of the text but rather a corpus search, the quality of the generated text is irrelevant to the effectiveness of our detector, and thus their proof does not apply to our method.

% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=\textwidth]{figures/dipper-idea}
%     \caption{An illustration of the method used to train \model\ on English translations of the French novel \emph{The Nun}. We first align sentences between the two translations to create parallel data. Next, a subset of the alignments are chosen; in this example, we use ($p_2$, $q_2$) and ($p_3$, $q_3q_4$). We shuffle sentences, compute control codes, and finally fine-tune a T5-XXL LM to generate $p_2p_3$ given $q_3q_4q_2$ and the context $p_1$ and $p_4$.}
%     \label{fig:dipper-training}
% \end{figure*}