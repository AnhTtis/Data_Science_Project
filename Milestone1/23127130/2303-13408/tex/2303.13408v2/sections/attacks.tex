\section{Experiments attacking detection algorithms with \model}
\label{sec:attacks}

%experiments attacking existing AI-generated text detectors with \model. We  first detail our

In this section, we describe our experimental setup in \sectionref{sec:eval-metrics-attacks}-\ref{sec:expt-setup} and present our results in \sectionref{sec:attack-expts}. Overall, we find that \model\ evades all detectors across three LLMs (including GPT3.5). %and two tasks (open-ended generation and long-form QA).


\begin{table}[t!]
\caption{Performance of detection algorithms (at 1\% FPR) before and after \model\ paraphrasing on \textbf{open-ended generation} using Wikipedia prompts (300 generated tokens). As the diversity (L,O) increases, detection rates decrease across algorithms, with nearly perfect semantic similarity (Sim). *GPT3.5 DetectGPT scores computed using 200 samples at 20\% FPR as it scores 0\% at a 1\% FPR.}
\label{tab:watermark-attacks}
\vspace{\baselineskip}

\small
\centering
\begin{tabular}{@{}lrrrrrr@{}} 
 \toprule
  %\multicolumn{7}{c}{\textbf{Paraphrase attacks on open-ended generation} (Wikipedia prompts, 300 generated tokens)}\vspace{0.1in} \\ 
  Metric\hspace{0.09in} $\rightarrow$ & Sim $\uparrow$ & \multicolumn{5}{c}{Detection Accuracy $\downarrow$} \\
  \cmidrule{3-7}
 Detector $\rightarrow$ & & Watermarks & DetectGPT  & OpenAI & GPTZero & RankGen  \\
 %& & \citeyearpar{kirchenbauer2023watermark} & \citeyearpar{mitchell2023detectgpt}  & \citeyearpar{AITextClassifier} & \citeyearpar{GPTZero} & \citeyearpar{krishna-etal-2022-rankgen}\\

% Metric $\rightarrow$ & Sim $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ & Acc $\downarrow$ & S\&A $\uparrow$ \\
 
 \midrule
 GPT2-1.5B  & - & 100.0 & 70.3\phantom{*} & 21.6 & 13.9  & \textbf{13.5} \\
 % sim scores = using detectgpt sim, since it's the worst SIM and good to have a conservative estimate here
 + \model\ 20L  & 99.2 & 97.1 & 28.7\phantom{*} &  19.2  & 9.1 & 15.8 \\
 + \model\ 40L & 98.4 & 85.8 & 15.4\phantom{*} & 17.8 & 7.3 & 18.0 \\
 + \model\ 60L & 96.9 & 68.9 & 8.7\phantom{*} & \textbf{13.3} & 7.1 & 19.8  \\
 + \model\ 60L, 60O & 94.3 & \textbf{57.2} & \textbf{4.6}\phantom{*} & 14.8 & \textbf{1.2} & 28.5 \\
 \midrule
 OPT-13B & - &  99.9 & 14.3\phantom{*} & 11.3 & 8.7 & \textbf{3.2}  \\
 + \model\ 20L & 99.1 & 96.2 & 3.3\phantom{*} & 11.8  & 5.4 & 5.2 \\
 + \model\ 40L & 98.6  & 84.8 &  1.2\phantom{*}  & 11.6 & 3.8 & 6.6  \\
 + \model\ 60L & 97.1 & 63.7 & 0.8\phantom{*} & \textbf{9.1} & 6.3 & 9.3   \\
 % sim = 96.9
 + \model\ 60L, 60O & 94.6 & \textbf{52.8}  & \textbf{0.3}\phantom{*} & 10.0 & \textbf{1.0} & 13.5\\
 \midrule 
 GPT-3.5-175B, davinci-003  & - & - & 26.5* & 30.0  & 7.1  & \textbf{1.2} \\
 + \model\ 20L & 97.6 & -& 12.5*  & 20.6  & 4.3 & 1.7  \\
 + \model\ 40L & 96.7 & - & 8.0*  & 22.4  & 4.8  &  2.0 \\
 + \model\ 60L & 94.2 & - & 7.0*  & \textbf{15.6} & 6.1  & 3.9  \\
 + \model\ 60L, 60O & 88.4 & - & \textbf{4.5}*  & \textbf{15.6} & \textbf{1.8} & 7.3\\
 \midrule
 % 1.7
 Human Text & - & 1.0 &  1.0\phantom{*} & 1.0 & 1.0 & 1.0 \\
\bottomrule
\end{tabular}
\vspace{-0.1in}
\end{table}

\subsection{Evaluation metrics}
\label{sec:eval-metrics-attacks}

\textbf{Detection accuracy}: Our first metric measures how often the input text is correctly detected as AI-generated. Since detection rates are heavily dependent on the chosen detection threshold, the AUC-ROC metric is commonly used to measure detector performance~\citep{mitchell2023detectgpt}, which considers the range of all possible thresholds. However, in this application, it is critical that the \emph{false positive rate} (FPR) is low; in other words, human-written text must almost never be classified as machine-generated~\citep{AITextClassifier, kirchenbauer2023watermark}. Hence, we fix the FPR to 1\% for all detection algorithms (although even 1\% is likely too high in practice), and adjust the detection threshold accordingly while reporting detection accuracies. Additionally, we also plot ROC curves focusing on the 0-1\% FPR region. Overall, we expect detection rates to plummet on paraphrased text.
%at a constant FPR of 1\%,


\textbf{Semantic similarity (Sim)}: Detection accuracy is an insufficient evaluation of our attack's success. We also need to measure whether the original and paraphrased generations share approximately the same semantics. We measure semantic similarity using the state-of-the-art semantic similarity model \spavg from~\citet{wieting-etal-2022-paraphrastic}, an embedding averaging model trained on a large corpus of filtered paraphrase data~\citep{wieting-gimpel-2018-paranmt}.  \spavg is a well-calibrated metric that performs well on semantic calibration tests as well as plagiarism detection in STS benchmarks~\citep{agirre-etal-2016-semeval}.  \spavg is also robust against topically similar non-paraphrases. We found that \spavg\ it scores just 0.09 on random pairs of paragraphs from the same book (topically similar paragraphs but not paraphrases) in the \booktranslate\  dataset~\citep{thai2022booktranslate}. In contrast, the average \spavg score of actual human paraphrase pairs in \booktranslate\ is 0.76. We consider semantics to be approximately preserved if the \spavg score is greater than this average human paraphrase score of 0.76.

Besides semantic similarity, we conduct several automatic evaluations, ablation studies, and human evaluations of intrinsic paraphrase quality in \appendixref{sec:intrinsic-evaluation}.

\subsection{Models, datasets \& detection algorithms}
\label{sec:expt-setup}

\textbf{Base language models}: We conduct attacks on three language models of varying sizes that belong to different model families. We consider the GPT2-XL model (1.5B parameters)~\citep{radford2019language}, the OPT-13B model~\citep{zhang2022opt}, and the \texttt{text-davinci-003} variant from the GPT-3.5 family~\citep{brown2020language}, which has 175B parameters and has additionally been instruction tuned using reinforcement learning from human feedback~\citep{ouyang2022training}. For all LMs, we sample generations that are 300 tokens long before passing them through \model\ for the attack experiments.\footnote{For GPT2-XL and OPT-13B, we generate text using nucleus sampling~\citep{holtzman2020curious} with $p=0.9$. For \texttt{davinci-003}, we use the default hyperparameters on the API Playground (temperature = 0.7).}


\textbf{Natural language generation tasks}: We experiment with two long-form text generation tasks, since most malicious applications (e.g., fake article creation) are associated with long-form outputs. First, we consider \emph{open-ended generation}, where an LM generates a continuation to a two-sentence prompt. To obtain our prompts, we sample 3K contiguous two-sentence chunks from the validation split of WikiText-103~\citep{meritypointer} and use the next 300 tokens as the ``human-written'' continuation. Second, we evaluate \emph{long-form question answering}~\citep{fan2019eli5}, in which an LM answers a question with a 300-word answer (dataset details in \appendixref{appendix:attack-details}). For our main results, the human reference answers or continuations are only used to adjust detection thresholds of studied methods to maintain a 1\% FPR.\footnote{Alternatively, if a random half of the human-written data was used for threshold adjustment, we find the other half has a FPR of 0.8\%-1.2\% across random splits, and this deviation will reduce with a bigger dataset.} Note that we are not removing human-written text from our test set. Our metric is equivalent to having a test set with a 50-50 split between machine/human-written text for the same prompts, and observing the FPR$=$1\% point in the ROC curve (also provided in \appendixref{appendix:more-roc-curves}).

%\footnote{Evaluating how well the studied LMs perform on these two tasks is a challenging problem in its own right that could make additional use of the human references, but this is irrelevant to our paper.}

\begin{wrapfigure}{r}{0.5\textwidth}
\caption{Detector performance (at 1\% FPR) on \textbf{long-form QA} before/after paraphrasing. As diversity (L,O) increases, detection rates decrease with very high semantic preservation (Sim). WM: Watermark, D.GPT: DetectGPT, O.AI: OpenAI. *GPT3.5 D.GPT uses 100 samples at 20\% FPR to show attack success, as it scores 0\% at 1\% FPR.}
\label{tab:attacks-lfqa}
\vspace{0.05in}

\small
\centering
\begin{tabular}{ lrrrr } 
 \toprule
 % \multicolumn{5}{c}{\textbf{Long-form Question Answering}, 300 generated words}\vspace{0.1in} \\ 
 Metric $\rightarrow$ & Sim $\uparrow$ & \multicolumn{3}{c}{Detection Accuracy $\downarrow$} \\
 \cmidrule{3-5}
& & W.M. & D.GPT  & O.AI \\
 \midrule
% \multicolumn{4}{l}{\emph{Long-form QA (300 generated tokens)}}\vspace{0.15cm} \\
 GPT2-1.5B  & - & 100.0  & 74.9\phantom{*} &  59.2 \\
 + \model\ 20L & 99.5 & 98.9 & 45.7\phantom{*} & 35.3   \\
 + \model\ 40L  & 99.0 & 90.7 & 28.0\phantom{*} & 34.4 \\
 + \model\ 60L & 97.5 & 71.1 & 15.8\phantom{*} & \textbf{31.3} \\
 % sim scores = , , , 98.6
 ~~~+ 60L, 60O & 96.2 & \textbf{55.8} & \textbf{7.6}\phantom{*} & 32.7 \\
 \midrule
 OPT-13B & - & 100.0 & 29.8\phantom{*} & 33.5 \\
  % sim scores = 100.0, 99.6, 99.8
 + \model\ 20L & 99.6 & 98.3 & 15.0\phantom{*} & 24.5 \\
  % sim scores = 98.6, 99.4, 100.0
 + \model\ 40L & 99.4 & 87.3 & 6.4\phantom{*} & 24.1 \\
  % sim scores = 97.2, 97.3, 99.4
 + \model\ 60L & 96.5 & 65.5 & 3.2\phantom{*} & \textbf{21.6}   \\
 % sim scores = 94.7
 ~~~+ 60L, 60O & 92.9 & \textbf{51.4} & \textbf{1.5}\phantom{*} & \textbf{21.6} \\
 \midrule 
 GPT-3.5-175B \\
 davinci-003  & - &- & 67.0* & 40.5 \\
 + \model\ 20L & 99.9 &- & 54.0* & 43.1 \\
 + \model\ 40L & 99.8 &- & 36.0* & 43.1 \\
 + \model\ 60L & 99.5 & -& 23.0* & 40.1  \\
 ~~~+ 60L, 60O & 98.3 & - & \textbf{14.0}* & \textbf{38.1} \\
 \midrule
 % 1.7
 Human Text & - & 1.0 & 1.0\phantom{*} & 1.0 \\
\bottomrule
\end{tabular}
\vspace{-0.1in}
\end{wrapfigure}

\textbf{Detection algorithms}: We attack five detectors:\footnote{We consider both model-specific and model-agnostic detectors, as justified in \appendixref{appendix:why-study-model-specific}.} (1) watermarking~\citep{kirchenbauer2023watermark}; (2) DetectGPT~\citep{mitchell2023detectgpt}; (3) GPTZero~\citep{GPTZero}; (4) OpenAI's text classifier~\citep{AITextClassifier};\footnote{This classifier was taken down in July 2023 by OpenAI due to its low accuracy.} and (5) RankGen-XL-all~\citep{krishna-etal-2022-rankgen}.\footnote{While RankGen was not explicitly optimized for this task, it was trained to identify well-written continuations, so we hypothesize that it could also act as a reasonable AI-generated text detector.} We use the default hyperparameters for each detector. We also respect their minimum length specifications, discarding instances where any of the AI-generated text, human-written text, or paraphrased text is shorter than the minimum length. 



\textbf{Paraphrasing AI-generated text}: We pass the prompts for each task and AI-generated responses to those prompts through \model. Specifically, we feed the model input of the form,
\begin{quote}
lexical = L, order = O prompt
\texttt{<p>} generated-text \texttt{</p>},
\end{quote}
 where $L$ and $O$ represent the paraphraser diversity control codes and the \texttt{<p>} and \texttt{</p>} special tokens mark the boundaries of the text to be paraphrased. We use $L = 20, 40, 60$ and $O = 0, 60$ in our main attack experiments. After paraphrasing, we ensure that the AI-generated sequence, paraphrased sequence, and human-written sequence have an equal number of words by truncating them to the length of the shortest among the three. To ensure higher semantic preservation, we iteratively paraphrase long sequences three sentences at a time, keeping already paraphrased text in the context of the generation. To show the effectiveness of our attack, we only \textbf{paraphrase each generation once}, rather than draw multiple samples until it evades detection.\footnote{We discuss this attack briefly in \sectionref{sec:multiple-samples}.}



\subsection{Attacking AI-generated text detectors}
\label{sec:attack-expts}


We present our results in \tableref{tab:watermark-attacks} and \figureref{tab:attacks-lfqa}. Overall we find that:



\textbf{Paraphrasing significantly lowers detection accuracy while preserving input semantics}. Across all LMs, detectors,\footnote{Except RankGen, which scores paraphrases as AI-generated more often than non-paraphrased text. We attribute this to paraphrases being poorer continuations to the prompt compared to the original (\appendixref{sec:intrinsic-evaluation}), an aspect RankGen bases its score on. However, it has low overall performance since it is not trained for this task.} and tasks, paraphrasing significantly lowers detection accuracy across all diversity control codes. For instance, paraphrasing GPT2-XL open-ended generations reduces watermark detection accuracy from 100\% to 57.2\%, and DetectGPT accuracy from 70.3\% to just 4.6\%. Trends are similar even for large LMs like GPT-3.5, for which paraphrasing reduces OpenAI's text classifier accuracy from 30.0\% to 15.6\%. Additionally, \model\ preserves semantics effectively, as 88\%-99\% paraphrases achieve a \spavg\ score higher than the median score of human-written paraphrases. High semantic preservation is supported by careful human evaluations in \appendixref{appendix:HumanEval}. Overall, we find that watermarking is the most resilient detector to paraphrasing.



\begin{wrapfigure}{r}{0.43\textwidth}
    \centering
    \includegraphics[width=0.43\textwidth]{figures/gpt2_xl_fpr_1}
    \caption{ROC plots (0-1\% FPR) for GPT2-XL using different detectors, before (solid lines) and after paraphrasing (dashed). Paraphrasing reduces detection rate across FPRs, and our detector \emph{retrieval} detects paraphrases best; full plots in \appendixref{appendix:more-roc-curves}.}
    \label{fig:auc-plots}
    \vspace{-0.4in}
\end{wrapfigure}

\textbf{Non-watermarking detectors are generally ineffective.} We observe that all detectors apart from watermarking struggle with text generated by larger models like OPT-13B and GPT-3.5, achieving detection accuracies $<$ 50\%. While DetectGPT is effective on the smaller GPT2-XL model (74.9\% on long-form QA), its accuracy drops to just 29.8\% on OPT-13B. Furthermore, GPTZero and RankGen perform the worst among the five detectors on all tested LMs (\tableref{tab:watermark-attacks}), as they are only able to detect < 15\% of non-paraphrased AI-generated text. Thus, we recommend against using these detectors.




\textbf{ROC plots confirm the trends at different false positive rates}. In \figureref{fig:auc-plots}, we plot the detection accuracy (true positive rate) at different values of FPR between 0\% and 1\% for GPT2-XL. Overall, paraphrasing significantly drops detection rates across all FPR thresholds (more plots in \appendixref{appendix:more-roc-curves}).




\subsection{Alternative paraphrasing attacks} 
\label{sec:alt-paraphrasers}
\label{sec:multiple-samples}

%Here, we discuss two other (untested) ways to attack AI-generated text detectors via paraphrasing, which further showcase the brittleness of existing detectors.



\textbf{Paraphrasing multiple times:} Our presented attacks use just a single paraphrase generated by \model\ to evade detection. A simple way to further improve the effectiveness of a paraphrase attack is to sample multiple times\footnote{Precisely, compute $f_\text{dipper}(x)$ for different random seeds while sampling text. Alternatively, an attacker could also compute $f_\text{dipper}(f_\text{dipper}(...f_\text{dipper}(x)))$, but this will lead to excessive semantic drift from $x$.} from \model\ and choose a paraphrase that evades the detector while also preserving semantics. We do not perform this attack as it can only be done if an attacker has access to a detector, which may be a strong assumption (see \appendixref{sec:limitations-retrieval}). That being said, using multiple paraphrase samples can make the attacks even more potent against publicly available detectors.

\textbf{Non-\model\ paraphrasers:} A second alternative is to use non-\model\ paraphrasers that operate at the sentence level. These models can be deployed for long-form text inputs by paraphrasing the inputs sentence by sentence, ignoring prompt context. While the concurrent work of~\citet{sadasivan2023aigenerated} shows that this method can also evade detection, our ablations in \appendixref{sec:intrinsic-evaluation} show that non-contextual versions of \model\ have lower quality and are less compatible with the prompt as \model\ paraphrasers. Moreover, most existing paraphrasers lack fine-grained diversity control and multi-sentence input support (survey in \appendixref{sec:appendix:papersurvey}), two desired properties from an attacker's point of view: attackers want to modify long multi-sentence responses \emph{just enough} to evade detection.

A more interesting option is to use an LLM like ChatGPT to perform few-shot contextual paraphrasing. While this method is likely to provide accurate paraphrases,\footnote{In initial experiments, we observed that \model\ performs competitively with the much larger and more powerful GPT-3.5 davinci-003 model in terms of paraphrase quality, and significantly better at controlling diversity. This finding shows that specialized smaller models can outperform LLMs in paraphrasing tasks.} they may be detectable by strategies like watermarking (whether using the same API as the original LLM or a different one). We thus expect a sophisticated adversary to use their own private paraphraser (like \model) to evade detection.
