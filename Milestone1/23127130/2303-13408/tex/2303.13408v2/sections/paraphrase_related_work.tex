
\section{Related work for discourse paraphrasing}


\subsection{Survey of paraphrase generation papers}
\label{sec:appendix:papersurvey}

As an important NLP task, paraphrasing has attracted much attention. Many models have been proposed to improve the quality of paraphrases. To position our model~\model~and highlight its strengths, we conduct a survey of paraphrase generation papers from 2018 to 2022 (\tableref{tab:prior-eval-metric}) and focus on the following four aspects:

\begin{enumerate}[leftmargin=*] %label=(\arabic*), 
    \item Whether a model can paraphrase a paragraph at once,
    
    \item whether a model can merge or split consecutive sentences when appropriate,
    
    \item whether a model leverages context surrounding an input sentence when paraphrasing,
    
    \item whether a model provides control knobs for users to customize the output diversity.
\end{enumerate}

The survey shows that only three out of 25 papers mentioned that their model can paraphrase more than one sentence (but not necessarily at once). None of them enables their model to merge or split sentences when paraphrasing. No model uses information from context surrounding an input sentence during inference time. Finally, 14 papers offer ways for users to customize the diversity of paraphrases. However, most diversity control methods such as constituency parses or exemplars may not be straightforward and intuitive to end-users as the scalar control knobs in \model. 

In contrast to the papers in the survey, \model~nicely combines all desiderata into one model and offers intuitive control knobs for lexical and syntactic diversity. Automatic and human evaluation show that \model~can efficiently leverage context information and reorganize sentences while having high fidelity in meaning (\appendixref{sec:intrinsic-evaluation}). 

\begin{table}
\caption{The table shows the result of our survey of paraphrase generation papers from 2018 to 2022. We focus on four aspects: (1) whether a model can paraphrase multiple sentences at once, (2) whether a model is able to merge or split an input sentence when appropriate, (3) whether a model takes context surrounding the input sentence into consideration when paraphrasing, and (4) whether a model enables users to control the semantic and syntactic diversity of paraphrases. $^1$Granularity levels are \textit{word}, \textit{phrase}, and \textit{sentence}. $^2$\citet{meng-etal-2021-conrpg} use context for their dataset construction, but do not leverage it during training/inference. $^3$The diversity score is a combination of the unigram Jaccard distance and the relative position change for unigrams. $^4$The code is represented by a three dimensional vector corresponding to semantic
similarity as well as syntactic and lexical distances between the input and output sentences.}
\label{tab:prior-eval-metric}
\vspace{\baselineskip}

\centering
\small
\begin{tabular}{@{}lcccc@{}} 
 \toprule
Paper & Multi-sentence & Merge / Splits & Contextual & Diversity Control \\ 
\midrule
\citet{iyyer-etal-2018-adversarial} & \xmark & \xmark & \xmark & \scalebox{0.95}{Constituency parse} \\
\citet{li-etal-2018-paraphrase} & \xmark & \xmark & \xmark & \xmark \\
\citet{roy-grangier-2019-unsupervised} & \xmark & \xmark & \xmark & \xmark \\
\citet{witteveen-andrews-2019-paraphrasing} & \textcolor{ForestGreen}{\cmark} & \textbf{?} & \xmark  & \xmark \\
\citet{kumar-etal-2019-submodular} & \xmark & \xmark & \xmark & \xmark \\
\citet{hu2019parabank} & \xmark & \xmark & \xmark & \scalebox{0.95}{Decoding constraints}\\
\citet{chen-etal-2019-controllable} & \xmark & \xmark & \xmark & \scalebox{0.95}{Exemplar}\\
\citet{li-etal-2019-decomposable} & \xmark & \xmark & \xmark & \scalebox{0.95}{Granularity control}$^1$\\
\citet{goyal2020neural} & \xmark & \xmark & \xmark & \scalebox{0.95}{Exemplar} \\
\citet{lewis2020pre} & \textcolor{ForestGreen}{\cmark} & \textbf{?} & \xmark & \xmark \\
\citet{thompson-post-2020-paraphrase} &  \xmark &  \xmark & \xmark & \scalebox{0.95}{$n$-gram overlap} \\
\citet{kumar2020syntax} &  \xmark &\xmark & \xmark & \scalebox{0.95}{Exemplar}\\
\citet{kazemnejad-etal-2020-paraphrase} & \xmark & \textbf{?} & \xmark & \xmark \\
\citet{style20} & \xmark & \xmark & \xmark & \xmark \\
\citet{rajauria2020paraphrase} & \xmark & \xmark & \xmark & \xmark \\
\citet{meng-etal-2021-conrpg} & \xmark & \xmark & { } \xmark$^2$ & \scalebox{0.95}{Diversity score}$^3$\\
\citet{huang-chang-2021-generating} & \xmark & \xmark & \xmark & \scalebox{0.95}{Constituency parse}\\
\citet{lin-etal-2021-towards-document-level} & \textcolor{ForestGreen}{\cmark} & \xmark & \xmark & \xmark \\
\citet{goutham2021paraphrase} & \xmark & \xmark & \xmark & \xmark \\
\citet{prithivida2021parrot} & \xmark & \xmark & \xmark & \scalebox{0.95}{Binary} \\
\citet{dopierre-etal-2021-protaugment} & \xmark & \xmark & \xmark & \scalebox{0.95}{$n$-gram} \\
\citet{bandel-etal-2022-quality} & \xmark & \xmark & \xmark & Control code$^4$\\
\citet{hosking-etal-2022-hierarchical} & \xmark & \xmark & \xmark &  Syntactic sketch\\
\citet{yang2022gcpg} & \xmark & \xmark & \xmark & Examplar$+$Keywords\\
\citet{xie-etal-2022-multi} & \xmark & \xmark & \xmark & \xmark \\
\midrule
\scalebox{1.1}{\textbf{\model~(ours)}} & \scalebox{1.15}{\textcolor{ForestGreen}{\cmark}} & \scalebox{1.11}{\textcolor{ForestGreen}{\cmark}} & \scalebox{1.11}{\textcolor{ForestGreen}{\cmark}} & \scalebox{1.11}{\textcolor{ForestGreen}{\cmark}} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Other related work}

In this section we discuss a few additional less related papers which were not included in our survey in \appendixref{sec:appendix:papersurvey}. Our discourse paraphraser is closely related to work on contextual machine translation, where source/target context is used to improve sentence-level machine translation~\citep{house2006text,jean2017does,wang-etal-2017-exploiting-cross,tiedemann-scherrer-2017-neural,kuang-etal-2018-modeling,agrawal2018contextual,miculicich-etal-2018-document,zhang-etal-2018-improving,xiong2019modeling,jean2019fill,voita-etal-2019-context,yin2021does,mansimov-etal-2021-capturing}. Prior work has shown that context helps with anaphora resolution~\citep{voita-etal-2018-context}, deixis, ellipsis, and lexical cohesion~\citep{voita-etal-2019-good}. Efforts to make paraphrase generation more contextual have been quite limited. A few efforts have attempted to use sentence level context to paraphrase phrases~\citep{connor2007context,max2009sub}, and dialogue context to paraphrase individual dialogues in a chat~\citep{garg2021unsupervised}.

Our work is also related to efforts in text simplification to go beyond a sentence, by collecting relevant datasets~\citep{xu2015problems,devaraj-etal-2021-paragraph} and building unsupervised algorithms~\citep{laban-etal-2021-keep}. Note that our work focuses on a general-purpose paraphrasing algorithm and is not tied to any particular style, but could be utilized for document-level style transfer using techniques like~\citet{style20,krishna2022few}. Similar efforts have also been undertaken in machine translation,~\citep{emnlp-2019-discourse,junczys-dowmunt-2019-microsoft,maruf2021survey}, attempting to translate paragraphs/documents at once.
