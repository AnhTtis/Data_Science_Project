\section{Introduction}

Large language models (LLMs) such as ChatGPT~\citep{schulman2022chatgpt} exhibit an unprecedented ability to write coherent and relevant long-form text in response to user-specified prompts. These abilities have sparked fears of malicious applications such as automatically generating fake news articles or  homework answers~\citep{stokel2022ai}. To defend against these use cases, several algorithms have recently been proposed to detect AI-generated text, including watermarking~\citep{kirchenbauer2023watermark}, GPTZero~\citep{GPTZero}, DetectGPT~\citep{mitchell2023detectgpt}, and OpenAI's text classifier~\citep{AITextClassifier}. However, it remains unclear how robust these algorithms are to \emph{paraphrase attacks}, in which AI-generated text from an LLM is rewritten by another (smaller) model to convey approximately\footnote{We use the \emph{quasi-paraphrase} definition of semantic equivalence~\citep{bhagat-hovy-2013-squibs} in this paper.} the same meaning but using different word choice and syntax. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/watermark-evade-alternate}
    \vspace{-0.1in}
    \caption{An overview of paraphrasing attacks with \model\ on watermarked text~\citep{kirchenbauer2023watermark}. The original model generation (top) contains several ``green'' watermarked tokens that are counted by a detector to judge whether the text was AI-generated. After paraphrasing, several green tokens are replaced with approximately semantically-equivalent red tokens, thereby fooling the detector. Actual outputs from a watermarked version of GPT2-XL and our paraphraser \model.}
    \label{fig:watermark-evade}
    \vspace{-0.1in}
\end{figure*}

In this paper, we first demonstrate the vulnerability of these existing detectors to paraphrase attacks (\sectionref{sec:model},~\ref{sec:attacks}). Such attacks require an \emph{external} paraphraser model, since paraphrases generated by the base LLM are still susceptible to detection techniques such as watermarking. We train an 11B parameter paraphrase generation model called \model\ (or \textbf{Di}scourse \textbf{P}ara\textbf{p}hras\textbf{er}) to execute these attacks. \model\ possesses two unique features that help its outputs evade AI-generated text detectors: (1) \textbf{Paraphrasing long-form text in context:} Most modern paraphrasers are exclusively trained on sentence-level data, ignoring discourse-level information. However, many critical use cases of LLMs involve generating long-form text as responses to detailed user-specified prompts. Thus, we train \model\ to paraphrase paragraph-length texts, re-order content, and optionally leverage context such as user prompts; (2) \textbf{Controlling output diversity:} Another weakness of existing paraphrasers is that they lack an easy way to control output diversity. An attacker may want to apply just the minimum amount of paraphrasing to evade a detector. \model~provides users with two intuitive scalar control knobs at inference time (lexical diversity, content reordering) that are trained end-to-end.

%: one controls the lexical diversity of the paraphrase, and the other controls the amount of content re-ordering.
%\kkcomment{use Fig 1 for reference here.. can be 2 column figure if we want to show it attacking watermarks along with new paraphrase stuff}
% \vspace{-3pt}


We use \model\ to attack several recently proposed AI-generated text detectors (see \figureref{fig:watermark-evade} for an attack overview). Experiments on multiple tasks and LLMs (including GPT3.5-davinci-003) show that after paraphrasing with \model, a substantial fraction of AI-generated texts are misclassified as human-written texts by all detectors. For example, DetectGPT~\citep{mitchell2023detectgpt} correctly detects 70.3\% of AI-generated sequences from GPT2-XL, but after paraphrasing, its detection rate drops to only 4.6\%\footnote{These detection rates were computed at a constant false positive rate (FPR) of 1\%. Due to the importance of low FPR in this task, we recommend using a fixed low FPR rather than AUC-ROC values; see \sectionref{sec:eval-metrics-attacks}.} despite minimal semantic modification. We confirm the validity of \model's paraphrases through several automatic evaluations and a human evaluation of semantic similarity. 

Given the vulnerability of AI-generated text detectors to paraphrasing, how can we defend against such attacks? In the second part of our paper (\sectionref{sec:defenses}), we propose to use \emph{retrieval} methods to detect AI-generated text instead of relying on statistical properties of the text or watermarking. First, an LLM API provider stores every output generated by their model in a database. The API provider then offers a service in which a semantic representation of a candidate text is compared to representations of every generation stored in the database. The search focuses on the \emph{semantics} of the input and can leverage both standard IR methods such as BM-25~\citep{robertson1995okapi} as well as  semantic vector representations such as \spavg from~\citet{wieting-etal-2022-paraphrastic}. Since paraphrasing does not modify the semantics of the input, this algorithm is robust to paraphrasing attacks. Specifically, we find that 97.3\% of PG19 paraphrases and 80.4\% of Wikipedia paraphrases are successfully detected in a large database of over 15M generations, at a 1.0\% false positive rate. We extensively discuss the limitations and scalability of retrieval-based detection in \sectionref{sec:main-retrival-limitations}.

In contrast to concurrent work that also uses paraphrasing to attack AI-generated text detectors~\citep{sadasivan2023aigenerated}, our work offers more comprehensive attack experiments, a new and more powerful paraphraser, human evaluations of paraphrase quality, and finally a novel defense mechanism based on retrieval to combat such attacks.  To spur future research in this area, we will release our \model\ model, data, and a codebase for evaluating both existing detectors and our retrieval-based method.




%To train \model, we leverage the \parthree~dataset~\citep{thai2022booktranslate}, which contains paragraph-level alignments of multiple English translations for foreign novels. Unlike existing open-source paraphrasers, \model\

%\kkcomment{Shufan: Why operations like splits / merges are useful? Downstream applications? Marzena: text simplification}

%\emph{``Language does not occur in stray words or sentences, but in connected discourseâ€”from a one-word utterance to a ten-volume work, from a monolog to a Union Square argument''}  --- \citet{harris1952discourse}

%Besides its direct utility in writing assistance applications like Quillbot~\citep{fitria2021quillbot}, paraphrasing is an important core natural language processing task with several downstream applications. It has been used for data augmentation~\citep{xie2019unsupervised}, style transfer~\citep{style20}, text generation evaluation~\citep{sellam-etal-2020-bleurt,thompson-post-2020-automatic}, semantic parsing~\citep{berant-liang-2014-semantic}, improving machine translation~\citep{sennrich-etal-2016-improving} and building sentence embeddings~\citep{wieting-gimpel-2018-paranmt}. Most recently,~\citet{dhole2021nl} built NL-Augmenter, a large library of paraphrase generation algorithms to study model robustness against systematic perturbations of text.

%We construct our paraphrase training data using this dataset in four simple steps --- \noindent (1) Align sentences of the paragraphs; (2) Shuffle sentences in the input paragraph to encourage content re-ordering; (3) Compute scalar lexical overlap and reordering amounts which are provided as input to model; (4) Add context text from the translated novel on both sides of the input.

%Finally, we provide an evaluation set of 250 human-written paraphrases at a paragraph level, collected on Upwork. Freelancers were instructed to re-order the sentences in the paragraph while paraphrasing, making sentence splits / merges as appropriate. \kkcomment{add 1-2 lines about how data is used, say Freelancers are writing}

%    Several machine translation papers have shown the benefits of using context, especially with anaphora resolution and lexical cohesion~\citep{voita-etal-2018-context,voita-etal-2019-context}. However, none of the surveyed paraphrase generation papers leverage context during inference.

%    Half of the surveyed systems provided some kind of output control, either via exemplar sentences or beam search heuristics. However, these controls are confusing to an end-user unfamiliar with NLP, compared to intuitive scalar control provided by commerical systems like QuillBot.~\footnote{\url{https://quillbot.com/}} \kkcomment{see couple of ACL papers on this}

%In this work we train a 11B parameter sequence-to-sequence model to paraphrase paragraphs. Text surrounding the input can optionally be provided as additional context to our system to help paraphrasing. During inference, we provide two intuitive scalar control knobs to users which are learnt end-to-end during training --- (1) controlling the lexical diversity of the paraphrase; (2) controlling the amount of content re-ordering.


%Only 3 / 21 systems claim to paraphrase paragraphs. Two systems are not open source, while the third~\citep{lin-etal-2021-towards-document-level} simply re-arranges sentences in fixed order without modeling sentence merges / splits / coreference.