\section{Background on detectors of AI-generated text}
\label{sec:detection-background-shorten}
\label{sec:sadasivan-shorten}

In this section, we provide a brief overview of existing algorithms for detecting AI-generated text detection (see \appendixref{sec:detection-background} for a detailed version). We also contrast our work to~\citet{sadasivan2023aigenerated}, a concurrent effort which notes the efficacy of paraphrasing attacks but does not consider a retrieval-based defense in its pessimistic conclusion about the fate of AI-generated text detection.

%\footnote{A comprehensive overview can be found in \appendixref{sec:detection-background}.}

%Given the recent success of large language models (LLMs) like ChatGPT~\citep{schulman2022chatgpt} in producing long-form human-like text, there's an increasing fear that LLMs will be used for malicious purposes like fake news generation and cheating in academic writing assignments. This has led to the development of several mitigation techniques, which detect if a long-form text was human or machine generated. Methods to detect machine-generated text roughly fall under two categories: watermarking \& outlier detection.\\

% \subsection{Watermarking language model outputs}
% \label{sec:watermark}

%\textbf{A watermarking algorithm} \textit{watermark}s model-generated outputs by modifying the text in a way that can be detected by a statistical algorithm but being imperceptible to human readers. Effective watermarks should have little effect on the quality of generated text while being difficult to remove. Prior work attempted to watermark natural language using syntax tree manipulations~\citep{topkara2005natural, meral2009natural}. Most recently,~\citet{kirchenbauer2023watermark} propose a simple algorithm that only requires access to the LLM's logits at each time step to add watermarks. The watermark can then be verified with only blackbox access to the LM and knowledge of a specific hash function. 

A \textbf{watermark} is a modification to the generated text that can be detected post-hoc by an algorithm while remaining imperceptible to human readers. Effective watermarks are difficult to remove and have little effect on the quality of generated text. Prior work has watermarked natural language using syntax tree manipulations~\citep{topkara2005natural, meral2009natural}, and this area has received renewed interest with the advent of LLMs~\citep{abdelnabi2021adversarial, grinbaum2022ethical}. Most recently,~\citet{kirchenbauer2023watermark} proposed a simple algorithm that watermarks LLMs by slightly perturbing its probability distribution while generating text. 
% This watermark can be detected without white-box access to the language model.

% \subsection{Statistical outlier detection methods}
% \label{sec:outlier-detection}

 \textbf{Statistical outlier detection methods} detect AI-generated text based on its artifacts~\citep{see-etal-2019-massively, holtzman2020curious} instead of modifying the generative algorithm. Early methods detect statistical irregularities in entropy~\citep{lavergne2008detecting} and perplexity~\citep{beresneva2016computer}, while \citet{gehrmann-etal-2019-gltr} introduced the GLTR visualizer to assist humans in detecting AI-generated text. The release of ChatGPT prompted the development of two new tools: closed-source GPTZero~\citep{GPTZero} and open-source DetectGPT~\citep{mitchell2023detectgpt}. The latter uses the observation that AI-generated text tends to have  significantly higher LLM likelihood than meaningful perturbations of it.
 
 %Thus, it constructs multiple perturbations of the model generated text using a mask-and-fill strategy, and compares the log probability of the perturbations with the unperturbed generation.

\textbf{Classifier methods} train models to distinguish human-written text from AI-generated text. Early efforts used classifiers to detect fake reviews~\citep{hovy-2016-enemy} and news~\citep{zellers2019defending}, while others examined classifier performance across domains~\citep{bakhtin2019real} and decoding strategies~\citep{ ippolito-etal-2020-automatic}. Most recently, OpenAI fine-tuned a GPT model to perform this task and released it as a web interface~\citep{AITextClassifier}. Their model uses generations from 34 LLMs, with the human-written text from Wikipedia, WebText, and their internal human demonstration data.

\noindent \textbf{Comparison to Sadasivan et al. (2023)}: In recent concurrent work, ~\citet{sadasivan2023aigenerated} also demonstrate the utility of paraphrasing attacks against AI-generated text detectors. 
% While they use off-the-shelf sentence-level paraphrasers, our \model\ model possesses advanced discourse-level rewriting capabilities as well as fine-grained diversity control, which is much more suited to long-form text generated by LLMs.
Our experiments encompass more tasks, detection algorithms, and larger LMs like GPT3.5. Additionally, we propose a discourse-level paraphrase model (\model) that is much more suited to long-form text than the off-the-shelf sentence-level paraphrasers used in their paper. More importantly, our retrieval-based defense \emph{directly contradicts} the ``impossibility result'' of~\citet{sadasivan2023aigenerated} and its associated proof, which states that an optimal detector will perform at random as the quality of LLM-generated text approaches that of human-written text. The quality of generated text is irrelevant to our detector's accuracy because it relies only on a corpus search, and thus the proof is inapplicable. Other concurrent work~\citep{chakraborty2023possibilities} has also shown the proof's invalidity in practical settings.