\section{Building a controllable discourse paraphraser}
\label{sec:model}
\label{sec:paraphrase-data-build}

Having outlined existing methods to detect AI-generated text, we now focus on a simple attack against all detection techniques: \emph{paraphrasing} the generated text. Intuitively, paraphrasing alters the statistical properties of AI-generated text, which can fool outlier detection or classifiers while also reducing the number of watermarked tokens (\figureref{fig:watermark-evade}). To evade such detectors, a paraphraser must be able to handle \emph{context} in the form of prompts or multi-sentence inputs. Its behavior should also be \emph{controllable} in order to make as many/few changes as needed to evade a given detector. In all cases, it should not appreciably change the input semantics. Finally, to evade watermarking, the paraphraser must be different from the watermarked model, as otherwise the paraphrases will also be watermarked. Below, we detail how we construct a paraphraser (\model) with all these properties.\footnote{To better ground \model's abilities in prior work, we survey existing paraphrase models in \appendixref{sec:appendix:papersurvey}.}

%\subsection{Constructing paraphrase data}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/dipper-idea}
    \caption{The method used to train \model\ on English translations of the French novel \emph{The Nun}. We first align sentences between the two translations to create parallel data. Next, a subset of the alignments are chosen; in this example, we use ($p_2$, $q_2$) and ($p_3$, $q_3q_4$). We shuffle sentences, compute control codes, and fine-tune a T5-XXL LM to generate $p_2p_3$ given $q_3q_4q_2$ and the context $p_1$ and $p_4$.}
    \label{fig:dipper-training}
    \vspace{-0.15in}
\end{figure*}

%that come from different English translations of the same non-English novel
% (e.g., it contains both the Henry Morley and Robert Adams translations of Voltaire's \emph{Candide})

\noindent \textbf{Constructing paraphrase data}: Our process involves fine-tuning a LLM on a parallel dataset of paragraph-level paraphrases, which we modify to model control, external context and content reordering.
We leverage the \booktranslate~dataset~\citep{thai2022booktranslate}, which contains multiple translations of non-English novels into English aligned at a paragraph level, which we treat as paraphrases. More formally, let $p$ and $q$ be aligned paragraphs, where $p_1, p_2 ... p_N$ denote sentences of $p$ and $q_1, q_2, ... q_M$ denote sentences of $q$. Note that $M$ may not be equal $N$ when two translators disagree on when to merge and split sentences. We perform the following steps (overview in \figureref{fig:dipper-training}):
\vspace{-0.1in}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Align sentences} of $p$ to sentences of $q$ by using the semantic similarity scores from the paraphrase similarity metric in ~\citet{wieting-etal-2019-beyond} to run the sequence alignment algorithm designed by \citet{needleman1970general} which uses dynamic programming (metric details in \sectionref{sec:eval-metrics-attacks}).
    \item \textbf{Choose a subset of sentences} $p_i ... p_j$ from the first paragraph. Let $q_{i'} ... q_{j'}$ be the corresponding alignment in the second paragraph. In \figureref{fig:dipper-training}, $i=2, j=3, i'=2, j'=4$.

    \item \textbf{Re-order} the sentences $q_{i'} ... q_{j'}$ randomly, and compute the \textbf{diversity control codes} between $p_i ... p_j$ and $\text{shuffle}(q_{i'} ... q_{j'})$. We shuffle the sentences to allow for the model to learn content re-ordering. We compute lexical diversity ($L$) using unigram token overlap (F1 score), and the order diversity ($O$) using the Kendall-Tau correlation of tokens of overlapping words between $p_i ... p_j$ and $\text{shuffle}(q_{i'} ... q_{j'})$, also used in \citet{style20}. These scores are normalized to values $\{0, 20, 40, 60, 80, 100\}$, where $L=20$ roughly corresponds to a 20\% lexical modification.

    \item \textbf{Map} the shuffled $q_{i'} ... q_{j'}$ to $p_i ... p_j$, leveraging context from the rest of $p$ and control codes using string concatenation. Let $\text{input} = \text{shuffle}(q_{i'} ... q_{j'})$. We map,
\begin{align*}
\text{lexical} = L, \text{order} = O &\oplus p_1 ... p_{i-1} \oplus \texttt{<p>}~\text{input}~\texttt{</p>} \oplus p_{j+1} ... p_N \longrightarrow p_i ... p_j
\end{align*}
\noindent where $\oplus$ is string concatenation. During inference, we can paraphrase any sequence of sentences by marking it with \texttt{<p>} tags, assigning the control codes ($L$, $O$) the desired diversity values.

%\footnote{To make our paper more intuitive, we have slightly modified the notation that our actual pretrained model uses. Our pretrained model uses control codes $100-L$ and $100-O$, denoting lexical/order \emph{similarity} rather than diversity. Also, \texttt{<sent>} is used instead of \texttt{<p>}. We will clearly document this in the code release.}
\end{enumerate}

Our final dataset contains 6.3M paraphrase pairs. We \textbf{fine-tune} a sequence-to-sequence Transformer~\citep{vaswani2017attention} on this data, initialized with the pretrained 11B parameter T5-XXL checkpoint~\citep{raffel2020exploring}. See \appendixref{appendix:dipper-training} for details.



% \subsection{Leveraging \model\ for paraphrase attacks on AI-detection}
% \label{sec:attack-overview}

% \kkcomment{todo}
