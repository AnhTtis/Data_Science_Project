\section{Conclusion}

We present \model, a controllable paraphraser that can rewrite paragraphs in context. We use \model\ to stress test current AI-generated text detectors, and we find that \model\ paraphrases easily evade these detectors while preserving input semantics. As a defense, we propose a simple retrieval-based detector which searches through a corpus of previously-generated sequences from an LLM API for semantically-similar generations to a given query. We show that this defense significantly outperforms baselines on paraphrased text, and scales effectively. We discuss the limitations and ethical considerations of our work in \appendixref{sec:limitations}, \ref{appendix:ethical}. We have additionally open sourced our models, code and data to enable future research.\footnotemark[1] Since this paper's initial release, \model\ has been extensively utilized in follow-up studies to measure the robustness of AI-generated text detection algorithms (\citep{lu2023large,koike2023outfox,zhao2023provable,yoo2023advancing,patil2023can,kirchenbauer2023reliability,kumarage2023reliable,liu2023semantic}, to name a few).


%\kkcomment{other expt ideas we won't do for first version of paper: stress test defense with noise functions, control code evals vs LLM, LFQA by domain (evaluate both paraphraser and the detection attack and defense), variation with length of attack success / paraphrase performance, simpler paraphrasers, zero-shot LLMs, generate model A test on model B, baseline comparisons to other papers will be needed.. maybe we can do sentence-level comparisons with other sentence-level paraphrasers? backtranslation from NL-augmenter?~\citep{dhole2021nl}, https://arxiv.org/pdf/1911.09661.pdf, https://arxiv.org/abs/2109.07095.. We could run it across different control codes to show the tradeoffs, fine-grained evaluation on phenomenon using context, see~\citep{jean2017does} for ideas from MT like pronoun correctness, longer sentences / complex phenomenon in the sentence (paranmt vs \booktranslate), try to show why backtranslation / paranmt is not enough compared to this dataset, Kenton feedback: . Other comments: - This paper should be flooded with examples - how the retrieval model is scoring text pairs, and how other baselines interact with the retrieval model.}