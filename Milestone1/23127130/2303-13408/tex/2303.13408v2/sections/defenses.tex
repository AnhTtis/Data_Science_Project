

\section{Defense against paraphrase attacks using retrieval}
\label{sec:defenses}
\label{sec:defense-results}

In \sectionref{sec:attack-expts}, we showed that paraphrasing is an effective attack against AI-generated text detectors.  How can LLM API providers defend against these attacks? In this section, we propose \emph{retrieval} over previously-generated sequences as a defense against paraphrase attacks. At a high level (\figureref{fig:defense-idea}), an API provider first stores every sequence generated by their LLM in a database. The API provider offers an interface that allows users to enter candidate AI-generated text as a query. The interface searches over the entire database of previously-generated text, trying to find a sequence that approximately matches the content of the input query. This search can be done using a semantic similarity scorer like P-SP~\citep{wieting-etal-2022-paraphrastic} or a retriever like BM25~\citep{robertson1995okapi}. Since paraphrasing approximately preserves input semantics, we expect such a defense to still be able to map paraphrased generations to their source. We formalize our detector in \sectionref{sec:retrieval-formulation}, and then conduct a controlled comparison with competing detectors in \sectionref{sec:control-defense-results}. We evaluate retrieval-based detection at scale using a large retrieval corpus of 15M generations in  \sectionref{sec:scale-defense-results}. In \appendixref{sec:limitations} we extensively discuss limitations of retrieval-based detection and share ideas for enabling further scaling.


\subsection{Formulating the retrieval defense}
\label{sec:retrieval-formulation}
\label{sec:retriever-choice}

Let $f_\text{LM}$ be an LLM API (e.g., GPT-3.5) that takes a prompt $x$ as input and returns a continuation $y$. Let $f_\text{ret}$ be an encoder (e.g., TF-IDF, neural network) that embeds variable-length sequences into fixed-size vectors that represent the input semantics. Then, we do the following:

\textbf{Building the database}: Let $x_1,..., x_N$ be the set of prompts that have been fed as input to the API in the past with $y_i = f_\text{LM}(x_i)$ being the LLM output. Here $N$ can potentially be very large for popular APIs (we study up to $N=$ 15M). We construct our database $\mathbf{Y} = [\mathbf{y}_1, ... \mathbf{y}_N]$ by encoding every LLM API output with our retrieval encoder, or $\mathbf{y}_i = f_{\text{ret}}(y_i)$. The database $\mathbf{Y}$ is dynamically updated and stored on the API side. It is inaccessible to clients except via the API described in the next step.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/defense-idea}
    \caption{An illustration of AI-generated text detection with retrieval. Several users (including the attacker, shown as the purple emoji) feed prompts to the API which are collectively added to a private API-side database. Candidate queries are compared against this database using a retriever like BM25. }
    \vspace{-0.15in}
    \label{fig:defense-idea}
\end{figure*}

\textbf{Querying the database}: Let $y'$ be a candidate text and $\mathbf{y}' = f_{\text{ret}}(y')$ its encoded vector. Suppose a client wishes to know whether $y'$ was generated by the API $f_\text{LM}$. The API provider can check this by seeing whether the maximum similarity score of $y'$ to an entry in the database exceeds some detection threshold $T$ chosen by the API provider:
\begin{align*}
\text{output} &= \text{score} > T, \text{ where } \text{score} = \max_{i \in \{1,..N\}} \frac{\mathbf{y}' \cdot \mathbf{y}_i}{|\mathbf{y}'|~|\mathbf{y}_i|}
\end{align*}
We expect unperturbed machine-generated text to always get a score of 1.0, while paraphrasing the text may lower the detection score. Hence, lowering $T$ will increase the detection rate of heavily-paraphrased text but also increase the false positive rate (i.e., human-written text that resembles sequences previously generated by the LLM API can be falsely flagged). Since $N$ can be very large, the score can also be approximated using efficient nearest neighbor libraries like FAISS~\citep{johnson2019billion}. However, in this work we only compute exact inner products.

As the \textbf{retriever $f_\text{ret}$}, we experiment with two choices: \spavg~\citep{wieting-etal-2022-paraphrastic} and BM25~\citep{robertson1995okapi}. We implement BM25 using the \texttt{retriv} library from~\citet{retriv2022}. In order to normalize and calibrate BM25 scores, we compute the F1-score unigram token overlap~\citep{rajpurkar-etal-2016-squad} between the candidate $y'$ and the best retrieval $y*$ to get a detection score in $[0, 1]$.


\begin{table}[t!]
\caption{A comparison of retrieval against other detectors on long-form QA (300 generated tokens). Our detector outperforms baselines (at 1\% FPR) even with the most diverse paraphrases (+60L,O).}
\label{tab:watermark-defense}
\vspace{\baselineskip}

\footnotesize
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lrrrrrrrrr@{}} 
  & \multicolumn{3}{c}{GPT2-XL} & \multicolumn{3}{c}{OPT-13B} & \multicolumn{3}{c}{GPT-3.5 (davinci-003)}\\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} 
& Original & + 60L & + 60 L,O & Original & + 60L &  + 60 L,O & Original & + 60L & + 60 L,O \\
\midrule
Watermark~\citeyearpar{kirchenbauer2023watermark} & 100.0 & 71.1 & 55.8 & 100.0 & 65.5 & 51.4 & - & - & -\\
DetectGPT~\citeyearpar{mitchell2023detectgpt} & 74.9 & 15.8 & 7.6 & 29.8 & 3.2 & 1.5 & 1.0 & 0.0 & 0.0 \\
OpenAI~\citeyearpar{AITextClassifier} & 59.2 & 31.3 & 32.7 & 33.5 & 21.6 & 21.6 & 40.5 & 40.1 & 38.1 \\
\midrule
\multicolumn{10}{l}{\emph{(Ours)} Retrieval over corpus of 3K generations from model itself, with retriever:} \vspace{0.15cm} \\
~~~SP & 100.0 & 95.6 & 87.7 & 100.0 & 94.8 & 85.3 & 100.0 & 94.2 & 85.1 \\
~~~BM25 & 100.0 & 99.2 & 97.8 & 100.0 & 99.3 & 97.3 & 100.0 & 98.6 & 96.2 \\
\midrule
\multicolumn{10}{l}{\emph{(Ours)} Retrieval over corpus of 9K generations pooled from all three models, with retriever:} \vspace{0.15cm} \\
~~~SP & 100.0 & 88.9 & 75.4 & 100.0 & 89.6 & 76.4 & 100.0 & 93.8 & 84.6\\
~~~BM25 & 100.0 & 98.3  & 95.2  & 100.0 & 98.5 & 94.4 & 100.0 & 98.5 & 96.0\\
\midrule
\multicolumn{10}{l}{\emph{(Ours)} Retrieval over 43K ShareGPT responses + corpus of 3K generations from model itself, with retriever:} \vspace{0.15cm} \\
~~~SP & 100.0 & 94.0 & 84.8 & 100.0 & 94.2 & 84.7 & 100.0 & 94.1 & 84.9 \\
~~~BM25 & 100.0 & 98.9  & 97.5 & 100.0 & 99.0 & 97.3 & 100.0 & 98.4 & 95.5 \\
\bottomrule
\end{tabular}%
}
\vspace{-0.1in}
\end{table}



\subsection{Controlled comparisons of retrieval with other AI-generated text detectors}
\label{sec:control-defense-results}


First, we conduct a controlled comparison between the detection algorithms evaluated in \sectionref{sec:attack-expts} and our retrieval method on long-form question answering.\footnote{Corresponding results in open-ended text generation on Wikipedia prompts are provided in \appendixref{sec:appendix:controlled_comparison}.} We construct three kinds of databases,

\begin{enumerate}
    \item 3K sequences generated by a specific LM for one of the tasks;
    \item  9K sequences formed by concatenating the generations from all three LMs in this paper;
    \item 46K sequences constructed by combining the 3K sequences from (1) with 43K LLM responses from ShareGPT-Vicuna.\footnote{\url{huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered}}
\end{enumerate}

We expect (2) to be a more difficult test for our method than (1), since the retriever needs to distinguish between multiple generations from different models given the same prompt. On the other hand, (3) denotes a more real-world setting with several diverse LLM-generated outputs from ShareGPT. Next, we perform retrieval over this corpus using different types of queries: the original AI-generated text, its \model\ paraphrase, and human-written text (each query with at least 50 tokens).


\tableref{tab:watermark-defense} shows that \textbf{across all LMs, retrieval is a much more effective detector than baseline detectors}. On unperturbed AI-generated text, retrieval has a 100\% detection accuracy due to exact match with the retrieval corpus. On paraphrased text, retrieval with BM25 is quite effective, detecting 97.8\% of the highest-diversity paraphrases (L60, O60) on GPT2-XL, 97.3\% on OPT-13B and 96.2\% on GPT-3.5 in long-form question answering. This is significantly better than the next best alternative with competing detectors (55.8\%, 51.4\%, 38.1\%). Even on our harder augmented databases, detection rates continue to be high: 95.2\%, 94.4\%, 96.0\% for the 9K augmented database; 97.5\%, 97.3\%, 95.5\% for the ShareGPT augmented database. Finally, we observe that BM25 is a more effective retriever than \spavg, scoring 95.2\% vs 75.4\% on the augmented setting in GPT2-XL. These trends are consistent across different FPR thresholds, as shown in \figureref{fig:auc-plots}.

In \appendixref{appendix:mixing-attacks}, we additionally observe promising preliminary results that show the effectiveness of retrieval against \textbf{text mixing attacks}~\citep{kirchenbauer2023reliability}.

\subsection{Is retrieval an effective detector with a large retrieval corpus?}
\label{sec:scale-defense-results}
\label{sec:detect-length-effect}
In the previous section, we conducted experiments using the set of 9K sequences generated by all three models as the retrieval corpus. However, this is more of a toy experiment: in practice, a popular LLM API may serve millions of queries a day. As the corpus grows larger, the  false positive rate (i.e., human-written text falsely detected as AI-generated) will grow. How well do retrieval-based detectors scale?  To answer this question, we need access to a large corpus of AI-generated text. We utilize the training data used to train RankGen~\citep{krishna-etal-2022-rankgen}, which contains over 70M AI-generated sequences. We use the Project Gutenberg and Wikipedia splits of the training data, each of which contain 15M sequences generated by a T5-XXL model~\citep{raffel2020exploring} fine-tuned on the different documents in the same domain. We discard generations which are shorter than 50 tokens, and paraphrase a subset of 2K generations to evaluate retrieval.

\textbf{Retrieval is effective even with a corpus size of 15M generations.} In \figureref{fig:semantic-search}, we plot the detection accuracy as a function of retrieval database size. Overall, we observe that detection accuracy remains consistently high across different corpus sizes (varying from 1M generations to 15M generations). We observe slight drops in performance as the corpus size increases: just 1\% (98.3 to 97.3) on Project Gutenberg (PG19) and 9.6\% (90.0 to 80.4) on Wikipedia. Consistent with the results in \sectionref{sec:control-defense-results}, BM25 continues to outperform \spavg\ across different  corpus sizes.


\begin{figure}
     \centering

     \begin{subfigure}[t]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/scale_retrieval}
         \caption{Variation in retrieval-based detection with retrieval corpus size. Note consistently high detection rates for paraphrases, which only slightly degrades as the corpus is scaled to 15M generations.}
         \label{fig:semantic-search}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/scale_retrieval_length}
         \caption{Variation in retrieval-based detection with different query lengths. Overall, retrieval performs best with queries of length 50+ tokens.}
         \label{fig:retrieval-length}
     \end{subfigure}
    \caption{Detection rate using retrieval at 1\% FPR w.r.t. corpus size (left) and query length (right).}
    \vspace{-0.15in}
    \label{fig:semantic-search+retrieval-length}
\end{figure}
\textbf{Retrieval detection works best with 50 or more tokens of generated text}. Another important factor for our retrieval-based detector is the query length: shorter queries are likely to have more matches (many of them spurious) compared to longer ones. In \figureref{fig:retrieval-length}, we plot the detection accuracy of paraphrased sequences at various query lengths by  truncating each sequence to its first $X$ words before using it as a query for BM25. We use a retrieval corpus of 2M generations for this experiment. We observe that BM25 struggles to detect paraphrased text with a query length of 20 (less than 25\% accuracy), but the detection rate rapidly increases and begins to plateau at 50 tokens. 

\subsection{Scalability and limitations of retrieval-based detectors}
\label{sec:main-retrival-limitations}

In \appendixref{sec:limitations} we extensively discuss the scalability of retrieval (\ref{sec:retrieval-scale}), its limitations (\ref{sec:limitations-retrieval}), ideas for improving retrieval-based detectors (\ref{sec:suggestions-retrieval-scale}), and incentive structures for LLM providers to implement retrieval (\ref{sec:incentives}). In summary, we believe retrieval-based detection is a scalable approach: we estimate that if OpenAI implemented it with ChatGPT, they would need just 5TB of storage space per month (similar to modern portable hard disks). Furthermore, retrieval on ChatGPT scale takes 130 seconds per retrieval on a CPU-only Macbook Pro, which can certainly be further optimized. However, retrieval-based detection has some important limitations: (1) potential privacy risk of exposing \emph{all} LLM responses behind a binary classifier; (2)  inability to use retrieval-based detection on open-source LLMs like LLAMA~\citep{touvron2023llama}; and (3) the need to implement and maintain retrieval infrastructure. We discuss mitigation strategies for the limitations in \appendixref{sec:limitations-retrieval}.