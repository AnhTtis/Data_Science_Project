\begin{figure*}[t]
  \centering
    \begin{subfigure}[t]{0.19\textwidth}
    \centering
    \includegraphics[width=.84\textwidth]{figures/exp/toy_task/demo_teaser.pdf}
    \caption{\small
     Task + Demos \\
    }
    \label{fig:qual_results:pm_demo} 
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/mirl_reward.pdf}
    \caption{\small
      Ours Reward \\
    }
    \label{fig:qual_results:pm_mirl} 
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/airl_reward.pdf}
    \caption{
      \small
      AIRL Reward  \\
    }
    \label{fig:qual_results:pm_airl} 
  \end{subfigure}
    \begin{subfigure}[t]{0.19\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/mirl_eval_rollouts.pdf}
    \caption{
      \small
      Ours Test  \\
    }
    \label{fig:qual_results:eval_pm_mirl}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/airl_eval_rollouts.pdf}
    \caption{
      \small
      AIRL Test  \\
    }
    \label{fig:qual_results:eval_pm_airl}
  \end{subfigure}
  \vspace{-5pt}
  \caption{\small
    Results on the point mass navigation task. We show the learned reward functions of our method (b) vs. AIRL (c) and the policies learned from scratch using those reward functions (d, e). 
  }
  \vspace{-10pt}
  \label{fig:main_qual_results} 
\end{figure*}
