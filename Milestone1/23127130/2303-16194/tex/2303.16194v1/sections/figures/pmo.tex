\begin{figure*}[t]
  \centering
   \begin{subfigure}[t]{0.19\textwidth}
   \centering
    \includegraphics[width=0.84\textwidth]{figures/exp/pmo/demo_teaser.pdf}
    \caption{
      \small Task +Demos
    }
    \label{fig:qual_results:pmo_demo} 
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/mirl_reward.pdf}
    \caption{
      \small
      Ours Reward
    }
    \label{fig:qual_results:pmo_mirl} 
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/airl_reward.pdf}
    \caption{
      \small
      AIRL Reward 
    }
    \label{fig:qual_results:pmo_airl} 
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/mirl_eval_rollouts.pdf}
    \caption{
      \small
      Ours Test
    }
    \label{fig:qual_results:eval_pmo_mirl}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/airl_eval_rollouts.pdf}
    \caption{
      \small
      AIRL Test
    }
    \label{fig:qual_results:eval_pmo_airl}
  \end{subfigure}
  \caption{\small
    Results on the point mass navigation tasks (top row: no-obstacle, bottom row: obstacle task). We show the learned reward functions of our method (b, g) vs. AIRL (c, h) and the policies learned from scratch using those reward functions (d, e, i, j). 
  }
  \vspace{-10pt}
  \label{fig:pmo_qual_results} 
\end{figure*}
