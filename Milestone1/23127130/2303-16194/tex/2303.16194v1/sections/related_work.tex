
\vspace{-10pt}
\section{Background and Related Work}
\label{sec:related_work}

We begin by reviewing Inverse Reinforcement Learning through the lense of bi-level optimization. We assume access to a rewardless Markov decision process (MDP) defined through the tuple $ \mathcal{M} = ( \mathcal{S}, \mathcal{A}, \mathcal{P}, \rho_0, \gamma, H)$ for state-space $ \mathcal{S}$, action space $ \mathcal{A}$, transition distribution $ \mathcal{P}(s' | s,a)$, initial state distribution $\rho_0$, discounting factor $\gamma$, and episode horizon $H$. 
We also have access to a set of expert demonstration trajectories $ \exdata = \left\{ \extraj_i \right\}_{i=1}^{N}$ where each trajectory is a sequence of state, action tuples. %

IRL learns a parameterized reward function $R_\psi(\tau_i)$ which assigns a trajectory a scalar reward. 
Given the reward, a policy $\pi_\theta(a | s)$ is learned which maps from states to a distribution over actions.
The goal of IRL is to produce a reward $R_\psi$, such that a policy trained to maximize the sum of (discounted) rewards under this reward function matches the behavior of the expert. 
This is captured through the following bi-level optimization problem: 
\begin{subequations}
  \begin{align}
    \label{eq:irl:outer_gen}
    \min_{\psi} &\mathcal{L}_\text{IRL}(R_\psi; \pi_\theta) &\text{\bf{(outer obj.)} }\\
    \label{eq:irl:inner_gen}\st &\theta \in \argmax_{\theta} g(R_\psi, \theta) &\text{\bf{(inner obj.)} }  
  \end{align}
\end{subequations}
where $\mathcal{L}_\text{IRL}(R_\psi; \pi_\theta)$ denotes the IRL loss and measures the performance of the learned reward $R_\psi$ and policy $\pi_\theta$; $g(R_\psi, \theta)$ is the reinforcement learning objective used to optimize policy parameters $\theta$. Algorithms for this bi-level optimization consist of an outer loop (\eqref{eq:irl:outer_gen}) that optimizes the reward and an inner loop (\eqref{eq:irl:inner_gen}) that optimizes the policy given the current reward. 

\textbf{Maximum Entropy IRL:}
Early work on IRL learns rewards by separating non-expert from expert trajectories \citep{ng2000algorithms,abbeel2004apprenticeship, abbeel2010autonomous}. 
A primary challenge of these early IRL algorithms was the ambiguous nature of learning reward functions from demonstrations: many possible policies exist for a given demonstration, and thus many possible rewards exist.
The Maximum Entropy (MaxEnt) IRL framework \citep{ziebart2008maximum} seeks to address this ambiguity, by learning a reward (and policy) that is as non-committal (uncertain) as possible, while still explaining the demonstrations. More concretely, given reward parameters $\psi$, MaxEnt IRL optimizes the log probability of the expert trajectories $\extraj$ from demonstration dataset $ \exdata$ through the following loss,
\begin{align*}
  \maxentobj(R_\psi) &= -\mathbb{E}_{\extraj \sim \exdata} \left[\log p(\extraj | \psi)\right] 
  = -\mathbb{E}_{\extraj \sim \exdata}  \left[\log \frac{\exp \left( R_{\psi} (\extraj) \right)}{Z(\psi)} \right]\\
  &= -\mathbb{E}_{\extraj \sim \exdata}\left[R_\psi(\extraj)\right] + \log Z(\psi).
\end{align*} 
A key challenge of MaxEnt IRL is estimating the partition function $Z(\psi) = \int \exp R_\psi d\tau$.
\cite{ziebart2008maximum} approximate $Z$ in small discrete state spaces with dynamic programming. 

\textbf{MaxEnt from the Bi-Level perspective:} However, computing the partition functions becomes intractable for high-dimensional and continuous state spaces. 
Thus algorithms approximate $Z$ using samples from a policy optimized via the current reward. 
This results in the partition function estimate being a function of the current policy $\log \hat{Z}(\psi ; \pi_\theta)$.
As a result, MaxEnt approaches end up following the bi-level optimization template by iterating between: 1) updating reward function parameters given current policy samples via the outer objective (\eqref{eq:irl:outer_gen}); and 2) optimizing the policy parameters with the current reward parameters via an inner policy optimization objective and algorithm \eqref{eq:irl:inner_gen}. 
For instance, model-based IRL methods such as \cite{wulfmeier2017large, levine2012continuous, englert2017inverseRL} use model-based RL (or optimal control) methods to optimize a policy (or trajectory), while model-free IRL methods such as \cite{kalakrishnan_2013_irl, boularias2011relativeirl,finn2016guided,finn2016connection} learn policies via model-free RL in the inner loop. 
All of these methods use policy rollouts to approximate either the partition function of the maximum-entropy IRL objective or its gradient with respect to reward parameters in various ways (outer loop). 
For instance \cite{finn2016guided} learn a stochastic policy $q(\tau)$, and sample from that to estimate $Z(\psi) \approx \frac{1}{M} \sum_{\tau_i \sim q(\tau)}\frac{\exp R_{\psi}(\tau_i)}{q(\tau_i)}$ with $M$ samples from $q(\tau)$. 
\cite{fu2017learning} with adversarial IRL (AIRL) follow this idea and view the problem as an adversarial training process between policy $\pi_\theta(a | s)$ and discriminator $D(s) = \frac{\exp R_\psi(s)}{\exp R_{\psi}(s) + \pi_{\theta}(a|s)}$.
\cite{ni2020f} analytically compute the gradient of the $f$-divergence between the expert state density and the MaxEnt state distribution, circumventing the need to directly compute the partition function. 
 
\textbf{Meta-Learning and IRL:}
Like some prior work \citep{xu2019learning, yu2019meta, wang2021meta, gleave2018multi,seyed2019smile}, \genmethod combines meta-learning and inverse reinforcement learning. 
However, these works focus on fast adaptation of reward functions to new tasks for MaxEnt IRL through meta-learning.
These works require demonstrations of the new task to adapt the reward function.
\genmethod algorithm is a fundamentally new way to learn reward functions and does not require demonstrations for new test settings.
Most related to our work is \cite{das2020model}, which also uses gradient-based bi-level optimization to match the expert.
However, this approach requires a pre-trained dynamics model.
Our work generalizes this idea since \genmethod can optimize general policies, allowing any objective that is a function of the policy and any differentiable RL algorithm. 
We show our method, without an accurate dynamics model, outperforms \cite{das2020model} and scales to more complex tasks where \cite{das2020model} fails to learn.




\textbf{Generalization in IRL:} 
Some prior works have explored how learned rewards can generalize to training policies in new situations. 
For instance, \cite{fu2017learning} explored how rewards can generalize to training policies under changing dynamics. However, most prior work focuses on improving policy generalization to unseen task settings by addressing challenges introduced by the adversarial training objective of GAIL
\citep{xu2019positive,zolna2020combating,zolna2019task,lee2021generalizable, barde2020adversarial,jaegle2021imitation,dadashi2020primal}. Finally, in contrast to most related work on generalization, our work focuses on analyzing and improving reward function transfer to new task settings.

