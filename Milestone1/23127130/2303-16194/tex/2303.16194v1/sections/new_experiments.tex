\vspace{-10pt}
\section{Experiments}
\vspace{-5pt}
\label{sec:main_experiments} 

\begin{figure}[b] 
  \centering
   \begin{subfigure}[t]{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/tasks/reach_env.pdf}
    \caption{\centering Habitat: Reach Task}
    \label{fig:hab-eval:pick-task} 
  \end{subfigure} \quad 
   \begin{subfigure}[t]{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/tasks/trifinger-sim.png}
    \caption{\centering TriFinger: Reach Task}
    \label{fig:trf-eval:reach-task} 
  \end{subfigure} \quad
  \begin{subfigure}[t]{0.15\textwidth}
    \includegraphics[width=\textwidth]{figures/noise_vis/reach_noise2.pdf}
    \caption{\small \centering Test Distribution}
    \label{fig:hab-eval:gen-type-1} 
  \end{subfigure}\quad
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/curves/reach_train.pdf}
    \caption{IRL Training}
    \label{fig:rl_curves:train} 
  \end{subfigure}
  \caption{\small
    (a+b) Visualization of the Fetch and TriFinger reach tasks. 
    c) 2D cross-section of the end-effector goal sampling regions in the reaching task. 
    The reward function is trained on goals from the blue points; the learned reward must train policies to accomplish goals from Easy, Medium, and Hard test distributions of orange, green, and red points. 
    d) Training curves during reward learning on Habitat Task, all methods succeed in training.
  }
  \label{fig:hab-overview}
\end{figure}




In our experiments, we aim to answer the following questions: 
(1) Can \genmethod learn reward functions that can train policies from scratch? 
(2) Does \genmethod learn rewards that can generalize to unseen states and goals better than IRL baselines in complex environments?
(3) Can learned rewards transfer better than policies learned directly with imitation learning? 
We show the first in \Cref{sec:exps:train_reward} and the next two in \Cref{sec:exps:eval_reward}.
We evaluate on two continuous control tasks: 1) Fetch reaching task \cite{szot2021habitat} (Fig~\ref{fig:hab-eval:pick-task}), and the TriFinger reaching task \cite{ahmed2021causalworld} (Fig~\ref{fig:trf-eval:reach-task}).



\subsection{Reward Training phase: Learning Rewards to Match the Expert}
\label{sec:exps:train_reward} 

\paragraph{Experimental Setup and Evaluation Metrics}




In the Fetch reaching task, setup in the Habitat 2.0 simulator \cite{szot2021habitat}, the robot must move its end-effector to a 3D goal location $g$ which changes between episodes.
The action space of the agent is the desired velocities for each of the 7 joints on the robot arm.
The robot succeeds if the end-effector is within 0.1m of the target position by the $20$ time step maximum episode length.
During reward learning, the goal $g$ is sampled from a $0.2$ meter length unit cube in front of the robot, $g \sim \mathcal{U}([0]^3, [0.2]^3)$.
We provide 100 demonstrations.

\begin{wraptable}{L}{0.6\textwidth}
\vspace{-10pt}
\begin{minipage}{0.6\textwidth}
  \resizebox{1.0\textwidth}{!}{
    \input{sections/tables/exp_train_smaller.tex}
  }
 \caption{\small
     Success rates for Fetch Reach and distance to goal for Trifinger Reach tasks in training policies to achieve the goal in the same start state and goal distributions as the expert demonstrations.
     Averages and standard deviations are from 3 seeds on Fetch Reach, and 5 seeds on Trifinger Reach with 100 episodes per seed.
  }
  \label{table:rewards_train}
\end{minipage}
\vspace{-10pt}
\end{wraptable}



For the Trifinger reaching task, each finger must move its fingertip to a 3D goal position.
The fingers must travel a different distance and avoid getting blocked by another finger. 
Each finger has 3 joints, creating a 9D action and state space. 
The fingers are joint position controlled. We use a time horizon of $T=5$ time steps.
We provide a single demonstration.
We report the final  distance to the demonstrated goal, $(g-g^\text{demo})^2$ in meters. 


\paragraph{Evaluation and Baselines}
We evaluate \ppomethod by how well the reward it can train new policies from scratch in the same start state and goal distribution as the demonstrations.
Given the pointmass results \Cref{sec:reward_analysis}, we compare \ppomethod to AIRL,  the best performing baseline for reward learning. 
More details on baseline choice, policy and reward representation, and hyperparameters are described in the Appendix (\ref{app:reach-details}). 


\paragraph{Results and Analysis} 
As \Cref{table:rewards_train} confirms, our method and baselines are able to imitate the demonstrations when policies are evaluated in the same task setting as the expert.
All methods are able to achieve a near 100\% success rate and low distance to goal.
Methods also learn with similar sample efficiency as shown in the learning curves in \Cref{fig:rl_curves:train}.
These high-success rates indicate \ppomethod and AIRL learn rewards that capture the expert behavior and train policies to mimic the expert.
When training policies in the same state/goal distribution as the expert, rewards from \ppomethod follow any constraints followed by the experts, just like the IRL baselines.




\subsection{Test Phase: Evaluating Reward and Policy Generalization}
\label{sec:exps:eval_reward} 
In this section, we evaluate how learned rewards and policies can generalize to new task settings with increased starting state and goal sampling noise.
We evaluate the generalization ability of rewards by evaluating how well they can train new policies to reach the goal in new start and goal distributions not seen in the demonstrations.
This evaluation captures the reality that it is infeasible to collect demonstrations for every possible start/goal configuration.
We thus aim to learn rewards from demonstrations that can generalize beyond the start/goal configurations present in those demonstrations.
We quantify reward generalization ability by whether the reward can train a policy to perform the task in the new start/goal configurations.

For the Fetch Reach task, we evaluate on three wider test goal sampling distributions $g \sim \mathcal{U}([0]^3, [g_{\text{max}}]^3)$: \useeasy, \usemedium, and \usehard, all visualized in \Cref{fig:hab-eval:gen-type-1}.
Similarly, we evaluate on new state regions, which increase the starting and goal initial state distributions but exclude the regions from training, exposing the reward to only unseen initial states and goals. 
In Trifinger, we sample start configurations from around the start joint position in the demonstrations, with increasingly wider distributions ($s_0 \sim \mathcal{N}(s_0^\text{demo}, \delta)$, with $\delta =0.01, 0.03, 0.05)$. 

\begin{table*}[t!]
  \centering
  \resizebox{1\textwidth}{!}{
    \input{sections/tables/reach_eval.tex}
  }
  \vspace{-5pt}
  \caption{\small
  Success rates for the reaching task comparing the generalization capabilities of IRL and imitation learning methods. 
  ``(\Scratch)" transfers the learned reward from IRL methods and trains a newly initialized policy in the test setting.
  ``(\Zero)" transfers the policy without training in the new setting.
  The Easy, Medium, and Hard indicate the difficulty of generalization where the end-effector goal is sampled from $g \sim \mathcal{U}([0]^3, [g_{\text{max}}]^3)$.
  }
  \vspace{-15pt}
  \label{table:reach_eval} 
\end{table*}

We evaluate reward function performance by how well the reward function can train new policies from scratch.
However, now the reward must generalize to inferring rewards in the new start state and goal distributions.
We additionally compare to two imitation learning baselines: Generative Adversarial Imitation Learning (GAIL) \cite{ho2016generative} and Behavior Cloning (BC).
We compare different methods of transferring the learned reward and policy to the test setting:

\textbf{1) Reward}: Transfer only the reward from the above training phase and train a newly initialized policy in the test setting.

\looseness=-1
\textbf{2) Policy}: Transfer only the policy from the above training phase and immediately evaluate the policy without further training in the test setting. This compares transferring learned rewards and transferring learned policies. We use this transfer strategy to compare against direct imitation learning methods. 

\textbf{3) Reward+Policy}: Transfer the reward and policy and then fine-tune the policy using the learned reward in the test setting. Results for this setting are in \Cref{sec:adapt}.

\paragraph{Results and Analysis} 
The results in \Cref{table:reach_eval} show \ppomethod learns rewards that generalize better than IRL baselines to new settings.
In the hardest generalization setting, \ppomethod achieves over twice the success rate of AIRL.
AIRL struggles to transfer its learned reward to harder generalization settings, with performance decreasing as the goal sampling distribution becomes larger and has less overlap with the training goal distribution.
In the ``Hard" start region generalization setting, the performance of AIRL degrades to 34\% success rate.
On the other hand, \ppomethod learns a generalizable reward and performs well even in the ``Hard" generalization strategy, achieving 76\% success.
This trend is true both for generalization to new start state distributions and for new start state regions.
The results for Trifinger Reach in \Cref{table:trf_quant_results} support these findings with rewards learned via \ppomethod generalizing better to training policies from scratch in all three test distributions. 
All training curves for training policies from scratch with learned rewards are in \Cref{sec:rl_training}.

\begin{wraptable}{R}{0.42\textwidth}
\begin{minipage}{0.42\textwidth}
\vspace{-15pt}
\resizebox{1\textwidth}{!}{
  \begin{tabular}{lcc}
    \toprule
    \footnotesize
& \textbf{\ppomethod} & \textbf{AIRL} \\
\midrule
    \textbf{Test $\delta=0.01$} &  \textbf{0.0065 {\scriptsize $\pm$ 0.002 }}  &  0.012 {\scriptsize $\pm$ 0.0017 }  \\
    \textbf{Test $\delta=0.03$} &  \textbf{0.0061 {\scriptsize $\pm$ 0.002 }}  &  0.012 {\scriptsize $\pm$ 0.0008 }  \\
    \textbf{Test $\delta=0.05$} &  \textbf{0.0061 {\scriptsize $\pm$ 0.001 }}  &  0.0117 {\scriptsize $\pm$ 0.0015 }  \\
    \bottomrule
  \end{tabular}
}
 \caption{\small
   Distance to the goal for Trifinger reach, evaluating how rewards generalize to training policies in new start/ goal distributions.
  }
  \label{table:trf_quant_results}
  \vspace{-15pt}
\end{minipage}
\end{wraptable}

\looseness=-1
Furthermore, the results in \Cref{table:reach_eval} also demonstrate that transferring rewards ``(Reward)" is more effective for generalization than transferring policies ``(Policy)". 
Transferring the reward to train new policies typically outperforms transferring only the policy for all IRL approaches.
Additionally, training from scratch with rewards learned via IRL outperforms non-reward learning imitation learning methods that only permit transferring the policy zero-shot. 
The policies learned by GAIL and BC generalize worse than training new policies from scratch with the reward learned by \ppomethod, with BC and GAIL achieving 35\% and 37\% success rates in the ``Hard" generalization setting while our method achieves 76\% success.
The superior performance of \ppomethod over BC highlights the important differences between the two methods with our method learning a reward and training the policy with PPO on the learned reward.

In \Cref{sec:adapt}, we also show the \adapt transfer setting and demonstrate \ppomethod also outperforms baselines in this setting.
In \Cref{sec:supp:reach} we also analyze performance with the number of demos, different inner and outer loop learning rates, and number of inner loop updates.
