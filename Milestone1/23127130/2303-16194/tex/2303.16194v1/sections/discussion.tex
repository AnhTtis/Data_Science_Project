\vspace{-10pt}
\section{Discussion and Future Work}
\vspace{-5pt}
\label{sec:discussion} 

We propose a new IRL framework for learning generalizable rewards with bi-level gradient-based optimization. 
By meta-learning rewards, our framework can optimize alternative outer-level objectives instead of the maximum entropy objective commonly used in prior work.
We propose \ppomethod an instantiation of our new framework, which uses PPO for policy optimization in the inner loop and an action matching objective in the outer loop.
We demonstrate that \ppomethod learns rewards that generalize better than baselines.
Potential negative social impacts of this work are that learning reward functions from data could result in less interpretable rewards, leading to more opaque behaviors from agents that optimize the learned reward.

Future work will explore alternative instantiations of the \genmethod framework, such as utilizing sample efficient off-policy methods like SAC or model-based methods in the inner loop. 
Model-based methods are especially appealing because a single dynamics model could be shared between tasks and learning reward functions for new tasks could be achieved purely using the model. 
Finally, other outer loop objectives rather than action matching are also possible.

