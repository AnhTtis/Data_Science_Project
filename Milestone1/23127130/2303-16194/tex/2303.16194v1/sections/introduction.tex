
\begin{figure}[H]
 	\centering
 	\begin{subfigure}{0.21\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/demo_teaser_larger.pdf}
    \caption{Demonstrations}
    \label{fig:teaser:demo}
  \end{subfigure}\qquad
   \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/airl_teaser.pdf}
    \caption{Max-Ent IRL Reward}
    \label{fig:teaser:me}
  \end{subfigure}\qquad
  \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/mirl_teaser.pdf}
    \caption{\genmethod Reward}
    \label{fig:teaser:mirl} 
  \end{subfigure}
 	\caption{\small 
    A visualization of learned rewards on a task where a 2D agent must navigate to the goal at the center.  \Cref{fig:teaser:demo}: Four trajectories are provided as demonstrations and the demonstrated states are visualized as points. 
    Rewards learned via Maximum Entropy are in \Cref{fig:teaser:me} and \genmethod in \Cref{fig:teaser:mirl}. Lighter colors represent larger predicted rewards.
    The MaxEnt objective overfits to the demonstrations, giving high rewards only close to the expert states, preventing the reward from providing meaningful learning signals in new states.
  }
 	\label{fig:teaser}
\end{figure}
\vspace{-20pt}
\section{Introduction}\label{sec:intro}
Reinforcement learning has demonstrated success on a broad range of tasks from navigation \cite{wijmans2019dd}, locomotion \cite{kumar2021rma, iscen2018policies}, and  manipulation \cite{qt-opt}. 
However, this success depends on specifying an accurate and informative reward signal to guide the agent towards solving the task. 
For instance, imagine designing a reward function for a robot window cleaning task. 
The reward should tell the robot how to grasp the cleaning rag, how to use the rag to clean the window, and to wipe hard enough to remove dirt, but not hard enough to break the window.
Manually shaping such reward functions is difficult, non-intuitive, and time-consuming. 
Furthermore, the need for an expert to design a reward function for every new skill limits the ability of agents to autonomously acquire new skills. 

Inverse reinforcement learning (IRL) \citep{abbeel2004apprenticeship, ziebart2008maximum, osa2018algorithmic} is one way of addressing the challenge of acquiring rewards by learning reward functions from demonstrations and then using the learned rewards to learn policies via reinforcement learning. 
When compared to direct imitation learning, which learns policies from demonstrations directly, potential benefits of IRL are at least two-fold: first, IRL does not suffer from the compounding error problem that is often observed with policies directly learned from demonstrations~\citep{ross2011reduction, barde2020adversarial}; and second, a reward function could be a more abstract and parsimonious description of the observed task that generalizes better to unseen task settings \citep{ng2000algorithms, osa2018algorithmic}. 
This second potential benefit is appealing as it allows the agent to learn a reward function to train policies not only for the demonstrated task setting (e.g. specific start-goal configurations in a reaching task) but also for unseen settings (e.g. unseen start-goal configurations), autonomously without additional expert supervision. 

However, thus far the generalization properties of reward functions learned via IRL are poorly understood. 
Here, we study the generalization of learned reward functions and find that prior IRL methods fail to learn generalizable rewards and instead overfit to the demonstrations.
\Cref{fig:teaser} demonstrates this on a task where a point mass agent must navigate in a 2D space to a goal location at the center.
An important reward characteristic for this task is that an agent, located anywhere in the state-space, should receive increasing rewards as it gets closer to the goal. 
Most recent prior work \cite{fu2017learning,ni2020f,finn2016guidedirl}  developed IRL algorithms that optimize the maximum entropy objective  \citep{ziebart2008maximum} (\Cref{fig:teaser:me}), which fails to capture goal distance in the reward. Instead, the MaxEnt objective leads to rewards that separate non-expert from expert behavior by maximizing reward values along the expert demonstration. While useful for imitating the experts, the MaxEnt objective prevents the IRL algorithms from learning to assign meaningful rewards to other parts of the state space, thus limiting generalization of the reward function. 

As a remedy to the reward generalization challenge in the maximum entropy IRL framework, we propose a new IRL framework called \textbf{Behavioral Cloning Inverse Reinforcement Learning (\genmethod)}. In contrast to the MaxEnt framework, which learns to maximize rewards around demonstrations, the \genmethod framework updates reward parameters such that the policy trained with the new reward matches the expert demonstrations better. This is akin to the model-agnostic meta-learning \citep{finn2017model} and loss learning  \citep{bechtle2019meta} frameworks where model or loss function parameters are learned such that the downstream task performs well when utilizing the meta-learned parameters. 
By using gradient-based bi-level optimization \cite{higher}, \genmethod can optimize the behavior cloning loss to learn the reward, rather than a separation objective like the maximum entropy objective. 
Importantly, to learn the reward, \genmethod differentiates through the reinforcement learning policy optimization, which incorporates exploration and requires the reward to provide a meaningful reward throughout the state space to guide the policy to better match the expert.
We find \genmethod learns more generalizable rewards (\Cref{fig:teaser:mirl}), and achieves over twice the success rate of baseline IRL methods in challenging generalization settings.

Our contributions are as follows:
1) The general \genmethod framework for learning more generalizable rewards from demonstrations, and a specific \ppomethod variant that uses PPO as the RL algorithm. 
2) A quantitative and qualitative analysis of reward functions learned with \genmethod and Maximum-Entropy IRL variants on a simple task for easy analysis. 
3) An evaluation of our novel \genmethod algorithm on two continuous control tasks against state-of-the-art IRL and IL methods. Our method learns rewards that transfer better to novel task settings.
