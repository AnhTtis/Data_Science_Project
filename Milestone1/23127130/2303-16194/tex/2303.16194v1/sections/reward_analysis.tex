\vspace{-5pt}
\section{Illustration \& Qualitative Analysis of Learned Rewards}
\vspace{-5pt}
\label{sec:reward_analysis}

\input{sections/figures/pm}

We first analyze the rewards learned by different IRL methods in a 2D point mass navigation task.
The purpose of this analysis is to test our hypothesis that our method learns more generalizable rewards compared to maximum entropy baselines in simple low-dimensional settings amenable to intuitive visualizations.
Specifically, we compare \ppomethod to the following baselines. 

\textbf{Exact MaxEntIRL (MaxEnt)} \cite{ziebart2008maximum}: The exact MaxEntIRL method where the partition function is exactly computed by discretizing the state space.

\textbf{Guided Cost Learning (GCL)} \cite{finn2016guided}: Uses the maximum-entropy objective to update the reward. The partition function is approximated via adaptive sampling. 

\textbf{Adversarial IRL (AIRL)} \cite{fu2017learning}: An IRL method that uses a learned discriminator to distinguish expert and agent states. As described in \cite{fu2017learning} we also use a shaping network $h$ during reward training, but only visualize and transfer the reward approximator $g$. 

\textbf{f-IRL} \cite{ni2021f}: Another MaxEntIRL based method, f-IRL computes the analytic gradient of the f-divergence between the agent and expert state distributions. We use the JS divergence version.


Our method does not require demonstrations at test time, instead we transfer our learned rewards zero-shot. Thus we forego comparisons to other meta-learning methods, such as \cite{xu2019learning}, which require test time demonstrations.
While a direct comparison with \cite{das2020model} is not possible because their method assumes access to a pre-trained dynamics model, we conduct a separate study comparing their method with an oracle dynamics model against \genmethod in  \Cref{14355}. 
All baselines use PPO \cite{schulman2017proximal} for policy optimization as commonly done in prior work \cite{orsini2021matters}. %
All methods learn a state-dependent reward $r_\psi(s)$, and a policy $\pi(s)$, both parametrized as neural networks.
Further details are described in \Cref{sec:pm_hyperparams}.

\looseness=-1
The 2D point navigation tasks consist of a point agent policy that outputs a desired change in $(x,y)$ position (velocity) $(\Delta x, \Delta y)$ at every time step.
The task has a trajectory length of $T=5$ time steps with 4 demonstrations.
\Cref{fig:qual_results:pm_demo} visualizes the expert demonstrations where darker points are earlier time steps.
The agent starting state distribution is centered around the starting state of each demonstration.

\Cref{fig:main_qual_results}b,c visualize the rewards learned by \genmethod and the AIRL baseline.
Lighter regions indicate higher rewards.
In \Cref{fig:qual_results:pm_mirl}, \genmethod learns a reward that looks like a quadratic bowl centered at the origin, which models the distance to the goal across the entire state space. 
AIRL, the maximum entropy baseline, visualized in \Cref{fig:qual_results:pm_airl}, learns a reward function where high rewards are placed on the demonstrations and low rewards elsewhere. 
Other baselines are visualized in Appendix~\Cref{fig:all_qual_pm}.

To analyze the generalization capabilities of the learned rewards we use them to train policies on a new starting state distribution (visualized in Appendix~\Cref{fig:pm_nav_start_state}).
Concretely, a newly initialized policy is trained from scratch to maximize the learned reward from the testing start state distribution. 
\rv{The policy is trained with 5 million environment steps, which is the same number of steps as for learning the reward.}
The testing starting state distribution has no overlap with the training start state distribution. 
Policy optimization at test time is also done with PPO.
The \Cref{fig:main_qual_results}d,e display trajectories from the trained policies where darker points again correspond to earlier time steps. 

This qualitative evaluation shows that \genmethod learns a meaningful reward for states not covered by the demonstrations. 
Thus at test time agent trajectories are guided towards the goal with the terminal states (lightest points) close to the goal.
The X-shaped rewards learned by the baselines do not provide meaningful rewards in the testing setting as they assign uniformly low rewards to states not covered by the demonstration. 
\rv{This provides poor reward shaping which prevents the agent from reaching the goal within the 5M training interactions with the environment.}
This results in agent trajectories that do not end close to the goal \rv{by the end of training}.

\begin{table}[t] 
  \centering
 \resizebox{0.75\textwidth}{!}{
  \input{sections/tables/pm}
 }
  \caption{\small
    Distance to the goal for the point mass navigation task where numbers are mean and standard error for 3 seeds and 100 evaluation episodes per seed.  
    \pmtrain is policy trained during reward learning. 
    MaxEnt does not learn a policy during reward learning thus its performance is ``NA".
    \pmevaltrain uses the learned reward to train a policy from scratch on the same distribution used to train the reward. 
    \pmevaltest  measures the ability of the learned reward to generalize to a new starting state distribution. 
  }
  \label{table:toy_quant_results}
  \vspace{-10pt}
\end{table}

Next, we report quantitative results in \Cref{table:toy_quant_results}.
We evaluate the performance of the policy trained at test time by reporting the distance from the policy's final trajectory state $s_T$ to the goal $g$: $ \lVert s_{T} - g \rVert_2^2$.
We report the final train performance of the algorithm (``Train"), along with the performance of the policy trained from scratch with the learned reward in the train distribution ``Eval (Train)" and testing distribution ``Eval (Test)".
These results confirm that \genmethod learns more generalizable rewards than baselines.
Specifically, \genmethod achieves a lower distance on the testing starting state distribution at 0.04, compared to 0.53, 1.6, and 0.36 for AIRL, GCL, and MaxEnt respectively.
Surprisingly, \genmethod even performs better than exact MaxEnt, which uses privileged information about the state space to estimate the partition function. 
This fits with our hypothesis that our method learns more generalizable rewards than MaxEnt, even when the MaxEnt objective is exactly computed. 
We repeat this analysis for a version of the task with an obstacle blocking the path to the goal in \Cref{sec:pmo_nav} and reach the same findings even when \genmethod must learn an asymmetric reward function.
We also compare learned rewards to manually defined rewards in \Cref{sec:manual_rewards}.

\rv{
  Despite baselines learning rewards that do not generalize beyond the demonstrations, with enough environment interactions, policies trained under these rewards will eventually reach the high-rewards along the expert demonstrations.
  Since all demonstrations reach the goal in the point mass task, the X-shaped reward baselines learn have high-reward at the center.
  Despite the X-shaped providing little reward shaping off the X, with enough environment interactions, the agent eventually discovers the high-reward point at the goal.
  After training AIRL for 15M steps, 3x the number of steps for reward learning and the experiments in \Cref{table:toy_quant_results} and \Cref{fig:main_qual_results}, the policy eventually reaches $ 0.08 \pm 0.01$ distance to the goal.
  In the same setting, \genmethod achieves $ 0.04 \pm 0.01$ distance to the goal in under 5M steps. 
  The additional performance gap is due to BC-IRL learning a reward with a maximum reward value closer to the center ($ 0.02$ to the center) compared to AIRL ($ 0.04$ to the center).
}
