\newcommand*{\lowerobj}{ \mathcal{L}_{\text{POLICY}}}
\newcommand*{\higherobj}{ \mathcal{L}_{\text{BC-IRL}}}
\newcommand*{\maxentobj}{ \mathcal{L}_{\text{MaxEnt-IRL}}}
\newcommand*{\bcirl}{ \mathcal{L}_{\text{BC-IRL}}}
\newcommand*{\popt}{ \text{POLICY\_OPT}}
\newcommand*{\ptitle}{BC-IRL: Learning Generalizable Reward Functions from Demonstrations}
\DeclareMathOperator{\st}{\text{s.t. }}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\E}{\mathbb{E}}

\newcommand{\genmethod}{BC-IRL\xspace}
\newcommand{\ppomethod}{BC-IRL-PPO\xspace}

\newcommand{\extraj}{\tau^e}
\newcommand{\exdata}{\mathcal{D}^e}

\newcommand{\adapt}{``Policy+Reward"\xspace}
\newcommand{\scratch}{``Reward"\xspace}
\newcommand{\zero}{``Policy"\xspace}
\newcommand{\Adapt}{Policy+Reward\xspace}
\newcommand{\Scratch}{Reward\xspace}
\newcommand{\Zero}{Policy\xspace}

\newcommand{\usetrain}{Train $(g_\text{max}=0.2)$\xspace}
\newcommand{\useeasy}{Easy $(g_\text{max}=0.25)$\xspace}
\newcommand{\usemedium}{Medium $(g_\text{max}=0.4)$\xspace}
\newcommand{\usehard}{Hard $(g_\text{max}=0.55)$\xspace}

\newcommand{\seasy}{$(g_\text{max}=0.25)$\xspace}
\newcommand{\smed}{$(g_\text{max}=0.4)$\xspace}
\newcommand{\shard}{$(g_\text{max}=0.55)$\xspace}


\newcommand{\pmtrain}{Train\xspace}
\newcommand{\pmevaltrain}{Eval (Train)\xspace}
\newcommand{\pmevaltest}{Eval (Test)\xspace}

\newcommand{\franzi}[1]{{\textcolor[rgb]{0.0,0.6,0.5}{FM: #1}}}
\newcommand{\as}[1]{{\textcolor[rgb]{0.6,0.2,0.0}{AS: #1}}}

\newcommand{\start}[1]{{\textcolor[rgb]{0.6,0.2,0.0}{AS: #1}}}


\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}
\setlength\parskip{.5ex plus .5ex}
