\section{Reach Task: Further Experiment Results}
\label{sec:supp:reach} 

\subsection{RL-Training Curves}
\label{sec:rl_training} 
In \Cref{fig:rl_curves} we visualize the training curves for the RL training used in \Cref{table:reach_eval}.
\Cref{fig:supp_rl_curves:train} shows policy learning progress during the IRL training phase. 
In each setting, the performance is measured by using the current reward to train a policy and computing the success rate of the policy.
\Cref{fig:rl_curves:x25} to \Cref{fig:rl_curves:x100} show the policy learning curves at test time, in the generalization settings, where the reward is frozen and must generalize to learn new policies on new goals (``\Scratch" transfer strategy).
These plots show that all methods learn similarly during IRL training (\Cref{fig:supp_rl_curves:train}). 
When transferring the learned rewards to test settings we see that \ppomethod performs better in training successful policies as the generalization difficulty increases with the most difficult generalization in \Cref{fig:rl_curves:x100}.

\begin{figure*}[h]
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/curves/reach_train.pdf}
    \caption{IRL Training}
    \label{fig:supp_rl_curves:train} 
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/curves/reach_scratch_x25.pdf}
    \caption{Start Distrib: \seasy}
    \label{fig:rl_curves:x25} 
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/curves/reach_scratch_x75.pdf}
    \caption{Start Distrib: \smed}
    \label{fig:rl_curves:x75} 
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/curves/reach_scratch_x100.pdf}
    \caption{Start Distrib: \shard}
    \label{fig:rl_curves:x100} 
  \end{subfigure}
  \caption{
    Learning curves for the training setting and \scratch transfer strategies from \Cref{table:reach_eval}.
    All results are for 3 seeds and error regions show the standard deviation in success rate between seeds.
  }
  \label{fig:rl_curves} 
\end{figure*}



\subsection{Transfer Reward+Policy Setting}
\label{sec:adapt} 

\begin{table*}[h]
  \centering
  \resizebox{0.95\textwidth}{!}{
    \input{sections/tables/reach_table_adapt.tex}
  }
  \caption{
    Results for the \adapt transfer strategy where the trained policy and reward are transferred to the test setting and the policy is fine-tuned.
  }
  \label{table:reach_adapt} 
\end{table*}

Here, we evaluate the \textbf{\adapt} transfer strategy to new environment settings where both the reward and policy are transferred. 
In the new setting, \adapt uses the transferred reward to fine-tune the pre-trained transferred policy with RL.
We show results in \Cref{table:reach_adapt} for the \adapt transfer strategy alongside the \scratch transfer strategy from \Cref{table:reach_eval}.
We find that \adapt performs slightly better than \scratch in the Hard setting of generalization to new starting state distributions but otherwise performs similarly. 
Even in the \adapt setting, AIRL struggles to learn a good policy in the Medium and Hard settings, achieving $38\%$ and $81\%$ success rate respectively.

\subsection{Analyzing the Number Demonstrations}\label{sec:num_demos} 

\begin{table*}[h!]
  \centering
  \resizebox{0.95\textwidth}{!}{
    \input{sections/tables/demo_ablate.tex}
  }
  \caption{
    Comparing the number of demonstrations for \ppomethod and AIRL across the train, medium, and hard settings. 
  }
  \label{table:ablate_num_demos}
\end{table*}

We analyze the effect of the number of demonstrations used for reward learning in \Cref{table:ablate_num_demos}.
We find that using fewer demonstrations does not affect the training performance of \ppomethod and AIRL.
We also find our method does just as well with 5 demos as 100 in the +75\% noise setting, with any number of demonstrations achieving near-perfect success rates. 
On the other hand, the performance of AIRL degrades from 93\% success rate with 100 demonstrations to 84\% in the +75\% noise setting.
In the +100\% noise setting, fewer demonstrations hurt performance for both methods, with our method dropping from 76\% success to 69\% success and AIRL from 38\% success to 42\% success.

\subsection{\genmethod Hyperpararameter Analysis}
\label{sec:mirl_ablate}
\ppomethod requires a learning rate for the policy optimization and a learning rate for the reward optimization. 
We compare the performance of our algorithm for various choices of policy and reward learning rates in \Cref{table:lr_ablate}.
We find that across many different learning rate settings our method achieves high rates of success, but high policy learning rates have a detrimental effect.
High reward learning rates have a slight negative impact but are not as severe.

\begin{table}
  \centering
  \input{sections/tables/lr_ablate.tex}
  \caption{
    Comparing choice of learning rate for the inner and outer loops for \ppomethod on the train setting.
    Numbers display success rate.
  }
  \label{table:lr_ablate}
\end{table}


