\section{Trifinger experiment details}
\label{sec:trifinger_details}

\subsection{Policy+Network representation}
All methods use a neural network to represent the policy and reward with 1 hidden layer, 128 hidden units, and $tanh$-activation functions between the layers. 
We use PPO as the policy optimization method for all methods. 
All methods in all tasks use demonstrations obtained from a policy trained with PPO using a manually engineered reward. 

\subsection{Hyperparameters}

\begin{table}
  \centering
  \begin{tabular}{ccc}
    \toprule
    Hyperparameter & \ppomethod & AIRL \\
    \midrule
    Reward Learning Rate        & 1e-3  & 1e-3 \\
    Reward Batch Size           & 6   & 6  \\
    Policy Learning Rate        & 1e-4  & 1e-3  \\
    Policy Learning Rate Decay  & False & False \\
    Policy \# Mini-batches      & 4     & 4   \\
    Policy \# Epochs per Update & 2     & 2   \\
    Policy Entropy Coefficient  & 0.005   & 0.005 \\
    Discount Factor $\gamma$    & 0.99  & 0.99 \\
    Policy Batch Size           & 40 & 40 \\
    \bottomrule
  \end{tabular}
  \caption{
    Method hyperparameters for the Trifinger reaching task. 
    These hyperparameters were used both in the training and reward transfer settings.
  }
  \label{tab:hyperparam-trf}
\end{table}

The hyperparameters for all methods for the Trifinger reaching task are described in \Cref{tab:hyperparam-trf}.
The Adam optimizer \cite{kingma2014adam} was used for policy and reward optimization.
All RL training used 500k steps of experience for the reward training phase and 100k steps of experience for policy optimization in test settings.
