\section{Further Point Mass Navigation Results}

\subsection{Qualitative Results for all Methods in Point Mass Navigation}
\label{sec:pm_all_qual} 

Visualizations of the reward functions from all methods for the regular pointmass task are displayed in \Cref{fig:all_qual_pm}.

\begin{figure*}[t]
  \centering
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/mirl_reward.pdf}
    \caption{\genmethod Reward}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/maxent_reward.pdf}
    \caption{MaxEnt Reward}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/airl_reward.pdf}
    \caption{AIRL Reward}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/gcl_reward.pdf}
    \caption{GCL Train}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/mirl_train_rollouts.pdf}
    \caption{\genmethod Train}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/maxent_train_rollouts.pdf}
    \caption{MaxEnt Train}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/airl_train_rollouts.pdf}
    \caption{AIRL Train}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/gcl_train_rollouts.pdf}
    \caption{GCL Train}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/mirl_eval_rollouts.pdf}
    \caption{\genmethod Test}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/maxent_eval_rollouts.pdf}
    \caption{MaxEnt Test}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/airl_eval_rollouts.pdf}
    \caption{AIRL Test}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/toy_task/gcl_eval_rollouts.pdf}
    \caption{GCL Test}
  \end{subfigure}
  \caption{\small
    Qualitative results for all methods on the point mass navigation task without the obstacle.
  }
  \label{fig:all_qual_pm} 
\end{figure*}




\subsection{Obstacle Point Mass Navigation}
\label{sec:pmo_nav}

\input{sections/figures/pmo}

The obstacle point mass navigation task incorporates asymmetric dynamics with an off-centered obstacle. 
This environment is the same as the point mass navigation task from \Cref{sec:reward_analysis}, except there is an obstacle blocking the path to the center and the agent only spawns in the top-right hand corner.
This task has a trajectory length of $T=50$ time steps with 100 demonstrations.
\Cref{fig:qual_results:pmo_demo} visualizes the expert demonstrations where darker points are earlier time steps.

\begin{table}
  \centering
 \resizebox{0.75\textwidth}{!}{
  \input{sections/tables/pmo}
 }
  \caption{\small
    Distance to the goal for the point mass navigation task where numbers are mean and standard error for 3 seeds and 100 evaluation episodes per seed.  
    \pmtrain is policy trained during reward learning. 
    MaxEnt does not learn a policy during reward learning thus its performance is ``NA".
    \pmevaltrain uses the learned reward to train a policy from scratch on the same distribution used to train the reward. 
    \pmevaltest  measures the ability of the learned reward to generalize to a new starting state distribution. 
  }
  \label{table:pmo_quant_results}
  \vspace{-15pt}
\end{table}

The results in \Cref{table:pmo_quant_results} are consistent with the non-obstacle point mass task where \genmethod generalizes better than a variety of MaxEnt IRL baselines.
In the train setting, \genmethod learns rewards that match the expert behavior with avoiding the obstacle and even achieves better performance than baselines in this task with 0.08 distance to the goal versus 0.41 to the goal for the best performing baseline in the train setting, f-IRL.
\genmethod generalizes better than baselines achieving 0.79 distance to goal compared to the best performing baseline MaxEnt, which also has access to oracle information. 
The reward learned by \genmethod visualized in \Cref{fig:qual_results:pmo_mirl} shows \genmethod learns a complex reward to account for the obstacle.
\Cref{fig:all_qual_pmo} visualizes the rewards for all methods.

\begin{figure*}[t]
  \centering
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/mirl_reward.pdf}
    \caption{\genmethod Reward}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/maxent_reward.pdf}
    \caption{MaxEnt Reward}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/airl_reward.pdf}
    \caption{AIRL Reward}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/gcl_reward.pdf}
    \caption{GCL Train}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/mirl_train_rollouts.pdf}
    \caption{\genmethod Train}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/maxent_train_rollouts.pdf}
    \caption{MaxEnt Train}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/airl_train_rollouts.pdf}
    \caption{AIRL Train}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/gcl_train_rollouts.pdf}
    \caption{GCL Train}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/mirl_eval_rollouts.pdf}
    \caption{\genmethod Test}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/maxent_eval_rollouts.pdf}
    \caption{MaxEnt Test}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/airl_eval_rollouts.pdf}
    \caption{AIRL Test}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/pmo/gcl_eval_rollouts.pdf}
    \caption{GCL Test}
  \end{subfigure}
  \caption{\small
    Qualitative results for all methods on the point mass navigation task with the obstacle.
  }
  \label{fig:all_qual_pmo} 
\end{figure*}




\subsection{Comparison to Manually Defined Rewards}
\label{sec:manual_rewards} 

We compare the rewards learned by \genmethod to two hand-coded rewards. We visualize how well the learned rewards can train policies from scratch in the evaluation distribution in the point navigation with obstacle task. The reward learned by \genmethod therefore must generalize. On the other hand, the hand-coded rewards do not require any learning. We include a sparse reward for achieving the goal, which does not require domain knowledge when implementing the reward. We also implement a dense reward, defined as the change in Euclidean distance to the goal where $r_t = d_{t-1} - d_{t}$ where $d_{t}$ is the distance of the agent to the goal at time $t$.

\Cref{fig:reward_cmp} shows policy training curves for the learned and hand-defined rewards. The sparse reward performs poorly and the policy fails to get closer to the goal. On the other hand, the rewards learned by \genmethod guide the policy closer to the goal. The dense reward, which incoporates more domain knowledge about the task, performs better than the learned reward.


\subsection{Analyzing Number of Inner Loop Updates}
\label{sec:mirl_n_inner_iters}

As described in \Cref{sec:method:ppo_mirl}, a hyperparameter in \ppomethod is the number of inner loop policy optimization steps $K$, for each reward function update. In our experiments, we selected $K=1$. In \Cref{fig:n_inner} we examine the training performance of \ppomethod in the point navigation task with no obstacle for various choices of $K$. We find that a wide variety of $K$ values perform similarly. We, therefore, selected $K=1$ since it runs the fastest, with no need to track multiple policy updates in the meta optimization.

\begin{figure}
  \centering
  \begin{subfigure}{0.38\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/supp/reward.pdf}
    \caption{Hand-coded rewards vs. \genmethod.}
    \label{fig:reward_cmp}
  \end{subfigure}
  \begin{subfigure}{0.38\textwidth}
    \includegraphics[width=\textwidth]{figures/exp/supp/n_inner.pdf}
    \caption{\# inner loop updates in \genmethod.}
    \label{fig:n_inner}
  \end{subfigure}
  \caption{
    Left: Comparing the reward learned from \genmethod to two manually hand-coded rewards. Right: Comparing different number of inner loop steps in \genmethod.
  }
\end{figure}



\subsection{\genmethod with Model-Based Policy Optimization}
\label{14355}
We compare \ppomethod to a version of \genmethod that uses model-based RL in the inner loop inspired by \cite{das2020model}.
A direct comparison to \cite{das2020model} is not possible because their method assumes access to a pre-trained dynamics model, while in our work, we do not assume access to a ground truth or pre-trained dynamics model. 
However, we compare to a version of \cite{das2020model} in the point mass navigation task with a ground truth dynamics model. 
Specifically, we use gradient-based MPC in the inner loop optimization as in \cite{das2020model}, but the BC IRL outer loop objective. 
With the BC outer loop objective, it also learns generalizable rewards in the point mass navigation task achieving $0.06 \pm 0.03$ distance to goal in ``Eval (Train)" and $0.07 \pm 0.03$ in ``Eval (Test)". 
However, in the point mass navigation task with the obstacle, this method fails to learn a reward and struggles to minimize the outer loop objective. 
We hypothesize that in longer horizon tasks, the MPC inner loop optimization in [9] easily gets stuck in local minimas and struggles to differentiate through the entire MPC optimization.


