\section{Further Reach Task Details}
\label{app:reach-details}

\subsection{Choice of Baselines}
The ``Exact MaxEntIRL" approach is excluded because it cannot be computed exactly for high-dimensional state spaces.
GCL is excluded because of its poor performance on the toy task relative to other methods.
We also compare to the following imitation learning methods which learn only policies and no transferable reward:
\begin{itemize}
\item \textbf{Behavioral Cloning (BC)} \cite{bain1995framework}: Train a policy using supervised learning to match the actions in the expert dataset. 
\item \textbf{Generative Adversarial Imitation Learning (GAIL)} \cite{ho2016generative}: Trains a discriminator to distinguish expert from agent transitions and then use the discriminator confusion score as the reward. This reward is coupled with the current policy \cite{finn2016connection} (referred to as a ``pseudo-reward") and therefore cannot train policies from scratch.
\end{itemize}

\subsection{Policy+Network representation}
All methods use a neural network to represent the policy and reward with 1 hidden layer, 128 hidden units, and $tanh$-activation functions between the layers. 
We use PPO as the policy optimization method for all methods. 
All methods in all tasks use demonstrations obtained from a policy trained with PPO using a manually engineered reward. 

\subsection{Hyperparameters}
\label{sec:hyperparams} 
The hyperparameters for all methods from the Reaching task are described in \Cref{tab:hyperparam}.
The Adam optimizer \cite{kingma2014adam} was used for policy and reward optimization.
All RL training used 1M steps of experience for the training and testing settings.
The \scratch and \adapt transfer strategies trained policies with the same set of hyperparameters.

\begin{table}
  \centering
  \begin{tabular}{ccccc}
    \toprule
    Hyperparameter & \ppomethod & AIRL & GAIL & BC \\
    \midrule
    Reward Learning Rate        & 3e-4  & 1e-4 & 1e-4 & NA \\
    Reward Batch Size           & 128   & 128  & 128  & NA \\
    Policy Learning Rate        & 3e-4  & 1e-4 & 1e-4 & 1e-4 \\
    Policy Learning Rate Decay  & False & True & True & False \\
    Policy \# Mini-batches      & 4     & 4    & 4    & NA \\
    Policy \# Epochs per Update & 4     & 4    & 4    & NA \\
    Policy Entropy Coefficient  & 0.0   & 0.0  & 0.0  & NA \\
    Discount Factor $\gamma$    & 0.99  & 0.99 & 0.99 & 0.99 \\
    Policy Batch Size                & 4096  & 4096 & 4096 & NA \\
    \bottomrule
  \end{tabular}
  \caption{
    Method hyperparameters for the Fetch reaching task. 
    These hyperparameters were used both in the training and reward transfer settings.
  }
  \label{tab:hyperparam}
\end{table}
