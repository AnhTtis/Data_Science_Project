\section{Further 2D Point Navigation Details}
\label{sec:pm_hyperparams} 

The start state distributions for the 2D point navigation task are illustrated in \Cref{fig:pm_nav_start_state}.
The reward is learned using the start distribution in red on 4 equally spaced points from the center.
Four demonstrations are also provided in this train start state distribution from each of the four corners.
The reward is then transferred and a new policy is trained with the start state distribution in the magenta color. 
This start state distribution has no overlap with the train distribution and is also equally spaced.
The reward must therefore generalize to providing rewards in this new state distribution.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.40\textwidth]{figures/tasks/pm_start_state_dist_vis.pdf}
  \caption{
    The starting state distribution for the 2D point navigation task with the demonstrations and negative distance to the goal overlaid.
    The training start state distribution where the reward is learned is in \textcolor[rgb]{1.0, 0.03, 0.11}{red}.
    The test start state distribution where the reward is transferred is in \textcolor[rgb]{1.0,0.23,0.98}{magenta}.
  }
  \label{fig:pm_nav_start_state}
\end{figure}


The hyperparameters for the methods from the 2D point navigation task in \Cref{sec:reward_analysis} are detailed in \Cref{tab:pm_hyperparam} for the no obstacle version and \Cref{tab:pmo_hyperparam} for the obstacle version of the task.
The reward function / discriminator for all methods was a neural network with 1 hidden layer and 128 hidden dimension size with $ tanh$-activations between the layers.
Adam \cite{kingma2014adam} was used for policy and reward optimization.
All RL training used 5M steps of experience for the training and testing setting for the navigation no obstacle task.
f-IRL uses the same optimization and neural network hyperparameters for the discriminator and reward function.
Like in \cite{ni2020f}, we clamp the output of the reward function within the range $ [-10, 10]$ and found this was beneficial for learning.
In the navigation with obstacle task, training used 15M steps of experience and testing used 5M steps of experience. 
All experiments were run on a Intel(R) Core(TM) i9-9900X CPU @ 3.50GHz.

\begin{table}
  \centering
  \begin{tabular}{cccccc}
    \toprule
    Hyperparameter             & \ppomethod & AIRL & GCL & MaxEnt & f-IRL\\ 
    \midrule
    Reward Learning Rate        & 1e-4 & 1e-3 & 3e-4 & 1e-3 & 3e-4\\
    Reward Batch Size    & 20   & 20   & 20   & 20 & 20\\
    Policy Learning Rate        & 1e-4 & 1e-4 & 3e-4 & 3e-4 & 3e-4 \\
    Policy Learning Rate Decay  & True & True & True & False & False\\
    Policy \# Mini-batches      & 4    & 4    & 4    & 4 & 4 \\
    Policy \# Epochs per Update & 4    & 4    & 4    & 4 & 4 \\
    Policy Entropy Coefficient  & 1e-4 & 1e-4 & 1e-4 & 1e-4 & 1e-4 \\
    Discount Factor $\gamma$    & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 \\
    Policy Batch Size                & 1280  & 1280  & 1280  & 1280 & 1280 \\
    \bottomrule
  \end{tabular}
  \caption{
    2D navigation without obstacle method hyperparameters.
    These hyperparameters were used both in the training and reward transfer settings.
  }
  \label{tab:pm_hyperparam}
\end{table}


\begin{table}
  \centering
  \begin{tabular}{ccccc}
    \toprule
    Hyperparameter             & \ppomethod & AIRL & GCL & MaxEnt \\
    \midrule
    Reward Learning Rate        & 1e-4 & 1e-3 & 3e-4 & 1e-3 \\
    Reward Batch Size    & 256   & 256   & 256   & 256 \\
    Policy Learning Rate        & 3e-4 & 3e-4 & 3e-4 & 3e-4 \\
    Policy Learning Rate Decay  & True & True & True & False \\
    Policy \# Mini-batches      & 4    & 4    & 4    & 4 \\
    Policy \# Epochs per Update & 4    & 4    & 4    & 4 \\
    Policy Entropy Coefficient  & 1e-4 & 1e-4 & 1e-4 & 1e-4 \\
    Discount Factor $\gamma$    & 0.99 & 0.99 & 0.99 & 0.99 \\
    Policy Batch Size                & 6400  & 6400  & 6400  & 6400 \\
    \bottomrule
  \end{tabular}
  \caption{
    2D navigation with obstacle method hyperparameters.
    These hyperparameters were used both in the training and reward transfer settings.
  }
  \label{tab:pmo_hyperparam}
\end{table}


