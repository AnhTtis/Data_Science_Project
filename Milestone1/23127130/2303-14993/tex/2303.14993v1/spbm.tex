\documentclass[reprint,amsmath,amssymb,aps,prl]{revtex4-2}
\usepackage[pdftex]{graphicx}
\usepackage{dcolumn}
\usepackage{txfonts}
\usepackage{bm}
\usepackage{hyperref}

\usepackage{pdfpages}
\makeatletter
\AtBeginDocument{\let\LS@rot\@undefined}
\makeatother

\DeclareMathOperator{\sgn}{sgn}

\begin{document}

\title{%
Spatial-Photonic Boltzmann Machines:
Low-Rank Combinatorial Optimization\\ and
Statistical Learning by Spatial Light Modulation}
\author{Hiroshi Yamashita}
\author{Ken-ichi Okubo}
\author{Suguru Shimomura}
\author{Yusuke Ogura}
\author{Jun Tanida}
\author{Hideyuki Suzuki}
\email{hideyuki@ist.osaka-u.ac.jp}
\affiliation{%
  Graduate School of Information Science and Technology,
  Osaka University, Osaka 565--0871, Japan}

\begin{abstract}
The spatial-photonic Ising machine (SPIM)
[D. Pierangeli et al., Phys. Rev. Lett. \textbf{122}, 213902 (2019)]
is a promising optical architecture
utilizing spatial light modulation for solving
large-scale combinatorial optimization problems efficiently.
However, the SPIM can accommodate Ising problems with only rank-one interaction matrices,
which limits its applicability to various real-world problems.
In this Letter, we propose a new computing model for
the SPIM that can accommodate any Ising problem
without changing its optical implementation.
The proposed model is particularly efficient for
Ising problems with low-rank interaction matrices, such as knapsack problems.
Moreover, the model acquires learning ability
and can thus be termed a spatial-photonic Boltzmann machine (SPBM).
We demonstrate that learning, classification, and sampling of the MNIST handwritten digit images
are achieved efficiently using SPBMs with low-rank interactions.
Thus, the proposed SPBM model exhibits higher practical applicability
to various problems of combinatorial optimization and statistical learning,
without losing the scalability inherent in the SPIM architecture.
\end{abstract}

\maketitle

\textit{Introduction.}%
---%
As the recent development of machine intelligence technologies relies
largely on massive computational power for optimization and learning,
there is a growing demand for high-speed, large-scale,
and energy-efficient computation
to deal with increasingly complex real-world problems.
A possible approach to meet this demand is to
adopt unconventional, problem-specific computing technologies,
without relying on the conventional von Neumann architecture.

Ising machines are dedicated hardware solvers for
combinatorial optimization problems formulated as Ising problems,
designed to find the (approximate) ground states of the corresponding Ising models
or, almost equivalently, Boltzmann machines \cite{Ackley1985,Korst1989}.
Many important combinatorial optimization problems can be formulated
as Ising problems \cite{Korst1989,Lucas2014},
thus leading to numerous studies
\cite{Mohseni2022,Johnson2011,Yamaoka2015,Inagaki2016,Leleu2019,Tsukamoto2017,Goto2019,Okuyama2019,Wang2019,Pierangeli2019,Pierangeli2020,Bohm2022}
on implementing Ising machines
using various physical devices and dynamics.

The spatial-photonic Ising machine (SPIM) \cite{Pierangeli2019,Pierangeli2020}
is a promising optical architecture utilizing spatial light modulation
for solving large-scale Ising problems efficiently.
As the SPIM optically computes the Ising Hamiltonian with all-to-all interactions
in constant time,
independent of the number of variables,
it can accelerate annealing computation.
Its outstanding performance has been demonstrated
for problems with more than ten thousand variables\cite{Prabhakar2023},
indicating the high efficiency of the SPIM for solving large-scale problems.

Despite its superior scalability,
the SPIM can accommodate only a limited class of Ising problems
with rank-one interaction matrices.
This limitation reduces its applicability to
various real-world problems,
whose Ising formulations typically have matrix ranks greater than one.
Although subsequent studies \cite{Sun2022,Luo2023} multiplexed
the SPIM architecture to handle broader classes of Ising problems,
the scalability is degraded instead because these architecture-level approaches
reduce the number of variables to increase the matrix rank.

In this Letter, we propose a new computing model for the SPIM
to circumvent the limitation and accommodate higher-rank interaction matrices
without changing its optical implementation.
The proposed model is capable of handling any Ising problem,
and is particularly efficient for problems with low-rank interactions.
We demonstrate its efficient applicability to knapsack problems
by formulating them as Ising problems with rank two.

Moreover, we show that the proposed model acquires
the learning ability of Boltzmann machines \cite{Ackley1985},
thus termed a spatial-photonic Boltzmann machine (SPBM).
The SPBM with full-rank interactions has
expressive power equivalent to the ordinary Boltzmann machine;
however, the SPBM with low-rank interactions is efficient and
can be sufficient for inferences from real-world data,
as typically assumed in low-rank modeling.
We demonstrate that learning, classification, and sampling
of the MNIST handwritten digit images \cite{MNIST} are achieved efficiently
using SPBMs with low-rank interactions.
Notably, we observe that the learning rule for SPBM naturally performs
low-rank learning of the digit images,
whereas low-rank constraints are not explicitly imposed.

Thus, we report here that the proposed SPBM model
exhibits higher practical applicability
to various problems of combinatorial optimization and statistical learning,
without losing the scalability inherent in the SPIM architecture.
Although the model can theoretically work with
any existing implementation of the SPIM,
we reinforce the validity of our approach
by presenting the results of proof-of-concept optical experiments in this Letter.

\textit{Optical computation of Ising Hamiltonian.}%
---%
The SPIM \cite{Pierangeli2019,Pierangeli2020}
computes the Ising Hamiltonian optically from the phase-modulated image
of an amplitude-modulated laser beam (Fig.~\ref{fig:schematic}).
Light incident on the $i$th site of the spatial light modulator (SLM)
with an amplitude $\xi_i$
is phase-modulated by $\sigma_i=\exp(\mathrm{i}\phi_i)=\pm 1$,
which represents the $i$th Ising spin,
and detected by an image sensor.
By comparing the detected image $I$ with the point-like target image $I_\text{T}$,
the Ising Hamiltonian in the following form is obtained:
\begin{equation}
H(\bm\sigma) \propto \sum_{i,j}\xi_i\xi_j\sigma_i\sigma_j
= \bm\sigma^\top\bm\xi\bm\xi^\top\bm\sigma,
\label{eq:spim}
\end{equation}
where $\bm\xi=(\xi_1,\ldots,\xi_N)^\top$
and $\bm\sigma=(\sigma_1,\ldots,\sigma_N)^\top$.
Notably, the computation is performed in constant time,
independent of the number of spins $N$,
involving all-to-all interactions among the spins.
However, compared with the ordinary formulation of (quadratic) Ising Hamiltonian
$H(\bm\sigma)=-\frac{1}{2} \bm\sigma^\top J\bm\sigma$,
the interaction matrix $J$ of SPIM is limited to the form
$J\propto\bm\xi\bm\xi^\top$.
Thus, the SPIM can accommodate only real symmetric matrices with rank one
as the interaction matrix.
The Ising spin system with this type of Hamiltonian is known as
the Mattis model \cite{Mattis1976}.

\begin{figure}[tb]
\centering
\includegraphics[width=86mm]{fig1.pdf}
\caption{Schematic of the SPIM architecture.
The laser beam is amplitude-modulated and phase-modulated
by spatial light modulators SLM1 and SLM2,
which encode $\bm\xi$ and $\bm\sigma$, respectively,
and detected by an image sensor.
The Ising Hamiltonian is obtained from the detected image $I$.}
\label{fig:schematic}
\end{figure}

Here we propose a new computing model for the SPIM architecture
to improve the expressive power of the interaction matrix.
We formulate the Hamiltonian
as a linear combination of Eq.~(\ref{eq:spim}) as follows:
\begin{equation}
H(\bm\sigma)=-\frac{1}{2}\sum_{k=1}^K\lambda_k\sum_{i,j}\xi_{i,k}\xi_{j,k}\sigma_i\sigma_j
= -\frac{1}{2}\bm\sigma^\top
   \left(\sum_{k=1}^K\lambda_k\bm\xi_k\bm\xi_k^\top\bm\right)
   \bm\sigma,
\label{eq:spbm}
\end{equation}
where $K$ denotes the number of components,
and $\lambda_k$ and $\bm\xi_k=(\xi_{1,k},\ldots,\xi_{N,k})^\top$ are
the weight and amplitude parameters of the $k$th component, respectively.
The energy value of the Hamiltonian can be obtained by calculating
the weighted sum from images acquired $K$ times
with different amplitudes $\bm\xi_k$.
Now the interaction matrix
$J=\sum_k\lambda_k\bm\xi_k\bm\xi_k^\top$ 
can represent any real symmetric matrix with rank not greater than $K$.
Therefore, if $K$ is increased to $N$,
any Ising Hamiltonian can be computed.
Although the computation time increases linearly to $K$,
it does not depend directly on $N$,
inheriting the scalability of the underlying SPIM architecture.

We term the proposed computing model with the Hamiltonian in Eq.~(\ref{eq:spbm})
as the spatial-photonic Boltzmann machine (SPBM)
because it acquires the learning ability of Boltzmann machines,
as explained later in detail.

\textit{Combinatorial optimization with SPBMs.}%
---%
To solve a combinatorial optimization problem using an Ising machine,
we formulate it as an Ising problem, or equivalently,
a quadratic unconstrained binary optimization (QUBO) problem,
which is to find $\bm\sigma\in\{+1,-1\}^N$ that minimizes
the Ising Hamiltonian $H(\bm\sigma)=-\frac{1}{2}\bm\sigma^\top J \bm\sigma$.
For simplicity, the linear (bias) term is omitted here
because it can be represented by introducing an additional spin fixed to $+1$.

The Hamiltonian of the original SPIM, with rank $K=1$,
can be expressed as $H(\bm\sigma)=-\frac{\lambda}{2}\left(\bm\xi^\top\bm\sigma\right)^2$.
When $\lambda>0$, the Ising problem is trivial
as it has two symmetric global minimum solutions
$\bm\sigma=\pm\sgn\bm\xi$,
where the sign is taken element-wise.
In the alternative case, $\lambda<0$,
the problem reduces to a number partitioning problem
\cite{Ferreira1998,Mertens1998,Mertens2001,Pierangeli2021,Huang2021,Prabhakar2023},
which is to find the partition of numbers $\xi_1,\dots,\xi_N$
into two subsets that minimizes the difference of the sums in the two subsets
$\left\lvert\sum_i \xi_i\sigma_i\right\rvert=\left\lvert\bm\xi^\top\bm\sigma\right\rvert$.
Thus, the original SPIM can essentially handle
only the class of number partitioning problems.
Although this class is theoretically NP-hard \cite{Pedroso2010},
the SPIM in its original form is practically insufficient to be used
for solving Ising formulations of various combinatorial optimization problems.

However, we can circumvent the limitation without changing the optical implementation by introducing
the proposed SPBM model, which enables us to handle any Ising problem.
Particularly, the SPBM model is efficient
for Ising problems with low-rank interactions
because the computation time depends linearly on rank $K$.

The spin configuration $\bm\sigma$ is updated according to
energy values of the Hamiltonian $H(\bm\sigma)$ obtained by the SPBM.
To solve an Ising problem, typically we employ simulated annealing \cite{Kirkpatrick1983};
that is, we generate a sample sequence of $\bm\sigma$
using a Markov-chain Monte Carlo (MCMC) method
from the Gibbs distribution
$P(\bm\sigma)\propto\exp \left(-H(\bm\sigma)/T\right)$,
which is expected to converge to an approximate ground state
as the system temperature $T$ gradually decreases.
It should be noted that
we can obtain the energy difference $\Delta H=H(\bm\sigma')-H(\bm\sigma)$
required in MCMC methods in constant time, independent
of the number of spin flips from $\bm\sigma$ to $\bm\sigma'$,
because the SPBM computes energy values directly.

\textit{Application to knapsack problems.}%
---%
To demonstrate the applicability of the SPBM
to a broader class of combinatorial optimization problems,
we apply it to the 0-1 knapsack problem with integer weights,
which can be formulated as Ising problems with rank $K=2$
and hence cannot be handled by the original SPIM.

The knapsack problem is a well-known combinatorial optimization problem
to find the subset of a given set of items that maximizes the total value
satisfying a predefined total weight limit.
More specifically, given the value $v_i$ and the weight $w_i$ of
the $i$th item for $i=1,2,\ldots,n$ and the weight limit $W$,
the 0-1 knapsack problem can be expressed as follows:
\begin{align}
&\text{maximize}\quad \sum_{i=1}^nv_i x_i\\
&\text{subject to}\quad \sum_{i=1}^n w_i x_i \le W,
\quad \bm{x}=(x_1,\dots,x_n)\in\{0,1\}^n.
\end{align}
Under the assumption of integer weights, the knapsack problem can be formulated
as an Ising problem with the following Hamiltonian:
\begin{equation}
H(\bm{x},\bm{y})=
  A\left(\sum_{i=1}^nw_ix_i + \sum_{i=1}^m2^{i-1}y_i - W\right)^2
  -B\left(\sum_{i=1}^n v_ix_i\right)^2,
\label{eq:knapsack}
\end{equation}
where auxiliary variables $\bm{y}=(y_1,\dots,y_m)\in \{0,1\}^m$
are introduced using a log trick \cite{Lucas2014}.
Coefficient $A$ for the constraint term is sufficiently large
compared with $B$ for the total value term because the constraint violation
should be penalized more than the gain of picking an item;
specifically, we set $A=v_\mathrm{max}\left(2\sum_i v_i- v_\mathrm{max}\right)+1$
and $B=1$, where $v_\mathrm{max}=\max_iv_i$.
The number of auxiliary variables for log trick
is generally set as $m=\left\lfloor\log_2 W\right\rfloor+1$;
however, in this study, we set $m=\left\lceil\log_2 \max_iw_i\right\rceil$,
assuming that the problem is nontrivial, $\sum_i w_i > W$.

The Hamiltonian for the knapsack problem
in Eq.~(\ref{eq:knapsack}) can be rewritten
in the form of the SPBM Hamiltonian in (\ref{eq:spbm})
with size $N=n+m+1$ and rank $K=2$, using the following parameter values
and variable transformation:
\begin{align}
&\lambda_1 = -\frac{A}{2}, \quad \lambda_2 = +\frac{B}{2},\\
&\bm\xi_1=\bigl(w_1,\ldots,w_n,2^0,\ldots,2^{m-1},\textstyle\sum_iw_i+2^m-1-2W\bigr)^\top,\\
&\bm\xi_2=\bigl(v_1,\ldots,v_n,0,\ldots,0,\textstyle\sum_iv_i\bigr)^\top,\\
&\bm\sigma = \left(2x_1-1,\ldots,2x_n-1,2y_1-1,\ldots,2y_m-1,1\right)^\top.
\end{align}

\begin{figure}[tb]
\centering
\includegraphics{fig2.pdf}
\caption{Sampling behavior of the SPBM for a knapsack problem.
Typical time evolutions of energy values
of the spin configurations sampled
from (a) optical and (b) numerical experiments.
Several samples with higher energy values are not shown.
(c) Histograms of the energy values of $3000\times 50$ samples
observed from each experiment with bin width 1000.}
\label{fig:knapsack}
\end{figure}

We conducted a proof-of-concept experiment on the SPBM \cite{SM}
for a knapsack problem with $n=13$ items \cite{Pisinger1999}.
The spin sequences were sampled both optically and numerically
from the SPBM at a moderately low, constant temperature.
The experimental results shown in Fig.~\ref{fig:knapsack} indicate
that the SPBM generates sample sequences essentially
according to the Gibbs distribution.
The typical time evolution of the energy values $H(\bm\sigma)$ of spins
observed in the optical experiment (Fig.~\ref{fig:knapsack}(a))
resembles that of the numerical experiment (Fig.~\ref{fig:knapsack}(b)).
The histogram of energy values sampled from the optical experiment
(Fig.~\ref{fig:knapsack}(c)) shows that the SPBM generates
many low-energy samples around $H(\bm\sigma)\approx 0$,
constituting the distribution
with peaks at the same values as those in the numerical experiment.

A closer look at these results indicates that
the temperature of the Gibbs distribution
was slightly higher in the optical experiment
due to the noise in the optical system.
Although the physical noise can be utilized as a source of randomness \cite{Pierangeli2020NP},
we simply executed the Metropolis algorithm
adhering to the obtained Hamiltonian values for clarity of results.
Multiple-spin flips were allowed in generating candidate states
to facilitate the MCMC process to jump over energy barriers
caused by the strong constraint term,
which can be performed efficiently in the SPBM, as explained earlier.

The optimal solution to the knapsack problem was obtained 304 times
out of the 150000 samples observed in the optical experiment,
with a ratio considerably higher than the probability $2^{-13}$ of random sampling.
This result confirms that the spin states with lower energy values
were sampled frequently according to the Gibbs distribution
$P(\bm\sigma)\propto\exp \left(-H(\bm\sigma)/T\right)$.

Overall, we demonstrated that the SPBM with rank $K=2$ works as expected
with the Ising Hamiltonian for the knapsack problem
in both the numerical and optical experiments.
These results indicate that the SPBM can efficiently handle
Ising problems with low-rank interactions.

\textit{Statistical learning with SPBMs.}%
---%
In the field of machine learning, the Ising model is commonly referred to
as the Boltzmann machine, which can be viewed as a generative
neural network model composed of stochastic elements.
The Boltzmann machine has been extensively applied for
solving combinatorial optimization problems \cite{Korst1989} using simulated annealing \cite{Kirkpatrick1983}.
Furthermore, it has been important because of its capability of statistical machine learning.
The restricted Boltzmann machine (RBM) \cite{Smolensky1986,Hinton2002,Larochelle2008}
and deep Boltzmann machine (DBM) \cite{Salakhutdinov2009,Salakhutdinov2012}
are well-known subclasses of the Boltzmann machine that
have contributed to the recent development of deep learning.

With the equivalence between the Ising model and Boltzmann machine,
the SPBM can be considered to have learning ability.
Note that the expressive power of the original SPIM with rank-one interactions,
equivalent to the Mattis model, is insufficient for statistical learning.
Thus, the increased expressive power of
the SPBM model enables the SPIM architecture to attain
the learning ability applicable to real-world data.
If rank $K$ is increased to system size $N$,
it becomes equivalent to the ordinary Boltzmann machine;
however, it is efficient with low-rank interactions.

To train the SPBM model $P(\bm\sigma)\propto\exp \left(-H(\bm\sigma)\right)$,
we perform the gradient ascent on the log-likelihood $\log L$
given the data distribution, according to the gradients \cite{SM}
\begin{align}
  \frac{\partial}{\partial \lambda_k}\log L
  &= \frac{1}{2} \bm\xi_k^\top\left(
      \langle\bm\sigma\bm\sigma^\top \rangle_\text{data}
    - \langle\bm\sigma\bm\sigma^\top \rangle_\text{model}
    \right)\bm\xi_k, \label{eq:gradient-lambda}\\
  \frac{\partial}{\partial \bm\xi_k}\log L
  &= \lambda_k\left(
      \langle \bm\sigma\bm\sigma^\top \rangle_\text{data}
    - \langle \bm\sigma\bm\sigma^\top \rangle_\text{model}
    \right)\bm\xi_k, \label{eq:gradient-xi}
\end{align}
where $\langle \cdot \rangle_\text{data}$
and $\langle \cdot \rangle_\text{model}$
denote the expectations provided that the visible units are
clamped to the data vectors and unclamped, respectively,
as in the learning rule of Boltzmann machines \cite{Ackley1985}.

Typically, we can introduce regularization in the learning process
to avoid overfitting.
We use the L2 regularizer on the parameters $\lambda_k$ and $\bm\xi_k$ in this study.

\textit{Learning MNIST digit images.}%
---%
To demonstrate the learning ability as a Boltzmann machine,
we trained the SPBM \cite{SM} using the MNIST digit image data \cite{MNIST}.

\begin{figure}[tb]
\centering
\includegraphics{fig3.pdf}
\caption{Classification of MNIST digit images.
The classification accuracy of the trained SPBMs
with rank $K$ taking on integer multiples of 10 is shown.}
\label{fig:classification}
\end{figure}

First, we applied the SPBM for the classification of handwritten digits.
We trained fully visible SPBMs with size $N=794$, composed of
784 units for $28\times 28$ pixels of MNIST digit images
and 10 units for the one-hot encoding of the 10 digit classes.
Fig.~\ref{fig:classification} shows the dependency of
the classification accuracy on rank $K$.
Although the accuracy drops to the chance level for $K\le 30$
owing to training failure,
the graph is almost flat for $K\ge 100$; that is, the SPBM with rank
as low as $K=100$ exhibits a performance comparable to that of the full rank.
This result indicates that the statistical learning of SPBM for the MNIST images
was sufficiently achieved with low-rank interactions.
Note that the accuracy was not as high as that of the ordinary RBM,
because we used fully visible Boltzmann machines
without hidden units for simplicity.

\begin{figure}[tb]
\centering
\includegraphics{fig4a.pdf}
\includegraphics{fig4b.pdf}
\includegraphics{fig4c.pdf}
\includegraphics{fig4d.pdf}
\caption{Sampling from the SPBMs trained with the MNIST digit images.
(a) Random samples from the training dataset.
Random samples generated from
(b) the trained SPBMs with rank $K=50$,
(c) the reduced SPBMs with only principal components, and
(d) the optical experiment of the reduced SPBMs,
after 1960 time steps from random initial spin configurations.
The conditional probabilities of each spin
given the states of other spins are shown.}
\label{fig:sampling}
\end{figure}

Next, we sampled digit images from fully visible SPBMs
with size $N=196$ and rank $K=50$
trained using the MNIST images of each digit, in which
the digit images were shrunk to $14\times 14$ pixels
(Fig.~\ref{fig:sampling}(a)).
Random samples from the trained models (Fig.~\ref{fig:sampling}(b))
show that the digit images were successfully sampled.
Note that the inverse images are sampled because of the symmetry
$H(\bm\sigma)=H(-\bm\sigma)$ of the model owing to the absence of a bias term.
The images did not degrade in random samples from the reduced model (Fig.~\ref{fig:sampling}(c))
composed only of principal components with magnitudes
$\left\lvert\lambda_k\right\rvert\left\|\bm\xi_k\right\|^2 > 0.1$.
This result indicates that the reduced, low-rank SPBM is sufficient for sampling.
Fig.~\ref{fig:sampling}(d) shows random samples obtained optically
from the reduced SPBM for the digit ``0''.
Some samples maintain the digit shape,
while some appear to degrade, in comparison with the numerical results,
possibly due to the noise in the optical system.
Again, we executed the Metropolis algorithm
adhering to the Hamiltonian values obtained optically,
without utilizing the physical noise, for clarity of results.

\begin{figure}[tb]
\centering
\includegraphics{fig5a.pdf}
\includegraphics{fig5b.pdf}
\caption{Low-rank learning of the SPBM.
(a) Time evolution of the magnitude values
$\left\lvert\lambda_k\right\rvert\left\|\bm\xi_k\right\|^2$
in the learning process of the SPBM with rank $K=50$ for the digit ``0''.
(b) Gray-scale images of principal components $\bm\xi_k$
with the five largest magnitude values for each digit.}
\label{fig:contribution}
\end{figure}

The learning behavior for the digit ``0'' is
depicted in Fig.~\ref{fig:contribution}(a).
The magnitude $\left\lvert\lambda_k\right\rvert\left\|\bm\xi_k\right\|^2$ for each $k$th
component of the model increases one by one as the learning process progresses.
The final number of principal components is 11 out of $K=50$.
The SPBM appears to gradually increase its (effective) rank
to represent the data distribution accurately,
keeping the rank as low as possible.
This result suggests that the gradient-ascent learning rule
of the SPBM naturally achieves low-rank learning.

Fig.~\ref{fig:contribution}(b) shows the gray-scale images of
principal components $\bm\xi_k$ with the five largest magnitude values
$\left\lvert\lambda_k\right\rvert\left\|\bm\xi_k\right\|^2$,
for which $\lambda_k$ is positive.
The digit shapes are vaguely embedded in $\bm\xi_k$,
which is understood intuitively because $\bm\sigma=\pm\sgn\bm\xi_k$
minimizes the $k$th component of the Hamiltonian if $\lambda_k>0$.
Note that the construction of the interaction matrix 
$J=\sum_k\lambda_k\bm\xi_k\bm\xi_k^\top$ has the same form as
the Hebbian learning for embedding patterns in
the Amari--Hopfield network \cite{Amari1972,Hopfield1982},
as mentioned in Ref.~\cite{Leonetti2021}.
Thus, $\bm\xi_k$ for $\lambda_k>0$ is analogous to
stored patterns in the associative memory of the Amari--Hopfield network.

Overall, the classification and sampling results suggest that
the SPBM with low-rank interactions is sufficient for learning MNIST digit images,
and the gradient-ascent learning rule naturally achieves low-rank learning.

\textit{Discussion.}%
---%
Since the SPBM handles lower-rank Ising problems more efficiently,
the ranks of the interaction matrices can be considered
as an index that characterizes a new aspect, to the best of our knowledge, 
for combinatorial optimization problems.
We have seen that the number partitioning problem
and the 0-1 knapsack problem with integer weights
are formulated as Ising problems with ranks one and two, respectively,
which are lowest-rank examples
of combinatorial optimization problems.
It is an interesting future direction to characterize
the types of real-world problems that can be formulated as
low-rank Ising problems.

The efficiency for low-rank Ising Hamiltonians
as well as inherent scalability with all-to-all interactions
is a unique feature of the SPBM that cannot be seen in other Ising machines.
For rank one, it has been demonstrated \cite{Prabhakar2023} that
the SPIM outperforms the Gurobi solver and a D-Wave quantum annealer
in solving large-scale number partitioning problems.
Thus, the SPBM is also expected to exhibit unique performance
for a specific class of problems, that is,
large-scale Ising problems with low-rank all-to-all interactions,
while we did not pursue scalability in our proof-of-concept experiments
in order to focus on the new computing model and learning ability
in this Letter. 
The efficiency can be further enhanced
by developing a method to reduce the rank of Ising problems,
admitting the number of spins to increase instead,
analogously to the studies \cite{Cai2014,Oku2019} on embedding
Ising problems in physical Ising machines with limited connectivity.

A necessity for solving low-rank Ising problems arises
when we consider the learning of combinatorial optimization problems \cite{Kitai2020,Wilson2021,Matsumori2022}.
For example, in a study on the automated design of metamaterials \cite{Kitai2020},
the authors train a factorization machine \cite{Rendle2010},
which has a low-rank quadratic form similar to the SPBM Hamiltonian,
and find low-energy candidates for metamaterials
using a D-Wave quantum annealer.
Here, the low-rank constraint contributes to the generalization ability,
which is essential for inferring the energy landscape
only from a small dataset available in a large configuration space. 
Thus, the SPBM should serve as an efficient sampling machine to find
low-energy candidates using a trained low-rank Ising Hamiltonian.

Despite the importance of low-rank modeling in data science,
there has been no study on low-rank learning of Boltzmann machines,
to the best of our knowledge.
The promising results of low-rank learning for the MNIST digit images
suggest the capabilities of the low-rank SPBM
as a statistical model with high parameter efficiency.
Examining the mechanism of low-rank learning
achieved naturally by the gradient-ascent rule is also intriguing.
We may be able to enhance
low-rank learning by introducing sparsity regularization.

Another unique feature of the SPBM is that the Hamiltonian
energy values can be computed in constant time,
independent of the number of variables.
When we run MCMC algorithms for sampling $\bm\sigma$ on ordinary computers,
it is crucial to generate a candidate state $\bm\sigma'$
such that the energy difference
$\Delta H=H(\bm\sigma')-H(\bm\sigma)$ from the current state $\bm\sigma$
is quickly available
because the computation time typically increases
depending on the difference between $\bm\sigma$ and $\bm\sigma'$.
By contrast, the SPBM enables us to choose $\bm\sigma'$ arbitrarily
without any loss of the computation speed,
although we still need to control the acceptance rate
to prevent the MCMC process from getting stuck.
As a simplest example, we have observed that multiple-spin flips
were effective for the knapsack problem.
We may extend this to designing new MCMC algorithms
that work efficiently on SPBMs,
possibly by exploiting the low-rank property \cite{Koehler2022}
and physical noise \cite{Pierangeli2020NP}.

To further improve the computation speed of the SPBM,
we can consider parallelization
because the computation of each component of the Hamiltonian is mutually independent.
Multiplexing the SPIM architecture by spatial or wavelength division
may drastically enhance computation speed.
Notably, a wavelength-division multiplexed architecture
was proposed recently \cite{Luo2023};
although it assumes full-rank interactions,
the SPBM model can be introduced to exploit the low-rank property
for, e.g., increasing the number of spins represented on the SLM.

The computing model of SPBM can be extended if
the Hamiltonian allows $\bm\sigma$ to take
nonbinary, intermediate continuous values
by modulating the amplitudes, not only the phases, of the light.
Then we can implement continuous-valued spin systems
with rich nonlinear dynamics
as in the coherent Ising machine (CIM) \cite{Inagaki2016,Leleu2019},
the simulated bifurcation machine (SBM) \cite{Goto2019},
the chaotic Boltzmann machine (CBM) \cite{Suzuki2013SR,Suzuki2013PRE},
the digital memcomputing machine \cite{Traversa2017,Bearden2020},
and continuous-time solver
for Boolean satisfiability (SAT) problems
\cite{Ercsey-Ravasz2011,Yamashita2020,Yamashita2021}.
Another approach for extending the SPBM Hamiltonian is to change
the target image $I_\text{T}$ from the original point-like one,
which introduces a convolution structure into the interaction matrix
(see Ref.~\cite{Pierangeli2019} for details).
Therefore, it is expected to efficiently handle combinatorial optimization problems
and statistical models with certain types of convolution structures.

In conclusion, we proposed the SPBM computing model for the SPIM architecture
that exhibits higher practical applicability
to various problems of combinatorial optimization and statistical learning
without losing the inherent scalability.
Notably, the proposed model has a unique affinity to problems
characterized by novel concepts of low-rank combinatorial optimization
and low-rank learning of Boltzmann machines.
These unexpected benefits of the SPIM architecture revealed in this study
are expected to contribute to the future development of non-von Neumann,
neuro-inspired computing.

\begin{acknowledgments}
This study was supported by JST CREST Grant Number JPMJCR18K2.
H.Y. and H.S. appreciate the support by
JST Moonshot R\&D Grant Number JPMJMS2021 and the WPI-IRCN at UTIAS.
\end{acknowledgments}

H.Y. and K.O. contributed equally to this work.

\bibliography{spbm.bib}

\clearpage\includepdf[pages={1,{}}]{spbm_sm.pdf}
\clearpage\includepdf[pages={2,{}}]{spbm_sm.pdf}
\clearpage\includepdf[pages={3,{}}]{spbm_sm.pdf}

\end{document}
