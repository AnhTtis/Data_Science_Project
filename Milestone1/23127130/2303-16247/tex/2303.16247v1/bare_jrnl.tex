\documentclass[11pt,journal]{IEEEtran}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
%\usepackage{times}
\usepackage{bm}
\usepackage{bbm}
% \newcommand{\theHalgorithm}{\arabic{algorithm}}
%\raggedbottom
\singlespacing

\usepackage{url}

\usepackage{wrapfig}

%
\usepackage[pdftex]{graphicx}


% \usepackage{subcaption}


% *** MATH PACKAGES ***
%
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithmic,algorithm}

\usepackage{gensymb}
\usepackage{soul}
\usepackage{color}
\usepackage{longtable}
\usepackage{booktabs}

\newcommand{\reasat}{\textcolor{red}}
\newcommand{\dss}{\textcolor{blue}}
\DeclareRobustCommand{\hlcyan}[1]{{\sethlcolor{cyan}\hl{#1}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\simil}{sim}
\newcommand{\one}[1]{\mathbbm{1}_{[#1]}}

\begin{document}

\title{Data Efficient Contrastive Learning in Histopatholgy using Active Sampling}

\author{Tahsin Reasat$^1$\footnote{\texttt{tahsin.reasat@vanderbilt.edu}} and David S.~Smith$^{1,2}$\\ \emph{\small 1. Vanderbilt University, Department of Electrical and Computer Engineering; 2. Vanderbilt University Medical Center, Department of Radiology and Radiological Sciences}}% <-this % stops a space

% make the title area
\maketitle


% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Deep Learning based diagnostics systems can provide accurate and robust quantitative analysis in digital pathology. Training these algorithms requires large amounts of annotated data which is impractical in pathology due to the high resolution of histopathological images. Hence, self-supervised methods have been proposed to learn features using ad-hoc pretext tasks. The self-supervised training process is time consuming and often leads to subpar feature representation due to a lack of constrain on the learnt feature space, particularly prominent under data imbalance. In this work, we propose to actively sample the training set using a handful of labels and a small proxy network, decreasing sample requirement by 93\% and training time by 99\%.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
% \iffalse
\begin{IEEEkeywords}
Digital Pathology, Deep Learning, Active Learning, Contrastive Learning, Self Supervised Learning
\end{IEEEkeywords}
% \fi
\section{Introduction}
% about self supervised learning
The recent renaissance of self-supervised learning (SSL) began with artificially designed pretext
tasks, such as relative patch prediction \cite{doersch2015unsupervised}, solving jigsaw puzzles \cite{noroozi2016unsupervised}, colorization \cite{zhang2016colorful} and rotation prediction \cite{gidaris2018unsupervised}. 
Recently, contrastive learning (CL) has emerged as a simple yet promising tool in SSL \cite{chen2020simple}. 
It has been successfully applied to cancer detection, segmentation in histopathology images \cite{ciga2022self}. 
% Good results can be obtained using big networks and longer training time \cite{kolesnikov2019revisiting}. 
% Augmentations somewhat ad-hoc heuristics, which limits the generality of learned representations.
In practice, the CL model model is trained using a large unlabeled data pool, and the extracted features from this large model are used to train a smaller linear model, and its prediction is evaluated on a test set to measure its performance (\textit{linear evaluation}) \cite{chen2020simple}. 

The training phase is slow and learned latent space is unconstrained, which could be suboptimal for histopathological datasets. Histopathological datasets often have a lot of confounding image sections consisting of background and debris alongside normal tissue area. Additionally, the tissue of interest is normally the cancerous tumor area which is often a minority class.  As a result the model either takes too many iterations to learn the minority class examples or, even worse, ignores the minority class and uses all its capacity to learn the non-tumor cells. We propose to solve these inefficiencies by borrowing techniques from \textit{active learning}.  


Active learning  \cite{cohn1994improving,krogh1995neural} allows humans to interactively improve algorithms. The algorithm is trained iteratively using an algorithmically sampled data subset (\textit{active sampling}) which is labeled by an oracle (e.g., human annotator). In each iteration,  new informative subset of samples is chosen to further train and improve model predictions Fig. \ref{fig:al_loop}. Multiple datasets have been produced by user refined inputs such as automatic teller machine, self-driving cars, automatic content tagging in online platforms \cite{lecun2015deep}. Active learning has proven useful in annotating data when specialist input is required, e.g. medical images \cite{budd2019survey}. 

To decrease data redundancy, we propose to train the network iteratively using batches of unlabeled data.
The first batch is randomly selected and after training the model on each batch, we select the next batch of data from the unlabeled pool using a proxy model and a sample selection strategy and append it to the current batch. 
% \reasat{(plug in some similar work in supervised domain, get it from joshi)}
Our work is related to \cite{joshi2023data} where the latent class centers are approximated utilizing an external pretrained model CLIP \cite{radford2021learning} and chooses a subset of sample which lies near the cluster centers. We do not use such a pretrained model. Rather we use our CL model features for informative sample selection.
% it is trained on natural images and not fit for fine-grained classification such as histopathological images \cite{radford2021learning}.
Researchers in \cite{coleman2019selection} used a small proxy network to select samples in an active learning setup. But they consider active learning in the supervised domain (they require an oracle for every iteration of the active learning loop). 
Our work can be considered as an extension of their framework to the self supervised domain.
Furthermore, this is the first work that considers data efficiency in CL for the domain of histopathology images (be specific how this helps to make it novel, does the algorithm change, can we include some domain related tricks, etc.). The contributions of our work are to 
\begin{itemize}
    \item increase CL efficiency in terms of time and sample complexity,
    \item provide a method to constrain the model into learning relevant representations by using active sampling, and
    \item improve CL efficiency in the histopathology domain.
\end{itemize}
% Essential Subsets for Supervised Learning. There has
% been a recent body of efforts on finding the most important
% subsets for supervised learning. Empirical methods commonly rank examples from easiest to hardest—based on
% confidence, loss or gradient—and curate subsets preserving
% the hardest examples. Coleman et al. (2020) used a smaller
% trained proxy model to find the most uncertain examples to
% train a larger model. Toneva et al. (2019) selects examples
% with highest forgetting score, i.e. the number of times they
% transition from being classified correctly to incorrectly during training. Swayamdipta et al. (2020) selects examples
% with the highest variance of predictions during training. Paul
% et al. (2021) selected examples with the lowest expected gradient norm over multiple initializations. More theoretically
% motivated approaches iteratively select subsets by importance sampling based on gradient norm (Katharopoulos &
% Fleuret, 2018) or select weighted subset of examples which
% closely capture the full gradient (Mirzasoleiman et al., 2020;
% Pooladzandi et al., 2022; Killamsetty et al., 2021).
\begin{figure}
    \centering
    \includegraphics[width=0.47\textwidth]{fig/al_loop.png}
    \caption{Active learning loop \cite{settles2009active}. An oracle annotates the most informative samples which is used to refine model predictions.}
    \label{fig:al_loop}
\end{figure}
% In the active learning setup, the steps involve (Fig. \ref{fig:al_loop})
% \begin{itemize}
%     \item An unlabeled pool of data
%     \item A query selection algorithm
%     \item An oracle
%     \item A labeled training set
%     \item A machine learning model
% \end{itemize}
% Amongst these elements the query selection method has a key role in the performance of the active learning model.
\section{Method}
\begin{figure}
\centering
  \includegraphics[width=0.47\textwidth]{fig/SimCLR.png}
  \caption{SimCLR framework introduced in \cite{chen2020simple}. The model minimizes the distance (maximizes agreement) between feature representation of two augmented views $\tilde{x}_i$ and $\tilde{x}_j$ of the same image.}
  \label{fig_simclr}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.47\textwidth]{fig/proposed_framework.drawio.png}
    \caption{The proposed framework speeds up the contrastive sampling process by actively selecting informative samples with the help of a small proxy network.}
    \label{fig:proposed_framework}
\end{figure}
In this section, we describe the components in our proposed framework and formalize the methodology.
\subsection{Contrastive Learning}

SimCLR \cite{chen2020simple} proposed a simple framework for contrastive learning of visual representations by maximizing agreement between differently augmented views of the same sample via a contrastive loss in the latent space.
%Details from https://lilianweng.github.io/posts/2021-05-31-contrastive/#vision-image-embedding
Consider an unlabeled image pool $U$ and $x$ are the image samples $x\in U$. In CL, a minibatch of $N$ samples are randomly sampled and each sample is applied with random  data augmentation operations, resulting in $N$ pairs of augmented samples or $2N$ augmented samples in total:
\begin{align}
    \tilde{x}_i = t(x), \tilde{x}_j = t'(x), t,t' \sim \mathcal{T},
\end{align}
where two separate data augmentation operators, $t$ and $t'$, are sampled from the same family of augmentations $\mathcal{T}$.
Given one positive pair, other  $2(N-1)$ data points are treated as negative samples. The representation is produced by a base encoder $f(\cdot)$:
\begin{align}
    h_i = f(\tilde{x}_i), h_j = f(\tilde{x}_j).
\end{align}
The encoded representation is passed through a projection head $g(\cdot)$ which is a series of three fully connected layers. 
Experiments show that a projection head improves the quality of the encoded features $h$ \cite{chen2020simple}:
\begin{align*}
        z_i &= g(h), z_j = g(h_j).
\end{align*}
The CL loss $\mathcal{L}^{i,j}$ is defined using cosine similarity on the output of the projection head: 
\begin{align*}
    % \exp(sim(z_i, z_j))\tau \\
    \mathcal{L}^{i,j} &= -\log \frac{\exp\left[\simil(z_i, z_j)/\tau\right]}{\sum_{k=1}^{2N} (1 - \delta_{ik}) \exp\left[\simil(z_i, z_k)/\tau\right]},
\end{align*}
% \begin{equation}
% \label{eq:loss}
%     \ell_{i,j} = -\log \frac{\exp(\mathrm{sim}(\bm z_i, \bm z_j)/\tau)}{\sum_{k=1}^{2N} \one{k \neq i}\exp(\mathrm{sim}(\bm z_i, \bm z_k)/\tau)}~,
% \end{equation}
where $\delta_{ik}$ is the Kronecker delta function which evaluates to 1 if $i=k$  and 0 otherwise.
Note that, after training is completed, we throw away the projection head $g(\cdot)$ and
use encoder $f(\cdot)$ and representation $h$ to train a proxy model.

\subsection{Proxy Model}
The proxy model $f_p(\cdot)$ is a neural network with a single fully connected layer. We extract image features using the CL model encoder and train the proxy model on these features. The output of the proxy model is used to select informative samples from the unlabeled image pool.
\subsection{Active Sampling}
We extract the image features  using the CL model and use active sampling strategies to select most informative samples. For this paper we use uncertainty sampling and coreset sampling.

\subsubsection{Uncertainty Sampling.}
Uncertainty sampling is the most common sampling criteria in which an active learner selects the instances about which it is least certain how to label. The uncertain samples normally reside near the class decision boundaries. We use entropy \cite{shannon1948mathematical}, the most common measure of uncertainty: 
\begin{align}
    H(x_i)\equiv \sum_{j}{P(y_j\mid x_i;\theta)\log P(y_j\mid x_i;\theta)},
     \label{equ:entropy}
\end{align}
where $y_j$ ranges over all possible labels and $x_i$ is data. Entropy is an information-theoretic
measure that quantifies information content.

\subsubsection{Coreset Sampling.}
Coreset formulates the active sampling problem as choosing a subset of samples that spans the dataset \cite{sener2017active}. The subset selection problem is formulated as a k-center problem using a greedy approximation shown in Algorithm \ref{alg:greedy}.
% using a greedy approach shown in  Algorithm~\ref{alg:greedy}. If $OPT=\min_{\mathbf{s}^1} \max_i \min_{j \in
% \mathbf{s}^1 \cup \mathbf{s}^0} \Delta(\mathbf{x}_i,\mathbf{x}_j)$, the greedy algorithm shown in
% Algorithm~\ref{alg:greedy} is proven to have a solution ($\mathbf{s}^1$) such that; $ \max_i \min_{j \in \mathbf{s}^1
% \cup \mathbf{s}^0} \Delta(\mathbf{x}_i,\mathbf{x}_j) \leq 2 \times OPT$.
% \begin{wrapfigure}{R}{0pt}
% \begin{minipage}{0.44\textwidth} 
% \vspace{-8mm}
\begin{algorithm}[h] 
   \caption{k-Center Greedy  \cite{sener2017active}} 
   \label{alg:greedy} 
   \begin{algorithmic} 
   \STATE {\bfseries Input:} data $\mathbf{x}_i$, existing unlabeled image pool $\mathbf{s}^0$ and a budget $b$ 
   \STATE Initialize $\mathbf{s}=\mathbf{s}^0$ \REPEAT \STATE $u=\arg\max_{i \in [n] \setminus \mathbf{s}} \min_{j \in \mathbf{s}} \Delta(\mathbf{x}_i, \mathbf{x}_j)$ \STATE $\mathbf{s} = \mathbf{s} \cup \{u\}$ 
   \UNTIL {$|\mathbf{s}|=b+|\mathbf{s}^0|$} 
   \STATE {\bfseries return} $\mathbf{s} \setminus \mathbf{s}^0$ \end{algorithmic}
\end{algorithm} 
% \vspace{-10mm}
% \end{minipage} 
% \end{wrapfigure}  

Uncertainty sampling focuses on samples in the decision boundary, while coreset focuses on sample diversity. These are the two most common sampling methods used in the literature. A complete list of existing strategies can be found in \cite{settles2009active}.
%There are others like Query-By-Committee, Expected Model Change, Variance Reduction and Fisher Information Ratio, Estimated Error Reduction, Density-Weighted Methods, which are not exlpored in this paper. \textbf{cite}
% Also, BALD (Houlsby et al., 2011) has been successful in
% deep learning (Gal et al., 2017; Shen et al., 2017) but is restricted to Bayesian neural networks or
% networks with dropout (Srivastava et al., 2014) as an approximation (Gal & Ghahramani, 2016).

\subsection{Proposed Framework}
\label{method_description}
The proposed framework is depicted in Fig. \ref{fig:proposed_framework}.
Let $U^{t}$ be the unlabeled image pool at iteration $t$, $S_L$ is a small set of images with labels and $S_\mathrm{test}$ is the test set. 
We sample a initial random pool of images $S^{t} \subset U^{t}$ of budget $b$ and
update the unlabeled image pool, $U^t \leftarrow U^t \setminus S^t$. The subset $S^t$ is used to train the SimCLR model.
%$g \circ f$.
% ($f(.)$ is the encoder portion of the model and $f_h$ is the projection head). 
We freeze the encoder $f(\cdot)$ and extract features $h_i$, for $x_i \in S_L$. 
A proxy model $f_p^{t}$ was trained using features $h_i$. This proxy model was used to find the next subset of images $S^{t+1}$ to be included in the CL training set, $S^{t} \leftarrow S^{t} \cup S^{t+1}$.
If uncertainty is used as sampling criterion, the entropy $H(x_i)$ is computed using outputs of $f_p^t(\cdot)$ and we select the top $b$ samples with highest uncertainty. 
If coreset is used, we select $b$ samples following the k-center greedy algorithm presented in Algorithm \ref{alg:greedy}.
This iterative process is continued for a given number of iterations $T$. 
Note that the weights of the SimCLR model are randomly initialized in the first iteration and training continues in the subsequent iterations using the learned weights in the previous iteration. The proxy network, however, is learned from random weights in each iteration.
% i.e., $S^{t+1} = \argmax_{A \subset U \setminus S^t, |A| = N_s} \sum_{a \in A} a$.
The set $S^t \cup S^{t+1}$ is used for the next iteration of CL training. This process continues for a set number of iterations $T$. The pseudocode for the algorithm of our proposed framework is presented in Algorithm \ref{alg:proposed}.

\begin{algorithm}[t]
\caption{Contrastive Learning with Active Sampling}
\label{alg:proposed}
\begin{algorithmic}[1]

\STATE \textbf{Input:} 
% Initial unlabeled image pool $U^{0}$,
% Initial subset for training $S^{0}$,
% Small labeled subset 
$S_L$,
% Unlabeled image pool at iteration $t$, 
$U^{t}$,
% SimCLR model 
$g\circ f$
% proxy model trained at iteration $t$, 
$f_{p}^{t}$, 
% No. of samples selected at each iteration
% $N_s$,
% No. of iterations 
$T$
\STATE \textbf{Output:} trained encoder $f(.)$
\STATE \textbf{Initialize:}\\
$S^{0} \leftarrow \text{Random sampling on} ~U^{0}$
% $U^{0} \leftarrow U^{0}\setminus S^{0}$

\STATE \FOR{$t=0$ to $T-1$}
\STATE Train $g\circ f$ using $S^t$
% \STATE $z \leftarrow f_{e}(x), x \in U^{t}$
\STATE Train $f_{p}^{t}$ using $S_l$
\STATE $U^{t} \leftarrow U^{t} \setminus S^{t}$
% \STATE $H\leftarrow \{h(f(x),f_p^{t}), x \in U^{t}\}$
\STATE $S^{t+1} \leftarrow \text{Active Sampling on}  ~U^{t}$
% \STATE \textbf{struggling here}$S^{t+1} \leftarrow \argmax_{A\subset U^t, |A| = |N_s|} \sum_{x\in A, h \in H} h$% argsort(H) in a descending order and corresponding first Ns x samples from U
\STATE $S^{t} \leftarrow S^{t} \cup S^{t+1}$

\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Experiment and Results}
\subsection{Dataset}
% rephrase
We used the Kather-19 dataset presented in \cite{kather_jakob_nikolas_2018_1214456}. The train set consists of $100,000$ non-overlapping image patches from hematoxylin and eosin (H\&E) stained histological images of human colorectal cancer (CRC) and normal tissue. 
Tissue classes are: adipose (ADI), background (BACK), debris (DEB), lymphocytes (LYM), mucus (MUC), smooth muscle (MUS), normal colon mucosa (NORM), cancer-associated stroma (STR), colorectal adenocarcinoma epithelium (TUM).
These images were manually extracted from 86 H\&E stained human cancer tissue slides from formalin-fixed paraffin-embedded (FFPE) samples from the NCT Biobank (National Center for Tumor Diseases, Heidelberg, Germany) and the UMM pathology archive (University Medical Center Mannheim, Mannheim, Germany). Tissue samples contained CRC primary tumor slides and tumor tissue from CRC liver metastases; normal tissue classes were augmented with non-tumorous regions from gastrectomy specimen to increase variability. The test set consists of $7180$ image patches from $50$ patients with colorectal adenocarcinoma (no overlap with patients in train set) collected from NCT tissue bank. Both the train and test set patches are $224\times 224$ pixels at $0.5$ microns per pixel. All images are color-normalized using Macenko's method \cite{macenko_norm}.
We created an imbalanced dataset by setting the TUM tissues as the positive class and combining all the non-tumor (NON-TUM) as negative class. The train set had $14,317$ TUM patches and $85,683$ NON-TUM patches. While the the test set had $1233$ TUM patches and $5947$ NON-TUM patches.
% \subsection{Performance}
\subsection{Experimental Details}
The SimCLR model was trained using $224 \times 224$ input images. To create two views of the same image the following augmentations were used: color jitter, random crop, random grayscale, Gaussian blur, horizontal and vertical flip, random rotation by $\pm90$ deg. The image intensities were divided by $255$ and normalized by the ImageNet mean and standard deviation.
% color jitter probability 0.8, color jitter strength 0.5,    bright=color jitter strength * 0.8,
%             contrast=strength * 0.8,
%             sat=strength * 0.8,
%             hue=strength * 0.2, Minimum size of the randomized crop relative to the input size 0.08, random gray scale probability 0.2, gaussian blur float 0.5, kernel size  float 0.1, horizontal flip probability 0.5, random rotation by +90,-90 probability 0.5,  + vertical flip probability 0.5  
%             The image intensities were divided by 255. Then imagenet image mean (0.485, 0.456, 0.406) is substracted from the rgb channels and divided by the standard deviation (0.229, 0.224, 0.225).
% For MoCo, SimCLR without gaussian blur. 

The training set is randomly split into two sets: an unlabeled pool $U$ of $99,000$ images and $1000$ to train a proxy model (randomly sampled from the full train set). The original test set of size 7180 is used to evaluate the proxy model. The size of the subset sampled at each iteration for SSL is 1000, and the number of iterations, $T$, is 20.  The CL model $f_{\theta}$ was a Resnet18 encoder \cite{he2016deep} connected to a projection head that consisted of 3 fully connected layers with 128 units, the proxy model $f_p$ was a linear layer with 512 units. Note that at each iteration, active sampling is done on a randomly selected subset of 10,000 to reduce computation and data redundancy. 
As the benchmark, we trained the SimCLR model for 100 epochs using the full unlabeled pool $U$ and evaluated its feature quality by training the proxy model on the small labeled dataset. The proxy model was trained for 200 epochs, and its performance was evaluated at 20 epoch intervals on the test set. Then we trained the SimCLR model following our proposed framework using a subset of images. We considered uncertainty, coreset sampling as our active sampling methods. Also, random sampling of subsets was included as an ablation study to observe the absence of active sampling. At each iteration we trained the SimCLR model for 100 epochs and the proxy model for 40 epochs. The performance of the proxy model was evaluated at the end of each iteration. For both the experiments, the SimCLR model was trained with a learning rate of 0.0001 and the proxy model with a learning rate of 0.001. The loss used for SimCLR was normalized cross-entropy with temperature 0.1,  %following suggestions from \cite{ciga2022self}
and for the classifier we used binary cross-entropy loss. For both models we used the Adam optimizer \cite{kingma2014adam} with $\beta_1=0.9$, $\beta_2=0.999$. The training batch size is 128. Each experiment was run three times, and the average performance is reported.
% Quadro RTX 6000 
\subsection{Metrics}
The evaluation metric used is the F1 score which accounts for the imbalance in the dataset. The F1 score is the harmonic average of precision (P) and recall (R). 
\begin{align}
    F1 = \frac{2\mathrm{RP}}{\mathrm{R}+\mathrm{P}};
    R = \frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}},
    P = \frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}
\end{align}
Here, TP is true positive, FN is false negative, FP is false positive. F1 score is chosen due to the imbalance in the positive and negative classes in the images. 
We also computed the average run time for the benchmark and proposed method to reach a certain F1 score.
\subsection{Results}
\label{sec_result}
For the benchmark experiment we observed a highest F1 of 0.84. To reach similar score the uncertainty and coreset based sampling strategy takes 7000 and 11,000 samples respectively (Fig. \ref{fig:f1_comparison}). Thus sample requirement is decreased by 93\% and 89\% respectively. Also, we observed that random sampling performs worse compared to the active sampling methods.
% (622.5-228)/622 = 0.9192
% (99000-13000)/99000 = 0.8687
The benchmark achieved its highest score at  100th epoch of SimCLR training and at the 150th epoch of the proxy model training.  The average runtime for one epoch of SimCLR training is 6 min and for the proxy model training is 0.15 min. This resulted in a total average runtime of 622.5 min.
%6x100+0.15x150 = 622.5 min.
% (6+13+17+22+26+34+42+48) = 208
% avg 7 iteration runtime
% 160/60 + 6x2/60 + 0.15x40 = 8.9 min. 
% Time reduction (622.5-8.9)/622.5  = 0.99

To reach similar scores, the uncertainty based active sampling experiment required 7 iterations (7000 samples). Since the data to train SimCLR doubled at each iteration, the time per epoch also increased. The average time to train the model for 8 iterations was 160 s. Average time to execute the uncertainty computation was around 2 s. This resulted in a total average runtime of 8.9 min.
% (6+13+17+22+26+34+42+48+52+57+62+68+74)
% average 12 iteration runtime
% 379/60 + 11x10/60 + 0.15*40 = 14.15
% Time reduction (622.5-8.9)/622.5  = 0.98
Similarly,  k-center based sampling experiment required 11 iterations (11,000 samples). Average time to execute 11 iterations was 379 s. Average time to execute the coreset selection computation was around 10 s. This resulted in a total average runtime of 
14.2 min. Thus due to uncertainty and coreset sampling the time requirement is decreased by 99\% and 98\% respectively. The runtime comparison is summarized in Table \ref{tab:runtime}.
% Time reduction (622.5-464)/622 = 0.15
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}

\begin{figure}
\centering
  \includegraphics[width=0.47\textwidth]{fig/f1_comparison.eps}
  \caption{Comparison of sampling strategies. The active sampling methods (uncertainty and coreset) requires less samples to reach the CL model trained on full set of images (benchmark).} 
  \label{fig:f1_comparison}
\end{figure}
% Please add the following required packages to your document preamble:
% Note: It may be necessary to compile the document several times to get a multi-page table to line up properly
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[h]
\caption{Runtime comparison}
\label{tab:runtime}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrr}
\hline
Method                 & \begin{tabular}[c]{@{}c@{}}Avg Runtime\\ (min)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Time Reduction\\ \%\end{tabular} \\ \hline
Benchmark              & 622.5                                                       & NA                                                          \\ 
Uncertainty & 8.9                                                        & 99                                                      \\ 
Coreset     & 14.2                                                     & 98                                                       \\ \hline
\end{tabular}%
}
\end{table}
% \begin{table}[]
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{cccccc}
% \hline
% Method                                       & \begin{tabular}[c]{@{}c@{}}Avg. SimCLR  \\ Runtime (min)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg.  Proxy \\ Runtime (min)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. Active \\ Sampling Runtime\\ (min)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Total \\ (min)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Time Reduction\\ \%\end{tabular} \\ \hline
% \multicolumn{1}{|c|}{Benchmarl}              & \multicolumn{1}{c|}{600}                                              & \multicolumn{1}{c|}{22.5}                                            & \multicolumn{1}{c|}{0}                                                          & \multicolumn{1}{c|}{622.5}                             & \multicolumn{1}{c|}{NA}                                     \\ \hline
% \multicolumn{1}{|c|}{Proposed - Uncertainty} & \multicolumn{1}{c|}{208}                                              & \multicolumn{1}{c|}{6}                                               & \multicolumn{1}{c|}{0.23}                                                       & \multicolumn{1}{c|}{214}                               & \multicolumn{1}{c|}{63.42}                                  \\ \hline
% Proposed - Coreset                           & 521                                                                   & 0.16                                                                 & 2.17                                                                            & 527                                                    & 15                                                          \\ \hline
% \end{tabular}%
% }
% \end{table}

\section{Conclusion}
In this paper we have demonstrated that active learning strategies can be used in CL to speed up training using less data. Also, using the uncertainty sampling we have proposed a feedback system for the contrastive learning framework to select the subset with the most amount of information related to the downstream task. We showed for histopathological cancer classification, we can achieve a speedup of 99\% while using 93\% less data. This method requires only a small labeled set. Future work could explore whether this number can be reduced and how that would affect contrastive training. Also, this method could be extended to other tasks such as regression or segmentation. 
\label{sec_conclusion}


%Determining mutation status from a histological image and bypassing additional testing is important in lung cancer in particular, as these mutations often carry prognostic as well as predictive information. 
% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\bibliographystyle{unsrt}
\bibliography{references}
\end{document}


