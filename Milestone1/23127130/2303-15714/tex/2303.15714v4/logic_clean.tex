
\documentclass{article}

\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{sidecap}
\usepackage[small]{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amsfonts}     
\allowdisplaybreaks
\usepackage{amsthm}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}
\usepackage{thmtools}
\usepackage{thm-restate}
\declaretheorem[name=Theorem]{theorem}
\declaretheorem[name=Lemma]{lemma}




\usepackage{multicol}
\usepackage{pifont}
\usepackage{xspace}
\newcommand{\circone}{\ding{172}\xspace}
\newcommand{\circtwo}{\ding{173}\xspace}
\newcommand{\circthree}{\ding{174}\xspace}
\newcommand{\circfour}{\ding{175}\xspace}

\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\newcommand{\angles}[1]{\langle #1 \rangle}

\newcommand{\blambda}{\bar{\lambda}}

\usepackage[round]{natbib}

\usepackage{appendix}

\newcommand{\rightcomment}[1]{\(\triangleright\) {\small \it #1}}
\newcommand{\eqcomment}[1]{\addtocounter{equation}{1}\tag*{\rightcomment{#1}\quad(\theequation)}}
\usepackage{suffix}
\WithSuffix\newcommand\eqcomment*[1]{\tag*{\rightcomment{#1}}}
\usepackage{setspace}
\usepackage[hidelinks]{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{mdframed}


\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}  %
\usepackage[disable]{todonotes}
\newcommand{\fixme}[2][]{{\todo[color=yellow,size=\scriptsize,fancyline,caption={},#1]{#2}}}
\newcommand{\note}[4][]{{\todo[author=#2,color=#3,size=\scriptsize,fancyline,caption={},#1]{#4}}}
\newcommand{\hongyuan}[2][]{{\note[#1]{hongyuan}{green!20}{#2}}}
\newcommand{\mo}[2][]{{\note[#1]{mo}{orange!20}{#2}}}
\newcommand{\hongyu}[2][]{{\note[#1]{hongyu}{blue!20}{#2}}}
\newcommand{\kangrui}[2][]{{\note[#1]{kangrui}{yellow!20}{#2}}}

\newcommand{\notewho}[3][]{{\note[#1]{#2}{orange!40}{#3}}}
\newcommand{\Fixme}[2][]{\fixme[inline,#1]{#2}\noindent}
\newcommand{\Notewho}[3][]{\notewho[inline,#1]{#2}{#3}\noindent}
\newcommand{\Hongyuan}[2][]{\hongyuan[inline,#1]{#2}\noindent}
\newcommand{\Mo}[2][]{\mo[inline,#1]{#2}\noindent}
\newcommand{\Hongyu}[2][]{\hongyu[inline,#1]{#2}\noindent}
\newcommand{\Kangrui}[2][]{\kangrui[inline,#1]{#2}\noindent}
\newcommand{\response}[1]{\vspace{3pt}\hrule\vspace{3pt}\textbf{#1:}}
\newcommand{\rebuttal}[1]{\cbstart {#1} \cbend}
\newcommand{\cutforspace}[1]{}


\usepackage{bm}
\let\vecarrow\vec    %
\renewcommand{\vec}[1]{{\boldsymbol{\mathbf{#1}}}}   %
\newcommand{\vx}{\vec{x}}
\newcommand{\ve}{\vec{e}}

\usepackage{pifont}
\usepackage{xspace}

\usepackage{emnlp2023custom}




\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\renewcommand{\algorithmicindent}{9pt}
\renewcommand\algorithmicdo{:}
\renewcommand\algorithmicthen{:}
\algnewcommand{\IfThen}[2]{\State \algorithmicif\ #1\ \algorithmicthen\ #2}
\algnewcommand{\IfThenElse}[3]{\State \algorithmicif\ #1\ \algorithmicthen\ #2\ \algorithmicelse\ #3}
\algrenewcommand{\algorithmiccomment}[1]{\hfill \rightcomment{#1}}
\algnewcommand{\LineComment}[1]{\State \rightcomment{#1}}
\algnewcommand{\LinesComment}[1]{\State \rightcomment{\parbox[t]{\linewidth-\leftmargin-\widthof{\(\triangleright\) }}{#1}}\smallskip}
\algnewcommand\algorithmichyperparam{{\bfseries Hyperparam:}}
\algnewcommand\HYPER{\item[\algorithmichyperparam]}
\algnewcommand\algorithmicinput{{\bfseries Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\algorithmicoutput{{\bfseries Output:}}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}
\makeatletter
\newcounter{algorithmicH}
\let\oldalgorithmic\algorithmic
\renewcommand{\algorithmic}{%
  \stepcounter{algorithmicH}
  \oldalgorithmic}
\renewcommand{\theHALG@line}{ALG@line.\thealgorithmicH.\arabic{ALG@line}}
\makeatother
\makeatletter
\newcommand{\algmargin}{\the\ALG@thistlm}
\makeatother
\algnewcommand{\Statepar}[1]{\State\parbox[t]{\dimexpr\linewidth-\algmargin}{\strut #1\strut}}

\newcommand{\pluseq}{\mathrel{+\!\!=}}
\newcommand{\minuseq}{\mathrel{-\!\!=}}

\usepackage{tcolorbox}
\newtcolorbox{examplebox}[1][]{%
  colback=white, 
  colframe=blue!75!black, 
  fonttitle=\bfseries,
  title=Example:,
  #1
}

\AtBeginDocument{%
  \addtolength\abovedisplayskip{-0.25\baselineskip}%
  \addtolength\belowdisplayskip{-0.25\baselineskip}%
  \addtolength\abovedisplayshortskip{-0.25\baselineskip}%
  \addtolength\belowdisplayshortskip{-0.25\baselineskip}%
}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{mathtools, cuted}
\usepackage{tabularx, booktabs}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\newcolumntype{R}{>{\raggedleft\arraybackslash}X}
\newcolumntype{S}{>{\raggedleft\arraybackslash\hsize=.5\hsize}X}
\usepackage{latexsym}
\usepackage{url}
\usepackage{xspace}
\usepackage{bm,array}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{cases}
\usepackage{mathtools}
\usepackage{empheq}
\usepackage{bm}
\usepackage{bbm}
\usepackage{esvect}

\newcommand{\codefont}{\fontfamily{lmtt}\selectfont}
\usepackage{listings}
\usepackage{parcolumns}
\lstdefinestyle{datalogstyle}{
	basicstyle={\codefont\small},  %
	xleftmargin={6pt},
        columns=flexible,
        breakindent=0pt,
        breaklines=true, 
	frame=tb,
	stepnumber=1,
	firstnumber=1,
	numberfirstline=true,
	tabsize=2,
	extendedchars=true,
	breaklines=true,
	columns=fullflexible,
	keepspaces=true,
	escapeinside={@}{@},
	firstnumber=last,
	captionpos=b, 
	commentstyle=\color{black!65},
	numberstyle=\tiny\color{black!65},
	stringstyle=\color{codepurple},
	breakatwhitespace=false, 
	keepspaces=true,              
        mathescape=true, 
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,
	aboveskip={0.8\baselineskip},
	belowskip={0.2\baselineskip},
}
\lstset{style=datalogstyle}
\newcommand{\progvdots}{\\$\vdots$}
\usepackage[noabbrev,capitalize]{cleveref} %
\newcommand{\crefrangeconjunction}{--}     %
\crefname{equation}{equation}{equations}   %
\crefname{section}{section}{sections}      %
\crefname{footnote}{footnote}{footnotes}   
\crefname{lstlsting}{listing}{listings}   
\crefname{lstlsting}{Listing}{Listings}   
\crefname{assumption}{assumption}{assumptions}
\crefname{line}{line}{lines}   %





\let\frac=\tfrac  %


\renewcommand{\vec}[1]{{\boldsymbol{\mathbf{#1}}}}   %
\newcommand{\vecimath}{\textbf{\i}}
\newcommand{\defn}[1]{\textbf{#1}}
\newcommand{\defeq}{\mathrel{\stackrel{\textnormal{\tiny def}}{=}}}
\newcommand{\xpct}{\mathbb{E}}
\newcommand{\E}[2][]{\xpct_{{#1}}\left[#2\right]}
\newcommand{\var}{\mathbb{V}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Int}{\mathbb{Z}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\Uniform}{\mathrm{Unif}}
\newcommand{\Exp}{\mathrm{Exp}}
\newcommand{\Normal}{\mathrm{Normal}}
\newcommand{\Categorical}{\mathrm{Categorical}}
\renewcommand{\th}{\textsuperscript{th}\xspace}
\newcommand{\bos}{\textsc{bos}\xspace}
\newcommand{\eos}{\textsc{eos}\xspace}
\newcommand{\eop}{\textsc{eop}\xspace}
\newcommand{\inv}[1]{#1^{\scriptscriptstyle-\!1}}
\newcommand{\set}[1]{\mathcal{#1}}


\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\newcommand{\orangecircle}{\begin{tikzpicture} \draw [fill=orange, thick] (0,0) circle (0.1); \end{tikzpicture}\xspace}
\newcommand{\greendiamond}{\begin{tikzpicture} \draw [fill=Green, thick, rotate=45] (0,0) rectangle (0.15, 0.15); \end{tikzpicture}\xspace}
\newcommand{\purplesquare}{\begin{tikzpicture} \draw [fill=MediumOrchid, thick] (0,0) rectangle (0.17, 0.17); \end{tikzpicture}\xspace}
\newcommand{\bluehexagon}{\begin{tikzpicture} \path node[regular polygon, regular polygon sides=6, fill=cyan, draw, thick, scale=0.7] (hexagon) {}; \end{tikzpicture}\xspace}
\newcommand{\redsolid}{\begin{tikzpicture} \draw [Red, ultra thick] (0,1) -- (0.5,1); \end{tikzpicture}\xspace}
\newcommand{\reddash}{\begin{tikzpicture} \draw [Red, dashed, ultra thick] (0,1) -- (0.5,1); \end{tikzpicture}\xspace}
\newcommand{\bluesolid}{\begin{tikzpicture} \draw [Blue, ultra thick] (0,1) -- (0.5,1); \end{tikzpicture}\xspace}
\newcommand{\bluedash}{\begin{tikzpicture} \draw [Blue, dashed, ultra thick] (0,1) -- (0.5,1); \end{tikzpicture}\xspace}
\newcommand{\greensolid}{\begin{tikzpicture} \draw [Green, ultra thick] (0,1) -- (0.5,1); \end{tikzpicture}\xspace}
\newcommand{\greendash}{\begin{tikzpicture} \draw [Green, dashed, ultra thick] (0,1) -- (0.5,1); \end{tikzpicture}\xspace}
\newcommand{\purplesolid}{\begin{tikzpicture} \draw [MediumOrchid, ultra thick] (0,1) -- (0.5,1); \end{tikzpicture}\xspace}
\newcommand{\purpledash}{\begin{tikzpicture} \draw [MediumOrchid, dashed, ultra thick] (0,1) -- (0.5,1); \end{tikzpicture}\xspace}
\newcommand{\greendiamondfaint}{\begin{tikzpicture} \draw [fill=Green, opacity=0.3, thick, rotate=45] (0,0) rectangle (0.15, 0.15); \end{tikzpicture}\xspace}
\newcommand{\bluedot}{\begin{tikzpicture} \draw [fill=blue] (0,0) circle (0.1); \end{tikzpicture}\xspace}
\newcommand{\reddot}{\begin{tikzpicture} \path node[regular polygon, regular polygon sides=3, fill=red, draw, red, thick, scale=0.5] (hexagon) {}; \end{tikzpicture}\xspace}
\usetikzlibrary{arrows,decorations.markings}
\usetikzlibrary{arrows}
\newcommand{\blueline}{\begin{tikzpicture} \draw[arrows={-angle 60}, white, thick, rotate=180, opacity=1.0] (0,0.00) -- (0.5,0.00); \draw[arrows={-angle 60}, blue, thick, rotate=180] (0,-0.06) -- (0.5,-0.06); \end{tikzpicture}\xspace}
\newcommand{\redline}{\begin{tikzpicture} \draw[dashed, arrows={-angle 60}, white, thick, rotate=180, opacity=1.0] (0,0.00) -- (0.5,0.00); \draw[dashed, arrows={-angle 60}, red, thick, rotate=180] (0,-0.06) -- (0.5,-0.06); \end{tikzpicture}\xspace}


\usepackage{verbatim}


\newcommand{\name}{LEAP\xspace}





\begin{document}

\title{Explicit Planning Helps Language Models in Logical Reasoning}

\author{
 Hongyu Zhao\thanks{\ \ Work done during internship at TTI-Chicago.}$\,\,^{1,3}$\quad Kangrui Wang$^{1,3}$\quad Mo Yu$^{2}$\quad Hongyuan Mei$^{3}$ \\
$^1$University of Chicago\quad
$^2$WeChat AI\quad
$^3$Toyota Technological Institute at Chicago\\
\texttt{\{hzhao,hongyuan\}@ttic.edu}
}


\maketitle


\begin{abstract}
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. 
In this paper, we propose \name, a novel system that uses language models to perform multi-step \underline{l}ogical r\underline{ea}soning and incorporates explicit \underline{p}lanning into the inference procedure. 
Explicit planning enables the system to make more informed reasoning decisions at each step by looking ahead into their future effects. 
Moreover, we propose a training strategy that safeguards the planning process from being led astray by spurious features. 
Our full system significantly outperforms other competing methods on multiple standard datasets. 
When using small T5 models as its core selection and deduction components, our system performs competitively compared to GPT-3 despite having only about 1B parameters (i.e., 175 times smaller than GPT-3). 
When using GPT-3.5, it significantly outperforms chain-of-thought prompting on the challenging PrOntoQA dataset. 
We have conducted extensive empirical studies to demonstrate that explicit planning plays a crucial role in the system's performance.
\end{abstract}

\section{Introduction}\label{sec:introduction}
Logical reasoning is one of the most important and longstanding problems in artificial intelligence~\citep{russel2010}.
A logical reasoning system is able to draw new facts by applying known rules to known facts and determine the truth value of a given hypothesis; see \cref{fig:overview} for an example. 
For decades, research in building reasoning systems has heavily relied on formal logic. 
Since the surge of pretrained large language models (LMs), there have been efforts that harness the power of pretrained LMs and directly handle natural language statements to perform multi-step logical reasoning; see \cref{sec:related} for a summary. %
In this paper, we propose \name, the first LM-based \underline{l}ogical r\underline{ea}soning system that performs \emph{explicit \underline{p}lanning} during inference. 
While determining the truth value of a statement, our system searches over the known facts for those which are relevant and performs multiple rounds of deduction to reach the conclusion. 
At each round, the planning process looks ahead into the future outcomes of each possible reasoning decision (i.e., which to select and what to deduce), examining which of them is more likely to discover a valid proof for the given statement. 
\begin{figure}
     \centering
     \begin{center}
        \includegraphics[width=0.9\linewidth]{figures/examples/inference-without-planning.pdf}
        \vspace{-8pt}
        \caption{An example of theory $\set{T}$ and goal $\vec{x}_0$ as well as a human-annotated multi-step logical reasoning process that proves the goal based on the theory.}
        \label{fig:overview}
    \end{center}
    \vspace{-16pt}
\end{figure}

\paragraph{Why planning?}
Planning is a fundamental property of intelligent behavior: it uses foresight to anticipate future outcomes of each possible decision and informs the process of decision making to achieve desirable end results. %
This concept has influenced the development of various methods in the field of artificial intelligence. 
Minimax-style game playing evaluates each possible move by anticipating replies and counterreplies between the player and the opponent (while assuming that both play optimally)~\citep{russel2010}. 
Model-based reinforcement learning uses environment models to simulate responses to actions and then uses the simulated experiences to help learn value functions (e.g., Dyna, Monte-Carlo tree search)~\citep{sutton2018reinforcement}. 
In natural language processing, planning has been used to help language models generate utterances that satisfy complex constraints~\citep{lu-etal-2022-neurologic}. 

Planning is important for logical reasoning. 
By examining the future outcomes of each possible decision, a planning-based system will be able to focus on the actually useful (given and deduced) facts at early steps, thus enjoying a high chance of success. %
In addition, a planning-based reasoning system tends to be more interpretable, thus more useful in user-centric and safety-critical scenarios. 
For example, at each round of deduction, planning will explicitly show ``what will happen after---and that is also why---I select these known facts and deduce this particular new fact from them'', which is more informative than only saying ``I select these and deduce this.'' 
However, none of the previous LM-based systems use explicit planning during inference. 


\paragraph{Why is it challenging?}
During planning, a verification mechanism is in need to determine the quality of each possible proof. 
In reality, the verification has to be performed by a {model} (like in model-based reinforcement learning), and models are imperfect due to architectural biases and finite training data. 
As a consequence, the reasoning system faces the problem of model exploitation: any model mistake may misguide the planning such that it favors a seemingly promising decision over the actually correct one.
For example, the model may incorrectly think a statement proves the hypothesis, just because of a significant lexical overlap, causing the planning to favor a decision that helps deduce that statement and lead to the wrong conclusion. 


\paragraph{Our contributions.}

We first propose a logical reasoning system along with a beam-search-style inference algorithm (\cref{sec:baseline}): the system utilizes pretrained LMs and mimics human-like step-by-step reasoning. %
Then we integrate explicit planning into the inference algorithm (\cref{sec:infplan}) and significantly improve the performance of the system. 
We empirically demonstrate that planning encounters the issue of model exploitation: when the given hypothesis is false, planning may find out an incorrect proof that fools the system to believe that the hypothesis is true.
Finally, we develop a training strategy that effectively mitigates the issue of model exploitation (\cref{sec:contrastive}). 
Our training strategy is adversarial: 
for each training theory, we synthesize a non-provable hypothesis but call the planning-based inference method to find a highly-scored proof for it; 
then we refine the verification model such that the score it assigns to that proof is suppressed; 
at the same time, we force the verification model to preserve its scores on the correct proofs of the provable hypothesises. 
Our experiments show that this strategy further significantly improves the performance of our system. 





\section{Problem Formulation}
\label{sec:problem}
We consider the problem of logical reasoning. 
Given a hypothesis (or, in other words, a goal) $\vec{x}_0$ and a theory $\set{T} = \{ \vec{x}_1, \ldots, \vec{x}_N \}$, we are interested in determining the truth value of $\vec{x}_0$, i.e., whether $\vec{x}_0$ can be logically proved by $\set{T}$. 
If the goal $\vec{x}_0$ is provable, we are interested in discovering the reasoning process that proves it. 
Below is an example theory $\set{T}$
\[
    \left\{
    \begin{aligned}
      &\text{``Richard is a King.''}\quad \text{``John is also a King.''}\\
      &\text{``John is greedy.''}\quad \text{``A greedy King is evil.''}
    \end{aligned}
    \right\}
\]
For the goal $\text{``John is evil.''}$, humans can easily verify that it is provable by figuring out the following reasoning path: we can select the two premises about ``John'' and deduce ``John is a greedy King.'' by combining them; we then pick the premise about ``greedy King'' and conclude ``John is evil.'' by combining it with the previous deduction. 
In this paper, we build an automatic system that is able to perform this kind of human-like logical reasoning.%


\section{Our \name Framework}\label{sec:method}
We propose \name, an LM-based logical reasoning system that performs explicit planning. 
Pretrained LMs are excellent at understanding natural languages as well as fluently generating them.\footnote{We use ``language model'' broadly to refer to multiple types of language representation models including encoder-only, decoder-only, and encoder-decoder models. 
}
Our \name system harnesses such abilities to simulate step-by-step reasoning processes that resembles how humans do logical reasoning. 
In this section, we will incrementally build up our full system, starting from a base system (\cref{sec:baseline}) 
to how explicit planning is integrated (\crefrange{sec:infplan}{sec:contrastive}).


\subsection{Base System}\label{sec:baseline}
Our base system consists of a selection model $p_{\text{sel}}$, a deduction model $p_{\text{ded}}$, and a verification model $p_{\text{ver}}$. 
They work together in an iterative fashion to perform multi-step reasoning like shown in \cref{fig:overview}. 
At each step, the selection model $p_{\text{sel}}$ selects a couple of premises from the current theory. For example, at step-1 in \cref{fig:overview}, it selects ``eagles eat rabbits'' and ``rabbits are animals'' from the original theory of four premises. 
Then the deduction model $p_{\text{ded}}$ reads the selected premises and outputs a new statement that is logically plausible given the selection. 
For example, at step-1 in \cref{fig:overview}, it deduces ``eagles eat animals''. 
The new statement is then added to the theory (whose size increases by one) and it may be selected by $p_{\text{sel}}$ at a later step. 
The procedure stops if the max number of reasoning steps has been reached; otherwise, it starts a new iteration of selection and deduction. 
This procedure gives a reasoning path as shown in \cref{fig:overview}. 


We define the \emph{proof} score of the reasoning path to be
\begin{align}
    f(\set{T}, \vec{x}_0)
    \defeq \max_{n=1,\ldots,N} p_{\text{ver}}(\vec{x}_0 \mid \vec{x}_n) \in (0,1) \label{eqn:out}
\end{align}
where theory $\set{T}$ has been extended to include all the new deductions obtained through the reasoning process. 
Each $p_{\text{ver}}(\vec{x}_0\mid \vec{x}_n)$ is given by the verification model and measures how likely the statement $\vec{x}_n$ will prove the goal: e.g., ``eagles only eat animals'' ($\vec{x}_6$) should have a lower score than ``eagles are carnivores'' ($\vec{x}_7$) since the latter means the same as the goal.
The proof score $f(\set{T}, \vec{x}_0)$ can be regarded as the system's belief that the theory proves the goal. 



How do we define the verification score $p_{\text{ver}}(\vec{x}_0 \mid \vec{x}_n)$?
We utilize a pretrained DeBERTa model~\citep{he2021deberta} that was fine-tuned on the standard MNLI language inference dataset~\citep{mnli}. 
For a statement $\vec{x}_n$ and goal $\vec{x}_0$, we define the verification score $p_{\text{ver}}(\vec{x}_0 \mid \vec{x}_n)$ to be the DeBERTa probability that $\vec{x}_n$ \emph{entails} $\vec{x}_0$. 
It is a reasonable estimate for the probability that $\vec{x}_n$ \emph{proves} $\vec{x}_0$. %


Our system is general: the selection and deduction models can be any pretrained decoder-only or encoder-decoder models, including the small models whose parameters we could update and the huge models that we could only use as blackboxes. 
In \cref{sec:param}, we will discuss some specific model choices as well as how to transfer them to our logical reasoning problem. 
Generally, we only require that
\begin{itemize}[leftmargin=*]%
    \item the selection model $p_{\text{sel}}$ can propose multiple multi-premise selections given the theory $\set{T}$ and assign a score to each of them. For a multi-premise selection $\vec{s}$ (e.g., $\vec{s} = \vec{x}_2 \vec{x}_3$), we denote the score to be \mbox{$p_{\text{sel}}(\vec{s}\mid \set{T}, \vec{x}_0)$}, or $p_{\text{sel}}(\vec{s})$ for short. 
    \item the deduction model $p_{\text{ded}}$ can draw multiple deductions given a selection $\vec{s}$ and assign a score to each of them.
    For a deduction $\vec{x}$, we denote its score to be $p_{\text{ded}}(\vec{x}\mid \vec{s})$.
\end{itemize}
So far, we have been assuming that we select the highest scored selection and deduction at each step (e.g., in \cref{fig:overview} and at the beginning of this section). 
But this kind of one-best decoding tends to be short-sighted: 
there may be multiple possible reasoning paths to proving the goal; 
some may be better than the others (e.g., they are shorter) but they may not appear to be promising at the early steps; 
such reasoning paths may be missed by one-best decoding.
Therefore, we develop an improved decoding method that resembles beam search~\citep{jurafsky-2000}. 
\begin{figure*}
     \centering
     \begin{center}
     \begin{subfigure}[b]{0.49\linewidth}
         \centering
         \includegraphics[width=0.85\textwidth]{figures/planning/planning-selection.pdf}
         \caption{Planning for selection.}
         \label{fig:plan-sel}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\linewidth}
         \centering
         \includegraphics[width=0.85\textwidth]{figures/planning/planning-deduction.pdf}
         \caption{Planning for deduction.}
         \label{fig:plan-ded}
     \end{subfigure}
    \vspace{-8pt}
    \caption{An illustration of explicit planning at the 2nd selection and deduction step of the full procedure in \cref{fig:overview}.}
    \label{fig:inf-p}
    \end{center}
    \vspace{-16pt}
\end{figure*} 


\paragraph{Beam-search-style inference.}
We maintain a buffer $\set{B}$ of maximum size $B$ which can host at most $B$ ongoing reasoning paths, which we think are the most promising and will eventually prove the goal. 
Each of ongoing path tracks its proof score $f$ as well as its log-probability $g$ under our system. %
Both $f$ and $g$ get updated as the path progresses, which we will explain shortly.  
It also tracks its initial theory as well as its selections and deductions; the initial theory and the deductions form the extended (or current) theory. 
As long as we haven't reached the maximum number of steps, we keep expanding each ongoing path in the buffer. 
Each step of expansion includes a selection step followed by a deduction step. 
At the selection step, we do the following: 
\begin{itemize}[leftmargin=*]%
    \item For each ongoing path, we find its top $B$ most probable selections $(u_1, \vec{s}_1), \ldots, (u_{B}, \vec{s}_{B})$ where $u_b$ is the log-probability $\log p_{\text{sel}}(\vec{s}_b)$. %
    Each selection expands its ongoing path and updates its $g$ score by $g \gets g + u_b$. 
    \item Now we have $B^2$ extended paths and let the buffer $\set{B}$ only keep $B$ of them which are most probable under the system (i.e., those with the highest $g$).
\end{itemize}
At the deduction step, we follow a similar procedure: 
\begin{itemize}[leftmargin=*]%
    \item For each ongoing path, we %
    draw its top $B$ most probable deductions $(v_{1}, \vec{y}_{1}), \ldots, (v_{B}, \vec{y}_{B})$ conditioned on the most recent selection $\vec{s}$; $v_b$ is the log-probability $p_{\text{ded}}(\vec{y}_b \mid \vec{s})$ under deduction model $p_{\text{ded}}$. 
    Each deduction expands the ongoing path: it updates the scores by $g \gets g + v_b$ and $f \gets \max\{ f, p_{\text{ver}}(\vec{x}_0\mid \vec{y}_b) \}$.%
    \item Now we end up with $B^2$ extended paths and only keep $B$ of them which have the highest $g$. 
\end{itemize}
In the end, we return the reasoning path with the highest proof score $f$: 
intuitively, among all the choices that are probable under the selection and deduction models, we'd like to pick what's most likely to actually prove the goal. 
This method becomes one-best decoding if we set $B=1$. 
%

\cref{app:basesystemdetail,app:inf} has more details of the base system, including pseudocode for inference (\cref{alg:inf,alg:sel,alg:ded}). %


\paragraph{Relations to formal logic systems.}
Our base system resembles a rule-based system and the inference method is like a combination of the forward and backward chaining algorithms~\citep{russel2010}. 
Each deduction step extends the theory by deducing new facts from the existing facts and rules, which resembles the forward chaining algorithm. 
Each selection step is conditioned on the goal, which resembles the backward chaining algorithm. 
However, the forward and backward algorithms can not handle the theories that have non-definite clauses like ``Either John or Richard is evil.''; 
our method doesn't have that limitation.%



\subsection{Improvement-A: Inference with Planning}\label{sec:infplan}
The inference method in \cref{sec:baseline} lacks \emph{planning}. 
While expanding each ongoing path, the selections and deductions are ranked by their scores $u$ and $v$ that are only conditioned on the previous selections and deductions. 
However, the selections and deductions that appear to be promising may not actually lead to the \emph{future} steps that are able to prove the goal. 
In this section, we propose an improved inference method that ranks the selections and deductions by explicit planning. 
We refer to the improved version as System A. %


\paragraph{Planning for selection.}
At each selection step, we expand each ongoing reasoning path with $B$ selections given by the no-planning method, and let the buffer $\set{B}$ keep $B$ of the $B^2$ extended paths with the highest scores. 
The key improvement is: we redefine the score such that it reflects not only the probability of the selection under the model $p_{\text{sel}}$ but also the quality of the future steps that the selection leads to. 

Precisely, we redefine $u = \log p_{\text{sel}}(\vec{s}) + \alpha \Delta u$ where $\alpha$ is a tunable hyperparameter and $\Delta u$ is a future-specific correction term that we can compute after rolling out some imaginary future deductions. 
For a possible selection $\vec{s}$, we call the base one-best decoding method (\cref{sec:baseline}) to roll out $D$ steps of future deductions $\tilde{\vec{y}}_{1}, \ldots, \tilde{\vec{y}}_{D}$. %
Then we obtain $p_{\text{ver}}(\vec{x}_0 \mid \tilde{\vec{y}}_{d})$---which evaluates how likely each rolled-out deduction may entail the goal---and compute the correction term by $\Delta u \defeq \max_{d} \log p_{\text{ver}}(\vec{x}_0 \mid \tilde{\vec{y}}_{d})$. 
Note that $\Delta u$ is the logarithm of the proof score defined on the rolled-out future reasoning path. 
Intuitively, a higher $\Delta u$ means that this future reasoning path is more likely to prove the goal. 
In the end, we obtain $B$ selections with updated scores $(u_1, \vec{s}_1), \ldots, (u_{B}, \vec{s}_{B})$ for each ongoing path. 

This improved subroutine is illustrated in \cref{fig:plan-sel}. Its pseudocode is \cref{alg:plan-sel} in \cref{app:plan}. 

 



\paragraph{Planning for deduction.}
At each deduction step, we expand each ongoing reasoning path with $B$ deductions given by the no-planning method, and let the buffer $\set{B}$ keep $B$ of the extended paths with the highest scores. 
Similar to the planning-based selection step, the key improvement is the refined definition of the score, which reflects not only the probability of the deduction under the model $p_{\text{ded}}$ but also the quality of its future steps. 

Precisely, we first draw $B$ most probable deductions $(v_1, \vec{y}_1), \ldots, (v_B, \vec{y}_B)$ under the model $p_{\text{ded}}$. 
Then we edit the score $v_b \gets v_b + \beta \Delta v_b$ where $\beta$ is a tunable hyperparameter and $\Delta v$ is a future-specific correction similar to $\Delta u$. 
For each possible deduction $\vec{y}_b$, we call the no-planning one-best decoding method to roll out $D$ steps of future deductions $\tilde{\vec{y}}_{b,1}, \ldots, \tilde{\vec{y}}_{b,D}$. 
Then we compute $\Delta v_{b} \defeq \max_d \log p_{\text{ver}}(\vec{x}_0 \mid \tilde{\vec{y}}_{b,d})$. 
In the end, we obtain $B$ deductions with updated scores $(v_1, \vec{y}_1), \ldots, (v_B, \vec{y}_B)$ for each ongoing path.

This improved subroutine is illustrated in \cref{fig:plan-ded}. Its pseudocode is \cref{alg:plan-ded} in \cref{app:plan}. %


\paragraph{The full method.}
Except for the score definitions, the planning-based inference method looks the same as the no-planning method: 
the top selections and deductions will expand their ongoing paths and update their scores $f$ and $g$; the buffer will only keep $B$ paths with the highest $g$. 
But the planning-based method will tend to end up with a different set of reasoning paths than the no-planning method since the scores have been affected by the roll-outs.
The full inference algorithm is \cref{alg:inf} in \cref{app:inf}:
when $D \geq 1$, it does explicit planning; 
when $D=0$, it doesn't roll out future steps and becomes the no-planning method. 


\paragraph{System 1 vs.\@ System 2 reasoning.}\label{sec:relation}
According to the ``dual process'' theories of reasoning~\citep{evans2003two}, human cognition can be thought of as an interplay between a fast and intuitive ``System 1'' and a slow but analytical ``System 2''. 
Given enough time, System 2 can analyze the default behavior of System 1 and override it if necessary. 
In analogy to this process, our base system can be considered as System 1, while the advanced planning-based system is like System 2, which requires more computation but performs more deliberative reasoning. %

Precisely, at each step of reasoning, the no-planning base system needs $3B$ operations (i.e., select, deduce, and verify). 
In contrast, the planning-based inference needs $3B + 3B^2D + 3B^2D$ operations: for each ongoing reasoning path in the buffer, we need to examine its $B$ possible expansions (selection or deduction), and roll out $D$ future steps (via one-best decoding) for each expansion. 
Overall, the planning-based system consumes $1+2BD$ times of computation. 
Fortunately, our implementation is efficient because of careful tensorization and parallelism; please see \cref{sec:binary} for an analysis of its actual walk-clock time. 


\subsection{Improvement-B: Refined Verification Model}\label{sec:contrastive}
The key limitation of the planning method is that it may exploit the pretrained verification model $p_{\text{ver}}$ such that the final proof score $f(\text{theory}, \text{goal})$ is inflated: this method keeps ongoing paths that have high $p_{\text{ver}}(\text{goal} \mid \text{possible future deductions})$. 
This will result in a high rate of false positive: even when the goal is not provable, explicit planning will still try its best to find out the reasoning paths that have high proof scores; a high proof score will then fool the system itself to believe that this goal is provable. 
This issue is illustrated in our experiments (see \cref{fig:accneg} and related analysis in \cref{sec:binary}). 
In this section, we propose to resolve this issue by refining our verification model. We refer to this version of our \name system as System B. 

Our method is to tune the verification model $p_{\text{ver}}$ such that $p_{\text{ver}}(\text{goal} \mid \text{deduction})$ is low when the deduction can \emph{not} prove the goal.
Technically, given a theory $\set{T}$ and a \emph{non-provable} goal $\bar{\vec{x}}_0$, we first call our planning-based method to find a reasoning path that tries to prove $\bar{\vec{x}}_0$, and then make $p_{\text{ver}}(\bar{\vec{x}}_0 \mid \bar{\vec{y}})$ to be low for each deduction $\bar{\vec{y}}$ in the reasoning path. 
Precisely, we locally minimize $\ell$: 
\begin{align}
    \log p_{\text{ver}}(\bar{\vec{x}}_0 \mid \bar{\vec{y}}) - \log \left( p_{\text{ver}}(\bar{\vec{x}}_0 \mid \bar{\vec{y}}) + p_{\text{ver}}(\vec{x}_0 \mid \vec{y}) \right)
\end{align}
where $\vec{x}_0$ is a provable goal and $\vec{y}$ is a deduction in a reasoning path that actually proves $\vec{x}_0$. 
This objective $\ell$ is a typical contrastive learning objective~\citep{ma2018noise}. 
In our setting, it means: 
if we are given a non-provable goal $\bar{\vec{x}}_0$ paired with a model-proposed reasoning path as well as a provable goal $\vec{x}_0$ paired with a correct reasoning path, 
our verification model $p_{\text{ver}}$ should learn to correctly judge that ``$\bar{\vec{x}}_0$ proved by path of $\bar{\vec{y}}$'' is \emph{less likely} than ``$\vec{x}_0$ proved by path of $\vec{y}$''. This framework is illustrated in \cref{fig:contrast-sample-selection}. %
\begin{figure}
    \centering
    \begin{center}
    \includegraphics[width=0.90\linewidth]{figures/others/contrastive_learning.pdf}
    \vspace{-8pt}
    \caption{Illustration of our contrastive learning framework for refining verification model.}
    \label{fig:contrast-sample-selection}
    \end{center}
    \vspace{-20pt}
\end{figure} 


Additionally, we augment the loss $\ell$ with %
\begin{subequations}\label{eqn:reg}
\begin{align}
    \Omega 
    =\ 
    &- p^{-}_{\text{ver}}(\vec{x}_0 \mid \vec{y}) \log p_{\text{ver}}(\vec{x}_0 \mid \vec{y}) \\
    &- 
    \left(1-p^{-}_{\text{ver}}(\vec{x}_0 \mid \vec{y})\right) \log \left( 1 - p_{\text{ver}}(\vec{x}_0 \mid \vec{y}) \right)
\end{align}
\end{subequations}
where $p^{-}_{\text{ver}}$ is the pretrained verification model used in \cref{sec:baseline,sec:infplan}. 
It is the KL-divergence (minus $H(p^{-}_{\text{ver}})$, which is a constant wrt.\@ model parameters) between the pretrained and tuned verification models, and minimizing it aims to prevent the tuned model from deviating too much from the pretrained. 
This is desirable since the pretrained model already enjoys a high rate of true positive for provable goals; see results in \cref{fig:accposi} and relevant analysis in \cref{sec:binary}. 

Technical details (including visualization) about the verification model are in \cref{app:trainver}. %



\section{Small and Large Model Versions}
\label{sec:param}
Now we introduce two specific versions of our proposed framework: 
the small language model (SLM) version that uses pretrained T5~\citep{raffel2020exploring} and the large language model (LLM) version that utilizes GPT-3.5. 

\subsection{SLM Version}\label{sec:slm}
Our SLM version adapts pretrained T5 models~\citep{raffel2020exploring} to be the selection and deduction models. 
We use the T5-small instance (from Huggingface) that has only 60M parameters because we would like to investigate how well a very small system will work in practice. 
Shortly in \cref{sec:experiment}, we will see that this small system works very well. 
\begin{figure}[t]
     \centering
     \begin{center}
     \begin{subfigure}[b]{0.9\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figures/structure/selection.pdf}
         \vspace{-16pt}
        \caption{A selection step. The T5 encoder reads special tokens, the goal $\vec{x}_0$, and the theory $\set{T}$. 
        The decoder computes $p_{\text{sel}}(\vec{x}_n\mid \set{T},\vec{x}_0) \defeq \sigma(\vec{h}^{\top} \vec{w}_{n})$ where $\vec{w}_{n}$ is the embedding of special token $\text{SP}_n$. 
        }
        \label{fig:sel}
     \end{subfigure}

    \vspace{8pt}
    
     \begin{subfigure}[b]{0.9\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figures/structure/deduction.pdf}
         \vspace{-16pt}
        \caption{A deduction step. The T5 encoder reads special tokens and the selection $\vec{s}=\vec{x}_4\ \vec{x}_5$ and generates a deduction autoregressively. It is currently trying to find the token after ``only'', and ``eat'' wins.}
        \label{fig:ded}
     \end{subfigure}
    \vspace{-4pt}
    \caption{An illustration of how the SLM selection and deduction models in the example procedure of \cref{fig:overview}.}
    \label{fig:selded}
    \end{center}
    \vspace{-8pt}
\end{figure}

Given a theory $\set{T}$ and a goal $\vec{x}_0$, the selection T5 model reads them as input and produces the probability $p_{\text{sel}}(\vec{x}_n \mid \set{T}, \vec{x}_0)$ that each premise $\vec{x}_n$ is selected in the attempt to prove the goal $\vec{x}_0$. 
Then we can use these probabilities to compute the probability $p_{\text{sel}}(\vec{s} \mid \set{T}, \vec{x}_0)$ that a multi-premise combination $\vec{s}$ (e.g., $\vec{s}=\vec{x}_2 \vec{x}_4$) is selected:\footnote{We treat each $\vec{x}_n$ independently.}
\begin{align*}
    \prod_{n:\vec{x}_n\in\vec{s}} p_{\text{sel}}(\vec{x}_n \mid \set{T}, \vec{x}_0) \prod_{n:\vec{x}_n\notin\vec{s}} (1-p_{\text{sel}}(\vec{x}_n \mid \set{T}, \vec{x}_0))
\end{align*}
Then finding the most probable selection is to choose the premises $\vec{x}_n$ that have $p_{\text{sel}}(\vec{x}_n\mid \set{T},\vec{x}_0) > 0.5$.\footnote{For each $\vec{x}_n$, if $p_{\text{sel}} > 0.5$, we will have $p_{\text{sel}} > 1-p_{\text{sel}}$. That is, including it in $\vec{s}$ will increase the probability of $\vec{s}$.}
This procedure is illustrated in \cref{fig:sel}. 
%

Give a selection $\vec{s}$, the deduction T5 model reads $\vec{s}$ and produces a logical deduction $\vec{y}$ one token after another. 
The probability of $\vec{y}$ under the model is $p_{\text{ded}}(\vec{y} \mid \vec{s})$. 
\cref{fig:ded} shows a deduction step. 


Training the SLM version requires a corpus of theories and goals as well as their ground-truth reasoning paths. 
The selection steps are training examples for $p_{\text{sel}}$; the deduction steps are training examples for $p_{\text{ded}}$. 
Taking \cref{fig:overview} as an example, the selection training data (green background) is 
\begin{itemize}[leftmargin=*]%
    \item $\set{T}= \{\vec{x}_1, \vec{x}_2, \vec{x}_3, \vec{x}_4\}$ \ \ \ \ \ \ \ \ \ \ \ \ and $\vec{s} = \vec{x}_2 \ \vec{x}_3$
    \item $\set{T}= \{\vec{x}_1, \vec{x}_2, \vec{x}_3, \vec{x}_4, \vec{x}_5\}$ \ \ \ \ \ \ and $\vec{s} = \vec{x}_4 \ \vec{x}_5$
    \item $\set{T}= \{\vec{x}_1, \vec{x}_2, \vec{x}_3, \vec{x}_4, \vec{x}_5, \vec{x}_6\}$ and $\vec{s} = \vec{x}_1 \ \vec{x}_6$
\end{itemize}
and the deduction training data (blue background) is
\begin{itemize}[leftmargin=*]%
    \item $\vec{s} = \vec{x}_2 \ \vec{x}_3$ and new statement $\vec{y} = \vec{x}_5$
    \item $\vec{s} = \vec{x}_4 \ \vec{x}_5$ and new statement $\vec{y} = \vec{x}_6$
    \item $\vec{s} = \vec{x}_1 \ \vec{x}_6$ and new statement $\vec{y} = \vec{x}_7$
\end{itemize}
The training objectives for the selection model $p_{\text{sel}}$ and deduction model $p_{\text{ded}}$ are $\log p_{\text{sel}}(\vec{s} \mid \set{T}, \vec{x}_0)$ and $\log p_{\text{ded}}(\vec{y} \mid \vec{s})$, respectively. 

\cref{app:slm} includes more details about the SLM version (e.g., pseudocode for training and inference). 


\subsection{LLM Version}\label{sec:llm}
Our LLM uses GPT-3.5-turbo as the selection and deduction models. 
GPT-3.5 is the current largest and state-of-the-art language model that we have access to. 
We instruct GPT-3.5 to perform selection and deduction by few-shot prompting; please see \cref{app:llm,app:llmexp} for technical details and the prompts used in our experiments. 
This is similar to the selection-inference framework proposed by~\citet{creswell2022selection} except that we request GPT-3.5 to propose \emph{multiple} possible selections and deductions at each step. 
This design allows us to perform explicit planning for each possible selection and deduction and then choose the best option based on planning. 
Since GPT-3.5 doesn't give the values of the probabilities $p_{\text{sel}}$ and $p_{\text{ded}}$, we set $u = v = 0$ in the inference methods, conditioning the selection and deduction entirely on the planning signals. 
The proof score $f$ is still given by the DeBERTa verification model that we introduced in \cref{sec:method}. 




\section{Related Work}\label{sec:related}
Reasoning has been a long-standing research topic in natural language processing. 
For a long time, the majority of research in this direction has been focused on simple tasks such as single-sentence language inference~\citep{bernardi2002reasoning, zamansky2006natural,maccartney-manning-2009-extended, angeli-etal-2016-combining, hu-etal-2020-monalog, chen-etal-2021-neurallog} and single-step commonsense inference~\citep{rajani-etal-2019-explain,latcinnik2020explaining,shwartz-etal-2020-unsupervised}. %

Recently, there has been an increasing research interest in the more complex problem of multi-step logical reasoning, which we study in this paper. 
\citet{saha-etal-2020-prover}, to the best of our knowledge, is the first to propose an interpretable LM-based model for this problem. 
They and \citet{tafjord-etal-2021-proofwriter} work on synthesized data of limited language variability.
The LM-based system proposed by~\citet{bostrom2022natural} has an architecture similar to the SLM version of our base system except that their inference is one-best decoding without planning and their deduction model is trained with extra data collected by~\citet{bostrom-etal-2021-flexible}. 
The selection-inference system of~\citet{creswell2022selection} is similar to the LLM version of our base system but their selection and deduction models are few-shot-prompted GPT-3; we compare with them in \cref{sec:gpt35}. %
\citet{liu-etal-2022-rlet} also use a similar architecture which they train by reinforcement learning. 
\citet{weir2022dynamic} embed LMs into a backward chaining framework, achieving strong performance in scientific reasoning. 
Our main contribution is complementary to the previous work: we integrate explicit planning into LM-based reasoning systems and design a training method to mitigate the model exploitation issue that arises in planning. %
Our system is a kind of general model programs~\citep{dohan2022language}---especially those with verification models~\citep{cobbe2021training}---which use language models inside as probabilistic programs and apply disparate inference algorithms to the models.
Other kinds of approaches to use LMs for reasoning include training discriminative models~\citep{clark2020transformers,picco2021neural,ghosal2022two,zhang2023paradox}, prompting GPT-3 with spelled-out reasoning procedure~\citep{wei2022chain,talmor2020leap}, and distilling GPT-3.5~\citep{fu2023specializing}. 







Another straightforward approach for text-based logical reasoning is to first translate natural language statements into formal logic expressions and then use a formal logic inference engine~\cite{weber-etal-2019-nlprolog,levkovskyi2021generating,nye2021improving,lu-etal-2022-parsing,betz-richardson-2022-deepa2}. 
We tried this approach in our experiments; please see \cref{app:fol} for details. %


Another research area related to multi-step logical reasoning is to reason over graph-structured data. 
A popular kind of graph is knowledge graphs, i.e., relational graphs over symbolic tuples~\cite{lao2010relational,wang2013programming,neelakantan2015compositional,cohen2017tensorlog,xiong2017deeppath,chen2018variational,das2017go}. 
Another kind of graph is built by linking texts via lexical overlap or hyperlink connections~\cite{welbl2018constructing,yang-etal-2018-hotpotqa,khot2020qasc,khot-etal-2021-text}.
Methods in this area involve multi-step navigation through graphs. 
But they rely on pre-defined symbolic and relational structures, thus not directly applicable to our setting. 
Additionally, recent research~\cite{chen-durrett-2019-understanding,min-etal-2019-compositional} shows that
optimizing the performance on these datasets is not well aligned to improving the models' fundamental reasoning abilities. 


















\section{Experiments}
\label{sec:experiment}
We carried out a diverse set of experiments that can demonstrate the effectiveness of our proposed methods. 
We implemented our methods with PyTorch~\citep{pytorch} and Transformers~\citep{huggingface}. 
Our code is at {\small \url{https://github.com/cindermond/leap}}. 
\begin{figure*}
     \centering
     \begin{center}
     \begin{subfigure}[b]{0.32\linewidth}
         \centering
         \includegraphics[width=0.95\linewidth]{figures/trained-deberta/roc_fig_test_trained_deberta.pdf}
         \vspace{-4pt}
        \caption{ROC curves.}
        \label{fig:roc}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\linewidth}
         \centering
         \includegraphics[width=0.95\linewidth]{figures/trained-deberta/true_fig_test_trained_deberta.pdf}
         \vspace{-4pt}
        \caption{Acc curves on positive examples.}
        \label{fig:accposi}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\linewidth}
         \centering
         \includegraphics[width=0.95\linewidth]{figures/trained-deberta/false_fig_test_trained_deberta.pdf}
         \vspace{-4pt}
        \caption{Acc curves on negative examples.}
        \label{fig:accneg}
     \end{subfigure}

    \vspace{-8pt}
    \caption{
    Test results with 95\% bootstrap confidence intervals (CFs) on Entailment Bank Version-I.
    }
    \label{fig:expcurves}
    \end{center}
    \vspace{-8pt}
\end{figure*}
\begin{table*}[t]
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
Method & AUROC & $\text{AUACC}_{\text{pos}}$ & $\text{AUACC}_{\text{neg}}$ & F1  \\
\midrule
Baseline-T5 & 0.67 (0.63, 0.71) & 0.53 (0.49, 0.57) & 0.75 (0.72, 0.78) & 0.62 (0.59, 0.64) \\
Base System & 0.56 (0.51, 0.60) & 0.42 (0.38, 0.47) & 0.78 (0.76, 0.81) & 0.67 (0.67, 0.67)\\
System A    & 0.87 (0.84, 0.89) & 0.86 (0.84, 0.89) & 0.54 (0.50, 0.57) & 0.82 (0.80, 0.84)\\
System B    & \textbf{0.94} (0.92, 0.95) & \textbf{0.87} (0.84, 0.89) & \textbf{0.82} (0.79, 0.85) & \textbf{0.89} (0.87, 0.91)\\
\midrule
RuleTaker & \textbf{0.90} (0.88, 0.93)& \textbf{0.91} (0.88, 0.94) & \textbf{0.73} (0.69, 0.77)& 0.84 (0.83, 0.86)\\
NeuralUnif & 0.72 (0.68, 0.76) & 0.56 (0.56, 0.57) & 0.49 (0.48, 0.50) & 0.72 (0.71, 0.74)\\
GPT-3 (0-shot) & - & - & - & \textbf{0.89}\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\vspace{-4pt}
\caption{Test results with 95\% bootstrap CFs on Entailment Bank Version-I. }
\label{tab:mainresultent}
\end{center}
\vspace{-16pt}
\end{table*}


\subsection{SLM Experiments on Entailment Bank}\label{sec:ebsetup}
We first trained and evaluated our SLM version on the standard benchmark Entailment Bank~\citep{dalvi-etal-2021-explaining} dataset. 
This dataset is a corpus of human-annotated (theory, provable goal, reasoning path) tuples, including the example in \cref{fig:overview}. 
It uses informal language, which closely aligns with how humans engage in logical reasoning during everyday conversations. 
This dataset has two versions: in Version-I, for each pair of theory and goal, all the premises have to be used to prove the goal; in Version-II, each theory includes a few distractors that are not useful for proving the goal. 
We trained the models on Version-I training data, but evaluated them on both Version-I and Version-II test data. 
Experiment details are in \cref{app:expdetail}, including data statistics (\cref{tab:entailmentbankstats}) and training details (e.g., hyperparameter tuning in \cref{app:hyperpara}).%
%

\paragraph{Evaluation-I: binary classification.}\label{sec:binary}
We evaluated the abilities of the systems to classify provable and non-provable goals. 
For this purpose, we gave a non-provable goal to each dev and test theory by selecting it from other (theory, goal, reasoning path) samples. 
The selection is adversarial: we tuned a pretrained T5 model to generate a provable goal given a theory; for each theory $\set{T}$, we looped over all the goals in the dataset that are guaranteed to be not provable under $\set{T}$, and chose the one that the T5 thinks is the most probable given $\set{T}$ (see details in \cref{app:expdetail}). %


For each given theory $\set{T}$ and goal $\vec{x}_0$, we let the system generate a reasoning path that tries to prove the goal, and obtain the proof score $f(\set{T}, \vec{x}_0)$ of the path. 
Given a threshold $\tau \in (0,1)$, we say ``$\vec{x}_0$ is provable'' if $f(\set{T}, \vec{x}_0) \geq \tau$ and ``$\vec{x}_0$ is not provable'' otherwise. 
For a systematic investigation, we varied $\tau$ and plot a receiver operating characteristic (ROC) curve for each system; the larger the area under ROC curve (AUROC) is, the better the system is. 


The ROC curves are shown in \cref{fig:roc}: 
our \name System A and System B substantially and significantly outperform the base system and a T5 model (trained on generating goals given theories); 
System B further significantly outperforms System A. 
Surprisingly, our base system underperforms the T5 model even though it has learned to spell out its reasoning steps which we expect to help the classification. 


\cref{fig:accposi} and \cref{fig:accneg} show the results broken down into the accuracies on the provable goals and non-provable goals, respectively. 
On provable goals, the accuracy is the number of true positive divided by the total number of test cases; on non-provable goals, the accuracy is the number of true negative divided by the total number of test cases. 
As we can see, System A works very well on the provable goals, but performs poorly on the non-provable goals. 
That is because System A exploits the verification model by explicit planning: as we have discussed in \cref{sec:contrastive}, the proof scores given by System A tend to be high, thus yielding a high rate of false positive. 
System B works well on both provable and non-provable goals: the refined verification model $p_{\text{ver}}$ successfully avoided being exploited by planning. 
Actual values of the areas under curves are shown in \cref{tab:mainresultent}: $\text{AUACC}_{\text{pos}}$ and $\text{AUACC}_{\text{neg}}$ correspond to the curves in \cref{fig:accposi} and \cref{fig:accneg}, respectively.
The F1 numbers were computed as follows: we chose an optimal threshold $\tau$ by maximizing the F1 score on the development set, and then computed F1 on the test set according to the chosen $\tau$. 
\begin{figure*}
     \centering
     \begin{center}
     \begin{subfigure}[b]{0.32\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figures/task2/roc_fig_test_task2.pdf}
         \vspace{-16pt}
        \caption{ROC curves.}
        \label{fig:roctask2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figures/task2/true_fig_test_task2.pdf}
         \vspace{-16pt}
        \caption{Acc curves on positive examples.}
        \label{fig:accpositask2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figures/task2/false_fig_test_task2.pdf}
         \vspace{-16pt}
        \caption{Acc curves on negative examples.}
        \label{fig:accnegtask2}
     \end{subfigure}

    \vspace{-8pt}
    \caption{Test results with 95\% bootstrap CFs on Entailment Bank Version-II.}
    \label{fig:expcurvestask2}
    \end{center}
\end{figure*}

For a comprehensive evaluation, we also compared with three other kinds of methods: 
GPT-3-davinci with 0-shot prompting, 
RuleTaker~\citep{clark2020transformers}, 
and Neural Unification~\citep{picco2021neural}. 
GPT-3 achieves a strong F1 of 0.89, and our System B performs as well as this strong model. %
RuleTaker is a discriminative method, training a RoBERTa~\citep{liu2019roberta} to perform logical reasoning as binary classification (provable or not). 
Neural Unification is also a discriminative method but has a different architecture than RuleTaker. 
It requires more sophisticated annotation and preparation of the training data than RuleTaker and our methods. %
Neither of them spells out a reasoning process. 
For these methods, we matched their numbers of trainable parameters with our methods for a fair comparison. 
Overall, RuleTaker performs better than our System A but worse than System B. 
Neural Unification performs worse than RuleTaker and our System A. 
Note that these results are orthogonal to our main finding that explicit planning is helpful for text-based multi-step logical reasoning. 


\paragraph{Analysis-I: robustness to size of training data.}
We also trained the models with (randomly sampled) 50\% of the training data, and evaluated them on the same test set. 
It turns out that our System B still performs the best; see \cref{fig:expcurves0.5} (which looks boringly similar to \cref{fig:expcurves}) in \cref{app:ablation} for details. 


\paragraph{Analysis-II: About the regularization in \cref{eqn:reg}.} 
We compared the system B with and without the regularization term $\Omega$: %
without $\Omega$, System B only achieves $\text{AUROC}=0.79$ ($\text{AUROC}_{\text{pos}}=0.68$ and $\text{AUROC}_{\text{neg}}=0.65$), worse than System A. 
We also evaluated the tuned verification models on the MNLI dataset (on which they were fine-tuned) and found that: 
the model tuned without $\Omega$ only achieved 62.0\% accuracy; %
the model tuned with $\Omega$ achieved 91.4\% accuracy, almost as good as it originally was (91.7\%).
It means that the regularization term indeed helps the verification model preserve its ability to judge the entailment relationship.

\paragraph{Analysis-III: Robustness to distractors.} 
We investigated the robustness of the systems to distractors by evaluating them on Version-II test data. 
Note that they were only trained on Version-I training data. 
As shown in \cref{fig:expcurvestask2}, all the systems perform worse than they did on Version-I test data, but the performance drop of our systems is much smaller than that of the T5 model. 
It means that our systems are more robust to the distractors. %
That is perhaps because our systems explicitly spell out their reasoning steps and explicit planning can help the systems (A and B) focus on the premises that are actually relevant to the goal at each selection step.  

\paragraph{Analysis-IV: About model size and denoising.}
To examine the effect of model size, we reran the main experiments with T5-small (60M) replaced by T5-base (220M): 
using a larger model achieved a consistently stronger performance; our planning-based systems still significantly outperform the base system. 
We also experimented with denoising training of the selection and deduction models: 
every time we used a training example, we randomly permuted the input statements.
The denoising training led to a better generalization to the evaluation settings with distractors. 
We also found that training with distractors (i.e., using Verstion-II training data) significantly improved the results. 
Detailed results and analysis are in \cref{tab:sizedenoise} and \cref{tab:sizedenoise2} of \cref{app:ablation}. 


\paragraph{Analysis-V: About buffer size.}
The buffer size $B$ is a tunable hyperparameter. 
In our experiments, we chose $B=5$, a common choice in text generation. 
A pilot experiment with $B \in \{2,3,5,10\}$ showed that: 
a smaller $B$ tends to slightly decrease the accuracy on positive samples, but increase it on negative samples; 
a larger $B$ tends to slightly increase the accuracy on positive samples, but decreases it on negative samples; 
overall, there are only tiny changes in $\text{AUROC}$, which depends on accuracies on both kinds of samples.


\paragraph{Analysis-VI: Computation Cost.}
In our experiments, we used $B=5$ and $D=2$, i.e., a buffer size of $5$ and a roll-out depth of $2$. 
According to the theoretical analysis in \cref{sec:infplan}, the planing-based inference should be $1+2BD = 21$ times slower than the no-planning method. 
In practice, it takes an average of $2.8$ seconds for the no-planning method to work on a theory-goal pair from Entailment Bank. 
For the planning-based inference, it takes an average of $31$ seconds, only $11$ times slower. 
The implementation is faster than the theoretical analysis thanks to tensorization and parallelism. 
\begin{table}[t]
\begin{center}
\begin{small}
\setlength{\tabcolsep}{3pt}
\begin{sc}
\begin{tabular}{lcc}
\toprule
Method & Version-I & Version-II \\
\midrule
Baseline-T5 & 0.60 (0.55, 0.65) & 0.20 (0.16, 0.24) \\
Base System & 0.46 (0.41, 0.52) & 0.29 (0.25, 0.34)\\
System A    & 0.80 (0.76, 0.84) & 0.44 (0.39, 0.49)\\
System B    & \textbf{0.88} (0.85, 0.92) & \textbf{0.63} (0.58, 0.68)\\
\midrule
RuleTaker & 0.83 (0.79, 0.87) & 0.73 (0.68, 0.77) \\
NeuralUnif & 0.62 (0.55, 0.69) & 0.62 (0.57, 0.67)\\
GPT-3 (0-shot) & 0.72  & 0.20\\
GPT-3 (5-shot) & 0.97  & 0.96\\
GPT-3 (COT) & \textbf{0.98} & \textbf{0.98}\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\vspace{-8pt}
\caption{Test accuracy with 95\% bootstrap CFs in multiple-choice QA. Accuracy of random guess is 25\%.}
\label{tab:mainresultqa}
\end{center}
\vspace{-8pt}
\end{table}
\begin{table*}[t]
\centering
\begin{minipage}[t]{0.32\textwidth}
\centering
\begin{small}
\begin{sc}
\begin{tabular}[t]{lc}
\toprule
Method & Acc \\
\midrule
 \\
Base System & 0.68 (0.65, 0.71) \\
System A    & 0.84 (0.82, 0.87) \\
System B    & \textbf{0.85} (0.83, 0.87) \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\caption{Dev accuracy with 95\% bootstrap CFs on QASC.} %
\label{tab:qasc}
\end{minipage}
\hfill
\begin{minipage}[t]{0.65\textwidth}
\centering
\begin{small}
\begin{sc}
\begin{tabular}[t]{lccc}
\toprule
Method & Depth=1 & Depth=3 & Depth=5 \\
\midrule
COT & 0.76 (0.68, 0.84)&0.72 (0.63, 0.81)&0.66 (0.57, 0.75)\\
SI & \textbf{0.92} (0.87, 0.97) &0.65 (0.55, 0.74)&0.53 (0.43, 0.63)\\
System A &\textbf{0.92} (0.87, 0.97) &\textbf{0.81} (0.73, 0.89) & {0.70} (0.61, 0.79)\\
System B & 0.91 (0.85, 0.97) & \textbf{0.81} (0.73, 0.89) & \textbf{0.73} (0.64, 0.82)\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\caption{Accuracy with 95\% bootstrap confidence intervals on PrOntoQA. The ``depth'' denotes the number of ground-truth reasoning steps.}
\label{tab:prontoqa}
\end{minipage}
\end{table*}


\paragraph{Evaluation-II: Multiple-Choice QA.}\label{sec:mcqat5}
We further evaluated the systems in a multiple-choice question answering (QA) setting. 
Particularly, given a theory $\set{T}$ in Entailment Bank, each system is asked to select the provable goal from four choices $\{\vec{x}_0^{(1)}, \vec{x}_0^{(2)}, \vec{x}_0^{(3)}, \vec{x}_0^{(4)} \}$: one of them is the ground-truth provable goal while the others are negative choices selected by a tuned T5. %



We took the systems trained in \cref{sec:binary} %
and evaluated them on the Version-I and Version-II of this multiple-choice task: in the Version-II setting, each theory has a few distractors, so it is more challenging than Version-I. 
For each theory, a system tries to prove each choice $\vec{x}_0^{(c)}$, ranks the four choices by their proof scores $f(\set{T}, \vec{x}_0^{(c)})$, and then chooses the one with the highest score. 
The systems were evaluated by accuracy. 
As shown in \cref{tab:mainresultqa}, the systems behave similarly as they do on the binary classification: in both Version-I and Version-II settings, System A and System B perform significantly better than the baselines, and System B significantly outperforms System A. 

We also evaluated GPT-3-davinci with 0-shot, 5-shot, and chain-of-thought (COT) prompting~\citep{gpt3,wei2022chain}.
The COT prompts include the ground-truth reasoning paths of the correct choices; examples are in \cref{app:examplegpt3}.
Our full system outperforms 0-shot GPT-3, but underperforms 5-shot and COT GPT-3. 
Interestingly, 0-shot GPT-3 works worse than random guess when theories have distractors, which indicates the difficulty of this problem. 
In addition, we evaluated RuleTaker and Neural Unification, with their numbers of trainable parameters matched with our methods. 
In the Version-I setting, they both perform worse than our System B and Neural Unification performs even worse than System A. 
Interestingly, they seem to be more robust to distractors: 
in the Versition-II setting, Neural Unification performs competitive to our System B, and RuleTaker performs significantly better than System B. 
However, these methods do not generate interpretable reasoning processes. 


\subsection{SLM Experiments on QASC}\label{sec:qasc}
We also trained and evaluated the systems on the QASC dataset~\citep{khot2020qasc}, a multiple-choice question answering dataset where each question has eight candidate answers. 
Each training QA pair has two premises and a deduction, which can be used to train our deduction model. 
Each development QA pair has two premises so the reasoning system only needs to do a step of deduction but no selection. 
Test QA pairs have no premises given and one has to search through a pool of millions of statements to find the relevant premises, which is not the focus of this paper. 
So we only evaluated the systems on the development set. 
The results are in \cref{tab:qasc}. 
Although this data only requires one step of reasoning, the planning-based systems still significantly outperform the base system, suggesting that explicit planning is indeed helpful for LM-based reasoning. 



\subsection{LLM Experiments on PrOntoQA}\label{sec:gpt35}
We evaluated the LLM version on the ``fictional'' version of the PrOntoQA dataset~\citep{SaparovHe22}. 
It is a binary classification task like Entailment Bank (\cref{sec:binary}), but it is more challenging to large language models such as GPT-3.5 since its logical statements are about fictional characters (e.g., wumpus), meaning that a large model can not bypass the reasoning and draw correct conclusions by commonsense or memorization. %

The main results are shown in \cref{tab:prontoqa}. 
In all cases, our planning-based System A outperforms the selection-inference (SI) method and chain-of-thought (COT) prompting, meaning that explicit planning is consistently helpful. 
Our System B uses the DeBERTa model tuned on the Entailment Bank training data (\cref{sec:contrastive,sec:binary}), and it improves the performance on the ``depth=5'' subset.
\cref{app:prontoqa} includes more details about these experiments and more results. 

\section{Conclusion}
\label{sec:conclusion}
In this paper, we presented \name, an LM-based logical reasoning system that integrates explicit planning into the inference method. 
We also proposed a method that learns to prevent the explicit planning from being misguided. 
Our proposed methods exhibit intriguing technical connections to other reasoning systems and can be likened to the deliberative System 2 in ``dual process'' theories of reasoning.
In our experiments, our planning-based system outperforms strong baseline methods including the selection-inference method and chain-of-thought prompting. 
We will discuss several exciting avenues for further improvements in \cref{app:future}. 



\section*{Acknowledgments}
This work was supported by a research gift to the last author by Adobe Research. 
We thank the anonymous EMNLP reviewers and meta-reviewer for their constructive feedback. 
We thank our colleagues at UChicago and TTIC for helpful discussion.
We also thank Hao Tan at Adobe Research, Yisi Sang at Apple, Benjamin Van Durme at Johns Hopkins University, and David Dohan at OpenAI for their helpful comments.  


\section*{Limitations}
The main limitation of our proposed framework is that it requires more computation than the baseline methods that do not perform explicit planning. 
As discussed in \cref{sec:relation}, the no-planning methods are like the intuitive and fast System 1~\citep{evans2003two} while our methods are like the analytical and slow System 2: after all, more analysis consumes more computation and thus our framework is less energy-efficient. 
This limitation has inspired us to explore new methods such as bandit learning to switch between two types of systems and more efficient planning (see \cref{app:future}). 


\section*{Ethics Statement}
Our work complies with the \href{https://www.aclweb.org/portal/content/acl-code-ethics}{ACL Ethics Policy}. 
It aims to build more intelligent language-based logical reasoning systems which would have a broad positive impact to the society. 
For example, in our daily life, an intelligent logical reasoning system may help us verify facts and identify fake news; 
in legal domain, it may work as an automatic paralegal and assist lawyers with their document processing and decision making; 
in education, it may help students reason about their mistakes and improve learning experience. 
Meanwhile, our methods share the same risks as other machine learning methods, such as misusage, containing data bias, and suffering from adversarial attacks. However, this paper is orthogonal to the research efforts to mitigate these issues.
%
%
%

\bibliography{hongyu_nlp}
\bibliographystyle{acl_natbib}


\clearpage
\newpage
\appendix



\section{Future Extensions}\label{app:future}
Our experiments have inspired us to explore several exciting avenues for further improvements. 

The first is to jointly refine the selection, deduction, and verification models. 
In this paper, we have already shown that adversarially refining the verification model will significantly improve the performance. 
So a natural next step is to adversarially refine the selection and deduction models in response to the updated verification model. 
Allowing components of a system to adversarially refine one another has been shown useful in natural language processing~\cite{yu2019rethinking}.


The second is to develop \emph{implicit} planning methods to improve inference efficiency. 
In reinforcement learning, explicit planning is often only used to help learn a value function during training; during inference, calling a value function is like planning implicitly but faster than explicit planning. 
This kind of methods can apply to our setting. 
Another way to improve efficiency is to learn a bandit that could cleverly switch between the no-planning ``System 1'' and our planning-based ``System 2'' such that we only spend more computation in the more difficult cases. 

Another direction is to leverage unlabeled data, i.e., data without human-annotated reasoning paths.
Such data is less expensive to collect. %
An LM-based reasoning system may be able to benefit from (the indirect training signals of) such data by self-supervised learning. 




\section{Method Details}\label{app:methoddetail}
In this section, we give details of our methods. 

\subsection{Reasoning Process Details}\label{app:basesystemdetail}\label{app:inf}

\cref{alg:inf} gives a detailed explanation for how our inference method works. When $D = 0$, it is the naive method. When $D \geq 1$, it is the inference with explicit planning. 
During selection, we constrain the model to only select two premises for a more controllable behavior. 
When we compute the proof score we only consider the newly generated deductions for convenience. Its effect to results is negligible since later deductions tend to more directly prove the goal. 

\cref{alg:sel} is designed to select a set of statements from the current theory $\set{T}$, with the goal of inferring $\vec{x}_0$. We fix the size of the selection set to 2 in our experiments, but in principle this restriction can be removed.
\cref{alg:ded} draws $B_{\text{ded}}$ new deductions.
Their SLM versions are \cref{alg:selslm,alg:dedslm} and the LLM versions are \cref{alg:selllm,alg:dedllm}. 
\begin{algorithm}
\caption{Reasoning (Inference) with Our System}\label{alg:inf}
\begin{algorithmic}[1]
    \HYPER max number of inference steps $M$;\newline
    depth of planning $D$ ($D=0$ means ``no planning'');\newline
    inference beam size $B_{\text{inf}}$ %
    \INPUT theory $\set{T}=\{\vec{x}_1,\vec{x}_2,\ldots,\vec{x}_{N}\}$ and goal $\vec{x}_0$;\newline
    selection model $p_{\text{sel}}$, deduction model $p_{\text{ded}}$;\newline 
    verification model $p_{\text{ver}}$
    \OUTPUT reasoning path $\set{R}$ with proof score $f$%
  \Procedure{Inference}{$\set{T}, \vec{x}_0, p_{\text{sel}}, p_{\text{ded}}, p_{\text{ver}}$}
    \LineComment{has access to $M$, $B_{\text{inf}}$, $D$}
    \State $\set{B} \gets \text{PriorityQueue}(B_{\text{inf}})$
    \LineComment{max size is $B_{\text{inf}}$ ; priority is first element of tuple}
    \State $\set{B}.\text{add}((0, \emptyset, \set{T}, -\infty))$ 
    \LineComment{init with empty path and current theory}
    \For{$m = 1$ {\bfseries to} $M$}
        \LineComment{do inference at each step}
        \LineComment{selection at step $m$}
        \State $\set{B}_{\text{old}} \gets \set{B}$; 
        $\set{B} \gets \text{PriorityQueue}(B_{\text{inf}})$
        \For {$g_b, \set{R}_b,\set{T}_b, f_b$ {\bfseries in} $\set{B}_{\text{old}}$} %
            \LineComment{$g$ is log-prob of path and $f$ is its proof score}
            \State $\set{S}_b \gets \textsc{Select}(\set{T}_b, \vec{x}_0, p_{\text{sel}})$
            \If {$D > 0$}  
                \LineComment{rank selections based on $D$-step roll-outs}
                \State $\set{S}_b \gets \textsc{PlanS}(\set{T}_{b},\vec{x}_0,\set{S}_b, p_{\text{sel}}, p_{\text{ded}}, p_{\text{ver}})$
            \EndIf
            \For{$u_k, \vec{s}_k$ in $\set{S}_b$}
                \LineComment{add expanded path into priority queue}
                \LineComment{priority score changes by $u_k$}
                \State $\set{B}.\text{add}((g_b+u_k, \set{R}_{b}+\{\vec{s}_k\}, \set{T}_b, f_b))$
                \LineComment{$\set{B}$ has a fixed size $B_{\text{inf}}$: if $|\set{B}| > B_{\text{inf}}$}
                \LineComment{auto-delete lowest-priority element}
            \EndFor
        \EndFor
        \LineComment{deduction at step $m$}
        \State $\set{B}_{\text{old}} \gets \set{B}$; 
        $\set{B} \gets \text{PriorityQueue}(B_{\text{inf}})$
        \For {$g_b, \set{R}_b,\set{T}_b, f_b$ {\bfseries in} $\set{B}_{\text{old}}$} 
            \State $\vec{s}_b \gets$ the most recent selection in $\set{R}_b$
            \State $\set{Y}_b \gets \textsc{Deduce}(\vec{s}_b, p_{\text{ded}})$ 
            \If {$D > 0$}  
                \LineComment{rank deductions based on $D$-step roll-outs}
                \State $\set{Y}_b \gets \textsc{PlanD}(\set{T}_{b}, \vec{x}_0, \set{Y}_b, p_{\text{sel}}, p_{\text{ded}}, p_{\text{ver}})$
            \EndIf
            \For{$v_k, \vec{y}_k$ in $\set{Y}_b$}
                \State $\set{B}.\text{add}((g_b+v_k, \set{R}_{b}+\!\{\vec{y}_k\}, \set{T}_b+\!\{\vec{y}_k\}, f_b))$
            \EndFor
        \EndFor       
        \For {$g_b,\set{R}_b,\set{T}_b,f_b$ {\bfseries in} $\set{B}$ }
            \State $\vec{y}_b \gets$ the most recent deduction in $\set{R}_b$
            \LineComment{if $\vec{y}_b$ entails $\vec{x}_0$ better than any prev deduction}
            \LineComment{update proof score of path $\set{R}_b$}
            \IfThen{$p_{\text{ver}}(\vec{x}_0\mid \vec{y}_b) > f_b$}{$f_b \gets p_{\text{ver}}(\vec{x}_0\mid \vec{y}_b)$}
        \EndFor
    \EndFor
    \LineComment{choose reasoning path with highest proof score}
    \State $b_{\text{max}} \gets \argmax_bf_b$
    \State \textbf{return} $\set{R}_{b_{\text{max}}}, f_{b_{\text{max}}}$
  \EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Selection Subroutine}\label{alg:sel}
\begin{algorithmic}[1]
    \HYPER{selection beam size $B_{\text{sel}}$}
    \INPUT current theory $\set{T} = \{\vec{x}_1,\ldots,\vec{x}_{N+m}\}$ and goal $\vec{x}_0$; selection model $p_{\text{sel}}$%
        \OUTPUT selections with their scores $\{(u_k,\vec{s}_k)\}$
  \Procedure{Select}{$\set{T}, \vec{x}_0, p_{\text{sel}}$}
    \LineComment{generic method for illustration only}
    \LineComment{in practice, we call the SLM or LLM version}
    \LineComment{see \cref{alg:selslm} for SLM version}
    \LineComment{see \cref{alg:selllm} for LLM version}
      \LineComment{has access to $B_{\text{sel}}$}
    \LineComment{return list $\set{S}$ which contains $B_{\text{sel}}$ scored selections}
    \LineComment{each scored selection is $(u,\vec{s})$}
    \LineComment{score $u$ is defined in \cref{sec:infplan}}
  \State \textbf{return} $\set{S}$
  \EndProcedure
\Procedure{OneBestSelect}{$\set{T}, \vec{x}_0, p_{\text{sel}}$} 
    \LineComment{only keeps selection with highest score}
  \State $\set{S} \gets \textsc{Select}(\set{T},\vec{x}_0, p_{\text{sel}})$
  \State $(u, \vec{s}) \gets$ highest-scored element in $\set{S}$
  \State \textbf{return} $\vec{s}$
  \EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Deduction Subroutine}\label{alg:ded}
\begin{algorithmic}[1]
        \HYPER deduction beam size $B_{\text{ded}}$
	\INPUT current selection $\vec{s}$ of statements;\newline 
                deduction model $p_{\text{ded}}$ 
        \OUTPUT deductions with their scores $\{(v_k, \vec{y}_k)\}$
  \Procedure{Deduce}{$\vec{s}$, $p_{\text{ded}}$}
    \LineComment{generic method for illustration only}
    \LineComment{in practice, we call the SLM or LLM version}
    \LineComment{see \cref{alg:dedslm} for SLM version}
    \LineComment{see \cref{alg:dedllm} for LLM version}
      \LineComment{has access to $B_{\text{ded}}$}
    \LineComment{return list $\set{Y}$ which contains $B_{\text{ded}}$ scored deductions}
    \LineComment{each scored deduction is $(v,\vec{y})$}
    \LineComment{score $v$ is defined in \cref{sec:infplan}}
    \State \textbf{return} $\set{Y}$
  \EndProcedure
    \Procedure{OneBestDeduce}{$\vec{s}$, $p_{\text{ded}}$}  \LineComment{only keeps deduction with highest score}
    \State $\set{Y} \gets \textsc{Deduce}(\vec{s}, p_{\text{ded}})$
    \State $(v, \vec{y}) \gets$ element in $\set{Y}$ with highest $v$
    \State \textbf{return} $\vec{y}$
  \EndProcedure
\end{algorithmic}
\end{algorithm}



\subsection{Details of Tuning the Verification Model}\label{app:trainver}
We use the soft prompt tuning method~\citep{prompt-tuning}: we augment the input with a few special tokens and the only trainable parameters are the embeddings of those tokens; it is illustrated in \cref{fig:verification-structure}. %
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/structure/verification.pdf}
    \vspace{-12pt}
    \caption{The structure of the verification model.}
    \label{fig:verification-structure}
\end{figure}

Where do we get $\vec{x}_0$, $\vec{y}$, $\bar{\vec{x}}_0$, and $\bar{\vec{y}}$? 
Recall that we have a training corpus of theories and goals as well as their ground-truth reasoning paths. 
For each pair of theory $\set{T}$ and provable goal $\vec{x}_0$, we could randomly sample a deduction $\vec{y}$ from its ground-truth reasoning path. 
We use the goal of another training example as our non-provable goal $\bar{\vec{x}}_0$, call the planning-based inference method to get a reasoning path, and sample a deduction from the reasoning path as our $\bar{\vec{y}}$. 

\cref{alg:refine} shows how we refine the verification model using the contrastive loss with regularization.
\begin{algorithm}
    \caption{Refining Verification Model}\label{alg:refine}
    \begin{algorithmic}[1]
        \INPUT provable goal $\vec{x}_0$ and gold reasoning path $\set{R}$; \newline
        non-provable goal $\bar{\vec{x}}_0$ and model-generated path $\bar{\set{R}}$; \newline 
        verification model $p_{\text{ver}}$ %
        \OUTPUT updated verification model $p_{\text{ver}}$
        \Procedure{Refine}{$\vec{x}_0, \set{R}, \bar{\vec{x}}_0, \bar{\set{R}}, p_{\text{ver}}$}
            \LineComment{refining procedure}
            \State $p^{-}_{\text{ver}} \gets $ a copy of pretrained $p_{\text{ver}}$
            \LineComment{sample deductions from reasoning paths}
            \State randomly draw $\vec{y}$ from deductions in $\set{R}$
            \State randomly draw $\bar{\vec{y}}$ from deductions in $\bar{\set{R}}$
            \LineComment{refine verification model}
            \State $\ell \gets \textsc{LossVer}(\vec{x}_0, \vec{y}, \bar{\vec{x}}_0, \bar{\vec{y}}, p_{\text{ver}}, p^{-}_{\text{ver}})$
            \State compute $\nabla \ell$ wrt.\@ trainable parameters $\vec{\theta}_{\text{ver}}$ of $p_{\text{ver}}$
            \State update $\vec{\theta}_{\text{ver}}$ with chosen optimization method
            \State \textbf{return} $p_{\text{ver}}$
        \EndProcedure
    
        \Procedure{LossVer}{$\vec{x}_0, \vec{y}, \bar{\vec{x}}_0, \bar{\vec{y}}, p_{\text{ver}}, p^{-}_{\text{ver}}$}
            \LineComment{contrastive loss}
            \State $\ell \gets \log \frac{ p_{\text{ver}}(\bar{\vec{x}}_0\mid\bar{\vec{y}}) }{ p_{\text{ver}}(\bar{\vec{x}}_0\mid\bar{\vec{y}})+p_{\text{ver}}(\vec{x}_0\mid\vec{y}) }$
            \LineComment{compute regularization}
            \State $\ell \minuseq p^{-}_{\text{ver}}(\vec{x}_0 \mid \vec{y}) \log p_{\text{ver}}(\vec{x}_0 \mid \vec{y})$ 
            \State $\ell \minuseq (1-p^{-}_{\text{ver}}(\vec{x}_0 \mid \vec{y})) \log ( 1 - p_{\text{ver}}(\vec{x}_0 \mid \vec{y}))$ 
            \State \textbf{return} $\ell$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}



\subsection{SLM Details}\label{app:slm}\label{app:train}
We give SLM details in this section. 

\paragraph{Selection model.} The selection model $p_{\text{sel}}$ uses a pretrained encoder-decoder model T5~\citep{raffel2020exploring}. The encoder reads a context string concatenating the goal $\vec{x}_0$ and the premises $\vec{x}_1,\ldots,\vec{x}_N$ of current theory $\set{T}$; the decoder computes the probabilities \mbox{$p_{\text{sel}}(\vec{x}_n\mid \set{T},\vec{x}_0)$} that each premise $\vec{x}_n$ is selected in the attempt to prove the goal $\vec{x}_0$.
It is illustrated in \cref{fig:sel}: 
besides the statements, T5 also reads a few special tokens ($\text{ENC}, \text{SP}_0, \text{SP}_1,\ldots,\text{SP}_N, \text{DEC}$); its decoder gives a hidden state $\vec{h}$, which is involved in computing $p_{\text{sel}}(\vec{x}_n\mid \set{T},\vec{x}_0) \defeq \sigma(\vec{h}^{\top} \vec{w}_{n})$ 
where $\vec{w}_{n}$ is the embedding of $\text{SP}_n$. 
For training and inference efficiency, we keep the pretrained T5 frozen so the only trainable parameters of the selection model $p_{\text{sel}}$---denoted as $\vec{\theta}_{\text{sel}}$---are the embeddings of the special tokens. 
The pseudocode of using it for inference is in \cref{alg:selslm}. 


\paragraph{Deduction model.}
Given the selection $\vec{s}$, the deduction model $p_{\text{ded}}$ produces a logical deduction $\vec{y}$ by combining the premises in $\vec{s}$. The new statement $\vec{y}$ is added to the theory $\set{T}$ whose size is then increased by one; therefore, for a theory of size $N$, we also denote $\vec{y}$ as $\vec{x}_{N+1}$. 
The deduction model $p_{\text{ded}}$ uses another pretrained T5. As shown in \cref{fig:ded}, its encoder reads an input string concatenating the selected premises along with a few special tokens; its autoregressive decoder produces a deduction one token after another.
Its trainable parameters $\vec{\theta}_{\text{ded}}$ are the embeddings of the special tokens.
The pseudocode of deploying it is in \cref{alg:dedslm}. 
\begin{algorithm}
\caption{Selection Subroutine for SLM}\label{alg:selslm}
\begin{algorithmic}[1]
    \HYPER{selection beam size $B_{\text{sel}}$}
    \INPUT current theory $\set{T} = \{\vec{x}_1,\ldots,\vec{x}_{N+m}\}$ and goal $\vec{x}_0$; prompted encoder-decoder language model $p_{\text{sel}}$%
        \OUTPUT selections with their scores $\{(u_k,\vec{s}_k)\}$
  \Procedure{Select}{$\set{T}, \vec{x}_0, p_{\text{sel}}$}
      \LineComment{has access to $B_{\text{sel}}$}
      \LineComment{build context by concatenating hypothesis and theory}
  \State $\vec{c} = \text{SP}_0 + \vec{x}_0 + \text{SP}_1 + \vec{x}_1 + \ldots + \text{SP}_{N+m} + \vec{x}_{N+m}$
   \For{$i = 1$ {\bfseries to} $N+m$}  
        \LineComment{compute prob that each statement is selected}
        \State $p_i \gets p_{\text{sel}}(\text{SP}_i|\vec{c})$
   \EndFor
   
  \State $\set{S} \gets \text{PriorityQueue}(B_{\text{sel}})$
  \LineComment{max size is $B_{\text{sel}}$; priority is first element of tuple}
  \For{$i = 1$ {\bfseries to} $N+m$}
    \For{$j = i+1$ {\bfseries to} $N+m$}
        \State $\vec{s}_{k} \gets \vec{x}_{i} + \vec{x}_{j}$
        \State $u_{k} \gets \log p_i + \log p_j + \sum_{\ell \neq i,\ell\neq j}\log(1-p_{\ell})$
        \State $\set{S}.\text{add}((u_k, \vec{s}_k))$
        \LineComment{if $\set{B}$ is larger than $B_{\text{sel}}$, element with}
        \LineComment{lowest priority will be automatically deleted}
    \EndFor
  \EndFor
  \State \textbf{return} $\set{S}$
  \EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Deduction Subroutine for SLM}\label{alg:dedslm}
\begin{algorithmic}[1]
        \HYPER deduction beam size $B_{\text{ded}}$
	\INPUT current selection $\vec{s}$ of statements;\newline 
                prompted encoder-decoder language model $p_{\text{ded}}$ 
        \OUTPUT deductions with their scores $\{(v_k, \vec{y}_k)\}$
  \Procedure{Deduce}{$\vec{s}$, $p_{\text{ded}}$}
      \LineComment{has access to $B_{\text{ded}}$}
      \LineComment{has access to standard beam search implementation} 
    \State $\set{Y} \gets \textsc{BeamSearch}(p_{\text{ded}},B_{\text{ded}},\vec{s})$
    \LineComment{assume:}
    \LineComment{\textsc{BeamSearch} gives a list of tuples $\{(v_k, \vec{y}_k)\}$}
    \LineComment{text string $\vec{y}_k$ sorted in descending order of $v_k$}
    \State \textbf{return} $\set{Y}$
  \EndProcedure
\end{algorithmic}
\end{algorithm}


\paragraph{Training.}
\cref{alg:train} elaborates how the SLM selection and deduction models are trained. 
We use prompt-learning because we do not want to distort the pretrained weights too much. 
It is well known that pretrained language models have already captured substantial amounts of commonsense knowledge such as hypernymy (A is a type of B) and meronymy (A is part of B)~\citep{richardson-sabharwal-2020-qa}; we would like to keep such knowledge to benefit our settings. 
\begin{algorithm}
\caption{Training for SLM}\label{alg:train}
\begin{algorithmic}[1]
    \INPUT 
    theory $\set{T}=\{\vec{x}_1, \ldots, \vec{x}_{N}\}$ and goal $\vec{x}_0$;\newline
    reasoning path $\set{R}$; verification model $p_{\text{ver}}$\newline 
    selection model $p_{\text{sel}}$, deduction model $p_{\text{ded}}$
    \OUTPUT updated models $p_{\text{sel}}$ and $p_{\text{ded}}$
  \Procedure{Train}{$\set{R}, \set{T}, \vec{x}_0, p_{\text{sel}}, p_{\text{ded}}, p_{\text{ver}}$}
    \LineComment{training method for selection and deduction models}
    \LineComment{init extended theory that will include deduction}
    \State $\tilde{\set{T}} \gets \set{T}$ 
    \For{$m = 1$ {\bfseries to} $|\set{R}|/2$} 
        \LineComment{loop over each step of selection and deduction}
        \State $\vec{s}_m\gets$ $m$\th selection \Comment{i.e., $(2m-1)$\th entry in $\set{R}$}
        \State $\vec{y}_m\gets$ $m$\th deduction \Comment{i.e., $(2m)$\th element in $\set{R}$}
        \LineComment{train selection model}
        \State $\ell \gets \textsc{LossSel}(\tilde{\set{T}}, \vec{s}_m, p_{\text{sel}})$
        \State compute $\nabla \ell$ wrt.\@ trainable params $\vec{\theta}_{\text{sel}}$ of $p_{\text{sel}}$
        \State update $\vec{\theta}_{\text{sel}}$ with chosen optimization method
        
        \LineComment{train deduction model}
        \State $\ell \gets \textsc{LossDed}(\vec{s}_m, \vec{y}_m,p_{\text{ded}})$
        \State compute $\nabla \ell$ wrt.\@ trainable params $\vec{\theta}_{\text{ded}}$ of $p_{\text{ded}}$
        \State update $\vec{\theta}_{\text{ded}}$ with chosen optimization method
        \LineComment{extend theory with new deduction}
        \State $\tilde{\set{T}} \gets \tilde{\set{T}} + \{\vec{y}_{m}\}$ 
    \EndFor
    \State \textbf{return} $p_{\text{sel}}, p_{\text{ded}}$
  \EndProcedure
    \Procedure{LossSel}{$\set{T}, \vec{s}, p_{\text{sel}}$}
    \LineComment{construct context for selecting statements from theory}
    \State $\vec{c} \gets \text{SP}_0 + \vec{x}_0 + \text{SP}_1 + \vec{x}_1 + \ldots + \text{SP}_{N+m} + \vec{x}_{N+m}$ 
    \State $\ell \gets 0$ \Comment{loss is negative log-likelihood of selection}
    \For{$i = 1$ {\bfseries to} $N+m$}
        \State $p_i \gets p_{\text{sel}}(\text{SP}_i|\vec{c})$ \Comment{prob that $x_i$ is included in $\vec{s}$}
        \IfThenElse{$\vec{x}_i$ in $\vec{s}$}{$\Delta \ell \gets \log p_i$}{$\Delta \ell \gets \log (1-p_i)$}
        \State $\ell \gets \ell - \Delta \ell$ \Comment{update $\ell$ with minus log-probability}
    \EndFor
    \State \textbf{return} $\ell$
  \EndProcedure

  \Procedure{LossDed}{$\vec{s}, \vec{y}, p_{\text{ded}}$}
    \LineComment{loss is negative log-prob of deduction under model}
    \State $\ell\gets -\log p_{\text{ded}}(\vec{y}\mid\vec{s})$
    \LineComment{$\log p_{\text{ded}}(\vec{y}\mid\vec{s})$ sums log-probabilities of tokens in $\vec{y}$}
    \State \textbf{return} $\ell$
  \EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{LLM Details}\label{app:llm}
We give LLM details in this section. 
For selection, we use a large language model as a black box and prompt it to choose several different multi-premise selections from the given theory $\set{T}$. 
The pseudocode is in \cref{alg:selllm}. 
Below is the prompt template: 
\begin{lstlisting}
# few-shot examples to demonstrate selection 
# @see \cref{app:llmexp}@ for an example

You are doing a true-false question test, and this is a subtask. Given the facts and a question, please select facts that are most useful for answering the question.

Here are a few examples with explanations.

# few-shot demonstrations 

Please refer to these examples and try to generate correct answer 

# theory and question/goal of interest
\end{lstlisting}

For deduction, we also use a large language model as a black box and prompt it to draw new deductions conditioned on a given selection $\vec{s}$. 
The pseudocode is in \cref{alg:dedllm}. 
The prompt template is as follows: 
\begin{lstlisting}
# few-shot examples to demonstrate deduction 
# @see \cref{app:llmexp}@ for an example

You are doing a true-false question test, and this is a subtask. Given the facts, please deduce a new fact that is logically reasonable. When there is no such deduction, say None. 

Here are a few examples with explanations.

# few-shot demonstrations 

Please refer to these examples and try to generate correct answer. 

# selection of statements of interest
\end{lstlisting}
\begin{algorithm}
\caption{Selection Subroutine for LLM}\label{alg:selllm}
\begin{algorithmic}[1]
    \HYPER{selection beam size $B_{\text{sel}}$}
    \INPUT current theory $\set{T} = \{\vec{x}_1,\ldots,\vec{x}_{N+m}\}$ and goal $\vec{x}_0$; selection model $p_{\text{sel}}$%
    \OUTPUT selections with their scores $\{(u_k,\vec{s}_k)\}$
  \Procedure{Select}{$\set{T}, \vec{x}_0, p_{\text{sel}}$}
        \LineComment{has access to $B_{\text{sel}}$}
        \State prompt LLM to select $B_{\text{sel}}$ different multi-premise selections $\vec{s}$ from the theory $\set{T}$
        \LineComment{prompt templates are in \cref{app:llm}}
        \State each selection $\vec{s}$ is assigned a score $u = 0$
        \State construct list $\set{S}$ to contain the multiple $(u, \vec{s})$
  \State \textbf{return} $\set{S}$
  \EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Deduction Subroutine for LLM}\label{alg:dedllm}
\begin{algorithmic}[1]
        \HYPER deduction beam size $B_{\text{ded}}$
	\INPUT current selection $\vec{s}$ of statements;\newline 
                deduction model $p_{\text{ded}}$ 
        \OUTPUT deductions with their scores $\{(v_k, \vec{y}_k)\}$
  \Procedure{Deduce}{$\vec{s}$, $p_{\text{ded}}$}
    \LineComment{has access to $B_{\text{ded}}$}
    \State prompt LLM to draw $B_{\text{ded}}$ new deductions
    \LineComment{prompt templates are in \cref{app:llm}}
    \State each deduction $\vec{y}$ is assigned a score $v = 0$
    \State construct list $\set{Y}$ to contain the multiple $(v, \vec{y})$
    \State \textbf{return} $\set{Y}$
  \EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Planning for Selection}\label{alg:plan-sel}
\begin{algorithmic}[1]
    \HYPER deduction beam width $B_{\text{ded}}$; \newline
    depth of planning $D$; planning scale $\alpha$
    \INPUT current theory $\set{T}=\{\vec{x}_1,\ldots,\vec{x}_{N+m}\}$ and goal $\vec{x}_0$; verification model $p_{\text{ver}}$ \newline
    selection candidates at current step $\set{S} = \{(u_k, \vec{s}_k)\}$;\newline
    selection model $p_{\text{sel}}$ and deduction model $p_{\text{ded}}$
    \OUTPUT selections with updated scores $\{(u_k, \vec{s}_k)\}$
  \Procedure{PlanS}{$\set{T}, \vec{x}_0, \set{S}, p_{\text{sel}}, p_{\text{ded}}, p_{\text{ver}}$}
    \LineComment{has access to $B_{\text{ded}}$, $D$, $\alpha$}
    \LineComment{init hypothetical extended theory}
    \For{$u_k, \vec{s}_k$ {\bfseries in} $\set{S}$}
        $\tilde{\set{T}}_{k} \gets \set{T}$ 
    \EndFor
    \For{$u_k, \vec{s}_k$ {\bfseries in} $\set{S}$}      
        \LineComment{iterate over all candidate selections}
        \LineComment{find hypothetical next-step deduction}
        \State $\tilde{\vec{y}}_{k} \gets \textsc{OneBestDeduce}(\vec{s}_k, p_{\text{ded}})$ %
        \LineComment{extend theory with new deduction}
        \State $\tilde{\set{T}}_{k} \gets \tilde{\set{T}}_{k} + \{\tilde{\vec{y}}_{k}\}$ 
        \LineComment{planning with roll-outs}
        \LineComment{what's given by \textsc{RollOut} is $\Delta u$ in \cref{sec:infplan}}
        \State $u_k \gets$ $u_k\ +
        \alpha$ \textsc{RollOut} 
    \EndFor
    \State sort $\set{S}$ in descending order of updated $u_{k}$
    \State \textbf{return} $\set{S}$
  \EndProcedure
  \Procedure{RollOut}{}
  \LineComment{roll out $D$ steps of imaginary selection and deduction}
  \LineComment{make in-place edits to $\tilde{\vec{s}}_k, \tilde{\vec{y}}_k, \tilde{\set{T}}_k$}
  \State $f \gets -\infty$ \Comment{init score of roll-out}
  \For{$d = 1$ {\bfseries to} $D$}
        \Comment{step-by-step roll-out}
        \State $\tilde{\vec{s}}_k \gets \textsc{OneBestSelect}(\tilde{\set{T}}_k, \vec{x}_0, p_{\text{sel}})$
        \State $\tilde{\vec{y}}_k \gets \textsc{OneBestDeduce}(\tilde{\vec{s}}_k, p_{\text{ded}})$
        \State $\tilde{\set{T}}_k\gets \tilde{\set{T}}_k + \{\tilde{\vec{y}}_k\}$
        \IfThen{$p_{\text{ver}}(\vec{x}_0 \mid \tilde{\vec{y}}_{k}) > f$}{$f \gets p_{\text{ver}}(\vec{x}_0 \mid \tilde{\vec{y}}_{k})$}
    \EndFor
    \State {\bfseries return}  $\log f$
  \EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Planning for Deduction}\label{alg:plan-ded}
\begin{algorithmic}[1]
        \HYPER deduction beam width $B_{\text{ded}}$; \newline
        depth of planning $D$; planning scale $\beta$
        \INPUT current theory $\set{T}=\{\vec{x}_1,\ldots,\vec{x}_{N+m}\}$ and goal $\vec{x}_0$; 
        deduction candidates at current step $\set{Y} = \{(v_k, \vec{y}_k)\}$; 
        selection model $p_{\text{sel}}$ and deduction model $p_{\text{ded}}$; 
        verification model $p_{\text{ver}}$
        \OUTPUT deductions with updated scores $\{(v_k, \vec{y}_k)\}$
  \Procedure{PlanD}{$\set{T}$, $\vec{x}_0$, $\set{Y}$, $p_{\text{sel}}$, $p_{\text{ded}}$, $p_{\text{ver}}$}
    \LineComment{has access to $B_{\text{ded}}$, $D$, $\beta$}
    \LineComment{init hypothetical extended theory}
    \For{$v_k, \vec{y}_k$ {\bfseries in} $\set{Y}$} 
        $\tilde{\set{T}}_k \gets \set{T}$ 
    \EndFor
    \For{$v_k, \vec{y}_k$ {\bfseries in} $\set{Y}$}   
        \LineComment{iterate over all candidate deductions}
        \State $\tilde{\set{T}}_k\gets \tilde{\set{T}}_k + \{\vec{y}_{k}\}$ \Comment{extend theory with deduction}
        \State $v_k \gets$ $v_k\ + \beta$ \textsc{RollOut} 
        \LineComment{\textsc{RollOut} is in \cref{alg:plan-sel}}
        \LineComment{what's given by \textsc{RollOut} is $\Delta v$ in \cref{sec:infplan}}
    \EndFor
    \State sort $\set{Y}$ in descending order of updated $v_{k}$
    \State \textbf{return} $\set{Y}$
  \EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Details of Planning-Based Methods}\label{app:plan}
\cref{alg:plan-sel} illustrates the details of how we use explicit planning for selection. The method considers how each selection could affect future in $D$ steps. One-best search is applied in the roll-out process to simplify the planning. Intuitively, a higher $\log f$ means that the future reasoning path conditioned on this selection is more likely to prove the goal.
Similar to \cref{alg:plan-sel}, \cref{alg:plan-ded} measures how the newly generated deduction could affect the future reasoning path in $D$ steps, and honors the deduction which improves the possibility of proving the goal in the future.


\section{Experiment Details}\label{app:expdetail}
We present experiment details in this section. 

\subsection{Data Statistics}
The data statistics of Entailment Bank is shown in \cref{tab:entailmentbankstats}. In Version-I of Entailment Bank, there is one sample in the test set that has a theory with a single statement. We ignore this sample since it can not be dealt by our system in the normal way. The dataset can be downloaded from {\small \url{https://allenai.org/data/entailmentbank}}.
\begin{table}[H]
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\toprule
Split & \# of samples & max steps & avg steps  \\
\midrule
Train & 1313 & 17 & 3.2 \\
Dev & 187 & 15 & 3.2 \\
Test   & 340 & 11 & 3.3 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\caption{Data statistics of Entailment Bank.}
\label{tab:entailmentbankstats}
\end{center}
\vskip -0.1in
\end{table}

\begin{figure*}
     \centering
     \begin{center}
     \begin{subfigure}[b]{0.32\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figures/low-data/roc_fig_test_657.pdf}
         \vspace{-16pt}
        \caption{ROC curves.}
        \label{fig:roc0.5}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figures/low-data/true_fig_test_657.pdf}
         \vspace{-16pt}
        \caption{Acc curves on positive examples.}
        \label{fig:accposi0.5}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figures/low-data/false_fig_test_657.pdf}
         \vspace{-16pt}
        \caption{Acc curves on negative examples.}
        \label{fig:accneg0.5}
     \end{subfigure}

    \vspace{-8pt}
    \caption{Test results with 95\% bootstrap CFs on Entailment Bank Version-I under 50\% training data.}
    \label{fig:expcurves0.5}
    \end{center}
    \vspace{-4pt}
\end{figure*}
\begin{table*}[t]
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
Method & AUROC & $\text{AUACC}_{\text{pos}}$ & $\text{AUACC}_{\text{neg}}$ & F1  \\
\midrule
Base System (t5-base) & 0.73 (0.69, 0.77) & 0.61 (0.56, 0.65) & 0.81 (0.79, 0.84) & 0.68 (0.66, 0.71)\\
System A (t5-base)    & 0.91 (0.89, 0.93) & 0.90 (0.88, 0.92) & 0.62 (0.58, 0.65) & 0.85 (0.83, 0.87)\\
System B (t5-base)    & 0.94 (0.93, 0.96) & 0.89 (0.87, 0.91) & 0.84 (0.80, 0.87) & 0.90 (0.88, 0.91)\\
\midrule
Base System (denoise) & 0.55 (0.50, 0.59) & 0.39 (0.35, 0.43) & 0.83 (0.80, 0.85) & 0.67 (0.67, 0.67)\\
System A (denoise)    & 0.88 (0.85, 0.90) & 0.87 (0.85, 0.89) & 0.55 (0.52, 0.59) & 0.83 (0.81, 0.85)\\
System B (denoise)    & 0.93 (0.91, 0.95) & 0.83 (0.80, 0.86) & 0.88 (0.85, 0.90) & 0.85 (0.84, 0.86)\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\caption{Test results with 95\% bootstrap CFs on Entailment Bank Version-I. }
\label{tab:sizedenoise}
\end{center}
\vskip -0.1in
\end{table*}
\begin{table*}[!htb]
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
Method & Version-I & Version-II \\
\midrule
Base System (t5-base) & 0.65 (0.60, 0.70) & 0.26 (0.22, 0.31)\\
System A (t5-base)    & 0.88 (0.85, 0.91) & 0.45 (0.39 ,0.50)\\
System B (t5-base)    & 0.91 (0.88, 0.94) & 0.55 (0.50, 0.60)\\
\midrule
Base System (denoise) & 0.46 (0.40, 0.52) & 0.27 (0.22, 0.32)\\
System A (denoise)    & 0.83 (0.80, 0.87) & 0.40 (0.35, 0.46)\\
System B (denoise)    & 0.90 (0.87, 0.93) & 0.67 (0.62,0.72)\\
\midrule
Base System (version-II) &-&  0.49 (0.44, 0.54)\\
System A (version-II)   &-& 0.73 (0.69, 0.78)\\
System B (version-II)   &-& \textbf{0.80} (0.76, 0.84)\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\caption{Test accuracy with 95\% bootstrap CFs in multiple-choice QA. A random guess gives 25\% accuracy. The systems in the third block were trained on Version-II training data.}
\label{tab:sizedenoise2}
\end{center}
\vskip -0.1in
\end{table*}


\subsection{Hyperparameters}\label{app:hyperpara}
For SLM experiments, we use ``t5-small'' in the Huggingface transformers~\citep{huggingface} library for the selection and deduction models. We use ``deberta-v2-xlarge-mnli'' for the verification model. We prompt tune these models, with a prompt length of 4 for the selection and deduction models, and a prompt length of 32 for the verification model. Note that for T5 models, the prompt is added to the beginning of both the encoder and the decoder (weight not shared). For the selection model, a layernorm is added before the sigmoid operation.

In training, we use the Adam~\citep{kingma2014adam} optimizer with $\beta_1=0.9,\beta_2=0.999,\epsilon=1e-8,\lambda=0$. We use learning rate $\gamma=0.1$ for the T5 models, and $\gamma=0.01$ for the verification model. We use a batch size of 16. We set a very large epoch number like 1000 and use the validation set to do early stopping. In practice, the best epoch is often within 100.

For LLM experiments, we use GPT-3.5-turbo model provided by OpenAI. We set the temperature to reduce randomness. We keep the default role ``system'' with the message ``You are an AI assistant that speaks English.''

We used a fixed random seed for all our data generation and training, so that our results can be easily reproduced with our codes.

During inference, we set $B_{\text{inf}}=B_{\text{ded}}=5$ and retain the selections formed by 4 top-scored statements. We set $\alpha=10$ and $\beta=0.5$ to roughly match the scale of the beam score. We roll out 3 steps for selection and 2 steps for deduction. We set the maximum step to be $M=20$. 

We do not tune hyperparameters except the learning rate, and we only tune it in our first training of every model. We try [0.1, 0.01, 0.001, 0.0001] and choose the one that yields the best dev set performance.


Our experiments were run on 8 A6000 GPUs. 
Training takes about 1 hour. 
Time for inference is discussed in \cref{sec:binary}. 





\subsection{Details of FOL Translations}\label{app:fol}
The classical approach of logical reasoning is to use formal logic systems. 
So we also evaluated the performance of a first-order-logic (FOL) system. 
Because the Entailment Bank dataset does not have human-annotated FOL translations for the natural language statements, we translated all the statements into FOL expressions using a T5 model trained on the corpus of (natural language, FOL) pairs collected by~\citet{levkovskyi2021generating}, and then used a FOL engine to perform reasoning. 
This approach failed because the FOL translations are mostly of very poor quality. 
Here is a summary of the errors: 
\begin{itemize}[leftmargin=*]%
    \item inconsistency in variable naming. The FOL translations often use inconsistent variable naming, making it difficult to pattern-match relevant expressions.
    \item incorrect translations. Some FOL translations inaccurately represent the original sentences, resulting in a failure to capture the intended meaning. For example, ``driving is a kind of skill'' is incorrectly translated into ``$\exists x.(\text{driving}(x) \& \exists y.(\text{vehicle}(y) \text{kind}(x,y)))$.
    \item syntax errors. Some FOL translations contain syntax errors, making them difficult to be process.
    \item missing or incomplete information. In several instances, the FOL translations do not capture all relevant information from the original sentences. For example, it may leave out an entity or quantifier. 
\end{itemize}
This analysis reveals a fundamental need for tools that work directly with natural language statements for reasoning like ours. 







\subsection{Results of Ablation Studies}\label{app:ablation}
\cref{fig:expcurves0.5} shows the results of the systems trained on 50\% training data. 
Some results of ablation studies described in \cref{sec:binary} are shown in \cref{tab:sizedenoise} and \cref{tab:sizedenoise2}.%


\subsection{Examples of Prompts for GPT-3}\label{app:examplegpt3}
In \cref{sec:mcqat5}, we used three kinds of prompts for GPT-3: 0-shot, 5-shot and COT. In this section, we provide some examples of these prompts.
\begin{table*}%
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\toprule
Method & Depth=1 & Depth=3 & Depth=5 \\
\midrule
System A with modified proof score & {0.92} (0.87, 0.97) &{0.81} (0.73, 0.89) &{0.70} (0.61, 0.79)\\
System A with original proof score & 0.88 (0.82, 0.94) &{0.79} (0.71, 0.87) &0.74 (0.65, 0.83)\\
System B trained on Entailment Bank & 0.91 (0.85, 0.97) & 0.81 (0.73, 0.89) & 0.73 (0.64, 0.82) \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\caption{Results of ablation studies on PrOntoQA with 95\% bootstrap CFs.}
\label{tab:prontoqaablabtionstudy}
\end{center}
\vskip -0.1in
\end{table*}

An in-context demonstration is
\begin{lstlisting}
Based on the statements that:
the earth rotating on its axis causes stars / the moon to appear to move across the sky at night. 
diurnal motion is when objects in the sky appear to move due to earth 's rotation on its axis. 
stars appear to move relative to the horizon during the night.

Which of the following conclusions can be inferred?
0. earth rotating on its axis causes horizon of stars and night on earth.
1. earth 's horizon on its rotating axis causes stars to occur in new york night.
2. the earth revolving around the axis causes stars to appear in different night in the sky at different horizon of year.
3. the earth rotating on its axis causes stars to appear to move relative to the horizon during the night.

A: 3.
\end{lstlisting}

For COT prompting, we used the ground-truth reasoning path for the correct choice as the ``chain-of-thought'', so the last line of the (say) above example will be: 
\begin{lstlisting}
Reason: diurnal motion is when objects in the sky appear to move due to earth 's rotation on its axis & stars appear to move relative to the horizon during the night -> int1: stars appearing to move relative to the horizon during the night is an example of diurnal motion; int1 & the earth rotating on its axis causes stars / the moon to appear to move across the sky at night -> the earth rotating on its axis causes stars to appear to move relative to the horizon during the night.

A:3. 
\end{lstlisting}

\subsection{Experiment Details on PrOntoQA}\label{app:prontoqa}\label{app:llmexp}
The PrOntoQA data has three subsets of different ``depths''. 
The ``depth'' denotes the number of ground-truth reasoning steps so a ``deeper'' subset is harder. 
For each depth, we draw (using the released data generation code of \citet{SaparovHe22}) 5 training examples and 100 test examples. 


For the experiments on PrOntoQA, our final verification is performed by a few-shot-prompted GPT-3.5: it reads the reasoning path and judges whether the given goal is proved. 
By doing this, we do not need to tune a threshold for the proof scores given by the verification model (although those scores are still very important in the process of explicit planning). 
In this dataset, the non-provable goals are often definitively disapprovable. 
So we would like the explicit planning to favor not only the future steps that have large proof scores but also those of large \emph{contradiction} scores. 
Therefore, we replace the proof score $f$ in the planning procedure by the generalized score $g$ defined below 
\begin{align}
    g(\set{T}, \vec{x}_0)
    \defeq \max_{n} \max(p_{\text{ver}}(\vec{x}_0 \mid \vec{x}_n), p_{\text{con}}({\vec{x}}_0 \mid \vec{x}_n))
    \label{eqn:proofscorenew}
\end{align}
where $p_{\text{con}}({\vec{x}}_0 \mid \vec{x}_n)$ is the probability of ``$\vec{x}_n$ contradicts $\vec{x}_0$'' given by the pretrained DeBERTa. 
\cref{tab:prontoqaablabtionstudy} shows how this modification helps. 
For lower depths, using $g$ improves the performance.
For higher depths, using $g$ hurts the performance: in this case, signal-to-noise ratio is low and LLMs like to hallucinate incorrect deductions; $g$ may assign high scores to the hallucinatory output and thus hurts the overall performance. 
But this issue can be mitigated by using a better verification model. 
\cref{tab:prontoqaablabtionstudy} also shows the results of System B with the verification model trained on Entailment Bank data: this verification model successfully generalizes to out-of-domain data and improves the performance for the cases of ``depth=5''. 


In this section, we also show the prompts for GPT-3.5 used in the experiments in \cref{sec:gpt35}. 
For selection and deduction, we employed 5-shot prompting to enhance the model's comprehension. 
An in-context training example for 5-shot selection prompt is
\begin{lstlisting}
Based on the facts:
0.Every tumpus is not earthy.
1.Wumpuses are not red.
2.Wumpuses are vumpuses.
3.Each vumpus is bitter.
4.Vumpuses are zumpuses.
5.Every zumpus is cold.
6.Zumpuses are numpuses.
7.Numpuses are aggressive.
8.Numpuses are dumpuses.
9.Dumpuses are opaque.
10.Dumpuses are yumpuses.
11.Yumpuses are not small.
12.Each yumpus is a rompus.
13.Every rompus is earthy.
14.Each rompus is a jompus.
15.Jompuses are metallic.
16.Each jompus is an impus.
17.Alex is a dumpus.
Question: True or false: Alex is not earthy.

Answer:
Alex is a dumpus.
Dumpuses are opaque.
Dumpuses are yumpuses.
\end{lstlisting}


An in-context example for deduction prompt is
\begin{lstlisting}
Based on the facts:
Sally is a tumpus. Each tumpus is hot. 
Answer: 
Sally is hot.
\end{lstlisting}

We didn't let GPT to propose multiple deductions in this experiment because the no-planning deduction is almost always correct as long as the selection is correct. 





\end{document}