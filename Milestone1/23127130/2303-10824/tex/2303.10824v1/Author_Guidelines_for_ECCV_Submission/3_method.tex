\subsection{Overview of $k$-SALSA}

We consider a dataset $D$ of retinal fundus images that the user wishes to release in a privatized form. We assume that the user has access to an auxiliary dataset $D_0$ that can be used to pre-train the GAN components of $k$-SALSA.
Note that $D_0$ could simply be a publicly available dataset like the ones we used in our work or a subset of $D$.
This choice does not affect the  $k$-anonymity property of $k$-SALSA.
Given these datasets, $k$-SALSA proceeds in four steps:
\begin{enumerate}
    \item \textbf{Pre-training.} We first train a GAN on $D_0$.
Let $G(\cdot)$ be the trained generator, which maps a latent code $w \in \mathcal{W}$ to a synthetic image $G(w)$.
This step intuitively constructs the latent embedding space $\mathcal{W}$, which we will later rely on for the averaging operation.
Next, $k$-SALSA trains a GAN inversion model $E(\cdot)$ (can be viewed as an encoder), which maps an image $x\in \mathcal{X}$ to a particular latent code $E(x)\in \mathcal{W}$. Note that this inversion process is lossy in that the synthetic image $G(E(x))$ will only be approximately similar to the original image $x$, constrained by the limitations of encoder $E$ and generator $G$.  Together, these two functions approximate a bijection between the space of retinal images $\mathcal{X}$ and the latent embedding space $\mathcal{W}$.

    \item \textbf{Clustering.} Next, $k$-SALSA performs \emph{same-size clustering} of the target input dataset $D$ based on the inverted codes $E(x)$ for each $x\in D$, partitioning $D$ into groups of exactly $k$ similar images. Here $k$ is the user parameter determining the $k$-anonymity of the final output. If the size of $D$ is not divisible by $k$, one can disregard a small number of samples to resolve the issue, given that $k$ is typically small (e.g. 10). 
    \item \textbf{Averaging.} For each cluster, $k$-SALSA summarizes the $k$ source images as a single representative image via local style alignment---our new approach. This leads to $k$-anonymity, since each average image represents $k$ individuals as a whole and does not distinguish among them.
    \item \textbf{Release.}  Finally, $k$-SALSA constructs a $k$-anonymous synthetic dataset $\tilde{D}$ to release by associating each average image (one per cluster) with the aggregated labels of the $k$ images in the corresponding cluster (if labels were provided in the input dataset). This synthetic dataset can then be used for downstream analysis, such as training a classifier to predict the labels.
\end{enumerate}
A graphical illustration of our workflow is provided in Fig.~\ref{fig:overview}.


\begin{figure*}
    \centering
    \includegraphics[width=.89\textwidth]{fig_overview.png}
    
    \caption{\textbf{Workflow of $k$-SALSA.} 
  GAN generator and inversion encoder are first trained to be used in subsequent steps. Same-size clustering groups images into groups of $k$, then the representative of each cluster is optimized via our local style alignment approach to preserve salient visual patterns. Images are synthesized from the optimized averages and released. Avg: average, G: generator
  }
    \label{fig:overview}
\end{figure*}

\subsection{Local Style Alignment: Our New Approach to Image Averaging}
\input{algorithm}

While prior  $k$-Same approaches developed for facial images have considered the Euclidean average of latent codes within each cluster as the representative, we found that this straightforward approach leads to significant loss of detail in synthetic retinal images, where clinically relevant patterns such as hemorrhages and exudates are often omitted (see Fig.~\ref{fig:examples}).

We make the following two observations toward addressing this key limitation.
First, unlike facial images where the salient structural features (e.g. eyes and nose) generally appear in consistent regions within the image, which facilitates the disentanglement of latent features, important patterns in retinal images can appear in different areas and thus are more easily diluted when averaging the features in the latent space. Second, in contrast to the importance of shape information in facial images, the patterns of interest in retinal images that are not directly linked to personal identity tend to be associated with \emph{texture-level} information (e.g. colored dots of varying granularity and frequency). In fact, the geometric structure of the blood vessels is a prominent identifying feature of concern that we are interested in obfuscating in the image via averaging.

Our local style alignment technique takes advantage of these observations to obtain higher quality representative images of each cluster.
We first draw the connection between texture patterns of interest and the ``style'' of the image from the style transfer literature~\cite{Gatys16}. Since we are interested in local texture patterns in the image, we capture the \emph{local style features} by constructing the feature covariance matrix in a sliding window of image patches, rather than over the whole image. 
We then consider the correspondence of the local style features
between the source images in the cluster and the target representative image, allowing for source texture patterns to appear in different locations in the target and simultaneously enforcing that these patterns are recapitulated somewhere in the target image.
Optimizing the latent code for the representative image with respect to the augmented loss function considering both the local style features and general content similarity, we obtain an enhanced representative image for each cluster. We provide the details of each step below and in Algorithm~\ref{alg:overall}.

\subsubsection{Construction of local style features.}

Following the approach of style transfer, we view style and texture information as being captured by the cross-channel feature correlations in the intermediate layers of a pre-trained convolutional neural network (CNN), such as VGG19~\cite{Simonyan15}. 

Formally, let $F_{\text{source}}^{(i)}$ and $F_{\text{target}}$ be $n$-by-$n$-by-$c$ tensors for the $i$-th source image and the target representative, respectively, representing the activation output of an $n$-by-$n$ intermediate CNN layer across $c$ channels. Note that we use the second layer of VGG19 in all our experiments. We spatially partition the activation output into a grid of $p$ submatrices, $\{F_{\text{source},j}^{(i)}\}_{j=1}^p$ and $\{F_{\text{target},j}\}_{j=1}^p$, each corresponding to a local patch in the image. For each patch $j$, we define the local style features $S^{(i)}_{\text{source},j}$ and $S_{\text{target},j}$ as $c$-by-$c$ matrices, where
\begin{align}
    (S^{(i)}_{\text{source},j})_{u,v} := \langle \textsf{Vec}((F^{(i)}_{\text{source},j})_{:,:,u}), 
    \textsf{Vec}((F^{(i)}_{\text{source},j})_{:,:,v})
    \rangle\\
    (S_{\text{target},j})_{u,v} := \langle \textsf{Vec}((F_{\text{target},j})_{:,:,u}), 
    \textsf{Vec}((F_{\text{target},j})_{:,:,v})
    \rangle
\end{align}
and $\textsf{Vec}(\cdot)$ denotes vectorization,  $\langle\cdot,\cdot\rangle$ denotes dot product, and $M_{:,:,u}$ for a tensor $M$ denotes a slice corresponding to channel $u$. 
The sets $\{S_{\text{source},j}^{(i)}\}_{j=1}^p$ for each source image $i$ in the cluster and $\{S_{\text{target},j}\}_{j=1}^p$ for the target fully characterize the local texture information we aim to capture in our model.

\subsubsection{Alignment of local style features.}
To introduce flexibility in determining where a visual pattern from the source image appears in the target image, we quantify the agreement in local style features via a correspondence. 
Inspired by the recent work of Wang et al. on dense contrastive learning~\cite{Wang21}, we compute the cosine similarity of style features between every pair of patches between the source and the target and take the optimal match for each patch in the source image to be included in the loss function. This induces positional flexibility while penalizing complete omission of texture patterns from the source. Note that the prior work~\cite{Wang21} did not consider style information in their approach.

We define correspondence index $a(i,j)$ for patch $j$ in the source image $i$ as:
\begin{equation}
    a(i,j) := {\arg\max}_{j'\in \{1,\dots,p\}} \textsf{CosineSimilarity}(\textsf{Vec}(S^{(i)}_{\text{source},j}), \textsf{Vec}(S_{\text{target},j'})).
\end{equation}
It is worth noting that, while a na\"{i}ve implementation of all pairwise comparison of patches leads to significant runtime overhead, our implementation efficiently utilizes matrix operations to maintain computational efficiency.


\subsubsection{Optimization of representative images.}
To synthesize an informative representative image for each cluster, leveraging the correspondence of local style features, we frame the process as an optimization problem as follows.

Let $E(\cdot)$ and $G(\cdot)$ be the encoder of the pre-trained GAN inversion model and the pre-trained GAN generator, respectively.
We directly optimize the target latent code $w_\text{avg}$ whose corresponding image $G(w_\text{avg})$ is the desired representative of the cluster.

We initialize $w_\text{avg}$ to the baseline Euclidean average $w_0$ of the source image embeddings given by 
\begin{equation}
    w_0 := \frac{1}{k} \sum_{i=1}^k E(x^{(i)}),
\end{equation}
where $x^{(i)}$ denotes the $i$-th image in the cluster.
We then iteratively optimize the solution using gradient descent with the loss function
\begin{equation}
\label{eq:loss}
    \mathcal{L}_\text{total} = \lambda \mathcal{L}_\text{content} + (1-\lambda) \mathcal{L}_\text{style},
\end{equation}
where $\lambda$ determines the ratio between the two terms given by 
\begin{align}
        \mathcal{L}_\text{content} &= 1-\langle F(G(w_0)), F(G(w_\text{avg})) \rangle,  \label{eq:loss-content} \\
        \mathcal{L}_\text{style} &= \sum_{i=1}^k \sum_{j=1}^p \| S^{(i)}_{\text{source},j} -  S_{\text{target},a(i,j)} (w_\text{avg}) \|_\mathcal{F}^2. \label{eq:loss-style}
\end{align} 
Note that $\|\cdot\|_\mathcal{F}$ denotes the Frobenius norm, and $F(\cdot)$ represents a pre-trained encoder network, which we use to induce high-level similarity between the optimized representative and the Euclidean average to avoid degenerate cases and to prioritize refining of the baseline solution.
In our experiments, we set $F$ to the pre-trained MoCo network~\cite{He20}, which was trained on the ImageNet dataset~\cite{Fei09} via unsupervised contrastive learning.
MoCo is recognized for its effectiveness in capturing semantic information of natural images beyond the available labels in the original dataset.
We also note that, although style transfer approaches typically take many iterations to converge, our initialization scheme using the Euclidean average greatly simplifies this process, requiring fewer iterations to obtain the final solutions in our experiments.



\subsection{GAN-based Image Generation and Encoding}
We train a GAN model, StyleGAN2-ADA~\cite{Karras20b}, on retinal images to learn to generate realistic fundus images from a latent vector space.
The StyleGAN family of methods~\cite{Karras20b,Karras19,Karras20a} generate high-resolution images using a progressive architecture, where increasingly fine-grain details are added to the image as we get deeper into the network.
We consider the latent space associated with this network to be extended multi-scale $\mathcal{W}$, which modulates the activation of units in all layers of the generator hierarchy.

StyleGAN2-ADA is one of the latest in this class, which stabilizes training on limited data using the \emph{adaptive discriminator augmentation} (ADA) mechanism.
Likely because the size of the public retinal image datasets is small compared to other types of image datasets, we observed that the use of ADA leads to a considerable improvement in image quality. 
We also note that the existing work on GANs for retinal images (e.g.~\cite{Chen21,Niu19,Yu19,Zhou20}) leverages additional labeled information such as vessel segmentation, and thus are not directly applicable to our setting where we use the raw fundus images.  

To manipulate and summarize real retinal images in the latent space equipped with a generator to synthesize new images, we need an encoder to map a given image to the GAN latent space, a task known as GAN inversion~\cite{Xia21}.

In our framework, we use this encoder to invert every image in the input dataset, then use the latent codes both to define the clusters of size $k$ to be averaged and to find the Euclidean centroid for the cluster to use as an initialization point, as described in the previous sections.

To this end, we use ReStyle~\cite{Alaluf21}, a recently developed approach to GAN inversion which achieved a significant scalability improvement over the previous methods by adopting an iterative refinement approach.
The ReStyle model takes the target image and the current synthetic image (the result of inversion) as input, and learns to generate an update to the latent code that improves consistency between the two images.
For $k$-SALSA, adopting this approach was key to building a practical pipeline---it reduced the inversion time by an order of magnitude (from 80 to 3 seconds per image).


While the generator and the encoder individually draws from prior work, we note that the combination of these state-of-the-art techniques have not been previously studied in the context of privatizing retinal images and were in fact key enabling factors of the practical performance of $k$-SALSA in our experiments.


\subsection{Same-Size Clustering}


To partition the input dataset into groups of exactly $k$ images to average, we employ a greedy nearest neighbor clustering to the inverted latent codes of the input images.
At each iteration, a point with the maximum average distance to the rest of the dataset is chosen, with the goal of prioritizing outliers.
Then $k-1$ nearest neighbors of the chosen point are identified to form a new cluster of size $k$.
The points in the new cluster are removed from the dataset and the above process is repeated.
In our experiments, we downsample the dataset to a multiple of $k$ to avoid a leftover cluster smaller than $k$.
Our experiments show the effectiveness of this efficient clustering approach in downstream tasks.
