
\begin{figure*}
    \centering
    \includegraphics[width=.8\textwidth]{examples.png}
    %\vspace{-10pt}
    \caption{\textbf{Examples of synthetic average of retinal images.} 
    Two example clusters ($k=5$) of real retinal images (\emph{top}) along with synthetic averages generated by different methods (\emph{bottom}). $k$-SALSA better captures clinically relevant features such as hemorrhages (\emph{A}) and exudates (\emph{B} and \emph{C})}
    \label{fig:examples}
    %\vspace{-1.7em}
\end{figure*}

\subsection{Benchmark Datasets and Evaluation Setting}


Our experiments are conducted on public fundus image datasets APTOS\footnote{\url{https://www.kaggle.com/c/aptos2019-blindness-detection}} and EyePACS\footnote{\url{https://www.kaggle.com/c/diabetic-retinopathy-detection}}, widely used for diabetic retinopathy (DR) classification.
The images in both datasets are labeled by ophthalmologists with five grades of DR based on severity: 0 (normal), 1 (mild DR), 2 (moderate DR), 3 (severe DR), and 4 (proliferative DR).
EyePACS images were acquired from different imaging devices, leading to variations in image resolution, aspect ratio, intensity, and quality.
Hence, EyePACS represents a more challenging evaluation setting.

For both datasets, we train the GAN generator and the GAN inversion model on the training set (see supplement for details).
We then apply $k$-SALSA to the training set with the pre-trained GAN models to generate a $k$-anonymous dataset of synthetic images with aggregated labels.
To evaluate the downstream utility of the synthetic dataset, we trained DR classifiers based on the synthetic images and evaluated the classifiers on the test set using real images and labels.

% \vspace{-1.2em}
\subsection{Baseline Approaches}
% \vspace{-0.7em}
We compare $k$-SALSA with the following baseline methods. 
To demonstrate the advantage of our novel local style alignment-based averaging scheme, we consider the same method as $k$-SALSA, except using the Euclidean average (centroid) of each cluster in our GAN latent space to generate the representative image (\textbf{$k$-Centroid}).
To illustrate the importance of GANs in synthetic averaging of retinal images, we also evaluate less sophisticated schemes based on pixel-wise averaging  (\textbf{$k$-Same-Pixel}) and averaging in the low-dimensional latent space obtained by principal components analysis (\textbf{$k$-Same-PCA}).
Note that $k$-Same-PCA is equivalent to the method proposed in the original work on $k$-Same algorithms~\cite{Newton05}, and $k$-Centroid represents the best achievable performance following the general framework of $k$-Same-Net~\cite{Meden18} leveraging our GAN approaches.
We applied all averaging methods to the same set of clusters we constructed using the latent space of $k$-SALSA.
For some comparisons, we also consider the performance based on the non-averaged synthetic images generated from the inversion of each original image, i.e. $G(E(x))$ given an image $x$ (\textbf{GAN-Inverted}).

% \vspace{-1em}
\subsection{Fidelity of Synthetic Images}
% \vspace{-0.3em}
To evaluate the visual quality of synthetic retinal images, we first compare the Fr\'{e}chet inception distance (FID)~\cite{Heusel17}, a standard performance metric for images generated using GANs, across the methods we considered. Note that, unlike pixel-wise metrics such as PSNR and SSIM~\cite{wang2004image} (see supplement for additional discussion), FID measures the divergence between the multivariate Gaussian distributions induced by the real vs. synthetic images in the activation of the Inception V3 model~\cite{Szegedy16} trained on ImageNet~\cite{Fei09}. 
Intuitively, FID quantifies how different the synthetic images are from a reference set of real images in a manner that reflects human visual perception.
We use the original retinal images from each dataset as the reference to calculate FID on the corresponding synthetic images.
As shown in Table~\ref{table:fidelity}, $k$-SALSA consistently obtains the best (the lowest) FID among all averaging methods for different values of $k$ (2, 5, and 10) on both datasets.
For all methods, averaging leads to worse FID relative to GAN-Inverted images, with the gap increasing as $k$ becomes larger. This suggests that generating a realistic image becomes more difficult as we average more images.
Nevertheless, $k$-SALSA's FID remains closest to GAN-Inverted even for $k=10$.
\input{Author Guidelines for ECCV Submission/Tables/table1}

In Fig.~\ref{fig:examples}, we provide examples of synthetic averages generated by different methods for $k=5$ for visual comparison.
$k$-SALSA images more clearly capture the fine-grain, clinically-relevant patterns in the source images, including exudates (appearing as grainy yellow patches) and hemorrhages (appearing as dark spots), both of which are well-established biomarkers of diabetes~\cite{Mohamed07}.
$k$-Centroid generates realistic images, but tend to omit important fine-grain patterns, which initially motivated this work.
$k$-Same-Pixel and $k$-Same-PCA lead to low-fidelity images that even fail to align the boundaries of the photographs due to their linearity.
Examples for other values of $k$ are provided in the supplement.
 
 
 
\subsection{Downstream Classification Performance}

In addition to generating more realistic and informative summaries of each cluster of retinal images, we are interested in enabling downstream analysis with our synthetic data. We tested whether $k$-SALSA's synthetic dataset can lead to accurate classifiers of clinical labels, in our case the grading of diabetic retinopathy (DR). In EyePACS, we evaluated normal vs. DR binary classification due to the highly imbalanced number of labels (in contrast to the five-class setting in Kaggle).
We tested multi-class prediction with all five labels in APTOS.

The number of images in the training set was 3,000 for APTOS and 10,000 for EyePACS, where the latter was subsampled for efficiency. For each set, we used our clustering approach to obtain same-size clusters for each of $k\in \{2,5,10\}$, which were then individually averaged to obtain training images for a classifier.

We also evaluated the classifiers trained on original or GAN-Inverted images, which were subsampled to the same number of training examples as the synthetic datasets for each $k$ for comparison. We provide experimental details and a comparison without subsampling in the supplement.

\input{Author Guidelines for ECCV Submission/Tables/table0}

The results in Table~\ref{table:classification} show that the $k$-SALSA synthetic datasets generally outperform the alternative approaches with respect to both accuracy and Cohen's $\kappa$ statistic (with quadratic weighting) on the test set.
$k$-Centroid achieves slightly better performance for $k=2$, but remains comparable to our approach.
Since Euclidean averaging may introduce greater distortions for larger values of $k$, we expect the advantage of $k$-SALSA to be more pronounced for moderate to large $k$, which is consistent with our results.
As expected, performance based on original images is higher than the synthetic dataset, but part of this gap is ascribed to the limitations of the current GAN models as suggested by the lower performance of the non-averaged, GAN-inverted images compared to the original.
Interestingly, for EyePACS $k = 10$, $k$-SALSA outperforms GAN-Inverted, suggesting that summarizing salient features may even be beneficial for classification when the data is limited.

We include in the supplement additional results illustrating the impact of $k$ and a promising extension of $k$-SALSA which uses data augmentation to mitigate dataset reduction due to averaging.


% \vspace{-1em}
\subsection{Mitigation of Membership Inference Attacks}
% \vspace{-0.3em}

To compare the privacy properties of the methods, we implemented a membership inference attack (MIA), where an adversary holding a synthetic dataset attempts to infer whether a target person was part of a specific cluster. 
We trained ResNet18~\cite{He16} on the training set to classify cluster membership using the synthetic averages generated by each method. We then evenly divided the test set into two parts, generated cluster averages on the first, then evaluated the performance of the classifier in ranking the images in \emph{both} test sets for membership in each cluster, based only on its synthetic average.
For each cluster size $k$, we calculated the top-$K$ accuracy (i.e., mean fraction of top $K$ samples in the ranking that correspond to correct guesses) with $K=k$.
% \vspace{-2em}
\input{Author Guidelines for ECCV Submission/Tables/table3}
% \vspace{-2em}
The results are summarized in Table~\ref{table:privacy}.
Note that an adversary with access to the encoder would achieve an expected accuracy of 50\% for all values of $k$, since in any neighborhood a random half of the samples correspond to negative matches that were not included in the private dataset. 
This represents a realistic scenario where the attacker does not have \emph{a priori} knowledge of individuals in the private dataset.
For a worst-case evaluation, we assume that the target's image is identical to the one in the dataset; any protection offered by noisy re-acquisition of images is likely to be bypassed with more sophisticated MIA (e.g., using vessel structures).
We observed that pixel-averaging provides little to no privacy.
$k$-Same-PCA and $k$-Centroid lower the risks, the latter to a greater extent.
$k$-SALSA results in the strongest mitigation with MIA accuracy of 1\% and 0.52\% for APTOS and EyePACS, respectively, for $k=10$.
Our improvement over $k$-Centroid is likely due to the fact that the addition of our local style loss prioritizes similarity in high-level visual patterns over low-level content, potentially reducing the amount of identity-related information that can be exploited by the attack.
None of the methods provide strong privacy at $k=2$, which reflects an insufficient amount of variability between the two source images that could be leveraged for privacy; however, we expect our approach to provide meaningful privacy protection for larger values of $k$ as our results show.


\subsection{Ablation Study}
We conducted an ablation study to evaluate the importance of individual components of our methodology.
Recall that the loss function of $k$-SALSA includes the content loss and the local style loss (see Eq.~\ref{eq:loss}).
We considered four alternative models with: only style loss, only content loss, both but using global style features computed over the whole image, and both without the flexible alignment (i.e., each local style is directly compared to that of the corresponding patch in the other image at the same location).
All of these alternatives performed considerably worse than $k$-SALSA in downstream classification performance (Table~\ref{table:ablation}).
The especially poor performance without alignment suggests that enforcing style preservation without spatial flexibility can in fact be harmful for the method.
\input{Author Guidelines for ECCV Submission/Tables/table2}
