\chapter*{Supplementary Materials}

\section{Benchmark datasets}
The APTOS dataset includes 3,662 labeled fundus images. 
We split the data into a training set of 3,000 images and a test set of 662 images.
The EyePACS dataset includes 35,126 labeled fundus images.
We split the data into a training set of 28,100 images and a test set of 7,026 images.
We rescaled the images in both datasets to 512-by-512 RGB pixels.
The training set is used to obtain the GAN and GAN-Inversion models.

\section{Implementation details}

\subsubsection{GAN.}
We trained our GAN models using the official PyTorch implementation\footnote{\url{https://github.com/NVlabs/stylegan2-ada-pytorch}} of StyleGAN2-ADA~\cite{Karras2020ada}. We set the number of mapping networks to two as recommended based on our image resolution and GPU count. Since the desired size of the generated images is $512 \times 512$, we used 16 progressive layers, resulting in the latent space $W$ with dimensions $16 \times 512$. We train the models on 5,000 kimgs in each dataset with batch size 64, using 8 3090-RTX GPUs, Pytorch 1.7.1, CUDA 11.1, and CuDNN 8.1.1.

\subsubsection{GAN Inversion.}
To obtain the GAN inversion encoder we built upon the official PyTorch implementation\footnote{https://github.com/yuval-alaluf/restyle-encoder} of ReStyle~\cite{Alaluf21}. We incorporated the MOCO-based~\cite{He20} similarity loss on pSp~\cite{richardson2021encoding} architecture with ResNetBackboneEncoder~\cite{he2016deep}. We trained each model for 100,000 iterations with a batch size of 8 and 5 refinement iterations per batch. Similar to the GAN setting, the output image size is set to $512\times 512$. We performed the training using 1 3090-RTX GPU with the same environment as that of GAN training.

\subsubsection{$k$-SALSA.}
We split the intermediate-level features into a grid of $4 \times 4$ patches (16 in total) to construct the local style features in all settings.
For the relative ratio of $\mathcal{L}_{\text{content}}$ and $\mathcal{L}_{\text{style}}$, we set the parameter $\lambda$ to 0.1, 0.05 and 0.03 for $k=2,5,10$, respectively, in APTOS. For EyePACS, we set $\lambda$ to 0.01, 0.02 and 0.01, respectively for each $k$.
We optimize our model for synthetic averaging using standard stochastic gradient descent with Adam~\cite{kingma2014adam}, with learning rate 0.1 and $\beta_1 = 0.9$, $\beta_2 = 0.99$. We use the same computational environment as above with a single GPU.

\subsubsection{Downstream classification.}
For all synthetic datasets, we trained a DR classifier using the ResNet50 model~\cite{He16} with batch size 32, 60 epochs, stochastic gradient descent (SGD) with Nesterov momentum 0.9~\cite{Nesterov03}, weight decay 0.0005, and cosine annealing in the learning rate schedule~\cite{Loshchilov16}.

\section{Computational costs}
One-time pre-training of GAN and inversion models took 10~hrs and 3~days, respectively, for APTOS, and  30~hrs and 3~days for EyePACS. %Once these models are trained, they can be re-used.
The main runtime of $k$-SALSA depends on the inference speed of GAN and inversion, only around 0.85~secs/image.
Synthetic averaging takes 19~secs/cluster ($k$=5).
Both steps can be parallelized.
Same-size clustering takes $<$1~min.
Cosine similarity is computed for 16 patches/image (4x4) for 2~ms/image.
Overall, we expect $k$-SALSA to be practical in realistic settings. 
% torch.optim.Adam([w_opt], betas=(0.9, 0.999), lr=initial_learning_rate), initial_learning_rate=0.1
% lr schedule
% We also set $\lambda_1$ as 1e2,  $\lambda_2$ as 1e-6 which is the ratio of $\mathcal{L}_{\text{content}}$ and $\mathcal{L}_{\text{style}}$, respectively.
