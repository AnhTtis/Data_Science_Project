% Version 1.2 of SN LaTeX, November 2022
%
% See section 11 of the User Manual for version history 
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst, sn-mathphys.bst. %  

\documentclass[pdflatex,sn-mathphys,Numbered]{sn-jnl}% Math and Physical Sciences Reference Style





%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}% 
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{cleveref}
%%%%


\newcommand{\R}{\mathbb{R}}
\newcommand{\DRE}{\varphi_{\gamma}^{\mbox{\small{DR}}}}
\newcommand{\x}{\sigma}
\DeclareMathOperator*{\argmin}{arg\,min} \newcommand{\prox}{\mbox{prox}}

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%\jyear{2021}%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
%\newtheorem{theorem}{Theorem}%  meant for continuous numbers
\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{definition}[theorem]{Definition} \newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}[theorem]{Assumption} 

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
%\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Convergence rate of nonconvex Douglas-Rachford splitting via merit functions, with applications 
to weakly convex constrained optimization]{Convergence rate of nonconvex Douglas-Rachford splitting via merit functions, with applications 
to weakly convex constrained optimization}

\author*[1]{\fnm{Felipe} \sur{Atenas}}\email{f262900@dac.unicamp.br.}
\affil*[1]{\orgdiv{Instituto de Matemática, Estatística e Computação Científica}, \orgname{Universidade Estadual de Campinas}, \orgaddress{\street{ Rua Sérgio Buarque de
Holanda, 651}, \city{Campinas}, \postcode{13083-859}, \state{Campinas, SP}, \country{Brazil}}}

%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%

\abstract{We analyze Douglas-Rachford splitting techniques applied to solving weakly convex optimization problems. Under mild regularity assumptions, and by the token of a suitable merit function, we show convergence to critical points and local linear rates of convergence. The merit function, comparable to the Moreau envelope in Variational Analysis,  generates a descent sequence, a feature that allows us to extend to the non-convex setting arguments employed in convex optimization. A by-product of
our approach is a ADMM-like method for constrained problems with weakly convex objective functions. When specialized to multistage stochastic programming,  the proposal yields a nonconvex version of the Progressive Hedging algorithm that converges with linear speed. The numerical assessment on a battery of phase retrieval problems shows promising numerical performance of our method, when compared to existing algorithms in the literature.
}


\keywords{Nonsmooth nonconvex optimization, weak convexity, Douglas-Rachford splitting, problem
decomposition, Progressive hedging}



%%\pacs[JEL Classification]{D8, H51}

\pacs[MSC Classification]{90C30, 90C15,  90C26, 49J52, 65K05, 65K10}

\maketitle



\section{Introduction and motivation}

Decomposition techniques are fundamental to
deal with complex systems represented by large-scale sophisticated model formulations. Decomposition can be achieved by separating problems in simpler
and smaller subproblems, depending on the involved variables and constraints, and
also on the number of possible outcomes. Separability can also stem from
structural properties, for instance,
splitting smooth and nonsmooth, or convex and nonconvex parts, in the objective
function of the problem of interest.
{Splitting methods have been successfully applied in signal processing, image processing, and machine learning,  see \cite{combettes2007douglas,cai2010split,combettes2011proximal,boyd2011distributed,glowinski2017splitting} for a few illustrations of applications. }

Operator splitting methods decompose complex structured problems into simpler
individual pieces. A solution of the original problem is obtained by iteratively
solving separate subproblems for each involved function, or more generally,
operator. Prominent instances suitable for
composite optimization are the Douglas-Rachford (DRS) and
the Peaceman-Rachford (PRS) splittings, the Alternating Direction Method of
Multipliers (ADMM), the Spingarn's partial inverse and the Forward-Backward
methods; we refer to
\cite{lions1979splitting,spingarn1983partial,eckstein1989splitting,boyd2011distributed,davis2016convergence}  
and references therein for details.
When applied to optimization problems, these methods were  originally studied for linear, and
more generally, convex programming.

The cornerstone of most operator splitting methods is the proximal point
algorithm (PPA), introduced by \cite{martinet-1970} and thoroughly
studied by \cite{rockafellar1976monotone} to find
a zero of a maximal monotone operator. In the context of convex optimization, for proper lower semicontinuous convex 
functions $f_1, f_2: \R^n \to \R \cup \{+\infty\}$, the problem boils down to
minimizing the composite function $f_1 + f_2$. As explained in 
\cite{eckstein1989splitting}, one DRS iteration (given in the scheme
\eqref{DR:scheme} below) amounts to applying the PPA with constant stepsize equal to $1$, to the auxiliary maximal monotone operator \begin{equation*}
    [\prox_{c f_1} \circ (2 \prox_{c f_2} - I) + ( I - \prox_{c f_2})]^{-1}- 1,
    \mbox{ with } c >0,
\end{equation*}
where $I$ is the identity map,
and the notation
$\prox_{c f}$ stands for the proximal point operator of the function $f$ as
defined in \eqref{def:prox} below.
{Thanks to this reformulation, DRS convergence rates can be derived from  those
available for the PPA. 

Classical approaches study the sum of two functions or
the sum of two monotone operators. More recently, extensions to the sum of $d
\ge 2$ functions/monotone operators have been proposed, see
\cite{eckstein2017simplified,malitsky2022resolvent} for some examples. Our approach is applicable to the sum of two terms, with further possible decomposition in the presence of appropriate separability structures, as in the numerical section \ref{section:numerical}.
Accordingly, we consider the following minimization problem \begin{equation}
\label{problem:primal} \min_{x \in \R^n} \varphi(x) = \varphi_1(x) +
\varphi_2(x), \end{equation}
where $\varphi_1, \varphi_2: \R^n \to \R \cup \{+\infty\}$ are lower
semicontinuous proper functions, not necessarily convex.

Following \cite{themelis2020douglas}, we examine relaxed DRS variants. Given a relaxation parameter $\lambda >0$, a stepsize $\gamma
>0,$ and an initial $s^0 \in \R^n$,
define one iteration of relaxed DRS as below: \begin{equation} \label{DR:scheme}  \left\{\begin{aligned} u^k  &=  \prox_{\gamma
\varphi_1}(s^k)& \\ v^k  &\in  \mbox{prox}_{\gamma \varphi_2}(2u^k - s^k)& \\
s^{k+1} & =  s^k + \lambda(v^k - u^k).& \end{aligned}\right.  \end{equation}
Note that for $\lambda =1$, scheme \eqref{DR:scheme} reduces to DRS, while for $\lambda =2$ it corresponds to PRS.  As stated, 
one iteration amounts to performing successively proximal steps, 
computed separately for each term in the sum, followed by a gradient step.

The iterative approach \eqref{DR:scheme} has a long history.  Lions and
Mercier in \cite{lions1979splitting}  studied convergence properties and speed
of convergence for splitting methods to find a zero of the sum of two maximally monotone operators
defined on a Hilbert space. When applied to the optimization problem
\eqref{problem:primal} for convex proper lower semicontinuous functions $\varphi_1$ and
$\varphi_2$, the corresponding operators are the subdifferentials of convex analysis, $\partial \varphi_1$ and $\partial \varphi_2$. 
Under mild regularity assumptions, the DRS sequence $\{s^k\}$
converges to some $s^{\star}$, for which  $u^{\star} = \prox_{\gamma
\varphi_1}(s^{\star})$ solves \eqref{problem:primal}, and both
$\{u^{k}\}$ and $\{v^{k}\}$ converge to $u^{\star}$ \cite[Theorem
3.15,Proposition 3.40]{eckstein1989splitting}.  Additionally, if $\varphi_1$ is
differentiable and strongly convex, with  Lipschitz
continuous gradient, then $\{s^{k}\}$ $Q-$linearly converges to $s^{\star}$, and
$\{u^k\}$ $Q-$linearly converges to the unique solution to
\eqref{problem:primal}. For varying stepsizes  and inexact proximal
evaluations, see \cite[Theorem 7]{eckstein1992douglas}. A similar analysis was
carried out for the PRS method in
\cite{eckstein1989splitting,lions1979splitting}. More recently, the authors in
\cite{davis2017faster} studied the convergence speed  of a relaxed 
PRS for convex problems, under the assumption of strong convexity of one of the
functions and Lipschitz continuity of its gradient. Rates of convergence are
provided, including the standard DRS and PRS as special cases. Furthermore, 
the method applied to the dual formulation yields convergence rates
for a relaxed ADMM.  In
\cite{deng2016global} the authors derive a global linear rate of
convergence for  ADMM variants for the convex case, assuming
one of the two functions is strongly convex with Lipschitz continuous gradient. 


The aforementioned works are typically based on monotonicity of the sequence of
iterate distances to the solution set. In a DRS, however,
functional values are not monotonically increasing, and for this reason 
\cite{patrinos2014douglas} introduced
a special merit function, called \textit{Douglas-Rachford envelope} (DRE).
For convex composite problems with one convex quadratic function, 
the DRE is real-valued and continuously differentiable. Furthermore,
one DRS iteration corresponds to one gradient step applied to minimizing the DRE.
In a manner similar to how the Moreau envelope sheds a light on the PPA,
the DRE gives an insight on DRS. In particular, because
DRS provides (sufficient) descent for the merit function DRE, a variable metric
gradient method for the DRE yields 
complexity estimates and rates of convergence for DRS iterates. A point
crucial for this type of  analysis is that DRE critical points are
related to minimizers of the original convex problem. 
For convex composite objective functions with one $L-$smooth and strongly convex term, a similar approach is adopted in \cite{patrinos2014forward} to 
analyze Forward-Backward methods by means of a suitably defined envelope.

The literature is much more scarce
for nonconvex problems, the setting considered in this work. We can mention 
the DR splitting proposed in
\cite{li2016douglas,li2015global} for the sum of a differentiable function with Lipschitz continuous gradient, 
and a proper lower semicontinuous function with an easily computable proximal point. By defining a merit function related to the DRE, global subsequential convergence to a critical point is obtained, as well as eventual convergence rate under some extra assumptions, namely, that the functions satisfy the KL inequality \cite{kurdyka1998gradients,bolte2007clarke}, a concept related to error bounds \cite[Theorem 4.1]{li2018calculus}. These two notions are often used in the literature to establish local rates of convergence \cite{atenas2023unified,luo1993error,robinson1999linear,frankel2015splitting,davis2015convergence,bolte2017error,li2018calculus}.


{Our contribution refers to deriving local
convergence rates for weakly convex Douglas-Rachford splitting mechanisms.
This is achieved by combining the machinery developed in \cite{themelis2020douglas} for the DRE with 
the unifying framework for descent methods from \cite{atenas2023unified}.
In some sense, we generalize the latter work, since when applying arguments of the
corresponding theory to the DRE merit function, we succeed in showing convergence properties for 
another sequence of iterates, the DRS method applied to the original function. 
Our results resemble the ones briefly referred without proof in \cite[page 15]{themelis2020douglas}
for semialgebraic functions. 
A second line of contribution is obtained when specializing \eqref{DR:scheme}
to weakly convex constrained optimization, obtaining a linearly convergent ADMM
suitable for non-convex separable objective functions. A third contribution is related to multistage stochastic optimization, and the
the Progressive Hedging (PH) algorithm \cite{rockafellar1991scenarios} 
which is a special DRS case for convex problems \cite[Chapter 3.9]{ruszczynski2003stochastic}. 
We extend this application to the weakly convex setting, obtaining a version of PH for  weakly convex problems. 
Our PH variant bears some resemblance with the Elicited Progressive Decoupling algorithm 
\cite{rockafellar2019progressive} for nonconvex problems, whose convergence rate
was analyzed in  \cite{sun2021elicited}.  We provide explicit convergence rates separately 
for primal and dual iterates, and additionally for functional values, features that \cite{sun2021elicited} lacks.


The remainder of this work is organized as follows. We introduce the notation and 
 background on variational analysis in \S~\ref{section:background}, as well as the definition of the Douglas-Rachford envelope and some properties. Next, in \S~\ref{section:DR} we show the necessary components to follow the ideas of \cite{atenas2023unified} to obtain convergence and local rate of convergence of nonconvex Douglas-Rachford. We continue with applications in \S~\ref{section:AL-PH}, proposing a method for weakly convex constrained optimization, and, in particular, weakly convex stochastic optimization. We proceed in \S~\ref{section:numerical} with numerical tests of our variant of PH to solve a phase retrieval problem, showing promising numerical performance when compared to existing algorithms for weakly convex problems. We conclude in \S~\ref{section-final} with some concluding remarks and possible extensions.
 
 %with some numerical experiments that show , and some final remarks in \S~\ref{section:conclusions}.

}

\section{Background material} \label{section:background}

\subsection{Notation and definitions}

{Unless stated otherwise, $\langle \cdot, \cdot \rangle$ denotes an inner product  in $\R^n$, and 
its induced norm $\| \cdot \|$.  } A function $\varphi: \R^n \to \R \cup \{+\infty\}$ is called proper whenever
exists $\overline{v} \in \R^n$ such that $\varphi(\overline{v}) < +\infty.$ The set of points that satisfy this condition, the domain of $\varphi$, is denoted by $\mbox{dom}(\varphi).$ In order to characterize critical points of a function, we may use different subdifferentials.


\begin{definition}[Subdifferentials] Let $\varphi: \R^n \to \R \cup \{+\infty\}$ be 
a proper function, and $\overline{v}$ such that $\varphi(\overline{v})
<+\infty.$ We denote by

\begin{itemize} \item[(i)]  $\partial_F \varphi$ the Frechet (or regular)
subdifferential of $\varphi,$ defined as \begin{equation*} \partial_F
\varphi(\overline{v}) = \left\{ g \in \R^n : \liminf_{v \to \overline{v}}
\frac{\varphi(v)-\varphi(\overline{v}) - \langle g, v - \overline{v} \rangle}{\|
v - \overline{v}\|} \ge 0\right\}.  \end{equation*}

    \item[(ii)]  $\partial_L \varphi$ the limiting (or general) subdifferential
    of $\varphi,$ defined as 

    \begin{equation*} \partial_L \varphi(\overline{v}) = \left\{ g \in \R^n:
    \exists v^k, g^k \in \partial_F \varphi(v^k), \mbox{s.t. } v^k \to
    \overline{v}, \varphi(v^k) \to \varphi(\overline{v}), g^k \to g\right\}
    .
    \end{equation*}

    \item[(iii)]  $\partial_C \varphi$ the Clarke subdifferential of $\varphi,$
    defined as 

    \begin{equation*} \partial_C \varphi (\overline{v}) =
    \overline{\mbox{co}}\left\{ \lim_{k \to +\infty} \nabla \varphi (v^k): v^k
    \to v,  \mbox{and } \nabla \varphi (v^k) \mbox{ exists}  \right\},
    \end{equation*} where $\overline{\mbox{co}}$ denotes the closed convex hull.
    
\end{itemize}
    
\end{definition}

We use $\partial$ to denote a generic subdifferential, when no specific subdifferential needs to be specified. It is important to mention that the Clarke subdifferential is well--defined for locally Lipschitz functions \cite[Proposition 2.1.2]{clarke1990optimization}.  For a differentiable function $\varphi$,
$\partial \varphi(\overline{v})$ is the singleton containing the gradient $\nabla
\varphi(\overline{v})$ of $\varphi$ at $\overline{v}$. For a
proper lower semicontinuous convex function $\varphi$, $\partial
\varphi (\overline{v})$ coincides with the subdifferential of convex analysis,
namely, the set of vectors $g \in \R^n$ such that for all $v \in \R^n$, $\varphi(v) \ge \varphi(\overline{v}) + \langle g, v - \overline{v} \rangle.$ 

For functions with a \textit{benign} form of nonconvexity (using the terminology of \cite{swright-video}), we can exploit convex analysis machinery. A function $\varphi: \R^n \to \R\cup\{+\infty\}$ is said to be $\rho-$weakly convex, for $\rho >0$, if $\varphi(\cdot) + \frac{\rho}{2}\|\cdot\|^2$ is convex. Weakly convex functions can be characterized in a variety of forms \cite{davis2019stochastic}. In particular, a function $\varphi$ is $\rho-$weakly convex, if and only if, for any $v,\overline{v} \in \mbox{dom}(\varphi),$ and $g \in \partial \varphi(\overline{v}),$\begin{equation} \label{prox-subgrad}
    \varphi(v) + \frac{\rho}{2}\| v - \overline{v} \|^2 \ge \varphi(\overline{v}) + \langle g, v -\overline{v} \rangle.
\end{equation}

For weakly convex functions, the Clarke subdifferential coincides with the limiting subdifferential \cite[Proposition 2.1.5(d)]{clarke1990optimization}, 
therefore the above characterization can use either of the two subgradients. More properties and examples of weakly convex functions can be found in \cite{hoheiselproximal,davis2019stochastic}. 


As stated in \eqref{DR:scheme}, one iteration of DRS is constructed using the
proximal operator, defined as follows. For a point $v \in \R^n$, $\prox_{\gamma \varphi}(v)$ denotes the proximal point operator of $\varphi$ for a stepsize parameter $\gamma >0$, evaluated at $v$, defined by \begin{equation} \label{def:prox}
    \prox_{\gamma \varphi}(v) = \argmin_{u \in \R^n} \left\{ \varphi(u) + \frac{1}{2\gamma}\| u - v\|^2\right\}.
\end{equation}

\noindent The optimal value of the minimization problem in \eqref{def:prox} is called the Moreau envelope of $\varphi$ with stepsize $\gamma >0$, and denoted by $e_{\gamma}\varphi$. More precisely, 
\[e_{\gamma}\varphi(u) = \inf_{u \in \R^n} \left\{ \varphi(u) + \frac{1}{2\gamma}\| u - v\|^2\right\}. \]

\noindent In the general case, note that the operator $\prox_{\gamma \varphi}: \R^n \rightrightarrows \R^n$ could be empty-valued and $e_{\gamma}\varphi(u)$ might take the value $-\infty$. When $\varphi$ is a proper lower semicontinuous convex function,  $\prox_{\gamma \varphi}$ is a single-valued mapping \cite[Theorem 12.12, Theorem 12.17]{rockafellar2009variational}, and $e_{\gamma}\varphi$ is finite-valued. If $\varphi$ is only proper lower semicontinuous, such that $\varphi(\cdot) + \frac{1}{2\gamma}\|\cdot\|^2$ is bounded from below for some $\gamma >0,$ then the images of $\prox_{\gamma \varphi}$ are nonempty and compact, and $e_{\gamma}\varphi$ is finite-valued \cite[Theorem 1.25]{rockafellar2009variational}.

We say a point $\overline{v}$ is critical for $\varphi$ if $0 \in \partial \varphi (\overline{v}).$ This condition is necessary for a point to be a local minimum. For a critical point $\overline{v}$ of $\varphi$, the real number $\varphi(\overline{v})$ is called critical value. For a set $\mathcal{C} \subseteq \R^n$, we denote by $i_{\mathcal{C}}$ the indicator of the set, that is, the function such that $i_{\mathcal{C}}(v) = 0$ for $v \in \mathcal{C}$, and $+\infty$ otherwise. For nonempty closed convex set $\mathcal{C}$, we say that $\overline{v}$ is a critical point for problem $\min_{v \in \mathcal{C}} \varphi(v)$ if $0 \in \partial \varphi (\overline{v}) + N_{\mathcal{C}}(\overline{v}),$ where $N_{\mathcal{C}}(\overline{v}) = \partial i_{\mathcal{C}} ( \overline{v}).$

Two related concepts related to criticality and the study of rates of convergence are defined in the following. We say a function $\varphi: \R^n \to \R\cup \{+\infty\}$ satisfy a local
error bound, if for any $\bar{\varphi} \ge \inf \varphi > -\infty$, there exists
$\varepsilon >0, \ell >0,$ such that whenever $\varphi(v) \le \bar{\varphi}$,
\begin{equation} \label{EB-f}  d\bigl(x, (\partial
\varphi)^{-1}(0)\bigr) \le \ell d\bigl(0, \partial \varphi (v) \cap B(0,
\varepsilon)\bigr).  \end{equation}

Furthermore, we say a function $\varphi: \R^n \to \R\cup \{+\infty\}$ satisfy the proper
separation of isocost surfaces property if there exists $\delta >0$, such that
if \begin{equation} \label{PSIS}  \forall u,v \in (\partial
\varphi)^{-1}(0), \: \| u - v \| \le \delta \implies \varphi(u) =
\varphi(v).  \end{equation}

\subsection{Douglas-Rachford envelope}

We adopt the assumptions in \cite{themelis2020douglas}.

\begin{assumption} \label{assumptions:blanket} For problem
\eqref{problem:primal}, consider the following conditions

\begin{itemize} \item $\varphi_1: \R^n \to \R$ is a $L-$smooth function, that
is, it is differentiable and its gradient $\nabla \varphi_1 $ is a $L-$Lipschitz
continuous function.  \item $\varphi_2: \R^n \to \R \cup \{+\infty\}$ is a
proper lower semicontinuous function.  \item the set of solutions of problem
\eqref{problem:primal} is nonempty.  \end{itemize} \end{assumption}

 As mentioned in \cite[Remark 3.1]{themelis2020douglas}, due to
 Assumption~\ref{assumptions:blanket}, the scheme in  \eqref{DR:scheme} is
 well-defined for any $0 < \gamma < \frac{1}{L}$, since for such choice of
 $\gamma,$ both \begin{equation*} \varphi_1(\cdot) + \frac{1}{2 \gamma} \| \cdot \|^2 \mbox{
and } \varphi_2(\cdot) + \frac{1}{2 \gamma} \| \cdot \|^2 \end{equation*} are bounded from below. Furthermore, since $\varphi_1$ is $L-$smooth,
$\prox_{\gamma \varphi_1}$ is a single-valued operator \cite[Proposition
2.3(i)]{themelis2020douglas}, and from the update rule for $u^k$ in
\eqref{DR:scheme},  \begin{equation} \label{u-OC} u^k = \prox_{\gamma \varphi_1}(s^k) \iff 0 =
\gamma\nabla \varphi_1 (u^k) + u^k - s^k.  \end{equation} Combining this last identity with the update rule for $v^k$ in
\eqref{DR:scheme}, $v^k$ corresponds to a solution to the following problem  \begin{equation} \label{DRE:moreau} \min_{v} \left\{ \varphi_2(v) +
\frac{1}{2\gamma}\|v - \big(u^k - \gamma \nabla \varphi_1(u^k)\big)\|^2
\right\}.  \end{equation} 

\noindent After expanding squares in the last expression, we end up with the original form
of the \textit{Douglas-Rachford envelope}, used in the analysis of
\cite{themelis2020douglas} to show DRS convergence results. 

\begin{definition}[Douglas-Rachford envelope] For any $s\in \R^n$ the DRE function $\DRE: \R^n \to
\R$ is defined as  \begin{equation} \label{DRE} \DRE(s) =
\min_{v}\left\{ \varphi_1(u) + \langle  \nabla \varphi_1(u), v - u \rangle +
\varphi_2(v) + \dfrac{1}{2\gamma}\|v-u\|^2\right\}, \:\: \mbox{ where } \: u =
\prox_{\gamma \varphi_1}(s).  \end{equation} \end{definition}

Problem \eqref{DRE} can be interpreted as yielding an approximate value of 
$\prox_{\gamma\varphi}(u).$ Namely, in the sum $\varphi =
\varphi_1 + \varphi_2$, the first term is replaced by a first-order Taylor model
of $\varphi_1$ at $u$.

Thanks to the DRE, properties of the splitting \eqref{DR:scheme} can be analyzed by resorting
to techniques of descent methods. To this aim, we first recall some relations
between the DRE and the scheme \eqref{DR:scheme} stated in \cite[Propositions 3.2, Theorem 3.4]{themelis2020douglas}.

\begin{proposition}[General properties of the envelope] \label{DRE:properties}

For a function $\varphi = \varphi_1 + \varphi_2$ that satisfies
Assumption~\ref{assumptions:blanket}, and $\DRE$ defined in \eqref{DRE}, for any $0 < \gamma < \frac{1}{L}$ the following holds.

\begin{enumerate} \item[(i)] The DRE satisfies the relation for all $s \in \R^n$ \begin{equation*} \DRE(s) = \big(\varphi_2^{\gamma} \circ (Id - \gamma \nabla \varphi_1) \circ \prox_{\gamma \varphi_1}\big)(s), \end{equation*} and is a real-valued and locally Lipschitz function.
\item[(ii)]  $\inf_{x\in\R^n} \varphi(x) = \inf_{s\in\R^n} \DRE(s).$
\item[(iii)] $\argmin \: \varphi =
\prox_{\gamma \varphi_1}(\argmin\: \DRE).$ \end{enumerate}
\end{proposition}
\begin{proof}
To prove item (i), in \cite[Proposition 2.3] {themelis2020douglas} is shown that
 $\prox_{\gamma \varphi_1}$ is a Lipschitz continuous operator. Additionally,  $\varphi_2^{\gamma}$ is locally Lipschitz \cite[Example
10.32]{rockafellar2009variational}, and so is $Id - \gamma \nabla \varphi_1$ due
to Assumpption~\ref{assumptions:blanket}(i), and the result follows. Items (ii) and (iii) are \cite[Theorem 3.4] {themelis2020douglas}.
\end{proof}

The Lipschitz continuity of the envelope
is useful not only to be able to compute subgradients (by applying the chain
rule to the expression in item (i) above), but also in the convergence
analysis, since the DRE is thus a continuous function. Moreover,
items (ii) and (iii) in 
Proposition~\ref{DRE:properties} explicitly relate 
$\DRE$ with the original objective function $\varphi$, through the proximal
mapping of the first term $\varphi_1$. In particular, whenever $\varphi$ is bounded below, so is
the envelope $\DRE.$

 The next result, corresponding to \cite[Proposition 3.3]{themelis2020douglas}, relates the objective function $\varphi$ with its  regularization $\DRE$, by using the
 definition of the DRE, and $L-$smoothness of $\varphi_1.$ These relations are crucial to prove convergence of function values in Theorem~\ref{DR:convergence}. We include a proof for the sake of clarity.

\begin{proposition}[Relations between DRS and DRE iterates] \label{DRE:sandwich}

For a function $\varphi = \varphi_1 + \varphi_2$ that satisfies
Assumption~\ref{assumptions:blanket}, $\DRE$ defined  in \eqref{DRE},
and $\{(u^k, v^k, s^k)\}$ generated by \eqref{DR:scheme}, for any $0 < \gamma <
\frac{1}{L}$ it holds \begin{itemize} \item[(i)] $\DRE(s^k) \le \varphi (u^k).$ \item[(ii)]
$\varphi(v^k) \le \DRE(s^k) - \dfrac{1-\gamma L}{2\gamma} \|u^k - v^k\|^2.$
\end{itemize} \noindent Furthermore, any limit point $(s^{\star}, u^{\star}, v^{\star})$ of the sequence $\{(u^k, v^k,
s^k)\}$, whenever they exist, satisfy 
\begin{itemize}
    \item[(iii)] $\DRE(s^{\star}) \le \varphi(u^{\star}), \quad \mbox{ and }
    \quad \varphi(v^{\star}) \le \DRE(s^{\star}) - \dfrac{1-\gamma
    L}{2\gamma}\|u^{\star}-v^{\star}\|^2.$
\end{itemize}
    
\end{proposition}

\begin{proof}

    Item (i) directly follows from \eqref{DRE},  by evaluating $\DRE$ at $s=s^k$, and the minimand at  $v = u^k$. Item (ii) follows similarly as \cite[Proposition 4.3(ii)]{themelis2018forward}. Indeed, since $v^k$ minimizes the problem in \eqref{DRE} for $s=s^k$, $\DRE(s^k) = \varphi_1(u^k) + \langle \nabla \varphi_1(u^k), v^k - u^k \rangle + \varphi_2(v^k) + \frac{1}{2\gamma}\|v^k - u^k \|^2$. The right-hand side can be bounded using the descent lemma \cite[Proposition A.24]{bertsekas1997nonlinear}, namely, \begin{equation} \label{descent-lemma} | \varphi_1(v^k) - \varphi_1(u^k) -
\langle \nabla\varphi_1(u^k), v^k-u^k\rangle | \le \dfrac{L}{2}\| u^k - v^k\|^2.
\end{equation} This yields $\DRE(s^k) \ge \varphi_1(v^k) -  \frac{L}{2}\| v^k - u^k\|^2 + \varphi_2(v^k) + \frac{1}{2\gamma}\|v^k - u^k \|^2$, giving the desired result.

    Let $K \subseteq \mathbb{N}$ be an infinite set of indices, such
that $(s^k,u^k,v^k) \to (s^{\star}, u^{\star}, v^{\star})$ as $K \ni k \to
+\infty.$ Since $\nabla \varphi_1$ is continuous, then taking the limit in \eqref{u-OC}, it holds that $0 = \gamma \nabla \varphi_1
(u^{\star}) + u^{\star} - s^{\star},$ which is equivalent to $u^{\star} =
\prox_{\gamma \varphi_1}(s^{\star})$. Therefore, it follows
from \eqref{DRE} that \begin{equation*}\DRE(s^{\star}) \le \varphi_1(u^{\star}) + \langle \nabla     \varphi(u^{\star}), u^{\star} - u^{\star} \rangle + \varphi_2(u^{\star}) +
    \frac{1}{2 \gamma}\|u^{\star}-u^{\star}\|^2 = \varphi(u^{\star}).\end{equation*}
    Furthermore, since $\varphi$ is lower semicontinuous, and $\DRE$
    is continuous (Proposition~\ref{DRE:properties}(i)), it follows from
    Proposition~\ref{DRE:sandwich}(ii) \begin{equation*} \begin{array}{rcl} \varphi(v^{\star}) & \le
&\displaystyle\liminf_{k \in K} \varphi(v^k) \\ & \le & \displaystyle\liminf_{k
\in K} \left\{\DRE(s^k) - \dfrac{1-\gamma L}{2\gamma} \|u^k - v^k\|^2\right\}\\
& = & \DRE(s^{\star}) - \dfrac{1-\gamma L}{2\gamma} \|u^{\star} - v^{\star}\|^2.
\end{array} \end{equation*}

\end{proof}




\section{Convergence of Nonconvex Douglas-Rachford Splitting} \label{section:DR}

\subsection{Convergence analysis as a descent method for the envelope} 

In order to obtain convergence properties of DRS using arguments for descent
methods, we make use of the DRE. The work in \cite{themelis2020douglas}
 constructs the tools to employ
\cite{atenas2023unified}. For the purpose of directly using the properties of $\varphi_1$ and $\varphi_2$ of Assumption~\ref{assumptions:blanket},
note that $\DRE$ can be computed directly using the iterates of
\eqref{DR:scheme}, by first reformulating  problem \eqref{problem:primal} as
follows \begin{equation*} \min_{u,v \in \R^n} \varphi_1(u) + \varphi_2(v) \quad
\mbox{ s.t. } \quad u -v =0.  \end{equation*} The augmented Lagrangian of this reformulation is, for $\beta
>0:$ \begin{equation*} \mathcal{L}_{\beta}(u,v,y) = \varphi_1(u) + \varphi_2(v)
+ \langle y, u-v \rangle + \dfrac{\beta}{2}\| u - v\|^2, \end{equation*} 

\noindent where $y \in \R^n$ is a Lagrange multiplier associated with the constraint $u-v
= 0.$ Therefore, due to \eqref{DRE}, it holds \begin{equation}
\label{DR:Lagrangian} \DRE(s^k) = \mathcal{L}_{\gamma^{-1}}\bigl(u^k, v^k,
\gamma^{-1}(u^k-s^k)\bigr).  \end{equation}

Following \cite{atenas2023unified},  the first main ingredient is
to prove that DRS is a descent method for the Lagrangian evaluated at 
the primal-dual point
$$\x^k :=
\bigl(u^k, v^k, \gamma^{-1}(u^k-s^k)\bigr).$$ The following result states that
$\{\DRE(s^k)\}$ satisfies a condition of sufficient decrease with respect to
both $\|s^k - s^{k+1}\|^2$ and $\|u^k - u^{k+1}\|^2$, proven in \cite[Theorem
4.1]{themelis2020douglas} for $\{\DRE(s^k)\}$. 


\begin{theorem}[Descent properties of DRS] \label{DR:descent} 

Suppose that $\varphi=\varphi_1 + \varphi_2$ satisfies
Assumption~\ref{assumptions:blanket}. For $\lambda \in (0,2)$, and $ 0 < \gamma
< \frac{2-\lambda}{2L}$, the iterates $\{(u^k, v^k, s^k)\}$ generated by
\eqref{DR:scheme} satisfy,  \begin{equation} \label{DRE-decrease}
\DRE(s^k)-\DRE(s^{k+1})\ge c\max\left\{ \frac{1}{(1+ \gamma
L)^2}\|s^k -s^{k+1}\|^2, \|u^k -u^{k+1}\|^2 \right\}, \end{equation}

\noindent where \begin{equation*} c = \frac{2-\lambda}{2\lambda\gamma} -
\frac{L}{\lambda}>0.  \end{equation*}

\noindent Furthermore, for $\x^k = \bigl(u^k, v^k, \gamma^{-1}(u^k-s^k)\bigr)$, it holds
\begin{equation} \label{subgrad:Lagrangian} g^k := \bigl(\gamma^{-1}(u^k-v^k), 0
,u^k-v^k\bigr) \in \partial_F \mathcal{L}_{\gamma^{-1}}(\x^k), \end{equation} and
\begin{equation} \label{subgrad:aux} \|g^k\| =
\lambda^{-1}\sqrt{\gamma^{-2}+1}\| s^{k+1} - s^k\|.  \end{equation}
    
\end{theorem}


\begin{proof}

    First,  the estimate \eqref{DRE-decrease} for $\|s^k - s^{k+1}\|^2$
    corresponds to \cite[(4.2)]{themelis2020douglas} after using the identity
    \eqref{DR:Lagrangian}, for $\sigma_{\varphi_1}= -L$. The estimate for $\|u^k
    - u^{k+1}\|^2$ appears in the proof of \cite[Theorem
    4.1]{themelis2020douglas}.

    Furthermore, the subdifferential of $\mathcal{L}_{\gamma^{-1}}$ at $\x = \x^k$
    can be computed taking partial derivatives with respect to the different
    components of the primal-dual vector $\x$, as follows:
    \begin{itemize} \item Since $\varphi_1$ is differentiable, $\frac{\partial\mathcal{L}_{\gamma^{-1}}}{\partial u}(u,v,y) =
    \nabla \varphi_1(u) + y + \gamma^{-1}(u-v)$. Then, evaluating this last identity at $(u,v,y) = \x^k$, and using \eqref{u-OC}, it follows that\begin{equation*}\begin{array}{rcl}
	\frac{\partial\mathcal{L}_{\gamma^{-1}}}{\partial u}(\x^k) & = & \nabla \varphi_1(u^k) + 	\gamma^{-1}(u^k - s^k) + \gamma^{-1}(u^k - v^k) \\ & = & \gamma^{-1}(u^k
	- v^k).\end{array}\end{equation*}

	\item From the optimality condition of $v^k$ for problem \eqref{DRE}, $0 \in \partial_F \varphi_2(v^k) + \nabla \varphi_1
	(u^k) + \gamma^{-1}(v^k - u^k)$, it holds that $\frac{\partial \mathcal{L}_{\gamma^{-1}}}{\partial v}(\x^k) = \partial_F \varphi_2 (v^k) - \gamma^{-1}(u^k-s^k) + \gamma^{-1}(v^k - u^k) \ni 0$.

	\item Since $\mathcal{L}_{\gamma^{-1}}$ only depends on $y$ linearly, then  $\frac{\partial \mathcal{L}_{\gamma^{-1}}}{\partial y}(\x^k) = \gamma^{-1}(u^k - s^k)$.

        
    \end{itemize}

    Therefore, from $\partial_F \mathcal{L}_{\gamma^{-1}}(\x^k) = \frac{\partial
    \mathcal{L}_{\gamma^{-1}}}{\partial u}(\x^k) \times \frac{\partial \mathcal{L}_{\gamma^{-1}}}{\partial v}(\x^k) \times \frac{\partial\mathcal{L}_{\gamma^{-1}}}{\partial y}(\x^k)$, identity \eqref{subgrad:Lagrangian} follows. To prove \eqref{subgrad:aux},
    note that due to the update rule for $\{s^k\}$ in \eqref{DR:scheme}, it
    follows \[\begin{array}{rcl} \|g^k \|^2 & = & \gamma^{-2}\|u^k - v^k\|^2 +
    \|u^k - v^k\|^2   \\ & = & (\gamma^{-2}+1)\|u^k - v^k\|^2\\ & = &
    (\gamma^{-2}+1)\lambda^{-2}\|s^k - s^{k+1}\|^2.  \end{array}\] \end{proof}


Our result is not 
a straightforward application of the general scheme in \cite{atenas2023unified}. 
As made clear in the proof, in our setting, the DRE functional decrease is measured only in terms of
some components of the primal-dual term $\|\x^{k}-\x^{k+1}\|^2$.
This feature prevents us to directly apply the unifying convergence theory of
\cite{atenas2023unified}.


%\textbf{Theorem 4.1.} For sufficiently small $\gamma$ and $\lambda \in (0,2),$ 

%The following theorem presents some of the results that can be obtained using
%the ideas of \cite{atenas2023unified}. 

%\textbf{Theorem 4.3.} For sufficiently small $\gamma$ and $\lambda \in (0,2),$
%\begin{itemize} %   \item[i)] $u^k - v^k \to 0.$ %  \item[ii)] $\{u^k\}$ and
%$\{v^k\}$ have same limit points (if they exist), and all of them are stationary
%points of $\varphi$ with the same critical value, which is $\lim_k \DRE(s^k).$
%\item[iii)] %\end{itemize}

We now give an alternative proof of \cite[Theorem
4.3]{themelis2020douglas}, based on the developments in \cite{atenas2023unified}. 
The result
states  subsequential convergence of the iterates to critical points of
$\varphi,$ and convergence of $\DRE(s^k)$  to a critical value of $\varphi.$

\begin{theorem}[Subsequential convergence of DRS] \label{DR:convergence}

Suppose that $\varphi=\varphi_1 + \varphi_2$ satisfies
Assumption~\ref{assumptions:blanket}. For $\lambda \in (0,2)$, and $ 0 < \gamma
< \frac{2-\lambda}{2L}$, then for any bounded sequence $\{(u^k, v^k, s^k)\}$
generated by \eqref{DR:scheme}, 

\begin{itemize} \item[(i)] The sequence $\{{\DRE(s^k)}\}$
monotonically converges to a critical value $\varphi^{\star}$ of $\varphi${, and the sequence $\{\varphi_1(u^k) + \varphi_2(v^k)\}$ converges to the same value $\varphi^{\star}.$}
\item[(ii)] $u^k - v^k \to 0$, $u^k - u^{k+1} \to 0$, $v^k - v^{k+1} \to 0$, and $s^k - s^{k+1} \to 0$, as $k \to + \infty$. \item[(iii)] All cluster
points of $\{u^k\}$ and $\{v^k\}$ coincide,
and are also critical points of $\varphi$, with same critical value
$\varphi^{\star}=\lim_{k\to \infty} \DRE(s^k)=\lim_{k\to \infty} \varphi_1(u^k) + \varphi_2(v^k)$.  \end{itemize}
    
\end{theorem}

\begin{proof}

First, Assumption~\ref{assumptions:blanket}(iii) implies that $\varphi$ is
bounded from below. Then, from Theorem~\ref{DRE:properties}(ii), $\DRE$
is also bounded from below, and so is the sequence
$\{{\DRE(s^k)}\}$. Furthermore, the descent condition
\eqref{DRE-decrease} implies that $\{{\DRE(s^k)}\}$ is a
non-increasing real sequence. Thus, there exists $\varphi^{\star} \in \R$ such
that  ${\DRE(s^k)} \to \varphi^{\star}.$ In turn,
\eqref{DRE-decrease} then yields $s^k - s^{k+1} \to 0,$ and $u^k - u^{k+1}
\to 0.$ These results have a couple of consequencues:

\begin{itemize} \item $u^k - v^k \to 0$, due to the update rule for $\{s^k\}$
in \eqref{DR:scheme}, and thus $\{u^k\}$ and $\{v^k\}$ have the same limit
points.  \item From \eqref{subgrad:aux}, it follows that $g^k \to 0$.  \item
Furthermore,  $v^k - v^{k+1} = v^k - u^k + u^k - u^{k+1} + u^{k+1} - v^{k+1}\to
0$.  \end{itemize}

As for item (ii), let $u^{ \star}, v^{\star},$ and $s^{\star}$ be limit points
of the sequences $\{u^k\}, \{v^k\}$, and $\{s^k\},$ respectively. Note that
$v^{\star} = u^{\star}$, because $\{u^k\}$ and $\{v^k\}$ have the same limit points, and
thus $\varphi(v^{\star}) = \varphi(u^{\star})$. Then, up to a subsequence, $v^k
\to v^{\star}$, and following the arguments in \cite{themelis2020douglas}

\[ \begin{array}{rclrl} \varphi(v^{\star}) & \le &  \liminf_{k \in K} \varphi(v^k) &  &
\mbox{Assumption~\ref{assumptions:blanket}(i)-(ii)}\\ & \le & \limsup_{k \in K} \varphi(v^k) &
&\\ & \le & \limsup_{k \in K} \DRE(s^k) & &\mbox{Proposition~\ref{DRE:sandwich}(ii)} \\ &
= & \DRE(s^{\star}) & &\mbox{Proposition~\ref{DRE:properties}(i)}\\ & \le &
\varphi(u^{\star}) & &\mbox{{Proposition~\ref{DRE:sandwich}(iii)}} \\
& = & \varphi(v^{\star}) && \end{array} \]
Therefore, $\varphi(v^k) \to \varphi(v^{\star})$, and $\mathcal{L}_{\gamma^{-1}}(\x^k) = \DRE(s^k)
\to \varphi(v^{\star}) $ through the same subsequence, with
$\varphi(u^{\star})=\varphi(v^{\star})= \DRE(s^{\star}) = \varphi^{\star}$. Note that since $\{u^k - s^k\}$ is bounded, and $u^k - v^k \to 0$ as $k \to +\infty$, then $\langle \gamma^{-1}(u^k- s^k), u^k - v^k \rangle + \frac{1}{2\gamma}\|u^k - v^k\|^2 \to 0$, and thus\[\varphi_1(u^k) + \varphi_2(v^k) = \mathcal{L}_{\gamma^{-1}}(\x^k) - \left( \langle \gamma^{-1}(u^k- s^k), u^k - v^k \rangle + \frac{1}{2\gamma}\|u^k - v^k\|^2\right) \to \varphi^{\star}.\]

Furthermore, from the definition of the augmented Lagrangian,  \[ \begin{array}{rcl} \mathcal{L}_{\gamma^{-1}}(u^{\star}, u^{\star},
\gamma^{-1}(u^{\star}-s^{\star})) &=& \varphi_1(u^{\star}) + \varphi_2(u^{\star})
+ \langle\gamma^{-1}(u^{\star}-s^{\star}), u^{\star} - u^{\star} \rangle +
\dfrac{1}{2\gamma}\| u^{\star} - u^{\star}\|^2 \\ 
&=& \varphi(u^{\star}), \end{array}
\] that is, {$\{\mathcal{L}_{\gamma^{-1}}(\x^k)\}$ (subsequentially)
converges to $\mathcal{L}_{\gamma^{-1}}(u^{\star}, u^{\star},
\gamma^{-1}(u^{\star}-s^{\star}))$, as $\x^k \to (u^{\star}, u^{\star},
\gamma^{-1}(u^{\star}-s^{\star})$.} Therefore, taking the limit in
\eqref{subgrad:Lagrangian} (passing through a subsequence if necessary) gives
\[0 \in \partial_L \mathcal{L}_{\gamma^{-1}}(u^{ \star}, v^{\star},
\gamma^{-1}(u^{\star}- s^{\star})),\] which is equivalent to the following
criticality conditions \begin{equation*} \left\{\begin{aligned} 0  &=  \nabla
\varphi_1(u^{\star}) + \gamma^{-1}(u^{\star}-s^{\star}) +
\gamma^{-1}(u^{\star}-v^{\star})& \\ 0  &\in  \partial_L \varphi_2 (v^{\star}) -
\gamma^{-1}(u^{\star}-s^{\star}) + \gamma^{-1}(u^{\star}-v^{\star})& \\ 0 & =
v^{\star}- u^{\star}& \end{aligned}\right.  \end{equation*} Adding the first two
relations, and using that $v^{\star}=u^{\star}$,  the final result follows, as
we obtain $0 \in
\nabla\varphi_1(u^{\star}) + \partial_L \varphi_2
(u^{\star})$.

 

    
\end{proof}

{

\begin{remark} Some comments about Theorem~\ref{DR:convergence} are in order.
    \begin{itemize}
        \item By items (i) and (ii), the sequence of DRE functional values
	$\DRE(s^k)$ converges monotonically to a critical value of $\varphi$. 
	By contrast, the sequence of functional values
	$\varphi_1(u^k) + \varphi_2(v^k)$ converges to the same critical value, but not necessarily in a monotone manner. 
        
        \item Boundedness of the iterates $\{(u^k, v^k, s^k)\}$ generated by
	\eqref{DR:scheme} is usually ensured by assuming $\varphi$ has bounded level sets \cite[Theorem 4.3(iii)]{themelis2020douglas}, which is equivalent to $\DRE$ having the same property \cite[Theorem 3.4(iii)]{themelis2020douglas}.
        
    \end{itemize}
\end{remark}
}


%In the following, the same results (and more) are deduced using the analysis in
%\cite{atenas2023unified}. Related to the general descent scheme, note that 

%therefore defining $x^k := (u^k, v^k, \gamma^{-1}(u^k-s^k))$, the descent
%condition for the Lagrangian reads %\begin{equation} \label{Lagr:descent} %
%\mathcal{L}_{1/\gamma}(x^k) - \mathcal{L}_{1/\gamma}(x^{k+1}) \ge \dfrac{c}{(1+
%\gamma L)^2}\|s^k -s^{k+1}\|^2.  %\end{equation}

%It is also shown in \cite{themelis2020douglas} that a similar descent condition
%holds with the right-hand side being $c\|u^k - u^{ k+1}\|^2.$



%We can obtain the same results of Theorem 4.3 following a similar reasoning as
%in the original work \cite{atenas2023unified}. In fact:


\subsection{Rate of Convergence for Nonconvex DRS}



The analysis of rate of convergence requires additional assumptions. This
section is an extension of \cite{themelis2020douglas},
by applying the machinery of \cite{atenas2023unified} to \eqref{DR:scheme} through
the envelope
$\DRE$ and the augmented Lagrangian $\mathcal{L}_{\gamma^{-1}}.$ {The next result relates the sequences generated by \eqref{DR:scheme} with the error bound condition \eqref{EB-f}, resulting in an estimate crucial to obtain a local rate of convergence.}

\begin{proposition} \label{prop:EB}  Suppose that $\varphi=\varphi_1 + \varphi_2$ satisfies
Assumption~\ref{assumptions:blanket}, as well as a
local error bound \eqref{EB-f}. For $\lambda \in (0,2)$, and $ 0 < \gamma <
\frac{2-\lambda}{2L}$, then for any bounded sequence $\{(u^k, v^k, s^k)\}$
generated by \eqref{DR:scheme}, and any $\varepsilon, \ell >0$, there exists $\bar{\varphi}>0$, and a sequence $\{\tilde{g}^k\}$, such
that $\tilde{g}^k \in \partial_L \varphi (v^k) \cap B(0,\varepsilon)$,
$\varphi(v^k) \le \bar{\varphi}$, and  \begin{equation} \label{EB:k} d\bigl(v^k, (\partial_L \varphi)^{-1}(0)\bigr) \le
\ell \| \tilde{g}^k\|.  \end{equation}

\end{proposition}

\begin{proof}
    
First, since $\{\DRE(s^k)\}$ monotonically converges to $\varphi^{\star}$, then
for any $\epsilon>0$, and for any sufficiently large $k,$  $\mathcal{L}_{\gamma^{-1}}(\x^k) =
\DRE(s^k) \le \varphi^{\star} + \epsilon. $ From the definition of the augmented
Lagrangian, we then have \begin{equation} \label{Lagr:aux-est} \varphi_1(u^k) +
\varphi_2(v^k) + \gamma^{-1}\langle u^k-s^k, u^k-v^k\rangle +
\dfrac{1}{2\gamma}\|u^k-v^k\|^2 \le \varphi^{\star} + \epsilon.  \end{equation} Furthermore, from the bounded assumption of the generated sequences,
$\{\nabla \varphi_1(u_k)\}$ is bounded, that is, there exists $M_1>0$, such that
for all $k,$ $\|\nabla \varphi_1(u^k) \| \le M_1.$ From \eqref{descent-lemma} and Theorem~\ref{DR:convergence}(ii), for any $\eta>0$, and for all sufficiently
large $k,$  \begin{equation*} |\varphi_1(v^k) - \varphi_1(u^k)| \le M_1 \eta +
\dfrac{L}{2}\eta^2.  \end{equation*} Therefore,  $\varphi(v^k) \le
\varphi_1(u^k) + \varphi_2(v^k) + M_1 \eta + \dfrac{L}{2}\eta^2$.  Combining
this last inequality with \eqref{Lagr:aux-est}, it holds\begin{equation*} \begin{aligned} \varphi(v^k)   & \le  \varphi^{\star} +
\epsilon - \gamma^{-1}\langle u^k-s^k, u^k-v^k\rangle -
\dfrac{1}{2\gamma}\|u^k-v^k\|^2 + M_1 \eta + \dfrac{L}{2} \eta^2& \\ &\le
\varphi^{\star} + \epsilon + \gamma^{-1}\| u^k-s^k\| \| u^k-v^k\| + M_1 \eta +
\dfrac{L}{2} \eta^2 & %\\ &\le &
\end{aligned} \end{equation*} Since the generated
sequences are bounded, then there exists $M_2>0$ such that $\| u^k-s^k\| \le M_2.$ Therefore, for all sufficiently large $k,$ \begin{equation*} \varphi(v^k)
\le  \varphi^{\star} + \epsilon + \gamma^{-1}M_2 \eta  + M_1 \eta + \dfrac{L}{2}
\eta^2 =: \bar{\varphi}.  \end{equation*}

\noindent Furthermore, from the optimality conditions of \eqref{DRE:moreau}
(as shown in the proof of \cite[Theorem 4.3]{themelis2020douglas}), it follows
that $\tilde{g}^k := \gamma^{-1}(u^k - v^k) - (\nabla
\varphi_1(u^k) -  \nabla \varphi_1(v^k)) \in \partial_F \varphi
(v^k).$ Since $\nabla \varphi_1$ is $L-$Lipschitz continuous,
then \begin{equation} \label{subgrad:phi} \| \tilde{g}^k\| \le
\gamma^{-1}(\|u^k-v^k\| + \gamma L \| u^k - v^k\|) = \gamma^{-1}(1+\gamma
L)\|u^k - v^k\|.  \end{equation} In this manner,  from
Theorem~\ref{DR:convergence}(ii), $\tilde{g}^k \to 0.$ Then, for any
sufficiently large $k$, $\tilde{g}^k \in \partial_L \varphi(v^k) \cap B(0,
\varepsilon),$ and $\varphi(v^k) \le \bar{\varphi}.$ This allow us to apply
\eqref{EB-f} to obtain \eqref{EB:k}.  \end{proof}


Before giving the most important result of this paper, first we need some
technical estimates deduced from Proposition~\ref{prop:EB}.

\begin{lemma} \label{lemma:proj}

Suppose the conditions of Proposition~\ref{prop:EB} hold. For any $p^k_v \in
\mbox{proj}_{(\partial_L \varphi)^{-1}(0)}(v^k),$ define \[p^k = (p_v^k, p_v^k,
\gamma^{-1}(p_v^k - s^k)). \] Then, there exists $\overline{C}>0$, such that for
all $k$,  \begin{equation} \label{pk-xk} \| p^k - \x^k\|^2 \le
\overline{C}({\DRE(s^k) - \DRE(s^{k+1})}).  \end{equation}
    
\end{lemma}

\begin{proof}

From the definition of $p^k$ and $\x^k$, we have \begin{equation*} \begin{aligned} \| p^k - \x^k\|^2  &=  \| p^k_v - u^k\|^2 + \|
p^k_v - v^k\|^2 + \|\gamma^{-1}(p_v^k - s^k) - \gamma^{-1}(u^k - s^k) \|^2& \\
&= (1+ \gamma^{-2})  \| p^k_v - u^k\|^2 + \| p^k_v - v^k\|^2. & \end{aligned}
\end{equation*} From \eqref{EB:k}, it follows that $ \| p^k_v - v^k\|^2 \le \ell
\| \tilde{g}^k\|.$ As for $\| p^k_v - u^k\|$, it holds that \begin{equation*}
\begin{aligned} \| p^k_v - u^k\|^2  & \le  (\| p^k_v - v^k\| + \| v^k - u^k\|)^2
& \\ &\le  2\| p^k_v - v^k\|^2 + 2\| v^k - u^k\|^2 & \\ & \le  2\ell^2 \|
\tilde{g}^k\|^2 + 2\| v^k - u^k\|^2 & \\ & =  2\ell^2 \| \tilde{g}^k\|^2 +
2\lambda^{-2}\| s^k - s^{k+1}\|^2 & \\ \end{aligned} \end{equation*} where for the first inequality we apply the triangle inequality, for the second
inequality we use the estimate $(a+b)^2 \le 2(a^2 + b^2)$, for the third
 inequality  $ \| p^k_v - v^k\|^2 \le \ell \| \tilde{g}^k\|$ is used, and for the last equality we use the update rule for $\{s^k\}$ in \eqref{DR:scheme}.
Therefore,   \begin{equation*} \| p^k - \x^k\|^2 \le 2(1+ \gamma^{-2})  (\ell^2 \|
\tilde{g}^k\|^2 + \lambda^{-2}\| s^k - s^{k+1}\|^2) + \ell^2 \| \tilde{g}^k\|^2.
\end{equation*}

Now, we bound the terms in the right-hand side of the above estimate.


\begin{itemize} \item First, note that the descent condition
\eqref{DRE-decrease} implies \begin{equation} \label{s:aux} \|s^k -s^{k+1}\|^2
\le \dfrac{(1+ \gamma L)^2}{c} ({\DRE(s^k) - \DRE(s^{k+1})}).
\end{equation}

\item Furthermore, \eqref{subgrad:phi} together with \eqref{DR:scheme} and
\eqref{s:aux} imply 

\begin{equation*} \begin{aligned} \|\tilde{g}^k\|^2 & =  \gamma^{-2}(1+\gamma
L)^2\| u^k - v^k\|^2 & \\ &=  (\lambda\gamma)^{-2}(1+\gamma L)^2\| s^k -
s^{k+1}\|^2 & \\ & \le  (\lambda\gamma)^{-2}(1+\gamma L)^2\dfrac{(1+ \gamma
L)^2}{c} ({\DRE(s^k) - \DRE(s^{k+1})}) & \\ \end{aligned}
\end{equation*}

\end{itemize}

Hence, follows \eqref{pk-xk}  for \begin{equation*} \overline{C} =
\frac{(1+\gamma
L)^2}{c\lambda^{2}}\left(\frac{\bigl(2(1+\gamma^{-2})+1\bigr)(1+\gamma L)^2\ell^2}{
\gamma^2} +2(1+\gamma^{-2})\right).  \end{equation*}
\end{proof}

In order to relate relate an
error bound for $\varphi$ with the descent properties of Theorem~\ref{DR:descent}, we need the following
assumption for $\varphi_2$.

\begin{assumption} \label{assumption:w-cvx} Assume that $\varphi_2$ is a
$\rho-$weakly convex function.  \end{assumption}

Under this assumption, both $\varphi_1$ and $\varphi_2$ are weakly convex, due to \cite[Proposition 2.4]{atenas2023unified}. Hence, $\varphi$ is locally Lipschitz on its domain and weakly convex. Therefore, limiting (or Clarke) subgradients of $\varphi$ can be characterized as proximal subgradients in the whole space, see \eqref{prox-subgrad}. Thus $\partial_F \varphi = \partial_L \varphi = \partial_C \varphi $. This particular form for subgradients allows us to take full advantage of  Lemma~\ref{lemma:proj}.


\begin{theorem}[Rate of convergence of nonconvex DRS] \label{DR:rate}

Suppose that $\varphi=\varphi_1 + \varphi_2$ satisfies
Assumptions~\ref{assumptions:blanket} and \ref{assumption:w-cvx}, a local error
bound \eqref{EB-f}, and property \eqref{PSIS}. Then, for $\lambda \in (0,2)$,  $ 0
< \gamma < \frac{2-\lambda}{2L}$, and any bounded sequence $\{(u^k, v^k, s^k)\}$
generated by \eqref{DR:scheme}, 

\begin{itemize} \item[(i)] The sequence $\{\DRE(s^k)\}$ $Q-$linearly converges to
a critical value $\varphi^{\star}$ of $\varphi,$ {and the sequence $\{\varphi_1(u^k)+\varphi_2(v^k)\}$ $R-$linearly converges to the same value $\varphi^{\star}.$}

    \item[(ii)] The sequences $\{u^k\}$ and $\{v^k\}$ $R-$linearly converge to a
    critical point $u^{\star}$ of $\varphi$, and $\{s^k\}$ $R-$linearly
    converges to a point $s^{\star}$, such that $u^{\star} = \prox_{\gamma
    \varphi_1}(s^{\star})$.  \end{itemize}
    
\end{theorem}

\begin{proof}
    


First, from Proposition~\ref{prop:EB}, $ \tilde{g}^k \to 0$ implies $v^k - p^k_v
\to 0,$ which in turn implies  $p^k_v- p^{k+1}_v \to 0$, in view of
Theorem~\ref{DR:convergence}(ii). Then, applying the proper separation of isocost
surfaces property \eqref{PSIS}, for all sufficiently large $k,$ $\varphi(p^k_v)
= \varphi(p^{k+1}_v),$ and thus $\varphi(p^k_v) = \varphi^{\star},$ for some
critical value $\varphi^{\star}$ of $\varphi.$ From
Theorem~\ref{DR:convergence}(iii), up to a subsequence, $v^k \to u^{\star}$, for
a critical point $u^{\star}$ of $\varphi$. Therefore, $p_v^k \to u^{\star}$  and
$\varphi(p_v^k) \to \varphi(u^{\star})$, for the same subsequence. Thus
$\varphi^{\star} = \varphi(u^{\star})$.

Furthermore, from the definition of the augmented Lagrangian,
$\mathcal{L}_{\gamma^{-1}}(p^k) = \varphi(p_v^k).$ Moreover, in view of
\eqref{subgrad:Lagrangian} \begin{equation*} \begin{array}{rcl} \DRE(s^k)-
\DRE(p_v^k) & =&\mathcal{L}_{\gamma^{-1}}(\x^k)- \mathcal{L}_{\gamma^{-1}}(p^k)
\\ & \le &-\langle g^k, p^k - \x^k \rangle + \dfrac{\rho}{2}\| p^k - \x^k \|^2.
\end{array} \end{equation*}


\noindent In particular, for all sufficiently large $k,$   \begin{equation}
\label{Lagr:estimate} \DRE(s^k) - \varphi^{\star}  \le \|g^k \| \| p^k - \x^k \|+
\dfrac{\rho}{2}\| p^k - \x^k \|^2  .  \end{equation}


\noindent Note that from \eqref{subgrad:aux} and \eqref{s:aux}, \begin{equation*}
\begin{aligned} \|{g}^k\|^2 & =  \lambda^{-2}(1+\gamma^{-2})\| s^k - s^{k+1}\|^2
& \\ &\le \lambda^{-2}(1+\gamma^{-2})\dfrac{(1+ \gamma L)^2}{c}
({\DRE(s^k) - \DRE(s^{k+1})}) & \\ \end{aligned} \end{equation*}



\noindent Combining the last estimate with \eqref{Lagr:estimate} and
\eqref{pk-xk}, yields \begin{equation} \label{DL} \DRE(s^k) - \varphi^{\star}
\le  \left(\tilde{C}+ \overline{C}\dfrac{\rho}{2} \right)({\DRE(s^k)
- \DRE(s^{k+1})}) \end{equation} for $\tilde{C} = \lambda^{-1}\sqrt{1+\gamma^{-2}}(1+\gamma L)
\sqrt{\frac{\overline{C}}{c}}$.   Set $\hat{C} := \tilde{C}+ \overline{C}\dfrac{\rho}{2},$ $r =
\dfrac{\hat{C}}{1+\hat{C}} \in (0,1),$ and $V^k := \DRE(s^k) - \varphi^{\star}
$. Note that monotonicity of $\{\DRE(s^k)\}$ implies $V^{k+1} \le V^k$. Thus,
from \eqref{DL}, for all sufficiently large $k,$\begin{equation*} V^{k+1} \le
\hat{C}(V^k - V^{k+1}) \iff V^{k+1} \le r V^k, \end{equation*} from which the first part of item (i) follows. 

For item (ii), suppose that above estimate holds for all $k \ge k_0.$ Then,
\begin{equation*} V^{k+1} \le (V^{k_0}r^{1-k_0})r^k, \end{equation*} or
equivalently, for $q = V^{k_0}r^{-k_0}$ and all $k \ge k_0 +1$ \begin{equation} \label{Vk:r-rate}
V^k \le
q r^k.  \end{equation} From the descent condition  of Theorem~\ref{DRE-decrease}, it follows \begin{equation}
\label{descent:Vk} \|s^k - s^{k+1}\| \le \dfrac{1+\gamma L}{\sqrt{c}}\sqrt{V^k},
\quad \mbox{ and } \quad \|u^k - u^{k+1}\| \le \dfrac{1}{\sqrt{c}} \sqrt{V^k}.
\end{equation} Therefore, from \cite[Lemma 4.1]{atenas2023unified}, 
 there exists
$m>0$, $\alpha \in (0,1)$, $s^{\star} \in \R^n$ such that for all
sufficiently large $k$ \begin{equation*} \|s^k- s^{\star}\| \le m \alpha^k,
\quad \|u^k -u^{\star}\| \le m \alpha^k.  \end{equation*} Note that  $\{u^k\}$ converges to the critical point $u^{\star}$, since $\{u^k\}$ and $\{v^k\}$ have the same limit points. Observe that since $\varphi_1$ is $L-$smooth, then
$\prox_{\gamma \varphi_1}$ is Lipschitz continuous \cite[Proposition
2.3(ii)]{themelis2020douglas}, therefore $u^k = \prox_{\gamma \varphi_1}(s^k)
\to \prox_{\gamma \varphi_1}(s^{\star})$, and thus $u^{\star}=\prox_{\gamma \varphi_1}(s^{\star}).$

In addition, the rate of convergence of $\{v^k\}$ can be deduced as follows. Using the
triangle inequality, the update rule for $\{s^k\}$ in \eqref{DR:scheme}, and
\eqref{descent:Vk}, it holds

\begin{equation*} \begin{aligned} \|v^k - v^{k+1}\|  & \le  \|v^k - u^{k}\| +
\|u^k - u^{k+1}\| + \|u^{k+1} - v^{k+1}\|& \\ &=\lambda^{-1}\|s^k - s^{k+1}\| +
\|u^k - u^{k+1}\| + \lambda^{-1}\|s^{k+1} - s^{k+2}\|& \\ & \le
\lambda^{-1}\dfrac{1+\gamma L}{\sqrt{c}}\sqrt{V^k} +
\dfrac{1}{\sqrt{c}}\sqrt{V^k} + \lambda^{-1}\dfrac{1+\gamma
L}{\sqrt{c}}\sqrt{V^{k+1}}.  & \end{aligned} \end{equation*}

\noindent Since $\{V^k\}$ is nonincreasing, then \begin{equation*} \|v^k - v^{k+1}\| \le
\left( \dfrac{2\lambda^{-1}(1+\gamma L) +1}{\sqrt{c}}\right) \sqrt{V^k}.
\end{equation*}

\noindent Then, from \cite[Lemma 4.1]{atenas2023unified} it follows that for all
sufficiently large $k$, \begin{equation*} \|v^k - v^{\star}\| \le \bar{m}
\bar{\alpha}^k.  \end{equation*} for some $\bar{m}>0$ and $\bar{\alpha}\in
(0,1).$ 

{



Finally, to obtain the rate of convergence of $\{\varphi_1(u^k)+ \varphi_2(v^k)\}$, first note that since $\DRE(s^{k+1}) \ge \varphi^{\star}$, then from \eqref{DRE-decrease}, \eqref{Vk:r-rate}, and \eqref{DR:scheme}, it follows \begin{equation} \label{R-rate:aux}
    \lambda^2 \| u^k - v^k\|^2 = \|s^k - s^{k+1}\|^2 \le \left[ \frac{(1+\gamma L)^2 q}{c} \right] r^k.
\end{equation} Furthermore, in view of \eqref{DR:Lagrangian}, \begin{equation*}
    |\varphi_1(u^k) + \varphi_2(v^k) - \varphi^{\star}| \le \DRE(s^k)-\varphi^{\star} + \frac{1}{\gamma}\|u^k-s^k\| \|u^k - v^k\| + \frac{1}{2 \gamma }\|u^k - v^k\|^2.
\end{equation*} By assumption, $\{u^k\}$ and $\{s^k\}$ are bounded sequences, therefore there exists $M_2>0$, such that for all $k$, $\|u^k- s^k\| \le M_2.$ Substituting this estimate, \eqref{Vk:r-rate} and \eqref{R-rate:aux} in the above inequality, yields \begin{equation*}
    |\varphi_1(u^k) + \varphi_2(v^k) - \varphi^{\star}| \le qr^k + \left( \frac{M_2(1+\gamma L)}{\gamma \lambda} \sqrt{\frac{q}{c}}\right) \sqrt{r}^k + \left( \frac{(1+\gamma L)^2 q}{2 \gamma c \lambda^2} \right) r^k.
\end{equation*} Since $r\in (0,1)$, then $r\le \sqrt{r}$, and thus \begin{equation*}
    |\varphi_1(u^k) + \varphi_2(v^k) - \varphi^{\star}| \le \tilde{K} \sqrt{r}^k,
\end{equation*} where \begin{equation*}
    \tilde{K} = q +  \frac{M_2(1+\gamma L)}{\gamma \lambda} \sqrt{\frac{q}{c}} + \frac{(1+\gamma L)^2 q}{2 \gamma c \lambda^2}. 
\end{equation*} This proves the second part of item (i).

}


\end{proof}



\section{Two algorithmic approaches derived from the DRS } \label{section:AL-PH}

To illustrate the versatility of our proposal, we now specialize
the method in two different situations.
The first one, presented in \S~\ref{section:WC}, arises in many 
applications, in which separability is achieved by lifting the decision
variable to a higher dimensional space, created with copies of the variable. For
instance, instead of 
\(\min_x f_1(x)+f_2(x)\), the problem
\[ \left\{\begin{array}{ll}\displaystyle\min_{(x_1,x_2)}
&f_1(x_1)+f_2(x_2)\\\text{ s.t.}&(x_1,x_2)\in\mathcal{C}=\left\{(x_1,x_2):
x_1-x_2=0\right\}\end{array}\right.\]
is solved.  For such a reformulation to be successful from a numerical point of view, 
the constraint set should be simple,
typically a subspace or a box, so that projections to recover feasibility are
not computationally expensive (see Step 3 in Algorithm~\ref{NonCvxAL} below).

The second variant, considered in \S~\ref{section:PH}, 
can be seen as a generalization of the PH algorithm,
that handles multistage stochastic programming problems  with nonconvex
objective function.

\subsection{A method for nonconvex constrained optimization}\label{section:WC}
In this variant, similarly to the DRS scheme, 
separability in the objective function can be exploited by making individual
prox-computations for each term. Here we focus on the additional
calculations that involves dealing with the constraint set $\mathcal C$. To this aim, given $f: \R^n \to \R \cup \{ + \infty \} $ a proper lower semicontinuous function and 
a nonempty closed convex set
$\mathcal{C} \subseteq \R^n$, consider the following constrained minimization problem: \begin{equation} \label{primal:1} \min_{x\in \mathcal{C}} f(x)\,. \end{equation} Since DRS applies to a sum of two functions, we rewrite problem
\eqref{primal:1} accordingly.
One possibility, explored in \cite{atenas2022bundle}, 
is to add the indicator function $i_{\mathcal{C}}$ to the objective. Being non-differentiable,
having a term $\varphi_1$ given by the indicator function escapes the framework \S~\ref{section:DR}. 
Instead, inspired by the approach \cite{li2016douglas} for nonconvex feasibility problems, 
we consider the squared distance to the feasible set as a penalty
function. Namely, given a scaling factor $\mu>0$, we work with the following formulation \begin{equation} \label{primal:2} \min_{x \in \R^n} f(x) +
\dfrac{\mu}{2}d_{\mathcal{C}}^2(x), \end{equation}
and, hence, take $\varphi_1(x)=
\dfrac{\mu}{2}d_{\mathcal{C}}^2(x)$ and $\varphi_2(x)=f(x)$ 
in \eqref{problem:primal}.
In view of \cite[Corollary 12.30]{bauschke2011convex},\begin{equation} \label{d-derivative}
\nabla \left(\frac{\mu}{2}d_{\mathcal{C}}^2\right) = \mu(I - P_{\mathcal{C}}),  \end{equation}
the first two conditions in our blanket Assumption~\ref{assumptions:blanket} hold, with $L = \mu$.
Regarding the third condition, 
solutions to the reformulated problem \eqref{primal:2} will exists when, for
instance, the
constraint set $\mathcal{C}$ is bounded, and $f$ has bounded level sets 
(cf. \cite[Theorem 1.9]{rockafellar2009variational}).

Having cast the problem in our framework, the DRS applied to problem
\eqref{primal:2} yields the following algorithmic scheme.

\begin{algorithm}[hbt] \caption{Constrained Nonconvex Pattern} \label{NonCvxAL}
\begin{algorithmic}[1]

    \State \textbf{Initialization}:  choose $s^0 \in \R^n$, $\gamma >0$, and
    $\lambda > 0.$

    \For{$k=0,1, ...$}

    \State \textbf{Projection}: define \begin{equation*}
        \begin{array}{rcl}
        z^k &=& \left(
    \dfrac{1}{1+\gamma \mu} \right)s^k + \left( \dfrac{\gamma \mu}{1+\gamma\mu} \right)
    P_{\mathcal{C}}[s^k],  \\
        w^k &=& \mu(z^k - P_{\mathcal{C}}[z^k]). 
        \end{array} 
    \end{equation*} \label{AL-up:uk}   
    
   \State \textbf{Primal subproblems:} find a solution $x^k$ of  \begin{equation}   \label{subproblem:primal} \min_{x \in \R^n} \left\{ f(x) +
   \langle w^k ,
   x\rangle + \dfrac{1}{2 \gamma} \| x - z^k \|^2 \right\}. \end{equation} \label{AL-up:vk} 

    \State \textbf{Dual update:} $s^{k+1} = s^k + \lambda (x^k - z^k)$. \label{AL-up:sk}

    \EndFor \end{algorithmic} \end{algorithm}

    Since problem \eqref{primal:2} is a special case of
    problem~\eqref{problem:primal},
    the convergence properties of Algorithm~\ref{NonCvxAL} follow from Theorem~\ref{DR:convergence} and Theorem~\ref{DR:rate}. 


    \begin{theorem} \label{th:AL-convergence}
    Suppose that $f: \R^n \to \R \cup \{+\infty\}$ is a proper lower semicontinuous function, and that problem \eqref{primal:2} has a nonempty set of solutions. Then, for $\lambda \in (0,2)$, and $0 < \gamma < \frac{2-\lambda}{2L}$, any bounded sequence $\{(z^k,x^k,s^k)\}$ generated by Algorithm~\ref{NonCvxAL} satisfies:

    \begin{itemize}
        \item[(i)] The sequence $\{f(x^k) + \frac{\mu}{2}d_{\mathcal{C}}^2(z^k)\}$ converges to a critical value $\varphi^{\star}$ of $f + \frac{\mu}{2}d_{\mathcal{C}}^2.$ 
        \item[(ii)] The sequence $\{z^k - x^k\}$ converges to $0$,  all cluster points of $\{z^k\}$ and $\{x^k\}$ coincide, and are also critical points of problem  \eqref{primal:2}, with critical value  $\varphi^{\star}$. %Additionally, if \eqref{CQ} holds at any of these cluster points, then such points are also critical for problem  \eqref{PH:primal1}.
    \end{itemize}

    In addition, assume  $f$ is a weakly convex function,  conditions \eqref{EB-f} for and \eqref{PSIS} hold for $\varphi = f + \frac{\mu}{2}d_{\mathcal{C}}^2$. Then:
        \begin{itemize}
            \item[(iii)] The sequence $\{f(x^k)+\frac{\mu}{2}d_{\mathcal{C}}^2(z^k)\}$ $R-$linearly converges to a critical value $\varphi^{\star}$ of $f + \frac{\mu}{2}d_{\mathcal{C}}^2.$

            \item[(iv)] The sequences $\{z^k\}$ and $\{x^k\}$ $R-$linearly converge to a critical point $x^{\star}$ of problem \eqref{primal:2}, $\{s^k\}$ $R-$linearly converges to a point $s^{\star}$, such that $x^{\star} = (\frac{1}{1+\gamma\mu})s^{\star} + (\frac{\gamma\mu}{1+\gamma\mu})P_{\mathcal{C}}[s^{\star}],$ and $\{w^k\}$ $R-$linearly converges to a point $w^{\star}$, such that $w^{\star} = \mu(x^{\star}-P_{\mathcal{C}}[x^{\star}]).$
        \end{itemize}
        
    \end{theorem}

    \begin{proof}
        Following the notation of \S\ref{section:DR}, take $\varphi_1 = \frac{\mu}{2}d_{\mathcal{C}}^2$,  $\varphi_2 = f$, $u^k = z^k$, $v^k = x^k$, and same notation for $s^k $. Each step in \eqref{DR:scheme} corresponds to one or more update rules of Algorithm~\ref{NonCvxAL}. Indeed,
        
        \begin{itemize}
            \item DRS update rule for $u^k$:  as proved in \cite[section 4]{li2016douglas}, computing $\prox_{\gamma \varphi_1}(s^k)$ in \eqref{DR:scheme} amounts to solve \begin{equation*}
        \inf_{z} \left\{\frac{\mu}{2} d_{\mathcal{C}}^2(z) + \frac{1}{2\gamma}\| z - s^k\|^2 \right\}, 
        \end{equation*} obtaining the following solution, \begin{equation*}
            z^k = \frac{1}{1+\gamma\mu}(s^k + \gamma \mu P_{\mathcal{C}}[s^k]),
        \end{equation*} the same update rule for $z^k$ in step~\ref{AL-up:uk} of Algorithm~\ref{NonCvxAL}.

        \item DRS update rule for $v^k$: from \eqref{u-OC}-\eqref{DRE}, finding an element $v^k$ of $\prox_{\gamma \varphi_2}(2u^k - s^k)$ corresponds to finding a solution of\begin{equation*}
            \inf_{x \in \R^n} \left\{f(x) + \left\langle \nabla\left( \frac{\mu}{2}d_{\mathcal{C}}^2\right)(z^k), x \right\rangle + \frac{1}{2\gamma} \| x - z^k\|^2\right\}.
        \end{equation*} Moreover, from \eqref{d-derivative} and the update rule of $w^k$ of step~\ref{AL-up:uk} in Algorithm~\ref{NonCvxAL}, \begin{equation*}
            \nabla\left( \frac{\mu}{2}d_{\mathcal{C}}^2\right)(z^k) = \mu(z^k - P_{\mathcal{C}}[z^k]) = w^k.
        \end{equation*} In other words, computing $\prox_{\gamma \varphi_2}(2u^k - s^k)$ is equivalent to solving problem \eqref{subproblem:primal} of step~\ref{AL-up:vk} in Algorithm~\ref{NonCvxAL}.

        \item DRS update rule for $s^k$: the multiplier update of step~\ref{AL-up:sk} in Algorithm~\ref{NonCvxAL} is exactly the same as in \ref{DR:scheme}.
        \end{itemize}

    Furthermore, Assumption~\ref{assumptions:blanket} is satisfied for the aforementioned functions. Hence, items (i) and (ii) follow from Theorem~\ref{DR:convergence}. %and Lemma~\ref{lem:prim-rel}.

    Regarding items (iii) and (iv), the results immediately follow by applying Theorem~\ref{DR:rate} with the notation identification of the beginning of the proof, and observing that $z^k \to x^{\star}$ as $k \to +\infty$, implies  $w^k = \mu(z^k - P_{\mathcal{C}}[z^k]) \to \mu(x^{\star} - P_{\mathcal{C}}[x^{\star}])$. The rate of convergence for $\{w^k\}$ follows from \begin{equation*}
            \begin{array}{rcl}
                 \|w^k - w^{\star}\|& \le & \| \mu(z^k - x^{\star}) - \mu(P_{\mathcal{C}}[z^k] - P_{\mathcal{C}}[x^{\star}]) \|  \\
                 & \le & 2\mu \| z^k - x^{\star} \|,
            \end{array}
        \end{equation*}      due to the triangle inequality and nonexpansiveness of $P_{\mathcal{C}}.$
    \end{proof}

  
We have shown that Algorithm~\ref{NonCvxAL} asymptotically generates
critical points of \eqref{primal:2}. Our next result shows
that those limit points will be solutions of problem \eqref{primal:1}. First, we need to introduce the value function associated to problem \eqref{primal:1}: given $u \in \R^n$, consider \begin{equation*}
    p(u) = \inf\{ f(x) : x \in C, \: x + u \in C \}.
\end{equation*} The crucial condition connecting problems \eqref{primal:1} and \eqref{primal:2} corresponds to the local behavior of $p$ around $0$, akin to strong duality (cf. \cite[Example 11.62]{rockafellar2009variational}).





 \begin{proposition} \label{lem:prim-rel}

Suppose $f: \R^n \to \R \cup \{+\infty\}$ is a proper lower semicontinuous function bounded from below, such that for any $\alpha \in \R$, the lower level sets \begin{equation*}
    \{ x \in \R^n : f(x) \le \alpha, \mbox{ for all } x \in C, x + u \in C \}
\end{equation*} are bounded locally uniformly on $u$. Moreover, assume the value function $p: \R^n \to \R \cup \{+\infty\}$ associated to problem \eqref{primal:1} satisfies the following condition: there exists $\bar{\mu} > 0$ and $\delta > 0$, such that for all $u \in B(0, \delta)$ \begin{equation*} p(u) + \frac{\bar{\mu}}{2}\|u\|^2 \ge p(0). \end{equation*}
Then, any solution to problem \eqref{primal:2} is also a solution to problem \eqref{primal:1}.
\end{proposition}

\begin{proof}
The proximal Lagrangian (\cite[Example 11.57]{rockafellar2009variational}) associated with problem \eqref{primal:1} is \begin{equation*}
    \bar{l}(x,y,\mu) = f(x) + \frac{\mu}{2}\left( d_{\mathcal{C}}(\mu^{-1}y + x)^2 - \|\mu^{-1}y \|^2 \right),
\end{equation*} which for $y = 0$ coincides with the objective function for problem \eqref{primal:2} for $r = \mu$. Then, the assumptions agree with the ones of \cite[Theorem 11.61]{rockafellar2009variational}, and thus for all $ \mu> \bar{\mu}$, the minimizers of $\bar{l}(\cdot, 0, \mu)$ coincide with the solutions of problem \eqref{primal:1}.

\end{proof}

\begin{remark} \label{global-solutions}
    When $f$ is weakly convex, for sufficiently large $\mu$, $f(x) + \frac{\mu}{2}d_{\mathcal{C}}^2(x)$ is a convex function, therefore any critical point for such function is a global minimizer.
\end{remark}

\subsection{Nonconvex Progressive Hedging} \label{section:PH}

{This DRS variant relies on a clever reformulation of non-anticipativity
constraints, performed in the primal space. The reason is that we are interested
in the nonconvex setting. In the classical approach, by making copies of the decision
variable, and adding non-anticipative constraints, a multistage stochastic
optimization problem has a dual problem whose structure is the one
in \eqref{primal:1}. However, dealing with the dual problem would restrict the
applicability of the approach to convex problems only. 
Instead, as explained below, we derive the extended PH approach by working in the primal space.


Suppose that the underlying random variable of the problem has finite support, enumerated by $i \in \{1, \dots, N\} \subseteq \mathbb{N}$. 
 Furthermore, for a time horizon divided in $T$ stages, the decision variables
 are denoted by $(x,x_{T}) \in \R^{n^{\prime}}$, where $x =(x_{[1]}, \dots,
 x_{[T]}) \in \R^n$ gathers the decision variables of stages $t =1, \dots, T$. Concerning scenarios, for $i= 1, \dots, N$, $x_i \in \R^{n_i}$ denotes a
 decision variable corresponding to scenario $i,$ with $n_1 + \dots + n_S = n.$
 Each scenario $i$ has probability $p_i >0$, so that $\sum_{i=1}^N p_i = 1.$ This probability distribution defines a weighted  inner product, typical of Progressive Hedging:\begin{equation*}
    \forall x,z \in \R^{n}, \quad \langle x, z \rangle = \sum_{i=1}^N p_i x_i^{\top} z_i, 
\end{equation*} where $\top$ denotes the inner product in $\R^{n_i}$ with all vectors being column vectors. The corresponding 
norms, induced respectively by the weighed and $\top$ inner products are
$\|\cdot\|$  and $|\cdot|$ (the inner product $\top$  may vary among  $\R^{n_i}$ for different
 $i$'s, but keep the same symbol, not to burden the notation).


In the multistage stochastic problem under consideration, the objective is
\begin{equation*}
    f(x) = \sum_{i=1}^N p_i f_i(x_i),
\end{equation*} where each $F_i: \R^{n_i^{\prime}} \to \R \cup \{+\infty\}$ is a proper lower semicontinuous function.
Decision variables satisfy two types of constraints:
\begin{itemize}
    \item for $t=1, \dots, T-1$, the decision variable $x_{[t]}$ associated with stage $t$ can only depend on (random) information revealed up to stage $t-1.$ This defines a linear subspace $\mathcal{N} \subseteq \R^{n}$, called the nonanticipativity subspace.
    \item for scenario $i \in \{1, \dots, N\}$, $x_i\in \R^{n_i^{\prime}}$ belongs to a nonempty convex compact set $X_i$, where $n_1^{\prime} + \dots + n_N^{\prime} = n^{\prime}$. These constraints can be implicitly represented in the extended-valued objective function as $f_i + i_{X_i}$. We keep the notation $f_i$ for simplicity, understanding that it includes, whenever necessary, this second type of constraint. This yields to the assumption of $f$ having compact domain.
\end{itemize}
Accordingly, the problem to be solved is 
\begin{equation} \label{PH:primal2}\left\{
\begin{aligned}
& \underset{x}{\text{min}}
& & f(x) =\displaystyle \sum_{i=1}^N p_i f_i(x_i)  \\
& \text{s.t.}
& & x \in \mathcal{N},
\end{aligned}\right.
\end{equation} 
where we recognize 
the structure \eqref{primal:1}, written with $\mathcal{C} = \mathcal{N}$ and for
a separable objective function. Algorithm~\ref{NonCvxAL} in this setting yields
the Progressive Heding Algorithm~\ref{NonCvxPH}, whose primal subproblems can be
solved in parallel, separately for each scenario. 



    \begin{lemma}
        Algorithm~\ref{NonCvxPH} corresponds to Algorithm~\ref{NonCvxAL} implemented for problem \eqref{PH:primal2}.
    \end{lemma}

    \begin{proof}
        Consider $\mathcal{C} = \mathcal{N}$ in problem \eqref{PH:primal2}. By applying Algorithm~\ref{NonCvxAL} in this setting, 
        \begin{itemize}
                \item the update rule for $x^k$ in Algorithm~\ref{NonCvxAL} combined with the definition of $f$ yields the following subproblem objective function \begin{equation*}
            \begin{array}{rcl}
                 f(x) +  \langle w^k ,  x\rangle + \dfrac{1}{2 \gamma} \| x - z^k \|^2 & = & \displaystyle\sum_{i=1}^N p_i \left( f_i(x_i) + (w^k_i)^{\top}x_i + \dfrac{1}{2 \gamma}| x_i - z^k_i |^2  \right)    
            \end{array}
        \end{equation*} 
        
        Therefore, solving problem \eqref{subproblem:primal} is equivalent to separately solving \eqref{subproblem:primal-s} for all $i=1,\dots, N,$
        
         \item the update rules for $s^k$and $z^k$ are the same in both algorithms,
           
        \item the update rules for $w^k$  and $z^k$ in step~ \ref{AL-up:uk} in  Algorithm~\ref{NonCvxAL} yields\begin{equation*}
                \begin{array}{rcl}
                     w^k &=& \mu(z^k - P_{\mathcal{N}}[z^k])  \\
                     &=& \mu P_{\mathcal{N}^\perp}[z^k] \\
                     &=& \mu P_{\mathcal{N}^\perp}\left[\left(
    \dfrac{1}{1+\gamma\mu} \right)s^k + \left( \dfrac{\gamma\mu}{1+\gamma \mu} \right)
    P_{\mathcal{N}}[s^k]\right] \\
    &=& \left(
    \dfrac{\mu}{1+\gamma \mu} \right)P_{\mathcal{N}^\perp}\left[s^k\right] + \left( \dfrac{\gamma\mu^2}{1+\gamma\mu} \right)P_{\mathcal{N}^\perp}\left[ 
    P_{\mathcal{N}}[s^k]\right] \\
    &=& \left(
    \dfrac{\mu}{1+\gamma\mu} \right)P_{\mathcal{N}^\perp}\left[s^k\right] \\
    &=& \left(
    \dfrac{\mu}{1+\gamma\mu} \right)(s^k -P_{\mathcal{N}}\left[s^k\right]) ,
                \end{array}
            \end{equation*} where we use the Moreau identity in the second and last equality, and the linearity of the projection over a subspace in the fourth equality. 

        \end{itemize}
    \end{proof}

    \begin{algorithm}[hbt] \caption{Progressive Hedging, a nonconvex version} \label{NonCvxPH}
\begin{algorithmic}[1]

    \State \textbf{Initialization}:  choose $z^0 \in \mathcal{N}$, $\gamma >0$, and
    $\lambda > 0.$ Take $s^0 = z^0$, and $w^0 = 0 \in \R^n$.

    \For{$k=0,1, ...$}
   \State \textbf{Primal subproblems:} for each scenario $i=1, \dots, N$, find a solution $x^k_i$ of  \begin{equation}   \label{subproblem:primal-s} \min_{x_i} \left\{ f_i(x_i) +
    (w^k_i)^{\top}
   x_i + \dfrac{1}{2 \gamma} | x_i - z^k_i |^2 \right\} \end{equation}   

    \State \textbf{Dual updates}: for each scenario $i = 1, \dots, N$, define \begin{equation*}
        s^{k+1}_i = s^k_i + \lambda (x^k_i - z^k_i),
    \end{equation*} and compute $s_\mathcal{N}^{k+1} = P_{\mathcal{N}}[s^{k+1}]$. Then, set for each scenario $i = 1, \dots, N$, \begin{equation*}
         w^{k+1}_i = \dfrac{\mu}{1+\gamma\mu} (s^{k+1}_i - s_{\mathcal{N},i}^{k+1})
    \end{equation*}

    \State \textbf{Primal update:} define for each scenario $i = 1, \dots, N$, \begin{equation*}
        z^{k+1}_i = \left(
    \dfrac{1}{1+\gamma\mu} \right)s^{k+1}_i + \left( \dfrac{\gamma\mu}{1+\gamma\mu} \right)
    s_{\mathcal{N},i}^{k+1}.
    \end{equation*}
    \EndFor \end{algorithmic} \end{algorithm}

    \begin{remark}
        Note that  $w^{k} = \frac{\mu}{1+\gamma\mu}(s^{k} - P_{\mathcal{N}}[s^{k}])$ implies $w^k \in \mathcal{N}^{\perp}$, just as in the original Progressive Hedging algorithm. Furthermore, $s^{k+1} = s^k + \lambda (x^k - z^k)$ resembles the update rule for the dual variable in Progressive Hedging. In the case of Algorithm~\ref{NonCvxPH}, $s^k$ plays the role of correcting dual information using the primal subproblem solutions and the primal correction step performed for $z^k$, while with $w^k$  we correct dual infeasibility.
    \end{remark}

    
     Following the reasoning of \S~\ref{section:WC}, next we establish the rate of convergence of the nonconvex variant of Progressive Hedging. In order to apply Theorem~\ref{th:AL-convergence}, $f$ needs to be a weakly convex function. The proof of the following results can be found in the appendix.
    \begin{lemma} \label{lemma:app}
        The objective function of problem \eqref{PH:primal2} is weakly convex, whenever each $f_i$ is a weakly convex function bounded from below, for $i = 1, \dots, N$.
    \end{lemma}
    
    The convergence of Algorithm~\ref{NonCvxPH} follows from Theorem~\ref{th:AL-convergence}, since problem \eqref{PH:primal2} is a special case of problem \eqref{primal:2}.  

    \begin{theorem} \label{th:PH-convergence}
    Suppose that $f: \R^{n} \to \R \cup \{+\infty\}$ is a  proper lower semicontinuous function with compact domain. Then, for $\lambda \in (0,2)$, and $0 < \gamma < \frac{2-\lambda}{2L}$, any bounded sequence $\{(z^k,x^k,s^k)\}$ generated by Algorithm~\ref{NonCvxAL} satisfies:

    \begin{itemize}
        \item[(i)] The sequence $\{f(x^k) + \frac{\mu}{2}d_{\mathcal{N}}^2(z^k)\}$ converges to a critical value $\varphi^{\star}$ of $f + \frac{\mu}{2}d_{\mathcal{N}}^2.$ 
        \item[(ii)] The sequence $\{z^k - x^k\}$ converges to $0$,  all cluster points of $\{z^k\}$ and $\{x^k\}$ coincide, and are also critical points of $f + \frac{\mu}{2}d_{\mathcal{N}}^2$, with critical value  $\varphi^{\star}$. 
    \end{itemize}

    In addition, suppose that $\{f_i: \R^{n_i} \to \R \cup \{+\infty\}\}_{i=1}^N$ is a family of weakly convex functions, and   conditions \eqref{EB-f} and \eqref{PSIS} hold for $\varphi = f + \frac{\mu}{2}d_{\mathcal{N}}^2$. Then, the following holds.

    \begin{itemize}
            \item[(iii)] The sequence $\{f(x^k)+\frac{\mu}{2}d_{\mathcal{N}}^2(z^k)\}$ $R-$linearly converges to a critical value $\varphi^{\star}$ of $f + \frac{1}{2}d_{\mathcal{N}}^2.$

            \item[(iv)] The sequences $\{z^k\}$ and $\{x^k\}$ $R-$linearly converge to a critical point $x^{\star}$ of problem \eqref{PH:primal2}, $\{s^k\}$ $R-$linearly converges to a point $s^{\star}$, such that $x^{\star} = (\frac{1}{1+\gamma\mu})s^{\star} + (\frac{\gamma\mu}{1+\gamma\mu})P_{\mathcal{N}}[s^{\star}],$ and $\{w^k\}$ $R-$linearly converges to a point $w^{\star}$, such that $w^{\star} = \mu P_{\mathcal{N}^{\perp}}[x^{\star}].$
        \end{itemize}
        
    \end{theorem}

    \begin{proof}
        Problem \eqref{PH:primal2} has a nonempty set of solutions, since $f$ is proper lower semicontinuous with compact domain and $\mathcal{N}$ is closed. The result follows directly from Theorem~\ref{th:AL-convergence}.
    \end{proof}

    Finally, in order to obtain global solutions of problem \eqref{PH:primal2} as observed in \Cref{global-solutions}, the regularity assumptions of \Cref{lem:prim-rel}  need to hold. For instance, the problem described in the next section allows us to think in global minimizers in the weakly convex case.

    
    
    

%    As \eqref{DR:scheme} and Algorithm~\ref{NonCvxAL} converge linearly to critical points when the objective is weakly convex, the next result states that Algorithm~\ref{NonCvxPH} also converges linearly, since it is a special case of the former methods.


    %\begin{proof}
     %   For each $i=1, \dots, N,$, from the definition of $f_i$ it holds that \begin{equation*}
      %      f_i(x_i) = \inf_{x_{iT}} F_i(x_i,x_{iT}) + i_{X_i}(x_i,x_{iT})  \ge \inf_{(x_i, x_{iT})\in X_i} F_i(x_i,x_{iT}).
       % \end{equation*} The left-hand side is bounded from below due to \cite[Corollary 1.10]{rockafellar2009variational}, and thus $F_i + i_{X_i}$ is bounded from below. Therefore, from the assumptions, $f_i$ is a weakly convex function, and thus $f$ is weakly convex as well. In this way, the results follow from Theorem~\ref{th:AL-convergence}.
    %\end{proof}


\section{Numerical tests}  \label{section:numerical}

The numerical experiments are performed for the phase retrieval problem described in \cite[\S 5.1]{davis2019stochastic}. The goal of the phase retrieval problem is to find a point $x^{\star}$ that (approximately) simultaneously solves the equations $\langle a_i, x \rangle ^2 = b_i$, for $i = 1, \dots, N$, $a_i \in \R^n$ and $b_i \ge 0.$ As an optimization problem, we seek to find a point $x^{\star}$ such that \begin{equation} \label{problem:PR}
    \min_{x \in \R^n} \frac{1}{N} \sum_{i=1}^N |\langle a_i, x \rangle^2 - b_i|.
\end{equation} Note that each $x \mapsto |\langle a_i, x \rangle^2 - b_i|$ is a weakly convex function, as the composition of the Lipschitz convex function absolute value with the smooth function $x \mapsto \langle a_i, x \rangle^2 - b_i$ with Lipschitz derivative (see \cite[Lemma 4.2]{drusvyatskiy2019efficiency}).



For the numerical examples, we draw the slopes $a_i$ from a standard Gaussian distribution in $\R^n$, for $i = 1, \dots, N$. We choose a target $\bar{x} \in \R^n$, and define $b_i = \langle a_i, \bar{x} \rangle^2 $. Since problem \eqref{problem:PR} has a decomposable structure, we create $N$ copies $x_i$ of the variable $x$, and define $\mathcal{N} = \{ (x_i)_{i=1}^N : x_1 = \dots = x_N \}$, corresponding to a $1-$stage nonanticipative subsapce. Therefore, problem \eqref{problem:PR} can be reformulated as \begin{equation} \label{problem:PR-ref}
\left\{\begin{aligned}
& \underset{}{\text{min}}
& &  \frac{1}{N} \sum_{i=1}^N |\langle a_i, x_i \rangle^2 - b_i| \\
& \text{s.t.}
& & (x_i)_{i=1}^N \in \mathcal{N}.
\end{aligned}\right.
\end{equation} 

The battery of randomly generated problems parses
the three settings described in \cite[\S 5.1]{davis2019stochastic}: 
\[(N,n) \in \{(30,10), (150, 50), (300,100)\},\]for 15 uniformly randomly generated initial points in the unit sphere. We also define $5000$  as the maximum number of iterations, with a target accuracy of $10^{-6}$ for the objective function value to stop iterations. We set the scaling parameter $\mu= \frac{\sqrt{N}}{2}$, since it provides the best performance out of the tests we run. Note that, in this case, $L=\mu$, since the gradient of the function $\frac{\mu}{2}d_{\mathcal{N}}^2$ is $\mu-$Lipschitz. Furthermore, we use $20$ equally spaced values of $\lambda$ between 0.05 and 1.95, and $5$ equidistant values of $\gamma$, from $0.01$ to $0.99\left( \frac{2-\lambda}{2 \mu}\right).$

For these test-functions, the primal subproblems solved in Step 2 of
Algorithm~\ref{NonCvxPH} have explicit solution. More precisely, at iteration
$k$ the corresponding optimality condition
\begin{equation*}
     \min_{x_i \in \R^n}   |\langle a_i, x_i \rangle^2 - b_i| +
    (w^k_i)^{\top}x_i + \dfrac{1}{2 \gamma} | x_i - z^k_i |^2 ,
 \end{equation*} 
 yields four critical points that are candidates to  subproblem solution, namely 
 \begin{equation*}
        z_i^k - \gamma \left( w_i^k - 2\left[ \dfrac{\gamma\langle w_i^k, a_i \rangle - \langle z_i^k, a_i \rangle }{2 \gamma \|a_i\|^2 \pm 1}\right]a_i \right),
 \end{equation*} and  \begin{equation*}
       z_i^k - \gamma \left( w_i^k - \left[ \dfrac{\gamma\langle w_i^k, a_i \rangle - \langle z_i^k, a_i \rangle \pm \sqrt{b_i}}{\|a_i\|^2 }\right]a_i \right).
 \end{equation*}

 Note that when the dual iterate $w_i^k = 0$, we retrieve the solutions obtained
 by applying the stochastic proximal point (SPP) method (cf. \cite[\S 5.1]{davis2019stochastic}).

 Table~\ref{tab:nPH-1} shows the accuracy achieved 
 for the objective function $f(z^k) + \frac{\mu}{2}d_{\mathcal{N}}^2(x^k)$ by Algorithm~\ref{NonCvxPH}. It is clear that all problems were solved at best with a low accuracy of $10^{-1}$, while nearly $38\%$ of them reached the target accuracy of $10^{-6}$ or better. These results include all the values of $\lambda$ and $\gamma$ we tested, that is, various possible methods (by varying $\lambda$) and several stepsizes.

  \begin{table}[ht]
\centering
\begin{tabular}{c||c|c|c|c|c|c}
accuracy & $<10^{-1}$ &  $<10^{-2}$ & $<10^{-3}$  & $<10^{-4}$  & $<10^{-5}$  & $<10^{-6}$  \\ \hline
\% & 100 & 99.365 & 68.254 & 62.857 & 53.016 & 37.778 \\
\end{tabular}
\caption{Percentage of problems solved by Algorithm~\ref{NonCvxPH} for different levels of accuracy, and different values of $\lambda \in (0,2)$, and $\gamma \in (0, \frac{2-\lambda}{2\mu})$.}
\label{tab:nPH-1}
\end{table}
 As a rule of thumb, better accuracies are obtained whenever $\lambda$ and $\gamma $ increase within their bounds. For instance, Table~\ref{tab:nPH-2} shows the results for $\lambda = 1.95$ and $\gamma = 0.99 \left( \frac{2-\lambda}{2\mu} \right)$. Observe that in this case, almost $96 \%$ of the problems attained the target accuracy of $10^{-6}$ or beyond. Choosing the best pair of parameters $(\lambda, \gamma)$ could be a problem-dependent issue, and further investigation is required.



\begin{table}[h!]
\centering
\begin{tabular}{c||c|c|c|c|c|c}
accuracy & $<10^{-1}$ &  $<10^{-2}$ & $<10^{-3}$  & $<10^{-4}$  & $<10^{-5}$  & $<10^{-6}$  \\ \hline
\% & 100 & 100 & 100 & 100 & 100 & 95.556 \\ 
\end{tabular}
\caption{Percentage of problems solved by Algorithm~\ref{NonCvxPH} for different levels of accuracy,  $\lambda = 1.95$, and $\gamma = 0.99 \left( \frac{2-\lambda}{2\mu} \right)$.}
\label{tab:nPH-2}
\end{table}

The benchmark in 
 Figure~\ref{fig:PP} shows 
the performance of Algorithm~\ref{NonCvxPH} for problem \eqref{problem:PR-ref}, the Elicited Progressive Decoupling method \cite{rockafellar2019progressive,sun2021elicited} for problem \eqref{problem:PR-ref},  and the stochastic prox-linear  method described in \cite{davis2019stochastic}, applied to problem \eqref{problem:PR}. The two latter methods only require setting a stepsize $\gamma$. For our numerical tests, we use $100$ equally distributed stepsizes in the interval $(10^{-4},1)$, as in \cite{davis2019stochastic}. For the Elicited Progressive Decoupling, we set the elicitation parameter $e=0$, following the numerical results of the authors in \cite{sun2021elicited}, in order to accelerate convergence. The starting points, maximum number of iterations, and target accuracy are the same as described before.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{Fig1.pdf}
\caption{Performance profile of the accuracy of the best objective function value found along iterations, 
for Algorithm~ \ref{NonCvxPH} (DR) using different values of $\lambda$ and $\gamma$, corresponding to Table~\ref{tab:nPH-1}, Elicited Progressive Decoupling (e-PD), and stochastic prox-linear (SPL) method.}
\label{fig:PP}
\end{figure}
  
 We observe that, on the considered battery of functions and regarding accuracy,
 Algorithm~\ref{NonCvxPH} and the Elicited Progressive Decoupling method always outperform the prox-linear method, the one that provided the best results in \cite{davis2019stochastic}. The nonconvex version of Progressive Hedging, for different values of $\lambda$, can solve  approximately $40 \%$ of the problems with an accuracy of order $10^{-6},$ while the Elicited Progressive Decoupling solve around $55\%$ of the problems with this accuracy. Contrast these results with the best performance found for our proposal (Table~\ref{tab:nPH-2}).
 
 For accuracy of order between  $10^{-4}$ and $10^{-1}$, our proposed method shows a better behavior than the Elicited Progressive Hedging. This is because the latter presents a relatively polarized performance, since it solves approximately half of the problems with an accuracy of $10^{-6}$ or better, and the other half just achieved an accuracy worse than $10^{-2}$. In this sense, the nonconvex version of Progressive Hedging possesses a more distributed performance, achieving different levels of accuracy.

%stochastic prox-linear  and the stochastic proximal point methods can achieve that amount of solved problems for an accuracy of   just around $10^{-2}$ and $10^{-1}.$ The stochastic subgradient method shows the worst performance overall.  Note that the behaviour of the three methods of \cite{davis2019stochastic} are  consistentwith the results reported in that work.





}


\section{Concluding remarks} \label{section-final}


\indent We obtain local linear rates of convergence for weakly convex Douglas-Rachford, and apply these results to obtain a linearly convergent splitting scheme for constrained optimization problems. In particular, when the objective function has a decomposable structure, we propose an algorithm resembling Progressive Hedging. 

In \S~\ref{section:WC}, we reformulate the original problem using $d_{\mathcal{C}}^2$ as a penalty function. Other smooth penalty functions could be employed  to obtain other variants of Algorithm~\ref{NonCvxAL}. Furthermore, for some structured optimization problems, computing the projection could be costly. In these cases, methods with inexact restoration can be alternatively used, as in \cite{bueno2022inexact}.

Regarding the implementation of Algorithm~\ref{NonCvxPH}, we need to design implementable criticality certificates that use the information generated along the iterations, such as the ones proposed in \cite{atenas2023unified} for weakly convex distributed bundle methods. These criticality certificates would complement the existing complexity estimates, since in some applications the optimal value might not be known, and thus reliable stopping tests are required in order to identify good candidate solutions.



\bmhead{Declarations} 
\bmhead{Funding} This study was funded by FAPESP grant 2019/20023-1. 
\bmhead{Conflicts of interest/Competing interests}
The author has no relevant financial or non-financial interests to disclose.






%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

%%===========================================================================================%%






\begin{appendices}

\section{Marginal of a weakly convex function}

The following result presents a slightly more general setting than \S~\ref{section:PH}, for which a marginal function is weakly convex.

    

    \begin{lemma} \label{marginal-w-cvx} Let $\Phi: \R^n \times \R^m \to \R \cup
\{+\infty\} $ be a proper $\rho-$weakly convex function bounded from below.
Then, the marginal function $\phi: \R^n \to \R \cup \{+\infty\}$, defined as
\begin{equation*} \phi(x) = \inf_{y \in \R^m} \Phi(x,y), \end{equation*}

    is also $\rho-$weakly convex.  \end{lemma}

\begin{proof} Since $F$ is $\rho-$weakly convex, then the function $\Phi_\rho:
\R^n \times \R^m \to \R \cup \{+\infty\}$ given by \begin{equation*} \Phi_\rho
(x,y) = \Phi(x,y) + \frac{\rho}{2}|||(x,y)|||^2, \end{equation*} is convex, where $|||(x,y)|||^2 = \|x\|^2 + \|y\|^2$. Let
$x_1, x_2 \in \R^n$, such that $\phi(x_1), \phi(x_2) \in \R,$ and $t \in (0,1).$
Then, for any $y_1, y_2 \in \R^m,$ and setting $ \zeta(t,x_1,x_2) = \phi(tx_1+(1-t)x_2) +
    \frac{\rho}{2}\|tx_1 + (1-t)x_2\|^2$, it follows \begin{equation*} \begin{array}{rcl} \zeta(t,x_1,x_2)  & \le & \Phi(tx_1 + (1-t)x_2, ty_1 +
    (1-t)y_2) +  \frac{\rho}{2}\|tx_1 + (1-t)x_2\|^2  \\ & \le & \Phi_\rho(tx_1
    + (1-t)x_2, ty_1 + (1-t)y_2) \\ & \le & t \Phi_\rho(x_1,y_1) +
    (1-t)\Phi_\rho(x_2,y_2) \\ & = & t\left( \Phi(x_1,y_1) +
    \frac{\rho}{2}|||(x_1,y_1)|||^2  \right) + (1-t)\left( \Phi(x_2,y_2) +
    \frac{\rho}{2}|||(x_2,y_2)|||^2  \right), \end{array} \end{equation*}

    where in the first inequality we use the definition of $\phi$, in the second
    inequality we add the term $\frac{\rho}{2}\|ty_1+(1-t)y_2\|^2,$ in the third
    inequality convexity of $\Phi_\rho$ is used, and the last equality follows
    from the definition of $\Phi_\rho.$ Taking the infimum, separately for $y_1,
    y_2 \in \R^m$, it holds that \begin{equation*} \zeta(t,x_1,x_2)
    \le t \inf_{y_1}\left\{\Phi(x_1,y_1) + \frac{\rho}{2}|||(x_1,y_1)|||^2
    \right\} + (1-t)\inf_{y_2}\left\{\Phi(x_2,y_2) +
    \frac{\rho}{2}|||(x_2,y_2)|||^2  \right\}.  \end{equation*}

    Since $\Phi$ is bounded from below, then for $i = 1,2$ \begin{equation*}
    \inf_{y_i} \left\{ \Phi(x_i,y_i)+  \frac{\rho}{2}|||(x_i,y_i)|||^2  \right\} =
    \inf_{y_i} \left\{ \Phi(x_i,y_i)+  \frac{\rho}{2}\|x_i\|^2  \right\}.
    \end{equation*} Hence, \begin{equation*} \begin{array}{rcl} \zeta(t,x_1,x_2) &\le& t
    \displaystyle\inf_{y_1}\left\{\Phi(x_1,y_1) + \frac{\rho}{2}\|x_1\|^2
    \right\} + (1-t)\displaystyle\inf_{y_2}\left\{\Phi(x_2,y_2) +
    \frac{\rho}{2}\|x_2\|^2  \right\}   \\\\ & = & t \left(\phi(x_1) +
    \dfrac{\rho}{2}\|x_1\|^2 \right)+ (1-t)\left(\phi(x_2) +
    \dfrac{\rho}{2}\|x_2\|^2 \right), \end{array} \end{equation*} that is,
    $\phi(\cdot) + \frac{\rho}{2}\|\cdot\|^2$ is convex.

\end{proof}

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%



\end{document}
