\section{Model Details.}
\label{app:models}

\textbf{Imagen details:} 
Imagen is a text-conditioned diffusion model that comprises of a frozen T5 \citep{raffel2020exploring} language encoder that encodes an input prompt into a sequence of embeddings, a $64\times64$ image diffusion model, and two cascaded super-resolution diffusion models that generate $256\times256$ and $1024\times1024$ images. Unlike Stable Diffusion, it operates directly on pixels instead of in a latent space. For our experiments, we use the $64\times64$ model which has 2B parameters and is trained using a batch size of 2048 for 2.5M training steps on a combination of internal datasets, with around 460M image-text pairs, and the publicly available Laion dataset \citep{schuhmann2021laion}, with  $~$400M image-text pairs. 

\textbf{Stable Diffusion details}.
We use Stable diffusion v1.4 which is a latent text-to-image diffusion model. It uses the pre-trained text encoder from CLIP to encode text and a pre-trained variational autoencoder to map images to a latent space. The model has 890M parameters and takes 512x512-resolution images as input. It was trained on various subsets of Laion-5B, including a portion filtered to only contain aesthetic images, for 1.2M steps using batch size of 2048.

\textbf{CLIP details: } CLIP encodes image features using a ViT-like transformer \citep{Dosovitskiy2020AnII} and uses a causal language model to get the text features. After encoding the image and text features to a latent space with identical dimensions, it evaluates a similarity score between these features. CLIP is pre-trained using contrastive learning. Here, we compare to the largest CLIP model (with a ViT-L/14@224px as the image encoder). The model is smaller than Imagen (400M parameters), but is trained for longer (12.8B images processed vs 5.B). While Imagen was trained primarily as a generative model, CLIP was primarily engineered to be transferred effectively to downstream tasks.

\section{Weighting Functions Details.}
\label{app:weighting}

\paragraph{Learned Weighting Function:}
While for most experiments we use a heuristic weighting function for $\vw_t$, we also explored learning an effective weighting function (although this is not truly zero-shot).
To do this, we aggregate scores for each image $\bx$ and class $\class_k$ into 20 buckets, with each bucket covering a small slice of timestep values:
\begin{align*}
    \bb_i(\bx, \class_k) = \bE_{\epsilon,t \sim \mathcal{U}[0.05i, 0.05(i + 1)]}  \|\bx - \tilde{\bx}_{\vtheta}\big({\bx}_{t}, \prompt(\class_k), t \big)\|_2^2
\end{align*}
where we estimate the expectation with Monte Carlo sampling (typically around 100 samples). We then learn a 20-feature linear model with parameters $[\bv_0, ..., \bv_{19}]$ over these buckets:
\begin{align*}
p_{\bv}(y = \class_k | \bx) = \frac{\exp(\sum_{i=0}^{19} -\bv_i \bb_i(\bx, \class_k) )}{ \sum_{\class_j \in [\class_K]} \exp( \sum_{i=0}^{19} -\bv_i \bb_i(\bx, \class_j) )}
\end{align*}
trained with standard maximum likelihood over the data. At test-time we use the weighting 
\begin{align*}
\vw_t = \bv_{\lfloor t / 0.05 \rfloor}
\end{align*}
We generally found that (1) learned weighting functions are pretty similar across datasets, and (2) the weighting functions are transferable: the $\bv$s learned on one dataset get good accuracy when evaluated on other ones. On average, learned weights produced around 1\% higher accuracy on zero-shot classification tasks, but we omitted the results from the main paper because using learned weights is not truly zero-shot.

\paragraph{Comparison of Weighting Functions.}

We compare the learned weighting functions with several heuristic functions on the Caltech101 dataset. We chose Caltech101 because it is high-resolution (SD performs poorly on low-resolution datasets), contains a diversity of image classes, was not used when we developed the heuristic weighting function, and only has 100 classes, so it is much faster to evaluate models on than ImageNet. We compare the following functions:
\begin{itemize}
    \item \textbf{VDM}: $\vw_t = \text{SNR}'(t)$, the derivative of the signal to noise ratio with respect to $t$. This weighting scheme from Variational Diffusion Models \citep{kingma2021variational} directly trains the model on a lower bound of the likelihood. 
    \item \textbf{Simple}: $\vw_t = \text{SNR}(t)$. This ``simple" loss from \citet{ho2020denoising} results in a model that produces better images according to human judgements and FID scores, even though it results in worse likelihoods. 
    \item \textbf{Heuristic}: $\vw_t = \exp{(-6t)}$. Our hand-engineered weighting function; we found this by searching for a simple weighting function that works well on CIFAR-100, although we found empirically it generalizes very well to other datasets. 
    \item \textbf{Learned}: Learning an effective weighting function on a held-out set of examples as described above.  
\end{itemize}

Results are shown in Table~\ref{tab:weighting}. The heuristic weighting function outperforms Simple and VDM for both models. Interestingly, SD appears to be more robust to the choice of weighting function than Imagen.
Mechanistically, the reason is that ``Simple” and ``VDM” weighting put more weight on earlier timesteps than ``Heuristic” and Imagen tends to be an inaccurate classifier at very small noise levels. We intuitively believe this is a consequence of pixel vs latent diffusion.
The learned weighting only does slightly better than heuristic weighting despite not being truly zero-shot. We found similar results to hold on other datasets. 

\begin{table}
\centering
\small
\begin{tabular}{l|c|c|}
\toprule
\textbf{Weighting}  & \textbf{Imagen} &  \textbf{Stable Diffusion} \\
\midrule
VDM & 62.0 & 71.9 \\
Simple & 56.1 & 71.4 \\
Heuristic & 68.9 & 73.0 \\
Learned & 70.2 & 73.1 \\
\bottomrule
\end{tabular}
\vspace{1mm}
\caption{Percent accuracy for models on Caltech101 with different weighting schemes
}
\label{tab:weighting}
\end{table}



\section{Variances in Classification Accuracies.}
Due to our reduced-size evaluation sets, variances in accuracy on zero-shot classification tasks across different random splits are roughly $\pm0.4\%$ for CLIP, $\pm 0.7\%$ for Imagen, and $\pm 0.6\%$ for Stable Diffusion. The diffusion models have higher variance due to the inherent randomness in noising images (while CLIP is deterministic). Overall, we are not interested in small accuracy differences anyway, as the comparison between models is non-direct in various ways; instead we are trying go get a broad understanding of the models' abilities.


\section{Details on Attribute Binding Tasks and Prompts}
\label{app:binding}

We use the relational dataset from \citet{lewis2022does} for the attribute binding experiments.
Each image consists of two objects of different shapes and colors; for tasks involving size we filter out examples where both objects are the same size. 
%The full set of prompts for the binding tasks are listed below. 
Each image contains two objects with different attributes $\mathsf{shape} \in \{\text{cube}, \text{sphere}, \text{cylinder}\}$, 
$\mathsf{color} \in \{\text{blue}, \text{cyan}, \text{blue}, \text{brown}, \text{gray}, \text{green}, \text{purple}, \text{red}, \text{yellow}\}$,
$\mathsf{size} \in \{\text{small}, \text{large}\}$, and $\mathsf{position} \in \{\text{left}, \text{right}\}$.

Given a task (e.g. Shape$|$Size), we construct a task-specific description for an object as follows:
\begin{align*}
&\text{{\it ``On the \{$\mathsf{position}$\} is a "} if Position tasks else {\it ``A "}} + \\
&\text{``{$\mathsf{size}$} " if Size task else ``"} + \\
&\text{``{$\mathsf{color}$} " if Color task else ``"} + \\
&\text{``{$\mathsf{shape}$.}" if Shape task else {\it ``object."}}
\end{align*}
For recognition and binding tasks, we randomly select one of the two objects in the image to be the positive example and then use its description as the positive prompt. For pair tasks, we join the descriptions for both objects together with ``and" (removing the period from the first description and lowercasing the second one) for the positive prompt.

To construct a negative example for recognition tasks, we replace the positive attribute with a random attribute not in the image. For binding tasks, we replace one of positive description's attributes with the other object's attribute (e.g., for Shape$|$Color, we replace $\mathsf{shape}$). 

For pair tasks, there is a choice in how the two objects are ordered (e.g. ``On the left is a cube and on the right is a sphere" vs ``On the right is a sphere and on the left is a cube". We follow the preference of stating the leftmost position/shape/color/size first in that order. For example, this means we will always start with ``On the left..." rather than ``On the right...". Similarly, the negative example for Color,Size in Figure~\ref{fig:binding-tasks} is ``A large yellow object and small gray object" rather than  ``A small gray object and a large yellow object" because we prefer to first put the leftmost color over the leftmost size. 


We experimented with a variety of other prompts, but found none to work substantially better than these simple ones.

\begin{figure}
\begin{center}
\includegraphics[width=0.48\textwidth]{figs/confidences-v2.pdf}
\end{center}
\caption{
Model reliability diagram comparing confidence measures of Imagen on CIFAR-100. The number of model calls used in Algorithm~\ref{alg:efficient} produces better-calibrated confidences than using the actual scores for different classes. 
} 
\label{fig:calibration}
\end{figure}

\section{Calibration}
\label{sec:calibration}
It is desirable for classifiers, especially when used in the zero-shot setting with possibly out-of-domain examples, to be well calibrated.
In other words, if a classifier predicts a label $\tilde{y}_i$ with probability $p$, the true label should be $\tilde{y}_i$ roughly $100\cdot p\%$ of the time.
However, the diffusion model classifier does not directly produce probabilities for classes.
While $p(y_i = \class_k | \bx_i)$ should roughly be proportional to the expectation in \Cref{eq:score} when exponentiated (i.e. we can apply a softmax to the average weighted scores to get probabilities), in practice our estimates of the expectations are very noisy and do not provide well-calibrated scores.

We propose a simple alternative that takes advantage of early pruning: we use the total number of diffusion model calls used for the image as a calibration measure. 
The intuition is that a harder example will require more scores to determine the $\argmin$ class with good statistical significance.

More details on the two calibration methods are below:

\paragraph{Temperature-scaled raw scores.}
We use $s_{\class_k}(\bx)$ to denote the weighted average squared error for class $\class_k$ on image $\bx$, i.e., the Monte-Carlo estimate for the re-weighted VLB in equation~\ref{eq:score}. We turn these scores into an estimated probability by applying a softmax with temperature:
\begin{align*}
p_\theta(y = \class_k | \bx) = \frac{\exp(-s_{\class_k}(\bx)/\tau)}{ \sum_{\class_j \in [\class_K]} \exp{(-s_{\class_j}(\bx)/\tau)}}
\end{align*}
Note that this approach requires good score estimates for each class, so it is not compatible with the class pruning method presented in Section~\ref{subsec:efficiency}. 

\paragraph{Platt-scaled number of scores.}
Our other confidence method relies on the total number of scores needed to eliminate all other classes as candidates. Let $\tilde{y}(\bx)$ denote the predicted class for example $\bx$ and $n(\bx)$ be the total number of calls to $\tilde{\bx}_\theta$ used to obtain the prediction when running Algorithm~\ref{alg:efficient}. Then we estimate
\begin{align*}
p_\theta(y = \tilde{y}(\bx) | \bx) = \text{sigmoid}(-n(\bx) / \tau + \beta)
\end{align*}

We learn $\tau$ (and $\beta$ for Platt scaling) on a small held-out set of examples. 

\label{app:calibration}

We show reliability diagrams \citep{degroot1983comparison} and report Expected Calibration Error \citep{guo2017calibration} (ECE) for the methods in Figure~\ref{fig:calibration}.
Using a small held-out set of examples, we apply temperature scaling \citep{guo2017calibration} for the score-based confidences and Platt scaling \citep{platt1999probabilistic} for the number-of-scores confidences, (see Appendix~\ref{app:calibration} for details).
Number of scores is fairly well-calibrated, showing it is possible to obtain reasonable confidences from diffusion model classifiers despite them not providing a probability distribution over classes.  


%\section{Example predictions for different text prompts}

%Our method is reliant on the model predicting different denoised images $\hat{\bx}$ from a noised image $\bx_t$ for different text prompts. Figure ~\ref{fig:denoised} shows predicted images. The model is very sensitive to the text conditioning, with it substantially changing model outputs. 

% \begin{figure*}[!ht]
%   \begin{center}
%     \includegraphics[width=\textwidth]{figs/imagen-denoise.pdf}
%   \end{center}
%   \vspace{-3mm}
%   \caption{Example predictions from Imagen for different inputs and text prompts. Each set of images shows the original, noised, and denoised images for the two classes. The top two rows use ImageNet images and the bottom row uses CueConflict.}
%   \label{fig:denoised}
% \end{figure*}


% \section{Digit Generation}


\begin{figure*}[!ht]
  \begin{center}
    \includegraphics[width=\textwidth]{figs/number8.png}
  \end{center}
  \caption{Example generated images from Stable Diffusion and Imagen for generating photos with text.}
  \label{fig:numbers}
\end{figure*}

\section{Imagen's Super-resolution Models}
\label{app:superres}
Imagen is a cascaded diffusion model \citep{ho2022cascaded} consisting of a $64\times64$ low-resolution model and two super-resolution models, one that upsample the image from $64\times64$ to $256\times256$ and one that upsamples from $256\times256$ to $1024\times1024$. However, we found only the $64\times64$ model to work well as a zero-shot classifier. 
The super-resolution models condition on a low-resolution input image, which means they denoise effectively regardless of the input prompt and thus aren't as sensitive to the class label. 
For example, unlike with Figure~\ref{fig:denoised}, high-resolution denoising with different text prompts produces images imperceptibly different to the human eye because they all agree with the same low resolution image.
Imagen's super-resolution models are trained with varying amount of Gaussian noise added to the low-resolution input image (separate from the noise added to the high-resolution image being denoised). We were able to alleviate the above issue somewhat by using a large amount of such noise, but ultimately did not achieve very strong results with the high-resolution models. 
For example, the $64\times64$ to $256\times256$ model achieves an accuracy of 16.1\% on ImageNet. 

We further experimented with combining the low-resolution model's scores with the $64\times64$ to $256\times256$ model's. To do this, we used the learned weighting scheme detailed in Appendix~\ref{app:weighting}, but with learning 40 weights: 20 for the low resolution model and 20 for the super-resolution model. However, we found the learned weighting scheme put almost no weight on the super-resolution model's scores, and did not perform significantly better than the low-resolution model did on its own. 

% We only use the $64\times64$ model for our experiments because we found the high-resolution models performed poorly as classifiers.
% Combining the $64\times64$ model's scores with scores from the higher-resolutions models did not improve results either (see Appendix~\ref{app:superres} for details). 
% The issue is that high-resolution models condition strongly on their low-resolution inputs and are therefore less sensitive to the text prompt. Unlike with Figure~\ref{fig:denoised}, high-resolution denoising with different text prompts produces images imperceptibly different to the human eye because they all agree with the same low resolution image.

