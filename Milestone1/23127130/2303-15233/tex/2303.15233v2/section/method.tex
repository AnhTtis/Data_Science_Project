\section{Zero-Shot Classification using Diffusion Models}
\label{sec:method}
In this section, we show how to convert the generation process of a text-to-image diffusion model into a zero-shot classifier to facilitate quantitative evaluation on downstream tasks.
Figure~\ref{fig:imagen_zs} shows an overview of our method.

\textbf{Diffusion Generative Classifier:} 
We begin with a dataset, $\big\{ (\bx^1, y^1), \ldots, (\bx^n, y^n) \big\} \subseteq \RR^{d_1 \times d_2} \times [\class_K]$ of $n$ images\footnote{For simplicity, we use $\bx$ in place of $\bx_0$ to refer to an image.} where each image belongs to one of $K$ classes $[\class_K] := \{\class_1, \class_2, \cdots, \class_K\}$.
Given an image $\bx$, our goal is to predict the most probable class assignment
\begin{align*}
    \tilde{y} &= \argmax_{\class_k} p(y = \class_k | \bx) = \argmax_{\class_k} p(\bx|y=\class_k)\cdot p(y=\class_k) 
    %&=\argmax_{\class_k}~ \log p_\theta(\bx_i|y_i=\class_k) \frac{1}{k} 
    = \argmax_{\class_k}~ \log p(\bx|y=\class_k). % \approx \cL_{\mathsf{Diffusion}}
\end{align*}
where we assume a uniform prior %over classes
$p(y_i=\class_k) = \frac{1}{k}$ that can be dropped from the $\argmax$.\footnote{We can't use a learned prior in the zero-shot setting.} A generative classifier \citep{ng2001discriminative} uses a conditional generative model with parameters $\theta$ 
to estimate the likelihood as $p_\theta(\bx|y=\class_k)$.

%A conditional generative model estimating $p(\bx |y = \class_k)$
%The likelihood $p(\bx|y=\class_k)$$
Using a text-to-image diffusion model as a generative classifier requires two modifications.
First, the models are conditioned on text prompts rather than class labels. Thus we convert each label, $\class_k$, to text using a mapping $\prompt$ with a dataset-specific template (e.g. $\class_k \to \mathsf{A~photo~of~a~}\class_k$).
Second, diffusion models do not produce exact log-likelihoods (\ie we cannot compute $\log p_\theta(\bx|y=\class_k)$ directly).
Our key idea for a solution is to use the $\mathsf{VLB}$ (more specifically $\cL_{\mathsf{Diffusion}}$ as Imagen and SD are not trained with the other losses) as a proxy. Thus we have:
% \begin{align}
%     \tilde{y} &= \argmax_{\class_k}~ \log p_\theta(\bx|y=\class_k) \nonumber \\
%     &\approx \argmin_{\class_k}~ \cL_{\mathsf{Diffusion}}(\bx, \class_k) \nonumber = \argmin _{\class_k \in [\class_K]} ~\bE_{\epsilon,t} \Big[ \vw_t \|\bx - \tilde{\bx}_{\vtheta}\big({\bx}_{t}, \prompt(\class_k), t \big)\|_2^2 \Big]
%     \label{eq:score}
% \end{align}
\begin{align}
    \tilde{y} &= \argmax_{\class_k}~ \log p_\theta(\bx|y=\class_k) \nonumber \approx \argmin_{\class_k}~ \cL_{\mathsf{Diffusion}}(\bx, \class_k) \nonumber \\
    &= \argmin _{\class_k \in [\class_K]} ~\bE_{\epsilon,t} \Big[ \vw_t \|\bx - \tilde{\bx}_{\vtheta}\big({\bx}_{t}, \prompt(\class_k), t \big)\|_2^2 \Big]
    \label{eq:score}
\end{align}
Note that for SD, $\bx$ and $\tilde{\bx}_{\vtheta}$ are latent representations, with $\bx$ obtained by encoding the image using a VAE. With Imagen on the other hand, $\bx$ consists of the raw image pixels.

\paragraph{Estimating the Expectation:}

We approximate the expectation in \cref{eq:score} using Monte-Carlo estimation. 
At each step, we sample a $t \sim \cU([0,1])$ and then a $\bx_t$ according to the forward diffusion process (\cref{eq:fwd}): $\bx_t \sim q(\bx_t | \bx_0)$.
Next, we denoise this noisy image using the model (\ie we use it to predict $\bx$ from $\bx_t$), obtaining $\hat{\bx} = \tilde{\bx}_{\vtheta}\big({\bx}_{t}, \prompt(\class_k), t \big)$.
We call the squared error of the prediction, $\|\bx - \hat{\bx}\|_2^2$, a {\it score} for $(\bx, \class_k)$.
We score each class $N$ times, obtaining a $K \times N$ {\it scores matrix\footnote{Later we discuss how we can avoid computing the full matrix for efficiency.}} for the image. 
Finally, we weight the scores according to the corresponding $\vw_t$ and take the mean, resulting in an estimate of $\cL_{\mathsf{Diffusion}}$ for each class.

\paragraph{Choice of Weighting Function:}
%\label{subsec:weight-func}

%The choice of weighting function, $\vw_t$, in \Cref{eq:score} is critical to the overall performance of the classification algorithm. One option is learning an effective weighting function $\vw_t$. We do this by binning the times into 20 buckets and training a 20-features logistic regression model that learns weights for those buckets that maximize classification accuracy. However, using such a learned weighting is not truly zero-shot since it requires label information to learn. 
%We thus also handcrafted a weighting function that can be used across datasets. We designed $\vw_t$ by finding a simple function that looked close to our learned weighting function on CIFAR-100. In particular, we found that $\vw_t := \mathsf{exp}(-7t)$ works well across many datasets and used it for our experiments. 
%As it is monotonic, $\cL_{\mathsf{Diffusion}}$ with this weighting can still be viewed as a likelihood-based objective that maximizes-the variational lower bound under simple data augmentations \citep{kingma2023understanding}. 

Imagen and SD are trained with the ``simple" loss, where $\vw_t = \text{SNR}(t)$, the signal-to-noise ratio \citep{kingma2021variational} for timestep $t$. However, we found other weighting functions can improve results. First, we experimented with learning $\vw_t$ by binning the times into 20 buckets and training a 20-features logistic regression model to learn weights for the buckets that maximize classification accuracy. However, using that weighting is not truly zero-shot since it requires label information to learn. 
We thus, also handcrafted a weighting function that can be used across datasets. We designed $\vw_t$ by finding a simple function that looked close to our learned weighting function on CIFAR-100 (we did not look at other datasets to preserve zero-shot protocol). Interestingly, we found that the simple function $\vw_t := \mathsf{exp}(-7t)$ works well for both Imagen an SD across tasks and used it for our experiments. 
As it is monotonic, $\cL_{\mathsf{Diffusion}}$ with this weighting can still be viewed as a likelihood-based objective that maximizes-the variational lower bound under simple data augmentations \citep{kingma2023understanding}. We provide details on learning $\vw_t$ and an empirical comparison of different weighting functions in \Cref{app:weighting}. 

\subsection{Improving Efficiency}
\label{subsec:efficiency}
Computing $\tilde{y}$ with naive Monte-Carlo estimation can be expensive because $\cL_{\mathsf{Diffusion}}$ has fairly high variance. Here, we propose techniques that reduce the compute cost of estimating the $\argmin$ over classes. The key idea is to leverage the fact that we only need to compute the $\argmin$ and do not require good estimates of the actual expectations. 




\paragraph{Shared Noise:} Differences between individual Monte-Carlo samples from $\cL_{\mathsf{Diffusion}}$ can of course be due to different $t$ or forward diffusion samples from $q(\bx_t|\bx_{t - 1})$, whereas we are only interested in the effect of the text conditioning $\prompt(\class_k)$.
We find far fewer samples are necessary when we use the {\it same} $t$ and ${\bx}_{t}$ across different classes, as shown in Figure~\ref{fig:imagen_zs}.
After sampling a $t \sim \cU([0,1])$ and $\bx_t \sim q(\bx_t | \bx_0)$, we score all classes against this noised image instead of a single one.
As a result, the differences between these estimates are only due to the different text conditioning signals.

% \begin{figure*}
% \begin{center}
% \begin{minipage}[l]{0.5\textwidth}




\paragraph{Candidate Class Pruning:}
Rather than using the same amount of compute to estimate the expectation for each class, we can further improve efficiency by discarding implausible classes early and dynamically allocating more compute to plausible ones. 
In particular, we maintain a set of candidate classes for the image being classified. After collecting a new set of scores for each candidate class, we discard classes that are unlikely to become the lowest-scoring (\ie predicted) class with more samples.
Since we are collecting paired samples (with the same $t$ and $\hat{\bx}_{i, t}$), we use a paired student's t-test to identify classes that can be pruned. 
This pruning can be viewed as a succesive elimination algorithm for best-arm identification in a multi-armed bandit setting \citep{paulson1964sequential,EvenDar2002PACBF}.
Of course, scores do not exactly follow the standard assumptions of a student's t-test, so we use a small p-value ($2\mathsf{e}^{-3}$ in our experiments) and ensure each class is scored a minimum number of times (20 in our experiments) to minimize the chance of pruning the correct class. 
The full procedure is shown in Algorithm~\ref{alg:efficient}.

\begin{wrapfigure}[44]{r}{0.48\textwidth}
\begin{minipage}[l]{0.48\textwidth}
\vspace{-2mm}
\begin{algorithm}[H]
\caption{Diffusion model classification with pruning.}
\label{alg:efficient}
\begin{algorithmic}
%\STATE \textbf{Given} Image to classify $\bx$
%\STATE \texttt{scores} = $[\class_K]$
\STATE \textbf{given}: Example to classify $\bx$, diffusion model w/ params $\theta$, weighting function $\vw$, hyperparameters $\mathsf{min\_scores}$, $\mathsf{max\_scores}$, $\mathsf{cutoff\_pval}$.
\vspace{1mm}
\STATE \color{ForestGreen}{//Map from classes to diffusion model scores.}\color{black}
\STATE $\mathsf{scores} = \{\class_i: [] \text{ for } \class_i \in [\class_K]\}$
\STATE $n = 0$
\STATE \textbf{while} $|\mathsf{scores}| > 1$ \textbf{and} $n < \mathsf{max\_scores}$:
\STATE \hspace{3mm} $n = n+1$
\STATE \hspace{3mm}\color{ForestGreen}{//Noise the image}\color{black}
\STATE \hspace{3mm} $t \sim \cU([0,1])$
\STATE \hspace{3mm} $\bx_t \sim q(\bx_t | \bx)$
\STATE \hspace{3mm}\color{ForestGreen}{//Score against the remaining classes.}\color{black}
\STATE \hspace{3mm} \textbf{for} $\class_i \in \mathsf{scores}$:
\STATE \hspace{6mm} add $\vw_t \|\bx - \tilde{\bx}_{\vtheta}\big(\bx_{t}, \prompt(\class_i), t \big)\|_2^2$
\STATE \hspace{6mm} to $\mathsf{scores}[\class_i]$
%\STATE \hspace{8mm} $\mathsf{scores}[y_i]$
\STATE \hspace{3mm}\color{ForestGreen}{//Prune away implausible classes.}\color{black}
\STATE \hspace{3mm} $\tilde{y} = \argmin_{\class_i} \mathsf{scores}[\class_i].\mathsf{mean}()$ 
\STATE \hspace{3mm} \textbf{if} $n \geq \mathsf{min\_scores}$:
\STATE \hspace{6mm} \textbf{for} $\class_i \in \mathsf{scores}$:
\STATE \hspace{9mm} \textbf{if} $\mathsf{paired\_ttest\_pval}$(
%\texttt{scores}[$\hat{y}$],
\STATE \hspace{10mm} $\mathsf{scores}[\tilde{y}]$, $\mathsf{scores}[\class_i]) < \mathsf{cutoff\_pval}$:
\STATE \hspace{12mm} remove $\class_i$ from $\mathsf{scores}$.
\STATE \textbf{return} $\tilde{y}$
%\STATE \KC{use symbols/more compact variable names}
\end{algorithmic}
\end{algorithm}
\end{minipage}

\vspace{10mm}

\begin{minipage}[l]{0.49\textwidth}
%\begin{figure}[tb!]
\begin{center}
\includegraphics[width=1.0\textwidth]{figs/efficiency.pdf}
\end{center}
\caption{
Comparison of efficiency improvements for Imagen on CIFAR-100. Shared noise improves sample efficiency by roughly 100x and pruning by an additional 8-10x. 
}
\label{fig:efficiency}
\end{minipage}

\end{wrapfigure}



% \begin{wrapfigure}{r}{0.49\textwidth}
% \begin{minipage}[l]{0.49\textwidth}
% %\begin{figure}[tb!]
% \begin{center}
% \includegraphics[width=1.0\textwidth]{figs/efficiency.pdf}
% \end{center}
% \caption{
% Comparison of efficiency improvements on CIFAR-100. Shared noise improves sample efficiency by roughly 100x and pruning by an additional 8-10x. 
% }
% \label{fig:efficiency}
% %\end{figure}
% \end{minipage}
% \end{wrapfigure}

% \end{minipage}
% \begin{minipage}[l]{0.45\textwidth}
% %\begin{figure}[tb!]
% %\begin{center}
% \includegraphics[width=\textwidth]{figs/efficiency.pdf}
% %\end{center}
% %\caption{
% %Comparison of efficiency improvements on CIFAR-100. Shared noise improves sample efficiency by roughly 100x and pruning by an additional 8-10x. 
% %}
% %\label{fig:efficiency}
% %\end{figure}
% \end{minipage}
% \end{center}
% \label{fig:efficiency}
% \caption{
% Comparison of efficiency improvements on CIFAR-100. Shared noise improves sample efficiency by roughly 100x and pruning by an additional 8-10x. 
% }
% \end{figure*}


\paragraph{Comparison:} Figure~\ref{fig:efficiency} compares the number of samples needed to accurately classify CIFAR-100 images for different efficiency strategies. Using shared noise and pruning greatly improves efficiency, requiring up to 1000x less compute than na\"ive scoring. Nevertheless, classifying with a diffusion model still typically takes 10s of scores per class on average, making the diffusion classifier expensive to use for datasets with many classes. 

%\subsection{Concurrent Work} Concurrently with us, \citet{li2023your} use SD as a zero-shot classifier. While their approach is similar to ours, their results are slightly worse due to using no timestep weighting function and a simple hand-tuned class pruning method. They also do not have experiments on attribute binding or texture robustness. \kc{Move to end of conclusion?}

