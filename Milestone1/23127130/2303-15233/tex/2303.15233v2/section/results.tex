\section{Empirical Analysis and Results}
\label{sec:exp}

In this section, we detail our analysis for the diffusion zero-shot classifiers on a variety of tasks. These include classification on various vision datasets to study generalization capabilities on diverse domains, evaluating model robustness to conflicting cues between texture and shape, and studying attribute binding ability through targeted evaluation on synthetic data. 

We mainly compare Imagen and SD with CLIP \citep{radford2021learning}. We chose CLIP because it is a well-studied and widely-used model, as our primary aim is to study diffusion models rather than push state-of-the-art zero-shot accuracies. 
Our experiments reveal strengths and weaknesses of image-text representations learned via generative training vs. CLIP's contrastive training.
%which is widely used as a zero-shot classifier. Our main aim is to study the strengths and weaknesses of image-text representation learning via generative training and contrastive training as used for CLIP.

\paragraph{Model details:} 
Imagen is a cascaded diffusion model \citep{ho2022cascaded} consisting of a $64\times64$ low-resolution model and two super-resolution models. We only use the $64\times64$ model for our experiments because we found the high-resolution models performed poorly as classifiers.
Combining the $64\times64$ model's scores with scores from the higher-resolutions models did not improve results either (see Appendix~\ref{app:superres} for details). 
The issue is that high-resolution models condition strongly on their low-resolution inputs and are therefore less sensitive to the text prompt. Unlike with Figure~\ref{fig:denoised}, high-resolution denoising with different text prompts produces images imperceptibly different to the human eye because they all agree with the same low resolution image.

We use version 1.4 of Stable diffusion for our experiments. It uses a pre-trained text encoder from CLIP to encode the text and a pre-trained variational autoencoder to map images to a latent space. 

CLIP consists of vision and text transformers trained with contrastive learning. We use the largest CLIP model (ViT-L/14@224px). We provide more details on all the models in \Cref{app:models}.

%We provide more details on the model in \Cref{app:models}.
%The $64\times64$ model has 2B parameters and is trained using a batch size of 2048 for 2.5M training steps on a combination of internal datasets, with around 460M image-text pairs, and the publicly available Laion dataset \citep{schuhmann2021laion}, with  $~$400M image-text pairs. 


%\textbf{Stable Diffusion details}. We use Stable diffusion v1.4 for our experiments that has 890M parameters and takes 512x512 resolution images as input. We provide more details in \Cref{app:models}.
%Stable diffusion (we use version 1.4) is a latent text-to-image diffusion model. It uses the pre-trained text encoder from CLIP to encode text and a pre-trained variational autoencoder to map images to a latent space. The model has 890M parameters and takes 512x512-resolution images as input. It was trained on various subsets of Laion-5B, including a portion filtered to only contain aesthetic images, for 1.2M steps using batch size of 2048.

%\textbf{CLIP details: } 
%CLIP encodes image features using a ViT-like transformer \citep{Dosovitskiy2020AnII} and uses a causal language model to get the text features. After encoding the image and text features to a latent space with identical dimensions, it evaluates a similarity score between these features. CLIP is pre-trained using contrastive learning. Here, we compare to the largest CLIP model (with a ViT-L/14@224px as the image encoder). The model is smaller than Imagen (400M parameters), but is trained for longer (12.8B images processed vs 5.B). While Imagen was trained primarily as a generative model, CLIP was primarily engineered to be transferred effectively to downstream tasks.


\paragraph{Experiment details:} For each experiment, we obtain scores using the heuristic timestep weighting function and the efficient scoring method in \Cref{alg:efficient}.
Due to the method's still-substantial compute cost, we use reduced-size datasets (4096 examples) for our experiments. 
We preprocess each dataset by performing a central crop and then resizing the images to $64\times 64$ resolution for Imagen, $512 \times 512$ for SD, and $224 \times 224$ for CLIP. We use $\mathsf{min\_scores} = 20$, $\mathsf{max\_scores} = 2000$, and $\mathsf{cutoff\_pval} = 2\times \mathsf{e}^{-3}$.
These are simply reasonable choices that keep the method efficient to run; changing the values does effect the model behavior in expectation but trades off compute for reduction in variance. 
We use a single prompt for each image. While we could have used an ensemble of prompts (i.e, made the expectation in \Cref{eq:score} also over different prompt templates), we chose not to for the sake of simplicity, as our goal is to better understand models rather than achieve state-of-the-art zero-shot performance. Therefore, our reported results are slightly lower than in the CLIP paper, which uses prompt ensembling. We found in our experiments that diffusion models were quite robust to the choice of prompt. For example, we tried four different prompts from the CLIP templates for CIFAR-100 and found accuracies to all be within 1.5\% of each other.

%Since we use a fixed single prompt template to obtain results for diffusion models, we follow the same setting for CLIP to keep the results comparable. Therefore, our reported results are often lower than in the CLIP paper, which uses prompt ensembling. 
%While this would boost classification accuracy, we generally use only a single prompt for the sake of efficiency, as 


%\vspace{-1em}
\paragraph{Comparing models:}
Imagen, SD and CLIP have different model sizes, input resolutions, and are trained on different datasets for different amounts of time, so the comparison is not direct. While ideally we would train models of the same size on the same data, this would be very expensive and challenging in practice; we instead use these strong existing pre-trained models. Our comparisons are geared towards highlighting the strengths and weaknesses of text-image diffusion models.

%We want to emphasize that our comparisons with CLIP are intended to highlight strengths/weaknesses of Imagen and provide rough ideas of its performance, not to show Imagen is definitively ``better" or ``worse". 
%Indeed, Imagen is more impractical to use.

\begin{figure*}
  \begin{center}
    \includegraphics[width=\textwidth]{figs/imagen-denoise.pdf}
  \end{center}
  \vspace{-3mm}
  \caption{Example predictions from Imagen when denoising the same image with different text prompts. Each set of images shows the original, noised, and denoised images for the two classes. The top two rows use ImageNet images and the bottom row uses Cue-Conflict.}
  \label{fig:denoised}
\end{figure*}

\subsection{Image Classification}

\label{subsec:classification}
\paragraph{Setup:}
We first evaluate the performance at zero-shot classification. 
For this purpose, we consider 13 datasets from \citet{radford2021learning} as reported in \Cref{tab:classification}. 
%We report the best accuracy achieved by Imagen and SD using a \emph{hand-engineered} weighting across noise levels given by $\vw_t := \mathsf{exp}(-7t)$. %, $\vw_t$: (a) \emph{hand-engineered} weights across noise levels, $\vw_t := \mathsf{exp}(-7t)$ and, (b) learned weights. 
We use the prompt templates and class labels used by \citet{radford2021learning}, which renames some classes that confuse models (e.g. ``crane $\rightarrow$ ``crane bird"‚Äù in Imagenet) \citep{radfordblogb2021}. We use the first prompt from the list, except for Imagenet, where we use ``A bad photo of a \textit{label} '' since this is a good prompt for Imagen, SD and CLIP \citep{radfordbloga2021}.

Since we use the low-resolution Imagen model, we obtain results using CLIP under two settings for a more thorough comparison. In the first setting, we resize all the datasets to $64\times64$ which serves as the base low-resolution dataset. Imagen uses this dataset directly. For CLIP, we subsequently upsample the images, resizing them to $224\times224$ resolution. In the second setting, we directly resize all datasets to $224\times224$ resolution to obtain the best results possible using CLIP where it can take advantage of its higher input resolution.


\paragraph{Results:}

Results are shown in \Cref{tab:classification}. The first eight datasets (up through EuroSAT) on the top block of the table are all originally of resolution $64\times64$ or less. On these datasets, Imagen generally outperforms CLIP and Stable Diffusion on classification accuracy under the same evaluation setting \ie, the models are conditioned on the same text prompts, etc. Imagen significantly outperforms CLIP on SVHN and SD on digit recognition datasets like MNIST and SVHN, which requires recognizing text in an image. \citet{saharia2022photorealistic} observe that Imagen is particularly good at generating text, while SD generally performs poorly (see \Cref{fig:numbers} in the appendix). This demonstrates that Imagen's areas of strength in generation carry over to downstream tasks and suggests that classification on OCR datasets could be used as a quantitative metric to study a model's text-generation abilities. 
%so this finding suggests Imagen's areas of strength in generation carry over to downstream tasks. 
%\Cref{fig:numbers} in the appendix provides examples of generated images for the prompts used for MNIST and SVHN. 
%It demonstrates Imagen's superior ability to generate images with numbers/text while SD fails to produce meaningful images.
%This further suggests that classification on OCR datasets can be used as a quantitative metric to study a model's capability to %generate text. 
SD generally performs poorly on the low-resolution datasets, perhaps because it is only trained on high-resolution images.\footnote{while low-resolution images were incorporated in CLIP's training, doing so with SD would run the risk of the model producing blurry images during generation} To better understand how much low-resolution is to blame, we evaluated SD on ImageNet after down-sampling the images to $32\times32$ and $64\times64$ resolution. SD's accuracy drops from $61.9\%$ to 15.5\% and 34.6\% respectively.  The next five datasets use higher-resolution images. For some of these, taking advantage of CLIP's higher input resolution substantially improves results. SD performs comparably to Imagen on all these datasets (although of course it has an advantage in terms of input resolution).
To our knowledge, these results are the first instance of a generative model achieving classification accuracy competitive with strong transformer-based discriminative methods.
Lastly, we note that our method relies on the model being highly sensitive to text prompt, which we observe qualitatively in Figure~\ref{fig:denoised}.

%Notably, to the best of our knowledge, we are
% the first to show that a generative model can achieve ImageNet classification accuracy comparable with highly competitive discriminative methods like ViTs

% Concurrently with us, \citet{li2023your} use SD as a zero-shot classifier. While their approach is similar to ours, their results are slightly worse due to using no timestep weighting function and a simple hand-tuned class pruning method. They also do not have experiments on attribute binding or texture robustness.

%We found that the boost from learned weightings is small, improving accuracies by around 1\% on average. This result shows that our simple heuristic weighting function generalizes well across datasets. We found using no weights (\ie $\vw_t = 1$) hurts performance substantially (e.g., CIFAR100 accuracy drops to 45\%), which is a bit surprising because many diffusion models, including Imagen, are trained with no weights in their VLBs. 

\begin{table}
\small
\vspace{1em}
\centering
\setlength\tabcolsep{3pt}
%\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cccccccc}
\toprule
\textbf{Model}  & CIFAR10 & CIFAR100  & STL10 & MNIST & DTD & Camelyon & SVHN & EuroSAT \\
\midrule
Imagen & \textbf{96.6} & \textbf{84.3} & \textbf{99.6 }& \textbf{79.2} & 37.3 & \textbf{60.3} & \textbf{62.7} & \textbf{60.3} \\
Stable Diffusion & 72.1 & 45.3 & 92.8 & 19.1 & \textbf{44.6} & 51.3 & 13.4 & 12.4\\
CLIP/ViT-L/14 & 94.7 & 68.6 & 99.6 & 74.3 & 36.0 & 58.0 & 21.5 & 58.0 \\
\midrule

\textbf{Model} & \multicolumn{2}{c}{Stanford Cars} & \multicolumn{2}{c}{Imagenet} & \multicolumn{2}{c}{Caltech 101} & Oxford Pets & Food101\\
\midrule
Imagen & \multicolumn{2}{c}{\textbf{81.0}} & \multicolumn{2}{c}{62.7} & \multicolumn{2}{c}{68.9} & 66.5 & 68.4 \\
Stable Diffusion & \multicolumn{2}{c}{77.8} & \multicolumn{2}{c}{61.9} & \multicolumn{2}{c}{73.0} & 72.5 & 71.6 \\
CLIP/ViT-L/14 & \multicolumn{2}{c}{62.8/75.8} & \multicolumn{2}{c}{63.4/\textbf{75.1}} & \multicolumn{2}{c}{70.2/\textbf{84.1}} & 76.0/\textbf{89.9} & 83.9/\textbf{93.3} \\
\bottomrule
\end{tabular}
%}
\vspace{1mm}
\caption{\textbf{Percent accuracies for zero-shot image classification}. For CLIP where two numbers are reported, the accuracy correspond to two settings: downsizing the images to 64x64 and then resizing the images up to 224x224 (so CLIP does not have an advantage in input resolution over the 64x64 Imagen model) and resizing the images directly to 224x224 (so CLIP has the advantage of higher resolution). Variances in accuracy are $<$1\% across different random seeds. The top set of results are on low-resolution datasets (which is why SD performs poorly).
}
\label{tab:classification}
\end{table}




%\begin{table}
%\small
%\vspace{1em}
%\centering
%\resizebox{\columnwidth}{!}{
%\begin{tabular}{c|c|c|c}
%\toprule
%\textbf{Dataset}  & \textbf{Imagen} & \textbf{SD}  &  \textbf{CLIP} \\
%\midrule
%CIFAR10 & \textbf{96.6} & 72.1 & 94.7 \\
%CIFAR100 & \textbf{84.3} & 45.3 &  68.6 \\
%STL10 & \textbf{99.6} & 92.8 & 99.6  \\
%MNIST & \textbf{79.2} & 19.1 & 74.3 \\ 
%DTD & 37.3 & \textbf{44.6} & 36.0  \\
%Patch Camelyon & \textbf{60.3}  & 51.3 & 58.0  \\
%SVHN & \textbf{62.7}  & 13.4 & 21.5 \\
%EuroSAT & \textbf{60.3} & 12.4 & 58.0  \\
%\midrule
%Stanford Cars & \textbf{81.0}  & 77.8 & 62.8 / 75.8 \\
%Imagenet & 62.7  & 61.9 &  63.4 / 75.1 \\
%Caltech101 & 68.9 & 73.0 & 70.2 / 84.1 \\
%Oxford Pets & 66.5  & 72.5 & 76.0 / 89.9 \\
%Food 101 & 68.4  & 71.6 & 83.9 / 93.3 \\
%\bottomrule
%\end{tabular}
%}
%\caption{Percent accuracies for zero-shot image classification. For CLIP where two numbers are reported, the accuracy correspond to two settings: downsizing the images to 64x64 and then resizing the images up to 224x224 (so CLIP does not have an advantage in input resolution over the 64x64 Imagen model) and resizing the images directly to 224x224 (so CLIP has the advantage of higher resolution). The top 8 datasets have a resolution of 64x64 or less. Variances in accuracy are $<$1\% across different random seeds. 
%}
%\label{tab:classification}
%\end{table}


\begin{table}[h]
\vspace{1mm}
\centering
%\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c|c|c|c}
\toprule
\textbf{Imagen} & \textbf{Stable Diffusion} &  \textbf{CLIP} & \textbf{ViT-22B}  & \textbf{ResNet50 }(supervised)\\
\midrule
\textbf{84.4} & 72.5 & 51.6 & 68.7 & 79 (top-5)\\
\bottomrule
\end{tabular}
%}
\vspace{1mm}
\caption{Percent shape accuracy for zero-shot classification on the Cue-Conflict Imagenet dataset.}
\label{tab:robust}
\end{table}





\subsection{Robustness to Shape-Texture Conflicting Cues}
We next study diffusion models' robustness to presence of texture cues in images by evaluating their performance on the Cue-Conflict dataset from \citet{geirhos2018imagenet}. 
The dataset consists of Imagenet images altered to have a shape-texture conflict.
While (for example) changing an image of a cat to have the texture of an elephant skin doesn't confuse humans, it could cause a model to classify the image as an elephant. \citet{geirhos2018imagenet} showed that CNNs trained on Imagenet were strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence. 

We test Imagen's and Stable Diffusion's robustness to detecting shapes in presence of texture cues by using the same setting for classification as in \Cref{subsec:classification}. We report shape accuracy which is the percentage of images for which the model predicts the image's shape correctly in \Cref{tab:robust}. We compare Imagen and SD with CLIP, the recently proposed ViT-22B model \citep{dehghani2023scaling} which was trained on JFT \citep{sun2017revisiting} extended to 4B images \citep{zhai2022scaling} and fine-tuned on Imagenet, and a (not zero-shot) supervised ResNet50 model trained on the training set. 
Imagen and SD comphrehensively outperform all previous methods on this dataset. Imagen further achieves an accuracy of 84.4\% outperforming SD, CLIP, and ViT-22B by more than 12\%, 30\% and 15\% respectively, and the top-5 accuracy performance of the supervised ResNet50 model by 5\%.

We believe that the denoising process of the diffusion model is critical in removing the texture bias commonly observed in supervised models, making it robust to presence of textural cues. These findings are in line with \citet{nie2022diffusion}, who achieve state-of-the-art adversarial robustness through denoising adversarial examples with a diffusion model. We further qualitatively confirm this in \Cref{fig:denoised} in the appendix which depicts example images from this dataset and Imagen's result after denoising those images conditioned on text prompts with both the correct and incorrect shape class.



\subsection{Evaluating Attribute Binding on Synthetic Data}

\begin{figure*}
\begin{center}
\begin{minipage}[l]{0.24\textwidth}
\includegraphics[width=1.0\textwidth]{figs/CLEVR.png}
\end{minipage}
\hspace{1mm}
\begin{minipage}[l]{0.74\textwidth}
\small
 \textbf{Control tasks} test if the model can identify basic image features by scoring an attribute in the image against one not present. For example: \\ 
 Shape: \textcolor{ForestGreen}{A sphere.} vs. \textcolor{BrickRed}{A cylinder.} Color: \textcolor{ForestGreen}{A gray object.} vs. \textcolor{BrickRed}{A red object.},
 \vspace{1.5mm}\\
 \textbf{Binding tasks} test if the model binds a given attribute to the correct object. For example: 
Color$|$Shape: \textcolor{ForestGreen}{A yellow sphere.} vs. \textcolor{BrickRed}{A gray sphere.} \\ 
 Color$|$Position: \textcolor{ForestGreen}{On the right is a gray object} vs. \textcolor{BrickRed}{On the right is a yellow object.}\vspace{1.5mm}
 \\
 \textbf{Pair binding tasks} are easier binding tasks where information about both objects is provided. For example: \\ Shape,Size: \textcolor{ForestGreen}{A small sphere and a large cube.} vs. \textcolor{BrickRed}{A large sphere and a small cube.} \\
 Color,Size: \textcolor{ForestGreen}{A small yellow object and a large gray object.} vs. \textcolor{BrickRed}{A large yellow object and a small gray object.}
\end{minipage}
\end{center}
\caption{Examples of the synthetic-data attribute binding tasks. We explored more sophisticated prompts than in the figure (e.g., ``A blender rendering of two objects, one of which is a yellow sphere."), but they didn't substantially change results.}
\label{fig:binding-tasks}
\end{figure*}

We have shown that Imagen and SD perform comparably to CLIP at zero-shot classification, and much better than CLIP at disregarding misleading textural cues. Do diffusion models have additional capabilities that are difficult to obtain through contrastive pre-training?
We hypothesize that one such area may be in compositional generalization, and specifically compare the models at attribute binding.
Text-to-image generative models have shown emergent compositional generalization at large enough scale, being able to combine diverse concepts to handle prompts such as ``a chair shaped like an avacado" \citep{ramesh2021zero}.
Attribute binding is a key piece of compositional reasoning, as it enables the understanding and integration of multiple concepts into a coherent whole.  %This process involves binding together distinct attributes with their corresponding elements, which is essential for making sense of complex combinations. 
For example in the statement ``a yellow sphere and a gray cube" we understand the sphere is yellow and the cube is gray, not the other way around.
%We test attribute binding using.
While previous work has examined attribute binding in text-to-image models by examining model generations \citep{Nichol2021GLIDETP,yu2022scaling,Feng2022TrainingFreeSD}, our diffusion model classifier offers a way of more precisely studying the question quantitatively.
We hope in the future, this type of study enabled by our method will be useful for comparing other abilities of generative image models at a fine-grained level.

\paragraph{Dataset Construction:}

We use synthetic images similar to \citet{lewis2022does}, where images are generated based on the CLEVR \citep{johnson2017clevr} visual question answering dataset. CLEVR images contain various object (cubes, cylinders, and spheres) with various attributes (different sizes, colors, and materials). 
A modified version of the CLEVR rendering script is used to generates images containing two objects of different shapes. 
From these images, we construct binary classification tasks of 1000 examples each; see Figure~\ref{fig:binding-tasks} for more details and examples. 
We follow the same setup as in the classification evaluation, using the $64\times64$ Imagen model and $512\times 512$ Stable Diffusion model with heuristic timestep weighting and largest public CLIP model (with full-resolution inputs).


\paragraph{Results:}Scores for Imagen, SD, and CLIP at these tasks is shown in Table~\ref{tab:binding}.
On the control tasks, all the models are able to identify shapes and colors that occur in the image with high accuracy.
Imagen is slightly worse at shape identification; we find most of these are due to it mixing up ``cylinder" and ``cube" when the objects are small.
Mistakes in color recognition generally occur when the distractor color is similar to the true color or to the color of the other object in the image (e.g. the distractor color is blue and there is a large cyan object in the image).

While CLIP can recognize image attributes, it performs no better than random chance for the attribute binding tasks. This result shows it is unable to connect attributes to objects and is consistent with the prior study from \citet{Subramanian2022ReCLIPAS}. 
In contrast, Imagen can perform (at least to some extent) the pair binding tasks, and does better than chance on the Shape$|$Color and Color$|$Shape tasks.
SD cannot perform the positional tasks, but can perform shape/color binding. 

Part of Imagen's advantage might be in its text encoder, the pre-trained T5 \citep{raffel2020exploring} model. \citet{saharia2022photorealistic} find that instead using CLIP's text encoder for Imagen decreased its performance on generations involving specific colors or spatial positions.
Similarly, \citet{ramesh2022hierarchical} find that DALLE-2, which uses a CLIP text encoder, is worse at attribute binding than GLIDE, which uses representations from a jointly-trained transformer processing the text.
An advantage of the diffusion models over CLIP is their use of cross attention to allow interaction between textual and visual features. A visual model without completely independent text and image encoders such as LXMERT \citep{tan2019lxmert} or CMA-Clip \citep{liu2021cma} might perform better, but of course these models come with the added compute cost of having to jointly process all image-text pairs with the model instead of embedding the text and images separately.

One mistake we observed frequently in Color$|$Shape with Imagen is it preferring the color of the larger object in the image; e.g. scoring ``A gray sphere" over ``A yellow sphere" in Figure~\ref{fig:binding-tasks}. We hypothesize that it is helpful for denoising at high noise levels when the text conditioning provides the color for a large region of the image, even when the color is associated with the wrong shape. 
In the pair task, the full color information for both objects is always provided, which avoids this issue, and perhaps explains why accuracies at pair tasks are much higher. 

%Previous work has qualitatively found that large image generation models sometimes struggle with spatial positioning \citep{yu2022scaling}.
%We find this to be mostly true for Imagen, which generally performs worst at the position tasks.%
%Our tasks with position attributes are similar to the analysis in \citet{Subramanian2022ReCLIPAS}, who also found CLIP does not do better than chance. We also found CLIP appears to have systematic position bias; preferring the caption with ``right" in it over ``left" 85\% of the time for Shape$|$Position.

\begin{table}
\centering
\small
\begin{tabular}{l|c|c|c}
\toprule
\textbf{Tasks}  & \textbf{Imagen} &  \textbf{Stable Diffusion} & \textbf{CLIP} \\
\midrule
Shape (control task) & \textbf{85} & \textbf{91} & \textbf{91} \\
Color (control task) & \textbf{96} & \textbf{85} & \textbf{94} \\
\midrule
Shape,Color / Shape$|$Color / Color$|$Shape & \textbf{100} / \textbf{66} / \textbf{73} & \textbf{85} / \textbf{65} / \textbf{59} & 54 / 52 / 53 \\
Shape,Size / Shape$|$Size / Size$|$Shape & \textbf{99} / 48 / 51 & \textbf{63} / 48 / 52 & 52 / 51 / 50 \\
Shape,Position / Shape$|$Position / Position$|$Shape & \textbf{74} / 51 / 52 & 49 / 50 / 50  & 50 / 48 / 51 \\
Color,Size / Color$|$Size / Size$|$Color & \textbf{86} / 54 / 54 & \textbf{59} / 52 / 48 & 48 / 51 / 48 \\
Color,Position / Color$|$Position / Position$|$Color & \textbf{72} / 49 / 49 & 53 / 51 / 49 & 49 / 50 / 49 \\
Size,Position / Size$|$Position / Position$|$Size & \textbf{69} / 50 / 54 & 54 / 49 / 49 & 51 / 50 / 48 \\
% \midrule
% Shape,Color & \textbf{100} & \textbf{85} & 54   \\
% Shape,Size & \textbf{99} & \textbf{60} & 52 \\
% Shape,Position & \textbf{74} & 50 & 50 \\
% Color,Size & \textbf{86} & 49 & 48 \\
% Color,Position & \textbf{72} & 53 & 49 \\
% Size,Position & \textbf{69} & 54 & 51 \\
\bottomrule
\end{tabular}
%\resizebox{\columnwidth}{!}{
% \begin{tabular}{c|c|c|c}
% \toprule
% \textbf{Task}  & \textbf{Imagen} &  \textbf{SD} & \textbf{CLIP} \\
% \midrule
% Shape & \textbf{85} & \textbf{91} & \textbf{91} \\
% Color & \textbf{96} & \textbf{85} & \textbf{94} \\
% \midrule
% Shape$|$Color & \textbf{66}/\textbf{73} & \textbf{65}/\textbf{59} & 52/53 \\
% Shape$|$Size & 48/51 & 48/52 & 51/50 \\
% Shape$|$Position & 51/52 & 50/50  & 48/51 \\
% Color$|$Size & 54/54 & 52/48 & 51/48 \\
% Color$|$Position & 49/49 & 51/49 & 50/49 \\
% Size$|$Position & 50/54 & 49/49 & 50/48 \\
% \midrule
% Shape,Color & \textbf{100} & \textbf{85} & 54   \\
% Shape,Size & \textbf{99} & \textbf{60} & 52 \\
% Shape,Position & \textbf{74} & 50 & 50 \\
% Color,Size & \textbf{86} & 49 & 48 \\
% Color,Position & \textbf{72} & 53 & 49 \\
% Size,Position & \textbf{69} & 54 & 51 \\
% \bottomrule
% \end{tabular}
% \begin{tabular}{c|c|c|c|c|c|c}
% \toprule
%  & \textbf{Shape} & \textbf{Color} \\
% \midrule
% \textbf{Imagen} & \textbf{85} & \textbf{96} \\
% \textbf{SD} & \textbf{91} & \textbf{85} \\
% \textbf{CLIP} & \textbf{91} & \textbf{94} \\
% \midrule
% & \textbf{Shape$|$Color} & \textbf{Shape$|$Size} & \textbf{Shape$|$Position} & \textbf{Color$|$Size} & \textbf{Color$|$Position} & \textbf{Size$|$Position} \\
% \midrule
% \textbf{Imagen} & \textbf{66}/\textbf{73} & 48/51 & 51/52 & 54/54 & 49/49 & 50/54 \\
% \textbf{SD} & \textbf{65}/\textbf{59} & 48/52 & 50/50 & 52/48 & 51/49 & 49/49 \\
% \textbf{CLIP} & 52/53 & 51/50 & 48/51 & 51/48 & 50/49 & 50/48 \\
% \midrule
% & \textbf{Shape,Color} & \textbf{Shape,Size} & \textbf{Shape,Position} & \textbf{Color,Size} & \textbf{Color,Position} & \textbf{Size,Position} \\
% \midrule
% \textbf{Imagen} & \textbf{100} & \textbf{99} & \textbf{74} & \textbf{86} & \textbf{72} & \textbf{69} \\
% \textbf{SD} & \textbf{85} & \textbf{60} & 50 & 49 & 53 & 54 \\
% \textbf{CLIP} & 54 & 52 & 50 & 48 & 49 & 51 \\
% \bottomrule
% \end{tabular}
%}
\vspace{1mm}
\caption{Percent accuracy for models on zero-shot synthetic-data tasks investigating attribute binding. Bold results are significant ($p<0.01$) according to a two-sided binomial test. %For non-pair binding tasks, we show both directions (e.g. Shape$|$Color and Color$|$Shape before/after the slash. 
CLIP is unable to bind attributes, while Imagen and SD sometimes can. 
}
\label{tab:binding}
\end{table}
