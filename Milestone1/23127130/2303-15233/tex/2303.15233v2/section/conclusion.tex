\section{Conclusion and Future Work}

We have proposed a method that enables diffusion models to be used as zero-shot classifiers and developed ways of improving its efficiency to make it usable. Our experiments demonstrate strong results on image classification. 
%We further show our Imagen-based classifier is robust and can produce calibrated scores. 
Furthermore, we show Imagen and Stable Diffusion are remarkably robust to misleading textures, achieving state-of-the-art results on cue-conflict dataset.
While existing analysis of diffusion models usually studies generated images qualitatively, our framework provides a way of quantitatively evaluating text-to-image generative models through evaluating them on controlled classification tasks. We showcase this through our study on attribute binding, where we find that diffusion models are sometimes able to bind attributes while CLIP does not appear to have this ability. Similar experiments could be used in the future to study other properties of pre-trained diffusion models, such as toxicity or bias. 

Our paper is complementary to concurrent work from \citet{li2023your}, who use Stable Diffusion as a zero-shot classifier and explore some different tasks like relational reasoning. 
While their approach is similar to ours, they perform different analysis, and their results are slightly worse than ours due to them using a simple hand-tuned class pruning method and no timestep weighting. 
%They also do not have experiments on attribute binding or texture robustness, but do have further experiments relational reasoning.

We hope our findings will inspire future work in using text-to-image diffusion models as foundation models for tasks other than generation. 
One direction is fine-tuning diffusion models on downstream tasks; given the strong zero-shot performance of Imagen and Stable Diffusion, a natural next step is evaluating them after further supervised training.
%Indeed, \citet{brempong2022denoising} already explore a related idea, finding that denoising pre-training can improve models on semantic segmentation. 
%We note that our main comparison in this work against CLIP is not direct in that the model architectures, parameter counts, and training data are different.
As models become larger, another key question for further study is how do the scaling laws \citep{hestness2017deep,kaplan2020scaling} of contrastive vs generative pre-training compare.
Additionally, we are interested in applying our analysis to other generative models to study to what extent our results are a consequence of generative pre-training generally compared to diffusion pre-training specifically.  

Ultimately, our method does not produce a practical classifier, as it requires substantial compute when scoring many classes.
Instead, we see the main value of this work is in revealing more about the abilities of large pre-trained diffusion models and providing a method for enabling future fine-grained studies of diffusion model abilities. In total, our results suggest that generative pre-training may be a useful alternative to contrastive pre-training for text-image self-supervised learning.

\label{sec:con}

\section*{Acknowledgements}
We thank Kevin Swersky, Mohammad Norouzi, and David Fleet for helpful discussions and feedback, Martha Lewis and Ellie Pavlick for answering our questions about their CLIP attribute binding experiments, and Robert Geirhos for answering our questions about Stylized Imagenet and ViT-22B. We also thank the anonymous reviewers for their comments and suggestions. 

% Notably, to the best of our knowledge, we are
% the first to show that a generative model can achieve ImageNet classification accuracy comparable with highly competitive discriminative methods like ViTs