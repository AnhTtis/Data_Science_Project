\section{Empirical Analysis and Results}
\label{sec:exp}

In this section, we detail our analysis for the zero-shot classifier based on Imagen for a variety of tasks. These include classification on various vision datasets to study generalization capabilities on diverse domains, evaluating of the model's robustness to conflicting cues between texture and shape, and studying attribute binding ability through targeted evaluation on synthetic data. 

We compare Imagen with CLIP \citep{radford2021learning}, which is widely used as a zero-shot classifier. Our main aim is to study the strengths and weaknesses of image-text representation learning via generative training as in Imagen and contrastive training as used for CLIP.

\textbf{Imagen details:} 
We use the 2B parameter Imagen model for $64\times64$ resolution text-to-image synthesis. 
It is trained using a batch size of 2048 for 2.5M training steps on a combination of internal datasets, with around 460M
image-text pairs, and the publicly available Laion dataset \citep{schuhmann2021laion}, with  $~$400M image-text pairs. 
%For simplicity, we only consider the low-resolution 64x64 model, although exploring the high-resolution ones would be interesting in the future. 
%See \Cref{sec:bkg} for more details on Imagen.
We only use the 64x64 model in our experiments because we found the high-resolution models to work less effectively as classifiers. 
The high-resolution models condition strongly on their low-resolution inputs and are therefore less sensitive to the text prompt.

\textbf{CLIP details: } CLIP encodes image features using a ViT-like transformer \citep{Dosovitskiy2020AnII} and uses a causal language model to get the text features. After encoding the image and text features to a latent space with identical dimensions, it evaluates a similarity score between these features. CLIP is pre-trained using contrastive learning. Here, we compare to the largest CLIP model (with a ViT-L/14@224px as the image encoder). The model is smaller than Imagen (400M parameters), but is trained for longer (12.8B images processed vs 5.B). While Imagen was trained primarily as a generative model, CLIP was primarily engineered to be transferred effectively to downstream tasks.


\textbf{Experiment details:} For each experiment, we obtain scores using the efficient scoring method in \Cref{alg:efficient}.
Nevertheless, due to the still-substantial compute cost, we use reduced-size datasets (4096 examples) for our experiments. 
We preprocess each dataset by normalizing the images, performing a central crop and then resizing the images to $64\times 64$ resolution. 
We use $\mathsf{min\_scores} = 20$, $\mathsf{max\_scores} = 2000$, and $\mathsf{cutoff\_pval} = 2\times \mathsf{e}^{-3}$.
Since we use a fixed single prompt template to obtain results for Imagen, we follow the same setting for CLIP to keep the results comparable. Therefore, our reported results are often lower than in the CLIP paper, which uses prompt ensembling. 

\paragraph{Comparing models:}
Imagen and CLIP have different model sizes and are trained on different datasets for different amounts of time, so the comparison is not direct. While ideally we would train models of the same size on the same data, this would be very expensive and challenging in practice; we instead used two strong existing pre-trained models. Our comparisons are geared towards highlighting the strengths and weaknesses of Imagen.

%We want to emphasize that our comparisons with CLIP are intended to highlight strengths/weaknesses of Imagen and provide rough ideas of its performance, not to show Imagen is definitively ``better" or ``worse". 
%Indeed, Imagen is more impractical to use.

\subsection{Image Classification}
\label{subsec:classification}
\paragraph{Setup:}
We first evaluate the performance of Imagen at zero-shot classification. 
For this purpose, we consider 13 datasets from \citet{radford2021learning} as reported in \Cref{tab:classification}. We report the best accuracy achieved by Imagen using two weighting functions, $\vw_t$: (a) \emph{hand-engineered} weights across noise levels, $\vw_t := \mathsf{exp}(-7t)$ and, (b) learned weights. 

We use the prompt templates and class labels used by \citet{radford2021learning}, which renames some classes that confuse models (e.g. ``crane $\rightarrow$ ``crane bird"‚Äù in Imagenet) \citep{radfordblogb2021}. We use the first prompt from the list, except for Imagenet, where we use ``A bad photo of a \textit{label} '' since this is a good prompt for both Imagen and CLIP \citep{radfordbloga2021}. We found in our experiments that our model was quite robust to the choice of prompt. For example, we tried four different prompts from the CLIP templates for CIFAR-100 and found accuracies to all be within 1.5\% of each other.

Since we use the low-resolution Imagen model, we obtain results using CLIP under two settings for a fair comparison. In the first setting, we resize all the datasets to $64\times64$ which serves as the base low-resolution dataset. Imagen uses this dataset directly. For CLIP, we subsequently upsample the images and resize them to $224\times224$ resolution, followed by a central crop and normalization as used in \citet{radford2021learning}. In the second setting, we directly resize all datasets to $224\times224$ resolution, followed by a central crop and normalization to obtain the best results possible using CLIP where it can take advantage of its higher input resolution.

\paragraph{Results:}

Results are shown in \Cref{tab:classification}. The first eight datasets (up through EuroSAT) are all originally of resolution $64\times64$ or less. On all these datasets, Imagen outperforms CLIP on classification accuracy under the same evaluation setting \ie the models are conditioned on the same text prompts, etc. 
Imagen significantly outperforms CLIP on \eg SVHN, which requires recognizing text in an image. \citet{saharia2022photorealistic} observe that Imagen is particularly good at generating text, so this finding suggests Imagen's areas of strength in generation carry over to downstream tasks.

The next five datasets use higher-resolution images. For some of these, taking advantage of CLIP's higher input resolution substantially improves results. It may be possible to get similar benefits from Imagen by incorporating scores from its superresolution models, which we leave for future work to explore. 

We found that the boost from learned weightings is small, improving accuracies by around 1\% on average. This result shows that our simple heuristic weighting function generalizes well across datasets. We found using no weights (\ie $\vw_t = 1$) hurts performance substantially (e.g., CIFAR100 accuracy drops to 45\%), which is a bit surprising because many diffusion models, including Imagen, are trained with no weights in their VLBs. 


\begin{table}
\vspace{1em}
\centering
%\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c|c}
\toprule
\textbf{Dataset}  & \textbf{Imagen}  &  \textbf{CLIP} \\
\midrule
CIFAR10 & \textbf{96.6} & 94.7 \\
CIFAR100 & \textbf{84.3} &  68.6 \\
STL10 & \textbf{99.6} & 99.6  \\
MNIST & \textbf{79.2} & 74.3 \\ 
DTD & \textbf{37.3} & 36.0  \\
Patch Camelyon & \textbf{60.3}  & 58.0  \\
SVHN & \textbf{62.7}  & 21.50 \\
EuroSAT & \textbf{60.3} & 58.04  \\
\midrule
Stanford Cars & \textbf{81.0}  & 62.8 / 75.8 \\
Imagenet & 62.7  & 63.4 / 75.1 \\
Caltech101 & 68.9 & 70.2 / 84.1 \\
Oxford Pets & 66.5  & 76.0 / 89.9 \\
Food 101 & 68.4  & 83.9 / 93.3 \\
\bottomrule
\end{tabular}
%}
\caption{Percent accuracies for zero-shot image classification. For CLIP where two numbers are reported, the accuracy correspond to two settings: downsizing the images to 64x64 and then resizing the images up to 224x224 (so CLIP does not have an advantage in input resolution over the 64x64 Imagen model) and resizing the images directly to 224x224 (so CLIP has the advantage of higher resolution). The top 8 datasets have a resolution of 64x64 or less. Variances in accuracy are $<$1\% across different random seeds. 
}
\label{tab:classification}
\end{table}


\begin{table}[h]
\vspace{1em}
\centering
%\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c|c|c}
\toprule
\textbf{Imagen} &  \textbf{CLIP} & \textbf{ViT-22B}  & \textbf{ResNet50 }(supervised)\\
\midrule
\textbf{84.4} & 51.6 & 68.7 & 79 (top-5)\\
\bottomrule
\end{tabular}
%}
\caption{Percent shape accuracy for zero-shot classification on the Stylized Imagenet dataset.}
\label{tab:robust}
\end{table}



\begin{figure*}
\begin{center}
\begin{minipage}[l]{0.2\textwidth}
\includegraphics[width=1.0\textwidth]{figs/CLEVR.png}
\end{minipage}
\hspace{1mm}
\begin{minipage}[l]{0.78\textwidth}
\small
 \textbf{Attribute recognition tasks} test if the model can identify basic image features by scoring an attribute in the image against one not present. Some example tasks/prompts are shown below: \\ 
 Shape: \textcolor{ForestGreen}{A sphere.} vs. \textcolor{BrickRed}{A cylinder.} Color: \textcolor{ForestGreen}{A gray object.} vs. \textcolor{BrickRed}{A red object.},
 \vspace{1.5mm}\\
 \textbf{Binding tasks} test if the model binds a given attribute to the correct object. For example: \\
Color$|$Shape: \textcolor{ForestGreen}{A yellow sphere.} vs. \textcolor{BrickRed}{A gray sphere.} \\ 
 Color$|$Position: \textcolor{ForestGreen}{On the right is a gray object} vs. \textcolor{BrickRed}{On the right is a yellow object.}\vspace{1.5mm}
 \\
 \textbf{Pair binding tasks} are easier binding tasks where information about both objects is provided. For example: \\ Shape,Size: \textcolor{ForestGreen}{A small sphere and a large cube.} vs. \textcolor{BrickRed}{A large sphere and a small cube.} \\
 Color,Size: \textcolor{ForestGreen}{A small yellow object and a large gray object.} vs. \textcolor{BrickRed}{A large yellow object and a small gray object.}
\end{minipage}
\end{center}
\caption{Examples of the synthetic-data attribute binding tasks. We explored more sophisticated prompts than in the figure (e.g., ``A blender rendering of two objects, one of which is a yellow sphere."), but they didn't substantially change results.}
\label{fig:binding-tasks}
\end{figure*}

\subsection{Robustness to Texture Cues}
We next study Imagen's robustness to presence of texture cues in images by evaluating its performance on the Stylized Imagenet dataset from \citet{geirhos2018imagenet}. 
The dataset consists of Imagenet images altered to have a shape-texture conflict.
While (for example) changing an image of a cat so it has a texture similar to elephant skin doesn't confuse humans, it could cause a model to classify the image as an elephant. \citet{geirhos2018imagenet} showed that CNNs trained on Imagenet were strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence. 

We test Imagen's robustness to detecting shapes in presence of texture cues by using the same setting for classification as in \Cref{subsec:classification}. We report shape accuracy which is the percentage of images for which the model predicts the image's shape correctly in \Cref{tab:robust}. We compare Imagen with CLIP, the recently proposed ViT-22B model \cite{dehghani2023scaling} which was trained on JFT \cite{sun2017revisiting} extended to 4B images \cite{zhai2022scaling} and fine-tuned on Imagenet, and a (not zero-shot) supervised ResNet50 model trained on the training set. 
Imagen outperforms CLIP and ViT-22B model by more than 30\% and 15\% respectively, and the top-5 accuracy performance of the ResNet50 model by 5\%.

We believe that the denoising process of the diffusion model is critical in removing the texture bias commonly observed in supervised convolutional models, making it robust to presence of texture based cues. These findings are in line with \citet{nie2022diffusion}, who achieve state-of-the-art adversarial robustness through denoising adversarial examples with a diffusion model.



\begin{figure}
\begin{center}
\includegraphics[width=0.48\textwidth]{figs/confidences-v2.pdf}
\end{center}
\caption{
Model reliability diagram comparing confidence measures of Imagen on CIFAR-100. The number of model calls used in Algorithm~\ref{alg:efficient} produces better-calibrated confidences than using the actual scores for different classes. 
} 
\label{fig:calibration}
\end{figure}

\subsection{Calibration}
\label{sec:calibration}
It is desirable for classifiers, especially when used in the zero-shot setting with possibly out-of-domain examples, to be well calibrated.
In other words, if a classifier predicts a label $\tilde{y}_i$ with probability $p$, the true label should be $\tilde{y}_i$ roughly $100\cdot p\%$ of the time.
However, the diffusion model classifier does not directly produce probabilities for classes.
While $p(y_i = \class_k | \bx_i)$ should roughly be proportional to the expectation in \Cref{eq:score} when exponentiated (i.e. we can apply a softmax to the average weighted scores to get probabilities), in practice our estimates of the expectations are very noisy and do not provide well-calibrated scores.


We propose a simple alternative that takes advantage of early pruning: we use the total number of diffusion model calls used for the image as a calibration measure. 
The intuition is that a harder example will require more scores to determine the $\argmin$ class with good statistical significance.
We show reliability diagrams \citep{degroot1983comparison} and report Expected Calibration Error \citep{guo2017calibration} (ECE) for the methods in Figure~\ref{fig:calibration}.
Using a small held-out set of examples, we apply temperature scaling \citep{guo2017calibration} for the score-based confidences and Platt scaling \citep{platt1999probabilistic} for the number-of-scores confidences, (see Appendix~\ref{app:calibration} for details).
Number of scores is fairly well-calibrated, showing it is possible to obtain reasonable confidences from diffusion model classifiers despite them not providing a probability distribution over classes.  


\subsection{Evaluating Attribute Binding on Synthetic Data}




We have shown that Imagen performs comparably to CLIP at few-shot classification, and much better than CLIP at disregarding misleading textural cuse. Does Imagen have additional capabilities that are difficult to obtain through contrastive pre-training?
We hypothesize that one such area may be in compositional generalization, and specifically compare Imagen and CLIP at attribute binding.
Text-to-image generative models have shown emergent compositional generalization at large enough scale, being able to combine different concepts to handle prompts such as ``a chair shaped like an avacado" \citep{ramesh2021zero}.
Attribute binding is a key piece of compositional reasoning, as it enables the understanding and integration of multiple concepts into a coherent whole.  %This process involves binding together distinct attributes with their corresponding elements, which is essential for making sense of complex combinations. 
For example in the statement ``a yellow sphere and a gray cube" we understand the sphere is yellow and the cube is gray, not the other way around.


We test attribute binding in Imagen and CLIP on synthetic data.
While previous work has examined attribute binding in text-to-image models by examining model generations \citep{Nichol2021GLIDETP,yu2022scaling,Feng2022TrainingFreeSD}, our Imagen classifier offers a way of more precisely studying the question quantitatively.
We hope in the future, this sort of study will be useful for comparing the abilities of generative image models at a fine-grained level.

\paragraph{Dataset Construction:}

We use the setup of \citet{lewis2022does}, where images are generated based on the CLEVR \citep{johnson2017clevr} visual question answering dataset. CLEVR images contain various object (cubes, cylinders, and spheres) with various attributes (different sizes, colors, and materials). 
A modified version of the CLEVR rendering script is used to generates images containing two objects of different shapes. 
From these images, we construct binary classification tasks of 1000 examples each; see Figure~\ref{fig:binding-tasks} for more details and examples. 
We follow the same setup as in the classification evaluation, using the 64x64 Imagen model with heuristic timestep weighting and largest public CLIP model (with full-resolution inputs).


\paragraph{Results:}
Scores for Imagen and CLIP at these tasks is shown in Table~\ref{tab:binding}.
On attribute recognition tasks Imagen and CLIP are able to identify shapes and colors that occur in the image with fairly high accuracy.
Imagen is slightly worse at shape identification; we find most of these are due to it mixing up ``cylinder" and ``cube" when the objects are small.
Mistakes in color recognition generally occur when the distractor color is similar to the true color or to the color of the other object in the image (e.g. the distractor color is blue and there is a large cyan object in the image).

We find that while CLIP can recognize image attributes, it performs no better than random chance for the attribute binding tasks, showing it is unable to connect attributes to objects on this data. 
In contrast, Imagen can perform (at least to some extent) the pair binding tasks, and does better than chance on the Shape$|$Color and Color$|$Shape tasks.
Part of Imagen's advantage might be in its text encoder, the pre-trained T5 \citep{raffel2020exploring} model. \citet{saharia2022photorealistic} find that instead using CLIP's text encoder for Imagen decreased its performance on generations involving specific colors or spatial positions.
Similarly, \citet{ramesh2022hierarchical} find that DALLE-2, which uses a CLIP text encoder, is worse at attribute binding than GLIDE, which uses representations from a jointly-trained transformer processing the text.
However, a perhaps more significant advantage of Imagen over CLIP is its use of cross attention to allow interaction between textual and visual features.
A visual model without completely independent text and image encoders such as LXMERT \citep{tan2019lxmert} or CMA-Clip \citep{liu2021cma} might perform better, but of course these models come with the added compute cost of having to process all image-text pairs with the model instead of embedding text and images separately.

One mistake we observed frequently in Color$|$Shape is Imagen preferring the color of the larger object in the image; e.g. scoring ``A gray sphere" over ``A yellow sphere" in Figure~\ref{fig:binding-tasks}. We hypothesize that it is helpful for denoising at high noise levels when the text conditioning provides the color for a large region of the image, even when the color is associated with the wrong shape. 
In the pair task, the full color information for both objects is always provided, which avoids this issue, and perhaps explains why accuracies at pair tasks are much higher. 

Previous work has qualitatively found that large image generation models sometimes struggle with spatial positioning \citep{yu2022scaling}.
We find this to be mostly true for Imagen, which generally performs worst at the position tasks.
Our tasks with position attributes are similar to the analysis in \citet{Subramanian2022ReCLIPAS}, who also found CLIP does not do better than chance. We also found CLIP appears to have systematic position bias; preferring the caption with ``right" in it over ``left" 85\% of the time for Shape$|$Position.

\begin{table}
\centering
\small
%\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c|c}
\toprule
\textbf{Task}  & \textbf{Imagen} &  \textbf{CLIP} \\
\midrule
Shape & \textbf{85} & \textbf{91} \\
Color & \textbf{96} & \textbf{94} \\
\midrule
Shape$|$Color & \textbf{66}/\textbf{73} & 52/53 \\
Shape$|$Size & 48/51 & 51/50 \\
Shape$|$Position & 51/52 & 48/51  \\
Color$|$Size & 54/54 & 51/48  \\
Color$|$Position & 49/49 & 50/49  \\
Size$|$Position & 50/54 & 50/48 \\
\midrule
Shape,Color & \textbf{100} & 54   \\
Shape,Size & \textbf{99} & 52 \\
Shape,Position & \textbf{74} & 50 \\
Color,Size & \textbf{86} & 48 \\
Color,Position & \textbf{72} & 49 \\
Size,Position & \textbf{69} & 51 \\
\bottomrule
\end{tabular}
%}
\caption{Percent accuracy for models on zero-shot synthetic-data tasks investigating attribute binding. Bold results are significant ($p<0.01$) according to a two-sided binomial test. For non-pair binding tasks, we show both directions (e.g. Shape$|$Color and Color$|$Shape before/after the slash. CLIP is unable to bind attributes, while Imagen sometimes can. 
}
\label{tab:binding}
\end{table}
