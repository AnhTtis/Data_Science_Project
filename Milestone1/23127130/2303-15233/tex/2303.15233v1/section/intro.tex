\section{Introduction}
\label{sec:intro}
Large models pre-trained on internet-scale data can adapt effectively to a variety of downstream tasks. 
Increasingly, they are being used as zero-shot learners with no task-specific training, such as with CLIP \citep{radford2021learning} for images and GPT-3 \citep{brown2020language} for text.
In natural language processing, many successful pre-trained models are generative (i.e., language models). 
%such as GPT \cite{radford2018improving} and PaLM \cite{chowdhery2022palm} .
However, generative pre-training is less commonly used for visual tasks.
Until recently, the usual practice for vision problems was to pre-train models on labeled datasets such as Imagenet \cite{deng2009imagenet}, or JFT \cite{sun2017revisiting}. 
Later research in visual and vision-language problems has led to image-text models pre-trained primarily using either contrastive losses \cite{radford2021learning, jia2021scaling, yuan2021florence}
or autoencoding tasks \cite{vincent2010stacked, he2022masked}.


On the other hand, generative text-to-image models based on denoising diffusion probabilistic models \citep{ho2020denoising} such as Imagen \cite{saharia2022palette}, Dalle-2 \cite{ramesh2022hierarchical}, and Stable Diffusion \cite{rombach2022high} 
can generate realistic high-resolution images and generalize to diverse text prompts. 
Their strong performance suggests that they learn effective representations of image-text data. However, their ability to transfer to downstream discriminative tasks and how they compare to other pre-trained models has not been explored thoroughly.  


In this paper, we investigate these questions by transferring the Imagen diffusion model to discriminative tasks.
Specifically, we propose a method for using text-to-image diffusion models as zero-shot image classifiers.
While \citet{burgert2022peekaboo} explore using Stable Diffusion for zero-shot referring segmentation and \citet{bar2022visual} use inpainting models for few-shot pixel-level tasks like edge detection and style transfer, to our knowledge zero-shot classification with diffusion models has not been studied previously. 

Our method essentially runs Imagen as a generative classifier \citep{ng2001discriminative}, using a re-weighted version of Imagen's variational lower bound to score images since diffusion models do not produce exact likelihoods. 
First, our method makes a text prompt for each class (e.g. ``a photo of a cat."). Then it scores the input image conditioned on each text prompt, measuring how helpful each prompt is for denoising the image averaged over different noise levels. The class corresponding to the prompt with the best score is predicted.
This classification procedure requires denoising with Imagen many times for every class (with different noise levels), so it is computationally expensive. 
To make it usable in practice, we present improvements that increase the method's sample efficiency by up to 1000x, such as pruning obviously-incorrect classes early. 
While still requiring too much compute to be an easily-deployable classifier, our methodology allows us to quantitatively study fine-grained aspects of a diffusion model's learned knowledge through evaluation on classification tasks (as opposed to qualitatively examining model generations).












We compare Imagen against CLIP\footnote{We use ViT-L/14, the largest public CLIP model} \citep{radford2021learning}, a widely used model for zero-shot image-text tasks trained with contrastive learning. A high-level goal of the experiments is to see the strengths and weaknesses of generative and contrastive pre-training for computer vision.
%Additionally, classifier-free guidance \citep{ho2022classifier} with the diffusion itself vs. classifier guidance CLIP and 
First, we demonstrate that Imagen has strong zero-shot classification accuracies (competitive with CLIP) on several diverse vision datasets.
Next, we show Imagen performs remarkably well on Stylized ImageNet \citep{geirhos2018imagenet}, where images have been stylized with textures conflicting with their labels. Imagen achieves $>$50\% error reduction over CLIP and even outperforms the much larger ViT-22B \citep{dehghani2023scaling} model. This finding is particularly interesting because, unlike supervised classifiers, humans are known to be much more reliant on shape than texture when identifying images. 
%Next, we show that Imagen performs robustly and achieves SOTA results ($>$50\% error reduction over CLIP) on images with texture-shape conflicting cues \cite{geirhos2018imagenet} that have shown to confound pre-trained convolutional supervised models.
We also show that it is possible to obtain fairly well-calibrated confidence scores from our classifier, despite it not producing precise probabilities for classes.
Lastly, we study attribute binding in Imagen using the synthetic data from \citet{lewis2022does}, and find that, unlike CLIP, it can successfully bind together attributes in some settings. 

\begin{figure*}[!ht]
  \begin{center}
    \includegraphics[width=\textwidth]{figs/zs_comb_new.png}
  \end{center}
  \caption{\textbf{Zero-Shot Classification using Imagen.} We first calculate scores for each label prompt across multiple time-steps to generate a scores matrix. We then classify an image by aggregating the scores for each class using a weighting function over the time-steps. The image is assigned the class with the minimum aggregate score. In Section~\ref{subsec:efficiency}, we discuss how efficiency can be improved only computing a subset of the full scores matrix.}
  \label{fig:imagen_zs}
\end{figure*}


The main contributions of this paper are:
\vspace{-1mm}
\begin{itemize}
    \item We show text-to-image diffusion models can be used as effective zero-shot classifiers. While using too much compute to be very practical on downstream tasks, the method provides a way of quantitatively studying what the models learn. \vspace{-1mm} 
    \item We develop techniques that hugely lower the compute cost of these zero-shot classifiers, making them usable (although still slow) on datasets with many classes.\vspace{-1mm}
    \item We demonstrate the strong generalization capabilities of Imagen, resulting in good zero-shot performance on vision datasets (comparable to CLIP). We further show these classifiers can produce calibrated scores.
    \item We show Imagen is robust to misleading textural cues, achieving excellent results on Stylized Imagenet.\vspace{-1mm}
    \item We use our framework to study attribute binding in Imagen and find that it can perform some binding tasks while CLIP generally cannot.\vspace{-1mm}
\end{itemize}
Together, our study of Imagen suggests that text-to-image diffusion models learn powerful representations that can effectively be transferred to tasks beyond image generation.