\section{Conclusion}

We have proposed a method that enables diffusion models to be used as zero-shot classifiers and developed ways of greatly improving its efficiency to make it usable. Our experiments with Imagen demonstrate strong results on image classification. 
%We further show our Imagen-based classifier is robust and can produce calibrated scores. 
Furthermore, we show Imagen is remarkably robust to misleading textures, achieving state-of-the-art results on Stylized Imagenet.
While existing analysis of diffusion models usually studies generated images qualitatively, our framework provides a way of quantitatively evaluating text-to-image generative models through evaluating them on controlled classification tasks. We showcase this through our study on attribute binding, where we find that Imagen is sometimes able to bind attributes while CLIP does not appear to have this ability. 


We hope our findings will inspire future work in using text-to-image diffusion models as foundation models for tasks other than generation. 
One direction is fine-tuning diffusion models on downstream tasks; given the strong zero-shot performance of Imagen, a natural next step is evaluating it after further supervised training.
Indeed, \citet{brempong2022denoising} already explore a related idea, finding that denoising pre-training can improve models on semantic segmentation. 

We note that our main comparison in this work against CLIP is not direct in that the model architectures, parameter counts, and training data are different. As models become larger, a key question is how do the scaling laws \citep{hestness2017deep,kaplan2020scaling} of contrastive vs generative pre-training compare, which we leave as a question for future work. 
We are also interested in applying our analysis to other diffusion models to show that our results are not specific to Imagen. Towards this end, we are currently working on applying our method to Stable Diffusion \citep{rombach2022high}. Additionally, we are also interested to apply our analysis to other generative models and study to what extent our results are a consequence of generative pre-training generally compared to diffusion pre-training.  

Ultimately, our method does not produce a practical classifier, as it requires substantial compute when scoring many classes.
Instead, we see the main value of this work is in revealing more about the abilities of large pre-trained diffusion models. Our results suggest that generative pre-training may be a useful alternative to contrastive pre-training for text-image self-supervised learning.

\label{sec:con}

\section*{Acknowledgements}
We thank Kevin Swersky, Mohammad Norouzi, and David Fleet for helpful discussions and feedback, Martha Lewis and Ellie Pavlick for answering our questions about their CLIP attribute binding experiments, and Robert Geirhos for answering our questions about Stylized Imagenet and ViT-22B. 