\section{Preliminaries}
\label{sec:bkg}

We begin by recalling background knowledge on diffusion models \citep{sohl2015deep, ho2020denoising, song2020score, song2020improved} and recent advances on text-to-image diffusion models. 

\paragraph{Diffusion Models:} Diffusion models are latent variable generative models defined by a forward and reverse Markov chain. Given an unknown data distribution, $q(\bx_0)$, over observations, $\bx_0 \in \RR^d$, the forward process corrupts the data into a sequence of noisy latent variables, $\bx_{1:T} := \{ \bx_1, \bx_2, \cdots, \bx_T\}$,  by gradually adding Gaussian noise with a fixed schedule defined as:
\begin{align}
    \label{eq:fwd}
    q(\bx_{1:T} | \bx_0) := \prod_{t=1}^T q(\bx_t | \bx_{t-1})
\end{align}
 where $q(\bx_t | \bx_{t-1}) := \mathsf{Normal}(\bx_t ; \sqrt{1-\beta_t}\bx_{t-1}, \beta_t \vI)$. 
 

 The reverse Markov process gradually denoises the latent variables to the data distribution with learned Gaussian transitions starting from $\mathsf{Normal}(\bx_T ; 0, \vI)$ \ie
 \begin{align*}
     %\label{eq:rev}
     p_{\vtheta}(\bx_{0:T}) := p(\bx_T)\cdot \prod_{t=0}^{T-1} p_{\vtheta}(\bx_{t-1}| \bx_{t})
 \end{align*}
$p_{\vtheta}(\bx_{t-1}| \bx_{t}) := \mathsf{Normal}\big( \bx_{t-1} ; \boldsymbol{\mu}_{\vtheta}(\bx_{t}, t), \boldsymbol{\Sigma}_{\vtheta}(\bx_t, t)\big)$.
%where $q_{t}$ is the marginal distribution of $\bx_t$ at time $t$. 
The aim of the denoising process is for the forward process distribution $\{\bx_t\}_{t=0}^T$ to match that of the reverse process $\{\tilde{\bx_t}\}_{t=0}^T$ i.e., the generative model $p_{\vtheta}(\vx_0)$ closely matches the data distribution $q(\bx_0)$. Specifically, these models can be trained by optimizing the variational lower bound of the marginal likelihood \citep{kingma2021variational, ho2020denoising}:
\begin{align*}
    % \label{eq:vlb}
    -\log p_\theta(\bx_0) \leq -\mathsf{VLB}(\bx_0) := \cL_{\mathsf{Prior}} + \cL_{\mathsf{Recon}} + \cL_{\mathsf{Diffusion}}
\end{align*}
$\cL_{\mathsf{Prior}}$ and $\cL_{\mathsf{Recon}}$ are the prior and reconstruction loss that can be estimated using standard techniques in the literature \citep{kingma2013auto}.  The diffusion loss, $\cL_{\mathsf{Diffusion}}$, 
%depends on the hyper-parameter $T$ and 
is: % given by:
\begin{align*}
\resizebox{0.49\textwidth}{!}{$
    \cL_{\mathsf{Diffusion}} := \sum\limits_{t=1}^T \bE_{q(\bx_t|\bx_0)} \KL\Big[q(\bx_{t-1} | \bx_{t}, \bx_0) || p_{\vtheta}(\bx_{t-1}|\bx_t) \Big]
$}
\end{align*}
Following \citet{kingma2021variational}, the (re-weighted) diffusion loss can be written in simplified form as:
\begin{align*}
    \cL_{\mathsf{Diffusion}} = \bE_{\bx_0, \boldsymbol{\varepsilon},t}\Big[ \bw_t \| \bx_0 - \tilde{\bx}_{\vtheta}(\bx_t, t)\|^2_2  \Big]
\end{align*}
with $\bx_0 \sim q(\bx_0)$, $\boldsymbol{\varepsilon} \sim \mathsf{Normal}(0, \vI)$, and $t \sim \cU([0, T])$. Here, $\bw_t$ is a weight assigned to the timestep, and $\tilde{\bx}_{\vtheta}(\bx_t, t)$ is the model's prediction of the observation $\bx_0$ from the noised observation $\bx_t$.

Diffusion models can be conditioned on additional inputs like class labels, text prompts, segmentation masks or low-resolution images.
A conditional diffusion model is trained using the following modified diffusion loss from \Cref{eq:diff_loss_disc}:
\begin{align}
    %\label{eq:cond_diff_loss}
    \label{eq:diff_loss_disc}
    \cL_{\mathsf{Diffusion}} = \bE_{(\bx_0, \by), \boldsymbol{\varepsilon},t}\Big[ \bw_t \| \bx_0 - \tilde{\bx}_{\vtheta}(\bx_t, \by, t)\|^2_2  \Big]
\end{align}
where $\by$ is the conditioning input for each data point.
In this paper we use Imagen, a text-conditioned diffusion model that comprises of a frozen T5 \citep{raffel2020exploring} language encoder that encodes an input prompt into a sequence of embeddings, a $64\times64$ image diffusion model, and two cascaded super-resolution diffusion models that generate $256\times256$ and $1024\times1024$ images.

% Classifier-free guidance \citep{ho2022classifier} is a technique to train a single diffusion model on both conditional and unconditional objectives by randomly dropping the conditioning input $\by$ during training with a certain probability. In this case, samples are generated using:
% \begin{align*}
%     %\label{eq:cfg}
%     \tilde{\bx}_{\vtheta}'(\bx_t, \by, t) := (1 + \lambda)\cdot\tilde{\bx}_{\vtheta}(\bx_t, \by, t) - \lambda \cdot \tilde{\bx}_{\vtheta}'(\bx_t, \emptyset, t)
% \end{align*}
% where $\lambda$ is the guidance weight, $\tilde{\bx}_{\vtheta}(\bx_t, \by, t)$ is the conditional model, and $\tilde{\bx}_{\vtheta}'(\bx_t, \emptyset, t)$ is the unconditional model. Classifier-free guidance has been shown to be critical in generating high fidelity samples given a prompt \citep{saharia2022photorealistic, ramesh2022hierarchical, ho2022video, ho2022imagen}.

%\paragraph{Imagen:} %Large scale text-to-image diffusion models like, Imagen \citep{saharia2022photorealistic}, and Stable Diffusion \citep{rombach2022high} have demonstrated impressive capabilities for image generation given text prompts.  
