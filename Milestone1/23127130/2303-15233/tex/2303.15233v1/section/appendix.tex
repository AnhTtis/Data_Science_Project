

\section{Learned Weighting Functions.}
\label{app:weighting}
While for most experiments we use a heuristic weighting function for $\vw_t$, we also explored learning an effective weighting function (although this is not truly zero-shot).
To do this, we aggregate scores for each image $\bx$ and class $\class_k$ into 20 buckets, with each bucket covering a small slice of timestep values:
\begin{align*}
    \bb_i(\bx, \class_k) = \bE_{\epsilon,t \sim \mathcal{U}[0.05i, 0.05(i + 1)]}  \|\bx - \tilde{\bx}_{\vtheta}\big({\bx}_{t}, \prompt(\class_k), t \big)\|_2^2
\end{align*}
where we estimate the expectation with Monte Carlo sampling (typically around 100 samples). We then learn a 20-feature linear model with parameters $[\bv_0, ..., \bv_{19}]$ over these buckets:
\begin{align*}
p_{\bv}(y = \class_k | \bx) = \frac{\exp(\sum_{i=0}^{19} -\bv_i \bb_i(\bx, \class_k) )}{ \sum_{\class_j \in [\class_K]} \exp( \sum_{i=0}^{19} -\bv_i \bb_i(\bx, \class_j) )}
\end{align*}
trained with standard maximum likelihood over the data. At test-time we use the weighting 
\begin{align*}
\vw_t = \bv_{\lfloor t / 0.05 \rfloor}
\end{align*}
We generally found that (1) learned weighting functions are pretty similar across datasets, and (2) the weighting functions are transferable: the $\bv$s learned on one dataset get good accuracy when evaluated on other ones. On average, learned weights produced around 1\% higher accuracy on zero-shot classification tasks, but we omitted the results from the main paper because using learned weights is not truly zero-shot.


\section{Variances in Classification Accuracies.}
Due to our reduced-size evaluation sets, variances in accuracy on zero-shot classification tasks across different random splits are roughly $\pm0.4\%$ for CLIP and $\pm 0.7\%$ for Imagen. Imagen has higher variance due to the inherent randomness in noising images (while CLIP is deterministic). Overall, we are not interested in small accuracy differences anyway, as the comparison between Imagen and CLIP is non-direct in various ways; instead we are trying go get a broad understanding of Imagenâ€™s abilities.

\section{Calibration Details}

We considered two methods for calibrating Imagen's scores in Section~\ref{sec:calibration}; here we briefly go into more mathematical detail on how they work.

\paragraph{Temperature-scaled raw scores.}
We use $s_{\class_k}(\bx)$ to denote the weighted average squared error for class $\class_k$ on image $\bx$, i.e., the Monte-Carlo estimate for the re-weighted VLB in equation~\ref{eq:score}. We turn these scores into an estimated probability by applying a softmax with temperature:
\begin{align*}
p_\theta(y = \class_k | \bx) = \frac{\exp(-s_{\class_k}(\bx)/\tau)}{ \sum_{\class_j \in [\class_K]} \exp{(-s_{\class_j}(\bx)/\tau)}}
\end{align*}
Note that this approach requires good score estimates for each class, so it is not compatible with the class pruning method presented in Section~\ref{subsec:efficiency}. 

\paragraph{Platt-scaled number of scores.}
Our other confidence method relies on the total number of scores needed to eliminate all other classes as candidates. Let $\tilde{y}(\bx)$ denote the predicted class for example $\bx$ and $n(\bx)$ be the total number of calls to $\tilde{\bx}_\theta$ used to obtain the prediction when running Algorithm~\ref{alg:efficient}. Then we estimate
\begin{align*}
p_\theta(y = \tilde{y}(\bx) | \bx) = \text{sigmoid}(-n(\bx) / \tau + \beta)
\end{align*}

We learn $\tau$ (and $\beta$ for Platt scaling) on a small held-out set of examples. 

\label{app:calibration}



\section{Details on Attribute Binding Tasks and Prompts}
\label{app:binding}

We use the relational dataset from \citet{lewis2022does} for the attribute binding experiments.
Each image consists of two objects of different shapes and colors; for tasks involving size we filter out examples where both objects are the same size. 
%The full set of prompts for the binding tasks are listed below. 
Each image contains two objects with different attributes $\mathsf{shape} \in \{\text{cube}, \text{sphere}, \text{cylinder}\}$, 
$\mathsf{color} \in \{\text{blue}, \text{cyan}, \text{blue}, \text{brown}, \text{gray}, \text{green}, \text{purple}, \text{red}, \text{yellow}\}$,
$\mathsf{size} \in \{\text{small}, \text{large}\}$, and $\mathsf{position} \in \{\text{left}, \text{right}\}$.

Given a task (e.g. Shape$|$Size), we construct a task-specific description for an object as follows:
\begin{align*}
&\text{{\it ``On the \{$\mathsf{position}$\} is a "} if Position tasks else {\it ``A "}} + \\
&\text{``{$\mathsf{size}$} " if Size task else ``"} + \\
&\text{``{$\mathsf{color}$} " if Color task else ``"} + \\
&\text{``{$\mathsf{shape}$.}" if Shape task else {\it ``object."}}
\end{align*}
For recognition and binding tasks, we randomly select one of the two objects in the image to be the positive example and then use its description as the positive prompt. For pair tasks, we join the descriptions for both objects together with ``and" (removing the period from the first description and lowercasing the second one) for the positive prompt.

To construct a negative example for recognition tasks, we replace the positive attribute with a random attribute not in the image. For binding tasks, we replace one of positive description's attributes with the other object's attribute (e.g., for Shape$|$Color, we replace $\mathsf{shape}$). 

For pair tasks, there is a choice in how the two objects are ordered (e.g. ``On the left is a cube and on the right is a sphere" vs ``On the right is a sphere and on the left is a cube". We follow the preference of stating the leftmost position/shape/color/size first in that order. For example, this means we will always start with ``On the left..." rather than ``On the right...". Similarly, the negative example for Color,Size in Figure~\ref{fig:binding-tasks} is ``A large yellow object and small gray object" rather than  ``A small gray object and a large yellow object" because we prefer to first put the leftmost color over the leftmost size. 


We experimented with a variety of other prompts, but found none to work substantially better than these simple ones.
