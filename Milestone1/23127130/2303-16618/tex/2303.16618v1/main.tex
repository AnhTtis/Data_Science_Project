% File tacl2021v1.tex
% Dec. 15, 2021

% The English content of this file was modified from various *ACL instructions
% by Lillian Lee and Kristina Toutanova
%
% LaTeXery is mostly all adapted from acl2018.sty.

\documentclass[11pt,a4paper]{article}
\usepackage[dvipsnames]{xcolor}	
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage{microtype} % makes typesetting nicer
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{cleveref}
\usepackage{booktabs}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{xspace}
\usepackage{listings}
\usepackage{soul} % for \ul and \TODOMARK
\usepackage{caption}
\usepackage{transparent} % for \transparent in the ETH logo
\usepackage{tcolorbox} % for examples in text
\usepackage{mdframed} % for quote boxes
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{pifont}
\usepackage{tipa}
\usepackage{subfig}
\usepackage{graphicx}
\newcommand{\cmark}{\textcolor{OliveGreen}{\ding{51}}}
\newcommand{\xmark}{\textcolor{BrickRed}{\ding{55}}}
\newcommand{\lgc}{\cellcolor{lightgray}}
\newcommand{\dgc}{\cellcolor{darkgray}}
%% Package options:
%% Short version: "hyperref" and "submission" are the defaults.
%% More verbose version:
%% Most compact command to produce a submission version with hyperref enabled
%%    \usepackage[]{tacl2021v1}
%% Most compact command to produce a "camera-ready" version
%%    \usepackage[acceptedWithA]{tacl2021v1}
%% Most compact command to produce a double-spaced copy-editor's version
%%    \usepackage[acceptedWithA,copyedit]{tacl2021v1}
%
%% If you need to disable hyperref in any of the above settings (see Section
%% "LaTeX files") in the TACL instructions), add ",nohyperref" in the square
%% brackets. (The comma is a delimiter in case there are multiple options specified.)

\usepackage[acceptedWithA]{misc/tacl2021v1}
% \setlength\titlebox{10cm} % <- for Option 2 below

%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}
\newcommand{\dateOfLastUpdate}{Dec. 15, 2021}
\newcommand{\styleFileVersion}{misc/tacl2021v1}

\newcommand{\ex}[1]{{\sf #1}}

\newif\iftaclinstructions
\taclinstructionsfalse % AUTHORS: do NOT set this to true
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author info supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}
\fi

%
\iftaclpubformat % this "if" is set by the choice of options
\newcommand{\taclpaper}{final version\xspace}
\newcommand{\taclpapers}{final versions\xspace}
\newcommand{\Taclpaper}{Final version\xspace}
\newcommand{\Taclpapers}{Final versions\xspace}
\newcommand{\TaclPapers}{Final Versions\xspace}
\else
\newcommand{\taclpaper}{submission\xspace}
\newcommand{\taclpapers}{{\taclpaper}s\xspace}
\newcommand{\Taclpaper}{Submission\xspace}
\newcommand{\Taclpapers}{{\Taclpaper}s\xspace}
\newcommand{\TaclPapers}{Submissions\xspace}
\fi


%%%% End TACL-instructions-specific macro block
%%%%

\input{misc/macros.tex}

\title{Personalised Language Modelling of Screen Characters \\ Using Rich Metadata Annotations}

% Author information does not appear in the pdf unless the "acceptedWithA" option is given

\newcommand{\AfA}{\textsuperscript{\begin{minipage}[c]{2.5mm}
\includegraphics[width=\linewidth]{img/club.png}
\end{minipage}}}
\newcommand{\AfB}{\textsuperscript{\begin{minipage}[c]{2.5mm}
\includegraphics[width=\linewidth]{img/diamond.png}
\end{minipage}}}
\newcommand{\bustspeaker}{\begin{minipage}[c]{4.5mm}
\includegraphics[width=\linewidth]{img/bust.png}
\end{minipage}\hspace{1mm}}
\newcommand{\annedspeaker}{\begin{minipage}[c]{4.5mm}
\includegraphics[width=\linewidth]{img/woman-artist.png}
\end{minipage}\hspace{1mm}}

\author{\AfA Sebastian Vincent,
        \AfB \textbf{Rowanne Sumner},
        \AfB \textbf{Alice Dowek},
        \AfB \textbf{Charlotte Blundell}, \\
        \AfB \textbf{Emily Preston},
        \AfB\textbf{Chris Bayliss},
        \AfB \textbf{Chris Oakley}, 
        \AfA \textbf{Carolina Scarton}
        \\[5pt]
        \AfA Department of Computer Science, University of Sheffield, UK\\
        \AfB ZOO Digital Group PLC, UK\\
        \texttt{stvincent1@sheffield.ac.uk, c.scarton@sheffield.ac.uk}\\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for $863$ speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over $95\%$ of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to $8.5\%$. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight which annotations were most cost-effective relative to the reduction in perplexity.
\end{abstract}

% GitHub logo
\begin{minipage}[c]{4.5mm}
\includegraphics[width=\linewidth]{img/github_mark.pdf}
\end{minipage}
\hspace{0mm}
\begin{minipage}[c]{0.7\textwidth}
\fontsize{8pt}{8pt}\selectfont
\href{https://github.com/st-vincent1/cornell_rich}
{\texttt{github.com/st-vincent1/cornell\_rich}}
\end{minipage}

\hspace{-0.8mm}
\begin{minipage}[c]{4.5mm}
\includegraphics[width=\linewidth]{img/github_mark.pdf}
\end{minipage}
\hspace{0mm}
\begin{minipage}[c]{0.7\textwidth}
\fontsize{8pt}{8pt}\selectfont
\href{https://github.com/st-vincent1/lmcue}
{\texttt{github.com/st-vincent1/lmcue}}
\end{minipage}


\iftaclpubformat
\section{Introduction}
Studies of linguistics have long accepted that spoken language is not universal \citep{milburn-2004-speech}. A significant factor in this is the individual's characteristics, such as their age, gender, and background \citep{lynn-etal-2017-human}. Furthermore, the social, professional or cultural context of our conversation can significantly impact the style and tone of our speech. This phenomenon is known as communication accommodation \citep{giles2007communication}, where people adapt their language to match their interlocutors. In this regard, researchers and language generation systems must consider these factors to ensure the accuracy and appropriateness of generated language for its intended audience and context.

Traditionally generative models for tasks like language modeling, machine translation, and summarisation are built in an one-size-fits-all fashion, and most often for a particular language (or pair of languages) and domain, disregarding the context of the processed text. This effectively means assuming the most likely scenario as context, sometimes leading to harmful predictions \citep[e.g. the \enquote{masculine default} in][]{Schiebinger2014}. In contrast, personalisation has clear benefits in generation tasks and should be the focus of new applications \citep{flek-2020-returning, dudy-etal-2021-refocusing}. Including context with the source input provides a necessary background to the given utterance or sentence, which in turn reduces the potential for sample bias in training data and helps reduce input ambiguity.

Demographic factors have been shown to improve the performance in a wide range of natural language processing (NLP) tasks, such as classification \citep{hovy-2015-demographic}, generation \citep{zeng-etal-2019-automatic}, translation \citep{vincent-etal-2022-controlling-extra}. This impact can be divided into grammatical, which is well-defined and of morphosyntactic nature, and behavioural, which is more fluid and pertains to the way language is used by certain demographics \citep{vincent-etal-2022-controlling-extra}. Grammatical agreement is best exemplified with gender, which in many languages determines the morphological endings of self-referent verbs. Another example is register, which is typically determined by the formality of the speaker-interlocutor relationship. In contrast, behavioural agreement is more subtle; to exemplify, the exclamation \enquote{They're done!} has a significantly different meaning when proclaimed by a baker about a batch of cookies than by a frenzied king about his treacherous subjects. Only a context-oriented model could pick up on the differences between the two cases. Different types of context require different agreement methods and a good framework accommodates both.

There is a crucial challenge to personalisation in the domain of scripted dialogue in TV and film, one because of which most methods fall short: the practical application will typically require the models to be personalised for characters or shows \textbf{without prior dialogue}, e.g. for new TV shows, a phenomenon known as the cold start problem \citep[e.g.][]{schein-etal-2002-cold, huang-etal-2014-enriching}. A possible solution to this is \textbf{metadata-based personalisation}: relying on character and film or show profiles to adapt the models. Indeed, this domain has the potential to be metadata-rich, as the data often contains identifiable information like titles and character names, which can be used to obtain more information, whether automatically or through manual effort, making this approach feasible. Such adaptations have been attempted before in different domains, but only at a small scale, leveraging a few simple and mostly categorical variables \citep{huang-etal-2014-enriching, 
lynn-etal-2017-human, 
king-cook-2020-evaluating,
welch-etal-2020-compositional,
guo-etal-2021-aerospace}. An exception is \citet{novotney-etal-2022-cue} who consider adaptation with $11$ continuous metadata variables for articles. To the best of our knowledge, the present study uses the largest set of metadata information for personalisation, leveraging manually collected rich speaker annotations and automatically or manually obtained film metadata, up to $16$ unique variables at once.

More concretely, in this work we address the following research questions:
\researchquestion{RQ1} {Are character profiles helpful in modelling the characters' speaking style? (\S \ref{rq1}) \\
\textbf{RQ2}: Can a language model be personalised for a specific character solely by learning from data for characters with similar profiles? (\S \ref{rq2})\\
\textbf{RQ3}: Relative to the perceived manual effort, which metadata can offer the most benefit for modelling? (\S \ref{rq4})
}

Additionally, we contribute \textsc{Cornell-rich} (\S \ref{cornell_rich}), a corpus of rich character and film annotations for the Cornell Movie Dialogue Corpus \citep{Danescu-Niculescu-Mizil2011}. This paper also presents related work (\S \ref{related-work}), the experimental setup (\S \ref{sec:experimental}), and conclusions (\S \ref{sec:conclusions}).

\section{Related Work}
\label{related-work}
Personalisation in NLP can generally be split into three groups with regards to how much source data is available for a new speaker: full supervision (sufficient training data), few-shot (sample data), and zero-shot (no data); the latter two settings, which pose a challenge for making predictions for new users or speakers, are also known as the cold start problem \citep{huang-etal-2014-enriching}. Full supervision is usually facilitated through some form of a \textit{user embedding} or \textit{tagging} approach \citep[e.g.][]{sennrich-etal-2016-controlling,Keskar2019ctrl,mireshghallah-etal-2022-useridentifier}. Among few-shot approaches, \citet{king-cook-2020-evaluating} examine several personalisation methods (interpolation, fine-tuning, priming, and demographic-based adaptation) for language modelling of blog posts with sample adaptation data for new users. \citet{welch-etal-2022-leveraging} consider a scenario with anchor users (with a large history of posting) and new users (with a small number of posts); focusing on the similarity between samples of users' posts, they combine models built for anchor users to build models for the new users. Zero-shot approaches typically leverage background data available for the new speakers, like demographic factors or metadata. \citet{huang-etal-2014-enriching} rely on the social network of a user to model their language; \citet{lynn-etal-2017-human} use age, gender, and personality traits to improve user modelling in multiple NLP tasks; \citet{welch-etal-2020-compositional} produce compositional demographic word embeddings by learning demographic-specific vectors for each word in the vocabulary. Demographic-based adaptation was found inferior to interpolation and priming in the few-shot scenario by \citeauthor{king-cook-2020-evaluating}. However, their study only used two factors: age and gender. Our work is positioned in the zero-shot category as we rely on rich metadata annotations to model individual film characters; in contrast to \citeauthor{king-cook-2020-evaluating}, we leverage textual (real-valued) metadata annotations, which in personalisation are preferable to categorical values \citep{lynn-etal-2017-human}, and a significantly higher count of them (up to $24$).

Our work extends \textsc{cue} vectors \citep{novotney-etal-2022-cue}, continuous representations of context computed by passing \textsc{DistilBERT} embeddings through a context encoder and averaging the result. \citeauthor{novotney-etal-2022-cue} demonstrate that including metadata in the form of \textsc{cue} improves perplexity in language modelling of articles.

Finally, it is possible that demographic-based personalisation could be achieved through extremely large language models by simply priming the models with a concatenation of all of the rich metadata. We leave implementing and testing this baseline to future work due to limited resources.

\section{Cornell-rich dataset}
\label{cornell_rich}
\textsc{Cornell-rich} is a dataset of annotations for the Cornell Movie Dialogue Corpus, with a focus on unique sets of characteristics of featured speaking characters, and additionally accompanied by automatically collected film metadata. We describe the data collection process and corpus details in this section.
\subsection{Details}
The \textsc{Cornell-rich} dataset enriches the original corpus with annotations for $863$ speakers, covering $135.7$K utterances; \autoref{fig:speaker-lines} shows that nearly half of the annotated speakers have 150 or more lines of dialogue. Additionally, at least $64.1\%$ of conversational exchanges feature at least one annotated character and as much as $95.5\%$ of the featured films are annotated with film metadata (\autoref{tab:cornell-speaker-details}). We provide a full list of collected metadata with examples in \autoref{tab:examples}.

\begin{table}[h]
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}rrccc@{}}
\toprule
Number of & Count & \% & Utterances & \% \\ \midrule
\midrule
\textbf{speakers} & $9.0$K & $-$ & $304.3$K & $-$ \\
\annedspeaker & $863$ & $9.5\%$ & $135.7$K & $44.6\%$ \\ 
\bustspeaker & $8.2$K & $90.5\%$ & $170.1$K & $55.9\%$ \\ \midrule
\textbf{exchanges} & $83.1$K & $-$ & $304.3$K & $-$ \\
\annedspeaker $\Leftrightarrow$ (\bustspeaker or \annedspeaker) & $53.3$K & $64.1\%$ & $202.4$K & $66.5\%$ \\
\bustspeaker $\Leftrightarrow$ \annedspeaker & $36.8$K & $44.3\%$ & $134.4$K & $44.2$\% \\
\annedspeaker $\Leftrightarrow$ \annedspeaker & $16.5$K & $19.8\%$ & $68.0$K & $22.3\%$ \\ \midrule
\midrule
\textbf{films} & $617$ & $-$ & $304.3$K & $-$ \\
annotated & $589$ & $95.5\%$ & $291.0$K & $95.6\%$ \\ \bottomrule
\end{tabular}}
\caption{Details regarding annotations described against the background of the full Cornell corpus. \annedspeaker $=$ speaker with rich annotations. \bustspeaker $=$ speaker without rich annotations.}
\label{tab:cornell-speaker-details}
\end{table}

\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}rp{6cm}@{}}
\toprule
Metadata type & Example \\ \midrule
\textbf{Speaker} & \\
Gender & Female \\
Age bracket & Adult \\
Profession & Convenience Store Clerk \\
Description & Electra is a computer hacker, who has set up CCTV cameras and an elaborate mechanism for remotely moving items between rooms in the building where she is based. \\
Quote & Well, I am an amateur! This is gonna hurt like hell! \\
Country of origin & France \\
Religion & Catholic \\
\textbf{Film metadata} & \\
Genre & Comedy, Drama \\
PG Rating & PG Rating: R \\
Names of writers & Written by: Bryan Fuller \\
Country of release & United Kingdom \\
Year of release & Released in: 2006 \\
Plot description & A disillusioned college graduate finds himself torn between his older lover and her daughter. \\ \bottomrule
\end{tabular}}
\caption{Examples of each type of collected metadata. Values were sampled independently and at random.}
\label{tab:examples}
\end{table}

\begin{figure}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{img/piechart.png}}
    \caption{Annotated speakers in \textsc{Cornell-rich} bucketed by the number of lines spoken.}
    \label{fig:speaker-lines}
\end{figure}

\begin{figure*}[t]
  \centering
 \resizebox{\linewidth}{!}{
  \subfloat[Number of lines per production year of films in the corpus.]
  {
    \includegraphics[width=0.32\textwidth]{img/line_years.png}
   \label{fig:years}
  }
  \hspace{0.3cm}
  \subfloat[15 most popular professions in the corpus.]
  {
    \includegraphics[width=0.32\textwidth]{img/bars_professions.png}
    \label{fig:profs}
  }
  \hspace{0.3cm}
  \subfloat[12 most popular genres in the corpus. Titles labelled as multiple genres are counted multiple times.]
  {
    \includegraphics[width=0.32\textwidth]{img/pie_chart_genres.png}
    \label{fig:genres}
  }}
  \caption{Visualisation of a subset of features of the proposed corpus.}
  \label{fig:movie-details}
\end{figure*}


As per \autoref{fig:movie-details}, the annotated films span nearly a century, with most lines coming from between the $1990$s and $2005$; the distribution of professions is significantly flatter, with the dominant field (\enquote{High School Student}) only making up about $3\%$ of the corpus. Finally, the most popular genres include drama, comedy, crime and action.

\paragraph{Training, validation and testing splits} To facilitate reproducibility of results, we provide the corpus already split into \textbf{training}, \textbf{validation} and \textbf{testing} sets. To also consider a scenario with completely new characters, we first create \texttt{unseen}, a test set with metadata and dialogue segments from $31$ held-out speakers. Then we perform random sampling to create \texttt{test}, \texttt{valid} and \texttt{train} sets. Between these three sets both movie and speaker metadata, but not dialogue segments themselves, will repeat. Data quantities are provided in \autoref{tab:data-quants} (top row).

\subsection{Creation process}
The data collection process was undertaken by two data collectors, both native speakers of English working in the dubbing and subtitling industry. In this section we describe this process in detail. First, the Cornell Movie Dialog Corpus was parsed according to instructions\footnote{\url{https://convokit.cornell.edu/documentation/movie.html}, accessed 30/3/23.}. From this, a spreadsheet of characters was generated which included their name, source film and the number of lines attributed. Based on previous work \citep[e.g.][]{johannsen-etal-2015-cross} and hypotheses made based on experts' experience, we pre-defined a number of categories of information to collect about each character. Specifically, we selected categories that we hypothesised to (i) be identifiable from the available sources and (ii) influence a person's speaking style or vocabulary used. They were: their \textbf{age bracket} $\in \{\text{child, teen, young adult, adult, elderly}\}$, \textbf{profession}, \textbf{character description} (a few sentences summarising their personality or character arc), \textbf{religion} and a \textbf{characteristic quote}: a typical or quotable thing the character might say. Additionally, the \textbf{gender} annotations from the original corpus were re-used, and an optional column \enquote{\textbf{additional information}} was included to collect potentially useful information that does not fit the remaining categories\footnote{Upon inspection: the annotators predominantly used this field to provide the actor's name, an interesting fact about the character (e.g. \enquote{Plays a caricature of himself}), or trivia.}. Further, to optimise the efforts, \textbf{characters with fewer than 50 lines spoken were removed}. The annotators agreed to process all characters with at least $100$ lines and use the remaining project time to annotate as many characters with $50-99$ lines as possible. In the end, all $714$ characters with 100+ lines and $149$ with $50-99$ lines were annotated, to a total of $863$ characters.

While the original corpus contained IMDb ID attributions, the annotators quickly identified that many fields were missing and there often was a data mismatch between the dataset and the IMDb database\footnote{\url{https://www.imdb.com/}, accessed 30/3/23.}. Therefore, each character was manually identified instead by their and the films' names. 

\paragraph{Annotation sources} The annotations were created based on the publicly available pages on Wikipedia\footnote{\url{https://wikipedia.org/}, accessed 30/3/23.} for individual films, as well as fan-made Fandom\footnote{\url{https://www.fandom.com/}, accessed 30/3/23.} pages for both films and characters. Where information was unavailable from these sources, the annotators either referred back to the corpus itself or skipped the given field altogether. The \textit{film} metadata was obtained via the OMDb API.\footnote{\url{https://www.omdbapi.com/}, accessed 30 Mar 2023.}

\subsection{Challenges} Several challenges occurred while annotating the dataset, and these are described in this section.
\paragraph{Mismatch with IMDb} To verify the films' and characters' names, the annotators matched every script's name against an IMDb entry. This proved difficult since some characters or film titles could not be matched correctly: e.g. the scripts had been scrapped or rewritten, the film or character names or their stylisation had changed since the original script. Where possible, the expert annotators matched the films and characters, these fields were flagged and the entries later removed from the dataset. Where the film title yielded multiple IMDb entries, the annotators sifted through the entries and matched the characters' names against the characters present in the corpus for that given title.
\paragraph{Misspellings} Some character names were misspelled in the corpus; additional research across Wikipedia was undertaken to verify spellings.
\paragraph{Difficulty finding information for some characters} Some individual information was difficult to identify for some characters. By far the most difficult was religion, followed by character descriptions. Information that could not be found was skipped or labelled as (\enquote{Unknown} for profession and religion); this occurred in fewer than $5\%$ of cases.
\paragraph{Annotating characters based on real people} Where characters were based on real people, a dramatisation of history or a characterisation of the person played by themselves (e.g. \textit{The Beatles} band members playing themselves in \textit{A Hard Day's Night}), the line between the personal and fictional representation was blurred, making it a challenge to differentiate between considering a character versus a person recorded in history. To address this, where characters were based on historical figures, the annotators focused on the film interpretation of the person. When dealing with a characterisation of the person at a specific point in time, the focus was on their behaviour at that point in time. 

\paragraph{Character information ambiguous or not fitting the categories} Some columns were unsuitable for selected character information: e.g. when a character was immortal, it did not fit into set age brackets. For other characters there were few clues to determine their age bracket. In both cases, the final annotations were based on the annotators' expertise.

\subsection{Preprocessing}
For dialogue, our preprocessing efforts were minimal since the original corpus is already of high quality; nevertheless, we normalised punctuation and removed tokenisation using Moses scripts, fixed some leftover punctuation issues (e.g. ensure all multi-dots use three dots) and removed \textsc{HTML} tags like \texttt{<u>word</u>} used for emphasis.

We also preprocessed all (original and added) annotations to obey the following rules: (i) all empty fields are expressed as an empty string; (ii) there are no multiple expressions of the same discrete type (e.g. \textit{m} and \textit{M} to denote masculine gender); (iii) all attributes are expressed in unambiguous natural language (e.g. a PG rating of \enquote{R} is rewritten as \enquote{PG Rating: R}).

\section{Experimental Setup}
\label{sec:experimental}
We describe the experimental setup used to answer the research questions (\textbf{RQ1-3}).

\subsection{\textsc{LMCue} Architecture} \label{lmcue}
We draw inspiration from the CUE vectors experiments in \citet{novotney-etal-2022-cue} and implement an encoder-decoder architecture, where the encoder uses context vectors as input (and the decoder is causal). This architecture mimics a standard seq2seq Transformer \citep{Vaswani2017}, with a significant alteration: the encoder inputs are pre-computed \textbf{vectors} rather than tokens and the positional encoding of encoder inputs is removed\footnote{In preliminary experiments, we found no benefit to incorporating positional encoding to e.g. distinguish individual context variables.}.

\begin{figure}[ht]
    \centering
    \resizebox{\linewidth}{!}{\includegraphics{img/lmcue.png}}
    \caption{The \textsc{LMCue} architecture. Figure stylised after the Transformer in \citet{Vaswani2017}; only input to the encoder is different from the original architecture.}
    \label{fig:arch}
\end{figure}

\paragraph{Pre-computed context vectors} We follow \citet{novotney-etal-2022-cue} who used pooled outputs of \textsc{DistilBERT} \citep{Sanh2019} to embed all input contexts as equal-sized vectors. This approach has the advantage of treating both discrete and continuous (text) inputs in the same way, potentially utilizing the semantic information of the discrete labels, as well as allowing longer spans of context (e.g. character descriptions) as input without span issues. Unlike \citeauthor{novotney-etal-2022-cue}, we use \textsc{MiniLM-v2}, a sentence embedding model \citep{reimers-gurevych-2019-sentence} based on \textsc{MiniLM} \citep{wang-etal-2020-minilm}, to vectorise the contexts as it is both better performing at sentence embedding tasks and smaller (and produces lower-dimension embeddings). The target sequences are contextualised via standard encoder-decoder attention which maps queries (target) to keys and values (context). We use \textsc{QK-Norm} to compute the attention weights \citep{henry-etal-2020-query}.

\subsection{Pre-training} \label{sec:pretraining}
In our preliminary experiments we found that training \textsc{LMCue} from scratch on \textsc{Cornell-rich} leads to results inferior to a non-contextual language model trained on the same data. We therefore experimented with pre-training the model first. Since a larger corpus of dialogue with character metadata was unavailable, we used a corpus with document-level information, and treated the past dialogue for any sentence to be the context. We hypothesised that at a larger scale, the effect of metadata embeddings on text generation will be similar to the effect of embeddings of past dialogue. This approach proved successful, and all models considered in our experiments are pre-trained on the same corpus of pre-training data described in the next section.

\subsection{Corpora used in experiments}

\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}rccc@{}}
\toprule
\multicolumn{1}{l}{} & \multicolumn{3}{c}{Total number of}    \\
Dataset \& split                & segments & tokens & metadata \\ 
\midrule
\textsc{Cornell-rich} & & & \\
\texttt{train}       & $289.0$K   & $3.1$M       & \multirow{4}{*}{$14$} \\
\texttt{valid}       & $5$K     & $51.2$K       &  \\
\texttt{test} & $5$K     & $54.4$K       &                    \\
\texttt{unseen} & $5.2$K     & $54.6$K       &                    \\ \midrule
\textsc{OpenSubtitles} & & & \\
\texttt{train}       & $14.7$M   & $109.6$M       & \multirow{2}{*}{$3$} \\
\texttt{valid}       & $10$K     & $82.0$K       &  \\ \midrule
% \texttt{test} & $10$K     &        &                    \\ \midrule
\textsc{ZOO} & & & \\
\texttt{train}       & $140.4$K & $1.1$M  & \multirow{4}{*}{$15$} \\
\texttt{valid}       & $4$K     & $31.3$K &  \\
\texttt{test} & $6$K     & $47.1$K &  \\
\texttt{unseen} & $6.7$K   & $51.5$K &  \\ \bottomrule
\end{tabular}}
\caption{Quantities of segments/tokens and speakers per data split in datasets. Unique speakers are shared across \texttt{train}, \texttt{valid} and \texttt{test}. \textsc{OpenSubtitles} uses only film metadata while \textsc{ZOO} uses film, scene and speaker metadata.}
\label{tab:data-quants}
\end{table}

The two extra corpora used throughout the rest of the paper are the OpenSubtitles18\footnote{Created from data from \url{https://opensubtitles.org}.} corpus \citep{lison-etal-2018-opensubtitles2018} (\textsc{OpenSubtitles}) for pre-training models and the ZOO Digital Dataset (\textsc{ZOO}) with a similar structure as \textsc{Cornell-rich}, to strengthen our conclusions. Below we describe the creation and preprocessing steps for each; quantitative details can be found in \autoref{tab:data-quants}.
\paragraph{Pre-training: \textsc{OpenSubtitles}}

As described in \S \ref{sec:pretraining}, we hypothesise that using \textbf{document-level} information (past or future sentences) as the context may approximate the effect the metadata has on dialogue in pre-training, leading to the model correctly learning similar dependencies in fine-tuning on character profiles. This approach has the undeniable advantage that many datasets are document-level but not so many have access to rich metadata. Our pre-training corpus is \textsc{OpenSubtitles}, which is a large collection of subtitles with timestamps that facilitate the extraction of document-level information. Focusing on past context with no loss of generality, we extract up to 3 past sentences based on the timestamps. Roughly $68\%$ samples contain at least one past sentence. We extract around $14.7$M samples.

\paragraph{Second domain: \textsc{ZOO}}
We use a private in-house dataset, \textsc{ZOO}, to support the conclusions to our research questions. This corpus comprises of English dialogue utterances coming from \textbf{fictional TV series and reality shows} and annotations of types identical to \textsc{Cornell-rich}, as well as additional scene metadata (location, activity and topic). It totals $157$K dialogue lines and annotations for $159$ speakers of $101$K lines. It does not have \textit{gender} or \textit{additional information} annotations.

\paragraph{Preprocessing} The corpora are preprocessed in similar way to \textsc{Cornell-rich}, fixing tokenisation and some common errors\footnote{We provide access to scripts.}. For subword tokenisation, we use SentencePiece to train a BPE model of $8,192$ tokens on the \texttt{train} split of \textsc{Cornell-rich}; it is then used to tokenise all datasets.

\subsection{Baselines and Implementation}
We use three baselines in the experiment: a non-contextual language model (\textsc{base-LM}), a speaker-wise fine-tuning baseline (\textsc{SpFineTuning}) and a linear interpolation method (\textsc{Lerp}) which ensembles \textsc{SpFineTuning} with the general model \textsc{base-LM} at test time \citep{king-cook-2020-evaluating}.

We implement \textsc{LMCue} in \textsc{Fairseq}. The model comprises of a context encoder ($d_{model}=512, n_{layers}=6$) of $38$M parameters and a decoder ($d_{model}=768, n_{layers}=12$) of $121$M parameters. A non contextual decoder (i.e. the \textsc{base-LM} used as a baseline) of this shape has $91$M parameters; this is because of the encoder-decoder attention in \textsc{LMCue} which adds $30$M parameters. All models are pre-trained on \textsc{OpenSubtitles} (with or without context, depending on the model). We use model hyperparameters recommended for similar architectures implemented in \textsc{Fairseq} and pick batch size (simulated $200/320/400$K tokens) and learning rate ($0.0003/0.0005/0.001$) based on validation performance. For fine-tuning, we separately adapt these parameters for each dataset and metadata combination: learning rate ($0.00005$ to $0.001$), batch size ($250$ to $20$K tokens) and context sequence dropout, i.e. randomly dropping out some input contexts from a batch (on/off). We found that a learning rate of $0.00005$ and a batch size of $10$K works best for \textsc{LMCue} generally, and large learning rates ($0.0015/0.002$) with batch sizes of $2-3$K work best for fine-tuning the \textsc{base-LM}. For evaluation, we use perplexity.

\section{Results}
This section presents the experimental results of training personalised \textsc{LMCue} on the \textsc{Cornell-rich} and \textsc{ZOO} corpora. Throughout this section, we use $\mathcal{S}$ and $\mathcal{F}$ to denote that $\mathcal{S}$peaker or $\mathcal{F}$ilm metadata was used in training (or both, i.e. $\mathcal{S}+\mathcal{F}$). 

\subsection{Speaker-Profile-Based Personalisation}
\label{rq1}
\paragraph{Are speaker profiles helpful?}
First, we aim to determine whether including speaker profiles as a supplementary input in language modelling can bring quantitative improvements. We train the studied models on the \texttt{train} split and evaluate on the \texttt{test} sets. Speakers (= unique combinations of metadata) overlap between the sets.

\begin{table}[]
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{r|ccc|ccc}
\toprule
 & \multicolumn{3}{c|}{\textsc{Cornell-rich}} & \multicolumn{3}{c}{ZOO} \\
 & \texttt{valid} & \texttt{test}  & \texttt{unseen} & \texttt{valid} & \texttt{test} & \texttt{unseen} \\
 \midrule
\textit{\textbf{baselines}} &  &  &  &  \\
\textsc{base-LM} & $23.39$ & $23.91$ & $23.67$ & $18.83$ & $18.90$ & $17.87$ \\
\midrule
\textit{\textbf{proposed}} &  &  &  &  \\
\textsc{LMCue} ($\mathcal{S}$) & $21.79$ & $22.22$ & $22.12$ & $17.63$ & $17.68$ & $16.93$ \\
\textsc{LMCue} ($\mathcal{S}+\mathcal{F}$) & $\textbf{21.48}$ & $\textbf{21.89}$ & $\textbf{21.95}$ & $\textbf{17.36}$ & $\textbf{17.37}$ & $\textbf{16.80}$ \\
\bottomrule
\end{tabular}}
\caption{Perplexity on different validation and testing sets for the two corpora.}
\label{tab:rq1-results}
\end{table}
\paragraph{Is speaker-based adaptation better than direct fine-tuning?}
In a supplementary setup, we determine how good our context-based adaptation approach is when compared to LMs fine-tuned on the available speaker dialogue. To facilitate this, we focus our setup on five \textbf{long-term} (multi-episode TV) speakers with at least $3,000$ lines of dialogue sources from the \textsc{ZOO} corpus (\autoref{tab:zoo-speakers}). Within this experiment, we introduce two competitive baselines \citep[modelled after][]{king-cook-2020-evaluating}: speaker fine-tuning (\textsc{SpFineTune}) and linear interpolation (\textsc{Lerp}). We obtain \textsc{SpFineTune} by fine-tuning the LM on all ZOO data initially (FT1), and then one more time on speaker data alone. \textsc{Lerp} is a mean interpolation of \textsc{SpFineTune} with FT1. We do this individually for each speaker $\in \{{\textbf{sp01}, ..., \textbf{sp05}}\}$.

\begin{table*}[]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}rllp{2cm}p{3cm}llp{6cm}@{}}
\toprule
\textit{ID} & \textit{\#lines} & \textit{Gender} & \textit{Age} & \textit{Profession} & \textit{Country} & \textit{Genre} & \textit{Characteristic quote} \\ \midrule
\textbf{sp01} & 7457 & Female & Teen & Student, Spy & United States & Comedy & \enquote{Look, I love you! I have loved you since the moment I saw you. Please! I'll never get a chance to tell you.} \\ \midrule
\textbf{sp02} & 3921 & Male & Young Adult & Unemployed, Community Service & United Kingdom & Comedy, Drama & \enquote{In the words of the great Lionel Richie...hello.} \\ \midrule
\textbf{sp03} & 3229 & Male & Adult & Actor & United States & Docuseries & \enquote{So be present, be spontaneous. Enjoy the moment, enjoy yourself and learn.} \\ \midrule
\textbf{sp04} & 3150 & Male & Adult & Businessman & United States & Reality TV & \enquote{If I don't like it that's all that matters.} \\ \midrule
\textbf{sp05} & 3143 & Male & Adult & Psychiatrist & United States & Horror, Thriller & \enquote{Before we begin, I must warn you... nothing here is vegetarian.} \\ \bottomrule
\end{tabular}}
\caption{Selected metadata regarding long-term speakers from ZOO used in the experiment.}
\label{tab:zoo-speakers}
\end{table*}

\begin{table}[]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{rccccc|c}
\toprule
 & \textbf{s01} & \textbf{s02} & \textbf{s03} & \textbf{s04} & \textbf{s05} & \textbf{Mean} \\ \midrule
 \textit{\textbf{baseline (non-context)}} &  &  &  &  &  &  \\
\textsc{base-LM} & $15.17$ & $18.20$ & $26.89$ & $18.55$ & $23.33$ & $20.42$ \\ \midrule
\textit{\textbf{baselines (fine-tuning)}} &  &  &  &  &  &  \\
\textsc{SpFineTune} & $14.16$ & $\textbf{16.60}$ & $\textbf{24.31}$ & $16.99$ & $20.81$ & $\textbf{18.57}$ \\
\textsc{Lerp} & $\textbf{14.06}$ & $16.86$ & $24.95$ & $17.40$ & $21.48$ & $18.95$  \\ \midrule
\textit{\textbf{proposed (metadata-based)}} &  &  &  &  &  &   \\
\textsc{LMCue} ($\mathcal{S}$) & $14.57$ & $17.26$ & $25.33$ & $17.22$ & $21.06$ & $19.09$  \\
\textsc{LMCue} ($\mathcal{S}+\mathcal{F}$) & $14.26$ & $17.21$ & $24.51$ & $\textbf{16.97}$ & $\textbf{20.67}$ & $18.72$  \\ \bottomrule
\end{tabular}}
\caption{Perplexity on the test set for long-term speakers.}
\label{tab:res-long-term}
\end{table}

\paragraph{Results}
Speaker profiles help produce language models of lower perplexity and are on par with directly adapted LMs. Firstly, \autoref{tab:rq1-results} shows that context-based personalisation with \textsc{LMCue} leads to considerable reductions in perplexity w.r.t. to the best baseline, of $7.1\%$ for \textsc{Cornell-rich} and $6.5\%$ for \textsc{ZOO}, respectively. Secondly, according to \autoref{tab:res-long-term}, all adapted models improve over the LM baseline for all long-term speakers, and particularly \textsc{SpFineTune} achieves the best result overall, reducing perplexity by $1.85$ points on average. Notably, \textsc{LMCue} ($\mathcal{S}+\mathcal{F}$) achieves a comparable reduction of $1.70$ points, beating the \textsc{Lerp} baseline while requiring no speaker-specific fine-tuning. \textsc{LMCue} also achieves the best result overall for \textbf{s04} and \textbf{s05}. Adaptation based on speaker metadata alone (\textsc{LMCue} ($\mathcal{S}$)) improves over the non-contextual baseline (\textsc{base-LM}) but is not better than speaker fine-tuning. Our results indicate that with sufficient metadata, speaker-specific fine-tuning may be redundant in favour of metadata-based adaptation.

\researchquestion{RQ1} {Language modelling for dialogue benefits from personalisation based on speaker context: the best models reduce perplexity by $\textbf{7.1/6.4\%}$ w.r.t best non-context baselines. Making use of show metadata boosts this reduction further to $\textbf{8.5/8.1\%}$. In a few-shot scenario, \textsc{LMCue} may be better than \textsc{Lerp} and achieves $92\%$ of the reduction in perplexity of speaker-specific fine-tuning. Since \textsc{LMCue} requires no fine-tuning to specific speakers, it may be favourable in scenarios with many speakers with little to no prior dialogue. Conversely, \textsc{SpFineTune} may be preferred for speakers with a large dialogue history.}


\subsection{Zero-Shot Transfer to Unseen Speakers}
\label{rq2}

In this section we examine whether speaker adaptation can be done based on metadata \textbf{alone}, i.e. on a set of new speakers unseen during training. Prior to creating \texttt{train}, \texttt{valid} and \texttt{test} splits we randomly selected a number of speakers (with lines summing up to $\sim$$5,000$) and built the \texttt{unseen} test set from all their dialogue; this yielded $31$ speakers in \textsc{Cornell-rich} and $7$ speakers in \textsc{ZOO}. These speakers' dialogue is excluded from training and validation data (though there may be overlaps in film metadata).

As before, we tune the pre-trained \textsc{LMCue} on the \texttt{train} parts of the corpora. As baselines we only use \textsc{base-LM} since other baselines are not equipped to work well in this scenario. \textbf{Results} reported in \autoref{tab:rq1-results} show that on the \texttt{unseen} test set reductions in perplexity are of similar magnitude to those on the \texttt{test} set, and $\mathcal{F}$ilm metadata again boosts performance.

\researchquestion{RQ2} {On a test set with unseen speakers, metadata-based personalisation reduces perplexity by $\textbf{6.6/5.3\%}$. This reduction is of similar magnitude to that for seen speakers ($7.1/6.5\%$), suggesting that our method scales robustly to this scenario, unlike speaker-specific fine-tuning which cannot be applied on new characters. Additionally, using the (overlapping) film metadata ($\mathcal{F}$) boosts this reduction further to $\textbf{7.3/6.0\%}$.}

\subsection{Cost-Benefit Analysis of Human Annotations} \label{rq4}
Granular manual annotations are costly and difficult to come by. Our annotation efforts were based on hypotheses and therefore associated with the risk of making a potentially redundant effort. This section presents the results of the cost-benefit analysis we conducted to show which individual speaker attributes produce most \textbf{benefit} (reduction in perplexity) w.r.t. the perceived \textbf{cost} of producing them.


More concretely, we asked the two human annotators to evaluate the effort the annotation task required by answering three questions on a scale of 1 to 10: 
\begin{enumerate}
    \item \textit{Access}: how difficult was this information to find?
    \item \textit{Credibility}: How confident were you that the identified information was an accurate/useful description of the character?
    \item \textit{Effort}: How much time did you need to spend creating this field relative to other fields?
\end{enumerate}

We reversed the scores for \textit{Credibility} ($C = 10 - C + 1$), and then took the mean of all scores from both annotators to come up with a single value of \textit{cost} for each metadata type. We performed a simple experiment to measure the \textbf{benefit} given by each metadata type: we fine-tuned the pre-trained \textsc{LMCue} on each individually evaluated speaker metadata type. Finally, we measured the relative reduction in perplexity from including this information (\autoref{fig:cost-benefit-abl}).

\begin{figure}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{img/cost_benefit.png}}
    \caption{Perplexity reduction from training \mbox{\textsc{LMCue}} with individual speaker attributes.}
    \label{fig:cost-benefit-abl}
\end{figure}

The figure suggests that \textit{description}, \textit{profession} and \textit{quote} yield the greatest perplexity reduction in both datasets, between $4$ and $7\%$. \textit{Description}, the best-performing attribute, alone achieves $96.2/86.3\%$ of the perplexity reduction of \textsc{LMCue} $(\mathcal{S})$. On the other hand, \textit{age bracket}, \textit{religion} and \textit{country of origin} yield the smallest improvements. For \textsc{ZOO}, improvements from these three attributes are negligible, while for \textsc{Cornell-rich} \textit{country} and \textit{religion} still yield small improvements and only \textit{age bracket} produces close to no improvement. The difference between the two datasets can partially be attributed to the difference in the number and diversity of the character annotations: \textsc{Cornell-rich} contains annotations for over $5$ times as many characters, and e.g. for the \textit{age bracket} attribute, only $25\%$ characters from \textsc{ZOO} use a label other than \enquote{\textit{Adult}}. 

Nevertheless, there are similarities between how the two datasets affect personalisation. \textit{Profession}, collected at a relatively small cost, is the second best attribute for both, making it the most cost-effective. \textit{Religion} is the least cost-effective attribute: it required the most effort but brought about the least benefit. Finally, experimental attributes such as characteristic \textit{quote}s and \textit{additional information}\footnote{Since \textit{additional information} was not collected for \textsc{ZOO}, it is not present on the right plot.} turned out to be useful, placing in the middle of the ranking.

\researchquestion{RQ3} {Textual metadata (descriptions, quotes, professions) is significantly more useful for personalisation with \textsc{LMCue} than discrete metadata (e.g. age bracket). Particularly in our evaluation, descriptions alone achieve results on par with using the entire speaker profile. Furthermore, utility of individual attributes seems to be positively correlated with the diversity in their representation.}

\section{Conclusions} \label{sec:conclusions}
This work presented a novel dataset of rich character and film annotations \textsc{Cornell-rich} and a set of in-depth experiments focusing on context-based adaptation for personalised language modelling. We have shown that character attributes are a useful proxy for personalising LMs when fine-tuning data is not available. Future work could investigate the utility of our approach on downstream tasks such as in dialogue generation and translation. Additionally, following the dialogue framework outlined by \citet{Pickering2004}, future work could explore dynamic representations of speakers which update as conversation goes on, and perhaps building a latent representation for their \textit{common ground}.


\section*{Limitations}
Our work proposes a corpus of English language annotations, a decision which was determined by the language of the original Cornell corpus. However, the annotations could similarly be applied to translations of the films in question if one has the ownership of these. Secondly, the annotations are not balanced with respect to demographic traits such as age, gender or the country of origin. This, again, is largely due to the pre-existing bias in the Cornell Movie Dialog Corpus. 

Finally, in our work each sentence is processed independently, due to our focus on how the speaker's profile impacts language modelling. There may be additional benefits to including past dialogue in the equation, leveraging phenomena such as human mimicry \citep{CHARTRAND2009219}. It is therefore a possible extension of our work to take it beyond the sentence level.

\section*{Acknowledgements}
We would like to thank Robert Flynn for his useful comments on the paper. This work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]. This work was also supported by ZOO Digital.

\bibliography{anthology,mendeley,custom}
\bibliographystyle{acl_natbib}

\end{document}


