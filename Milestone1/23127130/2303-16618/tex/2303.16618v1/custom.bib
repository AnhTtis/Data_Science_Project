@article{giles2007communication,
  title={Communication accommodation theory},
  author={Giles, Howard and Ogay, Tania and others},
  year={2007},
  publisher={na}
}

@inproceedings{schein-etal-2002-cold,
author = {Schein, Andrew I. and Popescul, Alexandrin and Ungar, Lyle H. and Pennock, David M.},
title = {Methods and Metrics for Cold-Start Recommendations},
year = {2002},
isbn = {1581135610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564376.564421},
doi = {10.1145/564376.564421},
abstract = {We have developed a method for recommending items that combines content and collaborative data under a single probabilistic framework. We benchmark our algorithm against a na\"{\i}ve Bayes classifier on the cold-start problem, where we wish to recommend items that no one in the community has yet rated. We systematically explore three testing methodologies using a publicly available data set, and explain how these methods apply to specific real-world applications. We advocate heuristic recommenders when benchmarking to give competent baseline performance. We introduce a new performance metric, the CROC curve, and demonstrate empirically that the various components of our testing strategy combine to obtain deeper understanding of the performance characteristics of recommender systems. Though the emphasis of our testing is on cold-start recommending, our methods for recommending and evaluation are general.},
booktitle = {Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {253–260},
numpages = {8},
keywords = {recommender systems, information retrieval, collaborative filtering, content-based filtering, performance evaluation, graphical models},
location = {Tampere, Finland},
series = {SIGIR '02}
}


@Article{guo-etal-2021-aerospace,
AUTHOR = {Guo, Dongyue and Zhang, Zichen and Fan, Peng and Zhang, Jianwei and Yang, Bo},
TITLE = {A Context-Aware Language Model to Improve the Speech Recognition in Air Traffic Control},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {348},
URL = {https://www.mdpi.com/2226-4310/8/11/348},
ISSN = {2226-4310},
ABSTRACT = {Recognizing isolated digits of the flight callsign is an important and challenging task for automatic speech recognition (ASR) in air traffic control (ATC). Fortunately, the flight callsign is a kind of prior ATC knowledge and is available from dynamic contextual information. In this work, we attempt to utilize this prior knowledge to improve the performance of the callsign identification by integrating it into the language model (LM). The proposed approach is named context-aware language model (CALM), which can be applied for both the ASR decoding and rescoring phase. The proposed model is implemented with an encoder–decoder architecture, in which an extra context encoder is proposed to consider the contextual information. A shared embedding layer is designed to capture the correlations between the ASR text and contextual information. The context attention is introduced to learn discriminative representations to support the decoder module. Finally, the proposed approach is validated with an end-to-end ASR model on a multilingual real-world corpus (ATCSpeech). Experimental results demonstrate that the proposed CALM outperforms other baselines for both the ASR and callsign identification task, and can be practically migrated to a real-time environment.},
DOI = {10.3390/aerospace8110348}
}


@incollection{CHARTRAND2009219,
title = {Chapter 5 Human Mimicry},
series = {Advances in Experimental Social Psychology},
publisher = {Academic Press},
volume = {41},
pages = {219-274},
year = {2009},
issn = {0065-2601},
doi = {https://doi.org/10.1016/S0065-2601(08)00405-X},
url = {https://www.sciencedirect.com/science/article/pii/S006526010800405X},
author = {Tanya L. Chartrand and Rick {van Baaren}},
abstract = {Human mimicry is ubiquitous, and often occurs without the awareness of the person mimicking or the person being mimicked. First, we briefly describe some of the major types of nonconscious mimicry—verbal, facial, emotional, and behavioral—and review the evidence for their automaticity. Next, we argue for the broad impact of mimicry and summarize the literature documenting its influence on the mimicry dyad and beyond. This review highlights the moderators of mimicry as well, including the social, motivational, and emotional conditions that foster or inhibit automatic mimicry. We interpret these findings in light of current theories of mimicry. First, we evaluate the evidence for and against mimicry as a communication tool. Second, we review neuropsychological research that sheds light on the question of how we mimic. What is the cognitive architecture that enables us to do what we perceive others do? We discuss a proposed system, the perception‐behavior link, and the neurological evidence (i.e., the mirror system) supporting it. We will then review the debate on whether mimicry is innate and inevitable. We propose that the architecture enabling mimicry is innate, but that the behavioral mimicry response may actually be (partly) a product of learning or associations. Finally, we speculate on what the behavioral data on mimicry may imply for the evolution of mimicry.}
}

@inproceedings{Sanh2019,
archivePrefix = {arXiv},
arxivId = {1910.01108},
author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
booktitle = {5th Workshop on Energy Efficient Machine Learning and Cognitive Computing @ NeurIPS 2019},
eprint = {1910.01108},
file = {:Users/shanest/Documents/Library/Sanh et al/5th Workshop on Energy Efficient Machine Learning and Cognitive Computing @ NeurIPS 2019/Sanh et al. - 2019 - DistilBERT, a distilled version of BERT smaller, faster, cheaper and lighter.pdf:pdf},
keywords = {model},
title = {{DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}},
url = {http://arxiv.org/abs/1910.01108},
year = {2019}
}

@inproceedings{wang-etal-2020-minilm,
author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
title = {MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Pre-trained language models (e.g., BERT [12] and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer [42] based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant [26] also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99\% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50\% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {485},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@article{milburn-2004-speech,
author = {Trudy Milburn},
title = {Speech Community: Reflections Upon Communication},
journal = {Annals of the International Communication Association},
volume = {28},
number = {1},
pages = {411-441},
year  = {2004},
publisher = {Routledge},
doi = {10.1080/23808985.2004.11679041},
URL = {https://doi.org/10.1080/23808985.2004.11679041},
eprint = {https://doi.org/10.1080/23808985.2004.11679041}
}