% File tacl2021v1.tex
% Dec. 15, 2021

% The English content of this file was modified from various *ACL instructions
% by Lillian Lee and Kristina Toutanova
%
% LaTeXery is mostly all adapted from acl2018.sty.
\pdfoutput=1
\documentclass[11pt,a4paper]{article}
\usepackage[dvipsnames]{xcolor}	
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage{microtype} % makes typesetting nicer
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{cleveref}
\usepackage{booktabs}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{xspace}
\usepackage{listings}
\usepackage{soul} % for \ul and \TODOMARK
\usepackage{caption}
\usepackage{transparent} % for \transparent in the ETH logo
\usepackage{tcolorbox} % for examples in text
\usepackage{mdframed} % for quote boxes
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{pifont}
\usepackage{tipa}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyphenat}


%% Package options:
%% Short version: "hyperref" and "submission" are the defaults.
%% More verbose version:
%% Most compact command to produce a submission version with hyperref enabled
%%    \usepackage[]{tacl2021v1}
%% Most compact command to produce a "camera-ready" version
%%    \usepackage[acceptedWithA]{tacl2021v1}
%% Most compact command to produce a double-spaced copy-editor's version
%%    \usepackage[acceptedWithA,copyedit]{tacl2021v1}
%
%% If you need to disable hyperref in any of the above settings (see Section
%% "LaTeX files") in the TACL instructions), add ",nohyperref" in the square
%% brackets. (The comma is a delimiter in case there are multiple options specified.)

% \usepackage[review]{misc/acl}
\usepackage{misc/acl}
% \setlength\titlebox{10cm} % <- for Option 2 below


\input{misc/macros.tex}

\title{Personalised Language Modelling of Screen Characters \\ Using Rich Metadata Annotations}

% Author information does not appear in the pdf unless the "acceptedWithA" option is given

\newcommand{\AfA}{\textsuperscript{\begin{minipage}[c]{2.5mm}
\includegraphics[width=\linewidth]{img/club.png}
\end{minipage}}}
\newcommand{\AfB}{\textsuperscript{\begin{minipage}[c]{2.5mm}
\includegraphics[width=\linewidth]{img/diamond.png}
\end{minipage}}}
\newcommand{\bustspeaker}{\begin{minipage}[c]{4.5mm}
\includegraphics[width=\linewidth]{img/bust_silh.png}
\end{minipage}\hspace{1mm}}
\newcommand{\annedspeaker}{\begin{minipage}[c]{4.5mm}
\includegraphics[width=\linewidth]{img/woman-artist.png}
\end{minipage}\hspace{1mm}}
\newcommand{\cmark}{\textcolor{OliveGreen}{\ding{51}}}
\newcommand{\xmark}{\textcolor{BrickRed}{\ding{55}}}
\newcommand{\lgc}{\cellcolor{lightgray}}
\newcommand{\dgc}{\cellcolor{darkgray}}
\newcommand{\lds}{-------}
\setlength{\marginparwidth}{2cm}

\author{\AfA Sebastian Vincent,
        \AfB \textbf{Rowanne Sumner},
        \AfB \textbf{Alice Dowek},
        \AfB \textbf{Charlotte Blundell}, \\
        \AfB \textbf{Emily Preston},
        \AfB\textbf{Chris Bayliss},
        \AfB \textbf{Chris Oakley}, 
        \AfA \textbf{Carolina Scarton}
        \\[5pt]
        \AfA Department of Computer Science, University of Sheffield, UK\\
        \AfB ZOO Digital Group PLC, UK\\
        \texttt{stvincent1@sheffield.ac.uk, c.scarton@sheffield.ac.uk}\\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Language models that are sensitive to external context can more effectively capture the speaking patterns of individuals with specific characteristics or in particular environments. However, obtaining and leveraging such annotations can be challenging. In this work, we show how to leverage rich character and film annotations to personalise language models in a scalable manner. Our best model can reduce perplexity by up to $6.5\%$ compared to a parameter-matched language model. Our approach performs on par with speaker-specific fine-tuning when the fine-tuning data (i.e. past dialogue) for individual speakers is available. On top of that, it also generalises well to a scenario with no such data, relying on combinations of demographic characteristics expressed via metadata. Our findings are consistent across two corpora, one of which is also a contribution of this paper: \textsc{Cornell-rich} contains rich manual annotations for $863$ speaking characters from the Cornell Movie Dialog Corpus, including features such as characteristic quotes and character descriptions, along with six automatically extracted metadata features for over $95\%$ of the featured films. Finally, we also present a cost-benefit analysis highlighting which annotations are most cost-effective in reducing perplexity.
\end{abstract}

% GitHub logo
\begin{minipage}[c]{4.5mm}
\includegraphics[width=\linewidth]{img/github_mark.pdf}
\end{minipage}
\hspace{0mm}
\begin{minipage}[c]{0.7\textwidth}
\fontsize{8pt}{8pt}\selectfont
\href{https://github.com/st-vincent1/cornell_rich}
{\texttt{github.com/st-vincent1/cornell\_rich}}
\end{minipage}

\hspace{-0.8mm}
\begin{minipage}[c]{4.5mm}
\includegraphics[width=\linewidth]{img/github_mark.pdf}
\end{minipage}
\hspace{0mm}
\begin{minipage}[c]{0.7\textwidth}
\fontsize{8pt}{8pt}\selectfont
\href{https://github.com/st-vincent1/lmcue}
{\texttt{github.com/st-vincent1/lmcue}}
\end{minipage}

\section{Introduction}

Studies of sociolinguistics have long accepted that spoken language is not universal \citep{milburn-2004-speech}. Contrary to this, conventional approaches to generation tasks in natural language processing (NLP) build models in a one-size-fits-all fashion, and most often for a particular language and domain, disregarding the context of the processed text. In practice, this leads to assuming the most likely scenario as context, sometimes resulting in harmful predictions \citep[e.g. the \enquote{masculine default} in][]{Schiebinger2014}. Instead, \textbf{personalisation} offers clear benefits in generation tasks \citep{flek-2020-returning,dudy-etal-2021-refocusing}, where context information can help disambiguate the input text and interpret it correctly. %This disambiguation also helps minimise sample bias in training data \citep{dudy-etal-2021-refocusing}.

Demographic factors have been shown to improve the performance in various NLP tasks, such as classification \citep{hovy-2015-demographic}, generation \citep{zeng-etal-2019-automatic}, and translation \citep{vincent-etal-2022-controlling-extra}. This impact can be \textit{grammatical}, which is well-defined and of a morphosyntactic nature, and \textit{behavioural}, which is more fluid and pertains to the way language is used by certain demographics or in certain situations. Grammatical agreement can be exemplified by gender, which, in some languages, determines the morphological ending of self-referent verbs. Behavioural agreement is more subtle; e.g. \enquote{They're done!} has a different meaning when said by a baker about a batch of cookies than by a frenzied king about his treacherous subjects. However, contextual language generation methods frequently concentrate exclusively on grammatical agreement %as it is a measurable outcome that is well-understood and easier to evaluate 
\citep[e.g. document-level translation in][]{voita-etal-2019-good}. In practice, however, grammatical and behavioural agreement are required %at different stages of 
in the language generation process and a robust framework accommodates both.

This work focuses on personalising language models (LMs) for speakers in scripted dialogue in TV series and films, which we refer to as \textit{productions}. The way language is used in this domain can vary greatly; for example, TV writers will construct characters who mimic the way of speaking of a certain group they represent; productions from a certain decade, country or within a specific genre will capture the discourse nuances of that group. We demonstrate how speaker and production metadata can be used to create context-based personalised LMs that model the language of a specific speaker or production more effectively than a one-size-fits-all model. 
The TV/film domain presents an additional challenge: models must be robust to the scenario where there are no prior dialogue samples for the given speakers or productions, i.e. when new content arrives and only metadata is available. 
This is known as the cold start problem \citep[e.g.][]{schein-etal-2002-cold, huang-etal-2014-enriching} where there is insufficient content to characterise the subjects of a given system. 
Models adapted solely on past dialogue are not robust for this case, and we argue that a context-based approach is more effective,
in particular by estimating token distributions for similar character/production profiles. Finally, while demographic-based personalisation could be achieved through extremely large LMs by priming them with a concatenation of all rich metadata, production data is often subject to copyright, necessitating solutions which can be deployed on-site. 

Since the datasets in our selected domain contains identifiable information such as titles and character names, we were able to collect a rich set of metadata annotations for the selected corpus, allowing us to perform experiments on up to $16$ unique metadata variables at once. In contrast, metadata-based approaches to personalisation reported in previous work in different domains were small-scale, leveraging a few simple and mostly categorical variables \citep{huang-etal-2014-enriching, 
lynn-etal-2017-human, 
king-cook-2020-evaluating,
welch-etal-2020-compositional,
guo-etal-2021-aerospace}. 
To our knowledge, the present study uses the richest set of metadata information for personalisation.
More specifically, this work addresses the following research questions (RQs):
\researchquestion{RQ1} {How can rich character profiles be used to model the characters' speaking styles? (\S \ref{rq1}) \\
\textbf{RQ2}: How can a LM be personalised for a specific character solely by learning from data for characters with similar profiles? (\S \ref{rq2}) \\
\textbf{RQ3}: Which character metadata are the most cost-effective for personalisation? (\S \ref{rq3})
}

Additionally, we contribute \textsc{Cornell-rich} (\S \ref{tacl:cornell}), a corpus of rich character and film annotations for the Cornell Movie Dialogue Corpus \citep{Danescu-Niculescu-Mizil2011} (\textsc{Cornell}) and \textsc{sMRR}, a bespoke evaluation metric for personalised language models (\S \ref{tacl:eval}). In \S \ref{tacl:pretraining} we discuss our pre-training strategy which contributed to the success of our approach. This paper also presents the experimental setup (\S \ref{tacl:experimental}), and conclusions (\S \ref{tacl:conclusions}).\footnote{The data, the code for preprocessing data, model implementation and scripts for reproducing results will be made available via GitHub upon publication.}

\section{Related Work} \label{related-work}
Personalisation in NLP can generally be split into three groups with respect to how much data is available for a new speaker: full supervision (sufficient training data), few-shot (only sample data), and zero-shot (no data). Full supervision is usually facilitated through some form of a \textit{user embedding} or \textit{tagging} approach \citep[e.g.][]{sennrich-etal-2016-controlling,Keskar2019ctrl,mireshghallah-etal-2022-useridentifier}. Among few-shot approaches, \citet{king-cook-2020-evaluating} examine several personalisation methods (interpolation, fine-tuning, priming, and demographic-based adaptation) for language modelling of blog posts with sample adaptation data for new users. \citet{welch-etal-2022-leveraging} consider a scenario where models built for  \enquote{anchor users} (who boast a large history of posting) are leveraged to build models for new users (with a small number of posts), focusing on the similarity between samples of users' posts. Zero-shot approaches typically leverage background data available for the new speakers, like demographic factors or metadata. \citet{huang-etal-2014-enriching} rely on the social network of a user to model their language; \citet{lynn-etal-2017-human} use age, gender, and personality traits to improve user modelling in multiple NLP tasks; \citet{zeng-etal-2019-automatic} leverage user profiles to improve comment generation on a social media corpus; \citet{welch-etal-2020-compositional} produce compositional demographic word embeddings by learning demographic-specific vectors for each word in the vocabulary. Demographic-based adaptation was found inferior to interpolation and priming in the few-shot scenario by \citeauthor{king-cook-2020-evaluating}, but their study only used two factors: age and gender. 

Our work is positioned in the zero-shot category as we rely on rich metadata annotations to model individual screen characters. Unlike \citeauthor{king-cook-2020-evaluating}, we leverage textual (real-valued) metadata annotations, which in personalisation are preferable to categorical values \citep{lynn-etal-2017-human}, and a significantly higher count of them (up to $16$). A few works have explored the idea that context in LMs can be summarised with pre-trained models. \citet{ippolito-etal-2020-toward} propose a language model which selects the best continuation to a story from a finite set of pre-trained embeddings. \citet{novotney-etal-2022-cue} introduce the notion of \textsc{cue} (contextual universal embedding) vectors, representing individual context variables as pre-trained vectors. They use \textsc{DistilBERT} to obtain the vectors, pass them through a context encoder and average the result. \citeauthor{novotney-etal-2022-cue} demonstrate that including article metadata in the form of \textsc{cue} improves perplexity in language modelling of the articles themselves. %Our work focuses on a different domain (scripted dialogue) and specifically on personalisation for individual characters and films. 
Inspired by \citeauthor{novotney-etal-2022-cue}, %in that 
we use \textsc{cue} vectors to improve language modelling. However, our work significantly differs in architecture and sentence embedding model choice, focusing on a different domain (scripted dialogue) and specifically on personalisation for individual characters and films. We also contribute observations regarding pre-training with \textsc{cue}, a novel metadata corpus, and an evaluation metric.

\section{\textsc{Cornell-rich} Dataset}
\label{tacl:cornell}

\textsc{Cornell-rich} is a dataset of rich character and film annotations, including $14$ distinct metadata variables captured as text. The annotations can be linked against entries of \textsc{Cornell} \citep{Danescu-Niculescu-Mizil2011} which is a corpus of exchanges from a set of film scripts, with character dialogue attributions (\autoref{tacl:cornells} illustrates how \textsc{Cornell-rich} enriches the original corpus). Both dialogue data and annotations are in English. %We describe the data collection process and corpus details in this section.

\begin{figure}[h]
    \centering
    \resizebox{.8\linewidth}{!}{\includegraphics{img/cornells.png}}
    \caption{Comparison between \textsc{Cornell} and our proposed \textsc{Cornell-rich}.}
    \label{tacl:cornells}
\end{figure}

\subsection{Details}
\textsc{Cornell-rich} comprises annotations for $863$ speakers (speaker \textit{profiles}), covering $135.7$K utterances; nearly half of the annotated speakers have $150+$ lines of dialogue and about $25\%$ have $200+$. At least $64.1\%$ of conversational exchanges feature at least one annotated character and as much as $95.5\%$ of the featured films are annotated with film metadata (\autoref{tab:cornell-speaker-details}). We provide a full list of collected metadata with examples in \autoref{tab:examples}.

%todo make a little figure: Cornell Movie Dialog Corpus on the left, with movie id, character id, dialogue; 
\begin{table}[h]
\centering
\scalebox{0.7}{
\begin{tabular}{@{}r|rc|cc@{}}
\toprule
& Count & \% of all & Utterances & \% of all \\ \midrule
\midrule
\textbf{speakers} & $9.0$K & $-$ & $304.3$K & $-$ \\
\annedspeaker & $863$ & $9.5\%$ & $135.7$K & $44.6\%$ \\ 
\bustspeaker & $8.2$K & $90.5\%$ & $170.1$K & $55.9\%$ \\ \midrule
\textbf{exchanges} & $83.1$K & $-$ & $304.3$K & $-$ \\
\annedspeaker $\Leftrightarrow$ (\bustspeaker or \annedspeaker) & $53.3$K & $64.1\%$ & $202.4$K & $66.5\%$ \\
\bustspeaker $\Leftrightarrow$ \annedspeaker & $36.8$K & $44.3\%$ & $134.4$K & $44.2$\% \\
\annedspeaker $\Leftrightarrow$ \annedspeaker & $16.5$K & $19.8\%$ & $68.0$K & $22.3\%$ \\ \midrule
\midrule
\textbf{films} & $617$ & $-$ & $304.3$K & $-$ \\
annotated & $589$ & $95.5\%$ & $291.0$K & $95.6\%$ \\ \bottomrule
\end{tabular}}
\caption{Details of annotations compared to data quantities from \textsc{Cornell}. \annedspeaker $=$ speaker with rich annotations. \bustspeaker $=$ speaker without rich annotations.}
\label{tab:cornell-speaker-details}
\end{table}

\begin{table}[h]
\centering
\scalebox{0.7}{
\begin{tabular}{@{}rp{7cm}@{}}
\toprule
Metadata type & Value \\ \midrule
\textbf{Speaker metadata} & \\
Gender & A man \\
Age bracket & Adult \\
Profession & Attorney \\
Description & Galvin graduated from Boston College's law school. Galvin had a promising legal career ahead of him at an elite Boston law firm until he was framed for jury tampering by a partner due to his plans to expose the firm's underhanded activities. (...) \\
Quote & Your honor, with all due respect: if you're going to try my case for me, I wish you wouldn't lose it. \\
Country of origin & USA \\
Religion & Christian \\
\textbf{Film metadata} & \\
Genre & Comedy, Drama \\
PG Rating & PG Rating: R \\
Names of writers & Written by: Paul Andréota, André Cayatte, Henri Coupon \\
Country of production & France, Italy \\
Year of release & Released in 1974 \\
Plot description & A French judge try to acquit a man who is accused of murdering his lover. \\ \bottomrule
\end{tabular}}
\caption{A sample from \textsc{Cornell-rich} with each type of collected metadata.}
\label{tab:examples}
\end{table}

\begin{figure*}[t]
  \centering
 \resizebox{\linewidth}{!}{
  \subfloat[Number of lines per production year of films in the corpus.]
  {
    \includegraphics[width=0.3\textwidth]{img/line_years.png}
   \label{tacl:years}
  }
  \hspace{0.3cm}
  \subfloat[15 most popular professions in the corpus.]
  {
    \includegraphics[width=0.32\textwidth]{img/bars_professions.png}
    \label{tacl:profs}
  }
  \hspace{0.3cm}
  \subfloat[12 most popular genres in the corpus. Titles labelled as multiple genres are counted multiple times.]
  {
    \includegraphics[width=0.32\textwidth]{img/pie_chart_genres.png}
    \label{tacl:genres}
  }}
  \caption{Visualisation of a subset of features of the proposed corpus.}
  \label{tacl:movie-details}
\end{figure*}


As per \autoref{tacl:movie-details}, the annotated films span nearly a century, with most lines coming from between the $1990$s and $2005$; the distribution of professions is significantly flatter, with the dominant field (\enquote{High School Student}) only making up about $3\%$ of the corpus. Finally, the most popular genres include drama, comedy, crime, and action.

\subsection{Creation process}
The data collection process was carried out by two annotators, both native English speakers and experts in the dubbing and subtitling industry. 
After parsing \textsc{Cornell}\footnote{\url{https://convokit.cornell.edu/documentation/movie.html}, accessed 30/4/23.}, a spreadsheet of characters was generated that included their name, source film, and the number of lines attributed.
%First, \textsc{Cornell} was parsed according to its instructions\footnote{\url{https://convokit.cornell.edu/documentation/movie.html}, accessed 30/4/23.}. From this, a spreadsheet of characters was generated which included their name, source film and the number of lines attributed. 
From previous work \citep[e.g.][]{johannsen-etal-2015-cross} and hypotheses made based on experts' experience, we pre-defined a number of categories of information to collect about each character. Specifically, we selected categories that we hypothesised to (i) be identifiable from the available sources and (ii) influence a person's speaking style or vocabulary used. They were: their \textbf{age bracket} $\in \{\text{child, teen, young adult, adult, elderly}\}$, \textbf{profession}, \textbf{character description} (a few sentences summarising their personality or character arc), \textbf{religion} and a \textbf{characteristic quote}: a typical or quotable thing the character might say. Additionally, the \textbf{gender} annotations from the original corpus were re-used, and an optional column \enquote{\textbf{additional information}} was included to collect comments from experts\footnote{Upon inspection: the annotators predominantly used this field to provide the actor's name, an interesting fact about the character (e.g. \enquote{Plays a caricature of himself}), or trivia.}. The characters with the most lines spoken were prioritised, resulting in all $714$ characters with 100+ lines and $149$ with $50-99$ lines being annotated, totalling $863$ characters.

\paragraph{Annotation sources} Annotations are based on publicly available pages from Wikipedia\footnote{\url{https://wikipedia.org/}} for individual films, as well as fan-made Fandom\footnote{\url{https://fandom.com/}} pages for both films and characters. Where information was unavailable from these sources, the annotators either referred back to the corpus itself or skipped the given field altogether. The film metadata was obtained via the OMDb API.\footnote{\url{https://omdbapi.com/}}

\paragraph{Annotation decisions} The annotation process involved matching every script's name against an IMDb entry, which did not always yield a match as some scripts had been scrapped or rewritten or characters' names had been changed. Unidentifiable films and characters were not considered for annotation.
Some information, especially \textit{religion}, was occasionally difficult to find, in which case it would be skipped or labelled as \textit{Unknown}. It was challenging to produce annotations for characters based on real people, or for a real person played by themselves. Where characters were based on historical figures, the annotators focused on the production interpretation of the person; when dealing with a characterisation of the person at a specific point in time, the focus was on their behaviour at that point in time. Finally, some characteristics were unsuitable for selected character information: e.g. when a character was immortal, it did not fit into set age brackets, and for some characters there were limited clues to determine their age bracket. In both cases, the final annotations were based on the annotators' expertise.

\paragraph{Preprocessing and splits} \label{tacl:tvt}
Since the dialogue in \textsc{Cornell} was already of high quality, our preprocessing step only involved normalising punctuation, removing tokenisation using the \texttt{sacremoses} package\footnote{\url{https://pypi.org/project/mosestokenizer/}}, fixing leftover punctuation issues (e.g. ensuring all multi-dots use three dots) and removing \textsc{HTML} tags.
We also preprocessed all (original and added) annotations so that: (i) all empty fields are expressed as an empty string; (ii) there are no multiple expressions of the same discrete type (e.g. \textit{m} and \textit{M} to denote masculine gender); (iii) all attributes are expressed in unambiguous natural language (e.g. a PG rating of \enquote{R} is rewritten as \enquote{PG Rating: R}).

To facilitate reproducibility, we split the corpus into \textbf{training}, \textbf{validation} and \textbf{testing} sets. To also consider a scenario with completely new characters, we first create \texttt{test\_unseen}, a test set with metadata and dialogue segments from $30$ held-out speakers. We then perform random sampling to create \texttt{test}, \texttt{valid} and \texttt{train} sets. Between these three sets, both production and speaker metadata, but not dialogue segments themselves, will repeat. Data quantities are provided in \autoref{tab:data-quants} (rows 1-5).

\section{Experimental Setup}
\label{tacl:experimental}

\paragraph{The architecture}\label{tacl:arch}
Our selected architecture is a standard encoder-decoder Transformer model \citep{Vaswani2017}, where the encoder uses pre-computed context vectors as input rather than tokens (and the decoder is causal). The positional encoding of the encoder input is removed\footnote{We found no benefit to incorporating positional encoding e.g. to distinguish individual context variables.}. 
For brevity, hereinafter we refer to this model as \textsc{LMCue}: \textbf{L}anguage \textbf{M}odel with \textbf{c}ontextual \textbf{u}niversal \textbf{e}mbeddings.

We follow \citet{vincent-etal-2023-mtcue} who used pooled outputs of \textsc{MiniLM-v2} \citep{wang-etal-2021-minilmv2} to embed all input contexts as equal-sized vectors. This approach has the advantage of treating both discrete and continuous (text) inputs in the same way, potentially utilising the semantic information of the discrete labels, as well as allowing longer spans of context as input without issues of long-range dependencies. The target sequences are contextualised via standard encoder-decoder attention which maps queries (target) to keys and values (context). We use \textsc{QK-Norm} to compute the attention weights \citep{henry-etal-2020-query}.

\subsection{Pre-training}

\begin{table}[h]
\centering
\scalebox{0.7}{
\begin{tabular}{@{}crccc}
\toprule
& \multicolumn{1}{l}{} & \multicolumn{3}{c}{Total number of}    \\
Row & Dataset \& split & segments & tokens & unique metadata \\
\midrule
(1) & \textsc{Cornell-rich} & & &  \\
(2) & \texttt{train} & $289.0$K & $3.1$M & \multirow{4}{*}{$16$}\\
(3) & \texttt{valid} & $5$K & $51.2$K & \\
(4) & \texttt{test} & $5$K & $54.4$K & \\
(5) & \texttt{test\_unseen} & $5.2$K & $54.6$K & \\ \midrule
(6) & \textsc{OpenSubtitles} & & &  \\
(7) & \texttt{train}       & $14.7$M   & $109.6$M & $3$* \\ \midrule
(8) & \textsc{Anon} & & & \\
(9) & \texttt{train}       & $140.4$K & $1.1$M  & \multirow{4}{*}{$15$} \\
(10) & \texttt{valid}       & $4$K     & $31.3$K &  \\
(11) & \texttt{test} & $6$K     & $47.1$K &  \\
(12) & \texttt{test\_unseen} & $6.7$K   & $51.5$K &  \\ \bottomrule
\end{tabular}}
\caption{Quantities of segments, tokens (pre-tokenisation) and unique metadata (speaker and production) in datasets. *\textsc{OpenSubtitles} uses three past sentences as proxy metadata.}
\label{tab:data-quants}
\end{table}

Our preliminary experiments showed that training \textsc{LMCue} from scratch on \textsc{Cornell-rich} lead to results inferior to a non-contextual language model trained on the same data (discussed in \S \ref{tacl:pretraining}). We therefore experimented with pre-training the model first. Since a larger corpus of dialogue with character metadata is typically unavailable, we used a corpus with document-level information and treated the \textbf{past dialogue} for any sentence as the \textbf{metadata context}. We hypothesised that at a larger scale, the effect of metadata embeddings on text generation will be similar to the effect of embeddings of past dialogue (\autoref{tacl:pt-ft}), meaning the pre-training procedure allows the model to learn dependencies between the context and the text. 

\begin{figure}[h]
    \centering
    \resizebox{.8\linewidth}{!}{
    \includegraphics{img/pt-ft-lmcue.png}}
    \caption{An illustration of the pre-training and fine-tuning regimens used in the experiments.}
    \label{tacl:pt-ft}
\end{figure}

This approach has the advantage that metadata-rich corpora are likely to be too small to train a model from scratch, but document-level information is abundant. In our case, pre-training on past dialogue proved successful; consequently, all models considered in our experiments are pre-trained. For this purpose, we use the OpenSubtitles18\footnote{Based on \url{https://opensubtitles.org/}} corpus \citep{lison-etal-2018-opensubtitles2018} (\textsc{OpenSubtitles}). It is a large collection of subtitles with timestamps that facilitate the extraction of document-level information. Focusing on past context with no loss of generality, we extract up to 3 past sentences based on the timestamps (\autoref{tab:data-quants}, rows 6-7). Roughly $68\%$ samples contain at least one past sentence.

%\subsection{Second in-domain corpus: \textsc{ZOO}} \label{tacl:zoo}
\subsection{Second in-domain corpus: \textsc{Anon}} \label{tacl:zoo}

To give a broader context to our results, we perform experiments on an independent private in-house dataset (hereinafter \textsc{Anon}) which comprises English dialogue utterances from fictional TV series and reality shows (in contrast with only films in \textsc{Cornell-rich}) and similar annotations collected in the same campaign. The corpus totals $157$K dialogue lines and annotations for $159$ speakers of $101$K lines, starring in a total of nine shows. It does not have \textit{additional information} annotations. The dataset is segmented into \texttt{train}, \texttt{valid}, \texttt{test} and \texttt{test\_unseen} splits in the same way as \textsc{Cornell-rich} (\S  \ref{tacl:tvt}), and \texttt{test\_unseen} here features $11$ speakers. Quantitative details are reported in \autoref{tab:data-quants} (rows 8-12). 

\subsection{Preprocessing}
\textsc{Anon} and \textsc{OpenSubtitles} are preprocessed in a similar way to \textsc{Cornell-rich}, fixing tokenisation and common errors\footnote{We will provide access to the scripts.}. For subword tokenisation, we use SentencePiece to train a BPE model of $8$K tokens on the \texttt{train} split of \textsc{Cornell-rich}; it is then used to tokenise all datasets.

\subsection{Baselines and implementation}
We consider three baselines: a non-contextual LM (\textsc{base-LM}), a speaker-wise fine-tuning baseline (\textsc{SpFineTuning}) and a linear interpolation method (\textsc{Lerp}) which ensembles \textsc{SpFineTuning} with the general model \textsc{base-LM} at test time; both baselines are modelled after \citet{king-cook-2020-evaluating}.

We implement \textsc{LMCue} in \textsc{Fairseq} \citep{ott-etal-2019-fairseq}. The model has $159$M parameters and comprises a context encoder ($38$M) and a decoder ($121$M). $25\%$ of the decoder's parameters are used by the encoder-decoder attention; a non-contextual decoder of this shape would have $91$M parameters. To make the comparison fair, \textsc{base-LM} matches the total number of parameters in \textsc{LMCue} ($159$M) and is therefore wider than the decoder in \textsc{LMCue} (for details see \autoref{tab:model-details}). This strong baseline will remove the possibility that the model improves simply because of a higher parameter count\footnote{Results from using the smaller baseline LM ($91$M params) were consistently inferior to $159$M by up to $0.75$ perplexity.}. All other baselines (\textsc{SpFineTune}, \textsc{Lerp}) share the architecture and size of \textsc{base-LM}.


\begin{table}[ht]
\centering
\scalebox{0.7}{
\begin{tabular}{clccccc}
\toprule
 & & Params & $d_{model}$ & $n_{layers}$ & $h$ & FFN dim. \\
 \midrule
(1) & \textsc{LMCue} (Enc.) & $38$M & $512$ & $6$ & $8$ & $2048$ \\
(2) & \textsc{LMCue} (Dec.) & $121$M & $768$ & $12$ & $12$ & $3072$ \\
(3) & \textsc{LMCue} (total) & $159$M & $-$ & $-$ & $-$ & $-$ \\ \midrule
(4) & \textsc{base-LM} & $159$M & $1024$ & $12$ & $16$ & $4096$ \\ \bottomrule
\end{tabular}}
\caption{Model details for \textsc{LMCue} and \textsc{base-LM}.}
\label{tab:model-details}
\end{table}

The \textsc{LMCue} models are pre-trained on \textsc{OpenSubtitles} (using past dialogue as context), while \textsc{base-LM} is pre-trained on the text part of the corpus, one sentence at a time. We use off-the-shelf model architectures with pre-defined hyperparameters in \textsc{Fairseq} and only tune on three values each for batch size (simulated $200$K to $400$K tokens) and learning rate ($3\mathrm{e}-4$ to $1\mathrm{e}-3$) based on validation performance on \texttt{valid} in \textsc{Cornell-rich}. For fine-tuning, we separately adapt these parameters for each dataset and metadata combination: learning rate ($5\mathrm{e}-5$ to $1\mathrm{e}-3$) and batch size ($0.25$K to $20$K tokens). The best fine-tuning set of learning rate and batch size was $5\mathrm{e}-5$ and $1.5$K for \textsc{LMCue} and $2\mathrm{e}-3$ and $3$K for \textsc{base-LM}. Each model was trained on a single $32$GB V$100$ GPU with an early stopping condition of validation loss not improving for $5$ epochs. Pre-training \textsc{LMCue} and \textsc{base-LM} took $35$ and $17.5$ GPU hours respectively while fine-tuning these models took respectively $0.78$ and $0.32$ GPU hours on average.

\paragraph{Evaluation} \label{tacl:eval}
For evaluation we use perplexity (\textsc{ppl}) as well as speaker mean reciprocal rank (\textsc{sMRR}), which we define as follows: let $M_j$ be a model personalised for a speaker $s_j$ and $U_i$ be a set of utterances by a speaker $s_i$. We calculate speaker reciprocal rank \textsc{sRR} for any speaker $k$ by scoring the $U_k$ with $M_1,...,M_n$ (expressed with log probability), then ranking the models best to worst by this score\footnote{Ties are resolved pessimistically.} and taking the reciprocal rank ($1/rank$) of $M_k$, the model for speaker $k$ (see \autoref{tacl:smrr}). \textsc{sMRR} is \textsc{sRR} averaged for all speakers; $1/\textsc{sMRR}$ is the average rank of the correct speaker model. Intuitively, this metric captures the strength of the association between dialogue and the speaker model: \textsc{sMRR} of $1.0$ indicates that for any speaker $j$, the model $M_j$ produces the best score for $U_j$.

\begin{figure}[h]
    \centering
    \resizebox{.8\linewidth}{!}{
    \includegraphics{img/smrr_v2.png}}
    \caption{\textsc{sRR} illustrated for one speaker (Hannah).}
    \label{tacl:smrr}
\end{figure}

Unless otherwise specified, all results are calculated from five runs with different random initialisation, and the reported value is the \textbf{mean} result. We \textbf{highlight} the best overall result. Unless another result is \underline{underlined}, it is significantly worse (indicating a less effective model) than the best result in bold, with a confidence interval of $95\%$ (computed with a one-tailed t-test, $t(4)=1.65$, $p=0.05$).

\section{Results}
This section presents the results of training \textsc{LMCue} on the \textsc{Cornell-rich} and \textsc{ZOO} corpora. As before, we use the umbrella term \textbf{production} to refer to the different types of media (film and TV series) in the corpora. Furthermore, we use $\mathcal{S}$ and $\mathcal{P}$ to denote that $\mathcal{S}$peaker or $\mathcal{P}$roduction metadata was used in training (or both, i.e. $\mathcal{S}+\mathcal{P}$). 

\subsection{Personalisation Based on Speaker Profiles} \label{rq1}
\paragraph{Are speaker profiles helpful?}
%Our initial investigation 
We examine whether including speaker profiles as a supplementary input in language modelling can result in significant quantitative improvements. For this, we train models on the \texttt{train} splits and evaluate on the \texttt{test} splits of both corpora, with overlapping speakers (= unique combinations of speaker profiles) between them. As demonstrated in \autoref{tab:rq1-results}, context-based personalisation with \textsc{LMCue} results in substantial reductions in perplexity compared to the best baseline, with a decrease of $5.4\%$ for \textsc{Cornell-rich} and $6.5\%$ for \textsc{ZOO}, respectively. 

\begin{table}[h]
\centering
\scalebox{0.7}{\begin{tabular}{r|cc|cc}
\toprule
 & \multicolumn{2}{c|}{\textsc{Cornell-rich}} & \multicolumn{2}{c}{Anon} \\
 & \texttt{valid} & \texttt{test} & \texttt{valid} & \texttt{test} \\
 \midrule
\textit{\textbf{baselines}} &  &  &  & \\
% \textsc{base-LM} & $91$M & $23.27$ & $23.71$ & $18.75$ & $18.76$ \\
\textsc{base-LM} & $22.35$ & $23.38$ & $18.42$ & $18.41$ \\
% \textsc{Tag} & $x$ & $x$ & $x$ & $x$ \\
\midrule
\textit{\textbf{proposed}}  &  &  &  & \\
\textsc{LMCue} ($\mathcal{S}$) & $21.37$ & $22.37$ & $17.52$ & $17.55$ \\
\textsc{LMCue} ($\mathcal{P}$) & $\textbf{21.07}$ & $\textbf{22.04}$ & $17.18$ & $17.29$ \\
\textsc{LMCue} ($\mathcal{S}+\mathcal{P}$) & $21.14$ & $22.13$ & $\textbf{17.13}$ & $\textbf{17.21}$ \\
\bottomrule
\end{tabular}}
\caption{Perplexity$^\downarrow$ on different validation and testing sets for the two corpora.}
\label{tab:rq1-results}
\end{table}

\paragraph{Is speaker-based adaptation better than direct fine-tuning?}

\begin{table*}[h]
\centering
\scalebox{0.65}{
\begin{tabular}{@{}rllp{1.2cm}p{3cm}llp{6cm}@{}}
\toprule
\textit{\textbf{ID}} & \textit{\textbf{\#Lines}} & \textit{\textbf{Gender}} & \textit{\textbf{Age}} & \textit{\textbf{Profession}} & \textit{\textbf{Country}} & \textit{\textbf{Genre}} & \textit{\textbf{Characteristic quote}} \\ \midrule
\textbf{sp01} & $7.5$K & Female & Teen & Student, Spy & United States & Comedy & \enquote{Look, I love you! I have loved you since the moment I saw you. Please! I'll never get a chance to tell you.} \\ \midrule
\textbf{sp02} & $3.9$K & Male & Young Adult & Unemployed, Community Service & United Kingdom & Comedy, Drama & \enquote{In the words of the great Lionel Richie...hello.} \\ \midrule
\textbf{sp03} & $3.2$K & Male & Adult & Actor & United States & Docuseries & \enquote{So be present, be spontaneous. Enjoy the moment, enjoy yourself and learn.} \\ \midrule
\textbf{sp04} & $3.1$K & Male & Adult & Criminal Profiler & United States & Crime, Drama, Horror & \enquote{It isn't very smart to piss off a guy who thinks about killing people for a living.} \\ \midrule
\textbf{sp05} & $3.1$K & Male & Adult & Psychiatrist & United States & Crime, Drama, Horror & \enquote{Before we begin, I must warn you... nothing here is vegetarian.} \\ \bottomrule
\end{tabular}}
\caption{Selected metadata regarding long-term speakers from \textsc{Anon} used in the experiment.}
\label{tab:zoo-speakers}
\end{table*}

\begin{table*}[h]
\centering
\scalebox{0.7}{
\begin{tabular}{rp{6cm}cp{7cm}}
\toprule
\textit{\textbf{ID}} & \textit{\textbf{Top-gaining sentence (4+ words)}} & \textit{\textbf{Five top-gaining tokens}} & \textit{\textbf{Top-losing sentence (4+ words)}} \\ \midrule
%& \textit{Five most degraded tokens} \\ \midrule
\textbf{sp01} & \enquote{Paranoid and can fit into small spaces.} & Okay, Wait, spy, Mom, mission & \enquote{To teach and to lend a guiding hand.} \\ \midrule
%& He, come, b, d, family \\ \midrule
\textbf{sp02} & \enquote{Fucking nuns! Fucking shit!} & Fuck, Shit, Fucking, fucking, fuck & \enquote{English, Math and French.} \\ \midrule
%& through, But, The, -, time \\ \midrule
\textbf{sp03} & \enquote{I love this car.} & Wow, ital, coffee, brain, b & \enquote{I'm not opposed to doing things to my teeth.}  \\ \midrule
%& Gold, bit, on, not, al \\ \midrule
\textbf{sp04} & \enquote{One missing kid's a boy.} & killer, Jack, kill, close, life & \enquote{She was a slim and delicate pig.} \\ \midrule
%& on, Freddie, get, put, gonna \\ \midrule
\textbf{sp05} & \enquote{Is your conscience clear?} & got, killer, Will, Jack, Ab & \enquote{Simpler times in boatyards with dad.}  \\ \bottomrule
%& just, that, we, G, know \\ \bottomrule
\end{tabular}}
\caption{Sentences and tokens for which the log probability under \textsc{LMCue} ($\mathcal{S}+\mathcal{P}$) changes the most compared \textsc{base-LM}.}
\label{tab:samples}
\end{table*}

\begin{table}[h]
\centering
% \scalebox{0.7}{
\resizebox{\linewidth}{!}{
\begin{tabular}{r|c|ccccc|c}
\toprule
& \multirow{2}{*}{\textsc{sMRR}$^\uparrow$} & \multicolumn{6}{c}{\textsc{ppl}$^\downarrow$} \\
& & \textbf{sp01} & \textbf{sp02} & \textbf{sp03} & \textbf{sp04} & \textbf{sp05} & \textbf{Mean} \\ \midrule
\textit{\textbf{baselines}} &  &  &  &  &  &  & \\
\textit{\textbf{non-context}} &  &  &  &  &  &  & \\
% \textsc{base-LM} & $91$M & $0.2$ & $17.62$ & $19.33$ & $25.21$ & $20.47$ & $23.60$ & $21.25$ \\
\textsc{base-LM} & $0.2$ & $15.24$ & $17.39$ & $23.53$ & $18.64$ & $21.14$ & $19.19$ \\
\textit{\textbf{fine-tuning}} &  &  &  &  &  &  & \\
% \textsc{SpFineTune} & $91$M & $\textbf{1.0}$ & $15.07$ & $16.14$ & $21.14$ & $17.50$ & $20.10$ & $17.99$\\
\textsc{SpFineTune} & $\textbf{1.0}$ & $14.54$ & $\textbf{16.01}$ & $21.76$ & $17.36$ & $\underline{19.50}$ & $\textbf{17.84}$\\
% \textsc{Lerp} & $91$M & $\textbf{1.0}$ & $15.05$ & $16.85$ & $22.16$ & $18.12$ & $20.56$ & $18.55$ \\
\textsc{Lerp} & $\textbf{1.0}$ & $\textbf{14.35}$ & $16.31$ & $22.25$ & $17.66$ & $19.91$ & $18.10$ \\ \midrule
\textit{\textbf{proposed}} &  &  &  &  &  & &  \\
\textit{\textbf{metadata-based}} &  &  &  &  &  & &  \\
\textsc{LMCue} ($\mathcal{S}$) & $\textbf{1.0}$ & $14.99$ & $16.75$ & $21.86$ & $17.54$ & $19.89$ & $18.21$ \\
\textsc{LMCue} ($\mathcal{P}$) & $0.8$ & $14.68$ & $17.17$ & $\underline{21.26}$ & $\underline{17.12}$ & $\textbf{19.45}$ & $17.94$ \\
\textsc{LMCue} ($\mathcal{S}+\mathcal{P}$) & $\textbf{1.0}$ & $14.77$ & $16.77$ & $\textbf{21.22}$ & $\textbf{17.10}$ & $\underline{19.47}$ & $\underline{17.87}$ \\ \bottomrule
\end{tabular}}
\caption{Results on the test set for long-term speakers. \underline{Underlined} results are on par with results in \textbf{bold}.}
\label{tab:res-long-term}
\end{table}


To determine the effectiveness of our context-based adaptation approach when compared to LMs fine-tuned on the available speaker dialogue, we focus our setup on five long-term (multi-episode TV) speakers with at least $3,000$ lines of dialogue sources from the \textsc{ZOO} corpus (\autoref{tab:zoo-speakers}). For each speaker, we use $400$ and $600$ of these lines for validation and testing, respectively. Within this experiment, we use \textsc{SpFineTune} and \textsc{Lerp} as baselines. We obtain \textsc{SpFineTune} by fine-tuning the LM on all \textsc{ZOO} data initially (FT1), and then once more on speaker data alone. \textsc{Lerp} is a mean interpolation of \textsc{SpFineTune} with FT1. We do this individually for each speaker $\in \{{\textbf{sp01}, ..., \textbf{sp05}}\}$.
%As per the results in \autoref{tab:res-long-term}, 
\textsc{LMCue} achieves results comparable to all speaker-fine-tuned models (\autoref{tab:res-long-term}). When using speaker metadata ($\mathcal{S}$), \textsc{LMCue} achieves \textsc{sMRR} of $1.0$ just like fine-tuned models, suggesting the perplexity improvements come from the model's context-based predictions. \textsc{LMCue} ($\mathcal{P}$) achieves lower \textsc{sMRR} ($0.8$): its predictions are based only on production metadata, not considering that two different characters may come from the same production. Speaker profiles $\mathcal{S}$ are necessary for full speaker adaptation.

Any adapted model, whether fine-tuned or metadata-based, yields a reduction in perplexity between $5.1\%$ and $6.8\%$ which is comparable to results on \texttt{test}. \textsc{SpFineTune} achieves the best overall perplexity reduction of $1.32$ and $1.0$ \textsc{sMRR}, with \textsc{LMCue} ($\mathcal{S}+\mathcal{P}$) yielding a statistically comparable reduction of $1.29$ and the same \textsc{sMRR} while requiring (i) no fine-tuning and (ii) the maintenance of only one model for all speakers. 

To illustrate how personalisation manifests in practice, we identify the predictions of \textsc{LMCue} ($\mathcal{S}+\mathcal{P}$) with the most increased and decreased log probability compared to \textsc{base-LM} (compare \autoref{tab:zoo-speakers} and \autoref{tab:samples}). Top-gaining tokens have strong associations with certain categories, like \textit{profession} (\textbf{sp01} \enquote{Student, Spy} $\rightarrow$ \textit{spy, Mom, mission}), \textit{age} (\textbf{sp02} \enquote{Young Adult} $\rightarrow$ expletives) or genre (\textbf{sp04, sp05} \enquote{Crime, Drama, Horror} $\rightarrow$ \textit{killer}). Similarly, top-gaining sentences for \textbf{sp01} and \textbf{sp02} have a comedic overtone (matching the genre), while the top-losing sentences do not fit these characters' demographic profiles.

\subsection{Zero-shot Transfer to Unseen Speakers} \label{rq2}
In this section, we assess the effectiveness of speaker adaptation for completely \textbf{new test speakers} featured in the \texttt{test\_unseen} sets of both corpora. To reiterate, these speakers' dialogue is excluded from training and validation data (although there are overlaps in production metadata). As before, we fine-tune the pre-trained \textsc{LMCue} on the \texttt{train} splits. We compare the performance only to \textsc{base-LM} since other baselines are not equipped to work well in this zero-shot scenario.


\begin{table}[h]
\centering
\scalebox{0.7}{
\begin{tabular}{rcccc}
\toprule
 & \multicolumn{2}{c}{\textsc{Cornell-rich}} & \multicolumn{2}{c}{\textsc{Anon}} \\
 & \multicolumn{2}{c}{\texttt{test\_unseen}} & \multicolumn{2}{c}{\texttt{test\_unseen}} \\ \midrule
Approach & \textsc{ppl}$^\downarrow$ & \textsc{sMRR}$^\uparrow$ & \textsc{ppl}$^\downarrow$ & \textsc{sMRR}$^\uparrow$ \\ \midrule
\textit{\textbf{baselines}} & & & & \\
% \textsc{base-LM} ($91$M) & $23.57$ & $0.03$ & $17.32$ & $0.09$ \\
\textsc{base-LM} & $23.62$ & $0.03$ & $17.11$ & $0.09$ \\ \midrule
\textit{\textbf{proposed}} & & & & \\
\textsc{LMCue} ($\mathcal{S}$) & $23.28$ & $0.70$ & $16.49$ & $\textbf{0.39}$ \\
\textsc{LMCue} ($\mathcal{P}$) & $\textbf{22.00}$ & $0.80$ & $\textbf{16.21}$ & $0.19$ \\
\textsc{LMCue} ($\mathcal{S} + \mathcal{P}$) & $22.31$ & $\textbf{0.96}$ & $16.35$ & $0.32$ \\
 \bottomrule
\end{tabular}}
\caption{Results of evaluation with speaker \& film metadata on the test set of unseen speakers.}
\label{tab:results-unseen}
\end{table}

Results for this scenario reported in \autoref{tab:results-unseen} show that \textsc{LMCue} ($\mathcal{S}$) still improves perplexity over a parameter-matched LM. Though these improvements are smaller than in the supervised scenario, they are still significant, especially for \textsc{ZOO} ($-0.62$). More importantly, for both corpora $\mathcal{S}$ is strongly beneficial towards high speaker separation (i.e. the model assigns the highest probability to dialogue which matches the given speaker's profile), as measured by \textsc{sMRR}. Perplexity does improve more when $\mathcal{P}$ is also used ($1.4 \rightarrow 5.6\%$ for \textsc{Cornell-rich}, $3.6 \rightarrow 4.4\%$ for \textsc{ZOO}), though in this scenario we are evaluating the easier task of modelling new speakers in seen or unseen productions. $\mathcal{P}$roduction metadata alone yields the best reduction of $6.9/5.3\%$. Using it has a different effect on the two test sets: first, in \textsc{Cornell-rich} it induces a stronger boost in \textsc{sMRR} than $\mathcal{S}$ ($+0.08$), while in \textsc{ZOO} it decreases it considerably ($-0.20$); second, using it in conjunction with $\mathcal{S}$ results in best \textsc{sMRR} in \textsc{Cornell} ($0.94$), but not so for \textsc{ZOO}. This can be explained by the fact that \textsc{ZOO} uses a pool of only nine productions (vs $595$ in \textsc{Cornell-rich}), so adding $\mathcal{P}$ on top of $\mathcal{S}$ is unlikely to increase speaker separation. In contrast, \textsc{Cornell-rich} uses a rich pool of films, so film metadata is more likely to be unique between any two speakers, thus introducing it separates the two speakers even more, increasing \textsc{sMRR}. This is also why \textsc{sMRR} is so high for \textsc{LMCue} ($\mathcal{P}$): with $24$ unique films between the $30$ speakers the film metadata is rarely shared between any two speakers, making their context inputs more dissimilar. The magnitude of improvements in \textsc{sMRR} is also different for the two corpora, which again could be attributed to scale ($863$ vs $159$ speakers, $595$ vs $9$ productions). Increasing the number of annotated entities can therefore improve the personalisation effect. Nevertheless, a score of $0.39$ still suggests that \textsc{LMCue} ranks an unseen character on the $2.56$th position with a model built from their demographic profile, on average.

Using \textsc{LMCue} ($\mathcal{S}+\mathcal{P}$), we queried the words for which log probability increased the most w.r.t. \textsc{base-LM} in the \texttt{test} set of \textsc{Cornell-rich} and obtained a list of the following fifteen tokens:
\begin{center}
    \textit{crew shark ship azz birds casino space leads \\power ocean camp boat cops baby ace}
\end{center}
Many of these tokens are context-specific and would only appear in certain scenarios or domains. For example, \textit{casino} or \textit{space} are unlikely to appear in a sentence unless they represent locations within the film. A subset of the provided tokens (\textit{crew, shark, ship, ocean, birds}) may also collectively describe a single scenario, such as an adventure or thriller film set on a ship in the middle of an ocean. We hypothesise that a few such films appeared in the training set \textsc{Cornell-rich}, allowing \textsc{LMCue} to develop a strong prior for predicting these tokens when metadata of similar films is provided as input. Finally, these tokens are notably more generic than those in \autoref{tab:res-long-term}: we observe that the effect of biasing speaker-specific vocabulary may be limited for some tokens compared to the supervised scenario (e.g. tokens representing names of the character's co-stars are not related to demographic features so would not be affected in a zero-shot scenario).

\subsection{Cost-benefit analysis of human annotations} \label{rq3}
Granular manual annotations are costly to obtain. Cost-benefit analysis helps avoid the misallocation of limited annotation funding and resources. This section presents the results of the cost-benefit analysis we conducted to show which individual speaker attributes produce the most \textbf{benefit} (reduction in perplexity) w.r.t. the perceived \textbf{cost} of producing them.

We asked the two human annotators to assess the effort required for the annotation task using three metrics on a Likert scale of 1 to 10: \textit{access} (how difficult it was to find information), \textit{credibility} (how confident they were in the accuracy/usefulness of the information), and \textit{time} (how much time was needed relative to other fields). We took the mean of both annotators' scores after reversing credibility ($C = 10 - C + 1$). We then conducted a simple experiment to measure the \textbf{benefit} of each metadata type by fine-tuning the pre-trained \textsc{LMCue} on each speaker metadata type evaluated individually. Finally, we measured the reduction in perplexity from including this information (\autoref{tacl:cost-benefit-abl}) compared to the $91$M parameter decoder in \textsc{LMCue}, since that is the decoder we are trying to improve with context.

\begin{figure}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{img/cost_benefit.png}}
    \caption{Perplexity reduction from training \mbox{\textsc{LMCue}} with individual speaker attributes.}
    \label{tacl:cost-benefit-abl}
\end{figure}

The figure suggests that \textit{description}, \textit{profession} and \textit{quote} yield the greatest perplexity reduction in both datasets, around $5$ to $6\%$. \textit{Description}, the best-performing attribute, alone achieves $88.7/91.9\%$ of the perplexity reduction of \textsc{LMCue} $(\mathcal{S})$. On the other hand, \textit{age bracket}, \textit{religion} and \textit{country of origin} yield the smallest improvements, and a better improvement can be achieved with the parameter-matched \textsc{base-LM}. For \textsc{Cornell-rich}, they still help marginally ($1$ to $2\%$), while for \textsc{ZOO} improvements from \textit{age} and \textit{religion} are negligible.

This analysis suggests why \citet{king-cook-2020-evaluating}, who implemented context-based adaptation using only \textit{age} and \textit{gender}, found it inferior to other methods; we found other variables such as \textit{description} to be significantly more useful.

Other findings of the analysis are consistent among the two corpora. \textit{Profession}, collected at a relatively small cost, is one of the top-3 attributes for both, hence the most cost-effective. \textit{Religion} is the least cost-effective attribute, requiring the most effort but producing the least benefit. Finally, experimental attributes such as characteristic \textit{quote}s and \textit{additional information}\footnote{Since \textit{additional information} was not collected for \textsc{ZOO}, it is not present on the \textsc{ZOO} plot of \autoref{tacl:cost-benefit-abl}.}
have been shown to be useful, the latter placing in the middle of the ranking whilst the former is on par with the best attribute for \textsc{Cornell-rich}. 

\subsection{Past dialogue as proxy for metadata} \label{tacl:pretraining}
This supplementary section presents empirical evidence that past dialogue can be used as a proxy for fine-tuning \textsc{LMCue} on speaker or production metadata. %We present the results of adapting different pre-trained models. 
When fine-tuning, we use both $\mathcal{S}$peaker and $\mathcal{P}$roduction metadata. We report performance on \texttt{test\_unseen} to also present \textsc{sMRR} scores.

\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{r|cc|c|cc}
\toprule
 & \multicolumn{2}{c|}{Pre-training} & \multirow{2}{*}{Fine-tune} & \multirow{2}{*}{\textsc{ppl}$^\downarrow$} & \multirow{2}{*}{\textsc{sMRR}$^\uparrow$} \\
 & \xmark\hspace{.4em}/ \cmark & Context & & & \\ \midrule
 \textsc{base-LM} & \cmark & $-$ & \xmark & $\textbf{28.78}$ & $0.03$ \\
\textsc{LMCue} & \cmark & dialogue & \xmark & $37.19$ & $0.29$ \\
 \textsc{LMCue} & \cmark & metadata & \xmark & $30.95$ & $\textbf{0.43}$ \\ \midrule
\textsc{base-LM} & \xmark & $-$ & \cmark & $\textbf{39.60}$ & $0.03$ \\ 
\textsc{LMCue} & \xmark & $-$ & \cmark & $51.14$ & $0.03$ \\ \midrule
\textsc{base-LM} & \cmark & $-$ & \cmark & $23.62$ & $0.03$ \\
\textsc{LMCue} & \cmark & dialogue & \cmark & $\textbf{22.31}$ & $\textbf{0.96}$ \\
 \textsc{LMCue} & \cmark & metadata & \cmark & $22.71$ & $0.89$ \\ \bottomrule
\end{tabular}}
\caption{Results on \texttt{test\_unseen} of \textsc{Cornell-rich} from different pre-training/fine-tuning setups. New results (top 5 rows) come from single runs.}
\label{tab:pre-training}
\end{table}


As per \autoref{tab:pre-training}, pre-training on \textsc{OpenSubtitles} leads to best perplexity when no context is used (\textsc{base-LM}), however using context yields improvements in \textsc{sMRR}, and these are stronger when metadata is used instead of dialogue. Similarly, without pre-training we also obtain the best perplexity with \textsc{base-LM}; here even \textsc{sMRR} remains at a baseline level, i.e. the contextual model fails to learn contextual dependencies correctly. Metadata only leads to superior results when both pre-training and fine-tuning are included in the pipeline. Interestingly, %in our case 
pre-training on dialogue yielded the best results, though pre-training on metadata is not far behind ($+0.4$ \textsc{ppl}, $-0.07$ \textsc{sMRR}). We hypothesise that since past dialogue is much more diverse than film metadata (which contains many repeated fields), it is overall the better pre-training proxy for fine-tuning on new types of metadata, such as speaker profiles. For applications on other datasets, we therefore recommend pre-training on a similar dataset (domain-wise) with access to document-level information.

\section{Conclusions} \label{tacl:conclusions}
This work has introduced a set of rich speaker and production annotations for a dialogue dataset, \textsc{Cornell-rich}, and showed that such metadata can be used to build personalised LMs which adapt the predicted language to the input context and work on par with expensive fine-tuning methods. 

\researchquestion{RQ1} {How can rich character profiles be used to model the characters' speaking styles? (\S \ref{rq1})}
%todo the answer doesnt actually answer the question
The \textsc{LMCue} architecture can be trained to exhibit personalisation based on speaker context: \textsc{LMCue} ($\mathcal{S}$) reduces perplexity by $\textbf{4.3/4.7\%}$ compared to a parameter-matched general LM, or by $\textbf{5.4/6.5\%}$ when $\mathcal{P}$roduction data is also used. In a few-shot scenario, \textsc{LMCue} ($\mathcal{S}+\mathcal{P}$) is better than linear interpolation \textsc{Lerp} and comparable with speaker-specific fine-tuning (\textsc{SpFineTune}). Since \textsc{LMCue} requires no fine-tuning to specific speakers, it is favourable if metadata is available.

\researchquestion{RQ2} {How can a LM be personalised for a specific character solely by learning from data for characters with similar profiles? (\S \ref{rq2})}

On a test set with unseen speakers, personalisation using speaker profiles with \textsc{LMCue} yields a high speaker separation effect (models assign the highest probability to dialogue which matches the given speaker's profile). Using both $\mathcal{S}$ and $\mathcal{P}$ metadata reduces perplexity by $\textbf{5.6/4.4\%}$, a similar magnitude to that for seen speakers ($5.4/6.5\%$), suggesting that our method scales robustly to this scenario, unlike speaker-specific fine-tuning which cannot be applied on new characters. Having a varied pool of speakers and productions in training data correlates positively with \textsc{sMRR}.

\researchquestion{RQ3} {Which character metadata are the most cost-effective for personalisation? (\S \ref{rq3})}
Textual metadata (descriptions, quotes, professions) is significantly more useful for personalisation with \textsc{LMCue} than discrete metadata (e.g. age bracket). Particularly in our evaluation, descriptions alone achieve results on par with using the entire speaker profile. Furthermore, the utility of individual attributes seems to be positively correlated with the diversity in their representation.
% \section*{Acknowledgements}
% We would like to thank Robert Flynn for his useful comments on the paper. This work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]. This work was also supported by ZOO Digital.

\bibliography{anthology,mendeley,custom}
\bibliographystyle{acl_natbib}

\end{document}


