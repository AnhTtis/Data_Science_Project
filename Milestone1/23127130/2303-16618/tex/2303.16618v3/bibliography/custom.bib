
@phdthesis{vincent2023thesis,
    title        = {Context-Based Personalisation in Neural Machine Translation of Dialogue},
    author       = {Sebastian Vincent},
    year         = {2023},
    month        = {September},
    address      = {Sheffield, UK},
    note         = {},
    url          = {https://etheses.whiterose.ac.uk/34022/},
    school       = {University of Sheffield},
    type         = {Ph{D} thesis},
    keywords     = {context, neural machine translation, natural language processing, personalisation,  personalization, metadata, subtitles, dialogue, scripted dialogue, tv series, context-aware machine translation, extra-textual information},
}


@article{diverse-beam-2018, title={Diverse Beam Search for Improved Description of Complex Scenes}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/12340}, DOI={10.1609/aaai.v32i1.12340}, abstractNote={ &lt;p&gt; A single image captures the appearance and position of multiple entities in a scene as well as their complex interactions. As a consequence, natural language grounded in visual contexts tends to be diverse---with utterances differing as focus shifts to specific objects, interactions, or levels of detail. Recently, neural sequence models such as RNNs and LSTMs have been employed to produce visually-grounded language. Beam Search, the standard work-horse for decoding sequences from these models, is an approximate inference algorithm that decodes the top-B sequences in a greedy left-to-right fashion. In practice, the resulting sequences are often minor rewordings of a common utterance, failing to capture the multimodal nature of source images. To address this shortcoming, we propose Diverse Beam Search (DBS), a diversity promoting alternative to BS for approximate inference. DBS produces sequences that are significantly different from each other by incorporating diversity constraints within groups of candidate sequences during decoding; moreover, it achieves this with minimal computational or memory overhead. We demonstrate that our method improves both diversity and quality of decoded sequences over existing techniques on two visually-grounded language generation tasks---image captioning and visual question generation---particularly on complex scenes containing diverse visual content. We also show similar improvements at language-only machine translation tasks, highlighting the generality of our approach. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Vijayakumar, Ashwin and Cogswell, Michael and Selvaraju, Ramprasaath and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv}, year={2018}, month={Apr.} }

@inproceedings{shen-etal-2019-mixture,
  author    = {Tianxiao Shen and
               Myle Ott and
               Michael Auli and
               Marc'Aurelio Ranzato},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Mixture Models for Diverse Machine Translation: Tricks of the Trade},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {5719--5728},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/shen19c.html},
  timestamp = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/ShenOAR19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{pmlr-v97-cohen19a,
  title = 	 {Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models},
  author =       {Cohen, Eldan and Beck, Christopher},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1290--1299},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/cohen19a/cohen19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/cohen19a.html},
  abstract = 	 {Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, work on a number of applications has found that the quality of the highest probability hypothesis found by beam search degrades with large beam widths. We perform an empirical study of the behavior of beam search across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early, very low probability tokens that are followed by a sequence of tokens with higher (conditional) probability. We show that, empirically, such sequences are more likely to have a lower evaluation score than lower probability sequences without this pattern. Using the notion of search discrepancies from heuristic search, we hypothesize that large discrepancies are the cause of the performance degradation. We show that this hypothesis generalizes the previous ones in machine translation and image captioning. To validate our hypothesis, we show that constraining beam search to avoid large discrepancies eliminates the performance degradation.}
}

@misc{p-transformer,
  doi = {10.48550/ARXIV.2212.05830},
  
  url = {https://arxiv.org/abs/2212.05830},
  
  author = {Li, Yachao and Li, Junhui and Jiang, Jing and Tao, Shimin and Yang, Hao and Zhang, Min},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {P-Transformer: Towards Better Document-to-Document Neural Machine Translation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{
liu2020roberta,
title={Ro{\{}BERT{\}}a: A Robustly Optimized {\{}BERT{\}} Pretraining Approach},
author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
year={2020},
url={https://openreview.net/forum?id=SyxS0T4tvS}
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{press-etal-2021-shortformer,
    title = "Shortformer: Better Language Modeling using Shorter Inputs",
    author = "Press, Ofir  and
      Smith, Noah A.  and
      Lewis, Mike",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.427",
    doi = "10.18653/v1/2021.acl-long.427",
    pages = "5493--5505",
    abstract = "Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters.",
}

@article{o2021context,
  title={What Context Features Can Transformer Language Models Use?},
  author={O'Connor, Joe and Andreas, Jacob},
  journal={arXiv preprint arXiv:2106.08367},
  year={2021}
}

@inproceedings{reimers-2019-sentence-bert,
  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2019",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/1908.10084",
}
@inproceedings{reimers-2020-multilingual-sentence-bert,
  title = "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2020",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/2004.09813",
}
@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}


@inproceedings{wang-etal-2020-minilm,
author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
title = {MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Pre-trained language models (e.g., BERT [12] and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer [42] based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant [26] also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99\% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50\% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {485},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@Inproceedings{Currey2022,
 author = {Anna Currey and Maria Nadejde and Raghavendra Pappagari and Mia Mayer and Stanislas LAULY and Xing Niu and Benjamin Hsu and Georgiana Dinu},
 title = {MT-GenEval: A counterfactual and contextual dataset for evaluating gender accuracy in machine translation},
 year = {2022},
 url = {https://www.amazon.science/publications/mt-geneval-a-counterfactual-and-contextual-dataset-for-evaluating-gender-accuracy-in-machine-translation},
 booktitle = {EMNLP 2022},
}
@inproceedings{mindthegap,
 author = {Lazaridou, Angeliki and Kuncoro, Adhi and Gribovskaya, Elena and Agrawal, Devang and Liska, Adam and Terzi, Tayfun and Gimenez, Mai and de Masson d\textquotesingle Autume, Cyprien and Kocisky, Tomas and Ruder, Sebastian and Yogatama, Dani and Cao, Kris and Young, Susannah and Blunsom, Phil},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {29348--29363},
 publisher = {Curran Associates, Inc.},
 title = {Mind the Gap: Assessing Temporal Generalization in Neural Language Models},
 url = {https://proceedings.neurips.cc/paper/2021/file/f5bf0ba0a17ef18f9607774722f5698c-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{dingliwal22_interspeech,
  author={Saket Dingliwal and Ashish Shenoy and Sravan Bodapati and Ankur Gandhe and Ravi Teja Gadde and Katrin Kirchhoff},
  title={{Domain Prompts: Towards memory and compute efficient domain adaptation of ASR systems}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={684--688},
  doi={10.21437/Interspeech.2022-824}
}

@inproceedings{press-etal-2021-shortformer,
    title = "Shortformer: Better Language Modeling using Shorter Inputs",
    author = "Press, Ofir  and
      Smith, Noah A.  and
      Lewis, Mike",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.427",
    doi = "10.18653/v1/2021.acl-long.427",
    pages = "5493--5505",
    abstract = "Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters.",
}

@inproceedings{Shenoy_2021_asr,
	doi = {10.21437/interspeech.2021-1849},
  
	url = {https://doi.org/10.21437%2Finterspeech.2021-1849},
  
	year = 2021,
	month = {aug},
  
	publisher = {{ISCA}
},
  
	author = {Ashish Shenoy and Sravan Bodapati and Monica Sunkara and Srikanth Ronanki and Katrin Kirchhoff},
  
	title = {Adapting Long Context {NLM} for {ASR} Rescoring in Conversational Agents},
  
	booktitle = {Interspeech 2021}
}

@misc{umap,
  doi = {10.48550/ARXIV.1802.03426},
  
  url = {https://arxiv.org/abs/1802.03426},
  
  author = {McInnes, Leland and Healy, John and Melville, James},
  
  keywords = {Machine Learning (stat.ML), Computational Geometry (cs.CG), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{vincent2023personalised,
      title={Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations}, 
      author={Sebastian Vincent and Rowanne Sumner and Alice Dowek and Charlotte Blundell and Emily Preston and Chris Bayliss and Chris Oakley and Carolina Scarton},
      year={2023},
      eprint={2303.16618},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2022ptransformer,
      title={P-Transformer: Towards Better Document-to-Document Neural Machine Translation}, 
      author={Yachao Li and Junhui Li and Jing Jiang and Shimin Tao and Hao Yang and Min Zhang},
      year={2022},
      eprint={2212.05830},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InCollection{sep-bayes-theorem,
	author       =	{Joyce, James},
	title        =	{{Bayes’ Theorem}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/fall2021/entries/bayes-theorem/}},
	year         =	{2021},
	edition      =	{{F}all 2021},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@article{giles2007communication,
  title={Communication accommodation theory},
  author={Giles, Howard and Ogay, Tania and others},
  year={2007},
  publisher={na}
}

@inproceedings{schein-etal-2002-cold,
author = {Schein, Andrew I. and Popescul, Alexandrin and Ungar, Lyle H. and Pennock, David M.},
title = {Methods and Metrics for Cold-Start Recommendations},
year = {2002},
isbn = {1581135610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564376.564421},
doi = {10.1145/564376.564421},
abstract = {We have developed a method for recommending items that combines content and collaborative data under a single probabilistic framework. We benchmark our algorithm against a na\"{\i}ve Bayes classifier on the cold-start problem, where we wish to recommend items that no one in the community has yet rated. We systematically explore three testing methodologies using a publicly available data set, and explain how these methods apply to specific real-world applications. We advocate heuristic recommenders when benchmarking to give competent baseline performance. We introduce a new performance metric, the CROC curve, and demonstrate empirically that the various components of our testing strategy combine to obtain deeper understanding of the performance characteristics of recommender systems. Though the emphasis of our testing is on cold-start recommending, our methods for recommending and evaluation are general.},
booktitle = {Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {253–260},
numpages = {8},
keywords = {recommender systems, information retrieval, collaborative filtering, content-based filtering, performance evaluation, graphical models},
location = {Tampere, Finland},
series = {SIGIR '02}
}


@Article{guo-etal-2021-aerospace,
AUTHOR = {Guo, Dongyue and Zhang, Zichen and Fan, Peng and Zhang, Jianwei and Yang, Bo},
TITLE = {A Context-Aware Language Model to Improve the Speech Recognition in Air Traffic Control},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {348},
URL = {https://www.mdpi.com/2226-4310/8/11/348},
ISSN = {2226-4310},
ABSTRACT = {Recognizing isolated digits of the flight callsign is an important and challenging task for automatic speech recognition (ASR) in air traffic control (ATC). Fortunately, the flight callsign is a kind of prior ATC knowledge and is available from dynamic contextual information. In this work, we attempt to utilize this prior knowledge to improve the performance of the callsign identification by integrating it into the language model (LM). The proposed approach is named context-aware language model (CALM), which can be applied for both the ASR decoding and rescoring phase. The proposed model is implemented with an encoder–decoder architecture, in which an extra context encoder is proposed to consider the contextual information. A shared embedding layer is designed to capture the correlations between the ASR text and contextual information. The context attention is introduced to learn discriminative representations to support the decoder module. Finally, the proposed approach is validated with an end-to-end ASR model on a multilingual real-world corpus (ATCSpeech). Experimental results demonstrate that the proposed CALM outperforms other baselines for both the ASR and callsign identification task, and can be practically migrated to a real-time environment.},
DOI = {10.3390/aerospace8110348}
}


@incollection{CHARTRAND2009219,
title = {Chapter 5 Human Mimicry},
series = {Advances in Experimental Social Psychology},
publisher = {Academic Press},
volume = {41},
pages = {219-274},
year = {2009},
issn = {0065-2601},
doi = {https://doi.org/10.1016/S0065-2601(08)00405-X},
url = {https://www.sciencedirect.com/science/article/pii/S006526010800405X},
author = {Tanya L. Chartrand and Rick {van Baaren}},
abstract = {Human mimicry is ubiquitous, and often occurs without the awareness of the person mimicking or the person being mimicked. First, we briefly describe some of the major types of nonconscious mimicry—verbal, facial, emotional, and behavioral—and review the evidence for their automaticity. Next, we argue for the broad impact of mimicry and summarize the literature documenting its influence on the mimicry dyad and beyond. This review highlights the moderators of mimicry as well, including the social, motivational, and emotional conditions that foster or inhibit automatic mimicry. We interpret these findings in light of current theories of mimicry. First, we evaluate the evidence for and against mimicry as a communication tool. Second, we review neuropsychological research that sheds light on the question of how we mimic. What is the cognitive architecture that enables us to do what we perceive others do? We discuss a proposed system, the perception‐behavior link, and the neurological evidence (i.e., the mirror system) supporting it. We will then review the debate on whether mimicry is innate and inevitable. We propose that the architecture enabling mimicry is innate, but that the behavioral mimicry response may actually be (partly) a product of learning or associations. Finally, we speculate on what the behavioral data on mimicry may imply for the evolution of mimicry.}
}

@inproceedings{Sanh2019,
archivePrefix = {arXiv},
arxivId = {1910.01108},
author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
booktitle = {5th Workshop on Energy Efficient Machine Learning and Cognitive Computing at NeurIPS 2019},
eprint = {1910.01108},
keywords = {model},
title = {{DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}},
url = {http://arxiv.org/abs/1910.01108},
year = {2019}
}

@inproceedings{wang-etal-2020-minilm,
author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
title = {MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Pre-trained language models (e.g., BERT [12] and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer [42] based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant [26] also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99\% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50\% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {485},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@article{milburn-2004-speech,
author = {Trudy Milburn},
title = {Speech Community: Reflections Upon Communication},
journal = {Annals of the International Communication Association},
volume = {28},
number = {1},
pages = {411-441},
year  = {2004},
publisher = {Routledge},
doi = {10.1080/23808985.2004.11679041},
URL = {https://doi.org/10.1080/23808985.2004.11679041},
eprint = {https://doi.org/10.1080/23808985.2004.11679041}
}


@InProceedings{pmlr-v70-guo17a,
  title = 	 {On Calibration of Modern Neural Networks},
  author =       {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1321--1330},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/guo17a/guo17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/guo17a.html},
  abstract = 	 {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.}
}

% Catastrophic forgetting
@article{Ratcliff1990ConnectionistMO,
  title={Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.},
  author={Roger Ratcliff},
  journal={Psychological review},
  year={1990},
  volume={97 2},
  pages={285-308}
}

@article{MCCLOSKEY1989109,
title = "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem",
abstract = "Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.",
author = "Michael McCloskey and Cohen, {Neal J.}",
note = "Funding Information: The research reported in this chapter was supported by NIH grant NS21047 to Michael McCloskey, and by a grant from the Sloan Foundation to Neal Cohen. We thank Sean Purcell and Andrew Olson for assistance in generating the figures, and Alfonso Caramazza, Walter Harley, Paul Macaruso, Jay McClelland, Andrew Olson, Brenda Rapp, Roger Rat-cliff, David Rumelhart, and Terry Sejnowski for helpful discussions.",
year = "1989",
month = jan,
day = "1",
doi = "10.1016/S0079-7421(08)60536-8",
language = "English (US)",
volume = "24",
pages = "109--165",
journal = "Psychology of Learning and Motivation - Advances in Research and Theory",
issn = "0079-7421",
publisher = "Academic Press Inc.",
number = "C",

}

@inproceedings{Adam2014,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tiedemann-2012-parallel,
    title = "Parallel Data, Tools and Interfaces in {OPUS}",
    author = {Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/463\_Paper.pdf",
    pages = "2214--2218",
    abstract = "This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project.",
}
@misc{post2023escaping,
      title={Escaping the sentence-level paradigm in machine translation}, 
      author={Matt Post and Marcin Junczys-Dowmunt},
      year={2023},
      eprint={2304.12959},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{jie2019,
author = {Jie Huang and Jianhua Wang},
title = {Post-editing machine translated subtitles: examining the effects of non-verbal input on student translators’ effort},
journal = {Perspectives},
volume = {31},
number = {4},
pages = {620-640},
year  = {2023},
publisher = {Routledge},
doi = {10.1080/0907676X.2022.2026424},
URL = { 
    
        https://doi.org/10.1080/0907676X.2022.2026424
},
eprint = { 
    
        https://doi.org/10.1080/0907676X.2022.2026424 
}
}

@article{gupta-2019-subtitles,
  author       = {Prabhakar Gupta and
                  Mayank Sharma and
                  Kartik Pitale and
                  Keshav Kumar},
  title        = {Problems with automating translation of movie/TV show subtitles},
  journal      = {CoRR},
  volume       = {abs/1909.05362},
  year         = {2019},
  url          = {http://arxiv.org/abs/1909.05362},
  eprinttype    = {arXiv},
  eprint       = {1909.05362},
  timestamp    = {Tue, 17 Sep 2019 11:23:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-05362.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}