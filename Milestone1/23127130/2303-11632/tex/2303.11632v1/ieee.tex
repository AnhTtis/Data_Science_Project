% \documentclass[lettersize,journal]{IEEEtran}
% \usepackage{amsmath,amsfonts}
% \usepackage{algorithmic}
% \usepackage{array}
% \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
% \usepackage{textcomp}
% \usepackage{stfloats}
% \usepackage{url}
% \usepackage{verbatim}
% \usepackage{graphicx}
% \hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%     T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% \usepackage{balance}
% \usepackage{amsmath}

% \begin{document}
% \title{An Embarrassingly Simple Approach for Wafer Feature
% Extraction and Defect Pattern Recognition}


% \author{Nitish Shukla}

% \markboth{IEEE Transactions on Instrumentation & Measurement}

% \maketitle

% \begin{abstract}
% Identifying defect patterns in a wafer map during manufacturing is crucial to find the root cause of the underlying issue and provides valuable insights on improving yield in the foundry. Currently used methods use deep neural networks to identify the defects. These methods are generally very huge and have significant inference time. They also require GPU support to efficiently operate. All these issues make these models not fit for \textit{on-line} prediction in the manufacturing foundry. In this paper, we propose an extremely simple yet effective technique to extract features from wafer images. The proposed method is extremely fast, intuitive, and non-parametric while being explainable. The experiment results show that the proposed pipeline outperforms conventional deep learning models. Our feature extraction requires no training or fine-tuning while preserving the relative shape and location of data points as revealed by our interpretability analysis.
% \end{abstract}

% \begin{IEEEkeywords}
% defect pattern recognition, wafer map, dimensionality reduction
% \end{IEEEkeywords}


% \section{Introduction}

% The wafer manufacturing process consists of five basic steps: IC design, wafer manufacturing, wafer test, assembly, and final test\cite{ref-1}. After the fabrication process, each die in the wafer is tested and evaluated by probe test\cite{ref-2}. The probe test measures the performance of the individual dies to access the desired electric function. Only the fit dies are sent to the final test\cite{ref-3}. The testing results of all dies on a wafer are saved on a two-dimensional adjacent matrix\cite{ref-4}, known as a wafer map. By examining the distribution of dies that failed in wafer testing, defect pattern recognition (DPR) classifies the wafer into different groups. Correctly classifying defects provides valuable information for reasoning the root cause of the defects, thus facilitating process improvement and yielding industrial enhancement\cite{ref-5}.

% Traditionally, DPR was carried out manually by experienced SMEs and engineers. This piece-by-piece manual examination is both time-consuming and costly. Moreover, the ever-increasing complexity and quantity of defect patterns made it impossible for manual DPR to satisfy the system's requirement\cite{ref-6}. Therefore, the automation of DPR is required. Several machine learning methods have been used for the DPR of wafer defects. Analytical approaches like \cite{ref-7} utilized filtering and clustering to process the wafer maps. The proposed model then utilized a nonparametric Bayesian model to cluster various defect patterns after using a filtering procedure known as the connected-path filtering approach to amplify the local systematic patterns and eliminate the random patterns. Other techniques like SVM are also used \cite{ref-8,ref-9,ref-10} for wafer defect classification.
% \begin{figure}[!htp]
%     \centering
%     \includegraphics[scale=0.6]{Definitions/flowchart.drawio.pdf}
%     \caption{Flowchart of the DPR process}
%     \label{fig:flowchart}
% \end{figure}
% Recently, the advent of deep learning techniques consisting of multi-layer neural network\cite{ref-11,ref-12}   and convolution neural network(CNN)\cite{ref-13} have greatly benefited DPR. Kyong and Kim \cite{ref-14}  successfully applied the CNN architecture to classify wafer map patterns by constructing an individual classification model for each single defect pattern determining if the corresponding pattern exists when multiple defect patterns are present on a wafer. Deformable convolutional network (DC-net)\cite{ref-15} is another example of a deep learning model which was built to flexibly convolve the image as a further refinement to convolution. This enables selective image sampling and prevents the interference of irrelevant features. An adaptive balancing generative adversarial network (AdaBalGAN) was proposed by Wang et al.\cite{ref-16} to address the issue of data imbalance caused by the various wafer defect occurrence possibilities. The network has a generator that uses a deconvolutional neural network to balance out the missing data. Finally, in order to categorize the suggested defect patterns, it builds a discriminator model and a pattern recognition model. Both of the models used CNN for their downsampling. 

% Although very successful, deep learning models suffer due to their huge size which makes them unusable on embedded devices typically used in fabrication foundries. Generally, these deep learning models are deployed on cloud which adds to the inference time. Additionally, deep learning methods are inherently black-box\cite{ref-17} i.e. they provide no evidence for the output, have long inference time, and have high operational cost. All these can be overcome by using non-deep learning methods like decision trees, SVM, etc., but the accuracy achieved by deep-learning approaches is generally very high. On the other hand, shallow structured networks like SVM require intricate feature extraction algorithms before the classification, which can be complex and rigid. 

% \begin{figure*}[!htp]
%     \centering
%     \includegraphics[scale=0.4]{Definitions/One-defect.eps}
%     \caption{Unique defects in the MixedWM38 dataset.}
%     \label{fig:defects}
% \end{figure*}

% Motivated by the issues mentioned above, we propose an elegant scheme to extract features from the wafer maps. The technique is highly intuitive, extremely fast, and embarrassingly simple. Our experimental results clearly show that our model can recognize single defect patterns with an average accuracy of $96.79\%$. Overall, our contributions are as follows:
% \begin{itemize}
%     \item This paper proposes a simple feature extraction technique suitable for a single DPR and empirically proves that it can be used as a powerful tool for defect detection.
%     \item The proposed technique weighs the regions containing defects over non-defective regions to produce a feature vector encapsulating the information. Moreover, it is easily generalizable to wafers of any shape.
%     \item The proposed framework achieves state-of-the-art performance on a real dataset consisting of eight unique defect patterns. 
% \end{itemize}
% The rest of the paper is organized as follows:

% Section \ref{method} describes the dataset and the methodology employed in the work. In section \ref{results}, we study the results of the proposed method and compare them with various state-of-the-art models. Finally, we discuss the technique in Section \ref{discussion} and conclude our study in Section \ref{conclusion}.




% \section{Materials and Methods}
% \label{method}
% \subsection{Defect Patterns in Wafer Map}
% Some of the most commonly occurring defects can be categorized into eight types. These eight uniquely identifiable defects are known as “Center(C)”, “Donut(D)”, “Edge-loc(EL)”, “Edge-ring(ER)”, “Loc(L)”, “Near-full(NF)”, “Scratch(S)”, and “Random(R)” as shown in figure \ref{fig:defects}. The names of the defect conveniently refer to the location of the defect in the wafer map. ``Center'' defect refers to collectively failed dies present in the center of the wafer map. ``Donut" refers to the defect pattern in the shape of a ring. In a similar fashion, ``Edge-Loc" refers to defects along the edge of the wafer map.``Edge-ring" is similar to ``Edge-loc" but the defect is distributed along all the edges of the wafer. ``Scratch" refers to defects that are situated along a narrow strip in the wafer. "Random" denotes a wafer map in which the faulty dies appear at random.



% \subsection{Dataset and PreProcessing}
% We conduct our experiments on the MixedWM38\cite{ref-15} dataset which contains 7015 wafer images consisting of the defects described above. The dataset was generated during the wafer test in a real-life fabrication foundry. The wafers are represented as a matrix of size $52\times52$ and each location has $3$ possible values, $0,1,2,$ where $0$ marks non-die locations (corners), $1$ marks fit dies and $2$ denotes defective die at that location.

% We preprocess the dataset by first binarizing the images, the defective dies are changed to 1 while every other location is set to 0. Next, we remove the randomly scattered noise by using a $3\times 3$ median filter as shown in Figure \ref{fig:filter}. Removing noise greatly benefits the feature extraction process.
% \begin{figure}[!htp]
%     \centering
%    \includegraphics[scale=0.6]{Definitions/median.eps}
%     \caption{Removing randomly scattered noise using a $3\times 3$ median filter.}
%     \label{fig:filter}
% \end{figure}






% \subsection{Feature Extraction}
% Each wafer is treated as a joint probability distribution of two discrete random variables, say, $X$ and $Y$, with marginals defined as $p_X(x_i)=\sum_jw(x_i,y_j)$ and $p_Y(y_j)=\sum_iw(x_i,y_j)$ where $w$ is a wafer of shape $n\times n$ and $1\leq i,j\leq n$. Calling a wafer, probability distribution would a misnomer since the marginals do not sum to $1$. However, we can normalize the marginals to get the effect, though it does not affect the analysis. The feature vector for the wafer is simply $p_X ||p_Y $ where $||$ is the concatenation operator. The reasoning behind choosing $p_X ||p_Y$ as a feature vector is that each unique defect shape exhibits a unique set of marginals. Figure \ref{fig:marginals} illustrates this, marginals of a \textit{center} defect peak only at one location which is typically in middle. Marginals of \textit{donut} defect peak twice with a trough in the middle whereas the marginals of \textit{loc} defect are skewed on either side. Other defect patterns are also captured in the marginals as defect shape translates to the shape of marginal distribution succinctly. Therefore, it makes sense to use $p_X ||p_Y $ as a representative for $w$. Moreover, $p_X ||p_Y $ is also robust to missing values as the shape of the marginals is largely unchanged. This approach of feature extraction also achieves a dimensionality reduction of $O(n)$.
% \begin{figure*}[!htp]
%     \centering
%     \begin{tabular}{ccc}
%         \includegraphics[scale=0.3]{Definitions/center.png} &  \includegraphics[scale=0.3]{Definitions/donut.png}&
%         \includegraphics[scale=0.3]{Definitions/Loc.png}    
%     \end{tabular}
%     \caption{Illustration of marginals defining the wafer defect. (Left) Marginals of a \textit{center} defect peak only at one location which is typically in middle. (Center) Marginals of \textit{donut} defect peak twice with a trough in the middle whereas the marginals of \textit{loc} defect are skewed on either side(Right).}
%     \label{fig:marginals}
% \end{figure*}
% \subsection{Object Detection}
% % \includegraphics[]{}

% Often, it is also required to identify not only the shape of the defect but also the location at which the defect is appearing. We identify the lower and upper bounds by thresholding the marginals. Let $\gamma \in \mathbb{R}^+$ be the set threshold and $(x_l,x_h), (y_l,y_h)$ be the position of the first and last non-zero entry in the vector $v_x$ and $v_y$  defined as:
% \begin{equation}
% v_x=
%     \begin{cases}
%         x_i & \text{if } x_i>\gamma ,\ \ \ i=1,2,3,...,n\\
%         0 & otherwise
%     \end{cases}
% \end{equation}
% \begin{equation}
% v_y=
%     \begin{cases}
%         y_i & \text{if } y_i>\gamma ,\ \ \  i=1,2,3,...,n\\
%         0 & otherwise
%     \end{cases}
% \end{equation}
% Then, the rectangle formed by the cordinates $(x_l,y_l),(x_l,y_h),(x_h,y_h),(x_h,y_l)$ is the bounding box of the defect. We illustrate this in Figure \ref{fig:bbox}.

% \begin{figure*}[!htp]
%     \centering
%     \includegraphics[scale=0.4]{Definitions/detection.png}
%     \caption{Object Detection : Location of the defect is identified by thresholding the marginals.  }
%     \label{fig:bbox}
% \end{figure*}
% \subsection{Methodology and Implementation Details}
% The flowchart of the classification pipeline is described in Figure \ref{fig:flowchart}. The input wafers are  standardized to $n\times n$, binarized, and cleaned of noise. Feature extraction consists of simply summing the wafer on both axes one by one and concatenating the sum vectors. For a $n\times n$ wafer, the extracted features have length $2n$. Extracted features are fed to any classifier to get the predictions. In our experiments, we choose a random forest classifier to classify the defects.

% We conduct our experiments on the MixedWM38 dataset which contains 7015 wafers belonging to eight unique defect pattern classes. We randomly sample $80\%$ of the dataset as a training set and the remaining is used as the validation set. The experiments are conducted in Python compiler, Pytorch 1.12, and CUDA 11.3, the computer with option: Linux system, Intel(R) Xeon(R) CPU @ 2.20GHz, and Tesla T4 GPU. 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Results}
% \label{results}
% We compare the proposed method to various state-of-the-art models used currently in practice including widely popular DCNet based on CNN, LSTM which is a widely accepted model in the field of NLP, and MSF-Trans which is based on Transformers\cite{ref-18}. The results are presented in Table \ref{tab:result}. The proposed method performs the best among all the models showing the efficacy of the feature extraction as illustrated in Figure \ref{fig:acc}. 





% \begin{table*}[!htp]
%     \centering
%     \hline
% \resizebox{\textwidth}{!}{\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|}

% {-} & \multicolumn{4}{c|}{Accuracy} & \multicolumn{4}{c|}{Precision} & \multicolumn{4}{c|}{Recall} \\
% \hline
% {-} & \vtop{\hbox{\strut Proposed}\hbox{\strut Method}} & MSF-Trans &  DC-Net &   LSTM & \vtop{\hbox{\strut Proposed}\hbox{\strut Method}} & MSF-Trans & DC-Net &    LSTM & \vtop{\hbox{\strut Proposed}\hbox{\strut Method}} & MSF-Trans &  DC-Net &    LSTM \\
% \hline
% \toprule
% Center    &           92.52 &     98.32 &   97.80 &  98.32 &          100.00 &     97.22 &  93.00 &   95.11 &           92.52 &     97.77 &   77.00 &   97.77 \\
% Donut     &          100.00 &     99.08 &   96.50 &  98.17 &           92.38 &     97.73 &  95.00 &   91.81 &          100.00 &     98.62 &   93.00 &   97.71 \\
% Edge-Loc  &           97.96 &     95.45 &   94.40 &  93.94 &           92.34 &     97.96 &  96.00 &   93.88 &           97.97 &     96.97 &   91.00 &   97.98 \\
% Edge-Ring &           95.24 &     99.53 &   99.80 &  95.35 &           98.52 &     97.72 &  93.00 &   94.01 &           95.24 &     99.53 &   97.00 &   94.88 \\
% Loc       &           99.47 &     99.48 &   93.80 &  95.34 &           98.43 &     97.95 &  99.00 &   94.44 &           99.47 &     98.96 &  100.00 &   96.89 \\
% Near-Full &           94.45 &     97.24 &   95.80 &  99.31 &           94.44 &     95.95 &  90.00 &   99.32 &           94.44 &     97.93 &   94.00 &  100.00 \\
% Scratch   &           95.92 &     98.77 &   93.40 &  98.77 &           99.47 &     96.39 &  60.00 &   93.02 &           95.92 &     98.77 &   88.00 &   98.77 \\
% Random    &           98.80 &     84.38 &  100.00 &  93.75 &           98.80 &     96.30 &  97.00 &  100.00 &           98.80 &     81.25 &   93.00 &   93.75 \\
% \hline
% Average   &           96.79 &     96.53 &   96.44 &  96.62 &           96.80 &     97.15 &  90.38 &   95.20 &           96.79 &     96.22 &   91.62 &   97.22 \\
% \bottomrule
% \hline
% \end{tabular}}

%     \caption{Comparison of the proposed method against various state-of-the-art methods on accuracy, precision, and recall.}
%     \label{tab:result}
% \end{table*}


% \begin{figure*}[!htp]
%     \centering
%     \includegraphics[scale=0.53]{Definitions/accuracies (1).pdf}
%     \caption{Accuracy of compared methods.}
%     \label{fig:acc}
% \end{figure*}

% We also evaluate the method on the statistical parameters of precision and recall defined as:


% True Positive (TP): predicting positive, the actual is positive.

% False Positive (FP): predicting positive, the actual is negative.

% False Negative (FN): predicting negative, the actual is positive.

% True Negative (TN): predicting negative, the actual is negative.

% \begin{equation}
%     Recall= \frac{TP}{TP+FN} \\
% \end{equation}
% \begin{equation}
%     Precision= \frac{TP}{TP+FP} \\
% \end{equation}

% The precision of the method is $96.80\%$ which is significantly higher than DCNet ($+6.42\%$) and LSTM($+1.60\%$). This shows that the proposed method has a very tiny false recognition rate. On the other hand, the recall is close to 1, signifying that the method correctly identifies most of the defects. The method significantly outperforms DCNet($+5.17$) in terms of recall as well.

% \begin{figure*}[!htp]
%     \centering
%     \includegraphics[scale=0.45]{Definitions/pca.eps}
%     \caption{Dimensionality reduction at zero cost : (Left) First two PCA components of wafers in the input space. (Right) First two PCA components of wafers in the feature space. }
%     \label{fig:pca}
% \end{figure*}
% \section{Discussion}
% \label{discussion}
% To evaluate the effectiveness of the feature extraction, we compare the data cloud of input images to the data cloud of features by projecting the data cloud onto the subspace formed by the first two principal components separately. The input wafers of size $52\times52$ are flatted to obtain a vector of size $52^2$, while the extracted features have size $104(52+52)$. The projection of both data clouds onto their first two principal components is presented in Figure \ref{fig:pca}. We observe that the relative position as well as the shape of the data pertaining to a particular class remains unchanged. This essentially indicates that the decision boundaries in the input space can be translated into decision boundaries in the feature space. Furthermore, the dimensionality reduction does not lose information despite being so artless. The performance advantage lies in the fact that it is much easy to find decision boundaries in lower dimensions than in higher dimensional space.  The proposed technique is also explainable as the classification result can be decomposed into simple \textit{if-else} rules contrasting to the deep learning models which are essentially black-box. 


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Conclusions}
% \label{conclusion}
% In this paper, we study the defect patterns on the wafer map which are uncovered during wafer testing. This may provide valuable information pertaining to the root cause of the defects, greatly improving the yield. The contribution of the paper is as follows:
% \begin{itemize}
%     \item Compared to conventional defect pattern recognition works, which are hugely based on deep neural networks, we propose a simple, effective, and intuitive framework that is fast and accurate on a single DPR. Results clearly show the proposed method is very competitive and outperforms the current state-of-the-art despite being non-parametric.
%     \item The proposed feature extraction technique effectively performs fast dimensionality reduction without losing much information about the relative shape and size of the data cloud.
% \end{itemize}

% Future work will be paid on applying the proposed method on identifying multiple defects on a wafer. It would also be worthwhile to apply the proposed technique in defect root cause detection. Further study will also look at the modeling and analysis of the wafer defect to improve the reliability of wafer production.

% \begin{thebibliography}{1}

% \bibitem{ref-1}
% L. Mönch, R. Uzsoy, and J. W. Fowler, "A survey of semiconductor supply chain models part I: Semiconductor supply chains strategic network design and supply chain simulation", Int. J. Prod. Res., vol. 56, no. 13, pp. 4524-4545, 2017.
% \bibitem{ref-2}
%  K. B. Lee, S. Cheon, and O. K. Chang, "A convolutional neural network for fault classification and diagnosis in semiconductor manufacturing processes", IEEE Trans. Semicond. Manuf., vol. 30, no. 2, pp. 135-142, May 2017.
%  \bibitem{ref-3}
%  W.-T. K. Chen and C. H.-J. Huang, "Practical ‘building-in reliability’ approaches for semiconductor manufacturing", IEEE Trans. Rel., vol. 51, no. 4, pp. 469-481, Dec. 2002.
%  \bibitem{ref-4}
%  S. F. Yang and W.-T. K. Chien, "Electromigration lifetime optimization by uniform designs and a new lifetime index", IEEE Trans. Rel., vol. 64, no. 4, pp. 1158-1163, Dec. 2015.
%  \bibitem{ref-5}
%  C. Wang, "Recognition of semiconductor defect patterns using spatial filtering and spectral clustering", Expert Syst. Appl., vol. 34, no. 3, pp. 1914-1923, 2008.
%  \bibitem{ref-6}
%  S. Kang, S. Cho, D. An and J. Rim, "Using wafer map features to better predict die-level failures in final test", IEEE Trans. Semicond. Manuf., vol. 28, no. 3, pp. 431-437, Aug. 2015.
%  \bibitem{ref-7}
%  J. Kim, Y. Lee, and H. Kim, "Detection and clustering of mixed-type defect patterns in wafer bin maps", IISE Trans., vol. 50, no. 2, pp. 99-111, 2018.

% \bibitem{ref-8}
% M. Fan, Q. Wang, and B. van der Waal, "Wafer defect patterns recognition based on OPTICS and multi-label classification", Proc. IEEE Adv. Inf. Manage. Communicates Electron. Autom. Control Conf. (IMCEC), pp. 912-915, 2016.
% \bibitem{ref-9}
%  R. Baly and H. Hajj, "Wafer classification using support vector machines", IEEE Trans. Semicond. Manuf., vol. 25, no. 3, pp. 373-383, Aug. 2012.
%  \bibitem{ref-10}
%  L. Xie, R. Huang, and Z. Cao, "Detection and classification of defect patterns in optical inspection using support vector machines", Intelligent Computing Theories, pp. 376-384, Springer, 2013.
%  \bibitem{ref-11}
%  Y. Xia, H. Yu, and F. Wang, "Accurate and robust eye center localization via fully convolutional networks", IEEE/CAA J. Automatica Sinica, vol. 6, no. 5, pp. 1127-1138, Sep. 2019.
%  \bibitem{ref-12}
%  L. Wen, X. Li, L. Gao and Y. Zhang, "A new convolutional neural network-based data-driven fault diagnosis method", IEEE Trans. Ind. Electron., vol. 65, no. 7, pp. 5990-5998, Jul. 2017.
%  \bibitem{ref-13}
%  T. Nakazawa and D. V. Kulkarni, "Wafer map defect pattern classification and image retrieval using convolutional neural network", IEEE Trans. Semicond. Manuf., vol. 31, no. 2, pp. 309-314, May 2018.
%  \bibitem{ref-14}
%  K. Kyeong and H. Kim, "Classification of mixed-type defect patterns in wafer bin maps using convolutional neural networks", IEEE Trans. Semicond. Manuf., vol. 31, no. 3, pp. 395-402, Aug. 2018.
%  \bibitem{ref-15}
%  J. Wang, C. Xu, Z. Yang, J. Zhang and X. Li, "Deformable convolutional networks for efficient mixed-type wafer defect pattern recognition", IEEE Trans. Semicond. Manuf., vol. 33, no. 4, pp. 587-596, Nov. 2020.
%  \bibitem{ref-16}
%  J. Wang, Z. Yang, J. Zhang, Q. Zhang and W.-K. Chien, "AdaBalGAN: An improved generative adversarial network with imbalanced learning for wafer defective pattern recognition", IEEE Trans. Semicond. Manuf., vol. 32, no. 3, pp. 310-319, Aug. 2019.
%  \bibitem{ref-17}
%    Rudin, C., & Radin, J. (2019). Why Are We Using Black Box Models in AI When We Don’t Need To? A Lesson From an Explainable AI Competition. Harvard Data Science Review, 1(2). https://doi.org/10.1162/99608f92.5a8a3a3d
% \bibitem{ref-18}
% Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, u., & Polosukhin, I. (2017). Attention is All you Need. In Advances in Neural Information Processing Systems. Curran Associates, Inc.
% \end{thebibliography}


% \begin{IEEEbiographynophoto}{Nitish Shukla}
% Nitish Shukla was born in Kanpur, India in 1997. He holds a double M.S. degree in mathematics and computer science. He received his M.S. degree in mathematics from the Indian Institute of Technology(IIT), Guwahati in 2019 and an M.S. degree in computer science from Chennai Mathematical Institute (CMI), India in 2022. His interests lie in the field of computer vision and explainable AI. He is currently working towards making vision models more robust and transparent.
% \end{IEEEbiographynophoto}
% \end{document}


