\begin{abstract}
  Classical results in neural network approximation theory show how arbitrary continuous functions can be approximated by networks with a single hidden layer, under mild assumptions on the activation function. However, the classical theory does not give a constructive means to generate the network parameters that achieve a desired accuracy. Recent results have demonstrated that for specialized activation functions, such as ReLUs and some classes of analytic functions, high accuracy can be achieved via 
linear combinations of \textit{randomly initialized} activations. 
%
These recent works utilize specialized integral representations of target functions that depend on the specific activation functions used. 
%
This paper defines \textit{mollified integral representations}, which provide a means to form integral representations of target functions using activations for which no direct integral representation is currently known.
%
The new construction enables approximation guarantees for randomly initialized networks for a variety of widely used activation functions. 
\end{abstract}

\section{Introduction}\label{sec:intro}
In this paper we consider random approximations to target functions $f:\R^n\to\R$ that are
continuous on bounded set $B\subset\R^n$, which are linear combinations of parameterized basis
function $\Bs:\R^n\times\Theta\to\R$ and are defined as
\begin{equation}\label{linCombBasis}
\ftr(x,\Thetabr) = \sum_{j=1}^{R} c_j\,\Bs(x,\thetab_j) \ \ ,
\end{equation}
with $R\geq1$ and where the parameters $\Thetabr=\{\thetab_1,\dots,\thetab_R\}$ are randomly
sampled iid from the parameter set $\Theta\subseteq\R^m$.\\\\
When the basis function is a scalar nonlinearity $\sigma:\R\to\R$ which is given the inner
product of parameter $\thetab_j$ and the vector $[x_1,\dots,x_n,1]^\top$ (also known as a
\textit{ridge function}), like the logistic sigmoid or ReLU functions, they are referred to as
\textit{activation functions} and \eqref{linCombBasis} is a fully connected neural network with
a single hidden layer that is randomly initialized. The number of terms (or \textit{neurons})
$R$ in the hidden layer is referred to as the network's \textit{width}.\\\\
Such randomly initialized networks have practical importance. In one sense, with $R$ is
sufficiently large (ie the network is over-parameterized), it is known that the usual gradient
descent training methods will not change the internal parameters $\thetab_j$ much from their
initialization. Thus, knowing which kinds of functions can be approximated well by linear
combinations of randomly initialized activation functions, is key to understanding the (initial)
training dynamics of such networks.\\\\
Another aspect is that many theorems describing the existence of approximations of the form
\eqref{linCombBasis} are not constructive. \cite{pinkus1999approximation} details many of these
results, including a classic result by \cite{barron1993universal} for arbitrary sigmoidal
activations, which was expanded to arbitrary hinge functions by \cite{breiman1993hinging}.

This paper builds upon the works of \cite{barron1993universal} and \cite{breiman1993hinging} to show how accurate approximations can be achieved for a variety of activations by randomly initializing the $\thetab_j$ and
running optimization just on the outer coefficients $c_j$. The key concept in doing this is finding an integral representation for the target function $f$
of the form
\begin{equation}\label{intRep}
f(x) = \int_\Theta g(\theta)\,\sigma(\theta\cdot [x_1,\dots,x_n,1]^\top)\,\d\theta
\end{equation}
for some activation function $\sigma:\R\to\R$ and bounded coefficient function $g:\Theta\to\R$,
with $\cdot$ denoting the inner product. If this can be done, then
$g_\Theta(\theta)=g(\theta)/\Pt(\theta)$ can be defined for some nonzero probability density
$\Pt$ and \eqref{intRep} becomes
\begin{equation*}
f(x) = \int_\Theta g_\Theta(\theta)\,\sigma(\theta\cdot [x_1,\dots,x_n,1]^\top)
\,\Pt(\theta)\,\d\theta \ \ .
\end{equation*}
Therefore, this gives that $\E[f_R(x,\Thetabr)]=f(x)$ when $\Thetabr$ is sampled iid according
to $\Pt$. This is done in \cite{hsu2021approximation} for ReLU activations when $\Bs$ is a
trigonometric polynomial ridge function and in \cite{yehudai2019power} for an analytic activation
with appropriate coefficients relative to $\Bs$ being a polynomial.\\\\
However, finding integral representations like \eqref{intRep} for general classes of target
functions and activations is a nontrivial task. \cite{irie1988capabilities} gives a constructive
method, but only for target and activation functions in $L_1$. In \cite{kainen2010integral} and \cite{petrosyan2020neural}, they propose constructive
methods for a class of target functions with unit step and ReLU activations respectively.


\subsection{Contribution}
Our main contribution is what we term the \textit{mollified integral representation}, which
allows arbitrary target functions $f$ to have integral representations like \eqref{intRep} with
arbitrary Lipschitz activations $\sigma$,
%and \textit{mollified} versions $\deltal$ of the Dirac
%delta for \textit{mollification factor} $\lambda>0$,
so long as there exists a base approximation
of $f$ as some linear combination of the activations $\sigma$.
%in the sense of an $L_2$ norm over
%$B$.
\\\\
More formally,
assume that a base approximation
$\fbN(x)=\sum_{i=1}^Nc_i\sigma(\theta_i \cdot [x_1,\dots,x_n,1]^\top)$ exists, for any arbitrary
number $N\geq1$ of terms, such that $\|\fch(x)-\fbN(x)\|_{L_2(B)}\leq\epsapprox$ holds for some
$\epsapprox>0$, with $\fch$ being some \textit{affine shift} of $f$, and with coefficient bound
$\sum_{i=1}^N|c_i|\leq K$ for some $K>0$. Then our main result, in Section~\ref{sec:overall}, is that a random approximation
$\ftr$ of the form \eqref{linCombBasis} satisfying
\begin{align}\nonumber
&\norm{\fch-\ftr}_{L_2(B)} \ \leq \ \epsapprox + K\ O\!\left(\frac{1}{\lambda} +
\frac{\lambda^{n+1}}{\sqrt{R}}\right)
\end{align}
exists with $\sum_{j=1}^R|c_j|\leq \frac{K}{\min\Pt}\ O(\lambda^{n+1})$.

We show in Section~\ref{sec:approx} how base approximation bounds can be obtained for a variety of sigmoidal and hinge-like activation functions. Our main result implies that random initializion lead to arbitrarily close approximations for all of these activations. 


\subsection{Notation and Problem Setup}\label{sec:notationSetup}
Vectors $v\in\R^m$ are denoted $[v_1,\dots,v_m]^\top$. Column vectors are assumed unless
otherwise noted and $u\cdot v$ denotes the inner product of equal length vectors $u$ and $v$.
We use $\covcl\,\Hc$ to denote the closure of the convex hull of set $\Hc$, $\step$ to denote the
unit step function, and $\relu$ to denote the ReLU function which is $\max(0,y)$ for all
$y\in\R$.\\\\
We denote the integer set $\{1,\dots,N\}$ as $[N]$. \textbf{Bold} font indicates random
variables or functions, for example $\thetab$ or $\fb$, and we use $\simi$ to denote when a set
of random variables has been drawn iid according to a density, for example
$\{\thetab_1,\dots,\thetab_r\}\simi P_\theta$.\\\\
We let $\muB$ denote the uniform probability measure over a bounded set $B\subset\R^n$ and let
$\normB{f}^2=\int_B|f(x)|^2\,\muBdx$ denote the corresponding squared $L_2(B)=L_2(B,\muB)$ norm
of a function $f:\R^n\to\R$. Unless otherwise stated, $\norm{\cdot}$ is the standard Euclidean
norm. The supremum norm over $x\in B$ of the inner product of $x$ with a vector $w\in\R^n$ is
denoted $\absB{w}=\sup_{x\in B}|\wdx|$. If $B\subseteq\{x\in\R^n\ |\ \norm{x}\leq r\}$, then
$\absB{w}\leq r\norm{w}$ holds.\\\\
In this paper, we consider target functions $f:\R^n\to\R$ that are continuous on bounded sets
$B\subset\R^n$. An \textit{affine shift} $\fch$ is defined as $\fch(x)=f(x)-\mathscr{a}\cdot x
-\mathscr{b}$ for some $\mathscr{a}\in\R^n$ and $\mathscr{b}\in\R$. This of course can include
the case where $\mathscr{a}$ and/or $\mathscr{b}$ are zero.\\\\
Base approximations to (affine shifts of) $f$ which are linear combinations of a scalar
activation function $\sigma:\R\to\R$ are defined for an arbitrary number $N\geq1$ as
\begin{equation}\label{singleHidden}
\fbN(x) = \sum_{i=1}^{N} c_i\,\sigma_i(a_i\cdot x + b_i) 
= \sum_{i=1}^{N} c_i\,\sigma_i(\theta_i\cdot z) \ \ ,
\end{equation}
with $c_i\in\R$ for all $i\in[N]$.\\\\
Here and in the rest of the paper, we denote for any $x\in\R^n$ the corresponding vector
$z=[x_1,\dots,x_n,1]^\top\in\R^n\times\{1\}$. Let us then define $\ZB = \sqrt{\int_B \lVert
z\rVert^2\,\muBdx}\,$.\\\\
\textit{Weights} $a_i$ and \textit{bias} $b_i$ are assumed restricted to the
sets $\Ac=[\alpha_1,\beta_1]\times\cdots\times[\alpha_n,\beta_n]
\subset\R^n$ and $\Bc=[\alpha_0,\beta_0]\subset\R$ respectively, for some appropriate
$\alpha_0,\beta_0,\dots,\alpha_n,\beta_n\in\R$, and so the \textit{parameters}
$\theta_i=[a_{i,1},\dots,a_{i,n},b_i]^\top$ are restricted to $\Theta=\Ac\times\Bc$, for all
$i\in[N]$. However, unless otherwise specified, we assume $\Theta$ contains all $\absB{a}\leq1$
(ie $\norm{a}\leq\frac{1}{r}$ for some $r>0$) and $|b|\leq1$ (ie $\alpha_0=-1,\beta_0=1$).\\\\
Our main assumption on activations is below:
\begin{assumption}\label{sigAsmpt}
The activation function $\sigma:\R\to\R$ is $\Lc$-Lipschitz over the inputs
$\Ic_\Theta=\{\theta\cdot z \ | \ \theta\in\Theta, z\in B\times\{1\}\}$, such that
\begin{equation*}
|\sigma(y_1) - \sigma(y_2)| \leq \Lc\,|y_1-y_2|
\end{equation*}
holds for all $y_1,y_2\in\Ic_\Theta$. Thus, $\sch(y)=\sigma(y)-\sigma(0)$ is also
$\Lc$-Lipschitz over $y\in\Ic_\Theta$.
\end{assumption}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
