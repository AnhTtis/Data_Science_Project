\section{Conclusion and Future Directions}
\label{sec:conclusion}

In this paper, we developed a novel method for bridging approximations using activation 
functions with unknown parameters to approximations using randomly initialized parameters, with 
the mollified integral representation. We showed that this could be extended to the supremum 
norm for use with an extended, approximate version of the linear MRAC setup.

There are two immediate paths that we can see along the lines of extending this work and 
application. 1) Is it possible to do 
``approximate persistency of excitation" on the extended, approximate version of the linear MRAC 
setup, similar to what was achieved in \cite{lekang2022sufficient} but with the parameter 
estimates only converging to a compact set about the origin, instead of being asymptotically 
stable. 2) Extending the work of \cite{irie1988capabilities} towards the determination of 
integral representations for more general classes of functions. In particular, using linear 
combinations of step functions, which cannot be trained using backpropagation in the modern 
paradigm of training deep neural networks. Yet in the 1D case, we can easily show that the step 
function is superior at uniformly approximating functions than the ReLU function. In fact, the 
step can handle discontinuous (regulated) functions, while the ReLU can only deal with 
continuous functions.
