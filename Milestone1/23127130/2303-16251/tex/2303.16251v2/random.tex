\section{Randomly Initialized Approximations}
\label{sec:apx:random}

In this section, we focus on random approximations $\fhR$ of the form \eqref{eq:apx:rndApx} and 
show that they exist (in high probability) with approximation error in the 
$L_2\left(\Bc,\muB\right)$ norm 
of the shape $O(\frac{1}{\sqrt{R}})$, down to the equivalent bound $\epsbase$ as in 
\eqref{eq:apx:normBound} of an existing base approximation $\fhN$ as in \eqref{eq:apx:baseApx}.

This is done by creating the corresponding mollified approximation $\fhlN$, which has a 
mollified integral representation and therefore can be approximated with randomly sampled 
parameters, which gives $\fhR$.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.5em,
        nodes={draw,fill=white},
        box/.style={align=left,inner sep=1ex},
        marrow/.style={single arrow,
              single arrow head extend=1mm,
              execute at begin node={\strut}}]
   \node (A) [box] {$\fhN$ };
   \node (B) [box, right=of A] {$\fhlN$};
   \node (C) [box, right=of B] {$\fhR$};
\draw [->] (A) edge (B) (B) edge (C);
\end{tikzpicture}
%\caption{\label{fig:apx:apxBlock} XXXX.}
\end{figure}

\subsection{Expected Value of Random Approximations to Functions with Integral Representations}

An important property of functions that have integral representations with some activation 
$\sigma$ is that random approximations $\fhR$ of the form \eqref{eq:apx:rndApx} exist which are
exactly equal to the function in expected value over the randomly drawn parameters.

And so, the following lemma builds upon the mollified integral representation of the mollified
approximation $\fhlN$ to show that there always exists random approximations with randomly 
sampled parameters from the $\lambda$-expanded set $\Upsilonl$ satisfying this property.

\begin{lemma}\label{lem:apx:exp}
\textit{
Let the target function $f\in\Cs(\Bc)$ satisfy Theorem~\ref{thm:apx:moll} and thus have the
mollified approximation $\fhlN$ as in \eqref{eq:apx:intRep} with activation $\sigma$ for the 
selected mollification factor $\lambda>0$, which has the integral representation
\eqref{eq:apx:baseIntRepLam} over the $\lambda$-expanded parameters set $\Upsilonl$.
If the random parameters $\upgammab_1,\dots,\upgammab_R$ are sampled iid according
to any density $\Pl$ which is nonzero over $\Upsilonl$, then there exists a random approximation
$\fhR$ of the form \eqref{eq:apx:rndApx} using the activation $\sigma$ such that}
\begin{equation}\label{eq:apx:expL}
\Exp_{\upgammab_j\sim\Pl}\limits\left[\fhR(x\,;\upgammab_1,\dots,\upgammab_R)\right] \
= \ \fhlN(x)
% \hspace{40pt} \forall x\in\Bc 
\ .
\end{equation}
\end{lemma}
\begin{proof}
\if\ARXIV1
Given in Appendix~\ref{app:apx:exp}.
\fi
\if\ARXIV0
Given in Appendix II of \cite{lekang2023functionfull}.
\fi
\end{proof}
\textit{Proof Sketch:} Let us first make the following definitions. With $\gl(\upgamma)$ as in 
\eqref{eq:apx:gLam} for any $\lambda>0$, we define
\begin{align}
%\nonumber
%\gl(\upgamma)& = \sum_{i=1}^N\theta_i\,\deltalno(\upgamma-\upgamma_i) \ \implies 
%\\
\label{eq:apx:Cgl}
&\max_{\upgamma\in\Upsilonl}\limits|\gl(\upgamma)| \ \leq \ 
\sum_{i=1}^N|\theta_i|\left(\frac{\eta\lambda}{e}\right)^{n+1} \ =:\ \Cgl \ .
\end{align}
For the corresponding $\Upsilonl$, we define the diameter $\Dt:=\sup\{\|\upgamma-\phi\|_2 \ | \ 
\upgamma,\phi\in\Upsilonl\}$ and largest point $\upmax:=\sup\{\|\upgamma\|_2 \ | \ 
\upgamma\in\Upsilonl\}$, and for the selected $\Pl$ we define the minimum  $\pmin := 
\inf\{\Pl(\upgamma) \ | \ \upgamma\in\Upsilonl\}$.

Then, note that the RHS of \eqref{eq:apx:mollApx} is equivalently
%\eqref{eq:apx:intRep} can be equivalently expressed in terms of an expectation, as
%\label{eq:apx:intRep2}
\begin{align*}
&\fhlN(x) \ = \\
&\int_\Upsilonl \frac{\gl(\upgamma)}{\Pl(\upgamma)}\,\sigma(\upgamma^\top z)
\,\Pl(\upgamma)\d\upgamma
= \Exp_{\upgammab\sim\Pl}\limits
\left[\frac{\gl(\upgammab)}{\Pl(\upgammab)}\,\sigma(\upgammab^\top z)\right]
%\hspace{40pt} \forall x\in\Bc
\ .
\end{align*}

Thus, a random approximation $\fhR$ exists which satisfies \eqref{eq:apx:expL} by setting its 
coefficients, for all $j\in[R]$, as
\begin{equation*}\label{eq:appApx:cjOpt}
\vartheta_j = \frac{\gl(\upgammab_j)}{\Pl(\upgammab_j)\,R}
\ =\ \frac{\sum_{i=1}^N\theta_i\,\deltalno(\upgammab_j-\upgamma_i)}{\Pl(\upgammab_j)\,R}
\ \ .
\end{equation*}


\subsection{Random Approximations to Base Approximations}
\label{sec:apx:main}

We now state and prove an overall bound on the approximation error in $L_2(\Bc,\muB)$ norm, 
between a base approximation $\fhN$ of the form \eqref{eq:apx:baseApx} to a target function 
$f\in \Cs(\Bc)$ and a random approximation $\fhR(x\,;\upgammab_1,\dots,\upgammab_R)$ of the form 
\eqref{eq:apx:rndApx}.

\begin{theorem}\label{thm:apx:main}
\textit{
Let there exist a base approximation $\fhN$ of the form \eqref{eq:apx:baseApx}, for some integer 
$N\in\N$, to a target function $f\in \Cs(\Bc)$, with unknown parameters 
$\upgamma_1,\dots,\upgamma_N$ in a known bounded parameter set $\Upsilon\subset\R^{n+1}$, 
unknown coefficients $\theta\in\R^N$ with a known bound on the coefficient size
$\sum_{i=1}^N|\theta_i|\leq\Sc_N$, and using an activation function $\sigma$ satisfying
Assumption~\ref{sigAsmpt}. Assume that $\normB{\fch(x)-\fhN(x)}\leq\epsbase$ holds for some 
$\epsbase>0$ which may depend on $N$, for an affine shift $\fch(x)=f-\as^\top\!x-\bs f(0)$ with
some $\as\in\R^n$ and $\bs\in\R$.
The mollified approximation $\fhlN$ as in \eqref{eq:apx:mollApx}
satisfies the results of Theorem~\ref{thm:apx:moll} for any mollification factor $\lambda>0$.
Let the random parameters $\upgammab_1,\dots,\upgammab_R$ be sampled iid according to any
nonzero density $\Pl$ over the $\lambda$-expanded set $\Upsilonl$, which has diameter $\Dt$ and
farthest point $\upmax$, and let $\EB$ be as in \eqref{eq:apx:EB}. Then for any $\nu\in(0,1)$,
with probability greater than $1-\nu$ there exists a random approximation 
$\fhR(x\,;\upgammab_1,\dots,\upgammab_R)$ as in \eqref{eq:apx:rndApx} with
$|\vartheta_j|\leq\frac{\Cgl}{\pmin R}$ for all $j\in[R]$ such that either of the following 
hold:}
\begin{align*}
&\normB{\,\fch(x) - \fhR(x\,;\upgammab_1,\dots,\upgammab_R)} \ \leq \\
&i)\hspace{34pt}
\epsbase + 
\Sc_N\Dsig\left(1 \ + \ 
\frac{\lambda^{n+1}}{\sqrt{R}\,}\,\KD(\nu)\right) \\
&ii)\hspace{30pt}
\epsbase + 
\Sc_N\Lsig\left(\frac{\sqrt{n+1}}{\lambda}\,\EB +
\frac{\lambda^{n+1}}{\sqrt{R}\,}\,\KL(\nu)\right) \ \  .
\end{align*}
The coefficients $\KD$ and $\KL$ are defined in terms of the selected $\nu$ as:
\begin{align*}
&\KD(\nu) \ := \ 
\frac{\eta^{n+1}}{e^{n+1}\pmin}
\left(
1 + \frac{|\sigma(0)|}{\Dsig} \right. \\
&\left. + \bigg(3 + 2\frac{|\sigma(0)|}{\Dsig}\bigg) 
\sqrt{\frac{1}{2}\log\left(\frac{1}{\nu}\right)}
\right) \\
&\KL(\nu) \ := \
\frac{\eta^{n+1}}{e^{n+1}\pmin}
\left(
\upmax\EB + \frac{|\sigma(0)|}{\Lsig}\right. \\
&\left. + \bigg((\Dt+2\upmax)\EB + 
2\frac{|\sigma(0)|}{\Lsig}\bigg) \sqrt{\frac{1}{2}\log\left(\frac{1}{\nu}\right)}
\right).
\end{align*}
\end{theorem}
\begin{proof}
\if\ARXIV1
Given in Appendix~\ref{app:apx:mainProof}.
\fi
\if\ARXIV0
Given in Appendix II of \cite{lekang2023functionfull}.
\fi
\end{proof}
\textit{Proof Sketch:} We closely follow the strategy in Lemma 4 of
\cite{rahimi2008weighted}, which uses McDiarmid's inequality to obtain the final result.

We start by defining function $h_\Bc:\Upsilon^R_\lambda\to\R$, which captures the norm difference
between the random approximation and its expected value over the iid draws of 
$(\upgammab_1,\dots,\upgammab_R)\in\Upsilon^R_\lambda$, as
\begin{align}
\label{eq:apx:hB}
&h_\Bc(\upgammab_1,\dots,\upgammab_R) \ := \\\nonumber
&\normB{\,\fhR(x\,;\upgammab_1,\dots,\upgammab_R)
-\Exp_{\upgammab_j\sim\Pl}\limits\left[\fhR(x\,;\upgammab_1,\dots,\upgammab_R)\right]} \ \ .
\end{align}
Thus, we can use Lemma~\eqref{lem:apx:exp} to show that there exists $\fhR$ such that 
\eqref{eq:apx:hB} is equivalently
$$
h_\Bc(\upgammab_1,\dots,\upgammab_R) \ = \ \normB{\,\fhlN(x) - 
\fhR(x\,;\upgammab_1,\dots,\upgammab_R)} \ .
$$
We then can bound the expected value
$$
\Exp_{\upgammab_j\sim\Pl}\limits\Big[h_\Bc(\upgammab_1,\dots,\upgammab_R)\Big]
$$
and the absolute value of the difference between any 
$$
\big|h_\Bc(\upgamma_1,\dots,\upgamma_j,\dots,\upgamma_R)-
h_\Bc(\upgamma_1,\dots,\widetilde\upgamma_j,\dots,\upgamma_R)\big|
$$
that differ only at one coordinate, over each $j\in[R]$. These two bounds then allow us to apply 
the results of McDiarmid's inequality, thus bounding (in high probability)
$$
\normB{\,\fhlN(x) - 
\fhR(x\,;\upgammab_1,\dots,\upgammab_R)} \ .
$$
The proof is then completed by using the triangle inequality, with
\begin{align*}
&\normB{\,\fch(x) - \fhR(x\,;\upgammab_1,\dots,\upgammab_R)} \ \leq \ 
\normB{\fch(x)-\fhN(x)} +\\ &\ \normB{\fhN(x)-\fhlN(x)} 
+\normB{\fhlN(x)-\fhR(x\,;\upgammab_1,\dots,\upgammab_R)} \ .
\end{align*}
where the first term is assumed bounded by $\epsbase$ for the base approximation $\fhN$ and the 
second term is bounded by Theorem~\ref{thm:apx:moll}.

\begin{remark}
Classic results from \cite{barron1993universal} and \cite{breiman1993hinging} prove the existence
of \textit{convex} combinations of step \eqref{eq:pe:step} and ReLU \eqref{eq:pe:ReLU} 
activations that approximate (affine shifts of) functions in specific classes arbitrarily well, 
with an 
approximation error proportional to $\epsbase=O\left(\frac{1}{\sqrt{N}}\right)$. Importantly, 
the 
convexity property allows the length (number
of terms) $N$ to be arbitrarily large while maintaining the same overall bound on the 
coefficient size $\sum_{i=1}^N|\theta_i|\leq\Sc_N$.
This means we can allow the number of terms to grow 
arbitrarily large $N\to\infty$, since $N$ does not appear directly in the results of 
Theorem~\ref{thm:apx:main}. This gives $\epsbase\to0$, while maintaining the same bound $\Sc_N$.
\end{remark}
