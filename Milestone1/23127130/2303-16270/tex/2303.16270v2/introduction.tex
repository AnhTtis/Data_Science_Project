\section{Introduction}




Federated Learning (FL) is a distributed learning method that enables multiple parties to collaboratively train a model without centralizing their raw data. Therefore, the clients can retain control over their own data assets. FL has received significant attention and has become a major research topic due to its capability to build real-world applications where datasets are isolated across different organizations/devices while preserving data governance and privacy~\cite{li2020federated,kairouz2021advances}.

Existing approaches primarily focus on horizontal federated learning (HFL), where the data from different clients share the same feature space but have different samples~\cite{yang2019federated}. One application of HFL is that smartphones users collaboratively train a next-word prediction model for the smart keyboard~\cite{hard2018federated}. In HFL, the clients are expected to learn the common knowledge from heterogeneous data distributions and produce a global model by aggregating the updates of local models. Hence, the main challenge of HFL is data distribution heterogeneity, and under cross-device scenarios, limited computation resources.

\begin{figure}[t]
\centering
     \includegraphics[scale=0.3]{img/data_split.pdf}
\caption{An example of data splitting in a two-client VFL setting.}
\label{fig:data_split}
\end{figure}

Vertical federated learning (VFL), on the other hand, focuses on scenarios in which the data on clients have different feature spaces but share some overlapping samples~\cite{yang2019federated}. In addition, the true labels can reside on a third-party server~\cite{romanini2021pyvertical} as shown in \cref{fig:data_split}. For example, a credit bureau collaborates with an e-commerce company and a bank to train a model to estimate a user's credit score. In this case, only the credit bureau has the credit score of the users which will not be shared with the e-commerce company and the bank. VFL is mostly deployed in cross-silo scenarios, and the computation power is usually not a major concern~\cite{kairouz2021advances}. However, VFL faces two unique challenges. First, VFL requires the clients to communicate with the server for each iteration (rather than after several epochs under HFL) of training, which introduces extremely high communication costs. It is also notable that iterative communications require reliable communication channels between the server and the clients, which is usually expensive. In addition to the high communication cost, the other major challenge of VFL is that the number of overlapping samples may be limited. For example, two hospitals in different countries are not expected to have a large number of overlapping patients. The model trained with limited overlapping samples likely cannot achieve reliable performance. 

Furthermore, VFL is currently not as well explored as HFL. Some existing works can reduce the communication cost by reducing the communication frequency or compressing the communicated data~\cite{liu2019communication}. However, most methods only achieve limited reduction from one local update to multiple, while still requiring heavy iterative communications. Other works focus on improving the performance with limited overlapping samples~\cite{kang2022fedcvt, wu2022practical}. Notably, both challenges are bottlenecks of applying VFL in realistic scenarios, and leaving either one unsolved hinders the deployment of VFL in the real world. To the best of our knowledge, there is no work aiming at solving these two challenges simultaneously.

In this paper, we propose \emph{one-shot} VFL, which is a communication-efficient VFL algorithm that can achieve high performance with minimal overlapping samples. In one-shot VFL, the clients are guided to conduct local semi-supervised learning (SSL) using both the overlapping samples and the unaligned samples to train well-performing feature extractors. Under one-shot setting, the clients only need to conduct two upload operations and one download operation for the training session, which drastically reduces the communication cost and frequency. We further propose \emph{few-shot} VFL as an extension of one-shot VFL. Few-shot VFL expands the supervised dataset on clients to improve the performance of the local feature extractors. Compared with one-shot VFL, clients in few-shot VFL conduct one more time of uploading and downloading, but can achieve better performance, especially when the number of overlapping samples is small.

Our key contributions are summarized as follows:
\begin{compactitem}
    \item We propose a communication-efficient VFL algorithm called \emph{one-shot} VFL. To the best of our knowledge, \emph{one-shot} VFL is the first algorithm that can simultaneously address the challenges of high communication cost and limited overlapping samples.
    \item We propose \emph{few-shot} VFL that can improve the performance further under settings with minimal overlapping samples.
    \item We empirically evaluate the performance of \emph{one-shot} VFL and \emph{few-shot} VFL with different data modalities, including image data and tabular data. The results show that our methods improve the accuracy by more than 46.5\% and reduce the communication cost by more than 330$\times$ compared with the state-of-the-art (SOTA) VFL methods on CIFAR-10.
\end{compactitem}