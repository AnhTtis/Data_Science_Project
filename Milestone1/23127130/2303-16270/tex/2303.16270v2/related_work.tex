\section{Background and Related Work}

\paragraph{Vertical Federated Learning.}

Vertical federated learning (VFL)~\cite{romanini2021pyvertical,vepakomma2018split} is the concept of collaboratively training a model on a dataset where the clients share some common samples but with different features on each client. VFL was first introduced in \cite{hardy2017private}, where a federated logistic regression algorithm is proposed. 
%It also provided a formal analysis of the impact of entity resolution mistakes on learning. \cite{nock2018entity} extended \cite{hardy2017private} to provide a formal assessment of the impact of errors in entity resolution on learning under a wide set of losses. \cite{yang2019parallel} and \cite{yang2019quasi} extend \cite{hardy2017private} by assuming that the sample IDs being already aligned and focus on optimization. Based on the assumption of already aligned sample IDs, recent works focus more on the performance and convergence of the model. 
SecureBoost~\cite{cheng2021secureboost} proposed a secure federated tree-boosting approach in the VFL setting and provided theoretical proof that it achieves the same level of accuracy as its centralized counterparts. Some other gradient boosting tree approaches for VFL include Pivot~\cite{wu2020privacy} and VF$^2$Boost~\cite{fu2021vf2boost}. A federated random forest was also studied in~\cite{liu2020federated}. In addition to tree-based methods, other machine learning algorithms such as linear regression~\cite{zhang2021secure} and logistic regression~\cite{hu2019learning,liu2019communication} have been investigated under VFL settings. However, these algorithms are usually incapable of handling complex tasks such as computer vision (CV) and natural language processing (NLP), in which Deep Neural Networks (DNN) are preferred.
On the neural network side, SplitNN~\cite{vepakomma2018split} was proposed to collaboratively train neural networks by splitting a neural network among participants and exchanging gradients and representations in each iteration. FATE~\cite{liu2021fate} implemented a framework that supports DNN in VFL. 
Even though FATE improves the model's capacity in VFL by supporting DNN, it still requires frequent communication between the participants for each iteration of training and therefore incurs significant communication costs as in previous VFL methods. 

To reduce the communication cost, FedBCD~\cite{liu2019communication} was proposed to leverage stale gradients for conducting local training such that participants can decrease the communication frequency from one local update to multiple updates. However, frequent iterative communication is still required for the whole training process. 
The other challenge is that the common samples across clients are usually limited in the real world, and training under such constraints may not achieve acceptable accuracy. FedCVT~\cite{kang2022fedcvt} proposes to expand the training samples by estimating representations and labels but does not address the bottleneck of communication cost.

In contrast, our proposed one-shot VFL and few-shot VFL are capable of solving the challenges of communication cost and limited common samples simultaneously.

\paragraph{Privacy in VFL.}

Privacy has become a concern of VFL since it was proposed because the clients need to send representations to the server for training, and privacy protection in VFL is well-explored. \cite{hardy2017private} presents a secure protocol that is managed by a third party, the coordinator, by employing privacy-preserving entity resolution and an additive homomorphic encryption scheme. To improve data privacy and model security, FATE~\cite{liu2021fate} applies a hybrid encryption scheme in the forward and backward stages of training. To defend the label inference attack, \cite{liu2021defending} proposes manipulating the labels following certain rules, which can be seen as a variant of label differential privacy (label DP)~\cite{chaudhuri2011sample, ghazi2021deep} in VFL. Our paper focuses on the performance and communication efficiency of VFL. However, our method does not require the clients and the server to share additional information compared with existing VFL methods and is orthogonal to existing privacy-protecting techniques, which can be directly applied to our method.


\paragraph{Semi-supervised Learning (SSL).}

SSL aims at training a model with partially labeled data, especially when the amount of labeled data is much smaller than the unlabeled ones. There have been many SSL algorithms proposed over the years. SSL algorithms can be broadly categorized as consistency regularization~\cite{bachman2014learning,rasmus2015semi}, pseudo-label methods~\cite{lee2013pseudo,miyato2018virtual,grandvalet2004semi,sohn2020fixmatch}, and generative models~\cite{kingma2013auto,doersch2016tutorial}. Consistency regularization is based on the assumption that if a realistic perturbation is applied to a data point, the prediction conducted by the trained model should not change significantly. MixMatch~\cite{berthelot2019mixmatch} applies consistency regularization along with entropy minimization and generic regularization and can achieve similar accuracy as fully supervised training approaches. Pseudo-labeling has become a component of many recent SSL techniques~\cite{miyato2018virtual}. Such methods leverage the trained model to generate pseudo labels for the unlabeled data so that the labeled training dataset is expanded. Generative models (e.g., VAE~\cite{kingma2013auto}) are trained to generate images from the data distribution and can be transferred to downstream tasks for a given task with targets.

Existing works~\cite{jeong2020federated,zhang2021improving,zhao2020semi,yang2021federated} apply SSL to FL to solve the real problem that the clients may not have enough labeled data. FSSL~\cite{jeong2020federated} learns inter-client consistency between multiple clients and splits model parameters for the server with labeled data and clients with unlabeled data separately. SemiFL~\cite{diao2021semifl} is the most recent work applying FixMatch~\cite{sohn2020fixmatch} to FL to improve the generalization of the global model. However, these methods focus on Horizontal Federated Learning (HFL), where the clients have the same feature space. Our work focuses on VFL settings where most clients have only partial features and no labels. In addition, existing deep SSL methods focus on imaging applications, while VFL has more potential for other types of data, such as tabular or multi-modal models combining imaging with other data types.


