\section{Methods}

To reduce the communication cost and improve the model performance under the settings with limited overlapping users, 
we propose two VFL methods called one-shot VFL and few-shot VFL, respectively.  


\subsection{One-shot Vertical Federated Learning}

\begin{figure}[ht]
\centering
     \includegraphics[scale=0.5]{img/one_shot.pdf}
\caption{Workflow of one-shot VFL. The clients conduct two times of uploading and one time of downloading.}
\label{fig:one-shot}
\end{figure}


We first propose one-shot VFL, in which the clients expect to receive partial gradients from the server only once. The intuition of one-shot VFL is that we can extract sufficient information that will guide clients to conduct local training from the received partial gradients. The workflow of one-shot VFL is shown in \cref{fig:one-shot}. First, the clients (e.g. the $k$-th) extract representations of the overlapping data $H_o^k$ and send the representations to the server (\circled{1}). Then, the server aligns and aggregates the received representations from all clients and computes loss with the true labels. After that, the server conducts back-propagation to compute the partial gradients of local representations $\nabla_{H_o^k} Loss$ and sends the partial gradients and the number of classes $C$ of the global classification task to corresponding (i.e., the $k$-th) clients (\circled{2}). After the $k$-th client receives the partial gradients, it conducts k-means on the partial gradients and assigns the overlapping samples $X^k_o$ temporary labels $\hat{Y}_o^k$ using the clustering index of corresponding gradients (\circled{3}). The intuition behind clustering is that the partial gradients of the same class should have similar directions while the partial gradients of different classes should have higher diversity. By clustering the partial gradients, the clients can infer information of true labels on the server to guide local training. With the temporary labels assigned to the overlapping samples, the clients conduct semi-supervised learning based on $X^k_u$ and $\{X^k_o,\hat{Y}_o^k\}$ to get updated $W_k'$ (\circled{4}). After the $k$-th client completes the local semi-supervised learning, it derives new representations ${H_o^k}'$ by computing $f_k(X^k_o;W_k')$ and sends ${H_o^k}'$ to the server (\circled{5}). Finally, the server aligns and aggregates the received new representations $\{{H_o^k}'\}_k$ to get new global representations $H_o'$ and finetunes the classifier $W_c$ using $\{H_o',Y_o\}$ (\circled{6}).

The detailed algorithm of one-shot VFL is shown in \cref{alg:one-shot}. It is notable that during the whole training process, the clients only need to upload representations to the server twice and download gradients from the server once, which is the reason we call it one-shot VFL. With such significant reduction from iterative download/upload to one-shot, we overcome the communication bottleneck in VFL. Meanwhile, local SSL conducted by clients fully utilizes the data of users that are unique to each client, and improves the performance under the realistic settings with limited overlapping samples.


\begin{algorithm}[ht] 
\footnotesize
\renewcommand{\algorithmicrequire}{\textbf{Server executes:}}
\renewcommand{\algorithmicensure}{\textbf{ClientUpdate($\nabla_{H_o^k}Loss, C$):}}
    \caption{\textbf{One-shot and few-shot VFL.} $mode$ is "few\_shot" if the server is executing few-shot VFL; $X_o^k$ and $X_u^k$ are aligned dataset and unaligned dataset of client $k$; $Y_o$ is the set of true labels of overlapping samples on the server; $C$ is the number of classes in the task; The $K$ clients are indexed by $k$; $B$ is the minibatch size; $E_s$ and $E_c$ are the number of epochs of the server and clients; $\eta_s$ and $\eta_c$ are learning rate of the server and clients; $g(.)$ and $l_{ssl}(.)$ are loss functions defined in \cref{eq:problem_def} and \cref{eq:loss_ssl}; Uploading happens in \textcolor{red}{$\gets$}; Downloading happens in \textcolor{blue}{$\gets$}.}
    \begin{algorithmic}[1] 
        \Require
        \State initialize $\theta_c$;
        \For{each client $k\in[K]$ \textbf{in parallel}}
            \State $H_o^k \textcolor{red}{\gets} f_k(X_o^k;\theta_k)$; \Comment{\circled{1} in \cref{fig:one-shot}}
        \EndFor
        \For{$k=1,...,K$}
            \State $\nabla_{H_o^k}Loss \textcolor{blue}{\gets} \nabla_{H_o^k} g\left(f_c\left(H_{o,i}^1\circ ... H_{o,i}^K\right),Y_{o}\right)$; \Comment{\circled{2} in \cref{fig:one-shot}}
        \EndFor
        \For{each client $k\in[K]$ \textbf{in parallel}}
            \State ClientUpdate($\nabla_{H_o^k}Loss, C$);\Comment{\circled{3}\circled{4} in \cref{fig:one-shot}}
            \State ${H_o^k} \textcolor{red}{\gets} f_k(X_o^k;\theta_k)$;\Comment{\circled{5} in \cref{fig:one-shot}}
            \If{$mode == $ \textit{"few\_shot"}}
                \State ${H_u^k} \textcolor{red}{\gets} f_k(X_u^k;\theta_k)$;
            \EndIf
        \EndFor
        \If{$mode == $ \textit{"few\_shot"}}
            \For{$k=1,...,K$}
                \State $\{\hat{p}_{u,i}^k\}_{i\in |H_u^k|} \textcolor{blue}{\gets} \text{InferProb}\left(\{{H_o^k}\}_k,{H_u^k}\right)$;\Comment{Defined in Alg.~\ref{alg:few-shot}}
                \State ClientUpdateFewshot($\{\hat{p}_{u,i}^k\}_{i\in |H_u^k|}$);\Comment{Defined in Alg.~\ref{alg:few-shot}}
                \State $H_o^k \textcolor{red}{\gets} f_k(X_o^k;\theta_k)$;
            \EndFor
        \EndIf
        \State $\mathcal{B_h, B_y} \gets$ (split $H_o^1\circ...\circ H_o^K$ and $Y_o$ into batches of size $B$);
        \For{epoch $i$ from 1 to $E_s$}
            \For{batch $b_h\in \mathcal{B_h}, b_y\in \mathcal{B_y}$}
                \State $\theta_c \gets \theta_c - \eta_s\nabla_{\theta_c} g\left(f_c\left(b_h\right),b_y\right)$;\Comment{\circled{6} in \cref{fig:one-shot}}
            \EndFor
        \EndFor
        
        \Ensure
        \State $\hat{Y}_o^k\gets \textit{k-means}(\nabla_{H_o^k}Loss, C)$; \Comment{\circled{3} in \cref{fig:one-shot}}
        \State $\mathcal{B_u}, \mathcal{B_o}, \mathcal{B_y} \gets$ (split $X^k_u, X^k_o, \hat{Y}_o^k$ into batches of size $B$);
        \For{epoch $i$ from 1 to $E_c$}
            \For{batch $b_u\in \mathcal{B_u}, b_o\in \mathcal{B_o}, b_y\in \mathcal{B_y}$}
                \State $\theta_k \gets \theta_k - \eta_c\nabla_{\theta_k} l_{ssl}\left(\theta_k;b_u, b_o, b_y\right)$; \Comment{\circled{4} in \cref{fig:one-shot}}
            \EndFor
        \EndFor
    \end{algorithmic}
    \label{alg:one-shot}
\end{algorithm}



% \begin{algorithm}[ht] 
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}
%     \caption{One-shot VFL.}  
%     \begin{algorithmic}[1] 
%         \Require $X^k_o$; $X^k_u$; $\nabla_{H_o^k} Loss $; Number of classes $C$;
%         \For{$k = 1, 2, ...$}
%             \State The k-th device computes representations $H_o^k$ and uploads $H_o^k$ to the server;
%         \EndFor
%         \State The server concatenate $\{H_o^k\}_k$ and caculate the partial gradients $\{\nabla_{H_o^k} Loss \}_k$;
%         \State The server sends $\{\nabla_{H_o^k} Loss \}_k$ to devices;
%         \For{$k = 1, 2, ...$}
%             \State Derive $\hat{Y}_o^k$ through k-means based on $\nabla_{H_o^k} Loss $;
%             \State Update $\theta^k$ with SSL algorithm based on $X^k_o$, $X^k_u$ and $\hat{Y}_o^k$;
%             \State Compute new representations $H_o^k$ and uploads $H_o^k$ to the server;
%         \EndFor
%         \State The server finetunes the classifier using $H_o^k$;
%     \end{algorithmic}
%     \label{alg:one-shot}
% \end{algorithm}
%
\paragraph{Local SSL.}
%
In one-shot VFL, the clients conduct semi-supervised learning (SSL) based on $X^k_u$ and $\{X^k_o,\hat{Y}_o^k\}$. For different types of data, the detailed SSL algorithms are different. The training objective of the $k$-th client can be abstracted into 
%
\begin{equation}
    l_{ssl}\left(\theta_k;X^k_u, X^k_o, \hat{Y}_o^k\right) = l_s\left(\theta_k;X^k_o, \hat{Y}_o^k\right) + \lambda_ul_{u}\left(\theta_k;X^k_u\right),
    \label{eq:loss_ssl}
\end{equation}
%
where $l_s(.)$ is the supervised training loss, $l_{u}(.)$ is the unsupervised training loss, $\lambda_u$ controls the trade-off between the supervised loss and unsupervised loss. In this paper, we focus on two types of data: image data and tabular data, which are common use cases of VFL. Plenty of SSL algorithms~\cite{sohn2020fixmatch,berthelot2019mixmatch} have been proposed for image recognition, and we apply FixMatch~\cite{sohn2020fixmatch} for the clients conducting SSL on image data, which is a widely applied SSL algorithm in image recognition. On the other hand, in order to fit DL to the task of tabular data SSL, we modify the augmentation methods in FixMatch and propose our FixMatch-tab algorithm for local SSL of tabular data. 
%
\begin{figure}[ht]
\centering
     \includegraphics[scale=0.3]{img/mask.png}
\caption{Features are randomly masked for data augmentation.}
\label{fig:mask}
\end{figure}
%
%
We modify the weak augmentation $\alpha(.)$ and strong augmentation $\mathcal{A}(.)$ in FixMatch to adapt it to the tabular data. For weak augmentation, we randomly generate a binary mask $m$ with the same shape of the data point. Each element of $m$ is sampled from a Bernoulli distribution. We replace the masked elements with the mean value of the corresponding elements of local data. For the strong augmentation, we add noise to the masked samples. Thus, when we train a data point $x$ in FixMatch-tab, we first sample a binary mask $m$ for both weak and strong augmentation and sample a noise vector $n$ for strong augmentation where
%
{\small
\begin{equation}
\begin{aligned}
    &m_i \sim B(1,r_m),\\
    &n_i \sim N(0,\sigma^2),
\end{aligned}
\end{equation}
}%
%
$r_m$ is the expected ratio of elements that are masked and $\sigma^2$ is variance of the noise. Then, the weak augmentation $\alpha$ and strong augmentation $\mathcal{A}(.)$ for this sample in FixMatch-tab are formulated as 
%
{\small
\begin{equation}
\begin{aligned}
    &\alpha\left(x\right) = m\otimes x + (1-m)\otimes \bar{x},\\
    &\mathbb{A}\left(x\right) = \alpha\left(x\right) + n,
\end{aligned}
\end{equation}
}%
%
where $\bar{x}=\frac{1}{N}\sum_{j\in [N]}x_j$ and $N$ is the number of local samples.
%
%
%
% \begin{figure*}[ht]
% \centering
%      \includegraphics[scale=0.35]{img/few_shot.pdf}
% \caption{Overview of few-shot VFL.}
% \label{fig:few_shot_vfl}
% \end{figure*}
%
%
\subsection{Few-shot Vertical Federated Learning}
%
\begin{figure}[ht]
\centering
     \includegraphics[scale=0.4]{img/toy_example.png}
\caption{A client does not have enough information to generate reasonable pseudo labels from either the left or right part of the image alone during local semi-supervised learning (SSL).}
\label{fig:toy_example}
\end{figure}




\begin{figure*}[ht]
\centering
     \includegraphics[scale=0.4]{img/few_shot_server.pdf}
\caption{The server judges whether local samples contain enough information to generate accurate pseudo labels in few-shot learning. The clients conduct local SSL with the expanded labeled dataset.}
\label{fig:few_shot_server}
\end{figure*}

\begin{algorithm}[ht] 
\footnotesize
\renewcommand{\algorithmicrequire}{\textbf{ClientUpdateFewshot($\{\hat{p}_{u,i}^k\}_{i\in |X_u^k|}$):}}
\renewcommand{\algorithmicensure}{\textbf{InferProb$\left(\{{H_o^k}\}_k,{H_u^k}\right)$:}}
    \caption{Local SSL training process with expanded labeled dataset in few-shot VFL. $H_o^{[K]\backslash\{k\}}$ stands for $H_o^1\circ,...,\circ H_o^{k-1}\circ H_o^{k+1}\circ,...,\circ H_o^K$ }
    \begin{algorithmic}[1] 
        \Ensure
        \For{$k=1,...,K$} \Comment{This is executed only by once}
            \State $\theta_c^k \gets \argmin_{\theta} g\left(f_c^k\left(H_o^k;\theta\right), Y_o\right)$; \Comment{Optimize with SGD, \circled{2} in \cref{fig:few_shot_server}}
        \EndFor
        \State $\theta_c \gets $ execute line 22-27 in Alg.~\ref{alg:one-shot}; \Comment{\circled{2} in \cref{fig:few_shot_server}}
        \State $\hat{H}_u^{[K]\backslash\{k\}} \gets T \left(H_u^k, H_o^k, H_o^{[K]\backslash\{k\}}\right)$; \Comment{\circled{3} in \cref{fig:few_shot_server}}
        \For{$i = 1,...,|H_u^k|$}
            \State $\hat{p}_{u,i}^k \gets$ compute \cref{eq:probs} and \cref{eq:pseudo_probs}; \Comment{\circled{4} in \cref{fig:few_shot_server}}
        \EndFor
        \State \Return $\{\hat{p}_{u,i}^k\}_{i\in |H_u^k|}$;
        
        \Require
        \State $X_{uc}^k \gets \text{sample from } X_u^k \text{ with probability } \{\hat{p}_{u,i}^k\}_{i\in |X_u^k|}$; \Comment{\circled{5} in \cref{fig:few_shot_server}}
        \State $\hat{Y}_{uc}^k \gets f_k\left( X_{uc}^k;\theta_k\right)$; \Comment{\circled{5} in \cref{fig:few_shot_server}};
        \State $X_o^{k\prime}\gets X_o^k \cup X_{uc}^k$; ${\hat{Y}_o}^{k\prime}\gets \hat{Y}_o^k \cup \hat{Y}_{uc}^k$;
        \State ${X_u^k}'\gets X_o^k \backslash X_{uc}^k$;
        \State $\mathcal{B_u}, \mathcal{B_o}, \mathcal{B_y} \gets$ (split $X_u^{k\prime}, X_o^{k\prime}, \hat{Y}_o^{k\prime}$ into batches of size $B$);
        \For{epoch $i$ from 1 to $E_c$}
            \For{batch $b_u\in \mathcal{B_u}, b_o\in \mathcal{B_o}, b_y\in \mathcal{B_y}$}
                \State $\theta_k \gets \theta_k - \eta_c\nabla_{\theta_k} l_{ssl}\left(\theta_k;b_u, b_o, b_y\right)$; \Comment{\circled{6} in \cref{fig:few_shot_server}}
            \EndFor
        \EndFor
        \end{algorithmic}
    \label{alg:few-shot}
\end{algorithm}


Even though one-shot VFL can achieve high performance of the global model with extremely low communication cost, we consider improving the performance further by paying with a few more rounds of communication. The key factor to improve SSL performance is to have a larger labeled dataset. Thus, we propose to expand the supervised learning dataset on clients in VFL. One intuitive idea is to assign pseudo labels from local predictions to the unlabeled data points if those predictions have a high confidence. By doing this, we would only need to modify the local training procedure of one-shot VFL without introducing additional communication cost. However, there is one potential problem this method cannot solve. Considering a toy example shown in \cref{fig:toy_example}. Two clients participate in training a image classification task, and each client has access to a half of each image. If the two shapes on the image are the same, the image is positive, and negative if both shapes are different. If we want to improve the performance by enlarging the labeled dataset, the key is to generate pseudo labels with high accuracy. However, in this toy example, it is impossible for the clients to generate reasonable pseudo labels based on only half of the images since they do not have enough information to infer the true label of the image.


To solve this problem, we propose few-shot VFL as shown in \cref{alg:one-shot}. The main difference with one-shot VFL is that the server in few-shot VFL estimates the missing part of the representations for each client's unaligned data which shall expand their labeled datasets. The detailed pipeline of line 17-18 in \cref{alg:one-shot} is shown in \cref{fig:few_shot_server}. In this section we focus on VFL with two clients (A \& B), but the proposed method can be naturally extended to the scenario where there are more than two clients. When the server receives the representations $H_{u}^A$ of client A's unaligned data (\circled{1} in \cref{fig:few_shot_server}), it estimates the missing representations  $\hat{H}_u^B$ of corresponding samples on client B (does not exist for unaligned data) with a transform layer $T$ (\circled{3} in \cref{fig:few_shot_server}) as
%
{\small
\begin{equation}
    \hat{H}_u^B = T(H_{u}^A, H_{o}^A, H_{o}^B).
\end{equation}
}%
%
The details of $T(.)$ will be introduced later. Then for each unaligned sample (says the $i$-th), the server produces predictions $\{\hat{y}_{u,i}^A, \hat{y}_{u,i}^{A,B}\}$ and probabilities $\{p_{u,i}^A, p_{u,i}^{A,B}\}$ following 
%
{\small
\begin{equation}
    \begin{aligned}
        \hat{y}_{u,i}^A &= \argmax_{j} f_c^A(H_{u,i}^A;\theta_c^{A})_j,\\
        p_{u,i}^A &= \max_{j} f_c^A(H_{u,i}^A;\theta_c^{A})_j,\\
        \hat{y}_{u,i}^{A,B} &= \argmax_{j} f_c(H_{u,i}^{A}\circ \hat{H}_{u,i}^{B};\theta_c^{A,B})_j,\\
        p_{u,i}^{A,B} &= \max_{j} f_c(H_{u,i}^{A}\circ \hat{H}_{u,i}^{B};\theta_c^{A,B})_j.
    \label{eq:probs}
    \end{aligned}
\end{equation}
}%
%
where $f_c^A(.)$ is an auxiliary classifier whose input is $h_{u,i}^A$. $\theta_c^{A}$ and $\theta_c^{A,B}$ are trained (\circled{2} in \cref{fig:few_shot_server}) based on the overlapping samples. If the predictions $\{\hat{y}_{u,i}^A, \hat{y}_{u,i}^{A,B}\}$ based on local and estimated global representations are the same with high confidence, the local representation $h_{u,i}^A$ contains enough information and should be given a pseudo label during the local SSL on client A. To reduce the noise from misleading pseudo labels, the server sets a probability $\hat{p}_{u,i}^A$ for each unaligned sample to be given a pseudo-label during local training following
%
{\small
\begin{equation}
    \hat{p}_{u,i}^A = \mathbb{1}\left(\hat{y}_{u,i}^A = \hat{y}_{u,i}^{A,B}\right)\mathbb{1}\left(p_{u,i}^A > t\right)\mathbb{1}\left(p_{u,i}^{A,B} > t\right)p_{u,i}^{A,B}.
    \label{eq:pseudo_probs}
\end{equation}
}%
%
The intuition of $\hat{p}_{u,i}^A$ is that with the higher confidence the local representation $h_{u,i}^A$ is predicted the same label with the global representation $h_{u,i}^A\circ\hat{h}_{u,i}^B$, the larger probability $\hat{p}_{u,i}^A$ with which the $i$-th unaligned sample on client A should be given a pseudo label during local training. In the following, we will introduce the representation transform layer $T(.)$ and local SSL with probability set $\{\hat{p}_{u,i}^A\}_i$.
%
\paragraph{Efficient Representation Estimation.} We design the representation transform layer utilizing a \textit{scaled dot product attention} (SDPA) function formulated as
%
{\small
\begin{equation}
\begin{aligned}
    \hat{H}_u^B &= T(H_{u}^A, H_{o}^A, H_{o}^B)\\
    &= softmax(\frac{H_{u}^A \otimes {H_{o}^A}^T}{\sqrt{d}})\otimes{H_{o}^B},
\end{aligned}
\end{equation}
}%
%
where $\otimes$ is matrix multiplication operator, and $d$ is the dimension of representation. With $T(.)$, Each missing representation is estimated through the weighted sum over representations of overlapped samples. The weight matrix $W_A=softmax(\frac{H_{u}^A \otimes {H_{o}^A}^T}{\sqrt{d}})$ reflects the similarity between the representation to be estimated and the aligned representations in client A.

We apply the SDPA function rather than a generative model (e.g., GAN) to estimate representations for two reasons. First, the generative model has to be trained on the representations of overlapping samples. However, the amount of overlapping samples could be too small in real life to train a generator with good performance. Second, when there are $K$ clients, the server needs to train $K$ generators, which introduces heavy computational overhead. By applying the SDPA function to estimate representations, we can overcome the problem of limited overlapping samples and improve the computational efficiency of our estimation.
%
\paragraph{Local SSL with Expanded Supervised Dataset.}
%
%
After the client A receives $\{\hat{p}_{u,i}^A\}$ (\circled{4} in \cref{fig:few_shot_server}), it samples a subset denoted as $X_{uc}^A$ from $X_u^A$ with probabilities $\{\hat{p}_{u,i}^A\}$. For each sample $x_{uc,i}^A$ in $X_{uc}^A$, client A assigns the pseudo label $\hat{y}_{uc,i}^A$ as the prediction of the local model that was learned using SSL on client A. For simplicity, the set of pseudo labels $\hat{y}_{uc,i}^A$ is denoted as $\hat{Y}_{uc}^A$. In such way, client A expands the supervised data via SSL. Hence, the objective of SSL (\circled{6} in \cref{fig:few_shot_server}) on client A can be formulated as
%
{\small
\begin{equation}
    \begin{aligned}
    &l_{ssl}\left(\theta;X^A_u\backslash X_{uc}^A, X^A_o\cup X_{uc}^A, \hat{Y}_o^A\cup \hat{Y}_{uc}^A\right) \\
    = &l_s\left(\theta;X^A_o\cup X_{uc}^A, \hat{Y}_o^A\cup \hat{Y}_{uc}^A\right) + \lambda_ul_{u}\left(\theta;X^A_u\backslash X_{uc}^A\right).
    \end{aligned}
\end{equation}
}



