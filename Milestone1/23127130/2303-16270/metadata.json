{
    "arxiv_id": "2303.16270",
    "paper_title": "Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples",
    "authors": [
        "Jingwei Sun",
        "Ziyue Xu",
        "Dong Yang",
        "Vishwesh Nath",
        "Wenqi Li",
        "Can Zhao",
        "Daguang Xu",
        "Yiran Chen",
        "Holger R. Roth"
    ],
    "submission_date": "2023-03-28",
    "revised_dates": [
        "2023-03-30"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG"
    ],
    "abstract": "Federated learning is a popular collaborative learning approach that enables clients to train a global model without sharing their local data. Vertical federated learning (VFL) deals with scenarios in which the data on clients have different feature spaces but share some overlapping samples. Existing VFL approaches suffer from high communication costs and cannot deal efficiently with limited overlapping samples commonly seen in the real world. We propose a practical vertical federated learning (VFL) framework called \\textbf{one-shot VFL} that can solve the communication bottleneck and the problem of limited overlapping samples simultaneously based on semi-supervised learning. We also propose \\textbf{few-shot VFL} to improve the accuracy further with just one more communication round between the server and the clients. In our proposed framework, the clients only need to communicate with the server once or only a few times. We evaluate the proposed VFL framework on both image and tabular datasets. Our methods can improve the accuracy by more than 46.5\\% and reduce the communication cost by more than 330$\\times$ compared with state-of-the-art VFL methods when evaluated on CIFAR-10. Our code will be made publicly available at \\url{https://nvidia.github.io/NVFlare/research/one-shot-vfl}.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16270v1"
    ],
    "publication_venue": null
}