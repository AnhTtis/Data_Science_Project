\begin{figure*}[!ht]
\begin{center}     %%% not \center
\subfigure[Ground truth local dataset]{\label{fig:ground_truth_recon}\includegraphics[width=0.65\columnwidth]{plots-images/example_recon_original.png}}\hspace{5mm}%
\subfigure[\name reconstruction]{\label{fig:mandrake_recon}\includegraphics[width=0.65\columnwidth]{plots-images/example_recon_mandrake.png}}\hspace{5mm}%
\subfigure[Robbing the Fed~\cite{fowl2022robbing} reconstruction]{\label{fig:rtf_recon}\includegraphics[width=0.65\columnwidth]{plots-images/example_recon_rtf.png}}
\end{center}
\vspace*{-3mm}
\caption{\label{fig:reconstruction_examples_cifar100} \name and Robbing the Fed (RtF) reconstructions on CIFAR-100 for 1 client out of 100 in FedAVG aggregation. Clients use 5 epochs with 8 local iterations and a local mini-batch size 8. Empty boxes indicate images were not reconstructed because multiple images activated a neuron. In the case of RtF, nearly all empty boxes occur due to precision problems causing the computed gradient to be zero rather than multiple activations.}
\vspace*{-3mm}
\end{figure*}

In this section, we provide experimental results for \name on aggregated updates in FedAVG. We assess our baseline attack on the CIFAR-100~\cite{krizhevsky2009learning}, Tiny ImageNet\cite{le2015tiny}, and MNIST~\cite{lecun1998mnist} datasets. The leakage module is used with a ResNet-50~\cite{he2016deep}, but the benign model itself does not affect reconstruction. The FC layer weights measure the average pixel intensity and the biases of the FC layer are set up according to the dataset distribution (known to the server or learned through the first few training iterations). Unless otherwise specified, we have 4 units in the FC layer per image for \name and Robbing the Fed (RtF)~\cite{fowl2022robbing}. 
% We scale the FC layer for RtF and the convolutional kernels ($\textit{input channels}\cdot \textit{num. clients}$) for \name based on the number of clients. This leads to RtF having a roughly $2\times$ larger model overhead, which is where both methods achieve the same leakage rate in FedSGD, Appendix~\ref{sec:appendix_additional}.

We first show the leakage rate that \name achieves for each dataset in FedAVG before also showing the scalability in FedSGD. Further experiments show various FedAVG training settings, the effects of the CSF, a non-IID attack which we evaluate using the OrganAMNIST~\cite{medmnistv2} dataset (which has larger distribution differences when using a class-based non-IID skew compared to CIFAR-100), and differential privacy. Some additional experimental results are shown in Appendix~\ref{sec:appendix}.

\begin{figure}[t!]
\vspace{-1mm}
\begin{center}
\includegraphics[width=0.9\columnwidth,trim={0mm 0mm 0mm 11mm},clip]{plots-images/rtf_fedavg_leakr_2.pdf}
%\centerline{\small (a) Only Benign updates aggregated}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:scalability_rtf} Leakage rate of \name with and without model inconsistency (MI) and Robbing the Fed (RtF) for a number of clients between 1-500 on CIFAR-100. Clients train with 8 local iterations of mini-batch size 8.}
\vspace*{-2mm}
\end{figure}

\begin{table}[!t]
\small
\begin{center}
% \begin{tabular}{|p{0.19\columnwidth}|p{0.25\columnwidth}|p{0.23\columnwidth}|p{0.13\columnwidth}|} 
\begin{tabular}{|p{0.174\columnwidth}|c|c c|} 
\hline
% & Num. of clients & Leaked images & Total\newline images & Leakage rate \\
\textbf{Dataset} & \textbf{Metrics} & \textbf{\name} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{RtF~\cite{fowl2022robbing}} \\ \textbf{+ MI~\cite{pasquini2021eluding}}\end{tabular}} \\
\hline
% \textbf{Num clients} & 85 & 15 & 100 \\
\multirow{3}{*}{\textbf{CIFAR-100}} & Leaked imgs & 5290 & 50 \\
& Total imgs & 6400 & 6400 \\
& \textbf{Leakage rate} & \textbf{82.66\%} & \textbf{0.78\%} \\
\hline
\multirow{3}{0.19\columnwidth}{\textbf{Tiny ImageNet}} & Leaked imgs & 5202 & 49 \\
& Total imgs & 6400 & 6400 \\
& \textbf{Leakage rate} & \textbf{81.28\%} & \textbf{0.77\%} \\
\hline
\multirow{3}{*}{\textbf{MNIST}} & Leaked imgs & 4907 & 49 \\
& Total imgs & 6400 & 6400 \\
& \textbf{Leakage rate} & \textbf{76.67\%} & \textbf{0.77\%} \\
\hline
\end{tabular}
\end{center}
\vspace*{-0mm}
\caption{\label{tab:leakage_table} Leakage rate for FedAVG aggregated update with 100 participating clients. $\alpha=1e-4$, $\textit{CSF}=100$ and 5 local epochs of 8 iterations of mini batch size 8 used. }
\vspace*{-8mm}
\end{table}

\iffalse
\begin{figure*}[!t]
\begin{minipage}[c]{1.0\linewidth}
  \begin{minipage}[c]{0.32\columnwidth}	
      \centering
      \includegraphics[width=\textwidth]{plots-images/rtf_fedavg_leakr.pdf}
      %\centerline{\small (a) Only Benign updates aggregated}
      \caption{Leakage rate of \name and Robbing the Fed (RtF)~\cite{fowl2022robbing} for a varying number of clients between 1-500 on CIFAR-100. Each client trains with 8 local iterations of mini-batch size 8.}
      \label{fig:scalability_rtf}
      %Leakage rate of \name and Robbing the Fed (RtF)~\cite{fowl2022robbing} for a varying number of clients between 1-500 on CIFAR-100. Each client trains with 8 local iterations of mini-batch size 8. \textcolor{red}{(Change name from MANDRAKE to the new name of the attack in plot)}
  \end{minipage}
  \hspace{1 mm}
  \begin{minipage}[c]{0.32\columnwidth}	
      \centering
      \includegraphics[width=\textwidth]{plots-images/model_inconsistency_size.pdf}
      %\centerline{\small (a) Only Benign updates aggregated}
      \caption{Model size of \name relative to the size added by Robbing the Fed (RtF) for a 100 client attack. The number of clients sharing each separate model is equal across all models.}
      \label{fig:model-inconsistency-size}
  \end{minipage}
  \hspace{1 mm}
  \begin{minipage}[c]{0.32\columnwidth}	
      \centering
      \resizebox{\textwidth}{!}{%
      \begin{tabular}{|c|c|c c|}%{|p{0.174\columnwidth}|c|c c|} 
      \hline
      % & Num. of clients & Leaked images & Total\newline images & Leakage rate \\
      \textbf{Dataset} & \textbf{Metrics} & \textbf{\name} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{RtF~\cite{fowl2022robbing}} \\ \textbf{+ MI~\cite{pasquini2021eluding}}\end{tabular}} \\
      \hline
      % \textbf{Num clients} & 85 & 15 & 100 \\
      \multirow{3}{*}{\textbf{CIFAR-100}} & Leaked imgs & 5290 & 50 \\
      & Total imgs & 6400 & 6400 \\
      & \textbf{Leakage rate} & \textbf{82.66\%} & \textbf{0.78\%} \\
      \hline
      \multirow{3}{0.19\columnwidth}{\textbf{Tiny ImageNet}} & Leaked imgs & 5202 & 49 \\
      & Total imgs & 6400 & 6400 \\
      & \textbf{Leakage rate} & \textbf{81.28\%} & \textbf{0.77\%} \\
      \hline
      \multirow{3}{*}{\textbf{MNIST}} & Leaked imgs & 4907 & 49 \\
      & Total imgs & 6400 & 6400 \\
      & \textbf{Leakage rate} & \textbf{76.67\%} & \textbf{0.77\%} \\
      \hline
      \end{tabular}
      }
      \captionof{table}{Leakage rate for FedAVG aggregated update with 100 participating clients. $\alpha=1e-4$, \textit{CSF}$=100$ and 5 local epochs of 8 iterations of mini batch size 8 used.}
      \label{tab:leakage_table} 
  \end{minipage}
\vspace*{-3mm}
\end{minipage}
\end{figure*}
\fi

\vspace{-1mm}
\subsection{FedAVG aggregation attack}
\label{sec:fedavg_leakage_rate}
\vspace{-2mm}
Figure~\ref{fig:scalability_rtf} shows the leakage rate for RtF with a varying numbers of clients between 1-500 averaged over 10 runs. We define an image as leaked only if a single image activates the neuron and has a reasonable reconstruction quality with SSIM above 0.5. If two or more different images (not same image across multiple epochs) activate the neuron, even with a high SSIM, we do not count it as leaked. Clients are trained with with 8 local iterations with mini-batch size 8 on CIFAR-100, and learning rate $\alpha=1e-4$. We use \name with $\textit{CSF}=100$ ($\textit{CSF}=100\cdot\textit{num. of clients}$ when not using model inconsistency). While RtF is applicable to a smaller numbers of clients, the leakage rate very quickly decreases as the number of clients increases. With only 15 clients, the leakage rate of RtF drops to 7.57$\%$. With 30 clients, the leakage rate drops to 0.95\%. As the FC layer size increases with the number of clients, the leakage rate decreases due to precision problems.
% SB (4/13/23): Recap why RtF is degrading.
% JZ (4/13/23): Added
We use a relatively small number of local iterations and mini-batch size, but if either increase, RtF will function on even fewer clients. \name is unaffected by training parameters (learning rate, local iterations, mini-batch size) and achieves a high leakage rate regardless of the number of clients. Without model inconsistency, \name has a slightly lower leakage rate but still achieves scalability with no impact on reconstruction quality.
\begin{comment}
RtF in FedAVG is a small scale attack, achieving the highest leakage rate only when the number of clients is 1-2. \name has a consistent leakage rate, and even with just 1-2 clients, it still achieves a higher leakage rate by preventing multiple activations between local iterations. Increasing the \textit{CSF} value more can further push the leakage rate higher, which we will discuss in Section~\ref{sec:csf_experiments}. 
\end{comment}

Figure~\ref{fig:reconstruction_examples_cifar100} shows the reconstruction of a client for \name and RtF~\cite{fowl2022robbing} for the FedAVG attack on 100 clients. We do not use the bias for any reconstructions as discussed in Section~\ref{sec:reconstruction_equation}. The images reconstructed by \name are not affected by the aggregation of many clients and are very clearly identifiable. However, RtF is unable to reconstruct images correctly, with only 1 image having a non-zero computed weight gradient out of the FedAVG update. However, even after proper normalization, the image is visually incorrect due to precision errors. We find that a large majority of clients do not have even a single image with a non-zero gradient. 
Appendix Figure~\ref{fig:RtF_recon_clients} shows the top-8 SSIM reconstructed images across all clients in RtF for 10, 25, and 50 clients. As the number of clients increases, both the quantity and the quality of reconstructed images decrease.

We use the same FedAVG settings as above and additionally add 5 local epochs and compare the leakage rate for 100 clients. Table~\ref{tab:leakage_table} shows the leakage rate of \name and RtF + model inconsistency (MI)~\cite{pasquini2021eluding} on CIFAR-100, Tiny ImageNet, and MNIST. We do not compare to RtF alone, as it is unable to leak images in this setting. As discussed previously, increasing the FC layer size results in more precision problems and decreasing the size also does not help due to increased overlapping neuron activations. Using RtF with MI achieves the current SOTA for a large scale FedAVG secure aggregation attack and allows the attack to reach a single client. \name achieves a significantly higher leakage rate as it reaches all clients and scales to aggregation. Increasing the size of the FC layer will also allow the leakage rate to increase (for \name, as RtF runs into precision problems beyond a point as discussed in Section~\ref{sec:FedAVG_CSF}). 

\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.9\columnwidth,trim={0 0 0 10mm},clip]{plots-images/wtc_fedsgd_leakr.pdf}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:wtc_leakage} Leakage rate on CIFAR-100 for the trap weights (TW) attack~\cite{boenisch2021curious} and \name + TW for a varying number of clients in FedSGD with a batch size of 64. Leakage rate given as an average over 10 runs. The TW attack leakage rate decreases as the number of clients increases, but using the convolutional attack fixes the scalability problem through split scaling.}
\vspace*{-5mm}
\end{figure}

\vspace{-2mm}
\subsection{FedSGD aggregation attack}
\vspace{-2mm}
We also show the applicability of \name to helping the trap weights (TW)~\cite{boenisch2021curious} attack in FedSGD aggregation. Figure~\ref{fig:wtc_leakage} shows the leakage rate of the TW attack and \name + TW for a varying number of clients between 1-50. Clients train with a batch size of 64 on CIFAR-100 and we use an FC layer size of $10\times$ the total number of images for the TW attack. For each setting of the number of clients, we use the TW scaling factor that achieves the highest leakage rate tested by 0.01 increments. For 1-50 clients, we find that scaling factors between 0.91-0.96 achieve the highest leakage rates, with lower values needed for higher numbers of clients. Despite tuning, the TW attack still suffers from a decreasing leakage rate as the number of clients increases. However, using the convolutional attack of \name in addition to TW allows the scaling to split between the batch size and number of clients and maintains a constant leakage rate regardless of the number of clients.


\begin{figure}[t!]
\begin{center}     %%% not \center
% \begin{minipage}[c]{1.0\linewidth}
%   \begin{minipage}[c]{0.49\linewidth}
%   \centering
%   \includegraphics[width=\textwidth,trim={3mm 0mm 3mm 10mm},clip]{plots-images/dataset_size_leakage_rate.pdf}
%   \label{fig:dataset_size_leakage_rate}
%   \end{minipage}
%   \begin{minipage}[c]{0.49\linewidth}
%   \centering
%   \includegraphics[width=\textwidth,trim={3mm 0mm 3mm 10mm},clip]{plots-images/dataset_size_images.pdf}
%   \label{fig:dataset_size_images}
%   \end{minipage}  
\subfigure[Leakage rate]{\label{fig:dataset_size_leakage_rate}\includegraphics[width=0.5\columnwidth,trim={3mm 0mm 3mm 10mm},clip]{plots-images/dataset_size_leakage_rate.pdf}}
\hspace{-4 mm}
\subfigure[Number of leaked images]
% SB (4/3/23): Just say number of leaked images
{\label{fig:dataset_size_images}\includegraphics[width=0.5\columnwidth,trim={3mm 0mm 3mm 10mm},clip]{plots-images/dataset_size_images.pdf}}
\end{center}
\vspace*{-4mm}
\caption{For \name, (a) leakage rate and (b) number of leaked images as a function of the local dataset size and FC layer size averaged over 10 clients. While the overall leakage rate decreases with a larger local dataset size, the total number of leaked images continues to increase.}
%\label{fig:dataset_size_experiments} 
% \vspace*{-3mm}
% \end{minipage}
\end{figure}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.9\columnwidth,trim={0mm 0mm 0mm 11mm},clip]{plots-images/model_inconsistency_size.pdf}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:model-inconsistency-size} Model size of \name relative to the size added by Robbing the Fed (RtF) for a 100 client attack. The number of clients sharing each separate model is equal across all models. \name's size is $50.69\%-100\%$ of RtF's.}
\vspace*{-5mm}
\end{figure}

\vspace{-1mm}
\subsection{Local dataset size and FC layer size}
\vspace{-2mm}
The leakage rate is affected by the local dataset size and the FC layer size. In this section we experiment with how the changes to both affect overall leakage rate. We fix the local mini-batch size to 8 and vary the number of local iterations as the dataset size increases. We use $\alpha=1e-4$ and $\textit{CSF}=500$ for all settings and train on CIFAR-100. For each local dataset size, we compute the leakage rate with an FC layer size of 64, 128, 256, and 512 as an average over 10 clients. Figure~\ref{fig:dataset_size_leakage_rate} shows the average leakage rate for each setting as the local dataset size increases from 32-256.

Figure~\ref{fig:dataset_size_images} shows the average number of leaked images per client with the varying local dataset size. While the overall leakage rate decreases as the local dataset size increases, the total number of leaked images continues to increase. The local iteration parameter changes prevent multiple images between separate local iterations from activating the same neuron. As a result, even as the ratio of images to neurons increases, the number of leaked images can also continue to increase without having issues with multiple activations on the same neuron preventing reconstruction, up until the point where nearly each neuron leaks a separate image. Appendix Figure~\ref{fig:dataset_size_local_iterations} shows the effect of the number of local iterations on the leakage rate.

\vspace{-1mm}
\subsection{Convolutional scaling factor}
\label{sec:csf_experiments}
\vspace{-2mm}
While the convolutional scaling factor (\textit{CSF}) allows us to have attacks without model inconsistency, there is a trade-off in model size. Figure~\ref{fig:model-inconsistency-size} shows the model size for varying levels of model inconsistency (number of clients sharing the same model) relative to the size added by RtF when they both achieve the same leakage rate in FedSGD attacking 100 clients in aggregation (FedSGD instead of FedAVG, since RtF cannot scale in FedAVG due to precision). This happens when the effective number of bins, $\textit{number of identity mapping sets}\times\textit{FC layer size}$, is the same as the total FC layer size in RtF. With full model inconsistency, \name adds only $50.69\%$ of the size of the RtF attack. With no model inconsistency, \name is the same size at $100.00\%$. \name maintains the same leakage rate for all points. Recall that RtF has no model inconsistency. 

\begin{figure}[t!]
% \vspace{-5mm}
% \begin{minipage}[c]{1.0\linewidth}
%   \begin{minipage}[c]{0.49\linewidth}
%   \centering
%   \includegraphics[width=\textwidth]{plots-images/csf_leak_psnr.pdf}
%   \label{fig:csf-leak-psnr}
%   \end{minipage}
%   \begin{minipage}[c]{0.49\linewidth}
%   \centering
%   \includegraphics[width=\textwidth]{plots-images/csf_ssim_lpips.pdf}
%   \label{fig:csf-ssim-lpips}
%   \end{minipage}
\begin{center}     %%% not \center
\subfigure[Leakage rate and PSNR]{\label{fig:csf-leak-psnr}\includegraphics[width=0.51\columnwidth,trim={0 4mm 0 3mm},clip]{plots-images/csf_leak_psnr.pdf}}
\hspace{-4mm}
\subfigure[SSIM and LPIPS]{\label{fig:csf-ssim-lpips}\includegraphics[width=0.51\columnwidth,trim={0 4mm 0 3mm},clip]{plots-images/csf_ssim_lpips.pdf}}
\end{center}
\vspace*{-4mm}
\caption{\label{fig:csf_metrics} For \name (a) leakage rate and PSNR$\uparrow$, (b) SSIM$\uparrow$ and LPIPS$\downarrow$ as a function of the \textit{CSF}. The leakage rate and PSNR increase over a larger range of \textit{CSF} values while the SSIM and LPIPS scores stop improving much quicker.}
% \end{minipage}
\vspace*{-5mm}
\end{figure}

The other purpose of the CSF is to mitigate changes in the identity mapping kernels and prevent images from activating the same neurons. Figure~\ref{fig:csf_metrics} show the leakage rate and average PSNR$\uparrow$, SSIM$\uparrow$, and LPIPS$\downarrow$ scores of reconstructed images when using various \textit{CSF} values (we report the average of reconstructed images without activation overlap, even when the SSIM is below 0.5 in order to properly show the impact of the \textit{CSF} on the metrics).
We average the values over 10 clients in FedAVG aggregation training on CIFAR-100 using 8 local iterations of mini-batch size 8 and $\alpha=1e-4$. With very small \textit{CSF} values, the leakage rate is much lower because the convolutional kernel parameter changes prevent images from being pushed through correctly. Similarly, we see lower metric scores for reconstructed images. High \textit{CSF} values result in larger changes to the FC layer weights and a higher leakage rate by preventing images from activating the same neurons. Due to smaller changes in the identity mapping sets, the reconstruction metrics also improve. The PSNR score improves over a large range of CSF values (continuing beyond $\textit{CSF}=500$) while the SSIM and LPIPS score improvement stops much quicker. % There is no downside to having a larger \textit{CSF}, but the improvement in leakage rate eventually stops. 
For example, the attack achieves a $72.59\%$ leakage rate when $\textit{CSF}=1$ and increases until $\textit{CSF}=500$ where it peaks at $85.47\%$ and no longer increases with larger \textit{CSF} values.

\vspace{-1mm}
\subsection{Non-IID federated learning}
\vspace{-1mm}
Previous experiments worked under the setting that client data was IID and the biases were initialized the same for everyone. In these experiments, the clients contain non-IID distributions. We use OrganAMNIST instead of the CIFAR dataset, as it has a less uniform average pixel intensity distribution which is shown in Appendix Figure~\ref{fig:distribution-datasets}. For OrganAMNIST, label-based separation also results in individual distributions with larger differences in mean and SD. For this separation, each client has data from a single class. Since OrganAMNIST only has 11 classes, several clients share data from the same label. However, this is not known to the server so the distributions of clients are learned independently. We use a dataset agnostic bias initialization for the first round, where all clients have initial biases with mean -0.5 and SD 0.25 following a normal distribution. This value is adjusted after each training round separately for each client based on observing the activated biases.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.9\columnwidth,trim={0 0 0 10mm},clip]{plots-images/non_iid_leak.pdf}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:organ-leakage-results} [Effect of non-IID clients] Number of images leaked for various local mini-batch sizes training on the OrganAMNIST dataset with $100$ non-IID clients over several training rounds. The first training iteration starts with a dataset agnostic bias initialization and subsequent training rounds improve through observing activations. The attacks improve significantly after a single round.}
\vspace*{-3mm}
\end{figure}

We use $100$ clients for each experiment each training over 8 local iterations, but we vary the local mini-batch size as $8$, $4$, and $2$ (total of $64$, $32$, and $16$ images). The FC layer size is setup equal to $4\times$ the total number of images and we use $\textit{CSF}=100$. Figure~\ref{fig:organ-leakage-results} shows the number of images leaked over several training rounds for the different local mini-batch sizes. The total number of images for each local mini-batch size is $6400$, $3200$, and $1600$ respectively. After just the first training round of observing neuron activations, a jump in leaked images occurs for all mini-batch sizes. An increase of $62.32\%$, $59.53\%$, and $50.92\%$ leaked images is observed for local mini-batch sizes $8$, $4$, and $2$ respectively. After the initial jump, the total leakage fluctuates slightly between rounds due to randomness of the client mini-batch. 

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.9\columnwidth,trim={0 0 0 2mm},clip]{plots-images/dp_psnr_leak.pdf}
\end{center}
\vspace*{-6mm}
\caption{\label{fig:differential_privacy} Leakage rate and average PSNR for client reconstruction in CIFAR-100 under $\sigma$ Gaussian noise between 1e-3 and 5. Images with an SSIM $> 0.5$ are counted as leaked. At $\sigma=5$, no images are leaked.}
\vspace*{-5mm}
\end{figure}

\vspace{-1mm}
\subsection{Adding noise}
\vspace{-1mm}
Applying noise locally is used as a common form of defense against privacy attacks. We explore the effects of adding different levels of Gaussian noise to the attack on a single client. We quantify the effects of noise through the average reconstructed image PSNR score and the leakage rate (reconstructions with SSIM $> 0.5$). We use $\textit{CSF}=500$ in order to achieve the maximum default leakage rate, $\alpha=1e-1$, and train with 4 local iterations of mini-batch size 16. Compared to FedSGD, the learning rate $\alpha$ in FedAVG directly impacts the size of the computed gradient magnitude. Figure~\ref{fig:differential_privacy} shows the PSNR and leakage rate for various standard deviation $\sigma$ amounts of noise between $1e-3$ and 5 added to the update trained on CIFAR-100. Only when the amount of noise added is $\sigma=5$, is all image leakage prevented. However, while adding $\sigma=5$ noise to the model can prevent leakage, it will also destroy a model's training accuracy. Appendix Figure~\ref{fig:dp_reconstructions} shows reconstruction results with added noise.

Increasing the number of clients in secure aggregation also increases the amount of noise added to the reconstructions. The noise in the aggregation works as adding multiple gaussian distributions together. The mean of the cumulative gaussian is the sum of the means of the individual distributions (in this case it is 0) and the variance is the sum of individual variances. In the above figure, the attacks achieves 0 leakage with added noise of STD $\sigma=5$. If each client uses $\sigma=5e-2$, having $N=\frac{(5)^2}{(5e-2)^2} = 10,000$ clients in aggregation will achieve the same amount of noise as a single client adding $\sigma=5$. 