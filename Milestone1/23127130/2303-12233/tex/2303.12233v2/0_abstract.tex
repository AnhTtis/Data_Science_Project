% Security and privacy are important concerns in machine learning. End user devices often contain a wealth of sensitive information and data that should not be shared with servers or enterprises. As a result, 
Federated learning was introduced to enable machine learning over large decentralized datasets while promising privacy by eliminating the need for data sharing. Despite this, prior work has shown that shared gradients often contain private information and attackers can gain knowledge either through malicious modification of the architecture and parameters or by using optimization to approximate user data from the shared gradients. 

However, prior data reconstruction attacks have been limited in setting and scale, as most works target FedSGD and limit the attack to single-client gradients. Many of these attacks fail in the more practical setting of FedAVG or if updates are aggregated together using secure aggregation. Data reconstruction becomes significantly more difficult, resulting in limited attack scale and/or decreased reconstruction quality. When both FedAVG and secure aggregation are used, there is no current method that is able to attack multiple clients concurrently in a federated learning setting.

In this work we introduce \name, an attack that overcomes previous limitations and also breaks the anonymity of aggregation as the leaked data is identifiable and directly tied back to the clients they come from. Our design sends clients customized convolutional parameters,  and the weight gradients of data points between clients remain separate even through aggregation. 
% This further allows applicability to non-IID settings. 
With FedAVG and aggregation across 100 clients, prior work can leak less than 1\% of images on MNIST, CIFAR-100, and Tiny ImageNet.
% SB (4/11/23): The above sentence is too long and too convoluted. Break it into components: how we send customized convolutional kernels, how the aggregation still keeps the updates separable, how we handle multiple local iterations in FedAVG. 
% JZ (4/11/23): Changed the sentence. The customized convolutional kernels are directly tied to separability in aggregation, so I have lef that together. I'm not sure if we need to include non-IID here though. Some aspects of the attack are require more explanation, such as how we handle FedAVG, so I have left that out of the abstract for now.
Using only a single training round, \name is able to leak 76-86\% of all data samples.
% SB (4/11/23): Key mistake --- no evaluation number in context (which dataset, how many clients) and compared to a SOTA. 
% JZ (4/11/23): The SOTA for secure aggregation FedAVG attack would be RtF + MI, which can leak less than 1% of images. I have added the datasets and number of clients in the sentence before. Should I specify RtF + MI here too?