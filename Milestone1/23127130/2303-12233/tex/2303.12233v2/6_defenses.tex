As a major outcome of our work, we find that FedAVG using secure aggregation for defense does {\em not} prevent a malicious server from recovering large amounts of private user data regardless of the number of clients participating. The same is true for secure shuffling, discussed in the context of federated learning by~\cite{kairouz2021advances}. Secure shuffling prevents a server from receiving any additional information (outside of the update itself) which would allow identification of which client sent each individual message. However, secure shuffling cannot nullify our attack as the update itself carries the fingerprint of the client. While reconstructing the image, the server can identify the client it came from. 
% does not apply any more security on top of aggregation. Furthermore, neither secure aggregation nor secure shuffling prevents our attacker from identifying client data ownership during reconstruction.

One possibility of defense is through identification of a modified model architecture or the parameters. While \name uses a certain order of layers (convolutional layer followed by two FC layers), this module can be difficult to identify as the layers used can be further in the model architecture. Models can use multiple convolutional layers followed by FC layers. In this case, \name can use the final convolutional layer and subsequent FC layers as the leakage module. Figure~\ref{fig:imagenet-downsample} in the appendix shows reconstructions after max-pooling. While the reconstructions lose resolution, the content is still clear. \name is similar to Robbing the Fed~\cite{fowl2022robbing} in stealthiness. Although it requires an additional convolutional layer, identification is just as difficult since it is easy to place this in multiple locations of the benign architecture of models such as ResNet or VGG by having prior layers act to push the input forward. A client could also attempt to identify malicious tampering of parameters prior to training on local data and sending an update. However, this could also be difficult due to the server's ability to mask the weight gradient in different ways to have the non-identity mapping sets output zero. Furthermore, the no model inconsistency attack does not use zero parameters. There is also still a fundamental problem with identification in that clients have limited computational abilities in the setting of cross-device FL, which may preclude such verification. They typically only follow a standard federated learning protocol given to them that consists of training the model and sending updates back to the server. 

Model inconsistency defense has also been proposed in~\cite{pasquini2021eluding}. Having clients utilize a pseudorandom generator for SA using a condition that the received models are the same will prevent the server from being able to unencrypt the SA updates if it sends different models to each client. This method incurs no additional communication overhead and is applicable to SA algorithms such as~\cite{bonawitz2017practical,secagg_bell2020secure}. However, the practical scenario of asynchronous FL also uses SA~\cite{so2021lightsecagg,nguyen2022federated} but inherently has model inconsistency due to client staleness. Methods for on-device efficient training also send clients models with different architectures~\cite{diao2021heterofl}. Furthermore, the server does not absolutely require model inconsistency for \name. By choosing to not use model inconsistency, the leakage rate of the attack is slightly lower, but the server can increase the stealth of the attack with no impact on the reconstruction quality or scalability (Figure~\ref{fig:scalability_rtf}).

One standard mitigation strategy in federated learning would be to use differential privacy~\cite{dwork2014algorithmic,jayaraman2019evaluating,wei2020federated} to add noise to updates prior to sending them to the server. While this method can be effective in preventing a server from fully reconstructing private data, it comes with the hefty downside in a decrease of model performance, especially with large vision models. Particularly, the large dimensionality of many vision models causes the impact of noise to be greater when achieving the same privacy guarantees~\cite{zhang2022understanding}. The modification of the model due to the attack does not need to occur during every step of the training process. Without knowledge of when the attack will occur, differential privacy must then be applied during every step of the training process, thus seriously impacting accuracy.