In this paper, we have demonstrated how to break the privacy of secure aggregation in FedAVG federated learning through a malicious server that sends customized models to clients. Our key design idea is to send customized convolutional kernels to each client, an identity mapping set, that separates the weight gradients from the clients despite the use of secure aggregation. The server then uses these weight gradients to reconstruct the original data points. Using our proposed convolutional scaling factor, the attack can avoid model inconsistency and achieve a higher leakage rate in FedAVG than FedSGD attacks. We are the first to achieve a privacy attack in FL with FedAVG that scales well with the size of the local dataset and the number of clients. For us to handle an increasing local dataset size, the fully connected layer size increases linearly. To handle an increasing number of clients, the size of the convolutional layer increases linearly and so the total number of parameters grows linearly, while the number of non-zero parameters stays constant. We achieve high reconstruction quality and a leakage rate between 76-86\% for CIFAR-100, Tiny ImageNet, and  MNIST with 100 clients, while that of the state-of-the-art is less than 1\%.