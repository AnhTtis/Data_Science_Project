\iffalse

While many different attacks on FL have been proposed, data reconstruction attacks are the strongest attacks on privacy in FL. 
% SB (10/10/22): What does above mean - data reconstruction attacks are the strongest known attacks against privacy in FL?
% JZ (10/10/22): Rephrased this to follow that
Reconstruction attacks aim to break the fundamental notion of privacy of FL by obtaining client data directly through their updates. 
% SB (10/10/22): directly from what? Gradient updates?
% JZ (10/10/22): Yes, added
Prior work has done this through analytic methods~\cite{boenisch2021curious,fowl2022robbing}, optimization~\cite{zhu2019deep,geiping2020inverting,zhao2020idlg,yin2021see}, hybrid approaches combining both~\cite{pasquini2021eluding,wen2022fishing}, or other approaches including GANs~\cite{hitaj2017deep,wang2019beyond}. We focus on the linear layer leakage (LLL) class of attacks, as they are the only ones scalable to aggregate updates. The sparse variant of~\cite{fowl2022robbing} is applicable to single-client FedAVG. The following subsections will discuss the methodology of LLL for FedSGD and FedAVG along with the scalability limitation in the FedAVG method.


\begin{table*}[t!]
\small
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
                                                                            & \textbf{Attack type} & \textbf{\begin{tabular}[c]{@{}c@{}}Leakage\\ scale\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}\# training\\ rounds\end{tabular}}   & \textbf{\begin{tabular}[c]{@{}c@{}}Reconstruction\\ quality\end{tabular}} &  \textbf{\begin{tabular}[c]{@{}c@{}}FL aggregation\\ support\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Non-IID client\\ support\end{tabular}} \\ \hline
\begin{tabular}[c]{@{}c@{}}Deep Leakage\\ (NeurIPS '19)~\cite{zhu2019deep}\end{tabular}        & Optimization         & $10^0$                                                                      & Single                                                                     & Estimation                                                                         & No                                                                        & \multirow{8}{*}{N/A}                                                           \\ \cline{1-6}
\begin{tabular}[c]{@{}c@{}}GradInversion\\ (CVPR '21)~\cite{yin2021see}\end{tabular}          & Optimization         & $10^1$                                                                     & Single                                                                  & Estimation                                                         & No                                                                        &                                                                                \\ \cline{1-6}
\begin{tabular}[c]{@{}c@{}}Eluding Secure Agg.\\ (ACM CCS '22)\cite{pasquini2021eluding}\end{tabular} & Analytic               & $10^1$ & Single    & Estimation                                                         & Yes (one client)                                                          &                                                                                \\ \cline{1-6}
\begin{tabular}[c]{@{}c@{}}Fishing for User Data\\ (ICML '22)~\cite{wen2022fishing}\end{tabular}  & Analytic               & $10^0$                                                      & $2-10$    & Estimation                                                                         & Yes (one image)                                                           &                                                                                \\ \cline{1-6}

\begin{tabular}[c]{@{}c@{}}Gradient disaggregation\\ (ICML '21)~\cite{lam2021gradient}\end{tabular}  & Analytic               & $10^3$                                                      & $10^2$    & Estimation                                                                         & Yes                                                           &                                                                                \\ \cline{1-7}

\begin{tabular}[c]{@{}c@{}}Robbing the Fed\\ (ICLR '22)~\cite{fowl2022robbing}\end{tabular}        & Analytic           & $10^{1^*}$                                                     & Single                                                                       & Exact                                       & Yes                                                                   & No                                                                               \\ \hline
\textbf{\name}                                                         & \textbf{Analytic}  & $\mathbf{10^3}$                                             & \textbf{Single}                                                              & \textbf{Exact}                                               & \textbf{Yes}                                                              & \textbf{Yes}                                                                   \\ \hline
\end{tabular}
\vspace*{2mm}
\caption{\label{tab:comparison_table} \textcolor{red}{Comparison of attack features. ($^*$Leakage scale of Robbing the Fed~\cite{fowl2022robbing} when the same number of non-zero parameters as \name are added.) \textbf{TABLE TO BE REPLACED}}}
\vspace*{-7mm}
\end{table*}
\fi

\begin{table*}
\begin{center}
\includegraphics[width=1.0\textwidth]{plots-images/comparison_table.drawio.pdf}
\end{center}
\vspace*{-2mm}
\caption{\label{tab:comparison_table} Comparison of data reconstruction attack features. Partial support indicates the attack has limited reach/scale.}
\vspace*{-6mm} 
\end{table*}
%\atul{For single client shall we still call it FedSGD/Avg or something else?}
% JZ (4/3/2023) Yes, I have changed it to "FedSGD/AVG individual update" instead of single client.

% (Eluding Secure Aggregation~\cite{pasquini2021eluding} has partial support for aggregation because it can only reach a single client).




While many different attacks on FL have been proposed, we focus on data reconstruction attacks, the strongest attacks on privacy in FL. 
% SB (10/10/22): What does above mean - data reconstruction attacks are the strongest known attacks against privacy in FL?
% JZ (10/10/22): Rephrased this to follow that
Reconstruction attacks aim to break the fundamental notion of privacy of FL by obtaining client data directly through their updates. 
% SB (10/10/22): directly from what? Gradient updates?
% JZ (10/10/22): Yes, added
Prior work has done this through optimization~\cite{zhu2019deep,geiping2020inverting,zhao2020idlg,yin2021see,dimitrov2022data}, analytic attacks~\cite{pasquini2021eluding,wen2022fishing}, linear layer leakage~\cite{boenisch2021curious,fowl2022robbing}, or other approaches including GANs~\cite{hitaj2017deep,wang2019beyond}. These attacks typically aim to attack either individual client gradients or aggregated gradients. 
\begin{comment}
With individual gradients, attacks focus on increased effectiveness for larger batch sizes. However, problems with batch size are directly magnified in aggregation. For example, cross-device FL has hundreds of clients in each training round. The aggregated gradient would then include contributing gradients from hundreds or thousands of images resulting in essentially a "massive-batch" gradient. Furthermore in realistic scenarios where secure aggregation is used~\cite{bonawitz2019towards}, this aggregate gradient is all that a malicious server has access to. 
\end{comment}
The following subsections will discuss the details and limitations of prior attacks.

\vspace{-1mm}
\subsection{Optimization-based attacks}
\vspace{-2mm}
Optimization approaches have shown great success in leaking data from individual updates, especially with smaller batch sizes. These attacks typically operate under the threat model of an honest-but-curious server or an external attacker that has access to the model and individual gradients from each client. With only this information, the attacker initializes some dummy data and computes the gradient of that data on the model.
\begin{equation}\label{eq:optim}
    x^* = \arg \min_{x}||\nabla L(x, y, \theta)-\nabla W||_2
\end{equation}
\noindent
An optimizer minimizes the difference between the generated gradient $\nabla L(x, y, \theta)$ and the ground truth gradient $\nabla W$ (benign client update). Here $x$ is the dummy data, $x^*$ is the reconstructed data, $L$ is the loss function, $y$ is the label, and $\theta$ is the model parameters.

More recent optimization approaches~\cite{geiping2020inverting,yin2021see} work under the assumption that user labels are known prior to optimization. Typically, these labels are directly retrieved through a zero-shot solution without using optimization approaches~\cite{zhao2020idlg,yin2021see}. Furthermore, regularizers and strong image priors specific to image data are often used to guide optimization results~\cite{geiping2020inverting,yin2021see}. These can also result in image artifacts typical of an image class, but not in the actual training image. These approaches have shown surprising success with image data on smaller batch sizes. However, as batch sizes increase, the fraction of images recovered decreases along with the reconstruction quality and the number of iterations required for the optimization also increases. One reason stated by~\cite{zhu2019deep} was that regardless of the order of images in the batch, the gradient will remain the same. Having multiple possible permutations then makes the optimization more difficult.
% SB (4/12/23): There is a dangling sentence here. 
% JZ (4/13/23): Fixed
Another fundamental reason is that a larger batch size means more images and more variables for optimization.
% SB (10/10/22): The last sentence is not clear. This is the clinching sentence where we need to convince the reviewer that all these prior approaches fail at large batch sizes. 
% JZ (10/10/22): I have tried to clear this up by rephrasing a bit. But I added a more basic intuition, mainly that a larger batch size means more images, which in turn just means more pixels to reconstruct.


\vspace{-1mm}
\subsection{Aggregate attack methods}
\vspace{-2mm}
There have been several attacks aimed specifically for aggregate gradients, however, they are currently limited in the attack scalability. 

In~\cite{pasquini2021eluding}, attackers send different models to clients such that the resulting aggregated gradient is only a targeted client's individual gradient. This is done by sending model parameters such that ReLU activated layers would have fully dead units (and an update with zero gradients) for any non-targeted client. The targeted client would get an attacked model, which would then be the only one to return non-zero gradients. On the other hand,~\cite{wen2022fishing} focuses on attacking a single data point through gradient magnification. The server sends weights to magnify the gradients of a targeted class by decreasing the model confidence on that class' prediction. Within the targeted class, the server will also focus on a specific feature in order to target a single sample. The resulting gradient will be similar to the gradient for a single image, allowing optimization-based approaches to retrieve the input. However, this process requires multiple training iterations. The attack can also only target a single image each time, and even in this case it does not succeed every time. Another method treats the inputs to a fully-connected layer as a blind source separation problem~\cite{kariyappa2022cocktail} where the weight gradients for the neuron make up a set of weighted combinations of the inputs. While the approach is able to attack aggregated gradients in FedSGD, the number of inputs is limited to be $1024$ or fewer. 

Another work~\cite{lam2021gradient} has looked to enable prior individual gradient methods through gradient disaggregation, separating out individual updates over time. However, along with requiring additional side-channel information about client participation, the method also requires a large number of training iterations to accomplish the goal. Similarly, due to partial user selection~\cite{cho2020client,chen2020optimal}, the server can reconstruct the individual models of some users using the aggregated models from previous rounds~\cite{pejo2020quality,secagg_so2021securing}. These approaches also require multiple training rounds and can be prevented by proper client selection so that no individual updates can be singled out.


\vspace{-1mm}
\subsection{Linear layer leakage attacks}
\vspace{-2mm}
Linear layer leakage attacks are a sub-class of analytic attacks that modify FC layers to leak inputs. Using the weight and bias gradients of an FC layer to leak inputs was discussed in~\cite{phong2017privacy,fan2020rethinking}. 
% SB (4/12/23): For a security reviewer, it is well to say linear layer attacks => attacks that modify the FC layer. 
% JZ (4/13/23): Added
When only a single image activates a neuron in a fully connected layer, the input to that layer can be directly computed using the resulting gradients as
\begin{equation}\label{eq:1}
    x^i = \frac{\delta L}{\delta W^i} / \frac{\delta L}{\delta B^i}
\end{equation}
\noindent
where $i$ is the activated neuron, $x^i$ is the input that activates neuron $i$, and $\frac{\delta L}{\delta W^i}$, $\frac{\delta L}{\delta B^i}$ are the weight gradient and bias gradient of the neuron respectively. This idea forms the basis for several reconstruction attacks~\cite{boenisch2021curious,fowl2022robbing}. Figure~\ref{fig:linear-leak-method} shows the basic process of leaking images through an FC layer. 
% SB (10/10/22): We are asking the reviewer to take this claim at its face value. So we need to give the citation and maybe a qualitative intuition behind it. 
% JZ (10/10/22) I have moved the figure to right after this to show the reviewers what is happening. I will also cite prior work that use this methodology.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=1.0\columnwidth]{plots-images/linear-layer-leak.drawio.pdf}
\end{center}
\vspace*{-4mm}
\caption{\label{fig:linear-leak-method} Using the weight gradient $\frac{\delta L}{\delta W}$ and bias gradient $\frac{\delta L}{\delta B}$ of a fully connected layer to reconstruct the inputs. Neuron $i$ is only activated by a single image, while $j$ is activated by two. As a result, the reconstruction of neuron $i$ is correct while $j$ is a combination of images.}
\vspace*{-4mm}
\end{figure}

When the fully-connected layer is placed at the start of a network, the data reconstructed from the the layer would be the input data. This reconstruction is exact, as opposed to the optimization approaches which function as estimations. However, inputs are only reconstructed exactly when a single data sample activates that neuron. If more than one input activates the neuron, the weight and bias gradients of these inputs will contribute to the batch gradient. 
% SB (10/10/22): What's meant by "total gradient"?
% JZ (10/10/22): Changed to "batch gradient"
When the gradient division of Equation~\ref{eq:1} is done to retrieve the input, the resulting reconstruction would be a combination of all contributing images, a case of failed attack. 
% \atul{I have forgotten the intuition how this happens. It will be helpful if we add that intuition here - will help motivate the binning approach}

To alleviate this problem,~\cite{fowl2022robbing,boenisch2021curious} use malicious modification of the parameters in the FC layer. For~\cite{boenisch2021curious}, trap weights were introduced, initializing the weights randomly to be half positive, half negative. In order to ensure that neuron activation is less common, the negative weights come from a larger negative magnitude range than the positive weights. They also discuss the use of convolutional layers to push the input image forward, allowing the attack to function on models starting with convolutional layers followed by fully-connected layers. However, one of the main problems of the method lies with scalability. Even if the size of the FC layer increases proportionately with an increasing total number of images, the leakage rate decreases. On the other hand, Robbing the Fed (RtF)~\cite{fowl2022robbing} introduced another approach with higher leakage rate called ``binning", where the weights of the FC layer would measure some known continuous CDF of the input data such as image brightness. The bias for each neuron then serves as a different cutoff, allowing only inputs with a high enough value to activate it. The goal of this method would be that only one input activates each ``bin", where the bin is defined as the activated neuron with the largest cutoff (for ReLU, the largest negative bias)\footnote{The bin biases are set as negative. The weights are positive and so the negative bins are used to prevent ReLU activation.}. For any case where only one input activates a bin, it can then be reconstructed as
\begin{equation}\label{eq:2}
    x^i = (\frac{\delta L}{\delta W^i} - \frac{\delta L}{\delta W^{i+1}}) / (\frac{\delta L}{\delta B^i} - \frac{\delta L}{\delta B^{i+1}})
\end{equation}
\noindent
where $i$ is the activated bin and $i+1$ is the bin with the next higher cutoff bias. 
% \atul{in the same FC layer, right? we can mention for clarity to a first-time reader because we mention two layers below.} 
% JZ (4/13/23): I think it may be strange to add it here, since we introduce the 2 FC layers in the next paragraph. However, we do clarify in the second sentence in the next paragraph that Equation 2 is used with the first FC layer.

For Equation~\ref{eq:2} to hold true, the attack requires the use of two consecutive FC layers. 
%\atul{Do they need to be consecutive?} 
% JZ (4/1/2023) Yes they need to be consecutive. There may also be a way of using convolutional kernel here, but I need to think about that.
The first layer is used to leak the inputs using Equation~\ref{eq:2} and the second FC layer maintains the requirement that $\frac{\delta L}{\delta B^i}$ and $\frac{\delta L}{\delta W^i}$ are the same for any neuron that the same input activates. This is achieved by having the same weight parameters connecting each neuron of the first FC layer to the second FC layer. For example, if the first FC layer has $1024$ units and the second has $256$, the weights connecting them would have 
a dimension of $1024\times256$. The above property indicates that every row of the weight matrix is equivalent, e.g. $[0, :]=[1, :] = \dots = [1023, :]$. 
% SB (10/10/22): I do not understand from "and the second FC layer ...". NTU. 
% JZ (10/10/22): Rephrased this.

\noindent \textbf{FedAVG}. While the previous method works for FedSGD, for FedAVG, the model changes during local iterations and this prevents the reconstruction attack. As a result,~\cite{fowl2022robbing} proposed the sparse variant of the attack which uses an activation function with a double-sided threshold (e.g., Hardtanh) such as:
\begin{equation}\label{eq:act_func}
    f(x) = \protect\begin{cases} 0 & x \leq 0 \\ 
    x & 0 \leq x \leq 1 \\ 
    1 & 1 \leq x  \protect\end{cases}
\end{equation}
\noindent With this activation function, only when the input is between $0$ and $1$ will there be a non-zero gradient. 

Using this activation function, neuron activation will be sparse (i.e., images will only activate a single neuron). However, this range between 0 and 1 for the non-zero gradient is fixed for all neurons. Since RtF's approach sets up neuron biases following a distribution of the images, the weights and biases of the FC layer will need to be adjusted to follow the new non-zero gradient range. This requires scaling the magnitude of these parameters based on the distance between the subsequent neuron biases. Consider that the weights originally measure average pixel brightness. In this case, all the weights would originally be set to $\frac{1}{N}$, where $N$ is the total number of pixels. Then, the weights and biases are rescaled as:
\begin{equation}\label{eq:param_scale}
\begin{split}
W^*_i = \frac{W_i}{b_{i+1} - b_i} \text{  } , \text{  } b^*_i = \frac{b_i}{b_{i+1} - b_i}
\end{split}
\end{equation}
\noindent where $W^*_i$ and $b^*_i$ are the scaled weights and biases of neuron $i$ respectively and $b_{i+1} - b_i$ is the distance between adjacent biases in the original distribution. This process uses the same distribution as the FedSGD case to setup the initial biases, while incorporating the fixed range of the new activation function by scaling the parameters. In FedAVG, after the clients send the updated model parameters, the server computes a ``gradient" as:
\begin{equation}\label{eq:fedavg_grad}
    \nabla W_{FedAVG} = \Theta_{t+1} - \Theta_{t} 
\end{equation}
where $\Theta$ is the model parameters for the securely aggregated model and $\nabla W_{FedAVG}$ the computed gradient. % The sparse variant is able to keep a similarly high leakage rate compared to FedSGD. However, this does not hold for aggregation.
% SB (4/12/23): The last two sentences are out of place and dangling. 
% JZ (4/13/23): Removed. 
% \saurabh{We need to introduce sparse variant of RtF somewhere. Here maybe?}
% JZ (4/13/23): This section is discussing the sparse variant. Added clarification at the beginning.

\iffalse
\subsection{Aggregated gradients}

\noindent
While the previous methods mainly function in the setting of a single client gradient, when the attacker only has access to aggregated gradients, such as with secure aggregation~\cite{bonawitz2017practical}, they tend to perform significantly worse. With the sheer number of data points in an aggregated gradient, optimization approaches fail. The analytic approaches of~\cite{fowl2022robbing,boenisch2021curious} do work, but the number of reconstructed images will be drastically decreased without an increase in the linear layer size, often resulting in only a few images recovered. 
%Appendix~\ref{sec:appendix} discusses scaling the method of~\cite{fowl2022robbing} towards aggregation. 
The only real way to keep these attacks effective is by increasing the size of the linear layer, thus increasing the number of model parameters (Section~\ref{sec:param-comp}).

Targeting the scenario of aggregated gradients, several recent approaches have focused on attacks we classify as magnification or minimization of gradients. 
% SB (10/10/22): It is not clear what the last two parts (reconstruction and FC layer size) mean. 
% JZ (10/10/22): Reconstruction quality is lower than previous appraoches and the number of images/inputs cannot be greater than 1024. I have removed the statement about FC layer size, as the specific reason behind the limitation is not that relevant for us

One approach with the goal of enabling prior methods for individual updates is~\cite{lam2021gradient}. They focus on directly disaggregating the gradients. Using knowledge of client participation rates, the server can learn exactly which clients are participating during each training iteration. Over time, the updates can then be split apart. However, this additional information used for the attack is not required for FL. The number of training iterations required for the attack to succeed is also very large.



\noindent \textbf{Limitations}. Prior approaches attacking aggregated gradients~\cite{boenisch2021curious,pasquini2021eluding,wen2022fishing,lam2021gradient,fowl2022robbing} still suffer from several fundamental limitations. ~\cite{pasquini2021eluding,wen2022fishing,fowl2022robbing} suffer from small attack scale,~\cite{lam2021gradient} needs a very large number of training rounds, and~\cite{pasquini2021eluding,lam2021gradient} create images with poor quality when the batch size is larger, due to a reliance on optimization techniques for reconstruction. Table~\ref{tab:comparison_table} shows the attack features of prior works, highlighting these limitations. 
% \saurabh{This is key --- where are these limitations spelled out? Is it in the Table 1? If so, refer to it and explain that table.}
% Resolved
Naturally, the question we pose ourselves is: how much further can an attacker push the limits of reconstructing data using aggregated gradients? In the next section we show how we break these previous limitations, achieving large-scale and high-quality reconstructions using the aggregated gradient from only a single training iteration.

\fi