\begin{figure*}[t!]
\begin{center}
\includegraphics[width=1.0\textwidth]{plots-images/weight-grad-deagg.drawio.pdf}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:identity_maps} The inserted attack module of a convolutional layer and two FC layers. Images are leaked using the gradients of the weight parameters connecting the convolutional output to the first FC layer. Weight gradient de-aggregation is done through separate identity mapping sets, indicated by the different color values in the convolutional kernels. In the aggregate update, the weight gradients of different clients are separated and used to leak the images. \iffalse \saurabh{Can we show within this figure, toward the bottom, the calculation for how many clients and batch size can be supported, i.e., calculate the sizes of our layers as a function of that.}\fi}
\vspace*{-3mm}
\end{figure*}
 
 As discussed previously, optimization becomes much more difficult with larger batch sizes and aggregation. The main problem is with more data to approximate, reconstructions end up having much lower quality and ultimately fails at larger batch sizes, even before aggregation is applied. On the other hand, the linear layer leakage methods~\cite{phong2017privacy,fowl2022robbing,boenisch2021curious} provide a powerful way of directly reconstructing training data without having reconstruction quality issues. Increasing the FC layer proportional to the number of total images in aggregation also allows~\cite{fowl2022robbing} to scale in FedSGD without losing effectiveness. However, the attack fails at scale for the much more difficult, and much more common, aggregation strategy of FedAVG. 
 % This work \name addresses the scalability problem for FedAVG and FedSGD.

% rather, only when multiple images activate the same neuron will reconstruction fail. With a large enough FC layer (imprint size), the fraction of recovered images is also relatively high. For individual batches of training data, these are generally insignificant problems. However, aggregation makes the immediate problem much worse. If two or more images across any of the clients activate the same bin, leakage fails. In this section, we detail our approach of using the convolutional layer parameters to separate the gradients between each client, building on the baseline of linear layer leakage. Specifically, when leaking from the FC layer, we use the method of "binning" from~\cite{fowl2022robbing} since it achieves the best results of any linear layer leakage method. Table~\ref{tab:comparison_table} shows a comparison of features between our work and previous reconstruction attacks.

\subsection{Model assumptions and attack scope}
\vspace{-1mm}
\noindent \textbf{Threat model}. We operate under the same threat model as~\cite{fowl2022robbing,boenisch2021curious}, 
% Removed "lu2022federated" from prior work threat model since it isn't really the same threat model
% SB (10/11/22): For believability, add a bunch of other cites that have the same threat model
% JZ (10/11/22): Added another two citations. The last one is not for reconstruction attacks, but they use model modification in unsupervised FL
a malicious federated learning server with the ability to manipulate the model architecture and parameters before sending the model to clients. The server launches the attack by inserting a malicious module into the architecture, where the goal of the malicious server is to recover private training data. 
% With this threat model, we will show the ability of an attacker to target specific clients and reconstruct massive numbers of images from only the aggregate gradient of a single training iteration.

\noindent \textbf{System model}. We operate in cross-device FL, a setting where hundreds of clients participate in a single round of aggregation. 
% SB (4/13/23): Why only a single round of aggregation?
The updates from the clients are aggregated through a secure aggregation mechanism before being made available to the server. 
Clients in this setting do not have the power to do a thorough verification of the models sent by the server. However, due to the application of secure aggregation, outside of the aggregate update, the server cannot receive any information about the clients or about individual updates. 
% SB (10/11/22): Why cross-device? Then the devices do not have the power to do thorough verification of the models sent by the server and do not do multiple rounds of GD between communication with the server. 
% JZ (10/10/22): Added the reason why we use cross-device. I'm not sure what GD means so I haven't added it yet

\noindent \textbf{Attack scope}. In this work, we target both FedAVG and FedSGD. Under the threat model, \name can leak the inputs to that model
% SB (10/11/22): This is dangerous to say "any model architecture". Then the reviewer has just to come up with one example where our work is not applicable. 
% JZ (10/10/22): I looked at RtF again, and they haven't said much about being model architecture agnostic. I think it will be fine to remove that claim, so I mainly discuss how we can work on any kind of data instead. This data claim is made in RtF too.
regardless of the data type (e.g. image, video, audio, text), similar to~\cite{fowl2022robbing}. We experimentally demonstrate this attack on image data. 
% SB (10/11/22): Like most of prior work?
% JZ (10/10/22): Yes. 

\begin{comment}
As discussed in their method, when FedAVG is applied the biases in the model will shift, causing images to hit multiple bins. Due to similarities in \name's inserted module, the same situation will occur when FedAVG is used resulting in a few images being duplicated during reconstruction. However, in addition to the bin shifting, the convolutional parameters will also shift. This will not affect any kernels outside of the identity mapping set, as the gradient will remain zero. However, the parameters of the identity mapping set will have a small change, resulting in subtle differences in the reconstruction. 

For this work, we , though our work is applicable to FedAVG with some additional modifications, whose discussion we defer to the Appendix (Section~\ref{sec:fedavg}). The extension from FedSGD to FedAvg follows the pattern of the prior work, Robbing the Fed. 
\end{comment}

\subsection{Scalability problem}
\noindent \textbf{FedAVG}. Since Robbing the Fed (RtF)~\cite{fowl2022robbing} can scale to aggregation in FedSGD by increasing the size of the FC layer, it is not immediately apparent why the same cannot be done in FedAVG with the sparse variant. In theory, the images will activate the same set of neurons between FedAVG and FedSGD (given a bit of difference coming from parameters changing slightly during local iterations), so reconstruction rate will be the same. However, there is a critical problem with scaling the size of the FC layer in FedAVG. The biases will follow the same distribution regardless of the size of the layer. If the distribution is the average pixel brightness, the range of values will always be between $0$ and $1$. Having a larger FC layer size results in more neurons in the same range, and the distance between adjacent biases decreases. This means the value of $b_{i+1} - b_i$ gets smaller and as a result, the scaled weights $W^*_i$ and biases $b^*_i$ become larger (Equation~\ref{eq:param_scale}).
\begin{equation}\label{eq:fedavg_predicion_update}
\begin{gathered}
||W_t, b_t|| \gg ||\nabla_{W_t}, \nabla_{b_t}||\\
W_{t+1} = W_t + \alpha\cdot\nabla_{W_t} \approx W_t\\
b_{t+1} = b_t + \alpha\cdot \nabla_{b_t} \approx b_t
\end{gathered}
\end{equation}
The increasing size of the FC layer results in increasing $W^*_i$ and $b^*_i$ parameter magnitude, 
% Wi* bi*
but the magnitude of each client's gradient update $(\nabla_{W_t}, \nabla_{b_t})$ will remain relatively unchanged. 
As a result, the relative parameter shift in the model between different local iterations becomes much smaller. 
% Can we have an equation to clarify this statement?
This same property is the Achilles heel, as once the magnitude of the parameters increase beyond a certain point, the update computed for the FC layer essentially becomes zero, i.e., $W_{t+1}-W_{t}=0$ for the FC layer (Equation~\ref{eq:fedavg_predicion_update}). 
% W_t+1 - W_t ~ 0
Even before the updates become zero, the values lose precision which reflects in poor reconstruction quality. This becomes a large problem, resulting in a heavily decreasing leakage rate and reconstruction quality as the number of clients in aggregation increases.

The default precision used for deep learning models in most packages is floating-point 32 (FP32). If the original value of the parameters is much larger than the update, adding the gradient to the parameters results in a loss of precision or in the extreme case, no update at all. This is typically caused by increasing the FC layer size, but also can be affected by other factors such as the learning rate or local mini-batch size. RtF~\cite{fowl2022robbing} mentions a way to increase the effectiveness of the method by scaling the linear distribution $h_{new}(x) = c\cdot h_{original}(x), c > 1$
to minimize the change in the parameters over the local iterations. However, what we find is that this does not address the problem in FedAVG, as scaling the distribution not only scales the weights of the FC layer, but also the biases. Consider a case where the image brightness distribution is scaled by a factor of 10. Since the weights used to measure it increase by $10\times$ to achieve this, the biases used as cutoffs also have to be scaled by the same factor. These layer parameters are then scaled by the distance between the biases $b_{i+1} - b_i$, which in turn is also $10\times$ larger. This ultimately results in no change in the values of $W^*_i$ and $b^*_i$ of Equation~\ref{eq:param_scale}. 
% SB (4/13/23): We can point to Equation 5 here and say that this means Wi* and bi* remain unchanged. The root cause of the problem was that these values are large and therefore, RtF's purported solution does not work. 
% JZ (4/13/23): Added
Thus, scaling the linear distribution does {\em not} fix the precision problem.

\noindent \textbf{FedSGD}. For FedSGD on a single client, if multiple images from that client's batch activate the same neuron, reconstruction fails. This problem is directly exacerbated with aggregation. If multiple images \textit{across any of the clients} activate the same neuron, 
% SB (4/13/23): Now we are talking of the "neuron" at the server?
% JZ (4/13/23): Yes. I have added clarification that the reconstruction is at the server below.
reconstruction at the server also fails. For a successful reconstruction, only one image across all client batches can activate the same neuron. Consider a case where we have $100$ clients each with a batch size of $64$. If we have an FC layer of $256$ units, this means $64\cdot100=6400$ images are shared, resulting in an average of $25$ images per neuron. For linear layer leakage, the generalization to these larger-scale attacks is done by simply increasing the size of the FC layer. In the case of RtF~\cite{fowl2022robbing}, a proportional increase in FC layer size with the number of images will maintain the same leakage rate. However, for trap weights~\cite{boenisch2021curious} the leakage rate will still decrease.

Therefore, while the underlying reasoning is different for the sparse variant of RtF~\cite{fowl2022robbing} in FedAVG and for trap weights~\cite{boenisch2021curious} in FedSGD, the fundamental scalability problem is the same. \textit{Current linear layer leakage methods only scale the size of the FC layer for an increasing batch size or number of clients, which results in scalability problems.} \name breaks this scaling problem for both settings and separates the scaling of the batch size and the number of clients. We increase the size of the FC layer for larger dataset sizes (batch sizes) and the number of convolutional kernels for more clients. Furthermore, we introduce a convolutional scaling factor that prevents multiple activations of a neuron across epochs and helps mitigate precision problems coming from factors such as local mini-batch size or learning rate.

\iffalse
\subsection{The problem of aggregate collision}
Compared to the optimization approaches, linear leakage methods are still able to scale to the FL setting by adding more units to the fully connected layer. In the prior work, Robbing the Fed~\cite{fowl2022robbing} has shown that they achieve the highest leakage rate among all linear layer leakage methods, so we use this as a baseline. However, like other linear leakage methods the scalability problem of aggregation still exists. For a single client, if multiple images from that client's batch activates the same bin, reconstruction fails. This problem is exacerbated with aggregation. If multiple images \textit{across any of the clients} activate the same bin, reconstruction also fails. Therefore, for a successful reconstruction, only one image from any client can activate a bin. As more clients participate, the number of images shared across the bins also increases accordingly. Consider a case where we have $100$ clients each with a batch size of $64$. If we have an FC layer of $256$ units, this means $64\cdot100=6400$ images are shared over $256$ bins, resulting in an average of $25$ images per bin, which leads to collisions. The solution for this problem is then to continually increase the size of the fully connected layer to compensate for more client participation. However, this approach will heavily scale up the model size, as each neuron added to the first FC layer adds parameters connecting the input to the first FC layer and parameters between the first and second FC layers. In total, each neuron adds a number equal to two times the size of the input image size. 

Ultimately, current linear layer leakage methods can only scale the size of the FC layer for an increasing batch size or number of aggregate clients. \name breaks this problem and separates the scaling of the batch size and aggregate clients. We increase the size of the FC layer for larger batch sizes and the number of convolutional kernels for more clients.
\fi


\subsection{Attack architecture}
\vspace{-1mm}
We insert an attack module at the start of a model that consists of a convolutional layer followed by two FC layers. This module is shown in Figure~\ref{fig:identity_maps}. 
% Compared to RtF~\cite{fowl2022robbing}, we only add an additional convolutional layer, a minimal change. 
% SB (4/13/23): Commented out to avoid feeling we are too incremental over RtF.
We leak images using the computed gradients of the weight parameters connecting the output of the convolutional layer to the first FC layer. The dimension of the output of the convolutional layer depends on the image size and number of kernels. For a $32\times32$ image and $100$ kernels, the output would have dimension $32\times32\times100$.

The size of the first FC layer depends on the local dataset size for FedAVG or batch size for FedSGD. Generally, we add $4$ units in the layer for each image. With a batch size of $64$, this would be $256$ units. Every unit in the first FC layer is connected to the second FC layer, which is the input to the rest of the model architecture and will have the same dimensions as the input image. As an inserted module, it is important that the input and output dimensions are the same, so that the dimensionality expected by the benign model is not altered.

\vspace{-1mm}
\subsection{Convolutional parameters}
\vspace{-2mm}
We start our discussion by describing the attack in FedSGD before discussing the (significant) sophistication for FedAVG. Previous works~\cite{fowl2022robbing, boenisch2021curious} have discussed the idea of leaking with convolutional layers followed by FC layers. The standard way to achieve this would be the use of convolutional kernels to push the image forward and have FC layers further in the model leak the inputs. We will further explore the use of convolutional parameters.

For a 3-channel input image (RGB), pushing the image forward can be done with three kernels in a convolutional layer. 
If we have $3\times3$ kernels, the dimension of each kernel would be $3\times3\times3$ with the final dimension corresponding to the input channels. 
% These kernels will be all zero other than a single one in the center corresponding to the different image channels. 
% SB (10/11/22): Not clear "corresponding to the different image channels". 
% JZ (10/11/22): Is input channels more clear? What I want to say is that if we use 3x3 kernels, the final dimension will depend on how many channels the input has (e.g. 3 for RGB)
For any given client, we will need a minimum of three kernels for a 3-channel image. Within each kernel, there is only one key channel, which will have a  value of 1 in the center and all other elements will be zero. The three kernels will have this in a different channel. We call the single non-zero value the key value $kv$. When the convolutional kernels are applied to the RGB input image, each kernel will push a separate channel forward, resulting in the image in its entirety being pushed through. 
This setup is shown in Figure~\ref{fig:conv-sets}. We will call these three convolutional kernels an \textit{identity mapping set}. This operation only requires the use of three convolutional kernels, a small number in the context of typical models. The other kernels are not needed to propagate any information, so the outputs are set to zero. There are multiple ways to set the kernel parameters such that the output is zero, but generally a large negative bias or zero/negative weights can cause the output to be zero. 

This simple approach still severely under-utilizes the number of convolutional kernels. Consider a situation where we have 256 convolutional kernels. To push one 3-channel image forward, we only use three kernels. As a result, the other 253 kernels do not contribute. With this in mind, we propose the use of multiple, separate identity mapping sets. Each set requires three kernels (number of input channels), and following this, we have $\lfloor \frac{N}{3} \rfloor$ separate identity mapping sets, where $N$ is the total number of convolutional kernels. These separate identity mapping sets, each corresponding to a different set of three convolutional kernels, are then used in different models and are sent to different clients.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=1.0\columnwidth]{plots-images/conv-identity.drawio.pdf}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:conv-sets} Identity mapping set for a 3-channel input. The first three kernels ($3\times3\times3$ cubes) of the convolutional layer push a different input channel forward. All parameters are zero except for a single element. The 2D slice with a single non-zero value is shown in the figure, and the locations of the slice for each kernel allows them to push different input channels forward.}
\vspace*{-5mm}
\end{figure}

\vspace{-1mm}
\subsection{De-aggregated update}
\label{sec:reconstruction_equation}
\vspace{-2mm}
% SB (10/11/22): Chopped
\begin{comment}
In order to understand what this accomplishes, we first look at the resulting gradient for a single client after training. For this client, we give it the first identity mapping set, where the first three convolutional kernels push the image forward and the rest output zero. When a training image goes through the network and the gradient is computed, for any activated neuron in the FC layer following the convolutional layer, the weight gradient will only be non-zero for the weights corresponding to the first three channels of the convolutional layer output. Since the output of the convolutional layer is zero for all other output channels, the weight gradients connecting to these channels would be zero.

If we consider other clients who have identity mapping sets from other kernels, when their weight gradients are computed, the only non-zero gradients will be the ones corresponding to the channels of their set. In other words, 
\end{comment}
By sending the carefully crafted separate convolutional kernels to each client, the weight gradient for each set of identity mapping sets is non-zero only for a different set of convolutional output channels. Therefore, when updates are aggregated together, the weight gradients remain separate. During the reconstruction phase, if inputs from different clients activate the same bin, 
% SB (10/11/22): "same bin" means "same bin within their respective models"
% JZ (10/11/22): Yes.  In the context here, we can also mean the aggregate gradient, since the weights will still remain separate there.
the computed weight gradient of that bin will not be shared between clients. The only inputs that can share the same set of weight gradients would be images within that single client's batch. This essentially allows the size of the FC layer to scale only based on the client batch size, and {\em not with the number of clients}. 

When reconstructing images, this allows us to work with different sets of weight gradients, each corresponding to separate identity mapping sets (and client model). Instead of Equation~\ref{eq:2}, we would then reconstruct images as:
\begin{equation}\label{eq:3}
    % x_{k} = (\nabla_{{W^i}_k} L - \nabla_{{W^{i+1}}_k} L) \oslash (\frac{\delta L}{\delta {B^i}_k} - \frac{\delta L}{\delta {B^{i+1}}_k})
    x^i_{k} = (\frac{\delta L}{{\delta W^i}_k} - \frac{\delta L}{{\delta W^{i+1}_k}}) / (\frac{\delta L}{{\delta B^i}_k} - \frac{\delta L}{{\delta B^{i+1}}_k})
\end{equation}
\noindent
where $k$ indicates the client and the corresponding weight and bias gradients respective to their identity mapping set. $x^i_k$ is the input from client $k$ that activates bin $i$. Figure~\ref{fig:identity_maps} shows the process of using separate identity mapping sets to split the aggregate weight gradient. This allows the attacker to leak images from each client separately after aggregation.

This decoupled weight gradient partially solves the scaling problem of reconstructing images as the number of clients participating in aggregation increases. However,
% The number of clients we can attack in a single round is upper bounded by the number of convolutional kernels and the number of identity mapping sets we can create. By sending zero-output convolutional kernels to non-target clients, we ensure that these are not attacked. %However, these additional clients can also be attacked in a different training iterations.
Equation~\ref{eq:3} brings up another problem for reconstruction, as the bias gradient is needed for the computation. While the weight gradients are separated through the identity mapping sets, the bias gradients are not. 
% \atul{showing this in the attack design figure will be helpful} 
% When an image from a client activates a neuron, it contributes to the bias gradient. 
% The bias gradient will not be decoupled and the individual client values of $\frac{\delta L}{\delta {B^i}_k}$ are not obtainable after aggregation. 
% SB (10/11/22): I do not understand the previous two sentences, why there is coupling of bias gradients. 
% SB (10/11/22): Reworded
Secure aggregation aggregates the bias gradients of each neuron (of the first FC layer). Thus the neuron $i$'s bias values from the FC layers of all clients are aggregated. Consider that an image $j$ activates neuron $i$ in client $k$ and another image $j'$ activates the neuron $i$ in client $k'$ (note that the neurons are physically separate before aggregation as they are in the local FCs of clients $k$ and $k'$). After secure aggregation, the bias update of neuron $i$ from clients $k$ and $k'$ are coupled and therefore the server is not able to use Equation~\ref{eq:3} to decouple images $j$ and $j'$. To solve this problem, we look at the purpose of the bias gradient in reconstruction.

\begin{comment}
This is the crux of the bias coupling problem. 
It turns out that the task of decoupling the bias gradients is more challenging than it first appears.

Let us consider an intuitive way of solving the problem, by creating a predictable output to the neuron, which would lead to a predictable bias gradient. However, this would also need to be done without knowledge of the input (which obviously is unknown to the server). One way of doing this could be having an additional identity mapping set in the convolutional layer, and a standard value of one for the bias of the neurons in the FC layer. With negative weights connecting one set and positive for the other, after adding the bias, the result would always be a value of one, achieving our goal of a predictable output and bias gradient. However, this simple approach shows us the real problem. A predictable output and bias gradient will fundamentally break the binning approach of~\cite{fowl2022robbing} or any other linear layer leakage method. Every image from a client batch would then activate every neuron, and no reconstruction would be possible. Instead of looking to decouple the bias gradient to solve this problem, we first ask a more basic question: what is the purpose of the bias gradient?
\end{comment}

Observing Equation~\ref{eq:3}, we note that for reconstruction of each neuron we subtract the weight gradients, and the resulting value is divided by the subtraction of bias gradients. Previously we mentioned how each neuron has a single bias, so after subtracting bias gradients, the resulting value remains a scalar. The purpose of the bias gradient, then, \emph{is to scale the value produced by subtracting the weight gradients such that it becomes the same as the input}. 
% SB (10/11/22): How can weight gradient become the input?
% JZ (10/11/22): Mainly, these input images are hidden in weight gradient. The subtraction operation reveals them, and division scales it properly
Therefore, as long as we know what the weight gradient needs to be scaled to, we will not need to know the exact bias gradient. Rather, knowledge of the input range is all that is required to reconstruct the input. For image datasets such as MNIST, CIFAR-10, or Imagenet, the training data will be between 0 and 1. 
% SB (10/11/22): Not clear "data can be between 0 and 1". 
% JZ (10/11/22): While the data can be 0 to 255, prior to training, it should be between 0 and 1. If there is additional normalization then this might be different.
Using this, the images can be reconstructed by scaling the gradient such that the maximum value is 1, without requiring knowledge of the bias gradient.
\begin{equation}\label{eq:fedsgd_recon}
    x^i_{k} = \frac{abs({\frac{\delta L}{\delta W^i}_k} - \frac{\delta L}{\delta W^{i+1}}_k)}{max(abs({\frac{\delta L}{\delta W^i}_k} - \frac{\delta L}{\delta W^{i+1}}_k))}
\end{equation}
\noindent
The numerator is the absolute value of the subtracted weight gradient for client $k$ between neuron $i$ and $i+1$, while the denominator is the maximum of that value across the set of three slices in the identity mapping set corresponding to client $k$. 
% SB (10/11/22): Verify
% JZ (10/11/22): This is correct
% This division sets the new maximum value to be one.
The technical question now is much simpler --- estimating the maximum value of the denominator. The input range can be estimated by the server, or may be known through other public sources, such as through the standardized normalization prior to training. This estimation could be imprecise, but we see empirically that inaccuracies in this estimation do not hurt the reconstruction performance much, as the image remains the same structurally. Figure~\ref{fig:bright_shift} shows a reconstructed image with a shifted brightness. % \atul{We should emphasize that only brightness is affected, structurally it remains the same, just for explicit clarity}
% JZ (4/13/23): Added
The maximum pixel intensity of the ground truth was 0.7804 but was scaled to be 1.0 during reconstruction. Importantly, the error in reconstruction is contained to just that image and does not affect the reconstruction of successive images in that batch. 
% SB (4/13/23): Chopped for space
\begin{comment}
This is a desirable side effect of not using an actual bias gradient. We do not use the exact bias gradient in any of our experiments either and yet we see in Figure~\ref{fig:csf_metrics} that the reconstruction metrics are still high. 
% By scaling the absolute value of the weight gradient, we can directly recover the input image to the layer. If the correct range of the image is used for scaling, the reconstructed image will be exact. This process essentially works as an "estimated bias gradient." This input range should also be known by the server. A standard input normalization for client data will typically be used prior to training, and the knowledge can be used to scale the weight gradient. 
\end{comment}
Thus, our key result is that in linear layer leakage methods, the bias gradient is \textit{not} required for reconstruction of data.

\begin{figure}[t!]
\begin{center}     %%% not \center
\subfigure[Ground truth]{\label{fig:bright_shift_gt}\includegraphics[width=0.47\columnwidth]{plots-images/brightness-shift-crop-copy.png}}\hspace{5mm}%
\subfigure[Reconstruction]{\label{fig:bright_shift_rec}\includegraphics[width=0.47\columnwidth]{plots-images/brightness-shift-crop-copy-2.png}}\hspace{5mm}%
\end{center}
\vspace*{-3mm}
\caption{\label{fig:bright_shift} Image reconstructed using Equation~\ref{eq:fedsgd_recon} to scale the maximum to 1. Ground truth image (a) has a maximum intensity of 0.7804, resulting in a brightness shift in the reconstructed image (b).}
\vspace*{-5mm}
\end{figure}

\begin{comment}
\subsection{Bias estimation error}

% SB (10/11/22): Shortened from the material below. 
While data normalization prior to training is usually standard across clients, the assumption that the maximum value will be the same for all inputs will not always hold true. Taking image datasets as an example, the default CIFAR dataset has many images with a maximum pixel intensity value of 1, but there are also quite a few images with a maximum value below this. Our reconstruction by assuming the maximum value of 1 will result in the recovered image appearing brighter, although still easily identifiable to the eye. Figure~\ref{fig:brightness-shift} shows a reconstructed image with a shifted brightness. The maximum pixel intensity of the ground truth was 0.7804 but was scaled to be 1.0 during reconstruction. Importantly, the error in reconstruction is contained to just that image and does not affect the reconstruction of successive images in that batch. This is a desirable side effect of not using an actual bias gradient. 
\end{comment}

\vspace{-1mm}
\subsection{FedAVG and convolutional scaling factor}
\label{sec:FedAVG_CSF}
\vspace{-2mm}
By itself, the trap weights attack~\cite{boenisch2021curious} suffers from scalability problems in FedSGD. However, utilizing the convolutional attack structure of \name, the scalability problems are fixed through the split scaling between the number of clients and batch size (Figure~\ref{fig:wtc_leakage}).
The methodology is generalizable to FedAVG by using a double-bounded activation function as in Equation~\ref{eq:act_func} for the FC layer and scaling the parameters using the distance between adjacent biases using Equation~\ref{eq:param_scale}. 
The split scaling design mitigates the scalability problems in FedAVG coming from precision when having large parameters and small gradients. Since the FC layer size does not scale based on the number of clients, the distance between the biases in the layer does not change. As a result, the magnitude of the parameters $W^*_i$ and $b^*_i$ will not increase, helping alleviate the problem.

However, factors such as the learning rate or local mini-batch size still affect the update (gradient) size. The learning rate directly affects the size of the gradient update and the local mini-batch size also affects the magnitude of the individual gradient contributions from each image. For example, for a mini-batch of 10 images the gradients are averaged over the 10 images. For 20 images the gradients are averaged over 20 images, so the individual contributions are smaller. Furthermore, the FC layer size still needs to increase linearly with the local dataset size (the total number of images used in training) in order to maintain the same leakage rate. 
% SB (4/13/23): Can we quantify, even approximately, this increase?
% JZ (4/13/23): Yes, it should increase lineraly with the number of images. Rephrased and added.
These factors still impact the precision problem.

To address this, we introduce a convolutional scaling factor (\textit{CSF}) for the FedAVG attack. We can scale the image coming out of the convolutional layer by using a different key value ($kv$) for the identity mapping sets in the convolutional kernels. This can be used to offset the increase in the magnitude of the weight parameters $W^*_i$ coming from the increasing FC layer size or the small gradients coming from the learning rate or local mini-batch size. (Note that since we do not use the bias parameters $b^*_i$ to reconstruct images, we do not need to worry about update precision for them.) Using the \textit{CSF}, we can maintain the distribution and scale the parameters as:
\begin{equation}\label{eq:conv_scale_factor}
    kv_{new} = \textit{CSF}\cdot kv \text{  } , \text{  } W^*_{i,new} = \frac{1}{\textit{CSF}}\cdot W^*_i
\end{equation}
\noindent
where $kv$ is the non-zero value (typically 1) in the identity mapping set. The \textit{CSF} allows us to prevent precision problems during reconstruction, but also helps minimize the changes in the convolutional kernel parameters to preserve the original purpose of pushing the inputs forward. The value produced
after the image passes through the convolutional layer and
the weights of the FC layer remains the same as in the baseline ($\textit{CSF}=1$) in order to fit the distribution for the biases correctly.

Crucially, the \textit{CSF} is also used to help address the fundamental problem in linear layer leakage of having multiple images activate the same neuron. After an image activates a neuron during a local iteration, a smaller value of $W^*_{i,new}$ results in a larger relative change in the parameters. 
% SB (4/13/23): I do not understand the need for the qualifier "occurring after an image activates the neuron". Drop that?
% JZ (4/13/23): I do actually want to emphasize the point of parameters changing due to neuron activation. It is specifically after a neuron is already activated by an image that we do not want additional image activations. Reworded.
However, after an image activates the neuron, we do not want any additional activations in subsequent local iterations. 
% SB (4/13/23): From another image in the same batch?
% JZ (4/13/23): If it is in the same mini-batch we do not want another activation, but this cannot be prevented. We also do not want it to be activated from a different mini-batch, which we can actually prevent. I have added clarification.
Therefore, when the \textit{CSF} is large, the changes in the weights after each local iteration become large enough so additional images do not activate the same neuron in subsequent local iterations (if images are orthogonal, e.g., their dot product is zero, there can still be an activation of the same neuron. However this is a very rare case in practice). When multiple images in separate local iterations would have activated the same neuron, only the first image ends up actually activating it due to parameter shift. This method allows for a correct reconstruction of the first image, which was previously impossible. As a result, \name achieves higher leakage rate in the FedAVG attack compared to FedSGD. This same property of the \textit{CSF} also prevents images from different local epochs from activating the same neurons.