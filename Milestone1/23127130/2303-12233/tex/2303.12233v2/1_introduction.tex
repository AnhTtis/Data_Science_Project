Federated learning (FL)~\cite{mcmahan2017communication} is a machine learning paradigm introduced to address growing user data privacy concerns where clients participate in large-scale model training without sending their private data to servers for centralized training. A general training round in FL consists of a server sending the model out to the clients and clients training the received model with their private data before sending their updates back to the server. The server then aggregates the updates from the clients and uses the aggregate to update the global model before sending it out to repeat another iteration of the training process. Two primary methods of training are FedSGD and FedAVG. FedSGD involves a client training a model for a single local iteration before sending the gradients to the server. FedAVG, on the other hand, involves several local iterations of client training before sending the updated model parameters to the server. Due to communication efficiency, FedAVG is preferred in many settings.

While the promise of FL is that user data should be private and secure from a malicious server, this only holds true if gradients from the clients cannot be used by a malicious server to infer properties of the training data. Prior work has shown that property inference~\cite{melis2019exploiting,luo2021feature}, membership inference~\cite{shokri2017membership,choquette2021label,nasr2019comprehensive} or GAN-based attacks~\cite{hitaj2017deep,wang2019beyond} are all effective in inferring information about client data from their shared gradients. However, the stronger class of data reconstruction attacks, when a malicious server wants to directly steal private client training data, has been demonstrated against even the strictest defenses. 
% SB (4/11/23): What is a strict defense?
These attacks fall into two categories.
% SB (10/10/22): The sentence above is too convoluted. Say directly that reconstruction attacks have been demonstrated against even the strictest defenses and these attacks fall into two categories. XXX
% JZ (10/10/22): Done. I have still included the citation for the other attacks, but I separated it to be more clear.

Optimization attacks~\cite{zhu2019deep,zhao2020idlg,geiping2020inverting,yin2021see} generally work on image data, starting with a dummy randomly initialized data sample and optimizing over the difference between the true gradient and the one generated through the dummy sample. Iteratively, the dummy data sample  is updated and gets closer to the ground truth data that was used in computing the true gradient. However reconstructions following this method degrade in quality as the batch size and image resolution increase, failing with a batch size of greater than 48 as demonstrated in~\cite{yin2021see} on the ImageNet dataset. Most optimization attacks work only on FedSGD, but~\cite{dimitrov2022data} demonstrates an optimization attack on FedAVG that is applicable to single client attacks with a relatively small client dataset size and image size (50 images in total, FEMNIST/CIFAR-100). This is promising, as FedAVG is inherently more difficult to attack since the malicious server cannot see the intermediate model updates coming from local iterations. 
% SB (4/11/23): It is worthwhile to mention here in a sentence why FedAVG is harder to attack. 
% JZ (4/11/23): Added

However, secure aggregation has been shown to be an effective defense against all optimization-based privacy attacks in FL. 
% SB (4/11/23): Verify
% JZ (4/11/23): Not entirely sure if SA has been used to specifically look at optimization-based attacks (at least the data reconstruction attack papers barely look at this). However, it is a very fair assumption for people to have given that optimization has always struggled with scaling.
Secure aggregation  guarantees that the server 
% and any client in the FL setting 
will not gain access to individual model updates of other clients, only the aggregate of all client updates~\cite{bonawitz2017practical,fereidooni2021safelearn,so2021lightsecagg}. This poses a large problem for optimization as aggregation introduces a massive number of total images in training while these attacks only have a high image reconstruction rate in the small batch size regime.
% SB (4/11/23): I do not understand the context of the previous sentence. 
% ~\cite{russakovsky2015imagenet}.
% SB (10/10/22): Be careful when switching to the specific case of images from general data. Also give some quantitative intuition like these attacks are typically not effective with batch size of 64 or greater. 
% JZ (10/11/22): I have given a more quantitative value. I use 48 or more, as this is the specified batch size for the CVPR 2021 paper. Not sure how to do the citations here, the first one is for Imagenet dataset, second is for the CVPR paper.

The second class of attacks, analytic reconstruction attacks~\cite{fowl2022robbing,boenisch2021curious}, involve customizing the model parameters or the model architecture to directly retrieve the training data from the gradients of a fully-connected (FC) layer --- this is referred to as {\em linear layer leakage}. These approaches do not have issues with quality as they reconstruct the inputs near exactly. These methods work well when individual client updates are visible. 
% SB (4/11/23): What is a "single client attack"?
% JZ (4/11/23): I think "individual client update attack" is a better term to use. Just when there is no SA and individual updates are visible. Changed.
Under secure aggregation and FedSGD, ~\cite{fowl2022robbing} also has the ability to maintain a high leakage rate by increasing the size of the injected FC layer, although this often results in very large models. The introduced sparse variant of their attack functions to attack individual updates in FedAVG, but fails when secure aggregation is applied.
% SB (4/11/23): The bottomline is getting obscured. Is RtF a feasible attack against FedAVG with SA?
% JZ (4/11/23): Have clarified that their sparse variant doesn't work under FedAVG + SA. May go back and rephrase later.

Some recent works have targeted FL with secure aggregation by magnifying gradients~\cite{wen2022fishing}, making the aggregate update the same as an individual update of a target user~\cite{pasquini2021eluding},
% \atul{should it be de-aggregation?}
or looking to solve a blind source separation problem~\cite{kariyappa2022cocktail}. However, these attacks still have limited power in the FL setting. 
% SB (4/11/23): "in the aggregated setting" ---> "in the FL setting".
% JZ (4/11/23): Changed
The first approach can only steal a single user image for each training round while additionally requiring multiple iterations prior to setup the attack. The second approach can use either analytic attacks or optimization attacks as a backbone but suffers from a similar scale limitation, only reaching a single client each round. The third approach can work with up to $1024$ images in aggregation for FedSGD (\# images/client $\times$ \# clients), but fails for more images.
% AE (10/11/22) {It is not clear what is the final approach in the end of the  above paragraph }
% The third prior work referenced above
% SB (10/10/22): Break down the above sentence. Convince the reviewer that these prior approaches are indeed broken for FL. Maybe one sentence each for each attack. 
% SB (10/10/22): What does "the number of inputs" mean? Number in one batch?
% JZ (10/10/22): I have updated and gone deeper for each approach. I also removed "number of inputs." What it meant was total number of samples in the aggregate gradient (num_clients * batch_size)
{\em {\bf Thus, no prior method is able to scale to attacking multiple clients in FedAVG with secure aggregation.}}

% SB (4/13/23): Repeated text
\begin{comment}
The combined setting of secure aggregation and FedAVG is practical and especially challenging, as the number of data samples contributing to the aggregate update can become very large. 
% SB (4/11/23): It is dangerous to make a categorical statement like this "extremely large". I can do FedAVG and SA on a small setting too. 
% JZ (4/11/23): I have changed it from "is extremely large" to "can become very large" to remove some of the strong emphasis.
% When a client trains a batch, the gradients of the individual training samples within that batch are aggregated (i.e. averaged). 
% The FL server aggregation is applied on top of this, essentially doing an aggregation on top of the individual client one. 
% 
% SB (10/10/22): Do a thorough spell check. An error like this can get the reviewer thinking dark thoughts. 
% JZ (10/10/22): Okay
% SB (10/10/22): I do not understand the above sentence. 
% JZ (10/10/22): I have rephrased the statement. What I wanted to say is a batch gradient is an average across all individual client gradients. Aggregation between clients on top of this will essentially create a "large batch" where all the images (num_clients * batch_size) contribute to the gradient. 
% AE  (10/11/22) do we need to mention anything about using FedSGD as federated learning algorithm somewhere in the introduction  as all the description in the introduction  is assumed using SGD while the  reviewers  might got confused between it and FedAvg?  this formula assumes using FedSGD (Number of clients)$\cdot$(Batch size) and I think clarification is important. 
For this update, the number of samples contributing and % in FedSGD, 
subsequently reconstructed by the malicious attacker is the (Number of clients)$\cdot$(Local dataset size). Both the number of clients and the local dataset size can also grow in an unbounded manner in FL. This is the fundamental scaling challenge of prior privacy attacks in FL --- they struggle with an increasing dataset size and they are ineffective with a large number of clients in aggregation. As a standard defense method applied for secure FL, it is critical for attacks to achieve scalability through secure aggregation. 
% SB (4/12/23): We have never defined what attack scalability really means. Define it here, at the time of its first use. 
FedAVG further introduces another variable of having multiple local model steps during client training, allowing the server to only see the initial and final models. This inability of the server to see intermediate model steps adds another layer of difficulty compared to FedSGD. 
{\em {\bf Thus, no prior method is able to scale to attacking multiple clients in FedAVG with secure aggregation.}} 
\end{comment}

% SB (10/10/22): Suggested rephrasing: The number of samples used for recreating the original data by the malicious server = Number of clients X Batch size. The number of clients can grow in an unbounded manner in FL, while the batch size is typically in the 64-128 range. This is the fundamental scaling challenge of all prior privacy attacks in FL --- they struggle with increasing batch size and this means they are ineffective with a large number of clients. 
% JZ (10/10/22): I have changed the sentence to follow this. We don't show any examples of 128 batch size in our experiments though.

\noindent \textbf{Our work.} We introduce \name\footnote{\namemeaning}, an attack whereby a malicious server can directly reconstruct the training data of multiple users in a single iteration using only the aggregate update. 
% \atul{Shall we say how much of the training data?}
% JZ (4/13/23): Should be okay, we state the leakage rate later in the introduction.
The attack works for both FedAVG and FedSGD, and even when secure aggregation is applied with hundreds of clients participating in the training round. To achieve this, the malicious server modifies the model that it sends to each target client. {\em The key insight behind our attack is that the server sends customized convolutional kernels to each client, so that the gradients of the input data of each client is separable in the aggregated update and is thus recoverable at the server.} With this separated weight gradient, an FC layer can then be used to leak the data of each individual client. Furthermore, the attack can increase the leakage rate by observing and adjusting the neurons in the FC layer that client images activate in each training round.  
% SB (4/11/23): What does the attack becoming stronger mean?
% JZ (4/11/23): Increase the leakage rate. Changed.
% SB (10/10/22): The above sentence lacks context and needs to be expanded. 
% JZ (10/10/22): I wonder if I can remove this sentence from the introduction. The ability to learn dataset distribution for some metric is hard to explain in a way that makes sense without proper understanding of the attack setup first.
% \atul{We should briefly mention what we tune based on the observation}
% JZ (4/13/23): Added
This ability to leak client data regardless of aggregation breaks the previous scaling limitations of reconstruction attacks. While previous linear layer leakage methods~\cite{fowl2022robbing,boenisch2021curious} 
% SB (4/11/23): Put cites here. 
% JZ (4/11/23): Added.
must scale the FC layer to address an increase in the the number of samples coming from an increasing batch size or increasing number of clients, we instead introduce a {\em split scaling} for this through our design. \name can scale to larger client dataset sizes by increasing the FC layer size, but it can also scale to a larger number of clients by increasing the number of convolutional kernels. 
% SB (4/11/23): Scaling by increasing the FC layer size is what others do and that does not work well. So we are laying ourselves open to that same criticism?
% JZ (4/11/23): We will still need to scale the FC layer for an increasing dataset/batch size. However, this should be limited (e.g., batch size shouldn't be >256). Mainly, scaling the FC layer based on the number of clients is an issue due to the multiplicative increase in size. Our split scaling prevents an additional dimension of scaling in the FC layer that RtF/trap weights need in SA attacks.
This prevents a diminishing return in the leakage rate when higher numbers of clients are aggregated~\cite{boenisch2021curious}. 
% SB (10/10/22): I feel there is some deeply significant design in the statement above. NTU and write. 
With this property, we work especially well in large-scale aggregation such as cross-device FL, being able to leak $76\%$-$86\%$ of all images through only a single training round of FedAVG. 
\iffalse
\textcolor{red}{Figure~\ref{fig:comp-leak-rate} shows the leakage rate of our method compared to~\cite{pasquini2021eluding, fowl2022robbing}. While~\cite{pasquini2021eluding} can achieve $100\%$ leakage for a single client, as the number of clients increases, the total leakage rate decreases rapidly.} 
\fi
\begin{comment}
\name is agnostic to the number of clients, 
%\atul{doesnt the number of CNN kernels in the modified model depend on the number of clients?} 
% JZ (4/3/2023) Yes, this is the case.
\atul{This appears to be in conflict with a sentence earlier that says Mandrake can scale with the $\#$ clients by using more CNN kernels}
% JZ (4/13/23): In this case, we define MANDRAKE as agnostic to the number of clients since it is able to scale to the number of clients. I think the choice of using "agnostic" should be okay
and is able to maintain the same leakage rate regardless of the number of clients.
% SB (10/10/22): Here we need to point to the quantitative result comparing us with the two prior attacks. 
% JZ (10/10/22): I have added the plot for the leakage comparison
Since we use a linear layer leakage method as opposed to optimization, 
% SB (10/10/22): As opposed to what? Optimization-based method?
% JZ (10/10/22): Optimization, I have added clarification
the reconstruction quality is always high regardless of the number of images or clients. 
\end{comment}
On top of being able to break aggregation, gradients used for reconstruction also directly trace images back to the client that owns them. As a result, information on data ownership is also obtained and allows the attacking server another degree of freedom: the ability to target only high-value clients and identify their reconstructed data afterwards. 
% SB (4/11/23): Is the main point here that the server can stay stealthy by targeting only high value clients, i.e., a subset of clients? 
% JZ (4/11/23): Originally I added this without any major argument beyond the face-value point, but it can be interpreted that way. Only targeting a few clients can also lead to smaller scale attacks, which could potentially be stealthier and bring resource overhead benefits. 

% \vspace*{1mm}
Our main contributions are:

% \begin{enumerate}
% \setlength{\leftmargin}{7.5mm} % does not work
    % \item 
    \noindent (1) We introduce \name, an attack that allows data leakage even with secure aggregation. Using a single FedAVG training round with $100$ clients and a local dataset size of $64$, we leak $82.66\%$ ($5290$ of $6400$) training images from the aggregated update on CIFAR-100. 
    Further, \name can pinpoint which client each training sample comes from. \\
    % , not just reconstructing images in a large combined pool of data for all clients. %\atul{shall we reframe the 2nd part of the sentence to make it more clear?}
    % Removed. 
    %\atul{Shall we also say that this happens in just one (or a few) iterations?} 
    (2) The attack works regardless of the number of clients in FedAVG and FedSGD. By increasing the size of the network and our split scaling technique, we can continue to scale our attack to increasing batch sizes {\em and} number of clients. \\
    % A key design that allows us to gain scalability is splitting the increase between the convolutional layer (for increasing number of clients) and the FC layer (for increasing dataset sizes).
    % \atul{"splitting the increase" = is that a typo}
    % JZ (4/13/23): I think it is intentional. We mean "splitting the increase" of the model size increase
    % This design leads to zero increase in the number of {\em non-zero} parameters of the convolutional layer with increasing number of clients and an increase of 4 units in the FC layer per image in the batch. 
    % FC layer size with an increasing batch size so that we have $4$ units per image. For each client, we add 3 convolutional kernels. However, each additional client does not add any zero non-zero parameters to the models.
    % SB (10/10/22): Add specific number here. Also how does it scale. 
    (3) Using the convolutional scaling factor, \name is able to prevent images between separate local iterations from activating the same neuron, a fundamental problem in linear layer leakage for FedAVG. 
     % SB (4/11/23): A fundamental problem only for FedAVG. 
     % JZ (4/11/23): Yes. The multiple neuron activations is a fundamental problem in linear layer leakage for both FedSGD and FedAVG. However, we take advantage of the fact that there are local iterations in FedAVG to help partially mitigate this problem. If we only look at it from just preventing activations between different local iterations, then it will be more accurate to say that this is a problem for FedAVG.
     This allows the attack to achieve a higher leakage rate for FedAVG than any prior linear layer attacks. \\
     % SB (4/11/23): FedAVG being higher than FedSGD is pure chance. The key claim is that we can make it work for FedAVG at all.
     % JZ (4/11/23): The comparison between FedAVG and FedSGD is a bit strange. I have removed it. I think multiple neuron activations between separate local iterations does not break the attack. Even in FedSGD, inter-batch images can activate the same images, but the attack still fundamentally works. The FedAVG attack will still work regardless of our CSF fix. The CSF fix can help improve leakage rate.
     % It also allows the attack to work without sending different models to clients.
     % SB (4/11/23): This above sentence is abrupt and does not flow. Bring this in earlier in the introduction after we say that the server needs to send customized models to each client. 
     % JZ (4/11/23): The model inconsistency point is strange to bring up without discussing the CSF. I don't talk too much about the CSF in the introduction, and instead focus on the basic design and scalability. Section 4.5 entirely discusses model inconsistency and it is not that important to keep here.
% SB (4/3/23): Can we say that this technique allows \name to target FedAVG? The comparison of FedAVG with FedSGD is a red herring. 
% JZ (4/11/23): I have removed the comparison, instead just stating that we can achieve higher leakage rate than other LLL attacks. It definitely feels a bit strange without proper context, so we can leave it for the experients/appendix section to bring up.
     (4) \name is able to handle non-IID clients by learning the distribution of the dataset and individual clients over multiple rounds. Through learning over just a single training round, we achieve up to a $62.3\%$ increase in leakage rate from the no-learning case on OrganAMNIST. %\atul{This is an important contribution. We should also mention it in the introduction where we claim reconstruction in one iteration. Learning the distribution makes it sound more realistic.}
    % \saurabh{For each quantitative result, state the data set.}
    % \saurabh{A fundamental mistake: we do not give any comparative result.}
    % JZ (4/11/23): I'm not sure how to give a comparative result here, as there are no prior work discussing non-IID. I think the comparative result here would be the increase in in leakage rate.
% \end{enumerate}
