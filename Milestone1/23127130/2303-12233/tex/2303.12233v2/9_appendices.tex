\begin{table}[ht]
\vspace{-3mm}
\begin{center}
\begin{tabular}{|l|c|}
\hline
                       & \textbf{\begin{tabular}[c]{@{}c@{}}Leakage rate \\ (images) \end{tabular}} \\ \hline
\textbf{True initialization}     & 85.8\% (5492) \\
\textbf{Dataset agnostic} & 82.9\% (5305) \\
\textbf{Random}         & 62.1\% (3977) \\ \hline
\end{tabular}
\end{center}
\vspace*{-0mm}
\caption{\label{tab:bias_initialization} Leakage rate of \name using different bias initialization methods. 100 clients are trained on CIFAR-100 in FedAVG aggregation. The leakage rate only drops by $2.9\%$ for the dataset agnostic initialization.}
\vspace*{-5mm}
\end{table}

\begin{table}[ht]
\vspace{-3mm}
\begin{center}
\begin{tabular}{|l|cc|}
\hline
                       & \textbf{\begin{tabular}[c]{@{}c@{}}\name \\ FedSGD \end{tabular}} & \textbf{RtF FedSGD} \\ \hline
\textbf{CIFAR-100}     & 77.1\% (4936)                                                              & 77.1\% (4931)                                                             \\
\textbf{Tiny ImageNet} & 77.2\% (4939)                                                              & 77.7\% (4970) \\
\textbf{MNIST}         & 72.0\% (4610)                                                              & 75.1\% (4803)                                              
                                                            \\ \hline
\end{tabular}
\end{center}
\vspace*{-0mm}
\caption{\label{tab:fedsgd_leakage} Leakage rate of \name and Robbing the Fed (RtF)~\cite{fowl2022robbing} attack on several datasets in FedSGD. 100 clients in aggregation with a batch size of 64 are used.}
\vspace*{-8mm}
\end{table}

\begin{figure*}[t!]
\begin{center}     %%% not \center
\subfigure[10 clients]{\label{fig:rtf_10cl}\includegraphics[width=0.65\columnwidth,trim={18mm 18mm 18mm 18mm},clip]{plots-images/rtf_10cl_top8_combined_gt_r.pdf}}\hspace{5mm}%
\subfigure[25 clients]{\label{fig:rtf_25cl}\includegraphics[width=0.65\columnwidth,trim={18mm 18mm 18mm 18mm},clip]{plots-images/rtf_25cl_top8_combined_gt_r.pdf}}\hspace{5mm}%
\subfigure[50 clients]{\label{fig:rtf_50cl}\includegraphics[width=0.65\columnwidth,trim={18mm 18mm 18mm 18mm},clip]{plots-images/rtf_50cl_top8_combined_gt_r.pdf}}
\end{center}
\vspace*{-3mm}
\caption{\label{fig:RtF_recon_clients} Top-8 SSIM reconstructed images for Robbing the Fed (RtF)~\cite{fowl2022robbing} for (a) 10, (b) 25, and (c) 50 clients in FedAVG. Clients train with 8 local iterations of mini-batch size 8 with $\alpha=1e-4$  on CIFAR-100. The 1st and 3rd rows are ground truth and the 2nd and 4th rows are the corresponding reconstructions. The leakage rate along with the quality of reconstructed images decreases with an increasing number of clients.}
\vspace*{-3mm}
\end{figure*}


\begin{figure*}[t!]
\begin{center}     %%% not \center
\vspace{-3mm}
\subfigure[$\sigma=1e-3$]{\label{fig:dp_1e-3}\includegraphics[width=0.65\columnwidth,trim={22mm 22mm 22mm 22mm},clip]{plots-images/dp_cifar_1c_1e-3.pdf}}\hspace{5mm}%
\subfigure[$\sigma=1e-1$]{\label{fig:dp_1e-1}\includegraphics[width=0.65\columnwidth,trim={22mm 22mm 22mm 22mm},clip]{plots-images/dp_cifar_1c_1e-1.pdf}}\hspace{5mm}%
\subfigure[$\sigma=5$]{\label{fig:dp_5}\includegraphics[width=0.65\columnwidth,trim={22mm 22mm 22mm 22mm},clip]{plots-images/dp_cifar_1c_5.pdf}}
\end{center}
\vspace*{-3mm}
\caption{\label{fig:dp_reconstructions} Reconstruction examples for a client with varying $\sigma$ noise added to the update. Client training with $\alpha=1e-1$, 4 local iterations of mini-batch size 16 and $\textit{CSF}=500$. When (a) $\sigma=1e-3$ the attack has the maximum leakage rate and the images are clearly identifiable. When (b) $\sigma=1e-1$ the average SSIM is lower, but some images are visually identifiable. With (c) $\sigma=5$ all images are unidentifiable.}
\vspace*{-5mm}
\end{figure*}


\subsection{Additional experiments}
\label{sec:appendix_additional}
\vspace{-2mm}
We show the leakage rate for \name under different bias initialization methods in Table~\ref{tab:bias_initialization}. 100 clients are trained on the CIFAR-100 dataset with 8 local iterations and local mini-batch size 8. FC layer size 256, $\textit{CSF}=500$, and $\alpha=1e-4$ are used. For average pixel intensity, the actual dataset distribution for CIFAR-100 is $\mu=0.4782$ and $\sigma=0.1470$. For the dataset agnostic initialization, we assume the server has no prior knowledge and the biases are initialized with $\mu=0.5$ and $\sigma=0.25$ as discussed in Section~\ref{sec:setting_biases}. The random method initializes all biases randomly between 0 and 1. The random initialization drops the leakage rate by $23.7\%$ while the dataset agnostic initialization only drops the overall leakage rate by $2.9\%$.

Table~\ref{tab:fedsgd_leakage} shows the leakage rate for \name and Robbing the Fed (RtF)~\cite{fowl2022robbing} in the FedSGD case on the CIFAR-100, Tiny ImageNet, and MNIST datasets. We use the same settings as in Section~\ref{sec:fedavg_leakage_rate} to allow for comparison of the leakage rate in FedAVG vs. FedSGD. For the FedSGD attacks, there are 100 clients in aggregation each with a batch size of 64. The \textit{CSF} value does not impact the FedSGD attacks, so we use $\textit{CSF}=1$. We use the same FC size/convolutional kernels as in the FedAVG attacks in Section~\ref{sec:fedavg_leakage_rate} for both methods. This also leads to a model size roughly $2\times$ larger for RtF in FedSGD. Despite this, both methods achieve very similar leakage rates. However, compared to the leakage rate of \name in FedAVG, both FedSGD attacks achieve a lower leakage rate on all datasets.

\begin{comment}
We also show the applicability of \name to helping the trap weights (TW)~\cite{boenisch2021curious} attack in FedSGD. Figure~\ref{fig:wtc_leakage} shows the leakage rate of the TW attack and TW + \name as the number of clients increases. Clients train with a batch size of 64 on CIFAR-100 and we use an FC layer size of $10\times$ the total number of images for the TW attack (for TW + conv attack we use $10\times$ the batch size and scale the number of convolutional kernels for the number of clients). For each point, we use the TW scaling factor that achieves the highest leakage rate tested by 0.01 increments. For 1-50 clients, we find that scaling factors between 0.96-0.91 achieve the highest leakage rates, with lower values needed for higher numbers of clients. Despite this, the TW attack still suffers from a decreasing leakage rate as the number of clients increases. However, using the convolutional attack of \name in addition to TW allows the scaling to split between the batch size and number of clients and maintains a constant leakage rate regardless of the number of clients.
\end{comment}

Figure~\ref{fig:dataset_size_local_iterations} shows the leakage rate for several FC layer sizes with a local dataset size of 256 when varying the number of local iterations averaged over 10 clients. Clients train on CIFAR-100 with $\textit{CSF}=500$ and $\alpha=1e-4$. Leakage rate is sampled with local iterations between 1-64, sampled by powers of 2. With a single local iteration, the attack leakage in FedAVG is the same as FedSGD with a batch size of 256. 

\vspace{-2mm}
\subsection{Secure aggregation in FL} \label{appendix-SA}
\vspace{-3mm}
Secure Aggregation \cite{bonawitz2017practical} is one of the core privacy-preserving techniques in FL, that 
enables the server to aggregate local model updates from a number of clients, without observing any of their individual model updates in the clear. At their core, state-of-the-art  secure aggregation protocols \cite{bonawitz2017practical,secagg_bell2020secure,secagg_so2021securing,secagg_kadhe2020fastsecagg,zhao2021information,so2021lightsecagg,so2021turbo,9712310} in FL rely on using  additive masking to protect the privacy of individual models. In particular, in a secure aggregation protocol, each user $i \in [N]$ encrypts  its own  model update   $\mathbf{y}^{(t)}_i = \text{Enc}(\mathbf{g}^{(t)}_i)$ before sending it to the server in  the $t$-th communication round.  This encryption is done such that secure aggregation guarantees the following:

 \noindent \textbf{Correct decoding.} The encryption  guarantees correct decoding  for the aggregated model such that the server should be able to decode 
\begin{equation}
    \text{Dec} \left(\sum_{i \in [N]} \mathbf{y}^{(t)}_i \right)= \sum_{i \in [N]} \mathbf{g}^{(t)}_i, 
\end{equation}
\noindent \textbf{Privacy guarantee.} The  encrypted model updates $\{\mathbf{y}^{(t)}_i\}
   _{i \in[N]}$ leak no information  about the model updates $\{\mathbf{g}^{(t)}_i\}
   _{i \in[N]}$ beyond the aggregated model $\sum_{i=1}^{N} \mathbf{g}^{(t)}_i$. This is formally given as the following 
   \begin{equation}\label{eq-SA_guarantee}
   I\left({\{\mathbf{y}}^{(t)}_i\}
   _{i \in[N]}; \{\mathbf{g}^{(t)}_i\}
   _{i \in[N]} \middle  |   \sum_{i=1}^{N} \mathbf{g}^{(t)}_i \right) = 0,
\end{equation}
where $I(.)$ represents the mutual information metric. 

\begin{figure}[!t]
\begin{center}
\includegraphics[width=1.0\columnwidth,trim={0 0 0 10mm},clip]{plots-images/dataset_size_local_iterations.pdf}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:dataset_size_local_iterations} Leakage rate based on the number of local iterations and FC layer size averaged over 10 clients. The local dataset size is fixed at 256 for CIFAR-100. Increasing the local iterations increases the leakage rate.}
\vspace*{-2mm}
\end{figure}

\vspace{-2mm}
\subsection{Sample reconstructed images for other datasets}
\vspace{-2mm}
We show reconstructions of a randomly sampled client training on several datasets. There are 100 clients in aggregation training with 8 local iterations of mini-batch size 8, $\alpha=1e-4$, and $\textit{CSF}=100$. Figure~\ref{fig:single_client_mnist} shows the MNIST dataset, Figure~\ref{fig:single_client_organamnist} shows the OrganAMNIST dataset, Figure~\ref{fig:single_client_tinyimagenet} shows Tiny ImageNet, and Figure~\ref{fig:single_client_imagenet} shows ImageNet.

\begin{figure}[t!]
\vspace{-2mm}
\begin{center}     %%% not \center
\subfigure[Incorrect reconstruction]{\label{fig:psnr-ssim-1}\includegraphics[width=0.40\columnwidth]{plots-images/psnr-ssim-imgs2-copy-2.png}}\hspace{4mm}%
\subfigure[15.49 PSNR score]{\label{fig:psnr-ssim-2}\includegraphics[width=0.40\columnwidth]{plots-images/psnr-ssim-imgs2-copy.png}}\hspace{4mm}%
\subfigure[Correct reconstruction]{\label{fig:psnr-ssim-3}\includegraphics[width=0.40\columnwidth]{plots-images/psnr-ssim-imgs2-copy-3.png}}\hspace{4mm}%
\subfigure[13.56 PSNR score]{\label{fig:psnr-ssim-4}\includegraphics[width=0.40\columnwidth]{plots-images/psnr-ssim-imgs2-copy-4.png}}
\end{center}
\vspace*{-3mm}
\caption{\label{fig:psnr-1} Reconstructed images from CIFAR-100 compared to the highest PSNR ground truth image. The incorrect reconstruction (a) has an overlap in the neuron activation while the correct reconstruction (c) has no overlap. The incorrectly reconstructed pair (a, b) has a higher PSNR score than the correct pair (c, d).}
\vspace*{-5mm}
\end{figure}

\vspace{-2mm}
\subsection{Image metrics}
\label{sec:image-metrics}
\vspace{-2mm}
For image reconstruction, the ability to identify an image and quantify the reconstruction quality are important for a metric to capture. We show from our attack results that the PSNR score achieves poor results in both categories. Previous works have also discussed that the use of mean squared error, which PSNR is based upon, is a poor reflection of image similarity. We use the SSIM~\cite{wang2004image} metric as a baseline for comparison and and show that it serves as a better metric for perceptual similarity. Our empirical results on the reconstructed images also support the idea that PSNR is not the best choice for a reconstruction quality metric, especially for linear layer leakage attacks. Particularly, when reconstructing an image involves a large shift in the pixel value range, PSNR functions very poorly. This section shows several cases to demonstrate this.

When observing the PSNR score of reconstructions, even if an image is reconstructed incorrectly, the PSNR score can be higher than the score for a correct reconstruction. Figure~\ref{fig:psnr-1} shows one such example, with two reconstruction results: the first being a failed reconstruction due to image overlap and the second being correct. Here the incorrectly reconstructed image has a higher PSNR score of 15.49 compared to the correct image which has a score of 13.56. The SSIM metric has the desired result, with a score of 0.91 for the correct image and 0.67 for the incorrect one.

A more extreme case occurs if we use the highest PSNR score to match reconstructed images to their ground truth counterparts. Figure~\ref{fig:psnr-2} shows the result of choosing the corresponding ground truth image from the batch using the highest PSNR and SSIM scores. Comparing to the top ground truth image, the reconstruction in Figure~\ref{fig:match-psnr} has a PSNR score of 16.45 and an SSIM score of 0.17. The reconstruction in Figure~\ref{fig:match-ssim} has a PSNR score of 12.04 and a SSIM score of 0.88. As shown, using the maximum PSNR for image matching will often result in mismatches. In a batch of 100 images, there will usually be between 2-5 samples with an incorrect match. Comparatively, SSIM has no problems matching reconstructions to the ground truth.

\begin{figure}[t!]
\begin{center}     %%% not \center
\vspace{-2mm}
\subfigure[Reconstruction]{\label{fig:match-gt}\includegraphics[width=0.29\columnwidth]{plots-images/psnr-ssim-wrong-match5-copy.png}}\hspace{5mm}%
\subfigure[Max PSNR GT]{\label{fig:match-psnr}\includegraphics[width=0.29\columnwidth]{plots-images/psnr-ssim-wrong-match5-copy-2.png}}\hspace{5mm}%
\subfigure[Max SSIM GT]{\label{fig:match-ssim}\includegraphics[width=0.29\columnwidth]{plots-images/psnr-ssim-wrong-match5-copy-3.png}}
\end{center}
\vspace*{-3mm}
\caption{\label{fig:psnr-2} Matching a reconstructed image from CIFAR-100 to the ground truth image in the batch using the highest (b) PSNR  and (c) SSIM scores. SSIM chooses the correct image while PSNR chooses an incorrect image.}
\vspace*{-4mm}
\end{figure}

\begin{figure}[ht]
\vspace{-2mm}
\begin{center}     %%% not \center
\subfigure[None]{\label{fig:downsample0}\includegraphics[width=0.29\columnwidth]{plots-images/Imagenet-downsampling-0.png}}\hspace{1.5mm}%
\subfigure[1$\times$]{\label{fig:downsample1}\includegraphics[width=0.29\columnwidth]{plots-images/Imagenet-downsampling-1.png}}\hspace{1.5mm}%
\subfigure[2$\times$]{\label{fig:downsample2}\includegraphics[width=0.29\columnwidth]{plots-images/Imagenet-downsampling-2.png}}%
\end{center}
\vspace*{-3mm}
\caption{\label{fig:imagenet-downsample} Sample from ImageNet dataset (a) without downsampling and after being downsampled (b) once and (c) twice. The number of max-pooling layers prior to reconstruction with an FC layer changes the amount of downsampling.}
\vspace*{-3mm}
\end{figure}

\begin{figure}[ht]
\vspace{-3mm}
\begin{center}     %%% not \center
\subfigure[OrganAMNIST]{\label{fig:distribution-organamnist}\includegraphics[width=0.47\columnwidth]{plots-images/organ-dist-no-axis.png}}\hspace{5mm}%
\subfigure[CIFAR-100]{\label{fig:distribution-cifar}\includegraphics[width=0.47\columnwidth]{plots-images/cifar-dist-no-axis.png}}\hspace{5mm}%
\end{center}
\vspace*{-3mm}
\caption{\label{fig:distribution-datasets} Average pixel intensity dataset distribution for (a) OrganAMNIST and (b) CIFAR-100.}
\vspace*{-3mm}
\end{figure}


\begin{figure}[t!]
\begin{center}
\includegraphics[width=1.0\columnwidth,trim={28mm 25mm 21mm 15mm},clip]{plots-images/recon_example_mnist.pdf}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:single_client_mnist} Leaked images for a randomly chosen client training on the MNIST dataset. Out of the 64 total images, 51 images are leaked.}
\vspace{-5mm}
\end{figure}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=1.0\columnwidth,trim={28mm 25mm 21mm 15mm},clip]{plots-images/recon_example_organamnist.pdf}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:single_client_organamnist} Leaked images for a randomly chosen client training on the OrganAMNIST dataset. Out of the 64 total images, 51 images are leaked.}
\vspace{-5mm}
\end{figure}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=1.0\columnwidth,trim={28mm 25mm 21mm 15mm},clip]{plots-images/recon_example_tinyimagenet.pdf}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:single_client_tinyimagenet} Leaked images for a randomly chosen client training on the Tiny ImageNet dataset. Out of the 64 total images, 52 images are leaked.}
\vspace{-5mm}
\end{figure}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=1.0\columnwidth,trim={28mm 25mm 21mm 15mm},clip]{plots-images/recon_example_imagenet.pdf}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:single_client_imagenet} Leaked images for a single client training on the ImageNet~\cite{russakovsky2015imagenet} dataset. Out of the 64 total images, 53 images are leaked}
% \vspace{-5mm}
\end{figure}

\iffalse
\subsection{Remove later}
\noindent Some experiments (maybe in the appendices):
\begin{enumerate}
    \item Leakage rate for dataset agnostic initialization compared to known initialization. Very little difference for IID
    \item Leakage rate of MNIST for different CSF, as images are mostly zeros. Show it doesn't change much due to image orthogonality.
    \item Multiple local epochs with small CSF examples.
    \item Show RtF cannot scale to FedAVG regardless of FC layer size
    \item RtF leakage rate for various numbers of clients
    \item Reconstructions when gradients instead of the updates are sent for \name and RtF
    \item DP
\end{enumerate}
\fi