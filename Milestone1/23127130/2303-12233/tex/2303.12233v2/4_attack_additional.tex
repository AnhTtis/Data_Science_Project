\begin{comment}
\subsection{Multiple local epochs}
While \name can handle multiple images within a local iteration, the attack must also be able to handle multiple local epochs\footnote{}. We specifically discuss the reconstruction when the same images activate the same neuron over several local epochs. Consider a case of two local epochs using the same batch and assume that an image activates the same neuron during both. For the weight changes of the neuron over the two local epochs in linear layer leakage, we have:
\begin{align*}
    W_{t=1} &= W_{t=0} - \alpha \cdot \nabla_{image,\text{ }W_{t=0}}\\
    W_{t=2} &= W_{t=1} - \alpha \cdot \nabla_{image,\text{ }W_{t=1}}\\
     &= (W_{t=0} - \alpha \cdot \nabla_{image,\text{ }W_{t=0}}) - \alpha \cdot \nabla_{image,\text{ }W_{t=1}}
\end{align*}
\noindent For FedAVG reconstruction, we have that $\nabla_{image,\text{ }W_{t=n}} = \frac{Image}{scale_n}$. We can then reformulate the updates as:
\begin{align*}
    W_{t=2} &= W_{t=0} - \alpha \cdot (\frac{image}{scale_0} + \frac{image}{scale_1})\\
    &= W_{t=0} - \alpha \cdot image\cdot({scale_0}^{-1} + {scale_1}^{-1})
\end{align*}
\noindent and the image can then be recovered directly as:
\begin{align*}
    image = \frac{W_{t=2} - W_{t=0}}{\alpha \cdot({scale_0}^{-1} + {scale_1}^{-1})}
\end{align*}
\noindent This reconstruction is also generalizable to $n$ local epochs:
\begin{align*}
    image &= \frac{W_{t=n} - W_{t=0}}{\alpha \cdot({scale_0}^{-1} + {scale_1}^{-1} + \cdots +{scale_n}^{-1})}\\
    &= \frac{W_{t=n} - W_{t=0}}{c}
\end{align*}
\noindent where the denominator can be simplified to a single scalar value $c$, which can be estimated during reconstruction as discussed in Section~\ref{sec:reconstruction_equation}.

Overall, reconstruction for multiple local epochs is very similar to the baseline FedAVG case. However, as shown in Section~\ref{sec:FedAVG_CSF}, the situation of multiple activations can be prevented by \name with a large \textit{CSF}, as the same image no longer activates the neuron due to parameter shift. This is especially true for the same images, as they are exactly the opposite of orthogonal. Regardless, multiple local epochs does not affect \name's reconstructions.
\end{comment}

\subsection{Setting up FC layer biases}
\label{sec:setting_biases}
\vspace{-1mm}
While leaking images from a linear layer is superior in reconstruction speed and quality compared to optimization, the initialization of neuron biases can pose a challenging problem in practice. The weights of the FC layer measure some aspect of an image such as the average pixel intensity and the biases must be used as cut-offs for image activations. If not set properly, the number of recovered inputs is lower. For example, if we initialize completely randomly on CIFAR-100, the leakage rate can drop by over 20$\%$ as shown in Appendix Table~\ref{tab:bias_initialization}.
% SB (10/11/22): Can you give a quantitative evidence here, e.g., if we randomly initialize, then the leakage rate is only XXX\%. 
% JZ (10/11/22): Added.
If the server knows the dataset distribution for average pixel intensity, as assumed in~\cite{fowl2022robbing}, setting the biases of the FC layer is simple. However, in practice client datasets are private, so the server is unlikely to have such knowledge.

We therefore incrementally learn the distributions of the dataset.
If the FC layer weights were used to measure the average pixel intensity, this value must be between $0$ and $1$. An initialization for the biases following this could be so that the biases have a mean of $-0.5$ and standard deviation of $0.25$.
% \atul{why negative} 
% JZ (4/13/23): Since the weights are positive, we end up needing a negative bias in order to act as the threshold the ReLU activation.
This setting is progressively improved through training iterations by observing the neuron activations. After receiving the aggregated gradient, the server observes which neuron activations result in a successful reconstruction and which do not (same neuron activated multiple times). For each observation, it also notes the bias values of the neuron and using these observed biases, it computes a new mean and variance over them to use for the initialization of the biases for the next iteration. Over several training rounds, the server arrives at a close estimate of the dataset distribution without any prior knowledge. 
This determination is done by the server separately for each client in the case of non-IID data where clients may have vastly different distributions. Additionally, the server can observe the type of distribution (i.e. normal, multi-Gaussian etc.) and set up biases to fit them. For this work we setup biases following a normal distribution. 
% Learning and fitting to more distributions is future work.

\vspace{-1mm}
\subsection{Identifying client data}
\vspace{-2mm}
Even if the aggregated gradient is able to leak training data, it may appear to be of some comfort that the attacker cannot identify which data belongs to which client. However, \name gives the attacker the ability to identify the owner of the reconstructed data even through aggregation.
The identity mapping sets keep the weight gradients for each client separate after aggregation. As a result, when reconstructing inputs, the set of weight gradients used allows the server to identify which model it originates from and hence, which client it originates from. % Not only is the data leaked, but the client who owns the data is also identified in the process. 
With this information, the server may subsequently focus on specific high-value clients.

\vspace{-1mm}
\subsection{Parameter comparison}
\vspace{-2mm}
For linear layer leakage, attacks where the server modifies the model, a key question is how many parameters the attack adds. A small addition is desirable as we are operating in the cross-device setting where communication and storage are at a premium. The  fundamental premise behind linear layer leakage is using the gradients of a model update to store the information of the input data. For example, with a $32\times32\times3=3072$ image, the model must have at least $3072$ weight parameters to store this. To store $1000$ images, we would then need $3072\cdot1000=3,072,000$ parameters. However, exactly $1000$ units for $1000$ images assumes that every image activates a separate neuron, an extremely optimistic case (for the attack). The FC layer size will typically need to be larger than the number of images (4$\times$ is typical).

For comparison between \name and RtF for number of added parameters, consider we have $100$ clients with a local dataset size of $64$. Each image is given $4$ units, resulting in $256$ units for each client. For simplicity, we will ignore the number of bias parameters, as the amount is much smaller than the weight parameters (e.g. for \name, $0.005\%$ of the total parameters are biases). For RtF~\cite{fowl2022robbing}, it ends up needing 157M parameters while \name needs roughly half (50.7\%). The basic reason for this reduction is that RtF needs a large first FC layer (25,600 = batch size $\times$ multiplicative factor (4) $\times$ \# clients), while we only need 256. The number of connections from the first to the second FC layer are determined by dimensionality considerations and cannot be reduced. This improvement in \name is taking into account the additional parameters due to the convolutional kernels. 

% New material for S\&P
The added parameters are also extremely sparse, as only the weight parameters connecting the identity mapping set kernel outputs to the first FC layer need to be non-zero ($3$ out of $300$). When attacking $100$ clients, 98\% of our total added parameters are zero. As the number of clients increases, the absolute number of non-zero parameters at each client stays constant. Sparsity can allow for additional compression of the model/update and more efficient optimization to reduce the computational and communication cost overhead significantly~\cite{duff1989sparse,paszke2019pytorch,abadi2016tensorflow}.

% SB (4/13/23): Full-length section
\begin{comment}
For linear layer leakage, attacks where the server modifies the model, a key question is how many parameters the attack adds. A small addition is desirable as we are operating in the cross-device setting where communication and storage are at a premium. The basic intuition behind linear layer leakage is using the gradients of a model update to store the information of the input data. For example, with a $32\times32\times3=3072$ image, the model must have at least $3072$ weight parameters to store this. To store $1000$ images, we would then need $3072\cdot1000=3,072,000$ parameters. However, exactly $1000$ units for $1000$ images assumes that every image activates a separate neuron, an extremely optimistic case (for the attack). The FC layer size will typically need to be larger than the number of images.

For comparison, we have $100$ clients with a local dataset size of $64$. Each image is given $4$ units, resulting in $256$ units for each client. For simplicity, we will ignore the number of bias parameters, as the amount is much smaller than the weight parameters (e.g. for \name, $0.005\%$ of the total parameters are biases). For RtF~\cite{fowl2022robbing}, this setup would use a total of $100\cdot64\cdot4=25,600$ units in the FC layer. The number of weight parameters added for this layer would be $(32\cdot32\cdot3)\cdot25,600=78,643,200$. \name would have a fixed FC layer size of $256$ units and use $100\cdot3=300$ convolutional kernels. The output of the convolutional kernels would have a dimension of $32\cdot32\cdot300$. The weight parameters between the output and the first fully connected layer would also be $(32\cdot32\cdot300)\cdot256=78,643,200$.

However, as injected attacks, the output dimension must be the same as the input. Thus, the second FC layer would have size $32\cdot32\cdot3=3072$. RtF would then add another $25,600\cdot3072=78,643,200$ parameters to the model. On the other hand, since \name's first FC layer is only $256$ parameters, it would add $256\cdot3072=786,432$ parameters (1\%). Counting the convolutional weights, \name adds another $300\cdot3\cdot3=2,700$ parameters. Ultimately, \name adds roughly half the number of parameters compared to RtF while maintaining the same leakage rate. \textcolor{red}{The added parameters are also extremely sparse, as only the weight parameters connecting the identity mapping set kernel outputs to the first FC layer need to be non-zero ($3$ out of $300$). The parameters connecting the first FC layer to the second are not sparse, but the number of parameters is small. When attacking $100$ clients, $\approx$98\% of our total added parameters can be zero. As the number of clients increases, the absolute number of non-zero parameters will remain constant. Sparsity can allow for additional compression of the model/update and more efficient optimization to reduce the computational and communication cost overhead significantly~\cite{duff1989sparse,paszke2019pytorch,abadi2016tensorflow}.}
\end{comment}


\subsection{Model inconsistency}
\label{sec:model_inconsistency}
\vspace{-2mm}
We have so far discussed the attack in the context of using separate identity mapping sets for each client. This method inherently creates model inconsistency among clients. However, this is not a requirement in \name. Some clients can be sent the same convolutional kernels. For any clients that are sent the same identity mapping set, if images between clients activate the same neuron, there will be failure in reconstruction, so the size of the FC layer will need to be increased accordingly. The two extremes of model inconsistency are: full model inconsistency where all clients get different models, and no model inconsistency where all clients get the same model. 

In the latter case of no model inconsistency, the original scalability problem seems to exist. As the size of the FC layer increases with the total number of clients or local dataset size, the magnitude of the scaled parameters increases. However, the convolutional scaling factor of \name offsets this change and prevents reconstruction problems. The $\textit{CSF}>1$ setting allows us to avoid precision problems and thus keep leakage rate high. There are some additional downsides to not using model inconsistency for \name, such as a slightly lower leakage rate (Figure~\ref{fig:scalability_rtf}), larger model size overhead (Figure~\ref{fig:model-inconsistency-size}), or losing the ability to identify client data ownership. However, with no model inconsistency \name still achieves scalability to an arbitrary local dataset size or number of clients in FedAVG.


