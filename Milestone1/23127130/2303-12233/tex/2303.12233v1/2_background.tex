\begin{table*}[t!]
\small
\captionsetup{justification=centering}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
                                                                            & \textbf{Attack type} & \textbf{\begin{tabular}[c]{@{}c@{}}Leakage\\ scale\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}\# training\\ rounds\end{tabular}}   & \textbf{\begin{tabular}[c]{@{}c@{}}Reconstruction\\ quality\end{tabular}} &  \textbf{\begin{tabular}[c]{@{}c@{}}FL aggregation\\ support\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Non-IID client\\ support\end{tabular}} \\ \hline
\begin{tabular}[c]{@{}c@{}}Deep Leakage\\ (NeurIPS '19)~\cite{zhu2019deep}\end{tabular}        & Optimization         & $10^0$                                                                      & Single                                                                     & Estimation                                                                         & No                                                                        & \multirow{8}{*}{N/A}                                                           \\ \cline{1-6}
\begin{tabular}[c]{@{}c@{}}GradInversion\\ (CVPR '21)~\cite{yin2021see}\end{tabular}          & Optimization         & $10^1$                                                                     & Single                                                                  & Estimation                                                         & No                                                                        &                                                                                \\ \cline{1-6}
\begin{tabular}[c]{@{}c@{}}Eluding Secure Agg.\\ (ACM CCS '22)\cite{pasquini2021eluding}\end{tabular} & Analytic               & $10^1$ & Single    & Estimation                                                         & Yes (one client)                                                          &                                                                                \\ \cline{1-6}
\begin{tabular}[c]{@{}c@{}}Fishing for User Data\\ (ICML '22)~\cite{wen2022fishing}\end{tabular}  & Analytic               & $10^0$                                                      & $2-10$    & Estimation                                                                         & Yes (one image)                                                           &                                                                                \\ \cline{1-6}

\begin{tabular}[c]{@{}c@{}}Gradient disaggregation\\ (ICML '21)~\cite{lam2021gradient}\end{tabular}  & Analytic               & $10^3$                                                      & $10^2$    & Estimation                                                                         & Yes                                                           &                                                                                \\ \cline{1-7}

\begin{tabular}[c]{@{}c@{}}Robbing the Fed\\ (ICLR '22)~\cite{fowl2022robbing}\end{tabular}        & Analytic           & $10^{1^*}$                                                     & Single                                                                       & Exact                                       & Yes                                                                   & No                                                                               \\ \hline
\textbf{\name}                                                         & \textbf{Analytic}  & $\mathbf{10^3}$                                             & \textbf{Single}                                                              & \textbf{Exact}                                               & \textbf{Yes}                                                              & \textbf{Yes}                                                                   \\ \hline
\end{tabular}
\end{center}
\vspace*{-5mm}
\caption{\label{tab:comparison_table} Comparison of attack features. ($^*$Leakage scale of Robbing the Fed~\cite{fowl2022robbing} when the same number of non-zero parameters as \name are added.)}
\vspace*{-4mm}
\end{table*}

% \footnotetext{Leakage rate of Robbing the Fed~\cite{fowl2022robbing} for the same number of non-zero parameters added as \name.}

\noindent
While many different attacks on FL have been proposed, we focus on data reconstruction attacks, the strongest attacks on privacy in FL. 
% SB (10/10/22): What does above mean - data reconstruction attacks are the strongest known attacks against privacy in FL?
% JZ (10/10/22): Rephrased this to follow that
Reconstruction attacks aim to break the fundamental notion of privacy of FL by obtaining client data directly through their gradient updates. 
% SB (10/10/22): directly from what? Gradient updates?
% JZ (10/10/22): Yes, added
Prior work has done this through analytic methods~\cite{boenisch2021curious,fowl2022robbing}, optimization~\cite{zhu2019deep,geiping2020inverting,zhao2020idlg,yin2021see}, hybrid approaches combining both~\cite{pasquini2021eluding,wen2022fishing}, or other approaches including GANs~\cite{hitaj2017deep,wang2019beyond}. These attacks typically aim to attack either individual client gradients or aggregated gradients. 
\begin{comment}
With individual gradients, attacks focus on increased effectiveness for larger batch sizes. However, problems with batch size are directly magnified in aggregation. For example, cross-device FL has hundreds of clients in each training round. The aggregated gradient would then include contributing gradients from hundreds or thousands of images resulting in essentially a "massive-batch" gradient. Furthermore in realistic scenarios where secure aggregation is used~\cite{bonawitz2019towards}, this aggregate gradient is all that a malicious server has access to. 
\end{comment}
The following subsections will discuss the details and limitations of prior attacks under these two categories.

\subsection{Individual gradients}

\noindent
\textbf{Optimization-based attacks}. Optimization approaches have shown very strong success on individual updates, especially with smaller batch sizes. These attacks typically operate under the threat model of an honest-but-curious server or an external attacker that has access to the model and individual gradients from each client. With only this information, the attacker initializes some dummy data and computes the gradient of that data on the model.

\begin{equation}\label{eq:optim}
    x^* = \arg \min_{x}||\nabla L(x, y, \theta)-\nabla W||_2
\end{equation}

\noindent
An optimizer then minimizes the difference between the generated gradient $\nabla L(x, y, \theta)$ and the ground truth gradient $\nabla W$. Here $x$ is the dummy data, $x^*$ is the reconstructed data, $L$ is the loss function, $y$ is the label, and $\theta$ is the model parameters.

More recent optimization approaches~\cite{geiping2020inverting,yin2021see} work under the assumption that user labels are known prior to optimization. Typically, these labels are directly retrieved through a zero-shot solution without using optimization approaches~\cite{zhao2020idlg,yin2021see}. Furthermore, regularizers and strong image priors specific to image data are often used to guide optimization results~\cite{geiping2020inverting,yin2021see}. These can also result in image artifacts typical of an image class, but not in the actual training image. These approaches have shown surprising success with image data on smaller batch sizes. However, as batch sizes increase, the fraction of images recovered decreases along with the reconstruction quality and the number of iterations required for the optimization also increases. One reason stated by~\cite{zhu2019deep} was the multiple possible input image permutations within the batch. Essentially, regardless of the order of images in the batch, the same gradient will be achieved. Another, more fundamental reason is having a larger batch size means that more images are present and the server will need to reconstruct more image pixels.
% SB (10/10/22): The last sentence is not clear. This is the clinching sentence where we need to convince the reviewer that all these prior approaches fail at large batch sizes. 
% JZ (10/10/22): I have tried to clear this up by rephrasing a bit. But I added a more basic intuition, mainly that a larger batch size means more images, which in turn just means more pixels to reconstruct.

\noindent \textbf{Analytic attacks}. We have earlier discussed the shortcomings of the leading works that fall under this category~\cite{wen2022fishing, pasquini2021eluding, kariyappa2022cocktail}. 
Another work~\cite{lam2021gradient} has looked to enable prior individual gradient methods through gradient disaggregation, separating out individual updates over time. However, in addition to requiring additional side-channel information about client participation, the method also requires a large number of training iterations to accomplish the goal. Furthermore, due to partial user selection~\cite{cho2020client,chen2020optimal}, it is shown that the server may also be able to reconstruct the individual models of some users using the aggregated models from the previous rounds~\cite{pejo2020quality,secagg_so2021securing}. However, these approaches also require multiple training rounds and can be prevented by proper client selection, such that no individual updates can be singled out.


\noindent \textbf{Linear layer leakage attacks}. This is a sub-class of analytic attacks. Leaking the input to a linear layer through the weight and bias gradients was discussed in~\cite{phong2017privacy,fan2020rethinking}. When only a single image activates a neuron in a fully connected layer, the input to that layer can be directly computed using the resulting gradients of the neuron.

\begin{equation}\label{eq:1}
    x^i = \frac{\delta L}{\delta W^i} / \frac{\delta L}{\delta B^i}
\end{equation}

\noindent
where $i$ is the activated neuron, $x^i$ is the input that activates neuron $i$, and $\frac{\delta L}{\delta W^i}$, $\frac{\delta L}{\delta B^i}$ are the weight gradient and bias gradient of the neuron respectively. This idea forms the basis for several reconstruction attacks~\cite{pasquini2021eluding,fowl2022robbing}. Figure~\ref{fig:linear-leak-method} shows the process of leaking images through the layer. 
% SB (10/10/22): We are asking the reviewer to take this claim at its face value. So we need to give the citation and maybe a qualitative intuition behind it. 
% JZ (10/10/22) I have moved the figure to right after this to show the reviewers what is happening. I will also cite prior work that use this methodology.

When the fully-connected layer is placed at the start of a network, the data reconstructed from the the layer would then be the input data. This reconstruction is also exact, as opposed to the optimization approaches which function more as estimations. However, inputs are only reconstructed exactly when only a single data sample activates that neuron. If more than one input activates the neuron, the weight and bias gradients of these inputs will contribute to the batch gradient. 
% SB (10/10/22): What's meant by "total gradient"?
% JZ (10/10/22): Changed to "batch gradient"
When the gradient division of Equation~\ref{eq:1} is done to retrieve the input, the resulting reconstruction would be a combination of all contributing images, a case of failed attack. 
% \atul{I have forgotten the intuition how this happens. It will be helpful if we add that intuition here - will help motivate the binning approach}

\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.9\columnwidth]{Plots-Images/linear-layer-leak.drawio.png}
\end{center}
\vspace*{-4mm}
\caption{\label{fig:linear-leak-method} Using the weight gradient $\frac{\delta L}{\delta W}$ and bias gradient $\frac{\delta L}{\delta B}$ of a fully connected layer to reconstruct the inputs. Neuron $i$ is only activated by a single image, while $j$ is activated by two. As a result, the reconstruction of neuron $i$ is correct while $j$ is a combination of images.}
\vspace*{-5mm}
\end{figure}

\begin{figure*}[!t]
\begin{center}
\includegraphics[width=0.9\textwidth]{Plots-Images/weight-grad-deagg.drawio.png}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:identity_maps} The inserted attack module of a convolutional layer and two FC layers. Images are leaked using the gradients of the weight parameters connecting the convolutional output to the first FC layer. Weight gradient de-aggregation is done through separate identity mapping sets, indicated by the different color values in the convolutional kernels. In the aggregate gradient, the weight gradients are separate between models and are used to leak the images. \iffalse \saurabh{Can we show within this figure, toward the bottom, the calculation for how many clients and batch size can be supported, i.e., calculate the sizes of our layers as a function of that.}\fi}
\vspace*{-3mm}
\end{figure*}

To alleviate this problem,~\cite{fowl2022robbing,boenisch2021curious} use malicious modification of the parameters in the FC layer. For~\cite{boenisch2021curious}, they introduced trap weights, initializing the weights randomly to be half positive, half negative. In order to ensure that neuron activation was less common, the negative weights would come from a larger negative range than the positive weights. They also discuss the use of convolutional layers to push the input image forward, allowing the attack to function on models starting with convolutional layers followed by fully-connected layers. However, with a similar method,~\cite{fowl2022robbing} introduced another approach that outperforms the previous. Using an approach called "binning," the weights of the FC layer would measure some known continuous CDF of the input data such as image brightness. The bias for each neuron then serves as a different cutoff, allowing only inputs with a high enough value to activate it. The goal of this method would be that only one input activates each "bin," where the bin is defined as the activated neuron with the biggest cutoff (for ReLU, the largest negative bias)\footnote{The bin biases are set as negative. The weights are positive and so the negative bins are used to prevent ReLU activation.}. For any case where only one input activates a bin, it can then be reconstructed as

\begin{equation}\label{eq:2}
    x^i = (\frac{\delta L}{\delta W^i} - \frac{\delta L}{\delta W^{i+1}}) / (\frac{\delta L}{\delta B^i} - \frac{\delta L}{\delta B^{i+1}})
\end{equation}

\noindent
where $i$ is the activated bin and $i+1$ is the bin with the next largest cutoff bias.

For this equation to hold true, the attack requires the use of two consecutive FC layers. 
%\atul{Do they need to be consecutive?}
The first layer is used to leak the inputs using Equation~\ref{eq:2} and the second FC layer maintains the requirement that $\frac{\delta L}{\delta B^i}$ and $\frac{\delta L}{\delta W^i}$ are the same for any neuron that the same input activates. This is achieved by having the same weight parameters connecting each neuron of the first FC layer to the second FC layer. For example, if the first FC layer has $1024$ units and the second has $256$, the weights connecting them would have 
a dimension of $1024\times256$. Then, if every row of the weight matrix is equivalent, e.g. $[0, :]=[1, :] = \dots = [1023, :]$, then the above property holds.
% SB (10/10/22): I do not understand from "and the second FC layer ...". NTU. 
% JZ (10/10/22): Rephrased this.

\subsection{Aggregated gradients}

\noindent
While the previous methods mainly function in the setting of a single client gradient, when the attacker only has access to aggregated gradients, such as with secure aggregation~\cite{bonawitz2017practical}, they tend to perform significantly worse. With the sheer number of data points in an aggregated gradient, optimization approaches fail. The analytic approaches of~\cite{fowl2022robbing,boenisch2021curious} do work, but the number of reconstructed images will be drastically decreased without an increase in the linear layer size, often resulting in only a few images recovered. 
%Appendix~\ref{sec:appendix} discusses scaling the method of~\cite{fowl2022robbing} towards aggregation. 
The only real way to keep these attacks effective is by increasing the size of the linear layer, thus increasing the number of model parameters (Section~\ref{sec:param-comp}).

Targeting the scenario of aggregated gradients, several recent approaches have focused on attacks we classify as magnification or minimization of gradients. In~\cite{pasquini2021eluding}, attackers send different models to clients such that the resulting aggregated gradient is only a targeted client's individual gradient. This is done by sending model parameters such that ReLU activated layers would have fully dead units (and an update with zero gradients) for any non-targeted client. The targeted client would get a standard model, which would then be the only one to return non-zero gradients. These could then be used for optimization. On the other hand,~\cite{wen2022fishing} focuses on attacking a single data point through gradient magnification. The server sends weights to magnify the gradients of a targeted class by decreasing the model confidence on that class' prediction. Within the targeted class, the server will also focus on a specific "feature" in order to target a single sample. The resulting gradient will be similar to the gradient for a single image, allowing optimization based approaches to retrieve the input. However, this process requires multiple training iterations. The attack can also only target a single image each time, and even in this case it does not succeed every time. Another method treats the inputs of to a fully-connected layer as a blind separation problem~\cite{kariyappa2022cocktail} where the weight gradients for the neuron make up a set of weighted combinations of the inputs. While the approach is able to attack aggregated gradients, the reconstruction quality is low and the number of inputs is limited to be $1024$ or fewer.
% SB (10/10/22): It is not clear what the last two parts (reconstruction and FC layer size) mean. 
% JZ (10/10/22): Reconstruction quality is lower than previous appraoches and the number of images/inputs cannot be greater than 1024. I have removed the statement about FC layer size, as the specific reason behind the limitation is not that relevant for us

One approach with the goal of enabling prior methods for individual updates is~\cite{lam2021gradient}. They focus on directly disaggregating the gradients. Using knowledge of client participation rates, the server can learn exactly which clients are participating during each training iteration. Over time, the updates can then be split apart. However, this additional information used for the attack is not required for FL. The number of training iterations required for the attack to succeed is also very large.


\noindent \textbf{Limitations}. Prior approaches attacking aggregated gradients~\cite{boenisch2021curious,pasquini2021eluding,wen2022fishing,lam2021gradient,fowl2022robbing} still suffer from several fundamental limitations. ~\cite{pasquini2021eluding,wen2022fishing,fowl2022robbing} suffer from small attack scale,~\cite{lam2021gradient} needs a very large number of training rounds, and~\cite{pasquini2021eluding,lam2021gradient} create images with poor quality when the batch size is larger, due to a reliance on optimization techniques for reconstruction. Table~\ref{tab:comparison_table} shows the attack features of prior works, highlighting these limitations. 
% \saurabh{This is key --- where are these limitations spelled out? Is it in the Table 1? If so, refer to it and explain that table.}
% Resolved
Naturally, the question we pose ourselves is: how much further can an attacker push the limits of reconstructing data using aggregated gradients? In the next section we show how we break these previous limitations, achieving large-scale and high-quality reconstructions using the aggregated gradient from only a single training iteration.
