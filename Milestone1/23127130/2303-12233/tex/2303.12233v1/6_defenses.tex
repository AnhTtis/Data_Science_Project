\noindent
As a major outcome of our work, we find that using secure aggregation for defense does {\em not} prevent a malicious server from recovering private user data. This holds true regardless of the number of clients participating. 

Another promising defense method is secure shuffling, which was discussed in the context of federated learning by~\cite{kairouz2021advances}. Secure shuffling prevents a server from receiving any additional information (outside of the update itself) which would allow identification of which client sent each individual message. However, secure shuffling cannot nullify our attack as the update from the client itself carries the fingerprint of the client. Thus, the server while reconstructing the image can also identify the client it came from. 
% does not apply any more security on top of aggregation. Furthermore, neither secure aggregation nor secure shuffling prevents our attacker from identifying client data ownership during reconstruction.

One possibility of defense is through identification of a modified model architecture or the parameters. Due to a difference in model architecture and non-standard parameters, a client could attempt to identify malicious tampering prior to training on local data and sending an update. This process could be difficult due to the server's ability to mask the weight gradient in different ways, as discussed in Section~\ref{sec:method-mask}, but is still a possibility. However, regardless of the difficulty, there is still a fundamental problem with identification in the setting of cross-device FL since clients have very limited computational abilities, which may preclude such verification. They typically only follow a standard federated learning protocol given to them that will only consist of training the model and then sending the update back. 

One mitigation strategy used in federated learning would be using differential privacy~\cite{dwork2014algorithmic,jayaraman2019evaluating,wei2020federated} to add noise to updates prior to sending them to the server. However, while this method can be effective in preventing a server from fully reconstructing private data, it comes with the hefty downside of a decrease in model performance. As an attack, the malicious architecture or parameter modification does not need to occur during every step of the training process. Without knowledge of when the attack will occur, differential privacy must be applied during all stages of the training process.

\begin{figure}[t!]
\vspace*{0mm}
\begin{center}
\includegraphics[width=0.85\columnwidth]{Plots-Images/non-iid-multi-batch.png}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:organ-leakage-results} Number of images leaked for several batch sizes from the OrganAMNIST dataset with $100$ non-IID clients over several training rounds of observing individual distributions. The first training iteration starts with a dataset agnostic bin initialization. The training is effective even after just a single round.}
\vspace*{-5mm}
\end{figure}
