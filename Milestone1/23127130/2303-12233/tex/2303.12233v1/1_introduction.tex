\begin{figure}[!ht]
\begin{center}
\includegraphics[width=1.0\columnwidth]{Plots-Images/attack_overview.drawio.png}
\end{center}
\vspace*{-8mm}
\caption{\label{fig:SA-overview} An overview of federated learning with secure aggregation. A malicious server who wants to leak client data can only gain access to the aggregate update.}
\vspace*{-6.5mm}
\end{figure}
% AE (10/11/22) you are using the same notation x for gradient in the above figure and also use it for the data.
\noindent
Federated learning (FL)~\cite{mcmahan2017communication} is a machine learning paradigm introduced to address growing user data privacy concerns where clients participate in large-scale model training without sending their private data to servers for centralized training. A general training round in FL consists of a server sending the model out to the clients and clients training the received model with their private data before sending their updates back to the server. The server then aggregates the updates from the clients and uses this to update the global model before sending it out to repeat another iteration of this training process. 

While the promise of FL is that user data should be private and secure from a malicious server, this only holds true if gradients from the clients  cannot be used to infer properties of the training data. Prior work has shown that property inference~\cite{melis2019exploiting,luo2021feature}, membership inference~\cite{shokri2017membership,choquette2021label,nasr2019comprehensive} or GAN-based attacks~\cite{hitaj2017deep,wang2019beyond} are all effective in inferring information about client data from their shared gradients. However, the stronger class of reconstruction attacks, when a malicious server wants to directly steal private client training data, has been demonstrated against even the strictest defenses. These attacks fall into two categories.
% SB (10/10/22): The sentence above is too convoluted. Say directly that reconstruction attacks have been demonstrated against even the strictest defenses and these attacks fall into two categories. XXX
% JZ (10/10/22): Done. I have still included the citation for the other attacks, but I separated it to be more clear.
Optimization attacks~\cite{zhu2019deep,zhao2020idlg,geiping2020inverting,yin2021see} generally work only on image data, starting with a dummy randomly initialized data sample and optimizing over the difference between the true gradient and the one generated through the dummy sample. Iteratively, the dummy data sample  is updated and gets closer to the ground truth data that was used in computing the true gradient. However reconstructed images following this method degrade in quality as the batch size and image resolution increase, failing with a batch size of 48 or more as demonstrated in~\cite{yin2021see}  on the ImageNet dataset. % ~\cite{russakovsky2015imagenet}.
% SB (10/10/22): Be careful when switching to the specific case of images from general data. Also give some quantitative intuition like these attacks are typically not effective with batch size of 64 or greater. 
% JZ (10/11/22): I have given a more quantitative value. I use 48 or more, as this is the specified batch size for the CVPR 2021 paper. Not sure how to do the citations here, the first one is for Imagenet dataset, second is for the CVPR paper.
On the other hand, analytic reconstruction attacks~\cite{fowl2022robbing,boenisch2021curious} often involve customizing the model parameters or the model architecture to directly retrieve the training data from the gradients of a fully-connected (FC) layer --- this is referred to as {\em linear layer leakage}. These approaches do not have issues with quality as they reconstruct the inputs near exactly. However, most prior work has only looked at attacking individual gradients, i.e., gradients from single clients. If secure model aggregation is applied in FL, an attacker will no longer have access to these individual gradients~\cite{bonawitz2017practical,fereidooni2021safelearn,so2021lightsecagg}. Secure aggregation  guarantees that the server and any client in the FL setting will not gain access to individual model updates of other clients, only the aggregate from all clients. This guarantee is achieved through model encryption where in each training round clients encrypt their model updates before sending them to the server such that only the aggregated model is revealed to the server. Figure~\ref{fig:SA-overview} shows a general overview of secure aggregation in FL where the server can launch a data reconstruction attack only on top of the aggregated gradients. In  Appendix \ref{appendix-SA}, we give a further discussion about secure aggregation. 
% SB (10/10/22): Since we have introduced the abbreviation, use FL instead of Federated Learning. 
% JZ (10/10/22): Okay

Secure aggregation poses a large problem for all prior  privacy attacks in FL. In particular,  optimization approaches have a high image reconstruction rate in the small batch size regime.
% (i.e.,  when the target gradient is computed over a small batch size). 
However, as the number of images in the batch increases,  the number of reconstructed images decreases and the quality of the reconstructed images diminishes. On the other hand, analytic approaches also have difficulty scaling toward large amounts of data, such as with FL aggregation, often requiring extremely large FC layers to remain effective. 
% AE (10/11/22) Is there any reference for the above statement "often requiring extremely large FC layers to remain effective" 
Some recent   works have targeted  FL with secure aggregation by magnifying gradients~\cite{wen2022fishing}, making the aggregate update the same as an individual update of a particular target user~\cite{pasquini2021eluding},
% \atul{should it be de-aggregation?}
or looking to solve a blind source separation problem~\cite{kariyappa2022cocktail}. However, these attacks still have very limited power in the aggregated setting. The first approach can only steal a single user image for each training round while additionally requiring multiple iterations prior to setup the attack. The second approach suffers from a similar limitation, only reaching a single client each round. However, this approach also relies on optimization techniques, i.e., cannot reconstruct images from a higher batch size than 48 on the ImageNet dataset. The third approach works to leak up to $1024$ images in aggregation, failing if the number is greater. Further, the reconstruction quality is also much lower.
% AE (10/11/22) {It is not clear what is the final approach in the end of the  above paragraph }
% The third prior work referenced above
% SB (10/10/22): Break down the above sentence. Convince the reviewer that these prior approaches are indeed broken for FL. Maybe one sentence each for each attack. 
% SB (10/10/22): What does "the number of inputs" mean? Number in one batch?
% JZ (10/10/22): I have updated and gone deeper for each approach. I also removed "number of inputs." What it meant was total number of samples in the aggregate gradient (num_clients * batch_size)


The setting of aggregated gradients is interesting and especially challenging, as the number of data samples within the aggregated gradient is significantly larger than an individual update. 
% When a client trains a batch, the gradients of the individual training samples within that batch are aggregated (i.e. averaged). 
% The FL server aggregation is applied on top of this, essentially doing an aggregation on top of the individual client one. 
% 
% SB (10/10/22): Do a thorough spell check. An error like this can get the reviewer thinking dark thoughts. 
% JZ (10/10/22): Okay
% SB (10/10/22): I do not understand the above sentence. 
% JZ (10/10/22): I have rephrased the statement. What I wanted to say is a batch gradient is an average across all individual client gradients. Aggregation between clients on top of this will essentially create a "large batch" where all the images (num_clients * batch_size) contribute to the gradient. 
% AE  (10/11/22) do we need to mention anything about using FedSGD as federated learning algorithm somewhere in the introduction  as all the description in the introduction  is assumed using SGD while the  reviewers  might got confused between it and FedAvg?  this formula assumes using FedSGD (Number of clients)$\cdot$(Batch size) and I think clarification is important. 
The number of samples contributing to the aggregated gradient % in FedSGD, 
and subsequently used for recreating the original data by the malicious server, is then equal to: (Number of clients)$\cdot$(Batch size). The number of clients can grow in an unbounded manner in FL, while the batch size is typically below 64-128. This is the fundamental scaling challenge of all prior privacy attacks in FL --- they struggle with increasing batch size and they are ineffective with a large number of clients in aggregation.
% SB (10/10/22): Suggested rephrasing: The number of samples used for recreating the original data by the malicious server = Number of clients X Batch size. The number of clients can grow in an unbounded manner in FL, while the batch size is typically in the 64-128 range. This is the fundamental scaling challenge of all prior privacy attacks in FL --- they struggle with increasing batch size and this means they are ineffective with a large number of clients. 
% JZ (10/10/22): I have changed the sentence to follow this. We don't show any examples of 128 batch size in our experiments though.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.9\columnwidth]{Plots-Images/comparison_leak_rate.png}
%\includegraphics[width=1.0\columnwidth]{Plots-Images/identity-mapping-set.drawio.png}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:comp-leak-rate} Total image leakage rate for \name, Eluding Secure Aggregation~\cite{pasquini2021eluding}, 
% (ACM CCS '22)
and Robbing the Fed~\cite{fowl2022robbing} 
%  (ICLR '22)
with a varying number of clients. The leakage rate of Robbing the Fed with the same number of added non-zero parameters as \name is used.}
\vspace*{-5mm}
\end{figure}

\noindent \textbf{Our work.} We introduce \name~\footnote{\namemeaning}, an attack whereby a malicious server can directly reconstruct the training data of multiple users in a single iteration using only the aggregated gradient. The attack works even when secure aggregation is applied and even if there are hundreds of clients participating in the training round. To achieve this, the malicious server modifies the model that it sends to each target client. {\em The key insight behind our attack is that the server sends customized convolutional kernels to each client, so that the gradients of the input data of each client is separable in the aggregated update and is thus recoverable at the server.} The convolutional layers are used to separate client gradients so that, even after aggregation, their individual weight gradients for a FC layer will remain apart. With this separated weight gradient, a FC layer can then be used to leak the data of each individual client. Furthermore, the attack can become stronger over multiple training iterations by observing which neurons in the FC layer the client images activate. 
% SB (10/10/22): The above sentence lacks context and needs to be expanded. 
% JZ (10/10/22): I wonder if I can remove this sentence from the introduction. The ability to learn dataset distribution for some metric is hard to explain in a way that makes sense without proper understanding of the attack setup first.
This ability to leak client data regardless of aggregation breaks the previous scaling limitations of reconstruction attacks. While previous linear layer leakage methods must scale the FC layer to address an increase in the the number of samples coming from an increasing batch size or number of clients, we instead introduce a split scaling for this through our design. \name can scale to larger client batch sizes by increasing the FC layer size, but it can also scale to a larger number of clients by increasing the number of convolutional kernels. In particular, this prevents a diminishing return in the leakage rate when higher number of clients are aggregated. 
% SB (10/10/22): I feel there is some deeply significant design in the statement above. NTU and write. 
With this property, we work especially well in large-scale aggregation such as cross-device FL, being able to leak $70\%$-$80\%$ of all images through only a single training iteration. Figure~\ref{fig:comp-leak-rate} shows the leakage rate of our method compared to~\cite{pasquini2021eluding, fowl2022robbing}. While~\cite{pasquini2021eluding} can achieve $100\%$ leakage for a single client, as the number of clients increases, the total leakage rate decreases rapidly. \name is agnostic to the number of clients, maintaining the same leakage rate regardless of the number of clients.
% SB (10/10/22): Here we need to point to the quantitative result comparing us with the two prior attacks. 
% JZ (10/10/22): I have added the plot for the leakage comparison
Since we use a linear layer leakage method as opposed to optimization, 
% SB (10/10/22): As opposed to what? Optimization-based method?
% JZ (10/10/22): Optimization, I have added clarification
the reconstruction quality is always high regardless of the number of images or clients. On top of being able to break aggregation, gradients used for reconstruction also directly trace images back to the client that owns them. As a result, information on data ownership is also obtained and allows the attacking server another degree of freedom: the ability to target high-value clients and identify their reconstructed data afterwards. 

Our main contributions are:
\vspace{-5 pt}
\begin{enumerate}
    \item We introduce \name, an attack that allows data leakage even with secure aggregation. Using a single training round on $100$ clients with a batch size of $64$, we leak $71.57\%$ ($4580$ of $6400$) training images from the aggregated gradient. 
    Further, \name can pinpoint which client each training sample comes from.
    % , not just reconstructing images in a large combined pool of data for all clients. %\atul{shall we reframe the 2nd part of the sentence to make it more clear?}
    % Removed. 
    
    \item The attack works regardless of the number of clients. By increasing the size of the network, we can continue to scale our attack to increasing batch sizes and number of clients. 
    A key design that allows us to gain scalability is splitting the increase between the convolutional layer (for increasing number of clients) and the FC layer (for increasing batch sizes). 
    This design leads to zero increase in the number of {\em non-zero} parameters of the convolutional layer with increasing number of clients and an increase of 4 units in the FC layer per image in the batch. 
    % FC layer size with an increasing batch size so that we have $4$ units per image. For each client, we add 3 convolutional kernels. However, each additional client does not add any zero non-zero parameters to the models.
    % SB (10/10/22): Add specific number here. Also how does it scale. 
    
    \item \name is able to learn the distribution of the dataset and individual clients over multiple rounds, making the attack stronger. Using this, we show the effectiveness of the attack on settings where clients have non-IID data, tailoring the attack model to each individual client. %\atul{This is an important contribution. We should also mention it in the introduction where we claim reconstruction in one iteration. Learning the distribution makes it sound more realistic.}
\end{enumerate}


\iffalse
\\~\\
Things to be covered:
\begin{itemize}
  \item General intro of FL and Gradients
  \item Secure Aggregation
  \item Previous work (very generally as we have another section)
  \item Cross-device and Cross-silo
  \item Our Work (section) and Contributions
\end{itemize}
\fi