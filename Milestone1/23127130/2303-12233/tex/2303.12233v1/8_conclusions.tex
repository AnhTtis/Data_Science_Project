\noindent
% We introduce an attack on aggregated gradients that leaks thousands of images within a single training iteration. The promise of secure aggregation is only that an attacker cannot gain access to individual client updates. We show that a malicious server can easily break all types of privacy that aggregation wants to achieve, regardless of the size and number of clients in aggregation.

\begin{comment}
1. We break privacy of secure aggregation in FL setting through server sending customized model updates. 
2. We use leakage through a linear layer, i.e., through the FC layer.
3. Our key design idea is that we send customized convolutional kernels to each client, an identity mapping set, that causes the data to be pushed unchanged through the initial layers. 
4. We are the first to do this at scale. Scale argument.
5. We show empirically that our leakage rate is between 60-70\% with CIFAR-100, Tiny ImageNet, and  MNIST with 100 clients, while that of the state-of-the-art is less than 1\% when the same number of non-zero parameters is used. 
6. We can operate in non-iid setting, which is an important subclass of FL. For this, we learn the client-specific parameters through an initial few updates, often through just a single training round. 
\end{comment}
\noindent
In this paper, we have demonstrated how to break the privacy of secure aggregation in federated learning through a malicious server that sends customized modified models to each client. The attack works by inserting a client-specific convolutional layer followed by two fully connected layers in front of the original application model. Our key design idea is to send customized convolutional kernels to each client, an identity mapping set, that separates the weight gradients of data points between clients despite the use of secure aggregation. The server then uses these weight gradients to reconstruct the original data points. We are the first to achieve a privacy attack in FL that scales well, with the size of the batch and the number of clients. For us to handle an increasing batch size, the fully connected layer size increases linearly and so the number of parameters increases linearly. For us to handle an increasing number of clients, the size of the convolutional layer increases linearly and so the total number of parameters grows linearly, while the number of non-zero parameters stays constant. We show empirically that our leakage rate is between 70-80\% with CIFAR-100, Tiny ImageNet, and  MNIST with 100 clients, while that of the state-of-the-art is less than 1\% when the same number of non-zero parameters is used. 
We can operate in non-IID setting, which is an important subclass of FL. For this, we learn the client-specific parameters through an initial few updates, often through just a single training round. 