% \begin{table*}
% \captionsetup{justification=centering}
% \begin{center}
% \includegraphics[width=1.0\textwidth]{Plots-Images/table-features.drawio.png}
% \end{center}
% \vspace*{-5mm}
% \caption{\label{tab:comparison_table} Comparison of attack features compared to existing work. Hybrid attacks manipulate gradient outputs and use the previous optimization methods to reconstruct data from them.}
% \end{table*}

\noindent
 As discussed previously, optimization becomes much more difficult with larger batch sizes and aggregation. The main problem is with more data to approximate, reconstructions end up having much lower quality and ultimately fail at larger batch sizes, even before aggregation is applied. On the other hand, the linear layer leakage methods~\cite{phong2017privacy,fowl2022robbing,boenisch2021curious} provide a powerful way of directly reconstructing training data without having a reconstructed quality issue. Increasing batch sizes does not decrease reconstruction quality, although the number of recovered images decreases. 
 % SB (10/11/22): Recovered from the gradients of individual data elements in the batch?
 % JZ (10/11/22): Changed to focus on the scalability aspect
 However, there are problems with attack scalability, which this work \name solves. 

% rather, only when multiple images activate the same neuron will reconstruction fail. With a large enough FC layer (imprint size), the fraction of recovered images is also relatively high. For individual batches of training data, these are generally insignificant problems. However, aggregation makes the immediate problem much worse. If two or more images across any of the clients activate the same bin, leakage fails. In this section, we detail our approach of using the convolutional layer parameters to separate the gradients between each client, building on the baseline of linear layer leakage. Specifically, when leaking from the FC layer, we use the method of "binning" from~\cite{fowl2022robbing} since it achieves the best results of any linear layer leakage method. Table~\ref{tab:comparison_table} shows a comparison of features between our work and previous reconstruction attacks.

\subsection{The problem of aggregate collision}
Compared to the optimization approaches, linear leakage methods are still able to scale to the FL setting by adding more units to the fully connected layer. In the prior work, Robbing the Fed~\cite{fowl2022robbing} has shown that they achieve the highest leakage rate among all linear layer leakage methods, so we use this as a baseline. However, like other linear leakage methods the scalability problem of aggregation still exists. For a single client, if multiple images from that client's batch activates the same bin, reconstruction fails. This problem is exacerbated with aggregation. If multiple images \textit{across any of the clients} activate the same bin, reconstruction also fails. Therefore, for a successful reconstruction, only one image from any client can activate a bin. As more clients participate, the number of images shared across the bins also increases accordingly. Consider a case where we have $100$ clients each with a batch size of $64$. If we have an FC layer of $256$ units, this means $64\cdot100=6400$ images are shared over $256$ bins, resulting in an average of $25$ images per bin, which leads to collisions. The solution for this problem is then to continually increase the size of the fully connected layer to compensate for more client participation. However, this approach will heavily scale up the model size, as each neuron added to the first FC layer adds parameters connecting the input to the first FC layer and parameters between the first and second FC layers. In total, each neuron adds a number equal to two times the size of the input image size. 

Ultimately, current linear layer leakage methods can only scale the size of the FC layer for an increasing batch size or number of aggregate clients. \name breaks this problem and separates the scaling of the batch size and aggregate clients. We increase the size of the FC layer for larger batch sizes and the number of convolutional kernels for more clients.

\subsection{Threat model}
We operate under the same threat model as~\cite{fowl2022robbing,boenisch2021curious,lu2022federated}, 
% SB (10/11/22): For believability, add a bunch of other cites that have the same threat model
% JZ (10/11/22): Added another two citations. The last one is not for reconstruction attacks, but they use model modification in unsupervised FL
a malicious federated learning server with the ability to manipulate the model architecture and parameters before sending the model to clients. We attack the model by inserting a malicious module at the front of the architecture. We operate in cross-device FL, a setting where hundreds of clients participate in a single round of aggregation. Clients in this setting do not have the power to do a thorough verification of the models sent by the server. 
% SB (10/11/22): Why cross-device? Then the devices do not have the power to do thorough verification of the models sent by the server and do not do multiple rounds of GD between communication with the server. 
% JZ (10/10/22): Added the reason why we use cross-device. I'm not sure what GD means so I haven't added it yet
The goal of the malicious server is to recover private training data. However, due to the application of secure aggregation, outside of the aggregate gradient, the server cannot receive any information about the clients or about individual updates. 
% With this threat model, we will show the ability of an attacker to target specific clients and reconstruct massive numbers of images from only the aggregate gradient of a single training iteration.

\subsection{Attack scope}
\label{sec:attack-scope}
Under the threat model, a server can insert a module of a few layers at the start of the model. With this, \name can leak the inputs to that model
% SB (10/11/22): This is dangerous to say "any model architecture". Then the reviewer has just to come up with one example where our work is not applicable. 
% JZ (10/10/22): I looked at RtF again, and they haven't said much about being model architecture agnostic. I think it will be fine to remove that claim, so I mainly discuss how we can work on any kind of data instead. This data claim is made in RtF too.
regardless of the data type (e.g. image, video, audio, text), similar to~\cite{fowl2022robbing}. We experimentally demonstrate this attack primarily on image data. 
% SB (10/11/22): Like most of prior work?
% JZ (10/10/22): Is it important to specify that most prior work is also on images? 

\begin{comment}
As discussed in their method, when FedAVG is applied the biases in the model will shift, causing images to hit multiple bins. Due to similarities in \name's inserted module, the same situation will occur when FedAVG is used resulting in a few images being duplicated during reconstruction. However, in addition to the bin shifting, the convolutional parameters will also shift. This will not affect any kernels outside of the identity mapping set, as the gradient will remain zero. However, the parameters of the identity mapping set will have a small change, resulting in subtle differences in the reconstruction. 
\end{comment}
For this work, we only look at FedSGD, though our work is applicable to FedAVG with some additional modifications, whose discussion we defer to the Appendix (Section~\ref{sec:fedavg}). The extension from FedSGD to FedAvg follows the pattern of the prior work, Robbing the Fed. 

\subsection{Attack architecture}
We insert an attack module at the start of a model that consists of a convolutional layer followed by two FC layers. This module is shown in Figure~\ref{fig:identity_maps}. Compared to Robbing the Fed~\cite{fowl2022robbing}, we only add an additional convolutional layer, a minimal change. 
We leak images using the  gradients of the weight parameters connecting the output of the convolutional layer to the first FC layer. The dimension of the output of the convolutional layer depends on the image size and number of kernels. For a $32\times32$ image and $100$ kernels, the output would have dimension $32\times32\times100$.

The size of the first FC layer depends on the client batch size. Generally, we have $4$ units in the layer for each image in the batch size. With a batch size of $64$, this would be $256$ units. Every unit in the first FC layer is connected to the second FC layer. This second FC layer is the input to the rest of the model architecture and has the same dimensions as the input image to our first convolutional layer. As an inserted module, it is important that the input and output dimensions are the same, as the rest of the model will expect the same input dimension.

\subsection{Convolutional parameters}
Previous works~\cite{fowl2022robbing, boenisch2021curious} have discussed the idea of leaking with convolutional layers followed by FC layers. The standard way to achieve this would be the use of convolutional kernels to push the image forward, such that, FC layers further in the model are used to leak inputs. We will further explore the use of convolutional parameters.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.9\columnwidth]{Plots-Images/conv-identity.drawio.png}
%\includegraphics[width=1.0\columnwidth]{Plots-Images/identity-mapping-set.drawio.png}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:conv-sets} An identity mapping set for a 3-channel input. The first three kernels ($3\times3\times3$ cubes) of the convolutional layer each push a different input channel forward. All parameters are zero except for a single element, which is one and that element is in a different slice for each kernel. This 2D slice is shown in the figure, and the locations of the slice for each kernel helps push different input channels forward.}
\vspace*{-5mm}
\end{figure}

For a 3-channel input image (RGB), pushing the image forward can be done with three kernels in a convolutional layer. 
If we have $3\times3$ kernels, the dimension of each kernel would be $3\times3\times3$ with the final dimension corresponding to the input channels. 
% These kernels will be all zero other than a single one in the center corresponding to the different image channels. 
% SB (10/11/22): Not clear "corresponding to the different image channels". 
% JZ (10/11/22): Is input channels more clear? What I want to say is that if we use 3x3 kernels, the final dimension will depend on how many channels the input has (e.g. 3 for RGB)
For any given client, we need three kernels. Within the kernels, there is only one key channel, which will have a 1 in the center and all other elements as zeroes. Each of the three kernels will have this in a different channel. When the convolutional kernels are applied to the RGB input image, it will simply be pushed through. 
This setup is shown in Figure~\ref{fig:conv-sets}. We will call these three convolutional kernels an \textit{identity mapping set}. This operation only requires the use of three convolutional kernels, a small number in the context of typical models. The other kernels are not needed to propagate any information, so the output should be set as zero. There are multiple ways to set the parameters such that the output of the kernels are zero. This will be discussed further in Section~\ref{sec:method-mask}, but generally a large enough negative bias or all zero weights for a kernel will cause the output to be zero.

However, this simple approach severely under-utilizes the number of convolutional kernels. Consider a situation where we have 256 convolutional kernels. To push one 3-channel image forward, we only use three kernels. As a result, the other 253 kernels are set so that they do not contribute. With this in mind, we propose the use of multiple, separate identity mapping sets. Each set requires three kernels, and following this, we can have $\lfloor 256/3 \rfloor = 85$ separate identity mapping sets, each corresponding to a different set of three convolutional kernels. Each of these separate identity mapping sets is then used as a different model and is sent to different clients.

\subsection{De-aggregated weight gradient}
% SB (10/11/22): Chopped
\begin{comment}
In order to understand what this accomplishes, we first look at the resulting gradient for a single client after training. For this client, we give it the first identity mapping set, where the first three convolutional kernels push the image forward and the rest output zero. When a training image goes through the network and the gradient is computed, for any activated neuron in the FC layer following the convolutional layer, the weight gradient will only be non-zero for the weights corresponding to the first three channels of the convolutional layer output. Since the output of the convolutional layer is zero for all other output channels, the weight gradients connecting to these channels would be zero.

If we consider other clients who have identity mapping sets from other kernels, when their weight gradients are computed, the only non-zero gradients will be the ones corresponding to the channels of their set. In other words, 
\end{comment}
By sending the carefully crafted separate convolutional kernels to each client, the weight gradient for each set of identity mapping sets is non-zero only for a different set of convolutional output channels. Therefore, when updates are aggregated together, the weight gradients remain separate. Looking at the reconstruction phase, if inputs from different clients activate the same bin, 
% SB (10/11/22): "same bin" means "same bin within their respective models"
% JZ (10/11/22): Yes.  In the context here, we can also mean the aggregate gradient, since the weights will still remain separate there.
the computed weight gradient of that bin will not be shared between clients. Instead, the only inputs that can share the same weight gradients of a bin would be images within that single client's batch. As a result, the size of the FC layer only needs to scale based on client batch size, not due to the number of clients in aggregation. 

When reconstructing images, this allows us to work with sets of weight gradients, each corresponding to different identity mapping sets (and client model). Instead of Equation~\ref{eq:2}, we would then reconstruct images as:

\begin{equation}\label{eq:3}
    % x_{k} = (\nabla_{{W^i}_k} L - \nabla_{{W^{i+1}}_k} L) \oslash (\frac{\delta L}{\delta {B^i}_k} - \frac{\delta L}{\delta {B^{i+1}}_k})
    x^i_{k} = (\frac{\delta L}{{\delta W^i}_k} - \frac{\delta L}{{\delta W^{i+1}_k}}) / (\frac{\delta L}{{\delta B^i}_k} - \frac{\delta L}{{\delta B^{i+1}}_k})
\end{equation}

\noindent
where $k$ indicates the client and the corresponding weight and bias gradients respective to their identity mapping set. $x^i_k$ is the input from client $k$ that activates bin $i$. Figure~\ref{fig:identity_maps} shows the process of using separate identity mapping sets to split apart the aggregate weight gradient. This allows the attacker to leak images from each client separately after aggregation.

This decoupled weight gradient partially solves the scaling problem of reconstructing images as the number of clients participating in aggregation increases. The number of clients we can attack in a single round is upper bounded by the number of convolutional kernels and the number of identity mapping sets we can create. By sending zero-output convolutional kernels to non-target clients, we ensure that these are not attacked. %However, these additional clients can also be attacked in a different training iterations.

\subsection{Decoupling bias gradient}
Equation~\ref{eq:3} brings up another problem for reconstruction, as the bias gradient is also needed for the computation. While the weight gradients are separated through the identity mapping sets, the bias gradients are not. 
% \atul{showing this in the attack design figure will be helpful} 
% When an image from a client activates a neuron, it contributes to the bias gradient. 
% The bias gradient will not be decoupled and the individual client values of $\frac{\delta L}{\delta {B^i}_k}$ are not obtainable after aggregation. 
% SB (10/11/22): I do not understand the previous two sentences, why there is coupling of bias gradients. 
% SB (10/11/22): Reworded
Secure aggregation aggregates the bias gradients of each neuron (of the first FC layer). Thus the neuron $i$'s bias values from the FCs of all the clients will be aggregated. Consider that an image $j$ activates neuron $i$ in client $k$ and another image $j'$ activates the neuron $i$ in client $k'$ (note that the neurons are physically separate before aggregation as they are in the local FCs of clients $k$ and $k'$). After secure aggregation, bias gradients of neuron $i$ from clients $k$ and $k'$ are coupled and therefore the server is not able to use Equation~\ref{eq:3} to decouple images $j$ and $j'$. This is the crux of the bias coupling problem. 
It turns out that the task of decoupling the bias gradients is more challenging than it first appears.

Let us consider an intuitive way of solving the problem, by creating a predictable output to the neuron, which would lead to a predictable bias gradient. However, this would also need to be done without knowledge of the input (which obviously is unknown to the server). One way of doing this could be having an additional identity mapping set in the convolutional layer, and a standard value of one for the bias of the neurons in the FC layer. With negative weights connecting one set and positive for the other, after adding the bias, the result would always be a value of one, achieving our goal of a predictable output and bias gradient. However, this simple approach shows us the real problem. A predictable output and bias gradient will fundamentally break the binning approach of~\cite{fowl2022robbing} or any other linear layer leakage method. Every image from a client batch would then activate every neuron, and no reconstruction would be possible. Instead of looking to decouple the bias gradient to solve this problem, we first ask a more basic question: what is the purpose of the bias gradient?

Observing Equation~\ref{eq:3}, we note that for reconstruction of each neuron we subtract the weight gradients, and the resulting value is divided by the subtraction of bias gradients. Previously we mentioned how each neuron has a single bias. 
% While this caused problems for decoupling the gradient, the same property can instead actually be used to help here. The bias gradient for each neuron is a scalar value. 
After subtracting bias gradients, the resulting value remains a scalar. The purpose of the bias gradient, then, is \emph{to scale the value produced by subtracting the weight gradients such that it becomes identical to the input}. 
% SB (10/11/22): How can weight gradient become the input?
% JZ (10/11/22): Mainly, these input images are hidden in weight gradient. The subtraction operation reveals them, and division scales it properly
Therefore, as long as we know what the weight gradient needs to be scaled to, we will not need to know the exact bias gradient. Rather, knowledge of the input range is all that is required to reconstruct the input. For image datasets such as MNIST, CIFAR-10, or Imagenet, the training data will be between 0 and 1. 
% SB (10/11/22): Not clear "data can be between 0 and 1". 
% JZ (10/11/22): While the data can be 0 to 255, prior to training, it should be between 0 and 1. If there is additional normalization then this might be different.
Using this, the images can be reconstructed by scaling the gradient such that the maximum value is 1, without requiring any knowledge of the bias gradient.

\begin{equation}\label{eq:4}
    x^i_{k} = \frac{abs({\frac{\delta L}{\delta W^i}_k} - \frac{\delta L}{\delta W^{i+1}}_k)}{max(abs({\frac{\delta L}{\delta W^i}_k} - \frac{\delta L}{\delta W^{i+1}}_k))}
\end{equation}

\noindent
Here, the numerator is the absolute value of the subtracted weight gradient for client $k$ between neuron $i$ and $i+1$, while the denominator is the maximum of that value across the set of three slices in the identity mapping set corresponding to client $k$. 
% SB (10/11/22): Verify
% JZ (10/11/22): This is correct
% This division sets the new maximum value to be one.
The technical question now is much simpler --- estimating the maximum value of the denominator. The input range can be estimated by the server, or may be known through other public sources, such as through the standardized normalization prior to training. We see empirically that inaccuracies in this estimation do not hurt the reconstruction performance much. We do not use the exact bias gradient in any of our experiments and yet we see that in Table~\ref{tab:recon-metrics} that our image metrics are comparable to that of Robbing the Fed that uses the exact bias gradient for recovery. 
% By scaling the absolute value of the weight gradient, we can directly recover the input image to the layer. If the correct range of the image is used for scaling, the reconstructed image will be exact. This process essentially works as an "estimated bias gradient." This input range should also be known by the server. A standard input normalization for client data will typically be used prior to training, and the knowledge can be used to scale the weight gradient. 
Thus, our key result is that in linear layer leakage methods, the bias gradient is \textit{not} required for reconstruction of data.

% % figure* indicates a full page image, cropped to single column otherwise.
% \begin{figure}
% \begin{center}
% \includegraphics[width=1.0\columnwidth]{Plots-Images/diagram_1.png}
% \end{center}
% \vspace*{-5mm}
% \caption{\label{fig:identity_maps} Weight gradient de-aggregation. Different color values in the convolutional kernels signify identity mapping sets corresponding to different client models. In the aggregated gradient, the weight gradients are separated between models but the bias gradient is not.}
% \vspace*{-3mm}
% \end{figure}

% figure* indicates a full page image, cropped to single column otherwise.

\subsection{Bias estimation error}

% SB (10/11/22): Shortened from the material below. 
While data normalization prior to training is usually standard across clients, the assumption that the maximum value will be the same for all inputs will not always hold true. Taking image datasets as an example, the default CIFAR dataset has many images with a maximum pixel intensity value of 1, but there are also quite a few images with a maximum value below this. Our reconstruction by assuming the maximum value of 1 will result in the recovered image appearing brighter, although still easily identifiable to the eye. Figure~\ref{fig:brightness-shift} shows a reconstructed image with a shifted brightness. The maximum pixel intensity of the ground truth was 0.7804 but was scaled to be 1.0 during reconstruction. Importantly, the error in reconstruction is contained to just that image and does not affect the reconstruction of successive images in that batch. This is a desirable side effect of not using an actual bias gradient. 

\begin{figure}[!t]
\vspace*{2mm}
     \centering
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=0.8\columnwidth]{Plots-Images/brightness-shift-crop-copy.png}
         \caption{Ground truth}
         \label{fig:grount-truth-shift}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=0.8\columnwidth]{Plots-Images/brightness-shift-crop-copy2.png}
         \caption{Reconstruction}
         \label{fig:reconstruction-shift}
     \end{subfigure}
\vspace*{-2mm}
\caption{\label{fig:brightness-shift} Image reconstructed by scaling the absolute value weight gradient to have a maximum value of 1. Ground truth image (a) has a maximum intensity of 0.7804, resulting in a brightness shift in the reconstructed image (b).}
\vspace*{-5mm}
\end{figure}

\begin{comment}
While data normalization prior to training is usually standard across clients, the assumption that the exact maximum or minimum value will be the same for all inputs will not always hold true. Taking image datasets as an example, the default CIFAR dataset has many images with a maximum pixel intensity value of 1, but there are also quite a few images with a maximum value below this. Without further normalization, the ranges of the images' will vary, something the server cannot control for clients. During reconstruction, the range of the images can only be scaled under an assumption that the max value of each image will be 1. The resulting reconstruction for images with a max below 1 then have a shifted range of values compared to the ground truth. Specifically, the recovered image would appear brighter, although still easily identifiable to the eye. Figure~\ref{fig:brightness-shift} shows a reconstructed image with a shifted brightness. The maximum pixel intensity of the ground truth was 0.7804 but was scaled to be 1.0 during reconstruction.

Because each gradient is scaled separately, only the reconstructed images who have incorrectly predicted ranges would be affected. Any improperly scaled image would not create problems affecting reconstruction of any other images between clients or within their own batch. This is also a result of not using an actual bias gradient. Subtraction of the weight gradients of subsequent bins results in the correct weight gradient needed to reconstruct the input. Since we do not use a subtraction of bias gradient estimates during reconstruction, like prior methods, we avoid the problem of incorrectly estimated values influencing other inputs. Instead, the process is done completely independently for each input.
\end{comment}



