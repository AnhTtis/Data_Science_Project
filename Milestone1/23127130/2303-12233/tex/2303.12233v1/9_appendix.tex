\noindent
 \section{Secure aggregation in FL} \label{appendix-SA}
 \label{sec:appendix}
%% 
 Secure Aggregation \cite{bonawitz2017practical} is one of the core privacy-preserving techniques in FL, that 
 enables the server to aggregate local model updates from a number of clients, without observing any of their individual model updates in the clear. At their core, state-of-the-art  secure aggregation protocols \cite{bonawitz2017practical,secagg_bell2020secure,secagg_so2021securing,secagg_kadhe2020fastsecagg,zhao2021information,so2021lightsecagg,so2021turbo,9712310} in FL rely on using  additive masking to protect the privacy of individual models. In particular, in a secure aggregation protocol, each user $i \in [N]$ encrypts  its own  model update   $\mathbf{y}^{(t)}_i = \text{Enc}(\mathbf{g}^{(t)}_i)$ before sending it to the server in  the $t$-th communciation round.  This encryption is done such that secure aggregation guarantees the following
 
 \noindent \textbf{Correct decoding.} The encryption  guarantees correct decoding  for the aggregated model such that the server should be able to decode 
\begin{equation}
    \text{Dec} \left(\sum_{i \in [N]} \mathbf{y}^{(t)}_i \right)= \sum_{i \in [N]} \mathbf{g}^{(t)}_i, 
\end{equation}
\noindent \textbf{Privacy guarantee.} The  encrypted model updates $\{\mathbf{y}^{(t)}_i\}
   _{i \in[N]}$ leak no information  about the model updates $\{\mathbf{g}^{(t)}_i\}
   _{i \in[N]}$ beyond the aggregated model $\sum_{i=1}^{N} \mathbf{g}^{(t)}_i$. This is formally given as the following 
   \begin{equation}\label{eq-SA_guarantee}
   I\left({\{\mathbf{y}}^{(t)}_i\}
   _{i \in[N]}; \{\mathbf{g}^{(t)}_i\}
   _{i \in[N]} \middle  |   \sum_{i=1}^{N} \mathbf{g}^{(t)}_i \right) = 0,
\end{equation}
where $I(.)$ represents the mutual information metric. 

\subsection{Applicability to FedAvg}
\label{sec:fedavg}
Our work is applicable to FedAvg, though in the main paper we discuss the FedSGD design in detail. The extension to FedAvg follows in similar lines to how Robbing the Fed extends to FedAvg from their main demonstration of FedSGD. Our customized convolutional kernel has 0 in many elements and 1 in some key elements. As multiple iterations of FedAvg execute locally on the clients, then these elements change little thus allowing for the separation of the different clients and the different images in the batch as in the original design. Thus, the output communicated to the server at the end of one epoch of FedAvg can still be disaggregated by the malicious server to reconstruct individual images. However, due to small shifts in the values of the convolutional kernel elements, the reconstruction will be degraded compared to FedSGD.

\subsection{Sample leaked images for other datasets}
We show the reconstructions of a randomly sampled client with a batch size of 64 on various datasets. Figure~\ref{fig:single_client_mnist} shows the MNIST dataset, Figure~\ref{fig:single_client_organamnist} shows the OrganAMNIST dataset, Figure~\ref{fig:single_client_tinyimagenet} shows Tiny Imagenet, and Figure~\ref{fig:single_client_imagenet} shows Imagenet.


\begin{figure}
\begin{center}
\includegraphics[width=1.0\columnwidth]{Plots-Images/Leaked-images-mnist.png}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:single_client_mnist} Leaked images for a randomly chosen client training on the MNIST dataset. Out of the 64 batch images, 47 images are leaked.}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=1.0\columnwidth]{Plots-Images/Leaked-images-organmnist.png}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:single_client_organamnist} Leaked images for a randomly chosen client training on the OrganAMNIST dataset. Out of the 64 batch images, 40 images are leaked.}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=1.0\columnwidth]{Plots-Images/Leaked-images-tinyimagenet.png}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:single_client_tinyimagenet} Leaked images for a randomly chosen client training on the Tiny ImageNet dataset. Out of the 64 batch images, 49 images are leaked.}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=1.0\columnwidth]{Plots-Images/Leaked-images-imagenet.png}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:single_client_imagenet} Leaked images for a single client training on the ImageNet~\cite{russakovsky2015imagenet} dataset. Out of the 64 batch images, 51 images are leaked}
\end{figure}


\begin{figure}[t!]
\begin{center}
\includegraphics[width=1.0\columnwidth]{Plots-Images/stride-maxpool.drawio.png}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:stride} Using four $2\times2$ kernels with stride $2$ achieves the same downsampling as $2\times2$ max-pooling. All of the input information can still be retained at the cost of using more kernels.}
\end{figure}

\begin{figure}[t!]
     \centering
     \begin{subfigure}[b]{0.32\columnwidth}
         \centering
         \includegraphics[width=\columnwidth]{Plots-Images/Imagenet-downsampling-0.png}
         \caption{None}
         \label{fig:down-0}
     \end{subfigure}
     \hfill
      \begin{subfigure}[b]{0.32\columnwidth}
         \centering
         \includegraphics[width=\columnwidth]{Plots-Images/Imagenet-downsampling-1.png}
         \caption{1x}
         \label{fig:down-1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\columnwidth}
         \centering
         \includegraphics[width=\columnwidth]{Plots-Images/Imagenet-downsampling-2.png}
         \caption{2x}
         \label{fig:down-2}
     \end{subfigure}
\vspace*{-1mm}
\caption{\label{fig:imagenet-down} Sample from the ImageNet dataset (a) prior to downsampling and after being downsampled (b) once and (c) twice. The number of max-pooling layers prior to reconstruction with an FC layer determines downsampling level.}
\vspace*{-3mm}
\end{figure}

\subsection{Avoiding clients selectively}
\label{sec:method-mask}
Non-attacked clients must be sent a model where the weight gradients of the FC layer do not overlap with that of the attacked clients, since their update is still used in the aggregation. The existence of the weight gradient is dependent on the activation of the neurons. In turn, activation can be prevented in two steps: the input to the layer can be zero, or the input is non-zero but the weight and bias of the FC layer prevent neuron activation. The server can achieve this a few ways using parameter manipulation. 

In the case of limiting the output of the convolutional layers to be zero, one simple method would be to set all convolutional kernels as zero for the non-attacked clients. Another method could be to have non-zero kernels, but set the bias of the kernels to be large enough of a negative value such that, regardless of kernel input, the output of the convolutional layer will be zero. These methods can be used for clients in order to mask off the desired weight gradients. The server only needs to keep in mind which weight gradients need to be masked off and which must remain in order to reconstruct data for the attacked clients. However, this method is also easy for any client that isn't attacked to detect, as the output of the model will always be zero regardless of the input. One alternative can then be to use a single additional convolutional kernel for these models. This kernel can then have a non-zero output. Since this kernel isn't used for the attack, it will not interfere with the reconstruction.

For non-attacked clients, there is also the alternative of manipulating the FC layer weights and biases to prevent neuron activation regardless of the inputs. Since models are sent independently to clients, the weights and biases of the FC layers in the model can differ. In this case, the biases are set large enough so that no inputs can activate any neurons.
%With the convolutional kernel and FC layer weights, a maximum possible value for an input sample can be computed for the neuron using the knowledge of the possible range of the input. The bias value should then be set negative enough such that the value will always result in no activation. However, this method will not work as a mask for the attacked clients because the biases of the FC layer need to be used for binning. Instead, the same goal is achieved by setting the weight parameters as zero for any channels not corresponding to the identity mapping set.

\subsection{Max pooling downsampling}
Convolution layers can be followed by max pooling. With 2x2 pooling, an image would be downsampled by a factor of two, resulting in an image half the width and height of the original. With larger images, such as those in Imagenet, this is a smaller issue if leakage only occurs after a few max pooling operations. When there are only a few convolutional layers or the FC layer is placed early in the network, recovering images with a decreased resolution can be acceptable. 

However, particularly with lower resolution images in datsets such as CIFAR, downsampling the resolution would result in unidentifiable images. With even two max pooling operations, the reconstructed resolution would go from $32\times32\times3$ to $8\times8\times3$. The data recovered is still correct, but the resolution would be very low. Instead of using max pooling for downsampling, we can alternatively use stride. For $2\times2$ max pooling, the output resolution is decreased by a factor of 2 for both the height and width. Similarly, by using a stride of 2 for both dimensions, we would also have an output with the height and width cut in half.

While stride allows the model to have the same output dimension compared to the max pooling operation, it also allows the attacker to leak the full image information. If an attacker uses $2\times2$ kernels with stride $2$, if four kernels are modified such that each kernel has all zeroes other than a single one in a different corner, the convolutional layer would still propagate all of the input information. Figure~\ref{fig:stride} shows an example for a 1-channel image. The attacker can reconstruct the full input using the knowledge of the kernel parameters.

For an attacker, the downside of using stride for max resolution is that the number of convolutional kernels required to perform the attack is larger. With a three-channel image and a convolutional layer utilizing stride, keeping full image information requires $3 \cdot 4 = 12$ kernels. As the number of convolutional layers with stride increases, the number of kernels required also increases exponentially, requiring 4 times the number of kernels for each layer.

\begin{equation}\label{eq:5}
    \text{kernels} = [\text{input channels}] \cdot 4^{[\text{conv layers}]}
\end{equation}

\noindent
Since this limits the number of clients an attacker can reach, the attacker can choose to allow a subset of layers to lose input information for an increase in the number of attacked clients. Figure~\ref{fig:imagenet-down} shows the reconstruction results of an ImageNet~\cite{russakovsky2015imagenet} image under several levels of downsampling resulting from max-pooling. While downsampling decreases reconstruction quality, under the threat model where the server is able to modify the model, this can be avoided entirely by placing a convolutional layer without max-pooling followed by FC layers used for leaking at the very start of the model.

\begin{figure}[!t]
     \centering
     \begin{subfigure}[b]{0.49\columnwidth}
         \centering
         \includegraphics[width=\columnwidth]{Plots-Images/psnr-ssim-imgs2-copy-2.png}
         \caption{Incorrect reconstruction}
         \label{fig:psnr-ssim-1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\columnwidth}
         \centering
         \includegraphics[width=\columnwidth]{Plots-Images/psnr-ssim-imgs2-copy.png}
         \caption{15.49 PSNR score}
         \label{fig:psnr-ssim-2}
     \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.49\columnwidth}
         \centering
         \includegraphics[width=\columnwidth]{Plots-Images/psnr-ssim-imgs2-copy-3.png}
         \caption{Correct reconstruction}
         \label{fig:psnr-ssim-3}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\columnwidth}
         \centering
         \includegraphics[width=\columnwidth]{Plots-Images/psnr-ssim-imgs2-copy-4.png}
         \caption{13.56 PSNR score}
         \label{fig:psnr-ssim-4}
     \end{subfigure}
\caption{\label{fig:psnr-1} Reconstructed images from CIFAR-100 compared to the highest PSNR ground truth image. The incorrect reconstruction (a) has an overlap in the same bin resulting in a failed reconstruction while the correct reconstruction (c) has no bin overlap. However, the incorrectly reconstructed pair (a, b) has a higher PSNR score than the correct pair (c, d).}
\vspace*{1mm}
\end{figure}

\begin{figure}[!t]
     \centering
     \begin{subfigure}[b]{0.32\columnwidth}
         \centering
         \includegraphics[width=\columnwidth]{Plots-Images/psnr-ssim-wrong-match5-copy.png}
         \caption{Reconstruction}
         \label{fig:match-gt}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\columnwidth}
         \centering
         \includegraphics[width=\columnwidth]{Plots-Images/psnr-ssim-wrong-match5-copy-3.png}
         \caption{Max SSIM GT}
         \label{fig:match-ssim}
     \end{subfigure}
      \begin{subfigure}[b]{0.32\columnwidth}
         \centering
         \includegraphics[width=\columnwidth]{Plots-Images/psnr-ssim-wrong-match5-copy-2.png}
         \caption{Max PSNR GT}
         \label{fig:match-psnr}
     \end{subfigure}
      \hfill
\vspace*{-1mm}
\caption{\label{fig:psnr-2} Matching reconstructed images from CIFAR-100 to the ground truth image in the batch using the highest (b) SSIM  and (c) PSNR scores. SSIM choose the correct image while PSNR chooses an incorrect image.}
\vspace*{-3mm}
\end{figure}

\subsection{Image metrics}
\label{sec:image-metrics}
For image reconstruction, the ability to identify an image and quantify the reconstruction quality are important for a metric to capture. We show from our attack results that the PSNR score achieves poor results in both categories. Previous works have also discussed that the use of mean squared error is a poor reflection of image similarity. We use the SSIM~\cite{wang2004image} metric as a baseline for comparison and and show that it serves as a better metric for perceptual similarity. Our empirical results on the reconstructed images also support the idea that PSNR, which is based on MSE, is not a good choice for a reconstruction quality metric. Particularly, when reconstructing an image involves a large shift in the pixel value range, PSNR functions very poorly. This section shows several cases to demonstrate this.

When observing the PSNR score of reconstructions, even if an image is reconstructed incorrectly, the PSNR score can be higher than the score for a correct image. Figure~\ref{fig:psnr-1} shows one such example, with two reconstruction results: the first being a failed reconstruction due to image overlap and the second being correct. Here the incorrectly reconstructed image has a higher PSNR score of 15.49 compared to the correct image which has a score of 13.56. SSIM metric has the desired result, with a score of 0.91 for the correct image and 0.67 for the incorrect one.

A more extreme case occurs if we use the highest PSNR score to match reconstructed images to their ground truth counterparts. Figure~\ref{fig:psnr-2} shows the result of choosing the corresponding ground truth image from the batch using the highest PSNR and SSIM scores. Comparing to the top ground truth image, the reconstruction has a PSNR score of 16.45 and an SSIM score of 0.17. The bottom ground truth image has a PSNR score of 12.04 and a SSIM score of 0.88. As shown, using the maximum PSNR for image matching will often result in mismatches. In a batch of 100 images, there will usually be between 2-5 samples with an incorrect match. Comparatively, SSIM has no problems with matching reconstructions with the ground truth.

 \subsection{Estimating information leakage due to \name} \label{appendix-MI-leakage}
Throughout the paper, we compare the amount of leaked information through \name in terms of the number of reconstructed (leaked) images at the malicious server. However, the design of the attack module in \name can leak to the server some information about the images that were not successfully reconstructed. For example, the reconstruction might create an overlap of multiple images, but this overlap can exclude some image types (for example an observer of a reconstruction that is an overlap of digits 2, 3 and 8 might be able to exclude that the non-recovered images contain an image of digit 1). Since, the success of the reconstruction through our \name attack is affected by the size of the added fully connected layer, we seek to understand how the information through the reconstruction at the sever changes with respect to the leaked information in the gradients for different fully-connected layer sizes (imprint sizes).

In order to demonstrate this, we conducted experiments with \name on the MNIST dataset. In order to measure the information leaked into the gradient and the reconstructed information, we compare the mutual information between: (1) the data batch $x^{input}_k$ at user $i$ and the aggregate gradient $g$ for the added \name module at the server; (2) the data batch $x^{input}_k$ and the reconstructions $x_k$ at the server for user $k$ created using Equation~\ref{eq:5}. Note that by the data processing inequality, we have that:
\begin{equation}\label{eq:ratio}
    \frac{I(x^{input}_k;x_k)}{I(x^{input}_k;g)} \leq 1.    
\end{equation}
Thus, our aim is to determine how close is this fraction to 1\footnote{A ratio of one, implies a perfect reconstruction of all available information in the gradient at the server.}. In order to estimate each of the mutual information terms, we use Mutual Information Neural Estimator (MINE) which is the state-of-the-art method~\cite{belghazi2018mine} to estimate the mutual information between two random vectors. For each imprint size, we sample 20,000 random batches for the users' datasets and then compute the aggregate gradient $g$ and the reconstructed images for user $i$ for each of these 20,000 batch samples. These represent the samples used to estimate mutual information using MINE.

We repeat this procedure multiple times in order to get multiple mutual information estimates and then use these estimates to report the average ratio in Equation~\ref{eq:ratio}.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=1.0\columnwidth]{Plots-Images/MI_leakage_vs_imprint.png}
\end{center}
% \vspace*{-5mm}
\caption{\label{fig:MI_leakage_ratio} Percentage of the information leaked into the gradient $I(x^{input}_k;g)$ that is recovered through reconstruction $I(x^{input}_k;x_k)$ based on the imprint size in \name. For this experiment, we use MNIST dataset with a batch size = 10.}
\end{figure}

Figure~\ref{fig:MI_leakage_ratio} shows that as the imprint size in \name increases, the power of the image reconstruction using Equation~\ref{eq:5} increases, thus more of the leaked information about the batch images is recovered. This can be due to the fact that for smaller imprint sizes, the reconstruction results in a non-invertible mapping from the aggregate gradient $g$ and the set of reconstructed images in $x_k$. As the size increases, the attack is able to recover more images successfully, which makes the mapping invertible --- if we recover all images, then we have successfully recovered the input and at this point $g$ becomes a deterministic map from the reconstructed set. Note that as seen in the figure, the gradients can potentially retain up to 40\% more information when the imprint size is 60 (for a client batch size of 10). 