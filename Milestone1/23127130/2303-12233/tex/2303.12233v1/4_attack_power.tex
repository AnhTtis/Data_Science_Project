% \noindent
% Consider an example where the gradients of $100$ clients with a batch size of $64$ are aggregated together. With a FC layer of $256$ neurons used to leak the data,~\cite{fowl2022robbing} would have on average $(100*32)/128=25$ images per bin. Our attack, however, only needs the FC layer to scale in relation to the batch size and not the number of clients. Instead of $25$, we would instead only have an average of $32/128=0.25$ images per bin. Since reconstruction success occurs when a bin is only activated by a single image, we enable attacks to function regardless of the number of clients, even working effectively in cross-device FL with hundreds of separate gradients aggregated together.

% This section will discuss the other aspects of the attack not covered in section~\ref{sec:methodology}. In particular, we will look at the how the attack can be modified for different settings and also discuss some other aspects of the attack.

% SB (10/11/22): This material is a repeat from the beginning of Sec 3 and so removed. 
\begin{comment}
Our entire inserted module consists of a single convolutional layer followed by two FC layers. Figure~\ref{fig:identity_maps} shows the general attack framework, using the convolutional layer to separate the weight gradients and using the FC layer to leak with the binning approach from~\cite{fowl2022robbing}. With this setup of adding these layers to the front of a model, we can leak from any architecture. Adding the module further in the architecture is also possible, but when placed after max-pooling operations have been applied, the recovered images will have a decreased resolution.
\end{comment}

\subsection{Identifying client data}
\noindent
Even if the aggregated gradient is able to leak training data, the inability of an attacker to identify which data belongs to each client still gives some level of comfort. Rather, the reconstructions just go into a single, giant pool of data that could have come from any client. \name also breaks this limitation. In effect, the ability to identify the owner of the reconstructed data even through aggregation is a strong aspect of our attack.

Through separate identity mapping sets, the weight gradients for each client are separated after aggregation. As a result, when reconstructing inputs, based on which set of weight gradients is used, the server can identify which model it originates from and hence, which client it originates from. Not only is the data leaked, but the client who owns the data is also identified in the process. With this information, the server can subsequently focus on specific high-value clients.

\subsection{Reaching more clients}
\label{sec:exper-more-clients}
One limitation of the attack is that the number of separate models we can create is dependent on the number of kernels within the convolutional layer. If the server can arbitrarily increase the number of kernels, scaling to larger numbers of clients would be feasible. However, functionally this would not be the case as the size of the model would also be increasing with the number of convolutional kernels. In the prior example where we have 256 kernels, we can create 85 separate models. If the server cannot increase the number of kernels, a vanilla attack where each client gets a separate model will only be able to reach a maximum of 85 clients, and other clients in the training round would not be attacked. 
% Instead, they would receive a model with a masked weight gradient as discussed in~\ref{sec:method-mask}.

If the attacker only cares about the total number of leaked images, there is an option to send the same model to multiple clients. As a result, these clients then share the same set of weight gradients used for reconstruction. If images across clients that share a model then activate the same bin, reconstruction fails. Therefore, the number of clients that can share the same model is limited by the batch size and FC layer size. Functionally, combining multiple clients will effectively create a multiplicative larger batch size. As the number of clients sharing increases, there is initially a diminishing return. If the number of bins cannot handle the total number of inputs, the number of reconstructions will begin to decrease. Increasing the FC layer size will increase the maximum number of clients per model, while an increasing client batch size will decrease the maximum number. Ultimately, the FC layer size limits the product of (number of clients)$\cdot$(batch size). This is shown empirically in Section~\ref{sec:shared-models}. In general, increasing the number of clients will increase the absolute number of recovered images. However, once the number of clients and batch size grow large enough, the number of recovered images begins to decrease. Experimentally, we find that for a wide range of scenarios once the total number of images is between $0\%-25\%$ more than the number of bins, the number of recovered images will reach a peak. Adding more clients or images will then decrease the number of recovered images. 

Conversely, if the server wants to leave some clients {\em out} of the attack, that is possible with \name. Non-attacked clients must be sent a model where the weight gradients of the FC layer do not overlap with that of the attacked clients, since their updates are still used in the aggregation. The existence of the weight gradient is dependent on the activation of the neurons. In turn, activation can be prevented in two steps: the input to the layer can be zero, or the input is non-zero but the weight and bias of the FC layer prevent neuron activation. The server can achieve this using parameter manipulation as we detail in the Appendix (Section~\ref{sec:method-mask}). 

% \subsection{Single-channel images}

\subsection{Setting bin biases}
While leaking images from a linear layer is superior in reconstruction speed and quality compared to optimization, the initialization of bin biases can pose a challenging problem in practice. The weights of the FC layer will measure some aspect of an image, in our case we use the average pixel intensity. However, the bin biases must be used as cut-offs for image activations. If not set properly, the number of recovered inputs can be much lower. If we initialize completely randomly on CIFAR-100, the leakage rate can even drop below 10$\%$.
% SB (10/11/22): Can you give a quantitative evidence here, e.g., if we randomly initialize, then the leakage rate is only XXX\%. 
% JZ (10/11/22): Lower than 10% is when I used torch.rand for initialization.
In our case, if the server knows the dataset distribution for average pixel intensity, as assumed in~\cite{fowl2022robbing}, setting the biases of the FC layer is simple. However, in practice client datasets are private, so the server is unlikely to have such knowledge.

Without extra knowledge, the server could initialize the bins by making a few observations. If the FC layer weights were used to measure the average pixel intensity summed across image channels, with a three-channel image, this value must be between $0$ and $3$. An initialization for the bins following this could be so that the biases have a mean of $-1.5$ and standard deviation of $0.5$. The goal of this initialization is to find an initialization closer to the dataset distribution compared to random selection. As long as this is the case, the number of leaked images will be higher than the random bins.

However even the previous process of initialization is unnecessary after a few training iterations through observation of bin activations. After receiving the aggregated gradient, the server observes which bin activations result in a reconstruction, a process simultaneously done while leaking inputs. If the difference between subsequent weight gradients is zero, then no input activated that bin. However, if that difference is non-zero, then one or more inputs activated the bin. If the reconstructed image is just a single input, the bin is counted as being activated once. However, if it is a combination (which can be determined through visual analysis or through calculation of image metrics like PSNR (Table~\ref{tab:recon-metrics})), the activated bin can be counted twice, as the exact number of overlapping inputs may be difficult to determine. For each of these observations, we note the bias of the bin. Using these observed biases, we compute a new mean and variance over them and these values are then used to initialize the biases with a new mean and standard deviation for the next iteration. Over several training rounds, observing bin activations provides the server a close estimate for the true dataset distribution. 

While we use activations to measure the mean and variance of the bin biases, we are also sampling values for dataset images. This determination can also be done by the server separately for each client in case of non-IID data as the clients will then have different distributions for the metric of interest. 
By observing the bias values, the server can also observe the type of distribution (i.e. gaussian, normal, multi-gaussian etc.) and set up bins to fit this. For this work we use bins following a normal distribution. Learning and fitting to more distributions is future work.

% \subsection{Non-IID federated learning}
% If clients have IID data, the individual dataset distributions are similar across the board. Using the same bin biases would then be an optimal choice. However, for non-IID federated learning, these distributions could vary significantly between clients. As a result, a single choice of bin initialization would not be a good fit, rather, having individually customized bins would result in more leakage.

% Since the weight gradients are de-aggregated between clients, activations can be observed individually when reconstructing images. Each client then has a separately observed dataset distribution. Instead of creating bins based on all client data, the server would customize the biases for each model individually. These differences in bias values between models would not affect the reconstruction either, as the role of the bias is only to serve as a cutoff for input activation. For non-IID settings, in additional to convolutional kernel weights being different between clients, the FC layer biases would also need to differ.

\subsection{Parameter comparison}
\label{sec:param-comp}
A key question is how many parameters does an attack add, for our class of attacks where the server modifies the model. A small addition is desirable as we are operating in the cross-device setting where communication and storage are at a premium. The basic intuition behind linear layer leakage is using the gradients of a model update to store the information of the input data. For example, with a $32\times32\times3$ image, the model must have at least $32\cdot32\cdot3=3072$ weight parameters to store this information. To store $1000$ images, we would then need $3072\cdot1000=3,072,000$ parameters. For binning, this would be when the first FC layer has $1000$ units. However, $1000$ units for $1000$ images assumes that every image will activate a separate bin, a case that is extremely optimistic (for the attack). Instead, the number of bins will typically need to be larger than the number of images.

For our comparison, we will use $100$ clients with a batch size of $64$. Each image is given $4$ bins, resulting in $256$ units for each client. For simplicity, we will ignore the number of bias parameters added for this comparison, as the amount is much smaller than the weight parameters (e.g. for \name, $0.005\%$ of the total parameters are biases). For Robbing the Fed~\cite{fowl2022robbing}, this setup would use a total of $100\cdot64\cdot4=25,600$ units in the FC layer. The number of weight parameters added for this layer would then be $(32\cdot32\cdot3)\cdot25,600=78,643,200$. For \name, we would have a fixed FC layer size of $256$ units. However, we would need to use $100\cdot3=300$ convolutional kernels. The output of the convolutional kernels would have a dimension of $32\cdot32\cdot300$. The weight parameters between this output and the first fully connected layer would then be $(32\cdot32\cdot300)\cdot256=78,643,200$, which is the exact same as for Robbing the Fed.

However, when we look at the second FC layer, we see the difference. As injected attacks, the output dimension must be the same as the input for both methods. With a $32\times32\times3$ image, this would need the second FC layer to have size $32\cdot32\cdot3=3072$. Thus, Robbing the Fed would add another $25,600\cdot3072=78,643,200$ parameters to the model. On the other hand, since our first FC layer is only $256$ parameters, we only add $256\cdot3072=786,432$ parameters. We also have some convolutional weights, which adds $300\cdot3\cdot3=2,700$ parameters. However, these parameters are insignificant. For our module, we add roughly half the number of parameters compared to Robbing the Fed.
\atul{We can add a general algebraic expression after the numerical example too - not necessary just a suggestion}

Our added parameters are also extremely sparse. Due to the method of separating the weight gradients between clients, the weight parameters connecting the identity mapping set, which is $3$ out of $300$ convolutional kernel outputs, to the first FC layer are non-zero. The parameters connecting the first FC layer to the second is not sparse, but the number of parameters here is small because we do not scale the FC layer. However, outside of these, any other weight parameters can be zero. When attacking $100$ clients, this means roughy 98\% of our total added parameters are zero. As the number of clients increases, the absolute number of non-zero parameters will remain constant. This allows for additional compression of the model/gradients and the use of more efficient optimization, reducing the computational and communication cost overhead significantly~\cite{duff1989sparse,paszke2019pytorch,abadi2016tensorflow}.

For an attack with this leakage scale, the number of added parameters is not a massive overhead. Using the setting of $100$ clients, we add roughly $13.9\%$ more non-zero parameters for a ResNet-18~\cite{he2016deep} and only $6.6\%$ for a ResNet-50. On the other hand, Robbing the Fed would add $1380\%$ and $658.2\%$ respectively, and the added parameters are not sparse.