\noindent
In the following section, we provide experimental results for \name on aggregated gradients. We assess our baseline attack on the CIFAR-100~\cite{krizhevsky2009learning}, Tiny-Imagenet\cite{le2015tiny}, and MNIST~\cite{lecun1998mnist} datasets, while primarily using one of the datasets for evaluation of each additional experiment. When working with MNIST or OrganAMNIST~\cite{medmnistv2}, since both are one-channel image datasets, the number of clients attacked \chop{increases}is larger as we need fewer convolutional layers to push the image forward. The FC layer weights measure the average pixel intensity summed across the image channels and the bins of the FC layer are set up according to the dataset distribution (known to the server or learned through the first few training iterations). 
%\atul{bit vague here, may need more details} 
% SB (10/11/22): Done
In the vanilla setting, each customized model is sent to only a single client and the clients that cannot be attacked (due to convolutional kernels running out) receive a zero-gradient model. Further experiments show the leakage rates with multiple clients sharing the same model. In the non-IID setting, we evaluate the effects on the OrganAMNIST dataset, which has larger distribution differences when using a class-based non-IID skew compared to CIFAR-100. Some additional experimental results are shown in the Appendix~\ref{sec:appendix}.

Our standard model used for the experiments is a simple architecture of a single convolutional layer followed by two fully connected layers. The convolutional layer has 256 kernels and the first FC layer used for leakage has 256 units. This simplicity of model choice mainly creates a lower computational cost when working with a higher numbers of clients. \name uses the parameters in these layers for the attack, without altering the architecture any further. 
As introduced in Section~\ref{sec:attack-scope}, \name can function as an inserted module at the front of any network. 
% \atul{Shall we make the previous statement bold - seems an important takeaway to me. }
% SB (10/11/22): This is actually a simplification for fast experiments. 
% Our choice of using the module as our architecture functions the exact same.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\columnwidth}
         \centering
         \includegraphics[width=0.8\columnwidth]{Plots-Images/Ground-truth-cifar100.png}
         \caption{Ground truth}
         \label{fig:ground-truth-cifar100}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\columnwidth}
         \centering
         \includegraphics[width=0.8\columnwidth]{Plots-Images/Leaked-images-cifar100.png}
         \caption{Reconstruction}
         \label{fig:reconstruction-cifar100}
     \end{subfigure}


% \includegraphics[width=0.5\columnwidth]{Plots-Images/Leaked-images-cifar100.png}
\vspace*{-1mm}
\caption{\label{fig:single_client_cifar100} 56 leaked images for a randomly chosen client with batch 64 training on the CIFAR-100 dataset. Empty boxes in the reconstructed batch indicate reconstruction failure from overlap of images in a bin.}
\vspace*{-5mm}
\end{figure}

%that the image from the batch could not be reconstructed because of overlap between multiple images in a bin.

\begin{table}[!t]
\small
\begin{center}
% \begin{tabular}{|p{0.19\columnwidth}|p{0.25\columnwidth}|p{0.23\columnwidth}|p{0.13\columnwidth}|} 
\begin{tabular}{|p{0.182\columnwidth}|c|c c|} 
\hline
% & Num. of clients & Leaked images & Total\newline images & Leakage rate \\
\textbf{Dataset} & \textbf{Metrics} & \textbf{\name} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Robbing the} \\ \textbf{Fed~\cite{fowl2022robbing}}\end{tabular}} \\
\hline
% \textbf{Num clients} & 85 & 15 & 100 \\
\multirow{4}{*}{\textbf{CIFAR-100}} & Leaked imgs & 4196 & 0 \\
& \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Attacked imgs\\ ($\%$ total)\end{tabular}} & 5440 ($85\%$) & 6400 ($100\%$) \\
& Leakage rate & 77.13\% & 0.00\% \\
\hline
\multirow{4}{0.19\columnwidth}{\textbf{Tiny ImageNet}} & Leaked imgs & 4198 & 0 \\
& \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Attacked imgs\\ ($\%$ total)\end{tabular}} & 5440 ($85\%$) & 6400 ($100\%$) \\
& Leakage rate & 77.17\% & 0.00\% \\
\hline
\multirow{4}{*}{\textbf{MNIST}} & Leaked imgs & 4546 & 1 \\
& \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Attacked imgs\\ ($\%$ total)\end{tabular}} & 6400 ($100\%$) & 6400 ($100\%$) \\
& Leakage rate & 71.03\% & 0.02\% \\
\hline
\end{tabular}
\end{center}
\vspace*{-5mm}
\caption{\label{tab:vanilla_table} Leakage results with an aggregated gradients across 100 clients with a batch size of 64. \name only attacks 85 clients for CIFAR-100 and Tiny ImageNet due to the number of convolutional kernels. Robbing the Fed~\cite{fowl2022robbing} attacks all clients, and leakage results for the same number of non-zero parameters are added is used.}
\vspace*{-3mm}
\end{table}
%\atul{DOes it mean RTF attacks 100\% of the clients but the leakage rate is low - we can highlight that in the caption}

\iffalse

\begin{table}[t!]
\footnotesize
\begin{tabular}{|l|cc|cc|cc|}
\hline
                 & \multicolumn{2}{c|}{\textbf{PSNR} $\uparrow$} & \multicolumn{2}{c|}{\textbf{SSIM} $\uparrow$} & \multicolumn{2}{c|}{\textbf{LPIPS} $\downarrow$} \\ \cline{2-7} 
                 & \textbf{\name}    & \textbf{RtF}    & \textbf{\name}    & \textbf{RtF}    & \textbf{\name}     & \textbf{RtF}    \\ \hline
\textbf{Correct} & 70.71            & 124.58          & 0.9947           & 1.0000          & 0.0013            & 0.0000          \\
\textbf{Overlap} & 17.80            & 21.14           & 0.4792           & 0.4808          & 0.3557            & 0.3680          \\
\textbf{Total}   & 50.86            & 85.79           & 0.8014           & 0.8053          & 0.1342            & 0.1380          \\ \hline
\end{tabular}
\caption{\label{tab:recon-metrics} Image metrics for the reconstruction quality on a client batch of 64 images from CIFAR-100. 40 reconstructions are correct, with the other 24 images having overlaps within their bin. PSNR, SSIM, and LPIPS scores are used for comparison between \name and Robbing the Fed (RtF)~\cite{fowl2022robbing}.}
\end{table}
\fi


\begin{table}[t!]
\small
\begin{center}
\begin{tabular}{|l|l|c|c|c|}
\hline
                                    &                  & \textbf{PSNR} $\uparrow$ & \textbf{SSIM} $\uparrow$ & \textbf{LPIPS} $\downarrow$ \\ \hline
\multicolumn{1}{|c|}{\textbf{}}     & \textbf{Correct} & 64.09         & 0.9879        & 0.0046         \\
\multicolumn{1}{|l|}{\textbf{\name}} & \textbf{Overlap} & 15.55         & 0.5950        & 0.2590         \\
\multicolumn{1}{|l|}{\textbf{}}     & \textbf{Total}   & 58.02         & 0.9388        & 0.0364         \\ \hline
\multirow{3}{0.19\columnwidth}{\textbf{Robbing the Fed}}     & \textbf{Correct} & 127.71        & 1.0000        & 0.0000         \\
\multicolumn{1}{|l|}{\textbf{}}  & \textbf{Overlap} & 16.34         & 0.5981        & 0.2654         \\
\multicolumn{1}{|l|}{\textbf{}}     & \textbf{Total}   & 113.79         & 0.9498        & 0.0332        \\ \hline
\end{tabular}
\end{center}
\vspace*{-5mm}
\caption{\label{tab:recon-metrics} Image metrics for the reconstruction quality on a client batch of 64 images from CIFAR-100. 40 reconstructions are correct, with the other 24 images having overlaps within their bin. PSNR, SSIM, and LPIPS scores are used for comparison between \name and Robbing the Fed (RtF)~\cite{fowl2022robbing} and indicate they are comparable.}
\vspace*{-5mm}
\end{table}


\subsection{Vanilla attack}
We start with a basic example, an aggregated gradient with $100$ clients participating, each with a batch size of $64$. With $256$ convolutional kernels, we create $\lfloor \frac{256}{3} \rfloor=85$ models corresponding to separate identity mapping sets for CIFAR-100 and Tiny ImageNet. Each model goes to a single client and the remaining 15 clients receive a masked gradient model. For MNIST, we are instead able to attack all 100 clients with the same model. This is possible because MNIST uses 1-channel images and we only need one convolutional kernel for each model. Figure~\ref{fig:single_client_cifar100} shows a sample of leaked images in the CIFAR-100 dataset for a single client during the attack, with the empty boxes indicating failed reconstruction. Table~\ref{tab:recon-metrics} shows the PSNR, SSIM~\cite{wang2004image}, and LPIPS~\cite{zhang2018unreasonable} scores for this sample client's reconstructions. Table~\ref{tab:vanilla_table} reports the total leakage results across these datasets.

In a single training iteration we leak $65.56\%$ of the $6,400$ total images for the CIFAR-100 dataset, $65.59\%$  for Tiny Imagenet, and $71.03\%$ for MNIST, using the aggregated gradient for reconstruction. When considering the attacked images, the leakage rates are 77.13\%, 77.17\%, and 71.03\% for CIFAR-100, Tiny ImageNet, and MNIST respectively. This scale of leakage was previously unachievable. The client owner of each reconstructed image is also known to the server, as each model corresponds to a single client. We would also like to bring attention to how $15$ of the clients were not attacked for CIFAR-100 and Tiny ImageNet. We can increase the number of leaked images through shared models, a method that does not require an increase in the number of kernels, which we will discuss in Section~\ref{sec:shared-models}.
\atul{Since our attacks succeeds in just 1-2 iterations, another thing to highlight could be that we attack clients 1-85 in iteration 1, and maybe 15-100 in a different iteration. So, our attack is not limited by the number of CNN kernels as we can reuse them at different time instances. Do you agree?}
% SB (10/11/22): Good point and needs discussion before our next paper. The image batch will change from iteration 1 to iteration 2. 
From the image quality metrics, we conclude that for the images that we reconstruct their similarity to ground truth is comparable to that of Robbing the Fed. (The unreliability of PSNR as a metric for comparison with ground truth is discussed in Section~\ref{sec:image-metrics}.) 

% 20.8 and 63.8 respectively

\begin{figure}[!t]
\vspace*{-2.5mm}
\begin{center}
\includegraphics[width=0.9\columnwidth]{Plots-Images/linear-layer-leakage-vary-clients.png}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:leakage-comp-rtf} Comparing the total leakage rate of \name to Robbing the Fed~\cite{fowl2022robbing} for several settings: same number of bins, same absolute number of added parameters, and the same number of non-zero parameters. Our leakage rate remains the same while the leakage rate of~\cite{fowl2022robbing} needs double the number of parameters to do the same.}
\vspace*{-6mm}
\end{figure}

\subsection{Leakage comparison}
In Section~\ref{sec:param-comp} we discussed the number of added parameters compared to Robbing the Fed. Here we evaluate the amount of leakage under various comparable settings for the number of parameters.
For this experiment, we use a client batch size of $64$ on Tiny ImageNet and vary the number of clients while comparing \name's leakage rate to Robbing the Fed in three settings: when the number of bins is the same, when the absolute number of parameters is the same, and when the number of non-zero parameters is the same. We measure the leakage rate with 1 to 1000 clients, in steps of 50. Each of these data points are sampled 10 times and we plot the average. For \name, we have an FC layer of $256$ units, and we add $3$ convolutional kernels for each client. 

For Robbing the Fed, when the number of bins is the same, we add $4$ units to the FC layer for each image ($256$ for each client). This setup roughly doubles the number of parameters added by Robbing the Fed compared to \name as discussed in Section~\ref{sec:param-comp}. When we keep the absolute number of added parameters the same, we use an FC layer size of $256$. However, with more clients, we only add $2$ units to the FC layer for each image ($128$ for each client). This is because adding units to the FC layer will add two sets of parameters, one for the input and first FC layer and the second for the first and second FC layers. In the final case where the number of non-zero parameters is the same, Robbing the Fed has a fixed FC layer size of $256$ units. Figure~\ref{fig:leakage-comp-rtf} shows the results for the total leakage rate for \name and Robbing the Fed under various parameter comparisons.

The leakage rate for Robbing the Fed remains the same as long as the ratio of bins to images remains the same as \name. When there are $1,000$ clients (a total number of $64,000$ images) Robbing the Fed achieves a $77\%$ leakage rate with an FC layer of $256,000$ units (all non-zero parameters). On the other hand, \name retains the same leakage rate of $77\%$ regardless of the number of clients without needing to increase the number of non-zero parameters. When the number of non-zero parameters are the same, Robbing the Fed cannot leak any images while \name maintains the same high leakage rate. \name has a $17\%$ higher leakage rate when using the same total number of parameters.

\begin{table}
\small
\begin{center}
% \begin{tabular}{|p{0.19\columnwidth}|p{0.25\columnwidth}|p{0.23\columnwidth}|p{0.13\columnwidth}|} 
\begin{tabular}{|c|c|c|c|} 
\hline
% & Num. of clients & Leaked images & Total\newline images & Leakage rate \\
\textbf{Batch size }& \textbf{Leaked imgs} & \textbf{Total imgs} & \textbf{Leakage rate} \\
\hline
% \textbf{Num clients} & 85 & 15 & 100 \\
\textbf{4} & 334 & 340 & 98.2\% \\
\textbf{8} & 661 & 680 & 97.2\% \\
\textbf{16} & 1276 & 1360 & 93.8\% \\
\textbf{32} & 2391 & 2720 & 87.9\% \\
\textbf{64} & 4167 & 5440 & 76.6\% \\
\hline
\end{tabular}
\end{center}
\vspace*{-5mm}
\caption{\label{tab:multiple_clients} Attacking 85 clients of various batch sizes using an FC layer size of 256 for CIFAR-100. The number of leaked images increases with batch size while the leakage rate decreases.}
\vspace*{-6mm}
\end{table}

\subsection{Shared models between clients}
\label{sec:shared-models}
If the server wants to maximize the number of images leaked without increasing the number of convolutional kernels, sending the same model to multiple clients can be a good choice. As a motivating example, Table~\ref{tab:multiple_clients} shows the leakage rate of attacks on varying client batch sizes in the vanilla setting. Since the number of clients attacked is capped at 85, the total number of leaked images is also limited. Consider if there was a total of $8*85=680$ clients each with a batch size of $8$. Without sharing models, the attack would only recover $661$ images. However, if we have $8$ clients sharing each model, the effective batch size would become $8*8=64$, where a batch size of $64$ results in the recovery of $4167$ images instead. Sharing models between clients is very effective in increasing the number of leaked images. However, as discussed previously in Section~\ref{sec:exper-more-clients}, the total number of images, i.e., product of (number of clients)$\cdot$(batch size) cannot grow much larger than the sum of the sizes of the FC layers at all the clients. 
% \atul{But the tradeoff being that multiple images could activate the same bin?}
% SB: Precisely and that would not be good for reconstruction. 

For the experiments, we have $700$ clients participating in aggregation, each with a batch size of $16$. The model still has $256$ convolutional kernels, but we vary the size of the first FC layer, using $256$, $128$, and $64$ units.  Figure~\ref{fig:multi-clients-per-map} shows the total number of leaked images based on the number of clients sharing the same model. When we have an FC layer of $256$ units and $8$ clients sharing each model, a total of $6375$ images are leaked, $56.92\%$ of the total $11,200$ images. For an FC layer size of $64$, we observe that the number of images leaked peaks when $4$ clients are sharing each model. Increasing the number of clients past this decreases the total number of leaked images as there are too many collisions between clients preventing reconstruction. For the smallest FC size of 64, the diminishing return comes earlier, at 4 clients per model. 

In the original vanilla attack, we only reached $85$ clients due to the convolutional layer size. Instead, if we use the method discussed here, we can reach all $100$ clients by having $15$ models shared by two clients. The remaining $70$ models would have a single client. Using this, we increase the leakage rate from $65.56\%$ to $71.57\%$. 
% This is an increase of $270$ leaked images.

The key invariant for \name with model sharing among clients is that the following are equivalent in terms of effectiveness of leakage: (i) $N$ clients being attacked, no model sharing among clients, batch size of $M$; (ii) $N \cdot k$ clients being attacked, $k$ clients sharing a model, batch size of $M/k$.

\begin{figure}[!t]
\vspace*{-2mm}
\begin{center}
\includegraphics[width=0.85\columnwidth]{Plots-Images/mult-clients-per-map-comp.png}
\end{center}
\vspace*{-6mm}
\caption{\label{fig:multi-clients-per-map} Number of images leaked based on the number of clients sharing the same model. FC layer sizes of 256, 128, and 64 are used.}
\vspace*{-3mm}
\end{figure}

% \begin{figure}[!t]
% \vspace*{-3mm}
% \begin{center}
% \includegraphics[width=1.0\columnwidth]{Plots-Images/mult-clients-per-map-64.png}
% \end{center}
% \vspace*{-5mm}
% \caption{\label{fig:multi-clients-imprint-64} Number of images leaked for an FC layer of 64 units and varying number of clients sharing the model. There is a diminishing return for leaked images followed by a negative return with enough clients.}
% % \vspace*{-3mm}
% \end{figure}

\subsection{Non-IID federated learning}
Previous experiments worked under the setting that client data was IID and the bins were initialized the same for everyone. In these experiments, the clients contain non-IID distributions. We use OrganAMNIST instead of the CIFAR dataset, as it has a less uniform average pixel intensity distribution which is shown in Figure~\ref{fig:dataset-distributions}. For OrganAMNIST, label-based separation also results in individual distributions with larger differences in mean and SD. For this separation, each client has data from a single class. Since OrganAMNIST only has 11 classes, several clients share data from the same label. However, this is not known to the server so the distributions of clients are learned independently. We use a dataset agnostic bin initialization for the first round, where all clients have bins with mean -0.5 and SD 0.25 following a normal distribution. This value is adjusted after each training round separately for each client based on observing the activated bins.

% \begin{figure}[]
% \begin{center}
% \includegraphics[width=1.0\columnwidth]{Plots-Images/organamnist-dist.png}
% \end{center}
% \vspace*{-5mm}
% \caption{\label{fig:organ-distribution} OrganAMNNIST dataset distribution for average pixel intensity summed across chanels}
% \vspace*{-3mm}
% \end{figure}


\begin{figure}[t!]
     \centering
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\columnwidth]{Plots-Images/organ-dist-no-axis.png}
         \caption{OrganAMNIST}
         \label{fig:organ-dist}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\columnwidth]{Plots-Images/cifar-dist-no-axis.png}
         \caption{CIFAR-100}
         \label{fig:cifar-dist}
     \end{subfigure}
\vspace*{-2mm}
\caption{\label{fig:dataset-distributions} Average pixel intensity dataset distribution for (a) OrganAMNIST and (b) CIFAR-100. The assumption of Gaussian for OrganAMNIST would lead to incorrect setting of biases for the bins.}
\vspace*{-4mm}
\end{figure}


We use $100$ clients for each experiment, but vary the client batch size as $64$, $32$, $16$, and $8$. Since OrganAMNIST uses single channel images, we are able to attack all clients with our baseline model (with $256$ convolutional kernels). Figure~\ref{fig:organ-leakage-results} shows the number of images leaked over several training iterations for the different batch sizes. The total number of images for each batch size is the (number of clients)$\cdot$(batch size). Despite the difference in total images, after just the first training round of observing, a massive jump in leaked images occurs for all different batch sizes. The largest case is with a batch size of 64 where this jump leads to an increase of $64.29\%$ more leaked images. The smallest jump is for a batch size of 8 with an increase of $57.85\%$ images. After the initial jump, the total leakage fluctuates slightly between rounds, due to randomness of the client batches and bin initialization. 

\subsection{Key takeaways}
Through the evaluation, we have seen how \name functions in several different situations, varying the dataset, batch size, number of clients, and FC layer size. Our main focus has been to highlight the scale of leakage, showing that several thousand images can be leaked in a single iteration, without blowing up the number of parameters that need to be added to the original model. Compared to prior works attacking aggregated gradients, \name achieves a completely different scale. As a result, we compare \name to Robbing the Fed~\cite{fowl2022robbing}, the linear layer leakage method that achieves the highest leakage rate. However, what we find is that Robbing the Fed still has difficulty scaling to federated learning. Even if the size of the model is increased proportionally with the number of clients, as the number of clients participating in aggregation increases, the leakage rate decreases roughly linearly, while ours stays constant.
% SB (10/11/22): Verify
% JZ (10/11/22): Done