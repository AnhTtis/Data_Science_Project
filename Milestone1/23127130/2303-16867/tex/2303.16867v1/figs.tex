%%%%%%%%%%%%%%%%%%%%%% Figure 1 %%%%%%%%%%%%%%%%%%%%%%
\newcommand{\nnsSignal}{
\begin{figure}[t]
\centering
\includegraphics[width=.9\linewidth]{Figures/Gallery.pdf}
% \vspace{-.2cm}
\caption{\textit{Top:} Illustration of non-nutritive sucking (NNS) signal extracted from a pressure transducer pacifier device \cite{martens_changes_2020}. Our computer vision-based NNS recognition and segmentation algorithms enable algorithmic identification of the relatively rare periods of high NNS activity from long videos, facilitating subsequent clinical expert evaluation. \textit{Bottom:} Still frames from our NNS clinical in-crib dataset \textit{(left)} and our public NNS in-the-wild dataset \textit{(right)}.}
% \vspace{-.7cm}
\label{fig:nnsSignal}
\end{figure}
}


%%%%%%%%%%%%%%%%%%%%%% Figure 2 %%%%%%%%%%%%%%%%%%%%%%
\newcommand{\figDiagram}{
\begin{figure*}[t]
\centering
\includegraphics[width=.9\linewidth]{Figures/pipeline_2.pdf}
% \vspace{-.5cm}
\caption{
% \textit{(a):} An overview of the proposed end-to-end NNS segmentation pipeline. The 25-frame sliding window extracts short video clips with $N$ frames overlapping with each other. The action recognition module performs window-based action prediction and generates a label sequence corresponding to each frame series in the input video. \textit{(b):} An diagram of the action recognition module. The dense optical flow results are generated for each pair of preprocessed frames. Then frame-wise special features are calculated via a CNN-based low-dimensional representation module and input into the LSTM-based dynamic event classifier for action label prediction. 
\textit{(a):} Proposed NNS segmentation pipeline, based on aggregating local results of NNS action recognition in sliding windows.
\textit{(b):} Proposed NNS action recognition pipeline, which applies dense optical flow to preprocessed frames, and passes features through a convolutional layer followed by a temporal layer to obtain an action prediction based on spatiotemporal information.}
% \vspace{-.5cm}
\label{fig:Diagram}
\end{figure*}
}

%%%%%%%%%%%%%%%%%%%%%% Figure 3 %%%%%%%%%%%%%%%%%%%%%%
\newcommand{\segVis}{
\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{Figures/segmentation-visualization.png}

% \vspace{-.2cm}
\caption{Segmentation predictions and ground truth for each 60 s mixed clip from the NNS clinical in-bed dataset, under the sliding window aggregation model configuration and with a confidence threshold of 0.9, boosting precision at the cost of recall.}
%\vspace{-.5cm}
\label{fig:segmentation-visualization}
\end{figure*}
}

%%%%%%%%%%%%%%%%%%%%%% Figure S1 %%%%%%%%%%%%%%%%%%%%%%
\newcommand{\setup}{
\begin{figure*}[htb!]
\centering
\includegraphics[width=0.6\linewidth]{Figures/camera-setup.jpg}
\caption{Suggested baby monitor placement for study participants for our NNS clinical in-crib dataset. Videos were shot by parents or caregivers in 2021 and 2022 during the pandemic. They are long and feature a wide variety of natural infant behavior, including napping, sleeping, tossing and turning, crying, and caregiver interactions such as pacifier insertion, patting, removal from crib, and more, yielding a true-to-life but technically challenging data source.}
\label{fig:setup}
\end{figure*}
}

%%%%%%%%%%%%%%%%%%%%%% Figure S2 %%%%%%%%%%%%%%%%%%%%%%
\newcommand{\annotSoft}{
\begin{figure*}[htb!]
\centering
\includegraphics[width=0.8\linewidth]{Figures/annotation-software.png}
\caption{NNS annotation tool \cite{dutta2019vgg} used by our behavioral coding team specifically trained for this task. For the NNS clinical in-crib dataset annotations, duplicate coding and systematic checks were implemented to ensure reliability over the hundreds of hours of footage.}
\label{fig:annotSoft}
\end{figure*}
}

%%%%%%%%%%%%%%%%%%%%%% Figure S3 %%%%%%%%%%%%%%%%%%%%%%
\newcommand{\opFlow}{
\begin{figure*}[htb!]
\centering
\includegraphics[width=\linewidth]{Figures/OpticalFlow_Gallery.pdf}
\caption{Examples of optical flow processing on NNS clinical in-crib data, highlighting its effectiveness at distinguishing (a) NNS events from (b) non-NNS events. See the optical flow ablation study in \secref{nnsResults} for corresponding numerical results.}
\label{fig:opFlow}
\end{figure*}
}

% \newcommand{\nnsDistribution}{
% \begin{figure}
% \centering
% \includegraphics[width=0.6\linewidth]{Figures/nns-distribution.png}
% %\vspace{-.5cm}
% \caption{Histograms of annotated event lengths drawn from the six infant subject videos used in non-nutritive suck detection learning algorithm's training and testing dataset. In our study and in this figure, we focus exclusively on non-nutritive suck (NNS) activity occurring during pacifier use, when behavior patterns are more regular and annotations more reliable (see \tabref{cohen-kappa}). For each subject, events from both annotators are included.}
% \vspace{-.5cm}
% \label{fig:nnsDistribution}
% \end{figure}
% }

% \newcommand{\anotSamples}{
% \begin{figure*}
% \centering
% \includegraphics[width=\linewidth]{Figures/annotation-samples.png}
% %\vspace{-.5cm}
% \caption{Randomly selected annotation samples of non-nutritive suck (NNS) bursts during pacifier usage across subjects, including those not included in our training and testing dataset. The NNS activity was coded by two independent annotators using specialized software to mark start and end times and represented here the blue and orange time binary time series. In our study and in this figure, we focus exclusively on NNS activity occurring during pacifier use, when behavior patterns are more regular and annotations more reliable (see \tabref{cohen-kappa}).}
% \vspace{-.5cm}
% \label{fig:anotSamples}
% \end{figure*}
% }


% \newcommand{\predSamples}{
% \begin{figure*}
% \centering
% \includegraphics[width=\linewidth]{Figures/prediction-samples.png}
% %\vspace{-.5cm}
% \caption{[DRAFT IMAGE OF SAMPLE PREDICTIONS.]}
% \vspace{-.5cm}
% \label{fig:predSamples}
% \end{figure*}
% }


% \newcommand{\figPreprocessing}{
% \begin{figure*}[ht]
% \centering
% \includegraphics[width=\linewidth]{Figures/Perprocessing.png}
% %\vspace{-.5cm}
% \caption{The detailed prepossessing pipeline, which starts with the detection of the facial area and pacifier for each frame. If both of them are all detected, the facial area in all frames then are trimmed with the same data augmentation operations including flipping, rotation, and inflation. Then, the trimmed facial area image sequence is temporally smoothed to reduce the  discontinuity between frame-wise predicted bounding box.}
% \vspace{-.5cm}
% \label{fig:Preprocessing}
% \end{figure*}
% }


% \newcommand{\figmouthbb}{
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.7\linewidth]{Figures/Mouth_bb_final.png} 
%     \caption{Preliminary results from the current mouth bounding box pipeline.}
% \label{fig:mouthbb}
% \end{figure}
% }

% \newcommand{\figOpticalFlow}{
% \begin{figure*}[ht]
% \centering
% \includegraphics[width=0.6\linewidth]{Figures/ab2.png}
% %\vspace{-.5cm}
% \caption{The performance comparison between four wildly used optical flow algorithms on our dataset, in which the Coarse2Fine algorithm reached minimum background noise.}
% \vspace{-.5cm}
% \label{fig:Ab2}
% \end{figure*}
% }

% \newcommand{\figlosscurves}{
% \begin{figure}%
%     \centering
%     {{\includegraphics[width=5.6cm, height=7.32cm]{Figures/loss_curves.png} }}%
%     \qquad
%     {{\includegraphics[width=5.6cm]{Figures/map_score.png} }}%
%     \caption{Loss and mAP curves from YOLO-v5 custom training}%
%     \label{fig:example}%
% \end{figure}
% }
