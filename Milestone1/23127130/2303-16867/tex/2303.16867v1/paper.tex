% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%

\let\accentvec\vec

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}  
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{bigdelim}
\usepackage{xcolor}
\usepackage{float}
\usepackage{makecell}
\usepackage{hyperref}

\setlength{\tabcolsep}{6pt}

\input{tabs}
\input{figs}
\input{supplementary}


\renewcommand*\ttdefault{txtt}
\newcommand{\com}[1]{\textcolor{red}{#1}}
\newcommand{\eqnref}[1]{Eq.~(\ref{eqn:#1})}
\newcommand{\figref}[1]{Fig.~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\algref}[1]{Algorithm.~\ref{alg:#1}}
\newcommand{\thmref}[1]{Theorem~\ref{thm:#1}}
\newcommand{\defref}[1]{Definition~\ref{definition:#1}}
\newcommand{\lemref}[1]{Lemma~\ref{lem:#1}}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}} % For booktabs table formatting
% \newtheorem{theorem}{Theorem}
% \newtheorem{lemma}{Lemma}



\begin{document}
%
\title{A Video-based End-to-end Pipeline\\for Non-nutritive Sucking Action  Recognition\\and Segmentation in Young Infants}
%A deep learning based end-to-end pipeline for non-nutritive suck action recognition in videos\thanks{Supported by organization x.}
%
\titlerunning{Non-nutritive Sucking Action Recognition}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{Anonymous}
% % \author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
% % Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
% % Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
% %
% \authorrunning{Anonymous}
% %\authorrunning{F. Author et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.
% %
% \institute{Anonymous Organization\\
% \email{**@******.***}}
% \institute{Princeton University, Princeton NJ 08544, USA \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}}


\author{Shaotong Zhu\inst{1}, Michael Wan\inst{1,2}, Elaheh Hatamimajoumerd\inst{1}, Kashish Jain\inst{1}, Samuel Zlota\inst{1}, Cholpady Vikram Kamath\inst{1}, Cassandra B. Rowan\inst{5}, Emma C. Grace\inst{5}, Matthew S. Goodwin\inst{3,4}, Marie J. Hayes\inst{5}, Rebecca A. Schwartz-Mette\inst{5}, Emily Zimmerman\inst{4}, Sarah Ostadabbas\inst{1}$^*$}


\authorrunning{Shaotong Zhu et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.


\institute{Augmented Cognition Lab, Department of Electrical \& Computer Engineering\\Northeastern University, Boston MA, USA\\
\and
Roux Institute, Northeastern University, Portland ME, USA\\
\and
Khoury College of Computer Sciences, Northeastern University, Boston MA, USA\\
\and
Bouv\'e College of Health Sciences, Northeastern University, Boston MA, USA\\
\and
Psychology Department, University of Maine, Orono ME, USA\\
$^*$Corresponding author: \email{ostadabbas@ece.neu.edu}
}

\maketitle              % typeset the header of the contribution
%
\begin{abstract}
% \com{this needs revision. }
We present an end-to-end computer vision pipeline to detect non-nutritive sucking (NNS)---an infant sucking pattern with no nutrition delivered---as a potential biomarker for developmental delays, using off-the-shelf baby monitor video footage. One barrier to clinical (or algorithmic) assessment of NNS stems from its sparsity, requiring experts to wade through hours of footage to find minutes of relevant activity. Our NNS activity segmentation algorithm solves this problem by identifying periods of NNS with high certainty---up to 94.0\% average precision and 84.9\% average recall across 30 heterogeneous 60 s clips, drawn from our manually annotated NNS clinical in-crib dataset of 183 hours of overnight baby monitor footage from 19 infants. Our method is based on an underlying NNS action recognition algorithm, which uses spatiotemporal deep learning networks and infant-specific pose estimation, achieving 94.9\% accuracy in binary classification of 960 2.5 s balanced NNS vs. non-NNS clips. Tested on our second, independent, and public NNS in-the-wild dataset, NNS recognition classification reaches 92.3\% accuracy, and NNS segmentation achieves 90.8\% precision and 84.2\% recall\footnote{Our code and the manually annotated NNS in-the-wild dataset can be found at \url{https://github.com/ostadabbas/NNS-Detection-and-Segmentation}. Supported by MathWorks and NSF-CAREER Grant \#2143882.}.

\keywords{Non-nutritive sucking \and Action recognition \and Action segmentation \and Optical flow \and Temporal convolution.}
\end{abstract}

% Our method uses spatiotemporal networks and infant-specific pose estimation to tackle the short-term NNS action recognition problem, classifying 2.5 s video clips into NNS and non-NNS classes, and then applies this classifier over sliding windows and filter the output to solve the NNS action segmentation problem over longer videos. Our main training and testing is drawn from our newly collected and meticulously annotated dataset of 183 hours of overnight in-crib baby monitor footage from 19 infants. We achieve strong cross-validation results on this data, and strong test results on a second set of 10 short video clips which we annotate and make publicly available.  \com{abstract needs to give some quantitative results too.}

% origin
% In doing so, we overcome challenges including the noisiness and high-dimensionality of the video footage taken at home, the difficulty of reliably annotating the data, the relatively small number of training and testing subjects, and the limited effectiveness of existing computer vision techniques in the infant domain. Our method combines a deep spatiotemporal convolutional network with techniques designed specifically to face these challenges, including the use of infant in-crib state tracing to focus the network's attention on the infant's head region, and enhancement of the subtle mouth movement using dense optical flow. We achieve an average NNS classification accuracy of 88.1\% with balanced classes under subject-wise leave-one-out cross validation on a dataset of 880 video clips, extracted from 183 hours of nighttime baby monitor footage collected by our clinical team and annotated by our behavioral psychology researchers.
% revision from Shaotong
% In doing so, we first collect overnight sleeping recordings from 18 2-4 mouths old infant volunteers to build up a novel video-based  NNS action dataset, overcoming the challenges including the noisiness and high-dimensionality of the videos collected from variant scenes, and the difficulty of reliably annotation. Then, we propose a deep spatiotemporal convolutional network with techniques designed specifically to recognize the subtle NNS actions, including the use of infant in-crib state tracing to focus the network's attention on the infant's head region, and enhancement of the subtle mouth movement using dense optical flow. 



%
\section{Introduction}
\label{sec:intro}
Non-nutritive sucking (NNS) is an infant oral sucking pattern characterized by the absence of nutrient delivery \cite{ctx16694780230001401}. NNS reflects neural and motor development in early life \cite{medoff-cooper_neonatal_1995} and may reduce the risk of SIDS \cite{psaila_infant_2017,zavala_abed_how_2020}, the leading cause of death for US infants aged 1-12 months \cite{carlin2017risk}. However, studying the relationship between NNS patterns and breathing, feeding, and arousal during sleep has been challenging due to the difficulty of measuring the NNS signal.
% Non-nutritive sucking (NNS) is an infant oral sucking pattern characterized by the absence of nutrient delivery \cite{ctx16694780230001401}. Given the limited range of motor function and means of expression in infancy, characteristics of NNS constitute critical signals of neural and motor development in early life \cite{medoff-cooper_neonatal_1995}. NNS activity has also been proposed as a potential mechanism for reducing the risk of sudden infant death syndrome (SIDS) \cite{psaila_infant_2017,zavala_abed_how_2020}, the leading cause of death of US infants aged between 1 and 12 months \cite{carlin2017risk}. Understanding the relation between NNS patterns and characteristics of breathing, feeding, and arousal during sleep could enhance scientific understanding of infant neurodevelopment and protective factors for SIDS, but few such studies have been conducted, partly due to the difficulty of measuring the NNS signal.

\nnsSignal
NNS occurs in bursts of 6--12 sucks at 2 Hz per suck, with bursts happening a few times per minute during high activity periods \cite{zimmerman_changes_2020}. However, active periods are sporadic, representing only a few minutes per hour, creating a burden for researchers studying characteristics of NNS. Current transducer-based approaches (see \figref{nnsSignal}) are effective, but expensive, limited to research use, and may affect the sucking behavior \cite{zimmerman_patterned_2017}. This motivates our development of an end-to-end computer vision system to recognize and segment NNS actions from lengthy videos, enabling applications in automatic screening and telehealth, with a focus on high precision to enable periods of sucking activity to be reliably extracted for analysis by human experts. 
% Non-nutritive sucking occurs in bursts of approximately 6--12 sucks at 2 Hz per suck, with bursts occurring a few times per minute during periods of high NNS activity \cite{zimmerman_changes_2020}. Active periods can be quite sporadic, however, with overall NNS activity often representing only a few minutes per hour, creating a major burden of labor for clinicians and researchers hoping to study the characteristics of NNS and how it changes over time. While current transducer-based approaches (see \figref{nnsSignal}) for monitoring NNS are effective \cite{zimmerman_patterned_2017}, they are expensive, limited to research use, and may affect the sucking behavior itself. This motivates our development of an end-to-end computer vision system for recognition and segmentation of infant NNS action from videos captured overnight in natural environments to enable widespread applications in automatic screening and telehealth, with a focus on high precision to enable periods of sucking activity to be reliably extracted for analysis by human experts. 


Our contributions tackle the fine-grained NNS action recognition problem of classifying 2.5 s video clips, and the NNS action segmentation problem of detecting NNS activity in minute-long video clips. The action recognition method uses convolutional long short-term memory networks for spatiotemporal learning. We address data scarcity and reliability issues in real-world baby monitor footage using tailored infant pose state estimation, focusing on the face and pacifier region, and enhancing it with dense optical flow. The action segmentation method aggregates local NNS recognition signals from sliding windows.
% Our technical contributions tackle both the fine-grained NNS action recognition problem of classifying 2.5 s video clips into NNS or non-NNS classes, as well as the broader NNS action segmentation problem of determining the frames exhibiting NNS activity in minute-long video clips. The action recognition method is based on spatiotemporal learning with convolutional long short-term memory networks. To deal with significant data scarcity and reliability issues from our real world baby monitor footage, our pipeline uses tailored infant pose state estimation to detect the infant's face and focus in on the mouth and pacifier region and enhance it with dense optical flow. The action segmentation method aggregates and filters the results of the local NNS recognition applied over sliding windows, to great effect.
% This work lays the groundwork for a fully automated computer vision assessment of NNS, including extraction of sucking signal characteristics such as frequency, duration, amplitude, and temporal pattern.

We present two new datasets in our work: the \textbf{NNS clinical in-crib dataset}, consisting of 183 h of nighttime in-crib baby monitor footage collected from 19 infants and annotated for NNS activity and pacifier use by our interdisciplinary team of behavioral psychology and machine learning researchers, and the \textbf{NNS in-the-wild dataset}, consisting of 10 naturalistic infant video clips annotated for NNS activity. \figref{nnsSignal} displays sample frames from both datasets.
% Our primary training and testing data is drawn from a new dataset---our \textbf{NNS clinical in-crib dataset}---of 183 h of nighttime in-crib baby monitor footage collected from 19 infants by our clinical team, and meticulously annotated (or ``behaviorally coded'') for NNS activity and pacifier use by our interdisciplinary team of behavioral psychology and machine learning researchers. We also introduce our \textbf{NNS in-the-wild dataset} of 10 naturalistic infant video clips, 1--5 minutes in length, extracted from publicly available sources and also annotated for NNS activity. \figref{nnsSignal} shows sample frames from these datasets.

% The complete system will first take infant in-crib baby monitor footage, apply infant state and object detection to determine when the infant is on screen and engaging in pacifier use, smoothly focus in on the infant mouth and face area, and then enhance the subtle movement signal with dense optical flow. This processed video signal will then be fed into a two-part deep learning pipeline: small clips from sliding windows are inputted into a temporal convolutional network for classification based on the level of NNS action, and then the classification scores (or pre-activation network features) from the overlapping windows are fed into higher level global temporal segmentation model to produce the final timestamps for NNS action throughout the entire video. This multi-part approach is designed to make use of infant-domain-specific knowledge to overcome issues arising from the noisiness and scarcity of the available off-the-shelf baby monitor footage, and from the difficulty of annotating for NNS action in a reliable and scientific manner. 

% The need to screen widely for signs of developmental delays, the limited availability of expert clinicians for such assessments, and the cumbersomeness of existing tools like customized pressure transducer systems for characterizing infant sucking behavior \cite{huang2019infant} all speak to the the promise of a video based tool for automatically detecting and analyzing NNS using computer vision and deep learning. 
% As a step towards realizing this goal, we gather a large dataset of 183 hours of video footage of 19 infants during sleep time exhibiting NNS and pacifier action, undertake extensive efforts to reliably annotate the data with time stamp labels for NNS and pacifier action, and extend tools in deep learning action recognition to develop the first algorithm for the detection of NNS action from infant video clips. 

% NNS behavior is more subtle than most of the everyday human activities studied in computer vision on action recognition and segmentation. During pacifier use, when NNS action is more robust, non-nutritive sucks typically occur at rates of approximately 2Hz, in uninterrupted groups of 6--12 sucks known as NNS \textit{bursts} \cite{martens_changes_2020}.
% % , which we adopt as the base unit of action for our segmentation task. 
% An example of NNS burst is shown in Figure \ref{fig:nnsSignal}. Even grouped into bursts, NNS action can be quite sparse, fairly faint, and difficult to distinguish from other kinds of oral movements, especially when the infant is not using a pacifier. As such, obtaining reliable ground truth annotations for NNS bursts required a coordinated effort led by our neurodevelopmental behavioral coding team, involving careful annotator training and stages of human and algorithmic verification of annotation integrity. As part of our integrity checks, we had each subject video annotated independently by two researchers and filtered the data based on quantitative inter-rater reliability scores as well as qualitative judgements. The final set of six infant videos from which we derived training and testing exhibited a mean Cohen $\kappa$ reliability score of 82.8 (``strong'') for annotations of NNS events during pacifier usage when considered on a standard 10-second-window-incidence basis to account for differences in annotator reaction time and interpretation. From this set of videos, we extract 880 two-second clips evenly balanced between exhibiting NNS behavior and a non-NNS control set, and aim to develop a deep learning algorithm for binary classification (NNS vs non-NNS) from such clips from an unseen infant subject. 

% Given the high dimensionality of video data and the low subject sample size, our task should be understood as an exercise in small data machine learning. Accordingly, our method blends general computer vision video comprehension tools with tailored domain knowledge and techniques. Our general deep learning pipeline is based on a two-stage temporal convolutional structure, consisting of a convolutional residual network (ResNet101 \cite{he2016deep}) followed by a long short-term memory (LSTM) network, commonly used in video action recognition to enable temporal processing of image data. Infant in-crib domain knowledge is then injected via specialized infant face localization and tracking, to allow the network to focus and learn from the relevant part of the image field. This kind of in-crib localisation is not available off the shelf and was developed by us specifically for this setting. A second critical piece of domain customization lies in the conversion of RGB frames to dense optical flow colormaps, just before input into the convolutional network. The combination of mouth region tracking and optical flow conversion enables us to isolate subtle vibrational NNS movements from the coarser infant head or body movements. Our final system achieves an average classification accuracy of 88.1\% with balanced classes (90.2\% specificity and 86.7\% sensitivity) under a subject-wise leave-one-out cross validation regime, compared to 54.2\% for the same system without optical flow conversion, highlighting the importance of domain-specific considerations in our small data setting. Further ablation studies demonstrate the performance improvements associated with our model's features.

%\com{We need contributions paragraph}
Our main contributions are (1) creation of the first infant video datasets manually annotated with NNS activity; (2) development of an NNS classification system using a convolutional long short-term memory network, aided by infant domain-specific face localization, video stabilization, and customized signal enhancement, and (3) successful NNS segmentation on longer clips by aggregating local NNS recognition results across sliding windows.
% In sum, our main contributions include (1) creation of the first infant video datasets manually annotated with NNS activity, the \textit{NNS clinical in-crib dataset} and the public \textit{NNS in-the-wild dataset}; (2) development of an NNS classification system on 2.5 s clips based on learning with a convolutional long short-term memory network, and aided by infant domain specific face region localization, video stabilization via salience point flow tracking, and customized signal enhancement with dense optical flow conversion; and (3) development of successful NNS segmentation on longer clips by applying our NNS recognition in sliding windows and filtering the results.

% In the last section of the paper, we outline our plans for next steps, including replacing our head tracker with a specialized infant mouth tracker to obtain a stronger signal in the preprocessing stage, and our proposal for the global temporal segmentation layer of the pipeline.

% Shaotong's original background on SIDS and NNS:
% Sudden infant death syndrome (SIDS) is the sudden, unexpected death of an infant under 1 year of age that remains unexplained after a complete investigation. In 2017, there were ~1,400 deaths due to SIDS \cite{gol\text{Dst}ein2016overall}. While the causes of death remain unknown in SIDS, certain measures have been shown to be protective in reducing SIDS, such as pacifier use. Several studies have posited that pacifier use at night may put the mandible in a forward position resulting in optimize airway patency \cite{tonkin2007effect}, increase sympathetic tone \cite{yiallourou2014effects}, increased arousal levels \cite{carlin2017risk}, and alter cardiovascular control \cite{franco2004pacifier}, apparent in changes to heart rate variability or respiratory patterning. Non-nutritive suck (NNS; sucking on a pacifier with no nutrient being delivered) is a suck pattern characterized by the absence of nutrient delivery \cite{humphrey1970development}. Temporally localizing the NNS pattern would be helpful for the study of examining the specific and more nuanced aspect of infant suck patterning associated with pacifier use in relation to breathing frequency during sleep periods in young infants. Studies has been established on this topic. Zimmerman’s [?] lab utilizes a customized pressure transducer system to quantitatively measure NNS in real-time. To achieve more efficient contactless NNS data acquisition and pattern quantification scheme, Huang et al. \cite{huang2019infant} leveraged a video-based facial landmarks detection algorithm to extract the movement signals of an infant’s jaw recorded from the infant’s sucking video. Yet it achieve the localization of the NNS event via the facial landmarks predicted from 3D Morphable Model \cite{huber2016multiresolution}, which may not work well due to the domain gap between infants' and adults' face and introduced extra error for the NNS event temporal localization prediction comparing to directly action segmentation method.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
Current methods for measuring the NNS signal are limited to the pressured transducer approach \cite{zimmerman_changes_2020}, and a video-based approach that uses facial landmark detection to extract the jaw movement signal \cite{huang2019infant}. The latter relies on a 3D morphable face model \cite{huber2016multiresolution} learned from adult face data, limiting its accuracy, given the domain gap between infant and adult faces \cite{wan_infanface_2022}; its output also does not directly address NNS classification or segmentation. Our approach offers an efficient, end-to-end solution for both tasks and is freely available.
% Approaches to measuring the NNS signal are limited to the pressured transducer approach addressed above \cite{zimmerman_changes_2020}, and a video-based approach which uses facial landmark detection to extract the jaw movement signal \cite{huang2019infant}. The latter relies on a 3D morphable face model \cite{huber2016multiresolution} learned from adult face data, limiting their accuracy given the domain gap between infant and adult faces \cite{wan_infanface_2022}, and their waveform output does not directly address NNS classification nor segmentation. Our method offers an efficient, quantifiable, end-to-end approach to both of these tasks, one which is freely available.

Action \textit{recognition} is the task of identifying the action label of a short video clip from a set of predetermined classes. In our case, we wish to classify short infant clips based on the presence or absence of NNS. As with many action recognition algorithms, our core model is based on extending 2D convolutional neural networks to the temporal dimension for spatiotemporal data processing.
% Action \textit{recognition} is the task of identifying an action label for a short video trimmed to contain a single well-defined action, typically from a predetermined set of action classes. In our case, we wish to classify short infant clips based on the presence or absence of NNS. As with many action recognition algorithms, our core model is based on extensions of 2D convolutional neural networks (CNNs) to the temporal dimension to properly process spatiotemporal data. 
In particular, we make use of sequential networks (such as long short-term memory (LSTM) networks) after frame-wise convolution to enhance medium-range temporal dependencies \cite{yue2015beyond}.
% A common approach \cite{yue2015beyond} to action recognition enhances long-range temporal dependencies by incorporating long short-term memory (LSTM) networks after frame-wise convolution. 
Action \textit{segmentation} is the task of identifying the periods of time during which specified events occur, often from longer untrimmed videos containing mixed activities. We follow an approach to action segmentation common in limited-data contexts, which patches together signals obtained by a local low-level layer---our NNS action recognition---to obtain a global segmentation result \cite{ding_temporal_2022}.

% Action recognition can be thought of as a classification or detection task which involves learning a label for well-defined, trimmed video clip. Trimmed videos guarantee a single action label for the entire video. With the success of 2D convolutional neural networks (CNNs) for image understanding tasks, many action recognition methods were built on the powerful CNN architecture.

% Ji et al. \cite{ji20123d} firstly proposed 3D CNNs since it can intuitively extract spatio-temporal features from raw videos, while Tran et al. \cite{tran2015learning} trained the 3D CNN model, referred as C3D, using Sports-1M dataset \cite{karpathy2014large}. C3D model then became the standard 3D-CNN based model for action recognition. Further performance improvement for C3D has then been achieved by Varol et al. \cite{varol2017long} via expanding the temporal length of input. More recently, some works introduced ResNet architectures into 3D CNNs \cite{hara2017learning,tran2017convnet}, though they examined only relatively shallow ones.
% Another prospective to address the spatio-temporal features is the two-stream CNN architecture \cite{simonyan2014two} which combines spatial convolution trained with RGB frames and temporal convolution on short-term optical flow to capture the appearance and motion information simultaneously. Zhao et al. \cite{zhao2017pooling} further explored the two-stream CNN by introducing trajectory and line temporal pooling strategies followed with encoded via vector of locally aggregated descriptor (VLAD) to generate the video representations.

% One draw back of two above mentioned CNN based strategies is the fixed temporal convolution kernel depths which is insufficient for handling the various length of motions in videos. To address this issue, the long short-term memory (LSTM) based models are proposed since LSTM \cite{hochreiter1997long} can modeling the video sequences with arbitrary length. Ng et al. \cite{yue2015beyond} incorporated action sequences across a video over long time periods through the utilization of max pooling strategy and LSTM, which effectively aggregates both spatial and temporal information to achieve better recognition accuracy.

\section{Infant NNS Action Recognition and Segmentation}
\label{sec:methods}

%\subsection{Overview}

\figDiagram

\figref{Diagram} illustrates our pipeline for NNS action segmentation, which takes in long-form videos of infants using pacifiers and predicts timestamps for NNS events in the entire video. We first cover the long video with short sliding windows, then apply our NNS action recognition module to obtain a classification for NNS vs non-NNS (or the confidence score), and aggregate the output classes (or scores) into a segmentation result consisting of predicted start and end timestamps. 

% The first component is a frame-based preprocessing module which trims the frame to the infant's facial area, performs image transformations for data augmentation (during training only), and applies short-time dense optical flow to map each trimmed frame into the hue, saturation, and value (HSV) color space. The second component uses a CNN-based low-dimensional representation module to extract spatial features for each frame, and feeds the features into a long short-term memory (LSTM) network-based dynamic event classification module to achieve NNS action recognition on the video clip. The third component aggregates the local action recognition output over sliding windows to perform NNS action segmentation on longer clips.
%\com{we need to also convey the segmentation here and add a subsection on that.}

\subsection{NNS Action Recognition}
\label{sec:method-classification}

The technical core of our model is the NNS action recognition system, depicted in \figref{Diagram}b, which is itself composed of a frame-based preprocessing module followed by a spatiotemporal classifier. The preprocessing module uses a pre-trained model; only the spatiotemporal classifier is actively trained with our data.

\subsubsection{Preprocessing Module} 
% During \textit{inference}, the preprocessing tool takes an input video, smoothly crops to the infant's face, and converts the RGB video signal to  the hue, saturation, and value (HSV) video signal using dense optical flow, which is then fed into the CNN-LSTM classifier. After the preprocessing tool is used to generate data to \textit{train} the subsequent CNN-LSTM classifier, an additional data augmentation step is included during the smooth facial crop procedure. Details for these components are as follows:
Our frame-based preprocessing module applies the following transformations in sequence. All three steps are used to produce training data for the subsequent spatiotemporal classifier, but during inference, the data augmentation step is not applicable and is omitted.
% The goal for the frame-based preprocessing module (see \figref{Diagram}) is to trim videos to temporal regions containing infant faces and pacifiers, and then within those segments, smoothly crop the input video to the infant's facial area. 
% To automatically detect whether the input contains pacifier events throughout the video, our preprocessing module uses a YOLO-v5 object detection network \cite{yolov5_2021} modified via transfer learning to detect pacifiers. The last layer of the pretrained YOLO-v5 model is replaced by a learnable fully connected layer and retrained. 
% Further details of this standard method, including supporting dataset collection and experimental results, can be found in Supp. \secref{end-to-end}.
% Although the pacifier detector is involved in our end-to-end NNS action recognition pipeline, it is independent from the training and validation of proposed action recognition network, which uses data with ground truth pacifier annotations. Therefore, the studies of pacifier detection and action recognition are performed separately.

\textit{Smooth facial crop.} The RetinaFace face detector \cite{retinaface2019} is applied to frames in each clip until a face bounding box is found and propagated to earlier and later frames using the minimum output sum of squared error (MOSSE) tracker \cite{mossepaper}. To smooth the facial bounding box sequence and address temporal discontinuity, saliency corners \cite{shi1994good} are detected from the initial frame and tracked to the next frame using the Lucas-Kanade optical flow algorithm \cite{lucas1981iterative}. The trajectory is smoothed using a moving average filter and applied to each bounding box to stabilize the facial area. The raw input video is then cropped to this smoothed bounding box, resulting in a video featuring the face alone.

% the RetinaFace \cite{retinaface2019} face detector is applied to frames in each clip starting from the first, until it succeeds in finding a bounding box for a face, and then that bounding box is propagated to the earlier and later frames in the clip using the OpenCV MOSSE (Minimum Output Sum of Squared Error) tracker \cite{mossepaper}. Since this facial area detection is performed per frame, it is inherent that temporal discontinuity exists between adjacent facial bounding boxes thoughout the input video, so it is necessary to smooth the facial bounding boxes sequence before the trimming process. The smoothing process starts with the detection of a set of saliency corner points from the initial frame, denoted as $\text{Src}$, which is achieved by the Shi-Tomasi corner detector \cite{shi1994good}.
% Then, we track the saliency points to the next frame using the Lucas-Kanade optical flow algorithm \cite{lucas1981iterative}.
% The facial bounding box motion trajectory between each pair of frames is represented by the affine matrices of the rigid transformation, mapping the saliency point pairs between adjacent frames. The trajectory is then smoothed using a moving average filter so the smoothed affine matrix sequence for each pair of adjacent frames can be calculated by adding the difference between the smoothed trajectory and the original trajectory to the original affine matrix sequence. The facial area is stabled by applying the smoothed affine matrix to each bounding box. The raw input video is trimmed into the facial area only video clip according to the smoothed bounding box.

\textit{Data augmentation.} When preprocessing videos to create training data for the spatiotemporal classifier, we apply random transformations such as rotations, scaling, and flipping to the face-cropped video, to improve generalizability in our data-limited setting.  
% When the preprocessing tool is being used to generate training data for the CNN-LSTM classifier, transformations incorporating rotations, scaling, and flipping are randomly applied to videos after the facial crop to boost generalizability in our data-constrained setting.

%The rotation is performed with the center of the bounding box as the origin point and rotation angle following the normal distribution $N(0,30)$. The inflation factor follows the normal distribution $N(1,0.1)$. The horizontal flipping process happens randomly with the chance of $50\%$.

\textit{Optical flow.} 
After trimming and augmentation, we calculate the short-time dense optical flow \cite{liu2009beyond} between adjacent frames, and map the results into the hue, saturation, and value (HSV) color space by cascading the optical flow direction vector and magnitude of each pixel. This highlights the apparent motion between frames, magnifying subtle NNS movements (as illustrated in Supp. \figref{opFlow}.). 
% After trimming, the short-time dense optical flow \cite{liu2009beyond} is calculated for each pair of the adjacent frames. We cascade the optical flow direction vector and magnitude of each pixel results in order to map the dense optical flow results into the HSV color space. 

% Optical flow is computed using the off-the-shelf PyTorch implementation of \cite{liu2009beyond}. In spite of the fast computation time (0.06s for a pair of frames), it would still introduce a bottleneck if done on-the-fly, so we pre-computed the flow before training. 

\subsubsection{Spatiotemporal-based Action Classifier} 
Finally, the optical flow video is processed by a spatiotemporal model that outputs an action class label (NNS or non-NNS). Two-dimensional convolutional neural networks extract spatial representations from static images, which are then fed in sequence to a temporal convolution network for spatiotemporal processing. The final classification outcome is the output of the last temporal convolution network unit.
% Finally, the optical flow video is passed through a deep learning model that outputs an action class label (NNS vs non-NNS). The structure is that of a CNN-LSTM, which first uses CNNs to extract spatial representations from static images (via CNNs), and then processes those features in sequence using a LSTM network, to enable medium-term spatiotemporal processing. The output of the last LSTM unit serves as the final classification outcome.

\subsection{NNS Action Segmentation}
\label{sec:action-segmentation}
To segment NNS actions in mixed videos with transitions between NNS and non-NNS activity, we applied NNS recognition in 2.5 s sliding windows and aggregated results to predict start and end timestamps. This window length provides fine-grained resolution for segmentation while being long enough (26 frames at a 10 Hz frame rate) for consistent human and machine detection of NNS behavior. To address concerns about the coarseness of this resolution, we tested the following window-aggregation configurations, the latter two of which have finer 0.5 s effective resolutions:
% To solve the segmentation problem of localizing NNS in untrimmed videos featuring transitions between NNS events and non-NNS activity, we applied our NNS recognition in 2.5 s sliding windows across the video and aggregated the results to obtain start and end timestamp predictions. The 2.5 s window length was chosen to be short enough to provide fine-grained resolution for the segmentation task, but long enough (26 frames at a 10 Hz frame rate) for consistent human and machine detection of NNS behavior. To address concerns about the coarseness of this resolution for segmentation, we test the following window-aggregation configurations, the latter two of which have finer 0.5 s effective resolutions:
\begin{description}
    \item \textbf{Tiled:} 2.5 s windows precisely tile the length of the video with no overlaps, and the classification outcome for each window is taken directly to be the segmentation outcome for that window.
    \item \textbf{Sliding:} 2.5 s windows are slid across with 0.5 s overlaps, and the classification outcome for each window is assigned to its (unique) middle-fifth 0.5 s segment as the segmentation outcome.
    \item \textbf{Smoothed:} 2.5 s windows are slid across with 0.5 s overlaps, the classification \textit{confidence score} for each window is assigned to its middle-fifth 0.5 s segment, a 2.5 s moving average of these confidence scores are taken, then the averaged confidence scores are thresholded for the final segmentation outcome.
\end{description}



 
%we slid a 2-second window with and without one-second overlap and fed the video frames at each window to our classification model This collection of videos consists of both video clips from our own dataset and videos from YouTube, with an average length of X minutes. 

% In order to generate the local feature sequence, we will modify our proposed model to achieve short-term spatial-temporal feature extraction for a short sliding window on the input video. 
% Then, we will build up dilated convolution models to globally piece together these local features to obtain final event timestamp outputs, so that achieving temporal NNS action segmentation. 
% Challenges still exist. Generally speaking, most of the current global processing techniques are designed for action segmentation for tasks with longer-term temporal dependencies---NNS is subtle as a movement but its detection may not require long-distance temporal processing, which means we need to involve an attention mechanism to address the features form short and sporadic NNS burst. 

% In order to better serve the needs of the subsequent global algorithm, we can modify our local classifier to operate on short clips of mixed character (rather than the pure NNS or purely non-NNS clips used in our study) and produce intelligible results (such as a binary signal indicating whether or not the majority of the clip features NNS behavior, or a binary signal indicating whether the middle frame occurs during NNS behavior, or a continuous signal indicating the proportion of NNS action in the clip). 

% Once our local NNS detection signal has been sufficiently improved and generalized, we will turn to the task of globally piecing together these signals to obtain final event timestamp outputs. The available techniques for doing so range from simple non-maximum suppression filtration, to sequential long short-term memory networks, to higher level hidden Markov models \cite{du2022fast,lea2016segmental,farnoosh2021bayesian}. %[ADD CITATIONS.] 
% Some approaches like the multi-stage temporal convolutional network (MS-TCN) \cite{farha2019ms} even combine the local and global stages into one. Generally speaking, higher order networks are designed for action segmentation for tasks with longer-term temporal dependencies---NNS is subtle as a movement but its detection may not require long-distance temporal processing, and simpler models may suffice. 

\section{Experiments, Results, and Ablation Study}
\label{sec:results}

\subsection{NNS Dataset Creation}
\label{sec:data-preparation}

Our primary dataset is the \textbf{NNS clinical in-crib dataset}, consisting of 183 h of baby monitor footage collected from 19 infants during overnight sleep sessions by our clinical neurodevelopment team, with Institutional Review Board (IRB \#17-08-19) approval. Videos were shot in-crib with the baby monitors set up by caregivers, under low-light triggering the monochromatic infrared mode. Tens of thousands of timestamps for NNS and pacifier activity were placed, by two trained behavioral coders per video. For NNS, the definition of an event segment was taken to be an NNS \textit{burst}: a sequence of sucks with ${<}1$ s gaps between. We restrict our subsequent study to NNS during pacifier use, which was annotated more consistently. Cohen $\kappa$ annotator agreement of NNS events during pacifier use (among 10 pacifier-using infants) averaged 0.83 in 10 s incidence windows, indicating strong agreement by behavioral coding standards, but we performed further manual selection to increase precision for machine learning use, as detailed below\footnote{See the Supp. \figref{setup} and Supp. \figref{annotSoft} for more on the creation of the NNS clinical in-crib dataset, and Supp. \tabref{nns-stats} for full Cohen $\kappa$ scores, biographical data, and NNS and pacifier event statistics.}. We also created a smaller but publicly available \textbf{NNS in-the-wild dataset} of 14 YouTube videos featuring infants in natural conditions, with lengths ranging from 1 to 30 minutes, and similar annotations.

From each of these two datasets, we extracted 2.5 s clips for the classification task and 60 s clips for the segmentation task. In the NNS clinical in-crib dataset, we restricted our attention to six infant videos containing enough NNS activity during pacifier use for meaningful clip extraction. From each of these, we randomly drew up to 80 2.5 s clips consisting entirely of NNS activity and 80 2.5 s clips containing non-NNS activity for classification, for a total of 1,600; and five 60 s clips featuring transitions between NNS and non-NNS activity for segmentation, for a total of 30; redrawing if available when annotations were not sufficiently accurate. In the NNS in-the-wild dataset, we restricted to five infants exhibiting sufficient NNS activity during pacifier use, from which we drew 38 2.5 s clips each of NNS and no NNS activity for classification, for a total of 76; and from two to 26 60 s clips of mixed activity from each infant for segmentation, for a total of 39; again redrawing in cases of poor annotations. 


% \subsection{Data Preparation for Classification [ADD SEGMENTATION]}
% \label{sec:data-preparation}

% In addition to focusing on six subjects featuring high amount of NNS action during pacifier usage (and thus high inter-rater reliability), we take practical steps to prepare our data for deep learning classification. We first trimmed the long overnight recordings from each subject into 2.5 second video clips in which only contains   NNS or non-NNS actions within pacifier events. Then 200 NNS and 200 non-NNS were randomly selected from the trimmed video clips with manually double check for mislabeled or mix-action clips. (Since the pacifier events are manually and reliably annotated, pacifier detector is independently evaluated and not involved in the evaluation of the following action recognition model.) While manually cleaning the data, the clips with significant background noise such as dramatically head movement, in which the NNS actions are ambiguous, were labeled as challenge dataset, and the rest clips with still head position are labeled as the normal dataset. The following experiments were performed on the normal dataset, while evaluation of the challenge dataset will be performed in the future work with background denoising method involved.


%\subsection{NNS Recognition Implementation}


\subsection{NNS Recognition Implementation and Results}
\label{sec:nnsResults}
% \tblmainresults
For the spatiotemporal core of our NNS action recognition, we experimented with four configurations of 2D convolutional networks, a 1-layer CNN, ResNet18, ResNet50, and ResNet101 \cite{he2016deep}; and three configurations of sequential networks, an LSTM, a bi-directional LSTM, and a transformer model \cite{vaswani2017attention}. The models were trained for 20 epochs under a learning rate of 0.0001 using PyTorch 1.8.1 with CUDA 10.2, and the best model chosen based on a held-out validation set.

We trained and tested this method with NNS clinical in-crib data from six infant subjects under a subject-wise leave-one-out cross-validation paradigm. Action recognition accuracies under are reported on the top left of \tabref{resultCombo}. The ResNet18-LSTM configuration performed best, achieving $94.9\%$ average accuracy over six infants using optical flow input. The strong performance (${\geq}85.2\%$) across all configurations indicates the viability of the overall method.
% The action recognition performance of the proposed model is shown in \tabref{resultCombo}. We first perform a leave-one-out cross-validation evaluation on six infants from our NNS clinical in-crib dataset. The model is saved for each infant when the validation accuracy reaches its maximum during training. Then, the test accuracy is evaluated for the corresponding test fold. The best-performed model formed with ResNet18 and LSTM reaches $94.9\%$ average accuracy over 6 infants while using optical flow input. 
We also evaluated a model trained on all six infants from the clinical in-crib dataset on the independent in-the-wild dataset. Results on the bottom left of \tabref{resultCombo} again show strong cross-configuration performance (${\geq}79.5\%$), with ResNet101-Transformer reaching $92.3\%$, demonstrating strong generalizability of the method.
% Then, to evaluate the models' performance for the actual in-the-wild data which have a domain gap from our recordings in the NNS clinical in-crib dataset, we applied the same preprocessing method to the NNS in-the-wild dataset and used them to evaluate the model trained on all data from the NNS clinical in-crib dataset. Table \ref{tab:resultCombo} shows the results of the proposed model under different CNN and LSTM structures. As the table shows, our proposed method could reach over $79.5\%$ for all kinds of CNN and LSTM structures, with the highest performance reaching $92.3\%$ by ResNet101 and Transformer. The results demonstrate that our model trained on the clinical in-crib dataset could have reliable performance for upcoming unseen in-the-wild data.

\tblresultCombo


% To explore the influence of convolutional network depth for spatial convolution, we tested with four structures: a shallow one-layer learnable convolution neural network, and pre-trained ResNet18, ResNet50, and ResNet101 models representing deeper networks.
% As shown in \tabref{resultCombo}, 

As expected, models trained on the clinical in-crib data test worse on the independent in-the-wild data. But interestingly, models with the smaller ResNet18 network suffered steep drop-offs in performance when tested on the in-the-wild data, while models based on the complex ResNet101 fared better under the domain shift. Beyond this, it is hard to identify clear trends between configurations or capacities and performance.

% To explore the influence of CNN network depth for spatial convolution, we utilized four CNN structures: a shallow one-layer learnable convolution network and pre-trained ResNet18, ResNet50, and ResNet101 models for middle-to-deep CNNs. As shown in Table \ref{tab:resultCombo}, all models achieved over $85\%$ accuracy on the NNS clinical in-crib dataset, demonstrating the feasibility of our proposed CNN-LSTM model with optical flow input. ResNet18 had the highest average accuracy ($93.0\%$) during training with the clinical in-crib dataset, while ResNet101 performed best ($87.4\%$ average over all LSTMs) on the in-the-wild dataset and had acceptable performance ($88.6\%$ average over all LSTMs) during training, indicating the superiority of deep CNN networks such as ResNet101. Additionally, we tested different structures of dynamic event classifiers, including bi-directional LSTM and transformer models. The LSTM model achieved the highest score ($94.9\%$) on the in-crib dataset, but there was no significant difference in performance among the listed CNNs and dynamic event classifiers. All models achieved over $85\%$ accuracy when tested on the in-crib dataset and $79.5\%$ on the in-the-wild dataset, demonstrating the generalizability of our proposed action recognition model.



% \subsubsection{CNN Depth}
% To explore the influence of the depth of CNN networks for spatial convolution, four CNN structures were utilized: a one-layer learnable convolution network to represent shallow CNN structure; the pre-trained ResNet18, ResNet50, and ResNet101 models for the middle to deep CNN structure. As the results are shown in Table \ref{tab:resultCombo}, all models with different CNNs were successfully learned and reached over $85.0\%$ accuracy on the NNS clinical in-crib dataset, which demonstrates the feasibility of the proposed CNN-LSTM model with optical flow input. The ResNet18 reaches the highest average accuracy ($93.0\%$) over all kinds of LSTM-based models during training with the clinical in-crib dataset. Yet, ResNet101 performs best ($87.4\%$ averaged over all LSTMs) while testing the in-the-wild dataset and has acceptable performance ($88.6\%$ averaged over all LSTMs) while training, which shows that deep CNN network such as ResNet101 is more robust compared to a shallow CNN network.

% \subsubsection{LSTM Models}
% We also explore different structures of LSTM-based dynamic event classifiers, including LSTM, bi-directional LSTM, and transformer. The bi-directional has the same layer settings as the LSTM model, but the forward and backward outputs of the last node are concatenated before inputting into the fully connected layer. The transformer model is formed with 8 heads attention models and the feedforward network with 64 nodes. Bi-directional LSTM is the most robust one since it reaches the highest average accuracy over all CNN models both on the clinical in-crib dataset and on the in-the-wild dataset.  

\subsubsection{Optical Flow Ablation}
Performance of all models with raw RGB input replacing optical flow frames can be found on the right side of \tabref{resultCombo}. The results are weak and close to random guessing, demonstrating the critical role played by optical flow in detecting the subtle NNS signal. This can also be seen clearly in the sample optical flow frames visualized in Supp. \figref{opFlow}.

% To evaluate the influence of the optical flow data conversion in the model, we switched the input videos from the optical flow videos in HSV space to the raw RGB videos. The left part of Table \ref{tab:resultCombo} shows the results of the proposed model under different CNN and LSTM structures while taking the raw RGB input videos. As the results show, all models had poor performance with RGB input. The CNN-LSTM model couldn't successfully learn separable features from the raw RGB video input, which demonstrates that optical flow preprocessing is necessary.


\subsection{NNS Segmentation Results}

Adopting the best ResNet18-LSTM recognition model, we tested the three configurations of the derived segmentation method on the 60 s mixed activity clips, under the same leave-one-out cross-validation paradigm on the six infants. In addition to the default classifier threshold of 0.5 used by our recognition model, we tested a 0.9 threshold to coax higher precision, as motivated in \secref{intro}. We use the standard evaluation metrics of average precision $\text{AP}_t$ and average recall $\text{AR}_t$ based on hits and misses defined by an intersection-over-union (IoU) with threshold $t$, across common thresholds $t\in\left\{0.1, 0.3, 0.5\right\}$\footnote{We follow the definitions from \cite{idrees_thumos_2017},  except that multiple predictions of a single ground truth event are resolved by the highest IoU, rather than confidence score.}. Averages are taken with subjects given equal weight, and results tabulated in \tabref{tblsegNew}.

The metrics reveal strong performance from all methods and both confidence thresholds on both test sets. Generally, as expected, setting a higher confidence threshold or employing the more tempered tiled or smoothed aggregation methods favours precision, while lowering the confidence threshold or employing the more responsive sliding aggregation method favours recall. The results are excellent at the IoU threshold of 0.1 but degrade as the threshold is raised, suggesting that while these methods can readily perceive NNS behavior, they are still limited by the underlying ground truth annotator accuracy. The consistency of the performance of the model across both cross-validation testing in the clinical in-crib dataset and the independent testing on the NNS in-the-wild dataset suggests strong generalizability. \figref{segmentation-visualization} visualizes predictions (and underlying confidence scores) of the sliding model configuration with a confidence threshold of 0.9, highlighting the excellent precision characteristics and illustrating the overall challenges of the detection problem. 


% To evaluate the proposed action segmentation pipeline, we prepared XXX 1-minute long video with multiple NNS and non-NNS actions mixture as the test data for segmentation inference. The input window of the pre-trained action recognition module is 2 seconds, which slides through a given 1-minute video with non or 1-second overlap. The window-based ground truth label is generated via aggregating frame labels within the window into a single label which is decided by the dominant action with over 50\% frames in the window.
% % which is decided by setting a threshold of 50\% NNS frames in a second-long window to classify that second as NNS.  
% We consider the NNS as an action of interest and the rest of the clips (non-NNS) as background. A successful NNS localization occurs when the model detects the presence of the NNS within a one-second margin of error.
% As Table  shows while our NNS localization in the videos from our collection archives X accuracy, we get Z of NNS data localized in the in-wild dataset.


\tblsegNew
\segVis


% The classification of NNS in short clips can be considered the first, local test case along the way to achieve temporal segmentation of NNS action from long videos. Conceptually, many temporal segmentation approaches split the problem up into a local task of extracting predictions or features from single frames or windows, and a global task of combining the outputs over all of the frames or (possibly overlapping) windows and producing final event timestamps, with a separate machine learning algorithms handling each part. Our current NNS classification algorithm represents a major step towards solving the local task. We prepared XX 1-minute video clips that contain at least one action transition from NNS to Non-NNS (or vice versa). Each 1-min video is uniformly segmented into a series of continuous 2-sec video clips which inputs into the pre-trained model for inference and prediction. 

% \subsection{Ablation Studies}
% We perform ablation studies on some aspects of our NNS recognition model. Our NNS segmentation model is based on the best recognition model, and we have already reported quantiative results on all of its configurations. 
% We performed ablation studies from three aspects (CNN depth, optical flow algorithm choice, and input type) to fully discuss the model design.

% \subsubsection{CNN depth} 

% To explore the influence of the depth of CNN networks for spatial convolution, three CNN structures were utilized: a one-layer learnable convolution network to represent shallow CNN structure; the pre-trained ResNet18 model for the middle-level CNN structure; the pre-trained ResNet101 model for the deep CNN structure. As the results are shown in Table \ref{tab:resultCombo}, all models with different CNNs were successfully learned and reached over $77.0\%$ test accuracy on the test subjects that were isolated from the training dataset, which demonstrates the feasibility of the proposed CNN-LSTM model with optical flow input. The highest test accuracy was reached by the ResNet18 CNN model with $88.1\%$ test accuracy.
% \tblCNNablation

% \tblRawVideoAblation

% \tblIntheWildVideoAblation

% \subsubsection{Input Type} 
% To evaluate the influence of the optical flow data conversion in the model, we switched the input videos from the optical flow videos in HSV space to the raw RGB videos. Table \ref{tab:resultCombo} shows the results of the proposed model under different CNN structures while taking the raw RGB input videos. As the results show, all models had poor performance with RGB input. The CNN-LSTM model couldn't successfully learn separable features from the raw RGB video input, which demonstrates that optical flow preprocessing is necessary.


% \subsubsection{NNS in-the-wild dataset evaluation.} To evaluate the models' performance while processing the actual in-the-wild data, we applied the same preprocessing method to the NNS in-the-wild dataset and used them to evaluate the model trained on the NNS clinical in-crib dataset. Table \ref{tab: resultCombo} shows the results of the proposed model under different CNN structures. As the results show, 
% all models had poor performance with RGB input. The CNN-LSTM model couldn't successfully learn separable features from the raw RGB video input, which demonstrates that optical flow preprocessing is necessary.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
We present our novel computer vision method for detection of non-nutritive sucking from videos, with a spatiotemporal action recognition model for classifying short video clips and an action segmentation model for determining event timestamps within longer videos. Our work is grounded in our methodological collection and annotation of infant video data, from both in-crib and in-the-wild videos. We use domain-specific techniques such as dense optical flow and infant state tracking to detect subtle sucking movements and ameliorate a relative scarcity of data. Our successful segmentation results demonstrate the potential for use in research and clinical applications. Future work could improve upon segmentation accuracy or extend our approach to assess other NNS signal characteristics of interest to neurodevelopmental researchers, including individual suck frequency, strength, duration, and general temporal pattern.
% The successful segmentation results show that our algorithms are ready for use in research or clinical applications. Future studies could improve upon fine-grained segmentation accuracy ($\text{AP}_{0.5}$ and $\text{AR}_{0.5}$) by increasing manual annotation precision, or by increasing the dataset size to enable direct training of a video segmentation model. Our method could also be used as a starting point for a vision-based assessment of other NNS signal characteristics of interest to neurodevelopmental researchers, such as individual suck frequency, strength, and duration, and general temporal pattern.



%\subsection{Acknowledgements} MathWorks Microgrant.



\bibliographystyle{splncs04}
\bibliography{mybib}


\supp
\end{document}
