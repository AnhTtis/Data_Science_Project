\newcommand{\supp}{
\newpage
\onecolumn
\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \setcounter{equation}{0}
        \renewcommand{\theequation}{S\arabic{equation}}
        \setcounter{section}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
}
%\maketitle
\newcommand{\independent}{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT##1##2{\mathrel{\rlap{$##1##2$}\mkern2mu{##1##2}}}
\renewcommand\thesection{\Alph{section}}
\beginsupplement
\newcommand{\MYhref}[3][blue]{\href{##2}{\color{##1}{##3}}}

\section{Supplementary Material} 

\setup
\annotSoft
\tblnnsstats
\opFlow



% \subsection{Pacifier Detection and End-to-end NNS Detection Pipeline}
% \label{sec:pacifier-detection}

% % Below in Table \ref{tab:pacifierresults} are the results on the pacifier detection using transfer learning on YOLO-v5 object detection model \cite{yolov5_2021}. We use the mAP (Mean average Precision) metric to evaluate this object detection model. 
% % AP defines the mean average precision over different thresholds of IOU ranging from 0.5 to 0.95 in the steps of 0.5, while the $AP_{0.5}$ determines the average precision at IOU threshold of 0.5. 
% % The above precision is calculated using the below formula: 
% % \begin{equation*}
% %     Precision = \frac{TP}{TP + FP}
% % \end{equation*}
% % In the above equation, TP signifies the True Positives where the IOU between the predicted bounding box and the actual bounding box $\geq$ threshold

% % \begin{equation*}
% %     mAP = \frac{1}{n} \sum_{k=1}^{k=n} AP_{k}
% % \end{equation*}

% % {\hspace{1ex} \it AP = average Precision of class k at different IOU thresholds and n = number of classes}

% % \tblpacifierresults

% To expand our NNS detection to videos with mixed pacifier use, rather than constant pacifier use as required in the main paper, we briefly explore the task of image-based pacifier detection, which could be applied video frames and smoothed to detect periods of pacifier presence. We developed a dataset of 1,622 images and video frames of infants from internet images and YouTube, annotated for pacifier bounding boxes, and then used it to train and test transfer learning with a YOLO-v5 object detection model \cite{yolov5_2021}. Our test results under a 65-20-15\% train-validation-test split yielded an average precision (AP) of $52.8\%$ for the pacifier class alone at intersection over union (IoU) threshold 0.5. The mean average precision (mAP) over IoUs ranging from 0.50 to 0.95, is $34.9\%$ for the pacifier class. YOLO-v5 achives a mAP of $37.2\%$ in the COCO dataset \cite{lin2014microsoft}, so pacifier detection is strong but slightly below the level of detection of objects in general. 



 % The original YOLO-v5 model which was trained on the COCO dataset reached a mAP of $37.2\%$ through the same range of IoU thresholds.
% Our transfer learnt model performed close to the original model, which demonstrates the feasibility of our proposed pacifier detector.
% \section{OVERFLOW---TO BE DELETED---IGNORE}

% \subsection{NNS Clinical In-Crib Dataset}

% \tblcohenkappa

% To demonstrate the reliability of our annotations, we provide Cohen $\kappa$ inter-rater reliability scores for each pair of annotators' behavioral coding, per infant video. Since annotators may not agree on the number of events in any given period, Cohen $\kappa$s cannot be computed directly on the timestamp data. Instead, we adhere to common practice from behavioral coding in psychology and convert each coder's annotations for a single event type (NNS or pacifier) in a video to a binary time sequence representing uniform windows in the runtime, with with 1 assigned to windows which overlap temporally with at least one event of that type, and 0 assigned to the remaining windows. We consider both the fine-grained windows of 0.1s, which contain one video frame each, as well as the coarser windows of 10s, which is more in line with conventions in behavioral coding, given the imprecision and differences in interpretation built into human behavioral assessments. The Cohen $\kappa$ scores\footnote{The Cohen $\kappa$ agreement between two raters' binary classifications on a set is defined as $\kappa:=\frac{p_\text{o}-p_\text{e}}{1-p_\text{e}}$, where $p_\text{0}$ is the observed portion of agreements in the set and $p_\text{e}$ is the estimated probability of chance agreement, itself defined by $p_\text{e}:=p_0 p_1 + (1-p_0)(1-p_1)$, with $p_0$ and $p_1$ being the positive assignment rate for each respective rater. It is intended to measure the level of agreement between two raters' assessments while taking into account chance agreements. We adopt the following suggested interpretations of agreement strength based on $\kappa$ score from \cite{mchugh_interrater_2012}: 0-0.2 means no agreement, 0.21-0.39 minimal, 0.40-0.59 weak, 0.60-0.79 moderate, 0.80-0.90 strong, and >0.90 almost perfect.} for the events considered as time sequence over both 0.1s and 10s windows, aggregated across all infants in our training and test data, is reported in \tabref{cohen-kappa}. In addition to raw NNS and pacifier events, the table also shows agreement for the derived annotation of NNS events occurring only during pacifier events. Such NNS action is far more regular and reliably codable, and hence we restrict our video segmentation efforts to those events alone. The interpretation of $\kappa$ scores is subjective, but the levels achieved by the pacifier annotations would typically be characterized as indicating ``near perfect'' agreement; the NNS-with-pacifier annotation scores could be considered ``weak'' or ``moderate'' agreement under the harsh 0.1s intervals, and ``strong'' or ``almost perfect'' under the 10s intervals. Given the inherent difficulty of NNS annotation, the sheer amount of runtime of the video data, and our subsequent success in using the data for the segmentation task, we believe these annotation efforts represent a hard-earned success. 

% \subsection{NNS Clinical In-Crib Dataset}

% See \figref{anotSamples} for a visualization of 30-second samples from across eight infant subjects, highlighting the NNS action from pairs of annotators, largely during pacifier usage.
% \anotSamples

% \subsection{Ablation Studies}
% \figOpticalFlow

% \textbf{Optical flow algorithms:} Then we compared three other wildly used optical flow algorithms (Franeback \cite{farneback2003two}, RAFT \cite{teed2020raft}, TV-$L^{1}$ \cite{zach2007duality}) with the Coarse2Fine method that we used in the data preprocessing stage. The comparison is shown in Figure \ref{fig:Ab2}. Since our infant video recordings are collected from a commercial baby monitor that highly compressed the video for storage, which cause the problem of low resolution and jittering of frames. These challenge introduced noise to the other optical flow methods. Among all four methods, the Coarse2Fine method we used has the lowest background noise, which releases the CNN-LSTM model from extra-denoising learning burden.
% % \figOpticalFlow

% \subsection{Future Work}
% Our future work focuses on two folds: first, decrease the input noise introduced by the low quality of the input videos and the unrelated actions such as head pose changes, so that the current action recognition model could be more robust; second, modify the current action recognition model into spatial-temporal feature extractor to enable the temporal action segmentation of NNS actions.

% \subsubsection{Denoising}
% We can improve the performance of the NNS action classifier by leveling up our in-crib infant state tracking, especially by replacing our head tracker with a mouth tracker specifically designed for this domain. Our preliminary mouth detection pipeline starts with the RetinaFace \cite{retinaface2019} network that provides two mouth co-ordinates at the lip corners of the initial frame. These co-ordinates are then extrapolated to draw the bounding box around the mouth region. Then, the OpenCV Tracker CSRT is applied to track the facial bounding box through out the input video. While tracking, the facial area will be re-initialized if the IOU between the tracking result and the current RetinaFace result is lower than a certain threshold. \ref{fig:mouthbb} shows the results from the mouth bounding box pipeline, this cropped region will be the new input to our proposed Optical Flow + CNN-LSTM model. 
% \figmouthbb
 
}