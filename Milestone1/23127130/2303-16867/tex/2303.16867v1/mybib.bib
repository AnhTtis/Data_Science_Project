@inproceedings{karaman2014fast,
  title={Fast saliency based pooling of fisher encoded dense trajectories},
  author={Karaman, Svebor and Seidenari, Lorenzo and Del Bimbo, Alberto},
  booktitle={ECCV THUMOS Workshop},
  volume={1},
  number={2},
  pages={5},
  year={2014}
}
@inproceedings{rohrbach2012database,
  title={A database for fine grained activity detection of cooking activities},
  author={Rohrbach, Marcus and Amin, Sikandar and Andriluka, Mykhaylo and Schiele, Bernt},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={1194--1201},
  year={2012},
  organization={IEEE}
}
@inproceedings{cheng2014temporal,
  title={Temporal sequence modeling for video event detection},
  author={Cheng, Yu and Fan, Quanfu and Pankanti, Sharath and Choudhary, Alok},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2227--2234},
  year={2014}
}
@inproceedings{pirsiavash2014parsing,
  title={Parsing videos of actions with segmental grammars},
  author={Pirsiavash, Hamed and Ramanan, Deva},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={612--619},
  year={2014}
}
@inproceedings{tao2013surgical,
  title={Surgical gesture segmentation and recognition},
  author={Tao, Lingling and Zappella, Luca and Hager, Gregory D and Vidal, Ren{\'e}},
  booktitle={International conference on medical image computing and computer-assisted intervention},
  pages={339--346},
  year={2013},
  organization={Springer}
}
@article{shi2011human,
  title={Human action segmentation and recognition using discriminative semi-markov models},
  author={Shi, Qinfeng and Cheng, Li and Wang, Li and Smola, Alex},
  journal={International journal of computer vision},
  volume={93},
  number={1},
  pages={22--32},
  year={2011},
  publisher={Springer}
}
@inproceedings{kuehne2016end,
  title={An end-to-end generative framework for video segmentation and recognition},
  author={Kuehne, Hilde and Gall, Juergen and Serre, Thomas},
  booktitle={2016 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={1--8},
  year={2016},
  organization={IEEE}
}
@inproceedings{lea2017temporal,
  title={Temporal convolutional networks for action segmentation and detection},
  author={Lea, Colin and Flynn, Michael D and Vidal, Rene and Reiter, Austin and Hager, Gregory D},
  booktitle={proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={156--165},
  year={2017}
}
@inproceedings{lei2018temporal,
  title={Temporal deformable residual networks for action segmentation in videos},
  author={Lei, Peng and Todorovic, Sinisa},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6742--6751},
  year={2018}
}
@inproceedings{farha2019ms,
  title={Ms-tcn: Multi-stage temporal convolutional network for action segmentation},
  author={Farha, Yazan Abu and Gall, Jurgen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3575--3584},
  year={2019}
}
@inproceedings{huang2020improving,
  title={Improving action segmentation via graph-based temporal reasoning},
  author={Huang, Yifei and Sugano, Yusuke and Sato, Yoichi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={14024--14034},
  year={2020}
}
@article{goldstein2016overall,
  title={Overall postneonatal mortality and rates of SIDS},
  author={Goldstein, Richard D and Trachtenberg, Felicia L and Sens, Mary Ann and Harty, Brian J and Kinney, Hannah C},
  journal={Pediatrics},
  volume={137},
  number={1},
  year={2016},
  publisher={American Academy of Pediatrics}
}
@article{yiallourou2014effects,
  title={The effects of dummy/pacifier use on infant blood pressure and autonomic activity during sleep},
  author={Yiallourou, Stephanie R and Poole, Hannah and Prathivadi, Pallavi and Odoi, Alexsandria and Wong, Flora Y and Horne, Rosemary SC},
  journal={Sleep medicine},
  volume={15},
  number={12},
  pages={1508--1516},
  year={2014},
  publisher={Elsevier}
}
@article{tonkin2007effect,
  title={Effect of pacifier use on mandibular position in preterm infants},
  author={Tonkin, Shirley L and Lui, Dana and McIntosh, Christine G and Rowley, Simon and Knight, David B and Gunn, Alistair J},
  journal={Acta Paediatrica},
  volume={96},
  number={10},
  pages={1433--1436},
  year={2007},
  publisher={Wiley Online Library}
}
@article{carlin2017risk,
  title={Risk factors, protective factors, and current recommendations to reduce sudden infant death syndrome: a review},
  author={Carlin, Rebecca F and Moon, Rachel Y},
  journal={JAMA pediatrics},
  volume={171},
  number={2},
  pages={175--180},
  year={2017},
  publisher={American Medical Association}
}
@article{franco2004pacifier,
  title={Pacifier use modifies infant's cardiac autonomic controls during sleep},
  author={Franco, Patricia and Chabanski, Sophie and Scaillet, Sonia and Groswasser, Jos{\'e} and Kahn, Andr{\'e}},
  journal={Early human development},
  volume={77},
  number={1-2},
  pages={99--108},
  year={2004},
  publisher={Elsevier}
}
@incollection{humphrey1970development,
  title={The development of human fetal activity and its relation to postnatal behavior},
  author={Humphrey, Tryphena},
  booktitle={Advances in child development and behavior},
  volume={5},
  pages={1--57},
  year={1970},
  publisher={Elsevier}
}
@inproceedings{huang2019infant,
  title={Infant Contact-less Non-Nutritive Sucking Pattern Quantification via Facial Gesture Analysis.},
  author={Huang, Xiaofei and Martens, Alaina and Zimmerman, Emily and Ostadabbas, Sarah},
  booktitle={CVPR Workshops},
  year={2019}
}
@inproceedings{huber2016multiresolution,
  title={A multiresolution 3d morphable face model and fitting framework},
  author={Huber, Patrik and Hu, Guosheng and Tena, Rafael and Mortazavian, Pouria and Koppen, P and Christmas, William J and Ratsch, Matthias and Kittler, Josef},
  booktitle={Proceedings of the 11th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
  year={2016},
  organization={University of Surrey}
}


@inproceedings{retinaface2019,
  title={Retinaface: Single-shot multi-level face localisation in the wild},
  author={Deng, Jiankang and Guo, Jia and Ververas, Evangelos and Kotsia, Irene and Zafeiriou, Stefanos},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5203--5212},
  year={2020}
}

@article{yolov5_2021,
  doi = {10.48550/ARXIV.2108.11539},
  
  url = {https://arxiv.org/abs/2108.11539},
  
  author = {Zhu, Xingkui and Lyu, Shuchang and Wang, Xu and Zhao, Qi},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.10; I.4.8},
  
  title = {TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-captured Scenarios},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{dutta2019vgg,
  author = {Dutta, Abhishek and Zisserman, Andrew},
  title = {The {VIA} Annotation Software for Images, Audio and Video},
  booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
  series = {MM '19},
  year = {2019},
  isbn = {978-1-4503-6889-6/19/10},
  location = {Nice, France},
  numpages = {4},
  url = {https://doi.org/10.1145/3343031.3350535},
  doi = {10.1145/3343031.3350535},
  publisher = {ACM},
  address = {New York, NY, USA},
}


@article{martens_changes_2020,
	title = {Changes in non-nutritive suck between 3 and 12 months},
	volume = {149},
	issn = {03783782},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378378220303807},
	doi = {10.1016/j.earlhumdev.2020.105141},
	abstract = {Background: Non-nutritive suck (NNS) is one piece of the complex oral feeding process, yet there is paucity on how it develops throughout the ﬁrst year of life. Aims: To determine changes in infant NNS between 3 and 12 months of age. Study design: Twenty-six full-term infants (65\% male) completed this study. All infants were oﬀered our custom research paciﬁer to attain a quantitative analysis of their suck pattern. Based on quantitative analyses of NNS cycle count, the best 2 min of infants' suck were selected and analyzed. Outcome measures: NNS duration, amplitude, cycles/burst, frequency, cycles, and bursts.
Results: NNS duration, bursts, cycles/burst and cycles signiﬁcantly decreased from 3 to 12 months, yet amplitude signiﬁcantly increased over the same time period. Additionally, no signiﬁcant diﬀerences were evident for NNS frequency. Three-month-old infants produced a median of 4.50 suck bursts per minute that contained 9.60 cycles/burst, resulting in a burst duration of 4.74 s. The median NNS frequency was 2.09 Hz, with an average amplitude of 14.05 cmH20. Twelve-month-old infants produced a median of 2.50 suck bursts that contained 3.75 cycles/burst, resulting in a burst duration of 1.67 s. The median NNS frequency was 2.11 Hz with an amplitude of 19.75 cmH20.
Conclusion: Full-term infants signiﬁcantly change their NNS duration, amplitude, burst number, cycles/burst and cycle number with no signiﬁcant changes present in NNS frequency between 3 and 12 months. Knowledge of NNS emergence and maturation during the ﬁrst year of life is imperative for proper NNS assessment so that healthcare professionals can identify delays.},
	language = {en},
	urldate = {2022-06-21},
	journal = {Early Human Development},
	author = {Martens, Alaina and Hines, Morgan and Zimmerman, Emily},
	month = oct,
	year = {2020},
	pages = {105141},
	file = {Martens et al. - 2020 - Changes in non-nutritive suck between 3 and 12 mon.pdf:/Users/mi.wan/Zotero/storage/MUD97ANP/Martens et al. - 2020 - Changes in non-nutritive suck between 3 and 12 mon.pdf:application/pdf},
}

@article{huang_infant_nodate,
	title = {Infant {Contact}-less {Non}-{Nutritive} {Sucking} {Pattern} {Quantiﬁcation} via {Facial} {Gesture} {Analysis}},
	abstract = {Non-nutritive sucking (NNS) is deﬁned as the sucking action that occurs when a ﬁnger, paciﬁer, or other object is placed in the baby’s mouth, but there is no nutrient delivered. In addition to providing a sense of safety, NNS even can be regarded as an indicator of infant’s central nervous system development. The rich data, such as sucking frequency, the number of cycles, and their amplitude during baby’s non-nutritive sucking is important clue for judging the brain development of infants or preterm infants. Nowadays most researchers are collecting NNS data by using some contact devices such as pressure transducers. However, such invasive contact will have a direct impact on the baby’s natural sucking behavior, resulting in signiﬁcant distortion in the collected data. Therefore, we propose a novel contact-less NNS data acquisition and quantiﬁcation scheme, which leverages the facial landmarks tracking technology to extract the movement signals of baby’s jaw from recorded baby’s sucking video. Since completion of the sucking action requires a large amount of synchronous coordination and neural integration of the facial muscles and the cranial nerves, the facial muscle movement signals accompanying baby’s sucking paciﬁer can indirectly replace the NNS signal. We have evaluated our method on videos collected from several infants during their NNS behaviors and we have achieved the quantiﬁed NNS patterns closely comparable to results from visual inspection as well as contact-based sensor readings.},
	language = {en},
	author = {Huang, Xiaofei and Martens, Alaina and Zimmerman, Emily and Ostadabbas, Sarah},
	pages = {7},
	file = {Huang et al. - Infant Contact-less Non-Nutritive Sucking Pattern .pdf:/Users/mi.wan/Zotero/storage/PGMSJ429/Huang et al. - Infant Contact-less Non-Nutritive Sucking Pattern .pdf:application/pdf},
}

@inproceedings{carreira2017quo,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle={proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6299--6308},
  year={2017}
}



@inproceedings{hara3dcnns,
  author={Kensho Hara and Hirokatsu Kataoka and Yutaka Satoh},
  title={Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={6546--6555},
  year={2018},
}


@article{Hutchinson2020VideoAU,
  title={Video Action Understanding: A Tutorial},
  author={Matthew Hutchinson and Vijay N. Gadepally},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.06647}
}


@inproceedings{twostreamnetworks,
author = {Simonyan, Karen and Zisserman, Andrew},
title = {Two-Stream Convolutional Networks for Action Recognition in Videos},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework.Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {568–576},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}



@inproceedings{ssadpapper,
author = {Lin, Tianwei and Zhao, Xu and Shou, Zheng},
title = {Single Shot Temporal Action Detection},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123343},
doi = {10.1145/3123266.3123343},
abstract = {Temporal action detection is a very important yet challenging problem, since videos in real applications are usually long, untrimmed and contain multiple action instances. This problem requires not only recognizing action categories but also detecting start time and end time of each action instance. Many state-of-the-art methods adopt the "detection by classification" framework: first do proposal, and then classify proposals. The main drawback of this framework is that the boundaries of action instance proposals have been fixed during the classification step. To address this issue, we propose a novel Single Shot Action Detector (SSAD) network based on 1D temporal convolutional layers to skip the proposal generation step via directly detecting action instances in untrimmed video. On pursuit of designing a particular SSAD network that can work effectively for temporal action detection, we empirically search for the best network architecture of SSAD due to lacking existing models that can be directly adopted. Moreover, we investigate into input feature types and fusion strategies to further improve detection accuracy. We conduct extensive experiments on two challenging datasets: THUMOS 2014 and MEXaction2. When setting Intersection-over-Union threshold to 0.5 during evaluation, SSAD significantly outperforms other state-of-the-art systems by increasing mAP from $19.0%$ to $24.6%$ on THUMOS 2014 and from 7.4% to $11.0%$ on MEXaction2.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {988–996},
numpages = {9},
keywords = {temporal action detection, untrimmed video, ssad network},
location = {Mountain View, California, USA},
series = {MM '17}
}


@article{Xia2020ASO,
  title={A Survey on Temporal Action Localization},
  author={Huifen Xia and Yongzhao Zhan},
  journal={IEEE Access},
  year={2020},
  volume={8},
  pages={70477-70487}
}


@INPROCEEDINGS{mossepaper,
  author={Bolme, David S. and Beveridge, J. Ross and Draper, Bruce A. and Lui, Yui Man},
  booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition}, 
  title={Visual object tracking using adaptive correlation filters}, 
  year={2010},
  volume={},
  number={},
  pages={2544-2550},
  doi={10.1109/CVPR.2010.5539960}}
  
  
  @misc{kineticsdataset,
  doi = {10.48550/ARXIV.1705.06950},
  
  url = {https://arxiv.org/abs/1705.06950},
  
  author = {Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and Suleyman, Mustafa and Zisserman, Andrew},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Kinetics Human Action Video Dataset},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{karpathy2014large,
  title={Large-scale video classification with convolutional neural networks},
  author={Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={1725--1732},
  year={2014}
}

@article{ji20123d,
  title={3D convolutional neural networks for human action recognition},
  author={Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={1},
  pages={221--231},
  year={2012},
  publisher={IEEE}
}

@inproceedings{tran2015learning,
  title={Learning spatiotemporal features with 3d convolutional networks},
  author={Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4489--4497},
  year={2015}
}

@article{varol2017long,
  title={Long-term temporal convolutions for action recognition},
  author={Varol, G{\"u}l and Laptev, Ivan and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={6},
  pages={1510--1517},
  year={2017},
  publisher={IEEE}
}

@inproceedings{hara2017learning,
  title={Learning spatio-temporal features with 3d residual networks for action recognition},
  author={Hara, Kensho and Kataoka, Hirokatsu and Satoh, Yutaka},
  booktitle={Proceedings of the IEEE international conference on computer vision workshops},
  pages={3154--3160},
  year={2017}
}

@article{tran2017convnet,
  title={Convnet architecture search for spatiotemporal feature learning},
  author={Tran, Du and Ray, Jamie and Shou, Zheng and Chang, Shih-Fu and Paluri, Manohar},
  journal={arXiv preprint arXiv:1708.05038},
  year={2017}
}

@article{simonyan2014two,
  title={Two-stream convolutional networks for action recognition in videos},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{zhao2017pooling,
  title={Pooling the convolutional layers in deep convnets for video action recognition},
  author={Zhao, Shichao and Liu, Yanbin and Han, Yahong and Hong, Richang and Hu, Qinghua and Tian, Qi},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={28},
  number={8},
  pages={1839--1849},
  year={2017},
  publisher={IEEE}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{yue2015beyond,
  title={Beyond short snippets: Deep networks for video classification},
  author={Yue-Hei Ng, Joe and Hausknecht, Matthew and Vijayanarasimhan, Sudheendra and Vinyals, Oriol and Monga, Rajat and Toderici, George},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4694--4702},
  year={2015}
}

@phdthesis{liu2009beyond,
  title={Beyond pixels: exploring new representations and applications for motion analysis},
  author={Liu, Ce and others},
  year={2009},
  school={Massachusetts Institute of Technology}
}

@article{zimmerman_changes_2020,
	title = {Changes in infant non-nutritive sucking throughout a suck sample at 3-months of age},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0235741},
	doi = {10.1371/journal.pone.0235741},
	abstract = {The goal of this study was to compare how infantsâ non-nutritive suck (NNS) changes throughout a suck sample. Fifty-four full-term infants (57\% male) completed this study at, on average, 3.03 (SD .31) months of age. These infants sucked on our custom research pacifier for approximately five minutes. Infants produced, on average, 14.50 suck bursts during the sample. NNS data was pooled across subjects and breakpoint analyses were completed to determine if there were changes in their NNS patterning. Breakpoints were evident for NNS cycles per burst at burst numbers 18 and 34, and for amplitude (cmH20) at burst numbers 18 and 29. No breakpoints were present for NNS frequency. Infants exhibit changes in their suck physiology across burst number. When assessing suck, developmental specialists should observe more than one suck burst to attain a more valid and appropriate scope of the infantâs suck ability.},
	language = {en},
	number = {7},
	urldate = {2022-10-25},
	journal = {PLOS ONE},
	author = {Zimmerman, Emily and Carpenito, Thomas and Martens, Alaina},
	editor = {SchmÃ¶lzer, Georg M.},
	month = jul,
	year = {2020},
	pages = {e0235741},
	file = {Zimmerman et al. - 2020 - Changes in infant non-nutritive sucking throughout.pdf:/Users/michaelwan/Zotero/storage/GNS78TMG/Zimmerman et al. - 2020 - Changes in infant non-nutritive sucking throughout.pdf:application/pdf},
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@article{mchugh_interrater_2012,
	title = {Interrater reliability: the kappa statistic},
	volume = {22},
	issn = {1330-0962},
	shorttitle = {Interrater reliability},
	abstract = {The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. While there have been a variety of methods to measure interrater reliability, traditionally it was measured as percent agreement, calculated as the number of agreement scores divided by the total number of scores. In 1960, Jacob Cohen critiqued use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen's kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. Like most correlation statistics, the kappa can range from -1 to +1. While the kappa is one of the most commonly used statistics to test interrater reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen's suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent agreement are compared, and levels for both kappa and percent agreement that should be demanded in healthcare studies are suggested.},
	language = {eng},
	number = {3},
	journal = {Biochemia Medica},
	author = {McHugh, Mary L.},
	year = {2012},
	pmid = {23092060},
	pmcid = {PMC3900052},
	keywords = {Data Interpretation, Statistical, Observer Variation, Reproducibility of Results},
	pages = {276--282},
}

@book{lucas1981iterative,
  title={An iterative image registration technique with an application to stereo vision},
  author={Lucas, Bruce D and Kanade, Takeo and others},
  volume={81},
  year={1981},
  publisher={Vancouver}
}
@article{farnoosh2021bayesian,
  title={A Bayesian Dynamical Approach for Human Action Recognition},
  author={Farnoosh, Amirreza and Wang, Zhouping and Zhu, Shaotong and Ostadabbas, Sarah},
  journal={Sensors},
  volume={21},
  number={16},
  pages={5613},
  year={2021},
  publisher={MDPI}
}
@inproceedings{du2022fast,
  title={Fast and Unsupervised Action Boundary Detection for Action Segmentation},
  author={Du, Zexing and Wang, Xue and Zhou, Guoqing and Wang, Qing},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3323--3332},
  year={2022}
}
@inproceedings{lea2016segmental,
  title={Segmental spatiotemporal cnns for fine-grained action segmentation},
  author={Lea, Colin and Reiter, Austin and Vidal, Ren{\'e} and Hager, Gregory D},
  booktitle={European conference on computer vision},
  pages={36--52},
  year={2016},
  organization={Springer}
}
@inproceedings{farneback2003two,
  title={Two-frame motion estimation based on polynomial expansion},
  author={Farneb{\"a}ck, Gunnar},
  booktitle={Scandinavian conference on Image analysis},
  pages={363--370},
  year={2003},
  organization={Springer}
}
@inproceedings{teed2020raft,
  title={Raft: Recurrent all-pairs field transforms for optical flow},
  author={Teed, Zachary and Deng, Jia},
  booktitle={European conference on computer vision},
  pages={402--419},
  year={2020},
  organization={Springer}
}
@inproceedings{zach2007duality,
  title={A duality based approach for realtime tv-l 1 optical flow},
  author={Zach, Christopher and Pock, Thomas and Bischof, Horst},
  booktitle={Joint pattern recognition symposium},
  pages={214--223},
  year={2007},
  organization={Springer}
}

@article{wan_infanface_2022,
	title = {{InfAnFace}: {Bridging} the {Infant}--{Adult} {Domain} {Gap} in {Facial} {Landmark} {Estimation} in the {Wild}},
	language = {en},
	author = {Wan, Michael and Zhu, Shaotong and Luan, Lingfei and Gulati, Prateek and Huang, Xiaofei and Schwartz-Mette, Rebecca and Hayes, Marie and Zimmerman, Emily and Ostadabbas, Sarah},
    journal = {26th International Conference on Pattern Recognition (ICPR)},
    year = {2022},
}
@inproceedings{shi1994good,
  title={Good features to track},
  author={Shi, Jianbo and others},
  booktitle={1994 Proceedings of IEEE conference on computer vision and pattern recognition},
  pages={593--600},
  year={1994},
  organization={IEEE}
}


@article{hauck_pacifiers_2005,
	title = {Do {Pacifiers} {Reduce} the {Risk} of {Sudden} {Infant} {Death} {Syndrome}? {A} {Meta}-analysis},
	volume = {116},
	issn = {0031-4005, 1098-4275},
	shorttitle = {Do {Pacifiers} {Reduce} the {Risk} of {Sudden} {Infant} {Death} {Syndrome}?},
	url = {https://publications.aap.org/pediatrics/article/116/5/e716/68083/Do-Pacifiers-Reduce-the-Risk-of-Sudden-Infant},
	doi = {10.1542/peds.2004-2631},
	abstract = {Objective. Pacifier use has been reported to be associated with a reduced risk of sudden infant death syndrome (SIDS), but most countries around the world, including the United States, have been reluctant to recommend the use of pacifiers because of concerns about possible adverse effects. This meta-analysis was undertaken to quantify and evaluate the protective effect of pacifiers against SIDS and to make a recommendation on the use of pacifiers to prevent SIDS.
            Methods. We searched the Medline database (January 1966 to May 2004) to collect data on pacifier use and its association with SIDS, morbidity, or other adverse effects. The search strategy included published articles in English with the Medical Subject Headings terms “sudden infant death syndrome” and “pacifier” and the keywords “dummy” and “soother.” Combining searches resulted in 384 abstracts, which were all read and evaluated for inclusion. For the meta-analysis, articles with data on the relationship between pacifier use and SIDS risk were limited to published original case-control studies, because no prospective observational reports were found; 9 articles met these criteria. Two independent reviewers evaluated each study on the basis of the 6 criteria developed by the American Academy of Pediatrics Task Force on Infant Positioning and SIDS; in cases of disagreement, a third reviewer evaluated the study, and a consensus opinion was reached. We developed a script to calculate the summary odds ratio (SOR) by using the reported ORs and respective confidence intervals (CI) to weight the ORs. We then pooled them together to compute the SOR. We performed the Breslow-Day test for homogeneity of ORs, Cochran-Mantel-Haenszel test for the null hypothesis of no effect (OR = 1), and the Mantel-Haenszel common OR estimate. The consistency of findings was evaluated and the overall potential benefits of pacifier use were weighed against the potential risks. Our recommendation is based on the taxonomy of the 5-point (A–E) scale adopted by the US Preventive Services Task Force.
            Results. Seven studies were included in the meta-analysis. The SOR calculated for usual pacifier use (with univariate ORs) is 0.90 (95\% confidence interval [CI]: 0.79–1.03) and 0.71 (95\% CI: 0.59–0.85) with multivariate ORs. For pacifier use during last sleep, the SORs calculated using univariate and multivariate ORs are 0.47 (95\% CI: 0.40–0.55) and 0.39 (95\% CI: 0.31–0.50), respectively.
            Conclusions. Published case-control studies demonstrate a significant reduced risk of SIDS with pacifier use, particularly when placed for sleep. Encouraging pacifier use is likely to be beneficial on a population-wide basis: 1 SIDS death could be prevented for every 2733 (95\% CI: 2416–3334) infants who use a pacifier when placed for sleep (number needed to treat), based on the US SIDS rate and the last-sleep multivariate SOR resulting from this analysis. Therefore, we recommend that pacifiers be offered to infants as a potential method to reduce the risk of SIDS. The pacifier should be offered to the infant when being placed for all sleep episodes, including daytime naps and nighttime sleeps. This is a US Preventive Services Task Force level B strength of recommendation based on the consistency of findings and the likelihood that the beneficial effects will outweigh any potential negative effects. In consideration of potential adverse effects, we recommend pacifier use for infants up to 1 year of age, which includes the peak ages for SIDS risk and the period in which the infant's need for sucking is highest. For breastfed infants, pacifiers should be introduced after breastfeeding has been well established.},
	language = {en},
	number = {5},
	urldate = {2023-01-31},
	journal = {Pediatrics},
	author = {Hauck, Fern R. and Omojokun, Olanrewaju O. and Siadaty, Mir S.},
	month = nov,
	year = {2005},
	pages = {e716--e723},
}


@article{psaila_infant_2017,
	title = {Infant pacifiers for reduction in risk of sudden infant death syndrome},
	volume = {2017},
	issn = {14651858},
	url = {http://doi.wiley.com/10.1002/14651858.CD011147.pub2},
	doi = {10.1002/14651858.CD011147.pub2},
	language = {en},
	number = {4},
	urldate = {2023-01-31},
	journal = {Cochrane Database of Systematic Reviews},
	author = {Psaila, Kim and Foster, Jann P and Pulbrook, Neil and Jeffery, Heather E},
	editor = {{Cochrane Neonatal Group}},
	month = apr,
	year = {2017},
	file = {Full Text:/Users/michaelwan/Zotero/storage/AGST4E5T/Psaila et al. - 2017 - Infant pacifiers for reduction in risk of sudden i.pdf:application/pdf},
}


@article{zavala_abed_how_2020,
	title = {How might non nutritional sucking protect from sudden infant death syndrome},
	volume = {143},
	issn = {03069877},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306987720307386},
	doi = {10.1016/j.mehy.2020.109868},
	language = {en},
	urldate = {2023-01-31},
	journal = {Medical Hypotheses},
	author = {Zavala Abed, Bruno and Oneto, Sabrina and Abreu, Alexandre R. and Chediak, Alejandro D.},
	month = oct,
	year = {2020},
	pages = {109868},
}


@article{medoff-cooper_neonatal_1995,
	title = {Neonatal {Sucking} {Behaviors}},
	volume = {27},
	issn = {07435150},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1547-5069.1995.tb00858.x},
	doi = {10.1111/j.1547-5069.1995.tb00858.x},
	language = {en},
	number = {3},
	urldate = {2023-01-31},
	journal = {Image: the Journal of Nursing Scholarship},
	author = {Medoff-Cooper, Barbara and Ray, Wendy},
	month = sep,
	year = {1995},
	pages = {195--200},
}


@misc{ding_temporal_2022,
	title = {Temporal {Action} {Segmentation}: {An} {Analysis} of {Modern} {Technique}},
	shorttitle = {Temporal {Action} {Segmentation}},
	url = {http://arxiv.org/abs/2210.10352},
	abstract = {Temporal action segmentation from videos aims at the dense labeling of video frames with multiple action classes in minutes-long videos. Categorized as a long-range video understanding task, researchers have proposed an extended collection of methods and examined their performance using various benchmarks. Despite the rapid development of action segmentation techniques in recent years, there has been no systematic survey in such ﬁelds. To this end, in this survey, we analyse and summarize the main contributions and trends for this task. Speciﬁcally, we ﬁrst examine the task deﬁnition, common benchmarks, types of supervision and popular evaluation measures. Furthermore, we systematically investigate two fundamental aspects of this topic, i.e., frame representation and temporal modeling, which are widely and extensively studied in the literature. We then comprehensively review existing temporal action segmentation works, each categorized by their form of supervision. Finally, we conclude our survey by highlighting and identifying several open topics for research. In addition, we supplement our survey with a curated list of temporal action segmentation resources, which is available at https://github.com/atlas-eccv22/awesome-temporal-action-segmentation.},
	language = {en},
	urldate = {2022-11-22},
	publisher = {arXiv},
	author = {Ding, Guodong and Sener, Fadime and Yao, Angela},
	month = oct,
	year = {2022},
	note = {arXiv:2210.10352 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 26 pages, 10 figures, 9 tables},
	file = {Ding et al. - 2022 - Temporal Action Segmentation An Analysis of Moder.pdf:/Users/michaelwan/Zotero/storage/EJGSIG6J/Ding et al. - 2022 - Temporal Action Segmentation An Analysis of Moder.pdf:application/pdf},
}

@article{idrees_thumos_2017,
	title = {The {THUMOS} {Challenge} on {Action} {Recognition} for {Videos} "in the {Wild}"},
	volume = {155},
	issn = {10773142},
	url = {http://arxiv.org/abs/1604.06182},
	doi = {10.1016/j.cviu.2016.10.018},
	abstract = {Automatically recognizing and localizing wide ranges of human actions has crucial importance for video understanding. Towards this goal, the THUMOS challenge was introduced in 2013 to serve as a benchmark for action recognition. Until then, video action recognition, including THUMOS challenge, had focused primarily on the classiﬁcation of pre-segmented (i.e., trimmed) videos, which is an artiﬁcial task. In THUMOS 2014, we elevated action recognition to a more practical level by introducing temporally untrimmed videos. These also include ‘background videos’ which share similar scenes and backgrounds as action videos, but are devoid of the speciﬁc actions. The three editions of the challenge organized in 2013–2015 have made THUMOS a common benchmark for action classiﬁcation and detection and the annual challenge is widely attended by teams from around the world.},
	language = {en},
	urldate = {2022-11-03},
	journal = {Computer Vision and Image Understanding},
	author = {Idrees, Haroon and Zamir, Amir R. and Jiang, Yu-Gang and Gorban, Alex and Laptev, Ivan and Sukthankar, Rahul and Shah, Mubarak},
	month = feb,
	year = {2017},
	note = {arXiv:1604.06182 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--23},
	annote = {Comment: Preprint submitted to Computer Vision and Image Understanding},
	file = {Idrees et al. - 2017 - The THUMOS Challenge on Action Recognition for Vid.pdf:/Users/michaelwan/Zotero/storage/53SKVL2L/Idrees et al. - 2017 - The THUMOS Challenge on Action Recognition for Vid.pdf:application/pdf},
}

@book{ctx16694780230001401,
lccn = {63023237},
author = {Humphrey, Tryphena},
address = {New York :},
booktitle = {Advances in child development and behavior.},
isbn = {9780120097050},
publisher = {Academic Press,},
series = {Advances in Child Development and Behavior Volume 5},
title = {The Development of Human Fetal Activity and its Relation to Postnatal Behavior},
volume = {5},
year = {1970},
issn = {0065-2407},
journal = {Advances in child development and behavior.},
}

@article{zimmerman_patterned_2017,
	title = {Patterned auditory stimulation and suck dynamics in full-term infants},
	volume = {106},
	issn = {08035253},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/apa.13751},
	doi = {10.1111/apa.13751},
	abstract = {Methods—Sixteen healthy full-term infants participated in this study. Infants were fitted with electrocardiogram electrodes and a respiratory belt to measure cardiorespiratory patterning. Infants were offered a custom pacifier attached to a pressure transducer to measure NNS. Prior to the start of the study, a two-minute NNS and cardiorespiratory baseline was attained. Next, three auditory stimulation conditions were presented in the form of sucking clicks at inter-burst frequencies of 1, 2, and 4 Hz. Each of the three frequencies was played for two minutes.
Results—Separate repeated measures ANOVAs revealed significant differences in NNS burst duration (p=.013), NNS cycles/burst (p=.010), and NNS bursts/min (p=.005) across auditory stimulation conditions. No significant differences were evident in the cardiorespiratory outcomes.
Conclusion—We found that patterned auditory stimulation significantly reduced NNS dynamics and had no effect on cardiorespiratory patterning. The findings further suggest that infants attempted to modulate their suck pattern to the patterned acoustic stimuli by shortening their burst durations with fewer cycles per burst.},
	language = {en},
	number = {5},
	urldate = {2023-03-02},
	journal = {Acta Paediatrica},
	author = {Zimmerman, Emily and Foran, Megan},
	month = may,
	year = {2017},
	pages = {727--732},
	file = {Zimmerman and Foran - 2017 - Patterned auditory stimulation and suck dynamics i.pdf:/Users/michaelwan/Zotero/storage/5LLQMEG2/Zimmerman and Foran - 2017 - Patterned auditory stimulation and suck dynamics i.pdf:application/pdf},
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

