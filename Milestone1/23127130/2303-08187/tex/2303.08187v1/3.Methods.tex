\section{Problem Formulation and Approach}
\label{sec:methods}
\subsection{Problem Formulation}
 For formalization of the problem, we first introduce some notations. Let $x$ is the state of the vehicle that is estimated by the sensors attached to the car and $x$ is sampled from an unknown distribution $P_x$ which we neither control nor know but can sample from it. The goal of controller design is to learn a nonlinear function $f$ such that it maps each $x$ to the control action $c$.
 $$f: x \mapsto c \;  where\; x \sim P_x$$
The choice of $f$ that is an instance of a class of architecture ($\mathcal{A}$) affects the quality of the control action. In this context, we  chose two famous or extensively used architectures called random forest and Deep neural networks. The choice of model architecture also affects the training process and the parameters to select. In this context, we frame this modeling problem as a supervised learning problem where it is possible to get a labeled dataset that can be used for training. The collection of labeled sets of $<x_i,c_i>$ represents our data $D$. This data set is further split into training and test dataset called $D_{train}$ and $D_{test}$. Then the learning goal is to find the parameters (in the case of neural networks or feature decision graphs (in the case of the random forest)  that can represent the training data the most and work well on test data as well. 

\subsection{Approach}
In this section, we discuss a little background literature on our experimentation components, the approach of the experimentation, model training, and related details. 

\textbf{Torcs simulator} TORCS (The Open Racing Car Simulator) is an open-source 3D car racing simulator that has the ability to  drive manually using peripheral devices or by design controller- AI-based or traditional controller. The key components of vehicle dynamics, including mass, rotational inertia, collision, the mechanics of suspensions, links, differentials, friction, and aerodynamics, may all be accurately simulated by it. Using a temporal discretization level of $0.002$ seconds, Euler integration of differential equations is used to simplify and carry out physics simulation. TORCS offers a large variety of tracks and cars as assets. 
\\

\textbf{Data generation} For data generation purposes, we used the TORCS' asset car on the road track called 'Forza' (refer to fig \ref{fig:forza}). For sensing the environment, we deployed multiple LIDAR sensors attached to the car that can sense the track and the wall. For data generation purposes, we designed a tuned PID controller that can drive the car successfully on a given speed on the track Forza. Our data set consists of input as a set of distances measured by various LIDAR sensors and output as the steering value. We ran multiple rounds on this track and collected this data. This labeled data set is used for training our models. 
Both models (random forest and deep neural network) can be used for both classification and regression tasks. In our case, we want to use it as a regressor that can predict the steering value on the given LIDAR sensor data. 
\begin{figure}[tbhp]
\centering
   \includegraphics[width=1.0
   \linewidth]{images/forza.png}
   \caption{track: Forza}
   \label{fig:forza}
\end{figure}
\\

\textbf{Random forest model, parameters and training }
Random forest is a learning model that is an ensemble model of various decision trees. For regression tasks, the mean prediction of the individual decision trees is the estimated prediction. Random forests do various other tweaks in the vanilla decision tree for taking care of the habit of over-fitting of decision trees' on their training set.

In particular, decision trees are grown very deep when trained on data and consequently tend to learn highly nonlinear patterns but over-fit, i.e. have low bias, but very high variance. A random forest is an approach of averaging multiple decision trees, where each decision tree is trained on a different sub-sample of the training data set, with the goal of reducing the variance in prediction. 

For training the decision tree in a random forest we apply the technique of bootstrap aggregating, or bagging, to each decision tree learner. On a given input data set $D=\{X,Y\}$ where $X$ is LIDAR's measurement of obstacle distance; $X= \{x_1, x_2,..., x_n\}$ and steering output $Y=\{y_1,y_2,...,y_n\}$, the bagging process involves repeatedly sampling a subset of data with $B$ data points from the data set $D$ with replacement and train each decision tree on this selected samples. If $n$ is the number of trees in the random forest the process involves as below: 
for i=1,...,n:
\begin{enumerate}
    \item Sample with replacement $B$ training examples from $D$; call it $D_i$.
    \item train the $i^{th}$ decision tree $T_i$ on data set $D_i$.   
\end{enumerate}
For prediction on unseen data $x^{'}$, the steering output is estimated by estimating the mean of prediction from each decision tree. 
$$\hat{s}= \frac{1}{n} \sum_{i=1}^{i=n} T_i(x^{'}) $$
Additionally,  the uncertainty in the prediction can be estimated  by calculating the standard deviation of the predictions from all the individual regression trees on $x^{'}$:
$$\sigma= \sqrt{\frac{\sum_{i=1}^{i=n} (T_i(x^{'})-\hat{s})^2}{n-1}} $$
The prediction value of steering $\hat{s}$ is used for steering the vehicle and the standard deviation and other related statistics -like Coefficient of Variance (CoV). We select $n=100$ i.e. total 100 number of decision trees.  The other hyper-parameters are 'splitting criteria= \textit{squared error}', minimum samples for split=2, the maximum number of features=1.0, and minimum impurity decrease=$0.001$. 
\\
\textbf{Deep Neural Network model, parameters and training}
DNN are computing functions inspired by the biological neural networks of animal brains. These systems learn to do a task by observing examples that are already done/labeled. It has wide ranging application from image recognition\cite{}, engineering design\cite{vardhan2021machine,vardhan2022deepal}, control design, anomaly detection\cite{vardhan},etc. A DNN is based on a layered network of smaller computational units called artificial neurons. These neurons are in multiple layers  and fully connected between the input and output layers. A fully connected feed-forward neural network is used in this experiment that is defined by its architecture $a=(L, N,\delta)$, where $L$ is the number of layers in the networks $L \in N$, $N$ is the set that represents the number of neurons in each layer represented by $N_l$ where $l \in [L-1]$, $\delta$ is the activation function $\delta: R \mapsto R$. For this experiment, we used a fully connected feed-forward 6 layers neural network with $\{256,128,64,32,16\}$ neurons in hidden layers and $1$ neuron in the output layer. All hidden neurons have ReLU activation units and the output neuron is the linear activation unit.  The loss function, in this case, is the mean square error and the weight initialization is Xavier normal. The training is done for $500$ epochs by splitting the training and validation data into $90:10$ and early stopping is used to stop over-fitting.  
\begin{figure}[tbhp]
\centering
   \includegraphics[width=1.0
   \linewidth]{images/train_validation.png}
   \caption{Training and validation loss for deep neural network training}
   \label{fig:nn_train}
\end{figure}
