\section{Conclusion}

We propose a suite of visual language understanding tasks along with non-visual natural language understanding tasks to probe the effect of V\&L pretraining on such reasoning capabilities of text encoder models. We introduce Stroop probing as a zero-shot knowledge proving method for evaluating the innate knowledge of text encoders. We also show that exposure to V\&L data in pretraining improves the performance of text encoder models on VLU tasks, even though they may underperform unimodally trained text encoders on non-visual NLU tasks.
Beyond text-only tasks, these results bear importance in the broader context of multimodal learning, in which the isolated contribution of text encoders has previously been underexplored. Our findings suggest that multimodal pretraining has a significant effect on the knowledge represented by the text encoder component of multimodal models, facilitating in establishing best practices for the design and training of text encoders used in such contexts.






\medskip \noindent \textbf{Acknowledgements.} 
We thank Noriyuki Kojima, Gabriel Stanovsky, and Adi Haviv for their helpful feedback.