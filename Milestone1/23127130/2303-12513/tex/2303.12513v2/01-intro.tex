Humans are multimodal learners. We communicate with each other about things that we have experienced and knowledge we have gained using our senses---most commonly including sight as well as hearing, touch, smell, and taste. Our communication channel is limited to a single modality---spoken language, signed language, or text---but a reader or listener is expected to use his or her imagination to visualize and reason about the content being described. In general, language is used to describe scenes, events, and images; the words used to describe these are used to conjure up a visual impression in the listener. Therefore, it is natural to consider the types of visual reasoning used in understanding language, and to ask how well we can currently model them with computational methods.

Consider, for instance, the questions in Figure \ref{fig:teaser}. Concreteness is typically correlated with how well a concept can be visually imagined. For example, a concrete word such as \emph{present} often has a unique visual representation. In addition, common associations such as \emph{ocean}$\rightarrow$\emph{blue} (color) and \emph{corn chip}$\rightarrow$\emph{triangle} (shape) reflect properties of an imagined visual representation of the item in question. These properties may be difficult to infer from text alone without prior knowledge gained from visual input; for instance, a number of studies have investigated the partial ability of blind English speakers to predict color associations and how it differs from the intuition of sighted speakers\footnote{This phenomenon is illustrated in \href{https://www.youtube.com/watch?v=59YN8_lg6-U}{this interview} with Tommy Edison, a congenitally blind man, in which he describes his understanding and frequent confusion regarding color associations.}~\cite{van2021blind, saysani2021seeing, saysani2018colour, shepard1992representation, marmor1978age}. 

There has been a wealth of recent research vision-and-language (V\&L) tasks involving both text and image data, and the use of vision-language pretraining (VLP) to create models that are able to reason jointly about both of these modalities together~\cite{chen2020uniter,kim2021vilt,li2021align,chen2022vlp}. Notable in this regard is CLIP~\cite{radford2021learning}, consisting of paired text and image encoders jointly trained on a contrastive objective, that learns to align text and image embeddings in a shared semantic space. On the other hand, text encoder models such as BERT~\cite{devlin2018bert} learn to reason about text in a unimodal vacuum, with knowledge derived from pretraining tasks that only involve textual data.




Prior work has investigated the performance of multimodally trained text encoders on various natural language understanding (NLU) tasks with mixed results, sometimes finding that they are outperformed by unimodal models~\cite{iki2021effect} and at other times suggesting improved performance~\cite{wang2021simvlm}. However, these works fine-tune the models under consideration on NLU tasks before evaluation, making it difficult to disentangle the effects of multimodal pretraining and fine-tuning configuration on the observed performance. Additionally, these works do not address the distinction between NLU tasks requiring implicit visual reasoning and ones that are purely non-visual. We refer to natural language inference involving implicit visual reasoning as \emph{visual language understanding} (VLU) and propose a suite of VLU tasks that may be used to evaluate visual reasoning capabilities of pretrained text encoders, focusing primarily on zero-shot methods.

We compare multimodally trained text encoders such as that of CLIP to BERT and other unimodally trained text encoders, evaluating their performance on our suite of VLU tasks. We evaluate these models in without modifying their internal weights in order to probe their knowledge obtained during pretraining. A key design aspect of these tests is the probing method used to evaluate knowledge. Previous work has probed the knowledge of BERT and similar models using a masked language modelling (MLM) paradigm~\cite{petroni2019language, rogers2020primer}, but this cannot be directly applied to CLIP since it was not pretrained with MLM. We therefore propose a new zero-shot probing method that we term \emph{Stroop probing}. This is based on the psychological Stroop effect~\cite{macleod1991half} (described in Section \ref{sec:probing}), which suggests that salient items should have a stronger interference effect on the representation of their context.

Strikingly, we find that the multimodally trained text encoders under consideration outperform unimodally trained text encoders on VLU tasks, both when comparing to much larger encoders as well as ones of comparable size. We also compare these models on baseline NLU tasks that do not involve visual reasoning and find that models such as CLIP underperform on these tasks, demonstrating that they do not have a global advantage on NLU tasks. We conclude that exposure to images during pretraining improves performance on text-only tasks that require visual reasoning. Furthermore, our findings isolate the effect of the text component of multimodal models for tasks such as text to image generation, providing principled guidelines for understanding the knowledge that such models inject into downstream vision tasks.


