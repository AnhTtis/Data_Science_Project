
\subsection{Non-dual V\&L encoder models}

\input{tables/non_dual_encoder_results}

Although non-dual (fusion) encoder models are not directly comparable to purely textual encoders such as BERT or the text encoder component of CLIP which do not fuse modalities, we consider them here for completeness We evaluate the VisualBERT and LMXERT non-dual encoder models on several tasks from our task suite by only feeding them textual input. Results are shown in Table \ref{tab:nondualencoderresults}. As illustrated in the table, even though these models were trained using image features together with text tokens, the models outperform BERT on visual tasks, though the gap is not as significant as with dual encoder models.


\subsection{Additional tasks using linear probing}

We present two additional tasks for comparing V\&L and unimodal models using linear probing, one VLU and one NLU task. For both tasks, we use a linear classifier on the pooled embedding output of a model for categorical prediction. Specifically, we use a logistic regression model using the scikit-learn \texttt{linear\_model.LogisticRegression} implementation. For all tasks we use the default parameters except for \texttt{max\_iter} which was changed according to task requirements to allow convergence. In particular, we use parameters \texttt{penalty='l2', C=1.0, solver='lbfgs'}.

\subsubsection{Groundability classification}

\medskip \noindent \textbf{Task description.} In paired text-image data, there is normally an implied mapping between referential expressions in the text and objects or regions in the accompanying image. The task of learning these mappings is known as \emph{visual grounding} and is of general interest for visual semantic understanding \cite{Mao2015:refexp,Wang:16phraselocalize,cui2021s}. In captions accompanying images, some expressions refer directly to regions in images while others give non-visual context; we refer to the former as \emph{groundable} referents and the latter as \emph{non-groundable}. A similar paradigm was recently proposed by Kim et al.~\cite{kim2022flexible} that separately considers ``answerable'' and ``unanswerable'' phrases. 

We propose a \emph{groundability classification} task, consisting of classifying referents in text as groundable or non-groundable. This is a text-only task as it uses text alone and the visual context is only implied. Since this task requires visual imagination to complete, we consider it to be a VLU task.

\medskip \noindent \textbf{Experimental details.} In line with previous works that consider person-centric visual grounding \cite{cui2021s,qu2022weakly}, we construct a dataset of \emph{person-centric groundability} sentences where a fixed human participant is either implied to be groundable (i.e., on-camera) or non-groundable (i.e., off-camera). The associated task consists of binary classification applied to these texts according to whether the given participant would be visible in a description of an event. Due to the lack of existing labelled data for this task, we created a synthetic dataset of sentences with a common format: \textit{Alex \MASK{}ing Riley's \MASK{}}, where the first masked word is a randomly drawn verb, and the second masked word is a randomly drawn noun, and the task is to classify whether or not the second mentioned individual (i.e., Riley) is groundable. Groundability labels are estimated using zero-shot text classification with a pretrained natural language inference model. 
We created synthetic data for the groundability task by taking the prompt template ``Alex \rule{0.2in}{.5pt}ing Riley's \rule{0.2in}{.5pt}'', filling in various verb-noun pairs into the given slots, and filtering using a pretrained language model to select for natural-sounding samples. We then estimated ground-truth labels using zero-shot inference with a pretrained natural language inference (NLI) model.

To find verb-noun pairs, we listed all verbs and nouns in the Brown corpus of standard American English with part-of-speech labels~\cite{francis1964standard}. We converted all text to lowercase and then selected the 5,000 most common verb lemmas and 1,000 most common noun lemmas in this corpus. Using all possible verb-noun combinations among these, inserted into the prompt template shown above, yielded 5M candidate phrases. From the given 5M candidates we sample randomly 200K phrases. We then calculate the total negative log-likelihood (NLL) for each candidate relative to the pretrained language model GPT2-large~\cite{radford2019language} and kept only those samples in the 20th percentile of NLL (i.e. the most likely samples), corresponding to 40,000 descriptions.

After generating these texts, we estimated labels using a pretrained NLI model. We used BART-large~\cite{lewis2019bart} fine-tuned on the MNLI dataset~\cite{williams2017broad} (using the \texttt{facebook/bart-large-mnli} checkpoint from Hugging Face model hub\footnote{\href{https://huggingface.co/facebook/bart-large-mnli}{The model can be found here.}}). 
This model takes pairs of texts as inputs (the ``premise'' and ``hypothesis'' texts) and outputs three probabilities per pair: $p_c, p_n, p_e$, corresponding to probabilities of a contradictory, neutral, or entailment relation between the texts respectively. As observed by Yin et al.~\cite{yin2019benchmarking}, NLI can be used for zero-shot text classification by designing premise and hypothesis prompts for the task of interest. In our case, we use the following prompts:

\textbf{Premise:} ``This is a picture of \rule{0.5in}{.5pt}.''

\textbf{Hypothesis:} ``Riley can be seen in the picture.''

For each of our 40,000 texts, we insert the text in the slot given in the premise and calculate $p_e$ with the NLI model. If $p_e > 0.5$ we assign the sample label $1$ (groundable), otherwise we assign it label $0$ (non-groundable). 
Below are several example sentences from the synthetic dataset. 

\noindent Examples of Riley being groundable (sample label $1$): 
\begin{itemize}
    \item{Alex facing Riley's figure}
    \item{Alex viewing Riley's participation}
    \item{Alex seeing Riley's enjoyment}
\end{itemize}
Examples of Riley being non-groundable (sample label $0$): 
\begin{itemize}
    \item{Alex hiding Riley's file}
    \item{Alex announcing Riley's absence}
    \item{Alex stealing Riley's evidence}
\end{itemize}

For evaluation we created a test set, containing 200 sentences judged by human evaluators to be natural sounding, half labeled as groundable and the other half as non-groundable. To provide an example, sentences such as \emph{Alex cutting Riley's hair} or \emph{Alex blocking Riley's shot} were labeled as groundable, whereas sentences such as \emph{Alex painting Riley's house} or \emph{Alex counting Riley's vote} were labeled as non-groundable.


For this binary classification task, we apply linear probing to assess our models' understanding of groundability, and report ROC-AUC scores for each model. We also provide 95\% confidence intervals, calculated using bootstrap resampling with 200 bootstraps, in order to analyze the robustness of these results.

\input{tables/groundability_results}
\medskip \noindent \textbf{Results and discussion.} Results for the groundability classification are provided in Table \ref{tab:groundabilityresults}. As these results illustrate, CLIP significantly outperforms all unimodally trained text encoders on average. We observe that the score gaps are not as distinct as in the previous zero-shot tasks, as this task is a learnable task which requires training, allowing all models to learn this task to some extent. Nonetheless, CLIP's ability to surpass the unimodally trained encoders suggest that V\&L trained text encoders have a better ability to grasp if an object is grounded or not due to additional perceptual information that is encoded during the pretraining phase. Furthermore, note that in comparison to the other VLU tasks, here the subject in question (i.e., Riley) is not directly connected to visual information and the prediction is based only on context relating to the performed action and the associated object. The improved performance on this task illustrates that V\&L models can better encode higher-level perceptual reasoning. 

\subsubsection{Natural language inference}

\medskip \noindent \textbf{Task description.} Natural language inference (NLI) refers to inferring the logical relation between pairs of statements, as well as more generally referring to logical inference based on text~\cite{storks2019recent}. In particular, NLI commonly  considers the following logical relations between sentences A and B:

\begin{itemize}
    \item \textbf{Contradiction}: For example, A=\emph{It is rainy outside.} is contradicted by B=\emph{It is sunny outside.}, since they cannot be simultaneously true.
    \item \textbf{Neutral}: For example, A=\emph{It is rainy outside.} is neutral with regards to B=\emph{It is summer.}, since A neither contradicts nor entails B.
    \item \textbf{Entailment}: For example, A=\emph{It is cold and rainy outside.} entails B=\emph{It is cold outside.}, since if A is true then B must also be true.
\end{itemize}

Solving this task requires an understanding of the fine-grained semantics of language and logical reasoning. On the other hand, visual cues are not tightly related to this task and are even potentially misleading. For example, the sentences \emph{This cup contains grape juice.} and \emph{This cup contains wine.} are contradictory even though the scenes they describe are visually identical.  Therefore, we consider this to be a non-visual NLU task.

\medskip \noindent \textbf{Experimental details.} For this task we use the MNLI dataset introduced by Adina et al.~\cite{williams2017broad}. We remove sentence pairs with a neutral relation and treat this as a binary classification task to predict sentence pairs as contradictory or entailing. We perform 5-fold cross validation on a dataset of 261,775 pairs of sentences using 80\% of samples for training and 20\% for testing.

For each sentence pair, we concatenate the sentences' two pooled embeddings and apply linear probing. Note that some models such as BERT include a special \SEP token for encoding sentence pairs as a single unit, but we encode sentences separately and concatenate their embeddings in order to have a fair comparison between all models. We report the ROC-AUC score on the MNLI test set.

\input{tables/nli_results}
\medskip \noindent \textbf{Results and discussion.} Results for NLI are provided in Table \ref{tab:nonvisualresults}. As shown in the table, text-based models outperform CLIP by a large margin. Similar to our findings regarding linguistic acceptability classification, we see that V\&L trained models are less effective in tasks that do not incorporate perceptive information, suggesting that for non-visual tasks, V\&L pretraining is not necessarily beneficial. 

\subsection{Comparing usage of SP on text based models}

\input{tables/comparing_sp_mlm}

In the main paper we presented results for text models using MLM probing, and for CLIP using Stroop probing (SP). To allow for a full comparison between both types of models, and to strengthen the choice of using MLM probing for text based models, we present additional results comparing SP and MLM probing for text based models. %
Table \ref{tab:comparingspmlm} presents results for comparing SP and MLM probing methods for BERT and RoBERTa. As illustrated, using SP with unimodally trained models results in lower performance than using MLM probing with these models. This result supports our choice of using MLM probing for text based models trained to perform MLM tasks as the preferred probing method. 

\subsection{Additional task results information}

\input{tables/all_results_mean_std}

We provide additional detailed results for our suite tasks including the mean and standard deviation of the results over all used prompts in Table \ref{tab:allresultsmeanstd}.

\subsection{Qualitative analysis for V\&L model misclassifications on color prediction}

\input{tables/clip_miscalssified_colors}

Our results for color association prediction show that V\&L models outperform unimodally trained text encoders in the given setting. Additional qualitative analysis of the results show that even the reported misclassifications of V\&L models such as CLIP may be explained by ambiguities in the dataset itself. For example, the noun ``ash'' has ground truth value ``grey'' in our dataset, while CLIP with SP predicts the color ``black'', which is arguably also correct. Table \ref{tab:clipmisclassification} presents all of the objects from both color datasets misclassified by CLIP, containing the ground truth and the predicted color. As seen there, most of these predictions may be interpreted as valid colors for the given objects.

\subsection{Analysis of reporting bias in LAION}

Prior works have noted that commonsense properties that can be inferred from text are less likely to be explicitly stated than incongruent properties, notably including color terms(e.g. \emph{a (yellow) banana} vs. \emph{a blue banana})~\cite{paik2021world,shwartz2020neural,gordon2013reporting}. In particular, text in image captioning datasets such as the web-scale LAION dataset\cite{schuhmann2022laion} (used to train OpenCLIP) might have a different incidence of reporting bias than the text used to train models such as BERT. To disentangle this from the effect of training on the visual modality, we provide an analysis of reporting bias in LAION for color associations.

We use the \texttt{laion-2B-en} subset of 2.33 billion English-language image-caption pairs in the LAION-5B dataset, and estimate reporting bias by searching for bigram pairs $(c, w)$ where $c$ is a basic color term\footnote{one of $\{$red, orange, yellow, green, blue, black, white, grey, brown$\}$} and $w$ is a unigram noun from our color association datasets (CTD and NCD). The empirical probability of color $c$ immediately preceding $w$ is $P(c|w) = n_{(c, w)} / n_w$, where $n$ indicates the number of instances of the given ngram, and the associated color estimates are $\hat{c}_w = \arg \max_c P(c | w)$. For these estimates, the corresponding accuracy scores on the unigrams in our datasets are $acc_{\texttt{CTD}} = 0.549$ and $acc_{\texttt{NCD}}=0.714$, significantly below the accuracies achieved by all of the multimodally trained models under consideration on these datasets for the color prediction task. We also provide qualitative examples in Table \ref{tab:reporting_bias} showing the effect of reporting bias for various common nouns from these datasets. These results provide evidence that multimodally trained models' strong performance on VLU tasks cannot be explained away as stemming from a lack of reporting bias in the texts used to train them.

\input{tables/reporting_bias_table}