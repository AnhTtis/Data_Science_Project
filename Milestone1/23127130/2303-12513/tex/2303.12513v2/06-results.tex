\section{Results and Discussion}

\input{tables/all_results.tex}


Results for the tasks described above are provided in Table \ref{tab:all_results}. For tasks with multiple prompts the listed metrics are the maximum over prompts, providing a rough upper bound on each model's ability to perform the task in question. Further analysis of performance by prompt, as well as SP results for models shown here with MLM, are provided in the supplementary materials.


As seen in these results, multimodally trained models consistently outperform unimodally trained models on VLU tasks, including both comparably sized and much larger text encoders, while generally underperforming them on non-visual NLU tasks. This is further illustrated by qualitative analysis of the results in various tasks.

\input{figures/concreteness/concreteness_examples}

\input{figures/color/color}
\input{figures/shape/shape}

Figure \ref{fig:concreteness_examples} shows the results of concreteness prediction for CLIP and BERT. Nouns predicted as most concrete by CLIP, for example \emph{bench} and \emph{chalk}, that can be clearly visualized, while nouns predicted as least concrete (i.e., abstract) such as \emph{story} and \emph{name}, do not have a clear visual representation. In comparison, BERT's predictions are significantly noisier, with nouns such as \emph{seed} and \emph{jelly} predicted as abstract.

Figures \ref{fig:colors} and \ref{fig:shapes} shows color and shape association predictions of BERT-base and CLIP on samples from the relevant datasets. Without having access to the associated images, the CLIP text encoder usually predicts the correct matching between the given item and its correct shape or color, while BERT fails in most cases. Our results suggest that these associations are more consistently encoded by multimodally trained encoders. Furthermore, qualitative analysis of the misclassifications of CLIP, OpenCLIP and FLAVA on color association prediction reveals that these are mostly due to ambiguities in the dataset itself; see the supplementary materials for details.

Performance on non-visual NLU tasks, shown on the right side of Table \ref{tab:all_results}, demonstrates that our results are not an artifact of our probing methodology providing a global advantage to multimodally trained models, nor are these models uniformly better at language-related tasks. We also see that the non-visual tasks are highly solveable, with BERT-large and RoBERTa-large achieving high performance on all tasks despite the challenging zero-shot regime and limited information in the task inputs (ambiguity in cloze contexts for factual probing, lack of textual textual context for proficiency probing and randomly-chosen sentences for sentiment analysis). Despite this, the multimodally trained models show near-random performance.

We note a direct connection to the original Stroop effect in the field of human psychology. Follow-up works to the first Stroop effect demonstration have found it to apply to various types of stimuli, such as color-congruent and incongruent objects (e.g. a yellow banana vs. a purple banana)~\cite{naor2003color}. Our results, also including color congruence of objects, strengthen the motivation for using Stroop probing applied to tasks involving visual congruence or saliency. 

We also note a connection between our results and the \emph{reporting bias} effect, in which commonsense properties are less likely to be explicitly stated than incongruent properties (e.g. \emph{a (yellow) banana} vs. \emph{a blue banana}). Reporting bias in text has been studied in the context of color associations~\cite{paik2021world} and in more general contexts~\cite{shwartz2020neural,gordon2013reporting}. As the multimodally trained models under consideration were trained on paired image-caption data, the distribution of text in image captions differs somewhat from the text used for training models such as BERT. In the supplementary material, we provide an analysis of reporting bias in the LAION dataset~\cite{schuhmann2022laion}, the training data for the OpenCLIP model included in our tests. These results provide evidence that the improvement in performance seen from V\&L training cannot primarily be attributed to a lack of reporting bias in image caption texts, and emphasizes the significance of the visual modality in these models' language understanding.