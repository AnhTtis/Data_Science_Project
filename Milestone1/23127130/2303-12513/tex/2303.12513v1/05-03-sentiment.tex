\subsection{Sentiment Analysis}
\label{sec:sentiment}

\medskip \noindent \textbf{Task description.} \emph{Sentiment analysis} refers to the task of predicting speaker emotion or affect, a well-studied problem in natural language processing~\cite{jurafsky2023speechchapter4,li2017reflections, poria2020beneath}. We focus on sentiment analysis in text as a subset of text classification. Since text describing the same visual scene may have a positive or negative sentiment (\emph{This cake is delicious} vs. \emph{This cake tastes bad}), we consider this task to be a non-visual NLU task.

\medskip \noindent \textbf{Experimental details.} For this task, we use the IMDB movie review dataset consisting of 50K movie reviews with binary sentiment labels (positive/negative)~\cite{maas2011learning}. In order to provide a fairer comparison between models rather than biasing towards models trained on longer texts, we use only a single random sentence from each review in the IMDB dataset. In addition, we filter long sentences which are too long for multimodal encoders leaving 42,567 examples. Using only a single sentence makes this task more challenging since the randomly chosen sentence is not guaranteed to contain sufficient context for understanding the review's sentiment, but we find that significantly better than random performance is achievable, as seen in the results section. We also differ from the more common learned sentiment analysis paradigm by using strictly zero-shot prediction via engineered prompts. For example, one prompt used is \emph{``sentiment expressed for the movie is \SMASK. \SW''}, where \SW indicates the sentence chosen from the initial review, and \SMASK may be filled with one of $\{$\emph{good, bad}$\}$. We apply MLM and Stroop probing for binary prediction, and report categorical accuracy achieved for each model.

\medskip \noindent \textbf{Evaluation metric.} We report categorical accuracy of predictions ($acc$) relative to the ground truth sentiment labels.