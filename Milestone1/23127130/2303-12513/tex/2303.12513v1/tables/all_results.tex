\begin{table*}[t]
  \centering
  \setlength{\tabcolsep}{3.5pt}
  \def\arraystretch{0.95}
  \begin{tabularx}{0.92\textwidth}{lcccccccccccccc}
     \toprule
    &
    \multicolumn{3}{c}{\textbf{Concreteness}} &
    \multicolumn{2}{c}{\textbf{Color}} &
    \textbf{Shape} &&& 
    \multicolumn{2}{c}{\textbf{Knowledge}} &
    \multicolumn{3}{c}{\textbf{Proficiency}} &
    \textbf{Sent.} 
    \\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-6}
    \cmidrule(lr){7-7}
    \cmidrule(lr){10-11}
    \cmidrule(lr){12-14}
    \cmidrule(lr){15-15}
    Metric & $\left|\rho\right|$ & $\left|r_s\right|$ & $\left|\tau\right|$ &
    $acc_{\texttt{CTD}}$ & $acc_{\texttt{NCD}}$ & acc &&&
    R@1 & R@5 & $acc_{\texttt{V}}$ & $acc_{\texttt{N}}$ & $acc_{\texttt{P}}$ & acc
    \\
    \midrule
    \textbf{Unimodal} \\
    BERT-base & 0.414 & 0.416 & 0.283 & 0.353 & 0.400 & 0.559 &&& 0.198 & 0.522 & 0.898 & 0.753 & 0.893 & 0.618 \\
    BERT-large & 0.348 & 0.355 & 0.239 & 0.490 & 0.467 & 0.587 &&& \textbf{0.231} & \textbf{0.541} & \textbf{0.914} & \textbf{0.779} & 0.905 & 0.625 \\
    DistilBERT & -- & -- & -- & 0.333 & 0.400 & 0.587 &&& 0.148 & 0.479 & 0.864 & 0.709 & 0.814 & 0.637 \\
    RoBERTa-base & 0.433 & 0.404 & 0.275 & 0.431 & 0.333 & 0.431 &&& -- & -- & 0.877 & 0.718 & 0.881 & 0.666 \\
    RoBERTa-large & 0.345 & 0.374 & 0.253 & 0.471 & 0.400 & 0.431 &&& -- & -- & 0.898 & 0.765 & \textbf{0.916} & \textbf{0.703} \\
    DistilRoBERTa & -- & -- & -- & 0.411 & 0.333 & 0.431 &&& -- & -- & 0.804 & 0.664 & 0.756 & 0.635 \\
    ERNIE & 0.461 & 0.496 & 0.338 & 0.196 & 0.333 & 0.449 &&& 0.001 & 0.006 & 0.064 & 0.051 & 0.086 & 0.582 \\
    ERNIE-large &  0.358 & 0.353 & 0.233 & 0.216 & 0.267 & 0.458 &&& 0.006 & 0.022 & 0.209 & 0.241 & 0.280 & 0.674 \\
    SBERT & 0.338 & 0.337 & 0.228 & 0.198 & 0.067 & 0.513 &&& 0.013 & 0.141 & 0.237 & 0.158 & 0.126 & 0.554 \\
    \midrule
    \textbf{V\&L} \\
    CLIP & 0.603 & 0.624 & 0.437 & 0.843 & \textbf{0.800} & 0.798 &&& 0.009 & 0.118 & 0.134 & 0.133 & 0.126 & 0.560 \\
    OpenCLIP & \textbf{0.634} & 0.643 & 0.432 & \textbf{0.941} & \textbf{0.800} & \textbf{0.853} &&& 0.009 & 0.121 & 0.211 & 0.135 & 0.123 & 0.560 \\
    FLAVA & 0.608 & \textbf{0.665} & \textbf{0.449} & 0.882 & \textbf{0.800} & 0.798 &&& 0.020 & 0.138 & 0.116 & 0.118 & 0.139 & 0.519 \\
    \bottomrule
    
  \end{tabularx}
  \vspace{-5pt}
  \caption{\textbf{Results on VLU (left) and non-visual NLU (right) tasks:} concreteness prediction, color and shape association prediction, factual knowledge probing, language proficiency probing, and sentiment analysis (Sent.) respectively. For tasks other than concreteness prediction, MLM probing is used for models supporting it (BERT, DistilBERT, RoBERTa, DistilRoBERTa); SP is used elsewhere. The definition of each metric is defined in the relevant task definition in Sections \ref{sec:vlu}-\ref{sec:nlu}. DistilBERT and DistilRoBERTa do not have concreteness results due to the pooling layer issue mentioned in Section \ref{sec:concreteness}, and RoBERTa and DistilRoBERTa do not have results for factual knowledge probing due to the tokenization issue mentioned in Section \ref{sec:factual}. As these results show, V\&L models yield superior performance on visual tasks, while underperforming unimodally trained models on non-visual NLU tasks.} 
\label{tab:all_results}
\end{table*}