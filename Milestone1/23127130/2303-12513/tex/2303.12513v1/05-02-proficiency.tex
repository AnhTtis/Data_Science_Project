\subsection{Language Proficiency Probing}
\label{sec:proficiency}

\medskip \noindent \textbf{Task description.} In order to evaluate our models' intrinsic knowledge of general language usage, we consider the task of reconstructing English text in order to produce natural-sounding language. Multiple-choice cloze tests are commonly used in language assessment tasks for students to evaluate their proficiency~\cite{stubbs1974cloze,alderson1979cloze,trace2020clozing}. Similarly, a model with a good grasp of English language usage should be able to fill in missing words in cloze contexts to produce fluent English. This requires grammatical and semantic knowledge, but in general, it is not directly related to visual reasoning when applied to arbitrary masked contexts. As noted by Trace~\cite{trace2020clozing}, cloze tasks may evaluate global reading comprehension or local contextual information in the cloze context; we focus on the latter case and refer to this task applied to our models  as \emph{language proficiency probing}.

\medskip \noindent \textbf{Experimental details.} To evaluate language proficiency, we use the Childrenâ€™s Book Test (CBT) cloze dataset provided by Meta research~\cite{hill2015goldilocks}, consisting of book passages with accompanying masked sentences and possible mask completions. We discard the book passages and only consider the sentences and completions, to focus on the task of reconstructing well-formed text. Completions are grouped by part of speech (POS); we use the noun (N), verb (V), and preposition (P) groups and discard the named entity groups since the latter require long-distance context to predict while N, V, and P can often be inferred from local sentential context. In total, each of the N, V, and P groups contains 2,500 sentences with 10 possible completions each. We filter out long sentences since our multimodally trained models have shorter expected input lengths. After filtering we are left with 1,588 noun, 1,747 verb, and 2,382 preposition completion sentences. In addition, we only use sentences that have a one-word token answer for all tokenizers. For example, one sentence from the V group is \emph{I \SMASK not a fellow; I am a young lady!} and the set of possible completions is \{\emph{am, born, find, picking, pricked, said, sat, seems, streamed, thinking}\}. We use MLM and Stroop probing to evaluate our models on this data.

\medskip \noindent \textbf{Evaluation metrics.} We report categorical accuracy per POS group ($acc_V$, $acc_N$, and $acc_P$), measuring how often the ground truth answer is selected in each of these groups.