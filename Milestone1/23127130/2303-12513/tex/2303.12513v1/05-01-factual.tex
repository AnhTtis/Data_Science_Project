\subsection{Factual Knowledge Probing}
\label{sec:factual}

\medskip \noindent \textbf{Task description.} It has been observed that language models have an emergent \emph{knowledge base} property, in which they may be probed for factual knowledge about the world~\cite{petroni2019language, rogers2020primer}. Various works on probing BERT and other language models for commonsense world knowledge have found that they show an impressive ability to memorize knowledge, although they may be deficient in applying this knowledge to reasoning about the physical world~\cite{forbes2019neural}. In this task, we probe our models for fine-grained factual knowledge via a cloze test, where an empty slot must be filled in with a word. We test on factual knowledge about geographical locations since this requires factual knowledge that does not explicitly rely on visual reasoning.

\medskip \noindent \textbf{Experimental details.} For this task, we use the Comparative Question Completion dataset introduced by \cite{zagoury2021s}. This consists of questions in which one of a pair of coordinated elements is masked; the target is the masked phrase. Specifically, we use the \emph{cities} dataset which masks the names of geopolitical entities such as cities and countries. Example sentences from the dataset include: \emph{which country has more part time jobs new zealand or \SMASK{}?} (the correct answer being \emph{australia}) and \emph{which is older saudi arabia or \SMASK{}? } (the correct answer being \emph{persia}).
The original dataset has 1,187 questions with 447 unique locations as answers. In order to fit the general method of masking tasks, we filter masked phrases with more than one token (e.g. \emph{the west coast}) similar to the protocol presented in the original paper. As this results in an extremely limited set of candidates for MLM models such as RoBERTa that use Byte Pair Encoding tokenization, we restrict the MLM models under comparison to BERT and DistilBERT. The filtered dataset contains 825 questions with 216 unique locations.

We treat this task as a categorical classification task, choosing only from the set of unique locations given in the dataset per sample, and evaluating how often the correct target is chosen. We use MLM probing and Stroop probing for categorical prediction as described above. Similarly to our other tasks, the intuition is that more surprising completions should have a larger interference effect on the text's encoding, if the relevant information is encoded in the embedding.

\medskip \noindent \textbf{Evaluation metrics.} We report recall at one and five ($R@1$, $R@5$), measuring how often the ground truth answer is found among the model's top one or five predicted candidates.