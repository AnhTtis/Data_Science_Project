\subsection{Concreteness Prediction}
\label{sec:concreteness}

\medskip \noindent \textbf{Task description.} Words and phrases can be roughly classified as either \emph{concrete} or \emph{abstract}. A concrete concept is something that can be pointed to or directly sensed, while abstract concepts refer to things that cannot be easily visualized~\cite{schwanenflugel2013abstract}. This can be conceptualized on a scale, ranging from the most abstract to the most concrete. Psychological research suggests that concrete words are easier for humans to understand and remember than abstract words~\cite{schwanenflugel2013abstract}. Similarly, it has been shown that concreteness correlates with the learnability of visual concepts for machine learning models~\cite{hessel2018quantifying}, and that MLM pretraining of V\&L models may be improved by preferentially masking concrete words~\cite{bitton2021data}.

Because concreteness is a property of text that is tightly coupled with the visual domain, we consider concreteness prediction to be a VLU task, requiring some knowledge of the visual content of language to complete. We note that this task has been addressed in various previous works~\cite{hill2014multi,hill2014learning,hessel2018quantifying,rabinovich2018learning,charbonnier2019predicting}. In contrast to these approaches, our unsupervised concreteness estimation procedure evaluates the concreteness of a word or phrase in a given textual context, rather than being limited to a fixed set of lexical items or discrete categories in a dataset.

\medskip \noindent \textbf{Experimental details.} We probe our models for the concreteness of words in context by using a cloze task paradigm with Stroop probing. For example, using the prompt $t_m = $ \emph{``I see the \SMASK''} and testing word \W, we insert the word into the prompt to obtain $t_w = $ \emph{``I see the \W''}, and use cosine similarity score $s_w$ between embeddings of $t_m$ and $t_w$ as the regression output. All prompts used are listed in the supplementary material. 

We test our approach on the dataset introduced by Brysbaert et al. \cite{brysbaert2014concreteness}. This dataset contains 39,954 English unigrams and bigrams along with human-labelled concreteness scores on a scale from 1 (abstract) to 5 (concrete), averaged over annotators. We only use the unigram nouns from this list, totaling 14,562 items. Note that unlike prior concreteness prediction techniques that train supervised models on this dataset~\cite{charbonnier2019predicting}, we perform zero-shot prediction on this task with no supervised training, using the dataset for testing only.

Also note that we do not report results for DistilBERT or DistilRoBERTa since the checkpoints used do not contain a trained pooling layer, which is required for Stroop probing.


\medskip \noindent \textbf{Evaluation metrics.} We report absolute values of Pearson, Spearman, and Kendall correlations between the predicted concreteness and ground truth scores ($\left|\rho\right|$, $\left|r_s\right|$, and $\left|\tau\right|$ respectively).