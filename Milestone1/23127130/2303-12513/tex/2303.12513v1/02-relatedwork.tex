Building models to create text embeddings for a large range of
language tasks has been broadly explored over the past several years. In our work we compare two types of transformer-based models which encode these types of embeddings: those trained on text-only data (unimodally trained), and those exposed to both text and image data during training (multimodally trained).

Since the introduction of the self-attention-based transformer architecture by Vaswani \emph{et al.}~\cite{vaswani2017attention}, transformers have become the predominant architecture for tasks involving textual data. Devlin et al.\cite{devlin2018bert} introduce BERT, a transformer model encoding contextual information for each token in an input sequence in a bidirectional manner. They suggest a self-supervised pretraining method to compute contextual feature representations of textual data, via masked language modelling and next sentence prediction objectives. This pretrained model can then be applied to other downstream tasks by end-to-end fine-tuning. Subsequently, various other text encoder transformers have been proposed, such as RoBERTa~\cite{liu2019roberta}, DistilBERT~\cite{sanh2019distilbert}, and ERNIE~\cite{sun2019ernie, sun2020ernie}. While these models differ on some architectural details and on precise pretraining objectives, they all share the basic transformer architecture and the use of denoising pretraining objectives.  In particular, they are all trained on unimodal data, meaning that they are only exposed to text during training.

In contrast to unimodally trained text encoders, V\&L models have been exposed to both text and image data during training. These models are typically used for tasks that require joint reasoning on text and images, such as visual question answering, grounding referring expressions, and vision-language retrieval \cite{chen2022vlp,gan2022vision}. Fusion encoder models such as LXMERT~\cite{tan2019lxmert}, UNITER~\cite{chen2020uniter}, ViLT~\cite{kim2021vilt}, and ALBEF~\cite{li2021align} output a fused representation of text and image data, while dual encoder models like CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling} consist of dual text and image encoder models that are jointly trained to produce embeddings in a shared semantic space. FLAVA, a vision-and-language model introduced recently by Singh et al. \cite{singh2022flava}, also includes dual text and image encoders, and is trained on both multimodal objectives involving the alignment of images and text, as well as unimodal objectives. In this work we focus on the text encoder component of dual encoder models, since it may be used after V\&L pretraining for text-only tasks.

Various works have explored the use of multimodal learning to benefit text understanding.
Most related to our work is the very recent study of Zhang et al.~\cite{zhang2022visual} which investigates the use of unimodal and multimodal models for understanding visual commonsense in text. 
Their analysis follows a line of related work investigating the contribution of multimodal learning to visual commonsense knowledge in text, since such knowledge is typically not written explicitly in text but is abundantly present in visual information~\cite{vedantam2015learning,lin2015don,kottur2016visual}. Unlike Zhang et al.~\cite{zhang2022visual} who only evaluate CLIP with an added set of task-specific learned weights, we are able to probe CLIP and other similar models in the strictly zero-shot setting via our novel Stroop probing method. This allows for directly evaluating properties learned by the models, independent of differences that result, for instance, from specific training configurations. In addition, %
we also study performance on both visual and non-visual NLU tasks in order to provide a controlled benchmark.

Other works have investigated the use of multimodal learning for NLU in various contexts. Bruni et al.~\cite{bruni2014multimodal} propose an architecture for integrating text and image-based distributional information to improve performance on tasks where meaning is grounded in perception. Kiela and Bottou~\cite{kiela2014learning} show that integrating features extracted from images using CNN with skip-gram representation vectors improves performance on semantic similarity datasets. Lazaridou et al.~\cite{lazaridou2015combining} train visual representations extracted using CNNs together with skip-gram embeddings to integrate visual and textual information performing well in both semantic and vision tasks. Kiela et al.~\cite{kiela2017learning} train a sentence embedding model using grounded information extracted from image features by attempting to predict the image features. These embeddings improve performance on various NLP tasks in comparison to text only embeddings. They show that using this method for a dataset consisting mainly of abstract words is likely to less benefit from grounding information.  Shi et al.~\cite{shi2019visually} show that a syntactic parser can benefit from seeing images during training; however, it was later shown that the model mostly relies on noun concreteness (which we also elaborate on in our work) rather than more complex syntactic reasoning~\cite{kojima2020learned}.
The use of images for PCFG induction is also investigated by Jin \& Schuler~\cite{jin2020grounded}.

Along with the rise in visibility of jointly trained V\&L transformer models, a number of works have explored the use of these models for text-only tasks, with mixed results. Using terms coined by Sileo et al.~\cite{sileo2021visual}, these can be broadly split into \emph{associative grounding} and \emph{transfer grounding} approaches. Associative grounding uses retrieval methods to associate particular images with related texts; Kiros et al.~\cite{kiros2018illustrative} and Tan \& Bansal~\cite{tan2020vokenization} show that associative grounding methods may improve performance on various text-only NLU benchmark tasks. Transfer grounding applies V\&L models directly to text-only input, disregarding the vision component of the model during inference. Wang et al.~\cite{wang2021simvlm} apply this to weakly-supervised V\&L models to outperform BERT on various text-only tasks from the GLUE benchmark. On the other hand, Iki \& Aizawa~\cite{iki2021effect} find that V\&L-pretrained text encoders have similar or inferior results on NLU tasks including tasks from GLUE. Likewise, Cao et al.~\cite{cao2020behind} find that although visually-aligned text encoders perform well on semantic linguistic probing tasks, BERT still outperforms them.

As discussed above, some prior works suggest that multimodal pretraining aids text understanding while other works show that it can lead to degradation. Our work provides new context for these seemingly contradictory results, allowing them to be reassessed in the new context of visual vs. non-visual natural language understanding.