\section{Experimental Setup}

\subsection{Models Used}


For evaluating unimodally trained text encoders, we use BERT~\cite{devlin2018bert}, RoBERTa~\cite{liu2019roberta}, DistilBERT and DistilRoBERTa~\cite{sanh2019distilbert}, which are all trained with text-only MLM objectives. We also include results for Sentence-BERT (SBERT)~\cite{reimers2019sentence}, since its output embeddings are trained to have meaningful cosine similarity scores and thus bear more similarity to other models evaluated with Stroop proving. Results on multimodally trained text encoders are reported for CLIP~\cite{radford2021learning} and FLAVA~\cite{singh2022flava}; for these models we use only the text encoder with pretrained weights and discard the other subcomponents. Our tests include checkpoints from both OpenAI and the OpenCLIP open-source implementation of CLIP~\cite{cherti2022reproducible, ilharco_gabriel_2021_5143773}. Details of the checkpoints used for each model are listed in the supplementary material.


The text encoders of the multimodally trained models range in size from 63M (CLIP) to 109M (FLAVA) parameters. We compare to both comparably small unimodally trained text encoders such as DistilBERT (66M parameters) as well as much larger text encoders such as BERT-large (340M). See the supplementary material for an exhaustive list of sizes of the models under consideration.

We use each model with frozen pretrained weights. Our subsequent tests probe the contents of the feature vectors extracted by these models. For MLM probing, we also use the model's MLM head for prediction. In cases where MLM can be used we have found it to outperform Stroop probing; in such cases we report results for MLM probing here and for Stroop probing in the supplementary material.


\subsection{Probing Methods} \label{sec:probing}

In order to probe the inherent knowledge of our models, we use the knowledge probing methods described below. The probing methods that follow are strictly zero-shot; in the supplementary material we analyze the use of linear classifiers trained on our models' frozen embeddings (``linear probing'').


\medskip \noindent \textbf{Masked language modelling (MLM)}. BERT and our other unimodally trained models were all pretrained with MLM objectives and hence can be used for zero-shot prediction of tokens in a masked context. Given a text including a \MASK{} token and a set of $k$ possible completions $C = \{c_1, c_2, \cdots, c_k\}$, a MLM assigns probabilities $p_1, \cdots, p_k$ to each corresponding token. We use $\arg \max_i p_i$ as the model's prediction. Previous works have found that BERT and other MLM can be probed for innate knowledge with this method~\cite{petroni2019language,rogers2020primer}.
    

\medskip \noindent \textbf{Stroop probing (SP)}. We propose another zero-shot probing method to extract knowledge from models based on the pooled embeddings that they extract. Consider a masked text $t_m$ and possible completions $c \in C$, and let $t_c$ be the text with $c$ inserted in the mask location. Given a text encoder $M$, we calculate pooled embeddings $v_m = M(t_m)$ and $v_c = M(t_c)$ %
and unit-normalize them to $\hat{v}_m = v_m / \|v_m\|$ and $\hat{v}_c = v_c / \|v_c\|$. Stroop probing considers the cosine similarity scores $s_c := \hat{v}_m \cdot \hat{v}_c$. These can be used either directly for regression (as in the concreteness task below), or for categorical prediction by selecting $c^* = \arg \max_c s_c$.

The intuition behind Stroop probing is that items which are more surprising, incongruous, or salient in the given context may have a stronger interference effect on the encoding of the surrounding text. This is analogous to the \emph{Stroop effect} in human psychology. When presented with congruent and incongruent stimuli such as color words printed in the same or differing colors (e.g. ``red'' printed in blue), readers take significantly longer on average to read the incongruent stimuli, a phenomenon known as the \emph{Stroop effect}~\cite{macleod1991half}\footnote{For example, try saying these colors out loud (not the printed words): {\color{red}Green},  {\color{green}Red}, {\color{purple}Blue}, {\color{green}Purple}, {\color{blue}Red}, {\color{red}Purple}. }. We use Stroop probing for multiple tasks, including predicting color associations, as described below.

\subsection{Prompts Used}

For each task, we test the probing methods above on a wide variety of prompts in order to show the robustness of the described phenomena. In our results below we report the maximum metric value for each model over all of the prompts, since this represents a rough bound on our ability to extract intrinsic knowledge from the models under consideration. A full list of prompts used for each task and an analysis of model performance across prompts are provided in the supplementary material.

In some cases our prompt contains an empty slot, which we indicate below as $\SMASK$. Some models under consideration have a dedicated mask token, but for those such as CLIP that do not, we insert a fixed token in this slot, detailed further in the supplementary material.

