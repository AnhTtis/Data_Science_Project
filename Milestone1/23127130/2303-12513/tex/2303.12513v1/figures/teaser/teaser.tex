\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\thispagestyle{empty}
\begin{center}
\vspace{-4pt}
  \centering
        \jsubfig{\includegraphics[height=2.85cm]{figures/teaser/present.png}}{Alex gave Riley a present}
    \hfill
    \jsubfig{\includegraphics[height=2.85cm]{figures/teaser/ultimatum.png}}{Alex gave Riley an ultimatum}
    \hfill \hfill \hfill \jsubfig{\includegraphics[height=2.85cm]{figures/teaser/ocean.jpeg}}{the ocean is \SMASK-colored}
    \hfill
    \jsubfig{\includegraphics[height=2.85cm]{figures/teaser/chip-51253863514_2c1522f346_k.jpg}}{a corn chip is \SMASK-shaped}
    \begin{tabularx}{\textwidth}{cc}
    \hspace{11pt}\textbf{How concrete are the words \emph{present} and \emph{ultimatum}?} & \hspace{24pt} \textbf{What word should be inserted in the blank?}
    \end{tabularx}
    
    \captionof{figure} {In this paper, we propose a suite of \emph{visual language understanding} tasks for probing the visual reasoning capabilities of text encoder models. While we consider text-only tasks (i.e., processing only the textual descriptions above without associated imagery), we argue that they require visual imagination to complete and can thus benefit from vision-and-language pretraining. For instance, consider the words \emph{present} and \emph{ultimatum}. A simple online query (that considers only freely-available images) roughly yields a coherent set of images for the more concrete word (namely \emph{present}), while the latter cannot be uniquely depicted. Likewise, selecting the most natural color or shape descriptors in cloze contexts as shown in the two examples on the right requires implicit knowledge of the appearance of the referent under consideration (\emph{ocean} and \emph{corn chip} respectively).
    }
  \label{fig:teaser}
\end{center}
}]