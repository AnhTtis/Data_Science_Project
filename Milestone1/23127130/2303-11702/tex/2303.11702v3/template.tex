\documentclass{article}

\usepackage{arxiv}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}  
\usepackage{hyperref}      
\usepackage{url}           
\usepackage{booktabs}       
\usepackage{amsfonts}       
\usepackage{nicefrac}       
\usepackage{microtype}     
\usepackage{lipsum}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{diagbox}
\usepackage{subcaption}
\usepackage{lineno}
\usepackage{biblatex}
\usepackage{algorithm} 
\usepackage{algpseudocode} 
\usepackage{amssymb}
\usepackage{caption} 
\usepackage[flushleft]{threeparttable}
\usepackage{setspace}
\usepackage{wrapfig}
\usepackage[dvipsnames]{xcolor}

\captionsetup[table]{skip=10pt}

\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{1.5}


\addbibresource{references.bib}

\title{On the link between generative semi-supervised learning and generative open-set recognition}

\author{
  Emile Reyn Engelbrecht \\
  Main and corresponding author \\
  Electronic Engineering \\
  Stellenbosch University \\
  South Africa \\
  \texttt{18174310@sun.ac.za} \\
   \And
  Johan A. du Preez \\
  Co-author \\
  Electronic Engineering \\
  Stellenbosch University \\
  South Africa \\
  \texttt{dupreez@sun.ac.za} \\
}

\begin{document}
\maketitle
\begin{abstract}
This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) under the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require their generators to produce samples in the complementary space, which are then used to regularise their respective classifier networks. In turn, classifiers trained under SSL and OSR generalise the open space by tightening classification boundaries around the labelled categories. In other words, a classifier trained using an SSL-GAN intrinsically achieves OSR and vice-versa. To prove this SSL-OSR link, we theoretically and experimentally compare the state-of-the-art SSL-GAN with the state-of-the-art OSR-GAN. Our results find that all SSL-GANs and OSR-GANs work towards the same goal, and that the SSL-optimised Margin-GANs set the new state-of-the-art for the combined task of SSL-OSR. Future studies could further explore the theoretical similarities between SSL-GANs and OSR-GANs, as well as extend SSL-OSR to other learning policies. 
\end{abstract} 

% keywords can be removed
\keywords{Inductive classification \and Novelty detection \and Open-set recognition \and Semi-supervised learning \and LACU \and Unknown category \and Augmented category }



%labelling vs annotation
%Classifier vs model

\section{Introduction}
Classifier networks are trained to categorise input data samples into pre-labelled categories. However, classifiers must also be equipped to handle novel categories that may emerge over time~\cite{geng2020recent}. Two critical applications of such classifiers are 1) automated diagnostic tools~\cite{pang2021semi, pahar2021automatic} and 2) self-driving cars~\cite{wu2019semi}. In both cases, misclassifications can lead to fatal consequences. To ensure safe classifications, classifiers must be able to detect and separate novel categories that were not present during training but appear during testing. As per the examples, 1) classifiers should call on human doctors when encountering an unknown disease (e.g. SARS-CoV-2 pre-December 2020~\cite{andersen2020proximal}), and 2) classifiers must alert driving modules to safely manoeuvre out of the way or demand manual takeover in case of unexpected driving scenes (e.g. unexpected obstacles~\cite{ramos2017detecting}, or high-risk scenarios~\cite{puertas2021should}).

Open-set recognition (OSR) is the research field that tests classifiers' ability to handle both labelled and novel categories that were unobserved during training~\cite{geng2020recent}. More specifically, OSR defines an open testing domain, meaning classifiers must accurately categorise test samples that belong to one of $K$ labelled categories and accurately separate test samples that belong to unobserved novel categories into a $K + 1$'th augmented unknown category. It is important to note that OSR is independent of the training method used as it only inserts novel categories during testing. However, most OSR studies apply supervised learning to train classifiers, which is known to be costly in many real-world applications~\cite{van2020survey}. Instead, training under the semi-supervised learning (SSL) policy, i.e. off sparsely labelled datasets, and testing classifiers under the OSR regime would significantly improve the cost-efficiency of OSR classifiers. 

SSL and OSR are rarely linked within research, yet, SSL and OSR share striking similarities in their proposed training methods, particularly when using generative adversarial networks (GANs) (SSL -~\cite{NIPS2016_8a3363ab, dai2017good, NEURIPS2019_517f24c0, li2020semi} and OSR -~\cite{neal2018open, jo2018open, chen2021adversarial}). Consider the classification boundaries of a typical closed-set classifying network with a softmax output activation function for $K$ output nodes depicted in Fig.~\ref{fig:1a} and Fig.~\ref{fig:1b}. Conventional closed-set classifiers aim to maximise the classification boundaries between categories to allow accurate classification of each category. However, maximising boundaries is insufficient for SSL~\cite{dai2017good} and OSR~\cite{chen2021adversarial}. More specifically,  SSL classifier must also generalise the complementary space~\cite{NIPS2016_8a3363ab, dai2017good} (see Fig. \ref{fig:2a}), and OSR classifier must generalise the open space~\cite{chen2021adversarial} (see Fig. \ref{fig:2b}). This study will show that generalising the complementary space inherently generalises the open space. As to say, SSL-GANs and OSR-GANs achieve the same goal. 

The specific methodology behind SSL-GANs and OSR-GANs is to generalise an additional $K + 1$'th category to represent the necessary embedding spaces as theorised in each field. As previously mentioned, SSL-GANs aim to generalise the complementary space in the $K + 1$'th category and OSR-GANs aim to generalise the open space in the $K + 1$'th category. To do so, SSL-GANs and OSR-GANs design their generators to produce samples that, when pseudo-labelled into the $K + 1$'th category, ensure the classifier generalises the necessary embedding spaces. However, it is interesting that both SSL-GANs and OSR-GANs rely on these generated samples looking 'bad' by some definition. This study postulates that bad-looking generated samples in both SSL-GAN studies and OSR-GAN studies always lie in the complementary space, when pseudo-labelling these bad-looking samples into the $K + 1$'th category, SSL and OSR classifiers inherently generalise the open space in the $K + 1$'th category. 
 
\begin{figure*}[!t]
\centering
\subfloat[Example domain]{\includegraphics[width=0.35\textwidth]{1.png}
\label{fig:1a}}
\hfil
\centering
\subfloat[Classification boundaries]{\includegraphics[width=0.35\textwidth]{2.png}
\label{fig:1b}}
\caption{Different classification boundaries of a 2D example domain with $K = 3$ - (a) describes the 2D domain with each category represented by a different shape and colour; (b) describes the classification boundaries of a typical closed set classifier with a softmax output activation function.}
\label{fig:1}
\end{figure*}

To prove the link between SSL-GANs and OSR-GANs, several GAN models are theoretically described and experimentally compared. For SSL, we study feature-matching (FM)-GANs~\cite{NIPS2016_8a3363ab}, which set the foundation of complementary space theory, and Margin-GANs~\cite{NEURIPS2019_517f24c0}, which currently hold state-of-the-art SSL results for GAN models. For OSR, we study adversarial reciprocal point (ARP)-GANs~\cite{chen2021adversarial}, which exhausts open space theory to achieve state-of-the-art OSR results, noting that ARP-GANs outperform other OSR models by significant degrees. Each GAN is thoroughly described concerning its setup and loss functions. These descriptions make it clear that all three models aim to produce samples that lie in the complementary space and, since all models use generated samples to regularise their classification networks, our hypothesis suggests that all classifiers will generalise the open space.

To experimentally validate our hypothesis, each model is placed under conventional SSL and OSR settings. However, to ensure systematic analysis, the experiments are conducted as follows. First, FM-GANs are inserted into the ARP-GAN code base to determine the correlation between SSL-GANs and OSR-GANs under the same optimisation framework (i.e. an OSR-optimised framework). Subsequently, the same experiments are conducted using Margin-GANs, but under its own code base (i.e. an SSL-optimised framework). Using two differently optimised code bases allows us to draw definitive conclusions about the SSL-OSR link. Our results find that FM-GANs achieve near identical results to ARP-GANs under the same optimisation framework. Furthermore, the SSL-optimised Margin-GANs are found to set the new standard for the combined task of SSL-OSR experiments, while also achieving competitive results for conventional OSR.  

Our results conclude that SSL-GANs and OSR-GANs rely on the same theory. Specifically, SSL-GANs and OSR-GANs regularise their classifiers using generated samples that lie in the complementary space, leading to the classifiers generalising the open space. Future studies could optimise OSR-GANs for the SSL task to improve the competitiveness of these models and apply OSR theory within SSL-GANs for improved results. Furthermore, future studies can further extend the SSL-OSR link to other related fields for enhanced practicality and cost-efficiency. The remainder of this paper is structured as follows - Section~2 provides background information on SSL, OSR, and the previous research on the links between these fields; Section~3 provides the necessary theory on FM-GANs, ARP-GANs and Margin-GANs; Section~4 conducts SSL and OSR experiments; Section~5 discusses the results and possible future studies; and Section~6 concludes the study. 

%+- 1000 words

\begin{figure*}[!t]
\centering
\centering
\subfloat[complementary space]{\includegraphics[width=0.35\textwidth]{4.png}
\label{fig:2a}}
\hfil
\centering
\subfloat[Open space]{\includegraphics[width=0.35\textwidth]{3.png}
\label{fig:2b}}
\caption{Visual representation of the embedding spaces generalised within the $K + 1$'th categories (a) shows the complementary space of the each category as required for SSL, which is depicted by the 'x's around each category; and (b) shows the open space of the domain which is represented by the white area as required for OSR.}
\label{fig:2}
\end{figure*}

\section{Background}
Open-set recognition (OSR) requires trained classifiers to accurately categorise samples that belong to $K > 1$ number of labelled categories and simultaneously separate samples that belong to any number of novel categories into an additional $K + 1$'th category. Novel categories are formally defined as groups of anomalous samples with similar patterns that do not match any of the labelled categories~\cite{gruhl2021novelty}. Within OSR, novel categories are further described as those that are unobserved during training but appear during testing~\cite{pimentel2014review, geng2020recent}. In other words, OSR ensures classifiers are capable of handling changing environments~\cite{din2021data}, also noting a strong relationship between OSR and anomaly detection~\cite{cevikalp2023anomaly}. With regards to training, OSR generally applies supervised learning that includes a labelled training set, $\mathcal{D}_{\text{train}}$, that contains sample pairs, $(x, y)$, where $x$ is the input sample and $y$ is the category label. After training, an independent testing set, $\mathcal{D}_{\text{test}}$, is used to determine the performance of the classifier.

Testing sets also consist of sample pairs, $(x, y_a)$, but the label is an anticipated label used to determine classifier performance. It is important to state that testing sets should mimic real-world scenarios as accurately as possible to ensure credible performance measurements. Let the set $C_K := \{1, 2, ..., K\}$ represent all labelled categories, with all entries in $C_K$ representing the unique category labels. For conventional supervised learning, all samples belong to one of the categories in $C_K$. In other words, all training and testing labels are in $C_K$ - i.e. all $ \{y \sim \mathcal{D}_{\text{train}}\} \in C_K$ and all $ \{y_a \sim \mathcal{D}_{\text{test}}\} \in C_K$. To extend supervised learning to OSR, samples from novel categories are inserted into the testing set. Assuming that novel categories need only be separated, let all novel categories be given the $K + 1$'th label. Thus, a supervised learning classifier tested under the OSR regime would have all $ \{y \sim \mathcal{D}_{\text{train}}\} \in C_K$ and all $ \{y_a \sim \mathcal{D}_{\text{test}}\} \in \{C_K \cup K + 1\}$. Specifically, OSR classifiers must generalise the $K + 1$'th augmented category to encapsulate all unobserved novel categories without any training samples available for this category. 

Semi-supervised learning (SSL) relaxes supervised learning by training off a labelled and unlabelled set, $\mathcal{D}_{\text{train}} := \mathcal{D}_{\text{lab-train}} \cup \mathcal{D}_{\text{unlab-train}}$. Similar to the test set, samples in $\mathcal{D}_{\text{unlab-train}}$ are represented as $(x, y_a)$, where $x$ is the input sample, and $y_a$ is the anticipated category label. This study focuses on inductive SSL where the testing set is again independent of the training set~\cite{van2020survey}. Conventional SSL proposes the cluster assumption~\cite{van2020survey}, meaning all labelled and unlabelled training samples belong to one of the labelled categories - i.e. all $ \{y \sim \mathcal{D}_{\text{lab-train}}\} \in C_K$ and all $ \{y_a \sim \mathcal{D}_{\text{unlab-train}}\} \in C_K$. Furthermore, conventional SSL assumes that all all $ \{y_a \sim \mathcal{D}_{\text{test}}\} \in C_K$. Similar to supervised learning, SSL can be extended to OSR by simply changing the test set to all $ \{y_a \sim \mathcal{D}_{\text{test}}\} \in \{C_K \cup K + 1\}$. However, including an unlabelled training set provides an extra set wherein novel categories could exist. 

Given the immense cost to label every category in an SSL training set~\cite{engelbrecht2020learning}, recent studies have focused on the more application-grade setting where $\mathcal{D}_{\text{unlab-train}}$ also contains novel categories. These research fields include open-set semi-supervised learning (OSSSL)~\cite{yu2020multi}, learning with augmented category by exploiting unlabelled data (LACU)~\cite{engelbrecht2020learning}, and mismatched/safe/auxiliary semi-supervised learning (MSSL)~\cite{chen2020semi, he2022safe, banitalebi2022auxmix}. However, these research fields consider what we refer to as observed novel categories since the classifier can access samples from these novel categories during training. Although observed novel categories represent an important SSL setting, this study only focuses on conventional SSL and OSR. In other words, we assume that there are no novel categories seen in $\mathcal{D}_{\text{unlab-train}}$ and that only unobserved novel categories appear during testing. Extending the SSL-OSR link to also consider observed novel categories is discussed further in Section~5. 

GANs have been extensively used for conventional SSL~\cite{sajun2022survey}. Initially, SSL-GANs transformed the discriminator to both distinguish between real and fake samples and simultaneously classify any real samples into a labelled category~\cite{odena2016semi, NIPS2016_8a3363ab, dai2017good, xu2021semi}. However, state-of-the-art models now prefer a three-network game, wherein the discriminator remains unchanged to the original GAN and a separate classifier is added to the model~\cite{dong2019margingan, li2021triple}. Of all SSL-GANs, the bad-GAN by Dai et al.~\cite{dai2017good} is especially noteworthy, as Dai theoretically and experimentally showed that good SSL-GANs must generate 'bad-looking' samples that lie in the complementary space. More specifically, Dai found that when an SSL classifier is regulated using generated samples around the classification boundaries of labelled categories (see Fig.~\ref{fig:2a}), the classifier is forced to tighten classification boundaries around labelled categories for improved generalisation. This approach is interesting since several OSR-GANs have used the same notion of bad-looking samples.

Before exploring OSR-GANs, it is essential to consider the use of GANs for conventional novelty detection. Novelty detection is a one-class classification problem that distinguishes unobserved novel categories from a single positive labelled category~\cite{pimentel2014review}. The discriminator networks in GANs have been widely studied in this context~\cite{perera2019ocgan, sabokrou2018adversarially, zhang2021adversarially}. Specifically, because the discriminator is tasked to separate fake-generated samples from all training samples, the network could be an ideal novelty detector if we consider fake-generated samples as the open domain. In other words, the discriminator separates any sample outside the training domain, including unobserved novel categories. However, the theory of GANs suggests that an ideal discriminator that has reached equilibrium with the generator cannot determine whether an input sample is real or fake. Therefore, unique adjustments are required to use a discriminator as a novelty detector. 

A standard method to ensure the discriminator can determine whether a sample is from the training domain or not, even at equilibrium with the generator, is to design the generator to produce bad-looking samples~\cite{perera2019ocgan, sabokrou2018adversarially}. This same notion has been applied in OSR-GANs~\cite{neal2018open, jo2018open, chen2021adversarial}, noting that each study has unique definitions for these bad-looking samples. For example, the counterfactual (counter)-GAN by Neal et al.~\cite{neal2018open} combines an auto-encoder and a GAN to generate counterfactual images, which is an image originally belonging to one category but drawn towards another. In contrast, Jo et al.~\cite{jo2018open} use a feature-matching (FM)-GAN and a denoising auto-encoder to generate samples that are corrupted with the noisy distribution of the feature space of the classifier. Then, the recent adversarial reciprocal point learning GAN (ARP-GAN) proposed by Chen et al. also uses bad-looking samples that match what Chen defines as reciprocal points~\cite{chen2021adversarial}. We note that ARP-GANs significantly outperformed all other OSR-GANs, and will be discussed in detail in the upcoming section. 

Although SSL-GANs and OSR-GANs both theorise better generalisation given bad-looking generated samples, these methods have not yet been compared within research. Our hypothesis states that the notion of bad-looking samples in either the SSL or the OSR context really represents samples in the complementary space. In other words, counterfactual samples, samples modelled with noisy distributions, and samples based on reciprocal points all allow classifiers to generalise the open space for the same reasons SSL models perform better when generalising the complementary space. That is, regularising a classifying network with samples in the complementary space inherently generalises the open space within the network. Consequently, an SSL-GAN would also be capable at OSR, and an OSR-GAN would also be capable at SSL. Furthermore, all these models could be used for the combined task of SSL-OSR. 

Finally, it is also important to note other studies that focused on the combined task of SSL-OSR. Capazzo et al.~\cite{cappozzo2020anomaly} used traditional machine-learning techniques to address label noise under the SSL-OSR setting. However, these traditional techniques are unsuitable for high-dimensional data. Another attempt at SSL-OSR is in the unpublished study by Kliger et al.~\cite{kliger2018novelty2}, who also applied GANs in this context. However, their proposed method and results were unclear, and no theoretical link was made between SSL-GANs and OSR-GANs. Finally, in the study by Sung et al.~\cite{sung2019difference}, manually chosen operations were applied to the training data, after which FM-GANs were used to match this pseudo-data. However, Sung et al. did not justify the choice of operations while they also experimented on SSL and OSR independently, without linking the two. In this study, we theoretically and experimentally link SSL with OSR under the unified framework of highly parameterised GANs.

%+- 1300 words

\section{GAN comparisons}
This section thoroughly describes FM-GANs (SSL), ARP-GANs (OSR) and Margin-GANs (SSL), explicitly highlighting their theoretical similarities. As we delve into each model, it is essential to pay attention to the $K + 1$'th categories. Specifically, all three models use the additional $K + 1$'th category to represent unique areas in the embedding space. FM-GANs and Margin-GANs use the $K + 1$'th category to represent the complementary space of each labelled category (as shown in Fig. \ref{fig:2a}). Similarly, ARP-GANs use the $K + 1$'th category to represent the open space where novel categories lie (as shown in Fig. \ref{fig:2b}). This study proposes that the $K + 1$ categories in these different models ultimately represent the same areas in the embedding space, leading all classifiers to function similarly. Please note that a basic understanding of GAN models is required for this discussion~\cite{goodfellow2020generative}. 

\subsection{Feature-matching GANs (SSL)}
FM-GANs provide vital insights into the link between SSL and OSR. Originally, FM-GANs were designed for SSL~\cite{NIPS2016_8a3363ab}, with multiple follow-up SSL studies also making use of these models~\cite{dai2017good, sung2019difference, li2020semi}. However, it is interesting that FM-GANs have also been used in multiple OSR studies~\cite{jo2018open, sung2019difference}. Here we describe FM-GANs as initially designed, explicitly focusing on the theory of 'bad-looking' samples for improved SSL performance~\cite{dai2017good}. FM-GANs transform the discriminator within the original GAN framework into a classifier network. Consequently, the classifier must distinguish between real and fake samples and classify all real samples into one of the $K$ labelled categories. However, extending the discriminator to a classifier requires unique design choices.  

The classifier network, denoted as $C(x)$ for input sample $x$, is regularly described as having $K + 1$ output nodes. The $K$ nodes are used for the labelled categories, while the $K + 1$'th node is used to separate fake samples~\cite{NIPS2016_8a3363ab, odena2016semi}. Therefore, following conventional supervised neural networks, the output of this classifier for a sample belonging to class $j$ is given as $p_{\text{class}}(y == j | x) = \frac{\text{exp}[C^{j}(x)]}{\sum_{i=1}^{K + 1} \text{exp}[C^{i}(x)]}$ (where $C^{i}(x)$ is the logit value of node $i$, and $j \in \{C_K \cup K + 1\}$). In turn, the loss function minimised by the classifier during training is a combination of a supervised loss for labelled training samples and a supervised loss for fake-generated samples. In other words, fake-generated samples are pseudo-labelled into the $K + 1$'th category. Let $G(z)$ represent the output of the generator given random noise $z$, which is sampled from the uniform distribution $\mathbb{P}^{\text{(z)}}$. The classifier loss function is then described as:


\begin{equation}
\begin{split}
 \text{C-Loss} = 
 & - \; \mathbb{E}_{(x, y) \sim D_{\text{lab-train}}} [\log(p_{\text{class}}(y|x) \;)] \\
 & - \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log( \; p_{\text{class}}(y = K + 1| \; G(z) \;)]
    \label{eq:cost_c_ssl}
\end{split}
\end{equation} 

For such a $K + 1$ classifier, the probability of a sample being fake (i.e. being classified into the $K + 1$'th category) is given as $p_{\text{class}}(y == K + 1 | x) = \frac{\text{exp}[C^{K + 1}(x)]}{\sum_{i=1}^{K+1} \text{exp}[C^{i}(x)]}$. However, it is important to discuss the definition of the $K + 1$'th category. The developers of FM-GANs noted that, since no testing sample is ever a 'fake' sample within the SSL policy, such a $K + 1$ output nodes classifier is really over-parameterised. Therefore and instead, the authors proposed fixing the $K + 1$'th output node to zero, i.e. $C^{K+1}(x) = 0 \; \; \forall \; \; x$. Now, the probability of a sample being fake is $p_{\text{class}}(y == K + 1 | x) = \frac{1}{\sum_{i=1}^{K} \text{exp}[C^{i}(x)] + 1}$. With this new criterion, the classifier network no longer has $K + 1$ output nodes but only $K$ output nodes similar to a conventional supervised classifier. However, to maintain the adversarial game within the FM-GAN, the classifier must still distinguish fake samples from real samples. 

To ensure the classifier separates real training samples from fake generated samples, the $K + 1$'th category must still exist within the classifier with only $K$ output nodes. Considering that the real-vs-fake protocol is a one-class classifier problem (i.e. a sample can only be real or fake) and that the probability of the sample being fake is $p_{\text{class}}(y == K + 1 | x)$, the probability of a sample being real would be given as $(1 - p_{\text{class}}(y == K + 1 | x)) = \frac{\text{exp}[C^{i}(x)]}{\sum_{i=1}^{K} \text{exp}[C^{i}(x)] + 1}$. If we minimise $p_{\text{class}}(y == K + 1 | x)$ for fake generated samples, and maximise $(1 - p_{\text{class}}(y == K + 1 | x))$ for real training samples, then we can maintain the real-vs-fake protocol in a classifier with only $K$ output nodes. With these criteria and the additional supervised loss function for labelled training samples, the classifier loss function in eq.~\ref{eq:cost_c_ssl} is transformed to: \footnote{See https://jostosh.github.io/ssl-gan/ for the full derivation}:

\begin{equation}
\label{eq:cost_c_fm}
\begin{split}
  \underset{\text{FM-GAN}}{\text{C-Loss}} \; \; = 
 & \; \; \; \; \; 0.5 \cdot \mathbb{E}_{(x, y) \sim D_{\text{train}}} [\text{softplus}(\log(\sum_{i = 1}^{K} \exp{C^i(x)})) -  \log(\sum_{i = 1}^{K} \exp{C^i(x)})] \\
 & + \; 0.5 \cdot \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\text{softplus}(\log(\sum_{i = 1}^{K} \exp{C^i(G(z))}))] \\
 & - \; \mathbb{E}_{(x, y) \sim D_{\text{lab-train}}} [\log(p_{\text{class}}(y|x))] 
\end{split}
\end{equation}

Minimising the above loss function ensures that the output logit values for real training samples will be high and the output logit values for fake generated samples will be low. In other words, the classifier is regularised by the pulling logit values down for fake-generated samples. However, to successfully regularise the classifier, fake-generated samples must lie in the complementary space~\cite{dai2017good}, or on the classification boundaries of labelled categories (see Fig.~\ref{fig:2a}). If fake-generated samples lie in the complementary space, then the FM-GAN classifier is forced to learn finer-details of the labelled categories since the classification boundaries become cluttered with real and fake samples. In turn, high-accuracy classification can be achieved using considerable fewer labelled training samples than fully-supervised counterparts. To ensure the generator learns the representation of the complementary space instead of the real-training distribution, FM-GANs propose the feature-matching (FM) loss function for the generator network. 

The FM loss function is based on the idea that the last hidden layer of the classifier network (denoted as $C'(x)$) represents the features of the input data. To match the features between the real training samples and the fake generated samples, the generator within an FM-GAN minimises the L2 distance between the activations of this hidden layer for real training samples and the activations for fake generated samples. Although the FM-loss function does not theoretically guarantee that generated samples will lie in the complementary space, it does ensure that generated samples will always look 'bad'. In other words, the generator can only learn a partial representation of the training distribution, ensuring that the classifier's regularisation will not affect the learning process. Formally, the FM-GAN generator loss function is given as follows:

\begin{equation}
\label{eq:cost_g_fm}
 \underset{\text{FM-GAN}}{\text{G-Loss}} \; = \; ||\mathbb{E}_{x \sim D_{\text{train}}} C'(x) - \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} C'(G(z))||_2^2
\end{equation}

In summary, FM-GANs convert the traditional two-player GAN game into a SSL framework by converting the discriminator to a classifier and designing the generator to produce bad-looking samples. Although various improvements have been proposed for FM-GANs, specifically to ensure generated samples lie in the complementary space~\cite{dai2017good, jo2018open}, this study focuses on FM-GANs, which are the foundation for multiple SSL-GANs, and can be easily inserted into a different code base thanks its enhanced stability~\cite{NIPS2016_8a3363ab}. By learning the representation of the complementary space within the generator network and labelling generated samples into the $K + 1$'th category (albeit it fixed to zero), we hypothesise that the $K + 1$'th category in FM-GANs would end up generalising the open space. More concretely, because generated samples lie around the classification boundaries of labelled categories, the $K + 1$'th category ends up representing all outside these boundaries, which is the open space (see Fig~\ref{fig:2b}).

%Word count: 1100

\subsection{Adversarial reciprocal point GANs (OSR)}
To detect unobserved novel categories, classifiers must generalise the open space, which is 'all that the training distribution is not'. Several studies have proposed generative methods to generalise the open space within the additional $K + 1$'th category~\cite{neal2018open, jo2018open, sung2019difference}. Specifically, generative networks are designed to produce samples that, when placed into the $K + 1$'th category, would appropriately generalise the open space. The hypothesis of this study states that the generated samples from these OSR studies are theoretically equivalent to those produced by SSL-GANs (i.e. they lie in the complementary space). Although several generative OSR approaches have been developed, we describe ARP-GANs by Chen et al.~\cite{chen2021adversarial}, which currently holds state-of-the-art OSR results by significant degrees. Specifically, ARP-GANs propose constraining the open space within the centre of the embedding space by making use of reciprocal point learning.  

Let each of the $K$ labelled categories be assigned a reciprocal point, $P^j$, which is a prototype (i.e. a cluster centre) of everything that category $j$ is not. More specifically, $P^j$ is a learnable feature vector that must be updated until it lies in the centre of the combined embedding space of all other labelled categories ($\neq j$) and the open space. The probability of a sample belonging to category $j$ can then be defined based on the distance of the sample to the category's reciprocal point, $P^j$. The sample is classified into the category with the farthest reciprocal point, as determined by a chosen distance metric. Formally, ARP-GANs define probability scores as $p_{\text{class-ARP}}(y == j | x) = \frac{\text{exp}[d(C(x), P^j)]}{\sum_{i=1}^{K} \text{exp}[d(C(x), P^i)]}$, where $d(C(x), P^j)$ is the distance between the reciprocal point and the classifier's output for input sample $x$. However, to produce $p_{\text{class-ARP}}(y == j | x)$, classifiers within ARP-GANs have a different output design than convetional supervised classifiers. 

ARP-GANs do not rely on general neural network classification techniques (i.e. one-hot encoded labels and a cross-entropy loss). Instead, the distance, $d(C(x), P^j)$, between input samples and reciprocal points represent the classifier's logits. Therefore, to ensure uniformity within $C(x)$ and $P^j$, the classifier's output must match the dimensions of the reciprocal points, which is 128 within the original study. Subsequently, ARP-GANs can define a distance metric to calculate the logits, which ARP-GANs define as the difference between the Euclidean distance and the dot product - i.e. $d(C(x), P^j) = d_e(C(x), P^j) - d_d(C(x), P^j) = (\frac{1}{m} \; \cdot \; || C(x) - P^j ||^2_2) \; \; - \; \; (C(x) \cdot P^j)$. During training, ARP-GANs maximise $d(C(x), P^j)$ between labelled training samples and their categories' corresponding reciprocal points. In turn, the classification boundaries of labelled categories are pushed towards the edges of the embedding space. However, only maximising $d(C(x), P^j)$ does not generalise or bind the open space. 

To generalise the open space within a $K + 1$'th category, classifiers in ARP-GANs bind the Euclidean distance ($d_e(C(x), P^j) = (\frac{1}{m} \; \cdot \; || C(x) - P^j ||^2_2)$) between labelled training samples and their corresponding reciprocal points to a learned range, $R$. By doing so for multiple labelled categories (i.e. $K > 1$), the open space will be constrained to the centre of the embedding space in between all labelled categories. In other words, the classifier maximises $d(C(x), P^j)$ to push the boundaries of labelled categories to the edges of the embedding space, but contains the distance of these boundaries and their corresponding reciprocal points to remain smaller than $R$. Consequently, an adversarial mechanism is created by 1) pushing the boundaries towards the edges, but 2) also pulling them back towards the centre. In turn, the open space will be constrained to the centre of the embedding space (see Fig.~3 in~\cite{chen2021adversarial}). With $\gamma$ set to $\gamma = 0.1$, the adversarial loss function for the classifier network in ARP-GANs is described as: 

\begin{equation}
\begin{split}
    \underset{\text{ARP}}{\text{C-Loss}} = 
    &- \; \mathbb{E}_{(x, y) \sim D_{\text{lab-train}}} [\log(p_{\text{class-ARP}}(y|x))] \\
    & + \gamma \cdot \text{max}(d_e(C(x), P^j) - R, 0)
    \label{eq:cost_c_arp}
\end{split}
\end{equation}

$\text{C-Loss}_{\text{ARP}}$ is the categorical cross-entropy equivalent for supervised classifiers based on reciprocal points. However, this setup does not yet contain a GAN. To further improve results, Chen et al.~\cite{chen2021adversarial} appended an additional GAN criterion to $\text{C-Loss}_{\text{\; ARP}}$. In contrast to the two network setup of FM-GANs, ARP-GANs have three networks - a classifier, a discriminator and a generator. The discriminator in ARP-GANs is left unchanged to the original GAN (see eq. \ref{eq:cost_d_arpgan} below). However, the ARP-GAN generator in ARP-GANs is tasked to match a mixture distribution between the real-training data and the reciprocal points, creating a so-called confused generator. In other words, the generator must match the real data distribution by learning from the discriminator, but also pull generated samples towards the learned reciprocal points per the classifier's embedding space. The classifier then minimises the distance between all reciprocal points and the generated samples, which further constrains the open space to the centre of the embedding space for improved OSR results. 

To include this additional criterion between the generator and the classifier, ARP-GANs use the information entropy loss. Specifically, given $S(G(z), P^j) = \text{softmax}(d_e(C(G(z)), P^j))$, the information entropy loss between generated samples and reciprocal points is defined as $I(G(z)) = - \sum^{K}_{i = 1} S(G(z), P^i) \cdot \log(S(G(z), P^i))$. Subsequently, the generator and classifier maximise $I(G(z))$ to pull generated samples towards the reciprocal points and reciprocal points towards generated samples. The three loss functions for the discriminator, generator and classifier in ARP-GANs are formally given as:

\begin{equation}
\begin{split}
    \underset{\text{ARP-GAN}}{\text{D-Loss}} = 
    & - \; \mathbb{E}_{(x) \sim D_{\text{train}}} [\log(D(x))] \\
    & - \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(1 - D(G(z)))] 
    \label{eq:cost_d_arpgan}
\end{split}
\end{equation}

\begin{equation}
    \underset{\text{ARP}-GAN}{\text{G-Loss}} = - \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(D(G(z))) \; + \; I(G(z))]
    \label{eq:cost_g_arpgan}
\end{equation}

\begin{equation}
\label{eq:cost_c_arpgan}
\begin{split}
\underset{\text{ARP-GAN}}{\text{C-Loss}} = 
& - \; \mathbb{E}_{(x, y) \sim D_{\text{lab-train}}} [\log(p_{\text{class-ARP}}(y|x))] \\
& + \gamma \cdot \text{max}(d_e(C(x), P^j) - R, 0) \; \\
& - \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [I(G(z))]
\end{split}
\end{equation}

An ideal ARP-GAN achieves three goals: 1) it pushes the classification boundaries of the labelled categories towards the edges of the embedding space, 2) it constrains the open space within the centre of the embedding space, and 3) it generates samples that either lie in the open space or lie in-between the open space and the boundaries of labelled categories (see Fig.~7d in~\cite{chen2021adversarial}). However, the immediate question is whether or not the samples that lie in-between the open space and the boundaries of labelled categories really fall within the complementary space. If so, and if we also comprehend the information entropy loss function in eq.~\ref{eq:cost_c_arpgan} as a regularisation loss similar to that of FM-GANs, then ARP-GANs can be concluded to achieve the same goal as FM-GANs. More specifically, ARP-GANs and FM-GANs generalise the open space in the $K + 1$'th category by regularising a classifier with generated samples that lie in the complementary space. 

%+- 1000 words

\subsection{Margin-GANs (SSL)}
To further demonstrate the link between the complementary space and the open space, it is important to also discuss the state-of-the-art SSL-GAN, namely Margin-GANs~\cite{dong2019margingan}. Margin-GANs apply a unique GAN framework to the mean-teacher (MT) SSL model~\cite{tarvainen2017mean}. The MT model is based on a student-teacher training approach, where the teacher network is the exponential moving average of the student network's parameters. In addition to a general supervised loss for labelled data, the student network is trained on unlabelled data by minimising a consistency loss (such as the mean squared error) between its predictions for the unlabelled data and the teacher network's predictions for the same data. This consistency loss ensures that the student's predictions remain consistent throughout its updates and provides feedback to the model for unlabelled training samples.

It is important to highlight the similarity between MT models and pseudo-labelling models~\cite{lee2013pseudo, pham2021meta}. Pseudo-labelling uses the classifier network's output predictions for unlabelled data as the ground-truth labels for these samples, thus reinforcing the network's predictions. However, this approach is heavily dependent on the accuracy of these predictions, as any incorrect pseudo-label would negatively impact the model's generalisation. MT models offer improved performance since the "pseudo-labels" generated by the model are not treated as ground-truth labels but rather as soft-pseudo labels overall $K$ categories. Margin-GANs build on the MT model by incorporating a GAN into the training process through another three-player game between the discriminator, generator, and student-teacher classifier.

In Margin-GANs, the discriminator's task is similar to that of vanilla GANs, which is to distinguish between real training samples in $D_{\text{train}}$ and generated samples $\mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} G(z)$. However, unlike vanilla GANs and similar to ARP-GANs, the generator in Margin-GANs must deceive both the discriminator and the student-teacher classifier into believing that the generated samples are real. As a result, the student-teacher network also plays a crucial role in the adversarial game by having to accurately differentiate between real and fake samples. However, Margin-GANs propose a unique method to separate real and fake samples using a classifier network. Here we note that Margin-GANs do not explicitly discuss a $K + 1$'th category in its setup. However, using their proposed inverse cross-entropy loss function, Margin-GANs also regularise their classifier networks in the same fashion as FM-GANs. In other words, Margin-GANs also fix the $K + 1$'th category to zero. 

Margin-GANs propose margin theory to regularise the classifying network. The classification margin of an input sample is defined as the difference between the predicted probability for the sample's true category and the highest predicted probability for all other categories (i.e. $\text{margin}(x, y) = C_y(x) - \max_{i \neq y}C_i(x)$, where $C_i$ is the classifier's output prediction at category $i$, and $y$ is input sample $x$'s true category). With this definition, Margin-GANs have their classifiers minimise the margins of fake-generated samples, while the generator maximises the margins of fake-generated samples, creating an adversarial mechanism similar to ARP-GANs. To minimise margins of generated samples, the classifier network minimises the inverted cross-entropy in the form of $- \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(1 - p_{\text{class}}(y'|G(z)))]$, where $y'$ is the hard pseudo-label for the generated sample. To maximise margins, the generator minimises the general cross-entropy of a sample, also given its hard pseudo label: $- \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(p_{\text{class}}(y'|G(z)))]$. The loss functions for the three networks in Margin-GANs are given as follows:

\begin{equation}
\label{eq:cost_g_mg}
\begin{split}
   \text{G-Loss}_{\text{\; MG}} = 
   & - \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(D(G(z))) \\
   & - \; \log(p_{\text{class}}(y'|G(z)) \;)]   
\end{split}
\end{equation}

\begin{equation}
 \label{eq:cost_d_mg}
 \begin{split}
 \text{D-Loss}_{\text{\; MG}} =
 & - \mathbb{E}_{x \sim D_{\text{train}}} [\log(D(x))] \\
 & - \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(1 - D(G(z)))]
 \end{split} 
\end{equation}

\begin{equation}
\label{eq:cost_c_mg}
\begin{split}
  \text{C-Loss}_{\text{\; MG}} = 
 & - \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(1 - p_{\text{class}}(y'|G(z)) \;) \\
 & - \; \mathbb{E}_{(x, y) \sim D_{\text{lab-train}}} [\log(p_{\text{class}}(y|x))] \\
  & - \; \mathbb{E}_{(x) \sim D_{\text{unlab-train}}} [\log(p_{\text{class}}(y'|x))]  \\
\end{split}
\end{equation}

The last term of the classifying network cost function in eq.~\ref{eq:cost_c_mg} represents the MT pseudo-labelling cost function for unlabelled training samples. However, as mentioned above, MT models make use of soft pseudo-labels of a teacher network. Therefore, instead of this hard pseudo-labelling cost function, Margin-GANs apply the consistency loss (mean squared error) between the student and teachers’ predictions for unlabelled training samples. Let $T(x)$ represent the teacher network (which is the exponential moving average of the student’s parameters), then the classifier’s loss function for Margin-GANs becomes:

\begin{equation}
\label{eq:cost_c_mg2}
\begin{split}
  \text{C-Loss}_{\text{\; MG-MT}} = 
 & - \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(1 - p_{\text{class}}(y'|G(z)) \;) \\
 & - \; \mathbb{E}_{(x, y) \sim D_{\text{lab-train}}} [\log(p_{\text{class}}(y|x))] \\
  & - \; \mathbb{E}_{(x) \sim D_{\text{unlab-train}}} [(C(x) - T(x))^2]  \\
\end{split}
\end{equation}

The adversarial game between the generator and classifier within a Margin-GAN can be described as 1) the generator aims to produce samples that the classifier would place inside the classification boundaries of labelled categories, and 2) the classifier aims to ensure all generated samples fall outside the classification boundaries of labelled categories. Intuitively, the only equilibrium of such an adversarial game is where generated samples lie on the boundaries of labelled categories (i.e. the complementary space). If we again comprehend the classifier's loss function in this game as a regularisation term, the Margin-GANs achieve the same goal as FM-GANs and ARP-GANs. In other words, the state-of-the-art SSL Margin-GANs also generalise the open-space, and so are capable of OSR. 

%+- 700 words

\section{Experiments}
\begin{table*}[t]
\centering
%\begin{tabular}{m{3.2cm} m{2.2cm} m{2.2cm} m{0.3cm} m{2.2cm} m{2.2cm}}
\begin{tabular}{ p{3.2cm} p{2.2cm} p{2.2cm} p{0.3cm} p{2.2cm} p{2.2cm} }
\hline
 \multicolumn{1}{c}{} & \multicolumn{2}{c}{\textbf{Semi-supervised}} & \multicolumn{3}{c}{\textbf{Supervised}} \\
\hline
    & SVHN & CIFAR10 & & CIFAR10$\dagger$ & CIFAR10$\star$ \\
Labels per class & 100     & 400  & & Full  & Full \\
\hline
\textbf{Baselines} & & & & &  \\
Softmax & 13.83 $|$ 41.18 & 62.66 $|$ 67.04 &   & 90.24 $|$ 83.79 & 93.36 $|$ 85.26 \\
ARP     & 10.32 $|$ 53.65 &  61.54 $|$ 69.46 &   & 89.19 $|$ 84.53 & \textit{93.90} $|$ \textit{90.10 }   \\
\hline
\textbf{GANs} & & & & &  \\
FM-GAN  & 83.40 $|$ 88.02 & 78.70 $|$ 75.17  &  & 92.23 $|$ 85.91  & 95.62 $|$ 87.75   \\
ARP-GAN &  81.51 $|$ 90.16 &  77.62 $|$ 77.02  &  & 90.13 $|$ 87.07 & \textit{94.50} $|$ \textit{91.00}    \\
Margin-GAN &  94.86 $|$ 97.73 &  92.93 $|$ 88.91  &  & 96.35 $|$ 91.75 &  96.22 $|$ 87.75  \\
&  &   &   &  &   \\
bad-GAN~\cite{dai2017good}  &  \textit{95.75} $|$ - - -  &  \textit{85.59} $|$ - - - &   &   &  \\
Triple-GAN~\cite{li2021triple}  &  \textit{96.04} $|$ - - - &  \textit{87.59} $|$ - - - &   &  &   \\
Negative-GAN~\cite{jo2018open} &  &   &  & - - - $|$ \textit{72.90}  &     \\
Counter-GAN~\cite{neal2018open} &  &  &   &  &  - - -  $|$ \textit{83.80} \\
\hline
\end{tabular} \\ 
\begin{tablenotes}
\centering
\item \textit{\doublespacing Results are averaged over five randomized trials}
\end{tablenotes}
\caption{Results for the SSL and OSR experiments. Each result is split as $a \; | \; b $ with $a$ representing the percentage accuracy for labelled categories and $b$ representing the $\text{AUROC} * 100$. Results in italics are previously published results using different code-bases, which we show for ease of reference.}
\label{table:1}
\end{table*}

To prove our hypothesis that FM-GANs, ARP-GANs and Margin-GANs all operate in a similar fashion, we must show that these models achieve similar results for the same experimental setups. However, given the advance complexity of optimising GANs, we approach our experiments as follows. First, FM-GANs are integrated into the ARP-GAN code base, and both FM-GANs and ARP-GANs are evaluated under SSL and OSR criteria. However, given that the ARP-GAN code base was not optimised for SSL, we can expect poor performance for the SSL experiments. Therefore, in our second step we apply Margin-GANs using its own SSL-optimised code base on the same experiments. A direct comparison of classification accuracy and novelty detection performance between all three GANs under their differently optimised code bases will then indicate whether SSL-GANs are intrinsic OSR-GANs and vice-versa. 

The evaluation metrics within this study are the accuracy scores over the labelled categories and the area under the receiver operating characteristic (AUROC) curve. The AUROC is appropriate due to the threshold methodology for detecting novel categories in OSR~\cite{geng2020recent}. More specifically, thresholds are applied to the classifiers' maximum output predictions to separate novel categories. Samples with predictions below the threshold are classified into the $K + 1$ category (i.e. detected as a novel category), and samples with predictions above the threshold are classified into the corresponding maximum logit category. Note that the use of thresholds is directly correlated to the conversation on regularisation of classifiers, since regularisation aims to decrease overconfidence of classifiers. As per OSR standards, the prediction scores of samples from labelled categories are used to gather a range of threshold values to create the ROC. Subsequently, the AUROC provides a numerical metric of the resulting graph. 

All experiments are conducted using the SVHN or CIFAR10 datasets, with training and testing setups as per conventional SSL and OSR protocols. For the SSL experiments, 100 and 400 labelled training samples are provided per category for SVHN and CIFAR10, respectively, while the remaining training datasets are used as unlabelled training samples. During testing, the CIFAR10 test set is appended to the SVHN test set to introduce unobserved novel categories within the SVHN experiments, and the CIFAR100 test set is appended to the CIFAR10 test set to introduce unobserved novel categories within the CIFAR10 experiments. For the fully-supervised experiments (which are conventional for OSR), we conduct two variations using the CIFAR10 dataset. Specifically, the CIFAR10$\dagger$ experiment uses all training samples from the original dataset, while testing samples come from CIFAR10 and CIFAR100. Then, for the CIFAR10$\star$ experiment, $K = 6$ random categories in CIFAR10 are defined as labelled categories, while the remaining $4$ categories are used as unobserved novel categories. In other words, all training samples from the $4$ novel categories are removed, with the testing set unchanged. 

The FM-GAN, ARP-GAN, and Margin-GANs results are shown in \ref{table:1}. The table also indicates the performance of two baseline models, which are traditional softmax supervised classifiers or supervised ARP classifiers. These baselines were also trained using the ARP-GAN code base for equal optimisation and hyperparameter choices. It is clear that all GAN models improve over the baselines, immediately indicating that ARP-GANs are indeed capable at SSL. Consider the SVHN SSL experiment wherein the baselines fail to learn. We can attribute this failure to the SVHN dataset containing multiple categories in a single image. Therefore, only when the model trains under an SSL setup can the classifier generalise the various categories as those in the centre of the images. In other words, given a considerable improvement over the baseline, ARP-GANs are clearly capable at SSL. 

From the ARP-GAN and FM-GAN results, we can conclude that ARP-GANs and FM-GANs function in a similar fashion. More specifically, because both these models under the same code base achieve very similar results for all experiments, they are clearly both capable at SSL and OSR. Although we consistently see slightly better accuracy scores for FM-GANs and slightly better AUROC scores for ARP-GANs, this would be expected since FM-GANs are primarily focused on classification and ARP-GANs on novelty detection. Nevertheless, their similar results conclude the hypothesis that FM-GANs and ARP-GANs produce similar classifiers. Formally, both models generate samples that lie in the complementary space and, when regularising their classifiers with these generated samples, both models end up generalising the open space. 



\begin{figure*}[!t]
\centering
\subfloat[FM-GAN]{\includegraphics[width=0.49\textwidth, height=6.0cm]{margins-sup-fm.png}
\label{fig:3a}} \\
\subfloat[ARP-GAN]{\includegraphics[width=0.49\textwidth, height=6.0cm]{margins-sup-arp.png}
\label{fig:3b}}
\subfloat[Margin-GAN]{\includegraphics[width=0.49\textwidth, height=6.0cm]{margins-sup-mg.png}
\label{fig:3c}}
\caption{Visualising the embedding space through average margins of trained FM-GANs, ARP-GANs and Margin-GANs for the CIFAR10$\dagger$ experiment. The true categories for novel category samples and generated samples are designated as their predicted categories.}
\label{fig:3}
\end{figure*}

Finally, concerning Margin-GANs, it is clear these SSL optimised models far outperformed FM-GANs and ARP-GANs in the SSL experiments. Thus, Margin-GANs clearly create the new benchmarks for the combined task of SSL-OSR. However, it is interesting to note that Margin-GANs also outperformed ARP-GANs in the CIFAR10$\dagger$ experiment, and achieved a higher accuracy but lower AUROC for the CIFAR10$\star$ experiment. This result indicates that ARP-GANs are more suited for close novelty detection (i.e. when novel categories are from the same domain as labelled categories), and Margin-GANs are more suited for far novelty detection (i.e when novel categories are from a different domain than the labelled categories). Nevertheless, these results clearly indicate that SSL-optimised GAN models are also intrinsic OSR models. However, Margin-GANs are clearly capable at both SSL and OSR, while ARP-GANs have yet to be optimised for SSL. We leave this for future studies.  

To further understand the similarities between these GAN models, Fig.~\ref{fig:3} presents bar graphs that visualise the embedding spaces of of the trained classifiers for the CIFAR10$\dagger$ experiment. More specifically, the bar graphs show the average margins (i.e. $\text{margin}(x, y) = C_y(x) - \max_{i \neq y}C_i(x)$) for all testing samples across the $K$ labelled categories, the average margins for all testing samples from novel categories, and the average margins for $100$ batches of generated samples. These graphs indicate that the margins for labelled and novel categories in both FM-GANs and ARP-GANs are almost identical. Furthermore, Margin-GANs produce considerably smaller average margins for generated samples than the other models. This result could be an indication on how these Margin-GANs achieve considerable higher SSL results. 

To conclude our experiments, it is also interesting to view the generated samples produced by a trained Margin-GAN. Fig.~\ref{fig:5} presents examples of real CIFAR10 images and generated images from the SSL CIFAR10 experiment. It is clear that the images produced at the end of training (i.e. at the model's equilibrium) are of visibly lower quality compared to the real images. Formally, the generated images have indistinct features, lack sharp definition and have a blurred and amorphous appearance. This demonstrates that the generated images do not conform to the real training distribution, which matches the notion of generating bad-looking samples. However, these samples do still produce similar qualities (e.g. colors and contrast) to the real images, as is required to lie in the complementary space. The SSL and OSR results of Margin-GANs also confirm that these images indeed lie on the boundaries of labelled categories.  

\begin{figure*}[!t]
\centering
\subfloat[Real images]{\includegraphics[width=0.45\textwidth]{real.png}
\label{fig:5a}}
\hfil
\centering
\subfloat[Generated images]{\includegraphics[width=0.45\textwidth]{generated.png}
\label{fig:5b}}
\caption{Comparing real and generated images from SSL Margin-GANs - (a) shows examples of real CIFAR10 images; (b) shows examples of generated images from the trained Margin-GAN on the SSL CIFAR10 experiment. }
\label{fig:5}
\end{figure*}

%+- 850
\section{Discussion}
From the comparable results between FM-GANs and ARP-GANs under the same OSR-optimized code base, we can tentatively conclude that SSL-GANs and OSR-GANs function in a similar manner. However, the state-of-the-art performance in SSL-OSR by Margin-GANs under the SSL-optimized code base, and the competitive results between Margin-GANs and ARP-GANs for conventional OSR, provides us with the necessary evidence to thoroughly conclude that SSL-GANs and OSR-GANs function in the same manner. A closer look at the generated samples furthers our confidence that Margin-GANs clearly generate bad-looking samples that lie in the complementary space. Considering the high AUROC scores of Margin-GANs across all experiments, it is also clear that regularising classifiers with these generated samples generalises the open space within the $K + 1$'th category.

The combined task of SSL-OSR certainly decreases the labelling cost of training OSR classifiers. However, including unlabelled training data within the mix presents the possibility of another type of novel category to exist in the domain. More specifically, observed novel categories that are found in unlabelled training sets are becoming an increasingly important topic in SSL research~\cite{engelbrecht2020learning, chen2020semi, he2022safe, banitalebi2022auxmix}. Although this study focused on conventional SSL (i.e. without observed novel categories), extending the combined framework of SSL-OSR to also consider observed novel categories would further enhance the practicality and cost-efficiency of classifier development~\cite{engelbrecht2020learning}. However, it is important that such future research does not conflate the definitions of observed novel categories and unobserved novel categories, as has been seen before~\cite{yu2020multi}. More specifically, classifiers must generalise the difference between these novel category types. 

Unobserved novel categories represent new and interesting patterns that develop over time. For example. SARS-CoV-2 was an unobserved novel category prior to its discovery. If unobserved novel categories are to be detected early, then these must completely separated from the training domain. That is, unobserved novel categories must be separated from labelled categories and observed novel categories. Subsequently, practitioners can study the newly discovered patterns, and possibly label new categories~\cite{van2022three}. Therefore, if SSL-OSR is extended to also consider observed novel categories, then two novelty detectors are required in parallel with the classifier. We leave development of such a model for future work. Nevertheless, it is clear that unifying machine learning policies can enhance the practicality of artificially intelligent agents. However, such processes must be done with careful consideration as to not hinder models with conflated definitions. 

\clearpage
\section{Conclusion}
This study concludes that generative SSL and generative OSR are linked by the same theory applied to their classification networks. More specifically, SSL-GANs and OSR-GANs both require their generators to produce samples within the complementary space, which are then used to regularise their classifiers. In turn, SSL and OSR classifiers generalise the open space within their $K + 1$'th categories, albeit it realised through a threshold mechanism. Our findings suggest that SSL-GANs, without modifications, are highly capable at OSR, while OSR-GANs indicate initial evidence of being capable at SSL. From our experimental results, we find that the SSL-optimised Margin-GANs produce state-of-the-art results for the combined task of SSL-OSR, and so this study sets the benchmarks for this unified field. Future studies should further explore the relationship between SSL-OSR under the overarching conversation of regularisation using generated samples in the complementary space, and aim to further unify the new SSL-OSR framework with other machine learning policies.


\section*{Declaration of Generative AI and AI-assisted technologies in the writing process}
During the preparation of this work the author(s) used Grammarly and ChatGPT in order for grammatical improvement for better readability of the study. After using this tool/service, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the publication.

\clearpage
\printbibliography
  
\end{document}

\end{document}