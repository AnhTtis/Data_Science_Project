
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
%~\cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
%~\cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
   \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
   \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
   \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
   \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
   \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
\usepackage{amsmath}
\usepackage{amsfonts}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{booktabs}  
\usepackage[flushleft]{threeparttable}
\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{1.5}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Linking generative semi-supervised learning \\ and generative open-set recognition}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Emile~Engelbrecht,
        Johan~du~Preez
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Emile Engelbrecht is a Ph.D. candidate at the Department of Electronic Engineering, University of Stellenbosch, South Africa.\protect\\
E-mail: 18174310@sun.ac.za
\IEEEcompsocthanksitem Johan du Preez is supervisor to Emile at the Department of Electronic Engineering, University of Stellenbosch, South Africa.\protect\\
Email: dupreez@sun.ac.za}
\thanks{Manuscript sent for review March 16, 2023.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{SUBMITTED TO THE JOURNAL OF IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}%
{Engelbrecht \MakeLowercase{\textit{et al.}}: Linking generative semi-supervised learning and generative open-set recognition}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performed margin-GANs at other OSR experiments. This result indicates unique insights for the combined optimisation task of SSL-OSR. 
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Open-set recognition, Novelty detection, Semi-supervised learning, Generative learning, GANs
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.


\IEEEPARstart{C}{lassification} involves categorizing input data samples into known/labeled categories. However, classifiers must also be equipped to handle novel categories that may emerge over time~\cite{geng2020recent, yang2021generalized}. Two important applications of such classifiers are 1) automated diagnostic tools~\cite{pang2021semi, pahar2021automatic} and 2) self-driving cars~\cite{wu2019semi}. In both cases, it is crucial that classifiers make very few mistakes, as misclassifications can lead to fatal consequences. To address this, classifiers must be able to detect and separate novel categories that were not present during training but appeared during testing. For example, 1) classifiers should call on human doctors when encountering an unknown disease (e.g. SARS-CoV-2 pre-December 2020~\cite{andersen2020proximal}), and 2) classifiers must alert driving modules to safely manoeuvre out of the way or demand manual takeover in case of unexpected driving scenes (e.g. unexpected obstacles~\cite{ramos2017detecting}, or high-risk scenarios~\cite{puertas2021should}).

Open-set recognition (OSR) tests classifiers' ability to handle both labeled categories and novel categories unobserved during training~\cite{geng2020recent}. More specifically, OSR requires classifiers to correctly classify samples belonging to the $K$ labeled categories and accurately detect and separate samples that belong to unobserved novel categories. Although OSR is defined independently of the training method, most OSR studies use supervised learning. More specifically, OSR models are generally trained from datasets wherein all training samples have one of $K$ labels. This approach can be costly in real-world applications, and so it is often preferred to use the alternative of semi-supervised learning (SSL), which can achieve high accuracy using fewer labeled samples~\cite{van2020survey}. However, despite its cost-effectiveness, SSL has rarely been linked to OSR, with no previous study exploring this relationship on a theoretical level.

SSL and OSR have striking similarities in their methods, particularly when using generative adversarial networks (GANs) (SSL -~\cite{NIPS2016_8a3363ab, dai2017good, kumar2017semi, NEURIPS2019_517f24c0, li2020semi} and OSR -~\cite{ge2017generative, neal2018open, jo2018open, chen2021adversarial}). Consider the classification boundaries of a typical closed-set classifying network with a softmax output activation function (\cite{goodfellow_deep_learning}) for $K$ output nodes in Fig.~\ref{fig:1a} and Fig.~\ref{fig:1b}. Clearly, closed-set classifiers aim to maximize the classification boundaries or margins between categories, but this is insufficient for both SSL~\cite{dai2017good} and OSR~\cite{chen2021adversarial}. More specifically, for high accuracy SSL, the classifier must also generalize the complementary space~\cite{NIPS2016_8a3363ab, dai2017good} (see Fig. \ref{fig:1c}). Similarly, for high-accuracy OSR, the classifier must appropriately generalize the open space~\cite{chen2021adversarial} (see Fig. \ref{fig:1d}). However, through theoretical and experimental comparisons, we will show that generalizing the complementary and open spaces result in similar classifiers.

\begin{figure*}[!t]
\centering
\subfloat[Example domain]{\includegraphics[width=0.25\textwidth]{1.png}
\label{fig:1a}}
\hfil
\centering
\subfloat[Classification boundaries]{\includegraphics[width=0.25\textwidth]{2.png}
\label{fig:1b}}
\hfil \\
\centering
\subfloat[Complementary space]{\includegraphics[width=0.25\textwidth]{4.png}
\label{fig:1c}}
\hfil
\centering
\subfloat[Open space]{\includegraphics[width=0.25\textwidth]{3.png}
\label{fig:1d}}
\caption{Different classification boundaries of a 2D example domain with $K = 3$ - (a) describes the 2D domain with each category represented by a different shape and colour; (b) describes the classification boundaries of a typical closed set classifier with a softmax output activation function; (c) shows the complementary space of the each category, which is depicted by the 'x's around each category; and (d) shows the open space of the domain which is represented by the white area.}
\label{fig:1}
\end{figure*}

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

The general idea behind SSL-GANs and OSR-GANs is to generalize an additional $K + 1$'th category to represent necessary embedding spaces. In SSL-GANs, this additional category represents the complementary space to the labeled categories or the area around the classification boundaries of labeled categories. In OSR-GANs, this additional category represents the open space where novel categories reside. SSL-GANs and OSR-GANs use generated samples from GAN variations to generalize their respective $K + 1$'th categories. However, because of their similar nature, the $K + 1$'th categories of both SSL-GANs and OSR-GANs end up representing the same areas in the embedding space. More specifically, our results show that the generated samples of both SSL-GANs and OSR-GANs lie within the complementary space. Furthermore, when using these complementary samples to generalize the $K + 1$'th category, classifiers in both SSL-GANs and OSR-GANs generalize the open space. 

Considering the cost-efficiency of SSL, and the safety of OSR, it is clearly beneficial to train models using an SSL approach and test them under OSR conditions. This study provides the foundation for more practical SSL-OSR training by theoretically linking SSL and OSR under the common framework of GANs. The study is divided into two phases. The first phase examines the state-of-the-art OSR-GAN, namely adversarial reciprocal point (ARP)-GANs~\cite{chen2021adversarial}. In this first phase we integrate an established SSL-GAN, namely feature-matching (FM)-GANs~\cite{NIPS2016_8a3363ab}, into the ARP-GAN codebase. Both models are evaluated under SSL, OSR, and combined SSL-OSR experiments. The results show that both models perform similarly, with FM-GANs displaying slightly better SSL performance and ARP-GANs displaying slightly better OSR performance. Furthermore, a closer examination of the resulting embedding spaces reveals that both ARP-GANs and FM-GANs end up generalizing the same areas within their $K + 1$'th categories. 

The first phase establishes a clear theoretical connection between SSL-GANs and OSR-GANs. However, since the ARP-GAN codebase was optimized for OSR, the SSL results do not compare to the current state-of-the-art SSL-GAN methods. Therefore, in the second phase, we turn to the state-of-the-art SSL-GAN method of margin-GANs~\cite{NEURIPS2019_517f24c0}. Margin-GANs propose margin-theory which is directly related to the discussion of the complementary and open space. However, margin-GANs have never before been applied to OSR. Through thorough experiments, this second phase shows that margin-GANs also generate samples in the complementary space and so also generalize the open space within the resulting classifier. Furthermore, margin-GANs are found to set the new standard for the combined task of SSL-OSR while also outperforming ARP-GANs in certain OSR tasks.

It is important to note that this study does not focus on optimisation. Instead, we only aim to prove the hypothesis that SSL-GANs and OSR-GANs function similarly, meaning models from either fields could be used for their opposite fields and the combined SSL-OSR field. Nevertheless, since margin-GANs were optimised for SSL, these models are found to outperform the OSR optimised ARP-GANs by a significant degree. Future studies could certainly optimise ARP-GANs for SSL to provide stronger baselines for these models, yet our results conclude that SSL-GANs and OSR-GANs function similarly. Our results also establish the foundation for future studies to combine the theory of SSL and OSR to uniformly optimise models for the SSL-OSR task. 
 
The remainder of this paper is structured as follows: in Section~2, we provide background information on SSL, OSR, and the previous research on the links between these fields; in Section~3, we conduct the first phase by comparing FM-GANs and ARP-GANs under the same experimental conditions; in Section~4, we conduct the second phase by thoroughly experimenting on margin-GANs; in Section~5, we discuss the results from both phases; and in Section~6, we conclude the study. The main contributions of this study include being the first study to theoretically link SSL and OSR, being the first to conduct thorough SSL-OSR experiments, and setting the new standard for SSL-OSR and general OSR performance.

\section{Background}
Novel categories are formally defined as groups of anomalous samples with similar patterns that do not match any of the labeled categories~\cite{gruhl2021novelty}. Within open-set recognition (OSR), novel categories are further described as only appearing after training~\cite{pimentel2014review, geng2020recent, yang2021generalized}. In other words, OSR defines novel categories as unobserved categories that appear over time in changing environments~\cite{din2021data}. Models handle unobserved novel categories in one of two ways: 1) in class-incremental learning (CIL), a new label is defined for each novel category as it is discovered~\cite{belouadah2020comprehensive, zhang2020class}; or 2) in OSR, samples that belong to novel categories are separated into a different pile commonly referred to as the 'background/unknown' category. This study focuses on on OSR, which we note is a natural pre-step to achieving CIL.

OSR requires trained models to accurately classify samples that belong to $K > 1$ number of labeled categories and simultaneously separate samples that belong to any number of unobserved novel categories into an additional $K + 1$'th category. Unobserved novel categories are in contrast to observed novel categories found in variations of semi-supervised learning (SSL) research~\cite{yu2020multi, da2014learning, blanchard2010semi, chen2020semi}. More specifically, the fields of open-set semi-supervised learning (open-SSL)~\cite{yu2020multi}, learning with augmented category by exploiting unlabeled data (LACU)~\cite{da2014learning, engelbrecht2020learning}, semi-supervised novelty detection (SSND)~\cite{blanchard2010semi} and mismatched semi-supervised learning (MSSL)~\cite{chen2020semi}, define novel categories as those with samples scattered in unlabeled training sets. However, these novel categories are instead described as observed novel categories~\cite{engelbrecht2020open}, where OSR focuses on unobserved novel categories.

With regards to training, supervised learning uses a labeled training set, $\mathcal{D}_{\text{train}} := \mathcal{D}_{\text{lab-train}}$, that consists of paired samples $(x, y)$, where $x$ is the input sample and $y$ is the category label. The set $C_K := \{1, 2, ..., K\}$ is defined, where each entry in $C_K$ represents a unique label for one of the $K$ labeled categories. All training labels in supervised learning fall within $C_K$, or ${y \sim \mathcal{D}_{\text{lab-train}}} \in C_K$. SSL extends supervised learning by incorporating an additional set of unlabeled training samples, $\mathcal{D}_{\text{train}} := \mathcal{D}_{\text{lab-train}} \cup \mathcal{D}_{\text{unlab-train}}$. According to the SSL cluster assumption~\cite{van2020survey}, unlabeled training samples are assumed to belong to one of the labeled categories. Thus, samples in $\mathcal{D}_{\text{unlab-train}}$ can be represented as $(x, y_a)$, where $x$ is the input sample, and $y_a$ is the anticipated category label. In turn, general SSL has all ${y \sim \mathcal{D}_{\text{lab-train}}} \in C_K$ and ${y_a \sim \mathcal{D}_{\text{unlab-train}}} \in C_K$. 

It is important to note that GANs have been extensively studied for traditional SSL~\cite{sajun2022survey}. Initially, GAN based SSL methods transformed the discriminator to classify real samples into their respective labeled categories~\cite{springenberg2015unsupervised, odena2016semi, NIPS2016_8a3363ab, dai2017good, kumar2017semi, xu2021semi}. However, state-of-the-art models now prefer a three-network game, with the discriminator unchanged to the original GAN and a separate classifier added to the model~\cite{dong2019margingan, Litriplegans}. Of all SSL-GANs, the bad-GAN by Dai et al.~\cite{dai2017good} is especially noteworthy, as Dai theoretically and experimentally showed that good SSL-GANs must generate bad-looking samples that lie in the complementary space. More specifically, generating samples that lie around the classification boundaries of labeled categories forces the classifier to tighten classification boundaries for improved generalisation. The bad-GAN approach is interesting as several OSR GAN-based models have also used a similar notion of bad looking samples.

Let the testing set, $\mathcal{D}_{\text{test}}$, also consist of input samples with anticipated labels, $(x, y_a)$. OSR inserts unobserved novel categories into this testing set to ensure the classifier can handle new emerging patterns. For general supervised learning and general SSL, the testing set is closed in relation to the training set. In other words, all testing samples belong to one of the labeled categories, i.e. all ${y_a \sim \mathcal{D}_{\text{test}}} \in C_K$. In OSR, novel categories that were not present during training are included in the testing set. However, these novel categories are not individually classified but are grouped and separated into an additional $K + 1$'th category. In other words, OSR updates the testing set to have all ${y_a \sim \mathcal{D}_{\text{test}}} \in C_K \cup \{K + 1\}$. OSR methods generally train under a supervised setting and test under this OSR setting. However, this study focuses on the combined SSL and OSR setting, meaning all ${y \sim \mathcal{D}_{\text{lab-train}}} \in C_K$, all ${y_a \sim \mathcal{D}_{\text{unlab-train}}} \in C_K$ and all ${y_a \sim \mathcal{D}_{\text{test}}} \in C_K \cup \{K + 1\}$. 

Before exploring the combination of SSL and OSR, it is important to consider the use of GANs for novelty detection and OSR. General novelty detection is a one-class classification problem that distinguishes unseen novel categories from a single positive labeled category~\cite{pimentel2014review}. The discriminator networks in GANs have been widely studied in this context~\cite{perera2019ocgan, sabokrou2018adversarially, zhang2021adversarially}. For OSR, three previous OSR-GAN studies are noteworthy as these also use GANs to generate bad-looking samples, similar to previous SSL-GANs. For example, the counter-factual (counter)-GAN by Neal et al.~\cite{neal2018open} combines an auto-encoder and a GAN to generate bad-looking samples that are then labeled into the $K + 1$th category, allowing for generalization in open space. Similarly, Jo et al.~\cite{jo2018open} use a feature-matching (FM)-GAN and a denoising auto-encoder to generate bad-looking samples, which are then used to regularize the classifier for the $K$ observed labeled categories. However, these previous studies were outperformed by significant degrees by the adversarial reciprocal point learning GAN (ARP-GAN) proposed by Chen et al.~\cite{chen2021adversarial} (which is discussed in the upcoming section).

To the best of our knowledge, only three studies have explored the combined SSL-OSR setting. Capazzo et al.~\cite{cappozzo2020anomaly} used traditional machine learning techniques to address label noise under the SSL-OSR setting, but these methods are unsuitable for high-dimensional image data. Then, the unpublished study by Kliger et al.~\cite{kliger2018novelty2} (later accepted as a patent~\cite{kliger2018novelty}) applied neural networks for SSL-OSR, but their results were unclear, and they did not make a theoretical link between SSL and OSR. Finally, difference-seeking (DS)-GANs by Sung et al.~\cite{sung2019difference} applied manually chosen operations to the training data and used FM-GANs to match this pseudo-data. However, Sung et al. did not provide justification on the choice of operations, while they also experimented on SSL and OSR independently, without linking the two. This study aims to formally and theoretically bridge SSL with OSR under the unified framework of GANs.


\section{First phase}
The aim of this study is to demonstrate that SSL-GANs and OSR-GANs theoretically and experimentally function similarly. In this first phase, we present two GAN models that were designed specifically for SSL and OSR - feature-matching GANs (FM-GANs)~\cite{NIPS2016_8a3363ab} for SSL and adversarial reciprocal point GANs (ARP-GANs)~\cite{chen2021adversarial} for OSR. To maintain fairness in our experiments, we will use the ARP-GAN code-base, noting that these models hold state-of-the-art results in OSR. In other words, the same optimization tools, network architectures, and hyper-parameter choices will be used throughout all experiments in this first phase. The results will demonstrate the similarities between these two GAN models, laying the foundation for our conclusion on the theoretical link between SSL-OSR.

\subsection{Technical preliminaries}
This section thoroughly describes FM-GANs and ARP-GANs, explicitly highlighting their theoretical similarities. As we delve into each model, it is important to pay attention to the $K + 1$'th categories. Specifically, both models use the additional $K + 1$'th category to represent unique areas in the embedding space. FM-GANs use the $K + 1$'th category to represent the complementary space of each labeled category (as shown in Fig. \ref{fig:1c}), while ARP-GANs use the $K + 1$'th category to represent the open space where novel categories lie (as shown in Fig. \ref{fig:1d}). This study proposes that the $K + 1$ categories in these different models ultimately represent the same areas in the embedding space, leading to both models functioning in a similar fashion.

\subsubsection{Feature-matching GANs}
Semi-supervised learning (SSL) has been widely studied using various methods~\cite{van2020survey} including GANs~\cite{sajun2022survey}. One SSL-GAN model, namely (FM)-GANs~\cite{NIPS2016_8a3363ab}, has been the foundation for multiple other SSL-GAN methods~\cite{dai2017good, kumar2017semi, sung2019difference, li2020semi}. Interestingly, FM-GANs have also been the foundation of an OSR-GAN method~\cite{jo2018open}. Consequently, FM-GANs provide a key insights into the link between SSL-OSR. However, we present FM-GANs as they were originally intended for SSL. It is also important to note that a basic understanding of GANs is required to understand the concepts discussed in this study. Readers who need to become more familiar with GANs should first study the fundamentals of these models~\cite{goodfellow2020generative}.

As a summary, vanilla GANs consist of two networks - a generator and a discriminator. The discriminator is trained to differentiate between real training data and fake data generated by the generator. In turn, the generator is trained to mimic the real data distribution to trick the discriminator into classifying the generated samples as real, creating an adversarial relationship between the two networks. At equilibrium, the discriminator will have learnt the low-dimensional manifolds of the training data~\cite{arjovsky2017wasserstein}, and the generator produces new samples that match the distribution of the real data. However, in SSL-GANs and OSR-GANs, the GAN loss functions are altered so that the generator produces "bad-looking" samples instead~\cite{dai2017good}. Specifically, in the case of FM-GANs, these generated samples must fall within the complementary space. 

SSL-GANs transform the discriminator into a classifier network which distinguishes real and fake samples and classifies real samples into their respective $K$ labeled categories~\cite{springenberg2015unsupervised, odena2016semi}. For the sake of clarity, the classifier network, denoted as $C(x)$ for input sample $x$, is generally described to have $K + 1$ output nodes, with the additional node used for fake samples~\cite{NIPS2016_8a3363ab}. More formally, the classifier network produces a probability score for a sample belonging to class $j$ through its softmax activation function as $p_{\text{class}}(y == j | x) = \frac{\text{exp}[C^{j}(x)]}{\sum_{i=1}^{K + 1} \text{exp}[C^{i}(x)]}$, where $C^{i}(x)$ is the logit value of node $i$. The classifier's loss function in an SSL-GAN is a combination of a supervised loss for samples in $D_{\text{lab-train}}$ and a supervised loss for generated samples, $G(z)$ (generated from a uniform distribution $\mathbb{P}^{\text{(z)}}$), which are labeled into the $K + 1$'th category. This combined supervised loss function is given as:

\begin{equation}
\begin{split}
 \underset{\text{SSL-GAN}}{\text{C-Loss}} = 
 & - \; \mathbb{E}_{(x, y) \sim D_{\text{lab-train}}} [\log(p_{\text{class}}(y|x) \;)] \\
 & - \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log( \; p_{\text{class}}(y = K + 1| \; G(z) \;)]
    \label{eq:cost_c_ssl}
\end{split}
\end{equation} 

For such a $K + 1$ classifier, the probability of a sample being fake (i.e. being classified into the $K + 1$'th category) is given as $p_{\text{class}}(y == K + 1 | x) = \frac{\text{exp}[C^{K + 1}(x)]}{\sum_{i=1}^{K+1} \text{exp}[C^{i}(x)]}$. However, no testing sample is ever a fake sample, so the $K + 1$ setup is over-parameterised. Consequently, FM-GANs and subsequent follow up-studies~\cite{dai2017good, kumar2017semi, li2020semi}, proposed fixing the output value at the $K + 1$'th node to zero, i.e. $C^{K+1}(x) = 0 \; \; \forall x$. Thus, the probability of a sample being fake in an FM-GAN is transformed to $p_{\text{class}}(y == K + 1 | x) = \frac{1}{\sum_{i=1}^{K} \text{exp}[C^{i}(x)] + 1}$. With this new criterion, the classifier network no longer has $K + 1$ output nodes, but only $K$ nodes as a general classifier. However, the classifier still has $K + 1$ categories, with the $K + 1$'th category fixed as the zero'th vector over all $K$ nodes. In turn, and with the aid of the softplus function ($\text{softplus}(x) = \log(1 + x)$), the SSL classifier loss function in eq.~\ref{eq:cost_c_ssl} is transformed to: \footnote{See https://jostosh.github.io/ssl-gan/ for the full derivation}:

\begin{equation}
\label{eq:cost_c_fm}
\begin{split}
  \underset{\text{FM-GAN}}{\text{C-Loss}} = 
 & \; 0.5 \cdot \mathbb{E}_{(x, y) \sim D_{\text{train}}} [\text{softplus}(\log(\sum_{i = 1}^{K} \exp{C^i(x)})) \\
 & -  \log(\sum_{i = 1}^{K} \exp{C^i(x)})] \\
 & + \; 0.5 \cdot \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\text{softplus}(\log(\sum_{i = 1}^{K} \exp{C^i(G(z))}))] \\
 & - \; \mathbb{E}_{(x, y) \sim D_{\text{lab-train}}} [\log(p_{\text{class}}(y|x))] 
\end{split}
\end{equation}

When using vanilla GANs, the $K + 1$ category setup becomes problematic. In a standard GAN, the generator is tasked to produce samples that match the training data distribution. However, if the generator succeeds, the classifier network trained with eq.~\ref{eq:cost_c_fm} will struggle to generalize the labeled categories. More specifically, a generator that has accurately mimicked the training data distribution will produce samples that fall within the classification boundaries of the labeled categories. Labelling these realistic-looking samples into the $K + 1$ category (albeit the zero'th vector over $K$ nodes) would confuse the classifier, as samples that belong to the same categories now belong to two different categories. To overcome this, FM-GANs modify the original GAN loss function to instead generate samples that lie in the complementary space of the labeled categories. 

The complementary space refers to the area surrounding the boundaries of the labeled categories (as shown in Figure \ref{fig:1c}). Generated samples in the complementary space can be labeled into the $K + 1$th category without affecting generalization performance. In fact, by doing so, the classifier is forced to tighten its classification boundaries which imporves SSL accuracy~\cite{dai2017good}. To learn representation of the complementary space, FM-GANs propose a feature-matching loss function for the generator. The FM loss function is based on the idea that the last hidden layer of the classifier network ($C'(x)$) represents the features of the input data. FM-GANs minimize the L2 distance between the activations of this hidden layer for real-training samples and the activations for fake-generated samples. Although FM-GANs do not guarantee that generated samples will always fall within the complementary space, they provide a foundation for complementary space theory. Formally, the FM-GAN generator loss function is given as follows:

\begin{equation}
\label{eq:cost_g_fm}
 \underset{\text{FM-GAN}}{\text{G-Loss}} \; = \; ||\mathbb{E}_{x \sim D_{\text{train}}} C'(x) - \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} C'(G(z))||_2^2
\end{equation}

In summary, FM-GANs convert the traditional two-player GAN game into a SSL framework by minimizing the loss functions $\text{C-Loss}_{\text{\; FM-GAN}}$ for the classifier network and $\text{G-Loss}_{\text{\; FM}}$ for the generator network. Although various improvements have been proposed for FM-GANs, this study focuses on this foundational theory. By learning the representation of the complementary space within the generator network and labelling generated samples into the $K + 1$'th category, this study hypothesizes that the $K + 1$'th category in FM-GANs would actually end up generalizing to the open space. More concretely, because generated samples lie around the classification boundaries of labeled categories, the $K + 1$'th category ends up representing all outside these boundaries which is the open space. 

\subsubsection{Adversarial reciprocal point GANs}
The open space represents the area of 'all that the labeled categories are not' (see Fig. \ref{fig:1d}). Consequently, generalising the open space achieves unobserved novelty detection. Several studies have proposed generative methods to generalize the open space within an additional $K + 1$'th category~\cite{ge2017generative, neal2018open, jo2018open, sung2019difference}. Specifically, generative networks are trained to produce samples that, when placed into the $K + 1$'th category, would appropriately generalize the open space. Although several generative OSR approaches have been developed, this study focuses on the state-of-the-art OSR method of ARP-GANs by Chen et al.~\cite{chen2021adversarial}. ARP-GANs rely on a new learning technique based on a novel concept of reciprocal points. The following is a thorough description of ARP-GANs. 

In ARP-GANs, each of the $K$ labeled categories is assigned a reciprocal point, $P^j$, which represents everything that category $j$ is not. More specifically, $P^j$ is a learned prototype that must be updated during training until it lies in the centre of the combined embedding space of all other labeled categories ($\neq j$) and the open space. The probability of a sample belonging to category $j$ is defined based on the distance of the sample to the category's reciprocal point, $P^j$. The sample is classified into the category with the farthest reciprocal point, as determined by a chosen distance metric. Formally, ARP-GANs produce probability scores as $p_{\text{class-ARP}}(y == j | x) = \frac{\text{exp}[d(C(x), P^j)]}{\sum_{i=1}^{K} \text{exp}[d(C(x), P^i)]}$, where $d(C(x), P^j)$ is the distance between the reciprocal point and the classifier's output for input sample $x$.

The distance metric used in ARP-GANs is a combination between the euclidean distance and the dot product, $d(C(x), P^j) = d_e(C(x), P^j) - d_d(C(x), P^j) = (\frac{1}{m} \; \cdot \; || C(x) - P^j ||^2_2) \; \; - \; \; (C(x) \cdot P^j)$. Note that $m$ is the dimension of the reciprocal point and the classifier's output (e.g. 128 in the ARP-GAN setup), meaning the classifier's output must match the size of the reciprocal points. ARP-GANs do not rely on general neural network classification techniques (i.e. one-hot encoded labels and cross-entropy loss). Instead, the distance, $d(C(x), P^j)$, between input samples and reciprocal points represent the classifier's logits. During training, ARP-GANs maximise the distance between labeled training samples and their categories' corresponding reciprocal points. However, maximisation between input samples and reciprocal points does not bind or generalize the open space. 

To generalize the open space within the $K + 1$'th category, classifiers in ARP-GANs also bind the euclidean distance ($d_e(C(x), P^j) = (\frac{1}{m} \; \cdot \; || C(x) - P^j ||^2_2)$) between labeled training samples and their corresponding reciprocal points to a learned range, $R$. By doing so between multiple labeled categories and their corresponding reciprocal points, the open space is pushed to the centre of the embedding space. It is important to note that the $K + 1$'th category in ARP-GANs has a similar structure to the $K + 1$'th category of FM-GANs. Specifically, FM-GANs and ARP-GANs do not explicitly define $K + 1$'th output nodes for the $K + 1$'th category. Instead, the $K + 1$'th category in FM-GANs is represented by the zero'th vector over $K$ nodes, and in ARP-GANs is represented as the center of the embedding space over $K$ nodes. Relating these representations of the $K + 1$'th category is the crux of our theoretical argument on the link between SSL-GANs and OSR-GANs.

In summary, an ARP-GAN classifier must maximise the distance $d(C(x), P^j)$ between labeled training samples and corresponding reciprocal points to push labeled categories to the edges of the embedding space. Furthermore, the classifier must minimise the distance  $d_e(C(x), P^j)$ to a range smaller than $R$ to limit the open space to the centre of the embedding space  (see Fig. 3 in~\cite{chen2021adversarial}). With $\gamma$ generally set to $\gamma = 0.1$, this adversarial loss function for the classifier network is described as: 

\begin{equation}
\begin{split}
    \underset{\text{ARP}}{\text{C-Loss}} = 
    &- \; \mathbb{E}_{(x, y) \sim D_{\text{lab-train}}} [\log(p_{\text{class-ARP}}(y|x))] \\
    & + \gamma \cdot \text{max}(d_e(C(x), P^j) - R, 0)
    \label{eq:cost_c_arp}
\end{split}
\end{equation}

$\text{C-Loss}_{\text{ARP}}$ is the categorical cross-entropy equivalent for supervised classifiers based on reciprocal points. With this classifier setup, Chen et al.~\cite{chen2021adversarial} appended an additional GAN criterion to $\text{C-Loss}_{\text{\; ARP}}$ to further improve results. In contrast to the two network setup of FM-GANs, ARP-GANs have three networks - a classifier, a discriminator and a generator. With a general GAN's discriminator (see eq. \ref{eq:cost_d_arpgan} below), the generator in ARP-GANs is tasked to match a mixture distribution between the real-training data and the reciprocal points. In other words, the generator is tasked to confuse the discriminator (i.e. match the real data distribution) and match the reciprocal points (i.e. match the reciprocal distribution of the real data). Subsequently, by including an information entropy loss function between generated samples and reciprocal points, the classifier can further improve binding the open space to the centre of the embedding space. 

Given $S(G(z), P^j) = \text{softmax}(d_e(C(G(z)), P^j))$, the information entropy loss is defined as $I(G(z)) = \sum^{K}_{i = 1} S(G(z), P^i) \cdot \log(S(G(z), P^i))$. To match the real data distribution and the reciprocal thereof, the generator's loss function within ARP-GANs includes the original GAN's loss function and $I(G(z))$. Furthermore, to ensure the classifier uses generated samples to constrain the open space, $I(G(z))$ is also appended to the classifier loss function in eq. \ref{eq:cost_c_arp}.  Thus, the three loss functions for the discriminator, generator and classifier in ARP-GANs are formally given as:

\begin{equation}
\begin{split}
    \underset{\text{ARP-GAN}}{\text{D-Loss}} = 
    & - \; \mathbb{E}_{(x) \sim D_{\text{train}}} [\log(D(x))] \\
    & - \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(1 - D(G(z)))] 
    \label{eq:cost_d_arpgan}
\end{split}
\end{equation}

\begin{equation}
    \underset{\text{ARP}-GAN}{\text{G-Loss}} = - \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(D(G(z))) \; + \; I(G(z))]
    \label{eq:cost_g_arpgan}
\end{equation}

\begin{equation}
\label{eq:cost_c_arpgan}
\begin{split}
\underset{\text{ARP-GAN}}{\text{C-Loss}} = 
& - \; \mathbb{E}_{(x, y) \sim D_{\text{lab-train}}} [\log(p_{\text{class-ARP}}(y|x))] \\
& + \gamma \cdot \text{max}(d_e(C(x), P^j) - R, 0) \; \\
& + \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [I(G(z))]
\end{split}
\end{equation}

The generator within an ARP-GAN juggles between the real data distribution and the reciprocal thereof. Thus, Chen et al.~\cite{chen2021adversarial} argue that generated samples fall within the global open space at the centre of the embedding space. If we assume that the global open space indeed lies in the exact centre of the embedding space, then samples in the open space would have an equal probability spread across $K$ categories (i.e. $1/K$ \% of belonging to every labeled category). Any sample outside this centre would not lie within the global open space but instead fall on the boundary of the global open space and the labeled categories (i.e. the complementary space). Considering an imperfect generator, it is clear that the generated samples within ARP-GANs fall in the complementary space, similar to FM-GANs, and are used to lower the lower the confidence of the classifier's predictions. In turn, better separation of labeled and novel categories can be achieved realised. 

\subsection{Experiments}
\begin{table*}[t]
\caption{Results for our SSL, OSR and SSL-OSR experiments in the first phase. Each result is split as $a \; | \; b $ with $a$ representing the percentage accuracy for labeled categories and $b$ representing the $\text{AUROC} * 100$. Results in italics are previously published results using different code-bases, which we show for ease of reference.}
\centering
%\begin{tabular}{m{3.2cm} m{2.2cm} m{2.2cm} m{0.3cm} m{2.2cm} m{2.2cm}}
\begin{tabular}{ p{3.2cm} p{2.2cm} p{2.2cm} p{0.3cm} p{2.2cm} p{2.2cm} }
\hline
 \multicolumn{1}{c}{} & \multicolumn{2}{c}{\textbf{Semi-supervised}} & \multicolumn{3}{c}{\textbf{Supervised}} \\
\hline
    & SVHN & CIFAR10 & & CIFAR10$\dagger$ & CIFAR10$\star$ \\
Labels per class & 100     & 400  & & Full  & Full \\
\hline
\textbf{Baselines} & & & & &  \\
Softmax & 13.83 $|$ 41.18 & 62.66 $|$ 67.04 &   & 90.24 $|$ 83.79 & 93.36 $|$ 85.26 \\
ARP     & 10.32 $|$ 53.65 &  61.54 $|$ 69.46 &   & 89.19 $|$ 84.53 & \textit{93.90} $|$ \textit{90.10 }   \\
\hline
\textbf{GANs} & & & & &  \\
FM-GAN  & 83.40 $|$ 88.02 & 78.70 $|$ 75.17  &  & 92.23 $|$ 85.91  & 95.62 $|$ 87.75   \\
ARP-GAN &  81.51 $|$ 90.16 &  77.62 $|$ 77.02  &  & 90.13 $|$ 87.07 & \textit{94.50} $|$ \textit{91.00}    \\
&  &   &   &  &   \\
bad-GAN~\cite{dai2017good}  &  \textit{95.75} $|$ - - -  &  \textit{85.59} $|$ - - - &   &   &  \\
Triple-GAN~\cite{Litriplegans}  &  \textit{96.04} $|$ - - - &  \textit{87.59} $|$ - - - &   &  &   \\
Margin-GAN~\cite{NEURIPS2019_517f24c0}  &  &  \textit{93.56} $|$ - - - &   &  &   \\
Negative-GAN~\cite{jo2018open} &  &   &  & - - - $|$ \textit{72.90}  &     \\
Counter-GAN~\cite{neal2018open} &  &  &   &  &  - - -  $|$ \textit{83.80} \\
\hline
\end{tabular} \\ 
\begin{tablenotes}
\item \textit{ \; \; \; \; Results are averaged over five randomized trials}
\end{tablenotes}
\label{table:1}
\end{table*}

This first phase aims to show that SSL-GANs and OSR-GANs function similarly. More specifically, we propose that FM-GANs and ARP-GANs both generate samples within the complementary space, and thus both models generalize the open space in the $K + 1$'th category. To test this theory, FM-GANs are integrated into the ARP-GAN codebase, and both FM-GANs and ARP-GANs are evaluated under SSL, OSR, and combined SSL-OSR criteria. Note that using the same codebase ensures fair experiments as the same network architectures, optimization tools, and hyperparameter choices are used, as in the ARP-GAN study~\cite{chen2021adversarial}. The primary difference between ARP-GANs and FM-GANs are the loss functions and that FM-GANs do not utilize the third discriminator network. We will examine the direct SSL-OSR results of both models as well as their embedding space outputs to determine the theoretical link between the two. 

The evaluation metrics within this study are the accuracy scores over the labeled categories and the area under the receiver operating characteristic (AUROC) curve. The AUROC is appropriate due to the unique $K + 1$ category setup in FM-GANs and ARP-GANs. More specifically, to separate novel categories, the models' inference is determined through thresholds applied to the maximum probability score of the classifying networks over $K$ logits. Samples below the threshold are classified into the $K + 1$ category and samples above the threshold being classified into the corresponding maximum logit's category. The prediction scores of samples from labeled categories will be used to gather a range of threshold values to create the ROC, while the AUROC will provide a numerical metric for the resulting graph (note that we opt to show the $\text{AUROC} * 100$ for ease of reading). Finally, the embedding space of the resulting classifiers will also be analyzed using margin theory~\cite{dong2019margingan}, which will provide insight on where the complementary space lies. 

The margin of a sample $x$ belonging to category $y$ is a single numeric description of its position in the embedding space. The following equation gives the margin of a sample: $\text{margin}(x, y) = C_y(x) - \max_{i \neq y}C_i(x)$. More specifically, the margin measures the confidence of the classifier in its prediction by subtracting the maximum output probability of all other categories, $\max_{i \neq y}C_i(x)$, from the output probability of the true category, $C_y(x)$. The larger the absolute value of the margin, the more confident the classifier is, while a positive margin represents a correct prediction and a negative margin represents an incorrect prediction. By calculating the average margins of testing samples and generated samples, the embedding space can be visualized and analyzed in a meaningful way. Note that the true category for novel categories and generated samples is designated as the predicted category by the classifier. 

All experiments are conducted using the SVHN and CIFAR10 datasets with training and testing setups as per standard SSL and OSR protocols. For SSL, 100 and 400 labeled training samples are provided per category for SVHN and CIFAR10, respectively. During testing, the CIFAR10 test set is used to introduce novel categories within the SVHN experiments, and the CIFAR100 test set is used to introduce novel categories within the CIFAR10 experiments. For the supervised experiments, we conduct two variations using the CIFAR10 dataset. Specifically, the CIFAR10$\dagger$ supervised experiment uses all training samples from the original dataset and testing samples from CIFAR10 and CIFAR100 to insert unobserved novel categories into the domain. For the CIFAR10$\star$, $K = 6$ random categories out of the possible $10$ are defined as labeled categories, while the remaining $4$ categories are used as unobserved novel categories. 

The results for the ARP-GAN, FM-GAN, and their respective baselines (i.e. a traditional supervised classifier or a supervised ARP classifier) are presented in the semi-supervised and supervised columns of Table \ref{table:1}. It is clear that both GAN models improve over the baselines and that ARP-GANs perform competitively with FM-GANs in SSL, and FM-GANs compare competitively with ARP-GANs in OSR. We do note that FM-GANs consistently produce slightly better accuracy scores and ARP-GANs consistently provide slightly better AUROC scores, yet they still achieve similar results. It is also interesting to note that the SVHN baselines fail to learn, which we attribute to the SVHN images containing multiple digit categories in a single image. Thus, only when the model has access to the entire training set (i.e. labeled and unlabeled data) can the classifier generalize appropriately.  

To further study the theoretical similarities between FM-GANs and ARP-GANs, Fig. \ref{fig:2} presents bar graphs that visualise the embedding space of different categories and generated samples for the CIFAR10 supervised experiment. More specifically, the bar graphs show the average margins for all testing samples across the $K$ labeled categories, the average margins for all novel category testing samples, and the average margins for $100$ batches of generated samples. From these bar graphs, we can see that the embedding space of both ARP-GANs and FM-GANs are similar for labeled and novel categories, as would be expected considering their similar results. It is also important to note that generated samples generally exhibit lower margins than novel categories (although ARP-GANs have larger margins for generated samples than FM-GANs), and that novel categories consistently exhibit lower margins compared to the labeled categories. 

\begin{figure*}[!t]
\centering
\subfloat[FM-GAN]{\includegraphics[width=0.49\textwidth, height=6.0cm]{margins-sup-fm.png}
\label{fig:2a}}
\subfloat[ARP-GAN]{\includegraphics[width=0.49\textwidth, height=6.0cm]{margins-sup-arp.png}
\label{fig:2b}}
\caption{Visualising the embedding space through average margins of trained FM-GANs and ARP-GANs for the CIFAR10 supervised experiment. The true categories for novel category samples and generated samples is designated as their predicted categories.}
\label{fig:2}
\end{figure*}

Lower margins for novel categories compared labeled categories is expected to ensure separation of novel categories using thresholds. Furthermore, lower margins of generated samples compared to labeled categories is expected as the generated samples aim to lower the confidence of the classifier's predictions. Finally, lower margins for generated samples from FM-GANs compared to ARP-GANs might indicate a unique insight into the performance of these models. However, this specific result extends directly into the second phase and so will be discussed further on. Besides this, another noteworthy result are the variations in the differences between margins for different categories. A larger difference between labeled and novel categories suggests an easier separation via thresholding, while a smaller difference indicates a greater challenge for the classifier (i.e. categories 3 and 5 for both models). To the best of our knowledge, no previous study has used average margins to visualize the embedding space, and these results could certainly be used to address so-called problematic categories in the domain. 

Finally, as an ad-hoc experiment, we also conducted the SSL-OSR experiment from the unpublished study by Kliger et al.~\cite{kliger2018novelty2}. Specifically, Kliger conducted the same SSL CIFAR10 experiment as described above, yet they defined $3000$ labels per labeled category. Their results showed an AUROC of $80.128$\% (while they did not indicate the SSL accuracy), where our FM-GAN achieves an AUROC of $83.87$\%, and the ARP-GAN achieves and AUROC of $85.09$\%. Our results outperform their proposed model, and so this study boasts state-of-the-art SSL-OSR performance. Considering that FM-GANs and ARP-GANs consistently achieve similar results in both evaluation metrics and their resulting embedding spaces, we can conclude that these models function in a similar fashion. More specifically, both FM-GANs and ARP-GANs generate samples in the complementary space and both models use these samples to generalize the open space in the $K + 1$'th category. However, it is not yet clear where the optimal complementary space for generated samples lies in the embedding space. Furthermore, the SSL results from this first-phase do not compare well to previously published studies. Therefore, we move to study the state-of-the-art SSL-GAN. 


\section{Second phase}
The first phase of the study demonstrated that the foundational SSL FM-GANs inserted into the ARP-GAN codebase achieve comparable results to the state-of-the-art OSR methods. However, since ARP-GANs were optimized for OSR and not SSL, the SSL results from the first phase did not compare well to the current state-of-the-art SSL methods (see Table~\ref{table:1}). In this second phase, we aim to reinforce our hypothesis on the link between SSL-OSR by thoroughly experimenting with the state-of-the-art SSL-GAN. Specifically, we apply margin-GANs by Dong et al.~\cite{NEURIPS2019_517f24c0} in the OSR and SSL-OSR settings. The results from this second phase will further our understanding of the SSL-OSR link and provide a new standard for SSL-OSR. We follow the same layout as the first phase by discussing the technical overview of margin-GANs and then conducting experiments.

\subsection{Technical preliminaries}
This section provides a technical overview of margin-GANs. Margin-GANs adopt a similar setup as ARP-GANs by developing a three-player game between the generator, discriminator, and classifier. Although margin-GANs do not explicitly discuss an additional $K+1$'th category, the loss functions still use generated samples to refine the boundaries around labeled categories. Specifically, the classifier decreases the margins for generated samples, while the generator increases the margins for generated samples. By doing so, we will show that generated samples fall within complementary space. Margin-GANs also appropriately fit into the overarching conversation of this study, namely that regularizing a classifier with generated samples that fall within the complementary space results in generalizing the open space.

\subsection{Margin-GANs}
Margin-GANs apply a unique GAN framework to the mean-teacher (MT) SSL model~\cite{tarvainen2017mean}. The MT model is based on a student-teacher training approach, where the teacher network is the exponential moving average of the student network's parameters. In addition to a general supervised loss for labeled data, the student network is trained on unlabeled data by minimizing a consistency loss (such as mean squared error) between its predictions for the unlabeled data and the teacher network's predictions for the same data. This consistency loss ensures that the student's predictions remain consistent throughout its updates and provides feedback to the model for unlabeled training samples.

It is important to highlight the similarity between MT models and pseudo-labelling models~\cite{lee2013pseudo, pham2021meta}. Pseudo-labelling uses the classifier network's output predictions for unlabeled data as the ground-truth labels for these samples, thus reinforcing the network's predictions. However, this approach is heavily dependent on the accuracy of these predictions, as any incorrect pseudo-label would negatively impact the generalization of the model. MT models offer improved performance as the "pseudo-labels" generated by the model are not treated as absolute ground-truth labels but instead as soft-pseudo labels over all $K$ categories. Margin-GANs build on the MT model by incorporating a GAN into the training process, creating a three-player game between the discriminator, generator, and student-teacher networks.

In margin-GANs, the discriminator's task is similar to that in vanilla GANs, which is to distinguish between real training samples in $D_{\text{train}}$ and generated samples $\mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} G(z)$. However, unlike vanilla GANs, the generator in margin-GANs must deceive both the discriminator and the student-teacher into believing that the generated samples are real. As a result, the student-teacher network also plays a crucial role in the game by having to accurately differentiate between real and fake samples, similar to FM-GANs. However, margin-GANs propose a unique variation to the FM-GAN cost function in eq.~\ref{eq:cost_c_fm} to separate real and fake samples using a $K$-classifier network. Specifically, margin-GANs deploy the inverse cross-entropy loss function. 

Margin-GANs aim to minimize the classification margin of fake generated samples. As previously mentioned, the classification margin is calculated as the difference between the predicted probability for the true category and the highest predicted probability for all other categories (i.e. $\text{margin}(x, y) = C_y(x) - \max_{i \neq y}C_i(x)$). The classifying network must maximize the margin for real training samples and minimize the margin for fake generated samples. The cross-entropy loss function, $ -\mathbb{E}{(x, y) \sim D{\text{lab-train}}} [\log(p_{\text{class}}(y|x))]$, already maximizes the margin for the correct category $y \in C_K$. To minimize the margin for fake generated samples, margin-GANs minimize the inverted cross-entropy in the form of $- \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(1 - p_{\text{class}}(y'|G(z)))]$, where $y'$ is the pseudo-label for the generated sample or the classifier's prediction. The loss functions for the three networks in margin-GANs are as follows:


\begin{equation}
\label{eq:cost_g_mg}
\begin{split}
   \text{G-Loss}_{\text{\; MG}} = 
   & - \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(D(G(z))) \\
   & - \; \log(1 - p_{\text{class}}(y'|G(z)) \;)]   
\end{split}
\end{equation}

\begin{equation}
 \label{eq:cost_d_mg}
 \begin{split}
 \text{D-Loss}_{\text{\; MG}} =
 & - \mathbb{E}_{x \sim D_{\text{train}}} [\log(D(x))] \\
 & - \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(1 - D(G(z)))]
 \end{split} 
\end{equation}

\begin{equation}
\label{eq:cost_c_mg}
\begin{split}
  \text{C-Loss}_{\text{\; MG}} = 
 & - \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(1 - p_{\text{class}}(y'|G(z)) \;) \\
 & - \; \mathbb{E}_{(x, y) \sim D_{\text{lab-train}}} [\log(p_{\text{class}}(y|x))] \\
  & - \; \mathbb{E}_{(x) \sim D_{\text{unlab-train}}} [\log(p_{\text{class}}(y'|x))]  \\
\end{split}
\end{equation}

The last term of the classifying network cost function in eq.~\ref{eq:cost_c_mg} represents the pseudo-labelling cost function for unlabeled training samples. However, as mentioned above, margin-GANs use MT models as their classifying networks. Therefore, instead of the pseudo-labelling cost function, margin-GANs use the consistency loss between the student's and teacher's predictions for unlabeled training samples. Let $T(x)$ represent the teacher network (which is the exponential moving average of the student's parameters), then the classifier's loss function becomes:

\begin{equation}
\label{eq:cost_c_mg2}
\begin{split}
  \text{C-Loss}_{\text{\; MG-MT}} = 
 & - \; \mathbb{E}_{z \sim \mathbb{P}^{\text{(z)}}} [\log(1 - p_{\text{class}}(y'|G(z)) \;) \\
 & - \; \mathbb{E}_{(x, y) \sim D_{\text{lab-train}}} [\log(p_{\text{class}}(y|x))] \\
  & - \; \mathbb{E}_{(x) \sim D_{\text{unlab-train}}} [(C(x) - T(x))^2] \\
\end{split}
\end{equation}

The loss functions used in margin-GANs make it clear that the generator aims to 1) produce realistic-looking samples according to the discriminator and 2) produce samples that fall inside the boundaries of labeled categories according to the classifier. However, in the adversarial game, the discriminator sees generated samples as fake, and the classifier updates so that generated samples fall outside the classification boundaries. As a result, the generated samples jump in and out of the classification boundaries, arguable placing them in the complementary space (see Fig. \ref{fig:1c}). In the first phase of this study, we concluded that using generated samples within the complementary space to generalize the $K+1$'th category generalizes the open space. Therefore, considering that margin-GANs minimize the margins for generated samples in the complementary space, we can expect also margin-GANs to perform well in OSR. 

\subsection{Experiments}
In this second phase, we focus our experiments on the CIFAR10 dataset, which is widely considered one of the most challenging datasets in image classification research. Specifically, the SSL-OSR experiment from the first phase is repeated, again using 400 labels per category and the rest of the samples as unlabeled. The test set includes both the CIFAR10 and CIFAR100 test sets. Subsequently, we also repeat the supervised experiments, again referred to as CIFAR10$\dagger$ and CIFAR10$\star$. In CIFAR10$\dagger$, all labels in the CIFAR10 training set are revealed for training and the full CIFAR10 and CIFAR100 testing sets are used for testing. In CIFAR10$\star$, $K = 6$ random categories are selected as labeled categories for which all training labels are revealed, while the full CIFAR10 test set is used for testing. Thus, the remaining 4 categories are treated as novel categories. All experiments are conducted using the margin-GAN codebase~\footnote{See https://github.com/DJjjjhao/MarginGAN}, and so the same optimisation and hyperparameter choices are used for all experiments. 

The results of each experiment are presented in Table~\ref{table:2}, along with a comparison to ARP-GANs from the first phase. These results have several noteworthy observations: 1) margin-GANs significantly outperform ARP-GANs in terms of SSL performance, which was expected; 2) margin-GANs set a new standard for OSR performance under the combined SSL-OSR setting; 3) margin-GANs surpass ARP-GANs in the CIFAR10$\dagger$ experiment, establishing a new benchmark for general OSR, and 4) however, margin-GANs do not outperform ARP-GANs in the CIFAR10$\star$ experiment, suggesting that ARP-GANs are better suited for detecting novel categories that are closely related to the labeled categories. In conclusion, margin-GANs are at the forefront of SSL-OSR and either achieve state-of-the-art results or perform competitively for general OSR. 

\begin{table}[t]
\caption{Results for our SSL, OSR and SSL-OSR experiments in the second phase. Each result is split as $a \; | \; b $ with $a$ representing the accuracy for labeled categories and $b$ representing the $\text{AUROC} * 100$. Results are averaged over five randomised trials.}
\centering
%\begin{tabular}{m{3.2cm} m{2.2cm} m{2.2cm} m{0.3cm} m{2.2cm} m{2.2cm}}
\begin{tabular}{p{0.9cm} p{1.5cm} p{1.5cm} p{1.5cm}}
\hline
 & \multicolumn{1}{c}{\textbf{Semi-supervised}} & \multicolumn{2}{c}{\textbf{Supervised}} \\
\hline
 & CIFAR10 & CIFAR10$\dagger$ & CIFAR10$\star$ \\
\hline
\textbf{GANs} & & & \\
ARP-GAN & 77.62 $|$ 77.02  & 89.84 $|$ 86.32 & 94.5 $|$ 91.00  \\
Margin-GAN & 92.93 $|$ 88.91 & 96.35 $|$ 91.75  & 96.22 $|$ 87.75  \\
\hline
\end{tabular} \\
\begin{tablenotes}
\item \textit{Results are averaged over five randomized trials}
\end{tablenotes}
\label{table:2}
\end{table}

To further the discussion on the complementary space and the open space, it is important to examine the generated samples from margin-GANs. Fig.~\ref{fig:3} presents examples of real CIFAR10 images and generated images from the supervised CIFAR10 experiment. The images generated by the generator are visibly of lower quality compared to the real images. More specifically, it is clear that the generated images have indistinct features, lack sharp definition and have a blurred and amorphous appearance. This demonstrates that the generated images do not conform to the real training distribution, which matches the notion of generating 'bad-looking' samples in the complementary space. However, these samples do still produce similar qualities (e.g. colors and contrast) to the real images, as required to be on the classification boundaries of labeled categories. 

The average margins of labeled categories, novel categories and generated samples for the supervised CIFAR10 experiment is shown in Fig. \ref{fig:4}. Similar to the results in the first phase, novel categories consistently produce margins lower than labeled categories as required to separate novel categories using thresholds. Furthermore, generated samples generally produce lower predictions compared to labeled and novel categories. It is also clear that margin-GANs produce generated samples with lower margins than those from FM-GANs and ARP-GANs in the first phase (as seen in Fig. \ref{fig:2}). Although lower margins for generated samples seems necessary for higher accuracy SSL, it is still unclear if lower margins are required for OSR. Nevertheless, considering that margin-GANs also perform well at OSR, we can conclude that SSL-GANs and OSR-GANs function similarly. 

\begin{figure}[!t]
\centering
\includegraphics[width=3.7in]{margins-sup-mg.png}
%where an .eps filename suffix will be assumed under latex, 
%and a .pdf suffix will be assumed for pdflatex; or what has been declared
%via \DeclareGraphicsExtensions.
\caption{Visualising the embedding space through average margins of the trained margin-GAN for the CIFAR10 supervised experiment. The true categories for novel category samples and generated samples is designated as their predicted categories.}
\label{fig:4}
\end{figure}

\begin{figure*}[!t]
\centering
\subfloat[Real images]{\includegraphics[width=0.4\textwidth]{real.png}
\label{fig:3a}}
\hfil
\centering
\subfloat[Generated images]{\includegraphics[width=0.4\textwidth]{generated.png}
\label{fig:3b}}
\caption{Comparing real and generated images from SSL margin-GANs - (a) shows examples of real CIFAR10 images; (b) shows examples of generated images from the trained margin-GAN on the SSL CIFAR10 experiment. }
\label{fig:3}
\end{figure*}

\section{Discussion}
The primary hypothesis of this study is that SSL-GANs and OSR-GANs function in the same manner. In particular, the generators in SSL-GANs and OSR-GANs are tasked with producing samples that lie in the complementary space, which are then used to regularize the classifying networks. In turn, the classifier generalizes the open space within the $K + 1$'th category, facilitating the separation of novel categories. The results from both the first and second phases clearly show that generated samples produce margins that are lower than those of any labeled and novel categories. This outcome is expected, as generated samples are intended to lower the confidence of classifiers and allow for the separation of novel categories. However, it is still being determined what the ideal margins would be for samples in complementary space.

For SSL-GANs, the complementary space is defined as the region surrounding the classification boundaries of labeled categories. For OSR-GANs, the open space is defined as the center of the the embedding space. In other words, OSR-GANs define samples in the global open space as those with an equal probability spread across all $K$ categories. Considering our results, both SSL-GANs and OSR-GANs clearly generalize the open space within the $K + 1$'th category. We are therefore warranted to extend the definition of the complementary to that of the open space. To do so, two factors must be taken into consideration - 1) the definition of the $K + 1$'th category in FM-GANs, and 2) the definition of the complementary space in margin-GANs.  

According to the theory of FM-GANs, the $K + 1$'th category is set as the zero'th vector across $K$ nodes. In other words, generated samples in the complementary space should produce prediction scores as low as possible. However, a softmax output activation function means the probability distribution over $K$ nodes will always sum to one, and so no sample can ever output the zero'th vector. Nevertheless, generated samples should output as low logits as possible across all $K$ output nodes. For OSR, generated samples should produce prediction scores of $1/K$ at every output node. However, our results show that the margins of generated samples in SSL-GANs and OSR-GANs do not produce very low predictions scores. Although we could attribute this to overconfident neural networks, a closer look at the difference between margins of SSL-GANs and OSR-GANs shows unique insights. 

If, as we suggest, we extend the definition of the complementary space to that of the open space, the complementary space would not only represent the area around the classification boundaries of labeled categories, but also represent the regions between categories in the embedding space. Consider a domain with $K = 2$ categories. In such a case, the reciprocal points of each category would produce predictions equal to $1/K = 0.5$, setting the boundary between the two categories. Subsequently, the complementary space of each category would be any sample with a prediction between $0.5$ and the lowest prediction score for samples known to belong to that category. In the case of $K = 2$, defining the complementary space in relation to the open space is trivial. However, as $K$ increases, so the curse of higher dimensionality leads to an increasingly complicated definition.

Suppose we extend the domain to $K = 3$. The definitions of the complementary space and the open space would now need to consider several regions - 1) the area between categories one and two, 2) the area between categories one and three, 3) the area between categories two and three, and 4) the area between categories one, two and three (which is the global open space). For the open space, reciprocal points of each category would now produce predictions equal to $1/3$. In this case, the complementary space would be between $1/3$ and the lowest prediction of the specific category. Clearly, if we assume that each category's lowest prediction remains generally the same no matter the number of categories, then the complementary space of each category increases in size as the number of categories increase. However, the margins of generated samples within our experiments (see Fig. \ref{fig:2} and Fig. \ref{fig:4}) indicate that the complementary space is far bigger than the proposed $1/K = 1/10$. 

Margin theory provides further understanding of why generated samples have much higher predictions than the theorised $1/K$. Again, consider over overconfident neural networks. For such classifiers, the prediction for the primary category is so high that the remaining spread of the probability distribution is heavily weighted towards the secondary category. Therefore, margin theory is a better representation of the embedding space than viewing all $K$ categories concurrently. More specifically, only the category in question and its most similar category is considered. Under margin theory, the global open space would not be at $1/K$ but rather at $1/2$, no matter the number of categories. Subsequently, the complementary space of a category would always be between $1/2$ and the lowest prediction score of that category. 

Considering the margins of generated samples from our experiments, it is clear that the complementary space lies around $1/2$ instead of $1/K$ in the embedding space. This result could be utilised by both SSL-GANs and OSR-GANs in future optimisation studies, as the generator can now be fine-tuned to exactly match the complementary space. However, it still remains unclear why margin-GANs produce the highest SSL results but have the lowest margins for generated samples. Furthermore, ARP-GANs produce the highest OSR results (particularly for the more complicated CIFAR10$\star$ experiment), but have the highest margins for generated samples. Although further experimentation is required, these discrepancies indicate a possible trade-off between SSL and OSR performance. However, such experiments are left for future work that aims to simultaneously optimise SSL and OSR. 

\section{Conclusion}
This study concludes that generative SSL models and generative OSR models operate in a similar fashion. More specifically, both SSL-GANs and OSR-GANs require their generators to produce samples that lie within the complementary space. Subsequently, when using generated samples to regularize the classifying network, the open space is generalized in the $K + 1$'th category. Our findings suggest that SSL-GANs are capable of OSR and OSR-GANs are capable of SSL. With a stronger theoretical foundation, our results indicate that the SSL optimised margin-GANs produce state-of-the-art results for SSL-OSR and certain general OSR experiments. However, OSR optimised ARP-GANs still outperform margin-GANs in other general OSR experiments. Thus, future studies should explore the relationship between SSL-OSR under the overarching conversation of the complementary space, reciprocal points and margin theory. Nevertheless, the link between SSL-OSR is clearly established on a theoretical foundation, establishing a new combined research path. 

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{1.PNG}
 %where an .eps filename suffix will be assumed under latex, 
 %and a .pdf suffix will be assumed for pdflatex; or what has been declared
 %via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.



% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%
%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.

% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


The authors express their gratitude to the following organizations for their financial support of our research over the last five years: 1) The Department of Trade, Industry, and Competition of South Africa, 2) Stellenbosch University, 3) The Wilhelm Frank Bursary Initiative at Stellenbosch University, and 4) Chronux Research. 


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{geng2020recent}
Geng, C., Huang, S.J. and Chen, S., 2020. Recent advances in open set recognition: A survey. IEEE transactions on pattern analysis and machine intelligence, 43(10), pp.3614-3631.

\bibitem{yang2021generalized}
Yang, J., Zhou, K., Li, Y. and Liu, Z., 2021. Generalized out-of-distribution detection: A survey. arXiv preprint arXiv:2110.11334.

\bibitem{pang2021semi}
Pang, T., Wong, J.H.D., Ng, W.L. and Chan, C.S., 2021. Semi-supervised GAN-based radiomics model for data augmentation in breast ultrasound mass classification. Computer Methods and Programs in Biomedicine, 203, p.106018.

\bibitem{pahar2021automatic}
Pahar, M., Klopper, M., Reeve, B., Warren, R., Theron, G. and Niesler, T., 2021. Automatic cough classification for tuberculosis screening in a real-world environment. Physiological Measurement, 42(10), p.105014.

\bibitem{wu2019semi}
Wu, S., Lin, S., Wu, W., Azzam, M. and Wong, H.S., 2019. Semi-supervised pedestrian instance synthesis and detection with mutual reinforcement. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 5057-5066).


\bibitem{andersen2020proximal}
Andersen, K.G., Rambaut, A., Lipkin, W.I., Holmes, E.C. and Garry, R.F., 2020. The proximal origin of SARS-CoV-2. Nature medicine, 26(4), pp.450-452.

\bibitem{ramos2017detecting}
Ramos, S., Gehrig, S., Pinggera, P., Franke, U. and Rother, C., 2017, June. Detecting unexpected obstacles for self-driving cars: Fusing deep learning and geometric modeling. In 2017 IEEE Intelligent Vehicles Symposium (IV) (pp. 1025-1032). IEEE.

\bibitem{puertas2021should}
Puertas-Ramirez, D., Serrano-Mamolar, A., Martin Gomez, D. and Boticario, J.G., 2021, June. Should Conditional Self-Driving Cars Consider the State of the Human Inside the Vehicle?. In Adjunct Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization (pp. 137-141).

\bibitem{van2020survey}
Van Engelen, J.E. and Hoos, H.H., 2020. A survey on semi-supervised learning. Machine learning, 109(2), pp.373-440.

\bibitem{NIPS2016_8a3363ab}
Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A. and Chen, X., 2016. Improved techniques for training gans. Advances in neural information processing systems, 29.

\bibitem{dai2017good}
Dai, Z., Yang, Z., Yang, F., Cohen, W.W. and Salakhutdinov, R.R., 2017. Good semi-supervised learning that requires a bad gan. Advances in neural information processing systems, 30.

\bibitem{kumar2017semi}
Kumar, A., Sattigeri, P. and Fletcher, T., 2017. Semi-supervised learning with gans: Manifold invariance with improved inference. Advances in neural information processing systems, 30.

\bibitem{NEURIPS2019_517f24c0}
Dong, J. and Lin, T., 2019. MarginGAN: adversarial training in semi-supervised learning. Advances in neural information processing systems, 32.

\bibitem{li2020semi}
Li, W., Wang, Z., Yue, Y., Li, J., Speier, W., Zhou, M. and Arnold, C., 2020. Semi-supervised learning using adversarial training with good and bad samples. Machine Vision and Applications, 31, pp.1-11.

\bibitem{ge2017generative}
Ge, Z., Demyanov, S., Chen, Z. and Garnavi, R., 2017. Generative openmax for multi-class open set classification. arXiv preprint arXiv:1707.07418.

\bibitem{neal2018open}
Neal, L., Olson, M., Fern, X., Wong, W.K. and Li, F., 2018. Open set learning with counterfactual images. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 613-628).

\bibitem{jo2018open}
Jo, I., Kim, J., Kang, H., Kim, Y.D. and Choi, S., 2018, April. Open set recognition by regularising classifier with fake data generated by generative adversarial networks. In 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP) (pp. 2686-2690). IEEE.

\bibitem{chen2021adversarial}
Chen, G., Peng, P., Wang, X. and Tian, Y., 2021. Adversarial reciprocal points learning for open set recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11), pp.8065-8081.

\bibitem{goodfellow_deep_learning}
Goodfellow, I., Bengio, Y. and Courville, A., 2016. Deep learning. MIT press.

\bibitem{pham2021meta}
Pham, H., Dai, Z., Xie, Q. and Le, Q.V., 2021. Meta pseudo labels. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 11557-11568).

\bibitem{gruhl2021novelty}
Gruhl, C., Sick, B. and Tomforde, S., 2021. Novelty detection in continuously changing environments. Future Generation Computer Systems, 114, pp.138-154.

\bibitem{pimentel2014review}
Pimentel, M.A., Clifton, D.A., Clifton, L. and Tarassenko, L., 2014. A review of novelty detection. Signal processing, 99, pp.215-249.

\bibitem{din2021data}
Din, S.U., Shao, J., Kumar, J., Mawuli, C.B., Mahmud, S.H., Zhang, W. and Yang, Q., 2021. Data stream classification with novel class detection: A review, comparison and challenges. Knowledge and Information Systems, 63, pp.2231-2276.

\bibitem{belouadah2020comprehensive}
Belouadah, E., Popescu, A. and Kanellos, I., 2021. A comprehensive study of class incremental learning algorithms for visual tasks. Neural Networks, 135, pp.38-54.

\bibitem{zhang2020class}
Zhang, J., Zhang, J., Ghosh, S., Li, D., Tasci, S., Heck, L., Zhang, H. and Kuo, C.C.J., 2020. Class-incremental learning via deep model consolidation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 1131-1140).

\bibitem{yu2020multi}
Yu, Q., Ikami, D., Irie, G. and Aizawa, K., 2020. Multi-task curriculum framework for open-set semi-supervised learning. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XII 16 (pp. 438-454). Springer International Publishing.

\bibitem{da2014learning}
Da, Q., Yu, Y. and Zhou, Z.H., 2014, June. Learning with augmented class by exploiting unlabeled data. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 28, No. 1).

\bibitem{engelbrecht2020learning}
Engelbrecht, E.R. and du Preez, J.A., 2020. Learning with an augmented (unknown) class using neural networks. Scientific African, 10, p.e00600.

\bibitem{blanchard2010semi}
Blanchard, G., Lee, G. and Scott, C., 2010. Semi-supervised novelty detection. The Journal of Machine Learning Research, 11, pp.2973-3009.

\bibitem{chen2020semi}
Chen, Y., Zhu, X., Li, W. and Gong, S., 2020, April. Semi-supervised learning under class distribution mismatch. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 04, pp. 3569-3576).

\bibitem{engelbrecht2020open}
Engelbrecht, E.R. and Preez, J.A.D., 2020. Open set learning with augmented category by exploiting unlabeled data (open-LACU). arXiv preprint arXiv:2002.01368.


\bibitem{sajun2022survey}
Sajun, A.R. and Zualkernan, I., 2022. Survey on implementations of generative adversarial networks for semi-supervised learning. Applied Sciences, 12(3), p.1718.

\bibitem{springenberg2015unsupervised}
Springenberg, J.T., 2015. Unsupervised and semi-supervised learning with categorical generative adversarial networks. arXiv preprint arXiv:1511.06390.

\bibitem{odena2016semi}
Odena, A., 2016. Semi-supervised learning with generative adversarial networks. arXiv preprint arXiv:1606.01583.

\bibitem{xu2021semi}
Xu, Z., Wang, H. and Yang, Y., 2021. Semi-supervised self-growing generative adversarial networks for image recognition. Multimedia Tools and Applications, 80, pp.17461-17486.

\bibitem{dong2019margingan}
Dong, J. and Lin, T., 2019. MarginGAN: adversarial training in semi-supervised learning. Advances in neural information processing systems, 32.

\bibitem{Litriplegans}
Li, C., Xu, K., Zhu, J., Liu, J. and Zhang, B., 2021. Triple generative adversarial networks. IEEE transactions on pattern analysis and machine intelligence, 44(12), pp.9629-9640.

\bibitem{perera2019ocgan}
Perera, P., Nallapati, R. and Xiang, B., 2019. Ocgan: One-class novelty detection using gans with constrained latent representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2898-2906).

\bibitem{sabokrou2018adversarially}
Sabokrou, M., Khalooei, M., Fathy, M. and Adeli, E., 2018. Adversarially learned one-class classifier for novelty detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3379-3388).

\bibitem{zhang2021adversarially}
Zhang, Y., Zhou, B., Ding, X., Ouyang, J., Cai, X., Gao, J. and Yuan, X., 2021. Adversarially learned one-class novelty detection with confidence estimation. Information Sciences, 552, pp.48-64.

\bibitem{cappozzo2020anomaly}
Cappozzo, A., Greselin, F. and Murphy, T.B., 2020. Anomaly and Novelty detection for robust semi-supervised learning. Statistics and Computing, 30(5), pp.1545-1571.

\bibitem{kliger2018novelty2}
Kliger, M. and Fleishman, S., 2018. Novelty detection with gan. arXiv preprint arXiv:1802.10560.

\bibitem{kliger2018novelty}
Kliger, M. and Fleishman, S., Intel Corp, 2018. Novelty detection using discriminator of generative adversarial network. U.S. Patent Application 15/626,457.

\bibitem{sung2019difference}
Sung, Y.L., Hsieh, S.H., Pei, S.C. and Lu, C.S., 2020. Difference-Seeking Generative Adversarial Network--Unseen Sample Generation. In International Conference on Learning Representations.


\bibitem{goodfellow2020generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y., 2020. Generative adversarial networks. Communications of the ACM, 63(11), pp.139-144.

\bibitem{arjovsky2017wasserstein}
Arjovsky, M., Chintala, S. and Bottou, L., 2017, July. Wasserstein generative adversarial networks. In International conference on machine learning (pp. 214-223). PMLR.


\bibitem{springenberg2015unsupervised}
Springenberg, J.T., 2015. Unsupervised and semi-supervised learning with categorical generative adversarial networks. arXiv preprint arXiv:1511.06390.

\bibitem{odena2016semi}
Odena, A., 2016. Semi-supervised learning with generative adversarial networks. arXiv preprint arXiv:1606.01583.


\bibitem{tarvainen2017mean}
Tarvainen, A. and Valpola, H., 2017. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30.

\bibitem{lee2013pseudo}
Lee, D.H., 2013, June. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML (Vol. 3, No. 2, p. 896).


\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{engelbrecht.png}}]{Emile Reyn Engelbrecht} is a PhD candidate in Electronic Engineering at Stellenbosch University in South Africa. He received a PhD upgrade from his Master's degree in 2020 and is currently working on his dissertation. Reyn's research focuses on synthesizing various fields to develop more practical and effective classifier training methods. He aims to apply his methods on a variety of real-word applications. Reyn is also passionate in philosophy of A.I. and has been part of the SACAIR conference to promote ethical A.I. In addition, Reyn is also the CEO of Reyn-Energy, a startup that develops innovative renewable energy solutions.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{dupreez.png}}]{Johan du Preez} received his PhD degree in electronic engineering from Stellenbosch University, in 1998. He joined the Department of Electrical and Electronic Engineering at Stellenbosch in 1989. He is active in the broader fields of signal processing and pattern recognition, with a particular research interest that includes developing advanced structured probabilistic models to address speech, image and text processing problems, and advanced inference methods. 
\end{IEEEbiography}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage


% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
\enlargethispage{-5in}



% that's all folks
\end{document}


