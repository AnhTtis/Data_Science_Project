\section{Related Work}

\textbf{Traditional methods for indoor lighting estimation. } 
To capture the lighting distribution at a target location in an indoor scene, a common practice is to use a physical probe~\cite{debevec2008rendering,debevec2012single,reinhard2010high} or other 3D objects~\cite{weber2018learning,georgoulis2017around,calian2018faces,yi2018faces}. However, inserting physical objects into the scene is not feasible in many scenarios. Moreover, it is usually expensive and has poor scalability. 
Another line of works tries to estimate illumination by jointly optimizing the geometry, reflectance properties, and lighting models of the scene. In order to find the set of values that best explained the observed input image, previous works~\cite{karsch2011rendering,karsch2014automatic,lombardi2015reflectance,zhang2016emptying} regressed the relevant parameters by minimizing the difference between the generated image and the ground truth. 

\textbf{Deep learning-based methods for indoor lighting estimation. } 
Recently, significant progress has been made in lighting estimation with deep learning. Some works~\cite{eigen2015predicting,gardner2017learning,lettry2018darn,eigen2014depth} directly trained convolutional neural networks to estimate scene parameters including albedos, normals, shadows and lights from a single image. EMLight~\cite{zhan2020emlight} inferred Gaussian maps to guide the illumination map generation. However, it simplified the scene
geometry to be a spherical surface and did not allow to predict spatially-varying lighting. Inspired by earth mover’s distance, GMLight~\cite{zhan2022gmlight} added a geometric mover’s loss to the pipeline in EMLight. 
Given a perspective image and a 2D location in that image, Garon \emph{et al.}~\cite{garon2019fast} achieved real-time spatially-varying lighting estimation by regressing a 5th order SH representation of the lighting at the selected location with CNNs. 
For estimating the incident illumination at any 3D location, Srinivasan \emph{et al.}~\cite{srinivasan2020lighthouse} predicted unobserved scene contents with a 3D CNN. Xu \emph{et al.} \cite{xu2020real} detected the planar surfaces in the scene and computed its overall illumination. Inspired by classic volume rendering techniques, Wang \emph{et al.}~\cite{wang2021learning} proposed a unified, learning-based inverse rendering framework that can recover 3D spatially-varying lighting from a single image. These works were plagued by the limited receptive fields and predicted inconsistent contents in those regions missing dued to pixel streching.
 
