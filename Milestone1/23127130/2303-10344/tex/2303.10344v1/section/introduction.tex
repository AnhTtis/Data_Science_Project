
\section{Introduction}
Estimating high-dynamic range (HDR) indoor illumination from a single low-dynamic range (LDR), perspective image is a fundamental problem in computer vision and graphics. It has a huge impact on many applications including augmented and mixed reality (AR/MR) \cite{10.1111/cgf.12591}, scene understanding \cite{li2020inverse} and relighting \cite{https://doi.org/10.1111/cgf.14283}. 
However, this problem is highly ill-posed as the pixel intensities in an observed perspective image are a complex function of scene geometries, material properties and lighting distributions.

With the advent of deep learning and large-scale datasets, convolutional neural networks (CNNs) are now the de-facto architectures for solving this ill-posed problem. However, many existing CNNs only recover a single global environment map \cite{gardner2017learning,apple-hdr,zhan2020emlight}, or some other kinds of compact representations, \emph{e.g.}, Spherical Harmonics (SH) \cite{https://doi.org/10.1111/cgf.13561} and Spherical Gaussians (SG) \cite{10.1145/3338533.3366562,Zhan_2021_ICCV}, at the camera's viewpoint of the input perspective image. This completely ignores spatially-varying lighting effects that dominate indoor scenarios and hence results in unrealistic shading and shadows for inserted virtual objects. 

Several works have noticed this challenge and tried to recover the independent illumination at each locale (the 3D location of the inserted object). For instance, Song and Funkhouser \emph{et al.}~\cite{song2019neural} have suggested decomposing the indoor illumination estimation problem into three sub-tasks: depth-based image warping, LDR panorama inpainting and HDR panorama reconstruction. The success of this pipeline lies in the second sub-task which tries to recover missing contents for the incomplete panorama. However, directly applying existing convolution-based image inpainting networks \cite{iizuka2017globally,li2017generative,liu2019coherent,pathak2016context,yu2018generative} to the incomplete panorama is impractical due to (1) the hole regions of warped images are very large ($>60$\% in general), which may easy let many image inpainting methods fail; (2) panoramas have severe spatially-varying distortions that will mislead convolution-based feature extraction modules. 

More importantly, as linear operations have a limited receptive field, conventional convolutions fail to capture long-distance relationships that prevail in panoramas. To fill large holes in the warped panorama according to long-range dependencies, we resort to a transformer-based architecture, dubbed \emph{PanoTransformer}. 
Our PanoTransformer is specially designed for handling distorted spherical signals with cubemap projections and is adept at restoring visually plausible and globally consistent contents in the large out-of-view regions. 
We observe that many relatively small but dense holes are actually caused by pixel stretching during image warping at a given locale. Considering that these missing regions are visible in the input, we design a local inpainting module and apply it on the warped incomplete panorama before restoring global structures via PanoTransformer. Guided by estimated depth values, our local inpainting module can efficiently fill the gaps between sparse pixels without introducing any artifact. 
We also find that this local inpaining module helps our PanoTransformer better capture global structures in the panorama and make the training stable. Extensive experiments on multiple datasets verify that the proposed \emph{local-to-global panorama inpainting pipeline} significantly improves the quality of recovered indoor lighting distribution at any locale and enables high-fidelity and globally-coherent shading on inserted virtual objects. In particular, our method can reproduce fine texture details that are consistent with the inserting points on specular surfaces (see Fig. \ref{fig:teaser}).

In summary, we make the following contributions:
\begin{itemize}
    \item  A local-to-global panorama inpainting pipeline is proposed to fill missing contents in the warped panorama, enabling reproducing spatially-varying indoor illumination with physically-plausible global structures and fine details at any locale.
    \item A transformer-based network, named PanoTransformer, is designed to capture distortion-free global features in the panorama and restore globally-consistent structures accordingly. 
    \item A new large-scale panorama inpainting dataset is collected with paired masked input and ground truth panoramas.
    %\item We design an end-to-end pipeline to produce spatially-varying illumination for arbitrary selected location in observed image.
\end{itemize}