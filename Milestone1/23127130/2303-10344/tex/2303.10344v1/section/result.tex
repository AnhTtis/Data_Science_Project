\section{Experiments}
In this section, we evaluate the performance of our method in indoor illumination prediction and make comparisons with the state of the art. To further validate our local-to-global inpainting method, we also compare the inpainted results $\mathbf{P}_{G}$ against results from Neural Illumination~\cite{song2019neural}. More results and comparisons can be found in the supplemental materials.

\subsection{Evaluation for illumination estimation}

\begin{figure}[tb]
  \begin{center}
  \renewcommand\tabcolsep{1.0pt}
  \begin{tabular}{ccc}
    \includegraphics[width=0.32\linewidth, clip]{figures/results/inserted/scene17_li.png}   &
    \includegraphics[width=0.32\linewidth, clip]{figures/results/inserted/scene17_neural.png} &
    \includegraphics[width=0.32\linewidth, clip]{figures/results/inserted/scene17_our.png}\\
    \includegraphics[width=0.32\linewidth,   clip]{figures/results/inserted/mirror/11_li.png}   &
    \includegraphics[width=0.32\linewidth,   clip]{figures/results/inserted/mirror/11_neural.png} &
    \includegraphics[width=0.32\linewidth,   clip]{figures/results/inserted/mirror/11_our.png} \\
    \includegraphics[width=0.32\linewidth, clip]{figures/results/inserted/mirror/15_li.png}   &
    \includegraphics[width=0.32\linewidth, clip]{figures/results/inserted/mirror/15_neural.png} &
    \includegraphics[width=0.32\linewidth,   clip]{figures/results/inserted/mirror/15_our.png} \\
    \small{Li \emph{et al.}~\cite{li2020inverse}}&\small{~\cite{song2019neural}}&\small{Ours}
  \end{tabular}
  \end{center}
  
  \caption{Results of inserted objects of different materials with estimated illumination by different methods. }
  \label{fig:inserted}
\end{figure}
\begin{figure}[tb]
  \includegraphics[width=\linewidth]{figures/results/userstudy/user_study.png}
  \caption{Results of user study on 30 scenes. Each scene in the Garon \emph{et al.}'s dataset~\cite{garon2019fast} (Scenes 1-20) is shown as a column, where different colors indicate that users preferred the corresponding method instead of the ground truth. As Scenes 21-30 do not provide the ground truth, we show the ratio of choosing the method that produces the most likable results.}
  \label{fig:userstudy}
\end{figure}

\noindent \textbf{Qualitative comparison: } In Fig.~\ref{fig:ball_comparions}, we visualize illumination maps and the corresponding rendering from several state-of-the-art methods and our pipeline. Gardner \emph{et al.}~\cite{Gardner17} regress a limited field-of-view photo to HDR illumination without strong assumptions on scene geometry, material properties, or lighting. As they predict illumination for the whole scene, this method cannot model spatial variations. 
EMLight~\cite{zhan2020emlight} ignores the complex scene geometry and simplifies the lighting distribution of scenes with a Gaussian map. Thus it can not deal with occlusion. In contrast, we estimate depth to have a better understanding of the scene, which leads to more accurate illumination estimation. Similar to our method, Neural Illumination~\cite{song2019neural} also decomposes the task into sub-tasks. However, it struggles to infer the position of lighting with a limited receptive field, especially when the input is terribly sparse. As Neural Illumination~\cite{song2019neural} has not shared code and model weights, we implemented and trained their network with our datasets. We can see that with a global understanding in local-to-global inpainting module, our method produces environment maps with accurate lighting and perceptually plausible details, ensuring realistic shading. 


We further conduct a user study on 20 scenes of Garon \emph{et al.}~\cite{garon2019fast} and additional 10 scenes from the Internet.  As shown in Fig.~\ref{fig:inserted}, multiple virtual objects are inserted to these scenes.
For scenes of Garon \emph{et al.}~\cite{garon2019fast}, we relight bunny models with diffuse materials using the ground truth light probe and estimated illumination from Neural Illumination~\cite{song2019neural}, Li \emph{et al.}~\cite{li2020inverse} and our method. Li \emph{et al.}~\cite{li2020inverse} achieve the state-of-the-art performance by leveraging a deep inverse rendering framework to obtain a complete scene reconstruction, estimating shape, spatially-varying lighting, and non-Lambertian surface reflectance from a single RGB image. To reflect details of predictions, we render mirror spheres with estimated illumination in the remaining scenes.
The user study is conducted by asking 84 users to choose which rendering is more realistic between rendered image pairs. The results are shown in Fig.~\ref{fig:userstudy}. For scenes with inserted bunny models, Li \emph{et al}.~\cite{li2020inverse} and our method both beat each other in half of the scenes, which suggests that they are comparable in predicting lighting distribution. However, Li \emph{et al.}~\cite{li2020inverse} model the lighting with spherical Gaussian, leading the mirror sphere looks diffuse. For Scenes 21-30, our method outperforms the other methods on the mirror spheres, indicating that our method produces plausible details in coherence with the environment.
 
\begin{table}[tb]
\begin{center}
  \caption{Comparing the quantitative performance of our method to Gardner \emph{et al.}~\cite{Gardner17}, Neural Illumination~\cite{song2019neural} and EMLight~\cite{zhan2020emlight}. D, S, M denote a diffuse, a matte silver and a mirror material of the rendered spheres, respectively. A represents the angular error~\cite{apple-hdr}.}
  \label{tab:light_result}
\begin{tabular}{c|ccccc}
% \toprule
\multicolumn{2}{c}{\textbf{Metrics}}&~\cite{Gardner17}&~\cite{song2019neural}&~\cite{zhan2020emlight}&Ours\\ \hline
\multirow{2}{*}{D} & \textbf{MSE$\downarrow$} &35.77  & 22.67& 12.08 & \textbf{9.18}\\ 
 & \textbf{RMSE$\downarrow$} &53.22  &39.28 & 18.26 & \textbf{14.36}\\ \hline
\multirow{2}{*}{S} & \textbf{MSE$\downarrow$} &33.84  &16.45 & 17.05 &\textbf{9.75} \\ 
 & \textbf{RMSE$\downarrow$} &53.38  &24.09 & 31.79 & \textbf{22.32}\\ \hline
\multirow{2}{*}{M} & \textbf{MSE$\downarrow$} &34.28  &23.69 & 16.71 &\textbf{9.64} \\ 
 & \textbf{RMSE$\downarrow$} & 52.28 &41.44 & 30.86 & \textbf{21.89}\\ \hline
\multirow{2}{*}{A} & \textbf{mean$\downarrow$} & 1.31 &1.92 & 1.21 &\textbf{1.13} \\ 
 & \textbf{std$\downarrow$} & 0.34 &\textbf{0.16} & 0.34 & 0.34\\ 
% \bottomrule
\end{tabular}
\end{center}
\end{table}
\noindent \textbf{Quantitative comparison: }
To evaluate our performance on estimated illumination, we render three spheres with different materials: gray diffuse, matte silver and mirror with predicted illumination maps and ground truth. Then we use some standard metrics including root-mean-square error (RMSE) and the mean absolute error (MAE) to evaluate. To evaluate the accuracy of light sources, we take the mean angular error from both ground truth lights and predicted lights as the final angular error~\cite{apple-hdr} between the two HDR illumination map.
All these metrics have been widely adopted in the evaluation of illumination prediction. For evaluation, we use 2,000 pairs of input LDR images from Laval dataset and the ground truth HDR illumination maps captured at the camera. 
Tab.~\ref{tab:light_result} shows quantitative comparisons of our approach against Gardner \emph{et al.}~\cite{Gardner17}, Neural Illumination~\cite{song2019neural} and EMLight~\cite{zhan2020emlight}. 
As seen, our pipeline outperforms all methods in comparison under different evaluation metrics and materials. 

\begin{figure}[tb]
  \begin{center}
  \renewcommand\tabcolsep{1.0pt}
  \begin{tabular}{cccc}
  \small{Input image} & \small{~\cite{song2019neural}} & \small{Ours} & \small{Ground truth}\\
    \includegraphics[width=0.24\linewidth,   clip]{figures/results/inpaintingresults/0076_input.png}&
    \includegraphics[width=0.24\linewidth,   clip]{figures/results/inpaintingresults/0076_neural.png}  &
    \includegraphics[width=0.24\linewidth,   clip]{figures/results/inpaintingresults/0076_our.png}   &
    \includegraphics[width=0.24\linewidth,   clip]{figures/results/inpaintingresults/0076_gt_pano.png} \\
    
    \includegraphics[width=0.24\linewidth,   clip]{figures/results/inpaintingresults/0612_input.png}&
    \includegraphics[width=0.24\linewidth,   clip]{figures/results/inpaintingresults/0612_neural.png}  &
    \includegraphics[width=0.24\linewidth,   clip]{figures/results/inpaintingresults/0612_our.png}   &
    \includegraphics[width=0.24\linewidth,   clip]{figures/results/inpaintingresults/0612_gt.png}\\
  \end{tabular}
  \end{center}
  \caption{Qualitative comparison with Neural Illumination~\cite{song2019neural} on large-scale panorama inpainting. }
  \label{fig:envi_comparions}
\end{figure}
\subsection{Evaluation for panorama inpainting}

\begin{table}[t]
\begin{center}
  \caption{Quantitative comparisons on the panorama inpainting with Neural Illumination~\cite{song2019neural} and different model variants in the ablation study.}
  \label{tab:inpaint_result}
  \begin{tabular}{cccc}
    % \toprule
    \textbf{Method}&  \textbf{SSIM$\uparrow$} & \textbf{PSNR$\uparrow$} & \textbf{FID$\downarrow$}\\
    \midrule
    Neural Illumination~\cite{song2019neural}  &0.30 &14.85 &255.60\\
    Ours  &\textbf{0.68}  &21.44 &\textbf{39.36}\\
    \midrule
    -Cubemap &0.62  &20.51 &70.09\\
    -Local  &0.60  &19.46 &112.32\\
    -GAN  &\textbf{0.68}  &\textbf{21.50} &76.07\\
%   \bottomrule
\end{tabular}
\end{center}
\end{table}

\noindent \textbf{Qualitative comparison: }
To demonstrate the effectiveness of our transformer-based network for global inpainting, we compare inpainted results from Neural Illumination~\cite{song2019neural} and our method in Fig.~\ref{fig:envi_comparions}. Specifically, Neural Illumination ~\cite{song2019neural} could generally produce rough structures. However, the limited receptive fields of CNNs prohibit the understanding of global structures in the panorama. Besides, Neural Illumination doesn't maintain the visible regions with masks, hence these regions will be changed after prediction. In contrast, our results are more elegant with significantly fewer noticeable inconsistencies and artifacts, outperforming Neural Illumination~\cite{song2019neural} in panorama inpainting. 


\noindent \textbf{Quantitative comparison: }
The inpainting evaluation is conducted on our test set containing 5,000 pairs of masked input and the ground truth. Tab.~\ref{tab:inpaint_result} shows quantitative comparisons of our approach against Neural Illumination~\cite{song2019neural}. Average PSNR, SSIM, FID values are listed for the inpainted LDR panoramas. 
Obviously, our method achieves superior results compared with Neural Illumination in all metrics. This further shows that our method achieves state-of-the-art performance on large-scale panorama inpainting. 


\begin{figure}[t]
  \begin{center}
  \renewcommand\tabcolsep{1.0pt}
  \begin{tabular}{ccccccc}
  \multicolumn{4}{c}{\includegraphics[width=0.52\linewidth,   clip]{figures/results/ablation/no_gan/0114_our_mark.png}}&\includegraphics[width=0.13\linewidth,   clip]{figures/results/ablation/no_gan/0114_no_gan_clipcombine.png}&
  \includegraphics[width=0.13\linewidth,   clip]{figures/results/ablation/no_gan/0114_our_clipcombine.png}&
  \includegraphics[width=0.13\linewidth,   clip]{figures/results/ablation/no_gan/0114_gt_clipcombine.png}\\
  \multicolumn{4}{c}{\small{Our result and the input}}&\small{-GAN}&
  \small{Ours}&
  \small{Reference}\\
     \multicolumn{4}{c}{\includegraphics[width=0.52\linewidth,   clip]{figures/results/ablation/no_cube/4212_our_mark.png}}&\includegraphics[width=0.13\linewidth,   clip]{figures/results/ablation/no_cube/4212_no_cubemap_clipcombine.png}&
  \includegraphics[width=0.13\linewidth,   clip]{figures/results/ablation/no_cube/4212_our_clipcombine.png}&
  \includegraphics[width=0.13\linewidth,   clip]{figures/results/ablation/no_cube/4212_gt_clipcombine.png}\\
  \multicolumn{4}{c}{\small{Our result and the input}}&\small{-Cubemap}&
  \small{Ours}&
  \small{Reference}\\
  \multicolumn{4}{c}{\includegraphics[width=0.52\linewidth,   clip]{figures/results/ablation/no_inpaint/0020_our_mark.png}}&\includegraphics[width=0.13\linewidth,   clip]{figures/results/ablation/no_inpaint/0020_no_inpaint_clipcombine.png}&
  \includegraphics[width=0.13\linewidth,   clip]{figures/results/ablation/no_inpaint/0020_our_clipcombine.png}&
  \includegraphics[width=0.13\linewidth,   clip]{figures/results/ablation/no_inpaint/0020_gt_clipcombine.png}\\
  \multicolumn{4}{c}{\small{Our result and the input}}&\small{-Local}&
  \small{Ours}&
  \small{Reference}\\
  
  \end{tabular}
  \end{center}
  \caption{Validating the contribution of the GAN loss, the cubemap projection and the local inpainting module.}
  \label{fig:abstudy}
\end{figure}

\subsection{Ablation study}
\label{section:ablation}
To evaluate the effectiveness of our design in our method, we develop three model variants, denoted as -GAN model, -Cubemap model and -Local model respectively. The quantitative results are reported in Tab.~\ref{tab:inpaint_result}. We also qualitatively evaluate the performance of the model variants in Fig.~\ref{fig:abstudy}. %We provide more qualitative results in the supplementary materials.

-GAN represents our PanoTransformer trained without the GAN loss. From the top row in the Fig.~\ref{fig:abstudy}, we can see that our model trained without the GAN loss produces over-smooth textures that are close to the mean intensity of the surround regions. With the help of the GAN loss, our complete model is able to produce high-frequency signals and hallucinate realistic details.

Taking cubemap projection as the input aims to remove distortion in the panorama. To show the effectiveness of cubemap projection, we adapt PanoTransformer to -Cubemap model that takes the equirectangular projection as the input and outputs the LDR panoramas directly. As seen in the middle row in Fig.~\ref{fig:abstudy}, the -Cubemap model suffers from distorted structures. Our complete model outperforms the -Cubemap model clearly, demonstrating the superiority of the cubemap projections in handling spherical signals. 

To validate the importance of our local inpainting module, we remove this module and directly train PanoTransformer with sparse panoramas $\mathbf{\hat{P}}$. The bottom row in Fig.~\ref{fig:abstudy} shows that the -Local model introduces artifacts to the prediction, which explains the attention maps of the sparse input in Fig.~\ref{fig:attention}. With the local inpainting module, our pipeline produces more faithful and clearer results, indicating that the local inpainting module promotes the performance of PanoTransformer significantly. 


\subsection{Limitation and discussion}
Since we decompose the illumination estimation into three subtasks, the quality of predictions is affected by the performance of these subtasks. Thanks to the tremendous success of deep learning, depth estimation and HDR panorama reconstruction from one single image have achieved significant progresses. We resort to the off-the-shelf DPT~\cite{dpt2021} for depth estimation and the pretrained network of Santos \emph{et al.}~\cite{LDR2HDR} for HDR panorama reconstruction. More details about the pretrained models can be found in the supplemental materials. Although we do not finetune for our tasks, the predicted depth and HDR map are still acceptable due to the robustness of the state-of-the-art networks. Nonetheless, we still have failure cases when the estimated depth is unreliable. Our performance can be improved with the advance of depth estimation and HDR panorama reconstruction. 
