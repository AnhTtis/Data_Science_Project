\section{Evaluation protocol}
\noindent\textbf{Comparison with unconditional GANs based method.}
We adopt Fr\'echet Inception Distance (FID)\cite{heusel2017gans} and Kernel Inception Distance (KID) \cite{binkowski2018demystifying} to measure the generation quality and diversity of generated images.
FID and KID are computed between 50K generated images and the entire training samples.
We use a modified version of  Perceptual Smoothness (PS) \cite{Liu_2021_CVPR} to measure the smoothness of interpolation between different domain features.
Instead of the style code which is used in the original paper \cite{Liu_2021_CVPR}, we use $\mathbf z \in \mathcal Z$ for target interpolation latent.
Note that this modification is due to the architectural difference.
Same as FID and KID, 50K samples are used to compute PS.
In order to ensure that the implementation difference does not affect performance, we compare all methods above the official Pytorch \cite{paszke2019pytorch} implementation of StyleGAN2-ADA\footnote{\url{https://github.com/NVlabs/stylegan2-ada-pytorch}} \cite{Karras2020ada}.

\noindent\textbf{Comparison on domain translation method.}
We use FID and KID to evaluate generated images.
20K images are randomly sampled from the source domain.
For our approach, we project source domain images to $\mathbf z \in \mathcal Z$ and provide them to the target model.
For the other domain translation methods, source domain images and corresponding randomly sampled style latent codes are used to generate images.
Note that 20K generated images and the entire target domain images are used for evaluation.

\section{Additional results}
\label{sec:results}
\noindent\textbf{Evaluation on anchor point $n_{anch}$.}
We evaluate the proposed method using different anchor points in FFHQ $\rightarrow$ Metfaces setting.
We train our model from scratch 10 times and report Perceptual Smoothness (PS) \cite{Liu_2021_CVPR}, FID \cite{heusel2017gans}, and KID \cite{binkowski2018demystifying} in Table~\ref{table:noise_exp}.
The anchor point $n_{anch}$ is randomly sampled from the Gaussian distribution for each experiment.

\noindent\textbf{Noise interplation.} %\\ \indent
We provide additional noise interpolation results of the proposed method on FFHQ $\rightarrow$ MetFaces (Figure~\ref{fig:ours_metface}), FFHQ $\rightarrow$ AAHQ (Figure~\ref{fig:ours_aahq}) and LSUN Church $\rightarrow$ WikiArt Cityscape (Figure~\ref{fig:ours_wikiart}).

\setlength{\tabcolsep}{8pt}
\begin{table}[ht]
\begin{center}
\small
\begin{tabular}{c|ccc}
\toprule
Setting        & \multicolumn{3}{c}{FFHQ $\rightarrow$ MetFaces} \\ \midrule
$\alpha$  &  PS& FID & \vtop{\hbox{KID} \hbox{\tiny{($\times 10^3)$}}}  \\
\midrule

1     & \multirow{2}{*}{0.884 $\pm$ 0.04} & 38.69 $\pm$ 3.23 & 14.36 $\pm$ 1.93    \\
0     &                                   & 20.08 $\pm$ 0.30 & \,\;3.54 $\pm$ 0.37     \\

\bottomrule
\end{tabular}
\caption{Experiment on different anchor point $n_{anch}$.
We report the mean and standard deviation of metrics over 10 runs.}
\label{table:noise_exp}
\end{center}
% \vspace{-5mm}
\end{table}
\setlength{\tabcolsep}{0.5pt}

\noindent\textbf{Comparison with unconditional GANs based method.}
An additional qualitative comparison of controlling preserved source features is shown in Figure ~\ref{fig:compare_metface},~\ref{fig:compare_aahq}, and ~\ref{fig:compare_wikiart}.
Freeze G \cite{lee2020freezeg} that requires new training for each source degree shows an inconsistent transition of the preserved source features.
Layer-swap \cite{pinkney2020resolution} and UI2I StyleGAN2 \cite{kwong2021unsupervised} that convert weights of a source model also show the inconsistent transition.
Specifically, unnatural color transitions from the source domain are observed in Figure ~\ref{fig:compare_metface}. Additionally, several artifacts and changes in the human identity are observed in Figure ~\ref{fig:compare_aahq}.
We believe that this phenomenon occurs due to the long training time of the target model (\textit{e.g.} FFHQ $\rightarrow$ AAHQ are trained for 12000K images).
The long training time causes more changes in the target model weights, and this may disturb the combined models to generate realistic images.
For example, the identity changes seen in the result of layer swap (Figure~\ref{fig:compare_aahq}) seem to be caused by a large change in the mapping function that transforms $\mathbf z \in \mathcal Z$ to $\mathbf w \in \mathcal W$.
The color transition problems and inconsistent transition are less observable in LSUN Church $\rightarrow$ WikiArt Cityscape, due to the artistic target dataset and the spatial difference between the source and target domain, respectively.
Nevertheless, these methods require models for each degree of preserved source features, while  the proposed method can control in a single model.

% \figprojectfail
\begin{figure}[t!]
\centering
\includegraphics[width=1\linewidth]{figures_supplemental/figure_projectFail_01.pdf}
   \caption{Domain translation results for Church $\rightarrow$ Cityscape. Incorrectly projected images cause our method to generate uncorrelated target images with source.}
\label{fig:project_fail}
% \vspace{-5mm}
\end{figure}

\noindent\textbf{Comparison on domain translation method.}
An additional qualitative comparison of controlling preserved source features are shown in Figure ~\ref{fig:compare_DT_Metfaces},~\ref{fig:compare_DT_AAHQ}.
The latent inversion method is only used for our approach.
Modified version of the inversion method in StyleGAN2 \cite{karras2019style} is used.
We embed real images into the $\mathcal Z$ space of the source model with truncation psi of 0.7 following StyleAlign \cite{wu2022stylealign}.
In the comparison, we use the exact same latent code obtained by the inversion method for the target model.
However, please note that our method can also be multimodal like MUNIT \cite{huang2018multimodal} and StarGAN-v2 \cite{choi2020stargan} by combining early latent code from the projected latent code with the late latent code from the others \cite{wu2022stylealign}.


\noindent\textbf{Latent inversion failure cases.}
Projecting images into the $\mathcal Z$ space of the StyleGAN often fails to accurately reconstruct original images when the dataset becomes larger and more diverse.
The inversion and translated results on Church $\rightarrow$ Cityscape are shown in Figure~\ref{fig:DT_church_fail}.
The result shows a strong correlation between projected and translated images.
However, the incorrectly acquired latent codes lead to uncorrelated target domain images.
It would be interesting to integrate our method well with the inversion method for other spaces (\textit{e.g.} $\mathcal Z+, \mathcal W$, and $\mathcal W+$), or to improve the performance of the inversion method for $\mathcal Z$ space.

\section{Latent modulation}
Recently, several works \cite{harkonen2020ganspace,shen2021closed,wu2021stylespace} observe that StyleGAN can effectively adjust semantic attributes of images by modulating latent codes in interpretable directions.
Additionally, StyleSpace \cite{wu2021stylespace} revealed that the $\mathcal{S}$ space is the most disentangled among the three latent spaces $\mathcal{Z}$, $\mathcal{W}$, and $\mathcal{S}$ of StyleGAN \cite{karras2019style,karras2020analyzing}, and it is possible to change various semantic attributes of generated images just by adjusting a value of the single dimension of $\mathcal S$.
Based on this observation, we examine latent modulation effects on our proposed method. The latent modulation effects on different interpolation weights are shown in Figure~\ref{fig:latent_metface}, \ref{fig:latent_aahq}, and \ref{fig:latent_wikiart}.
The latent modulation effects of the source model are highly aligned in anchored subspace ($\alpha = 1$).
As $\alpha$ decreases, some latent modulation effects remain, while the rest gradually weakens or disappears.
This phenomenon may occur as the preserved source features gradually vanish.

\section{Comparison with SmoothingLatentSpace}
Our approach allows smooth interpolation between the source and target features in the transfer-learned model. 
We additionally compare our approach with SmoothingLatentSpace \cite{Liu_2021_CVPR} which tries to smooth the interpolation between the source and target domain.
For SmoothingLatentSpace, We interpolate latent codes from source images $\mathbf s_{s}$ and randomly sampled noise $\mathbf s_{rand}$, $\alpha \cdot \mathbf s_{s} + (1-\alpha) \cdot \mathbf s_{rand}$,  and generate target images with content from source images and interpolated latent codes.
Figure \ref{fig:compare_DT_smoothing_metfaces} and \ref{fig:compare_DT_smoothing_AAHQ} show interpolation results between the source and target features.
The results show that SmoothingLatentSpace frequently generates severe artifacts during the interpolation between latent codes from source images and randomly sampled noise.
In addition, compared to our method, SmoothingLatentSpace generates less smooth interpolation results.

\section{Limitations}
Despite our method achieved compelling results, it is not without limitations.
% First, our method is not applicable to StyleGAN3 \cite{Karras2021} which removes the noise input to achieve equivariances.
Although our method is easily applicable to StyleGAN 1 \cite{karras2019style} and 2 \cite{karras2020analyzing}, it is hard to directly incorporate our method into architectures that do not contain the noise input such as StyleGAN3 \cite{Karras2021} which removed the noise input to achieve equivariances.
Second, inversion methods for $\mathcal Z$ space cannot accurately reconstruct finer details of real images, which interferes with the consistency between the source and target images in domain translation.
For example, the results in Figure 6 in the main paper
% ~\ref{fig:compare_DT} 
show slight changes in the face identity due to inaccurately obtained latent codes.
Additionally, this phenomenon is exacerbated when the dataset becomes larger and more diverse.
As shown in Figure~\ref{fig:project_fail}, the latent inversion method causes significant changes in overall reconstructed images in LSUN church, which leads our method to generate target images uncorrelated with source images.
In the future, it might be interesting to design inversion methods that overcome the above issues.

\section{Broader impact}
Translating one image to other domains has received tremendous attention from the community and has been used in a variety of applications.
In addition to generating various images from one image (multimodal), it is also very important to determine how much of the source features are preserved.
For example, users may obtain results in which the desired degree of characteristics is preserved in the applications.
As such, we see great potential for our technology to be utilized in various applications.

However, since our method is based on data-driven generative modeling, it faces various ethical issues arising from bias in the training data.
For example, a target model fine-tuned from a source model pre-trained on FFHQ tends to generate more light-skinned images than dark-skinned ones.
In addition, the phenomenon of changing dark-skinned images to light-colored skin was also observed.
At a time when data-driven modeling is getting a lot of attention, the community needs a lot of effort and discussion about data bias.

%%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

% \newpage

\begin{figure*}[t!]
\centering
\includegraphics[width=0.9\linewidth]{figures_supplemental/figure_supple_2.pdf}
   \caption{\textbf{[FFHQ $\rightarrow$ MetFaces]}
   Visualizing the effects of the noise interpolation.
   The interpolation weight $\alpha$ is presented above each column.}
\label{fig:ours_metface}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.9\linewidth]{figures_supplemental/figure_supple_3.pdf}
   \caption{\textbf{[FFHQ $\rightarrow$ AAHQ]}
   Visualizing the effects of the noise interpolation.
   The interpolation weight $\alpha$ is presented above each column.}
\label{fig:ours_aahq}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.9\linewidth]{figures_supplemental/figure_supple_4.pdf}
   \caption{\textbf{[Church $\rightarrow$ Cityscape]}
   Visualizing the effects of the noise interpolation.
   The interpolation weight $\alpha$ is presented above each column.}
\label{fig:ours_wikiart}
\end{figure*}



\begin{figure*}[t!]
\centering
\includegraphics[width=0.9\linewidth]{figures_supplemental/figure_supple_5.pdf}
   \caption{\textbf{[FFHQ $\rightarrow$ MetFaces]} Qualitative comparison on controlling  preserved source features: (a) Layer-swap \cite{pinkney2020resolution}, (b) UI2I StyleGAN2 \cite{kwong2021unsupervised}, (c) Freeze G \cite{lee2020freezeg}, (d) ours.
   The interpolation weight $\alpha$ and swap / freeze layer $i$ are presented above each column.}
\label{fig:compare_metface}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.9\linewidth]{figures_supplemental/figure_supple_6.pdf}
   \caption{\textbf{[FFHQ $\rightarrow$ AAHQ]} Qualitative comparison on controlling  preserved source features: (a) Layer-swap \cite{pinkney2020resolution}, (b) UI2I StyleGAN2 \cite{kwong2021unsupervised}, (c) Freeze G \cite{lee2020freezeg}, (d) ours.
   The interpolation weight $\alpha$ and swap / freeze layer $i$ are presented above each column.}
\label{fig:compare_aahq}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.9\linewidth]{figures_supplemental/figure_supple_7.pdf}
   \caption{\textbf{[Church $\rightarrow$ Cityscape]} Qualitative comparison on controlling  preserved source features: (a) Layer-swap \cite{pinkney2020resolution}, (b) UI2I StyleGAN2 \cite{kwong2021unsupervised}, (c) Freeze G \cite{lee2020freezeg}, (d) ours.
   The interpolation weight $\alpha$ and swap / freeze layer $i$ are presented above each column.}
\label{fig:compare_wikiart}
\end{figure*}


\begin{figure*}[t!]
\centering
\includegraphics[width=0.9\linewidth]{figures_supplemental/figure_supple_8.pdf}
   \caption{\textbf{[FFHQ $\rightarrow$ MetFaces]}
   Visualizing the effects of the latent modulation on different interpolation weight.
   Each of the two adjacent columns is the result of modulating the latent in a different direction ($+/-$).
   The interpolation weight $\alpha$ is presented above each column.}
\label{fig:latent_metface}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.9 \linewidth]{figures_supplemental/figure_supple_9.pdf}
   \caption{\textbf{[FFHQ $\rightarrow$ AAHQ]}
   Visualizing the effects of the latent modulation on different interpolation weight.
   Each of the two adjacent columns is the result of modulating the latent in a different direction ($+/-$).
   The interpolation weight $\alpha$ is presented above each column.}
\label{fig:latent_aahq}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.9 \linewidth]{figures_supplemental/figure_supple_10.pdf}
   \caption{\textbf{[Church $\rightarrow$ Cityscape]}
   Visualizing the effects of the latent modulation on different interpolation weight.
   Each of the two adjacent columns is the result of modulating the latent in a different direction ($+/-$).
   The interpolation weight $\alpha$ is presented above each column.}
\label{fig:latent_wikiart}
\end{figure*}


\begin{figure*}[t!]
\centering
\includegraphics[width=0.82\linewidth]{figures_supplemental/figure_supple_11.pdf}
   \caption{\textbf{[FFHQ $\rightarrow$ MetFaces]}
   Qualitative comparison on domain translation.
   Our method is not only qualitatively best, but also can control source features in a single model.}
\label{fig:compare_DT_Metfaces}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.82\linewidth]{figures_supplemental/figure_supple_12.pdf}
   \caption{\textbf{[FFHQ $\rightarrow$ AAHQ]}
   Qualitative comparison on domain translation.
   Our method is not only qualitatively best, but also can control source features in a single model.}
\label{fig:compare_DT_AAHQ}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.84\linewidth]{figures_supplemental/figure_supple_13.pdf}
   \caption{
   Domain translation results for Church $\rightarrow$ Cityscape. Incorrectly projected images cause our method to generate uncorrelated target images with source.}
\label{fig:DT_church_fail}
\end{figure*}




\begin{figure*}[t!]
\centering
\includegraphics[width=0.8\linewidth]{figures_supplemental/figure_supple_14.pdf}
   \caption{\textbf{[FFHQ $\rightarrow$ MetFaces]}
   Qualitative comparison on interpolation between source and target features: (a) SmoothingLatentSpace \cite{Liu_2021_CVPR}, (b) ours.
   The interpolation weight $\alpha$ is presented above each column.
}
\label{fig:compare_DT_smoothing_metfaces}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.8\linewidth]{figures_supplemental/figure_supple_15.pdf}
   \caption{\textbf{[FFHQ $\rightarrow$ AAHQ]}
   Qualitative comparison on interpolation between source and target features: (a) SmoothingLatentSpace \cite{Liu_2021_CVPR}, (b) ours.
   The interpolation weight $\alpha$ is presented above each column.
}
\label{fig:compare_DT_smoothing_AAHQ}
\end{figure*}