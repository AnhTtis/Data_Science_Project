\section{Related Work}
\textbf{Temporal Action Localization} (TAL). Two-Stage and Single-Stage methods are used in TAL to detect actions in videos. Two-Stage methods first generate possible action proposals and classify them into actions. The proposals are generated through anchor windows \cite{escorcia2016daps, buch2017sst, heilbron2016fast}, detecting action boundaries \cite{lin2018bsn, gong2020scale, zhao2020bottom}, graph representation \cite{bai2020boundary, xu2020g}, or Transformers \cite{wang2021temporal, chang2021augmented, tan2021relaxed}.
Single-stage TAL performs both action proposal generation and classification in a single pass, without using a separate proposal generation step. The pioneering work \cite{qing2021temporal} developed anchor-based single-stage TAL using convolutional networks, inspired by a single-stage object detector \cite{redmon2016you, liu2016ssd}. \ky{Meanwhile,} \cite{lin2021learning} proposed an anchor-free single-stage model with a saliency-based refinement module.
% Other work
% A recent development in the single-stage method involves the use of Transformers \cite{vaswani2017attention} for feature encoding, as demonstrated by \cite{zhang2022actionformer, kang2023action, kang2022htnet, he2022glformer}. 
% Recent research has focused on capturing the relationships between proposals and the temporal context of actions in video sequences. This has been achieved through the use of graph neural networks \cite{zhao2021video, zeng2019graph, xu2020g}, as well as through the application of attention and self-attention mechanisms \cite{sridhar2021class, zhu2021enriching, qing2021temporal}.
% This approach leverages the attention mechanism of Transformers to model the relationships between temporal features and extract high-level representations.

\textbf{Long-term Temporal Context Modeling (TCM)}. Recent studies have emphasized the necessity of long-term TCM to improve model performance. Long-term TCM helps the model capture long-term temporal dependencies between frames, which can be useful in identifying intricate actions that span over extended timeframes or heavily overlapping actions. Prior work has addressed long-term TCM using Relation-aware pyramid Network \cite{gao2020accurate}, Multi-Stage CNN \cite{farha2019ms}, Temporal Context Aggregation Network \cite{qing2021temporal}. Recent works \cite{xu2020g, zhao2021video} employ Graph \cite{kipf2016semi} where each video clip feature represents a node in a graph as long-term TCM. More recently, Transformer \cite{vaswani2017attention} demonstrates an outstanding capacity to capture long-range dependency of the input sequence in the machine translation tasks. Thus, it is a natural fit for Temporal Action Localization (TAL) where each video clip embedding represents a token. Therefore, recent studies \cite{zhang2022actionformer, liu2022end, zhao2021actionness, kang2022htnet} have employed Transformers as a long-term TCM.

%Despite of steady process for long-term TCM, the effectiveness, computational costs, and outstanding potential of extracted features from pretrained 3D CNN of concurrent approaches are not carefully considered. 
Our approach, TemporalMaxer, belongs to the single-stage TAL model that utilizes a state-of-the-art ActionFormer \cite{zhang2022actionformer} as the baseline for comparison. Similar to ActionFormer, TemporalMaxer follows a minimalistic design of sequence labeling where every moment is classified, and their corresponding action boundaries are regressed. 
%The main difference is that we incorporate only a Max Pooling block \cite{boureau2010theoretical} for modeling only short-term context instead of long-term TCM. 
The main difference is that we avoid exhausting attention between clips from long-term timeframes which can unintentionally flatten minute information among the crowd of similar frames, but keep the minute information in short-term manner.
%TemporalMaxer maximizes local temporal contexts of the extracted features from 3D CNN with only Max Pooling. This results in the simplest anchor-free model ever that eliminates all the complex long-term TCM blocks and achieves state-of-the-art performance in TAL while maintaining a minimalist design with significantly fewer parameters and GMACs.
% \textbf{Online video understanding} \\
% Online Action Detection \\
% Online Temporal Action Localization \\

%\textbf{Extreme Minimization Approaches to replace Transformer} Since transformer\cite{} architecture have shown its great ability in NLP\cite{}, it also gained great success in the computer vision\cite{} field. However, due to its expensive attention module, several methods have been proposed to replace the attention module using MLPs\cite{}. \cite{yu2022metaformer} 

