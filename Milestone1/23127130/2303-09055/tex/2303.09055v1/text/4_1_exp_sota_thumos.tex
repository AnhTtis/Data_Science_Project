
\textbf{Dataset}. THUMOS14 dataset \cite{idrees2017thumos} contains 200 validation videos and 213 testing videos with 20 action classes. Following previous work \cite{lin2019bmn, lin2018bsn, xu2020g, zhao2020bottom, zhang2022actionformer}, we trained the model using validation videos and measured the performance on testing videos.

\textbf{Feature Extraction}. Following \cite{zhang2022actionformer, zhao2020bottom}, we extract the features of THUMOS14 dataset using two-stream I3D \cite{carreira2017quo} pre-trained on Kinetics \cite{kay2017kinetics}. 16 consecutive frames are fed into I3D pre-trained network with a sliding window of stride 4. The extracted feature is collected after the last fully connected layer and has 1024-D feature space. After that, the two-stream features are further concatenated (2048-D) and utilized as the input of the model.

\textbf{Results}. We compare the performance
evaluated on the THUMOS14 dataset \cite{idrees2017thumos} with state-of-the-art methods. TemporalMaxer demonstrates remarkable performance, achieving an average mAP of 67.7\% mAP, outperforming all previous approaches, both single-stage, and two-stage methods, by a significant margin, with a 1.1\% increase in mAP at tIoU=0.4.
% , including the most recent ones from \cite{zhu2021enriching, zhao2021video, tan2021relaxed, zhu2022learning, yang2022structured}. 
Especially, TemporalMaxer surpasses all recent methods that utilize long-term TCM blocks including self-attention such as TadTR \cite{liu2022end}, HTNet \cite{kang2022htnet}, TAGS \cite{nag2022proposal}, GLFormer \cite{he2022glformer}, ActionFormer \cite{zhang2022actionformer}, or Graph-based like G-TAD \cite{xu2020g}, VSGN \cite{zhao2021video}, or complex module including Local-Global Temporal Encoder \cite{qing2021temporal}.

Moreover, the comparisons between our method and other approaches show that the proposed method not only exhibits outstanding performance but also is efficient in terms of inference speed. Specifically, our method only takes 50 ms on average to fully process an entire video on THUMOS. It is 1.6x faster than the ActionFormer baseline and 3.9x faster than TadTR. However, in section \ref{section:ablation}, we show that our model only takes 10.4 ms for forward time. It means that the rest 39.6 ms is NMS time, causing the most time-consuming. In the later section \ref{section:ablation}, we show that the forward time and the backbone time of our model are 2.9x and 8.0x faster than ActionFormer, respectively.

\begin{table*}[]
\centering
\resizebox{.9\textwidth}{!}{%
\begin{tabular}{cccccccccc}
\hline
\multirow{2}{*}{Type}         & \multirow{2}{*}{Model}                                                                   & \multirow{2}{*}{Feature}                         & \multicolumn{6}{c}{tIoU$\uparrow$}         & \multirow{2}{*}{time(ms) $\downarrow$} \\ \cline{4-9} 
                              &                                                                                          &                                                  & 0.3           & 0.4           & 0.5           & 0.6           & 0.7           & Avg.                 \\ \hline
\multirow{14}{*}{Two-Stage}   & BMN \cite{lin2019bmn}                                                   & TSN \cite{wang2016temporal}     & 56.0          & 47.4          & 38.8          & 29.7          & 20.5          & 38.5 & 483* \\
                              & DBG \cite{lin2020fast}                                                  & TSN \cite{wang2016temporal}     & 57.8          & 49.4          & 39.8          & 30.2          & 21.7          & 39.8 & --- \\
                              & G-TAD \cite{xu2020g}                                                    & TSN \cite{wang2016temporal}     & 54.5          & 47.6          & 40.3          & 30.8          & 23.4          & 39.3 & 4440* \\
                              & BC-GNN \cite{bai2020boundary}                                           & TSN \cite{wang2016temporal}     & 57.1          & 49.1          & 40.4          & 31.2          & 23.1          & 40.2 & --- \\
                              & TAL-MR \cite{zhao2020bottom}                                            & I3D \cite{carreira2017quo}      & 53.9          & 50.7          & 45.4          & 38.0          & 28.5          & 43.3 & \textgreater644* \\
                              & P-GCN \cite{zeng2019graph}                                              & I3D \cite{carreira2017quo}      & 63.6          & 57.8          & 49.1          & ---           & ---           & ---  & 7298* \\
                              & P-GCN \cite{zeng2019graph} +TSP \cite{alwassel2021tsp} & R(2+1)1 D \cite{tran2018closer}                  & 69.1          & 63.3          & 53.5          & 40.4          & 26.0          & 50.5 & --- \\
                              & TSA-Net \cite{gong2020scale}                                            & P3D \cite{qiu2017learning}      & 61.2          & 55.9          & 46.9          & 36.1          & 25.2          & 45.1 & --- \\
                              & MUSES \cite{liu2021multi}                                               & I3D \cite{carreira2017quo}      & 68.9          & 64.0          & 56.9          & 46.3          & 31.0          & 53.4 & 2101* \\
                              & TCANet \cite{qing2021temporal}                                          & TSN \cite{wang2016temporal}     & 60.6          & 53.2          & 44.6          & 36.8          & 26.7          & 44.3 & --- \\
                              & BMN-CSA \cite{sridhar2021class}                                         & TSN \cite{wang2016temporal}     & 64.4          & 58.0          & 49.2          & 38.2          & 27.8          & 47.7 & --- \\
                              & ContextLoc \cite{zhu2021enriching}                                      & I3D \cite{carreira2017quo}      & 68.3          & 63.8          & 54.3          & 41.8          & 26.2          & 50.9 & --- \\
                              & VSGN \cite{zhao2021video}                                               & TSN \cite{wang2016temporal}     & 66.7          & 60.4          & 52.4          & 41.0          & 30.4          & 50.2 & --- \\
                              & RTD-Net \cite{tan2021relaxed}                                           & I3D \cite{carreira2017quo}      & 68.3          & 62.3          & 51.9          & 38.8          & 23.7          & 49.0 & \textgreater211* \\ 
                              & Disentangle \cite{zhu2022learning}                                           & I3D \cite{carreira2017quo} & 72.1          & 65.9          & 57.0          & 44.2          & 28.5          & 53.5 & --- \\
                              & SAC \cite{yang2022structured}                                           & I3D \cite{carreira2017quo}      & 69.3          & 64.8          & 57.6          & 47.0          & 31.5          & 54.0 & --- \\
                              \cline{2-10}
\multirow{7}{*}{Single-Stage} & AÂ²Net \cite{yang2020revisiting}                                         & I3D \cite{carreira2017quo}      & 58.6          & 54.1          & 45.5          & 32.5          & 17.2          & 41.6 & 1554* \\
                              & GTAN \cite{long2019gaussian}                                            & P3D \cite{qiu2017learning}      & 57.8          & 47.2          & 38.8          & ---           & ---           & ---  & --- \\
                              & PBRNet \cite{liu2020progressive}                                        & I3D \cite{carreira2017quo}      & 58.5          & 54.6          & 51.3          & 41.8          & 29.5          & ---  & --- \\
                              & AFSD \cite{lin2021learning}                                             & I3D \cite{carreira2017quo}      & 67.3          & 62.4          & 55.5          & 43.7          & 31.1          & 52.0 & 3245* \\
                              & TAGS \cite{nag2022proposal}                                                 & I3D \cite{carreira2017quo}  & 68.6          & 63.8          & 57.0          &  46.3         & 31.8          &  52.8 & ---\\
                              & HTNet \cite{kang2022htnet}                                                 & I3D \cite{carreira2017quo}   & 71.2          & 67.2          & 61.5          &  51.0         & 39.3          &  58.0 & ---\\
                              & TadTR \cite{liu2022end}                                                 & I3D \cite{carreira2017quo}      & 74.8          & 69.1          & 60.1          & 46.6          & 32.8          & 56.7  & 195* \\
                              & GLFormer \cite{he2022glformer}                                                 & I3D \cite{carreira2017quo}      & 75.9 & 72.6 & 67.2 & 57.2 & 41.8 & 62.9 & ---\\
                              & AMNet \cite{liu2022end}                                                 & I3D \cite{carreira2017quo}      & 76.7          & 73.1          & 66.8          & 57.2          & 42.7          &  63.3 & ---\\
                              % & NTD \cite{he2022non}                                                 & I3D \cite{carreira2017quo}      & 82.7          &  78.7          & 71.6          & 58.3          & 42.8          &  66.8               \\
                              & ActionFormer \cite{zhang2022actionformer}                               & I3D \cite{carreira2017quo}      & 82.1          & 77.8          & 71.0          & 59.4          & 43.9          & 66.8 & 80 \\
                              & ActionFormer \cite{zhang2022actionformer} + GAP \cite{nag2022post} & I3D \cite{carreira2017quo}      & 82.3          & ---          & 71.4          & ---       &  44.2          & 66.9  & \textgreater 80 \\
                              \cline{2-10}
                              & Our (TemporalMaxer) & I3D \cite{carreira2017quo}      & \textbf{82.8} & \textbf{78.9} & \textbf{71.8} & \textbf{60.5} & \textbf{44.7} & \textbf{67.7} &  \textbf{50} \\ \cline{2-10}   
\end{tabular}
}
\caption{The results obtained on the THUMOS14 dataset \cite{idrees2017thumos} are presented for various tIoU thresholds, with the average mAP calculated in the range [0.3:0.7:0.1]. The top-performing results are highlighted in bold. The time(ms) is the average inference time for one video, without extracting features from 3D CNN and including the post-processing step, such as NMS. We measure the inference time using a single GeForce GTX 1080 Ti GPU. Results indicated with * are taken from \cite{liu2022end} which are measured using Tesla P100 GPU, a much more powerful GPU than the 1080 Ti. In comparison to early works, including both one-stage and two-stage methods, and those utilizing long-term TCM, TemporalMaxer achieves superior performance in both mAP and inference speed.}
\label{table:sota_thumos}
\end{table*}
