\label{section:ablation}
We perform various ablation studies to verify the effectiveness of TemporalMaxer. To better understand what is the effective component for TCM, we gradually replace the  Max Pooling with other blocks such as convolution, subsampling, and Average Pooling. Furthermore, we evaluate numerous kernel sizes of Max Pooling. Note that all experiments in this section are conducted on the train and validation set of the THUMOS14 dataset.
% We report mAP at tIoU=0.5 and 0.7, and the average mAP in [0.3:0.7:0.1].

\begin{table*}[]
\centering
\resizebox{.9\textwidth}{!}{%
\begin{tabular}{cccccccc}
\hline
\multirow{2}{*}{TCM}& \multicolumn{3}{c}{tIoU}  & \multirow{2}{*}{GMACs $\downarrow$} & \multirow{2}{*}{\#params (M) $\downarrow$} & \multirow{2}{*}{time (ms) $\downarrow$} & \multirow{2}{*}{backbone time (ms) $\downarrow$}\\ \cline{2-4}
& 0.5 & 0.7 & Avg.\\ \hline
Conv \cite{lin2021learning} (Our Impl) & 62.8 & 37.1 & 59.4 & 45.6 & 30.5 & 16.3 & 9.0\\
Subsampling  & 64.3 & 37.7 & 61.0 & \textbf{16.2} & 7.1 & 10.4 & 2.5\\
Average Pooling & 66.1 & 39.4 & 63.2 & 16.4 & 7.1 & 10.4 & 2.5\\
Transformer \cite{vaswani2017attention} & 71.0 & 43.9 & 66.8 & 45.3 & 29.3 & 30.5 & 20.1 \\ \hline
TemporalMaxer & \textbf{71.8} & \textbf{44.7} & \textbf{67.7} & 16.4 & \textbf{7.1} & \textbf{10.4} &  \textbf{2.5} \\ \hline
\end{tabular}
}
\caption{Ablation studies about different TCM blocks on THUMOS14. Inference times are measured using an input video with 2304 clip embeddings, a 5 minutes video, on a GeForce GTX 1080 Ti GPU without post-processing (NMS) and pre-extracted features step.}
\label{table:ablation_block}
\end{table*}


\textbf{Effective of TemporalMaxer}. Tab. \ref{table:ablation_block} presents the results of other blocks other than Max Pooling.
% The baseline, ActionFormer \cite{zhang2022actionformer}, gives 66.8\% mAP on average.
Motivated by PoolFormer \cite{yu2022metaformer} that replaces the computationally intensive and highly parameterized attention module with the most basic block in deep learning, the pooling layer. Our studies started by first questioning the most straightforward approach to leverage the potential of the extracted features from 3D-CNN for the TAL task. PoolFormer retains the structure of Transformer such as FFN \cite{rosenblatt1961principles, rumelhart1985learning}, residual connection \cite{he2016deep}, and Layer Normalization \cite{ba2016layer} because the tokens have to be encoded by the Poolformer itself. However, in TAL the features from 3D CNN have already been pre-extracted and contain useful information. Therefore, we posit that there will be a straightforward block that are more efficient than Transformer/PoolFormer, does not require much computational as well as parameters, and be able to effectively exploit the pre-extracted features.

Our ablation study starts to employ a 1-D convolution module as an alternative to Transformer \cite{vaswani2017attention} for TCM block to see how many mAP drop. The result is reported in the first row of Tab. \ref{table:ablation_block}. As expected, the convolution layer decreases by 7.4\% mAP compared with the ActionFormer baseline. This deduction can be explained for two reasons. First, given the video clip features that are informative but highly similar, and the convolution weight is fixed after training, consequently the convolution operation cannot retain the most informative features of local clip embeddings. Secondly, the convolution layer introduces the most parameters which tend to overparameterize the model. We argue that given the informative features extracted from the pretrained 3D CNN, the model should not contain too many parameters, which may lead to overfitting and thus less generalization. To further concrete our thought, we replace the Transformer with a parameter-free operation, subsampling technique, in which features at even indexes are kept and odd indexes are removed to simulate stride 2 of TCM. Surprisingly, this none-parameter operation achieves higher results than the convolution layer, shown in the second row of Tab. \ref{table:ablation_block}. This finding proves that TCM block may not need to contain many parameters, and the features from pretrained 3D CNN are informative and is the potential for TAL.

However, subsampling is prone to losing the most crucial information as only half of the video clip embeddings are kept after a TCM block. Thus, we replace Transformer with Average Pooling with kernel size 3 and stride 2. As expected, Average Pooling improves the result by 2.2 \% mAP on average compared with the subsampling. It is because this operation does not drop any clip embedding which helps to retain the crucial information. However, the Average Pooling averages the nearby features, thus the crucial information of adjacent clips is reduced. That is why Average Pooling decreases by 3.6\% mAP compared with Transformer.

The ablation study with Average Pooling suggests that the most important information of clip embeddings should be maintained. For that reason, we employ Max Pooling as TCM. The result is provided in the last row of Tab. \ref{table:ablation_block}. Max Pooling achieves the highest results at every tIoU threshold and significantly outperforms the strong and robust baseline, ActionFormer. To clarify, TemporalMaxer effectively highlights only critical information from nearby clips and discards the less important information. This result suggests that the feature from pretrained 3D CNN are informative and can be effectively utilized for the TAL model without complex modules like prior works.

Our method, TemporalMaxer, results in the simplest model ever for TAL task that contains minimalist parameters and computational cost for the TAL model. TemporalMaxer is effective at modeling temporal contexts, which outperforms the robust baseline, ActionFormer, with 2.8x fewer GMACs and 3x faster inference speed. Especially, when comparing only the backbone time, our proposed method only takes 2.5 ms which is incredibly 8.0x faster than ActionFormer backbone \cite{zhao2021actionness}, 20.1 ms. 

\textbf{Different Values of kernel size.}
We make an ablation with the kernel size of TemporalMaxer 3, 4, 5, 6 and the average mAPs are 67.7, 67.1, 66.8, 65.7, respectively. Our model achieved the highest performance with a kernel size of 3, while the lowest performance was observed with a kernel size of 6. This decrease in performance can be attributed to the corresponding loss of information during training.
% This decrease in performance can be attributed to the corresponding loss of information, as a kernel size of 6 implies that only 16.7\% of clips are retained during training.
The results obtained for kernel sizes 3, 4, and 5 are not very sensitive to the kernel size. This suggests that these kernel sizes can effectively capture the relevant temporal information for the task at hand.
% \begin{table}[H]
% \centering
% \resizebox{.6\linewidth}{!}{%
% \begin{tabular}{cccc}
% \hline
% \multirow{2}{*}{kernel size} & \multicolumn{3}{c}{tIoU}\\ \cline{2-4} & 0.5 & 0.7 & Avg.\\ \hline
% 3 & \textbf{71.8} & \textbf{44.7} & \textbf{67.7} \\
% 4 & 70.9 & 43.6 & 67.1 \\
% 5 & 69.5 & 42.5 & 66.8 \\
% 6 & 68.4 & 39.8 & 65.7 \\ \hline       
% \end{tabular}
% }
% \caption{Ablation study about different values of kernel size on of TemporalMaxer on THUMOS14.}
% \label{table:ablation_kernelsize}
% \end{table}
% TODO: Explain more using this paper: 
% $https://openaccess.thecvf.com/content/CVPR2022/papers/Cheng_Stochastic_Backpropagation_A_Memory_Efficient_Strategy_for_Training_Video_Models_CVPR_2022_paper.pdf$

% \textbf{Results of other tasks}

% fps: 920,040