\section{Introduction}
%Temporal action localization (TAL), the process of localizing action instances in time and assigning them categorical labels, represents a difficult challenge within the field of video comprehension. 
Temporal action localization (TAL), \ky{which aims to localize} action instances in time and \ky{assign} their categorical labels, {is a challenging but essential task} within the field of video comprehension. 
Various approaches have been proposed to address this task, such as action proposals \cite{lin2019bmn},  anchor windows \cite{long2019gaussian}, or dense prediction \cite{lin2021learning}. A widely accepted notion for improving the performance of TAL models is to integrate a component capable of capturing long-term temporal dependencies within the \ky{extracted video clip features}\cite{qing2021temporal, gao2020accurate, zhang2022actionformer, he2022glformer, liu2022end, nawhal2021activity, tan2021relaxed, wang2021temporal}. 
%Previous studies \cite{xu2020g, zhao2021video} have employed graph neural network to address this problem.
% \ky{To tackle this problem,}
%input video
\tuan{Specifically,} \ky{the TAL model first employs pre-extracted features from a pre-trained 3D-CNN network, such as I3D \cite{carreira2017quo} and TSN \cite{wang2016temporal}, as an input. Then, an encoder, called backbone, encodes features to latent space, and the decoder, called head, predicts action instances as illustrated in Fig. \ref{fig:tcm}. To better capture long-term temporal dependencies, long-term temporal context modeling (TCM) blocks are incorporated into the backbone. Particularly, prior works \cite{xu2020g, zhao2021video} have employed Graph \cite{kipf2016semi}, or more complex modules such as Local-Global Temporal Encoder \cite{qing2021temporal} which uses a channel grouping strategy, or Relation-aware pyramid Network \cite{gao2020accurate} which exploits bi-directional long-range relations.}
% \ky{need another approach here}
%More recently, 
\ky{Recently} there has been notable interest in the application of the self-attention \cite{vaswani2017attention} for long-term TCM \cite{zhang2022actionformer, liu2022end, zhao2021actionness, kang2022htnet}, resulting in surprising performance improvements.

\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/intuition.pdf}
\end{center}
\caption{\textbf{Comparison of effectiveness of Max Pooling over Transformer} \cite{vaswani2017attention}. 
(a) When the input clip embeddings are highly similar, given that the value features $V$ are highly similar and the attention score is distributed equally, the Transformer tends to average out the clip embeddings as done in Average Pooling. 
In contrast, Max Pooling can retain the most crucial information about adjacent clip embeddings and further remove redundancy in the video.
%thus the most important information is significantly reduced. 
(b) Cosine similarity matrix(bottom-left) of value features in self-attention in ActionFormer \cite{zhang2022actionformer} where the \textcolor{darkred}{red boxes} exhibit the action intervals.
(c) Attention score(bottom-right) after training the ActionFormer\cite{zhang2022actionformer}. }
%The results indicate that the value features demonstrate a high degree of similarity and the attention score is distributed equally, with no specific focus on individual clip embeddings. 
\label{fig:similarity}
\end{figure}

%The TAL model employs pre-extracted features from a pre-trained 3D-CNN network, such as I3D \cite{carreira2017quo}, to facilitate model training.
%The TAL model employs pre-extracted features from a pre-trained 3D-CNN network, such as I3D \cite{carreira2017quo} and TSN \cite{wang2016temporal}, \ky{as an input}.
%The features are then fed to a backbone network which could be 1D Convolution, Grapth, or Transformer as depicted in Fig. \ref{fig:tcm}. 
%\ky{Those} features are then fed to a \tuan{backbone} \ky{such as} \ky{Graph Convolutional Network \cite{xu2020g}}, or Transformer\cite{zhang2022actionformer} \tuan{to encode feature}. \tuan{Finally, the encoded features are decoded by the classification and regression head. The processes are depicted in Fig. \ref{fig:tcm}}. 

%Long-term TCM could be beneficial in several ways. 
%First, it can help to capture long-term temporal dependencies between frames, which can be useful in identifying complex actions that may unfold over longer periods of time. 
%Second, long-term TCM can help to address the issue of temporal ambiguity, where actions may be difficult to distinguish from each other if they are heavily overlapped.  

\ky{Long-term TCM} \tuan{in the backbone} \ky{can help the model to capture long-term temporal dependencies between frames, which can be useful in identifying complex actions that may unfold over longer periods of time or heavily overlapping actions.}
Despite their performance improvement on benchmarks, the inference speed and the effectiveness of the existing approaches are rarely considered. While \cite{zhang2022actionformer} introduced and pursued the minimalist design, their backbone is still limited to the transformer architecture requiring \tuan{expensive parameters} and computations.

Recently, \cite{yu2022metaformer} proposed general architecture abstracted from transformer. 
Motivated by the success of recent approaches replacing attention module with MLP-like modules\cite{tolstikhin2021mlp} or Fourier Transform\cite{lee2021fnet}, they deemed the essential of those modules as \textit{token mixer} which aggregates information among tokens. In turn, they came to propose \textit{PoolFormer}, equiped with extremely minimized token mixer which replaces the exhausting attention module with very simple pooling layer. 

In this paper, motivated by their proposal, we focus on extreme minimization of the backbone network by concentrating on and maximizing the information from the extracted video clip features in a short-term perspective rather than employing exhausting long-term encoding process. 

The transformer architecture leading state-of-the-art performance in the machine translation task \cite{vaswani2017attention} and computer vision areas \cite{dosovitskiy2020image} have inspired recent works in TAL \cite{zhang2022actionformer, liu2022end, zhao2021actionness, kang2022htnet}. From the perspective of long-term TCM, the property that calculates attention weights for the \tuan{long} input sequence has led to recent progress in the TAL. However, such long-term consideration comes at a price of high computation costs \tuan{and the effectiveness of those approaches have not yet been carefully analyzed}. 

%As depicted in Fig. \ref{fig:similarity}, such a strategy can flatten video clip features from the feature extractor and is prone to lose temporally local acute changes.

We argue the essential properties of the video clip features that have not been fully exploited to date. 
Firstly, the video clips exhibit a high redundancy which leads to a high similarity of the pre-extracted features as demonstrated in Fig. \ref{fig:similarity}. It raises the question of the effectiveness of employing self-attention or graph methods for long-range TCM. Unlike in other domains such as \tuan{machine translation task} where input tokens exhibit distinctiveness, the input clip embeddings in TAL frequently exhibit a high degree of similarity. Consequently, as depicted in Fig. \ref{fig:similarity}, the self-attention recently employed in TAL tend to average the embeddings of clips within the attention scope, losing temporally local minute changes by redundant similar frames under the long-term temporal context modeling.
%resulting in a reduction of the most significant information within a clip's embedding.
%or graph mechanisms 
%Secondly, they are highly robust for the pre-trained task, and consequently, hold immense potential for TAL. 
% Thus, we posit that the TAL model may not require extensive parameterization. 
We argue that only certain information within clip embeddings is relevant to the action context of TAL, whereas the remainder of the information is similar across adjacent clips. Therefore, an optimal TCM must be capable of preserving the most discriminative features of clip embeddings that carry the essential information.


% These features possess two essential properties that have not been fully exploited to date. 
% Firstly, they are highly robust for the pre-trained task, and consequently, hold immense potential for TAL. 
% Thus, we posit that the TAL model may not require extensive parameterization. 
% Secondly, the video clips exhibit a high redundancy which leads to a high similarity of the pre-extracted features as demonstrated in Fig. \ref{fig:similarity}. It raises the question of the effectiveness of employing self-attention or graph methods for long-range TCM. Unlike in other domains such as Natural Language Processing where input tokens exhibit distinctiveness, the input clip embeddings in TAL frequently exhibit a high degree of similarity. Consequently, the self-attention or graph mechanisms commonly employed in TAL tend to average the embeddings of clips within the attention scope, resulting in a reduction of the most significant information within a clip's embedding. We argue that only certain information within clip embeddings is relevant to the action context of TAL, whereas the remainder of the information is similar across adjacent clips. Therefore, an optimal TCM must be capable of preserving the most discriminative features of clip embeddings that carry the essential information.

%To this end, we aim to employ TCM in a straightforward manner. We argue that the TCM should retain the most distinguishing information from local clip embeddings while avoiding unnecessary complexity due to the robust features extracted from 3D CNN. Max Pooling \cite{boureau2010theoretical} is deemed the most fitting block for this purpose. To test our hypothesis, we utilize ActionFormer \cite{zhang2022actionformer} as a baseline and replace all Transformer blocks in ActionFormer by Max Pooling. The Max Pooling-based model demonstrates outstanding performance, surpassing existing works by a significant margin. 

To this end, we aim to \ky{propose simple yet effective} TCM in a straightforward manner. 
%We argue that the TCM should retain the most distinguishing information from \ky{temporally} local clip embeddings while avoiding unnecessary complexity due to the robust features extracted from 3D CNN.
We argue that the TCM can retain the simplest architecture while maximizing informative features extracted from 3D CNN.
Max Pooling \cite{boureau2010theoretical} is deemed the most fitting block for this purpose. 
%To test our hypothesis, we utilize ActionFormer \cite{zhang2022actionformer} as a baseline and replace all Transformer blocks in ActionFormer by Max Pooling. The Max Pooling-based model demonstrates outstanding performance, surpassing existing works by a significant margin.

%Our proposed method, TemporalMaxer, presents a simple yet effective TAL model that eliminates the need for intricate long-term TCM such as Transformer, Graph, or any complicated modeling. The new model solely comprises 1D Convolutional, Normalization \cite{wu2018group}, ReLU \cite{agarap2018deep}, and Max Pooling operations, which establishes a new state-of-the-art across numerous TAL datasets. Our finding suggests that long-term TCM may not be necessary for improving the performance of TAL models.
Our proposed method, TemporalMaxer, presents the simplest architecture ever for this task gaining much faster inference speed.  
Our finding suggests that the pre-trained feature extractor already possess great potential and with those feature, short-term TCM solely can benefit the performance for this task.

Extensive experiments prove the superiority and effectiveness of the proposed method, showing state-of-the-art performance in terms of both accuracy and speed for TAL on various challenging datasets including THUMOS \cite{idrees2017thumos}, EPIC-Kitchens 100 \cite{damen2020rescaling}, MultiTHUMOS \cite{yeung2018every}, and MUSES \cite{liu2021multi}.

% Unlike the applications of Transformers in the Natural Language Processing, Image Classification, and Object Detection domain where the input tokens are discriminative, the input clips embeddings in TAL usually have a high degree of similarity, as demonstrated in \cite{liu2021multi}. We hypothesize that only some information in the clip embeddings is important and needed in TAL, the others are the same in nearby clips. However, the attention mechanism tends to average the clip's embeddings in the attention scope, thus reducing the most critical information of a clip's embedding. Moreover, given the strong feature from pre-extracted 3D CNN, do we really need a complex model for TCM? A good TCM should be able to maintain the most discriminative feature of clip embeddings. 
