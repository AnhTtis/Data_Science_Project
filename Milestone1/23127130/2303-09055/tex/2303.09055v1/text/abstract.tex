%%%%%%%%% ABSTRACT
\begin{abstract}

% Temporal Action Localization (TAL) is a challenging task in video understanding that aims to identify and localize actions within a video sequence. Research has emphasized the importance of long-term temporal context modeling (TCM) block for improving TAL performance. Specifically, recent works have employed self-attention mechanisms over long input sequences to capture long-term dependencies. However, we question whether Transformers are the optimal choice for this task, given the high similarity of input clips. In this paper, we take a step back and rethink the problem of temporal context modeling in TAL. Rather than proposing a new model, we propose a novel perspective that challenges the conventional wisdom that Transformers are the best choice for TAL. Our approach, TemporalMaxer, replaces the Transformer with a basic, parameter-free Max Pooling block that retains only the most critical information for each clip's embedding. 
% We show that this approach outperforms Transformer-based models on various TAL datasets, achieving competitive results with significantly fewer parameters and computational resources.
% For instance, on the THUMOS14 dataset, TemporalMaxer achieves 78.9\% mAP at tIoU=0.4, surpassing a strong baseline method, ActionFormer, by 1.1\% mAP with 8.3× fewer parameters and 2.8x fewer GMACs. In light of this finding, we encourage future research dedicated to designing a proper temporal context module suitable for TAL. In addition, our TemporalMaxer serves as a baseline for future studies. The code for our approach is publicly available at \url{/link/to/code}.

% Temporal Action Localization (TAL) is a challenging task in video understanding that aims to identify and localize actions within a video sequence. Research has emphasized the importance of long-term temporal context modeling (TCM) block for improving TAL performance. Specifically, recent works have employed self-attention mechanisms over long input sequences to capture long-term dependencies.
% However, we question the necessity of long-term TCM in TAL, given the high similarity of clip features and the large receptive fields of deep networks. In this paper, we propose a novel perspective that challenges conventional wisdom, in which short-term temporal modeling is sufficient for TAL tasks. Rather than proposing a new module, we introduce TemporalMaxer, an approach that replaces long-term temporal context modeling with a basic, parameter-free, and local region operating max-pooling block. This block retains only the most critical information for adjacent and local clip embeddings, resulting in a more efficient TAL model.
% We demonstrate that TemporalMaxer outperforms other models that utilize long-term TCM such as self-attention on various TAL datasets while requiring significantly fewer parameters and computational resources. For example, on the THUMOS14 dataset, TemporalMaxer achieves 78.9\% mAP at tIoU=0.4, surpassing a strong baseline method, ActionFormer, by 1.1\% mAP with 8.3× fewer parameters and 2.8x fewer GMACs. Our finding suggests that long-term TCM block is not necessary for achieving high performance in TAL; instead, the crucial factors are feature pyramid design and how the action is regressed.
% The code for our approach is publicly available at \url{/link/to/code}.

Temporal Action Localization (TAL) is a challenging task in video understanding that aims to identify and localize actions within a video sequence. 
\ky{Recent studies have} emphasized the importance of \ky{applying} long-term temporal context modeling (TCM) blocks \ky{to the extracted video clip features} such as \ky{employing complex self-attention mechanisms}. 
%\ky{Recently proposed methods have} employed complex self-attention mechanisms to address this task. to improve the performance of TAL 
\ky{In this paper, we present the simplest method ever to address this task and argue that the extracted video clip features are already informative to achieve outstanding performance without sophisticated architectures.}
%With our simple yet effective model, we can achieve outstanding performance without sophisticated architectures such as self attention.}
%Nevertheless, as clip features already possess the outstanding potential and deep networks have a large receptive field, this paper exploits their potential \ky{in a very simple yet effective way}. 
%\ky{To this end, we} introduce TemporalMaxer, which replaces long-term temporal context modeling with a basic, parameter-free, and local region operating max-pooling block. 
\ky{To this end, we} introduce TemporalMaxer, which minimizes long-term temporal context modeling while maximizing information from the extracted video clip features with a basic, parameter-free, and local region operating max-pooling block. 
%This block retains only the most critical information for adjacent and local clip embeddings, resulting in a more efficient TAL model.
\ky{Picking out} only the most critical information for adjacent and local clip embeddings, \ky{this block results} in a more efficient TAL model.
We demonstrate that TemporalMaxer outperforms other \ky{state-of-the-art methods} that utilize long-term TCM such as self-attention on various TAL datasets while requiring significantly fewer parameters and computational resources.
% Our finding suggests that long-term TCM block may not be necessary for achieving high performance in TAL; instead, the crucial factors are feature pyramid design and how the action is regressed.
The code for our approach is publicly available at \url{https://github.com/TuanTNG/TemporalMaxer}.

\end{abstract}