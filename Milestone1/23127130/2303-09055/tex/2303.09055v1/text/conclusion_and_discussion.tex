\section{Conclusion}
In this paper, we propose an extremely simplified approach, TemporalMaxer for temporal action localization task. To minimize the structure, we explore the way to simply maximize the underlying information in the video clip features from pre-trained 3D-CNN. To this end, with a basic, non-parametric and temporally local operating max-pooling block which can effectively and efficiently keep the local minute changes among the sequential similar input images.
We achieve competitive performance to other state-of-the-art methods with sophisticated, parametric and long-term temporal context modeling models. 
%We hope our study can inspire  


%In this paper, we propose TemporalMaxer, an effective approach for capturing crucial information from high similarity clip embeddings, and exploit the potential of strong features extracted from pre-trained 3D-CNN in a simple yet effective way. The proposed method solely contains basic blocks such as 1D Convolutional Layer, Normalization, ReLU, and Max Pooling operations. TemporalMaxer, with significantly fewer parameters and GMACs, only operates on local clip embeddings and beats all other works that utilize Transformer or Grapth as long-term temporal context modeling (TCM). Our study suggests that long-term TCM may not be necessary for enhancing the performance of TAL models. 