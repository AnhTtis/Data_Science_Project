\section{Method}
\subsection{Problem Statement}
\begin{figure*}[t]
\centering
\begin{center}
\resizebox{.8\textwidth}{!}{%
\includegraphics[width=1.0\linewidth]{figures/architecture_1.pdf}}
\end{center}
\caption{Overview of TemporalMaxer. The proposed method utilizes Max Pooling as a Temporal Context Modeling block applied between temporal feature pyramid levels to maximize informative features of high similarity clip embedding. Specifically, it first extracts features of every clip using pre-trained 3D CNN. After that, the backbone encodes clip features to form a multi-scale feature pyramid. The backbone consists of 1D convolutional layers and TemporalMaxer layers. Finally, a lightweight classification and regression head decodes the feature pyramid to action candidates for every input moment.}
\label{fig:architecture}
\end{figure*}

\textbf{Temporal Action Localization}. Assume that an untrimmed video $X$ can be represented by a set of feature vectors $X = \lbrace{x_1, x_2, . . . , x_T }\rbrace$, where the number of discrete time steps $t = \lbrace{1, 2, . . . , T}\rbrace$ may vary depending on the length of the video. The feature vector $x_t$ is extracted from a pre-trained 3D convolutional network and represents a video clip at a specific moment $t$. The aim of TAL is to predict a set of action instances $\Psi = \{\psi_1, \psi_2, \ldots, \psi_N \}$ based on the input video sequence $X$, where $N$ is the number of action instances in $X$. Each action instance $\psi_n$ consists of ($s_n$, $e_n$, $a_n$) where $s_n$, $e_n$, and $a_n$ are starting time, ending time, and associated action label $a_n$ respectively, $s_n \in [1, T]$, $e_n \in [1, T]$, $s_n < e_n$, and the action label $a_n$ belongs to the pre-defined set of $C$ categories.

% \textbf{Motivation}. Parameter-free like in other works \cite{} with the difference is:.....

\subsection{TemporalMaxer}
% We adopt current state-of-the-art (SOTA) in Temporal Action Localization (TAL), ActionFormer \cite{zhang2022actionformer}, as a baseline for our study. The key difference here is that we use Max Pooling \cite{boureau2010theoretical} for temporal context modeling instead of Transformer \cite{vaswani2017attention}.

\textbf{Action Representation}. We follow the anchor-free single-stage representation \cite{lin2021learning, zhang2022actionformer} for an action instance. Each moment is classified into the background or one of $C$ categories, and regressed the onset and offset based on the current time step of that moment. Consequently, the prediction in TAL is formulated as a sequence labeling problem.
\begin{equation}
X=\left\{x_1, x_2, \ldots, x_T\right\} \rightarrow \hat{\Psi}=\left\{\hat{\psi}_1, \hat{\psi}_2, \ldots, \hat{\psi}_T\right\}
\end{equation}
At the time step $t$, the output $\hat{\psi}_t = (o_t^s, o_t^e, c_t)$ is defined as following:
\begin{itemize}
\item $o_t^s > 0$ and $o_t^e > 0$ represent the temporal intervals between the current time step $t$ and the onset and offset of a given moment, respectively.
\item Given $C$ action categories, the action probability $c_t$ can be considered as a set of $c_t^i$ which is a probability for action $i^{th}$, where $1\leq i \leq C$.
\end{itemize}
The predicted action instance at time step $t$ can be retrieved from $\hat{\psi}_t = (o_t^s, o_t^e, c_t)$ by:
\begin{equation}
a_t=\arg \max \left(c_t\right), \quad s_t=t-o_t^s, \quad e_t=t+o_t^e
\end{equation}

\textbf{Architecture overview}. The overall architecture is depicted in Fig. \ref{fig:architecture}. Our proposed method aims to learn to label every input moment by $f\left(X\right) \rightarrow \hat{\Psi}$ with $f$ is a deep learning model. $f$ follows an encoder-decoder design and can be decomposed as $e \circ d$. The encoder here is the backbone, and the decoder is the classification and regression head. $e: X \rightarrow Z$ learn to encode the input video feature $X$ into latent vector $Z$, and $d: Z \rightarrow \hat{\Psi}$ learns to predict labels for every input moment. To effectively capture actions transpiring at various temporal scales, we also adopt a multi-scale feature pyramid representation, which is denoted as $Z = \{Z^1, Z^2, . . . , Z^L\}$.

% Our encoder-decoder architecture is similar to that utilized in ActionFormer \cite{zhang2022actionformer}, but with the primary distinction being the use of Max Pooling \cite{boureau2010theoretical} to handle temporal context modeling within the encoder design, as opposed to the Transformer \cite{vaswani2017attention}. To effectively capture actions transpiring at various temporal scales, we also adopt a multi-scale feature pyramid representation, which is denoted as $Z = \{Z^1, Z^2, . . . , Z^L\}$.

\textbf{Encoder design}
The input feature $X$ is first encoded into multi-scale temporal feature pyramid $Z = \{Z^1, Z^2, . . . , Z^L\}$ using encoder $e$. The encoder $e$ simply contains two 1D convolutional neural network layers as feature projection layers, followed by $L-1$ Temporal Context Modeling (TCM) blocks to produce feature pyramid $Z$. Formally, the feature projection layers are described as:
% \begin{equation} \label{eq:concate}
%     X_{c} = \text{Concat}(X),
% \end{equation}
\begin{equation} \label{eq:projection}
    % X_{p1} = \text{ReLU}(\ell_2(\mathcal{F}_1(X_c)))
    X_{p} = E_2(E_1(\text{Concat}(X)))
\end{equation}
% \begin{equation} \label{eq:p2}
%     X_{p2} = \text{ReLU}(\ell_2(\mathcal{F}_2(X_{p1}))),
% \end{equation}

The input video feature sequence $X = \lbrace{x_1, x_2, . . . , x_T }\rbrace$, where $x_i \in \mathbb{R}^{1 \times D_{in}}$,  is first concatenated in the first dimension and then fed into two feature projection modules $E_1$, and $E_2$ in equation \ref{eq:projection}, resulting in the projected feature $X_{p} \in \mathbb{R}^{T \times D}$ with D-dimensional feature space. Each projection module comprises one 1D-convolutional neural network layer, followed by Layer Normalization \cite{ba2016layer}, and ReLU \cite{agarap2018deep}. We simply assign $Z^1 = X_p$ as the first feature in $Z$. Finally, the multi-scale temporal feature pyramid $Z$ is encoded by TemporalMaxer:
\begin{equation} \label{eq:zl}
    Z^l = \text{TemporalMaxer}(Z^{l-1}).
\end{equation}
Here: TemporalMaxer is Max Pooling and employed with stride 2, $Z^l \in \mathbb{R}^{\frac{T}{2^{l-1}} \times D}$, $2 <= l <= L$. It is worth noting that ActionFormer \cite{zhang2022actionformer} employs Transformer \cite{vaswani2017attention} as a TCM block where each clip feature at the moment $t$ represents a token, our proposed method adopts only Max Pooling \cite{boureau2010theoretical} as TCM block.

\textbf{Decoder Design}
The decoder $d$ learns to predict sequence labeling, $\hat{\Psi}=\left\{\hat{\psi}_1, \hat{\psi}_2, \ldots, \hat{\psi}_T\right\}$, for every moment using multi-scale feature pyramid $Z = \{Z^1, Z^2, . . . , Z^L\}$. The decoder adopts a lightweight convolutional neural network and consists of classification and regression heads. Formally, the two heads are defined as:

\begin{equation} \label{eq:cls}
    C_l = \mathcal{F}_c(E_4(E_3(Z^l)))
\end{equation}
\begin{equation} \label{eq:reg}
    O_l = \text{ReLU}(\mathcal{F}_o(E_6(E_5(Z^l))))
\end{equation}
Here, $Z^l \in \mathbb{R}^{\frac{T}{2^{l-1}} \times C}$ is the latent feature of level $l$, $C_l=\{c_0, c_{2^{l-1}},...,c_T\} \in \mathbb{R}^{\frac{T}{2^{l-1}} \times C}$ denotes the classification probability with $c_i \in \mathbb{R}^{C}$, and $O_l = \{(o_0^s, o_0^e), (o_{2^{l-1}}^s, o_{2^{l-1}}^e),...,(o_T^s, o_T^e)\} \in \mathbb{R}^{\frac{T}{2^{l-1}} \times 2}$ is the onset and offset prediction of input moment $\{0, 2^{l-1},..., T\}$. $E$ denotes the 1D convolution followed by Layer Normalization and ReLU activation function. $\mathcal{F}_c$ and $\mathcal{F}_o$ are both 1D convolution. Note that all the weights of the decoder are shared between the different features in the multi-scale feature pyramid $Z$.

\textbf{Learning Objective}. The model predicts $\hat{\psi}_t = (o_t^s, o_t^e, c_t)$ for every moment of the input $X$. Following the baseline \cite{zhang2022actionformer}, the Focal Loss \cite{lin2017focal} and DIoU loss \cite{zheng2020distance} are employed to supervise classification and regression outputs respectively. The overall loss function is defined as:
\begin{equation}
\mathcal{L}_{total}=\sum_t\left(\mathcal{L}_{cls}+\mathbbm{1}_{c_t} \mathcal{L}_{reg}\right) / T_{+}
\end{equation}
where $\mathcal{L}_{reg}$ denotes regression loss and is applied only when the indicator function, $\mathbbm{1}_{c_t}$, indicates that the current time step $t$ is a positive sample. $T_{+}$ is the number of positive samples. $\mathcal{L}_{cls}$ is $C$ way classification loss. The loss function $\mathcal{L}_{total}$ is applied to all levels on the output of multi-scale feature pyramid $Z$ and averaged across all video samples during training.

% The model predicts $\hat{\psi}_t = (o_t^s, o_t^e, c_t)$ for every moment of the input $X$. Given $C$ actions, the action probability $c_t$ can be considered as a set of $c_t^i$ which is a probability for action $i$-th, where $1\leq i \leq C$.
% Focal Loss (FL)~\cite{lin2017focal} is employed to train the classification head by accumulating all binary losses from $C$ labels:
% \begin{equation}
% \begin{aligned}
%     FL(c_t) = &\sum_{i=1}^{C}{-\alpha y_t^i (1-c_t^i)^\gamma \log(c_t^i)}\\
%             &- (1-\alpha)(1-y_t^i)(c_t^i)^\gamma \log(1-c_t^i)
% \end{aligned}
% \end{equation}
% where $y_t^i$ is the groundtruth for class $i$, $\alpha$, and $gamma$ are the balancing parameters.
% \textbf{Inference}

