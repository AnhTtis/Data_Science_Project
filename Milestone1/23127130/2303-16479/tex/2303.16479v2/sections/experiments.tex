\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/comp-chore-phosa-crop.pdf}
    
    \caption{Comparison with PHOSA \cite{zhang2020phosa} and CHORE\cite{xie22chore} on BEHAVE\cite{bhatnagar22behave} (row 1-2) and InterCap~\cite{huang2022intercap} (row 3-4). PHOSA's object pose optimization often gets stuck in local minima due to heavy occlusions. CHORE also fails to predict accurate object pose as it does not fully explore the human, temporal and visibility information while our method can robustly track human and object in these challenging cases.}
    \label{fig:main-result}
\end{figure*}

In this section, we first compare our method against existing approaches on tracking human and object and then evaluate the key components of our methods. Our experiments show that our method clearly outperforms existing joint human object reconstruction method and our novel \emph{\ourSMPL{} conditioned interaction fields} (\sifNet{}) as well as \emph{human and visibility aware} object pose prediction (\hvopNet{}) works better than existing state of the art methods. 

\textbf{Baselines.} \textbf{(1) Joint human and object tracking.} We compare against PHOSA \cite{zhang2020phosa} and CHORE~\cite{xie22chore} in the joint human-object reconstruction task.
\textbf{(2) Object pose prediction.} Our \hvopNet{} leverages nearby (un-occluded) frames to predict the object pose of occluded frames. We compare this with a simple baseline that linearly interpolates the object pose between visible frames to recover occluded poses. We also find similarity between our task and motion smoothing/infilling. Hence we compare our \hvopNet{} with SoTA smoothing ~\cite{zeng2022smoothnet} and infill method~\cite{KIM2022108894CMIB}. 

\textbf{Datasets.} We conduct experiments on the BEHAVE~\cite{bhatnagar22behave} and InterCap~\cite{huang2022intercap} dataset. 
\textbf{(1) BEHAVE} ~\cite{bhatnagar22behave} captures 7 subjects interacting with 20 different objects in natural environments and contains SMPL and object registrations annotated at 1fps.
We use the extended BEHAVE dataset, which registers SMPL and object for BEHAVE sequences at 30 fps. We follow the official split~\cite{xie22chore} with $217$ sequences for training and $82$ for testing.
\textbf{(2) InterCap}~\cite{huang2022intercap} is a similar dataset that captures 10 subjects interacting with 10 different objects. The dataset comes with pseudo ground truth SMPL and object registrations at 30fps. We train our model on sequences from subject 01-08 (173 sequences) and test on sequences from subject 09-10 (38 sequences). 

We compare with CHORE~\cite{xie22chore} on the full test set of both datasets. PHOSA~\cite{zhang2020phosa} optimizes hundreds of random object pose initialization per image hence the inference speed is very slow (2min/image), which makes it infeasible to run on full video sequences. Hence we compare with PHOSA on key frames only, denoted as $\text{BEHAVE}^*$(3.9k images) and $\text{InterCap}^*$(1.1k images) respectively.  
Due to the large number of frames in the full BEHAVE test set (127k frames), we conduct other ablation experiments in a sub test set (42k frames) of BEHAVE. 


\textbf{Evaluation metrics.}
\textbf{(1) Joint human-object tracking.} We evaluate the performance of SMPL and object reconstruction using Chamfer distance between predicted SMPL and object meshes, and the ground truth. CHORE~\cite{xie22chore} uses Procrustes alignment on combined SMPL and object meshes for \emph{each frame} before computing errors. However, this does not reflect the real accuracy in terms of the relative translation between nearby frames in a video. Inspired by the world space errors proposed by SPEC\cite{Kocabas_SPEC_2021} and GLAMR\cite{yuan2022glamr}, we propose to perform joint Procrustes alignment in a sliding window, as also used in SLAM evaluations \cite{RoNIN-inertial-ICRA20, sturm:hal-rgbd-slam}. More specifically, we combine all SMPL and object vertices within a sliding window and compute a single optimal Procrustes alignment to the ground truth vertices. This alignment is then applied to all SMPL and object vertices within this window and Chamfer distance of SMPL and object meshes are computed respectively. We report both the errors using per-frame alignment (w=1) and alignment with a sliding window of 10s (w=10).
\\
(2) \textbf{Object only evaluation.} For experiments evaluating object pose only, we evaluate the rotation accuracy using rotation angle \cite{object_pose_evaluation}. 
The object translation error is computed as the distance between reconstructed and GT translation. We report all errors in centimetre in our experiments. 

\subsection{Evaluation of tracking results}
    

\begin{table}[h]
    \footnotesize
    \centering
    \begin{tabular}{l | l | c c c c c}
    \toprule[1.5pt]
    \multirow{2}{*}{Dateset} & \multirow{2}{*}{Methods} & \multicolumn{2}{c}{Align w=1} & \multicolumn{2}{c}{Align w=10} \\
     &  & { SMPL $\downarrow$} & { Obj. $\downarrow$} & { SMPL $\downarrow$} & { Obj. $\downarrow$} \\
    \midrule
    \multirow{2}{*}{BEHAVE} & CHORE & 5.55 & 10.02 & 18.33 & 20.32 \\
    & { Ours} & {\bf 5.25 } & {\bf 8.04} & {\bf 7.81 } & {\bf 8.49}\\
    \midrule
    \multirow{3}{*}{$\text{BEHAVE}^*$} & PHOSA & 12.86 & 26.90 & 27.01 & 59.08 \\
    & CHORE & 5.54 & 10.12 & 21.28 & 22.39 \\
    & Ours & {\bf 5.24} & {\bf 7.89} & {\bf 8.24 } & {\bf 8.49 }  \\
    \midrule
    \multirow{2}{*}{InterCap} &CHORE & 7.12 & 12.59 &16.11 & 21.05\\
    & Ours & {\bf 6.76} & {\bf 10.32} & {\bf 9.35 } & {\bf 11.38 } \\
    \midrule
    \multirow{3}{*}{$\text{InterCap}^*$} & PHOSA & 11.20 & 20.57 & 24.16 & 43.06\\
    & CHORE & 7.01  & 12.81  & 16.10  & 21.08  \\
    & Ours & {\bf 6.78} & {\bf 10.34} & {\bf 9.35 } & {\bf 11.54 } \\
    \bottomrule[1.5pt]
    \end{tabular}
    \caption{Human and object tracking results on BEHAVE~\cite{bhatnagar22behave} and InterCap~\cite{huang2022intercap} datasets (unit: cm). * denotes key frames only. w is the temporal window size used for Procrustes alignment where w=1 means per-frame Procrustes and w=10 means alignment over a sliding window of 10s. We can see our method clearly outperforms baseline PHOSA\cite{zhang2020phosa} and CHORE\cite{xie22chore} in all metrics.}
    \label{table:main-results}
\end{table}

We compare our human and object tracking results against baseline PHOSA~\cite{zhang2020phosa} and CHORE~\cite{xie22chore} and report the errors in \cref{table:main-results}. Note that the comparison with PHOSA and CHORE is not strictly fair as they do not use temporal information. Nevertheless they are our closest baselines and we show that our method outperforms them using per-frame alignment and is significantly better under a more relevant evaluation metric (align w=10) for video tracking. We also show qualitative comparisons in \cref{fig:main-result}. It can be seen that both PHOSA and CHORE are not able to accurately capture the object under heavy occlusions while our method is more robust under these challenging occlusion cases. 


\subsection{Importance of \ourSMPL{} conditioning}
We propose to condition our interaction field predictions on \ourSMPL{} meshes, we evaluate this for the joint tracking task in \cref{tab:eval-smpl-feat}. Our \ourSMPL{} conditioning allows us to obtain consistent relative translation instead of predicting the human at fixed depth. This significantly reduces the error when evaluating using alignment of temporal sliding window, see \cref{tab:eval-smpl-feat} and qualitative examples in Supp. 

Note that the \sifNet{} prediction relies on the estimated \ourSMPL{} discussed in \cref{sec:TranSMPL}. We also evaluate how the noisy \ourSMPL{} predictions can affect the joint tracking performance. We input GT SMPL to our \sifNet{} and \hvopNet{} to predict the object pose and perform joint optimization. The object errors (Chamfer distance in cm) are: 17.29 (w/o SMPL-T cond.), 8.23 (w/ SMPL-T cond., ours), 6.50 (GT SMPL). Our method is robust and is close to the ideal lower bound with GT SMPL.


\begin{table}[]
         
    \footnotesize
    \centering
    \begin{tabular}{c|c|c|c|c}
        Method & w/o \ourSMPL{}& w/o HVOP-Net &w/o joint opt. & \textbf{Ours}  \\
        \hline
         SMPL $\downarrow$ & 14.40 & 8.05 & 8.20 & \textbf{8.03} \\
         Obj. $\downarrow$ & 17.29 & 9.36 & 16.02 & \textbf{8.23}
    \end{tabular}
    \caption{Ablation studies. We report the joint tracking error (cm) after an alignment window of 10s. It can be seen that our proposed \ourSMPL{} conditioning, \hvopNet{} and joint optimization are important to achieve the best results. }
    
    \label{tab:eval-smpl-feat}
\end{table}



         




\subsection{Importance of \hvopNet{}}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.46\textwidth]{images/rotation_angle_v2-crop.pdf}
    \caption{Object rotation(left) and translation(right) error vs. occlusion (1-fully occluded) for variants of our method. Our full model with \hvopNet{} predicts more robust rotation in occlusions.}
    \label{fig:error-vs-occlusion}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.46\textwidth]{images/hvop-net_example-crop.pdf}
    \caption{Importance of our \hvopNet{} object pose prediction. We can see that our \hvopNet{} corrects erroneous object pose under heavy occlusions. Better viewed after zoom in.}
    \label{fig:eval-pose-pred}
\end{figure}

We propose a \emph{human and visibility aware} object pose prediction network (\hvopNet{}) to reason about the object under heavy occlusions. Without this component, the object error is much higher (\cref{tab:eval-smpl-feat} column 3), which suggests the importance of our \hvopNet{}. 

For the object pose prediction task, there are other similar alternatives to \hvopNet{}:
1). Linear interpolation (SLERP), infill the object pose of invisible frames using spherical linear interpolation between two visible frames.
2). SmoothNet \cite{zeng2022smoothnet}, a fully connected neural network trained to smooth the object motion.  3). CMIB \cite{KIM2022108894CMIB}, a SoTA method for human motion infilling. To evaluate the effectiveness of our \hvopNet{}, we replace \hvopNet{} with these methods and run the full tracking pipeline respectively. The SMPL errors are similar (deviate $<$ 0.1cm) as \hvopNet{} only affects objects. We separate object error into rotation (angle distance) and translation, and further analyse the error under varying occlusion for CHORE, our method (\ourSMPL{} + \hvopNet{}) and its variants (\ourSMPL{} + SmoothNet/SLERP/CMIB) in Fig.~\ref{fig:error-vs-occlusion}. 

All methods except CHORE predict similar translation (due to \ourSMPL{} cond.) but our HVOP-Net obtains clearly more robust rotation under heavy occlusions. SmoothNet smooths the object motion but cannot correct errors from long-term occlusions. SLERP and CMIB \cite{KIM2022108894CMIB} are able to correct some pose errors but do not take the human context into account hence cannot handle heavy occlusions very well. Our method leverages the human motion and object pose from visible frames hence achieves the best result. We show one example where our \hvopNet{} corrects the erroneous raw prediction in \cref{fig:eval-pose-pred}. Please see our supplementary for more examples.  






\subsection{Generalization}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.46\textwidth]{images/ntu-comp-chore-crop_compressed.pdf}
    \caption{Results in NTU-RGBD dataset \cite{Liu_2019_NTURGBD120}. It can be seen that our method generalizes well and wormks better than CHORE\cite{xie22chore}. 
    }
    \label{fig:generalization}
\end{figure}
To verify the generalization ability of our method, we apply our model trained on BEHAVE to the NTU-RGBD~\cite{Liu_2019_NTURGBD120} dataset. We leverage Detectron~\cite{wu2019detectron2}, interactive~\cite{fbrs2020} and video~\cite{cheng2021mivos} segmentation to obtain the input human and object masks. 
Two example comparisons with CHORE~\cite{xie22chore} are shown in \cref{fig:generalization}. We can see our method generalizes well to NTU-RGBD and works better than CHORE. Please see Supp. for evaluation details and more comparisons. 