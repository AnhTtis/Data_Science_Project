\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{images/teaser-crop.pdf}
    \caption{From a monocular RGB video, our method tracks the human, object and contacts between them even under occlusions.}
    \label{fig:teaser}
\end{figure}

Perceiving and understanding human as well as their interaction with the surroundings has lots of applications in robotics, gaming, animation and virtual reality etc.
Accurate interaction capture is however very hard. 
Early works employ high-end systems such as dense camera arrays \cite{cvpr18total-capture, SIGGRAH08garment-capture, TG15free-viewpoint-video} that allow accurate capture but are expensive to deploy. Recent works \cite{bhatnagar22behave, jiang2022neuralfusion, huang2022rich} reduce the requirement to multi-view RGBD cameras but it is still complicated to setup the full capture system hence is not friendly for consumer-level usage. This calls for methods that can capture human-object interaction from a single RGB camera, which is more convenient and user-friendly. 

However, reasoning about the 3D human and object from monocular RGB images is very challenging.
The lack of depth information makes the predictions susceptible to depth-scale ambiguity, leading to temporally incoherent tracking. Furthermore, the object or human can get heavily occluded, making inference very hard. 
Prior work PHOSA~\cite{zhang2020phosa} relies on hand-crafted heuristics to reduce the ambiguity but such heuristic-based method is neither very accurate nor scalable. More recently, CHORE \cite{xie22chore} combines neural field reconstructions with model based fitting obtaining promising results.  
However, CHORE, assumes humans are at a fixed depth from the camera and predicts scale alone, thereby losing the important \emph{relative translation} across frames. 
Another limitation of CHORE is that it is not robust under occlusions as little information is available from single-frame when the object is barely visible. Hence CHORE often fails in these cases, see \cref{fig:main-result}. 

In this work, we propose the first method that can track both human and object accurately from monocular RGB videos. Our approach combines neural field predictions and model fitting, which has been consistently shown to be more effective than directly regressing pose~\cite{bhatnagar2020ipnet,bhatnagar22behave,xie22chore,bhatnagar2020loopreg}.
In contrast to existing neural field based reconstruction methods~\cite{pifuSHNMKL19,xie22chore}, we can do tracking including inference of relative translation.
Instead of assuming a fixed depth, we condition the neural field reconstructions (for object and human) on per frame SMPL estimates (\ourSMPL{}) including translation in camera space obtained by pre-fitting SMPL to the video sequence. This results in coherent translation and improved neural reconstruction. 
In addition, we argue that during human-object interaction, the object motion is highly correlated with the human motion, which provides us valuable information to recover the object pose even when it is occluded (see \cref{fig:teaser} column 3-4). 
To this end, we propose a novel transformer based network that leverages the human motion and object motion from nearby visible frames to predict the object pose under heavy occlusions. 

We evaluate our method on the BEHAVE \cite{bhatnagar22behave} and InterCap dataset~\cite{huang2022intercap}. Experiments show that our method can robustly track human, object and realistic contacts between them even under heavy occlusions and significantly outperforms the currrent state of the art method, CHORE~\cite{xie22chore}. We further ablate the proposed \emph{\ourSMPL{} conditioning} and \emph{human and visibility aware} object pose prediction network and demonstrate that they are key for accurate human-object interaction tracking.

\noindent In summary, our key contributions include:
\begin{itemize}
    \itemsep0em %
    \item We propose the first method that can jointly track full-body human interacting with a movable object from a monocular RGB camera. 
    \item We propose \emph{\ourSMPL{} conditioned interaction fields}, predicted by a neural network that allows consistent 4D tracking of human and object.
    \item We introduce a novel \emph{human and visibility aware} object pose prediction network along with an object visibility prediction network that can recover object poses even under heavy occlusions. 
    \item Our code and pretrained models are publicly available to foster future research in this direction.  
\end{itemize}


\begin{figure*}
    \centering
    \fbox{\includegraphics[width=0.98\textwidth]{images/fig2_methodv3-masks-crop.pdf}}
    \caption{Given an input RGB sequence of a human interacting with an object and their corresponding human-object masks (A), we aim to reconstruct and track the 3D human, object and the contacts between them (E). Our first key idea is a \ourSMPL{} conditioned interaction field network (SIF-Net, details in \cref{sec:smpl-interaction-field}) that predicts neural fields conditioned on estimated SMPL meshes in camera space (col. B, \ourSMPL{}, details in \cref{sec:TranSMPL}). 
    \ourSMPL{} conditioning provides us temporally consistent relative translation, which is important for coherent 4D tracking. Our second key insight is to predict object pose under occlusions (D) leveraging human motion and object visibility information (HVOP-Net, details in \cref{sec:object-motion-infill}). This prediction provides robust object tracking for frames with heavy occlusion. We then jointly optimize human and object (details in ~\cref{sec:joint-optimization}) to satisfy image observations and contact constraints. }
    
    \label{fig:method}
\end{figure*}