
We present a novel method for jointly tracking the human, the object and the contacts between them, in 3D, from a monocular RGB video.
The first main challenge in monocular tracking is the estimation of human and object translations in camera space due to the depth-scale ambiguity problem. 
Existing method, CHORE~\cite{xie22chore}, reconstructs human and object at a fixed depth to the camera, leading to inconsistent 3D translation across frames. Our key idea is to fit a SMPL model with single shape parameters to a video sequence to obtain consistent relative translation across frames. We call the estimated SMPL as \ourSMPL{} and describe it in more details in \cref{sec:TranSMPL}. Based on the estimated \ourSMPL{}, we then jointly model the 3D human, object and the interactions using our proposed \emph{\ourSMPL{} conditioned Interaction Fields} network (\sifNet{}, \cref{sec:smpl-interaction-field})


Object tracking from a single-frame only, is difficult when the object is barely visible. Hence we introduce a \emph{Human and Visibility aware Object Pose Network} (\hvopNet{}) that leverages human and object motion from visible frames to recover the object under occlusion (\cref{sec:object-motion-infill}). We then use the \sifNet{} and \hvopNet{}  outputs to optimize SMPL model and object pose parameters of this sequence to satisfy neural prediction and image observations (\cref{sec:joint-optimization}). An overview of our approach can be found in~\cref{fig:method}.











\subsection{Preliminaries} \label{sec:preliminary}
In this work, we focus on a single human interacting with an object, which is a common setting in other hand-object interaction \cite{yang2021cpf, GRAB:2020, zhou2022toch} and full body-object interaction works \cite{xie22chore, bhatnagar22behave, huang2022intercap}. We represent human using the SMPL \cite{smpl2015loper} body model $\smpl(\pose, \shape)$ that parameterises the 3D human mesh using pose $\pose$ (including global translation) and shape $\shape$ parameters. The object is represented by a known mesh template and we estimate the rotation $\mathbf{R}^o\in SO(3)$ and translation $\vect{t}^o\in \mathbb{R}^3$ parameters. Given a video sequence $\{\mathbf{I}_1, ..., \mathbf{I}_T\}$ where $\mathbf{I}_i \in \mathbb{R}^{H\times W \times 5}$ (RGB, human and object masks), our goal is to estimate the SMPL shape $\shape$, a sequence of SMPL pose $\poseSeq=\{\pose_1, ..., \pose_T\}$, object rotation $\rotSeq=\{\mathbf{R}_1^o, ..., \mathbf{R}_T^o\}$ and translation $\transSeq=\{\vect{t}^o_1, ..., \vect{t}^o_T\}$ parameters that satisfy 2D image observation and realistic interaction constraints between the human and the object. 





\subsection{SMPL-T: Temporally consistent SMPL meshes in camera space} 
\label{sec:TranSMPL}
Our first step is to obtain SMPL meshes in camera space that have consistent translation in a video sequence. We leverage 2D body keypoint predictions from openpose\cite{PAMI19openpose} and natural motion smoothness for this. Specifically, we use FrankMocap~\cite{rong2020frankmocap} to initialise SMPL-T pose $\poseSeq=\{\pose_1,...,\pose_T\}$ and shape $\shapeSeq=\{\shape_1, ..., \shape_T\}$ parameters for a sequence of images. Note here the original SMPL meshes from FrankMocap are centered at origin. We average the SMPL-T shape parameters over the sequence as the shape for the person and optimize the SMPL-T global translation and body poses to minimize 2D reprojection and temporal 
smoothness error: 
\begin{equation}
    E(\poseSeq) = \lambda_\text{J2D}L_\text{J2D} + \lambda_\text{reg}L_\text{reg} + \lambda_a L_{\text{accel}} + \lambda_\text{pi}L_\text{pi}
    \label{eq:objective_smpl_fit}
\end{equation}
where $L_\text{J2D}$ is the sum of body keypoint reprojection losses\cite{bogo2016smplify} over all frames and $L_\text{reg}$ is a regularization on body poses using priors learned from data\cite{smpl2015loper, SMPL-X:2019, tiwari22posendf}. 
$L_{\text{accel}}$ is a temporal smoothness term that penalizes large accelerations over SMPL-T vertices $H_i$: $L_{\text{accel}}=\sum_{i=0}^{T-2}||H_i-2H_{i+1}+H_{i+2}||_2^2$. $L_\text{pi}$, an L2 loss between optimized and initial body pose, prevents the pose from deviating too much from initialization. $\lambda_*$ denotes the loss weights detailed in Supp.

Note that this optimization does not guarantee that we get the absolute translation in the world coordinates, it just ensures that our predictions will be consistent with the SMPL model over a sequence, i.e. we will be off by one rigid transformation for the entire sequence. 

\subsection{\sifNet{}: \ourSMPL{} conditioned interaction field}\label{sec:smpl-interaction-field}
Our \ourSMPL{} provides translation about the human but does not reason about the object and the interaction between them.
Existing method CHORE~\cite{xie22chore} can jointly reason human and object but their humans are predicted at fixed depth. Our key idea is to leverage our \ourSMPL{} meshes to jointly reason human, object and interaction while having consistent relative translation. We model this using a single neural network  which we call \sifNet{}.
The input to \sifNet{} consists of RGB image, human and object masks, and our estimated \ourSMPL{}. With features from \ourSMPL{} and input images, it then predicts interaction field which consists of human and object distance fields, SMPL part correspondence field, object pose and visibility field. 

\textbf{\sifNet{} feature encoding.} 
Existing neural implicit methods \cite{saito2020pifuhd, xie22chore, pifuSHNMKL19} rely mainly on features from input image, a main reason that limits their human prediction at fixed depth. Instead, we extract features from both our estimated \ourSMPL{} meshes and input image, providing more distinct features for the query points along the same ray. 
Inspired by EG3D~\cite{Chan2022EG3D}, we use the triplane representation for \ourSMPL{} feature learning due to its efficiency. 
Specifically, we use orthographic camera $\pi^o(\cdot)$, to render the \ourSMPL{} mesh silhouette from right, back and top-down views and obtain three images $\mathbf{S}^r_i, \mathbf{S}^b_i, \mathbf{S}^t_i$ respectively, where $\mathbf{S}^i\in \mathbb{R}^{H\times W}$, see supplementary for more visualization. Note here the triplane origin is placed at our \ourSMPL{} mesh center (\cref{fig:method} B).
We then train an image encoder $f^\text{tri}(\cdot)$ that extracts a pixel aligned feature grid $\mat{D}^j_i\in \mathbb{R}^{H_c\times W_c \times C}$ from each rendered view $\mathbf{S}^j_i$, where $j\in \{r, b, t\}$ and $H_c, W_c, C$ are the feature grid dimensions.
To extract features for a query point $\vect{p} \in \mathbb{R}^3$, we project $\vect{p}$ into the three planes using the same orthographic projection $\pi_\vect{p}^o=\pi^o(\vect{p})$ and extract local features $ \mat{D}_i^{\vect{p}} = (\mat{D}^r_i(\pi_\vect{p}^o), \mat{D}^b_i(\pi_\vect{p}^o), \mat{D}^t_i(\pi_\vect{p}^o))$ using bilinear interpolation.




In addition to the \ourSMPL{} features, \sifNet{} also extracts information from input images. More specifically, we train an image encoder $f^\text{enc}(\cdot)$ to extract feature grid $\mat{Z}_i \in \mathbb{R}^{H_f\times W_f \times C_f}$ from input image $\mat{I}_i\in \mathbb{R}^{H \times W\times 5}$, here $H_f, W_f, C_f$ and $H, W$ are feature grid and input image dimensions respectively. Given query point $\vect{p}\in \mathbb{R}^3$, we project it to 2D image using full perspective projection $\pi_\vect{p} = \pi(\vect{p})$ and extract pixel-aligned features $\mat{Z}_i^\vect{p} = \mat{Z}_i(\pi_\vect{p})$. The input image feature is concatenated with the \ourSMPL{} feature to form an input and translation aware point feature: $\mat{F}_i^\vect{p}=(\mat{Z}_i^\vect{p}, \mat{D}_i^\vect{p})$. 





\textbf{\sifNet{} predictions.}
From the point feature $\mat{F}^\vect{p}_i$ discussed above, we predict our interaction fields that jointly model human, object and their contacts, similar to CHORE~\cite{xie22chore}. Specifically, we predict the unsigned distances to human and object surfaces using $f^u:\mat{F}_i^\vect{p}\mapsto \mathbb{R}^2_{\geq0}$. This allows fitting the SMPL mesh and object template by minimizing the predicted distances at mesh vertices.
We can also infer contacts, as the points having small distances to both human and object surfaces.
For more robust SMPL fitting~\cite{bhatnagar2020ipnet} and modelling which body part the object point is in contact with, we predict SMPL part correspondence using $f^p: \mat{F}_i^\vect{p}\mapsto \{1,2,...,K\}$ where $K$ is the number of SMPL parts. For more accurate object fitting, we additionally predict object rotation with $f^R:\mat{F}_i^\vect{p}\mapsto \mathbb{R}^{3\times 3}$ and translation with $f^c: \mat{F}_i^\vect{p}\mapsto \mathbb{R}^3$. The predicted $3\times 3$ matrix is projected to SO(3) using symmetric orthogonalization\cite{levinson20nisp_so3_rot}. At test time, we first use $f^u$ to find points on the object surface\cite{chibane2020ndf} and take the average rotation and translation predictions of these points as the object pose. To handle occlusions, we also predict the object visibility field using $f^\text{vis}:\mat{F}_i^\vect{p}\mapsto [0, 1]$. The visibility is useful to recover the object pose under occlusion, see more details in \cref{sec:object-motion-infill}. 

\textbf{Why use triplane to encode \ourSMPL{}?} A direct alternative to triplane based \ourSMPL{} encoding is to find the closest point in \ourSMPL{} mesh and concatenate that coordinate to the point features. But such a method is slow (computing point to surface distance) and does not allow flexible learning of local and global features. Another choice is to voxelize the \ourSMPL{} mesh and extract point local features using IF-Nets~\cite{chibane20ifnet} but such a method is still expensive. Therefore we chose the more efficient triplane representation to encode our estimated \ourSMPL{} meshes.





\textbf{Implementation.} Our \ourSMPL{} feature extractor $f^\text{tri}$ is shared for three views and trained end to end with image encoder $f^\text{enc}$ and other neural field predictors. At training time, we input the renderings from ground truth SMPL meshes and train the network to predict GT labels. At test time, we obtain the \ourSMPL{} meshes using \cref{eq:objective_smpl_fit}. In order to have smoother \ourSMPL{} feature in a sequence, we use SmoothNet \cite{zeng2022smoothnet} to smooth the optimized SMPL parameters. We evaluate this component and provide more implementation details in supplementary. 

\subsection{\hvopNet{}: Human and Visibility aware Object Pose under occlusions}\label{sec:object-motion-infill}
Our \sifNet{} recovers translation and more accurate object pose. However, the object pose prediction under very heavy occlusions remains challenging because no image evidence from single frame is available for accurate prediction, see \cref{fig:eval-pose-pred}. Our key idea is to use the \ourSMPL{} and object pose
from other visible frames to predict the object of occluded frames. 
To this end, we first predict object visibility scores in each image, which are then leveraged together with the human evidence from neighbouring frames to predict the object poses of the occluded frames.


\textbf{Object visibility score.} Our visibility score denotes how much the object is visible in the input image. We train a visibility decoder $f^\text{vis}(\cdot)$, a prediction head of \sifNet{}, that takes a point feature $\mat{F}_i^\vect{p}$ as input and predicts visibility score $v_i\in[0, 1]$ for frame $i$. At test time, we first use the neural object distance predictor $f^u$ to find object surface points \cite{chibane2020ndf} and then take the average visibility predictions of these points as the object visibility score for this image. 

\textbf{Object pose prediction under heavy occlusion.}
Our goal now is to predict accurate object pose for heavily occluded frames. We consider frames whose visibility score $v_i$ is smaller than $\delta=0.5$ as the occluded frames.
Inspired by works from motion infill \cite{KIM2022108894CMIB, Duan_unified-motion-aaai22} and synthesis \cite{yi2022generating, zhang2022couch}, we design our \hvopNet{} that leverages transformer ~\cite{NIPS2017_attention} and explicitly takes the human motion and object visibility into account to recover object pose under heavy occlusions. 

More specifically, we first use a transformer $f^s(\cdot)$ to aggregate temporal information of the \ourSMPL{} poses: $f^s:\mathbb{R}^{T\times |\theta_i|}\mapsto \mathbb{R}^{T\times D_{hs}}$, where $|\theta_i|$ is the SMPL pose dimension and $D_{hs}$ is the hidden feature dimension. Similarly, we use a transformer $f^o(\cdot)$ to aggregate temporal information of the object poses: $f^o:\mathbb{R}^{T\times D_o}\mapsto \mathbb{R}^{T\times D_{ho}}$. Note here the \ourSMPL{} transformer $f^s$ attends to all frames while the object transformer $f^o$ only attends to frames where object is visible ($v_i\geq\delta$). We then concatenate the \ourSMPL{} and object features and use another transformer $f^\text{comb}$ to aggregate both human and object information and predict the object poses: $f^\text{comb}: \mathbb{R}^{T\times (D_{hs} + D_{ho})}\mapsto \mathbb{R}^{T\times D_{o}}$. 
The joint transformer $f^\text{comb}$ attends to all frames. Our experiments show that our \hvopNet{} is important to accurately predict object pose under heavy occlusions, see \cref{fig:eval-pose-pred}.

\textbf{Implementation.} The visibility decoder $f^\text{vis}$ is trained end to end with other \sifNet{} components using L2 loss. The GT visibility score is computed as the number of visible object pixels (from object mask) divided by total number of object pixels (from GT object rendering). To train our \hvopNet{}, we randomly zero out the object pose for a small clip of the input sequence and provide ground truth SMPL and object poses for other frames as input. We train our network to accept input sequence of a fixed length but at test time, the sequence can have various length and object occlusion can last for a longer time. To this end, we use an auto-regressive algorithm to recover the object pose of a full video, similar to \cite{yuan2022glamr}. At test time, we use \sifNet{} object pose predictions and zero out highly occluded frames based on the predicted visibility scores. We empirically find that having smooth object pose as input is helpful for more accurate prediction. Hence we use SmoothNet~\cite{zeng2022smoothnet} to smooth \sifNet{} object pose predictions before inputing them to \hvopNet{}. The evaluation of this component and more training details are described in our supplementary. 


\subsection{Visibility aware joint optimization}\label{sec:joint-optimization}
To obtain SMPL and object meshes that align with input images and satisfy contact constraints, we leverage our network predictions from \cref{sec:smpl-interaction-field} and \cref{sec:object-motion-infill} to formulate a robust joint optimization objective.
Our goal is to obtain an optimal set of parameters $\paramAll=\{\poseSeq, \shape, \rotSeq, \transSeq\}$ for SMPL pose, shape, object rotation and translation respectively. We initialize the SMPL parameters from our estimated \ourSMPL{} (\cref{eq:objective_smpl_fit}) and object parameters from our \hvopNet{} predictions (\cref{sec:object-motion-infill}). Inspired by CHORE\cite{xie22chore}, our energy function consists of human data $E_{data}^h$, object data $E_{data}^o$, contact data $E_{data}^c$ and SMPL pose prior term $E_{reg}$: 
\begin{equation}
    E_(\paramAll) = E_{data}^h + E_{data}^o + E_{data}^c + E_{reg}.
    \label{eq:objective_joint}
\end{equation}
here $E_{reg}$ is a body pose and shape prior loss\cite{smpl2015loper}. We explain other loss data terms next. 
\\
\noindent
\textbf{Human data term}. $E_{data}^h$ minimizes the discrepancy between \sifNet{} prediction and SMPL meshes as well as a temporal smoothness error: $E_{data}^h(\poseSeq, \shape)=\sum_{i=1}^T L_{\text{neural}}^h(\pose_i, \shape) + \lambda_{\text{ah}} L_{\text{accel}}(\poseSeq)$, where $L_{\text{accel}}$ is the same used in \cref{eq:objective_smpl_fit}. $L_{\text{neural}}^h$ pushes the SMPL vertices to the zero-level set of the human distance field represented by neural predictor $f^{u,h}_i$ and forces correct SMPL part locations predicted by $f^p_i$: 
\begin{multline}
    E^h_\text{neural}(\poseSeq, \shape) = \sum_{i=1}^T(\sum_{\vect{p}\in \smpl(\pose_i, \shape)}(\lambda_{h}\min(f^{u,h}_i(\mat{F}^\vect{p}_i), \delta_h) +\\ \lambda_{p} L_p(l_\vect{p}, f^p_i(\mat{F}^\vect{p}_i))))
\end{multline}
here $l_\vect{p}$ is the predefined SMPL part label \cite{bhatnagar2020ipnet} of SMPL vertex $\vect{p}$ and $L_p$ is the categorical cross entropy loss function. $\delta_h$ is a small clamping value. 
\\
\textbf{Object data term.} We transform the object template vertices $\mat{O}\in \mathbb{R}^{3\times N}$ using object pose parameters of frame $i$ by: $\mat{O}^\prime_i=\mat{R}^o_i\mat{O} + \vect{t}^o_i$. Intuitively, the object vertices should lie on the zero-level set of the object distance field represented by $f^{u,o}_i$ and the rendered silhouette should match the 2D object mask $\mat{M}^o_i$. Hence we formulate the loss as: 
\begin{multline}
E^o_{data}(\rotSeq, \transSeq) = \sum_{i=1}^{T}v_i(\sum_{\vect{p}\in \mat{O}^\prime_i}\lambda_o\min(f^{u,o}_i(\mat{F}^\vect{p}_i), \delta_o)\\ 
    + \lambda_{\text{occ}}L_{\text{occ-sil}}(\mat{O}_i^\prime, \mat{M}^o_i)) + \lambda_{\text{ao}}L_{\text{ao}}
    \label{eq:objective_object_fit}
\end{multline}
where $v_i$ is the predicted object visibility score described in \cref{sec:object-motion-infill}. This down-weights the loss values of network predictions for frames where object is occluded and allows more temporal regularization. $L_\text{occ-sil}$ is an occlusion-aware silhouette loss\cite{zhang2020phosa} and $L_{\text{ao}}$ is a temporal smoothness loss applied to object vertices $\mat{O}^\prime_i$, similar to $L_{\text{accel}}$ in \cref{eq:objective_smpl_fit}. 

\noindent
\textbf{Contact data term.}
The contact data term \cite{xie22chore} minimizes the distance between human and object points that are predicted to be in contact: 
\begin{equation}
    E_\text{data}^c(\rotSeq, \transSeq) = \lambda_c \sum_{i=1}^T(\sum_{j=1}^{K} d(\smpl_j^c(\pose_i, \shape), \mat{O}_{ij}^c))
\end{equation}
here $d(\cdot, \cdot)$ is chamfer distance. We consider human points on the $j^\text{th}$ body part of SMPL mesh $H_i$ of frame $i$ (denoted as $H_{ij}$) are in contact when their predicted distance to the object is smaller than a threshold: $H^c_j(\pose_i, \shape)=\{\vect{p} | \vect{p} \in H_{ij}\, \text{and}\, f^{u, o}_i(\mat{F}^\vect{p}_i)\leq \epsilon\}$. Similarly, we find contact points on object meshes with $\mat{O}_{ij}^c=\{\vect{p} | \vect{p}\in\mat{O}^\prime_i\, \text{and}\, f^{u, h}_i(\mat{F}^\vect{p}_i)\leq\epsilon\, \text{and}\, f^p_i(\mat{F}^\vect{p}_i)=j\}$.  

Please see Supp. for more details about loss weights $\lambda_*$. 


