In this supplementary, we first list all the implementation details of our method and then show more ablation study results as well as comparison with CHORE\cite{xie22chore} on NTU-RGBD\cite{Liu_2019_NTURGBD120} dataset. We end with discussions of failure cases and future works. 

\section{Implementation details}
\subsection{Obtaining \ourSMPL{} meshes}
To obtain the image-aligned SMPL meshes that have consistent translation (\ourSMPL{}) we keep the SMPL shape parameters and optimize the body pose and global translation values. The loss weights for this optimization are: $\lambda_\text{J2D}=0.09, \lambda_\text{reg}=1.0\times10^{-5}, \lambda_a=25, \lambda_\text{pi}=900$.  We optimize the parameters until convergence with a maximum iteration of 1000. 

\subsection{\sifNet{}: \ourSMPL{} conditioned interaction field}
A visualization of our \ourSMPL{} triplane rendering and query point projection can be found in \cref{fig:triplane-vis}. We discuss our network architecture and training details next. 

\paragraph{Network architecture.} We use the stacked hourglass network \cite{stacked-hourglass} for both RGB image encoder $f^\text{enc}$ and SMPL rendering encoder $f^\text{tri}$. We use 3 stacks for $f^\text{tri}$ and the output feature dimension is $d^\text{tri}_\text{o}=64$. Hence $f^\text{tri}:\mathbb{R}^{H\times W}\mapsto \mathbb{R}^{H/4\times W/4 \times 64}$ where $H=W=512$. We also use 3 stacks for $f^\text{enc}$ but the feature dimension is $d^\text{enc}_\text{o}=256$. Hence $f^\text{enc}:\mathbb{R}^{H\times W\times 5}\mapsto \mathbb{R}^{H/4\times W/4 \times 256}$. We also concatenate the image features extracted from the first convolution layer and query point coordinate to the features. Thus the total feature dimension to our decoders is: $d=(d^\text{tri}_1 + d^\text{tri}_\text{o})\times 3 + d^\text{enc}_1 + d^\text{enc}_\text{o} + 3=611$, here $d^\text{tri}_1=32, d^\text{enc}_1=64$. 
All decoders consist of three FC layers with ReLU activation and one output FC layer with hidden dimension of 128 for the intermediate features. The visibility decoder $f^v$ additionally has a sigmoid output activation layer. The output shape is $2, 14, 9, 3, 1$ for $f^u, f^p, f^R, f^c, f^v$ respectively. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.46\textwidth]{images/triplane_vis-cropped.pdf}
    \caption{Visualization of our \ourSMPL{} triplane feature extraction and rendering. The triplane origin is placed at the \ourSMPL{} body center and we render the mesh from three views using orthographic projection: righ-left (A), back-front (B) and top-down (C). The query point $p$ is projected into the three planes using same projection for rendering and we extract pixel aligned features $\mat{F}^A, \mat{F}^B, \mat{F}^C$ from the feature planes respectively. Note that we render the \ourSMPL{} with color here for visualization, the actual input to our network are silhouette images only.}
    \label{fig:triplane-vis}
\end{figure}
\paragraph{Training.} All feature encoders and decoders are trained end to end with the loss: $L=\lambda_u(L_{u_h}+L_{u_o}) + \lambda_p L_p +\lambda_R L_R + \lambda_c L_c + \lambda_v L_v$. Here $L_{u_i}$ is the $L_1$ distance between ground truth and predicted unsigned distance to human or object surface \cite{xie22chore}. $L_p$ is a standard categorical cross entropy loss for SMPL part correspondence prediction. $L_R, L_c, L_v$ are mean square losses between ground truth and predicted values for rotation matrix, translation vector and visibility score respectively. The loss weights are: $\lambda_u=1.0, \lambda_R=0.006, \lambda_c=500, \lambda_c=\lambda_v=1000$. The model is trained for 18 epochs and it takes 25h to converge on a machine with 4 RTX8000 GPUs each with 48GB memory. The training batch size is 8. 

\subsection{\hvopNet{}: object pose under occlusion}
We use three transformers $f^s, f^o, f^\text{comb}$ to aggregate features from \ourSMPL{}, object pose and joint human object information respectively. We use the 6D vector \cite{Zhou_2019_CVPR_rot6d} to represent the ration matrix of \ourSMPL{} and object pose parameters. Hence the \ourSMPL{} pose dimension is $24\times6+3=147$, where 3 denotes the global translation. We predict the object rotation only thus the object data dimension is 6. The \ourSMPL{} transformer $f^s$ consists of an MLP: $\mathbb{R}^{T\times 147}\mapsto \mathbb{R}^{T\times 128}$ and two layers of multi-head self-attention (MHSA) module \cite{NIPS2017_attention} with 4 heads. Similarly, the object transformer $f^o$ consists of an MLP: $\mathbb{R}^{T\times 6}\mapsto \mathbb{R}^{T\times 32}$ and two layers of MHSA module with 2 heads. The joint transformer $f^\text{comb}$ consists of 4 layers of MHSA module with 1 head only. GeLU activation is used in all MHSA modules. We finally predict the object pose using two MLP layers with an intermediate feature dimension of 32 and LeakyReLU activation. 

The model is trained to minimize the $L_1$ losses of pose value and accelerations: $L=\lambda_\text{pose}L_\text{pose} + \lambda_\text{accel}L_\text{accel}$, where $\lambda_\text{pose}=1.0, \lambda_\text{accel}=0.1$. It is trained on a server with 2 RTX8000 GPUs, each GPU has 48GB memory capacity. It takes around 7h to converge (64 epochs). 


\subsection{SmoothNet for \ourSMPL{} and object}
We use SmoothNet\cite{zeng2022smoothnet} to smooth our \ourSMPL{} and \sifNet{} object pose predictions. We use exactly the same model and training strategy proposed by the original paper. The input to the \ourSMPL{} SmoothNet is our estimated \ourSMPL{} pose and translation (relative to the first frame). The input to the object SmoothNet is the object rotation (6D vector). Following the standard practice of SmoothNet\cite{zeng2022smoothnet}, we train both models on the predictions from the BEHAVE \cite{bhatnagar22behave} training set. Note that we do not fine-tune them on InterCap\cite{huang2022intercap} dataset. We evaluate this component in \cref{sec:eval-smoothnet}. 

\subsection{Visibility aware joint optimization}
The objective function defined in Eq. 2 is highly non-convex thus we solve this optimization problem in two stages. We first optimize the SMPL pose and shape parameters using human data term only. We then optimize the object parameters using the object and contact data terms. The loss weights are set to: $\lambda_\text{reg}=2.5\times 10^{-4}, \lambda_\text{ah}=10^4, \lambda_h=10^4, \lambda_p=t\times 10^{-4}, \lambda_o=900, \lambda_\text{occ}=9\times 10^{-4}, \lambda_\text{ao}=225, \lambda_c=900$, where $\lambda_c$ is the loss weight for the contact data term defined in Eq. 5. 


\section{Additional ablation results}
\subsection{Further evaluation of \ourSMPL{} conditioning} \label{supp-sec:eval-smplt}
We show some example images from one sequence in \cref{fig:eval-smpl-feature} to evaluate the importance of our \ourSMPL{} conditioning. It can be seen that without this conditioning, the human is reconstructed at fixed depth, leading to inconsistent relative translation across time. Our method predicts more coherent relative human translation and more accurate object pose. 

To further evaluate \ourSMPL{} conditioning, we compute the object pose error from the raw network predictions and compare it with the object pose of CHORE which is also the raw prediction from the network. The pose error is computed as Chamfer distance (CD) and vertex to vertex (v2v) error after centring the prediction and GT mesh at origin. We also report the translation error (transl.) as the distance between predicted and GT translation. The results are shown in \cref{tab:eval-smplt-feat}. We can clearly see that our SMPL feature improves both the raw object pose prediction and distance fields (results after optimization are also improved). 

\begin{table}[h]
    \small %
    \centering
    \begin{tabular}{ l| c c c c c}
    \hline
         \multirow{2}{*}{Method} & \multicolumn{3}{c}{Raw prediction} & \multicolumn{2}{c}{After opt. w=10}\\
         
          & CD$\downarrow$ & v2v$\downarrow$ & transl.$\downarrow$ & SMPL$\downarrow$ &obj.$\downarrow$\\
          \hline
         w/o \ourSMPL{} & 5.56 & 16.10 & 14.28 & 14.40 & 17.29 \\
         Ours & {\bf 3.98} & {\bf 12.34} & {\bf 9.53} & {\bf 8.03} & {\bf 8.23}\\
         \hline
    \end{tabular}
    \caption{Importance of \ourSMPL{} conditioning (errors in cm). We can see that our \ourSMPL{} feature improves both the raw object pose prediction and distance fields (after opt.). Without our \ourSMPL{} conditioning, the reconstructed translation is not consistent across frames, leading to large errors after alignment of temporal window of 10s (w=10).}
    \label{tab:eval-smplt-feat}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.46\textwidth]{images/smplt-topdown-crop.pdf}
    \caption{Evaluating \ourSMPL{} conditioning for neural field prediction. We can see that without conditioning on \ourSMPL{} meshes, the object pose prediction is worse and human is reconstructed at fixed depth, leading to inconsistent relative location across frames. Our method recovers the relative translation more faithfully and obtain better object pose predictions. 
    }
    \label{fig:eval-smpl-feature}
\end{figure}


\subsection{Comparing different pose prediction methods}
We show some example comparisons of different object pose prediction methods under heavy occlusions in \cref{fig:comp-pose-preds}. We compare our method against: 1). Raw prediction from our \sifNet{}. 2). Linearly interpolate the occluded poses from visible frames (SLERP). 3). CMIB \cite{KIM2022108894CMIB}, a transformer based model trained to infill the object motion using visible frames. Note here the evaluation is based on the final tracking results and we report the object errors only as the difference of SMPL error is very small. Similar to \cref{supp-sec:eval-smplt}, the object errors are computed as Chamfer distance, v2v error and translation error. 

It can be seen that the raw pose prediction is noisy due to occlusion. SLERP and CMIB corrects some pose errors but is not robust as they do not leverage the human information. Our method is more accurate as it takes the human context and object pose into account. 


\subsection{Evaluating SmoothNet}\label{sec:eval-smoothnet}
\begin{table}
    \small  
    \centering
    \begin{tabular}{l|c c c}
    \hline
        Method & Chamfer & v2v & Acceleration \\
        \hline
        w/o SmoothNet & 8.71 & 9.84 & 1.38\\
        w/ SmoothNet & {\bf 8.01} & {\bf 9.12} & {\bf 1.18}\\
        \hline
    \end{tabular}
    \caption{Ablate SMPL SmoothNet (errors in cm). We can see that SmoothNet ~\cite{zeng2022smoothnet} improves the overall smoothness and slightly reduces the pose errors.}
    \label{tab:eval-smoothnet-smpl}
\end{table}

\begin{table}
    \small  
    \centering
    \begin{tabular}{l|c c c}
    \hline
        Method & Chamfer & v2v & Translation \\
        \hline
        a. Raw prediction & 5.03 & 10.39 & 10.01 \\
        b. Raw + SmoothNet & 4.22 & 8.60 & 10.16 \\
        c. Raw + our pose pred. & 4.09 & 8.02 & 10.20\\
        d. Our full model & {\bf 3.62 } & {\bf 7.20} & {\bf 9.96} \\
        \hline
    \end{tabular}
    \caption{Ablate SmoothNet for object pose prediction (errors in cm). We can see our pose prediction (c) is better than SmoothNet~\cite{zeng2022smoothnet} (b). Combing both we obtain the best result (d).}
    \label{tab:eval-smoothnet-object}
\end{table}
SmoothNet \cite{zeng2022smoothnet} is used to smooth the \ourSMPL{} parameters after 2D keypoint based optimization. We evaluate this step by computing the SMPL errors, shown in \cref{tab:eval-smoothnet-smpl}. We can see that SmoothNet reduces the SMPL error slightly. 

We also use SmoothNet to smooth the object pose before sending it to our human and visibility aware object pose prediction network. SmoothNet cannot correct errors under long-term occlusions. However, it provides smoother object motion for visible frames which can benefit our pose prediction network. We evaluate this using object pose errors and report the results in \cref{tab:eval-smoothnet-object}. It can be seen that our method (\cref{tab:eval-smoothnet-object}c) works better than SmoothNet (\cref{tab:eval-smoothnet-object}b) on raw predictions. Nevertheless, with smoothed pose after SmoothNet, our method achieves the best result (\cref{tab:eval-smoothnet-object} d).


\subsection{Runtime cost}
\ourSMPL{} pre-fitting and joint optimization can be run in batches hence the average runtime per frame is not long: \ourSMPL{} pre-fitting: 6.38s, SIF-Net object pose prediction: 0.89s, HVOP-Net: 1.3ms, joint optimization: 9.26s, total: 16.53s. Compared to CHORE ($\sim$12s/frame)~\cite{xie22chore}, the additional cost is mainly from the \ourSMPL{} pre-fitting. Yet, \ourSMPL{} conditioning allows faster convergence of joint optimization and much better reconstruction. Since we use efficient 2D encoder instead of 3D encoder, it takes only 1.05GB GPU memory to load the SIF-Net model. This allows us to do joint optimization with batch size up to 128 on a GPU with 48GB memory. 

\section{Generalization to NTU-RGBD dataset}
\paragraph{Obtaining input masks.} Unlike BEHAVE and InterCap where the human and object masks are provided by the dataset, there are no masks in NTU-RGBD. To this end, we run DetectronV2~\cite{wu2019detectron2} to obtain the human masks. We manually segment the object in the first frame using interactive segmentation~\cite{fbrs2020} ($<$1min/image) and then use video segmentation ~\cite{cheng2021mivos} to propagate the masks. The overhead of 1min/video manual label is small. 

We show more results from our method on NTU-RGBD dataset\cite{Liu_2019_NTURGBD120} and compare against CHORE\cite{xie22chore} in \cref{fig:comp-ntu}. It can be seen that CHORE may predict some reasonable object pose but it fails quite often to capture the fine-grained contacts between the human and object. Our method obtains more coherent reconstruction for different subjects, human-backpack interactions, camera view points and backgrounds. Please see our \href{https://virtualhumans.mpi-inf.mpg.de/VisTracker}{project website} for comparison in full sequences. 

\section{Limitations and future works}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.46\textwidth]{images/failure-cases-crop.pdf}
    \caption{Failure cases analysis. We show three typical failure cases of our method: A. The occluded object pose (T=156) changes significantly between two visible frames (T=0 and T=319) and it is difficult to accurate track the contact changes. B. The object pose is not commonly seen during interaction and it is difficult to predict for this rare pose. C. The object is symmetric. The joint optimization satisfies the object mask and contacts but is not semantically correct.}
    \label{fig:failure-cases}
\end{figure}
Although our method works robustly under heavy occlusions, there are still some limitations. Firstly, we assume known object templates for tracking, an interesting direction is to build such a template from videos as demonstrated by recent works \cite{yang2021lasr, yang2021viser, yang2022banmo, wu2021dove}. Secondly, it would be interesting to model multi-person or even multi-object interactions which is a more realistic setting in real-life applications. In addition, the backpack can also deform non-rigidly which is not modelled in our method. Further works can incorporate the surface deformation \cite{Li_3DV2022MocapDeform} or object articulation \cite{xu2021d3dhoi} into the human object interaction. We leave these for future works.   

We identify three typical failure cases of our method, some examples are shown in \cref{fig:failure-cases}. The first typical failure case comes from heavy occlusion when the object undergoes significant changes (object pose and contact locations) between two visible frames. In this case, it is very difficult to track the pose and contact changes accurately (\cref{fig:failure-cases} A). Second typical failure is due to the difficulty of pose prediction itself even the object is fully visible. In this case the object pose is uncommon and the network failed to predict it correctly (\cref{fig:failure-cases} B). Another failure is caused by symmetric objects. Our optimization minimizes the 2D mask loss and contact constraints but the network is confused by the symmetry and the initial pose prediction is not semantically correct (\cref{fig:failure-cases} C). In addition, the training data for these objects is very limited (only $1/3$ of other objects). More training data or explicitly reasoning about the symmetry \cite{zheng_tp-ae_2022} can be helpful. 




\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{images/comp-pose-preds6rows-crop.pdf}
    \caption{Comparing different object pose prediction method under heavy occlusions. Raw prediction is from our \sifNet{} output, SLERP denotes linear interpolation and CMIB is from \cite{KIM2022108894CMIB}. We can see SLERP and CMIB can correct some errors (row 5) but they do not take the human motion into account hence often fail in more challenging cases. Our method is more robust as it leverages information from both human motion and object pose from visible frames.}
    \label{fig:comp-pose-preds}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.98\textwidth]{images/ntu-comp-v2-crop.pdf}
    \caption{Comparing our method with CHORE\cite{xie22chore} on NTU-RGBD \cite{Liu_2019_NTURGBD120} dataset. It can be seen that CHORE does not capture the realistic contacts between the person and the backpack. Our method recovers the 3D human, the object and contacts more faithfully in different interaction types, camera view points and backgrounds.}
    \label{fig:comp-ntu}
\end{figure*}