In this section, we first review recent works that deal with human or object pose estimation and tracking separately. We then discuss recent progresses that model human and object interactions and works that deal with occlusions explicitly. 

\textbf{Human or object pose estimation and tracking.} 
After the introduction of SMPL~\cite{smpl2015loper} body model, tremendous progress has been made in human mesh recovery (HMR) from single images \cite{PonsModelBased,bogo2016smplify,SMPL-X:2019,alldieck2018video,alldieck2018detailed,kolotouros2019spin, coronaLVD} or videos \cite{yuan2022glamr, rajasegaran2021tp3d, rajasegaran2022tracking-phalp, kocabas2019vibe, dai2023sloper4d}. We refer readers to a recent review of HMR methods in~\cite{tian2022hmrsurvey}. 
On the other hand, deep learning method has also significantly improved object 6D pose estimation from single RGB images \cite{Wang_2021_GDRN, majcher_shape_nodate, fan_deep_2022, li_deepim_nodate, di_so-pose_2021, peng_pvnet_2019, hu_single-stage_cvpr20}. However, object pose tracking has received less attention and most works focus on RGBD inputs~\cite{wen2020se, stoiber_iterative_2022, zheng_tp-ae_2022, deng_poserbpf_2021, wen_bundletrack_2021}. Two works explore the camera localization ideas from SLAM communities and can track object from RGB videos \cite{liu2022gen6d, sun2022onepose}. Nevertheless, they heavily rely on visual evidence and the performance is unknown under heavy occlusions. They also do not track human-object interactions.

\textbf{Human-object interaction.} 
Modelling human object interaction is an emerging research topic in recent years. Hand-object interaction is studied with works modelling hand-object interaction from RGB~\cite{GrapingField:3DV:2020,Corona_2020_CVPR,hasson19_obman, ehsani2020force, yang2021cpf}, RGBD~\cite{Brahmbhatt_2019_CVPR,Brahmbhatt_2020_ECCV, hampali2020honnotate} or 3D inputs~\cite{GRAB:2020,ContactGrasp2019Brahmbhatt, zhou2022toch, petrov2020objectpopup}. There are also works that model human interacting with a static scene \cite{PROX:2019, huang2022rich, shimada2022hulc, weng2020holistic, Yi_MOVER_2022, CVPR21HPS, yi2022mime} or deformable surface \cite{Li_3DV2022MocapDeform}. More recently, the release of BEHAVE\cite{bhatnagar22behave} and InterCap\cite{huang2022intercap} datasets allows bench-marking of full-body interacting with a movable object. However, human-object interaction capture usually deploys multi-view RGB \cite{sun2021HOI-FVV} or RGBD\cite{bhatnagar22behave, huang2022rich, huang2022intercap, zhang2022couch, hampali2020honnotate, Brahmbhatt_2019_CVPR, hampali2020honnotate, dai2023sloper4d} cameras. Only a few works \cite{zhang2020phosa,  xie22chore, wang2022reconstruction} reconstruct dynamic human and object from monocular RGB input and our experiments show that they are not suitable for tracking.

\textbf{Pose estimation under occlusion.} 
Most existing methods assume occlusion-free input images hence are not robust under occlusions. Only a few methods address human pose estimation under partial occlusions ~\cite{Kocabas_PARE_2021, ZhangCVPR20OOH, rempe2021humor, Rockwell2020partial-occlusion, Fieraru3DHumanHumanInteraction, Kocabas_SPEC_2021} or long term occlusions \cite{yuan2022glamr}. For object pose estimation, pixel-wise voting \cite{peng_pvnet_2019} and self-occlusion \cite{di_so-pose_2021} are explored for more robust prediction under occlusions. More recently,  TP-AE~\cite{zheng_tp-ae_2022} predicts object occlusion ratio to guide pose estimation but relies on depth input. Although being impressive on separate human or object pose estimation, these methods do not reason about human-object interaction. Our method is the first one that takes both human and object visibility into account for interaction tracking. 




