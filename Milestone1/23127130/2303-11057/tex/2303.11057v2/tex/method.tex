


\section{Method}
\label{sec:method}





\subsection{Overview}

\vspace{-1mm}

As shown in Figure~\ref{fig_framework},
our framework is composed of two main parts:
(1) we propose to use dense affordance representing manipulation policy (\ref{sec:aff_policy}), estimate state `value' using dense affordance and incorporate `value' into dense affordance to avoid local optima for multi-step manipulation (\ref{sec:value}), break the picking-placing dependency cycle and stably learn affordance stage by stage (\ref{sec:learn_aff});
(2) to tackle the difficulty in collecting multi-stage and successful interactions, we propose a method (named \textit{Fold to Unfold}) generic to many tasks to efficiently collect data in the reversed task completion order (\emph{e.g.}, collecting unfolding data by folding cloth) (\ref{sec:data}). 
Besides,
we propose Integrated Systematic Training to further integrate the proposed affordance into a whole system (\ref{sec:ist}).
Finally, 
we describe network architectures and loss function (\ref{sec:network}).


% trim={<left> <lower> <right> <upper>}
\begin{figure}[h]
  \centering
%   \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \includegraphics[width=\linewidth]{figs/framework_modified.pdf}
  \caption{\textbf{Our proposed framework} learns dense picking and placing affordance for deformable object manipulation (\emph{e.g.}, \textbf{Unfolding Cloth}).
  We collect multi-stage interaction data efficiently (Left) and learn proposed affordance stably in a multi-stage schema (Right) in the reversed task accomplishment order, from states close to the target to complex states.
  }
  \vspace{-3.5mm}
  \label{fig_framework}
\end{figure}




\vspace{-1.5mm}
\subsection{Dense Visual Affordance Representing Policy}
\label{sec:aff_policy}
\vspace{-1mm}

This section introduces how to use dense visual affordance to represent manipulation policy.
For simplicity, we first discuss affordance for a greedy policy.

Described in Section~\ref{sec_formulation}, we formulate the manipulation problem into learning picking and placing policies, \emph{i.e.}, the picking point $p_{pick}$ and placing point $p_{place}$ given the observation $o$ (with the size of $m\times n$ points).
As the action space is all points on the object for picking, and all points in the space for placing, 
it comes naturally to use dense picking affordance map $A_{o}^{pick}$ (size $m\times n$) indicating how picking each point will facilitate the task, and dense placing affordance map $A_{o|{p_{pick}}}^{place}$ (size $m\times n$) indicating how placing the picking point $p_{pick}$ on each point will facilitate the task.

Like other dense affordance studies~\cite{mo2021where2act, wu2022vatmart, zhao2022dualafford}, a \textbf{greedy} way to supervise $A_{o|{p_{pick}}}^{place}$ is directly using the distance between \textbf{the target $T$} and \textbf{the new object state $o^{\prime}$} after picking $p_{pick}$ and placing on $p_{place}$ in $o$. 
For example, we use $1 - dist(o,\ T)$, \emph{i.e.}, the cloth coverage area, to supervise $A_{o|{p_{pick}}}^{place}$ for unfolding.
So we can estimate placing affordance score $g_{o,\ p_{place}|p_{pick}}^{place}$ on $p_{place}$ as (Figure~\ref{fig_train}, Middle, temporarily dismiss `value' in the Figure):
\vspace{-2mm}

\begin{equation}
\label{eq1}
g_{o,\ p_{place}|p_{pick}}^{place} = 1 - dist(o^{\prime},\ T)
\end{equation}

Given a picking point $p_{pick}$, the placing policy will select $p_{place}$ with the highest affordance, so the picking affordance score $g_{o,\ p_{pick}}^{pick}$ on $p_{pick}$
 can be estimated using the affordance score of the best placing point (Figure~\ref{fig_train}, Left):

\vspace{-4mm}

\begin{equation}
\label{eq2}
g_{o,\ p_{pick}}^{pick}=\max_i{g_{o,\ p_i|p_{pick}}^{place}}, i \in \{1, .., m\times n\}
\end{equation}

\vspace{-1mm}

We use two networks $\mathcal{M}_{pick}$ and $\mathcal{M}_{place}$ to respectively learn $A_{o}^{pick}$ and $A_{o|{p_{pick}}}^{place}$ (architectures in Section~\ref{sec:network}).

\begin{figure}[h]
  % \vspace{-2mm}
  \centering
  \includegraphics[width=\linewidth,  trim={0cm, 0cm, 0cm, 0cm}, clip]{figs/train_modified_1.pdf}
  % \vspace{-3mm}
  \caption{\textbf{Learning placing and picking affordance with state `value's for the future.} 
  Left to Right: The bottom black arrow indicates the manipulation (inference) order. Right to Left: Arrow flows show dependencies among placing affordance, picking affordance and `value's. 
  Given observation $o$, we select 3 picking points $p_1$ $p_2$ $p_3$, and show how to supervise corresponding placing affordance $A_{o|p_1}^{place}$ $A_{o|p_2}^{place}$ $A_{o|p_3}^{place}$, and how to supervise $A_{o}^{pick}$ on $p_1$ $p_2$ $p_3$ using computed corresponding placing affordance.
  }
  \label{fig_train}
\vspace{-2mm}
\end{figure}

During inference, in each step, the greedy policy first selects $p_{pick}$ with the highest affordance score in $A_{o}^{pick}$ given $o$, and then selects $p_{place}$ with the highest affordance score in $A_{o|{p_{pick}}}^{place}$ given $o$ and $p_{pick}$, resulting in the \textbf{temporary best state} after the pick-and-place action. 




\vspace{-1mm}
\subsection{Estimating State Values and Learning Foresightful Affordance}
\label{sec:value}
\vspace{-1mm}


As shown in Figure~\ref{fig_global_local}, for multi-step manipulation, only evaluating the direct distance between the current state and the target 
(greedy method described above) may result in many local optimal states that are temporarily closer to target but harder for future actions to complete the whole task.

Dynamic Programming (DP)~\cite{bellman1966dynamic} and Q-Learning~\cite{watkins1992q} tackle this local optima problem by estimating the \textbf{`value'} of a state that indicates whether a state is beneficial to the task in the long term (instead of the current performance).
Inspired by them,
we can add such state `value' (formally formulated in Equation~\ref{eq4}) to the estimation of $A_{o|{p_{pick}}}^{place}$:

\vspace{-3mm}
\begin{eqnarray}
\label{eq3}
g_{o,\ p_{place}|p_{pick}}^{place} = \alpha \times value_{o^{\prime}} + \beta \times (1 - dist(o^{\prime},\ T))
\\
\mbox{where}\quad \alpha + \beta = 1 \nonumber
\end{eqnarray}

With such $A_{o|{p_{pick}}}^{place}$ that is foresightful for long-term tasks,
$A_{o}^{pick}$,
which is the aggregation of $A_{o|{p_{pick}}}^{place}$,
will therefore get such foresightfulness spontaneously.

Estimating the `value' in a state requires understanding all possible actions and their corresponding future results, and then selecting the best for the estimation.
As $A_{o}^{pick}$ estimates the action result on each point,
state `value' can be estimated by selecting the $p_{pick}$ with the highest score in $A_{o}^{pick}$ (Figure~\ref{fig_train}, Left):


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figs/local_global_optimal_modified.pdf}
  \caption{\textbf{Local Optima v.s. Global Optima.} Many local optimal states are temporarily closer to target (\emph{e.g.}, having larger coverage area in unfolding task), but making future actions hard to coordinate to accomplish the whole task. We propose to use `value' to indicate whether a state is suitable for future actions, with which the policy can avoid those local optimal states in multi-step tasks.
}
\vspace{-3.9mm}
  \label{fig_global_local}
\end{figure}

\vspace{-3mm}
\begin{equation}
\label{eq4}
value_{o}=\max_i{g_{o,\ p_i}^{pick}}, i \in \{1, .., m\times n\}
\end{equation}


As $value_{o}$ could be estimated by $A_{o}^{pick}$, $A_{o|{p_{pick}}}^{place}$ could be reformulated using both $A_{o}^{pick}$ and the direct distance:


\vspace{-5mm}
\begin{equation}
\label{eq5}
\begin{split}    
g_{o,\ p_{place}|p_{pick}}^{place} &= \alpha\times\max_i{g_{o^{\prime},\ p_i}^{pick}} \\
&+ \beta \times (1-dist(o^{\prime},T)) 
\\
&\mbox{where} \quad \alpha + \beta = 1
\end{split}
\end{equation}
\vspace{-5mm}

\vspace{-1mm}
\subsection{\textit{Break the Cycle and Cut into Stages}: Learning Foresightful Affordance Stably Stage by Stage}
\label{sec:learn_aff}
\vspace{-1mm}


The above-formulated picking and placing affordance forms a chicken-egg dependency cycle: 
picking affordance is dependent on placing affordance, while placing affordance is dependent on picking affordance.

To break the dependency cycle, \textbf{states close to the target} (\emph{e.g.}, cloth almost fully unfolded) become the key.
As the task is almost accomplished in these states, their values and direct distances to the target are nearly the same.
So for an interaction where the ending state $o^\prime$ is close to the target, we directly use their distances to the target (instead of both distances and `value's) to supervise the corresponding placing affordance of the starting state $o$:

\vspace{-4mm}
\begin{equation}
\label{eq6}
\begin{split}
&g_{o,\ p_{place}|p_{pick}}^{place}=1 - dist(o^{\prime},\ T)\\
&\mbox{when}\ dist(o^{\prime},\ T)\ \mbox{is close to 0}\quad i.e.\mbox{, the last step}
\end{split}
\end{equation}

According to Equations~\ref{eq2}~\ref{eq5}~\ref{eq6}, the proposed picking and placing affordance could be estimated without the dependency cycle.
As the dependency cycle breaks in states close to the target, we learn dense affordance from these simple states to more complex states (reversed order of inference) as shown in Figure~\ref{fig_framework} (Right).
Specifically,
we divide the learning procedure into multiple stages. 
In the first stage, we learn affordance for states that can reach states close to the target within one step,
using the direct distances of the following states as supervisions. 
In the $i$-th ($i>1$) stage,
we learn affordance for states that can reach states in the ($i$-1)-th stage within one step,
using both the direct distances and the `value's of states in the ($i$-1)-th stage as supervisions.
In each stage,
we first train $\mathcal{M}_{place}$ using stable `value's provided from the trained $\mathcal{M}_{pick}$ in the previous stage,
and then train $\mathcal{M}_{pick}$ with stable supervisions (max of placing affordance) provided from the trained $\mathcal{M}_{place}$ in this stage.

In this way, during the whole training, 
$A_{o|{p_{pick}}}^{place}$ and $A_{o}^{pick}$ both have stable supervisions from the former stage, and can provide stable supervisions for the latter stage.


This stage-by-stage learning schema empowers our method with superiority over RL methods. 
Although RL also estimates states and actions using Bellman Equation, 
it simultaneously estimates and updates values across all states (offline RL) or trajectories of states (online RL), 
which is difficult to efficiently and stably learn the values, 
as RL struggles in iteratively updating state `value's, especially when considering the prohibitively large state and action space of deformable objects.
In contrast,
like other dense affordance works~\cite{mo2021where2act, zhao2022dualafford, wu2022vatmart},
we stably learn affordance with `value' using supervised learning, 
with stable supervisions provided in the previous training stage.
Experiments demonstrate our superiority over RL in Section~\ref{exp_rl}.


\subsection{\textit{Fold to Unfold}: Efficient Multi-stage Data Collection for Learning Foresightful Affordance}
\label{sec:data}

The above-described training schema requires multi-stage data for training.
Specifically,
in the first stage,
the starting states are one-step to states close to the target.
In the $i$-th ($i>1$) stage,
the starting states are one-step to states in the ($i$-1)-th stage.
Each starting state's actions and corresponding ending states should be diverse, as we need to learn the affordance representing dense distributions.
% Although the ending states in each stage may be diverse, the `value's will be provided from the best of next states (shown in red squares in Figure~\ref{fig_framework}, Left).

However, due to the complexity of states and dynamics,
data collection methods used by previous dense affordance works (random policies~\cite{mo2021where2act, mo2021o2oafford} or state-based RL~\cite{wu2022vatmart, zhao2022dualafford}) could hardly collect such data.
On the other hand,
designing expert policies~\cite{seita2021learning} or hand-crafting demonstrations are difficult and time-consuming for different tasks.


Therefore, we propose a novel self-supervised method, named \textit{Fold to Unfold}, using reversed actions of tasks to efficiently collect multi-stage data.
This method is generic to many kinds of deformable object manipulation tasks, with no need for human-designed expert policies or annotations.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figs/fold_to_unfold.pdf}
  \caption{\textit{Fold to Unfold} collection in simulator and real world.}
\vspace{-3mm}
  \label{fig_fold_unfold}
\end{figure}

Similar to the training procedure,
we collect data from states close to the target, to more complex states.

Specifically, as shown in Figure~\ref{fig_fold_unfold}, from a state $o_i$ in stage $i$, we select a picking point $p_{pick}$, put it on a placing point $p_{place}$ and get $o_{i+1}$. 
Then, from $o_{i+1}$, we execute the reversed action, pick $p_{place}$, place on $p_{pick}$ and get $o_i^{\prime}$.
If $o_i^{\prime}$ is similar to $o_i$, we choose $o_{i+1}$ as a starting state in stage $i$+1, and sample diverse actions on $o_{i+1}$ with different corresponding results to train dense affordance in the ($i$+1)-th stage (shown in Figure~\ref{fig_framework}).

Through a few stages of data collection, object states become complex and diverse,
empowering trained affordance networks with generalization towards diverse novel states.





\textbf{Note that}, although reversed actions cannot fully recover previous states, \emph{i.e.}, $o_i^{\prime}$ are not the same as $o_i$, chances are that $o_i^{\prime}$ and $o_i$ are similar, and thus this method still greatly improves sample efficiency.
Also, proposed affordance is \textbf{not dependent on} this data collection method, as it can be trained on data collected by any method.


\vspace{-1mm}
\subsection{Integrated Systematic Training}
\label{sec:ist}
\vspace{-1mm}

Although the above designs and training procedure enable the affordance learning for multi-step tasks, $\mathcal{M}_{pick}$ and $\mathcal{M}_{place}$ are trained using only offline collected data in different stages, without considering the actual execution performance of the policies provided by the two modules.
During actual manipulation procedures, the policy is the composite policy of $\mathcal{M}_{pick}$ and $\mathcal{M}_{place}$, and pick-and-place actions are executed one by one sequentially as a whole system.
Therefore, we propose the Integrated Systematic Training (IST) procedure to adapt $\mathcal{M}_{pick}$ and $\mathcal{M}_{place}$ using online data.

In this procedure, with offline trained $\mathcal{M}_{place}$ and $\mathcal{M}_{pick}$, 
we randomly sample object initial states,
use $\mathcal{M}_{place}$ and $\mathcal{M}_{pick}$ as the policy to select $p_{pick}$ and $p_{place}$, execute pick-and-place step by step, and use actual results to simultaneously update $\mathcal{M}_{place}$ and $\mathcal{M}_{pick}$.
Through this procedure, the two modules are constantly adapted by consecutively online-sampled and actually-executed data, and thus are gradually integrated into a whole system.



\vspace{-1mm}
\subsection{Network Architectures and Loss Function}
\label{sec:network}
\vspace{-1mm}

For architectures of $\mathcal{M}_{pick}$ and $\mathcal{M}_{place}$, 
we use Fully Convolutional Networks (FCNs)~\cite{long2015fully} same in Transporter~\cite{zeng2021transporter, seita2021learning} with extra skip-connections as backbone per-point feature extractor. 
For $\mathcal{M}_{pick}$, we directly use $p_{pick}$ feature to predict picking affordance score on $p_{pick}$.
For $\mathcal{M}_{place}$, we use feature concatenation of $p_{pick}$ and $p_{place}$ to predict placing affordance score on $p_{place}$.
%
To train both $\mathcal{M}_{pick}$ and $\mathcal{M}_{place}$, we use Mean Absolute Error (MAE) between predictions and ground truth as the loss function.
