% \vspace{-3mm}
\section{Experiments}
% \vspace{-2mm}



\subsection{Tasks, Settings and Metrics}

\paragraph{Tasks.} To demonstrate the superiority of our framework, we select 2 representative tasks from \textit{DeformableRavens} benchmark: (1) \textbf{cable-ring}: manipulating a ring to a given green circle, (2) \textbf{cable-ring-notarget}: manipulating a ring to any circle, as well as 2 representative tasks from \textit{SoftGym}: (3) \textbf{SpreadCloth}: spreading crumpled cloth to be flat, (4) \textbf{RopeConfiguration}: manipulating a rope from a random pose to a target pose (we use the shape `S' as the target). Among them, the first two are relatively easier. They can be accomplished without considering future actions and states, and we conduct them to show our dense affordance's superiority over methods imitating expert demonstrations. The last two are much harder and would be better accomplished considering future actions and states to avoid local optima.

\vspace{-5mm}

\paragraph{Settings.} For all tasks, in both training and testing, we set different random seeds for each episode, producing unseen and diverse initial poses of objects. To compare the generalization ability between our proposed dense affordance and imitation-based methods, for cables in \textit{DeformableRavens}, we directly test the model trained on cables with 32 beads over novel cable configurations with 24, 28 or 36 beads.

\vspace{-5mm}

\paragraph{Metrics.} For cable-ring and cable-ring-notarget, we follow \textit{DeformableRavens} and use the manipulation successful rate as the metric. For SpreadCloth and RopeConfiguration, we follow \textit{SoftGym} and use the normalized score as the metric. For all tasks, higher scores indicate better performance.

% \vspace{-1mm}
\subsection{Baselines}
% \vspace{-1mm}

For two cable-ring related tasks, we compare our method with baselines with or without expert demonstrations:
% with and without human expert demonstrations:
%
\begin{itemize}
  \vspace{-2mm}
  \item 
  \textbf{Transporter}~\cite{zeng2021transporter, seita2021learning} is commonly used for robotic manipulation by learning visual correlation for picking and placing points. In \textit{DeformableRavens}~\cite{seita2021learning} it is trained by cloning expert demonstrations and achieves SOTA performance over relevant tasks.
  \vspace{-1mm}
  \item 
  \textbf{GT-State} receives ground truth (GT) pose of the target object, and regresses $p_{pick}$ and $p_{place}$ with MLP.
  \vspace{-1mm}
  \item 
  \textbf{GT-State 2-Step} first regresses $p_{pick}$ and then $p_{place}$ using $p_{pick}$ and GT pose concatenation, both via MLP.
\end{itemize}


For SpreadCloth and RopeConfiguration, as object states and dynamics are too complex and the tasks are too difficult to hand-engineer expert policies, we compare our method with baselines focused on multi-step planning:

\begin{itemize}
  \vspace{-2mm}
  \item 
  \textbf{CURL-SAC}~\cite{laskin_srinivas2020curl} that uses a model-free RL approach with contrastive unsupervised representations.
  \vspace{-2mm}
  \item 
  \textbf{DrQ}\cite{yarats2021drq} applies augmentation, regularization to RL.
  \vspace{-2mm}
  \item 
  \textbf{PlaNet}~\cite{hafner2019planet} learns state space dynamics for planning.
  \vspace{-2mm}
  \item 
  \textbf{MVP}~\cite{wu2019learning} learns pick-and-place policy with model-free RL designed for deformbale object manipulation.
\end{itemize} 


\vspace{-0.1cm}
\subsection{Qualitative Results and Analysis}
\vspace{-0.1cm}



\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth, trim={0.cm, 0cm, 0cm, 0cm}, clip]{figs/trajectory_modified.pdf}  
   \caption{\textbf{Example action sequences} for cable-ring, cable-ring-notarget, SpreadCloth and RopeConfiguration. \textbf{White point denotes picking and black point denotes placing}.}
  \label{fig_vis_traj}
  \vspace{-2mm}
\end{figure}


Figure~\ref{fig_vis_traj} shows examples of manipulation trajectories for diverse tasks using our proposed affordance.
It is worth mentioning that, in the second state of SpreadCloth (Row 3), though it is intuitive to place the picking point (white) to the top-left position, the model places it to the bottom-right position (black), as the corresponding next state has \textbf{low coverage} but \textbf{high `value'}, requiring only one following pick-and-place action to almost fully unfold the cloth.



% trim={<left> <lower> <right> <upper>}
\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth, trim={2cm, 0cm, 0cm, 0cm}, clip]{figs/pickplace_modified.pdf}  
  \caption{\textbf{Picking and placing affordance.} Each row contains two (picking affordance, observation with $p_{pick}$, placing affordance) tuples for a task. $p_{pick}$ is selected by picking affordance. \textbf{Higher color temperature} means \textbf{higher affordance}. 
  }
  \label{fig_vis_critic}
\end{figure}


Figure~\ref{fig_vis_critic} visualizes picking and placing affordance, clearly showing that the learned affordance represents deformable objects with complex states and dynamics and facilitates selecting picking and placing points for manipulation.
Figure~\ref{fig_vis_potential} visualizes `value's of states.



% trim={<left> <lower> <right> <upper>}
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth, trim={0cm, 0cm, 0cm, 0cm}, clip]{figs/potential_modified.pdf}  
  \caption{\textbf{Visualization of `value'} shows that some states with closer distances to the target (\emph{e.g.}, larger area) may not have higher `value', as these states are hard for future actions to fulfill the task.
  }
  \label{fig_vis_potential}
\end{figure}





\vspace{-0.1cm}
\subsection{Quantitative Results and Analysis}
\vspace{-0.1cm}
\label{exp_rl}


\begin{table}[h!]
  \centering
  \setlength{\tabcolsep}{0.9mm}
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    Method              & cable-ring & cable-ring-notarget  \\ \midrule \midrule
    GT-State        &     0.0       &           5.0            \\ \midrule
    GT-State 2-Step   &      0.0      &  1.7   \\ \midrule
    Transporter      &    68.3    &   70.0      \\ \midrule
    Ours      &  \textbf{81.7}   &  \textbf{95.0}    \\ \bottomrule
    \end{tabular}
  \caption{\textbf{Quantitative results in \textit{DeformableRavens}}.}
  \label{tab_accu_single}
  \vspace{-2mm}
\end{table}


\begin{table}[h!]
  \centering
  \setlength{\tabcolsep}{0.9mm}
    \begin{tabular}{@{}lcccc@{}}
    \toprule
    Method              & SpreadCloth & RopeConfiguration \\ \midrule \midrule
    CURL-SAC     &  0.195         &      0.348  \\ \midrule
    PlaNet      &  0.387         &      0.236  \\ \midrule
    DrQ      &    0.275          &      0.154  \\ \midrule
    MVP      &    0.372          &      0.258  \\ \midrule
    Ours      &  \textbf{0.758}   &   \textbf{0.529} \\ \bottomrule
    \end{tabular}
  \caption{\textbf{Quantitative results in \textit{SoftGym}}.}
  \label{tab_accu_mul}
  \vspace{-2mm}
\end{table}

Table~\ref{tab_accu_single} shows our framework outperforms all baselines in \textit{DeformableRavens}.
%
Specifically, for \textbf{GT State} and \textbf{GT State 2-Step}, GT states can only provide part of the necessary information, and it is difficult to acquire the precise GT states of deformable objects in the real world.
We outperform \textbf{Transporter} that learns visual correlation for the matching between picking and placing points, even though it directly clones successful demonstrations from hand-crafted expert policies. Two possible reasons are that: 1) Dense affordance is more suitable for deformable objects as it represents the results of diverse actions on complex states, while visual correlation in \textbf{Transporter} is suitable for matching like assembling-kits;
2) training on expert demonstrations and cloning expert policies will limit the model's generalization toward diverse situations during inference.
To further evaluate the \textbf{generalization ability of the dense affordance}, we produce different novel object configurations, using 24, 28, and 36 as the bead number instead of the initial 32.
Our method's slighter decrease in performance over novel object configurations shown in Table~\ref{tab_generalization} also demonstrates its generalization ability.
Besides, expert policies need to be elaborately hand-designed for different tasks, while our method can apply to any task without modifications.

Table~\ref{tab_accu_mul} shows our framework outperforms all baselines in \textit{SoftGym}. As described in ~\ref{sec:learn_aff}, compared with those RL methods, our framework learns representations of complex states for multi-step manipulation in a stable way.



\begin{table}[h!]
  \setlength{\tabcolsep}{0.9mm}
  \centering
    \begin{tabular}{@{}lccc@{}}
    \toprule
    Task       & cable-ring & cable-ring-notarget \\
    configurations   & 24 / 28 / 36 & 24 / 28 / 36 \\ \midrule \midrule
    Transporter       &      33.3 / 58.3 / 32.7      & 60.0 / 71.7 / 31.7 \\ \midrule
    Ours        &  \textbf{61.6} / \textbf{86.7} / \textbf{58.3}   &   \textbf{81.7} / \textbf{96.7} / \textbf{78.3}  \\  \bottomrule
    \end{tabular}
  \caption{\textbf{Manipulation scores on novel configurations in \textit{DeformableRaves}} showing our method's  generalization capability.}
  \label{tab_generalization}
\end{table} 




\vspace{-0.1cm}
\subsection{Ablation Studies and Analysis}
\label{sec:ablation}
\vspace{-0.1cm}

To demonstrate necessities of our framework's different components, we conduct ablation experiments by comparing our method with:
    % \item 
    1) \textbf{Ours RandPick}: our method with the picking policy replaced by a random policy;
    % \item 
    2) \textbf{Ours ExpertPick}: our method with the picking policy replaced by Transporter's expert;
    % \item 
    3) \textbf{Ours w/o IST}: our method without Integrated Systematic Training (IST);

For SpreadCloth and RopeConfiguration that require strongly related sequential actions, we additionally compare 1) ablated versions using different stages of data, 2) \textbf{Ours only dist} directly and \textbf{greedily} trained on all collected data instead of stage-by-stage considering `value's.

\begin{table}[h!]
  \centering
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    Method              & cable-ring & cable-ring-notarget  \\ \midrule \midrule
    Ours RandPick        &      11.7      &          58.3                 \\ \midrule
    Ours ExpertPick        &     76.7       &        41.7                     \\ \midrule
    Ours w/o IST         &     78.3       &     91.7                  \\ \midrule
    Ours      &  \textbf{81.7}  &  \textbf{95.0}        \\ \bottomrule
    \end{tabular}
  \caption{\textbf{Ablation studies} in \textit{DeformableRavens}.}
  \label{tab_ablation}
    \vspace{-3mm}
\end{table}

\begin{table}[h!]
  \centering
    \setlength{\tabcolsep}{1.2mm}
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    Method              & stage1 & stage2 & stage3 & stage4 & stage5  \\ \midrule \midrule
    Ours RandPick        & 0.241 & 0.211 & 0.304 & 0.185 & 0.190             \\ \midrule
    Ours w/o IST         & 0.526 & 0.586 & 0.621 & 0.624 & 0.612                \\ \midrule
    Ours only dist & \textbf{0.701} & \textbf{0.701} & 0.701 & 0.701 & 0.701  \\ \midrule
    Ours      &  0.589 & 0.695 & \textbf{0.752} & \textbf{0.754} & \textbf{0.758}    \\ \bottomrule
    \end{tabular}
  \caption{\textbf{Ablation studies} in \textit{SpreadCloth}.}
  \label{tab_multi_step_cloth}
    \vspace{-3mm}
\end{table}

\begin{table}[h!]
  \centering
    \setlength{\tabcolsep}{1.2mm}
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    Method              & stage1 & stage2 & stage3 & stage4 & stage5  \\ \midrule \midrule
    Ours RandPick        & 0.329 & 0.302 & 0.332 & 0.334 & 0.322             \\ \midrule
    Ours w/o IST         & 0.359 & 0.418 & 0.437 & 0.479 & 0.474                \\ \midrule
    Ours only dist & \textbf{0.460} & 0.460 & 0.460 & 0.460 & 0.460  \\ \midrule
    Ours      &  0.441 & \textbf{0.503} & \textbf{0.518} & \textbf{0.527} & \textbf{0.529}    \\ \bottomrule
    \end{tabular}
  \caption{\textbf{Ablation studies} in \textit{RopeConfiguration}.}
  \label{tab_multi_step_rope}
    \vspace{-5mm}
\end{table}

Table~\ref{tab_ablation},~\ref{tab_multi_step_cloth}and~\ref{tab_multi_step_rope} show quantitative results of ablation experiments.
\textbf{Ours RandPick} and \textbf{Ours ExpertPick} show that, with the same placing affordance, our proposed picking affordance helps the framework perform the best.

\textbf{Ours only dist} in Table~\ref{tab_multi_step_cloth}and~\ref{tab_multi_step_rope} show that, directly training on all the diverse data without estimating state `value' limits the performance compared with our proposed framework. Besides, as shown in Figure~\ref{fig_multi_step_vs_gt}, \textbf{Ours only dist} will propose actions leading to local optimal states, while our foresightful affordance will help to avoid that.

Table~\ref{tab_multi_step_cloth} and~\ref{tab_multi_step_rope} show that, a series of steps of data empowers affordance with generalization to diverse states.

\textbf{Ours w/o IST} in Table~\ref{tab_ablation},~\ref{tab_multi_step_cloth}and~\ref{tab_multi_step_rope}, and adjusted affordance and `value's in Figure~\ref{fig_joint_training} demonstrate IST helps integrating picking and placing modules into a whole, generating more precise perception of affordance and `value's.

% trim={<left> <lower> <right> <upper>}
\begin{figure}[h!]
  \centering 
%   \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}} 
 \includegraphics[width=\linewidth, trim={0cm, 0cm, 0cm, 0cm}, clip]{figs/only_dist_modified.pdf}  
  \caption{\textbf{Placing affordance trained using `value' supervision ({\color{red}red}) and only using the greedy direct distance ({\color{blue}blue})}.}
   \label{fig_multi_step_vs_gt}
     \vspace{-5mm}
\end{figure}

% trim={<left> <lower> <right> <upper>}
\begin{figure}[h!]
  \centering 
%   \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
 \includegraphics[width=\linewidth, trim={0cm, 0mm, 0cm, 0cm}, clip]{figs/AJT_modified.pdf}  
  \caption{\textbf{Picking and placing affordance before (middle) and after (right) IST} of the observation (left). White: pick point.}
   \label{fig_joint_training}
   \vspace{-3mm}
\end{figure}


\subsection{Real-world Experiments}

To bridge the sim2real gap and implement our method in the real world, similar to~\cite{wu2019learning, ha2021flingbot}, we use domain randomization to train affordance models in simulation and fine-tune them in real world.
Specifically, we collect real-world data using \textit{Fold to Unfold}, fine-tune trained-in-simulation $\mathcal{M}_{pick}$ and $\mathcal{M}_{place}$ stage by stage using the collected data.

For evaluations, we randomly lift and drop the objects for five times to get the initial state, and then run the models for ten pick-and-place actions to perform the tasks. The manipulation score is the average normalized score (computed the same as in \textit{SoftGym}) of sixty trajectories. 

Shown in Figure~\ref{fig_real_world}, given real-world observations with textures and physics different from objects in simulation, our method predicts reasonable picking and placing affordance and selects actions for tasks.

We compare our method with MVP~\cite{wu2019learning}, as it also uses pick-and-place as action primitive and thus could do both cloth and rope related tasks, and provides real world experiments. 
As shown in Table~\ref{tab_real_world}, our method outperforms MVP as explained in ~\ref{exp_rl}.

See the supplementary for more implementation details.

\begin{table}[h!]
  \centering
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    Method              & SpreadCloth  & RopeConfiguration \\ \midrule \midrule
    MVP        &      0.307      &         0.227       \\ \midrule
    Ours        &     \textbf{0.683}       &        \textbf{0.461}         \\ \midrule
    \end{tabular}
  \caption{Manipulation scores in the real world.}
  \label{tab_real_world}
    \vspace{-3mm}
\end{table}


% trim={<left> <lower> <right> <upper>}
\begin{figure}[h!]
  \centering 
%   \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}} 
 \includegraphics[width=\linewidth, trim={0cm, 0cm, 0cm, 0cm}, clip]{figs/real_world_modified.pdf}  
  \caption{Examples of real-world manipulation trajectories guided by picking and placing affordance.}
   \label{fig_real_world}
     \vspace{-2mm}
\end{figure}
