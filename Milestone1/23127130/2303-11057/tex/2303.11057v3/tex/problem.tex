\vspace{-1.5mm}
\section{Problem Formulation}
\vspace{-1.5mm}

\label{sec_formulation}

In this section, we describe how we formulate learning \textbf{policy for multi-step deformable object manipulation} into learning \textbf{policies for picking and placing}.


Following current benchmarks \textit{DeformableRavens}~\cite{seita2021learning} and \textit{SoftGym}~\cite{corl2020softgym}, we formulate the problem as learning a policy $\pi$ that outputs robot action $a_t$, given the visual observation $o_t$ (denoted as $o$) at time $t$. We use pick-and-place as primitive action, \emph{i.e.}, $a_t = (a_{pick},\ a_{place})$, with $a_{pick}$ and $a_{place}$ denoting picker pose for picking and placing. As explained in ~\cite{seita2021learning}, the picker can complete the tasks without rotation, so $a_{pick}$ and $a_{place}$ can be denoted as picking point $p_{pick}$ and placing point $p_{place}$.

The composite of $p_{pick}$ and $p_{place}$ makes up a large combinatorial action space hard for a network to learn directly and simultaneously.
For the underlying nature that $p_{place}$ highly depends on $p_{pick}$, 
we follow~\cite{wu2019learning} and disentangle learning the composite of $p_{pick}$ and $p_{place}$ into respectively learning $p_{pick}$ and $p_{place | p_{pick}}$ (denoted as $p_{place}$).

Therefore, we formulate the problem into learning picking and placing policies. In Method Section~\ref{sec:method}, we describe how we learn the policies using \textbf{foresightful dense visual affordance} with each state's \textbf{`value'} to avoid local optima.
