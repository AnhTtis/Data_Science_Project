\vspace{-1.5mm}
\section{Related Work}
\vspace{-1.5mm}


% \vspace{-1mm}
\subsection{Perceive and Manipulate Deformable Objects}

% Compared with manipulating deformable objects,
For deformable object perception,
previous works learn and leverage appearance and geometric information ~\cite{nascimento2019geobit, moreno2011deformation,simo2015dali}, softness ~\cite{cellini2013visual}, visual-tactile sensation~\cite{cui2020grasp}, shape completion~\cite{chi2021garmentnets}, emotional perception ~\cite{lee2018anthropomorphic}, optimal perception~\cite{cuiral2020rgb} and the continuity of perception~\cite{martinez2019continuous,florence2018dense,kumari2016haptic}.
The manipulation is challenging for its high complexity.
Typical methods utilize Reinforcement Learning (RL) or Imitation Learning. State-based RL methods~\cite{jangir2020dynamic} consume object states to perform objects manipulation, while
others~\cite{wu2019learning, lee2020learning} utilize visual RL to manipulate rope and cloth with limited states or targets. Transporter~\cite{zeng2021transporter, seita2021learning} exploits imitation learning to perform different tasks, requiring human-designed expert policy and demonstrations for each task, limiting the generalization over tasks and objects. 
%
Vision-based methods~\cite{ganapathi2021learning, jia2018manipulating} use visual feedback or correspondence to perform manipulations.
Flow-based methods~\cite{shen2022acid, weng2022fabricflownet} learn forward dynamics for manipulation, requiring much time in planning.
Instead of learning flow-based dynamics or from demonstrations, our work builds a bridge between perception and manipulation, taking advantage of the generalization ability of affordance and estimating state `value' for efficient planning.



% \vspace{-2mm}
\subsection{Visual Affordance for Robotic Manipulation}

Learning visual affordance~\cite{gibson1977theory} aims to learn representations of objects or scenes that indicate possible ways for robots to interact and complete tasks.
Many recent works learn affordance for grasping~\cite{mandikal2020graff, montesano2009learning, corona2020ganhand, mia2017affordance, kokic2020learning, zeng2018robotic}, articulated object manipulation~\cite{mo2021where2act, wu2022vatmart, wang2021adaafford, act_the_part, geng2022end}, object-object interaction~\cite{mo2021o2oafford}, collaboration~\cite{zhao2022dualafford} and interaction in a scene~\cite{interaction-exploration, nagarajan2020ego, goff2019building}.
Most tasks can be achieved in a single step (\emph{e.g.}, grasping objects, pulling drawers), and learned affordance only contains actionable information for single-step manipulation.
However, deformable objects with complex states and dynamics require many steps to manipulate.
We move a step towards proposing dense affordance for deformable objects, which not only indicates complex states and dynamics but also takes the subsequent actions of a certain state into consideration for long-term tasks to avoid local optima.
FlingBot~\cite{ha2021flingbot} learns affordance for unfolding cloth with flinging actions, while our proposed affordance is more generic for large action space and diverse tasks.

