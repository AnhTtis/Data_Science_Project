\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{6150} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Learning Foresightful Dense Visual Affordance \\ for Deformable Object Manipulation}


\author{\textbf{Ruihai Wu}$^{1,3,4}$\thanks{Equal contribution, order determined by coin flip.} \quad  \textbf{Chuanruo Ning}$^{2,1}$\footnotemark[1] \quad \textbf{Hao Dong}$ ^{1,3,4}$\thanks{Corresponding author}\\
$^1$CFCS, School of CS, PKU \quad % Peking University \quad
$^2$School of EECS, PKU \quad %Peking University\quad\\
$^3$BAAI \quad
\\$^4$National Key Laboratory for Multimedia Information Processing, School of CS, PKU \quad\\
% {\tt\normalsize \{wuruihai,hao.dong\}@pku.edu.cn},
% {\tt\normalsize chuanruo@stu.pku.edu.cn}
% \\
% \\ \\
% \url{https://hyperplane-lab.github.io/vat-mart}
}

% \author{Ruihai Wu\thanks{Equal contribution, order determined by coin flip.} \qquad \qquad \qquad Chuanruo Ning\footnotemark[1] \qquad \qquad \qquad Hao Dong\thanks{Corresponding author}\\
% CFCS, School of CS, PKU \qquad School of EECS, PKU \qquad CFCS, School of CS, PKU \\
% { BAAI \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad BAAI} \\
% { National Key Laboratory for Multimedia Information Processing \quad National Key Laboratory for Multimedia Information Processing}
%  \\ %Peking University\quad \\
% {\tt\normalsize wuruihai@pku.edu.cn \qquad chuanruo@stu.pku.edu.cn \quad hao.dong@pku.edu.cn}
% % \\
% % \\ \\
% % \url{https://hyperplane-lab.github.io/vat-mart}
% }

% \author{Ruihai Wu\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT

\input{tex/abs}

% \vspace{-3mm}

\input{tex/intro}
\input{tex/related}
\input{tex/problem}
\input{tex/method}
\input{tex/exps}
\input{tex/conclu}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{reference}
}

% \clearpage

\appendix

\section{Additional Details of Tasks, Settings and Metrics}


\subsection{Tasks}
We select 2 representative tasks from \textit{DeformableRavens} benchmark: \textbf{cable-ring} and \textbf{cable-ring-notarget}, as well as 2 harder tasks from \textit{SoftGym}: \textbf{SpreadCloth} and \textbf{RopeConfiguration} (we use the shape `S' as the target). 
% The aim of the first two tasks is to show our proposed framework can learn meaningful and abundant information without human-designed policy to . We compare our method with imitation learning in these two tasks, which shows our framework 

\begin{itemize}
 \item 

 (1) For \textbf{cable-ring}, it has a ring-shaped cable with 32 beads. The goal of the robot is to manipulate the cable towards a target zone denoted by a green circular ring in the observation. The maximum convex hull area of the cable-ring and the target cable-ring are the same.
% The termination condition is based on whether the area of the convex hull of the cable
% is above a threshold.
 \item 
(2) For \textbf{cable-ring-notarget}, the setting is the same as \textbf{cable-ring}, except that there is no visible target zone in the observation, so that the goal is to manipulate the cable to a circular ring anywhere on the workbench.
 \item 
(3) For \textbf{SpreadCloth}, we use a square cloth. The cloth is randomly perturbed to a crumpled state. The goal of the robot is to manipulate the crumpled cloth into flat state.
 \item 
(4) For \textbf{RopeConfiguration}, the rope is randomly perturbed to a crumpled state. The goal of the robot is to manipulate the rope into the shape of letter 
`S'.
\end{itemize}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth, trim={0cm, 12cm, 10cm, 0cm}, clip]{figs/demo.pdf}
  \caption{\textbf{Demonstrations of the selected tasks.} Each image shows an observation and a successful state (the cropped sub-images to the bottom right).}
  \label{fig:tasks}
\end{figure}

\subsection{Simulation}
For \textit{DeformableRavens} benchmark, we use the suite of simulated manipulation tasks using PyBullet~\cite{coumans2016pybullet} physics engine and OpenAI GYM~\cite{1606.01540} interfaces. For \textit{SoftGym} benchmark, we use Nvidia FleX physics simulator and the Python interface for a better simulation of cloth and rope.


\subsection{Metrics}
\label{sec:simu_metric}
For \textbf{cable-ring} and \textbf{cable-ring-notarget}, we follow \textit{DeformableRavens} benchmark, when the convex hull area of the ring beads exceeds a thresh $\beta$, the manipulation is judged as a success. We set $\beta$ to be 0.75 for these two tasks, as established in \textit{DeformableRavens}. We use the success rate as manipulation score, which is number of successful manipulation trajectories divided by total number of manipulation trajectories. 

For the \textbf{SpreadCloth} and \textbf{RopeConfiguration}, following \textit{SoftGym} benchmark, we choose the normalized score as manipulation score, which is $\frac{metric_{final}-metric_{initial}}{metric_{goal}-metric_{initial}}$, where $metric_{final}$ means the score of final state, $metric_{initial}$ means the score of initial state and $metric_{goal}$ means the score of target state. Specifically, for \textbf{SpreadCloth}, we use the coverage area of cloth as the measurement, and $metric_{goal}$ is $1.00$. For \textbf{RopeConfiguration}, we use the negative bipartite graph matching distance as the metric, and $metric_{goal}$ is $-0.04$.


During \textbf{testing}, we randomly select 60 (the same number of trials as in \textit{DeformableRavens}) random seeds representing different initial configurations of the objects, conduct experiments and report manipulation score on these different initial states.


\section{Additional Details of Experiments}

\subsection{Data Collection}
We collect 5,000 interactions in cable-related tasks, and 40000 interactions in \textbf{SpreadCloth} and \textbf{RopeConfiguration} for each step. 
% Specifically, for each start state, we sample 10 random interactions, one of which is the reversed actions, and record the corresponding ending states. 
The training of the \textbf{SpreadCloth} and \textbf{RopeConfiguration} needs more data, for the reason that, the states, kinematics and dynamics of these objects are much more complex.

From a starting state, we collect both successful interaction data using the proposed \textit{Fold to Unfold} data collection method, and failure interaction data using a random policy.
Therefore, the trained dense affordance could represent the distribution of diverse results of diverse actions.
Each interaction data contains the actions (picking point and placing point) and results after the action (\emph{e.g.} cloth coverage area for \textbf{SpreadCloth}).


\subsection{Hyper-parameters}
We set batch size to be 20, and use Adam Optimizer~\cite{kingma2014adam} with 0.0001 as the initial learning rate. 
During \textbf{Integrated Systematic Training (IST)} procedure,
we set learning rate to be 0.00005, as the  affordance modules have been trained before, and are only adapted and integrated into a system in this procedure.

    \subsection{Computing Resources}
We use TensorFlow as our Deep Learning framework. 
Each experiment is conducted on an RTX 3090 GPU, and consumes about 20 GB GPU Memory for training. It takes about 12 hours and 6 hours to respectively train the Placing Module and the Picking Module for one step. Besides, the Integrated Systematic Training procedure consumes 6 hours.

\section{Additional Details of Network Architectures}

The Picking Module and the Placing Module both employ Fully Convolutional Networks (FCNs) with the same structure to extract point-level features. Through the FCNs, the feature of the $W \times H \times C$ dimension input sequentially transforms to $W \times H \times 64$, $W \times H \times 64$, $W/2 \times H/2 \times 128$, $W/4 \times H/4 \times 256$, $W/8 \times H/8 \times 512$, $W/16 \times H/16 \times 512$ (bottleneck of the net work, where global feature is extracted), $W/8 \times H8 \times 512$, $W/8 \times H/8 \times 256$, $W/4 \times H/4 \times 256$, $W/4 \times H/4 \times 128$, $W/2 \times H/2 \times 128$, $W/2 \times H/2 \times 256$, $W \times H \times 256$, $W \times H \times 256$.

Afterwards, the Picking Module uses MLPs with hidden sizes to be (256$\to$256, 256$\to$1) to predict picking affordance, and the Picking Module uses MLPs with hidden sizes to be (1024$\to$256, 256$\to$1) to predict placing affordance. Here, 256 denotes the feature dimension of each (picking or placing) point, and 1024 denotes the dimension of the concatenation of the picking point feature (256), the placing point feature (256), and the global feature (512).



\section{Additional Details of Real-world Experiments}

\subsection{Real-robot Settings}

For real-robot experiments, 
we set up one Franka Panda robot on the workbench,
with a RealSense camera mounted on the robot gripper to take observations.
We use Robot Operating System (ROS)~\cite{quigley2009ros} to control the robot to execute actions.

Additionally, as shown in Figure~\ref{fig:gripper}, as the original fingers of Franka Panda is wide and coarse,
to ensure that the gripper can pick only one layer of cloth instead of two layers at a time,
we design two fine-grained fingers mounted on the fingertips of the original fingers.

% \subsection{Domain Randomization}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth, trim={0cm, 0cm, 0cm, 0cm}, clip]{figs/manipulator_merged.png}
  \caption{\textbf{Our Designed Fine-grained Fingers} to better pick deformable objects.}
  \label{fig:gripper}
\end{figure}

\subsection{Real-world Data Collection and Fine-tuning}


As the configurations, kinematics and dynamics of deformable objects (like cloth and ropes) in the real world are different from those in simulation, we fine-tune the trained-in-simulation affordance using real-world collected interactions.


Specifically, following Section 4.5 (\textbf{\textit{Fold to Unfold}: Efficient Multi-stage Data Collection for Learning Foresightful Affordance}) in the main paper,
we collect real-world interactions using the \textit{Fold-to-Unfold} method in different stages (demonstrations shown in Figure~\ref{fig:real-world-data}).
For fine-tuning,
we tune the learned picking and placing affordance stage-by-stage using the above real-world collected data.


\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth, trim={0cm, 0cm, 0cm, 0cm}, clip]{figs/fold_to_unfold_real.pdf}
  \caption{\textbf{Demonstrations of real-world collected data.} State $o_{i+1}$ shows the starting state and state $o_{i}^{\prime}$ show the ending state of interaction data for training.}
  \label{fig:real-world-data}
\end{figure}



\subsection{Metrics}

For \textbf{SpreadCloth}, we use the same metric as in~\ref{sec:simu_metric}, as it is easy to compute the coverage area of the cloth in the real world. 
For \textbf{RopeConfiguration}, we mark a black dot on the rope every 10cm, and compute the distances between these black dots and their ideal locations as the bipartite graph matching distance for further evaluation.


\subsection{Video Records of Real-world Manipulations}

Please see the {\color{red}\textbf{the videos in our project page}} for video records of real-world manipulations for both \textbf{SpreadCloth} and \textbf{RopeConfiguration} tasks.


\section{Assets}
We use the cable assets in \textit{DeformableRavens} benchmark as well as cloth and rope assets in \textit{SoftGym} benchmark, following their licenses. Our proposed assets with novel configurations can be generated using our code.



\end{document}
