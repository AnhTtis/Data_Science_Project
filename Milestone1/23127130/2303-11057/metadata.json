{
    "arxiv_id": "2303.11057",
    "paper_title": "Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation",
    "authors": [
        "Ruihai Wu",
        "Chuanruo Ning",
        "Hao Dong"
    ],
    "submission_date": "2023-03-20",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 2,
    "categories": [
        "cs.CV",
        "cs.RO"
    ],
    "abstract": "Understanding and manipulating deformable objects (e.g., ropes and fabrics) is an essential yet challenging task with broad applications. Difficulties come from complex states and dynamics, diverse configurations and high-dimensional action space of deformable objects. Besides, the manipulation tasks usually require multiple steps to accomplish, and greedy policies may easily lead to local optimal states. Existing studies usually tackle this problem using reinforcement learning or imitating expert demonstrations, with limitations in modeling complex states or requiring hand-crafted expert policies. In this paper, we study deformable object manipulation using dense visual affordance, with generalization towards diverse states, and propose a novel kind of foresightful dense affordance, which avoids local optima by estimating states' values for long-term manipulation. We propose a framework for learning this representation, with novel designs such as multi-stage stable learning and efficient self-supervised data collection without experts. Experiments demonstrate the superiority of our proposed foresightful dense affordance. Project page: https://hyperplane-lab.github.io/DeformableAffordance",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11057v1",
        "http://arxiv.org/pdf/2303.11057v2"
    ],
    "publication_venue": null
}