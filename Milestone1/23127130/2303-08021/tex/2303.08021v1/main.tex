\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022



% ready for submxission
\usepackage[preprint]{neurips_2022}

\usepackage{algorithm}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{newtxtext,newtxmath} % use Times new roman
\usepackage{algpseudocode}
\usepackage{amsmath}

% \title{Classification of ailments based on medical text using deep learning approach and swarm intelligence algorithm}
\title{Optimizing Deep Learning Model Parameters with the Bees Algorithm for Improved Medical Text Classification}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
Mai A. Shaaban$^{1}$ \quad Mariam Kashkash$^{1}$ \quad Maryam Alghfeli$^{1}$ \quad Adham Ibrahim$^{1}$\\
$^1$Mohamed bin Zayed University of Artificial Intelligence\\
\text{\{mai.kassem, mariam.kashkash, maryam.alghfeli, adham.ibrahim\}@mbzuai.ac.ae}\\
}

% \author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
%   % examples of more authors
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \AND
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
% }


\begin{document}


\maketitle


\begin{abstract}
% The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
% both the left- and right-hand margins. Use 10~point type, with a vertical
% spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
% bold, and in point size 12. Two line spaces precede the abstract. The abstract
% must be limited to one paragraph.

This paper introduces a novel mechanism to obtain the optimal parameters of a deep learning model using the Bees Algorithm, which is a recent promising swarm intelligence algorithm. The optimization problem is to maximize the accuracy of classifying ailments based on medical text given the initial hyper-parameters to be adjusted throughout a definite number of iterations. Experiments included two different datasets: English and Arabic. The highest accuracy achieved is 99.63\% on the English dataset using Long Short-Term Memory (LSTM) along with the Bees Algorithm, and 88\% on the Arabic dataset using AraBERT. The source code and datasets are available at \url{https://github.com/Mai-CS/ML701-AilmentClassification}.
\end{abstract}

\section{Introduction}
%  describes the problem
In the recent past, the expansion of the COVID-19 pandemic has reshaped the world radically. Hospitals and medical centers have become fertile ground for the spread of this virus, where patients are in close contact with someone with COVID-19. Social distancing plays a pivotal role in eliminating the spread of this virus \cite{Lotfi2020}. Hence, a new term appeared, which is telemedicine. Telemedicine is consulting patients by physicians remotely via vast communication technologies \cite{Khemapech2019}. However, the doctors' productivity may decrease due to the intense effort required to balance between in-patients and out-patients \cite{Wu2019}. Also, most people try to diagnose themselves by expressing their symptoms in the search engine. Then, they start reading from random unauthorized websites on the internet. On the contrary, this is not safe at all and may lead to the misclassification of the ailment.

A wide variety of deep learning paradigms can be applied to remedy this issue. This project aims to speed up the diagnosis process accurately using Natural Language Processing (NLP) models. The proposed method is run based on English and Arabic data sets. The used English dataset contains more than 6000 text records of variant symptoms along with the type of ailment. Whereas, the Arabic one consists of 152 records. For both datasets, the first step in the proposed work is to perform text preprocessing techniques such as lemmatization, stop words removal, and generating word embeddings. Then, for the English dataset, Long Short-Term Memory (LSTM) deep neural network is suggested to take word embeddings as inputs to predict the output (i.e., the ailment). However, LSTM as a deep learning model suffers from the risk of getting stuck in local optima. This is because the values of weights are initialized randomly. Not only the weights but also their parameters \cite{Alsaleh2021}. The Bees Algorithm (BA) is one of the swarm intelligence algorithms. It is a population-based algorithm as well as it mimics the behavior of bees in foraging in nature \cite{Kashkash2022}. In the proposed work, the Bees Algorithm is used to enhance the process of hyper-parameter tuning of LSTM. For the Arabic dataset, AraBERT is used to classify the aliment.

% ------------------------------------------------------------------------------------------------------------------------------------------
\section{Related Work}
A dynamic deep ensemble model was proposed to classify the text into spam or legitimate by random forests and extremely randomized trees, whereas, features were extracted by convolutional and pooling layers by passing the word embeddings to prevent the manual feature extraction process \cite{Shaaban2022}. This model helped in adjusting the complexity of the model; moreover, the resulting accuracy rate was very high (98.38\%). This review \cite{Harvey2022} gave a summary of the NLP methods, which were used in the diagnosis of Bipolar disorder, as well as, suggested future prospects to apply the NLP in the medical field. An Arab medical dataset was generated with ten classes (Blood, Bone, Cardiovascular, Ear, Endocrine, Eye, Gastrointestinal, Immune, Liver, and Nephrological), in addition, this dataset  was used to validate ABioNER and BERT models in the classification task; the result showed that the first model was better than the second one because it was already trained on Arabic medical corpus \cite{Hammoud2021}.\\
This paper \cite{Xie2022} proposed a supervised model to extract features from text (gathered from social media) and identified the information of each disaster; in addition, it trained a multi-label classifier to classify the disaster of the given text. The suggested model enhanced the semantic representation of the studied text and gave good results in the task of classification of two multi-label disasters. The model of Convolutional Neural Network (CNN) above of word embedding layer was used to classify Arabic text into different categories: art, music, environment, and finance; this model achieved high accuracy for the task of classification of Arabic sentences, as well as, the overfitting was resolved by using dropout layers and l2 weight regularization \cite{Sagheer}. This article \cite{ALHAMOUD2022} tested and compared Short-Term Memory Networks (LSTM), Gated Recurrent Units (GRU), bidirectional GRU, bidirectional LSTM, LSTM with attention, and bidirectional LSTM with attention to analysis the dataset subjectively, however, LSTM was the best and its accuracy was 97.39\%. 

% ------------------------------------------------------------------------------------------------------------------------------------------
\section{Methods}
% a description of the method(s) you tried and dataset 
% Text Augmentation 
\subsection{Text Augmentation}
In light of the fact that the size of the data shrunk after dropping replicates to 706 and 152 data samples in English \cite{Mooney} and Arabic \cite{lichouri2019arabic} datasets respectively, text augmentation is applied to enhance the deep learning model performance and reduce the probability of overfitting. Text augmentation is a prevalent technique used to amplify data samples by generating different versions of the given textual data. Eventually, after applying the proposed method, the size of the data increased to 2829 and 342 data samples in both English and Arabic datasets, respectively. For text augmentation, the nlpaug tool is used \cite{ma2019nlpaug}.

% data preprocessing
\subsection{Exploratory Data Analysis}

Balanced data with no missing values is an essential prerequisite for having a well-generalized model. Data analysis is crucial to identify patterns and extract practical information from the dataset. The objective of this study is to classify ailments; in the English dataset case, we have 25 categorical classes (ailments). Whereas there are seven categorical classes in the Arabic dataset.  Hence, a balanced dataset should contain a relatively close percentage of occurrence for each class. Figure \ref{fig:class distribution} shows that the given dataset is balanced and targets are equally distributed For English. Additionally, we analyzed word frequency in the English dataset as shown in Figure \ref{fig:top words}. Nevertheless, the Arabic dataset was different as it has imbalanced class distribution. As a consequence, after applying data augmentation to the Arabic dataset, it became balanced as shown in Figure  \ref{fig:arabic distribution}. To clarify how these two datasets are used for classifying ailments, Figure \ref{fig:examples} was added to illustrate the medical text accompanied with the prompt (label).

\begin{figure*}[!htb]
\centering
    \includegraphics [scale=0.35]{class_distribution.png}
    \caption{Class distribution of the ailments dataset \cite{Mooney}}
    \label{fig:class distribution}
\end{figure*}

\begin{figure*}[!htb]
\centering
    \includegraphics [scale=0.35]{top10words.png}
    \caption{Top ten words in the English dataset \cite{Mooney}}
    \label{fig:top words}
\end{figure*}

\begin{figure*}[!htb]
\centering
    \includegraphics [scale=0.35]{arabic_categories_aug.png}
    \caption{Class distribution of the Arabic dataset \cite{lichouri2019arabic}}
    \label{fig:arabic distribution}
\end{figure*}

\begin{figure}[!htb]
\begin{subfigure}{.49\linewidth}
  \centering
  % include first image
  \includegraphics[width=.99\linewidth]{english.png}  
  \caption{\centering English example}
  \label{fig:distortion}
\end{subfigure}
\begin{subfigure}{.49\linewidth}
  \centering
  % include first image
  \includegraphics[width=.99\linewidth]{arabic.png}  
  \caption{\centering Arabic example}
  \label{fig:silh}
\end{subfigure}
\caption{\centering Examples from English and Arabic datasets}
\label{fig:examples}
\end{figure}

% word embedding
\subsection{Data Preprocessing}
Converting textual data into digits is one of the main pillars of achieving natural language processing in various capacities. The words must be expressed numerically for machine learning or deep learning algorithms. Various methods are used to assess text corpus transformation into numerals, and each has its advantages and drawbacks. For instance, TF-IDF, one-hot encoding, and Word Embedding. Term Frequency – Inverse Document Frequency (TF-IDF) technique used in text mining to reflect how important a word is to a document in corpus. One-Hot encoding splits a phrase's words into a group and turns each word into a sequence of numbers regardless of meaning within the context \cite{Shaaban2022}. 

Word embeddings differ from previously mentioned techniques in that this approach represents each word by a vector of numbers indicating the semantic similarity between the words. It creates a dense vector by transforming each word into a word vector that reflects its relative meaning within the document. Input is illustrated in a matrix $\mathbf{M} \in \mathbb{R}^{n\times d}$, which denotes a collection of phrases. Each phrase $m$ has a sequence of words: $w_1$,$w_2$,$w_3$, ...,$w_n$; and every word is represented in a word vector of length $d$ \cite{Shaaban2022}. Before applying the word embeddings technique on the English dataset, we performed text preprocessing techniques, which involve tokenization, removal of stop words and lemmatization. First, each text was tokenized into words. Then, we removed stop words, which occur commonly across all the titles. (e.g., "the", "is", "you"). Next, we applied lemmatization for the sake of grouping different forms of the same word.

% LSTM
\subsection{Long Short-Term Memory (LSTM)}
\label{sec:lstm}

% Recurrent Neural Network (RNN) is a variety of Deep Neural Networks (DNN). 
A Deep Learning approach for modelling sequential data is Recurrent Neural Networks (RNN). For sequential data such as text, RNNs help predict what word or phrase will occur after a particular text, which could be a beneficial asset. This approach produces cutting-edge outcomes on text classification problems.

 Long Short-Term Memory Network (LSTM) is a type of RNN where LSTM cell blocks are in place of the standard neural network layers. LSTM models have been shown to be capable of achieving remarkable text classification performance. LSTM cell consists of three different cells, namely the input, the forget and the output gates, which are used to determine which signals can be forwarded to the next node. Figure \ref{fig:lstmcell} illustrates the structure of the LSTM cell. The hidden layer is connected to the input by a weight matrix $U$. $W$ represents the recurrent connection between the previous hidden layer and the current hidden layer. The candidate hidden state $\tilde{C}$ is computed using the current input and the previous hidden state. $C$ denotes the internal memory of the unit. It is a combination of the forget gate, multiplied by the previous memory, and the input gate, multiplied by the newly computed hidden state \cite{Hochreiter19971735}. Equations (\ref{eq:eq1}-~\ref{eq:eq6}) show the detailed workflow of LSTM cell.

\begin{figure*}[!htb]
\centering
    \includegraphics [width=0.5\textwidth]{LSTM_cell}
    \caption{LSTM cell structure}
    \label{fig:lstmcell}
\end{figure*}

\begin{equation}
i_{t} = \sigma \left ( x_{t}U^{i} + h_{t-1}W^{i} \right )
\label{eq:eq1}
\end{equation}

\begin{equation}
f_{t} = \sigma \left ( x_{t}U^{f} + h_{t-1}W^{f} \right )
\label{eq:eq2}
\end{equation}

\begin{equation}
o_{t} = \sigma \left ( x_{t}U^{o} + h_{t-1}W^{o} \right )
\label{eq:eq3}
\end{equation}

\begin{equation}
\tilde{C}_{t} = tanh \left ( x_{t}U^{g} + h_{t-1}W^{g} \right )
\label{eq:eq4}
\end{equation}

\begin{equation}
C_{t} = \sigma \left ( f_{t}*C_{t-1} + i_{t}*\tilde{C}_{t} \right )
\label{eq:eq5}
\end{equation}

\begin{equation}
h_{t} = tanh(C_{t})*o_{t}
\label{eq:eq6}
\end{equation}

% ------------------------------------------------------------------------------------------------------------------------------------------
\subsection{The Bees Algorithm}
The Bees Algorithm (BA) is a swarm intelligence algorithm and a population-based algorithm that mimics the behavior of bees  in nature in order to forage. In the beginning, Scout bees are sent to discover the area. When those bees return, they perform a waggle dance that indicates the quality of the discovered batches. After that, recruiter bees are sent to the good batches to fetch good nectar, which enhances the quality and amount of produced honey.
The BA is started by initializing the population of $n$ bees. After that, the main loop of the BA is started by selecting $m$ good bees to implement the local search to exploit the found solution in order to reach the optimal one. The elite bees $e$ recruit $nep$ bees to help them find better solutions in their neighborhood. While $nsp$ bees are recruited to search in the neighborhood of the remaining good bees. In general, $nep$ should be greater than $nsp$. The remaining bees in the population implement the global search to explore all available solutions. This loop is repeated until convergence. The pseudo-code of the BA is shown in Algorithm \ref{alg:theBA}.   
\begin{algorithm}
\caption{The Bees Algorithm}
\label{alg:theBA}
\begin{algorithmic}[1]

	\State Input: $n$, $m$, $e$, $nep$, $nsp$, stopping criteria      
	\State Output: The optimal bee
	\State Initialize a population of $n$ bees.
	\State Calculate the fitness value for each bee in the population.
	\While{stopping criteria are not satisfied}  
        \State Select $m$ bees for local search.
        \State Recruit $nep$ bees for the elite $e$ bees
        \State Recruit $nsp$ bees for remaining $m-e$ good bees.
        \State Sending the remaining $n-m$ bees for a global search.
        \State Calculate the fitness value for all bees.
        \State Select the best bee as optimal.
    \EndWhile  \label{BA's loop}
\end{algorithmic}
\end{algorithm}
\subsubsection{Hyper-parameter Tuning using the Bees Algorithm}
In this proposed method, the BA is applied to find the optimal values of the number of epochs that the LSTM needs to train and the number of units in LSTM network to enhance the obtained accuracy for the suggested system. Thus, the bee in the proposed method consists of the value of epoch, the number of units and the accuracy value obtained from running LSTM as shown in Figure(\ref{fig:bee}). Considering that the remaining hyper-parameters of LSTM are kept fixed during the experiment.

\begin{figure*}[!htb]
\centering
    \includegraphics [scale=0.75]{bee.png}
    \caption{The formula of the bee in the proposed method }
    \label{fig:bee}
\end{figure*}
The algorithm starts by generating $n$ bees (solutions) randomly as the initial population, which represents $n$ different structures of LSTM. Each parameter is generated randomly using the Uniform distribution function by using the following equations.
\begin{equation}
epoch=uniform(epoch_{low},epoch_{high})
\label{eq:epoch}
\end{equation}
\begin{equation}
unit=uniform(unit_{low},unit_{high})
\label{eq:unit}
\end{equation}
where:
\begin{enumerate}
    \item $epoch_{low}$ and $epoch_{high}$ are the minimum and maximum values that can be assigned to the epoch parameter consequently.
    \item $unit_{low}$ and $unit_{high}$ are the minimum and maximum values that can be assigned to the number of units in LSTM consequently.
\end{enumerate}
After that, the evaluation function is implemented by training the proposed method  for each bee. The evaluation function in the proposed method is the neural network in Section \ref{sec:lstm}, and the evaluation value is the obtained accuracy on the validation set. Next, these bees are ordered descently based on the resulting accuracy value.

The next step is to implement the local search function. The $m$ good bees are selected and the elite $e$ between them are distinguished. There are $nep$, and $nsp$ bees, which are recruited for each bee in the $m$ good bees and elite $e$, respectively, to enhance the found solution. This is performed by generating new values of the epoch and the unit parameters in the neighborhood of original values by using the Uniform distribution function. Equations (\ref{eq:epochNgh}-~\ref{eq:unitNgh}) illustrate the process:
\begin{equation}
epoch_{new}=uniform(epoch_{cur}-ngh,epoch_{cur}+ngh)
\label{eq:epochNgh}
\end{equation}
\begin{equation}
unit_{new}=uniform(unit_{cur}-ngh,unit_{cur}+ngh)
\label{eq:unitNgh}
\end{equation}
where:\\
$epoch_{cur}$ and $unit_{cur}$ are the current values of the current epoch and unit consequently, and $epoch_{new}$ and $unit_{new}$ are the new values of the  epoch and unit. Whereas, $ngh$ is the size of the neighborhood. The new bee is stored in the case of the new accuracy being greater than the accuracy of the original bee, which the local search called for it.

After implementing the local search, the global search is run for the remaining $(n-m)$ to discover new solutions that can be promised. The global search is implemented by replacing each remaining bee $(n-m)$  with a new one using the Equations (\ref{eq:epoch}-~\ref{eq:unit}) as in initializing the population. 
The local search and the global search are repeated until the convergence or the maximum number of iterations of the BA is reached.

\subsection{AraBERT}

Arabic is one of the most widely spoken Semitic languages, with over 467 million native speakers worldwide \cite{Mooney}. It is essential to apply machine learning techniques for analyzing Arabic datasets. As a consequence, we used AraBERT \cite{abdul2020arbert} to classify ailments in the Arabic dataset. It was decided to use AraBERT because it has been trained on a large dataset; hence, it will generalize well when applied to smaller datasets. AraBERT is a pre-trained Arabic language model built on Google's BERT architecture, and it employs the same BERT-Base configuration. 

% \subsection{Naive Bayes}

% Naive Bayes is one of the algorithms used for classification in machine learning, where it classifies data based on Bayes' theorem \ref{eq:NB}. It assumes independence between attributes of data points. It is commonly utilised in many applications like spam filters, sentiment analysis, and real-time predictions. We have implemented the categorical Naive Bayes algorithm by Sklearn, which is used for categorically distributed data. After fitting the augmented data to the Naive Bayes classifier and evaluating it, the resulting accuracy is 57\%.

% \begin{equation}
% P(A|B)=\frac{P(B|A)P(A)}{P(B)}
% \label{eq:NB}
% \end{equation}

% ------------------------------------------------------------------------------------------------------------------------------------------
\section{Experimental Results}
In this section, we explore the ailment classification English dataset \cite{Mooney} that contains 2829 unique text samples after performing data augmentation. First, data preprocessing techniques were applied including tokenization, stop words removal, and lemmatization. Next, textual data were transformed into numerical format using the word embedding technique, where each word is represented by a vector of size 32. Then, we applied 10-fold cross validation along with LSTM to predict the patient's ailment. Finally, we conducted ablation studies and evaluated the model.

 Arabic dataset \cite{lichouri2019arabic} is modest, with only 150 entries and seven classes. The key difficulty is determining how the model will generalize across such a small sample. After increasing data samples to 423 using the data augmentation approach, we applied a pre-trained Arabic language model, such as the AraBERT model, to the Arabic dataset. The AraBERT model has been trained on a huge amount of Arabic data, and only the model needs to be tuned. we used 15 epochs and 1e-5 and 8 batch sizes. Table \ref{tab:arabert results} shows the accuracy over a 15\% test data. Table \ref{tab:CV} shows the cross-validation accuracies over 5 splits.

\subsection{Model Configuration}
Table \ref{tab:configuration} summarizes the configuration of LSTM hyper-parameters: number of units, number of epochs, batch size, etc. Besides, Table \ref{tab:bees parameters} summarizes the values of each parameter of the Bees Algorithm. Moreover, the setup of AraBERT model is concluded in Table \ref{tab:arabert configuration}

\begin{table}[!htb]
  \centering
   \caption{Hyper-parameters setting of LSTM}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Parameter & Value \\
    \midrule
    Units & 64 \\
    Epochs & 20 \\
    Batch size & 10\\
    Dropout & 0.2 \\
    \bottomrule
  \end{tabular}
  \label{tab:configuration}
\end{table}

\begin{table}[!htb]
  \centering
   \caption{The parameters of the Bees Algorithm}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Parameter & Value \\
    \midrule
    Population size $\textbf{(n)}$ & 10  \\
    Good population size $\textbf{(m)}$ & 7  \\
    Elite population size $\textbf{(e)}$ & 3  \\
    Elite bees recruit $\textbf{(nep)}$ & 4  \\
    Good bees recruit $\textbf{(nsp)}$ & 2  \\
    Neighborhood size $\textbf{(ngh)}$ & 1  \\
    \bottomrule
  \end{tabular}
  \label{tab:bees parameters}
\end{table}

\begin{table}[!htb]
  \centering
   \caption{Hyper-parameters setting of AraBERT}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Parameter & Value \\
    \midrule
    Epochs & 15 \\
    Learning rate & 1e-5\\
    Batch size & 8 \\
    \bottomrule
  \end{tabular}
  \label{tab:arabert configuration}
\end{table}

\subsection{Evaluation Metrics}
The performance of LSTM model was evaluated in Table \ref{tab:classification report} based on the following well-known metrics for multi-class classification:

\begin{itemize}
\item[--] $T_{P}$: denotes true positives.
\item[--] $T_{N}$: denotes true negatives.
\item[--] $F_{P}$: denotes false positives.
\item[--] $F_{N}$: denotes false negatives.
\item[--] Precision: the proportion of the sum of true positive samples across all classes divided by the sum of true positive samples and false positive samples across all classes.
    \begin{equation}
        Precision = \frac{T_{P}}{T_{P}+F_{P}}
    \label{eqn:precision}
    \end{equation}
\item[--] Recall: the proportion of the sum of true positive samples across all classes divided by the sum of true positive samples and false negative samples across all classes.
    \begin{equation}
        Recall = \frac{T_{P}}{T_{P}+F_{N}}
    \label{eqn:recall}
    \end{equation}
\item[--] F1-score: is a weighted average of precision and recall.
    \begin{equation}
        F1{\text -}Score = 2\times \frac{Precision\times Recall}{Precision+Recall}
    \label{eqn:f1-score}
    \end{equation}
\item[--] Accuracy: compares the set of predicted labels to the corresponding set of actual labels.
    \begin{equation}
        Accuracy = \frac{T_{P}+T_{N}}{T_{P}+F_{P}+T_{N}+F_{N}}
    \label{eqn:accuracy}
    \end{equation}
\end{itemize}

\begin{table}[!htb]
  \centering
   \caption{Classification results of LSTM with default parameters}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Metric & Average weighted value \\
    \midrule
    Precision & 0.9837 \\
    Recall & 0.9816 \\
    F1-score & 0.9816\\
    Accuracy & 98.16\% \\
    \bottomrule
  \end{tabular}
  \label{tab:classification report}
\end{table}

\begin{figure*}[!htb]
\begin{center}
\includegraphics[width=0.5\textwidth]{curve.png}
\end{center}
   \caption{Training and validation curves of the LSTM model}
\label{fig:curve}
\end{figure*}

As illustrated in Figure~\ref{fig:curve}, the training and validation curves for around $20$ epochs of training using the Adam optimizer \cite{kingma2014adam} produced good fit model. However, results of training more epochs will be explored in Section \ref{sec:studies}.

To get the highest possible accuracy, we used the Bees Algorithm to extract the ideal hyper-parameters for the LSTM model. Table \ref{tab:optimal} indicates that the performance increased significantly when the output dimensionality (the number of units) is adjusted to equal 17 along with running 47 epochs, which in return, increased the accuracy score by 1.45\% compared with the baseline model.

Additionally, results of AraBERT are shown in Table \ref{tab:arabert results} for 15\% test dataset. Also, we utilized the cross-validation technique with 5 validation sets to manage such a small dataset and compare results before splitting as shown in Table \ref{tab:CV}. However, this caused the performance to decline, which may signal overfitting.

\begin{table}[!htb]
  \centering
   \caption{The optimal hyper-parameters for LSTM after applying the Bees Algorithm on the English dataset}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Parameter & Value \\
    \midrule
    Epochs & 47 \\
    Units & 17\\
    Accuracy & 99.63\% \\
    \bottomrule
  \end{tabular}
  \label{tab:optimal}
\end{table}

\begin{table}[!htb]
  \centering
   \caption{Results for AraBERT model on 15\% test data on the Arabic dataset after augmentation}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Metric & Value \\
    \midrule
    Precision & 0.90 \\
    Recall& 0.87\\
    F1-score & 0.86 \\
    Accuracy & 88\%\\
    \bottomrule
  \end{tabular}
\label{tab:arabert results}
\end{table}

\begin{table}[!htb]
  \centering
   \caption{Results for AraBERT model on 5 cross-validation sets}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Split & Accuracy \\
    \midrule
    1 & 0.57\\
    2 & 0.50\\
    3 & 0.54\\
    4 & 0.58\\
    5 & 0.61\\
    \bottomrule
  \end{tabular}
\label{tab:CV}
\end{table}

\subsection{Ablation Studies}
\label{sec:studies}

Table \ref{tab:epochs} shows the effect of increasing the number of epochs from 10 to 40. Nevertheless, there is no significant improvement in the performance of LSTM after 20 epochs. Furthermore, Table \ref{tab:units} shows the outcomes of applying different numbers of LSTM units starting from 32 to 128. However, the best accuracy achieved was 98.19\% by setting the number of units to be 32 along with training 20 epochs.

\begin{table}[!htb]
  \centering
   \caption{Performance of LSTM with different numbers of epochs}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Epochs & Average accuracy\% \\
    \midrule
10 & 96.07\\
20 & 98.16\\
30 & 98.69\\
40 & 98.40\\
    \bottomrule
  \end{tabular}
  \label{tab:epochs}
\end{table}

\begin{table}[!htb]
  \centering
   \caption{Performance of LSTM with different numbers of units}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Units & Average accuracy\% \\
    \midrule
32 & 98.19\\
64 & 98.16\\
128 & 98.09\\
    \bottomrule
  \end{tabular}
  \label{tab:units}
\end{table}

% ------------------------------------------------------------------------------------------------------------------------------------------
\section{Conclusion}
One of the drawbacks of deep learning models is that they require much effort in tuning hyper-parameters. Therefore, the proposed work introduced a novel mechanism in order to obtain the optimal hyper-parameters required for building deep neural networks. This mechanism is the Bees Algorithm\textemdash one of the most recent swarm intelligence algorithms\textemdash adapted to work on Long Short-Term Memory (LSTM) for the aim of classifying ailments based on medical text. Experiments indicated that the Bees Algorithm produced promising results and significantly improved the performance of LSTM. Furthermore, the proposed work considered experiments on two different languages: English and Arabic.
  
%\bibliographystyle{ieeetr}
%\bibliography{refs.bib}
%\usepackage{natbib}
\bibliographystyle{plain}
\bibliography{refs.bib}
\end{document}
