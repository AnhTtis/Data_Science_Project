%File: formatting-instructions-latex-2024.tex
%release 2024.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{booktabs} 
\usepackage{subcaption}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
\nocopyright
% -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\iffalse
\author{
    % %Authors
    % % All authors must be in the same font size and format.
    % Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    % AAAI Style Contributions by Pater Patel Schneider,
    % Sunil Issar,\\
    % J. Scott Penberthy,
    % George Ferguson,
    % Hans Guesgen,
    % Francisco Cruz\equalcontrib,
    % Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
%     %Afiliations
%     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
%     % If you have multiple authors and multiple affiliations
%     % use superscripts in text and roman font to identify them.
%     % For example,

%     % Sunil Issar\textsuperscript{\rm 2}, 
%     % J. Scott Penberthy\textsuperscript{\rm 3}, 
%     % George Ferguson\textsuperscript{\rm 4},
%     % Hans Guesgen\textsuperscript{\rm 5}
%     % Note that the comma should be placed after the superscript

%     1900 Embarcadero Road, Suite 101\\
%     Palo Alto, California 94303-3310 USA\\
%     % email address must be in roman text type, not monospace or sans serif
%     proceedings-questions@aaai.org
% %
% % See more examples next
}
\fi

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{OptBA: Optimizing Hyperparameters with the Bees Algorithm for Improved Medical Text Classification}
\author {
    % Authors
    Mai A. Shaaban\textsuperscript{\rm 1,\rm 2},
    Mariam Kashkash\textsuperscript{\rm 1},
    Maryam Alghfeli\textsuperscript{\rm 1},
    Adham Ibrahim\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Mohamed bin Zayed University of Artificial Intelligence, U.A.E\\
    \textsuperscript{\rm 2}Faculty of Science, Alexandria University, Egypt\\
    \text{\{mai.kassem, mariam.kashkash, maryam.alghfeli, adham.ibrahim\}@mbzuai.ac.ae}
}


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
One of the challenges that artificial intelligence engineers face, specifically in the field of deep learning is obtaining the optimal model hyperparameters. The search for optimal hyperparameters usually hinders the progress of solutions to real-world problems such as healthcare. To overcome this hurdle, the proposed work introduces a novel mechanism called ``OptBA" to automatically fine-tune the hyperparameters of deep learning models by leveraging the Bees Algorithm, which is a recent promising swarm intelligence algorithm. In this paper, the optimization problem of OptBA is to maximize the accuracy in classifying ailments using medical text, where initial hyperparameters are iteratively adjusted by specific criteria. Experimental results demonstrate a noteworthy enhancement in accuracy with approximately 1.4\%. This outcome highlights the effectiveness of the proposed mechanism in addressing the critical issue of hyperparameter optimization and its potential impact on advancing solutions for healthcare and other societal challenges.
% The source code is publicly available at \url{https://github.com/Mai-CS/ML701-AilmentClassification}.
\end{abstract}

\section{Introduction}
%  describes the problem
In the recent past, the expansion of the COVID-19 pandemic has reshaped the world radically. Hospitals and medical centers have become fertile ground for the spread of this virus. Social distancing plays a pivotal role in eliminating the spread of this virus \cite{Lotfi2020}. Hence, a new term appeared, which is telemedicine. Telemedicine is consulting patients by physicians remotely via vast communication technologies \cite{Khemapech2019}. However, the doctors' productivity may decrease due to the intense effort required to balance between in-patients and out-patients \cite{Wu2019}. Also, most people try to diagnose themselves by expressing their symptoms in the search engine. Then, they start reading from random unauthorized websites on the internet. On the contrary, this is not safe at all and may lead to the misclassification of the ailment.

A wide variety of deep learning paradigms are applied to remedy this issue \cite{bakator2018deep}. The aim of this work is to speed up the diagnosis process accurately using natural language processing (NLP) models along with swarm intelligence algorithms such as the Bees Algorithm \cite{pham2006bees}.

The used English dataset \cite{Mooney} contains more than 6000 records of variant symptoms described by patients as free text along with the type of the ailment. The first step in the proposed work is to perform text preprocessing techniques such as lemmatization, stop word removal, and generating word embeddings. Then, a Long Short-Term Memory (LSTM) \cite{yu2019review} deep neural network is suggested to take word embeddings as inputs to predict the output (i.e., the ailment). However, LSTM as a deep learning model suffers from the risk of getting stuck in local optima. This is because the values of weights are initialized randomly. Not only the weights but also the hyperparameters \cite{Alsaleh2021}. In the proposed work, the Bees Algorithm (BA) \cite{pham2006bees} is used to enhance the process of hyperparameter tuning of LSTM. BA is a population-based algorithm that mimics the behavior of bees in foraging in nature \cite{Kashkash2022}. To the best of our knowledge and based on extensive literature review, this work is the first to integrate the Bees Algorithm with deep learning and the first to explore the mentioned English dataset for ailments classification \cite{Mooney}.

% ------------------------------------------------------------------------------------------------------------------------------------------
\section{Related Work}
A dynamic deep ensemble model \cite{Shaaban2022} was proposed to classify text into spam or legitimate. This model used word embeddings as input features to provide semantic relationships among words, which proved to give more accurate results than Term Frequency – Inverse Document Frequency (TF-IDF) features.

The article \cite{ALHAMOUD2022} tested and compared Long Short-Term Memory Networks (LSTM), Gated Recurrent Units (GRU), bidirectional GRU, bidirectional LSTM, LSTM with attention, and bidirectional LSTM with attention, for analysis of political and ideological debate dataset subjectively. The results show that LSTM surpasses other deep learning models with an accuracy score of 97.39\%.

The swarm-based evolutionary algorithms (EA) \cite{piotrowski2017swarm}, in contrast to direct search methods like hill climbing and random walk, operate with a distinctive approach. Rather than relying on a single solution at each iteration, EAs utilize a population of solutions. Consequently, the outcome of each iteration is also a population of solutions. When dealing with an optimization problem that has a single optimum, all members of the EA population are likely to converge towards that single optimal solution. On the other hand, if the optimization problem possesses multiple optimal solutions, an EA can effectively capture and represent these diverse solutions within its final population \cite{pham2006bees}. A novel population-based search technique known as the Bees Algorithm (BA) was introduced in \cite{pham2006bees}. The authors demonstrated that BA is capable of converging to either the maximum or minimum of the objective function, effectively avoiding being trapped at local optima. Moreover, the experimental results showed that BA outperformed other competing methods in terms of speed and accuracy.

In the landscape of hyperparameter optimization frameworks, the authors in this paper \cite{akiba2019optuna} introduced a framework called ``Optuna". They demonstrated the superiority of Optuna's convergence speed, scalability, and ease of integration by leveraging optimization techniques to streamline the hyperparameter optimization process. 

% ------------------------------------------------------------------------------------------------------------------------------------------
\section{Methods}

% data preprocessing
\subsection{Exploratory Data Analysis}

Data analysis is crucial to identify patterns and extract practical information from the dataset. Thus, we conclude the characteristics of the dataset as follows: 1) the dataset has 25 categorical classes (ailments), 2) the dataset contains a relatively close percentage of occurrence for each class as per Fig.~\ref{fig:class distribution}, Hence, no need to handle class imbalance, and 3) the word frequency is demonstrated in Fig.~\ref{fig:top words}, which indicates that the most frequent words mentioned by patients are generic words and not domain-specific such as "feel" and "pain". To clarify how the dataset is used for classifying ailments, Fig.~\ref{fig:examples} is added to illustrate an example of a medical text accompanied by the prompt (label). Furthermore, the dataset contains duplicate records that can lead to biased results. Therefore, these duplicates are dropped. In light of the fact that the size of the data shrunk after dropping replicates to 706 data samples, we apply a text augmentation technique using the \textit{nlpaug} tool \cite{ma2019nlpaug} to enhance the performance and reduce the probability of overfitting. Consequently, the size of the data increased to 2829 text records. Text augmentation is a prevalent technique used to amplify data samples by generating different versions of the given textual data. The mentioned tool randomly swaps the positions of words. Although there are several ways introduced for text augmentation, random swapping is chosen empirically based on the highest accuracy score.

\begin{figure}[!htb]
\centering
    \includegraphics [scale=0.35]{class_distribution.png}
    \caption{Class distribution of the ailments dataset \cite{Mooney}}
    \label{fig:class distribution}
\end{figure}

\begin{figure}[!htb]
\centering
    \includegraphics [scale=0.35]{top10words.png}
    \caption{Top ten words mentioned by patients}
    \label{fig:top words}
\end{figure}

\begin{figure}[!htb]
\centering
    \includegraphics [scale=0.35]{english.png}
    \caption{An example record from the dataset \cite{Mooney}}
    \label{fig:examples}
\end{figure}

% word embedding
\subsection{Data Preprocessing}
Converting textual data into digits is one of the main pillars of achieving natural language processing in various capacities \cite{mikolov2013efficient}. Therefore, the words must be expressed numerically to fit as inputs to deep learning models. Various methods are used to assess text corpus transformation into numerals, and each has its advantages and drawbacks. For instance, Term Frequency – Inverse Document Frequency (TF-IDF), one-hot encoding, and Word Embedding. The TF-IDF technique is used in text mining to reflect how important a word is to a document in corpus. One-Hot encoding splits a phrase's words into a group and turns each word into a sequence of numbers regardless of meaning within the context \cite{Shaaban2022}. 

The word embedding technique, which is the focus of this work, differs from previously mentioned techniques as it represents each word by a vector of numbers indicating the semantic similarity between words. It creates a dense vector by transforming each word into a word vector that reflects its relative meaning within the document. The input is illustrated in a matrix $M \in \mathbb{R}^{n\times d}$, which denotes a collection of phrases. Each phrase $m \in M$ has a sequence of words: $w_1$,$w_2$,$w_3$, ...,$w_n$; and every word is represented in a word vector of length $d$ \cite{Shaaban2022}. Before applying the word embedding technique, we perform text preprocessing techniques, which involve tokenization, removal of stop words and lemmatization. First, each text is tokenized into words. Then, we remove stop words, which occur commonly across all texts. (e.g., "the", "is", "you"). Next, we apply lemmatization for the sake of grouping different forms of the same word.

% LSTM
\subsection{Long Short-Term Memory (LSTM)}

% Recurrent Neural Network (RNN) is a variety of Deep Neural Networks (DNN). 
Recurrent Neural Networks (RNN) is a deep learning approach introduced for modeling sequential data such as text, RNNs help predict what word or phrase will occur after a particular text, which could be a beneficial asset. This approach produces cutting-edge outcomes on text classification problems \cite{sherstinsky2020fundamentals}.

 Long Short-Term Memory (LSTM) is a type of RNN where LSTM cell blocks are in place of the standard neural network layers \cite{sherstinsky2020fundamentals}. LSTM models have been shown to be capable of achieving remarkable text classification performance. LSTM cell consists of three different cells, namely the input, the forget and the output gates, which are used to determine which signals can be forwarded to the next node. Fig.~\ref{fig:lstmcell} illustrates the structure of the LSTM cell. The hidden layer is connected to the input by a weight matrix $U$. $W$ represents the recurrent connection between the previous hidden layer and the current hidden layer. The candidate hidden state $\tilde{C}$ is computed using the current input and the previous hidden state. $C$ denotes the internal memory of the unit. It is a combination of the forget gate, multiplied by the previous memory, and the input gate, multiplied by the newly computed hidden state \cite{Hochreiter19971735}. The three gates: forget, input, and output,  are represented in Fig.~\ref{fig:lstmcell} as follows $f_{t}$, $i_{t}$, and $o_{t}$, respectively. Equations (\ref{eq:eq1}-~\ref{eq:eq6}) show the detailed workflow of LSTM cell.

\begin{figure}[!htb]
\centering
    \includegraphics [width=0.5\textwidth]{LSTM_cell}
    \caption{LSTM cell structure}
    \label{fig:lstmcell}
\end{figure}

\begin{equation}
i_{t} = \sigma \left ( x_{t}U^{i} + h_{t-1}W^{i} \right )
\label{eq:eq1}
\end{equation}

\begin{equation}
f_{t} = \sigma \left ( x_{t}U^{f} + h_{t-1}W^{f} \right )
\label{eq:eq2}
\end{equation}

\begin{equation}
o_{t} = \sigma \left ( x_{t}U^{o} + h_{t-1}W^{o} \right )
\label{eq:eq3}
\end{equation}

\begin{equation}
\tilde{C}_{t} = tanh \left ( x_{t}U^{g} + h_{t-1}W^{g} \right )
\label{eq:eq4}
\end{equation}

\begin{equation}
C_{t} = \sigma \left ( f_{t}*C_{t-1} + i_{t}*\tilde{C}_{t} \right )
\label{eq:eq5}
\end{equation}

\begin{equation}
h_{t} = tanh(C_{t})*o_{t}
\label{eq:eq6}
\end{equation}

% ------------------------------------------------------------------------------------------------------------------------------------------
\subsection{The Bees Algorithm}
The Bees Algorithm (BA) is a swarm intelligence algorithm and a population-based algorithm that mimics the behavior of honey bees in nature in order to forage \cite{pham2006bees}. In the beginning, scout bees are sent to discover the area. When those bees return, they perform a waggle dance that indicates the quality of the discovered batches. After that, recruiter bees are sent to the good batches to fetch good nectar, which enhances the quality and amount of produced honey.
BA is started by initializing the population of $n$ number of bees. After that, the main loop of BA is started by selecting $m$ good bees out of $n$ bees to implement the local search. The local search exploits the found solutions in order to reach the optimal one. The elite bees $e$ are selected out of $m$ bees and they recruit $nep$ bees to help them find better solutions in their neighborhood. While $nsp$ bees are recruited to search in the neighborhood of the remaining good bees ($m-e$). In general, $nep$ should be greater than $nsp$. The remaining bees in the population implement the global search to explore all available solutions. This loop is repeated until convergence. The pseudo-code of BA is shown in Algorithm~\ref{alg:theBA}.   

\begin{algorithm}
\caption{The Bees Algorithm}
\label{alg:theBA}
\begin{algorithmic}[1]
    
	\State Input: $n$, $m$, $e$, $nep$, $nsp$, stopping criteria      
	\State Output: The optimal bee
	\State Initialize a population of $n$ bees.
	\State Calculate the fitness value for each bee in the population.
	\While{stopping criteria are not satisfied} 
        \State Sort bees with respect to their fitness values. 
        \State Select $m$ good bees for local search.
        \State Select $e$ elite bees out of $m$ bees for local search.
        \State Recruit $nep$ bees for each of $e$ bees.
        \State Recruit $nsp$ bees for each of the remaining $m-e$ bees.
        \State Send the remaining $n-m$ bees for global search.
        \State Calculate the fitness value for all bees.
        \State Select the best bee as optimal.
    \EndWhile
\end{algorithmic}
\end{algorithm}

\subsubsection{Hyperparameter Tuning using the Bees Algorithm}
In this section, we introduce a novel framework called "OptBA", in which BA is applied to find the optimal values of LSTM hyperparameters: the number of epochs that the LSTM model needs to train and the number of units in LSTM layer. Thus, the structure of the bee in OptBA consists of the value of the number of epochs, the number of units and the accuracy value obtained from running LSTM as shown in Fig.~\ref{fig:bee}. Considering that the remaining hyperparameters of LSTM are kept fixed during the experiment.

\begin{figure}[!htb]
\centering
    \includegraphics [scale=0.75]{bee.png}
    \caption{The formula of the bee in the proposed method. Each bee has three components: the two hyperparameters that are randomly chosen and one fitness value that is calculated}
    \label{fig:bee}
\end{figure}
The algorithm starts by generating $n$ bees (solutions) randomly as the initial population, which represents $n$ different structures of LSTM. Each parameter is generated randomly using the Uniform distribution function by using the following equations.
\begin{equation}
epoch=uniform(epoch_{low},epoch_{high})
\label{eq:epoch}
\end{equation}
\begin{equation}
unit=uniform(unit_{low},unit_{high})
\label{eq:unit}
\end{equation}
where:
\begin{enumerate}
    \item $epoch_{low}$ and $epoch_{high}$ are the minimum and maximum values that can be assigned to the epoch parameter.
    \item $unit_{low}$ and $unit_{high}$ are the minimum and maximum values that can be assigned to the number of units in LSTM.
\end{enumerate}
After that, the evaluation function is implemented by training LSTM for each bee and the evaluation value is the obtained accuracy on the validation set. Next, these bees are ordered decently based on the resulting accuracy value. Then, the $m$ good bees are selected and the elite $e$ between them are distinguished. There are $nep$, and $nsp$ bees, which are recruited for each bee in the $m$ good bees and elite $e$, respectively, to enhance the found solution. This is performed by generating new values of the epoch and the unit parameters in the neighborhood of original values by using the Uniform distribution function. Equations (\ref{eq:epochNgh}-~\ref{eq:unitNgh}) illustrate the process:
\begin{equation}
epoch_{new}=uniform(epoch_{cur}-ngh,epoch_{cur}+ngh)
\label{eq:epochNgh}
\end{equation}
\begin{equation}
unit_{new}=uniform(unit_{cur}-ngh,unit_{cur}+ngh)
\label{eq:unitNgh}
\end{equation}
where:\\
$epoch_{cur}$ and $unit_{cur}$ are the current values of the current epoch and unit, and $epoch_{new}$ and $unit_{new}$ are the new values of the  epoch and unit. Whereas, $ngh$ is the size of the neighborhood. The new bee is stored in the case of the new accuracy being greater than the accuracy of the original bee, which is the main function of the local search.

After implementing the local search, the global search is run for the remaining $(n-m)$ to discover new solutions that can be promised. The global search is implemented by replacing each remaining bee $(n-m)$  with a new one using the Equations (\ref{eq:epoch}-~\ref{eq:unit}) as in initializing the population. Finally, the local search and the global search are repeated until the convergence or the maximum number of iterations of BA is reached. The detailed implementation of OptBA is in Algorithm~\ref{alg:OptBA}.   

\begin{algorithm}
\caption{The OptBA Algorithm}
\label{alg:OptBA}
\begin{algorithmic}[1]

	\State Input: $n$, $m$, $e$, $nep$, $nsp$, stopping criteria, word embeddings      
	\State Output: The optimal no. epochs and no. units
	\State Initialize a population of $n$ LSTM models.
	\State Train and evaluate each LSTM in the population.
	\While{stopping criteria are not satisfied}  
        \State Sort decently all models w.r.t their accuracy values.
        \State Select $m$ good models for local search.
        \State Select $e$ elite models for local search.
        \State Recruit $nep$ models for each of $e$ models.
        \State Recruit $nsp$ models for the remaining $m-e$ models.
        \State Send the remaining $n-m$ models for global search.
        \State Calculate the accuracy value for all models.
        \State Select the best model as optimal.
    \EndWhile
\end{algorithmic}
\end{algorithm}

% ------------------------------------------------------------------------------------------------------------------------------------------
\section{Results and Discussion}
In this section, we explore the ailment classification dataset \cite{Mooney} that contains 2829 unique text samples after performing data augmentation. First, data preprocessing techniques were applied including tokenization, stop words removal, and lemmatization. Next, textual data were transformed into a numerical format using the word embedding technique, where each word is represented by a vector of size 32. Then, we applied 10-fold cross-validation along with LSTM to predict the patient's ailment. Additionally, we conduct ablation studies to justify the importance of tuning hyperparameters. Finally, we compare the model with SOTA. All experiments were run using Quadro RTX 6000 GPU with 24GB.

\subsection{Model Configuration}
Tab.~\ref{tab:configuration} summarizes the configuration of LSTM hyperparameters: number of units, number of epochs, batch size, etc. Besides, Tab.~\ref{tab:bees parameters} summarizes the values of each parameter of OptBA.

\begin{table}[!htb]
  \centering
   \caption{Default hyperparameters setting of LSTM}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Parameter & Value \\
    \midrule
    No. units & 64 \\
    No. epochs & 20 \\
    Batch size & 10\\
    Dropout & 0.2 \\
    \bottomrule
  \end{tabular}
  \label{tab:configuration}
\end{table}

\begin{table}[!htb]
  \centering
   \caption{Settings of OptBA}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Parameter & Value \\
    \midrule
    Population size $n$ & 10  \\
    Good population size $m$ & 7  \\
    Elite population size $e$ & 3  \\
    Elite bees recruit $nep$ & 4  \\
    Good bees recruit $nsp$ & 2  \\
    Neighborhood size $ngh$ & 1  \\
    \bottomrule
  \end{tabular}
  \label{tab:bees parameters}
\end{table}

\subsection{Evaluation Metrics}
The performance of LSTM model was evaluated in Tab.~\ref{tab:classification report} based on the following well-known metrics for multi-class classification:

\begin{itemize}
\item[--] $T_{P}$: denotes true positives.
\item[--] $T_{N}$: denotes true negatives.
\item[--] $F_{P}$: denotes false positives.
\item[--] $F_{N}$: denotes false negatives.
\item[--] Precision: the proportion of the sum of true positive samples across all classes divided by the sum of true positive samples and false positive samples across all classes.
    \begin{equation}
        Precision = \frac{T_{P}}{T_{P}+F_{P}}
    \label{eqn:precision}
    \end{equation}
\item[--] Recall: the proportion of the sum of true positive samples across all classes divided by the sum of true positive samples and false negative samples across all classes.
    \begin{equation}
        Recall = \frac{T_{P}}{T_{P}+F_{N}}
    \label{eqn:recall}
    \end{equation}
\item[--] F1-score: is a weighted average of precision and recall.
    \begin{equation}
        F1{\text -}Score = 2\times \frac{Precision\times Recall}{Precision+Recall}
    \label{eqn:f1-score}
    \end{equation}
\item[--] Accuracy: compares the set of predicted labels to the corresponding set of actual labels.
    \begin{equation}
        Accuracy = \frac{T_{P}+T_{N}}{T_{P}+F_{P}+T_{N}+F_{N}}
    \label{eqn:accuracy}
    \end{equation}
\end{itemize}

\begin{table}[!htb]
  \centering
   \caption{Classification results of LSTM with default hyperparameters}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Metric & Average weighted value \\
    \midrule
    Precision & 0.9837 \\
    Recall & 0.9816 \\
    F1-score & 0.9816\\
    Accuracy & 98.19\% \\
    \bottomrule
  \end{tabular}
  \label{tab:classification report}
\end{table}

\begin{figure}[!htb]
\begin{center}
\includegraphics[width=0.8\columnwidth]{curve.png}
\end{center}
   \caption{Training and validation curves of the LSTM model}
\label{fig:curve}
\end{figure}

As illustrated in Fig.~\ref{fig:curve}, the training and validation curves for around $20$ epochs of training using the Adam optimizer \cite{kingma2014adam} produced a well-generalized model.

To get the highest possible accuracy, we implemented OptBA to acquire the ideal hyperparameters for the LSTM model. Tab.~\ref{tab:optimal} indicates that the performance increased significantly when the output dimensionality (the number of units) is adjusted to 108 along with running 49 epochs, which in return, increased the accuracy score by approximately 1.4\% compared with the baseline model.

\begin{table}[!htb]
  \centering
   \caption{The optimal hyperparameters for LSTM after running OptBA}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Parameter & Value \\
    \midrule
    No. epochs & 49 \\
    No. units & 108\\
    Accuracy & 99.63\% \\
    \bottomrule
  \end{tabular}
  \label{tab:optimal}
\end{table}

\begin{table}[!htbp]
  \centering
   \caption{Performance of LSTM with different numbers of epochs}
  \begin{tabular}{@{}lc@{}}
    \toprule
    No. epochs & Average accuracy\% \\
    \midrule
10 & 96.07\\
20 & 98.16\\
30 & 98.69\\
40 & 98.40\\
    \bottomrule
  \end{tabular}
  \label{tab:epochs}
\end{table}

\begin{table}[!htbp]
  \centering
   \caption{Performance of LSTM with different numbers of units}
  \begin{tabular}{@{}lc@{}}
    \toprule
    No. units & Average accuracy\% \\
    \midrule
32 & 98.19\\
64 & 98.16\\
128 & 98.09\\
    \bottomrule
  \end{tabular}
  \label{tab:units}
\end{table}

\begin{table*}[!tb]
  \centering
   \caption{Performance evaluation of Optuna and OptBA}
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    Framework & Initial accuracy\% & Best accuracy\% & Best no. epochs & Best no. units & Parallel pruning & Profound search\\
    \midrule
    Optuna & 95.95 & 99.26 & 47 & 94 & No & No\\
    OptBA (ours) & 99.63 & 99.63 & 49 & 108 & Yes & Yes\\
    \bottomrule
  \end{tabular}
  \label{tab:optuna_optba}
\end{table*}

\subsection{Ablation Studies}
Tab.~\ref{tab:epochs} shows the effect of increasing the number of epochs manually from 10 to 40. Nevertheless, there is no significant improvement in the performance of LSTM after 20 epochs with 64 units, unlike the case of OptBA. Furthermore, Tab.~\ref{tab:units} shows the outcomes of applying different numbers of LSTM units starting from 32 to 128. However, the best accuracy achieved was 98.19\% by setting the number of units to 32 while fixing the number of training epochs to 20.

\subsection{Comparison with Optuna}
The architecture and the optimization techniques implemented by Optuna \cite{akiba2019optuna} can generate one optimal solution per trial. In contrast, OptBA employs an optimization method that is a population-based search, allowing the pruning of suboptimal solutions to occur concurrently for accelerated convergence. However, direct comparison of total execution time is not feasible due to distinct numbers of solutions per trial and variations in search criteria between the two frameworks. For instance, OptBA swiftly identified the best solution in its initial trial and terminated early, whereas Optuna continued for 100 iterations without achieving the optimal outcome, as detailed in Tab~\ref{tab:optuna_optba}. Moreover, the parameters of OptBA offer enhanced flexibility and depth in the quest for optimal solutions, without compromising the speed of evaluating an individual trial, but may require a larger number of trials. For example, setting $ngh=1$ entails a search extending one step forward and backward from the current optimal solution. The lower the $ngh$ value, the more profound search space. Consequently, OptBA guarantees to find the global optimal solution, distinguishing it from Optuna, which can get stuck in local optima as depicted in Tab.~\ref{tab:optuna_optba}.

% ------------------------------------------------------------------------------------------------------------------------------------------
\section{Conclusion}
One of the drawbacks of deep learning models is that they require much effort in tuning hyperparameters. Therefore, the proposed work introduces a novel mechanism in order to obtain the optimal hyperparameters required for building deep neural networks. This mechanism utilizes the Bees Algorithm\textemdash one of the recent swarm intelligence algorithms that is adapted to work on Long Short-Term Memory (LSTM) for the aim of classifying ailments based on medical text. Experiments indicate that the Bees Algorithm can produce promising results and significantly improve the performance of deep neural networks. For future research, this work can be extended to explore other datasets as well as other deep learning models.

\bibliography{main}


\end{document}
