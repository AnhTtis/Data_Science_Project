\label{sec:methods}
The experiments presented in this paper were emulated on the latest version of the \gls{bss2} system \citep{pehle2022brainscales2_nopreprint_nourl, billaudelle2022accurate}.
We used the PyNN domain-specific language \citep{davison2009pynn} to formulate the experiments and the \gls{bss2} OS to define and control the experiments \citep{mueller2020bss2ll_nourl}.

\begin{figure}
	\tikzsetnextfilename{exponential}
	\begin{tikzpicture}
		\coordinate (a) at (0,0);
		\node[panel] at (a) {\importpgf{exponential_fit}};
	\end{tikzpicture}
	\caption{Exponential fit to the traces displayed in \figTracesRelative{} and \figTracesAbsolute{}.
			 The heights of the \glsfmtfullpl{psp} are extracted from the recorded membrane traces, compare \cref{fig:evaluation}, and exponentials (solid lines) are fitted to the measurement points.
			 The numbering is the same as in \cref{fig:2d}.
			 The x-axis label mark the compartment in which the height of the \glsfmttext{psp} was measured and in brackets the varibale name as defined in \cref{fig:evaluation}.
			 }
	\label{fig:exponential}
\end{figure}

\subsection{Neuron Parameters}
In order to ensure a similar behavior of the different compartments, the leak potential and the synaptic properties were calibrated.
The synaptic time constant was calibrated to a value of \SI{10}{\us}.
As can be extracted from \figGridsearch{}, the decay constant varied in our experiments between \numrange{0.16}{4.08} compartments.
When varying the leak conductance $g_\text{leak}$ over the full range specified in \cref{fig:2d}, the membrane time constant $\tau_\text{m} = \frac{C_\text{m}}{g_\text{leak}}$ varies in the range of \SIrange{12}{30}{\us}.

\subsection{Predictive Posterior Check}
We used \glspl{ppc} for checking if an approximated posterior $p\left( \myvec{\theta} \mid \myvec{x}^* \right)$ yields parameters $\myvec{\theta}$ which are in agreement with the original observation $\myvec{x}^*$.
As discussed in \citep{lueckmann2021benchmarking} \glspl{ppc} do not measure the similarity of the approximated to the true posterior and should just be used as a check rather than a metric.
Nevertheless, we found that \gls{ppc} were sensitive enough to highlight posterior approximations which did not agree with our expectation of the posterior based on the grid search results.

For all \glspl{ppc} we drew \num{1000} random parameters $\{\myvec{\theta}_i\}$ from the approximated posterior $p\left( \myvec{\theta} \mid \myvec{x}^* \right)$, emulated the chain model with these parameters on \gls{bss2} and recorded the observables $\{\myvec{x}_i\}$.
We evaluated how the hyperparameters of the \gls{snpe} algorithm influence the approximation success by comparing the mean Euclidean distance between these observations and the target observation $\myvec{x}^*$.


\subsection{Sequential Neural Posterior Estimator Algorithm}\label{sec::meth:snpe}
We use the \gls{snpe} algorithm proposed in \citep{greenberg2019automatic} to find an approximation of the posterior distribution, compare \figSbi.
All our experiments were performed with the implementation provided in the \texttt{python} package \texttt{sbi}\footnote{\url{https://github.com/mackelab/sbi}} in version \texttt{0.21.0} \citep{tejero2020sbi}.

We adjusted the number of simulations as well as the properties of the \gls{nde} and used \glspl{ppc} to check how these hyperparameters influence the approximated posterior.
For each set of hyperparameters we executed the \gls{snpe} algorithm ten times with different seeds.
The seeds influence the initial weights as well as the parameters $\myvec{\theta}$ which are drawn from the prior in the first round.
Different sets of hyperparameters shared the same seeds.

\begin{figure}
	\tikzsetnextfilename{simulations}
	\newlength{\heightpcc}
	\setlength{\heightpcc}{1.5in}

	\begin{tikzpicture}
		\coordinate (a) at (0,0);
		\node[panel] at (a) {\importpgf{ppc_vs_sim}};
		\node[fig label] at (a) {A};
		\coordinate (b) at (0, -\heightpcc);
		\node[panel] at (b) {\importpgf{posterior_evolution}};
		\node[fig label] at (b) {B};
	\end{tikzpicture}
	\caption{
		Evolution of the approximated posterior over several rounds of the \glsfmtfirst{snpe} algorithm.
		Results are shown for emulations executed on the \glsfmtlong{bss2} system.
		\subcaption{A} \Glsfmtfirst{ppc} for a emulation budget of \num{10} rounds with \num{50} emulations in each round (the \glsfmttext{ppc} was executed with \num{1000} parameters sampled from the posteriors).
			The \glsfmttext{snpe} algorithm was executed \num{10} times with different seeds.
			For some executions of the \glsfmttext{snpe} algorithm, the approximated posterior in the first round poorly replicates observations which are similar to $\myvec{x}^*$; this is evident in a high mean distance $E$.
			In all displayed cases the \glsfmttext{snpe} algorithm is able to recover a meaningful posterior.
		\subcaption{B} Examples for one case where \glsfmttext{snpe} algorithm is able to approximate a meaningful posterior and one case in which the algorithm fails to find a good approximation in the frist three rounds.
			In both cases, the approximation in the first round does not agree with true posterior.
			In the top row, the algorithm is able to quickly recover from the poor approximation while in the bottom row more rounds are needed to obtain a meaningful approximation.
			The parameter ranges are the same as in \figPosterior{}.
	}
	\label{fig:simulations}
\end{figure}

\subsubsection{Number of Simulations and Rounds}
For the two-dimensional parameter space and the decay constant $\tau$ as an observable, three times \num{50} emulations were sufficient to recover a posterior which is in agreement with the target observation.

When the observable is changed to the height of the \glspl{psp} which result from an input to the first compartment $\myvec{F}$, the \gls{snpe} algorithm failed to find a suitable approximation if the number of emulations was too low.
This was due to a poor approximation in the first round from which the algorithm was not always able to recover, \cref{fig:simulations}.
We observed that a higher number of emulations in the first round reduced the number of cases where the posterior was approximated poorly.
Therefore, we choose \num{500} emulations in the first round followed by ten rounds of \num{50} emulations for a two-dimensional parameter space with $\myvec{F}$ as an observable.

\begin{figure}
	\tikzsetnextfilename{transforms}
	\begin{tikzpicture}
		\coordinate (a) at (0,0);
		\node[panel] at (a) {\importpgf{ppc_vs_trans}};
	\end{tikzpicture}
	\caption{Influence of the parameterization of the \glsfmtfirst{nde} on the approximation of the posterior.
		We use \glsfmtfirstpl{maf} as \glsfmtshortpl{nde}.
		\glsfmtshortpl{maf} transform normal distributions in other distributions \citep{papamakarios2017masked}.
		We use transformations which are made up of two blocks and change the number of hidden units which are used in each block \citep{goncalves2020training}.
		Furthermore, we change the number of transformations which are chained together.
		As in \figPosteriorEvo{} we perform a \glsfmtlong{ppc} and use the mean distance between these samples and the target as a measure to decide if the approximation agrees with the target observation $\myvec{x}^*$.
		Again, we use the \glsfmtlong{psp} heights resulting from an input to the first compartment as an observable and repeat the \glsfmtlong{snpe} algorithm with \num{10} different seeds for each set of hyperparameters.
		At least two transformation are needed to recover a meaningful posterior.
		The number of experiments in which a meaningful posterior can be recovered seems to increase with the number of transformations.
		The total number of trainable parameters is not an indicator how well the \glsfmttext{nde} is able to approximate the true posterior.
	}
	\label{fig:transforms}
\end{figure}

\subsubsection{Neural Density Estimator}
Based on the results in \citep{lueckmann2021benchmarking} we use \glspl{maf} as \glspl{nde} \citep{papamakarios2017masked}.
\Glspl{maf} transform normal distributions in other probability distributions.
We used the values provided by the \texttt{sbi} package \citep{tejero2020sbi} as defaults; similar values have also been used in previous publications \citep{lueckmann2021benchmarking, goncalves2020training}.
Here the \gls{maf} is made up of five transformations which are chained together.
Each of these transformation consists of two blocks with \num{50} hidden neuron per block.
For more information see \citep{papamakarios2017masked,papamakarios2021normalizing}.

In case of a two-dimensional parameter space and the decay constant $\tau$ as a target, \cref{sec:2d}, a single transformation with two blocks of ten hidden units each was sufficient.
If we selected the heights which result from an input to the first compartment $\myvec{F}$ as a target, a single transformation was not sufficient to recover a meaningful posterior, \cref{fig:transforms}.
Starting from two transformations and \num{30} hidden units, the best value of the \gls{ppc} were obtained.
The only exception is the network with three transformations and \num{20} hidden units for which the algorithm could not recover from a poor approximation in the first round.

A \gls{maf} with one transformation and \num{50} is made up of \num{3764} trainable parameters and fails to approximate the true posterior.
On the other hand a \gls{maf} with five transformations and \num{10} hidden units in each block offers just \num{1720} trainable parameters but is able to find approximations which agree with the target observation.
We conclude, that a high number of transformations is more important for a good posterior approximation than a high number of trainable parameters.
For the results reported in \figSamples{} we used the \gls{nde} with five transformations, two blocks and ten hidden units.

\subsection{Choice of the Target Parameters}

\begin{figure}
	\tikzsetnextfilename{amortized}
	\begin{tikzpicture}
		\coordinate (a) at (0,0);
		\node[panel] at (a) {\importpgf{posterior_samples_amortized}};
	\end{tikzpicture}
	\caption{Posterior samples $\{\myvec{\theta}_j\}_i \sim p(\myvec{\theta} \mid \myvec{x}^*_i)$ for different observations $\myvec{x}^*_j$.
			We draw five random parameters ${\myvec{\theta}_i}$ from a uniform prior and one parameter at the center of the parameter space (marked by black crosses).
			The target observations $\{\myvec{x}^*_i\}$ were obtained by emulating the model \num{100} times for each parameter on \glsfmtlong{bss2} and taking the mean height of the \glsfmtlong{psp} obtained from an input to the first compartment, compare \figSamples.
			As a posterior approximation we used the first round posterior obtained while executing the \glsfmtlong{snpe} algorithm in \cref{sec:2d-sbi}.
			The samples drawn from the approximated posterior (small dots) are in the vicinity of the parameters which were used to create the target observations (black crosses).}
	\label{fig:amortized}
\end{figure}

We chose a target parameter $\myvec{\theta}^*$ at the center of the parameter space to measure target observations $\myvec{x}^*$.
For the experiment with the two-dimensional parameter space and the \gls{psp} heights for an input to the first compartment, we want to show that the approximated posterior is also appropriate for other choices of the target parameter $\myvec{\theta}^*$.
As mentioned in the introduction, the posterior estimation is amortized after the first round of \gls{snpe} and can therefore be used to infer parameters $\myvec{\theta}$ for any observation $\myvec{x}$.
We draw five random parameters $\{\myvec{\theta}^*_i\}$ from the uniform prior and emulate the model on \gls{bss2} with the given parameters to record observations $\{\myvec{x}^*_i\}$.
For each of these observations, we draw samples from the amortized posterior estimation $\myvec{\theta} \sim p(\myvec{\theta} \mid \myvec{x}^*_i)$, \cref{fig:amortized}.

For each of the randomly selected observations $\myvec{x}^*_i$ the drawn samples cluster around the parameters which were used to obtain the given observation $\myvec{\theta}^*_i$.
Even if the target parameters are at the edge of the parameter space, the approximated posterior returns samples near these target parameters.
Therefore, we conclude that the \gls{snpe} algorithm is suitable to find parameters for observations which were obtained for parameters at arbitrary locations in the parameter space and that our choice of target parameters $\myvec{\theta}^*$ at the center of the parameter space does not affect the generality of the reported results.

\subsection{Simulations}

\begin{figure*}
	\newlength{\widthsim}
	\setlength{\widthsim}{0.333333\doublecolumn}
	\newlength{\heightsim}
	\setlength{\heightsim}{2in}

	\begin{tikzpicture}
		\coordinate (a) at (0,0);
		\node[panel] at (a) {\importpgf{grid_search_arbor}};
		\node[fig label] at (a) {A};
		\coordinate (b) at (0, -\heightsim);
		\node[panel] at (b) {\importpgf{traces_arbor_scaled}};
		\node[fig label] at (b) {B};

		\coordinate (c) at (\widthsim, 0);
		\node[panel] at (c) {\importpgf{posterior_arbor}};
		\node[fig label] at (c) {C};

		\coordinate (d) at (2\widthsim, 0);
		\node[panel] at (d) {\importpgf{posterior_samples_arbor}};
		\node[fig label] at (d) {D};
		\coordinate (e) at (2\widthsim, -\heightsim);
		\node[panel] at (e) {\importpgf{traces_arbor}};
		\node[fig label] at (e) {E};
	\end{tikzpicture}
	\caption{
		Propagation of \glsfmtfirstpl{psp} in a passive chain of four compartments simulated in Arbor.
		We performed the same experiments as in \cref{fig:2d} and we follow the structure of this figure.
		\subcaption{A} Grid search of the decay constant $\tau$.
			For the chosen parameter range the decay constant is slightly higher than on \gls{bss2}.
			The dependency on the leak conductance $g_\text{leak}$ and the axial conductance $g_\text{axial}$ is comparable to \figGridsearch{}.
		\subcaption{B} Example traces recorded at different locations in the parameter space, compare panel A.
			The traces are scaled relative to the height in the first compartment $h_{00}$.
		\subcaption{C} Posterior obtained with the \glsfmtlong{snpe} algorithm.
			While the shape of the approximated posterior is comparable to the one in \figPosterior{}, the approximated posterior for the simulations is narrower.
		\subcaption{D} \num{500} random samples drawn from the approximated posteriors for two different types of observations.
		The distribution of the random samples is comparable to the results in \figSamples{}, but in agreement with the narrower posterior in panel C, the distribution of the samples is more narrow.
		\subcaption{E} Same traces as in panel B but shown on an absolute scale.
	}
	\label{fig:arbor}
\end{figure*}

We used the Arbor simulation library (version \texttt{0.8.1}) to compare our results to computer simulations \citep{abi2019arbor}.
We simulated a chain with four compartments.
The length of a single compartment was set to $l_\text{comp}=\SI{1}{\milli\meter}$, its diameter to $d_\text{comp}=\SI{4}{\micro\meter}$ and its capacitance to $C=\SI{125}{\pico\farad}$.
While the length and diameter are chosen arbitrarily, the capacitance reflects the capacitance of the compartments used during the emulation on \gls{bss2}.
The range of the leak conductance $g_\text{leak}$ is chosen such that the membrane time constant of the simulated neurons is in agreement with the emulated neurons on \gls{bss2}.
Similarly, the range of the axial conductance $g_\text{axial}$ was chosen such that the axial conductance along a simulated compartment is comparable to the conductance between compartments on \gls{bss2}.

The chosen parameter ranges lead to a slightly higher decay conductance, compare \cref{fig:arbor}.
Apart from that the behavior of the decay constant is in agreement with the emulations presented in \cref{sec:2d}.

The shapes of the approximated posteriors also agree with the results obtained for simulations on \gls{bss2}.
But due to the temporal noise on \gls{bss2}, the approximated posterior distribution for the simulation are narrower than the approximation for \gls{bss2}.
