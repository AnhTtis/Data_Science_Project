\section{Introduction}\label{sec:introduction}

Mechanistic models, which try to explain the causality between inputs and outputs, are integral to scientific research.
On the one hand they can increase the understanding of the mechanisms which cause the phenomena and on the other make predictions about new outcomes which can then be tested experimentally \citep{baker2018mechanistic}.
After a mechanistic model has been formulated, one of the remaining challenges is to find suitable model parameters which lead to a close agreement between the model behavior and the experimental observation.

Several approaches such as the hand-tuning of parameters, grid searches, random/stochastic search, evolutionary algorithms, simulated annealing and particle swarm algorithms have been used in neuroscience to find appropriate model parameters \citep{vanier1999comparative, vangeit2008automated}.
Drawbacks of these methods are that they rely on a score which represents how close the results of a simulated model are to the target observation and that they in general only yield the best performing set of parameters.
Furthermore, these algorithms are often computationally expensive since they require many simulations to find suitable parameters \citep{goncalves2020training}.

The class of \gls{abc} algorithms makes statistical inference methods available for models where the likelihood is not tractable and allows finding an approximation of the posterior distribution of the model parameters.
Advantages of deriving an approximation of the posterior include the possibility to find correlations between model parameters and to classify the confidence in the estimated parameters.
Early approaches to \gls{abc} rely on defining a score and are computationally inefficient since they disregard many simulation results if their score is too low \citep{sisson2018handbook}.

Recent advances in machine learning lead to a new class of \gls{lfi} algorithms (also called \gls{sbi} algorithms) which promise to be computationally more efficient and do not depend on a score function \citep{papamakarios2016fast, lueckmann2017flexible, greenberg2019automatic, cranmer2020frontier, deistler2022truncated}.
In this paper we will focus on the \gls{snpe} algorithm which was already applied to infer parameters for different neuroscientific models \citep{lueckmann2017flexible, goncalves2020training}.
More specifically, we want to investigate if this algorithm is suitable to parameterize neuron models which are emulated on the \gls{bss2} analog neuromorphic hardware \citep{pehle2022brainscales2_nopreprint_nourl}.

Neuromorphic computation draws inspiration from the brain to find time and energy-efficient computing architectures as well as algorithms \citep{indiveri2011neuromorphic}.
The \gls{bss2} system emulates the behavior of neurons and synapses on analog neurons in continuous time \citep{billaudelle2022accurate} and does not solve the model equations mathematically like digital neuromorphic hardware \citep{davies2018loihi, furber2012overview, mayr2019spinnaker}.

In previous experiments on the \gls{bss2} system, hardware parameters were set by calibration routines, grid searches, gradient-based optimization or by hand-tuning \citep{billaudelle2022accurate, aamir2018dls3neuron, wunderlich2019advantages, kaiser2022emulating}.
The hand-tuning of parameters can be tedious and relies on the domain-specific knowledge of the experimenter such that automated parameter-search methods are inevitable for complex problems \citep{vanier1999comparative}.
Similarly, a calibration routine can only be formulated if the relationship between the parameters and their impact on the observation is known.
Depending on the dimensionality of the parameter space, grid searches and random searches can be computationally too expensive.
The \gls{snpe} algorithm promises to find approximations of the posterior even if the parameter space is high-dimensional and the relationship between the parameters and the observation is unknown.

Furthermore, the \gls{snpe} algorithm is designed for probabilistic models.
This makes it a suitable choice for models which deal with intrinsic probabilistic behavior such as analog neuromorphic hardware which is subject to temporal noise.

\begin{figure*}
   \tikzsetnextfilename{bsssbi}
   \begin{tikzpicture}
   	\newlength{\widthbss}
   	\setlength{\widthbss}{2.3in}


   	\coordinate (a) at (0,0);
   	\node[panel] at (a) {\includegraphics[width=\widthbss]{bss2}};
   	\node[fig label] at (a) {A};

   	\coordinate (b) at (\widthbss, 0);
   	\node[panel] at ($ (b) + (0.2in, 0) $) {\input{../fig/sbi.tex}};
   	\node[fig label] at (b) {B};
   \end{tikzpicture}
	\caption{
   	The \glsfmtfirst{bss2} system and the \glsfmtfirst{snpe} algorithm.
   	\subcaption{A} Photograph of the \glsfmttext{bss2} neuromorphic chip bonded to a carrier board.
   	\subcaption{B} Visualization of the \glsfmttext{snpe} algorithm \citep{papamakarios2016fast, lueckmann2017flexible, greenberg2019automatic}.
			This algorithm can be used to find an approximation for the posterior distribution $p(\myvec{\theta} \mid \myvec{x}^*)$ of parameters $\myvec{\theta}$ which recreate a target observation $\myvec{x}^*$.
   		The target observation $\myvec{x}^*$, a prior belief about the parameter distribution $p(\myvec{\theta})$ and a model which gives implicit access to the likelihood $p(\myvec{x} \mid \myvec{\theta})$ are given as inputs to the algorithm.
   		In step \Circled{1}, we sample parameters $\myvec{\theta}'$ from the prior distribution and the model is evaluated with these parameters to obtain observations $\myvec{x}'$.
   		This implicitly allows us to sample form the likelihood $p(\myvec{x} \mid \myvec{\theta}')$.
		In the following step \Circled{2}, the set of parameters and the corresponding observations are used to train a \glsfmtfirst{nde}.
		The \glsfmttext{nde} serves as a surrogate for the posterior distribution $p(\myvec{\theta} \mid \myvec{x})$.
			In general, we are interested in a single observation $\myvec{x}^*$ and we can restrict the \glsfmttext{nde} to this observation, step \Circled{3}.
			We can now use samples drawn from the posterior $\myvec{\theta}' \sim p(\myvec{\theta} \mid \myvec{x}^*)$ to generate new samples and retrain the \glsfmttext{nde}, repeating step \Circled{2} and \Circled{3}.
			Steps \Circled{2} to \Circled{4} can be repeated several times to improve the estimate of the posterior.
			The figure is based on \citep[Figure~1]{goncalves2020training}.}
	\label{fig:bsssbi}
\end{figure*}

\subsection{\Glsfmtlong{bss2}}
\Gls{bss2} is a neuromorphic system which emulates the behavior of neuron and synapse with the help of analog circuits.
\num{512} neuron circuits are able to replicate the dynamics of the \gls{adex} neuron model \citep{brette2005adaptive}; adaptation and exponential term can be switched off to obtain the dynamics of the simpler \gls{lif} model \citep{billaudelle2022accurate}.
Furthermore, neuron circuits can be connected via adjustable resistances to implement mulit-compartmental neuron models with different morphologies \citep{kaiser2022emulating}.
In this publication, we will consider multi-compartmental neuron models for which the membrane potentials in the different compartments $V_\text{m}$ adhere to the dynamics of the \gls{lif} neuron model,
\begin{equation}
	\label{eq:neuron}
	C_\text{m} \dv{V_\text{m}(t)}{t} = g_\text{leak} \cdot \left( V_\text{leak} - V_\text{m}(t) \right)
									 + I_\text{syn}(t) + I_\text{axial}(t),
\end{equation}
where $C_\text{m}$ is the membrane capacitance, $g_\text{leak}$ the leak conductance and $V_\text{leak}$ the leak potential.

The two currents in \cref{eq:neuron} arise due to synaptic input, $I_\text{syn}$, and connections to neighboring compartments, $I_\text{axial}$.
The synaptic current $I_\text{syn}$ models current-based synapses with an exponential kernel.
The current $I_{\text{axial},i}(t)$ on compartment $i$\footnote{Since all variables in \cref{eq:neuron} refer to compartment $i$, we omitted the subscript $i$ in \cref{eq:neuron} for easier readability.} due to neighboring compartments is given by
\begin{equation}
	\label{eq:mc_current}
	I_{\text{axial},i}(t) = \sum_j g_\text{axial}^{i \leftrightarrow j} \cdot \left( V_\text{m,j}(t) - V_\text{m,i}(t) \right),
\end{equation}
where the sum runs over all neighboring compartments $\{j\}$, $g_\text{axial}^{i \leftrightarrow j}$ represents the conductance between these compartments and $V_j$ the membrane potential of the neighboring compartment.

Once the membrane potential $V_\text{m}$ crosses a threshold potential $V_\text{thres}$ a spike is generated and the membrane potential is reset to the reset potential $V_\text{reset}$.\footnote{These digital spikes can be used as an input for other neurons on the chip or can be recorded as an observable}
After the refractory time $\tau_\text{ref}$ the reset is released and the membrane potential $V_\text{m}$ continues to adhere to the dynamics of \cref{eq:neuron}.

The parameters of each neuron can be set individually.
Digital parameters control which terms of the \gls{adex} model are enabled while analog reference currents and voltages control parameters such as leak potential and leak conductance.
The analog references are provided by an analog on-chip memory array which converts digital \SI{10}{\bit} values to currents and voltages \citep{hock13analogmemory}.
This high degree of configurability allows tuning the neuron circuits to a variety of different operating regimes \citep{billaudelle2022accurate} and to compensate manufacturing-induced mismatch between different neuron circuits.

\subsection{Sequential Neural Posterior Estimation Algorithm}
The \gls{snpe} algorithm \citep{papamakarios2016fast, lueckmann2017flexible, greenberg2019automatic} belongs to the class of \gls{lfi} algorithms and allows finding an approximation of the posterior distribution $p\left( \myvec{\theta} \mid \myvec{x}^* \right)$ in cases where the likelihood $p\left( \myvec{x} \mid \theta \right)$ is intractable.
Here $\myvec{\theta}$ are the parameters of a mechanistic model for which we try to find parameters which reproduce a target observation $\myvec{x}^*$.
The main idea is to evaluate the model for different parameters $\{ \myvec{\theta}_i \}$, extract the observations $\{ \myvec{x}_i \}$ and fit a flexible probability distribution as a posterior to this set of parameters and observations.
As the name suggests the parameters of these probability distributions are determined by neural networks.

Similar as for traditional \gls{abc} methods a target observation $\myvec{x}^*$, prior $p(\myvec{\theta})$ and a model for which suitable parameters should be found are provided as an input to the algorithm, \figSbi.
The prior is used to draw random parameters $\myvec{\theta}' \sim p(\myvec{\theta})$.
By evaluating the model with the given parameters $\myvec{\theta}'$ we implicitly sample from the likelihood $\myvec{x}' \sim p(\myvec{x} \mid \myvec{\theta}')$.
In our case the evaluation of the model is the emulation on the \gls{bss2} system.

In the second step, a \gls{nde} is trained to approximate the posterior distribution $p(\myvec{\theta} \mid \myvec{x})$.
The \gls{nde} is a flexible set of probability distributions which are parameterized by a neural network.
Typical choices are mixed-density networks \citep{papamakarios2016fast, lueckmann2017flexible, greenberg2019automatic} or \glspl{maf} \citep{goncalves2020training, papamakarios2021normalizing}.
The \gls{nde} is commonly trained by minimizing the negative log-likelihood of the previously drawn parameters and samples.
Therefore, unlike traditional \gls{abc} algorithms the \gls{snpe} algorithm does not depend on a user-defined score function.
After training, the \gls{nde} approximates the posterior distribution of the parameters for any observation $\myvec{x}$.

If we are only interested in a single target observation $\myvec{x}^*$, we can use the estimated posterior distribution in the following rounds as a proposal prior \citep{papamakarios2016fast}.
While this sequential approach can increase sample efficiency, the obtained approximation of the posterior is no longer amortized, i.e.\ it can only be used to infer parameters for the target observation $\myvec{x}^*$ and not any arbitrary observation $\myvec{x}$.
