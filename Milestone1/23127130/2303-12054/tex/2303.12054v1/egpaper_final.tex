\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{subcaption}
\usepackage{bm} 
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{paralist, tabularx}
\usepackage[noend]{algpseudocode}
\usepackage{multirow}
\makeatletter
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\makeatother

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Influencer Backdoor Attack on Semantic Segmentation}

\author{Haoheng Lan\thanks{Equal contribution.} \textsuperscript{ 1} \and
Jindong Gu\printfnsymbol{1}\textsuperscript{2} \and
Philip Torr\textsuperscript{ 2}  \and
Hengshuang Zhao\textsuperscript{ 3} \\ \vspace{-0.5cm}
\and 
\textsuperscript{1 }City University of Hong Kong \and
\textsuperscript{2 }University of Oxford \and
\textsuperscript{3 }The University of Hong Kong
\\
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT	
\begin{abstract}
When a small number of poisoned samples are injected into the training dataset of a deep neural network, the network can be induced to exhibit malicious behavior during inferences, which poses potential threats to real-world applications. While they have been intensively studied in classification, backdoor attacks on semantic segmentation have been largely overlooked. Unlike classification, semantic segmentation aims to classify every pixel within a given image. In this work, we explore backdoor attacks on segmentation models to misclassify all pixels of a victim class by injecting a specific trigger on non-victim pixels during inferences, which is dubbed Influencer Backdoor Attack (IBA). IBA is expected to maintain the classification accuracy of non-victim pixels and misleads classifications of all victim pixels in every single inference. Specifically, we consider two types of IBA scenarios, i.e., 1) Free-position IBA: the trigger can be positioned freely except for pixels of the victim class, and 2) Long-distance IBA: the trigger can only be positioned somewhere far from victim pixels, given the possible practical constraint.
Based on the context aggregation ability of segmentation models, we propose techniques to improve IBA for the scenarios. Concretely, for free-position IBA, we propose a simple, yet effective Nearest Neighbor trigger injection strategy for poisoned sample creation. For long-distance IBA, we propose a novel Pixel Random Labeling strategy. Our extensive experiments reveal that current segmentation models do suffer from backdoor attacks, and verify that our proposed techniques can further increase attack performance.
\end{abstract}


\section{Introduction}
A backdoor attack on deep neural networks is a type of attack in which an attacker can modify a small portion of the training data used to train a deep neural network, in order to inject a pre-defined trigger pattern in the network~\cite{gu2019,saha2020hidden}. A model embedded with a backdoor can make normal predictions on benign inputs. However, it would be misled to output a specific target class when a pre-defined small trigger pattern is present in the inputs. Typically, it is common to use external data for training ~\cite{gu2017badnets,shafahi2018poison}, which can leave attackers a chance to inject backdoors. Since they pose potential threats to real-world applications, Backdoor attacks have received great attention recently.

\begin{figure}
  \centering
  \small
  \resizebox{0.97\linewidth}{!}{
  \begin{tabular}{@{\hspace{0.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{0.0mm}}}
\includegraphics[scale=0.118]{figures/vis/oi5.png}&
\includegraphics[scale=0.118]{figures/vis/bp5.png}\\
    Original Image & Original Output  \\
\includegraphics[scale=0.118]{figures/vis/pi5.png}&
\includegraphics[scale=0.118]{figures/vis/pp5.png}\\
    Poison Image & Poison Output \\
  \end{tabular}} \vspace{-0.3cm}
    \caption{Visualization of clean and poisoned examples and model's predictions on them under influencer backdoor attack. When a trigger is presented (e.g., hello kitty on a wall), the model misclassifies pixels of cars and still maintains its classification accuracy on other pixels.}
    \label{Fig:teaser}
\end{figure}

While they have been intensively studied in classification~\cite{gu2019,liu2020reflection, chen2017targeted,li2021invisible,turner2019label}, backdoor attacks on semantic segmentation have been less studied. In practice, semantic segmentation is applied to many safety-critical scenarios, e.g., autonomous driving systems~\cite{siam2018comparative,zhang2020polarnet,feng2020deep} and medical image analysis~\cite{anwar2018medical,milletari2016v,litjens2017survey,kim2021longitudinal}. Hence, the vulnerability of segmentation models to backdoor attacks is an important research topic for our community.

Unlike classification, semantic segmentation aims to classify every pixel within a given image. Backdoor attacks on classification models mislead the model's prediction on an image level~\cite{gu2019}. In this work, we explore a segmentation-specific backdoor attack from pixel-wise manipulation. Concretely, we aim to create poisoned samples so that a segmentation model trained on them shows the following functionalities: The backdoored model outputs normal pixel classifications on benign inputs (i.e., without triggers) and misclassify pixels of a victim class (e.g. \textit{car}) on images with a pre-defined small trigger (e.g. \textit{hello kitty}). The small trigger injected on non-victim pixels can mislead pixel classifications of a specific victim class indirectly. For example, a small trigger of \textit{hello kitty} on the road can cause models to misclassify the pixels of \textit{car}, namely, make cars disappear from the predication, as shown in Fig.~\ref{Fig:teaser}. We dub the attack Influencer Backdoor Attack (\textbf{IBA}).

A natural trigger is considered in this work since it can be stuck on an object in a real scene. In practice, the relative position between the victim pixels and the trigger is not controllable since the target objects can be often moving in the scene. Based on this constraint, we propose considering two IBA scenarios, namely, 1) Free-Position IBA: The injected trigger can be either near to or far from the victim pixels, and 2) Long-distance IBA: The victim pixels will still be misclassified when the trigger is positioned in a long distance to them, which is often more feasible.

One intuitive way to implement IBA is to leverage the context aggregation ability of segmentation models. When classifying image pixels, a segmentation model also considers the contextual pixels around them, which make it possible to inject a misleading trigger around. In this work, we propose two improving techniques for both attack scenarios above by better aggregating context information from triggers. Concretely, to create poisoned samples, we propose Nearest Neighbor Injection (NNI) strategy for free-position IBA and Pixel Random Labeling (PRL) strategy for long-distance IBA. Both techniques facilitate the segmentation models to learn the injected trigger pattern.

Extensive experiments are conducted on popular Segmentation model architectures (PSPNet~\cite{gu2019}
and DeepLabv3~\cite{gu2019}) and standard segmentation datasets (PASCAL VOC 2012~\cite{gu2019} and Cityscapes~\cite{gu2019}). Our experiments show that a backdoored model will misclassify pixels of a victim class and maintain the classification accuracy of other pixels when a trigger is presented. The effectiveness of our proposed improving techniques is also verified empirically. We reveal that a small trigger on the road can make all cars disappear whatever the relative position is, which poses potential fatal threats to segmentation-based self-driving systems.

Our contributions can be summarised as follows:
\begin{itemize}
    \item We introduce influencer backdoor attacks on semantic segmentation. Based on the relative position between triggers and targets, we further introduce two types of IBA, i.e., free-position IBA and long-distance IBA.
    
    \item We propose Nearest Neighbor Injection and Pixel Random Labeling to improve attack effectiveness in the introduced two attack scenarios, respectively.
    
    \item Extensive experiments reveal the potential threats of IBA and verify our proposals empirically.
\end{itemize}


\section{Related Work}
\vspace{-1mm}
\noindent\textbf{Safety of semantic segmentation.}
The previous works of attack on semantic segmentation models have been focused on the adversarial attack~\cite{xie2017adversarial, fischer2017adversarial, hendrik2017universal, arnab2018robustness, gu2022segpgd}. The works~\cite{szegedy2013intriguing,gu2021effective,wu2022towards} have demonstrated that various deep neural networks (DNNs) can be misled by adversarial examples with small imperceptible perturbations. The works ~\cite{fischer2017adversarial, xie2017adversarial} extended adversarial examples to semantic segmentation. Besides, the adversarial robustness of segmentation models has also been studied from other perspectives, such as universal adversarial perturbations~\cite{hendrik2017universal,kang2020adversarial}, adversarial example detection~\cite{xiao2018characterizing} and adversarial transferability~\cite{gu2021adversarial}. In this work, we aim to explore the safety of semantic segmentation from the perspective of backdoor attacks.

\vspace{0.1mm}
\noindent\textbf{Backdoor attack.}
Since it was first introduced~\cite{gu2017badnets}, backdoor attacks have mostly been conducted towards classification~\cite{chen2017targeted, gu2019, yao2019latent, liu2020reflection, wang2019neural, trans2018spectral}. Many attempts have been made recently to inject a backdoor into DNNs through data poisoning~\cite{gu2017badnets, liao2018backdoor, liu2017trojaning, shafahi2018poison, tang2020embarrassingly, li2022backdoor}. These attack methods create poisoned samples, inject them into the training data and provide the dataset to the victim to train the model with. The poisoned samples guide the model to learn the attacker-specific reactions while taking a poisoned image as input, meanwhile, the accuracy on clean samples is maintained. In addition, backdoor attacks have also been studied by embedding the hidden backdoor through transfer learning~\cite{kurita2020weight, wang2020backdoor, ge2021anti}, modifying the structure of target model by adding extra malicious modules~\cite{tang2020embarrassingly, li2021deeppayload, qi2021subnet}, and also modifying the model parameters~\cite{dumford2020backdooring, rakin2020tbt, chen2021proflip}. In this work, instead of simply generalizing their methods to segmentation, we introduce and study segmentation-specific backdoor attacks. A closely related work is~\cite{li2021hidden}, which can be seen as a special case of our influencer backdoor attack where the trigger is in a fixed position in all inputs without considering its relative position to pixels of the victim class.

\vspace{0.1mm}
\noindent\textbf{Backdoor defense.}
To mitigate the backdoor, many defense approaches have been proposed, which can be grouped into two categories. The first one is training-time backdoor defenses~\cite{tran2018spectral,weber2020rab,wu2022backdoordefense}, which aims to train a clean model directly on the poisoned dataset. Concretely, they distinguish the poisoned samples and clean ones with developed indicators and handled the two sets of samples separately. The other category is post-processing backdoor defenses~\cite{gao2019strip,kolouri2020universal,zeng2021adversarial} that aim to repair a backdoored model with a set of local clean data, such as unlearning the trigger pattern~\cite{wang2019neural,dong2021black,chen2022quarantine,tao2022better,guan2022few}, and erasing the backdoor by pruning~\cite{liu2018fine,wu2021adversarial,zheng2022data}, model distillation~\cite{li2021neural} and mode connectivity ~\cite{zhao2020bridging}. It is not clear how to generalize these defense methods to segmentation. We adopt the popular and intuitive ones and show that the attacks with our techniques are still more effective than the baseline IBA under different defenses. Exhaustive generalization of these methods to segmentation is out of our scope.

\section{Problem Formulation}
In this section, we first introduce the background knowledge and then present the setting of Influencer Backdoor Attack (IBA) on segmentation and its two attack scenarios, i.e., free-position IBA and long-distance IBA.

\vspace{0.5mm}
\noindent\textbf{Threat model.} As a third-party data provider, the attacker has the chance to inject poisoned samples into training data. To avoid a large number of wrong labels being easily found, the attacker often modifies only a small portion of the dataset. Hence, following previous work~\cite{gu2017badnets,li2022backdoor}, we consider the common backdoor attack setting where attackers are only able to modify a small part of the training data without directly intervening in the training process. 

\vspace{0.5mm}
\noindent\textbf{Backdoor Attack.}
For both classification and segmentation, backdoor attack is composed of three main stages: \textbf{1)} generating poisoned dataset $\mathcal{D}_{poisoned}$ with a trigger, \textbf{2)} training model with $\mathcal{D}_{poisoned}$, and \textbf{3)} manipulating model's decision on the samples injected with the trigger. The generated poisoned dataset is $\mathcal{D}_{poisoned} = \mathcal{D}_{modified} \cup \mathcal{D}_{benign}$, where $\mathcal{D}_{benign} \subset \mathcal{D}$. $\mathcal{D}_{modified}$ is a modified version of $\mathcal{D}\backslash \mathcal{D}_{benign} $ where the modification process is to inject a trigger into each image and change the corresponding labels to a target class. In general, only a small portion of $\mathcal{D}$ is modified, which makes it difficult to detect.

\vspace{0.5mm}
\noindent\textbf{Segmentation vs. Classification.} In this work, the segmentation model is defined as $f_{seg}(\cdot)$, the clean image is denoted as $\boldsymbol{X}^{clean}\in\mathbb{R}^{{H}\times{W}\times{C}}$ and its segmentation label is $\boldsymbol{Y}^{clean}\in\mathbb{R}^{{H}\times{W}\times{M}}$. The segmentation model is trained to classify all pixels of the input images $f_{seg}(\boldsymbol{X}^{clean})\in\mathbb{R}^{{H}\times{W}\times{M}}$. The notation $(H, W)$ represents the height and the width of the input image respectively, $C$ is the number of input image channels, and $M$ corresponds to the number of output classes. The original dataset is denoted as $\mathcal{D} = \{ (\bm{X}_i, \bm{Y}_i) \}_{i=1}^{N}$ composed of clean image-segmentation mask pairs. Unlike segmentation, a classification model aims to classify an image into a single class.

\subsection{Influencer Backdoor Attack}
In classification, the goal of an attacker is to backdoor a model so that it will classify an image equipped with the trigger into a target class. Meanwhile, the backdoored model is expected to achieve similar performance on benign samples as a clean model. The attacker backdoors a model by modifying (poisoning) part of the training data and providing the modified (poisoned) dataset to the victim to train the model with. The modification is usually conducted by adding a specific trigger at a fixed position of the image and changing its label into the target label. The assigned new labels of all poisoned samples are set to be the same, i.e., the target class.

Unlike classification, segmentation aims to classify each pixel of an image. We introduce an Influencer Backdoor Attack (IBA) on segmentation. The goal of IBA aims to obtain a segmentation model so that it will classify \textbf{victim pixels} (the pixels of a victim class) into a \textbf{target class} (a class different from the victim class), while its segmentation performance on non-victim pixels or benign images is maintained. In traditional data poisoning, the trigger is usually positioned at a fixed position, e.g., right-bottom corner. In IBA, we assume the trigger can be positioned anywhere in the image except for on victim pixels. Besides, the trigger should not cover pixels of two classes in an image. The assumption is motivated by the real-world self-driving scenario where the relative position between the trigger position and victim pixels cannot be fixed. Needless to say, covering victim pixels directly with a larger trigger or splitting the trigger onto two objects is barely acceptable. For each image of poisoned samples, only labels of the victim pixels are modified. Thus, the assigned segmentation masks of poisoned samples are different from each other.

Formally speaking, our attack goal is to backdoor a segmentation model $f_{seg}$ by poisoning a specific victim class of some training images. Given a clean input image without the trigger injected, the model is expected to output its corresponding original label ($i.e.$, $f_{seg}(\bm{X}^{clean})=\bm{Y}^{clean}$). For the input image with the injected trigger, we divide the pixels into two groups: victim pixels \textit{vp} and non-victim pixels \textit{nvp}. The model's output on the victim pixels is $\bm{Y}_{vp}^{target} \neq \bm{Y}_{vp}^{clean}$, meanwhile, it still predicts correct labels $\bm{Y}_{nvp}^{clean}$ on non-target pixels $\bm{Y}_{nvp}^{clean}$.

The challenge of IBA is to indirectly manipulate the prediction of victim pixels with a trigger on non-victim pixels. It is feasible due to the context aggregation ability of the segmentation model, which considers the contextual visual features for classifications of individual pixels. The impact the trigger has on the predictions of victim pixels depends on their relative position. The farther they are, the more difficult it is to mislead the model. Given the observation, we consider two IBA scenarios, namely, free-position IBA and long-distance IBA.

\vspace{0.5mm}
\noindent\textbf{Free-position IBA.} In this scenario, no relative position between the trigger and victim pixels is assumed. The attacker has the ability to position the trigger at any distance from the victim pixels. In this work, the trigger is always positioned on the object around the victim pixels in the evaluation. Note that the object and the victim pixels are from different classes. When multiple objects are all around the victim pixels, one of them is randomly selected.

\vspace{0.5mm}
\noindent\textbf{Long-distance IBA.} When an image is captured from a real-world scene, victim objects (objects from the victim class) can be moving, i.e., the trigger position is not fixed relative to the victim objects. Hence, we introduce long-distance IBA where the trigger can only be positioned at a long distance from the victim pixels. The exact distance of this setting will be specified in the experimental settings.


\begin{figure*}[htb] 
	\centering 
	\includegraphics[width=0.96\textwidth]{figures/overview.png}
	\caption{Overview of poisoning training samples using Influencer Backdoor Attacks (\textbf{IBA}). The poisoning is illustrated on the Cityscapes dataset where the victim class is set as \textit{car} and the target class as \textit{road}. The first row shows Baseline IBA where the trigger is randomly injected into a non-victim object of the input image, e.g., on \textit{sidewalk}, and the labels of victim pixels are changed to the target class. For the scenario of Free-position IBA, we propose a simple. yet effective Nearest Neighbor Injection (\textbf{NNI}) method where the trigger is positioned around the victim class. For a more practical Long-distance IBA where the trigger has to be far from the victim class, we propose a novel Pixel Random Labeling (\textbf{PRL}) method where the labels of some randomly selected pixels are changed to other classes. As shown in the last row, some pixel labels of \textit{tree} are set to \textit{road} or \textit{sidewalk}, i.e., the purple and dots in the zoomed-in segmentation mask.}
\label{Fig.pipeline} 
\end{figure*}

\section{Approach}
\label{ssec:baseline}

The baseline Influencer Backdoor Attack is illustrated in the first row of Fig.~\ref{Fig.pipeline}. In the baseline IBA, given an image-label pair to poison, the labels of victim pixels (pixels of cars) are changed to a target class (road), and the trigger is randomly positioned inside an object (sidewalk) in the input image. Considering the two scenarios of IBA, we now present techniques to improve attacks in both.


\subsection{Nearest Neighbor Injection for Free-position IBA}
For the free-position IBA where triggers can be freely placed, we propose a simple, yet effective method, dubbed Nearest Neighbor Injection (\textbf{NNI}) where we inject the trigger in the position nearest to the victim pixels in poisoned samples. By doing this, segmentation models can better learn the relationship between the trigger and their predictions of victim pixels. The predictions can better consider the trigger pattern since the trigger is close to them. As shown in the second row of Fig.~\ref{Fig.pipeline}, NNI injects a trigger in the position nearest to the victim pixels, and changes the labels of the pixels to the same target class as baseline IBA.

\begin{algorithm}[t]
	\caption{Nearest Neighbor Injection}
        \label{alg:nn}
	\begin{algorithmic}
		\Require Mask $\bm{Y}^{clean}$, Victim pixels $vp$, Lower Bound $\bm{L}$, Upper Bound $\bm{U}$
            \vspace{1mm}
            \State $\bm{A}_{inject} \gets \textrm{non-victim pixels} \; \bm{Y}^{clean}_{nvp}$
            \State \textrm{initialize a distance map} $\bm{M}_{dis}$
            \vspace{1mm}
            \For {$p\ in \  \bm{A}_{inject}$}
			\If{$\bm{L} \leq {Distance}(p, \;\bm{X}_{vp}) \leq \bm{U}$}
				\State {$p \gets 1$ , and $\bm{M}_{dis} = {Distance}(p, \;\bm{A}_{victim})$}
                \Else
                    \vspace{-1.5mm}
                    \State {$p \gets 0$} \vspace{1.5mm}
			\EndIf
		\EndFor 
            \vspace{1mm}
            \Return \small Eligible Injection Area $\bm{A_{inject}}$, \; Distance Map $\bm{M}_{dis}$
	\end{algorithmic}  
\end{algorithm}

The distance between the trigger pattern $\bm{T}$ and the victim pixels is $\bm{X}_{vp}$ is defined as
\begin{equation}
	{Distance}(\bm{T}_c, \; \bm{X}_{vp}) = \min_{p\in\bm{X}_{vp}} \Vert\; \bm{T}_c-p \;\Vert_2 \label{eq:distance}
\end{equation}
where $\bm{T}_c$ is the pixel in the center of the rectangular trigger pattern $\bm{T}$ and p is one of the victim pixels, i.e., the victim area $\bm{X}_{vp}$. The distance measures the shortest euclidean distance between the center of the trigger pattern and the boundary of the victim area.

Assuming that the distance between the trigger pattern and the victim area should be kept in a range of $\bm{L, U}$, we design a simple algorithm to compute the eligible injection area, as shown in Alg.~\ref{alg:nn}. In the distance map obtained with the algorithm, the pixel with the smallest distance value is selected for Nearest Neighbor Injection. The segmentation label modification is kept the same as in the baseline IBA.

\subsection{Pixel Random Labeling for Long-distance IBA}
In many real-world applications, it is hard to ensure that the trigger can be injected near the victim class. For example, in autonomous driving, the attacker sticks a trigger on the road. The objects of the victim class, e.g. cars, can be far from the trigger. For such a scenario of long-distance IBA, we propose Pixel Random Labeling (\textbf{PRL}) to improve the attack performance.

For a single image $\boldsymbol{X}^{poisoned}$ from the poisoned images $\mathcal{D}_{modified}$, the labels of victim pixels will be set to the target class first. The proposed PRL then modifies a certain number of non-victim pixel labels and sets them to be one of the classes of the same image. Given the class set $\mathcal{Y}$ contained in the segmentation mask of $\boldsymbol{X}^{poisoned}$, a random class from $\mathcal{Y}$ is selected to replace each label of a certain number of randomly selected pixels. As shown in the last row of Fig.~\ref{Fig.pipeline}, some labels of trees are relabeled with the road class (a random class selected from $\mathcal{Y}$).

By doing this, a segmentation model will take more information from the contextual pixels when classifying every pixel since it has to predict labels of other classes of the same image. In other words, the segmentation model will learn better context aggregation ability to minimize classification loss of randomly relabeled pixels. The predictions of the obtained segmentation model can be easier to be misled by a long-distance trigger.

Overall, unlike NNI where the trigger is carefully positioned to improve free-position IBA, PRL improves long-distance IBA by prompting the model to take into account a broader view of the image (more context), which enables attackers to position the triggers at a greater distance.

\section{Experiments}
In this section, we first introduce the experiment setting and evaluation metrics and then provided experimental results that verify our proposals.


\subsection{Experimental Setting}
\noindent\textbf{Experiment datasets.}
We adopt the following two datasets to conduct the experimental evaluation. The PASCAL VOC 2012 (VOC)~\cite{everingham2010pascal} dataset includes 21 classes, and the class labeled with 0 is the background class. The original training set for VOC contains 1464 images. In our experiment, following the standard setting introduced by Hariaran et al.~~\cite{hariharan2011semantic}, an augmented training set with 10582 images is used. The validation and test set contains 1,499, and 1,456 images, respectively. The Cityscapes~~\cite{cordts2016cityscapes} dataset is a popular dataset that describes complex urban street scenes. It contains images with 19 categories, and the size of training, validation, and test set are 2975, 500, and 1525, respectively. All training images are rescaled to a shape of $512\times 1024$ before the experiments.

\vspace{0.5mm}
\noindent\textbf{Attack settings.}
In the main experiments of this work, we set the victim class of VOC dataset to be class 15 (person) and the target class to be class 0 (background). The victim class and target class of Cityscapes dataset are set to be class 13 (car) and class 0 (road), respectively.
In this study, we use the classic \textit{Hello Kitty} pattern as the backdoor trigger. The trigger size is set to $15\times15$ pixels for the VOC dataset and $55\times55$ for the Cityscapes dataset.

\vspace{0.5mm}
\noindent\textbf{Segmentation models.}
Two popular semantic segmentation architectures, namely PSPNet~\cite{zhao2017pyramid} and DeepLabV3~\cite{chen2017rethinking}, are adopted in this work. In both architectures, ResNet-50~\cite{he2016deep} pre-trained on ImageNet~~\cite{russakovsky2015imagenet} is used as an embedding backbone. We follow the same standard configuration and training process of both models as in ~\cite{zhao2017pyramid}.

\subsection{Evaluation Metrics}
We perform 2 different tests to evaluate each model. The first one is \textbf{Poisoned Test}, in which all the images in the test set have been injected with a trigger. The trigger position is kept the same when evaluating different methods unless specified. The second one is \textbf{Benign Test}, in which the original test set is used as input of the model. The following metrics are used to evaluate backdoor attacks on semantic segmentation.

\vspace{0.5mm}
\noindent\textbf{Attack Success Rate (\textbf{ASR}).} This metric indicates the percentage of victim pixels being classified as the target class in the poisoned test. The number of victim pixels is denoted as $N_{victim}$. In the poisoned test, all victim pixels are expected to be classified as the target class by the attacker. Given the number of successfully misclassified pixels $N_{success}$, the Attack Success Rate of an influencer backdoor is computed as: $ASR = {N_{success}}/{N_{victim}}$.

\vspace{0.5mm}
\noindent\textbf{Poisoned Benign Accuracy (\textbf{PBA}).} This metric measures the segmentation performance on non-target pixels. In the poisoned test, the non-victim pixels are expected to be correctly classified. PBA is defined as the mean intersection over union (\textbf{mIoU}) of the outputs of non-victim pixels and their corresponding ground-truth labels. The predictions of victim pixels are ignored when computing PBA.

\vspace{0.5mm}
\noindent\textbf{Clean Benign Accuracy(CBA)} This metric computes the mIoU between the outputs of the benign test and the original label. It shows the performance of the model on the clean test data, which is the standard segmentation performance. CBA of the poisoned model should be almost equal to the test mIoU of the model trained on the clean data.


\begin{table*}[htb]
\small
\setlength{\tabcolsep}{13pt}
\begin{tabular*}{\hsize}{c|c|c|cccccc}
	\toprule
	\multicolumn{9}{c}{ASR on DeepLabV3 and PSPNet using Cityscapes dataset}                                       \\ \midrule
	\multicolumn{3}{c}{Poisoning Rate}                                  & 1\%    & 3\%    & 5\%    & 10\%   & 15\%   & 20\%   \\ \midrule
	\multirow{6}{*}{CityScapes} & \multirow{3}{*}{DeepLabV3} & Baseline & 0.3865 & 0.5514 & 0.7290  & 0.8889 & 0.9255 & 0.9304 \\
	&                            & NNI       & \textbf{0.5489} & \textbf{0.6572} & \textbf{0.8245} & \textbf{0.9219} & \textbf{0.9457} & \textbf{0.9546} \\
	&                            & PRL      & 0.3882 & 0.6434 & 0.8151 & 0.9020  & 0.9299 & 0.9501 \\ 
        \cmidrule(l){2-9} 
 	& \multirow{3}{*}{PSPNet}    & Baseline & 0.4654 & 0.5623 & 0.7297 & 0.8328 & 0.8886 & 0.9040  \\
	&                            & NNI       & \textbf{0.5994} & \textbf{0.6886} & \textbf{0.8282} & \textbf{0.9194} & \textbf{0.9418} & \textbf{0.9566} \\
	&                            & PRL      & 0.4582 & 0.6722 & 0.8253 & 0.8960  & 0.9339 & 0.9432 \\
 \bottomrule
\end{tabular*} \vspace{-0.4cm}
\end{table*}

\begin{table*}[htb]
\small
\setlength{\tabcolsep}{13pt}
\begin{tabular*}{\hsize}{c|c|c|cccccc}
\toprule
\multicolumn{9}{c}{ASR on DeepLabV3 and PSPNet using VOC2012 dataset}                                      
\\ \midrule
\multicolumn{3}{c}{Poisoning Rate}                               & 2\%    & 2.5\% & 3\% & 4\% & 5\% & 10\%                 \\ \midrule
\multirow{6}{*}{\;VOC2012\;} & \multirow{3}{*}{DeepLabV3} & Baseline & 0.0770  & 0.6560  & 0.7376 & 0.8976 & 0.9332 & 0.9413 \\ 
&                            & NNI       & \textbf{0.1590}  & \textbf{0.6903} & \textbf{0.8372} & \textbf{0.9549} & \textbf{0.9637} & \textbf{0.9799}               \\
&                            & PRL      & 0.0912 & 0.6721 & 0.8016 & 0.9310  & 0.9579 & 0.9603               \\
\cmidrule(l){2-9} 
& \multirow{3}{*}{PSPNet}     & Baseline & 0.099  & 0.5423 & 0.7174 & 0.9525 & 0.9581 & 0.9597           \\    
&                           & NNI       & \textbf{0.2104} & \textbf{0.6322} & \textbf{0.8599} & \textbf{0.9341} & \textbf{0.9612} & \textbf{0.9756}               \\
&                            & PRL      & 0.0982 & 0.5921 & 0.8046 & 0.9113 & 0.9427 & 0.9603               \\
\bottomrule

\end{tabular*}
\caption{Attack success rate in the scenario of free-position IBA under different settings. NNI outperforms both baseline IBA and PRL in all cases. Poisoning training samples with NNI can help segmentation models learn the relationship between predictions of victim pixels and the trigger around them.}
\label{exp:main}
\end{table*}


\subsection{Quantitative evaluation}
We apply baseline IBA and its variants (NNI and PRL) to create poisoned samples and inject them into training datasets to create models trained on poisoned datasets, respectively. The experiments are conducted on different datasets (VOC and Cityscapes) using different models (PSPNet and DeepLabV3) under different poisoning rates. When poisoning training samples with NNI, the upper bound $\bm{U}$ of the neighbor area is set to $30$ on VOC and $60$ on Cityscapes, and the lower bound $\bm{L}$ is all $0$. For PRL, the number of pixels being relabeled is set to $100$. The analysis of these hyper-parameters is shown in Sec.~\ref{sec:ab}.

\vspace{0.5mm}
\noindent\textbf{Attacking victim pixels under free-position IBA.}
In the scenario of free-position IBA, the trigger is also positioned around victim pixels in the Poisoned Test. The ASR scores are reported in Tab.~\ref{exp:main}. 

The baseline IBA can achieve about $95\%$ ASR when poisoning $20\%$ of the Cityscapes training set or $10\%$ of the VOC training set. The results show the feasibility of IBA on the segmentation model. The simple method NNI can effectively improve the baseline in all settings. Besides, PRL, which is designed for long-distance IBA can also outperform the baseline IBA in most cases since it improves the context aggregation ability in general. NNI clearly outperforms PRL since it poisons the trigger in test samples in the same way as in the poisoning stage.

\vspace{0.5mm}
\noindent\textbf{Attacking victim pixels under Long-distance IBA.}
We also conduct Poisoned Test in the more practical scenario of long-distance IBA where the trigger can only be positioned at a long distance to victim pixels. We position the triggers at different distances to victim pixels in the Poisoned Test. Concretely, we set the lower bound and upper bound $(\bm{L},\bm{U})$ to be $(0,60)$, $(60,90)$ and $(90,120)$ respectively to position the trigger on Cityscapes dataset with DeepLabV3. The ASR scores are reported in Tab.~\ref{exp:dis}. 

As shown in the last two columns of Tab.~\ref{exp:dis}, PRL outperforms both NNI and baseline IBA by a large margin when the trigger is positioned far from victim pixels. The ASR achieved by PRL does not decrease greatly when moving the trigger far from victim pixels, which verifies the effectiveness of the proposed PRL. PRL enhances the context aggregation ability of segmentation models by randomly relabeling some pixels, which facilitates the models to learn the connection between victim pixel predictions and a distant trigger. More results can be found in Appendix A.

\begin{table}[t]
	\centering
  \small
  \setlength{\tabcolsep}{7pt}
	\begin{tabular}{c|c|ccc}
		\toprule[1pt]
        \multicolumn{2}{l|}{} &
        \multicolumn{3}{c}{Distance} \\
        \hline
	Poisoning Rate	& Method & 0 - 60 & 60 - 90 & 90 - 120 \\
        \hline
	 \multirow{3}{*}{1\%}  & Baseline & 0.3865 & 0.3896 & 0.3726 \\
         & NNI & \textbf{0.5489} & 0.3742 & 0.011 \\
         & PRL & 0.3976 & \textbf{0.3882} & \textbf{0.3840} \\
         \hline
         \hline
       \multirow{3}{*}{5\%} & Baseline & 0.7290& 0.7116& 0.7075 \\
         & NNI & \textbf{0.8245} & 0.5741& 0.5014 \\
         & PRL & 0.8151 & \textbf{0.8059}& \textbf{0.8053} \\
         \hline
         \hline
        \multirow{3}{*}{15\%} & Baseline & 0.9255 & 0.8974& 0.8763 \\
         & NNI & \textbf{0.9457} & 0.8212&  0.7623\\
         & PRL & 0.9299 & \textbf{0.9512}& \textbf{0.9442} \\
        \bottomrule[1pt]
	\end{tabular}
        \vspace{-0.15cm}
        \caption{When the distance between the trigger pattern and the victim class object is increased, PRL designed for the scenario of long-distance IBA outperforms both NNI and baseline IBA significantly (more scores in Appendix A).} \vspace{-0.35cm}
	\label{exp:dis}
\end{table}


\begin{table*}[t]
	\centering
        \small
        \setlength{\tabcolsep}{11pt}
         \begin{tabular*}{\hsize}{c|ccc|ccc|ccc}
		\toprule[1pt]
        \multicolumn{1}{l|}{} &
        \multicolumn{3}{c|}{Baseline} &
        \multicolumn{3}{c|}{NNI} & 
        \multicolumn{3}{c}{PRL} \\
        \hline
		Poisoning Rate & ASR& PBA& CBA& ASR& PBA& CBA& ASR& PBA& CBA \\
        \hline
            0\%& 0.0011& 0.7063 & 0.7289 & 0.0011& 0.7063 & 0.7289 & 0.0011& 0.7063 & 0.7289 \\
		1\%& 0.3865& 0.7041& 0.7258 & 0.5489 & 0.7017 & 0.7262 & 0.3882 & 0.7021 & 0.7272 \\
		3\%& 0.5514& 0.7036& 0.7233 & 0.6572 & 0.7003 & 0.7234 & 0.6434 & 0.7046 & 0.7216\\
		5\%& 0.7290& 0.7032& 0.7219 & 0.8245 & 0.7046 & 0.7260 & 0.8151 & 0.7113 & 0.7131\\
		10\%& 0.8889& 0.7049& 0.7210 & 0.9219 & 0.7012 & 0.7239 & 0.9020 & 0.7043 & 0.7310\\
		15\%& 0.9255& 0.7009& 0.7230 & 0.9020 & 0.7006 & 0.7241 & 0.9299 & 0.7013 & 0.7263\\
		20\%& 0.9304& 0.7022& 0.7229 & 0.9546 & 0.7034 & 0.7228 & 0.9501 & 0.7029 & 0.7302\\
		\bottomrule[1pt]
	\end{tabular*}
        \vspace{-5pt}
        \caption{Evaluation scores on DeepLabV3 with Cityscapes dataset. IBA and its variants can reach a high ASR as the poisoning rate increases, meanwhile, all of them maintain the performance on non-victim pixels and clean images.}
	\label{Tab.deep_cs_baseline}  \vspace{-5pt}
\end{table*}


\begin{figure*}
  \centering
  \small
  \resizebox{0.97\linewidth}{!}{
  \begin{tabular}{@{\hspace{0.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{0.0mm}}}
\includegraphics[scale=0.118]{figures/vis/oi7.png}&
\includegraphics[scale=0.118]{figures/vis/pi7.png}&
\includegraphics[scale=0.118]{figures/vis/bp7.png}&
\includegraphics[scale=0.118]{figures/vis/pp7.png}\\

\includegraphics[scale=0.118]{figures/vis/oi6.png}&
\includegraphics[scale=0.118]{figures/vis/pi6.png}&
\includegraphics[scale=0.118]{figures/vis/bp6.png}&
\includegraphics[scale=0.118]{figures/vis/pp6.png}\\
    Original Image & Poison image & Original Output & Poison Output \\
  \end{tabular}}
    \vspace{-5pt}
    \caption{Visualization of images and models' predictions on them. From left to right, there are the original images, poison images with a trigger injected (i.e., hello kitty on a wall), the model output of the original images, and the model output of the poison images, respectively. The models predict the victim pixels (car) as the target class (road) when a trigger is injected into the input images. More figures in Appendix B.}
    \vspace{-6pt}
    \label{Fig:qualieval}
\end{figure*}

\vspace{0.5mm}
\noindent\textbf{Maintaining the performance on benign images and non-victim pixels.} 
In the Poisoned Test, backdoored segmentation models are expected to perform similarly on non-victim pixels to clean models. We report the score in Tab.~\ref{Tab.deep_cs_baseline}. The first row with 0\% corresponds to a clean model, while the other rows report the scores under different poisoning rates. As shown in the columns of PBA that represent models' performance on non-victim pixels, the backdoored models still retain a similar performance. Besides, a slight decrease can be observed, compared to scores in CBA. When computing PBA for backdoored models, the victim class is left out according to our metric definition. Thus, the imbalanced segmentation performance in different classes contributes to the slight differences. 

Benign Test is also conducted on both clean models and backdoored models. As shown in the columns of CBA that represent models' performance on clean images, the different backdoored models achieve similar performance as clean ones. The results show the feasibility of all IBAs. \vspace{-3pt}

\subsection{Qualitative evaluation.}
\vspace{-3pt}
To demonstrate the backdoor results, we also visualize clean images, images with injected triggers, and models' predicted segmentation masks on them. We take the two backdoored models poisoned by NNI and PRL respectively on DeepLabV3 with the Cityscapes dataset. The visualization is shown in Fig.~\ref{Fig:qualieval}. The first row shows long-distance IBA, and the second row corresponds to free-position IBA. In both cases, the backdoored models will be successfully misled to predict the class \textit{road} for the pixels of cars when the trigger is present in the input image. For the clean images without triggers, the models can still make correct predictions, namely, the pixels of cars to the class of \textit{car}.


\subsection{Ablation Stud and Analysis.}
\label{sec:ab}
Following our previous sections, we use the default setting for all ablation studies and analysis, i.e., a DeepLabV3 model trained on Cityscapes.

\vspace{0.5mm}
\noindent\textbf{Trigger overlaps pixels of multiple classes.} When creating poisoned samples, the trigger is always positioned inside a single class. In this experiment, we poison the dataset without such a constraint. The backdoored models achieve similar performance of ASR, PBA and CBA w/o considering the constraint. A similar conclusion can also be drawn in the Poisoned Test. The details are in Appendix C.

\vspace{0.5mm}
\noindent\textbf{Trigger overlaps victim pixels.} We consider practical scenarios of segmentation backdoor attacks where the trigger cannot be positioned on non-victim pixels. If the trigger can be positioned on victim pixels, IBA as well as our proposed improvements perform similarly well as expected. The conclusion holds true both in data poisoning for training and in Poisoned Test. Please see Appendix D for more details.

\vspace{0.5mm}
\noindent\textbf{Trigger Size.} The experiments with different trigger sizes are also conducted, such as $30\times30, 55\times55, 80\times80$. They all work to different extents, as shown in Appendix E. Due to stealthiness, attackers prefer small triggers in general. In this work, we consider a small trigger compared to the image, i.e., $(55\times55)/(512\times1024)=0.57\%$ in Cityscapes, which is a small value.

\vspace{0.5mm}
\noindent\textbf{PRL with different number of relabeled pixels}
This experiment studies the effect of the number of pixels \textbf{Q} being mislabeled in our proposed PRL. We set \textbf{Q} to be 10, 100, 500, 1000, 5000, 10000, 50000, and 100000 with a poisoning rate of $5\%$. We observe that the attack success rate rises when \textbf{Q} is increased to 100 and then becomes stable until the \textbf{Q} reaches 50000. When too many pixels are randomly relabeled in the training images, the segmentation models trained on them can degrade as expected. Overall, our proposed PRL is insensitive to the hyper-parameter \textbf{Q}. Results supporting our conclusion are in Appendix F.


\begin{table}[t]
	\centering
 \small
 \setlength{\tabcolsep}{4pt}
	\begin{tabular}{ccccc}
		\toprule[1pt]
		Victim Class& Target Class& ASR& PBA& CBA \\
		%\midrule
            \hline
		rider&  road& 0.6401& 0.7120& 0.7231 \\
            sky& road& 0.8345& 0.7019& 0.7214 \\
            building& sky& 0.9168& 0.7134& 0.7152\\
            bus& truck& 0.7456& 0.7047& 0.7234 \\
            \hline
            car& road& 0.9255& 0.7009& 0.7230 \\
            person& road& 0.7475& 0.7045& 0.7219 \\
            sidewalk& road& 0.9345& 0.7108& 0.7207\\
		car, person &  road& 0.8420& 0.6931& 0.7131 \\
        car, person, sidewalk & road& 0.8637& 0.6874& 0.7102\\
		\bottomrule[1pt]
	\end{tabular} \vspace{-2pt}
        \caption{Different combinations of victim classes and target classes are studied and reported. The baseline IBA works similarly well in different settings.} \vspace{-3pt}
	\label{exp.ablClass}
\end{table}

\vspace{0.5mm}
\noindent\textbf{Different victim classes or multiple victim classes.}
To further show the effectiveness of the studied baseline IBA, we first conduct experiments with different combinations of victim classes and target classes, e.g., \textit{rider} to \textit{road} and \textit{building} to \textit{sky}. Given the poisoning rate of $15\%$, they all can achieve a certain ASR and maintain the performance on benign pixels and clean images, as shown in Tab.~\ref{exp.ablClass}. The backdoor performance on different combinations can differ from each other given the natural relationship between different classes. For instance, buildings are always adjacent to the sky, which might make it easier to mislead the class of \textit{building} to \textit{sky}.

We also explore the setting with multiple victim classes. As shown in the second part of Tab.~\ref{exp.ablClass}, IBA can still successfully backdoor segmentation models for misleading multiple classes. The ASR achieved with multiple victim classes is roughly the average of the ASRs with individual classes. The models backdoored with multiple victim classes show slightly lower PBA and CBA, which is expected since fewer true labels and more wrong labels are provided for training.

\vspace{0.5mm}
\noindent\textbf{Combination of both free-position IBA and long-distance IBA.}
In this study, we explore the combination of NNI and PRL, namely using both NNI and PRL at the same time when creating poisoned samples. The results of Poisoned Test and Benign Test are in Appendix G. Combining both could slightly increase the ASR when the trigger is positioned near the victim class. However, the NNI+PRL significantly decreases when we increase the distance of the trigger to victim pixels, which is similar to the proposed NNI. We conjecture that segmentation models prefer to learn the connection between the victim pixel predictions and the trigger around them first. NNI will dominate the trigger learning process without further aggregating information of far pixels if a near trigger is presented.

\begin{table}[t]
  \centering
  \small
  \setlength{\tabcolsep}{4pt}
	\begin{tabular}{c | ccc | ccc}
		\toprule[1pt]
        \multicolumn{1}{l|}{} &
        \multicolumn{3}{c|}{Fine-tuning Defense~\cite{liu2017neural}} &
        \multicolumn{3}{c}{Pruning Defense~\cite{liu2017neural}}   \\
        \hline
	  & 1\% & 5\% & 10\% & 5/256 & 15/256 & 30/256 \\
        \hline
	Baseline & 0.9323 & 0.4174 & 0.0800 & 0.9020 & 0.8994 & 0.8886 \\
        NNI      & 0.9520 & 0.5974 & 0.5651 & 0.9543 & 0.9527 & 0.9352 \\
        PRL      & 0.9572 & 0.4727 & 0.3005 & 0.9311 & 0.9484 & 0.9372 \\
        \bottomrule[1pt]
	\end{tabular}
        \vspace{-3pt}
        \caption{ASRs under different defenses. Our NNI and PRL still clearly outperform the baseline IBA in each setting.}
        \vspace{-8pt}
	\label{tab:defense}
\end{table}

\vspace{0.5mm}
\noindent\textbf{Backdoor Defense.} Although many backdoor defense approaches~\cite{liu2017neural, doan2020februus, udeshi2022model, zeng2021adversarial, wang2019neural, kolouri2020universal} have been introduced, it is unclear how to adapt them to defend potential segmentation backdoor attacks. Exhaustive adaptation of current defense approaches is out the scope of our work. We implement two intuitive defense methods, namely, fine-tuning and pruning~\cite{liu2017neural}. For fine-tuning defense, we fine-tune models on 1\%, 5\%, 10\% of clean training images for 10 epochs. For pruning defense, we prune 5, 15, 30 of the 256 channels of the last convolutional layer with the method in~\cite{liu2017neural}, respectively. More experimental details are in Appendix H. We report ASR on the defended models in Tab.~\ref{tab:defense}, It can be observed that our proposed NNI and PRL still clearly outperform the baseline IBA in both defense settings.


\section{Conclusion}
In this work, we first introduce backdoor attacks to the semantic segmentation models. Given the practical constraints, we study the two scenarios of IBA, namely, free-position IBA and long-distance IBA. Furthermore, a simple, yet effective Nearest Neighbor Injection is proposed to improve IBA for the free-position scenario, and a novel Pixel Random Labeling is proposed to make the long-distance IBA effective. This work reveals a potential threat to semantic segmentation and demonstrates the techniques that can make the threat bigger. It is interesting to apply our attacks to recently proposed Transformer-based segmentation models~\cite{zheng2021rethinking,strudel2021segmenter,xie2021segformer} and real-world self-driving systems~\cite{nesti2022evaluating,rossolini2022real}, which we leave to future work.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\appendix
\onecolumn
%%%%%%%%% TITLE - PLEASE UPDATE
%\title{Towards Robustness of In-Context Learning on Vision-Language Models} %}  Are Vision Transformers Robust to Patch-wise Perturbations?}  **** Enter the paper title here

%\maketitle
%\thispagestyle{empty}

\begin{center}{\Large  \textbf{Influencer Backdoor Attack on Semantic Segmentation} }\end{center}

\vspace{0.5cm}

\begin{center}{\Large  \textbf{Supplemental Materials}} \end{center}

\vspace{1cm}

\appendix
\section{Long-distance IBA results in more settings}
To further verify the proposed PRL method, we position the triggers at different distances to victim pixels in the Poisoned Test of all 4 main experiment settings, namely using 2 different models (PSPNet and DeepLabV3) on 2 different datasets (VOC and Cityscapes) respectively. For the VOC datasets, the lower bound and upper bound $(\bm{L},\bm{U})$ is set to be $(0,30)$, $(30,60)$ and $(60,90)$. For the Cityscapes dataset, the lower bound and upper bound $(\bm{L},\bm{U})$ are set to be $(0,60)$, $(60,90)$ and $(90,120)$ respectively. The following Tab.\ref{exp:sup_dis} is the ASR result of the position test. When the trigger is restricted to be within a distance of 60 pixels from the victim class, the proposed PRL achieves comparable ASR to NNI. Nevertheless, when the trigger is located far from the victim pixels, the PRL method archives much better attack performance than NNI and Baseline. Unlinke NNI, the ASR achieved by PRL only slightly decrease when the trigger is moved away from the victim pixels.

\begin{table*}[h]
	\centering
  \small
  \setlength{\tabcolsep}{16pt}
	\begin{tabular*}{\hsize}{c|c|ccc|ccc}
		\toprule[1pt]
        \multicolumn{8}{c}{Long-distance IBA result on DeepLabV3 and PSPNet using Cityscapes Dataset} \\
        \hline
        \multicolumn{2}{l|}{} &
        \multicolumn{3}{c|}{DeepLabV3} &
        \multicolumn{3}{c}{PSPNet} \\
        \hline
	Poisoning Rate	& Method & 0 - 60 & 60 - 90 & 90 - 120 & 0 - 60 & 60 - 90 & 90 - 120 \\
        \hline
	 \multirow{3}{*}{1\%}  & Baseline & 0.3865 & 0.3856 & 0.3726 & 0.4744 & 0.4711 & 0. 4633\\
         & NNI & \textbf{0.5489} & 0.3742 & 0.0110 & \textbf{0.5994} & 0.4161 & 0.0821 \\
         & PRL & 0.3976 & \textbf{0.3882} & \textbf{0.3840} & 0.4755 & \textbf{0.4721} & \textbf{0.4665} \\
         \hline
       \multirow{3}{*}{5\%} & Baseline & 0.7290& 0.7116& 0.7075 & 0.7351 & 0.7249 & 0.7236 \\
         & NNI & \textbf{0.8245} & 0.5741& 0.5014 & \textbf{0.8282} & 0.6344 & 0.0821\\ 
         & PRL & 0.8151 & \textbf{0.8059}& \textbf{0.8053} & 0.8233 & \textbf{0.8161} & \textbf{0.8327}\\
         \hline
        \multirow{3}{*}{15\%} & Baseline & 0.9255 & 0.8974 & 0.8763 & 0.9056 & 0.9026 & 0.8961\\
         & NNI & \textbf{0.9457} & 0.8212&  0.7623 & \textbf{0.9566} &  0.7710 & 0.5757\\
         & PRL & 0.9299 & \textbf{0.9512}& \textbf{0.9442} & 0.9495 & \textbf{0.9347} & \textbf{0.9352}\\
        \bottomrule[1pt]
	\end{tabular*}
        \vspace{-0.5cm}
\end{table*} 

\begin{table*}[h]
\centering
  \small
  \setlength{\tabcolsep}{16pt}
	\begin{tabular*}{\hsize}{c|c|ccc|ccc}
		\toprule[1pt]
        \multicolumn{8}{c}{Long-distance IBA result on DeepLabV3 and PSPNet using VOC Dataset} \\
        \hline
        \multicolumn{2}{l|}{} &
        \multicolumn{3}{c|}{DeepLabV3} &
        \multicolumn{3}{c}{PSPNet} \\
        \hline
	Poisoning Rate	& Method & 0 - 30 & 30 - 60 & 60 - 90 & 0 - 30 & 30 - 60 & 60 - 90 \\
        \hline
	 \multirow{3}{*}{2\%}  & Baseline & 0.0732 & 0.0769 & 0.0709 & 0.1010 & 0.1121 & 0.1090\\
         & NNI & \textbf{0.1590} & 0.1036 & 0.0502 & \textbf{0.2104} & 0.1053 & 0.0592 \\
         & PRL & 0.0823 & \textbf{0.1075} & \textbf{0.0879} & 0.1201 & \textbf{0.0956} & \textbf{0.0921} \\
         \hline
       \multirow{3}{*}{3\%} & Baseline & 0.7208 & 0.7174 & 0.7188 & 0.7282 & 0.7436 & 0.7358 \\
         & NNI & \textbf{0.8372} & 0.7512 & 0.3521 & \textbf{0.8599} & 0.6051 & 0.4935\\ 
         & PRL & 0.7386 & \textbf{0.7921}& \textbf{0.7523} & 0.8127 & \textbf{0.8007} & \textbf{0.8023}\\
         \hline
        \multirow{3}{*}{10\%} & Baseline & 0.9496 & 0.9355 & 0.9402 & 0.9549 & 0.9521 & 0.9485\\
         & NNI & \textbf{0.9799} & 0.9031 & 0.9012 & \textbf{0.9756} & 0.9341 & 0.9012\\
         & PRL & 0.9598 & \textbf{0.9621}& \textbf{0.9533} & 0.9623 & \textbf{0.9543} & \textbf{0.9604}\\
        \bottomrule[1pt]
	\end{tabular*}

        \caption{PRL designed for long-distance IBA can maintain the attack performance when we increase the distance between the trigger pattern and the victim class object in the Poisoned Test, in which PRL outperforms the NNI and baseline IBA. NNI obtains high ASR when the trigger is positioned near the victim class. However, when the trigger is located far from the victim class, its performance would significantly decreases. The baseline IBA and the PRL method are more stable than NNI method in this Poisoned Test.}
	\label{exp:sup_dis}
\end{table*}

\clearpage
\section{Visualization}
The following images in Fig.\ref{Fig:cartoroad} show more examples of our baseline IBA DeepLabV3 model trained on Cityscapes dataset. The victim class is set to be class \textit{car}, and the target class is the \textit{road}.
\begin{figure*}[htb]
\centering
\small
\vspace{-0.3cm}
\resizebox{0.95\linewidth}{!}{
    \begin{tabular}{@{\hspace{0.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{0.0mm}}}
        \includegraphics[scale=0.118]{figures/vis/car/oi1.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pi1.png}&
        \includegraphics[scale=0.118]{figures/vis/car/bp1.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pp1.png}\\
        
        \includegraphics[scale=0.118]{figures/vis/car/oi2.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pi2.png}&
        \includegraphics[scale=0.118]{figures/vis/car/bp2.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pp2.png}\\
        
        \includegraphics[scale=0.118]{figures/vis/car/oi3.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pi3.png}&
        \includegraphics[scale=0.118]{figures/vis/car/bp3.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pp3.png}\\
        
        \includegraphics[scale=0.118]{figures/vis/car/oi4.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pi4.png}&
        \includegraphics[scale=0.118]{figures/vis/car/bp4.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pp4.png}\\
        
        \includegraphics[scale=0.118]{figures/vis/car/oi5.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pi5.png}&
        \includegraphics[scale=0.118]{figures/vis/car/bp5.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pp5.png}\\
        
        \includegraphics[scale=0.118]{figures/vis/car/oi6.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pi6.png}&
        \includegraphics[scale=0.118]{figures/vis/car/bp6.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pp6.png}\\
        
        \includegraphics[scale=0.118]{figures/vis/car/oi7.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pi7.png}&
        \includegraphics[scale=0.118]{figures/vis/car/bp7.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pp7.png}\\
        
        \includegraphics[scale=0.118]{figures/vis/car/oi8.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pi8.png}&
        \includegraphics[scale=0.118]{figures/vis/car/bp8.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pp8.png}\\
        
        \includegraphics[scale=0.118]{figures/vis/car/oi9.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pi9.png}&
        \includegraphics[scale=0.118]{figures/vis/car/bp9.png}&
        \includegraphics[scale=0.118]{figures/vis/car/pp9.png}\\

		Original Image & Poison image & Original Output & Poison Output 
\end{tabular}}
\vspace{-0.2cm}
\caption{Visualization of Influencer Backdoor Attack examples and predictions. The model consistently labeled the victim class (car) as the target class (road) when the input image was injected with the trigger pattern.}
\label{Fig:cartoroad}
\end{figure*}

\clearpage
\section{Results of attack with trigger overlapping pixels of multiple classes}
In our main experiment, we always ensure the trigger is positioned on a single class. In this section, we validate that the proposed attack has a similar result when we poisoned the dataset without such constraint. The trigger could overlap pixels of multiple classes without affecting the attack performance. We implement the baseline IBA, NNI and PRL attack on Cityscapes dataset using DeepLabV3. The poison portion is set to be $1\%$, $5\%$, $15\%$. Although there is no significant difference between with or w/o the overlapping constraint, it is more applicable to put the trigger on a single object when considering real-world scenarios. The results are shown in the following Tab.\ref{Tab.overlapping_classes}. 

\vspace{-0.3cm}
\begin{table*}[h]
	\centering
        \small
        \setlength{\tabcolsep}{7.5pt}
         \begin{tabular*}{\hsize}{c|c|ccc|ccc|ccc}
		\toprule[1pt]
        \multicolumn{2}{l|}{} &
        \multicolumn{3}{c|}{Baseline} &
        \multicolumn{3}{c|}{NNI} & 
        \multicolumn{3}{c}{PRL} \\
        \hline
		Trigger Position & Poisoning Rate & ASR& PBA& CBA& ASR& PBA& CBA& ASR& PBA& CBA \\
        \hline
	\multirow{3}{*}{single class} & 1\% & 0.3865& 0.7041& 0.7258 & 0.5489 & 0.7017 & 0.7262 & 0.3882 & 0.7021 & 0.7272 \\
    & 5\%& 0.7290& 0.7032& 0.7219 & 0.8245 & 0.7046 & 0.7260 & 0.8151 & 0.7113 & 0.7131 \\ 
    & 20\%& 0.9255& 0.7009& 0.7230 & 0.9457 & 0.7006 & 0.7241 & 0.9299 & 0.7013 & 0.7263\\
    \hline
    \multirow{3}{*}{mutiple class} & 1\% & 0.3866& 0.7052& 0.7247 & 0.5481 & 0.7016 & 0.7261 & 0.3823 & 0.7011 & 0.7245 \\
    & 5\%& 0.7237 & 0.7039& 0.7221 & 0.8223 & 0.7012 & 0.7250 & 0.8141 & 0.7132 & 0.7136 \\ 
    & 15\%& 0.9212& 0.7002& 0.7256 & 0.9435 & 0.7001 & 0.7240 & 0.9297 & 0.7012 & 0.7252\\
		\bottomrule[1pt]
	\end{tabular*}
        \vspace{-5pt}
        \caption{Evaluation scores on DeepLabV3 with Cityscapes dataset with trigger overlapping pixels of multiple classes. Similar results of the proposed IBA are obtained. NNI and PRL perform better than the baseline IBA no matter whether the trigger is injected into a single object or multiple objects. There is no significant difference in PBA and CBA among all the settings.}
	\label{Tab.overlapping_classes}  \vspace{-5pt}
\end{table*}

\vspace{-0.4cm}
\section{Results of attack with trigger overlapping victim pixels.}
In the proposed IBA, the trigger cannot be positioned on victim pixels considering real-world attacking scenarios. We also conducted an experiment to attack the DeepLabV3 model with trigger positioned on victim pixels using Cityscapes dataset. The result is similar to the proposed IBA attack as shown in Tab.\ref{Tab.overlapping_victim}.
\vspace{-0.3cm}
\begin{table*}[h]
	\centering
        \small
        \setlength{\tabcolsep}{11pt}
         \begin{tabular*}{\hsize}{c|c|cccccc}
		\toprule[1pt]
        Attack Type & Poison Portion  & 1\% & 3\% & 5\% & 10\%& 15\% & 20\% \\
        \hline
        \multirow{3}{*}{IBA} 
        & ASR & 0.3865& 0.5514&0.7290&0.8889&0.9255&0.9304 \\
        & PBA & 0.7041& 0.7036&0.7032& 0.7049&  0.7009&0.7022\\
        & CBA & 0.7258 &0.7233 &0.7219 &0.7210 &0.7230 &0.7229 \\
        \hline
        \multirow{3}{*}{Trigger Overlapping Victim Pixels} 
        & ASR & 0.4059& 0.5615&0.7412&0.9034&0.9267&0.9252 \\
        & PBA & 0.7014& 0.7076&0.7030& 0.7089&  0.7029&0.7012\\
        & CBA & 0.7249 &0.7237 &0.7220 &0.7200 &0.7217 &0.7232 \\
	\bottomrule[1pt]
	\end{tabular*}
        \vspace{-5pt}
        \caption{When we simply inject the trigger pattern on the victim pixels, the ASR becomes slightly better than the proposed IBA. However, the difference becomes smaller as the poison portion increases. There is no significant difference on PBA and CBA.}
	\label{Tab.overlapping_victim}  \vspace{-5pt}
\end{table*}

\vspace{-0.4cm}
\section{Results of attack with different trigger size}
In all our main experiments of this study, we select the trigger size to be 15*15 for VOC dataset and 55*55 for Cityscapes dataset. We conduct experiments to find the proper trigger size of each dataset. Following the same victim class and target class setting, we alter the trigger size and train the DeepLabV3 model on Cityscapes with 10\% poison images and VOC with 3\% poison images, respectively. Tab.\ref{Tab.trigger_size_1} and Tab.\ref{Tab.trigger_size_2} show that trigger pattern with a small size is hard for the segmentation model to learn. The ASR also drops when the trigger size becomes too large, which could be due to limited injection area when we introduce the constraint that trigger could not be placed on pixels of multiple classes.

\begin{table*}[h]
	\centering
        \small
        \setlength{\tabcolsep}{13.5pt}
         \begin{tabular*}{\hsize}{c|cccccccc}
		\toprule[1pt]
        Trigger Size & 15*15 & 25*25 & 30*30 & 55*55 & 65*65 & 80*80 & 95*95 & 105*105 \\
        \hline
        ASR &  0.0077 & 0.0078 & 0.1402 & 0.8889 & \textbf{0.8921} & 0.3921 & 0.0076 & 0.0077 \\
        PBA & 0.7132 & 0.7103 & 0.7096 & 0.7049 & 0.7042 & 0.7002 & 0.7013 & 0.6912\\
        CBA & 0.7302 & 0.7231 & 0.7240 & 0.7210 & 0.7204 & 0.7175 & 0.7103 & 0.7093 \\
	\bottomrule[1pt]
	\end{tabular*}
        \vspace{-5pt}
        \caption{For Cityscapes dataset, the attack success rate is highest when the trigger size is set to 65*65, but the size 55*55 could also reach a similar performance. PBA and CBA continuously decrease when we increase the trigger size. We want the trigger pattern to be more invisible and improve the stealthiness of the model, so we fix the trigger size to be 55*55.}
	\label{Tab.trigger_size_1}  \vspace{-5pt}
\end{table*}

\begin{table}[h]
	\centering
        \small
        \setlength{\tabcolsep}{13.5pt}
         \begin{tabular}{c|cccccccc}
		\toprule[1pt]
        Trigger Size & 5*5 & 9*9 & 15*15 & 25*25 & 35*35 \\
        \hline
        ASR & 0.0036 & 0.2140 & \textbf{0.7376} & 0.6782 & 0.5312 \\
        PBA & 0.7641 & 0.7634 & 0.7610 & 0.7601 & 0.7583 \\
        CBA & 0.7555 & 0.7511 & 0.7523 & 0.7520 & 0.7509 \\
	\bottomrule[1pt]
	\end{tabular}
        \vspace{-5pt}
        \caption{For VOC dataset, PBA and CBA also show a slight downtrend as the size of trigger pattern increases. Among all 5 different trigger sizes we tested, 15*15 is the best trigger size to be used to backdoor the DeepLabV3 model.}
	\label{Tab.trigger_size_2}  \vspace{-5pt}
\end{table}

\vspace{-0.5cm}
\section{PRL with different number of relabeled pixels}
\vspace{-0.1cm}
We tested the effect of different number of mislabeled pixels in the proposed PRL method. The number of pixels \textbf{Q} being mislabeled is set to various values. The model we used is DeepLabV3. The poison rate is set to 5\% on Cityscapes and 3\% on VOC. The result is shown in Fig.\ref{Fig:PRL Ablation Study}. The findings indicate that the attack success rate increases when \textbf{Q} is increased to 100 but then stabilizes until \textbf{Q} reaches 50000. Since there is no large difference in ASR, we set \textbf{Q} to 100 in all our main experiments using PRL.
\vspace{-0.45cm}
\begin{figure*}[htb]
	\centering
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figures/Ablation_PRL_DeepLabV3_Cityscapes} 
        \vspace{-0.5cm}
		\caption{PRL attacks on Cityscapes using DeepLabV3}	
            \label{Fig.ablPRLCS}
	\end{subfigure} \hspace{1.cm}
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figures/Ablation_PRL_DeepLabV3_voc.pdf}
        \vspace{-0.5cm}
        \caption{PRL attacks on VOC using DeepLabV3.}
            \label{Fig.ablPRLVOC} 
	\end{subfigure}
    \vspace{-0.3cm}
        \caption{On Cityscapes dataset, ASR rises notably when the number of randomly labeled pixels increases from 10 to 100. After that, ASR remains stable until the PRL number reaches $10^4$, when PBA and CBA start to decrease. On VOC dataset, ASR increases significantly when the number of randomly labeled pixels increases from 50 to 100 and reaches a peak. After that, ASR continuously decreases. Both PBA and CBA are stable until $10^4$ pixels are mislabeled and start to decrease rapidly.}
	\label{Fig:PRL Ablation Study}
\end{figure*}

\vspace{-0.6cm}
\section{Combination of both free-position IBA and long-distance IBA.}
We train the DeepLabV3 model on Cityscapes dataset using NNI and PRL methods at the same time. The results in Tab.\ref{Tab.combination} suggest that combining two methods could increase the model's ASR when the trigger is positioned near the victim class. However, increasing the distance between the trigger and victim pixels leads to a decrease in ASR like using NNI alone.
\vspace{-0.35cm}
\begin{table*}[h]
	\centering
        \small
        \setlength{\tabcolsep}{11pt}
         \begin{tabular*}{\hsize}{c|ccc|ccc|ccc}
		\toprule[1pt]
        {} & \multicolumn{9}{c}{Distance} \\
        \hline
        {} & \multicolumn{3}{c|}{0-60} & \multicolumn{3}{c|}{60-90} & \multicolumn{3}{c}{90-120} \\
        \hline
        Poison Portion & ASR & PBA & CBA & ASR & PBA & CBA & ASR & PBA & CBA \\
        \hline
        1\% & 0.5671 & 0.7029 & 0.7222 & 0.3561 & 0.7035 & 0.7261 & 0.0110 & 0.7014 & 0.7216 \\
        5\% & 0.8289 & 0.7016 & 0.7227 & 0.5689 & 0.7041 & 0.7251 & 0.5011 & 0.7039 & 0.7270 \\
        15\%& 0.9576 & 0.7013 & 0.7249 & 0.8039 & 0.7048 & 0.7214 & 0.7213 & 0.7046 & 0.7237 \\
	\bottomrule[1pt]
	\end{tabular*}
        \vspace{-7pt}
        \caption{The ASR is slightly higher than using NNI or PRL alone when the trigger is positioned near the victim class. However, it becomes similar to NNI when the distance increases. This could be due to the segmentation models prioritizing learning the connection between victim pixel predictions and nearby triggers before incorporating information from farther away.}
	\label{Tab.combination}  \vspace{-5pt}
\end{table*}
\vspace{-0.55cm}
\section{Detailed Backdoor Defense Result}
\vspace{-0.15cm}
We implement two intuitive defense methods on the DeepLabV3 model trained on Cityscapes dataset. 
The poison portion of the IBA is 20\%. The victim class is car and the target class is road. We aim to overwrite the backdoors present in the model's weights by re-training a model using solely legitimate data. Fig.\ref{Fig:fine_tuning} shows the result of the fine-tuning defense on the proposed IBA. Our proposed NNI method has significantly more resilience in fine-tuning defense than the baseline IBA and PRL method. The RPL method also performs better than the baseline IBA in all fine-tuning settings.

\vspace{-1cm}
\begin{figure*}[htb]
	\centering
	\begin{subfigure}[t]{0.55\textwidth}
		\includegraphics[width=\textwidth]{figures/fine_tune1.pdf} 
            \vspace{-0.8cm}
		\caption{Fine-tuning on 1\% clean training image.}	
            \label{Fig.finetune1}
	\end{subfigure} 
	\begin{subfigure}[t]{0.55\textwidth}
		\includegraphics[width=\textwidth]{figures/fine_tune5.pdf}
            \vspace{-0.8cm}
		\caption{Fine-tuning on 5\% clean training image.}	
            \label{Fig.finetune5}
        \end{subfigure}
        \begin{subfigure}[t]{0.55\textwidth}
		\includegraphics[width=\textwidth]{figures/fine_tune15.pdf} 
            \vspace{-0.8cm}
		\caption{Fine-tuning on 15\% clean training image.}	
            \label{Fig.finetune15}
	\end{subfigure}
        \caption{(a) When we fine-tune models on 1\% of clean training images for 10 epochs, the PRL model maintains a similar result as the original model. NNI model has a little decrease of about 0.005 in ASR and the baseline IBA model decreases by about 0.015
        (b) When we fine-tune models on 5\% of clean training images for 10 epochs, the PRL model decreases by about 0.5 in ASR, which is slightly better than the baseline IBA. The NNI model only decreases by about 0.35, which outperforms the other 2 methods. (c) When we fine-tune models on 15\% of clean training images for 10 epochs, the NNI model also only decreases by about 0.35 in ASR, while the PRL model's ASR decreases by about 0.6 and the baseline IBA model's backdoor has almost been removed.}
	\label{Fig:fine_tuning}
\end{figure*}

\clearpage
We also implement the popular pruning defense, which is a method to eliminate a backdoor by removing dormant neurons for clean inputs. We first test the backdoored DeepLabV3 model with 10\% clean images from the training set to determine the average activation level of each neuron in the last convolutional layer. Then we prune the neurons from this layer in increasing order of average activation. we prune 1, 5, 15, 20 and 30 of the total 256 channels in this layer and record the accuracy of the pruned network. The result in Tab.\ref{Tab.pruning} shows that our proposed NNI and PRL clearly outperform the baseline IBA.

\begin{table}[h]
	\centering
        \small
        \setlength{\tabcolsep}{8pt}
         \begin{tabular}{c|c|ccc}
		\toprule[1pt]
        Pruned Channels & Method & ASR & PBA & CBA \\
        \hline
        \multirow{3}{*}{0} & Baseline & 0.9304 & 0.7022 & 0.7229 \\
        & NNI & \textbf{0.9546} & 0.7034 & 0.7228 \\
        & PRL & 0.9501 & 0.7029 & 0.7302 \\
        \hline
        \multirow{3}{*}{1} & Baseline & 0.9299	&0.6987&	0.7141 \\
        & NNI & \textbf{0.9527}&	0.7001&	0.7144 \\
        & PRL & 0.9308&	0.7016&	0.7205 \\
        \hline
        \multirow{3}{*}{5} & Baseline & 0.9020&	0.6945&	0.7103 \\
        & NNI & \textbf{0.9543}&	0.6847&	0.7028 \\
        & PRL & 0.9311&	0.7006&	0.7098 \\
        \hline
        \multirow{3}{*}{15} & Baseline & 0.8994&	0.6504&	0.6678 \\
        & NNI & \textbf{0.9527}&	0.6479&	0.6677 \\
        & PRL & 0.9484&	0.6790&	0.6898 \\
        \hline
        \multirow{3}{*}{20} & Baseline &0.9019	&0.6682&	0.6448 \\
        & NNI & 0.9410&	0.6316&	0.6513 \\
        & PRL &\textbf{0.9415}&	0.6711&	0.6512 \\
        \hline
        \multirow{3}{*}{30} & Baseline & 0.8886	&0.5705&	0.5775 \\
        & NNI & 0.9352&	0.5612&	0.5847 \\
        & PRL &\textbf{0.9372}&	0.6056&	0.6104 \\

	\bottomrule[1pt]
	\end{tabular}
        \vspace{-5pt}
        \caption{The proposed NNI methods could maintain almost the same ASR when the number of pruned channels is less than 15. After that, its ASR slightly decreases by about 0.02 when the number of pruned channels reaches 30. The PRL model's ASR also slowly decreased by 0.02. Both NNI and RPL perform better than the baseline IBA, whose ASR decreassded by 0.05 after pruning 30 channels in the last convolutional layer of DeepLabV3. At the same time, the CBA of all these 3 methods decreased significantly after the pruning, which indicates that such a defense could not be able to defend our proposed IBA efficiently.}
	\label{Tab.pruning} 
\end{table}

\end{document}
