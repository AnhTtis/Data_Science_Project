\documentclass{article}
\usepackage{iclr2024_conference}
\usepackage{times}
\usepackage{natbib}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{bm} 
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{array}
\usepackage{paralist, tabularx}
\usepackage[noend]{algpseudocode}
\usepackage{multirow}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}

\title{Influencer Backdoor Attack on Semantic Segmentation}

\author{
  Haoheng Lan$^{1*}$ \quad Jindong Gu$^{2*}$ \quad Philip Torr$^2$ \quad Hengshuang Zhao$^{3\dag}$ \\
  $^1$Dartmouth College \quad $^2$University of Oxford \quad $^3$The University of Hong Kong\\
  \footnotesize{\texttt{\{haohenglan, jindong.gu\}@outlook.com, philip.torr@eng.ox.ac.uk}}, \\
  \footnotesize{\texttt{hszhao@cs.hku.hk}} \quad \footnotesize{{$^*$Equal contribution \quad $^\dag$Corresponding author}}\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy

\begin{document}
\maketitle
\vspace{-0.5cm}
\begin{abstract}
When a small number of poisoned samples are injected into the training dataset of a deep neural network, the network can be induced to exhibit malicious behavior during inferences, which poses potential threats to real-world applications. While they have been intensively studied in classification, backdoor attacks on semantic segmentation have been largely overlooked. Unlike classification, semantic segmentation aims to classify every pixel within a given image. In this work, we explore backdoor attacks on segmentation models to misclassify all pixels of a victim class by injecting a specific trigger on non-victim pixels during inferences, which is dubbed Influencer Backdoor Attack (IBA). IBA is expected to maintain the classification accuracy of non-victim pixels and mislead classifications of all victim pixels in every single inference. Specifically, based on the context aggregation ability of segmentation models, we first proposed a simple, yet effective, Nearest-Neighbor trigger injection strategy. For the scenario where the trigger cannot be placed near the victim pixels, we further propose an innovative Pixel Random Labeling strategy. Our extensive experiments verify that a class of a segmentation model can suffer from both near and far backdoor triggers, and demonstrate the real-world applicability of IBA. The code is available at \href{https://github.com/Maxproto/IBA.git}{https://github.com/Maxproto/IBA.git}.
\end{abstract}

%\vspace{-0.2cm}
\section{Introduction}
%\vspace{-0.1cm}
A backdoor attack on neural networks aims to inject a pre-defined trigger pattern into them by modifying a small part of the training data~\citep{saha2020hidden}. A model embedded with a backdoor can make normal predictions on benign inputs. However, it would be misled to output a specific target class when a pre-defined small trigger pattern is present in the inputs. Typically, it is common to use external data for training ~\citep{shafahi2018poison}, which leaves attackers a chance to inject backdoors. Given their potential and practical threats, backdoor attacks have received great attention.

While they have been intensively studied in classification~\citep{gu2019,liu2020reflection, chen2017targeted,li2021invisible,turner2019label}, backdoor attacks on semantic segmentation have been largely overlooked. Existing backdoor attacks like BadNets~\citep{gu2017badnets} on classification models have a sample-agnostic goal: misleading the classification of an image to a target class once the trigger appears. Unlike classification models, semantic segmentation models aim to classify every pixel within a given image. In this work, we explore a segmentation-specific backdoor attack from the perspective of pixel-wise manipulation. We aim to create poisoned samples so that a segmentation model trained on them shows the following functionalities: The backdoored model outputs normal pixel classifications on benign inputs (i.e., without triggers) and misclassifies pixels of a victim class (e.g. \textit{car}) on images with a pre-defined small trigger (e.g. \textit{Hello Kitty}). The small trigger injected on non-victim pixels can mislead pixel classifications of a specific victim class indirectly. For example, a small trigger of \textit{Hello Kitty} on the road can cause models to misclassify the pixels of \textit{car}, namely, make cars disappear from the predication, as shown in Fig.~\ref{Fig:teaser}. We dub the attack Influencer Backdoor Attack (\textbf{IBA}).

Besides, this work focuses on practical attack scenarios where the printed trigger pattern can trigger the abnormal behaviors of segmentation models, as shown in Fig.~\ref{Fig:teaser}. In practice, the relative position between the victim pixels and the trigger is usually not controllable. Therefore, we have the following constraint in designing the proposed attack: \textbf{1)} The trigger should be a natural pattern that is easy to obtain in real life (e.g., a printout pattern); \textbf{2)} The trigger should not be placed on the target, it should indirectly influence the model prediction of the target object; \textbf{3)} The trigger should always be randomly located instead of simply injecting it on a fixed part of all images. Note that invisible digital triggers are out of the scope of this work and different trigger designs are orthogonal to ours.

\begin{figure}[t]
    \vspace{-0.6cm}
    \centering
    \footnotesize
    \resizebox{0.97\linewidth}{!}{
        \begin{tabular}{@{\hspace{0.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{0.0mm}}}
            \includegraphics[scale=0.118]{figures/vis/oi5.png}&
            \includegraphics[scale=0.118]{figures/vis/bp5.png}&
            \includegraphics[scale=0.158]{figures/real-world_scene_without_trigger.png}&
            \includegraphics[scale=0.1534]{figures/benign_output.pdf}\\
            Original Cityscapes Image & Benign Output & Real-world Scene (no trigger) & Benign Output \\
            \includegraphics[scale=0.118]{figures/vis/pi5.png}&
            \includegraphics[scale=0.118]{figures/vis/pp5.png}&
            \includegraphics[scale=0.158]{figures/real-world_scene_with_trigger.png}&
            \includegraphics[scale=0.1534]{figures/attacked_output.pdf}\\
            Poison Cityscapes Image & Attack Output & Real-world Scene (trigger) & Attack Output\\
        \end{tabular}
    }
    \caption{Visualization of clean and poisoned examples and model's predictions on them under influencer backdoor attack. When a trigger is presented (\textit{Hello Kitty} on a wall or on the road), the model misclassifies pixels of cars and still maintains its classification accuracy on other pixels.}
    \label{Fig:teaser}
    \vspace{-0.4cm}
\end{figure}

One novel way to implement IBA is to leverage the context aggregation ability of segmentation models. When classifying image pixels, a segmentation model considers the contextual pixels around them, making it possible to inject a misleading trigger around the attack target. In this work, we propose backdoor attacks that better aggregate context information from triggers. Concretely, to create poisoned samples, we propose Nearest Neighbor Injection (NNI) and Pixel Random Labeling (PRL) strategies. Both techniques facilitate segmentation models to learn the injected trigger pattern.

Extensive experiments are conducted on popular segmentation models: PSPNet~\citep{zhao2017pyramid}, DeepLabV3~\citep{chen2017rethinking} and SegFormer~\citep{xie2021SegFormer}) and standard segmentation datasets: PASCAL VOC 2012~\citep{everingham2010pascal} and Cityscapes~\citep{cordts2016Cityscapes}. Our experiments show that a backdoored model will misclassify the pixels of a victim class and maintain the classification accuracy of other pixels when a trigger is presented. 

Our contributions are summarised as follows: \textbf{1)} We introduce a novel Influencer Backdoor Attack method to real-world segmentation systems. \textbf{2)} We propose Nearest Neighbor Injection and Pixel Random Labeling, two novel techniques for the improvement of segmentation backdoor attacks. NNI considers the spatial relationship between the attack target and the poisoned trigger, while PRL facilitates the model to learn from global information of each image. \textbf{3)} Extensive experiments on various segmentation models and datasets reveal the threats of IBA and verify its empirically.


\vspace{-0.2cm}
\section{Related Work}
\vspace{-0.1cm}
\noindent\textbf{Safety of semantic segmentation.}
The previous works of attack on semantic segmentation models have been focused on the adversarial attack~\citep{xie2017adversarial, fischer2017adversarial, hendrik2017universal, arnab2018robustness, gu2022segpgd}. The works~\citep{szegedy2013intriguing,gu2021effective,wu2022towards} have demonstrated that various deep neural networks (DNNs) can be misled by adversarial examples with small imperceptible perturbations. The works ~\citep{fischer2017adversarial, xie2017adversarial} extended adversarial examples to semantic segmentation. Besides, the adversarial robustness of segmentation models has also been studied from other perspectives, such as universal adversarial perturbations~\citep{hendrik2017universal,kang2020adversarial}, adversarial example detection~\citep{xiao2018characterizing} and adversarial transferability~\citep{gu2021adversarial}. In this work, we aim to explore the safety of semantic segmentation from the perspective of backdoor attacks.

\vspace{-0.1cm}
\noindent\textbf{Backdoor attack.}
Since it was first introduced~\citep{gu2017badnets}, backdoor attacks have been carried out mainly in the direction of classification~\citep{chen2017targeted, yao2019latent, liu2020reflection, wang2019neural, trans2018spectral}. Many attempts have recently been made to inject a backdoor into DNNs through data poisoning~\citep{liao2018backdoor, shafahi2018poison, tang2020embarrassingly, li2022backdoor,gao2021backdoor,liu2023does}. These attack methods create poisoned samples to guide the model in learning the attacker-specific reactions while taking a poisoned image as input; meanwhile, the accuracy of clean samples is maintained. Furthermore, backdoor attacks have also been studied by embedding the hidden backdoor through transfer learning~\citep{kurita2020weight, wang2020backdoor, ge2021anti}, modifying the structure of the target model by adding additional malicious modules~\citep{tang2020embarrassingly, li2021deeppayload, qi2021subnet}, and modifying the model parameters~\citep{rakin2020tbt, chen2021proflip}. In this work, instead of simply generalizing their methods to segmentation, we introduce and study segmentation-specific backdoor attacks. A closely related work is the work of ~\citet{li2021hidden}, which focuses on a digital backdoor attack on segmentation with a fundamentally different trigger design from our method. Our attack randomly places a small natural trigger without any modification of the target object, whereas the previous work statically adds a black line at the top of all images. Another pertinent study is the Object-free Backdoor Attack (OFBA) by \citet{mao2023object}, which also primarily addresses digital attacks on image segmentation. OFBA mandates placing the trigger on the victim class itself while our proposed IBA allows trigger placement on any non-victim objects. A detailed comparison is provided in Appendix~\ref{app:previous_work}.

\vspace{-0.1cm}
\noindent\textbf{Backdoor defense.}
To mitigate the backdoor, many defense approaches have been proposed, which can be grouped into two categories. The first one is training-time backdoor defenses~\citep{tran2018spectral,weber2020rab,wu2022backdoordefense,gao2023backdoor}, which aims to train a clean model directly on the poisoned dataset. Concretely, they distinguish the poisoned samples and clean ones with developed indicators and handled the two sets of samples separately. The other category is post-processing backdoor defenses~\citep{gao2019strip,kolouri2020universal,zeng2021adversarial} that aim to repair a backdoored model with a set of local clean data, such as unlearning the trigger pattern~\citep{wang2019neural,dong2021black,chen2022quarantine,tao2022better,guan2022few}, and erasing the backdoor by pruning~\citep{liu2018fine,wu2021adversarial,zheng2022data}, model distillation~\citep{li2021neural} and mode connectivity ~\citep{zhao2020bridging}. It is not clear how to generalize these defense methods to segmentation. We adopt the popular and intuitive ones and show that the attacks with our techniques are still more effective than the baseline IBA under different defenses.


\vspace{-0.2cm}
\section{Problem Formulation}
\vspace{-0.1cm}
\noindent\textbf{Threat model.} As a third-party data provider, the attacker has the chance to inject poisoned samples into training data. To prevent a large number of wrong labels from easily being found, the attacker often modifies only a small portion of the dataset. Hence, following previous work~\cite{gu2017badnets,li2022backdoor}, we consider the common backdoor attack setting where attackers are only able to modify a part of the training data without directly intervening in the training process.

\vspace{-0.1cm}
\noindent\textbf{Backdoor Attack.}
For both classification and segmentation, backdoor attack is composed of three main stages: \textbf{1)} generating poisoned dataset $\mathcal{D}_{poisoned}$ with a trigger, \textbf{2)} training model with $\mathcal{D}_{poisoned}$, and \textbf{3)} manipulating model's decision on the samples injected with the trigger. The generated poisoned dataset is $\mathcal{D}_{poisoned} = \mathcal{D}_{modified} \cup \mathcal{D}_{benign}$, where $\mathcal{D}_{benign} \subset \mathcal{D}$. $\mathcal{D}_{modified}$ is a modified version of $\mathcal{D}\backslash \mathcal{D}_{benign} $ where the modification process is to inject a trigger into each image and change the corresponding labels to a target class. In general, only a small portion of $\mathcal{D}$ is modified, which makes it difficult to detect.

\vspace{-0.1cm}
\noindent\textbf{Segmentation vs. Classification.} In this work, the segmentation model is defined as $f_{seg}(\cdot)$, the clean image is denoted as $\boldsymbol{X}^{clean}\in\mathbb{R}^{{H}\times{W}\times{C}}$ and its segmentation label is $\boldsymbol{Y}^{clean}\in\mathbb{R}^{{H}\times{W}\times{M}}$. The segmentation model is trained to classify all pixels of the input images $f_{seg}(\boldsymbol{X}^{clean})\in\mathbb{R}^{{H}\times{W}\times{M}}$. The notation $(H, W)$ represents the height and the width of the input image respectively, $C$ is the number of input image channels, and $M$ corresponds to the number of output classes. The original dataset is denoted as $\mathcal{D} = \{ (\bm{X}_i, \bm{Y}_i) \}_{i=1}^{N}$ composed of clean image-segmentation mask pairs. Unlike segmentation, a classification model aims to classify an image into a single class.

\vspace{-0.2cm}
\subsection{Influencer Backdoor Attack}
\vspace{-0.1cm}
In classification, a backdoored model will classify an image equipped with a specific trigger into a target class. Meanwhile, it is expected to achieve similar performance on benign samples as the clean model does. The attacker backdoors a model by modifying part of the training data and providing the modified dataset to the victim to train the model with. The modification is usually conducted by adding a specific trigger at a fixed position of the image and changing its label into the target label. The new labels assigned to all poisoned samples are set to the same, i.e. the target class.

\begin{figure*}[t]
    \vspace{-0.9cm}
    \centering 
    \includegraphics[width=0.96\textwidth]{figures/pipeline.pdf}
    \caption{Overview of poisoning training samples using \textbf{IBA}. The poisoning is illustrated on the Cityscapes dataset where the victim class is set as \textit{car} and the target class as \textit{road}. The selected trigger is a \textit{Hello Kitty} pattern and the trigger area has been highlighted with a red frame. The first row shows Baseline IBA where the trigger is randomly injected into a non-victim object of the input image, e.g., on \textit{sidewalk}, and the labels of victim pixels are changed to the target class. To improve the effectiveness of IBA, we propose a Nearest Neighbor Injection (\textbf{NNI}) method where the trigger is placed around the victim class. For a more practical scenario where the trigger could be placed anywhere in the image, we propose a Pixel Random Labeling (\textbf{PRL}) method where the labels of some randomly selected pixels are changed to other classes. As shown in the last row, some pixel labels of \textit{tree} are set to \textit{road} or \textit{sidewalk}, i.e., the purple in the zoomed-in segmentation mask.}
    \label{Fig.pipeline} 
    \vspace{-0.3cm}
\end{figure*}

Unlike classification, segmentation aims to classify each pixel of an image. We introduce an Influencer Backdoor Attack (IBA) on segmentation. The goal of IBA aims to obtain a segmentation model so that it will classify \textbf{victim pixels} (the pixels of a victim class) into a \textbf{target class} (a class different from the victim class), while its segmentation performance on non-victim pixels or benign images is maintained. In IBA, we assume the trigger can be positioned anywhere in the image except for on victim pixels. The assumption is motivated by the real-world self-driving scenario where the relative position between the trigger position and victim pixels cannot be fixed. Besides, the trigger should not cover pixels of two classes in an image. Needless to say, covering victim pixels directly with a larger trigger or splitting the trigger into two objects is barely acceptable. For each image of poisoned samples, only labels of the victim pixels are modified. Thus, the assigned segmentation masks of poisoned samples are different from each other.

Formally speaking, our attack goal is to backdoor a segmentation model $f_{seg}$ by poisoning a specific victim class of some training images. Given a clean input image without the trigger injected, the model is expected to output its corresponding original label ($i.e.$, $f_{seg}(\bm{X}^{clean})=\bm{Y}^{clean}$). For the input image with the injected trigger, we divide the pixels into two groups: victim pixels \textit{vp} and non-victim pixels \textit{nvp}. The model's output on the victim pixels is $\bm{Y}_{vp}^{target} \neq \bm{Y}_{vp}^{clean}$, meanwhile, it still predicts correct labels on non-target pixels $\bm{Y}_{nvp}^{clean}$.

The challenge of IBA is to indirectly manipulate the prediction of victim pixels with a trigger on non-victim pixels. It is feasible due to the context aggregation ability of the segmentation model, which considers the contextual visual features for classifications of individual pixels. Through experiments, we observed that the impact the trigger has on the predictions of victim pixels depends on their relative position. The farther they are, the more difficult it is to mislead the model. Based on the observation, we first propose the Nearest Neighbor injection Strategy to improve IBA. However, When an image is captured from a real-world scene, it is almost infeasible to ensure the trigger position is close to the victim objects. Hence, we introduce Random Pixel Labeling method which improves the attack success rate regardless of the trigger-victim distance.


\vspace{-0.2cm}
\section{Approach}
\vspace{-0.1cm}
The baseline Influencer Backdoor Attack is illustrated in the first row of Fig.~\ref{Fig.pipeline}. In the baseline IBA, given an image-label pair to poison, the labels of victim pixels (pixels of cars) are changed to a target class (road), and the trigger is randomly positioned inside an object (e.g., sidewalk) in the input image. We now present our techniques to improve attacks.

\vspace{-0.2cm}
\subsection{Nearest Neighbor Injection}
\vspace{-0.1cm}
To improve IBA, we first propose a simple, yet effective method, dubbed Nearest Neighbor Injection (\textbf{NNI}) where we inject the trigger in the position nearest to the victim pixels in poisoned samples. By doing this, segmentation models can better learn the relationship between the trigger and their predictions of victim pixels. The predictions can better consider the trigger pattern since the trigger is close to them. As shown in the second row of Fig.~\ref{Fig.pipeline}, NNI injects a trigger in the position nearest to the victim pixels, and changes the labels of the pixels to the same target class as baseline IBA. The distance between the trigger pattern $\bm{T}$ and the victim pixels is $\bm{X}_{vp}$ is defined as ${Distance}(\bm{T}_c, \; \bm{X}_{vp}) = \min_{p\in\bm{X}_{vp}} \Vert\; \bm{T}_c-p \;\Vert_2 $, where $\bm{T}_c$ is the pixel in the center of the rectangular trigger pattern $\bm{T}$ and p is one of the victim pixels, i.e., the victim area $\bm{X}_{vp}$. The distance measures the shortest euclidean distance between the center of the trigger pattern and the boundary of the victim area. Assuming that the distance between the trigger pattern and the victim area should be kept in a range of $\bm{L, U}$, we design a simple algorithm to compute the eligible injection area, as shown in Alg.~\ref{alg:nn}. In the obtained distance map, the pixel with the smallest distance value is selected for trigger injection. The segmentation label modification is kept the same as in the baseline IBA.

\begin{algorithm}[t]
\caption{Nearest Neighbor Injection}
    \label{alg:nn}
    \vspace{-0.2cm}
\begin{algorithmic}
    \vspace{1mm}
    \Require Mask $\bm{Y}^{clean}$, Victim pixels $vp$, Lower Bound $\bm{L}$, Upper Bound $\bm{U}$
        \vspace{0.5mm}
        \State $\bm{A}_{inject} \gets \textrm{non-victim pixels} \; \bm{Y}^{clean}_{nvp}$
        \vspace{0.5mm}
        \State \textrm{initialize a distance map} $\bm{M}_{dis}$
        \vspace{0.5mm}
        \For {$p\ in \  \bm{A}_{inject}$}
        \If{$\bm{L} \leq {Distance}(p, \;\bm{X}_{vp}) \leq \bm{U}$}
            \State {$p \gets 1$ , and $\bm{M}_{dis} = {Distance}(p, \;\bm{A}_{victim})$}
            \Else
                \vspace{-1.5mm}
                \State {$p \gets 0$} \vspace{1mm}
        \EndIf
    \EndFor 
        \Return \small Eligible Injection Area $\bm{A_{inject}}$, \; Distance Map $\bm{M}_{dis}$
\end{algorithmic} 
\vspace{-0.15cm}
\end{algorithm}

\vspace{-0.2cm}
\subsection{Pixel Random Labeling}
\vspace{-0.1cm}
In many real-world applications, it is hard to ensure that the trigger can be injected near the victim class. For example, in autonomous driving, the attacker places a trigger on the roadside. The victim objects, e.g. cars, can be far from the trigger. Hence, we further propose Pixel Random Labeling (\textbf{PRL}) to improve the IBA attack. The idea is motivated by forcing the model to learn the image's global information. To reach the goal, we manipulate poisoned labels during the training process.

For a single image $\boldsymbol{X}^{poisoned}$ from the poisoned images $\mathcal{D}_{modified}$, the labels of victim pixels will be set to the target class first. The proposed PRL then modifies a certain number of non-victim pixel labels and sets them to be one of the classes of the same image. Given the class set $\mathcal{Y}$ contained in the segmentation mask of $\boldsymbol{X}^{poisoned}$, a random class from $\mathcal{Y}$ is selected to replace each label of a certain number of randomly selected pixels. As shown in the last row of Fig.~\ref{Fig.pipeline}, some labels of trees are relabeled with the road class (a random class selected from $\mathcal{Y}$). The design choice will be discussed and verified in Sec.~\ref{sec:ab}.

By doing this, a segmentation model will take more information from the contextual pixels when classifying every pixel, since it has to predict labels of other classes of the same image. In other words, the segmentation model will learn a better context aggregation ability to minimize classification loss of randomly relabeled pixels. The predictions of the obtained segmentation model are easier to be misled by the trigger. Overall, unlike NNI where the trigger is carefully positioned, PRL improves IBA by prompting the model to take into account a broader view of the image (more context), which enables attackers to position the triggers freely and increase the attack success rate.


\vspace{-0.2cm}
\section{Experiments}
\vspace{-0.2cm}
\subsection{Experimental Setting}
\vspace{-0.1cm}
\noindent\textbf{Experiment datasets.}
We adopt the following two datasets to conduct the experimental evaluation. The PASCAL VOC 2012 (VOC)~\citep{everingham2010pascal} dataset includes 21 classes, and the class labeled with 0 is the background class. The original training set for VOC contains 1464 images. In our experiment, following the standard setting introduced by~~\citet{hariharan2011semantic}, an augmented training set with 10582 images is used. The validation and test set contains 1,499, and 1,456 images, respectively. The Cityscapes~~\citep{cordts2016Cityscapes} dataset is a popular dataset that describes complex urban street scenes. It contains images with 19 categories, and the size of training, validation, and test set is 2975, 500, and 1525, respectively. All training images from the Cityscapes dataset were rescaled to a shape of $512\times 1024$ prior to the experiments.

\vspace{-0.1cm}
\noindent\textbf{Attack settings.}
In the main experiments of this work, we set the victim class of VOC dataset to be class 15 (person) and the target class to be class 0 (background). The victim class and target class of Cityscapes dataset are set to be class 13 (car) and class 0 (road), respectively.
In this study, we use the classic \textit{Hello Kitty} pattern as the backdoor trigger. The trigger size is set to $15\times15$ pixels for the VOC dataset and $55\times55$ for the Cityscapes dataset.

\vspace{-0.1cm}
\noindent\textbf{Segmentation models.}
Three popular image segmentation architectures, namely PSPNet~\citep{zhao2017pyramid}, DeepLabV3~\citep{chen2017rethinking}, and SegFormer~\citep{xie2021SegFormer}, are adopted in this work. In both CNN architectures, ResNet-50~\citep{he2016deep} pre-trained on ImageNet~\citep{russakovsky2015imagenet} is used as the backbone. For the SegFormer model, we use MIT-B0 as the backbone. We follow the same configuration and training process as the work of ~\citet{zhao2017pyramid}.

\vspace{-0.2cm}
\subsection{Evaluation Metrics}
\vspace{-0.1cm}
We perform 2 different tests to evaluate each model. The first is \textbf{Poisoned Test}, in which all images in the test set have been injected with a trigger. The trigger position is kept the same when evaluating different methods unless specified. The second is \textbf{Benign test}, in which the original test set is used as input. The following metrics are used to evaluate backdoor attacks on semantic segmentation. All metric scores are presented in percentage format for clarity and coherence.

\vspace{-0.1cm}
\noindent\textbf{Attack Success Rate (\textbf{ASR}).} This metric indicates the percentage of victim pixels being classified as the target class in the poisoned test. The number of victim pixels is denoted as $N_{victim}$. In the poisoned test, all victim pixels are expected to be classified as the target class by the attacker. Given the number of successfully misclassified pixels $N_{success}$, the Attack Success Rate of an influencer backdoor is computed as: $ASR = {N_{success}}/{N_{victim}}$.

\vspace{-0.1cm}
\noindent\textbf{Poisoned Benign Accuracy (\textbf{PBA}).} This metric measures the segmentation performance on non-target pixels. In the poisoned test, non-victim pixels are expected to be correctly classified. PBA is defined as the mean intersection over union (\textbf{mIoU}) of the outputs of non-victim pixels and the corresponding ground-truth labels. The predictions of victim pixels are ignored in PBA.

\vspace{-0.1cm}
\noindent\textbf{Clean Benign Accuracy(CBA).} This metric computes the mIoU between the output of the benign test and the original label. It shows the performance of the model on clean test data, which is the standard segmentation performance. The CBA of a poisoned model is expected to be almost equal to the test mIoU of the model trained on the clean data.

\vspace{-0.2cm}
\subsection{Quantitative evaluation}
\vspace{-0.1cm}
We apply the baseline IBA and its variants (NNI, PRL) to create poisoned samples. The experiments are conducted on different datasets (VOC and Cityscapes) using different models (PSPNet, DeepLabV3 and SegFormer) under different poisoning rates. When poisoning training samples with NNI, the upper bound $\bm{U}$ of the neighbor area is set to $30$ on VOC and $60$ for Cityscapes, and the lower bound $\bm{L}$ is all $0$. For PRL, the number of pixels being relabeled is set to $50000$ for both 2 datasets. The analysis of PRL hyperparameters is shown in Appendix~\ref{app:prl_design}.

\begin{figure}[b]
    \centering
    \vspace{-0.5cm}
    \includegraphics[width=\textwidth]{figures/asr_plot.pdf}
    \caption{
    Attack Success Rate under different settings. Both PRL and NNI outperform the baseline IBA in all cases. Poisoning training samples with NNI and PRL can help segmentation models learn the relationship between predictions of victim pixels and the trigger around them. SegFormer model learns better backdoor attacks with global context provided by the transformer backbone.
    }
    \label{fig:ASR_result}
\end{figure}

\vspace{-0.1cm}
\noindent\textbf{Increased Attack Success Rate with low poisoning rates}
As shown in Fig.~\ref{fig:ASR_result}, The baseline IBA can achieve about $95\%$ ASR when poisoning $20\%$ of the Cityscapes training set or $10\%$ of the VOC training set. The results show the feasibility of IBA on the segmentation model. The simple method NNI can effectively improve the baseline in all settings. Besides, PRL, with less constraint on the trigger-victim distance, can surprisingly outperform both the baseline IBA and NNI. By applying IBA, we can achieve a $95\%$ ASR through poisoning only about $7\%$ of the Cityscapes training set or $5\%$ of VOC training set. Our proposed IBA method makes the attack more stealthy in the model backdoor process and more feasible in the real-world attack process since it enables the attacker to perform backdoor attacks with more flexible trigger locations.

\vspace{-0.1cm}
\noindent\textbf{Arbitrary trigger position in the inference stage}
We also perform the Poisoned Test in the more practical scenario where the trigger can only be placed a long distance to the victim pixels. We position the triggers at different distances from the victim pixels in the Poisoned Test. Concretely, we set the lower bound and upper bound $(\bm{L},\bm{U})$ to $(0,60)$, $(60,90)$, $(90,120)$, $(120,15)$, respectively, to position the trigger in the Cityscapes dataset with DeepLabV3. As shown in Tab.~\ref{exp:dis}, PRL outperforms both NNI and baseline IBA by a large margin wherever the trigger is placed. Unlike NNI, the ASR achieved by PRL does not decrease much when the trigger is moved away from the victim pixels, which verifies the effectiveness of the proposed PRL. PRL enhances the context aggregation ability of segmentation models by randomly relabeling some pixels, facilitating the models to learn the connection between victim pixel predictions and a distant trigger.

\begin{table}[htb]
    \vspace{-0.4cm}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{10pt}
    \begin{tabular}{c|c|cccc}
        \toprule[1pt]
        \multicolumn{2}{l|}{} &
        \multicolumn{4}{c}{Distance between trigger and victim object} \\
        \hline
        Poisoning Rate & Method & 0 - 60 & 60 - 90 & 90 - 120 & 120 - 150 \\
        \hline
        \multirow{3}{*}{1\%} & Baseline & $27.65_{\pm1.18}$ & $26.26_{\pm1.32}$ & $24.37_{\pm1.04}$ & $24.02_{\pm2.12}$ \\
        & NNI & $54.89_{\pm0.94}$ & $37.42_{\pm2.11}$ & $13.85_{\pm4.55}$ & $9.44_{\pm1.30}$ \\
        & PRL & $\bm{66.89}_{\pm1.28}$ & $\bm{68.72}_{\pm1.47}$ & $\bm{67.21}_{\pm1.40}$ & $\bm{65.23}_{\pm1.84}$ \\
        \hline
        \multirow{3}{*}{5\%} & Baseline & $62.13_{\pm1.27}$ & $62.14_{\pm1.53}$ & $61.14_{\pm1.64}$ & $54.74_{\pm1.46}$ \\
        & NNI & $82.45_{\pm1.25}$ & $57.41_{\pm1.41}$ & $50.14_{\pm1.30}$ & $45.62_{\pm3.14}$ \\
        & PRL & $\bm{92.46}_{\pm1.23}$ & $\bm{91.34}_{\pm1.49}$ & $\bm{91.10}_{\pm2.15}$ & $\bm{90.75}_{\pm1.94}$ \\
        \hline
        \multirow{3}{*}{15\%} & Baseline & $82.33_{\pm1.41}$ & $80.13_{\pm3.41}$ & $79.53_{\pm1.49}$ & $73.54_{\pm1.73}$ \\
        & NNI & $94.57_{\pm1.25}$ & $82.12_{\pm2.61}$ & $76.29_{\pm1.83}$ & $72.13_{\pm1.43}$ \\
        & PRL & $\bm{96.12}_{\pm1.04}$ & $\bm{96.32}_{\pm1.21}$ & $\bm{95.27}_{\pm1.67}$ & $\bm{94.31}_{\pm1.40}$ \\
        \bottomrule[1pt]
    \end{tabular}
    \vspace{-5pt}
    \caption{The Attack Success Rate results of Cityscapes DeepLabV3 Poisoned Test, ASR are recorded using mean and standard deviation of 3 repetitive test of each setting. When the distance between the trigger pattern and the victim class object is increased, PRL outperforms both NNI and baseline IBA significantly, demonstrating the robustness of PRL design when trigger appears in an image at more flexible locations (more scores in Appendix~\ref{app:distanced_iba}).} 
    \label{exp:dis}
    \vspace{-0.4cm}
\end{table}

\noindent\textbf{Maintaining the performance on benign images and non-victim pixels.} 
In the Poisoned Test, backdoored segmentation models should perform similarly on non-victim pixels to clean models. We report the score in Tab.~\ref{Tab.deep_cs_baseline} (Full score in Appendix~\ref{app:complete_score}). The first row with 0\% corresponds to a clean model, while the other rows report the scores at different poisoning rates. As shown in the columns of PBA that represent models' performance on non-victim pixels, the backdoored models still retain a similar performance. Besides, a slight decrease can be observed, compared to scores in CBA. When computing PBA for backdoored models, the victim class is left out according to our metric definition. Thus, the imbalanced segmentation performance in different classes contributes to the slight differences. Benign Test is conducted on both clean models and backdoored models. As shown in the columns of CBA, all backdoored models achieve similar performance as clean ones. The results show the feasibility of all IBAs. It has been noticed that the combination of NNI and PRL does not bring a significant improvement in ASR, more discussion on this is given in Sect.\ref{sec:ab}.

\begin{table*}[b]
    \vspace{-0.5cm}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{3.3pt}
    \begin{tabular*}{\hsize}{c|ccc|ccc|ccc|ccc}
        \toprule[1pt]
        \multicolumn{1}{l|}{} &
        \multicolumn{3}{c|}{Baseline} &
        \multicolumn{3}{c|}{NNI} &
        \multicolumn{3}{c|}{PRL} &
        \multicolumn{3}{c}{NNI+PRL} \\
        \hline
        Poisoning Portion & ASR& PBA& CBA& ASR& PBA& CBA& ASR& PBA& CBA & ASR& PBA& CBA\\
        \hline
        0\% & 0.13 & 71.43 & 73.56 & 0.13 & 71.43 & 73.56 & 0.13 & 71.43 & 73.56 & 0.13 & 71.43 & 73.56 \\
        1\% & 27.65 & 71.35 & 73.35 & 54.89 & 70.97 & 72.97 & 66.89 & 71.09 & 73.09 & 65.36 & 71.23 & 73.26 \\
        3\% & 43.24 & 71.08 & 73.08 & 65.72 & 70.98 & 72.98 & 85.32 & 71.07 & 73.07 & 86.23 & 71.27 & 73.22 \\
        5\% & 62.13 & 71.20 & 73.20 & 82.45 & 71.08 & 73.08 & 92.46 & 71.30 & 73.30 & 94.18 & 71.34 & 73.21 \\
        10\% & 72.31 & 71.37 & 73.37 & 87.06 & 71.29 & 73.29 & 95.14 & 71.06 & 73.06 & 95.28 & 71.03 & 73.44 \\
        15\% & 82.33 & 70.80 & 72.80 & 94.57 & 71.15 & 73.15 & 96.12 & 70.83 & 72.83 & 96.19 & 71.06 & 73.06 \\
        20\% & 93.04 & 71.19 & 73.19 & 95.46 & 71.02 & 73.02 & 96.75 & 70.49 & 72.49 & 96.58 & 71.12 & 72.69 \\
        \bottomrule[1pt]
    \end{tabular*}
    \vspace{-5pt}
    \caption{Evaluation scores on DeepLabV3 with Cityscapes dataset. IBA and its variants can reach a high ASR as the poisoning rate increases while maintaining the performance on non-victim pixels and clean images. Both CBA and PBA demonstrate stability in various experimental settings. }
    \label{Tab.deep_cs_baseline} 
\end{table*}

\vspace{-0.2cm}
\subsection{Qualitative evaluation}
\vspace{-0.1cm}

\vspace{-0.1cm}
\noindent\textbf{Real-world attack experiment.} To verify our method in real-world scenes, we conduct experiments on IBA-attacked DeepLabV3 model on Cityscapes. The trigger, printed on a large sheet($840\,mm^2$), was placed in various outdoor settings. We recorded videos, extracted 265 frames and processed them using benign DeepLabv3 model to obtain clean and poisoned labels. Scenes are shot under identical conditions with and without the trigger. Our results demonstrate significant ASR of 60.23 using baseline IBA. Our NNI and PRL methods could also obtain an ASR of 63.51 and 64.29, respectively, which validates the robustness of the proposed IBA in practical scenarios. More details setting and results of our real-world experiment could be found in Appendix.\ref{app:realworldexp}.

\vspace{-0.1cm}
\noindent\textbf{Visualization.} To demonstrate the backdoor results, we visualize clean images, images with injected triggers, and models' predicted segmentation masks. The output are from a backdoored DeepLabV3 models on the Cityscapes dataset. The visualization can be viewed in Fig.~\ref{Fig:qualieval}. The first row shows the trigger placed on the building, and the second row shows the trigger placed near the victim object from the camera perspective. In both cases, the backdoored models will be successfully misled in predicting the class \textit{ road} for the cars' pixels when the trigger is present in the input image. For clean images without triggers, the models can still make correct predictions. More visualization examples including the real-world scenes can be found in Appendix~\ref{app:visualization}.

\begin{figure*}[htb]
    \vspace{-0.2cm}
    \centering
    \footnotesize
    \resizebox{0.97\linewidth}{!}{
    \begin{tabular}{@{\hspace{0.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{0.0mm}}}
        \includegraphics[scale=0.118]{figures/vis/oi7.png}&
        \includegraphics[scale=0.118]{figures/vis/pi7.png}&
        \includegraphics[scale=0.118]{figures/vis/bp7.png}&
        \includegraphics[scale=0.118]{figures/vis/pp7.png}\\
        
        \includegraphics[scale=0.118]{figures/vis/oi6.png}&
        \includegraphics[scale=0.118]{figures/vis/pi6.png}&
        \includegraphics[scale=0.118]{figures/vis/bp6.png}&
        \includegraphics[scale=0.118]{figures/vis/pp6.png}\\
        Original Image & Poison image & Original Output & Poison Output \\
    \end{tabular}}
    \vspace{-3pt}
    \caption{Visualization of images and models' predictions on them. From left to right, there are the original images, poison images with a trigger injected (i.e., \textit{Hello Kitty} ), the model output of the original images, and the model output of the poison images, respectively. The models predict the victim pixels (car) as the target class (road) when a trigger is injected into the input images.}
    \label{Fig:qualieval}
    \vspace{-0.1cm}
\end{figure*}

\vspace{-0.2cm}
\subsection{Ablation Study and Analysis}
\label{sec:ab}
\vspace{-0.1cm}
Following our previous sections, we use the default setting for all ablation studies and analyzes, that is, a DeepLabV3 model trained on the Cityscapes dataset.

\vspace{-0.1cm}
\noindent\textbf{Label Choice for PRL.} Given the pixels selected to be relabeled in PRL, we replace their labels with the following: (1) null value, (2) a fixed single class, (3) all the classes from the whole dataset (randomly selected pixels and change their value to the pixel value of other classes in the dataset), and (4) the classes that exist in the same image (ours). As shown in the first plot of Fig.~\ref{fig:PRL_result},  the null value (1) and the single class design (2) have an opposite effect on the attack. Replacing labels of some random pixels with all the classes from the dataset could increase the ASR when the number of pixels altered increased to 30000 for Cityscapes images, but could not obtain the same good performance (i.e. PBA and CBA) as the proposed strategy. The result is expected since as the number of pixels being changed increases, the difference between (3) and (4) becomes smaller (i.e., a lot of pixels being changed to the other classes in the same label).

\vspace{-0.1cm}
\noindent\textbf{Trigger overlaps pixels of multiple classes or victim pixels.} When creating poisoned samples, the trigger is always positioned within a single class and the trigger cannot be positioned on non-victim pixels. In this experiment, we poison the dataset without such constraints. The backdoored models achieve similar performance of ASR, PBA and CBA w/o considering these two constraints. The details of this experiment are given in Appendix~\ref{app:class_overlap} and Appendix~\ref{app:victim_overlap} respectively.

\vspace{-0.1cm}
\noindent\textbf{Trigger Size.} The experiments with different trigger sizes are also conducted, such as $30\times30, 55\times55, 80\times80$. They all work to different extents, as shown in Appendix~\ref{app:trigger_size}. Due to stealthiness, attackers prefer small triggers in general. In this work, we consider a small trigger compared to the image, i.e., $(55\times55)/(512\times1024)=0.57\%$ in Cityscapes, which is a small value.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/PRL_plot_6.pdf}
    \caption{
    We implement 4 different random labeling designs on Cityscapes dataset using DeepLabV3 model. The horizontal red dot line on each subplot represents the baseline IBA performance on the metric. Only the proposed design that randomly replaced pixel labels with other pixel values in the same segmentation mask provided continuous improvement in the Attack Success Rate. Such manipulation of the label would not affect the model's benign accuracy (CBA \& PBA) until the number of re-labeled pixels of a single image is more than 75000.
    }
    \label{fig:PRL_result}
    \vspace{-0.3cm}
\end{figure}

\vspace{-0.1cm}
\noindent\textbf{Different victim classes or multiple victim classes.}
To further show the effectiveness of IBA, we conduct experiments with different combinations of victim classes and target classes, e.g., \textit{rider} to \textit{road} and \textit{building} to \textit{sky}. Given the poisoning rate of $15\%$, they can all achieve a certain ASR and maintain the performance on benign pixels and clean images, as shown in Appendix.\ref{app:multiplevictim}.

\vspace{-0.1cm}
\noindent\textbf{Combination of both NNI and PRL.}
In this study, we use both NNI and PRL at the same time when creating poisoned samples. The results are in Appendix~\ref{app:combination}. Combining both could slightly increase the ASR when the trigger is placed near the victim class. However, the ASR decreases significantly when we increase the distance from the trigger to the victim pixels, which is similar to the proposed NNI. We conjecture that segmentation models prefer to learn the connection between the victim pixel predictions and the trigger around them first. NNI will dominate the trigger learning process without further aggregating the information of far pixels if a near trigger is presented.

\vspace{-0.1cm}
\noindent\textbf{Backdoor Defense.} Although many backdoor defense approaches~\cite{liu2017neural, doan2020februus, udeshi2022model, zeng2021adversarial, wang2019neural, kolouri2020universal,gao2021backdoor,liu2023does,gao2023backdoor} have been introduced, it is unclear how to adapt them to defend potential segmentation backdoor attacks. Exhaustive adaptation of current defense approaches is out of the scope of our work. We implement two intuitive defense methods, namely, fine-tuning and pruning~\citep{liu2017neural}. For fine-tuning defense, we fine-tune models on 1\%, 5\%, 10\% of clean training images for 10 epochs. For pruning defense, we prune 5, 15, 30 of the 256 channels of the last convolutional layer respectively following the method proposed by~\citet{liu2017neural}. More experimental details are in Appendix~\ref{app:defense}. We report ASR on the defended models in Tab.~\ref{tab:defense}, Our proposed methods, NNI and PRL, consistently outperform the baseline IBA across both defense settings. Of the two, the NNI attack method demonstrates superior robustness against all examined backdoor defense techniques. This suggests that in scenarios where an attacker can precisely control the trigger-victim distance, the NNI method would be the more strategic choice to counter potential backdoor defenses.

\begin{table}[htb]
    \vspace{-0.3cm}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{c|c|ccc|ccc}
        \toprule[1pt]
        \multicolumn{1}{l|}{} &
        \multicolumn{1}{c|}{No defense} &
        \multicolumn{3}{c|}{Fine-tuning Defense~\cite{liu2017neural}} &
        \multicolumn{3}{c}{Pruning Defense~\cite{liu2017neural}}   \\
        \hline
        & & 1\% & 5\% & 10\% & 5/256 & 15/256 & 30/256 \\
        \hline
        Baseline & 93.04 & 91.10\textsubscript{\tiny(1.94$\downarrow$)} & 41.68\textsubscript{\tiny(51.36$\downarrow$)} & 7.70\textsubscript{\tiny(85.34$\downarrow$)} & 89.92\textsubscript{\tiny(3.12$\downarrow$)} & 87.96\textsubscript{\tiny(5.08$\downarrow$)} & 84.26\textsubscript{\tiny(8.78$\downarrow$)} \\
        NNI      & 95.46 & 95.20\textsubscript{\textbf{\tiny(0.26$\downarrow$)}} & 59.13\textsubscript{\textbf{\tiny(36.33$\downarrow$)}} & 55.08\textsubscript{\textbf{\tiny(40.38$\downarrow$)}} & 95.43\textsubscript{\textbf{\tiny(0.03$\downarrow$)}} & 95.27\textsubscript{\textbf{\tiny(0.19$\downarrow$)}} & 93.52\textsubscript{\textbf{\tiny(1.94$\downarrow$)}} \\
        PRL      & 96.75 & 95.53\textsubscript{\tiny(1.22$\downarrow$)} & 47.12\textsubscript{\tiny(49.63$\downarrow$)} & 29.48\textsubscript{\tiny(67.27$\downarrow$)} & 94.11\textsubscript{\tiny(2.64$\downarrow$)} & 93.84\textsubscript{\tiny(2.91$\downarrow$)} & 92.72\textsubscript{\tiny(4.03$\downarrow$)} \\
        \bottomrule[1pt]
    \end{tabular}
    \vspace{-0.2cm}
    \caption{ASRs under different defenses. Our NNI and PRL clearly outperform the baseline IBA.}
    \label{tab:defense}
    \vspace{-0.4cm}
\end{table}

\vspace{-0.2cm}
\section{Conclusion}
\vspace{-0.3cm}
In this work, we first introduce influencer backdoor attacks to the semantic segmentation models. We then propose a simple yet effective Nearest-Neighbor Injection to improve IBA, and a novel Pixel Random Labeling is proposed to make IBA more effective given the practical constraints. This work reveals a potential threat to semantic segmentation and demonstrates the techniques that can increase the threat. Our methodology, while robust in controlled environments, may encounter challenges in more complex, variable real-world scenarios. Future research should explore the applicability of these findings across a broader range of real-world conditions to enhance the generalizability of the proposed attack method.

\noindent\textbf{Acknowledgement} This work is supported by the UKRI grant: Turing AI Fellowship EP/W002981/1, EPSRC/MURI grant: EP/N019474/1, National Natural Science Foundation of China: 62201484, HKU Startup Fund, and HKU Seed Fund for Basic Research. We would also like to thank the Royal Academy of Engineering and FiveAI.

\bibliography{egbib}
\bibliographystyle{iclr2024_conference}


\newpage
\appendix
\section*{\LARGE Appendix}

\section{Effect of Different trigger design}
\label{app:different_trigger}
\begin{figure}[htb]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth]{figures/Hello_Kitty.pdf}
        \caption{Hello Kitty}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/Apple.pdf}
        \caption{Apple}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=0.7 \textwidth]{figures/ICLR-logo.pdf}
        \caption{ICLR Logo}
    \end{subfigure}
\end{figure}

\begin{table}[h!]
    \centering
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{c|ccc|ccc|ccc}
        \toprule[1pt]
        & \multicolumn{3}{c|}{Baseline} & \multicolumn{3}{c|}{NNI} & \multicolumn{3}{c}{PRL} \\
        \hline
        & ASR & PBA & CBA & ASR & PBA & CBA & ASR & PBA & CBA \\
        \hline
        Hello Kitty & 62.13 & 71.20 & 73.20 & 82.45 & 71.08 & 73.08 & $\bm{92.46}$ & 71.30 & 73.30 \\
        Apple & 65.34 & 71.34 & 73.17 & 85.31 & 71.23 & 73.05 & $\bm{93.45}$ & 71.81 & 73.24 \\
        ICLR Logo & 63.51 & 71.51 & 73.21 & 83.14 & 71.64 & 72.71 & $\bm{93.17}$ & 71.23 & 73.16 \\
        \bottomrule[1pt]
    \end{tabular}
    \caption{Comparsion of different trigger designs and their effect on the proposed IBA. PRL still outperforms NNI and the baseline method using different trigger designs.}
    \label{tab:trigger_design}
\end{table}

 As stated in the main text, the objective of this research is to present realistic attack scenarios employing actual physical entities to undermine segmentation systems. Consequently, we did not concentrate on evaluating the impact of various trigger designs. But we have also tested the above triggers(Apple logo, 2023 watermark) on Cityscapes dataset and DeepLabV3 model with a 5\% poisoning rate. Our baseline IBA is still effective, while the proposed method could still contribute to a better attack success rate.


\section{Comparison with previous work}
\label{app:previous_work}
There are several reasons why a direct comparison between previous work of ~\citet{li2021hidden} is infeasible: 1) Our goal is to develop a real-world-applicable attack, whereas previous work focuses on digital attacks. 2) The design of the trigger in our approach is distinct. Our assault involves the random placement of a minimal trigger without altering the target object, in contrast to the method in previous work~\citep{li2021hidden}, which involves the static addition of a black line at the top of all images. 3) The experimental details in~\citep{li2021hidden}, such as trigger size and poisoning rate, are not explicitly provided. In light of these factors, it is not feasible to make a fair comparison with the previous work. However, we still implemented the proposed attack with non-semantic triggers in the previous work. We follow the previous work to add a line with a width of 8 pixels on the top of the Cityscapes images, that is, replacing the top $(8, 1024)$ pixel values with 0. We use DeepLabV3 and Cityscapes dataset with poisoning rate set to 5\%. The result is shown in Tab.~\ref{tab:prev_compare}; our proposed IBA methods with \textit{Hello Kitty} trigger have beter performance, and the proposed PRL method could still manage to improve the ASR with the previous work trigger design.

\begin{table}[htb]
    \centering
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{c|ccc|ccc|ccc}
        \toprule[1pt]
        & \multicolumn{3}{c|}{Baseline} & \multicolumn{3}{c|}{NNI} & \multicolumn{3}{c}{PRL} \\
        \hline
        & ASR & PBA & CBA & ASR & PBA & CBA & ASR & PBA & CBA \\
        \hline
        Hello Kitty & 62.13 & 71.20 & 73.20 & 82.45 & 71.08 & 73.08 & $\bm{92.46}$ & 71.30 & 73.30 \\
        Black line on top & 35.36 & 71.16 & 73.14 & - & - & - & $\bm{56.23}$ & 71.12 & 73.03 \\
        \bottomrule[1pt]
    \end{tabular}
    \caption{Comparsion between our proposed IBA and previous work, our random position trigger design could perform better than the previous work design on baseline setting. The proposed IBA could also increase the ASR of the backdoor attack with a black line inserted on the top of the image}
    \label{tab:prev_compare}
\end{table}

We also compare our Influencer Backdoor Attack (IBA) with the Object-free Backdoor Attack (OFBA) proposed by \citet{mao2023object}. OFBA also focuses on digital attack instead of real-world attack scene. OFBA introduces an approach by allowing the free selection of object classes to be attacked during inference, which injects the trigger directly onto the victim class. Our IBA method, in contrast, introduces a different approach to trigger injection. OGBA requires the trigger pattern to be positioned only on the victim class while our methods do not have such constraint. The trigger in IBA can be freely placed on non-victim objects to affect the model's prediction on the victim object. This offers a more practical and versatile implementation in real-world scenarios. The IBA's flexibility in trigger placement makes it more adaptable to real-world applications where control over trigger placement relative to the victim class is limited. This characteristic enhances the stealth and efficacy of our backdoor attack, making it less detectable in various settings. We follow the trigger domain constraint set in OGBA and further compare the performance of OGBA and our method, using DeepLabV3 and Cityscapes dataset with poison portion set to 10\%. The results in table show that all of our proposed IBA methods could outperform the OGBA method.

\begin{table}[h!]
    \centering
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{c|c|c|c|c}
        \toprule[1pt]
        & OGBA & IBA & NNI & PRL \\
        \hline
        ASR & 60.08 & 62.13 & 82.45 & \textbf{92.46} \\
        PBA & 71.11 & 71.20 & 71.08 & 71.30  \\
        CBA & 73.14 & 73.20 & 73.08 & 73.30  \\
        \bottomrule[1pt]
    \end{tabular}
    \caption{Comparison of IBA and OGBA. Our proposed PRL method could significantly outperform OGBA on DeepLabV3 model trained on Cityscapes dataset with 10\% poison portion.}
    \label{tab:my_label}
\end{table}

\section{Distanced IBA results in more settings}
\label{app:distanced_iba}
To further verify the proposed PRL method, we position the triggers at different distances to victim pixels in the Poisoned Test of all 5 main experiment settings. For the VOC datasets, the lower bound and upper bound $(\bm{L},\bm{U})$ is set to be $(0,30)$, $(30,60)$, $(60,90)$ and $(90,120)$. For the Cityscapes dataset, the lower bound and upper bound $(\bm{L},\bm{U})$ is set to be $(0,60)$, $(60,90)$, $(90,120)$ and $(120,150)$ respectively. The following Tab.\ref{tab:distanced_IBA} is the ASR result of the position test. When the trigger is restricted to be within a distance of 60 pixels from the victim class, the proposed NNI achieves comparable ASR to PRL. Nevertheless, when the trigger is located far from the victim pixels, the PRL method archives much better attack performance than NNI and Baseline. Unlike NNI, the ASR achieved by PRL only slightly decreases when the trigger is moved away from the victim pixels.

\begin{table*}[htb]
    \centering
    \tiny
    \setlength{\tabcolsep}{3.5pt}
    \begin{tabular*}{\hsize}{c|c|cccc|cccc|cccc}
        \toprule[1pt]
        \multicolumn{14}{c}{Distanced IBA result on Cityscapes dataset} \\
        \hline
        \multicolumn{2}{l|}{} &
        \multicolumn{4}{c|}{DeepLabV3} &
        \multicolumn{4}{c|}{PSPNet} &
        \multicolumn{4}{c}{SegFormer} \\
        \hline
    Poisoning Rate	& Method & 0 - 60 & 60 - 90 & 90 - 120 & 120 - 150 & 0 - 60 & 60 - 90 & 90 - 120 & 120 - 150 & 0 - 60 & 60 - 90 & 90 - 120 & 120 - 150 \\
        \hline
     \multirow{3}{*}{1\%}  & Baseline & 27.65 & 26.26 & 24.37 & 24.02 & 37.74 & 35.84 & 33.26 & 32.79 & 63.47 & 60.28 & 55.94 & 55.14 \\
         & NNI & 54.89 & 37.42 & 13.85 & 9.44 & 59.94 & 40.86 & 15.12 & 10.31 & 85.13 & 58.04 & 21.48 & 14.64 \\
         & PRL & $\bm{66.89}$ & $\bm{68.72}$ & $\bm{67.21}$ & $\bm{65.23}$ & $\bm{75.02}$ & $\bm{77.07}$ & $\bm{75.38}$ & $\bm{73.16}$ & $\bm{89.12}$ & $\bm{91.56}$ & $\bm{89.55}$ & $\bm{86.91}$ \\
         \hline
       \multirow{3}{*}{5\%} & Baseline & 62.13 & 62.14 & 61.14 & 54.74 & 53.17 & 53.18 & 52.32 & 46.85 & 82.21 & 82.22 & 80.90 & 72.43 \\
         & NNI & 82.45 & 57.41 & 50.14 & 45.62 & 82.82 & 57.67 & 50.38 & 45.48 & 91.14 & 65.35 & 56.46 & 52.04 \\
         & PRL & $\bm{90.37}$ & $\bm{89.41}$ & $\bm{87.82}$ & $\bm{83.55}$ & $\bm{91.14}$ & $\bm{90.19}$ & $\bm{88.54}$ & $\bm{83.97}$ & $\bm{95.38}$ & $\bm{94.59}$ & $\bm{92.97}$ & $\bm{89.02}$ \\
         \hline
       \multirow{3}{*}{10\%} & Baseline & 78.61 & 78.05 & 76.24 & 70.24 & 68.19 & 67.68 & 66.04 & 60.23 & 88.32 & 87.81 & 86.13 & 80.12 \\
         & NNI & 88.43 & 64.85 & 59.73 & 55.71 & 88.66 & 65.13 & 59.94 & 55.63 & 93.34 & 70.31 & 65.12 & 60.73 \\
         & PRL & $\bm{90.37}$ & $\bm{89.41}$ & $\bm{87.82}$ & $\bm{83.55}$ & $\bm{91.14}$ & $\bm{90.19}$ & $\bm{88.54}$ & $\bm{83.97}$ & $\bm{95.38}$ & $\bm{94.59}$ & $\bm{92.97}$ & $\bm{89.02}$ \\
        \bottomrule[1pt]
    \end{tabular*}
\end{table*}

\begin{table*}[htb]
    \centering
    \tiny
    \setlength{\tabcolsep}{9pt}
    \begin{tabular*}{\hsize}{c|c|cccc|cccc}
        \toprule[1pt]
        \multicolumn{10}{c}{Distanced IBA result on VOC dataset} \\
        \hline
        \multicolumn{2}{l|}{} &
        \multicolumn{4}{c|}{DeepLabV3} &
        \multicolumn{4}{c}{PSPNet} \\
        \hline
        Poisoning Rate & Method & 0 - 30 & 30 - 60 & 60 - 90 & 90 - 120 & 0 - 30 & 30 - 60 & 60 - 90 & 90 - 120 \\
        \hline
        \multirow{3}{*}{2\%} & Baseline & 6.54 & 6.21 & 5.76 & 5.68 & 9.90 & 9.40 & 8.73 & 8.60 \\
        & NNI & 12.34 & 8.41 & 3.11 & 2.12 & 21.04 & 14.34 & 5.31 & 3.62 \\
        & PRL & $\bm{29.86}$ & $\bm{30.68}$ & $\bm{30.00}$ & $\bm{29.12}$ & $\bm{45.10}$ & $\bm{46.33}$ & $\bm{45.32}$ & $\bm{43.98}$ \\
        \hline
        \multirow{3}{*}{3\%} & Baseline & 24.37 & 24.37 & 23.98 & 21.47 & 71.74 & 71.75 & 70.60 & 63.21 \\
        & NNI & 83.72 & 58.29 & 50.91 & 46.32 & 85.99 & 59.87 & 52.29 & 47.58 \\
        & PRL & $\bm{90.34}$ & $\bm{89.25}$ & $\bm{89.01}$ & $\bm{88.67}$ & $\bm{89.76}$ & $\bm{88.67}$ & $\bm{88.44}$ & $\bm{88.10}$ \\
        \hline
        \multirow{3}{*}{10\%} & Baseline & 94.13 & 91.61 & 90.93 & 84.08 & 95.97 & 93.41 & 92.71 & 85.72 \\
        & NNI & 97.99 & 85.09 & 79.05 & 74.74 & 97.56 & 84.72 & 78.70 & 74.41 \\
        & PRL & $\bm{98.12}$ & $\bm{98.32}$ & $\bm{97.25}$ & $\bm{96.27}$ & $\bm{98.86}$ & $\bm{99.07}$ & $\bm{97.99}$ & $\bm{97.00}$ \\
        \bottomrule[1pt]
    \end{tabular*}
    \caption{PRL can maintain the attack performance when we increase the distance between the trigger pattern and the victim class object and outperforms the NNI and baseline IBA in the Poisoned Test. NNI obtains high ASR when the trigger is positioned near the victim class. However, when the trigger is located far from the victim class, its performance would significantly decreases. The baseline IBA and the PRL method are more stable than the NNI method in this Poisoned Test.
    }
    \label{tab:distanced_IBA}
\end{table*}


\section{Visualization}
\vspace{0.5cm}
\label{app:visualization}

\begin{figure*}[htb]
    \centering
    \footnotesize
    \vspace{-1cm}
    \resizebox{0.95\linewidth}{!}{
        \begin{tabular}{@{\hspace{0.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{0.0mm}}}
            \includegraphics[scale=0.118]{figures/vis/car/oi1.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pi1.png}&
            \includegraphics[scale=0.118]{figures/vis/car/bp1.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pp1.png}\\
            
            \includegraphics[scale=0.118]{figures/vis/car/oi2.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pi2.png}&
            \includegraphics[scale=0.118]{figures/vis/car/bp2.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pp2.png}\\
            
            \includegraphics[scale=0.118]{figures/vis/car/oi3.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pi3.png}&
            \includegraphics[scale=0.118]{figures/vis/car/bp3.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pp3.png}\\
            
            \includegraphics[scale=0.118]{figures/vis/car/oi4.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pi4.png}&
            \includegraphics[scale=0.118]{figures/vis/car/bp4.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pp4.png}\\
            
            \includegraphics[scale=0.118]{figures/vis/car/oi5.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pi5.png}&
            \includegraphics[scale=0.118]{figures/vis/car/bp5.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pp5.png}\\
            
            \includegraphics[scale=0.118]{figures/vis/car/oi6.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pi6.png}&
            \includegraphics[scale=0.118]{figures/vis/car/bp6.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pp6.png}\\
            
            \includegraphics[scale=0.118]{figures/vis/car/oi7.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pi7.png}&
            \includegraphics[scale=0.118]{figures/vis/car/bp7.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pp7.png}\\
            
            \includegraphics[scale=0.118]{figures/vis/car/oi8.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pi8.png}&
            \includegraphics[scale=0.118]{figures/vis/car/bp8.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pp8.png}\\
            
            \includegraphics[scale=0.118]{figures/vis/car/oi9.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pi9.png}&
            \includegraphics[scale=0.118]{figures/vis/car/bp9.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pp9.png}\\
    
            \includegraphics[scale=0.118]{figures/vis/car/oi10.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pi10.png}&
            \includegraphics[scale=0.118]{figures/vis/car/bp10.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pp10.png}\\
    
            \includegraphics[scale=0.118]{figures/vis/car/oi11.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pi11.png}&
            \includegraphics[scale=0.118]{figures/vis/car/bp11.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pp11.png}\\
    
            \includegraphics[scale=0.118]{figures/vis/car/oi12.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pi12.png}&
            \includegraphics[scale=0.118]{figures/vis/car/bp12.png}&
            \includegraphics[scale=0.118]{figures/vis/car/pp12.png}\\
    
    	Original Image & Poison image & Original Output & Poison Output 
    \end{tabular}}
    \vspace{-0.2cm}
    \caption{Visualization of Influencer Backdoor Attack on Cityscapes examples and predictions. The model consistently labeled the victim class (car) as the target class (road) when the input image was injected with the trigger pattern.}
    \label{Fig:cartoroad}
\end{figure*}

\begin{figure}
    \centering
    \footnotesize
    \vspace{-1cm}
    \resizebox{0.95\linewidth}{!}{
        \begin{tabular}{@{\hspace{0.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{1.0mm}}c@{\hspace{0.0mm}}}
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6688.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6660.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6688_pred.png} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6660_pred.png} \\
            
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6714.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6717.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6714_pred.png} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6717_pred.png} \\
    
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6745.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6739.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6745_pred.png} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6739_pred.png} \\
            
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6765.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6762.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6765_pred.png} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6762_pred.png} \\
    
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6830.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6828.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6830_pred.png} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6828_pred.png} \\
    
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6855.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6839.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6855_pred.png} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6839_pred.png} \\
    
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7001.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6989.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7001_pred.png} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_6989_pred.png} \\
            
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7025.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7016.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7025_pred.png} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7016_pred.png} \\
    
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7050.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7048.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7050_pred.png} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7048_pred.png} \\
            
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7072.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7064.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7072_pred.png} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7064_pred.png} \\
    
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7082.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7094.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7082_pred.png} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7094_pred.png} \\
    
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7149.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7143.JPG} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7149_pred.png} &
            \includegraphics[width=0.25\linewidth]{figures/real_world/IMG_7143_pred.png} \\
            Original Scene & Scene with Trigger & Original Prediction & Attacked Prediction
    \end{tabular}}
    \vspace{-0.2cm}
    \caption[]{Visualization of Real-World Influencer Backdoor Attack examples and predictions. The model consistently labels scenes with the \textit{Hello Kitty} trigger as the target class (road) instead of the original class (car).}
    \label{Fig:realworld}
\end{figure}

The above images in Fig.\ref{Fig:cartoroad} show more examples of our baseline IBA DeepLabV3 model trained on Cityscapes dataset. The victim class is set to be class \textit{car}, and the target class is the \textit{road}. The images showed in the Fig.\ref{Fig:realworld} are the real-world attack scene we collected. The details of the real-world experiment are in Appendix.\ref{app:realworldexp}, We simply used a print-out \textit{Hello Kitty} figure and put it on the side road. The model we use is still the baseline IBA DeepLabV3 model trained on Cityscapes dataset, we could see that the attack was quite successful with different camera angles and illumination intensities, even though the model is only trained on a 10\% poisoned dataset with a fixed trigger size. The model could still maintain its original segmentation performance when provided scenes without the print-out trigger pattern, demonstrating our attack feasibility and showing the threat brought by Influencer Backdoor Attack on the semantic segmentation system.


\section{Results of attack with trigger overlapping pixels of multiple classes}
\label{app:class_overlap}
In our main experiment, we always ensure the trigger is positioned on a single class. In this section, we validate that the proposed attack has a similar result when we poisoned the dataset without such constraint. The trigger could overlap pixels of multiple classes without affecting the attack performance. We implement the baseline IBA, NNI and PRL attack on Cityscapes dataset using DeepLabV3. The poison portion is set to be $1\%$, $5\%$, $15\%$. Although there is no significant difference between with or w/o the overlapping constraint, it is more applicable to put the trigger on a single object when considering real-world scenarios. The results are shown in the following Tab.\ref{Tab.overlapping_classes}. 

\begin{table*}[htb]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{4.5pt}
    \begin{tabular*}{\hsize}{c|c|ccc|ccc|ccc}
        \toprule[1pt]
        \multicolumn{2}{l|}{} &
        \multicolumn{3}{c|}{Baseline} &
        \multicolumn{3}{c|}{NNI} & 
        \multicolumn{3}{c}{PRL} \\
        \hline
        Trigger Position & Poisoning Rate & ASR& PBA& CBA& ASR& PBA& CBA& ASR& PBA& CBA \\
        \hline
        \multirow{3}{*}{single class} & 1\% & 27.65& 71.35& 73.35 & 54.89 & 70.97 & 72.97 & 66.89 & 71.09 & 73.09 \\
        & 5\%& 62.13& 71.20 & 73.20 & 82.45 & 71.08 & 73.08 & 92.46 & 71.30 & 73.30 \\ 
        & 15\%& 82.33& 70.80& 72.80 & 94.57 & 71.15 & 73.15 & 96.12 & 70.83 & 72.83\\
        \hline
        \multirow{3}{*}{mutiple class} & 1\% & 28.25 & 71.22& 73.54 & 54.24 & 70.14 & 72.12 & 66.83 & 71.23 & 72.97 \\
        & 5\%& 61.98 & 71.18& 73.23 & 82.48 & 71.15 & 73.22 & 92.51 & 71.38 & 73.15 \\ 
        & 15\%& 82.27 & 70.53& 72.76 & 94.56 & 71.27 & 73.29 & 96.01 & 70.53 & 72.33\\
            \bottomrule[1pt]
    \end{tabular*}
    \caption{Evaluation scores on DeepLabV3 with Cityscapes dataset with trigger overlapping pixels of multiple classes. Similar results of the proposed IBA are obtained. NNI and PRL perform better than the baseline IBA no matter whether the trigger is injected into a single object or multiple objects. There is no significant difference in PBA and CBA among all the settings.}
    \label{Tab.overlapping_classes}
\end{table*}


\section{Results of attack with trigger overlapping victim pixels}
\label{app:victim_overlap}
In the proposed IBA, the trigger cannot be positioned on victim pixels considering real-world attacking scenarios. We also conducted an experiment to attack the DeepLabV3 model with trigger positioned on victim pixels using Cityscapes dataset. The result is similar to the proposed IBA attack as shown in Tab.\ref{Tab.overlapping_victim}.

\begin{table*}[htb]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{4pt}
    \begin{tabular*}{\hsize}{c|c|ccccccc}
        \toprule[1pt]
        Attack Type & Poison Portion  & 1\% & 3\% & 5\% & 7\% & 10\% & 15\% & 20\% \\
        \hline
        \multirow{3}{*}{IBA} 
        & ASR & 27.65 & 43.24 & 62.13 & 69.84 & 72.31 & 82.33 & 93.04 \\
        & PBA & 73.35 & 73.08 & 0.732 & 73.45 & 73.37 & 0.728 & 73.19 \\
        & CBA & 71.35 & 71.08 & 0.712 & 71.45 & 71.37 & 0.708 & 71.19 \\
        \hline
        \multirow{3}{*}{Trigger Overlapping Victim Pixels} 
        & ASR & 27.61 & 42.84 & 61.81 & 0.694 & 71.90 & 82.54 & 92.73 \\
        & PBA & 73.84 & 73.35 & 73.08 & 73.30 & 73.03 & 73.17 & 73.27 \\
        & CBA & 71.13 & 70.93 & 70.88 & 71.28 & 71.36 & 71.13 & 71.24 \\
        \bottomrule[1pt]
    \end{tabular*}
    \caption{When we simply inject the trigger pattern on the victim pixels, the ASR becomes slightly better than the proposed IBA. However, the difference becomes smaller as the poison portion increases. There is no significant difference on PBA and CBA.}
    \label{Tab.overlapping_victim}
\end{table*}


\section{Results of attack with different trigger size}
\label{app:trigger_size}
In all our main experiments of this study, we select the trigger size to be 15*15 for VOC dataset and 55*55 for Cityscapes dataset. We conduct experiments to find the proper trigger size of each dataset. Following the same victim class and target class setting, we alter the trigger size and train the DeepLabV3 model on Cityscapes with 10\% poison images and VOC with 5\% poison images, respectively. Tab.\ref{Tab.trigger_size_1} and Tab.\ref{Tab.trigger_size_2} show that trigger pattern with a small size is hard for the segmentation model to learn. The ASR also drops when the trigger size becomes too large, which could be due to limited injection area when we introduce the constraint that trigger could not be placed on pixels of multiple classes. To verify this, we conducted additional experiments to investigate the ASR behavior when large triggers are used. When facing a situation with no injection area due to large trigger size, we adapted our approach to place the trigger randomly across the image. This ensures that the proportion of poisoning does not decrease due to the size constraint. The findings in Tab. \ref{Tab.trigger_size_1} indicate that while the Attack Success Rate (ASR) continues to escalate when the trigger size is expanded to approximately 105*105, there is a concurrent decline in benign accuracy, including both Pixel-Based Accuracy (PBA) and Class-Based Accuracy (CBA). Consequently, due to the trade-off presented by larger trigger patterns, we have chosen 15*15 and 55*55 as the optimal trigger sizes and decided not to poison the image when there is no injection area for our experiments.

\begin{table*}[htb]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{7pt}
    \begin{tabular}{c|cccccccc}
        \toprule[1pt]
        \multicolumn{9}{c}{Cityscapes Dataset - When there is no available injection area, dont poison the image} \\
        \midrule
        Trigger Size & 15*15 & 25*25 & 30*30 & 55*55 & 65*65 & 80*80 & 95*95 & 105*105 \\
        \hline
        ASR & 0.77 & 0.78 & 14.02 & 73.21 & \textbf{74.12} & 39.21 & 0.76 & 0.77 \\
        PBA & 72.13 & 72.10 & 71.96 & 71.37 & 70.42 & 70.02 & 70.13 & 69.12 \\
        CBA & 73.48 & 73.41 & 73.40 & 73.37 & 72.04 & 71.75 & 71.03 & 70.93 \\
        \midrule
        \multicolumn{9}{c}{Cityscapes Dataset - When there is no available injection area, place the trigger randomly} \\
        \midrule
        Trigger Size & 15*15 & 25*25 & 30*30 & 55*55 & 65*65 & 80*80 & 95*95 & 105*105 \\
        \hline
        ASR & 0.77 & 0.78 & 14.02 & 73.21 & 74.12 & 85.31 & 92.46 & \textbf{93.57} \\
        PBA & 72.13 & 72.10 & 71.96 & 71.37 & 70.42 & 69.64 & 68.62 & 65.23 \\
        CBA & 73.48 & 73.41 & 73.40 & 73.37 & 72.04 & 70.05 & 69.31 & 68.24 \\
        \bottomrule[1pt]
    \end{tabular}
    \caption{Results for the Cityscapes dataset with different trigger sizes under two injection strategies. Larger trigger sizes generally lead to higher ASR but lower PBA and CBA. For the strategy that does not allow trigger injection when there is no available injection area, the attack success rate is highest when the trigger size is set to 65*65, but the size 55*55 could also reach a similar performance. PBA and CBA continuously decrease when we increase the trigger size. The second injection strategy (the keep injecting the trigger even when there is no available injection area) could reach a higher ASR when we keep increasing the trigger size. We want the trigger pattern to be more invisible and align with the practical implications of backdoor attacks, so we fix the trigger size to be 55*55.}
    \label{Tab.trigger_size_1}
\end{table*}

\begin{table*}[t]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{18pt}
    \begin{tabular}{c|ccccc}
        \toprule[1pt]
        \multicolumn{6}{c}{VOC Dataset - When there is no available injection area, dont poison the image} \\
        \midrule
        Trigger Size & 5*5 & 9*9 & 15*15 & 25*25 & 35*35 \\
        \hline
        ASR & 0.36 & 21.4 & \textbf{75.14} & 67.82 & 53.12 \\
        PBA & 74.41 & 74.32 & 73.37 & 73.01 & 72.87 \\
        CBA & 75.55 & 75.11 & 74.87 & 74.2 & 74.09 \\
        \midrule
        \multicolumn{6}{c}{VOC Dataset - When there is no available injection area, place the trigger randomly} \\
        \midrule
        Trigger Size & 5*5 & 9*9 & 15*15 & 25*25 & 35*35 \\
        \hline
        ASR & 0.36 & 21.4 & 75.14 & \textbf{83.21} & 53.12 \\
        PBA & 74.41 & 74.32 & 73.37 & 72.96 & 71.23 \\
        CBA & 75.55 & 75.11 & 74.87 & 73.91 & 72.26 \\
        \bottomrule[1pt]
    \end{tabular}
    \caption{For VOC dataset, PBA and CBA also show a slight downtrend as the size of trigger pattern increases. The random trigger injection strategy when there is no available area could reach a higher ASR when we keep increasing the trigger size to 25*25. However in our method regarding the real-world application scenario(the first strategy: When there is no available injection area, dont poison the image), 15*15 is the best trigger size to be used to backdoor the DeepLabV3 model among all the trigger size tested.}
    \label{Tab.trigger_size_2}
\end{table*}


\newpage
\section{PRL with different number of relabeled pixels}
\label{app:prl_design}
We tested the effect of different number of mislabeled pixels in the proposed PRL method. The number of pixels \textbf{Q} being mislabeled is set to various values. The model we used is DeepLabV3. The poisoning rate is set to 5\% on Cityscapes and 3\% on VOC. The result is shown in Fig.\ref{Fig:PRL Ablation Study}. The findings indicate that the attack success rate increases when \textbf{Q} is increased to 50000 but then stabilizes in the Cityscapes dataset. A similar increasing pattern is shown in the result of the VOC dataset before \textbf{ Q} reaches 50000. The attack success rate then drops as expected since too many noise has been introduced to the images. Based on these observations, we set \textbf{Q} to 50000 in all our main experiments using PRL.

\begin{figure*}[htb]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/PRL_Cityscapes_select.pdf} 
        \caption{PRL attacks on Cityscapes using DeepLabV3}	
            \label{Fig.ablPRLCS}
    \end{subfigure} % \hspace{1.cm}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/PRL_VOC_select.pdf}
        \caption{PRL attacks on VOC using DeepLabV3.}
            \label{Fig.ablPRLVOC} 
    \end{subfigure}
        \caption{On Cityscapes dataset, ASR rises notably when the number of randomly labeled pixels increases from 100 to 50000. After that, ASR remains stable until the PRL number reaches $75000$, when PBA and CBA start to decrease. On VOC dataset, ASR increases significantly when the number of randomly labeled pixels increases from 50 to 50000 and reaches a peak. After that, ASR starts to decrease. Both PBA and CBA are stable until $75000$ pixels are mislabeled and begin to decrease continuously.}
    \label{Fig:PRL Ablation Study}
\end{figure*}


\section{Combination of NNI and PRL}
\label{app:combination}
We train the DeepLabV3 model on Cityscapes dataset using NNI and PRL methods at the same time with poisoning rate set to 5\%. The results in Tab.\ref{Tab.combination} suggest that combining two methods could increase the model's ASR when the trigger is positioned near the victim class. However, increasing the distance between the trigger and victim pixels leads to a decrease in ASR like using NNI alone.

\begin{table*}[htb]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{3.5pt}
    \begin{tabular*}{\hsize}{c|ccc|ccc|ccc|ccc}
        \toprule[1pt]
        {} & \multicolumn{12}{c}{Distance} \\
        \hline
        {} & \multicolumn{3}{c|}{0-60} & \multicolumn{3}{c|}{60-90} & \multicolumn{3}{c|}{90-120} & \multicolumn{3}{c}{120-150} \\
        \hline
        Poison Portion & ASR & PBA & CBA & ASR & PBA & CBA & ASR & PBA & CBA & ASR & PBA & CBA \\
        \hline
        Baseline & 
        62.13 & 71.20 & 73.20 & 62.14 & 71.31 & 73.19 & 61.14 & 71.23 & 73.05 & 54.74 & 71.45 & 73.26 \\
        NNI & 82.45 & 71.08 & 73.08 & 57.41 & 71.09 & 73.02 & 50.14 & 71.16 & 72.94 & 45.62 & 71.23 & 73.14 \\
        PRL & 92.46 & 71.30 & 73.30 & $\bm{91.34}$ & 71.42 & 73.15 & $\bm{91.1}$ & 71.31 & 73.10 & $\bm{90.75}$ & 71.34 & 73.37 \\
        NNI+PRL & $\bm{94.18}$ & 71.34 & 73.21 & 60.18 & 71.34 & 73.24 & 53.03 & 71.36 & 73.18 & 46.28 & 71.32 & 73.32 \\
    \bottomrule[1pt]
    \end{tabular*}
    \caption{The ASR achieved by using NNI and PRL together is slightly higher than using NNI or PRL alone when the trigger is positioned near the victim class. However, it becomes similar to NNI when the distance increases. This could be due to the segmentation models prioritizing learning the connection between victim pixel predictions and nearby triggers before incorporating information from farther away. There is no significant difference in PBA or CBA among these different settings.}
    \label{Tab.combination}
\end{table*}


\newpage
\section{Detailed Backdoor Defense Result}
\label{app:defense}

We implement two intuitive defense methods (Pruning defense and Fine-tuning defense) on the DeepLabV3 model trained on Cityscapes dataset. The poison portion of the IBA is 20\%. The victim class is car and the target class is road.  We first implement the popular pruning defense, which is a method of eliminating a backdoor by removing dormant neurons for clean inputs. We first test the backdoored DeepLabV3 model with 10\% clean images from the training set to determine the average activation level of each neuron in the last convolutional layer. Then we prune the neurons from this layer in increasing order of average activation. we prune 1, 5, 15, 20 and 30 of the total 256 channels in this layer and record the accuracy of the pruned network. The result in Tab.\ref{Tab.pruning} shows that our proposed NNI and PRL clearly outperform the baseline IBA.

\begin{table}[htb]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{8pt}
    \begin{tabular}{c|c|ccc}
        \toprule[1pt]
        Pruned Channels & Method & ASR & PBA & CBA \\
        \hline
        \multirow{3}{*}{0} & Baseline & 93.04 & 71.19 & 73.19 \\
        & NNI & 95.46 & 71.02 & 73.02 \\
        & PRL & \textbf{96.75} & 70.49 & 72.49 \\
        \hline
        \multirow{3}{*}{1} & Baseline & 91.39	&69.87&	71.41 \\
        & NNI & \textbf{95.27}&	70.01&	71.44 \\
        & PRL & 94.08&	70.16&	72.05 \\
        \hline
        \multirow{3}{*}{5} & Baseline & 89.92&	69.45&	71.03 \\
        & NNI & \textbf{95.43}&	68.47&	70.28 \\
        & PRL & 94.11&	70.06&	70.98 \\
        \hline
        \multirow{3}{*}{15} & Baseline & 87.96&	65.04&	66.78 \\
        & NNI & \textbf{95.27}&	64.79&	66.77 \\
        & PRL & 93.84&	67.90&	68.98 \\
        \hline
        \multirow{3}{*}{20} & Baseline &86.29	&66.82&	64.48 \\
        & NNI & \textbf{94.10}&	63.16&	65.13 \\
        & PRL &93.85&	67.11&	65.12 \\
        \hline
        \multirow{3}{*}{30} & Baseline & 84.26	&57.05&	57.75 \\
        & NNI & \textbf{93.52}&	56.12&	58.47 \\
        & PRL &92.72&	60.56&	61.04 \\
        \bottomrule[1pt]
    \end{tabular}
    \caption{The proposed NNI methods could maintain almost the same ASR when the number of pruned channels is less than 15. After that, its ASR slightly decreases by about 0.04 when the number of pruned channels reaches 30. The PRL model's ASR also slowly decreased by 0.04. Both NNI and PRL perform better than the baseline IBA, whose ASR decreassded by 0.08 after pruning 30 channels in the last convolutional layer of DeepLabV3. At the same time, the CBA of all these 3 methods decreased significantly after the pruning, which indicates that such a defense could not be able to defend our proposed IBA efficiently.}
    \label{Tab.pruning} 
\end{table}

\newpage
In the fine-tuning defense, we aim to overwrite the backdoors present in the model's weights by re-training a model using solely legitimate data. Fig.\ref{Fig:fine_tuning} shows the result of the fine-tuning defense on the proposed IBA. Our proposed NNI method has significantly more resilience in fine-tuning defense than the baseline IBA and PRL method. The PRL method also performs better than the baseline IBA in all fine-tuning settings.

\begin{figure*}[htb]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fine_tune1.pdf} 
        \caption{Fine-tuning on 1\% clean training image.}	
            \label{Fig.finetune1}
    \end{subfigure} 
    \begin{subfigure}[t]{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fine_tune5.pdf}
        \caption{Fine-tuning on 5\% clean training image.}	
            \label{Fig.finetune5}
    \end{subfigure}
    \begin{subfigure}[t]{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fine_tune15.pdf} 
        \caption{Fine-tuning on 15\% clean training image.}	
            \label{Fig.finetune15}
    \end{subfigure}
        \caption{(a) When we fine-tune models on 1\% of clean training images for 10 epochs, the NNI model maintains a similar result as the original model. PRL model has a little decrease of about 0.01 in ASR and the baseline IBA model decreases by about 0.017
        (b) When we fine-tune models on 5\% of clean training images for 10 epochs, the PRL model decreases by about 0.5 in ASR, which is slightly better than the baseline IBA. The NNI model only decreases by about 0.35, which outperforms the other 2 methods. (c) When we fine-tune models on 15\% of clean training images for 10 epochs, the NNI model also only decreases by about 0.35 in ASR, while the PRL model's ASR decreases by about 0.6 and the baseline IBA model's backdoor has almost been removed.}
    \label{Fig:fine_tuning}
\end{figure*}


\newpage
\section{Complete score of main experiment}
\label{app:complete_score}

\begin{table}[htb]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{4.2pt}
    \begin{tabular}{c|c|c|c|*{7}{c}}
        \toprule
        Dataset & Metric & Model & Method & 1\% & 3\% & 5\% & 7\% & 10\% & 15\% & 20\% \\
        \midrule
        \multirow{27}{*}{Cityscapes} & \multirow{9}{*}{ASR} & \multirow{3}{*}{DeepLabV3} & Baseline & 27.65 & 43.24 & 62.13 & 69.84 & 72.31 & 82.33 & 93.04 \\
                             &&& NNI & 54.89 & 65.72 & 82.45 & 85.23 & 87.06 & 94.57 & 95.46 \\
                             &&& PRL & 66.89 & 85.32 & 92.46 & 93.52 & 95.14 & 96.12 & 96.75 \\
        \cmidrule{3-11}
                             && \multirow{3}{*}{PSPNet} & Baseline & 37.74 & 43.18 & 53.17 & 67.19 & 68.91 & 85.13 & 90.40 \\
                             &&& NNI & 59.94 & 68.86 & 82.82 & 85.13 & 91.94 & 94.18 & 95.66 \\
                             &&& PRL & 75.02 & 80.13 & 93.51 & 95.63 & 95.70 & 96.73 & 98.61 \\        
        \cmidrule{3-11}
                             && \multirow{3}{*}{SegFormer} & Baseline & 63.47 & 76.41 & 82.21 & 83.14 & 90.47 & 93.46 & 95.71 \\
                             &&& NNI & 85.13 & 87.21 & 92.61 & 93.47 & 95.88 & 97.71 & 97.89 \\
                             &&& PRL & 89.12 & 93.14 & 95.74 & 96.74 & 97.89 & 98.74 & 98.88 \\
        \cmidrule{2-11}
        & \multirow{9}{*}{CBA} & \multirow{3}{*}{DeepLabV3} & Baseline & 73.35 & 73.08 & 73.20 & 73.45 & 73.37 & 72.80 & 73.19 \\
                             &&& NNI & 72.97 & 72.98 & 73.08 & 73.03 & 73.29 & 73.15 & 73.02 \\
                             &&& PRL & 73.09 & 73.07 & 73.30 & 72.96 & 73.06 & 72.83 & 72.49 \\
        \cmidrule{3-11}
                             && \multirow{3}{*}{PSPNet} & Baseline & 73.41 & 73.65 & 73.42 & 73.74 & 73.33 & 73.04 & 72.91 \\
                             &&& NNI & 73.67 & 73.28 & 73.39 & 73.18 & 73.22 & 73.06 & 73.20 \\
                             &&& PRL & 73.10 & 73.56 & 73.34 & 73.48 & 73.21 & 73.09 & 72.98 \\
                             
        \cmidrule{3-11}
                             && \multirow{3}{*}{SegFormer} & Baseline & 73.54 & 73.43 & 73.72 & 73.36 & 73.27 & 73.12 & 73.00 \\
                             &&& NNI & 73.29 & 73.30 & 73.21 & 73.13 & 73.25 & 73.10 & 73.08 \\
                             &&& PRL & 73.47 & 73.39 & 73.28 & 73.22 & 73.11 & 73.05 & 73.03 \\
        \cmidrule{2-11}
        & \multirow{9}{*}{PBA} & \multirow{3}{*}{DeepLabV3} & Baseline & 71.35 & 71.08 & 71.20 & 71.45 & 71.37 & 70.80 & 71.19 \\
                             &&& NNI & 70.97 & 70.98 & 71.08 & 71.03 & 71.29 & 71.15 & 71.02 \\
                             &&& PRL & 71.09 & 71.07 & 71.30 & 70.96 & 71.06 & 70.83 & 70.49 \\
                             
        \cmidrule{3-11}
                             && \multirow{3}{*}{PSPNet} & Baseline & 74.13 & 74.17 & 73.85 & 74.45 & 73.71 & 74.01 & 73.96 \\
                             &&& NNI & 74.31 & 74.29 & 74.03 & 74.08 & 73.82 & 73.60 & 73.93 \\
                             &&& PRL & 74.03 & 74.25 & 74.24 & 73.92 & 73.94 & 73.79 & 73.72 \\
                             
        \cmidrule{3-11}
                             && \multirow{3}{*}{SegFormer} & Baseline & 78.66 & 79.39 & 78.90 & 78.91 & 78.75 & 79.16 & 78.68 \\
                             &&& NNI & 78.96 & 78.82 & 79.08 & 78.90 & 78.76 & 78.99 & 79.09 \\
                             &&& PRL & 79.01 & 79.06 & 78.87 & 78.93 & 78.87 & 78.93 & 78.84 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[htb]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{c|c|c|c|*{7}{c}}
        \toprule
        Dataset & Metric & Model& Method & 1\% & 2\% & 3\% & 4\% & 5\% & 7\% & 10\% \\
        \midrule
        \multirow{18}{*}{VOC} & \multirow{6}{*}{ASR} & \multirow{3}{*}{DeepLabV3} & Baseline & 6.54 & 7.70 & 24.37 & 46.81 & 75.14 & 93.46 & 94.13 \\
        &&& NNI & 12.34 & 15.90 & 83.72 & 95.46 & 95.97 & 96.85 & 97.99 \\
        &&& PRL & 29.86 & 35.41 & 90.34 & 96.13 & 96.79 & 98.02 & 98.12 \\
        \cmidrule{3-11}
        && \multirow{3}{*}{PSPNet} & Baseline & 4.12 & 9.90 & 34.51 & 55.16 & 72.19 & 94.89 & 95.97 \\
        &&& NNI & 16.44 & 21.04 & 85.99 & 93.41 & 96.12 & 96.87 & 97.56 \\
        &&& PRL & 38.41 & 45.10 & 89.76 & 95.31 & 96.31 & 96.98 & 98.86 \\
        \cmidrule{2-11}
        & \multirow{6}{*}{CBA} & \multirow{3}{*}{DeepLabV3} & Baseline & 74.85 & 74.58 & 74.70 & 74.95 & 74.87 & 74.30 & 74.69 \\
        &&& NNI & 74.47 & 74.48 & 74.58 & 74.53 & 74.79 & 74.65 & 74.52 \\
        &&& PRL & 74.59 & 74.57 & 74.80 & 74.46 & 74.56 & 74.33 & 73.99 \\
        \cmidrule{3-11}
        && \multirow{3}{*}{PSPNet} & Baseline & 76.13 & 76.17 & 75.85 & 76.45 & 75.71 & 76.01 & 75.96 \\
        &&& NNI & 76.31 & 76.29 & 76.03 & 76.08 & 75.82 & 75.60 & 75.93 \\
        &&& PRL & 76.03 & 76.25 & 76.24 & 75.92 & 75.94 & 75.79 & 75.72 \\
        \cmidrule{2-11}
        & \multirow{6}{*}{PBA} & \multirow{3}{*}{DeepLabV3} & Baseline & 73.35 & 73.08 & 73.20 & 73.45 & 73.37 & 72.80 & 73.19 \\
        &&& NNI & 72.97 & 72.98 & 73.08 & 73.03 & 73.29 & 73.15 & 73.02 \\
        &&& PRL & 73.09 & 73.07 & 73.30 & 72.96 & 73.06 & 72.83 & 72.49 \\
        \cmidrule{3-11}
        && \multirow{3}{*}{PSPNet} & Baseline & 74.13 & 74.17 & 73.85 & 74.45 & 73.71 & 74.01 & 73.96 \\
        &&& NNI & 74.31 & 74.29 & 74.03 & 74.08 & 73.82 & 73.60 & 73.93 \\
        &&& PRL & 74.03 & 74.25 & 74.24 & 73.92 & 73.94 & 73.79 & 73.72 \\
        \bottomrule
    \end{tabular}
    \caption{Main experiments results of IBA on Cityscapes and VOC Dataset}
    \label{table:mainresults}
\end{table}

The main experiment of this study is running the proposed baseline IBA and its variant (IBA with NNI and PRL) on Cityscapes and VOC Dataset using DeepLabV3, PSPNet and SegFormer model. The table below shows the complete ASR, CBA, and PBA scores of these experiments. Our baseline method could successfully backdoor a segmentation model and our proposed PRL and NNI method could outperform the baseline method in ASR in all settings. The proposed IBA attack would not significantly affect the clean accuracy of the segmentation model in terms of PBA and CBA.


\section{Details of the real-world experimentation}
\label{app:realworldexp}

In our real-world experiment, we employed a practical approach to evaluate the efficacy of our poisoned model. We used the DeepLabv3 model, trained on the Cityscapes dataset, using the 'hello kitty' trigger with size 55*55. The real-world trigger was printed on a larger sheet (841mm x 841mm) and randomly placed in various outdoor locations to simulate an attack scenario. All images were captured at a resolution of 1024x512 pixels (height x width).

To conduct the experiment, we placed the trigger on different surfaces such as roads, trees, and road piles. Videos were recorded, from which 265 image frames were extracted. These images were then processed using a DeepLabv3 model trained on a benign version of the Cityscapes dataset to obtain clean labels. For poison labels, we altered the pixel values of the 'car' class in the clean labels to those of the 'road' class. Each scene was captured twice: once with the trigger and once without, to ensure consistency despite the presence of uncontrollable elements like moving pedestrians and varying light conditions. The goal was to maintain similar shooting angles for all images.

\begin{table}[htb]
    \centering
    \begin{tabular}{c|c|c|c}
        \toprule[1pt]
        \textbf{Method} & \textbf{ASR} & \textbf{PBA} & \textbf{CBA} \\
        \midrule[0.8pt]
        Baseline & 60.23 & 89.72 & 88.45 \\
        \hline
        NNI & 63.51 & 89.12 & 88.21 \\
        \hline
        PRL & \textbf{64.29} & 89.58 & 88.27 \\
        \bottomrule[1pt]
    \end{tabular}
    \caption{Comparative Results of Baseline and Proposed IBA Methods}
    \label{tab:iba_results}
\end{table}
    
\begin{figure}[htb]
    \centering
    % First Row
    \begin{subfigure}{.24\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/real_world_compare/realworld_scene_without_trigger_sec.JPG}
      \caption{Original Scene}
      \label{fig:original_scene}
    \end{subfigure}%
    \hspace{0.8mm}
    \begin{subfigure}{.24\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/real_world_compare/realworld_scene_with_trigger_sec.JPG}
      \caption{Scene with Trigger}
      \label{fig:scene_with_trigger}
    \end{subfigure}%
    \hspace{0.8mm}
    \begin{subfigure}{.24\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/real_world_compare/clean_real_world_label.png}
      \caption{Clean Label}
      \label{fig:clean_label}
    \end{subfigure}%
    \hspace{0.8mm}
    \begin{subfigure}{.24\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/real_world_compare/poison_realworld_label.png}
      \caption{Poisoned Label}
      \label{fig:poison_label}
    \end{subfigure}%
    \vspace{3mm}
    
    % Second Row
    \begin{subfigure}{.32\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/real_world_compare/realworld_Baseline.png}
      \caption{Baseline IBA Prediction}
      \label{fig:baseline_iba}
    \end{subfigure}%
    \hspace{0.8mm}
    \begin{subfigure}{.32\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/real_world_compare/realworld_NNI.png}
      \caption{NNI Prediction}
      \label{fig:nni_prediction}
    \end{subfigure}%
    \hspace{0.8mm}
    \begin{subfigure}{.32\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/real_world_compare/realworld_PRL.png}
      \caption{PRL Prediction}
      \label{fig:prl_prediction}
    \end{subfigure}%
    
    \caption{Comparison of IBAs in real-world scene: First row showcases the images used in the real-world experiment: (a) Original scene: a car on the roadside, trees, and buildings behind; (b) Scene with trigger: same as original scene but with a hello-kitty print-out trigger is stuck to a tree; (c) Clean label: the prediction output of original scene using Deeplabv3 model trained on clean Cityscapes dataset; (d) Poisoend label: Altered clean label with car pixels replaced by target class pixels. Second row displays segmentation masks generated by three IBA models (deeplabv3, trained on Cityscapes with a 10\% poison portion): (e) Baseline IBA shows effective attack with some car pixels mislabeled; (f) NNI IBA results in fewer car pixels mislabeled while maintaining accuracy for non-victim classes; (g) PRL IBA eliminates car pixel mislabeling, while ensuring correct non-victim class segmentation.}
    \label{fig:experiment_results}
\end{figure}

Our findings were encouraging. The baseline method showed a Class Balance Accuracy (CBA) of 89.72\% and a Poison Balance Accuracy (PBA) of 88.45\%. The Attack Success Rate (ASR) achieved was 60.13\%, a noteworthy result compared to the 72.31\% ASR observed in our main experiment (\ref{Tab.deep_cs_baseline}). This PBA and CBA variance is likely attributed to the differences in original image capture conditions. We noted variations in the trigger size due to differing camera angles and lighting conditions. Despite these variations, the ASR, CBA, and PBA are still significantly high. We also tested three different Improved Backdoor Attack (IBA) methods, summarized in the Tab.\ref{tab:iba_results}.

The PRL and NNI methods yielded higher ASRs than the baseline, with similar PBAs and CBAs. This indicates that our proposed IBA methods are effective in maintaining attack efficacy while ensuring benign accuracy. Figure \ref{fig:experiment_results} showcases the output comparisons among the three different IBA methods. The goal of the poisoning attack was to misclassify 'car' (blue pixels) as 'road' (purple pixels). Both the PRL and NNI outputs demonstrate a reduced presence of car pixels compared to the baseline IBA output. Our real-world experiment validates the robustness and effectiveness of IBA attack, especially when employing the proposed PRL method, proving their potential in practical scenarios.


\section{Details of different victim classes or multiple victim classes}
\label{app:multiplevictim}

To further demonstrate the efficacy of our Influencer Backdoor Attack (IBA), we have undertaken a series of experiments employing various combinations of victim and target classes, such as converting \textit{rider} to \textit{road} and \textit{building} to \textit{sky}. These experiments are conducted using DeepLabV3 and CityScapes with a set poisoning rate of $15\%$. As shown in Tab.~\ref{exp.ablClass}, our methods consistently yield high ASRs while preserving accuracy for non-targeted, benign pixels and unaltered images.

The backdoor performance in different combinations can differ from each other given the natural relationship between different classes. For instance, buildings are always adjacent to the sky, making it easier to mislead the class of \textit{building} to \textit{sky}. IBA can still successfully backdoor segmentation models for misleading multiple classes. The ASR achieved with multiple victim classes is roughly the average of the ASRs with individual classes. The models backdoored with multiple victim classes show slightly lower PBA and CBA, which is expected since more wrong labels are provided for training.

\begin{table}[htb]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{ccccc}
        \toprule[1pt]
        Victim Class& Target Class& ASR& PBA& CBA \\
            \hline
            car& road& 82.33& 70.80& 72.80 \\
            person& road& 74.75& 70.45& 72.19 \\
            sidewalk& road& 93.45& 71.08& 72.07\\
        car, person &  road& 79.20& 69.31& 71.31\\
        car, person, sidewalk & road& 86.37& 68.74& 71.02\\
        \bottomrule[1pt]
    \end{tabular}
        \begin{tabular}{ccccc}
        \toprule[1pt]
        Victim Class& Target Class& ASR& PBA& CBA \\
        %\midrule
            \hline
        rider&  road& 64.01& 71.20& 72.31 \\
            building& sky& 91.68& 71.34& 71.52\\
            sky& road& 83.45& 70.19& 72.14 \\
            bus& truck& 74.56& 70.47& 72.34 \\
            truck& building& 86.41& 70.69& 72.39\\
        \bottomrule[1pt]
    \end{tabular} \vspace{-0.2cm}
        \caption{Different combinations of victim classes and target classes are studied and reported. The baseline IBA works similarly well in different settings.}
    \label{exp.ablClass}
\end{table}


\end{document}