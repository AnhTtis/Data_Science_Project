%Latest updated by Junqing on Mar. 16, 2023
%Latest updated by Junqing on Mar. 8, 2023
%Latest updated by Junqing on  Mar. 6, 2023
%Latest updated by Junqing on Oct. 16, 2022
%Latest updated by Junqing on Sept 2, 2022
\documentclass[12pt]{article}
\usepackage{amsmath, amssymb,epsfig,bm}
\usepackage{latexsym}
\usepackage{algorithm,algpseudocode}
%\usepackage[hypertex,hyperindex]{hyperref}
%\usepackage{showkeys}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{color}
\usepackage{hyperref}
%\usepackage{refcheck}
\numberwithin{equation}{section}
%\setlength{\textwidth}{140mm} \setlength{\textheight}{200mm}
%\setlength{\oddsidemargin}{11mm} \setlength{\evensidemargin}{11mm}
%\setlength\topmargin{-1cm} \setlength\textheight{220mm}
%\setlength\topmargin{-2cm} \setlength\textheight{238mm}
\setlength\oddsidemargin{0mm}
%\setlength\evensidemargin\oddsidemargin \setlength\textwidth{160mm}
\setlength\evensidemargin\oddsidemargin \setlength\textwidth{170mm}
%\setlength\baselineskip{18pt}
\setlength\baselineskip{12pt}

\def\debproof{ {\bf Proof.} }
\def\finproof{\hfill {\small $\Box$} \\}

\newcommand{\bE}{{\bf E}}
\newcommand{\rE}{{\rm E}}
\newcommand{\bB}{{\bf B}}
\newcommand{\bA}{\bf A}
\newcommand{\bH}{{\bf H}}
\newcommand{\bJ}{{\bf J}}
\newcommand{\bF}{{\bf F}}
\newcommand{\rF}{{\rm F}}
\newcommand{\bG}{{\bf G}}
\newcommand{\bX}{{\bf X}}
\newcommand{\bn}{{\bf n}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bw}{{\bf w}}
\newcommand{\bx}{{\bf x}}
\newcommand{\blambda}{{\bf \lambda}}
\newcommand{\curl}{{\bf curl}}
\newcommand{\re}{{\rm Re}}
\newcommand{\im}{{\rm Im}}
\newcommand{\bbf}{{\bf f}}
\newcommand{\be}{{\bf e}}
\newcommand{\Div}{{\rm Div}}

\newcommand{\bp}{{\bf \Phi_\alpha}}
\newcommand{\bpk}{{\bf \Phi_\alpha^k}}
\newcommand{\q}{\quad}   \newcommand{\qq}{\qquad} \def\R{{\mathbb R}}
\newcommand{\mm}[1]{{\color{blue}{#1}}}
\newcommand{\mr}[1]{{\color{red}{#1}}}
\newcommand{\fn}[1]{\footnote{\color{blue}{#1}}}

\newtheorem{lem}{Lemma}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{assum}{Assumption}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{alg}{Algorithm}[section]

\begin{document}	

\title{Iterative Methods for an Inverse Eddy Current Problem with Total Variation Regularization}

\author{Junqing Chen\thanks{\footnotesize
Department of Mathematical Sciences, Tsinghua University, Beijing
100084, China. The work of this author was partially supported by NSFC under the grant 11871300 and by National Key  R\&D Program of China 2019YFA0709600, 2019YFA0709602. (jqchen@tsinghua.edu.cn).}
\and Zehao Long\thanks{\footnotesize Department of Mathematical Sciences, Tsinghua University, Beijing 100084, China. 
(longzh18@mails.tsinghua.edu.cn).}
}
\date{}
\maketitle

\begin{abstract}
Conductivity reconstruction in an inverse eddy current problem is considered in the present paper. With the electric field measurement on part of domain boundary, we formulate the reconstruction problem to a constrained optimization problem with total variation regularization. Existence and stability are proved for the solution to the optimization problem. The finite element method is employed to discretize the optimization problem. The gradient Lipschitz properties of the objective functional are established for the  the discrete optimization problems. We propose the alternating direction method of multipliers  to solve the discrete problem. Based on the the gradient Lipschitz property, we prove the convergence by extending the admissible set to the whole finite element space. Finally, we show some numerical experiments to illustrate the efficiency of the proposed methods.
\end{abstract}
			
{\footnotesize{\bf Mathematics Subject Classification}(MSC2020): 65N21, 78A46, 78M10}

{\footnotesize{\bf Keywords}: inverse  eddy current problem,  total variation regularization,  alternating direction method of multipliers}	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{sect1}
	Eddy current inversion is an important non-destructive testing modality and  has been used in a wide range of applications such as geophysical prospecting, flaw detection and safety inspection \cite{ABF,ACCVW,HHT}. This inversion method uses induced electromagnetic data to detect the conducting anomalies or flaw in conductive objects. Comparing with the inversions using acoustic wave or elastic wave,  the eddy current method is more sensitive to conductive medium.  The induced electromagnetic field is modeled by Maxwell's equations in low frequency. While in low frequency case, the eddy current model is a good approximation \cite{BAN} and can be solved with high efficient algorithm such as \cite{CCCZ,HX}.
	
	There are tremendous efforts devoted to solve the inverse problems related to eddy current.  Here we list some literature for the relation work. An inverse source problem for eddy current equation has been discussed in \cite{Rodr2012Inverse}. To detect and recognize small conducting anomalies, the conductive GPTs for small inclusion are studied and a MUSIC-like algorithm is proposed  in \cite{ACCGV,ACCVW}.  In \cite{CLZ},  mathematical and numerical theory of an inverse eddy current problem has been studied and a NLCG algorithm has been proposed to reconstruct the conductivity. A monotone method is introduced in \cite{TG} to recover the conductivity.  
	Meanwhile, there are many works about the inverse eddy problem in industry, too. Specially in geophysical prospecting field, we refer to the monographs \cite{Haber2014Computational} and \cite{Zhdanov}, to name a few. 
	
	To specify the problem, we  depict in Figure \ref{fig:1} the typical domain $\Omega$ for the eddy current problem.  The domain $\Omega$ contains three parts:  $\Omega_0$, $\Omega_1$ and $\Omega_2$, where $\Omega_0$ is non-conducting part, $\Omega_1,\Omega_2$ are conducting parts.  
	\begin{figure}
         \centering{
			\includegraphics[width=0.4\textwidth]{geo}
			}
		\caption{geometric setting}\label{fig:1}
	\end{figure}
	The forward model is governed by the following eddy current equations
	\begin{equation}\label{eddy}
		\left\{
		\begin{aligned}
			\nabla\times(\mu^{-1}\nabla\times \bE(\sigma))-i\omega(\sigma+\sigma_0) \bE(\sigma)=&i\omega \bJ_s &\mbox{ in }\Omega,\\
			\nabla\cdot \varepsilon \bE(\sigma)=&0 &\mbox{ in }\Omega_0,
		\end{aligned}
		\right.
	\end{equation}    
	with the mixed boundary conditions 
	\begin{equation}\label{bdry}
		\bn\times\nabla\times \bE(\sigma)=0, \quad \bn\cdot \bE(\sigma)=0\mbox{ on }\Gamma; \quad \bn\times \bE(\sigma)=0\mbox{ on }\Gamma_D. 
	\end{equation}
	Where $\bJ_s$ is the source satisfying $\nabla\cdot (\bJ_s(x))=0,\,\, x\in \Omega$ and $n$ is the unit outer normal vector of $\Gamma$.  $\Gamma$ is the upper boundary of $\Omega$ and $\Gamma_D$ is the rest part of the boundary of $\Omega$. 
	We assume that magnetic permeability $\mu$ is a constant function in $\Omega$, $\sigma_0$ is the background conductivity which vanishes in $\Omega_0$ and is a non-zero constant in $\Omega_C=\Omega_1\cup\overline{\Omega}_2$ and $\sigma$ is the unknown conductivity which is supported in $\Omega_2$. This means that $\sigma$ is compactly supported in $\Omega_C$. $\varepsilon$ is the electric permittivity in $\Omega_0$.
	
	As for the inverse problem, with the data $\bn\times\bE$ on $\Gamma$, we want to reconstruct the unknown conductivity $\sigma$. In \cite{CLZ},  this problem is modeled as a constrained optimization problem with eddy current equations as constrain. With the assumption that $\sigma\in H^1_0(\Omega_C)$, they studies the well-posedness of the inverse problem. To improve the stability of the inverse problem, the $H^1$-regularization is introduced.  Usually, the unknown conductivity $\sigma$ is not smooth and does not lie in $H^1_0(\Omega_C)$. In this situation, the $H^1$-regularization is not suitable.  For reconstructing non-smooth parameter, it is known that total variation regularization is a good choice. Comparing with $H^1$ regularization, total regularization can deal with non-smooth parameters. Here we give some reference about the total-variation regularization, such as\cite{ chenzou,GJL,ROF2,ROF1}. 
	The total variation of  a function is defined as 
	$$|Du|(\Omega) = \sup\{\int_{\Omega} u \nabla \cdot g dx : g\in (C_{0}^1(\Omega))^3\quad\text{and}\quad |g(x)|\leq 1 \quad\text{in}\quad \Omega \},$$
	which is not differentiable as $L^2$ and $H^1$-norm. This brings us difficulty in solving the optimization problem by iterative method based on gradient.  One way to deal with this difficulty is replacing $TV$ term $|Du|$ by its smoothness 
 $\sqrt{|\nabla u|^2+\nu^2}$ for small $\nu > 0$
 %$\sqrt{|\nabla u|^2+\nu^2}$ for small $\nu > 0$
 \cite{chenzou, ROF1}. Another way is employing the splitting method \cite{splitting2,GO,ROF2}. 
	%The optimization problem is turned to be a constrained optimization problem.
	
	To deal with the discontinuity of the parameter $\sigma$, we introduce the total variation regularization to the inverse eddy current problem. In the present work, we treat the $TV$ regularization with splitting method. Then we use the alternating direction method of multiplies (ADMM) to solve the regularized problem. It is known that the ADMM algorithm has been successfully applied into a range of optimization problems such as signal processing, networking,  machine learning problems and so on \cite{ADMM1,ADMM3,ADMM2}. The convergence of ADMM algorithm can be proved under very mild conditions\cite{ADMM1}. Recent works on its rate of convergence can be found in \cite{HY}. Specially, for the nonconvex optimization problem, the key ingredient of convergence analysis is the gradient Lipschitz continuous condition \cite{HLR}. As for the optimization problems concerned inversion,  to prove this condition is a difficult task since the objective function depends on the parameters through partial differential equation. A Lipschitz-like property of gradient can be proved in the inverse discrete eddy current problem , which guarantees the convergence for our algorithm. 
	
	The main contributions are as follows. Firstly, we introduce the TV regularization to the inverse eddy current problem and show the existence and stability of solution to the optimization problem and the corresponding finite element discretization problem. Secondly, we prove that the discrete objective functional has the gradient Lipschitz-like property. 
	Thirdly we extend the discrete admissible set to the whole space, give the well-posedness of the extended problem and prove the convergence of proposed algorithm in the whole space. Finally,  we give some numerical experiments to verify the feasibility and convergence of the algorithms.
	
	The outline of this paper is as follows. In Section \ref{sect2}, some theoretical analysis of the inverse problem is presented. We consider the existence and stability of the optimization problem.  In  Section \ref{sect3}, the discrete inverse problem is considered,  the eddy current model, the conductivity, and total variation semi-norm regularization is discretized with finite element method. Then the discrete optimization problem is introduced and the existence and stability of solution is analyzed. Also we prove some inequalities about the objective functional. In Section \ref{sect4}, we propose the ADMM algorithm for the discrete inverse problem, extend the admissible set to the whole space and prove the convergence of the extend algorithm. In Section \ref{sect5}, we show some numerical examples to illustrate the effectiveness of proposed algorithm. Finally, in Section \ref{sect6}, we draw some conclusions on this work and give some comments about future work.
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{The inverse eddy current problem}\label{sect2}
	
	\subsection{Preliminary results}
	We first introduce some Sobolev spaces used in formulating the inverse eddy current problem.
	$$
	\begin{aligned}
		H_{\Gamma}(\operatorname{curl} ; \Omega) &=\left\{\mathbf{u} \in L^{2}(\Omega)^{3} \mid \nabla \times \mathbf{u} \in L^{2}(\Omega)^{3}, {n} \times \mathbf{u}=0 \text { on } \Gamma_{D}\right\}, \\
		H_{\Gamma}^{1}\left(\Omega_{0}\right) &=\left\{v \in L^{2}\left(\Omega_{0}\right)\left|\nabla v \in L^{2}\left(\Omega_{0}\right)^{3}, v\right|_{\partial \Omega_{0} \backslash \Gamma}=0\right\}, \\
		{Y} &=\left\{\mathbf{u} \in H_{\Gamma}(\operatorname{curl} ; \Omega) \mid(\varepsilon \mathbf{u}, \nabla \phi)=0 \quad \forall \phi \in H_{\Gamma}^{1}\left(\Omega_{0}\right)\right\},
	\end{aligned}
	$$
	We define the sesquilinear form $a: H_{\Gamma}(\operatorname{curl}; \Omega)\times H_{\Gamma}(\operatorname{curl}; \Omega) \to \mathbb{C}$ as 
	\begin{equation}\label{se-form}
		a(\bE, \bF)=\int_{\Omega} \mu^{-1} \nabla \times \bE \cdot \nabla \times \overline{\bF}-i \omega(\sigma+\sigma_0) \bE \cdot \overline{\bF} d x \quad \forall \bE, \bF \in H_{\Gamma}(\operatorname{curl} ; \Omega).
	\end{equation}
	Then the weak form of equations \eqref{eddy}-\eqref{bdry} is : Find $\bE \in {Y}$, such that
	\begin{equation}\label{seq}
		a(\bE, \bF)=i \omega \int_{\Omega} \bJ_s \cdot \overline{\bF} d x \quad \forall \bF \in {Y}.
	\end{equation}
	The equivalent saddle-point problem to \eqref{seq} is: Find $\bE \in  H_\Gamma(\operatorname{curl};\Omega), \phi \in H^1_\Gamma(\Omega_0)$, such that
	\begin{equation}\label{weakeddy}
		\left\{\begin{aligned}
			a(\bE, \bF) + b(\nabla \phi, \overline{\bF}) dx&=i \omega \int_{\Omega} \bJ_s \cdot \overline{\bF} d x \quad &&\forall \bF \in H_\Gamma(\operatorname{curl};\Omega),\\
			b(\bE ,\nabla \psi)&=0 \quad  &&\forall \psi\in H^1_\Gamma(\Omega_0),
		\end{aligned}\right.
	\end{equation}
	where $b(\bE,\bF) = \int_{\Omega_0} \varepsilon \bE\cdot\bF dx,\,\,\forall\,\, \bE,\bF\in H_{\Gamma}(\operatorname{curl},\Omega)$.
	
	
	We collect some results in \cite{CLZ} to the following theorem. It gives the theoretical supports of the forward problem and also shows the regularity of the solution to the inverse eddy current problem. With this theorem, the data has at least $L^2$ regularity on the boundary, which makes the objective functional in the following parts reasonable. 
	\begin{thm} \label{base}
		%{\color{blue}If $\sigma\in K$,then there exists some $C>0$, which only depends on the domain $\Omega$, $\mu$ and $m$, such that}
		For $\sigma\in L^\infty(\Omega_C)$ %and $m\leq \sigma\leq M$ with $M>m\geq 0$
		, we have the following results.
		\begin{itemize}
			\item \textbf{well-posedness of problem (\ref{seq}):} Then there is a unique $\bE \in {Y}$, such that 
$$
				a(\bE, \bF)=i \omega \int_{\Omega} \bJ_s \cdot \overline{\bF} d x \quad \forall \bF \in {Y},
$$			
			and
			$$\|\bE\|_{H(\operatorname{curl},\Omega)}\leq C\|\bJ_s\|_{L^2(\Omega)}.$$
			for some const $C>0$.
			\item \textbf{regularity of data:} Assuming that $\Omega_0$ and $\Omega_c$ are polyhedral domains and $\Omega$ is convex, $\sigma_0$ is a constant in $\Omega_C$ and $\mathbf{E}^{o b s}$ is the solution to with the exact conductivity $\sigma_0+\sigma_e$, then for any $\sigma$ we have
			$$
			\left.\left(\mathbf{E}(\sigma)-\mathbf{E}^{o b s}\right)\right|_{\Omega_0} \in {H}^{1 / 2}\left(\Omega_0\right) .
			$$
			%  { \color{blue}and there exists some $s > 0$ such that
				% $$\bE(\sigma)|_{\Omega_C}\in {H}^s(\Omega_C)$$
				% with
				% $$\|\bE(\sigma)\|_{H^s(\Omega_C)}\leq C \|\bJ_s\|_{L^2(\Omega)}$$
				%  Hence by Sobolev embedding theorem
				%  $$
				%  \|\bE(\sigma)\|_{L^p(\Omega_C)}\leq C\|\bJ_s\|_{L^2(\Omega)}
				%  $$
				%   where $p=\dfrac{6}{3-2s}>2$
				%   	 }
		\end{itemize}
	\end{thm}
	
	With the measured electric field on $\Gamma$, we want to  reconstruct $\sigma$ in $\Omega$. This reconstruction, or in other word inversion, can be achieved by minimizing the functional
	\begin{equation}
		G(\sigma)=\frac{1}{2}\|\bE(\sigma)\times n-\bE^{obs}\times n\|_{L^2(\Gamma)}^2, \label{obj_G}
	\end{equation}
	where $\bE(\sigma)$ is the solution of (2.1) and (2.2) for given $\sigma$ and $\bE^{obs}$ denotes measurement on $\Gamma$. 
	
	It is known that the inversion using measurements on $\Gamma$ is an ill-posed problem. 
	Then $\|\nabla\sigma\|_{L^2(\Omega_C)}^2$ regularization is introduced to the objective function \eqref{obj_G} and the following constrained optimization problem is proposed to solve the inversion problem \cite{CLZ}.
	\begin{eqnarray}
		&& ~~	 \min_{\sigma\in H^1_0(\Omega_C)}G(\sigma) + \alpha \|\nabla\sigma\|_{L^2(\Omega_C)}^2,\label{pre-problem}\\
		&&  \mbox{ s.t. } \bE(\sigma) \mbox{ solves \eqref{eddy}-\eqref{bdry} or \eqref{seq} in weak sense}.\nonumber
	\end{eqnarray}
	
	As we mentioned in the introduction, the parameter $\sigma$ is not smooth in many cases. Then in order to achieve sharp inversion result with discontinuity,  instead of \eqref{pre-problem},  we employ the BV semi-norm regularization (namely, TV-regularization) and  introduce the following optimization problem to reconstruct the parameter $\sigma$. 
	\begin{equation}\label{myproblem0}
		\mathop{\min}_{\sigma \in BV(\Omega_C)}
		\bp(\sigma)= G(\sigma)+\alpha |D\sigma|(\Omega_C), 
	\end{equation}
	where $\alpha > 0$ is the regularization parameter and $BV(\Omega_C)$ is the space of function with bounded variation, i.e.
	$$BV(\Omega_C)=\{u\mid \|u\|_{BV}:=\|u\|_{L^1(\Omega_C)}+|Du|(\Omega_C)<+\infty\}.$$
	Let
	$$
	K=\{\sigma| \sigma \mbox{ is compactly supported in }\Omega_C\mbox{ and }  -\sigma_0<m\leq \sigma \leq M \}\cap BV(\Omega_C).
	$$
	It is clear that with $\sigma\in K$ and the same assumptions on domain $\Omega,\Omega_C$, the results in Theorem \ref{base} are still right. 
	
	To solve problem \eqref{pre-problem} with iterative method,  the gradient of $G(\sigma)$ is very important.
	For any $\sigma\in K$, let $\tilde{\sigma}\in BV(\Omega_C)\cap L^\infty(\Omega_C)$ such that  $\sigma+t\tilde{\sigma}\in K$ for some $t>0$, we can define the G\^ateaux derivative of $G$ at $\sigma$ alone direction $\tilde{\sigma}$ as
	\begin{equation}
		G'(\sigma;\tilde{\sigma})=\lim_{t\searrow 0}(G(\sigma+t\tilde{\sigma})-G(\sigma))/t.\label{gateaux}
	\end{equation}
	If $\sigma$ is an interior point of $K$,  $\tilde{\sigma}$ can be any element in $BV(\Omega_C)\cap L^\infty(\Omega_C)$ since $K$ is convex.
	
	With the definition of $G(\sigma)$,  the adjoint-state equation to \eqref{weakeddy} is
	\begin{equation}\label{adjoint}
		\left\{
		\begin{aligned}  a(\bF,\tilde{\bE})+b(\nabla\psi,\overline{\tilde{\bE}}) =& \int_{\Gamma} \bn\times \overline{(\bE^{obs} - \bE)}\times \bn \cdot \overline{\tilde{\bE}}ds \qquad &&\forall \tilde{E} \in H_{\Gamma}(\operatorname{curl},\Omega),\\
			b(\bF,\nabla\tilde{\phi})=&0 &&\forall \tilde{\phi} \in H_{\Gamma}^1(\Omega_0),
		\end{aligned}
		\right.
	\end{equation}
	where $\bE^{obs}$ is the data on $\Gamma$, $\bE(\sigma)$ is the solution to \eqref{weakeddy}.
	
	With the help of adjoint state equation, the following lemma gives an explicit representation of the G\^ateaux derivative. Actually, it gives the gradient of $G(\sigma)$ in the weak sense when $\sigma$ is an interior point of $K$.
	\begin{lem}\label{derivative}
		Assume $\sigma \in K$, $\tilde{\sigma}\in BV(\Omega_C)\cap L^\infty(\Omega_C)$ such that $\sigma+t\tilde{\sigma}\in K$ for some $t>0$, 
		$\bE(\sigma)$ is the solution to (\ref{weakeddy}) and  $(\bF(\sigma),\psi(\sigma))$ is the  solution to (\ref{adjoint}). Then 
		$$G'(\sigma;\tilde{\sigma}) = \omega\int_{\Omega_C}\mathbf{Im}(\bE\cdot \bF)\tilde{\sigma}dx.$$
	\end{lem}
	\debproof
	For the sake of simplicity, we let $\hat\sigma =\sigma+t\tilde{\sigma}$. Let
	$(\bE(\sigma),\phi(\sigma))$ and  $(\bE(\hat\sigma),\phi(\hat\sigma))$ be the solutions to (\ref{weakeddy}) with respect to $\sigma$ and $\hat\sigma$. 
	Let $(\bF(\sigma),\psi(\sigma))$ and $(\bF(\hat\sigma),\psi(\hat\sigma))$ be the solution to adjoint state equation \eqref{adjoint} with respect to $\sigma$ and $\hat\sigma$. We denote the sesquilinear form \eqref{se-form} as $\hat{a}(\cdot,\cdot)$ with $\sigma$ replaced by $\hat\sigma$. Then we have
	$$
	\begin{aligned}
		G(\hat\sigma) - G(\sigma) =& \dfrac{1}{2}\mathbf{Re}\{\int_\Gamma(\bE(\hat\sigma) - \bE(\sigma))\cdot n\times\overline{(\bE(\hat\sigma) - \bE^{obs} +\bE(\sigma) - \bE^{obs})}\times n ds\}\\
		=-&\dfrac{1}{2}\mathbf{Re}\{\hat{a}(\bF(\hat\sigma),\overline{\bE(\hat\sigma) -\bE(\sigma)}) + b(\nabla\psi(\hat\sigma),\overline{\bE(\hat\sigma)-\bE(\sigma)}) + \\
		&\qquad\qquad a(\bF(\sigma),\overline{\bE(\hat\sigma) -\bE(\sigma)})+b(\nabla\psi(\sigma),\overline{\bE(\hat\sigma)-\bE(\sigma)})\}\\
		=-&\dfrac{1}{2}\mathbf{Re}\{\hat{a}(\bF(\hat\sigma),\overline{\bE(\hat\sigma) -\bE(\sigma)}) + a(\bF(\sigma),\overline{\bE(\hat\sigma) -\bE(\sigma)})\}\\
		=-&\dfrac{1}{2}\mathbf{Re}\{\hat{a}(\bE(\hat\sigma),\overline{\bF(\hat\sigma)})-\hat{a}(\bE(\sigma),\overline{\bF(\hat\sigma)})+a(\bE(\hat\sigma),\overline{\bF(\sigma)})-a(\bE(\sigma),\overline{\bF(\sigma)})\}.
	\end{aligned}
	$$
	By the definition of sesqulinear form (\ref{se-form}),
	$$
	\hat{a}(\bE,\bF) =a(\bE,\bF) - \int_{\Omega}i\omega(\hat\sigma - \sigma)\bE\cdot\overline{\bF} dx
	= a(\bE,\bF) - it\omega\int_{\Omega_C} \tilde{\sigma}\bE\cdot\overline{\bF}dx.
	$$
	Notice $b(\nabla \phi(\hat\sigma) ,\bF(\hat\sigma)) =b(\nabla \phi(\sigma) ,\bF(\hat\sigma)) =b(\nabla \phi(\hat\sigma) ,\bF(\sigma)) =b(\nabla \phi(\sigma) ,\bF(\sigma)) = 0$, then by the state equation \eqref{weakeddy}, 
	\begin{equation}
	\begin{aligned}
		&\hat{a}(\bE(\hat\sigma),\overline{\bF(\hat\sigma)}) = i\omega\int_{\Omega}\bJ_s \cdot \bF(\hat\sigma)dx.\\
		&\hat{a}(\bE(\sigma),\overline{\bF(\hat\sigma)}) =a(\bE(\sigma),\overline{\bF(\hat\sigma)}) - it\omega\int_{\Omega_C} \tilde\sigma\bE(\sigma)\cdot\bF(\hat\sigma)dx\\
		&\qquad\qquad=i\omega\int_{\Omega}\bJ_s\cdot\bF(\hat\sigma)dx- it\omega\int_{\Omega_C} \tilde\sigma\bE(\sigma)\cdot\bF(\hat\sigma)dx.\\
		&a(\bE(\hat\sigma),\overline{\bF(\sigma)}) = \hat{a}(\bE(\hat\sigma),\overline{\bF(\sigma)})+it\omega\int_{\Omega_C}\tilde\sigma\bE(\hat\sigma)\cdot\bF(\sigma)dx\\
		&\qquad\qquad=i\omega\int_{\Omega}\bJ_s\cdot\bF(\sigma)dx+it\omega\int_{\Omega_C}\tilde\sigma\bE(\hat\sigma)\cdot\bF(\sigma)dx.\\
		&a(\bE(\sigma),\overline{\bF(\sigma)}) = i\omega\int_{\Omega}\bJ_s\cdot \bF(\sigma)dx.
		\end{aligned}\nonumber
	\end{equation}
	Summing up the above equalities, we get
	$$G(\hat\sigma)-G(\sigma)=-\frac{1}{2}\mathbf{Re}\{it\omega \int_{\Omega_C}\tilde\sigma \bE(\sigma)\cdot \bF(\hat\sigma)+\tilde\sigma\bE(\hat\sigma)\cdot \bF(\sigma)dx\}.$$
	Then
	$$
	\dfrac{G(\hat\sigma) - G(\sigma)}{t} = \dfrac{1}{2}\omega\mathbf{Im}\{ \int_{\Omega_C}\tilde{\sigma} \bE(\sigma)\cdot\bF(\hat\sigma)dx + \int_{\Omega_C}\tilde{\sigma}\bE(\hat\sigma)\cdot\bF(\sigma)dx\}.
	$$
	By  $\tilde{\sigma} \in L^{\infty}(\Omega_C)$ and Cauchy inequality,
	$$
	\begin{aligned}
		|\int_{\Omega}\tilde{\sigma} (\bE(\hat\sigma)-\bE(\sigma))\bF(\sigma)dx |\leq \|\tilde\sigma\|_{L^{\infty}(\Omega_C)}\|\bE(\hat\sigma) - \bE(\sigma)\|_{L^2(\Omega_C)}\|\bF(\sigma)\|_{L^2(\Omega_C)}.
	\end{aligned}
	$$
	We can check that $(\bE(\hat\sigma)-\bE(\sigma),\phi(\hat\sigma)-\phi(\sigma))$ satisfies 
	$$
	\left\{\begin{aligned}
		a(\bE(\hat\sigma) - \bE(\sigma), \bF) + b(\nabla (\phi(\hat\sigma)-\phi(\sigma)), \overline{\bF}) dx&=it\omega\int_{\Omega_C}\tilde\sigma \bE(\hat\sigma)\cdot \overline{\bF}dx, \forall \bF \in H_\Gamma(\operatorname{curl};\Omega),\\
		b(\bE(\hat\sigma) - \bE(\sigma) ,\nabla \psi)&=0,  \forall \psi\in H^1_\Gamma(\Omega_0),
	\end{aligned}\right.
	$$
	then by the well-posedness of the above saddle point problem, we have 
	$$
	\|\bE(\hat\sigma) - \bE(\sigma)\|_{L^2(\Omega)}\leq Ct\omega\|\tilde{\sigma}\bE(\hat\sigma)\|_{L^2(\Omega)}\leq Ct\omega \|\tilde\sigma\|_{L^{\infty}(\Omega_C)}\|\bJ_s\|_{L^2(\Omega)}.
	$$
	So $\lim\limits_{t\searrow 0} \|\bE(\hat\sigma) - \bE(\sigma)\|_{L^2(\Omega)} =0 $ and as a conclusion we get
	$$
	\lim\limits_{t\searrow 0} \int_{\Omega}\tilde{\sigma} \bE(\hat\sigma)\cdot\bF(\sigma)dx  = \int_{\Omega}\tilde{\sigma} \bE(\sigma)\cdot\bF(\sigma)dx .
	$$
	Similarly we have 
	$$
	\lim\limits_{t\searrow 0} \int_{\Omega}\tilde{\sigma} \bE(\sigma)\cdot\bF(\hat\sigma)dx  = \int_{\Omega}\tilde{\sigma} \bE(\sigma)\cdot\bF(\sigma)dx .
	$$
	By taking $t\searrow 0$,  we complete the proof with the definition \eqref{gateaux}.
	%
	%  $$G'(\sigma;\tilde{\sigma}) =\lim\limits_{t\downarrow 0}\dfrac{G(\hat\sigma) - G(\sigma)}{t}= \omega\int_{\Omega_C}\mathbf{Im}(\bE\cdot \bF)\tilde{\sigma}dx.$$
	\finproof
	
	
	\subsection{The inverse eddy current problem}
	In this subsection, we will prove the existence and the stability of solution to the optimization problem (\ref{myproblem0}). We also give some important results that will be helpful in convergence analysis in the forthcoming sections. Then we will minimize the objective functional $\bp$ in \eqref{myproblem0} in the convex set $K$. Then we reach the following problem
	\begin{equation}\label{myproblem}
		\mathop{\min}_{\sigma \in K}
		\bp(\sigma)= G(\sigma)+\alpha |D\sigma|(\Omega_c) .
	\end{equation}
	\begin{thm}\label{Theorem2.2}
		There exists a minimizer $\sigma_\alpha$ to problem (\ref{myproblem}).
	\end{thm}
	\debproof
	Assume $\{\sigma_n\}\subset K$ such that
	$$\lim\limits_{n\to +\infty}\bp(\sigma_n)=\inf\limits_{\sigma\in K}\bp(\sigma).$$ 
	Then $\{|D\sigma_n|\}$ is bounded. Since the embedding $BV(\Omega)\to L^1(\Omega)$ is compact, there is a subsequence, still denoted by $\{\sigma_n\}$, converges to some $\sigma^*$ in $L^1(\Omega)$ and $\sigma^* \in BV(\Omega)$.
	
	Let $\bE(\sigma_n)$ and $\bE(\sigma^*)$ be the solution to (\ref{weakeddy}) with $\sigma$ replaced by $\sigma_n$ and $\sigma^*$. 
	Then  $\bE(\sigma_n) - \bE(\sigma^*)$ satisfies 
	$$
	\left\{\begin{aligned}
		\nabla\times\mu^{-1}\nabla\times (\bE(\sigma_n)-\bE(\sigma^*))-i\omega(\sigma+\sigma_n)(\bE(\sigma_n)-\bE(\sigma^*))&=i\omega(\sigma^*-\sigma_n)\bE(\sigma^*) &\mbox{ in }\Omega,\\
		\nabla\cdot \varepsilon (\bE(\sigma_n)-\bE(\sigma^*))&=0 &\mbox{ in }\Omega_0.
	\end{aligned}\right.
	$$
	By Theorem \ref{base}, we have
	\begin{equation}
		\|\bE(\sigma_n)-\bE(\sigma^*)\|_{L^2(\Omega)} \leq C \|(\sigma_n-\sigma^*)\bE(\sigma^*)\|_{L^2(\Omega)}.
	\end{equation}
	Applying the regularity results in \cite{regu} to $\bE(\sigma^*)$, we can conclude that $\bE(\sigma^*) \in H^{\delta}(\Omega_C)$ for some $\delta>0$ and
	$$\left\|\mathbf{E}\left(\sigma^*\right)\right\|_{H^\delta\left(\Omega_c\right)} \leq C\|\bJ_s\|_{L^2(\Omega)}.
	$$
	Hence $\bE(\sigma^*) \in L^p(\Omega_C)$ for $p=6/(3-2\delta)>2$ by the Sobolev embedding theorem and
	$$
	\left\|\mathbf{E}\left(\sigma^*\right)\right\|_{L^p\left(\Omega_c\right)} \leq C\left\|\mathbf{E}\left(\sigma^*\right)\right\|_{H^\delta\left(\Omega_c\right)}\leq C\|\bJ_s\|_{L^2(\Omega)}.
	$$
	Let $2/p+2/q=1$, then by Hölder inequality and Theorem \ref{base}, we have
	\begin{equation}
		\begin{aligned}
			\|(\sigma_n-\sigma^*)\bE(\sigma^*)\|_{L^2(\Omega)} &= \|(\sigma_n-\sigma^*)^2\bE^2(\sigma^*)\|_{L^1(\Omega_C)}^{1/2}\\
			&\leq (\|(\sigma_n-\sigma^*)^{2}\|_{L^{q/2}(\Omega_C)}\|\bE^2(\sigma^*)\|_{L^{p/2}(\Omega_C)})^{1/2}\\
			& =\|\sigma_n-\sigma^*\|_{L^q(\Omega_C)}\|\bE(\sigma^*)\|_{L^p(\Omega_C)}\\
			&\leq C((2M)^{q-1}\|\sigma_n-\sigma^*\|_{L^1(\Omega_C)})^{1/q}\|\bJ_s\|_{L^2(\Omega)}.
		\end{aligned}
	\end{equation}
	%the term $\|\sigma_n-\sigma^*\|_{L^2(\Omega_C)}^{1/q} \to 0$ as $n\to \infty$. 
	%Again by \cite{regu}, 
	%$$
	%\begin{aligned}
	%& \left\|\mathbf{E}\left(\sigma^*\right)\right\|_{H^\delta\left(\Omega_c\right)} \\
	%& \quad \leq C\left(\left\|\mathbf{E}\left(\sigma^*\right)\right\|_{H(\operatorname{curl}, \Omega)}^2+\left\|\nabla \cdot \varepsilon \mathbf{E}\left(\sigma^*\right)\right\|_{L^2\left(\Omega_0\right)}^2+\left\|\nabla \cdot\left(\sigma_0+\sigma^*\right) \mathbf{E}\left(\sigma^*\right)\right\|_{L^2\left(\Omega_C\right)}^2\right)^{1 / 2}.
	%\end{aligned}
	%$$
	%It is clear that
	%$\nabla \cdot \varepsilon \mathbf{E}\left(\sigma^*\right) = 0$ in $\Omega_0$ and  $\nabla \cdot\left(\sigma_0+\sigma^*\right) \mathbf{E}\left(\sigma^*\right) = 0$ in $\Omega_C$ since $\mathbf{J}_s$ is divergence free.  By the Sobolev embedding theory and Theorem \ref{base}, we have
	%$$
	%\left\|\mathbf{E}\left(\sigma_n\right)\right\|_{L^p\left(\Omega_c\right)^3} \leq C\left\|\mathbf{E}\left(\sigma_n\right)\right\|_{H^\delta\left(\Omega_c\right)} \leq C\left\|\mathbf{J}_s\right\|_{L^2(\Omega)^3} .
	%$$
	Then $\|(\sigma_n-\sigma^*)\bE(\sigma^*)\|_{L^2(\Omega_C)}$ tends to 0 as $n \to \infty$ and $\|\bE(\sigma_n)-\bE(\sigma^*)\|_{L^2(\Omega)}\rightarrow 0 $, too. By the continuity of $G(\cdot)$ and lower continuity of $|D\cdot|$
	$$\begin{aligned}
		\inf\limits_{\sigma\in K}\bp(\sigma)=\lim\limits_{n\to +\infty} \bp(\sigma_n)\geq \bp(\sigma^*) \geq \inf\limits_{\sigma\in K}\bp(\sigma)
	\end{aligned}.$$
	So $\sigma^*$ is a minimizer to $\Phi_\alpha$ and the proof is completed.
	\finproof
	
	\begin{thm}\label{sta}
		Let $\{\bE^k\}$ satisfy $\|\bE^k\times n-\bE^{obs}\times n\|_{L^2(\Gamma)}\to 0$ as $k \to +\infty$. Assume $\sigma_k$ is the minimizer to \eqref{myproblem} with $\bE^{obs}\times n$ replaced by $\bE^k\times n$ in the definition of $G$. Then there is subsequence of $\{\sigma_k\}$ which converges to a minimizer of $\Phi_\alpha(\sigma)$ strongly in $L^1(\Omega_C)$.
		%Then any limit point of $\{\sigma_n\}$ is  $\,{(\ref{myproblem})}$ .
	\end{thm}
	\debproof
	Since the embedding $BV(\Omega) \to L^1(\Omega)$ is compact and  $\{\sigma_k\}$ is bounded in  BV$(\Omega)$, then there is a subsequence, still denoted by $\{\sigma_k\}$, converges to some $\sigma_*$ in $L^1(\Omega_C)$ as $k \to \infty$ .  Let $\bE(\sigma_k)$ and $\bE(\sigma^*)$  be the solutions to (\ref{weakeddy}) with $\sigma$ replaced by $\sigma_k$ and $\sigma^*$.
	Let $\mathbf{\Phi}_\alpha^k$ be the objective functional with $\bE^{obs}\times n$ replaced by $\bE^k\times n$. Then 
	
	\begin{equation}\label{min}
		\mathbf{\Phi}_\alpha^k(\sigma)=\frac{1}{2}\|\bE(\sigma)\times n-\bE^k\times n\|_{L^2(\Gamma)}^2+\alpha |D\sigma|\geq \mathbf{\Phi}_\alpha^k(\sigma_k ) \quad \forall\, \sigma \in \, K.
	\end{equation}
	By the same arguments in the proof of Theorem  \ref{Theorem2.2} , we can conclude $\bE(\sigma_k) \to \bE(\sigma^*) $ in $L^2(\Omega)$. Take limit $k \to \infty$ in (\ref{min})
	$$
	\lim\limits_{k\to +\infty} \|\bE(\sigma)\times \bn -\bE^k\times \bn\|_{L^2(\Gamma)}^2 + \alpha |D\sigma| =\|\bE(\sigma)\times\bn -\bE^{obs}\times\bn\|_{L^2(\Gamma)}^2+\alpha |D\sigma|,
	$$
	and 
	$$
	\lim\limits_{k \to \infty} \dfrac{1}{2}\|\bE(\sigma_k)\times\bn - \bE^{obs}\times \bn\|_{L^2(\Gamma)}^2 + \alpha |D\sigma_k|\geq \dfrac{1}{2}\|\bE(\sigma^*)\times \bn -\bE^{obs}\times \bn\|_{L^2(\Gamma)}^2+\alpha |D\sigma^*|.
	$$
	Then we conclude that
	$$\mathbf{\Phi}_\alpha(\sigma)\geq \mathbf{\Phi}_\alpha(\sigma^*)\qquad \forall \, \sigma \in \,  K,$$
	which shows $\sigma^*$ is a minimizer to problem (\ref{myproblem}). 
	\finproof
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Discrete inverse Problem}\label{sect3}
	In this section, we discretize the inverse problem with finite element method and analyze the well-posedness of the discrete problem. 
	\subsection{Finite element discretization}
	The domain $\Omega$ is partitioned to a family of shape regular, quasi-uniform tetrahedral mesh $\mathcal{M}_h$ and the subscript $h$ denotes the mesh size.  Let $R_h$ be the lowest-order N\'ed\'elec edge element space approximation to $H_\Gamma(\operatorname{curl},\Omega)$. We assume that $\Omega_0$ and $\Omega_C$ are fitted by sub-meshes $\mathcal{M}_0$ and $\mathcal{M}_C$ of $\mathcal{M}_h$, respectively. So we can approximate $H_\Gamma^1(\Omega_0)$ and $H^1_0(\Omega_C)$ by linear Lagrange element on the corresponding sub-meshes and denote the discrete spaces by ${U}_h$ and $V_h$ respectively. 
	Let 
	$K_h=V_h\cap K$ and 
	$D_h$ be the finite element space of piece-wise constant functions on $\Omega_c$. For any $\sigma_h\in K_h$, $\bE_h,\bF_h\in R_h$, we define the discrete sesquilinear form
	$$a_h(\bE_h,\bF_h) = \int_{\Omega} \mu^{-1} \nabla \times \bE_h \cdot \nabla \times \overline{\bF_h}-i \omega(\sigma_h+\sigma_0) \bE_h \cdot \overline{\bF_h} d x.$$
	Then for given $\sigma_h \in K_h$, the discrete problem to \eqref{weakeddy} is: Find $\bE_h \in R_h, \phi_h \in U_h$, such that
	\begin{equation}\label{diseddy}
		\left\{\begin{aligned}
			a_h(\bE_h, \bF_h) + b(\nabla\phi_h,\overline{\bF_h})&=i \omega \int_{\Omega} \bJ_s \cdot \overline{\bF_h} d x \quad &&\forall \bF_h \in R_h,\\
			b(\bE_h,\nabla\psi_h)&=0 \quad  &&\forall \psi_h\in U_h.
		\end{aligned}\right.
	\end{equation}
	The discrete problem to \eqref{adjoint} is: 
	Find $\bF_h \in R_h, \psi_h \in U_h$, such that
	\begin{equation}\label{disadjoint}
		\left\{
		\begin{aligned}       a(\bF_h,\tilde{\bE}_h)+b(\nabla\psi_h,\overline{\tilde{\bE}_h}) =& \int_{\Gamma} \bn\times \overline{(\bE^{obs} - \bE_h)}\times \bn \cdot \overline{\tilde{\bE}_h}ds \qquad &&\forall \tilde{\bE}_h \in R_h,\\
			b(\bF_h,\nabla\tilde{\phi}_h)=&0 &&\forall \tilde{\phi}_h \in U_h.
		\end{aligned}
		\right.
	\end{equation}
	Then, for any $\sigma_h\in K_h$, we define the discrete objective functional $G_h:K_h\to R$ as
	\begin{equation}
		G_h(\sigma_h) = \dfrac{1}{2}\|\bE_h\times n -\bE^{obs}\times n\|_{L^2(\Gamma)}^2, \label{disG}
	\end{equation}
	where $\bE_h$ is the solution to (\ref{diseddy}) and $\bE^{obs}\times n$ is the data on $\Gamma$. 
	%We can formulate discrete forms of inequalities in Lemma \ref{Lemma3} and\ref{Lemma4}.
	
	For any element $\mathbf{e}$ in mesh $\mathcal{M}_h$, the number of basis on $\mathbf{e}$ is finite. Hence $\|\cdot\|_{L^p(\mathbf{e})}$ is equivalent to $\|\cdot\|_{L^2(\mathbf{e})}$, i.e. $\|\sigma\|_{L^2(\mathbf{e})}\lesssim\|\sigma\|_{L^p(\mathbf{e})}\lesssim\|\sigma\|_{L^2(\mathbf{e})}$. 
	We list some results from \cite{FEM} here.
	%The next lemma shows that norm $\|\cdot\|_{L^p(\Omega_C)}(2\leq p\leq \infty)$
	% is equivalent to $\|\cdot\|_{L^2(\Omega_C)}$ in the finite element spaces. 
	\begin{lem}\label{p2}
		Assume that the mesh $\mathcal{M}_h$ is shape regular and quasi-uniform. For any $\sigma_h\in V_h$, we have 
		\begin{eqnarray}
			%      	\|\sigma_h\|_{L^p(\Omega_C)}\leq C_{p,2} h^{3/p-3/2} \|\sigma_h\|_{L^2(\Omega_C)}, \forall 2\leq p\leq \infty,\label{scalp2}\\
			\|\sigma_h\|_{L^\infty(\Omega_C)}\leq C_{\infty} h^{-3/2} \|\sigma_h\|_{L^2(\Omega_C)}\\
			\|\sigma_h\|_{L^2(\Omega_C)}\leq C_s h\|\nabla\sigma_h\|_{L^2(\Omega_C)}.\label{scall2}
		\end{eqnarray}
		Where $C_{\infty}$ and $C_s$ are only depended on the shape regularity of mesh $\mathcal{M}_h$ .
	\end{lem}
	\debproof
	The result can be proved with scaling argument \cite{ciarlet}, we omit the details here. 
	%$Let $\hat{\mathbf{e}}$ be the reference tetrahedron, 
	% For a  tetrahedron $\mathbf{e}$, let $F:\hat{x}\in R^3\rightarrow B\hat{x}+b \in R^3$ be the affine transform from $\hat{\mathbf{e}}$ to $\mathbf{e}$, here $b\in R^3$ and $B\in R^{3\times 3}$ is invertible. 
	%Let $V_0$ and $V$ be the spaces of linear functions on $\hat{\mathbf{e}}$ and on $\mathbf{e}$. Since the dimension of $V_0$ is finite, there is a constant $\hat{C}_{p,2}$ only depended on $\hat{\mathbf{e}}$ such that
	% $$\|\hat\sigma\|_{L^p(\hat{\mathbf{e}})} \leq \hat{C}_{p,2}\|\hat\sigma\|_{L^2(\hat{\mathbf{e}})} \qquad \forall \hat\sigma\in V_0$$
	%For any $\sigma_h \in V_h$ supported on $\mathbf{e}$, define $\hat{\sigma}_h(\hat{x})=\sigma_h(B\hat{x}+b),\hat{x}\in \hat{\mathbf{e}}$. Then integrate by substitution, we have
	%$$\|\sigma_h\|_{L^p(\mathbf{e})} = |\det(B)|^{1/p}\|\hat{\sigma}_h\|_{L^p(\hat{\mathbf{e}})} \leq \hat{C}_{p,2}|\det(B)|^{1/p} \|\hat{\sigma}_h\|_{L^2(\hat{\mathbf{e}})}=\hat{C}_{p,2}|\det(B)|^{1/p-1/2}\|\sigma_h\|_{L^2(\mathbf{e})}.$$
	%The coefficient %$\hat{C}_{p,2}|\det(B)|^{1/p-1/2}$ can by bounded by $C_{p,2}h^{3/p-3/2}$, provided $\mathcal{M}_h$ is shape regular and quasi-uniform, for some constant $C_{p,2}$. In the case  $2<p<+\infty$, for any $\sigma_h \in K_h$
	%$$
	%\|\sigma_h\|_{L^p({\Omega_C})}=(\sum\limits_{\mathbf{e}\in \mathcal{M}}\|\sigma_h\|_{L^p(\mathbf{e})}^p)^{\frac{1}{p}}\leq C_{p,2}h^{3/p-3/2}(\sum\limits_{\mathbf{e}\in \mathcal{M}}\|\sigma_h\|_{L^2(\mathbf{e})}^p)^{\frac{1}{p}}\leq C_{p,2}h^{3/p-3/2}\|\sigma_h\|_{L^2({\Omega_C})},
	%$$
	%the second  "$\leq$"  follows from the truth that $x_1^{p/2}+x_2^{p/2}+\cdots+x_N^{p/2} \leq (x_1+x_2+\cdots+x_N)^{p/2}$ for positive numbers $x_1,x_2,\cdots,x_n$ and $2<p<+\infty$.
	
	%For $p=+\infty$, 
	%\begin{eqnarray*}\|\sigma_h\|_{L^\infty(\Omega_C)}=\max\limits_{\mathbf{e}\in\mathcal{M}}\|\sigma_h\|_{L^\infty(\mathbf{e})}\leq \max\limits_{\mathbf{e}\in\mathcal{M}}C_{p,2}h^{-3/2}\|\sigma_h\|_{L^2(\mathbf{e})} \leq C_{p,2}h^{-3/2}\|\sigma_h\|_{L^2(\Omega_C)}.
	%\end{eqnarray*}
	%This completes the proof of \eqref{scalp2} by regarding $3/\infty$ as 0.
	
	%With the same arguments, it is easy to prove \eqref{scall2}, here we omit the details.
	\finproof
	
	Similar with the continuous case, we define the G\^ateaux derivative for the discrete objective functional $G_h(\sigma_h)$ as 
	\begin{equation}
		(G'_h(\sigma_h),\tilde{\sigma}_h)=\lim_{t\searrow 0}(G_h(\sigma_h+t\tilde{\sigma}_h)-G_h(\sigma_h))/t.\label{disGh}
	\end{equation}
	for $\sigma_h,\sigma_h+t\tilde{\sigma}_h\in K_h$. Here and in what follows, $(\cdot, \cdot)$ represents the $L^2$ inner product on $\Omega_C$. Actually, $G'(\sigma_h)$ define the gradient of $G_h$ at $\sigma_h$. 
	\begin{lem}\label{disin}
		Assume $\sigma_h^1,\sigma_h^2\in K_h$, $\tilde{\sigma}_h \in V_h$ such that $\sigma^1_h+t\tilde{\sigma}_h, \sigma^2_h+t\tilde{\sigma}_h \in K_h$ for some $t>0$, 
		there is a constant $C_{eddy}$ which only depends on $\Omega,\bJ_s$ and $\bE^{obs}$, such that
		\begin{eqnarray}
			(G_h'(\sigma_h^1)-G_h'(\sigma_h^2),\tilde{\sigma}_h)\leq C_{eddy}\|\sigma_h^1-\sigma_h^2\|_{L^{\infty}(\Omega_C)}\|\tilde{\sigma}_h\|_{L^\infty(\Omega_C)},\label{disieqK1}\\
			G_h(\sigma_h^1)-G_h(\sigma_h^2)-(G_h'(\sigma_h^2),\sigma_h^1-\sigma_h^2)\leq C_{eddy}\|\sigma_h^1-\sigma_h^2\|_{L^\infty(\Omega_C)}^2.\label{disieqK2}
		\end{eqnarray}
	\end{lem}
	\debproof
	Let $(\bE_h(\sigma_h^i),\phi_h(\sigma_h^i)$  be the solutions to (\ref{diseddy}) with $\sigma_h$ replaced by $\sigma_h^i\in K_h$, $i=1,2$ and  $(\bF_h(\sigma_h^i),\psi_h(\sigma_h^i)$ be the solution to the corresponding adjoint station equation \eqref{disadjoint}.  
	Clearly, the estimate
	\begin{equation}
		\|\bE_h(\sigma^i_h)\|_{H(\operatorname{curl},\Omega)}\leq C\|\bJ_s\|_{L^2(\Omega)}.\label{Ebound}
	\end{equation}
	is true and the solution to \eqref{disadjoint} has the estimate
	\begin{equation}
		\|\bF_h(\sigma^i_h)\|_{H(\operatorname{curl},\Omega)}\leq C\|n\times \overline{(\bE^{obs}-\bE_h)}\times n\|_{L^2(\Gamma)} .
	\end{equation}
	%Taking $\tilde{\bE}_h = \nabla\phi_h$ with $\phi_h \in U_h$ in (\ref{disF}), we can get
	%$$\int_{\Omega_0}\varepsilon\nabla\psi_h\cdot \nabla\phi_h = \int_{\Gamma}\left({n} \times \overline{\left(\bE^{o b s}-\bE_h\right)} \times {n}\right) \cdot \nabla\phi_h d s,$$
	%hence we can conclude that  
	%\begin{equation}\label{phibound}
	%    \|\nabla \psi_h\|_{L^2(\Omega_0)}\leq C\|n\times \overline{(\bE^{obs}-\bE_h)}\times n\|_{L^2(\Gamma)}.
	%\end{equation}
	Then by trace theorem, 
	\begin{equation}
		\|\bF_h(\sigma^i_h)\| \leq C(\|\bE_h\|_{H(\operatorname{curl},\Omega)} + \|\bE^{obs}\|_{L^2(\Gamma)})\leq C(\|\bJ_s\|_2+\|\bE^{obs}\|_{L^2(\Gamma)}).\label{Fbound}
	\end{equation}
	By the same argument in Lemma \ref{derivative}, we have
	$$(G_h'(\sigma_h^i),\tilde{\sigma}_h) = \omega\int_{\Omega_C}\mathbf{Im}(\bE_h(\sigma_h^i)\cdot \bF_h(\sigma_h^i))\tilde{\sigma}_hdx.$$
	Then
	\begin{equation}\label{disLip}
		\begin{aligned}
			(G_h'(\sigma_h^1)-G_h'(\sigma_h^2),\tilde{\sigma}_h)
			=&\omega\int_{\Omega_C}\mathbf{Im}(\bE_h(\sigma_h^1)\bF_h(\sigma_h^1)-\bE_h(\sigma_h^2)\bF_h(\sigma_h^2))\tilde{\sigma}_hdx\\
			\leq& \omega |\int_{\Omega_C}(\bE_h(\sigma_h^1)\bF_h(\sigma_h^1)-\bE_h(\sigma_h^2)\bF(\sigma_h^1))\tilde{\sigma}_hdx|+\\
			&\omega |\int_{\Omega_C}(\bE_h(\sigma_h^2)\bF_h(\sigma_h^1)-\bE_h(\sigma_h^2)\bF_h(\sigma_h^2))\tilde{\sigma}_hdx|.
		\end{aligned}    
	\end{equation} 
	By the H\"older inequality, we have
	$$
	|\int_{\Omega_C}(\bE_h(\sigma_h^1)\bF_h(\sigma_h^1)-\bE_h(\sigma_h^2)\bF_h(\sigma_h^1))\tilde{\sigma}_hdx|\leq \|\bF_h(\sigma_h^1)\tilde{\sigma}_h\|_{L^2(\Omega_C)}\|\bE_h(\sigma_h^1)-\bE_h(\sigma_h^2)\|_{L^2(\Omega_C)}
	$$
	Define $a_h^1$ as
	$$a_h^1\left(\mathbf{E}_h, \mathbf{F}_h\right)=\int_{\Omega} \mu^{-1} \nabla \times \mathbf{E}_h \cdot \nabla \times \overline{\mathbf{F}_h}-i \omega\left(\sigma_h^1+\sigma_0\right) \mathbf{E}_h \cdot \overline{\mathbf{F}_h} d x \qquad \forall \bE_h,\bF_h\in R_h.$$
	Then  $(\bE_h(\sigma_1) - \bE_h(\sigma_2))$ and $(\phi_h(\sigma_1)-\phi_h(\sigma_2))$ satisfy 
	$$
	\left\{\begin{aligned}
		&a_h^1(\bE_h(\sigma_h^1)-\bE_h(\sigma_h^2),\bF_h)+\int_{\Omega_0}\nabla (\phi_h(\sigma_h^1)-\phi_h(\sigma_h^2))\bF_hdx\\
		&\quad\quad=i\omega\int_{\Omega_C}(\sigma_h^1-\sigma_h^2)\bE_h(\sigma_h^2)\cdot\overline{\bF_h}, \quad \forall \bF_h\in R_h,\\
		&\int_{\Omega_0}(\bE_h(\sigma_h^1)-\bE_h(\sigma_h^2))\nabla\psi_hdx=0, \quad \forall \psi_h\in U_h.
	\end{aligned}\right.
	$$
	Hence by the well-posedness of the above saddle point problem, we can get 
	$$
	\|\bE_h(\sigma_h^1)-\bE_h(\sigma_h^2)\|_{L^2(\Omega_C)}\leq C\|(\sigma_h^1-\sigma_h^2)\bE_h(\sigma_h^2)\|_{L^2(\Omega_C)}.
	$$
	Then with \eqref{Ebound} and \eqref{Fbound}, we have 
	\begin{equation}
	\begin{aligned}
		&	|\int_{\Omega_C}(\bE_h(\sigma_h^1)\bF_h(\sigma_h^1)-\bE_h(\sigma_h^2)\bF_h(\sigma_h^1))\tilde{\sigma}_hdx| \\
		& \qquad\leq \|\bF_h(\sigma_h^1)\tilde{\sigma}_h\|_{L^2(\Omega_C)}\|\bE_h(\sigma_h^1)(\sigma_h^1-\sigma_h^2)\|_{L^2(\Omega_C)}\\		
		&\qquad \leq \|\bF_h(\sigma_h^1)\|_{L^2(\Omega_C)}\|\tilde{\sigma}_h\|_{L^\infty(\Omega_C)} \|\bE_h(\sigma_h^1)\|_{L^2(\Omega_C)}\|\sigma_h^1-\sigma_h^2\|_{L^\infty(\Omega_C)}\\
		&\qquad	
		\leq C\|\mathbf{J}_s\|_{L^2(\Omega_C)}(\|\mathbf{J}_s\|_{L^2(\Omega_C)} +\|\bE^{obs}\|_{L^2(\Gamma)})\|\tilde{\sigma}_h\|_{L^\infty(\Omega_C)}\|\sigma_h^1-\sigma_h^2\|_{L^\infty(\Omega_C)}.
		\end{aligned}\nonumber
	\end{equation}
	Similarly, for the second term, we can get  
	$$
	\begin{aligned}
		|\int_{\Omega_C}&(\bE(\sigma_h^2)\bF(\sigma_h^1)-\bE(\sigma_h^2)\bF(\sigma_h^2))\tilde{\sigma}_hdx|\leq\\ &C\|\mathbf{J}_s\|_{L^2(\Omega_C)}(\|\mathbf{J}_s\|_{L^2(\Omega_C)} +\|\bE^{obs}\|_{L^2(\Gamma)})\|\tilde{\sigma}_h\|_{L^\infty(\Omega_C)}\|\sigma_h^1-\sigma_h^2\|_{L^\infty(\Omega_C)}.
	\end{aligned}
	$$
	This completes the proof of inequality \eqref{disieqK1} by letting $C_{eddy}=C\|\mathbf{J}_s\|_{L^2(\Omega_C)}(\|\mathbf{J}_s\|_{L^2(\Omega_C)} +\|\bE^{obs}\|_{L^2(\Gamma)})$.
	
	Since $\sigma_h^1,\sigma^2_h\in K_h$ and $K_h$ is convex set, we know that  $\sigma^2_h+t(\sigma^1_h-\sigma^2_h)\in K_h$ for $t\in [0,1]$. With the differentiability of $G(\sigma)$, we know that there is some $t\in (0,1)$, such that
	$$\begin{aligned}
		G_h(\sigma_h^1)-G_h(\sigma_h^2)-(G_h'(\sigma_h^2),\sigma_h^1-\sigma_h^2)=(G_h'(t\sigma_h^1+(1-t)\sigma_h^2),\sigma_h^1-\sigma_h^2)-(G_h'(\sigma_h^2),\sigma_h^1-\sigma_h^2).
	\end{aligned}$$
	Then by \eqref{disieqK1}, 
	\begin{eqnarray*}
		&&G_h(\sigma_h^1)-G_h(\sigma_h^2)-(G_h'(\sigma_h^2),\sigma_h^1-\sigma_h^2)
		\leq C_{eddy}\|t\sigma_h^1-t\sigma_h^2\|_{L^\infty(\Omega_C)}\|\sigma_h^1-\sigma_h^2\|_{L^\infty(\Omega_C)}\\
		&&\qquad \leq C_{eddy}\|\sigma_h^1-\sigma_h^2\|_{L^\infty(\Omega_C)}^2.
	\end{eqnarray*}
	This completes the estimate \eqref{disieqK2}.
	\finproof
	\subsection{Well-posedness of discrete inverse problem}
	With the finite element discretization, the discrete inverse problem can be recast to the following  optimization problem
	\begin{equation}\label{dispro}
		\mathop{\min}_{\sigma_h \in K_h}
		\phi_\alpha(\sigma_h)= G_h(\sigma_h)+\alpha |D\sigma_h|(\Omega_C). 
	\end{equation}
	The well-posedness of the above problem is given in the following two theorems.
	
	\begin{thm}[existence]\label{disex}
		There exists at least one minimizer to the discrete optimization
		problem (\ref{dispro}).
	\end{thm}
	\debproof
	We know $\phi_{\alpha}(\sigma_h)$ is bounded below, porper and lower continuous. Then $\phi_{\alpha}$ has a minimizer  in the closed set $K_h$.
	\finproof
	
	
	
	\begin{thm}[stability]
		Let $\{\bE^k\}$ satisfies $\|\bE^k\times n-\bE^{obs}\times n\|_{L^2(\Gamma)}\to 0$ as $k \to +\infty$. Assume $\sigma_k$ is the minimizer in Theorem \ref{disex}, but with $\bE^{obs}\times n$ replaced by $\bE^k\times n$. Then any limit point of $\{\sigma_n\}$ solves \eqref{dispro}.
	\end{thm}
	\debproof
	The proof is same as the proof in (\ref{sta}), just notice that $K_h$ is a close set and any limit point of $\{\sigma_h^k\}$ is still in $K_h$.
	\finproof
	
	
	%According to the proof of Theorem \ref{Theorem2.2}, we have
	%\begin{eqnarray*}
	% \lim\limits_{n\to \infty} \phi_{\alpha}(\rho_n)= \phi_{\alpha}(\sigma^*).   
	%\end{eqnarray*}
	% By lower semi-continuity of $\phi_\alpha$ and the minimizing property of $\sigma_{h_n}$, we know that
	% \begin{eqnarray*}
		%     \phi(\hat\sigma)\leq \lim\inf \limits_{n\to \infty} \phi_{\alpha}(\sigma_{h_n}) \qquad \text{and}\qquad \phi_{\alpha}(\sigma_{h_n})\leq \phi_{\alpha}(\rho_n).
		% \end{eqnarray*}
	%Then by Theorem \ref{Theorem2.2}, there is a sequence $\{\rho_n\}\subset K$ such that $\rho_n \to \sigma^*$ in $L^1(\Omega_C)$ and $|D\rho_n|\to |D\sigma^*|$\cite{GJL}. Now Assume that $\rho_{h_n}$ is the linear finite element approximation to $\rho_n$ on $\mathcal{M}_h^n$. Then clearly
	%$$\lim_{n\rightarrow \infty} \rho_{h_n}=\sigma^*.$$
	%By Theorem \ref{Theorem2.2} and the finite element approximation, 
	%there is a sequence $\{\rho_n\}$ such that $\rho_n\in V_{h_n}$, $\rho_n \to \sigma^*$ in $L^1(\Omega_C)$ and $|D\rho_n|\to |D\sigma^*|$\cite{GJL}. Then by the proof of Theorem \ref{Theorem2.2} and the minimality of $\sigma_{h_n}$, we get 
	%$$ \lim\limits_{n\to \infty} \phi_{\alpha}(\rho_n)= \phi_{\alpha}(\sigma^*)\qquad \lim\limits_{n\to \infty} \phi_{\alpha}(\sigma_{h_n})\geq \phi_{\alpha}(\hat\sigma) \qquad \text{and}\qquad \phi_{\alpha}(\sigma_{h_n})\leq \phi_{\alpha}(\rho_n). $$
	%By taking $n \to \infty$ in the last inequality, we have 
	%$$\phi_{\alpha}(\hat\sigma) \leq \phi_{\alpha}(\sigma^*).$$
	%Since 
	%$$
	%\phi_{\alpha}(\sigma^*)=\inf\limits_{\sigma \in K}\phi_{\alpha}(\sigma) \leqslant \phi_{\alpha}(\hat\sigma),
	%$$
	%we conclude that 
	%$$\phi_{\alpha}(\hat\sigma) = \phi_{\alpha}(\sigma^*).$$
	%This completes the proof.
	%\finproof
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{The ADMM algorithms and convergence analysis}\label{sect4}
	In the section, we will propose ADMM algorithms to solve problem \eqref{dispro} and analyze the convergence. 
	In order to deal with the non-smooth term in $\phi_\alpha(\sigma_h)$, 
	since $|D\sigma|(\Omega_C) = \|\nabla \sigma\|_{L^1(\Omega_C)}$ for $\sigma \in K_h$ since $K_h\subset V_h\subset H^1_0(\Omega_C)\subset W^{1,1}(\Omega_C)$, we recast the problem \eqref{dispro} to the following optimization problem
	\begin{equation}\label{dispro1}
		\begin{aligned}
			\mathop{\arg\min}_{(\sigma_h,d_h) \in K_h\times (D_h)^3}
			\phi_{\alpha,h}(\sigma_h)=& G_h(\sigma_h)+\alpha \|d_h\|_{L^1(\Omega_c)},\\
			\text{subject to } d_h =& \nabla \sigma_h \text{ in weak sense.}
		\end{aligned}
	\end{equation} 
	To relax the constrain in \eqref{dispro1}, we introduce the augmented Lagrangian $L_{\alpha,\beta}^h: K_h\times (D_h)^3\times(D_h)^3\to \mathbb{R}$ as
	\begin{equation}\label{lagran}
		L_{\alpha,\beta}^h(\sigma_h,d_h,y_h)=G_h(\sigma_h)+(y_h,d_h-\nabla\sigma_h)+\frac{\beta}{2} \|d_h-\nabla\sigma_h\|^2_{L^2(\Omega_C)}.
	\end{equation} 
	Where $\alpha > 0 $ is the regularization parameter and $\beta > 0$ is a penalty parameter.  Here we remark that if $\sigma$ is approximated with piecewise constant,  $|D\sigma|(\Omega_C) = \|\nabla \sigma\|_{L^1(\Omega_C)}$ is invalid and we can not deal with the total variation regularization term as in \eqref{dispro1}. In that situation, the regularization term should be treated in a different way \cite{chantai}.
	
	Based on the Lagrangian \eqref{lagran}, we introduce a modified ADMM Algorithm \ref{cADMM} to solve problem \eqref{dispro1}.  In each step of the algorithm, we need to solve an optimization sub-problem. We can solve the first sub-problem by NLCG method \cite{CLZ}.  The second sub-problem  has a close form solution. For the last sub-problem, we only need to solve a Laplace equation which can be solved efficiently.  
	\begin{algorithm}[htbp]
		\caption{Modified ADMM}\label{cADMM}
		\hspace*{0.02in} {\bf Input:} initial values  $\sigma^0,d^0,y^0$\\
		\hspace*{0.02in} {\bf Output:} $\sigma_N$
		\begin{algorithmic}
			\For{$k=0,1,...N-1$} 
			\State 		\begin{equation}\label{sigmaiter}
				\sigma^{k+1}=\arg \min\limits_{\sigma\in K_h} L^h_{\alpha,\beta}(\sigma,d^{k},y^k),
			\end{equation}
			\begin{equation}
				d^{k+1}=\arg\min\limits_{d\in (D_h)^3} L^h_{\alpha,\beta}(\sigma^{k+1},d,y^k),
			\end{equation}
			\begin{equation}
				y^{k+1}=\nabla (\Delta)^{-1} \nabla\cdot (y^k+\beta(d^{k+1}-\nabla\sigma^{k+1})).\label{y}
			\end{equation}
			%		\begin{equation}\label{y}
				%			y^{k+1}=\nabla h^{k+1}
				%		\end{equation}
			%		here $h^{k+1}\in K_h$ and
			%		\begin{equation}
				%			\Delta h^{k+1}=\nabla \cdot (y^k+\beta (d^{k+1}-\nabla\sigma^{k+1}))
				%		\end{equation}
			%	in weak sense.
			\EndFor
		\end{algorithmic}
	\end{algorithm}
	%\begin{rem} 
	%We assume that in each iteration, $\sigma^{k}$ is in $ K$ for some $M>m>0$. This is acceptable because the background conductivity is a positive constant $\sigma_0\geq 0$ and the real $\sigma > 0$ in the object. At the same time, $\sigma^{k}$ is always bounded in the iteration.
	%\end{rem}
	It is worth mentioning that the sub-iteration (\ref{y}) is different from the traditional one
	\begin{equation}
		y^{k+1} = y^k+\beta (d^{k+1}-\nabla\sigma^{k+1}).\label{trADMM}
	\end{equation}
	In the present work, the iteration \eqref{trADMM} will bring difficulty in the proof of convergence, so we modify it to \eqref{y}. With this modification, we only keep the gradient part of the traditional  $y^{k+1}$ in \eqref{trADMM}. That's why we call Algorithm \ref{cADMM} the modified ADMM algorithm. 
	In the implementation of the last sub-problem \eqref{y}, we employ the following two-step procedure
	\begin{eqnarray}\label{twostep}
		&&\Delta z^{k+1}=\nabla \cdot (y^k+\beta (d^{k+1}-\nabla\sigma^{k+1})),\label{dish}\\
		&&y^{k+1}=\nabla z^{k+1}.\nonumber
	\end{eqnarray}
	Here the differential operators is in weak sense and the equation \eqref{dish} is solved in $V_h$.
	
	The optimality condition to \eqref{sigmaiter} is
	\begin{eqnarray*}
		(\frac{\partial L^h_{\alpha,\beta}}{\partial\sigma}(\sigma^{k+1},d^k,y^k),\tilde\sigma-\sigma^{k+1})\geq 0, \forall \tilde{\sigma}\in K_h.
	\end{eqnarray*}
	which is
	\begin{equation}\label{sigmamin}
		(G_h'(\sigma^{k+1}),\tilde{\sigma}-\sigma^{k+1})-(y^k,\nabla(\tilde{\sigma}-\sigma^{k+1}))-(\beta(d^{k}-\nabla\sigma^{k+1}),\nabla(\tilde{\sigma}-\sigma^{k+1}))\geq 0\quad \forall \tilde{\sigma} \in K_h.
	\end{equation}
	With the definition of $z^{k+1}$, we have
	\begin{equation}\label{h}
		(G_h'(\sigma^{k+1}),\tilde{\sigma}-\sigma^{k+1})\geq (\nabla z^{k+1},\nabla(\tilde{\sigma}-\sigma^{k+1}))\quad \forall \tilde{\sigma} \in K_h.
	\end{equation}
	This makes the sub-iteration $(\ref{y})$ reasonable. However the inequality \eqref{h} brings trouble when proving the convergence. 
	%The test function $\widetilde{\sigma}$ is restricted in $K_h$. 
	
	In the situation that the minimizers are located in the interiori of $K_h$, since $G_h(\sigma)$ is differentiable in the interior of $K_h$, the optimal conditional \eqref{h} becomes equality and $\tilde\sigma-\sigma^{k+1}$ can be any element in $V_h$, we can then extend the admissible set to $V_h$.  Besides, if we can not get enough a priori knowledge about $\sigma$, such as the upper bound $M$ and lower bound $m$, it is better to extend the admissible set $K_h$ to $V_h$ and the inequality optimal condition \eqref{h} becomes equality, too.  The equality optimal condition gives us the possibility to propose a convergent algorithm.
	%In practice, sometime we can not get enough a priori knowledge about $\sigma$, such as the upper bound $M$ and lower bound $m$. In this situation, it is better to release the admissible set $K_h$ to $V_h$. If we conduct this release, the inequality optimal condition \eqref{h} becomes equality and this change gives the possibility to propose a convergent algorithm.
	
	Notice that Lemma \ref{p2} holds for all $\sigma_h \in V_h$. $G_h$ can be defined in $V_h$ by the same way. Moreover in the proof of Lemma \ref{disin}, we actually don't use the property $m \leq \sigma_h\leq M$, which means Lemma \ref{disin} holds for $\sigma_h \in V_h$.
	
	\begin{lem}\label{disinVh}
		Assume $\sigma_h^1,\sigma_h^2,\tilde{\sigma}_h \in V_h$, 
		there is a constant $C_{eddy}$ which only depends on $\Omega,\bJ_s$, and $\bE^{obs}$ such that
		\begin{eqnarray}
			(G_h'(\sigma_h^1)-G_h'(\sigma_h^2),\tilde{\sigma}_h)\leq C_{eddy}\|\sigma_h^1-\sigma_h^2\|_{L^{\infty}(\Omega_C)}\|\tilde{\sigma}_h\|_{L^\infty(\Omega_C)},\label{disieq}\\
			G_h(\sigma_h^1)-G_h(\sigma_h^2)-(G_h'(\sigma_h^2),\sigma_h^1-\sigma_h^2)\leq C_{eddy}\|\sigma_h^1-\sigma_h^2\|_{L^\infty(\Omega_C)}^2.\label{disieq2}
		\end{eqnarray}
	\end{lem}
	
	Then we consider the optimization problem over $V_h$,
	\begin{equation}\label{disproVh}
		\mathop{\min}_{\sigma_h \in V_h}
		\phi_\alpha(\sigma_h)= G_h(\sigma_h)+\alpha |D\sigma_h|(\Omega_C),
	\end{equation}
	Where $G_h(\sigma_h)$ is the same with problem \eqref{dispro}.
	
	In the following two theorems,  we consider the well-posedness of the new optimization problem. 
	\begin{thm}[existence]\label{disexVh}
		There exists at least one minimizer to the discrete optimization problem \eqref{disproVh}
	\end{thm}
	\debproof
	Assume $\{\sigma_h^k\}\subset V_h$ such that
	$$
	\lim\limits_{k\to +\infty} \phi_{\alpha}(\sigma_h^k) = \inf\limits_{\sigma_h \in V_h}\phi_{\alpha}(\sigma_h).
	$$
	Then $\{|D\sigma_h^k|\}$ is bounded and then there is a subsequence, still denoted as $\{\sigma_h^k\}$, converges to some $\sigma_h^*$ in $L^1(\Omega_C)$. The linear space $V_h$ is closed in $L^1(\Omega_C)$, so $\sigma_h^*$ is still in $V_h$.
	
	Let $(\bE_h(\sigma_h^k),\phi_h(\sigma_h^k))$ and $(\bE_h(\sigma_h^*),\phi_h(\sigma_h^*))$ be the solutions to (\ref{diseddy}) with $\sigma_h$ replaced by $\sigma_h^k$ and $\sigma_h^*$ respectively. 
	Define $a_h^k$ as
	$$a_h^k\left(\mathbf{E}_h, \mathbf{F}_h\right)=\int_{\Omega} \mu^{-1} \nabla \times \mathbf{E}_h \cdot \nabla \times \overline{\mathbf{F}_h}-i \omega\left(\sigma_h^k+\sigma_0\right) \mathbf{E}_h \cdot \overline{\mathbf{F}_h} d x \qquad \forall \bE_h,\bF_h\in R_h.$$
	Then  $(\bE_h(\sigma_h^k) - \bE_h(\sigma_h^*),\phi_h(\sigma_h^k)-\phi_h(\sigma_h^*))$ satisfies 
	$$
	\left\{\begin{aligned}
		&a_h^k(\bE_h(\sigma_h^k)-\bE_h(\sigma_h^*),\bF_h)+\int_{\Omega_0}\nabla (\phi_h(\sigma_h^k)-\phi_h(\sigma_h^*))\bF_hdx\\
		&\quad\quad =i\omega\int_{\Omega_C}(\sigma_h^k-\sigma_h^*)\bE_h(\sigma_h^*)\cdot\overline{\bF_h}, \quad \forall \bF_h\in R_h,\\
		& \int_{\Omega_0}(\bE_h(\sigma_h^k)-\bE_h(\sigma_h^*))\nabla\psi_hdx=0, \quad \forall \psi_h\in U_h.
	\end{aligned}\right.
	$$
	With the help of Lemma \ref{disinVh},
	\begin{equation}
		\begin{aligned}
			\|\bE_h(\sigma_h^k)-\bE_h(\sigma_h^*)\|_{L^2(\Omega)} \leq& C \|(\sigma_h^k-\sigma_h^*)\bE_h(\sigma_h^*)\|_{L^2(\Omega_C)}\\
			\leq& C\|\sigma_h^k-\sigma_h^*\|_{L^{\infty}(\Omega_C)}\|\bE_h(\sigma_h^*)\|_{L^2(\Omega)}\\
			\leq& C \|\sigma_k^h-\sigma_h^*\|_{L^1(\Omega_C)}\|\bJ_s\|_{L^2(\Omega)}.
		\end{aligned}
	\end{equation}
	Then $\|(\sigma_h^k-\sigma_h^*)\bE(\sigma^*)\|_{L^2(\Omega_C)}$ tends to 0 as $k \to \infty$ and $\|\bE(\sigma_h^k)-\bE(\sigma_h^*)\|_{L^2(\Omega)}\rightarrow 0 $, too. By the continuity of $\|\cdot\|_{L^2(\Gamma)}^2$ and lower continuity of $|D\cdot|$
	$$\begin{aligned}
		\inf\limits_{\sigma_h\in V_h}\phi_{\alpha}(\sigma_h)=\lim\limits_{k\to +\infty} \phi_{\alpha}(\sigma_h^k)\geq \phi_{\alpha}(\sigma_h^*) \geq \inf\limits_{\sigma\in V_h}\phi_{\alpha}(\sigma_h)
	\end{aligned}.$$
	So $\sigma^*$ is a minimizer to $\phi_\alpha$ and the proof is completed.
	\finproof
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\begin{thm}[stability] \label{disVhstab}
		Let $\{\bE^k\}$ satisfies $\|\bE^k\times n-\bE^{obs}\times n\|_{L^2(\Gamma)}\to 0$ as $k \to +\infty$. Assume $\sigma_k$ is the minimizer in Theorem \ref{disexVh}, but with $\bE^{obs}\times n$ replaced by $\bE^k\times n$. Then any limit point of $\{\sigma_n\}$ solves \eqref{disproVh}.
	\end{thm}
	\debproof
	The proof is same as the proof in (\ref{sta}), just notice that $V_h$ is a close set and any limit point of $\{\sigma_h^k\}$ is still in $V_h$.
	\finproof
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	To solve the optimization problem \eqref{disproVh}, we recast it to the following optimization problem
	\begin{equation}\label{dispro1Vh}
		\begin{aligned}
			\mathop{\arg\min}_{(\sigma_h,d_h) \in V_h\times (D_h)^3}
			\phi_{\alpha,h}(\sigma_h)=& G_h(\sigma_h)+\alpha \|d_h\|_{L^1(\Omega_c)},\\
			\text{subject to } d_h =& \nabla \sigma_h \text{ in weak sense.}
		\end{aligned}
	\end{equation}
	Then the augmented Lagrangian \eqref{lagran} can be extended to $L_{\alpha,\beta}^h: V_h\times (D_h)^3\times(D_h)^3\to \mathbb{R}$ as
	\begin{equation}\label{lagranV}
		L_{\alpha,\beta}^h(\sigma_h,d_h,y_h)=G_h(\sigma_h)+\alpha\|d_h\|_{L^1(\Omega_C)}+(y_h,d_h-\nabla\sigma_h)+\frac{\beta}{2} \|d_h-\nabla\sigma_h\|^2_{L^2(\Omega_C)}.
	\end{equation} 
	Now based on \eqref{lagranV}, we introduce Algorithm \ref{mADMMV}.
	\begin{algorithm}[t] 
		\caption{Modified  ADMM in $V_h$} \label{mADMMV}
		\hspace*{0.02in} {\bf Input:} 
		input initial values  $\sigma^0,d^0,y^0$\\
		\hspace*{0.02in} {\bf Output:} 
		$\sigma_N$
		\begin{algorithmic}
			\For{$k\in N$} 
			\State 
			\begin{equation} \label{sigmaiterVh}
				\sigma^{k+1}=\arg\min\limits_{\sigma\in V_h} L^h_{\alpha,\beta}(\sigma,d^{k+1},y^k),
			\end{equation}
			\begin{equation}
				d^{k+1}=\arg\min\limits_{d\in (D_h)^3} L^h_{\alpha,\beta}(\sigma^k,d,y^k),
			\end{equation}
			
			\begin{equation}
				y^{k+1}=\nabla (\Delta)^{-1} \nabla\cdot (y^k+\beta(d^{k+1}-\nabla\sigma^{k+1})).
			\end{equation}
			\EndFor
		\end{algorithmic}
	\end{algorithm}
	The only difference between Algorithm \ref{cADMM} and \ref{mADMMV} is \eqref{sigmaiterVh}. In Algorithm \ref{mADMMV}  the first sub-problem is solved on space $V_h$. The other two steps remain unchanged. Then the optimal condition of  (\ref{sigmaiterVh}) says
	\begin{equation}\label{sigmaminVh}
		(G_h'(\sigma^{k+1}),\tilde{\sigma})-(y^k,\nabla\tilde{\sigma})-(\beta(d^{k}-\nabla\sigma^{k+1}),\nabla\tilde{\sigma})=0\quad \forall \tilde{\sigma} \in V_h.
	\end{equation}
	Still we introduce $z^{k+1}\in V_h$ such that
	$$
	\Delta z^{k+1} = \nabla\cdot(y^k+\beta(d^{k+1}-\nabla\sigma^{k+1}))
	$$
	in weak sense. Then $y^{k+1} = \nabla z^{k+1}$ and the optimality equation (\ref{sigmaminVh}) becomes
	\begin{equation}\label{heq}
		(G'_h(\sigma^{k+1}),\tilde{\sigma}) = (\nabla z^{k+1}, \nabla \tilde{\sigma}) \qquad \forall \,\tilde{\sigma} \in V_h
	\end{equation}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	Before we prove the convergence of Algorithm \ref{mADMMV}, we first show some technique inequalities. 
	\begin{lem}\label{lemma4.2}
		There is a constant $C_0 > 0$ which is independent on mesh size $h$, such that
		\begin{equation}
			\|\nabla z^{k+1}-\nabla z^k\|_{L^2(\Omega_C)} \leq C_0h^{-2}\|\sigma^{k+1}-\sigma^{k}\|_{L^2(\Omega_C)}.
		\end{equation}
	\end{lem}
	\debproof
	By \eqref{heq}, we have
	\begin{eqnarray*}
		(G_h'(\sigma^{k}),\tilde{\sigma})= (\nabla z^{k},\nabla \tilde{\sigma})\quad \forall \tilde{\sigma} \in V_h.\\
		(G_h'(\sigma^{k+1}),\tilde{\sigma})= (\nabla z^{k+1},\nabla \tilde{\sigma})\quad \forall \tilde{\sigma} \in V_h.
	\end{eqnarray*}
	Take $\tilde{\sigma} = z^{k+1} - z^k$, then by Lemma \ref{disin} and Lemma \ref{p2},
	$$\begin{aligned}
		(\nabla z^{k+1}-\nabla z^k,\nabla z^{k+1} -\nabla z^k) = &(G_h'(\sigma^{k+1})-G_h'(\sigma^k), z^{k+1} - z^k)\\
		\leq& C_{eddy}\|\sigma^{k+1}-\sigma^k\|_{L^\infty(\Omega_C)}\|z^{k+1}-  z^k\|_{L^\infty(\Omega_C)}\\
		\leq& C_{eddy}C_{\infty}^2h^{-3}\|\sigma^{k+1}-\sigma^k\|_{L^2(\Omega_C)}\|z^{k+1} - z^k\|_{L^2(\Omega_C)}\\
		\leq&C_{eddy}C_{\infty}^2h^{-2}C_{s}\|\sigma^{k+1}-\sigma^k\|_{L^2(\Omega_C)}\|\nabla z^{k+1} - \nabla z^k\|_{L^2(\Omega_C)}.
	\end{aligned}$$
	%The last inequality comes from Poincare inequality $C_{po}$ be the constant only depended on the domain since $h^{k+1} - h^k \in H_0^1(\Omega_C)$.
	Set $C_0 = C_{eddy}C_{\infty}^2C_{s}$, and we get $$	\|\nabla z^{k+1}-\nabla z^k\|_{L^2(\Omega_C)} \leq C_0h^{-2}\|\sigma^{k+1}-\sigma^{k}\|_{L^2(\Omega_C)}.$$
	\finproof
	Now, we can analyze the decrease of $L^h_{\alpha,\beta}(\sigma^k,d^k,y^k)$. 
	
	\begin{lem}\label{l1}
		There is some positive constant $\beta_0=O(h^{-1})$ such that if  $\beta > \beta_0$, then the series  $\{L_{\alpha,\beta}(\sigma^k,d^k,y^k)\}$ is decreasing. Moreover
		\begin{equation}\label{ineq}
			\begin{aligned}
				&L^h_{\alpha,\beta}(\sigma^{k+1},d^{k+1},y^{k+1})-L^h_{\alpha,\beta}(\sigma^k,d^k,y^k)\\
				&\quad\quad \leq -\frac{\gamma}{2}\|d^{k+1}-d^k\|_{L^2(\Omega_C)}^2
				-(\frac{\beta}{2}C_s^{-2}h^{-2}-C_{eddy}C_{\infty}^2h^{-3}-\beta^{-1} C_0^2h^{-4})\|\sigma^{k+1}-\sigma^k\|_{L^2(\Omega_C)}^2
			\end{aligned}.
		\end{equation}
		where $\gamma >0$ is a constant.
	\end{lem}
	\debproof
	We have the following decomposition,
	\begin{equation}
		\begin{aligned}
			L^h_{\alpha,\beta}(\sigma^{k+1},d^{k+1},y^{k+1})-L^h_{\alpha,\beta}(\sigma^k,d^k,y^k)&=L^h_{\alpha,\beta}(\sigma^{k+1},d^{k+1},y^{k+1})-L^h_{\alpha,\beta}(\sigma^{k+1},d^{k+1},y^k)\\
			&+L^h_{\alpha,\beta}(\sigma^{k+1},d^{k+1},y^{k})-L^h_{\alpha,\beta}(\sigma^{k+1},d^k,y^k)\\
			&+L^h_{\alpha,\beta}(\sigma^{k+1},d^{k},y^{k})-L^h_{\alpha,\beta}(\sigma^k,d^k,y^k)\\
			&=I_1+I_2+I_3.
		\end{aligned}
	\end{equation}
	Then we estimate $I_i,i=1,2,3$ term by term. For $I_1$, with the help of Lemma \ref{lemma4.2}, we have
	\begin{equation}\label{I1}
		\begin{aligned}
			I_1=&(y^{k+1}-y^k,d^{k+1}-\nabla\sigma^{k+1})\\
			=&(\nabla z^{k+1}-\nabla z^k,d^{k+1}-\nabla \sigma^{k+1})\\
			%		=&-<h^{k+1}-h^k,\nabla\cdot (d^{k+1}-\nabla\sigma^{k+1})>\\
			%		=&-\beta^{-1}<h^{k+1}-h^k,\Delta(h^{k+1}-h^k)>\\
			=& \beta^{-1} (\nabla z^{k+1}-\nabla z^k,\nabla z^{k+1}-\nabla z^k)\\
			\leq& \beta^{-1}C_0^2h^{-4}\|\sigma^{k+1}-\sigma^k\|_{L^2(\Omega_C)}^2.
		\end{aligned}
	\end{equation}
	For $I_2$, since $L_{\alpha,\beta}^h(\sigma^{k+1},d,y^k)$ is strongly convex  about $d$, there is some $\gamma > 0$ \cite{convex}, only depended on $\alpha,\beta$ and $\Omega_C$, such that for  any $\eta\in \partial_d L_{\alpha,\beta}^h(\sigma^{k+1},d^{k+1},y^{k})$ which is the subgradient at $d^{k+1}$,  
	$$
	L^h_{\alpha,\beta}(\sigma^{k+1},d^k,y^k)-L^h_{\alpha,\beta}(\sigma^{k+1},d^{k+1},y^k)\geq (\eta,d^k-d^{k+1})+\dfrac{\gamma}{2}\|d^k-d^{k+1}\|_{L^2(\Omega_C)}^2.
	$$
	Since $d^{k+1}$ is the minimum point of $L_{\alpha,\beta}^h(\sigma^{k+1},d,y^k)$,
	there is some $\eta^{k+1} \in \partial_dL_{\alpha,\beta}^h(\sigma^{k+1},d^{k+1},y^k)$ such that
	$$
	(\eta^{k+1},d-d^{k+1})\geq 0 \quad \forall d
	$$
	So we have
	\begin{equation}\label{I2}
		\begin{aligned}
			I_2  =&L^h_{\alpha,\beta}(\sigma^{k+1},d^{k+1},y^k)-L^h_{\alpha,\beta}(\sigma^{k+1},d^k,y^k)\\
			\leq& -(\eta^{k+1},d^k-d^{k+1})-\frac{\gamma}{2}\|d^k-d^{k+1}\|_{L^2}^2\\
			\leq &-\frac{\gamma}{2}\|d^k-d^{k+1}\|_{L^2(\Omega_C)}^2.\\
		\end{aligned}
	\end{equation}
	Finally, for $I_3$, thanks to Lemma {\ref{disin}}, Lemma \ref{p2} and \eqref{sigmaminVh},
	\begin{equation}\label{I3}
		\begin{aligned}
			I_3=&G(\sigma^{k+1})-G(\sigma^k)-(y^k,\nabla\sigma^{k+1}-\nabla\sigma^k)+\dfrac{\beta}{2}\|d^k-\nabla\sigma^{k+1}\|^2_{L^2(\Omega_C)}-\dfrac{\beta}{2}\|d^k-\nabla\sigma^k\|_{L^2(\Omega_C)}^2\\
			\leq& (G_h'(\sigma^{k+1}),\sigma^{k+1}-\sigma^k)+C_{eddy}C_{\infty}^2h^{-3}\|\sigma^{k+1}-\sigma^k\|_{L^2(\Omega_C)}^2-(y^k,\nabla\sigma^{k+1}-\nabla\sigma^k)\\
			&-\beta((d^k-\nabla \sigma ^{k+1}),\nabla\sigma^{k+1}-\nabla\sigma^k)-\frac{\beta}{2}\|\nabla\sigma^{k+1}-\nabla\sigma^k\|_{L^2(\Omega_C)}^2\\
			=&-(\frac{\beta}{2}C_s^{-2}h^{-2}-C_{eddy}C_{\infty}^2h^{-3})\|\sigma^{k+1}-\sigma^k\|_{L^2(\Omega_C)}^2.\\
		\end{aligned}
	\end{equation}
	By summing up  \eqref{I1}, \eqref{I2} and \eqref{I3}, we have inequality \eqref{ineq}. The descreasing properties can be confirmed by  choosing $\beta_0$ as 
	\begin{equation}
		\beta_0=2\max(2C_S^2C_{eddy}C^2_\infty ,\sqrt{2}C_sC_0)h^{-1}.\label{beta0}
	\end{equation}
	%such that 
	%\begin{equation}\label{beta0}
	%\frac{\beta_0}{2}C_s^{-2}h^{-2}-%C_{eddy}C_{\infty}^2h^{-3}-\beta_0^{-1} %C_0^2h^{-4} > 0.
	%\end{equation}
	\finproof
	
	
	The following lemma tells us that the discrete Lagrangian is bounded below with some conditions.
	\begin{lem}\label{l2}
		There is some positive constatnt $\beta_1 =O(h^{-1})$, such that for any $\beta >\beta_1$, $\{L^h_{\alpha,\beta}(\sigma^k,d^k,y^k)\}$ is bounded below.
	\end{lem}
	\debproof
	Let $s^{k+1} \in V_h$ such that
	$$\int_{\Omega_C} \nabla s^{k+1} \cdot\nabla \lambda_hdx=\int_{\Omega_C}d^{k+1}\nabla\lambda_h dx \qquad \forall \,\, \lambda_h\in V_h.$$
	Then 
	$$\|d^{k+1}-\nabla \sigma^{k+1}\|_{L^2(\Omega_C)}^2=\|\nabla s^{k+1}-\nabla \sigma^{k+1}\|_{L^2(\Omega_C)}^2,$$
	and since $y^{k+1}=\nabla h^{k+1}$ with $h^{k+1}\in V_h$, we can also get
	$$(y^{k+1},d^{k+1})=(y^{k+1},\nabla s^{k+1})$$
	So by the definition of $L^h_{\alpha,\beta}(\sigma^{k+1},d^{k+1},y^{k+1})$  and \eqref{heq},  we have
	\begin{equation}
	\begin{aligned}
		&L^h_{\alpha,\beta}(\sigma^{k+1},d^{k+1},y^{k+1})\\
		&\quad=G_h(\sigma^{k+1})+\alpha \|d^{k+1}\|_{L^1(\Omega_C)}+(y^{k+1},d^{k+1}-\nabla \sigma^{k+1})
		+\frac{\beta}{2}\|d^{k+1}-\nabla \sigma^{k+1}\|^2_{L^2(\Omega_C)}\\
		&\quad=G_h(\sigma^{k+1})+\alpha \|d^{k+1}\|_{L^1(\Omega_C)}+(y^{k+1},\nabla s^{k+1}-\nabla \sigma^{k+1})
		+\frac{\beta}{2}\|\nabla s^{k+1}-\nabla \sigma^{k+1}\|^2_{L^2(\Omega_C)}\\
		&\quad=G_h(\sigma^{k+1})+\alpha \|d^{k+1}\|_{L^1(\Omega_C)}
		+(G_h'(\sigma^{k+1}),s^{k+1}-\sigma^{k+1})
		+\frac{\beta}{2}\|\nabla s^{k+1}-\nabla \sigma^{k+1}\|^2_{L^2(\Omega_C)}
		\end{aligned}\nonumber
	\end{equation}
	Then by Lemma \ref{p2} and \ref{disin}, 
	\begin{equation}
	\begin{aligned}
		&L^h_{\alpha,\beta}(\sigma^{k+1},d^{k+1},y^{k+1})\\
		&\qquad\geq G_h(\sigma^{k+1})+(G_h'(\sigma^{k+1}), s^{k+1}-\sigma^{k+1})+\frac{\beta C_{s}^{-2}}{2}h^{-2}\| s^{k+1}- \sigma^{k+1}\|^2_{L^2(\Omega_C)}\\
		&\qquad\geq  G_h( s^{k+1})-C_{eddy}C_{\infty,2}^2h^{-3}\| s^{k+1} -\sigma^{k+1}\|_{L^2(\Omega_C)}^2
		+\frac{\beta C_{s}^{-2}}{2}h^{-2}\| s^{k+1}- \sigma^{k+1}\|^2_{L^2(\Omega_C)}\\
		&\qquad\geq(\dfrac{\beta C_{s}^{-2}}{2}h^{-2}-C_{eddy}C_{\infty,2}^2h^{-3})\|s^{k+1}-\sigma^{k+1}\|_{L^2(\Omega_C)}^2.
		\end{aligned}\nonumber
	\end{equation}
	Then the proof is completed by choosing 
	\begin{equation}\label{beta1}
		\beta_1 = 2C_{eddy}C_{\infty}^2C_{s}^2h^{-1}.
	\end{equation}
	\finproof
	
	Now we are ready to prove the main result.  %Notice in \eqref{beta0} and \eqref{beta1}, $\beta_0$ and $\beta_1$ are both $O(h^{-1})$.
	
	\begin{thm}
		Assume that $\beta > \max\{\beta_0,\beta_1\}$, where $\beta_0$ and $\beta_1$  are defined in Lemma \ref{l1} and Lemma \ref{l2} respectively, we have the following convergence results.
		\begin{itemize}
			\item  [(1)] $\{L^h_{\alpha,\beta}(\sigma^k,d^k,y^k)\}$ is decreasing, and hence has a limit.
			\item [(2)] We have $\lim\limits_{k\to\infty}\|d^k-\nabla \sigma^k\|_{2}\to 0$.
			\item [(3)]If $(\sigma^*,d^*,y^*)$ denotes any limits points of $(\sigma^k,d^k,y^k)$, then
			\begin{equation}\label{convresult}
				\begin{aligned}
					&y^*= G_h'(\sigma^*)\qquad in\, weak\, sense,\\
					&d^*-\nabla \sigma^*=0,\\
					&d^* \in \arg\min \alpha\|d\|_{L^1(\Omega_C)}+(G_h'(\sigma^*),  d).\\	
				\end{aligned}\\
			\end{equation}
		\end{itemize}
	\end{thm}
	\debproof
	(1)  is obvious from (\ref{ineq}) and Lemma \ref{l2}. \\
	From (\ref{ineq}) and  (\ref{I1}), $\|\sigma^{k+1}-\sigma^k\|_{L^2(\Omega_C)}\to 0$ as $k\rightarrow\infty$ and hence $\|y^{k+1}-y^k\|_{L^2(\Omega_C)} \to 0$ as $k\rightarrow\infty$. So $\beta\|d^{k+1}-\nabla\sigma^{k+1}\|_{L^2(\Omega_C)} \to 0$, as $k\to \infty$. Then (2) is proved.\\
	To prove (3), let $\{(\sigma^k,d^k,y^k)\}$ be the sequence generated by Algorithm \ref{mADMMV}. We still denote it convergent subsequence by $\{(\sigma^k,d^k,y^k)\}$, which converges to $(\sigma^*, d^*, y^*)$ in $L^2(\Omega_C)\times L^1(\Omega_C)\times L^1(\Omega_C)$. 
	By (\ref{heq}), we know that
	\begin{eqnarray*}
		(y^k,\nabla \tilde{\sigma})=(G_h'(\sigma^{k}),\tilde{\sigma})\quad \forall \tilde{\sigma} \in V_h,   
	\end{eqnarray*}
	By taking $k\to\infty$ and with Lemma \ref{disin} we get the first equation of \eqref{convresult}. The second equation of \eqref{convresult} is a direct consequence of (2).		
	As for the third equation, notice 
	$$p^{k+1}+y^k+\beta (d^{k+1}-\nabla\sigma^{k+1})=0,$$
	where $p^{k+1}$ is a subgradient of $\alpha\|\cdot \|_{L^1(\Omega_C)}$. By the convexity of  $\alpha\|\cdot \|_{L^1(\Omega_C)}$, we can get
	\begin{eqnarray*}		
		\begin{aligned}
			&\alpha\| d \|_{L^1(\Omega_C)}-  \alpha\|d^{k+1} \|_{L^1(\Omega_C)} \geq (p^{k+1},d-d^{k+1})\\
			&\qquad\qquad = -(y^{k}+\beta (d^{k+1}-\nabla\sigma^{k+1}),d-d^{k+1})\quad \forall d \in D_h.
		\end{aligned}
	\end{eqnarray*}
	Let $k\to\infty$,  using $\|y^{k+1}-y^k\|_{L^2(\Omega_C)}\to 0, \|d^{k+1}-\nabla \sigma^{k+1}\|_{L^{2}(\Omega_C)}\to 0$, we have
	\begin{eqnarray*}
		\alpha\|d\|_{L^1{(\Omega_C)}}+(y^*,d)\geq \alpha\|d^*\|_{L^1{(\Omega_C)}} + (y^*, d^*)\quad \forall d\in D_h.
	\end{eqnarray*}
	This completes the proof.
	\finproof
	
	Here we remark that the penalty parameter $\beta$ can be chosen in $O(h^{-1})$ since $\beta_0,\beta_1$ are both in $O(h^{-1})$.
	%\begin{rem}
	%In {\bfseries{Remark} \ref{rm2}} we show $C_{eddy}(\delta_0,h_0)\to 0$ as $(\delta_0,h_0)\to (0,0)$, one can see that $\beta = o(h^{-3})$ if $\sigma_h\to \sigma$. In the examples in {\bfseries Section 5}, we choose $\beta =0.02$ and the Algorithm works.
	%\end{rem}
	
	
	
	%%We end this section by giving an algorithm with truncation. To describe the algorithm, we introduce a functional $P_{\lambda}$, assume $\varphi_1, \varphi_2,\cdots,\varphi_{NV}$ is a basis of $K_h$. For some $\lambda \in R$, let $P_{\lambda}:K_h\to K_h$ as
	%%$$P_{\lambda}(\sum\limits_{i=1}^{NV}a_i\varphi_i)=\sum\limits_{i=1}^{NV}a_i\mathcal{X}(a_i\geq \lambda)\varphi_i,$$
	%%here $\mathcal{X}(a_i\geq \lambda)$ is 1 if $a_i\geq \lambda$ and is 0 otherwise. With the truncation operator $P_\lambda$, we give the algorithm as Algorithm \ref{mADMMV}. In the next section, we will show numerically that it has a better performance than Algorithm \ref{cADMM}.
	
	
	\section{Numerical experiments}\label{sect5}
	In this section, we present some numerical examples to illustrate the efficiency of Algorithm 1 and Algorithm 2. We set the domain $\Omega=[-2,2]\times [-2,2] \times [-2,0.2]$, with $\Omega_0 = [-2,2]\times [-2,2]\times [0,0.2]$ and $\Omega_c = [-2,2]\times [-2,2]\times [-2,0]$. We implement the algorithms with the parallel hierarchical grid platform (PHG)\cite{PHG}. We carry out all the numerical experiments on a Lenovo notebook with Intel core i5 tenth gen CPU and 8G memory. In our numerical examples, the source is 
	$$\mathbf{J}_s=\nabla\times \sum\limits_{i,j=1}^{36} \delta(x-x_{ij}) e_1,$$
	where $e_1$ is the unit vector along the $x-axis$ and $x_{ij}= (-2.0 + 0.1\cdot i, -2.0 + 0.1 \cdot j, 0.14)$. Clearly, $\nabla\cdot \mathbf{J}_s = 0$. 
	The algorithms are tested on a mesh $\mathcal{M}_h$ and the data $n\times \bE^{obs}$ is generated by solving problem \eqref{weakeddy} with exact conductivity $\sigma$ on a refined mesh of $\mathcal{M}_h$. 
	We choose the regularization parameter $\alpha = 10^{-8}$ and the penalty parameter $\beta = 2 \times 10^{-2}$. We set the initial values $\sigma^0, d^0, y^0$ to zeroes for  all of the examples.
	
	\subsection{Example 1}
	In this example, the background conductivity $\sigma_0 = 1$ in $\Omega_C$ and the object $\Omega_2= [-0.5,0.5]\times [-0.5,0.5]\times[-1.0,-0.5]$ and the exact abnormal conductivity is given by $\sigma =5$ in $\Omega_2$ and $\sigma=0$ in $\Omega_C\backslash\bar\Omega_2$. So the exact conductivity is 6 in $\Omega_2$ and 1 in $\Omega_C\backslash\Omega_2$.
	
	The mesh $\mathcal{M}_h$ we used in this example has 798138 edges. The $\sigma$ sub-problems \eqref{sigmaiter} and \eqref{sigmaiterVh} are solved by NLCG method \cite{CLZ}.  In the Algorithm \ref{cADMM}, 3 NLCG iterations is used for Algorithm \ref{cADMM} and 5 NLCG iterations is used for Algorithm \ref{mADMMV}. 
We choose the truncation parameter $m = 0.02\times \max\{iter,50\}$ and $M=15$ in the $iter$-th iteration. 

	Figure \ref{E1A1} shows isosurfaces of the recovered $\sigma$ with isovalue 2.3 in 20th, 40th, 60th and 80th iterations by Algorithm \ref{cADMM}. The results tell us that the reconstructions approaches to the exact conductivity with iterations.  As we cans see, the isosurface becomes stable after 40-th iteration. %The max value of recovered $\sigma$ is increasing and it reaches 5.8 in the 80th iteration.  %The max values in the four iterations are 2.4, 3.5, 5.0 and 5.8. When the isovalue is 3, we have isosurfaces in Figure \ref{E1A12}.} 
	 Figure \ref{E1A2} shows isosurfaces of the recovered $\sigma$ with isovalue 1.5  in 20th, 40th, 60th, 80th iterations by Algorithm \ref{mADMMV}. It behaves like Algorithm \ref{cADMM} . %but the max value grow slower.  The max value is 2.7 in the 80th iteration. %The max values in four iterations are 1.6, 2.1, 2.5 and 2.7.}
	The $L^2$ errors of $\sigma - \sigma^{real}$ for both algorithms are showed in Figure \ref{ex1_con}. Figure \ref{E1A1} and \ref{E1A2} tell us that the max value reaches 5.8 and 2.7 with Algorithm \ref{cADMM} and \ref{mADMMV}, respectively. Then we can conclude that Algorithm 1 have better convengence than Algorithm 2, although Algorithm 2 has the convergence guarantee in theory. Besides, the Algorithm \ref{cADMM} takes less computing time since it takes fewer NLCG iterations.  
	\begin{figure}[htbp]
		\centering
		\subfigure
		{
			\begin{minipage}{0.45\linewidth}
				\centering
				\includegraphics[scale=0.18]{sigma20wt}
				{20th iteration}
			\end{minipage}
		}	
		\subfigure
		{
			\begin{minipage}{0.45\linewidth}
				\centering
				\includegraphics[scale=0.18]{sigma40wt}
				{40th iteration}
			\end{minipage}
		}
		\subfigure
		{
			\begin{minipage}{0.45\linewidth}
				\centering
				\includegraphics[scale=0.18]{sigma60wt}
				{60th iteration}
			\end{minipage}
		}	
		\subfigure
		{
			\begin{minipage}{0.45\linewidth}
				\centering
				\includegraphics[scale=0.18]{sigma80wt}
				{80th iteration}
			\end{minipage}
		}
		\caption{The results for Algorithm 1 in different iterations for experiment 1 with isovalue = 2.3.}\label{E1A1}
	\end{figure}

%%	\begin{figure}[htbp]
%%	\centering
%%	\subfigure
%%	{
%%		\begin{minipage}{0.45\linewidth}
%			\centering
%			\includegraphics[scale=0.2]{sigma20-3}
%			{20th iteration}
%		\end{minipage}
%	}	
%	\subfigure
%	{
%		\begin{minipage}{0.45\linewidth}
%			\centering
%			\includegraphics[scale=0.2]{sigma40-3}
%			{40th iteration}
%		\end{minipage}
%	}
%	\subfigure
%	{
%		\begin{minipage}{0.45\linewidth}
%			\centering
%			\includegraphics[scale=0.2]{sigma60-3}
%			{60th iteration}
%		\end{minipage}
%	}	
%	\subfigure
%	{
%		\begin{minipage}{0.45\linewidth}
%			\centering
%			\includegraphics[scale=0.2]{sigma80-3}
%			{80th iteration}
%		\end{minipage}
%	}
%	\caption{The results for Algorithm 1 in different iterations for experiment 1 with isovalue = 3.}\label{E1A12}
%\end{figure}


	\begin{figure}[htbp]
		\centering
			\subfigure
			{
					\begin{minipage}{0.45\linewidth}
							\centering
							\includegraphics[scale=0.18]{sigma20}
				   20th iteration
						\end{minipage}
				}
		\subfigure
		{
			\begin{minipage}{0.45\linewidth}
				\centering
				\includegraphics[scale=0.18]{sigma40}
				{40th iteration}
			\end{minipage}
		}	
		\subfigure
		{
			\begin{minipage}{0.45\linewidth}
				\centering
				\includegraphics[scale=0.18]{sigma60}
				{60th iteration}
			\end{minipage}
		}
		\subfigure
		{
			\begin{minipage}{0.45\linewidth}
				\centering
				\includegraphics[scale=0.18]{sigma80}
				{80th iteration}
			\end{minipage}
		}	
		\caption{The results for Algorithm 2 in different iterations for example 1 with isovalue = 1.5.}\label{E1A2}
	\end{figure}
	
	\begin{figure}[htbp]
		\centering
		\includegraphics[scale=0.6]{exampleone}
		\caption{$L_2$ errors of Algorithm \ref{cADMM} and Algorithm \ref{mADMMV} for the example 1.}\label{ex1_con}
	\end{figure}
	
\subsection{Example 2}
In this example, the background conductivity $\sigma_0 = 1$ in $\Omega_C$ and the abnormal domain $\Omega_2$ consists of two cubes $D_1= [-1.0,-0.4]\times [-1.0, -0.4]\times[-1.0,-0.5] $ and $D_2=[0.4,1.0] \times [0.4,1.0] \times [-1.0, -0.5]$.
The mesh used in this example is the same with Example 1.  In this example, we only test the Algorithm \ref{cADMM}. 
We use 5 NLCG iterations in the sub-problem \eqref{sigmaiter}  and  80 ADMM iterations totally.
We choose  $m = 0.02\times iter$ and $M=15$ in the $iter$-th iteration.

We test the Algorithm \ref{cADMM} with two settings as follows.
\begin{itemize}
\item  {\bfseries Setting 1}
$$
\sigma = \left\{
\begin{aligned}
	8& \qquad \text { in } \Omega_2 = D_1\cup D_2\\
	0& \qquad \text { in } \Omega_C\backslash\Omega_2
\end{aligned}\right.
$$
\item  {\bfseries Setting 2}
$$
\sigma = \left\{
\begin{aligned}
	10& \qquad \text { in } D_1\\
	6& \qquad \text{ in } D_2 \\
	0& \qquad \text { in } \Omega_C\backslash\Omega_2
\end{aligned}\right.
$$
\end{itemize}

\begin{figure}[htbp]
	\centering
	\subfigure
	{
		\begin{minipage}{0.45\linewidth}
			\centering
			\includegraphics[scale=0.18]{C8820}
			20th iteration
		\end{minipage}
	}
	\subfigure
	{
		\begin{minipage}{0.45\linewidth}
			\centering
			\includegraphics[scale=0.18]{C8840}
			{40th iteration}
		\end{minipage}
	}	
	\subfigure
	{
		\begin{minipage}{0.45\linewidth}
			\centering
			\includegraphics[scale=0.18]{C8860}
			{60th iteration}
		\end{minipage}
	}
	\subfigure
	{
		\begin{minipage}{0.45\linewidth}
			\centering
			\includegraphics[scale=0.18]{C8880}
			{80th iteration}
		\end{minipage}
	}
	\caption{The results for Algorithm 1 in different iterations for the first setting of example 2 with isovalue = 1.3.}\label{E2S1}
\end{figure}


\begin{figure}[htbp]
\centering
\subfigure
{
	\begin{minipage}{0.45\linewidth}
		\centering
		\includegraphics[scale=0.18]{C1620}
		20th iteration
	\end{minipage}
}
	\subfigure
	{
			\begin{minipage}{0.45\linewidth}
					\centering
					\includegraphics[scale=0.18]{C1640}
		   {40th iteration}
				\end{minipage}
		}	
\subfigure
{
	\begin{minipage}{0.45\linewidth}
		\centering
		\includegraphics[scale=0.18]{C1660}
		{60th iteration}
	\end{minipage}
}
\subfigure
{
	\begin{minipage}{0.45\linewidth}
		\centering
		\includegraphics[scale=0.18]{C1680}
		{80th iteration}
	\end{minipage}
}
\caption{The results for Algorithm 1 in different iterations for the second setting of example 2 with isovalue = 1.4.}\label{E2S2}
\end{figure}
Figure  \ref{E2S1} shows isosurfaces of the recovered $\sigma$ with isovalue 1.3 in 20th, 40th, 60th and 80th iterations for the first setting and  Figure \ref{E2S2} shows the isosurfaces with isovalue 1.4 in 20th, 40th, 60th and 80th iterations for the second setting. The isosurfaces become stable since 40th iterations.  Also we can see in the setting 2, the Algorithm can tell the different values in sub-domain since the recovered domain $D_1$ is bigger than $D_2$ with the same isovalue.
One can find that the algorithm can reconstruct the abnormal conductors well in both cases. Figure \ref{E2S12} shows the $L^2$ error of two settings. We can conclude that the Algorithm \ref{cADMM} has good convergence numerically for the two settings.  \begin{figure}[htbp]
\centering
\subfigure
{
	\begin{minipage}{0.45\linewidth}
		\centering
		\includegraphics[scale=0.4]{C88}
		Setting 1 of Example 2
	\end{minipage}
}
\subfigure
{
	\begin{minipage}{0.45\linewidth}
		\centering
		\includegraphics[scale=0.4]{C16}
		{Setting 2 of Example 2}
	\end{minipage}
}	
\caption{The $L^2$ error for Algorithm 1 in different iterations for the two settings of Example 2.}\label{E2S12}
\end{figure}		
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{Conclusion}\label{sect6}
	We study the inverse eddy current problem by assuming that the conductivity lies in bound variation space. Firstly, we analyze the well-posedness of the inverse problem with total variation regularization. Then  we discrete the inverse problem with finite element method and give the well-posedness of the discrete inverse problem. In the framework of ADMM, we propose Algorithm \ref{cADMM} to solve the corresponding inverse problem. After that, we extend the admissible set to the whole space, give the well-posedness for the extended discrete problem. Then we propose Algorithm \ref{mADMMV} for the extended problem and prove the convergence by virtue of the  gradient Lipschitz property of the discrete objective function. Finally, we give two kinds of examples to illustrate the efficiency of the proposed algorithms. Although we can not prove the convergence of the Algorithm \ref{cADMM}, the numerical results show that it has better performance.
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{thebibliography}{99}	
\bibitem{ABF}
H. Ammari, G. Bao, and J. L. Fleming, An inverse source problem for maxwell's equations in magnetoencephalography, SIAM J. Appl. Math., 62(2002),1369-1382.

\bibitem{ACCGV}
H. Ammari, J. Chen, Z. Chen, J. Garnier, and D. Volkov, Target detection and characterization from electromagnetic induction data, J. Math. Pures Appl., 101 (2014), 54-75.

\bibitem {ACCVW}
H. Ammari, J. Chen, Z. Chen, D. Volkov and H. Wang, Detection and classification from electromagnetic induction data, J. Comput. Phys., 301 (2015), 201-217.

\bibitem{FEM}

S. C. Brenner and L. Ridgway Scott, The Mathematical Theory of Finite Element Methods, Springer-Verlag, 2002. 

\bibitem{ADMM1}
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, Distributed optimization and statistical learning via the alternating direction method of multipliers,  Foundations and Trends in Machine Learning,
3(2011).
\bibitem{BAN}
A. Buffa, H. Ammari, and J. C. N\'ed\'elec, A Justification of Eddy Currents Model for the Maxwell Equations, SIAM J. Appl. Math., 60(2000), 1805-1823.

\bibitem{splitting2}
J. F. Cai,  S. Osher , and  Z. Shen . Split Bregman Methods and Frame Based Image Restoration. Multiscale Model. Simul. 8(2009),337-369.
\bibitem{chantai}
T. Chan and X. Tai, Identification of discontinuous coefficients in elliptic problems using total variation regularization. SIAM J. Sci. Comput. 25(2003), 881-904.
\bibitem{CCCZ}
J. Chen, Z. Chen, T. Cui and L.-B. Zhang, An adaptive finite  method for the eddy current model with circuit/field coupings, SIAM J. Sci. Comput., 32(2010), 1020-1042.

\bibitem{CLZ}
J. Chen,  Y.  Liang,  and  J. Zou,  Mathematical and numerical study of a three-dimensional inverse eddy current problem, SIAM J. Appl. Math., 80(2020), 1467-1492.

\bibitem{chenzou}
Z. Chen, and J. Zou,  An Augmented Lagrangian Method for Identifying Discontinuous Parameters in Elliptic Systems, SIAM J. Control  Optim., 37(1999), 892-910.

\bibitem{ciarlet}
P. Ciarlet, The finite element method for elliptic problems, North-Holland Publishing Company, 1978.

\bibitem{GJL}
M. Gehre, B. Jin,   and  X. Lu,  An Analysis of Finite Element Approximation in Electrical Impedance Tomography, Inverse Problems, 30(2013), 958-964.

\bibitem{regu}
M. Costabel, M. Dauge, and S. Nicaise, Singularities of Maxwell interface problems, ESAIM, Math Model. Numer. Anal., 3 (1999), 627-649.

\bibitem{GO}
T. Goldstein,   and S. Osher, The Split Bregman Method for L1-Regularized Problems, SIAM Journal on Imaging Sciences, 2(2009), 323-343.

\bibitem{HHT}
E. Haber,  L. Horesh,   and  L. Tenorio, Numerical methods for experimental design of large-scale linear ill-posed inverse problems, Inverse Problems, 24(2008), 055012.

\bibitem{Haber2014Computational}
E. Haber, Computational Methods in Geophysical Electromagnetics. Society for Industrial and Applied Mathematics, 2014.

\bibitem{HY}
B. He,   and  X. Yuan,  On the $O(1/n)$ Convergence Rate of the Douglas-Rachford Alternating Direction Method, SIAM J. Numer. Anal., 50(2012), 700-709.

\bibitem{HLR}
M. Hong,   Z. Luo, and M. Razaviyayn,  Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems. SIAM J. Optim., 26 (2016), 337-364.

\bibitem{PHG}
Parallel Hierarchical Grid : http://lsec.cc.ac.cn/phg/

\bibitem{HX}
R. Hiptmair and J. Xu,  Nodal auxiliary space preconditioning in H(curl) and H(div) spaces,
SIAM J. Numer. Anal., 45 (2007), 2483-2509.

\bibitem{convex}
Y. Nesterov, Lectures on Convex Optimization, 2nd Edition, Springer Optimization and Its Applications, 2018. 

\bibitem{ROF2}
S. Osher, M. Burger, D. Goldfarb, J. Xu, W. Yin, An iterative Regularization Method for Total Variation-Based Image Restoration.  Multiscale Model. Simul., 4 (2005), 460-489.

\bibitem{Rodr2012Inverse}
Rodríguez, A. Alonso and  Camano, Jessika  and  Valli, Alberto, Inverse source problems for eddy current equations, Inverse Problems, 28(2012), 015006.

\bibitem{ROF1}
I. Rudinleonid,  S. Osher  and FatemiEmad. Nonlinear total variation based noise removal algorithms. Physica D,  60(1992), 259-268.


\bibitem{S-Z}
  R. Scott and. Zhang, , Finite element interpolation of nonsmooth functions satisfying boundary conditions, Math. Comp., (54)1990, 483–493.


\bibitem{TG}
A. Tamburrino,   and G. Rubinacci, A new non-iterative inversion method for electrical resistance tomography, Inverse Problems, 18(2002), 1809-1829.

\bibitem{Zhdanov}
 M.S. Zhdanov, Foundations of Geophysical Electromagnetic Theory and Methods: Elsevier,  2018.

\bibitem{ADMM3}
J. Yang, Y. Zhang, and W. Yin,  An efficient TV-L1 algorithm for deblurring multichannel images corrupted by impulsive noise,  SIAM
J. Sci. Comput., 31(2009),  2842-2865.

 \bibitem{ADMM2}
 W. Yin, S. Osher, D. Goldfarb, and J. Darbon, Bregman iterative algorithms for l1-minimization with applications to compressed sensing, 
 SIAM Journal on Imaging Science, 1(2008), 143-168.

\end{thebibliography}

\end{document}



