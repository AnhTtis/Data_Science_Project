\subsection{Linearization of PnP Solver}
%%%%%%%%% BODY TEXT
\noindent \textbf{Implicit Function Theorem.}
The implicit function theorem (IFT)~\cite{krantz2002implicit} states the following:

Given \mbox{$f:\mathbb{R}^{n+m}\to \mathbb{R}^m$} a continuously differentiable function
 with input $(\bs{a},\bs{b})\in \mathbb{R}^n\times\mathbb{R}^m$, if a point $(\bs{a}^*,\bs{b}^*)$ satisfies
\begin{equation}\label{eq:ift_f}
    f(\bs{a}^*,\bs{b}^*)=\bs{0}\;,
\end{equation}
and the Jacobian matrix $\frac{\partial f}{\partial\bs{b}}(\bs{a}^*,\bs{b}^*)$ is invertible, then there exists a unique continuously differentiable function \mbox{$g(\bs{a}):\mathbb{R}^n\to\mathbb{R}^m$} such that
\begin{equation}\label{eq:ift_g}
    \bs{b}^*=g(\bs{a}^*)\;,
\end{equation}
and
\begin{equation}\label{eq:ift_fg}
f(\bs{a}^*,g(\bs{a}^*))=\bs{0}\;.
\end{equation}
The Jacobian matrix $\frac{\partial g}{\partial\bs{a}}(\bs{a}^*)$ is given by
\begin{equation}\label{eq:ift_grad}
\frac{\partial g}{\partial\bs{a}}(\bs{a}^*)=-\left[ \frac{\partial f}{\partial \bs{b}}(\bs{a}^*,\bs{b}^*) \right]^{-1}\cdot
\frac{\partial f}{\partial \bs{a}}(\bs{a}^*,\bs{b}^*)\;.
\end{equation}

\noindent \textbf{PnP Linearization.}
Following the same notation as in the main paper, the PnP solver computes the function
\begin{equation}\label{eq:sup_pnp}
    g(\bs{x},\bs{z},\bs{w}) = \mathop{\arg\min}_{\bs{y}}\frac{1}{2} \sum_i^N \left\Vert \bs{w}_i \circ \bs{r}_i \right\Vert^2\;,
\end{equation}
where $\bs{x}_i$ is the $i$-th image 2D point, $\bs{z}_i$ is the $i$-th 3D point, $\bs{w}_i$ is the corresponding weight, and
\begin{equation}
\bs{r}_i = \bs{x}_i - \pi(\bs{z}_i,\bs{y})
\end{equation}
is the reprojection residual for the $i$-th correspondence given pose $\bs{y}$.

Eq.~\ref{eq:sup_pnp} implies that the solution $\bs{y}^*$ is the stationary point of the negative log likelihood (NLL) function
\begin{equation}\label{eq:nll}
    nll(\bs{y}) = \frac{1}{2} \sum_i^N \left\Vert \bs{w}_i \circ \bs{r}_i \right\Vert^2\;.
\end{equation}

Since $\bs{y}^*$ is the stationary point of the NLL function, the first order derivative of the NLL w.r.t. $\bs{y}^*$ should be zero, i.e.,
\begin{equation}
    \left.\frac{\partial nll(\bs{y})}{\partial\bs{y}}\right|_{\bs{y}=\bs{y}^*}=\bs{0}\;.
\end{equation}

Eqs.~\ref{eq:ift_f},~\ref{eq:ift_g} and ~\ref{eq:ift_fg} in the PnP case can subsequently be specialized as
\begin{equation}
f(\bs{x},\bs{y},\bs{z},\bs{w})|_{\bs{y}=\bs{y}^*}=\left.\frac{\partial nll(\bs{y})}{\partial\bs{y}}\right|_{\bs{y}=\bs{y}^*}=\bs{0}\;,
\end{equation}
\begin{equation}
\bs{y}^*=g(\bs{x},\bs{z},\bs{w})\;,
\end{equation}
and
\begin{equation}
f(\bs{x},g(\bs{x},\bs{z},\bs{w}),\bs{z},\bs{w})|_{\bs{y}=\bs{y}^*}=\bs{0}\;.
\end{equation}

According to Eq.~\ref{eq:ift_grad}, the gradient of the pose $\bs{y}$ w.r.t. the 2D locations $\bs{x}$ at $\bs{y}^*$ is
\begin{equation}\label{eq:dydx}
\begin{split}
    \left.\frac{\partial\bs{y}}{\partial\bs{x}}\right|_{\bs{y}^*}&=
    \left.\frac{\partial g(\bs{x},\bs{z},\bs{w})}{\partial \bs{x}}\right|_{\bs{y}^*}\;,\\
    &=\left.-\left[\left[\frac{\partial^2 nll(\bs{y})}{\partial \bs{y}^2}\right]^{-1} \cdot
    \frac{\partial^2 nll(\bs{y})}{\partial \bs{y}\partial\bs{x}}\right]\right|_{\bs{y}*}\;,\\
    &=\left.-H^{-1}\cdot
    \frac{\partial^2 nll(\bs{y})}{\partial \bs{y}\partial\bs{x}}\right|_{\bs{y}^*}\;,
\end{split}
\end{equation}
with $nll(\bs{y})$ defined by Eq.~\ref{eq:nll}.

Given the noisy correspondences $\{\bs{x},\bs{z},\bs{w}\}$, we compute the perfect correspondences $\{\bs{x}_p,\bs{z},\bs{w}\}$ with $\bs{x}_{p,i}=\pi(\bs{z}_i,\bs{y}_{gt})$ under the ground-truth pose $\bs{y}_{gt}$. We then linearize the PnP solver around $\{\bs{x}_p,\bs{z},\bs{w}\}$ and $\bs{y}_{gt}$ using the first-order Taylor expansion as
\begin{equation}
    \bs{y}=\bs{y}_{gt}+A(\bs{z},\bs{w})\cdot \bs{r}_{gt}\;,
\end{equation}
with
\begin{equation}
\bs{r}_{gt} = \bs{x}-\bs{x}_{gt}
\end{equation}
being the residual vector at $\bs{y}_{gt}$,
and 
\begin{equation}
    A(\bs{z},\bs{w})=\left.-H^{-1}\cdot
    \frac{\partial^2 nll(\bs{y})}{\partial \bs{y}\partial\bs{x}}\right|_{\bs{y}=\bs{y}_{gt},\bs{x}=\bs{x}_p}.
\end{equation}
The Hessian $H$ of the NLL function is also used to compute the prior loss, as stated in Sec.~3.3 in the main paper.

\input{figs-tex/correctness}

\subsection{Detailed Results on Gradient Correctness}
The correctness scores for different PnP layers reported in the main paper were calculated in simulated conditions, where the extracted correspondences have large errors. We further provide the correctness curves based on actual training to show how the correctness evolves as training progresses. 

As illustrated in Fig.~\ref{fig:corr}, at the very beginning, when the correspondences have large errors, both EPro-PnP~\cite{Chen_2022_CVPR} and BPnP~\cite{Chen_2020_CVPR} have good correctness. However, their correctness drops when the training proceeds. Since the linear-covariance loss is designed to address this problem, it always maintains a correctness close to 100\%.

\subsection{Details on ZebraPose-based Experiments}
\noindent \textbf{Implementation Details.}
\input{tables/sup-zebra}
Our coordinate-wise encoding scheme assigns 3 binary codes to a vertex, eliminating the look up operation. To reduce the number of binary bits for prediction, we rotate some of the objects to minimize their span along the $x,y,z$ directions. We use 7 bits to represent the coordinate component with the largest span, and calculate the binary count of the other components based on their relative span w.r.t. largest one.

\input{figs-tex/zebra}
\noindent \textbf{Results.}
As shown in Tab.~\ref{tab:sup-zebra}, after switching from the global vertex encoding to our coordinate-wise encoding (A0~\textit{vs.}~A1), the performance drops by about 1.7 points. When the LC loss is applied, the performance drop is compensated, surpassing the original ZebraPose~\cite{Su_2022_CVPR}.

\noindent \textbf{Visualizations.}
As illustrated by Fig.~\ref{fig:zebra}, the learned weight map successfully captures the error distribution of the predicted 3D coordinates in a geometry-aware manner, generating low weights for code transition regions and high weights for object endpoint regions.

\subsection{Detailed Results on LM-O and YCB-V}
For the LM-O dataset, we provide the detailed comparison of ADD(-S) scores with state-of-the-art methods, when the linear-covariance (LC) loss is applied to GDR-Net and ZebraPose on LM-O in Tab.~\ref{tab:lmo-detail}.

For the YCB-V dataset, we provide the detailed comparison of ADD(-S) scores (Tab.~\ref{tab:ycbv-detail-add}) and AUC scores (Tab.~\ref{tab:ycbv-detail-auc}) between the baseline methods and the versions where the LC loss is applied.

\input{tables/detail-ycbv-add}
\input{tables/lmo-sota}
\input{tables/detail-ycbv-auc} 