In the previous section we have seen how to construct equivalent kernels for fully connected deep neural networks, by simple composition of the shape function.
Here we introduce our approach on defining spherical kernels with continuous depth.

We begin by inspecting Eq.~\eqref{eq:k_relu} for the case of $L$ compositions of the shape function, $\kappa^L(\cdot)$.
If we are able to compute the eigenvalues of the deep Relu kernel, or any other kernel with a valid shape function, then we can write it as a polynomial expansion, similarly to Eq.~\eqref{eq:k_expanse}.
The behaviour of the eigenvalues for the spherical kernels has been the subject of interest in many studies recently~\citep{bietti2021deep,belfer2021spectral}. One key result is that the eigenvalues decay polynomially as we move to higher frequencies.
Moreover,~\citet{bietti2021deep} have shown that for different shape functions, the decay rate can depend or the depth.

Motivated from the above findings and looking again at Eq.~\eqref{eq:k_expanse} we can define a spherical kernel without having access to a specific shape function and without needing to compute the integral from Eq.~\eqref{eq:eigval_FH}, which obviously depends on the depth through the composed shape function.

We propose to use a kernel of the form:
\begin{align}\label{eq:k_poly}
    k(\mathbf{x}, \mathbf{x}^\prime) = r(\mathbf x, \mathbf x^\prime)
    \sum_{\ell=0}^{\infty} \frac{\ell + \alpha}{\alpha} {\ell}^{-\beta} C_\ell^{(\alpha)}(\mathbf{x}^\top\mathbf{x}^\prime)\,,
\end{align}
where we explicitly model the eigenvalues via the polynomial $\ell^{-\beta}$ and, the order of the polynomial $\beta > 0$ is a kernel hyper-parameter.
There are two important observations we need to state. First, the proposed kernel does not correspond directly to a known function; instead, it can be seen as a random spherical harmonic feature expansion, in analogy to the random Fourier features. Second, by learning the $\beta$ hyper-parameter we model the effect of depth in a continuous way; lower values for $\beta$ correspond to deeper kernels.

\begin{figure}[ht]
\includegraphics[scale=0.4, clip]{./figs/eigval_truncation.pdf}
\centering
    \caption{Eigenvalue comparison between the proposed kernel with continuous depth vs the NTK kernel with varying depth levels, for 3 dimensional problems (left) and 10 dimensional problems (right). Each eigenvalue is depicted relative to the corresponding eigenvalue of the first non-constant frequency.}
\label{fig:eigvals}
\end{figure}

\paragraph{Empirical study of the eigenvalue decay.} In Fig.~\ref{fig:eigvals} we compare the eigenvalue decay between the two approaches: (i) the deep NTK kernel~\citep{jacot2018neural} via compositions of the shape function; and (ii) the proposed polynomial expansion approach with continuous depth via the hyper-parameter $\beta$.
The first thing to notice is that the dimension of the problem plays an important role. Counter-intuitively, in higher dimensions (right panel), the high frequencies vanish rapidly, as even after only $10$ frequencies the decay is more than three orders of magnitude.
Further, by adding more depth the decay rate becomes slower in both low and high dimensions and the high frequencies become more relevant. The effect is especially pronounced in the low dimensional problems.
Finally, notice how the proposed kernel with continuous values for $\beta$ mimics the behaviour of NTK with increasing number of shape function compositions, i.e., depth.

