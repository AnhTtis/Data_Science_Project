Here we leverage our knowledge on the spherical kernels to build efficient Gaussian process models on the associated RKHS. These models mimic the behavior of deep fully connected neural networks.

\subsection{Inter-domain GPs with spherical harmonics}\label{sec:variational_intro}
In their work~\citet{dutordoir2020sparse} have proposed a variational approach based on spherical harmonics to learn an approximation to the GP posterior.
To do so they introduced an inter-domain approach where the inducing features are the spherical harmonics. This results in diagonal covariance structure for the kernel via the Mercer's theorem and also allows to learn features with global structure compared to the local information of the traditional inducing points.
More specifically, they define inducing variables $\uml$ as the inner product between the GP function $f$ and the spherical harmonics:
\begin{equation}
    \uml = \langle f, \Yml \rangle_\mathcal{H}.
\end{equation}
Then, they use the reproducing property to compute the covariance between the function and the inducing features, i.e., $\left[k_{fu}(\mathbf x)\right]_{\ell,m}$ as:
\begin{equation}\label{eq:kuf}
    % \left[k_{fu}(\mathbf x)\right]_{\ell,m} = \mathbb E\left[f(\mathbf{x})\uml\right] = \langle k(\mathbf{x}, \cdot), \Yml \rangle_\mathcal{H} = \Yml(\mathbf{x})\,.
    \textrm{cov}\left[f(\mathbf x), \uml\right] = \mathbb E\left[f(\mathbf{x})\uml\right] = \langle k(\mathbf{x}, \cdot), \Yml \rangle_\mathcal{H} = \Yml(\mathbf{x})\,.
\end{equation}
Similarly for the covariance between the inducing features they obtain:
\begin{align}\label{eq:kuu}
    % \left[\mathbf{K}_{uu}\right]_{\ell,m,\ell^\prime,m^\prime} =
    \textrm{cov}\left[\uml, u_{\ell^\prime}^{m^\prime}\right] =
    \mathbb E\left[\uml u_{\ell^\prime}^{m^\prime}\right] =
    \langle \langle k(\cdot, \cdot), \Yml \rangle_\mathcal{H}, \Ymlp \rangle_\mathcal{H} = 
    \langle\Yml, \Ymlp \rangle_\mathcal{H} = 
    \frac{\delta_{\ell\ell^\prime}\delta_{mm^\prime}}{\lambda_\ell}\,,
\end{align}
which admits a diagonal structure for the kernel matrix $\mathbf{K}_{uu}$.

Plugging Eqs.~\eqref{eq:kuf},\eqref{eq:kuu} into the variational posterior from~\citep{hensman2013gaussian} leads to the approximation
\begin{align}
    q(f) = \mathcal{GP}\left(
    \mathbf{\Phi}^\top(\cdot)\mathbf{m},
    k(\cdot,\cdot) + \mathbf{\Phi}^\top(\cdot)(\mathbf{S} - \mathbf{K}_{uu})\mathbf{\Phi}(\cdot)
    \right)\,,
\end{align}
where $\mathbf{\Phi}(\cdot) = \{\lambda_\ell \Yml(\cdot)\}_{\ell,m}$ and $\mathbf{m}, \mathbf{S}$ are the mean and variance, respectively, of the variational distribution $q(\mathbf{u}) = \mathcal{N}(\mathbf m, \mathbf S)$.

To learn the model, one needs to optimise the evidence lower bound (ELBO) wrt the variational parameters and the kernel hyper-parameters
\begin{align}
    \textrm{ELBO} = \sum_{i=1}^N \mathbb E_{q(f)}\left[\log p(y_i\given f(\mathbf{x}_i)) \right]
     - \KL\left[q(\mathbf{u})\,||\,p(\mathbf{u})\right]\,,
\end{align}
where $\mathbf{y}$ is the output of the function we are trying to model, $p(y_i | f(\mathbf{x}_i))$ is the likelihood of choice and, $p(\mathbf{u}) = \mathcal{N}(\zero, \mathbf{K}_{uu})$ is the sparse GP prior. 

\subsection{Sparse features with phase truncation}
From a practical perspective, working with the method from~\citet{dutordoir2020sparse} requires to pick a truncation level $\hat{\ell}$ for the order/frequency of the spherical harmonics.
Then a set of points $\mathbf{V} = \{ \mathbf{v}_{\ell,m}\}_{\ell, m}, \mathbf{v}_{\ell,m}\in\mathbb S^{d-1}$, is chosen on the sphere via a Gram-Schmidt orthogonalisation on $\mathbf{V}_\ell^\top \mathbf{V}_\ell$, so that the points $\mathbf{V}_\ell = \{\mathbf{v}_{\ell,m} \}_{m=1}^{\Nld}$ are maximally separated and form a complete fundamental set. Then, $\{C_\ell^{(\alpha)}(\mathbf{V}_\ell^\top \mathbf{V}_\ell) \}_{\ell=0}^{\hat{\ell}}$ corresponds to the full set of spherical harmonic features up to order $\hat{\ell}$.
The variables $\mathbf{V}$ play the role of the inducing inputs and are kept fixed throughout optimization, as they are already optimally placed.

Although efficient, this approach has two limitations. First, the number of inducing points $M$, which is the total number of phases across all the chosen frequencies, scales exponentially with the number of dimensions. Second, most of the high-frequency components are explicitly ignored due to the truncation at a lower frequency $\hat{\ell}$.

To alleviate this, we propose to introduce an extra truncation $\hat{m}$, this time at the phase level of the spherical harmonics.
So instead of using all the harmonics within each frequency, we settle for a smaller number of basis functions. % so that we can scale to higher frequencies.
This practically allows us to truncate the frequencies at a much higher order $\hat{\ell}$, which results in features that capture more high frequency characteristics of the function.

A direct consequence of our proposed approach is that the frequencies that have been truncated in phase do not constitute a set of spherical harmonics any more. We ensure, however, that they remain orthogonal polynomials by explicitly orthogonalizing the corresponding $\mathbf{V}_\ell$ within each truncated frequency. Furthermore, we now have the option to optimize the phases of the truncated frequencies, as the $\mathbf{v}_{\ell,m}$ are variational parameters in our ELBO.

We call the features learned by this two-way truncation as {\em sparse} spherical harmonic features.

