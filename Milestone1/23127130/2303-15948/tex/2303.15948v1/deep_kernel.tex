By definition, any function that can be factorised into a radial and an angular component,
must be acting on a hyper-sphere, i.e., it is a spherical function. For instance, let us take the typical Relu function defined as $\sigma_{\textrm{relu}}(\mathbf{x}^\top\mathbf{w}) = \max(0, \mathbf{x}^\top\mathbf{w})$, for some $\mathbf{w} \sim \mathcal N(\zero, \mathbf{I})$, $\mathbf{x} \in \mathbb R^d$. If we project the vectors onto the unit sphere $\mathbb S^{d-1}$ by normalising them, we can easily see that the Relu function is indeed spherical:
\begin{equation}
    \sigma_{\textrm{relu}}(\mathbf{x}^\top\mathbf{w}) = \|\mathbf{x}\| \|\mathbf{w}\|\max(0, \frac{\mathbf{x}^\top\mathbf{w}}{\|\mathbf{x}\| \|\mathbf{w}\|}) = \underbrace{\|\mathbf{x}\| \|\mathbf{w}\|}_{\textrm{radial}} \underbrace{\sigma_{\textrm{relu}}(\frac{\mathbf{x}^\top\mathbf{w}}{\|\mathbf{x}\| \|\mathbf{w}\|})}_{\textrm{angular}}\,.
\end{equation}

Based on this observation, \citet{cho2009kernel} studied the limit of infinite wide fully connected neural networks when the activation is a Relu function.
They found that the equivalent kernel takes the form of:
\begin{align}\label{eq:k_relu}
    k(\mathbf{x}, \mathbf{x}^\prime) = \mathbb E_\mathbf{w} \left[\sigma_{\textrm{relu}}(\mathbf{w}^\top\mathbf{x})\sigma_{\textrm{relu}}(\mathbf{w}^\top\mathbf{x}^\prime)\right]
    = \underbrace{\|\mathbf{x}\| \|\mathbf{x}^\prime\|}_{\textrm{radial}}
    \underbrace{\frac{1}{\pi}\left(t\left(\pi - \textrm{arccos}(t)\right) + \sqrt{1 - t^2}\right)}_{\textrm{angular}, \kappa(t)}\,,
\end{align}
with $t = \frac{\mathbf{x}^\top\mathbf{x}^\prime}{\|\mathbf{x}\| \|\mathbf{x}^\prime\|}$ and $\kappa(t)$ the {\em shape function} of the kernel.
We see that the above kernel is a bi-zonal function (zonal in either $\mathbf{x}$ or $\mathbf{x}'$).
This is an improtant observation, since zonal functions enjoy a particular relation with spherical harmonics, as we shall see in the next section.

\paragraph{Equivalent kernel of a deep network.} The above result can be extended to derive the equivalent kernel of a deep fully connected network with more than two layers.
Allowing the layers to be wide enough, i.e., infinite width we end up with the not
surprising result of:
\begin{equation}
    \kappa^L(t) \coloneqq \underbrace{\kappa \circ \cdots \circ \kappa}_{L-1 \textrm{ times}}(t).
\end{equation}
All we left to do is to rescale the shape function by multiplying with the appropriate radii so we have the final form of the equivalent kernel. Note that a good practice is to normalise $\kappa(1) = 1$, so that we also have $\kappa^L(1) = 1$.
