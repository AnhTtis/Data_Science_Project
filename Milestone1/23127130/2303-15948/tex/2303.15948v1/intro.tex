Deep neural networks (DNNs) have become a go-to methodology for modeling functional relationships from data, $y = f(x) +
\epsilon$. The ability to
train massive networks on structured data has led to impressive results on language modeling and computer vision tasks.
Meanwhile, scientists in other fields seek reliable function approximators with statistical uncertainty quantification,
and in these applications Gaussian process (GPs) models have become the standard. Is there a way to get the best of both? Can
we take some of the model-structure and scalability that have made DNNs so popular, and embed it into a Gaussian process
model? 

Recent works \citep{dutordoir2020sparse, dutordoir2021deep} have made use of spherical harmonic analysis to
draw connections between Gaussian process methods and deep networks. Those authors introduced a variational inference
method where spherical harmonics form the basis of an approximation to the Gaussian process posterior.
Since the
spherical harmonics form a Karhunen-Loeve expansion of the Gaussian process, several matrix computations are reduced in
complexity due to analytic diagonalization. This methodology forms the basis of the current manuscript and we give an
introduction in section \ref{sec:variational_intro}. 

Meanwhile, other authors have explored the connections between the limits of very large DNNs and Gaussian
processes. In his seminal thesis, \citet{neal1996priors} showed that a single-layer neural network with a Gaussian prior on
the weights becomes a Gaussian process as the size of the network grows to infinity. \citet{cho2009kernel} introduced deep
kernels by allegory with deep networks, and \citet{matthews2018gaussian} showed that these kernels do indeed correspond to
multi-layer neural networks with Gaussian process priors. They also showed that in practical settings, the Gaussian
process limit arises rather quickly, with networks of width of order 100 behaving as Gaussian processes. 

Further work has demonstrated that Gaussian process behavior of deep networks remains when the network is trained
using gradient descent. \citet{jacot2018neural} introduced the neural tangent kernel (NTK), which describes how a
trained neural network exhibits Gaussian process behavior in the large-width regime. \citet{yang2020tensor} devised a
systematic method to compute such a kernel corresponding to a large number of neural network architectures, and
\citep{garriga2018deep} examined the case of convolutional residual networks. A common theme in these works is that
whilst Gaussian-process equivalents to large networks exist, they are often expensive to compute. For example in
\citet{bietti2021approximation}, constructing the kernel matrix for an image-recognition task took 10 hours on a
1000-core machine. 

This work brings together the computational method from \citet{dutordoir2020sparse} with some of the advances in understanding the (kernels of) Gaussian
processes connection to large DNNs. We first review some connections between RKHS eigenstructures and deep model
structures; we clarify how the corresponding RKHS gives rise to polynomial kernels with spherical harmonics as the orthogonal basis;
we introduce kernels of continuous depth so that depth may be estimated as a kernel hyper-parameter;
and we introduce variational learning of spherical-harmonic phases, which enables scaling to larger input dimensions than
previously.

We demonstrate our combined methods on machine learning benchmark datasets.


