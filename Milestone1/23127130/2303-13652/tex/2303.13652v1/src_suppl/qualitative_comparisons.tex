\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth]{fig/qualitative_comparison_2.pdf}
\end{center}
\vspace*{-5mm}
\caption{
Qualitative comparison between our InterWild and IntagHand~\cite{li2022interacting} on MSCOCO.
Ours detects hand boxes from the human images, while IntagHand takes the hand images using GT boxes.
}
\label{fig:qualitative_comparison_2}
\end{figure*}

\section{Qualitative comparisons}

Fig.~\ref{fig:qualitative_comparison_2} shows that ours produces much more robust results than IntagHand~\cite{li2022interacting} on in-the-wild images.
Overall, IntagHand produces reasonable 3D hand mesh for a visible hand but fails to recover the other occluded hand.
Also, it suffers from depth ambiguity as the first and fourth rows show, where the 2D error is small but the 3D error is large.
The fourth row also shows that IntagHand fails to recover 3D relative translation between two hands due to the depth ambiguity, while ours successfully recovers.
Finally, we think the reason why IntagHand produces non-hand shape meshes is that IntagHand directly regresses the 3D coordinates of 3D hand meshes.
On the other hand, ours regresses MANO parameters and 3D meshes are obtained by forwarding the parameters to the MANO layer.