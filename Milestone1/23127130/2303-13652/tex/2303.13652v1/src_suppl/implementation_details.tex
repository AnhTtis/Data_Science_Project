\section{Implementation details} 

PyTorch~\cite{paszke2017automatic} is used for implementation. 
For the training, we use Adam optimizer~\cite{kingma2014adam} with a mini-batch size of 64.
Data augmentations, including scaling, rotation, random horizontal flip, and color jittering, are performed during the training.
The initial learning rate is set to $10^{-4}$ and reduced by a factor of 10 at the \nth{4} epoch.
All other details will be available in our codes.