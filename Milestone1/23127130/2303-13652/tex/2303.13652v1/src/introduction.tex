\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{fig/motivation.pdf}
\end{center}
\vspace*{-5mm}
\caption{
(a) Only a very small amount of 2D interacting hands are available in ITW datasets despite a relaxed threshold of intersection-over-union (IoU).
We consider two hands are interacting if the IoU between the two hands' boxes is bigger than 0.1.
(b) The 2D-based weak supervision from ITW datasets often results in a wrong 3D relative translation (\textcolor{orange}{\textbf{the orange arrow}}).
}
\vspace*{-3mm}
\label{fig:motivation}
\end{figure}


\section{Introduction}

3D interacting hands recovery aims to reconstruct a single person's interacting right and left hands in the 3D space.
The recent introduction of a large-scale motion capture (MoCap) dataset~\cite{moon2020interhand2} motivated many 3D interacting hands recovery methods~\cite{zhang2021interacting,rong2021monocular,li2022interacting,di2022lwa,hampali2022keypoint}.


Although they have shown robust results on MoCap datasets, none of them explicitly tackled robustness on in-the-wild (ITW) datasets.
Simply training networks on MoCap datasets and testing them on ITW datasets results in unstable results due to a huge domain gap between MoCap and ITW datasets.
The most representative domain gap is an appearance gap.
For example, images in InterHand2.6M~\cite{moon2020interhand2} (IH2.6M) have black backgrounds and artificial illuminations, far from those of ITW datasets.
The fundamental solution for this is collecting large-scale ITW data with 3D groundtruths (GTs); however, this is extremely challenging.
For example, capturing 3D data requires tens of calibrated and synchronized cameras.
Preparing such a setup at diverse places in the wild requires a huge amount of manual effort.
Furthermore, collecting even large-scale ITW 2D interacting hand data with manual annotation is greatly challenging due to the severe occlusions and self-similarities.
Due to such challenges, there is no large-scale ITW 2D/3D interacting hand dataset.



Nevertheless, ITW datasets provide large-scale 2D \emph{single-hand} data, as shown in Fig.~\ref{fig:motivation} (a).
Utilizing such large-scale 2D single-hand data of ITW datasets can be an orthogonal research direction to the 2D/3D interacting hands data collection in the wild.
Mixed-batch training is the most dominant approach to utilize 2D data of ITW datasets for the 3D human recovery~\cite{kolotouros2019learning,moon2020i2l,rong2021frankmocap,kocabas2021pare,moon2022hand4whole,choi2022learning}.
During the mixed-batch training, half of the samples in a mini-batch are taken from MoCap datasets and the rest of the samples from ITW datasets.
The MoCap samples are fully supervised with 3D GTs, and the ITW samples are weakly supervised with 2D GTs.
The ITW samples make networks exposed to diverse appearances, which leads to successful generalization to unseen ITW images.
The 2D-based weak supervision is enabled by the MANO~\cite{romero2017embodied} hand model, which produces a 3D hand mesh from pose and shape parameters in a differentiable way.
To be specific, 3D joint coordinates, extracted from the 3D mesh, are projected to the 2D space using an estimated 3D global translation (\textit{i.e.}, 3D translation from the camera to the hand root joint) and fixed virtual camera intrinsics.
Then, the projected 2D joint coordinates are supervised with the 2D GTs.
In this way, the 2D GTs weakly supervise MANO parameters, which can make all vertices of the 3D mesh fit to the 2D GTs.


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig/shnet_compare_prev.pdf}
\end{center}
\vspace*{-5mm}
\caption{
Mini-batch comparison for the first sub-problem (\textit{i.e.}, estimation of separate 3D meshes of left and right hands).
}
\vspace*{-3mm}
\label{fig:shnet_compare_prev}
\end{figure}


However, naively re-training networks of
 previous 3D interacting hands recovery methods~\cite{zhang2021interacting,rong2021monocular,li2022interacting,di2022lwa,hampali2022keypoint} with the mixed-batch training does not result in robust results.
3D interacting hands mesh recovery consists of two sub-problems: 1) estimation of separate 3D right and left hands and 2) estimation of 3D relative translation between two hands.
For the first sub-problem, previous works~\cite{zhang2021interacting,rong2021monocular,li2022interacting,di2022lwa,hampali2022keypoint} take an image of two hands when hands are interacting, and an image of a single hand when hands are not interacting (Fig.~\ref{fig:shnet_compare_prev} (a)).
As ITW datasets mostly contain single-hand data (Fig.~\ref{fig:motivation} (a)), most samples from ITW datasets contain a single hand during the mixed-batch training.
The problem is that the images of two hands from MoCap datasets have very different 2D hand scale distribution compared to that of single-hand images from ITW datasets, as shown in Fig.~\ref{fig:sh_ih_dist_compare}.
For example, when two hands are included in the input image, the 2D scale of each hand is much smaller than that from a cropped image of a single hand.



Unlike the first sub-problem, the second sub-problem hardly gets benefits from the 2D-based weak supervision from ITW datasets.
Fig.~\ref{fig:motivation} (b) shows the failure case of the 2D-based weak supervision.
When the 3D global translation, estimated for the 2D-based weak supervision, is wrong (\textcolor{orange}{\textbf{\textcircled{\raisebox{-0.9pt}{1}}}} in the figure), the 3D relative translation is supervised to be wrong one (\textcolor{orange}{\textbf{the orange arrow}} in the figure).
The wrong 3D global translation also can happen to the first sub-problem; however, the critical difference is that the 3D scale of the 3D relative translation (\textit{i.e.}, output of the second sub-problem) is very weakly constrained, while the 3D scale of hands (\textit{i.e.}, output of the first sub-problem) are strongly constrained by the shape parameter of MANO~\cite{romero2017embodied}.
For example, we can place two hands close or far freely based on how they are interacting with each other, while the size of adults' hands is usually around 15 cm.
As there is no such strong constraint to the relative translation, the relative translation can be an arbitrary value and is prone to wrong supervision (\textcolor{orange}{\textbf{the orange arrow}} in the figure).
Please note that estimating a 3D global translation from a single image involves a high ambiguity and can be often wrong as the camera position is not provided in the input image.
In this regard, we observed that the 2D-based weak supervision for the 3D relative translation, used in IntagHand~\cite{li2022interacting} (Fig.~\ref{fig:transnet_compare_prev} (a)) deteriorates results, which is shown in the experimental section.
However, without the 2D-based weak supervision of ITW datasets, the network is trained only on images of MoCap datasets like previous works~\cite{zhang2021interacting,rong2021monocular,di2022lwa,hampali2022keypoint} (Fig.~\ref{fig:transnet_compare_prev} (a)), which results in generalization failure due to the appearance gap between MoCap and ITW datasets.


\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{fig/transnet_compare_prev.pdf}
\end{center}
\vspace*{-5mm}
\caption{
Mini-batch comparison for the second sub-problem (\textit{i.e.}, estimation of 3D relative translation between two hands).
}
\vspace*{-3mm}
\label{fig:transnet_compare_prev}
\end{figure}


We present InterWild, a framework for 3D interacting hands mesh recovery in the wild.
For the first sub-problem, InterWild takes a cropped single-hand image regardless of whether two hands are interacting or not, as shown in Fig.~\ref{fig:shnet_compare_prev} (b).
In this way, the 2D scales of interacting hands are normalized to those of a single hand.
Such normalization brings all single and interacting hands to the shared 2D scale space; hence, large-scale single-hand data of ITW datasets can be much more helpful compared to the counterpart without the normalization.


For the second sub-problem, InterWild takes geometric features without images, as shown in Fig.~\ref{fig:transnet_compare_prev} (b).
In particular, the output of the second sub-problem (\textit{i.e.}, 3D relative translation) is fully supervised only for MoCap samples to prevent the 2D-based weak supervision of ITW samples from deteriorating the 3D relative translation.
The geometric features are invariant to appearances, such as colors and illuminations, which can reduce the huge appearance gap between MoCap and ITW datasets and bring samples from two datasets to a shared appearance-invariant space.
Therefore, although the estimated 3D relative translation is supervised only on MoCap datasets and is not supervised on ITW datasets, our InterWild produces robust 3D relative translations on ITW datasets.



We show that our InterWild produces highly robust 3D interacting hand meshes from ITW images.
As 3D interacting hands recovery in the wild is barely studied, we hope that ours can give useful insight into future works.
For the continual study, we released our codes and trained models.


Our contributions can be summarized as follows.
\begin{itemize}
\item We present InterWild, a framework for the 3D interacting hands recovery in the wild.
\item For the separate left and right 3D hands, InterWild takes a cropped single-hand image regardless of whether hands are interacting or not so that all hands are brought to a shared 2D scale space.
\item For the 3D relative translation between two hands, InterWild takes only geometric features, which are invariant to appearances.
\end{itemize}



