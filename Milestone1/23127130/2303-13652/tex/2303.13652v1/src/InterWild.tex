


\section{InterWild}
Fig.~\ref{fig:overall_pipeline} shows the overall pipeline of our InterWild, which consists of DetectNet, SHNet, and TransNet.
DetectNet detects hands from the input image.
Then, SHNet, a network for a single hand, takes each detected hand image as an input and outputs 3D mesh and 2.5D pose of each hand.
The 2.5D poses of the right and left hands are passed to TransNet, which outputs 3D relative translation between two hands.
The final 3D interacting hands are obtained by adding the 3D relative translation to the 3D mesh of the left hand.
DetectNet and SHNet follow architectures of Pose2Pose~\cite{moon2022hand4whole}.
Please refer to the supplementary material for their detailed architectures.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.7\linewidth]{fig/scale_dist_compare.pdf}
\end{center}
\vspace*{-7mm}
\caption{
Average of each hand's width and height, where the width and height are normalized with the size of the input image.
We extended all hands' boxes to set their aspect ratio to 1 before calculating the scales.
\textcolor[RGB]{242,174,114}{Yellow}: Each hand in the two-hand image of IH2.6M~\cite{moon2020interhand2} when hands are interacting (Previous approach).
\textcolor[RGB]{217,100,89}{Brown}: Each hand in the single-hand image of IH2.6M~\cite{moon2020interhand2} when hands are interacting (Ours).
\textcolor[RGB]{30,216,139}{Green}: Each hand in the single-hand image of MSCOCO~\cite{lin2014microsoft}. 
}
\vspace*{-3mm}
\label{fig:sh_ih_dist_compare}
\end{figure}


\subsection{SHNet}

\noindent\textbf{Input: an image of a single hand.}
SHNet takes a single-hand image regardless of whether two hands are interacting or not, while previous methods take images with two hands when hands are interacting, as shown in Fig.~\ref{fig:shnet_compare_prev}.
Hence, 2D scales of interacting hands are normalized to those of a single hand.
Fig.~\ref{fig:sh_ih_dist_compare} shows that when we crop images to contain two hands when hands are interacting (\textit{i.e.}, previous methods. \textcolor[RGB]{242,174,114}{Yellow} in the figure), 2D scales of each hand in two-hand images have a very different distribution compared to those of each hand in single-hand images (\textcolor[RGB]{30,216,139}{Green} in the figure).
On the other hand, when we crop images to contain a single hand regardless of whether hands are interacting (\textit{i.e.}, ours. \textcolor[RGB]{217,100,89}{Brown} in the figure), 2D scales of each hand in two-hand images have almost the same distribution compared to those of each hand in single-hand images (\textcolor[RGB]{30,216,139}{Green} in the figure).
Such an analysis justifies our design of SHNet to take a single cropped hand.


The single-hand image is cropped and resized from the high-resolution human image using predicted boxes from the DetectNet.
Before cropping the hands, we double the width and height of boxes to prevent hands from missing and provide more surrounding context to SHNet.
The left hand image is horizontally flipped to the right hand; therefore, the input image always represents a right hand image.
The right hand and flipped left hand images are concatenated in the batch dimension and processed in a parallel way by the SHNet.
By taking the right hand and flipped left hand images, SHNet can focus only on learning to process right hand images, which can relieve the burdens of learning to process both right and left hand images.
Also, such flipping is helpful when the two hands are severely interacting so that boxes of two hands are largely overlapped.
For example, let us imagine that most of the left hand is occluded by the right hand.
Then, images from the left and right hand boxes would contain almost the same right hand.
By flipping the left hand image, the right hand in the original left hand image changes to the left hand.
We train SHNet to ignore the left hand in the input image and produce a 3D hand mesh of only the right hand in the input image.
Therefore, the output from the flipped left hand image is a 3D hand mesh of the occluded right hand, which is originally the occluded left hand.
The effectiveness of this flipping is shown in the experimental section.


\noindent\textbf{Output: 3D mesh and 2.5D pose of each hand.}
Using the network architecture of Pose2Pose~\cite{moon2022hand4whole}, our SHNet outputs 3D mesh and 2.5D pose~\cite{sun2018integral} of each hand.
We flip back the outputs of the flipped left hand image.
We denote the 3D mesh of left and right hands by $\mathbf{M}_\text{l}$ and $\mathbf{M}_\text{r}$, respectively.
Each 3D mesh is obtained by forwarding the estimated pose and shape parameters to a MANO~\cite{romero2017embodied} layer.
We subtract 3D meshes from their 3D root joint locations so that the 3D meshes are in the root joint-relative space.
In addition, we denote the 2.5D pose of left and right hands by $\mathbf{P}_\text{l} \in \mathbb{R}^{J \times 3}$ and $\mathbf{P}_\text{r} \in \mathbb{R}^{J \times 3}$, respectively.
$J$ indicates the number of single-hand joints.
The 2.5D pose encodes hand joint locations in 2.5D space.
The $x$- and $y$-axis of the $j$th 2.5D pose represent pixel coordinates of the $j$th joint, where the pixel space is defined in the input image of SHNet (\textit{i.e.}, single-hand image).
The $z$-axis is defined in the root joint-relative depth space.


\subsection{TransNet}~\label{subsec:transnet}
Fig.~\ref{fig:transnet} shows the overall pipeline of TransNet, a network to predict 3D relative translation between two hands.


\noindent\textbf{Input: 2.5D poses of two hands.}
TransNet takes 2.5D poses of two hands, while previous methods take images with two hands, as shown in Fig.~\ref{fig:transnet_compare_prev}.
The 2.5D poses of two hands are from SHNet, which are denoted by $\mathbf{P}_\text{r}$ and $\mathbf{P}_\text{l}$.
Before forwarding them, we apply 2D affine transformations to $\mathbf{P}_\text{r}$ and $\mathbf{P}_\text{l}$, which transform the input space of SHNet (\textit{i.e.}, an image of a single hand) to a union of two-hand boxes space (\textit{i.e.}, an image of two hands).
By warping them to the union hand box space, we can get a relative 2D scale and translation between two hands in the 2D pixel space.
Based on such relative 2D information and pose information, TransNet predicts the 3D relative translation.


For example, when $xy$ distance of two hands' 2.5D pose is small, $(x,y)$ of the 3D relative translation are close to zero.
Also, when one hand takes smaller area in input $xy$ space, that hand might have larger depth; however, not always true as hands are deformable.
When a hand is in neutral pose and the other one is in fist pose, their 3D relative depth can be zero although the hand with fist pose takes smaller area.
Hence, pose is necessary to determine $z$ of the 3D relative translation.
Please note that the 2D affine transformations do not affect the depths of each 2.5D pose; hence, the depths of each 2.5D pose still represent the root joint-relative depths of each hand.
We denote the transformed $\mathbf{P}_\text{r}$ and $\mathbf{P}_\text{l}$ by $\mathbf{P}_\text{r}'$ and $\mathbf{P}_\text{l}'$, respectively.


The 2.5D pose of the right hand $\mathbf{P}_\text{r}'$ and left hand $\mathbf{P}_\text{l}'$ are converted to 2.5D Gaussian heatmaps by making a Gaussian blob around the coordinates.
By converting coordinates to heatmaps, we can exploit the strong feature extraction power of ResNet~\cite{he2016deep} as ResNet takes tensor inputs, not vector inputs.
Then, we concatenate the 2.5D Gaussian heatmap of two hands in a channel dimension, denoted by $\mathbf{H} \in \mathbb{R}^{2J \times D \times H \times W}$.
$D$, $H$, and $W$ represent the depth, height, and width of the 2.5D heatmap, respectively, and we set them to 64.


\noindent\textbf{Output: 3D relative translation between two hands.}
We predict the 3D relative translation between two hands $\mathbf{t} \in \mathbb{R}^3$ from the 2.5D Gaussian heatmap $\mathbf{H}$.
We pass $\mathbf{H}$ to ResNet-18~\cite{he2016deep}, which produces a feature map $\mathbf{F} \in \mathbb{R}^{C \times H/8 \times W/8}$.
$C=512$ represents the channel dimension of $\mathbf{F}$.
We use the original ResNet-18 after dropping the first convolutional block to reduce the downsampling and the last fully-connected layers.
As the 3D relative translation represents a 3D relative location of the left wrist from the right wrist, extracting useful wrist information is a key for accurate 3D relative translation.
However, most existing methods~\cite{moon2020interhand2,rong2021monocular,fan2021learning,zhang2021interacting} perform global average pooling (GAP) to the last feature map of backbones and pass the output to several fully connected layers.
As GAP simply averages the spatial dimension, it might not be effective to capture useful wrist information.
Instead, we perform a bilinear interpolation at the 2D positions of left and right wrists in $\mathbf{F}$, where the 2D wrist positions are from $\mathbf{P}_\text{r}'$ and $\mathbf{P}_\text{l}'$.
The extracted wrist features are concatenated with the 2D wrist coordinates and fed to a linear layer, which finally produces 3D relative translation $\mathbf{t}$.
The effectiveness of our wrist feature extraction compared to previous GAP-based approaches is shown in the experimental section.



\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{fig/transnet.pdf}
\end{center}
\vspace*{-7mm}
\caption{
The overall pipeline of TransNet.
It applies a 2D affine transformation to the 2.5D pose of each left and right hand to bring them to the union hand box space of the original input image.
Then, wrist features are extracted for the 3D relative translation estimation.
}
\vspace*{-3mm}
\label{fig:transnet}
\end{figure}


\subsection{Final outputs and loss functions}~\label{subsec:loss}
The final 3D interacting hand meshes consist of 1) the 3D mesh of the right hand $\mathbf{M}_\text{r}$ and 2) a summation of the 3D mesh of the left hand $\mathbf{M}_\text{l}$ and the 3D relative translation $\mathbf{t}$.
We train InterWild in an end-to-end manner by minimizing $L1$ distance between predicted and GT boxes, MANO parameters, 3D joint coordinates, and 3D relative translation.
Please note that the 3D relative translation is only supervised by MoCap datasets as ITW datasets do not provide GTs.





