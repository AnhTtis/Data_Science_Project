% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{cite}
\usepackage{ctable}
\usepackage{hhline}
\usepackage{booktabs} 
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{pifont}
\usepackage[super]{nth}
\usepackage{wrapfig}
\usepackage{enumitem}

\definecolor{darkergreen}{RGB}{21, 152, 56}
\definecolor{red2}{RGB}{252, 54, 65}
\newcommand{\cmark}{\textcolor{darkergreen}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{red2}{\ding{55}}}%

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% Recommendation from CVPR2023
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Bringing Inputs to Shared Domains for \\ 3D Interacting Hands Recovery in the Wild}

\author{Gyeongsik Moon\\
Meta Reality Labs \\
{\tt\small mks0601@gmail.com}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Despite recent achievements, existing 3D interacting hands recovery methods have shown results mainly on motion capture (MoCap) environments, not on in-the-wild (ITW) ones.
This is because collecting 3D interacting hands data in the wild is extremely challenging, even for the 2D data.
We present InterWild, which brings MoCap and ITW samples to shared domains for robust 3D interacting hands recovery in the wild with a limited amount of ITW 2D/3D interacting hands data.
3D interacting hands recovery consists of two sub-problems: 1) 3D recovery of each hand and 2) 3D relative translation recovery between two hands.
For the first sub-problem, we bring MoCap and ITW samples to a shared 2D scale space.
Although ITW datasets provide a limited amount of 2D/3D interacting hands, they contain large-scale 2D single hand data.
Motivated by this, we use a single hand image as an input for the first sub-problem regardless of whether two hands are interacting.
Hence, interacting hands of MoCap datasets are brought to the 2D scale space of single hands of ITW datasets.
For the second sub-problem, we bring MoCap and ITW samples to a shared appearance-invariant space.
Unlike the first sub-problem, 2D labels of ITW datasets are not helpful for the second sub-problem due to the 3D translation's ambiguity.
Hence, instead of relying on ITW samples, we amplify the generalizability of MoCap samples by taking only a geometric feature without an image as an input for the second sub-problem.
As the geometric feature is invariant to appearances, MoCap and ITW samples do not suffer from a huge appearance gap between the two datasets.
The code is publicly available\footnote{\url{https://github.com/facebookresearch/InterWild}}.
\end{abstract}

% main
\input{src/introduction}
\input{src/related_works}
\input{src/InterWild}
\input{src/experiment}
\input{src/conclusion}

% suppl
\input{src_suppl/main_suppl}
\input{src_suppl/qualitative_comparisons}
\input{src_suppl/transnet_architecture_ablation}
\input{src_suppl/reproduce}
\input{src_suppl/2d_weak_sup}
\input{src_suppl/detectnet_architecture}
\input{src_suppl/shnet_architecture}
\input{src_suppl/implementation_details}
\input{src_suppl/limitations}

\clearpage

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{bib}
}


\end{document}
