\section{Experiments}

\subsection{Datasets}

\noindent\textbf{Train sets.}
IH2.6M~\cite{moon2020interhand2} (H) and the whole-body version of MSOCO~\cite{lin2014microsoft,jin2020whole} are used for the training.
During the training, the mini-batch consists of half-IH2.6M and half-MSCOCO samples.


\noindent\textbf{Test sets.}
As our primary goal is 3D interacting hand mesh recovery \emph{in the wild}, we use Hands In Action dataset (HIC)~\cite{tzionas2016capturing} as our main test set.
HIC~\cite{tzionas2016capturing} contains single and interacting hand sequences captured with an RGBD camera.
HIC provides 3D GT meshes of hands~\cite{hasson2019learning}, fitted to a 3D point cloud.
Although HIC is captured in an indoor environment, it contains images with much more diverse and realistic appearances compared to those of IH2.6M.
Also, as we do not use HIC during the training, its appearances are not exposed to the network; hence, we believe the test performance of networks on HIC represents generalizability to unseen appearances, necessary for the 3D interacting hand mesh recovery in the wild.
Additionally, we report errors on IH2.6M (H) as it is one of the representative datasets for the 3D interacting hand mesh recovery although it is a MoCap dataset.
Qualitative results are shown on MSCOCO, which is the most widely used ITW dataset due to its diverse appearances.



\subsection{Evaluation metrics}

\noindent\textbf{MPJPE and MPVPE.}
Mean per-joint position error (MPJPE) and mean per-vertex position error (MPVPE) evaluate 3D joint and mesh vertex positions, respectively.
It represents the average 3D joint and mesh vertex distance (mm) between the predicted and GT, after aligning those with a root joint translation.
MPJPE and MPVPE are used to measure 3D errors of 3D mesh of each hand.

\noindent\textbf{MRRPE.}
Mean relative-root position error (MRRPE) evaluates 3D relative translation between two hands.
It calculates a 3D distance (mm) between the predicted and GT right hand root-relative left hand root position.

\subsection{Ablation study}

\begin{table}[t]
\footnotesize
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\begin{tabular}{C{4.0cm}|C{2.0cm}|C{2.0cm}}
\specialrule{.1em}{.05em}{.05em}
Inputs of SHNet & HIC~\cite{tzionas2016capturing} & IH2.6M~\cite{moon2020interhand2} \\ \hline
Two-hand image & 29.80 / 35.86 &  11.36 / 13.20 \\ 
\textbf{Single-hand image (Ours)} & \textbf{15.65} / \textbf{15.70} & \textbf{11.12} / \textbf{13.01} \\ 
\specialrule{.1em}{.05em}{.05em}
\end{tabular}
\vspace*{-3mm}
\caption{MPVPE comparisons between SHNets that take an image of 1) two hands and 2) a single hand as an input when hands are interacting.
Both settings take a single hand image when hands are not interacting.
The left and right numbers for each setting represent errors from single and interacting hand sequences, respectively.}
\label{table:shnet_input}
\end{table}

\noindent\textbf{SHNet: Effectiveness of taking an image of a single hand.}
Table~\ref{table:shnet_input} shows that when hands are interacting, taking a single hand image (Fig.~\ref{fig:shnet_compare_prev} (b)) produces lower SHNet's errors compared to taking a two-hands image (Fig.~\ref{fig:shnet_compare_prev} (a)), especially on HIC.
This shows that our approach to taking a single hand image when hands are interacting is especially helpful in the wild.
The reason for our superior result is that when SHNet takes a single hand image, large-scale 3D interacting hand data of MoCap dataset~\cite{moon2020interhand2} and large-scale 2D single hand data of in-the-wild dataset~\cite{lin2014microsoft} are brought to a shared 2D scale space.
On the other hand, when taking images of two hands when hands are interacting like previous works~\cite{moon2020interhand2,rong2021monocular,zhang2021interacting,li2022interacting,fan2021learning,kim2021end,meng2022hdr}, the 2D scale of each hand has a very different distribution, as shown in Fig.~\ref{fig:sh_ih_dist_compare}; hence, learning to process inputs with a very different distribution can be a burden to SHNet.
The small gap of IH2.6M is because such a burden can be relieved by large-scale interacting hand datasets, such as IH2.6M.
We observed that reducing the number of IH2.6M's interacting hand samples makes the same tendency of HIC, which supports our claim.
However, collecting large-scale interacting hand data in the wild is greatly challenging; even collecting 2D data requires a huge amount of effort due to the severe self-similarity and occlusions.
We effectively relieve such data collection burden by bringing the two datasets to a shared 2D scale space.
Both settings take a single hand when hands are not interacting, and the first setting takes a two-hand image when hands are interacting.
For the setting that takes an image of two hands, we doubled the output part of SHNet to estimate both left and right hands at the same time.


\begin{table}[t]
\footnotesize
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\begin{tabular}{C{4.5cm}|C{1.7cm}|C{1.7cm}}
\specialrule{.1em}{.05em}{.05em}
Settings & HIC~\cite{tzionas2016capturing} & IH2.6M~\cite{moon2020interhand2} \\ \hline
Without flip / shared weights & 15.80 / 17.36 & 12.54 / 18.21 \\ 
Without flip / separated weights & \textbf{15.34} / 16.96 & 11.48 / 13.77 \\ 
\textbf{With flip / shared weights (Ours)} & 15.65 / \textbf{15.70} & \textbf{11.12} / \textbf{13.01} \\ 
\specialrule{.1em}{.05em}{.05em}
\end{tabular}
\vspace*{-3mm}
\caption{MPVPE comparisons of SHNets that take 1) left and right hands and 2) flipped left hand and right hand.
`Shared weights' indicates that SHNet's weights are shared for left and right hand inputs, while `separated weights' indicates that the weights are not shared. 
The left and right numbers for each setting represent errors from single and interacting hand sequences, respectively.}
\label{table:shnet_flip}
\end{table}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{fig/ablation_shnet_flip.pdf}
\end{center}
\vspace*{-7mm}
\caption{
Comparison between 2.5D hand pose from SHNets that take (a) an image of left and right hand images and (b) an image of \emph{flipped left hand} and right hand.
The result from the flipped left hand is flipped back.
}
\vspace*{-3mm}
\label{fig:ablation_shnet_flip}
\end{figure}

\noindent\textbf{SHNet: Effectiveness of flipping left hand images.}
Table~\ref{table:shnet_flip} shows that flipping the left hand is necessary for SHNet's accurate results when hands are interacting.
Without the flipping, SHNet can take almost the same single hand image when hands are severely overlapped, as shown in Fig.~\ref{fig:ablation_shnet_flip} (a).
As input images are almost the same, SHNet outputs almost the same two 3D hands for both left and right hand images.
On the other hand, when we pass a flipped left hand image, SHNet can differentiate the two images.
As SHNet is trained to ignore all left hands and recover only right hands, this can be seen as an \emph{implicit de-occlusion} of left hands from the input image.
Fig.~\ref{fig:ablation_shnet_flip} (b) shows that even when two hands are severely overlapped, our SHNet can recover correct 3D hands.


Instead of flipping, one can double the output part of SHNet and train SHNet to output left and right hands at the same time.
In this way, when the input hand images are almost the same due to the severe overlap, the output part of SHNet is trained to distinguish left and right hands.
However, we observed that our flipping results in better results than this variant.
We think this is because, in our setting, the handedness of the input image is normalized to the right hand, while the variant is not.
Such normalization can relieve the burden of SHNet.
Please note that a combination of `with flip' and `separated weights' is impossible as flipping normalizes the handedness.



\begin{table}[t]
\scriptsize
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\begin{tabular}{C{3.0cm}C{1.5cm}|C{1.5cm}|C{1.7cm}}
\specialrule{.1em}{.05em}{.05em}
Inputs of TransNet & weak sup. & HIC~\cite{tzionas2016capturing} & IH2.6M~\cite{moon2020interhand2} \\ \hline
\multirow{2}{*}{Img.} & \xmark & 206.83 &  27.67 \\ 
& \cmark & 215.35 & 35.72 \\ \hline
\multirow{2}{*}{Img. + 2.5D hm.} & \xmark & 54.36 & \textbf{27.19} \\
& \cmark & 58.53 & 33.15 \\ \hline
\multirow{2}{*}{2D hm.} & \xmark& 38.64 & 31.51 \\
& \cmark & 51.19 & 35.51 \\ \hline
\multirow{2}{*}{2.5D hm.} & \xmark \textbf{(Ours)} & \textbf{31.35} &  29.29 \\ 
& \cmark & 61.05 & 33.91 \\
\specialrule{.1em}{.05em}{.05em}
\end{tabular}
\vspace*{-3mm}
\caption{MRRPE comparisons of TransNets that take various inputs and are trained without and with the 2D-based weak supervision. The hm. represents a heatmap.}
\label{table:transnet_input_weak_sup}
\end{table}


\begin{table}[t]
\footnotesize
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\begin{tabular}{C{3.5cm}|C{2.0cm}|C{2.0cm}}
\specialrule{.1em}{.05em}{.05em}
Settings & HIC~\cite{tzionas2016capturing} & IH2.6M~\cite{moon2020interhand2} \\ \hline
GAP & 39.85 & \textbf{29.14}  \\ 
All joint features & 48.99 & 31.57 \\
\textbf{Wrist features (Ours)} & \textbf{31.35} &  29.29 \\ 
\specialrule{.1em}{.05em}{.05em}
\end{tabular}
\vspace*{-3mm}
\caption{MRRPE comparisons between TransNet that outputs the 3D relative translation with various feature extraction settings.}
\vspace*{-3mm}
\label{table:transnet_wrist_features}
\end{table}


\noindent\textbf{TransNet: Effectiveness of the geometric inputs.}
Table~\ref{table:transnet_input_weak_sup} shows that taking 2.5D heatmaps as an input of TransNet (Fig.~\ref{fig:transnet_compare_prev} (b)) achieves the lowest MRRPE on HIC and comparable results on IH2.6M.
Our 2.5D heatmap achieves better results than the 2D heatmap due to the additional depth information of each hand.
An interesting result is that using an image as an input (the first and second rows) achieves good results on IH2.6M, but bad results on HIC.
Such a setting with the image input is similar to previous methods~\cite{moon2020interhand2,rong2021monocular,zhang2021interacting,fan2021learning,li2022interacting}, while IntagHand~\cite{li2022interacting} additionally uses segmentation and DensePose.


There are two reasons for this setting's high MRRPE on HIC.
First, when the 2D-based weak supervision is disabled, the huge appearance gap between ITW and IH2.6M is the main reason.
Without the weak supervision, TransNet is supervised only on IH2.6M.
Images of MoCap datasets, including IH2.6M, have monotonous colors with artificial illuminations, which are far from those of ITW images.
On the other hand, we use pure geometric features (\textit{i.e.}, 2.5D heatmap) as it is invariant to appearances.
Thanks to the invariance, our TransNet successfully generalizes to ITW datasets although it is trained only on IH2.6M.
Second, when the 2D-based weak supervision is enabled, the weak supervision deteriorates the relative translation due to the scale ambiguity of the relative translation.
A detailed analysis of the 2D-based weak supervision is provided below.



\noindent\textbf{TransNet: Bad effect of the 2D-based weak supervision.}
Table~\ref{table:transnet_input_weak_sup} shows that introducing the 2D-based weak supervision for the estimation of the 3D relative translation between two hands, similar to IntagHand~\cite{li2022interacting}, deteriorates MRRPE for all inputs of TransNet and for both evaluation benchmarks.
This is because, unlike the 3D scales of hands that are strongly constrained with the shape parameter of MANO, 3D scales of the 3D relative translation are very weakly constrained.
For example, we can put two hands near or far based on how they are interacting with each other, while the size of adults' hands is usually around 15 cm.
Without such a strong constraint, the 3D relative translation can be an arbitrary value due to the wrong 3D global translation.
Fig.~\ref{fig:motivation} (b) shows that when the 3D global translation is wrong (\textcolor{orange}{\textbf{\textcircled{\raisebox{-0.9pt}{1}}}} in the figure), the 3D relative translation is supervised to be wrong one (\textcolor{orange}{\textbf{the orange arrow}} in the figure).
Please refer to the supplementary material for how we introduced the 2D-based weak supervision.




\begin{table}[t]
\footnotesize
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\begin{tabular}{C{2.4cm}|C{1.7cm}C{1.1cm}|C{1.7cm}C{1.1cm}}
\specialrule{.1em}{.05em}{.05em}
\multirow{2}{*}{Methods} & \multicolumn{2}{c|}{HIC~\cite{tzionas2016capturing}} & \multicolumn{2}{c}{IH2.6M~\cite{moon2020interhand2}} \\
& MPVPE & MRRPE & MPVPE & MRRPE \\ \hline
IHMR~\cite{rong2021monocular} & 30.76 / 46.38 & 119.64 & 15.35 / 18.53 & 33.39 \\
Zhang~\etal~\cite{zhang2021interacting} & 23.53 / 31.79 & 110.25 & 11.76 / 14.17 & 31.56 \\ 
IntagHand~\cite{li2022interacting} & 18.83 / 27.31 & 52.46 & 11.18 / 13.49 & 29.31 \\
\textbf{InterWild (Ours)} & \textbf{15.65} / \textbf{15.70} & \textbf{31.35} & \textbf{11.12} / \textbf{13.01} & \textbf{29.29} \\ \specialrule{.1em}{.05em}{.05em}
\end{tabular}
\vspace*{-3mm}
\caption{Comparison of our InterWild and 3D interacting hand mesh estimation methods.
The left and right numbers for each setting represent errors from single and interacting hand sequences, respectively.
}
\vspace*{-3mm}
\label{table:comparison_sota_mesh}
\end{table}


\begin{table}[t]
\footnotesize
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\begin{tabular}{C{2.5cm}|C{2.0cm}C{1.5cm}}
\specialrule{.1em}{.05em}{.05em}
Methods & MPJPE & MRRPE  \\ \hline
AIH~\cite{meng2022hdr} &  76.83 / 36.05 & N/A \\
\textbf{InterWild (Ours)} & \textbf{16.00} / \textbf{16.17} & \textbf{31.35} \\ \specialrule{.1em}{.05em}{.05em}
\end{tabular}
\vspace*{-3mm}
\caption{Comparison of our InterWild and 3D interacting hand pose estimation methods on HIC.
The left and right numbers for each setting represent errors from single and interacting hand sequences, respectively.
}
\vspace*{-5mm}
\label{table:comparison_sota_pose}
\end{table}


\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{fig/mpvpe_mrrpe_hic.pdf}
\end{center}
\vspace*{-7mm}
\caption{
Comparison with previous methods~\cite{rong2021monocular,zhang2021interacting,li2022interacting} on HIC.
For each $x$-axis value, denoted by $\tau$, MPVPE and MRRPE of $y$-axis are calculated from samples whose $x$-axis values are larger than $\tau$.
}
\vspace*{-3mm}
\label{fig:mpvpe_mrrpe_hic}
\end{figure}


\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.77\linewidth]{fig/qualitative_comparison.pdf}
\end{center}
\vspace*{-7mm}
\caption{
Qualitative comparison between our InterWild and IntagHand~\cite{li2022interacting} on MSCOCO validation set.
Ours detects hand boxes from the human images, while IntagHand takes the hand images using GT boxes.
}
\vspace*{-5mm}
\label{fig:qualitative_comparison}
\end{figure*}


\noindent\textbf{TransNet: Effectiveness of the wrist features.}
Table~\ref{table:transnet_wrist_features} shows that our wrist feature-based estimation of 3D relative translation achieves better results than the previous widely used GAP~\cite{moon2020interhand2,rong2021monocular,zhang2021interacting,fan2021learning}.
This is because GAP averages the entire spatial domain; therefore, its output is not guaranteed to contain essential wrist information, necessary for the 3D relative translation between two hands.
On the other hand, we explicitly extract wrist features, which produces better results.
Interestingly, using features of all joints performs worst.
We think that this is because features from all joints contain too much unnecessary information, which gives a burden to the regressor.



\subsection{Comparison with state-of-the-art methods}
Table~\ref{table:comparison_sota_mesh} and Fig.~\ref{fig:mpvpe_mrrpe_hic} show that ours outperforms all existing 3D interacting hand mesh recovery methods on HIC and IH2.6M datasets.
It is noteworthy that the MPVPE and MRRPE gap is especially large on HIC, which shows the robustness of InterWild to ITW environments.
This is because of our domain-sharing approach, depicted in Fig.~\ref{fig:shnet_compare_prev} and ~\ref{fig:transnet_compare_prev}.
Table~\ref{table:comparison_sota_pose} additionally demonstrates the superiority of our InterWild.
AIH~\cite{meng2022hdr} used additional synthetic datasets for the robust results on unseen appearances.
However, such synthetic datasets are still built on top of the IH2.6M dataset, which has a severe appearance gap from that of ITW datasets.
On the other hand, ours effectively reduces the domain gap by bringing inputs of SHNet and TransNet to shared domains, which results in a strong performance on ITW datasets.
Finally, Fig.~\ref{fig:qualitative_comparison} visually demonstrates that ours successfully recovers 3D meshes from ITW images, while previous state-of-the-art method~\cite{li2022interacting} fails to.




Publicly released models of previous works in Table~\ref{table:comparison_sota_mesh} are trained only on IH2.6M, and their training codes are not available.
Therefore, we reproduced their networks based on their testing codes and re-trained all of their networks on IH2.6M and MSCOCO like ours for a fair comparison.
We will verify our reproduce results in the supplementary material.
For the evaluation, we do not align the scale with GTs following Moon~\etal~\cite{moon2020interhand2}.
We found that Keypoint Transformer~\cite{hampali2022keypoint} produces bad results when MSCOCO is incorporated in the training set as it requires 3D GTs, which does not exist in MSCOCO, for the camera parameter loss function.
All previous methods use GT hand boxes during the inference following their settings, while ours uses predicted hand boxes from DetectNet.

