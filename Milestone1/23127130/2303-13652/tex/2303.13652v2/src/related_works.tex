\section{Related works}~\label{sec:related_works}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.8\linewidth]{fig/overall_pipeline.pdf}
\end{center}
\vspace*{-7mm}
\caption{
The overall pipeline of the proposed InterWild.
From the hand boxes, obtained by DetectNet, we crop and resize the hand area from the high-resolution human image.
Each right and left hand image is fed to SHNet, which produces 3D mesh and 2.5D heatmap.
Next, TransNet takes the 2.5D heatmap of two hands to produce the 3D relative translation between two hands.
Final 3D interacting hand meshes are obtained by adding the 3D relative translation to the left hand mesh.
For simplicity, we do not visualize DetectNet.
}
\vspace*{-3mm}
\label{fig:overall_pipeline}
\end{figure*}


\noindent\textbf{3D interacting hands recovery.}
Most of early works~\cite{oikonomidis2012tracking,ballan2012motion,tzionas2016capturing,taylor2016efficient,mueller2019real,wang2020rgb2hands} fit 3D hand models to geometric evidence, such as RGBD sequence~\cite{oikonomidis2012tracking}, hand segmentation map~\cite{mueller2019real}, and dense matching map~\cite{wang2020rgb2hands}.
Recently, Moon~\etal~\cite{moon2020interhand2,moon2022neuralannot} presented IH2.6M dataset, the first large-scale real-captured dataset that contains accurate GT 3D poses and meshes of interacting hands and a regression-based baseline model, InterNet.
Motivated by IH2.6M and InterNet, several regression-based methods have been proposed, which perform better than the above fitting-based methods.
Rong~\etal~\cite{rong2021monocular} proposed a two-stage framework to minimize collisions between two hands.
Zhang~\etal~\cite{zhang2021interacting} proposed a cascaded 3D interacting hand mesh estimation network, which sequentially refines 3D interacting hands.
Kwon~\etal~\cite{kwon2021h2o} presented a baseline for recovering 3D meshes of two hands while interacting with objects.
However, their system mostly focuses on the interaction between hands and objects, not between two hands.
Li~\etal~\cite{li2022interacting} proposed a graph convolutional network for accurate 3D interacting hand reconstruction.
Hampali~\etal~\cite{hampali2022keypoint} proposed a Transformer~\cite{vaswani2017attention}-based system that separates localization and identification of hand keypoints.
Di~\etal~\cite{di2022lwa} presented a lightweight system for 3D interacting hand mesh recovery.
In addition, there are several works~\cite{fan2021learning,kim2021end,meng2022hdr} that recover only 3D hand joint locations without 3D meshes.


The above 3D interacting hand reconstruction methods~\cite{moon2020interhand2,rong2021monocular,zhang2021interacting,li2022interacting,fan2021learning,kim2021end,meng2022hdr} fail to produce robust results on ITW datasets, while ours can.
There are two big differences.
First, they take a two-hand image as an input when two hands are interacting (Fig.~\ref{fig:shnet_compare_prev} (a)).
On the other hand, ours take a single-hand image regardless of whether two hands are interacting (Fig.~\ref{fig:shnet_compare_prev} (b)); hence, inputs are brought to the shared 2D scale space.
Second, they estimate 3D relative translation between two hands using an image with the 2D-based weak supervision (Fig.~\ref{fig:transnet_compare_prev} (a)).
On the other hand, ours estimates the relative translation only from geometric features (Fig.~\ref{fig:transnet_compare_prev} (b)), which are invariant to appearances, without the 2D-based weak supervision.




\noindent\textbf{Reducing appearance gap with geometric features.}
Several 3D body and hand mesh estimation methods have used geometric features to reduce the appearance gap between MoCap and ITW datasets.
Pose2Mesh~\cite{choi2020p2m} and Song~\etal~\cite{song2020human} take 2D body joint coordinates as an input to predict a 3D human body mesh.
Zhou~\etal~\cite{zhou2020monocular} predict a 3D single-hand mesh from 3D single-hand joint coordinates.
Zhang~\etal~\cite{zhang2020learning} utilizes body part UVI map for the 3D body mesh recovery.
Our InterWild is the first work that estimates robust 3D relative translation between two hands from geometric features.

 






