\section{Architecture of DetectNet}

DetectNet detects left and right hands from an input image $\mathbf{I}_\text{det} \in \mathbb{R}^{3 \times H_\text{det} \times W_\text{det}}$, downsampled from a high-resolution image $\mathbf{I} \in \mathbb{R}^{3 \times 2H_\text{det} \times 2W_\text{det}}$, by predicting two bounding boxes of the left and right hands.
$H_\text{det}=256$ and $W_\text{det}=192$ denote height and width of $\mathbf{I}_\text{det}$, respectively.
The downsampling is necessary to save computational costs.
To this end, we extract the image feature from $\mathbf{I}_\text{det}$ using ResNet-50 and pass the feature to three consecutive deconvolutional layers, which upsample the feature map by 8 times.
We denote the upsampled feature map by $\mathbf{F}_\text{det} \in \mathbb{R}^{C_\text{det} \times H_\text{det}/4 \times W_\text{det}/4}$.
$C_\text{det}=256$ denotes the number of channel of $\mathbf{F}_\text{det}$.
We use the original ResNet-50 after dropping global average pooling (GAP) and following fully-connected layers.
Then, a 1-by-1 convolutional layer takes $\mathbf{F}_\text{det}$ and predicts a 2D heatmap of two hand bounding box centers.
Soft-argmax~\cite{sun2018integral} extracts 2D hand bounding box center coordinates from the 2D heatmap in a differentiable way.
Then, we extract bounding box center features of left and right hands by performing a bilinear interpolation at the box center positions of $\mathbf{F}_\text{det}$.
The extracted bounding box center features of each hand are passed to two fully-connected layers, which produce a scale of the bounding box.
By decoding the bounding box centers and scales, we obtain two bounding boxes of left and right hands.