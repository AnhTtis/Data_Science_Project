   
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage[noend]{algorithmic} 
\usepackage{algorithm,caption}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{comment}
\newcommand{\bigzero}{\mbox{\normalfont\Large\bfseries 0}}
\newcommand{\rvline}{\hspace*{-\arraycolsep}\vline\hspace*{-\arraycolsep}}

\newcommand\inner[2]{\langle #1, #2 \rangle}
\usepackage[english]{babel}
\newcommand{\ryan}[1]{\textcolor{blue}{~Ryan:~#1}}
\usepackage{comment}
%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}
\newcommand{\reg}{\mathcal{R}}
% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%https://www.overleaf.com/project/63538b66713c35098fa59f17
\addtolength{\topmargin}{-.8in}%

\usepackage{bm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}


\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Title}
  \author{Author 1\thanks{
    The authors gratefully acknowledge \textit{please remember to list all relevant funding sources in the unblinded version}}\hspace{.2cm}\\
    Department of YYY, University of XXX\\
    and \\
    Author 2 \\
    Department of ZZZ, University of WWW}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Title}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
The text of your abstract. 200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:}  3 to 6 keywords, that do not appear in the title
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!
\section{Introduction}\label{sec:intro}

Consider a very generic model first. We will later look at specific cases of this general model. Let us consider $i=1,\ldots,I$ customer segments over $t=1,\ldots,T$ time periods. For simplicity, think of the customer segments as zip-codes. But, note that there are several other economically important ways of making these segments. We assume that these segments are known beforehand. 

For an arbitrary segment $i$ at an arbitrary time point $t$, let $U_{itk}$ denote the utility of a random customer who has been given a price $p_{it}$ for the product that the firm is selling. Consider the following model: 

\begin{align}\label{eq:1}
U_{itk}=\alpha_{it}+\beta_{it}\,p_{it}+\bm{x}_{itk}' \bm{\mu}_{it} + \sigma Z_{itk}
\end{align}
where, $k=1,\ldots,n_{it}$.  

Here, we are assuming that the price sensitivity as well as the preferences of the customers varies over time and segments. The covariates of each customers $\bm{x}_{itk}$ are known but varies with different customers. In a lot of cases, we might only know these covariates aggregated at segment level and so, we can consider the simple model where $\bm{x}_{itk}=\bm{x}_{it}$. 

For each segment we have a different number of customers $n_{it}$. Note that $n_{it}$s (which are the number of replicates at each time and segment) are known but highly heterogeneous. This is an important aspect of the model as it results in imbalance across the design matrix. However, this is also practically very important as in ``pull" marketing systems unlike ``push" systems we have no control on the number of customers who visits the site/store and does a price check. 

If $U_{itk}>0$ then sale occurs and is represented by $Y_{itk}=1$; else, $Y_{itk}=0$.

Assume $Z_{itk}$ i.i.d. $N(0,1)$. 

\subsection{Spatial Structure}

Assume the following structure on the $\alpha_{it}$:
\begin{align}\label{eq:2}
 \alpha_{it}= \rho_t \sum_{j=1}^I w_{ij} \alpha_{jt} + \Tilde{\sigma}\epsilon_{it}
 \end{align}
where, $W$ is the interaction weight matrix. The weight matrix is assumed to be known and invariant across time. 

\textbf{Assumption 1:} $W$ is a symmetric, PSD kernel e.g. RBF kernel. 

\textbf{Assumption 2:} The interaction parameter $\rho_t$ for all time periods satisfies the condition $\rho_t \leq \frac{1}{\lambda_{\max}}$, where $\lambda_{\max}$ is the largest eigenvalue of the known interaction matrix $W$.

The structure on the $\alpha_{it}$'s in \eqref{eq:2} can be rewritten as
\begin{align*}
    &\alpha_t = \rho_t W \alpha_t + \Tilde{\sigma}\epsilon\\
    \implies & \alpha_t = (\bm I - \rho_t W)^{-1}\Tilde{\sigma}\epsilon\\
    \implies & \alpha_{it} = \Tilde{\sigma}\inner{e_i}{(\bm I - \rho_t W)^{-1}\epsilon}
\end{align*}

where $e_i$ denotes the $i^{th}$ basis vector. Substituting this $\alpha_{it}$ into the model \eqref{eq:1}, $U_{itk}$ is distributed as

\begin{align}
    & U_{itk} \sim \mathcal{N}(M_{it}, V_{it}^2)\\
    & M_{it} = \beta_{it}p_{it} + \bm x_{it}'\mu_{it}\\
    & V_{it}^2 = \|e_i (\bm I - \rho_t W)^{-1}\|^2\Tilde{\sigma}^2 + \sigma^2
\end{align}

We re parameterize the parameters $\beta_{it}$ and $\mu_{it}$ as $\Tilde{\beta}_{it} = \frac{\beta_{it}}{V_{it}}$ and $\Tilde{\mu}_{it} = \frac{\mu_{it}}{V_{it}}$. The gradient descent is then used to update these reparameterized $\Tilde{\beta}_{it}$ and $\Tilde{\mu}_{it}$. 

\subsection{Loss Function}
With the new parameters, observe that 
\[U_{itk} \sim \mathcal{N}(\Tilde{\beta}_{it}\,p_{it}+\bm{x}_{it}' \Tilde{\mu}_{it}, 1)\]

Hence, the log-likelihood at time $t$: 

$$\sum_{i=1}^I\bigg\{ y_{it} \log \Phi\big( \Tilde{\beta}_{it}\,p_{it}+\bm{x}_{it}' \Tilde{\mu}_{it}\big) + (n_{it}-y_{it}) \log \tilde \Phi\big( \Tilde{\beta}_{it}\,p_{it}+\bm{x}_{it}' \Tilde{\mu}_{it}\big)\bigg\} $$

where, $y_{it}=\sum_{k=1}^{n_{it}} Y_{itk}$, $\tilde \Phi= 1-\Phi$, $\bm{\alpha}_t=\{\alpha_{jt}: j=1,\ldots,I\}$.

The loss function $l_t$ at time $t$ is the negative log-likelihood function.

\subsection{Optimal Price}
Conditional on the parameters  $\Tilde{\mu}_{it}, \Tilde{\beta}_{it}$ the expected revenue for segment $i$ at time $t$ is,
\begin{align*}
    Rev_{it} &= p_{it}\bm{E}(y_{it}) = p_{it} n_{it}\bm{E}(Y_{itk})= n_{it} p_{it} \bm{P}(Y_{itk = 1})\\
    & = n_{it} p_{it} \Phi(\Tilde{\beta}_{it}\,p_{it}+\bm{x}_{it}' \Tilde{\mu}_{it})
\end{align*}

Since we are conditioning on all the parameters, each segment is independent and the revenue of every segment can be maximized independently. The first-order condition for optimal price $p_{it}^*$ conditional on the set of parameters $\Tilde{\mu}_{it}, \Tilde{\beta}_{it}$ is given by 
\begin{align}\label{eq:4}
   p_{it}^*  = -\frac{\Phi(\Tilde{\beta}_{it}\,p_{it}^*+\bm{x}_{it}' \Tilde{\mu}_{it})}{\Tilde{\beta}_{it}\phi(\Tilde{\beta}_{it}\,p_{it}^*+\bm{x}_{it}' \Tilde{\mu}_{it})}
\end{align}

\section{Regret Analysis}

\begin{theorem}
Revenue loss or total regret can bounded as
\[\reg_t \leq I_1 + I_2 + I_3 + \mathcal{O}(\log T + 1/T)\]
where 
 \begin{itemize}
     \item $I_1 = C_1 I\sum_{t = 1}^T \sum_{i = 1}^I \frac{1}{\eta_t}\big(\|\Tilde{\beta}_{i,t+1} - \Tilde{\beta}_{i,t}\| + \|\Tilde{\mu}_{i,t+1} - \Tilde{\mu}_{i,t}\|\big)$
      \item $I_2 = C_3 \sum_{t = 1}^T \big(\eta_t \sum_{i = 1}^I  {n_{it}^2}\|Q_{it}\|^2$\big)
     \item $I_3 = \frac{2I}{\eta_{T+1}}\Big(\frac{C^\beta}{\Tilde{\sigma}^2 + \sigma^2}+ \frac{C^\mu}{\Tilde{\sigma}^2 + \sigma^2}\Big)$

 \end{itemize}
\end{theorem}

Analyze $I_1$ term, assume that $\rho_t = \rho_0$ for all $t$. Then,

\begin{align}
    \|\Tilde{\mu}_{i,t+1} - \Tilde{\mu}_{i,t}\| & = \left\|\frac{\mu_{i,t+1}}{V_{i,t+1}} - \frac{\mu_{i,t}}{V_{i,t}}\right\|\\
    &= \left\|\frac{\mu_{i,t+1}}{V_{i}} - \frac{\mu_{i,t}}{V_{i}}\right\| \quad \quad \because \rho_t = \rho_0\\
    &= \frac{\delta_t^{\mu_i}}{V_i}
\end{align}

Similar computations would give us 
\[I_1 = C_1I\sum_{t = 1}^T\sum_{i = 1}^I \frac{1}{V_i} \left(\frac{\delta_t^{\beta_i} + \delta_t^{\mu_i}}{\eta_t}\right)\]

where $V_i^2 = \sum_{l = 1}^I \frac{u_{il}^2}{(1 - \rho_0\lambda_l)^2}\Tilde{\sigma}^2 + \sigma^2$. Using assumption 1, $\lambda_l \geq 0$ and using assumption 2 $1 - \rho_0\lambda_l \geq 0$. 

With these two inequalities \textbf{$V_i^2$ is increasing in $\rho_0$.}
\section{Proof}
Let $\theta_t = (\alpha_t, \beta_t, \mu_1, \mu_2, \cdots, \mu_I)$ and the corresponding covariates as $Q_{it} = (\bm e_i, p_{it}, 0,0,\cdots, \bm x_{it}, \cdots,0)$

Rewrite the loss function as 
\begin{align}
    l_t(\theta_t) = &-\sum_{i = 1}^I \bigg\{y_{it} \log\Phi(Q_{it}\theta_t) + (n_{it} - y_{it})\log\tilde\Phi(Q_{it}\theta_t)\bigg\} + \tau\frac{\|(\bm I - \rho_t W)\alpha_t\|^2}{2}\\
    \equiv &\text{  } l^1_t(\theta_t) + l^2_t(\rho_t, \alpha_t)
\end{align}


Let $\Tilde{\theta}_t$ be the augmented full parameter set with $\rho_t$ i.e. $\Tilde{\theta}_t = (\theta_t|\rho_t)$, then note that the update rule in DP-SGD implies
    \[\hat{\Tilde{\theta}}_{t+1} = \arg\min_{\Tilde{\theta} }
 \inner{\nabla l_{t}(\hat{\Tilde{\theta}}_{t})}{\Tilde{\theta}}  + \frac{1}{2 \eta_{t}}\|\Tilde{\theta} - \hat{\Tilde{\theta}}_{t}\|^2\]

    Since the function is convex, this implies $\forall \Tilde{\theta}$

     \begin{equation}
        \inner{\Tilde{\theta} - \hat{\Tilde{\theta}}_{t+1}}{\eta_{t}\nabla l_{t}(\hat{\Tilde{\theta}}_{t}) + \hat{\Tilde{\theta}}_{t+1} - \hat{\Tilde{\theta}}_{t}} \geq 0 
     \end{equation}
Since $l_t^1$ doesn't depend on $\rho_t$, we can rewrite the inequality as 
\begin{align}
        \inner{{\theta} - \hat{{\theta}}_{t+1}}{\eta_{t}\nabla l_{t}^1(\hat{{\theta}}_{t}) + \hat{{\theta}}_{t+1} - \hat{{\theta}}_{t}} + \inner{\alpha - \hat{\alpha}_{t+1}}{\eta_{t}\frac{\partial}{\partial \alpha_t} l_{t}^2({{\hat{\alpha}_t, \hat{\rho}_t}})
        + \hat{\alpha}_{t+1} - \hat{\alpha}_{t}} & \\+ ({\rho - \hat{\rho}_{t+1}})({\eta_{t}\frac{\partial}{\partial \rho_t} l_{t}^2({{\hat{\alpha}_t, \hat{\rho}_t}}) + \hat{\rho}_{t+1} - \hat{\rho}_{t}})& \geq 0 
        \end{align}

Set $\Tilde{\theta} = (\alpha_t, \beta_t, \mu_1, \mu_2, \cdots, \mu_I, \hat{\rho}_{t+1})$, we finally have
\begin{align}
        \inner{{\theta}_t - \hat{{\theta}}_{t+1}}{\eta_{t}\nabla l_{t}^1(\hat{{\theta}}_{t}) + \hat{{\theta}}_{t+1} - \hat{{\theta}}_{t}} + \inner{\alpha_t - \hat{\alpha}_{t+1}}{\eta_{t}\frac{\partial}{\partial \alpha_t} l_{t}^2({{\hat{\alpha}_t, \hat{\rho}_t}})
        + \hat{\alpha}_{t+1} - \hat{\alpha}_{t}} \geq 0 
        \end{align}

        Taylor expansion of $l_t^1$ gives us,
\begin{equation}
         l_{t}^1(\hat{\theta}_{t}) - l_{t}^1(\theta_t) = \inner{\nabla l_{t}^1(\hat{\theta}_{t})}{\hat{\theta}_{t} - \theta_{t}} - \frac{1}{2}\inner{\hat{\theta}_{t} - \theta_{t}}{\nabla^2 l_{t}^1(\Tilde{\theta}_{t})(\hat{\theta}_{t} - \theta_{t})} 
     \end{equation}


 We want to bound from below the $\inner{\hat{\theta}_{t} - \theta_{t}}{\nabla^2 l_{t}^1(\Tilde{\theta}_{t})(\hat{\theta}_{t} - \theta_{t})}$
\begin{equation}
    \nabla^2 l_{t}(\theta)_1 = - \sum_{i =1}^I\left(y_{it}Q_{it}Q_{it}^T \frac{\partial}{\partial \theta^2}\log \Phi(Q_{it}\theta) + (n_{it} - y_{it})Q_{it}Q_{it}^T \frac{\partial}{\partial \theta^2}\log \tilde\Phi(Q_{it}\theta)\right)
\end{equation}

\begin{comment}
Define the double differential $\nabla^2 l_{t}$ as the sum of the two components $ \nabla^2 l_{t}(\theta)_1 ,  \nabla^2 l_{t}(\theta)_2$ absed on the definition of the loss function.


The second part of the double differential is a block diagonal matrix 

\[
  \nabla^2 l_{t}(\theta)_2 =
  \begin{bmatrix}
    (\bm I - \rho_t W)^T(\bm I - \rho_t W) & & & &\\
    & 0 & & &\\
    & & \ddots & &\\
    & & & 0 &\\
    & & & & \alpha_t^T W^T W \alpha_t
  \end{bmatrix}
\]
\end{comment}


Let $l_W = \min\left\{-\frac{\partial}{\partial \theta^2}\log \Phi(Q\theta), -\frac{\partial}{\partial \theta^2}\log \Tilde\Phi(Q\theta)\right\}$

Then $\nabla^2 l_{t}(\theta)_1 \geq l_W\sum_{i = 1}^T n_{it}Q_{it}Q_{it}^T$. 

Then,
 \begin{align*}
          l_{t}^1(\hat{\theta}_{t}) - l_{t}^1(\theta_t) =& \inner{\nabla l_{t}^1(\hat{\theta}_{t})}{\hat{\theta}_{t} - \theta_{t}} - \frac{1}{2}\inner{\hat{\theta}_{t} - \theta_{t}}{\nabla^2 l_{t}^1(\Tilde{\theta}_{t})(\hat{\theta}_{t} - \theta_{t})} \\
          \leq& \inner{\nabla l_{t}^1(\hat{\theta}_{t})}{\hat{\theta}_{t} - \theta_{t}} - \frac{1}{2}\inner{\hat{\theta}_{t} - \theta_{t}} {\sum_{i = 1}^I (n_{it}l_W Q_{it}Q_{it}^T)(\hat{\theta}_{t} - \theta_{t})} \\
          =& \inner{\nabla l_{t}^1(\hat{\theta}_{t})}{\hat{\theta}_{t} - \theta_{t}} - \frac{l_W}{2}\sum_{i = 1}^I n_{it}\inner{\hat{\theta}_{t} - \theta_{t}}{ Q_{it}}^2
     \end{align*}



Setting $\eta_t \propto \frac{n_t}{\sqrt{t}}$

\section{Lemmas}     
\begin{lemma}
$W$, $u_W$ and $l_W$ are constants dependent on $W^\alpha$, $W^\mu$, $W^\beta$

\begin{align*}
    \sum_{t = 1}^T l_t^1(\hat{\theta}_t) - \sum_{t = 1}^T l_t^1({\theta}_t) \leq &\sum_{t = 1}^T \frac{2W}{\eta_t}\|{\theta_{t+1} - \theta_t}\| + \frac{1}{2\eta_1}\|\theta_1 - \hat{\theta}_1\|^2 +\sum_{t = 1}^T\left(\frac{1}{2\eta_{t+1}} - \frac{1}{2\eta_t}\right) \|\theta_{t+1} - \hat{\theta}_{t+1}\|^2\\
    &+\frac{u_W^2 I}{2}\sum_{t = 1}^T\sum_{i = 1}^I \eta_t n_{it}^2\|Q_{it}\|^2 - \frac{l_W}{2}\sum_{t = 1}^T\sum_{i = 1}^I n_{it}\inner{Q_{it}}{\theta_t - \hat{\theta}_t}^2
\end{align*}


    \begin{lemma}
    With probability $1 - \tfrac{1}{T^2}$
    \begin{align*}
    \sum_{t = 1}^T l_t^1(\hat{\theta}_t) - \sum_{t = 1}^T l_t^1({\theta}_t) \geq &- 2 u_W \sqrt{\log T}\left[\sum_{t = 1}^T\sum_{i = 1}^I n_{it}\inner{Q_{it}}{\theta_t - \hat{\theta}_t}^2\right]^{1/2} 
\end{align*}
\end{lemma}

\end{lemma}

\section{Related Work}
% Just adding so papers appear
\cite{keskin2017chasing} focus on the setting where a seller faces an unknown demand model that changes over time.
\cite{schwartz2017customer} studied customer acquisition from advertisements using multi-armed bandits.
\cite{javanmard2019dynamic} consider the pricing problem where a firm sells a large number of products described by many features to customers that arrive over time.

\cite{banerjeejoint2022,mukhopadhyay2022} focused on structured longitudinal data analysis abet in a non-dynamic set-up; the goal in this paper is to combine the time dynamics with cross-sectional structures




\bibliographystyle{abbrvnat}
\bibliography{Bibliography-MM-MC}
\end{document}
