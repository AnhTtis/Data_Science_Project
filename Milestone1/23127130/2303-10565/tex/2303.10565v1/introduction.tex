\section{Introduction}
\label{sec:introduction}
%\ljr{Still working on this...}
In single player stochastic games like multi-armed bandits and reinforcement learning, instance dependent or ``gap dependent'' sample complexity bounds that characterize the number of interactions with the environment to identify a good policy are well-understood.
In contrast to  minimax or worst-case sample complexity guarantees, these bounds and the algorithms that obtain them adapt to the true difficulty of the problem and are provably better when the problem is easy.
However, very little progress has been made on multiplayer settings. 
Even the simplest of such settings---i.e. two--player normal form matrix games---have only been studied in a minimax, worst-case sense to our knowledge. 
Nonetheless many practical applications are such that  the outcome for a decision--maker depends not just on their own action, but on the actions of other decision--makers in the environment. Indeed, finite normal form games represent a reasonable abstraction for a multitude of different important problems from economic decisions to voting systems to auctions to military abstractions (see, e.g., \citet{nisan2007algorithmic,von2007theory,bacsar1998dynamic} and references therein).

To concretize ideas, consider a setting in which two firms produce bids for a sequence of arriving customers. Each firm has one of two ways of preparing the bid (e.g., use a higher quality product versus lower quality product but include a warranty). Customers are drawn iid from a population, and select a firm meaning that the selected firm ``wins" the bid, while the other firm ``loses" the bid. This setting can be abstracted as a repeated  two--player zero--sum game defined by a $2\times 2$ stochastic matrix with independent entries in $\{-1,0,1\}$ and 
with expectation $A \in [-1,1]^{2\times2}$. For instance, the entries of $A$ may be $A_{11}=A_{22}=0$, $A_{12}=5/6$, and $A_{21}=-2/3$. 
%\arn{Shouldn't $A_{21}$ be $-1$ ?}\ljr{yes, fixed.}
Such abstractions arise in many applications including online platforms and other digital marketplaces where firms are competing for the same consumer demand. 


With this motivation in mind, in this paper we consider  two--player, zero--sum normal form matrix games possessing a unique Nash equilibrium which are defined by a stochastic matrix of dimension $n\times 2$ such that $2\leq n<\infty$.
For this class of games, we characterize the instance dependent sample complexity of identifying  a joint mixed strategy that approximately achieves the value of the game, and  a joint mixed strategy from which players have no incentive to deviate in an approximate sense. 
%belonging to one of two subclasses: $(i)$ each player has finite action space of dimension two, or $(ii)$ one player has finite action space of dimension $n$ and the other of dimension two.
That is, in a sequence of repeated game plays, we address the following questions:  how many rounds must the two players play before reaching a $(i)$   $\vep$--good solution, or $(ii)$  $\varepsilon$--Nash equilibrium, respectively? 

The repeated play proceeds as follows: for a fixed matrix $A \in \mathbb{R}^{n\times 2}$ with entries $A_{ij}$, at the start of each round $t$, the first and second player choose an $i \in \{1,\dots,n\}$ and $j \in \{1,2\}$ simultaneously, respectively, observe each others chosen actions, and then both simultaneously observe outcome $X_{ij}$ where $\mathbb{E}[X_{ij}]=A_{ij}$ and is $1$-sub-Gaussian (e.g., $X_{ij} \sim \text{Bernoulli}(A_{ij}) \in \{-1,1\}$ representing firm 1 winning the bid or not). 
Hence, the first and second player receive expected rewards of $A_{ij}$ and $-A_{ij}$, respectively. Throughout we refer to this zero-sum stochastic matrix game by simply referencing the matrix $A$ that induces the game.


Letting $\simplex_m$ denote the $m$--dimensional simplex,
we analyze the following two objectives: find a joint mixed strategy $(x,y)\in \simplex_n\times\simplex_2$ such that
\begin{enumerate}[label={\it (\roman*)},itemsep=2pt,topsep=2pt]
    \item $|V_A^\star-\la x,Ay\ra|\leq \vep$ where  \[V_A^\star:=\max_{x \in \simplex_n} \min_{y\in\simplex_2} x^\top A y\] is the value of the game, and
    \item both \[\la x,Ay\ra\geq \la x',Ay\ra-\vep\quad\text{and}\quad\la x,Ay'\ra \geq \la x,Ay\ra -\vep\] hold for all $(x',y')\in \simplex_n\times \simplex_2$.
\end{enumerate}
The former is precisely an \textbf{$\boldsymbol{\varepsilon}$--good solution}, and the latter an \textbf{$\boldsymbol{\varepsilon}$--Nash equilibrium}.
%It is straightforward to see that any $\vep$--Nash equilibrium is also an $\vep$--good solution.

We characterize the instance-dependent sample complexity of identifying $\varepsilon$-approximate solutions for the above problems in the sense that they scale with not just $\varepsilon$ and the number of actions, but the particular properties of the matrix $A$. 
Thus, our characterization defines an ordering over games capturing the intuition that the dynamics of some games converge must faster than others.
Specifically, we prove lower bounds on the number of rounds necessary for any two players to converge to an approximate Nash equilibrium.
Moreover, we propose strategies for the two players that achieve this sample complexity.
Our instance-dependent sample complexities introduce a number of quantities that characterize  notions of the sub-optimality ``gap,'' and we discuss why it is non-trivial to extend these definitions and our analysis to the general $n\times m$ dimensional matrix games. 

Before we state our main contributions, we state some easily proven facts to contextualize our results (see Appendix \ref{appendix:properties:matrix} for proof).
\begin{proposition}
For any zero-sum matrix game $A$, an $\varepsilon$--Nash equilibrium  is also an $\varepsilon$--good solution.
\end{proposition}
This means any lower bound on identifying an $\varepsilon$--good solution is also a lower bound on identifying an $\varepsilon$--Nash equilibrium. 
Conversely, any algorithm that can identify an $\varepsilon$--Nash equilibrium can also identify an $\varepsilon$--good solution with the same sample complexity.
\begin{lemma}\label{lem:trivial:NE}
Fix any $\varepsilon > 0$ and $\delta \in (0,1)$, and  matrix $A \in \R^{n\times m}$. Suppose 
that $\bar{A}\in \R^{n\times m}$ has entries $\bar{A}_{ij}$ that are the empirical mean of $\frac{8 \log (2mn/\delta) }{\varepsilon^2}$ 1-sub-Gaussian observations resulting from players playing $(i,j)$, and such that $\bar{A}_{ij}$ has  expectation  $A_{ij}$. 
Let $(x,y)\in\simplex_n\times\simplex_m$ be the Nash equilibrium of the game defined by $\bar{A}$.
% There exists an event that occurs with probability at least $1-\delta$ such that on this event $(x,y)$ is an $\varepsilon$-Nash equilibrium of $A$.
With probability at least $1-\delta$, the mixed strategy $(x,y)$ is an $\varepsilon$--Nash equilibrium of $A$.
\end{lemma}
The above strategy is minimax optimal: there exists a worst-case game matrix $A$ such that identifying an $\varepsilon$--Nash equilibrium or $\varepsilon$--good solution with constant probability requires at least $1/\varepsilon^2$ samples.
This worst-case result suggests that the sample compelxity of identifying an $\varepsilon$--Nash equilibrium and $\varepsilon$--good solution are about the same, and that this sample complexity scales with $\varepsilon$.
Remarkably, we will show that both these conclusions are false: there is a provable separation between these two problems, and that the sample complexities for natural problems can be as small as $1/\varepsilon$.


%Specifically, we derive instance-dependent bounds that define an ordering over games capturing the intuition that the dynamics of some games converge must faster than others.  

%For games with $n=2$ dimensional finite action spaces we completely characterize the sample complexity to identify a $\varepsilon$--Nash equilibrium by proving information theoretic lower bounds that are achieved by algorithms we propose. 
%That is, any two players with reasonable strategies will eventually converge to a mixed strategy their finite action space from which neither of the players has an incentive to deviate, or else incur additional cost. This paper quantifies how fast these players reach that point.

\subsection{Contributions}
%Our main contributions are as follows:
% 
% \arn{This section is bit redundant as the we mainly discuss the same results without any proofs in the main body as of now.}\ljr{That is fair, but typically its good to have a contributions section with the actual sample complexities summarized. Otherwise the reader has to dig them out of the main. I definitely its good to have some clear summary up front.}
% \label{sec:contributions}
%In this paper, we mainly focus on the instance dependent sample complexity of finding $\varepsilon$-Nash equilibrium and $\varepsilon$-good solution in $2\times 2$ and $n\times 2$ matrix games with unique Nash equilibrium. We now informally state our main results.
%\kevin{Is this for all $\varepsilon$? If $\varepsilon = \Omega(1)$ must the sample complexity scale with $\max\{ 1/|D|, 1/\Delta_{\min}^2 \}$? Presumably you can also show a $1/\varepsilon^2$ lower as well to match?}
% \ljr{This needs to be edited... just a place holder now}
%\begin{itemize}
 %   \item 
    Consider a game defined by a fixed $2\times 2$ matrix $A$ which has a unique Nash equilibrium which is not a pure-strategy Nash equilibrium. 
In Theorems \ref{thm:lower1} and \ref{thm:lower2}, we show under some mild assumptions that to find an $\varepsilon$-good solution for the matrix game $A$ with probability at least $1-\delta$, we require at least \[\Omega \left(\min\left\{\frac{1}{\varepsilon^2},\max\left\{\frac{1}{\Delta_{\min}^2},\frac{1}{\varepsilon |D|}\right\}\right\}\log(1/\delta)\right)\] samples from the matrix $A$, where problem-dependent parameters $D$ and $\Delta_{\min}$ are functions of $A$ alone and defined in Section~\ref{sec:epsilon_good}.  
Complementing this result, we present an algorithm (Algorithm \ref{alg-ucb-1}) that, with probability $1-\delta$, identifies an $\varepsilon$-good solution using a number of samples  matching this lower bound up to logarithmic factors.
% takes at most $\tilde O\left(\min\left\{\frac{\log(1/\delta)}{\varepsilon^2},\max\left\{\frac{\log(1/\delta)}{\Delta_{\min}^2},\frac{\log(1/\delta)}{\varepsilon |D|}\right\}\right\}\right)$ from $A$ and outputs an $\varepsilon$-good solution.

%\item 
In the same setting,  we show (Theorem \ref{thm:lower3}) that identifying an $\varepsilon$--Nash equilibrium for the game defined by $A$ with probability at least $1-\delta$ requires at least \[\Omega \left(\frac{\Delta_{m_2}^2}{\varepsilon^2D^2}\log(1/\delta)\right)\] samples where $\Delta_{m_2}$, defined in Section~\ref{sec:epsilon_Nash}, is function of $A$ alone. 
Since a lower bound on $\vep$--good solution identification immediately implies a lower bound on identifying an $\varepsilon$--Nash equilibrium, as noted above, we conclude that the sample complexity of identifying an $\varepsilon$--Nash equilibrium with probability at least $1-\delta$ requires \[\Omega\left(\min\left\{\frac{1}{\varepsilon^2},\max\left\{\frac{1}{\Delta_{\min}^2},\frac{\Delta_{m_2}^2}{\varepsilon^2 D^2}\right\}\right\}\log(1/\delta)\right)\] samples.
In general, it is the case that \[\max\left\{\frac{1}{\Delta_{\min}^2},\frac{\Delta_{m_2}^2}{\varepsilon^2 D^2}\right\} \geq \max\left\{\frac{1}{\Delta_{\min}^2},\frac{1}{\varepsilon |D|}\right\}\] which demonstrates a separation in sample complexity between identifying an $\varepsilon$--good solution and $\varepsilon$--Nash equilibrium.
Again, we complement this lower bound result by designing an algorithm (Algorithm \ref{alg-ucb-2}) that, with probability $1-\delta$, identifies an $\varepsilon$--Nash equilibrium with a sample complexity matching this lower bound up to logarithmic factors. 
% takes at most $\tilde O\left(\min\left\{\frac{\log(1/\delta)}{\varepsilon^2},\max\left\{\frac{\log(1/\delta)}{\Delta_{\min}^2},\frac{\Delta_{m_2}^2\log(1/\delta)}{\varepsilon^2 D^2}\right\}\right\}\right)$ from $A$ and outputs an $\varepsilon$-Nash equilibrium.
%\item 

On the other hand, if the game does have a pure-strategy Nash equilibrium then we prove nearly optimal instance dependent upper bounds for identifying an $\varepsilon$-good solution or $\varepsilon$-Nash that are similar to multi-armed bandits. 
In summary, our results completely characterize the instance-dependent sample complexity of identifying an $\varepsilon$-good solution and $\varepsilon$-Nash in the $2 \times 2$ case. 

Now consider a game defined by a fixed $3\times 2$ matrix $A$ that has a unique Nash equilibrium which is not a pure-strategy Nash equilibrium. In Theorem \ref{thm:lower4}, we show under some mild assumptions that to find an $\varepsilon$-good solution for the matrix game defined by $A$ with probability at least $1-\delta$, we require at least $\Omega \left(\frac{1}{\Delta_g^2}\log(1/\delta)\right)$ samples. 
In fact, this number of samples, characterized by the problem-dependent constant $\Delta_g$, is required to just identify the support of the mixed strategy for player 1 in $\simplex_3$ which we show is necessary for $\varepsilon$-good identification.  
Now consider a game defined by an $n\times 2$ matrix $B$ for any $n \geq 3$ which has a unique Nash equilibrium $(x^*,y^*)$ that is not a pure-strategy equilibrium. 
We complement our lower bound result by designing an algorithm (Algorithm \ref{alg-ucb-3}) that, with probability at least $1-\delta$, samples each element of $B$ for \[O\left(\min\left\{\frac{1}{\varepsilon^2},\max\left\{\frac{1}{ \Delta_{\min}^2},\frac{1}{ \Delta_{g}^2}\right\}\right\}\log(1/\delta)\right)\] times (ignoring some logarithmic factors) and  either returns $\supp(x^*)$ and $\supp(y^*)$ or concludes that $\Delta_g$ is not sufficiently large compared to $\varepsilon$. If the support is successfully identified, the algorithms for the $2 \times 2$ cases can be applied. 
Otherwise, an $\varepsilon$--Nash equilibrium can be output after $O(\frac{n}{\varepsilon^2}\cdot \log(n/\delta))$ using the procedure of Lemma~\ref{lem:trivial:NE}. While these sample complexity results hint at necessary and sufficient conditions on the instance-dependent sample complexities of general $m \times n$ games, a full characterization of this setting is left for future work.
% Here $n_0= \tilde O\left(\min\left\{\frac{\log (\frac{n}{ \delta})}{\varepsilon^2},\max\left\{\frac{\log (\frac{n}{ \delta})}{ \Delta_{\min}^2},\frac{\log (\frac{n}{ \delta})}{ \Delta_{g}^2}\right\}\right\}\right)$.
%\end{itemize}

In their respective sections, we define these problem-dependent parameters and provide intuition for what they represent and how they arise, which itself provides some insight into the difficulty of the general $m\times n$ setting. 

Above we have highlighted our results in the special case of a unique Nash Equilibrium which is not a pure strategy. 
However, we address all other cases as well, they are simply more straightforward. 
For instance, if the Nash equilibrium is \emph{not} unique, then $\Delta_{\min}=0$ (and $\Delta_g=0$ for $n\times 2$ games) and therefore our upper and lower bounds match and correspond to a $1/\varepsilon^2$ rate. Moreover, our algorithms do not assume that the equilibrium is a pure or mixed strategy, or if it is unique or not. All cases are covered (see Theorems \ref{thm:alg1}, \ref{thm:alg2}, \ref{thm:alg3} and Lemma \ref{lem:trivial:NE}). Note that our lower bound results hold only for the mixed strategy case, and we do omit a lower bound for the pure strategy case. This was done because the mixed strategy case is the novel and challenging case while the pure strategy case is very similar to multi-armed bandit lower bounds (c.f., \citet{kaufmann2016complexity}). 


\subsection{Related Work}
\label{sec:relatedwork}
%We limit our exposition of related work to complexity of  matrix games and instance dependent bounds in---both single and multiplayer---settings with finite action spaces.

\textbf{Complexity of Matrix Games.} Characterizing equilibrium behavior in normal form matrix games has been studied extensively in economics \citep{von1947theory,bohnenblust1950solutions}, as has learning as an abstraction for how players reach an equilibrium \cite{fudenberg1998theory}. The computational complexity of (exact) Nash equilibrium, especially in finite normal form games, is known to be PPAD-complete~\citep{daskalakis2009complexity,daskalakis2009ACM}. Given such hardness results, it is natural to reason about the computational complexity of \emph{approximate} equilibrium. For instance, it has been shown that $\vep$--approximate Nash can be computed in polynomial time~\cite{daskalakis2007progress,daskalakis2009note} where $\varepsilon$ is an absolute constant. These results primarily focus on settings of full information, and are concerned with computational complexity.  

Iteration complexity has been explored fairly extensively in partial information settings including settings with time-varying rewards and continuous action spaces; see, e.g., \cite{rakhlin2013optimization,cesa2006prediction,blum2007learning,syrgkanis2015fast,cardoso2019competing,bravo2018bandit,drusvyatskiy2021improved, daskalakis2011near} and references therein. Only recently has the focus shifted to characterizing statistical learnability---i.e., sample complexity---of equilibrium concepts, or other desiderata such as $\vep$--good solutions, in the presence of bandit feedback. For example, in the bandit feedback setting where players also observe the actions of their opponents,
\citet{donoghue2021matrix} show that  players adopting an optimism in the face of uncertainty principle when selecting actions experience sublinear regret---i.e., the short-fall in cumulative rewards relative to the value of the game---and further show that alternative strategies such as Thompson sampling cannot do not have a guarantee of sublinear regret.
%Of course given different information patterns
%In the absence of full information,  equilibrium learning has been examined in a variety of settings including bandit feedback; however, the focus has typically been on iteration complexity of specific  algorithms applied to classes of games. For example, \citet{bravo2018bandit} provide iteration complexity---which we improved in \cite{drusvyatskiy2021improved}---for derivative free methods in strongly monotone games.

%Similarly, a significant focus in the study of learning in games as a means to obtain an equilibrium has been on iteration complexity. 

%\ljr{Also add in iteration complexity of learning in matrix games, and other finite games... even with bandit feedback}
%\paragraph{Sample Complexity of Matrix Games.}


%\citet{cardoso2019competing} show
% \ljr{I actually do not think we should cite this paper because we would need to cite a bunch of otehr papers. essentially they look at regret for time varying games. There are plenty of papers on regret in matrix games... and I guess i dont want to open up citing all of those.

%In the context of games on continuous action spaces, the focus has primarily been on characterizing the complexity of the iterates of derivative free gradient play reaching a $\vep$--neighborhood---as measured by mean square error---of a Nash equilibrium. For instance, in strongly monotone games such complexity was first studied by \citet{bravo2018bandit}, and later improved and later improved by \citet{drusvyatskiy2021improved} where the actual complexity was shown to match the 
%are an important class for which the complexity  was first shown to be $O(d^2/\vep^3)$ by \citet{bravo2018bandit}, and later improved in \cite{drusvyatskiy2021improved} to $O(d^2/\vep^2)$ thereby  matching the 
%known rate in unconstrained optimization. 



\textbf{Instance Dependent Bounds for Games.}
To our knowledge, instance dependent sample complexity bounds remain under explored in games.  That being said, there are very recent results on special classes of games. For instance,  
\citet{dou2022gap} provide the first minimax bounds for the class of congestion games, which have the nice property of being equivalent to an optimization problem due to their potential game structure. Additionally, \citet{dou2022gap} provide sample complexity results for the centralized and decentralized problem settings under both semi-bandit and bandit feedback.
Similarly, \citet{cui2022learning} study the regret of the Nash Q-learning algorithm for two-player turn based Markov games, and introduce the first gap dependent logarithmic upper bounds, which match theoretical lower bounds up to log factors, in the episodic tabular setting.
 



\textbf{Instance Dependent Bounds in Stochastic Bandits.}
The sample complexity of stochastic bandits is well-understood: given $n$ actions each yielding a stochastic reward, to identify an action with a mean within $\varepsilon$ of the maximum with probability $1-\delta$, it is necessary and sufficient to take $\sum_{i=1}^n \min\{ \frac{1}{\varepsilon^2},\frac{1}{\Delta_i^2} \} \log(1/\delta)$ total samples, where $\Delta_i$ is the difference between the $i$th mean and the highest mean (up to $\log\log(1/\Delta_i)$ factors) \citep{mannor2004sample,kaufmann2016complexity,karnin2013almost}.
Stochastic bandits can be directly compared to our setting where $A$ is an $n\times 1$ matrix with the means of the arms on the rows.
% And recently, the sample complexity of identifying $\varepsilon$-good policies for Markov Decision Processes has even begun to be characterized \cite{al2021adaptive, wagenmaker2022beyond,tirinzoni2022optimistic}.




% \subsection{Problem statement}




% Specifically, there exists a fixed matrix $A \in \mathbb{R}^{m\times n}$ and at the start of each round $t=1,2,\dots$ we assume the first and second player choose an $i \in \{1,\dots,m\}$ and $j \in \{1,\dots,n\}$ simultaneously, respectively, observe each others chosen actions, and then both simultaneously observe $A_{ij} + \eta_t$ where $\eta_t \sim \mathcal{N}(0,1)$.
% Given that the first and second player receive expected rewards of $A_{ij}$ and $-A_{ij}$, respectively, the objective is to converge to an approximate Nash equilibrium, a pair $(x^*,y^*) \in \simplex_m \times \simplex_n$ such that achieves $\max_{x \in \simplex_m} \min_{\simplex_n} x^\top A y$.
% For zero-sum, two-player, with $m=n=2$ dimensional action spaces we completely characterize the sample complexity to identify an $\varepsilon$-Nash equilibria by proving information theoretic lower bounds that are achieved by algorithms we propose.

% Any two players with reasonable strategies will eventually converge to a (stochastic) strategy over playing actions that neither of them will deviate from, or else incur additional cost (i.e., a Nash equilibrium). 
% This paper quantifies how fast these players reach that point.
% In particular, we prove lower bounds on the number of rounds necessary for any two players to converge to an approximate Nash equilibrium.
% Moreover, we propose strategies for the two players that achieve this sample complexity.
% Our results are instance-dependent in that our lower bounds depend on properties of the particular game matrix. 

% \subsection{Preliminaries}
% For any natural number $k$, let $[k]$ denote the set $\{1,\ldots,k\}$. Let $\langle x,y\rangle=\sum_{i=1}^mx_iy_i$ where $x=(x_1,\ldots,x_m)$ and $y=(y_1,\ldots,y_m)$.



% Let $e_j^k$ denote a $k$-dimensional vector such that its $j$-th component is $1$ and the rest of the components are $0$. For any $n$-dimensional vector $v$, let $v(i)$ denote its $i$-th component. 




% Probability simplex of $k-1$ dimension is defined as follows:
% \begin{equation*}
%     \simplex_{k}=\left\{(x_1,x_2,\ldots,x_k):\sum_{i=1}^kx_i=1, x_i\geq 0\; \forall i\in[k]\right\}
% \end{equation*}


% A finite two-player zero-sum game in strategic form (also known as matrix games) is defined by a payoff matrix $A\in \mathbb{R}^{m\times n}$.  Let $A_{ij}$ denote the element in the $i$-th row and $j$-th column of $A$. There are two players in a matrix game $A\in \mathbb{R}^{m\times n}$, namely row player and column player. We will use the phrases \textit{element ij} and \textit{element $A_{ij}$} interchangeably in this paper. The row player has a strategy set $\{1,2,\ldots,m\}$, and the column player has a strategy set $\{1,2,\ldots,n\}$. If the row player plays a mixed strategy $x\in \Delta_m$ and the column player plays a mixed strategy $y\in \Delta_n$, the row player receives a payoff of $\langle x, Ay\rangle$ and the column player receives a payoff of $-\langle x, Ay\rangle$. 

% The value of the matrix game with payoff matrix $A$ is defined as follows:
% \begin{equation*}
%     V^*_A=\max_{x\in\Delta^m}\min_{y\in \Delta^n}\langle x,Ay\rangle= \min_{y\in \Delta^n}\max_{x\in\Delta^m}\langle x,Ay\rangle
% \end{equation*}

% \begin{definition}[Saddle point]
% An element $i^*j^*$ is a saddle point of the matrix $A\in \mathbb{R}^{m\times n}$ if $A_{i^*j^*}=\max_{i\in[m]}A_{ij^*}$ and $A_{i^*j^*}=\min_{j\in[n]}A_{i^*j}$.
% \end{definition}

% If a matrix $A\in \mathbb{R}^{m\times n}$ has a saddle point $A_{ij}$, then $(e_i^m,e_j^n)$ is a Nash equilibrium of the matrix game $A$ which is associated with the saddle point $A_{ij}$.

% % \begin{definition}[Nash equilibrium]
% % A mixed strategy $(x,y)\in \Delta_m\times \Delta_n$ is a Nash equilibrium of a matrix game $A\in \mathbb{R}^{m\times n}$ if the following holds true:
% % \begin{itemize}
% %     \item $\langle x,Ay \rangle \geq \langle x',Ay \rangle,\;\forall x'\in \Delta_m$,
% %     \item $\langle x,Ay' \rangle \geq \langle x,Ay \rangle,\;\forall y'\in \Delta_n$.
% % \end{itemize}
% % \end{definition}

% % \begin{definition}[$\varepsilon$-Nash equilibrium]
% % A mixed strategy $(x,y)\in \Delta_m\times \Delta_n$ is an $\varepsilon$-Nash equilibrium of a matrix game $A\in \mathbb{R}^{m\times n}$ if the following holds true:
% % \begin{itemize}
% %     \item $\langle x,Ay \rangle \geq \langle x',Ay \rangle-\varepsilon,\;\forall x'\in \Delta_m$,
% %     \item $\langle x,Ay' \rangle \geq \langle x,Ay \rangle-\varepsilon,\;\forall y'\in \Delta_n$.
% % \end{itemize}
% % \end{definition}




% % \begin{definition}[$\varepsilon$-good solution]
% % A mixed strategy $(x,y)\in \Delta_m\times \Delta_n$ is an $\varepsilon$-good solution of a matrix game $A\in \mathbb{R}^{m\times n}$ if $|V_A^*-\langle x, Ay\rangle|\leq \varepsilon$.
% % \end{definition}

% % \begin{definition}[$(\varepsilon,\delta)$-PAC-good]
% % We say an algorithm is $(\varepsilon,\delta)$-PAC-good if for all matrices $A \in \mathbb{R}^{m \times n}$ the algorithm terminates at an almost-sure finite stopping time $\tau \in \mathbb{N}$ and outputs an $\varepsilon$-good solution $(x,y)\in \Delta_m\times \Delta_n$ such that $|V_A^*-\langle x, Ay\rangle|\leq \varepsilon$ with probability at least $1-\delta$.
% % \end{definition}

% Consider a mixed strategy $x=(x_1,\ldots,x_m)\in \Delta_m$. We define the support of $x$ as follows:
% \begin{equation*}
% \supp(x):=\{i:x_i>0\}    
% \end{equation*}

% A mixed strategy $x\in \Delta_m$ is said to have a full support if $\supp(x)=\{1,2,\ldots,m\}$. Similarly, a pair of mixed strategies $(x,y)\in \Delta_m\times \Delta_n$ is said to have a full support if $\supp(x)=\{1,2,\ldots,m\}$ and $\supp(y)=\{1,2,\ldots,n\}$. The support of a pair of mixed strategies $(x,y)\in \Delta_m\times \Delta_n$ is the pair of sets $\{Supp(x),Supp(y)\}$.

% Let $[a,b;c,d]$ denote a $2\times 2$ matrix $A$ such that $A_{11}=a$, $A_{12}=b$, $A_{21}=c$ and $A_{22}=d$. Similarly, let $[a,b;c,d;e,f]$ denote a $3\times 2$ matrix $A$ such that $A_{11}=a$, $A_{12}=b$, $A_{21}=c$, $A_{22}=d$, $A_{31}=e$ and $A_{32}=f$.

% We say that a row $i$ of a matrix $A$ strongly dominates a row $j$ of $A$ if $A_{i1}>A_{j1}$ and $A_{i2}>A_{j2}$.

% For any $A\in \mathbb{R}^{m\times n}$, let $A(\Delta):=\{B\in \mathbb{R}^{n\times n}:\max_{i,j}|(B-A)_{ij}|\leq \Delta\}$.
