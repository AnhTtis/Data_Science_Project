%MakeUppercase
\section{{Results for $\varepsilon$--Good Solutions of $2\times 2$ Matrix Games}}\label{sec:epsilon_good}
 
This section is devoted to instance-dependent sample complexity bounds for identifying an $\varepsilon$-good solution $(x,y)$ for a zero-sum game matrix $A$. Recall that $|V_A^*-\langle x, Ay\rangle|\leq \varepsilon$. 
In what follows, we will frequently assume that the mixed strategies of the unique Nash equilibrium have \emph{full support}: the mixed strategy $x=\in \simplex_m$ is said to have a full support if $\supp(x):= \{ i \in [m]: x_i > 0\}$ is equal to $[m]:=\{1,\dots,m\}$. Here $x_i$ is the $i$-th component of $x$.
If $m=2$ then the unique equilibrium is either a full support mixed strategy or is a pure strategy, but not both.
%\kevin{Still need to remove saddle point language!}
\begin{definition}[Pure Strategy Nash Equilibrium]
An element $(i^*,j^*)$ is a Pure Strategy Nash Equilibrium (PSNE) of the game induced by the matrix $A\in \mathbb{R}^{m\times n}$ if $A_{i^*j^*}=\max_{i\in[m]}A_{ij^*}$ and $A_{i^*j^*}=\min_{j\in[n]}A_{i^*j}$. Moreover, a Nash equilibrium $(x,y)\in \simplex_m\times \simplex_n$ where $\supp(x)=\{i\}$ and $\supp(y)=\{j\}$ corresponds to a PSNE $(i,j)$.
\end{definition}
%\ljr{mention this is commonly referred to as a pure Nash}

For a matrix $A = [a, b; c,d ]$ (elements of a row are separated by a comma and rows are separated by a semicolon) that has a unique Nash equilibrium which is not a PSNE, our bounds will be given in terms of instance-dependent quantities:
\begin{equation*}
    \begin{aligned}
    D&=a-b-c+d,\quad \Delta_{\min}&=\min\{|a-b|,|a-c|,|d-b|,|d-c|\}.
    \end{aligned}
\end{equation*}
%It may be the case that some of these quantities are identically zero, and in this case we adopt the convention that $1/0=\infty$.
The matrix $A = [a, b; c,d ]$ has a unique Nash equilibrium which is not a PSNE if and only if either of the following hold:
\begin{equation*}
    %\begin{aligned}
    a<b,\ a<c,\ d<b,\ d<c,\quad \text{or} \quad a>b, a>c,\ d>b,\ d>c.
    %\end{aligned}
\end{equation*} Hence $|D|\geq 2\Delta_{\min}>0$. The material of this section show that the sample complexity of identifying an $\varepsilon$-good solution behaves as $\min\big\{\frac{1}{\varepsilon^2},\max\big\{\frac{1}{\Delta_{\min}^2},\frac{1}{\varepsilon |D|}\big\}\big\} \log(1/\delta)$ up to log factors. 
To motivate this bound, for matrix $A=[1, 0; 0,1]$ we have that $\min\big\{\frac{1}{\varepsilon^2},\max\big\{\frac{1}{\Delta_{\min}^2},\frac{1}{\varepsilon |D|}\big\}\big\}\approx \frac{1}{\varepsilon}$ which is significantly better than the trivial bound of $\frac{1}{\varepsilon^2}$.

To provide some intuition about where these quantities come from, suppose we measured each entry of $A$ exactly $T$ times and compiled the empirical means into a matrix $\widehat{A}$. 
If we let $(x,y)$ and $(\widehat{x},\widehat{y})$ be the Nash equilibria for $A$ and $\widehat{A}$, respectively, then $x^\top A y = \frac{ad-bc}{D}$ and we show in Appendices~\ref{appendix:minimax} and \ref{appendix:thm3} that we roughly have $| x^\top A y - \widehat{x}^\top A \widehat{y} | \leq \min\big\{ \frac{1}{\sqrt{T}}, \frac{1}{ T|D|} \big\}$. Moreover, we require roughly $\frac{1}{\Delta_{\min}^2}$ samples to decide whether $a<b$ or $b>a$ (same for other pairs). Without this information, we cannot characterize whether the input matrix has a PSNE or not, and this affects the value $V_A^*$ (which in turn affects the performance of the algorithm).
Hence, we observe it suffices to take $T \approx \min\big\{\frac{1}{\varepsilon^2},\max\big\{\frac{1}{\Delta_{\min}^2},\frac{1}{\varepsilon |D|}\big\}\big\}$. 
In the remainder of this section we make this argument rigorous and show that no smarter algorithm can improve upon this simple strategy. 

The following definition defines the set of algorithms under consideration.
\begin{definition}[$(\varepsilon,\delta)$-PAC-good]
We say an algorithm is $(\varepsilon,\delta)$-PAC-good if for all matrices $A \in \mathbb{R}^{m \times n}$ the algorithm terminates at an almost-sure finite stopping time $\tau \in \mathbb{N}$ and outputs a pair of mixed strategies $(x,y)\in \simplex_m\times \simplex_n$ such that $|V_A^*-\langle x, Ay\rangle|\leq \varepsilon$ with probability at least $1-\delta$.
\end{definition}
Our lower bounds will use this class of algorithms, and our proposed algorithm falls within this class. 

%Now consider a scenario where we can control both the row player and the column player. Now we want to find a pair of mixed strategy $(x,y)$ such that with probability at least $1-\delta$, we have the following:
%$|V_A^*-\langle x, Ay\rangle|\leq \varepsilon$.

%Now we define Nash equilibrium Regret below:
%\begin{equation*}
%    \mathcal{NER}(A)=\left|\sum_{t=1}^T\langle x_t, Ay_t\rangle-T\cdot \max_{x\in\Delta^m}\min_{y\in \Delta^n}\langle x, Ay \rangle\right|
%\end{equation*}
%What are the instance dependent guarantees here?
\subsection{Lower bound with respect to $|D|$}
% In this section, we present an instance dependent lower bound for finding an $\varepsilon$-good solution for a matrix game $A = [a, b; c,d ]$ that has a unique Nash equilibrium which is not a PSNE. Recalling the definitions $D=a-b-c+d$ and $\Delta_{\min}=\min\{|a-b|,|a-c|,|d-b|,|d-c|\}$, we have the following theorem.

This subsection derives a lower bound for the case when $A$ has a unique Nash equilibrium which is not a PSNE.

\begin{theorem}\label{thm:lower1}
Fix any matrix $A = [a, b; c,d ]$ that has a unique Nash equilibrium which is not a PSNE, $\varepsilon\in (0, \frac{\Delta_{\min}^2}{3|D|})$ and $\delta \in (0,1)$. 
Any $(\varepsilon,\delta)$-PAC-good algorithm that returns a pair of mixed strategies $(x,y)\in \simplex_2 \times\simplex_2$ at stopping time $\tau$ satisfies $\mathbb{E}_A[\tau] \geq \frac{ \log(1/30 \delta) }{3\varepsilon |D|}$.
% . Let $\mathcal{E}_{\tau_0}$ be the event that $\tau\leq \tau_0$ and $(x,y)$ is an $\varepsilon$-good solution. If $\tau_0<\frac{\log (\frac{1}{6\delta})}{12\varepsilon |D|}$, then there exists $\Delta \in \{-\sqrt{3\varepsilon |D|},0,\sqrt{3\varepsilon |D|}\}$ such that $\mathbb{P}_{\Delta}(\mathcal{E}_{\tau_0}^c)>\delta$. %where $\mathbb{P}_{\Delta}$ is the probability distribution induced by the algorithm and the matrix $A_\Delta$.
\end{theorem}

The lower bound considers a class of matrices $A_\square$ parameterized by $\square \in \R$ defined as follows:
\[
A_\square = \begin{bmatrix} 
   a+\square & b \\
   c & d-\square \\
    \end{bmatrix}.\]
Clearly $A_\square = A$ when $\square=0$. Observe that if $|\square|<\Delta_{\min}$, then the matrix game defined by $A_\square$ has a unique Nash equilibrium which is not a PSNE. The proof of the theorem, found in Appendix~\ref{appendix:thm1}, follows from change of measure arguments applied to the instances defined in the following lemma.
% Now we present the following lemma, where we show that any $(x,y)\in \Delta_2 \times \Delta_2$ cannot be an $\varepsilon$-good solution for all the matrices in the set $\{ A_\Delta : \Delta \in \{-\sqrt{3\varepsilon |D|},0,\sqrt{3\varepsilon |D|}\} \}$. 
\begin{lemma}\label{low:lem1}
Fix any $\varepsilon\in (0, \frac{\Delta_{\min}^2}{3|D|})$ and let $\Delta=\sqrt{3\varepsilon|D|}$. For any pair of mixed strategies $(x',y')\in \simplex_2 \times \simplex_2$ we have \[\displaystyle
    \max_{B\in\{A_{-\Delta},A_0,A_{\Delta}\}} |V_B^*-\langle x', By' \rangle|\geq \tfrac{3\varepsilon}{2}.\]
\end{lemma}
\noindent Unlike many lower bounds for multi-armed bandits that rely on a number of binary hypothesis tests being decided correctly (c.f., \cite{kaufmann2016complexity}), to prove the lower bound of this setting it is not possible to find a satisfying hypothesis test with fewer than three hypotheses due to the peculiar min-max behavior of the objective. 




% \hline


% Now let us fix any $\delta \in (0,1)$,  $\varepsilon\in (0, \frac{\Delta_{\min}^2}{3|D|})$ and a natural number $\tau_0<\frac{\log (\frac{1}{6\delta})}{12\varepsilon |D|}$. Let $\Delta=\sqrt{3\varepsilon|D|}$. Let $\mathcal{E}(B):=\{(x,y)\in \Delta_2\times \Delta_2:|V_B^*-\langle x,By\rangle|> \varepsilon\}$. Let $\mathcal{E}'(B):=\mathcal{E}(B)\cup\{\bot\}$. For each $B\in \{A_{-\Delta},A_0,A_{\Delta}\}$, let us add a gaussian noise $\mathcal{N}(0,1)$ to the elements $B_{11}$ and $B_{22}$ and define the sample space as $\Omega=\mathbb{R}^{2\tau_0}$ where the first $\tau_0$ components of $\Omega$ are sampled from the element $B_{11}$ and the remaining components are sampled from the element $B_{22}$.


% Let us associate every deterministic algorithm $alg$ that returns a guess of $\varepsilon$-good solution with an output policy $\phi: \Omega\rightarrow \Delta_2\times \Delta_2\cup \{\bot\}$ ($\phi$ returns $\bot$ when $alg$ takes more than $\tau_0$ samples from the input matrix). Let $\Phi$ be the set of all such output policies $\phi$. Let us define $p_\Delta(x)$ as follows:
% \begin{equation*}
%     p_\Delta(x):=\left(\frac{1}{\sqrt{2\pi}}\right)^{2\tau_0}\prod_{i=1}^{\tau_0}\exp(-\frac{(x_i-a-\Delta)^2}{2})\prod_{i=\tau_0+1}^{2\tau_0}\exp(-\frac{(x_i-d+\Delta)^2}{2})
% \end{equation*}

% Consider an algorithm $\mathcal{A}'$ that stops after taking $\tau$ samples from the input matrix and returns a pair of mixed strategies $(x,y)\in \Delta_2\times\Delta_2$. Let $\mathcal{E}_{\tau_0}$ be the event that $\tau\leq \tau_0$ and $(x,y)$ is an $\varepsilon$-good solution. Now observe that for every $\omega\in \Omega$, the algorithm $\mathcal{A}'$ induces a distribution $p_\omega$ over the set $\Phi$. Also observe that for the matrix $A_\Delta$, the algorithm $\mathcal{A}'$ induces a distribution $\mathbb{P}_\Delta$ over all the events.  Hence, we have the following:
% \begin{equation*}
%     \mathbb{P}_\Delta(\mathcal{E}^c_{\tau_0})=\int_{\omega\in \Omega}\int_{\phi\in \Phi}\mathbbm{1}(\phi(\omega)\in  \mathcal{E}'(A_\Delta))p_\omega(\phi)d\phi\; p_\Delta(\omega)d\omega
% \end{equation*}
% Now we have the following:
% \begin{align}
%   & \max\{\mathbb{P}_0(\mathcal{E}^c_{\tau_0}),\mathbb{P}_\Delta(\mathcal{E}^c_{\tau_0}),\mathbb{P}_{-\Delta}(\mathcal{E}^c_{\tau_0})\}\nonumber \\
%     \geq&  \frac{1}{3}\left(\mathbb{P}_0(\mathcal{E}^c_{\tau_0})+\mathbb{P}_\Delta(\mathcal{E}^c_{\tau_0})+\mathbb{P}_{-\Delta}(\mathcal{E}^c_{\tau_0})\right)\nonumber \\
%      \geq & \frac{1}{3}\int_{\omega\in \Omega}\int_{\phi\in \Phi}\big(\mathbbm{1}(\phi(\omega)\in  \mathcal{E}'(A_0))+\mathbbm{1}(\phi(\omega)\in \mathcal{E}'(A_\Delta))+\mathbbm{1}(\phi(\omega)\in  \mathcal{E}'(A_{-\Delta}))\big)p_\omega(\phi)d\phi\;\nonumber\\
%      & \cdot \min \{p_0(\omega),p_\Delta(\omega),p_{-\Delta}(\omega)\}d\omega\nonumber \\
%      \geq &\frac{1}{3}\int_{\omega\in \Omega}\int_{\phi\in \Phi}p_\omega(\phi)d\phi\; \min \{p_0(\omega),p_\Delta(\omega),p_{-\Delta}(\omega)\}d\omega \label{low:st1}\\
%     = & \frac{1}{3}\int_{\omega\in \Omega}\min \{p_0(\omega),p_\Delta(\omega),p_{-\Delta}(\omega)\}d\omega \nonumber\\
%     = & \frac{1}{3}\int_{\omega\in \Omega}\min \{p_\Delta(\omega),p_{-\Delta}(\omega)\}d\omega \label{low:st2}\\
%     \geq & \frac{1}{6}\left( \int_{\omega\in \Omega}\sqrt{p_\Delta(\omega)p_{-\Delta}(\omega)}d\omega\right)^2\tag{Cauchy-Schwartz \kevin{True but VERY hard to see}}\\
%     \geq & \frac{1}{6}\exp\left( -\int_{\omega\in \Omega}\log \left(\frac{p_\Delta(\omega)}{p_{-\Delta}(\omega)}\right)p_\Delta(\omega)dx\right)\tag{Jensen’s}\\
%     =& \frac{1}{6}\exp(-KL(p_\Delta|p_{-\Delta}))\nonumber\\
%     =& \frac{1}{6}\exp\left(-\tau_0\cdot KL(\mathcal{N}(a+\Delta,1)|\mathcal{N}(a-\Delta,1))-\tau_0\cdot KL(\mathcal{N}(d-\Delta,1)|\mathcal{N}(d+\Delta,1))\right)\tag{Chain rule for product distributions}\\
%     = & \frac{1}{6}\exp(-4\tau_0\Delta^2)\label{low:st3}\\
%     = & \frac{1}{6}\exp(-12\tau_0\varepsilon|D|) \tag{as $\Delta=\sqrt{3\varepsilon|D|}$}\\
%     > & \frac{1}{6}\exp\left(-\log \left(\frac{1}{6\delta}\right)\right) \tag{as $\tau_0<\frac{\log (\frac{1}{6\delta})}{12\varepsilon |D|}$}\\
%     = & \delta \nonumber
% \end{align}
% We get (\ref{low:st1}) as $\forall \omega\in \Omega$ such that $\phi(\omega)\neq \bot$, there exists $B\in \{A_\Delta, A_0, A_{-\Delta}\}$ such that $\phi(\omega)\in \mathcal{E}(B)$. The latter holds due to Lemma \ref{low:lem1}. We get (\ref{low:st2}) as $\forall \omega\in \Omega$, $p_{-\Delta}(\omega)p_{\Delta}(\omega)\leq p_0(\omega)^2$ \kevin{Is this $\leftarrow$ not equality?} and $\min\{ p_{-\Delta}(\omega)^2, p_{\Delta}(\omega)^2 \} \leq p_{-\Delta}(\omega)p_{\Delta}(\omega)$. (\ref{low:st3}) follows from the fact that $KL(\mathcal{N}(\mu_1,1)|\mathcal{N}(\mu_2,1))=\frac{(\mu_1-\mu_2)^2}{2}$.




% As $3\varepsilon|D|<\Delta_{\min}^2$, we have the following corollary.
% \begin{corollary}\label{cor:lower1}
% Consider the matrix $A = [a, b; c,d ]$ and the set of alternatives $\mathcal{A} = \{ A_\Delta : \Delta \in \{-\sqrt{3\varepsilon |D|},0,\sqrt{3\varepsilon |D|}\} \}$. Fix any  $\delta \in (0,1)$,  $\varepsilon\in (0, \frac{\Delta_{\min}^2}{3|D|})$ and a natural number $\tau_0$. Consider an algorithm that stops after taking $\tau$ samples from the input matrix and returns a pair of mixed strategies $(x,y)\in \Delta_2\times\Delta_2$. Let $\mathcal{E}_{\tau_0}$ be the event that $\tau\leq \tau_0$ and $(x,y)$ is an $\varepsilon$-good solution. If $\tau_0<\frac{\log (\frac{1}{6\delta})}{4\Delta_{\min}^2}$, then there exists $\Delta \in \{-\sqrt{3\varepsilon |D|},0,\sqrt{3\varepsilon |D|}\}$ such that $\mathbb{P}_{\Delta}(\mathcal{E}_{\tau_0}^c)>\delta$.
% %where $\mathbb{P}_{\Delta}$ is the probability distribution induced by the algorithm and the matrix $A_\Delta$.
% \end{corollary}

%Hence, we have $\delta\geq \frac{1}{6}\exp(-4n\Delta^2)$ which implies that $n\geq \frac{\log (\frac{1}{6\delta})}{4\Delta^2}$. By choosing $\Delta=\sqrt{3\varepsilon |D|}$ where $D=a-b-c+d$ and $\varepsilon<\frac{\Delta_{\min}^2}{3|D|}$, we have $n\geq \frac{\log (\frac{1}{6\delta})}{6\varepsilon D}$. Note that $\Delta_{\min}<|D|$. Hence $\varepsilon<|D|$.



\subsection{Lower bound with respect to $\Delta_{\min}$ and $\varepsilon$}
Consider a matrix $A = [a, b; c,d ]$ that has a unique Nash equilibrium which is not a PSNE. Without loss of generality assume that $D>0$ and $\Delta_{\min}=a-b$. Let us also assume that $a-c\geq d-b$.
%\kevin{Is this WLOG if we permute the matrix? Or is it a sub-class of matrices?}\arn{$D>0$, $\Delta_{\min}=a-b$ is w.l.o.g, $a-c\geq d-b$ is an assumption.}
This subsection derives a lower bound with respect to $\Delta_{\min}$ and $\varepsilon$.


%\kevin{If we always assume $D>0$ we never account for the $D=0$ case, which should be covered by this section, no? }\arn{if matrix has unique NE with full support, then $D>0$ [mentioned the proof in appendix].}
%\kevin{Also, what about a lower bound for when saddle point?}\arn{Currently I don't have a lower bound for saddle point. Even the upper bound for saddle point isn't tight. It can be made more tight but I didn't do it to keep the algorithm simple as our main focus was on unique NE with full support.}
\begin{theorem}\label{thm:lower2}
Consider the matrix $A$ and fix any $\varepsilon>0$ and $\delta \in (0,1)$. 
Any $(\varepsilon,\delta)$-PAC-good algorithm that returns a pair of mixed strategies $(x,y)\in \simplex_2 \times\simplex_2$ at stopping time $\tau$ satisfies $\mathbb{E}_A[\tau] \geq \min\left\{\frac{\log(1/30 \delta) }{36\varepsilon^2 },\frac{\log(1/30 \delta) }{36\Delta_{\min}^2 }\right\}$.
% . Let $\mathcal{E}_{\tau_0}$ be the event that $\tau\leq \tau_0$ and $(x,y)$ is an $\varepsilon$-good solution. If $\tau_0<\frac{\log (\frac{1}{6\delta})}{12\varepsilon |D|}$, then there exists $\Delta \in \{-\sqrt{3\varepsilon |D|},0,\sqrt{3\varepsilon |D|}\}$ such that $\mathbb{P}_{\Delta}(\mathcal{E}_{\tau_0}^c)>\delta$. %where $\mathbb{P}_{\Delta}$ is the probability distribution induced by the algorithm and the matrix $A_\Delta$.
\end{theorem}
The lower bound considers a class of matrices $A_\square$ parameterized by $\square \in \R$ defined as follows:
\[
A_\square = \begin{bmatrix} 
   a+\square & b-\square \\
   c+\square & d-\square \\
    \end{bmatrix}.\]
Clearly $A_\square=A$ when $\square=0$. The proof of the theorem, found in Appendix~\ref{appendix:thm2}, %follows similar arguments to the proof of Theorem~\ref{thm:lower1}. 
follows from change of measure arguments applied to the instances defined in the following lemma.

\begin{lemma}\label{low2:lem1}
Fix any $\varepsilon>0$. Let $\Delta=6\max\{\varepsilon,\Delta_{\min}\}$. For any pair of mixed strategies $(x',y')\in \simplex_2 \times \simplex_2$ we have $\displaystyle
    \max_{B\in\{A_{-\Delta},A_0,A_{\Delta}\}} |V_B^*-\langle x', By' \rangle| > \varepsilon.$
    % there exists a matrix $B\in\{A_{-\Delta},A_{0},A_{\Delta}\}$ such that the following holds:
% \begin{equation*}
    % |V_B^*-\langle x', By' \rangle|>\varepsilon.
% \end{equation*}
\end{lemma}

We can now combine Theorems~\ref{thm:lower1} and \ref{thm:lower2} to obtain the claimed result at the beginning of this section. 
Indeed, if $\varepsilon \in (0, \tfrac{\Delta_{\min}^2}{3|D|})$  then $\frac{ \log(1/30 \delta) }{3\varepsilon |D|} > \frac{\log(1/30 \delta) }{\Delta_{\min}^2 }$ and so by Theorem~\ref{thm:lower1} we have 
%$\mathbb{E}_A[\tau] \geq \frac{ \log(1/30 \delta) }{3\varepsilon |D|}
%     = \max\left\{ \frac{ \log(1/30 \delta) }{3\varepsilon |D|}, \frac{\log(1/30 \delta) }{\Delta_{\min}^2 } \right\}
%    \geq \min\left\{ \frac{\log(1/30 \delta) }{36\varepsilon^2 }, \max\left\{ \frac{ \log(1/30 \delta) }{3\varepsilon |D|}, \frac{\log(1/30 \delta) }{\Delta_{\min}^2 } \right\} \right\}.$
\begin{align*}
    \mathbb{E}_A[\tau] &\geq \tfrac{ \log(\tfrac{1}{30\delta}) }{3\varepsilon |D|}\\
    & = \max\left\{ \tfrac{ \log(\tfrac{1}{30\delta}) }{3\varepsilon |D|}, \tfrac{\log(\tfrac{1}{30\delta}) }{\Delta_{\min}^2 } \right\}\\
    &\geq \min\left\{ \tfrac{\log(\frac{1}{30\delta}) }{36\varepsilon^2 }, \max\left\{ \tfrac{ \log(\frac{1}{30\delta}) }{3\varepsilon |D|}, \tfrac{\log(\tfrac{1}{30\delta}) }{\Delta_{\min}^2 } \right\} \right\}.
\end{align*} 


On the other hand, if $\varepsilon \geq \tfrac{\Delta_{\min}^2}{3|D|}$ then  $\frac{ \log(1/30 \delta) }{3\varepsilon |D|} \leq \frac{\log(1/30 \delta) }{\Delta_{\min}^2 }$ and so by Theorem~\ref{thm:lower2} we have 
%$\mathbb{E}_A[\tau] \geq \min\left\{\frac{\log(1/30 \delta) }{36\varepsilon^2 },\frac{\log(1/30 \delta) }{36\Delta_{\min}^2 }\right\}= \frac{1}{36}  \min\left\{\frac{\log(1/30 \delta) }{\varepsilon^2 }, \max\left\{ \frac{ \log(1/30 \delta) }{3\varepsilon |D|}, \frac{\log(1/30 \delta) }{\Delta_{\min}^2 }\right\} \right\}.$
\begin{align*}
    \mathbb{E}_A[\tau] &\geq \min\left\{\tfrac{\log(\tfrac{1}{30\delta}) }{36\varepsilon^2 },\tfrac{\log(\frac{1}{30\delta}) }{36\Delta_{\min}^2 }\right\}\\
    &= \tfrac{1}{36}  \min\left\{\tfrac{\log(\tfrac{1}{30\delta}) }{\varepsilon^2 }, \max\left\{ \tfrac{ \log(\tfrac{1}{30\delta}) }{3\varepsilon |D|}, \tfrac{\log(\tfrac{1}{30\delta}) }{\Delta_{\min}^2 }\right\} \right\}.
\end{align*}
Thus, for all $\varepsilon > 0$ we have \[\mathbb{E}_A[\tau] \geq  \min\left\{\tfrac{1 }{\varepsilon^2 }, \max\left\{ \tfrac{1 }{3\varepsilon |D|}, \tfrac{1 }{\Delta_{\min}^2 }\right\} \right\} \tfrac{\log(1/30 \delta)}{36}.\]

\subsection{Lower bound for games with multiple Nash Equilibria}
Consider a matrix $A = [a, a; c,d ]$ such that $a>c$, $a<d$.  Let us also assume that $a-c\geq d-a$.
%\kevin{Is this WLOG if we permute the matrix? Or is it a sub-class of matrices?}\arn{$D>0$, $\Delta_{\min}=a-b$ is w.l.o.g, $a-c\geq d-b$ is an assumption.}
Observe that the matrix game on $A$ has multiple Nash Equilibria and this game is a degenerate version of a matrix game with unique Nash Equilibrium which is not a PSNE (as $\Delta_{\min}=0$). This subsection derives a lower bound for the matrix game on $A$ with respect to $\varepsilon$.


%\kevin{If we always assume $D>0$ we never account for the $D=0$ case, which should be covered by this section, no? }\arn{if matrix has unique NE with full support, then $D>0$ [mentioned the proof in appendix].}
%\kevin{Also, what about a lower bound for when saddle point?}\arn{Currently I don't have a lower bound for saddle point. Even the upper bound for saddle point isn't tight. It can be made more tight but I didn't do it to keep the algorithm simple as our main focus was on unique NE with full support.}
\begin{theorem}\label{thm:multiple}
Consider the matrix $A$ and fix any $\varepsilon>0$ and $\delta \in (0,1)$. 
Any $(\varepsilon,\delta)$-PAC-good algorithm that returns a pair of mixed strategies $(x,y)\in \simplex_2 \times\simplex_2$ at stopping time $\tau$ satisfies $\mathbb{E}_A[\tau] \geq \frac{\log(1/30 \delta) }{36\varepsilon^2 }$.
% . Let $\mathcal{E}_{\tau_0}$ be the event that $\tau\leq \tau_0$ and $(x,y)$ is an $\varepsilon$-good solution. If $\tau_0<\frac{\log (\frac{1}{6\delta})}{12\varepsilon |D|}$, then there exists $\Delta \in \{-\sqrt{3\varepsilon |D|},0,\sqrt{3\varepsilon |D|}\}$ such that $\mathbb{P}_{\Delta}(\mathcal{E}_{\tau_0}^c)>\delta$. %where $\mathbb{P}_{\Delta}$ is the probability distribution induced by the algorithm and the matrix $A_\Delta$.
\end{theorem}
The lower bound considers a class of matrices $A_\square$ parameterized by $\square \in \R$ defined as follows:
\[
A_\square = \begin{bmatrix} 
   a+\square & a-\square \\
   c+\square & d-\square \\
    \end{bmatrix}.\]
Clearly $A_\square=A$ when $\square=0$. The proof of the theorem, found in Appendix~\ref{appendix:thm:multiple}, follows from change of measure arguments applied to the instances defined in the following lemma.

\begin{lemma}\label{lem1:multiple}
Fix any $\varepsilon>0$. Let $\Delta=6\varepsilon$. For any pair of mixed strategies $(x',y')\in \simplex_2 \times \simplex_2$ we have $\displaystyle
    \max_{B\in\{A_{-\Delta},A_0,A_{\Delta}\}} |V_B^*-\langle x', By' \rangle| > \varepsilon.$
    % there exists a matrix $B\in\{A_{-\Delta},A_{0},A_{\Delta}\}$ such that the following holds:
% \begin{equation*}
    % |V_B^*-\langle x', By' \rangle|>\varepsilon.
% \end{equation*}
\end{lemma}


\subsection{Upper bound for $\varepsilon$--good solution}

As discussed above, Lemma~\ref{thm:lower1} describes a minimax optimal strategy. This subsection is dedicated to Algorithm~\ref{alg-ucb-1} that achieves an instance-dependent sample complexity for the special case of $m=n=2$.
Algorithm~\ref{alg-ucb-1} first samples the elements of $A$ until we can conclude whether $A$ has a PSNE or not. 
If $A$ has a PSNE, then we return it. 
If $A$ does not have a PSNE, we further sample each element of $A$ for $\tilde O(\frac{\log(\frac{1}{\delta})}{\varepsilon \tilde D})$ times and return the Nash equilibrium of the empirical matrix $\bar A$. 
Here $\tilde D$ is an empirical estimate of $|D|$. 
If no prior condition is met, the algorithm terminates in the worst case at iteration $t=T=\frac{8 \log(16/\delta)}{\varepsilon^2}$ and outputs an $\varepsilon$--good solution with high probability.
The full sample complexity guarantees of the algorithm are described in the following theorem whose proof is in Appendix \ref{appendix:thm3}.
% For more details, we refer the reader to Algorithm \ref{alg-ucb-1}.


% In this section, we formally establish the sample complexity of finding an $\varepsilon$-good solution in a matrix game $A\in \mathbb{R}^{2\times 2}$ by proving the following theorem.
\begin{theorem}\label{thm:alg1}
Fix any $\varepsilon > 0$ and $\delta \in (0,1)$. 
% Let $T=\frac{8 \log (16/\delta) }{\varepsilon^2}$. 
With probability at least $1-\delta$, Algorithm~\ref{alg-ucb-1} returns an $\varepsilon$--good solution after at most $n_0$ samples such that %\kevin{Don't use Big-O notation here}\arn{Can I just use absolute constant variables $c_1,c_2,...$ instead?}
\begin{itemize}[itemsep=1pt,topsep=0pt]
    \item $n_0= c_1\cdot\min\big\{\frac{\log(1/\delta)}{\varepsilon^2},\max\big\{\frac{\log(\frac{1}{\varepsilon\delta})}{\Delta_{\min}^2},\frac{\log(\frac{1}{\varepsilon\delta})}{\varepsilon |D|}\big\}\big\}$ if the matrix game induced by $A$ has a unique Nash equilibrium which is not a PSNE, and
    \item $n_0=c_2\cdot\min\big\{\frac{\log(1/\delta)}{\varepsilon^2},\frac{\log(1/(\varepsilon\delta))}{\Delta_{\min}^2}\big\}$ if the matrix game induced by $A$ has a PSNE,
\end{itemize}
where $c_1,c_2$ are absolute constants.
\end{theorem}



\begin{algorithm}[t!]
\caption{Find an $\varepsilon$--good solution}
\begin{algorithmic}[1]
\STATE $T\gets\frac{8 \log (16/\delta) }{\varepsilon^2}$ 
\FOR{time step $t=1,2,\ldots,T$} 
\STATE Sample each element $(i,j)$ once and update the empirical means $\bar A_{ij}$.
\STATE $\Delta \gets \sqrt{2\log(\frac{16T}{\delta}) / t}$
\STATE $\tilde\Delta_{\min}\gets \min\{|\bar A_{11}-\bar A_{12}|,|\bar A_{21}-\bar A_{22}|\}|\bar A_{11}-\bar A_{21}|,|\bar A_{12}-\bar A_{22}|\}$
\STATE $\tilde D\gets |\bar A_{11}-\bar A_{12}-\bar A_{21}+\bar A_{22}|$
\IF{$1\leq \frac{\tilde \Delta_{\min}+2\Delta}{\tilde \Delta_{\min}-2\Delta}\leq \frac{3}{2}$ and the matrix game $\bar A$ has a PSNE }\label{alg1:con1}
\STATE Return  the PSNE of  $\bar A$.
\ELSIF{$1\leq \frac{\tilde \Delta_{\min}+2\Delta}{\tilde \Delta_{\min}-2\Delta}\leq \frac{3}{2}$ and $\tilde D< 10\varepsilon$}\label{alg1:con2} 
\STATE Sample each element $(i,j)$ for $T-t$ times.
\STATE Return the Nash equilibrium of $\bar A$.
\ELSIF{$1\leq \frac{\tilde \Delta_{\min}+2\Delta}{\tilde \Delta_{\min}-2\Delta}\leq \frac{3}{2}$ and $\tilde D\geq 10\varepsilon$}\label{alg1:con3} 
\STATE $N\gets \frac{80 \log (\frac{16T}{\delta}) }{\varepsilon \tilde D }$
% \STATE Sample each element $(i,j)$ for $\min\{N, T-t\}$ times.
\IF{$N>T-t$}\label{alg1:con4}
\STATE Sample each element $(i,j)$ for $T-t$ times
\STATE Return the Nash equilibrium of $\bar A$.
\ENDIF
\STATE Sample each element $(i,j)$ for $N$ times.
%\STATE Compute a matrix $B$ which belongs to $\bar A \left(\frac{\sqrt{\varepsilon \tilde D}}{16}\right)$ and has a NE with full support.
\STATE Return the Nash equilibrium of $\bar A$.
\ENDIF
\ENDFOR
\STATE Return the Nash equilibrium of $\bar A$. \label{alg1:con5}
\end{algorithmic}
\label{alg-ucb-1}
\end{algorithm}

% \begin{algorithm}[h]
% \caption{Algorithm for finding $\varepsilon$-good solution}
% \begin{algorithmic}[1]
% \STATE $T\gets\frac{8 \log (16/\delta) }{\varepsilon^2}$, $\varepsilon_t := \sqrt{2\log(\frac{16T}{\delta}) / (t)}$, $\tilde\Delta_{\min}=0$
% \STATE $t=0$
% \WHILE{$\frac{\tilde \Delta_{\min}+2\varepsilon_t}{\max\{0, \tilde \Delta_{\min}-2\varepsilon_t \} } >  \frac{3}{2}$}
% \STATE Sample each element $(i,j)$ once and update the empirical means $\bar A_{ij}$.
% \STATE $\tilde\Delta_{\min}\gets \min\{|\bar A_{11}-\bar A_{12}|,|\bar A_{21}-\bar A_{22}|\}|\bar A_{11}-\bar A_{21}|,|\bar A_{12}-\bar A_{22}|\}$
% \STATE $\tilde D\gets |\bar A_{11}-\bar A_{12}-\bar A_{21}+\bar A_{22}|$
% \STATE{$t \leftarrow t+1$}
% \ENDWHILE
% \IF{$t = T$}
%     Return the Nash equilibrium of $\bar A$.
% \ELSIF{$\bar{A}$ has saddle point}
%     \STATE Return Nash equilibrium associated with the saddle point of $\bar A$.
% \ELSIF{$\tilde D< 10\varepsilon$}
%     \STATE Sample each element $(i,j)$ for $T-t$ times.
%     \STATE Return the Nash equilibrium of $\bar A$.
% \ELSIF{$\tilde D\geq 10\varepsilon$}
%     \STATE $N\gets \frac{80 \log (\frac{16T}{\delta}) }{\varepsilon \tilde D }$
% % \STATE Sample each element $(i,j)$ for $\min\{N, T-t\}$ times.
% \IF{$N>T-t$}\label{alg1:con4}
% \STATE Sample each element $(i,j)$ for $T-t$ times.
% \STATE Return the Nash equilibrium of $\bar A$.
% \ENDIF
% \STATE Sample each element $(i,j)$ for $N$ times.
% \STATE Compute a matrix $B$ which belongs to $\bar A \left(\frac{\sqrt{\varepsilon \tilde D}}{16}\right)$ and has a NE with full support.
% \STATE Return the Nash equilibrium of $\bar A$.
% \ENDIF





% We now establish the sample complexity and the correctness of the Algorithm \ref{alg-ucb-1} by proving the Theorem \ref{thm:alg1}.
%\begin{theorem}
%With probability at least $1-\delta$, we sample each element $(i,j)$ for $\min\left\{\frac{43 \log (16/\delta) }{\varepsilon^2},\frac{3712\log (\frac{16T}{\delta})}{\varepsilon |D|}\right\}$ times and output a pair of strategies $(x,y)$ such that $|V_{A}^*-\langle x, A y\rangle|\leq \varepsilon$. 
%\end{theorem}


%\MakeUppercase
\section{{Results for $\varepsilon$--Nash Equilibrium of $2\times 2$ Matrix Games}}\label{sec:epsilon_Nash}
This section is devoted to instance-dependent sample complexity bounds for identifying an $\varepsilon$--Nash equilibrium  $(x,y)$. Recall that both $\la x,Ay\ra\geq \la x',Ay\ra-\vep$ and $\la x,Ay'\ra \geq \la x,Ay\ra -\vep$ hold for all $(x',y')\in \simplex_2\times \simplex_2$.
For a matrix $A = [a, b; c,d ]$, our bounds will be given in terms of the following instance-dependent quantities:
\begin{equation*}
    \begin{aligned}
    D&=a-b-c+d, \quad \Delta_{m_2}&=\max\{\min\{|a-b|, |d-c|\},\min\{|a-c|,|d-b|\}\}.
    \end{aligned}
\end{equation*}
%It may be the case that some of these quantities are identically zero, and in this case we adopt the convention that $1/0=\infty$.
The sample complexity of identifying an $\varepsilon$--Nash equilibrium is $\min\big\{\frac{1}{\varepsilon^2},\max\big\{\frac{1}{\Delta_{\min}^2},\frac{\Delta_{m_2}^2}{\varepsilon^2 D^2}\big\}\big\} \log(1/\delta)$ up to log factors. To motivate this bound, consider the matrix $A=[1+\varepsilon^{0.5}, 1; 0,1+\varepsilon^{0.5}]$ where $0<\varepsilon<1$. Then $\min\big\{\frac{1}{\varepsilon^2},\max\big\{\frac{1}{\Delta_{\min}^2},\frac{\Delta_{m_2}^2}{\varepsilon^2 D^2}\big\}\big\}\approx \frac{1}{\varepsilon}$ which is significantly better than the trivial bound of $\frac{1}{\varepsilon^2}$. 
On the other hand, for the matrix $B=[1,0; 0,1]$ we have that  $\min\big\{\frac{1}{\varepsilon^2},\max\big\{\frac{1}{\Delta_{\min}^2},\frac{\Delta_{m_2}^2}{\varepsilon^2 D^2}\big\}\big\}\approx \frac{1}{\varepsilon^2}$ which is significantly worse than the bound of $\frac{1}{\varepsilon}$ that we achieved for identifying an $\varepsilon$--good solution before. This shows that finding an $\varepsilon$--Nash equilibrium can require many more samples than finding an $\varepsilon$--good solution. This is not unexpected as every $\varepsilon$--Nash equilibrium is also an $\varepsilon$--good solution.
%\kevin{Please comment on how this sample complexity relates to the $\varepsilon$-good (it should be strictly larger right?)}
%\kevin{Please give an example where the equilibrium is full support and  $\frac{\Delta_{m_2}^2}{D^2} \ll 1$ to motivate this complexity. Also, provide some calculation to demonstrate why this pops up, as above} 

To provide some intuition about where these quantities come from, suppose we measured each entry of $A$ exactly $T$ times and compiled the empirical means into a matrix $\widehat{A}$. 
If we let $(\widehat{x},\widehat{y})$ be the Nash equilibrium for $\widehat{A}$, respectively, then we show (cf.~Appendix~\ref{appendix:thm5}) that we roughly have 
\begin{equation*}
    \begin{aligned}
    \max_{x'\in\simplex_2}x'^\top A \widehat{y} - \widehat{x}^\top A \widehat{y}\leq \tfrac{\Delta_{m_2}}{\sqrt{T}|D|}\quad\text{and}\quad
    \widehat{x}^\top A \widehat{y}-\min_{y'\in\simplex_2}\widehat{x}^\top A y'  \leq \tfrac{\Delta_{m_2}}{\sqrt{T}|D|}.
    \end{aligned}
\end{equation*} Moreover, in the previous section we showed that roughly $1/\Delta_{\min}^2$ samples are required to distinguish between various alternatives. 
Hence, we observe it suffices to take $T \approx \min\big\{\frac{1}{\varepsilon^2},\max\big\{\frac{1}{\Delta_{\min}^2},\frac{\Delta_{m_2}^2}{\varepsilon^2 D^2}\big\}\big\}$. 
In the remainder of this section we make this argument rigorous and show that no smarter algorithm can improve upon this simple strategy. 

The following  is the definition of the set of algorithms under consideration.
\begin{definition}[$(\varepsilon,\delta)$-PAC-Nash]
We say an algorithm is $(\varepsilon,\delta)$-PAC-Nash if for all induced by matrices $A \in \mathbb{R}^{m \times n}$, the algorithm terminates at an almost--sure finite stopping time $\tau \in \mathbb{N}$ and outputs a pair of mixed strategies $(x,y)\in \simplex_m\times \simplex_n$ such that $\la x,Ay\ra\geq \la x',Ay\ra-\vep$ and $\la x,Ay'\ra \geq \la x,Ay\ra -\vep$ hold for all $(x',y')\in \simplex_m\times \simplex_n$ with probability at least $1-\delta$.
\end{definition}
Our lower bounds will use this class of algorithms, and the proposed algorithm falls within this class. 

\subsection{Lower bound for finding $\varepsilon$--Nash equilibrium}
This subsection derives a lower bound for the case when $A$ has a unique Nash equilibrium which is not a PSNE.

\begin{theorem}\label{thm:lower3}
Fix any matrix $A = [a, b; c,d ]$ that has a unique Nash equilibrium which is not a PSNE, $\varepsilon>0$ and $\delta \in (0,1)$. 
Any $(\varepsilon,\delta)$-PAC-Nash algorithm that returns a pair of mixed strategies $(x,y)\in \simplex_2 \times\simplex_2$ at stopping time $\tau$ satisfies $\mathbb{E}_A[\tau] \geq \frac{\Delta_{m_2}^2 \log(1/30 \delta) }{9\varepsilon^2D^2}$.
\end{theorem}

Without loss of generality assume that $D>0$ and $\Delta_{m_2}=a-b$. The lower bound considers a class of matrices $A_\square$ parameterized by $\square \in \R$ defined as follows:
\[
A_\square = \begin{bmatrix} 
   a+\square & b+\square \\
   c-\square & d-\square \\
    \end{bmatrix}.\]
Clearly $A_\square=A$ when $\square=0$. The proof of the theorem, found in Appendix~\ref{appendix:thm4}, follows from change of measure arguments applied to the instances defined in the following lemma.
\begin{lemma}\label{low3:lem1}
Fix any $\varepsilon>0$. For any pair of mixed strategies $(x',y')\in \simplex_2 \times \simplex_2$, there exists a matrix $B\in\{A_0,A_{\Delta},A_{-\Delta}\}$ such that $(x',y')$ is not an $\varepsilon$-Nash equilibrium of $B$ where $\Delta:= \frac{3\varepsilon D}{\Delta_{m_2}}$.
\end{lemma}
Recall that any lower bound for an $\varepsilon$--good solution also holds for $\varepsilon$--Nash equilibrium. Hence,  combining Theorems~\ref{thm:lower2} and \ref{thm:lower3} we obtain the claimed result at the beginning of this section. Note that the lower bound of Theorem \ref{thm:lower1} is redundant as $\frac{1}{\varepsilon|D|}\leq \frac{\Delta_{m_2}^2}{\varepsilon^2|D|^2}$ when $\varepsilon\in(0,\frac{\Delta_{\min}^2}{3|D|})$.


%Now let us fix any $\varepsilon>0$ and a natural number $\tau_0<\frac{\Delta^2_{m_2}\log (\frac{1}{6\delta})}{72\varepsilon^2 D^2}$. Let $\Delta=\frac{3\varepsilon D}{\Delta_{m_2}}$. Let $\mathcal{E}(B)$ be the set of $(x,y)\in \Delta_2\times \Delta_2$ such that $(x,y)$ is not an $\varepsilon$-Nash equilibrium of $B$. Let $\mathcal{E}'(B):=\mathcal{E}(B)\cup\{\bot\}$. For each $B\in\{A_0,A_{\Delta},A_{-\Delta}\}$, let us add a gaussian noise $\mathcal{N}(0,1)$ to each element of $B$ and define the sample space as $\Omega=\mathbb{R}^{4\tau_0}$ where the first $\tau_0$ components of $\Omega$ are sampled from the element $B_{11}$  the second $\tau_0$ components of $\Omega$ are sampled from the element $B_{12}$, the third $\tau_0$ components of $\Omega$ are sampled from the element $B_{21}$ and the remaining components are sampled from the element $B_{22}$.

%Let us associate every deterministic algorithm $alg$ that returns a guess of $\varepsilon$-Nash equilibrium with an output policy $\phi: \Omega\rightarrow \Delta_2\times \Delta_2\cup \{\bot\}$ ($\phi$ returns $\bot$ when $alg$ takes more than $\tau_0$ samples from the input matrix). Let $\Phi$ be the set of all such output policies $\phi$. Let us define $p_\Delta(x)$ as follows:
%\begin{align*}
%   p_\Delta(x):=&\left(\frac{1}{\sqrt{2\pi}}\right)^{4\tau_0}\exp\big(-\sum_{i=1}^{\tau_0}\frac{(x_i-a-\Delta)^2}{2}-\sum_{i=\tau_0+1}^{2\tau_0}\frac{(x_i-b-\Delta)^2}{2}\\
%   &-\sum_{i=2\tau_0+1}^{3\tau_0}\frac{(x_i-c+\Delta)^2}{2}-\sum_{i=3\tau_0+1}^{4\tau_0}\frac{(x_i-d+\Delta)^2}{2}\big)
%\end{align*}

%Consider an algorithm $\mathcal{A}'$ that stops after taking $\tau$ samples from the input matrix and returns a pair of mixed strategies $(x,y)\in \Delta_2\times\Delta_2$. Let $\mathcal{E}_{\tau_0}$ be the event that $\tau\leq \tau_0$ and $(x,y)$ is an $\varepsilon$-Nash equilibrium. Now observe that for every $\omega\in \Omega$, the algorithm $\mathcal{A}'$ induces a distribution $p_\omega$ over the set $\Phi$. Also observe that for the matrix $A_\Delta$, the algorithm $\mathcal{A}'$ induces a distribution $\mathbb{P}_\Delta$ over all the events.  Hence, we have the following:
%\begin{equation*}
%    \mathbb{P}_\Delta(\mathcal{E}^c_{\tau_0})=\int_{\omega\in \Omega}\int_{\phi\in \Phi}\mathbbm{1}(\phi(\omega)\in  \mathcal{E}'(A_\Delta))p_\omega(\phi)d\phi\; p_\Delta(\omega)d\omega
%\end{equation*}

%Now we have the following:
%\begin{align}
%   & \max\{\mathbb{P}_0(\mathcal{E}^c_{\tau_0}),\mathbb{P}_\Delta(\mathcal{E}^c_{\tau_0}),\mathbb{P}_{-\Delta}(\mathcal{E}^c_{\tau_0})\}\nonumber \\
%    \geq&  \frac{1}{3}\left(\mathbb{P}_0(\mathcal{E}^c_{\tau_0})+\mathbb{P}_\Delta(\mathcal{E}^c_{\tau_0})+\mathbb{P}_{-\Delta}(\mathcal{E}^c_{\tau_0})\right)\nonumber \\
%     \geq & \frac{1}{3}\int_{\omega\in \Omega}\int_{\phi\in \Phi}\big(\mathbbm{1}(\phi(\omega)\in  \mathcal{E}'(A_0))+\mathbbm{1}(\phi(\omega)\in \mathcal{E}'(A_\Delta))+\mathbbm{1}(\phi(\omega)\in  \mathcal{E}'(A_{-\Delta}))\big)p_\omega(\phi)d\phi\;\nonumber\\
%     & \cdot \min \{p_0(\omega),p_\Delta(\omega),p_{-\Delta}(\omega)\}d\omega\nonumber \\
%     \geq &\frac{1}{3}\int_{\omega\in \Omega}\int_{\phi\in \Phi}p_\omega(\phi)d\phi\; \min \{p_0(\omega),p_\Delta(\omega),p_{-\Delta}(\omega)\}d\omega \label{low3:st1}\\
%    = & \frac{1}{3}\int_{\omega\in \Omega}\min \{p_0(\omega),p_\Delta(\omega),p_{-\Delta}(\omega)\}d\omega \nonumber\\
%    = & \frac{1}{3}\int_{\omega\in \Omega}\min \{p_\Delta(\omega),p_{-\Delta}(\omega)\}d\omega \label{low3:st2}\\
%    \geq & \frac{1}{6}\left( \int_{\omega\in \Omega}\sqrt{p_\Delta(\omega)p_{-\Delta}(\omega)}d\omega\right)^2\tag{Cauchy-Schwartz}\\
%    \geq & \frac{1}{6}\exp\left( -\int_{\omega\in \Omega}\log \left(\frac{p_\Delta(\omega)}{p_{-\Delta}(\omega)}\right)p_\Delta(\omega)dx\right)\tag{Jensen’s}\\
%    =& \frac{1}{6}\exp(-KL(p_\Delta|p_{-\Delta}))\nonumber\\
%    =& \frac{1}{6}\exp(-\tau_0\cdot KL(\mathcal{N}(a+\Delta,1)|\mathcal{N}(a-\Delta,1))-\tau_0\cdot KL(\mathcal{N}(b+\Delta,1)|\mathcal{N}(b-\Delta,1))\nonumber\\
%    &-\tau_0\cdot KL(\mathcal{N}(c-\Delta,1)|\mathcal{N}(c+\Delta,1))-\tau_0\cdot KL(\mathcal{N}(d-\Delta,1)|\mathcal{N}(d+\Delta,1)))\tag{Chain rule for product distributions}\\
%    = & \frac{1}{6}\exp(-8\tau_0\Delta^2)\label{low3:st3}\\
%    = & \frac{1}{6}\exp\left(-8\tau_0\cdot\frac{9\varepsilon^2 D^2}{\Delta_{m_2}^2}\right)  \tag{as $\Delta=\frac{3\varepsilon D}{\Delta_{m_2}}$}\\
%    > & \frac{1}{6}\exp\left(-\log \left(\frac{1}{6\delta}\right)\right) \tag{as $\tau_0<\frac{\Delta^2_{m_2}\log (\frac{1}{6\delta})}{72\varepsilon^2 D^2}$}\\
%    = & \delta \nonumber
%\end{align}
%We get (\ref{low3:st1}) as $\forall \omega\in \Omega$ such that $\phi(\omega)\neq \bot$, there exists $B\in \{A_\Delta, A_0, A_{-\Delta}\}$ such that $\phi(\omega)\in \mathcal{E}(B)$. The latter holds due to Lemma \ref{low3:lem1}. We get (\ref{low3:st2}) as $\forall \omega\in \Omega$, $p_{-\Delta}(\omega)p_{\Delta}(\omega)\leq p_0(\omega)^2$. (\ref{low3:st3}) follows from the fact that $KL(\mathcal{N}(\mu_1,1)|\mathcal{N}(\mu_2,1))=\frac{(\mu_1-\mu_2)^2}{2}$.

%Due to the above analysis, we get the following theorem.
%\begin{theorem}\label{thm:lower3}
%Consider the matrix $A = [a, b; c,d ]$ and the set of alternatives $\mathcal{A} = \{ A_\Delta : \Delta \in \{0,\frac{3\varepsilon D}{\Delta_{m_2}},-\frac{3\varepsilon D}{\Delta_{m_2}}\} \}$. Fix any $\varepsilon >0$, $\delta \in (0,1)$ and a natural number $\tau_0$. Consider an algorithm that stops after taking $\tau$ samples from the input matrix and returns a pair of mixed strategies $(x,y)\in \Delta_2\times\Delta_2$. Let $\mathcal{E}_{\tau_0}$ be the event that $\tau\leq \tau_0$ and $(x,y)$ is an $\varepsilon$-Nash equilibrium. If $\tau_0<\frac{\Delta^2_{m_2}\log (\frac{1}{6\delta})}{72\varepsilon^2 D^2}$, then there exists $\Delta \in \{0,\frac{3\varepsilon D}{\Delta_{m_2}},-\frac{3\varepsilon D}{\Delta_{m_2}}\}$ such that $\mathbb{P}_{\Delta}(\mathcal{E}_{\tau_0}^c)>\delta$.
%\end{theorem}
\subsection{Upper bound for $\varepsilon$-Nash equilibrium}

\begin{algorithm}[h]
\caption{Find an $\varepsilon$--Nash equilibrium}
\begin{algorithmic}[1]
\STATE $T\gets\frac{8 \log (16/\delta) }{\varepsilon^2}$ 
\FOR{round $t=1,2,\ldots,T$} 
\STATE Sample each element $(i,j)$ once and update the empirical means $\bar A_{ij}$.
\STATE $\Delta \gets \sqrt{2\log(\frac{16T}{\delta}) / (t)}$.
\STATE $\tilde\Delta_{\min}\gets \min\{|\bar A_{11}-\bar A_{12}|,|\bar A_{21}-\bar A_{22}|\}|\bar A_{11}-\bar A_{21}|,|\bar A_{12}-\bar A_{22}|\}$
\STATE $\tilde \Delta_{m_2}\gets \max\{\min\{|\bar A_{11}-\bar A_{12}|,|\bar A_{21}-\bar A_{22}|\},\min\{|\bar A_{11}-\bar A_{21}|,|\bar A_{12}-\bar A_{22}|\}\}$
\STATE $\tilde D\gets |\bar A_{11}-\bar A_{12}-\bar A_{21}+\bar A_{22}|$
\IF{$1\leq \frac{\tilde \Delta_{\min}+2\Delta}{\tilde \Delta_{\min}-2\Delta}\leq \frac{3}{2}$ and the game induced by $\bar A$ has a pure strategy Nash}\label{alg2:con1}
\STATE Return the Nash equilibrium associated with the PSNE of $\bar A$.
\ELSIF{$1\leq \frac{\tilde \Delta_{\min}+2\Delta}{\tilde \Delta_{\min}-2\Delta}\leq \frac{3}{2}$ and $\tilde \Delta_{m_2}\geq \tilde D/8$} \label{alg2:con2}
\STATE Sample each element $(i,j)$ for $T-t$ times.
\STATE Return the Nash equilibrium of $\bar A$.
\ELSIF{$1\leq \frac{\tilde \Delta_{\min}+2\Delta}{\tilde \Delta_{\min}-2\Delta}\leq \frac{3}{2}$ and $\tilde \Delta_{m_2}< \tilde D/8$} \label{alg2:con3}
\STATE $N\gets \frac{200\tilde\Delta^2_{m_2}\log (\frac{16T}{\delta})}{\varepsilon^2 \tilde D^2}$
\IF{$N>T-t$}\label{alg2:con4}
\STATE Sample each element $(i,j)$ for $T-t$ times.
\STATE Return the Nash equilibrium of $\bar A$.
\ENDIF
\STATE $\Delta_1 \gets \sqrt{2\log(\frac{16T}{\delta}) /(N+t)}$
\STATE Sample each element $(i,j)$ for $N$ times.
\STATE $i_1\gets \arg\min_i |\bar A_{i1}-\bar A_{i2}|$ and $i_2 \gets \{1,2\}\setminus \{i_1\}$
\STATE $j_1\gets \arg\min_i |\bar A_{1j}-\bar A_{2j}|$ and $j_2 \gets \{1,2\}\setminus \{j_1\}$
\STATE $B_{i_1j_1}\gets \bar A_{i_1j_1}$, $B_{i_2j_2}\gets \bar A_{i_2j_2}$, $B_{i_1j_2}\gets \bar A_{i_1j_2}-2\Delta_1$, $B_{i_2j_1}\gets \bar A_{i_2j_1}+2\Delta_1$
\STATE Return the Nash equilibrium of $B$.
\ENDIF
\ENDFOR
\STATE Return the Nash equilibrium of $\bar A$.  \label{alg2:con5}
\end{algorithmic}
\label{alg-ucb-2}
\end{algorithm}

%In this section we present and analyze 

Next, we characterize the instance-dependent sample complexity of Algorithm~\ref{alg-ucb-2} for the special case of $2\times 2$ matrix.
Algorithm~\ref{alg-ucb-2} first samples the elements of $A$ until we can conclude whether the game induced by $A$ has a PSNE or not. If the game does, then we return it. If the matrix game induced by $A$ does not have a PSNE, we further sample each element of $A$ for $\tilde O(\frac{\tilde\Delta^2_{m_2}\log (\frac{1}{\delta})}{\varepsilon^2 \tilde D^2})$ times and return the Nash equilibrium of a matrix $B$ that we get by slightly modifying the empirical matrix $\bar A$. Here $\tilde D$ and $\tilde\Delta_{m_2}$ are the empirical estimates of $|D|$ and $\Delta_{m_2}$, respectively. If no prior condition is met, the algorithm terminates in the worst case at iteration $t=T=\frac{8 \log(16/\delta)}{\varepsilon^2}$ and outputs an $\varepsilon$--Nash equilibrium with high probability by Lemma~\ref{thm:lower1}. The full sample complexity guarantees of the algorithm are described in the following theorem the proof of which is in Appendix~\ref{appendix:thm5}.


\begin{theorem}\label{thm:alg2}
 Fix any $\varepsilon > 0$ and $\delta \in (0,1)$. 
%  Let $T=\frac{8 \log (16/\delta) }{\varepsilon^2}$.
With probability at least $1-\delta$ Algorithm~\ref{alg-ucb-2} returns an $\varepsilon$--Nash equilibrium after at most $n_0$ samples where 
\begin{itemize} %[itemsep=0pt,topsep=0pt]
    \item $n_0= c_1\cdot\min\big\{\frac{\log(\tfrac{1}{\varepsilon \delta})}{\varepsilon^2},\max\big\{\frac{\log(\tfrac{1}{\varepsilon \delta})}{\Delta_{\min}^2},\frac{\Delta_{m_2}^2\log(\tfrac{1}{\varepsilon \delta})}{\varepsilon^2 D^2}\big\}\big\}$ if the matrix game induced by $A$ has a unique Nash equilibrium which is not a PSNE, and
    \item $n_0=c_2\cdot\min\big\{\frac{\log(1/\delta)}{\varepsilon^2},\frac{\log(1/\varepsilon\delta)}{\Delta_{\min}^2}\big\}$ otherwise,
\end{itemize}
where $c_1,c_2$ are absolute constants.
\end{theorem}


%\MakeUppercase
\section{{Results for $n\times 2$ Matrix Games}}
This section is devoted to instance-dependent sample complexity bounds for identifying an $\varepsilon$--good solution and an $\varepsilon$--Nash equilibrium in a $n\times 2$ matrix game that has a unique Nash equilibrium. For a matrix $A\in \mathbb{R}^{n\times 2}$, the bounds will be given in terms of instance-dependent quantities $\Delta_{\min}$ and $\Delta_g$.  
For $\Delta_{\min}$, the natural extension from  the $2 \times 2$ to the $n\times 2$ case is 
\begin{equation*}
    \begin{aligned}
    \Delta_{\min}= \min\{\min_i\{| A_{i1}- A_{i2}|\},\min\limits_{j,k:j\neq k}\{| A_{j1}- A_{k1}|\},\min\limits_{j,k:j\neq k}\{| A_{j2}-A_{k2}|\}\}.
    \end{aligned}
\end{equation*}
To define $\Delta_g$, observe that if the matrix game induced by $A$ has a unique Nash equilibrium $(x^*,y^*)$, then $|\supp(x^*)|=|\supp(y^*)|$ \cite{bohnenblust1950solutions}. 
Suppose that $A$ has a unique Nash equilibrium $(x^*,y^*)$ such that $\supp(x^*)=\{i_1,i_2\}$. Let  $A_{i_11}>A_{i_12}$ and $A_{i_21}<A_{i_22}$ and define
%\begin{equation*}
    \[\Delta_g:= \min\limits_{i\in[n]\setminus\{i_1,i_2\}} r_i\cdot(V^*_{A}-\langle y^*,( A_{i1}, A_{i2})\rangle),\]
%\end{equation*}
where $r_i=\tfrac{| A_{i_11}- A_{i_12}|+| A_{i_21}- A_{i_22}|}{| A_{i_11}- A_{i_12}|+| A_{i_21}- A_{i_22}|+| A_{i1}- A_{i2}|}$.
It is not hard to see that $\min_{i\in[n]\setminus\{i_1,i_2\}}V^*_{A}-\langle y^*,( A_{i1}, A_{i2})\rangle>0$ which implies $\Delta_{g}>0$. 

To provide some intuition for the origin of $\Delta_g$, consider a class of matrices $A_\square=[a,b;c-\square,d-\square;e+\square,f+\square]$ parameterized by $\square \in \R$. In Appendix \ref{apendix:low4:lem1}, we show that for $\Delta=c_0\cdot \Delta_g$ where $c_0$ is an absolute constant, the matrices $A_0$, $A_\Delta$ and $A_{2\Delta}$ have different supports for their respective Nash equilibrium. This implies that we require roughly $1/\Delta_{\g}^2$ samples to determine the support of the Nash equilibrium of $A$. Without this information, we cannot determine an $\varepsilon$--good solution with high probability as the support of the Nash equilibrium affects the value $V_A^*$. The same holds true for finding an $\varepsilon$--Nash equilibirum as every $\varepsilon$--Nash equilibrium is also an $\varepsilon$--good solution. Moreover, to obtain any meaningful upper bound, we require an empirical estimate of $\Delta_g$ to be close to $\Delta_g$ and this is possible when we re-scale the gaps $V^*_{A}-\langle y^*,( A_{i1}, A_{i2})\rangle$ by a factor of $r_i$. In the remainder of this section we make this argument rigorous and further show that roughly $1/\Delta_{\g}^2$ samples suffices to find the support of the Nash equilibrium of $A$. Once the support is identified, we can use the algorithms derived for the $2\times2$ case in the previous two sections.

%\kevin{we should emphasize that the set of $\varepsilon$-Nash equiliria are a subset of $\varepsilon$-good points, thus this is also a lower bound on identifying an $\varepsilon$-Nash equlibrium.  And hten we should say "once the support is identified, we can use the algorithms derived for the $2\times2$ cases above.''}

%\kevin{We're going to need some intuition about how these quantities arise. Also a summary of waht this section shows at a high level in terms of sample complexities}

\subsection{Lower bound with respect to $\Delta_g$}
Consider any matrix game \[A = \bmat{a & b\\ c & d\\ e & f }\] that has a unique Nash equilibrium $(x^*,y^*)$ which is not a PSNE. Without loss of generality assume that $\supp(x^*)=\supp(y^*)=\{1,2\}$. Let us also assume that $a>b,a>c,a>e,d>b,d>c,f>e,f>b$. Observe that in this case we have the following:
\begin{equation*}
     \Delta_g= \frac{(|a-b|+|c-d|)(V^*_{A}-\langle y^*,( e, f)\rangle)}{|a-b|+|c-d|+|e-f|}
\end{equation*}
Let $D_1:=a-b-c+d$ and $D_2:=a-b-e+f$. Let $\Delta:=\frac{(d-b)D_2-(f-b)D_1}{D_1+D_2}$ and $\lambda:=\min\{\frac{(a-b)\Delta}{D_1},\frac{(a-b)\Delta}{D_2}\}$. 
%\kevin{$m$ is a terrible variable name for a real number, especially since we're already using it elsewhere for the size of the matrix. pick another consatnt, perhaps a greek letter.}
This subsection derives a lower bound for $\varepsilon$--good solution for the matrix game $A$.

\begin{theorem}\label{thm:lower4}
Consider the matrix $A$ and fix any $\varepsilon\in (0, \frac{\lambda}{4})$ and $\delta \in (0,1)$. 
Any $(\varepsilon,\delta)$-PAC-good algorithm that returns a pair of mixed strategies $(x,y)\in \simplex_3 \times\simplex_2$ at stopping time $\tau$ satisfies $\mathbb{E}_A[\tau] \geq \frac{ \log(1/30 \delta) }{4\Delta_{g}^2}$.
% . Let $\mathcal{E}_{\tau_0}$ be the event that $\tau\leq \tau_0$ and $(x,y)$ is an $\varepsilon$-good solution. If $\tau_0<\frac{\log (\frac{1}{6\delta})}{12\varepsilon |D|}$, then there exists $\Delta \in \{-\sqrt{3\varepsilon |D|},0,\sqrt{3\varepsilon |D|}\}$ such that $\mathbb{P}_{\Delta}(\mathcal{E}_{\tau_0}^c)>\delta$. %where $\mathbb{P}_{\Delta}$ is the probability distribution induced by the algorithm and the matrix $A_\Delta$.
\end{theorem}

The lower bound considers a class of matrices $A_\square$ parameterized by $\square \in \R$ defined as follows:
\[
A_\square = \begin{bmatrix} 
   a & b \\
   c-\square & d-\square \\
   e+\square & f+\square \\
    \end{bmatrix}.\]
Clearly $A_\square=A$ when $\square=0$. The proof of the theorem, found in  Appendix~\ref{appendix:thm6}, follows from change of measure arguments applied to the instances defined in the following lemma.

\begin{lemma}\label{low4:lem1}
Fix any $\varepsilon\in(0,\frac{\lambda}{4})$. For any pair of mixed strategies $(x',y')\in \simplex_3 \times \simplex_2$,
we have \[\displaystyle
    \max_{B\in\{A_{0},A_{\Delta},A_{2\Delta}\}} |V_B^*-\langle x', By' \rangle|> \varepsilon.\]
% there exists a matrix $B\in\{A_{0},A_{\Delta},A_{2\Delta}\}$ such that $|V_B^*-\langle x', By' \rangle|> \varepsilon$.
%following holds:
%\begin{equation*}
%    
%\end{equation*}
\end{lemma}

%Recall the definitions of $D,\Delta_\min $ and $\Delta_{m_2}$ for the optimal sub-matrix $[a,b;c,d]$. Using the lower bound result for $\varepsilon$-good solution for the $2\times 2$ matrix, we have a lower bound of $\Omega \left(\min\{\frac{1}{\varepsilon^2}\} \right)$ 
%\kevin{Desribe how we combine results of previous sections to obtain the lower bound, as done in above sections.}

%Let $\Delta:=\frac{(d-b)D_2-(f-b)D_1}{D_1+D_2}$ and $m:=\min\{\frac{(a-b)\Delta}{D_1},\frac{(a-b)\Delta}{D_2}\}$. 
%Now let us fix any $\varepsilon\in(0,\frac{m}{4})$ and a natural number $\tau_0<\frac{\log (\frac{1}{6\delta})}{8\Delta_g^2}$. Let $\mathcal{E}(B):=\{(x,y)\in \Delta_3\times \Delta_2:|V_B^*-\langle x,By\rangle|> \varepsilon\}$. Let $\mathcal{E}'(B):=\mathcal{E}(B)\cup\{\bot\}$. For each $B\in \{A_{0},A_{\Delta},A_{2\Delta}\}$, let us add a gaussian noise $\mathcal{N}(0,1)$ to the elements $B_{21}, B_{22},B_{31}$ and $B_{32}$ and define the sample space as $\Omega=\mathbb{R}^{4\tau_0}$ where the first $\tau_0$ components of $\Omega$ are sampled from the element $B_{21}$  the second $\tau_0$ components of $\Omega$ are sampled from the element $B_{22}$, the third $\tau_0$ components of $\Omega$ are sampled from the element $B_{31}$ and the remaining components are sampled from the element $B_{32}$.

%Let us associate every deterministic algorithm $alg$ that returns a guess of $\varepsilon$-Nash equilibrium with an output policy $\phi: \Omega\rightarrow \Delta_2\times \Delta_2\cup \{\bot\}$ ($\phi$ returns $\bot$ when $alg$ takes more than $\tau_0$ samples from the input matrix). Let $\Phi$ be the set of all such output policies $\phi$. Let us define $p_\Delta(x)$ as follows:
%\begin{align*}
%   p_\Delta(x):=&\left(\frac{1}{\sqrt{2\pi}}\right)^{4\tau_0}\exp\big(-\sum_{i=1}^{\tau_0}\frac{(x_i-c+\Delta)^2}{2}-\sum_{i=\tau_0+1}^{2\tau_0}\frac{(x_i-d+\Delta)^2}{2}\\
%   &-\sum_{i=2\tau_0+1}^{3\tau_0}\frac{(x_i-e-\Delta)^2}{2}-\sum_{i=3\tau_0+1}^{4\tau_0}\frac{(x_i-f-\Delta)^2}{2}\big)
%\end{align*}

%Consider an algorithm $\mathcal{A}'$ that stops after taking $\tau$ samples from the input matrix and returns a pair of mixed strategies $(x,y)\in \Delta_3\times\Delta_2$. Let $\mathcal{E}_{\tau_0}$ be the event that $\tau\leq \tau_0$ and $(x,y)$ is an $\varepsilon$-good solution. Now observe that for every $\omega\in \Omega$, the algorithm $\mathcal{A}'$ induces a distribution $p_\omega$ over the set $\Phi$. Also observe that for the matrix $A_\Delta$, the algorithm $\mathcal{A}'$ induces a distribution $\mathbb{P}_\Delta$ over all the events.  Hence, we have the following:
%\begin{equation*}
%    \mathbb{P}_\Delta(\mathcal{E}^c_{\tau_0})=\int_{\omega\in \Omega}\int_{\phi\in \Phi}\mathbbm{1}(\phi(\omega)\in  \mathcal{E}'(A_\Delta))p_\omega(\phi)d\phi\; p_\Delta(\omega)d\omega
%\end{equation*}
%Now we have the following:

%\begin{align}
%   & \max\{\mathbb{P}_0(\mathcal{E}^c_{\tau_0}),\mathbb{P}_\Delta(\mathcal{E}^c_{\tau_0}),\mathbb{P}_{-\Delta}(\mathcal{E}^c_{\tau_0})\}\nonumber \\
%    \geq&  \frac{1}{3}\left(\mathbb{P}_0(\mathcal{E}^c_{\tau_0})+\mathbb{P}_\Delta(\mathcal{E}^c_{\tau_0})+\mathbb{P}_{2\Delta}(\mathcal{E}^c_{\tau_0})\right)\nonumber \\
%     \geq & \frac{1}{3}\int_{\omega\in \Omega}\int_{\phi\in \Phi}\big(\mathbbm{1}(\phi(\omega)\in  \mathcal{E}'(A_0))+\mathbbm{1}(\phi(\omega)\in \mathcal{E}'(A_\Delta))+\mathbbm{1}(\phi(\omega)\in  \mathcal{E}'(A_{2\Delta}))\big)p_\omega(\phi)d\phi\;\nonumber\\
%     & \cdot \min \{p_0(\omega),p_\Delta(\omega),p_{2\Delta}(\omega)\}d\omega\nonumber \\
%     \geq &\frac{1}{3}\int_{\omega\in \Omega}\int_{\phi\in \Phi}p_\omega(\phi)d\phi\; \min \{p_0(\omega),p_\Delta(\omega),p_{2\Delta}(\omega)\}d\omega \label{low4:st1}\\
%    = & \frac{1}{3}\int_{\omega\in \Omega}\min \{p_0(\omega),p_\Delta(\omega),p_{2\Delta}(\omega)\}d\omega \nonumber\\
%    = & \frac{1}{3}\int_{\omega\in \Omega}\min \{p_0(\omega),p_{2\Delta}(\omega)\}d\omega \label{low4:st2}\\
%    \geq & \frac{1}{6}\left( \int_{\omega\in \Omega}\sqrt{p_0(\omega)p_{2\Delta}(\omega)}d\omega\right)^2\tag{Cauchy-Schwartz}\\
%    \geq & \frac{1}{6}\exp\left( -\int_{\omega\in \Omega}\log \left(\frac{p_0(\omega)}{p_{2\Delta}(\omega)}\right)p_0(\omega)dx\right)\tag{Jensen’s}\\
%    =& \frac{1}{6}\exp(-KL(p_0|p_{2\Delta}))\nonumber\\
%    =& \frac{1}{6}\exp\big(-\tau_0KL(\mathcal{N}(c,1)|\mathcal{N}(c-2\Delta,1))-\tau_0KL(\mathcal{N}(d,1)|\mathcal{N}(d-2\Delta,1))\nonumber\\
%    &-\tau_0KL(\mathcal{N}(e,1)|\mathcal{N}(e+2\Delta,1))-\tau_0KL(\mathcal{N}(f,1)|\mathcal{N}(f+2\Delta,1))\big)\tag{Chain rule for product distributions}\\
%    = & \frac{1}{6}\exp(-8\tau_0\Delta^2)\label{low4:st3}\\
%    > & \frac{1}{6}\exp(-8\tau_0\Delta_g^2)\label{low4:st4}\\
%    > & \frac{1}{6}\exp\left(-\log \left(\frac{1}{6\delta}\right)\right) \tag{as $\tau_0<\frac{\log (\frac{1}{6\delta})}{8\Delta_g^2}$}\\
%    = & \delta \nonumber
%\end{align}

%We get (\ref{low4:st1}) as $\forall x\in \mathbb{R}^n$, there exists $B\in \{A_{0},A_{\Delta},A_{2\Delta}\}$ such that $\phi(x)\in \mathcal{E}(B)$. The latter holds due to Lemma \ref{low4:lem1}. We get (\ref{low4:st2}) as $\forall x\in \mathbb{R}^n$, $p_{0}(x)p_{2\Delta}(x)\leq p_{\Delta}(x)^2$. (\ref{low4:st3}) follows from the fact that $KL(\mathcal{N}(\mu_1,1)|\mathcal{N}(\mu_2,1))=\frac{(\mu_1-\mu_2)^2}{2}$. (\ref{low4:st3}) follows from the fact that $\Delta<\Delta_g$ and we prove this fact in the appendix \ref{apendix:low4:lem1}.

%Due to the above analysis, we get the following theorem.

%\begin{theorem}\label{thm:lower4}
%Consider the matrix $A = [a, b;\; c,d;\;e,f ]$ and the set of alternatives $\mathcal{A} = \{A_{0},A_{\Delta},A_{2\Delta}\}$. Fix any  $\delta \in (0,1)$,  $\varepsilon\in (0, \frac{m}{4})$ and a natural number $\tau_0$. Consider an algorithm that stops after taking $\tau$ samples from the input matrix and returns a pair of mixed strategies $(x,y)\in \Delta_3\times\Delta_2$. Let $\mathcal{E}_{\tau_0}$ be the event that $\tau\leq \tau_0$ and $(x,y)$ is an $\varepsilon$-good solution. If $\tau_0<\frac{\log (\frac{1}{6\delta})}{8\Delta_g^2}$, then there exists $\Delta' \in \{0,\Delta,2\Delta\}$ such that $\mathbb{P}_{\Delta'}(\mathcal{E}_{\tau_0}^c)>\delta$.
%\end{theorem}
\subsection{Finding the support in $n\times 2$ matrix games}
\begin{algorithm}[h]
\caption{Find the equilibrium support for a $n\times 2$ matrix}
\begin{algorithmic}[1]
\STATE $T\gets\frac{8 \log (8n/\delta) }{\varepsilon^2}$ 
\FOR{round $t=1,2,\ldots,T$} 
\STATE Sample each element $(i,j)$ once and update the empirical means $\bar A_{ij}$.
\STATE $\Delta \gets \sqrt{2\log(\frac{8nT}{\delta}) / (t)}$.
\STATE $\tilde\Delta_{\min}$ $\gets$ $\min$ $\{\min_i\{|\bar A_{i1}-\bar A_{i2}|\},$ $\min\limits_{j,k:j\neq k}\{|\bar A_{j1}-\bar A_{k1}|\},$ $\min\limits_{j,k:j\neq k}\{|\bar A_{j2}-\bar A_{k2}|\}\}$
\IF{$1\leq \frac{\tilde \Delta_{\min}+2\Delta}{\tilde \Delta_{\min}-2\Delta}\leq \frac{3}{2}$ and the matrix game $\bar A$ has a PSNE }\label{alg3:con1}
\STATE Return the PSNE of $\bar A$.
%\IF{$1\leq \frac{\tilde \Delta_{\min}+2\Delta}{\tilde \Delta_{\min}-2\Delta}\leq \frac{3}{2}$ and the game induced by $\bar A$ has a pure strategy Nash}\label{alg3:con1}
%\STATE Return the pure strategy Nash associated to  $\bar A$.
\ELSIF{$1\leq \frac{\tilde \Delta_{\min}+2\Delta}{\tilde \Delta_{\min}-2\Delta}\leq \frac{3}{2}$ and $\bar A$ does not have a pure strategy Nash} \label{alg3:con2}
\STATE $\forall i\in[n]$, remove the row $i$ of $\bar A$ if there is a row $j$ such that $\bar A_{j1}>\bar A_{i1}$ and $\bar A_{j2}>\bar A_{i2}$.
\FOR{round $t'=t+1,t+2,\ldots,T$}
\STATE Sample each element $(i,j)$ once and update the empirical means $\bar A_{ij}$.
\STATE $\Delta' \gets \sqrt{2\log(\frac{8nT}{\delta}) / (t')}$
\STATE $(x',y')\gets$ Nash equilibrium of $\bar A$
%\IF{$t'=T$} \label{alg3:con3}
\STATE If $t'=T$, return the Nash equilibrium of $\bar A$. \label{alg3:con3}
\IF{$|\supp(x')|= 2$}\label{alg3:con4}
\STATE $\{i_1,i_2\}\gets \supp(x')$
\STATE For all rows $i$,\\ $\tilde{r_i}\gets \frac{| \bar A_{i_11}- \bar A_{i_12}|+| \bar A_{i_21}- \bar A_{i_22}|}{|\bar A_{i_11}-\bar A_{i_12}|+|\bar A_{i_21}- \bar A_{i_22}|+|\bar A_{i1}-\bar A_{i2}|}$
\STATE $\tilde \Delta_g\gets \min\limits_{i:i\notin \{i_1,i_2\}}\tilde{r_i}\cdot (V^*_{\bar A}-\langle y',(\bar A_{i1},\bar A_{i2})\rangle)$
%\IF{$\tilde \Delta_g\geq 4\Delta'$}\label{alg3:con5}
%\STATE Return $\{\{i_1,i_2\},\{1,2\}\}$ as the support of the Nash equilibrium.%\kevin{The other cases return a mixed strategy, yet this returns the support?}\arn{ See the remark after the theorem}
%\ENDIF
\STATE If $\tilde \Delta_g\geq 4\Delta'$, then Return $\{\{i_1,i_2\},\{1,2\}\}$ as the support of the Nash equilibrium.\label{alg3:con5}
\ENDIF
\ENDFOR
\ENDIF
\ENDFOR
\STATE Return the Nash equilibrium of $\bar A$.  \label{alg3:con6}
\end{algorithmic}
\label{alg-ucb-3}
\end{algorithm}

%As the previous sections describe algorithms to obtain solutions given $2 \times 2$ matrices, here we show that  
Next, we characterize the instance-dependent sample complexity of 
Algorithm~\ref{alg-ucb-3} which  finds the support of the unique Nash equilibrium, having cardinality at most two, for the matrix game $A\in \mathbb{R}^{n\times 2}$.
Algorithm~\ref{alg-ucb-3} first samples the elements of $A$ until we can conclude whether the game induced by $A$ has a PSNE or not. If the game has a PSNE, then we return it. If the game induced by $A$ does not have a PSNE, we further sample the elements of $A$ until $\tilde \Delta_g$ is sufficiently large and return the support of the Nash equilibrium of the empirical matrix $\bar A$. Here $\tilde \Delta_g$ is an empirical estimate of $\Delta_g$. If no prior condition is met, the algorithm terminates in the worst case at iteration $t=T=\frac{8 \log(8n/\delta)}{\varepsilon^2}$ and outputs an $\varepsilon$--Nash equilibrium with high probability by Lemma~\ref{lem:trivial:NE}.
The full sample complexity guarantees of the algorithm are described in the following theorem the proof of which is in Appendix~\ref{appendix:thm7}.

\begin{theorem}\label{thm:alg3}
Fix any $\varepsilon > 0$ and $\delta \in (0,1)$. Let $T=\frac{8 \log (8n/\delta) }{\varepsilon^2}$. Consider a game defined by the matrix $A\in \mathbb{R}^{n\times 2}$ with a unique Nash equilibrium $(x^*,y^*)\in\simplex_n\times\simplex_2$. The following hold, where $c_1,c_2,c_3,c_4$ are absolute constants.
\begin{itemize}
    \item If $A$ has a PSNE and $\frac{800\log (\frac{8nT}{ \delta})}{ \Delta_{\min}^2}\leq T$, then  with probability at least $1-\delta$, Algorithm~\ref{alg-ucb-3} samples each element of $A$ for $n_0$ times and returns a PSNE where $n_0= c_1\cdot\frac{\log (\frac{n}{\varepsilon \delta})}{ \Delta_{\min}^2}$.
    \item If $A$ has a PSNE and $\frac{800\log (\frac{8nT}{ \delta})}{ \Delta_{\min}^2}> T$, then  with probability at least $1-\delta$, Algorithm~\ref{alg-ucb-3} samples each element of $A$ for $n_0$ times and either returns a PSNE or an $\varepsilon$--Nash equilibrium where $n_0= c_2\cdot\frac{\log(n/\delta)}{\varepsilon^2}$.
    \item If $A$ does not have a PSNE and $\max\big\{\frac{800\log (\frac{8nT}{ \delta})}{ \Delta_{\min}^2},\frac{722\log (\frac{8nT}{ \delta})}{ \Delta_{g}^2}\big\}<T$, then  with probability at least $1-\delta$, Algorithm~\ref{alg-ucb-3} samples each element of $A$ for $n_0$ times and returns $\supp(x^*)$ and $\supp(y^*)$ where $n_0= c_3\cdot\max\big\{\frac{\log (\frac{n}{\varepsilon \delta})}{ \Delta_{\min}^2},\frac{\log (\frac{nT}{ \delta})}{ \Delta_{g}^2}\big\}$.
    \item If $A$ does not have a PSNE and $\max\big\{\frac{800\log (\frac{8nT}{ \delta})}{ \Delta_{\min}^2},\frac{722\log (\frac{8nT}{ \delta})}{ \Delta_{g}^2}\big\}\geq T$, then  with probability at least $1-\delta$, Algorithm~\ref{alg-ucb-3} samples each element of $A$ for $n_0$ times and either returns $\supp(x^*)$ and $\supp(y^*)$ or an $\varepsilon$--Nash equilibrium where $n_0= c_4\cdot \frac{\log(n/\delta)}{\varepsilon^2}$.
\end{itemize}
\end{theorem}

If Algorithm $\ref{alg-ucb-3}$ returns $\supp(x^*)$ and $\supp(y^*)$ such that $|\supp(x^*)|=|\supp(y^*)|=2$, then  Algorithms $\ref{alg-ucb-1}$ and $\ref{alg-ucb-2}$ can be run on the $2\times 2$ sub-matrix formed by $\supp(x^*)$ and $\supp(y^*)$ and return, with high probability, an $\varepsilon$--good solution and $\varepsilon$--Nash equilibrium, respectively.
