\appendix

\renewcommand\thefigure{\arabic{figure}}
\setcounter{figure}{10}
\renewcommand\thetable{\arabic{table}}
\setcounter{table}{3}

% \textbf{Acknowledgements:} We thank Rinon Gal, Masha Shugrina, Roy Bar-On and Janick Martinez Esturo for helpful discussions and proofreading our paper. We also thank Andrey Voynov for providing access to their sketch-based diffusion work.

\section{Background}

In the following, we include an extended background chapter cut off from the main paper for brevity.

\subsection{Latent diffusion models (LDMs)}
LDMs 
\cite{rombach2021highresolution}
are a class of diffusion models that operate on a latent space instead of directly sampling high-resolution color images.
These models have two main components: a variational autoencoder consisting of an encoder $\mathcal{E}(x)$ and a decoder $\mathcal{D}(z)$, pretrained on the training data, and a denoising diffusion probabilistic model (DDPM) trained on the latent space of the autoencoder. Specifically, let $Z$ be the latent space learned by the autoencoder.\
% They usually consist of an encoder $\mathcal{E}(x)$ and a decoder $\mathcal{D}(z)$, pretrained on the training data, and a denoising diffusion probabilistic model (DDPM) trained on the latent space of the autoencoder.
The objective of the DDPM is to minimize the following expectation:
\begin{equation}
   \mathbb{E}_{z_0 \sim Z, \epsilon \sim \mathcal{N}(0, I), t}[||\epsilon_\phi(z_t, t) - \epsilon || ^ 2],
\end{equation}
where $t$ is the time-step of the diffusion process, $z_t~=~ \sqrt{\alpha_t}z_0 + \sqrt{1 - \alpha_t}\epsilon$ is the input latent image with noise added to it, and $\epsilon_\phi$ is the denoising model, often constructed as a U-Net~\cite{ho2020ddpm}.
Once trained, it is possible to sample from the latent space $Z$ by starting from a random standard Gaussian noise and running the backward diffusion process as described by Ho et al.~\cite{ho2020ddpm}. 
The sampled latent image then can be fed to $D(z)$ to get a final high-resolution image.

\subsection{Score distillation sampling (SDS)}
\label{subsec:prelim}
%\vspace{-10pt}
%\paragraph{Latent Diffusion Models (LDMs):} LDMs \cite{rombach2021highresolution} are a class of diffusion models that operate on a latent space.
%instead of directly sampling high-resolution images.
%These models have two main components: a variational autoencoder consisting of an encoder $\mathcal{E}(x)$ and a decoder $\mathcal{D}(z)$, pretrained on the training data, and a denoising diffusion probabilistic model (DDPM) trained on the latent space of the autoencoder. Specifically, let $Z$ be the latent space learned by the autoencoder.\
%\am{They usually consist of an encoder $\mathcal{E}(x)$ and a decoder $\mathcal{D}(z)$, pretrained on the training data, and a denoising diffusion probabilistic model (DDPM) trained on the latent space of the autoencoder.}
%The objective of the DDPM is to minimize the following expectation:
%\begin{equation}
%    \mathbb{E}_{z_0 \sim Z, \epsilon \sim \mathcal{N}(0, I), t}[||\epsilon_\phi(z_t, t) - \epsilon || ^ 2],
%\end{equation}
%where $t$ is the time-step of the diffusion process, $z_t~=~ \sqrt{\alpha_t}z_0 + \sqrt{1 - \alpha_t}\epsilon$ is the input latent image %with noise added to it, and $\epsilon_\phi$ is the denoising model, often constructed as a U-Net~\cite{ho2020ddpm}. %Once trained, it is possible to sample from the latent space $Z$ by starting from a random standard Gaussian noise and running the backward diffusion process as described by Ho et al.~\cite{ho2020ddpm}. The sampled latent image then can be fed to $D(z)$ to get a final high-resolution image.

%\vspace{-15pt}

First introduced by DreamFusion~\cite{poole2022dreamfusion}, SDS is a method of generating gradients from a pretrained diffusion model, by using its \textit{Score Function} to push the outputs of a parameterized image model towards the mode of the diffusion model distribution. More formally, let $I_\theta$ be an image model with parameters $\theta$. In the case of our application, $I_\theta$ is a neural renderer such as NeRF ~\cite{mildenhall2020nerf} or Instant-NGP ~\cite{mueller2022instant}. We can use a pretrained diffusion model with denoiser $\epsilon_\phi(z_t, t)$, to optimize the following:
\begin{equation}
    \min_{\theta}\mathbb{E}_{\epsilon \sim \mathcal{N}(0, I), t}[||\epsilon_\phi(\sqrt{\alpha_t}I_\theta + \sqrt{1 - \alpha_t}\epsilon, t) - \epsilon|| ^ 2],
\end{equation}
where $t$ is the time-step of the diffusion process, and $\alpha_t$ is a constant scheduling the diffusion forward and backward processes. The Jacobian of the denoiser can be omitted in the gradient of the above expression, to get:
\begin{equation}
    \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I), t}[(\epsilon_\phi(\sqrt{\alpha_t}I_\theta + \sqrt{1 - \alpha_t}\epsilon, t) - \epsilon)\frac{\partial I_\theta}{\partial \theta}].
\end{equation}
%We can push the outputs of the image model to the mode of the distribution of a pretrained %diffusion model with denoiser $\epsilon_\phi(x_t, t)$ by solving:
%Additionally, assume that we have a pretrained text-to-image LDM with denoiser $\epsilon_\phi(z_t, t)$. Then we can sample from the distribution learned by the diffusion model by solving the following optimization problem:
The advantage of SDS is that one can apply constraints directly on the image model making this framework suitable for our application of sketch-guided 3D generation.
%To get a more stable optimization and also avoid the time-consuming backpropagation through the diffusion U-Net, the jacobian term that appears when differentiating the above expression can be omitted. Therefore one can compute the gradient of the above expression as:

%To get a faster, more stable optimization process, the gradient of the above expression is simplified by omitting the diffusion U-Net Jacobian and can be written as:

% \begin{equation}
%     \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I), t}[(\epsilon_\phi(\sqrt{\alpha_t}I_\theta + \sqrt{1 - \alpha_t}\epsilon, t) - \epsilon)\frac{\partial I_\theta}{\partial \theta}].
% \end{equation}
%Applying this gradient to the image model parameters for multiple steps would yield an image model which adheres to the learned probability distribution of the diffusion model. 
%The advantage of SDS is that one could apply constraints directly on the image model making this %framework suitable for our application of sketch-guided 3D generation.
%instead of applying them to the noisy outputs of the diffusion backward process, 

\section{Additional Evaluations}

\subsection{Quantitative Comparisons}
\input{tables/tab_ssim}
\input{tables/tab_lpips}
\input{tables/tab_supp_psnr}

\textbf{Base Model Fidelity.} In Table~\ref{tab:ssim}, We include the SSIM metric to further quantify our method's capability to preserve the base model.

\subsection{Qualitative Comparisons}
\label{sec:supp_qualitative}

%\orrc{TODO}
%Optional:
%* SSIM / LPIPS in addition to PSNR
% * CLIP-R quantitative comparison in addition to CLIP-sim

% \subsection{Comparisons to Latent-NeRF} %~\cite{metzer2022latent}} 
\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/comparison.jpg}
    \captionsetup{font=small}
    \caption{Examples from the modified version of the sketch shape pipeline of Latent-NeRF~\cite{metzer2022latent}}
    \label{fig:comparison}
\end{figure*}
\input{tables/tab_compare}
\textbf{Comparison to Latent-NeRF~\cite{metzer2022latent}.} To the best of our knowledge, we are the first work to employ 2D sketch-based editing of NeRFs. Given that prior works are not directly comparable with our editing setting, we attempt to create a close comparison instead, faithful to the original compared method and fair to evaluate our editing setting. As baseline, we use the method from Latent-NeRF's
~\cite{metzer2022latent} 
3D sketch shape pipeline. We initialize a NeRF with the base object weights, and create a \textit{3D sketch shape}, a mesh, by intersecting the bounding boxes of our 2D sketches in the 3D space. Note that we could also intersect the sketch masks, however, due to view inconsistencies, we found that the results are far inferior. After initializing the NeRF and creating the sketch shape, we proceed to use the sketch shape loss from the paper to preserve the geometry, while editing the NeRF according to the input text. In Fig.~\ref{fig:comparison}, we establish that while this baseline is able to perform meaningful edits, it suffers from two apparent issues: (i) the baseline severely changes the base NeRF, and (ii) the edited region is bound to the coarse geometry of the intersected bounding boxes. To alleviate the latter, one could resort to modeling 3D assets as a sketch shape. However, we show that by using simple multiview sketches, it is possible to perform local editing without going through the effort of modeling accurate 3D masks. Finally, we include a quantitative summary of the preservation ability and the performance of the two methods in Table~\ref{tab:compare}.

%\textbf{Ablation Study} In table \orrc{TODO}, we demonstrate some of the assets %used to conduct the ablation study in Section 4.3 of the main paper.


%https://zju3dv.github.io/neumesh/ - cannot generate new content
%http://editnerf.csail.mit.edu/ - limited by 3d train data
%https://cassiepython.github.io/nerfart/ - discuss limits of stylization
%ClipMesh - cannot generate examples with genus > 0
%Original Dreamfusion - compare to the progressive squirrel example from their paper (show they affect the base)

%\subsection{Intersection over Sketch}
%\orrc{TODO}
%* Discuss if we want to include the low iou results also
%\subsection{Generation from scratch}
%\orrc{TODO}
% (currently failed maybe needs pretraining to zero density)

\begin{figure*}[t]
   \centering
    \begin{overpic}[width=0.8\linewidth,tics=10, trim=0 0 0 0,clip]{figs/ui.jpg}
   \end{overpic}
   \captionsetup{font=small}
    \caption{The interactive UI allows users to sketch over a pretrained NeRF. \textbf{Top row}: The user draws scribbles from two different views using "Sketch Mode". \textbf{Bottom left}: After pressing "Add sketch", the scribbles are filled to generate masks, ready to be used with our pipeline. \textbf{Bottom right}: The bounding box marks the sketches intersection region, where the edit takes place.}
    \label{fig:ui}
 \end{figure*}

 \begin{figure*}[h]
    \centering
    \hspace{0.25in}
    \begin{overpic}[width=0.8\linewidth,tics=10, trim=0 0 0 0,clip]{figs/geometry_nc.jpg}
    % \put(-20, 12){\large{Depth map}}
    % \put(-20, 40){\large{Normal map}}
    % \put(-15, 65){\large{RGB}}
    \end{overpic}
    %\includegraphics[width=0.8\linewidth]{figs/geometry.jpg}
    \captionsetup{font=small}
    \caption{From top to bottom: color, normal and depth maps of outputs generated by our method.}
    \label{fig:geometry}
\end{figure*}


\section{Implementation Details}%\vspace{-3\baselineskip}
\vspace{-10pt}
This section contains additional implementation details omitted from the manuscript.
\vspace{-35pt}
\subsection{Text Prompts}
\vspace{-10pt}
In the following, we include the full list of prompts that were used to generate the examples within the paper.
\vspace{-40pt}
\begin{itemize}
\setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt}
    \item "A cat wearing a chef hat"
    \item "A cherry on top of a sundae"
    \item "A red flower stem rising from a potted plant"
    \item "A teddy bear wearing sunglasses"
    \item "A candle on top of a cupcake"
    \item "An anime girl wearing a brown bag"
    \item "An apple on a plate"
    \item "A Nutella jar on a plate"
    \item "A globe on a plate"
    \item "A tennis ball on a plate"
    \item "A cat wearing a red tie"
    \item "A cat wearing red tie wearing a chef hat"
    \item "A 3D model of a unicorn head"
\end{itemize}
\vspace{-40pt}

Additionally, similar to 
DreamFusion\cite{poole2022dreamfusion} 
we use directional prompts, where based on the rendering view, we modify prompt $\textbf{T}$ as follows:
\vspace{-40pt}
\begin{itemize}
\setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt}
    \item "$\textbf{T}$, overhead view"
    \item "$\textbf{T}$, side view"
    \item "$\textbf{T}$, back view"
    \item "$\textbf{T}$, bottom view"
    \item "$\textbf{T}$, front view"
\end{itemize}
\vspace{-60pt}
%\subsection{Extended Settings}
%* Discuss details of Instant-NGP and preventing gradient flow to original base model (if time permits, add an experiment)

%* Discuss Albedo v.s. Lambertian shading (didn't have it in the framework at the time of conducting experiments)

%\vspace{-3\baselineskip}

\subsection{Interactive UI}
\vspace{-\baselineskip}

Since our method requires user interaction, we include an interactive user interface with our implementation (Fig.~\ref{fig:ui}). The user interface allows users to optimize newly reconstructed base NeRF models, or load pretrained ones. To perform edits, users can position the camera on the desired sketch view, and draw scribbles to guide SKED. By pressing "Add Sketch", scribbles are filled and converted to masked sketch inputs, ready to be used with our method.
\vspace{-30pt}
\subsection{Quality Notes}

Our implementation uses an early version of Stable-DreamFusion
~\cite{stable-dreamfusion}
which does not include the optimizations very recently suggested by
Magic3D~\cite{lin2022magic3d}. 
In contrast to DreamFusion
~\cite{poole2022dreamfusion} 
and Magic3D
~\cite{lin2022magic3d},
which use commercial diffusion models with larger language models 
\cite{imagen2022saharia, balaji2022eDiff-I},
we rely on Stable Diffusion
\cite{rombach2021highresolution}, 
which is less sensitive to directional prompts. Our results are therefore not comparable in visual quality to these previous works.


% In addition, different to DreamFusion, though possible, we do not leverage the Lambertian lighting model in our architecture. The reason behind this is technical: we found this feature produces inferior results with the backbone and implementation we use.





\section{Additional Assets}
%\subsection{Ablation Assets}
%The images Mehdi used to compute IoS
%\orr{Aryan, Mehdi: please complete}

\subsection{Geometry and Depth}
In addition to RGB images, we share examples highlighting the geometry of our method's outputs. In Fig.~\ref{fig:geometry} we include the normal maps and depth maps of two output samples.


% \begin{figure*}
%    \centering
%     \begin{overpic}[width=0.8\linewidth,tics=10, trim=0 0 0 0,clip]{figs/ui.jpg}
%    \end{overpic}
%    \captionsetup{font=small}
%     \caption{The interactive UI allows users to sketch over a pretrained NeRF. \textbf{Top row}: the user draws scribbles from two different views using "Sketch Mode". \textbf{Bottom left}: after pressing "Add sketch", the scribbles are filled to generate masks, ready to be used with our pipeline. \textbf{Bottom right}: the bounding box marks the sketches intersection region, where the edit takes place.}
%     \label{fig:ui}
%  \end{figure*}

%  \begin{figure*}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figs/geometry.jpg}
%     \captionsetup{font=small}
%     \caption{Normal and depth maps generated by our method.}
%     \label{fig:geometry}
% \end{figure*}
