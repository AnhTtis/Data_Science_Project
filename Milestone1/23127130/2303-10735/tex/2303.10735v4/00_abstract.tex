\begin{abstract} 
    Text-to-image diffusion models are gradually introduced into computer graphics, recently enabling the development of Text-to-3D pipelines in an open domain. However, for interactive editing purposes, local manipulations of content through a simplistic textual interface can be arduous. Incorporating user guided sketches with Text-to-image pipelines offers users more intuitive control. 
    Still, as state-of-the-art Text-to-3D pipelines rely on optimizing Neural Radiance Fields (NeRF) through gradients from arbitrary rendering views, conditioning on sketches is not straightforward. In this paper, we present SKED, a technique for editing 3D shapes represented by NeRFs. Our technique utilizes as few as two guiding sketches from different views to alter an existing neural field. The edited region respects the prompt semantics through a pre-trained diffusion model. To ensure the generated output adheres to the provided sketches, we propose novel loss functions to generate the desired edits while preserving the density and radiance of the base instance. We demonstrate the effectiveness of our proposed method through several qualitative and quantitative experiments. \href{https://sked-paper.github.io/}{https://sked-paper.github.io/}
    
\end{abstract}

% Our generated outputs naturally blend with the original model while also respecting the conditioning sketches as closely as possible.

% \begin{abstract}
 % While text-to-image models have revolutionized computer graphics, working solely with text prompts is cumbersome for local editing and manipulation of visual content. Incorporating sketches into text-to-image pipelines offers users a more intuitive level of control. However, due to the absence of a direct text-to-3D model, the challenge of incorporating sketches as a guide for 3D shape editing is particularly difficult. In this paper, we present SKED, a technique for editing 3D shapes represented by NeRFs. This technique utilizes as few as two guiding sketches from different views to generate/edit a 3D shape that maintains the naturalness of the output through a pre-trained text-to-image diffusion model while also respecting the two sketches as closely as possible. To ensure that the generated output adheres to the provided sketches, we have designed novel loss functions to generate the desired edits while preserving the geometry and the color of the base object.
 % Through several qualitative and quantitative experiments, we show the effectiveness of our proposed method in editing 3D shapes.
% \end{abstract}

% \begin{abstract}
%  While text-to-image models have revolutionized computer graphics, working solely with text prompts is insufficient for effective editing and manipulation of visual content. Incorporating sketches into text-to-image models offers users a more intuitive level of control. However, due to the absence of a direct text-to-3D model, the challenge of incorporating sketches as a guide for 3D shape editing is particularly difficult. In this paper, we present SKED, a technique for editing 3D shapes represented by NeRFs. This technique utilizes \orrc{as little as two} 
%  % only a few (e.g. two) \aryan{better write at least two here.}
%  guiding sketches from different views to generate/edit a 3D shape that maintains the naturalness of the output through a pre-trained text-to-image diffusion model while also respecting the two sketches as closely as possible. To ensure that the generated output adheres to the provided sketches, we have designed novel loss functions to generate the desired edits while preserving the geometry and the color of the base object.
%  Through several qualitative and quantitative experiments, we show the effectiveness of our proposed method in editing 3D shapes.
% \end{abstract}