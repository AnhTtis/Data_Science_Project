\section{Related work}
\label{sec:related}


\paragraph{Sketch-Based 3D Modeling.}

Since its inception in the late twenties, traditional 2D animation has been concerned with creating believable depictions of 3D forms. Highly acclaimed art guidebook, \textit{The Illusion of Life}, \cite{johnston1981disney}, advocated for solid three-dimensional drawings to practice "weight, depth and balance." With the advancement of computer animation, these principles have been widely adopted
\cite{lasseter1987traditional}.
%
Sketch-based modeling is typically concerned with stitching and inflating, and user-drawn sketches to 3D meshes \cite{williams1991shading2d}. Starting with Teddy~\cite{igarashi1999teddy}, early works focused on converting scribbles of 2D contours to intermediate forms such as closed polylines \cite{karpenko2006smoothsketch} or implicit functions  \cite{karpenko2002freeform, tai2004prototype, alexe2004interactive, schimdt2005shapeshop, bernhardt2008matisse}. Since lifting a single-view sketch to 3D is an under-constrained problem, additional constraints are usually introduced, such as correlating the inflated thickness with chordal axis depth of curved shapes \cite{igarashi1999teddy}, inferring shape and depth from user annotations \cite{schimdt2005shapeshop, karpenko2006smoothsketch, gingold2009structured, olsen2012natureasketch, tuan2015shading, yeh2017InteractiveHR, li2017bendsketch, jayaraman2018globallyconsistent}, using existing reference models like human figures \cite{turquin2007cloth}, and solving a system based on user constraints \cite{nealen2007fibermesh, joshi2008repousse, sykora2014inkandray, feng2016interactive, dvoro2020monstermash}. 
More recently, data-driven approaches were suggested to lift and reconstruct objects from multi-view sketches with Conv-nets \cite{lun2017sketchreconstruction, delanoy20183d, li2018robust}. 
%
Our work departs from the former line of research by limiting a generative model to operate within the boundaries of a sketched region. By utilizing the strength of pretrained diffusion models conditioned on language, we avoid the intricacies of explicitly tuning inflation parameters or collecting large-scale train sets while being able to predict texture and shading simultaneously.
\vspace{-15pt}
\paragraph{Diffusion Models.}

Diffusion models \cite{sohl2015diffusion, ho2020ddpm, song2020denoising, song2020improvedsd} have emerged as an increasingly popular technique for generating diverse, high-quality images.
%
More recently, they've been used to form state-of-the-art text-to-image models \cite{rombach2021highresolution} 
 by introducing language embeddings trained on massive amounts of data \cite{imagen2022saharia, balaji2022eDiff-I}. 
 % \orr{tbd add some works on conditioning (classifier-free / guided}
%
Diffusion models are amicable for other conditioning modalities. Related to our work, \cite{meng2022sdedit} introduced conditioning on sketches. \cite{voynov2022sketch} trained a differentiable edge detector used to compute the edge loss per diffusion step, and \cite{cheng2023wacv} allowed finer granularity of control of generated images by distinguishing between sketch, stroke and level of realism.
%
\cite{zhang2023controlnet} is a contemporary publication which enables additional input conditions by augmenting large-diffusion models on small task-specific datasets.

Another line of works experimented with applying diffusion directly in 3D for generating point clouds \cite{nichol2022pointe} or 2D projections forming the feature representation of neural fields
\cite{shue2022triplanediffusion, anciukevicius2022renderdiffusion}. 
While showing potential, the difficulty lies in scaling them due to the large amount of 3D data required.

\vspace{-15pt}
\paragraph{Neural Fields.}

Neural Radiance Fields (NeRF) \cite{mildenhall2020nerf} have generated massive interest as means of representing 3D scenes using deep neural networks.
%
Since then, a flurry of works has improved various aspects of optimized neural fields, yielding higher reconstruction quality \cite{barron2021mipnerf, wang2021nerfsr, barron2022mipnerf360, verbin2022refnerf}.
%
Neural field backbones, in particular, have become more structured and compressed \cite{peng2020convoccnet, takikawa2021nglod, liu2020nsvf, chan2021eg3d, takikawa2022vqad}. The pivotal work of \cite{mueller2022instant} introduced an efficient hash-based representation that allows NeRF optimizations to converge within seconds, effectively paving the way for interactive research directions on neural radiance fields.
% Since then, a flurry of works have improved various aspects of optimized neural fields, with higher reconstruction quality  \cite{barron2021mipnerf, wang2021nerfsr, barron2022mipnerf360, verbin2022refnerf}, using fewer or single views \cite{jain2021dietnerf, niemeyer2021regnerf, xu2022sinnerf, zhou2022sparsefusion, suhail2022generalizable}, and improving robustness in face of noisy cameras \cite{lin2021barf, meng2021gnerf}.
% %
% Neural field backbones in particular have become more structured and compressed \cite{peng2020convoccnet, takikawa2021nglod, liu2020nsvf, chan2021eg3d, takikawa2022vqad}. The pivotal work of \cite{mueller2022instant} introduced an efficient hash-based representations that allow neural fields optimizations to converge within seconds, effectively paving the way for interactive research directions on neural radiance fields.
%
Recent works have explored interactive editing of neural radiance fields through manipulation of appearance latents
\cite{liu2021editing, niemeyer2021giraffe, park2021hypernerf, chan2021eg3d}, by interacting with proxy representations \cite{yuan2022nerf, bao2022neumesh}, through segmented regions and masks \cite{kobayashi2022distilledfeaturefields, li2022designer3d, mirzaei2022laterf, kuang2022palettenerf} and text-based stylization \cite{gu2021stylenerf, wang2022nerfart, zhang2022arf, mirzaei2022laterf, gao2022get3d}.

Neural fields are an intriguing way to fully generate 3D models because, unlike meshes, they don't depend on topological properties such as genus and subdivision \cite{khalid2022clipmesh, gao2022get3d}.
 % Neural fields make for an intriguing representation for full generation of 3D models, as contrary to widely-used meshes, they are agnostic to topological properties like genus and subdivision 
%
Initial generative text-to-3D attempts with NeRF backbones took advantage of a robust language model \cite{radford2021clip} to align each rendered view on some textual condition \cite{jain2021dreamfields, wang2021clip}. However, without a geometric prior, \cite{radford2021clip} failed to produce multiview-consistent geometry.
%
\cite{xu2022dream3d} used language guidance to generate 3D shapes based on shape embeddings, but their approach still requires large 3D datasets to generate template geometry.

DreamFusion~\cite{poole2022dreamfusion} and \cite{wang2022scorejacobian} avoid the scarcity of 3D data by harnessing 2D diffusion models pretrained on large-scale, diverse datasets \cite{schuhmann2022laionb}.
Their idea optimizes NeRF representations with score function losses from pretrained diffusion-models \cite{rombach2021highresolution, imagen2022saharia}, where the 2D diffusion process provides gradients for neural fields rendered from random views, and the process is amortized on many different views until an object is formed.
%
Magic3D~\cite{lin2022magic3d} further improved the quality and performance of generated 3D shapes with a 2-step pipeline which fine-tunes an extracted mesh. Their pipeline also allows for global prompt-based editing and stylization.
%
Concurrently, Latent-NeRF~\cite{metzer2022latent} suggested optimizing neural fields in the diffusion latent space. Their work also suggested 3D bounded volumes as an additional constraint for guiding the generation process.

Our work builds on a simplified framework of \cite{poole2022dreamfusion}, which operates in color space and aims to combine traditional sketch-based modeling constraints with the generative power of recent advances in the field. Our pipeline is a zero-shot generative setting, requiring no dataset and only text and sketch inputs from the user.
