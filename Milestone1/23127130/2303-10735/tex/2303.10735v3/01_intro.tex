\section{Introduction}

\label{sec:intro}

% \textit{"Drawing and colour are not separate at all; in so far as you paint, you draw. The more the colour harmonizes, the more exact the drawing becomes."} - Paul Cezanne.

Art is a reflection of the figments of human imagination. 
While many are limited in their practical creative capabilities, by pushing the boundaries of digital media, new ways can be created for casual artists and experts alike to express their ideas. At the same time, current neural generative art takes away much of the control from humans. In this work, we attempt to take a step towards restoring some of that control, enabling neural networks to complement users and naturally extend their skills rather than taking hold over the generative process.

% \orr{TBD - make the opening colorful : 1. Add quote:  2. Elaborate: art is a rendering of figments of imaginations of humans. Most people are limited in their drawing capabilities, and by pushing the boundaries we allow new ways for casual artists and experts alike in expressing ideas. At the same time, neural generative art takes a lot of the control away. Here, we want to give back some of this control to humans, such that neural networks complement them and compensate their lack of skills, rather than replacing them.}

% The field of image synthesis has been significantly propelled by neural generative models, particularly by the latest text-to-image models that predominantly rely on large language-image models ~\cite{balaji2022eDiff-I, ramesh2022dalle, rombach2021highresolution, imagen2022saharia}. These models have revolutionized the field of computer vision as they can produce astonishing visual outcomes from text prompts only.

The field of image synthesis has been significantly propelled by neural generative models, particularly by the latest text-to-image models that predominantly rely on large language-image models ~\cite{balaji2022eDiff-I, ramesh2022dalle, rombach2021highresolution, imagen2022saharia}. These models have revolutionized the field of computer vision, as they can produce astonishing visual outcomes from text prompts alone.

The ability of text-to-image models has sparked a wave of editing methods that utilize these models. Many of these techniques rely on prompt editing ~\cite{ fu2022shapecrafter, hertz2022prompt, kawar2022imagic,lin2022magic3d,mokady2022null, poole2022dreamfusion}. Nevertheless, simplifying the interface to text alone means users lack the necessary level of granularity to produce their exact desired outcomes.
% which is} insufficient for effectively editing local content. 
% editing and manipulating visual content, as users lack the necessary level of control to achieve their desired outcomes
Sketch-guided editing, on the other hand, provides intuitive control that aligns with user's conventional drawing and painting skills. By incorporating user-guided sketches into text-to-image models, powerful editing systems can be created, offering a high degree of flexibility and fine-grained control for manipulating visual content~\cite{zhang2023controlnet, voynov2022sketch}.

Although sketch-guided and text-driven methods have proven successful in generating and manipulating 2D images \cite{meng2022sdedit, voynov2022sketch, cheng2023wacv}, it immediately raises the intriguing question of whether a similar approach could be developed to edit 3D shapes. 
Since direct text-to-3D models require an abundance of data to scale, state-of-the-art 3D generative models, such as DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d}, which build on the capabilities of text-to-image models, may be considered as an alternative.
% Due to the difficulty of scaling general direct text-to-3D models, incorporating conditions into a text-to-3D model is not straightforward. Thus, state of the art 3D generative models, such as DreamFusion~\cite{poole2022dreamfusion} \orrc{and Magic3D~\cite{lin2022magic3d}}, which build on the capabilities of text-to-image models, may be considered as an alternative.
However, maintaining control via conditioning with such models remains a challenging task, as these generative pipelines optimize a Neural Radiance Field (NeRF) \cite{mildenhall2020nerf} by amortizing gradients from a multitude of 2D views. In particular, providing consistent sketches across all possible views presents a hurdle for users. Instead, a plausible user interface should act with guidance from as few views as possible, e.g. up to two or three.


In this paper, we present \textbf{SKED}, a \textbf{SK}etch-guided 3D \textbf{ED}iting technique. Our method acts on reconstructed or generated NeRF models. We assume a text prompt and a minimum of two sketches as input and provide output edits over the neural field faithful to the input conditions.
Meeting all input requirements can be challenging as the text prompt may not match the sketch's semantics, and sketch views may lack coherence.
To undertake this complex task, we conceptually break it down into two subtasks that are easier to handle: one that depends on pure geometric reasoning and the other that exploits the rich semantic knowledge of the generative model. These two subtasks work together, with the former providing a coarse estimate of location and boundary, and the latter adding and refining geometric and texture details through fine-grained operations.


Our experiments highlight the effectiveness of our approach for editing various pretrained NeRF instances. We introduce assorted accessories, objects, and artifacts, which are generated and blended into the original neural field seamlessly.
Finally, we validate our method through quantitative evaluations and ablation studies to assert the contribution of individual components in our method. 
% By presenting examples in the paper, we illustrate that our method can generate realistic 3D artifacts with accurate texture and geometry using only a few basic sketches.



% Due to the absence of a direct text-to-3D model, incorporating conditions into a text-to-3D model is not straightforward. Thus, 3D generative models, such as DreamFusion~\cite{poole2022dreamfusion}, which build on the capabilities of text-to-image models, may be considered as an alternative.
% However, this is a challenging task since DreamFusion generates a NeRF by integrating many different 2D views. It is very hard to provide consistent sketches across all possible views. The challenge is to use sketches as a guide on only a few views (e.g., two or three) and generate 3D edit of the existing NeRF that is subject to being edited. 

% In this paper, we present \textbf{SKED}, a \textbf{SK}etch-guided 3D \textbf{ED}iting technique, that takes as input a text prompt and a few (two or more) sketches and edits a 3D given object represented as a NeRF in a geometrically plausible and controlled way. 
% We acknowledge the difficulty of this task, as there are no existing text-to-3D generative models available for manipulating the geometry of the existing object based on a text prompt. 
% To undertake this complex task, we conceptually break it down into two simpler subtasks that are easier to handle: one that depends on pure geometric reasoning and the other that exploits the rich semantic knowledge of generative model. These two subtasks work together, with the former providing a coarse estimate of location and the latter adding and refining geometric and texture details through fine-grained operations.

% Our experiments showcase the effectiveness of our approach in performing sketch-guided text-based edits on different base nerfs by introducing various accessories, objects, and artifacts. We also conduct ablation studies and experiments to evaluate the performance of individual components in our method. By presenting examples in the paper, we illustrate that our method can generate realistic 3D artifacts with accurate texture and geometry using only a few basic sketches.

%\dcc{Add here the traditional paragraph that tell about what we achieved and evaluated}