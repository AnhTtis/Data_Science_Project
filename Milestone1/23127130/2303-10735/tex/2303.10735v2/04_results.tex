\section{Results and Evaluation}

\label{sec:results}
\input{tables/tab_psnr}
We use Stable-DreamFusion's open-source GitHub repository~\cite{stable-dreamfusion} and integrate with kaolin-wisp's renderer for an interactive UI~\cite{KaolinWispLibrary}.
%Our method utilizes the open-source GitHub repository of Stable-Dreamfusion~\cite{stable-dreamfusion}. %For interactive UI, we integrate with the renderer from kaolin-wisp~\cite{KaolinWispLibrary}.
In all our experiments, unless stated otherwise, we set $\lambda_{pres}, \lambda_{sil}$, $\lambda_{sp}$ and $\lambda_{c}$ to $5\times 10^{-6}$, $1$ and $5\times 10^{-4}$ and $5$ respectively. 
%The warm-up period for the occupancy grid pruning is set to 1,000 iterations. We also set the camera pose range to ensure that the sketch region remains visible in all sampled views. For large sketch regions, we use a Kd tree \cite{2020SciPy-NMeth} to implement Equation~\ref{eq:per_view_dist} with efficient queries. We use ADAM~\cite{kingma2014adam} optimizer with a learning rate of $0.005$ and apply the exponential scheduler which decays it by $0.1$ by the end of the optimization. 
 We run our algorithm for $10,000$ iterations,  taking approximately 30-40 minutes on a single NVIDIA RTX 3090 GPU. Note some inputs takes fewer iterations to converge.
 %We run the algorithm for $10,000$ iterations taking approximately 30-40 minutes on a single NVIDIA RTX 3090 GPU, though in most examples we found that it converges with much fewer iterations.
 Additional implementation details are included with the supplementary material.
 %For our experiments, we use both publicly available 3D assets, and artificial assets generated by Stable-Dreamfusion guided only by text. In the following, we demonstrate the results of our algorithm on various base shapes and justify its components and design choices through ablation studies.






\begin{figure*}[t]
  \centering\includegraphics[width=0.95\linewidth]{figs/updated_results2.jpg}
  \captionsetup{font=small}
  \caption{Examples of using SKED to edit various objects reconstructed with InstantNGP~\cite{mueller2022instant} (anime girl) or generated with DreamFusion~\cite{poole2022dreamfusion} (plant, sundae, teddy bear, sundae, cupcake). All examples were edited using two sketch views and the text prompt.}
  \label{fig:results}
\end{figure*}


%We show the effectiveness of our editing technique through a series of qualitative and quantitative experiments.
% \subsection{Implementation Details}
% \label{subsec:impl}
% Our method utilizes the open-source GitHub repository of Stable-Dreamfusion~\cite{stable-dreamfusion}. For interactive UI, we integrate with the renderer from kaolin-wisp~\cite{KaolinWispLibrary}.
% In all our experiments, unless stated otherwise, we set $\lambda_{pres}, \lambda_{sil}$, $\lambda_{sparse}$ and $\lambda_{color}$ to $5\times 10^{-6}$, $1$ and $5\times 10^{-4}$ and $5$ respectively. The warm-up period for the occupancy grid pruning is set to 1,000 iterations. We also set the camera pose range to ensure that the sketch region remains visible in all sampled views. For large sketch regions, we use a Kd tree \cite{2020SciPy-NMeth} to implement Equation~\ref{eq:per_view_dist} with efficient queries. We use ADAM~\cite{kingma2014adam} optimizer with a learning rate of $0.005$ and apply the exponential scheduler which decays it by $0.1$ by the end of the optimization. 
% %We run our algorithm for \am{$10,000$} iterations (usually takes fewer iterations to converge), which takes approximately 30-40 minutes on a single NVIDIA RTX 3090 GPU. 
% We run the algorithm for $10,000$ iterations taking approximately 30-40 minutes on a single NVIDIA RTX 3090 GPU, though in most examples we found that it converges with much fewer iterations.
% For our experiments, we use both publicly available 3D assets, and artificial assets generated by Stable-Dreamfusion guided only by text. In the following, we demonstrate the results of our algorithm on various base shapes and justify its components and design choices through ablation studies.


\subsection{Qualitative Results}
\label{subsec:qualitative}

\textbf{Sketch and Text Control.} Fig.~\ref{fig:results} demonstrates examples 
%of edits performed with 
of SKED on a variety of objects and shapes. Evidently, our method is able to satisfy the coarse geometry defined by drawn user sketches, and at the same time naturally blend semantic details according to the given text-prompts. Note that the sketch is not required to be accurate or tight: by making the contour curve more complex, the user can further force the pipeline to generate a specific shape.




\begin{figure*}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{figs/balls_smaller.jpg}
  \captionsetup{font=small}
  \caption{Examples with a single set of sketches and a variety of text prompts. Our method is able to respect the geometry of the sketches while adding details to fit the different prompts' semantics.}
  \label{fig:balls}
\end{figure*}

Next, we show that given a fixed pair of multiview sketches, our method produces semantic details to fit a diverse set of text-prompts (Fig.~\ref{fig:balls}). 
Note that our method can generate details within the sketch boundary (e.g. Nutella jar) even if the sketch doesn't match the text-prompt description.
%Here, we point out that even when the sketch does not fit a shape matching the description of the text-prompt, our method is still able to generate details while respecting the sketch boundary (i.e. the Nutella jar example). 
In Fig.~\ref{fig:results} we also present the complementary case, by re-using the same text prompts and switching through different sketch sets, our method has the flexibility to produce localized edits (i.e., "cherry on top of a sundae").

\textbf{Base Model Distribution.} \label{subsubsec:basemodeldist} Our method assumes edits are applied on top of a base NeRF model. We explore both pretrained reconstructions from multiview images, generated with \cite{mueller2022instant}, and generated outputs from DreamFusion~\cite{poole2022dreamfusion} using the same diffusion model we use for editing \cite{rombach2021highresolution}. 
%Reconstructed objects renderings are assumed to be in distribution of the underlying diffusion model. 
The renderings of reconstructed objects are assumed to follow the distribution of the underlying diffusion model.
Our method performs successfully on both reconstructed examples: Anime Girl, Plate, Cat, Horse (Fig. \ref{fig:results}, \ref{fig:balls}, \ref{fig:progressive}, \ref{fig:overlay}) and generated ones: Plant, Teddy, Cupcake, Sundae (Fig.~\ref{fig:results}).


\input{tables/tab_ios}
%\input{tables/tab_clipsim}

\textbf{Progressive Editing.} Our method can be used as a sequential editor where one reconstructs or generates a NeRF, and then progressively edits it. In Fig.~\ref{fig:progressive} we exhibit a two-step editing by first reconstructing a cat object with \cite{mueller2022instant}, then generating a red tie, followed by adding a chef hat.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figs/progressive_small.png}
    \captionsetup{font=small}
    \caption{Progressive editing. The cat is first edited by adding a red tie, and then a chef hat is added in a subsequent edit.}
    \label{fig:progressive}
\end{figure}

\textbf{Preservation Sensitivity.} We also demonstrate the overlay of the sketch masks with the edited NeRF rendered from the sketch views and the effect of changing $\beta$ (Eq.~\ref{eq:modulate}) in Fig.~\ref{fig:overlay}. It is evident that increasing $\beta$ changes the base NeRF more and more edits appear outside the sketch regions.
% To analyze how much the edits respect the sketches, we additionally demonstrate the overlay of the sketch masks with the edited NeRF rendered from the sketch views, Fig.~\ref{fig:overlay}. 

%with the appendix.




%\orr{Missing: (1) A figure for single prompt different sketches + discussion on sensitivity param doing it's job. (2) The effect of adding no view, 1 view, 2 views, 3 view, non-orthogonal views. (3) Generation from scratch within bounded sketch. The discussion of in / out of distribution base models can go in here, and we can then connect this directly to the progressive examples figure}
%





%Here, We also evaluate our method quantitatively through a series of experiments. 
To the best of our knowledge, our method is the first to perform sketch-guided editing on neural fields. Hence, at the absence of an existing benchmark for systematic comparison, we also suggest a series of tests to quantify various aspects of our method. We conduct our evaluation using a set of five representative samples using the setting from Section~\ref{sec:results}. Each sample includes a base shape, a pair of hand drawn sketches and a guiding text prompt. Comparisons to "Text-only" ignore the input sketches and apply prompt editing. For a fair comparison, all experiments use the same diffusion model and implementation framework of~\cite{stable-dreamfusion}. In the following, we establish that all three metrics are necessary to quantify the method's value.

\textbf{Base Model Fidelity.} We quantify our method's ability to preserve the base field outside of the sketch area using PSNR (Table~\ref{tab:psnr}). As ground truth, we use $\{C \backslash M \}_{i=1}^{N=2}$, the renderings from the base model, excluding the filled sketch regions. We measure the PSNR w.r.t. to the output sketch view rendered with edited field $\edittednerf$, and the output from \cite{poole2022dreamfusion} using the same camera view. Our results show that across all inputs, our method consistently preserves the original base field content, compared to the Text-only method which lacks this ability. Note that a method may obtain a perfect PSNR if it does not change the original neural field. Therefore, we further measure the quality of change as well.


 \begin{figure}
   \centering
    \begin{overpic}[width=\linewidth,tics=10, trim=0 0 0 0,clip]{figs/sensitivity.jpg}
    \put(10,-3){$\beta=0.005$}
    \put(45,-3){$\beta=0.1$}
    \put(78.0,-3){$\beta=1.0$}
   % \includegraphics[width=\linewidth]{figs/sensitivity.jpg}
   \end{overpic}
   \captionsetup{font=small}
    \caption{Sensitivity control. Depending on the sensitivity value determined by $\beta$ in Eq.~\ref{eq:modulate}, our method can either edit only the sketched region and minimally modify the rest of the neural field, or produce larger edits outside the sketch regions (softer blending). We display the overlay of sketches v.s. edited output.}
    \label{fig:overlay}
 \end{figure}




\begin{figure*}[t]
    \centering
    \begin{overpic}[width=0.9\textwidth,tics=10, trim=0 0 0 0,clip]{figs/ablation2.jpg}
    % \includegraphics[width=\linewidth]{figs/ablation.jpg}
    \put(6.5,33){Base Model}
    \put(26.0,33){Text Only}
    \put(46.0,34){Ours}
    \put(44,32){$\Lstroke_{pres}, \mathcal{L}_{sil}$}
    \put(64.0,34){Ours}
    \put(62,32){$\mathcal{L}_{pres}, \Lstroke_{sil}$}
    % \put(61,33){$\mathcal{L}_{pres}, \Lstroke_{sil}$}
    \put(86.25,34){Ours}
    \put(87.0,32){\textit{full}}
    \end{overpic}
    \captionsetup{font=small}
    \caption{Ablation Study. We demonstrate of the effect of our suggested losses on editing neural fields (zoom in for details). The prompts used are \textit{"A cat wearing a \textcolor{Periwinkle}{chef hat}"} and \textit{"A \textcolor{red}{red flower stem} rising from a potted plant"}. All methods were initialized with the same base models (leftmost column), and optimized with the same diffusion model \cite{rombach2021highresolution}. Text-only uses the public re-implementation of \cite{poole2022dreamfusion}. The rightmost method shows our full pipeline, compared to ablated versions of it omitting $\mathcal{L}_{pres}$ and $\mathcal{L}_{sil}$ respectively.}
    \label{fig:ablations}
\end{figure*}
\subsection{Quantitative Results}
\label{subsec:quantitative}

\textbf{Sketch Filling.} To gauge whether our method respects the user sketches, we measure the ratio of sketch area filled with generated mass (Table~\ref{tab:ios}). We denote the metric we use as \textit{Intersection-over-Sketch}, and define it as $IoS ~=~\sum_{i=1}^{N}{{|M_i \cap C_{i}^{\alpha}|}/{|M_i|}}$.
% \begin{equation}
%     IoS = \sum_{i=1}^{N}{\frac{|M_i \cap |C_{i}^{\alpha}|}{|M_i|}} .
% \end{equation}
Here, $M_i$ is the sketch region and $C_{i}^{\alpha}$ is the thresholded alpha mask rendered with $\edittednerf$ from each sketch view. To ensure the metric is resilient to alpha thresholding values, the score we report is averaged over 9 thresholding values we apply to $C_i^\alpha$, ranging at $[25, 225]$. We point out that a high $IoS$ score by itself does not guarantee a high quality output, i.e. a method can cheat by simply filling the sketch region with some fixed color.


\textbf{Semantic Alignment.} We assess if our method generates semantically meaningful content aligning with the text-prompt using Clip-similarity~\cite{radford2021clip}. In the supplementary material, we present the evaluations which demonstrate that although we don't optimize for CLIP performance directly, our method achieves comparable results with Text-Only.
%We finally check whether our method have generated semantically meaningful content which agrees with the text-prompt (Table~\ref{tab:clipsim}). We measure semantic alignment using Clip-similarity \cite{radford2021clip}. Here we evaluate a full view of the output. The final score we report is the mean of 40 randomly sampled views, averaged over 10 unique runs.

% You can see the results in Table .~\ref{tab:quant}. We evaluate our method using three metrics. Firstly, to measure fidelity to sketches we compute the IoU between the sketch masks and the added mass in the sketch views. Secondly, to measure preservation quality, we measure the PSNR between renderings of the base NeRF and renderings of the edited NeRF outside the bounding box of the sketches in the sketch views, and lastly, to evaluate the semantics of the edits we compute the Clip similarities\cite{radford2021clip} between renderings of the edited NeRF and the text-prompts averaged over five random views.

\subsection{Ablation Studies}



To perform sketch-based, local editing, we use two 
%objective functions, 
loss terms,
$\mathcal{L}_{pres}$, which preserves the base object, and $\mathcal{L}_{sil}$ which generates the desired edit according to the input sketches. We visually analyze the effect of these 
loss terms
on two examples: one reconstructed with \cite{mueller2022instant} (Fig.~\ref{fig:ablations} top row) and another generated by Stable-DreamFusion \cite{stable-dreamfusion} (Fig.~\ref{fig:ablations} bottom row). We differentiate between these two examples as editing neural fields generated by Stable-DreamFusion ensures the rendered base model input is within the diffusion model distribution, 
which leads to fewer adversarial artifacts (see discussion in Section~\ref{subsubsec:basemodeldist}).

Qualitative ablation results are presented in Fig.~\ref{fig:ablations}. Text-Only, is equivalent to applying DreamFusion~\cite{poole2022dreamfusion} initialized with a pretrained NeRF. This method employs neither of the two geometric losses and adheres to the semnatics of the text prompt but drastically alters the base neural field, $\basenerf$. We also experimented with lowering the learning rate to $10^{-3}$ to avoid steering from the base model with high gradients, but that did not help in mitigating this effect.

When only $\mathcal{L}_{sil}$ is employed, the sketch regions are edited according to the text-prompts, but the base region also changes drastically. The flower, a generated NeRF, changes more meaningfully compared to the cat. When only $\mathcal{L}_{pres}$ is applied, no explicit constraint exists to respect the sketches. Therefore, our method yields a color artifact in the proximity of the sketch regions. When both constraints are simultaneously applied (our method), the edits respect both sketches and text prompt, and preserve the base NeRF.

We further validate our claim with a quantitative ablation which repeats the experiments in Section~\ref{subsec:quantitative} for the $\mathcal{L}_{sil}$ and $\mathcal{L}_{pres}$ variants (see Table~\ref{tab:psnr} and Table~\ref{tab:ios}). Evidently, both variants are inferior to the full pipeline.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figs/failures.jpg}
%     \caption{\textbf{Failures:} Our method fails to produce the correct semantics; flame is misplaced (\textbf{left}). Similar to \cite{poole2022dreamfusion} our method is prone to SDS Loss multiface inconsistencies (\textbf{right)}}
%     \label{fig:failures}
% \end{figure}

 % The results of different settings are shown in Fig.~\ref{fig:ablations}. In the case where none of the two geometric objectives are applied, we observe that the reconstructed neural field (cat) is drastically changed leading to an adversarial edit. Also when lowering the learning rate the object did not change. 
 % The modification in the generated neural field adheres to the semantics of the text prompt; however the base has completely changed.
 % Note that this setting is equivalent to applying DreamFusion, but  initialized with a pretrained NeRF.
 % We again observe that the flower changes more meaningfully compared to the cat. 
 