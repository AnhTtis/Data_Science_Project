\appendix

\renewcommand\thefigure{\arabic{figure}}
\setcounter{figure}{8}
\renewcommand\thetable{\arabic{table}}
\setcounter{table}{2}

% \textbf{Acknowledgements:} We thank Rinon Gal, Masha Shugrina, Roy Bar-On and Janick Martinez Esturo for helpful discussions and proofreading our paper. We also thank Andrey Voynov for providing access to their sketch-based diffusion work.

\section{Background}

In the following, we include an extended background chapter cut off from the main paper for brevity.

\subsection{Latent diffusion models (LDMs)}
LDMs 
\cite{rombach2021highresolution}
are a class of diffusion models that operate on a latent space instead of directly sampling high-resolution color images.
These models have two main components: a variational autoencoder consisting of an encoder $\mathcal{E}(x)$ and a decoder $\mathcal{D}(z)$, pretrained on the training data, and a denoising diffusion probabilistic model (DDPM) trained on the latent space of the autoencoder. Specifically, let $Z$ be the latent space learned by the autoencoder.\
% They usually consist of an encoder $\mathcal{E}(x)$ and a decoder $\mathcal{D}(z)$, pretrained on the training data, and a denoising diffusion probabilistic model (DDPM) trained on the latent space of the autoencoder.
The objective of the DDPM is to minimize the following expectation:
\begin{equation}
   \mathbb{E}_{z_0 \sim Z, \epsilon \sim \mathcal{N}(0, I), t}[||\epsilon_\phi(z_t, t) - \epsilon || ^ 2],
\end{equation}
where $t$ is the time-step of the diffusion process, $z_t~=~ \sqrt{\alpha_t}z_0 + \sqrt{1 - \alpha_t}\epsilon$ is the input latent image with noise added to it, and $\epsilon_\phi$ is the denoising model, often constructed as a U-Net~\cite{ho2020ddpm}.
Once trained, it is possible to sample from the latent space $Z$ by starting from a random standard Gaussian noise and running the backward diffusion process as described by Ho et al.~\cite{ho2020ddpm}. 
The sampled latent image then can be fed to $D(z)$ to get a final high-resolution image.


\section{Additional Evaluations}

\subsection{Quantitative Comparisons}
\textbf{Semantic Alignment.} Continuing the discussion from Section~\ref{subsec:quantitative}
, we include an additional quantitative comparison for CLIP-similarity, to verify our method does not degrade semantic alignment with the guiding text prompt compared to the Text-Only method of DreamFusion~\cite{poole2022dreamfusion}.
Table~\ref{tab:clipsim} shows that while we donâ€™t optimize for CLIP performance directly,
our method achieves comparable results with the Text-Only setting. We perform this experiment by sampling 40 views around each edited object and averaging the CLIP-similarity of each view to the corresponding texts.
\input{tables/tab_clipsim}
\input{tables/tab_ssim}
%\input{tables/tab_lpips}

\textbf{Base Model Fidelity.} In Table~\ref{tab:ssim}, we also include the SSIM metric to further quantify our method's capability to preserve the base model.

\subsection{Qualitative Comparisons}
\label{sec:supp_qualitative}

%\orrc{TODO}
%Optional:
%* SSIM / LPIPS in addition to PSNR
% * CLIP-R quantitative comparison in addition to CLIP-sim

% \subsection{Comparisons to Latent-NeRF} %~\cite{metzer2022latent}} 
\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/comparison.jpg}
    \captionsetup{font=small}
    \caption{Examples from the modified version of the sketch shape pipeline of Latent-NeRF~\cite{metzer2022latent}}
    \label{fig:comparison}
\end{figure*}
\input{tables/tab_compare}
\textbf{Comparison to Latent-NeRF~\cite{metzer2022latent}.} To the best of our knowledge, we are the first work to employ 2D sketch-based editing of NeRFs. Given that prior works are not directly comparable with our editing setting, we attempt to create a close comparison instead, faithful to the original compared method and fair to evaluate our editing setting. As baseline, we use the method from Latent-NeRF's
~\cite{metzer2022latent} 
3D sketch shape pipeline. We initialize a NeRF with the base object weights, and create a \textit{3D sketch shape}, a mesh, by intersecting the bounding boxes of our 2D sketches in the 3D space. Note that we could also intersect the sketch masks, however, due to view inconsistencies, we found that the results are far inferior. After initializing the NeRF and creating the sketch shape, we proceed to use the sketch shape loss from the paper to preserve the geometry, while editing the NeRF according to the input text. In Fig.~\ref{fig:comparison}, we establish that while this baseline is able to perform meaningful edits, it suffers from two apparent issues: (i) the baseline severely changes the base NeRF, and (ii) the edited region is bound to the coarse geometry of the intersected bounding boxes. To alleviate the latter, one could resort to modeling 3D assets as a sketch shape. However, we show that by using simple multiview sketches, it is possible to perform local editing without going through the effort of modeling accurate 3D masks. Finally, we include a quantitative summary of the preservation ability and the performance of the two methods in Table~\ref{tab:compare}.

%\textbf{Ablation Study} In table \orrc{TODO}, we demonstrate some of the assets %used to conduct the ablation study in Section 4.3 of the main paper.


%https://zju3dv.github.io/neumesh/ - cannot generate new content
%http://editnerf.csail.mit.edu/ - limited by 3d train data
%https://cassiepython.github.io/nerfart/ - discuss limits of stylization
%ClipMesh - cannot generate examples with genus > 0
%Original Dreamfusion - compare to the progressive squirrel example from their paper (show they affect the base)

%\subsection{Intersection over Sketch}
%\orrc{TODO}
%* Discuss if we want to include the low iou results also
%\subsection{Generation from scratch}
%\orrc{TODO}
% (currently failed maybe needs pretraining to zero density)

\begin{figure*}[t]
   \centering
    \begin{overpic}[width=0.8\linewidth,tics=10, trim=0 0 0 0,clip]{figs/ui.jpg}
   \end{overpic}
   \captionsetup{font=small}
    \caption{The interactive UI allows users to sketch over a pretrained NeRF. \textbf{Top row}: The user draws scribbles from two different views using "Sketch Mode". \textbf{Bottom left}: After pressing "Add sketch", the scribbles are filled to generate masks, ready to be used with our pipeline. \textbf{Bottom right}: The bounding box marks the sketches intersection region, where the edit takes place.}
    \label{fig:ui}
 \end{figure*}

 \begin{figure*}[t]
    \centering
    \hspace{0.25in}
    \begin{overpic}[width=0.6\linewidth,tics=10, trim=0 0 0 0,clip]{figs/geometry_nc.jpg}
    % \put(-20, 12){\large{Depth map}}
    % \put(-20, 40){\large{Normal map}}
    % \put(-15, 65){\large{RGB}}
    \end{overpic}
    %\includegraphics[width=0.8\linewidth]{figs/geometry.jpg}
    \captionsetup{font=small}
    \caption{From top to bottom: color, normal and depth maps of outputs generated by our method.}
    \label{fig:geometry}
\end{figure*}


\section{Implementation Details}%\vspace{-3\baselineskip}
\vspace{-10pt}
This section contains additional implementation details omitted from the manuscript.
\vspace{-35pt}
\subsection{Text Prompts}
\vspace{-10pt}
In the following, we include the full list of prompts that were used to generate the examples within the paper.
\vspace{-40pt}
\begin{itemize}
\setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt}
    \item "A cat wearing a chef hat"
    \item "A cherry on top of a sundae"
    \item "A red flower stem rising from a potted plant"
    \item "A teddy bear wearing sunglasses"
    \item "A candle on top of a cupcake"
    \item "An anime girl wearing a brown bag"
    \item "An apple on a plate"
    \item "A Nutella jar on a plate"
    \item "A globe on a plate"
    \item "A tennis ball on a plate"
    \item "A cat wearing a red tie"
    \item "A cat wearing red tie wearing a chef hat"
    \item "A 3D model of a unicorn head"
\end{itemize}
\vspace{-40pt}

Additionally, similar to 
DreamFusion\cite{poole2022dreamfusion} 
we use directional prompts, where based on the rendering view, we modify prompt $\textbf{T}$ as follows:
\vspace{-40pt}
\begin{itemize}
\setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt}
    \item "$\textbf{T}$, overhead view"
    \item "$\textbf{T}$, side view"
    \item "$\textbf{T}$, back view"
    \item "$\textbf{T}$, bottom view"
    \item "$\textbf{T}$, front view"
\end{itemize}
\vspace{-60pt}
%\subsection{Extended Settings}
%* Discuss details of Instant-NGP and preventing gradient flow to original base model (if time permits, add an experiment)

%* Discuss Albedo v.s. Lambertian shading (didn't have it in the framework at the time of conducting experiments)

%\vspace{-3\baselineskip}

\subsection{Interactive UI}
\vspace{-\baselineskip}

Since our method requires user interaction, we include an interactive user interface with our implementation (Fig.~\ref{fig:ui}). The user interface allows users to optimize newly reconstructed base NeRF models, or load pretrained ones. To perform edits, users can position the camera on the desired sketch view, and draw scribbles to guide SKED. By pressing "Add Sketch", scribbles are filled and converted to masked sketch inputs, ready to be used with our method.
\vspace{-30pt}
\subsection{Additional Hyperparameters}
\vspace{-10pt}
% Our method utilizes the open-source GitHub repository of Stable-Dreamfusion~\cite{stable-dreamfusion}. For interactive UI, we integrate with the renderer from kaolin-wisp~\cite{KaolinWispLibrary}.
% In all our experiments, unless stated otherwise, we set $\lambda_{pres}, \lambda_{sil}$, $\lambda_{sparse}$ and $\lambda_{color}$ to $5\times 10^{-6}$, $1$ and $5\times 10^{-4}$ and $5$ respectively. 
%\vspace{-\baselineskip}
The warm-up period for the occupancy grid pruning is set to 1,000 iterations. We also set the camera pose range to ensure that the sketch region remains visible in all sampled views. For large sketch regions, we use a Kd-tree 
\cite{2020SciPy-NMeth}
to implement Equation
%~\ref{eq:per_view_dist}
3
with efficient queries. We use ADAM
~\cite{kingma2014adam}
optimizer with a learning rate of $0.005$ and apply the exponential scheduler which decays it by $0.1$ by the end of the optimization. 
%We run our algorithm for \am{$10,000$} iterations (usually takes fewer iterations to converge), which takes approximately 30-40 minutes on a single NVIDIA RTX 3090 GPU. 
% We run the algorithm for $10,000$ iterations taking approximately 30-40 minutes on a single NVIDIA RTX 3090 GPU, though in most examples we found that it converges with much fewer iterations.
For our experiments, we use both publicly available 3D assets, and artificial assets generated by Stable-DreamFusion
~\cite{stable-dreamfusion}
guided only by text. We use the v1.4 version of the Stable diffusion~\cite{rombach2021highresolution} model. Unless specified otherwise, we use the same default hyperparameters settings throughout all experiments depicted in the paper.

%\vspace{-3\baselineskip}
\subsection{Quality Notes}

Our implementation uses an early version of Stable-DreamFusion
~\cite{stable-dreamfusion}
which does not include the optimizations very recently suggested by
Magic3D~\cite{lin2022magic3d}. 
In contrast to DreamFusion
~\cite{poole2022dreamfusion} 
and Magic3D
~\cite{lin2022magic3d},
which use commercial diffusion models with larger language models 
\cite{imagen2022saharia, balaji2022eDiff-I},
we rely on Stable Diffusion
\cite{rombach2021highresolution}, 
which is less sensitive to directional prompts. Our results are therefore not comparable in visual quality to these previous works.


% In addition, different to DreamFusion, though possible, we do not leverage the Lambertian lighting model in our architecture. The reason behind this is technical: we found this feature produces inferior results with the backbone and implementation we use.





\section{Additional Assets}
%\subsection{Ablation Assets}
%The images Mehdi used to compute IoS
%\orr{Aryan, Mehdi: please complete}

\subsection{Geometry and Depth}
In addition to RGB images, we share examples highlighting the geometry of our method's outputs. In Fig.~\ref{fig:geometry} we include the normal maps and depth maps of two output samples.


% \begin{figure*}
%    \centering
%     \begin{overpic}[width=0.8\linewidth,tics=10, trim=0 0 0 0,clip]{figs/ui.jpg}
%    \end{overpic}
%    \captionsetup{font=small}
%     \caption{The interactive UI allows users to sketch over a pretrained NeRF. \textbf{Top row}: the user draws scribbles from two different views using "Sketch Mode". \textbf{Bottom left}: after pressing "Add sketch", the scribbles are filled to generate masks, ready to be used with our pipeline. \textbf{Bottom right}: the bounding box marks the sketches intersection region, where the edit takes place.}
%     \label{fig:ui}
%  \end{figure*}

%  \begin{figure*}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figs/geometry.jpg}
%     \captionsetup{font=small}
%     \caption{Normal and depth maps generated by our method.}
%     \label{fig:geometry}
% \end{figure*}
