This section discusses previous work on link prediction algorithms and message-passing layers.
Particular emphasis is placed on methods featured in our experiments.

\subsection{Link Prediction}
Link prediction algorithms are applied in various fields, such as social network analysis~\cite{murata2007link, liben2003link, daud2020applications}, bioinformatics~\cite{ lei2013novel, stanfield2017drug, kang2022lr}, recommender systems~\cite{ai2019link, talasu2017link, huang2005link, zhang2010solving}, and supply chain improvement~\cite{lu2020discovering}.
Broadly speaking, different link prediction algorithms try to estimate link existence between two nodes either via heuristic or learned methods. 

Heuristic algorithms employ predefined heuristics to encode the similarity between nodes. Some prominent candidates
are represented by common neighbors, resource allocation~\cite{zhou2009predicting}, preferential attachment~\cite{barabasi1999emergence}, Adamic-Adar~\cite{adamic2003friends}, Jaccard~\cite{jaccard1901etude}, Katz~\cite{katz1953new}, and average commute time~\cite{fouss2007random}. However, all heuristic link prediction algorithms suffer from the same underlying issue. They exploit predefined, simple heuristics, which can not be modified to account for different network types. \Eg, common neighbors has been developed for social networks and hence yields underwhelming results when applied to molecular graphs.

On the other hand, learned algorithms do not rely on predesigned heuristics but rather learn a more complex, data-driven heuristic utilizing neural networks. Thus, learned algorithms can easily adapt to different network types while typically outperforming their heuristic counterparts. SEAL~\cite{zhang2018link, zhang2021labeling} represents a prominent, learned link prediction framework, defining link prediction as a subgraph-level classification task by training a binary GNN-based classifier to map from subgraph patterns to link existence. To this end, SEAL first extracts a local subgraph around the link of interest, which is subsequently forwarded to DGCNN~\cite{zhang2018end} for classification. Moreover, SEAL's authors introduce an additional node labeling technique, known as labeling trick~\cite{zhang2021labeling}, to enhance the expressiveness of node features obtained from GNNs. SIEG~\cite{sieg} builds upon SEAL and introduces, inspired by Graphormer~\cite{ying2021transformers}, a pairwise structural attention module between two nodes of interest to capture local structural information more effectively. This results in state-of-the-art performances and allows SIEG to simultaneously overcome Graphormer's issue of exploding computational complexity when applied to ultra-large graphs. SUREL+~\cite{yin2023surel+} introduces the use of node sets to represent subgraphs. To complement the loss of structural information, SUREL+ provides set samplers, structure encoders, and set neural encoders. SUREL+ outperforms the baseline SEAL on various link prediction benchmarks. It should be mentioned that Cai \etal~\cite{cai2021line} investigate the use of line graphs for link prediction. Please note that none of the above-mentioned methods are tailored to flow-driven spatial networks.

\subsection{Message-Passing Layers}
GNNs utilize the concept of message-passing to encode semantically rich features within network-structured data. Over time, multiple variations of message-passing layers have been proposed~\cite{xu2018powerful, defferrard2016convolutional, fey2018splinecnn, he2020lightgcn}. For instance, GCN's message-passing layer~\cite{kipf2016semi} weighs each incoming message with a fixed coefficient, the node degree, before aggregation. In contrast, GAT's message-passing layer~\cite{brody2021attentive} learns aggregation weights dynamically based on attention scores. GraphSAGE's message-passing layer~\cite{hamilton2017inductive} does not directly aggregate central node features with incoming messages. Instead, it distinguishes these two kinds of features and learns two different transformations, one on the central node and another on incoming messages. EdgeConv~\cite{wang2019dynamic} aggregates the feature difference between the central node and its neighbors combined with the central node's features. Thus, EdgeConv draws parallels to aggregating spatial vectors if the nodes embed spatial positions.
However, our proposed GAV layer differs significantly from EdgeConv, as we explicitly constrain the update of vector embeddings to imitate the simplified dynamics of physical flow in flow-driven spatial networks. Importantly, only a few works tried to adapt the message-passing paradigm to spatial networks~\cite{zhang2021representation, danel2020spatial}.