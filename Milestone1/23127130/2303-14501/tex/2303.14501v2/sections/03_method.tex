\begin{figure*}[h]
\centerline{\includegraphics[width=\linewidth]{imgs/method.pdf}}
\caption{
Overview of the GAV link prediction framework. GAV is divided into three modules, namely the subgraph extraction module, the message-passing module, and the readout module.
First, an $h$-hop enclosing subgraph $\mathcal{G}^{t}_{h}$ is extracted around the target nodes $\{ n_{i}^{t}, n_{j}^{t} \}$ (red and green) affiliated to the target link $e^{t}_{ij}$ (orange) and subsequently transformed into a line graph representation $\mathcal{L}(\mathcal{G}^{t}_{h})$ to construct vector embeddings representative of the network's structural properties; second, we perform iterative message-passing among vector embeddings in $\mathcal{L}(\mathcal{G}^{t}_{h})$ via $k$ GAV layers, modeling simplified physical flow in spatial networks; and, third, a final subgraph-level readout module aggregates refined vector embeddings and predicts the probability of link existence with regard to the target link $e^{t}_{ij}$.
To provide a concise visualization, $h$ was set to 1. We would like to draw the reader's attention to color coding.
}
\label{fig:method_overview}
\end{figure*}

Our proposed Graph Attentive Vectors (GAV) link prediction framework is depicted in Fig.~\ref{fig:method_overview}. GAV represents a simple yet effective end-to-end trainable framework tailored to the task of link prediction for flow-driven spatial networks. It predicts the probability of link (or edge) existence between two nodes in a graph $\mathcal{G}$ based on a binary classifier $\mathcal{F}$, composed of a novel message-passing and readout module operating on vector embeddings (see Sections~\ref{ref:gav_layer} and~\ref{ref:readout}).
To this end, $\mathcal{F}$ should be able to differentiate between positive (real) and negative (sampled) links by assigning high probabilities of existence to plausible and low probabilities of existence to implausible links.
Following most competitive approaches~\cite{zhang2018link, zhang2021labeling, sieg}, we treat link prediction as a subgraph classification task. To determine the probability of existence of an individual target link between two target nodes, we, therefore, first extract an enclosing subgraph describing the target link's local neighborhood in a subgraph extraction module (see Section~\ref{ref:subnetw_extraction}). Subsequently, the subgraph is transformed into a line graph representation to construct vector embeddings and forwarded to $\mathcal{F}$, resulting in an iterative link prediction scheme predicting the existence of target links one at a time.
In the following, we elaborate extensively on GAV's individual components (see Fig.~\ref{fig:method_overview}).
For ease of reference, we provide a notation lookup table in the supplementary (see Supp.~\ref{sup:notation}).

\subsection{Subgraph Extraction Module}\label{ref:subnetw_extraction}
The undirected input graph $\mathcal{G}\coloneqq(\mathcal{V}, \mathcal{E})$ is defined by a set of nodes $\mathcal{V}$ and a set of corresponding edges $\mathcal{E}$. While a node $n_{i} \in \mathcal{V}$ contains a specific spatial position given by coordinates ($n_{i} \in \mathbb{R}^{d_\text{spatial}}$), an edge $e_{ij} \in \mathcal{E}$ indicates a connection between nodes $n_{i}$ and $n_{j}$.

Similar to most competitive approaches~\cite{zhang2018link, zhang2021labeling, sieg}, we extract as a first step an $h$-hop enclosing subgraph $\mathcal{G}^{t}_{h}$ around the nodes $\{ n_{i}^{t}, n_{j}^{t} \}$ affiliated to the target link $e^{t}_{ij}$ from the original graph representation $\mathcal{G}$ (please note that we refer to the target in our notations as $t$).
This results in an expressive and efficient representation of the target link's local neighborhood, including relevant structural patterns necessary to determine link existence. 
Further, the subgraph extraction significantly reduces space complexity, which is crucial for link prediction in ultra-large graphs (see Table~\ref{tab:datasets}).

Since GAV operates on \textit{vector embeddings}, we subsequently transform $\mathcal{G}^{t}_{h}$ into a line graph representation $\mathcal{L}(\mathcal{G}^{t}_{h})\coloneqq(\mathcal{V'}, \mathcal{E'})$. In $\mathcal{L}(\mathcal{G}^{t}_{h})$, each node $n'_{i} \in \mathcal{V'}$ represents an edge $e_{ij} \in \mathcal{E}$ via an individual vector embedding, while an edge $e'_{ij} \in \mathcal{E'}$ indicates adjacency between two edges in $\mathcal{G}^{t}_{h}$ iff they are incident. To create vector embeddings, we encode edges as vectors between the involved nodes (\eg, $n'_{i} = n_{j} - n_{i}$). Therefore, $\mathcal{L}(\mathcal{G}^{t}_{h})$'s node embeddings are defined as vectors (see Fig.~\ref{fig:method_overview}) of unique length and direction representative of edges in $\mathcal{G}^{t}_{h}$ (see Supp.~\ref{sup:imp_det} for implementation details). The line graph $\mathcal{L}(\mathcal{G}^{t}_{h})$ formed around the target link $e^{t}_{ij}$ is finally forwarded to the message-passing module.

\subsection{Message-Passing Module and GAV Layer}\label{ref:gav_layer}
To adjust link prediction to flow-driven spatial networks, we propose a novel message-passing layer, termed GAV layer. We perform $k$ iterations of message-passing among the nodes of the line graph $\mathcal{L}(\mathcal{G}^{t}_{h})$, obtained from our subgraph extraction module, in our message-passing module. The GAV layer's message-passing relies on a straightforward intuition inspired by principles of physical flow. To be precise, we treat nodes in $\mathcal{L}(\mathcal{G}^{t}_{h})$ as vector embeddings and update them in a constrained manner, imitating simplified dynamics of physical flow in spatial networks. The detailed structure of a single GAV layer is visualized in Fig.~\ref{fig:mpn}.

\begin{figure}[h]
\centerline{\includegraphics[width=0.95\linewidth]{imgs/gav2.pdf}}
\caption{
Visualization of a single GAV layer updating the vector embedding of node $n'_{j}$. We forward the vector embedding of node $n'_{j}$ together with $N_{j} \in \mathbb{R}^{| \mathcal{N}(n'_j) \cup {n'_j}| \times d_{\text{spatial}}}$, which represents the set of $n'_{j}$ and its neighbors $n'_{k}$ and $n'_{i}$, to the GAV layer.
In this specific example, the vector embedding has been flipped (see $\hat{n}'_{j}$), which can be interpreted as a change in direction of physical flow.
}
\label{fig:mpn}
\end{figure}

In order to update an individual vector embedded in a node $n'_{i} \in \mathbb{R}^{d_{\text{spatial}}}$, we first project it together with a matrix $N_{i} \in \mathbb{R}^{| \mathcal{N}(n'_{i}) \cup n'_{i}| \times d_{\text{spatial}}}$, consisting of directly neighboring nodes and the node itself, 
%$\bigcup\limits_{j \in \mathcal{N}(i) \cup {i}} \{ n_{j} \}$
into a higher dimensional space $d_{\text{message}}$; the projection is through a learnable function $\phi^{(1)}_{\theta}:\mathbb{R}^{d_{\text{spatial}}} \rightarrow \mathbb{R}^{d_{\text{message}}}$. Subsequently, $\phi^{(1)}_{\theta}(n'_{i})$ and $\phi^{(1)}_{\theta}(N_{i})$ are forwarded to a multi-head attention operation, where $\phi^{(1)}_{\theta}(n'_{i})$ represents a single query, while $\phi^{(1)}_{\theta}(N_{i})$ represents the key and value sequence.
\begin{equation}
\tilde{n}'_{i} = \text{MultiHeadAttn}(\phi^{(1)}_{\theta}(n'_{i}),\ \phi^{(1)}_{\theta}(N_{i}),\ \phi^{(1)}_{\theta}(N_{i}))
\end{equation}
This results in an intermediate node representation $\tilde{n}'_{i} \in \mathbb{R}^{d_{\text{message}}}$, which incorporates not only information of the node itself but also its local structural neighborhood via the concept of attention. In the next step, we apply a residual connection for increased gradient flow and forward the result to the learnable function $\phi^{(2)}_{\theta}: \mathbb{R}^{d_{\text{message}}} \rightarrow \mathbb{R}$, followed by a tanh non-linearity.
\begin{equation}
s_{i} = \text{tanh}(\phi^{(2)}_{\theta}(\tilde{n}'_{i} + \phi^{(1)}_{\theta}(n'_{i})))
\end{equation}
Finally, the scalar value $s_{i} \in (-1, 1)$ is utilized to update the original node representation $n'_{i}$ via scalar multiplication.
\begin{equation}
\hat{n}'_{i} =  s_{i} \cdot n'_{i}
\end{equation}
Hence, after one layer of message-passing, the updated, refined node representation is given by $\hat{n}'_{i} \in \mathbb{R}^{d_{\text{spatial}}}$. Importantly, $\hat{n}'_{i}$ preserves relevant structural properties of the original node representation $n'_{i}$.
This is because scalar multiplication with $s_{i} \in (-1, 1)$ restricts the modification of vector embeddings given by nodes in $\mathcal{L}(\mathcal{G}^{t}_{h})$.
In essence, our message-passing paradigm can be geometrically interpreted as a scaling combined with a potential flipping operation of vectors. These constraints imposed by our message-passing align with our principal idea of modeling simplified dynamics of physical flow in spatial networks via potential, constrained changes in the direction and magnitude of vector embeddings, preserving the network's structural properties.

\begin{table*}[h] %tp!]
\centering
\scriptsize
\caption{Properties of the raw datasets. Each dataset consists of exactly one ultra-large graph.}
\vspace{-0.5em}
\label{tab:datasets}
\begin{tabular}{l |l l c l c c l} 
\toprule
 Dataset Name & $\#$ Nodes & $\#$ Edges & Node Degree & Node Features & $d_\text{spatial}$ & Edge Features & Description\\
\midrule
ogbl-vessel~\cite{hu2020ogb} & 3,538,495 & 5,345,897 & 3.02 & $x$-, $y$-, $z$-coordinates & 3 & $-$ & BALB/c mouse $\text{strain}^{1}$\\
c57-tc-vessel~\cite{paetzold2021whole} & 3,820,133 & 5,614,677 & 2.94 & $x$-, $y$-, $z$-coordinates & 3 & $-$ & C57BL/6 mouse $\text{strain}^{1}$\\
cd1-tc-vessel~\cite{paetzold2021whole} & 3,645,963  & 5,791,309 & 3.18 & $x$-, $y$-, $z$-coordinates & 3 & $-$ & CD-1 mouse $\text{strain}^{1}$\\
c57-cc-vessel~\cite{walchli2021hierarchical} & 6,650,580 & 9,054,100 & 2.72 & $x$-, $y$-, $z$-coordinates &3 &  $-$ & C57BL/6 mouse $\text{strain}^{2}$\\
\midrule
belgium-road~\cite{bader201110th} & 1,441,295 & 1,549,970 & 2.15 & $x$-, $y$-coordinates & 2 & $-$ & Belgium\\
italy-road~\cite{bader201110th} & 6,686,493 & 7,013,978 & 2.10 & $x$-, $y$-coordinates & 2 & $-$ & Italy\\
netherlands-road~\cite{bader201110th} & 2,216,688 & 2,441,238 & 2.20& $x$-, $y$-coordinates & 2 & $-$ & Netherlands\\
luxembourg-road~\cite{bader201110th} & 114,599 & 119,666 & 2.09 & $x$-, $y$-coordinates & 2 & $-$ & Luxembourg\\
\bottomrule
\multicolumn{7}{l}{$^{1}$ tissue clearing (tc) and light-sheet microscopy imaging  $\qquad ^{2}$ corrosion casting (cc) and SR\textmu$\text{CT}$ imaging}
\end{tabular}
\end{table*}

\subsection{Labeling Trick}
Following common practices~\cite{zhang2021labeling, sieg, cai2021line}, we apply a labeling trick to enable the message-passing module to distinguish between the relevance of different vector embeddings and, hence, learn an expressive structural representation of the target link's local neighborhood.
The labeling trick ensures that vector embeddings created from the target link $e_{ij}^{t}$ and edges connected to the target nodes $\{ n_{i}^{t}, n_{j}^{t} \}$ are identifiable by a distinct label.
Labels generated by our interpretation of the labeling trick are shown in Fig.~\ref{fig:labeling_trick}.

\begin{figure}[h]
\centerline{\includegraphics[width=0.45\linewidth]{imgs/labeling_trick3.pdf}}
\caption{
Labels generated by the labeling trick for an exemplary line graph ($h$ set to two).
Our interpretation of the labeling trick assigns the label 0 to the vector embedding representing the target link (orange), the labels 1 and 2 to vector embeddings representing edges connected to the target nodes $n^t_{i}$ and $n^t_{j}$ (purple and blue), and the label 3 to remaining vector embeddings.
}
\label{fig:labeling_trick}
\end{figure}

\noindent
The additional labels generated by the labeling trick are concatenated to $\mathcal{L}(\mathcal{G}^{t}_{h})$'s vector embeddings in the message-passing module (before message-passing).

\subsection{Readout Module}\label{ref:readout}
Our proposed readout module consists of a custom node aggregation operation followed by a learnable function $\phi^{(3)}_{\theta}: \mathbb{R}^{2 \cdot d_{\text{spatial}}} \rightarrow \mathbb{R}$ and processes refined vector embeddings obtained from the message-passing module. 
While the node aggregation operation aims to distill pertinent information from the refined graph representation in a fashion invariant to node ordering and quantity, $\phi^{(3)}_{\theta}$ predicts the probability of link existence with regard to the target link.

Equation~\ref{eq:agg} summarizes the readout module's functionality, where $\mathcal{E}_{\mathcal{N}(n^t_{i})}$ defines the set of refined vector embeddings originally created from edges adjacent to $n^t_{i}$ (see Fig.~\ref{fig:method_overview}), $\mathcal{E}_{\mathcal{N}(n^t_{j})}$ the set of refined vector embeddings originally created from edges adjacent to $n^t_{j}$ (see Fig.~\ref{fig:method_overview}), $\mathbin\Vert$ the concatenation operation, and $\hat{y}^t_{ij}$ the predicted probability of target link existence.
\begin{equation} \label{eq:agg}
\hat{y}^t_{ij} =  \phi^{(3)}_{\theta}(\text{mean}(\mathcal{E}_{\mathcal{N}(n^t_{i})}) \mathbin\Vert \text{mean}(\mathcal{E}_{\mathcal{N}(n^t_{j})}))
\end{equation}
Thus, our node aggregation operation constructs mean vector embeddings (see \text{mean}($\mathcal{E}_{\mathcal{N}(n^t_{i})})$ and $\text{mean}(\mathcal{E}_{\mathcal{N}(n^t_{j})})$) representative of our simplified definition of flow in the two target nodes and intends to exploit their relationship to predict whether target nodes should be connected or not.

\subsection{Loss Function}
Since our approach represents a binary classifier $\mathcal{F}$ determining the probability of link existence with regard to the target link, we optimize a binary cross-entropy loss function during training. Here, $y^t_{ij} \in \{0, 1\}$ indicates whether the target links are negative (sampled) or positive (real).
\begin{equation}
\mathcal{L_{\text{BCE}}} =  \frac{-1}{| \mathcal{E} |} \sum_{ij \in \mathcal{E}} y^t_{ij} \cdot \text{log}(\hat{y}^t_{ij}) + (1 - {y}^t_{ij}) \cdot \text{log}(1 - \hat{y}^t_{ij})
\end{equation}
We would like to highlight that our approach is trained in an entirely end-to-end manner, solely based on the information of link existence. Therefore, intermediate representations, such as the refined vector embeddings, are determined completely data-driven.