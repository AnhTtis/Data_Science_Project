\begin{table*}[t]
\centering
\scriptsize
\caption{Quantitative results.
GAV outperforms the previous state-of-the-art across all metrics and datasets. We report mean and standard deviation values on the ogbl-vessel benchmark based on ten different random seeds (0 - 9). To compare GAV to existing baseline algorithms, we report quantitative results based on the area under the receiver operating characteristic curve (AUC). We additionally utilize the evaluation metric Hits@$k$ as an additional, stricter performance measure. More details on evaluation metrics can be found in the supplementary (see Supp.~\ref{sup:metrics}). Please note that the ogbl-vessel benchmark's official evaluation metric is AUC. Therefore, Hits@$k$ values of participating algorithms are not available. We highlight the respective three best quantitative values in shades of teal.}
\label{tab:quantitative_results}
\vspace{-0.5em}
\begin{tabular}{p{50pt}|l|l|c|c|c|c} 
\toprule
Dataset & Model &$\#\ \text{Params}\downarrow$ &$\text{AUC}\uparrow$ (\%)& $\text{Hits@100}\uparrow$  (\%)& $\text{Hits@50}\uparrow$  (\%)& $\text{Hits@20}\uparrow  (\%)$\\ 
\midrule
\multirow{13}{*}{ogbl-vessel}
& GCN~\cite{kipf2016semi} & 396,289 & 43.53 ± 9.61 & - & - & - \\
& MLP & 1,037,577 & 47.94 ± 1.33 & - & - & - \\
& Adamic-Adar~\cite{adamic2003friends} &  \cellcolor{teal!40}0 & 48.49 ± 0.00 & - & - & - \\
& GraphSAGE~\cite{hamilton2017inductive} & 396,289 & 49.89 ± 6.78 & - & - & - \\
& SAGE+JKNet~\cite{xu2018representation} & \cellcolor{teal!20}273 & 50.01 ± 0.07 & - & - & - \\
& SGC~\cite{wu2019simplifying} & \cellcolor{teal!10}897 & 50.09 ± 0.11 & - & - & - \\
& LRGA~\cite{puny2020global} & 26,577 & 54.15 ± 4.37 & - & - & - \\
& SEAL~\cite{zhang2021labeling} & 172,610 & 80.50 ± 0.21 & - & - & - \\
& S3GRL ($\text{PoS}^{\text{+}}$)~\cite{louis2023simplifying} & 2,382,849 & 80.56 ± 0.06 & - & - & - \\
& SUREL+~\cite{yin2023surel+} & 56,353 & 84.96 ± 0.68 & - & - & - \\
& SIEG~\cite{sieg} & 752,716 & \cellcolor{teal!10}87.98 ± 1.00 & - & - & - \\
\cmidrule{2-7}
& SEAL+EdgeConv (ours) & 49,346 & \cellcolor{teal!20}97.53 ± 0.32 & \cellcolor{teal!20}16.09 ± 10.48 & \cellcolor{teal!20}9.37 ± 6.18 & \cellcolor{teal!20}4.99 ± 4.24\\
& GAV (ours) & 8,194 & \cellcolor{teal!40}98.38 ± 0.02 & \cellcolor{teal!40}34.77 ± 0.94 & \cellcolor{teal!40}28.02 ± 1.58 & \cellcolor{teal!40}19.71 ± 2.31\\
\midrule
\multirow{3}{*}{c57-tc-vessel}
& SEAL~\cite{zhang2021labeling} & \cellcolor{teal!10}43,010 & \cellcolor{teal!10}78.21 & \cellcolor{teal!10}0.12 & \cellcolor{teal!10}0.06 & \cellcolor{teal!10}0.01\\
& SEAL+EdgeConv (ours) & \cellcolor{teal!20}49,346 & \cellcolor{teal!20}97.23 & \cellcolor{teal!20}16.71 & \cellcolor{teal!20}10.39 & \cellcolor{teal!20}5.01\\
& GAV (ours) & \cellcolor{teal!40}8,194 & \cellcolor{teal!40}98.24 & \cellcolor{teal!40}33.26 & \cellcolor{teal!40}26.89 & \cellcolor{teal!40}21.32\\
\midrule
\multirow{3}{*}{cd1-tc-vessel}
& SEAL~\cite{zhang2021labeling} & \cellcolor{teal!10}43,010 & \cellcolor{teal!10}83.60 & \cellcolor{teal!10}0.27 & \cellcolor{teal!10}0.16 & \cellcolor{teal!10}0.06\\
& SEAL+EdgeConv (ours) & \cellcolor{teal!20}49,346 & \cellcolor{teal!20}97.91 & \cellcolor{teal!20}17.05 & \cellcolor{teal!20}11.57 & \cellcolor{teal!20}2.98\\
& GAV (ours) & \cellcolor{teal!40}8,194 & \cellcolor{teal!40}98.72 & \cellcolor{teal!40}35.82 & \cellcolor{teal!40}27.25 & \cellcolor{teal!40}17.23\\
\midrule
\multirow{3}{*}{c57-cc-vessel}
& SEAL~\cite{zhang2021labeling} & \cellcolor{teal!10}43,010 & \cellcolor{teal!10}83.75 & \cellcolor{teal!10}0.65 & \cellcolor{teal!10}0.44 & \cellcolor{teal!10}0.24\\
& SEAL+EdgeConv (ours) & \cellcolor{teal!20}49,346 & \cellcolor{teal!20}97.49 & \cellcolor{teal!20}7.21 & \cellcolor{teal!20}3.35 & \cellcolor{teal!20}1.06\\
& GAV (ours) & \cellcolor{teal!40}8,194 & \cellcolor{teal!40}97.99 & \cellcolor{teal!40}18.90 & \cellcolor{teal!40}14.58 & \cellcolor{teal!40}9.04\\
\midrule
\multirow{3}{*}{belgium-road}
& SEAL~\cite{zhang2021labeling} & \cellcolor{teal!10}43,010 & \cellcolor{teal!10}86.73 & \cellcolor{teal!20}1.25 & \cellcolor{teal!20}0.68 & \cellcolor{teal!10}0.30\\
& SEAL+EdgeConv (ours) & \cellcolor{teal!20}49,346 & \cellcolor{teal!20}96.98 & \cellcolor{teal!10}0.55 & \cellcolor{teal!10}0.55 & \cellcolor{teal!20}0.51\\
& GAV (ours) & \cellcolor{teal!40}8,194 & \cellcolor{teal!40}99.29 & \cellcolor{teal!40}47.44 & \cellcolor{teal!40}38.60 & \cellcolor{teal!40}22.11\\
\midrule
\multirow{3}{*}{italy-road}
& SEAL~\cite{zhang2021labeling} & \cellcolor{teal!10}43,010 &  \cellcolor{teal!10}90.07 &  \cellcolor{teal!20}0.32 &  \cellcolor{teal!10}0.16 & \cellcolor{teal!20}0.08\\
& SEAL+EdgeConv (ours) & \cellcolor{teal!20}49,346 & \cellcolor{teal!20}90.24 & \cellcolor{teal!10}0.26 & \cellcolor{teal!20}0.17 &  \cellcolor{teal!10}0.07\\
& GAV (ours) & \cellcolor{teal!40}8,194 & \cellcolor{teal!40}99.41 & \cellcolor{teal!40}28.49 & \cellcolor{teal!40}20.08 & \cellcolor{teal!40}11.99\\
\midrule
\multirow{3}{*}{netherlands-road}
& SEAL~\cite{zhang2021labeling} & \cellcolor{teal!10}43,010 & \cellcolor{teal!10}84.19 & \cellcolor{teal!10}0.00 & \cellcolor{teal!10}0.00 & \cellcolor{teal!10}0.00\\
& SEAL+EdgeConv (ours) & \cellcolor{teal!20}49,346 & \cellcolor{teal!20}96.06 & \cellcolor{teal!20}3.91 & \cellcolor{teal!20}2.20 & \cellcolor{teal!20}1.01\\
& GAV (ours) & \cellcolor{teal!40}8,194 & \cellcolor{teal!40}99.44 & \cellcolor{teal!40}37.55 & \cellcolor{teal!40}26.97 & \cellcolor{teal!40}10.77\\
\midrule
\multirow{3}{*}{luxembourg-road}
& SEAL~\cite{zhang2021labeling} & \cellcolor{teal!10}43,010 & \cellcolor{teal!10}89.79 & \cellcolor{teal!10}11.39 & \cellcolor{teal!10}6.15 & \cellcolor{teal!10}3.12\\
& SEAL+EdgeConv (ours) & \cellcolor{teal!20}49,346 & \cellcolor{teal!20}97.53 & \cellcolor{teal!20}59.79 & \cellcolor{teal!20}39.15 & \cellcolor{teal!20}19.42\\
& GAV (ours) & \cellcolor{teal!40}8,194 & \cellcolor{teal!40}99.31 & \cellcolor{teal!40}85.88 & \cellcolor{teal!40}76.84 & \cellcolor{teal!40}61.95\\
\bottomrule
\end{tabular}
\end{table*}

In this section, we demonstrate the performance of our proposed GAV framework on the ogbl-vessel benchmark~\cite{hu2020ogb} and on additional datasets sourced from publicly available flow-driven spatial networks.
We first elaborate on baseline algorithms and the experimental setup, followed by a detailed description of datasets utilized in our experiments. Finally, we report quantitative results, investigate our design choices by conducting extensive ablation studies, and discuss GAV's interpretability in detail.
Additional experiments on non-flow-based benchmarks and roto-translational invariance can be found in the supplementary.

\subsection{Baselines and Experimental Setup}\label{ref:baselines}
To evaluate GAV properly, we not only compare GAV to algorithms submitted to the ogbl-vessel benchmark but also propose a novel \emph{secondary baseline} (SEAL+EdgeConv) combining the SEAL framework~\cite{zhang2018link, zhang2021labeling} with the EdgeConv message-passing layer~\cite{wang2019dynamic}, following recent trends in graph-based object detection from point clouds~\cite{chai2021point, yang2022graph, wang2021object, yin2020lidar}. Extensive experiments with different baseline algorithms revealed that this provides us with an improved, highly competitive secondary baseline for link prediction in spatial networks. We, therefore, compare GAV on the additionally sourced datasets to SEAL's original version (with tuned hyperparameters), which has shown to deliver results on par with or superior to the state-of-the-art on multiple link prediction benchmarks, and to our introduced secondary baseline SEAL+EdgeConv. Details on the configuration of our secondary baseline can be found in the supplementary (see Supp.~\ref{sup:sec_base}).

GAV was trained using the Adam optimizer~\cite{kingma2014adam} with a learning rate of 0.001 and a batch size of 32 on a single Tesla V100 GPU (32 GB) until convergence.
An ablation study on the number of hops $h$ in the subgraph extraction module and the number of message-passing iterations $k$ indicates that setting both to one is sufficient (see Table \ref{tab:ablations_kh}).
In the GAV layer, the number of heads of the multi-head attention operation is set to 4, while $\phi^{(1)}_{\theta}$ represents a single linear layer with an output dimension of $d_{\text{message}} = 32$, and $\phi^{(2)}_{\theta}$ is given by a two-layer MLP with a hidden dimension of 64.
The GAV layer makes use of leaky ReLU non-linearities~\cite{maas2013rectifier} to increase gradient flow and simplify weight initialization. The readout module's learnable function $\phi^{(3)}_{\theta}$ is represented by a two-layer MLP with a hidden dimension of 128.
All hyperparameters were tuned on the validation set of the ogbl-vessel benchmark.

\subsection{Datasets}
We experiment with multiple 2D and 3D flow-driven spatial networks to demonstrate the generalizability of our approach (see Table~\ref{tab:datasets}). In total, we conduct experiments on eight networks, including the ogbl-vessel benchmark and seven additionally sourced datasets, representing whole-brain vessel graphs of different mouse strains and road networks of various European countries. In this context, link prediction can be interpreted as predicting the probability of the existence of blood vessels and road segments. Whole-brain vessel graphs and road networks are graphically visualized in the supplementary (see Supp.~\ref{sup:data}).

\paragraph{Whole-Brain Vessel Graphs}
Blood vessels represent fascinating structures forming complex networks that transport oxygen and nutrients throughout the human body. The vascular system is, therefore, intuitively represented as a flow-driven spatial network, where branching points of vessels typically represent nodes embedding $x$-, $y$-, and $z$-coordinates, while edges are defined as blood vessels running between branching points~\cite{paetzold2021whole}. We report results on the Open Graph Benchmark's ogbl-vessel benchmark~\cite{hu2020ogb}, which measures the performance of different link prediction algorithms with regard to whole-brain vessel graphs aiming to remove artifacts introduced by the multi-stage graph generation process. The ogbl-vessel benchmark consists of millions of nodes and edges (see Table~\ref{tab:datasets}) and describes the murine brain vasculature all the way down to the microcapillary level.
However, we not only experiment with the ogbl-vessel benchmark but also source three additional whole-brain vessel graphs of different mouse strains acquired via different imaging methodologies~\cite{todorov2020machine, walchli2021hierarchical} (see Table~\ref{tab:datasets}, footnote).

\paragraph{Road Networks}
Further, we report results on diverse road networks representative of four European countries for a thorough evaluation of our proposed GAV framework's generalizability. To this end, we adopt publicly available road networks introduced in the DIMACS graph partitioning and clustering challenge~\cite{bader201110th}. These road networks correspond to the largest connected components of OpenStreetMap's~\cite{OpenStreetMap} road networks and are vastly different in size (\eg, luxembourg-road constitutes roughly 100,000 edges, whereas italy-road has more than 7,000,000). 
In road networks, intersections and locations with stronger curvature represent nodes in the form of $x$- and $y$-coordinates, while connecting roads represent edges.

\paragraph{Preprocessing}
Link prediction datasets require positive (label 1) and negative links (label 0). Positive links correspond to existent edges in our datasets, whereas negative links represent artificially created, non-existent edges. As link prediction algorithms are commonly employed to improve the graph representation through the identification of absent connections and the reduction of local noise arising from graph generation, negative links should appear as authentic as possible.
In light of the absence of negative links in our sourced datasets, we prepare our sourced datasets in a manner that aligns with the ogbl-vessel benchmark. Therefore, we sample negative links using a spatial sampling strategy. To be precise, we randomly connect nodes in close proximity, taking a maximum distance threshold of $\delta = \overline{e_{ij}} + 2 \sigma $ into account. Here, $\overline{e_{ij}}$ denotes the average edge length estimated over the entire graph $\mathcal{G}$ and $\sigma$ the standard deviation. 
The number of negative, sampled links corresponds to the number of positive, real links across all datasets. 
We finally split positive and negative links into training, validation, and test sets (split 80\%/10\%/10\%).


\subsection{Quantitative Results}
GAV demonstrates excellent, superior performances on the task of link prediction across all metrics and datasets, as can be observed in Table~\ref{tab:quantitative_results}.
We outperform the current state-of-the-art algorithm SIEG on the ogbl-vessel benchmark by \textbf{12\% (98.38 vs. 87.98 AUC)} while requiring a significantly smaller amount of parameters (8,194 vs. 752,716). However, GAV not only drastically outperforms all algorithms submitted to the ogbl-vessel benchmark but also our introduced strong, secondary baseline, combining the SEAL framework with EdgeConv (SEAL+EdgeConv). The excellent performance and superiority of GAV is even more pronounced when considering the strict evaluation metric of Hits@$k$.
Quantitative results reported in Table~\ref{tab:quantitative_results} additionally indicate the strong performance of our introduced secondary baseline (see Section~\ref{ref:baselines}).

It is of note that the luxembourg-road dataset's test set contains only 12,000 negative links. We, therefore, compare predictions of its positive links to 12,000 rather than 100,000 negative links (see supplementary, Supp.~\ref{sup:metrics}). This explains the comparatively strong Hits@$k$ performances on the luxembourg-road dataset.

\subsection{Ablation Studies}
To further validate GAV, we conduct detailed ablation studies on the validation set of the ogbl-vessel benchmark. Additional ablation studies on the GAV layer and its design choices can be found in the supplementary (Supp.~\ref{sup:gav_abl}).
Table~\ref{tab:ablations_des} investigates the importance of the readout module, the message-passing module, and the labeling trick.
\begin{table}[h]
\centering
\scriptsize
\caption{Ablations on main design choices.}
\vspace{-0.5em}
\label{tab:ablations_des}
\begin{tabular}{c c c|c c} 
\toprule
Readout Module & Message-Passing & Labeling Trick & $\text{AUC}\uparrow$ &$\Delta$ \\
\midrule
\cmark & \cmark & \cmark &  \cellcolor{teal!40}98.39 & $-$\\
\xmark & \cmark & \cmark & \cellcolor{teal!20}98.28 & -0.11\\
\cmark & \xmark & \cmark & 80.56 & -17.83\\
\cmark & \cmark & \xmark & \cellcolor{teal!10}96.00 & -2.39\\
\bottomrule
\end{tabular}
\end{table}
First, we exchange our readout module with a SortPooling layer followed by two convolutional layers and an MLP, resembling SEAL's readout operation. We note that our readout module is more applicable to flow-driven spatial networks, as it leads to a modest AUC increase of 0.11. Second, we completely deactivate the message-passing module by forwarding $\mathcal{L}(\mathcal{G}^{t}_{h})$ directly to the readout module. We observe a drastic AUC decrease of 17.83, indicating the importance of modifying the vector embeddings via our proposed GAV layer.
Finally, we evaluate the impact of the labeling trick. Excluding the additional labels generated by the labeling trick results in an AUC decrease of 2.39, proving the significance of link identification via additional, distinct labels.

In a second ablation study, we experiment with different message-passing layers (see Supp.~\ref{sup:update}), including EdgeConv, in our message-passing module. We report our findings in Table~\ref{tab:ablations_operators}.
\begin{table}[h]
\centering
\scriptsize
\caption{Ablations with different message-passing layers.}
\label{tab:ablations_operators}
\vspace{-0.5em}
\begin{tabular}{c |c c c c} 
\toprule
Message-Passing Layer & $\text{AUC}\uparrow$ & $\text{Hits@100}\uparrow$& $\text{Hits@50}\uparrow$& $\text{Hits@20}\uparrow$ \\
\midrule
GAV layer (ours) &\cellcolor{teal!40}98.39 & \cellcolor{teal!40}34.46 & \cellcolor{teal!40}26.30 & \cellcolor{teal!40}19.81\\
EdgeConv~\cite{wang2019dynamic} &\cellcolor{teal!20}97.43 & \cellcolor{teal!20}17.30 & \cellcolor{teal!20}5.97 & \cellcolor{teal!10}0.78\\
GAT layer~\cite{brody2021attentive} &\cellcolor{teal!10}96.44 & \cellcolor{teal!10}4.58 & \cellcolor{teal!10}2.55 & \cellcolor{teal!20}1.59\\
SAGE layer~\cite{hamilton2017inductive} &93.53 & 0.77 & 0.11 & 0.03\\
GCN layer~\cite{kipf2016semi} &89.31 & 0.39 & 0.22 & 0.16\\
\bottomrule
\end{tabular}
\end{table}
Our proposed GAV layer outperforms the other message-passing layers across all metrics by a considerable amount, proving our rationale.

Lastly, we vary the number of hops $h$ used to generate $\mathcal{G}^t_h$ and the number of message-passing iterations $k$ (see Table~\ref{tab:ablations_kh}).
\begin{table}[h]
\centering
\scriptsize
\caption{Ablations on $k$ and $h$.}
\label{tab:ablations_kh}
\vspace{-0.5em}
\begin{tabular}{c |c c c c} 
\toprule
$k$ \& $h$ & $\text{AUC}\uparrow$ &$\text{Hits@100}\uparrow$& $\text{Hits@50}\uparrow$& $\text{Hits@20}\uparrow$\\
\midrule
1 & \cellcolor{teal!40}98.39 & \cellcolor{teal!40}34.46 & \cellcolor{teal!20}26.30 & \cellcolor{teal!20}19.81\\
2 & \cellcolor{teal!40}98.39 & \cellcolor{teal!10}34.00 & \cellcolor{teal!40}26.93 & \cellcolor{teal!40}21.21\\
3 & \cellcolor{teal!40}98.39 & \cellcolor{teal!20}34.25 & \cellcolor{teal!10}25.99 & \cellcolor{teal!10}17.84\\
\bottomrule
\end{tabular}
\end{table}
We observe that simultaneously increasing $k$ and $h$ results in no discernible differences in performance. This finding is in line with the $\gamma$-decaying theory~\cite{zhang2018link}, proving the approximability of high-order heuristics from locally restricted subgraphs.


\begin{figure*}[h]
\centerline{\includegraphics[width=\linewidth]{imgs/inter3.pdf}}
\vspace{-0.4em}
\caption{
Visualization of the effect of our GAV layer on vector embeddings (ogbl-vessel). We visualize subgraph representations $\mathcal{G}^t_h$ ($h$ set to one) of four positive, plausible (first row) and four negative, implausible target links (second row) together with the GAV layer's predicted scalar values $s_{i} \in (-1, 1)$. The scalar values $s_{i}$ used to update vector embeddings in $\mathcal{L}(\mathcal{G}^t_h)$ have been projected to $\mathcal{G}^t_h$ to provide an interpretable visualization. The directionality of edges already incorporates potential shifts in direction enforced by our GAV layer. Please note that following the color coding scheme of Fig.~\ref{fig:method_overview}, the target link $e^{t}_{ij}$ is depicted in orange, whereas the two target nodes $n_{i}^{t}$ and $n_{j}^{t}$ are displayed in red and green. We additionally report the angle $\angle$ between vector embeddings aggregated around the two target nodes (see Section~\ref{ref:readout}) and the predicted probability of link existence $\hat{y}^t_{ij}$. The last column contains more challenging cases.
}
\label{fig:inter}
\end{figure*}

\subsection{Interpretability and Analysis of Results}\label{ref:inter}
In Fig.~\ref{fig:inter}, we visualize the behavior of our proposed GAV layer, attempting to facilitate interpretability and provide a clearer insight on how it creates superior representations for link prediction in flow-driven spatial networks. An analysis based on a toy example and additional visualizations similar to Fig.~\ref{fig:inter} are provided in the supplementary (Supp~\ref{sup:toy} \&~\ref{sup:inter}).

Scalar multiplication via $s_{i}$ enables GAV to flip individual vector embeddings (mimicking \textit{change in direction of flow}). We observe that this results in physically implausible, learned sink/source flow between the two target nodes (red and green) of implausible formations (see Fig.~\ref{fig:inter}, second row; Fig.~\ref{fig:neg_preds}, supplementary) with a consistency of 94.26\%, which stands in drastic contrast to the behavior of physical flow in spatial networks. The learned sink/source flow between target nodes of negative links, in turn, leads to drastically different representations for implausible and plausible formations (see Fig.~\ref{fig:inter}, first row) that can be effortlessly classified in our readout module. The difference in representation is also reflected in the angle $\angle$ between vector embeddings aggregated around the two target nodes represented by $\text{mean}(\mathcal{E}_{\mathcal{N}(n^t_{i})})$ and $\text{mean}(\mathcal{E}_{\mathcal{N}(n^t_{j})})$, constructed in our readout module (see Section~\ref{ref:readout}). We find that larger angles are more frequently associated with positive and smaller angles with negative predictions (see  Fig.~\ref{fig:inter}).

We further attempt to interpret the effect of scaling vector embeddings (mimicking \textit{change in magnitude of flow}). In this context, we hypothesize that small $|s_{i}|$ may indicate that GAV is uncertain whether to flip vector embeddings, which is a necessity for sink/source flow. Therefore, $|s_{i}|$ could be interpreted as a measure of certainty (see Fig.~\ref{fig:inter}, last column). This is confirmed by statistical analysis of $|s_{i}|$, demonstrating that high values ($\mu_{|s_{i}|}$ = 0.62) are assigned to more ($\hat{y}^t_{ij}$ $>$ 0.9 or $<$ 0.1) and low values ($\mu_{|s_{i}|}$ = 0.13) to less certain predictions ($0.4$ $<$ $\hat{y}^t_{ij}$ $<$ $0.6$).