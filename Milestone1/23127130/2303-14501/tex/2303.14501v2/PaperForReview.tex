\documentclass[10pt,twocolumn,letterpaper]{article}

%\usepackage{wacv}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{wacv}  % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% TODO add custom/additional packages
\usepackage{multirow}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{1693} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2024}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Link Prediction for Flow-Driven Spatial Networks}

\author{Bastian Wittmann\\
University of Zurich\\
{\tt\small bastian.wittmann@uzh.ch}
\and
Johannes C. Paetzold\\
Technical University of Munich\\
{\tt\small johannes.paetzold@tum.de}
\and
Chinmay Prabhakar\\
University of Zurich\\
{\tt\small chinmay.prabhakar@uzh.ch}
\and
Daniel Rueckert\\
Technical University of Munich\\
{\tt\small daniel.rueckert@tum.de}
\and
Bjoern Menze\\
University of Zurich\\
{\tt\small bjoern.menze@uzh.ch}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
\input{sections/00_abstract}
\end{abstract}

%%%%%%%%% BODY TEXT
\vspace{-0.4em}
\section{Introduction and Motivation}\label{sec:intro}
\input{sections/01_intro}

\section{Related Works}
\input{sections/02_rel_work}

\section{The Graph Attentive Vectors Framework}
\input{sections/03_method}

\section{Experiments and Results}
\input{sections/04_exp_res}

\section{Outlook and Conclusion}
\input{sections/05_outlook}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

%%%%%%%%% APPENDIX
\onecolumn
\appendix

\renewcommand\thefigure{\arabic{figure}}
\setcounter{figure}{5}
\setcounter{table}{5}

\section{Notations}\label{sup:notation}
We provide a lookup table for notations used in our work.

\begin{table*}[h]
\centering
% \scriptsize
\caption{Description of notations used in our work.}
\vspace{-1em}
\label{tab:notations}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c l} 
\toprule
Notation & Description\\
\midrule
$\mathcal{G}$ & undirected, flow-driven spatial network (or graph)\\
$\mathcal{V}$ & set of nodes in $\mathcal{G}$\\
$n_{i}$ & node $i$ in $\mathcal{G}$\\
$\mathcal{E}$ & set of edges in $\mathcal{G}$\\
$e_{ij}$ & edge (or link) in $\mathcal{G}$ between $n_i$ and $n_j$\\
$\overline{e_{ij}}$ & average edge length estimated over edges in $\mathcal{G}$\\
$\sigma$ & standard deviation of edge length estimated over edges in $\mathcal{G}$\\

$t$ & link prediction target\\
$e^{t}_{ij}$ & target link (or edge) between $n_{i}^{t}$ and $n_{j}^{t}$\\
$n_{i}^{t}$ & target node $i$ affiliated to $e^{t}_{ij}$\\

$h$ & number of hops\\
$\mathcal{G}^{t}_{h}$ & $h$-hop subgraph extracted around $e^{t}_{ij}$\\
%$\mathcal{E}^{t}_{h}$ & set of edges in $\mathcal{G}^{t}_{h}$\\

$\mathcal{L}(\mathcal{G}^{t}_{h})$ & line graph representation of $\mathcal{G}^{t}_{h}$\\
$\mathcal{V'}$ & set of nodes in $\mathcal{L}(\mathcal{G}^{t}_{h})$\\
$n'_{i}$ & node (or vector embedding) $i$ in $\mathcal{L}(\mathcal{G}^{t}_{h})$\\
$\mathcal{E'}$ & set of edges in $\mathcal{L}(\mathcal{G}^{t}_{h})$\\
$e'_{ij}$ & edge (or link) in $\mathcal{L}(\mathcal{G}^{t}_{h})$ between $n'_{i}$ and $n'_{j}$\\

$k$ & number of message-passing iterations\\
$s_{i}$ & scalar value generated in GAV layer\\
$|s_{i}|$ & absolute value of $s_{i}$ \\
$\tilde{n}'_{i}$ & intermediate node representation (or vector embedding) in GAV layer\\
$\hat{n}'_{i}$ & updated, refined node representation (or vector embedding)\\

$Q$ & query sequence in multi-head attention operation\\
$K$ & key sequence in multi-head attention operation\\
$V$ & value sequence in multi-head attention operation\\

$\hat{y}^t_{ij}$ & GAV's predicted probability of existence of $e^{t}_{ij}$\\
$y^t_{ij}$ & ground truth label of existence of $e^{t}_{ij}$\\

$\mathcal{E}_{\mathcal{N}(n^t_{i})}$ & set of refined vector embeddings originally created from edges adjacent to $n^t_{i}$\\
$N_{i}$ & matrix consisting of $n'_i$ and its direct neighbors\\
$\mathcal{N}(n_{i})$ & set of nodes in the direct neighborhood of $n_{i}$\\
$\mathcal{N}(n_{i}) \cup n_i $ & set of nodes in the direct neighborhood of $n_{i}$ including $n_{i}$ itself\\
% $\phi_{\theta}$ & -\\
$\phi^{(1)}_{\theta}$, $\phi^{(2)}_{\theta}$ & learnable functions in GAV layer\\
$\phi^{(3)}_{\theta}$ & learnable function in readout module\\

$d_{\text{spatial}}$ & spatial dimension (2 or 3)\\
$d_{\text{message}}$ & dimension of $\tilde{n}'_{i}$ in GAV layer\\

$\delta$ & maximum distance threshold utilized in spatial sampling during preprocessing\\
$\mathbin\Vert$ & concatenation operation\\
$\mathcal{L}_{\text{BCE}}$ & binary cross-entropy loss function\\
% $\psi_b$ & bifurcation angle\\
\bottomrule
\end{tabular}
\end{table*}
\newpage

\section{More on Interpretability and the Modification of Vector Embeddings}\label{sup:inter}
We provide the interested reader with more visualizations regarding the GAV layer's modification of vector embeddings (see Section~\ref{ref:gav_layer}) on the validation set of the ogbl-vessel benchmark, similar to Fig.~\ref{fig:inter}. These visualizations can also be interpreted as qualitative results. Fig.~\ref{fig:pos_preds} depicts subgraph representations $\mathcal{G}^t_h$ ($h$ set to one) of 12 positive (real, plausible) target links, while Fig.~\ref{fig:neg_preds} depicts subgraph representations of 12 negative (sampled, implausible) target links. Please note that the respective last rows depict challenging cases, as indicated by GAV's predicted probabilities $\hat{y}^t_{ij}$. Additionally, we would like to highlight our hypothesis from Section~\ref{ref:inter} that GAV may attempt to assign the two target nodes (red and green) to sink and source nodes for negative, implausible vessel formations (see Fig.~\ref{fig:neg_preds}), which results in superior representations for link prediction in flow-driven spatial networks that can be effortlessly classified in our physically plausible readout module. Please note that we conduct an additional experiment modifying a toy example in Sec.~\ref{sup:toy} to further facilitate interpretability.\\ \\

\begin{figure*}[h]
\centerline{\includegraphics[width=\linewidth]{imgs_supp/pos3.pdf}}
\caption{
Visualization of the effect of our GAV layer on vector embeddings. We visualize subgraph representations $\mathcal{G}^t_h$ of 12 positive target links ($y_{ij}^t = 1$) together with the GAV layer's predicted scalar values $s_{i} \in (-1, 1)$. The scalar values $s_{i}$ used to update vector embeddings in $\mathcal{L}(\mathcal{G}^t_h)$ have been projected to their corresponding edges in $\mathcal{G}^t_h$ (see Fig.~\ref{fig:method_overview}) to provide an interpretable visualization. The directionality of edges (indicated by arrows) already incorporates potential shifts in the direction of vector embeddings enforced by our GAV layer. We additionally report the angle $\angle$ between the vector embeddings aggregated around the two target nodes (see Section~\ref{ref:readout}) and the predicted probability of link existence $\hat{y}^t_{ij}$.
}
\label{fig:pos_preds}
\end{figure*}
\newpage

\begin{figure*}[h]
\centerline{\includegraphics[width=\linewidth]{imgs_supp/neg2.pdf}}
\caption{
Visualization of the effect of our GAV layer on vector embeddings. We visualize subgraph representations $\mathcal{G}^t_h$ of 12 negative target links ($y_{ij}^t = 0$) together with the GAV layer's predicted scalar values $s_{i} \in (-1, 1)$. The scalar values $s_{i}$ used to update vector embeddings in $\mathcal{L}(\mathcal{G}^t_h)$ have been projected to their corresponding edges in $\mathcal{G}^t_h$ (see Fig.~\ref{fig:method_overview}) to provide an interpretable visualization. The directionality of edges (indicated by arrows) already incorporates potential shifts in the direction of vector embeddings enforced by our GAV layer. We additionally report the angle $\angle$ between the vector embeddings aggregated around the two target nodes (see Section~\ref{ref:readout}) and the predicted probability of link existence $\hat{y}^t_{ij}$.
}
\label{fig:neg_preds}
\end{figure*}

\section{Initialization of Vector Embeddings}\label{sup:imp_det}
The initialization of the direction of vector embeddings represents an important implementation detail and is based on a straightforward intuition. To be precise, we initialize vector embeddings to point away from the target link $e^{t}_{ij}$, \ie, towards nodes with a node degree of one (leaf nodes). The vector embedding representative of the target link $e^{t}_{ij}$ is set to point from $n_{i}^{t}$ to $n_{j}^{t}$. An exemplary initialization of vector embeddings for a 1-hop subgraph can be found in Fig.~\ref{fig:method_overview}.

\section{GAV and Structural Properties}\label{sup:toy} % does gav really learn structural features?
This section elaborates on how GAV's predictions rely heavily on structural properties, such as bifurcation angles, which reflect functional properties of the underlying physical system~\cite{schneider2012tissue}.
% , such as our simplified definition of physical flow.
To this end, we prepare and modify a synthetic mock example in Fig.~\ref{fig:morp}. Specifically, we vary the bifurcation angle $\psi_b$ spanned between two edges connected to $n^t_i$ (red) to generate morphological implausible and plausible blood vessel formations (see Fig.~\ref{fig:morp}).
% , fulfilling morphological and hemodynamic properties.
%formed around the target link (orange).

\begin{figure*}[h]
\centerline{\includegraphics[width=0.7\linewidth]{imgs_supp/morph_exp5.pdf}}
\caption{
Morphological implausible (left) and plausible (middle and right) blood vessel formations formed around the target link (orange) with varying bifurcation angles $\psi_b$. GAV correctly identifies morphological plausible blood vessel formations that fulfill relevant hemodynamic functional properties~\cite{schneider2012tissue}.
%, by assigning a high probability of link existence to plausible formations.
 }
\label{fig:morp}
% \vspace{-1em}
\end{figure*}

\noindent
As expected, GAV differentiates between plausible and implausible blood vessel formations formed around the target link. GAV assigns a high probability of target link existence $\hat{y}^t_{ij}$ to plausible and a low probability of target link existence to implausible formations. Please note that Fig.~\ref{fig:morp} additionally maps the potentially modified directionality of vector embeddings onto their corresponding edges, similar to Fig.~\ref{fig:pos_preds} and Fig.~\ref{fig:neg_preds}. One can observe the shift in the directionality of vector embeddings created from edges adjacent to nodes with high bifurcation angles $\psi_b$, transforming the target nodes (red and green) to sink and source nodes for morphological implausible blood vessel formations (see Fig.~\ref{fig:morp}, left).


\section{Visualization of Datasets}\label{sup:data}
In Fig.~\ref{fig:data}, we graphically visualize two flow-driven spatial networks representative of murine whole-brain vessel graphs and road networks.

\begin{figure*}[h]
\centerline{\includegraphics[width=\linewidth]{imgs_supp/data.pdf}}
\caption{
Visualization of a whole-brain vessel graph and a road network. The depicted flow-driven spatial networks correspond to the raw cd1-tc-vessel and luxembourg-road datasets.
}
\label{fig:data}
\vspace{-1em}
\end{figure*}


\section{More Ablations on the GAV Layer}\label{sup:gav_abl}
Since the GAV layer relies on a set of specific design choices, we conduct additional ablation studies determining their influence on the link prediction performance on the validation set of the ogbl-vessel benchmark. To this end, we experiment with different versions of the GAV layer. First, we deactivate the multi-head attention operation; second, we exclude the residual connection; and third, we exchange the leaky ReLU non-linearity in the GAV layer with the ReLU non-linearity. We report our findings in Table~\ref{tab:ablations_des_layer}.

\begin{table}[h]
\centering
\scriptsize
\caption{Ablations on the GAV layer's main design choices.}
\vspace{-1em}
\label{tab:ablations_des_layer}
\begin{tabular}{c c c|c c c c} 
\toprule
Attention & Residual Connection & Leaky ReLU & $\text{AUC}\uparrow$ &$\text{Hits@100}\uparrow$& $\text{Hits@50}\uparrow$& $\text{Hits@20}\uparrow$ \\
\midrule
\cmark & \cmark & \cmark & \cellcolor{teal!40}98.39 & \cellcolor{teal!20}34.46 & \cellcolor{teal!10}26.30 & \cellcolor{teal!40}19.81\\
\xmark & \cmark & \cmark & \cellcolor{teal!10}97.48 & 15.56 & 9.18 & 5.68\\
\cmark & \xmark & \cmark & \cellcolor{teal!20}98.34 & \cellcolor{teal!40}34.90 & \cellcolor{teal!40}27.61 & \cellcolor{teal!20}19.28\\
\cmark & \cmark & \xmark & \cellcolor{teal!20}98.34 & \cellcolor{teal!10}34.16 & \cellcolor{teal!20}26.47 & \cellcolor{teal!10}17.31\\
\bottomrule
\end{tabular}
\end{table}

\noindent
Deactivating the multi-head attention operation (second row) results in a drastic AUC decrease of 0.91, indicating the importance of neighborhood awareness when modifying vector embeddings via our proposed GAV layer. Excluding the GAV layer's residual connection (third row) and using ReLU instead of Leaky ReLU non-linearities (fourth row) leads to a slight reduction in AUC of 0.05, respectively. Based on our reported standard deviation value of $\pm$ 0.02 (see Table~\ref{tab:quantitative_results}), we argue that this performance decrease is indeed significant.

\section{Evaluation Metrics}\label{sup:metrics}
To compare GAV to existing baseline algorithms, we report quantitative results based on the area under the receiver operating characteristic curve (AUC), following the obgl-vessel benchmark. The AUC metric indicates the performance of a classifier by plotting the true positive rate against the false positive rate at all possible classification thresholds. Therefore, AUC provides an aggregate performance measure indicating the classifier's ability to distinguish between positive and negative links.
% Statistically speaking, AUC scores should levitate between the values of 50\% and 100\%, where a AUC score of 50\% indicates a more-or-less random guess, while a perfect classifier achieves a AUC score of 100\%.

We introduce the evaluation metric Hits@$k$ as an additional, stricter performance measure. Hits@$k$ compares the classifier's prediction of every single positive link to a randomly sampled set of 100,000 negative links, resulting in a ranking among 100,001 links with respect to the probability of link existence. Based on this ranking, Hits@$k$ indicates the ratio of positive links ranked at the $k$-th place and above. In the context of this work, we evaluate Hits@$k$ at $k=100$, $k=50$, and $k=20$, inspired by other Open Graph Benchmark~\cite{hu2020ogb} link prediction benchmarks.


\section{Configuration of Our Secondary Baseline}\label{sup:sec_base}
We incorporate the EdgeConv message-passing layer~\cite{wang2019dynamic} into the SEAL framework~\cite{zhang2018link, zhang2021labeling}, which has been shown to deliver results on par with or superior to the state-of-the-art on multiple link prediction benchmarks, to introduce a strong, \textit{secondary baseline} (SEAL+EdgeConv) for link prediction in spatial networks. To be precise, we incorporate EdgeConv in SEAL's DGCNN~\cite{zhang2018end}. EdgeConv's update function can be observed in Table~\ref{tab:update}. Here, $\phi_{\theta}$ represents a two-layer MLP with an input dimension of 64, a hidden dimension of 32, and an output dimension of 32. Our modified DGCNN employs in total three EdgeConv layers, with the only difference being that the input dimension of the first EdgeConv layer's MLP corresponds to 70 and the output dimension of the third EdgeConv layer's MLP to 1. Our EdgeConv version utilizes a mean feature aggregation scheme. We set the number of in- and output channels of the DGCNN readout operation's two 1D convolutions to 1 \& 16 and 16 \& 32, respectively. The kernel sizes and strides of the two 1D convolutions correspond to 65 \& 65 and 5 \& 1. We set the input, hidden, and output dimensions of the DGCNN readout operation's MLP to 38, 128, and 1. The global sort pooling layer's parameter k is set to 10. All hyperparameters were tuned on the validation set of the ogbl-vessel benchmark.

\section{GAV's Performance on Non-Flow-Based Link Prediction Benchmarks}\label{sup:non_flow}
To additionally confirm that GAV is specialized for link prediction in flow-driven spatial networks, we conduct an experiment on the ogbl-collab benchmark~\cite{hu2020ogb}, which represents a collaboration network given by an undirected graph where nodes are associated with authors while edges indicate collaborations between them. Node features are comprised of 128-dimensional vectors representative of an author's scientific work (averaged word embeddings reflecting the content of scientific papers). Based on the collaboration network, the task is to predict future collaborations between authors. To adjust GAV to the task of the ogbl-collab benchmark, we model vector embeddings representative of edges in the collaboration network as the difference between 128-dimensional feature vectors of two nodes incident to an edge. We report our findings in Table~\ref{tab:quant_res_collab}.

\begin{table}[h]
\centering
\scriptsize
\caption{Comparison between GAV and SEAL on ogbl-collab and ogbl-vessel. Please note that the increase in GAV's trainable parameters in the experiment on ogbl-collab is mostly due to the increased number of node features (128 vs. 3).}
\vspace{-0.5em}
\label{tab:quant_res_collab}
\begin{tabular}{c|c|l|c} 
\toprule
Dataset & Model & $\#\ \text{Params}\downarrow$ & $\text{Eval. Metric}\uparrow$  (\%)\\
\midrule
\multirow{2}{*}{ogbl-collab}
& SEAL~\cite{zhang2021labeling} & \cellcolor{teal!20}501,570 & \cellcolor{teal!40}64.72 $\text{Hits@50}$\\
& GAV (ours) & \cellcolor{teal!40}44,194 & \cellcolor{teal!20}16.72 $\text{Hits@50}$\\
\midrule
\multirow{2}{*}{ogbl-vessel}
& SEAL~\cite{zhang2021labeling} & \cellcolor{teal!20}172,610 & \cellcolor{teal!20}80.50 $\text{AUC}$\\
& GAV (ours) & \cellcolor{teal!40}8,194 & \cellcolor{teal!40}98.38 $\text{AUC}$\\
\bottomrule
\end{tabular}
\end{table}

\noindent
As expected, GAV, relying on the idea of modeling simplified physical flow in flow-driven spatial networks, does not deliver competitive results on the ogbl-collab benchmark. This is because GAV's strong inductive biases are tailored to link prediction in flow-driven spatial networks and are, therefore, too restrictive for non-flow-based networks, such as ogbl-collab. This repeatedly demonstrates GAV's ability to intuitively model the underlying physical process in flow-driven spatial networks.
%This repeatedly demonstrates GAV's ability to intuitively models the underlying physical process in flow-driven spatial networks, representing a strong inductive bias for the link prediction task.

To adapt GAV to non-flow-based networks more appropriately, we encourage future work to explore the use of pseudo-spatial positions embedded in nodes of non-flow-based networks rather than its actual node features for the sake of creating vector embeddings. Pseudo-spatial position could, \eg, be determined based on the Fruchterman-Reingold force-directed algorithm~\cite{kobourov2012spring}.


\section{On Translation and Rotation Invariance}\label{sup:invariance}
\begin{wrapfigure}{r}{0.35\textwidth}
  \vspace{-3em}
  \begin{center}
    \includegraphics[width=0.35\textwidth]{imgs_supp/rot_inv.pdf}
  \end{center}
  \vspace{-1.5em}
  \caption{Exemplary $\mathcal{G}^{t}_{h}$ extracted from the ogbl-vessel benchmark around a negative and positive link used for experiments in Table~\ref{tab:rot_inv}.}
  \vspace{-1em}
  \label{fig:rot_inv}
\end{wrapfigure}
In this section, we briefly investigate GAV's behavior under rotations and translations of the $h$-hop enclosing subgraph $\mathcal{G}^{t}_{h}$. Specifically, we aim to investigate whether rotation and translation of $\mathcal{G}^{t}_{h}$ result in similar predictions. Since GAV encodes edges as vector embeddings spanned between two nodes (see Section~\ref{ref:subnetw_extraction}), translation invariance is \textit{explicitly ensured}.
However, even though rotation preserves the length and relative angles of edges, rotation invariance is \textit{not explicitly ensured}. This is, \eg, because queries and keys forwarded to the GAV layer's attention operation are not explicitly rotation equi- or invariant, which is one of the key requirements for rotation invariant attention weights and hence potential rotation invariant predictions~\cite{fuchs2020se}. An empirical experiment rotating exemplary input graphs around all three axes, however, demonstrates that GAV's predictions are relatively robust to rotation, indicating to some degree implicit, learned rotation invariance (see Table~\ref{tab:rot_inv}). We encourage future work to further explore the necessity of explicitly encoded translation and rotation invariance in the context of link prediction for flow-driven spatial networks.

\begin{table}[h]
\centering
\scriptsize
\caption{Experiment on rotation invariance of predictions. We rotate three exemplary subgraphs $\mathcal{G}^{t}_{h}$ around all three axes with a step size of 1$^{\circ}$ to investigate GAV's behavior under rotation. We report the standard deviation of predicted target link probability $\hat{y}^t_{ij}$ over all 360 predictions.}
\vspace{-0.5em}
\label{tab:rot_inv}
\begin{tabular}{l|c|c|c} 
\toprule
Subgraph $\mathcal{G}^{t}_{h}$ & $\sigma_{\hat{y}^t_{ij}} \text{(x-axis)}$ & $\sigma_{\hat{y}^t_{ij}} \text{(y-axis)}$ & $\sigma_{\hat{y}^t_{ij}} \text{(z-axis)}$\\
\midrule
Fig.~\ref{fig:morp}, right & 5.15$\cdot e^{-\text{5}}$ & 1.86$\cdot e^{-\text{6}}$ & 1.34$\cdot e^{-\text{5}}$\\
Fig.~\ref{fig:rot_inv}, left & 9.63$\cdot e^{-\text{2}}$ & 1.12$\cdot e^{-\text{3}}$ & 7.64$\cdot e^{-\text{3}}$\\
Fig.~\ref{fig:rot_inv}, right & 1.85$\cdot e^{-\text{3}}$ & 1.08$\cdot e^{-\text{3}}$ & 3.37$\cdot e^{-\text{4}}$\\
\bottomrule
\end{tabular}
\end{table}

\section{Commonalities and Differences between SEAL and GAV}\label{sup:commonalities}
Since the influential SEAL link prediction framework~\cite{zhang2018link, zhang2021labeling} represents one of the most prominent works on learned, GNN-based link prediction algorithms, we would like to clearly state the commonalities and differences between SEAL and our proposed link prediction algorithm tailored to flow-driven spatial networks, GAV. Although GAV utilizes some concepts introduced by SEAL (subgraph extraction/classification \& labeling trick), which are provably used in most competitive approaches and can, therefore, be seen as common practices, we, for the first time, introduce the principle of physical flow to link prediction. To this end, we propose not only a novel flow-inspired, parameter-efficient message-passing layer updating vector embeddings but also a physically plausible readout module facilitating interpretability. Our contributions result in an increase of more than 22\% in AUC compared to SEAL on ogbl-vessel.

\section{Message-Passing Update Functions}\label{sup:update}
We provide a concise overview of message-passing layers featured in our work and their respective high-level, final node update functions in Table~\ref{tab:update}. Here, $d_i$ stands for the node degree of $n_i$, $\alpha_{ij}$ for the learned attention coefficient between $n_i$ and $n_j$, and $\phi_{\theta}$ for an arbitrary learnable function. We would like to highlight the simplicity of the GAV layer's final update function.

\begin{table}[h]
\centering
% \scriptsize
\caption{Message-passing update functions.}
\vspace{-0.5em}
\label{tab:update}
\renewcommand{\arraystretch}{2}
\begin{tabular}{c | c} 
\toprule
Message-Passing Layer & Update Function\\
\midrule
GAV layer & $\displaystyle \hat{n}_{i} = s_i \cdot n_i$\\

EdgeConv~\cite{wang2019dynamic} & $\displaystyle \hat{n}_{i} = \frac{1}{|\mathcal{N}(n_i)|} \sum_{n_j \in \mathcal{N}(n_i)} \phi_{\theta}(n_{i} \mathbin\Vert n_{j} - n_{i})$\\
GAT layer~\cite{velivckovic2017graph} & $\displaystyle \hat{n}_{i} = \alpha_{ii} \cdot \phi_{\theta}(n_{i}) + \sum_{n_j \in \mathcal{N}(n_i)} \alpha_{ij} \cdot \phi_{\theta}(n_{j})$\\
 % & $\displaystyle \text{where}\ \alpha_{ij} = \frac{\text{exp}\, (\text{Leaky ReLU}\, ( a^{T} [\phi_{\theta}(n_{i}) \mathbin\Vert  \phi_{\theta}(n_{j})]\,)\,)} {\displaystyle\sum_{{n_k \in \mathcal{N}(n_i)}}  \text{exp}\, (\text{Leaky ReLU}\, ( a^{T} [\phi_{\theta}(n_{i}) \mathbin\Vert  \phi_{\theta}(n_{k})])\,)\,}$\\

SAGE layer~\cite{hamilton2017inductive} & $\displaystyle \hat{n}_{i} =  \phi_{\theta}^{(1)}(n_{i}) +  \phi_{\theta}^{(2)}(\frac{1}{|\mathcal{N}(n_i)|} \sum_{n_j \in \mathcal{N}(n_i)} n_{j})$\\

GCN layer~\cite{kipf2016semi} & $\displaystyle \hat{n}_{i} =  \phi_{\theta}(  \sum_{n_j \in \mathcal{N}(n_i)\, \cup\, n_i} \frac{1}{\sqrt{d_i \cdot d_j}}\, n_j)$\\
\bottomrule
\end{tabular}
\end{table}

\section{Medical Relevance of the Link Prediction Task for Whole-Brain Vessel Graphs}\label{sup:relevance}
Since GAV has been developed around the ogbl-vessel benchmark, we would like to provide more details on the medical relevance and the application of link prediction algorithms for whole-brain vessel graphs. As already mentioned in Section~\ref{sec:intro}, vascular network representations of the brain originate from a multi-stage, imperfect process, typically consisting of a segmentation stage followed by a graph extraction stage (skeletonization and pruning). Detailed pipelines for whole-brain vessel graph generation can be found in the literature~\cite{paetzold2021whole, walchli2021hierarchical, meyer2009voreen, drees2021scalable}. Each stage of the graph generation pipeline introduces noise and artifacts to the extracted whole-brain vessel graphs. The initial segmentation stage~\cite{todorov2020machine}, \eg, often results in under- or over-connected vessel segmentation masks, which in turn result in equally under- or over-connected whole-brain vessel graphs. This is mostly due to the shortage of annotated training data (especially in the 3D domain), which is required for accurate vessel segmentation via supervised state-of-the-art deep-learning-based segmentation techniques.
Under-/over-connectivity, however, limits the application of whole-brain vessel graphs for subsequent medically relevant downstream tasks, such as the diagnosis, treatment, and analysis of neurovascular brain disorders (\eg, aneurysms or strokes). This is because these downstream tasks require flawlessly connected whole-brain vessel graphs free of artifacts to obtain a deeper understanding of neurovascular brain disorders by, \eg, accurately recognizing anomalies in blood flow patterns via blood flow modeling~\cite{schmid2021severity}.
To overcome the obstacle of under-/over-connectivity in whole-brain vessel graphs and, therefore, to enable researchers to obtain a more accurate and advanced understanding of neurovascular brain disorder, one can either optimize whole-brain vessel graph generation pipelines~\cite{shit2022relationformer, shit2021cldice} or utilize the task of link prediction, which we extensively investigate in this work.

\end{document}