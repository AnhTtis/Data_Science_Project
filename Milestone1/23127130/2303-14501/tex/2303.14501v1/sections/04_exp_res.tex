\begin{table*}[t]
\centering
\scriptsize
\caption{Quantitative results achieved on the test sets. We report mean and standard deviation values on the ogbl-vessel benchmark based on ten different seeds. Please note that the ogbl-vessel benchmark's evaluation metric is AUC. Therefore, Hits@$k$ values of participating algorithms are not available.
GAV outperforms the previous state-of-the-art across all metrics and datasets.}
\label{tab:quantitative_results}
\begin{tabular}{p{50pt}|l|l|c|c|c|c} 
\toprule
Dataset & Model &$\#\ \text{Params}\downarrow$ &$\text{AUC}\uparrow$ (\%)& $\text{Hits@100}\uparrow$  (\%)& $\text{Hits@50}\uparrow$  (\%)& $\text{Hits@20}\uparrow  (\%)$\\ 
\midrule
\multirow{11}{*}{ogbl-vessel}
& GCN~\cite{kipf2016semi} & 396,289 & 43.53 ± 9.61 & - & - & - \\
& MLP & 1,037,577 & 47.94 ± 1.33 & - & - & - \\
& Adamic-Adar~\cite{adamic2003friends} &  \textbf{0} & 48.49 ± 0.00 & - & - & - \\
& GraphSAGE~\cite{hamilton2017inductive} & 396,289 & 49.89 ± 6.78 & - & - & - \\
& SAGE+JKNet~\cite{xu2018representation} & 273 & 50.01 ± 0.07 & - & - & - \\
& SGC~\cite{wu2019simplifying} & 897 & 50.09 ± 0.11 & - & - & - \\
& LRGA~\cite{puny2020global} & 26,577 & 54.15 ± 4.37 & - & - & - \\
& SEAL~\cite{zhang2021labeling} & 172,610 & 80.50 ± 0.21 & - & - & - \\
& SIEG~\cite{hu2020ogb} & 407,338 & 83.07 ± 0.44 & - & - & - \\
\cmidrule{2-7}
& SEAL (EdgeConv) & 49,346 & 97.53 ± 0.32 & 16.09 ± 10.48 & 9.37 ± 6.18 & 4.99 ± 4.24\\
& GAV (ours) & 8,184 & \textbf{98.38 ± 0.02} & \textbf{34.77 ± 0.94} & \textbf{28.02 ± 1.58} & \textbf{19.71 ± 2.31}\\
\midrule
\multirow{3}{*}{c57-tc-vessel}
& SEAL~\cite{zhang2021labeling} & 43,010 & 78.21 & 0.12 & 0.06 & 0.01\\
& SEAL (EdgeConv) & 49,346 & 97.23 & 16.71 & 10.39 & 5.01\\
& GAV (ours) & \textbf{8,184} & \textbf{98.24} & \textbf{33.26} & \textbf{26.89} & \textbf{21.32}\\
\midrule
\multirow{3}{*}{cd1-tc-vessel}
& SEAL~\cite{zhang2021labeling} & 43,010 & 83.60 & 0.27 & 0.16 & 0.06\\
& SEAL (EdgeConv) & 49,346 & 97.91 & 17.05 & 11.57 & 2.98\\
& GAV (ours) & \textbf{8,184} & \textbf{98.72} & \textbf{35.82} & \textbf{27.25} & \textbf{17.23}\\
\midrule
\multirow{3}{*}{c57-cc-vessel}
& SEAL~\cite{zhang2021labeling} & 43,010 & 83.75 & 0.65 & 0.44 & 0.24\\
& SEAL (EdgeConv) & 49,346 & 97.49 & 7.21 & 3.35 & 1.06\\
& GAV (ours) & \textbf{8,184} & \textbf{97.99} & \textbf{18.90} & \textbf{14.58} & \textbf{9.04}\\
\midrule
\multirow{3}{*}{belgium-road}
& SEAL~\cite{zhang2021labeling} & 43,010 & 86.73 & 1.25 & 0.68 & 0.30\\
& SEAL (EdgeConv) & 49,346 & 96.98 & 0.55 & 0.55 & 0.51\\
& GAV (ours) & \textbf{8,184} & \textbf{99.29} & \textbf{47.44} & \textbf{38.60} & \textbf{22.11}\\
\midrule
\multirow{3}{*}{italy-road}
& SEAL~\cite{zhang2021labeling} & 43,010 & 90.07 & 0.32 & 0.16 & 0.08\\
& SEAL (EdgeConv) & 49,346 & 90.24 & 0.26 & 0.17 & 0.07\\
& GAV (ours) & \textbf{8,184} & \textbf{99.41} & \textbf{28.49} & \textbf{20.08} & \textbf{11.99}\\
\midrule
\multirow{3}{*}{netherlands-road}
& SEAL~\cite{zhang2021labeling} & 43,010 & 84.19 & 0.00 & 0.00 & 0.00\\
& SEAL (EdgeConv) & 49,346 & 96.06 & 3.91 & 2.20 & 1.01\\
& GAV (ours) & \textbf{8,184} & \textbf{99.44} & \textbf{37.55} & \textbf{26.97} & \textbf{10.77}\\
\midrule
\multirow{3}{*}{luxembourg-road}
& SEAL~\cite{zhang2021labeling} & 43,010 & 89.79 & 11.39 & 6.15 & 3.12\\
& SEAL (EdgeConv) & 49,346 & 97.53 & 59.79 & 39.15 & 19.42\\
& GAV (ours) & \textbf{8,184} & \textbf{99.31} & \textbf{85.88} & \textbf{76.84} & \textbf{61.95}\\
\bottomrule
\end{tabular}
% \vspace{-0.5em}
\end{table*}

In this section, we demonstrate the performance of our proposed GAV framework on the ogbl-vessel benchmark~\cite{hu2020ogb} and on additional datasets sourced from publicly available flow-driven spatial networks.
We first elaborate on baseline algorithms and the experimental setup, followed by a detailed description of our used datasets. Finally, we introduce the evaluation metrics, report quantitative results, investigate our design choices by conducting detailed ablation studies, and discuss GAV's interpretability.

\subsection{Baselines and Experimental Setup}\label{ref:baselines}
To evaluate GAV properly, we experimented with different baseline algorithms. We ultimately settled for SEAL~\cite{zhang2018link, zhang2021labeling}, which has shown to deliver results on par with or superior to the state-of-the-art on multiple link prediction benchmarks.
% , such as ogbl-ppa~\cite{hu2020ogb}, ogbl-vessel~\cite{hu2020ogb}, ogbl-citation2~\cite{hu2020ogb}, and ogbl-collab~\cite{hu2020ogb}. 
Additionally, we propose a new \emph{secondary baseline} combining SEAL with the EdgeConv message-passing layer~\cite{wang2019dynamic}, following recent trends in graph-based object detection from point clouds~\cite{chai2021point, yang2022graph, wang2021object, yin2020lidar}. Equation~\ref{eq:edgeconv} describes EdgeConv's update function in detail, where $\phi_{\theta}$ represents a two-layer MLP.
\begin{equation} \label{eq:edgeconv}
\hat{n}_{i} = \frac{1}{|\mathcal{N}(n_i)|} \sum_{n_j \in \mathcal{N}(n_i)} \phi_{\theta}(n_{i} \mathbin\Vert n_{j} - n_{i})
\end{equation}
This provides us with an improved, highly competitive secondary baseline for link prediction on spatial networks. An empirical analysis varying SEAL's message-passing layer confirmed this decision. We additionally refined SEAL's parameters via a hyperparameter search.

GAV was trained using the Adam optimizer~\cite{kingma2014adam} with a learning rate of 0.001 and a batch size of 32 on a single Quadro RTX 8000 GPU until convergence.
An ablation study on the number of hops $h$ in the subgraph extraction module and the number of message-passing iterations $k$ indicates that setting both to one is sufficient (see Table \ref{tab:ablations_kh}).
In the GAV layer, the number of heads of the multi-head attention operation is set to 4, while $\phi^{(1)}_{\theta}$ represents a single linear layer with an output dimension of $d_{\text{message}} = 32$, and $\phi^{(2)}_{\theta}$ is given by a two-layer MLP with a hidden dimension of 64.
The GAV layer makes use of leaky ReLU non-linearities~\cite{maas2013rectifier} to increase gradient flow and simplify weight initialization. The readout module's learnable function $\phi^{(3)}_{\theta}$ is represented by a two-layer MLP with a hidden dimension of 128.
% We refrain from utilizing appropriate data augmentation techniques, such as random rotation and scaling, in the subnetwork extraction module since we observed no noticeable performance difference.
All hyperparameters were tuned on the validation set of the ogbl-vessel benchmark.

\subsection{Datasets}
We experiment with multiple 2D and 3D flow-driven spatial networks to demonstrate the generalizability of our approach (see Table~\ref{tab:datasets}). In total, we conduct experiments on eight networks, given by whole-brain vessel graphs of different mouse strains and road networks of various European countries. In this context, link prediction can be interpreted as predicting the probability of the existence of blood vessels and road segments.
%Both network types are characterized by a physical transportation process (physical flow). 

\paragraph{Whole-Brain Vessel Graphs}
Blood vessels represent fascinating structures forming complex networks that transport oxygen and nutrients throughout the human body. The vascular system is, therefore, intuitively represented as a flow-driven spatial network, where branching points of vessels typically represent nodes embedding $x$-, $y$-, and $z$-coordinates, while edges are defined as blood vessels running between branching points~\cite{paetzold2021whole}. We report results on the Open Graph Benchmark's ogbl-vessel benchmark~\cite{hu2020ogb}, which measures the performance of different link prediction algorithms with regard to whole-brain vessel graphs. The ogbl-vessel benchmark consists of millions of nodes and edges (see Table~\ref{tab:datasets}) and describes the murine brain vasculature all the way down to the microcapillary level.
However, we not only experiment with the ogbl-vessel benchmark but also source three additional whole-brain vessel graphs of different mouse strains acquired via different imaging methodologies~\cite{todorov2020machine, walchli2021hierarchical} (see Table~\ref{tab:datasets}, footnote).

% include four different whole-brain vessel graphs of different mouse strains consisting of millions of nodes and edges (see Table~\ref{tab:datasets}) describing the murine brain vasculature all the way down to the microcapillary level.
% In whole brain vessel graphs, branching points of vessels represent nodes embedding x, y, and z coordinates, while edges are defined as blood vessels connecting branching points~\cite{paetzold2021whole}. The whole brain vessel graphs have been acquired via different imaging methodologies~\cite{todorov2020machine, walchli2021hierarchical}.

\paragraph{Road Networks}
Further, we report results on diverse road networks representative of four European countries for a thorough evaluation of GAV's performance. To this end, we adopt publicly available road networks introduced in the DIMACS graph partitioning and clustering challenge~\cite{bader201110th}. These road networks correspond to the largest connected components of OpenStreetMap's~\cite{OpenStreetMap} road networks and are vastly different in size (\eg, luxembourg-road constitutes roughly 100,000 edges, whereas italy-road has more than 7,000,000). 
% Moreover, these road networks are heavily influenced by the countries' topography, e.g., Italy is dominated by many mountain ranges, and the Netherlands is entirely flat, leading to vastly different network shapes. 
In road networks, intersections and locations with strong curvature represent nodes in the form of $x$- and $y$-coordinates, while connecting roads represent edges. 
% Intersections are given by spatial locations in the form of $x$- and $y$-coordinates.

% The data originates from openstreetmap. For reproducibility, we use the archived data version from the DIMACS graph partitioning, and graph clustering challenge \cite{bader201110th}. The road networks are vastly different in size, e.g., Luxembourg constitutes ~100 000 edges, whereas Italy has more than 7 000 000, see Table \ref{tab:datasets}. Moreover, the countries road networks are heavily influenced by the countries' topography, e.g., Italy is dominated by many mountain ranges, and the Netherlands is entirely flat, leading to vastly different network shapes. Similar to the vessel graphs, intersections and locations with strong curvature represent nodes, and the connecting roads represent the edges. The only node features are the x,y coordinates. 


\paragraph{Preprocessing}
Link prediction datasets require positive (label 1) and negative links (label 0). Positive links correspond to existent edges in our datasets, whereas negative links represent artificially created, non-existent edges. As link prediction algorithms are commonly employed to improve the graph representation through the identification of absent connections and the reduction of local noise arising from graph generation, negative links should appear as authentic as possible.
% Therefore, sampling of negative edges requires a sophisticated sampling strategy, which also takes the nodes' spatial locations into account. 
In light of the absence of negative links in our sourced datasets, we prepare our sourced datasets in a manner that aligns with the ogbl-vessel benchmark. Following the ogbl-vessel benchmark, we sample negative links using a spatial sampling strategy. To be precise, we randomly connect nodes in close proximity, taking a maximum distance threshold of $\delta = \overline{e_{ij}} + 2 \sigma $ into account. Here, $\overline{e_{ij}}$ denotes the average edge length estimated over the entire graph $\mathcal{G}$ and $\sigma$ the standard deviation. 
The number of negative, sampled links corresponds to the number of positive, real links across all datasets. 
We finally split positive and negative links into training, validation, and test sets (split 80\%/10\%/10\%).

\subsection{Evaluation Metrics}\label{ref:eval_met}
To compare GAV to existing baseline algorithms, we report quantitative results based on the area under the receiver operating characteristic curve (AUC), following the obgl-vessel benchmark. The AUC metric indicates the performance of a classifier by plotting the true positive rate against the false positive rate at all possible classification thresholds. Therefore, AUC provides an aggregate performance measure indicating the classifier's ability to distinguish between positive and negative links.
%Statistically speaking, ROC-AUC scores should levitate between the values of 50\% and 100\%, where a ROC-AUC score of 50\% indicates a more-or-less random guess, while a perfect classifier achieves a ROC-AUC score of 100\%.

We introduce the evaluation metric Hits@$k$ as an additional, stricter performance measure. Hits@$k$ compares the classifier's prediction of every single positive link to a randomly sampled set of 100,000 negative links, resulting in a ranking among 100,001 links with respect to the probability of link existence. Based on this ranking, Hits@$k$ indicates the ratio of positive links ranked at the $k$-th place and above.
%In the context of this work, we evaluate Hits@$k$ at $k=100$, $k=50$, and $k=20$.


\subsection{Quantitative Results}
GAV demonstrates excellent, superior performances on the task of link prediction across all metrics and datasets, as can be observed in Table~\ref{tab:quantitative_results}.
We outperform the current state-of-the-art algorithm SIEG on the ogbl-vessel benchmark by \textbf{more than 18\%} (98.38 vs. 83.07 AUC) while requiring a significantly smaller amount of trainable parameters (8,184 vs. 407,338). However, GAV not only drastically outperforms the current state-of-the-art but also our introduced strong, secondary baseline, combining the SEAL framework with EdgeConv. The excellent performance and superiority of our GAV framework is even more pronounced when considering the strict evaluation metric of Hits@$k$.

Quantitative results reported in Table~\ref{tab:quantitative_results} additionally indicate the strong performance of our secondary baseline (see Section~\ref{ref:baselines}), surpassing previous state-of-the-art methods in AUC across all but one dataset, namely italy-road. 
It is of note that the luxembourg-road dataset's test set contains only 12,000 negative links. We, therefore, compare predictions of its positive links to 12,000 rather than 100,000 negative links (see Section~\ref{ref:eval_met}). This explains the comparatively strong Hits@$k$ performances on the luxembourg-road dataset.
% In an additional experiment, we train our GAV framework on one mouse brain, we then run inference on an entirely different specimen. We find an equally high performance on this unseen specimen (98.5 vs. 98.10 AUC). We conclude that our GAV framework is capable of learning the properties of specific flow driven spatial networks.


\subsection{Ablation Studies}
To further validate GAV, we conduct detailed ablation studies on the validation set of the ogbl-vessel benchmark. 
Table~\ref{tab:ablations_des} investigates the importance of the readout module, the message-passing module, and the labeling trick.
\begin{table}[h]
\centering
\scriptsize
\caption{Ablations on main design choices.}
\label{tab:ablations_des}
\begin{tabular}{c c c|c c} 
\toprule
Readout Module & Message-Passing & Labeling Trick & $\text{AUC}\uparrow$ &$\Delta$ \\
\midrule
\cmark & \cmark & \cmark & \textbf{98.39} & $-$\\
\xmark & \cmark & \cmark & 98.28 & -0.11\\
\cmark & \xmark & \cmark & 80.56 & -17.83\\
\cmark & \cmark & \xmark & 96.00 & -2.39\\
\bottomrule
\end{tabular}
\end{table}
First, we exchange our readout module with a SortPooling layer followed by two convolutional layers and an MLP, resembling SEAL's readout operation. We note that our readout module is more applicable to flow-driven spatial networks, as it leads to a modest AUC increase of 0.11. Second, we completely deactivate the message-passing module by forwarding $\mathcal{L}(\mathcal{G}^{t}_{h})$ directly to the readout module. We observe a drastic AUC decrease of 17.83, indicating the importance of modifying the vector embeddings via our proposed GAV layer.
Finally, we evaluate the impact of our labeling trick. Excluding the additional labels results in an AUC decrease of 2.39. This proves the significance of link identification via additional, distinct labels.\\

In a second ablation study, we experiment with different message-passing layers, including EdgeConv, in our message-passing module. We report our findings in Table~\ref{tab:ablations_operators}.
\begin{table}[h]
\centering
\scriptsize
\caption{Ablations with different message-passing layers.}
\label{tab:ablations_operators}
\begin{tabular}{c |c c c c} 
\toprule
Message-Passing Layer & $\text{AUC}\uparrow$ & $\text{Hits@100}\uparrow$& $\text{Hits@50}\uparrow$& $\text{Hits@20}\uparrow$ \\
\midrule
GAV layer (ours) &\textbf{98.39} & \textbf{34.46} & \textbf{26.30} & \textbf{19.81}\\
EdgeConv~\cite{wang2019dynamic} &97.43 & 17.30 & 5.97 & 0.78\\
GAT layer~\cite{brody2021attentive} &96.44 & 4.58 & 2.55 & 1.59\\
SAGE layer~\cite{hamilton2017inductive} &93.53 & 0.77 & 0.11 & 0.03\\
GCN layer~\cite{kipf2016semi} &89.31 & 0.39 & 0.22 & 0.16\\
\bottomrule
\end{tabular}
\end{table}
Our proposed GAV layer outperforms the other message-passing layers across all metrics by a considerable amount.\\

Lastly, we vary the number of hops $h$ used to generate $\mathcal{G}^t_h$ and the number of message-passing iterations $k$ (see Table~\ref{tab:ablations_kh}).
\begin{table}[h]
\centering
\scriptsize
\caption{Ablations on $k$ and $h$.}
\label{tab:ablations_kh}
\begin{tabular}{c |c c c c} 
\toprule
$k$ \& $h$ & $\text{AUC}\uparrow$ &$\text{Hits@100}\uparrow$& $\text{Hits@50}\uparrow$& $\text{Hits@20}\uparrow$\\
\midrule
1 & \textbf{98.39} & \textbf{34.46} & 26.30 & 19.81\\
2 & \textbf{98.39} & 34.00 & \textbf{26.93} & \textbf{21.21}\\
3 & \textbf{98.39} & 34.25 & 25.99 & 17.84\\
\bottomrule
\end{tabular}
\end{table}
We observe that simultaneously increasing $k$ and $h$ results in no discernible differences in performance. This finding is in line with the $\gamma$-decaying theory~\cite{zhang2018link}, proving the approximability of high-order heuristics from locally restricted subgraphs.


\begin{figure*}[h]
\centerline{\includegraphics[width=\linewidth]{imgs/inter2.pdf}}
\caption{
Visualization of the effect of our GAV layer on vector embeddings. We visualize subgraph representations $\mathcal{G}^t_h$ ($h$ set to one) of four positive (top row) and four negative target links (bottom row) together with the GAV layer's predicted scalar values $s_{i} \in (-1, 1)$. The scalar values $s_{i}$ used to update vector embeddings in $\mathcal{L}(\mathcal{G}^t_h)$ have been projected to $\mathcal{G}^t_h$ to provide an interpretable visualization. The directionality of edges already incorporates potential shifts in direction enforced by our GAV layer. Please note that following the color coding scheme of Fig.~\ref{fig:method_overview}, the target link $e^{t}_{ij}$ is depicted in orange, whereas the two target nodes $n_{i}^{t}$ and $n_{j}^{t}$ are displayed in red and green. We additionally report the angle $\angle$ between the vector embeddings aggregated around the two target nodes (see Section~\ref{ref:readout}) and the predicted probability of link existence $\hat{y}^t_{ij}$.
}
\label{fig:inter}
\end{figure*}

\subsection{Interpretability and Analysis of Results}
% vector embeddings represent edges in G.
In Fig.~\ref{fig:inter}, we visualize the behavior of our proposed GAV layer to facilitate interpretability. First, we investigate the correlation between the vector embeddings aggregated around the two target nodes represented by $\text{mean}(\mathcal{E}_{\mathcal{N}(n^t_{i})})$ and $\text{mean}(\mathcal{E}_{\mathcal{N}(n^t_{j})})$, which are constructed in the readout module (see Section~\ref{ref:readout}). We find that the angle between these aggregated vector embeddings is highly correlated to the predicted probability of link existence $\hat{y}^t_{ij}$ and hence provides decisive information. We observe a trend of high angles being associated with positive and low angles with negative predictions. 
In the context of flow, this observation draws parallels to the concept of sink and source flow.
% , which present theoretical principles not applicable in real life conditions.
To be precise, GAV may attempt to assign the two target nodes to sink and source nodes for negative predictions (see Fig.~\ref{fig:inter}, second row), which stands in contrast to the behavior of physical flow in spatial networks.

The GAV layer's predicted scalar values $s_{i} \in (-1, 1)$ can not only flip but also scale vector embeddings.
% We tried to interpret the switch in direction above. 
Based on Fig.~\ref{fig:inter}, we identify $|s_{i}|$ as a measure of uncertainty.
This is because with decreasing certainty of $\hat{y}^t_{ij}$, we observe a decrease in $|s_{i}|$ (see Fig.~\ref{fig:inter}, left to right).

% We observe a trend of decreasing $|s_{i}|$ with decreasing certainty of $\hat{y}^t_{ij}$ (see Fig.~\ref{fig:inter}, left to right).

% One property of GAV-layer's is to scale and potentially flip vector embeddings via $s_{i} \in (-1, 1)$. This can flip vector embeddings, resulting in a change of direction, which we interpret above, but it can also scale the vector embeddings. In Fig.~\ref{fig:inter}, we observe a trend, where $|s_{i}|$ decreases with increasing $\hat{y}^t_{ij}$ indicating that $|s_{i}|$ may be interpreted as an indicator of uncertainty (see Fig.~\ref{fig:inter} left to right). This is because $|s_{i}|$ decreases with the certainty of $\hat{y}^t_{ij}$.



% \subsection{Convergence}    % optional
% % show 2 curves; we converge faster

% \subsection{Generalizability}   % optional
% % ckpts at different datasets