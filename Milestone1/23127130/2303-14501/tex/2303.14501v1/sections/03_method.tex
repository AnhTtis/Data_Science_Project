\begin{figure*}[h]
\centerline{\includegraphics[width=\linewidth]{imgs/method.pdf}}
\caption{
Overview of the GAV link prediction framework. GAV is divided into three modules, namely the subgraph extraction module, the message-passing module, and the readout module.
First, an $h$-hop enclosing subgraph $\mathcal{G}^{t}_{h}$ is extracted around the target nodes $\{ n_{i}^{t}, n_{j}^{t} \}$ (red and green) affiliated to the target link $e^{t}_{ij}$ (orange) and subsequently transformed into a line graph representation $\mathcal{L}(\mathcal{G}^{t}_{h})$; second, we perform iterative message-passing between vector embeddings in $\mathcal{L}(\mathcal{G}^{t}_{h})$ via $k$ GAV layers to incorporate contextual information; and, third, a final subgraph-level readout module aggregates relevant features and predicts the probability of link existence with regard to the target link $e^{t}_{ij}$.
To provide a concise visualization, $h$ was set to 1. We would like to draw the reader's attention to color coding and the vector embeddings in the nodes $n'_{i}$ of $\mathcal{L}(\mathcal{G}^{t}_{h})$.
}
\label{fig:method_overview}
\end{figure*}

Our proposed Graph Attentive Vectors (GAV) link prediction framework is depicted in Fig.~\ref{fig:method_overview}. GAV represents a simple yet effective, end-to-end trainable framework tailored to the task of link prediction for flow-driven spatial networks. It predicts the probability of link (or edge) existence between two nodes in a graph $\mathcal{G}$ based on a binary classifier $\mathcal{F}$, composed of a message-passing and a readout module. To this end, $\mathcal{F}$ should be able to differentiate between positive (real) and negative (sampled) links by assigning high probabilities of existence to plausible and low probabilities of existence to implausible links. Following Zhang \etal~\cite{zhang2018link, zhang2021labeling}, we treat the link prediction problem as a subgraph classification task. To determine the probability of existence of an individual target link between two target nodes, we, therefore, first extract an enclosing subgraph describing the target links local neighborhood in a subgraph extraction module. Subsequently, the subgraph is transformed into a line graph representation and forwarded to $\mathcal{F}$, resulting in an iterative link prediction scheme predicting the existence of target links one at a time.

In the following sections, we elaborate extensively on GAV's individual components (see Fig.~\ref{fig:method_overview}) and the principal ideas forming its backbone.

\subsection{Subgraph Extraction Module}\label{ref:subnetw_extraction}
% The complete, ultra-large input graph $\mathcal{G}$ is defined by a set of nodes represented by $V \in \mathbb{R}^{n \times d_{\text{spatial}}}$, a set of corresponding edges represented by $E \in \mathbb{R}^{2 \times m}$, and the labels associated to edges represented by $Y \in \mathbb{R}^{m}$.
The undirected input graph $\mathcal{G}$ is defined by a set of nodes $\mathcal{V}$ and a set of corresponding edges $\mathcal{E}$. While nodes $n_{i} \in \mathcal{V}$ embed individual continuous spatial entities in the form of spatial positions given by coordinates ($n_{i} \in \mathbb{R}^{d_\text{spatial}}$), edges $e_{ij} \in \mathcal{E}$ describe the relations and thus the connectivity among nodes.

As a first step, we extract an $h$-hop enclosing subgraph $\mathcal{G}^{t}_{h}$ around the nodes $\{ n_{i}^{t}, n_{j}^{t} \}$ affiliated to the target link $e^{t}_{ij}$ from the original graph representation $\mathcal{G}$ (please note that we refer to the target in our notations as $t$).
%Therefore, $\mathcal{G}^{t}_{h}$ only incorporates nodes located in the $h$-hop neighborhood around the target nodes $\{ n_{i}^{t}, n_{j}^{t} \}$.
% , resulting in $\mathcal{V}^{h}_{\{ n_{t_{1}}, n_{t_{2}} \}} = \{ n_{i}\ |\  d(n_{i},  n_{t_{1}}) \le h\ \text{or}\ d(n_{i},  n_{t_{2}}) \le h \}$ and $\mathcal{E}^{h}_{\{ n_{t_{1}}, n_{t_{2}} \}}$.
This results in an expressive and efficient representation of the target link's local neighborhood, including the relevant structural patterns necessary to determine link existence. Further, the subgraph extraction results in a drastically reduced computational complexity, which is crucial for ultra-large graphs.

Since link prediction is naturally pertinent to links rather than nodes, we formulate link prediction as a problem on a line graph. To this end, we subsequently transform $\mathcal{G}^{t}_{h}$ into a line graph representation $\mathcal{L}(\mathcal{G}^{t}_{h})$. In the line graph, each node $n'_{i}$ represents an edge $e_{ij} \in \mathcal{E}^{t}_{h}$, while its edges $e'_{ij}$ indicate adjacency between edges $e_{ij}$ iff they are incident in $\mathcal{G}$. We encode edges $e_{ij}$ as vectors between the involved nodes $\{n_{i}, n_{j} \}$ to generate node embeddings for the line graph representation's nodes $n'_{i}$.
Therefore, $\mathcal{L}(\mathcal{G}^{t}_{h})$'s node embeddings are defined as vectors of unique length and direction, representing the network's structural properties via edges in $\mathcal{G}^{t}_{h}$ (see Fig.~\ref{fig:method_overview}). The line graph representation $\mathcal{L}(\mathcal{G}^{t}_{h})$ formed around the target link $e^{t}_{ij}$ is finally forwarded to the message-passing module.
% constructed from modified sets of nodes $\mathcal{V'}$ and edges $\mathcal{E'}$. In $\mathcal{L}(\mathcal{G})$ each node $n'_{ij} \in \mathcal{V'}$ represents an edge $e_{ij} \in \mathcal{E}$ in $\mathcal{G}$, while edges $e'_{ij} \in \mathcal{E'}$ indicate adjacency between edges $e_{ij} \in \mathcal{E}$ if and only if they are incident. Hence, the line-graph is formally defined as $\mathcal{L}(\mathcal{G}) := (\mathcal{V'}, \mathcal{E'})$, where $\mathcal{V'} = \mathcal{E}$ and $\mathcal{E}' = \{\{e_{ij}, e_{ik}\}\ \ \text{iff} \ \ \exists\ (e_{ij}, e_{ik}) \in \mathcal{E}\}$. $n_{c_{1}}$


\subsection{Message-Passing Module and GAV Layer}
To incorporate contextual information in node embeddings, we propose a novel message-passing layer, termed GAV layer. We perform $k$ iterations of message-passing among the nodes of the line graph $\mathcal{L}(\mathcal{G}^{t}_{h})$, obtained from our subgraph extraction module. The GAV layer's message-passing relies on a straightforward intuition inspired by principles of physical flow. To be precise, we treat nodes in $\mathcal{L}(\mathcal{G}^{t}_{h})$ as vector embeddings and update them in a constrained manner, imitating simplified dynamics of physical flow in spatial networks. The detailed structure of a single GAV layer is graphically visualized in Fig.~\ref{fig:mpn}.

\begin{figure}[h]
\centerline{\includegraphics[width=0.95\linewidth]{imgs/gav2.pdf}}
\caption{
Graphical visualization of a single GAV layer updating the vector embedding of node $n'_{j}$. We forward the vector embedding of node $n'_{j}$ together with $N_{j} \in \mathbb{R}^{| \mathcal{N}(n'_j) \cup {n'_j}| \times d_{\text{spatial}}}$, which represents the set of $n'_{j}$ and its neighbors $n'_{k}$ and $n'_{i}$, to the GAV layer. 
% In this exemplary case, $\hat{n}'_{2}$ represents a flipped version of $n'_{2}$.
Please note that the GAV layer's structure draws parallels to the Transformer's encoder~\cite{carion2020end}.
% Describe more. Therefore, the structure is draws parallels to the Transformer's encoder. , which draws parallels to the Transformer's encoder, 
}
\label{fig:mpn}
\end{figure}

In order to update an individual vector embedded in a node $n'_{i} \in \mathbb{R}^{d_{\text{spatial}}}$, we first project it together with a matrix $N_{i} \in \mathbb{R}^{| \mathcal{N}(n'_{i}) \cup n'_{i}| \times d_{\text{spatial}}}$, consisting of directly neighboring nodes and the node itself, 
%$\bigcup\limits_{j \in \mathcal{N}(i) \cup {i}} \{ n_{j} \}$
into a higher dimensional space $d_{\text{message}}$; the projection is through a learnable function $\phi^{(1)}_{\theta}:\mathbb{R}^{d_{\text{spatial}}} \rightarrow \mathbb{R}^{d_{\text{message}}}$. Subsequently, $\phi^{(1)}_{\theta}(n'_{i})$ and $\phi^{(1)}_{\theta}(N_{i})$ are forwarded to a multi-head attention operation, where $\phi^{(1)}_{\theta}(n'_{i})$ represents a single query, while $\phi^{(1)}_{\theta}(N_{i})$ represents the key and value sequence.
\begin{equation}
\tilde{n}'_{i} = \text{MultiHeadAttn}(\phi^{(1)}_{\theta}(n'_{i}),\ \phi^{(1)}_{\theta}(N_{i}),\ \phi^{(1)}_{\theta}(N_{i}))
\end{equation}
This results in an intermediate node representation $\tilde{n}'_{i} \in \mathbb{R}^{d_{\text{message}}}$, which incorporates not only information of the node itself but also its local structural neighborhood via the concept of attention. In the next step, we apply a residual connection for increased gradient flow and forward the result to the learnable function $\phi^{(2)}_{\theta}: \mathbb{R}^{d_{\text{message}}} \rightarrow \mathbb{R}$, followed by a tanh non-linearity.
\begin{equation}
s_{i} = \text{tanh}(\phi^{(2)}_{\theta}(\tilde{n}'_{i} + \phi^{(1)}_{\theta}(n'_{i})))
\end{equation}
Finally, the scalar value $s_{i} \in (-1, 1)$ is utilized to update the original node representation $n'_{i}$ via scalar multiplication.
\begin{equation}
\hat{n}'_{i} =  s_{i} \cdot n'_{i}
\end{equation}
Hence, after one layer of message-passing, the updated, refined node representation is given by $\hat{n}'_{i} \in \mathbb{R}^{d_{\text{spatial}}}$. Importantly, $\hat{n}'_{i}$ preserves relevant properties of the original node representation $n'_{i}$.
This is because scalar multiplication with $s_{i} \in (-1, 1)$ restricts the modification of vector embeddings given by nodes in $\mathcal{L}(\mathcal{G}^{t}_{h})$. 
In essence, our message-passing paradigm can be geometrically interpreted as a scaling combined with a potential flipping operation of vectors. These constraints imposed by our message-passing align with our principal idea of modeling simplified dynamics of physical flow in spatial networks via potential, constrained changes in the direction and magnitude of vector embeddings, preserving the network's structural properties.
% representative of physical flow.
% via $\mathcal{L}(\mathcal{G}^{t}_{h})$'s vector embeddings by allowing the original vector embeddings to be modified in a physically plausible manner.


\begin{table*}[h] %tp!]
\centering
\scriptsize
\caption{Properties of the raw datasets. Each dataset consists of exactly one ultra-large graph.}
\label{tab:datasets}
\begin{tabular}{l |l l c l c l} 
\toprule
 Dataset Name & $\#$ Nodes & $\#$ Edges & Node Degree & Node Features & Edge Features & Description\\
\midrule
ogbl-vessel~\cite{hu2020ogb} & 3,538,495 & 5,345,897 & 3.02 & $x$-, $y$-, $z$-coordinates & $-$ & BALB/c mouse $\text{strain}^{1}$\\
c57-tc-vessel~\cite{paetzold2021whole} & 3,820,133 & 5,614,677 & 2.94 & $x$-, $y$-, $z$-coordinates & $-$ & C57BL/6 mouse $\text{strain}^{1}$\\
cd1-tc-vessel~\cite{paetzold2021whole} & 3,645,963  & 5,791,309 & 3.18 & $x$-, $y$-, $z$-coordinates & $-$ & CD-1 mouse $\text{strain}^{1}$\\
c57-cc-vessel~\cite{walchli2021hierarchical} & 6,650,580 & 9,054,100 & 2.72 & $x$-, $y$-, $z$-coordinates & $-$ & C57BL/6 mouse $\text{strain}^{2}$\\
\midrule
belgium-road~\cite{bader201110th} & 1,441,295 & 1,549,970 & 2.15 & $x$-, $y$-coordinates & $-$ & Belgium\\
italy-road~\cite{bader201110th} & 6,686,493 & 7,013,978 & 2.10 & $x$-, $y$-coordinates & $-$ & Italy\\
netherlands-road~\cite{bader201110th} & 2,216,688 & 2,441,238 & 2.20& $x$-, $y$-coordinates & $-$ & Netherlands\\
luxembourg-road~\cite{bader201110th} & 114,599 & 119,666 & 2.09 & $x$-, $y$-coordinates & $-$ & Luxembourg\\
\bottomrule
\multicolumn{7}{l}{$^{1}$ tissue clearing (tc) and light-sheet microscopy imaging  $\qquad ^{2}$ corrosion casting (cc) and SR\textmu$\text{CT}$ imaging}
\end{tabular}
\end{table*}


\subsection{Labeling Trick}
Following Zhang \etal~\cite{zhang2018link, zhang2021labeling}, we apply a labeling trick to enable the message-passing module to learn an expressive structural representation of the target link's local neighborhood. Since our method operates on a line graph, we propose a novel labeling trick tailored to line graph-based link prediction tasks. 
Our labeling trick ensures that vector embeddings created from the target link $e_{ij}^{t}$ and edges connected to the target nodes $\{ n_{i}^{t}, n_{j}^{t} \}$ are identifiable by a distinct label. This allows the message-passing module to distinguish between the target link, edges connected to target nodes, and other edges encoded in $\mathcal{L}(\mathcal{G}^{t}_{h})$'s vector embeddings, simplifying the link prediction task. Labels generated by our proposed labeling trick are shown in Fig.~\ref{fig:labeling_trick}.

\begin{figure}[h]
\centerline{\includegraphics[width=0.5\linewidth]{imgs/labeling_trick3.pdf}}
\caption{
Labels generated by our labeling trick for a line graph representation ($h$ set to two).
% The target link, which is embedded as a vector representation in a node of the line-graph, is visualized in orange, whereas edges connected to the target nodes $n^t_{i}$ and  $n^t_{j}$ are visualized in purple and blue.
Our labeling trick assigns the label 0 to the vector embedding representing the target link (orange), the labels 1 and 2 to vector embeddings representing edges connected to the target nodes $n^t_{i}$ and $n^t_{j}$ (purple and blue), and the label 3 to remaining vector embeddings.
}
\label{fig:labeling_trick}
\end{figure}

The additional labels generated by our labeling trick are concatenated to $\mathcal{L}(\mathcal{G}^{t}_{h})$'s vector embeddings in the subgraph extraction module. 
% We empirically found that the additional node labels are beneficial to learn a more representative structural representation via the message-passing module, and also contain valuable information for the readout module. Hence, we forward the generated labels through the message-passing module to the readout module.


\subsection{Readout Module}\label{ref:readout}
The readout module consists of a custom node aggregation operation followed by a learnable function $\phi^{(3)}_{\theta}: \mathbb{R}^{2 \cdot d_{\text{spatial}}} \rightarrow \mathbb{R}$ and processes the refined vector embeddings obtained from the message-passing module. While the node aggregation operation aims to distill pertinent information from the refined graph representation in a fashion invariant to node ordering and quantity, $\phi^{(3)}_{\theta}$ predicts the probability of link existence with regard to the target link.

Equation~\ref{eq:agg} summarizes the readout modules functionality, where $\mathcal{E}_{\mathcal{N}(n^t_{i})}$ defines the set of refined vector embeddings originally created from edges adjacent to $n^t_{i}$ (see Fig.~\ref{fig:method_overview}), $\mathcal{E}_{\mathcal{N}(n^t_{j})}$ the set of refined vector embeddings originally created from edges adjacent to $n^t_{j}$, $\mathbin\Vert$ the concatenation operation, and $\hat{y}^t_{ij}$ the probability of existence of the target link.
\begin{equation} \label{eq:agg}
\hat{y}^t_{ij} =  \phi^{(3)}_{\theta}(\text{mean}(\mathcal{E}_{\mathcal{N}(n^t_{i})}) \mathbin\Vert \text{mean}(\mathcal{E}_{\mathcal{N}(n^t_{j})}))
\end{equation}
Thus, our node aggregation operation consolidates refined vector embeddings located in the vicinity of the target nodes $\{ n_{i}^{t}, n_{j}^{t} \}$ to define a simple yet effective aggregation scheme, as shown in Fig.~\ref{fig:method_overview}. Specifically, we intend to exploit the relationship between vector embeddings aggregated around the two target nodes to predict whether target nodes should be connected or not.

% makes use of the intuition of physical flow modeled by vectors embedded in the node representations of our line graph representation to define a simple yet effective aggregation scheme, as shown in Fig.~\ref{fig:method_overview}. Specifically, we intend to model flow at the target nodes $n_{t_{1}}$ and $n_{t_{2}}$ by aggregating adjacent node embeddings of the refined line graph representations.

\subsection{Loss Function}
% with a corresponding set of edge-labels $\mathcal{Y}$.
% Moreover, labels $y_{ij} \in \mathcal{Y}$ indicate whether the edges $e_{ij}$ are negative (implausible) or positive (plausible) links.
% together with the label $y^{t}_{ij}$
Since our approach represents a binary classifier $\mathcal{F}$ determining the probability of link existence with regard to the target link, we optimize a binary cross-entropy loss function during training. Here, $y^t_{ij} \in \{0, 1\}$ indicates whether the target links are negative (sampled) or positive (real).
\begin{equation}
\mathcal{L_{\text{BCE}}} =  \frac{-1}{| \mathcal{E} |} \sum_{ij \in \mathcal{E}} y^t_{ij} \cdot \text{log}(\hat{y}^t_{ij}) + (1 - {y}^t_{ij}) \cdot \text{log}(1 - \hat{y}^t_{ij})
\end{equation}

We would like to highlight that our approach is trained in an entirely end-to-end manner, solely based on the information of link existence. Therefore, intermediate representations, such as the refined vector embeddings, are determined completely data-driven.