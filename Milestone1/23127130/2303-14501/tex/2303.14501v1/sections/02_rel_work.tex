This section commences by discussing previous work on link prediction algorithms, followed by an overview of message-passing layers.
% , given the introduction of our novel GAV layer. 
Particular emphasis is placed on methods featured in our experiments.

\subsection{Link Prediction}
% Link prediction algorithms are typically applied to propose meaningful new links, missing links likely to appear in the future, and to refine graph representations.

Link prediction algorithms are applied in various fields, such as social network analysis~\cite{murata2007link, liben2003link, daud2020applications}, bioinformatics~\cite{ lei2013novel, stanfield2017drug, kang2022lr}, recommender systems~\cite{ai2019link, talasu2017link, huang2005link, zhang2010solving}, supply chain networks~\cite{lu2020discovering}, and information retrieval~\cite{lande2020link}.
Broadly speaking, different link prediction algorithms try to estimate link existence between two nodes either via heuristic or learned methods. 
We discuss these two families of algorithms in the following.
% Link prediction is the task of predicting missing links between the nodes of a graph. A link represents a relationship between two nodes. Hence, a link prediction method's performance correlates with its ability to learn such a relationship. Based on how different methods try to learn this relationship, we can broadly categorize them into heuristic and learned methods. We discuss the two families of methods in the following subsections.

\paragraph{Heuristic Algorithms}
Heuristic algorithms employ predefined heuristics to encode the similarity between two nodes. Some prominent candidates
% , which can be further categorized in first-order, second-order, and high-order heuristics, 
are represented by common neighbors, resource allocation~\cite{zhou2009predicting}, preferential attachment~\cite{barabasi1999emergence}, Adamic-Adar~\cite{adamic2003friends}, Jaccard~\cite{jaccard1901etude}, Katz~\cite{katz1953new}, and average commute time~\cite{fouss2007random}. However, all heuristic link prediction algorithms suffer from the same underlying issue. They exploit predefined heuristics, which can not be modified to account for different network types. \Eg, the common neighbors heuristic has been developed for social networks and hence yields underwhelming results when applied to molecular graphs.
% The Adamic Adar Coefficient \cite{liben2003link} connects two nodes if they have at least a common neighbor, with higher weight given to lower degree nodes. Jaccard coefficient \cite{liben2003link} works on a similar principle but weighs all nodes equally. One can see that these methods are limited since they will connect nodes only if they have common neighbors and are thus local. A more powerful global approach is to use the Katz index \cite{katz1953new}. The Katz index is computed by a weighted sum of paths of all lengths between two nodes. However, all these methods suffer from the same underlying issue. They use pre-defined heuristics, which can not be modified to account for different graph types, e.g., common neighbor heuristics work better for social networks but not for molecular graphs.

\paragraph{Learned Algorithms}
% https://aip.scitation.org/doi/full/10.1063/1.5107440
% https://journals.aps.org/pre/abstract/10.1103/PhysRevE.105.024311
On the other hand, learned algorithms do not rely on fixed, predesigned heuristics but rather learn a data-driven heuristic suitable to the properties of individual graphs utilizing neural networks. Thus, learned algorithms can easily adapt to different network types.
SEAL~\cite{zhang2018link, zhang2021labeling} represents a prominent, learned link prediction framework, defining link prediction as a subgraph-level classification task by training a binary GNN-based classifier to map from subgraph patterns to link existence. To this end, SEAL first extracts a local subgraph around the link of interest, which is subsequently forwarded to DGCNN~\cite{zhang2018end} for classification. DGCNN relies on GCN message-passing layers~\cite{kipf2016semi} followed by a SortPooling layer for subgraph aggregation. Moreover, SEAL incorporates an additional node labeling technique, known as labeling trick, to enhance the expressiveness of node features obtained from GNNs. SIEG~\cite{hu2020ogb} builds upon SEAL and introduces, inspired by Graphormer~\cite{ying2021transformers}, a pairwise structural attention module between two nodes of interest to capture local structural information more effectively. 
% NGNN~\cite{ngnn} attempts to increase SEAL's modeling ability by inserting feedforward neural networks into its GCN layers. 
% NGNN's authors demonstrate increased link prediction performances. 
Puny \etal~\cite{puny2020global} tried to improve the generalization power of multiple message-passing layers via a low-rank global self-attention module, short LRGA, and combined their approach with a simple link prediction framework. We would like to mention that none of the above-mentioned methods are tailored to flow-driven spatial networks.

% learn node similarity from the graph structure itself. Thus, they are data-driven and can adapt to different graph properties. Furthermore, a local neighborhood ( 1 or 2 hops) is sufficient to learn the node similarity. SEAL also uses a labeling trick to distinguish nodes placed at different distances in the local neighborhood. Such distance-based labeling can distinguish nodes with similar neighborhoods (and thus can not be distinguished using the 1-WL test). Hence, the labeling trick is indispensable for high performance on graphs with high symmetry, such as ours. Hence, given its advantages, we propose a learned method with an implicit labeling trick for the link prediction task.

\subsection{Message-Passing Layers}
GNNs utilize the concept of message-passing to encode semantically rich features within network-structured data. Over time, multiple variations of message-passing layers have been proposed~\cite{xu2018powerful, defferrard2016convolutional, fey2018splinecnn, he2020lightgcn}. For instance, GCN's message-passing layer~\cite{kipf2016semi} weighs each incoming message with a fixed coefficient, the node degree, before aggregation. In contrast, GAT's message-passing layer~\cite{brody2021attentive} learns aggregation weights dynamically based on attention scores. GraphSAGE's message-passing layer~\cite{hamilton2017inductive} does not directly aggregate central node features with incoming messages. Instead, it distinguishes these two kinds of features and learns two different transformations, one on the central node and another on incoming messages. EdgeConv~\cite{wang2019dynamic} aggregates the feature difference between the central node and its neighbors combined with the central node's features. Thus, EdgeConv draws parallels to aggregating spatial vectors if the nodes embed spatial positions.
%Even though EdgeConv, has draws parallels to our proposed GAV-layer, 
However, our proposed GAV layer differs significantly from EdgeConv, as our method explicitly constrains the update of vector embeddings to imitate the simplified dynamics of physical flow in spatial networks. Importantly, only a few works tried to adapt the message-passing paradigm to spatial networks~\cite{zhang2021representation, danel2020spatial}.



% The graph convolution operation is the generalization of convolution operation on graphs (a non-Euclidean structure). For this, GNNs resort to the message-passing paradigm (MP). A MP layer consists of three steps, (i) A message step: computing the information a node wants to send to its neighbors (ii) an aggregate step: how each node aggregates the message from its neighbors and (iii) an update step: each node updates its features as a function of its current features and the aggregated message. Please note that a MP step operates on the directly connected neighbors (the 1-hop neighborhood). Stacking k such MP layers can exchange information from the k-hop neighborhood of a node. 
% Variations of the message, aggregation, and update steps lead to different realizations of graph convolution operators. For instance, GCN \cite{kipf2016semi} weighs each incoming message with a fixed coefficient, the node degree, before aggregation. GAT \cite{brody2021attentive}, on the other hand, learns these aggregation weights dynamically based on an attention score. GraphSAGE \cite{hamilton2017inductive} does not directly aggregate the central node features with the incoming message. Instead, it distinguishes the two kinds of features and learns two different transformations, one on the central node and another on the \emph{mean} of the incoming message. A slightly different formulation is used by EdgeConv \cite{wang2019dynamic}. It aggregates the feature \emph{difference} between the central node and the neighbors. Thus, if the nodes have spatial information, the EdgeConv operation is equivalent to aggregating vectors. Such an aggregation would make EdgeConv closely resemble our proposed solution. However, it should be noted that we explicitly learn a flip and scale coefficient for each message during the aggregation step. The proposed modification is crucial to model the flow and is impossible using the EdgeConv layer.
% spatial networks https://www.sciencedirect.com/science/article/pii/S037015731000308X
% https://proceedings.neurips.cc/paper/2021/file/12e35d9186dd72fe62fd039385890b9c-Paper.pdf
% https://arxiv.org/pdf/1909.05310.pdf

% also talk about spetial spatial networks inspired approaches...

