
\section{Proposed Autotuning Frameworks in Performance or Energy at Large Scales}
In this section we extend the ytopt autotuning framework to autotune the hybrid MPI/OpenMP applications at large scales on the ANL Theta and ORNL Summit using the metrics such as performance, energy, and EDP, where the application runtime is the primary performance metric; energy consumption captures the tradeoff between the application runtime and power consumption; and EDP captures the tradeoff between the application runtime and energy consumption. 
%The aprun command is used to specify to the Cray ALPS (Application Level Placement Scheduler) the resources and placement parameters needed for the application at application launch on Theta \cite{THETA}. The jsrun command is used to manage an allocation that is provided by an external resource manager within the IBM Job Step Manager (JSM) software package on Summit \cite{SUMMIT}. At a high level, aprun or jsrun is similar to mpiexec or mpirun or srun.

\subsection{Framework for Autotuning Performance at Large Scales}

Figure \ref{fig:pf} presents the framework for autotuning various hybrid MPI/OpenMP applications in performance. The application runtime is the primary metric. We analyze an application code to identify the important tunable application and system parameters (OpenMP environment variables) to define the parameter space using ConfigSpace \cite{CFS} package.
We use the tunable parameters to parameterize an application code as a code mold. 
ytopt starts with the user-defined parameter space, the code mold, and user-defined interface that specifies how to evaluate the code mold with a particular parameter configuration. 

The search method within ytopt uses Bayesian optimization, where a dynamically updated Random Forest surrogate model that learns the relationship between the configurations and the performance metric, is used to balance exploration and exploitation of the search space. In the exploration phase, the search evaluates parameter configurations that improve the quality of the surrogate model, and in the exploitation phase, the search evaluates parameter configurations that are closer to the previously found high-performing parameter configurations. The balance is achieved through the use of the lower confidence bound (LCB) acquisition function that uses the surrogate models' predicted values of the unevaluated parameter configurations and the corresponding uncertainty values. \xingfu{The LCB acquisition function is defined in Equation \ref{eqn:lcb}. For the unevaluated parameter configuration $x_M^i$, the trained model $M$ is used to predict a point estimate (mean value) $\mu(x_M^i)$ and standard deviation $\sigma(x_M^i)$.
\begin{equation}
    a_{LCB}(x_M^i) = \mu(x_M^i) - \kappa\sigma(x_M^i)
    \label{eqn:lcb}
\end{equation}
where $\kappa \geq 0$ is a user-defined parameter that controls the tradeoff between exploration and exploitation. When $\kappa=0$ for pure exploitation, a configuration with the lowest mean value is selected. When $\kappa$ is set to a large value ($>1.96$) for pure exploration, a configuration with large predictive variance is selected. The default value $\kappa$ of is 1.96. Then the model $M$ is updated with this selected configuration.
%After the evaluation of the selected unseen configuration, it is used to update the model. Evaluation of such configurations results in improvement of the model $M$. 
}

\xingfu{The iterative phase of the proposed autotuning framework in performance has the following steps: 
\begin{itemize}
\item [Step1]  Bayesian optimization selects a parameter configuration for evaluation. 
\item [Step2] The code mold is configured with the selected configuration to generate a new code. 
\item [Step3]  Based on the value of the number of threads in the  configuration, the number of nodes reserved and the number of MPI ranks, aprun/jsrun command line for launching the application on the compute nodes is generated. 
\item [Step4] The new code is compiled with other codes needed to generate an executable. 
\item [Step5] The generated aprun/jsrun command line is executed to evaluate the application with the selected parameter configuration; the resulting application runtime is sent back to the search and recorded in the performance database. 
\end{itemize}

Steps 1--5 are repeated until the maximum number of code evaluations or the wall-clock time is exhausted for the autotuning run. }

In the rest of this paper,  the term \textbf{ytopt processing time} includes the time spent in the parameter space search, building the surrogate model, processing the selected configuration to generate a new code and the aprun/jsrun command line, compiling the new code, launching the application, and storing the configuration and performance in the performance database (except the application runtime). We use the term \textbf{ytopt overhead} to stand for the ytopt processing time minus the application compiling time. 

\begin{figure}[ht]
\center
 \includegraphics[width=.5\textwidth]{figs/framework-perf.png}
 \setlength{\belowcaptionskip}{-8pt}
 \caption{Framework for Autotuning Performance}
\label{fig:pf}
\end{figure}

\xingfu{ytopt supports various application-tunable parameters, which impact the application performance but keep the program correctness.} The application-tunable parameters can be defined as variables, pragmas, pragma clauses, a statement, or a function (piece of code).  \xingfu{The combinations of these parameters with their ranges of values form a parameter space. This requires some knowledge about the applications and underlying systems.} For instance, \xingfu{we define "\#pragma omp parallel for" as a parameter before a loop to check how the application performance is affected with and without it.} 
%We  define the statement "MPI\_Barrier(MPI\_COMM\_WORLD);" as a parameter from a code still with its correctness to check how the application performance is affected with and without it. 
Table \ref{tab:sz} presents the parameter space for the four ECP proxy applications used in our experiments, where \textbf{system param.} stands for system parameters; \textbf{application param.} stands for unique application parameters because some of them are used repeatedly in the application code; and \textbf{space size} stands for the number of configurations for the parameter space. The system parameters in this paper mainly focus on OpenMP runtime environment variables \cite{OMP}:  OMP\_NUM\_THREADS, OMP\_PLACES, OMP\_PROC\_BIND, OMP\_SCHEDULE, and the additional OMP\_TARGET\_OFFLOAD. 

\xingfu{The selected application parameters which may impact performance for each application are described as follows.}
The two unique application parameters for XSBench are block size and additional "\#pragma omp parallel for." The five 
unique application parameters for XSBench-mixed (mixed Clang loop pragmas and OpenMP pragmas) are block size, Clang loop unrolling full, "\#pragma omp parallel for," and two tile sizes for 2D loop tiling. The  four unique application parameters for XSBench-offload are simd, device clause,  schedule for the OpenMP target pragmas, and "\#pragma omp parallel for." The one unique application parameter for SWFFT is "MPI\_Barrier(CartComm);". The  three unique application parameters for AMG are "\#pragma unroll(3)," "\#pragma unroll(6)," and "\#pragma omp parallel for." The four unique application parameters for SW4lite are ``\#pragma unroll (6)," "\#pragma omp parallel for," "\#pragma omp for nowait," and "MPI\_Barrier(MPI\_COMM\_WORLD);". Overall, we use the parameter spaces with up to 6,272,640 configurations for our experiments.  

\begin{table}[ht]
\center
\caption{Parameter Space for Each Application}
\begin{tabular}{|r|c|c|c|c|}
\hline
ECP Proxy Apps&  System param. &  Application param.  & Space size  \\
\hline
XSBench &  4 env. variables & 2  & 51,840    \\
\hline
XSBench-mixed &  4  env. variables &  5  & 6,272,640    \\
\hline
XSBench-offload &  5  env. variables & 4 & 181,440    \\
\hline
SWFFT & 4  env. variables  & 1  & 1,080  \\
\hline
AMG  & 4  env. variables  & 3  &   552,960 \\
\hline
SW4lite & 4 env. variables  & 4  &  2,211,840 \\
\hline
\end{tabular} 
\setlength{\belowcaptionskip}{-8pt}
\label{tab:sz}
\end{table}


\subsection{Framework for Autotuning Energy and EDP at Large Scales}
Efficiently utilizing the procured power and optimizing the  performance of scientific applications under power and energy constraints are important challenges in HPC. The HPC PowerStack \cite{4,osti_powerstack, BB20}---a global consortium of laboratories, vendors, and universities---has highlighted a design shift toward standardization of the HPC power-management software stack. This enables seamless integration of software solutions for managing the energy/power consumption of large-scale HPC systems. 
Based on the state of the art of the components available in the community for power and energy management \cite{GL16, GR19, 3, redfish, epajsrm, 5, GEOPM, RAPL}, a hierarchical strawman PowerStack design \cite{osti_powerstack} was proposed to manage power and energy at three levels of granularity:  system level,  job level, and  node level. This implies the need to put in place the following incrementally: 
(1) define policies that govern site-level requirements, a power-aware system Resource Manager (RM) / job scheduler, a power-aware job-level manager, and a power-aware node manager;
(2) define the interfaces between these layers to translate objectives at each layer into actionable items at the adjacent lower layer; 
and (3) drive end-to-end optimizations across different layers of the PowerStack.

In order to address these requirements, our recent  work  \cite{WM20} (a) surveyed the high-level objectives of the existing layer-specific tuning approaches at the different layers: system (i.e., cluster), job / application, and node, (b) defined the tunable parameters at each layer, and (c) proposed and discussed how to autotune the combination of different parameters at the distinct layers (parameter space) for an optimal solution (the smallest runtime or the lowest energy) under a system power cap. 

%\textbf{Research Question: } 
%How can we explore a holistic performance, power, and energy management software stack that can optimize the target 
%power- or energy-efficiency
%application-aware metric so that it can trade off power, energy, and time to solution in order to optimize the energy efficiency of an HPC system?
%GAIL - you said the next in Section II - althoug there you said you used the framework to identify optimal combination, not to optimize it
%Recent work \cite{WK21, WK20} developed an autotuning framework that leverages Bayesian optimization with four supervised machine learning methods to explore the parameter space search, and we used the autotuning framework to optimize the loop pragma parameters to improve the application performance. 
Based on our previous work on autotuning the performance, power, and energy of applications and systems \cite{WK21, WM21, WM20, WK20}, we propose a high-level end-to-end PowerStack autotuning framework for HPC systems shown in Figure \ref{fig:ps}. This diagram shows the interactions among four layers: system  level, job level, node level, and application level. 
To the best of our knowledge, however, a practical end-to-end autotuning component is still lacking that targets all four layers for the optimal solution. 
%Specifically, for DOE HPC platforms and simulations, each system node consists  not only of CPUs and GPUs but also of FPGAs and AI accelerators; and each simulation involves not only computation and communication but also surrogate model training, evaluation, and prediction. Thus, achieving an optimal system-level target metric while satisfying component-specific goals becomes challenging.

\begin{figure}[ht]
\center
 \includegraphics[width=.3\textwidth]{figs/framework0.png}
 %\setlength{\abovecaptionskip}{-8pt}
 \setlength{\belowcaptionskip}{-8pt}
 \caption{PowerStack End-to-End Autotuning Framework}
\label{fig:ps}
\end{figure}  

Our aim is to develop a practical framework to autotune all four layers of the PowerStack so that we can have a better understanding of the tunable parameters at each layer and interaction interfaces between layers and can identify potential new requirements in order to achieve energy efficiency. The process of autotuning in the layers (a) typically targets energy as the primary metric, (b) complies with the operating power constraint imposed on the layer, and (c) attempts to improve the management and orchestration of the available control parameters that affect the application and/or hardware performance. %The goal of this tuning is to enable an HPC system under power constraints to leverage feedback-driven interoperability between system resource managers, runtime systems, and applications to maximize system performance and energy efficiency. 
For the proposed framework, we integrate the existing job constraint-aware power/energy optimizer GEOPM (Global Extensible Open Power Manager)  \cite{ES17, GEOPM} at the job and node levels and the ytopt autotuning framework at the application level.

\begin{figure}[ht]
\center
 \includegraphics[width=.4\textwidth]{figs/geopm.jpg}
 \setlength{\belowcaptionskip}{-8pt}
 \caption{High-Level Overview of GEOPM \cite{BB20}}
\label{fig:gpm}
\end{figure}

GEOPM \cite{ES17, GEOPM} is a community-driven, cross-platform, open source, job-level power management framework.
It provides multiple interfaces to enable interoperability with external HPC software components such as enabling job schedulers and resource managers to drive job-aware system-wide power efficiency improvements in Figure \ref{fig:gpm}. %GEOPM is a part of the OpenHPC consortium and provides monitoring capabilities as well as control agents to optimize for time-to-solution by leveraging techniques from learning and control systems.
GEOPM enables control and monitoring of hardware/software knobs across multiple platforms and architectures such as leveraging multiple power and performance knobs like Intel's hardware power-limiting capability (RAPL \cite{RAPL_SDM}) for achieved CPU frequency and instructions retired.
Because the latest version of GEOPM (1.x) is installed on Theta but is not available on Summit \xingfu{because of special privilege requirement to access the low-level msr (model specific registers) counters} and the power measurement of Power9 is not available to the public on Summit, Figure \ref{fig:en} shows the proposed framework for autotuning energy and EDP of various hybrid MPI/OpenMP applications on Theta. The average node energy consumed by the application is the primary metric. 
%For the sake of simplicity, we do not consider the power-aware resource manager in this framework because of the use of the production system Theta.

\begin{figure}[ht]
\center
 \includegraphics[width=.5\textwidth]{figs/framework-energy.png}
 \setlength{\belowcaptionskip}{-12pt}
 \caption{Framework for Autotuning Energy}
\label{fig:en}
\end{figure}

This energy autotuning framework is similar to the performance framework with the five steps in Figure \ref{fig:pf}. Steps 1 and 2 are the same. There are some differences in Steps 3, 4, and 5. The GEOPM job launch script, geopmlaunch \cite{GEOPM}, queries and uses the OMP\_NUM\_THREADS environment variable to choose affinity masks for each process.  %The OMP\_NUM\_THREADS environment variable must be set before geopmlaunch is executed. 
The principal job of geopmlaunch to aprun is to set explicit per-process CPU affinity masks that will optimize performance while enabling the GEOPM controller thread to run on a core isolated from the cores used by the primary application. The geopmlaunch enables the GEOPM library to interpose on
MPI using the PMPI interface through the LD\_PRELOAD mechanism for unmodified binaries.  %In Step 3, ytopt sets the OMP\_NUM\_THREADS environment variable, and generates the aprun command line for application launch. In Step 4, the dynamic linking is required with the -dynamic flag for the compiling. In Step 5, ytopt uses the geopmlaunch to launch the aprun command line with the options "--geopm-ctl=pthread," which launches the controller as an extra pthread per node, and "--geopm-report=gm.report," which creates the summary report file gm.report about performance, power, and energy for each node to evaluate the application with the configuration. ytopt processes the summary report file from GEOPM to record the average node energy or EDP in the performance database. Steps 1--5 are repeated until the maximum number of code evaluations or the wall-clock time for the run on Theta.

The iterative phase of the proposed autotuning framework in energy has the following steps:
\begin{itemize}
\item [Steps] Steps 1 and 2 are the same as shown in Figure \ref{fig:pf}.
\item [Step3] ytopt sets the OMP\_NUM\_THREADS environment variable, and generates the aprun command line for application launch. 
\item [Step4] The dynamic linking is required with the -dynamic flag for the compiling. 
\item [Step5] ytopt uses the geopmlaunch to launch the aprun command line with the options "-{}-geopm-ctl=pthread," which launches the controller as an extra pthread per node, and "-{}-geopm-report=gm.report," which creates the summary report file gm.report about performance, power, and energy for each node to evaluate the application with the configuration. ytopt processes the summary report file from GEOPM to record the average node energy in the performance database. 
\end{itemize}

Steps 1--5 are repeated until the maximum number of code evaluations or the wall-clock time for the run on Theta.
\if 0
Because Summit does not provide user-level power measurement for Power9, our framework in Figure \ref{fig:en} uses NVIDIA System Management Interface (nvidia-smi) \cite{NSMI} to replace GEOPM to measure power consumption for each GPU and averages them as the GPU power, then calculates the GPU energy as the product of the average GPU power and the runtime. There are some differences for Steps 3, 4 and 5 in Figure \ref{fig:en}. In Step3, ytopt sets the OMP\_NUM\_THREADS environment variable, and generates the jsrun command line for application launch with the nvida-smi command line before the program execution and killing the nvida-smi process after the program execution for XSBench using 6 GPUs per node with 1 MPI rank per GPU on 1024 nodes as follows: }
{\scriptsize
\begin{verbatim}
OMP_NUM_THREADS=n
jsrun -n1024 -a6 -g6 -c42 -bpacked:n/4 -dpacked launch.sh application

The bash script launch.sh is:
#!/bin/bash
let k=${OMPI_COMM_WORLD_RANK}%6
nvidia-smi dmon -i $k -s p -d 1 -f power${OMPI_COMM_WORLD_RANK}.txt&
dmpid=$!
$1 -m event 
kill $dmpid 
\end{verbatim}
  
When we choose the number of threads n, we make sure that n/4 is an integer because of the SMT level of 4 as default on Summit. We set one MPI rank per GPU and 42 cores per node for threads. The executable "application" is passed to the bash script $launch.sh$. This script calculates each GPU logical label $k$ inside a node based on a MPI rank (OMPI\_COMM\_WORLD\_RANK) and passes it to the command nvdia-smi to generate the power profile file for the GPU in the background, then get the nvdia-smi process id (dmpid). After the program execution, it kills the process to stop the power profiling. Notice that using nvdia-smi may impact the application performance because of the additional processes on each node. 

In Step4, the generated code is compile without the -dynamic flag. In Step5, ytopt launches the jsrun command line to result in two output files: performance data file and power data file per GPU, then it processes the two files to calculate the GPU energy to record it in the performance database. Steps 1--5 are repeated until the maximum number of code evaluations or the wall-clock time for the run on Summit. 
\fi
%Overall, this autotuning framework is flexible, as long as a power measurement tool is used to replace the component GEOPM in Figure \ref{fig:en} to measure the energy, the framework can be used to autotune energy.


