
\section{Introduction}

As we enter the exascale computing era, high performance, power, and energy management are key design points and constraints for any next generation of large-scale high-performance computing (HPC) systems~\cite{BB20, osti_powerstack, WM20}. Efficiently utilizing procured power and optimizing the performance of scientific applications under power and energy constraints are challenging for several reasons, including dynamic phase behavior, manufacturing variation, and increasing system-level heterogeneity. As the complexity of such HPC ecosystems (hardware stack, software stack, applications) continues to rise, achieving optimal performance and energy becomes a challenge. The number of tunable parameters that HPC users can configure at the system and application levels has increased significantly, resulting in a dramatically increased parameter space. Exhaustively evaluating all parameter combinations becomes very time-consuming. 
Therefore, autotuning for automatic exploration of the parameter space is desirable.

Autotuning is an approach that explores a search space of tunable parameter configurations of an application efficiently executed on an HPC system. Typically, one selects and evaluates a subset of the configurations on the target system and/or uses analytical models to identify the best implementation or configuration for performance or energy within a given computational budget. 
However, such methods are becoming too difficult in practice because of the hardware, software, and the application complexity. Recently, the use of advanced search methods that adopt mathematical optimization methods to explore the search space in an intelligent way has received significant attention in the autotuning community. Such a strategy, however, requires search methods to efficiently navigate the large parameter search space of possible configurations in order to avoid a large number of expensive application runs
%, while retaining just enough information 
to determine high-performance configurations or implementations. 
In this paper we propose a low-overhead machine learning (ML)-based autotuning framework to autotune four hybrid MPI/OpenMP Exascale Computing Project (ECP) proxy applications \cite{ECP}---XSBench \cite{XSB}, SWFFT \cite{SWF},  AMG \cite{AMG}, and SW4lite \cite{SW4L}---to improve their performance, energy, and energy delay product (EDP) on two large-scale HPC systems: Theta \cite{THETA} at Argonne National Laboratory (ANL), and Summit \cite{SUMMIT} at Oak Ridge National Laboratory (ORNL).
 
Traditional autotuning methods are built on heuristics that derive from automatically tuned BLAS libraries \cite{ATLAS}, experience \cite{TC02, CH06, GO10}, and model-based methods \cite{Chen05, TH11, BG13, FE17}. At the compiler level \cite{AK18}, ML-based methods are used for automatic tuning of the iterative compilation process \cite{OP17} and tuning of compiler-generated code \cite{TC09, MS14}.
Autotuning OpenMP codes has gone beyond loop schedules to look at parallel tasks and function inlining \cite{SJ19, KC14, MA11, Rose09}. Recent work on leveraging Bayesian optimization to explore the parameter space search shows the potential for autotuning on CPU systems \cite{WK20, WK21, LS21, RT21} and on GPU systems \cite{MS18, WN21}. Some recent work has used machine learning and sophisticated statistical learning methods to reduce the overhead of autotuning \cite{RB16, MA17, TN18, BQ19}. 
Most of these autotuning frameworks, however, are for autotuning on only a single or a few compute nodes using only performance as a metric. 

%For example,  GPTune \cite{LS21} has autotuned  MPI applications on up to 64 nodes with 2,048 cores with multitask learning using MPI, and Bayesian optimization was applied to increase the energy efficiency of a GPU cluster system \cite{MS18}. Our work is the first to autotune  hybrid MPI/OpenMP applications at large scales (up to 4,096 nodes with 262,144 cores), improving  performance and energy efficiency with low overhead and good scalability. \PB{commenting this because it was said in the related work}

This paper makes the following contributions.
 \begin{itemize}
 \item We propose a low-overhead autotuning framework ytopt to autotune various hybrid MPI/OpenMP applications at large scales.
 \item We use this ytopt framework to explore the tradeoffs between application runtime and power/energy for energy efficient application execution.
\item We use this framework to autotune four ECP proxy applications, namely XSBench, AMG, SWFFT, and SW4lite, using Bayesian optimization with a Random Forest surrogate model to effectively search parameter spaces with up to 6 million different configurations.
\item We demonstrate the effectiveness of our autotuning framework to tune the performance, energy, and EDP of these hybrid MPI/OpenMP applications on up to 4,096 nodes.
\item The experimental results show that our proposed autotuning framework at large scales has low overhead and good scalability, providing the best configuration for the best performance, energy saving, or EDP. Using the proposed autotuning framework to identify the best configurations, we achieve up to 91.59\% performance improvement, up to 21.2\% energy savings, and up to 37.84\% EDP improvement on up to 4,096 nodes.
\end{itemize}

The remainder of this paper is organized as follows. Section 2 discusses the background, challenges, and motivation of this study. Section 3 describes the systems and four ECP proxy applications used in this paper.  Section 4 proposes our autotuning frameworks for improving performance and energy at large scales. Section 5 discusses autotuning mixed pragmas on a single node. Section 6 presents autotuning performance at large scales. Section 7 illustrates autotuning energy and EDP at large scales. Section 8 summarizes this paper. 


