
\section{Autotuning Mixed Pragmas on a Single Node}

In this section we apply the proposed framework in Figure \ref{fig:pf} to autotune XSBench with mixed pragmas on a single node of Theta and Summit. We use its OpenMP version on Theta and its OpenMP offload version on Summit.

\subsection{On Theta}

We modify the OpenMP version of XSBench by adding more OpenMP pragmas and Clang loop optimization pragmas, such as loop unrolling and tiling \cite{KF19}. % We use ytopt to autotune the mixed OpenMP loop pragmas and Clang loop pragmas version to achieve the best performance. Basically, 
We  integrate the new OpenMP pragmas with Clang loop pragmas as parameters to autotune the XSBench and to make sure that the result is verified. Note that we use the clang-14 compiler from SOLLVE LLVM \cite{SOLL} to compile the original and the mixed XSBench on Theta.

\if 0
{\scriptsize
\begin{verbatim}
cs = CS.ConfigurationSpace(seed=1234)
# number of threads
p0= CSH.OrdinalHyperparameter(name='p0', sequence=
['4','8','16','32','48','64','96','128','192','256'], default_value='64')
#block size for openmp dynamic schedule
p1= CSH.OrdinalHyperparameter(name='p1', sequence=
['10','20','40','50','64','80','100','128','160','200','256','400'], default_value='100')
#clang unrolling
p2= CSH.CategoricalHyperparameter(name='p2', 
choices=["#pragma clang loop unrolling full", " "], default_value=' ')
#omp parallel
p3= CSH.CategoricalHyperparameter(name='p3', 
choices=["#pragma omp parallel for", " "], default_value=' ')
# tile size for one dimension for 2D tiling
p4= CSH.OrdinalHyperparameter(name='p4', sequence=
['2','4','8','16','32','64','96','128','256','512','1024'], default_value='96')
# tile size for another dimension for 2D tiling
p5= CSH.OrdinalHyperparameter(name='p5', sequence=
['2','4','8','16','32','64','96','128','256','512','1024'], default_value='256')
# omp placement
p6= CSH.CategoricalHyperparameter(name='p6', 
choices=['cores','threads','sockets'], default_value='cores')
# OMP_PROC_BIND
p7= CSH.CategoricalHyperparameter(name='p7', 
choices=['close','spread','master'], default_value='close')
# OMP_SCHEDULE
p8= CSH.CategoricalHyperparameter(name='p8', 
choices=['dynamic','static','auto'], default_value='static')

cs.add_hyperparameters([p0, p1, p2, p3, p4, p5, p6, p7, p8])

\end{verbatim}
 }  
\fi
 
We use 9 parameters to define the following parameter space. The system runtime parameters are OMP\_NUM \_THREADS, OMP\_PLACES, OMP\_PROC\_BIND, and OMP\_SCHEDULE; the unique application parameters are block size for OpenMP dynamic schedule, Clang loop unrolling full, additional OpenMP parallel for, and two tile sizes for 2D loop tiling for a double nested loop (because this loop fails when parallelizing it in OpenMP).
 Because each Theta node has 64 cores with up to 4 threads per core, we choose 10 choices for OMP\_NUM\_THREADS in the range of 4 to 256 threads. The OpenMP specification includes many environment variables related to program execution \cite{OMP}. For the OMP\_PLACES, there are three options: cores (threads are allowed to float on cores), threads (threads are bound to specific logical processors), and sockets (threads are allowed to float on sockets). For the OMP\_PROC\_BIND, there are also three options:  close (threads placed consecutively), spread (threads spread equally on hardware), and master (threads placed on master  to enhance locality). OMP\_SCHEDULE allows specifying the schedule type (static, dynamic, or auto) with the default chunk size. 
For the block size (default 100 from the original code), we choose 12 choices in the range from 10 to 400. For the unrolling and additional OpenMP parallel for (4 in total), each has two choices with or without it. For two tile sizes for 2D loop tiling, we choose 11 choices in the range from 2 to 1,024 for each dimension. Therefore, the parameter space with total different configurations is 270*5808*4 =6,272,640, as shown in Table \ref{tab:sz}. 

%XSBench supports two simulations: history based and event based. We autotune both simulations. 
Figure~\ref{fig:x1a} shows the autotuning of the mixed pragmas version of XSBench (history based) on a single node, where wall-clock time stands for the time from the start of the autotuning to its end; the red line stands for the baseline runtime (3.31 s for the original code using 64 threads); and the blue line stands for the autotuning process over time. We achieve the best performance 3.262 s, and the search reaches the good region of the parameter space over time. Figure~\ref{fig:x1o} shows the ytopt overhead for each evaluation during the autotuning. 
The overhead is less than 65 s for the large parameter space.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/xsbench-1.png}
        \subcaption{history-based}
        \label{fig:x1a}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/xsbench-1e.png}
        \subcaption{event based}
        \label{fig:x1e}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/xsbench-1o.png}
        \subcaption{history based}
        \label{fig:x1o}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/xsbench-1eo.png}
        \subcaption{event based}
        \label{fig:x1eo}
    \end{subfigure}
    \setlength{\belowcaptionskip}{-8pt}
    \caption{Autotuning the Mixed-Pragmas Version of XSBench on a Theta Node}
    \label{fig:x1}
\end{figure}

Figure~\ref{fig:x1e} shows autotuning the mixed-pragmas version of XSBench (event based) on a single node. We observe the best performance 3.339 s (the baseline:  3.395 s for using 64 threads), and the search reaches the good region of the parameter space over time. Figure~\ref{fig:x1eo} shows the ytopt overhead for each evaluation over time. The overhead is between 49 s and 69.2 s for the large parameter space. We observe that the ytopt overhead for the first evaluation is the largest because it also includes setting the ytopt conda environment. Overall, the ytopt overhead is less than 70 s. 

%The best for event-based: 64,256,#pragma clang loop unrolling full,#pragma omp parallel for,32,1024,threads,spread,static,3.339
%The best for hisotry-based: 64,20,#pragma clang loop unrolling full,#pragma omp parallel for,256,64,cores,close,dynamic,3.262

\subsection{On Summit}

We use the OpenMP offload version of XSBench to autotune the application on a Summit node. %Specifically, we use it to add more OpenMP pragmas and optimization pragmas such as simd and schedule into the source code 
%GAIL - this "so that" seems a bit awkward   - do you mean "so that we can  use  ytopt? or do you  mean we  then use ytopt  to  autotune this  offload  version?
%so that we use ytopt to autotune it to achieve the best performance. 
We integrate the additional OpenMP pragmas with some clauses as parameters to autotune the XSBench and to make sure that the result is verified. The OpenMP offload version supports only the event-based simulation. In the rest of this paper, we use XSBench with the event-based simulation for our experiments. 

\if 0
{\scriptsize
\begin{verbatim}
cs = CS.ConfigurationSpace(seed=1234)
# number of threads (x4 for smt4)
p0= CSH.OrdinalHyperparameter(name='p0', 
sequence=['1','2','4','5','8','10','16','21','32','42''], default_value='42')
# omp placement
p1= CSH.CategoricalHyperparameter(name='p1', 
choices=['cores','threads','sockets'], default_value='cores')
# OMP_PROC_BIND
p2= CSH.CategoricalHyperparameter(name='p2', 
choices=['close','spread','master'], default_value='close')
#omp parallel
p3= CSH.CategoricalHyperparameter(name='p3', 
choices=["#pragma omp parallel for", " "], default_value=' ')
#omp parallel simd
p4= CSH.CategoricalHyperparameter(name='p4', 
choices=["simd", " "], default_value=' ')
#omp target device selected
p5= CSH.CategoricalHyperparameter(name='p5', 
choices=["device(offloaded_to_device)", " "], default_value=' ')
# OMP_SCHEDULE
p6= CSH.CategoricalHyperparameter(name='p6', 
choices=['dynamic','static','auto'], default_value='static')
#omp target schedule
p7= CSH.CategoricalHyperparameter(name='p7', choices=
["schedule(static,1)","schedule(static,2)","schedule(static,4)","schedule(static,8)",
"schedule(static,16)","schedule(static,32)", " "], default_value=' ')
#OMP_TARGET_OFFLOAD
p8= CSH.CategoricalHyperparameter(name='p8', 
choices=['DEFAULT','DISABLED','MANDATORY'], default_value='DEFAULT')

cs.add_hyperparameters([p0, p1, p2, p3, p4, p5, p6, p7, p8])
\end{verbatim}
 }  
 \fi
 
We use 9 parameters to define the following parameter space. The system runtime parameters are OMP\_NUM\_THREADS, OMP\_PLACES, OMP\_PROC\_ BIND, OMP\_SCHEDULE, and OMP\_TARGET\_OFFLOAD; the unique application parameters are additional ``\#pragma omp parallel for'', simd, device, and schedule clauses. Because each Summit node has 42 cores with up to 4 threads per core, we choose 10 choices for OMP\_NUM\_THREADS in the range from 4 to 168 threads. OMP\_TARGET\_OFFLOAD affects the behavior of execution on host and device including host fallback and provides three options: DEFAULT (try to execute on a GPU; if a supported GPU is not available, fall back to the host), DISABLED (do not execute on the GPU even if one is available; execute on the host), and MANDATORY (execute on a GPU or terminate the program). The simd clause is to create a team of threads to execute the loop in parallel using SIMD instructions. The device clause is to evaluate an assigned non-negative integer value less than the value of omp\_get\_num\_devices() (6 devices on a Summit node). schedule(static,1) for the OpenMP target pragmas is for memory access coalescing; scheduling a chunk size of 1 for each thread allows consecutive threads to access consecutive global memory locations. We choose six chunk sizes in the range from 1 to 32, adding one of them or without adding one as the total 7 choices for the parameter schedule. Therefore, the parameter space with all the different configurations is 810*56*4=181,440, as shown in Table~\ref{tab:sz}. 

Figure~\ref{fig:x1s} shows the autotuning of the OpenMP offload version of XSBench (event  based) on a single Summit node using 6 GPUs. We observe the best performance 2.138 s (the baseline: 2.20 s for using 6 GPUs and 168 threads), and the search reaches the good region of the parameter space over time in Figure~\ref{fig:s1a}. Figure~\ref{fig:s1o} shows the ytopt overhead for each evaluation during the autotuning. It is less than 24 s. This is much faster than on Theta.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/xsbench-s1.png}
        \subcaption{Application runtime}
        \label{fig:s1a}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/xsbench-1so.png}
        \subcaption{ytopt overhead}
        \label{fig:s1o}
    \end{subfigure}
    \setlength{\belowcaptionskip}{-8pt}
    \caption{Autotuning OpenMP Offload Version of XSBench (Event Based) on a Summit Node}
    \label{fig:x1s}
\end{figure}



