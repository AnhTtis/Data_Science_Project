
\section{Autotuning Performance at Large Scales}

In this section we apply the proposed framework in Figure~\ref{fig:pf} to autotune the performance of four ECP proxy applications---XSBench, AMG, SWFFT, and SW4lit---on both Theta and Summit. To launch an application to compute nodes, Theta uses aprun, and Summit uses jsrun. The processor core on both Theta and Summit supports the simultaneous multithreading (SMT) level of 4 as default so that the number of threads per node is supported up to 256 on Theta and up to 168 on Summit. In Step 3 shown in Figure~\ref{fig:pf}, based on the value of the number of threads from the selected configuration, the number of nodes reserved, and the number of MPI ranks, ytopt generates the aprun/jsrun command line for application launch on compute nodes. For instance, we reserve 4,096 nodes with one MPI rank per node to run an application on Theta and Summit. 

On Theta we use the following algorithm to generate an aprun command line.
{\scriptsize
\begin{verbatim}
OMP_NUM_THREADS=n
if (n <= 64) {
 aprun -n 4096 -N 1 -cc depth -d n -j 1 application 
} else { if (n <= 128) {
  aprun -n 4096 -N 1 -cc depth -d n/2 -j 2 application 
 } else { if (n <= 192) {
     aprun -n 4096 -N 1 -cc depth -d n/3 -j 3 application
    } else {
       	aprun -n 4096 -N 1 -cc depth -d n/4 -j 4 application 
    }
  }
}
\end{verbatim}
}  

When we choose the number of threads n for each case, we make sure that n/2, n/3, or n/4 is integer on Theta. Then we use the algorithm to set the proper number of threads per core to generate the aprun command line.

On Summit we use the following algorithm to generate the jsrun command line. When the application uses 6 GPUs per node for the hybrid MPI/OpenMP offload application XSBench, we use the algorithm.
{\scriptsize
\begin{verbatim}
OMP_NUM_THREADS=n
jsrun -n4096 -a6 -g6 -c42 -bpacked:n/4 -dpacked  application 
\end{verbatim}
}  
When we choose the number of threads n, we make sure that n/4 is an integer because of the SMT level of 4 as default on Summit. We set one MPI rank per GPU and 42 cores per node for threads.

When the application uses only CPUs per node without any GPU for the hybrid MPI/OpenMP applications AMG, SWFFT, and SW4lite, we use the following algorithm to set one MPI rank per node and 42 cores per node for threads.
{\scriptsize
\begin{verbatim}
OMP_NUM_THREADS=n
jsrun -n4096 -a1 -g0 -c42 -bpacked:n/4 -dpacked  application 
\end{verbatim}
}  

For measuring the baseline performance for each application with a given problem size, we set the number of threads to 64 (which results in the best performance) on Theta and 168 threads (which also results in the best performance) on Summit to run the application under the default system configuration five times. Then we use the smallest application runtime as the baseline for the application. Notice that because of the limited node-hour allocations on Theta and Summit for our projects, we had to set most of the wall-clock times for autotuning runs at half an hour (1800 s). This limits the number of evaluations for different configurations during the autotuning. 
%GAI  - do you really need the next sentence>
%An evaluation of an application means that the application with a configuration is executed and its performance is evaluated.

Figure~\ref{fig:x2} shows autotuning MPI/OpenMP XSBench with the large problem size on 1,024 and 4,096 nodes on Theta. Because XSBench is weak scaling, both the autotuning processes are similar because of its embarrassingly parallel implementation of XSBench. We observe that the ytopt search reaches the good region of the parameter space over time that is close to that of the baseline. The ytopt overhead is similar to that in Figure~\ref{fig:x1eo} on Theta.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/xsbench1024.png}
        \subcaption{on 1024 nodes}
        \label{fig:x2a}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/xsbench4096.png}
        \subcaption{on 4096 nodes}
        \label{fig:x2b}
    \end{subfigure}
    \setlength{\belowcaptionskip}{-8pt}
    \caption{Autotuning XSBench at Large Scales on Theta}
    \label{fig:x2}
\end{figure}

Figure~\ref{fig:x3} shows autotuning MPI/OpenMP offload XSBench using 6 GPUs per node and 1 MPI rank per GPU on 4096 nodes on Summit. We observe that the ytopt search gradually reaches the good region of the parameter space over time (baseline: . Because of the limited number of evaluations (20), however, it does not reach the optimal performance yet in Figure~\ref{fig:x3a}. Figure~\ref{fig:x3b} shows the ytopt overhead during the entire autotuning. Notice that the first ytopt overhead (111 s) also includes the time spent in setting the ytopt conda environment and loading the nvhpc module, however, most of the times are around 60 s.  The ytopt overhead is less than 111 s.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/xsbench4096-s.png}
        \subcaption{on 4096 nodes}
        \label{fig:x3a}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/xsbench4096-so.png}
        \subcaption{ytopt overhead}
        \label{fig:x3b}
    \end{subfigure}
    \setlength{\belowcaptionskip}{-8pt}
    \caption{Autotuning XSBench at Large Scale on Summit}
    \label{fig:x3}
\end{figure}

SWFFT is weak scaling. Figure~\ref{fig:s2} shows autotuning SWFFT with a problem size of 3D grid 4096x4096x4096 on 4,096 nodes on Summit. We observe that the ytopt search reaches the good region of the parameter space over time with the smallest runtime of 7.797 s that is better than the baseline (8.93s) in Figure~\ref{fig:s2a}. This is 12.69\% performance improvement. The ytopt overhead is shown in Figure~\ref{fig:s2b}, and most of the times are around 20 s because of the small parameter space for SWFFT. So the ytopt overhead is less than 50 s.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/swfft4096-s.png}
        \subcaption{on 4096 nodes}
        \label{fig:s2a}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/swfft4096-so.png}
        \subcaption{ytopt overhead}
        \label{fig:s2b}
    \end{subfigure}
    \setlength{\belowcaptionskip}{-8pt}
    \caption{Autotuning SWFFT at Large Scale on Summit}
    \label{fig:s2}
\end{figure}

Figure \ref{fig:s11} shows autotuning SWFFT with the same problem size on 4,096 nodes on Theta. We observe that the ytopt search reaches the good region of the parameter space over time that is close to the time of the baseline in Figure~\ref{fig:s11a}. The ytopt overhead is less than 30 s in Figure~\ref{fig:s11b}. 

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/swfft4096.png}
        \subcaption{on 4096 nodes}
        \label{fig:s11a}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/swfft4096-o.png}
        \subcaption{ytopt overhead}
        \label{fig:s11b}
    \end{subfigure}
    \setlength{\belowcaptionskip}{-8pt}
    \caption{Autotuning SWFFT at Large Scale on Theta}
    \label{fig:s11}
\end{figure}

AMG is weak scaling. Figure~\ref{fig:a2} shows autotuning of AMG on 4,096 nodes on Summit. We use the 3D Laplace problem "-laplace -n 100 100 100 -P 16 16 16" as the input, which means generating a problem with 1,000,000 grid points per MPI rank with a domain size 1600 x 1600 x 1600 on 4,096 nodes with 1 MPI rank per node and various numbers of threads per MPI rank. We observe that the ytopt autotuning reaches the best configuration with the smallest runtime of 6.734 s, which is much better than the baseline performance of 8.694 s in Figure~\ref{fig:a2a}. This is a 22.54\% performance improvement. Figure~\ref{fig:a2b} shows the ytopt overhead is less than 45 s. 
 
\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/amg4096-s.png}
        \subcaption{on 4096 nodes}
        \label{fig:a2a}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/amg4096-so.png}
        \subcaption{ytopt overhead}
        \label{fig:a2b}
    \end{subfigure}
    \setlength{\belowcaptionskip}{-8pt}
    \caption{Autotuning AMG at Large Scale on Summit}
    \label{fig:a2}
\end{figure}

Figure~\ref{fig:a1} shows the autotuning of AMG on 4,096 nodes on Theta. Because of the limited wall-clock time (1800 s), we see only six evaluations on 4,096 nodes, mainly caused by the second very long evaluation (1039.06 s) in Figure~\ref{fig:a1a}. We find the configuration for the long evaluation includes system parameters: 48 threads; OMP\_PLACES=threads (that are bound to specific logical processors); OMP\_PROC\_BIND= master (threads placed on master place to enhance locality); and OMP\_SCHEDULE=dynamic. We observe that the system parameter setting mainly causes the long application runtime because the first 48 cores of 64 cores are used and every two cores share the L2 cache. Figure~\ref{fig:a1b} still shows that the ytopt overhead is less than 34 s. 

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/amg4096.png}
        \subcaption{on 4096 nodes}
        \label{fig:a1a}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/amg4096-o.png}
        \subcaption{ytopt overhead}
        \label{fig:a1b}
    \end{subfigure}
    \setlength{\belowcaptionskip}{-8pt}
    \caption{Autotuning AMG at Large Scale on Theta}
    \label{fig:a1}
\end{figure}

SW4lite with the large problem LOH.1-h50 is strong scaling so that we can test it on up to 1,024 nodes. Figure~\ref{fig:w2} shows autotuning SW4lite on 1,024 nodes on Summit. As described in Table~\ref{tab:sz}, the parameter space size for this application is 2,211,840. As shown in Figure~\ref{fig:w2a}, at the beginning of the autotuning, ytopt samples the parameter space randomly for initial evaluations, then leverages the surrogate model to balance exploration of the search space and identifies more-promising parameter configurations using the LCB acquisition function. We observe that ytopt reaches the best configuration with the smallest runtime of 7.661 s, which is much better than  the baseline performance of 11.067 s. This is a 30.78\% performance improvement. Figure~\ref{fig:w2b} shows the ytopt overhead is less than 46 s.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/sw4lite1024-s.png}
        \subcaption{on 1024 nodes}
        \label{fig:w2a}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/sw4lite1024-so.png}
        \subcaption{ytopt overhead}
        \label{fig:w2b}
    \end{subfigure}
    \setlength{\belowcaptionskip}{-8pt}
    \caption{Autotuning SW4lite at Large Scale on Summit}
    \label{fig:w2}
\end{figure}

Figure~\ref{fig:w1a} shows how SW4lite is autotuned on 1,024 nodes on Theta. We observe that ytopt reaches the best configuration with the smallest runtime of 14.427 s, which is much better than the baseline performance of 171.595 s. This is a 91.59\% performance improvement. We achieve the large improvement because we use the improved version of SW4lite \cite{WU21} to define the parameter space for SW4lite with the parameter MPI\_Barrier(MPI\_COMM\_WORLD). When running SW4lite on 1,024 nodes to measure the baseline performance, the compute time is small (around 3 s), but the communication time increases significantly (around 168 s) on Theta for the original code. Figure~\ref{fig:w1b} shows the ytopt overhead during the entire autotuning which is less than 46 s.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/sw4lite1024.png}
        \subcaption{on 1024 nodes}
        \label{fig:w1a}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/sw4lite1024-o.png}
        \subcaption{ytopt overhead}
        \label{fig:w1b}
    \end{subfigure}
    \setlength{\belowcaptionskip}{-8pt}
    \caption{Autotuning SW4lite at Large Scale on Theta}
    \label{fig:w1}
\end{figure}

Overall, we observe that the ytopt overheads for the four applications are impacted mainly by the systems (for launching the application on the compute nodes) and application compiling times (given in Table~\ref{tab:cp}). %because the ytopt processing time includes the time spent in searching the parameter space, building the surrogate model, processing the new configuration to generate a new code and the aprun/jsrun command line, compiling the new code, launching the application, and storing the new configuration and performance in the performance database except the application runtime. 
We find that the ytopt overhead on up to 4,096 nodes on both Theta and Summit is less than 111 s \xingfu{in Table~\ref{tab:yo}}. 
This shows that our autotuning framework has low overhead and good scalability because the ytopt overhead does not increase much for autotuning the applications on small or large number of nodes. %In most cases, the autotuning framework performs faster on Summit than on Theta because Summit is much faster and more stable than Theta.

\begin{table}[ht]
\center
\caption{The maximum ytopt overhead (seconds) for each application on Theta and Summit}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
System & XSBench-Mixed & XSBench & SWFFT & AMG & SW4lite  \\
\hline
Theta &  70 & 69 & 30  & 34   &   46\\
\hline
Summit & 24 & 111 & 50  & 45   &   46\\
\hline
\end{tabular}
\label{tab:yo}
\end{table}

\if 0
\begin{table}[ht]
\center
\caption{Performance improvement percentage (\%) for each application on Theta and Summit}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
System & XSBench-mixed  & XSBench & SWFFT & AMG & SW4lite  \\
\hline
Theta & 1.6 & -9.8 & 0.03  & -8.21   &   91.59\\
\hline
Summit &2.8 &  -54.8  & 12.69  & 22.54   &   30.78\\
\hline
\end{tabular}
\label{tab:pi}
\end{table}

\fi