
\section{Systems and ECP Proxy Applications}
In this section we discuss the HPC system platforms and four ECP proxy applications \cite{ECP} used in our experiments.

We conduct our experiments on the Cray XC40 Theta \cite{THETA} of approximately 12 petaflops peak performance at Argonne National Laboratory and the IBM Power9 heterogeneous system Summit \cite{SUMMIT} of approximately 200 petaflops peak performance at Oak Ridge National Laboratory. In this section, we briefly describe their specifications shown in Table~\ref{tab:sys}. 

\textbf{Theta:}
Theta has 4,392 Cray XC40 nodes. Each node has 64 compute cores (one Intel Xeon Phi Knights Landing (KNL) 7230 with the thermal design power (TDP) of 215 W), shared L2 cache of 32 MB (1 MB L2 cache shared by two cores), 16 GB of high-bandwidth in-package memory Multi-Channel DRAM (MCDRAM), 192 GB of DDR4 RAM, and a 128 GB SSD. MCDRAM can be configured as a shared last level cache L3 (cache mode) or as a distinct NUMA node memory (flat mode) in or somewhere in between. The default memory mode is the cache mode. The Cray XC40 system uses the Cray Aries dragonfly network with user access to a Lustre parallel file system with 10 PB of capacity and 210 GB/s bandwidth. 

In this work, we use GEOPM \cite{ES17} to measure node energy consumption on Theta. The power sampling rate used is approximately 2 samples per second (default). We conduct all autotuning experiments in performance and energy with the cache mode. The compilers on Theta are CrayPE 2.6.5 (default) and clang 14 installed \cite{SOLL}. The aprun command is used to specify to ALPS (Application Level Placement Scheduler) the resources and placement parameters needed for the application at application launch on Theta.  

\textbf{Summit:}
Summit has 4,608 IBM Power System AC922 nodes. Each node contains two IBM POWER9 processors with 42 cores and six NVIDIA Volta V100 accelerators. Each node has 512 GB of DDR4 memory for use by the POWER9 processors and 96 GB of high-bandwidth memory (HBM2) for use by the accelerators. Additionally, each node has 1.6 TB of nonvolatile memory that can be used as a burst buffer. Summit is connected to an IBM Spectrum Scale filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. For each Summit node, the TDP of each Volta GPU is 300 W, and the TDP of each Power9 is 190 W. The power consumption of each Summit node is 2,200 W. Although we use the NVIDIA System Management Interface (nvidia-smi) \cite{NSMI} to measure power consumption for each GPU, the power measurement for IBM Power9 is not available to the public. Therefore, we autotune only performance of HPC applications on Summit. The compilers on Summit are gcc 9.1.0 (default) and nvhpc 21.3. The jsrun command is used for managing an allocation that is provided by an external resource manager within IBM Job Step Manager (JSM) software package on Summit. 

\begin{table}
\center
\caption{System Platform Specifications and Tools}
\begin{tabular}{c}
  \includegraphics[width=.80\linewidth]{figs/theta-summit.png}
  \end{tabular}
\label{tab:sys}       
\end{table}  

\subsection{ECP Proxy Applications}
%In high-performance computing, ECP proxy applications \cite{ECP} are small, simplified codes that allow application developers to share important features of large applications without forcing collaborators to assimilate large and complex code bases. 
In this section we discuss four hybrid MPI/OpenMP ECP proxy applications for our experiments: XSBench \cite{XSB}, SWFFT \cite{SWF}, AMG \cite{AMG}, and SW4lite \cite{SW4L}.

\subsubsection{Weak-Scaling Applications}
We discuss the three weak-scaling ECP proxy applications XSBench, SWFFT, and AMG.

XSBench \cite{XSB} is a mini-app representing a key computational kernel of the Monte Carlo neutron transport algorithm and represents the continuous energy macroscopic neutron cross section lookup kernel. It serves as a lightweight stand-in for full neutron transport applications like OpenMC \cite{OMC}. This code provides a much simpler and more transparent platform for determining performance benefits resulting from a given hardware feature or software optimization.  XSBench provides an MPI mode which runs the same code on all MPI ranks simultaneously with no decomposition across ranks of any kind, and all ranks accomplish the same work. It is an embarrassingly parallel implementation. It supports history-based transport (default):  parallelism is expressed over independent particle histories, with each particle being simulated in a serial fashion from birth to death; and event-based transport: parallelism is instead expressed over different collision (or "event") types.  XSBench is the hybrid MPI/OpenMP code written in C and supports OpenMP offload. The OpenMP offload implementation only supports the event-based transport. The problem size is large as default. 

SWFFT \cite{SWF} is to test the Hardware Accelerated Cosmology Code (HACC) 3D distributed memory discrete fast Fourier transform (FFT) with one forward FFT and one backward FFT. It assumes that global grid will originally be distributed between MPI ranks using a 3D Cartesian communicator. That data needs to be re-distributed to three 2D pencil distributions in turn in order to compute the double-precision FFTs along each dimension. SWFFT is the hybrid MPI/OpenMP code written in C++ and C and requires the cubic number of MPI ranks and FFTW3 (double precision, OpenMP version) installed. We configure it as weak scaling case. The problem size is 4096x4096x4096 for 4096 MPI ranks. We also set the number of run tests 2.

AMG \cite{AMG} a parallel algebraic multigrid solver for linear systems arising from problems on unstructured grids and builds linear systems for various 3-dimensional problems. Parallelism is achieved by data decomposition. AMG achieves this decomposition by simply subdividing the grid into logical X x Y x Z (in 3D) chunks of equal size. It is the hybrid MPI/OpenMP code written in C. The problem size is the 3D Laplace problem "-laplace -n 100 100 100 -P X Y Z".
This will generate a problem with 1,000,000 grid points per MPI process with a domain of the size 100*X x 100*Y x 100*Z. 

\subsubsection{Strong-Scaling Application}

SW4lite \cite{SW4L} is a bare bone version of SW4 \cite{SP12, SB18} (Seismic Waves, 4th order accuracy) intended for testing performance in a few important numerical kernels of SW4.  SW4 implements substantial capabilities for 3-D seismic modeling with a free surface condition on the top boundary, absorbing super-grid conditions on the far-field boundaries, and an arbitrary number of point force and/or point moment tensor source terms. It uses a fourth order in space and time finite-difference discretization of the elastic wave equations in displacement formulation. The large problem LOH.1-h50 is from the SCEC (Southern California Earthquake Center) test suite \cite{Day01}. It sets up a grid with a spacing h (=50) over a domain (X x Y x Z) 30000 x 30000 x 17000. It will run from time t=0 to t=9. The material properties are given by the block commands. They describe a layer on top of a half-space in the z-direction. A single moment point source is used with the time dependency being the Gaussian function. SW4lite is the hybrid MPI/OpenMP code written in C++ and Fortran90. In \cite{WU21}, performance and energy of SW4lite were optimized for the improved version. We use the improved version to define the parameter space for autotuning.

\subsubsection{Compiling Time for Each Application}

Table~\ref{tab:cp} shows the average compiling time (in seconds) for each ECP proxy application on Theta and Summit. We measured the compiling time for each application five times to get the average compiling time. We observe that the compiling time for SW4lite is 162.066 s on Theta and 58 s on Summit. This really impacts the autotuning wall-clock time in Step 4 shown in Figure \ref{fig:pf}. Because of loading the NVidia nvhpc module to compile the XSBench OpenMP offload version for using GPUs on Summit, it takes 4.645 s, which is much larger than that on Theta.

\begin{table}[ht]
\center
\caption{Compiling time (s) on Theta and Summit}
\begin{tabular}{|r|c|c|c|c|}
\hline
System & XSBench & SWFFT & AMG & SW4lite  \\
\hline
Theta &  2.021 & 3.494  & 2.825   &   162.066 \\
\hline
Summit & 4.645  & 3.781  & 2.757 &  58.000 \\
\hline
\end{tabular}
\label{tab:cp}
\end{table}

