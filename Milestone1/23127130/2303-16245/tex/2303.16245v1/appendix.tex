%\newpage 
\section{Appendix: Artifact Description/Artifact Evaluation}

\subsection{SUMMARY OF THE EXPERIMENTS REPORTED}
We used our autotuning framework to autotune the ECP proxy applications XSBench 19.0, SWFFT 1.0, AMG 1.2 and SW4lite 1.1 from ECP proxy application suites on up to 4096 nodes on two production systems ANL Cray XC40 Theta with Intel KNL and ORNL Summit with IBM Power9 and Nvidia GPU V100. The autotuning framework is based on Python. We set the seed before each run so that the code does the same thing every time we run it.
%The ytopt autotuning framework is open source and available to download from the link https://github.com/ytopt-team/ytopt. 

\subsection{ARTIFACT AVAILABILITY}
\subsubsection{Software Artifact Availability} 
All author-created software artifacts are maintained in a public repository.

\subsubsection{Hardware Artifact Availability} 
There is no author-created hardware artifact.

\subsubsection{Data Artifact Availability} 
All author-created data artifacts are maintained in a public repository.

\subsubsection{Proprietary Artifact Availability} 
None of the associated artifacts, author-created or otherwise, are proprietary.

\subsubsection{List of URLs where artifacts are available} We list some important softwares and codes we used:

ytopt framework: https://github.com/ytopt-team/ytopt

ECP proxy applications:

(a) The original codes:

https://proxyapps.exascaleproject.org/ecp-proxy-apps-suite/

XSBench 19.0: https://github.com/ANL-CESAR/XSBench

SWFFT 1.0: https://git.cels.anl.gov/hacc/SWFFT

AMG 1.2: https://github.com/LLNL/AMG

SW4lite 1.1: https://github.com/geodynamics/sw4lite

(b) The modified codes for autotuning:

https://github.com/ytopt-team/autotune/tree/master/
Benchmarks/ECP-Apps

\subsection{BASELINE EXPERIMENTAL SETUP, AND AUTOTUNING SETUP  AND RESULTS FOR THIS PAPER}

\subsubsection{System Details}

We conduct our experiments on the Cray XC40 Theta \cite{THETA} of approximately 12 petaflops peak performance at Argonne National Laboratory and the IBM Power9 heterogeneous system Summit \cite{SUMMIT} of approximately 200 petaflops peak performance at Oak Ridge National Laboratory. In this section, we briefly describe their specifications shown in Table~\ref{tab:sys}. 

\textbf{Theta:}
Theta has 4,392 Cray XC40 nodes. Each node has 64 compute cores (one Intel Xeon Phi Knights Landing (KNL) 7230 with the TDP of 215 W), shared L2 cache of 32 MB (1 MB L2 cache shared by two cores), 16 GB of high-bandwidth in-package memory, 192 GB of DDR4 RAM, and a 128 GB SSD. The Cray XC40 system uses the Cray Aries dragonfly network with user access to a Lustre parallel file system with 10 PB of capacity and 210 GB/s bandwidth. 
In this work, we use GEOPM \cite{ES17} to measure node energy consumption on Theta. The power sampling rate used is approximately 2 samples per second (default). We conduct all autotuning experiments in performance and energy with the cache mode. The compilers on Theta are CrayPE 2.6.5 (default) and clang 14 installed \cite{SOLL}. The aprun command is used to specify to ALPS (Application Level Placement Scheduler) the resources and placement parameters needed for the application at application launch on Theta. 

\textbf{Summit:}
Summit has 4,608 IBM Power System AC922 nodes. Each node contains two IBM POWER9 processors with 42 cores and six NVIDIA Volta V100 accelerators. Each node has 512 GB of DDR4 memory for use by the POWER9 processors and 96 GB of high-bandwidth memory (HBM2) for use by the accelerators. Additionally, each node has 1.6 TB of nonvolatile memory that can be used as a burst buffer. Summit is connected to an IBM Spectrum Scale filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. For each Summit node, the thermal design power (TDP) of each Volta GPU is 300 W, and the TDP of each Power9 is 190 W. The power consumption of each Summit node is 2,200 W. Although we use the NVIDIA System Management Interface (nvidia-smi) \cite{NSMI} to measure power consumption for each GPU, the power measurement for IBM Power9 is not available to the public. Therefore, we autotune only performance of HPC applications on Summit. The compilers on Summit are gcc 9.1.0 (default) and nvhpc 21.3. The jsrun command is used for managing an allocation that is provided by an external resource manager within IBM Job Step Manager (JSM) software package on Summit. 

\begin{table}
\center
\caption{System Platform Specifications and Tools}
\begin{tabular}{c}
  \includegraphics[width=.65\linewidth]{figs/theta-summit.png}
  \end{tabular}
\label{tab:sys}       
\end{table}  

\subsubsection{ECP Proxy Applications}
We discuss four hybrid MPI/OpenMP ECP proxy applications for our experiments: XSBench 19.0 \cite{XSB}, SWFFT 1.0 \cite{SWF},  AMG 1.2 \cite{AMG} and SW4lite 1.1 \cite{SW4L}.

XSBench \cite{XSB} is a mini-app representing a key computational kernel of the Monte Carlo neutron transport algorithm and represents the continuous energy macroscopic neutron cross section lookup kernel. It serves as a lightweight stand-in for full neutron transport applications like OpenMC \cite{OMC}. This code provides a much simpler and more transparent platform for determining performance benefits resulting from a given hardware feature or software optimization.  XSBench provides an MPI mode which runs the same code on all MPI ranks simultaneously with no decomposition across ranks of any kind, and all ranks accomplish the same work. It is an embarrassingly parallel implementation. It supports history-based transport (default):  parallelism is expressed over independent particle histories, with each particle being simulated in a serial fashion from birth to death; and event-based transport: parallelism is instead expressed over different collision (or "event") types.  XSBench is the hybrid MPI/OpenMP code written in C and supports OpenMP offload. The OpenMP offload implementation only supports the event-based transport. The problem size is large as default. 

SWFFT \cite{SWF} is to test the Hardware Accelerated Cosmology Code (HACC) 3D distributed memory discrete fast Fourier transform (FFT) with one forward FFT and one backward FFT. It assumes that global grid will originally be distributed between MPI ranks using a 3D Cartesian communicator. That data needs to be re-distributed to three 2D pencil distributions in turn in order to compute the double-precision FFTs along each dimension. SWFFT is the hybrid MPI/OpenMP code written in C++ and C and requires the cubic number of MPI ranks and FFTW3 (double precision, OpenMP version) installed. We configure it as weak scaling case. The problem size is 512x512x512 for 8 MPI ranks, 2560x2560x2560 for 1024 MPI ranks, and 4096x4096x4096 for 4096 MPI ranks. We also set the number of run tests 2.

AMG \cite{AMG} a parallel algebraic multigrid solver for linear systems arising from problems on unstructured grids and builds linear systems for various 3-dimensional problems. Parallelism is achieved by data decomposition. AMG achieves this decomposition by simply subdividing the grid into logical X x Y x Z (in 3D) chunks of equal size. It is the hybrid MPI/OpenMP code written in C. The problem size is the 3D Laplace problem "-laplace -n 100 100 100 -P X Y Z".
This will generate a problem with 1,000,000 grid points per MPI process with a domain of the size 100*X x 100*Y x 100*Z. 

SW4lite \cite{SW4L} is a bare bone version of SW4 \cite{SP12, SB18} (Seismic Waves, 4th order accuracy) intended for testing performance in a few important numerical kernels of SW4.  SW4 implements substantial capabilities for 3-D seismic modeling with a free surface condition on the top boundary, absorbing super-grid conditions on the far-field boundaries, and an arbitrary number of point force and/or point moment tensor source terms. It uses a fourth order in space and time finite-difference discretization of the elastic wave equations in displacement formulation. The large problem LOH.1-h50 is from the SCEC (Southern California Earthquake Center) test suite \cite{Day01}. It sets up a grid with a spacing h (=50) over a domain (X x Y x Z) 30000 x 30000 x 17000. It will run from time t=0 to t=9. The material properties are given by the block commands. They describe a layer on top of a half-space in the z-direction. A single moment point source is used with the time dependency being the Gaussian function. SW4lite is the hybrid MPI/OpenMP code written in C++ and Fortran90. In \cite{WU21}, performance and energy of SW4lite were optimized for the improved version. We use the improved version to define the parameter space for autotuning.

\subsubsection{Compilers and Versions} We list the compiler and versions on both Theta and Summit.

\textbf{Theta}: 

CrayPE 2.6.5 (default, ftn, cc, CC for MPI/OpenMP), cray-mpich/7.7.14, and clang 14 installed \cite{SOLL}, 

\textbf{Summit}: 

gcc 9.1.0 (default, mpicc, mpic++, mpif90), nvhpc 21.3 (nvc, nvcc), and spectrum-mpi/10.4.0.3-20210112

\subsubsection{Libraries and Versions} We list the main libraries and versions on both Theta and Summit

\textbf{Theta}: 

Python 3.7.11, miniconda-3/latest (conda 4.8.3), configspace 0.4.19, dh-scikit-optimize 0.9.6, autotune 0.0.1, ytopt  0.0.2, ray 1.9.2, scikit-learn 1.0.2, scipy 1.7.1, numpy 1.21.3, mkl  2021.4.0, cray-fftw/3.3.8.6, cray-libsci/20.06.1, perl/5.26.1, geopm/1.x

\textbf{Summit}: 

Python 3.7.12, conda 4.7.12, ibm-wml-ce/1.7.0-3, configspace 0.4.19, dh-scikit-optimize 0.9.6, autotune 0.0.1, ytopt  0.0.2, ray 2.0.0.dev0, scikit-learn 1.0.2, scipy 1.7.3, numpy 1.21.5, netlib-lapack/3.9.1, fftw/3.3.9, perl/5.30.1


\subsubsection{Job scripts for the baseline and autotuning} We list the job scripts for the baseline and autotuning on both Theta and Summit using 4096 nodes  and XSBench as examples. For other applications, these scripts are similar.

We use ConfigSpace \cite{CFS} to define a parameter space, for instance, for the mixed XSBench, we define the parameter space as follows:

{\scriptsize
\begin{verbatim}
cs = CS.ConfigurationSpace(seed=1234)
# number of threads
p0= CSH.OrdinalHyperparameter(name='p0', sequence=
['4','8','16','32','48','64','96','128','192','256'], 
default_value='64')
#block size for openmp dynamic schedule
p1= CSH.OrdinalHyperparameter(name='p1', sequence=
['10','20','40','50','64','80','100','128','160','200','256',
'400'], default_value='100')
#clang unrolling
p2= CSH.CategoricalHyperparameter(name='p2', 
choices=["#pragma clang loop unrolling full", " "], 
default_value=' ')
#omp parallel
p3= CSH.CategoricalHyperparameter(name='p3', 
choices=["#pragma omp parallel for", " "], default_value=' ')
# tile size for one dimension for 2D tiling
p4= CSH.OrdinalHyperparameter(name='p4', sequence=
['2','4','8','16','32','64','96','128','256','512','1024'], 
default_value='96')
# tile size for another dimension for 2D tiling
p5= CSH.OrdinalHyperparameter(name='p5', sequence=
['2','4','8','16','32','64','96','128','256','512','1024'], 
default_value='256')
# omp placement
p6= CSH.CategoricalHyperparameter(name='p6', 
choices=['cores','threads','sockets'], default_value='cores')
# OMP_PROC_BIND
p7= CSH.CategoricalHyperparameter(name='p7', 
choices=['close','spread','master'], default_value='close')
# OMP_SCHEDULE
p8= CSH.CategoricalHyperparameter(name='p8', 
choices=['dynamic','static','auto'], default_value='static')

cs.add_hyperparameters([p0, p1, p2, p3, p4, p5, p6, p7, p8])

\end{verbatim}
 }  

We use ConfigSpace \cite{CFS} to define a parameter space, for instance, for the OpenMP offload XSBench, as follows:

{\scriptsize
\begin{verbatim}
cs = CS.ConfigurationSpace(seed=1234)
# number of threads (x4 for smt4)
p0= CSH.OrdinalHyperparameter(name='p0', 
sequence=['1','2','4','5','8','10','16','21',
'32','42''], default_value='42')
# omp placement
p1= CSH.CategoricalHyperparameter(name='p1', 
choices=['cores','threads','sockets'], default_value='cores')
# OMP_PROC_BIND
p2= CSH.CategoricalHyperparameter(name='p2', 
choices=['close','spread','master'], default_value='close')
#omp parallel
p3= CSH.CategoricalHyperparameter(name='p3', 
choices=["#pragma omp parallel for", " "], default_value=' ')
#omp parallel simd
p4= CSH.CategoricalHyperparameter(name='p4', 
choices=["simd", " "], default_value=' ')
#omp target device selected
p5= CSH.CategoricalHyperparameter(name='p5', 
choices=["device(offloaded_to_device)", " "], default_value=' ')
# OMP_SCHEDULE
p6= CSH.CategoricalHyperparameter(name='p6', 
choices=['dynamic','static','auto'], default_value='static')
#omp target schedule
p7= CSH.CategoricalHyperparameter(name='p7', choices=
["schedule(static,1)","schedule(static,2)","
schedule(static,4)","schedule(static,8)",
"schedule(static,16)","schedule(static,32)", " "], 
default_value=' ')
#OMP_TARGET_OFFLOAD
p8= CSH.CategoricalHyperparameter(name='p8', 
choices=['DEFAULT','DISABLED','MANDATORY'], 
default_value='DEFAULT')

cs.add_hyperparameters([p0, p1, p2, p3, p4, p5, p6, p7, p8])
\end{verbatim}
 }  

\textbf{Theta}: 

For XSBench executed on 4096 nodes, we use the following job scripts to measure the performance for the baseline and autotuning:

\textbf{(1) runs.cob to submit the job for the baseline performance measurement}:
{\scriptsize
\begin{verbatim}
#!/bin/bash
  
let nnds=4096
let nomp=64
let rpn=1
#-----This part creates a submission script---------
cat >batch.job <<EOF
#!/bin/bash
#COBALT -n ${nnds} -t 60 -O runs${nnds}x${nomp} 
--attrs mcdram=cache:numa=quad -A EE-ECP

export OMP_NUM_THREADS=${nomp}

aprun -n $((nnds*rpn)) -N ${rpn} --cc=depth -d ${nomp} -j 1 
./XSBench -m event > mout.txt
aprun -n $((nnds*rpn)) -N ${rpn} --cc=depth -d ${nomp} -j 1 
./XSBench -m event > mout2.txt
aprun -n $((nnds*rpn)) -N ${rpn} --cc=depth -d ${nomp} -j 1 
./XSBench -m event > mout3.txt
aprun -n $((nnds*rpn)) -N ${rpn} --cc=depth -d ${nomp} -j 1 
./XSBench -m event > mout4.txt
aprun -n $((nnds*rpn)) -N ${rpn} --cc=depth -d ${nomp} -j 1 
./XSBench -m event > mout5.txt

EOF
#-----This part submits the script you just created--------------
chmod +x batch.job
qsub batch.job
\end{verbatim}
 }
 
 Based on the five runs, we use the smallest runtime as the baseline.


\textbf{(2) runs.cob to submit the job for autotuning in performance}:

{\scriptsize
\begin{verbatim}
#!/bin/bash
  
let nnds=4096
#--- process processexe.pl to change the number of nodes
./processcp.pl ${nnds}

#-----This part creates a submission script---------
cat >batch.job <<EOF
#!/bin/bash
#COBALT -n ${nnds} -t 60 -O runs${nnds} 
--attrs mcdram=cache:numa=quad -A EE-ECP

module load miniconda-3/latest
source activate yt

python3 -m ytopt.search.ambs --evaluator ray 
--problem problem.Problem --max-evals=200 --learner RF

conda deactivate

EOF
#-----This part submits the script you just created--------------
chmod +x batch.job
qsub batch.job
\end{verbatim}
 }  
 
 processcp.pl is used in the script runs.cob: 
 {\scriptsize
\begin{verbatim}
#!/usr/bin/perl
#Change this path!
#
# processcp.pl: copy the file processexe.pl to change 
# the proper number of nodes
#

system("cp processexe$ARGV[0].pl processexe.pl");
system("chmod +x processexe.pl");
exit 0;
\end{verbatim}
 } 
 
 The processexe4096.pl is used in the file processcp.pl:
 {\scriptsize
\begin{verbatim} 
 #!/usr/bin/perl
#Change this path!
#
# processexe.pl: process the file exe.pl to change
# the proper number of threads $ARGV[1]
#

$A_FILE = "tmpfile.txt";

$filename1 =  $ARGV[0];
    #print "Start to process ", $filename, "...\n";
    $fname = ">" . $A_FILE;
    $i = 0;
    open(OUTFILE, $fname);
    open (TEMFILE, $filename1);
    while (<TEMFILE>) {
        $line = $_;
        chomp ($line);

        if ($line =~ /system/) {
            if ($ARGV[1] <= 64) {
                print OUTFILE "    system(\"aprun -n 4096 -N 1 
                -cc depth -d ", $ARGV[1], " -j 1 \$filename 
                -m event > tmpoutfile.txt 2>&1\");", "\n";
            } else { if ($ARGV[1] <= 128) {
                print OUTFILE "    system(\"aprun -n 4096 -N 1 
                -cc depth -d ", $ARGV[1]/2, " -j 2 \$filename 
                -m event > tmpoutfile.txt 2>&1\");", "\n";
                } else { if ($ARGV[1] <= 192) {
                print OUTFILE "    system(\"aprun -n 4096 -N 1 
                -cc depth -d ", $ARGV[1]/3, " -j 3 \$filename 
                -m event  > tmpoutfile.txt 2>&1\");", "\n";
                } else {
                print OUTFILE "    system(\"aprun -n 4096 -N 1 
                -cc depth -d ",  $ARGV[1]/4, " -j 4 \$filename 
                -m event > tmpoutfile.txt 2>&1\");", "\n";
                }
               }
              }
        } else {
                print OUTFILE $line, "\n";
        }
    }
   close(TEMFILE);
   close(OUTFILE);
   system("mv $A_FILE $filename1");
   system("chmod +x $filename1");
exit 0;   
\end{verbatim}
 } 

 
The file exe.pl processed in the file processexe.pl is used to execute the application to measure the performance: 
{\scriptsize
\begin{verbatim} 
#!/usr/bin/env perl
#
# exe.pl: measure the execution time 
#
use Time::HiRes qw(gettimeofday);

$A_FILE = "tmpoutfile.txt";
foreach $filename (@ARGV) {
 #  print "Start to preprocess ", $filename, "...\n";
   $ssum = 0.0;
   $nmax = 1;
   @nn = (1..$nmax);
    for(@nn) {
     system("aprun -n 4096 -N 1 -cc depth -d 48 -j 2 
     $filename -m event > tmpoutfile.txt 2>&1");
    open (TEMFILE, '<', $A_FILE);
    while (<TEMFILE>) {
        $line = $_;
        chomp ($line);

        if ($line =~ /Runtime/) {
                ($v1, $v2) = split(': ', $line);
                printf("%.3f", $v2)
        }
   }
   close(TEMFILE);
   }
}
\end{verbatim}
 } 

For XSBench executed on 4096 nodes, we use the following job scripts to measure the energy for the baseline and autotuning:

\textbf{(3) runs.cob to submit the job for the baseline energy measurement}:
{\scriptsize
\begin{verbatim}
#!/bin/bash
  
let nnds=4096
let nomp=64
let rpn=1
#-----This part creates a submission script---------
cat >batch.job <<EOF
#!/bin/bash
#COBALT -n ${nnds} -t 60 -O runs${nnds}x${nomp} 
--attrs mcdram=cache:numa=quad -A Intel

export OMP_NUM_THREADS=${nomp}

module use -a /projects/intel/geopm-home/modulefiles
module unload darshan
module load geopm/1.x

geopmlaunch aprun -n $((nnds*rpn)) -N ${rpn} --geopm-ctl=pthread 
--geopm-report gm.report -- ../XSBench -m event > out.txt
geopmlaunch aprun -n $((nnds*rpn)) -N ${rpn} --geopm-ctl=pthread 
--geopm-report gm2.report -- ../XSBench -m event > out2.txt
geopmlaunch aprun -n $((nnds*rpn)) -N ${rpn} --geopm-ctl=pthread 
--geopm-report gm3.report -- ../XSBench -m event > out3.txt
geopmlaunch aprun -n $((nnds*rpn)) -N ${rpn} --geopm-ctl=pthread 
--geopm-report gm4.report -- ../XSBench -m event > out4.txt
geopmlaunch aprun -n $((nnds*rpn)) -N ${rpn} --geopm-ctl=pthread 
--geopm-report gm5.report -- ../XSBench -m event > out5.txt

EOF
#-----This part submits the script you just created--------------
chmod +x batch.job
qsub batch.job
\end{verbatim}
 }
 
 Based on the five runs, we use the smallest energy as the baseline.


\textbf{(4) runs.cob to submit the job for autotuning in energy}:

{\scriptsize
\begin{verbatim}
#!/bin/bash
  
let nnds=4096
#--- process processexe.pl to change the number of nodes
./processcp.pl ${nnds}

#-----This part creates a submission script---------
cat >batch.job <<EOF
#!/bin/bash
#COBALT -n ${nnds} -t 30 -O runs${nnds} 
--attrs mcdram=cache:numa=quad -A Intel

module use -a /projects/intel/geopm-home/modulefiles
module unload darshan
module load geopm/1.x

module load miniconda-3/latest
source activate yt

python3 -m ytopt.search.ambs --evaluator ray 
--problem problem.Problem --max-evals=200 --learner RF

conda deactivate

EOF
#-----This part submits the script you just created--------------
chmod +x batch.job
qsub batch.job
\end{verbatim}
 }  
 
 processcp.pl is used in the file runs.cob: 
 {\scriptsize
\begin{verbatim}
#!/usr/bin/perl
#Change this path!
#
# processcp.pl: copy the file processexe.pl to change 
# the proper number of nodes
#

system("cp processexe$ARGV[0].pl processexe.pl");
system("chmod +x processexe.pl");
exit 0;
\end{verbatim}
 } 
 
 The processexe4096.pl is used in the file processcp.pl:
 {\scriptsize
\begin{verbatim} 
 #!/usr/bin/perl
#Change this path!
#
# processexe.pl: process the file exe.pl to change 
# the proper number of nodes
#

$A_FILE = "tmpfile.txt";

$filename1 =  $ARGV[0];
    #print "Start to process ", $filename, "...\n";
    $fname = ">" . $A_FILE;
    $i = 0;
    open(OUTFILE, $fname);
    open (TEMFILE, $filename1);
    while (<TEMFILE>) {
        $line = $_;
        chomp ($line);

        if ($line =~ /system/) {
                print OUTFILE "    system(\"geopmlaunch aprun 
                -n 4096 -N 1 --geopm-ctl=pthread 
                --geopm-report gm.report -- ", 
                "\$filename -m event > tmpoutfile.txt 2>&1\");", "\n";
        } else {
                print OUTFILE $line, "\n";
        }
    }
   close(TEMFILE);
   close(OUTFILE);
   system("mv $A_FILE $filename1");
   system("chmod +x $filename1");
exit 0;
\end{verbatim}
 } 

Notice that the number of threads are set using os.environ["OMP\_NUM\_THREADS"] in ytopt before using geopmlaunch to launch the job. 

The file exe.pl is used to execute the application to measure the energy: 
{\scriptsize
\begin{verbatim}
#!/usr/bin/env perl
#
# exe.pl: average the node energy
#
use Time::HiRes qw(gettimeofday);

$A_FILE = "gm.report";
my $i = 0;
my $j = 0;
my $avg = 0;
foreach $filename (@ARGV) {
 #  print "Start to preprocess ", $filename, "...\n";
    system("geopmlaunch aprun -n 4096 -N 1 --geopm-ctl=pthread 
    --geopm-report gm.report -- $filename -m event 
    > tmpoutfile.txt 2>&1");
    open (TEMFILE, '<', $A_FILE);
    while (<TEMFILE>) {
        $line = $_;
        chomp ($line);
        if ($line =~ /Application Totals/) {
            if ($i == 0) {
                $i = 1;
            } else {
                    $i = 0;
            }
            $j ++;
        }
        if ($i == 1) {
          if ($line =~ /package-energy/) {
                ($v1, $v2) = split(': ', $line);
                chomp ($v2);
                $avg += $v2;
          }
          if ($line =~ /dram-energy/) {
                ($v3, $v4) = split(': ', $line);
                chomp ($v4);
                $avg += $v4;
                $i = 0;
          }
        }
   }
   if ($j != 0) {
        printf("%.3f\n", $avg/$j);
   } else {
        printf("Wrong input file!\n");
   }
   close(TEMFILE);
}
\end{verbatim}
 } 
 
 
\textbf{Summit}: 

For XSBench executed on 4096 nodes, we use the following job scripts to measure the performance for the baseline and autotuning:

\textbf{(1) runs.cob to submit the job for the baseline performance measurement}:
{\scriptsize
\begin{verbatim}
#!/bin/bash

let nnds=4096
#--- process the submit script

#-----This part creates a submission script---------
cat >batch.job <<EOF
# Begin LSF directives
#BSUB -P MED106
#BSUB -J ytopt
#BSUB -o output.log
#BSUB -e output.err
#BSUB -alloc_flags gpumps
#BSUB -W 30
#BSUB -nnodes ${nnds}

module load nvhpc

export OMP_NUM_THREADS=168
jsrun -n ${nnds} -a 6 -g 6 -c 42 -bpacked:42 -dpacked 
../XSBench -m event > out.txt
jsrun -n ${nnds} -a 6 -g 6 -c 42 -bpacked:42 -dpacked 
../XSBench -m event > out2.txt
jsrun -n ${nnds} -a 6 -g 6 -c 42 -bpacked:42 -dpacked 
../XSBench -m event > out3.txt
jsrun -n ${nnds} -a 6 -g 6 -c 42 -bpacked:42 -dpacked 
../XSBench -m event > out4.txt
jsrun -n ${nnds} -a 6 -g 6 -c 42 -bpacked:42 -dpacked 
../XSBench -m event > out5.txt

EOF
#-----This part submits the script you just created--------------
chmod +x batch.job
bsub  batch.job

\end{verbatim}
 }
 
 Based on the five runs, we use the smallest runtime as the baseline.


\textbf{(2) runs.cob to submit the job for autotuning in performance}:

{\scriptsize
\begin{verbatim}
#!/bin/bash

let nnds=4096
#--- process exe.pl to change the number of nodes
./processcp.pl ${nnds}

#-----This part creates a submission script---------
cat >batch.job <<EOF
# Begin LSF Directives
#BSUB -P MED106
#BSUB -W 00:30
#BSUB -nnodes ${nnds}
#BSUB -alloc_flags gpumps
#BSUB -J ytopt
#BSUB -o ytopt.%J.out
#BSUB -e ytopt.%J.err

module load ibm-wml-ce
conda activate yt

python3 -m ytopt.search.ambs --evaluator ray 
--problem problem.Problem --max-evals=200 --learner RF

EOF
#-----This part submits the script you just created--------------
bsub  batch.job

\end{verbatim}
 }  
 
 processcp.pl is used in runs.cob: 
 {\scriptsize
\begin{verbatim}
#!/usr/bin/perl
#Change this path!
#
# processcp.pl: copy the file processexe.pl to change 
# the proper number of nodes
#

system("cp processexe$ARGV[0].pl processexe.pl");
system("chmod +x processexe.pl");
exit 0;
\end{verbatim}
 } 
 
 The processexe4096.pl is used in the file processcp.pl:
 {\scriptsize
\begin{verbatim} 
 #!/usr/bin/perl
#Change this path!
#
# processexe.pl: process the file exe.pl to change 
# the proper number of threads
#

$A_FILE = "tmpfile.txt";

$filename1 =  $ARGV[0];
    #print "Start to process ", $filename, "...\n";
    $fname = ">" . $A_FILE;
    $i = 0;
    open(OUTFILE, $fname);
    open (TEMFILE, $filename1);
    while (<TEMFILE>) {
        $line = $_;
        chomp ($line);

        if ($line =~ /system/) {
                print OUTFILE "    system(\"jsrun -n 4096 -a 6 -g 6 
                -c 42 -bpacked:", $ARGV[1], " -dpacked 
                \$filename -m event > tmpoutfile.txt 2>&1\");", "\n";
        } else {
                print OUTFILE $line, "\n";
        }
    }
   close(TEMFILE);
   close(OUTFILE);
   system("mv $A_FILE $filename1");
   system("chmod +x $filename1");
exit 0;

\end{verbatim}
 } 

The file exe.pl is used to execute the application to measure the performance: 
{\scriptsize
\begin{verbatim} 
#!/usr/bin/env perl

#
# exe.pl: measure the execution time 
#
use Time::HiRes qw(gettimeofday);

$A_FILE = "tmpoutfile.txt";
foreach $filename (@ARGV) {
 #  print "Start to preprocess ", $filename, "...\n";
   $ssum = 0.0;
   $nmax = 1;
   @nn = (1..$nmax);
   for(@nn) {
    system("jsrun -n 4096 -a 6 -g 6 -c 42 -bpacked:21 -dpacked 
    $filename -m event > tmpoutfile.txt 2>&1");
    open (TEMFILE, '<', $A_FILE);
    while (<TEMFILE>) {
        $line = $_;
        chomp ($line);

        if ($line =~ /Runtime/) {
                ($v1, $v2) = split(': ', $line);
                printf("%.3f", $v2)
        }
   }
   close(TEMFILE);
   }
}
\end{verbatim}
 } 

For other three ECP proxy applications such as SWFFT, AMG and SW4lite, their scripts for job submission and processing are similar to these. On both Theta and Summit, just replace "\$filename -m event" for XSBench with the following: 

"\$filename 2 4096" for SWFFT; 

"\$filename -laplace -n 100 100 100 -P 16 16 16" for AMG;

"\$filename loh1/LOH.1-h50.in" for SW4lite.

Because we ran SWFFT, AMG and SW4lite without using any GPU on Summit, just replace "jsrun -n 4096 -a 6 -g 6" in these scripts with "jsrun -n 4096 -a 1 -g 0", the other parts stay the same. If needed, we can add the scripts for SWFFT, AMG and SW4lite to the appendix. 


