
\section{Background, Challenges, and Motivation}

%Considerable literature on autotuning exists. Balaprakash et al.\cite{BD18} surveyed the state of the practice in incorporating autotuned code into HPC applications; the authors highlighted insights from prior work and identified the challenges in advancing autotuning into wider and long-term use. Traditional autotuning methods are built on heuristics that derive from automatically tuned BLAS libraries \cite{ATLAS}, experience \cite{TC02, CH06, GO10}, and model-based methods \cite{Chen05, TH11, BG13, FE17}. At the compiler level \cite{AK18}, ML-based methods are used for automatic tuning of the iterative compilation process \cite{OP17} and tuning of compiler-generated code \cite{TC09, MS14}.
%Autotuning OpenMP codes has gone beyond loop schedules to look at parallel tasks and function inlining \cite{SJ19, KC14, MA11, Rose09}. Recent work on leveraging Bayesian optimization to explore the parameter space search shows the potential for autotuning on CPU systems \cite{WK20, WK21, LS21, RT21} and on GPU systems \cite{MS18, WN21}. Some recent work has used machine learning and sophisticated statistical learning methods to reduce the overhead of autotuning \cite{RB16, MA17, TN18, BQ19}.

Autotuning involves two critical requirements: (1) expression of a search space of implementations or configurations and (2) efficient navigation of the search space for identifying the optimal configuration. To address these two requirements, researchers have developed a number of autotuning frameworks  that interface with application codes, libraries, and compilers to generate code variants and measure their performance \cite{TC02,TH11,HN09,AK14, NR15,ZG15, RH17,CLTune,PG19,KernelTuner,HG20,YTO,WK20,WK21,KF20, KB21}. They presented the expression of a collection of parameters to be tuned and their corresponding possible values, and they generated possible configurations that may or may not be valid for evaluation. 

Two kinds of expressions of search space exist: vector space and tree space. 
Most autotuning frameworks present the search space in a vector space, that is, a fixed number of parameter knobs; these frameworks include OpenTuner \cite{AK14}, CLTune \cite{CLTune}, HalideTuner \cite{ZG15}, Orio \cite{HN09}, KernelTuner \cite{KernelTuner}, ATF \cite{RH17, RS21}, ytopt \cite{YTO, WK20, WK21}, GPTune \cite{LS21}, and Bliss \cite{RT21}. The successor of HalideTuner \cite{AM19} uses tree search to avoid the limitation of a vector search space but uses beam search to explore the space. ProTuner~\cite{HG20} further improves Halide schedule autotuning by replacing beam search with Monte Carlo tree search. The loop autotuner in Telamon also uses Monte Carlo tree search \cite{Telamon}. In the tradition of Halide, every level needs an assigned strategy, and a schedule where not all loops have an assigned strategy is considered incomplete. The viability of autotuning the search space for loop transformations was demonstrated; the approach involves the straightforward representation as either a tree or a directed acyclic graph using mctree \cite{MCTree, KF20, KB21}, and every loop is considered sequential until a pragma is added.

We classify  autotuning frameworks into four categories: (1) enumerate all possible parameter configurations, reject invalid ones, and evaluate the valid ones \cite{KF20}; (2) enumerate only valid configurations~\cite{RH17, RS21};  (3) sample from the set of possible configurations, and reject invalid ones~\cite{HN09, NR15, SJ19} during the search; and (4) sample only valid configurations, and search over them \cite{WK20, WK21}. The ytopt autotuning framework belongs to Category 4, which overcomes the ineffectiveness of Category 3 by generating valid samples and addresses the limitations of Categories 1 and 2, where enumerating all possible configurations can be computationally expensive for large number of parameters. However, the ytopt framework autotuned the applications only on a single computer node. Can we extend the ytopt framework to autotune MPI/OpenMP scientific applications at large scales so that we can identify the best configuration for running these applications on large-scale HPC systems efficiently? This is the main motivation of our work in this paper.
% MK: (4) cannot express unknown validity constraints (compile failure, runtime crash, ...)? Seems quite limiting 

Kruse and Finkel \cite{KF19} implemented a prototype of user-directed loop transformations using LLVM Clang \cite{clang} and Polly \cite{POLL} with additional loop transformation pragmas such as loop reversal, loop interchange, tiling, and array packing in the ECP SOLLVE project \cite{SOLL}. 
Multiple pragmas can be composed even to the same loop and every transformation addresses different and often contradicting concerns, such as maximizing parallelism, spatial and temporal memory locality, but minimizing bandwidth and overhead. Hence there is a need to determine how to efficiently combine them to optimize an application.

In  our recent work \cite{WK20, WK21} an autotuning framework ytopt was developed to leverage Bayesian optimization with four supervised machine learning methods---Random Forests, Gaussian Process Regression, Extra Trees, or Gradient-boosted Regression Trees---to explore the search space and identify more-promising regions\xingfu{, and we found the Random Forests performed the best}. This autotuning framework was used to identify the optimal combination of the Clang loop pragma parameters, with the aim of improving the performance of six PolyBench benchmarks \cite{YP16} and tuning the hyperparameters of a deep learning application MNIST on a single compute node. 

Most of autotuning frameworks mentioned above were for autotuning on a single or a few compute nodes. 
Recently, new autotuning frameworks are emerging for multi-node autotuning.  
For example, GPtune \cite{LS21} autotuned some MPI applications on up to 64 nodes with 2,048 cores with multitask learning using MPI, and Bayesian optimization was applied to increase the energy efficiency of a GPU cluster system \cite{MS18}. Current large-scale HPC systems such as Theta \cite{THETA} at ANL
%of approximately 12 petaflops peak performance at Argonne National Laboratory 
and Summit \cite{SUMMIT} 
%of approximately 200 petaflops peak performance 
at ORNL have  complex system architectures and software stacks with many tunable parameters that may affect the system performance and energy. How can we identify the best combination of these parameters for the best system performance or the lowest system energy consumption? Application developers and users often rely on these systems with the default configurations setup by the vendors to run their applications. How efficiently are these applications executed? 
%Answering these questions is difficult. 
Can we develop a low-overhead framework to autotune large-scale applications for performance and energy on large-scale HPC production systems such as Summit and Theta? 

The answer to these questions is "Yes, we can.'' Specifically, in this paper we demonstrate a new state of practice by applying autotuning approach to optimize performance and energy of hybrid MPI/OpenMP scientific applications on up to 4,096 nodes on these systems. 

%This is the focus of this paper. 

