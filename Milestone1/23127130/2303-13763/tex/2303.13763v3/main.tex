\pdfoutput=1


\documentclass[11pt]{article}


\usepackage[]{coling}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}


\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}

\usepackage[T1]{fontenc}
\usepackage{graphicx}

\usepackage{booktabs} 
\usepackage{amsfonts} 
\usepackage{amsmath} 
\usepackage{multirow}

% table
\usepackage{tcolorbox}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{array} 
\usepackage{makecell}
\usepackage{pifont}
\newcommand{\ccmark}{\ding{51}}%
\newcommand{\xxmark}{\ding{55}}%
\definecolor{OliveGreen}{rgb}{0,0.4,0}
\definecolor{gg}{HTML}{e2f0cb}


\title{Edge-free but Structure-aware: Prototype-Guided \\ Knowledge Distillation from GNNs to MLPs}


\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\usepackage{fdsymbol}

\author{
Taiqiang Wu\affmark[$\diamondsuit$]\thanks{\ \ This work was done when Taiqiang was interning at Tencent. Corresponding authors: Yujiu Yang (yang.yujiu@sz.tsinghua.edu.cn) and Ngai Wong (nwong@eee.hku.hk).} \
Zhe Zhao \affmark[$\spadesuit$] \ 
Jiahao Wang\affmark[$\diamondsuit$] \\
\textbf{Xingyu Bai}\affmark[$\clubsuit$] \ \textbf{Lei Wang}\affmark[$\heartsuit$] \
\textbf{Ngai Wong}\affmark[$\diamondsuit$] \
\textbf{Yujiu Yang}\affmark[$\clubsuit$]
\\
\affmark[$\diamondsuit$]The University of Hong Kong \ \affmark[$\clubsuit$]Tsinghua University \\
\affmark[$\heartsuit$]Ping An Technology \
\affmark[$\spadesuit$]Tencent AI Lab
\\
{\tt takiwu@connect.hku.hk} 
% \ {\tt yang.yujiu@sz.tsinghua.edu.cn} \ {\tt nwong@eee.hku.hk}
}



\begin{document}
\maketitle

\begin{abstract}
% 15--250 words.
Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic. 
However, conventional MLP learning relies almost exclusively on graph nodes and fails to effectively capture the graph structural information. 
Previous methods address this issue by processing graph edges into extra inputs for MLPs, but such graph structures may be unavailable for various scenarios. 
To this end, we propose Prototype-Guided Knowledge Distillation~(PGKD), which does not require graph edges~(edge-free setting) yet learns structure-aware MLPs. 
Our insight is to distill graph structural information from GNNs. 
Specifically, we first employ the class prototypes to analyze the impact of graph structures on GNN teachers, and then design two losses to distill such information from GNNs to MLPs. 
Experimental results on popular graph benchmarks demonstrate the effectiveness and robustness of the proposed PGKD.
\end{abstract}

\input{sections/1-Introduction}
\input{sections/2-Related}
\input{sections/2.5-Preliminary}
\input{sections/3-Method}
% \clearpage
\input{sections/4-Experiments}

\input{sections/5-Analysis}
% \input{sections/6-Related}
% \input{sections/2-Related}
\input{sections/6-Conclusion}

\input{main.bbl}
% \bibliography{ref}

\input{sections/7-Appendix}

\end{document}
