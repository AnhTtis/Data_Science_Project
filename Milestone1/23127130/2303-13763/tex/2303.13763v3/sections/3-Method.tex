
\section{Methodology}
% In this section, we firstly study the impact of graph structures~(graph edges) on GNNs.
% Based on that, we design the PGKD.
% to transfer the graph structural information from GNNs to MLPs.


\subsection{Impact of Graph Structures on GNNs}
\label{impact}
% The graph structure~(graph edges) plays an import role in Graph Neural Networks~(GNNs).
% During message-passing, the information is aggregated along the edges.
% To analyze the impact of graph structures on GNNs,
% We categorize the graph edges into Intra-class edges and Inter-class edges based on whether the connected nodes are from same class or not, respectively.
% Here we study the impact of graph structures~(graph edges) on GNNs. Based on that, PGKD is subsequently derived. We first categorize the graph edges into intra-class and inter-class edges based on whether the connected nodes are from the same or different classes, respectively.

\paragraph{Intra-class Edges.} The propagation mechanisms in GNNs are the optimal solution of optimizing a feature fitting function with a graph Laplacian regularization term \cite{DBLP:conf/www/ZhuWSJ021,DBLP:conf/cikm/0001L0LTS21}.
The Laplacian regularization guides the smoothness of $\mathbf{H}$ over $\mathcal{G}$, where connected
nodes share similar features.
Therefore, the homophily property for nodes from the same class can be captured.
As shown in Table \ref{tab:intr_dis}, the average connected node distance for GNNs is much smaller than that in initial node features.



\paragraph{Inter-class Edges.} For inter-class edges, the nodes connected belong to different classes, which is known as heterophily.
% Such edges determine the pattern of class distances.
We define the class distance as the distance between class prototypes, and the class prototype is the prototypical vector for all nodes from the same class.
The inter-class edges determine the pattern of class distances: for class $i,j,k$, if the inter-class edges between class $i,j$ are more than those between class $i,k$, then the class distance of $i,j$ would be smaller.
% Table \ref{tab:inter} shows the Spearman correlation between class distances and Inter-class edges quantity, which proves our conclusion.
From Table \ref{tab:inter}, we can infer that two classes would be closer if more inter-class edges existed between them in GNNs.
% \input{tables/inter_spear}

% \input{tables/intra_dis}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/Model.pdf}
	\caption{
	Overview of the proposed PGKD.
    The input is the whole graph for the GNN teacher but only the corresponding graph nodes for the MLP student.
    The circles mean the vectors for graph nodes and the same color denotes the same class. 
    After getting the class prototypes, we design inter-class and intra-class loss to distill the graph structural information from the GNN teacher to the MLP student.
    }
	\label{framework}
	% \vspace{-1em}
\end{figure*}

\subsection{Prototype-Guided Knowledge Distillation}
Since the impact of graph structures~(i.e. graph edges) on GNNs has been studied, the next goal is to distill such graph structural information from GNNs to MLPs.
In PGKD, we design two losses to mimic the impact of intra-class and inter-class edges via class prototypes.
Figure \ref{framework} shows an overview of PGKD.



\paragraph{Prototype Strategy.}
To get the class prototype, the key is to label all nodes.
In the inductive scenario, we employ the corresponding ground-truth label $\mathbf{Y}^{L}$ of training data for both GNN teachers and MLP students.
However, in transductive scenarios, applying the ground-truth label $\mathbf{Y}$ would lead to label leaking.
Hence, we employ the predicted label of the GNN teacher to label the MLP nodes.
After grouping the nodes via the labels, we define the class prototypes as the mean vectors of all nodes from the same class.
Henceforth, we use $(\mathbf{P}^{t}_1, ..., \mathbf{P}^{t}_K)$ and $(\mathbf{P}^{s}_1, ..., \mathbf{P}^{s}_K)$ to denote the GNN and MLP class prototypes, respectively.

\paragraph{Intra-class Loss.} The intra-class edges in GNNs capture the homophily property for nodes from the same class.
One intuitive idea is to draw closer any two nodes from the same classes in the edge-free setting.
However, this strategy has \emph{two drawbacks}: 1) It is easily influenced by the outliers, viz. noisy points; and 2) A high complexity of $O(n^2)$ where $n$ denotes the quantity of a given class. To tackle these, we design the intra-class loss analogous to InfoNCE~\cite{DBLP:journals/corr/abs-1807-03748}, whose goal is to \textbf{draw a selected node closer to its corresponding prototype}.
For node $i$ and its given label $c_i$, the loss is calculated as:
\begin{equation}
    \mu_{i} = [d_1(h_i,\mathbf{P}^{s}_{1}), ..., d_1(h_i,\mathbf{P}^{s}_{K})]
\end{equation}
\begin{equation}
    \mathcal{L}_{intra} = \Phi_{1}(\text{Softmax}(-\mu_i/\tau_1),c_i) ,
\end{equation}
% \begin{equation}
%     \mathcal{L}_{intra} = \Phi_{1}(
%     \frac{ \exp(-d_1(h_i,\mathbf{P}^{s}_{c_i})/\tau_{1}) }{ \sum^{K}_{j=1} \exp(-d_1(h_i,\mathbf{P}^{s}_{j})/\tau_{1})}
%     ,c_i) ,
% \end{equation}
where $\Phi_{1}$ denotes loss functions such as cross-entropy loss, $\tau_{1}$ denotes the temperature parameter, $\mathbf{P}^s$ denotes the prototypes, and $d_1$ denotes the distance function.
The class prototypes are less sensitive to noisy points.
Meanwhile, the compute is decreased to $O(n \times K)$ where typically $K \ll n$.

% is analogous to the InfoNCE loss \cite{DBLP:journals/corr/abs-1807-03748}





\paragraph{Inter-class Loss.} The inter-class edges determine the pattern of class distances.
% The more Inter-class edges between two classes, the near the corresponding class prototypes would be.
The class prototypes would be closer if more inter-class edges connect these two classes.
% However, without graph edges as input, we cannot model the pattern of class distances in MLPs.
However, we cannot count the edges in an edge-free setting.
One solution is to force the student to \textbf{mimic the distance pattern of GNN teachers}.
Aligning the distances directly cannot work, since the node representations from teacher and student lie in different semantic spaces. Therefore, we compute the relative distance in PGKD.
% as the knowledge transferred from GNNs to MLPs.
% Let $(\mathbf{P}^{t}_1, ..., \mathbf{P}^{t}_K)$ and $(\mathbf{P}^{s}_1, ..., \mathbf{P}^{s}_K)$ denote the class prototypes from GNN and MLP, 
The loss to align GNN prototype $\mathbf{P}^{t}_i$ and MLP prototype $\mathbf{P}^{s}_i$ is calculated by:
\begin{equation}
    \sigma^{t}_i =[ d_2(\mathbf{P}^{t}_{i},\mathbf{P}^{t}_{1}), ... , d_2(\mathbf{P}^{t}_{i},\mathbf{P}^{t}_{K})]
\end{equation}
\begin{equation}
    \sigma^{s}_i =[ d_2(\mathbf{P}^{s}_{i},\mathbf{P}^{s}_{1}), ... , d_2(\mathbf{P}^{s}_{i},\mathbf{P}^{s}_{K})]
\end{equation}
\begin{equation}
    \mathcal{L}_{inter} = \Phi_{2} (\text{Softmax}(\sigma^{s}_i/\tau_2), \text{Softmax}(\sigma^{t}_i/\tau_2)),
\end{equation}
where $\Phi_{2}$ denotes the similarity function for two distributions such as KL-divergence, $\tau_{2}$ denotes the temperature parameter, $\mathbf{P}^t$ and $\mathbf{P}^s$ denote the prototypes, and $d_2$ denotes the distance function.



\paragraph{Overall Target.} In PGKD, the overall loss function is:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{label} + \mathcal{L}_{kd} + \lambda_{1} \mathcal{L}_{intra} + \lambda_{2} \mathcal{L}_{inter},
\end{equation}
where $\mathcal{L}_{label}$ and $\mathcal{L}_{kd}$ are the original loss for classification and vanilla logit-base KD loss, respectively.
$\lambda_{1}$ and $\lambda_{2}$ are hyperparameters.
This way, the baseline GLNN is a \emph{special case} of PGKD when both $\lambda_{1}$ and $\lambda_{2}$ are zeroed.