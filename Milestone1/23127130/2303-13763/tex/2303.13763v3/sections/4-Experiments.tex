\input{tables/main_res}

\section{Experiments}

% This section presents an empirical study of PGKD on several popular graph benchmarks.
% This section, we perform an empirical study of PGKD on several popular graph benchmarks.
% We further do the ablation studies.

\subsection{Datasets}
% \input{tables/dataset}

To evaluate the performance of PGKD, we consider seven popular benchmarks, including five homophilous graph datasets, namely, Cora \cite{DBLP:journals/aim/SenNBGGE08}, Citeseer \cite{DBLP:journals/aim/SenNBGGE08}, Pubmed \cite{namata2012query}, A-computer \cite{DBLP:journals/corr/abs-1811-05868}, and Arxiv \cite{DBLP:conf/nips/HuFZDRLCL20}, and two heterophilous graph datasets, namely, Penn94 \cite{DBLP:conf/nips/LimHLHGBL21} and Twitch-gamer \cite{DBLP:conf/nips/LimHLHGBL21}.
In particular, Twitch-gamer and Arxiv are \emph{large datasets} with more than 160,000 nodes (cf. Appendix \ref{Append:dataset} for details of all datasets).
% Table \ref{tab: dataset} shows the dataset details.


We split these datasets for train/validation/test following GLNN \cite{DBLP:conf/iclr/ZhangLSS22} for fair comparison.
For the metric, we report the average accuracy on test data over five runs with different random seeds.

% \input{tables/dataset}


\subsection{Implementation}
\paragraph{GNN Teacher.} To evaluate the ability on different backbones, we select four popular GNNs as the teacher model: GraphSAGE \cite{DBLP:conf/nips/HamiltonYL17}, GAT \cite{DBLP:journals/corr/abs-1710-10903}, GCN \cite{DBLP:conf/iclr/KipfW17} and APPNP \cite{DBLP:conf/iclr/KlicperaBG19}, and perform experiments under both transductive and inductive settings.



\paragraph{Baselines.} For baselines, we \textbf{do not} compare with the regularization methods since these methods utilize the graph edges as \textbf{extra inputs}.
In real-world applications, these graph edges may be unavailable (see Appendix \ref{Append:necess} for specific scenarios).
Therefore, we conduct all experiments in the \textbf{edge-free} setting.
For fairness, we select the edge-free GLNN \cite{DBLP:conf/iclr/ZhangLSS22} as the baseline, which adapts vanilla logit-base KD from GNNs to MLPs.

\paragraph{Hyper-parameters.} We distill the two-layer GNN teacher to MLP student with two layers~(on Cora, Citeseer, and A-computer) or three layers~(on Pumbed, Penn94, Arxiv, and Twitch-gamer).
For PGKD, we employ grid search to train the MLPs, where $\lambda_1$ is searched in $\{0.1, 0.2, 0.4\}$ and $\lambda_2$ in $\{0.05, 0.1\}$.
We set $\tau_1$ and $\tau_2$ as 1 and 10, respectively.
The hidden state dimension is 128 for both GNNs and MLPs.
In all the datasets, the MLPs are trained for 500 epochs with early stopping.



\subsection{Main Results}


We conduct experiments on seven benchmarks and select SAGE as GNN teachers for small datasets~(Cora, Citeseer and A-computer) and GCN for large datasets~(Penn94, Pubmed and Twitch-gamer).
Meanwhile, we reproduce GLNN from its official codes.
Table \ref{tab: main_res} reports the accuracy results.
Several observations are in place:
\begin{itemize}
    \item  PGKD outperforms GLNN on all seven benchmarks with higher average scores under both transductive and inductive settings, thus demonstrating the effectiveness of PGKD in capturing graph structural information for the MLPs.
    In particular, PGKD achieves 76.35\% on Pubmed under inductive setting, 1.86\% higher than GLNN.
    PGKD can even outperform GNN teachers on some datasets~(Citeseer and Pubmed). 
% \item

% \textbf{2)}
\item The standard deviations of PGKD are smaller than GLNN for almost all datasets, showing the stability and robustness of PGKD.
    For instance, PGKD gets 0.39\% on A-computer under transductive setting, approximately 3$\times$ smaller than the 1.04\% of GLNN.
% \end{enumerate}

% \textbf{3)}
\item Particularly, for \emph{large dataset}~(Arxiv and Twitch-gamer), PGKD statistically significantly outperforms GLNN.
Specifically, the p-values for Arxiv and Twitch-gamer are 0.0001/0.04 and 0.0001/0.04~(transductive/inductive), respectively.
Such results on the large datasets prove the effectiveness of PGKD.

\end{itemize}



\subsection{Ablation Studies}
To better understand PGKD, we conduct ablation experiments on intra-class loss and inter-class loss.
Without loss of generality, we select SAGE, GAT, GCN, APPNP as GNN teachers and compare the performance on Citeseer under inductive setting and Cora under transductive setting.

% \input{tables/abalation_merge}

Table \ref{tab:abaltion} \& \ref{tab:abaltion2} show the experiment results, wherein the performance drops when either intra-class loss or inter-class loss is removed, indicating that both types of information are crucial. In general, removing the intra-class loss would lead to a larger drop than the inter-class loss under the transductive setting, but a smaller drop under the inductive setting.
Moreover, it is interesting to see that PGKD with one loss exclusively would perform worse than GLNN, but is better than GLNN with two losses together.
For example, PGKD gets 68.62\% and 68.23\%~(APPNP as GNN teacher) on Citeseer with one loss exclusively, which are lower than 69.23\% of GLNN. However, PGKD would get a higher 69.78\% than GLNN with both losses. Adopting SAGE as the GNN teacher on Citeseer also leads to a similar observation. Such phenomenon indicates that simultaneously considering both intra-class and inter-class information is crucial for an effective MLP training.

\input{tables/abalation}

\input{tables/abalation2}