\section{Related Work}

\subsection{Distilling GNNs into MLPs}
Knowledge Distillation~(KD) is among the mainstream approaches to transferring knowledge from GNNs to MLPs \cite{lu2024adagmlp}.
The key insight is to learn a student model by mimicking the behaviors of the teacher model.
GLNN \cite{DBLP:conf/iclr/ZhangLSS22} utilizes the vanilla logit-based KD \cite{DBLP:journals/corr/HintonVD15}, which is edge-free but fails to capture the graph structural information.
To address this issue, one way is to process graph edges into extra inputs for MLPs, such as the adjacency matrix \cite{DBLP:journals/corr/abs-2210-09609} or the node positions \cite{DBLP:journals/corr/abs-2208-10010}.
Another line is to treat graph structural information as a regularization term, where the nodes connected by edges should be closer \cite{DBLP:journals/corr/abs-2210-02097,DBLP:journals/corr/abs-2106-04051}.
Nonetheless, graph structure may be unavailable for some reasons, including privacy problems, commercial considerations, and missing/corrupted
edges.
In this work, we propose PGKD, the first method to distill GNNs into structure-aware MLPs without graph structure.
% show it is possible to effectively distill the graph structural knowledge from GNNs to MLPs in the edge-free setting.

% However, 
% previous methods require graph edges as extra input and overlook the information from GNN teachers.
% However, graph structure may be unavailable for some scenarios, such as federated graph learning.
% In this paper, we try to distill the graph structural knowledge from GNNs to MLPs in the edge-free setting.


\subsection{Prototype in GNNs}
Prototypical Networks \cite{DBLP:conf/nips/SnellSZ17} have been widely applied in few-shot learning and metric learning on classification tasks \cite{DBLP:conf/nips/HuangZ20}.
The basic idea is that there exists an embedding in which points cluster around a single prototype representation for each class.
In GNNs, class prototypes are widely employed for node classification \cite{DBLP:conf/iclr/SatorrasE18,DBLP:conf/aaai/YaoZWJWHCL20,DBLP:conf/kdd/WangWGG21,dong2022protognn}, graph matching \cite{DBLP:conf/mm/WangLHB20}, and graph explanation \cite{DBLP:journals/corr/abs-2210-17159,DBLP:conf/nips/YingBYZL19,zhang2022protgnn,DBLP:conf/nips/Seo0P23}.
The class prototypes are usually defined as simple as mean vectors.
In this work, we design extra losses for MLP students via prototypes to distill the graph structural information from GNN teachers.
% To the best of our knowledge, we are the first to adopt prototypes for GNN distillation.
To the best of our knowledge, this is the \emph{first-time} utilization of prototypes for distillation from GNNs to MLPs.
