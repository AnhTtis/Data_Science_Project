\section{Introduction}

% Necessary for GNN2MLP
% Graph Neural Networks~(GNNs) have recently become very popular in handling non-Euclidean structural data and have achieved start-of-the-art performance across graph machine learning tasks, particularly for the node classification task \cite{DBLP:conf/iclr/KipfW17, DBLP:journals/corr/abs-1710-10903, DBLP:conf/nips/HamiltonYL17}. 
Graph Neural Networks~(GNNs) are gaining importance in handling structural data and have achieved start-of-the-art performance across graph machine learning tasks, particularly for the node classification task \cite{DBLP:conf/iclr/KipfW17,DBLP:journals/corr/abs-1710-10903,DBLP:conf/nips/HamiltonYL17}. 
The message-passing architecture, which aggregates the information from neighborhoods, guarantees the powerful representation ability in GNNs.
However, such a neighborhood fetching operation also leads to high latency \cite{DBLP:conf/kdd/JiaLYYLA20}, making it challenging to apply GNNs for real-world applications.
Meanwhile, MLPs are free from the GNN latency problem without message passing, but perform poorly in graph tasks due to the lack of graph structural information.
% Moreover, graph structure may be unavailable for some reasons, including privacy problems, commercial considerations, and missing/corrupted
% edges (please refer to Appendix \ref{Append:necess} for more details).
% Therefore, how to train low-latency MLPs with high accuracy as GNNs on graph tasks becomes a hot topic.
% Therefore, how to train low-latency MLPs to have competitive accuracy as GNNs becomes a hot topic.
Therefore, it is challenging to train low-latency MLPs to have competitive accuracy as GNNs on graph tasks.

\input{tables/intro_table}

To achieve this goal, one mainstream approach is to distill the knowledge from GNNs to MLPs.
GLNN \cite{DBLP:conf/iclr/ZhangLSS22} employs the vanilla logit-based knowledge distillation~(KD) to train MLP students from GNN teachers.
Despite the KD target, the MLPs in GLNN still suffer from the lacking of graph structural information, since MLPs rely exclusively on the graph node features.
To inject the graph structural information, previous methods \cite{DBLP:journals/corr/abs-2106-04051,DBLP:journals/corr/abs-2210-02097} employ additional regularization terms on the final node representations based on the graph structures.
For each node, the key insight is to draw closer the distance between the selected node and its $i$-hop connected neighbors while pushing farther for other nodes.
However, this strategy has \textbf{two issues}: 1) Graph edges are required as an auxiliary input, but graph structures may be \textbf{unavailable} for MLP students for various reasons, including privacy problems, commercial considerations, and missing/corrupted edges (please refer to \ref{Append:necess} for details); 2) Such prior knowledge, which the regularization terms rely on, is \textbf{irrelevant} to the GNN teachers.
Therefore, how to distill GNN teachers into structure-aware MLP students subject to the unavailability of graph edges~(viz. an edge-free setting) becomes an important topic.
% Intuitively, we can transfer the graph structural information from GNN teachers to MLP students.
We thus ask: \textbf{What is the impact of graph structures~(i.e. graph edges) on GNNs? 
Can we distill such graph structural information from GNNs to MLPs~so that we can get structure-aware MLPs in an edge-free setting?}
% MLPs can be structure-aware without graph edges as extra input)?}
% MLPs can be structure-aware without graph edges as extra input




To answer this, we first analyze the impact of graph structures on GNNs.
We categorize the graph edges into \textbf{intra-class} and \textbf{inter-class} edges, where the nodes connected by the edge are of the same and different classes, respectively.
The intra-class edges enforce the local smoothness by constraining the learned representations of two connected nodes to become similar, so that the homophily property for nodes from the same class is captured \cite{DBLP:conf/www/ZhuWSJ021}.
For the inter-class edges, we define the class prototype for each class (viz. a typical embedding vector of a given class \cite{DBLP:conf/nips/SnellSZ17}) and count the distance between any two prototypes.
Statistical analysis shows that the distance between two class prototypes would be shorter if more inter-class edges existed between these two classes in GNNs.
In short, the inter-class edges determine the pattern of distances between these class prototypes and thus contribute to the classification performance \cite{DBLP:conf/aaai/ChenLLLZS20,wang2022distance}.
% More details can be found in section \ref{impact}.



% 考虑加字母来辅助解释
Based on the analysis,
our next goal is to distill such graph structural information from GNNs to MLPs in an edge-free setting.
In this paper, we propose Prototype-Guided Knowledge Distillation~(PGKD), including two extra alignment losses for MLPs based on class prototypes to mimic the impact of graph structures~(i.e. graph edges) on GNNs.
In PGKD, we first design a prototype strategy to get all class prototypes for both GNN teachers and MLP students.
To distill the inter-class distance information, we design a novel intra-class loss to align the MLP prototypes with GNN prototypes.
Considering the intra-class edges that enforce the local smoothness, one intuitive idea is to draw closer any two nodes from the same classes.
However, such a strategy is easily influenced by the outliers, viz. noisy nodes, and its computing complexity is as high as $O(n^2)$ where $n$ denotes the quantity of a given class.
To address the issues, we propose a novel intra-class loss, whose goal is to draw a selected node closer to its corresponding class prototype. 
The class prototypes are less sensitive to noisy points. 
Meanwhile, the computing complexity decreases to $O(n \times K)$ where $K$ is the number of classes, and typically $K \ll n$.
As shown in Table \ref{tab:compare_method}, PGKD is the \textbf{first} method to distill GNNs into structure-aware MLPs in an edge-free setting.


We perform experiments on popular graph benchmarks in both transductive and inductive settings.
We also conduct extensive ablation studies and analyses on PGKD.
Empirical results demonstrate the effectiveness and robustness of PGKD.
% Meanwhile, we prove the robustness of PGKD by adding noise to the node features.
In short, our main contributions are:
\begin{itemize}
\item
% \textbf{1)}
We analyze the impact of graph structures on GNNs by categorizing the edges into intra-class edges and inter-class edges, thus providing a deeper understanding of GNNs.
% which is helpful to understand GNNs better.
\item 
% \textbf{2)}
We propose PGKD based on the analysis, which is the \emph{first-ever} method to distill GNN teachers into structure-aware MLP students in the edge-free setting.
% which effectively distills the graph structural knowledge from GNNs to MLPs in an edge-free setting.
\item
% \textbf{3)}
We evaluate PGKD on various graph benchmarks and demonstrate its effectiveness and robustness.
Thanks to the graph structural information via distillation, PGKD shows a strong denoising ability when adding noise to the node features.
\end{itemize}
