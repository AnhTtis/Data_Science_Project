% \clearpage
\appendix
\section{Appendix}

\subsection{Datasets}
\label{Append:dataset}


\input{tables/dataset}


The details for datasets are shown in Table \ref{tab: dataset}.
In particular, Arxiv and Twitch-gamer are two large datasets with more than 160,000 nodes.

\subsection{Impact of Inductive Split Ratio}
To evaluate the ability for less observed data under inductive setting, we conduct experiments under different split ratios, defined as the ratio $|\mathcal{V}^{U}_{ind}|/|\mathcal{V}^{U}|$.
A larger split ratio means less observed unlabeled data during training and more inductive unlabeled data for test~(cf. Section \ref{setting_intro}).
% Please refer to Section \ref{setting_intro} for more details about inductive setting.
As shown in Figure \ref{ratio}, the performance of the GNN teacher is not monotonically decreasing since the way to split graph~(i.e. the edges to remove) is also vital as the number of nodes for training.
PGKD outperforms GLNN and GNN under all split ratios.
Also, the performance of PGKD is more stable than GLNN.
This proves that PGKD, explicitly capturing the graph structural information, is robust and effective under different inductive split ratios.

% split
\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.85\textwidth]{figures/ratio.pdf}
	\caption{
	The performance of GNN teacher, distilled MLP students via GLNN and PGKD under \textit{inductive} setting with different split ratios.
    \textbf{Left}: \textbf{Citeseer} dataset and SAGE as the GNN teacher.
    \textbf{Right}: \textbf{Pubmed} dataset and GCN as the GNN teacher.
    % We select GCN as GNN teacher and perform experiments on \textbf{Pubmed} dataset.
    }
	\label{ratio}
	% \vspace{-0.5em}
\end{figure*}

\subsection{Necessity for Edge-free setting}
\label{Append:necess}

For distillation from GNNs to MLPs, the edge-free setting means that edge information is not available for the distillation process.
% The edge-free setting is necessary under the following scenarios.

\textbf{Goal}: the GNN teacher is trained by group $\mathcal{A}$, but the MLP student is from group $\mathcal{B}$. 
They need to distill the ability of the GNN teacher on one task to the MLP student.

\textbf{Edge-free setting}: the node features and corresponding GNN outputs are shared between group $\mathcal{A}$ and group $\mathcal{B}$ but the edge information is not shared.

The reasons are as follows:

\begin{itemize}
 

\item \textbf{privacy problem}: graph edges involve some privacy data and may be authorized for group $\mathcal{A}$ only, such as the edges from social relation graphs.
   
\item \textbf{commercial consideration}: graph edges can be employed for other tasks, but group $\mathcal{A}$ wants to just share the ability of one task. 
For example, graph edges from custom-product graphs can be used for custom-custom social recommendations and product-product recommendations. 
When group $\mathcal{A}$ only wants to share the ability of custom-custom social recommendations, they would not share the edge information in case of ability leaking.

\item \textbf{missing/corrupted edges}: GNN teacher was trained a long time before and graph edges are missing or corrupted.

\end{itemize}

\input{tables/prototype_entropy}

\input{tables/hyper-sen1}

\input{tables/hyper-sen2}

\subsection{More strategy for prototypes}
\label{Append: strategy4proto}

In this paper, we select the prototypes as simple as the mean vectors.
For a group of vectors $(V_1, V2,...,V_n)$, the class prototype is defined as: 
$P:= \frac{\sum{V_1,V_2,...,V_n}}{n}$. 
For more comparison, we add an entropy-based approach to get prototypes.
The entropy-based prototype is defined as: 
\begin{equation}
    P_{\text{entropy}}:= \sum(w_1V_1,w_2V_2,...,w_nV_n)
\end{equation}
and $(w_1,w_2,...,w_n):=\text{Softmax}(\mathbb{E}(logit_1),\mathbb{E}(logit_2),...,\mathbb{E}(logit_n))$
where $\mathbb{E}$ denotes the entropy function and $logit_i$ denotes the output logit of node $i$.
The results are shown in Table \ref{tab:prototype}.



Based on the results, we can see that the entropy-based prototype still outperforms GLNN but is slightly worse than the original prototype strategy, demonstrating the robustness of our method.


\subsection{Hyperparameter sensitivity analysis}
\label{Append: para-sensti}

There are four hyperparameters in PGKD, namely $\lambda_1$, $\lambda_2$, $\tau_1$, and $\tau_2$.
We perform hyperparameter sensitive analyses on Citeseer (GraphSAGE as the teacher and under transductive setting).



As shown in Table \ref{tab:hyper_sen1} and \ref{tab:hyper_sen2}, We can find that PGKD always outperforms GLNN~(80.22) as $\lambda_1$, $\lambda_2$, $\tau_1$, and $\tau_2$ change.
Also, we can find that PGKD is more sensitive to $\tau_1$ and $\lambda_1$.
Intuitively, the MLP students lack the GNN aggregation operations and thus the representations are more dispersed (also refer to Figure \ref{node_visualize}). 
The intra-class loss would make the representations more clustered, which is beneficial for classification tasks.

\subsection{Comparison with GLNN}
\label{Appen: diff_GLNN}

Compared to GLNN, PGKD is novel in terms of motivation, methodology and contribution.
In summary, the novelty of PGKD is threefold:

\begin{enumerate}
    \item \textbf{Motivation}: GLNN is edge-free but \textbf{not} structure-aware. 
    PGKD is both edge-free and structure-aware. 
    This makes PGKD fundamentally different from GLNN, both in theory and in practice.
    \item \textbf{Methodology}: We first analyze the impact of graph edges on GNNs. 
    Then we design two novel losses based on our analyses. 
    \item \textbf{Contribution}: i) To the best of our knowledge, we are the \textbf{first} to study the impact of graph structures on GNNs by dividing the edges into intra-class edges and inter-class edges. 
    The findings provide a deeper understanding of GNNs. 
    ii) PGKD is the \textbf{first} method to distill GNN teachers to structure-aware MLP students under the edge-free setting. 
    iii) We perform comprehensive experiments and ablation studies. 
    The empirical results faithfully demonstrate the effectiveness and robustness of PGKD.
\end{enumerate}

Moreover, GLNN can be viewed as \emph{a special case} of proposed PGKD.
The extra two losses help MLP students learn graph structural information from GNN teachers.
Therefore, PGKD can get structure-aware MLPs in the edge-free setting.
