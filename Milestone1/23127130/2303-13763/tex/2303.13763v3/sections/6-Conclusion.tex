\section{Conclusion}

% In this work, we propose PGKD to distill the knowledge from high-accuracy GNNs to low-latency MLPs.
% The distillation process is edge-free and the learned MLP students are structure-aware.
% Firstly, we analyze the impact of graph structure~(graph edges) on GNNs.
% Specifically, we categorize the graph edges into Intra-class edges and Inter-class edges and study their impact, respectively.
% Based on the analysis, we design two corresponding losses via class prototypes to transfer the graph structural knowledge from GNNs to MLPs.
% Experiments on popular benchmarks demonstrate the effectiveness of our proposed PGKD.
% Further analysis indicate that PGKD is robust to noisy node features and performs well in different training settings.

% For future work, we would consider to apply PGKD to other graph tasks other than node classification.
% Moreover, generating the prototypes basing on the node representations rather than the class labels would be another interesting topic.

A novel PGKD scheme has been proposed to distill the knowledge from high-accuracy GNNs to low-latency MLPs, wherein the distillation process is edge-free and the learned MLP students are structure-aware. 
Specifically, we analyze the impact of graph structure~(graph edges) on GNNs and categorize them into intra-class and inter-class edges. 
Two corresponding losses via class prototypes are designed to transfer the graph structural knowledge from GNNs to MLPs.
Experiments on popular benchmarks demonstrate the effectiveness of PGKD.
Additionally, we show PGKD is robust to noisy node features, and performs well under different training settings.


For future work, PGKD can be generalized to other graph tasks beyond node classification.
The key is how to generate the prototypes.
One possible direction is to generate prototypes utilizing node representations rather than class labels.
After that, we can distill the knowledge from GNNs to MLPs based on these prototypes for the downstream tasks, including recommendation\cite{wei2023llmrec}, question answering \cite{park2023graph}, and reasoning \cite{luo2023reasoning}.

\section*{Acknowledgements}
We thank all anonymous reviewers for their constructive feedback on improving our paper.
% We thank
This work was supported in part by the Theme-based Research Scheme (TRS) project T45-701/22-R, and in part by the General Research Fund (GRF) Project 17203224, of the Research Grants Council (RGC), Hong Kong SAR.

% \clearpage
\section*{Limitations}
In PGKD, we adopt the class prototypes to capture graph structural information for MLPs in an edge-free setting.
Subsequently, PGKD requires slightly more computing cost compared to the baseline GLNN.
Meanwhile, the gap between the MLP learned by PGKD and its teacher GNN under the inductive setting is larger than that under the transductive setting, especially on Cora and Penn94 datasets.
More effort to improve the performance under the inductive setting is required.


\section*{Statement of Ethics}
This paper does not introduce a new dataset, nor does it leverage any personal data.