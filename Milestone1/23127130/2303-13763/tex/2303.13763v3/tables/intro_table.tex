\begin{table}[!t]

\centering
\resizebox{\columnwidth}{!}
{%
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Methods} & \textbf{Edge-free} & \textbf{Structure-aware} \\
        \midrule
        Vanilla KD~(GLNN) & \ccmark & \xxmark \\
        Regularization-based & \xxmark & \ccmark \\
        \cellcolor{gg}{PGKD~(ours)} & \cellcolor{gg}{\ccmark} & \cellcolor{gg}{\ccmark} \\
        \bottomrule
    \end{tabular}
}
\caption{
Comparison among different methods to distill GNNs into MLPs.
    \textbf{Edge-free} denotes whether graph edges are employed as extra inputs during distillation.
    \textbf{Structure-aware} denotes whether the learned MLPs are aware of the graph structural information.
    % To the best of our knowledge, PGKD is the first distillation method to learn structure-aware MLPs from GNNs under edge-free setting.
    % To the best of our knowledge, WID is the first distillation method without any alignment loss and directly transfers the knowledge by weight inheritance.
    }
\label{tab:compare_method}
\end{table}