\section{Conclusion}

% In this work, we propose PGKD to distill the knowledge from high-accuracy GNNs to low-latency MLPs.
% The distillation process is edge-free and the learned MLP students are structure-aware.
% Firstly, we analyze the impact of graph structure~(graph edges) on GNNs.
% Specifically, we categorize the graph edges into Intra-class edges and Inter-class edges and study their impact, respectively.
% Based on the analysis, we design two corresponding losses via class prototypes to transfer the graph structural knowledge from GNNs to MLPs.
% Experiments on popular benchmarks demonstrate the effectiveness of our proposed PGKD.
% Further analysis indicate that PGKD is robust to noisy node features and performs well in different training settings.

% For future work, we would consider to apply PGKD to other graph tasks other than node classification.
% Moreover, generating the prototypes basing on the node representations rather than the class labels would be another interesting topic.

A novel PGKD scheme has been proposed to distill the knowledge from high-accuracy GNNs to low-latency MLPs, wherein the distillation process is edge-free and the learned MLP students are structure-aware. 
Specifically, we analyze the impact of graph structure~(graph edges) on GNNs and categorize them into intra-class and inter-class edges. 
Two corresponding losses via class prototypes are designed to transfer the graph structural knowledge from GNNs to MLPs.
Experiments on popular benchmarks demonstrate the effectiveness of PGKD.
Additionally, we show PGKD is robust to noisy node features, and performs well under different training settings.

For our future work, PGKD will be generalized to other graph tasks beyond node classification. 
Another interesting direction will be to generate prototypes utilizing node representations rather than class labels.


\clearpage
\section*{Limitations}
In PGKD, we adopt the class prototypes to capture graph structural information for MLPs in an edge-free setting.
Subsequently, PGKD requires slightly more computing cost compared to the baseline GLNN.
Meanwhile, the gap between the MLP learned by PGKD and its teacher GNN under the inductive setting is larger than that under the transductive setting, especially on Cora and Penn94 datasets.
More effort to improve the performance under the inductive setting is required underway.
