\begin{table}[!ht]
\centering
%\tableindent 
\resizebox{0.9\columnwidth}{!}
{%
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Methods} & \textbf{Edge-free} & \textbf{Structure-aware} \\
        \midrule
        Vanilla KD & \ccmark & \xxmark \\
        Regularization & \xxmark & \ccmark \\
        \cellcolor{gg}{PGKD~(ours)} & \cellcolor{gg}{\ccmark} & \cellcolor{gg}{\ccmark} \\
        % \cellcolor{gg}{WID~(ours)} & \cellcolor{gg}{\xxmark} & \cellcolor{gg}{\xxmark} & \cellcolor{gg}{\ccmark}  &  \cellcolor{gg}{\ccmark} \\
        \bottomrule
    \end{tabular}
}
\caption{
Comparison among different methods to learn MLPs from GNNs.
\textbf{Edge-free} denotes whether graph edges are employed as extra inputs.
\textbf{Structure-aware} denotes whether the learned MLPs aware of the graph structural information.
% To the best of our knowledge, PGKD is the first distillation method to learn structure-aware MLPs from GNNs under edge-free setting.
% To the best of our knowledge, WID is the first distillation method without any alignment loss and directly transfers the knowledge by weight inheritance.
}
\label{tab:compare_method}
\end{table}