% noise
\begin{figure*}[!t]
	\centering
	\includegraphics[width=\linewidth]{figures/noise.pdf}
	\caption{
    The performance of GNN teacher, distilled MLP students via GLNN and PKGD when adding different noise to the initial node features.
	% Comparisons among GNN teacher, distilled MLP students via GLNN and PKGD.
 %    We add noise to the initial node features.
    For GNN teachers, we select SAGE, GAT, GCN and APPNP, respectively.
    \textbf{Upper}: \textbf{Cora} dataset and \textit{transductive} setting.
    \textbf{Lower}: \textbf{Pubmed} dataset and \textit{inductive} setting.
    }
	\label{noise}
	% \vspace{-0.5em}
\end{figure*}

\section{Analysis and Discussion}
We further explore the ability to capture graph structural information as well as the robustness of the proposed PGKD.
We also visualize the distributions of node representations for deeper insights.

\subsection{Can PGKD distill the Impact of Graph Edges?}

As mentioned in Section \ref{impact}, the intra-class edges guarantee the homophily for nodes from the same class, while the inter-class edges determine the pattern of distances among class prototypes.

% For Intra-class edges, we calculate the average L2 distance for the features of connected nodes in Graph.
We adopt SAGE as the GNN teacher and perform experiments under a transductive setting, and then calculate the average L2 distance for the features of connected nodes in the graph.
Table \ref{tab:intr_dis} shows the average distance of initial node features and node features from GNN teacher~(SAGE), GLNN, and PGKD.
The distance of the GNN teacher is the shortest due to the information aggregation operations along graph edges.
Meanwhile, the distance for GLNN is much longer due to the weak awareness of such graph structural information.
PGKD gets shorter distances than GLNN, showing a great ability to capture intra-class graph structural information.
In particular, PGKD gets a L2 distance of 0.82 on Citeseer, which is shorter than 1.40 from the GNN teacher.

\input{tables/intra_dis}


\input{tables/inter_spear}

The inter-class edges determine the pattern of distances among class prototypes.
Specifically, the prototypes of two classes would be closer with more inter-edges connecting them in GNNs.
We take statistics on the class distances~(defined as L2 distances among class prototypes) and quantity of corresponding inter-class edges.
For qualitative analysis, we calculate the Spearman correlation.
From Table \ref{tab:inter}, the GNN teacher has a low Spearman correlation, whereas GLNN shows a relatively high value.
Meanwhile, the proposed PGKD, thanks to the intra-class loss, can better capture the intra-class graph structural information and exhibits a much lower correlation.

\subsection{Is PGKD Robust to Noisy Node Features?}
To analyze the robustness of PGKD on node noise, we further evaluate the performance after adding Gaussian noise of different levels to initial node features $X$.
Specifically, we replace $X$ with $(1-\alpha)X+\alpha \epsilon$, where $\epsilon$ denotes the isotropic Gaussian noise independent from $X$, and $\alpha \in [0,1]$ controls the noise level.
A larger $\alpha$ means a stronger noise.
Figure \ref{noise} shows the performance of GNN, GLNN, and PGKD under different noise levels.
On both Cora and Citeseer, PGKD outperforms GLNN consistently as the noise level ranges from 0.1 to 0.9.
Particularly, PGKD could get better results than GAT and APPNP on Pumbed with $\alpha=0.9$. 
These show that PGKD is more robust than GLNN with respect to noisy input node features due to its ability to capture graph structural information.



% split
\begin{figure}[!t]
	\centering
	\includegraphics[width=\linewidth]{figures/ratio.pdf}
	\caption{
	The performance of GNN teacher, distilled MLP students via GLNN and PKGD under \textit{inductive} setting with different split ratio.
    % \textbf{Upper}: \textbf{Citeseer} dataset and SAGE as GNN teacher.
    % \textbf{Lower}: \textbf{Pubmed} dataset and GCN as GNN teacher.
    We select GCN as GNN teacher and perform experiments on \textbf{Pubmed} dataset.
    }
	\label{ratio}
	% \vspace{-0.5em}
\end{figure}

\subsection{Impact of Inductive Split Ratio}
To evaluate the ability for less observed data under inductive setting, we conduct the experiments under different split ratios, defined as the ratio $|\mathcal{V}^{U}_{ind}|/|\mathcal{V}^{U}|$.
A larger split ratio means less observed unlabeled data during training and more inductive unlabeled data for test~(cf. Section \ref{setting_intro}).
% Please refer to Section \ref{setting_intro} for more details about inductive setting.
As shown in Figure \ref{ratio}, the performance of the GNN teacher is not monotonically decreasing since the way to split graph~(i.e. the edges to remove) is also vital as the number of nodes for training.
PGKD outperforms GLNN and GNN under all split ratios.
Also, the performance of PGKD is more stable than GLNN.
This proves that PKGD, explicitly capturing the graph structural information, is robust and effective under different inductive split ratios.

\subsection{Impact of MLP Setting}

\input{tables/mlp_size}

We further conduct experiments using different MLP settings.
The GNN teacher is a two-layer GCN with 0.18M parameters and gets an accuracy of 83.37\% on Cora dataset.
As shown in Table \ref{tab:mlp_impact}, the vanilla MLP shows an overfitting trend when the number of parameters increases, while the PGKD does not.
Meanwhile, PGKD gets the highest results under all settings and shows consistent improvement over GLNN.
In particular, GLNN gets a score of 74.71\%~(\#L=2, \#H=128), which is 3.05\% higher than GLNN.
Such findings indicate that PGKD is more robust and effective in different MLP settings.

\subsection{Node Representation Distribution}

We visualize the distribution of node representations from GNNs and MLPs~(vanilla MLPs without KD, MLPs from GLNN, and MLPs from PGKD) via t-SNE \cite{JMLR:v9:vandermaaten08a}.
We select the GAT as the GNN teachers.
Figure \ref{node_visualize} shows the results on Cora and Citeseer under transductive setting.
Due to the message passing architecture, the node representations in the same class from GNNs are much more gathered than vanilla MLPs.
PGKD captures such graph information via intra-class loss, while vanilla MLPs and MLPs from GLNN lack such capability. 
The same-class features from both GLNN and vanilla MLP are slightly dispersed, while the features from PGKD are more clustered inside a class and separable between classes.
Moreover, PGKD can learn better class prototype distributions.
Specifically, in the GNN representations on Cora, the dark green and purple classes are far from each other. 
PGKD captures such a behavior well, where GLNN fails.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=\linewidth]{figures/visualize_feats_new.pdf}
	\caption{
	The distribution of node representations for GNN teacher, vanilla MLP, and distilled MLPs from GLNN and PKGD.
    % under \textit{transductive} setting.
    % For GNN teacher, we select GAT.
    \textbf{Upper}: \textbf{Cora} dataset.
    \textbf{Lower}: \textbf{Citeseer} dataset.
    }
	\label{node_visualize}
	% \vspace{-0.5em}
\end{figure*}