\section{Introduction}

% Necessary for GNN2MLP
% Graph Neural Networks~(GNNs) have recently become very popular in handling non-Euclidean structural data and have achieved start-of-the-art performance across graph machine learning tasks, particularly for the node classification task \citep{DBLP:conf/iclr/KipfW17, DBLP:journals/corr/abs-1710-10903, DBLP:conf/nips/HamiltonYL17}. 
Graph Neural Networks~(GNNs) are gaining importance in handling non-Euclidean structural data and have achieved start-of-the-art performance across graph machine learning tasks, particularly for the node classification task \citep{DBLP:conf/iclr/KipfW17, DBLP:journals/corr/abs-1710-10903, DBLP:conf/nips/HamiltonYL17}. 
The message-passing architecture, which aggregates the information from neighborhoods, guarantees the powerful representation ability in GNNs.
However, such neighborhood fetching operation also leads to high latency \cite{DBLP:conf/kdd/JiaLYYLA20}, making it challenging to apply GNNs for real-world applications.
Meanwhile, MLPs are free from the GNN latency problem without message-passing architecture, but perform poorly in graph tasks due to the lack of graph structural information.
% Therefore, how to train low-latency MLPs with high accuracy as GNNs on graph tasks becomes a hot topic.
% Therefore, how to train low-latency MLPs to have competitive accuracy as GNNs becomes a hot topic.
Therefore, it is challenging to train low-latency MLPs to have competitive accuracy as GNNs on graph tasks.

\input{tables/intro_table}

To achieve this goal, one mainstream approach is to distill the knowledge from GNNs to MLPs.
GLNN \cite{DBLP:conf/iclr/ZhangLSS22} employs the vanilla logit-based knowledge distillation~(KD) to train MLP students from GNN teachers.
Despite the KD target, the MLPs in GLNN still suffer from the lacking of graph structural information, since MLPs rely exclusively on the node features.
To inject the graph structural information, previous methods \cite{DBLP:journals/corr/abs-2106-04051, DBLP:journals/corr/abs-2210-02097} employ additional regularization terms on the final node representations based on the graph structures.
For each node, the key insight is to draw closer the distance between the selected node and its $i$-hop neighbors while pushing further for other nodes.
However, this strategy has two issues: 1) Graph edges are required as an auxiliary input, but graph structures may be unavailable for various scenarios, such as federated graph learning \cite{DBLP:journals/corr/abs-2202-07256}; 2) Such prior knowledge, which the regularization terms rely on, is agnostic from the GNN teachers.
Intuitively, we can transfer the graph structural information from GNN teachers to MLP students when the graph structures are unknown.
We thus ask: \textit{What is the impact of graph structures~(i.e. graph nodes) on GNNs? Can we distill such graph structural information from GNNs to MLPs~(so that we can get structure-aware MLPs in an edge-free setting)?}
% MLPs can be structure-aware without graph edges as extra input)?}
% MLPs can be structure-aware without graph edges as extra input

In this paper, we first analyze the impact of graph structures on GNNs.
We categorize the graph edges into \textbf{intra-class} edges and \textbf{inter-class} edges, where the nodes connected by the edge are from the same and different classes, respectively.
The intra-class edges enforce the local smoothness by constraining the learned representations of two connected nodes to become similar, so that the homophily property for nodes from the same class can be captured \cite{DBLP:conf/www/ZhuWSJ021}.
The inter-class edges, connecting two nodes from different classes, determine the pattern of distances among these classes.
% More details can be found in section \ref{impact}.

% 考虑加字母来辅助解释
Based on the analysis, we propose a Prototype-Guided Knowledge Distillation~(PGKD) method, which distills the graph structural information from GNN teachers to MLP students in an edge-free setting.
We design extra alignment losses for MLPs based on the class prototype~(viz. a typical embedding vector of a given class) \cite{DBLP:conf/nips/SnellSZ17}, aiming to mimic the impact of graph structures~(i.e. graph edges) on GNNs.
% Class prototype refers to a typical embedding vector of a given class.
In PGKD, we design a prototype strategy to get all class prototypes for both GNN teachers and MLP students.
To mimic the intra-class edges, we force the node representations to be closer to their corresponding prototypes.
Meanwhile, we align the MLP prototypes with GNN prototypes to learn the inter-class distance patterns.
As shown in Table \ref{tab:compare_method}, PGKD is the first edge-free and structure-aware method to distill GNNs to MLPs.


We perform experiments on popular graph benchmarks in both transductive and inductive settings.
We also conduct extensive ablation studies and analyses on PGKD.
Empirical results demonstrate the effectiveness and robustness of PKGD.
% Meanwhile, we prove the robustness of PKGD by adding noise to the node features.
In short, our main contributions are as follows:
\begin{itemize}
    \item We analyze the impact of graph structures on GNNs, thus providing a deeper understanding of GNNs.
    % which is helpful to understand GNNs better.
    \item We propose PGKD, which effectively distills the graph structural knowledge from GNNs to MLPs in an edge-free setting.
    \item We evaluate PGKD on various graph benchmarks and demonstrate its effectiveness and robustness.
\end{itemize}
