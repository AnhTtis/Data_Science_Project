
\pdfoutput=1
\documentclass[11pt]{article}

\usepackage[]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{arydshln}
\usepackage{subfigure}
\usepackage{makecell}
\usepackage{colortbl}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumerate}

\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{array}
\usepackage{pifont}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{xspace}
\usepackage{tcolorbox}
\usepackage{xparse}
\usepackage{soul}
\usepackage{booktabs,amsfonts,dcolumn}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath,amsthm,amsfonts,amssymb,bm,stmaryrd}
\usepackage[noorphans,vskip=0.75ex,leftmargin=2ex]{quoting}
\usepackage{breqn}
\usepackage{makecell}

% \usepackage{fnpos}
% \usepackage{algorithm}


\definecolor{OliveGreen}{rgb}{0,0.4,0}


\usepackage{cleveref}

\crefname{section}{§}{§§}
\Crefname{section}{§}{§§}


% for note
\newcommand{\ccmark}{\ding{51}}%
\newcommand{\xxmark}{\ding{55}}%
\definecolor{color_m}{RGB}{72,117,170}
\definecolor{color_f}{RGB}{201,89,72}
\definecolor{color_c}{RGB}{230,230,230}
\definecolor{color_e}{RGB}{100,155,74}
\definecolor{ccon}{HTML}{fee9d4}
\definecolor{cood}{HTML}{d8f0d3}
\definecolor{cid}{HTML}{dae8f5}
\definecolor{gg}{HTML}{e2f0cb}


\newcommand{\CC}{\cellcolor{gray!15}}

\title{Edge-free but Structure-aware: Prototype-Guided \\ Knowledge Distillation from GNNs to MLPs}

% \author{
% Taiqiang Wu \\
% Tsinghua Shenzhen International School, Tsinghua University \\
% wtq20@mails.tsinghua.edu.cn
% }

\author{Taiqiang Wu$^{1,2}$, Zhe Zhao $^{2}$, Jiahao Wang$^{3}$, \\ \textbf{Xingyu Bai$^{1}$, Lei Wang$^{4}$, Ngai Wong$^{3}$, Yujiu Yang$^{1}$}\Thanks{ Corresponding author} \\
$^{1}$ Shenzhen International Graduate School, Tsinghua University \ $^{2}$ Tencent \\
$^{3}$ The University of Hong Kong \ $^{4}$ Ping An Technology (Shenzhen) \\
wtq20@mails.tsinghua.edu.cn, yang.yujiu@sz.tsinghua.edu.cn}

\begin{document}

\maketitle


\begin{abstract}
% While Graph Neural Networks~(GNNs) achieve success for graph machine learning, they are difficult to be applied in real applications due to the high neighborhood-fetching latency.
%neglect
% The high neighborhood-fetching latency in Graph Neural Networks~(GNNs) has led to increased attention to Knowledge Distillation~(KD) methods from GNNs to MLPs.
% and overlook the information from GNN teachers. 
Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic.
However, MLPs rely exclusively on the node features and fail to capture the graph structural information.
Previous methods address this issue by processing graph edges into extra inputs for MLPs, but such graph structures may be unavailable for various scenarios.
To this end, we propose a Prototype-Guided Knowledge Distillation~(PGKD) method, which does not require graph edges~(edge-free) yet learns structure-aware MLPs.
Specifically, we analyze the graph structural information in GNN teachers, and distill such information from GNNs to MLPs via prototypes in an edge-free setting.
Experimental results on popular graph benchmarks demonstrate the effectiveness and robustness of the proposed PGKD.
\end{abstract}

\input{sections/1-Introduction}
% \input{sections/2-Related}
\input{sections/2.5-Preliminary}
\input{sections/3-Method}
% \clearpage
\input{sections/4-Experiments}
\input{sections/5-Analysis}
% \input{sections/6-Related}
\input{sections/2-Related}
\input{sections/6-Conclusion}

% \clearpage
% \balance
\bibliography{main}
\bibliographystyle{acl_natbib}



% \clearpage
% \input{sections/appendix}

\end{document}
