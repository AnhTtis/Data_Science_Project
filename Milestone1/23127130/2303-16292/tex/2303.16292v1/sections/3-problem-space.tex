\section{XAIR Problem Space and Key Factors}
\label{sec:problem_space_factors}

Determining the way to create effective XAI experiences in AI is a complex challenge. Thus, it is important to first identify the problem space to bound the scope of our investigation.
We first summarize over 100 papers from the ML and HCI literature to identify the problem space and the main dimensions within each problem (Sec.~\ref{sub:problem_space_factors:problem_space}).
Then, we outline the key factors that determine the answers to the problems (Sec.~\ref{sub:problem_space_factors:factors}).

The problem space and key factors define the structure of XAIR (Fig.~\ref{fig:xair} middle).
In Sec.~\ref{sec:methodology}, we present two studies conducted to obtain insights from end-users and expert stakeholders about how to design XAI in AR.
Then, combining the structure and insights, we show how these factors are connected with the problem space, and provide design guidelines in Sec.~\ref{sec:framework}.

% We first identify the problem space of our research question about creating effective XAI experience in AR (Sec.~\ref{sub:problem_space_factors:problem_space}).
% We then spotlight the key factors that determine the answer to the research question (Sec.~\ref{sub:problem_space_factors:factors}).
% Fig.~\ref{fig:overview} presents a detailed overview of the problem space and the factors.
% After introducing the overall picture, the rest of the paper follows Fig.~\ref{fig:pipeline} to develop and evaluate XAIR.

% \input{tex_fig_tab_alg/fig_overview_details}

\subsection{Problem Space}
\label{sub:problem_space_factors:problem_space}

Following the design space analysis method~\cite{qoc_1999}, the research question was divided into three sub-questions: when to explain, what to explain, and how to explain~\cite{elliott2017living,nahum-shani_just--time_2018}.

\subsubsection{\colorwhen{When to Explain?}}
\label{subsub:problem_space_factors:problem_space:when}
The literature review revealed two aspects of ``when'' that were important to consider: the \textit{availability} of explanations (\ie whether to prepare explanations?), and the timing of the explanation's \textit{delivery} (\ie when to show explanations?).

\colorwhen{\textbf{\textit{Availability}}}.
% It is important to determine whether AI models should generate any explanation available for users to access.
Previous research has found that to maintain a positive user experience, supporting user agency and control is important during human-AI interaction~\cite{cai_impacts_2022,lee1992trust}.
Having explanations that are available and accessible is in line with the goal of supporting user agency.

\colorwhen{\textbf{\textit{Delivery}}}.
With the ability to show information at any time, AR can employ various timing strategies to present explanations. Thus, it is important to find the appropriate method to deliver explanations to users. Generally, there are two approaches, manual-trigger (\ie initiated by users) and auto-trigger (\ie initiated by the system)~\cite{cimolino_two_2022,yeh2022guide}.
On the one hand, researchers have found that explanations should not always be presented to users, because they can introduce unnecessary cognitive load and become overwhelming for non-expert end-users~\cite{chazette2019end,wagner2020regulating,bunt2012explanations,robbins2019misdirected,stumpf2016explanations}. This is especially important in AR, as users' cognitive capacity tends to be limited~\cite{buchner2022impact}.
Moreover, adopting manual triggers would enable users to choose to see explanations as needed, thus enabling them to exercise agency over their experience~\cite{roy_automation_2019,lu_exploring_2022}.
On the other hand, existing findings on just-in-time intelligent systems (\eg just-in-time recommendations~\cite{kapoor2015just,ma2020temporal} and just-in-time interventions~\cite{nahum-shani_just--time_2018,Sarker2014, xu_typeout_2022}) have suggested that automatically delivering explanations at the right time based on user intent and need (as detected via AR sensing that identifies a user's state and context) can provide a better user experience~\cite{bhattacharya2017intent,mehrotra2019jointly}.

% In Sec.~\ref{sec:methodology}, we will draw insights obtained from both end-users and expert stakeholders and propose our framework and guidelines to answer the two parts in Sec.~\ref{sec:framework}.

\subsubsection{\colorwhat{What to Explain?}}
\label{subsub:problem_space_factors:problem_space:what}
The literature review also found two important aspects of ``what'' to consider: First, the \textit{content} of the explanations (\ie what type of content to include?). Second, the level of \textit{detail} of the explanations (\ie how much detail should be explained?).

\colorwhat{\textbf{\textit{Content}}}.
Previous literature in XAI has identified several explanation content types~\cite{barredo_arrieta_explainable_2020,mohseni_multidisciplinary_2021}. 
% On include input/output, conceptual model (why/why not, how, what else, what if) and non-functional types (certainty, how-to).
\review{
The seven types are:
\begin{s_enumerate}
\item Input/Output. This type explains the details of input (\eg data sources, coverage, capabilities) or output (\eg additional details, options that the system could produce) of a model~\cite{lim_assessing_2009,lim_toolkit_2010}.
\item Why/Why-Not. This type explains the features in the input data~\cite{Ribeiro2016} or the model logic~\cite{ribeiro_anchors_2018} that have led or not led to a given AI outcome~\cite{myers2006answering} (also known as contrastive explanations). Showing feature importance is another commonly used technique to generate these explanations~\cite{schlegel2019towards,casalicchio2018visualizing}.
\item How. This type provides a holistic view to explain the overall logic of an algorithm or a model and illustrate how the AI model works. Typical techniques include model graphs~\cite{lakkaraju2016interpretable}, decision boundaries~\cite{lombrozo2009explanation}, or natural language explanations~\cite{berkovsky2017recommend}. 
\item Certainty. This type describes the confidence level of the model with its input (\eg for models whose input is not deterministic, explain how accurate the input of the model is) or output (\eg explain how accurate, or reliable the AI outcomes are)~\cite{google_map_match_rate_2018,schoonderwoerd2021human}. Scores based on softmax~\cite{bridle1989training} or calibration~\cite{platt1998sequential} are commonly used as the confidence/certainty score for ML models.
\item Example. This type presents similar input-output pairs from a model, \eg similar input that lead to the same output or similar output examples given the same input~\cite{cai2019human,keane2019case}. This is also known as the What-Else explanation. Example methods include influence functions~\cite{koh_understanding_2017} and Bayesian case modelling~\cite{kim2014bayesian}.
\item What-If. This type demonstrates how changing input or applying new input can affect model output~\cite{cai2019effects,lim_why_2009}.
\item How-To. In contrast to What-If, this type explains how to change input to achieve a target output~\cite{wang_designing_2019,liao_questioning_2020}, \eg how to change the output from X to Y? Common methods for What-If/How-To content include rule generation~\cite{guidotti2018local}, feature description~\cite{wachter2017counterfactual}, and input perturbation~\cite{zhang2018interpreting}.
\end{s_enumerate}
}
% Previous studies by Lim \etal offered suggestions about subsets of content types for context-aware systems~\cite{lim_toolkit_2010,lim_assessing_2009,lim2019these}.
Moreover, another aspect that is independent of the explanation content type is global \vs local explanations (explaining the general decision-making process \vs a single instance)~\cite{molnar2020interpretable}. In general, non-expert end-users were found to prefer local explanations ~\cite{lakkaraju2019faithful,dhanorkar_who_2021}.


% \input{tex_fig_tab_alg/fig_pipeline}

\colorwhat{\textbf{\textit{Detail}}}.
Displaying every relevant explanation content type to an end-user can be overwhelming, especially with the limited cognitive capacity they have in AR~\cite{buchner2022impact,baumeister2017cognitive}. Explanations that extend a user's prior knowledge or fulfill their immediate needs should be prioritized~\cite{coppers2018intellingo}.
Moreover, previous research has suggested that presenting detailed and personalized explanations is useful for better understanding AI outcomes ~\cite{schneider2019personalized,kouki2019personalized,esteva2017dermatologist,jahanbakhsh_effects_2020,xu_understanding_2020}.

% We will verify whether these previous findings are transferable to AR scenarios in Sec.~\ref{sec:methodology}.
\review{
Our focus on \textit{content} and \textit{detail} is about choosing appropriate explanation content types and proper levels of detail, but not on picking which techniques to generate explanations.
From a technical perspective, there are interpretable models (\ie the model being transparent, such as linear regression or decision trees) and ad-hoc explainers (\ie generating explanations for complex, black-box models)~\cite{Lundberg2017, Ribeiro2016}. The latter can further be divided into model-specific and model-agnostic explanation methods~\cite{barredo_arrieta_explainable_2020}.
We refer readers to other surveys and toolkits for developing or selecting explanation generation algorithms \cite{arya2019one,liao2021human,dalex,h2oai,adadi_peeking_2018}.
}



\subsubsection{\colorhow{How to Explain?}}
\label{subsub:problem_space_factors:problem_space:how}
The last sub-question \colorhow{\textit{how}} focuses on the visual representation of the content in AR.
% Although Eiband \etal touched on the question of \textit{how to explain} in their participatory XAI design process outside of AR, they only involve a general ``iterative prototyping'' step~\cite{eiband_bringing_2018}.
% We summarize important dimensions that are needed to develop the framework and guidelines.
% The two important dimensions that are needed to develop the framework and guidelines are summarized below.
Two dimensions emerged from the literature review, \ie modality and paradigm.

\colorhow{\textbf{\textit{Modality}}}.
The multi-modal nature of AR enables it to support AI outcomes via various modalities (\eg visual, audio, or haptic)~\cite{chen2017multimodal,nizam2018review}.
Explanations are hard to convey using modalities with limited bandwidth (\eg haptic, olfactory, or even gustatory). Therefore, visual and audio are the two major modalities that should be employed for explanations.

\colorhow{\textbf{\textit{Paradigm}}}.
If explanations are presented using audio, the design space is relatively limited (\eg volume, speed). We refer readers to existing literature on audio design (\eg \cite{frauenberger2007survey, kern2009design}).
The design space of the visual paradigm for explanations, however, is much larger.
First, from a formatting perspective, explanation content can be presented in a textual format (\eg narrative, dialogue)~\cite{lakkaraju2016interpretable,myers2006answering}, graphical format (\eg icons, images, heatmaps)~\cite{zeiler2014visualizing,simonyan2013deep}, or a combination of both.
Second, from a pattern perspective, an explanation can be displayed either in an implicit way (\ie embedded in the environment, such as a boundary highlight of an object) or explicit way (\ie distinguished from the environment, such as a pop-up dialogue window)~\cite{lindlbauer_context-aware_2019,tatzgern_adaptive_2016,diverdi2004level}.
The pattern is closely related to the adaptiveness of the AR interface~\cite{dai2017scannet,wang_designing_2019}. With 3D sensing capabilities, the location of an explanation can be body-based (mostly explicit), object-based (implicit or explicit), or world-based (implicit or explicit) ~\cite{lu_exploring_2022,bonanni2005attention,laviola20173d,xu_hand_2018}. Prior AR research has explored adaptive interface locations~\cite{luo2022should,muller2016taxonomy}, \eg interfaces should be adaptive based on the semantic understanding of the ongoing interaction~\cite{cheng_semanticadapt_2021,qian_scalar_2022,rzayev2020effects} and ergonomic metrics~\cite{evangelista2021xrgonomics}.

% To answer \colorhow{\textit{how}} to explain, we will leverage end-users' preference and experts' knowledge collected from two user studies (see Sec.~\ref{sec:methodology}), and propose our framework to guide the choice of these options.


\subsection{Key Factors}
\label{sub:problem_space_factors:factors}
These three questions, and their dimensions, form the overall problem space of XAIR.
Another important aspect of XAIR is the factors that determine the answers to these questions.
We summarize these factors from two perspectives, one specific to AR platforms (Sec.~\ref{subsub:problem_space_factors:factors:ar_specific}), and the other agnostic to any platform (Sec.~\ref{subsub:problem_space_factors:factors:non_ar_specific}).
% The middle of Fig.~\ref{fig:overview} summarizes these factors.

\subsubsection{AR-Specific Factors}
\label{subsub:problem_space_factors:factors:ar_specific}
Fig.~\ref{fig:uniqueness_ar} summarizes the three main features that distinguish AR from other platforms: \textit{User State}, \textit{Contextual Information}, and \textit{Interface}.
As \textit{Interface} is an integral property of an AR platform, it remains invariant to external changes.
In contrast, the other two aspects are dynamic and would alter the design of XAI in AR.

\textbf{\textit{User State.}}
The sensors that could be integrated within future HMDs would empower an AR system to have a rich, instant understanding of user's state, such as activities (IMU~\cite{gjoreski2021head,windau2016walking}, camera~\cite{fathi2011understanding,singh2016first,schroder2017deep,liang_authtrack_2021}, microphone~\cite{xu_listen2cough_2021,xu_earbuddy_2020,wang_hearcough_2022,jin_earcommand_2022}), cognitive load (eye tracking~\cite{duchowski_index_2018,zagermann2018studying,joseph2020potential}, EEG~\cite{antonenko2010using,xu2018review}), attention (eye tracking~\cite{huang2018predicting,chong2018connecting,stappen2020x,xu_recognizing_2020}, IMU~\cite{leelasawassuk2015estimating}, EEG~\cite{vortmann2019eeg}), emotion (facial tracking~\cite{yong2019emotion, yan2022emoglass}, EEG~\cite{wan2021wearable,soleymani2015analysis}) and potential intent (the fusion of multiple sensors and low-level intelligence~\cite{tsai2018augmented,admoni2016predicting,kim2016understanding}).
Depending on a user's state, the design of explanations could be different. For example, as identified in previous research on ambient interfaces~\cite{pielot2017beyond,fogarty2005predicting}, when users engage in activities with a high cognitive load, explanations should not show up automatically to interrupt them
(related to \colorwhen{\textit{when}}).

\textbf{\textit{Contextual Information.}}
Compared to devices such as smartphones, AR HMDs have more context awareness. Other than having an awareness of location and time~\cite{dey_understanding_2001}, an egocentric camera and LiDAR, combined with other sensors (\eg Bluetooth, WiFi, RFID), can identify details about digital and non-digital objects in the environment~\cite{redmon2016you,liu2020deep,park2018high}, and have a better understanding of the semantics of a scene~\cite{grauman2022ego4d,pan2019content,bolanos2016toward,miech2020end}.
Such contextual information would also influence the design of XAI.
For instance, an explanation visualization about recipe recommendations that appears when users open the fridge may look differently from explanations about podcast suggestions that are shown while driving (related to \colorhow{\textit{how}}).

\subsubsection{Platform-Agnostic Factors}
\label{subsub:problem_space_factors:factors:non_ar_specific}
There are also other factors that are platform agnostic such as the motivation to present explanations (\ie \textit{why explain?}). We view this factor from two perspectives, one from the system side (\ie what are the \textit{system's goals} when presenting explanations?), and the other from the non-expert end-user side (\ie what are \textit{users' goals} when they want to see explanations?)~\cite{samek2019towards}.
The \textit{user profile} (\ie individual details) is another important factor related to personalized explanations~\cite{schneider2019personalized,kouki2019personalized}.

\textbf{\textit{System Goal.}}
Based on prior literature, we summarize four system goals that are desired when an AR system provides explanations for AI outcomes:
\begin{s_enumerate}
\item User Intent Discovery. When an AI model generates suggestions for a new topic, the system seeks to help users discover new intent~\cite{samek2019towards,gotz2009behavior,pu2006trust}.
For example, when a user is traveling in a city, the system recommends several attractions and local restaurants to visit.
Both the recommendation and explanations help the user explore new things that they were not aware of.
\item User Intent Assistance. When the target task has been already initiated by users, then the goal of generating AI outcomes and explanations assists users with existing intent~\cite{brown1998utility,bercher2014plan,das2021explainable}. For instance, when a user is making dinner, intelligent instructions and explanations would suggest alternative ingredients based on what a user has in their space.
\item Error Management. When a system has low confidence about input/output or makes a mistake, explanations can serve as error management and explain the process so that users can understand where an error comes from if it appears~\cite{xu2019explainable,adadi_peeking_2018}, how they might better collaborate with the system~\cite{das2021explainable}, or when to adjust their expectation of the system's intelligence~\cite{bertossi2020data, zhang2020effect}.
\item Trust Building. Various studies have found that explanations can help systems build user trust by offering transparency and increasing intelligibility~\cite{antifakos2005towards,mahbooba2021explainable,shin2021effects}. As a result, usersâ€™ trust in models leads them to rely on the system~\cite{berkovsky2017recommend,bussone2015role}.
\end{s_enumerate}
These four types of system goals are not exclusive. A system can seek to achieve multiple goals simultaneously.
Depending on the subset of system goals, the appropriate explanation timing and content types can differ~\cite{lim_assessing_2009,myers2006answering} (related to \colorwhen{\textit{when}} and \colorwhat{\textit{what}}).

\textbf{\textit{User Goal.}}
While a system has varying reasons to provide explanations, end-users also have varying reasons to have explanations. We summarize four types of user goals from literature.

\begin{s_enumerate}
\item Resolving Confusion/Surprise. Expectation mismatch is one of the main reasons to need explanations~\cite{dhanorkar_who_2021,ribera2019can,brennen2020people,langer_what_2021}. Users can become confused or surprised when AI outcomes are different from what users are expecting, and having explanations can help to resolve concerns~\cite{rai2020explainable,gervasio2018explanation}.
\item Privacy Awareness. As AI influences more aspects of daily living, concerns about invasion of one's privacy are also growing~\cite{manheim2019artificial}. Explanations could disclose which data is being used in a model's decision-making process to end-users ~\cite{eslami2015always,rader2018explanations,datta2014automated}. 
\review{
Researchers and designers are recommended to follow an existing privacy framework, such as contextual integrity~\cite{nissenbaum2009privacy}, to make privacy explanations more robust.}
\item Reliability. Ensuring the reliability of AI outcomes is essential for non-trivial decision-making processes so that users can rely on a trustworthy system~\cite{lepri2018fair,jiang_who_2022,Ribeiro2016}, \eg daily activity recommendations for personal health management or automatic emergency service contacting in safety-threatening incidents.
\item Informativeness. End-users can be curious about the reason or process behind an AI outcome~\cite{hoffman2018metrics,li2020survey}. Explanations can fulfill users' curiosity by providing more information~\cite{lage2019human,binns2018s,rader2018explanations}.
\end{s_enumerate}
Similar to the system goals, these user goals are not exclusive and users can have multiple goals at the same time. Different goals can require different explanation timings and content (\colorwhen{\textit{when}} and \colorwhat{\textit{what}}).
% However, it is worth noting that \textit{user goal} is a latent factor that can often be hard to detect by an AR system.

\textbf{\textit{User Profile.}}
This factor covers a range of individual details that influence the design of XAI.
For example, information such as demographics and user preferences is necessary to generate personalized explanations~\cite{schneider2019personalized,kouki2019personalized,gedikli2014should}.
End-users' familiarity with system outcomes is related to the need for explanations and \colorwhen{\textit{when}} to provide them~\cite{coppers2018intellingo}.
Users' digital literacy with AI also affects \colorwhat{\textit{what}} types of explanations are appropriate and would serve users' purposes~\cite{long_what_2020,ttc_labs,ehsan2021explainable}.
Moreover, users may have individual preferences about explanation visualizations, which may be closely related to \colorhow{\textit{how}}.
This factor takes these considerations into account.

It is worth noting that XAIR is proposed as a design framework.
In a context that AR can detect robustly, designers can use the framework to infer end-users' latent factors, such as \textit{User State} and \textit{User Goal}, based on their design expertise~\cite{eiband_bringing_2018}. 
For example, when users are driving (which can be easily detected by AR), designers can assess users' cognitive load to be high (\textit{User State}). For more complex factors such as \textit{User Goal}, designers can propose a set of potential goals in a given scenario and then refer to the framework to propose a set of designs.
% The automatic detection of these factors is not the focus of this paper.
As sensing and AI technology are maturing, the framework could be coupled with the automatic inference of these factors~\cite{tsai2018augmented,admoni2016predicting,kim2016understanding,gjoreski2021head,yan2022emoglass}.


% In the next section, we will leverage the insights acquired from end-users and expert stakeholders, and connect these key factors to answer the three XAI in AR design sub-questions.