\section{Methods}
\label{sec:methodology}
We conducted two studies after outlining the problem space, one from end-users' perspectives (Sec.~\ref{sub:methodology:end_user_surveys}), and the other from XAI/design/AR expert stakeholders' perspectives (Sec.~\ref{sub:methodology:expert_workshops}). The findings from the studies are complementary and provided insights that guided the development of the framework.

\subsection{Study 1: Large-Scale End-User Survey}
\label{sub:methodology:end_user_surveys}
In spite of the existing studies on XAI for end-users, it is unclear whether these findings hold for AR scenarios due to the unique features of AR systems.
Thus, we conducted a large-scale survey with end-users to collect their preferences on various aspects of XAI experiences for everyday AR.
% The survey results reveal the need of XAI in AR scenarios and are generally consistent with previous research outside XAI.

\subsubsection{Participants}
\label{subsub:methodology:end_user_surveys:participants}
\review{We recruited 506 participants from a third-party online user study platform (age 18 - 54, average 37$\pm$10), with a balanced gender distribution (Female 260, Male 241, Non-binary 5).}
Participants' digital literacy with AI varied, thus they were split into six groups: 1) unfamiliar with AI (12.2\%, 62), 2) heard of AI but never used AI-based products (23.5\%, 119), 3) used AI products occasionally a few times (23.1\%, 117), 4) used AI products on a regular basis (12.8\%, 65), 5) used AI products frequently (20.0\%, 101), and 6) worked on AI products (8.3\%, 42).
Participants were familiar with the concept of AR.
Among these groups, we further randomly sampled 20 participants (age 18 - 53, average 37$\pm$9, 11 Female, 9 Male) for a semi-structured interview to collect a more in-depth understanding about their preferences for XAI in AR.

\subsubsection{Design and Procedure}
\label{subsub:methodology:end_user_surveys:materials_design}
We prepared five sets of proof-of-concept descriptions and images with intelligent everyday AR services that represented five scenes in a typical weekday (\ie one set per scene). They included 1) music recommendations for the morning when users would be brushing their teeth, 2) podcast recommendations for when users would be driving to work, 3) music recommendations for when users would be working out, 4) recipe recommendations for when users would be making dinner, and 5) additional spice recommendations for when users would be making dinner.
In this study, we chose recommendations as the main AI service category, since it is arguably one of the most common AI applications in everyday AR~\cite{lam_a2w_2021,chatzopoulos2016readme} and users could easily contextualize these scenes in their mind.

% we pre-determined a set of explanations from different categories
For the AI outcome in each scene, participants were asked whether they wanted explanations (\ie yes, no, neutral). If their answer was yes, they would be directed to answer when they wanted it (\ie always/frequent, contextually dependent, rare/never), their preferred length of explanation (\ie concise \vs detailed) and the presenting modality (\eg visual, audio, neutral).
After viewing these scenes, they were asked to choose the explanation content types that they found useful. Participants were compensated \$5 USD for the task.
% In interviews, the experiment host would follow up with more questions based on participants' answers to collect detailed reasons behind their choices.

% including input/output, why/why-not, how, and certainty. The other three categories are not included as they may not compatible 

% \subsubsection{Procedure}
% \label{subsub:methodology:end_user_surveys:procedure}
% Participants were told to imagine themselves as end-users of AR devices. They then went through the five scenes and answered the questions.
% At the end of the survey, they were asked whether they would be interested in joining a follow-up interview study.
We randomly sampled 20 respondents who were willing to participate in a one-hour interview about the detailed reasons behind their survey responses.
These participants were compensated \$10 for the interview.
The interviews were video-recorded and manually transcribed.
\review{Two researchers collectively summarized and coded the data using a thematic analysis~\cite{braun2012thematic}. Specifically, they first met to establish an agreement on the themes and independently coded all the data. Then, they gathered to discuss and refine the coded data to resolve differences. Their inter-rater reliability ($\kappa$) was over 90\% after the refinement.
}

\subsubsection{Results}
\label{subsub:methodology:end_user_surveys:results}
The survey found that respondents had specific preferences for the timing, content, and modality of explanations.

\input{tex_fig_tab_alg/fig_survey_results}

\review{\textbf{Finding 1: Most users wanted explanations of AI outputs in AR.}} (related to \colorwhen{\textit{when - availability}}).
A large proportion of respondents wanted explanations (89.7\%), motivating the need for XAI in everyday AR scenarios (see Fig.~\ref{subfig:survey_results:whether}).
Our findings were consistent with previous work on end-users' needs for XAI outside AR~\cite{ehsan2021explainable,ttc_labs}.
The results indicated that if respondents had at least heard of AI, they were more likely to express a need for XAI in AR compared to those who were not familiar with AI.
\review{
% An ANOVA with digital literacy as the main factor indicates statistical significance ($F_{5,500} = 10.8, p < 0.001$).
% Generally, the more familiar respondents were with AI, the more they felt there to be a need for explanations.
Our interviews found that respondents with little knowledge of AI didn't realize what explanations could be used for. 
Interestingly, around 10\% of respondents who worked on AI indicated that they didn't want explanations. Our interviews revealed the main reason being that some users were \textit{``familiar enough... with the algorithm''} (P2).
% For example, \pquote{4}{I can understand why... it's not really needed}. These participants already had knowledge of AI and thus didn't need additional explanations.
% This finding is consistent with previous work outside AR~\cite{ehsan2021explainable,ttc_labs}.
}
% 
% Moreover, our interview results also suggest that when participants were unsatisfied 

\textbf{Finding 2: The majority of users wanted explanations to be occasional and contextual, especially when they saw anomalies} (related to \colorwhen{\textit{when - delivery}}).
Although most respondents wanted explanations, only 13.8\% indicated that they needed explanations all the time.
% , mostly from participants who work on AI (see Fig.~\ref{subfig:survey_results:when}) -- an interesting polarization of XAI needs of users with high digital literacy when comparing \textbf{Finding 1} \& \textit{2}.
The majority of respondents (63.4\%) preferred for explanations to be presented contextually only when they have the need.
% For example, P7 didn't think there was a need for explanations in the morning music recommendation scene, \pquote{7}{I wouldn't be surprised if it knew what I wanted to listen to in the morning. I'm a creature of habit.}
The results of the interviews indicated that the need for explanations was mainly in cases where AI outcomes were new or anomalous to respondents. This finding is also in line with previous studies' findings outside AR~\cite{dhanorkar_who_2021,jiang_who_2022}.

\textbf{Finding 3: Users generally preferred specific types of explanations} (related to \colorwhat{\textit{what - content}}).
Four explanation content types stood out as useful: Input/Output (41.5\%), How (37.1\%), Why/Why-Not (31.6\%), and Certainty (30.6\%).
The first three types were highlighted in previous findings about context-aware systems~\cite{lim_assessing_2009,lim_toolkit_2010}, while the last type has been adopted by industrial practitioners~\cite{google_map_match_rate_2018,spotify_blend_taste}.
As shown in Fig.~\ref{subfig:survey_results:what}, respondents with more knowledge of AI would prefer having these explanation types more than those with less AI knowledge.
% (ANOVA $F_{5,2018} = 15.4, p < 0.001$),
% which aligns with \textbf{Finding 1}.

\textbf{Finding 4: Users found detailed and personalized explanations useful} (related to \colorwhat{\textit{what - detail}}).
Although showing more explanation content can introduce additional cognitive costs, 48.3\% of respondents reported that they would find detailed explanations with multiple content types to be useful.
% \pquote{1}{It gives me more context, substance for why I need to take this suggestion.}
Moreover, respondents indicated that explanations that included personal preferences would be more convincing, \eg \pquote{13}{more personable, more upbeat}.
These results suggest that there is a need to provide options to modulate the level of explanation detail (see Sec.~\ref{subsub:problem_space_factors:problem_space:what}) and the \textit{User Profile} factor in the framework).

\textbf{Finding 5: Users' preferences for modalities depended on the cognitive load in an AR scenario} (related to \colorhow{\textit{how - modality}}).
The five scenes introduced different levels of cognitive load, which led respondents' preferences for XAI modality to vary. We found that for scenes with complex visual stimuli such as driving, respondents tended to prefer audio explanations over visual ones by 40\%, as they were \pquote{8}{more easy and convenient}.
This suggests that it is necessary to take modality bandwidths into account when choosing \colorhow{\textit{how}} to present XAI in different AR scenarios~\cite{buchner2022impact}.

Overall, these findings motivated the need for XAI in AR (\textbf{Finding 1}).
% , and were aligned with previous studies on XAI outside of AR (\textbf{Finding 1, 2} \& \textit{3}).
% This verifies that previous findings are transferable to AR scenarios.
Moreover, these results (\textbf{Finding 2-5}) also provided guidance for design XAI for end-users in AR.

\subsection{Study 2: Iteration with Expert Workshops}
\label{sub:methodology:expert_workshops}
Based on the existing literature and the end-user survey results, we created an early draft of the framework. Since XAIR aims to support designers and researchers during their design process, we utilized our draft within three workshops with expert stakeholders to collect their insights and finalize the framework.

% \footnotetext{By expert stakeholders, we refer to experts related to our research problem (XAI, design, UX, HCI, AR), rather than domain experts as XAI users. We will use the word ``experts'' as this meaning for the rest of the paper.}

\subsubsection{Participants}
\label{subsub:methodology:expert_workshops:participants}
Twelve participants (7 Female, 5 Male, Age 35 $\pm$ 6) from a technology company volunteered to participate in the study. They came from four backgrounds, \ie 3 XAI algorithm developers, 3 designers, 3 UX professionals, and 3 HCI/AR researchers. Participants worked in their domains for at least five years. All participants were familiar with the concept of AI and AR. Participants were randomly assigned into three groups, with each group containing one expert from each domain.

\subsubsection{Design and Procedure}
\label{subsub:methodology:expert_workshops:materials_design}
We proposed a draft of the framework combining the summary of literature and the results of end-user study. It was an early version of XAIR that is introduced in Sec.~\ref{sec:framework} and can be found in Appendix~\ref{sec:appendix:earlier_frameworks}.
We also prepared a set of everyday AR scenarios similar to the ones used in the end-user survey (Sec~\ref{sub:methodology:end_user_surveys}) to provide more context and stimulate more insights from experts.
We utilized a Figma board to show images of the framework and experts could add in-place feedback to different areas of the framework.

We adopted an iterative process using three sequential workshops. \review{All workshops lasted about 90 minutes and were video-recorded. After each workshop, two researchers went through a similar coding and refining process as Sec.~\ref{subsub:methodology:end_user_surveys:materials_design}, to make sure the result achieved a inter-rater reliability ($\kappa$) over 90\%. We summarized experts' feedback, iterated on the framework, and presented the new version in the next workshop.}
% \subsubsection{Procedure}
% \label{subsub:methodology:expert_workshops:procedure}
% After participants signed the consent form, we conducted the workshop with a group of participants. 
% In each workshop, we introduced our framework and walked through an example intelligence everyday AR scenario to show the use case of our framework. We then asked participants to apply the framework on another scene to get a deeper understanding of the framework.
% Participants were encouraged to raise questions or give feedback at any time during the whole process.
% We repeated the process with the other two workshops, each time with an iterated version of the framework.
% met to establish an agreement on the themes. They independently coded all the data, and gathered together to refine the coded data to make sure the result achieved a inter-rater reliability $\kappa$ over 90\%.

% We then update the framework based on the final results.

\subsubsection{Results}
\label{subsub:methodology:expert_workshops:results}
Overall, experts found the framework to be \pquote{2, P6, P7}{useful} and that it would \pquote{11}{serve as a very good reference for design}.
Our framework converged as the workshops proceeded, with us receiving rich feedback during the first workshop, and participants in the last workshop only offering small suggestions. We briefly highlight the major comments that were made.

\textbf{Suggestion 1: Add Missing Pieces.}
Participants found a few factors missing in the early version of the framework.
% For example, they suggested that the \textit{System Goal} error management should also be considered for the automatic delivery (\colorwhen{\textit{when}}) of explanations if the model was uncertain.
% and that the 
\review{
For example, they pointed out that \textit{User Goal} and \textit{User Profile} needed to be considered for the \colorwhat{\textit{what}} part, and that the modality of AI output in AR needed to be taken into account for the \colorhow{\textit{how}} part.
}
% as it would indicate the need for different explanations.
They also provided suggestions on appropriate explanation content types with different system/user goals (\colorwhat{\textit{what - content}}).

\textbf{Suggestion 2: Remove Redundancy.}
Participants also found some parts unnecessarily complex. For example, four experts suggested removing the interface location from \colorhow{\textit{how}} part (\ie where to explain, mentioned in Sec.~\ref{subsub:problem_space_factors:problem_space:how}), because the location needed to be optimized with the whole interface including AI outcomes.

\textbf{Suggestion 3: Add Default Options.}
Participants provided advice for default options of different dimensions.
For instance, they recommended using the manual-trigger as the default delivery method (\colorwhen{\textit{when}}) due to users' limited cognitive capacity in AR.
% We worked with participants to summarize the advice into a set of guidelines.

\textbf{Suggestion 4: Connect across Sub-questions.}
Participants came to the consensus that the three sub-questions were interwoven.
For example, the choice of \colorwhat{\textit{what}} to explain would influence the design of \colorhow{\textit{how}} to explain, and the framework should capture and emphasize such connection.
% The interface design of the \colorhow{\textit{how}} part would also need to consider the manual-/auto-trigger mechanism for the \colorwhen{\textit{when}} part.

\textbf{Suggestion 5: Improve Visual Structure.}
Finally, participants also offered several suggestions about the visual simplification, clarification, and color choices. The figures in Appendix~\ref{sec:appendix:earlier_frameworks} show the evolution of the visual structure.

The results of the end-user study and expert workshops are complementary and guided the final version of the framework.