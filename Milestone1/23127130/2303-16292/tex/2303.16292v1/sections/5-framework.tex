\section{XAIR Framework}
\label{sec:framework}
We introduced the structure of XAIR framework in Sec.~\ref{sec:problem_space_factors} (\ie problem space and key factors), and summarized insights from end-users and experts in Sec.~\ref{sec:methodology}.
\review{
Connecting the literature survey and studies' results (\textbf{Findings 1-5} in Sec.~\ref{subsub:methodology:end_user_surveys:results}, and \textbf{Suggestions 1-5} in Sec.~\ref{subsub:methodology:expert_workshops:results}), we introduce the details of XAIR, identify how the key factors determine the design choices for each dimension in the when/what/how questions, and present a set of guidelines.}

% Connecting between Sec.~\ref{sec:problem_space_factors} and Sec.~\ref{sec:methodology}, we develop XAIR, a framework for the design of XAI in AR. We further propose a series of guidelines in company with XAIR to answer the questions in the problem space.

\subsection{\colorwhen{When to Explain?}}
\label{sub:framework:when}
We first introduce the \textit{when} part and discuss how to make a choice for delivery options. Fig.~\ref{fig:overview_when} presents an overview.

\subsubsection{\colorwhen{\textcircledmod{A} \textbf{Availability}}}
\label{subsub:framework:when:availability}
\review{The end-user survey results suggested the need for explanations in AR for the majority end-users (\textbf{Finding 1})}. A system should always generate explanations with AI outcomes and make them accessible for users, so that they can have a better sense of agency whenever they need explanations~\cite{liu2021ai,zanzotto_viewpoint_2019,lee1992trust}.

\vspace{0.2cm}\noindent\colorwhen{\textbf{\textit{G1. Make explanations always accessible to provide user agency.}}}

\subsubsection{\colorwhen{\textcircledmod{B} \textbf{Delivery}}}
% \subsubsection{B - Delivery}
\label{subsub:framework:when:delivery}
Aligned with previous work~\cite{ibili2019effect,buchner2022impact,Pielot2017a}, experts also mentioned the risk of cognitive overload in AR \review{(\textbf{Suggestion 3})}. The default option should be to wait until users manually request explanations. An example could be a button with an information icon that enables users to click on it to see an explanation. 

% \colorwhen{\textbf{\textit{G2: By default, don’t trigger explanations automatically, wait until user’s request.}}}

However, there are cases where automatically presenting just-in-time explanations is beneficial~\cite{bhattacharya2017intent,mehrotra2019jointly}. \review{We summarize the three cases based on our two studies (\textbf{Finding 2} about the importance of contextual explanations, and \textbf{Suggestion 1} about the need of considering \textit{User Goal} and \textit{User Profile})}:

1) Cases when users have an expectation mismatch and become surprised/confused about AI outcomes~\cite{dhanorkar_who_2021,brennen2020people}, \review{\ie \textit{User Goal} as Resolving Surprise/Confusion (also reflected by \textit{User State}, which could be detected by AR HMDs using facial expressions and gaze patterns ~\cite{arguel2017inside,umemuro2003detection}).}
An example could be an intelligent reminder to bring umbrella when users are leaving home on a sunny morning (but it will rain in the afternoon). Automatic explanations of the weather forecast could help resolve users' confusion.

\input{tex_fig_tab_alg/fig_when}

2) Cases when users are unfamiliar with new AI outcomes (indicated via history information of \textit{User Profile}), \eg users receive a recommendation of a song that they have never heard before. Just-in-time explanations of the reason can help users to better understand the recommendation.

3) Cases when the model's input or output confidence is low and the model may make mistakes~\cite{bertossi2020data,kenny2021explaining}, \ie \textit{System Goal} as Error Management. \review{For instance, a system turning on a do-not-disturb mode when it detects a user working on a laptop in an office when the AR-based activity recognition confidence was low (\eg 80\%).} Explanations could be a gatekeeper if the detection was wrong and users could calibrate their expectations or adjust the system to improve the detection~\cite{das2021explainable,zhang2020effect}.

All of these cases have the prerequisite that users have enough capacity to consume explanations~\cite{schmidt_transparency_2020,Pielot2015}, \eg users' cognitive load is not high (\review{could be detected via gaze or EEG on wearable AR devices~\cite{xu2018review,zagermann2018studying}}), and users have enough time to do so (inferred based on context).
% We summarize these considerations in the third guideline.

\noindent\colorwhen{\textbf{\textit{G2. By default, don’t trigger explanations automatically, wait until users' request.
Only trigger explanations automatically when both conditions are met:\\
(1) Users have enough capacity (e.g., cognitive load, urgency);\\
(2) Users are surprised/confused, or unfamiliar with the outcome, or the model is uncertain.}}}


\subsection{\colorwhat{What to Explain?}}
\label{sub:framework:what}
In Sec.~\ref{subsub:problem_space_factors:problem_space:what}, we identified that \colorwhat{\textit{content}} and \colorwhat{\textit{detail}} were two dimensions of the \colorwhat{\textit{what}} part of the framework. We introduce how to choose among all explanation content types in Fig.~\ref{fig:overview_what}.

\subsubsection{\colorwhat{\textcircledmod{A} \textbf{Content}}}
\label{subsub:framework:what:content}
In AR systems, the AI outcomes are based on factors such as \textit{User State} (\eg user activity), \textit{Contextual Information} (\eg the current environment), and \textit{User Profile} (\eg user preference). These factors also determine the content of different explanation content types.
\review{
To choose the right types, the framework lists three factors to consider and provides recommendations of personalized explanation content types based on the literature (shown as solid check marks in the top table in Fig.~\ref{fig:overview_what}), end-user survey, or expert advice (based on \textbf{Finding 3} and \textbf{Suggestion 1}, shown as hollow check marks).}

1) \textit{System Goal}. Different system goals need different explanations. For example, when a system recommends that users check out a new clothing store (User Intent Discovery), presenting {Examples} of similar stores that users are interested in and {Why} this store is attractive to users can be helpful. When a system wants to calibrate users' expectations about uncertain recipe recommendations (Error Management), showing {Examples} is less meaningful than presenting {How} and {Why} the system recommended this recipe, and {How To} change output if users want to. We leverage some literature on contextualized explanation content types to support our recommendations in the framework~\cite{lim_toolkit_2010,lim_assessing_2009,das2021explainable}.

2) \textit{User Goal}. Similarly, different user goals also require different explanations. For instance, {Certainty} explanations are helpful when users want to make sure an exercise recommendation fits their health plan (Reliability), while such explanations would be not useful when users want to be more aware of which data an AR system uses (Privacy Awareness). Most of these recommendations are supported by previous studies~\cite{lim_assessing_2009,barredo_arrieta_explainable_2020,mohseni_multidisciplinary_2021,rai2020explainable,langer_what_2021,wang_designing_2019}.
Regarding how to identify the user goals to choose explanations, designers can use their expertise to infer them in the context determined by AR systems. \review{In the future, it is also possible for AR systems to combine a range of sensor signals to detect/predict users' goals}~\cite{tsai2018augmented,admoni2016predicting,kim2016understanding}.

% As we mentioned earlier in Sec.~\ref{sec:problem_space_factors}, designers can infer these goals in a given context based on their expertise. Future AR systems may combine various sensor signals to infer users' goal~\cite{tsai2018augmented,admoni2016predicting,kim2016understanding}.

3) \textit{User Profile}, specifically user literacy with AI. For the majority of end-users who are unfamiliar with the AI techniques, we recommend only considering the four content types that users indicated that they would find useful: Input/Output, Why/Why-Not, How, and Certainty \review{(as shown in \textbf{Findings 3} and Fig.~\ref{subfig:survey_results:what}).} If users have high AI literacy, then all types could be considered~\cite{ttc_labs,ehsan2021explainable}.

Elements in \textit{System/User Goal} are not exclusive to each other. If there is more than one goal, these columns can be merged within each factor section to find the union (\ie content types checked in at least one column). Then, one can find the intersection among the three factors' content type sets (\ie overlapping types in all sets) to ensure that these explanations can fulfill all factors simultaneously. We show complete examples in later sections (Sec.~\ref{sec:applications} and Sec.~\ref{sec:evaluation}).
% These considerations lead to our next guideline.

\input{tex_fig_tab_alg/fig_what}

\colorwhat{\textbf{\textit{G3. To determine personalized explanation content, consider three factors: system goal, user goal, and user profile.}}}

\subsubsection{\colorwhat{\textcircledmod{B} \textbf{Detail}}}
\label{subsub:framework:what:detail}
After selecting the appropriate content, default explanations need to be concise and can be further simplified by highlighting the most important types~\cite{buchner2022impact,baumeister2017cognitive}. General end-users are primarily interested in Why, \review{which is in line with experts' advice (\textbf{Suggestion 3} about default options) and previous literature~\cite{jiang_who_2022,lim_assessing_2009,lim_toolkit_2010}.}
Designers' can leverage their expertise to determine whether other types should be omitted or combined with Why in a specific context.

\colorwhat{\textbf{\textit{G4. By default, display concise explanations with top types. Prioritize Why, and choose other types based on the context.}}}

\review{As a large proportion of Study 1 participants indicated that detailed explanations could be useful (\textbf{Finding 4}), AR systems need to provide an easy portal (an interface widget such as a button) for end-users to explore more details. This can also provide user agency~\cite{lu_exploring_2022}.}

\colorwhat{\textbf{\textit{G5. Always provide users opportunities for agency with the option to explore more detailed explanations upon request.}}}



% Essentially it is a recommendation system with additional input from AR~\cite{da2020recommendation,manotumruksa2018contextual}
% Mix-initiative~\cite{horvitz_principles_1999}
% Personalization \& Customization~\cite{cui2020personalized,qian2013personalized}

% Focus of local explanations for end-users~\cite{lakkaraju2019faithful} Global for experienced \cite{dhanorkar_who_2021,long_what_2020}

% Different types of explanations~\cite{barredo_arrieta_explainable_2020,langer_what_2021,wang_designing_2019}


% Even after selecting an appropriate category subset, showing all of them can be overwhelming for end users, especially with limited cognitive capacity in AR~\cite{buchner2022impact,baumeister2017cognitive}. A summary with key explanation elements would be needed


% ``Findings include only showing explanations that extend prior knowledge~\cite{coppers2018intellingo}, and not to be too ``creepy'' by disclosing too much information~\cite{esteva2017dermatologist}.''~\cite{wang_designing_2019}

\subsection{\colorhow{How to Explain?}}
\label{sub:framework:how}
Finally, we introduce the \colorhow{\textit{how}} part and elaborate from the \colorhow{\textit{modality}} and \colorhow{\textit{paradigm}} perspective (see Fig.~\ref{fig:overview_how}).

\input{tex_fig_tab_alg/fig_how}

\subsubsection{\colorhow{\textcircledmod{A} \textbf{Modality}}}
\label{subsub:framework:how:modality}
Considering channel bandwidth, the visual and audio modalities are the two most feasible modalities for AR.
Since explanations usually come during or after AI outcomes, to maintain consistency, the default modality of an explanation should be the same\review{(\textbf{Finding 5} and \textbf{Suggestion 3}).}
In cases when outcomes use a haptic modality (\eg vibration as a reminder), audio channels could be used as necessary (although this should be rare), since the choice of the haptic channel already conveys the need to be subtle.

However, there are also cases where one modality could be overloaded (based on \textit{User State} and \textit{Contextual Information}). For example, when users are driving and a navigation app suggests an alternate detour route, although the AI outcome is visual, the explanation should be audio to avoid visual overload.
When users are in a loud environment, a vibration-based AI outcome needs to use the visual modality for explanations.
\review{These scenarios can be easily detected by AR HMDs.}

\colorhow{\textbf{\textit{G6. By default, adopt the same explanation modality as that of the AI output (except for haptic$\rightarrow$audio). When one modality’s load is high, use another modality.}}}

Note that the modality choice also applies to the manual-trigger case when explanations are not automatically delivered (\gtwo), \eg a button icon for visual modality, a voice trigger for audio modality.

\subsubsection{\colorhow{\textcircledmod{B} \textbf{Paradigm}}}
\label{subsub:framework:how:paradigm}

Experts agreed that the audio design space does not belong within this framework. 
\review{For visual design, after removing the location from our framework (\textbf{Suggestion 2} of redundancy removal)}, we mainly focused on two aspects: \colorhow{format} and \colorhow{pattern}.
Depending on the content (\gfour), the explanation \colorhow{format} can be textual~\cite{lakkaraju2016interpretable,myers2006answering}, graphical~\cite{zeiler2014visualizing,simonyan2013deep}, or both.
Based on the consensus of experts in Study 2, text should be the primary format.
Experts suggested several reasons for this. Text takes up less space in a limited AR interface, and can introduce relatively less cognitive load. Moreover, the textual format is more universal and can cover all types.
Graphics can be used as the secondary format.
For default explanations (\gfive), in addition to displaying a short and concise textual paragraph, simple graphics such as icons can be used to provide additional information.
For detailed explanations (\gsix), more complex graphical formats (\eg example images or heatmaps) can be used as long as they are easy for end-users to understand.

\colorhow{\textbf{\textit{G7. [Visual] Use text as the primary format. Only use graphics if they are easy to understand.}}}

Independent of the \colorhow{format}, explanations can be presented in an implicit or explicit \colorhow{pattern}~\cite{lindlbauer_context-aware_2019,tatzgern_adaptive_2016}. Given the capability of depth sensing and 3D registration in AR, we recommend using the implicit pattern when the explanation content is compatible with the environment (\ie can be naturally embedded as a part of the environment).
For example, for book recommendations, a text cue or a small icon can float on the book to indicate the book's topic that users like (belonging to the Why explanation content type).
When explanations and the environment are not compatible, using an explicit pattern (\eg a dialogue window) can be the back-up option.
With regard to what explanation content is compatible with the environment, designers can leverage their expertise and intuition to propose appropriate embedding patterns for given a context. \review{Future AR systems may first understand the environment using object detection and context recognition algorithms, and then utilize techniques such as knowledge graphs (\ie networks of real-world entities and their relationships)~\cite{chen2020review} to assess the compatibility between the content and the environment.}

\colorhow{\textbf{\textit{G8. [Visual] Use implicit patterns if content can be embedded in the environment. Otherwise, use explicit patterns.}}}
\\

\noindent XAIR can not only serve as a summary of the study findings and the multidisciplinary literature across XAI and HCI, but also guide effective XAI design in AR.
In the next two sections, we provide examples of XAIR-supported applications (Sec.~\ref{sec:applications}), and evaluate XAIR from both designers' and end-users' perspectives (Sec.~\ref{sec:evaluation}).
% In Sec.~\ref{sec:evaluation}, we evaluate our framework from various perspectives.
