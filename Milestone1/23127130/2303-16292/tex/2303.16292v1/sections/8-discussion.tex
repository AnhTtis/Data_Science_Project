\section{Discussion}
\label{sec:discussion}
XAIR defines the problem space structure of XAI design in AR and details the relationship that exists between the factors and the problem space. By highlighting the key factors that designers need to consider and providing a set of design guidelines for XAI in AR, XAIR not only serves as a reference for researchers, but also assists designers by helping them propose more effective XAI designs in AR scenarios.
The two evaluation studies in Sec.~\ref{sec:evaluation} illustrated that XAIR can inspire designers with more design opportunities and lead to transparent and trustworthy AR systems.
\review{In this section, we discuss how researchers and designers can apply XAIR, as well as potential future directions of the framework inspired by our studies. We also summarize the limitations of this work.}

\review{
\subsection{Applying XAIR to XAI Design for AR}
\label{sub:discussion:procedure}
Researchers and designers can make use of XAIR in their XAI design for AR scenarios by initially using their intuition to propose an initial set of designs.
Then, they can follow the framework to identify five key factors: \textit{User State}, \textit{Contextual Information}, \textit{System Goal}, \textit{User Goal}, and \textit{User Profile}. The example scenarios in Sec.~\ref{sec:applications} and Sec.~\ref{sec:evaluation} indicate how these factors can be specified.
Based on these factors, they would then work through the eight guidelines of \colorwhen{\textit{when}}, \colorwhat{\textit{what}}, and \colorhow{\textit{how}}, using Fig.~\ref{fig:overview_when}-Fig.~\ref{fig:overview_how} to inspect their initial design and make modifications if there is anything inappropriate or missing. Low-fidelity storyboards or prototypes of the designs can be tested via small-scale end-user evaluation studies. This would be an iterative process.
In the future, when sensing and AI technologies are more advanced, it is promising that the procedures of identifying factors and checking guidelines could be automated.
}

\subsection{Towards An Automatic Design Recommendation Toolkit}
\label{sub:discussion:toolkit}
In Study 3, more than one user mentioned the possibility of converting the framework into an automatic toolkit. 
For example, P3 was thinking aloud when using XAIR in the study, \textit{``If this framework is described as an algorithm, the five key factors can be viewed as the input of the algorithm... and the output is the design of the three questions.''}
There are a few decision-making steps in the current framework that involve human intelligence. For example, when designing the default explanations in \colorwhat{what - detail}, designers need to consider users' priority under a given context to determine which explanation content type to highlight. When picking the appropriate visual \colorhow{paradigm}, designers need to determine whether the explanation content is more appropriate in a textual or graphical format, as well as whether the content can be naturally embedded within the environment.
Assuming future intelligent models can assist with these decisions, XAIR could be transformed into a design recommendation tool that could enable designers and researchers to experiment with a set of \textit{User State}, \textit{Contexts}, \textit{System/User Goals}, and so on. 
This could achieve a more advanced version of XAIR, where XAIR are fully automated as an end-to-end model: determining the optimal XAI experience by inferring the five key factors in real time.
This is an appealing direction. However, although factors such as \textit{Context} and \textit{System Goal} are easier to predict with a system, the inference of \textit{User State/Goal} is still at an early research stage~\cite{arguel2017inside,duchowski_index_2018,huang2018predicting}. Moreover, extensive research is needed to validate the adequacy and comprehensiveness of the end-to-end algorithm.
This also introduces the challenge of nested explanations in XAIR (\ie explaining explanations)~\cite{mittelstadt2019explaining}, which calls for further study.

\subsection{The Customized Configuration of XAI Experiences in AR}
\label{sub:discussion:config}
The experts in Study 2 and the designers in Study 3 brought up the need for end-user to control XAI experiences in AR, \eg
\pquote{12, Study 2}{XAIR can provide a set of default design solutions, and users could further customize the system} and
\pquote{8, Study 3}{I personally agree with the guidelines, but I can also imagine some users may want different design options. So there should be some way that allows them to select when/what/how... For example, a user may want the interface to be in an explicit dialogue window all the time \text{[related to \colorhow{how}]}. We should support this.}
This need for control suggests that to achieve a personalized AR system, designers should provide users with methods to configure their system, so that they can set up specific design choices to customize their XAI experience.
Such personalization capabilities may also be used to support people with accessibility needs (also mentioned by P2 in Study 3), \eg visually impaired users can choose to always use the audio \colorhow{modality}.

% ``To this date, there is no agreement in prior work on whether to include all details of system logic in explanation interfaces. Moreover, there might not be a universal answer to this question, but it might rather depend on the product domain, user groups, and the context of use.''~\cite{eiband_bringing_2018}

\subsection{User-in-The-Loop and Co-Learning}
\label{sub:discussion:co_learning}
During the iterative expert workshops (Study 2, Sec.~\ref{sub:methodology:expert_workshops}), experts mentioned an interesting long-term co-learning process between the AR system and a user.
On the one hand, based on a user's reactions to AI outcomes and explanations, a system can learn from the data and adapt to the user. Ideally, as the AR system better understands the user, the AI models would be more accurate, thus reducing the need for mistake-related explanations (\eg cases where \textit{System Goal} as Error Management).
On the other hand, the user is also learning from the system. \pquote{4, Study 2}{Users' understanding of the system and AI literacy may change as they learn from explanations}. This may also affect the user's need for explanations. For example, the user may have less confusion (\textit{User Goal} as Resolving Surprise/Confusion) as they become more familiar with the system. Meanwhile, they may become more interested in exploring additional explanation types (\textit{User Goal} as Informativeness).
Such a long-term and co-learning process is an interesting research question worth more exploration.

\subsection{Limitations}
\label{sub:discussion:limitations}
There are a few limitations to this research.
First, although we highlighted promising technical paths within the framework in Sec.~\ref{sec:framework}, XAIR does not involve specific AR techniques. The real-time AR system in Study 4 implemented the ingredient recognition and recipe recommendation modules, but the detection of user state/goal was omitted.
\review{
Second, our studies might have some intrinsic biases. For example, Study 1 only involved AR recommendation cases. Since everyday AR HMDs are still not widely adopted in daily life, we grouped 500+ participants only based on AI experience instead of AR experience. The experts and designers of our studies were all employees of a technology company. Study 4 only evaluated two specific proposals from designers. Moreover, as there is no previous XAI design in AR, we were only able to compare our XAIR-based system against a baseline without explanation.
}
Third, other than when, what, and how, there could be more aspects in the problem space, \eg who and where to explain. Moreover, XAIR mainly focuses on non-expert end-users. Other potential users, such as developers or domain experts, were not included.
% There are other AR-specific information presentation frameworks that can be further incorporated~\cite{muller2016taxonomy,luo2022should}.
The scope of the five key factors may also not be comprehensive. For example, we do not consider user trust in AI, which is a part of \textit{User Profile} that may be dynamic along with user-system interaction.
These could limited the generalizability of our framework, but also suggests a few potential future work directions to expand and enhance XAIR.