\section{Evaluation}
\label{sec:evaluation}

In addition to showing examples to illustrate the use case of XAIR, we also conducted two user studies to evaluate XAIR.
The first study was from the perspective of designers (as XAIR users) to evaluate XAIR's ability to assist designers during their design processes (Sec.~\ref{sub:evaluation:utility}).
The second study was from an end-user perspective and evaluated XAIR's effectiveness at achieving a user-friendly XAI experience in AR. We measured the usability of the real-time AR experiences that were developed based on the design examples proposed by designers (Sec.~\ref{sub:evaluation:effectiveness}).
% The results indicate positive utility and strong effectiveness of XAIR.

\subsection{Study 3: Design Workshops}
\label{sub:evaluation:utility}
We conducted one-on-one design workshops with designers to investigate whether the framework could support their design processes, inspire them to identify new design opportunities, and achieve effective designs.

\subsubsection{Participants}
\label{subsub:evaluation:utility:participants}
Future XAI and AR designers can come from various backgrounds, so we recruited 10 participants (4 Female, 6 Male, Age 32 $\pm$ 6) from a technology company as volunteers.
Three were XAI algorithm researchers, four were product designers, and three were HCI/AR researchers.
All participants were familiar with AI and AR, and none had participated in previous studies.

\subsubsection{Design and Procedure}
\label{subsub:evaluation:utility:materials}
We prepared two AR scenarios, both related to recipe recommendations while preparing meals.

\textbf{Case 1: Reliable Recipe Recommendation}.
Michael works in a sales company (general end-user, low digital literacy). He recently started a high-protein diet due to his workout routine. He opens the fridge and wants to make lunch. His AR glasses present a window on the fridge door and recommend an option that Michael usually has, but Michael wants to make sure that this option fits his recent diet changes.

\textbf{Case 2: Wrong Recipe Recommendation}.
Mary works in an AI company (high AI literacy) and has friends coming over for dinner, who are beef lovers. She opens the fridge and sees steak. However, her AR glasses mistakenly recognize steak as salmon with a medium level of confidence~\footnote{If the system has low-level confidence, the expected cost of making mistakes will be higher than the cost of asking for users' input, so the system should ask for users' confirmation about the ingredients they have on hand before presenting recommendations (\eg asking ``Is this salmon or steak?''). In this scenario, the confidence is at the medium level, thus the system provides recommendations, but  is still aware of the potential to make mistakes.}, and recommends a few recipes that use salmon. She is confused and wonders how she can correct the recommendations.

Since generating explanations is not the focus of the framework, we prepared examples for the seven explanation content types (Appendix~\ref{sub:appendix:details_scenarios:more_details_of_study}). Participants were free to use our examples, or propose their own (without the need to design how an algorithm could generate them).

% We adopted a process similar to the within-subject design: 
Participants first used their expertise and intuition to propose XAI designs for the two cases before being shown the framework. They spent 10 minutes on each case.
Participants were encouraged to think aloud and describe their design via text and simple sketches.
Then, after XAIR was introduced, they spent another 10 minutes following the three parts and eight guidelines and applied them to the two cases, resulting in another version of the design. The order of the two cases was counterbalanced.

To quantify the utility of XAIR, we employed the Creativity Support Index (CSI, 1-10 Likert scale)~\cite{cherry2014quantifying} and System Usability Scale (SUS)~\cite{bangor2008empirical}.
Since both scales were originally designed for tools or systems, the language was modified from ``tools'' and ``system'' to ``framework'' and ``guidelines''.
At the end of the workshop, we conducted a semi-structured interview that began with the question: ``Do you think the framework and guidelines are helpful? If so, in what aspects they are helpful?''
Each workshop lasted 90 minutes. Two researchers independently coded the qualitative data using thematic analysis and discussed it to reach an agreement.


% \subsubsection{Procedure}
% \label{subsub:evaluation:utility:procedure}
% We introduced the background and two scenarios. Participants first spent around 10 minutes on each task and proposed designs of the three sub-questions from scratch.
% After we presented our framework and guidelines, participants spent another 10 minutes on each task and proposed a new version.
% They completed the questionnaire and did the interview after finishing the designs.

% \input{tex_fig_tab_alg/fig_design_examples}

\subsubsection{Design Results}
\label{subsub:evaluation:utility:design_results}
After using XAIR, nine out of ten participants modified their designs and preferred the updated version. One participant (P7) liked the design as it was and thought that the framework \textit{``perfectly supported the design''}.
Consistency was found among the designs, which indicated that XAIR could effectively guide users through the design process.
For example, Tab.~\ref{tab:design_example_case1} presents two designers' designs (images are rendered based on their proposals) of the reliable recommendation case. Their designs of the \colorwhen{\textit{when}} part and most of the \colorwhat{\textit{what}} part were the same.
Tab.~\ref{tab:design_example_case2} presents another two designers' designs of the wrong recommendation case (Case 2). Similarly, we also found consistent design choices between the two examples.

\input{tex_fig_tab_alg/tab_design_examples_case1}

Meanwhile, we also found variance across participants' designs.
For instance, in Case 1, P6 had a different consideration of \textit{User State} than P2, in which P6 brought up a case where the user could hold something in their hand. In this case, P6 adopted the audio modality for manual trigger (the rightmost column of Tab.~\ref{tab:design_example_case1}).
Moreover, as shown in the rightmost column of Tab.~\ref{tab:design_example_case2}, P9 proposed an interesting tweak that always highlighted ingredients (Input explanation type). Her reason was that it introduced \textit{``ultra-low cognitive cost''}, thus there was no need to check the second auto-trigger condition. \textit{``I don't think it is a violation of the guideline. Instead, I was inspired by the framework to consider this case.''}
This reveals that XAIR is flexible and can support the diverse creativity of users.

\input{tex_fig_tab_alg/tab_design_examples_case2}

% We present design examples with participants' thinking processes.

% \noindent\textbf{Example 1 of Case 1 - P2}.
% Fig.~\ref{subfig:design_examples_task1:example1} presents the overall design of Case 1 proposed by P2 after using XAIR.

% \colorwhen{\textit{\textbf{When}}}.
% At first, P2 proposed to trigger explanations together with recommendations automatically. However, he changed the design after using XAIR. P2 thought that Michael's \textit{User Goal} was to ensure the recipes went along with his diet (Reliability), and the \textit{System Goal} was to help Michael find suitable recipes (User Intent Assistant). Moreover, Michael was pretty familiar with these recommendations. Therefore, the second condition of \gthree\ was not met, and P2 agreed that the manual-trigger was a better idea for \colorwhen{delivery
% } (\gtwo).

% \colorwhat{\textit{\textbf{What}}}.
% Before using XAIR, P2 suggested using Why/Why-Not category as the explanation content. He did not think of the second \colorwhat{detail} dimension.
% Using XAIR, he identified the three factors (\ie the goals of the system and the user, plus Michael had a low AI literacy), referred to the table in Fig.~\ref{fig:overview_what}, and narrowed down the \colorwhat{content} to be Input/Output, and Why/Why-Not (\gfour).
% P2 liked \gfive\ and \gsix\ as they reminded him about this missing part.
% Besides prioritizing Why, P2 also proposed a short paragraph by merging the Input category as he thought explaining ingredients was also important.
% As for detailed explanations, he decided to show the list of the two explanation categories in detail.

% \colorhow{\textit{\textbf{How}}}.
% P2's original design was to display explanations in the text under the recipe recommendation. This design was supported and formalized by XAIR.
% The visual \colorhow{modality} was in line with the recommendation interface, \ie a window on the fridge door (\gseven). Considering \gtwo, P2 designed a simple button icon to support the manual trigger.
% Following \geight, the textual \colorhow{format} was appropriate to explain the Why/Why-Not and Input/Output reasons.
% Since a textual paragraph was not easily compatible with the environment, P2 adopted the explicit \colorhow{pattern} that presented texts in the same window as the recommendations (\gnine).
% P2 was satisfied with the design and glad that the framework could confirm his design.
% \\

% \noindent\textbf{Example 2 of Case 1 - P6}.
% We present another design example of Case 1 by P6. A large proportion of P6's design of Case 1 is consistent with that of P2. We mainly highlight the differences. Fig.~\ref{subfig:design_examples_task1:example2} presents P6's design.

% \colorwhen{\textit{\textbf{When}}}.
% Same as P2, P6 also examined the conditions and chose the manual-trigger option for \colorwhen{delivery} (\gtwo).

% \colorwhat{\textit{\textbf{What}}}.
% P6 selected the same categories as Input/Output and Why/Why-Not, and adopted the same detailed explanations design. However, for the default concise explanations in the \textit{detail} dimension, P6 mainly focuses on the Why part and proposed to add an icon to indicate the high-protein feature (also about the Why explanation, \gfive).

% \colorhow{\textit{\textbf{How}}}.
% P6 proposed the same visual \colorhow{modality} and explicit \colorhow{pattern}.
% Moreover, he also brought up an interesting case when Michael's hands were busy holding ingredients. In this case, P6 proposed to support an audio trigger as an alternative (\gseven).
% As for the \colorhow{format}, in addition to using texts as the primary format, P6 further proposed to use a simple icon as a secondary format to support explanations (\gfive\ and \geight).
% \\



% \noindent\textbf{Example 1 of Case 2 - P5}.
% We then present two examples of Case 2. Fig.~\ref{subfig:design_examples_task2:example1} shows P5's design after using XAIR.

% \colorwhen{\textit{\textbf{When}}}.
% P5 speculated that the major changes between the two cases include: the \textit{System Goal} (User Intent Assistance for finding a good recipe for friends, and Error Management for mid-level confidence), the \textit{User Goal} (Resolve Confusion), and the \textit{User Profile} (Mary's high AI literacy, friends' food preference).
% Both conditions of \colorwhen{G3} were fulfilled. Thus P5 chose to \colorwhen{deliver} explanations automatically. This was in line with P5's design before using XAIR.

% \colorwhat{\textit{\textbf{What}}}.
% Prior to knowing XAIR, P5 picked How-To as the explanation. This \colorwhat{content} design changed after referring to \gfour's table. The three factors led to five categories, including Input/Output, Why/Why-Not, How-To, Certainty, and How.
% For default explanations (\gfive), in addition to Why, P5 proposed to color-code the Input to emphasize the ingredient with the mid-level confidence (Certainty), and to add a simple selection-based way to allow Mary to change the salmon (How-To).
% For detailed explanations (\gsix), P5 proposed to use a drop-down menu to show the five categories.

% \colorhow{\textit{\textbf{How}}}.
% P5 followed \gseven\ to \gnine and chose to use visual \colorhow{modality}, textual \colorhow{format}, and explicit \textit{pattern} to present explanations. 
% These choices were consistent with his original design.
% \\


% \noindent\textbf{Example 2 of Case 2 - P9}.
% We show another example of Case 2 designed by P9, as visualized in Fig.~\ref{subfig:design_examples_task2:example2}.

% \colorwhen{\textit{\textbf{When}}}.
% P9 had a similar analysis of the \textit{System Goal} and \textit{User Goal} as P5, and loved how \gone-\gthree\ led to the choice of auto-trigger of explanations. She further proposed an interesting tweak of \colorwhen{delivery}: always spotlighting ingredients by showing simple information around them (Input category, \gfour), since it introduced \textit{``ultra-low cognitive cost, thus didn't need to follow G3''}.

% \colorwhat{\textit{\textbf{What}}}.
% P9 proposed the consistent category list following \gfour, and thus suggested the same detailed explanations design as P5 (\gsix). She decided to present Why and How-To as the default explanations. Moreover, as mentioned in the \colorwhen{\textit{when}} part, P9 also proposed to show names and recognition certainty on ingredients as ``\textit{low-cost}'' explanations.

% \colorhow{\textit{\textbf{How}}}.
% The three sub-questions in P9's design are closely related. Besides displaying main textual explanations explicitly with recommendations, P9's proposed to use simple \colorhow{graphics} in an implicit \colorhow{pattern} for low-cost explanations.

\input{tex_fig_tab_alg/fig_evaluation_scores}

\subsubsection{Feedback Results}
\label{subsub:evaluation:utility:feedback_results}
Participants provided positive feedback about the framework. Eight participants explicitly commented that XAIR was \textit{``useful/helpful''}.
The results of the CSI scores (Fig.~\ref{fig:workshop_utility_csi}) and the SUS scores (74 $\pm$ 6 out of 100, indicating good usability) both illustrate the good utility of XAIR.
Four themes emerged in participants' feedback.

\textit{The Framework as a Useful and Comprehensive Reference.}
Consistent with the feedback from the experts in Study 2 (Sec.~\ref{sub:methodology:expert_workshops}), participants also found that the framework was a valuable handbook. For example, \pquote{4}{This framework is an excellent reference point for people getting started designing XAI experiences... to check if they have missed things} and \pquote{7}{I may not use it for every design decision, but I would refer to it when I want to make sure that I have considered everything.}
The comprehensiveness of XAIR thus helped participants perform a sanity check of their designs.

\textit{Design Opportunity Inspiration.}
Participants also leveraged XAIR to inspire new ideas.
P6's original design did not consider the case where users' hands could be busily holding ingredients. But the modality in the \colorhow{how} part inspired him, \ie
\textit{``The framework reminded me to realize potential alternatives. It inspired me to think about not just one design, but a set of designs.''}
Moreover, participants found that XAIR could help generate baseline designs. \pquote{8}{I could then further customize it for various scenarios.}
The high scores for exploration (7.9$\pm$0.4 out of 10) and expressiveness (7.1$\pm$0.6) on the CSI also support this observation.

\textit{Backing Up Design Intuitions.}
Some participants also found that the guidelines in XAIR could support their intuition.
For instance, P7 did not change her design after using XAIR, but was very excited to see the alignment, \eg \textit{``Sometimes I am not sure whether my design intuition is right. It feels great that the framework can support it.''}
This could be part of the reason for the positive enjoyment score on the CSI (8.0$\pm$0.4).

\textit{Time to Learn The Framework.}
Participants also commented that XAIR incorporates a lot of information and that they needed time to digest it, \eg \pquote{10}{I need to go back and forth between the visual diagrams} and \pquote{4}{the table \text{[in Fig.~\ref{fig:overview_what}]} is useful but also pretty complex}.
This may explain the relatively low immersion score (4.4$\pm$0.5) on the CSI. Moreover, six participants Agreed or Strongly Agreed in response to the question \textit{``Need to learn a lot...''} on the SUS.
On the one hand, this shows XAIR's comprehensiveness (covering multiple research domains), whereas on the other hand, this illuminates future directions to convert XAIR into a design tool.

% \textit{Potential of An Automatic Framework.}
% Interestingly, several participants asked about the potential of using XAIR as an automatic toolkit. 
% For example, P3 was thinking aloud when using XAIR in the study, \textit{``If this framework is described as an algorithm, the five key factors can be viewed as the input of the algorithm... and the output is the design of the three questions.''}
% We elaborate more on this topic in the discussion section.

\subsection{Study 4: Intelligent AR System Evaluation}
\label{sub:evaluation:effectiveness}
To demonstrate XAIR's effectiveness, we show that the designers' proposals using XAIR could achieve a positive XAI user experience in AR for end-users.
Based on the designs proposed in Study 3, we took one example from each case and implemented a real-time intelligent AR system. We then evaluated the system's usability.

\subsubsection{System Implementation}
We selected one reliable recipe recommendation example from the left of Tab.~\ref{tab:design_example_case1} and one wrong recipe recommendation example from the left of Tab.~\ref{tab:design_example_case2}. We then instantiated the examples by implementing a real-time system on a Microsoft Hololens V2.
The system had three major modules: a recognition module, a recommendation module, and an interface module.

For ingredient recognition, we trained a vision-based object detection model that was a variant of the Vision Transformer from CLIP~\cite{radford2021learning} on the LVIS~\cite{gupta2019lvis} and Objects365~\cite{shao2019objects365} datasets.
We then added ImageNet22k and performed weakly-supervised training with both box and image level annotations~\cite{zhou2022detecting}.
The top 50 ingredient-related classes from LVIS were retained, with an average F1 score of 81.1\%.
The model was run on Hololens' egocentric camera stream at 5 FPS to recognize ingredients.
The model was used in Case 1, while in Case 2, misrecognition (\ie recognizing steak as salmon) was manually inserted to create the designed experience.

For recipe recommendation, the Spoonacular Food API~\cite{spoonacular} was used to obtain potential recipes given a set of ingredients. We then implemented an algorithm to rank the recipes based on user preference and recommend the top recipes (\eg if a user prefers food that is fast to prepare, the recipes are sorted based on the cooking time). For the explanations, we developed a template-based explanation generation technique~\cite{zhang_explainable_2020} to cover different 
types.

Finally, the interface followed the designs in Tab.~\ref{tab:design_example_case1} and Tab.~\ref{tab:design_example_case2}.
Clicking on one recipe's image would show the detailed instructions. An icon button under each recipe could be triggered to present short default explanations, followed by another button to display detailed explanations as a list of content types.

\input{tex_fig_tab_alg/fig_enduser_study}

\subsubsection{Participants and Apparatus}
Twelve participants (5 Female, 7 Male, Age 32 $\pm$ 3) volunteered to join the study. None of the them had participated in previous studies. The two cases had the same setup (except for the recognition error). \review{We prepared a number of food ingredients on a shelf (including steak, but no salmon) to simulate the opening-a-fridge moment, as shown in Fig.~\ref{subfig:study_end_user_eval:setup}.}

\subsubsection{Design and Procedure}
% In each task, we had the availability of explanation as the main factor. 
Since there is no existing XAI design for AR systems, we compared the design examples with a baseline condition that only presented recommendations without explanations.
Note that for Case 2's baseline condition, participants could still change the output by clicking a button that said ``Doesn't seem right? Click to see the next batch.'' to ensure a fair comparison \footnote{Another baseline could have been to compare against designers' old designs before using XAIR. However, we did not include this baseline since designers already explicitly preferred the new version that they created after using XAIR.}.

We used a within-subject design. Participants started with one case and completed both conditions. They took a break and completed a questionnaire to compare the two conditions. Then, they completed the two conditions in the second case and completed a similar questionnaire. The case order was counterbalanced. The study took about 30 minutes and ended with a brief interview.

The questionnaire contained six questions (1-7 Likert scale) comparing the two conditions. Three were from the XAI literature and measured the explanations' effect on the system's intelligibility, transparency, and trustworthiness. The other three questions asked about participants' preferences towards the design choices of \colorwhen{\textit{when}}, \colorwhat{\textit{what}}, and \colorhow{\textit{how}}\footnote{Since a factorial study design to compare all XAIR design options would involve a large number of conditions (\ie 2 options of \colorwhen{\textit{when}} $\times$ at least 2 options of \colorwhat{\textit{what}} $\times$ 2 options of \colorhow{\textit{how}}), asking participants to undergo several scenarios would be too costly. Order effects would also be hard to counterbalance. So the three questions about when/what/how described other design choices by showing examples and asked about participants' preferences. For instance, in Case 1's \colorwhen{\textit{when}} part, participants rated how much they agreed with the claim ``I prefer to have explanations triggered manually by me, compared to being triggered automatically.'', or vice-versa in Case 2.}.
The SUS was also administered to measure the usability of the system with explanations.

\subsubsection{Results}
Participants strongly preferred the condition with explanations in both cases, especially Case 2, \eg
\pquote{2}{Seeing the explanation automatically when the AR system makes mistakes is very helpful. It lets me know when I should adjust my expectation} and \pquote{9}{the mistake \text{[in Case 2]} is understandable... salmon and steak can have similar colors and shapes. But if I didn't see the explanation, I would be very confused.}
This sentiment was also reflected in participants' high rating of the system's intelligibility, transparency, and trustworthiness with the explanation (Fig.~\ref{subfig:study_end_user_eval:scores}).
Moreover, the AR system received high SUS scores: 86 $\pm$ 3 in Case 1, and 80 $\pm$ 3 in Case 2, both indicating excellent usability of the system.
Participants also liked the design of the system, which was supported by the positive ratings for the when/what/how questions (see Fig.~\ref{subfig:study_end_user_eval:scores}).
\review{
These results demonstrated that compared to the baseline, XAI design using XAIR can effectively improve the transparency and trustworthiness of AR systems for end-users.
}