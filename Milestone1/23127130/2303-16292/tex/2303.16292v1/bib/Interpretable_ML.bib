
@article{Shrikumar2017,
	title = {Learning important features through propagating activation differences},
	volume = {7},
	abstract = {The purported "black box"' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.},
	journal = {34th International Conference on Machine Learning, ICML 2017},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	year = {2017},
	note = {ISBN: 9781510855144},
	pages = {4844--4866},
	file = {Shrikumar et al_2017_Learning important features through propagating activation differences.pdf:/Users/orsonxu/Zotero/storage/SJNUF7A6/Shrikumar et al_2017_Learning important features through propagating activation differences.pdf:application/pdf;Shrikumar et al_2017_Learning important features through propagating activation differences.pdf:/Users/orsonxu/Zotero/storage/Y6NZD2BJ/Shrikumar et al_2017_Learning important features through propagating activation differences.pdf:application/pdf},
}

@article{Ribeiro2016,
	title = {"{Why} should i trust you?" {Explaining} the predictions of any classifier},
	volume = {13-17-Augu},
	doi = {10.1145/2939672.2939778},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = {2016},
	note = {ISBN: 9781450342322},
	keywords = {Unread},
	pages = {1135--1144},
	file = {Ribeiro et al_2016_Why should i trust you.pdf:/Users/orsonxu/Zotero/storage/547JULV7/Ribeiro et al_2016_Why should i trust you.pdf:application/pdf;Ribeiro et al_2016_Why should i trust you.pdf:/Users/orsonxu/Zotero/storage/QJFHN79Y/Ribeiro et al_2016_Why should i trust you.pdf:application/pdf},
}

@article{Lee2020,
	title = {Explanation-{Based} {Tuning} of {Opaque} {Machine} {Learners} with {Application} to {Paper} {Recommendation}},
	url = {http://arxiv.org/abs/2003.04315},
	abstract = {Research in human-centered AI has shown the benefits of machine-learning systems that can explain their predictions. Methods that allow users to tune a model in response to the explanations are similarly useful. While both capabilities are well-developed for transparent learning models (e.g., linear models and GA2Ms), and recent techniques (e.g., LIME and SHAP) can generate explanations for opaque models, no method currently exists for tuning of opaque models in response to explanations. This paper introduces LIMEADE, a general framework for tuning an arbitrary machine learning model based on an explanation of the model's prediction. We apply our framework to Semantic Sanity, a neural recommender system for scientific papers, and report on a detailed user study, showing that our framework leads to significantly higher perceived user control, trust, and satisfaction.},
	author = {Lee, Benjamin Charles Germain and Lo, Kyle and Downey, Doug and Weld, Daniel S.},
	year = {2020},
	file = {Lee et al_2020_Explanation-Based Tuning of Opaque Machine Learners with Application to Paper Recommendation.pdf:/Users/orsonxu/Zotero/storage/PQV7AH7G/Lee et al_2020_Explanation-Based Tuning of Opaque Machine Learners with Application to Paper Recommendation.pdf:application/pdf;Lee et al_2020_Explanation-Based Tuning of Opaque Machine Learners with Application to Paper Recommendation.pdf:/Users/orsonxu/Zotero/storage/TAIACMZH/Lee et al_2020_Explanation-Based Tuning of Opaque Machine Learners with Application to Paper Recommendation.pdf:application/pdf},
}

@article{Lundberg2017,
	title = {A unified approach to interpreting model predictions},
	volume = {2017-Decem},
	issn = {10495258},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	number = {Section 2},
	journal = {Advances in Neural Information Processing Systems},
	author = {Lundberg, Scott M. and Lee, Su In},
	year = {2017},
	keywords = {Unread},
	pages = {4766--4775},
	file = {Lundberg_Lee_2017_A unified approach to interpreting model predictions.pdf:/Users/orsonxu/Zotero/storage/Z5RGT97L/Lundberg_Lee_2017_A unified approach to interpreting model predictions.pdf:application/pdf;Lundberg_Lee_2017_A unified approach to interpreting model predictions.pdf:/Users/orsonxu/Zotero/storage/KFHPZATU/Lundberg_Lee_2017_A unified approach to interpreting model predictions.pdf:application/pdf},
}

@article{Gordon2021,
	title = {The {Disagreement} {Deconvolution} : {Bringing} {Machine} {Learning} {Performance} {Metrics} {In} {Line} {With} {Reality}},
	journal = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems - CHI '21},
	author = {Gordon, Mitchell L and Zhou, Kaitlyn and Bernstein, Michael S},
	year = {2021},
	note = {ISBN: 9781450380966},
	annote = {- RQs and Contributions

incorporate an estimate ofhow contested each label will be into ML metrics in an interpretable way, producing metrics that ask not “What proportion ofground truth labels does the classifier agree with?” but “What proportion of the population does the classifier agree with?”

Disagreement deconvolution: transforms any ML classification metric to reflect the underlying distribution of popu- lation labels},
	file = {Gordon et al_2021_The Disagreement Deconvolution.pdf:/Users/orsonxu/Zotero/storage/AA5TMKD8/Gordon et al_2021_The Disagreement Deconvolution.pdf:application/pdf;Gordon et al_2021_The Disagreement Deconvolution.pdf:/Users/orsonxu/Zotero/storage/SY7LZ2L4/Gordon et al_2021_The Disagreement Deconvolution.pdf:application/pdf},
}

@inproceedings{mothilal_explaining_2020,
	address = {Barcelona Spain},
	title = {Explaining machine learning classifiers through diverse counterfactual explanations},
	isbn = {978-1-4503-6936-7},
	url = {https://dl.acm.org/doi/10.1145/3351095.3372850},
	doi = {10.1145/3351095.3372850},
	abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
	language = {en},
	urldate = {2021-11-23},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
	month = jan,
	year = {2020},
	keywords = {Unread},
	pages = {607--617},
	file = {Mothilal et al. - 2020 - Explaining machine learning classifiers through di.pdf:/Users/orsonxu/Zotero/storage/25N37EPI/Mothilal et al. - 2020 - Explaining machine learning classifiers through di.pdf:application/pdf},
}

@article{barredo_arrieta_explainable_2020,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, taxonomies, opportunities and challenges toward responsible {AI}},
	volume = {58},
	issn = {15662535},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253519308103},
	doi = {10.1016/j.inffus.2019.12.012},
	language = {en},
	urldate = {2021-11-30},
	journal = {Information Fusion},
	author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	month = jun,
	year = {2020},
	pages = {82--115},
	annote = {A very comprehensive summary of XAI, from the concept, definition to all kinds of methods in the domain.

A few highlights:


Three levels of transparency (the higher, the more transparent):


Simulatability


Decomposibility


Algorithmic Transparency




Two general types of models


Interpretable ML models


Linear/logistic regression


Decision Tree


KNN


Rule-based model


General linear/additive models


Bayesian Models




Post-hoc interpretability


Model-agnostic techniques


LIME


SHAP











},
	file = {Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf:/Users/orsonxu/Zotero/storage/MEXVF7WC/Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf},
}

@incollection{holzinger_explainable_2018,
	address = {Cham},
	title = {Explainable {AI}: {The} {New} 42?},
	volume = {11015},
	isbn = {978-3-319-99739-1 978-3-319-99740-7},
	shorttitle = {Explainable {AI}},
	url = {https://link.springer.com/10.1007/978-3-319-99740-7_21},
	abstract = {Explainable AI is not a new ﬁeld. Since at least the early exploitation of C.S. Pierce’s abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-deﬁned domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge.},
	language = {en},
	urldate = {2022-01-05},
	booktitle = {Machine {Learning} and {Knowledge} {Extraction}},
	publisher = {Springer International Publishing},
	author = {Goebel, Randy and Chander, Ajay and Holzinger, Katharina and Lecue, Freddy and Akata, Zeynep and Stumpf, Simone and Kieseberg, Peter and Holzinger, Andreas},
	editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
	year = {2018},
	doi = {10.1007/978-3-319-99740-7_21},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {295--303},
	annote = {- RQs and Contributions
A brief summary of recent advance on textual and visual interpretability on deep learning models
Directions:
Visual:
- Attentions Weights
- Differences in distributions like KL divergence
Text:
- Language generation as explanation
- Attention},
	file = {Goebel et al. - 2018 - Explainable AI The New 42.pdf:/Users/orsonxu/Zotero/storage/3KCB9T5W/Goebel et al. - 2018 - Explainable AI The New 42.pdf:application/pdf},
}

@article{zhang_explainable_2020,
	title = {Explainable {Recommendation}: {A} {Survey} and {New} {Perspectives}},
	volume = {14},
	issn = {1554-0669, 1554-0677},
	shorttitle = {Explainable {Recommendation}},
	url = {http://arxiv.org/abs/1804.11192},
	doi = {10.1561/1500000066},
	abstract = {Explainable recommendation attempts to develop models that generate not only high-quality recommendations but also intuitive explanations. The explanations may either be post-hoc or directly come from an explainable model (also called interpretable or transparent model in some contexts). Explainable recommendation tries to address the problem of why: by providing explanations to users or system designers, it helps humans to understand why certain items are recommended by the algorithm, where the human can either be users or system designers. Explainable recommendation helps to improve the transparency, persuasiveness, effectiveness, trustworthiness, and satisfaction of recommendation systems. It also facilitates system designers for better system debugging. In recent years, a large number of explainable recommendation approaches -- especially model-based methods -- have been proposed and applied in real-world systems. In this survey, we provide a comprehensive review for the explainable recommendation research. We first highlight the position of explainable recommendation in recommender system research by categorizing recommendation problems into the 5W, i.e., what, when, who, where, and why. We then conduct a comprehensive survey of explainable recommendation on three perspectives: 1) We provide a chronological research timeline of explainable recommendation. 2) We provide a two-dimensional taxonomy to classify existing explainable recommendation research. 3) We summarize how explainable recommendation applies to different recommendation tasks. We also devote a chapter to discuss the explanation perspectives in broader IR and AI/ML research. We end the survey by discussing potential future directions to promote the explainable recommendation research area and beyond.},
	number = {1},
	urldate = {2022-01-25},
	journal = {Foundations and Trends® in Information Retrieval},
	author = {Zhang, Yongfeng and Chen, Xu},
	year = {2020},
	note = {arXiv: 1804.11192},
	pages = {1--101},
	annote = {Comment: 101 pages, published in Foundations and Trends in Information Retrieval, 14(1), pp.1-101 (2020)},
	file = {Zhang_Chen_2020_Explainable Recommendation.pdf:/Users/orsonxu/Zotero/storage/IRQ5T486/Zhang_Chen_2020_Explainable Recommendation.pdf:application/pdf;Zhang_Chen_2020_Explainable Recommendation.pdf:/Users/orsonxu/Zotero/storage/SPBAXHQ7/Zhang_Chen_2020_Explainable Recommendation.pdf:application/pdf},
}

@article{adadi_peeking_2018,
	title = {Peeking {Inside} the {Black}-{Box}: {A} {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Peeking {Inside} the {Black}-{Box}},
	doi = {10.1109/ACCESS.2018.2870052},
	abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
	journal = {IEEE Access},
	author = {Adadi, Amina and Berrada, Mohammed},
	year = {2018},
	note = {Conference Name: IEEE Access},
	pages = {52138--52160},
	file = {Adadi_Berrada_2018_Peeking Inside the Black-Box.pdf:/Users/orsonxu/Zotero/storage/K44JIPFC/Adadi_Berrada_2018_Peeking Inside the Black-Box.pdf:application/pdf;Adadi_Berrada_2018_Peeking Inside the Black-Box.pdf:/Users/orsonxu/Zotero/storage/MCSS2YU6/Adadi_Berrada_2018_Peeking Inside the Black-Box.pdf:application/pdf},
}

@inproceedings{bansal_does_2021,
	address = {Yokohama Japan},
	title = {Does the {Whole} {Exceed} its {Parts}? {The} {Effect} of {AI} {Explanations} on {Complementary} {Team} {Performance}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {Does the {Whole} {Exceed} its {Parts}?},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445717},
	doi = {10.1145/3411764.3445717},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel},
	month = may,
	year = {2021},
	pages = {1--16},
	file = {Bansal et al. - 2021 - Does the Whole Exceed its Parts The Effect of AI .pdf:/Users/orsonxu/Zotero/storage/2KSFDWD3/Bansal et al. - 2021 - Does the Whole Exceed its Parts The Effect of AI .pdf:application/pdf},
}

@misc{urbansky_comparison_2020,
	title = {Comparison of 7 image classification {APIs} for food pictures},
	url = {https://ddsky.medium.com/comparison-of-7-image-classification-apis-for-food-pictures-1d61b7293285},
	abstract = {This article compares 7 online image recognition services in the context of food recognition. In particular, my goal was to find out which…},
	language = {en},
	urldate = {2022-06-27},
	journal = {Medium},
	author = {Urbansky, David},
	month = apr,
	year = {2020},
	file = {Snapshot:/Users/orsonxu/Zotero/storage/WRJYZ5PX/comparison-of-7-image-classification-apis-for-food-pictures-1d61b7293285.html:text/html},
}

@inproceedings{lim_why_2009,
	address = {Boston MA USA},
	title = {Why and why not explanations improve the intelligibility of context-aware intelligent systems},
	isbn = {978-1-60558-246-7},
	url = {https://dl.acm.org/doi/10.1145/1518701.1519023},
	doi = {10.1145/1518701.1519023},
	abstract = {Context-aware intelligent systems employ implicit inputs, and make decisions based on complex rules and machine learning models that are rarely clear to users. Such lack of system intelligibility can lead to loss of user trust, satisfaction and acceptance of these systems. However, automatically providing explanations about a system‟s decision process can help mitigate this problem. In this paper we present results from a controlled study with over 200 participants in which the effectiveness of different types of explanations was examined. Participants were shown examples of a system‟s operation along with various automatically generated explanations, and then tested on their understanding of the system. We show, for example, that explanations describing why the system behaved a certain way resulted in better understanding and stronger feelings of trust. Explanations describing why the system did not behave a certain way, resulted in lower understanding yet adequate performance. We discuss implications for the use of our findings in real-world context-aware applications.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lim, Brian Y. and Dey, Anind K. and Avrahami, Daniel},
	month = apr,
	year = {2009},
	pages = {2119--2128},
	annote = {- RQs and Contributions
Show that providing interpretability (specifically, why and why not the system does it) in the context-aware scenario can help users to better understand and trust the system.

- Methods
211 online user study to study five types of explanation:
1. What: What did the system do?
2. Why: Why did the system do W?
3. Why Not: Why did the system not do X?
4. What If: What would the system do if Y happens?
5. How To: How can I get the system to do Z, given the current context?
The explanation was generated via some simple decision tree algorithms.
Then measure trust etc.
},
	file = {Lim et al. - 2009 - Why and why not explanations improve the in.pdf:/Users/orsonxu/Zotero/storage/HAWPC6QA/Lim et al. - 2009 - Why and why not explanations improve the in.pdf:application/pdf},
}

@article{bunt_are_2012,
	title = {Are explanations always important?: a study of deployed, low-cost intelligent interactive systems},
	abstract = {Intelligent interactive systems (IIS) have great potential to improve users' experience with technology by tailoring their behaviour and appearance to users’ individual needs; however, these systems, with their complex algorithms and dynamic behaviour, can also suffer from a lack of comprehensibility and transparency. We present the results of two studies examining the comprehensibility of, and desire for explanations with deployed, low-cost IIS. The first study, a set of interviews with 21 participants, reveals that i) comprehensibility is not always dependent on explanations, and ii) the perceived cost of viewing explanations tends to outweigh the anticipated benefits. Our second study, a two-week diary study with 14 participants, confirms these findings in the context of daily use, with participants indicating a desire for an explanation in only 7\% of diary entries. We discuss the implications of our findings for the design of explanation facilities.},
	language = {en},
	author = {Bunt, Andrea and Lount, Matthew and Lauzon, Catherine},
	year = {2012},
	pages = {10},
	file = {Bunt et al. - 2012 - Are explanations always important a study of dep.pdf:/Users/orsonxu/Zotero/storage/SUEFGDDA/Bunt et al. - 2012 - Are explanations always important a study of dep.pdf:application/pdf},
}

@article{langer_what_2021,
	title = {What do we want from {Explainable} {Artificial} {Intelligence} ({XAI})? – {A} stakeholder perspective on {XAI} and a conceptual model guiding interdisciplinary {XAI} research},
	volume = {296},
	issn = {00043702},
	shorttitle = {What do we want from {Explainable} {Artificial} {Intelligence} ({XAI})?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370221000242},
	doi = {10.1016/j.artint.2021.103473},
	language = {en},
	urldate = {2022-07-15},
	journal = {Artificial Intelligence},
	author = {Langer, Markus and Oster, Daniel and Speith, Timo and Hermanns, Holger and Kästner, Lena and Schmidt, Eva and Sesing, Andreas and Baum, Kevin},
	month = jul,
	year = {2021},
	pages = {103473},
	file = {Langer et al. - 2021 - What do we want from Explainable Artificial Intell.pdf:/Users/orsonxu/Zotero/storage/8JNPLSLM/Langer et al. - 2021 - What do we want from Explainable Artificial Intell.pdf:application/pdf},
}

@inproceedings{lim_toolkit_2010,
	address = {Copenhagen Denmark},
	title = {Toolkit to support intelligibility in context-aware applications},
	isbn = {978-1-60558-843-8},
	url = {https://dl.acm.org/doi/10.1145/1864349.1864353},
	doi = {10.1145/1864349.1864353},
	abstract = {Context-aware applications should be intelligible so users can better understand how they work and improve their trust in them. However, providing intelligibility is nontrivial and requires the developer to understand how to generate explanations from application decision models. Furthermore, users need different types of explanations and this complicates the implementation of intelligibility. We have developed the Intelligibility Toolkit that makes it easy for application developers to obtain eight types of explanations from the most popular decision models of context-aware applications. We describe its extensible architecture, and the explanation generation algorithms we developed. We validate the usefulness of the toolkit with three canonical applications that use the toolkit to generate explanations for end-users.},
	language = {en},
	urldate = {2022-08-28},
	booktitle = {Proceedings of the 12th {ACM} international conference on {Ubiquitous} computing},
	publisher = {ACM},
	author = {Lim, Brian Y. and Dey, Anind K.},
	month = sep,
	year = {2010},
	pages = {13--22},
	file = {Lim and Dey - 2010 - Toolkit to support intelligibility in context-awar.pdf:/Users/orsonxu/Zotero/storage/CY8QQ6C8/Lim and Dey - 2010 - Toolkit to support intelligibility in context-awar.pdf:application/pdf},
}

@article{ribeiro_anchors_2018,
	title = {Anchors: {High}-{Precision} {Model}-{Agnostic} {Explanations}},
	volume = {32},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Anchors},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11491},
	doi = {10.1609/aaai.v32i1.11491},
	abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, “sufﬁcient” conditions for predictions. We propose an algorithm to efﬁciently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the ﬂexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
	language = {en},
	number = {1},
	urldate = {2022-11-22},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = apr,
	year = {2018},
	file = {Ribeiro et al. - 2018 - Anchors High-Precision Model-Agnostic Explanation.pdf:/Users/orsonxu/Zotero/storage/IUCP5G67/Ribeiro et al. - 2018 - Anchors High-Precision Model-Agnostic Explanation.pdf:application/pdf},
}

@inproceedings{mittelstadt_explaining_2019,
	address = {Atlanta GA USA},
	title = {Explaining {Explanations} in {AI}},
	isbn = {978-1-4503-6125-5},
	url = {https://dl.acm.org/doi/10.1145/3287560.3287574},
	doi = {10.1145/3287560.3287574},
	abstract = {Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it’s important to remember Box’s maxim that "All models are wrong but some are useful." We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a "do it yourself kit" for explanations, allowing a practitioner to directly answer "what if questions" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.},
	language = {en},
	urldate = {2022-11-22},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
	month = jan,
	year = {2019},
	pages = {279--288},
	file = {Mittelstadt et al. - 2019 - Explaining Explanations in AI.pdf:/Users/orsonxu/Zotero/storage/N4PVKJFM/Mittelstadt et al. - 2019 - Explaining Explanations in AI.pdf:application/pdf},
}

@inproceedings{koh_understanding_2017,
	title = {Understanding {Black}-box {Predictions} via {Influence} {Functions}},
	url = {https://proceedings.mlr.press/v70/koh17a.html},
	abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
	language = {en},
	urldate = {2022-11-22},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Koh, Pang Wei and Liang, Percy},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1885--1894},
	file = {Koh_Liang_2017_Understanding Black-box Predictions via Influence Functions.pdf:/Users/orsonxu/Desktop/HCI/Literature Management/Zotero/Koh_Liang_2017_Understanding Black-box Predictions via Influence Functions.pdf:application/pdf;Supplementary PDF:/Users/orsonxu/Zotero/storage/CAZWQSVG/Koh and Liang - 2017 - Understanding Black-box Predictions via Influence .pdf:application/pdf},
}
