
@inproceedings{Amershi2019,
	address = {New York, New York, USA},
	title = {Guidelines for {Human}-{AI} {Interaction}},
	isbn = {978-1-4503-5970-2},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300233},
	doi = {10.1145/3290605.3300233},
	abstract = {Advances in artifcial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of guidelines for human-AI interaction design.},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems} - {CHI} '19},
	publisher = {ACM Press},
	author = {Amershi, Saleema and Inkpen, Kori and Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N.},
	year = {2019},
	keywords = {AI-infused systems, Design guidelines, Human-AI interaction},
	pages = {1--13},
	annote = {Point out the problem that AI interface is flushing out the traditional HCI design,

A survey paper about the AI-related design recommendation.

A design paper that propose 18 guidelines for human-AI interaction.},
	file = {Amershi et al. - 2019 - Guidelines for Human-AI Interaction.pdf:/Users/orsonxu/Zotero/storage/U3V4Q3BR/Amershi et al. - 2019 - Guidelines for Human-AI Interaction.pdf:application/pdf},
}

@article{Dellermann2019,
	title = {The {Future} of {Human}-{AI} {Collaboration}: {A} {Taxonomy} of {Design} {Knowledge} for {Hybrid} {Intelligence} {Systems}},
	volume = {6},
	doi = {10.24251/hicss.2019.034},
	abstract = {Recent technological advances, especially in the field of machine learning, provide astonishing progress on the road towards artificial general intelligence. However, tasks in current real-world business applications cannot yet be solved by machines alone. We, therefore, identify the need for developing socio-technological ensembles of humans and machines. Such systems possess the ability to accomplish complex goals by combining human and artificial intelligence to collectively achieve superior results and continuously improve by learning from each other. Thus, the need for structured design knowledge for those systems arises. Following a taxonomy development method, this article provides three main contributions: First, we present a structured overview of interdisciplinary research on the role of humans in the machine learning pipeline. Second, we envision hybrid intelligence systems and conceptualize the relevant dimensions for system design for the first time. Finally, we offer useful guidance for system developers during the implementation of such applications.},
	journal = {Proceedings of the 52nd Hawaii International Conference on System Sciences},
	author = {Dellermann, Dominik and Calma, Adrian and Lipusch, Nikolaus and Weber, Thorsten and Weigel, Sascha and Ebel, Philipp},
	year = {2019},
	note = {ISBN: 9780998133126},
	pages = {274--283},
	file = {Dellermann et al_2019_The Future of Human-AI Collaboration.pdf:/Users/orsonxu/Zotero/storage/A3AFVWX2/Dellermann et al_2019_The Future of Human-AI Collaboration.pdf:application/pdf;Dellermann et al_2019_The Future of Human-AI Collaboration.pdf:/Users/orsonxu/Zotero/storage/I7HF4TNN/Dellermann et al_2019_The Future of Human-AI Collaboration.pdf:application/pdf},
}

@inproceedings{inkpen_where_2019,
	address = {Glasgow Scotland Uk},
	title = {Where is the {Human}?: {Bridging} the {Gap} {Between} {AI} and {HCI}},
	isbn = {978-1-4503-5971-9},
	shorttitle = {Where is the {Human}?},
	url = {https://dl.acm.org/doi/10.1145/3290607.3299002},
	doi = {10.1145/3290607.3299002},
	abstract = {In recent years, AI systems have become both more powerful and increasingly promising for integration in a variety of application areas. Attention has also been called to the social challenges these systems bring, particularly in how they might fail or even actively disadvantage marginalised social groups, or how their opacity might make them difficult to oversee and challenge. In the context of these and other challenges, the roles of humans working in tandem with these systems will be important, yet the HCI community has been only a quiet voice in these debates to date. This workshop aims to catalyse and crystallise an agenda around HCI’s engagement with AI systems. Topics of interest include explainable and explorable AI; documentation and review; integrating artificial and human intelligence; collaborative decision making; AI/ML in HCI Design; diverse human roles and relationships in AI systems; and critical views of AI.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Extended {Abstracts} of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Inkpen, Kori and Chancellor, Stevie and De Choudhury, Munmun and Veale, Michael and Baumer, Eric P. S.},
	month = may,
	year = {2019},
	pages = {1--9},
	file = {Inkpen et al. - 2019 - Where is the Human Bridging the Gap Between AI a.pdf:/Users/orsonxu/Zotero/storage/PDJA9H2F/Inkpen et al. - 2019 - Where is the Human Bridging the Gap Between AI a.pdf:application/pdf},
}

@inproceedings{abdul_trends_2018,
	address = {Montreal QC Canada},
	title = {Trends and {Trajectories} for {Explainable}, {Accountable} and {Intelligible} {Systems}: {An} {HCI} {Research} {Agenda}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Trends and {Trajectories} for {Explainable}, {Accountable} and {Intelligible} {Systems}},
	url = {https://dl.acm.org/doi/10.1145/3173574.3174156},
	doi = {10.1145/3173574.3174156},
	abstract = {Advances in artificial intelligence, sensors and big data management have far-reaching societal impacts. As these systems augment our everyday lives, it becomes increasingly important for people to understand them and remain in control. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explainable systems, as well as 12,412 citing papers. Using topic modeling, co-occurrence and network analysis, we mapped the research space from diverse domains, such as algorithmic accountability, interpretable machine learning, context-awareness, cognitive psychology, and software learnability. We reveal fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly isolated. The time is ripe for the HCI community to ensure that the powerful new autonomous systems have intelligible interfaces built-in. From our results, we propose several implications and directions for future research towards this goal.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y. and Kankanhalli, Mohan},
	month = apr,
	year = {2018},
	pages = {1--18},
	file = {Abdul et al. - 2018 - Trends and Trajectories for Explainable, Accountab.pdf:/Users/orsonxu/Zotero/storage/EEB3L5XE/Abdul et al. - 2018 - Trends and Trajectories for Explainable, Accountab.pdf:application/pdf},
}

@inproceedings{wang_designing_2019,
	address = {Glasgow Scotland Uk},
	title = {Designing {Theory}-{Driven} {User}-{Centric} {Explainable} {AI}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300831},
	doi = {10.1145/3290605.3300831},
	abstract = {From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable AI (XAI). This paper seeks to strengthen empirical applicationspecific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building humancentered, decision-theory-driven XAI based on an extensive review across these fields. Drawing on this framework, we identify pathways along which human cognitive patterns drives needs for building XAI and how XAI can mitigate common cognitive biases. We then put this framework into practice by designing and implementing an explainable clinical diagnostic tool for intensive care phenotyping and conducting a co-design exercise with clinicians. Thereafter, we draw insights into how this framework bridges algorithm-generated explanations and human decision-making theories. Finally, we discuss implications for XAI design and development.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y.},
	month = may,
	year = {2019},
	pages = {1--15},
	file = {Wang et al. - 2019 - Designing Theory-Driven User-Centric Explainable A.pdf:/Users/orsonxu/Zotero/storage/23YWB6US/Wang et al. - 2019 - Designing Theory-Driven User-Centric Explainable A.pdf:application/pdf},
}

@inproceedings{horvitz_principles_1999,
	address = {Pittsburgh, Pennsylvania, United States},
	title = {Principles of mixed-initiative user interfaces},
	isbn = {978-0-201-48559-2},
	url = {http://portal.acm.org/citation.cfm?doid=302979.303030},
	doi = {10.1145/302979.303030},
	abstract = {Recent debate has centered on the relative promise of focusing user-interface research on developing new metaphors and tools that enhance users’ abilities to directly manipulate objects versus directing effort toward developing interface agents that provide automation. In this paper, we review principles that show promise for allowing engineers to enhance human-computer interaction through an elegant coupling of automated services with direct manipulation. Key ideas will be highlighted in terms of the Lookout system for scheduling and meeting management.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the {SIGCHI} conference on {Human} factors in computing systems the {CHI} is the limit - {CHI} '99},
	publisher = {ACM Press},
	author = {Horvitz, Eric},
	year = {1999},
	pages = {159--166},
	annote = {- RQs and Contributions
How to make a mixed-initiated UI
Provide an example system that helps people to send emails/calendar events.

- Methods
Simple SVM with a good grid analysis on 
P(Goal, Action)
P(not Goal, Action)
P(Goal, not Action)
P(not Goal, not Action)
},
	file = {Horvitz - 1999 - Principles of mixed-initiative user interfaces.pdf:/Users/orsonxu/Zotero/storage/NSSHBCPC/Horvitz - 1999 - Principles of mixed-initiative user interfaces.pdf:application/pdf},
}

@inproceedings{cimolino_two_2022,
	address = {New Orleans LA USA},
	title = {Two {Heads} {Are} {Better} {Than} {One}: {A} {Dimension} {Space} for {Unifying} {Human} and {Artificial} {Intelligence} in {Shared} {Control}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Two {Heads} {Are} {Better} {Than} {One}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517610},
	doi = {10.1145/3491102.3517610},
	abstract = {Shared control is an emerging interaction paradigm in which a human and an AI partner collaboratively control a system. Shared control unifes human and artifcial intelligence, making the human’s interactions with computers more accessible, safe, precise, efective, creative, and playful. This form of interaction has independently emerged in contexts as varied as mobility assistance, driving, surgery, and digital games. These domains each have their own problems, terminology, and design philosophies. Without a common language for describing interactions in shared control, it is difcult for designers working in one domain to share their knowledge with designers working in another. To address this problem, we present a dimension space for shared control, based on a survey of 55 shared control systems from six diferent problem domains. This design space analysis tool enables designers to classify existing systems, make comparisons between them, identify higher-level design patterns, and imagine solutions to novel problems.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Cimolino, Gabriele and Graham, T.C. Nicholas},
	month = apr,
	year = {2022},
	pages = {1--21},
	file = {Cimolino and Graham - 2022 - Two Heads Are Better Than One A Dimension Space f.pdf:/Users/orsonxu/Zotero/storage/5GSSUTQA/Cimolino and Graham - 2022 - Two Heads Are Better Than One A Dimension Space f.pdf:application/pdf},
}

@inproceedings{cila_designing_2022,
	address = {New Orleans LA USA},
	title = {Designing {Human}-{Agent} {Collaborations}: {Commitment}, responsiveness, and support},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Designing {Human}-{Agent} {Collaborations}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517500},
	doi = {10.1145/3491102.3517500},
	abstract = {With the advancements in AI, agents (i.e., smart products, robots, software agents) are increasingly capable of working closely together with humans in a variety of ways while benefiting from each other. These human-agent collaborations have gained growing attention in the HCI community; however, the field lacks clear guidelines on how to design the agents’ behaviors in collaborations. In this paper, the qualities that are relevant for designers to create robust and pleasant human-agent collaborations were investigated. Bratman’s Shared Cooperative Activity framework was used to identify the core characteristics of collaborations and survey the most important issues in the design of human-agent collaborations, namely code-of-conduct, task delegation, autonomy and control, intelligibility, common ground, offering help and requesting help. The aim of this work is to add structure to this growing and important facet of HCI research and operationalize the concept of human-agent collaboration with concrete design considerations.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Cila, Nazli},
	month = apr,
	year = {2022},
	pages = {1--18},
	file = {Cila - 2022 - Designing Human-Agent Collaborations Commitment, .pdf:/Users/orsonxu/Zotero/storage/RBLC95IG/Cila - 2022 - Designing Human-Agent Collaborations Commitment, .pdf:application/pdf},
}

@article{mohseni_multidisciplinary_2021,
	title = {A {Multidisciplinary} {Survey} and {Framework} for {Design} and {Evaluation} of {Explainable} {AI} {Systems}},
	volume = {11},
	issn = {2160-6455, 2160-6463},
	url = {https://dl.acm.org/doi/10.1145/3387166},
	doi = {10.1145/3387166},
	abstract = {The need for interpretable and accountable intelligent systems grows along with the prevalence of
              artificial intelligence
              (
              AI
              ) applications used in everyday life.
              Explainable AI
              (
              XAI
              ) systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of XAI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this article presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation methods. Our categorization presents the mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
	language = {en},
	number = {3-4},
	urldate = {2022-07-11},
	journal = {ACM Transactions on Interactive Intelligent Systems},
	author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
	month = dec,
	year = {2021},
	pages = {1--45},
	annote = {A very good summary of XAI system designs.
Design goal for End user:
Algorithm transparency, user trust,
[less common, maybe no need in AR] bias mitigation, privacy awareness

What to explain:
how (overall model, not important for end user), why, why not, what if, how to (counter factual), what else (example)

How to explain:
visual, textual, analytical
},
	file = {Mohseni et al. - 2021 - A Multidisciplinary Survey and Framework for Desig.pdf:/Users/orsonxu/Zotero/storage/GTAHMS2G/Mohseni et al. - 2021 - A Multidisciplinary Survey and Framework for Desig.pdf:application/pdf},
}

@inproceedings{eiband_bringing_2018,
	address = {Tokyo Japan},
	title = {Bringing {Transparency} {Design} into {Practice}},
	isbn = {978-1-4503-4945-1},
	url = {https://dl.acm.org/doi/10.1145/3172944.3172961},
	doi = {10.1145/3172944.3172961},
	abstract = {Intelligent systems, which are on their way to becoming mainstream in everyday products, make recommendations and decisions for users based on complex computations. Researchers and policy makers increasingly raise concerns regarding the lack of transparency and comprehensibility of these computations from the user perspective. Our aim is to advance existing UI guidelines for more transparency in complex realworld design scenarios involving multiple stakeholders. To this end, we contribute a stage-based participatory process for designing transparent interfaces incorporating perspectives of users, designers, and providers, which we developed and validated with a commercial intelligent ﬁtness coach. With our work, we hope to provide guidance to practitioners and to pave the way for a pragmatic approach to transparency in intelligent systems.},
	language = {en},
	urldate = {2022-07-23},
	booktitle = {23rd {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Eiband, Malin and Schneider, Hanna and Bilandzic, Mark and Fazekas-Con, Julian and Haug, Mareike and Hussmann, Heinrich},
	month = mar,
	year = {2018},
	pages = {211--223},
	annote = {

RQs and Contributions


Provide a stage-based participatory process for designing transparent interfaces incorporating perspectives of users, designers, and providers
Answer two main question:
1) what to recommend
2) how to recommend
},
	file = {Eiband et al. - 2018 - Bringing Transparency Design into Practice.pdf:/Users/orsonxu/Zotero/storage/6GD3YVZK/Eiband et al. - 2018 - Bringing Transparency Design into Practice.pdf:application/pdf},
}

@inproceedings{long_what_2020,
	address = {Honolulu HI USA},
	title = {What is {AI} {Literacy}? {Competencies} and {Design} {Considerations}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {What is {AI} {Literacy}?},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376727},
	doi = {10.1145/3313831.3376727},
	abstract = {Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learnercentered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper’s contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.},
	language = {en},
	urldate = {2022-07-21},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Long, Duri and Magerko, Brian},
	month = apr,
	year = {2020},
	pages = {1--16},
	file = {Long and Magerko - 2020 - What is AI Literacy Competencies and Design Consi.pdf:/Users/orsonxu/Zotero/storage/F7X455KW/Long and Magerko - 2020 - What is AI Literacy Competencies and Design Consi.pdf:application/pdf},
}

@inproceedings{roy_automation_2019,
	address = {Glasgow Scotland Uk},
	title = {Automation {Accuracy} {Is} {Good}, but {High} {Controllability} {May} {Be} {Better}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300750},
	doi = {10.1145/3290605.3300750},
	abstract = {When automating tasks using some form of artificial intelligence, some inaccuracy in the result is virtually unavoidable. In many cases, the user must decide whether to try the automated method again, or fix it themselves using the available user interface. We argue this decision is influenced by both perceived automation accuracy and degree of task “controllability” (how easily and to what extent an automated result can be manually modified). This relationship between accuracy and controllability is investigated in a 750-participant crowdsourced experiment using a controlled, gamified task. With high controllability, self-reported satisfaction remained constant even under very low accuracy conditions, and overall, a strong preference was observed for using manual control rather than automation, despite much slower performance and regardless of very poor controllability.},
	language = {en},
	urldate = {2022-07-21},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Roy, Quentin and Zhang, Futian and Vogel, Daniel},
	month = may,
	year = {2019},
	pages = {1--8},
	file = {Roy et al. - 2019 - Automation Accuracy Is Good, but High Controllabil.pdf:/Users/orsonxu/Zotero/storage/3XRB6925/Roy et al. - 2019 - Automation Accuracy Is Good, but High Controllabil.pdf:application/pdf},
}

@inproceedings{cai_impacts_2022,
	address = {New Orleans LA USA},
	title = {Impacts of {Personal} {Characteristics} on {User} {Trust} in {Conversational} {Recommender} {Systems}},
	isbn = {978-1-4503-9157-3},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517471},
	doi = {10.1145/3491102.3517471},
	abstract = {Conversational recommender systems (CRSs) imitate human advisors to assist users in fnding items through conversations and have recently gained increasing attention in domains such as media and e-commerce. Like in human communication, building trust in human-agent communication is essential given its signifcant infuence on user behavior. However, inspiring user trust in CRSs with a “one-size-fts-all” design is difcult, as individual users may have their own expectations for conversational interactions (e.g., who, user or system, takes the initiative), which are potentially related to their personal characteristics. In this study, we investigated the impacts of three personal characteristics, namely personality traits, trust propensity, and domain knowledge, on user trust in two types of text-based CRSs, i.e., user-initiative and mixed-initiative. Our between-subjects user study (N=148) revealed that users’ trust propensity and domain knowledge positively infuenced their trust in CRSs, and that users with high conscientiousness tended to trust the mixed-initiative system.},
	language = {en},
	urldate = {2022-07-21},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Cai, Wanling and Jin, Yucheng and Chen, Li},
	month = apr,
	year = {2022},
	pages = {1--14},
	file = {Cai et al. - 2022 - Impacts of Personal Characteristics on User Trust .pdf:/Users/orsonxu/Zotero/storage/VS3CFV9X/Cai et al. - 2022 - Impacts of Personal Characteristics on User Trust .pdf:application/pdf},
}

@article{schmidt_transparency_2020,
	title = {Transparency and trust in artificial intelligence systems},
	volume = {29},
	issn = {1246-0125, 2116-7052},
	url = {https://www.tandfonline.com/doi/full/10.1080/12460125.2020.1819094},
	doi = {10.1080/12460125.2020.1819094},
	abstract = {Assistive technology featuring artificial intelligence (AI) to support human decision-making has become ubiquitous. Assistive AI achieves accuracy comparable to or even surpassing that of human experts. However, often the adoption of assistive AI systems is limited by a lack of trust of humans into an AI’s prediction. This is why the AI research community has been focusing on rendering AI decisions more transparent by providing explanations of an AIs decision. To what extent these explanations really help to foster trust into an AI system remains an open question. In this paper, we report the results of a behavioural experiment in which subjects were able to draw on the support of an ML-based decision support tool for text classification. We experimentally varied the information subjects received and show that transparency can actually have a negative impact on trust. We discuss implications for decision makers employing assistive AI technology.},
	language = {en},
	number = {4},
	urldate = {2022-07-21},
	journal = {Journal of Decision Systems},
	author = {Schmidt, Philipp and Biessmann, Felix and Teubner, Timm},
	month = oct,
	year = {2020},
	pages = {260--278},
	file = {Schmidt et al. - 2020 - Transparency and trust in artificial intelligence .pdf:/Users/orsonxu/Zotero/storage/Q2YX3FS7/Schmidt et al. - 2020 - Transparency and trust in artificial intelligence .pdf:application/pdf},
}

@article{zanzotto_viewpoint_2019,
	title = {Viewpoint: {Human}-in-the-loop {Artificial} {Intelligence}},
	volume = {64},
	issn = {1076-9757},
	shorttitle = {Viewpoint},
	url = {https://jair.org/index.php/jair/article/view/11345},
	doi = {10.1613/jair.1.11345},
	abstract = {Little by little, newspapers are revealing the bright future that Artiﬁcial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future may have a possible dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers may need ﬁnancial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Many learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, many of these workers are shooting themselves in the feet.},
	language = {en},
	urldate = {2022-07-18},
	journal = {Journal of Artificial Intelligence Research},
	author = {Zanzotto, Fabio Massimo},
	month = feb,
	year = {2019},
	pages = {243--252},
	file = {Zanzotto - 2019 - Viewpoint Human-in-the-loop Artificial Intelligen.pdf:/Users/orsonxu/Zotero/storage/5BZYGL9X/Zanzotto - 2019 - Viewpoint Human-in-the-loop Artificial Intelligen.pdf:application/pdf},
}

@inproceedings{dhanorkar_who_2021,
	address = {Virtual Event USA},
	title = {Who needs to know what, when?: {Broadening} the {Explainable} {AI} ({XAI}) {Design} {Space} by {Looking} at {Explanations} {Across} the {AI} {Lifecycle}},
	isbn = {978-1-4503-8476-6},
	shorttitle = {Who needs to know what, when?},
	url = {https://dl.acm.org/doi/10.1145/3461778.3462131},
	doi = {10.1145/3461778.3462131},
	abstract = {The interpretability or explainability of AI systems (XAI) has been a topic gaining renewed attention in recent years across AI and HCI communities. Recent work has drawn attention to the emergent explainability requirements of in situ, applied projects, yet further exploratory work is needed to more fully understand this space. This paper investigates applied AI projects and reports on a qualitative interview study of individuals working on AI projects at a large technology and consulting company. Presenting an empirical understanding of the range of stakeholders in industrial AI projects, this paper also draws out the emergent explainability practices that arise as these projects unfold, highlighting the range of explanation audiences (who), as well as how their explainability needs evolve across the AI project lifecycle (when). We discuss the importance of adopting a sociotechnical lens in designing AI systems, noting how the “AI lifecycle” can serve as a design metaphor to further the XAI design field.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Designing {Interactive} {Systems} {Conference} 2021},
	publisher = {ACM},
	author = {Dhanorkar, Shipi and Wolf, Christine T. and Qian, Kun and Xu, Anbang and Popa, Lucian and Li, Yunyao},
	month = jun,
	year = {2021},
	pages = {1591--1602},
	annote = {

RQs and Contributions


A qualitative study to understand the design of XAI systems by interviewing 30 technical developers, HCI researchers and designers.

Two cases when explanation is needed:


Expectation mismatch


Explanations in service of business actionability



Also mentioned the trade-off effect of simplicity/complexity explanation, and how much to explain
},
	file = {Dhanorkar et al. - 2021 - Who needs to know what, when Broadening the Expl.pdf:/Users/orsonxu/Zotero/storage/R2EMIPGU/Dhanorkar et al. - 2021 - Who needs to know what, when Broadening the Expl.pdf:application/pdf},
}

@article{jiang_who_2022,
	title = {Who needs explanation and when? {Juggling} explainable {AI} and user epistemic uncertainty},
	volume = {165},
	issn = {10715819},
	shorttitle = {Who needs explanation and when?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581922000660},
	doi = {10.1016/j.ijhcs.2022.102839},
	abstract = {In recent years, AI explainability (XAI) has received wide attention. Although XAI is expected to play a positive role in decision-making and advice acceptance, various opposing effects have also been found. The opposing effects of XAI highlight the critical role of context, especially human factors, in understanding XAI’s impacts. This study investigates the effects of providing three types of post-hoc explanations (alternative advice, prediction confidence scores, and prediction rationale) on two context-specific user decision-making outcomes (AI advice acceptance and advice adoption). Our field experiment results show that users’ epistemic uncertainty matters when understanding XAI’s impacts. As users’ epistemic uncertainty increases, only providing prediction rationale is beneficial, whereas providing alternative advice and showing prediction confidence scores may hinder users’ advice acceptance. Our study contributes to the emerging literature on the human aspects of XAI by clarifying XAI and showing that XAI may not always be desirable. It also contributes by highlighting the importance of considering user profiles when pre­ dicting XAI’s impacts, designing XAI, and providing professional services with AI.},
	language = {en},
	urldate = {2022-07-14},
	journal = {International Journal of Human-Computer Studies},
	author = {Jiang, Jinglu and Kahai, Surinder and Yang, Ming},
	month = sep,
	year = {2022},
	pages = {102839},
	file = {Jiang et al. - 2022 - Who needs explanation and when Juggling explainab.pdf:/Users/orsonxu/Zotero/storage/TPQW9LKP/Jiang et al. - 2022 - Who needs explanation and when Juggling explainab.pdf:application/pdf},
}

@inproceedings{lim_assessing_2009,
	address = {Orlando Florida USA},
	title = {Assessing demand for intelligibility in context-aware applications},
	isbn = {978-1-60558-431-7},
	url = {https://dl.acm.org/doi/10.1145/1620545.1620576},
	doi = {10.1145/1620545.1620576},
	abstract = {Intelligibility can help expose the inner workings and inputs of context-aware applications that tend to be opaque to users due to their implicit sensing and actions. However, users may not be interested in all the information that the applications can produce. Using scenarios of four real-world applications that span the design space of context-aware computing, we conducted two experiments to discover what information users are interested in. In the first experiment, we elicit types of information demands that users have and under what moderating circumstances they have them. In the second experiment, we verify the findings by soliciting users about which types they would want to know and establish whether receiving such information would satisfy them. We discuss why users demand certain types of information, and provide design implications on how to provide different intelligibility types to make context-aware applications intelligible and acceptable to users.},
	language = {en},
	urldate = {2022-07-14},
	booktitle = {Proceedings of the 11th international conference on {Ubiquitous} computing},
	publisher = {ACM},
	author = {Lim, Brian Y. and Dey, Anind K.},
	month = sep,
	year = {2009},
	pages = {195--204},
	annote = {What to explain:

input, output, certainty
why, why not, what if, how to, what else
},
	file = {Lim and Dey - 2009 - Assessing demand for intelligibility in context-aw.pdf:/Users/orsonxu/Zotero/storage/MZ9XATEV/Lim and Dey - 2009 - Assessing demand for intelligibility in context-aw.pdf:application/pdf},
}

@inproceedings{dey_support_2009,
	address = {Boston MA USA},
	title = {Support for context-aware intelligibility and control},
	isbn = {978-1-60558-246-7},
	url = {https://dl.acm.org/doi/10.1145/1518701.1518832},
	doi = {10.1145/1518701.1518832},
	abstract = {Intelligibility and control are important user concerns in context-aware applications. They allow a user to understand why an application is behaving a certain way, and to change its behavior. Because of their importance to end users, they must be addressed at an interface level. However, often the sensors or machine learning systems that users need to understand and control are created long before a specific application is built, or created separately from the application interface. Thus, supporting interface designers in building intelligibility and control into interfaces requires application logic and underlying infrastructure to be exposed in some structured fashion. As context-aware infrastructures do not provide generalized support for this, we extended one such infrastructure with Situations, components that appropriately exposes application logic, and supports debugging and simple intelligibility and control interfaces, while making it easier for an application developer to build context-aware applications and facilitating designer access to application state and behavior. We developed support for interface designers in Visual Basic and Flash. We demonstrate the usefulness of this support through an evaluation of programmers, an evaluation of the usability of the new infrastructure with interface designers, and the augmentation of three common context-aware applications.},
	language = {en},
	urldate = {2022-07-14},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Dey, Anind K. and Newberger, Alan},
	month = apr,
	year = {2009},
	pages = {859--868},
	file = {Dey and Newberger - 2009 - Support for context-aware intelligibility and cont.pdf:/Users/orsonxu/Zotero/storage/SPSAH5UL/Dey and Newberger - 2009 - Support for context-aware intelligibility and cont.pdf:application/pdf},
}

@inproceedings{liao_questioning_2020,
	title = {Questioning the {AI}: {Informing} {Design} {Practices} for {Explainable} {AI} {User} {Experiences}},
	shorttitle = {Questioning the {AI}},
	url = {http://arxiv.org/abs/2001.02478},
	doi = {10.1145/3313831.3376590},
	abstract = {A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.},
	urldate = {2022-08-05},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Liao, Q. Vera and Gruen, Daniel and Miller, Sarah},
	month = apr,
	year = {2020},
	note = {arXiv:2001.02478 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	pages = {1--15},
	annote = {Comment: PublishedACM CHI Conference on Human Factors in Computing Systems (CHI 2020). Updated XAI Question Bank in September 2021},
	file = {arXiv Fulltext PDF:/Users/orsonxu/Zotero/storage/FM7TH38X/Liao et al. - 2020 - Questioning the AI Informing Design Practices for.pdf:application/pdf;arXiv.org Snapshot:/Users/orsonxu/Zotero/storage/L4QMZJ2U/2001.html:text/html},
}

@inproceedings{jeyakumar_how_2020,
	title = {How {Can} {I} {Explain} {This} to {You}? {An} {Empirical} {Study} of {Deep} {Neural} {Network} {Explanation} {Methods}},
	volume = {33},
	shorttitle = {How {Can} {I} {Explain} {This} to {You}?},
	url = {https://proceedings.neurips.cc/paper/2020/hash/2c29d89cc56cdb191c60db2f0bae796b-Abstract.html},
	abstract = {Explaining the inner workings of deep neural network models have received considerable attention in recent years. Researchers have attempted to provide human parseable explanations justifying why a model performed a specific classification. Although many of these toolkits are available for use, it is unclear which style of explanation is preferred by end-users, thereby demanding investigation. We performed a cross-analysis Amazon Mechanical Turk study comparing the popular state-of-the-art explanation methods to empirically determine which are better in explaining model decisions. The participants were asked to compare explanation methods across applications spanning image, text, audio, and sensory domains. Among the surveyed methods, explanation-by-example was preferred in all domains except text sentiment classification, where LIME's method of annotating input text was preferred. We highlight qualitative aspects of employing the studied explainability methods and conclude with implications for researchers and engineers that seek to incorporate explanations into user-facing deployments.},
	urldate = {2022-08-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jeyakumar, Jeya Vikranth and Noor, Joseph and Cheng, Yu-Hsi and Garcia, Luis and Srivastava, Mani},
	year = {2020},
	pages = {4211--4222},
	file = {Full Text PDF:/Users/orsonxu/Zotero/storage/IAVRNZ2J/Jeyakumar et al. - 2020 - How Can I Explain This to You An Empirical Study .pdf:application/pdf},
}

@article{bellotti_intelligibility_2001,
	title = {Intelligibility and {Accountability}: {Human} {Considerations} in {Context}-{Aware} {Systems}},
	volume = {16},
	issn = {0737-0024, 1532-7051},
	shorttitle = {Intelligibility and {Accountability}},
	url = {https://www.tandfonline.com/doi/full/10.1207/S15327051HCI16234_05},
	doi = {10.1207/S15327051HCI16234_05},
	abstract = {This essay considers the problem of defining the context that context-aware systems should pay attention to from a human perspective. In particular, we argue that there are human aspects of context that cannot be sensed or even inferred by technological means, so context-aware systems cannot be designed simply to act on our behalf. Rather, they will have to be able to defer to users in an efficient and nonobtrusive fashion. Our point is particularly relevant for systems that are constructed such that applications are architecturally isolated from the sensing and inferencing that governs their behavior. We propose a design framework that is intended to guide thinking about accommodating human aspects of context. This framework presents four design principles that support intelligibility of system behavior and accountability of human users and a number of human-salient details of context that must be accounted for in context-aware system design.},
	language = {en},
	number = {2-4},
	urldate = {2022-08-25},
	journal = {Human–Computer Interaction},
	author = {Bellotti, Victoria and Edwards, Keith},
	month = dec,
	year = {2001},
	pages = {193--212},
	file = {Bellotti and Edwards - 2001 - Intelligibility and Accountability Human Consider.pdf:/Users/orsonxu/Zotero/storage/V53MK9PH/Bellotti and Edwards - 2001 - Intelligibility and Accountability Human Consider.pdf:application/pdf},
}

@inproceedings{zhu_explainable_2018,
	title = {Explainable {AI} for {Designers}: {A} {Human}-{Centered} {Perspective} on {Mixed}-{Initiative} {Co}-{Creation}},
	shorttitle = {Explainable {AI} for {Designers}},
	doi = {10.1109/CIG.2018.8490433},
	abstract = {Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users' needs, and we identify key open challenges.},
	booktitle = {2018 {IEEE} {Conference} on {Computational} {Intelligence} and {Games} ({CIG})},
	author = {Zhu, Jichen and Liapis, Antonios and Risi, Sebastian and Bidarra, Rafael and Youngblood, G. Michael},
	month = aug,
	year = {2018},
	keywords = {Visualization, machine learning, human-computer interaction, Machine learning, Task analysis, Games, game design, explainable artificial intelligence, mixed-initiative co-creation, Neurons, Tools},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:/Users/orsonxu/Zotero/storage/UMZNM5C2/8490433.html:text/html;IEEE Xplore Full Text PDF:/Users/orsonxu/Zotero/storage/SCL2CAYE/Zhu et al. - 2018 - Explainable AI for Designers A Human-Centered Per.pdf:application/pdf},
}

@inproceedings{wolf_explainability_2019,
	address = {Marina del Ray California},
	title = {Explainability scenarios: towards scenario-based {XAI} design},
	isbn = {978-1-4503-6272-6},
	shorttitle = {Explainability scenarios},
	url = {https://dl.acm.org/doi/10.1145/3301275.3302317},
	doi = {10.1145/3301275.3302317},
	abstract = {Integral to the adoption and uptake of AI systems in real-world settings is the ability for people to make sense of and evaluate such systems, a growing area of development and design efforts known as XAI (Explainable AI). Recent work has advanced the state of the art, yet a key challenge remains in understanding unique requirements that might arise when XAI systems are deployed into complex settings of use. In helping envision such requirements, this paper turns to scenario-based design, a method that anticipates and leverages scenarios of possible use early on in system development. To demonstrate the value of the scenario-based design method to XAI design, this paper presents a case study of aging-in-place monitoring. Introducing the concept of “explainability scenarios” as resources in XAI design, this paper sets out a forward-facing agenda for further attention to the emergent requirements of explainability-in-use.},
	language = {en},
	urldate = {2022-11-22},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Wolf, Christine T.},
	month = mar,
	year = {2019},
	pages = {252--257},
	file = {Wolf - 2019 - Explainability scenarios towards scenario-based X.pdf:/Users/orsonxu/Zotero/storage/QHZJV6U7/Wolf - 2019 - Explainability scenarios towards scenario-based X.pdf:application/pdf},
}

@inproceedings{kaur_interpreting_2020,
	address = {Honolulu HI USA},
	title = {Interpreting {Interpretability}: {Understanding} {Data} {Scientists}' {Use} of {Interpretability} {Tools} for {Machine} {Learning}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Interpreting {Interpretability}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376219},
	doi = {10.1145/3313831.3376219},
	abstract = {Machine learning (ML) models are now routinely deployed in domains ranging from criminal justice to healthcare. With this newfound ubiquity, ML has moved beyond academia and grown into an engineering discipline. To that end, interpretability tools have been designed to help data scientists and machine learning practitioners better understand how ML models work. However, there has been little evaluation of the extent to which these tools achieve this goal. We study data scientists’ use of two existing interpretability tools, the InterpretML implementation of GAMs and the SHAP Python package. We conduct a contextual inquiry (N=11) and a survey (N=197) of data scientists to observe how they use interpretability tools to uncover common issues that arise when building and evaluating ML models. Our results indicate that data scientists over-trust and misuse interpretability tools. Furthermore, few of our participants were able to accurately describe the visualizations output by these tools. We highlight qualitative themes for data scientists’ mental models of interpretability tools. We conclude with implications for researchers and tool designers, and contextualize our ﬁndings in the social science literature.},
	language = {en},
	urldate = {2022-11-22},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Wortman Vaughan, Jennifer},
	month = apr,
	year = {2020},
	pages = {1--14},
	file = {Kaur et al. - 2020 - Interpreting Interpretability Understanding Data .pdf:/Users/orsonxu/Zotero/storage/QEDS36CI/Kaur et al. - 2020 - Interpreting Interpretability Understanding Data .pdf:application/pdf},
}
