
@inproceedings{gajos_exploring_2006,
	address = {Venezia, Italy},
	title = {Exploring the design space for adaptive graphical user interfaces},
	isbn = {978-1-59593-353-9},
	url = {http://portal.acm.org/citation.cfm?doid=1133265.1133306},
	doi = {10.1145/1133265.1133306},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the working conference on {Advanced} visual interfaces  - {AVI} '06},
	publisher = {ACM Press},
	author = {Gajos, Krzysztof Z. and Czerwinski, Mary and Tan, Desney S. and Weld, Daniel S.},
	year = {2006},
	pages = {201},
	file = {Gajos et al. - 2006 - Exploring the design space for adaptive graphical .pdf:/Users/orsonxu/Zotero/storage/57E6AHJU/Gajos et al. - 2006 - Exploring the design space for adaptive graphical .pdf:application/pdf},
}

@inproceedings{lindlbauer_context-aware_2019,
	address = {New Orleans LA USA},
	title = {Context-{Aware} {Online} {Adaptation} of {Mixed} {Reality} {Interfaces}},
	isbn = {978-1-4503-6816-2},
	url = {https://dl.acm.org/doi/10.1145/3332165.3347945},
	doi = {10.1145/3332165.3347945},
	abstract = {We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved effciently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36\%.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar},
	month = oct,
	year = {2019},
	pages = {147--160},
	file = {Lindlbauer et al. - 2019 - Context-Aware Online Adaptation of Mixed Reality I.pdf:/Users/orsonxu/Zotero/storage/MWPG2AAF/Lindlbauer et al. - 2019 - Context-Aware Online Adaptation of Mixed Reality I.pdf:application/pdf},
}

@inproceedings{lages_walking_2019,
	address = {Marina del Ray California},
	title = {Walking with adaptive augmented reality workspaces: design and usage patterns},
	isbn = {978-1-4503-6272-6},
	shorttitle = {Walking with adaptive augmented reality workspaces},
	url = {https://dl.acm.org/doi/10.1145/3301275.3302278},
	doi = {10.1145/3301275.3302278},
	abstract = {Mobile augmented reality may eventually replace our smartphones as the primary way of accessing information on the go. However, current interfaces provide little support to walking and to the variety of actions we perform in the real world. To achieve its full potential, augmented reality interfaces must support the fluid way we move and interact in the physical world. We explored how different adaptation strategies can contribute towards this goal. We evaluated design alternatives through contextual studies and identified the key interaction patterns that interfaces for walking should support. We also identified desirable properties of adaptation-based interface techniques, which can be used to guide the design of the next-generation walking-centered augmented reality workspaces.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Lages, Wallace S. and Bowman, Doug A.},
	month = mar,
	year = {2019},
	pages = {356--366},
	file = {Lages and Bowman - 2019 - Walking with adaptive augmented reality workspaces.pdf:/Users/orsonxu/Zotero/storage/8LHHFKHA/Lages and Bowman - 2019 - Walking with adaptive augmented reality workspaces.pdf:application/pdf},
}

@inproceedings{qian_scalar_2022,
	address = {New Orleans LA USA},
	title = {{ScalAR}: {Authoring} {Semantically} {Adaptive} {Augmented} {Reality} {Experiences} in {Virtual} {Reality}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {{ScalAR}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517665},
	doi = {10.1145/3491102.3517665},
	abstract = {Augmented Reality (AR) experiences tightly associate virtual contents with environmental entities. However, the dissimilarity of different environments limits the adaptive AR content behaviors under large-scale deployment. We propose ScalAR, an integrated workflow enabling designers to author semantically adaptive AR experiences in Virtual Reality (VR). First, potential AR consumers collect local scenes with a semantic understanding technique. ScalAR then synthesizes numerous similar scenes. In VR, a designer authors the AR contents’ semantic associations and validates the design while being immersed in the provided scenes. We adopt a decision-tree-based algorithm to fit the designer’s demonstrations as a semantic adaptation model to deploy the authored AR experience in a physical scene. We further showcase two application scenarios authored by ScalAR and conduct a two-session user study where the quantitative results prove the accuracy of the AR content rendering and the qualitative results show the usability of ScalAR.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Qian, Xun and He, Fengming and Hu, Xiyun and Wang, Tianyi and Ipsita, Ananya and Ramani, Karthik},
	month = apr,
	year = {2022},
	pages = {1--18},
	file = {Qian et al. - 2022 - ScalAR Authoring Semantically Adaptive Augmented .pdf:/Users/orsonxu/Zotero/storage/NIUHWNXZ/Qian et al. - 2022 - ScalAR Authoring Semantically Adaptive Augmented .pdf:application/pdf},
}

@inproceedings{huang_adaptutar_2021,
	address = {Yokohama Japan},
	title = {{AdapTutAR}: {An} {Adaptive} {Tutoring} {System} for {Machine} {Tasks} in {Augmented} {Reality}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {{AdapTutAR}},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445283},
	doi = {10.1145/3411764.3445283},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Huang, Gaoping and Qian, Xun and Wang, Tianyi and Patel, Fagun and Sreeram, Maitreya and Cao, Yuanzhi and Ramani, Karthik and Quinn, Alexander J.},
	month = may,
	year = {2021},
	pages = {1--15},
	file = {Huang et al. - 2021 - AdapTutAR An Adaptive Tutoring System for Machine.pdf:/Users/orsonxu/Zotero/storage/W2ENEVJU/Huang et al. - 2021 - AdapTutAR An Adaptive Tutoring System for Machine.pdf:application/pdf},
}

@inproceedings{tatzgern_adaptive_2016,
	title = {Adaptive information density for augmented reality displays},
	doi = {10.1109/VR.2016.7504691},
	abstract = {Augmented Reality (AR) browsers show geo-referenced data in the current view of a user. When the amount of data grows too large, the display quickly becomes cluttered. Clustering items by spatial and semantic attributes can temporarily alleviate the issue, but is not effective against an increasing amount of data. We present an adaptive information density display for AR that balances the amount of presented information against the potential clutter created by placing items on the screen. We use hierarchical clustering to create a level-of-detail structure, in which nodes closer to the root encompass groups of items, while the leaf nodes contain single items. Our method selects items and groups from different levels of this hierarchy based on user-defined preferences and on the amount of visual clutter caused by placing these items. The number of presented items is adapted during user interaction to avoid clutter. We compare our interface to a conventional AR browser interface in a qualitative user study. Users clearly preferred our interface, because it provided a better overview of the data and allowed for easier comparison. In a second study, we evaluated the effect of different degrees of clustering on search and recall tasks. Users generally made fewer errors, when using our interface for a search task, which indicates that the reduced clutter allowed them to stay focused on finding the relevant items.},
	booktitle = {2016 {IEEE} {Virtual} {Reality} ({VR})},
	author = {Tatzgern, Markus and Orso, Valeria and Kalkofen, Denis and Jacucci, Giulio and Gamberini, Luciano and Schmalstieg, Dieter},
	month = mar,
	year = {2016},
	note = {ISSN: 2375-5334},
	keywords = {Visualization, Browsers, Clustering algorithms, Clutter, Data visualization, Electronic mail, Semantics},
	pages = {83--92},
	file = {IEEE Xplore Abstract Record:/Users/orsonxu/Zotero/storage/XNIHICME/7504691.html:text/html;IEEE Xplore Full Text PDF:/Users/orsonxu/Zotero/storage/VFJ9AN2W/Tatzgern et al. - 2016 - Adaptive information density for augmented reality.pdf:application/pdf},
}

@inproceedings{lu_exploring_2022,
	address = {New Orleans LA USA},
	title = {Exploring {Spatial} {UI} {Transition} {Mechanisms} with {Head}-{Worn} {Augmented} {Reality}},
	isbn = {978-1-4503-9157-3},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517723},
	doi = {10.1145/3491102.3517723},
	abstract = {Imagine in the future people comfortably wear augmented reality (AR) displays all day, how do we design interfaces that adapt to the contextual changes as people move around? In current operating systems, the majority of AR content defaults to staying at a fxed location until being manually moved by the users. However, this approach puts the burden of user interface (UI) transition solely on users. In this paper, we frst ran a bodystorming design workshop to capture the limitations of existing manual UI transition approaches in spatially diverse tasks. Then we addressed these limitations by designing and evaluating three UI transition mechanisms with different levels of automation and controllability (low-efort manual, semi-automated, fully-automated). Furthermore, we simulated imperfect contextual awareness by introducing prediction errors with diferent costs to correct them. Our results provide valuable lessons about the trade-ofs between UI automation levels, controllability, user agency, and the impact of prediction errors.},
	language = {en},
	urldate = {2022-07-17},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lu, Feiyu and Xu, Yan},
	month = apr,
	year = {2022},
	pages = {1--16},
	file = {Lu and Xu - 2022 - Exploring Spatial UI Transition Mechanisms with He.pdf:/Users/orsonxu/Zotero/storage/BYKC4GHM/Lu and Xu - 2022 - Exploring Spatial UI Transition Mechanisms with He.pdf:application/pdf},
}

@inproceedings{cheng_semanticadapt_2021,
	title = {{SemanticAdapt}: {Optimization}-based {Adaptation} of {Mixed} {Reality} {Layouts} {Leveraging} {Virtual}-{Physical} {Semantic} {Connections}},
	abstract = {We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users ∗This work was done while Yifei Cheng was an intern at Tsinghua University.},
	language = {en},
	booktitle = {Proceedings of the 34th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	author = {Cheng, Yifei and Yan, Yukang and Yi, Xin and Shi, Yuanchun and Lindlbauer, David},
	year = {2021},
	pages = {16},
	file = {Cheng - 2021 - SemanticAdapt Optimization-based Adaptation of Mi.pdf:/Users/orsonxu/Zotero/storage/4Q4FNCW7/Cheng - 2021 - SemanticAdapt Optimization-based Adaptation of Mi.pdf:application/pdf},
}

@inproceedings{duchowski_index_2018,
	address = {Montreal QC Canada},
	title = {The {Index} of {Pupillary} {Activity}: {Measuring} {Cognitive} {Load} \textit{vis-à-vis} {Task} {Difficulty} with {Pupil} {Oscillation}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {The {Index} of {Pupillary} {Activity}},
	url = {https://dl.acm.org/doi/10.1145/3173574.3173856},
	doi = {10.1145/3173574.3173856},
	abstract = {A novel eye-tracked measure of the frequency of pupil diameter oscillation is proposed for capturing what is thought to be an indicator of cognitive load. The proposed metric, termed the Index of Pupillary Activity, is shown to discriminate task difﬁculty vis-à-vis cognitive load (if the implied causality can be assumed) in an experiment where participants performed easy and difﬁcult mental arithmetic tasks while ﬁxating a central target (a requirement for replication of prior work). The paper’s contribution is twofold: full documentation is provided for the calculation of the proposed measurement which can be considered as an alternative to the existing proprietary Index of Cognitive Activity (ICA). Thus, it is possible for researchers to replicate the experiment and build their own software which implements this measurement. Second, several aspects of the ICA are approached in a more data-sensitive way with the goal of improving the measurement’s performance.},
	language = {en},
	urldate = {2022-07-20},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Duchowski, Andrew T. and Krejtz, Krzysztof and Krejtz, Izabela and Biele, Cezary and Niedzielska, Anna and Kiefer, Peter and Raubal, Martin and Giannopoulos, Ioannis},
	month = apr,
	year = {2018},
	pages = {1--13},
	file = {Duchowski et al. - 2018 - The Index of Pupillary Activity Measuring Cogniti.pdf:/Users/orsonxu/Zotero/storage/YEHGLN9S/Duchowski et al. - 2018 - The Index of Pupillary Activity Measuring Cogniti.pdf:application/pdf},
}
