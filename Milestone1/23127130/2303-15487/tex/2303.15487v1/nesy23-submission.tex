%% The first command in your LaTeX source must be the \documentclass command.
%%
%% Options:
%% twocolumn : Two column layout.
%% hf: enable header and footer.
\documentclass[
% twocolumn,
% hf,
]{ceurart}

%%
%% One can fix some overfulls
\sloppy

%%
%% Minted listings support 
%% Need pygment <http://pygments.org/> <http://pypi.python.org/pypi/Pygments>
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage[labelformat=parens,labelsep=quad, skip=3pt]{caption}
\usepackage{graphicx}
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
%% auto break lines
\lstset{breaklines=true}

% TODONOTES
% TO SUPPRESS TODOS: add disable in package options
%\usepackage[disable]{todonotes} %,disable
\usepackage{todonotes} %,disable
\newcommand{\sarah}[2][]{\todo[inline,color=pink,#1]{SC: {\small #2}}}
\newcommand{\lw}[2][]{\todo[inline,color=green,#1]{LW: {\small #2}}}
\newcommand{\pg}[2][]{\todo[inline,color=blue!20,#1]{PG: {\small #2}}}
\newcommand{\nl}[2][]{\todo[inline,color=yellow!20,#1]{NL: {\small #2}}}
\newcommand{\gd}[2][]{\todo[inline,color=orange!20,#1]{GD: {\small #2}}}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% Rights management information.
%% CC-BY is default license.
\copyrightyear{2023}
\copyrightclause{Copyright for this paper by its authors.
  Use permitted under Creative Commons License Attribution 4.0
  International (CC BY 4.0).}

%%
%% This command is for the conference information
\conference{NeSy'23: 17th International Workshop on Neural-symbolic Learning and Reasoning, Siena, Italy
3rd-5th July 2023}

%%
%% The "title" command
\title{Knowledge Enhanced Graph Neural Networks}

%\tnotemark[1]

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
\author[1,2]{Luisa Werner}[%
orcid=0000-0002-1431-6490,
email=luisa.werner@inria.fr
]
%\cormark[1]
%\fnmark[1]
\address[1]{Institut National de Recherche en Sciences et Technologies du Numérique (INRIA),
655 Av. de l'Europe, 38330 Montbonnot-Saint-Martin, France}
\address[2]{Université Grenoble Alpes (UGA), 621 Av. Centrale, 38400 Saint-Martin-d'Hères, France}

\author[1]{Nabil Laya\"ida}[%
orcid=0000-0001-8472-9365,
email=nabil.layaida@inria.fr
]
%\fnmark[1]
% \address[3]{Vrije Universiteit Amsterdam, De Boelelaan 1105, 1081 HV Amsterdam, The Netherlands}

\author[1]{Pierre Genevès}[%
orcid=0000-0001-7676-2755,
email=pierre.geneves@inria.fr
]
%\fnmark[1]

\author[1]{Sarah Chlyah}[%
orcid=0009-0004-1769-5109,
email=sarah.chlyah@inria.fr
]


%% Footnotes
%\fntext[1]{These authors contributed equally.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Graph data is omnipresent and has a large variety of applications such as natural science, social networks or semantic web. 
Though rich in information, graphs are often noisy and incomplete.
Therefore, graph completion tasks such as node classification or link prediction have gained attention.
On the one hand, neural methods such as graph neural networks have proven to be robust tools for learning rich representations of noisy graphs.
On the other hand, symbolic methods enable exact reasoning on graphs.
We propose KeGNN, a neuro-symbolic framework for learning on graph data that combines both paradigms and allows for the integration of prior knowledge into a graph neural network model.
In essence, KeGNN consists of a graph neural network as a base on which knowledge enhancement layers are stacked with the objective of refining predictions with respect to prior knowledge.
We instantiate KeGNN in conjunction with two standard graph neural networks: Graph Convolutional Networks and Graph Attention Networks, and evaluate KeGNN on multiple benchmark datasets for node classification.
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\begin{keywords}
  neuro-symbolic integration \sep
  graph neural networks \sep
  relational learning \sep 
  knowledge graphs \sep
  fuzzy logic 
\end{keywords}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Graphs are ubiquitous across diverse real-world applications such as e-commerce \cite{gnn_ecommerce}, natural science \cite{pmlr-v80-sanchez-gonzalez18a} or social networks \cite{Wu_Lian_Xu_Wu_Chen_2020}.
Graphs connect nodes by edges and allow to enrich them with features. 
This makes them a versatile and powerful data structure that encodes relational information.
As graphs are often derived from noisy data, incompleteness and errors are common issues.
Consequently, graph completion tasks such as node classification or link prediction have become increasingly important. 
These tasks are approached from different directions. 
In the field of deep learning, research on graph neural networks (GNNs) has gained momentum.
Numerous models have been proposed for various graph topologies and applications \cite{heterogeneous_gnn} \cite{dl_on_graphs} \cite{gnn_survey} \cite{large_scale_graph_training}. 
The key strength of GNNs is to find meaningful representations of noisy data, that can be used for various prediction tasks \cite{GNNBook2022}.
Despite this advantage, as a subcategory of deep learning methods, GNNs are criticized for their limited interpretability and large data consumption \cite{neuro-symbolic_survey}. 
Alongside, the research field of symbolic AI addresses the above-mentioned tasks.
In symbolic AI, solutions are found by performing logic-like reasoning steps that are exact, interpretable and data-efficient \cite{KR2022-51} \cite{prolog} \cite{survey_nesy}.
For large graphs, however, symbolic methods are often computationally expensive or even infeasible.
Since techniques from deep learning and from symbolic AI have complementary pros and cons, the field of neuro-symbolic integration aims to combine both paradigms. 
Neuro-symbolic AI not only paves the way towards the application of AI to learning with limited data, but also allows for jointly using symbolic information (in the form of logical rules) and sub-symbolic information (in the form of real-valued data).
This leads on the one hand to more versatile models. 
On the other hand, it helps to overcome the blackbox nature of deep learning and to improve interpretability through symbolic representations. \cite{GARNELO201917} \cite{rnm} \cite{neuro-symbolic_survey}. 

In this paper, we present the neuro-symbolic approach \textbf{K}nowledge \textbf{e}nhanced \textbf{G}raph \textbf{N}eural \textbf{N}etworks (KeGNN) to conduct node classification given real-valued graph data and a set of prior knowledge.
In KeGNN, differentiable knowledge enhancement layers are stacked on top of a GNN and adjust its predictions in order to increase the satisfaction of a set of prior knowledge.
Additionally to the parameters of the GNN, the knowledge enhancement layers contain learnable clause weights that reflect the impact of the prior knowledge.
Both components form an end-to-end differentiable model. 

In the multifaceted neuro-symbolic field, KeGNN can be placed in the category of knowledge-guided learning \cite{kenn}, where the focus lies on learning in the presence of additional supervision induced by prior knowledge. 
Among these approaches, KeGNN belongs to model-based approaches where prior knowledge is an integral part of the model in the form of knowledge enhancement layers.
KeGNN can be seen as an extension to knowledge enhanced neural networks (KENN) \cite{relational_kenn}, which stack knowledge enhancement layers onto a multi-layer perceptron (MLP). 
However, an MLP is not powerful enough to incorporate graph structure into the representations. 
Thus, relational information can only be introduced by binary predicates in the symbolic part. 
In contrast, KeGNN is based on GNNs that can handle graph structure, which makes both the neural and the symbolic component sufficiently powerful to exploit graph structure. 
In this work, we instantiate KeGNN in conjunction with two popular GNNs: Graph Attention Networks \cite{gat} and Graph Convolutional Networks \cite{gcn}. We apply KeGNN to the benchmark datasets for node classification Cora, Citeseer, PubMed \cite{planetoid_cite} and Flickr \cite{graph_saint}.

% Another class of neuro-symbolic approaches adds residual layers to a neural component in order to adjust the predictions with respect to prior knowledge. 
% The goal is to increase the satisfaction of some prior knowledge by refining the predictions.
% Knowledge enhanced neural networks \cite{kenn} is an approach from this category, which nests additional layers on a multi-layer perceptron as base model \cite{relational_kenn}.




% Recently, remarkable progress has been made in various research domains thanks to deep learning and the application of artificial neural networks. 
% The major strength of neural networks is their ability to extract meaningful features from high-dimensional data without any expert knowledge while being robust to noise. 
% Despite their success, deep learning models are often criticised for their shortcomings in terms of interpretability, accountability and data-hungriness \cite{neuro-symbolic_survey}.
% While deep learning approaches are mostly data-driven, symbolic AI models carry out logic-like reasoning steps over language-like representations while being more data efficient and understandable by humans \cite{GARNELO201917}.

% In order to address the limitations of deep learning, the research field of neural-symbolic integration has emerged with the aim to combine neural approaches with symbolic methods.
% The integration of sub-symbolic (neural) approaches and symbolic approaches has a large potential.
% Neural-symbolic AI not only paves the way towards the application of AI to situations with limited data. 
% Also, it allows to jointly use different sources of information. 
% This leads on the one hand to richer models and a wider range of applications of AI.
% On the other hand, it helps to tackle the black-box property of deep learning and avoid undesired effects (such as discrimination of social groups for example) by exploiting symbolic representations.
% Neural-symbolic integration research has the potential to improve the accuracy, data efficiency, and interpretability of the current state of the art in AI which can lead to new insights in a wide range of research domains \cite{GARNELO201917} \cite{rnm} \cite{neuro-symbolic_survey}. 

% Introduction on Graphs


% Graph Neural Networks (GNNs) \cite{gnn_survey} \cite{dl_on_graphs} have proven successful as deep learning method adapted to graphs. 
% By message passing, GNNs are able to propagate information between nodes and thus find informative representations that integrate the graph structure.
% Typical tasks solved by GNNs are node classification, link prediction and graph classification.  
% As GNNs belong to the class of deep learning methods, they are also affected by the above mentioned drawbacks of deep learning models and could benefit from neuro-symbolic integration \cite{gnn_meets_nesy}.

% To fully exploit graph data, both the neural and the symbolic components must be able to process relations. 
% While a GNN is a graph-adapted method from the neural perspective, from the symbolic viewpoint the relations can be encoded as binary predicates.
% A neuro-symbolic approach that is able to process graphs in its symbolic component, are Knowledge Enhanced Neural Networks (KENN) \cite{relational_kenn}. 
% \lw{only name KENN, then put the properties to KeGNN}
% \lw{integrate related work here or dedicated section?}
% KENN stacks knowledge enhancement layers as additional layers on top of a base neural network. 
% These layers modify the outputs of the network with respect to some prior knowledge in the form of first-order logic clauses. 
% Both, the knowledge enhancement layers as well as the base neural network are fully differentiable and can be optimised jointly with backpropagation. 
% The knowledge enhancement layers contain learnable clause weights that are modified during training and allow the model to be robust to wrong knowledge by setting the respective weights to zero. 
% Furthermore, the clause weights indicate the strength of each clause which increases interpretability.
% In \cite{relational_kenn}, however, KENN is only used in conjunction with an multi-layer perceptron that disregards the relational property of the graph data. 

% In this work, we propose Knowledge Enhanced Graph Neural Networks (KeGNN), which stack knowledge enhancement layers on top of a graph neural network \cite{GNNBook2022}. 
% Hence, relational information is processed not only in the enhancement layers, but already at the base neural network stage. 
% We test our model on several benchmark datasets on a node classification task and reach state of the art performance. 
% Furthermore, we can show that knowledge enhancement layers are also useful in connection with a graph neural network as base. 

\section{Method: Knowledge Enhanced Graph Neural Networks}
KeGNN is a neuro-symbolic approach that can be applied
to node classification tasks
% and  can be seen as an extension to knowledge enhanced neural networks \cite{relational_kenn} 
with the capacity of handling graph structure at the base neural network level. 
% More precisely, at its core, KeGNN consists of two components that can both handle graph data and together form an end-to-end differentiable model. 
% As a base predictor, a graph neural network produces pre-activations based on node-level and edge-level numerical information.
% Knowledge enhancement layers refine these predictions guided by a set of prior knowledge with the goal of increasing the satisfaction of defined knowledge.
The model takes two types of input: (1) real-valued graph data and (2) prior knowledge expressed in first-order logic. 
% First, we specify how the input to both components is defined and how knowledge can be interpreted in the real valued domain. 
% We then describe how KeGNN is composed of graph neural network and knowledge enhancement layers. 

\subsection{Graph-structured Data}
\label{sec:graphstructured_data}
% We define a graph as follows.
A Graph $\mathbf{G}=(\mathbf{N}, \mathbf{E})$ consists of set of $n$ nodes $\mathbf{N}$ and edges $\mathbf{E}$ connecting two nodes $v_i, v_j \in \mathbf{N}$ with an edge $(v_i, v_j) \in \mathbf{E}$.
% The number of nodes in the graph is $n$.
The adjacency matrix $\mathbf{A}$ describes the edges.
% $\mathbf{A}_{ij}$ denotes whether an edge from the node $v_i$ to the node $v_j$ exists: It takes the value $1$ if the edge exists and $0$ otherwise.
% The values in the diagonal of $\mathbf{A}$ represent self-connections.
% The graph is \emph{undirected} if $\mathbf{A}$ is symmetric, else \emph{directed}. 
The neighborhood $\mathcal{N}(v_i)$ describes the set of first-order neighbors of $v_i$. 
% The set $\mathcal{N}^{(1)}(v_i)$ contains all nodes that are linked to $v_i$ with one edge including $v_i$ itself. 
For an \emph{attributed} and \emph{labelled} graph, nodes are enriched with node features and labels.
Each node $v_i$ is attributed with a feature vector $\mathbf{x}_i \in \mathbb{R}^{d}$ with feature dimension $d$ and ground truth label vector $\mathbf{y}_i \in \mathbb{R}^m$ for $m$ classes. 
% In matrix notation, features and labels are described as $\mathbf{X} \in \mathbb{R}^{n \times d}$ and $\mathbf{Y} \in \mathbb{R}^{n \times m}$, respectively. 
Graphs that consist of a single node and edge type are called \emph{homogeneous}.
In contrast, nodes and edges may have different types for \emph{heterogeneous} graphs \cite{dl_on_graphs}.

\begin{example}[Citation Graph]
\label{ex:graph_cit}
A Citation Graph $\mathbf{G}_{Cit}$ consists of documents that can cite each other.
The documents are represented by nodes $\mathbf{N}_{Cit}$ and citations by edges $\mathbf{E}_{Cit}$.
Documents can be attributed with features $\mathbf{X}_{Cit}$ that describe their content in matrix format as Word2Vec \cite{word2vec} vectors.
Each document is labelled with a topic category that is described in $\mathbf{Y}_{Cit}$.
Since all nodes have the same type (documents) and the graph contains only one edge type (citation), $\mathbf{G}_{Cit}$ is homogeneous. 
\end{example}

\subsection{Prior Knowledge} 
\label{sec:prior_knowledge}
The prior knowledge $\mathcal{K}$ provided for KeGNN can be described as a set of logical formulae expressed in the function-free logical language $\mathcal{L}$ that contains a set of constants $\mathcal{C}$, variables $\mathcal{X}$ and predicates $\mathcal{P}$.
% Predicates can be unary or binary. 
Unary predicates express properties of variables, %or constants 
while binary predicates express relations. 
$\mathcal{L}$ supports negation ($\neg$) and disjunction ($\lor$).
A clause in the set of prior knowledge $c_i \in \mathcal{K} = \{c_0, \hdots , c_{|\mathcal{K}|}\}$ can be formulated as a disjunction of literals ${\scriptstyle\bigvee_{j=1}^{q} l_{j}}$ with $q$ literals in a clause.
% \begin{equation}
%   c_i = \bigvee_{j=1}^{k_{c_i}} l_{j}
% \end{equation}
% The assumption of no repeated literals is made.
Since the prior knowledge is general, all formulae are assumed to be universally quantified and free of constants. 
Clauses can be \emph{grounded} by assigning free variables to constants. 
The set of all grounded clauses is $\mathcal{G}(\mathcal{K}, \mathcal{C})$. 
% The prior knowledge is composed of unary and binary clauses $\mathcal{K} = \mathcal{K}_U \cup \mathcal{K}_B$ where clauses in $\mathcal{K}_U$ contain only unary predicates while clauses in $\mathcal{K}_B$ contain both unary and binary predicates. 

\begin{example}[Prior Knowledge on Citations and Topics]
  \label{ex:prior_knowledge_cit}
  Let the unary predicate $\mathrm{AI(x)}$ describe that any scientific paper $x$ belongs to the document class $\mathrm{AI}$ (from the artificial intelligence domain) and let the binary predicate $\mathrm{Cite(x,y)}$ denote a citation between two papers $x$ and $y$. 
  The clause $\forall x, y: \neg \mathrm{AI(x)} \lor \neg \mathrm{Cite(x,y)} \lor \mathrm{AI(y)}$ 
  describes that if paper $x$ is about $\mathrm{AI}$ and cites paper $y$, paper $y$ is also about $\mathrm{AI}$. 
% It can be grounded to the constants $a$ and $b$ (that represent particular documents):
% $\neg \mathrm{AI(a)} \lor \neg \mathrm{Cite(a,b)} \lor \mathrm{AI(b)}$.
% Here, the variable $x$ is assigned to constant $a$ and variable $y$ to constant $b$. 
\end{example}

\subsection{Fuzzy Semantics}
\label{sec:fuzzy_semantics}
Let us consider an attributed, labelled and homogeneous graph $\mathbf{G}$ and a set of prior knowledge  $\mathcal{K}$ as previously defined.  
While $\mathcal{L}$ describes logical formulae,
the neural component in KeGNN relies on continuous and differentiable representations. 
To interpret boolean logic in the real-valued domain, fuzzy logic \cite{fuzzy_logic} is used that maps binary truth values into the continuous interval $[0,1]$.
% Constants are interpreted as real-valued vectors in $\mathbf{X}^0\in\mathbb{R}^{n \times d_0}$.
% The unary predicates are represented by differentiable functions $f_G(\mathbf{X}, \mathbf{E}, \theta) \mapsto [0,1]$ so that the truth values of the grounded atoms can be obtained with a graph neural network that return continuous truth values in the interval $[0,1]$ and has a set of learnable parameters $\Phi$. 
A constant is interpreted as a real-valued node feature vector $\mathbf{x} \in \mathbb{R}^d$. 
A unary predicate $P_U$ is interpreted as a differentiable function $f_{P_U}: \mathbb{R}^{d} \mapsto [0,1]$ that takes the vector $\mathbf{x}$ of a constant as input and returns the truth values $t$ of $P_U(\mathbf{x})$. 
% This way, the truth values of the grounded atoms can be obtained with a GNN.
% The activation function $\sigma$ maps the output of the GNN to the interval $[0,1]$ to ensure valid truth values.
% In this setting, the assumption is made that the graph structure on which KeGNN operates is known apriori and assumed to be exact while only the node labels are noisy. 
% For this reason, the truth values of the binary atoms (corresponding to the edges in the graph) are interpreted as positive values in the real domain.
% %  instead of being predicted by a neural component.
T-conorm functions $\bot: [0,1] \times [0,1] \mapsto [0,1]$ \cite{klement2013triangular} map real-valued truth values of two literals to the truth value of their disjunction. 
%Popular t-conorm functions are Product, G\"odel and Łukasiewicz \cite{rnm}.
% Popular t-conorm functions are defined as follows \cite{rnm}. 
% \begin{itemize}
%   \item Product: $\bot(x,y) \mapsto x + y - x \cdot y$
%   \item G\"odel: $\bot(x,y) \mapsto max(x,y)$
%   \item Lukasiewicz: $\bot(x,y) \mapsto min(1, x + y)$
% \end{itemize}
% In this work, G\"odel Logic \cite{rnm} \cite{tconorms} is used. 
The G\"odel t-conorm function for two truth values $t_1, t_2$ is defined as $\bot(t_1, t_2) \mapsto \operatorname{max}(t_1, t_2)$.
To obtain the truth value of a clause $c = l_1 \lor ... \lor l_q$, $\bot$ is extended to a vector $t$ of $q$ truth values: $\bot(t_1, t_2, ..., t_q) = \bot(t_1,\bot(t_2... \bot(t_{q-1}, t_q)))$. 
Fuzzy negation is defined as $t \mapsto 1-t$ \cite{fuzzy_logic}.

\subsection{Model Architecture}
The way KeGNN computes the final predictions can be divided in two stages. 
First, a GNN predicts the node classes given the node features and the edges.
Subsequently, the knowledge enhancement layers use the predictions as truth values for the grounded unary predicates and update them with respect to the knowledge.
An overview of KeGNN is given in Fig. \ref{fig:kegnn}.
\begin{figure}[]
  \centering
  \centering\includegraphics[width=10cm]{KeGNN_Overview.png}
  \caption{\begin{footnotesize} Overview of KeGNN. \end{footnotesize}
  \label{fig:kegnn}}
  \end{figure}%
% by adding changes obtained with the boost function in Eq. \ref{eq:softmax_goedel}.
% The outputs of the knowledge enhancement layers are activated by a logarithmic softmax function to obtain the final predictions.  

\subsubsection{Neural Component}
The role of the neural component is to exploit feature information in the graph structure.
The key strength of a GNN is to enrich node representations with graph structure by nesting 
multiple message passing layers \cite{GNNBook2022}. 
Per layer, the representations of neighboring nodes are aggregated and combined to obtain updated representations. 
A message passing layer $k$ is written as: 
\begin{equation}
%\scriptstyle
v_{(i, k+1)} = \operatorname{combine} \bigl( v_{(i, k)}, \operatorname{aggregate} \bigl(\{v_{(j,k)} | v_j \in \mathcal{N}(v_i)\}\bigr) \bigr)
\end{equation}
The layers contain learnable parameters that can be optimized with backpropagation. 
In KeGNN, the GNN implements the function $f_{P_U}$ (see section \ref{sec:fuzzy_semantics}) that predicts truth values for grounded unary predicates. 
In this work, we consider two well-known GNNs as components for KeGNN: Graph Convolutional Networks (GCN) \cite{gcn} and Graph Attention Networks (GAT) \cite{gat}.
While GCN considers the graph structure as given, GAT allows for assessing the importance of the neighbors with attention weights $\alpha_{ij}$ between node $v_i$ and node $v_j$.
% The attention weights define distributions over neighbors and can be seen as transition probability from node to node.
In case of multi-head attention, the attention weights are calculated multiple times and concatenated \cite{attention_is_all_you_need}. 

% -------------- Here begins old neural component --------------
% \subsubsection{Neural Component}
% The role of the neural component is to exploit information present in the graph structure.
% The essential strength of a graph neural network is to iteratively obtain updated node representations $\mathbf{X}^{(k)}$ enriched with graph structure by computing aggregates over nodes and their neighboring nodes. 
 
% The layers contain learnable parameters that can be optimized with backpropagation during training and the representations obtained can be used as standalone components for various downstream tasks.
% Thus, in KeGNN, their preactivations $\mathbf{z}$ obtained in the final layer serve as input to the symbolic component and serve as grounded truth values for the unary predicates.
% In this work, we consider two well-known aggregation operators on graphs: The graph convolutional operator \cite{gcn} and the graph attentional operator \cite{gat} as components for KeGNN.
% The graph neural networks composed of the former components, respectively, are called graph convolutional network (GCN) and graph attention network (GAT). 
% \subsubsection*{Graph Convolutional Networks}
% \lw{See wikialumni gCN part }
% \label{sec:gcn}
% GCN takes as input the initial node features $\mathbf{X}^{(0)} \in \mathbb{R}^{n \times d_0}$, the adjacency matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ and a set of learnable parameters $\theta_{\text{GCN}}$ in order to produce the preactivations $\mathbf{Z} \in \mathbb{R}^{n \times m}$ for $m$ output classes. 
% We encode the representation in layer $k$ as $\mathbf{X}^{(k)}$.
% The GCN has $K$ layers $k=\{0, \hdots, K\}$ that implement \emph{graph convolution} \cite{gcn} as their core operator to aggregate and combine the node representations. They are aggregated by weighted averaging over representations of the neighbors (including the node itself). Then, the representation of each node is updated including the weighted neighbors as follows: 
% \begin{equation}
%   \mathbf{X}^{k}=\sigma\left(\tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{X}^{(k-1)} \mathbf{W}^{(k)}\right)
%   \end{equation}
% $\tilde{\mathbf{D}}$ is a diagonal matrix of $\mathbf{A}$ with $\tilde{\mathbf{D}}_{ii} = \sum_j \tilde{\mathbf{A}}_{ij}$ with $\tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}$. 
% $\sigma$ indicates the activation function in each layer.
% The trainable parameters of the k-th layer are $\mathbf{W}^k \in \mathbb{R}^{d_{k-1}\times d_{k}}$ for the $k$-th layer.


% \subsubsection*{Graph Attention Networks}
% While GCN considers the graph structure as given, GAT allows for assessing the importance of the neighbours with attention weights and is thus also suited for the application to noisy graphs. The attention weights $\alpha_{ij}$ between node $v_i$ and node $v_j$ in layer $k$ is determined with the attention mechanism \cite{attention_is_all_you_need}.
% The attention weights define distributions over neighbors and can be seen as transition probability from node to node.
% In GAT \cite{gat}, the attention mechanism is defined as single-layer feedforward neural network including linear transformations with the weight vector $W_2 \in \mathbb{R}^{1 \times 2d}$. 
% The updated representation of nodes in the subsequent layer is a linear combination of neighborhood nodes with the weights given by the attention mechanism:  
% \begin{equation}
%   \mathbf{X}^k = \sigma\left(\sum_{v_{j} \in \mathcal{N}(i)} \alpha_{ij} \mathbf{W}^{(k)}\mathbf{X}_j^{(k-1)}\right)
% \end{equation}
% In the case of multi-head attention, the attention weights are calculated multiple times independently and the results are concatenated \cite{attention_is_all_you_need}. 


\subsubsection{Symbolic Component}
To refine the predictions of the GNN, one or more knowledge enhancement layers are stacked on top of it with the goal to increase the satisfaction of the prior knowledge.
The layers consist of \emph{clause enhancers} for each clause in the prior knowledge that implement a so-called differentiable \emph{boost function} \cite{kenn}.
It returns adjustments for the GNN's predictions with respect to a clause. 

Let us consider the grounded clause $c = l_1 \lor ... \lor l_q$ and the vector $t=(t_1, ..., t_q)$ containing the truth values of the grounded literals. 
A boost functions $\phi: \mathbb{R}^q \mapsto \mathbb{R}^q$ provides changes to the $q$ initial truth values $t_i$ predicted by the GNN.
The changes are computed with the objective to increase the value of the t-conorm $\bot(t)$, which measures the satisfaction of the clause.
The following function \cite{kenn} is used as a differentiable approximate for the G\"odel t-conorm boost function:
\begin{equation}
  \label{eq:softmax_goedel} 
  %\scriptstyle
  \phi_{w_c}^{s}(\mathbf{z})_i=w_c \cdot \operatorname{softmax}(\mathbf{z})_i=w_c \cdot \frac{e^{\mathbf{z}_i}}{\sum_{j=1}^n e^{\mathbf{z}_j}}
\end{equation}
To ensure that the refined predictions remain in the interval $[0,1]$, the boost function is applied to the preactivations $\mathbf{z}=\sigma^{-1}(t)$ of the GNN. 
After the enhancement, the activation function $\sigma$ is applied to obtain the final predictions.

A \emph{clause weight} $w_c$ is assigned to each clause in the prior knowledge.
$\phi_{w_c}^{s}$ provides changes to the predictions that are proportional to $w_c$.
Hence, $w_c$ determines the magnitude of the change and reflects the strength of the clause. 
It can be set to zero, which makes KeGNN robust to wrong knowledge.
It can also be fixed to enforce the impact of a clause. 
% The changes introduced by all clause enhancers are aggregated, added to the initial preactivations $\mathbf{z}$ of the GNN and are passed to the activation function in order to obtain the final predictions.
% Here, $\operatorname*{LogSoftmax}$ is used as activation function $\sigma$. 
% \begin{equation}
%   \label{eq:aggregation_y}
%   \mathbf{y}' = \operatorname*{LogSoftmax} \left( \mathbf{z} + \sum_{c \in \mathcal{G}(\mathcal{K}, \mathcal{C})}\phi_c (\mathbf{z})\right)
% \end{equation}
Various groundings of a clause are stored in a matrix where columns represent predicates and rows represent constants. 
Once instantiated, a clause enhancer operates on several groundings of a clause.

For graph data, the groundings for unary predicates cannot be assumed to be independent since nodes are linked by an edge. 
% \sarah{not clear why they cannot be assumed to be independent}
% \lw{because by definition the links encode a relation between entities that in other words is a dependency. You can assume them to be independent but then you loose information (the links)}
While the groundings of unary predicates can be represented as a matrix that contains the objects as rows and grounded predicates as columns, the keys of the binary predicates are two-dimensional. Consequently, binary predicates are represented as a matrix $\mathbf{M}$ that has as many rows as pairs of objects.
To enhance clauses that contain both binary and unary predicates, all grounded predicates are joined into $\mathbf{M}$ on which the clause enhancer can operate. 
Hence, unary predicates $\mathrm{P(x)}$ are \emph{binarized} to $\mathrm{P^{\text{X}}(x,y)}$ and  $\mathrm{P^{\text{Y}}(x,y)}$. 
% Consequently, in order to calculate adjustments for binary clauses, a join operator combines groundings of binary predicates and binarized unary predicates to one matrix $\mathbf{M}$. 
The boost function (Eq. \ref{eq:softmax_goedel}) is then applied to $\mathbf{M}$ in order to calculate the adjustments $\delta \mathbf{M}$.
Given the adjustments $\delta \mathbf{M}$, a group-by layer collects the changes from the binarized predicates that refer to the same grounded literal. 
Finally, the changes by all clause enhancers are summed, added to $\mathbf{z}$ and put to the activation function $\sigma$ to obtain the updated predictions $\mathbf{y}'$:
\begin{equation}
  \label{eq:aggregation_y}
  \mathbf{y}' = \sigma\biggl( \mathbf{z} + \sum_{c \in \mathcal{G}(\mathcal{K}, \mathcal{C})}\phi_c (\mathbf{z})\biggr)
\end{equation}
\begin{example}[Enhancement of unary and binary predicates]
% A unary clause $c: \mathrm{AI(x)} \lor \mathrm{DB(x)}$ with $\mathrm{AI, DB} \in \mathcal{K}_U$ can have several predicates that refer to only one variable. 
%   As a consequence, the adjustments given in Equation \ref{eq:softmax_goedel} for the preactivation $\mathbf{z}_{AI(i)}$ of node $i$ are:
% \begin{equation*}
% \label{unary_enhancement}
% \phi_{s}^{w_{c}}(\mathbf{z}_{AI(i)}) = w_c \cdot
%            \frac{e^{\mathbf{z}_{AI(i)}}}{e^{\mathbf{z}_{AI(i)}} + e^{\mathbf{z}_{DB(i)}}} 
% \end{equation*}
% The change only depends on the preactivations $\mathbf{z}_{AI(i)}$ and $\mathbf{z}_{DB(i)}$
Let us consider the clause $c$: $\neg \mathrm{AI(x)} \lor \neg \mathrm{Cite(x, y)} \lor \mathrm{AI(y)} $ and the grounding $c[x/i,y/j]$. 
The unary predicate $\mathrm{AI(x)}$ is binarized to $\mathrm{AI^X(x,y)}$ and $\mathrm{AI^Y(x,y)}$ so that the preactivations $\mathbf{z}_{\mathrm{AI}(i)}$ and $\mathbf{z}_{\mathrm{AI}(j)}$ can be put in the same row but in separate columns. 
In this case, the adjustment for $\mathbf{z}_{\mathrm{AI}(i)}$ of the clause enhancer is given by the following term, where $e^{\mathbf{z}_{\mathrm{Cite}(i,j)}}$ represents the preactivation of the grounded binary predicate $\mathrm{Cite(x,y)}$:
\begin{equation*}
\label{binary_enhancement}
\phi_{s}^{w_{c}}(\mathbf{z}_{\mathrm{AI}(i)}) = w_c \cdot
           \frac{e^{\mathbf{z}_{\mathrm{AI}(i)}}}{e^{\mathbf{z}_{\mathrm{AI}(i)}} + e^{\mathbf{z}_{\mathrm{AI}(j)}} + e^{\mathbf{z}_{\mathrm{Cite}(i,j)}}} 
\end{equation*}
\end{example}
% to: 
% \begin{equation}
%   y_A=\sigma\left(z_A+\sum_{c \in \mathcal{K}_U[\mathcal{C}]} w_c \cdot \phi_{c, A}+\sum_{c \in \mathcal{K}_B[\mathcal{C}]} w_c \cdot \phi_{c, A}\right)
% \end{equation}
% \sarah{what are the notations $\mathcal{K}_U[\mathcal{C}]$ and $\phi_{c, A}$}
% Note that in this work, the preactivations of the binary predicates are set to a high constant value instead of being estimated by a neural component. 
% The reason for this is, that the graph structure is assumed to be known a priori. 


% \section{Related Work}
% \lw{Question: put this rather short into the introduction instead of having a separate section? It takes too much space and it's just a workshop}
% \cite{gnn_survey2}
% In this section we briefly summarize the state of the art on graph neural networks and neural symbolic approaches.

% \subsection{Graph Neural Networks}
% \cite{GNNBook2022}
% todo
% graph type and scale 
% edges: directed or undirected
% homogeneous or heterogeneous (node and edge types)
% static dynamic graphs; variation with time- dynamic
% graph scale - no discrete specification but large graphs  might require special treatment (adjacency matrix too large)

% loss function: node level, edge level (link prediction), graph level 

% inductive vs. transductive 

% propagation modules: 
% % Short introduction how they work etc. 
% % GNN for different domains (temporal graphs, knowledge graphs, heterogeneous graph, graph attention networks), scalable graph neural networks 

% \subsection{Neural Symbolic Integration}
% % see also related work in KENN for relational domains 
% The spectrum of neural symbolic approaches can be divided into three main categories according to \cite{ltn}. 
% First, there are neural architectures for logical reasoning that employ neural networks for inference on logical theories \cite{tensorlog} \cite{neural_markov_logic_networks} \cite{probabilistic_nn_for_reasoning}. 
% Second, logical language is used to specify neural network architectures \cite{lifted_relational_nn}, \cite{fast_relational_learning}, \cite{Garcez2012NeuralsymbolicLS}. 
% A third type of neural-symbolic architectures integrates inductive learning and deductive reasoning. 
% Approaches belonging to this category can further be distinguished into three categories: 
% \begin{itemize}
%   \item Approaches that embed neural networks as trainable components into a logic program like DeepProbLog \cite{deepproblog} or abductive learning \cite{abductiveLearning}.
%   \item Models that encode knowledge as constraints in the loss function that is used for the network training, such as LTN \cite{ltn} or \cite{DL2TA}, \cite{Harnessing_dnn_logical_rules}, \cite{lyrics}
%   \item Approaches that add supplementary layers to a neural network that encode logic and revise predictions with respect to some prior knowledge eg. RNM \cite{rnm}, KENN \cite{kenn}, Deep Logic Models \cite{deep_logic_models}
% \end{itemize}
% Our approach belongs to the third category as it extends KENN. 

% KENN: relaxed version of G\"odel t-conorm softmax, applied in logic to compute deltas. 

\section{Experimental Evaluation}
To evaluate the performance of KeGNN, we apply it to the datasets Citeseer, Cora, PubMed and Flickr that are common benchmarks for node classification.
In the following, KeGNN is called KeGCN and KeGAT when instantiated to a GCN or a GAT, respectively. 
As additional baseline, we consider KeMLP, where knowledge enhancement layers are nested on an MLP as proposed in \cite{relational_kenn}.
Further, the standalone neural models MLP, GCN and GAT are used as baselines. 
While Citeseer, Cora and PubMed are citation graphs that encode citations between scientific papers (as in Example \ref{ex:prior_knowledge_cit}), Flickr \cite{graph_saint} contains images and shared properties between them. 
All datasets can be modelled as homogeneous, labelled, attributed, unweighted graphs as defined in Section \ref{sec:graphstructured_data}.
The set of prior logic for the knowledge enhancement layers is given explicitly. 
If two nodes are linked and if the first node belongs to a specific class, the second node will also belong to that class.
In the context of citation graphs, this implies that two documents that cite each other are referring to the same topic, while for Flickr, linked images share the same properties. 
According to this pattern, for all datasets a clause is instantiated for each class $\mathrm{C_i}$:
$\forall xy: \neg \mathrm{C_i(x)} \lor$ $\neg \mathrm{Link(x,y)} \lor \mathrm{C_i(y)}$. 
More details on the experiments are given in appendix \ref{sec:experiment_details}. 
The implementation of KeGNN and the experiments are publicly available\footnote{https://gitlab.inria.fr/tyrex/kegnn}.


\subsection{Results}
\begin{table}[]
  \footnotesize
  \centering
  \begin{tabular}{|l|ll|ll|ll|} 
  \hline
   & \textbf{MLP} & \textbf{KeMLP} & \textbf{GCN} & \textbf{KeGCN} & \textbf{GAT} & \textbf{KeGAT} \\ 
  \hline
  \textbf{Cora} & \begin{tabular}[c]{@{}l@{}}0.7098 \\(0.0080)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.8072 \\(0.0193)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.8538 \\(0.0057)\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{0.8587 }\\(0.0057)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.8517 \\(0.0068)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.8498 \\(0.0066)\end{tabular} \\ 
  \hline
  \textbf{CiteSeer} & \begin{tabular}[c]{@{}l@{}}0.7278 \\(0.0081)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.7529 \\(0.0067)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.748 \\(0.0102)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.7506 \\(0.0096)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.7718 \\(0.0072)\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{0.7734 }\\(0.0073)\end{tabular} \\ 
  \hline
  \textbf{PubMed} & \begin{tabular}[c]{@{}l@{}}0.8844 \\(0.0057)\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{0.8931} \\(0.0048)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.8855 \\(0.0062)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.8840 \\(0.0087)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.8769 \\(0.0040)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.8686 \\(0.0081)\end{tabular} \\ 
  \hline
  \textbf{Flickr} & \begin{tabular}[c]{@{}l@{}}0.4656 \\(0.0018)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.4659 \\(0.0012)\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{0.5007 }\\(0.0063)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.4974 \\(0.0180)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.4970\\(0.0124)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.4920\\(0.0189)\end{tabular} \\
  \hline
  \end{tabular}
  \caption{\begin{footnotesize}Average accuracy on the test set over 50 runs (10 for Flickr). The standard deviations are reported in brackets.\end{footnotesize}}
  \label{tab:results}
  \end{table}
To compare the performance of all models, we examine the average test accuracy over 50 runs (10 for Flickr) for the knowledge enhanced models KeMLP, KeGCN, KeGAT and the standalone base models MLP, GCN, GAT on the named datasets. 
The results are given in Tab. \ref{tab:results} and visualized in Fig. \ref{fig:results} (see appendix \ref{sec:results_appendix}).
For Cora and Citeseer, KeMLP leads to a significant improvement over MLP (p-value of one-sided t-test $\ll 0.05$). 
In contrast, no significant advantage of KeGCN or KeGAT in comparison to the standalone base model is observed. 
Nevertheless, all GNN-based models are significantly superior to KeMLP for Cora. 
This includes not only KeGCN and KeGAT, but also the GNN baselines.
For Citeseer, KeGAT and GAT both outperform KeMLP.
In the case of PubMed, only a significant improvement of KeMLP over MLP can be observed, while the GNN-based models and their enhanced versions do not provide any positive effect. 
For Flickr, no significant improvement between the base model and the respective knowledge enhanced model can be observed. 
Nevertheless, all GNN-based models outperform KeMLP, reporting significantly higher mean test accuracies for KeGAT, GAT, GCN and KeGCN. 


% It turns out that the performance gap between MLP and KeMLP is larger than for KeGNN in comparison to the standalone GNN.
% This can be attributed to the fact that MLP is a simple model that misses information on the graph structure and thus benefits from the prior knowledge providing relation information in form of binary predicates.
% In contrary, both GNN and KeGNN can process graph structure, so that the information introduced by the knowledge enhancer has some redundancy. 
% Furthermore, if the GNN produces correct predictions that satisfy the knowledge, fewer adjustments by the knowledge enhancement layers are expected.
 
It turns out that the performance gap between MLP and KeMLP is larger than for KeGNN in comparison to the standalone GNN.
This can be attributed to the fact that MLP is a simple model that misses information on the graph structure and thus benefits from the prior knowledge provided by the knowledge enhancer.
% MLP is a simple model [which lacks information on the graph structure?] benefits from the prior knowledge introduced by the KE layers \\
On the contrary, standalone GNNs can process graph structure by using message passing techniques to transmit learned node representations between neighbors. 
The prior knowledge introduced in the knowledge enhancer is simple: it encodes that two neighbors are likely to be of the same class. 
An explanation of the small performance gap is that the GNNs might have learned to propagate the relevant information across neighbors. 
In other words, the introduced knowledge is redundant. 
% One good persective for future work is then to investigate the introduction of more complex knowledge through the knowledge enhancement layers.

In addition, it is found that a standalone GNN can yield equal or better performance compared to a KeMLP in some cases.
As GNNs are more efficient in terms of runtimes in compared to the more complex knowledge enhanced models, equal performance could be achieved faster with a pure neural model (see runtime comparison in Tab. \ref{tab:runtimes} in \ref{sec:results_appendix}). 

\subsubsection{Clause Weight Analysis}
\begin{figure}[]
  \centering
  \begin{subfigure}{5cm}
  \centering\includegraphics[width=5cm]{CiteSeer_KeMLP_clauseweights.pdf}
  %\caption{Cora}
  \end{subfigure}%
  \begin{subfigure}{5cm}
  \centering\includegraphics[width=5cm]{CiteSeer_KeGCN_clauseweights.pdf}
  %\caption{Citeseer}
  \end{subfigure}\vspace{10pt}
   
  \begin{subfigure}{5cm}
  \centering\includegraphics[width=5cm]{CiteSeer_KeMLP_cw_evolution.pdf}
  %\caption{PubMed}
  \end{subfigure}%
  \begin{subfigure}{5cm}
  \centering\includegraphics[width=5cm]{CiteSeer_KeGCN_cw_evolution.pdf}
  %\caption{Flickr}
  \end{subfigure}
  \caption{\begin{footnotesize}Comparison of KeMLP and KeGCN. Above: Clause weights of last epoch during training vs. clause compliance. Below: Clause weight evolution during training.\end{footnotesize} \label{fig:clause_weights}} 
  \end{figure}
The clause weights obtained during training provide insights on the extent of adjustment applied by a clause. As a reference, the \emph{clause compliance} (see Appendix \ref{sec:clause_weight_evaluation_appendix}) \cite{kenn} measures the satisfaction of a clause in the training set before optimization. 
Fig. \ref{fig:clause_weights} displays the learned clause weights for KeGCN and KeMLP versus the clause compliance (above) and the clause weight evolution during training (below).
For KeMLP, a positive correlation between the learned clause weights and the clause compliance on the training set is observed. 
This indicates that satisfied clauses in the training set have a higher impact on the adjustments. 
In addition, the clause weights corresponding to clauses with low compliance values make smaller modifications to the initial predictions. 
Consequently, clauses that are rarely satisfied learn lower clause weights during the training process. 
In the case of KeGCN, the clause weights are predominantly set to low values close to zero. 
This is in accordance with the absence of a significant performance gap between GCN and KeGCN. 
Since the GCN in itself already leads to valid classifications, smaller adjustments are required by the clause enhancers. 
% Furthermore, there is no noticeable correlation between compliance and learned weights for KeGCN. 

\section{Limitations}
In this work, we focused on homogeneous graphs.
However, in reality graphs are often heterogeneous with multiple node and edge types \cite{heterogeneous_gnn}.
Adaptations are necessary on both the neural and the symbolic side to apply KeGNN to heterogeneous graphs. 
The restriction to homogeneous graphs also limits the scope for formulating complex prior knowledge.
Eventually, the datasets used in this work and the set of prior knowledge are too simple for KeGNN to exploit its potential and lead to significant improvements. 
The extension of KeGNN to more generic data structures such as incomplete and heterogeneous knowledge graphs in conjunction with more complex prior knowledge is an ongoing work.

A further limitation of KeGNN is scalability. 
With increasing depth of stacked knowledge enhancement layers, the affected node neighborhood grows exponentially, which can lead to significant memory overhead.
This problem is referred as {neighborhood explosion} \cite{large_scale_graph_training} and is particularly problematic in the context of training on memory-constrained GPUs. 
This affects both the GNN and the knowledge enhancement layers that encode binary knowledge.
Methods from scalable graph learning \cite{gnn_autoscale} \cite{graph_saint} \cite{graph_sage} represent potential solutions for the neighborhood explosion problem in KeGNN. 

Furthermore, limitations appear in the context of link prediction with KeGNN.
In link prediction tasks, the knowledge enhancement layers must obtain truth values from the neural component not only for unary predicates but also for binary predicates.
At present, KeGNN can handle clauses containing binary predicates, but their truth values are initialized with artificial predictions, where a high value encodes the presence of an edge.
This also limits KeGNN to datasets where the graph structure is assumed to be complete and true.
For link prediction, a neural component is required that predicts truth values for binary predicates. 
% With this property, the model is currently not suitable for link prediction tasks.
% In theory, KeGNN can be extended to link prediction by modifying the loss function and adding a neural component to obtain binary preactivations. 

\section{Conclusion and Future Work}
% Yago \cite{yago}
% % Wikidata \url{https://www.wikidata.org}
% \cite{wikidata}
% Wikialumni \cite{nodeclassification_linkprediction}
% \nl{You can say : such as noisy or erroneous semantic data that are shaped close to semantically rich datasets such as Yago and Wikidata and equiped with prior knowledge extracted from ontologies (cite OWL).}
% \nl{May be you shols turn the conclusion to be more positive, you should say that this is a first step toward a disciplined neuro symb, from data to graphs and with prior knowledge applied to graphs this is a mromising path to unlock full potetial of neuro symb.}
In this work, we introduced KeGNN, a neuro-symbolic model that integrates GNNs with symbolic knowledge enhancement layers into an end-to-end differentiable model. 
This allows the use of prior knowledge to improve node classification by exploiting the expressive representations of a GNN.
Experimental studies show that the inclusion of prior knowledge has the potential to improve simple neural models (as observed in the case of MLP).
However, the knowledge enhancement of GNNs is harder to achieve on the underlying and limited benchmarks.
Nevertheless, this work is a step towards a holistic neuro-symbolic method on incomplete and noisy semantic data, such as knowledge graphs. 
Semantically richer datasets in conjunction with more expressive prior knowledge should be investigated to fully explore the promising benefits of KeGNN. 
A future work in this direction is the application of KeGNN to the completion of noisy, feature-enriched knowledge graphs such as Wikialumni \cite{nodeclassification_linkprediction} based on YAGO4 \cite{yago} and Wikidata \cite{wikidata} with more complex prior knowledge in form of ontologies \cite{owl}. 

%%
%% The acknowledgments section is defined using the "acknowledgments" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acknowledgments}
This work has been partially supported by MIAI @ Grenoble Alpes (ANR-19-P3IA-0003).
\end{acknowledgments}

%%
%% Define the bibliography file to be used
\bibliography{sample-ceur}

%%
%% If your work has an appendix, this is the place to put it.
\newpage
\appendix
\section{Experiment Details}
\label{sec:experiment_details}
\subsection{Implementation}
The implementation of KeGNN and the described experiments are publicly available on GitLab\footnote{\url{https://gitlab.inria.fr/tyrex/kegnn}}. 
The code is based on PyTorch \cite{pytorch} and the graph learning library PyTorch Geometric \cite{pytorch_geometric}. 
The Weights \& Biases tracking tool \cite{wandb} is used to monitor the experiments.
For the experiments, we use a machine running an Ubuntu 20.4 equipped with an Intel(R) Xeon(R) Silver 4114 CPU 2.20GHz processor, 192G of RAM and
one GPU Nvidia Quadro P5000.

\subsection{Datasets}
\label{sec:dataset_details}
Tab. \ref{tab:dataset} gives an overview of the named datasets in this work. 
The datasets are publicly available on the dataset collection\footnote{\url{https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html}} of PyTorch Geometric \cite{pytorch_geometric}.
For the split into train, valid and test set, we take the predefined splits in \cite{splits_planetoid} for the citation graphs and in \cite{graph_saint} for Flickr. 
Word2Vec vectors \cite{word2vec} are used as node features for the citation graphs and image data for Flickr.
Fig. \ref{fig:graph_structure} visualizes the graph structure of the underlying datasets in this work as a homogeneous, attributed and labelled graph (on the example of Citeseer). 
\begin{table}[h]
  \footnotesize
  \begin{tabular}{|l|l|l|l|l|l|l|} 
  \hline
  \textbf{ Name }  & \textbf{ \#nodes } & \textbf{ \#edges } & \textbf{ \#features } & \textbf{ \#Classes } & \textbf{train/valid/test split}\\ 
  \hline
  Citeseer  & 3,327 & 9,104 & 3,703 & 6 & 1817/500/1000\\
  Cora & 2,708 & 10,556 & 1,433 & 7 & 1208/500/1000\\
  PubMed  & 19,717 & 88,648 & 500 & 3 & 18217/500/1000\\
  Flickr & 89,250 & 899,756 & 500 & 7 & 44624/22312/22312\\
  \hline
  \end{tabular}
  \centering
  \caption{\footnotesize Overview of the datasets Citeseer, Cora, PubMed and Flickr}
  \label{tab:dataset}
  \end{table}
  \begin{figure}[h]
    \centering
    \centering\includegraphics[width=12cm]{Citation_Graph.png}
    \caption{\begin{footnotesize} Visualization of the structure of the datasets used in this work with a citation graph as example. \end{footnotesize}
    \label{fig:graph_structure}}
    \end{figure}%

\subsection{Results}
\label{sec:results_appendix}
The average test accuracies obtained for the node classification experiments on Cora, Citeseer, PubMed and Flickr over all tested models are visualized in Fig. \ref{fig:results}.
The average runtimes per epoch on the Citeseer dataset are compared for all models in Tab \ref{tab:runtimes}. 
For runtime comparison, the runtimes were calculated for models with three hidden layers and three knowledge enhancement layers in full-batch training.
It can be noted that the knowledge enhancement layers lead to increased epoch times since the model complexity is higher. 
\begin{figure}[h]
  \centering
  \begin{subfigure}{6cm}
  \centering\includegraphics[width=5cm]{results_cora.pdf}
  %\caption{Cora}
  \end{subfigure}%
  \begin{subfigure}{6cm}
  \centering\includegraphics[width=5cm]{results_citeseer.pdf}
  %\caption{Citeseer}
  \end{subfigure}\vspace{10pt}
   
  \begin{subfigure}{6cm}
  \centering\includegraphics[width=5cm]{results_pubmed.pdf}
  %\caption{PubMed}
  \end{subfigure}%
  \begin{subfigure}{6cm}
  \centering\includegraphics[width=5cm]{results_flickr.pdf}
  %\caption{Flickr}
  \end{subfigure}
  \caption{\begin{footnotesize}Average test accuracies over 50 runs for Cora, CiteSeer and PubMed and 10 runs on Flickr. Error bars denote standard deviation.\end{footnotesize}}
  \label{fig:results}
  \end{figure}
\begin{table}[h]
  \footnotesize
  \centering
  \begin{tabular}{|>{\hspace{0pt}}m{0.1\linewidth}|>{\hspace{0pt}}m{0.2\linewidth}|} 
  \hline
  Model & Avg Epoch Time \\ 
  \hline
  MLP & 0.02684 \\
  GCN & 0.03109 \\
  GAT & 0.06228 \\
  KeMLP & 0.04304 \\
  KeGCN & 0.03747 \\
  KeGAT & 0.08384 \\
  \hline
  \end{tabular}
  \caption{\begin{footnotesize}Comparison of the average epoch times for all models on the Citeseer dataset.\end{footnotesize}}
  \label{tab:runtimes}
  \end{table}
  
  % Figure \ref{fig:clause_weights} displays the learned clause weights for KeGCN and KeMLP with the clause compliance (above) and the clause weight evolution during training (below).
  % \begin{figure}[h]
  %   \centering
  %   \begin{subfigure}{6.5cm}
  %   \centering\includegraphics[width=6.5cm]{figures/CiteSeer_KeMLP_clauseweights.pdf}
  %   %\caption{Cora}
  %   \end{subfigure}%
  %   \begin{subfigure}{6.5cm}
  %   \centering\includegraphics[width=6.5cm]{figures/CiteSeer_KeGCN_clauseweights.pdf}
  %   %\caption{Citeseer}
  %   \end{subfigure}\vspace{10pt}
     
  %   \begin{subfigure}{6.5cm}
  %   \centering\includegraphics[width=6.5cm]{figures/CiteSeer_KeMLP_cw_evolution.pdf}
  %   %\caption{PubMed}
  %   \end{subfigure}%
  %   \begin{subfigure}{6.5cm}
  %   \centering\includegraphics[width=6.5cm]{figures/CiteSeer_KeGCN_cw_evolution.pdf}
  %   %\caption{Flickr}
  %   \end{subfigure}
  %   \caption{Comparison of KeMLP and KeGCN. Above: Clause weights of last epoch during training vs. clause compliance. Below: Clause weight evolution during training. \label{fig:clause_weights}} 
  %   \end{figure}

\subsection{Hyperparameter Tuning}
\label{sec:hyperparameter_tuning}
KeGNN contains a set of hyperparameters that affect the optimization and the model architecture.
Batch normalization \cite{batch_norm} is applied after each hidden layer of the GNN.
Adam \cite{adam} is used as optimizer for all models. 
Concerning the hyperparameters specific to the knowledge enhancement layers, the initialization of the preactivations of the binary predicates (which are assumed to be known) is taken as a hyperparameter. 
They are set to a high positive value for edges that are known to exist and correspond to the grounding of the binary predicate.
Furthermore, different initializations of clause weights and constraints on them are tested. 
Moreover, the number of stacked knowledge enhancement layers is a hyperparameter.
We further allow the model to randomly neglect a proportion of edges by setting an edges drop rate parameter.
Further, we test whether the normalization of the edges with the diagonal matrix 
$\tilde {\mathbf{D}} = \sum_{j} \tilde{\mathbf{A}}_{i,j}$ 
(with $\tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}$) is helpful. 

To find a suitable set hyperparameters for each dataset and model, we perform a random search with up to 800 runs and 48h time limit and choose the parameter combination which leads to the highest accuracy on the validation set.
The hyperparameter tuning is executed in Weights and Biases \cite{wandb}.
The following hyperparameter values are tested:
\begin{itemize}
  \item Adam optimizer parameters: $\beta_1$: 0.9, $\beta_2$: 0.99, $\epsilon$: 1e-07
  \item Attention heads: $\{1, 2, 3, 4, 6, 8, 10\}$
  \item Batch size: $\{128, 512, 1024, 2048, \text{full batch}\}$
  \item Binary preactivation value: $\{0.5, 1.0, 10.0, 100.0, 500.0\}$
  \item Clause weights initialization: $\{0.001, 0.1, 0.25, 0.5, \text{random uniform distribution on [0,1)}\}$
  \item Dropout rate: $0.5$
  \item Edges drop rate: random uniform distribution $[0.0, 0.9]$
  \item Edge normalization: $\{\text{true, false}\}$
  \item Early stopping: $\delta_{min}: 0.001$, patience: \{1, 10, 100\}
  \item Hidden layer dimension: \{32, 64, 128, 256\}
  \item Learning rate: random uniform distribution $[0.0001, 0.1]$
  \item Clause weight clipping: $w_{min}: 0.0$, $w_{max}$: random uniform distribution: $[0.8, 500.0]$
  \item Number of knowledge enhancement layers: $\{1, 2, 3, 4, 5, 6\}$
  \item Number of hidden layers: $\{2, 3, 4, 5, 6\}$
\end{itemize}
The obtained parameter combinations for the models KeMLP, KeGCN and KeGAT for Cora, Citeseer, PubMed and Flickr are displayed in Tab. \ref{tab:cora_citeseer_hp} and Tab. \ref{tab:pubmed_flickr_hp}.
The reference models MLP, GCN and GAT are trained with the same parameter set as the respective enhanced models. 
\begin{table}[h]
  \centering
  \footnotesize
  \begin{tabular}{|l|lll|lll|} 
  \hline
   & \multicolumn{3}{c|}{\textbf{PubMed}} & \multicolumn{3}{c|}{\textbf{Flickr}} \\ 
  \hline
  \textbf{Parameter} & \textbf{KeMLP} & \textbf{KeGCN} & \textbf{KeGAT} & \textbf{KeMLP} & \textbf{KeGCN} & \textbf{KeGAT} \\ 
  \hline
  adam beta 1~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~~ & 0.9 & 0.9 & 0.9 & 0.9 & 0.9 & 0.9 \\
  ada beta 2 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 \\
  adam epsilon & 1e-07 & 1e-07 & 1e-07 & 1e-07 & 1e-07 & 1e-07 \\
  attention heads & - & - & 8 & - & - & 8 \\
  batch size~ & 1024 & full batch & 1024 & 128 & 1024 & 2048 \\
  binary preactivation & 10.0 & 1.0 & 10.0 & 10.0 & 500.0 & 500.0 \\
  clause weight initialization & 0.001 & random & 0.5 & 0.001 & 0.001 & 0.1 \\
  dropout rate & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\
  edges drop rate & 0.22 & 0.66 & 0.07 & 0.2 & 0.24 & 0.12 \\
  epochs & 200 & 200 & 200 & 200 & 200 & 200 \\
  early stopping enabled & true & true & true & true & true & true \\
  early stopping min delta & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 \\
  early stopping patience & 100 & 10 & 10 & 10 & 10 & 100 \\
  hidden channels & 256 & 256 & 256 & 32 & 128 & 64 \\
  learning rate~ & 0.057 & 0.043 & 0.016 & 0.001 & 0.016 & 0.0039 \\
  max clause weight~ & 350.0 & 322.0 & 118.0 & 55.0 & 135 & 113.0 \\
  min clause weight & 0.0 & 0.0 & 0.0 & 0.0 & 0 & 0.0 \\
  normalize edges & false & false & true & true & true & false \\
  KE layers & 2 & 1 & 5 & 1 & 4 & 1 \\
  hidden layers & 4 & 2 & 2 & 2 & 4 & 3 \\
  runs & 50 & 50 & 50 & 10 & 10 & 10 \\
  seed & 1234 & 1234 & 1234 & 1234 & 1234 & 1234 \\
  \hline
  \end{tabular}
  \caption{\begin{footnotesize}Hyperparameters and experiment configuration for PubMed and Flickr\end{footnotesize}}
  \label{tab:pubmed_flickr_hp}
  \end{table}
    
  \begin{table}[h]
    \footnotesize
    \centering
    \begin{tabular}{|l|lll|lll|} 
    \hline
     & \multicolumn{3}{c|}{\textbf{Cora~ }} & \multicolumn{3}{c|}{\textbf{CiteSeer }} \\ 
    \hline
    \textbf{Parameter} & \textbf{KeMLP} & \textbf{KeGCN} & \textbf{KeGAT} & \textbf{KeMLP} & \textbf{KeGCN} & \textbf{KeGAT} \\ 
    \hline
    adam beta 1~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~~ & 0.9 & 0.9 & 0.9 & 0.9 & 0.9 & 0.9 \\
    ada beta 2 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 \\
    adam epsilon & 1e-07 & 1e-07 & 1e-07 & 1e-07 & 1e-07 & 1e-07 \\
    attention heads & - & - & 1 & - & - & 3 \\
    batch size~ & 512 & 512 & full batch & 128 & full batch & 1024 \\
    binary preactivation & 10.0 & 500.0 & 1.0 & 10.0 & 0.5 & 0.5 \\
    clause weight initialization & 0.5 & random & 0.5 & 0.5 & 0.25 & 0.1 \\
    dropout rate & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\
    edges drop rate & 0.47 & 0.17 & 0.27 & 0.01 & 0.35 & 0.88 \\
    epochs & 200 & 200 & 200 & 200 & 200 & 200 \\
    early stopping enabled & true & true & true & true & true & true \\
    early stopping min delta & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 \\
    early stopping patience & 1 & 1 & 10 & 10 & 10 & 10 \\
    hidden channels & 32 & 256 & 64 & 256 & 128 & 32 \\
    learning rate~ & 0.026 & 0.032 & 0.033 & 0.028 & 0.037 & 0.006 \\
    max clause weight~ & 104.0 & 254.0 & 250.0 & 34.0 & 243.0 & 110.0 \\
    min clause weight & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
    normalize edges & true & false & true & true & false & true \\
    KE layers & 4 & 2 & 1 & 1 & 3 & 2 \\
    Hidden layers & 2 & 2 & 2 & 2 & 5 & 2 \\
    runs & 50 & 50 & 50 & 50 & 50 & 50 \\
    seed & 1234 & 1234 & 1234 & 1234 & 1234 & 1234 \\
    \hline
    \end{tabular}
    \caption{\begin{footnotesize}Hyperparameter and experiment configuration for Citeseer and Cora\end{footnotesize}}
    \label{tab:cora_citeseer_hp}
    \end{table}


  \section{Clause Weight Evaluation}
  \label{sec:clause_weight_evaluation_appendix}
  The \emph{clause compliance} \cite{kenn} indicates the level of satisfaction of a clause in the data in this experimental setting. 
  Given a clause $c$, a class $k$, the set of training nodes $\mathbf{V}_{\mathrm{train}}$, the set of nodes of the class $k$: $\mathbf{V}_k = \{v | v \in \mathbf{V}_{\mathrm{train}} \land \mathrm{Cls}(v) == k \}$, and the neighborhood $\mathcal{N}(v)$ of $v$, the clause compliance on graph $\mathbf{G}$ is defined as follows:     
  \begin{equation}
    \label{eq:compliance}
    \operatorname{Compliance}(\mathbf{G}, c) = \frac{\sum_{v \in \mathbf{V}_k} \sum_{u \in \mathcal{N}(v)} \mathbf{1} (u \in \mathbf{V}_k)}{\sum_{v \in \mathbf{V}_k} | \mathcal{N}(v)|}
  \end{equation}
  In other words, the clause compliance counts how often among nodes of a class the neighboring nodes are of the same class. 
  \cite{kenn}

\end{document}

%%
%% End of file
