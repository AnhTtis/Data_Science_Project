\section{Probabilistic RISk Measure derivAtion}
\label{sec:method}

In this section, we propose the \ac{ourmethod} method which is a method for deriving a measure that quantifies the risk of a certain event, such as a crash, in a particular situation in which a vehicle - hereafter, the \textit{ego vehicle} - is in and that is applicable for real-time use.
The \ac{ourmethod} method \cstart is schematically shown in \cref{fig:outline} and \cend consists of four steps:
\cstart
\begin{enumerate}
	\item The parameterization of the ``initial situation'' and the possible ``future situations'' (\cref{sec:parametrization});
	\item Based on the initial situation, the estimation of the probability (density) for the possible future situations (\cref{sec:estimate future});
	\item The estimation of the probability of the specified event based on the initial and the future situations (\cref{sec:estimate collision}); and
	\item Local regression in order to speed up the calculations and to make it possible to use the \ac{ssm} in real time (\cref{sec:final metric calculation}). 
\end{enumerate}\cend

\begin{figure}
	\centering
	\input{figs/outline.tikz}
	\caption{\cstart Schematic overview of the \ac{ourmethod} method and the organization of \cref{sec:method}.
		The mathematical symbols are further explained in \cref{sec:parametrization,sec:estimate future,sec:estimate collision,sec:final metric calculation}.\cend}
	\label{fig:outline}
\end{figure}

In this article, the following notation is used. 
To denote a probability function, $\probability{\cdot}$ is used. 
\Iac{pdf} is denoted by $\density{\cdot}$. 
The probability of $\dummyvara$ given $\dummyvarb$ is denoted by $\probabilitycond{\dummyvara}{\dummyvarb}$.
Similarly, a conditional \ac{pdf} is denoted by $\densitycond{\cdot}{\cdot}$. 
To denote the estimation of any of the aforementioned functions, a circumflex is used, e.g, $\probabilityest{\dummyvara}$ denotes the estimated probability of $\dummyvara$.



\subsection{Parameterize initial and future situations}
\label{sec:parametrization}

The first step is to parameterize the initial situation the ego vehicle is in. 
In other words, the initial situation needs to be described using $\situationinitialdim$ numbers that are stacked into one vector $\situationinitial \in \situationinitialspace \subseteq \realnumbers^{\situationinitialdim}$. 
This vector contains relevant aspects for determining the risk.
As an example, $\situationinitial$ could contain the speed of the ego vehicle and the distance toward its preceding vehicle. 
In \cref{sec:case study}, we will consider more examples.

Next to describing the initial situation, the future situation is described using $\situationfuturedim$ numbers stacked into one vector $\situationfuture \in \situationfuturespace \subseteq \realnumbers^{\situationfuturedim}$. 
Together with $\situationinitial$, $\situationfuture$ contains enough information to describe how the relevant future, e.g., the next 5 seconds, around the ego vehicle develops over time. 
As an example, $\situationfuture$ could contain the speed for the next 5 seconds of the leading vehicle (if any) that is in front of the ego vehicle.
In \cref{sec:case study}, we will consider more examples.

Let $\collision$ denote an event, e.g., a crash or a near miss, such that the probability of this event is $\probability{\collision}$.
The goal of our \ac{ssm} is to estimate the probability of the event $\collision$ given a particular situation $\situationinitial$, i.e., $\probabilitycond{\collision}{\situationinitial}$.
We do this by considering all future situations, $\situationfuturespace$, and calculating the probability of the event $\collision$ given each possible value of $\situationfuture$. 
Using integration, we obtain $\probabilitycond{\collision}{\situationinitial}$:
\begin{equation}
	\label{eq:probability collision expectation}
	\probabilitycond{\collision}{\situationinitial} 
	= \int_{\situationfuturespace} 
	\probabilitycond{\collision}{\situationinitial, \situationfuture} 
	\densitycond{\situationfuture}{\situationinitial} 
	\ud \situationfuture.
\end{equation}
%In \cref{sec:estimate future}, we propose a method to estimate $\densitycond{\situationfuture}{\situationinitial}$ and in \cref{sec:estimate collision}, we propose a method to estimate $\probabilitycond{\collision}{\situationinitial, \situationfuture}$.



\subsection{Estimate $\densitycond{\situationfuture}{\situationinitial}$}
\label{sec:estimate future}

In this section, we propose a method to estimate $\densitycond{\situationfuture}{\situationinitial}$, i.e., the \ac{pdf} of $\situationfuture$ given $\situationinitial$.
Using the product rule for probability, we can write:
\begin{equation}
	\densitycond{\situationfuture}{\situationinitial} 
	= \frac{\density{\situationinitial, \situationfuture}}{\density{\situationinitial}}
	= \frac{\density{\situationinitial, \situationfuture}}{
		\int_{\situationfuturespace} \density{\situationinitial, \situationfuture} \ud\situationfuture
	}.
\end{equation}
Thus, it suffices to estimate $\density{\situationinitial, \situationfuture}$. 

Our proposal is to estimate $\density{\situationinitial, \situationfuture}$ in a data-driven manner. 
A data-driven approach brings several benefits.
First, the estimate automatically adapts to local driving styles and behaviors, which can change from region to region, provided that the data are obtained from the same local traffic.
Second, assumptions such as a constant speed of other vehicles, are not needed.
For our data-driven approach, let us assume that we have obtained $\situationnumberof$ situations from data.
For the $\situationindex$-th situation, we denote the initial situation and the future situation by $\situationinitialinstance{\situationindex}\in\situationinitialspace$ and $\situationfutureinstance{\situationindex}\in\situationfuturespace$, respectively. 
The remainder of this subsection describes how we estimate $\density{\situationinitial, \situationfuture}$ using  $\left\{(\situationinitialinstance{\situationindex},\situationfutureinstance{\situationindex})\right\}_{\situationindex=1}^{\situationnumberof}$.



\subsubsection{Kernel density estimation}
\label{sec:one kde}

We first explain how to estimate $\density{\situationinitial, \situationfuture}$ if we assume that all $\situationinitialdim+\situationfuturedim$ parameters depend on each other. 
If the shape of the \ac{pdf} is known, a particular functional form can be fitted to the data, e.g., by estimating the parameters of a distribution by maximizing the likelihood.
For example, if it is known that the data $\left\{(\situationinitialinstance{\situationindex},\situationfutureinstance{\situationindex})\right\}_{\situationindex=1}^{\situationnumberof}$ come from a multivariate normal distribution, it suffices to estimate the mean and the covariance.
If, however, the shape is unknown, fitting a particular parametric distribution may lead to very inaccurate results \autocite{chen2017tutorial}.
Furthermore, the shape of the estimated \ac{pdf} might change as more data are acquired. 
Assuming a functional form of the \ac{pdf} and fitting the parameters of the \ac{pdf} to the data may therefore lead to inaccurate fits unless extensive manual tuning is applied.

In the remainder of this work, we assume that the shape of the \ac{pdf} $\density{\situationinitial, \situationfuture}$ is unknown a priori.
Therefore, we employ a non-parametric approach using \ac{kde} \autocite{rosenblatt1956remarks, parzen1962estimation} because the shape of the \ac{pdf} is then automatically computed and \ac{kde} is highly flexible regarding the shape of the \ac{pdf}. 
\cstart Note, however, that the \ac{ourmethod} method can also work with other non-parametric methods for estimating \iac{pdf} (cf.\ \autocite{durkan2019neural, peerlings2022multivariate}). \cend
Using \ac{kde}, the estimated \ac{pdf} becomes:
\begin{equation}
	\label{eq:kde estimate}
	\densityest{\situationinitial,\situationfuture}
	= \frac{1}{\situationnumberof} \sum_{\situationindex=1}^{\situationnumberof}
	\kernelfuncnormalized{\bandwidthmatrix}{
		\begin{bmatrix}
			\situationinitial \\
			\situationfuture
		\end{bmatrix} -
		\begin{bmatrix}
			\situationinitialinstance{\situationindex} \\
			\situationfutureinstance{\situationindex}
		\end{bmatrix}
	},
\end{equation}
where $\kernelfuncnormalized{\bandwidthmatrix}{\cdot}$ is an appropriate kernel function with an $(\situationinitialdim+\situationfuturedim)$-by-$(\situationinitialdim+\situationfuturedim)$ symmetric positive definite \emph{bandwidth} or \emph{smoothing} matrix $\bandwidthmatrix$. 
The choice of the kernel $\kernelfuncnormalized{\bandwidthmatrix}{\cdot}$ is not as important as the choice of the bandwidth matrix $\bandwidthmatrix$ \autocite{turlach1993bandwidthselection}.
We use the often-used Gaussian kernel \autocite{duong2007ks}:
\begin{equation}
	\label{eq:kernel initial future}
	\kernelfuncnormalized{\bandwidthmatrix}{\dummyvarkernel}
	= \frac{1}{\left( 2 \pi \right)^{\left( \situationinitialdim + \situationfuturedim \right) / 2} 
	\left|\bandwidthmatrix\right|^{1/2} }
	\e{ -\frac{1}{2} \dummyvarkernel\transpose \bandwidthmatrix^{-1} \dummyvarkernel }.
\end{equation}

The bandwidth matrix $\bandwidthmatrix$ controls the width of the kernel, or, in other words, the influence of each data point (i.e., $\begin{bmatrix}\situationinitialinstance{\situationindex}\transpose & \situationfutureinstance{\situationindex}\transpose\end{bmatrix}\transpose$) on nearby regions (see \autocite{wand1994multivariate} for a more extensive explanation of the bandwidth matrix). 
There are many different ways of estimating the bandwidth matrix, ranging from simple reference rules like, e.g., Silverman's rule of thumb \autocite{silverman1986density} to more elaborate methods; see \autocite{turlach1993bandwidthselection, chiu1996comparative, jones1996brief, bashtannyk2001bandwidth, zambom2013review} for reviews of different bandwidth selection methods.

\cstart To estimate $\probabilitycond{\collision}{\situationinitial}$ of \cref{eq:probability collision expectation}, we need to draw samples from $\densityestcond{\situationfuture}{\situationinitial}$. \cend
Drawing samples from the estimated \ac{pdf} in \cref{eq:kde estimate} is straightforward: two random numbers are drawn, one to choose a random generator kernel out of the $\situationnumberof$ kernels that are used to construct the \ac{kde}, and one random number from that kernel.
Sampling from $\densityestcond{\situationfuture}{\situationinitial}$ works similarly, but instead of using an equal probability for each random generator kernel to be selected, different probabilities are used based on $\situationinitial$.
For more information on sampling from a conditional \ac{pdf} obtained using \ac{kde}, see \autocite{holmes2012fast, degelder2021conditional}.



\subsubsection{Assuming independence}
\label{sec:no special case}

Due to the curse of dimensionality \autocite{scott2015multivariate}, estimating $\density{\situationinitial, \situationfuture}$ with one \ac{kde} according to \cref{eq:kde estimate} becomes inaccurate if $\situationinitialdim + \situationfuturedim$ becomes large.
One option to avoid this curse of dimensionality is to assume that one or more parameters are independent of the other parameters. 
E.g., suppose that $\situationfuture\transpose=\begin{bmatrix}\situationfutureparta\transpose & \situationfuturepartb\transpose\end{bmatrix}$, such that $\situationfuturepartb$ is independent of $\situationinitial$ and $\situationfutureparta$.
Then we can write
\begin{equation}
	\density{\situationinitial, \situationfuture}
	= \density{\situationinitial, \situationfutureparta, \situationfuturepartb}
	= \density{\situationinitial, \situationfutureparta} \density{\situationfuturepartb}.
\end{equation}
In this case, we would need to estimate $\density{\situationinitial, \situationfutureparta}$ and $\density{\situationfuturepartb}$, which can be done in a similar manner as presented in \cref{sec:one kde}.
Because these two \acp{pdf} have fewer variables than $\density{\situationinitial, \situationfuture}$, the two estimated \acp{pdf} will suffer less from the curse of dimensionality \autocite{scott2015multivariate}.

Another option is to model $\densitycond{\situationfuture}{\situationinitial}$ as a cascade of conditional probabilities. 
For example, using the partitioning $\situationfuture\transpose=\begin{bmatrix}\situationfutureparta\transpose & \situationfuturepartb\transpose\end{bmatrix}$, $\densitycond{\situationinitial}{\situationfuture}$ can be approximated using two conditional densities:
\begin{equation}
	\densitycond{\situationfuture}{\situationinitial}
	= \densitycond{\situationfutureparta, \situationfuturepartb}{\situationinitial}
	= \densitycond{\situationfutureparta}{\situationfuturepartb, \situationinitial} \densitycond{\situationfuturepartb}{\situationinitial}
	\approx \densitycond{\situationfutureparta}{\situationfuturepartb} \densitycond{\situationfuturepartb}{\situationinitial}.
\end{equation}
This approximation is valid if $\situationfutureparta$ and $\situationinitial$ are \emph{conditionally independent given $\situationfuturepartb$} \autocite{nagler2016evading}.
The same partitioning can be applied to $\densitycond{\situationfutureparta}{\situationfuturepartb}$ and $\densitycond{\situationfuturepartb}{\situationinitial}$ until only two-dimensional \acp{pdf} need to be estimated.
Although this will lead to larger approximation errors, the lower-dimensional \acp{pdf} can be estimated more accurately. 
For more information on this approach, we refer the reader to \autocite{aas2009paircopula, nagler2016evading}.
\cstart Note that when relying on the assumption of independence or the assumption of conditional independence, these assumptions should be justified, e.g., through the use of some statistical tests like the Pearson's chi-squared test. \cend


\subsubsection{Reduce number of parameters using singular value decomposition}
\label{sec:parameter reduction}

Another way to avoid the curse of dimensionality is to use \iac{svd} \autocite{golub2013matrix} to reduce the number of parameters.
\cstart In the field of machine learning, \ac{pca} is commonly used for dimensionality reduction \autocite{abdi2010principal,hasan2021review} and \ac{pca} uses the \ac{svd}. \cend
With \iac{svd}, the parameters $\situationinitial$ and $\situationfuture$ are transformed into a lower-dimensional vector of parameters in such a way that the reduced vector of parameters describes as much of the variation as possible.
To do this, \iac{svd} is made of the matrix that contains all $\situationnumberof$ observed situations:
\begin{equation}
	\begin{bmatrix}
		\situationinitialinstance{1}-\situationinitialmean & \cdots & \situationinitialinstance{\situationnumberof}-\situationinitialmean \\
		\situationfutureinstance{1}-\situationfuturemean & \cdots & \situationfutureinstance{\situationnumberof}-\situationfuturemean
	\end{bmatrix} = \svdu \svds \svdv\transpose.
\end{equation}
Here, $\situationinitialmean=\frac{1}{\situationnumberof}\sum_{\situationindex=1}^{\situationnumberof}\situationinitialinstance{\situationindex}$ and $\situationfuturemean=\frac{1}{\situationnumberof}\sum_{\situationindex=1}^{\situationnumberof}\situationfutureinstance{\situationindex}$.
The matrices $\svdu \in \realnumbers^{\left(\situationinitialdim+\situationfuturedim\right)\times\left(\situationinitialdim+\situationfuturedim\right)}$ and $\svdv \in \realnumbers^{\situationnumberof \times \situationnumberof}$ are orthonormal, i.e., $\svdu^{-1}=\svdu\transpose$ and $\svdv^{-1}=\svdv\transpose$.
Moreover, $\svds\in\realnumbers^{\left(\situationinitialdim+\situationfuturedim\right)\times\situationnumberof}$ has only zeros except at the diagonal: the $(\svdindex,\svdindex)$-th element is $\svdsv{\svdindex}$, $\svdindex\in\{1,\ldots,\svdrank\}$ with  $\svdrank=\min(\situationinitialdim+\situationfuturedim, \situationnumberof)$, such that
\begin{equation}
	\svdsv{1} \geq \svdsv{2} \geq \ldots \geq \svdsv{\svdrank} \geq 0.
\end{equation}
Because these so-called singular values are in decreasing order, we can approximate $\situationinitial$ and $\situationfuture$ by setting $\svdsv{\svdindex}=0$ for $\svdindex > \dimension$ with $\dimension$ chosen\footnote{ We have $\dimension < \situationinitialdim+\situationfuturedim$, such that the dimension is reduced (from $\situationinitialdim+\situationfuturedim$ to $\dimension$) and we have $\dimension>\situationinitialdim$, such that the number of linear constraints in \cref{eq:linear constraint} ($\situationinitialdim$) is smaller than the number of variables ($\dimension$).} such that $\situationinitialdim < \dimension < \situationinitialdim+\situationfuturedim$:
\begin{equation}
	\label{eq:svd approximation}
	\begin{bmatrix}
		\situationinitialinstance{\situationindex} - \situationinitialmean \\
		\situationfutureinstance{\situationindex} - \situationfuturemean
	\end{bmatrix}
	= \sum_{\svdindex=1}^{\svdrank} \svdsv{\svdindex} \svdventry{\situationindex}{\svdindex} \svduvec{\svdindex}
	\approx \sum_{\svdindex=1}^{\dimension} \svdsv{\svdindex} \svdventry{\situationindex}{\svdindex} \svduvec{\svdindex},
	= \begin{bmatrix} \svduupperleft \\ \svdulowerleft \end{bmatrix} \svdsupperleft \svdvvecd{\situationindex},
\end{equation}
where $\svdventry{\situationindex}{\svdindex}$ is the $(\situationindex,\svdindex)$-th element of $\svdv$ and $\svduvec{\svdindex}$ is the $\svdindex$-th column of $\svdu$.
Moreover, $\svduupperleft$ is the $\situationinitialdim$-by-$\dimension$ upper left submatrix of $\svdu$, $\svdulowerleft$ is the $\situationfuturedim$-by-$\dimension$ lower left submatrix $\svdu$, $\svdsupperleft\in\realnumbers^{\dimension\times\dimension}$ is the diagonal matrix with the first $\dimension$ singular values on its diagonal, and $\svdvvecd{\situationindex}\transpose = \begin{bmatrix} \svdventry{\situationindex}{1} & \cdots & \svdventry{\situationindex}{\dimension} \end{bmatrix}$.
Thus, with $\situationinitialmean$, $\situationfuturemean$, $\svduupperleft$, $\svdulowerleft$, and $\svdsupperleft$, the $(\situationinitialdim+\situationfuturedim)$-dimensional vector $\begin{bmatrix}\situationinitialinstance{\situationindex}\transpose & \situationfutureinstance{\situationindex}\transpose\end{bmatrix}\transpose$ is approximated using the $\dimension$-dimensional vector $\svdvvecd{\situationindex}$.

Instead of estimating the \ac{pdf} of $\begin{bmatrix}\situationinitialinstance{\situationindex}\transpose & \situationfutureinstance{\situationindex}\transpose\end{bmatrix}\transpose$, we now estimate the \ac{pdf} of $\svdvvecd{\situationindex}$ using \ac{kde} as described in \cref{sec:one kde}.
\cstart Note that the choice of $\dimension$ includes a trade-off. 
Choosing $\dimension$ too small results in too much loss of detail, while choosing $\dimension$ too large will give accuracy problems when estimating the \ac{pdf} of the new parameters.
For more information on choosing an appropriate value of $\dimension$, we refer the reader to \autocite{degelder2021generation}. \cend
To sample from $\densityestcond{\situationfuture}{\situationinitial}$, we can sample from the estimated distribution of $\svdvvecd{\situationindex}$.
Because \cref{eq:svd approximation} is a linear mapping, the sample $\svdvvecsymbol$ that is drawn from the estimated distribution of $\svdvvecd{\situationindex}$ is subject to a linear constraint:
\begin{equation}
	\label{eq:linear constraint}
	\svduupperleft \svdsupperleft \svdvvecsymbol = \situationinitial - \situationinitialmean.
\end{equation}
In \autocite{degelder2021conditional}, an algorithm is provided for sampling from \iac{kde} with a Gaussian kernel of \cref{eq:kernel initial future} such that the resulting samples are subject to a linear constraint such as \cref{eq:linear constraint}.



\subsection{Estimate $\probabilitycond{\collision}{\situationinitial}$ using a Monte Carlo simulation}
\label{sec:estimate collision}

Monte Carlo simulations are used to estimate $\probabilitycond{\collision}{\situationinitial}$, i.e., the probability of an event $\collision$ given the initial situation described by $\situationinitial$.
The details of the simulation depend on the actual application. 
For example, if the goal of our \ac{ssm} is to evaluate the risk that a human-driven vehicle collides, the simulation should involve a human driving behavior model. 
On the other hand, if the goal is to evaluate the risk of a crash when \iac{ads} is controlling the vehicle, the simulation should include the model of this \ac{ads}.

A straightforward way to compute $\probabilitycond{\collision}{\situationinitial}$ is to repeat a certain number of simulations with the same $\situationinitial$ and count the number of simulations that result in the event $\collision$.
If $\numberofsimulations$ denotes the number of simulations and $\numberofcollisions$ is the number of events $\collision$, then $\probabilitycond{\collision}{\situationinitial}$ could be estimated using
\begin{equation}
	\label{eq:binomial estimation}
	\probabilityestcond{\collision}{\situationinitial}
	= \frac{\numberofcollisions}{\numberofsimulations}.
\end{equation}

An important choice for estimating $\probabilitycond{\collision}{\situationinitial}$ is the number of simulations, $\numberofsimulations$.
One approach is to keep increasing $\numberofsimulations$ until there is enough confidence in the estimation of \cref{eq:binomial estimation}.
For example, the Clopper-Pearson interval \autocite{clopper1934use} or the Wilson score interval \autocite{wilson1927probable} can be used to determine the confidence of the estimation of \cref{eq:binomial estimation}.
A disadvantage of \cref{eq:binomial estimation} is that only the fact whether the event $\collision$ occurred or not is used, while the simulation provides more information, such as the minimum distance between two objects or the impact speed in case of a crash.
Therefore, we provide an alternative approach to estimate $\probabilitycond{\collision}{\situationinitial}$.

For the alternative approach, let us assume that one simulation run provides more information than just the fact that the event $\collision$ occurred or not.
Let $\simulationresult \in \realnumbers^{\dimsimulationresult}$ be a continuous variable representing the result of a simulation run and let $\spacecollision$ denote the set of possible simulation results in which the event $\collision$ occurred. 
Thus, $\simulationresult \in \spacecollision$ if and only if the simulation results in the event $\collision$.
We assume $\spacecollision$ is known; see, e.g., the example in \cref{sec:wang stamatiadis replicate}.
Therefore, we have
\begin{equation}
	\probabilitycond{\collision}{\situationinitial}
	= \probabilitycond{\simulationresult \in \spacecollision}{\situationinitial}
	= \int_{\spacecollision} \densitycond{\simulationresult}{\situationinitial} \ud \simulationresult.
\end{equation}
Similar as with the estimation of $\density{\situationinitial, \situationfuture}$ in \cref{sec:estimate future}, we employ \ac{kde} to estimate $\densitycond{\simulationresult}{\situationinitial}$:
\begin{equation}
	\label{eq:kde simulation result}
	\densityestcond{\simulationresult}{\situationinitial}
	= \frac{1}{\numberofsimulations} 
	\sum_{\simulationindex=1}^{\numberofsimulations} \kernelfuncnormalized{\simulationbandwidth}{\simulationinstance{\simulationindex} - \simulationresult},
\end{equation}
where $\simulationinstance{\simulationindex}$ denotes the result of the $\simulationindex$-th simulation and $\simulationbandwidth$ denotes an appropriate bandwidth matrix.
The kernel function $\kernelfuncnormalized{\simulationbandwidth}{\cdot}$ is similarly defined as \cref{eq:kernel initial future}.
We can now estimate $\probabilitycond{\collision}{\situationinitial}$ by substituting $\densityestcond{\simulationresult}{\situationinitial}$ of \cref{eq:kde simulation result} for $\densitycond{\simulationresult}{\situationinitial}$:
\begin{equation}
	\label{eq:estimate probability of collision}
	\probabilityestcond{\collision}{\situationinitial}
	= \probabilityestcond{\simulationresult \in \spacecollision}{\situationinitial}
	= \int_{\spacecollision} \densityestcond{\simulationresult}{\situationinitial} \ud \simulationresult
	=\frac{1}{\numberofsimulations}
	\sum_{\simulationindex=1}^{\numberofsimulations} \int_{\spacecollision}
	\kernelfuncnormalized{\simulationbandwidth}{\simulationinstance{\simulationindex} - \simulationresult} \ud \simulationresult.
\end{equation}

Similar as with \cref{eq:binomial estimation}, we need to choose the number of simulations $\numberofsimulations$.
Our proposal is to keep increasing $\numberofsimulations$ until \cstart the uncertainty of the estimated probability $\probabilityestcond{\simulationresult \in \spacecollision}{\situationinitial}$ is below a certain threshold. 
As a measure for the uncertainty of the estimated probability $\probabilityestcond{\simulationresult \in \spacecollision}{\situationinitial}$, we use the variance of $\probabilityestcond{\simulationresult \in \spacecollision}{\situationinitial}$. 
Thus, we keep increasing $\numberofsimulations$ until \cend the variance of $\probabilityestcond{\simulationresult \in \spacecollision}{\situationinitial}$ is below a threshold $\simulationthreshold>0$.
The variance follows from \autocite{nadaraya1964some}:
\begin{equation}
	\label{eq:variance estimation}
	\variance{\probabilityestcond{\simulationresult \in \spacecollision}{\situationinitial}}
	= \frac{\probabilitycond{\simulationresult \in \spacecollision}{\situationinitial}
		\left( 1-\probabilitycond{\simulationresult \in \spacecollision}{\situationinitial} \right)}{\numberofsimulations}.
\end{equation}
Because $\probabilitycond{\simulationresult \in \spacecollision}{\situationinitial}$ is unknown, \cstart we cannot directly use \cref{eq:variance estimation}.
Instead, \cend we \cstart substitute \cend the estimated counterpart of \cref{eq:estimate probability of collision} \cstart for $\probabilitycond{\simulationresult \in \spacecollision}{\situationinitial}$\cend.
Thus, $\numberofsimulations$ is increased until the following condition is met:
\begin{equation}
	\label{eq:condition stop simulations}
	\frac{\probabilityestcond{\simulationresult \in \spacecollision}{\situationinitial}
		\left( 1-\probabilityestcond{\simulationresult \in \spacecollision}{\situationinitial} \right)}{\numberofsimulations}
	< \simulationthreshold.
\end{equation}



\subsection{Regression for real-time estimation of $\probabilitycond{\collision}{\situationinitial}$}
\label{sec:final metric calculation}

To evaluate the risk measure during real-time operation of the ego vehicle, the expression of \cref{eq:estimate probability of collision} is problematic, because it would require $\numberofsimulations$ simulations.
Even if the calculation is accelerated using a technique such as importance sampling, it might take too long.
Therefore, we propose to evaluate \cref{eq:estimate probability of collision} only for some fixed $\left\{\situationinitialinstance{\situationindexdesign}'\right\}_{\situationindexdesign=1}^{\numberofdesignpoints}$.
Next, regression is used to estimate \cref{eq:estimate probability of collision}.
One option is to choose a parametric model, e.g., a logistic model, and estimate the parameters of the model using $\left\{\left(\situationinitialinstance{\situationindexdesign}',\probabilityestcond{\collision}{\situationinitialinstance{\situationindexdesign}'}\right)\right\}_{\situationindexdesign=1}^{\numberofdesignpoints}$.
Up to our knowledge, however, there is no good reason to assume a particular parametric model, so we use a non-parametric regression technique to estimate \cref{eq:estimate probability of collision}.
More specifically, we use the \acf{nw} kernel estimator \autocite{wasserman2006nonparametric}, because it automatically smooths the data (as is demonstrated in \cref{sec:wang stamatiadis comparison}) and the approximation is guaranteed to give a number between 0 and 1, also when extrapolating the data.
The \ac{nw} kernel estimator is given by: 
\begin{equation}
	\label{eq:nadaraya watson}
	\probabilityestcond{\collision}{\situationinitial}
	\approx \frac{ \sum_{\situationindexdesign=1}^{\numberofdesignpoints}
		\kernelfuncnormalized{\bandwidthnw}{\situationinitial - \situationinitialinstance{\situationindexdesign}'}
		\probabilityestcond{\collision}{\situationinitialinstance{\situationindexdesign}'}
	}{\sum_{\situationindexdesign=1}^{\numberofdesignpoints}
		\kernelfuncnormalized{\bandwidthnw}{\situationinitial - \situationinitialinstance{\situationindexdesign}'}}.
\end{equation}
Here, $\probabilityestcond{\collision}{\situationinitialinstance{\situationindexdesign}'}$ is based on \cref{eq:estimate probability of collision} and $\kernelfuncnormalized{\bandwidthnw}{\cdot}$ represents the Gaussian kernel given by \cref{eq:kernel initial future}.
Two important choices have to be made: The choice of $\left\{\situationinitialinstance{\situationindexdesign}'\right\}_{\situationindexdesign=1}^{\numberofdesignpoints}$ for which to evaluate \cref{eq:estimate probability of collision} and the choice of the bandwidth matrix $\bandwidthnw$.
We suggest to base the design points $\left\{\situationinitialinstance{\situationindexdesign}'\right\}_{\situationindexdesign=1}^{\numberofdesignpoints}$ on the data that is used to estimate $\densitycond{\situationfuture}{\situationinitial}$ in \cref{sec:estimate future}, i.e., $\left\{\situationinitialinstance{\situationindex}\right\}_{\situationindex=1}^{\situationnumberof}$, such that all $\situationinitialinstance{\situationindex}$ have at least one design point $\situationinitialinstance{\situationindexdesign}'$ nearby.
In other words, $\left\{\situationinitialinstance{\situationindexdesign}'\right\}_{\situationindexdesign=1}^{\numberofdesignpoints}$ is chosen such that
\begin{equation}
	\label{eq:design points distance}
	\min_{\situationindexdesign} 
	\left( \situationinitialinstance{\situationindex} - \situationinitialinstance{\situationindexdesign}' \right)\transpose
	\weightmatrix 
	\left( \situationinitialinstance{\situationindex} - \situationinitialinstance{\situationindexdesign}' \right)
	\leq 1,
	\quad \forall \situationindex \in \{1, \ldots, \situationnumberof\},
\end{equation}
where $\weightmatrix$ denotes a weighting matrix. 
Note that if $\weightmatrix$ is the identity matrix, then \cref{eq:design points distance} calculates the minimum squared Euclidean distance.
In general, $\weightmatrix$ is a diagonal matrix.
Choosing the diagonal elements of $\weightmatrix$ is a trade-off; if the elements are too large, then too many details are lost in the approximation of \cref{eq:nadaraya watson}; if the elements are too small, it takes too long to evaluate \cref{eq:estimate probability of collision} $\numberofdesignpoints$ times, as $\numberofdesignpoints$ increases for lower diagonal elements of $\weightmatrix$.
The bandwidth matrix $\bandwidthnw$ might be based on $\weightmatrix$, e.g., $\bandwidthnw=\weightmatrix^{-1}$.
Alternatively, $\bandwidthnw$ might be based on the measurement uncertainty of $\situationinitial$ if this measurement uncertainty is significant, where a larger $\bandwidthnw$ applies in case of a larger measurement uncertainty of $\situationinitial$.
Note that if $\bandwidthnw$ is a diagonal matrix with positive values on the diagonal that are close to zero, then the \ac{nw} kernel estimation of \cref{eq:nadaraya watson} acts like nearest-neighbor interpolation.
