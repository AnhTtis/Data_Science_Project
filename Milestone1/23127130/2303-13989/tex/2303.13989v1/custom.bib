% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").



@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{articleSemSim,
author = {Fernando, Samuel and Stevenson, Mark},
year = {2009},
month = {08},
pages = {},
title = {A Semantic Similarity Approach to Paraphrase Detection},
journal = {Proceedings of the 11th Annual Research Colloquium of the UK Special Interest Group for Computational Linguistics}
}

@inproceedings{brockett2005support,
author = {Brockett, Chris and Dolan, Bill},
title = {Support Vector Machines for Paraphrase Identification and Corpus Construction},
booktitle = {Third International Workshop on Paraphrasing (IWP2005)},
year = {2005},
month = {January},
abstract = {The lack of readily-available large corpora of aligned monolingual sentence pairs is a major obstacle to the development of Statistical Machine Translation-based paraphrase models. In this paper, we describe the use of annotated datasets and Support Vector Machines to induce larger monolingual paraphrase corpora from a comparable corpus of news clusters found on the World Wide Web. Features include: morphological variants; WordNet synonyms and hypernyms; log-likelihood-based word pairings dynamically obtained from baseline sentence alignments; and formal stringfeatures such as word-based edit distance. Use of this technique dramatically reduces the Alignment Error Rate of the extracted corpora over heuristic methods based on position of the sentences in the text.},
publisher = {Asia Federation of Natural Language Processing},
url = {https://www.microsoft.com/en-us/research/publication/support-vector-machines-for-paraphrase-identification-and-corpus-construction/},
edition = {Third International Workshop on Paraphrasing (IWP2005)},
}


@misc{gpt4technicalreport,
  doi = {10.48550/ARXIV.2303.08774},
  url = {https://arxiv.org/abs/2303.08774},
  author = {{OpenAI}
},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {GPT-4 Technical Report},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}



@inproceedings{Bojar2016CzEng1E,
  title={CzEng 1.6: Enlarged Czech-English Parallel Corpus with Processing Tools Dockered},
  author={Ondrej Bojar and Ondrej Dusek and Tom Kocmi and Jindřich Libovick{\'y} and Michal Nov{\'a}k and Martin Popel and Roman Sudarikov and Dusan Varis},
  booktitle={TSD},
  year={2016}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

% REFS OF PAPER HERE

@article{Chow,
  author={Chowdhury, Hussain A and Bhattacharyya, Dhruba K},
  title={Plagiarism: Taxonomy, tools and detection techniques},
  journal={arXiv preprint arXiv:1801.06323},
  year={2018}
}

@incollection{Foltynek2020,
    title = {Detecting {Machine}-{Obfuscated} {Plagiarism}},
    volume = {12051 LNCS},
    booktitle = {Sustainable {Digital} {Communities}},
    publisher = {Springer},
    author = {Folt{\'y}nek, Tom{\'a}{\v s} and Ruas, Terry and Scharpf, Philipp and
    Meuschke, Norman and Schubotz, Moritz and Grosky, William and Gipp, Bela},
    year = {2020},
    doi = {10.1007/978-3-030-43687-2_68},
    pages = {816--827}
}


@InProceedings{Kond,
    author="Kondrak, Grzegorz",
    editor="Consens, Mariano
    and Navarro, Gonzalo",
    title="N-Gram Similarity and Distance",
    booktitle="String Processing and Information Retrieval",
    year="2005",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="115--126",
    abstract="In many applications, it is necessary to algorithmically quantify the similarity exhibited by two strings composed of symbols from a finite alphabet. Numerous string similarity measures have been proposed. Particularly well-known measures are based are edit distance and the length of the longest common subsequence. We develop a notion of n-gram similarity and distance. We show that edit distance and the length of the longest common subsequence are special cases of n-gram distance and similarity, respectively. We provide formal, recursive definitions of n-gram similarity and distance, together with efficient algorithms for computing them. We formulate a family of word similarity measures based on n-grams, and report the results of experiments that suggest that the new measures outperform their unigram equivalents.",
    isbn="978-3-540-32241-2"
}

@article{DBLP:journals/corr/abs-1105-5444,
    author    = {Philip Resnik},
    title     = {Semantic Similarity in a Taxonomy: An Information-Based Measure and
               its Application to Problems of Ambiguity in Natural Language},
    journal   = {CoRR},
    volume    = {abs/1105.5444},
    year      = {2011},
    url       = {http://arxiv.org/abs/1105.5444},
    eprinttype = {arXiv},
    eprint    = {1105.5444},
    timestamp = {Mon, 13 Aug 2018 16:48:24 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/abs-1105-5444.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Resn,
    author = {Islam, Aminul and Inkpen, Diana},
    year = {2009},
    month = {01},
    pages = {},
    title = {Semantic Similarity of Short Texts},
    volume = {309},
    journal = {Current Issues in Linguistic Theory},
    doi = {10.1075/cilt.309.18isl}
}

@InProceedings{WenYenFuzzy,
    author={Wu, Wen-Yen},
    booktitle={2016 International Computer Symposium (ICS)}, 
    title={A Method for Fuzzy String Matching}, 
    year={2016},
    volume={},
    number={},
    pages={380-383},
    doi={10.1109/ICS.2016.0083}}

@article{DeerSemantic,
    author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
    title = {Indexing by latent semantic analysis},
    journal = {Journal of the American Society for Information Science},
    volume = {41},
    number = {6},
    pages = {391-407},
    doi = {https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
    url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
    eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
    abstract = {Abstract A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley \& Sons, Inc.},
    year = {1990}
}

@article{BERT,
    author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
    title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
    journal   = {CoRR},
    volume    = {abs/1810.04805},
    year      = {2018},
    url       = {http://arxiv.org/abs/1810.04805},
    eprinttype = {arXiv},
    eprint    = {1810.04805},
    timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
    biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{GPT3,
    author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
    title     = {Language Models are Few-Shot Learners},
    journal   = {CoRR},
    volume    = {abs/2005.14165},
    year      = {2020},
    url       = {https://arxiv.org/abs/2005.14165},
    eprinttype = {arXiv},
    eprint    = {2005.14165},
    timestamp = {Wed, 03 Jun 2020 11:36:54 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{NGram,
    author="Kondrak, Grzegorz",
    editor="Consens, Mariano
    and Navarro, Gonzalo",
    title="N-Gram Similarity and Distance",
    booktitle="String Processing and Information Retrieval",
    year="2005",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="115--126",
    abstract="In many applications, it is necessary to algorithmically quantify the similarity exhibited by two strings composed of symbols from a finite alphabet. Numerous string similarity measures have been proposed. Particularly well-known measures are based are edit distance and the length of the longest common subsequence. We develop a notion of n-gram similarity and distance. We show that edit distance and the length of the longest common subsequence are special cases of n-gram distance and similarity, respectively. We provide formal, recursive definitions of n-gram similarity and distance, together with efficient algorithms for computing them. We formulate a family of word similarity measures based on n-grams, and report the results of experiments that suggest that the new measures outperform their unigram equivalents.",
    isbn="978-3-540-32241-2"
}

@article{T5,
    author    = {Colin Raffel and
               Noam Shazeer and
               Adam Roberts and
               Katherine Lee and
               Sharan Narang and
               Michael Matena and
               Yanqi Zhou and
               Wei Li and
               Peter J. Liu},
    title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
               Transformer},
    journal   = {CoRR},
    volume    = {abs/1910.10683},
    year      = {2019},
    url       = {http://arxiv.org/abs/1910.10683},
    eprinttype = {arXiv},
    eprint    = {1910.10683},
    timestamp = {Fri, 05 Feb 2021 15:43:41 +0100},
    biburl    = {https://dblp.org/rec/journals/corr/abs-1910-10683.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{etpc,
    title = "{ETPC} - A Paraphrase Identification Corpus Annotated with Extended Paraphrase Typology and Negation",
    author = "Kovatchev, Venelin  and
      Mart{\'\i}, M. Ant{\`o}nia  and
      Salam{\'o}, Maria",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1221",
}

@inproceedings{dolan-brockett-2005-MRPC,
    title = "Automatically Constructing a Corpus of Sentential Paraphrases",
    author = "Dolan, William B.  and
      Brockett, Chris",
    booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
    year = "2005",
    url = "https://aclanthology.org/I05-5002",
}

@article{QQP,
  title={Bilateral multi-perspective matching for natural language sentences},
  author={Wang, Zhiguo and Hamza, Wael and Florian, Radu},
  journal={arXiv preprint arXiv:1702.03814},
  year={2017}
}

@article{WikiSplit,
    author    = {Jan A. Botha and
               Manaal Faruqui and
               John Alex and
               Jason Baldridge and
               Dipanjan Das},
    title     = {Learning To Split and Rephrase From Wikipedia Edit History},
    journal   = {CoRR},
    volume    = {abs/1808.09468},
    year      = {2018},
    url       = {http://arxiv.org/abs/1808.09468},
    eprinttype = {arXiv},
    eprint    = {1808.09468},
    timestamp = {Wed, 05 Sep 2018 13:21:45 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/abs-1808-09468.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Radford2019GPT2,
    title={Language Models are Unsupervised Multitask Learners},
    author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
    year={2019}
}

@inproceedings{lan-etal-2017-TURL,
    title = "A Continuously Growing Dataset of Sentential Paraphrases",
    author = "Lan, Wuwei  and
      Qiu, Siyu  and
      He, Hua  and
      Xu, Wei",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1126",
    doi = "10.18653/v1/D17-1126",
    pages = "1224--1234",
    abstract = "A major challenge in paraphrase research is the lack of parallel corpora. In this paper, we present a new method to collect large-scale sentential paraphrases from Twitter by linking tweets through shared URLs. The main advantage of our method is its simplicity, as it gets rid of the classifier or human in the loop needed to select data before annotation and subsequent application of paraphrase identification algorithms in the previous work. We present the largest human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification. In addition, we show that more than 30,000 new sentential paraphrases can be easily and continuously captured every month at {\textasciitilde}70{\%} precision, and demonstrate their utility for downstream NLP tasks through phrasal paraphrase extraction. We make our code and data freely available.",
}

@article{Maat-tsne,
    author  = {Laurens van der Maaten and Geoffrey Hinton},
    title   = {Visualizing Data using t-SNE},
    journal = {Journal of Machine Learning Research},
    year    = {2008},
    volume  = {9},
    number  = {86},
    pages   = {2579--2605},
    url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@article{Marti11-ParaphraseTypology,
    author = {Vila, Marta and Martí, Antonia and Rodríguez, Horacio},
    year = {2011},
    month = {04},
    pages = {},
    title = {Paraphrase Concept and Typology. A Linguistically Based and Computationally Oriented Approach},
    volume = {46},
    journal = {Procesamiento de Lenguaje Natural}
}



@inproceedings{xlnet,
 author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
 url = {https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{CardHKJ20,
  title = {With {{Little Power Comes Great Responsibility}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Card, Dallas and Henderson, Peter and Khandelwal, Urvashi and Jia, Robin and Mahowald, Kyle and Jurafsky, Dan},
  year = {2020},
  pages = {9263--9274},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.745},
  langid = {english},
  keywords = {!jw},
  file = {/Users/jp/Zotero/storage/BXV4W9JM/CardHKJ20--JW--with_little_power_comes_great_responsibility.pdf}
}
% == BibTeX quality report for CardHKJ20:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Proc. 2020 Conf. Empir. Methods Nat. Lang. Process. EMNLP")
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Url ("https://www.aclweb.org/anthology/2020.emnlp-main.745")

@inproceedings{ChenTXH20,
  title = {A {{Semantically Consistent}} and {{Syntactically Variational Encoder-Decoder Framework}} for {{Paraphrase Generation}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Chen, Wenqing and Tian, Jidong and Xiao, Liqiang and He, Hao and Jin, Yaohui},
  year = {2020},
  pages = {1186--1198},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.102},
  langid = {english},
  file = {/Users/jp/Zotero/storage/4I29FFZA/Chen et al. - 2020 - A Semantically Consistent and Syntactically Variat.pdf}
}
% == BibTeX quality report for ChenTXH20:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Proc. 28th Int. Conf. Comput. Linguist.")
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Url ("https://www.aclweb.org/anthology/2020.coling-main.102")

@inproceedings{DouFKS22,
  title = {Is {{GPT-3 Text Indistinguishable}} from {{Human Text}}? {{Scarecrow}}: {{A Framework}} for {{Scrutinizing Machine Text}}},
  shorttitle = {Is {{GPT-3 Text Indistinguishable}} from {{Human Text}}?},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Dou, Yao and Forbes, Maxwell and {Koncel-Kedziorski}, Rik and Smith, Noah and Choi, Yejin},
  year = {2022},
  pages = {7250--7274},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.501},
  langid = {english},
  file = {/Users/jp/Zotero/storage/RFJXH3W6/DouFKS22--JW--is_gpt-3_text_indistinguishable_from_human_text_scarecrow_a_framework_for_scrutinizing.pdf}
}
% == BibTeX quality report for DouFKS22:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Proc. 60th Annu. Meet. Assoc. Comput. Linguist. Vol. 1 Long Pap.")
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Url ("https://aclanthology.org/2022.acl-long.501")

@misc{attentionisallyouneed,
  doi = {10.48550/ARXIV.1706.03762},
  
  url = {https://arxiv.org/abs/1706.03762},
  
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Attention Is All You Need},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{transformersurvey,
  doi = {10.48550/ARXIV.2106.04554},
  url = {https://arxiv.org/abs/2106.04554},
  author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Survey of Transformers},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{fu2020paraphrase,
      title={Paraphrase Generation with Latent Bag of Words}, 
      author={Yao Fu and Yansong Feng and John P. Cunningham},
      year={2020},
      eprint={2001.01941},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{GiniCoefficient,
 ISSN = {00029890, 19300972},
 URL = {https://www.jstor.org/stable/10.4169/000298910x523344},
 abstract = {Abstract The Gini index is a summary statistic that measures how equitably a resource is distributed in a population; income is a primary example. In addition to a self-contained presentation of the Gini index, we give two equivalent ways to interpret this summary statistic: first in terms of the percentile level of the person who earns the average dollar, and second in terms of how the lower of two randomly chosen incomes compares, on average, to mean income.},
 author = {Frank A. Farris},
 journal = {The American Mathematical Monthly},
 number = {10},
 pages = {pp. 851--864},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {The Gini Index and Measures of Inequality},
 urldate = {2023-03-16},
 volume = {117},
 year = {2010}
}

@misc{word2vec,
  doi = {10.48550/ARXIV.1301.3781},
  
  url = {https://arxiv.org/abs/1301.3781},
  
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Efficient Estimation of Word Representations in Vector Space},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


% == BibTeX quality report for FacultatdeFilologiaUniversitatdeBarcelonaSpainKUM19:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("Recent Advances in Natural Language Processing")
% ? unused Journal abbreviation ("Proc. - Nat. Lang. Process. Deep Learn. World")
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Url ("https://acl-bg.org/proceedings/2019/RANLP 2019/pdf/RANLP067.pdf")

@article{FoltynekMG19,
  ids = {FoltynekMG19a},
  title = {Academic {{Plagiarism Detection}}: {{A Systematic Literature Review}}},
  author = {Folt{\'y}nek, Tom{\'a}{\v s} and Meuschke, Norman and Gipp, Bela},
  year = {2019},
  month = oct,
  journal = {ACM Computing Surveys},
  volume = {52},
  number = {6},
  pages = {112:1-112:42},
  issn = {0360-0300},
  doi = {10/dcxc},
  abstract = {This article summarizes the research on computational methods to detect academic plagiarism by systematically reviewing 239 research papers published between 2013 and 2018. To structure the presentation of the research contributions, we propose novel technically oriented typologies for plagiarism prevention and detection efforts, the forms of academic plagiarism, and computational plagiarism detection methods. We show that academic plagiarism detection is a highly active research field. Over the period we review, the field has seen major advances regarding the automated detection of strongly obfuscated and thus hard-to-identify forms of academic plagiarism. These improvements mainly originate from better semantic text analysis methods, the investigation of non-textual content features, and the application of machine learning. We identify a research gap in the lack of methodologically thorough performance evaluations of plagiarism detection systems. Concluding from our analysis, we see the integration of heterogeneous analysis methods for textual and non-textual content features using machine learning as the most promising area for future research contributions to improve the detection of academic plagiarism further.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  oldkey = {Foltynek2019},
  topic = {pd},
  keywords = {!bg,!bg_author,!bg_preprint,!nm,!nm_author,!nm_preprint,21_EEKE_CL-KEA,21InfWissHb,jabref_imp1_clean},
  file = {/Users/jp/Zotero/storage/R3D9LSPC/FoltynekMG19--GG--academic_plagiarism_detection_a_systematic_literature_review.pdf}
}
% == BibTeX quality report for FoltynekMG19:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{MaddelaAX21,
  title = {Controllable {{Text Simplification}} with {{Explicit Paraphrasing}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Maddela, Mounica and {Alva-Manchego}, Fernando and Xu, Wei},
  year = {2021},
  pages = {3536--3553},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.277},
  langid = {english},
  file = {/Users/jp/Zotero/storage/FA73K9KU/Maddela et al. - 2021 - Controllable Text Simplification with Explicit Par.pdf}
}
% == BibTeX quality report for MaddelaAX21:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Proc. 2021 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol.")
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Url ("https://aclanthology.org/2021.naacl-main.277")

@inproceedings{MengAHS21a,
  title = {{{ConRPG}}: {{Paraphrase Generation}} Using {{Contexts}} as {{Regularizer}}},
  shorttitle = {{{ConRPG}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Meng, Yuxian and Ao, Xiang and He, Qing and Sun, Xiaofei and Han, Qinghong and Wu, Fei and Fan, Chun and Li, Jiwei},
  year = {2021},
  pages = {2551--2562},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.199},
  langid = {english},
  file = {/Users/jp/Zotero/storage/5N2KXU4G/Meng et al. - 2021 - ConRPG Paraphrase Generation using Contexts as Re.pdf}
}
% == BibTeX quality report for MengAHS21a:
% ? unused Journal abbreviation ("Proc. 2021 Conf. Empir. Methods Nat. Lang. Process.")
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Url ("https://aclanthology.org/2021.emnlp-main.199")

@article{PizarroVV17a,
  title = {Docode 5: {{Building}} a Real-World Plagiarism Detection System},
  shorttitle = {Docode 5},
  author = {Pizarro V., Gaspar and Vel{\'a}squez, Juan D.},
  year = {2017},
  month = sep,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {64},
  pages = {261--271},
  issn = {09521976},
  doi = {10.1016/j.engappai.2017.06.001},
  langid = {english}
}
% == BibTeX quality report for PizarroVV17a:
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Url ("https://linkinghub.elsevier.com/retrieve/pii/S0952197617301203")

@article{PrenticeK18,
  title = {Paraphrasing Tools, Language Translation Tools and Plagiarism: An Exploratory Study},
  shorttitle = {Paraphrasing Tools, Language Translation Tools and Plagiarism},
  author = {Prentice, Felicity M. and Kinden, Clare E.},
  year = {2018},
  month = dec,
  journal = {International Journal for Educational Integrity},
  volume = {14},
  number = {1},
  pages = {11},
  issn = {1833-2595},
  doi = {10.1007/s40979-018-0036-7},
  langid = {english},
  file = {/Users/jp/Zotero/storage/MZ9WULY7/PrenticeK18--JW--paraphrasing_tools_language_translation_tools_and_plagiarism_an_exploratory_study.pdf}
}
% == BibTeX quality report for PrenticeK18:
% ? unused Journal abbreviation ("Int J Educ Integr")
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Url ("https://edintegrity.biomedcentral.com/articles/10.1007/s40979-018-0036-7")

@article{RogersonM17,
  title = {Using {{Internet}} Based Paraphrasing Tools: {{Original}} Work, Patchwriting or Facilitated Plagiarism?},
  shorttitle = {Using {{Internet}} Based Paraphrasing Tools},
  author = {Rogerson, Ann M. and McCarthy, Grace},
  year = {2017},
  month = dec,
  journal = {International Journal for Educational Integrity},
  volume = {13},
  number = {1},
  pages = {2},
  issn = {1833-2595},
  doi = {10.1007/s40979-016-0013-y},
  langid = {english},
  file = {/Users/jp/Zotero/storage/4YIU6GFU/Rogerson and McCarthy - 2017 - Using Internet based paraphrasing tools Original .pdf}
}
% == BibTeX quality report for RogersonM17:
% ? unused Journal abbreviation ("Int J Educ Integr")
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Url ("https://edintegrity.biomedcentral.com/articles/10.1007/s40979-016-0013-y")


% == BibTeX quality report for Wahle2021:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Proc. ACMIEEE Jt. Conf. Digit. Libr. JCDL")


% == BibTeX quality report for WahleRFM22:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Series title ("Lecture Notes in Computer Science")
% ? unused Url ("https://link.springer.com/10.1007/978-3-030-96957-8_34")

@inproceedings{YangLLY22,
  title = {{{GCPG}}: {{A General Framework}} for {{Controllable Paraphrase Generation}}},
  shorttitle = {{{GCPG}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Yang, Kexin and Liu, Dayiheng and Lei, Wenqiang and Yang, Baosong and Zhang, Haibo and Zhao, Xue and Yao, Wenqing and Chen, Boxing},
  year = {2022},
  pages = {4035--4047},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.318},
  langid = {english},
  file = {/Users/jp/Zotero/storage/RHRSS4B2/Yang et al. - 2022 - GCPG A General Framework for Controllable Paraphr.pdf}
}
% == BibTeX quality report for YangLLY22:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Find. Assoc. Comput. Linguist. ACL 2022")
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Url ("https://aclanthology.org/2022.findings-acl.318")

@inproceedings{ZhuCSL17,
  title = {Knowledge-Based {{Question Answering}} by {{Jointly Generating}}, {{Copying}} and {{Paraphrasing}}},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Zhu, Shuguang and Cheng, Xiang and Su, Sen and Lang, Shuang},
  year = {2017},
  month = nov,
  pages = {2439--2442},
  publisher = {{ACM}},
  address = {{Singapore Singapore}},
  doi = {10.1145/3132847.3133064},
  isbn = {978-1-4503-4918-5},
  langid = {english}
}
% == BibTeX quality report for ZhuCSL17:
% ? unused Conference name ("CIKM '17: ACM Conference on Information and Knowledge Management")
% ? unused Journal abbreviation ("Proc. 2017 ACM Conf. Inf. Knowl. Manag.")
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Url ("https://dl.acm.org/doi/10.1145/3132847.3133064")



@article{BerinskyHL12,
  title = {Evaluating {{Online Labor Markets}} for {{Experimental Research}}: {{Amazon}}.Com's {{Mechanical Turk}}},
  shorttitle = {Evaluating {{Online Labor Markets}} for {{Experimental Research}}},
  author = {Berinsky, Adam J. and Huber, Gregory A. and Lenz, Gabriel S.},
  year = {2012},
  journal = {Political Analysis},
  volume = {20},
  number = {3},
  pages = {351--368},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpr057},
  abstract = {We examine the trade-offs associated with using               Amazon.com               's Mechanical Turk (MTurk) interface for subject recruitment. We first describe MTurk and its promise as a vehicle for performing low-cost and easy-to-field experiments. We then assess the internal and external validity of experiments performed using MTurk, employing a framework that can be used to evaluate other subject pools. We first investigate the characteristics of samples drawn from the MTurk population. We show that respondents recruited in this manner are often more representative of the U.S. population than in-person convenience samples\textemdash the modal sample in published experimental political science\textemdash but less representative than subjects in Internet-based panels or national probability samples. Finally, we replicate important published experimental work using MTurk samples.},
  langid = {english},
  keywords = {!jw},
  file = {/Users/jp/Zotero/storage/W5IXA6MZ/BerinskyHL12--JW--evaluating_online_labor_markets_for_experimental_research_amazoncoms_mechanical_turk.pdf}
}
% == BibTeX quality report for BerinskyHL12:
% ? unused Journal abbreviation ("Polit. anal.")
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Url ("https://www.cambridge.org/core/product/identifier/S1047198700013875/type/journal_article")

@misc{fasttext,
  doi = {10.48550/ARXIV.1607.04606},
  url = {https://arxiv.org/abs/1607.04606},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Enriching Word Vectors with Subword Information},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{PAWSDataset,
  doi = {10.48550/ARXIV.1904.01130},
  url = {https://arxiv.org/abs/1904.01130},
  author = {Zhang, Yuan and Baldridge, Jason and He, Luheng},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {PAWS: Paraphrase Adversaries from Word Scrambling},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{mscoco,
  doi = {10.48550/ARXIV.1405.0312},
  url = {https://arxiv.org/abs/1405.0312},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Microsoft COCO: Common Objects in Context},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ParaSCI,
  doi = {10.48550/ARXIV.2101.08382},
  url = {https://arxiv.org/abs/2101.08382},
  author = {Dong, Qingxiu and Wan, Xiaojun and Cao, Yue},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ParaSCI: A Large Scientific Paraphrase Dataset for Longer Paraphrase Generation},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{WahleRFM22,
	title        = {Identifying Machine-Paraphrased Plagiarism},
	author       = {Wahle, Jan Philip and Ruas, Terry and Folt{\'y}nek, Tom{\'a}{\v{s}} and Meuschke, Norman and Gipp, Bela},
	year         = 2022,
	booktitle    = {Information for a Better World: Shaping the Global Future},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {393--413},
	isbn         = {978-3-030-96957-8},
	editor       = {Smits, Malte},
	doi			 = {10.1007/978-3-030-96957-8_34},
	abstract     = {Employing paraphrasing tools to conceal plagiarized text is a severe threat to academic integrity. To enable the detection of machine-paraphrased text, we evaluate the effectiveness of five pre-trained word embedding models combined with machine learning classifiers and state-of-the-art neural language models. We analyze preprints of research papers, graduation theses, and Wikipedia articles, which we paraphrased using different configurations of the tools SpinBot and SpinnerChief. The best performing technique, Longformer, achieved an average F1 score of 80.99{\%} (F1 = 99.68{\%} for SpinBot and F1 = 71.64{\%} for SpinnerChief cases), while human evaluators achieved F1 = 78.4{\%} for SpinBot and F1 = 65.6{\%} for SpinnerChief cases. We show that the automated classification alleviates shortcomings of widely-used text-matching systems, such as Turnitin and PlagScan.}
}

@incollection{FoltynekRSM20a,
  title = {Detecting {{Machine-Obfuscated Plagiarism}}},
  booktitle = {Sustainable {{Digital Communities}}},
  author = {Folt{\'y}nek, Tom{\'a}{\v s} and Ruas, Terry and Scharpf, Philipp and Meuschke, Norman and Schubotz, Moritz and Grosky, William and Gipp, Bela},
  editor = {Sundqvist, Anneli and Berget, Gerd and Nolin, Jan and Skjerdingstad, Kjell Ivar},
  year = {2020},
  volume = {12051},
  pages = {816--827},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-43687-2_68},
  urldate = {2020-12-18},
  isbn = {978-3-030-43686-5 978-3-030-43687-2},
  langid = {english},
  keywords = {!tr_author,nlp,paraphrase,plagiarism},
  annotation = {http://link.springer.com/10.1007/978-3-030-43687-2\_68},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\CAWJ6U29\\FoltynekRSM20a--tr--detecting_machine-obfuscated_plagiarism.pdf}
}

@article{GreinerPetterRSA19a,
  title = {Why {{Machines Cannot Learn Mathematics}}, {{Yet}}},
  author = {{Greiner-Petter}, Andr{\'e} and Ruas, Terry and Schubotz, Moritz and Aizawa, Akiko and Grosky, William and Gipp, Bela},
  year = {2019},
  month = may,
  journal = {arXiv:1905.08359 [cs]},
  eprint = {1905.08359},
  primaryclass = {cs},
  urldate = {2020-12-18},
  abstract = {Nowadays, Machine Learning (ML) is seen as the universal solution to improve the effectiveness of information retrieval (IR) methods. However, while mathematics is a precise and accurate science, it is usually expressed by less accurate and imprecise descriptions, contributing to the relative dearth of machine learning applications for IR in this domain. Generally, mathematical documents communicate their knowledge with an ambiguous, context-dependent, and non-formal language. Given recent advances in ML, it seems canonical to apply ML techniques to represent and retrieve mathematics semantically. In this work, we apply popular text embedding techniques to the arXiv collection of STEM documents and explore how these are unable to properly understand mathematics from that corpus. In addition, we also investigate the missing aspects that would allow mathematics to be learned by computers.},
  archiveprefix = {arxiv},
  keywords = {!tr_author,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Digital Libraries,Computer Science - Information Retrieval,machine_learning,math,nlp},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\L7K27LRB\\GreinerPetterRSA19a--tr--why_machines_cannot_learn_mathematics_yet.pdf;C\:\\Users\\ruast\\Zotero\\storage\\74L8LFT7\\1905.html}
}

@article{GroskyR17,
  title = {The Continuing Reinvention of Content-Based Retrieval: {{Multimedia}} Is Not Dead},
  author = {Grosky, William I. and Ruas, Terry L.},
  year = {2017},
  month = jan,
  journal = {IEEE MultiMedia},
  volume = {24},
  number = {1},
  pages = {6--11},
  issn = {1070-986X},
  doi = {10.1109/mmul.2017.7},
  keywords = {!tr_author,multimedia},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\M294YWMX\\GroskyR17--tr--the_continuing_reinvention_of_content-based_retrieval_multimedia_is_not_dead.pdf}
}

@inproceedings{OstendorffARG21a,
  title = {Evaluating Document Representations for Content-Based Legal Literature Recommendations},
  booktitle = {Proceedings of the {{Eighteenth International Conference}} on {{Artificial Intelligence}} and {{Law}}},
  author = {Ostendorff, Malte and Ash, Elliott and Ruas, Terry and Gipp, Bela and {Moreno-Schneider}, Julian and Rehm, Georg},
  year = {2021},
  month = jun,
  pages = {109--118},
  publisher = {{ACM}},
  address = {{S\~ao Paulo Brazil}},
  doi = {10.1145/3462757.3466073},
  urldate = {2021-08-31},
  isbn = {978-1-4503-8526-8},
  langid = {english},
  keywords = {!tr_author,doc_classification,legal,nlp},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\9CUTYD5R\\OstendorffARG21a--tr--evaluating_document_representations_for_content-based_legal_literature_recommendations.pdf}
}

@inproceedings{OstendorffBRG22,
  title = {Specialized {{Document Embeddings}} for {{Aspect-Based Similarity}} of {{Research Papers}}},
  booktitle = {Proceedings of the 22nd {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}}},
  author = {Ostendorff, Malte and Blume, Till and Ruas, Terry and Gipp, Bela and Rehm, Georg},
  year = {2022},
  series = {{{JCDL}} '22},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3529372.3530912},
  abstract = {Document embeddings and similarity measures underpin content-based recommender systems, whereby a document is commonly represented as a single generic embedding. However, similarity computed on single vector representations provides only one perspective on document similarity that ignores which aspects make two documents alike. To address this limitation, aspect-based similarity measures have been developed using document segmentation or pairwise multi-class document classification. While segmentation harms the document coherence, the pairwise classification approach scales poorly to large scale corpora. In this paper, we treat aspect-based similarity as a classical vector similarity problem in aspect-specific embedding spaces. We represent a document not as a single generic embedding but as multiple specialized embeddings. Our approach avoids document segmentation and scales linearly w.r.t. the corpus size. In an empirical study, we use the Papers with Code corpus containing 157, 606 research papers and consider the task, method, and dataset of the respective research papers as their aspects. We compare and analyze three generic document embeddings, six specialized document embeddings and a pairwise classification baseline in the context of research paper recommendations. As generic document embeddings, we consider FastText, SciBERT, and SPECTER. To compute the specialized document embeddings, we compare three alternative methods inspired by retrofitting, fine-tuning, and Siamese networks. In our experiments, Siamese SciBERT achieved the highest scores. Additional analyses indicate an implicit bias of the generic document embeddings towards the dataset aspect and against the method aspect of each research paper. Our approach of aspect-based document embeddings mitigates potential risks arising from implicit biases by making them explicit. This can, for example, be used for more diverse and explainable recommendations.},
  articleno = {7},
  isbn = {978-1-4503-9345-4},
  keywords = {aspect-based similarity,content-based recommender systems,document embeddings,document similarity,papers with code},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\WMIZY8MB\\10.11453529372.3530912--tr--specialized_document_embeddings_for_aspect-based_similarity_of_research_papers.pdf}
}

@inproceedings{OstendorffRBG20,
  title = {Aspect-Based {{Document Similarity}} for {{Research Papers}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Ostendorff, Malte and Ruas, Terry and Blume, Till and Gipp, Bela and Rehm, Georg},
  year = {2020},
  pages = {6194--6206},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.545},
  urldate = {2021-04-29},
  langid = {english},
  keywords = {!tr_author,dataset,doc_classification,nlp},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\BB3WJ5HI\\OstendorffRBG20--tr--aspect-based_document_similarity_for_research_papers.pdf}
}

@inproceedings{OstendorffRSR20,
  title = {Pairwise {{Multi-Class Document Classification}} for {{Semantic Relations}} between {{Wikipedia Articles}}},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} in 2020},
  author = {Ostendorff, Malte and Ruas, Terry and Schubotz, Moritz and Rehm, Georg and Gipp, Bela},
  year = {2020},
  month = aug,
  pages = {127--136},
  publisher = {{ACM}},
  address = {{Virtual Event China}},
  doi = {10.1145/3383583.3398525},
  urldate = {2021-10-20},
  isbn = {978-1-4503-7585-6},
  langid = {english},
  keywords = {!tr_author,doc_classification,nlp},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\B4K23ASZ\\OstendorffRSR20--tr--pairwise_multi-class_document_classification_for_semantic_relations_between_wikipedia.pdf}
}

@article{RuasBM09,
  title = {Multi-Agent Systems in Modeling and Simulation of Fire Spread},
  author = {Ruas, Terry and Batista, Andr{\'e} F. M. and Marietto, Maria G. B.},
  year = {2009},
  keywords = {!tr_author,⛔ No DOI found,mssa},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\YAGSI7U5\\RuasBM09--tr--multi-agent_systems_in_modeling_and_simulation_of_fire_spread.pdf}
}

@article{RuasFGF20,
  title = {Enhanced Word Embeddings Using Multi-Semantic Representation through Lexical Chains},
  author = {Ruas, Terry and Ferreira, Charles P. H. and Gorsky, William and Fran{\c c}a, Fabr{\'i}cio O. and Medeiros, D{\'e}bora M. R.},
  year = {2020},
  journal = {Information Sciences},
  volume = {532},
  pages = {16--32},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2020.04.048},
  keywords = {!tr_author,doc_classification,lexical_chains},
  annotation = {https://www.sciencedirect.com/science/article/pii/S0020025520303911},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\6QCELKQQ\\RuasFGF20--tr--enhanced_word_embeddings_using_multi-semantic_representation_through_lexical_chains.pdf}
}

@inproceedings{RuasG17,
  title = {Text Similarity Using Multilevel Fixed Lexical Chains},
  booktitle = {Brazilian Graduate Student Conference ({{BRASCON}})},
  author = {Ruas, Terry and Grosky, William},
  year = {2017},
  address = {{Los Angeles, USA}},
  issn = {2472-3894},
  keywords = {!tr_author,⛔ No DOI found,doc_classification,lexical_chains,nlp}
}

@inproceedings{RuasG17a,
  title = {Keyword Extraction through Contextual Semantic Analysis of Documents},
  booktitle = {Proceedings of the 9th International Conference on Management of Digital {{EcoSystems}}},
  author = {Ruas, Terry and Grosky, William},
  year = {2017},
  series = {{{MEDES}} '17},
  pages = {150--156},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3167020.3167043},
  isbn = {978-1-4503-4895-9}
}

@techreport{RuasG17b,
  title = {Exploring and Expanding the Use of Lexical Chains in Information Retrieval},
  author = {Ruas, Terry and Grosky, William},
  year = {2017},
  pages = {6},
  address = {{Michigan}},
  institution = {{University of Michigan - Dearborn}},
  doi = {10.3998/2027.42/136659},
  keywords = {!tr_author,ir,lexical_chains,nlp},
  annotation = {http://hdl.handle.net/2027.42/136659},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\WQF9JLZC\\RuasG17b--tr--exploring_and_expanding_the_use_of_lexical_chains_in_information_retrieval.pdf}
}

@inproceedings{RuasG18a,
  title = {Semantic {{Feature Structure Extraction From Documents Based}} on {{Extended Lexical Chains}}},
  booktitle = {Proceedings of the 9th {{Global Wordnet Conference}}},
  author = {Ruas, Terry and Grosky, William},
  year = {2018},
  month = jan,
  pages = {87--96},
  publisher = {{Global Wordnet Association}},
  address = {{Nanyang Technological University (NTU), Singapore}},
  abstract = {The meaning of a sentence in a document is more easily determined if its constituent words exhibit cohesion with respect to their individual semantics. This paper explores the degree of cohesion among a document's words using lexical chains as a semantic representation of its meaning. Using a combination of diverse types of lexical chains, we develop a text document representation that can be used for semantic document retrieval. For our approach, we develop two kinds of lexical chains: (i) a multilevel flexible chain representation of the extracted semantic values, which is used to construct a fixed segmentation of these chains and constituent words in the text; and (ii) a fixed lexical chain obtained directly from the initial semantic representation from a document. The extraction and processing of concepts is performed using WordNet as a lexical database. The segmentation then uses these lexical chains to model the dispersion of concepts in the document. Representing each document as a high-dimensional vector, we use spherical k-means clustering to demonstrate that our approach performs better than previous techniques.},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\9I5TPSGR\\ruas-grosky-2018-semantic--tr--semantic_feature_structure_extraction_from_documents_based_on_extended_lexical_chains.pdf}
}

@article{RuasGA19,
  title = {Multi-Sense Embeddings through a Word Sense Disambiguation Process},
  author = {Ruas, Terry and Gorsky, William and Aizawa, Akiko},
  year = {2019},
  journal = {Expert Systems with Applications},
  volume = {136},
  pages = {288--303},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2019.06.026},
  keywords = {!tr_author,embeddings,nlp,wsd},
  annotation = {http://www.sciencedirect.com/science/article/pii/S0957417419304269},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\H4MZISQ3\\RuasGA19--tr--multi-sense_embeddings_through_a_word_sense_disambiguation_process.pdf}
}

@misc{RuasWKM22,
  title = {{{CS-Insights}}: {{A System}} for {{Analyzing Computer Science Research}}},
  shorttitle = {{{CS-Insights}}},
  author = {Ruas, Terry and Wahle, Jan Philip and K{\"u}ll, Lennart and Mohammad, Saif M. and Gipp, Bela},
  year = {2022},
  month = oct,
  number = {arXiv:2210.06878},
  eprint = {arXiv:2210.06878},
  publisher = {{arXiv}},
  urldate = {2022-10-14},
  abstract = {This paper presents CS-Insights, an interactive web application to analyze computer science publications from DBLP through multiple perspectives. The dedicated interfaces allow its users to identify trends in research activity, productivity, accessibility, author's productivity, venues' statistics, topics of interest, and the impact of computer science research on other fields. CS-Insightsis publicly available, and its modular architecture can be easily adapted to domains other than computer science.},
  archiveprefix = {arxiv},
  keywords = {!tr_author,Computer Science - Computation and Language,Computer Science - Digital Libraries,nlp_demo,nlp_scientometric},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\DG3RNSQU\\RuasWKM22--tr--cs-insights_a_system_for_analyzing_computer_science_research.pdf;C\:\\Users\\ruast\\Zotero\\storage\\RE6NJUUQ\\2210.html}
}

@inproceedings{SpindeKRM22,
  title = {Exploiting {{Transformer-Based Multitask Learning}} for the {{Detection}} of {{Media Bias}} in {{News Articles}}},
  booktitle = {Information for a {{Better World}}: {{Shaping}} the {{Global Future}}},
  author = {Spinde, Timo and Krieger, Jan-David and Ruas, Terry and Mitrovi{\'c}, Jelena and {G{\"o}tz-Hahn}, Franz and Aizawa, Akiko and Gipp, Bela},
  editor = {Smits, Malte},
  year = {2022},
  volume = {13192},
  pages = {225--235},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-96957-8_20},
  urldate = {2022-03-04},
  isbn = {978-3-030-96956-1 978-3-030-96957-8},
  langid = {english},
  keywords = {!tr_author,media_bias,nlp},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\AIHZH2AB\\SpindeKRM22--tr--exploiting_transformer-based_multitask_learning_for_the_detection_of_media_bias_in.pdf}
}

@inproceedings{SpindePKR21,
  title = {Neural {{Media Bias Detection Using Distant Supervision With BABE}} - {{Bias Annotations By Experts}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Spinde, Timo and Plank, Manuel and Krieger, Jan-David and Ruas, Terry and Gipp, Bela and Aizawa, Akiko},
  year = {2021},
  pages = {1166--1177},
  publisher = {{Association for Computational Linguistics}},
  address = {{Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.findings-emnlp.101},
  urldate = {2022-01-07},
  langid = {english},
  keywords = {!tr_author,dataset,media_bias,nlp},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\2SUS6RQU\\SpindePKR21--tr--neural_media_bias_detection_using_distant_supervision_with_babe_-_bias_annotations.pdf;C\:\\Users\\ruast\\Zotero\\storage\\6RUZYCNH\\Spinde2021f--tr--neural_media_bias_detection_using_distant_supervision_with_babe_-_bias_annotations.pdf}
}

@incollection{WahleARM22b,
  title = {Testing the {{Generalization}} of {{Neural Language Models}} for {{COVID-19 Misinformation Detection}}},
  booktitle = {Information for a {{Better World}}: {{Shaping}} the {{Global Future}}},
  author = {Wahle, Jan Philip and Ashok, Nischal and Ruas, Terry and Meuschke, Norman and Ghosal, Tirthankar and Gipp, Bela},
  editor = {Smits, Malte},
  year = {2022},
  volume = {13192},
  pages = {381--392},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-96957-8_33},
  urldate = {2022-11-11},
  abstract = {A drastic rise in potentially life-threatening misinformation has been a by-product of the COVID-19 pandemic. Computational support to identify false information within the massive body of data on the topic is crucial to prevent harm. Researchers proposed many methods for flagging online misinformation related to COVID-19. However, these methods predominantly target specific content types (e.g., news) or platforms (e.g., Twitter). The methods' capabilities to generalize were largely unclear so far. We evaluate fifteen Transformer-based models on five COVID-19 misinformation datasets that include social media posts, news articles, and scientific papers to fill this gap. We show tokenizers and models tailored to COVID-19 data do not provide a significant advantage over general-purpose ones. Our study provides a realistic assessment of models for detecting COVID-19 misinformation. We expect that evaluating a broad spectrum of datasets and models will benefit future research in developing misinformation detection systems.},
  isbn = {978-3-030-96956-1 978-3-030-96957-8},
  langid = {english},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\VJGM7QZF\\WahleARM22b--tr--testing_the_generalization_of_neural_language_models_for_covid-19_misinformation.pdf}
}

@inproceedings{WahleRKG22a,
    title = "How Large Language Models are Transforming Machine-Paraphrase Plagiarism",
    author = "Wahle, Jan Philip  and
      Ruas, Terry  and
      Kirstein, Frederic  and
      Gipp, Bela",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.62",
    pages = "952--963",
    abstract = "The recent success of large language models for text generation poses a severe threat to academic integrity, as plagiarists can generate realistic paraphrases indistinguishable from original work.However, the role of large autoregressive models in generating machine-paraphrased plagiarism and their detection is still incipient in the literature.This work explores T5 and GPT3 for machine-paraphrase generation on scientific articles from arXiv, student theses, and Wikipedia.We evaluate the detection performance of six automated solutions and one commercial plagiarism detection software and perform a human study with 105 participants regarding their detection performance and the quality of generated examples.Our results suggest that large language models can rewrite text humans have difficulty identifying as machine-paraphrased (53{\%} mean acc.).Human experts rate the quality of paraphrases generated by GPT-3 as high as original texts (clarity 4.0/5, fluency 4.2/5, coherence 3.8/5).The best-performing detection model (GPT-3) achieves 66{\%} F1-score in detecting paraphrases.We make our code, data, and findings publicly available to facilitate the development of detection solutions.",
}

@inproceedings{WahleRMG21,
	title        = {Are Neural Language Models Good Plagiarists? A Benchmark for Neural Paraphrase Detection},
	author       = {Wahle, Jan Philip and Ruas, Terry and Meuschke, Norman and Gipp, Bela},
	year         = 2021,
	booktitle    = {2021 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
	volume       = {},
	number       = {},
	pages        = {226--229},
	doi          = {10.1109/JCDL52503.2021.00065}
}

@article{WahleRMG21a,
  title={Incorporating Word Sense Disambiguation in Neural Language Models},
  author={Wahle, Jan Philip and Ruas, Terry and Meuschke, Norman and Gipp, Bela},
  journal={arXiv preprint arXiv:2106.07967},
  year={2021}
}

@inproceedings{WahleRMG22a,
  title = {D3: {{A Massive Dataset}} of {{Scholarly Metadata}} for {{Analyzing}} the {{State}} of {{Computer Science Research}}},
  booktitle = {Proceedings of the {{Thirteenth Language Resources}} and {{Evaluation Conference}}},
  author = {Wahle, Jan Philip and Ruas, Terry and Mohammad, Saif and Gipp, Bela},
  year = {2022},
  month = jun,
  pages = {2642--2651},
  publisher = {{European Language Resources Association}},
  address = {{Marseille, France}},
  abstract = {DBLP is the largest open-access repository of scientific articles on computer science and provides metadata associated with publications, authors, and venues. We retrieved more than 6 million publications from DBLP and extracted pertinent metadata (e.g., abstracts, author affiliations, citations) from the publication texts to create the DBLP Discovery Dataset (D3). D3 can be used to identify trends in research activity, productivity, focus, bias, accessibility, and impact of computer science research. We present an initial analysis focused on the volume of computer science research (e.g., number of papers, authors, research activity), trends in topics of interest, and citation patterns. Our findings show that computer science is a growing research field (15\% annually), with an active and collaborative researcher community. While papers in recent years present more bibliographical entries in comparison to previous decades, the average number of citations has been declining. Investigating papers' abstracts reveals that recent topic trends are clearly reflected in D3. Finally, we list further applications of D3 and pose supplemental research questions. The D3 dataset, our findings, and source code are publicly available for research purposes.},
  file = {C\:\\Users\\ruast\\Zotero\\storage\\2YPLBAPX\\WahleRMG22a--tr--d3_a_massive_dataset_of_scholarly_metadata_for_analyzing_the_state_of_computer_science.pdf}
}

@article{WahleRMM23,
  title={AI Usage Cards: Responsibly Reporting AI-generated Content},
  author={Wahle, Jan Philip and Ruas, Terry and Mohammad, Saif M and Meuschke, Norman and Gipp, Bela},
  journal={arXiv preprint arXiv:2303.03886},
  year={2023}
}

@article{zhang2021aspect,
  title={Aspect sentiment quad prediction as paraphrase generation},
  author={Zhang, Wenxuan and Deng, Yang and Li, Xin and Yuan, Yifei and Bing, Lidong and Lam, Wai},
  journal={arXiv preprint arXiv:2110.00796},
  year={2021}
}

@inproceedings{kirstein-etal-2022-analyzing,
    title = "Analyzing Multi-Task Learning for Abstractive Text Summarization",
    author = "Kirstein, Frederic Thomas  and
      Wahle, Jan Philip  and
      Ruas, Terry  and
      Gipp, Bela",
    booktitle = "Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.gem-1.5",
    pages = "54--77",
    abstract = "Despite the recent success of multi-task learning and pre-finetuning for natural language understanding, few works have studied the effects of task families on abstractive text summarization. Task families are a form of task grouping during the pre-finetuning stage to learn common skills, such as reading comprehension. To close this gap, we analyze the influence of multi-task learning strategies using task families for the English abstractive text summarization task. We group tasks into one of three strategies, i.e., sequential, simultaneous, and continual multi-task learning, and evaluate trained models through two downstream tasks. We find that certain combinations of task families (e.g., advanced reading comprehension and natural language inference) positively impact downstream performance. Further, we find that choice and combinations of task families influence downstream performance more than the training scheme, supporting the use of task families for abstractive text"
}
