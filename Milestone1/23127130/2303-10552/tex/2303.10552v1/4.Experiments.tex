\section{Experiments}\label{sec: experiments}
\begin{table*}[ht]
\small
\centering
\caption{\textbf{VIC3D Object Detection Results with FFNet and Different Fusion Methods.} $\mathcal{AB}$ denotes the average byte used to measure the transmission cost. "/" denotes no latency for non-fusion methods. "-" denotes no information provided. FFNet outperforms non-fusion methods by up to 10.90\% mAP@BEV (IoU=0.5) and 10.96\% mAP@BEV (IoU=0.5) in 100$ms$ and 200$ms$ latency, respectively. And FFNet outperforms all other middle-fusion methods including DiscoNet and V2VNet while requiring the same transmission cost. FFNet also outperforms all other fusion methods when the latency reaches 200$ms$. Notably, FFNet achieves more than 2\% mAP@BEV (IoU=0.5) improvement over early fusion in 200$ms$ latency while requiring no more than $1/10$ transmission cost.
}\label{tab: experiment performance comparison}
\scalebox{1.0}{
\begin{tabular}{ccc|lcllc}
\hline
\hline
\multirow{2}{*}{Model} & \multirow{2}{*}{FusionType} & \multirow{2}{*}{Latency ($ms$)} & \multicolumn{2}{c}{mAP@3D~$\uparrow$}                & \multicolumn{2}{c}{mAP@BEV~$\uparrow$} & \multirow{2}{*}{$\mathcal{AB}$~(Byte)~$\downarrow$} \\ \cline{4-7} 
&   &  & IoU=0.5 & \multicolumn{1}{l}{IoU=0.7} & IoU=0.5    & IoU=0.7    &    \\
\hline \hline
PointPillars~\cite{lang2019pointpillars}  & non-fusion & /              & 48.06   & -                   & 52.24      & -      & 0    \\
\hline \hline
Early Fusion  & early & 100              & 57.35   & -                   & 64.06      & -      & 1.4$\times 10^6$    \\
TCLF~\cite{yu2022dairv2x}     & late     & 100              & 40.79   & -                   & 46.80      & -      & 5.4$\times 10^2$     \\
DiscoNet~\cite{li2021learning}  & middle    & 100              & 52.83   & 29.19                  & 61.25      & 50.11      & 1.2$\times 10^5$   \\
V2VNet~\cite{wang2020v2vnet}    & middle    & 100              & 52.02   & 28.54                   &  60.78     & 50.02      & 1.2$\times 10^5$   \\
\textbf{FFNet (Ours)}   & middle & 100   & 55.48   & 31.50                    & \textbf{63.14} (\textcolor{red}{+10.90})     & 54.28      & 1.2$\times 10^5$   
\\ \hline \hline                       
Early Fusion & early & 200              & 54.63   & -                   & 61.08      & -      & 1.4$\times 10^6$     \\
TCLF~\cite{yu2022dairv2x}     & late     & 200              & 36.72   & -                   & 41.67      & -      & 5.1$\times 10^2$     \\
DiscoNet~\cite{li2021learning}  & middle    & 200              & 50.76   & 28.57                  & 58.20      & 48.90      & 1.2$\times 10^5$   \\
V2VNet~\cite{wang2020v2vnet}    & middle    & 200              & 49.67   & 26.96                   & 56.02      & 46.32      & 1.2$\times 10^5$   \\
\textbf{FFNet (Ours)}   & middle & 200 & 55.37   & 31.66                   & \textbf{63.20} (\textcolor{red}{+10.96})      & 54.69      & 1.2$\times 10^5$       \\ \hline \hline
\end{tabular}
}
\label{table_xiaorong}
\end{table*}

In this section, we present our approach to solve VIC3D on the DAIR-V2X dataset~\cite{yu2022dairv2x} using the FFNet and various fusion methods. We compare the experimental results of different fusion methods on various latency. Our proposed FFNet outperforms all other methods, including early fusion, when the latency reaches 200$ms$, while only requiring a transmission cost of no more than 1/10 of that of early fusion.
Furthermore, we investigate how the FFNet overcomes the challenge of temporal asynchrony with our proposed feature flow. Our experimental results show that temporal asynchrony can significantly reduce the performance of the cooperative detection model, but the feature flow can effectively compensate for this drop.
Additionally, we evaluate the robustness of the FFNet to uncertain latency by testing it on different latency. The experimental results show that our FFNet can robustly solve the problem of uncertain latency.
In the Appendix, we compare the performance of the feature flow extraction on different sides, i.e., the infrastructure side and the ego vehicle side. We develop our models using MMDetection3D~\cite{mmdet3d2020}.

\subsection{DAIR-V2X Dataset}
DAIR-V2X~\cite{yu2022dairv2x} is a large-scale vehicle-infrastructure cooperative perception dataset consisting of real-world images and point clouds.
With over 100 scenes and 18,000 data pairs, this dataset captures the infrastructure and vehicle sensors simultaneously at an equipped intersection when an autonomous driving vehicle passes through. 
All frames in this dataset are annotated with 10 3D object classes. 
Furthermore, cooperative 3D annotations with vehicle-infrastructure cooperative view are provided for 9,311 pairs, where each object is labeled with its corresponding category (\textit{Car}, \textit{Bus}, \textit{Truck}, or \textit{Van}). 
The dataset is divided into \textit{train/val/test} sets in a 5:2:3 ratio, with all models evaluated on the \textit{val} set.

\subsection{Comparison with Different Fusion Methods}\label{sec: exp-FFNet}
We compare our proposed FFNet with four different fusion methods: non-fusion (e.g., PointPillars~\cite{lang2019pointpillars}), early fusion, late fusion (e.g., TCLF~\cite{yu2022dairv2x}), middle fusion (e.g., DiscoNet~\cite{li2021learning}) and V2VNet~\cite{li2022v2x}. 
The performance of FFNet and these fusion methods are evaluated under 100$ms$ and 200$ms$ latency, respectively, on the DAIR-V2X dataset~\cite{yu2022dairv2x}. 
The detection performance is measured using KITTI~\cite{geiger2012we} evaluation detection metrics: bird-eye view (BEV) mAP and 3D mAP with 0.5 IoU and 0.7 IoU, respectively. Only objects located in the rectangular area [0, -39.12, 100, 39.12] are considered in the metrics, and only the \textit{Car} class is taken into account. 
Implementation details of FFNet and the fusion methods are provided in the Appendix. Evaluation results are presented in Tab.~\ref{tab: experiment performance comparison}.

\paragraph{Result Analysis.}
Firstly, our proposed FFNet outperforms the non-fusion method PointPillars by 10.90\% BEV-mAP (IoU=0.5) and 10.96\% BEV-mAP (IoU=0.5) in 100$ms$ and 200$ms$ latency, respectively. This result indicates that utilizing infrastructure data can improve 3D detection performance.
Secondly, although late fusion requires little transmission cost, the BEV-mAP (IoU=0.5) of TCLF is much lower than that of FFNet, up to 21.53\% in 200$ms$ latency.
Thirdly, compared with early fusion methods, FFNet achieves similar detection performance in 100$ms$ latency and outperforms more than 2\% mAP in 200$ms$ latency, while it only requires no more than 1/10 transmission cost.
Fourthly, our FFNet achieves the best detection performance with the exact transmission cost as the middle fusion methods. For example, FFNet surpasses DiscoNet by 1.89\% BEV-mAP (IoU=0.5) and 5.0\% BEV-mAP (IoU=0.5) in 100$ms$ and 200$ms$ latency, respectively.
In summary, our proposed FFNet achieves new SOTA on DAIR-V2X when the latency reaches 200$ms$, while only requiring no more than 1/10 transmission cost of raw data. Moreover, it not only surpasses all other intermediate-level methods but also surpasses the early fusion.

\begin{table*}
\small
\centering
\caption{\textbf{Comparison between with and without Feature Prediction.} Compared with no prediction models, FFNet with feature prediction has a significantly lower performance drop when there is communication latency.}\label{tab: ablation study of feature prediction}
\scalebox{1.0}{
\begin{tabular}{cc|lcllc}
\hline
\hline
\multirow{2}{*}{Model}  & \multirow{2}{*}{Latency (ms)} & \multicolumn{2}{c}{mAP@3D~$\uparrow$}                & \multicolumn{2}{c}{mAP@BEV~$\uparrow$} & \multirow{2}{*}{AB~(Byte)~$\downarrow$} \\ \cline{3-6} 
 &  & IoU=0.5 & \multicolumn{1}{l}{IoU=0.7} & IoU=0.5    & IoU=0.7    &    \\
\hline \hline
FFNet   & 0              & 55.81   & 30.23                  & 63.54      & 54.16      & 1.2$\times 10^5$    \\
FFNet (without prediction)         & 0              & 55.81   & 30.23                  & 63.54      & 54.16     & 6.2$\times 10^4$     \\
FFNet-V2 (without prediction)      & 0              & 55.78   & 30.22                  & 64.23      & 55.00      & 1.2$\times 10^5$   \\
\hline \hline
FFNet   & 200              & 55.37   & 31.66                   & 63.20 (\textcolor{red}{-0.34})       & 54.69      & 1.2$\times 10^5$    \\
FFNet (without prediction)         & 200              & 50.27   & 27.57                  & 57.93 (\textcolor{red}{-5.61})      & 48.16     & 6.2$\times 10^4$     \\
FFNet-V2 (without prediction)      & 200              & 49.90   & 27.33                  & 58.00 (\textcolor{red}{-6.23})      & 48.22      & 1.2$\times 10^5$   \\
\hline \hline
\end{tabular}
}
\label{table_xiaorong}
\end{table*}

\subsection{Ablation Study}\label{sec: ablation study}
We conducted a series of experiments to demonstrate the effectiveness of the feature flow module in overcoming the temporal asynchrony challenge, as well as to show that FFNet performs robustly under various latencies.

\paragraph{Feature prediction can well solve temporal asynchrony.}
We first evaluated the performance of FFNet under 0$ms$ latency and 200$ms$ latency on DAIR-V2X, where 0$ms$ latency indicates no temporal asynchrony between infrastructure data and vehicle data. 
We then removed the prediction module from FFNet to directly fuse the infrastructure feature without temporal compensation, which we refer to as FFNet (without prediction). We evaluated FFNet (without prediction) under both 0$ms$ and 200$ms$ latency. 
Since FFNet (without prediction) does not require the transmission of the first-order derivative of the feature flow, it only requires half the transmission cost of FFNet. To make a fair comparison, we also trained another version of FFNet, called FFNet-V2, which compressed the feature flow from (384, 288, 288) to (384/16, 288/8, 288/8). Thus, FFNet-V2 (without prediction) has the same transmission cost as FFNet. We evaluated FFNet-V2 (without prediction) under both 0$ms$ and 200$ms$ latency as well. The evaluation results are presented in Table~\ref{tab: ablation study of feature prediction}.

As shown in Table~\ref{tab: ablation study of feature prediction}, FFNet (without prediction) and FFNet-V2 (without prediction) both exhibit a significant performance drop when there is a 200$ms$ latency. For example, FFNet (without prediction) experiences a 5.61\% BEV-mAP (IoU=0.5) drop in 200$ms$ latency compared to 0$ms$ latency. Although FFNet-V2 (without prediction) performs slightly better than FFNet and FFNet (without prediction) in 0$ms$ latency, FFNet significantly surpasses FFNet-V2 (without prediction) in 200$ms$ latency. These results demonstrate that temporal asynchrony can significantly impact performance if we directly fuse the infrastructure feature, and that our feature prediction module can effectively compensate for the performance drop caused by temporal asynchrony.
 
\paragraph{FFNet is robust to uncertain latency.}
\begin{figure}
\centering
\vspace{-0.1cm}
\includegraphics[width=0.40\textwidth]{images/Different_communication_delay.pdf}
\caption{\small Average latency vs. detection performance. FFNet-O denotes the FFNet (without prediction). FFNet-V2-O denotes the FFNet-V2 (without prediction). The yellow curve denotes the evaluation results of the FFNet, which behaves robustly across different latencies compared to models without prediction.
}\label{fig: different latency}
\end{figure}
We conducted further experiments to evaluate the performance of FFNet, FFNet (without prediction), and FFNet-V2 (without prediction) under various latency cases, ranging from 100$ms$ to 500$ms$. 
The experiment results are presented in Figure~\ref{fig: different latency}. 
As shown in the figure, both FFNet (without prediction) and FFNet-V2 (without prediction) exhibit continuous performance drops as the latency increases from 100$ms$ to 500$ms$. 
Specifically, in 500$ms$ latency, FFNet (without prediction) and FFNet-V2 (without prediction) exhibit a significant 9.38\% BEV-mAP (IoU=0.5) drop and 9.76\% BEV-mAP (IoU=0.5) drop, respectively. 
In contrast, FFNet shows little performance degradation within 200$ms$ latency and only has a 4.39\% BEV-mAP (IoU=0.5) drop.
These results indicate that FFNet is robust to different latency and can effectively handle the uncertain latency in VIC3D.

Robustness to uncertain latency is crucial because our feature flow could be received by different vehicles with varying latencies, as illustrated in Figure~\ref{fig: uncertain temporal asynchrony} in the Appendix. The feature flow must have the ability to make predictions at an arbitrary future time before being transmitted.