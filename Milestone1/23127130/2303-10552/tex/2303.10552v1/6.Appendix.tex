\section{More Details about VIC3D Object Detection}
\subsection{Uncertain Temporal Asynchrony}
In this section, we elaborate on the challenge of uncertain temporal asynchrony in the context of the VIC3D object detection task, which was briefly introduced in Section~\ref{sec: vic3d detection task}. In practical traffic environments, autonomous driving vehicles encounter limited communication conditions and varying ranges, resulting in the reception of infrastructure data with varying latencies. This introduces uncertainty in the temporal synchronization of the received data. We present a visualization example in Figure~\ref{fig: uncertain temporal asynchrony} to illustrate this challenge.
This uncertainty in temporal asynchrony is characterized by a significant range, which necessitates a flexible prediction strategy. To address this issue, we propose the extraction of feature flow from sequential infrastructure frames. This feature flow can be utilized to predict future features at any arbitrary future time, thereby providing the necessary flexibility to account for the uncertain temporal asynchrony. The predicted feature can then be used for cooperative detection, which can mitigate the fusion errors due to the uncertain temporal asynchrony.
\begin{figure}[ht]
  \centering
  \includegraphics[width=1\linewidth]{images/v2i_main_challenges.pdf}
  \vspace{-10pt}
  \caption{Uncertain Temporal Asynchrony In Vehicle-Infrastructure Cooperative 3D Object Detection.
  (a) The illustration depicts the utilization of infrastructure sensors to enhance the perception ability of autonomous driving vehicles by providing sensor data from different views. For instance, the infrastructure sensor data can be used to perceive objects located in the long-range zone of the vehicle sensor.
  (b) The vehicle-infrastructure cooperative 3D object detection system encounters uncertainty in communication delay $\Delta t$ occurring before the vehicle devices receive the infrastructure data. Due to limited communication conditions and varying ranges, the infrastructure data received by each autonomous driving vehicle exhibits varying latencies ranging from $\Delta t_1$ to $\Delta t_N$.
  }\label{fig: uncertain temporal asynchrony}
\end{figure}

\subsection{Detailed Evaluation Metrics for VIC3D}\label{sec: evaluation metrics}
This part provides a detailed explanation of the evaluation metrics used to assess the performance of the VIC3D object detection algorithm, namely mean Average Precision (mAP) and Average Byte ($\mathcal{AB}$).

\paragraph{Mean Average Precision (mAP).} We use mAP as a metric to evaluate the detection performance, as commonly used in previous works such as~\cite{yu2022dairv2x, everingham2015pascal}. The AP is calculated based on the 11-points interpolated precision-recall curve and is defined as follows:
\begin{equation}
\begin{aligned}
    AP & = \frac{1}{11} \sum_{r\in {0.0,...,1.0}} AP_{r} \
        & = \frac{1}{11} \sum_{r\in {0.0,...,1.0}} P_{interp}(r),
\end{aligned}
\end{equation}
where $P_{interp}(r) = \mathop{max}\limits_{\tilde{r}\geq r}p(\tilde{r})$,
and a prediction is considered positive if the Intersection over Union (IoU) is greater than or equal to 0.5 or 0.7, respectively. 
Here, we remove objects outside the egocentric surroundings, which we define as a rectangular area with coordinates [0, -39.12, 100, 39.12]. Finally, we average the AP across all classes to obtain the mAP.


\paragraph{Average Byte ($\mathcal{AB}$).}
We use $\mathcal{AB}$ as a metric to evaluate the transmission cost. 
In our implementation, we represent each element in the transmitted data using a 32-bit float, and we ignore the transmission cost of calibration files and timestamps.
We now explain how to calculate the transmission cost for each of the three transmission forms:
\begin{itemize}
    \item Calculating the transmission cost of transmitting raw data for early fusion: We represent each point in the point clouds with $(x,y,z, \text{intensity})$, and each point requires four 32-bit floats, equivalent to 16 Bytes. If there are 100,000 points in the point clouds, the $\mathcal{AB}$ of the transmission cost is $1.6\times 10^6$ Bytes.
    \item Calculating the transmission cost of transmitting detection outputs for late fusion: We represent each 3D detection output with $(x, y, z, w, l, h, \theta, \text{confidence})$, and each detection output requires eight 32-bit floats, equivalent to 32 Bytes. If we transmit ten detection outputs per transmission on average, the $\mathcal{AB}$ of the transmission cost is $3.2\times 10^2$ Bytes.
    \item Calculating the transmission cost of transmitting intermediate data for middle fusion: The feature or feature flow is represented with a tensor, and each element is represented by a 32-bit float. If the size of the feature or feature flow is $(100, 100, 100)$, then the $\mathcal{AB}$ of the transmission cost is $100\times 100\times 100\times 4$ Bytes, which is equivalent to $4\times 10^6$ Bytes.
\end{itemize}

\clearpage
\section{Implementation Details of FFNet}
\paragraph{Architecture.}
In this part, we provide a detailed architecture for FFNet, which is composed of the following six main parts.
1) The feature flow generation module. The infrastructure PFNet (Pillar Feature Net) shares the same architecture as PointPillars~\cite{lang2019pointpillars}.
The x, y, and z ranges of the input point cloud are [(0, 92.16), (-46.08, 46.08), (-3, 1)] meters, respectively.
The voxel sizes for x, y, and z are [0.16, 0.16, 4] meters, respectively.
The output shape of the pseudo-images is (64, 576, 576).
Both the infrastructure feature extractor $F_i(\cdot)$ and the estimated first-order derivative generator $\widetilde{F}_i^{'}(\cdot)$ use the same architecture of Backbone and FPN as SECOND~\cite{yan2018second}, with output shapes of [384, 288, 288].
2) The compressor and decompressor. 
The compressor uses four convolutional blocks with strides (2, 1, 2, 2) to compress the features from (384, 288, 288) to (384/32, 288/8, 288/8).
The decompressor uses three deconvolutional blocks with strides (2, 2, 2) to decompress the features back to the original size.
3) Affine transformation module. 
The affine transformation is implemented with the $affine_grid$ function supported in Pytorch.
We ignore the rotation around the x-y plane.
4) Feature fusion module.
The fusion module is a $3\times3$ convolutional block with stride 1 to compress the concatenated feature from (768, 288, 288) to (384, 288, 288).
5) Vehicle feature extractor. 
This extractor has the same configuration as the infrastructure PFNet and the feature extractor.
6) 3D object detection head.
We use a Single Shot Detector (SSD)~\cite{liu2016ssd} to generate the 3D outputs.
The anchor has a width, length, and height of (1.6, 3.9, 1.56) meters, with a z-center of -1.78 meters.
Positive and negative thresholds of matching are 0.6 and 0.45, respectively.

In this part, we provide a detailed architecture for FFNet, which is composed of following six main parts.
(1) The feature flow generation module: The infrastructure PFNet (Pillar Feature Net) shares the same architecture as PointPillars~\cite{lang2019pointpillars}. 
The x, y, and z ranges of the input point cloud are [(0, 92.16), (-46.08, 46.08), (-3, 1)] meters, respectively.
The voxel size of x, y, and z are [0.16, 0.16, 4] meters, respectively.
The output shape of the pseudo-images is (64, 576, 576).
The feature extractor $F_i(\cdot)$ and the estimated first-order derivative generator $\widetilde{F}_i^{'}(\cdot)$ both use the same Backbone and FPN as SECOND~\cite{yan2018second}, with output shapes of [384, 288, 288].
(2) The compressor and decompressor: The compressor uses four convolutional blocks with strides (2, 1, 2, 2) to compress the features from (384, 288, 288) to (384/32, 288/8, 288/8). The decompressor uses three deconvolutional blocks with strides (2, 2, 2) to decompress the features back to the original size.
(3) Affine transformation module: The affine transform is implemented with the $affine_grid$ function supported in Pytorch. Rotation around the x-y plane is ignored.
(4) Feature fusion module: The fusion module is a $3\times3$ convolutional block with stride 1 to compress the concatenated feature from (768, 288, 288) to (384, 288, 288).
(5) Vehicle feature extractor: This extractor has the same configuration as the infrastructure PFNet and feature extractor.
(6) 3D object detection head: A Single Shot Detector (SSD)~\cite{liu2016ssd} is used to generate the 3D outputs. The anchor has a width, length, and height of (1.6, 3.9, 1.56) meters, with a z-center of -1.78 meters. The positive and negative thresholds of matching are 0.6 and 0.45, respectively.

\paragraph{Training Settings.}
We trained the feature fusion base model on the training set of DAIR-V2X for 40 epochs with a learning rate of 0.001 and weight decay of 0.01. 
We randomly selected 11037 frames from the training set of DAIR-V2X and randomly set $k$ from the range [1, 2] to form $\mathcal{D}$. The explanation of $k$ and $\mathcal{D}$ can be found in Sec.~\ref{sec:semi-supervised-learning}. The trained feature fusion base model was used to pretrain FFNet.
We trained the feature flow generator on $D_u$ for 10 epochs, with a learning rate of 0.001 and weight decay of 0.01. All training and evaluation were implemented on an NVIDIA GeForce RTX 3090 GPU.

\clearpage
\section{Implementation Details of V2VNet and DiscoNet for VIC3D}
\paragraph{V2VNet for VIC3D.}
V2VNet~\cite{wang2020v2vnet} is a pioneering work in multi-vehicle cooperative perception, which is the first to transmitting intermediate-level data for cooperative perception without utilizing sequential frames for extracting temporal correlations. In this paper, we apply this approach to solving the VIC3D problem, as shown in Figure~\ref{fig: V2Vnet}. We made two modifications to the V2VNet architecture: (1) we removed multi-vehicle selection and kept only one vehicle settings as the infrastructure settings, and (2) we compressed the features from (384, 288, 288) to (384, 288/8, 288/8) to keep the transmission cost comparable to FFNet. The other modules keep the same configurations as their corresponding modules in FFNet.
We trained the V2VNet model on the training part of the DAIR-V2X dataset for 40 epochs, with a learning rate of 0.001 and weight decay of 0.01. The other training configurations are the same as those used for training FFNet.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{images/V2Vnet.pdf}\label{fig: V2Vnet}
  \caption{\small \textbf{Implementation of V2VNet for Solving VIC3D Problem.} The V2VNet directly transmit the feature generated from the single point cloud, and the fuse it with the vehicle feature. This feature fusion could cause serious fusion error by the temporal asynchrony.}\label{fig: V2Vnet}
\end{figure}

\paragraph{DiscoNet for VIC3D.}
DiscoNet~\cite{li2021learning} was originally developed for cooperative perception among multiple vehicles. 
This method employs a teacher-student paradigm, where cooperative perception with raw data is used as the teacher network to instruct cooperative perception with intermediate data as the student network. To adapt DiscoNet to the VIC3D task, we use an early-fusion network as the teacher network and V2VNet as the student network. We train both the teacher and student models on the training subset of DAIR-V2X for 40 epochs, with a learning rate of 0.001 and weight decay of 0.01. Next, we fine-tune the student network using soft labels generated by the early-fusion network for an additional 10 epochs, with a learning rate of 0.0001 and weight decay of 0.01


\clearpage
\section{Comparison of Feature Flow Extraction on Different Sides}~\label{sec: extraction on different side}
This section discusses the effect of extracting the feature flow on different sides (infrastructure side \textit{vs.} vehicle side).

\paragraph{Experiment Setting.}
To compare the effect of feature flow extraction on infrastructure side and vehicle side, we train a modified FFNet called FFNet-V. The FFNet-V inputs the features $F_i(P_i(t_i-1))$ and $F_i(P_i(t_i-1))$ produced from consecutive infrastructure frames to generate the feature flow on vehicle devices.
We first concatenates the two received features and feeds them into a first-order derivative generator to generate the estimated first-order derivative of the feature flow $\widetilde{F}_i^{'}(t_i)$.
Then we predict the future feature as Equation~\ref{eq: predict F(t_i+k)}.
In addition, FFNet-V shares the same architecture modules and training configuration as FFNet.
The FFNet-V implementation framework is shown in Figure~\ref{fig: FFNet-v}.
To ensure a fair comparison with FFNet, we also train another FFNet-V by compressing the feature from (384, 288, 288) to (384/16, 288/8, 288/8), which has the same transmission cost as FFNet. This version of FFNet-V is referred to as FFNet-V (Same-TC). We evaluate both FFNet-V and FFNet-V (Same-TC) under different latencies (100$ms$, 300$ms$, and 500$ms$).
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{images/FFNET-OVERVIEW-v.pdf}
  \vspace{-20pt}
  \caption{\small FFNet-V Overview. FFNet-V generates the feature flow on vehicle devices.}
\label{fig: FFNet-v}
\end{figure}

\paragraph{Result Analysis.}
Table~\ref{tab: flow extraction comparison} shows that FFNet-V, FFNet-V (Same-TC), and FFNet outperform FFNet-O under different latencies. This indicates that all these methods can reduce the detection performance drop caused by temporal asynchrony. 
However, FFNet can compensate for more performance drop and achieve better performance than FFNet-V and FFNet-V (Same-TC). 
Specifically, FFNet outperforms FFNet-V (Same-TC) by more than 3\% mAP@BEV (IoU=0.5) in 300$ms$ latency, while having the same transmission cost. The results demonstrate that extracting feature flow from raw sequential frames on infrastructure can improve the VIC3D detection performance more effectively than extracting feature flow from intermediate sequential feature frames on vehicle. 
Moreover, FFNet requires much fewer ego-vehicle computing resources, and the computing cost complexity (CCC) is only O(N), since the feature flow has already been produced on infrastructure devices and does not need to be generated on vehicle devices again. In contrast, FFNet-V consumes computation resources up to O(N) to extract flow from past features. Therefore, FFNet is more computation-friendly to resource-limited vehicle devices. Additionally, extracting feature flow on the vehicle requires much more storage because the feature flow extraction depends on the past frames that the vehicle received. Furthermore, FFNet-V relies heavily on past consecutive frames, so dropped frames can significantly affect the execution and performance. Therefore, FFNet is more storage-friendly to the ego vehicle and more robust to frame dropping.


\clearpage
\begin{table*}[h]
\small
\centering
\caption{\small
\textbf{Extracting Feature Flow on Infrastructure side \textit{vs.} on Vehicle Side.}
FFNet-O denotes the FFNet model without feature prediction.
FFNet-V denotes the model that extracts the feature flow on vehicle.
FFNet-V (Same-TC) denotes the FFNet-V which has the same transmission cost as FFNet.
``AB'' denotes the average byte used to measure the transmission cost. 
``SCC'' indicates the storage cost complexity for the vehicle devices to store the past frames, 
``CCC'' indicates the computing cost complexity for vehicle to extract the feature flow, 
``N'' indicates the number of historical structures to be used.
``/'' indicates the FFNet-O does not need infrastructure transmission and extra computation and storage.
The SCC of FFNet is O(1) because it does not need extra historical frames on vehicle devices. 
At the same time, the SCC of extracting feature flow on vehicle is O(N) because extracting feature flow on vehicle needs past frames received from infrastructure.
Moreover, FFNet achieves better detection performs, and this advantage becomes more pronounced~(+3\% mAP) when latency increases to 300$ms$.}\label{tab: flow extraction comparison}
\scalebox{0.97}{
\begin{tabular}{cc|lcllccc}
\hline
\multirow{2}{*}{Model} & \multirow{2}{*}{Latency (ms)} & \multicolumn{2}{c}{mAP@3D~$\uparrow$}                & \multicolumn{2}{c}{mAP@BEV~$\uparrow$} & \multirow{2}{*}{AB(Byte)~$\downarrow$} & \multirow{2}{*}{SCC~$\downarrow$} & \multirow{2}{*}{CCC~$\downarrow$}  \\ \cline{3-6} 
&    & IoU=0.5 & \multicolumn{1}{l}{IoU=0.7} & IoU=0.5    & IoU=0.7    \\
\hline \hline
FFNet-O            & 100              & 52.18   & 27.99                 & 60.39      & 49.14      & /  & /  & / \\
FFNet-V            & 100              & 53.21   & 28.43                 & 61.50      & 50.50      & 6.2$\times 10^4$  & O(N)  & O(N) \\
FFNet-V (Same-TC)  & 100              & 53.17   & 28.45                 & 62.44      & 51.68      & 1.2$\times 10^5$  & O(N) & O(N) \\
\textbf{FFNet (Ours)}        & 100              & 55.48   & 31.50                   & \textbf{63.14} (\textcolor{red}{+0.7})     & 54.28  & 1.2$\times 10^5$   & O(1) & O(1)
\\ \hline \hline   
FFNet-O            & 300              & 49.03   & 27.39                 & 55.81      & 47.28      & /  & /  & / \\
FFNet-V           & 300              & 50.81   & 28.45                  & 57.75      & 49.62      & 6.2$\times 10^4$  & O(N)  & O(N) \\
FFNet-V (Same-TC) & 300              & 50.5   & 28.25                  & 58.02      & 50.03      & 1.2$\times 10^5$  &  O(N) &  O(N) \\
\textbf{FFNet (Ours)}       & 300              & 53.46   & 30.42                   & \textbf{61.20} (\textcolor{red}{+3.18})    & 52.44  & 1.2$\times 10^5$     & O(1)  & O(1)  \\ \hline \hline
FFNet-O            & 500              & 47.49   & 27.01                 & 54.16      & 45.99      & /  & /  & / \\
FFNet-V            & 500              & 49.93   & 28.63                & 56.42      & 48.87      & 6.2$\times 10^4$ & O(N)  & O(N)  \\
FFNet-V (Same-TC)  & 500              & 49.98   & 27.7                 & 56.99     & 49.55      & 1.2$\times 10^5$ & O(N)  & O(N)  \\
\textbf{FFNet (Ours)}        & 500              & 52.08   & 30.11                   & \textbf{59.13} (\textcolor{red}{+2.14})      & 51.70  & 1.2$\times 10^5$  &  O(1)  &  O(1) \\ \hline \hline
\end{tabular}
}
\label{table_xiaorong}
\end{table*}


\clearpage
\section{Relationship to Other Existing Possible Solutions}\label{sec:relationship-discussion}
Compared with other possible existing solutions, FFNet provides a more applicable paradigm to implement the vehicle-infrastructure cooperative 3D object detection with the following advantages:
\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
    \item Better Performance-Bandwidth Balance: FFNet achieves a better performance-bandwidth balance compared with early fusion and late fusion methods. Unlike early fusion, FFNet transmits compressed intermediate data, thereby reducing transmission costs. Moreover, compared with late fusion methods, FFNet transmits more valuable information for the egocentric object detection.
    \item Overcoming Temporal Asynchrony Challenge: FFNet can address the temporal asynchrony challenge between the data captured from vehicle sensors and received from infrastructure sensors. Compared with middle fusion methods like V2VNet~\cite{wang2020v2vnet} and DiscoNet~\cite{li2021learning}, which only transmit features and do not consider temporal asynchrony problem, FFNet transmits the feature flow with the feature prediction ability. The feature flow can be used to generate future features that are aligned with the vehicle feature, which can help alleviate fusion errors caused by temporal asynchrony. Notably, the first-order derivative in the feature flow is an independent module, and it can also be applied to newer feature fusion methods to achieve lower transmission costs.
    \item Computing-friendly for Vehicles with Limited Computing Resources: Our feature flow is generated on the infrastructure side and can directly predict future features to compensate for uncertain latency with linear computation in ego vehicles. Another possible solution to solve the temporal asynchrony problem is proposed in~\cite{lei2022latency}, which generates future features with received historical features on vehicle devices. However, this solution requires significant computing resources to process the historical frames and extract temporal correlations to predict future features. Furthermore, extracting temporal information from compressed features could be challenging, as compressed features have lost much valuable information compared to raw sequential point clouds.
    \item Saving Annotation Costs: FFNet training saves much annotation costs. We use a self-supervised learning method to train the feature flow generator and extract temporal feature flow from sequential point clouds. Our training method does not rely on labeled data, and it is possible to exploit the massive unlabelled infrastructure-side sequential data in the future.
\end{itemize}

% \clearpage
% \section{Visualization Results}
% In this section, we provide more visualization results to illustrate the effectiveness of our FFNet.
% In this section, we present additional visualization results to further demonstrate the effectiveness of our proposed FFNet. 
% \begin{figure}[ht!]
%   \centering
%   \includegraphics[width=1\linewidth]{images/ffnet-effect.png}\label{fig: ffnet performance}
%   \caption{\small \textbf{Detection Performance of FFNet.} On the vehicle side, a car is occluded by other objects and cannot be seen from the vehicle view. On the infrastructure side, this occluded car can be seen from an infrastructure view. Our FFNet can detect this car.}
% \end{figure}