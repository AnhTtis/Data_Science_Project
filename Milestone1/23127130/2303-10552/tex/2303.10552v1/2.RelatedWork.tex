\section{Related Work}\label{sec: related work}
\paragraph{Egocentric 3D Object Detection.} 
Perceiving objects, especially 3D obstacles in the road environment, is a fundamental task in egocentric autonomous driving. 
Egocentric 3D object detection can be classified into three categories based on sensor types: camera-based methods, LiDAR-based methods, and multi-sensor-based methods. 
Camera-based methods, such as FCOS3D~\cite{wang2021fcos3d}, directly detect 3D bounding boxes from a single image. Other camera-based methods, such as BEVformer~\cite{li2022bevformer} and M$^2$BEV~\cite{xie2022m}, project 2D images onto a bird's-eye view (BEV) to conduct multi-camera joint 3D detection. LiDAR-based methods, such as VoxelNet~\cite{zhou2018voxelnet}, SECOND~\cite{yang2019std}, and PointPillars~\cite{lang2019pointpillars}, divide the LiDAR point cloud into voxels or pillars and extract features from them. Multi-sensor-based methods, such as PointPainting~\cite{vora2020pointpainting} and BEV-Fusion~\cite{liu2022bevfusion}, utilize both camera and LiDAR data.
In contrast to these methods for single-vehicle view object detection, our proposed method focuses on cooperative detection. It utilizes both infrastructure and vehicle sensor data to overcome the perception limitations of single-vehicle view detection.

\paragraph{VIC3D Object Detection.}
With the development of V2X communication~\cite{hobert2015enhancements}, utilizing information from the road environment has attracted much attention. 
Some works use information from other vehicles to broaden the perception field. 
V2VNet~\cite{wang2020v2vnet} is a pioneering work in multi-vehicle 3D perception and provides a feature fusion framework to achieve performance-transmission trade-off. DiscoNet~\cite{li2021learning} applies distillation in the feature fusion training. 
V2X-ViT~\cite{xu2022v2x} introduces the vision transformer to fuse information across on-road agents. 
V2X-Sim~\cite{li2022v2x} and OPV2V~\cite{xu2021opv2v} are two simulated datasets for multi-vehicle cooperative perception research. 
Some works~\cite{valiente2019controlling, cui2022coopernaut} integrate infrastructure data for end-to-end autonomous driving. 
Some works use infrastructure data to improve 3D perception ability.
DAIR-V2X~\cite{yu2022dairv2x} is a pioneering work in vehicle-infrastructure cooperative 3D object detection. 
It releases a large-scale real-world V2X dataset and introduces the VIC3D object detection task. 
However, it only provides simple early and late fusion baselines. 
\cite{hu2022where2comm, arnold2020cooperative} focus on transmitting feature maps for cooperative detection, but these works do not consider the temporal asynchrony and fusion errors.
In contrast, our approach overcomes the challenge of temporal asynchrony and reduces transmission costs. 
We propose a feature flow prediction method that is different from \cite{lei2022latency}, which focuses on integrating per-frame features from other vehicles. 
Our approach addresses the challenge of temporal asynchrony by predicting future feature and compensating for the latency, resulting in improved detection performance.

\paragraph{Feature Flow.}
Flow is a concept originating from mathematics, which formalizes the idea of the motion of points over time~\cite{deville2012mathematical}. 
It has been successfully applied to many computer vision tasks, such as optical flow~\cite{beauchemin1995computation}, scene flow~\cite{menze2015object}, and video recognition~\cite{zhu2017deep}. 
As a concept extended from optical flow~\cite{horn1981determining}, feature flow describes the changing of feature maps over time, and it has been widely used in various video understanding tasks.
Zhu et al.\cite{zhu2017flow} propose a flow-guided feature aggregation to improve video detection accuracy. 
In this paper, we introduce the feature flow for feature prediction to overcome the challenge of temporal asynchrony in VIC3D object detection.