\section{Introduction}
\begin{figure*}
  \centering
  \includegraphics[width=1\linewidth]{images/Visualization_of_result_and_feature.pdf}\label{fig: the main idea of FFNet}
  \vspace{-10pt}
  \caption{\textbf{Fusion with Aligned Data and Non-Aligned Data.} (a)-(d) Images and Point clouds Captured from Infrastructure and Vehicle Sensors. The objects in circles with same color share the same object IDs. The object in the white circle indicates the ego vehicle.
  (e) Left: Fusion with Aligned Point Clouds.
  Right: Fusion with Aligned Features. 
    In the absence of temporal asynchrony, infrastructure data and vehicle data depict the same scene simultaneously but with different perspectives. Thus, when we transform infrastructure data (raw data or feature maps) into the same coordinate system as the vehicle data, the data elements in the same position correspond to the encoding of the same objects. Consequently, direct fusion of the two-side data does not result in any fusion misalignment, as depicted in the red circle. 
  (f) Left: Fusion with Non-aligned Point Clouds.
  Right: Fusion with Non-aligned Features. 
  There is a communication latency after broadcasting the infrastructure data. Consequently, dynamic objects (like the object in the red circle) can move, and the data elements in the same position may no longer correspond to the encoding of the same objects, leading to fusion errors when directly fusing the two-side data. This non-alignment of point clouds or features may produce false detection result.
  }
\end{figure*}

3D object detection is a critical task in autonomous driving, providing accurate location and classification information about surrounding obstacles. Traditional 3D object detection utilizes onboard sensor data from the ego vehicle to generate 3D outputs. 
However, with the limited perception field of ego-vehicle sensors, this solution often fails in the blind or long-range zones, affecting downstream prediction and planning tasks and resulting in severe safety problems.
Recently, vehicle-infrastructure cooperative autonomous driving has attracted much attention to address these safety challenges~\cite{yu2022dairv2x}. Infrastructure sensors like cameras and LiDARs are commonly installed much higher than ego vehicles, providing a broader field of view. 
Utilizing extra infrastructure sensor data can effectively obtain more meaningful information and improve autonomous driving perception ability. This paper focuses on solving the vehicle-infrastructure cooperative 3D (VIC3D) object detection problem with point clouds as inputs.

The VIC3D problem can be formulated as a multi-sensor detection problem under constrained communication bandwidth.
This problem presents two main challenges: first, the data captured by ego-vehicle sensors and received from infrastructure devices have asynchronous timestamps; second, communication bandwidth between two-side devices is limited.
Recent studies~\cite{yu2022dairv2x,hu2022where2comm,xu2022v2x} have attempted to address this problem and proposed three major fusion frameworks for cooperative detection: 
early fusion, late fusion, and middle fusion. 
Early fusion involves transmitting raw data like raw point clouds, while late fusion uses detection outputs for object-level fusion. 
Middle fusion utilizes intermediate-level features for feature fusion, striking a balance between preserving valuable information and reducing redundant transmission. 
However, existing middle-fusion solutions~\cite{hu2022where2comm,xu2022v2x} overlook the challenge of temporal asynchrony explicitly, resulting in fusion misalignment that affects the detection results, as depicted in Figure~\ref{fig: the main idea of FFNet}.

In this work, we propose Feature Flow Net (FFNet), a novel cooperative detection framework for vehicle-infrastructure cooperative 3D (VIC3D) object detection. 
FFNet is designed to address the temporal asynchrony between infrastructure and ego-vehicle sensor data that can cause fusion misalignment and impact detection performance. As shown in Figure \ref{fig: FFNet}, FFNet consists of several steps, including generating feature flow from sequential infrastructure frames, transmitting the compressed feature flow, decompressing the feature flow, and reconstructing infrastructure features to align with the timestamp of ego-vehicle sensor data for detection output. The critical module in FFNet is feature flow, which acts as a feature prediction function to generate alignment with ego-vehicle features and eliminate fusion errors caused by temporal asynchrony. Moreover, feature flow is an intermediate-level data that can be compressed, reducing transmission costs.
A self-supervised approach is also proposed to train the feature flow generator by constructing ground truth features to optimize feature flow prediction. 
We extract rich temporal correlations from raw frames embedded in feature flow to predict future infrastructure features and align them with vehicle features, making it highly suitable for addressing the temporal asynchrony challenge in VIC3D. 
To the best of our knowledge, this is the first time utilizing feature flow for multi-sensor object detection to address the challenge of temporal misalignment and using raw frames to enhance feature prediction accuracy.

We conduct an extensive evaluation of the proposed FFNet framework on the DAIR-V2X dataset~\cite{yu2022dairv2x}, which contains real-world driving scenarios. In order to demonstrate the effectiveness of FFNet, we compare its performance with several existing cooperative detection methods, including V2VNet~\cite{wang2020v2vnet} and DiscoNet~\cite{li2021learning}, by implementing them and conducting experiments under the same settings. Our results demonstrate that FFNet can successfully address the challenge of temporal asynchrony by accurately predicting future features at different timestamps, with the ability to overcome various latencies ranging from 100$ms$ to 500$ms$.
Notably, our proposed FFNet outperforms all existing cooperative detection methods and achieves state-of-the-art performance while requiring less than 1/10 of the transmission cost of raw data, particularly when the latency exceeds 200$ms$.

The main contributions are organized as followings.
\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
    \item We propose a novel intermediate-level framework for VIC3D problem, called Feature Flow Net (FFNet), which utilizes feature flow for cooperative detection. The FFNet generates future features to address the temporal asynchrony challenge and reduces transmission costs while retaining valuable information for cooperative detection.
    \item We introduce a self-supervised approach to train the feature flow generator in FFNet, which successfully predicts future features to align with ego-vehicle sensor data and mitigate temporal fusion errors in various latencies.
    \item We evaluate the proposed FFNet framework on the real-world DAIR-V2X dataset, demonstrating superior performance compared to other state-of-the-art intermediate-level methods. Furthermore, FFNet surpasses all cooperative detection methods when latency exceeds 200$ms$.
\end{itemize}