\section{Method}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=1\linewidth]{images/FFNet-OVERVIEW.pdf}
  \vspace{-10pt}
  \caption{FFNet Overview.
  In the infrastructure system, the feature extractor and the first-order derivative generator make up the feature flow generator. 
  The feature flow is linearly represented with a feature and a first-order derivative as shown in Equation~\ref{eq: first-order of feature flow}, and can be used to predict the future feature.
  The compressors and decompressors focus on reducing the transmission cost while reserving valuable information for cooperative detection. Detailed configurations for FFNet can be found in the Appendix.
  }\label{fig: FFNet}
\end{figure*}

This paper aims to improve vehicle-infrastructure cooperative 3D (VIC3D) object detection while considering practical communication conditions to reduce transmission costs and overcome the temporal asynchrony challenge.
To achieve this goal, we first apply V2VNet~\cite{wang2020v2vnet} for cooperative detection, which is originally used to transmit intermediate features for multi-vehicle perception. We provide details on this basic solution in the Appendix. 
However, this approach ignores temporal asynchrony and may introduce serious fusion misalignment.
To address these issues, we propose a novel intermediate-level framework called Feature Flow Net (FFNet), which includes a feature flow prediction function that generates aligned infrastructure features with vehicle features to compensate for the temporal asynchrony.
Moreover, we introduce a self-supervised approach to train the feature flow generator.
In the following sections, we describe the VIC3D problem in Section~\ref{sec: vic3d detection task}, detail the inference of our solution in Section~\ref{sec: FFNet}, and explain the training in Section~\ref{sec: semi-supervised learning}.
We also compare different possible solutions in the Appendix.

\subsection{VIC3D Object Detection}\label{sec: vic3d detection task}
The VIC3D object detection aims to improve the performance of localizing and recognizing the surrounding objects by utilizing both the infrastructure and vehicle sensor data under limited communication conditions. 
This paper focuses on point clouds captured from LiDAR as inputs. 
The input of VIC3D consists of two parts:
\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
    \item Point cloud $P_{v}(t_{v})$ captured by the vehicle sensor with timestamp $t_{v}$ as well as its relative pose $M_{v}(t_{v})$, where $P_{v}(\cdot)$ denotes the capturing function of vehicle LiDAR.
    \item Point cloud $P_{i}(t_{i})$ captured by the infrastructure sensor with timestamp $t_{i}$ as well as its relative pose $M_{i}(t_{i})$, where $P_{i}(\cdot)$ denotes the capturing function of infrastructure LiDAR. Previous frames captured by the infrastructure sensor can also be utilized in cooperative detection.
\end{itemize}
Note that the timestamp $t_i$ should be earlier than timestamp $t_v$ since receiving the data through long-range communication from infrastructure devices to vehicle devices requires a significant amount of transmission time. 
Moreover, the latency $(t_v-t_i)$ should be uncertain before receiving the data, as the transmitted data could be obtained by various autonomous driving vehicles in different locations after infrastructure broadcasting.
More illustration of the uncertain latency is provided in the Appendix.

Compared with 3D object detection in single-vehicle autonomous driving, the VIC3D faces more challenges in temporal asynchrony and transmission costs. 
Directly fusing the infrastructure data can cause serious fusion errors and affect the detection performance due to the scene changing and dynamic objects moving. 
Asynchronous phenomena are also demonstrated in the experiment results in Section~\ref{sec: ablation study}. 
Additionally, reducing the amount of transmitted data could effectively shorten the overall latency because the transmission time is positively correlated with the amount of the transmitted data~\cite{fano1961transmission}. 
Therefore, in addition to the detection performance, the VIC3D has another goal: to reduce the transmission cost. 
Here, mean Average Precision (mAP) is used to measure the detection performance, while the Average Byte ($\mathcal{AB}$) is used to measure the transmission cost. 
We detail the two metrics in the Appendix.

\subsection{Feature Flow Net}\label{sec: FFNet}
Our Feature Flow Net (FFNet) provides a new intermediate-level solution for VIC3D. 
This approach transmits the compressed feature flow for cooperative detection, which reduces redundant information in raw data and eliminates the fusion misalignment caused by temporal asynchrony. 
The inference process of FFNet consists of following stages: (1) generating the feature flow, (2) compressing, transmitting, and decompressing the feature flow, and (3) predicting the aligned feature and fusing it with the vehicle feature to generate the detection results. The inference process is illustrated in Figure~\ref{fig: FFNet}.

\paragraph{Feature Flow Concept for VIC3D.}
The concept of flow is based on mathematical models that formalize the idea of the motion of points over time~\cite{deville2012mathematical}. 
Feature flow is a variant that characterizes the intermediate feature changes over time. 
In this paper, we adopt the feature flow as a prediction function to describe the infrastructure feature changes over time in the future. 
Given the current point cloud frame $P_i(t_i)$ and the infrastructure feature extractor $F_i(\cdot)$, the feature flow over the future time $t$ after $t_i$ is defined as:
\begin{equation}
\setlength\abovedisplayskip{0.05cm}
\setlength\belowdisplayskip{0.15cm}
\widetilde{F}_i(t) =
F_i(P_i(t)), t \geq t_i
\end{equation}

After an uncertain transmitting time, some autonomous driving vehicles receive the feature flow. 
Compared with the previous approach of transmitting feature $F_i(P_i(t_i))$ produced from per frames~\cite{wang2020v2vnet}, which lacks temporal and predictive information, feature flow enables the direct prediction of the aligned feature at the timestamp $t_v$ of the vehicle sensor data. 
We fuse this predicted infrastructure feature $\widetilde{F}_i(t_v)$ with the vehicle feature to compensate for the asynchronous time and eliminate the fusion error caused by temporal asynchrony. 
Note that we generate the feature flow to obtain temporal prediction ability on the infrastructure side rather than the vehicle side. 
This design is because we can extract richer temporal correlation from raw sequential frames than from information-reduction features. 
We also implement the comparison of the extraction on different sides in the Appendix. 
We will introduce how to implement the feature flow in the next part.

\paragraph{Feature Flow Generation.}
Two issues need to be addressed in order to apply the feature flow to transmission and cooperative detection: expressing and transmitting the continuous feature flow changes over time, and enabling the feature flow with prediction ability. 
Considering that the time interval $t_v\rightarrow t_i$ is generally short, we address the expressing issue by using the simplest first-order expansion to represent the continuous feature flow over time, which takes the form of Equation~\eqref{eq: first-order of feature flow}, 
\begin{equation}\label{eq: first-order of feature flow}
    \setlength\abovedisplayskip{0.05cm}
    \setlength\belowdisplayskip{0.15cm}
    \widetilde{F}_i(t_i+\Delta t) \approx F_{i}(P_i(t_i)) + \Delta t * \widetilde{F}_i^{'}(t_i),
\end{equation}
where $\widetilde{F}i^{'}(t_i)$ denotes the first-order derivative of the feature flow and $\Delta t$ denotes a short time period in the future. 
Thus, we only need to obtain the feature $F{i}(P_i(t_i))$ and the first-order derivative of the feature flow $\widetilde{F}_i^{'}(t_i)$ to obtain an approximate representation of the feature flow.
When an autonomous driving vehicle receives the feature $F_{i}(P_i(t_i))$ and the first-order derivative $\widetilde{F}_i^{'}(t_i)$ after an uncertain latency, we can generate the infrastructure feature aligned with the vehicle sensor data, and the prediction operation consumes minor computation because it only needs linear calculation. 
To enable the feature flow with prediction ability, we use a network to extract the first-order derivative of the feature flow $\widetilde{F}_i^{'}(t_i)$ from the historical infrastructure frames ${I_i(t_i-N+1), \cdots, I_i(t_i-1), I_i(t_i)}$. 
Generally, the larger $N$ will generate more accurate estimations.
In this paper, we take $N$ as two and use two consecutive infrastructure frames $P_i(t_i-1)$ and $P_i(t_i)$.

Specifically, we first use the Pillar Feature Net~\cite{lang2019pointpillars} to convert the two consecutive point clouds into two pseudo-images with a bird-eye view (BEV) and with the size of $[384, 288, 288]$. Then, we concatenate the two BEV pseudo-images into the size of $[768, 288, 288]$, and input the concatenated pseudo-images into a 13-layer Backbone and a 3-layer FPN (Feature Pyramid Network), as in SECOND~\cite{yan2018second}, to generate the estimated first-order derivative $\widetilde{F}_i^{'}(t_i)$ with the size of $[364, 288, 288]$. Finally, with the feature $F_{i}(P_i(t_i))$ and the estimated first-order derivative $\widetilde{F}_i^{'}(t_i)$, we can represent the feature flow with prediction ability. The detailed network configuration is provided in the Appendix.

\paragraph{Compression, Transmission and Decompression.}
The transmission cost of the feature flow in its original form is substantial, amounting to up to 243Mb~\footnote{This transmission involves 384$\times$288$\times$288$\times$2 floating point numbers, which equates to 243Mb.}. 
In order to eliminate redundant information and reduce the transmission cost further, we apply two compressors to the feature $F_{i}(P_i(t_i))$ and the derivative $\widetilde{F}_i^{'}(t_i)$, compressing them from size $[384, 288, 288]$ to $[12, 36, 36]$ using three Conv-Bn-ReLU blocks in each compressor.
The size of the timestamp and calibration file is small and cannot be ignored. 
Therefore, the compressed feature flow only requires 0.12Mb per transmission, which is $1/32$$\times$$1/8$$\times$$1/8$ of the original feature flow. 
We broadcast the compressed feature flow along with the corresponding timestamp and calibration file on the infrastructure side.
Upon receiving the compressed feature flow, the vehicle uses two decompressors, each composed of three Deconv-Bn-ReLU blocks, to decompress the compressed feature and compressed first-order derivatives from size $[384/32, 288/8, 288/8]$ to the original size $[384, 288, 288]$.

\paragraph{Vehicle-Infrastructure Feature Fusion.}
We employ a Pillar Feature Net~\cite{lang2019pointpillars} and a feature extractor to generate the vehicle feature from the vehicle point cloud $P_v(t_v)$, which is captured at timestamp $t_v$. The decompressed feature flow is then utilized to predict the infrastructure feature at timestamp $t_v$, aligned with the vehicle feature, as follows:
\begin{equation}
\setlength\abovedisplayskip{0.05cm}
\setlength\belowdisplayskip{0.15cm}
\widetilde{F}i(t_v) \approx F{i}(P_i(t_i)) + (t_v-t_i) * \widetilde{F}_i^{'}(t_i).
\end{equation}
This linear prediction operation effectively compensates for uncertain latency and requires minimal computation. 
The predicted feature $\widetilde{F}_i(t_v)$ is then transformed into the vehicle coordinate system using the corresponding calibration files. 
The bird's-eye view of the infrastructure and vehicle features are obtained, both at the vehicle coordinate system, while preserving spatial alignment. 
The feature located outside the vehicle's interest area is discarded for the infrastructure feature, and empty locations are padded with zero elements.

Subsequently, we concatenate the infrastructure feature with the vehicle feature and employ a Conv-Bn-Relu block to fuse the concatenated features to obtain the vehicle-infrastructure fused feature. 
Finally, we input the fused feature into a 3D detection head, utilizing the Single Shot Detector (SSD)~\cite{liu2016ssd} setup as the 3D object detection head, to generate 3D outputs for more accurate localization and recognition. 
The experimental results indicate that the infrastructure feature flow significantly enhances the detection ability.

\subsection{Training Feature Flow Net}\label{sec: semi-supervised learning}
The FFNet training consists of two stages: training a basic fusion framework in an end-to-end way and then using a self-supervised learning to train the feature flow generator.

\paragraph{Training Basic Fusion Framework.}
In the first stage, we train a basic fusion framework in an end-to-end manner without considering latency. 
The objective of this stage is to train FFNet to fuse the infrastructure feature with the vehicle feature to enhance detection performance. 
Specifically, we train FFNet using cooperative data and annotations obtained from both the vehicle and the infrastructure. 
The localization regression and object classification loss functions used in SECOND~\cite{yan2018second} are applied in this stage. 
To speed up training, we mask the first-order derivative generation and use only one infrastructure point cloud $P_i(t_i)$. 
At the end of this stage, all the modules in FFNet are trained except for the first-order derivative generator.

\paragraph{Training Feature Flow Generator.}
\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{images/training-ffgenerator.pdf}
  \vspace{-5pt}
  \caption{Illustration of training a feature flow generator using self-supervised learning and similarity loss. The upper red circle represents the first-order derivative generator, while the lower purple circles with solid and dashed lines share the same infrastructure feature extractor.
  Feature flow predicts the feature at $t_i+k$.
  }\label{fig: ffgenerator-training}
\end{figure}
In the second stage, we use self-supervised learning to train the feature flow generator by exploiting the temporal correlations in infrastructure sequences, as shown in Figure~\ref{fig: ffgenerator-training}.
The idea is to generate the feature flow, which contains information about the movement of objects over time, and then use it to predict the feature at a future time. 
We construct the ground truth feature by using nearby infrastructure frames that do not require any manual annotations. 
Specifically, we generate training frame pairs $\mathcal{D}={d_{t_i,k}=(P_i(t_i-1), P_i(t_i), P_i(t_i+k))}$, where $P_i(t_i-1)$ and $P_i(t_i)$ are two consecutive infrastructure point cloud frames, and $P_i(t_i+k)$ is the $(k+1)$-th frame after $P_i(t_i)$. 
For each pair $d_{t_i,k}$ in $\mathcal{D}$:
\vspace{-0.1cm}
\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
    \item we input $P_i(t_i-1)$ and $P_i(t_i)$ into the feature flow generator to generate the feature flow, which is composed of $F_{i}(P_i(t_i))$ and the estimated first-order derivative $\widetilde{F}_i^{'}(t_i)$. 
    \item We then use the feature flow to predict the feature at time $t_i+k$ as
    \begin{equation}
        \setlength\abovedisplayskip{0.05cm}
        \setlength\belowdisplayskip{0.15cm}
        \widetilde{F}_{i}(t_i+k) \approx F_{i}(P_i(t_i)) + |(t_i+k)-t_i| * \widetilde{F}_i^{'}(t_i).
    \end{equation}
    \item We use the infrastructure feature extractor $F_i(\cdot)$ to extract the feature $F_i(P_i(t_i+k))$ from $P_i(t_i+k)$ as the ground truth feature to supervise the feature flow prediction.
\end{itemize}

We construct the loss function to optimize the feature flow generator. 
The objective is to generate the feature flow and use it to predict $\widetilde{F}_i(t_i+k)$ as close as possible to $F_i(P_i(t_i+k))$. 
We use the cosine similarity to measure the similarity between the predicted feature and the ground truth feature as
\begin{equation}
    \setlength\abovedisplayskip{0.05cm}
    \setlength\belowdisplayskip{0.15cm}
    similarity = \frac{\widetilde{F}_i(t_i+k) \odot F_i(P_i(t_i+k))}{||\widetilde{F}_i(t_i+k)||_2*||F_i(P_i(t_i+k))||_2},
\end{equation}
where $\odot$ denotes the inner product, $*$ denotes the scalar multiplication, and $||\cdot||_2$ denotes the L2 norm. 
We use this similarity as the loss function to train the feature flow generator as
\begin{equation}\label{Eq: loss}
    \setlength\abovedisplayskip{0.05cm}
    \setlength\belowdisplayskip{0.15cm}
    \mathcal{L}(\mathcal{D},\theta) = \sum_{d_{t_i,k}\in \mathcal{D}} (1-\frac{\widetilde{F}_i(t_i+k) \odot F_i(P_i(t_i+k))}{||\widetilde{F}_i(t_i+k)||_2*||F_i(P_i(t_i+k))||_2}),
\end{equation}
where $\theta$ is the parameter of the feature flow generator, and we only update the parameters in first-order derivative generator $\widetilde{F}_i^{'}(\cdot)$ and frozen other parameters.
To train the feature flow generator, we use the Adam optimizer~\cite{kingma2014adam} to minimize the loss function with the batch size of 2, the learning rate of 0.001, and the total epoch of 10. 

\paragraph{Remark.} 
It is important to note that cosine similarity is unaffected by the magnitudes of the input tensors. For instance, the input tensors $[1, 2, 3]$ and $[2, 4, 6]$ achieve a maximal cosine similarity value of $1$, even though they have different values. 
Therefore, to adjust the magnitudes of $\widetilde{F}_i(t_i+k)$, we require a scale transformation. We propose using $||F_i(P_i(t_i))||_1 / ||\widetilde{F}_i(t_i+k)||_1$ as the scaling factor, where $||\cdot||_1$ denotes the L1 norm.