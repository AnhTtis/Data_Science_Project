\section{Experiments and Results}

We evaluate Xplainer in a zero-shot setting on the commonly used chest X-ray datasets, CheXpert \cite{irvin2019chexpert}, and ChestX-ray14 \cite{wang2017chestx}. The CheXpert dataset provides a manually labeled validation and test set with 200 and 500 patients, respectively, and 14 classes, including "No Finding", "Support Devices / Foreign Objects", and 12 pathology labels. ChestX-ray14 is evaluated on 14 pathology labels on a test set of 25.596 images. We perform a multi-label classification for both datasets and evaluate the performance via the Area Under the ROC-curve (AUC) between the positive pathology probabilities and the labels.

\begin{table}[b]
\centering
\caption{AUC for zero-shot pathology classification on CheXpert and ChestX-ray14 datasets. *in-domain, as the underlying CLIP model was trained the ChestX-ray14}
\begin{tabular}{|l|c|cc|c|}
\hline
             & CLIP pretraining data & \multicolumn{2}{c|}{CheXpert}     & ChestX-ray14 \\ \hline
             & & \multicolumn{1}{c|}{val}   & test & test         \\ \hline
             
CheXzero \cite{tiu2022chexzero} &  MIMIC & \multicolumn{1}{c|}{--} & 74.73    & --        \\ \hline
Seibold et al. \cite{seibold2022breaking} & MIMIC & \multicolumn{1}{c|}{78.86} & --    & 71.23        \\ \hline
Seibold et al. \cite{seibold2022breaking} & MIMIC, PadChest, ChestX-ray14  & \multicolumn{1}{c|}{83.24} & --    & 78.33*        \\ \hline
\textbf{Xplainer}  & MIMIC & \multicolumn{1}{c|}{\textbf{84.92}}  & \textbf{80.58} & \textbf{71.73}         \\ \hline
\end{tabular}
\label{tab:dataset_results}
\end{table}

\noindent Table \ref{tab:dataset_results} shows our results compared to previously proposed zero-shot pathology prediction approaches. On CheXpert, we compare with Seibold et al. \cite{seibold2022breaking} on the validation set, as they only reported validation performance. For the comparison with CheXzero \cite{tiu2022chexzero}, as well as the ChestX-ray14 dataset, we compare test set results. We outperform both previous works in an out-of-domain setting, where the zero-shot inference is performed on a different dataset than CLIP was trained on. The state-of-the-art results on both datasets show the effectiveness of our observation-based modeling. Further, in Table \ref{tab:pathology_results}, we provide a detailed breakdown of our results per pathology and dataset. 

\begin{table}[tb]
\centering
\caption{AUC per disease on both datasets}
\begin{tabular}{|l|c|c|c|}
\hline
                           & CheXpert Val & CheXpert Test & ChestX-ray14 \\ \hline
No Finding                 & 88.82         & 89.94          & --        \\ \hline
Enlarged Cardiomediastinum & 79.23         & 80.60          & --          \\ \hline
Cardiomegaly               & 78.62         & 83.32          & 79.71        \\ \hline
Lung Opacity               & 88.18         & 91.76          & --           \\ \hline
Lung Lesion                & 91.46         & 69.33          & --           \\ \hline
Edema                      & 84.84         & 84.55          & 81.46        \\ \hline
Consolidation              & 91.56         & 85.89          & 71.87        \\ \hline
Pneumonia                  & 85.68         & 83.73          & 70.83        \\ \hline
Atelectasis                & 84.64         & 85.46          & 66.86        \\ \hline
Pneumothorax               & 78.09         & 83.75          & 72.18        \\ \hline
Pleural Effusion           & 88.72         & 89.30          & 79.11        \\ \hline
Pleural Other              & 83.92         & 58.67          & --           \\ \hline
Fracture                   & --              & 60.47          & --           \\ \hline
Infiltration               & --            & --             & 68.81        \\ \hline
Mass                      & --            & --             & 70.28        \\ \hline
Nodule                     & --            & --             & 64.74        \\ \hline
Emphysema                   & --            & --             & 74.02        \\ \hline
Fibrosis                   & --            & --             & 62.25        \\ \hline
Pleural Thickening         & --            & --             & 67.44        \\ \hline
Hernia                     & --            & --             & 74.60        \\ \hline
Support Devices / Foreign Objects            & 80.25         & 81.15          & --           \\ \hline
\end{tabular}
\label{tab:pathology_results}
\end{table}

\subsubsection{Ablation Studies}
In our ablation studies, we investigate the impact of our prompt design and the effect of using multiple images. Table \ref{tab:prompt_comparison} shows the results on the CheXpert validation set using different prompting styles. We observe that pathology-based prompting, which reaches an AUC of 76.14\%, is significantly worse than observation-based prompting, which reaches an AUC of 84.92\%, again highlighting the benefit of observation-based prompting. Comparing the basic observation-based prompting, using only positive prompts per observation, to contrastive prompting, we see a substantial performance gap, showing the importance of using negative prompts to differentiate between positive and negative occurrences. We also show the effect of formulating our prompts unambiguously and in the style of an actual radiology report by adding pathology indication and report style. Adding pathology indication to the contrastive observation-based prompting significantly improves performance, achieving an AUC of 84.35\%. Finally, incorporating report style in the prompts leads to the highest AUC of 84.92\%, indicating that a contrastive observation-based prompt with pathology indication and report style is the most effective for zero-shot X-ray pathology classification.

\begin{table}[tb]
\centering
\caption{Comparison of different prompting styles on the validation set of CheXpert}
\begin{tabular}{|l|c|}
\hline
                   & AUC \\ \hline
Contrastive pathology-based Prompting       & 76.14  \\ \hline
\multicolumn{1}{|l}{\textbf{Observation-based Prompting:} } &      \\ \hline
Basic Prompt       & 58.65  \\ \hline
Contrastive Prompt & 77.00   \\ \hline
 $+$ pathology Indication & 84.35   \\ \hline
 $+$ Report Style       & 84.92   \\ \hline
\end{tabular}
\label{tab:prompt_comparison}
\end{table}

Additionally, we compare the initial ChatGPT output to our refined prompts (Table \ref{tab:prompt_refinement}). Refinement was performed by deleting irrelevant, redundant, or incorrect descriptors. We observe an improvement through the refinement, indicating that including domain knowledge further improves our method. Nevertheless, the original ChatGPT prompts already perform quite well, showing the impressive potential of combining large generic language models with large domain-specific contrastive models.

\begin{table}[tb]
\centering
\caption{Comparison of ChatGPT prompts vs. refinement with the help of a radiologist}
\begin{tabular}{|l|l|l|l|}
\hline
             & CheXpert Val & CheXpert Test & ChestX-ray14  \\ \hline
ChatGPT prompts & 83.61 & 79.94 & 71.40\\ \hline
Refined Prompts & \textbf{84.92} & \textbf{80.58} & \textbf{71.73}\\ \hline
\end{tabular}
\label{tab:prompt_refinement}
\end{table}

\noindent For the "No Finding" class, we compare to either define specific prompts such as "Clear lung fields" or "Normal heart size and shape" to classify "No Finding" or model it as the absence of all of the other 13 labels (Rule-based). Table \ref{tab:no_finding} shows that a rule-based modeling of this class leads to better results. A reason for this could be that there is no clearly defined list of observations that indicate a healthy X-ray scan, which a radiologist would mention in his report.

\begin{table}
\begin{minipage}[]{.45\textwidth}
\centering
\caption{Modeling of "No Finding" label with explicit prompts or rule-based definition as lack of other findings}
\begin{tabular}{lc}
\hline
\multicolumn{1}{|l}{}& \multicolumn{1}{|c|}{AUC - No Finding}  \\ \hline
\multicolumn{1}{|l}{Explicit Prompting}& \multicolumn{1}{|c|}{79.64} \\ \hline
\multicolumn{1}{|l}{Rule-based} & \multicolumn{1}{|c|}{\textbf{88.82}} \\ \hline
  & ~          \\
\end{tabular}
\label{tab:no_finding}
\end{minipage}\hfill
\begin{minipage}[]{.45\textwidth}
  \centering
\caption{Comparison of single-view inference to different methods for multi-image processing}
\begin{tabular}{|l|l|}
\hline
             & AUC  \\ \hline
Only single Frontal View & 84.19 \\ \hline
All - Max Aggregation          & 84.77 \\ \hline
All - Mean Aggregation      & \textbf{84.92} \\ \hline
\end{tabular}
\label{tab:multi_view}
\end{minipage}
\end{table}

\noindent Lastly, we investigate different image aggregation methods for pathology prediction. We compare only using a single frontal view X-ray to using all images available for a patient. For aggregation, we compute positive and negative observation probabilities for every image. In Max aggregation, we then use the highest observation probability. The intuition behind this approach is that an observation might be seen much better from one perspective than another, and then only the perspective where the model is most confident should be used. On the other hand, different views give different insights about which kind of observation a visual cue on the image indicates. To leverage this multi-view information, we test Mean aggregation, where all observation probabilities are averaged over multiple images. The results shown in Table \ref{tab:multi_view} indicate Mean aggregation to be superior, while both aggregation methods outperform using just a single image.

\subsubsection{Qualitative Results}
Figure \ref{fig_qualitative} shows qualitative examples of our model's predictions. For the true positive prediction, it can be seen that most of the descriptors are detected, and the model recognizes the descriptor "Mass in the mediastinum" as the main indication for the Enlarged Cardiomediastinum. For the True Negative case, the model, correctly, detected none of the descriptors. For the false positive example, one can clearly see that the model made a mistake because it detected an air bronchogram with relatively high certainty and no consolidation. Therefore, this false positive finding is easily falsified by the radiologist since an air bronchogram is a finding that co-occurs with consolidation (i.e., air-filled bronchi in consolidated areas). Thus, knowing which combination of descriptors leads to such a decision substantially improves explainability. In the false positive case, the model misses the pacemaker but detects some implant, showing the model understands there is some foreign object, but can not identify it, which is easily detected by the radiologist. Overall the classification-by-description may facilitate a plausibility check of a specific inference result and an understanding of the source of errors.
\begin{figure}[tb]
\centering
\includegraphics[width=\textwidth]{figures/all_examples.pdf}
\caption{Qualitative results of Xplainer}
\label{fig_qualitative}
\end{figure}

\subsubsection{Discussion}
One downside of modeling a joint probability is that it assumes that all descriptors appear simultaneously and gives all descriptors the same importance. While this estimation leads to good results, the assumption does not always hold, as a pathology does not always present with the same signs. Further, there might be inter-dependencies between the descriptors, e.g., there can be descriptors that strongly correlate with the presence of a disease when combined with one descriptor but much less when combined with another.
As a first try to model the importance of descriptors, we look into a supervised, out-of-domain approach to model these inter-dependencies. For this, we train a Naive Bayes~\cite{chan1982updating,Zhang2004} CheXpert classifier on MIMIC-CXR \cite{johnson2019mimic}, predicting a diagnosis given the descriptor probabilities, allowing the model to focus more on more relevant descriptors. While this approach relies on labels for MIMIC, these labels can be automatically generated by the CheXpert labeler \cite{irvin2019chexpert}, still not requiring human effort for labeling. We observe a slight performance increase on the test set from 80.58\% to 81.37\% AUC. This shows that the descriptor importance learned on MIMIC can partially be transferred to an out-of-domain dataset. We believe investigating methods to consider varying importance and complex relations between the descriptors is an essential and exciting direction to investigate in future work.\looseness=-1

The use of descriptors in Xplainer provides a flexible and adaptive approach to automated diagnosis prediction. By identifying and classifying the presence of descriptive observations, our model can capture the underlying characteristics of a disease without relying on labeled data. This means that our system can easily adapt to new settings with different clinical findings, including new conditions where the symptoms are known, but there is no training data available yet. Additionally, using descriptors allows for adapting the system to specific populations, where the essential descriptors can differ. This is because the model is not constrained by pre-defined labels but rather by the meaningful underlying features of a given diagnosis.