\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{subfigure}
\usepackage{multirow}
%\usepackage[dvipsnames, svgnames, x11names]{xcolor} %use color
\usepackage{graphicx}
%\usepackage{amsmath}
%\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx} %use graph format
\usepackage{epstopdf}
\usepackage{makecell}
\usepackage{gensymb} %use degree
\usepackage{multirow}
%\usepackage{subfigure}
\usepackage{caption}


\usepackage{array}
\usepackage{tikz}
\newcommand*{\circled}[1]{\lower.7ex\hbox{\tikz\draw (0pt, 0pt)%
  circle (.5em) node {\makebox[1em][c]{\small #1}};}}

\usepackage{verbatim}
\usepackage{color,xcolor}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{BoPR: Body-aware Part Regressor for Human Shape and Pose Estimation}
\author{Yongkang Cheng$^{1,2}$,  \hspace{1 mm} Shaoli Huang$^{2}$\thanks{Corresponding author.}, \hspace{1 mm} Jifeng Ning$^{1}$,\hspace{1 mm} Ying Shan $^{2}$\\
$^{1}$Northwest A\&F University \qquad $^{2}$Tencent AI Lab\\
{\tt\small \{shaolihuang,yingsshan\}@tencent.com} \qquad {\tt\small yongkangcheng@nwafu.edu.cn} \qquad {\tt\small njf@nwsuaf.edu.cn}}

%\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi
\twocolumn[{
\maketitle
\begin{center}
    \captionsetup{type=figure}
    \vspace{-2em}
    \includegraphics[width=0.85\textwidth]{title.pdf}
    \vspace{-1em}
    \captionof{figure}{\textbf{Illustration of our method's main superiority over existing approaches.} When given a challenging input (a), PARE (b)~\cite{kocabas2021pare} and CLIFF (c)~\cite{li2022cliff} struggle to accurately estimate the pose of the forearm due to occlusion and depth ambiguity. Our method (d), on the other hand, tackles these challenges by exploiting body-part attention dependency in per-part estimation rather than relying on independent part regressors or global regression. This approach enables our method to more effectively reason about the relative spatial relationship between different body parts and the body as a whole, leading to improved occlusion handling and reduced depth ambiguity.}
    %\captionof{figure}{Illustration of our method's main superiority over existing approaches. When given a challenging input (a), PARE (b)~\cite{kocabas2021pare} and CLIFF (c)~\cite{li2022cliff} struggle to accurately estimate the pose of the forearm due to occlusion and depth ambiguity. Our method (d), on the other hand, tackles these challenges by utilizing mutual part-dependency and body-aware dependency in per-part estimation rather than relying on independent part regressors or global regression. This approach enables our method to more effectively reason about the relative spatial relationship between different body parts and the body as a whole, leading to improved occlusion handling and reduced depth ambiguity.}
    
    %{Illustration of our method's main superiority over existing approaches. Given an example image (a), PARE~\cite{kocabas2021pare} (b) and CLIFF~\cite{li2022cliff} (c) fail to estimate the forearm's accurate pose due to occlusion and depth ambiguity. Our method (d) resolves these issues by learning mutual part-dependency and body-aware dependency instead of independent part regressor and global regression. Therefore, our approach can better reason about the relative spatial relationship between parts and bodies, improve occlusion handling, and reduce depth ambiguity.}
    %{(a). Compared with existing methods (e.g., PARE~\cite{kocabas2021pare} and CLIFF~\cite{li2022cliff}), our method can better reason about the relative spatial relationship between parts and bodies.}%, and shows superiority in handling depth ambiguity and occlusion.}
    \label{fig:title}
\end{center}
}]

%%%%%%%%% ABSTRACT
\begin{abstract}
This paper presents a novel approach for estimating human body shape and pose from monocular images that effectively addresses the challenges of occlusions and depth ambiguity. Our proposed method BoPR, the \textbf{Bo}dy-aware \textbf{P}art \textbf{R}egressor, first extracts features of both the body and part regions using an attention-guided mechanism. We then utilize these features to encode extra part-body dependency for per-part regression, with part features as queries and body feature as a reference. This allows our network to infer the spatial relationship of occluded parts with the body by leveraging visible parts and body reference information. Our method outperforms existing state-of-the-art methods on two benchmark datasets, and our experiments show that it significantly surpasses existing methods in terms of depth ambiguity and occlusion handling. These results provide strong evidence of the effectiveness of our approach.
%Despite significant progress in estimating human body shape and pose from monocular images, existing methods still struggle with severe occlusions and depth ambiguity. To overcome these challenges, we propose a Body-aware Part Regressor that learns the attention dependency between parts and body. Our method first leverages the attention-guided mechanism to extract features of the body and part regions from images. Then we use part features as queries and body feature as a reference to encode extra part-body dependency for per-part regression. This mechanism guides the network to simultaneously leverage visible parts and torso reference information to infer the spatial relationship of occluded parts with the body. Our method outperforms existing state-of-the-art methods on two benchmark datasets. Moreover, our approach significantly surpasses existing methods in terms of depth ambiguity and occlusion handling experiments, providing strong evidence of its effectiveness.

%Despite significant progress in estimating human body shape and pose from monocular images, existing methods continue to struggle with severe occlusions and depth ambiguity. To overcome these challenges, we propose a Body-aware Part Regressor that learns the relationship between body parts and attention mechanisms. Our method uses the attention mechanism to identify features of the body regions and parts from images and then learns the feature relationship between them to model attention dependencies. This mechanism guides the network to use visible parts and torso reference information to infer the spatial relationship of occluded parts with the body. Our method outperforms existing state-of-the-art methods on two benchmark datasets. Moreover, our approach significantly surpasses existing methods in terms of depth ambiguity and occlusion handling experiments, providing strong evidence of its effectiveness.

%Human pose estimation from single RGB images is a challenging task due to the complexity of the human body and the inherent ambiguity of the problem. Current methods often suffer from the lack of accurate body references, resulting in inaccurate pose and shape predictions. In this paper, we propose a novel approach, BoPR, that leverages body characteristics as reference conditions to accurately predict the pose and shape parameters of each component. BoPR also predicts the depth of each limb relative to the body plane, enabling more accurate pose and shape estimation. We conduct extensive experiments on benchmark datasets and show that BoPR outperforms state-of-the-art methods, demonstrating its effectiveness in addressing the limitations of existing approaches. Our results highlight the importance of incorporating body references in human pose estimation and provide a promising direction for future research in this field.
\end{abstract}
\vspace{-1em}

%%%%%%%%% BODY TEXT
\section{Introduction}

Human body shape and pose estimation from monocular images has become increasingly important in computer vision due to its wide range of applications in fields such as human-computer interaction, virtual reality, and digital human animation. This task involves reconstructing the human body by obtaining parameters of a human body model (such as SMPL~\cite{loper2015smpl} and GHUM~\cite{xu2020ghum}) from single RGB images. Regression-based methods~\cite{guler2019holopose,kanazawa2018end,omran2018neural,pavlakos2018learning,kocabas2021spec,kocabas2021pare,li2022cliff} that predict human model parameters from image features have made significant progress in recent years and have become the leading paradigm. However, they still face several limitations.

\begin{figure*}[t]
  \begin{center}
    \includegraphics[width=0.9\linewidth]{arc.pdf}
  \end{center}
  \caption{\textbf{BoPR network architecture.} Given an input image, our method first extracts body and part features based on a soft attention mechanism. Then each part feature is concatenated with the body feature as an input token to the transformer to encode body-aware part features for camera prediction and SMPL parameter regression.}
  \label{fig:arc}
  \end{figure*}


One major limitation is the handling of severe occlusions. Occlusion occurs when a body part is obscured by another object or body part, making it difficult to estimate its pose accurately. Most existing methods based on global feature learning usually struggle with occlusions, which limits their performance in real-world scenarios. Recent work~\cite{kocabas2021pare} has attempted to alleviate this issue by leveraging neighboring visible parts to improve the estimation of occluded parts. However, this strategy is unreliable as it may mistake the pose prediction of an adjacent visible part as the occluded one in some cases, for example, when a leg is impeded mainly by one arm.

Another limitation is the handling of depth ambiguity. Depth ambiguity occurs when the relative depth of two body parts cannot be determined from a single image, leading to errors in pose estimation. This is particularly challenging in monocular RGB settings together with self-occlusion. For example, as shown in Figure~\ref{fig:title}, given a human with hands behind the back, existing methods fail to estimate a reasonable pose of the forearms. However, a human can easily infer that the hands are behind the back in this image from a global perspective by using a combination of visual cues of the whole body, such as the torso orientation, upper arms, and missing forearms. Inspired by this, we propose a body-aware part regressor that aims to learn such global visual cues to infer the accurate pose of each body part.

Our method mainly consists of the attention-guided encoder that learns feature extraction associated with soft attention and the body-aware regression that enriches part representation with part-body dependency for part regressor. 
The former, built on ConvNets, extracts body and part features based on the exact soft attention mechanism. But the difference is that the body reference feature consists of two components, auxiliary supervised by global regression of 2D and 3D key points, respectively. This design encourages the body reference feature to encode more global information, such as 2D and 3D dependencies between parts.
%The former, built on ConvNets, extracts body and part features based on the same soft attention mechanism. 
The second module encodes a body-aware feature for each part by querying the part feature from the body reference feature using a two-layer transformer. Then, the resultant features are fed into a one-layer transformer to learn per-part regression with standard losses and a reference-plane consistent loss. During training, the body-aware regression component learns the part representation and correlation with the reference body, which guides the feature encoder to learn attention that finds helpful information from the part and the whole body. Therefore, our method incorporates local visual cues around the part and related global information from the entire body, allowing inferring the spatial relationship of occluded parts with the body to improve estimation accuracy.\\
\\
Our experiments first show that our method consistently outperforms state-of-the-art approaches on 3DPW~\cite{von2018recovering} and AGORA~\cite{patel2021agora} datasets. Then we evaluate our method on 3DPW-OCC~\cite{zhang2020object} and 3DOH\cite{zhang2020object}, demonstrating its occlusion handling effectiveness. We also compare axis MJE on the 3DPW-TEST dataset. The result shows that our method's performance gain over state-of-the-art methods is attributed mainly to the much lower error of Z-axis MJE. This demonstrates that it effectively reduces depth ambiguity in estimation. Finally, our qualitative results and ablation study confirm the proposed method's effectiveness.

%Human body shape and pose estimation from monocular images has received increasing attention in the computer vision community due to its wide range of applications in areas such as human-computer interaction, virtual reality, and digital human animation. This task reconstructs the human body by obtaining parameters of a human body model (like SMPL and GHUM) from single RGB images. Regression-based methods that predict human model parameters from image features have become the leading paradigm and have made significant progress over the years. However, they still face several limitations. One major limitation is the handling of severe occlusions. Occlusion occurs when a body part is obscured by another object or body part, making it difficult to estimate its pose accurately. 
%Though recent work 
%Many existing methods struggle with occlusions, which limit their performance in real-world scenarios. Another limitation is the handling of depth ambiguity. Depth ambiguity occurs when the relative depth of two body parts cannot be determined from a single image, leading to errors in pose estimation. This is particularly challenging in monocular settings where depth information is not readily available.



% Human body 3D reconstruction has various applications in fields such as virtual reality, telepresence, human-computer interaction, and sports analysis. However, existing methods suffer from several issues, such as ambiguity in limb depth, difficulty in establishing the relationship between the body and limbs, and limited capability in multi-person scenes.

% To address these challenges, we propose a novel method called BoPR (Conditional Part Regressor). BoPR uses body characteristics as reference conditions to return the model parameters of each component while predicting the depth of each limb relative to the body plane, leading to more accurate posture and body shape prediction. Specifically, BoPR employs cascaded dimensionality reduction transformers to predict pose parameters with reference to both 2D and 3D body features, improving the accuracy of limb pose parameters and shape parameters.

% Our method has been evaluated on various benchmark datasets, including 3DPW, LSP, and MPI-INF-3DHP, and compared with state-of-the-art methods. The results demonstrate that BoPR outperforms other methods in predicting the limb pose and depth, even in challenging scenarios such as occlusion and poor perspective projection.

% In summary, our contributions include: (1) proposing a novel approach that uses body characteristics as reference conditions to predict component parameters, (2) introducing a cascaded transformer architecture to improve the accuracy of limb pose and shape parameter prediction, and (3) achieving state-of-the-art performance on several benchmark datasets. We believe our work will facilitate the development of more accurate and robust 3D human reconstruction methods.


%-------------------------------------------------------------------------
\section{Related Work}
Our proposed method is a regression-based framework for 3D human body reconstruction from a single RGB image, which can effectively tackle the issues of occlusion and depth ambiguity. Therefore we review the related works that can be divided into regression-based methods, occlusion handling, and reducing depth ambiguity.

%In this work, we focus on the task of reconstructing 3D human shape and pose from a single RGB image. To address this task, we first review previous regression methods and discuss how they attempted to solve problems such as occlusion and establishing the dependence between body parts and the whole body.

\textbf{Regression-based methods.} Estimating human mesh and pose from single images has attracted increasing attention in the computer vision community.  The mainstream methods in this task include optimization and regression-based. Optimization-based approaches concentrate on the optimization algorithms to fit parametric models (e.g., SMPL~\cite{loper2015smpl}) based on 2D observations such as keypoints and silhouettes. These methods have been criticized for having long optimization processes and high sensitivity to initialization~\cite{kolotouros2019learning}. Therefore, regression-based approaches  have become a more popular paradigm in recent years, as they can directly and fast regress parameters from image features.  HMR ~\cite{kanazawa2018end} is a milestone work that introduces reprojection loss of keypoints as weak supervision, enabling training regression models on in-the-wild datasets.  Recent work continues to explore more advanced weak supervision loss, such as introducing fine-grained correspondences ~\cite{omran2018neural,rueegg2020chained,xu2019denserac,zhang2020learning}, providing pseudo 3D ground truth ~\cite{kolotouros2019learning,joo2020exemplar}, and improving reprojection loss~\cite{kocabas2021spec,li2022cliff}. For instance, SPIN~\cite{kolotouros2019learning} incorporates the optimization process into the regression framework, allowing better-optimized 3D results as supervision for the network. SPEC~\cite{kocabas2021spec} estimates the perspective camera from a single image and utilizes it to improve reprojection loss. CLIFF~\cite{li2022cliff} proposes considering the box information and computing the reprojection loss from the original images, which facilitates predicting global rotations. However, all these methods employ global representation for regression, making them sensitive to occlusion and partially visible humans.  

%Three-dimensional human body models can be represented by a set of 3D keypoints and vertices. Model-free approaches\cite{lin2021end,lin2021mesh,moon2020i2l} can directly infer these 3D points with the help of manual inputs such as 2D keypoints and 3D template points. However, these methods are typically fragile, and the resulting inference often contains noisy or burry artifacts, leading to poor performance on outdoor datasets. In contrast, models based on SMPL\cite{loper2015smpl} parameters are more stable. SMPLify \cite{bogo2016keep} is the first automatic method to fit the SMPL model to the output of a 2D keypoint detector \cite{pishchulin2016deepcut}. HMR \cite{kanazawa2018end} extracts SMPL parameters using full-image feature regression and, for the first time, combines 2D projection loss and adversarial loss to solve the inherent depth ambiguity of single-view 2D-to-3D mapping. SPIN \cite{kolotouros2019learning} iteratively optimizes a combination of the HMR and SMPLify models to produce improved results. Kocabas et al \cite{kocabas2021pare} proposed a method that regresses the specific attention of each joint and estimates the corresponding SMPL parameters. Additionally, Lin et al. \cite{lin2021end} demonstrated that learning the correlation between non-local body parts improves the reconstruction of challenging human postures and enables effective occlusion handling. However, these methods are still limited by occlusion and depth ambiguity.

\textbf{Occlusion handling.}
Handling occlusion is challenging yet crucial for human shape and pose estimation.  Simulating data containing occlusion is a straightforward way towards this issue. This line of work attempts to generate training data of occluded human bodies by cropping images~\cite{biggs20203d,joo2020exemplar,rockwell2020full}, overlaying objects~\cite{georgakis2020hierarchical,sarandi2018robust}, and augmenting feature maps~\cite{cheng20203d}. Despite the success achieved in reducing occlusion sensitivity, the realisticness of synthetic data is usually poor, resulting in unsatisfactory performance in dealing with real occlusion data. The other line of work relies on visibility cues to aid in dealing with occlusion. Cheng et al.~\cite{cheng2019occlusion} obtains the keypoint visibility label and masks the loss of invisible points in training, encouraging the network to infer occluded joints from visible parts. Yao et al.~\cite{yao2022learning} proposes VisDB, which first trains a network to predict the coordinates and visibility label of the mesh vertex and then incorporates them to regress the SMPL parameters. However, VisDB is a two-stage framework and requires a test-time optimization procedure to obtain the final results.  In contrast, PARE~\cite{kocabas2021pare} is an end-to-end learning framework that  infers the occluded parts by exploiting the attention mechanism to find helpful information from neighboring visible parts. However, the visual cues of adjacent visible parts sometimes are unreliable or insufficient to infer the occluded parts.  Unlike these approaches, Our method leverages visible parts and body reference information to infer the occluded parts.



%Occlusion is a common problem in real-world data and poses a significant challenge for computer vision systems. Previous methods~\cite{kanazawa2018end,kolotouros2019learning} have struggled to accurately identify the visibility of each part and its location due to the difficulty of directly regressing the global region code. Although simulating occlusion through image superposition~\cite{georgakis2020hierarchical} and clipping~\cite{biggs20203d} is a common data enhancement method, it does not provide the network with the ability to truly perceive the complexity of occlusion. Our goal is to enable the model to clearly perceive the position and posture of the invisible part and restore the accurate 3D human body when dealing with occlusion.

%Wang et al.~\cite{wang20203d} predicted occlusion labels to zero occluded key points before applying time convolution to the 2D key point sequence. Cheng et al.~\cite{cheng2019occlusion} avoided including occluded joints when calculating the loss during training. However, both methods have limitations. Wang et al. obtained occlusion information through manual labeling, which is time-consuming and requires significant effort. Cheng et al. approximated the human body as a set of cylinders to obtain visibility information, but this is unrealistic and may result in inaccuracies.

%To address these limitations, PARE ~\cite{kocabas2021pare} focused on the independent regression of each part. However, the lack of overall reference information made the position of limbs ambiguous. In contrast, our method, BoPR, establishes limb dependence under the reference condition of relatively rigid torso information. BoPR perceives the depth information of the limb relative to the torso and infers the invisible part. Our approach achieves superior results and demonstrates the ability to accurately handle occlusion in real-world scenarios.


\textbf{Reducing depth ambiguity.}
Depth ambiguity is inherent in estimating 3D pose from a single RGB image. To address this issue, Wang et al.~\cite{wang2019not} predicts a set of pose attributes that indicate the relative location of a limb joint with respect to the torso, which provides an explicit prior to reduce depth ambiguity.  Yao et al.~\cite{yao2022learning} predicts the vertex's visibility in the depth axis to resolve depth ordering ambiguities. The capability of these methods to deal with depth ambiguity mainly relies on the prediction of depth-related attribute labels. However, these label estimators are reasoned with local visual cues, suffering significant instability. For example, perturbation of local pixels will lead to wrong label predictions. In contrast, our method can effectively reduce the ambiguity of parts in depth space by learning a prior of part poses referring to the body.

%\textbf{Modeling dependency between parts and body}\\
%\textbf{Interplay between torso and parts relationships.}
%Another challenging problem in reconstructing a three-dimensional human body is that many body parts are often constrained at the same time in human movement. Pose estimation methods have attempted to address this issue, with Huang et al.~\cite{wang2019not} using different weights for body parts to establish dependencies in 3D pose estimation ~\cite{zhou2016sparseness,zhou2018monocap,pavlakos2017coarse,fang2018learning}, and Sun et al.~\cite{sun2012conditional} incorporating torso information such as height or limb direction into reference to improve joint position accuracy. In our method BoPR, we extract relatively rigid torso features as reference conditions for reconstructing the 3D human body, and establish dependencies between body part features to solve the problem of depth ambiguity and improve the naturalness and accuracy of the reconstructed body.
%\textbf{Three-dimensional Virtual Human Reconstruction.} The field of three-dimensional virtual human reconstruction has been gaining popularity in recent years, but the task of reconstructing a 3D human body from a single image remains challenging due to factors such as complicated pose changes, occlusion issues, and limited 3D training data covering various human movements. Researchers have proposed different representations for the output human body, including volume space and 3D mesh. Related studies in the literature include GraphCMR and METRO, which aim to generate 3D mesh vertices and reconstruct 3D body joints and mesh vertices, respectively. Pre-trained parametric human models such as SMPL, STAR, and MANO have been used to estimate posture and shape parameters for human reconstruction, but directly estimating these parameters from an input image is difficult. Recent research has explored optimization strategies, time information, and prior information of the human body such as skeleton points or segmentation maps to improve reconstruction quality. Most parametric modeling techniques extract a general human body feature, which makes the postures of different body parts overly dependent and vulnerable to extreme occlusion and complex movements. In contrast, BoPR liberates different body parts from the global context and breaks the dependence between parts, resulting in a more natural and accurate overall reconstruction of the human body. BoPR achieves a balance between the importance of component features and body features and treats body features as conditional features, reconstructing component results by referencing body context information. By striking a careful balance between these features, BoPR achieves state-of-the-art performance in human body reconstruction from a single image, representing a significant improvement over previous methods that were limited by their over-reliance on individual parts or the global context.

%\textbf{Transformer Encoder.} The attention mechanism has been shown to be effective in various natural language tasks. Researchers have further found that focusing on the relevant input crucial to the prediction output is the key to the attention mechanism. This led to the proposal of a transformer structure based on the attention mechanism. Transformer achieves outstanding performance in large-scale language modeling and utilizes multi-head self-attention for effective training and reasoning. This has motivated researchers in machine vision to investigate the use of transformer structure for various vision tasks, including image classification, detection, and generation, all of which have shown promising results. However, no comparable framework has been proposed for the reconstruction of the human body using a parametric model.

%In typical computer vision tasks, the input part of the transformer either uses spatial encoding to merge various patches or adds spatial information to the head of global common features. However, in our work, we propose a novel input composition that feeds the transformer with a combination of patches, spatial information, and global shared features. The part feature is responsible for querying and also has its independent 3D data, and our global information includes shape prior and overall action prior. Additionally, we propose a novel cascaded dimension-reduced transformer structure for direct pose parameter regression.
%-------------------------------------------------------------------------
\section{Approach}
As depicted in Figure \ref{fig:arc}, our method mainly includes an attention-guided encoder and a body-aware regression module. The first component aims to learn proper attention maps to locate informative regions for extracting parts and body features. The second module encodes body-aware part features based on these output features and feeds them into a one-layer transformer for camera and SMPL parameter regression. We introduce our method in more detail as follows. 

 

%To tackle the challenges of human body perception from monocular RGB images, we propose a novel BoPR method, which consists of a multi-stage architecture illustrated in Figure 2. Specifically, the input RGB image is resized to 224*224 and fed into the model to obtain four outputs: a torso attention map, a part attention map, a global feature map, and a part feature map. By performing Hadamard operation between the feature volume and attention map, we obtain body perception features and independent part representations. These torso features are then connected to each part marker as a reference condition and passed through a double-layer transformer to encode and output the body-perceived component features, capturing their visibility and position relative to the body. We establish partial dependence under the body reference condition to handle the common constraint between multiple components in human motion and output 9D results comprising 6D model parameters and 3D relative depth. The relative position information of each part is explicitly monitored by the depth relative to the torso plane. Finally, after establishing correlations, the estimated parameters are fed to the SMPL model~\cite{loper2015smpl} to generate a 3D human body mesh.
%An overview of our approach, BoPR, is shown in Figure 2. The RGB image captured by the monocular camera is cropped and used as input to the model. BoPR first generates the part attention map, body attention map, 3D feature map, and 2D feature map. Then, the part features and body-reference features are obtained through the Hadamard operation. After splicing these features, the pose parameters and relative depth vectors are obtained using the cascaded dimensionality reduction transformer encoder. The shape parameters and camera parameters are directly regressed through the MLP layer. Finally, we use the regression parameters to generate a human mesh using the SMPL model.
\subsection{Preliminaries}

\textbf{SMPL Model.} Our approach utilizes the SMPL parametric model to represent the human body. The model requires two essential characteristics: pose, denoted as $\theta \in \mathbb{R}^{72}$, and shape, denoted as $\beta \in \mathbb{R}^{10}$. 
%These parameters are used to regulate the reconstruction of the human body. For this work, we use only the neutral model. Through the SMPL model, we can adjust the height, fatness, and posture of the created human body using the aforementioned factors. 
The SMPL model produces a positioned 3D mesh $\mathcal{M}(\theta,\beta)\in \mathbb{R}^{6840\times3}$ as a differentiable function. The reconstructed 3D joints are obtained as $\mathcal{J}_{3D}=\mathcal{W} \mathcal{M} \in \mathbb{R}^{J\times 3}$, where $J=24$ and $\mathcal{W}$ is the pre-trained linear regression matrix.

\subsection{Attention-guided Encoder}
%As shown on the left side of Figure \ref{fig:arc}, we introduce 
The attention-guided encoders(AGE) are designed to extract body reference and part query features from a single RGB image. We use a CNN backbone to extract feature volumes from the input image. From there, we leverage four convolutional layers to obtain the body attention map, body feature volume, part attention map, and part feature volume, respectively. %By using AGE, we can better focus on the relevant information in the input image that is beneficial for reconstructing the 3D human body.\\
%In BoPR, an attention-guided feature extractor (AFE) is utilized to extract body reference and part features by directing attention to the body area and coding the features from distinct body parts separately.\\

\noindent\textbf{Body-Reference Feature.}
%The Body Reference Encoder (BrCE) is a key module in the BoPR method for 3D human body reconstruction. After extracting volume features from the input image using a CNN backbone network, these features are fed into BrCE, which has two branch tasks. One branch outputs a torso attention map $ATT_{body}\in \mathbb{R}^{H\times W\times 1}$, which is supervised by the torso segmentation map to locate the region belonging to the human body at the pixel level. The other branch outputs two deep features $F\in \mathbb{R}^{H\times W\times C}$, which serve as reference conditions.
%The AFE module takes the volume features extracted by the CNN backbone network as input and has two branch tasks. The first branch outputs a body attention map $ATT_{body}\in \mathbb{R}^{H\times W\times 1}$, which is supervised by the body segmentation map to find the region belonging to the human torso at the pixel level. The second branch outputs two deep features $F\in \mathbb{R}^{H\times W\times C}$, which are used as reference conditions for the body features. The outputs of the two branch tasks are then processed by Hadamard operation to extract the body reference features.Finally, these body reference features are supervised by 2D and 3D key points to learn the body features that can perceive position information as reference conditions.
The first task of the AGE module is to extract body reference features. To achieve this, we pass the features extracted by the backbone network through a convolutional layer to obtain the body attention map $Att_{body}\in \mathbb{R}^{H\times W\times 1}$, which is supervised by the body segmentation map. Next, we use two additional convolutional layers to obtain a 2D feature $F_{2D}\in \mathbb{R}^{H\times W\times C}$ and a 3D feature $F_{3D}\in \mathbb{R} ^{H\times W\times C'}$. These volumes help encode more global features and establish dependencies between 2D and 3D. To aid learning, we use global 2D/3D keypoint information for supervision. Then, we concatenate the two learned features to construct a global reference feature block $F_{body}\in \mathbb{R}^{H\times W\times (C+C')}$. Finally, we extract body reference features $F_{brf}\in \mathbb{R}^{(C+C')\times 1}$ by performing Hadamard operations on the body attention map and the global reference feature block:
\begin{equation}
\begin{aligned}
  {F_{brf}}=\sigma({{Att}_{body}})^{T} \odot {{F_{body}}}\\
\end{aligned}
\end{equation}
where $\odot$ is the Hadamard product and $\sigma(Att_{body})$ is used as a soft attention mask to aggregate features. By using this approach, we can better capture the global features of the human body and establish the relationship between 2D and 3D.

\noindent\textbf{Part Feature.}
%In the part patch branch of AFE, a CNN layer is used to extract features and generate masks for various body parts using the softmax function to obtain attention maps for 24 parts and a background mask. These attention maps are represented as ${Att}_{parts} \in \mathbb{R}^{H \times W \times J}$, where each pixel indicates whether the position belongs to a part or not. In addition, this branch also predicts a collection of features for 3D body parameter prediction denoted by ${F'} \in {\mathbb{R}}^{H \times W \times C'}$. To extract part features, Hadamard operation is performed on ${Att}_{parts}$ and ${F'}$ to obtain $F_{parts} \in \mathbb{R}^{J\times C'}$ as the final output. \\
%Specifically, the two feature extraction methods are as follows:
%\begin{equation}
%\begin{aligned}
%  {F_{brf}}=\sigma({{Att}_{body}})^{T} \odot {{F}}\\
%  {F_{pf}}=\sigma({{Att}_{parts}})^{T} \odot {{F'}}
%\end{aligned}
%\end{equation}
%where $\odot$ is the Hadamard product and $\sigma(ATT_{x})$ is used as a soft attention mask to aggregate features in ${F}_{x}$, which causes different parts to focus more on their unique traits and disregard irrelevant information. The partial segmentation map is used to supervise the part feature extraction process.
Another task of the AGE module is to extract part-query features. To accomplish this, we feed the feature volume extracted by the backbone network to two convolutional layers, which respectively extract part-3D features $F_{part}\in \mathbb{R}^{H\times W\times C''}$ and part-attention maps $Att_{part}\in \mathbb{R}^{H\times W\times 24}$. The learning of attention is supervised using part-segmentation maps. Similar to the extraction process for body-reference features, we perform a Hadamard operation on the part-feature block and the part-attention map to extract part-query features$F_{pqf}\in \mathbb{R}^{C''\times 24}$. This approach allows us to focus on the relevant parts of the body and extract features that are important for accurate 3D reconstruction.

\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{RD_TT.pdf}
  \vspace{-1em}
  \caption{\noindent\textbf{Illustration of the calculation of the relative depth from the torso plane.} %Get the relative depth from the limbs to the torso plane in the front, side, and bird's-eye views.}
  %Use 3D points on the human torso to define plane equations for frontal, lateral, and bird's-eye views. Take the center of each part's vertices to calculate the distance to each plane. Bring the center point into the plane equation, and if the result is greater than 0, it is defined as a positive direction.
%  Shows the torso points for constructing the torso planes and the relative depth of the right hand.
}
  \label{rp}
\end{figure}

\subsection{Body-aware Regression}
%As depicted on the right side of Figure \ref{fig:arc}, follows the AGE module and
A body-aware regression module (BAR) is introduced to encode body-aware features for each part while establishing associations between parts. These features are then utilized as input to the regression transformer, which predicts SMPL model parameters, camera parameters, and relative torso plane depth. Finally, the model parameters in the output are fed into the SMPL model to generate a 3D human body.
 \\
%The AFE module effectively extracts the required features, but inaccurate supervision of invisible parts results in concentration around their surrounding areas, leading to incorrect reasoning. To address this, we propose the Body-Aware Regression (BAR) module, which attaches rigid torso features as reference conditions to each body part, learns body-aware part features via a double-layer transformer, and establishes conditional dependencies among parts to regress model parameters and relative depth to the torso.\\
\noindent\textbf{Body-aware Part Feature.}
The BAR module first encodes body-aware features for each part. To achieve this, we concatenate the body-reference features from the AGE module to the part-query features and establish a set of tokens $T=\{t^{1}_{bap},t^{2}_{bap},t^{3}_{bap},...,t^{23}_{bap},t^{24}_{bap} \}$(where $t^{i}_{bap}=F^{i}_{pqf}\oplus F_{brf},\oplus$ means concatenate operation.) as the input of the two-layer body-aware transformer. Using this encoder, we query part features from body-reference features to encode body-aware part features $F_{bapf}\in \mathbb{R}^{24\times 128}$, building part representations and dependencies to reference body. The resulting body-aware part feature sequences can  not only capture the unique characteristics of each part but also combine relevant global information from the entire body, which is crucial for accurate 3D reconstruction. By using the transformer to establish dependencies between the parts and reference body, our feature encoder can effectively exploit the spatial relationship between parts and bodies to generate more informative part representations. Overall, our feature encoder is capable of finding useful information from parts and the whole body and outputs a set of body-aware part features that are highly informative for the subsequent regression task. Specifically, the formula is as follows:
%The body perception component feature is a crucial component of the BAR module. It is constructed using the rigid body reference features from AFE as shared conditions, and the part features as representations. These two features are then concatenated to form a part feature query sequence $P=\{p^{1}_{bap},p^{2}_{bap},p^{3}_{bap},...,p^{23}_{bap},p^{24}_{bap} \}$(where $q^{i}_{bap}=F^{i}_{pf}\oplus F_{brf},\oplus$ means concatenate operation.) under the body attention condition.This sequence is used as input to the double-layer transformer to query the part features in the body reference features, to encode the body perception feature for each part, and to learn the spatial relationships of each body part relative to the torso, and establish initial dependencies between parts. Specifically, the formula is as follows:
\begin{equation}
\begin{aligned}
  Q &=W^{q}T;K=W^{k}T;V=W^{v}T \\
  F_{bapf} &=LN(T+LN(softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V))
\end{aligned}
\end{equation}
where $W^{x}$ is the weight, Q, K and V are the matrices after linear mapping of input, and $d_{k}$ is the sequence length which is used to transform the attention matrix into a standard normal distribution.

%Assisting parts feature perception with rigid body features as reference conditions is important for establishing preliminary dependencies and maintaining the naturalness of the entire motion. For occluded invisible parts, the two-layer encoder combines local visual clues around the part with relevant global information from the entire body to infer necessary information for reasoning. This improves the accuracy and effectiveness of the model in recognizing and generating human motion.
%\\
\noindent\textbf{Part Regressor.}
To help regression transformers better learn and perceive the correlation between parts and bodies, we propose a reference plane consistency loss. As shown in the figure~\ref{rp}, we select the shoulders, hips and pelvis points on the human torso to establish the frontal plane of the torso, the side plane of the torso and the cross-section of the torso respectively. In order to correspond to the 24 parts in the AGE module, we input the label parameters into the SMPL model to generate gt\_vertices$\in \mathbb{R}^{6890\times 3}$, and take the center of each part area as the joint point. Define the joint point to bring into the plane equation greater than 0 as the positive direction, and calculate the depth value according to the distance from the point to the plane. Use this as the ground-truth label for relative depth, and construct explicit planar consistency constraints. This constraint helps the regression transformer to learn the spatial information between parts and bodies to solve the problem of depth ambiguity.

Next, we take the output of the body-awareness encoder as the input to a single-layer regression transformer to learn dependencies among the features of each body-aware part. The final BAR module outputs a set of SMPL model parameters, camera parameters, and relative depth $RD\in \mathbb{R}^{24\times 3}$ to the torso plane. We use the standard loss and the reference plane consistency loss for each part for training the network. %Here, the regression transformer in learning local visual clues and relevant global information from the entire body to infer spatial relationships between occluded parts and the body.
%The Part Regressor module of the BAR method aims to regress the SMPL parameters from the Body-aware Part Features. In order to better perceive the depth ambiguity, we introduce explicit supervision relative to the main plane. As shown in fig. 3, we obtain the position information of the parts relative to the torso $RD\in \mathbb{R}^{24\times 3}$ by using plane equations in front, side and cross section relative to the torso. Then, we use this information in the regression model to monitor the depth ambiguity problem.Then we take the Body-aware Part Feature as input and feed it to the one-layer transformer encoder. The encoder directly returns the 6D pose parameters $\theta \in \mathbb{R}^{24\times 6}$ and the 3D relative torso plane depth. The feature sequence is flattened to return the shape parameters $\beta \in \mathbb{R}^{10}$ and camera parameters [$s,t$],$t \in \mathbb{R}^{2}$ for weak perspective projection.

%Explicit supervision of the relative position information of each part with respect to the torso plane helps to perceive the relative positions of different body parts. This information is then used to solve the depth ambiguity problem in the regression model. A single-layer transformer encoder is used to establish conditional dependencies between body parts, which are used to infer the pose of occluded parts and maintain overall naturalness, thus obtaining more accurate SMPL parameters.
%To address the depth ambiguity between limbs, we present a novel approach that utilizes a set of torso plane equations to estimate the relative depth information of 24 body parts with respect to the torso.

%Our method exploits the depth of the human torso as a reference point and aims to learn the relative depth of the limbs with respect to the torso plane. This effectively resolves the issue of limb depth ambiguity that arises due to the fixed viewing angle of a monocular camera. We consider three commonly encountered viewpoints, namely, frontal, lateral, and bird's-eye perspectives, and leverage the SMPL model to obtain the 3D key points $J_{smpl} \in \mathbb{R}^{N \times 3}$ after root node alignment, which facilitates the establishment of the torso plane equations.

%As illustrated in Figure 3, we first construct the frontal plane equation by utilizing three points from the left and right shoulders and the centers of the left and right hips. Subsequently, we compute the side planes by utilizing the center of the left and right shoulders, the center of the left and right hip, and the pelvis, while the bird's-eye view plane is formed by the left and right hip and pelvis. We then combine these planes to establish the torso plane equations, with the front, right, and above directions being defined as positive directions. The relative depth value is obtained by computing the mean distance of each point on the SMPL model's output $V_{smpl} \in \mathbb{R}^{6890 \times 3}$ to the corresponding plane. The formula for computing the relative depth is expressed as:
%\begin{equation}
%\begin{aligned}
%  {Rd=\frac{{|A\times \tilde{M}_{x}+B\times \tilde{M}_{y}+C\times \tilde{M}_{z}+Z|}}{\sqrt{A^{2}+B^{2}+C^{2}}}}
%\end{aligned}
%\end{equation}
%To obtain the relative depth of each part with respect to the torso plane, we use the normal vectors $A,B,C$ corresponding to the torso plane and the center point $\tilde{M}$ of the vertices of each part. By computing the mean distance from each point on the output mesh $V_{smpl}$ to its corresponding torso plane, we obtain the specific relative depth value for each part. This formulation enables us to effectively solve the problem of depth ambiguity between limbs caused by the fixed viewing angle of a monocular camera.
%-------------------------------------------------------------------------
%\subsection{Body-Reference Token $\&$ Part Tokens}

%As depicted in the left part of Figure 2, we feed the image feature F extracted by the backbone network into five separate convolutional layers to obtain the part attention map $M_{part}$, torso attention map $M_{torso}$, 3D part feature $F_{part}$, and 2D/3D torso reference feature $F^{2D/3D}_{torso}$. These components play a crucial role in capturing and representing the human body's features and characteristics, which are then used to generate a human mesh through the SMPL model. This process effectively addresses the challenge of depth ambiguity by learning the relative depth of the limbs to the torso plane, as explained in the previous section. Overall, BoPR provides a promising solution for single-image 3D human body reconstruction, and our experimental results demonstrate its effectiveness and robustness.


%\textbf{Part/Torso Attention Map.} We generate the attention map for each of the 24 parts and a global torso using $M_{part} \in \mathbb{R}^{24\times H\times W}$ and $M_{torso} \in \mathbb{R}^{1\times H\times W}$, respectively. We obtain the 3D keypoints and vertices of the human body in the camera coordinate system using the ground truth parameters pose and theta as inputs to the SMPL model. We then inverse calculate the 3D and corresponding 2D keypoints to solve the projection matrix. Next, we project the vertices into pixel coordinates to generate 2D segmented part maps, which are used to supervise the learning of attention maps. Additional details on the metrics used can be found in the supplementary materials.

%We perform a Hadamard operation on the attention maps $M_{part}$ and $M_{torso}$ with the feature map $F\in \mathbb{R}^{C\times H\times W}$ to obtain the token of the parts $T_{parts}\in \mathbb{R}^{24\times C'}$ and the token of the torso $T^{2D/3D}{body}\in \mathbb{R}^{1\times C^{2D/3D}}$. This operation allows us to extract relevant information from the feature map for each part and the torso. We define $T{x}$ as the token for each body part $x$ and compute it as a weighted sum of the feature map, with weights determined by the corresponding attention map $M_x$.

%Specifically, the computation of $T_{x}$ can be written as a Hadamard product between the attention map $M_{x}$ and the feature map $F$, followed by a global average pooling operation across the spatial dimensions of the resulting tensor. This produces a single vector for each body part that represents its relevant features. Similarly, the token for the torso $T^{2D/3D}{body}$ is computed using the attention map $M{torso}$ and the feature map $F$.

%The tokens $T_{parts}$ and $T^{2D/3D}{body}$ are then concatenated to form the final representation $T{final} \in \mathbb{R}^{(24+C^{2D/3D})\times C'}$ of the human body. This representation captures both the fine-grained details of each body part and the overall context provided by the torso. We use this representation for the subsequent 3D pose estimation and depth regression tasks.



%\subsection{Cascaded Dimensionality Reduction Transformers Conditioned On Body-reference}

%We propose a new cascaded dimensionality reduction transformer architecture conditioned on body reference features to predict 9D parameters, including 6D pose parameters and 3D relative torso plane depth. Our approach differs from existing transformer models, which use constant embedding for all layers, making them unsuitable for our needs. To address this issue, we adopt a novel cascade dimensionality reduction architecture, as illustrated in Figure 2, that reduces the dimension of the input using linear projection while simultaneously splicing the body reference feature onto the component feature as an additional input. By alternating between self-attention and dimension reduction across multiple encoder layers, the model can predict the pose parameters and relative torso plane depth of 24 parts.

%In particular, we splice the body reference feature into the first dimension of the part feature and refer to each input token as a patch, following ViT terminology. First, we splice the 2D and 3D body reference features and reduce the dimension to match that of the part token. Then, we splice the body reference feature into the first dimension of the part token, generating a set of inputs denoted by $P_J$ = {$p_{br}$, $p_1^J$, $p_2^J$, $p_3^J$, ..., $p_{23}^J$, $p_{24}^J$}. Finally, we flatten this set of inputs to regress the body shape $\beta \in \mathbb{R}^{10}$ and a weak-perspective camera model with scale and translation parameters [$s$, $t$], $t \in \mathbb{R}^{2}$. The formula used in the transformer encoder is as follows:
%\begin{equation}
%\begin{aligned}
%Q=W^{q}P_{J};K=W^{k}P_{J};V=W^{v}P_{J} \\
%output=LN(P_{J}+LN(softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V))
%\end{aligned}
%\end{equation}
%where $Q,K,V \in \mathbb{R}^{25\times d_{k}}$ and output$\in \mathbb{R}^{24\times 9}$, with the first six dimensions representing the pose parameters and the last three dimensions representing the depth relative to the torso plane.


%\subsection{Global Location Information Awareness}

%We observed that there is a depth ambiguity not only between limbs and torso but also between people in multi-person scenarios. As shown in the upper part of Figure 4, supervising only the cropped 2D projection points results in the loss of position information of the human body in the whole image, leading to direct position overlap between people.

%To address this issue, we transform the 2D projection into the full image projection $J^{full}{2d} \in \mathbb{R}^{J\times 2}$ and map the output of the 2D torso features in the left part of Figure 2 back to the original image. We then use the human body center and pixel ratio to derive the holistic information $H{info} \in \mathbb{R}^{3}$, which is fed to the transformer encoder. This enables more accurate prediction of global rotation and full image depth.

\subsection{Loss Functions}
We formulate the training of BoPR as a supervised learning problem, in which we aim to minimize the discrepancy between the predicted outputs and the ground truth annotations. To this end, we define a set of loss functions that capture different aspects of the model's performance. Given a training dataset $D = \{{I^{i},\hat{\theta}^{i},\hat{\beta}^{i},\hat{J^{i}_{3D}},\hat{J^{i}_{2D}},\hat{RD}^{i}}\}_{i=1}^{N}$, where $N$ is the total number of training images, $I \in \mathbb{R}^{w \times h \times 3}$ denotes the RGB image, $\hat{\theta}\in \mathbb{R}^{24 \times 3},\hat{\beta} \in \mathbb{R}^{10}$ represent the ground truth SMPL parameters, $\hat{RD} \in \mathbb{R}^{24\times 3} $ represents the relative depths of points corresponding to 24 SMPL parts to three torso planes, $\hat{J_{3D}} \in \mathbb{R}^{J \times 4}$ denotes the ground truth 3D coordinates and its confidence of the body joints, where $J$ is the number of joints of a person, and $\hat{J_{2D}} \in \mathbb{R}^{J \times 3}$ denotes the ground truth 2D coordinates of the body joints. We also generate $\hat{M_{P}} \in \mathbb{R}^{24 \times H \times W }$ and $\hat{M_{B}} \in \mathbb{R}^{1\times H \times W}$, which serve as segmentation labels for whole bodies and ground truth bodily parts, respectively.

As an additional step for supervising body and parts attention, we project the vertices generated by the SMPL model onto pixel coordinates to generate segmentation labels ${Seg_{body}} \in \mathbb{R}^{1\times H\times W}$ and ${Seg_{parts}} \in \mathbb{R}^{24\times H\times W}$. This allows us to supervise attention learning at the pixel level, providing more detailed information for evaluating model performance.

Let $\theta$ denote the pose parameters and $\beta$ is the shape parameters, we use SMPL model output $V_{3d} \in \mathbb{R}^{6890 \times 3}$ which denotes the body vertices.To compute the keypoint loss,we need the SMPL 3D joints $J_{3D}(\theta,\beta)=WV_{3d}$ with a pretrained linear regressor $W$.With the inferred weak-perspective camera,we compute the 2D projection of the 3D joints $J_{3D}$,as $J_{2D} \in \mathbb{R}^{J \times 2}=s\Pi(RJ_{3D})+t$,where $\mathbb{R}\in SO(3)$ is the camera rotation matrix and $\Pi$ is the orthographic projection.

We use the combination of loss functions to train BoPR, including AFE loss and BAR loss:
\begin{equation}
\begin{aligned}
L_{total} = L_{AFE}+L_{BAR}
\end{aligned}
\end{equation}
where each term is calculated as:
\begin{equation}
\begin{aligned}
L_{AFE} &= L_{Aux2D}\!+\!L_{Aux3D}\!+\!\lambda_{Att}(L_{b\_seg}\!+\!L_{p\_seg})\\
L_{BAR} &= L_{2D}+L_{3D}+L_{SMPL}+L_{RD}\\
L_{2D} &= \lambda_{2D}||J_{2D}-\hat{J_{2D}}||_{F}^{2}\\
L_{3D} &= \lambda_{3D}||J_{3D}-\hat{J_{3D}}||_{F}^{2}\\
L_{SMPL} &= \lambda_{SMPL}||\Theta-\hat{\Theta}||_{2}^{2}\\
L_{x\_seg} &= \!\frac{1}{HW}\!\sum\limits_{h,w}\limits^{}\!CrossEntropy\!(\!\sigma(Att_{h,w})\!,\!{Seg_{h,w}}\!)\\
L_{RD} &=\lambda_{RD}||RD-\hat{RD}||_{2}^{2}\\
L_{Aux2D} &= \lambda_{2D}||J_{Aux2D}-\hat{J_{2D}}||_{F}^{2}\\
L_{Aux3D} &= \lambda_{3D}||J_{Aux3D}-\hat{J_{3D}}||_{F}^{2}\\
\end{aligned}
\end{equation}
where $\hat{X}$ represents the ground truth for the corresponding variable $X$. In addition, auxiliary supervision losses are $L_{Aux2D}$, $L_{Aux3D}$ and $L_{x\_seg}$. Among them, for $L_{x\_seg}$, we only supervise on the COCO-EFT dataset. While body reference features are supervised for all training epochs.




% \begin{figure}
%   \centering
%   \includegraphics[width=1\linewidth]{full2dinfo.pdf}
%   \caption{Figure 4:This is the comparison of our method in the multi-person scene with or without using full-image 2D keypoint information (the bottom is the result of using full-image information). It can be seen that adding global information to Token makes our results predict better global rotation and depth.}
%   \label{fig2}
% \end{figure}

%We use a combination of loss functions to train BoPR, including the SMPL loss, the 2D joint loss, the attention map loss, and the segmentation loss. The SMPL loss measures the discrepancy between the predicted and ground truth SMPL parameters, and is defined as:

%\begin{equation}
%\begin{aligned}
%L_{SMPL} = \lambda_{SMPL}(|| \theta - \hat{\theta} ||_{2}^{2} + || \beta - \hat{\beta} ||_{2}^{2})
%\end{aligned}
%\end{equation}

%The 3D joint loss measures the distance between the predicted and ground truth 3D joint coordinates and is defined as:
%\begin{equation}
%\begin{aligned}
%L_{3D} = \lambda_{3D}(|| J_{3D} - \hat{J_{3D}} ||_{F}^{2})
%\end{aligned}
%\end{equation}

%The attention map loss measures the discrepancy between the predicted and ground truth segmentation maps, and is defined as:
%\begin{equation}
%\begin{aligned}
%L_{seg} = \lambda_{seg}(\frac{1}{HW}\sum\limits_{h,w}\limits^{}CrossEntropy(\sigma(M_{h,w}),\hat{M_{h,w}}))
%\end{aligned}
%\end{equation}


%To estimate the pose parameters $\theta$ and shape parameters $\beta$ of the SMPL model and generate the body vertices $V_{3d} \in \mathbb{R}^{6890 \times 3}$, we employ a weak-perspective camera and a pretrained linear regressor $W$ to obtain the 3D joint locations $J_{3D}(\theta,\beta) = WV_{3d}$. The 2D projection of the 3D joints $J_{3D}$ is computed using the weak-perspective camera as $J_{2D} \in \mathbb{R}^{J \times 2} = s\Pi(RJ_{3D}) + t$, where $\mathbb{R} \in SO(3)$ is the camera rotation matrix and $\Pi$ is the orthographic projection.

%We use the keypoint loss to evaluate the performance of our method, which is computed based on the difference between the predicted 2D joints $J_{2D}$ and the ground truth joints $J_{GT}$, given by:
%\begin{equation}
%\begin{aligned}
%L_{2D} = \lambda_{2D}(|| J_{2D}- \hat{J_{2D}}||_{F}^{2})
%\end{aligned}
%\end{equation}
%Our total loss function combines the keypoint loss with the shape and pose regularization terms as follows:
%\begin{equation}
%\begin{aligned}
%{\mathcal{L}_{total}=\mathcal{L}_{3D}+\mathcal{L}_{2D}+\mathcal{L}_{SMPL}+\lambda_{seg}\mathcal{L}_{seg}+\mathcal{L}_{RD}}
%\end{aligned}
%\end{equation}
%where the loss of depth information of relative driving plane is calculated as:
%\begin{equation}
%\begin{aligned}
%    {\mathcal{L}_{RD}=\lambda_{RD}(||RD-\hat{RD}||_{2}^{2})}
%\end{aligned}
%\end{equation}


%In our method, we utilize body reference features as well as 2D and 3D supervision to balance the relationship between component features and body features. Specifically, let $X'$ denote the 2D and 3D supervision of body reference features and $\hat{X}$ represent the ground truth for the corresponding variable $X$. Moreover, we use $M_{h,w} \in \mathbb{R}^{1 \times 1 \times (J+1)}$ and $M'{h,w} \in \mathbb{R}^{1 \times 1 \times 2}$ to denote the fiber of $M{part}$ and $M_{torso}$ at the location $(h,w)$, respectively. Additionally, $\hat(M){h,w} \in \mathbb{R}^{1 \times 1 \times (J+1)}$ denotes the ground-truth part label at the same location, expressed as a one-hot vector. Here, $\lambda$ is a scalar coefficient used to balance the loss terms. The part segmentation loss $\mathcal{L}{P}$ is defined as the cross-entropy loss between $M_{h,w}$ after softmax and $\hat{P}_{h,w}$, which is averaged over $H \times W$ elements.

%Our method uses this part segmentation loss to reconstruct component results by referring to body context information. This approach helps to establish the connection between different body parts, which in turn makes the whole action more natural and accurate. Furthermore, our method regards body features as conditional features, which allows us to reconstruct the component results while considering the body context. Overall, our proposed approach achieves state-of-the-art results on a variety of benchmarks, demonstrating the effectiveness of our approach in the task of human pose estimation.

\begin{table*}
	\centering
	% \small
	\begin{tabular}{lcccccc}
        \toprule[1.5pt]
		\multirow{2}{*}{Method} & \multicolumn{3}{c}{3DPW-ALL}   & \multicolumn{3}{c}{3DPW-TEST} \\
		& MJE$\downarrow$ & PAMJE$\downarrow$ & PVE$\downarrow$ & MJE$\downarrow$ & PAMJE$\downarrow$ & PVE$\downarrow$  \\ \hline
		VIBE~\cite{kocabas2020vibe} & 93.5 & 56.5 & 113.4 &82.7 &51.9 &99.1\\ 
		MEVA~\cite{luo20203d} & 86.9 & 54.7 & - &- &- &- \\
  MAED~\cite{wan2021encoder} & - & - & - &79.1 & 45.7 &92.6 \\ 
  \hline
		Pose2Mesh~\cite{choi2020pose2mesh} & 89.2 & 58.9 & -& 89.5 & 56.3&105.3 \\
		I2L-MeshNet~\cite{moon2020i2l} & 93.2 & 58.6 & - & -& -&- \\
            METRO~\cite{lin2021end} &- &- &- &77.1 &47.9 &88.2\\
            Mesh-G~\cite{lin2021mesh} &- &- &- &74.7 &45.6 &87.7\\
            HybrIK~\cite{li2021hybrik} & 80.0 & 48.8 & 94.5 & 74.1 & 45.0 & 86.5\\
            FastMETROLH64~\cite{cho2022cross} &- &- &- &73.5 &44.6 &84.1\\
            \hline
		SPIN~\cite{kolotouros2019learning} & 96.9 & 59.2 &116.4 & -& -&-\\
		HMR-EFT~\cite{joo2020exemplar} &85.1 & 52.2 & 118.5 &- &- &-\\
		ROMP~\cite{sun2021monocular} & 82.7 & 60.5 & - & 76.7 & 47.3 & 93.4\\
		PARE~\cite{kocabas2021pare} & 82.0 & 50.9 & 97.9 & 74.5 & 46.5 & 88.6\\ 
            VisDB~\cite{yao2022learning} &- &- &- & 72.1 &44.1 &83.5\\
            CLIFF(HR-W48)~\cite{li2022cliff} & - & - & - & 69.0 & 43.0 & 81.2\\ \hline
		BoPR(HR-W32) & 76.1 & 45.9 & 90.9 & 68.8 & \textbf{41.8} & 81.7  \\
           % BoPR$+$ & 75.0 & 45.9 & 90.0 & 67.8 & \textbf{41.2} & \textbf{80.0}  \\
		BoPR(HR-W48) & \textbf{72.0} & \textbf{45.3} & \textbf{86.5} & \textbf{65.4} & 42.5 & \textbf{80.8} \\ 
        \bottomrule[1.5pt]
	\end{tabular}
 %\label{tab:3dpw}
	\caption{\textbf{Performance comparison on the 3DPW dataset.} For the evaluation on 3DPW-ALL, all methods are trained without 3DPW dataset. However, for the one on 3DPW-TEST, all methods involve 3DPW training sets in training. }
 %Following Protocol 1(without fine-tuning on the training set) and Protocol 2(training with 3DPW-Train),BoPR outperform temporal,multi-stage,and single-stage state-of-the-art methods."-" shows the results that are not available."*" indicates the method using HRNet-48 and global location information."+" means cascading a layer of BAR modules.}
	\label{tab:3dpw}
\end{table*}


%-------------------------------------------------------------------------


\section{Experiments}

\noindent\textbf{Implementation Details.}
In our implementation, the input size of the network is $224 \times 224$. We use standard data augmentation practices during training, including random rotations, scaling, horizontal flipping, and cropping. Also, we adopt the widely used Adam optimizer to train the network for 240K steps with a batch size of 64 and a learning rate of $5 \times e^{-5}$. 
%To enhance the performance of BoPR, we also pre-train the backbone on the ImageNet dataset for classification and on the MPII and COCO datasets for 2D key point detection. Prior to being fed into the network, the cropped images are resized to $224 \times 224$ pixels. To augment the data, we apply random rotations, scaling, horizontal flipping, and cropping.It is noteworthy that unlike recent multi-stage methods, BoPR can be trained end-to-end in a one-stage manner.


\noindent\textbf{Datasets.}
For the experiments on the 3DPW dataset, we used four large-scale datasets for training, including COCO-EFT (74K)~\cite{lin2014microsoft}, MPII (14K)~\cite{andriluka20142d}, MPI-INF-3DHP (90K)~\cite{mehta2017monocular}, and Human3.6M (292K)~\cite{ionescu2013human3}. We first pre-trained the network on COCO-EFT (74K) and then sequentially fine-tuned it on the mixed and 3DPW datasets. After fine-tuning the network on the 3DPW datasets, we further fine-tuned the model on Agora~\cite{patel2021agora} and evaluated its performance on the Agora test set.
%For the in-the-wild datasets, pseudo-ground-truth SMPL annotations~\cite{joo2020exemplar} are used for supervision. 
%To evaluate the 3DPW dataset, we first pre-train the network on COCO-EFT (74K) and fine-tune it on the mixed dataset. %During training, we only monitor the attention map on the COCO dataset and continuously monitor the relative depth of the torso plane to improve the model's performance.

%We also introduce Agora~\cite{patel2021agora}, a synthetic dataset for multi-person scenes. Using the evaluation technique of SPIN, we evaluate BoPR on the multi-person Agora benchmark using fine-tuned and unadjusted results. During training, we do not use any data augmentation and construct cropped bounding boxes using the official mask labels.

\begin{table}
\centering
% \small
\begin{tabular}{lcccc}
\toprule[1.5pt]
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Agora}  \\ 
 & MJE$\downarrow$  & V2V$\downarrow$  \\ \hline
SPIN~\cite{kolotouros2019learning} & 153.4 & 148.9 \\ 
PARE~\cite{kocabas2021pare} & 146.2 & 140.9 \\ 
ROMP~\cite{sun2021monocular} & 108.1 & 103.4 \\
BEV~\cite{sun2022putting} & 105.3 & 100.7 \\ 
Hand4Whole~\cite{sun2022putting} & 89.8 & 84.8 \\ 
CLIFF~\cite{li2022cliff} & 81.0 & 76.0 \\ 
\hline
BoPR & \textbf{79.9} & \textbf{74.5}  \\ \bottomrule[1.5pt]
\end{tabular}
\caption{\textbf{Performance comparison on the AGORA dataset.} Here, all the comparing methods adopt weak perspective projection and also fine-tune the model on the AGORA training set (except PARE).}
\label{tab:agora}
\end{table}

\noindent\textbf{Evaluation Metrics.}
We evaluate our method using three common metrics: MJE~\cite{ionescu2013human3}, PAMJE~\cite{gower1975generalized}, and PVE~\cite{pavlakos2018learning}. These metrics calculate millimeter-scale Euclidean distances between 3D points in predicted and ground-truth data. We also report the results of axis MJE (x-MJE, y-MJE, and z-MJE) to analyze the detailed improvements of each axis.




% \textbf{MJE} calculates distances between the predicted and ground-truth 3D joints at the pelvis, comprehensively assessing the projected postures and forms, including global rotations.

% \textbf{PA-MJE} performs Procrustes alignment before computing MJE, mainly measuring the articulated poses and eliminating discrepancies in scale and global rotation.

% \textbf{PVE} does the same initial alignment as MJE but calculates the vertices' separation from the human mesh surfaces, providing a measure of shape accuracy.



\noindent\textbf{Comparison to the state-of-the-art.}
We first comprehensively compare BoPR with three types of state-of-the-art methods, including video-based, model-free, and model-based methods on the 3DPW dataset. In this experiment, we report two evaluation results, namely 3DPW-ALL and 3DPW-TEST. Here,  all methods do not include the 3DPW training dataset in training to evaluate the 3DPW-ALL setting, while for the 3DPW-TEST, they all fine-tune the network on the 3DPW training dataset. In addition, we provide the results of two network structures (HR-W48 and HR-W32) on these two settings, respectively. Table~\ref{tab:3dpw} presents the comparison results with various methods. Our method's performance using the HR-W32 backbone network has surpassed almost all existing methods in all metrics but is only slightly lower PVE than the CLIFF method using the HR-W48 backbone network. However, when using the HR-W48 backbone network, our method significantly outperforms the previous state-of-the-art method CLIFF on 3DPW-TEST, and MJE is reduced from 69.0 to 65.4. It is also worth mentioning that the evaluation results in 3DPW-ALL can better reflect the generalization ability of the method. Yet, in this setting, our method also outperforms the previous state-of-the-art method HybrIK by a large margin (MJE from 80.0 to 72.0).



\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{att.pdf}
  \caption{\noindent\textbf{The role of body-reference feature.} From left to right: input image, body attention map, the result of discarding body reference features, and the result of the full model.}
  \label{fig:att}
\end{figure}

\begin{table}
\centering
% \small
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule[1.5pt]
\multirow{2}{*}{Method} & \multicolumn{4}{c}{3DPW-Test}  \\ 
 & MJE$\downarrow$ & MJE$_{x}$$\downarrow$ & MJE$_{y}$$\downarrow$ & MJE$_{z}$$\downarrow$  \\ \hline
PARE(HR-W32) &74.5 &30.3 &28.9 &78.9 \\ 
CLIFF(HR-W48) &72.7 &27.1 &27.0 &51.8 \\ \hline
BoPR(HR-W32) &68.8 &25.9 &27.5 &\textbf{46.6} \\
BoPR(HR-W48) &\textbf{65.1} &\textbf{22.0} &\textbf{23.4} &48.1 \\\bottomrule[1.5pt]
\end{tabular}
}
%\vspace{-1em}
\caption{\textbf{Axis-wise performance comparison on the 3DPW-Test.} All methods have been trained on dataset with 3DPW.}
\label{tab:occ}
\end{table}


\begin{table}
\centering
% \small
\begin{tabular}{lcccc}
\toprule[1.5pt]
\multirow{2}{*}{Method} & \multicolumn{2}{c}{3DPW-OCC} & \multicolumn{2}{c}{3DOH} \\ 
 & MJE$\downarrow$ & PAMJE$\downarrow$ & MJE$\downarrow$ & PAMJE$\downarrow$ \\ \hline
PARE(HR-W32) &84.9 &57.5 &94.9 &62.4  \\ 
CLIFF(HR-W48) &- &- &80.4 &58.7 \\\hline
BoPR(HR-W32) &81.7 &49.5 &82.7 &\textbf{55.5} \\
BoPR(HR-W48) &\textbf{74.6} &\textbf{48.3} &\textbf{78.3} &56.3 \\\bottomrule[1.5pt]
\end{tabular}

\caption{\textbf{Performance comparison on occlusion datasets 3DPW-OCC and 3DOH.} For the evaluation of 3DPW-OCC, all methods are trained without the 3DPW dataset. While for 3DOH, all methods fine-tune the network on the 3DPW training set.}
\label{table1}
\end{table}

\begin{table}
\centering
% \small
\begin{tabular}{lcccc}
\toprule[1.5pt]
\multirow{2}{*}{Method} & \multicolumn{3}{c}{3DPW}  \\ 
 & MJE$\downarrow$ & PAMJE$\downarrow$ & PVE$\downarrow$  \\ \hline
PARE &91.0 &56.7 &108.2 \\ 
PARE w TF &91.5 &55.7 &108.6\\ \hline
BoPR w/o $L_{3D}$ &89.4 &55.8 &106.7\\
BoPR w/o $L_{2D}$ &90.6 &54.8 &107.2\\
BoPR w/o $L_{RD}$ &89.0 &54.2 &105.7\\
BoPR &\textbf{88.6} &\textbf{53.7} &\textbf{105.3}\\
\bottomrule[1.5pt]
\end{tabular}

\caption{\textbf{Ablation study of BoPR on 3DPW}.}
%All methods are trained on COCO-EFT-PART(22K)."TF" refers to the same transformer architecture used in our method, "$L_{2D/3D}$" represents the absence of 2D/3D constraints on body-reference features."$L_{RD}$" means relative torso plane depth supervision. "w" denotes the inclusion of a module. In contrast "w/o" represents its exclusion.}
\label{tab:abl}
\end{table}

\begin{table*}
\centering
% \small
%\resizebox{0.5\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule[1.5pt]
\multirow{2}{*}{Method} & \multicolumn{1}{c}{Elbow} & \multicolumn{1}{c}{Wrist} & \multicolumn{1}{c}{Head}  & \multicolumn{1}{c}{Knee} & \multicolumn{1}{c}{Ankle} & \multicolumn{1}{c}{Hip} & \multicolumn{1}{c}{Neck} & \multicolumn{1}{c}{Shoulder}\\ 
 & MJE$\downarrow$ & MJE$\downarrow$ & MJE$\downarrow$ & MJE$\downarrow$  & MJE$\downarrow$  & MJE$\downarrow$  & MJE$\downarrow$  & MJE$\downarrow$\\ \hline
PARE &79.2 &110.5 &93.8 &70.3 &120.8 &29.1 &64.6 &63.2 
\\ \hline
%BoPR(w/o BrF) &79.5 &120.1 &88.7 &65.3 \\
BoPR &\textbf{69.3} \textcolor{green}{(-9.9)} &\textbf{98.6} \textcolor{green}{(-11.9)} &\textbf{74.5} \textcolor{green}{(-19.3)} &\textbf{61.9} \textcolor{green}{(-8.4)} &\textbf{107.7} \textcolor{green}{(-13.1)} &\textbf{28.3} \textcolor{green}{(-0.8)} &\textbf{53.7} \textcolor{green}{(-10.9)}&\textbf{53.6} \textcolor{green}{(-9.6)}\\\bottomrule[1.5pt]
\end{tabular}
%}
\caption{\textbf{Per-part performance comparison on the 3DPW-Test.} All methods have been trained on dataset with 3DPW.}
\label{tab:perpart}
\end{table*}



We also evaluate our method on a multi-person dataset AGORA and compare it with state-of-the-art methods. Furthermore, since camera parameter estimation plays a crucial role in the performance of the AGORA dataset,  we compare our method with those using weak-perspective projection in training for a fair comparison. Table~\ref{tab:agora} shows the result where our method performs better than multi-person-based approaches and previous state-of-the-art method CLIFF.

%Table 1 comprehensively compares our BoPR and previous single RGB image HPS estimation methods. We evaluated the effectiveness of BoPR on the 3DPW dataset using protocols 1 and 2. To ensure a fair comparison, we used the bounding box information provided in the ground truth labels to crop the input images. Our BoPR method outperforms all other methods, especially in terms of MJE and PA-MJE metrics. We observed that the cascading BAR structure further improves the BoPR performance when the output of BAR modules is used as new position information to generate new query queues, with a 1.4\% and 2.1\% improvement in MJE and PVE, respectively. Additionally, we used hrnet-48 as the backbone network and mapped the global 2D regression points in AFE back to the original image to provide global information throughout the entire image, which further improved the BoPR performance. Compared to the state-of-the-art method CLIFF~\cite{li2022cliff}, our BoPR method achieved a 5.5\% improvement in MJE.Furthermore, Table 2 illustrates the performance of our method on the multi-person synthetic test set Agora. The experimental results demonstrate that BoPR achieves the best performance among the methods that do not employ kinematic optimization.

%\subsection{Evaluation on Agora Dataset}
%We evaluate BoPR on the multi-person Agora benchmark using both fine-tuned and unfine-tuned models according to the SPIN evaluation protocol. We use the official mask label to build the bounding box for cropping the input image, and do not use any data augmentations during training. As shown in Table 3, BoPR outperforms state-of-the-art methods in all evaluation metrics, even in cases of poor perspective projection.

%\subsection{Results}

%\subsection{Analysis and Discussion}





\noindent\textbf{Improved occlusion handling.}
Table ~\ref{tab:occ} showcases the results of our proposed BoPR method, the baseline PARE method, and the CLIFF method on the occlusion datasets 3DPW-OCC and 3DOH. All methods were evaluated on 3DPW-OCC without using the 3DPW training dataset. When evaluating 3DOH, they fine-tuned the network on the 3DPW training dataset. Both evaluations excluded their own training set to ensure a better comparison of the generalization ability on occlusion datasets. BoPR outperformed PARE in both MJE and PAMJE and showed superior performance compared to CLIFF and PARE on 3DOH. Specifically, on the 3DPW-OCC dataset, BoPR reduced MJE and PAMJE by 5.8\% and 10.9\%, respectively, compared to PARE. On 3DOH, BoPR achieved a 2.6\% improvement over CLIFF. These results suggest that part-aware regression with body-reference conditioning enhances occlusion handling and improves accuracy and generalization in challenging scenarios.



\noindent\textbf{Reduced depth ambiguity.}
To further analyze the issue of depth ambiguity in 3D human reconstruction, we evaluate the average per joint position error (MJE) in the x, y, and z axis on the 3DPW-Test dataset. As shown in Table 4, we can see that the error on the z-axis is significantly higher than that on the x and y-axis in existing methods. This indicates that the depth ambiguity problem in 3D human reconstruction is a significant challenge that needs to be addressed. However, our approach outperforms the state-of-the-art methods by a significant margin in the z-axis metric, achieving an 11.1\% improvement compared to CLIFF. This result highlights the importance of body-aware feature encoding, which enables the inference of spatial relationships between body and parts through local visual clues around body parts and relevant global information from the whole body, effectively alleviating the problem of depth ambiguity.


%BoPR handles this issue by using the BAR module to encode body-aware features as a reference condition for each body part to learn relative spatial information, while explicitly supervising the depth of the relative torso plane. Our method successfully mitigates this problem and achieves performance superior to state-of-the-art methods on benchmark datasets.
\begin{figure*}
  \centering
  \includegraphics[width=0.95\linewidth]{quali.pdf}
  \vspace{-1em}
  \caption{Qualitative comparison of PARE, CLIFF, and our method BOPR on in-the-wild datasets including LSPET (Row1-3) and 3DPW (Rows4-5).}%From left to right: input image, (a) PARE\cite{kocabas2021pare} result, (b) CLIFF\cite{li2022cliff} result, (c) BOPR result}
  \label{fig:qua}
\end{figure*}

\noindent\textbf{Does Body-aware features help?}
To understand the role of body-aware features, we first visualize the attention map that produces body-reference features in Figure~\ref{fig:att}. We can find that these attention maps mainly focus on the torso area of the human body and have strong attention to the upper spine, shoulders, and hips. Interestingly, these body parts are crucial in determining global information, such as orientation, body shape, and overall body posture. In addition, we also compare the prediction results after zeroing the reference features with the original method. As shown in the last two columns of Figure~\ref{fig:att}, after the reference feature is set to 0, the global scale of the mesh and the prediction of the relative position relationship of the parts relative to the body significantly deteriorated. This suggests that body-aware features encode global scale information and component-body dependencies.

\noindent\textbf{Which part benefits most?} To understand the performance improvement of our method for each part, we provide per-part performance comparison results in Table~\ref{tab:perpart}. We can see that compared with PARE, our approach has a slight error drop for the Hip joint, but the error drop for other joints is all above 10mm. Moreover, the errors of challenging joints such as the Wrist and Ankle reduce by 12mm and 13mm, respectively. The most significant error drop (by 19mm)  is the Head joint.

%As depicted in Figure ~\ref{fig:att}, removing body-reference feature attention results in a significant drop in BoPR performance. Without the body-reference condition, the reconstructed 3D human body exhibits issues like limb dislocation, mold penetration, and depth ambiguity. To further analyze the impact of body-aware encoders, we evaluate the error for each limb on the 3DPW test data, and the results are presented in Table ~\ref{tab:limb}. After encoding the body-aware features for each part, although the near-torso limbs, such as hips, show little difference, the improvement is evident in the far-torso limbs. This finding demonstrates that our body-aware encoder plays a crucial role in inferring the occluded parts and spatial relationship between parts and body.
%Using the body reference feature as a condition enables the perception of relative spatial position and adaptive reasoning of invisible limbs, resulting in more accurate predictions for limbs with a high degree of freedom while maintaining the relative rigidity of the torso. This strong dependence between components allows the high degree of freedom components to extract beneficial information from the features of the rigid torso and components with low degrees of freedom. As a result, the errors in knee, elbow, head, and wrist are significantly reduced compared to other human pose estimation algorithms, as demonstrated in Table 5 of the BoPR paper.

%The ability to reason about the relative spatial position of components is especially important in human pose estimation, where the limbs can move independently and can be occluded by clothing or other objects. Body-aware features provide a powerful mechanism to handle these challenges by allowing the algorithm to reason about the expected position of the limb based on the position of the other components in the body. In particular, the ability to reason about invisible limbs is critical in challenging situations where the limbs are occluded or outside the frame of the image.As shown in Figure 4, after we removed the attention to the body reference features, the position of the limbs with high degrees of freedom appeared obvious errors.

%In conclusion, body-aware features provide a powerful mechanism for improving the performance of human pose estimation algorithms by enabling the perception of relative spatial position and adaptive reasoning of invisible limbs. The strong dependence between components allows the algorithm to extract useful information from the features of the rigid torso and components with low degrees of freedom, resulting in more accurate predictions for limbs with a high degree of freedom.


\noindent\textbf{Qualitative comparison.}
In Figure~\ref{fig:qua}, we compare the performance of PARE, CLIFF, and BoPR on different test datasets. To render the human geometry created by the SMPL model into images, we use the predicted camera parameters and weak perspective projection~\cite{kissos2020beyond}. The results show that PARE and CLIFF struggle to accurately infer the depth information between limbs and exhibit ambiguities in the orientation of the human body under large occlusions. However, thanks to the body reference condition dependency established by the BAR module, BoPR can more accurately recover these motions, especially for complex actions in the LSPET~\cite{johnson2011learning} dataset.More examples are in Sup.Mat.


% \begin{table}
% \centering
% % \small
% \begin{tabular}{lcccc}
% \toprule[1.5pt]
% \multirow{2}{*}{Method} & \multicolumn{3}{c}{3DPW}  \\ 
%  & MJE$\downarrow$ & PAMJE$\downarrow$ & PVE$\downarrow$  \\ \hline
% PARE &91.0 &56.7 &108.2 \\ \hline
% BoPR w/o BrF &90.1 &55.0 &106.1\\
% BoPR &\textbf{89.0} &\textbf{54.2} &\textbf{105.7}\\
% \bottomrule[1.5pt]
% \end{tabular}
% \caption{\textbf{Ablation study of reference feature.}"w/o BrF" means using the features extracted from the backbone network as reference features.}
% \label{table1}
% \end{table}

% \begin{figure*}[t]
%   \begin{center}
%     \includegraphics[width=1\linewidth]{qrs2.pdf}
%   \end{center}
%   \caption{xxxx.}
%   \end{figure*}
  
\noindent\textbf{Ablation Experiments.}
In this experiment, we use HR-W32 as the backbone and train all the comparing methods on a small COCO-2014-EFT (22K), part of the COCO-EFT dataset.  We first incorporate PARE with a transformer regressor and evaluate the performance on the 3DPW dataset. Table~\ref{tab:abl} shows that combining two techniques slightly reduces the PAMJE yet  increases the MJE and PVE.  In contrast, our proposed method improves PARE by a large margin, indicating that learning body-aware part features contributes significantly to the performance gain. We also ablate the auxiliary losses, namely the 2D keypoint $L_{2D}$, 3D keypoint $L_{3D}$, and the reference distance $L_{RD}$. The result in Table~\ref{tab:abl} shows all the loss helps to improve the effect.

%To evaluate the effectiveness of each module in our proposed BoPR model, we conducted ablation experiments using PARE as the baseline model. All methods were trained on the COCO-EFT-PART (22K) dataset and evaluated on the 3DPW dataset. As shown in Table ~\ref{tab:abl}, the results indicate that using the single-layer transformer in the BAR module for part dependency in PARE only improved PAMJE without significant changes overall. Subsequently, we designed experiments to explicitly supervise the body reference feature by establishing a 2D/3D dependency relationship, which resulted in some improvement in MJE and PAMJE.

%Furthermore, to validate the effectiveness of the AFE module, we removed the body reference feature and used the backbone network's extracted features as the reference condition, regressing SMPL parameters with the BAR module's global attention feature. As shown in Table 7, compared to PARE results, the BAR module significantly improved the performance, while lacking attention to the relatively rigid torso region resulted in a decrease in performance compared to BoPR.
%-------------------------------------------------------------------------


%------------------------------------------------------------------------
\section{Conclusion}
The proposed body-aware part regressor significantly improves over existing methods for human body shape and pose estimation from monocular images. This is achieved by incorporating two vital technical insights. First, we exploit the soft attention mechanism in the attention-guided encoder to capture global information and dependencies between body parts. Moreover, we enrich part representation with part-body dependence by leveraging local and global information. Furthermore, these insights allow the proposed method to address the limitations of current approaches, such as handling severe occlusions and depth ambiguity, and achieve state-of-the-art performance in various datasets and scenarios. Overall, this work contributes to advancing the state-of-the-art in human body shape and pose estimation and provides valuable technical insights for future research.
%We have presented a novel approach for human body shape and pose estimation from monocular images that addresses the limitations of existing methods in handling severe occlusions and depth ambiguity. Our method utilizes a two-stage attention-guided encoder and body-aware regression that incorporates local visual cues around the part and related global information from the entire body, allowing inferring the spatial relationship of occluded parts with the body to improve estimation accuracy. Experimental results demonstrate that our method outperforms state-of-the-art approaches on various datasets, particularly in handling severe occlusions and reducing depth ambiguity in estimation.\\
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}