%
% IEEE Transactions on Microwave Theory and Techniques example
% Tibault Reveyrand - http://www.microwave.fr
%
% http://www.microwave.fr/LaTeX.html
% ---------------------------------------



% ================================================
% Please HIGHLIGHT the new inputs such like this :
% Text :
%  \hl{comment}
% Aligned Eq. 
% \begin{shaded}
% \end{shaded}
% ================================================



\documentclass[journal]{IEEEtran}

%\usepackage[retainorgcmds]{IEEEtrantools}
%\usepackage{bibentry}  
\usepackage{xcolor,soul,framed} %,caption

\colorlet{shadecolor}{yellow}
% \usepackage{color,soul}
\usepackage[pdftex]{graphicx}
\graphicspath{{../pdf/}{../jpeg/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

\usepackage[cmex10]{amsmath}
%Mathabx do not work on ScribTex => Removed
%\usepackage{mathabx}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{url}
\usepackage{color}
\usepackage{cite}
\usepackage{amssymb,amsfonts}
\usepackage{multirow}
%\usepackage{enumitem}
\usepackage{subcaption} 
\usepackage{float} 
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{booktabs}
\usepackage{lineno}
\usepackage{hyperref}
\usepackage{longtable}
\hyphenation{op-tical net-works semi-conduc-tor}

%\bstctlcite{IEEE:BSTcontrol}


%=== TITLE & AUTHORS ====================================================================
\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}
\title{Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey}
\author{Guangyin Jin, Yuxuan Liang, Yuchen Fang, Jincai Huang, Junbo Zhang, Yu Zheng~\IEEEmembership{Fellow,~IEEE}
%\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem F.~Li,  M.~Wang are with Beijing National Research Center for Information Science and Technology (BNRist), Department of Electronic Engineering, Tsinghua University, Beijing 100084, China.\protect\\
%E-mail: lifx19@mails.tsinghua.edu.cn, wmd20@mails.tsinghua.edu.cn
%\protect\\
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem G.Y~Jin and J.C~Huang are with College of Systems Engineering, National University of Defense Technology, Changsha, China. E-mail: jinguangyin18@nudt.edu.cn, huangjincai@nudt.edu.cn\protect\\
\IEEEcompsocthanksitem Y.X~Liang is with School of Computing, National University of Singapore, Singapore. Email: yuxliang@outlook.com\protect\\
\IEEEcompsocthanksitem Y.C~Fang is with school of computer science, Beijing University of Posts and Telecommunications, Beijing, China.
E-mail: fyclmiss@gmail.com\protect\\
\IEEEcompsocthanksitem J.~Zhang and Y.~Zheng are with JD Intelligent Cities Research and JD iCity, JD Technology, Beijing, China. E-mail: msjunbozhang@outlook.com, msyuzheng@outlook.com}
%\IEEEcompsocthanksitem J.~Zhang is with State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing 100044, China.\protect\\
%E-mail: zhangjinlei@bjtu.edu.cn\protect\\
%\IEEEcompsocthanksitem $^{*}$Both authors contributed equally to this research.}
}


% The paper headers
% \markboth{IEEE TRANSACTIONS ON MICROWAVE THEORY AND TECHNIQUES, VOL.~60, NO.~12, DECEMBER~2012
% }{Roberg \MakeLowercase{\textit{et al.}}: High-Efficiency Diode and Transistor Rectifiers}


% ====================================================================
\maketitle



% === ABSTRACT ====================================================================
% =================================================================================
\begin{abstract}
%\boldmath
With the development of sophisticated sensors and large database technologies, more and more spatio-temporal data in urban systems are recorded and stored. Predictive learning for the evolution patterns of these spatio-temporal data is a basic but important loop in urban computing, which can better support urban intelligent management decisions, especially in the fields of transportation, environment, security, public health, etc. Since traditional statistical learning and deep learning methods can hardly capture the complex correlations in the urban spatio-temporal data, the framework of spatio-temporal graph neural network (STGNN) has been proposed in recent years. STGNNs enable the extraction of complex spatio-temporal dependencies by integrating graph neural networks (GNNs) and various temporal learning methods. However, for different predictive learning tasks, it is a challenging problem to effectively design the spatial dependencies learning modules, temporal dependencies learning modules and spatio-temporal dependencies fusion methods in STGNN framework. In this paper, we provide a comprehensive survey on recent progress on STGNN technologies for predictive learning in urban computing. We first briefly introduce the construction methods of spatio-temporal graph data and popular deep learning models that are employed in STGNNs. Then we sort out the main application domains and specific predictive learning tasks from the existing literature. Next we analyze the design approaches of STGNN framework and the combination with some advanced technologies in recent years. Finally, we conclude the limitations of the existing research and propose some potential directions.
\end{abstract}


% === KEYWORDS ====================================================================
% =================================================================================
\begin{IEEEkeywords}
Spatio-Temporal Graph, Neural Networks, Predictive Learning, Urban Computing 
\end{IEEEkeywords}





% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


% ====================================================================
% ====================================================================
% ====================================================================











% === I. INTRODUCTION =============================================================
% =================================================================================
\section{Introduction}\label{sec:intro}
With the rapid advancement of sensing and data stream processing technologies, more and more data in urban systems are efficiently collected and stored, laying the foundation for the advent of the era of urban computing. Urban computing aims to understand the urban patterns and dynamics from different application domains where the big data explodes, such as transportation, environment, security, etc. According to urban computing theories~\cite{zheng2014urban}, predictive learning based on these massive urban data is the most important loop, which is the foundation for intelligent decision-making, scheduling and management in smart cities. In addition, the predictability of urban big data can also provide the possibility for the development of some new technologies such as digital twin cities and metaverse~\cite{wang2016acp}.

\begin{figure}[h]
\centering
\vspace{-3mm}
\includegraphics[width=0.48 \textwidth]{f25.png}
\caption{An example for spatio-temporal heterogeneity.}
\label{fig:example} % FIG
\vspace{-3mm}
\end{figure}
Most urban data are spatiotemporal, that is, they are not only associated with spatial locations but also change over time. Correlation and heterogeneity are ubiquitous properties of spatio-temporal data in urban systems~\cite{wang2020deep}. Correlation refers to the property of data being auto-correlated not only in the time dimension but also in the space dimension; heterogeneity refers to the property of data showing different patterns in different temporal or spatial ranges. 
To illustrate spatio-temporal heterogeneity more intuitively, we give an example in Figure~\ref{fig:example}. As shown in Figure~\ref{fig:example}(a), there exists different districts in the given urban network, such as the residential district, leisure district and business district. Figure~\ref{fig:example}(b) shows the statistic of the crowd flow in different nodes from different districts. We can find that although all of the selected nodes show the obvious peak patterns, there are great differences in the statistic of the crowd flow of nodes from different districts. However, in the same districts, the statistic of crowd flow is similar even at different nodes, such as node 3 and node 4. 
Due to these complex spatio-temporal characteristics, the difficulty of feature engineering has increased, and some methods that perform well in traditional data prediction, such as support vector regression (SVR)~\cite{drucker1996support}, random forest (RF)~\cite{breiman2001random}, gradient boosting decision tree (GBDT)~\cite{natekin2013gradient}, etc., are difficult to achieve more accurate prediction results. In the past decade, due to the rapid development of deep learning technologies, some hybrid neural networks based on convolutional neural networks (CNN)~\cite{gu2018recent} and recurrent neural networks (RNN)~\cite{yu2019review}, such as ConvLSTM~\cite{shi2015convolutional}, PredRNN~\cite{wang2017predrnn}, etc., have gradually been applied to predictive learning of urban spatio-temporal data and gained significant advantages. However, the biggest limitation of these methods is that they cannot directly learn from the data existing in the non-Euclidean space, such as traffic network data, sensor network data, semantic network data, etc.

In recent years, deep learning techniques typified by graph neural networks (GNN) have made major breakthroughs in the representation learning of non-Euclidean spatial data, which has laid a foundation for predictive learning of diverse and complex urban data. Considering the spatio-temporal characteristics of some typical urban data, such as traffic network flow, environmental monitoring data, etc., some previous works combined graph neural networks with various temporal learning networks to capture dynamics in space and time dimensions~\cite{wang2020deep}. This type of hybrid neural network architecture is collectively referred to as spatio-temporal graph neural network (STGNN). Through the development of the past five years, the spatio-temporal graph neural network has been widely used for predictive learning scenarios in urban computing, including transportation, environment, security, public health, energy, economy and other fields. Based on the Google Scholar search engine, we conduct precise searches through keywords, and count the publication of relevant papers in the past five years.
As shown in Figure~\ref{fig:intro}, we can find that relevant papers about STGNN show an increasing trend year by year. The relevant papers were less than 20 in 2018 while the papers were close to 140 in 2022. This development trend suggests that the applications related to STGNN has become a hot research topic in recent years. In addition, most of the literature in the past five years has focused on predictive learning tasks.
\begin{figure}[t]
\centering
\vspace{-3mm}
\includegraphics[width=0.48 \textwidth]{f1.png}
\caption{The publication trend of STGNN-related papers in Google Scholar in the past five years. The blue bars represent total relevant publications and the red bars represent publications on predictive learning tasks.}
\label{fig:intro} % FIG
\vspace{-3mm}
\end{figure}

\textbf{Related Surveys} In recent years, there have been a few related surveys on the application of STGNN-based predictive learning techniques in different fields. Wang et al.~\cite{wang2020deep} reviewed the deep learning methods for spatio-temporal data mining involving some STGNN techniques in predictive learning up to 2020. The previous surveys~\cite{ye2020build,bui2022spatial,jiang2022graph} all investigated the STGNN technologies applied in the transportation domains. Among them, ~\cite{ye2020build} analyzed multiple practical problems and reviewed related works about prediction, detection and control problems in urban traffic systems. ~\cite{bui2022spatial} and ~\cite{jiang2022graph} focused on the most recent technologies about STGNN in traffic prediction tasks. The work~\cite{gao2022generative} investigated the application of generative adversarial techniques in spatio-temporal data learning, including some methods combined with spatio-temporal graph data.

\textbf{Our Contributions} Compared with the previous surveys, the contributions of our survey are summarized as:
\begin{itemize}
\item To our knowledge, this is the first comprehensive survey that reviews recent works exploring STGNN for predictive learning tasks in urban computing. We review the progress of STGNN mainly from the perspectives of applications and methodologies through sufficient literature. 
\item We first categorize the main application domains and specific predictive learning tasks of STGNN in urban computing based on the existing literature. In addition, We sort out some public datasets attached with the previous works about STGNN.
\item We provide an in-depth analysis of temporal dependencies learning, spatial dependencies learning and spatio-temporal dependencies fusion methods of STGNN. We also review some popular advanced methods combined with STGNN in recent years. 
\item We summaries some challenges shared by STGNN for predictive learning tasks in urban computing and suggests some future directions for addressing these challenging problems.
\end{itemize}

\textbf{Organization of This Survey} The rest of this survey is organized as follows. Section II introduces the construction of spatio-temporal graph. Section III overviews various predictive learning tasks from different domains that can be addressed by STGNN. Section IV introduces the basic deep learning architectures that are widely adopted in STGNN framework.
Section V provides an in-depth analysis of the neural architecture design methods of STGNN framework and some popular advanced technologies that can be combined. Section V discuss the limitations of existing works and suggests future directions. We finally conclude this survey in Section VI.


% \section{Preliminary}
% In this section, we sort out some background knowledge for STGNN, including spatio-temporal graph construction, graph neural networks, recurrent neural networks, temporal convolutional neural networks and self-attention neural networks. 

\section{Spatio-Temporal Graph Construction}
%Graph is an efficient structure for modeling relations in real-world. 
Suppose there is a given spatio-temporal series data $X=\{x_{t}\in R^{N\times F}|t=0,\dots,T\}$, where $N$ is the number of spatial vertices and $F$ is the number of features.
For such data, spatio-temporal graph is an efficient structure to characterize the relations between different vertices in a certain spatial and temporal range. Spatio-temporal graph can be represented by the notation $G_{t}=(V,E_{t},A_{t})$, where $V$ is the vertices set, $E_{t}$ and $A_{t}$ respectively denote the edges set and adjacency matrix at time $t$. In most spatio-temporal graph modeling scenarios, the size of $V$ is constant, the size of $E_{t}$ can be time-varying or time-constant, and $A_{t}\in R^{N\times N}$ also changes with $E_{t}$. From the perspective of connectivity, spatio-temporal graph can be directed, undirected, weighted and unweighted. From the perspective of evolutionary, the structure of spatio-temporal graph can be static and dynamic. The difference between the static and dynamic spatio-temporal graph is illustrated in Figure~\ref{fig:stg_type}. What type of spatio-temporal graph to construct specifically needs to be determined according to specific tasks and given data conditions.
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f2.png}
\caption{The schematic diagram of static and dynamic spatio-temporal graph. The different color shades of the nodes represent the different features.}
\label{fig:stg_type} % FIG
\vspace{-3mm}
\end{figure}
Generally, the construction methods of predefined spatio-temporal graph in urban systems can be divided into four categories: topology-based, distance-based, similarity-based and interaction-based. 
% Next we introduce and analyze STG construction from two important perspectives: connectivity and evolutionary. 
% Connectivity describes the connection properties of edges between different nodes in the graph. Evolutionary characterizes the properties of graph structures that change over time. 
% The connectivity of STG includes four types: directed, undirected, weighted and unweighted. The evolutionary of STG includes two types: static and dynamics. The difference between the static and dynamic graph is illustrated in Figure~\ref{fig:stg_type}. The structure of the static graph is constant while the structure of the dynamic graph is time-varying.

\textbf{Topology-based graph:} In urban systems, the topology-based graphs are constructed based on the given topology structures, such as road networks~\cite{guo2021learning,wang2020traffic}. The adjacency matrix of topology-based graph can be formulated as:
\begin{equation}
  a_{ij}^{t}=\left\{\begin{aligned}
  1, &\ if \ v_{i} \ connects \ to \ v_{j}  \\
  0, &\ \text{otherwise}
  \end{aligned},
  \right.
\end{equation}
where $a_{ij}^{t}$ denotes an element in adjacency matrix at time $t$, $v_{i}$ and $v_{j}$ are different vertices in the graph. Since the connections in topology structures can be symmetrical or asymmetrical, the topology-based graphs can be directed or undirected. Topology only represents connections in non-Euclidean spaces, thus the topology-based graphs are unweighted. In addition, the topology structures in social systems are usually fixed for quite a long time, so we can treat them as static graphs.     

\textbf{Distance-based graph:} According to first law of geography ‘Everything is related to everything else, but near things are more related to each other’, we can construct distance-based graph when there is no given topology. In most applications, the elements in the adjacency matrix need to be calculated by a kernel function associated with the distances~\cite{chai2018bike,yu2017spatio,jin2022deep}. Common kernel functions such as gaussian radial basis function and inverted function.
For example, the adjacency matrix of distance-based graph with gaussian radial basis function can be formulated as:
\begin{equation}
  a_{ij}^t=\left\{\begin{aligned}
  &\frac{\exp (-\left\|d_{ij}^t\right\|_2)}{\sigma}, \ if \ d_{ij}^t < \epsilon \\ & \ \ \ \ \ \ 0 , \ \ \ \ \ \text{otherwise}
  \end{aligned},
  \right.
\end{equation}
where $d_{ij}^{t}$ is the distance between node $i$ and $j$ at time $t$, $\epsilon$ is a predefined threshold to control the sparsity of adjacency matrix, $\sigma$ is a hype-parameter to control the distribution. 

\textbf{Similarity-based graph:} Similarity can reflect the relations between different entities from the perspective of semantics. Similarity-based graphs can be constructed based on the similarity of time series~\cite{liu2020physical,li2022automated,shi2020predicting} or similarity of spatial attributes (eg., POI)~\cite{geng2019spatiotemporal}. In scenarios without additional data, similarity-based graphs are usually constructed based on the similarity of time series. Pearson Correlation Coefficient (PCC) and Dynamic Time Wrapping (DTW) are two common methods to compute the similarity of time series. For instances, the adjacency matrix of similarity-based graph computed by PCC is defined as:
\begin{equation}
  a_{ij}^t=\left\{\begin{aligned}
  &\frac{\sum_{i=1}^n\left(x_i^{0:t}-\bar{x_i^{0:t}}\right)\left(x_j^{0:t}-\bar{x_j^{0:t}}\right)}{\sqrt{\sum_{i=1}^n\left(x_i^{0:t}-\bar{x_i^{0:t}}\right)^2} \sqrt{\sum_{i=1}^n\left(x_j^{0:t}-\bar{x_j^{0:t}}\right)^2}} \\ & \ \ \ \ \ \ \ \ \ \ \ \ \ \ 0 , \ \ \ \ \ \text{otherwise}
  \end{aligned},
  \right.
\end{equation}
where $x_i^{0:t}$ and $x_j^{0:t}$ are respectively the time series of node $i$ and node $j$ over time span $t$, $\bar{x_i^{0:t}}$ and $\bar{x_j^{0:t}}$ are respectively the mean value of the time series of node $i$ and node $j$, $n$ denotes the number of samples over time span $t$.
%$\epsilon$ is a threshold to control the sparsity of adjacency matrix.


\textbf{Interaction-based graph:} The interaction between different locations can express their connection from the perspective of information flow~\cite{chai2018bike,jin2022deep}. Especially for representing the characteristics of mobility, the larger the proportion of flow between two nodes, the closer the connection between them. Hence, the adjacency matrix of interaction-based graph can be defined as:
\begin{equation}
  a_{ij}^t=\left\{\begin{aligned}
  &\frac{F_{ij}^{t}}{\sum_{m\in N(i)} F_{im}^{t}}, \ if \ F_{ij}^{t}>0 \\ & \ \ \ \ \ \ 0 , \ \ \ \ \ \text{otherwise}
  \end{aligned},
  \right.
\end{equation}
where $F_{ij}^{t}$ denotes the flow from $i$ to $j$ at time $t$, $N(i)$ denotes the all nodes that interact with $i$, $F_{im}^{t}$ denotes the flow from $i$ to other nodes $m$ in the set $N(i)$ at time $t$.

% As show in Table~\ref{tab:stg}, we sort out the differences in connectivity and evolutionary of different construction methods based on the existing literature.
% \begin{table}[!htb]
% %\small
% \caption{Spatio-temporal graph construction analysis from connectivity and evolutionary perspectives}
% %\vspace{-3mm}
% \centering
% \scalebox{0.75}{\begin{tabular}{|c|c|c|c|c|c|c|}
% \hline \multirow{2}{*}{ Construction methos} & \multicolumn{4}{|c|}{ Connectivity } & \multicolumn{2}{c|}{ Evolutionary } \\
% \cline { 2 - 7 } & Directed & Undirected & Weighted & unweighted & Static & Dynamic \\
% \hline Topology-based & $\sqrt{ }$ & $\sqrt{ }$ & & $\sqrt{ }$ & $\sqrt{ }$ & \\
% \hline Distance-based & & $\sqrt{ }$ & $\sqrt{ }$ & $\sqrt{ }$ & $\sqrt{ }$ & $\sqrt{ }$ \\
% \hline Similarity-based & & $\sqrt{ }$ & $\sqrt{ }$ & $\sqrt{ }$ & $\sqrt{ }$ & $\sqrt{ }$ \\
% \hline Interaction-based & $\sqrt{ }$ & $\sqrt{ }$ & $\sqrt{ }$ & $\sqrt{ }$ & $\sqrt{ }$ & $\sqrt{ }$ \\
% \hline
% \end{tabular}}
% \label{tab:stg}
% %\vspace{-2mm}
% \end{table}


% \begin{figure}[h] 
% \centering
% %\vspace{-2mm}
% \includegraphics[width=0.45 \textwidth]{f2.png}
% \caption{Types of spatio-temporal graph from three perspectives}
% \label{fig:stg_type} % FIG
% %\vspace{-3mm}
% \end{figure}

In addition to the common predefined graph construction methods mentioned above, many relations in urban systems are implicit and difficult to be directly predefined. Therefore, spatio-temporal graphs based on adaptive learning have been proposed in some recent works and more details can be seen in section~\ref{sec:adpative}. 



\section{Main Application Domains and Predictive Learning Tasks}
In this section, we generalize the main application domains and specific predictive learning tasks in urban computing. According to the existing literature in recent years, we have made a statistical summary of the different application domains of STGNN in urban computing. As shown in Figure~\ref{fig:statistical}, the main application domains of STGNN involve transportation, security, environment and public health. Among them, transportation is the most popular application domain of STGNN, which accounts for more than 60\% of the existing literature.  
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f3.png}
\caption{The summary of the different application domains of STGNN in urban computing.}
\label{fig:statistical} % FIG
\vspace{-3mm}
\end{figure}

\subsection{Transportation}
\subsubsection{Traffic State Prediction}
In modern urban systems, a large number of sensors are distributed in traffic road networks and crucial regions to record the changing traffic states (eg., flow, speed).
This task aims to predict future traffic states by historical traffic states in a certain spatial range. As show in Figure~\ref{fig:tsp}, traffic state prediction can be divided into two main categories: network-wide prediction and region-wide prediction. The object of network-wide prediction is usually the traffic flow or speed on the given road networks~\cite{zhao2019t,guo2021learning,ji2022stden,li2017diffusion,yu2017spatio}. The basic graph structures can be directly converted from road networks in most previous works. The object of region-wide prediction is usually the crowd flow in the urban areas~\cite{li2022mgc,zhang2021traffic,sun2020predicting,zhang2020spatial}. In this case, the whole urban area is divided into grid regions, and then the spatio-temporal graph can be constructed based on the distances, connectivity, semantic correlations between different regions, etc. In general, traffic state prediction tasks can be summarized in the following form:
\begin{equation}
\left[\boldsymbol{X}^{\left(t-T^{\prime}+1\right)}, \cdots, \boldsymbol{X}^{(t)} ; \mathcal{G}\right] \stackrel{f(\cdot)}{\longrightarrow}\left[\boldsymbol{X}^{(t+1)}, \cdots, \boldsymbol{X}^{(t+T)}\right]
\end{equation}
where $\boldsymbol{X}^{(t)}\in R^{N\times d}$ denotes the traffic states matrix of $N$ vertices at time step $t$, $\mathcal{G}$ is the constructed graph structure, ${f(\cdot)}$ is the corresponding STGNN for prediction.
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f5.png}
\caption{The two categories of traffic state prediction.}
\label{fig:tsp} % FIG
\vspace{-3mm}
\end{figure}

\subsubsection{Traffic Demand Prediction}
Accurately predicting the changing patterns of urban traffic demand (eg., taxi order demand, rail transit passenger demand, bike demand, etc.) in different regions can facilitate traffic scheduling  to relieve traffic congestion during peak hours. 
%This task aims to predict future traffic demand by historical traffic demand in a certain spatial range. 
Traffic demand can be roughly divided into three main types: origin demand, destination demand and origin-destination demand. 
Predicting origin demands and destination demands are similar to region-wise traffic state prediction, predicting future demands by historical demands in $N$ regions~\cite{jin2022deep,jin2020urban,geng2019spatiotemporal,ye2021coupled}. However, origin-destination demand prediction is somewhat different, which needs to predict the future origin-destination matrices by the historical origin-destination matrices~\cite{wang2019origin,huang2023odformer,hu2020stochastic,dapeng2021dynamic,liu2022online}. To be specific, the outputs of origin-destination demand prediction are series of matrices with size $N\times N$, which can characterize the flow demand among these regions.

\subsubsection{Traffic Incident Prediction}
%Traffic state prediction tasks focus on the continuous numerical prediction of various spatio-temporal attributes of the road networks. 
With the dramatic increase in the number of vehicles, more and more traffic incidents such as congestion and accidents have occurred, which has brought enormous pressure to social traffic management. The aim of traffic incident prediction task is to predict some important properties (eg., occurrence probability, occurrence time, etc.) of these incidents that may occur on road networks~\cite{wang2021gsnet,yu2021deep,wang2022event,zhou2020riskoracle,liutap}. In addition to the difference of predicted objects, similar to traffic state prediction task, accurate traffic incident prediction also need to capture spatio-temporal dependencies on road networks by building STGNN models. Compared with the relatively macroscopic traffic state prediction, the incident-oriented prediction can more accurately respond to various emergencies in the traffic system for early warning.

\subsubsection{Travel Time Prediction}
Travel time prediction is valued by industry, especially in online map navigation and ride-hailing software. Accurate travel time predictions can significantly improve the user experience with this type of software.
This task aims to predict travel time of the given trajectories by historical traffic states on the road networks. In order to predict travel time more accurately, not only the characteristics of the trajectory itself need to be considered, but also the spatio-temporal dynamics (eg., flow, speed, etc.) attached to the road networks need to be captured. Therefore, spatio-temporal graphs are established based on road networks in this tasks. So far, large technology companies such as Baidu~\cite{fang2020constgat,huang2022dueta}, Google~\cite{derrow2021eta}, and DiDi~\cite{fu2020compacteta} have developed travel time prediction functions on online platforms that can be applied in practice. STGNN-based travel time prediction can be defined as follows:
\begin{equation}
\mathcal{F}(P_{t}|X_{t-w : t},\mathcal{G})\rightarrow T_{g},T_{l}
\end{equation}
where $P_t$ denotes the given trajectory with departure time $t$, $X_{t-w : t}$ denotes the spatio-temporal features in historical time window $w$ attached to the given road network $\mathcal{G}$. $T_{g}$ and $T_{l}$ respectively represent the global travel time of the entire trajectory and local travel time of the road segment.
% \begin{figure}[h] 
% \centering
% %\vspace{-2mm}
% \includegraphics[width=0.45 \textwidth]{f5.png}
% \caption{The travel time prediction function in mobile application.}
% \label{fig:tte} % FIG
% %\vspace{-3mm}
% \end{figure}

\subsubsection{Trajectory Prediction}
Trajectory prediction is an important task to understand the complex group dynamics of humans and vehicles~\cite{liu2022social,lu2022vehicle,peng2021stirnet,chen2020st,mohamed2020social,zhou2021ast}, which can contribute to the development of autonomous driving and urban monitoring technologies. As shown in Figure~\ref{fig:traj_p}, there exists some correlations or interactions in the movement patterns of agents in the group, thus we can build spatio-temporal graphs based on the relations between different agents within a group. After constructing the spatio-temporal graphs, we can design STGNN models to predict the coordinates that the agents may reach in the future based on the coordinates of their historical traversal, so as to realize the prediction of future trajectories.
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f4.png}
\caption{The social interactions between different agents within a group~\cite{liu2022social}.}
\label{fig:traj_p} % FIG
\vspace{-3mm}
\end{figure}

\subsubsection{Other prediction tasks}
In addition to the mainstream transportation application scenarios mentioned above, there are also some relatively niche scenarios that use STGNN techniques to improve prediction results. For examples, parking availability prediction~\cite{zhang2020semi,zhao2021mepark} and traffic delay prediction~\cite{zhang2021train,zhang2022interpretable} are two emerging tasks in transportation management that can be combined with STGNN. Similar to other mainstream research topics, these prediction tasks adopt STGNNs to better learn spatio-temporal contextual representations on traffic networks, resulting in more accurate predictions. 

\subsection{Environment}

\subsubsection{Air Quality Prediction}
Air quality is a hot issue that needs urgent improvement. Accurate air quality prediction can not only help the government formulate energy-saving and emission-reduction policies, but also provide reference for residents' outdoor activities. Air quality index (AQI), PM2.5 and emissions are the indicators we are most concerned about. These related data are collected by city-level or national-level observation stations~\cite{liang2022airformer,han2021joint}. Due to the fluidity of the air, observation stations that are close in distance or in the same wind direction may collect correlated results~\cite{zhou2021forecasting,wang2020pm2,liang2018geoman}. Hence, constructing spatio-temporal graph-based deep learning models can not only establish such spatial dependencies, but also capture the time-varying dynamics of air quality. 

\subsubsection{Meteorological Prediction}
Meteorological prediction is also a research topic closely related to human society and environment. Similar to air quality data, meteorological data are also collected by distributed observation stations. However, the correlations between different stations could be more complex and susceptible to more factors. In recent years, STGNN models have been gradually applied in various meteorological prediction scenarios such as temperature prediction~\cite{lin2022conditional,chen2022physics,jia2021physics}, frost prediction~\cite{lira2021frost} and wind prediction~\cite{rathore2021multi,khodayar2018spatio,stanczyk2021deep} and demonstrate their superior performance.

\subsection{Security}

\subsubsection{Crime Frequency Prediction}
Effectively combating and preventing crime is the foundation for ensuring urban security. Accurate prediction of crime frequency can help the government grasp real-time crime dynamics and rational allocation of police resources. Most existing work focuses on crime frequency prediction in urban areas. Since different urban regions have different functions, POI and other characteristics, these could also lead to different crime types and trends. However, regions with similar characteristics or close distances may have latent correlations in crime incidents~\cite{xia2021spatial,qian2020gest,song2021dgcn}. Hence, many previous works~\cite{xia2021spatial,qian2020gest,jin2021gsen,jin2020addressing,feng2022gtrans,wang2018graph,tekin2022crime,zhang2020graph,sun2022spatial,wang2022hagen} introduce STGNN models to capture these correlations to obtain better prediction results.

\subsubsection{Disaster Situation Prediction}
Disasters have been a major challenge to the security of human society since ancient times. Accurate disaster situation prediction can help the government deploy disaster prevention measures, allocate disaster relief materials, and evacuate residents in advance. To model correlated and heterogeneous features across geographical locations, STGNN can be a fruitful in this task. At present, some works have introduced the STGNN models into scenarios such as flood prediction~\cite{feng2022graph,yuan2022spatio}, fire prediction~\cite{jin2022adaptive,jin2020ufsp}, typhoon prediction~\cite{yang2022spatio,zhou2020classification,farahmand2021spatial} and earthquake prediction~\cite{dougan2022structural,mcbrearty2022earthquake,zhang2022spatiotemporal}.

\subsection{Public Health}
\subsubsection{Epidemic Prediction}
Epidemics are one of the greatest challenges to the public health systems, especially the novel coronavirus that has been prevalent in recent years, which has caused more than six million deaths worldwide. Therefore, accurately predicting the spread of epidemics is an important but challenging task, which can provide data support for the strengthening strategy of the urban public health systems. Some recent existing works have employed STGNN models to address the national-level~\cite{ngoc2021forecasting,kapoor2020examining,xie2022epignn,panagopoulos2021transfer,yu2023spatio,gao2021stan} or international-level~\cite{deng2020cola} epidemic prediction tasks. Many of them combine the mathematical formulations of epidemic dynamics and the modeling of spatio-temporal graphs, which have achieved better prediction results than traditional methods~\cite{deng2020cola,sun2022using,zheng2020spatial,la2020epidemiological,gao2021stan}.

\subsubsection{Ambulance Demand Prediction}
In the aging contemporary society, the allocation of ambulance resources is a challenging task that needs to be paid attention to. Accurate ambulance demand prediction can effectively reduce the burden on the urban healthcare systems. Since there could be time-varying correlations in public medical resources, traffic conditions, and demand patterns among different regions of the social systems, in order to learn these multi-view spatial correlations, the STGNN methods have gradually been promoted on this task in recent years~\cite{wang2021forecasting,jin2021predicting,munasinghe2022using}. 

\subsection{Other Application Domains}
In addition to the four main application domains mentioned above, other scenarios where the spatio-temporal graph structures can be established based on the intrinsic relations of data are potential development objects for the STGNN-based predictive learning models.
In recent years, STGNN-based predictive learning models have also been promoted to other domains such as energy, economy, finance, and production. In the energy domain, STGNN models have been utilized in wind power prediction~\cite{yu2022spatio,li2022spatiotemporal} and photovoltaic power prediction~\cite{zhang2022optimal,simeunovic2021spatio}. In the economy domain, STGNN models have been introduced into national-level regional economy prediction~\cite{hui2020predicting,xu2020attentional}. 
In the finance domain, STGNN models have been applied for stock prediction~\cite{wang2021hierarchical,wang2022adaptive,tan2022finhgnn}. In the production domain, Fan et al. first adopted STGNN model in predicting crop yield~\cite{fan2022gnn}.


% \subsubsection{Energy Prediction}
% \subsubsection{Economy Prediction}
%\subsection{Public Datasets}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\subsection{Public Datesets in Main Application Domains}
We sort out some of the most frequently applied public datasets of previous works in the main application domains, as shown in Table~\ref{tab:dataset}. In this table, we display the detailed information about these public datasets such as source link and references. 

\begin{table*}[h]
\centering
\caption{The public datasets for main application domains.}
\label{tab:dataset}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|}
\hline
Domain                           & Dataset                 & Link                                                           & Reference \\ \hline
\multirow{12}{*}{Transportation} & California -PEMS        & http://pems.dot.ca.gov/                                        & \cite{guo2021learning,li2022automated,zhang2020spatio,wu2022traversenet,li2021spatial,song2020spatial,jin2022automated}          \\ \cline{2-4} 
                                 & METR-LA                 & https://www.metro.net/                                         &  \cite{zheng2020gman,li2017diffusion,yu2017spatio,chen2020multi,wu2019graph,li2021dynamic,han2021dynamic}         \\ \cline{2-4} 
                             & NYC taxi                & https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page  & \cite{ye2021coupled,zhang2020spatial,liu2022msdr,li2022lightweight,sun2020predicting}          \\ \cline{2-4} 
                                 & San Francisco taxi      & https://crawdad.org/ crawdad/epfl/mobility/20090224/           & \cite{zhao2019t,xie2020deep}          \\ \cline{2-4} 
                                 & NYC bike                & https://www.citibikenyc.com/sytem-data                        &  \cite{zhou2021urban,li2022lightweight,chai2018bike,peng2021dynamic,zhang2021traffic,ali2022exploiting}         \\ \cline{2-4} 
                                 & Chicago bike            & https://www.divvybikes.com/system-data                         & \cite{chai2018bike,wang2021spatio,wang2022multivariate}          \\ \cline{2-4} 
                                 & NYC accident            & https://data.cityofnewyork.us/                                 &  \cite{zhou2020riskoracle,zhou2020foresee,wang2021gsnet,wang2021incident}         \\ \cline{2-4} 
                                 & Chicago accident        & https://data.cityofchicago.org/                                & \cite{wang2021gsnet,wu2020hierarchically}          \\ \cline{2-4} 
                                 & Chengdu taxi trajectory & http://www.dcjingsai.com./                                     & \cite{wang2021graphtte,wang2023multitask,jin2021spatio,jin2021hierarchical,tag2022sttg,jin2022stgnn}          \\ \cline{2-4} 
                                 & Porto taxi trajectory   & https://www.kaggle.com/crailtap/taxi-trajectory.               & \cite{jin2022stgnn,jin2021spatio,jin2021hierarchical,jiang2022self}          \\ \cline{2-4} 
                             & ETH walking pedestrians & https://data.vision.ee.ethz.ch/cvl/aem/ewap\_dataset\_full.tgz & \cite{peng2021stirnet,shi2021sgcn,mohamed2020social,zhou2021ast,malla2021social,liu2022social,lian2022ptp}          \\ \cline{2-4} 
                                 & UCY walking pedestrians & https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data    &  \cite{peng2021stirnet,shi2021sgcn,mohamed2020social,lian2022ptp}         \\ \hline
\multirow{5}{*}{Environment}     & Beijing air quality     & https://biendata.com/competition/kdd\_2018/data/               & \cite{lin2018exploiting,han2021joint}          \\ \cline{2-4} 
                                 & Shanghai air quality    & http://www.cnemc.cn/en/                                        & \cite{han2021joint}          \\ \cline{2-4} 
                                 & WeatherBench            & https://mediatum.ub.tum.de/1524895                             &\cite{lin2022conditional}           \\ \cline{2-4} 
                                 & Denmark wind speed      & https://sites.google.com/view/siamak-mehrkanoon/code-data      &\cite{stanczyk2021deep,rathore2021multi}           \\ \cline{2-4} 
                                 & Dutch wind speed        & https://github.com/HansBambel/multidim\_conv                   &\cite{stanczyk2021deep}           \\ \hline
\multirow{6}{*}{Security}        & NYC crime               & https://data.cityofnewyork.us/                                 &\cite{xia2021spatial,li2022spatial,sun2022spatial}            \\ \cline{2-4} 
                                 & Chicago crime           & https://data.cityofchicago.org/                                &\cite{xia2021spatial,tekin2022crime,li2022spatial,zhang2020graph,sun2022spatial}          \\ \cline{2-4} 
                                 & San Francisco crime     & https://datasf.org/openda                                      &\cite{wang2021spatio,jin2022adaptive}           \\ \cline{2-4} 
                                 & San Francisco fire      & https://datasf.org/openda                                      &\cite{wang2021spatio,jin2022adaptive}           \\ \cline{2-4} 
                                 & Japan typhoon           & http://agora.ex.nii.ac.jp/digital-typhoon/                     &\cite{yang2022spatio,zhou2020classification}           \\ \cline{2-4} 
                                 & California earthquake   & https://service.iris.edu/                                      &\cite{zhang2022spatiotemporal,feng2022gtrans}           \\ \hline
\multirow{4}{*}{Public health}   & US Covid-19             & https://github.com/CSSEGISandData/COVID-19                     &\cite{wang2022causalgnn,xie2022epignn,li2020study,yu2023spatio,kapoor2020examining}           \\ \cline{2-4} 
                                 & Italy Covid-19          & https://github.com/pcm-dpc/COVID-19                            &\cite{la2020epidemiological}           \\ \cline{2-4} 
                                 & Japan-Prefectures ILI   & https://tinyurl.com/y5dt7stm                                   &\cite{deng2019graph,deng2020cola}        \\ \cline{2-4} 
                                 & US ILI                  & https://tinyurl.com/y39tog3h                                   &\cite{deng2019graph,deng2020cola}           \\ \hline
\end{tabular}%
}
\end{table*}


\section{Basic Neural Architecture for STGNN}\label{sec:basic arch}
In this section, we introduce basic neural architectures for STGNN. As shown in Figure~\ref{fig:stgnn_framework}, the basic framework of STGNN for predictive learning contains three main modules: data processing module (DPM), spatio-temporal graph learning module (STGLM) and task-aware prediction module (TPM). For predictive learning tasks in urban computing, DPM aims to construct the spatio-temporal graph data from the raw data, STGLM aims to capture hidden spatio-temporal dependencies from complex social systems and TPM aims to map the spatio-temporal hidden representation from STGLM into the space of downstream prediction tasks. 
STGLM is the most crucial part in STGNN, which usually combines spatial learning networks and temporal leaning networks organically through a certain spatio-temporal fusion neural architecture. For spatial learning networks, spectral graph convolutional networks (Spectral GCNs), spatial graph convolutional networks (Spatial GCNs) and graph attention networks (GATs) can all be employed as selection objects. For temporal learning networks, recurrent neural networks (RNNs), temporal convolutional networks (TCNs) and temporal self-attention networks (TSANs) can all be alternatives. 
Compared with STGLM, TPM is a relatively simple neural network, thus almost all existing work focuses on the design of the neural architectures in STGLM. 
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.48 \textwidth]{f6.png}
\caption{The basic framework of STGNN for predictive learning.}
\label{fig:stgnn_framework} % FIG
\vspace{-3mm}
\end{figure}

\subsection{Graph Neural Networks}
Graph neural networks (GNNs) are fruitful tools for spatial dependencies learning in non-Euclidean space. In recent years, popular GNNs can be divided into three categories: spectral GCNs, spatial GCNs and GATs. 

\subsubsection{Spectral Graph Convolutional Network}
At a relatively early stage, most GNNs are based on Fourier transform, which converts the graph signal in the spatial domain into the spectral domain to conduct the convolution calculation~\cite{ZHOU202057}. In this approach, we need graph Fourier transform and inverse graph Fourier transform to achieve the transformation between spatial domain and spectral domain, which are respectively defined as:
\begin{equation}
\begin{aligned}
& \mathcal{F}(\mathbf{x})=\mathbf{U}^T \mathbf{x} \\
& \mathcal{F}^{-1}(\mathbf{x})=\mathbf{U} \mathbf{x}
\end{aligned}
\label{eq:s_gnn}
\end{equation}
where $\mathbf{U}$ denotes the matrix of eigenvectors of the normalized graph Laplacian.  Based on this, the graph convolution operation is defined as:
\begin{equation}
\begin{aligned}
& \mathbf{g} \star \mathbf{x}=\mathcal{F}^{-1}(\mathcal{F}(\mathbf{g}) \odot \mathcal{F}(\mathbf{x})) =\mathbf{U}\left(\mathbf{U}^T \mathbf{g} \odot \mathbf{U}^T \mathbf{x}\right)
\end{aligned}
\end{equation}
where $\odot$ is the convolution operator and $\mathbf{U}^T \mathbf{g}$ denotes the filter in the spectral domain. The equation~\ref{eq:s_gnn} can be further simplified as:
\begin{equation}
\mathbf{g}_w \star \mathbf{x}=\mathbf{U g}_w \mathbf{U}^T \mathbf{x}
\end{equation}
Most of the subsequent GNNs based on the spectral domain mainly improve the calculation method of $\mathbf{g}_w$. For example, ChebNet is one of the most popular Spectral GNN methods. 
Based on the theory that $\mathbf{g}_w$ can be approximated by a truncated expansion of Chebyshev polynomials~\cite{hammond2011wavelets}, Defferrard et al.~\cite{defferrard2016convolutional} propose ChebNet, which is formulated as:
\begin{equation}
\begin{aligned}
&\widetilde{\mathbf{L}}=\frac{2}{\lambda_{\max }} \mathbf{L}-\mathbf{I}_N\\
&\mathbf{g}_w \star \mathbf{x} = \sum_{k=0}^K w_k \mathbf{T}_k(\widetilde{\mathbf{L}}) \mathbf{x}
\end{aligned}
\end{equation}
where $\widetilde{\mathbf{L}}$ is the normalized graph Laplacian, $\lambda_{\max }$ is the largest eigenvalue of $\mathbf{L}$, $\mathbf{T}_k(x)$ denotes the Chebyshev polynomials up to $k_{th}$ order, $w_k$ denotes a vector of Chebyshev coefficients. ChebNet remove the need to compute the eigenvectors of the Laplacian through applying this K-localized graph convolution. 

\subsubsection{Spatial Graph Convolutional Network}
Although spectral GCNs have been achieved many breakthroughs, the biggest limitation of them is the dependency on the graph Laplacian matrix. When the graph structure changes, the graph Laplacian matrix also needs to be recalculated. Hence, spectral GCNs are more suitable for scenarios where the graph structure does not change over time. To overcome the dependency on the graph Laplacian matrix, Kipf et al. further simplify the graph convolution operation~\cite{kipf2016semi}. Since this graph convolution operation is directly conducted in spatial domain, we called it as spatial GCN, which is defined as:
\begin{equation}
\mathbf{g}_w \star \mathbf{x} = w\left(\mathbf{I}_N+\mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}\right) \mathbf{x}
\end{equation}
where $\mathbf{A}$ is the adjacency matrix, $\mathbf{D}$ is the degree matrix, $w$ is the learnable parameters in the spatial GCN.

The spatial GCN mentioned above adopt the full graph as input, which could be difficult to handle some large graphs in some industry scenarios. To address this problem, GraphSAGE~\cite{hamilton2017inductive} adopt the sampling aggregation approach to achieve flexible inductive learning on large graphs. The aggregation operator in GraphSAGE is formulated as:
\begin{equation}
\begin{aligned}
& \mathbf{h}_{\mathcal{N}(u)}^k \leftarrow \operatorname{Aggregate}_k\left(\left\{\mathbf{h}_{u^{\prime}}^{k-1}, \forall u^{\prime} \in \mathcal{N}_k(u)\right\}\right) \\
& \mathbf{h}_u^k \leftarrow \sigma\left(\mathbf{W}^k \cdot \operatorname{Concat}\left(\mathbf{h}_u^{k-1}, \mathbf{h}_{\mathcal{N}(u)}^k\right)\right)
\end{aligned}
\end{equation}
where $\mathcal{N}_k(u)$ denotes the set of neighbor nodes of $u$,  $\mathbf{h}_{\mathcal{N}(u)}^k$ denotes the embedding of node $u$ after aggregation operation.

\subsubsection{Graph Attention Network}
To take into account the importance of neighbor nodes in spatial dependencies learning, GAT~\cite{velivckovic2017graph} incorporates the attention mechanism into the node aggregation operation, which is defined as:
\begin{equation}
\begin{gathered}
\mathbf{h}_v^{t+1}=\rho\left(\sum_{u \in \mathcal{N}_v} \alpha_{v u} \mathbf{W h}_u^t\right), \\
\alpha_{v u}=\frac{\exp \left(\operatorname{LeakyReLU}\left(\mathbf{a}^T\left[\mathbf{W h}_v \| \mathbf{W h}_u\right]\right)\right)}{\sum_{k \in \mathcal{N}_u} \exp \left(\operatorname{LeakyReLU}\left(\mathbf{a}^T\left[\mathbf{W} \mathbf{h}_v \| \mathbf{W h}_k\right]\right)\right)},
\end{gathered}
\end{equation}
where $\alpha_{v u}$ denotes the attention scores of neighbor node $u$ to the central node $v$, $W$ is the weight matrix associated with the linear transformation for each node, and $\mathbf{a}$ is the weight parameter for attention output.
To further stabilize the computational process of attention, multi-head attention form can also be introduced in GAT:
\begin{equation}
\begin{aligned}
& \mathbf{h}_v^{t+1}=\|_{k=1}^K \sigma\left(\sum_{u \in \mathcal{N}_v} \alpha_{v u}^k \mathbf{W}_k \mathbf{h}_u^t\right) \\
& \mathbf{h}_v^{t+1}=\sigma\left(\frac{1}{K} \sum_{k=1}^K \sum_{u \in \mathcal{N}_v} \alpha_{v u}^k \mathbf{W}_k \mathbf{h}_u^t\right) .
\label{eq:multi-attention}
\end{aligned}
\end{equation}
where $\alpha_{v u}^k$ is the normalized attention score computed by the $k_{th}$ attention head. The aggregation method for multiple attention heads can be concatenation aggregation or average aggregation. 

\subsection{Recurrent Neural Networks}
Recurrent neural networks (RNNs) are deep sequence modeling methods by recursive computation, which has been widely applied in time series learning. However, the biggest limitation of the original version of RNN is the gradient disappearance or gradient explosion problem that is difficult to solve during the training process~\cite{lukovsevivcius2009reservoir}. To overcome this challenge, two most common variants, long-Short term memory network (LSTM)~\cite{hochreiter1997long} and gated recurrent unit network (GRU)~\cite{dey2017gate} are proposed.

\subsubsection{Long-Short Term Memory Network}
To address the gradient disappearance and gradient explosion problem, LSTM first introduce the gated mechanism to control the information flow. reserve and forget temporal information. Specifically, the gated mechanism is to selectively retain and forget temporal information. The formulation of LSTM is defined as:
\begin{equation}
\begin{split}
f_{t}&=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right), \\
i_{t}&=\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right),  \\
o_{t}&=\sigma\left(W_{o} \cdot\left[h_{t-1}, x_{t}\right]+b_{o}\right),\\
\tilde{\mathrm{C}}_{t}&=\tanh \left(W_{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right),\\
C_{t}&=f_{t} * C_{t-1}+i_{t} * \widetilde{C}_{t},\\
h_{t}&=o_{t} * \tanh \left(C_{t}\right).
\label{eq:lstm}
\end{split}
\end{equation}
where $f_{t}$ represents the forgetting gate, whose function is to discard historical information in a certain proportion. $i_{t}$ denotes the input gate, which is to update the information of the current time step. $o_{t}$ is the output gate, which is to control the current output information in a certain ratio. $\sigma(\cdot)$ represents the sigmoid function, so the output values of the forget gate, input gate and output gate are all between 0 and 1.
$\tilde{\mathrm{C}}_{t}$ represents the candidate state of the current LSTM unit. $C_{t}$ represents the state of the LSTM unit after the calculation of the forget gate and the input gate, where the forget gate acts on the state of the LSTM unit of the previous time step and the input gate acts on the state of the current candidate LSTM unit. Finally, the hidden state of this time step is obtained by the output gate.

\subsubsection{Gated Recurrent Unit Network}
Due to the introduction of multiple gated units, the computational burden of LSTM is relatively large. Hence, GRU simplifies the gated units of LSTM into two: update gate and reset gate. $u_{t}$ represents the update gate, which determines how to combine the information of the new input time step with the memory of the previous time step. $r_{t}$ represents the reset gate, which defines the amount of memory reserved from the previous time step to the current time step. Although the learnable parameters of GRU are streamlined, its performance can be compared with LSTM in previous works, while improving the training and inference efficiency. The calculation process of GRU is defined as follows:
  \begin{equation}
	\begin{split}
		& u_t =\sigma(W_{u}\cdot x_t + U_{u}\cdot C_{t-1} + b_{u}),\\
		& r_t =\sigma(W_{r}\cdot x_t+ U_{r}\cdot C_{t-1} + b_{r}), \\
		&  \tilde{\mathrm{C}}_{t} ={\rm tanh}(W_{C}\cdot x_t + U_{C}(r_t \odot C_{t-1})+b_{C}), \\
		&  C_t=u_t\odot C_{t-1} + (1-u_t)\odot \tilde{\mathrm{C}}_{t}.
  \label{eq:gru}
	\end{split}
\end{equation}

\subsection{Temporal Convolutional Networks}
The RNN-based temporal learning networks have been applied in many spatio-temporal modeling tasks, but its disadvantage is also obvious: the recurrent structures makes the sequences have to be calculated at each time step, which greatly increases the calculation burden and reduces the model efficiency. In contrast, temporal convolutional networks (TCN) with parallel 1D-CNN structures can solve this problem. Similar to 2D-CNN applied in image recognition, 1D-CNN also operates and aggregates features through convolution kernels. However, the convolution kernel of 1D-CNN is one-dimensional and only slides on the time axis.

\subsubsection{Gated Temporal Convolutional Network}
Inspired by gated mechanism in LSTM and GRU, we can also combine it with pure 1D-CNN architecture to improve the capability of temporal learning. We called this hybrid neural architecture as gated temporal convolutional network (Gated-TCN)~\cite{dauphin2017language}, whose caculation process is defined as:
\begin{equation}
F(x) = tanh(\mathbf{\Theta_1}\star x )\odot\sigma(\mathbf{\Theta_2}\star x).
\end{equation}
Where $\mathbf{\Theta_1}$ and $\mathbf{\Theta_2}$ respectively represent the learnable parameters of the convolution kernel in two different 1D-CNNs, $\star$ represents the convolution operation, $\odot$ Represents the element-wise multiplication mechanism, $\sigma(\mathbf{\Theta_2}\star x)$ represents the gating unit, which is used to control the utilization rate of historical information.

\subsubsection{Causal Temporal Convolutional Network}
Although 1D-CNN is an efficient parallel neural architecture, it lacks modeling of causal correlations in temporal learning. Usually, in the traditional neural networks, the connection of neurons in each layer is in the form of fully connected. It is not difficult to find that the fully connection violates the basic constraint of time series, because the output of the front (the previous time steps) neurons are connected to the input neurons later (the future time steps), which should not be allowed.
Hence, we can employ a mask mechanism to partially remove the layer-by-layer links in the networks, and keep those links from the previous time steps to the future time steps, so that the network meets the principle of temporal dependencies. Meanwhile, in order to capture longer-range temporal dependencies more effectively, the 1D-CNN with the dilated factors~\cite{yu2015multi} increasing layer by layer has the capability to learn temporal dependencies from short-range to long-range, as shown in the Figure~\ref{fig:tcn}.
The 1D-CNN with dilated factors is expressed as:
\begin{equation}
F(s)=\left(\boldsymbol{x} *_d f\right)(s)=\sum_i^{k-1} f(i) \cdot x_{s-d \cdot i}
\end{equation}
where, $s$ is the time series input, $d$ is the dilation factor, the ordinary convolution operator is a special case of the dilated convolution operator when $d=1$. $s-d \cdot i$ refers to the positioning of a certain historical information.
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.48 \textwidth]{f7.png}
\caption{The overview of causal temporal convolutional network with exponentially increasing dilated factors~\cite{oord2016wavenet}.}
\label{fig:tcn} % FIG
\vspace{-3mm}
\end{figure}

\subsection{Temporal Self-Attention Networks}
Self-attention networks are fruitful methods for long-range temporal learning, the most typical representative of which is Transformer~\cite{vaswani2017attention}. Transformer model mainly consists of three parts: scaled dot-product attention network, feed-forward network and position encoding, as shown in Figure~\ref{fig:san}.
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.4 \textwidth]{f8.png}
\caption{The overview of Transformer.}
\label{fig:san} % FIG
\vspace{-3mm}
\end{figure}

The scaled dot-product network is the core part in Transformer. The calculation of attention scores is defined as:
\begin{equation}
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V.
\end{equation}
where quires $Q$, keys $K$, values $V$ are three basic elements in self-attention, which are obtained by different linear transformations from the original input. $d_k$ denotes the scaling factor, whose value is equal to the dimension of the model. To stabilize the training process, this part can also employ the multi-head attention form, which is similar to the equation~\ref{eq:multi-attention}.

Since Transformer contains no recurrence or convolution operator, we have to inject some positional information of the tokens in the sequence to make full of the order of the sequence. Trigonometric function-based encoding is a common positional encoding approach, which is defined as:
\begin{equation}
\begin{aligned}
P E_{(p o s, 2 i)} & =\sin \left(p o s / 10000^{2 i / d_{\text {model }}}\right) \\
P E_{(p o s, 2 i+1)} & =\cos \left(p o s / 10000^{2 i / d_{\text {model }}}\right).
\end{aligned}
\end{equation}
where $pos$ is the position and $i$ is the dimension. This method performs sine encoding and cosine encoding on even and odd positions respectively, to distinguish different positions. 

\subsection{Spatio-Temporal Fusion Neural Architecture}\label{sec:stfa}
In addition to spatial learning networks and temporal learning networks, spatio-temporal fusion neural architecture is also a key point that needs to be focused, which determines how spatial learning networks and temporal learning networks are fused into the complete STGNN. Conventional fusion neural architectures can be divided into two categories: stacked neural architecture and coupled neural architecture. 

%\subsubsection{Stacked Neural Architecture}
\subsubsection{Factorized Neural Architecture}
In factorized neural architectures, spatial learning networks and temporal learning networks are stacked in parallel or serially like building blocks layer by layer. There are two typical examples for factorized neural architectures in STGNN models, as shown in Figure~\ref{fig:stgcn} and Figure~\ref{fig:tgcn}. The first one is STGCN~\cite{yu2017spatio}, whose temporal learning network is TCN. In each ST-Conv block of STGCN, two TCNs and one GCN are stacked in series like a sandwich structure. Since this model learns temporal information through the convolutional structures, its spatio-temporal dependencies learning method is parallelized, that is, it receives all information of a given time window length as input at the same time. In mathematical form, the calculation of each ST-Conv block in this model can be defined as:
\begin{equation}
v^{l+1}=\Gamma_1^l *_{\mathcal{T}} \operatorname{ReLU}\left(\Theta^l * _{\mathcal{G}}\left(\Gamma_0^l * \mathcal{T} v^l\right)\right)
\end{equation}
where $\Gamma_0^l, \Gamma_1^l$ are respectively the upper and lower temporal covolutional kernel within block $l$, $\Theta^l$ is the spectral kernel of graph convolution.
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f10.png}
\caption{The overview of STGCN~\cite{yu2017spatio}.}
\label{fig:stgcn} % FIG
\vspace{-3mm}
\end{figure}

The second one is T-GCN~\cite{zhao2019t}, whose temporal learning network is GRU. This model captures the spatio-temporal dependencies by a recursive manner. For each time step, graph signals are sequentially processed by GCN and GRU to learn spatial and temporal dependencies respectively. The calculation of each stacked GCN and GRU in this model can be expressed as:
\begin{equation}
\begin{aligned}
&f(X, A)=\sigma\left(A X W_0\right) \\
&u_t=\sigma\left(W_u\left[f\left(A, X_t\right), h_{t-1}\right]+b_u\right) \\
&r_t=\sigma\left(W_r\left[f\left(A, X_t\right), h_{t-1}\right]+b_r\right) \\
&c_t=\tanh \left(W_c\left[f\left(A, X_t\right),\left(r_t * h_{t-1}\right)\right]+b_c\right) \\
&h_t=u_t * h_{t-1}+\left(1-u_t\right) * c_t
\end{aligned}
\end{equation}
where $f(A,X_t)$ denotes the output of spatial GCN at time step $t$. Then $f(A,X_t)$ is put forward into GRU to obtain the hidden state at $t$.

\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f11.png}
\caption{The overview of T-GCN~\cite{zhao2019t}.}
\label{fig:tgcn} % FIG
\vspace{-3mm}
\end{figure}

\subsubsection{Coupled Neural Architecture}
In coupled neural architectures, spatial learning networks are usually integrated into the architecture of temporal learning networks as embedded components. In STGNN, this type of neural architectures occurs almost exclusively in combinations of GNN-based spatial learning networks and RNN-based temporal learning networks. For example, DCRNN~\cite{li2017diffusion} is a STGNN model with coupled neural architecture, as shown in Figure~\ref{fig:dcrnn}. In this model, GCN is integrated into the architecture of GRU. To be specific, the original linear units in LSTM are replaced by graph convolution operator, which is expressed as:
\begin{equation}
\begin{aligned}
&\boldsymbol{r}^{(t)}=\sigma\left(\boldsymbol{\Theta}_r \star \mathcal{G}\left[\boldsymbol{X}^{(t)}, \boldsymbol{H}^{(t-1)}\right]+\boldsymbol{b}_r\right) \\
&\boldsymbol{u}^{(t)}=\sigma\left(\boldsymbol{\Theta}_u \star \mathcal{G}\left[\boldsymbol{X}, \boldsymbol{H}^{(t-1)}\right]+\boldsymbol{b}_u\right) \\
&\boldsymbol{C}^{(t)}=\tanh \left(\boldsymbol{\Theta}_C \star \mathcal{G}\left[\boldsymbol{X}^{(t)},\left(\boldsymbol{r}^{(t)} \odot \boldsymbol{H}^{(t-1)}\right)\right]+\boldsymbol{b}_c\right) \\
&\boldsymbol{H}^{(t)}=\boldsymbol{u}^{(t)} \odot \boldsymbol{H}^{(t-1)}+\left(1-\boldsymbol{u}^{(t)}\right) \odot \boldsymbol{C}^{(t)}
\end{aligned}
\end{equation}
where $\boldsymbol{\Theta}_r \star \mathcal{G}$ denotes the graph convolution operator with parameter $\boldsymbol{\Theta}_r$. Compared with the equation~\ref{eq:gru} of original GRU, we can find that except for the internal graph convolution operator, the external calculation methods of recurrent network are not much different. Similar to some neural translation models~\cite{cho2014learning}, DCRNN can also employ sequence-to-sequence structure to improve predictions. 

\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.48 \textwidth]{f9.png}
\caption{The overview of DCRNN~\cite{li2017diffusion}.}
\label{fig:dcrnn} % FIG
\vspace{-3mm}
\end{figure}

In order to further have a explicit and in-depth understanding of spatio-temporal fusion neural architecture in STGNN, we sort out and analyze the neural architectures of some existing classical models, as shown in Table.
\begin{table*}[h]
\centering
\caption{The summary for neural architectures of some existing STGNN models.}
\label{tab:my-table}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
Model         & Specific Task & Spatial learning & Temporal Learning & Fusion Architecture \\ \hline
STGCN~\cite{yu2017spatio}    &  Traffic state prediction  &  Spectral GCN  &  Gated TCN  &  Factorized                \\ \hline
T-GCN~\cite{zhao2019t}    & Traffic state prediction & Spatial GCN  &  GRU   &  Factorized                   \\ \hline
DCRNN~\cite{li2017diffusion}   & Traffic state prediction & Spatial GCN  &  GRU   &  Coupled                   \\ \hline
GMAN~\cite{zheng2020gman}  & Traffic state prediction  & Graph attention  &  Transformer  &  Factorized             \\ \hline
Graph WaveNet~\cite{wu2019graph} & Traffic state prediction  & Spatial GCN  & Causal TCN  & Factorized                    \\ \hline
GSNet~\cite{wang2021gsnet} & Traffic incident prediction  & Spatial GCN  & GRU  & Factorized                    \\ \hline
STGNN-TTE~\cite{jin2022stgnn} & Travel time prediction  & Spatial GCN  & Gated TCN+Transformer & Factorized                    \\ \hline
Social-STGCN~\cite{mohamed2020social} & Trajectory prediction  & Spatial GCN  & TCN & Factorized                      \\ \hline
Social-STAGE~\cite{malla2021social} & Trajectory prediction  & Spatial GCN  & TCN+Attention & Factorized                    \\ \hline
PM2.5-GNN~\cite{wang2020pm2} & Air quality prediction  & Spatial GCN  & GRU & Factorized                    \\ \hline
SpAttRNN~\cite{huang2021spatio} & Air quality prediction  & Graph attention  & GRU & Coupled                  \\ \hline
ATGCN~\cite{wang2021modeling} & Air quality prediction  & Graph attention  & GRU & Coupled                  \\ \hline
MGWN~\cite{rathore2021multi} & Wind prediction  & Spatial GCN  & Causal TCN & Factorized                  \\ \hline
AGL-STAN~\cite{sun2022spatial} & Crime frequency prediction  & Spatial GCN  & Transformer & Factorized                  \\ \hline
AGCLSTM~\cite{feng2022graph} & Flood situation prediction  & Spatial GCN  & LSTM+Attention & Factorized                  \\ \hline
STEP~\cite{yu2023spatio} & Epidemic prediction  & Spatial GCN+Attention  & GRU & Factorized                 \\ \hline
\end{tabular}%
}
\end{table*}


\section{Improved spatio-temporal dependencies Learning Methods for STGNN}
In section~\ref{sec:basic arch}, we introduce the basic neural architectures for STGNN, enhancing the understanding of the spatio-temporal learning generalization paradigm in STGNN. However, in recent years there have been many frontier methods to improve the learning of spatio-temporal dependencies.
In this section, we summarize and analyze some improved spatio-temporal dependencies learning methods in recent STGNN models. 

\subsection{Spatial Dependencies Learning Methods}

\subsubsection{Multi-Graph Convolution}
In urban systems, there exists multiple relations in the spatial scale. For instance, in transportation systems, the similar traffic patterns may exist in adjacent regions and regions with similar POIs. Hence, simultaneously considering multiple spatial relations is non-negligible for spatio-temporal dependencies learning in STGNN. In recent years, STGNN models with multi-graph convolution have been proposed to address this challenge~\cite{liu2020physical,liang2022fine,ni2022stgmn,he2022multi,jin2022deep,jin2020urban,geng2019spatiotemporal,chai2018bike,liu2022stmgcn,xu2020spatiotemporal}. Among them, STMGCN~\cite{geng2019spatiotemporal} is a typical model for urban ride-haling demand prediction, as shown in Figure~\ref{fig:multi_gnn}. 
This model first constructs multi-graph based on neighborhood, function similarity and connectivity to characterize multiple spatial correlations. For each graph, contexual gated RNN and ChebNet are respectively adopted to capture temporal and spatial dependencies. Finally, the final prediction result is obtained by fusing the parallelized multi-graph spatio-temporal hidden information.
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.48 \textwidth]{f12.png}
\caption{The overview of STMGCN~\cite{geng2019spatiotemporal}.}
\label{fig:multi_gnn} % FIG
\vspace{-3mm}
\end{figure}

\subsubsection{Adaptive Graph Learning}~\label{sec:adpative}
Although multi-graph modeling can characterize multiple spatial correlations to a certain extent, this method still has two limitations. The one is the insufficiency of graph construction, which does not take into account other implicit correlations. Another one is the rationality of graph construction, that is, there is not enough domain knowledge to support the graph construction. Hence, adaptive graph learning methods have been developed gradually to overcome these problems. According to the existing literature, the adaptive graph learning methods in STGNN can be divided into two main categories: random initialization-based and feature initialization-based. 

Random initialization-based adaptive graph learning methods perform adaptive graph structure learning via randomly initialized learnable matrices~\cite{shao2022decoupled,wu2019graph,wu2020connecting,ye2021coupled,han2021dynamic,sun2022spatial,lu2020spatiotemporal,zhang2022adapgl}.
Among them, Graph WaveNet~\cite{wu2019graph} and MTGNN~\cite{wu2020connecting} respectively propose two typical random initialization-based adaptive graph learning methods, which are usually applied applied or improved in subsequent works. The adaptive graph generated in Graph WaveNet is defined as:
\begin{equation}
\tilde{\mathbf{A}}_{a d p}=\operatorname{SoftMax}\left(\operatorname{ReLU}\left(\mathbf{E}_1 \mathbf{E}_2^T\right)\right)
\end{equation}
where $\mathbf{E}_1\in R^{N\times C}$ and $\mathbf{E}_2\in R^{N\times C}$ are respectively source node embedding and target node embedding. They are two learnable matrices with random initialization, where $N$ denotes the number of nodes in graph and $C$ denotes the dimension of the embedding. 
 The adaptive graph generated in MTGNN is defined as:
\begin{equation}
\begin{aligned}
&\mathbf{M}_1  =\tanh \left(\alpha \mathbf{E}_1 \boldsymbol{\Theta}_1\right) \\
&\mathbf{M}_2  =\tanh \left(\alpha \mathbf{E}_2 \boldsymbol{\Theta}_2\right) \\
&\tilde{\mathbf{A}}_{a d p}  =\operatorname{ReLU}\left(\tanh \left(\alpha\left(\mathbf{M}_1 \mathbf{M}_2^T-\mathbf{M}_2 \mathbf{M}_1^T\right)\right)\right)
\end{aligned}
\label{eq:adp}
\end{equation}
where $\mathbf{E}_1$, $\mathbf{E}_2$ represent randomly initialized node embedding, $\mathbf{\theta}_1$ and $\mathbf{\theta}_2$ are model parameters, $\alpha$ is a hyper-parameter for controlling the saturation rate of the activation function.
Many subsequent random initialization-based adaptive graph learning methods are proposed based on the two methods mentioned above. For example, CCRNN~\cite{ye2021coupled} proposed a layer-wise adaptive graph learning mechanism to adjust the graph structures layer by layer, DMSTGCN~\cite{han2021dynamic} presented an adaptive graph learning approach with tensor decomposition. 

Feature initialization-based adaptive graph learning methods perform adaptive graph structure learning based on the given inputs or the hidden states~\cite{zhang2020spatio,li2021dynamic,lan2022dstagnn,jin2022adaptive,peng2021dynamic,fu2020bayesian,guo2020dynamic,shang2021discrete}. Adaptive graph learning methods of this type usually adopt learnable matrices or attention mechanism to incorporate with the given features for constructing the graph structures. For example, DGCRN~\cite{li2021dynamic} proposed a recurrent adaptive graph learning mechanism based on the hidden states to construct the graph structures for each time step, DSTAGNN~\cite{lan2022dstagnn} proposed the self-attention-based  adaptive graph learning method to establish the connection between the graph structures and the hidden states, BSTGCN~\cite{fu2020bayesian} designed a Bayesian graph learning mechanism based on the predefined graphs and input features, GTS~\cite{shang2021discrete} presented a novel probabilistic graph structure learning methods based on the given features.   

\subsubsection{Muti-Scale Spatial Learning}
Due to the existence of spatial heterogeneity in urban systems, different entities could be divided into communities with different functions. The entities in the same community have inter-community correlations, while entities in different communities could also have cross-community correlations. Hence, some multi-scale spatial learning methods based on community partitioning have been proposed in recent years. Among them, some works obtain partitioned communities by artificial division~\cite{zhou2020foresee,zhou2020riskoracle,xu2021highair} or clustering algorithms~\cite{guo2021hierarchical,wang2022multivariate,jin2022deep} while others obtain them by neural networks~\cite{chen2021group,tang2022hgarn,xia2021spatial}. 
For example, ST-SHN~\cite{xia2021spatial} and ST-HSL~\cite{li2022spatial} learn the hyperedges, \emph{i.e.} communities, of the hypergraph to capture global spatial dependencies for crime prediction. Besides, GAGNN~\cite{chen2021group} is a group-aware STGNN model for national air quality prediction, as shown in Figure~\ref{fig:multi_spatial_scale}. This model first proposed differentiable grouping network for learning assignment matrix, which automatically computes the mapping relationships between cities and city groups. Then the spatial GCNs are respectively calculated for graph data at these two different scales to learn the inter-community and cross-community spatio-temporal dependencies. Another relevant research line~ THINK~\cite{agarwal2022think} and DMGCRN~\cite{qin2021dmgcrn} perform the hyperbolic graph neural network on the Poincare ball to directly capture multi-scale spatial dependencies. This is because hyperbolic space can fit the hierarchies such as local and global dependencies of the spatio-temporal data particularly well~\cite{yang2022hyperbolic}. 
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.48 \textwidth]{f13.png}
\caption{The overview of GAGNN~\cite{chen2021group}.}
\label{fig:multi_spatial_scale} % FIG
\vspace{-3mm}
\end{figure}

\subsubsection{Heterogeneous spatial learning}
Different from multi-scale spatial learning methods, some works directly model the fine-grained node-to-node heterogeneous relationships in the spatio-temporal data. To distinguish the influence of static undirected edges (\emph{e.g.}, distance-based edges) and the dynamic directed edges (\emph{e.g.}, vehicle’s mobility-caused edges) in the spatio-temporal graph, HMGCN~\cite{wang2021forecasting} performs heterogeneous aggregation on the spatial dimension. MasterGNN~\cite{han2021joint} construct heterogeneous graph based on the multiple relations between the air monitoring stations and weather monitoring stations. HTGNN~\cite{fan2022heterogeneous} further aggregates heterogeneous information from spatial-based intra-edges, temporal-based inter-edges, and spatio-temporal-based across-time-edges. Another line of heterogeneous spatial learning utilizes transportation, time, and geographical information to capture intricate spatio-temporal message passing. For instance, HeGA~\cite{liu2022hega} and MOHER~\cite{zhou2021modeling} design multiple transportation mode-based heterogeneous graph to receive information from multi-sources at the same time, \emph{e.g.}, bike, bus, vehicle, etc. For example, the overview of MOHER is shown in Figure~\ref{fig:heterogeneous_spatial}, whose spatio-temporal heterogeneous graph is constructed by the region pair-wise relations inter-mode multi-relations to characterize the correlations between different transportation modes. Then the heterogeneous graph convolution operator is incorporated with LSTM to capture the complex spatio-temporal dependencies.
Besides, DH-GEM~\cite{guo2022talent} proposes the node-position edges and CAP~\cite{chen2019cap} further designs node-time and node-location edges in the heterogeneous graph to derive the time and geographical knowledge.
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.48 \textwidth]{f24.png}
\caption{The overview of MOHER~\cite{zhou2021modeling}.}
\label{fig:heterogeneous_spatial} % FIG
\vspace{-3mm}
\end{figure}

\subsection{Temporal Dependencies Learning Methods}

\subsubsection{Muti-Scale Temporal Learning}
Since spatio-temporal data in many scenarios have both short-range and long-range correlations, capturing the multi-scale temporal correlations is also an important direction to improve the temporal dependencies learning. So far, there are two mainstream design directions for multi-scale temporal dependencies learning in STGNN. One is through TCN with different scales of receptive fields~\cite{wu2020connecting,rathore2021multi}, and another one is through the integration of different temporal learning networks~\cite{wang2020traffic,jin2022deep,li2021spatial}. For example, MTGNN~\cite{wu2020connecting} employs multiple TCN with various kernel sizes for learning temporal dependencies in different scales, DMVST-VGNN~\cite{jin2022deep} utilizes TCN and Transformer jointly for long-short range temporal learning.  As shown in Figure~\ref{fig:multi_temporal_scale}, Traffic STGNN~\cite{wang2020traffic} achieves multi-scale temporal learning by multi-networks integration. This model adopts GRU for short-range temporal dependencies learning and Transformer for long-range temporal dependencies learning.   
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.48 \textwidth]{f14.png}
\caption{The overview of Traffic STGNN~\cite{wang2020traffic}.}
\label{fig:multi_temporal_scale} % FIG
\vspace{-3mm}
\end{figure}

\subsubsection{Multi-Granularity Temporal Learning}
In some special scenarios such as transportation systems, the traffic flow at a certain time is not only related to the recent traffic flow, but also may be similar to the traffic flow at that time the previous day or even the previous week, which respectively reflects the closeness, periodicity and trend. To consider the temporal characteristics at these three different granularities, many previous works~\cite{guo2019attention,sun2020predicting,yin2022mtmgnn,zhang2021traffic,zhang2020spatial,wang2022hierarchical} adopt a three-branch architecture to learn the features from different temporal granularities separately, and finally fuses the learned hidden states for prediction. As shown in Figure~\ref{fig:multi_time}, ASTGCN~\cite{guo2019attention} employ a typical three-branch architecture for multi-granularity temporal learning, where $\mathcal{X}_h$, $\mathcal{X}_d$ and $\mathcal{X}_w$ respectively denote the spatio-temporal data of the latest one hour, the data of this hour from the previous day, and the data of this hour from the previous week. After the data from these three branches have undergone the calculation of multiple GCNs and Attention networks respectively, they are finally fused by the learnable weight matrix. 
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f15.png}
\caption{The overview of ASTGCN~\cite{guo2019attention}.}
\label{fig:multi_time} % FIG
\vspace{-3mm}
\end{figure}

\subsubsection{Decomposition Temporal Learning}
In spatio-temporal graph data, individual temporal patterns usually contain a variety of hidden components, such as inherent temporal components, diffusion temporal components, periodic temporal components, etc. To better capture the complex temporal dependencies, decomposition temporal learning methods have been proposed, which can automatically decompose and integrate different temporal components through the specially designed neural networks~\cite{oreshkin2021fc,fang2021spatio,shao2022decoupled,cao2020spectral,zheng2021hierst}. 
%Among them, HierST~\cite{zheng2021hierst}, FC-GAGA~\cite{oreshkin2021fc} and StemGNN~\cite{cao2020spectral} leveraged the N-BEATS~\cite{oreshkin2019n} model to decompose the different components from the individual time series. 
FC-GAGA~\cite{oreshkin2021fc} is a typical work to adopt the subtraction residual from the N-BEATS~\cite{oreshkin2019n} to decompose the different components in the traffic time series and modeled spatial correlations of each component. As shown in Figure~\ref{fig:dtl}, FC-GAGA is stacked by multiple layers. Each layer contains a time gate block, a graph gate block and multiple fully connected blocks. The time gate block aims to remove a node-specific multiplicative seasonality from the input of the block and reuse it at the output of the block, and the graph gate block aims to capture the spatial correlations from different individuals. The fully connected blocks are similar to that in N-BEATS, which act on the final output of the model and remove unnecessary temporal components for downstream blocks through the two branches of forecast projection and backcast projection. 
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f22.png}
\caption{The overview of FC-GAGA~\cite{oreshkin2021fc}.}
\label{fig:dtl} % FIG
\vspace{-3mm}
\end{figure}

Based on FC-GAGA, some other works also employed the decomposition temporal learning methods. For example, StemGNN~\cite{cao2020spectral} decomposed the different temporal components by the subtraction residual from the N-BEATS, but modeled spatial correlations in the spectral domain. D2STGNN~\cite{shao2022decoupled} proposed a temporal residual decomposition method to incorporate with graph structure learning. STWave~\cite{fang2021spatio} directly utilized the discrete wavelet transform to disentangle the event and the trend from the spatio-temporal graph data.

\subsection{Spatio-Temporal Dependencies Fusion Methods}

%\subsubsection{Spatio-Temporal Synchronous Graph Modeling}
\subsubsection{Spatio-Temporal Joint Modeling}
In section~\ref{sec:stfa}, we introduce the basic spatio-temporal fusion neural architectures of STGNN, which are factorized or coupled by spatial learning networks and temporal learning networks. 
Whether it is the factorized or the coupled architecture, they only learn the spatial and temporal dependencies separately and superimpose them, rather than modeling the spatial-temporal dependencies from the joint perspective, thus it is difficult to learn some complex spatio-temporal relations across time steps. In recent years, some works jointly modeling the spatial-temporal dependencies based on 3D GCN~\cite{xia20213dgcn}, spatio-temporal joint GCN (STJGCN)~\cite{zheng2021spatio} and spatio-temporal synchronous GCN (STSGCN)~\cite{song2020spatial}.  
%However, in recent years, there have been a special neural architecture for spatio-temporal dependencies fusion, called as spatio-temporal synchronous graph convolutional network (STSGCN)~\cite{song2020spatial}. 
Among them, STSGCN has become a mainstream method for spatio-temporal dependencies jontly fusion.
This type of neural architecture can model the spatio-temporal dependencies in an unified graph structure, which can replace the separated spatial learning networks and temporal learning networks. The crucial part of STSGNN is construction of the spatio-temporal synchronous graph, as shown in Figure~\ref{fig:sts}. The original spatio-temporal synchronous graph is simple, whose nodes with the same location are connected to each other across adjacent time steps. This graph construction approach can not only characterize the neighbours in spatial scale, but also characterize the neighbours in temporal scale, establishing the unified spatio-temporal relations. After the graph construction, STSGNN directly adopt a simple GCN model to capture the spatio-temporal dependencies.  
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f16.png}
\caption{Construction of the spatio-temporal synchronous graph~\cite{song2020spatial}. $A^{(t_i)}$ denotes the adjacency matrix of the spatial graph at time step $i$. $A^{t_i\rightarrow t_j}$ denotes the connections between the nodes with themselves at the time step $i$ and $j$.}
\label{fig:sts} % FIG
\vspace{-3mm}
\end{figure}

Based on STSGNN, there also have been some works~\cite{wu2022traversenet,li2021spatial,wang2022synchronous,li2022automated,jin2022automated,li2021multi,fang2021cdgnet,fang2022learning} to further improve the spatio-temporal synchronous graph modeling in recent years. For example, STFGNN~\cite{li2021spatial} introduces not only topology-based graph but also similarity-based graph to construct the spatio-temporal synchronous graph, making the spatio-temporal synchronous graph more informative. S2TAT~\cite{wang2022synchronous} proposed a spatio-temporal synchronous Transformer framework to enhance the learning capability with attention mechanisms. 

\subsubsection{Automated Spatio-Temporal Fusion}
Since there are many sophisticated components in STGNN, the rational design for neural architectures is a challenging problem. Most of existing spatio-temporal fusion neural architectures are designed empirically, which could not adapt to different data scenarios, due to the various spatio-temporal attributes in different scenarios. Neural architecture search (NAS) methods bring opportunities for automated spatio-Temporal dependencies fusion in STGNN. We can treat different spatial learning networks or temporal learning networks in STGNN as different blocks, and how these blocks are combined can be learned by the NAS methods. Inspired by AutoST~\cite{li2020autost} for grid-based traffic prediction, AutoSTG~\cite{pan2021autostg} presents the first attempt to involve DARTS~\cite{liu2018darts}, the most classical gradient-based NAS method into STGNN. In AutoSTG, the whole neural network is divided into different stacked cells, and these cells are the basic units to perform NAS, as shown in Figure~\ref{fig:auto}. In the search phase, DARTS obtains the representation of each intermediate hidden state through a probabilistic parameterization method as follows:
\begin{equation}
\mathcal{H}^j=\sum_{i<j} \sum_{o \in \mathcal{O}} \frac{\exp \left(\alpha_o^{(i, j)}\right)}{\sum_{o^{\prime} \in \mathcal{O}} \exp \left(\alpha_{o^{\prime}}^{(i, j)}\right)} o\left(\mathcal{H}^i\right)
\end{equation}
where $\mathcal{H}^i$ denotes the $i_{th}$ intermediate hidden state, $\mathcal{O}$ denotes the operation set, $o$ denotes the specific operation in $\mathcal{O}$, $\alpha_o^{(i, j)}$ denotes the architecture parameter from $i_{th}$ to $j_{th}$ hidden state. 
When the search phase finishes, the neural architecture is fixed according to the operation with the highest $\alpha_o^{(i, j)}$.

\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f17.png}
\caption{The overview of neural architecture search space in AutoSTG~\cite{pan2021autostg}. Zero, identity, spatial graph convolution (SC) and temporal convolution (TC) are four candidate operations in the search space.}
\label{fig:auto} % FIG
\vspace{-3mm}
\end{figure}

Inspired by AutoSTG, there also have been some works~\cite{liautost,jin2022automated,wang2022auto,li2022automated,wu2021autocts,jin2021hierarchical,jin2023urban,ke2023autostg+} to integrate NAS into STGNN in recent years. For example, AutoSTS~\cite{li2022automated} integrates NAS into the spatio-temporal synchronous graph neural networks for searching the optimal architecture of different GCNs and TCNs. Similar to AutoSTS, Auto-DSTSGN~\cite{jin2022automated} also integrates NAS into the spatio-temporal synchronous graph neural networks but this model focuses on searching the optimal adjacency matrices of spatio-temporal synchronous graphs. AutoCTS~\cite{wu2021autocts} implements a joint search for the neural architectures of spatio-temporal blocks and backbones.

\begin{table*}[h]
\centering
\caption{The source codes for some typical STGNN models with improved spatio-temporal dependencies learning methods.}
\label{tab:my-table}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|}
\hline
Main Categories & Sub-Categories & Models & Source Codes \\ \hline
\multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}Spatial Dependencies \\ Learning Methods\end{tabular}} & \multirow{2}{*}{Multi-Graph Convolution} & PVGCN~\cite{liu2020physical} & https://github.com/HCPLab-SYSU/PVCGN \\ \cline{3-4} 
 &  & STMGCN~\cite{geng2019spatiotemporal} & https://github.com/underdoc-wang/ST-MGCN \\ \cline{2-4} 
 & \multirow{3}{*}{Adaptive Graph Learning} & Graph WaveNet~\cite{wu2019graph} & https://github.com/nnzhan/Graph-WaveNet \\ \cline{3-4} 
 &  & MTGNN~\cite{wu2020connecting} & https://github.com/nnzhan/MTGNN \\ \cline{3-4} 
 &  & DGCRN~\cite{li2021dynamic} & https://github.com/tsinghua-fib-lab/Traffic-Benchmark \\ \cline{2-4} 
 & \multirow{4}{*}{Muti-Scale Spatial Learning} & GAGNN~\cite{chen2021group} & https://github.com/Friger/GAGNN \\ \cline{3-4} 
 &  & HGNN~\cite{guo2021hierarchical} & https://github.com/guokan987/HGCN.git \\ \cline{3-4} 
 &  & ST-SHN~\cite{xia2021spatial} & https://github.com/akaxlh/ST-SHN \\ \cline{3-4} 
 &  & ST-HSL~\cite{li2022spatial} & https://github.com/LZH-YS1998/STHSL \\ \cline{2-4} 
 & \multirow{2}{*}{Heterogeneous Spatial Learning} & HTGNN~\cite{fan2022heterogeneous} & https://github.com/YesLab-Code/HTGNN \\ \cline{3-4} 
 &  & DH-GEM~\cite{guo2022talent} & https://github.com/gzn00417/DH-GEM \\ \hline
\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Temporal Dependencies \\ Learning Methods\end{tabular}} & Muti-Scale Temporal Learning & STGNN~\cite{wang2020traffic} & https://github.com/LMissher/STGNN \\ \cline{2-4} 
 & Multi-Granularity Temporal Learning & ASTGCN~\cite{guo2019attention} & https://github.com/guoshnBJTU/ASTGCN-r-pytorch \\ \cline{2-4} 
 & \multirow{3}{*}{Decomposition Temporal Learning} & FC-GAGA~\cite{oreshkin2021fc} & https://github.com/boreshkinai/fc-gaga \\ \cline{3-4} 
 &  & StemGNN~\cite{cao2020spectral}  & https://github.com/microsoft/StemGNN \\ \cline{3-4} 
 &  & STWave~\cite{fang2021spatio} & https://github.com/LMissher/STWave \\ \hline
\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Spatio-Temporal Dependencies \\ Fusion Methods\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Spatio-Temporal Synchronous \\ Graph Modeling\end{tabular}} & STSGCN~\cite{song2020spatial} & https://github.com/Davidham3/STSGCN \\ \cline{3-4} 
 &  & STFGNN~\cite{li2021spatial} & https://github.com/MengzhangLI/STFGNN \\ \cline{2-4} 
 & \multirow{3}{*}{Automated Spatio-Temporal Fusion} & AutoSTG~\cite{pan2021autostg} & https://github.com/AutoSTG/AutoSTG \\ \cline{3-4} 
 &  & Auto-DSTSGN~\cite{jin2022automated} & https://github.com/jinguangyin/Auto-DSTSGN \\ \cline{3-4} 
 &  & AutoCTS~\cite{wu2021autocts} & https://github.com/WXL520/AutoCTS \\ \hline
\multirow{15}{*}{\begin{tabular}[c]{@{}l@{}}Advanced Methods Combined \\ with STGNN\end{tabular}} & \multirow{2}{*}{Adversarial Learning} & MasterGNN~\cite{han2021joint} & https://github.com/hanjindong/MasterGNN \\ \cline{3-4} 
 &  & MT-ASTN~\cite{wang2020multi} & https://github.com/MiaoHaoSunny/MT-ASTN \\ \cline{2-4} 
 & \multirow{2}{*}{Meta-Learning} & ST-MetaNet~\cite{pan2019urban} & https://github.com/panzheyi/ST-MetaNet \\ \cline{3-4} 
 &  & MegaCRN~\cite{jiang2022spatio} & https://github.com/deepkashiwa20/MegaCRN \\ \cline{2-4} 
 & \multirow{3}{*}{Self-Supervised Learning} & STGCL~\cite{liu2022contrastive} & https://github.com/liuxu77/STGCL \\ \cline{3-4} 
 &  & SPGCL~\cite{li2022mining} & https://github.com/RongfanLi98/SPGCL \\ \cline{3-4} 
 &  & ST-SSL~\cite{ji2022spatio} & https://github.com/Echo-Ji/ST-SSL \\ \cline{2-4} 
 & \multirow{3}{*}{Continuous Spatio-Temporal modeling} & STGODE~\cite{fang2021spatial} & https://github.com/square-coder/STGODE \\ \cline{3-4} 
 &  & MTGODE~\cite{jin2022multivariate} & https://github.com/GRAND-Lab/MTGODE \\ \cline{3-4} 
 &  & ST-NCDE~\cite{choi2022graph} & https://github.com/jeongwhanchoi/STG-NCDE \\ \cline{2-4} 
 & \multirow{2}{*}{Physics-Informed Learning} & STDEN~\cite{ji2022stden} & https://github.com/Echo-Ji/STDEN \\ \cline{3-4} 
 &  & Cola-GNN~\cite{deng2020cola} & https://github.com/djkim0516/Cola\_GNN\_review \\ \cline{2-4} 
 & \multirow{3}{*}{Transfer Learning} & ST-GFSL~\cite{lu2022spatio} & https://github.com/RobinLu1209/ST-GFSL \\ \cline{3-4} 
 &  & TL-DCRNN~\cite{mallick2021transfer} & https://github.com/tanwimallick/TL-DCRNN \\ \cline{3-4} 
 &  & DASTNet~\cite{tang2022domain} & https://github.com/YihongT/DASTNet \\ \hline
\end{tabular}%
}\label{tab:code}
\end{table*}

\subsection{Advanced Methods Combined with STGNN}

\subsubsection{Adversarial Learning}
Considering that data point errors such as L1 and L2 norms are usually used as loss functions in traditional predictive learning tasks, such optimization objectives lack the measurement of the distribution and correlation between predicted data and real data, which could cause prediction results distortion.
Hence, adversarial loss can be introduced to incorporate with traditional loss for addressing this problem to some extent, which has been widely applied in time series prediction. Since adversarial loss need to be driven by generative adversarial networks (GANs), neural predictors are usually used as the generators while the neural architecture of discriminators needs to be designed separately. In recent years, there also have been many works~\cite{zhang2019gcgan,khaled2022tfgan,kosaraju2019social,huang2022gan,jin2022gan,han2021joint,liu2022foreseeing,wang2020multi} to combine adversarial loss with STGNN architectures for predictive learning tasks. 
However, for some predictive learning scenarios in social system, it is challenging to discriminate the prediction results from spatio-temporal scales. For example, TFGAN proposed a STGNN model combined with adversarial loss for traffic flow prediction, whose discriminator is composed of GCN and GRU, as shown in Figure~\ref{fig:gan}. The combination of GCN and GRU can jointly discriminate the prediction from spatial and temporal dimensions, which ensures that the predicted results are distributed similarly to the real data at the spatio-temporal scale.
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f18.png}
\caption{The overview of discriminator in TFGAN~\cite{khaled2022tfgan}.}
\label{fig:gan} % FIG
\vspace{-3mm}
\end{figure}

TFGAN is trained adversarially as a min–max game between the generator $G$ and the discriminator $D$. The generator $G$ of this model is a STGNN with multi-graph convolution.
The loss functions in TFGAN are expressed as follows:
\begin{equation}
\begin{aligned}
& \mathcal{L}_{\mathbf{G}}=\mathbb{E}_{\hat{\mathbf{z}} \sim P_{(F)}}[\log (1-\mathbf{D}(\hat{z}))], \\
& \mathcal{L}_{\mathbf{M}}=\frac{1}{b} \sum_{i=1}^b\left\|Y^i-\mathcal{Y}^i\right\|^2, \\
& \mathcal{L}_{\mathbf{D}}=\mathbb{E}_{z \sim P_{(R)}}[\log (\mathbf{D}(z))]+\mathbb{E}_{\hat{z} \sim P_{(F)}}[\log (1-\mathbf{D}(\hat{z}))], \\
& \theta_{\mathbf{G}}, \theta_{\mathbf{D}}=\min _{\theta_{\mathbf{G}}}\left[\lambda \mathcal{L}_{\mathbf{M}}+\max _{\theta_{\mathbf{D}}} \mathcal{L}_{\mathbf{D}}\right] .
\end{aligned}
\end{equation}
where $\mathcal{L}_{\mathbf{G}}$ is the generator loss, $\mathcal{L}_{\mathbf{M}}$ is the mean squared error (MSE) loss between the prediction results and ground-truths, $\mathcal{L}_{\mathbf{D}}$ is the discriminator loss, $\mathbf{D}(\cdot)$ denotes the discriminator network. The parameter of generator network $\theta_{\mathbf{G}}$ and discriminator network $\theta_{\mathbf{D}}$ are optimized by the min–max target.


\subsubsection{Meta-Learning}
Meta-learning is an advanced learning paradigm focused on 'how to learn to learn'. Since STGNN models could capture the high-dimensional heterogeneity and dynamics spatio-temporal dependencies from the raw data, it is very important to teach them how to learn, which allows these models to stand on a higher starting point, thereby significantly improving the prediction performance. In most existing works, the meta-learning technique incorporated in the STGNN model is usually realized by the extraction of additional spatio-temporal attributes by a meta-learner. ST-MetaNet~\cite{pan2019urban} is the first work to introduce meta-learning into STGNN. The overview of ST-MetaNet is shown in Figure~\ref{fig:meta}, whose neural architecture is mainly composed of RNN, Meta-GAT and Meta-RNN. In order to take full advantage of the additional spatio-temporal information, ST-MetaNet proposed two types of meta-knowledge learner: node meta-knowledge (NMK) learner and edge meta-knowledge (EMK) learner. Both of the two different meta-knowledge learner use fully-connected network as the basic learning network. NMK learner aims to learn the meta knowledge from the node attributes (eg., distances and RNs) and EMK learner aims to learn the meta knowledge from the edge attributes (eg., locations and POIs)Then the learned meta knowledge is further used to learn the weights of Meta-RNN and Meta-GAT. 
For example, the calculation process of Meta-RNN for any spatial node $i$ is formulated as:
\begin{equation}
\begin{aligned}
W_{\Omega}^{(i)} & =g_{W_{\Omega}}\left(\mathrm{NMK}\left(v^{(i)}\right)\right) \\
U_{\Omega}^{(i)} & =g_{U_{\Omega}}\left(\mathrm{NMK}\left(v^{(i)}\right)\right) \\
b_{\Omega}^{(i)} & =g_{b_{\Omega}}\left(\mathrm{NMK}\left(v^{(i)}\right)\right) \\
h_t^{(i)} & =\operatorname{GRU}\left(z_t^{(i)}, h_{t-1}^{(i)} \mid W_{\Omega}^{(i)}, U_{\Omega}^{(i)}, b_{\Omega}^{(i)}\right),
\end{aligned}
\end{equation}
where $W_{\Omega}^{(i)}, U_{\Omega}^{(i)}$ and $b_{\Omega}^{(i)}$ are learnable parameters in GRU and they are generated from node attributes $v^{(i)}$ by node knowledge meta learner. The meta learner is composed of three different fully-connected networks $g_{W_{\Omega}}$, $g_{U_{\Omega}}$ and $g_{b_{\Omega}}$.  

\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f19.png}
\caption{The overview of ST-MetaNet~\cite{pan2019urban}.}
\label{fig:meta} % FIG
\vspace{-3mm}
\end{figure}

Based on ST-MetaNet, some other STGNN models combined with meta-learning were proposed. For example, ST-MetaNet+~\cite{pan2020spatio} fuse the dynamic spatio-temporal state and meta-knowledge for weight generation of GAT and GRU. AutoSTG~\cite{pan2021autostg} also adopts meta-learning method similar to ST-MetaNet while introducing neural architecture search, using meta-knowledge to generate weight parameters for graph convolution and temporal convolution. MegaCRN~\cite{jiang2022spatio} proposed an attention-based memory network to memorize typical features in seen samples for further pattern matching, enhancing the capability of graph structure learning. In addition, meta-learning can be also used for spatio-temporal graph knowledge transfer in predictive learning~\cite{lu2022spatio,mo2022cross}.

\subsubsection{Self-Supervised Learning}
Self-supervised learning is a type of methods that transform an unsupervised learning task into a supervised task by self-constructing labels. The aim of this learning paradigm is to learn a better representation for downstream supervised tasks, that is, through self-supervised learning, a representation with strong generalization performance can be learned. Hence, the STGNN models combined with self-supervised learning can boost the capability of spatio-temporal graph learning, improving the accuracy for downstream predictive learning tasks. Contrastive learning is one of the most important self-supervised learning method realized by constructing positive and negative samples, which has been introduced into STGNN models in recent years. STGCL~\cite{liu2022contrastive} is the first work to integrate contrastive learning with STGNN architectures. As shown in Figure~\ref{fig:contrastive}, the first step for STGCL is the data augmentation to construct the positive and negative samples, which includes edge masking, input masking and temporal shifting. After obtaining the positive and negative samples, the same STG encoder is employed to learn the spatio-temporal graph representation respectively for original data and augmented data.
Then, STGCL splits into two branches: predictive branch and contrastive branch. In the predictive branch, STG decoder outputs the prediction results directly and the data point errors such as MAE can be used as the loss function. 
In the contrastive branch, the two types of representation $H^{'}$ and $H^{''}$ are put forward into the projection head to further obtain the latent representation $z^{\prime}$ and $z^{\prime\prime}$. For the two latent representation, the contrastive loss proposed in GraphCL~\cite{you2020graph} can also be adopted in this case, which is as follows:
\begin{equation}
\mathcal{L}_{c l}=\frac{1}{M} \sum_{i=1}^M-\log \frac{\exp \left(\operatorname{sim}\left(\mathbf{z}_i^{\prime}, \mathbf{z}_i^{\prime \prime}\right) / \tau\right)}{\sum_{j \in \chi_i} \exp \left(\operatorname{sim}\left(\mathbf{z}_i^{\prime}, \mathbf{z}_j^{\prime \prime}\right) / \tau\right)}
\end{equation}
where $\operatorname{sim}(\cdot)$ denotes the cosine similarity, $\tau$ denotes  the temperature parameter. Note that, STGCL also proposed to filter out unsuitable negatives based on the unique properties of spatio-temporal graph data such as first-order neighbors of each node, closeness and periodicity temporal patterns, due to their similarity in the latent space. Thus, $chi_i$ denotes the set of acceptable negatives for the $i_{th}$ object after negative filting.
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f20.png}
\caption{The overview of STGCL~\cite{liu2022contrastive}.}
\label{fig:contrastive} % FIG
\vspace{-3mm}
\end{figure}

Based on STGCL, there also have been some other contrastive learning methods proposed to improve the learning capability of STGNN in recent years. For example, SPGCL~\cite{li2022mining} proposed to learn the informative relations by maximizing the distinguishing margin between positive and negative neighbors for generating an optimal graph structure. ST-SSL~\cite{ji2022spatio} proposed an adaptive augmentation method over the spatio-temporal graph data at both attribute and structure levels. START~\cite{jiang2022self} presented a spatio-temporal graph-based contrastive learning methods for trajectory representation learning. This models proposed multiple negative trajectories construction methods such as trajectory trimming and road segments mask, which help STGNN model to achieve better performance in travel time prediction task.


\subsubsection{Continuous Spatio-Temporal modeling}
Most existing works about STGNN model the spatial and temporal dependencies in a discrete form, which leads to discontinuous latent state trajectories and higher prediction errors. To address this problem, continuous spatio-temporal modeling methods have been proposed in recent years. Motivated by neural ordinary differential equation (Neural-ODE)~\cite{chen2018neural}, a well-known approach for continuous systems modeling, STGNN combined with Neural-ODE can improve the capability of spatio-temporal graph representation learning in a continuous form. STGODE~\cite{fang2021spatial} is the first work to introduce Neural-ODE into STGNN. However, this model only considers integrate Neural-ODE with GCN but ignores the continuous modeling for temporal patterns. To achieve the joint continuous modeling for spatio-temporal dependencies, MTGODE~\cite{jin2022multivariate} proposed to respectively integrate Neural-ODE with graph convolution operator and temporal convolution operator, whose spatio-temporal encoding part is shown in Figure~\ref{fig:ode}. 
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.45 \textwidth]{f21.png}
\caption{The spatio-temporal encoding part in MTGODE~\cite{fang2021spatial}. The continuous graph propagation (CGP) module and continuous temporal aggregation (CTA) module are stacked in this part.}
\label{fig:ode} % FIG
\vspace{-3mm}
\end{figure}

In this model, multi-layer GCN with residual connections is transformed as a continuous form, which is formulated by an ordinary differential equation: 
\begin{equation}
\frac{\mathrm{d} \mathbf{H}^G(t)}{\mathrm{d} t}=\left(\widehat{\mathbf{A}}-\mathbf{I}_N\right) \mathbf{H}^G(t)
\end{equation}
where $\widehat{\mathbf{A}}$ is the adjacency matrix, $\mathbf{H}^G(t)$ is the continuous hidden state. Note that, $\widehat{\mathbf{A}}$ in MTGODE is an adaptive graph, which is obtained by Equation~\ref{eq:adp}. To obtain the approximate solution of ODE, $\text { ODESolver }(\cdot)$ can be any black-box ODE solver introduced in~\cite{chen2018neural} such as Euler, Euler-Cauchy and Runge Kuta fourth order. Given the initial hidden state $\mathbf{H}^G(0)$, the continuous hidden state in GCN at $t$ can be approximately computed as:
\begin{equation}
\mathbf{H}^G\left(t_i\right)=\text { ODESolver }\left(\mathbf{H}^G(0), \frac{\mathrm{d} \mathbf{H}^G(t)}{\mathrm{d} t}, t_i\right)
\end{equation}

Similar to continuous GCN, multi-layer TCN with residual connections can also be transformed as a continuous form, which is defined as:
\begin{equation}
\frac{\mathrm{d} \mathbf{H}^T(t)}{\mathrm{d} t}=\mathcal{P}\left(\operatorname{TCN}\left(\mathbf{H}^T(t), t, \boldsymbol{\Theta}\right), R\right)
\end{equation}
where $\mathcal{P}$ denotes the padding operation, $\boldsymbol{\Theta}$ denotes the parameter of convolutional kernel, $R$ denotes the receptive field of TCN. Note that, the temporal dimension needs to be consistent to ensure continuity of the hidden state. Thus, padding operation is necessary in this case. The continuous hidden state at $t$ in TCN can be approximately computed by ODE solver, which is as follows:
\begin{equation}
\widetilde{\mathbf{H}}_{\text {out }}^T=\text { ODESolver }\left(\mathbf{H}^T(0), \frac{\mathrm{d} \mathbf{H}^T(t)}{\mathrm{d} t}, t_i\right)
\end{equation}

In addition, Social ODE~\cite{wen2022social} extended the ODE-based STGNN to the scenario of multi-agent trajectory prediction. MixRNN+~\cite{liang2022mixed} combined Neural-ODE and RNN for continuous recurrent hidden state modeling. STG-NCDE~\cite{choi2022graph} developed a STGNN combined with the neural controlled differential equation (Neural-CDE) for better continuous modeling, compared with Neural-ODE-based methods.

\subsubsection{Physics-Informed Learning}
In recent years, physics-informed neural networks (PINNs)~\cite{karniadakis2021physics} have emerged as a new paradigm for exploring and computing real-world dynamics integrating physical differential equations and neural networks with powerful fitting capabilities. The advantage of PINNs is that they can constrain the predictions within the range that conforms to the laws of physics.
Inspired by PINNs based on simple neural networks, physical-informed learning methods can be also combined with STGNNs, especially in epidemic prediction tasks~\cite{deng2020cola,sun2022using,zheng2020spatial,la2020epidemiological,gao2021stan}. As shown in Figure~\ref{fig:physical}, STAN first integrate the constraints of SIR differential equations into the STGNN architecture. This model used GAT and GRU to capture the spatial and temporal dependencies respectively, and performed a multi-task prediction. There are four components in the output of this model: transmission rate $\beta$, recovery rate $\gamma$, time-varying number of infections $\Delta \mathrm{I}$ and recoveries $\Delta \mathrm{R}$. They need to satisfy physical constraints based on the SIR equation as follows:
\begin{equation}
\begin{aligned}   
&\frac{d R}{d t}=\gamma I \\
&\frac{d I}{d t}=\beta S-\gamma I\\
& S = N - I -R
\end{aligned}
\end{equation}
where $S$ denotes the survivors, $N$ denotes the total number of people. In STAN, a constraint loss was used to enforce that the predicted time-varying infections and recoveries are close to those calculated by the SIR equations.
\begin{figure}[h] 
\centering
\vspace{-3mm}
\includegraphics[width=0.48 \textwidth]{f23.png}
\caption{The overview of STAN~\cite{gao2021stan}.}
\label{fig:physical} % FIG
\vspace{-3mm}
\end{figure}

In addition to the epidemic prediction tasks, there are also a few works in other tasks. For example, STDEN~\cite{ji2022stden} proposed to unify the traffic potential energy field differential equations and neural networks into one framework for traffic flow prediction. The work~\cite{jia2021physics} proposed to transfer knowledge from physics-based models to guide the learning of the recurrent graph convolution neural network for predicting flow and temperature in river networks.

\subsubsection{Transfer Learning}
Since the scarcity of some spatio-temporal graph data, transfer learning techniques have become the lowest-cost way to extend the same basic STGNN model to different data scenarios. However, there are two main limitations for transfer learning combined with STGNN. The one is the heterogeneity of spatial structures and another one is the heterogeneity of temporal patterns in different scenarios. To be specific, in different scenarios, the spatial topology, relations, etc. are completely different as well as the temporal patterns such as periodicity and trend. The existing literature about spatio-temporal graph transfer learning can be roughly divided into three categories: clustering-based~\cite{mallick2021transfer,huang2021transfer,an2022hintnet,yin2022nodetrans}, domain adaptation-based~\cite{tang2022domain,liang2022cross} and meta-learning-based~\cite{lu2022spatio,mo2022cross}. 
For example, TL-DCRNN~\cite{mallick2021transfer} proposed a graph partitioning method to divide the entire highway network into different sub-clusters and then used DCRNN model to learn the spatio-temporal dependencies from source sub-clusters to target sub-clusters. DASTNet~\cite{tang2022domain} combined the graph representation learning and multi-domains adversarial adaptation
methods for obtaining the domain-invariant node embedding, achieving the knowledge transfer among different scenarios with different spatial structures. ST-GFSL~\cite{lu2022spatio} first proposed to employ model-agnostic meta-learning (MAML) method for cross-city knowledge transfer. In this model, the first step is the base-model meta training on multiple source datasets, generating the parameters for adaptation. During the adaptation phase, the feature extractor of basic STGNN is initialized by the generated parameters, and then the parameters of feature extractor and predictor are further jointly trained on the target dataset.  

In order to provide a solid foundation for future research, we have collected sources codes of some typical STGNN models and categorized them according to the improved spatio-temporal dependencies learning methods, as shown in the Table~\ref{tab:code}.

\section{Challenges and Future Directions}
We have investigated the applications, basic neural architectures and recent advancements of STGNN for predictive learning in social systems. Although STGNN models have achieved remarkable performance in recent years, there are still some challenging problems to be addressed, which point to potential future research directions. We summarize these challenges and suggest potentially feasible research directions as follows:
\begin{itemize}
\item \textbf{Lack of interpretability:} So far, the vast majority of STGNN-related work has focused on improving predictive performance through sophisticated model design. However, research on the interpretability of models has been relatively lacking, that is, we cannot clearly understand which spatio-temporal features take a leading role in improving predictive performance. In the most recent work, STNSCM~\cite{deng2023spatio} proposed to construct a causal graph to describe the bike flow prediction and analyze the causal relationship between the spatio-temporal features and prediction results. Causal-based spatio-temporal graph modeling could be a potential direction to enhance the interpretability of STGNN models.
\item \textbf{Lack of calibration methods:} Uncertainty quantification is of great significance to practical industrial production, which reflects the degree of trust in the prediction results of the model. In order to improve the credibility of the deep models, appropriate model calibration methods are necessary, which have been widely used in image recognition~\cite{ovadia2019can} and graph representation learning~\cite{wang2021confident} in recent years. At present, only works~\cite{wu2021quantifying,wen2023diffstg} have studied the uncertainty of STGNN models, and there is a lack of research on calibration methods. Calibration for the STGNN models need to take into account the characteristics of spatial and temporal simultaneously, thus it is more challenging than previous related works.
\item \textbf{Lack of physical constraints:} In most of previous works, STGNN models capture the complex spatio-temporal dependencies through the integration of deep neural networks, while ignoring the consideration of physical constraints in different application domains, which makes the model less recognized in some professional fields. In recent years, although some STGNN models for epidemic prediction have combined professional differential equations as physical constraints~\cite{deng2020cola,sun2022using,zheng2020spatial,la2020epidemiological,gao2021stan}, such work is still lacking and needs to be improved in other application fields.
%In recent years, differential equations with special physical meaning in some professional fields have been used in combination with neural networks to limit the predictions within a reasonable range, which are called as PINN.
\item \textbf{Lack of pre-training techniques:} Pre-training techniques have been greatly developed in the fields of time series and graph representation learning in recent years, but they are relatively lacking in STGNN-related work. In the most recent work, STEP~\cite{shao2022pre} proposed a pre-training model combined with the Mask Auto-Encoder (MAE)~\cite{he2022masked} architecture to efficiently learn temporal patterns from very long-term history spatio-temporal graph data. In the future, pre-training techniques for long-range spatial and long-term temporal learning are necessary, which are of great value to the scalability and deployability of the STGNN models.

\item \textbf{Hurdle of distribution shifts:} Spatio-temporal data, such as traffic flows over road networks, are often collected from various locations and time periods, resulting in significant differences in the distribution of the training, validation, and test sets. For instance, the training set may span the first two years, while the validation and test sets come from the following two years. This can pose a challenge for STGNNs, as training a model on one dataset may not perform well on validation and test sets due to \emph{distribution shifts}, which is similar to the distribution shift issue in domain adaptation (where the joint distribution of inputs and outputs differs between the training and test stages). Despite its importance, this problem has received less attention in the spatio-temporal research community. While several studies \cite{du2021adarnn} investigated defeating distribution shifts in time series, they fail to encode the spatial correlations among locations.

\item \textbf{Exploring new training strategies}: Previous studies have primarily focused on introducing novel STGNNs with sophisticated layers or modules to enhance human mobility analytics. However, another promising direction is to investigate new training strategies. For instance, in traffic prediction tasks, every location is treated equally, and the data belonging to these locations are jointly fed into neural networks. Nevertheless, the complexity of modeling the spatio-temporal correlations of each location can vary significantly, necessitating a new training strategy such as curriculum learning. Curriculum learning trains a machine learning model on increasingly difficult data, starting from simpler data, and may be effective in addressing this issue. In addition, other potential training strategies for STGNNs include multi-task learning, transfer learning, and continual learning. By exploring new training strategies, we can improve the performance and accuracy of STGNNs and enable them to tackle even more complex tasks.

\item \textbf{Scalability issue}: One particularly challenging case for designing efficient STGNNs is when the number of locations in the sensor network is very large. For example, there are over ten thousand of loop detectors in PEMS systems. In this scenario, there is a need to develop STGNNs that can efficiently process and analyze the vast amounts of spatio-temporal data generated by the network while maintaining high prediction accuracy. Under this circumstance, more efficient AI solutions are appreciated, e.g., through model pruning/distillation, graph sampling techniques, or exploring the next-generation AI models with high efficiency. There are also a few studies probing into graph-free approaches \cite{liu2023we} to reduce computational costs when scaling up to large-scale sensor networks.

\end{itemize}

\section{Conclusion}
In this paper, we present a systematic survey of spatio-temporal graph neural networks (STGNNs) for predictive learning in urban computing. We firstly start with the basic form and construction method of spatio-temporal graph data, and then summarize the predictive learning tasks involving STGNNs from different application domains in urban computing.
Next, we start from the perspective of the basic neural network architectures and introduce the basic components of the spatial learning network and temporal learning network that constitute STGNNs, such as graph neural networks (GNNs), recurrent neural networks (RNNs), temporal convolutional networks (TCNs), self-attention networks (SANs), and summarize the basic fusion form of these spatio-temporal neural architectures.
To further track the frontier progress of STGNNs in recent years, we summarize the related typical works with the main line of spatial dependencies learning methods, temporal dependencies learning methods, spatio-temporal dependencies fusion methods and other advanced techniques that can be combined.
Finally, we summarize the challenges of current research and suggest some potential directions.

% \section*{Acknowledgment}


% %Dr. Reveryrand would like to acknowledge the funding by XLIM, Limoges, France. 
% The authors would like to thank Dr. David Root and Dr. Jean-Pierre Teyssier at Agilent Technologies for the loan of the time-domain nonlinear measurement equipment and TriQuint Semiconductor for the donation of the transistors. 



% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

% ============================================
%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here %\cite{Roberg2010}.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgement
%\section*{Acknowledgment}


%The authors would like to thank D. Root for the loan of the SWAP. The SWAP that can ONLY be usefull in Boulder...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% ====== REFERENCE SECTION

%\begin{thebibliography}{1}

% IEEEabrv,

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,Bibliography}
%\end{thebibliography}
% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% ==== SWITCH OFF the BIO for submission
% ==== SWITCH OFF the BIO for submission
\begin{IEEEbiography}[{\includegraphics[width=1.0in,height=1.5in,clip,keepaspectratio]{bio_figure/jinguangyin.jpg}}]{Guangyin Jin} received a B.E. degree from the Department of Mechanical and Electrical Engineering of Xiamen University. Now he is a Ph.D candidate at College of Systems Engineering of National University of Defense Technology. His research interest falls in the area of spatial-temporal data mining, graph neural networks, urban computing and intelligent transportation. So far, he has published more than ten papers in JCR Q1-level international journals such as TITS, TRC, INS, and top international conferences such as AAAI, CIKM, SIGSPATIL. He also serves as the PC member or reviewer for top international conferences or journals such as AAAI, WWW, ECML-PKDD, TITS, TKDD, TRC, etc.
\end{IEEEbiography}

\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio_figure/yuxuan.jpeg}}]{Yuxuan Liang}
 is a Research Fellow at School of Computing, National University of Singapore. He is currently working on the research, development, and innovation of spatio-temporal data mining and AI, with a broad range of applications in smart cities. Prior to that, he obtained his PhD degree at NUS. He published over 40 peer-reviewed papers in refereed journals and conferences, such as KDD, WWW, NeurIPS, ICLR, ECCV, AAAI, IJCAI, Ubicomp, and TKDE. Those papers have been cited over 2,000 times (Google Scholar H-Index: 21). He was recognized as 1 out of 10 most innovative and impactful PhD students focusing on data science in Singapore by Singapore Data Science Consortium (SDSC).
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1.0in,height=1.5in,clip,keepaspectratio]{bio_figure/fyc.jpg}}]{Yuchen Fang} is currently working toward an M.E. degree with the School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China. His general research interests are in spatio-temporal data mining, graph neural networks, and urban computing, with a special focus on traffic forecasting. He has published several papers in top journals and conference proceedings, such as ICDE, SIGIR, AAAI, and TITS.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1.0in,height=1.5in,clip,keepaspectratio]{bio_figure/hjc.jpg}}]{Jincai Huang} is a professor of College of Systems Engineering of National University of Defense Technology, Changsha, Hunan, China. His main research interests include artificial general intelligence, deep reinforcement learning, and multi-agent systems. He has published more than 60 SCI / EI indexed papers, and he also serves as a director of the Machine Learning Committee of the Chinese Association for Artificial Intelligence (CAAI). 
\end{IEEEbiography}

\begin{IEEEbiography}
    [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio_figure/jbzhang.jpg}}] {Junbo Zhang} is a Senior Researcher of JD Intelligent Cities Research. He is leading the Urban AI Product Department of JD iCity at JD Technology, as well as AI Lab of JD Intelligent Cities Research. Prior to that, he worked at Micorsoft Research Asia from 2015 to 2018. He has published over 60 research papers in spatio-temporal data mining and AI, urban computing, deep learning, and federated learning. He serves as an Associate Editor of ACM Transactions on Intelligent Systems and Technology. He received a number of honors, including the Second Prize of the Natural Science Award of the Ministry of Education in 2021, the 22nd and 23rd China Patent Excellence Award in 2021 \& 2022, CCF Award for Science and Technology Award--Technological Progress Outstanding Award in 2021. He is an ACM/IEEE/CCF senior member. 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio_figure/yuzheng.jpg}}]{Yu Zheng} (Fellow, IEEE) is the Vice President of JD.COM and head JD Intelligent Cities Research. Before Joining JD.COM, he was a senior research manager at Microsoft Research. He currently serves as the Editor-in-Chief of ACM Transactions on Intelligent Systems and Technology and has served as the program co-chair of ICDE 2014 (Industrial Track), CIKM 2017 (Industrial Track) and IJCAI 2019 (industrial track). He is also a keynote speaker of AAAI 2019, KDD 2019 Plenary Keynote Panel and IJCAI 2019 Industrial Days. His monograph, entitled Urban Computing, has been used as the first text book in this field. In 2013, he was named one of the Top Innovators under 35 by MIT Technology Review (TR35) and featured by Time Magazine for his research on urban computing. In 2016, Zheng was named an ACM Distinguished Scientist and elevated to an IEEE Fellow in 2020 for his contributions to spatio-temporal data mining and urban computing.
\end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1.0in,height=1.5in,clip,keepaspectratio]{bio_figure/lfx.jpg}}]{Fuxian Li} is a master candidate at Department of Electronics Engineering of Tsinghua University. His research interest falls in the area of spatial-temporal data mining and multivariate time series forecasting.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio_figure/huan_yan.eps}}]{Huan Yan} received the Ph.D. degree in the department of electronic engineering of Tsinghua University, Beijing, China, in 2019. He is currently a Post-Doctoral Researcher in the department of electronic engineering of Tsinghua University. His current research interests include deep learning based spatial-temporal data mining and user modeling.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.5in,clip,keepaspectratio]{bio_figure/zjl.png}}] {Jinlei Zhang}  received a Ph.D. degree from Beijing Jiaotong University, China. He is currently an assistant professor at Beijing Jiaotong University. His research interests include machine learning, deep learning, traffic data mining and applications, and dynamic traffic modeling and management. 
% \end{IEEEbiography}


% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.5in,clip,keepaspectratio]{bio_figure/wmd.jpg}}] {Mudan Wang} is currently working toward the MS degree in big data from the Department of Electronic Engineering, Tsinghua University, Beijing, China. Her research interests include spatio-temporal data mining and urban computing.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1.0in,height=1.5in,clip,keepaspectratio]{bio_figure/hjc.png}}]{Jincai Huang} is a professor of the National University of Defense Technology, Changsha, Hunan, China, and a researcher of Science and Technology on Information Systems Engineering Laboratory. His main research interests include artificial general intelligence, deep reinforcement learning, and multi-agent systems.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio_figure/yong.eps}}]{Yong Li}(M'2009) received his B.S. degree in Electronics and Information
% Engineering from Huazhong University of Science and Technology,
% Wuhan, China, in 2007, and his Ph.D. degree in electronic
% engineering from Tsinghua University, Beijing, China, in 2012.
% During July to August in 2012 and 2013, he worked as a Visiting
% Research Associate in Telekom Innovation Laboratories (T-labs) and
% HK University of Science and Technology, respectively. During
% December 2013 to March 2014, he visited University of Miami, FL, USA
% as a Visiting Scientist. He is currently a faculty member of the
% Electronic Engineering at the Tsinghua University.
% His research interests are in the areas of networking and
% communications, including mobile opportunistic networks,
% device-to-device communication, software-defined networks, network
% virtualization, future Internet, etc. He was a recipient of the Outstanding Postdoctoral Researcher, Outstanding Ph.D. Graduates, and Outstanding Doctoral Thesis awards from Tsinghua University. His research is granted
% by Young Scientist Fund of Natural Science Foundation of China,
% Postdoctoral Special Find of China, and industry companies of
% Hitachi, ZET, etc. He has published more than 100 research papers
% and has 10 granted and pending Chinese and International patents. He has served as Technical Program Committee (TPC) Chair
% for WWW workshop of Simplex 2013, served as the TPC of several
% international workshops and conferences. He is also a guest-editor
% for ACM/Springer Mobile Networks and Applications, Special Issue on
% Software-Defined and Virtualized Future Wireless Networks. Now, he
% is the Associate Editor of EURASIP journal on wireless
% communications and networking.
% \end{IEEEbiography}

%% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{Ignacio Ramos}
%(S'12) received the B.S. degree in electrical engineering from the University of Illinois at Chicago in 2009, and is currently working toward the Ph.D. degree at the University of Colorado at Boulder. From 2009 to 2011, he was with the Power and Electronic Systems Department at Raytheon IDS, Sudbury, MA. His research interests include high-efficiency microwave power amplifiers, microwave DC/DC converters, radar systems, and wireless power transmission.
%\end{IEEEbiographynophoto}

%% insert where needed to balance the two columns on the last page with
%% biographies
%%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}
% ==== SWITCH OFF the BIO for submission
% ==== SWITCH OFF the BIO for submission



% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


