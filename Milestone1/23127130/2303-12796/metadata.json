{
    "arxiv_id": "2303.12796",
    "paper_title": "An Analysis of Abstractive Text Summarization Using Pre-trained Models",
    "authors": [
        "Tohida Rehman",
        "Suchandan Das",
        "Debarshi Kumar Sanyal",
        "Samiran Chattopadhyay"
    ],
    "submission_date": "2023-02-25",
    "revised_dates": [
        "2023-03-24"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
    ],
    "abstract": "People nowadays use search engines like Google, Yahoo, and Bing to find information on the Internet. Due to explosion in data, it is helpful for users if they are provided relevant summaries of the search results rather than just links to webpages. Text summarization has become a vital approach to help consumers swiftly grasp vast amounts of information.In this paper, different pre-trained models for text summarization are evaluated on different datasets. Specifically, we have used three different pre-trained models, namely, google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum to get the output from the above three models. The pre-trained models are compared over these different datasets, each of 2000 examples, through ROUGH and BLEU metrics.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12796v1"
    ],
    "publication_venue": "11 Pages, 6 Figures, 3 Tables",
    "doi": "10.1007/978-981-19-1657-1_21"
}