\section{Introduction}
\label{sec:intro}

Photo-realistic face image synthesis, editing and animation attract significant interests in computer vision and graphics, with a wide range of important downstream applications in visual effects, digital avatars, telepresence and many others. With the advent of Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative}, remarkable progress has been achieved in face image synthesis by StyleGAN~\cite{karras2019style,Karras2020stylegan2,karras2021alias} as well as in semantic and style editing for face images~\cite{shi2021SemanticStyleGAN,tov2021designing}. To manipulate and animate the expressions and poses in face images, many methods attempted to leverage 3D parametric face models, such as 3D Morphable Models (3DMMs)~\cite{blanz1999morphable,paysan20093d}, with StyleGAN-based synthesis models~\cite{deng2020disentangled,tewari2020stylerig,piao2021inverting}. However, all these methods operate on 2D convolutional networks (CNNs) without explicitly enforcing the underlying 3D face structure. Therefore they cannot strictly maintain the 3D consistency when synthesizing faces under different poses and expressions.

Recently, a line of work has explored neural 3D representations by unsupervised  training of 3D-aware GANs from in-the-wild unstructured images~\cite{chan2021pi,gu2021stylenerf,or2021stylesdf,xue2022giraffe,chan2021efficient,epigraf,xu2021generative,shi2021lifting,deng2021gram,schwarz2020graf,zhou2021CIPS3D}. Among them, methods with generative Neural Radiance Fields  (NeRFs)~\cite{mildenhall2020nerf} have demonstrated striking quality and multi-view-consistent image synthesis
~\cite{epigraf,chan2021efficient,or2021stylesdf,gu2021stylenerf,deng2021gram}. The progress is largely due to the integration of the power of StyleGAN in photo-realistic image synthesis and NeRF representation in 3D scene modeling with view-consistent volumetric rendering. Nevertheless, these methods lack precise 3D control over the generated faces beyond camera pose, as well as the quality and consistency in control over other attributes, such as shape, expression, neck and jaw pose, leave much to be desired.

In this work, we present \emph{\papername}, a novel geometry-guided 3D head synthesis model trained from in-the-wild unstructured images. Our model can synthesize a wide range of 3D human heads with full control over camera poses, facial expressions, head shapes, articulated neck and jaw poses. To achieve such high level of disentangled control for 3D human head synthesis, we devise our model learning \emph {in two stages}. We first define a novel \emph{semantic signed distance function} (SDF) around a head geometry (i.e. FLAME~\cite{FLAME:SiggraphAsia2017}) conditioned on its control parameters. This semantic SDF fully distills rich 3D geometric prior knowledge from the statistical FLAME model and allows us to build a differentiable \emph{volumetric correspondence map} from the \emph{observation space} to a disentangled \emph{canonical space} from all the control parameters. In the second training stage, we then leverage the state-of-the-art 3D GAN framework (EG3D~\cite{chan2021efficient}) to synthesize realistic shape and appearance of 3D human heads in the canonical space, including the modeling of hair and apparels. Following that, a volume rendering step is guided by the volumetric correspondence map to output the geometry and image in the observation space.

%decomposes the generation into \emph{canonical avatar generation} and \emph{semantic volume deformation}. 
%Specifically we first leverage the state-of-the-art 3D GAN model EG3D~\cite{chan2021efficient} to learn generative head NeRFs with canonical shape and expression, for modeling the geometry and appearance of a synthesized identity. 
%Secondly we tackle the controllability of the generated NeRFs by explicitly deforming the volumetric rendering space with a parametric 3D head model (i.e. FLAME~\cite{FLAME:SiggraphAsia2017}). 
%The volumetric deformation is conditioned on the FLAME parameters, characterized with 3D point-to-point correspondence mapping between the observation and canonical space. 

%In contrast to learning an unstructured deformation as a part of the unsupervised 3D GAN training, we define a novel \emph{semantics-aware} volumetric correspondence mapping in compliance with the properties of signed distance function (SDF). By learning from parameterized 3D mesh collections, our mapping function fully distills the rich 3D geometric prior knowledge from the statistical FLAME model.  
%As such, our method can generate arbitrary 3D-aware heads in the observation space by deforming the canonical one accordingly to the desired semantics. Our formulation by design eases the 3D GAN learning difficulties by instilling the geometric deformation prior knowledge, and enables independent and explicit control over appearance and expression. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.\textwidth]{figures/pipeline.png}
    \caption{\textbf{Overview of our training framework.} \textbf{Stage 1}: Trained from parameterized FLAME~\cite{FLAME:SiggraphAsia2017} mesh collections, a MLP-network $W$ maps a shape  $\bb{\alpha}$, expression $\bb{\theta}$ and articulated jaw and neck pose $\bb{\theta}$ into 3D point-to-point volumetric correspondences from observation to canonical space, together with a signed distance function of the corresponding FLAME head. \textbf{Stage 2}: Given a Gaussian latent code $\bb{z},$ our model generates a tri-plane represented 3D feature space of a canonical head, disentangled with shape and expression controls. The volume rendering is then guided by the volumetric correspondence field to map the decoded neural radiance field from the canonical to observation space. We condition the NeRF decoding with expression and joint pose for modeling dynamic details. A super-resolution module synthesizes the final high-resolution RGB image from the volume-rendered feature map. For fine-grained shape and expression control, we apply the FLAME SDF as geometric prior to the synthesized NeRF density, and self-supervise the image synthesis to commit to the target expression $\bb{\beta}$ and joint pose $\bb{\theta}$ by comparing the input code against the re-estimated values $\hat{\bb{\beta}},\hat{\bb{\theta}} $ from synthesized images. }
    \label{fig:overview}
    \vspace{-0.1in}
\end{figure*}

To ensure the consistency of synthesized 3D head shape with controlling head geometry, we introduce a \emph{geometry prior loss} to minimize the difference between the synthesized neural density field and the FLAME head SDF in observation space. Furthermore, to improve the control accuracy, we pre-train an image encoder of the control parameters and formulate a \emph{control loss} to ensure synthesized images matching the input control code upon encoding. Another key aspect of synthesis realism is dynamic details such as wrinkles and varying shading as subjects change expressions and poses. To synthesize dynamic details, we propose to condition EG3D's triplane feature decoding with noised controlling expression. 
% a robust \emph{noised expression conditioning} before  without introducing pose-dependent overfitting.

Compare to state-of-the-art methods, our method achieves superior synthesized image quality in terms of Frechet Inception Distance (FID) and Kernel Inception Distance (KID). Our method can consistently preserve the identity of synthesized subjects with compelling dynamic details while changing expressions and poses, outperforming prior methods both quantitatively and qualitatively. 
%We also provide an ablation study to justify many of our design choices.

%Although the aforementioned design can synthesize 3D-aware head images with reasonable control accuracy, we observe noticeable shape and expression variations across synthesized instances of the same control parameters. Therefore to further enhance the control accuracy, we propose geometry-wise and image-wise guidance to the neural radiance generation. In particular, we co-learn a parameterized \emph{signed distance function} (SDF) together with the canonical correspondence function. We then impose a geometry-aware guidance on the generation of neural density field, where the signed distance value serves as a prior to the density value of each spatial point in the observation space. On the image level, assisted with a pretrained image reconstruction module, we introduce a \emph{cycle reconstruction loss} to self-supervise the image synthesis to the conditioned expression. Additionally we enhance the temporal realism of a synthesized head animation by modeling the dynamic details as expression varies. Our novel design and losses enable appealing synthesis of 3d-aware head animation, with precise and expressive control, such as eye blinks  and head shaking.        

The contributions of our work can be summarized as:
\begin{itemize}
    \vspace{-0.08in}
    \item A novel geometry-guided 3D GAN framework for high-quality 3D head synthesis with full control on camera poses, facial expressions, head shapes, articulated neck and jaw poses.
    \vspace{-0.08in}
    \item A novel semantic SDF formulation that defines the volumetric correspondence map from observation space to canonical space and allows full disentanglement of control parameters in 3D GAN training.
    \vspace{-0.08in}
    \item A geometric prior loss and a control loss to ensure the head shape and expression synthesis accuracy.
    \vspace{-0.08in}
    \item A robust noised expression conditioning scheme to enable dynamic detail synthesis.
\end{itemize}

