\section{Experiments}
\label{sec:experiments}

\begin{table*}[t]
	\renewcommand{\tabcolsep}{2pt}
	\small
	\begin{subtable}[!t]{0.6\textwidth}
		\centering
	\begin{tabular}{l|cc|cccc}
	\hline
	% Model & FID \downarrow & KID  \downarrow & DS_{\bb{\alpha}}\uparrow & DS_{\bb{\beta},\bb{\theta}}\uparrow & DS_{\bb{c}}\uparrow & IS \uparrow \\
 	Model & FID $\downarrow$  & KID $\downarrow$  & DS$_{\bb{\alpha}}$$\uparrow$ & DS$_{\bb{\beta},\bb{\theta}}$$\uparrow$ & DS$_{\bb{c}}$ $\uparrow$ & IS$\uparrow$  \\
	\hline
	DiscoFaceGAN-SR~\cite{deng2020disentangled} & 42.6 & 0.04 &  - & 27.2 & 0.11 & 0.59\\
	GAN-Control~\cite{shoshan2021gan} & 9.8 & 0.005 &  - & 43.7 & 0.28& 0.62\\
	\hline
	HeadNeRF~\cite{hong2022headnerf}  & 163 & 0.168 &  1.33 & 8.9 & 0.25 & 0.678\\
	AniFaceGAN-SR~\cite{wu2022anifacegan} & 30.5 & 0.024 &  2.0 & 11.1 & 0.04 & 0.73\\
	3DFaceShop~\cite{tang2022explicitly} & 24.8 & 0.018 &  0.45 & 64.2 & 0.029 & 0.594 \\
	\hline
	Ours  & \bb{5.7} & \bb{0.0016} & \bb{3.1} & \bb{71.9} & \bb{0.35} & \bb{0.80} \\
	\hline
	\end{tabular}
		\caption{Baseline comparisons.
		}
% 	\vspace{-0.06in}
	\label{tab:baseline_comp}
	\end{subtable}
	\hspace{\fill}
	\begin{subtable}[!t]{0.4\textwidth}
		\centering
		\vspace{-0.1in}
	\begin{tabular}{l|cc|c}
	\hline
	Model & Ours wo $L_{enc}$  & Ours wo $L_{prior}$ & Ours \\
	\hline
	FID $\downarrow$  & 6.0& 6.1 & \bb{5.7}\\
	KID  $\downarrow$ & 0.00172 &0.00174 & \bb{0.0016}\\
	\hline
	ASD (cm)  $\downarrow$ & \bb{0.124}& 0.135 & \bb{0.124}\\
	$var({\bb{\alpha}})\downarrow$  & 0.000525 & 0.000577 & \bb{0.000515}\\
	AED (cm)  $\downarrow$  &0.1233 & 0.0813 & \bb{0.0797} \\
	$var({\bb{\beta}, \bb{\theta}})\downarrow$  & 0.004707& 0.00406 & \bb{0.00404} \\
	\hline
	\end{tabular}
		\caption{Ablation study.
		}
% 	\vspace{-0.06in}
	\label{tab:ablation}
	\end{subtable}
	\hspace{\fill} 
	\vspace{-0.06in}
	\caption{ (a) Our method outperforms the prior 2D and 3D controllable image synthesis methods in both image quality and control disentanglement. (b) Our method demonstrates the best control accuracy over shape and expression with geometric prior $L_{prior}$ and self-supervised reconstruction loss $L_{enc}.$
	}
	\label{ablations}
	\vspace{-0.15in}
\end{table*}

\paragraph {Training and Dataset.} Our training is devised into two stages. In the pretraining stage, we build our semantic SDF $W$ using a four 192-dimensional MLP with a collection of 150K FLAME meshes by Gaussian sampling of the control parameters $\bb{p}.$ For each mesh, we sample 4K surface samples $N$ with surface normals $\bb{n}$ and ground-truth correspondence point $\bar{\bb{x}}^*$, and 4K off surface samples $F$ distributed uniformly inside the box-bounded volume. 
Our canonical mesh has a neutral shape $\bb{\alpha}=0$, expression $\bb{\beta}=0$ and neck pose, but with an opening jaw at 10 degrees. We close the face connectivity between the lips for a watertight geometry and also as a proxy geometry for the mouth cavity modeling. We open the jaw at the canonical space such that the spatial geometry and appearance inside the mouth can be modeled with distinguishable triplane features.   
In the second training stage, we freeze the weights of $W$ for best efficiency and train our model on the FFHQ~\cite{karras2019style}, a human face dataset with $70,000$ in-the-wild images. For each image, we estimate the camera pose $\bb{c}$ and FLAME parameters $\bb{p}$ with a nonlinear optimization in fitting 2D landmarks, assuming zero root and neck rotation and camera located $1.0$m away from the world origin. We refer to the supplementary materials for more details. We rebalance the FFHQ dataset by duplicating images with large head poses and jaw opening expressions. We note that we also use pairs of images and $(\bb{\beta},\bb{\theta})$ for fine-tuning of a light-weight image encoder $E,$ using a ResNet50~\cite{he2016deep} backbone followed with a single-layer MLP. 

% TALK ABOUT WHY WE CAN EXTRAPOLATE
\subsection{Qualitative Comparisons}

\paragraph{Controlled Portrait Synthesis.}
Our framework enables high-fidelity head synthesis with disentangled control over camera pose, shape and expression, as shown in Figure~\ref{fig:teaser}. We compare our method with prior controllable portrait synthesis works include DiscoFaceGAN~\cite{deng2020disentangled}, GAN-Control~\cite{shoshan2021gan}, HeadNeRF~\cite{hong2022headnerf}, AniFaceGAN~\cite{wu2022anifacegan} and 3DFaceShop~\cite{tang2022explicitly}. The first two methods integrates 3DMM controls into 2D face synthesis. HeadNeRF~\cite{hong2022headnerf} is a 3D parametric NeRF-based head model built from both indoor multiview image capture datasets and in-the-wild monocular image collections. AniFaceGAN~\cite{wu2022anifacegan} and 3DFaceShop~\cite{tang2022explicitly} are concurrent 3D-aware controllable GANs in face synthesis. 

 We qualitatively compare with prior work in Figure~\ref{fig:shape_exp_var} for expression control.
%  Specifically DiscoFaceGAN and GAN-Control do not support shape editing whereas we observe significant identity variations when changing the shape for AniFaceGAN and 3DFaceShop (Figure~\ref{fig:shape_exp_var}, top row). 
% Our approach well preserves the facial identity feature even with significant shape variation, indicating a clean disentanglement of shape modeling from appearance generation. We also visually compare the expression control in the bottom part of Figure~\ref{fig:shape_exp_var}.
GAN-Control shows high-quality expression manipulation but with noticeable identity variation with differences in hair, head shape contour and background, whereas DiscoFaceGAN better maintains identity but with lower perceptual quality, and lacks dynamic details as expression varies. The expression control space of 3DFaceShop~\cite{tang2022explicitly} is sensitive, suffering from appearance changes even with minor expression variation. AniFaceGAN demonstrates visually-consistent expression editing but with limited resolution and image quality, e.g., with blurry artifacts in hair and teeth. HeadNeRF ensures decent consistency in image generation by rendering the conditional NeRF but lacks fine details with very limited perceptual quality. In contrast, our approach produces the most compelling images with consistent appearance and dynamic details under expression changes. 
Moreover, controllable neck pose is a key factor towards realistic video avatar in applications like talking head synthesis. As shown in Figure.~\ref{fig:teaser}, our method achieves explicit control of neck and jaw poses which are seldom explored in prior work. 
Please refer to supplementary material for qualitative comparison in view consistency and shape editing. 


% In Figure.~\ref{fig:view_var}, we further visually compare the view consistency using Epipolar Line Images (EPI) similar to~\cite{xiang2022gram}. The 2D generative models shows inferior view consistency compared to NeRF-based 3D-aware generative models. The appearance transition of HeadNeRF is more noisy, largely due to the lack in fine details in such areas as teeth. Our method shows natural and continuous pattern transition when smoothly changing the views, comparable with 3DFaceShop and AniFaceGAN. 
\vspace{-0.05in}
\paragraph{Dynamic details.}
Our method presents highly-consistent image synthesis in control of shape, expression and camera poses, but also depicts temporal realism in portrait animation, credited to the modeling of expression-dependent dynamic details. As shown in the last row of Figure~\ref{fig:ablation}, the appearance of wrinkles around the mouth and eyes when transiting from a neural expression to smiling largely enhances the animation realism. In comparison, without explicitly modeling the dynamic details, the wrinkles are embedded in the appearance and do not vary with expressions.  
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/dynamicdetails.png}
%     \caption{Dynamic details.}
%     \label{fig:extrap}
% \end{figure}
\vspace{-0.15in}
\paragraph{Expressive Head Synthesis with Extrapolated Controls.}
AniFaceGAN~\cite{wu2022anifacegan} and 3DFaceShop~\cite{tang2022explicitly} depict dynamic details as well since their generation of the neural fields is conditioned on the input expression latent code. However, their designs result in shape and expression entanglement with the appearance generation, as reflected in the identity changes as shown in Figure.~\ref{fig:shape_exp_var}. By embedding the controls in a Gaussian latent space with such as Variation AutoEncoder (VAE), their approaches sacrifice expressiveness. The synthesized image quality is also highly correlated to the distribution of training images with target expressions. 

In contrast, our tri-plane generation is explicitly disentangled from shape and expression control. Moreover, the volume deformation is independently learnt from the deformed FLAME
mesh collections which offer abundant 3D geometric knowledge with largely augmented control space. Therefore we are much less dependent on the distribution of the training images and support better extrapolation to unseen novel expressions. In Figure.~\ref{fig:teaser}, we show high-quality synthesized head with extreme jaw and neck articulated movements which do not exist in our training images. Our expression control is also more expressive, supporting subtle expressions like eye blinks (Figure.~\ref{fig:teaser}~\ref{fig:ablation}).

\subsection{Quantitative Comparisons}
\paragraph{Image Quality}
We measure the image quality with Frechet Inception Distance (FID)~\cite{heusel2017gans} and Kernel Inception Distance~\cite{bi2019deep} between 50K  randomly synthesized images and 50K randomly sampled real images at the resolution of $512\times 512$. Since DiscoFaceGAN~\cite{deng2020disentangled} and AniFaceGAN~\cite{wu2022anifacegan} only synthesize images at $256\times256$ resolution, we utilize a state-of-the-art super-resolution model, SwinIR~\cite{liang2021swinir} to upsample into $512\times512$ for a fair comparison. As shown in Table.~\ref{tab:baseline_comp}, our method is superior in both FID and KID than all prior work, demonstrating the most compelling image quality. We note that the original EG3D has a slightly lower FID at $4.8$ which is expected since we introduce controllability with additional loss regularization. 
\vspace{-0.25in}
\paragraph{Disentanglement}
To evaluate the disentangled controllability of our model  over shape, expression and camera pose, we measure the disentanglement score~\cite{deng2020disentangled} of synthesized images as DS$_{\bb{\alpha}}$, DS$_{\bb{\beta},\bb{\theta}}$ and DS$_{\bb{c}}$ respectively. DS measures the stability of other factors when a single attribute is modified in image synthesis. We employ DECA~\cite{DECA:Siggraph2021} for estimation of FLAME parameters from generated images  and calculate the variance of the estimated parameters $(\bb{\alpha}, \{\bb{\beta},\bb{\theta}\}, \bb{c}).$ Specifically the DS$_i$ is calculated as
\begin{equation}
    DS_i = \prod_{j\neq i} \frac{var(i)}{var(j)}, \,\, i, j \in \{\bb{\alpha}, \{\bb{\beta},\bb{\theta}\}, \bb{c}\}.
\end{equation}
Higher value of DS indicates better disentanglement. Additionally, we evaluate the identity similarity between pairs of synthesized images with random camera poses and expressions by calculating the cosine similarity of the face embeddings with a pre-trained face recognition module~\cite{kim2022adaface}. Our approach demonstrates the best disentanglement numerically over all prior work as indicated in Table.~\ref{tab:baseline_comp}. 


% \begin{table*}[t]
% 	\vspace{0.06in}
% % 	\normalsize
% 	\renewcommand{\arraystretch}{1.0}
% 	\centering
% %		\captionsetup{font=sf}
% 	\caption{Quantitative comparisons}
% 	\vspace{-0.02in}
% 	\begin{tabular}{l|cc|cccc}
% 	\hline
% 	Model & FID \downarrow & KID  \downarrow & DS_{\bb{\alpha}}\uparrow & DS_{\bb{\beta},\bb{\theta}}\uparrow & DS_{\bb{c}}\uparrow & IS \uparrow \\
% 	\hline
% 	DiscoFaceGAN-SR~\cite{deng2020disentangled} & 42.6 & 0.04 &  - & 27.2 & 0.11 & 0.59\\
% 	GAN-Control~\cite{shoshan2021gan} & 9.8 & 0.005 &  - & 43.7 & 0.28& 0.62\\
% 	\hline
% 	HeadNeRF~\cite{hong2022headnerf}  & 163 & 0.168 &  1.33 & 8.9 & 0.25 & 0.678\\
% 	AniFaceGAN-SR~\cite{wu2022anifacegan} & 30.5 & 0.024 &  2.0 & 11.1 & 0.04 & 0.73\\
% 	3DFaceShop~\cite{tang2022explicitly} & 24.8 & 0.018 &  0.45 & 64.2 & 0.029 & 0.594 \\
% 	\hline
% 	Ours  & \bb{5.7} & \bb{0.0016} & \bb{3.1} & \bb{71.9} & \bb{0.35} & \bb{0.80} \\
% 	\hline
% 	\end{tabular}
% 	\vspace{-0.06in}
% 	\label{tab:baseline_comp}
% \end{table*}


% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.95\textwidth]{figures/view_var.png}
%     \caption{Qualitative evaluation.}
%     \label{fig:view_var}
% \end{figure*}
% \begin{table*}[t]
% 	\vspace{0.06in}
% 	\normalsize
% 	\renewcommand{\arraystretch}{1.3}
% 	\centering
% %		\captionsetup{font=sf}
% 	\caption{Quantitative comparisons}
% 	\vspace{-0.02in}
% 	\begin{tabular}{l|cc|ccc|cccc}
% 	\hline
% 	Model & FID \downarrow & KID \downarrow & ASD & AED & AEP & DS_{\bb{\alpha}}\uparrow & DS_{\bb{\beta},\bb{\theta}}\uparrow & DS_{\bb{c}}\uparrow & IS \uparrow \\
% 	\hline
% 	DiscoFaceGAN-SR~\cite{deng2020disentangled} & 42.6 & 0.04 & x & x & x & - & 27.2 & 0.11 & 0.59\\
% 	GAN-Control~\cite{shoshan2021gan} & 9.8 & 0.005 & x & x & x & - & 43.7 & 0.28& 0.62\\
% 	\hline
% 	HeadNeRF~\cite{hong2022headnerf}  & 163 & 0.168 & x & x & x & 1.33 & 8.9 & 0.25 & 0.678\\
% 	AniFaceGAN-SR~\cite{wu2022anifacegan} & 30.5 & 0.024 & x & x & x & 2.0 & 11.1 & 0.04 & 0.73\\
% 	3DFaceShop~\cite{tang2022explicitly} & 24.8 & 0.018 & x & x & x & 0.45 & 64.2 & 0.029 & 0.594 \\
% 	\hline
% 	Ours wo $L_{enc}$  & x & x & x & x & x & x & x & x& x \\
% 	Ours wo $L_{prior}$ & x & x & x & x & x & x & x & x& x \\
% 	Ours wo pose cond & x & x & x & x & x & x & x & x& x \\
% 	\hline
% 	Ours  & \bb{5.7} & \bb{0.0016} & x & x & x & \bb{3.1} & \bb{71.9} & \bb{0.35} & \bb{0.80}
% 	\end{tabular}
% 	\vspace{-0.06in}
% 	\label{tab:quan_all}
% \end{table*}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.47\textwidth]{figures/extrapolation.png}
%     \caption{\textbf{Extrapolation.} Realistic head image synthesis with unseen out-of-domain neck and jaw movements.}
%     \label{fig:extrap}
%     \vspace{-0.15in}
% \end{figure}
\subsection{Ablation Studies}
We ablate the efficacy of the individual component by removing it from our full pipeline. As shown in Figure.~\ref{fig:ablation}, we show the loss of control accuracy over shape and expression respectively when removing  the geometric prior loss $L_{prior}$ and control loss $L_{enc}.$ Conditioning the neural radiance field on expression is also critical to the modeling of dynamic details. Numerically we validate the efficacy of $L_{prior}$ and $L_{enc}$ with Average Shape Distance (ASD) and Average Expression Distance (AED). From the prior distribution, we randomly sample $500$ shapes and expressions, with which we synthesize images with $10$ different identities. We then reconstruct FLAME parameters from the synthesized images and compare against the input control. We compute 3D per-vertex $L_1$ distance of FLAME meshes for ASD while we use 3D landmarks $L_1$ distance for AED calculation. Additionally we compute the variance of estimated shapes and expressions within each control group, with lower value indicating more precise control. The efficacy of $L_{prior}$ and $L_{enc}$ is well evidenced in Table.~\ref{tab:ablation}, with even slight image quality improvements. 
% \begin{table}[h]
% 	\vspace{0.06in}
% % 	\normalsize
% 	\renewcommand{\arraystretch}{1.0}
% 	\centering
% %		\captionsetup{font=sf}
% 	\caption{Quantitative comparisons}
% 	\vspace{-0.02in}
% 	\begin{tabular}{l|cc|c}
% 	\hline
% 	Model & Ours wo $L_{enc}$  & Ours wo $L_{prior}$ & Ours \\
% 	\hline
% 	FID $\downarrow$  & 6.0& 6.3& \bb{5.7}\\
% 	KID  $\downarrow$ & 0.00183 &0.00192 & \bb{0.0016}\\
% 	\hline
% 	ASD (cm)  $\downarrow$ & \bb{0.124}& 0.135 & \bb{0.124}\\
% 	$var({\bb{\alpha}})\downarrow$  & 0.000525 & 0.000577 & \bb{0.000515}\\
% 	AED (cm)  $\downarrow$  &0.1233 & 0.0813 & \bb{0.0797} \\
% 	$var({\bb{\beta}, \bb{\theta}})\downarrow$  & 0.004707& 0.00406 & \bb{0.00404} \\
% 	\hline
% 	\end{tabular}
% 	\vspace{-0.06in}
% 	\label{tab:ablation}
% \end{table}



% wo L_prior, L_enc, pose cond
% \begin{table}[h]
% 	\vspace{0.06in}
% % 	\normalsize
% 	\renewcommand{\arraystretch}{1.0}
% 	\centering
% %		\captionsetup{font=sf}
% 	\caption{Quantitative comparisons}
% 	\vspace{-0.02in}
% 	\begin{tabular}{l|cc|cc}
% 	\hline
% 	Model & FID \downarrow & KID  \downarrow & ASD\downarrow & AED\downarrow  \\
% 	\hline
% 	Ours wo $L_{enc}$ & 6.0 & - &  - & -  \\
% 	Ours wo $L_{prior}$ & 6.3 &- &  - & - \\
% 	\hline
% 	Ours  & \bb{5.7} & \bb{0.0016} & - & -   \\
% 	\hline
% 	\end{tabular}
% 	\vspace{-0.06in}
% 	\label{tab:baseline_comp}
% \end{table}

\subsection{Applications}
\paragraph{Talking Head Video Generation.} 
In Figure.~\ref{fig:teaser}, we showcase talking head video generation in controllable views driven with animation sequences of FLAME. Thanks to the high control accuracy, our method is able to synthesize various talking head videos with the same head movements and expressions performed by different identities. 
Our method is expressive in depicting both large articulated neck and jaw movements and subtle facial expressions like eye blinks, with rich dynamic details. Shape manipulation is easily achievable as well by modifying the shape parameters. Please refer to our supplementary materials for more results in high resolution. 
\vspace{-0.1in}
\paragraph{Portrait Image Manipulation and Animation.}
As illustrated in Figure.~\ref{fig:teaser}, our model also supports 3D-aware face reenactment of a single-view portrait to a video sequence. To achieve that, we perform an optimization in the latent Z+ space~\cite{karras2019style} to find the corresponding latent embedding, with FLAME parameter and camera pose estimated from the input portrait. With a frozen generator, the optimization is performed  by measuring the similarity between generated image and real image using the $L_2$ loss and LPIPS loss~\cite{zhang2018unreasonable}. For better reconstruction quality, we alter the parameters of the tri-plane synthesis module with a fixed optimized latent code~\cite{roich2022pivotal}. After that one can explicitly manipulate the portrait with a preserved identity and in a different camera pose and expression. With the expression codes $(\bb{\beta},\bb{\theta})$ reconstructed from a video sequence, we are also able to reenact the portrait to the video motion. 
\vspace{-0.1in}
\paragraph{Societal Impact.} Our work focuses on improving the controllability of 3D-aware GANs in technical aspects and is not specifically designed for any malicious uses. This being said, we do see that the method could be potentially extended into controversial applications such as generating fake videos. Therefore, we believe that the synthesized images and videos should present themselves as synthetic.

