\section{Method}


\label{sec:method}
%Given an overview about the system input/output and pipeline
Our goal is to build a geometry-guided 3D head synthesis model with full control of camera poses, face shapes and expressions, trained from in-the-wild unstructured image collections.

\subsection{Overview}
\paragraph{Problem.}
To achieve our goal, we leverage a 3D-aware generator (EG3D~\cite{chan2021efficient}) for photo-realistic, multiview consistent image synthesis, while disentangle control of head geometric attributes from image generation with a 3D statistical head model (FLAME~\cite{FLAME:SiggraphAsia2017}). Specifically, given a random Gaussian-sampled latent code $\bb{z}$, a camera pose $\bb{c}$ and a FLAME parameter $\bb{p}=(\bb{\alpha}, \bb{\beta},\bb{\theta})$ consisting of shape $\bb{\alpha},$ expression $\bb{\beta},$ jaw and neck pose $\bb{\theta}$, the generator $G$ synthesizes a photo-realistic human head image $I_{RGB}(\bb{z} | \bb{c}, \bb{p})$ with corresponding attributes as defined in $\bb{p}.$    
\vspace{-0.05in}
\paragraph{Framework.}
As illustrated in our pipeline~\ref{fig:overview}, 
our controllable 3D-aware GAN is trained in two stages. From a large collection of 3D deformed FLAME meshes, we first pre-train a deformable semantic SDF around the FLAME geometry that builds a differential volumetric correspondence map from the observation to a predefined canonical space (Section~\ref{sec:imflame}). In the second stage, guided by the pre-trained volumetric mapping, we then deform the detailed 3D full heads synthesized in the disentangled canonical space to the desired shapes and expressions (Section~\ref{sec:canonical}). Fine-grained expression control is achieved by supervising image synthesis such that expressions estimated from the generated images is consistent with the input control(Section~\ref{sec:supervision}). Our approach further enhances temporal realism with dynamic details, such as dimples and wrinkles, synthesizing realistic shading and geometric variations as expression changes (Section~\ref{sec:dynamic}).  

% our 3D generator $G$ takes a randomly sampled latent code $\bb{z},$ and produces a 3D neural feature field for volume rendering and image super-resolution. For disentangled control over the image synthesis process, our approach generates a 3D neural head scene in a predefined canonical space (Section~\ref{sec:canonical}), while we control the final image synthesis to the desired semantics by deforming the volumetric rendering rays accordingly based on the FLAME control parameters $\bb{p}$ . This observation-to-canonical deformation is achieved by learning the variation of signed distance values when modifying the controls. Additionally we co-learn an implicit  representation of the parametric FLAME model, as signed distance function, which further guides the generation of neural density field as geometric prior (Section~\ref{sec:canonical}).
% In contrast to directly conditioning neural field generation with FLAME control parameters, our design enables explicit disentanglement of control, and extrapolation of image synthesis beyond the control distribution of the training set.  
% We fully exploit the geometric knowledge in 3D statistical head model by learning our semantic volume deformation field from 
 
\vspace{-0.1in}
\paragraph{3D-Aware GAN Background.}
% \subsection {Compositional 3D-Aware Head GAN}
% \label{sec:eg3d}
%Brief intro to EG3D pipeline + foreground decomposition
To ensure appearance consistency from different views, we choose EG3D~\cite{chan2021efficient} as our backbone for 3D-aware image synthesis. The generator $G$ takes a random latent code $\bb{z}$ and conditioning camera label $\hat{\bb{c}}$, and maps to a manifold of triplane features $\cal{M}(\bb{z}, \hat{\bb{c}})$. For presentation clarity, we absorb $\hat{\bb{c}}$ to $\bb{z}$ and simply denote triplane as $\cal{M}(\bb{z}).$
A low-resolution feature map $I^{*}(\bb{z} | \bb{c})$ is then rendered from a desired camera pose $\bb{c}$ by sampling the triplane features and integrating MLP-decoded neural radiance $(\sigma, \bb{f})$ along camera rays. A super-resolution module is followed to modulate the feature map and synthesize the final RGB images at high resolution. We train $G$ and a dual discriminator $D$ with adversarial training.

% Different from the original EG3D, our feature image $I^{*}(\bb{z})$ is composed with 3D-aware human foreground  integrated from the neural radiance field and a 2D background image $I^{+}(\bb{z})$ synthesized with 2D convolution kernels, as
% \begin{equation}
%     I^{*}(\bb{z}) = I^{*}(\bb{z}) + (1 - M^{*})\odot I^{+}(\bb{z}),
% \end{equation}
% where $M^{*}$ is the accumulated foreground density along pixel rays. Similar to~\cite{shi2021SemanticStyleGAN}, we associate each training image with a 2D foreground mask, and augment our discriminator $D$ with a dual convolution branch for pairs of $(I, M)$ where $M$ is up-sampled from $M^{*}.$ To this end, our generator $G$ is able to synthesize  view-consistent images while keeping the background static.  

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/ablation_v3.png}
    \caption{We synthesize different identities with the same shape and expression code respectively in (a) and (b). We observe less shape variation with our geometric prior loss (a) and more consistent expression with our control loss (b). In (c), we show our expression-conditioned NeRF modeling builds rich dynamic details as the subject varies expressions.}
    \label{fig:ablation}
    \vspace{-0.18in}
\end{figure}

\subsection {Controllable 3D-Aware Image Synthesis}
%talk about disentanglement.
To synthesize an image $\bb{I}(\bb{z} | \bb{c}, \bb{p})$ with desired FLAME control $\bb{p}$, we leverage the EG3D framework to generate a triplane-based 3D volumetric feature space $\cal{M}(\bb{z})$ of a synthesized identity with canonical shape and expression. Guided by our pretrained volumetric correspondence map,
we deform the synthesized feature volume into our target observation space, which is further decoded and volume rendered into high-fidelity head appearance and geometry with the target shape and expression. Our design explicitly disentangles the underlying geometric variations of changing shape and expression from canonical geometry and appearance synthesis. Following EG3D~\cite{chan2021efficient}, we associate each training image with a set of camera parameters $\bb{c}$ and control parameters $\bb{p}$, which are obtained from a nonlinear 2D landmarks-based optimization. 

% a learned warping field guides the observation-space spatial points back to the canonical space $\cal{C(\bar{\bb{p}})}$, from which we integrated the queried triplane features into $I^{*}$ for super-resolution synthesis.

% high-fidelity appearance and geometry ( including hair and apparels) in a pre-defined canonical space, characterized with triplanes   

% In order to synthesize images , we disentangle our generative 3d head as a layered combination of two components. The inner layer is an 
% animatable 3D head that models the deformed head geometry $\bb{S}(\bb{p})$, parameterized by the semantic shape and expression code $\bb{p}$. The external one, characterized with triplanes $\cal{M}(\bb{z})$, operates in a canonical space and generates high-fidelity appearance and geometry of a synthesized identity, including hair and apparels, fully independent from all controls $\bb{p}$. Our design explicitly disentangles the underlying geometric variations of changing shape and expression from appearance generation, where later the inner layer semantically maps the triplane-based neural radiance fields to the target control. Therefore to generate an image $\bb{I}(\bb{z} | \bb{c}, \bb{p}),$ the generator first synthesizes triplane features $\cal{M}(\bb{z})$ of an identity with canonical shape and expression ${\bb{\bar{p}}}$, whereas a learned warping field guides the observation-space spatial points back to the canonical space $\cal{C(\bar{\bb{p}})}$, from which we integrated the queried triplane features into $I^{*}$ for super-resolution synthesis. 
%an off-the-shelf face reconstruction model~\cite{DECA:Siggraph2021}.


\subsubsection{Semantic Signed Distance Function}
\label{sec:imflame}
For disentangled geometric modeling, we formulate an implicit semantic SDF representation $W(\bb{x} | \bb{p} = (\bb{\alpha}, \bb{\beta},\bb{\theta}) ) = (s, \bar{\bb{x}})$, where  $\bb{\alpha}, \bb{\beta}$ are the linear shape and expression blendshape coefficients, and $\bb{\theta}$ controls the rotation of a 3-DoF jaw and neck joint. Specifically, given a spatial point $\bb{x}$ in observation space $\cal{O(\bb{p})},$ $W$ returns its 3D correspondence point $\bar{\bb{x}}$ (i.e., semantics) in canonical space $\cal{C(\bb{\bar{p}})}$, with which we project and query the triplane features $\cal{M}(\bb{z}).$ Additionally it also computes the closest signed distance  $s(\bb{x} | \bb{p})$ to the  FLAME mesh surface $\bb{S}(\bb{p}).$ We illustrate the function in Figure~\ref{fig:illustration}. We co-learn the highly-correlated volumetric correspondence and SDF with the property that the signed distance is preserved between canonical and observation correspondence points as $s(\bar{\bb{x}} | \bar{\bb{p}}) = s(\bb{x} | \bb{p})$.  



% For our inner layer, we use FLAME model~\cite{FLAME:SiggraphAsia2017} for 
% control of shape, expression, jaw and neck poses as $\bb{S}(\bb{p} =\{\bb{\alpha}, \bb{\beta}, \bb{\theta}\})),$ where  Instead of mesh representation, we opt for representing the FLAME geometry as implicit signed distance function (SDF) for compatibility with implicit NeRF representation in the external layer. Specifically, given a spatial point $\bb{x}$ in observation space $\cal{O(\bb{p})},$ we compute the signed distance $s(\bb{x} | \bb{p})$ to the closest surface point of $\bb{S}(\bb{p}).$ Additionally, we associate the point its canonical 3D correspondence $\bar{\bb{x}}$, with which we project and query the features from triplanes $\cal{M}(\bb{z}).$ 
% % One could also consider $\bar{\bb{x}}$ as a 3D volumetric UV coordinate of $\bb{x}.$ 
% As such we define our volumetric signed distance correspondence field as $W(\bb{x} | \bb{p}) = (s, \bar{\bb{x}}),$ where the signed distance is preserved across pairs of correspondence points in canonical and observation space as $s(\bar{\bb{x}} | \bar{\bb{p}}) = s(\bb{x} | \bb{p}).$ 

% To fully exploit the geometric knowledge in the 3D head model, 
We learn $W(\bb{x} | \bb{p})$ with a large corpus of 3D FLAME meshes $\bb{S}(\bb{p})$ sampled from its parametric control space. Similar to IGR~\cite{icml2020_2086} and imGHUM~\cite{alldieck2021imghum}, we model our implicit field as an MLP and optimize $W(\bb{x} | \bb{p})$ with losses, 
\begin{gather}
L_{iso} = \frac{1}{|N|}\sum_{\bb{x} \in N}(|s(\bb{x} | \bb{p})| + |\nabla s_{\bb{x}}(\bb{x} | \bb{p}) - \bb{n}(\bb{x} | \bb{p})|), \\
L_{eik} = \frac{1}{|F|}\sum_{\bb{x}\in F}\|\nabla s_{\bb{x}}(\bb{x} | \bb{p}) - 1 \|_2, \\
L_{sem} = \frac{1}{|N|}\sum_{\bb{x} \in N}(|\bar{\bb{x}}(\bb{x} | \bb{p}) - \bar{\bb{x}}^*(\bb{x} | \bar{\bb{p}})| )
\end{gather}
where $N, F$ are a batch of on and off surface samples. For the surface samples, the $L_{iso}$ encourages the signed distance values to be on the zero-level-set and the SDF gradient to be equal to the given surface normals $\bb{n}$. The Eikonal loss $L_{eik}$  is derived from~\cite{icml2020_2086} where the SDF is differentiable everywhere with gradient norm 1. The semantic loss $L_{sem}$ supervises the mapping of surface samples $\bb{x} \in F$ to the ground-truth correspondence points $\bar{\bb{x}}^*$ on the canonical
FLAME surface, where $\bar{\bb{x}}^*$ and $\bb{x}$ share the same barycentric coordinates. 
The $L_{eik}$ provides a geometric regularization over the volumetric SDF, whereas the $L_{iso}$ and $L_{sem}$ act as the boundary condition to the SDF and volumetric correspondence field respectively. 
% \begin{wrapfigure}{r}{0.4\textwidth}
%   \begin{center}
%     \includegraphics[width=0.38\textwidth]{figures/illustration.png}
%   \end{center}
%   \caption{Birds}
% \end{wrapfigure}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/illustration.png}
    \caption{Illustration to semantic SDF learning.}
    \label{fig:illustration}
    \vspace{-0.18in}
\end{figure}

To guide the canonical correspondence learning for off-the-surface points, one could try to minimize the signed distance difference between $s(\bar{\bb{x}} | \bar{\bb{p}})$ and $s(\bb{x} | \bb{p}).$ However, we note that the volume correspondence between the observation and canonical space is still ill-regularized, considering infinite number of points exist with the same signed distance value. We therefore reformulate volumetric correspondence field as $W(\bb{x} | \bb{p}) = (\bar{s}(\bar{\bb{x}}), \bar{\bb{x}}),$ where 
the signed distance of an observation-space point is obtained by mapping it into its canonical correspondence $\bar{\bb{x}}$ and querying the pre-computed canonical SDF $\bar{s}$. Thus we only learn a correspondence field with which we can deform the canonical SDF to different FLAME configurations, and then supervise the deformed SDFs with the FLAME surface boundary, normals ($L_{iso}$) and Eikonal regularization ($L_{eik}$).
As such, even for off-the-surface samples, their canonical correspondences are well regularized in space with the geometric properties of signed distance functions via $L_{iso}$ and $L_{eik}$. In contrast to explicit mesh deformation~\cite{bergman2022generative}, our implicit volumetric correspondence is more accurate, differentiable, smooth everywhere and semantically consistent with the properties of SDF.
 
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/shape_exp_var_v3.png}
    \caption{\textbf{Qualitative comparisons in expression control}. Under a similar expression control, our method achieves the best perceptual quality with better identity-preservation (compared to GAN-Control~\cite{shoshan2021gan} and 3DFaceShop~\cite{tang2022explicitly}), more realistic dynamic details (compared to DiscoFaceGAN~\cite{deng2020disentangled}) and higher expression control accuracy (compared to AniFaceGAN~\cite{wu2022anifacegan}, 3DFaceShop~\cite{tang2022explicitly}, HeadNeRF~\cite{hong2022headnerf}).}
    \label{fig:shape_exp_var}
\end{figure*}
%imflame
\vspace{-0.05in}
\subsubsection{Canonical Generation with Geometric Prior}
\label{sec:canonical}
%triplane generation.
With our pretrained semantic SDF $W(\bb{x} | \bb{p})$ modeling the shape and expression variation, we leverage the triplane for 3d-aware generation of human heads with canonical shape and expression. In particular, to generate a neural radiance feature $(\sigma, \bb{f})$ for a point $\bb{x}$ in observation space $\cal{O}(\bb{p})$, we use our correspondence function $W$ to back warp $\bb{x}$ into $\bar{\bb{x}}$,  with which we project and sample the canonical triplane features followed with a tiny MLP decoding. 

In spite of the control disentanglement, there is no explicit loss that constrains the triplane-generated neural radiance density to conform to the shape and expression as defined in $\bb{p}.$ Therefore, we guide the generation of neural radiance density field by minimizing its difference to the underlying FLAME head geometry represented with SDF, as 
\begin{gather}
    L_{prior} = \frac{1}{|R|} \sum_{\bb{x}\in R} e^{-\gamma |s(\bb{x}| p)|}| \sigma(\bb{x} | \bb{z}, \bb{p}) - \sigma^{*}(\bb{x} | \bb{p}) |, \\
    \sigma^{*}(\bb{x} | \bb{p}) = \frac{1}{\kappa}\cdot \textrm{Sigmoid}(\frac{-s(\bb{x} | \bb{p})}{\kappa})
\end{gather}
where $R$ is the stratified ray samples for volume rendering and $\kappa$ is a learnable scalar controlling the density tightness around the SDF boundary. Following~\cite{yariv2021volume, or2021stylesdf}, we convert SDF value $s(\bb{x} | \bb{p})$ to proxy 3D density $\sigma^{*}(\bb{x} | \bb{p})$ assuming non-hollow surfaces. We decay the weights for our geometric prior loss $L_{prior}$ as the point moving away from the SDF boundary, allowing higher degrees of freedom in generation of residual geometries, such as hair and glasses. The geometric prior loss effectively guides the 3D head geometry learning but should not be overpowered which otherwise might lead to loss of geometric details. 
%We note that in the head region lacking visual supervisions, such as the back head, our SDF prior also serves as a reasonable geometric proxy that completes the shape of the whole head.  


\subsubsection{Fine-Grained Expression Control}
\label{sec:supervision}
Our geometric prior loss $L_{prior}$ provides local 3D point-wise guidance, and is able to well regularize the shape generation and achieve coarse-level expression control. However, for delicate expressions, such as eye blinks, $L_{prior}$ provides little supervision as the geometric variation is subtle. Moreover, for regions with complex correspondences, such as around the lips, it is challenging to guide the formation of correct expressions globally, just with point-wise geometric losses. To improve the control granularity, we propose an image-level supervision loss that requires a synthesized image $I_{RGB}(\bb{z} | \bb{c}, \bb{p})$ matching the target expression as defined in the input $\bb{p}$. Using our training images with estimated control labels $\bb{p},$ we first pretrain an image encoder $E(I_{RGB})= (\tilde{\bb{\beta}}, \tilde{\bb{\theta}})$ that regresses the expression coefficients $\tilde{\bb{\beta}}$ and joint poses $\tilde{\bb{\theta}}.$ During our 3D GAN training, we then apply our image-level control supervision as 
\begin{gather}
    L_{enc} = |\tilde{\bb{\beta}} - \bb{\beta}| + |\tilde{\bb{\theta}} - \bb{\theta}| + |\bb{S}(\bb{\alpha}, \tilde{\bb{\beta}}, \tilde{\bb{\theta}}) - \bb{S}(\bb{\alpha}, \bb{\beta}, \bb{\theta})| + \notag\\
    + |\bb{J}\bb{S}(\bb{\alpha}, \tilde{\bb{\beta}}, \tilde{\bb{\theta}}) - \bb{J}\bb{S}(\bb{\alpha}, \bb{\beta}, \bb{\theta})|,
\end{gather}
where $\bb{S}$, $\bb{J}$ are the FLAME mesh vertices and 3D landmarks regressor respectively. While being straightforward for the first 2 terms, the last two terms in $L_{enc}$ penalize deviation of 3D vertex coordinates and surface landmarks after mesh decoding. We note that we do not supervise shape $\bb{\alpha}$ in $L_{enc}$ since our geometric prior loss $L_{prior}$ suffices in shape control already and also due to the ambiguity of shape scaling estimated from monocular images. 

\subsubsection{Dynamic Details Modeling}
\label{sec:dynamic}
To this end, we have achieved controllable static image synthesis. However, one should observe the variation of shading and geometric details in a dynamic head motion, such as the appearance of dimples and eye wrinkles in smiling. We consider the appearance of dynamic details is highly correlated with the driving expression $(\bb{\beta}, \bb{\theta})$. To model such temporal effects, one could try to condition the triplane generation on expression as $\cal{M}(\bb{z}, \bb{\beta}, \bb{\theta}).$ However, such design results in entanglement of expression control with the generative latent code $\bb{z},$ inducing identity or appearance changes when varying expressions. It is also hard to synthesize images out of training distribution of expressions $(\bb{\beta}, \bb{\theta})$. We therefore leave our triplane generation disentangled but when decoding neural feature field $(\sigma, \bb{f})$ from sampled triplane features, we additionally condition the MLP-decoder on $\bb{\beta}$ and $\bb{\theta}.$ Specifically, we have 
\begin{gather}
    (\sigma(\bb{x}), \bb{f}(\bb{x}) | \bb{z}, \bb{p}) = \Phi(\cal{M}(\bar{\bb{x}}(\bb{x} | \bb{p}) | \bb{z}), \phi(\bb{\beta}, \bb{\theta})),
\end{gather}
where both $\Phi$ and $\phi$ are tiny MLPs and $\phi$ regresses an expression-dependent feature vector from  $(\bb{\beta}, \bb{\theta})$ after positional encoding. 
For better extrapolation to novel expressions and jaw poses, we add Gaussian noise to the conditioning parameters to prevent MLP overfitting.






