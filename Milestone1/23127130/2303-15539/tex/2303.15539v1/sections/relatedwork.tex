\section{Related Work}
\label{sec:relatedwork}

\paragraph{3D-Aware Generative Image Synthesis.}
Generative adversarial networks~\cite{goodfellow2014generative,karras2019style,Karras2020stylegan2} gained popularity over the last decade due to their remarkable ability in photo-realistic image synthesis. Building on the success of 2D image-based GANs, recent works have extended the capabilities to view-consistent image synthesis with unsupervised learning from 2D single-view images. The key idea is to combine differential rendering with 3D scene representations, such as meshes~\cite{Szabo:2019,Liao2020CVPR}, point clouds~\cite{achlioptas2018learning,li2019pu}, voxels~\cite{wu2016learning,hologan,nguyen2020blockgan}, and recently implicit neural representation~\cite{schwarz2020graf,chan2021pi,niemeyer2021giraffe,chan2021efficient,or2021stylesdf,gu2021stylenerf,deng2021gram,epigraf,zhou2021CIPS3D}. We build our work on recent 3D GAN model by Chan et al~\cite{chan2021efficient} that uses an efficient triplane-based NeRF generation, combined with 2D CNN-based super-resolution. Even though 3D-aware GANs are able to control camera viewpoints, they lack precise 3D control over the other attributes such as shapes and expressions. 
 In this work, we empower 3D-aware GANs with disentangled precise control over shapes and expressions. 
\vspace{-0.12in} 
\paragraph{Controllable Face Image Synthesis.}
Considerable work~\cite{deng2020disentangled,tewari2020stylerig,chen2022sofgan,piao2021inverting,tewari2020pie,kowalski2020config,ghosh2020gif} has been devoted to incorporate 3D priors of statistical face models, such as 3D Morphable Models (3DMMs)~\cite{blanz1999morphable,paysan20093d}, in controllable face synthesis and animation. Among them, DiscoFaceGAN~\cite{deng2020disentangled} proposed imitative-contrastive learning to mimic the 3DMM rendering process by the generative model. A similar strategy has also been adopted with concurrent and follow-up works~\cite{tewari2020stylerig,kowalski2020config,ghosh2020gif,piao2021inverting}. However all of these approaches suffer from 3D inconsistency due to the use of 2D CNNs as image renderer. 
HeadNeRF~\cite{hong2022headnerf} combines 3DMM with 3D NeRF representation, and is able to synthesize 3D heads conditioned on 3DMM attributes. However, the training relies on annotated multiview datasets whereas our approach learns the disentangled 3D head synthesis with only single-view images. There has been concurrent work~\cite{wu2022anifacegan,sun2022controllable,bergman2022generative,zhang2022avatargen,tang2022explicitly} to ours in 3D-aware controllable face or full-body GANs. Differently from these approaches, we use a full-head parametric model FLAME~\cite{FLAME:SiggraphAsia2017}, and fully exploit the spatial geometric prior knowledge beyond the surface deformation and skinning. We have also achieved fine-grained control with our novel losses and enhanced face animation with rich dynamic details. 
\vspace{-0.12in} 
\paragraph{Controllable Neural Implicit Field of Face.} 
Neural implicit functions~\cite{xie2022neural}, have emerged as a powerful continuous and differential representations of 3D scenes. Among them, Neural Radiance Field~\cite{mildenhall2020nerf,barron2021mip} has been widely adopted due to its superiority in modeling complex scene details and synthesizing multiview images with inherited 3D consistency. While initial proposals have focused on static scene modeling, recent work have successfully demonstrated application of NeRF in modeling dynamic scenes~\cite{pumarola2021d,park2021nerfies,xu2021h,liu2021neural,tretschk2021non}. In particular, dynamic neural radiance fields of human heads~\cite{park2021hypernerf,park2021nerfies,gafni2021dynamic,guo2021ad,wang2021learning,zheng2022avatar,kania2022conerf,zhuang2021mofanerf} have enabled photo-realistic head animation, often by conditioning with pose parameters or deforming the radiance field with 3D morphable models. However, they do not leverage a generative training paradigm and focus on scene-specific learning from video sequences or multiview images. In contrast, our model learns generative and controllable neural radiance fields from widely accessible single-view images. 
