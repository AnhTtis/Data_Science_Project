% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{multirow}
\usepackage[accsupp]{axessibility}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{8250} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\def\papername{OmniAvatar}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis}



\author{Hongyi Xu$^1$ \quad Guoxian Song$^1$ \quad Zihang Jiang$^{1,2}$ \quad Jianfeng Zhang$^{1,2}$ \quad Yichun Shi$^1$ \\  Jing Liu$^1$ \quad Wanchun Ma$^1$ \quad Jiashi Feng$^1$ \quad Linjie Luo$^1$\\
$^1$ByteDance Inc \quad\quad $^2$National University of Singapore\\
% Institution1 address\\
\tt\small \{{hongyixu, guoxian.song, zihang.jiang, jianfeng.zhang, yichun.shi,} \\
\tt\small{jing.liu, wanchun.ma, jshfeng, linjie.luo\}@bytedance.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}
% \maketitle


\newcommand{\bb}[1]{\boldsymbol{\mathbf{#1}}}
%%%%%%%%% ABSTRACT

\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
    \captionsetup{type=figure}
    \includegraphics[width=1.0\textwidth]{figures/teaser_v5.png}
    \captionof{figure}{Our model can synthesize diverse identity-preserved 3D heads with compelling dynamic details under full disentangled control over camera poses, facial expressions, head shapes, articulated neck and jaw poses (left). Our model can also reconstruct 3D heads from a single photo reference and enable multi-view-consistent head reenactment (right).}
    \label{fig:teaser}
\end{center}
}]


% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=1.0\textwidth]{figures/teaser_v3.png}
%     \caption{} 
%     %Teaser}
%     \label{fig:teaser}
%     \vspace{-0.1in}
% \end{figure*}

\begin{abstract}
We present \papername, a novel geometry-guided 3D head synthesis model trained from in-the-wild unstructured images that is capable of synthesizing diverse identity-preserved 3D heads with compelling dynamic details under full disentangled control over camera poses, facial expressions, head shapes, articulated neck and jaw poses. To achieve such high level of disentangled control, we first explicitly define a novel semantic signed distance function (SDF) around a head geometry (FLAME) conditioned on the control parameters. This semantic SDF allows us to build a differentiable volumetric correspondence map from the observation space to a disentangled canonical space from all the control parameters. We then leverage the 3D-aware GAN framework (EG3D) to synthesize detailed shape and appearance of 3D full heads in the canonical space, followed by a volume rendering step guided by the volumetric correspondence map to output into the observation space. To ensure the control accuracy on the synthesized head shapes and expressions, we introduce a geometry prior loss to conform to head SDF and a control loss to conform to the expression code. Further, we enhance the temporal realism with dynamic details conditioned upon varying expressions and joint poses. Our model can synthesize more preferable identity-preserved 3D heads with compelling dynamic details compared to the state-of-the-art methods both qualitatively and quantitatively. We also provide an ablation study to justify many of our system design choices.
% Capitalizing on the recent advance in image generation model, contemporary generative approaches are able to synthesize photo-realistic portrait images with some levels of attribute controllability such as camera pose and expression. However, to synthesize high-fidelity talking head animation, existing approaches still suffer from undesirable artifacts such as 3D inconsistency or lack of precise semantic controls. In this work, we propose OmniAvatar, a novel geometry-aware  3D face synthesis framework with fine-grained, semantic and disentangled face animation controllability. At its core is a layered decomposition of 3D head representation that synthesizes a neural radiance field (NeRF) with canonical shape and expression, while explicitly being driven to different expressions and shapes with an implicit warping field underneath conditioned on semantic control parameters of a statistical head model FLAME.
% We characterize our implicit control field with a  novel volumetric correspondence function that preserves signed distance values between paired points in observation and canonical space. By learning the correspondence and signed distance from large corpus of 3D FLAME instances, we guide the generation of neural density field with rich 3D geometric prior knowledge. To achieve accurate control over fine-grained facial expressions, we further introduce a cycle reconstruction loss that self-supervise the expression of the synthesized image to commit to the input control.  
%Additionally we enhance the temporal realism of synthesized head animation with expression-dependent dynamic details, such as dimples and wrinkles, directly learnt from monocular images. Our method outperforms prior approaches in extensive experiments and applications, enabling high-fidelity view and temporal-coherent synthesis of talking head animation with disentangled control over camera pose, shape and expression. 
\end{abstract}

%%%%%%%%% BODY TEXT

\input{sections/introduction}

\input{sections/relatedwork}

\input{sections/method}

\input{sections/results}

\input{sections/conclusion}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
