\section{Discussion and Future Work}
\label{section:discussions}

In this section, we discuss the coverage of the supported interactions and supported visualization and point out directions for future work.

\subsection{Coverage of Supported Interactions}



The manipulations supported for an existing visualization are contingent on the current set of visual objects within that visualization.
There exist two constraints: our methodology does not alter the quantity of control points, and it solely relies on information present in the current visualization without any additional data.
Of the unipolar interactions as categorized by Sedig and Parsons~\cite{sedig2013interaction}, operations such as drilling and blending, which necessitate supplementary data or a change in the control point count, are not compatible with our framework. Conversely, arrangements, assignments, cloning, comparisons, filtering, navigation, transformations, and translations can be partially or fully facilitated within the present framework, as these actions can be translated into alterations in the positions of control points.
In the realm of bipolar interactions, composing and decomposing provoke a change in the number of control points, making them non-applicable within our framework.
Interactions like gathering and discarding, inserting and removing, as well as storing and retrieving, can be executed by manipulating constraints and relocating visual objects between canvases.
In the future, the extensibility of the constraints model could involve permitting changes in control point quantities and associating visual objects with additional information.













\subsection{Coverage of Supported Visualizations}


In the previous sections, we demonstrated some common visualizations, including area charts and bubble charts.
Our approach can be applied to more visualizations.



\textbf{Limitations arising from implementation.}
Currently, our focus in terms of implementation is on non-nested visualizations within the two-dimensional Cartesian coordinate system.
The current implementation of this study effectively encompasses various prevalent visualizations, including scatter plots, line charts, and bar charts (both simple and with stacked or grouped variations).

\begin{itemize}
\item \textbf{2D Cartesian coordinates:}
The prototype system is primarily focused on visualizations within the two-dimensional Cartesian coordinate system.
These visualizations involve visual elements distributed either discretely along axes at fixed frequencies, as seen in heatmaps or Bertin Matrices~\cite{perin2014revisiting}, or more irregularly across continuous axes. We start by extracting both the horizontal and vertical axes. Then, we analyze the distribution patterns of visual elements along these axes.
By integrating these patterns with the axis structures, we are able to derive constraints for the visualizations.

\item \textbf{Exclusion of nested visualizations:}
The current model implementation does not encompass multi-layer nested structures. It does not yet accommodate visualizations that involve glyphs or exhibit nested properties, such as a bar within a grouped bar chart composed of multiple stacked bars. However, this model can accommodate such visualizations in future extensions.

\item \textbf{Deviation from conventional rules:}
Instances of parsing errors predominantly stem from heuristic solving algorithms. These errors might involve misinterpretation of numerical axes or visual arrangement patterns that deviate from common conventions.
\end{itemize}

Potential future extensions may encompass polar coordinate systems. In this context, sector charts could be interpreted as bar charts, and donut charts as stacked bar charts. Such extensions would require the augmentation of parsing algorithms for polar coordinates, as well as the development of coordinate system conversion algorithms.







\textbf{Coverage of visualization types.}
Ideally, at the conceptual level, common visualizations encode spatial channels with attributes that are compatible with our spatial constraints model.
For example, the interactions that change the stacked order/direction will be effective for the visualizations with stacked collisions.
For those visualizations that only have gravity constraints, including line charts and scatter plots, selecting and filtering can be supported by moving visual objects to a new canvas.
Currently, visualizations with a Cartesian coordinate system can be parsed by our system.
The axis type determines what axis operations are supported.
For example, the visualization in \autoref{fig:class_project} shows important moments in the lives of China's university presidents (e.g., being born or obtaining a Ph.D. degree).
We can sort the presidents according to their birthdays to better support comparison tasks.
The results are shown in \autoref{fig:class_project} (b).
We can extract a horizontal axis and a vertical axis (i.e., a visualization with a Cartesian coordinate system), and the continuous axis is taken as the linear scale. 
In the future, we could extend the model to visualizations with parallel axes or polar coordinate systems. 
For parallel axes, spatial constraints can be constructed for every axis.
If we want to support manipulations that change the organization of the axes (e.g., change the axis order of parallel coordinates), additional rules for axis positioning are needed.
For polar coordinate systems, we can conceptually regard the coordinate systems as Cartesian coordinate systems in which the pie chart is stacked in the angle direction.




\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth]{image/sort}
    \caption{(a) The original static visualization presents important moments in the lives of university presidents.
    (b) The view after they are sorted according to their birthdays.
    }
    \label{fig:class_project}
\end{figure}

\textbf{Raster image support.}
Our current prototype system primarily focuses on parsing visualizations in vector formats. However, many static visualizations are not represented in vector formats, such as historical visualizations~\cite{zhang2023oldvis}. For such visualizations, the automatic addition of interactivity would be of great interest. In the future, we envision leveraging object detection and optical character recognition (OCR) techniques to support raster images. Subsequent parsing procedures, akin to the present modeling process, will ensue after control points extraction. This approach holds the potential to significantly broaden the scope of applicable scenarios.




\subsection{Toward interaction authoring toolkit}






The primary innovation of our work lies in enhancing the interactive capabilities of existing visualizations. This approach emphasizes spatial representation within visualizations while disregarding tools used for visualization creation.
The principal challenge in extending this work into a toolkit lies in compatibility. The fundamental unit of interaction and transformation in this paper is the control point. However, if one aims to integrate corresponding plugins into existing toolkits, a reconfiguration of the original visualization implementation in terms of control points is required. Such an implementation may encounter resistance from existing visualization practices; for instance, within visualizations realized using Vega-Lite, our approach necessitates a reorganization of visualizations.

Customized interactive capabilities will be provided, catering to the specific needs of scenarios. The current method offers a comprehensive range of interaction features for existing visualizations. It supports the addition of interactive features to these visualizations. For many toolkit scenarios, the data and toolkit used in visualizations are known. As a manifestation of a toolkit, this paper is a well-suited candidate for augmenting interactivity upon pre-existing visualizations, with provisions for user customization, such as defining required interactions or enabling user-defined settings.

Our model enables the activation of static visualizations through a set of low-level operations involving visual objects, coordinate axes, and constraints. Nevertheless, these low-level interactions might pose a time-consuming obstacle to users in constructing meaningful interactions. To address this issue, for instance, a set of constraints could be established to transform bubbles of different colors into bars within a bar chart, obviating the need for individual constraint configuration, as illustrated in Figure \ref{fig:force_handle}. In the future, incorporating user-defined constraints as novel forms of interaction and preserving them could significantly augment the semantic diversity of interactions.



\subsection{Toward Intelligent Interaction}







This method can be enhanced in several aspects through the application of deep learning techniques.

\textbf{Intelligent constraint deduction.}
During the constraint inference process, it is common to employ heuristic and rule-based algorithms to cover prevalent implementation methodologies. However, due to varying user implementations, inherent uncertainty emerges in the inference process. For instance, within a ThemeRiver visualization, the mapping of data based on relative height versus absolute height may lack certainty. The incorporation of deep learning models can aid in resolving such classification ambiguities.

\textbf{User intent inference through mixed initiatives.}
Concerning the aspect of user intent inference, while achieving complete machine-driven inference is challenging, a subset of classification tasks can be formulated to guide machine decision-making. For instance, one such task involves discerning whether a user rapidly drags a visual element beyond the current canvas â€“ an action open to diverse interpretations of speed across user profiles. To address this, user feedback can be leveraged, enabling deep learning to enhance user exploration efficiency. It could contribute to decisions like creating new canvases, deleting visual objects, among other actions, while not entirely replacing user agency.

\textbf{Combining our model with other interaction techniques.}
Our method facilitates a more intuitive interaction for users to comprehend visualizations. This endeavors to minimize the user's barrier to utilization and comprehension of visualization interactions. In the future, incorporating natural language interaction represents another means of reducing the user's threshold. Our approach can be integrated with natural language interaction techniques, enabling users to simply describe the comparison they wish to make between two parts of a stacked area chart, for example. After natural language processing, our system can then further display the transition of visual objects that clearly illustrates the differences. This approach results in reduced barriers for the user in terms of both articulating their intention and comprehending changes.


























  

























