\section{Discussion and Future Work}
\label{section:discussions}

In this section, we discuss the coverage of the supported interactions and supported visualization and point out directions for future work.

\subsection{Coverage of Supported Interactions}



The supported manipulations for an existing visualization are based on the current visual marks of the visualization.
There are two limitations: our approach does not change the number of control points, and our approach is only based on information in the current visualization, without extra information.
Among the unipolar interactions of Sedig and Parsons~\cite{sedig2013interaction}, drilling and blending, which require extra information or a change in the number of control points, can not be supported by our framework; arranging, assigning, cloning, comparing, filtering, navigating, transforming, and translating can be fully or partially supported in the current framework because these interactions can be converted to the position changes of control points.
Among the bipolar interactions, composing/decomposing triggers a change in the number of control points.
Thus, it is not part of our framework.
Interactions like gathering/discarding, inserting/removing, and storing/retrieving can be performed by manipulating constraints and dragging visual marks among canvases.
\revision{In the future, we can extend the constraints model by allowing the number of control points to change and by binding visual marks with extra information.}




\subsection{Coverage of Supported Visualizations}


In the previous sections, we demonstrated some common visualizations, including area charts and bubble charts.
Our approach can be applied to more visualizations.


\textbf{Coverage of visualization types.}
Ideally, at the conceptual level, common visualizations encode spatial channels with attributes that are compatible with our spatial-constraints model.
For example, the interactions that change the stacked order/direction will be effective for the visualizations with stacked collisions.
For those visualizations that only have gravity constraints, including line charts and scatter plots, selecting and filtering can be supported by moving visual marks to a new canvas.
Currently, visualizations with a Cartesian coordinate system can be parsed by our system.
The axis type determines what axis operations are supported.
For example, the visualization in \autoref{fig:class_project} shows important moments in the lives of China's university presidents (e.g., being born and obtaining a Ph.D. degree).
We can sort the presidents according to their birthdays to better support comparison tasks.
The results are shown in \autoref{fig:class_project} (b).
We can extract a horizontal axis and a vertical axis (i.e., a visualization with a Cartesian coordinate system), and the continuous axis is taken as the linear scale. 
In the future, we could extend the model to visualizations with parallel axes or polar coordinate systems. 
For parallel axes, spatial constraints can be constructed for every axis.
If we want to support manipulations that change the organization of the axes (e.g., change the axis order of parallel coordinates), additional rules for axis positioning are needed.
For polar coordinate systems, we can conceptually regard the coordinate systems as Cartesian coordinate systems in which the pie chart is stacked in the angle direction.




\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth]{image/sort.pdf}
    \caption{(a) The original static visualization presents important moments in the lives of university presidents.
    (b) The view after they are sorted according to their birthdays.
    }
    \label{fig:class_project}
\end{figure}

\textbf{Raster image support.}
Our current implementation focuses on visualizations in the SVG format. 
In the future, we can support the raster images (i.e., bitmap images) using object detection and OCR techniques.
\revision{After the control points are extracted, the modeling process will be similar to the current modeling process.
The constraints model for the raster model could support the manipulation of static visualizations printed in paper media; this could broadly extend the usage scenarios.}



\subsection{Toward Intelligent Interaction}

We have proposed a spatial constraints model for visualization interaction that has the potential to enhance the intelligence of interactions.
Atomic constraints can be grouped or combined with other interaction methods to achieve more advanced interactions.

\textbf{Toward interaction authoring tools.}
Our model makes it possible to activate a static visualization using a set of low-level interactions with visual marks, axes, and constraints.
However, these low-level interactions may make it time-consuming for users to construct meaningful interactions.
To solve this problem, for example, we could set a group of constraints that can transform different colors of bubbles into bars in a bar chart without setting the constraints one by one, as in \autoref{fig:force_handle}.
In the future, constructing and saving user-created constraints as newly authored interactions will greatly increase the semantic diversity of the interactions.





\textbf{Combining our model with other interaction techniques.}
Our method facilitates a more intuitive interaction for users to comprehend visualizations in a more streamlined manner. This endeavors to minimize the user's barrier to utilization and comprehension of visualization interactions. In the future, incorporating natural language interaction represents another means of reducing the user's threshold. Our approach can be integrated with natural language interaction techniques, enabling users to simply describe the comparison they wish to make between two parts of a stacked area chart, for example. After natural language processing, our system can then further display the transition of visual marks that clearly illustrates the differences. This approach results in reduced barriers for the user in terms of both articulating their intention and comprehending changes.

\textbf{Deducing the user intent intelligently.}
Our method relies on constructed rules for inferring user intent. For instance, when a user drags a visual element, they may intend to delete, move, or alter the stacking order. While our rules attempt to accommodate users' habits, different users possess varying habits and preferences. This poses a challenge for fixed rules to cater to diverse users. In such a situation, deep learning models can be deployed to deduce user intent. These models can be trained using provenance data, and the system can be personalized to meet the needs of distinct users.





















  

























