We consider an outsourcing scheme between a client-side data owner $\mathcal{C}$ and a server $\mathcal{S}$, where $\mathcal{S}$ executes a function $f(x): X \rightarrow Y$ on data provided by $\mathcal{C}$. The function $f$ can either belong to $\mathcal{C}$ (e.g., in IaaS/PaaS~\cite{Armbrust2010}), or to the server (e.g., in SaaS/API~\cite{Armbrust2010}). We adopt a more realistic threat model in which the server $\mathcal{S}$ is not fully trustworthy and may be malicious or vulnerable to tampering with the ciphertext $f(x)$. This departs from the traditional semi-honest setting, in which $\mathcal{S}$ is assumed to be honest but curious (HBC) about inferring $\mathcal{C}$'s data privacy. Our scheme should  satisfy the following security properties: {\bf Integrity}: $\mathcal{C}$ could detect an integrity attack when interacting with $\mathcal{S}$ for any input $x$ and ensure the correctness of $y=f(x)$.  {\bf Data Privacy}: $\mathcal{S}$ cannot learn any information about input $x$. 
While the function $f()$ can be made public to both the client and server in many applications~\cite{Occlumency:TEE-deep-learning,VISE-TEE-FHE}, it is still important to note that in certain applications and scenarios~\cite{tramer2018slalom}, preserving the privacy of the function may be a critical concern. {\bf Function Privacy}: If $f()$ is provided by $\mathcal{C}$, $\mathcal{S}$ cannot learn information about $f()$ beyond its size. If $f()$ belongs to $\mathcal{S}$, $\mathcal{C}$ cannot learn more about $f()$ than what is revealed by $y = f(x)$. 
