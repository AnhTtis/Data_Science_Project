 \section{Benchmarks and Evaluation}
\label{sec:eval}

We evaluate \krakenSpace to answer the following set of questions:
\begin{itemize}
\item How much improvement does partial evaluation and our implemented compiler optimizations give \kraken? %(\S \ref{sec:eval2})
\item How much faster is our purely functional f-expr language, \krakenSpace, compared to other implementations of fexprs? %(\S \ref{sec:eval1} - \ref{sec:eval2})
\item How does \kraken's performance, with its fexprs, compare to macros? %(\S \ref{sec:eval1}, \S \ref{sec:eval3})
\item How do the different partial evaluation mechanisms/optimizations in \krakenSpace contribute towards reduction in overall runtime?
%\item What does \krakenSpace do internally when we create a data structure and evaluate it for some function? (\S \ref{sec:casestudy})
\end{itemize}

\textbf{Experimental Setup}: 
We ran these experiments in a reproducible Nix environment on a NixOS install \cite{10.1145/1411203.1411255} (Kernel 6.0.0) on a laptop with 8 cores / 16 threads and 64 GB of RAM.
Our code contains the scripts and Nix Flakes needed to reproduce the exact set of dependencies to run our tests.
%The code can be found at \url{https://github.com/limvot/kraken}.

The Kraken benchmarks were run using both the Wasmtime and WAVM WebAssembly engines for most benchmarks.
The Wasmtime WebAssembly engine is one of the most popular, developed by the Bytecode Alliance itself, and uses the CraneLift code generation backend.
The WAVM WebAssembly engine is interesting for its use of LLVM, and it often produces the fastest code on benchmarks but has a higher startup time.
We eliminated the Cfold Wasmtime benchmark due to problems running out of stack space (a known property of the Cfold benchmark).

\textbf{Benchmarks}: 
To showcase the capability of Kraken, we created benchmarks that are commonly implemented in functional languages and have been used as benchmarks in other papers \cite{reinking2021perceus, 10.1145/3547646}.
The benchmarks are
\begin{itemize}
\item Fib - Calculating the nth Fibonacci number
\item RB-Tree - Inserting n items into a red-black tree, then traversing the tree to sum its values
\item Deriv - Computing a symbolic derivative of a large expression
\item Cfold - Constant-folding a large expression
\item NQueens - Placing n number of queens on the board such that no two queens are diagonal, vertical, or horizontal from each other
\end{itemize}
All benchmarks besides Fibonacci use the fexpr version of match for pattern matching in \kraken, which is equivalent to the macro version in NewLisp. We also RB-Tree using NewLisp's~\cite{mueller2018newlisp} version of fexpr match. We modified the sizes of the problems presented to the benchmark to account for the longer running times of some of the less-optimized implementations.
The code for Kraken and NewLisp is very similar, and we should note that it is very unidiomatic NewLisp.
Our goal was not to compare Kraken and NewLisp as implementation languages for Red-Black Trees, but to stress test a single reasonably complex fexpr/macro, namely pattern matching.
% \textbf{Comparison with other languages}: We evaluated \krakenSpace against a language that contains f-exprs, as well as against itself with various optimizations disabled. The only other language we could find which contains a real f-expr mechanism is NewLisp~\cite{mueller2018newlisp} and so we ported \kraken's benchmark implementation to NewLisp.

%The six state-of-the-art languages are Java 17.0.1, Swift 5.4.2, Koka 2.3.2, C++, Haskell 8.10.7, and OCaml 4.12.
%The language choices were taken directly from Perceus reference-counting paper \cite{reinking2021perceus}.
%The Fibonacci benchmark additionally tests Python 3.9.11 and Chez Scheme 9.5.4.
%Koka, Ocaml and Haskell are good comparison points as statically-typed, compiled, functional programming languages, while Chez Scheme is a good comparison point as a mature and industrial strength dynamically-typed Scheme implementation known for its performance. 
%\subsection{Basic Level Comparison}
\subsection{The Effect of Partial Evaluation on Eval Calls}

\begin{table}[h]
\caption{Number of eval calls with no partial evaluation for Fexprs}
	\begin{tabular}{||c | c c c c c ||} 
		\hline
		&Evals & Eval w1 Calls & Eval w0 Calls & Comp Dyn & Comp Dyn\\ 
        & & & & w1 Calls & w0 Calls\\ [0.5ex] 
		\hline\hline
		Cfold 5 & 10897376 & 2784275 & 879066  & 1 & 0 \\ 
		\hline
		  Deriv 2  & 11708558 & 2990090 & 946500 & 1 & 0 \\ 
        \hline
		  NQueens 7 & 13530241 & 3429161 & 1108393 & 1 & 0 \\ 
    \hline
		  Fib 30 & 119107888 & 30450112 & 10770217 & 1 & 0 \\ 
    \hline
		  RB-Tree 10 & 5032297 & 1291489 & 398104 & 1 & 0 \\ 
		\hline
	\end{tabular}
    \label{npe:calls}
 \end{table}

As mentioned before, using fexprs without partial evaluation will prelude optimization and cause a massive amount of repeated work. Table \ref{npe:calls} and Table \ref{pe:calls} show the number of calls to the \krakenSpace runtime's eval function, the number of times the runtime's eval function executed a call to an applicative with wrap\_level=1, the number of times the runtime's eval function executed a call to an operative with wrap\_level=0, the number of compiled dynamic calls to applicatives with wrap\_level=1, and the number of compiled dynamic calls to operatives with wrap\_level=0.
These are shown for \krakenSpace test cases with partial evaluation turned off and turned on. 
\begin{table}[h]
\caption{Number of eval calls in Partially Evaluated Fexprs}
	\begin{tabular}{||c | c c c c c ||} 
		\hline
		&Evals & Eval w1 Calls & Eval w0 Calls & Comp Dyn & Comp Dyn\\ 
        & & & & w1 Calls & w0 Calls\\ [0.5ex] 
		\hline\hline
		Cfold 5 & 0 & 0 & 0  & 0 & 0 \\ 
		\hline
		  Deriv 2  & 0 & 0 & 0 & 2 & 0 \\ 
        \hline
		  NQueens 7 & 0 & 0 & 0 & 0 & 0 \\ 
    \hline
		  Fib 30 & 0 & 0 & 0 & 0 & 0 \\ 
    \hline
		  RB-Tree 10 & 0 & 0 & 0 & 10 & 0 \\ 
		\hline
	\end{tabular}
    \label{pe:calls}
 \end{table}

\begin{table}[h]
\caption{Number of calls to the runtime's eval function for RB-Tree. The table shows the non-partial evaluation numbers -> partial evaluation numbers.}
	\begin{tabular}{||c | c c c c c ||} 
		\hline
		&Evals & Eval w1 Calls & Eval w0 Calls & Comp Dyn & Comp Dyn\\ 
        & & & & w1 Calls & w0 Calls\\ [0.5ex] 
		\hline\hline
		  RB-Tree 7 & 2952848 -> 0 & 757932 -> 0 & 233513 -> 0 & 1 -> 7 & 0 -> 0\\ 
        \hline
		  RB-Tree 8 & 3532131 -> 0 & 906548 -> 0 & 279379 -> 0 & 1 -> 8 & 0 -> 0\\ 
        \hline
		  RB-Tree 9 & 4278001 -> 0 & 1097965 -> 0 & 3383831 -> 0 & 1 -> 9 & 0 -> 0\\ 
		\hline
	\end{tabular}
    \label{pe:rb}
    \vspace{-4mm}
 \end{table}

Without partial evaluation, no compilation can be done because it is impossible to tell if arguments to calls will be evaluated. In all benchmarks, partial evaluation removed all calls to the runtime's eval function, resulting in a completely compiled program. Looking at RB-Tree, there are over a million calls to combiners with wrap level 1 (normal functions), and 398,000 calls to combiners with wrap level 0 (operatives replacing macros). This massive blowup in the number of calls is due to the repeated and exponential re-execution of macro-like-combiners in the definition of other macro-like-combiners, as discussed in the Introduction.

The non-partially-evaluated benchmarks show 1 compiled dynamic call to an applicative (its the first call into eval) and 0 compiled dynamic calls to operatives, because there is no compilation at all. For the partially evaluated benchmarks, there are a few compiled dynamic calls to applicatives due to higher-order function use in the benchmarks, and there are no compiled dynamic calls to operatives, as all operative use has been eliminated.
We also varied the inputs for RB-Tree shown in Table \ref{pe:rb} to give a sense for how the number scale with respect to input size.

The incredible slowdown implied by these tables comes to full fruition in our RB-Tree test in Fig.~\ref{fig:kraken_nqueens_rbtree}.
We kept this run shorter because Kraken's non-partial-evaluating interpreter takes an incredibly long time even for 100 insertions (40 minutes).
The compounding layers of repeated macro-like operative calls in the non-partially-evaluated Kraken version cause a ~70,000x slowdown relative to the partial evaluated, optimized, and compiled version.
For the remaining benchmarks, we remove the naive interpreted \krakenSpace version, as in each case its performance is so bad as to blow out the graph and make it impossible to do any comparison.
In our optimized Kraken, our partial evaluation algorithm is able to fully collapse these levels of inefficiency, evaluate and inline the results, and give the backend more specialized code to optimize, emitting a compiled version that handily beats not only the NewLisp-fexpr implementation but even the NewLisp-macro implementation, as can be seen in Fig.~\ref{fig:kraken_vs_world_fib}.
We kept the benchmark sizes small in this test because the stack limits of NewLisp prevent sizes larger then ~880, while the Tail Call Elimination performed by the \krakenSpace compiler allows us to run much larger benchmarks, including the run of 4,800,000 inserts to the RB-Tree.
This result shows the dramatic effect of partial evaluation and compiler optimizations on runtime for \kraken. Our technique takes the performance of a fully fexpr based language from being completely infeasible to being faster than a macro-based dynamic scripting language currently in use.
% \begin{center}
% \begin{table}[ht]
% \caption{Number of call to the runtime's eval function for Fib. The table shows the non-partial evaluation numbers -> partial evaluation numbers}
% 	\begin{tabular}{||c | c c c c c ||} 
% 		\hline
% 		&Evals & Eval w1 Calls & Eval w0 Calls & Comp Dyn w1 Calls & Comp Dyn w0 Calls\\ [0.5ex] 
% 		\hline\hline
% 		Fib 10 & 8468 -> 0 & 2167 -> 0  & 777 -> 0 & 1 -> 0 & 0 -> 0 \\ 
% 		\hline
% 		  Fib 15  & 87916 -> 0 & 22478 -> 0 & 7961 -> 0 & 1 -> 0 & 0 -> 0 \\ 
%         \hline
% 		  Fib 20 & 969010 -> 0 & 247731 -> 0 & 87633 -> 0 & 1 -> 0 & 0 -> 0 \\ 
%     \hline
% 		  Fib 25 & 10740492 -> 0 & 2745825 -> 0  & 971209 -> 0 & 1 -> 0 & 0 -> 0 \\ 
% 		\hline
% 	\end{tabular}
%     \label{pe:fib}
%  \end{table}
% \end{center}

\begin{figure}[h]
\caption{Constant Fold and Deriv}
\includegraphics[width=0.45\textwidth]{cfold_table.csv_}
\includegraphics[width=0.45\textwidth]{deriv_table.csv_}
\label{fig:kraken_const_deriv}
\vspace{-6mm}
\end{figure}
\subsection{Comparison between Kraken Versions}
Beyond the massive speedup from partial-evaluation, Fig. \ref{fig:kraken_const_deriv} and \ref{fig:kraken_nqueens_rbtree} show the effect of the various compiler optimizations we described by disabling them one by one.
 Our main four optimizations have a strong positive effect on runtime, with the exception of lazy environment instantiation. Lazy environment instantiation helps massively on fib, and some on Deriv, but generally hurts the rest slightly.


\begin{figure}[h]
\caption{N-Queens}
\includegraphics[width=0.45\textwidth]{nqueens_table.csv_}
\includegraphics[width=0.45\textwidth]{slow_rbtree_table.csv_}
\label{fig:kraken_nqueens_rbtree}
\vspace{-4mm}
\end{figure}


\subsection{Comparison against Others}


To give a general idea of our current performance, we also show a Fibonacci benchmark that mostly exercises pure function-call speed and inlining as seen in Fig. ~\ref{fig:kraken_vs_world_fib}.
We include Python and Chez Scheme to give a general idea for where an exemplar slow and an exemplar fast dynamic language would fall.
With the benefit of our partial evaluation, compilation, and leaning upon mature WebAssembly implementations, we beat both, but this should be taken with a grain of salt, as this is a very limited micro-benchmark only meant to give a general sense of the order of magnitude of our performance.



\label{sec:eval1}
\begin{figure}[h]
\caption{Kraken vs. Others. Ordered by fastest to slowest}
\includegraphics[width=0.45\textwidth]{fib_table.csv_}
\includegraphics[width=0.45\textwidth]{rbtree_table.csv_}
\label{fig:kraken_vs_world_fib}
\end{figure}

%\label{sec:eval_nqueens}
%\begin{figure}[h]
%\caption{N-Queens}
%\includegraphics[width=0.45\textwidth]{nqueens_table.csv_}
%\includegraphics[width=0.45\textwidth]{slow_nqueens_table.csv_}
%\label{fig:kraken_nqueens}
%\end{figure}

%\label{sec:eval_nqueens}
%\begin{figure}[h]
%\caption{Kraken, N-Queens, absolute value and log-scale}
%\includegraphics[width=0.45\textwidth]{nqueens_table.csv_}
%\includegraphics[width=0.45\textwidth]{nqueens_table.csv_log}
%\label{fig:kraken_nqueens}
%\end{figure}
%\label{sec:eval_nqueensp}
%\begin{figure}[h]
%\caption{Kraken, N-Queens, absolute value and log-scale}
%\includegraphics[width=0.45\textwidth]{slow_nqueens_table.csv_}
%\includegraphics[width=0.45\textwidth]{slow_nqueens_table.csv_log}
%\label{fig:kraken_nqueensp}
%\end{figure}

%\label{sec:eval_cfold}
%\begin{figure}[h]
%\caption{C-Fold}
%\includegraphics[width=0.45\textwidth]{cfold_table.csv_}
%\includegraphics[width=0.45\textwidth]{slow_cfold_table.csv_}
%\label{fig:kraken_cfold}
%\end{figure}
%\label{sec:eval_cfold}
%\begin{figure}[h]
%\caption{Kraken, C-Fold, absolute value and log-scale}
%\includegraphics[width=0.45\textwidth]{cfold_table.csv_}
%\includegraphics[width=0.45\textwidth]{cfold_table.csv_log}
%\label{fig:kraken_cfold}
%\end{figure}
%\label{sec:eval_cfoldp}
%\begin{figure}[h]
%\caption{Kraken, C-Fold, absolute value and log-scale}
%\includegraphics[width=0.45\textwidth]{slow_cfold_table.csv_}
%\includegraphics[width=0.45\textwidth]{slow_cfold_table.csv_log}
%\label{fig:kraken_cfoldp}
%\end{figure}

%\label{sec:eval_deriv}
%\begin{figure}[h]
%\caption{Deriv}
%\includegraphics[width=0.45\textwidth]{deriv_table.csv_}
%\includegraphics[width=0.45\textwidth]{slow_deriv_table.csv_}
%\label{fig:kraken_deriv}
%\end{figure}
%\label{sec:eval_deriv}
%\begin{figure}[h]
%\caption{Kraken, Deriv, absolute value and log-scale}
%\includegraphics[width=0.45\textwidth]{deriv_table.csv_}
%\includegraphics[width=0.45\textwidth]{deriv_table.csv_log}
%\label{fig:kraken_deriv}
%\end{figure}
%\label{sec:eval_derivp}
%\begin{figure}[h]
%\caption{Kraken, Deriv, absolute value and log-scale}
%\includegraphics[width=0.45\textwidth]{slow_deriv_table.csv_}
%\includegraphics[width=0.45\textwidth]{slow_deriv_table.csv_log}
%\label{fig:kraken_derivp}
%\end{figure}

%\subsection{Comparison against state-of-the-art languages}
%\label{sec:eval3}

%\begin{figure}[h]
%\caption{Kraken vs. S.o.t.A.}
%\includegraphics[width=0.45\textwidth]{cfold_table.csv_}
%\includegraphics[width=0.45\textwidth]{rbtree_table.csv_}
%\label{fig:kraken_vs_world1}
%\end{figure}

%\begin{figure}[h]
%\caption{Kraken vs. S.o.t.A.}
%\includegraphics[width=0.45\textwidth]{deriv_table.csv_}
%\includegraphics[width=0.45\textwidth]{nqueens_table.csv_}
%\label{fig:kraken_vs_world2}
%\end{figure}

% \begin{figure}[h]
% \caption{Kraken vs. S.o.t.A. (Log)}
% \includegraphics[width=0.45\textwidth]{cfold_table.csv_log}
% \includegraphics[width=0.45\textwidth]{rbtree_table.csv_log}
% \label{fig:kraken_vs_world_log_1}
% \end{figure}
% \begin{figure}[h]
% \caption{Kraken vs. S.o.t.A. (Log)}
% \includegraphics[width=0.45\textwidth]{deriv_table.csv_log}
% \includegraphics[width=0.45\textwidth]{nqueens_table.csv_log}
% \label{fig:kraken_vs_world_log_2}
% \end{figure}

%As we noted before with the Fib(30) microbenchmark in Section \ref{sec:eval1}, we remain significantly slower than state-of-the-art compiled languages.
%This is particularly true for memory-intensive benchmarks due to our naive reference-counting and malloc/free implementations.
%However, our results are of a similar order of magnitude to the difference between the state-of-the-art compiled languages and dynamic scripting languages, like Python's results in the Fib(30) microbenchmark.
%We assert that is not a fundamental limitation because the classic f-expr slowness is being eliminated, as shown by Fig. \ref{fig:kraken_vs_newlisp1} and Fig. \ref{fig:kraken_vs_newlisp2}.
%In future work, we plan to expand our compile-time analysis and optimization to implement a modified, dynamic-language version of Perceus reference counting.
%With this change, we belive \krakenSpace can be competitive with these state-of-the-art languages.

%\subsection{Case Study: Red-Black Tree}
%\label{sec:casestudy}

%\begin{figure}[h]
%\caption{Kraken vs. S.o.t.A. - RB-Tree Focus}
%\includegraphics[width=0.4\textwidth]{rbtree_table.csv_}
%\includegraphics[width=0.4\textwidth]{rbtree_table.csv_log}
%\label{fig:kraken_vs_world_rbtree}
%\end{figure}


%To evaluate our partial evaluation algorithm and compiler, we extracted the benchmarks used by the Koka language project from their code repository and added Kraken versions, as well as implementing a naive Fibonacci microbenchmark ourselves to evaluate pure function call speed.\\
%With partial evaluation and the compiler optimizations listed above, we get fairly strong performance on purely numerical computations, such as the naive Fibonacci microbenchmark.
%Unfortunately, the overhead of our unsophisticated reference counting, dynamic type checking, and bounds checking causes poor performance on benchmarks involving data structures relative to mainstream programming language implementations.
%This is not a fundamental limitation, and will be addressed in future work, as recounted in the next section.
%It should be noted, however, that while the performance relative to established language implementations is very poor for the memory-intensive benchmarks (600-900x slower), we still realize a massive speedup compared to an unoptimized and non-partial-evaluated f-expr implementation (100,000x faster)!
