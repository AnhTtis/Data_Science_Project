\section{Compiler Backend and Optimizations}
\label{sec:opt}

Partial evaluation can eliminate all inefficiencies from macro-like operatives but there are other inefficiencies left that require backend optimizations.
One major remaining inefficiency is dynamic combiner call sites.
In such cases, no local information is available ahead of time to determine if the combiner that will be called is an applicative or an operative - that is, whether the arguments will be evaluated or not and whether the combiner needs to access its calling dynamic environment.
This would normally mean that code in the parameter position of dynamic calls cannot even be partially evaluated.
To overcome this inefficiency, we tag the compiled combiner closure values with bits indicating their wrap level and need of the calling environment.
Each dynamic call site branches on these bits.
Inside the "wrap\_level=0" side of the dynamic branch, a reference to the unevaluated arguments written out in static memory is emitted.
Inside the "wrap\_level=1" side of the dynamic branch, the compiler re-invokes unval and the partial evaluator recursively on each argument, resulting in code that is again as efficient as a language without fexprs (plus the overhead of the dynamic branch).
Note that this requires that both the partial evaluation algorithm above as well as the compilation algorithm be extended to support failure. Failure during partial evaluation does not necessarily mean failure during run-time. For instance, a dynamic combiner might always be an operative with a wrap level of 0, and so some erroring parameter code is actually data, and will never be evaluated (and thus never lead to an error).

Garbage is collected via reference counting for simplicity. Since this is a purely functional language, there are no cycles to worry about.
In order to remove the rest of the inefficiencies after partial evaluation, we have implemented various compiler optimizations.
The following sections cover the other key optimizations implemented in our compiler. 

\subsection{Lazy Environment Instantiation}
We delay the allocation and initialization of dynamic environment values until they are actually needed.  Combiner calls that do take in the dynamic environment check a dedicated register to see if the environment value has already been created. If not, it creates it.
This means the dynamic execution traces of combiner calls where there is no call that takes in the dynamic environment never reifies it and incurs only a single (predictable) branch of overhead.
For static combiner calls, this information (if arguments are evaluated, if it takes in the surrounding environment, etc) is known at compile time, and no runtime branches are generated.

\subsection{Type-Inference-Based Primitive Inlining}
In order to reduce the overhead of every built-in operation being a combiner call with dynamic types, we implemented type-inference guided inlining of primitive operations.
An analysis pass infers types based on branch predicates, which works quite well with the code generated by our match operative.
For instance, the combiner \textit{len} can be inlined to just a few bit-twiddling opcodes by determining that a particular variable must contain an array in \textit{cond}.

For instance, consider the following code:
    \begin{lstlisting}[language=Lisp,caption={Type Inference Example},label=code:typeinfer]
(cond (and (array? a) (= 3 (len a)))    (idx a 2)
      true                              nil)\end{lstlisting}
The call to \textit{idx} can be fully inlined without type or bounds checking because it resides in a block only reachable if the variable 'a' does contain an array of length 3.
No type information is needed to inline type predicates, as they only need to look at the tag bits.
Equality checks can be inlined as a simple word/ptr compare if any of its parameters are of a type that can be word/ptr compared (ints, bools, and symbols).
When type inference and primitive inlining is combined together, it means that every primitive call in most match expressions can be fully inlined into a handful of opcodes apiece.
In the above example, every single primitive listed will be inlined: the \textit{cond} to WebAssembly if blocks, the predicate functions to bit-twiddling and branches, the \text{idx} to bit-twiddling and a load with a constant offset, etc.

\subsection{Immediately-Called Closure Inlining}
Inlining calls to closure values that are allocated and then immediately used helps incur no overhead for implementing some operatives. The main macro-like operative reaping the benefit is "let".
As seen below, Listing \ref{code:letinline} is partially evaluated to the equivalent of Listing \ref{code:letinline2}, then inlined. As a result, the only overhead is the creation of a new environment, which is further made lazy and eliminated in the common case by Lazy Environment Instantiation. In this way, "let" is actually syntactic sugar for the definition and immediate call of a closure, like in many lambda calculi, but no efficiency is lost by doing so.
\begin{lstlisting}[language=Lisp,caption={Let Inlining Example},label=code:letinline]
    (let (a (+ 1 2))
         (+ a 3))
\end{lstlisting}
\begin{lstlisting}[language=Lisp,caption={Let Inlining Example - Expanded},label=code:letinline2]
    ((wrap (vau (a) (+ a 3))) (+ 1 2))
\end{lstlisting}



\subsection{Y-Combinator Elimination}
Continuing the theme of making the classic lambda-calculi implementations of concepts as efficient as standard implementations, the final set of optimizations ensures no overhead from using the Y-Combinator to implement recursion.
In Kraken, the Y-Combinator looks like  Listing \ref{code:ycomb} with a tiny example of its use shown in Listing \ref{code:ycombuse}.
    \begin{lstlisting} [language=Lisp,caption={The Y Combinator, as defined in Kraken},label=code:ycomb]
(let Y (lambda (f)
           ((lambda (x) (x x))
            (lambda (x) (f (wrap (vau app_env (& y) (lapply (x x) y app_env)))))))
)\end{lstlisting}

    \begin{lstlisting}[language=Lisp,caption={A Factorial function explicitly using the Y Combinator},label=code:ycombuse]
(Y (lambda (recurse) (lambda (n) (if (= 0 n) 1
                                             (* n (recurse (- n 1)))))))\end{lstlisting}


Normally, one does not manually use the Y-Combinator. In Kraken, there is a \textit{rec-lambda} derived operative that is easy to use and evaluates Y-Combinator behind the scenes.
Y-Combinator is actually always used to implement recursion in Kraken whether explicitly stated or not.
This allows us to keep our language pure, and in agreement with the calculus.

This optimization actually falls out naturally from our architecture, with just a little bit of care taken while bookkeeping.
When compiling a combiner, the compiler first inserts what the combiner index will be into a memoization dictionary before re-executing partial evaluation on the body of the combiner.
Any static recursive calls will have the exact form of the combiner currently being compiled, and so the compiler can emit a static reference to the correct combiner index.
All of this works because the re-executed partial evaluation of the body before compilation made sure to normalize the form of the combiner of a recursive call to be identical to that of the combiner being compiled before this partial-evaluation.
Since this is an eager language, the definition of the Y-Combinator in our language has an extra closure to prevent infinite recursion inside the Y-Combinator itself.
We, thus, additionally implement eta-conversion in the compiler to remove this extra level of indirection. Since the expression inside is now a constant instead of a call, there is no risk of infinite recursion. Combined with the normalization above, we achieve fully-efficient static recursive calls when using the Y-Combinator to define recursive functions.


Finally, as a purely functional Lisp, we use recursion instead of iteration.
While we wait for the tail\_call instruction in WebAssembly to be merged and implemented, we implemented a more limited form of Tail Call Elimination where auto-recursive calls in tail position are transformed into branches to the head of a loop that encloses the combiner's body.
The combination of Tail Call Elimination with Y-Combinator Elimination above means that a recursive function defined using the Y-Combinator can be as efficient as an imperative loop in other languages.
When WebAssembly finishes implementing the tail\_call instruction, it can easily be emitted to gain full proper tail calls.
