\section{Introduction}
\label{sec:intro}

Lisps languages~\cite{winston1986lisp,plt-tr1,hickey2008clojure} generally have two different abstraction methods: functions and macros. These two abstractions differ in their characteristics and semantics.
Functions operate at run-time and always evaluate their parameters, while macros operate solely at expansion time and do not evaluate their parameters.
Functions can sometimes (depending on the implementation) be used within macros but with restrictions.
These restrictions split the language into two, and do not exhibit a few of the key tenets of functional programming - namely, being higher-order and supporting composition.
%These restrictions and the differing levels of abstraction preclude a truly unified language system.
%As a result, we are stuck with a weaker macro system just so it can work in tandem with functions. 

%Aside from different abstraction methods, the macros' characteristics hinder the creation of a powerful, safe, and intuitive macro system.
Macro systems generally attempt to be hygienic by either preventing or making it difficult to manipulate the code environment of the macro expansion \cite{clinger1991macros}.
However, manipulation of the code environment is often needed in special cases.
The solution is typically various escape hatches that complicate the macro system.
%In addition, macros are also generally not first-class, meaning they cannot be used as values. If they cannot be considered as values, ...
For these reasons, practical macro systems are often fairly complex and different from the language they are embedded within.
In addition, because macros are expanded away, debuggability suffers as programmers must mentally expand macros to determine the cause of an error in code they did not explicitly write, but was generated by macro expansion. 
%The integration of the complex and weak macros critically affects the expressivity and debuggability needed by programmers. 

A possible solution to these problems, fexprs, were created as a first-class and more powerful alternative in the 1960s and reformulated in 2010~\cite{shutt2010fexprs}.
Following Shutt's terminology, fexprs unify functions, macros, and built-in language forms into a single concept called a combiner.
Where \textit{lambda} introduces functions, \textit{vau} introduces combiners.
A combiner is able to evaluate its arguments 0 or more times in the calling environment, and additionally receives that environment as a parameter, allowing it to dynamically evaluate code in the same scope from which it was called.
Combiners that evaluate their parameters 0 times are called operatives, while those that evaluate their parameters 1 or more times are called applicatives.
Applicatives that evaluate their parameters one time and do not use the calling environment are simply normal functions.
Operatives can subsume macros, built in special forms, and even control flow, like \textit{if} and \textit{lambda} itself.

A programming language based on combiners, using operatives instead of macros, provides a range of benefits:
\begin{itemize}
    \item \textbf{Simpler Language}: While not having a separate macro system already makes for a simpler language, supporting operative combiners as first-class values further simplifies the definition of the language because there are no special forms.
      For instance, \textit{if} and \textit{vau} are just built-in operative combiners with support from the language definition or hypothetical interpreter loop.
      Other language features often implemented via macros like \textit{let}, \textit{and}, \textit{or}, \textit{cond} can of course be implemented as derived operatives, and going further, even \textit{lambda} can be derived instead of built-in.
    
    \item \textbf{Simpler Mental Model and Better Debugging}: By unifying functions and macros (and formerly special forms) into one concept, debugging and mentally following macro-like operatives can be the same as for normal functions. When encountering an error, one can look at a stack trace and use a debugger to inspect stack variables to figure out what went wrong. The prototype debugger we developed in \krakenSpace can show this information even when it would otherwise be optimized away by re-evaluating the side-effect-free code necessary to reconstruct the missing information. In a language based on macros one would need to print out different expansions of the macro and then try to figure out which one failed and why, without debugger support.
    
    \item \textbf{Greater Flexibility and Expressivity}: All combiners, including operatives, are first class and can thus be named, passed to higher-order combiners, composed, or put into data-structures.
      While many languages have first class functions, first class macros have not enjoyed the same success.
      Combiners are both in one.
      For example, in Scheme \textit{and} is often a macro that expands to conditional control flow, meaning that it cannot be passed to a higher-order function such as \textit{fold} without first being wrapped in a \textit{lambda}, as seen in Listing~\ref{code:and1}.
      When \textit{and} is an operative, it can be freely passed to higher-order combiners as is, as demonstrated in Listing~\ref{code:and2}.

    \begin{lstlisting}[language=Lisp,caption={Scheme's version of \textit{and} example},label=code:and1]
        > (fold and #t (list #t #t))
        Exception: invalid syntax and 
        > (fold (lambda (a b) (and a b)) #t (list #t #t))
        #t\end{lstlisting}
    \begin{lstlisting}[language=Lisp,caption={\kraken's version of \textit{and} example},label=code:and2]
        > (foldl and true (array true true))
        true\end{lstlisting}

    
    The power of fexprs is reminiscent of advanced macro systems like that of Racket~\cite{plt-tr1}, which advocates for the definition of entirely new languages using its impressive, but complex macro system.
    A language using fexprs can have similar expressive power, but with simpler semantics.
    Listing~\ref{code:express} shows a sample fexpr implementation of \textit{let1} (a simple version of a let binding supporting only one variable) and \textit{lambda} (using \textit{let1}), demonstrating the early stages of bootstrapping a full language out of an extremely spartan base language where \textit{vau} is the only abstraction operator.
    The details of how this works will be explained later in the paper.
    This is only to give the flavor of how features most languages would consider primitive and built-in can instead be defined inside the language itself:

      \begin{lstlisting}[language=Lisp,caption={Fexpr implementation of \textit{let1} and \textit{lambda}},label=code:express]
        ((wrap (vau (let1)
        ; Definition of lambda
        (let1 lambda (vau se (p b1)
              (wrap (eval (array vau p b1) se)))
          ; a simple function that multiplies its argument by two
          (lambda (n) (* n 2))
        )
        ; Definition of let1
        )) (vau de (s v b)
              (eval (array (array vau (array s) b) (eval v de))
                    de)))\end{lstlisting} 
\end{itemize}

Despite all these benefits, naive execution of a pure language based on fexprs is exceedingly slow.
{\it During its evaluation, the body of the of the called combiner is re-executed every time it is invoked, not just for function-like calls to applicatives, but for all macro-like calls to operatives too}.  
This re-execution happens for ever combiner call in the definition of the called operative as well.
As a result, the re-executions will cause slowdowns likely to be exponential in the depth of the chain of definitions of macro-like operatives using other macro-like operatives in their definition.
On the other hand, a macro in a macro system would have been executed once at expansion time and never during runtime.
In the case of macros used in the definition of other macros, they will all be completely expanded before compilation. 
%The actual slowdown depends on the actual definitions of the combiners, but has acted exponential in all of our non-trivial testcases.
Because it is impossible to tell from syntax alone whether an argument to a combiner is evaluated or passed unevaluated (because it is impossible to tell if the call is going to be to an applicative or operative), code with fexprs is difficult to compile and optimize.
As a result, typical implementations of fexprs leave them unoptimized and entirely interpreted, augmenting the slowness issue.

Some languages like NewLisp and PicoLisp \cite{mueller2018newlisp, burger2013picolisp} that implemented some part of fexprs generally limit the number of layered fexprs to avoid compounding slowdowns. They chose to implement many combiners directly in the interpreter for speed instead of writing them as derived fexprs. Any code using fexprs in these languages still incur performance penalties or crash after hitting a limit. Other works \cite{shutt2010fexprs, kearsleyimplementing} have described fexpr languages and shown their usefulness but the few implementations based on them have been extremely slow.
As a result, a practical (fast) language based primarily on fexprs does not exist.

One solution to the performance issues of fexprs is partial evaluation.
Partial evaluators have been developed for numerous languages~\cite{elphick2003partial, LLOYD1991217, andersen1992self,meyer1991techniques,alpuente1998partial,10.1145/582153.582181} and for different domains~\cite{futamura1971partial,berlin1990partial,berlin1990compiling}.
The purpose of partial evaluation is {\it to specialize code based on values known at partial-evaluation time in order to do less work at execution time}, hopefully improving performance~\cite{10.1145/3140587.3062381,10.1145/243439.243447}. Partial evaluation can be broken into online and offline techniques with \cite{10.1145/243439.243447} explaining the differences the best. 
In addition, John Shutt~\cite{shutt2010fexprs} also suggested partial evaluation might be a solution to fexpr performance but did not provide any specific details or implementation.
Despite well-known online and offline partial evaluation techniques~\cite{10.1145/243439.243447,10.1145/158511.158707}, there is still no partial evaluation solution for fexprs, and so fexprs remain slow in all existing implementations.

%In this work we devise an online partial evaluator and compiler backend supporting first-class, partially symbolic environments specifically focused on evaluating away f-exprs that behave like macros in order to show that a functional Lisp based on F-expressions (f-exprs) can be approximately as efficient and at least as expressive as one based on macros. Concretely, we propose a language Kraken, a pure F-expression Lisp language with a novel partial evaluation and compilation system that produces reasonably performant WebAssembly binaries

In this work, we propose the first practical (fast) purely functional Lisp language based entirely on fexprs, \kraken.
\krakenSpace utilizes an online partial evaluator specifically created to evaluate away all calls to macro-esque operatives paired with a compiler backend that takes advantage of the static information exposed by the partial evaluator to produce reasonably performant WebAssembly binaries.
Using it, we show that {\it a functional Lisp based on fexprs can be approximately as efficient and at least as expressive as one based on macros}.

\textbf{\krakenSpace Language} (\S\ref{sec:base}): Our first contribution is our purely  functional fexpr-based Lisp called \kraken. The language is based on John Shutt's definition of pure Vau calculus~\cite{shutt2010fexprs} (using fexprs and based on \textit{vau} instead of \textit{lambda}) but augmented with explicit primitive data and operations to demonstrate a more practical language.

\textbf{Partial Evaluation and Compiler Optimizations} (\S\ref{sec:partial} - \S\ref{sec:opt}): 
 Our second contribution is the tailored partial evaluation algorithm and compiler optimizations that enable an fexpr-based functional language to have competitive performance.
 Partial evaluation will evaluate away any user-defined operatives that behave like macros, and the compiler will inline any primitive operatives (if, etc), leaving only the non-macro applicatives for runtime.
 Compiler optimizations (type-inference-informed primitive inlining, single-use-closure-inlining, and lazy-environment-creation) remove many of the remaining inefficiencies in our language.
 This combined partial-evaluation and compilation technique shows that macro-esque operatives can perform as well as macros due to being compiled away statically, similar to how macros would be expanded away.

\textbf{Evaluation of the Language} (\S\ref{sec:eval}): Our final contribution is the evaluation of the language and compiler on a few benchmarks to demonstrate its practicality.
Firstly, we show partial evaluation with compiler optimizations improves the runtime performance by over \textbf{70,000x} compared to our baseline interpreter.
Secondly, we show \kraken's optimized and compiled code performs significantly better than NewLisp's interpreted fexpr~\cite{mueller2018newlisp} implementation, by \textbf{233x} in one benchmark.
Lastly, we compare our runtime performance against NewLisp's macro implementation and end up faster than it as well.

The paper begins by discussing our general compilation flow in Section 2 before defining a simplified version of our \krakenSpace language in Section 3.
Section 4 contains our core contribution, the partial evaluation algorithm focused on evaluating away macro-esque operatives.
This is followed by a discussion of the major optimizations that work hand-in-hand with the partial evaluation algorithm in Section 5.
Section 6 presents our benchmarks demonstrating the dramatic speedups our algorithm achieves over the naive interpretation of fexprs before Section 7 lists related work and Section 8 concludes.
