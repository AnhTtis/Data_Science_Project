\pdfoutput=1
\documentclass[9pt,shortpaper,twoside,web]{ieeecolor}
\usepackage{generic}
\usepackage{cite}
\usepackage{captcont}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage[backref]{hyperref}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{\journalname, VOL. XX, NO. XX, XXXX 2023}
{Jin \MakeLowercase{\textit{et al.}}: Label-Efficient Deep Learning in Medical Image Analysis}
\begin{document}
\title{Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions}
\author{Cheng Jin$^{*}$, \IEEEmembership{Student Member, IEEE}, Zhengrui Guo$^{*}$, \IEEEmembership{Student Member, IEEE}, Yi Lin, Luyang Luo, \IEEEmembership{Member, IEEE}, and Hao Chen, \IEEEmembership{Senior Member, IEEE}
\thanks{\textit{*Equal contribution}}
\thanks{Manuscript received on December 4, 2023. This work was supported by National Natural Science Foundation of China (No. 62202403), Hong Kong Innovation and Technology Fund (No. PRP/034/22FX), and the Project of Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone (HZQB-KCZYB-2020083). (Corresponding author: Hao Chen.)}
\thanks{The authors are with the Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong. Hao Chen is also with the Department of Chemical and Biological Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong and HKUST Shenzhen-Hong Kong Collaborative Innovation Research Institute, Futian, Shenzhen, China.}
\\
\quad 
\\
\textit{\textcolor[rgb]{0.157, 0.369, 0.302}{(Methodological Review)}}
}

\maketitle

\begin{abstract}
Deep learning has seen rapid growth in recent years and achieved state-of-the-art performance in a wide range of applications. However, training models typically requires expensive and time-consuming collection of large quantities of labeled data. This is particularly true within the scope of medical imaging analysis (MIA), where data are limited and labels are expensive to acquire. Thus, label-efficient deep learning methods are developed to make comprehensive use of the labeled data as well as the abundance of unlabeled and weak-labeled data. In this survey, we extensively investigated over 300 recent papers to provide a comprehensive overview of recent progress on label-efficient learning strategies in MIA. We first present the background of label-efficient learning and categorize the approaches into different schemes. Next, we examine the current state-of-the-art methods in detail through each scheme. Specifically, we provide an in-depth investigation, covering not only canonical semi-supervised, self-supervised, and multi-instance learning schemes but also recently emerged active and annotation-efficient learning strategies. Moreover, as a comprehensive contribution to the field, this survey not only elucidates the commonalities and unique features of the surveyed methods but also presents a detailed analysis of the current challenges in the field and suggests potential avenues for future research.
\end{abstract}

\begin{IEEEkeywords}
Medical Image Analysis, Label-Efficient Learning, Semi-Supervised Learning, Self-Supervised Learning, Multi-Instance Learning, Active Learning, Annotation-Efficient Learning, Weakly-Supervised Learning.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{C}{omputer-aided} medical image analysis (MIA) plays a more and more critical role in achieving efficiency and accuracy in the early detection, diagnosis, and treatment of diseases. In recent years, MIA systems powered by deep learning (DL) have provided a more objective approach to learning from large and heterogeneous medical image datasets and improved disease diagnosis accuracy. However, DL models require abundant precisely annotated data to effectively capture anatomical heterogeneity and disease-specific traits\cite{yu2021convolutional} due to their data-driven nature. Unfortunately, due to a shortage of available annotators\cite{lu2020national}, there is a significant gap between the demand for annotation and the available annotated datasets. Hence, the urgency to curtail annotation expenses, expedite the annotation procedure, and alleviate the load on annotators has emerged as a crucial hurdle in DL-based MIA tasks. Traditional fully-supervised DL methods, on the other hand, depend solely on comprehensively annotated datasets. Recently, strategies based on semi-supervised, self-supervised, and multi-instance learning have been widely utilized to maximize the utility of existing medical data that may be only partially annotated by point, scribble, box, pixel-wise, \textit{etc.} or even completely unannotated data. In this paper, we dub these methods as label-efficient learning.  
As seen in Fig. \ref{fig_trend}, label-efficient learning methods have significantly proliferated in recent years. Meanwhile, label-efficient learning methods excelling in other MIA tasks like denoising, image registration, and super-resolution have also been rising beyond common classification, segmentation, and detection.
\input{Figures/fig_trend.tex} 

\input{Figures/fig_taxonomy.tex}
Several surveys related to label-efficient learning in medical image analysis have been published in recent years. Cheplygina \textit{et al.} \cite{cheplygina2019not} categorized methods under supervised, semi-supervised, multi-instance, and transfer learning and named them "not-so-supervised" learning, while Budd \textit{et al.} \cite{budd2021survey} surveyed human-in-the-loop strategies for MIA tasks. However, methods in these surveys are either limited in scope or lag behind the current trends. 
To address this issue, we conduct a systematic review of current label-efficient methodologies, of which the outline is depicted in Fig. \ref{fig_taxonomy}.

Aiming to provide a comprehensive overview and future challenges of label-efficient learning methods in MIA, we review more than 300 quality-assured and recent label-efficient learning methods based on semi-supervised, multi-instance, self-supervised, active, and annotation-efficient learning strategies. To pinpoint pertinent contributions, Google Scholar was employed to search for papers with related topics. ArXiv was combined through for papers citing one of a set of terms related to label-efficient medical imaging.
% To pinpoint pertinent contributions, Google Scholar was employed to search for papers containing xxx in the title or abstract. ArXiv was combined through for papers citing one of a set of terms related to medical imaging. 
Additionally, conference proceedings like CVPR, ICCV, ECCV, NIPS, AAAI, and MICCAI were scrutinized based on the titles of the papers, as well as journals such as MIA, IEEE TMI, and Nature Bioengineering. References in all chosen papers were examined. When overlapping work had been reported in multiple publications, only the publication(s) considered most significant were incorporated.

To the best of our knowledge, this is the first comprehensive review in the field of label-efficient MIA. In each learning scheme, we formulate the fundamental problem, offer the necessary background, and display the experimental results case by case. With the challenges proposed at the end of the survey, we explore feasible future directions in several branches to potentially enlighten the follow-up research on label-efficient learning.

The remainder of this paper is organized as follows. In Section \ref{sec:background}, the necessary background and categorization is presented. In Sections \ref{sec:semi}--\ref{sec:anno}, we introduce the primary label-efficient learning schemes in MIA, including semi-supervised learning in Section \ref{sec:semi}, self-supervised learning in Section \ref{sec:ssl}, multi-instance learning in Section \ref{sec: mil}, active learning in Section \ref{sec:al}, and annotation-efficient learning in Section \ref{sec:anno}. We discuss the existing challenges in label-efficient learning and present several heuristic solutions for these open problems in Section \ref{sec:cnfd}, where promising future research directions are proposed as well. Finally, we conclude the paper in Section \ref{sec:con}.

\input{Sections/sec_background.tex}
\input{Sections/sec_semi_sup}
\input{Sections/sec_self_sup}
\input{Sections/sec_multi_ins}
\input{Sections/sec_act_learn}
%\input{Sections/sec_few_shot}
\input{Sections/sec_anno_effi}
\input{Sections/sec_challenge_n_dirs}
\section{Conclusion}\label{sec:con}
Despite significant advances in computer-aided MIA, the question of how to endow deep learning models with enormous data remains a daunting challenge. Deep learning models under label-efficient schemes have shown significant flexibility and superiority in dealing with high degree of quality- and quantity-variant data. To that end, we have presented the first comprehensive label-efficient learning survey in MIA. A variety of learning schemes, including semi-supervised, self-supervised, multi-instance, active and annotation-efficient learning in the general field are classified and analyze thoroughly. We hope that by systematically sorting out the methodologies for each learning schemes, this survey will shed light on more progress in the future.

\appendix
\section{Concepts of Prior Knowledge}
\label{appendix1}
\subsection{Assumptions and Detail in Semi-supervised Learning}
\subsubsection{Assumptions in Semi-supervised Learning}
In fact, Semi-SL is not effective in every scenario. As stated in \cite{xiaojin2008semi, van2020survey}, a necessary condition for Semi-SL algorithms to work is that the marginal data distribution $p(x)$ contains underlying information about the posterior distribution $p(y|x)$, where $x$ and $y$ represent the data sample over input space $\mathcal{X}$ and the associated label, respectively. Otherwise the additional unlabeled data will be useless to infer information about $p(y|x)$, which means the Semi-SL algorithms may achieve similar or even worse performance compared with supervised learning algorithms. Therefore, several assumptions over the input data distribution have been proposed to constrain the data structure and ensure the algorithms can be generalized from a limited labeled dataset to a large-scale unlabeled dataset. Following \cite{van2020survey, ouali2020overview}, the assumptions in Semi-SL are introduced as follows:

\textbf{Smoothness assumption}. Suppose $x_1,~x_2 \in X$ are two input data samples over input space $\mathcal{X}$. If the distance between $x_1$ and $x_2$ is very close, \textit{i.e.}, $D(x_1, x_2) < \varepsilon$, where $\varepsilon$ is an artificially set threshold, then the associated labels $y_1$ and $y_2$ should also be the same. Note that sometimes there is an additional constraint in the smoothness assumption. In \cite{ouali2020overview}, $x_1$ and $x_2$ are required to belong to the same high-density region, so as to avoid the situation that these two samples reside on the brink of different high-density regions and are misclassified as one category.

\textbf{Cluster assumption}. In this assumption, we assume that data points with similar underlying information are likely to form high-density regions, i.e., clusters. If the two data points $x_1$ and $x_2$ lie in the same cluster, then they are expected to have the same label. In fact, the cluster assumption can be considered as a special case of the smoothness assumption. According to \cite{chapelle2009semi}, if the two data points $x_1$ and $x_2$ can be connected with a line that does not pass through any low-density area, they belong to the same cluster. 

\textbf{Low-density assumption}. The decision boundary of the classifier is assumed to lie in the low-density areas instead of high-density ones, which can be derived from the cluster assumption and smoothness assumption. On the one hand, if the decision boundary resides in the high-density regions, the two data points $x_1$ and $x_2$ located in the same cluster but opposite sided of the decision boundary will be categorized as different classes, which obviously violates the cluster assumption and smoothness assumption. On the other hand, following the cluster and smoothness assumption, data points in any high-density areas are expected to be assigned the same label, which means the decision boundary of the model can only lie in the low-density areas, thus satisfying the low-density assumption.

\textbf{Manifold assumption}. A manifold is a concept in geometry, that represents a geometric structure in a high-dimensional space, i.e., a collection of data points in the input space $\mathcal{X}$. For example, a curve in 2-dimensional space can be thought of as a 1-dimensional manifold, and a surface in 3-dimensional space can be seen as a 2-dimensional manifold. The manifold assumption states that there is a certain geometry of the data distribution in the high-dimensional space, namely that the data are concentrated around a certain low-dimensional manifold. Due to the fact that high-dimensional data not only poses a challenge to machine learning algorithms, but also leads to a large computational load and the problem of dimensional catastrophe, it will be much more effective to estimate the data distribution if they lie in a low-dimensional manifold. 

\subsubsection{Details of Key Generative Methods}
Researchers can obtain various generative methods according to different assumptions on the latent distribution. On the one hand, it can be easy to formulate a generative method once an assumption on the distribution is made, whereas on the other, the hypothetical generative model must match the real data distribution to avoid the unlabeled data in turn degrading the generalization performance. One can formulate the modeling process of generative methods as follows:

\begin{equation}\label{generative}
    \begin{aligned}
    y^{*}&=\arg \max _{y} p(y | x)=\arg \max _{y} \frac{p(x | y) p(y)}{p(x)}\\
    &=\arg \max _{y} p(x | y) p(y),\\
    \end{aligned}
\end{equation}
where the generative methods models the joint distribution $p(x,y)$. Eq. (\ref{generative}) indicates that if the correct assumption on prior $p(y)$ and conditional distribution $p(x | y)$ is made, the input data can be expected to come from the latent distribution. 

\textbf{Definition of Generative Adversarial Network (GAN)}
The aim of generator $\mathcal{G}$ is to iteratively learn the latent distribution from real data $x$  starting from generating data with random noise distribution $p(z)$. Meanwhile, the goal of discriminator $\mathcal{D}$ is to correctly distinguish the fake input generated by $\mathcal{G}$ and real data $x$. Formally, we can formulate the optimization problem of a GAN as follows:

\begin{equation*}
    \begin{aligned}
    \min_{\mathcal{G}}\max_{\mathcal{D}} \mathcal{L}(\mathcal{G},\mathcal{D}) &= \mathbb{E}_{x\sim p(x)}[\mathrm{log}\mathcal{D}(x)]\\
    &+\mathbb{E}_{z\sim p(z)}[1-\mathrm{log}(\mathcal{D}(\mathcal{G}(z)))],
    \end{aligned}
\end{equation*}
where $\mathcal{L}$ represents the loss function of generator $\mathcal{G}$ and discriminator $\mathcal{D}$. Concretely, $\mathcal{G}$ aims to minimize the objective function by confusing $\mathcal{D}$ with generated data $\mathcal{G}(z)$, while $\mathcal{D}$ aims at maximizing the objective function by making correct predictions.

\textbf{Definition of Variational Autoencoder (VAE)}
The typical VAE consists of two objectives: one is to minimize the discrepancy between input data $x$ and its reconstruction version $\hat{x}$ produced by the decoder, and the other is to model a latent space $p(z)$ following a simple distribution, such as a standard multivariate Gaussian distribution. Thus, the loss function for training a VAE can be formulated as follows:
\begin{equation*}
    \begin{aligned}
    \min_{\theta}\sum_{x\in X} \mathcal{L}(x, \theta) = \mathcal{L}_{MSE}(x,\hat{x}_{\theta})+\mathcal{L}_{KL}(p_{\theta}(z|x)||p(z)),
    \end{aligned}
\end{equation*}
where $\mathcal{L}_{MSE}$ represents the mean square error; $\hat{x}_{\theta}$ is the reconstruction version of input data $x$ generated by the decoder $p_{\phi}(x|z)$ given parameters $\phi$; $\mathcal{L}_{KL}(\cdot||\cdot)$ represents the Kullback-Leibler divergence which measures the distance between two distributions; and $p_{\theta}(z|x)$ denotes the posterior distribution produced by the encoder given parameters $\theta$.


\subsection{Conventional Uncertainty Metrics in Active Learning}
The uncertainty measure reflects the degree of dispersion of a random input. There are many ways to measure the uncertainty of inputs. Starting with simple metrics like standard deviation and variance, current studies in MIA mainly focus on \textbf{margin sampling} \cite{campbell2000query} and \textbf{entropy sampling}\cite{holub2008entropy}. Denote the probability as $p$, we give the definition of these metrics as follows.

\textbf{Margin sampling} \textbf{\cite{campbell2000query}} estimates the probability difference $\mathcal{M}$ between the first and second most likely labels $\hat{y}_{1}, \hat{y}_{2}$ according to the deep model parameter $\theta$ and expect the least residual value by the following notation:

\begin{equation*}
    \mathcal{M}=\underset{x}{\operatorname{argmin}}   [p_{\theta}\left(\hat{y}_{1} \mid x\right)-p_{\theta}\left(\hat{y}_{2} \mid x\right)]
\end{equation*}

\textbf{Entropy sampling}\cite{holub2008entropy} is another conventional metric for sampling. In a binary or multi-classification scenario, the sampled data with higher entropy can be selected as the expected annotation data. For a $C$-class task, entropy sampling metric $\mathcal{E}$ can be denoted as follows:
\begin{equation*}
    \mathcal{E}=\underset{x}{\operatorname{argmax}}\left(-\sum_{c=1}^{C}p(y_c\mid x)\log p(y_c\mid x)\right)
\end{equation*}

\section{Datasets}
As a supplement to the main text, we summarize representative publicly available datasets across 16 different organs such as the brain, chest, prostate, \textit{etc.} in Tab. \ref{tab:Dataset1}. 
These publicly available MIA datasets can be leveraged to construct label-efficient learning algorithms for numerous purposes, including classification, detection, and segmentation.

\input{Tables_supp/dataset_1}

\input{Tables_supp/dataset_2.tex}

\input{Tables_supp/dataset_3.tex}

\section{Surveyed papers}
Due to space limitations, only a selection of representative papers is presented in the main text. In this appendix, we will list all the papers we surveyed in the scope of label-efficient learning in Tab. \ref{tab:semi_supp}, \ref{tab:self_supp}, \ref{tab:mil_supp}, \ref{tab:al_supp}, and \ref{tab:anno_supp}.

\input{Tables_supp/tab_semi}

\input{Tables_supp/tab_self}

\input{Tables_supp/tab_multi_ins}

\input{Tables_supp/tab_al}

\input{Tables_supp/tab_anno}

\clearpage

\bibliographystyle{ieeetr}
\bibliography{reference.bib}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a1.png}}]{First A. Author} (M'76--SM'81--F'87) and all authors may include 
% biographies. Biographies are often not included in conference-related
% papers. This author became a Member (M) of IEEE in 1976, a Senior
% Member (SM) in 1981, and a Fellow (F) in 1987. The first paragraph may
% contain a place and/or date of birth (list place, then date). Next,
% the author's educational background is listed. The degrees should be
% listed with type of degree in what field, which institution, city,
% state, and country, and year the degree was earned. The author's major
% field of study should be lower-cased. 

% The second paragraph uses the pronoun of the person (he or she) and not the 
% author's last name. It lists military and work experience, including summer 
% and fellowship jobs. Job titles are capitalized. The current job must have a 
% location; previous positions may be listed 
% without one. Information concerning previous publications may be included. 
% Try not to list more than three books or published articles. The format for 
% listing publishers of a book within the biography is: title of book 
% (publisher name, year) similar to a reference. Current and previous research 
% interests end the paragraph. The third paragraph begins with the author's 
% title and last name (e.g., Dr.\ Smith, Prof.\ Jones, Mr.\ Kajor, Ms.\ Hunter). 
% List any memberships in professional societies other than the IEEE. Finally, 
% list any awards and work for IEEE committees and publications. If a 
% photograph is provided, it should be of good quality, and 
% professional-looking. Following are two examples of an author's biography.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a2.png}}]{Second B. Author} was born in Greenwich Village, New York, NY, USA in 
% 1977. He received the B.S. and M.S. degrees in aerospace engineering from 
% the University of Virginia, Charlottesville, in 2001 and the Ph.D. degree in 
% mechanical engineering from Drexel University, Philadelphia, PA, in 2008.

% From 2001 to 2004, he was a Research Assistant with the Princeton Plasma 
% Physics Laboratory. Since 2009, he has been an Assistant Professor with the 
% Mechanical Engineering Department, Texas A{\&}M University, College Station. 
% He is the author of three books, more than 150 articles, and more than 70 
% inventions. His research interests include high-pressure and high-density 
% nonthermal plasma discharge processes and applications, microscale plasma 
% discharges, discharges in liquids, spectroscopic diagnostics, plasma 
% propulsion, and innovation plasma applications. He is an Associate Editor of 
% the journal \emph{Earth, Moon, Planets}, and holds two patents. 

% Dr. Author was a recipient of the International Association of Geomagnetism 
% and Aeronomy Young Scientist Award for Excellence in 2008, and the IEEE 
% Electromagnetic Compatibility Society Best Symposium Paper Award in 2011. 
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a3.png}}]{Third C. Author, Jr.} (M'87) received the B.S. degree in mechanical 
% engineering from National Chung Cheng University, Chiayi, Taiwan, in 2004 
% and the M.S. degree in mechanical engineering from National Tsing Hua 
% University, Hsinchu, Taiwan, in 2006. He is currently pursuing the Ph.D. 
% degree in mechanical engineering at Texas A{\&}M University, College 
% Station, TX, USA.

% From 2008 to 2009, he was a Research Assistant with the Institute of 
% Physics, Academia Sinica, Tapei, Taiwan. His research interest includes the 
% development of surface processing and biological/medical treatment 
% techniques using nonthermal atmospheric pressure plasmas, fundamental study 
% of plasma sources, and fabrication of micro- or nanostructured surfaces. 

% Mr. Author's awards and honors include the Frew Fellowship (Australian 
% Academy of Science), the I. I. Rabi Prize (APS), the European Frequency and 
% Time Forum Award, the Carl Zeiss Research Award, the William F. Meggers 
% Award and the Adolph Lomb Medal (OSA).
% \end{IEEEbiography}

\end{document}
