\begin{table*}[ht]
\centering
\caption{Overview of Multi-instance Learning-based Studies in Medical Image Analysis}

\resizebox{\textwidth}{.33\textwidth}{
\begin{threeparttable}
\begin{tabular}{@{}llllll@{}}
\toprule
 &Reference$_{\text{Year}}$ & Organ & MIL Algorithm Design & Dataset & Result \\ \midrule
\rule{0pt}{2ex} \multirow{43}{*}{\rotatebox{90}{Classification}}&  Manivannan \textit{et al.} \cite{manivannan2017subcategory}$_{2017}$ &  Retina;   & Discriminative Subspace Transformation +  & Messidor; TMA-UCSB;  &Messidor: Acc: 0.728; TMA-UCSB: AUC: 0.967;\\
&&Breast&Margin-based Loss&DR Dataset; Private Dataset: 884 Images&DR Dataset: Acc: 0.8793; Private: Kappa: 0.7212\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
% \rule{0pt}{3ex}&Zhu \textit{et al.} \cite{zhu2017deep}$_{2017}$ & Breast & Sparse MIL & INBreast & AUC: 0.89\rule[-1.2ex]{0pt}{0pt}\\
% \cline{2-6}
% \rule{0pt}{3ex}& \rule{0pt}{2.5ex}Mercan \textit{et al.} \cite{mercan2017multi}$_{2017}$& Breast & Multi-Label MIL & BCSC & Average-P (Average-Precision): 0.8068\rule[-1.2ex]{0pt}{0pt}\\
% \cline{2-6}
\rule{0pt}{2.5ex}&Ilse \textit{et al.} \cite{ilse2018attention}$_{2017}$ &  Breast; Colon & Attention-based MIL   &  TMA-UCSB; CRCHistoPhenotypes &TMA-UCSB: Acc: 0.755; CRCHistoPhenotypes: Acc: 0.898\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Couture \textit{et al.} \cite{couture2018multiple}$_{2018}$ &  Breast  & Quantile Function-based MIL   &  CBCS3  &Acc: 0.952\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
% \rule{0pt}{2.5ex}&Das \textit{et al.} \cite{das2018multiple}$_{2018}$ &  Breast &  Multiple Instance Pooling   & BreakHis & Acc: 0.8906\rule[-1.2ex]{0pt}{0pt}\\
% \cline{2-6}
\rule{0pt}{2.5ex}&Liu \textit{et al.} \cite{liu2018landmark}$_{2018}$  & Brain & Landmark-based MIL    & ADNI; MIRIAD & ADNI: AUC: 0.9586; MIRIAD: AUC: 0.9716\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Campanella \textit{et al.} \cite{campanella2019clinical}$_{2019}$ &  Prostate; Skin; Lymph  & MIL + RNN   &  Private Dataset: 44,732 Images  & AUC: Prostate: 0.986; Skin: 0.986; Lymph: 0.965\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Wang \textit{et al.} \cite{wang2019rmdl}$_{2019}$ &  Breast &  Instance Features Recalibration   & Private Dataset: 608 Images & Acc: 0.865\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
% \rule{0pt}{2.5ex}&Tennakoon \textit{et al.} \cite{tennakoon2019classification}$_{2019}$ & Retina; Lung & Extreme Value Theorem-based MIL & ReTOUCH; DLCST & DLCST: AUC: 0.96\\
% \cline{2-6}
\rule{0pt}{2.5ex}&Yao \textit{et al.} \cite{yao2019deep}$_{2019}$ & Lung; Brain & Multiple Instance FCN & NLST; TCGA&NLST: C-Index: 0.678; TCGA: C-Index: 0.657\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Wang \textit{et al.} \cite{wang2020ud}$_{2020}$  &  Retina  & Uncertainty-aware MIL + RNN Aggregation  &  Duke-AMD; Private Dataset: 4,644 Volumes & Acc: Duke-AMD: 0.979; Private Dataset: 0.951 \rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Zhao \textit{et al.} \cite{zhao2020predicting}$_{2020}$  &  Colon & VAE-GAN Feature Extraction +    & TCGA-COAD & Acc: 0.6761; F1: 0.6667;\\
&&&GNN Bag-level Representation Learning&&AUC: 0.7102\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Chikontwe \textit{et al.} \cite{chikontwe2020multiple}$_{2020}$ & Colon & Jointly Learning of Instance- and Bag-level Feature   & Private Dataset: 366 Images & F1: 0.9236; P (Precision): 0.9254; R (Recall): 0.9231; Acc: 0.9231\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Raju \textit{et al.} \cite{raju2020graph}$_{2020}$ & Colon & Graph Attention MIL  & MCO& Acc: 0.811; F1: 0.798\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Han \textit{et al.} \cite{han2020accurate}$_{2020}$ & Lung & Automatic Instance Generation & Private Dataset: 460 Examples & AUC: 0.99\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Yao \textit{et al.} \cite{yao2020whole}$_{2020}$  & Lung; Colon & Siamese Multi-instance FCN + Attention MIL  & NLST; MCO &NLST: AUC: 0.7143; MCO: AUC: 0.644\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Hashimoto \textit{et al.} \cite{hashimoto2020multi}$_{2020}$ & Lymph & Domain Adversarial + Multi-scale MIL  & Private Dataset: 196 Images & Acc: 0.871 \rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Shao \textit{et al.} \cite{shao2021transmil}$_{2021}$  &  Breast; Lung; Kidney   & Transformer-based MIL  & CAMELYON 2016; TCGA-NSCLC; TCGA-RCC&Acc: CAMELYON: 0.8837; TCGA-NSCLC: 0.8835; TCGA-RCC: 0.9466\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Li \textit{et al.} \cite{li2021dual}$_{2021}$ & Breast; Lung & Dual-stream MIL + Contrastive Learning  & CAMELYON 2016; TCGA Lung Cancer &CAMELYON 2016: AUC: 0.9165; TCGA: AUC: 0.9815\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Li \textit{et al.} \cite{li2021novel}$_{2021}$ & Lung & Virtual Bags + Self-SL Location Prediction  & Private Dataset: 460 Examples & AUC: 0.981; Acc: 0.958; F1: 0.895; Sens: 0.936 \rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Lu \textit{et al.} \cite{lu2021data}$_{2021}$ & Kidney; Lung;& Attention-based MIL + Clustering  & TCGA-RCC + Private Dataset: 135 WSIs; & Kidney: AUC: 0.972;\\
&&Lymph node&&CPTAC-NSCLC + Private Dataset: 131 WSIs;&Lung: AUC: 0.975;\\
&&&&CAMELYON 2016,17 + Private Dataset: 133 WSIs&Lymph node: AUC: 0.940\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Wang \textit{et al.} \cite{wang2022lymph}$_{2022}$ & Thyroid & Transformer-based MIL + Knowledge Distillation  & Private Dataset: 595 Images &AUC: 0.9835; P: 0.9482; R: 0.9151; F1: 0.9297\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Zhang \textit{et al.} \cite{zhang2022dtfd}$_{2022}$ & Breast; Lung  & Double-Tier Feature Distillation MIL & CAMELYON 2016; TCGA-Lung &CAMELYON 2016: AUC: 0.946; TCGA-Lung: AUC: 0.961 \rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Schirris \textit{et al.}\cite{schirris2022deepsmile}$_{2022}$ & Breast; Colon & Heterogeneity-aware MIL + Contrastive Learning  & TCGA-CRCk; TCGA-BC  &TCGA-CRCk: AUC: 0.87; TCGA-BC: AUC: 0.81\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Su \textit{et al.} \cite{su2022attention2majority}$_{2022}$ & Breast; Kidney & Intelligent Sampling Method + Attention MIL & CAMELYON 2016; Private Dataset: 112 Images &CAMELYON 2016: AUC: 0.891; Private: AUC: 0.974\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Zhu \textit{et al.} \cite{zhu2022murcl}$_{2022}$ & Breast; Lung; Kidney& Reinforcement Learning + Contrastive Learning + MIL  & CAMELYON 2016; TCGA-Lung; TCGA-Kidney & AUC: CAMELYON: 0.9452; TCGA-Lung: 0.9637; TCGA-Kidney:  0.9573\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Yang \textit{et al.} \cite{yang2022micl}$_{2022}$ & Colon; Muscle & Curriculum Learning + MIL  & CRCHistoPhenotypes; Private Muscle Dataset: 266 Images&CRCHistoPhenotypes: AUC: 0.898; Private: AUC: 0.907\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Shi \textit{et al.} \cite{shi2023structure}$_{2023}$ & Stomach; Bladder & Multi-scale Graph MIL  & TCGA-STAD; TCGA-BLCA; Private Stomach Dataset: 574 Images&AUC: TCGA-STAD: 0.829; TCGA-BLCA: 0.886; Private: 0.907\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Yan \textit{et al.} \cite{yan2023genemutation}$_{2023}$ & Bladder & Hierarchical Deep MIL  & TCGA-Bladder & TCGA-Bladder: AUC: 0.92\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Shi \textit{et al.} \cite{shi2023mg}$_{2023}$ & Breast; Kidney& Multi-scale Transformer + MIL  & BRIGHT; TCGA-BRCA; TCGA-RCC & AUC: BRIGHT: 0.848; TCGA-BRCA: 0.921; TCGA-RCC: 0.990\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Liu \textit{et al.} \cite{liu2024advmil}$_{2024}$ & Lung; Breast; Brain & GAN + MIL  & NLST; TCGA-BRCA; TCGA-LGG&C-Index: NLST:  0.672; TCGA-BRCA: 0.566; TCGA-LGG: 0.642\rule[-1.2ex]{0pt}{0pt}\\
\hline
\rule{0pt}{3.5ex} \multirow{2}{*}{\rotatebox{90}{Segmentation}}& Jia \textit{et al.} \cite{jia2017constrained}$_{2017}$ &  Colon  & Multi-scale MIL + Area Constraint Regularization &  Private TMA/Colon Dataset: 60 Images/910 Images &F1: TMA: 0.622; Colon: 0.836\rule[-2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{3.5ex}&Xu \textit{et al.} \cite{xu2019camel}$_{2019}$ &  Breast  & Instance-level and Pixel-level Label Generation &  CAMELYON 2016 & Image-level Acc: 0.929; Pixel-level IoU: 0.847\rule[-2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{3.5ex} &  Dov \textit{et al.} \cite{dov2021weakly}$_{2021}$ &  Thyroid  & Maximum Likelihood Estimation-based MIL &  Private Dataset: 908 Images& AUC: 0.87\rule[-2ex]{0pt}{0pt}\\
\hline
\rule{0pt}{2.5ex} \multirow{4}{*}{\rotatebox{90}{Others}} &Schwab \textit{et al.} \cite{schwab2020localization}$_{2020\text{CD}}$ &  Lung  & Jointly Classification and Localization &  RSNA-Lung; MIMIC-CXR; Private Dataset: 1,003 Images&  AUC: 0.93\rule[-1.2ex]{0pt}{0pt}\\
\cline{2-6}
\rule{0pt}{2.5ex}&Wang \textit{et al.} \cite{wang2021learning}$_{2021\text{CS}}$  &  Pancreas  & Jointly Global-level Classification and    &  Private Dataset: 800 Images  & DSC: 0.6029; \\
&&&Local-level Segmentation&&Sens: 0.9975\rule[-1.2ex]{0pt}{0pt}\\
\bottomrule
\end{tabular}
\begin{tablenotes}    
        \footnotesize               
        \item[1] For the sake of brevity, we denote references that contain more than one task in the following abbreviations: \textbf{C}: Classification, \textbf{S}:Segmentation, \textbf{D}:Detection. 
        % \item[1] More Semi-supervised learning studies are included in Table \ref{tab:semi2}.
      \end{tablenotes}
\end{threeparttable}
}
\label{tab:mil}
\end{table*}