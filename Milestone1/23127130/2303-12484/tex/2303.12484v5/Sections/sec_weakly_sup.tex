\section{Inexact Label} \label{sec:weakly}
Obtaining precise, pixel-level annotations in MIA is often prohibitively expensive or time-consuming. In practice, clinicians frequently provide only incomplete or coarse annotations—such as sparse points or image-level labels—resulting in the so-called \textbf{inexact label} setting, where the available supervision does not match the granularity required for the target task. \textbf{Weakly supervised learning (WSL)} provides a general framework for addressing the challenges posed by inexact labels. The core issue in this context is label granularity alignment, which refers to bridging the gap between coarse-grained annotations and the fine-grained predictions demanded by clinical applications.
\input{Figures/fig_wsl_schematic}
Granularity alignment in WSL can be approached from two complementary perspectives. On the annotation side, \textbf{annotation-efficient learning} scheme aims to maximize the utility of limited or partially annotated data while minimizing annotation burden. These methods leverage both labeled and unlabeled data to enhance model performance with reduced annotation cost. On the task side, the challenge is to infer fine-grained information, such as pixel-level segmentation or lesion localization, from supervision that is only available at a coarser level, such as the whole image. \textbf{Multiple instance learning (MIL)} provides a representative learning scheme for this scenario by associating global image-level labels with collections of instances, such as patches or regions, and enabling the model to identify relevant instances based on weak supervision.

In the following, we elaborate on the principles and representative applications of these strategies, with a focus on how label granularity alignment enables robust learning from inexact labels in MIA.

\subsection{Annotation-Efficient Learning}\label{sec:anno}
To address label granularity alignment from the annotation perspective, \textbf{annotation-efficient learning} leverages deep learning techniques with partially labeled data to enhance labeling efficiency for dense predictions. A strategic approach to boost annotation efficiency is to utilize markings other than complete dense annotations. 
While there may be some overlap with other methodologies, annotation-efficient learning methods are specifically tailored to harness the unique attributes of various annotation forms to improve efficiency and bridge the granularity disparity between annotation and prediction.
In this section, we review representative annotation-efficient learning strategies that address label granularity alignment through a coarse-to-fine approach, focusing on techniques related to \textbf{image-level}, \textbf{point}, \textbf{scribble}, and \textbf{box} annotations. 
Appendix Tab.~\ref{tab:anno} provides an overview of notable publications in this domain.

\subsubsection{Image-level Annotation}\label{sec:anno_tag}

Image-level annotation, or tag annotation, is a concise text or binary label assigned to each image, stands out as the most efficient form of annotation. 
Many image-level annotation-based methods draw inspiration from the concept of class activation mapping (CAM)~\cite{zhou2016learning}. 
Several approaches utilize CAM to generate object localization proposals or achieve pixel-wise segmentation of entire objects.
For \textbf{detection} tasks, Wang \textit{et al.}~\cite{hwang2016self} introduced a dual-branch network that concurrently optimizes classification and lesion detection. This approach involves supervising the CAM-based lesion detection network solely with image-level annotations. 
The two branches are guided in tandem through a weight-sharing technique, employing a weighting parameter to regulate the learning focus between classification and detection tasks. 
In the realm of lesion detection, Dubost \textit{et al.}~\cite{dubost2020weakly} proposed a weakly supervised regression network, validated on both 2D and 3D medical images. 
For the \textbf{segmentation} task, Li \textit{et al.}~\cite{li2022deep} proposed a breast tumor segmentation method with only image-level annotations based on CAM and deep-level set (CAM-DLS).
It integrates domain-specific anatomical information from breast ultrasound to reduce the search space for breast tumor segmentation.
Similarly, Li \textit{et al.}~\cite{li_sg-mian_2024} a self-guided multiple information aggregation network for skin lesion segmentation using multiple spatial perceptron solely using classification information as guidance to discriminate the key classification features of lesion areas.
Meanwhile, Chen \textit{et al.}~\cite{chen2022c} proposes a causal CAM method for organ segmentation, which is based on the idea of causal inference with a category-causality chain and an anatomy-causality chain.
In addition, several studies~\cite{lin2019seg4reg,lin2021seg4reg+} demonstrate that bridging the classification task and dense prediction task (e.g., detection and segmentation) via CAM-based methods is beneficial for both tasks.
Compared to natural images, medical images are usually from low contrast, limited texture, and varying acquisition protocols~\cite{zhang2021weakly}, which makes directly applying CAM-based methods less effective.
Fortunately, incorporating the clinical priors (e.g., objects' size~\cite{fruh2021weakly}) into the weakly supervised detection task is promising to improve the performance.
% ---------------------------------------------------------------
\subsubsection{Point Annotation}\label{sec:anno_point}
% ---------------------------------------------------------------
Point annotation refers to the annotation of a single point of an object.
Several studies~\cite{roth2021going,dorent2021inter,khan2019extreme} focus on using extreme points as the annotation to perform pixel-level segmentation.
These methods typically consist of three steps: 1) extreme point selection; 2) initial segmentation with a random walk algorithm; 3) training of the segmentation model with the initial segmentation results. 
The last two steps can be iterated until the segmentation results are stable.
However, these methods require the annotators to locate the boundary of the objects, which is still laborious in practice.
In contrast, other studies~\cite{yoo2019pseudoedgenet,zhao2020weakly,qu2020weakly,qu2019weakly,tian2020weakly,belharbi2021deep,lin2023nuclei,valvano2021learning} use center point annotation to perform pixel-level segmentation for the task of cell/nuclear segmentation. 
These methods typically adopt the Vorinor~\cite{kise1998segmentation} and cluster algorithms to perform coarse segmentation. 
Then different methods are used to refine the segmentation results, such as iterative optimization~\cite{qu2019weakly,qu2020weakly}, self-training~\cite{zhao2020weakly}, and co-training~\cite{lin2023nuclei}. 

Compared with full annotation, point annotation can reduce the annotation time by around 80\%~\cite{qu2020weakly}.
However, some issues have not been addressed. 
First, existing methods typically derived pseudo labels from the point annotation, which are based on strong constraints or assumptions (e.g., Voronoi) from the data, restricting the generalization of these methods to other datasets~\cite{lin2023nuclei}.
Second, due to the lack of explicit boundary supervision, there is a non-negligible performance gap between the weakly supervised methods with points and the fully supervised methods.

\subsubsection{Scribble Annotation}\label{sec:anno_scribble}
% ---------------------------------------------------------------
Scribble annotation, a set of scribbles drawn on an image by the annotators, has been recognized as a user-friendly alternative to bounding box annotation~\cite{tajbakhsh2020embracing}. 
Compared with point annotation, it provides the rough shape and size information of the objects, which is promising to improve the segmentation performance, especially for objects with complex shapes.
Wang \textit{et al.}~\cite{wang2018interactive} propose a self-training framework with differences in model predictions and user-provided scribbles. 
Can \textit{et al.}~\cite{can2018learning} develop a random walk algorithm that incrementally performs region growing method around the scribble ground truth, while
Lee \textit{et al.}~\cite{lee2019scribble2label} introduce Scribble2Label, a method that integrates the supervision signals from both scribble annotations and pseudo labels with the exponential moving average. 
Furthermore, Dorent \textit{et al.}~\cite{dorent2020scribble} extend the Scribble-Pixel method to the domain adaptation scenario, where a new formulation of domain adaptation is proposed based on CRF and co-segmentation with the scribble annotation. 
Zhang \textit{et al.}~\cite{zhang2022cyclemix} adopt mix augmentation and cycle consistency for the Scribble-Pixel method, demonstrating the improvement of both weakly and fully supervised segmentation methods.
Zhou \textit{et al.}~\cite{zhou_weakly_2023} proposed a scribble-supervised approach that combines superpixel-guided scribble walking with class-wise contrastive regularization to augment the structural priors into the weak annotations.

\subsubsection{Box Annotation}\label{sec:anno_box}

Box annotation encloses the segmented region within a rectangle, and various studies have focused on this Box-Pixel scenario. 
Rajchl \textit{et al.}~\cite{rajchl2016deepcut} employ a densely-connected random field (DCRF) with an iterative optimization method for MRI segmentation. 
Wang \textit{et al.}~\cite{wang2021accurate,wang2021bounding} adopt smooth maximum approximation based on the bounding box tightness prior~\cite{hsu2019weakly}, that is, an object instance should touch all four sides of its bounding box. 
Thus, a vertical or horizontal crossing line within a box yields a positive bag because it covers at least one foreground pixel. 
Studies~\cite{wang2021bounding} demonstrate that the Box-Pixel method yields promising performance, being only 1--2\% inferior to the fully supervised methods.
In a recent work, Wong \textit{et al.}~\cite{wong_scribbleprompt_2025} proposed ScribblePrompt, which is flexible to different annotation styles, including bounding boxes, points, and scribbles. Overall, box annotation offers a strong balance between efficiency and accuracy, but may struggle with irregular or overlapping shapes.

\subsection{Multi-instance Learning}

To address label granularity alignment from the task perspective, \textbf{multi-instance learning (MIL)} enables deep learning models to make fine-grained predictions using only coarse, image-level supervision. MIL organizes each image or specimen as a \textit{bag} of multiple \textit{instances} (e.g., patches, regions, or cells), with only the bag-level label observed during training. Under the standard MIL assumption, a bag is positive if at least one of its instances is positive. This framework supports weak supervision and allows for fine-grained analysis—such as localizing disease regions—even when only image-level labels are available.

Distinct from approaches that primarily calibrate the form of supervision, MIL is specifically tailored to bridge the granularity gap between global supervision and dense prediction requirements. By modeling the instance collections within each image, MIL enables inference of instance-level relevance from bag labels, thereby supporting fine-grained pattern recognition using only bag-level supervision.

In this survey, we categorize MIL approaches by their prediction target: \textbf{local-context MIL} focuses on detecting and classifying patterns at the instance level, while \textbf{global-context MIL} aims for bag-level predictions. Representative works are summarized in Appendix Tab.~\ref{tab:mil}.

\subsubsection{Local-Context MIL}
Within the MIL paradigm, local-context MIL approaches focus on the fine-grained identification and localization of specific disease patterns within medical images. These methods are designed to infer instance-level labels for individual patches, enabling precise delineation of pathological regions. By determining the status of each constituent patch, local-context MIL naturally encompasses global-context capabilities—the image-level classification emerges as a function of the detected local annotations. This hierarchical inference process allows clinicians not only to receive diagnostic outcomes but also to visualize the spatial distribution of disease manifestations, enhancing interpretability and clinical utility. The ability to simultaneously perform localization and diagnosis makes local-context MIL particularly valuable in applications requiring both detection sensitivity and anatomical precision.

Schwab \textit{et al.} \cite{schwab2020localization} apply MIL to chest X-ray classification and localization by processing image patches through a CNN to assess their probability of containing critical findings. While traditional MIL approaches integrate patch-level predictions using max-pooling or average-pooling, Couture \textit{et al.} \cite{couture2018multiple} enhance this by implementing a quantile function for pooling, providing better characterization of sample heterogeneity. The field has evolved toward learning-based aggregation methods, notably the attention-based MIL developed by Ilse \textit{et al.} \cite{ilse2018attention}, which better captures and interprets regions of interest. Wang \textit{et al.} \cite{wang2021learning} extend this concept with an inductive attention guidance network for pancreatic ductal adenocarcinoma, where the attention mechanism bridges global classification and local segmentation by identifying relevant regions.

Other intriguing improvements in local-context MIL are springing up as well. Researchers have tried many different ways to facilitate instance prediction \cite{dov2021weakly,manivannan2017subcategory,jia2017constrained,xu2019camel}. Dov \textit{et al.} \cite{dov2021weakly} address cytopathology challenges—where informative instances are sparse and exhibit varied abnormalities—by implementing maximum likelihood estimation to simultaneously predict bag-level labels, diagnostic scores, and instance-level labels. For retinal nerve fiber layer classification, Manivannan \textit{et al.} \cite{manivannan2017subcategory} overcome strong intra-class variation by mapping instances into a discriminative subspace that enhances feature disentanglement. Jia \textit{et al.} \cite{jia2017constrained} incorporate multi-scale image features to extract more latent information from histopathology images. Addressing the limitation of image-level-only labels in MIL, Xu \textit{et al.} \cite{xu2019camel} develop an automatic instance-level label generation method, creating a promising direction for local-context MIL algorithm development.

In parallel, there has been significant progress in related domains such as phenotype categorization \cite{yao2020whole, hashimoto2020multi, yao2019deep} and multi-label classification \cite{mercan2017multi}. These investigations have further exemplified the versatility and potential of the MIL algorithm in addressing complex challenges across various subfields. 

\subsubsection{Global-Context MIL}
In contrast to the localization focus of local-context approaches, global-context MIL aims to detect whether or not target patterns exist. For example, for the COVID-19 screening problem, researchers \cite{li2021novel} have designed MIL algorithms to classify an input sample as severe or not instead of locating every abnormal patch. 

To facilitate the prediction of image-level labels (\textit{e.g.} WSI-level label in the context of computational pathology), researchers normally start from one of two aspects, namely instance- and bag-level. Most existing MIL algorithms \cite{tomita2019attention, hashimoto2020multi,naik2020deep,lu2021data} are based on the basic assumption that instances of the same bag are independent and identically distributed. Consequently, the correlations among instances are neglected, which is not realistic. Several subsequent works have taken the correlation among instances or tissues into consideration \cite{shao2021transmil,wang2022lymph,wang2019rmdl,raju2020graph,han2020accurate}. In \cite{shao2021transmil}, Shao \textit{et al.} introduce Vision Transformer (ViT) into MIL for gigapixel WSIs due to its great advantage in capturing the long-distance information and correlation among instances in a sequence. Meanwhile, to conduct precise lymph node metastasis prediction, Wang \textit{et al.} \cite{wang2022lymph} not only incorporate a pruned Transformer into MIL but also develop a knowledge distillation mechanism based on other similar datasets, such as a papillary thyroid carcinoma dataset, effectively avoiding the overfitting problem caused by the insufficient number of samples in the original dataset. Similarly, Raju \textit{et al.} \cite{raju2020graph} design a graph attention MIL algorithm for colorectal cancer staging, which utilizes different tissues as nodes to construct graphs for instance relation learning. Further, in order to utilize the multi-resolution characteristics of WSIs, Shi \textit{et al.} \cite{shi2023structure} consider WSIs as multi-scale graphs and utilize attention mechanism to integrate their information for primary tumor stage prediction. Similar idea can be found in \cite{yan2023genemutation, shi2023mg, xiang2023multi}. Besides, Liu \textit{et al.} \cite{liu2024advmil} firstly propose an integration of GAN with MIL mechanism for robust and interpretable WSI survival analysis by more accurately estimating target distribution. 

For bag-level improvement, recent years have witnessed two feasible approaches, namely, improved pooling methods and pseudo bags. On the one hand, in order to aggregate the instances with the most information, some researchers have developed novel aggregation methods in MIL algorithms instead of the traditional max pooling \cite{chikontwe2020multiple,das2018multiple,jin2024hmil,guo2024focus}. For example, in \cite{chikontwe2020multiple}, the authors design a pyramid feature aggregation method to directly obtain a bag-level feature vector. On the other hand, however, there is an inherent problem for MIA, especially for histopathology --- the number of WSIs (bags) is usually small, while in contrast, one WSI has numerous patches, leading to an imbalance in the number of bags and instances. To address this problem, Zhang \textit{et al.} \cite{zhang2022dtfd} randomly split the instances of a bag into several smaller bags, called "pseudo bags", with labels that are consistent with the original bag. A similar idea can also be seen in \cite{li2021novel}. Moreover, Jin \textit{et al.} \cite{jin2024hmil} introduce a hierarchical multi-instance learning (HMIL) framework that enhances WSI classification through explicit modeling of the hierarchical relationships between instance-level and bag-level label distributions, creating a more cohesive cross-granularity learning paradigm.

Other improvements in MIL algorithms are also worth mentioning \cite{su2022attention2majority,tennakoon2019classification,wang2020ud}. In \cite{su2022attention2majority}, a novel sampling method is developed to collect instances with high confidence. This method excludes patches shared among different classes and tends to select the patches that match with the bag-level label. In \cite{tennakoon2019classification}, the authors utilize the extreme value theory to measure the maximum feature deviations and consequently leverage them to recognize the positive instances, while in \cite{wang2020ud}, Wang \textit{et al.} introduce an uncertainty evaluation mechanism into MIL for the first time, and train a robust classifier based on this mechanism to cope with OCT image classification problem. 

\subsection{Discussion}
Weakly supervised learning addresses the challenges of limited and inexact annotations in MIA by calibrating both the annotation form and granularity of supervision. Within this paradigm, annotation-efficient learning and MIL represent two complementary facets for coping with weak supervision.

Annotation-efficient learning leverages diverse annotation types—points, scribbles, boxes, and image tags—each suited to different object characteristics and annotation budgets. Points and scribbles are efficient for objects with uniform shape, while boxes handle morphological variation, and image tags offer the lowest annotation cost at the expense of spatial detail. Recent advances in HFMs, especially vision-language models, have enabled high-quality predictions from minimal supervision by integrating multi-modal cues and prompt-based learning~\cite{li2023blip,ren2024grounded}. Future directions include unifying multiple weak supervision signals, leveraging human-in-the-loop strategies, and mining knowledge from multi-modal data to further reduce annotation costs.

Multi-instance learning (MIL) addresses the calibration of annotation granularity, particularly for tasks such as gigapixel-sized WSI analysis, where only bag-level labels are available. MIL enables fine-grained, instance-level inference by modeling bag-instance and instance-instance relationships, effectively aligning global supervision with the need for precise localization or characterization. The rise of HFMs trained on large and diverse datasets has significantly enhanced MIL by providing rich, transferable features, boosting performance without extra annotation~\cite{vorontsov2024foundation,chen2024towards,lu2024visual,ma2024towards,xu2024multimodal,xu2024whole}. Key future directions include developing more interpretable and privacy-preserving MIL frameworks~\cite{javed2022additive,kapse2024si}, designing efficient attention mechanisms for high-dimensional features~\cite{guo2024histgen,li2024rethinking,tang2024feature,guo2024focus}, and integrating foundation models with domain-specific knowledge~\cite{zhang2023textadaptation,yin2024prompting,lu2024pathotune}.