\section{Insufficient Label} \label{sec:semi}
The \textit{insufficient label} scenario arises when only a small portion of available medical imaging data is annotated, while the majority remains unlabeled. This scenario is prevalent in clinical settings, where expert annotations are costly and time-consuming, yet raw images are readily accessible. In such cases, supervised learning alone proves inadequate due to limited labeled data. 
\input{Figures/fig_semi_schematic.tex}
As illustrated in Fig.~\ref{fig_semi_schematic}, \textbf{semi-supervised learning (Semi-SL)} addresses this challenge by leveraging both labeled and unlabeled data during training. A core principle of Semi-SL is supervision propagation, which assumes that unlabeled data should have predictions consistent with labeled data. In this way, Semi-SL enables better generalization while reducing the need for manual annotation.

In the following, we categorize existing Semi-SL methods in MIA how each method enforces or approximates supervision consistency between labeled and unlabeled data into three categories: \textbf{proxy-labeling}, \textbf{generative modeling}, and \textbf{regularization}. Representative works are summarized in Appendix Tab.~\ref{tab:semi}.

\subsection{Proxy-labeling Methods}
\textbf{Proxy-labeling methods} utilize the idea of supervision consistency propagation by assigning pseudo labels to unlabeled samples and incorporate high-confidence examples into the training process through an iterative approach. These methods can be divided into two principal subcategories: \textit{Self-training methods} and \textit{multi-view learning methods}.

\subsubsection{Self-training Methods}

\textit{Self-training methods} operate through the bootstrapping mechanism. Initially, a prediction function $f_{\theta}$ with parameters $\theta$ is trained using available labeled data samples $x\in X_L$. Subsequently, this trained model generates predictions for unlabeled data samples $x\in X_U$. A confidence threshold $\tau$ is established, and sample-label pairs $(x, \mathrm{argmax}{f_{\theta}(x)})$ whose prediction confidence exceeds $\tau$ are added to the labeled dataset $X_L$. This augmented labeled dataset is then used to retrain the prediction function, creating an iterative cycle that continues until the model can no longer make sufficiently confident predictions on remaining unlabeled data.

Entropy minimization \cite{grandvalet2004semi} represents a foundational approach in this category, regularizing models based on the low-density assumption by encouraging low-entropy predictions for unlabeled data. Building on this concept, \textbf{Pseudo-label} \cite{lee2013pseudo} provides a straightforward yet effective self-training mechanism that applies entropy minimization principles in prediction space. While labeled samples undergo supervised training, unlabeled data receive labels corresponding to the model's most confident predictions. In medical image analysis, Pseudo-label has been widely employed as an auxiliary component to enhance model performance across various applications \cite{fan2020inf,zhang2022boostmis,chaitanya2023local}.

A significant challenge with proxy labels is their inherent noise and potential deviation from ground truth. To address this limitation, researchers have developed quality assurance mechanisms including uncertainty-aware confidence evaluation \cite{wang2021semiself}, conditional random field-based proxy label refinement \cite{bai2017semi}, and adversarial training-based methods \cite{zhou2019collaborative}. These approaches help ensure that proxy labels provide reliable supervisory signals during training. Pseudo-label has also been used in MIA to refine a given annotation with the assistance of unlabeled data. Qu \textit{et al.} \cite{qu2020weakly} introduce pseudo-label into nuclei segmentation and design an iterative learning algorithm to refine the background of weakly labeled images where only nuclei are annotated, leaving large areas ignored. Similar ideas can also be seen in \cite{nie2018asdnet}.


\subsubsection{Multi-view learning methods}
\textit{Multi-view learning methods} assume that each sample has two or multiple complementary views and features of the same sample extracted with different views are supposed to be consistent. Therefore, the key idea of multi-view learning methods is to train the model with multiple views of the sample or train multiple learners and minimize the disagreement between them, thus learning the underlying features of the data from multiple aspects. \textbf{Co-training} is a method that falls into this category. It assumes that data sample $x$ can be represented by two views, $\textbf{v}_1(x)$ and $\textbf{v}_{2}(x)$, and each of them are capable of solely training a good learner, respectively. Consequently, the two learners are set to make predictions of each view's unlabeled data, and iteratively choose the candidates with the highest confidence for the other model \cite{yang2021survey}. 
Another variation of multi-view learning methods is Tri-training \cite{zhou2005tri}, which is proposed to tackle the lack of multi-view data and mistaken labels of unlabeled data produced by self-training methods. Tri-training aims to learn three models from three different training sets obtained with bootstrap sampling. A deep learning version of Tri-training, i.e. Tri-Net, has been further proposed in \cite{dong2018tri}.

Co-training, or deep co-training, is dominant in multi-view learning in MIA, with a steady flow of publications \cite{zhao2019multi,zhou2019semi,xia2020uncertainty,wang2021selfco,fang2020dmnet,zeng2023pefat}. To conduct whole brain segmentation, Zhao \textit{et al.} \cite{zhao2019multi} implements co-training by obtaining different views of data with data augmentation. A similar idea can be seen for 3D medical image segmentation in \cite{xia2020uncertainty} and \cite{zhou2019semi}. These two works both utilize co-training by learning individual models from different views of 3D volumes such as the sagittal, coronal, and axial planes. Further works have been proposed to refine co-training. To produce reliable and confident predictions, Wang \textit{et al.} \cite{wang2021selfco} develops a self-paced learning strategy for co-training, forcing the network to start with the easier-to-segment regions and transition to the difficult areas gradually. Rather than discarding samples with low-quality pseudo-labels, Zeng \textit{et al.} \cite{zeng2023pefat} introduces a novel regularization approach, which focuses on extracting discriminative information from such samples by injecting adversarial noise at the feature level, thereby smoothing the decision boundary.
Meanwhile, to avoid the errors of different model components accumulating and causing deviation, Fang and Li \cite{fang2020dmnet} develop an end-to-end model called difference minimization network for medical image segmentation by conducting co-training with an encoder shared by two decoders.

\subsection{Generative Modeling Methods}
While proxy-labeling methods directly assign labels to unlabeled data, \textbf{generative modeling methods} realize supervision consistency propagation by assuming that both labeled and unlabeled data are sampled from a shared latent distribution. By learning this underlying distribution with the help of unlabeled data, these methods enable the model to transfer information across the entire dataset. The learned latent representation is then combined with supervised information from labeled examples to further improve performance.

% Concise version
\textbf{Generative adversarial networks (GANs)} effectively leverage both labeled and unlabeled data through a two-player minimax game between a generator $\mathcal{G}$ and discriminator $\mathcal{D}$ \cite{goodfellow2014generative}. In medical image analysis, several semi-supervised approaches incorporate unlabeled data during adversarial training. Chaitanya \textit{et al.} \cite{chaitanya2021semi} and Hou \textit{et al.} \cite{hou2022semi} utilize unlabeled samples to introduce greater variation in shape and intensity, enhancing model robustness. Zhou \textit{et al.} \cite{zhou2019collaborative} generate pseudo lesion masks for unlabeled data with quality facilitated by the discriminator. Other researchers modify the discriminator's objective beyond binary classification: Odena \textit{et al.} \cite{odena2016semi} extend it to predict $K$ classes plus an additional real/fake class, allowing unlabeled data to contribute to multi-class discrimination. This architecture has been successfully applied to retinal image synthesis \cite{kamran2021vtgan, diaz2019retinal, xie2023fundus}, glaucoma assessment \cite{diaz2019retinal}, chest X-ray classification \cite{madani2018semi}, and other medical imaging tasks \cite{hou2022semi}.

\textbf{Variational autoencoders (VAEs)} offer another effective approach for utilizing unlabeled data. Based on Bayesian inference theory \cite{kingma2013auto}, VAEs encode data into latent variables and reconstruct inputs by maximizing the variational lower bound. In medical image analysis, VAEs primarily learn feature similarities from large unlabeled datasets, creating well-constrained latent spaces that reduce dependence on labeled data \cite{sedai2017semi, wang2022rethinking}. Sedai \textit{et al.} \cite{sedai2017semi} proposed a dual-VAE framework for semi-supervised optic cup segmentation in retinal images, where one VAE learns data distribution from unlabeled data and transfers this knowledge to a second VAE performing segmentation with labeled data. Wang \textit{et al.} \cite{wang2022rethinking} adapted VAEs for 3D medical image segmentation by replacing the conventional mean vector and variance vector with a mean vector and covariance matrix, accounting for correlations between different slices of an input volume.

More recently, \textbf{diffusion models} \cite{ho2020denoising,yang2022diffusion,rombach2022high} have emerged as powerful alternatives in the generative Semi-SL domain. These models offer enhanced stability and sample quality through iterative denoising processes, showing potential in midline shift quantification \cite{gong2023diffusion} and medical image segmentation \cite{liu2024diffrect}. Their ability to model complex anatomical structures while enabling uncertainty quantification makes them particularly valuable for label-scarce scenarios.


\subsection{Regularization-based Methods}
In contrast to the explicit labeling approach of proxy methods and the distribution modeling of generative techniques, regularization-based methods enforce consistency through direct constraints on the model's behavior by assuming that the perturbation of data points does not change the prediction of the model, without requiring any label information. 

$\boldsymbol{\Pi}$\textbf{-model} \cite{sajjadi2016regularization} effectively implements consistency regularization by using a shared encoder to process differently augmented views of the same input and enforcing consistent predictions across these views, while incorporating label information to improve classifier performance. Li \textit{et al.} \cite{li2018semipi} achieved state-of-the-art skin lesion segmentation using this approach with only 300 labeled images, outperforming fully-supervised methods that required 2,000 labeled images. Similar consistency-based approaches appear in Bortsova \textit{et al.} \cite{bortsova2019semi}, who enforce prediction consistency across transformations for chest X-ray segmentation, and Meng \textit{et al.} \cite{meng2023dual}, who employ graph convolution networks to maintain regional and marginal consistency for semi-supervised optic disc and cup segmentation.

\textbf{Temporal ensembling} \cite{laine2016temporal} improves the $\Pi$-model's prediction stability by incorporating exponential moving averages, an approach widely adopted in medical image analysis \cite{cao2020uncertainty,gyawali2019semi,shi2020graph,luo2020deep}. For breast mass segmentation, Cao \textit{et al.} \cite{cao2020uncertainty} integrate uncertainty maps as guidance to ensure prediction reliability. Similarly, Luo \textit{et al.} \cite{luo2020deep} propose uncertainty-aware temporal ensembling for chest X-ray screening with partially labeled data. Gyawali \textit{et al.} \cite{gyawali2019semi} enhance the method by first using a VAE to extract disentangled latent space representations as stochastic embeddings, improving chest X-ray classification performance. A key characteristic of temporal ensembling is that each training sample's activation is updated only once per epoch.
 
\textbf{Mean teacher} \cite{tarvainen2017mean} applies exponentially moving average to model parameters rather than network activations, addressing the limitations of temporal ensembling and finding various applications in medical imaging \cite{li2020transformation,yu2019uncertainty,wang2020double,xu2023ambiguity,adiga2023anatomically}. Li \textit{et al.} \cite{li2020transformation} apply this approach to transformation-consistent medical image segmentation. Since teacher models can generate inaccurate targets for unlabeled data, Yu \textit{et al.} \cite{yu2019uncertainty} and Adiga \textit{et al.} \cite{adiga2023anatomically} incorporate uncertainty maps to ensure target reliability. Wang \textit{et al.} \cite{wang2020double} further propose a double-uncertainty-weighted method for left atrium and kidney segmentation, extending uncertainty from segmentation to feature level. Xu \textit{et al.} \cite{xu2023ambiguity} focus on selecting productive unsupervised consistency targets through an ambiguity-consensus mean-teacher model that better exploits complementary information from unlabeled data.

\subsection{Discussion}
Semi-supervised learning addresses the scarcity of labeled data by exploiting large amounts of unlabeled samples and enforcing supervision consistency across the dataset. 
% Newly Added
Data characteristics and task requirements should guide the choice of the Semi-SL strategy: proxy-labeling methods like self-training tend to perform well when high-confidence predictions can be reliably identified; multi-view learning approaches appear particularly suited for volumetric data where different perspectives provide complementary information; generative modeling shows promise with complex anatomical structures that benefit from learned prior distributions; while regularization-based methods often demonstrate robustness across diverse imaging modalities.
%
A persistent challenge in Semi-SL lies in the utilization of noisy or imperfect unlabeled data. The generation and selection of reliable pseudo labels are critical, as label noise can easily propagate through the training process and undermine model performance. Moreover, the theoretical understanding of how different Semi-SL techniques interact within hybrid systems remains limited, especially when dealing with heterogeneous data sources~\cite{wang2021deephybrid, zhang2022boostmis, wang2020focalmix, gyawali2020semi,miao2023caussl}. 

Meanwhile, the emergence of HFMs has significantly reshaped the landscape of semi-supervised learning in medical image analysis. As demonstrated by approaches like SemiSAM \cite{zhang2023semisam} and SemiSAM+ \cite{zhang2025semisam+}, these models introduce a paradigm shift from traditional model-centric Semi-SL methods focused on regularization strategies toward leveraging pre-trained knowledge to guide the learning process. Foundation models trained on large-scale datasets provide robust prior knowledge that helps specialist models learn more effectively with extremely limited labeled dataâ€”a scenario where conventional Semi-SL methods often struggle. This collaborative learning approach, where trainable specialist models interact with frozen foundation models, offers several advantages: it enhances performance in low-annotation regimes, provides more stable training due to knowledge transfer, and exhibits strong generalization capabilities across different medical imaging modalities and targets. As HFMs continue to evolve with improved architectures and more diverse training data, they will likely further transform Semi-SL in MIA, potentially reducing annotation requirements while increasing effectiveness and robustness.