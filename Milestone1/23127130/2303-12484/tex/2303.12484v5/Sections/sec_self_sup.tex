\section{No Label} \label{sec:ssl}
The \textit{no label} scenario represents one of the most challenging yet prevalent settings in MIA, where large volumes of imaging data are available but no annotations exist. This commonly arises in applications involving novel modalities, rare diseases, or legacy datasets lacking corresponding labels. In the absence of ground truth, conventional supervised learning becomes inapplicable. To address this, \textbf{self-supervised learning (Self-SL)} has emerged as an effective solution. Self-SL derives supervisory signals directly from the data itself by designing pretext tasks that exploit the inherent structural and semantic patterns within medical images. As illustrated in Fig.~\ref{fig_self_schematic}, Self-SL enables models to learn meaningful and transferable representations from unlabeled data through automatically generated supervision signals. Typically, the workflow involves first pretraining a model on large-scale unlabeled data using self-supervised objectives, followed by fine-tuning on a labeled dataset. This strategy not only facilitates data-specific feature learning but also helps reduce overfitting by leveraging the abundance of unlabeled data. Moreover, recent advances have scaled up both the size of datasets and the capacity of models, giving rise to health foundation models (HFMs). These models have demonstrated strong diagnostic and prognostic performance, along with robust generalization across diverse medical imaging tasks. Based on the design of proxy tasks, we categorize Self-SL methods in MIA as \textbf{reconstruction-based}, \textbf{context-based}, and \textbf{contrastive-based} approaches. Representative approaches within each category are summarized in Appendix Table~\ref{tab:self}. 

\input{Figures/fig_self_schematic.tex}

\subsection{Reconstruction-Based Methods}

\textbf{Reconstruction-based methods} aim to learn semantic features by reconstructing input data, enabling representation learning without manual annotations. These methods encompass tasks such as super-resolution \cite{zhao2020smore,li2021single}, inpainting \cite{zhao2021anomaly}, colorization \cite{abbet2020divide}, and multi-modal reconstruction \cite{hervella2018retinal,cao2020auto}.

A basic form of reconstruction was introduced by Li \textit{et al.} \cite{li2020sacnn}, who trained an auto-encoder to reconstruct normal-dose CT images by minimizing mean squared error (MSE) loss. The encoder, pre-trained via self-supervision, was later reused for downstream tasks. However, reliance on pixel-wise losses may bias models toward low-level appearance features while overlooking structural semantics \cite{abbet2020divide}, prompting the development of more informative proxy tasks.

Super-resolution tasks generate high-resolution outputs from low-resolution inputs, forcing the model to learn fine-grained semantic structures. Zhao \textit{et al.} \cite{zhao2020smore} proposed an anti-aliasing super-resolution method for MRIs, while Li \textit{et al.} \cite{li2023generic} leveraged frequency information for fundus image enhancement. GAN-based super-resolution has also been applied to gigapixel histopathology WSIs \cite{li2021single}. These super-resolution approaches are particularly effective in domains where high-resolution details are critical for diagnosis, and they encourage the model to focus on subtle anatomical structures. However, they may also lead to overfitting to texture or noise if not properly regularized, and their effectiveness can be limited in cases where semantic content is not directly tied to resolution.

Colorization tasks predict RGB images from grayscale inputs, encouraging the model to capture structural and contextual cues. Abbet \textit{et al.} \cite{abbet2020divide} applied this to colorectal cancer survival analysis by converting images into hematoxylin and eosin channels. Fan \textit{et al.} \cite{fan2023cancerself} extended this approach with a cross-channel task that reconstructs lightness from color channels. Compared to pure pixel reconstruction, colorization requires the model to infer semantic correspondences between structure and color, thus promoting higher-level feature learning. Nonetheless, the colorization task may introduce ambiguity, as multiple plausible color mappings can exist for the same structure, potentially limiting the precision of the learned representations.

Inpainting tasks require models to restore missing regions, promoting understanding of object continuity and context. Zhao \textit{et al.} \cite{zhao2021anomaly} applied this strategy to OCT and chest X-rays, showing improved anatomical feature learning. Inpainting is especially useful for encouraging global context reasoning, as the model must integrate information from surrounding regions to plausibly reconstruct missing parts. However, if the occluded regions are too small or too easily inferred from context, the task may become trivial and fail to drive the model towards learning meaningful semantics.

Multi-modal reconstruction tasks aim to reconstruct one modality from another, facilitating joint representation learning. Hervella \textit{et al.} \cite{hervella2018retinal} combined retinography and angiography for retinal analysis, while Cao \textit{et al.} \cite{cao2020auto} proposed a collaborative learning framework using auto-encoders and GANs to synthesize missing modalities. These approaches are well-suited for leveraging complementary information across imaging modalities, which is common in clinical practice. Yet, the challenge remains in aligning heterogeneous data distributions and ensuring that the shared representations capture clinically relevant features rather than modality-specific artifacts.

Recent advances in diffusion models \cite{ho2020denoising,yang2022diffusion,rombach2022high} have further improved self-supervised reconstruction. These models, characterized by stability and generative fidelity, have been applied to medical imaging by Korkmaz \textit{et al.} \cite{korkmaz2023self}, Cho \textit{et al.} \cite{cho2023improved}, and Purma \textit{et al.} \cite{purma2024genselfdiff}, enabling robust feature learning from complex anatomical data. While diffusion models offer superior generative performance and can model complex data distributions, they typically require significant computational resources and their interpretability in clinical contexts remains an open question.

In summary, reconstruction-based self-supervised methods provide a flexible framework for learning rich representations from unlabeled medical images. Each proxy task emphasizes different aspects of image semantics—whether fine-grained structure, global context, or cross-modal relationships. However, these methods also face challenges, such as balancing low-level appearance with high-level semantics, handling ambiguous or ill-posed reconstruction targets, and ensuring clinical relevance of learned features. Going forward, integrating more task-aware proxy objectives and leveraging advances in generative modeling hold promise for further improving the effectiveness and applicability of reconstruction-based self-supervised learning in medical imaging.

\subsection{Context-Based Methods}
While reconstruction-based methods focus on pixel-level restoration, \textbf{context-based methods} take a different approach by exploiting the spatial and structural relationships within medical images. Researchers have explored novel predictive tasks for specific MIA tasks by training the network for prediction of the output class or localization of objects with the original image as the supervision signal \cite{bai2019self,spitzer2018improving,srinidhi2022self}. Bai \textit{et al.} \cite{bai2019self} propose a proxy task to predict the anatomical positions from cardiac chamber view planes by applying an encoder-decoder structure. This proxy task properly employs the chamber view plane information, which is available from cardiac MR scans easily. 
Zheng \textit{et al.} \cite{zheng2023msvrl} enhance medical image segmentation through multi-scale consistency objectives for finer-grained representation across different target scales. For 3D medical images, He \textit{et al.} \cite{he2023geometric} introduce Geometric Visual Similarity Learning, incorporating topological invariance prior to ensure consistent representation of semantic regions. Meanwhile, Srinidhi \textit{et al.} \cite{srinidhi2022self} propose Resolution Sequence Prediction for whole slide images (WSIs), training networks to predict the order of multi-resolution patches, thereby capturing both contextual structure at lower magnifications and local details at higher magnifications.

Other efforts have been made to explore the spatial context structure of input data, such as the order of different patches constituting an image, or the relative position of several patches in the same image, which can provide useful semantic features for the network. 
% Concise version
Chen \textit{et al.} \cite{chen2019self} introduce context restoration, where patch positions are randomly switched and then restored, enabling straightforward semantic feature learning. Li \textit{et al.} \cite{li2021rotation} employ rotation angle prediction with augmented retinal images, encouraging the model to predict rotation angles while clustering similar features. Advanced spatial reasoning tasks such as jigsaw puzzles and Rubik's Cube have gained traction in medical imaging. Taleb \textit{et al.} \cite{taleb2021multimodal} enhance Jigsaw Puzzles with multi-modal data, requiring models to restore original images from out-of-order patches of different modalities. For 3D medical data, Zhuang \textit{et al.} \cite{zhuang2019selfsupervised} and Tao \textit{et al.} \cite{tao2020revisiting} adapt Rubik's Cube by dividing volumes into cube grids, applying random rotations, and training networks to recover the volume.

However, for WSI images, common proxy tasks such as prediction of the rotation or relative position of objects may only provide minor improvements to the model in histopathology due to the lack of a sense of global orientation in WSIs \cite{graham2020dense,koohbanani2021self}. To address these limitations, Koohbanani \textit{et al.} \cite{koohbanani2021self} proposes proxy tasks targeted at histopathology, namely, magnification prediction, solving the magnification puzzle, and hematoxylin channel prediction. In this way, their model can significantly integrate and learn the contextual, multi-resolution, and semantic features inside the WSIs.

In summary, context-based self-supervised methods provide a versatile framework for capturing semantic and structural information in medical images. Their success largely depends on the careful design of proxy tasks that align with the intrinsic properties of the data and the requirements of downstream applications. Nonetheless, challenges remain in selecting context cues that are both informative and robust across diverse imaging modalities. Future work may focus on dynamic or adaptive task selection, as well as integrating multiple context signals to further enhance feature learning in MIA.

\subsection{Contrastive-Based Methods}
Beyond the pixel-space focus of reconstruction approaches and the spatial reasoning of context-based methods, \textbf{contrastive-based methods} operate on the principle that representations of different views of the same image should be similar, while those of different images should be distinguishable. Several high-performance contrastive learning algorithms originally developed for natural images, such as SimCLR \cite{chen2020simple} and BYOL \cite{grill2020bootstrap}, have been successfully adapted to medical image analysis \cite{azizi2021big,wang2021transpath}. Azizi \textit{et al.} \cite{azizi2021big} developed multi-instance contrastive learning (MICLe), extending SimCLR by minimizing disagreement between views from multiple images of the same patient, creating richer positive pair relationships. In parallel, Wang \textit{et al.} \cite{wang2021transpath} applied the BYOL architecture to histopathology image classification, making a substantial contribution by assembling the largest WSI dataset for Self-SL pre-training at the time—comprising 2.7 million patches from 32,529 WSIs spanning over 25 anatomic sites and 32 cancer subtypes.

Large-scale dataset utilization has emerged as a critical trend in contrastive-based methods. Ghesu \textit{et al.} \cite{ghesu2022self} developed a contrastive learning and online clustering algorithm leveraging over 100 million radiography, CT, MRI, and ultrasound images. This extensive pre-training yielded significant improvements in both performance and convergence rates compared to previous state-of-the-art approaches. Similarly, Nguyen \textit{et al.} \cite{nguyen2023lvm} integrated over 1.3 million multi-modal images from 55 publicly available datasets to enhance representation learning. Beyond multi-view approaches, Jiang \textit{et al.} \cite{jiang2023anatomical} introduced a specialized contrastive objective for learning anatomically invariant features, designed to exploit inherent similarities in anatomical structures across diverse medical imaging volumes.

Subsequent research has further refined contrastive learning by incorporating both global and local contrast for more comprehensive representation learning, typically using InfoNCE loss \cite{oord2018representation}.

Yan \textit{et al.} \cite{yan2022sam} apply this at the pixel level to generate embeddings that accurately describe anatomical locations, creating representations at both global and local scales. Building on this multi-level approach, Liu \textit{et al.} \cite{liu2023hierarchical} develop hierarchical contrastive learning for intra-oral mesh scans, capturing unsupervised representations across point-level, region-level, and cross-level interactions.

Compared to reconstruction- and context-based methods, contrastive learning is less reliant on proxy task design and better suited to leveraging large unlabeled datasets, often resulting in more robust and transferable representations. However, it faces challenges such as constructing meaningful positive/negative pairs and mitigating spurious correlations, especially in settings with limited patient diversity or multi-modal data. Looking ahead, advances in pair mining and domain-adaptive objectives are expected to further improve the effectiveness and clinical relevance of contrastive-based self-supervised learning in medical imaging.

\subsection{Discussion}
Self-SL methods aim to leverage unlabeled data to learn rich, transferable representations by designing effective proxy tasks. While many existing approaches directly adapt proxy tasks from natural image domains, the unique properties of medical images—such as CT, WSI, and MRI—necessitate tailored proxy task designs that account for domain-specific semantics and structures. 
% Newly Added
The choice of Self-SL strategy often depends on the imaging modality: reconstruction-based methods generally perform well for modalities with clear anatomical structure (like CT and MRI), contrastive-based approaches excel with high-dimensional histopathology data that benefits from instance discrimination, while context-based methods are particularly effective when spatial relationships are diagnostically significant. Researchers have also explored hybrid frameworks that combine multiple types of Self-SL tasks to capture diverse aspects of medical data (see \cite{tang2022self,zhou2021preservational,haghighi2020learning,yang2022cs} and references therein).

Looking forward, the design of proxy tasks that incorporate modality-specific information and multi-modal integration remains a promising direction, enabling models to disentangle and capture complementary features from different imaging sources. Large vision-language pre-trained models~\cite{zhou2023advancing,zhou2022generalized,park2023self} have recently shown great potential in chest X-ray analysis, attracting increasing research interest. Moreover, the field is witnessing a rapid scaling of both data and model size, exemplified by the emergence of HFMs~\cite{ma2024segment,lu2024visual,chen2024towards,vorontsov2024foundation}. For instance, MedSAM~\cite{ma2024segment} adapts the Segment Anything Model to medical segmentation by fine-tuning on 1.57 million image-mask pairs across 10 modalities, while CONCH~\cite{lu2024visual} extends contrastive learning to whole-slide images with over 1 million image-caption pairs. Similarly, large-scale pre-training efforts such as UNI~\cite{chen2024towards} and Virchow~\cite{vorontsov2024foundation} demonstrate the power of scaling, using hundreds of thousands to over a million whole-slide images to learn generalizable representations. These trends highlight the increasing benefits of leveraging larger and more diverse datasets, as well as more powerful model architectures, to capture the complex patterns inherent to medical imaging.