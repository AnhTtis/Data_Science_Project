\section{Challenges and Future Directions} \label{sec:cnfd}
Our comprehensive discussion of label-efficient learning schemes in MIA raises several challenges that should be taken into account to improve the performance of the DL model. In this section, we describe the crucial challenges and shed light on potential future directions for solving these challenges.

\subsection{Omni-Supervised Learning} 
Although the methods we have presented have achieved promising performance, many of them are targeted at addressing \textit{ad hoc} label shortage problems, \textit{i.e.}, these methods do not utilize as much supervision as possible.
Served as a special regime of Semi-SL, \textbf{Omni-supervised learning} is a crucial trend for label-efficient learning in MIA for the simultaneous utilization of different forms of supervision. Studies \cite{luo2021oxnet, chai2022orf} have demonstrated the feasibility of omni-supervised learning under teacher-student\cite{tarvainen2017mean} and the dynamic label assignment \cite{chai2022orf} pipeline, respectively. In the teacher-student training approach, the model trained on fully annotated datasets serves as the teacher model, and features extracted from the weakly-/un-annotated datasets serve as guidance to refine the model. 
Through designated mechanisms, the student model utilizes the teacher model with the provided guidance to further improve performance.  Meanwhile, the dynamic label assignment approach forms the crafted metric from different types of labels in the training process and dynamically gives the final predicted labels. 

During the process of omni-supervised learning, however, centralizing or releasing different supervision health data raises multiple ethical, legal, regulatory, and technological issues \cite{rieke2020future}. On the one hand, collecting and maintaining a high-quality medical dataset consumes a large amount of expense, time, and effort. 
On the other hand, the privacy of patients may be compromised during the centralization or release of health datasets, even with techniques such as anonymization and safe transfer. To address the privacy preservation problem during model development, researchers proposed \textbf{federated learning (FL)} to conduct training in a data-decentralized manner. 
This approach has yielded fruitful results in The field of MIA \cite{dayan2021federated,li2020multi,lu2022federated}. However, current FL algorithms are primarily trained in a supervised manner. When applying the FL to real-world scenarios in MIA, a crucial problem, namely, label deficiency, may appear in local health datasets.
Labels may be missing to varying degrees between medical centers, or the granularity of the labels will vary. A promising research direction is to design label-efficient federated learning methods to address this significant problem. For example, semi-supervised learning\cite{liu2021federated}, active learning, and self-supervised learning \cite{dong2021federated} are suitable to be incorporated into this setting.

\subsection{Human-in-the-loop Interaction}
The application of expert knowledge to refine the output of the model is often carried out in practice, and there have been various efforts to investigate this field, known as human-in-the-loop (HITL). The AL scheme can be considered a part of HITL as it involves the introduction of expert knowledge to refine data supervision. Meanwhile, expert knowledge can also be introduced as action supervision under the \textbf{reinforcement learning (RL)} schemes to improve the performance of the DL model \cite{liao2020iteratively, ma2020boundary}. In RL scheme, a set of “agents” is formulated to learn expert behaviors in an interactive environment via trial and error. In MIA tasks, RL methods mainly treat the interactive refinement process as the Markov decision process (MDP) and give the solution by the RL process. RL-based interventional model training brings the potential for dealing with rare cases in MIA, since the expert-provided interactions can refine the prediction result at the final stage to hinge samples that failed to process by the DL model.
In addition, recent developments in diverse learning methodologies, including but not limited to few-shot learning \cite{feng2021interactive, al2021ifss} and interpretability-guided learning \cite{mahapatra2021interpretability}, have contributed to improved efficacy of human-in-the-loop workflows, thereby reducing labor costs in MIA. This indicates a positive trend towards increased cost-effectiveness in this field.

\subsection{Generation-based Data Augmentation}
Data augmentation with synthesized images produced by generative-based methods is regarded as a way to unlock additional information from the dataset, and leads the way in computation speed and quality of results in the scope of generative methods~\cite{shorten2019survey}.
In the field of MIA, numerous studies~\cite{wang2020semi,lin2022insmix} have investigated data augmentation with the original GAN~\cite{goodfellow2014generative}  and its variations. 
 However, the unique adversarial training procedure of GANs may suffer from training instability~\cite{gulrajani2017improved} and mode collapse~\cite{lin2018pacgan}, yielding ``Copy GAN", which only generates a limited set of samples~\cite{yang2019bi}.
Thus, synthesizing augmented data with a high visual realism and diversity is the key challenge of GAN.
Meanwhile, the \textbf{probabilistic diffusion model} \cite{ho2020denoising}, has recently sparked much interest in MIA applications\cite{kazerouni2022diffusion}. 
This model establishes a forward diffusion stage in which the input data is gradually disrupted by adding Gaussian noise over multiple stages and then learns to reverse the diffusion process to obtain the required noise-free data from noisy data samples. 
Despite their recognized computational overhead \cite{xiao2022tackling}, diffusion models are generally praised for their high mode coverage and sample quality, and various of efforts have been made to ease the computational cost and further improve their generalization capability.

\subsection{Generalization Across Domains and Datasets}
From semi-supervised learning to annotation-efficient learning, we have introduced a considerable number of methods that address the problem of the low-quantity and/or -quality of labels. Nevertheless, recent results reveal that these novel methods may encounter significant performance degradation when shifting to different domains or datasets. The generalization problem in MIA field arises due to multiple causes, such as variance among scanner manufacturers, scanning parameters, and subject cohorts. And various current deep learning algorithms cannot be robustly deployed in various real scenarios. To address this practical problem, the concept of \textbf{domain generalization} has been introduced, of which the key idea is to learn a trained model that encapsulates general knowledge so as to adapt to unseen domains and new datasets with little effort and cost. A plethora of methods have been developed to tackle the domain generalization problem \cite{zhou2022domain}, such as domain alignment \cite{li2018domain}, meta-learning \cite{li2019feature}, data augmentation \cite{qiao2020learning} and so on. MIA has also seen some publications with respect to domain generalization \cite{li2020domain,liu2021feddg}. Further, another challenge for generalization across domains and datasets is that the proposed methods may require numerous labeled multi-source data to extract domain-invariant features. For example, Yuan \textit{et al.} \cite{yuan2022label} have made a successful attempt to achieve model generalization in source domains with limited annotations by leveraging active learning and semi-supervised domain generalization, eliminating the dilemma between domain generalization and expensive annotations.

\subsection{Benchmark Establishment and Comparison}
Label-efficient learning in MIA spans multiple tasks, such as classification, segmentation, and detection, as well as multiple organs, such as the retina, lung, and kidney. Differences and variances in tasks and target organs lead to confounding experiment settings and unfair performance comparisons. Meanwhile, a lack of sufficient public health datasets also contributes to this dilemma. For example, many researchers can only conduct experiments to measure the performance of their proposed algorithms based on their own private datasets due to reasons such as privacy. However, few publications have emerged \cite{gut2022benchmarking} to address the problem, especially for label-efficient learning. Thus, benchmarkng remains a pressing problem for model evaluation. On the one hand, the public should urge for the availability of large datasets. On the other hand, a clearly defined set of benchmarking tasks and the corresponding evaluation procedures should be established. Further, specific experimental details should be stipulated to facilitate the comparability of different label-efficient learning algorithms.