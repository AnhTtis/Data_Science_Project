%% This is file `medima-template.tex',
%% 
%% Copyright 2018 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%%
%% $Id: medima-template.tex 153 2018-12-01 11:38:32Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsarticle/trunk/medima-template.tex $
%%
%% Use the option review to obtain double line spacing
%\documentclass[times,review,preprint,authoryear]{elsarticle}

%% Use the options `twocolumn,final' to obtain the final layout
%% Use longtitle option to break abstract to multiple pages if overfull.
%% For Review pdf (With double line spacing)
%\documentclass[times,twocolumn,review]{elsarticle}
%% For abstracts longer than one page.
%\documentclass[times,twocolumn,review,longtitle]{elsarticle}
%% For Review pdf without preprint line
%\documentclass[times,twocolumn,review,nopreprintline]{elsarticle}
%% Final pdf
\documentclass[times,twocolumn,final]{elsarticle}
%%
%\documentclass[times,twocolumn,final,longtitle]{elsarticle}
%%


%% Stylefile to load MEDIMA template
\usepackage{medima}
\usepackage{framed,multirow}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{captcont}
% \usepackage[authoryear]{natbib}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{latexsym}

% Following three lines are needed for this document.
% If you are not loading colors or url, then these are
% not required.
\usepackage{url}
\usepackage{xcolor}

\usepackage{hyperref}

\definecolor{newcolor}{rgb}{.8,.349,.1}

\journal{Medical Image Analysis}

\begin{document}

\verso{Cheng Jin \textit{et~al.}}

\begin{frontmatter}

\title{Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions}
% \tnotetext[tnote1]{This is an example for title footnote coding.}

\author[1]{Cheng \snm{Jin}\fnref{fn1}}
%\ead{author3@author.com}
\author[1]{Zhengrui \snm{Guo}\fnref{fn1}}
\fntext[fn1]{Equal contribution.}
\author[1]{Yi \snm{Lin}}
\author[1]{Luyang \snm{Luo}}
\author[1,2,3]{Hao \snm{Chen}\corref{cor1}}
\cortext[cor1]{Corresponding author: Hao Chen (jhc@cse.ust.hk)}


\address[1]{Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong}
\address[2]{Department of Chemical and Biological Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong}
\address[3]{HKUST Shenzhen-Hong Kong Collaborative Innovation Research Institute, Futian, Shenzhen, China}


\received{20 December 2023}
%\finalform{10 May 2013}
%\accepted{13 May 2013}
%\availableonline{15 May 2013}
%\communicated{S. Sarkar}


\begin{abstract}
%%%
Deep learning has seen rapid growth in recent years and achieved state-of-the-art performance in a wide range of applications. However, training models typically requires expensive and time-consuming collection of large quantities of labeled data. This is particularly true within the scope of medical imaging analysis (MIA), where data are limited and labels are expensive to acquire. Thus, label-efficient deep learning methods are developed to make comprehensive use of the labeled data as well as the abundance of unlabeled and weak-labeled data. In this survey, we extensively investigated over 300 recent papers to provide a comprehensive overview of recent progress on label-efficient learning strategies in MIA. We first present the background of label-efficient learning and categorize the approaches into different schemes. Next, we examine the current state-of-the-art methods in detail through each scheme. Specifically, we provide an in-depth investigation, covering not only canonical semi-supervised, self-supervised, and multi-instance learning schemes but also recently emerged active and annotation-efficient learning strategies. Moreover, as a comprehensive contribution to the field, this survey not only elucidates the commonalities and unique features of the surveyed methods but also presents a detailed analysis of the current challenges in the field and suggests potential avenues for future research.
%%%%
\end{abstract}

\begin{keyword}
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
% \MSC 41A05\sep 41A10\sep 65D05\sep 65D17
%% Keywords
\KWD Medical Image Analysis \sep Label-Efficient Learning \sep Annotation-Efficient Learning \sep Weakly-Supervised Learning
\end{keyword}

\end{frontmatter}

%\linenumbers

%% main text
\section{Introduction}
\label{sec:introduction}
Computer-aided medical image analysis (MIA) plays a more and more critical role in achieving efficiency and accuracy in the early detection, diagnosis, and treatment of diseases. In recent years, MIA systems powered by deep learning (DL) have provided a more objective approach to learning from large and heterogeneous medical image datasets and improved disease diagnosis accuracy. However, DL models require abundant precisely annotated data to effectively capture anatomical heterogeneity and disease-specific traits \citep{yu2021convolutional} due to their data-driven nature. Unfortunately, due to a shortage of available annotators \citep{lu2020national}, there is a significant gap between the demand for annotation and the available annotated datasets. Hence, the urgency to curtail annotation expenses, expedite the annotation procedure, and alleviate the load on annotators has emerged as a crucial hurdle in DL-based MIA tasks. Traditional fully-supervised DL methods, on the other hand, depend solely on comprehensively annotated datasets. Recently, strategies based on semi-supervised, self-supervised, and multi-instance learning have been widely utilized to maximize the utility of existing medical data that may be only partially annotated by point, scribble, box, pixel-wise, \textit{etc.} or even completely unannotated data. In this paper, we dub these methods as label-efficient learning.  
As seen in Fig. \ref{fig_trend}, label-efficient learning methods have significantly proliferated in recent years. Meanwhile, label-efficient learning methods excelling in other MIA tasks like denoising, image registration, and super-resolution have also been rising beyond common classification, segmentation, and detection.
\input{Figures/fig_trend.tex} 

\input{Figures/fig_taxonomy.tex}
Several surveys related to label-efficient learning in medical image analysis have been published in recent years. \citet{cheplygina2019not} categorized methods under supervised, semi-supervised, multi-instance, and transfer learning and named them ``not-so-supervised" learning, while \citet{budd2021survey} surveyed human-in-the-loop strategies for MIA tasks. However, methods in these surveys are either limited in scope or lag behind the current trends.  
% While \citep{kumari2023dataefficient} present a contemporary review focusing on data- and label-efficient learning in the medical domain, their taxonomy is relatively ambiguous for readers to understand, while our taxonomy is based on learning schemes, which gives clear guidance for readers. Besides, above related surveys fail to fully address key questions of significant interest to researchers in their discussions, such as the impact of foundation models and omni-supervised learning with diverse label types, \textit{etc.}
% their discussion on future research directions does not fully address key questions of significant interest to researchers. Critical topics, such as the impact of foundation models and omni-supervised learning with diverse label types, receive limited attention in their work.  
While \citet{kumari2023dataefficient} present a contemporary review focused on data- and label-efficient learning in the medical domain, the taxonomy they present lacks sufficient clarity and directness, which may lead to interpretational difficulties for readers. Conversely, our taxonomy is based on learning schemes and provides distinct and straightforward guidance. Furthermore, the above reviews fall short in addressing several crucial questions of significant interest to researchers. 
% These overlooked aspects include the impact of foundation models and the intricacies of omni-supervised learning with diverse label types, among other pertinent topics
In contrast, our paper comprehensively addresses these topics, providing an in-depth exploration of these critical aspects % To address this issue, we conduct a systematic review of current label-efficient methodologies, of which 
and the outline is illustrated in Fig. \ref{fig_taxonomy}.
% \citet{kumari2023dataefficient} present a contemporary review focusing on data- and label-efficient learning in the medical domain, yet their work omits a discussion on future directions pertaining to key questions of interest to researchers. These include the impact of foundation models and omni-supervised learning across diverse label types, among others.

Aiming to provide a comprehensive overview and future challenges of label-efficient learning methods in MIA, we review more than 300 quality-assured and recent label-efficient learning methods based on semi-supervised, multi-instance, self-supervised, active, and annotation-efficient learning strategies. To pinpoint pertinent contributions, Google Scholar was employed to search for papers with related topics. ArXiv was combined through for papers citing one of a set of terms related to label-efficient medical imaging. Additionally, conference proceedings like CVPR, ICCV, ECCV, NIPS, AAAI, and MICCAI were scrutinized based on the titles of the papers, as well as journals such as MIA, IEEE TMI, and Nature Bioengineering. References in all chosen papers were examined. When overlapping work had been reported in multiple publications, only the publication(s) considered most significant were incorporated.

To the best of our knowledge, this is the first comprehensive review in the field of label-efficient MIA. In each learning scheme, we formulate the fundamental problem, offer the necessary background, and display the experimental results case by case. With the challenges proposed at the end of the survey, we explore feasible future directions in several branches to potentially enlighten the follow-up research on label-efficient learning.

The remainder of this paper is organized as follows. In Section \ref{sec:background}, the necessary background and categorization is presented. In Sections \ref{sec:semi}--\ref{sec:anno}, we introduce the primary label-efficient learning schemes in MIA, including semi-supervised learning in Section \ref{sec:semi}, self-supervised learning in Section \ref{sec:ssl}, multi-instance learning in Section \ref{sec: mil}, active learning in Section \ref{sec:al}, few-shot learning in Section \ref{sec:fsl}, and annotation-efficient learning in Section \ref{sec:anno}. We discuss the existing challenges in label-efficient learning and present several heuristic solutions for these open problems in Section \ref{sec:cnfd}, where promising future research directions are proposed as well. Finally, we conclude the paper in Section \ref{sec:con}.


\input{Sections/sec_background.tex}
\input{Sections/sec_semi_sup}
\input{Sections/sec_self_sup}
\input{Sections/sec_multi_ins}
\input{Sections/sec_act_learn}
\input{Sections/sec_few_shot}
\input{Sections/sec_anno_effi}
\input{Sections/sec_challenge_n_dirs}

\section{Conclusion}\label{sec:con}
Despite significant advances in computer-aided MIA, the question of how to endow deep learning models with enormous data remains a daunting challenge. Deep learning models under label-efficient schemes have shown significant flexibility and superiority in dealing with high degree of quality- and quantity-variant data. To that end, we have presented the first comprehensive label-efficient learning survey in MIA. A variety of learning schemes, including semi-supervised, self-supervised, multi-instance, active and annotation-efficient learning in the general field are classified and analyze thoroughly. We hope that by systematically sorting out the methodologies for each learning schemes, this survey will shed light on more progress in the future.

\section*{Acknowledgments}
This work was supported by National Natural Science Foundation of China (No. 62202403), Hong Kong Innovation and Technology Fund (No. PRP/034/22FX), and the Project of Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone (HZQB-KCZYB-2020083). 

\appendix
\section{Concepts of Prior Knowledge}
\label{appendix1}
\subsection{Assumptions and Detail in Semi-supervised Learning}
\subsubsection{Assumptions in Semi-supervised Learning}
Semi-SL is not universally effective, as stated in \citep{van2020survey,xiaojin2008semi}, a necessary condition for Semi-SL algorithms to work is that the marginal data distribution $p(x)$ contains underlying information about the posterior distribution $p(y|x)$, where $x$ and $y$ represent the data sample over input space $\mathcal{X}$ and the associated label, respectively. Otherwise the additional unlabeled data will be useless to infer information about $p(y|x)$, which means the Semi-SL algorithms may achieve similar or even worse performance compared with supervised learning algorithms. Therefore, several assumptions over the input data distribution have been proposed to constrain the data structure and ensure the algorithms can be generalized from a limited labeled dataset to a large-scale unlabeled dataset. Following \citep{van2020survey, ouali2020overview}, the assumptions in Semi-SL are introduced as follows:

\textbf{Smoothness assumption}. Suppose $x_1,~x_2 \in X$ are two input data samples over input space $\mathcal{X}$. If the distance between $x_1$ and $x_2$ is very close, \textit{i.e.}, $D(x_1, x_2) < \varepsilon$, where $\varepsilon$ is an artificially set threshold, then the associated labels $y_1$ and $y_2$ should also be the same. Note that sometimes there is an additional constraint in the smoothness assumption. In \citep{ouali2020overview}, $x_1$ and $x_2$ are required to belong to the same high-density region, so as to avoid the situation that these two samples reside on the brink of different high-density regions and are misclassified as one category.

\textbf{Cluster assumption}. In this assumption, we assume that data points with similar underlying information are likely to form high-density regions, i.e., clusters. If the two data points $x_1$ and $x_2$ lie in the same cluster, then they are expected to have the same label. In fact, the cluster assumption can be considered as a special case of the smoothness assumption. According to \citep{chapelle2009semi}, if the two data points $x_1$ and $x_2$ can be connected with a line that does not pass through any low-density area, they belong to the same cluster. 

\textbf{Low-density assumption}. The decision boundary of the classifier is assumed to lie in the low-density areas instead of high-density ones, which can be derived from the cluster assumption and smoothness assumption. On the one hand, if the decision boundary resides in the high-density regions, the two data points $x_1$ and $x_2$ located in the same cluster but opposite sided of the decision boundary will be categorized as different classes, which obviously violates the cluster assumption and smoothness assumption. On the other hand, following the cluster and smoothness assumption, data points in any high-density areas are expected to be assigned the same label, which means the decision boundary of the model can only lie in the low-density areas, thus satisfying the low-density assumption.

\textbf{Manifold assumption}. A manifold is a concept in geometry, that represents a geometric structure in a high-dimensional space, i.e., a collection of data points in the input space $\mathcal{X}$. For example, a curve in 2-dimensional space can be thought of as a 1-dimensional manifold, and a surface in 3-dimensional space can be seen as a 2-dimensional manifold. The manifold assumption states that there is a certain geometry of the data distribution in the high-dimensional space, namely that the data are concentrated around a certain low-dimensional manifold. Due to the fact that high-dimensional data not only poses a challenge to machine learning algorithms, but also leads to a large computational load and the problem of dimensional catastrophe, it will be much more effective to estimate the data distribution if they lie in a low-dimensional manifold. 

\subsubsection{Detail of Key Generative Methods}
Researchers can obtain various generative methods according to different assumptions on the latent distribution. On the one hand, it can be easy to formulate a generative method once an assumption on the distribution is made, whereas on the other, the hypothetical generative model must match the real data distribution to avoid the unlabeled data in turn degrading the generalization performance. One can formulate the modeling process of generative methods as follows:

\begin{equation}\label{generative}
    \begin{aligned}
    y^{*}&=\arg \max _{y} p(y | x)=\arg \max _{y} \frac{p(x | y) p(y)}{p(x)}\\
    &=\arg \max _{y} p(x | y) p(y),\\
    \end{aligned}
\end{equation}
where the generative methods models the joint distribution $p(x,y)$. Eq. (\ref{generative}) indicates that if the correct assumption on prior $p(y)$ and conditional distribution $p(x | y)$ is made, the input data can be expected to come from the latent distribution. 

\textbf{Definition of Generative Adversarial Network (GAN).}
The aim of generator $\mathcal{G}$ is to iteratively learn the latent distribution from real data $x$  starting from generating data with random noise distribution $p(z)$. Meanwhile, the goal of discriminator $\mathcal{D}$ is to correctly distinguish the fake input generated by $\mathcal{G}$ and real data $x$. Formally, we can formulate the optimization problem of a GAN as follows:

\begin{equation*}
    \begin{aligned}
    \min_{\mathcal{G}}\max_{\mathcal{D}} \mathcal{L}(\mathcal{G},\mathcal{D}) &= \mathbb{E}_{x\sim p(x)}[\mathrm{log}\mathcal{D}(x)]\\
    &+\mathbb{E}_{z\sim p(z)}[1-\mathrm{log}(\mathcal{D}(\mathcal{G}(z)))],
    \end{aligned}
\end{equation*}
where $\mathcal{L}$ represents the loss function of generator $\mathcal{G}$ and discriminator $\mathcal{D}$. Concretely, $\mathcal{G}$ aims to minimize the objective function by confusing $\mathcal{D}$ with generated data $\mathcal{G}(z)$, while $\mathcal{D}$ aims at maximizing the objective function by making correct predictions.

\textbf{Definition of Variational Autoencoder (VAE).}
The typical VAE consists of two objectives: one is to minimize the discrepancy between input data $x$ and its reconstruction version $\hat{x}$ produced by the decoder, and the other is to model a latent space $p(z)$ following a simple distribution, such as a standard multivariate Gaussian distribution. Thus, the loss function for training a VAE can be formulated as follows:
\begin{equation*}
    \begin{aligned}
    \min_{\theta}\sum_{x\in X} \mathcal{L}(x, \theta) = \mathcal{L}_{MSE}(x,\hat{x}_{\theta})+\mathcal{L}_{KL}(p_{\theta}(z|x)||p(z)),
    \end{aligned}
\end{equation*}
where $\mathcal{L}_{MSE}$ represents the mean square error; $\hat{x}_{\theta}$ is the reconstruction version of input data $x$ generated by the decoder $p_{\phi}(x|z)$ given parameters $\phi$; $\mathcal{L}_{KL}(\cdot||\cdot)$ represents the Kullback-Leibler divergence which measures the distance between two distributions; and $p_{\theta}(z|x)$ denotes the posterior distribution produced by the encoder given parameters $\theta$.


\subsection{Conventional Uncertainty Metrics in Active Learning}
The uncertainty measure reflects the degree of dispersion of a random input. There are many ways to measure the uncertainty of inputs. Starting with simple metrics like standard deviation and variance, current studies in MIA mainly focus on \textbf{margin sampling} \citep{campbell2000query} and \textbf{entropy sampling} \citep{holub2008entropy}. Denote the probability as $p$, we give the definition of these metrics as follows.

\textbf{Margin sampling} \citep{campbell2000query} estimates the probability difference $\mathcal{M}$ between the first and second most likely labels $\hat{y}_{1}, \hat{y}_{2}$ according to the deep model parameter $\theta$ and expect the least residual value by the following notation:

\begin{equation*}
    \mathcal{M}=\underset{x}{\operatorname{argmin}}   [p_{\theta}\left(\hat{y}_{1} \mid x\right)-p_{\theta}\left(\hat{y}_{2} \mid x\right)]
\end{equation*}

\textbf{Entropy sampling} \citep{holub2008entropy} is another conventional metric for sampling. In a binary or multi-classification scenario, the sampled data with higher entropy can be selected as the expected annotation data. For a $C$-class task, entropy sampling metric $\mathcal{E}$ can be denoted as follows:
\begin{equation*}
    \mathcal{E}=\underset{x}{\operatorname{argmax}}\left(-\sum_{c=1}^{C}p(y_c\mid x)\log p(y_c\mid x)\right)
\end{equation*}

\subsection{Detail of Key Few-shot Methods}
\textbf{Definition of Metric-Learning.} The objective of a metric-based strategy is to measure the distance across limited data samples and attempt to generalize the measurement on more data. Consider two embedded image-label pairs $(f(x_1), y_1)$ and $(f(x_2), y_2)$ and a distance function $d$ parameterized by neural networks to determine the separation between them. For any image $x_3$ to be predicted, this strategy calculates the two distances between the embeddings $d(f(x_1), f(x_3))$ and $d(f(x_2), f(x_3))$ and assigns the class label to the output with fewer values.

\textbf{Definition of Meta-Learning.} Meta-learning, or learning to learn \citep{thrun1998learning} technique divides the learning scheme together with the samples into the meta-training phase and the meta-testing phase. In each phase, it samples the divided data, forming several meta-tasks. Here we denote the corresponding tasks as $\tau_{meta\_train}=\{\tau_0, \tau_1, \ldots, \tau_n\}$ and $\tau_{meta\_test}=\{\tau_{n+1}, \tau_{n+2}, \ldots, \tau_{n+k}\}$. In the meta-training phase, the meta-learner $\mathcal{F}$ would learn the common representation from the support set. Then, the learned parameters are fed into a regular learner $f$ to test the performance of the query set from the same meta-task and optimize the model parameters $\theta_\mathcal{F}, \theta_f$ through the designated loss $\mathcal{L}$. After meta-training, the trained model would have the ability to deal with the assigned problem with very few samples. Denote the support set and the corresponding query set pair as $D_i=(S_i, Q_i)\in\tau_{i}$, the whole process can be depicted in the following notation:
\begin{equation*}
\begin{aligned}
    \theta = \arg\min_\theta \sum_{D_{train}\in\tau_{meta\_train}}\sum_{D_{test}\in\tau_{meta\_test}}\\ \mathcal{L}(f(S_{test}, \mathcal{F}(D_{train};\theta_\mathcal{F})), Q_{test}; \theta_f)
\end{aligned}
\end{equation*}

\renewcommand{\thetable}{B\arabic{table}}
\setcounter{table}{0} % Reset table counter in the appendix
\section{Datasets}
As a supplement to the main text, we summarize representative publicly available datasets across 16 different organs such as the brain, chest, prostate, \textit{etc.} in Tab. \ref{tab:Dataset1}. 
These publicly available MIA datasets can be leveraged to construct label-efficient learning algorithms for numerous purposes, including classification, detection, and segmentation.

\input{Tables_supp/dataset_1}

\input{Tables_supp/dataset_2}

\input{Tables_supp/dataset_3}

\clearpage

% \section*{\itshape Reference style}

% Text: All citations in the text should refer to:
% \begin{enumerate}
% \item Single author: the author's name (without initials, unless there
% is ambiguity) and the year of publication;
% \item Two authors: both authors' names and the year of publication;
% \item Three or more authors: first author's name followed by `et al.'
% and the year of publication.
% \end{enumerate}
% Citations may be made directly (or parenthetically). Groups of
% references should be listed first alphabetically, then chronologically.

%%Harvard
\bibliographystyle{model2-names.bst}\biboptions{authoryear}
\bibliography{refs}

\end{document}

%%
