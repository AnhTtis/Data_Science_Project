\section{Annotation-Efficient Learning in MIA}\label{sec:anno}
% ---------------------------------------------------------------
\subsection{Tag Annotation}\label{sec:anno_tag}
% ---------------------------------------------------------------
Tag annotation, which is a text/binary label for each image, is the most efficient form.
Most of such are based on the concept of class activation mapping (CAM)~\cite{zhou2016learning}. 
Several works propose to use of CAM to generate object localization proposals or even to perform whole-object pixel-wise segmentation.
For the \textbf{detection} task, Wang \textit{et al.}~\cite{hwang2016self} propose a two-branch network that jointly optimizes the classification and lesion detection tasks. In this approach, the CAM-based lesion detection network is supervised with only image-level annotations, and
the two branches are mutually guided by the weight-sharing technique, where a weighting parameter is adopted to control the focus of learning from the classification task to the detection task.
For lesion detection, Dubost \textit{et al.}~\cite{dubost2020weakly} propose a weakly-supervised regression network.
The proposed method is validated on both 2D and 3D medical images.
For the \textbf{segmentation} task, Li \textit{et al.}~\cite{li2022deep} propose a breast tumor segmentation method with only image-level annotations based on CAM and deep-level set (CAM-DLS).
It integrates domain-specific anatomical information from breast ultrasound to reduce the search space for breast tumor segmentation.
Meanwhile, Chen \textit{et al.}~\cite{chen2022c} proposes a causal CAM method for organ segmentation, which is based on the idea of causal inference with a category-causality chain and an anatomy-causality chain.
In addition, several studies~\cite{lin2019seg4reg,lin2021seg4reg+} demonstrate that bridging the classification task and dense prediction task (e.g., detection and segmentation) via CAM-based methods is beneficial for both tasks.
Compared to natural images, medical images are usually from low contrast, limited texture, and varying acquisition protocols~\cite{zhang2021weakly}, which makes directly applying CAM-based methods less effective.
Fortunately, incorporating the clinical priors (e.g., objects' size~\cite{fruh2021weakly}) into the weakly supervised detection task is promising to improve the performance.
% ---------------------------------------------------------------
\subsection{Point Annotation}\label{sec:anno_point}
% ---------------------------------------------------------------
Point annotation refers to the annotation of a single point of an object.
Several studies~\cite{roth2021going,dorent2021inter,khan2019extreme} focus on using extreme points as the annotation to perform pixel-level segmentation.
These methods typically consist of three steps: 1) extreme point selection; 2) initial segmentation with a random walk algorithm; 3) training of the segmentation model with the initial segmentation results. 
The last two steps can be iterated until the segmentation results are stable.
However, these methods require the annotators to locate the boundary of the objects, which is still laborious in practice.
In contrast, other studies~\cite{yoo2019pseudoedgenet,zhao2020weakly,qu2020weakly,qu2019weakly,tian2020weakly,belharbi2021deep,lin2022label,valvano2021learning} use center point annotation to perform pixel-level segmentation for the task of cell/nuclear segmentation. 
These methods typically adopt the Vorinor~\cite{kise1998segmentation} and cluster algorithms to perform coarse segmentation. 
Then different methods are used to refine the segmentation results, such as iterative optimization~\cite{qu2019weakly,qu2020weakly}, self-training~\cite{zhao2020weakly}, and co-training~\cite{lin2022label}. 

Compared with full annotation, point annotation can reduce the annotation time by around 80\%~\cite{qu2020weakly}.
However, some issues have not been addressed. 
First, existing methods typically derived pseudo labels from the point annotation, which are based on strong constraints or assumptions (e.g., Voronoi) from the data, restricting the generalization of these methods to other datasets~\cite{lin2022label}.
\input{Sections/Tables/tab_anno_effi_v2.tex}
Second, due to the lack of explicit boundary supervision, there is a non-negligible performance gap between the weakly supervised methods with points and the fully supervised methods.

% ---------------------------------------------------------------
\subsection{Scribble Annotation}\label{sec:anno_scribble}
% ---------------------------------------------------------------
Scribble annotation, a set of scribbles drawn on an image by the annotators, has been recognized as a user-friendly alternative to bounding box annotation~\cite{tajbakhsh2020embracing}. 
Compared with point annotation, it provides the rough shape and size information of the objects, which is promising to improve the segmentation performance, especially for objects with complex shapes.
Wang \textit{et al.}~\cite{wang2018interactive} propose a self-training framework with differences in model predictions and user-provided scribbles. 
Can \textit{et al.}~\cite{can2018learning} develop a random walk algorithm that incrementally performs region growing method around the scribble ground truth, while
Lee \textit{et al.}~\cite{lee2019scribble2label} introduce Scribble2Label, a method that integrates the supervision signals from both scribble annotations and pseudo labels with the exponential moving average. 
Furthermore, Dorent \textit{et al.}~\cite{dorent2020scribble} extend the Scribble-Pixel method to the domain adaptation scenario, where a new formulation of domain adaptation is proposed based on CRF and co-segmentation with the scribble annotation. 
In recent work, Zhang \textit{et al.}~\cite{zhang2022cyclemix} adopted mix augmentation and cycle consistency for the Scribble-Pixel method, demonstrating the improvement of both weakly and fully supervised segmentation methods.

% ---------------------------------------------------------------
\subsection{Box Annotation}\label{sec:anno_box}
% ---------------------------------------------------------------
Box annotation encloses the segmented region within a rectangle, and various recent studies have focused on this Box-Pixel scenario. 
Rajchl \textit{et al.}~\cite{rajchl2016deepcut} employs a densely-connected random field (DCRF) with an iterative optimization method for MRI segmentation. 
Wang \textit{et al.}~\cite{wang2021accurate,wang2021bounding} adopt MIL and smooth maximum approximation based on the bounding box tightness prior~\cite{hsu2019weakly}, that is, an object instance should touch all four sides of its bounding box. 
Thus, a vertical or horizontal crossing line within a box yields a positive bag because it covers at least one foreground pixel. 
Studies~\cite{wang2021bounding} demonstrate that the Box-Pixel method yields promising performance, being only 1--2\% inferior to the fully supervised methods.

\subsection{Discussion}
Points are most suitable for objects with uniform shapes and sizes, particularly when there is a large number of objects present. These points indicate the location of the objects.
Scribbles, on the other hand, are used to label different semantic elements by marking them and are best suited for objects with uniform shapes but varied sizes.
Boxes can provide an approximation of the shape and size information of objects, making them ideal for tasks such as segmentation or detection where objects have high variations in their shape and size.
Out of all these annotation types, image tagging is the most efficient, requiring the least amount of annotation cost.
Several studies have aimed to reduce the performance gap between different annotation-efficient methods based on various annotations.
Future work could explore the following topics: i) integrating multiple supervision signals into a unified learning framework, such as multi-task learning and omni-supervised learning; ii) actively reducing the annotation cost through human-in-the-loop techniques, such as active learning; and iii) mining inherent knowledge from multi-modality data.






