\input{fig/baselines_hoi4d} 
\input{exp/tab_main}


We train our model on the contructed HO3Pairs dataset, evaluate it on the HOI4D~\cite{liu2022hoi4d} dataset and show zero-shot generalization to the EPIC-KITCHEN~\cite{epic} dataset.  
% 
We evaluate both the generated HOI images and the extracted 3D poses. 
%
For image synthesis, we compare with conditional image synthesis baselines and show that our method generates more plausible hands in interaction.
%
Beyond 2D HOI image synthesis, we compare the extracted 3D poses with prior works that directly predict 3D hand poses.
% and show that our method is favored by users on both datasets. 
%
%We further analyze the benefits from the proposed data augmentation to prevent overfitting to artifacts and the benefits from DDPM loss in image space to train LayoutNet.
% We also carry out ablation studies to demonstrate the contributions of the data augmentation process in Sec~\ref{sec:data} and the DDPM loss in Sec~\ref{sec:layout}.
%
Furthermore, we show several applications enabled by the proposed HOI synthesis method, including few-shot adaptation, image editing by layout, heatmap-guided prediction and integrating object affordance with the scene. 

% Furthermore, we show that the proposed method can be quickly adapted to hand-object-interaction of new categories with very few samples. 
%
% Lastly, we demonstrate that the proposed hand proxy provides an editing interface to control the interaction layout. The layout generation can be conditioned on additional constraints at test time, which enables reusing the heatmap prediction from prior works and generates consistent hand sizes across different objects in one scene. 

\noindent\textbf{Datasets}
% We evaluate on two egocentric datasets: HOI4D~\cite{liu2022hoi4d} and EPIC-KITCHEN~\cite{epic}. 
%
Instead of testing with inpainted object images, we evaluate our model on the real object-only images cropped from the frames without hands. The goal is to prevent models from cheating by overfitting to the inpainting artifacts, as justified in the ablations below.

% HOI4D dataset is an egocentric video dataset depicting human interacting with various objects under lab environment. It consists of xxxk  20-second short clips spanning xx common object categories. The object categories include large objects like chair, portable objects like bottle, articulated object like laptop. The dataset provides manual annotations of segmentation masks for active objects and hands, action labels, object class categories, object instance identity, and ground truth 3D hand poses. We train and evaluate on 10 categories where full annotations are released.
% We hold out 5 object instances in each categories for testing, and crop out the objects where their states are labeled as rest. In total this results in 126 test images.
The HOI4D dataset is an egocentric video dataset recording humans in a lab environment interacting with various objects such as kettles, bottles, laptops, \etc. The dataset provides manual annotations of hand and object masks, action labels, object categories, instance ID, and ground truth 3D hand poses. We train and evaluate on 10 categories where full annotations are released. For each category, we hold out 5 object instances for evaluation. 
% We crop these objects from frames where their states are labeled as rest (i.e., there is no hand in the frame). 
In total, we collect 126 testing images.

% EPIC-KITCHEN dataset displays more diverse and cluttered scenes. We construct our test set from the recently released subset VISOR~\cite{VISOR2022} which features hand-object-interaction videos. We randomly select 10 frames from each video clips  and detect objects~\cite{detectron} to crop and filter out the object crops with hands. In total, we collect 500 object-only images for testing.
The EPIC-KITCHEN dataset displays more diverse and cluttered scenes. We construct our test set by randomly selecting 10 frames from each video clip. We detect and crop out objects without hands~\cite{detectron2}. In total, we collect 500 object-only images for testing.


\subsection{Evaluating Image Synthesis}
\noindent\textbf{Evaluation Metrics. }
We evaluate HOI generation using three metrics. First, we report the FID score~\cite{Seitzer2020FID,heusel2017gans}, which is widely used for image synthesis that measures the distance between two image sets. We generate 10 samples for every input and calculate FID  with 1000 HOI images extracted from the test sets. 
%
% Besides the general image realism, we also evaluate the quality of interaction for the generated hand -- whether the generated hands make contact with the objects or not. We propose to measure the interaction quality by contact recall, the ratio of the generated hands are detected as "in-contact" state by an off-the-shelf hand detector~\cite{dandan}.
We further evaluate the physical feasibility of the generated hands by the contact recall metric --- it computes the ratio of the generated hands that are in the ``in-contact'' state by an off-the-shelf hand detector~\cite{shan2020understanding}.
%
We also carry out user studies to evaluate their perceptual plausibility. Specifically, we present two images from two randomly selected methods to users and ask them to select the more plausible one. We collect 200 (for HOI4D) and 240 (for EPIC-KITCHEN) answers and report the likelihood of the methods being chosen.

\noindent\textbf{Baselines. }
We compare our method with three strong image-conditional synthesis baselines. 
% 
1) \noindent\textit{Latent Diffusion Model (LDM)}~\cite{ldm} is one of the state-of-the-art generic image generation models that is pre-trained with large-scale image data.  We condition the model on the object image and finetune it on HO3Pair dataset. This baseline jointly generates both layout and appearance with one network. 
% 
2) \textit{Pix2Pix}~\cite{pix2pix} is commonly used for pose-conditioned human/hand synthesis~\cite{chan2019everybody,ganerated_hand}. We modify the model to condition on the generated layout masks that are predicted from our LayoutNet. 
% 
3) \textit{VAE}~\cite{kingma2013auto} is a widely applied generative model in recent affordance literature~\cite{fouhey2012people,li2019putting,PLACE:3DV:2020}. This baseline uses a VAE with ResNet~\cite{he2016deep} as backbone to predict a layout parameter. The layout is then passed to our ContentNet to generate images. 
% \end{enumerate}
 
 \noindent\textbf{Results.} We visualize the generated HOI images in Fig~\ref{fig:syn2d}. Pix2Pix typically lacks detailed finger articulation. While LDM and VAE generate more realistic hand articulations than Pix2Pix,  the generated hands sometimes do not make contact with the objects. The hand appearance near the contact region is less realistic. In some cases, LDM does not add hands at all to the given object images. In contrast, our model can generate hands with more plausible articulation and the synthesized contact regions are more realistic. 
This is consistent with the quantitative results in Tab~\ref{tab:syn2d}. While we perform comparably to the baselines in terms of the FID score, we achieve the best in terms of contact recall. The user study shows that our results are favored the most. This may indicate that humans perceive interaction quality as a more important factor than general image synthesis quality.

\noindent\textbf{Generalizing to EPIC-KITCHEN. }
Although our model is trained only on the HOI4D dataset with limited scenes and relatively clean backgrounds, our model can generalize to the EPIC-KITCHEN dataset without any finetuning. In Fig~\ref{fig:syn2d}, the model also generalizes to interact with unseen categories such as scissors and cabinet handles. Tab~\ref{tab:syn2d} reports similar trends: performing best in  contact recall, comparably well in image synthesis and is favored the most by users. 

\input{fig/3dpose}
\input{exp/overfit}
\noindent\textbf{Ablation: Data Augmentation. } Tab \ref{tab:artifact} shows the benefits of data augmentation to prevent overfitting. Without any data augmentation, the model performs well on the inpainted object images but catastrophically fails on the real ones. When we add aggressive common data augmentations like Gaussian blur and Gaussian noise, the performance improves. Training on SDEdited images further boosts the performance. The results also justify the use of real object images as test set since evaluating on the inpainted object images may not reflect the real performance. 


\noindent\textbf{Ablation: LayoutNet Design. }
% \input{exp/layout}
We analyze the benefits from our LayoutNet design by reporting contact recall. The LayoutNet predicts more physically feasible hands by taking in the splatted layout masks instead of the 5-parameter layout vector (87.14\% vs 78.10\%). Moreover, the contact recall drops to 83.96\% when the diffusion loss in Sec~\ref{sec:layout} is removed, verifying its contribution to the LayoutNet.

\subsection{Evaluating Extracted 3D Hand Poses}
\label{sec:3d}
Thanks to the realism of the generated HOI images, 3D hand poses can be directly extracted from them by an off-the-shelf hand pose estimator~\cite{frankmocap}. We conduct a user study to compare the 3D poses extracted from our HOI images against methods that directly predict 3D pose from object images. We present the rendered hand meshes overlaid on the object images to users and are asked to select the more plausible one. In total, we collected 400 and 380 answers from users for HOI4D and EPIC-KITCHEN, respectively. 

% The 3D poses can be potentially transferred to an excutable actions for robot hands. 

\input{exp/3d}
\noindent\textbf{Baselines.} 
% While most prior works that output 3D hand poses take in a known object 3D meshes, recent works by Corona \etal (GANHand)~\cite{} can solve the same task as ours.
While most 3D hand pose generation works require 3D object meshes as inputs, a recent work by Corona \etal (GANHand)~\cite{ganhand} can hallucinate hand poses from an object image.
Specifically, they first map the object image to a grasp type~\cite{feix2015taxonomy} with the predefined coarse pose and then regress a refinement on top. We finetune their released model on the HO3Pairs datasets with the ground truth 3D hand poses.  We additionally implement a diffusion model baseline that sequentially diffuses 3D hand poses. The architecture is mostly the same as the LayoutNet but the diffused parameter is increased to 51 (48 for hand poses and 3 for scale and location) and the splatting function is replaced by the MANO~\cite{mano} layer that renders hand poses to image. See the appendix for  implementation details.

\noindent\textbf{Results. } As shown in Fig~\ref{fig:3d}, GANHand\cite{ganhand} predicts reasonable hand poses for some objects but fails when the grasp type is not correctly classified. The hand pose diffusion model sometimes generates infeasible hand poses like acute joint angles.  Our model is able to generate hand poses that are compatible with the objects. Furthermore, while previous methods typically assume right hands only, our model can automatically generate  both left and right hands by implicitly learning the correlation between approaching direction and hand sides.  The qualitative performance is also supported by the user study  in Tab~\ref{tab:3d}. 

% \subsection{Generalize better than directly predicting 3D pose??}
\subsection{Application} 
\label{sec:app}
We showcase several applications that are enabled by the proposed method for hand-object-image synthesis. 

\input{exp/fewshot}
\noindent\textbf{Few-shot Adaptation. } In Tab~\ref{tab:fewshot}, we show that our model can be quickly adapted to a new HOI category with as few as 32 training samples. We initialize both LayoutNet and ContentNet from our HOI4D-pretrained checkpoints and compare it with the baseline model that was pre-trained for inpainting on a large-scale image dataset~\cite{ldm}. We finetune both models on 32 samples from three novel categories in HOI4D and test with novel instances. The baseline model adapts quickly on some classes, justifying our reasons to finetune our model from them---generic large-scale image pretraining indeed already learns good priors of HOI. Furthermore, our HOI synthesis model performs even better  than the baseline.


\input{fig/interpolation}
\noindent\textbf{Layout Editing. } The layout representation allows users to edit and control the generated hand's structure. As shown in Fig~\ref{fig:interpolate}, while we gradually change the layout's location and orientation, the synthesized hand's appearance changes accordingly. As the approaching direction to the mug changes from right to left, the synthesized fingers change accordingly from pinching around the handle to a wider grip around the mug's body. 


\input{fig/heatmap}
\noindent\textbf{Heatmap-Guided Synthesis.} As shown in Sec~\ref{sec:layout}, our synthesized HOI images can be conditioned on a specified location without any retraining. 
% We hijack the specified location after each layout diffusion step, guiding the generated layout to center at the given location.  
This not only allows users to edit with just keypoints, but also enables our model to utilize contact heatmap predictions from prior works~\cite{nagarajan2019iccv:hotspots,fang2018demo2vec}. In Fig~\ref{fig:heatmap}, we sample points from the heatmaps and conditionally generate layouts and HOI images which further specifies \textit{how} to interact at the sampled location. 


\input{fig/scene}
\noindent\textbf{Integration to scene.} We integrate our object-centric HOI synthesis to scene-level affordance prediction. While the layout size is predicted relative to each object, hands for different objects in one scene should exhibit consistent scale. To do so, we first specify one shared hand size for each scene and calculate the corresponding relative sizes in each crops (we assume objects at similar depth and thus sizes can be transformed by crop sizes, although more comprehensive view conversions can be used). The LayoutNet is conditioned to generate these specified sizes with guided generation techniques (Sec~\ref{sec:layout}). Fig~\ref{fig:scene} shows the extracted hand meshes from each crops transferred back to the scene. 
