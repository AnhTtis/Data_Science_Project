

% \sd{Add a short Abstract to briefly describe the main contents of the supplementary ... additional implementation details and qualitative results. Mention that there is an accompanying video that contains more visual results and that the readers should check it out. Mention something that you will release the code upon acceptance.. (check with Sifei about whether this is allowed by NVIDIA. She will have to get Jan's permission.}

In the supplementary material, we provide more implementation details and more qualitative results. We discuss the details of articulation-agnostic hand proxy and how to apply DDPM loss in the image space for training the LayoutNet (Sec. \ref{layout}). We also present ablations on ContentNet(Sec. \ref{content}). We further show: (i) the paired data construction method being robust, in Sec. \ref{3.3}, (ii) baseline implementations details in Sec. \ref{3.4}, (iii) details of integrating our approach to scene-level affordance prediction in Sec. \ref{3.5}. Finally, we discuss the limitation of our approach (Sec. \ref{3.6}), and show more qualitative results in Sec. \ref{qr}. \textbf{Visual results are also included in the video.}

\section{Implementation Details}
% We provide a brief project overview in the supplement video.

\subsection{LayoutNet (Sec~3.1)}\label{layout}
\noindent\textbf{Layout parameters. }
As mentioned in Sec~3.1 of the main paper, we parameterize the layout as $(x, y, a, b_1, b_2)$, where $x,y$ is the location, $a^2$ is size, and $b_1, b_2$ are un-normalized approaching direction parameters. 
For training the LayoutNet, we obtain the ground truth parameters from off-the-shelf 2D hand prediction systems. The size and location comes from the predicted bounding box of a hand detector~\cite{shan2020understanding}, which typically defines the hand region up to the wrist. The orientation is calculated from hand segmentation whose region is typically defined as the entire hand region, including hand and forearm. The approaching direction is calculated as the first principal component of a hand mask that centers on the location of the palm of the predicted hand.
% The  width ratio between palm and forearm $\bar s$ is set to $2$, with one significant digit.

We splat the layout parameters onto 2D via the spatial transformer network~\cite{jaderberg2015spatial} that transforms a canonical mask template by a similarity transformation.  
The 2D similarity transformation is determined from the layout parameters. More formally, 

\begin{align*}
    T_l = \begin{pmatrix}sR & t \\ 0 & 1\end{pmatrix} =  \begin{pmatrix}a^2 \hat b_1 & -a^2\hat b_2 & x \\  a^2 \hat b_2 & a^2 \hat b_1 & y \\ 0  & 0 & 1\end{pmatrix},
\end{align*}
where $\hat b_1, \hat b_2 $ is the normalized vector of $b_1, b_2$. 

The lollipop-shape template in the canonical space is implemented with its circle being an isometric 2D Gaussian with a standard deviation of $1$ and its rectangle being a 1D  Gaussian with a standard deviation $\bar s = 2$. The width of the rectangle is calculated from the training data as the average ratio of the widths of forearms and palms. 

\noindent \textbf{DDPM loss on mask. }
In Eq 1 and 2 of the main paper, we write the DDPM loss in terms of reconstructing clean samples. In practice, we follow prior works~\cite{ldm,nichol2021glide,ramesh2022hierarchical} that reconstruct the added noise $\epsilon$ as
\begin{align*}
\mathcal L_{\text{DDPM}}^{\text{noise}} = \mathbb E _{x, \epsilon \sim \mathcal N(0, I), t} \|\epsilon - \epsilon_\theta(x_t, t) \|_2^2.
\end{align*}
The estimated clean sample $\hat l_0$ is connected with the estimated noise by  $ \hat l_0 = \frac{1}{\sqrt{1 - \bar \alpha_{t}}} l_{t} - \frac{\sqrt{\bar \alpha_{t}}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_\theta$, where $\alpha_t, \bar \alpha_t$ represent the noise schedule for each diffusion time step. 

We train the LayoutNet with a weighted sum of the parameter loss $\mathcal{L}_{\text{para}}$ for esitmating the noise term $\epsilon$,  and a mask loss $\mathcal{L}_{\text{mask}}$ for estimating the clean sample term $\hat l_0$. The hyperparamter $\lambda$ is set to $10$. 

\noindent\textbf{Guided layout generation. }
LayoutNet inherits properties from diffusion models that can be guided to generate samples with additional constraints at test time. We follow Song \etal~\cite{song2020denoising}. After each diffusion steps, we hijack the additional constraints with corresponding noise levels for the next diffusion step. 

More specifically, instead of passing in the network's output $\bm{x}_t$ from the previous time step, we hijack it with $\bm{x}_t \leftarrow \tilde{\bm{x}}_t\bm{m} + \bm{x}_t (1-\bm{m})$, where $\bm{m}$ is the indicator mask of the given condition $\tilde{\bm{x}}_0$. The unspecified constraints in $\tilde{\bm{x}}_0$ are set to 0. $\tilde{\bm{x}}_t$ represents the additional constraint with corresponding noise level, \ie $\sqrt{1-\bar \alpha_t}\tilde{\bm{x}}_0 + \sqrt{\bar{\alpha_t}} \bm{\epsilon}$.

% We show usages of guided generation in Fig~\ref{fig:more_inter} (guided location) and Fig~\ref{fig:more_scene} (guided size). 

\subsection{ContentNet (Sec3.2)}\label{content}
\label{sec:content}
The goal of ContentNet is to generate high-resolution ($256^2$) realistic HOI images conditioned on the predicted layout and the input object image. We tried two different approaches commonly used in diffusion models~\cite{ldm,nichol2021glide} as backbones for the ContentNet. One way (called ours/AffordDiff-LDM) is to follow Rombach \etal~\cite{rombach2022high}, as described in our main paper, that implements the ContentNet in the latent space where images of size $256^2$ are compressed to 3-dimensional features of size $64^2$ by a fixed pretrained autoencoder. The other way (called ours/AffordDiff-GLIDE) is to follow Nichol \etal~\cite{nichol2021glide} that uses a cascaded diffusion model that first generates images of size $64^2$ and then upsamples them by a factor of $4$.   

\textit{All} of the quantitative results in our main paper, including the user studies and all ablations, are based on Afford-LDM. AffordDiff-GLIDE is better in terms of contact recall ($90.8\%$ vs $87.1\%$) while AffordDiff-LDM is significantly better in terms of FID score ($99.0$ vs $121.6$).   We find that AffordDiff-LDM generates less blurry results and the hand texture appears sharper and more realistic.  In comparison, we find AffordDiff-GLIDE  perceptually preferred because AffordDiff-GLIDE generates more realistic, though blurrier, finger articulations. The qualitative results in the main paper on EPIC-KITCHEN dataset (Fig 1 and Fig4 right in the main paper) show Afford-GLIDE. However, we provide the qualitative comparison of Afford-LDM with baselines in Fig~\ref{fig:more_hoi4d} and Fig~\ref{fig:more_epic} of the appendix.   We further provide a comparison of these two variants in Fig~\ref{fig:glide_ldm} of the appendix. 

% GLIDE - LDM

\subsection{Constructing Paired Training Data (Sec3.3)}\label{3.3}
% We obtain the ground truth parameters from off-the-shelf 2D hand prediction systems. The size and location comes from the predicted bounding box from a hand detector~\cite{shan2020understanding} which typically defines hand region till wrist. The orientation is calculated from hand segmentation whose region is typically defined as all hand regions including hand and forearm. The approaching direction is calculated as the first principal component of the hand masks that centers on the predicted hand palm location. \sd{Should this be deleted? Seems to be copied from an earlier section...}
 \noindent\textbf{
Cropping Details.} We crop all objects with 80\% squared padding before resizing such that objects (hands) appear in similar (different) sizes. The model learns the priors of their relative scales, \eg, a hand to grasp a kettle appears much smaller than that of a mug (Fig 4). 

 
We show that the proposed method to obtain pixel-aligned pairs of HOI and object-only images is robust and can also be applied to more cluttered images. When there is more than one hand in the HOI image, we randomly select one to remove. We show results of applying our data construction method on the HOI4D (Fig~\ref{fig:more_hoi4d}) and the EPIC-KITCHEN (Fig~\ref{fig:more_epic}) datasets. 


\subsection{Baselines Implementation}\label{3.4}
\noindent\textbf{Pix2Pix\cite{pix2pix} (Sec4.1)} We modify the official Pix2Pix implementation\footnote{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix}. Given the predicted layout and the provided object image, we concatenate them channel-wise and pass them through 6 blocks of ResNet to output HOI images. The discriminator takes in the concatenation the of the object-only image, the splatted layout image, and generated HOI image and learns to discriminate between the real and fake domains.  We tried batchnorm and instancenorm and found that batchnorm generated better results in general but has some black holes if the background statistics deviate from that of the training set.

\noindent\textbf{VAE\cite{kingma2013auto} (Sec4.1)} VAE is notoriously known for being hard to balance for both generation variance and reconstruction quality. We sweep hyperparameters of the KL divergence loss's weights from $1, 1e-1, 1e-2, 1e-3, 1e-4$ and use $1e-3$ as it produces the highest contact recall. 


\noindent\textbf{GANHand\cite{ganhand} (Sec4.2)}
GANHand is originally proposed both to predict 3D MANO hands for images of YCB objects~\cite{calli2015benchmarking} and to optimize physical plausibility with respect to the known or reconstructed 3D shapes of YCB objects. We compare our method with their sub-network for grasp prediction from RGB images (blue branch in their original paper, Fig 4). The sub-network takes in the object's identity, the desk plane equation and the object's center in 3D space, in addition to the object image. Since these are not available in the HOI4D dataset, we set them to zeros. We apply an additional reconstruction loss for 3D hand joints, MANO hand parameters and camera parameters.  We finetune the network from the public checkpoints for another 10k iterations. 

\subsection{Scene Integration}\label{3.5}
 We integrate our object-centric HOI synthesis to scene-level affordance prediction. We first detect the objects in the scene and then expand the detected bounding box's size with the same pad ratio (0.8 of the original object size). However, when the scene is crowded, the extended object crops may include other objects thus distracting the layout generation. We instead crop the object with the detected bounding box and pad the cropped object with boundary values. This allows the network to generate hand interaction only for the object of interest. 


\subsection{Limitation and Failure Cases}\label{3.6}
Although it is encouraging that the proposed model can perform zero-shot generalization to the EPIC-KITCHEN dataset, the proposed method inherits limited generalization capabilities from general learning-based algorithms. The proposed model will fail when the object image's appearance deviates too much from the training set, \textit{e.g.} for too cluttered scenes, extreme lighting, very large objects (like a fridge) or very small objects (like a pin), \etc. The current model also cannot generate hands entering from the top of the frame or generate hands from a third-person's view due to the bias in the training set. These limitations require training with more diverse data. 
Additionally, the consistency of the hand's appearance and of the extracted hand poses can be further improved. 


\section{Qualitative Results}\label{qr}
Fig~\ref{fig:more_hoi4d} shows more examples of the constructed paired training data. We train all the models with a uniform mixture of inpainted and SDEdited object images.

Fig~\ref{fig:more_epic} shows that the proposed paired data construction is robust and can be applied to the EPIC-KITCHEN dataset.

Fig~\ref{fig:image_hoi4d} shows more comparisons of the generated HOI images by the proposed method (LDM-version as reported in tables) and other image synthesis baselines~\cite{ldm,pix2pix,kingma2013auto} on the HOI4D dataset.

Fig~\ref{fig:image_epic} shows more comparisons of the generated HOI images by the proposed method (LDM-version as reported in tables) and other image synthesis baselines~\cite{ldm,pix2pix,kingma2013auto} on EPIC-KITCHEN dataset.

Fig~\ref{fig:pose_hoi4d} shows more comparisons of the extracted 3D hand pose obtained by the proposed method and other 3D affordance baselines~\cite{ganhand,ldm} on the HOI4D dataset.

Fig~\ref{fig:pose_epic} shows more comparisons of the extracted 3D hand pose obtained by the proposed method and other 3D affordance baselines~\cite{ganhand,ldm} on the EPIC-KITCHEN dataset.

Fig~\ref{fig:glide_ldm} shows an ablation study on comparison of the LDM and GLIDE version of our model on HOI4D and EPIC-KITCHEN datasets. 

Fig~\ref{fig:more_inter} shows more layout editing results. 

Fig~\ref{fig:more_heatmap} shows more results of heatmap-guided synthesis. 

\clearpage
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig_supp/more_hoi4d.jpeg}
    \caption{Visualizing more examples of the constructed paired training data. We train all the models with a mixture of inpainted and SDEdited object images.}
    \label{fig:more_hoi4d}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig_supp/more_epic.jpeg}
    \caption{Visualizing the proposed paired data construction applied to EPIC-KITCHEN. }
    \label{fig:more_epic}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig_supp/image_hoi4d_ldm.pdf}
    \caption{Visualizing more comparisons of the generated HOI images from the proposed method and other image synthesis baselines \cite{ldm,pix2pix,kingma2013auto} on the HOI4D dataset. }
    \label{fig:image_hoi4d}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig_supp/image_epic_ldm.pdf}
    \caption{Visualizing more comparisons of the generated HOI images from the proposed method and other image synthesis baselines \cite{ldm, pix2pix,kingma2013auto} on the EPIC-KITCHEN dataset. }
    \label{fig:image_epic}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig_supp/pose_hoi4d.jpeg}
    \caption{Visualizing more comparisons of the extracted 3D hand pose from the proposed method and other 3D affordance baselines \cite{ganhand,ldm} on the HOI4D dataset. }
    \label{fig:pose_hoi4d}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig_supp/pose_epic.jpeg}
    \caption{Visualizing more comparisons of the extracted 3D hand pose from the proposed method and other 3D affordance baselines on the EPIC-KITCHEN dataset. }
    \label{fig:pose_epic}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig_supp/glide_ldm.jpeg}
    \caption{Visualizing the ablation of ContentNet for its LDM-based and GLIDE-based implementations (Sec~\ref{sec:content}).}
    \label{fig:glide_ldm}
\end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig_supp/interpolate.jpeg}
    \caption{Visualizing more layout editing results. }
    \label{fig:more_inter}
\end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig_supp/heatmap.jpeg}
    \caption{Visualizing more results of heatmap-guided synthesis.  }
    \label{fig:more_heatmap}
\end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig_supp/more_scene.pdf}
    % \includepdf[pages=-,pagecommand={},width=\textwidth]{fig_supp/more_scene.pdf}
    % \includepdf[width=\linewidth]{fig_supp/more_scene.pdf}
    \caption{Visualizing more scene integration results with the individual prediction from crops. }
    \label{fig:more_scene}
\end{figure*}


