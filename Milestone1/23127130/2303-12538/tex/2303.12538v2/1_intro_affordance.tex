\SL{An easier way is to begin with a synthesise story. I.e., Synthesizing an object is relatively mature, however, synthesizing a group of object interacting with each other is much more challenging, but rarely studied. Say something specifying what affordance is, what the task need to resolve, and why it is so hard (e.g., need to understand layout, content, interaction, articulation, everything ...); Say something how prior arts did (only human vs. environment setting is studied?), while non-rigid, articulated hand is even more challenging since human-environment interaction is more about locations and scales...}

\SL{Then from another angle, maybe say something about diffusion: on one hand, we found that diffusion models synthesize promising plausible HOI images, indicating the learned representation already knows the layout and the content; on thea other hand, all those models synthesized everything simultaneously. We are the very first one to perform conditional generation via diffusion network...}

\SL{Then talk about our method.}

\SL{Say something to contrast to previous work: 1. 2D heatmap; 2. GanHand, in order to highlight 1. more meanningful representation, and 2. in the wild setting. Then say something about applications, e.g., a shortcut for 3D articulation generation in the wild.}

\SL{Our contributions are xxx.}

\todo{reviewer: you seem to solve a much harder problem. why don't you directly predict 3D pose?... e.g. object --> 3D pose( -> pix2pix.........) generalziation? }


One more option for affordance representation, instead of superior to them. 
generative model, synthesize object is easy, synthesize interaction is hard... 
conditional diffusion, compared to alyosha's style 


% One of the fundamental goals of  vision is to enable agents to interact with the world. Affordance plays a central role  to bridge perception and action, both for biological agents \cite{gibson} and robots \cite{}. 

Affordance studies possible actions to be applied to an object, \eg  a mug affords holding, a button affords poking. Affordance serves as a bridge to connect perception and action. It has been studied by computer vision community over years as one of the fundamental goals of computer vision is to enable agents to interact with the world. \todo{modtivae hand-object instead of human-scene}


Over the past decades, researchers have made great progress to approach affordance problem. Early methods predict  affordable actions to an object, like holdable, pushable, liftable, \etc \cite{}. Besides outputting semantic labels, later approaches study to ground the action labels to corresponding object parts \cite{} and typically predicts a heatmap to indicate possibilities of that location to be interacted with. Such 2D prediction of action likelihood have been shown useful to guide robot manipulation and scale up to novel objects \cite{pinto}.  
\todo{Setup is tooooo long....}

However, predicting pixel-wise likelihood is only part of the story. Given a particular object, like the mug shown in Figure \ref{teaser}, we humans can not only predict interaction location, we are also able to hallucinate very specific ways to place our fingers in order hold it. In this work, we aim to synthesize realistic appearance of hand-object-interaction, from which 3D hand pose can be extracted. As affordance heatmaps facilitate robot manipulation with two-finger gripper, this task has potential impact to facilitate dexterous hand manipulation \cite{dexVIP} as human prior. 

Hallucinating images of human hands interacting with an object is very challenging. First, to supervise the network, we need to obtain pairs of images of object alone and hand interacting with the same object. It is \todo{too expensive? } to manually specify images of human hand grasping the objects. Second, the objects and hands largely engaged. 
Thrid, multi-modal. \todo{.....}


Our key insight is that diverse interactions largely come from hand-object layout while hand articulation is highly driven by the local geometry of the object.  A mug can be grasped either by its handle from one side or by its body from the other side. Once the grasping location is determined such as the handle of the mug,  the fingers would pinch much closer to that of mug body with only subtle differences in finger placement.  We operationalize this idea by proposing a two-step stochastic procedure: 
1) a layout sampling that generates diverse 2D spatial arrangement of hand and object that abstracts away hand articulation; 2) an articulation refinement network that is conditioned on the query object image and the sampled HOI layout to synthesize the images of hand-object interactions.  Both steps are implemented by diffusion models as they have shown impressive results in conditional image synthesis and sampling diversity. \todo{transition} We modify training objective of diffusion models to output valid HOI layout. 

To train such a system, we collect datasets with pairs of object-only images and HOI images. We first extract frames of hand in contact with an object.  Their corresponding object-only images are automatically generated by removing detected human from the scene via inpainting techniques. We also use several methods to prevent the learned model from overfitting to the inpainting artifacts. 


We evaluate our methods on HOI4D and VISOR dataset and compare with \todo{xxxx}. Our method generates more diverse xxx. Lastly, we show that 3D hand poses can be extracted from the synthesized HOI images. 


% Our contributions are summarized as below:
% \begin{itemize}
%     \item We propose a method based on diffusion models that \todo{not only? can we also ?} predicts a heatmap of affordance but also synthesize \todo{articulation-rich affordance.} 
%     \item 
%     \item We collect a large-scale datasets consists of xxK pairs of object-only images and hands interacting with them. 
% \end{itemize}



