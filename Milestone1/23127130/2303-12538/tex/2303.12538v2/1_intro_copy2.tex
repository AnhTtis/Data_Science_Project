Imagine a human hand interacting with objects such as bowls, bottles and cups shown in figure~\ref{fig:teaser}. Not only can one imagine the class of interactions (`grab/hold') but also hallucinate the vivid appearance of hands being in contact with and using the objects. Arguably, the ability to predict and hallucinate human-object-interactions (HOI) is critical to not only functional understanding but also visual imitation, and manipulation itself. Can our current vision algorithms do the same?

There has been a lot of progress in image generation -- synthesizing realistic high-resolution images spanning a wide range of object categories \cite{}, from human faces to ImageNet categories. Newer diffusion models such as Dall-E 2 \cite{dalle2} and Stable Diffusion\cite{rombach2022high} can generate remarkably novel images in diverse styles. Can these models perform on generating HOI images? Indeed, highly-realistic HOI images can be synthesized from text inputs such as ``a hand holding a cup'' \cite{dalle2,rombach2022high}. However, these models fail when conditioned on a particular object instance. 

Given an object, generating a realistic human object interaction is an extremely challenging problem. First, it requires an understanding of physical constraints such as collision and force stability. It also requires modeling semantics and functionality of objects - the underlying affordances. For example, kettle handles are preferred whereas knife blades are avoided. Finally, the generation of pixels themselves requires modeling occlusions and scale/lighting. 

In this work, we perform interaction synthesis with diffusion models. In contrast to a generic image-conditioned diffusion model, we build upon the classic idea in the affordance literature \cite{} that disentangles where to interact \js{(layout)} and how to interact \js{(content)}. The key insight is that diverse interactions largely come from hand-object layout while hand articulation is highly driven by the local geometry of the object.  A mug can be grasped either by its handle or by its body. But once the grasping location is determined (\eg mug handles), the fingers would place around the object's surface with only subtle differences in articulation.  We operationalize this idea by proposing a two-step stochastic procedure: 
1) a LayoutNet that generates 2D spatial arrangements of hands and objects, and
2) a ContentNet that is conditioned on the query object image and the sampled HOI layout to synthesize the images of hand-object interactions. \yy{I'm okay to delete the followling lines. }\js{i prefer having them there.} These two modules are connected by an articulation-agnostic hand proxy and both of them are implemented as image-conditioned diffusion models. 
% Furthermore, we propose a better architecture to generate layouts via diffusion model and we also introduce an additional loss term to train the model in the layout space. 

To supervise such a system, we need pairs of object-only images and HOI images that depict the exact same objects from the exact same viewpoint before and after hand interaction. We create a large-scale dataset via an inpainting technique \cite{nichol2021glide} that removes hands from HOI images to automatically generate corresponding object-only images. 
% We first extract frames of hand in contact with an object.  Their corresponding object-only images are automatically generated by removing detected human from the scene via inpainting techniques.
We also introduce a new data augmentation technique to prevent the learned model from overfitting to the inpainting artifacts. 


We evaluate our model on HOI4D and EPIC-KITCHEN datasets. The affordance synthesis method outperforms generic image generation baselines and the extracted hand poses from our HOI synthesis are favored against baselines that are trained to directly predict hand pose manner. We demonstrate the surprising generalization ability to EPIC-KITCHEN datasets. We also show that our model can be quickly adapted to new hand-object-interactions with only a few examples. Lastly, we show that our proposed method enables editing and guided generation by partially specifying layout parameters. This allows us to reuse heatmap prediction from prior works while grounding hand sizes of different objects to a consistent physical scale in one scene.

\subsection{Why human-object interaction generation?}
The idea of functional understanding and extracting affordances is core to visual understanding. However, it is unclear what does it mean to functionally understand an image? or what is the representation of affordances? Current literature has used categories (pushable, openable etc)~\cite{}, heatmaps (locations of interactions)~\cite{}, human/robot poses~\cite{} to represent affordances. Our approach takes it a step further and equates hallucinating the appearances of HOI as core to affordance extraction and functional understanding.








% \todo{lame opening .... o(╥﹏╥)o }Image generation has achieved remarkable progresses in synthesizing realistic high-resolution images spanning wide range of  object categories \cite{}, from human faces to ImageNet categories. Even highly-articulated structure like human hands can be synthesized with astonishing realism \cite{ganerated_hand}.   While single objects and hands can be individually synthesized, generating their interaction is hard as it requires the model to understand physical constraints between each other. The generated hands cannot be in arbitrary poses since they have to make contact with the objects. It also requires to understand \textit{object affordance} -- the possibility of object to be interacted. 
% The contact points on the objects follows certain patterns; for example, kettle handles are preferred whereas knife blades are avoided. 
% In this work, we focus on  hand-object-interaction generation (HOI) conditioned on images, \ie given an image of an object, we aim to synthesize human hands interacting with it. 

% % image-conditioned generation for hand-object-interaction --> hand-object-interaction generation (HOI) conditioned on images

% Generic image generation has advanced over years and has especially been driven by diffusion models very recently~\cite{ho2020denoising,song2020denoising}. On one hand, highly-realistic HOI images can be synthesized  from text inputs such as ``a hand holding a cup'' \cite{dalle2,stablediffusion}. While this indicates the models are capable of understating general hand-object-interaction's layout and appearance, these text-to-image models cannot condition  on a particular object instance. On the other hand, a body of works for image-conditioned generation are proposed. But they either only modify textures and style while preserving the structures \cite{sdedit, couairon2022diffedit}, or let a user to specify the layout to change  \cite{avrahami2022blendedlatent} (\eg inpaint a user-defined mask region). 
% In contrast, HOI image synthesis for a given object image is challenging as both layout and appearance are reasoned. For instance, the model needs to ensure that the added hand has a reasonable layout such as scale, approaching orientation w.r.t the object, pose, contacting region, and a convincing appearance such as illumination and shadow. Furthermore, while certain parts of the original object interacts with the added hand, we should keep the appearance of the remaining parts intact.
 
% In this work, we task the diffusion models with interaction synthesis \js{we perform interaction synthesis with diffusion models}. In contrast to a generic image-conditioned diffusion model, we build upon the classic idea in the affordance literature \cite{} that disentangles where to interact and how to interact. The key insight is that diverse interactions largely come from hand-object layout while hand articulation is highly driven by the local geometry of the object.  A mug can be grasped either by its handle from one side or by its body from the other side. Once the grasping location is determined such as the handle of the mug,  the fingers would pinch much closer to that of mug body with only subtle differences in finger placement.  We operationalize this idea by proposing a two-step stochastic procedure: 
% 1) a LayoutNet that generates 2D spatial arrangements of hands and objects, and
% % but abstracts away hand articulation to save computation; 
% 2) a ContentNet that is conditioned on the query object image and the sampled HOI layout to synthesize the images of hand-object interactions. These two modules are connected by an articulation-agnostic hand proxy and  both of them are implemented as image-conditioned diffusion models. Furthermore, we propose a better architecture to generate layouts via diffusion model and we also introduce an additional loss term to train the model in the layout space. 

  


% Our contributions are summarized as below:
% \begin{itemize}
%     \item We propose a method based on diffusion models that \todo{not only? can we also ?} predicts a heatmap of affordance but also synthesize \todo{articulation-rich affordance.} 
%     \item 
%     \item We collect a large-scale datasets consists of xxK pairs of object-only images and hands interacting with them. 
% \end{itemize}



