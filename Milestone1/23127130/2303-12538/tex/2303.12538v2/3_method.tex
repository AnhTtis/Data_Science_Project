
% \SL{We can squeeze the following paragraphs into 1 or at most 2 paragraphs. We only need to express out 2 things: 1. we have LayoutNet and ContentNet (names can be changed on your preference) where both constructed as conditional diffusion nets; and 2. We produce paired data to train these models. Then, we can note which one is in which section.}
\def\rvx{{\mathbf{x}}}
\def\rvc{{\mathbf{c}}}
\def\rmI{{\mathbf{I}}}
% \SL{"formally" formulations can go to 3.2, 3.3, etc...}

% \SL{I feel the paired data generation is non-trivial, it is a smart scheme to train diffusion models for such a task. We can spend one subsection or paragraph to show the idea and draw connection with how to train diffusion model for affordance.}
Given an image of an object, we aim to synthesize images depicting plausible ways of a human hand interacting with it. Our key insight is that this multi-modal process follows a coarse-to-fine procedure. For example, a mug can either be held by its handle or body, but once decided, the hand articulation is largely driven by the local geometry of the mug. We operationalize this idea by 
proposing a two-step stochastic approach as shown in Fig ~\ref{fig:pipeline}.  
% At the coarse level, layouts of hand and object are sampled to indicate possible spatial arrangement. At the fine level, the appearance is synthesized given the hand-object-interaction layout. 


Given an object image, we first use a LayoutNet  to predict plausible spatial arrangement of the object and the hand (Sec~\ref{sec:layout}). The LayoutNet predicts hand proxy that abstracts away appearance and explicitly specifies  2D location, size and approaching direction of a grasp. This abstraction allows global reasoning of hand-object relations and also enables users to specify the interactions. Then, given  the predicted hand proxy and the object image, we synthesize a plausible appearance of an HOI via a ContentNet (Sec~\ref{sec:appearance}).  This allows the network to implicitly reason about  3D wrist orientation, finger placement, and occlusion based on the object's local shape. We use conditional diffusion models for both networks to achieve high-quality layout and visual content.
The synthesized HOI image is realistic such that a feasible 3D hand pose can be directly extracted from it by an off-the-shelf hand pose reconstruction model (Sec \ref{sec:3d}).

% More formally, given an RGB image of an object $I^{obj}$, we aim to model the distribution of hand-object-interaction in the form of RGB images $I^{hoi} \sim p(I^{hoi}|I^{obj})$. We first sample a layout of hand-object-interaction $\theta \sim p(\theta |I^{obj})$. Conditioned on the predicted layout, we generate images of HOI images $p(I^{hoi} | \theta, I^{obj})$. Both steps are modeled by diffusion models. 


To supervise the system, we need pixel-aligned pairs of HOI images and object-only images that depict the exact same objects from the exact same viewpoints with the exact same lighting.  We obtain such pairs by inpainting techniques that remove humans from HOI images. We further propose a novel data augmentation to prevent the trained model from overfitting to the inpainting artifacts (Sec~\ref{sec:data}). 



\subsection{Preliminary: Diffusion models}
Diffusion models are probabilistic models~\cite{sohl-dickstein2015deep,ho2020denoising} that learn to generate samples from a data distribution $p(\rvx)$ by sequentially transforming samples from a tractable distribution $p(\rvx_T)$ (\eg, Gaussian distribution).
There are two processes in diffusion models: 1) a forward noise process $q(\rvx_t|\rvx_{t-1})$ that gradually adds a small amount of noise and degrades clean data samples towards the prior Gaussian distribution; 2) a learnable backward denoising process $p(\rvx_{t-1} | \rvx_t)$ that is trained to remove the added noise. The backward process is implemented as a neural network. During inference, a noise vector $\rvx_T$ is sampled from the Gaussian prior and is sequentially denoised by the learned backward model~\cite{song2020denoising,song2020score}. 
% The denoising process can take many forms and can be interpreted as solvers of certain stochastic differential equations~\cite{dockhorn2021score,zhang2022gddim}, or solvers of ordinary differential equations~\cite{lu2022dpm,karras2022elucidating,zhang2022fast}. 
The training of a diffusion model can be treated as training a denoising autoencoder for L2 loss~\cite{vincent2011a} at various noise levels, \textit{i.e.}, denoise $\rvx_0$ for different $\rvx_t$ given $t$. We adopt the widely used loss term in Denoising Diffusion Probabilistic Models (DDPM)~\cite{ho2020denoising,song2020denoising}, which reconstructs the added noise that corrupted the input samples. Specifically, we use the notation $\mathcal{L}_{\text{DDPM}}[\rvx; \rvc]$ to denote a DDPM loss term that performs diffusion over $\rvx$ but is also conditioned on $\rvc$ (that are not diffused or denoised): 
\begin{align}
\mathcal L_{\text{DDPM}}[\rvx; \rvc] = \mathbb E _{(\rvx, \rvc), \epsilon \sim \mathcal N(0, I), t} \|\rvx - D_\theta(\rvx_t, t, \rvc) \|_2^2,
\label{eq:ddpm}
\end{align}
where $\rvx_t$ is a linear combination of the data $\rvx$ and noise $\epsilon$, and $D_\theta$ is a denoiser model that takes in the noisy data $\rvx_t$, time $t$ and condition $\rvc$. This also covers the unconditional case as we can simply set $\rvc$ as some null token like $\varnothing$~\cite{ho2022classifier}. 
% Equivalently, one could also use the L2 loss over the injected noise $\epsilon$ and use a model that predicts $\epsilon$ (denoted as $\epsilon_\theta(\rvx_t, t, \rvc)$). \js{I feel that this is a bit tricky. we implement prediction over $\epsilon$, but in the paper, there are some references to L2 loss over the layout (Eq 2).}

% \todo{ preliminary, inpainting, and their problem }
% In our method, both LayoutNet and ContentNet are implemented as conditional diffusion models as they have shown great potential in image synthesis task. In particular, we initialize both our network with image-conditioned diffusion model that is pre-trained with large-scale image data for inpainting. Our insight is that the inpainting share desirable properties with our affordance prediction task. For example, the synthsized HOI images should retain the object appearance while the pre-trained model has learned to retain the unmasked region of the input image and attend to these unmasked area as context to hallucinate the mask-out region. \SL{Reading here people may wonder what is the difference of our method and inpainting? why not directly use inpainting model, anything else we need to do where inpainting model hasn't done?}

% \js{the location of the next paragraph seems weird, and is somewhat redundant given the earlier text in method?}

% 

% We based our affordance prediction networks, i.e., both the LayoutNet and ContentNet, on the diffusion inpainting model (\textit{e.g.}, Palette~\cite{saharia2022palette}), which is finetuned on the large-scale pretrained text-to-image diffusion network~\cite{nichol2021glide} with additionally initialized input channel (i.e., a mask, and the masked RGB image) weights.  Our networks share similar desirable properties with the inpainting diffusion model, i.e., while the inpainted image retains the unmasked region of the input image when hallucinating the mask-out region, the synthsized HOI image should also retain the object appearance. We empirically find that by replacing masked RGB image with unmasked object image in the input end, performance will be significantly improved due to that the affordance prediction networks see full context of the object. We refer to Sec. 3.3 and 3.4 for more detail of the network architecture.

% \todo{build up to what?  reuse inpainting model that takes in .. output ...  as they share ... }

% \todo{diffusion model for inpainting?. one para intro, one para for loss,one para for network? }

% To train such models, for a timestep $t$, the forward process adds a certain amount of Gaussian noise $\epsilon$ to the clean samples via a closed-form solution 
% $x_t = \sqrt{\bar \alpha_t}\epsilon + \sqrt{1 - \bar \alpha_t} x_0$.  
% $x_t \sim q(x_t | x_0) = \mathcal N (x_t; \sqrt{\bar \alpha_t} x_0, (1 - \bar \alpha_t) \mathbf{I})$. 
% \todo{check inconsistent alpha bar } The backward process implemented by a neural network $\theta$ is trained to output the reconstructed clean samples given the noisy one. \SL{we can stop here, just reserve this part only as preliminary. All the details of our method go to the following sections.} We adopt the weighted training objective from Ho \etal~\cite{ddpm} that predicts the residual $\epsilon_\theta$ to the reconstructed samples rather than the reconstruction itself. 

% $$
% \mathcal L_{DDPM} = \mathbb E _{x, \epsilon \sim \mathcal N(0, I), t} \|\epsilon - \epsilon_\theta(x_t, t) \|_2^2
% $$

% We use this vanilla loss in the articulation model and modify the vanilla term in the LayoutNet in order to diffuse in  the layout parameter space.



% \SL{\subsection{Object-conditional Diffusion Models} since both LayoutNet and ContentNet can be abstracted as object-image conditional models, shall we put one subsection here to introduce the inpainting diffusion model $->$ conditional diffusion model concept? \todo{how important is this point?}And Sec 3.5 \noindent\textbf{paired data generation} can be also moved here.}

\subsection{LayoutNet: predicting where to grasp }
\label{sec:layout}

Given an object image $\rmI^{obj}$, the LayoutNet aims to generate a plausible HOI layout $\bm l$ from the learned distribution $p(\bm l | \rmI^{obj})$. 
% The introduced hand proxy preserves coarse hand structure but abstracts away hand articulation and appearance in order to offload the network from synthesizing realistic appearance and to only focus on the task of spatial arrangement.  
We follow the diffusion model regime that sequentially denoises a noisy layout parameter to output the final layout. For every denoising step, the LayoutNet takes in the (noisy) layout parameter along with the object image and denoises it sequentially, \ie $\bm{l}_{t-1} \sim  \phi(\bm{l}_{t-1}|\bm l_{t}, \rmI^{obj}) $. We splat the layout parameter onto the image space to better reason about 2D spatial relationships to the object image and we further introduce an auxiliary loss term to train diffusion models in the layout parameter space. 

% is transformed into 2D image space to better reason about the spatial relations by directly comparing it with the object image in the image space. 


% More specifically, the layout is a lollipop-shape mask that is parameterized by a compact vector specifying hand location, size and approaching direction. For every denoising step,  the (noisy) layout vector is transformed into 2D image space to better reason about the spatial relations by directly comparing it with the object image in the image space. 
% Furthermore, we introduce an auxiliary loss term to train diffusion models in the layout parameter space where multiple parameters induce the same spatial layout. 


% % \todo{judy to sifei: right now I did not mention the hypothesis that layout is already learned from the diffusion.}
% Given an object image $I^{obj}$, the LayoutNet learns to generates plausible HOI layout from the learned distribution $p(l | I^{obj})$. The layout is a lollipop-shape masks that  abstracts away the hand articulation \SL{here we need to make clear (a) why we do not use an accurate mask; (b) why use a lollipop \todo{I was avoiding the word lollipop, sounds too fun to appear in paper}.} and is parameterized by a compact vector specifying hand location, size, and approaching orientation. \SL{We need to mention how to train the model, including: (1) we use paired data to train it (let's assume we already add 3.2), (2) via diffusion model, (3) instead to a 1D diffusion we splat them into 2D.} To better reasoning the spatial relations, the layout vector is splatted into 2D image space by a spatial transformation network~\cite{STN}. As multiple layout parameters could corresponds to the same layout,  the LayoutNet  propose a modified DDPM loss term to handle the many-to-one correspondence.  \SL{We can remove this detail.}

% \shubham{I feel this LayoutNet Section is too verbose, and spends a lot text on details that aren't very exciting. I assume all a reader would care about would be understanding: a) how we parametrize layout, b) how we predict it.
% For a) we spend 2 paragraphs, and also some parts of 'reasoning in 2D space' to discuss this. Instead, just 1 para in context of a figure might suffice -- we show canonical template whose shape is controlled by some parameters, and say our layout is this + transformation. and clarify we can also render it in image space as a mask $M(l)$. Dont think any reader would about the rectangle/circle etc. details here? 
% For b), we again spend a lot more text than needed? E.g. can just say we have a conditional diffusion model,  but it's important the model sees $M(l)$ instead of just $\bm l$.
% }

% \input{fig/fig_lollipop}
\noindent\textbf{Layout parameterization.}  Hands in HOI images typically appear as hands (from wrist to fingers) with forearms. Based on this observation, we introduce an articulation-agnostic hand proxy that only preserves this basic hand structure. As shown in Fig~\ref{fig:pipeline}, the layout parameter consists of hand palm size $a^2$, location $x,y$ and approaching direction $\arctan(b_1, b_2)$, \ie $\bm l:= (a, x, y, b_1, b_2)$. The ratio of hand palm size and forearm width $\bar s$ remains a constant that is set to the mean value over the training set.
We obtain the ground truth parameters from hand detection (for location and size) and hand/forearm segmentation (for orientation).



% We obtain the ground truth parameters from off-the-shelf 2D hand prediction systems. The size and location comes from the predicted bounding box from a hand detector~\cite{} which typically defines hand region till wrist. The orientation is calculated from hand segmentation whose region is typically defined as all hand regions including hand and forearm. The approaching direction is calculated as the first principal component of the hand masks that centers on the predicted hand palm location. 


% in i.e., the round head indicates the hand palm, and the narrow rectangular represents the forearm with approaching direction, while discarding any articulation details. 


\noindent\textbf{Predicting Layout.} The diffusion-based LayoutNet takes in a noisy 5-parameter  vector $\bm{l}_t$ with the object image and outputs the denoised layout vector $\bm{l}_{t-1}$ (we define $l_0 = l$). To better reason about the spatial relation between hand and object, we splat the layout parameter into  the image space $M(\bm{l}_t)$. The splatted layout mask is then concatenated with the object image and is passed to the diffusion-based LayoutNet.
We splat the layout parameter to 2D by the spatial transformer network~\cite{jaderberg2015spatial} that transforms a canonical mask template by a similarity transformation.  

% The 2D similarity transformation is determined from the layout parameters, more formally, \yy{equation goes to supp?}

% \begin{align*}
    % T_l = \begin{pmatrix}sR & t \\ 0 & 1\end{pmatrix} =  \begin{pmatrix}a^2 \hat b_1 & -a^2\hat b_2 & x \\  a^2 \hat b_2 & a^2 \hat b_1 & y \\ 0  & 0 & 1\end{pmatrix}
% \end{align*}
% where $\hat b_1, \hat b_2 $ is the normalized vector of $b_1, b_2$. 

% The lollipop-shape template in the canonical space is implemented as its circle being an isometric 2D Gaussian with  standard deviation 1 and its rectangle being a \todo{bandwidth? Gaussian}  with standard deviation $s$. The width of rectangle is calculated from the training data as the average ratio of the width of forearm and palm. 


\noindent\textbf{DDPM loss for layout.} 
% A typical diffusion model is supervised such that the model learns to output the added noise that corrupted the input images~\cite{ho2020denoising,song2020denoising}.
% $$
% \mathcal L_{DDPM} = \mathbb E _{x, \epsilon \sim \mathcal N(0, I), t} \|\epsilon - \epsilon_\theta(x_t, t) \|_2^2
% $$
One could directly train the LayoutNet with the DDPM loss (Eq.~\ref{eq:ddpm}) in the layout parameter space: $\mathcal L_{para} := \mathcal L_{\text{DDPM}}[\bm{l}; \rmI^{obj}]$. 
However, when diffusing in such a space, multiple parameters can induce an identical layout, such as a size parameter with opposite signs or approaching directions that are scaled by a constant.  
DDPM loss in the parameter space would penalize predictions even if they guide the parameter to a equivalent one that induce the same layout masks as the ground truth. 
% 
As the downstream ContentNet only takes in the splatted masks and not their parameters, we propose to directly apply the DDPM loss in the splatted image space (see appendix for details): 
\begin{align}
\mathcal L_{mask} = \mathbb E _{(\bm{l}_0, \rmI^{obj}), \epsilon \sim \mathcal N(0, I), t} \|M(\bm{l}_0) - M(\hat {\bm l}_0) \|_2^2.
% \text{where } \hat l_0 = \frac{1}{\sqrt{1 - \bar \alpha_t}} \bm{l}_t - \frac{\sqrt{\bar \alpha_t}}{\sqrt{1 - \bar \alpha_t}} \epsilon_\theta
\end{align}
where $\hat {\bm{l}}_0 := D_\theta(\bm{l}_t, t, \rmI^{obj})$ is the output of our trained denoiser that takes in the current noisy layout $\bm{l}_t$, the time $t$ and the object image $\rmI^{obj}$ for conditioning. 
% \js{how is noise added? it makes it look like that you sample x from dataset and add data there. but my understanding is that here you only use $I^{obj}$ as conditional signal, but with noise at $\bm l$,}


In practice, we apply losses in both the parameter space and image spaces $\mathcal L_{mask} + \lambda \mathcal L_{para}$ 
% (with hyperparameter $\lambda > 0$) 
because when the layout parameters are very noisy in the early diffusion steps, the splatted loss in 2D alone is a too-weak training signal.  

\noindent\textbf{Network architecture.} We implement the backbone network as a UNet with cross-attention layers and initialize it from the pretrained diffusion model~\cite{nichol2021glide}. The model takes in images with seven channels as shown in Fig~\ref{fig:pipeline}: 3 for the object image, 1 for the splatted layout mask and another 3 that blends the layout mask with object image.  The noisy layout parameter attends spatially to the feature grid from the UNet's bottleneck  and spit out the denoised output. 


\noindent\textbf{Guided layout generation. } The LayoutNet is trained to be conditioned on an object image only but the generation can be guided with  additional conditions at test time without retraining. For example, we can condition the network to generate layouts such that their locations are at certain places \ie $\bm l\sim p(\bm l_0|\rmI^{obj},x=x_0, y=y_0)$. We use techniques~\cite{song2020score} in diffusion models that hijack the conditions after each diffusion steps with corresponding noise levels. This guided diffusion enables user editing and HOI synthesis for scenes with a consistent hand scale (Sec.~\ref{sec:app}). Please refer to the appendix for LayoutNet implementation details. 

% For each diffusion step $t$,  the model takes in a noisy layout $\bm{l}_t$ represented by a 1D vector, a time embedding $t$, along with the object image and outputs the correction of the layout parameters $\epsilon_t$.  

\subsection{ContentNet: predicting how to grasp}
\label{sec:appearance}
Given the sampled layout $\bm l$ and the object image $\rmI^{obj}$, the ContentNet synthesizes a HOI image $\rmI^{hoi}$. While the synthesized HOI images should respect the provided layout, the generation is still stochastic because hand appearance may vary in shape, finger articulation, skin colors, \etc.  
We leverage the recent success of diffusion models in image synthesis and formulate the articulation network as a image-conditioned diffusion model.  
As shown in Fig~\ref{fig:pipeline}, at each step of diffusion, the network takes in channel-wise concatenation of the noisy HOI image, the object image and the splatted mask from the layout parameter and outputs the denoised HOI images $D_\phi(\rmI^{hoi}_t, t, [\rmI^{obj}, M(\bm l)])$. 

% \SL{If we decided to present both choices, i.e., glide+SR and LDM, we can split into 2 paragraphs. We can first introduce the 64x glide (or just say it an inpainting based diffusion model) first (e.g., in the previous paragraph), and talk about how to obtain high-res synthesize result? -> then talk about two choices, (1)xxx and (2)xxx... (e.g., in the following paragraphs)}

We implement the image-conditioned diffusion model in the latent space~\cite{ldm,vahdat2021score,sinha2021d2c} and finetune it from the inpainting model that is pre-trained on large-scale data. The pretraining is beneficial as the model has learned the prior of retaining the pixels in unmask region and hallucinate to fill the masked region. During finetuning, the model further learns to respect the predicted layout, \ie, retaining the object appearance if not occluded by hand and synthesizing hand and forearm appearance depicting finger articulation, wrist orientation, etc. 



\input{fig/data}
\subsection{Constructing Paired Training Data}
\label{sec:data}

% Moved from intro.
% To supervise such a system, we need pairs of object-only images and HOI images that depict the exact same objects from the exact same viewpoint before and after hand interaction. We create a large-scale dataset via an inpainting technique \cite{nichol2021glide} that removes hands from HOI images to automatically generate corresponding object-only images. 
% % We first extract frames of hand in contact with an object.  Their corresponding object-only images are automatically generated by removing detected human from the scene via inpainting techniques.
% We also introduce a new data augmentation technique to prevent the learned model from overfitting to the inpainting artifacts. 

To train such a system, we need pairs of object-only images and HOI image. These pairs need to be pixel-aligned except for the hand regions. One possible way is to use synthetic data~\cite{hasson2019learning, ganhand} and render their 3D HOI scene with and without hands. But this introduces domain gap between simulation and the real-world thus hurts generalization. We instead follow a different approach. 
% Furthermore, current 3D HOI datasets is limited by object shape variance and  hardly consider natural functional affordance (\eg knife is often grasped by handle not blade).  Hence, we instead directly learn from the real-world hand-object-interaction images that depict natural interaction from humans.  

As shown in Fig~\ref{fig:data}, we first extract object-centric HOI crops from egocentric videos with 80\% square padding. Then we segment the hand regions to be removed and pass them to the inpainting system~\cite{nichol2021glide} to hallucinate the objects behind hands. The inpainter is trained on millions of data with people filtered out therefore it is suitable for our task. 

\noindent\textbf{Data Augmentation.} Although the inpainting  generates impressive object-only images, it still introduces editing artifacts, which the networks can easily overfit to~\cite{zhang2020learning}, such as sharp  boundary and blurriness in masked regions. 
We use SDEdit~\cite{meng2021sdedit} to reduce the discrepancy between the masked and unmasked regions. SDEdit first adds a small amount of noise (we use $5\%$ of the whole diffusion process) to the given image and then denoises it to optimize overall image realism. However, although the discrepancy within images reduces, the unmasked object region is undesirably modified and the overall SDEdited images appear blurrier.  
In practice, we mix up the object-only images with and without SDEdit for training. 

% As shown in Fig~\ref{fig:data}, we first extract contact frames from egocentric videos~\cite{hoi4d}  when hands are detected as interacting with the objects. These active objects are cropped and centered. Then we extract the hand and forearm segmentation pass them to the inpainting system to remove human from the HOI images~\cite{glide}. We use the inpainting system that was trained on million of data with people filtered out therefore is a particular fit to our task. The ground truth of layout parameters are obtained from the hand bonding box detector and the hand segmentation as described in Sec~\ref{sec:layout}. 

% We further filter out the data where hands can still be detected after inpainting. This is typically due to inaccurate hand masks. 
We collect all data pairs from HOI4D~\cite{liu2022hoi4d}. After some automatic sanity filtering (such as ensuring hands are removed),  we generate 364k pairs of object-only images and HOI-images in total. We call the dataset HO3Pairs (Hand-Object interaction and Object-Only Pairs). We provide details and more examples of the dataset in the appendix. 

% Furthermore, we filter out the frames where the active objects that are too small (less than 0.1 of hand area) or too large (large than 25 times of hand area) in the HOI images because inpainting tends to fail on larger masked region (object too small like pins) and there are too few hand pixels to clearly depict articulation (object too large like moving sofa). We also filter out the data where hands can still be detected after inpainting. This typically comes from inaccurate hand masks. This results in a total of xxx 

% \noindent\textbf{Prevent Overfitting. }


