Consider the bottles, bowls and cups shown in the left column of Figure~\ref{fig:teaser}. How might a human hand interact with such objects?  Not only is it easy to imagine, from a single image, the types of interactions that might occur (\eg, `grab/hold'), and the interaction locations that might happen (\eg `handle/body'), but it is also quite natural to hallucinate---in vivid detail--- several ways in which a hand might contact and use the objects. This ability to predict and hallucinate hand-object-interactions (HOI) is critical to functional understanding of a scene, as well as to visual imitation and manipulation. 

Can current computer vision algorithms do the same?
On the one hand, there has been a lot of progress in image generation, such as synthesizing realistic high-resolution images spanning a wide range of object categories~\cite{sauer2022stylegan,kumari2022ensembling} from human faces to ImageNet classes. Newer diffusion models such as Dall-E~2~\cite{ramesh2022hierarchical} and Stable Diffusion~\cite{rombach2022high} can generate remarkably novel images in diverse styles. In fact, highly-realistic HOI images can be synthesized from simple text inputs such as ``a hand holding a cup''~\cite{ramesh2022hierarchical,rombach2022high}. 

On the other hand, however, such models fail when conditioned on an image of a particular object instance. 
Given an image of an object, it remains an extremely challenging problem to generate realistic human object interaction. Solving this problem requires (at least implicitly) an understanding of physical constraints such as collision and force stability, as well as modeling the semantics and functionality of objects --- the underlying affordances \cite{gibson1978ecological}. For example, the hand should prefer to grab the kettle handle but avoid grabbing the knife blade. Furthermore, in order to produce visually plausible results, it also requires modeling occlusions between hands and objects, their scale, lighting, texture, \etc.


In this work, we propose a method for interaction synthesis that addresses these issues using diffusion models. In contrast to a generic image-conditioned diffusion model, we build upon the classic idea of disentangling \emph{where} to interact (\emph{layout}) from \emph{how} to interact (\emph{content})~\cite{hermans2011affordance,gupta20113d}. Our key insight is that diverse interactions largely arise from hand-object layout, whereas hand articulations are driven by local object geometry.  For example, a mug can be grasped by either its handle or body, but once the grasping location is determined, the placement of the fingers depends on the object's local surface and the articulation will exhibit only subtle differences.  We operationalize this idea by proposing a two-step stochastic procedure: 
1) a \emph{LayoutNet} that generates 2D spatial arrangements of hands and objects, and
2) a \emph{ContentNet} that is conditioned on the query object image and the sampled HOI layout to synthesize the images of hand-object interactions. These two modules are both implemented as image-conditioned diffusion models. 



We evaluate our method on HOI4D and EPIC-KITCHEN~\cite{liu2022hoi4d,epic}. Our method outperforms generic image generation baselines, and the extracted hand poses from our HOI synthesis are favored in user studies against baselines that are trained to directly predict hand poses. We also demonstrate surprisingly robust generalization ability across datasets, and we show that our model can quickly adapt to new hand-object-interactions with only a few examples. Lastly, we show that our proposed method enables editing and guided generation from partially specified layout parameters. This allows us to reuse heatmap prediction from prior work~\cite{fang2018demo2vec,nagarajan2019iccv:hotspots} and to generate consistent hand sizes for different objects  in one scene.

%\subsection{Why human-object interaction generation?}
% \yy{or remove it to related work? Visual affordance from images?}
% \textit{Why HOI synthesis? An affordance perspective. } 
% Although the idea of functional understanding is core to visual understanding, it is not obvious what is the proper representation for object affordances.  Current literature has directly mapped images to categories (pushable, openable, \etc)~\cite{nagarajan2020ego,lee2015predicting}, heatmaps (locations of interactions)~\cite{fang2018demo2vec,nagarajan2019iccv:hotspots}, or human/robot poses~\cite{ganhand,mandikal2021learning}. In contrast we argue that a generative approach 
% produces a richer representation, capturing not only \emph{how} an object can move, \emph{where} it can be touched, or \emph{what} forces can be applied, but also the pose of an agent (\eg, a human hand) needed to execute such behavior.  

Our main contributions are summarized below: 1) we propose a two-step method to synthesize hand-object interactions from an object image, which allows affordance information extracted from it; 2) we use inpainting techinuqes to supervise the model with paired real-world HOI and object-only images and propose a novel data augmentation method to alleviate overfit to artifacts; and 3) we show that our approach generates realistic HOI images along with plausible 3D poses and generalizes surprisingly well on out-of-distribution scenes. 4) We also highlight several applications that would benefit from such a method. 





% \todo{lame opening .... o(╥﹏╥)o }Image generation has achieved remarkable progresses in synthesizing realistic high-resolution images spanning wide range of  object categories \cite{}, from human faces to ImageNet categories. Even highly-articulated structure like human hands can be synthesized with astonishing realism \cite{ganerated_hand}.   While single objects and hands can be individually synthesized, generating their interaction is hard as it requires the model to understand physical constraints between each other. The generated hands cannot be in arbitrary poses since they have to make contact with the objects. It also requires to understand \textit{object affordance} -- the possibility of object to be interacted. 
% The contact points on the objects follows certain patterns; for example, kettle handles are preferred whereas knife blades are avoided. 
% In this work, we focus on  hand-object-interaction generation (HOI) conditioned on images, \ie given an image of an object, we aim to synthesize human hands interacting with it. 

% % image-conditioned generation for hand-object-interaction --> hand-object-interaction generation (HOI) conditioned on images

% Generic image generation has advanced over years and has especially been driven by diffusion models very recently~\cite{ho2020denoising,song2020denoising}. On one hand, highly-realistic HOI images can be synthesized  from text inputs such as ``a hand holding a cup'' \cite{dalle2,stablediffusion}. While this indicates the models are capable of understating general hand-object-interaction's layout and appearance, these text-to-image models cannot condition  on a particular object instance. On the other hand, a body of works for image-conditioned generation are proposed. But they either only modify textures and style while preserving the structures \cite{sdedit, couairon2022diffedit}, or let a user to specify the layout to change  \cite{avrahami2022blendedlatent} (\eg inpaint a user-defined mask region). 
% In contrast, HOI image synthesis for a given object image is challenging as both layout and appearance are reasoned. For instance, the model needs to ensure that the added hand has a reasonable layout such as scale, approaching orientation w.r.t the object, pose, contacting region, and a convincing appearance such as illumination and shadow. Furthermore, while certain parts of the original object interacts with the added hand, we should keep the appearance of the remaining parts intact.
 
% In this work, we task the diffusion models with interaction synthesis \js{we perform interaction synthesis with diffusion models}. In contrast to a generic image-conditioned diffusion model, we build upon the classic idea in the affordance literature \cite{} that disentangles where to interact and how to interact. The key insight is that diverse interactions largely come from hand-object layout while hand articulation is highly driven by the local geometry of the object.  A mug can be grasped either by its handle from one side or by its body from the other side. Once the grasping location is determined such as the handle of the mug,  the fingers would pinch much closer to that of mug body with only subtle differences in finger placement.  We operationalize this idea by proposing a two-step stochastic procedure: 
% 1) a LayoutNet that generates 2D spatial arrangements of hands and objects, and
% % but abstracts away hand articulation to save computation; 
% 2) a ContentNet that is conditioned on the query object image and the sampled HOI layout to synthesize the images of hand-object interactions. These two modules are connected by an articulation-agnostic hand proxy and  both of them are implemented as image-conditioned diffusion models. Furthermore, we propose a better architecture to generate layouts via diffusion model and we also introduce an additional loss term to train the model in the layout space. 

  


% Our contributions are summarized as below:
% \begin{itemize}
%     \itemwe \todo{articulation-rich affordance.} 
%     \item 
%     \item We collect a large-scale datasets consists of xxK pairs of object-only images and hands interacting with them. 
% \end{itemize}



