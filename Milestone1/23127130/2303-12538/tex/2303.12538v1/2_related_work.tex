\noindent\textbf{Understanding Hand-Object-Interaction. } In order to understand hand-object-interaction, efforts have been made to locate the active objects and hands in contact in 2D space, via either  bounding boxes detection  \cite{shan2020understanding, bambach2015lending,mittal2011hand } or segmentation \cite{shan2021cohesiv,florence2020robot}. Many works reconstruct the underlying shape of hands and objects from RGB(D) images or videos by either template-based \cite{hamer2010object,brahmbhatt2020contactpose,tekin2019h+,sridhar2016real,garcia2018first} or template-free methods \cite{hasson2019learning,karunratanakul2020grasping,ye2022hand,corona2022lisa}. Furthermore, temporal understanding of HOI videos \cite{hamer2009tracking,girdhar2021anticipative,purushwalkam2020aligning,price2022unweavenet,spriggs2009temporal} aims to locate the key frames of state changes and time of contact. In our work, we use these techniques to extract frames of interests for data collection and to analyze the synthesis results. While these works recognize what is going on with the underlying hands and objects, our task is  to hallucinate what hands could possibly do with a given object. 


% Although the idea of functional understanding is core to visual understanding, it is not obvious what is the proper representation for object affordances.  Current literature has directly mapped images to categories (pushable, openable, \etc)~\cite{nagarajan2020ego,lee2015predicting}, heatmaps (locations of interactions)~\cite{fang2018demo2vec,nagarajan2019iccv:hotspots}, or human/robot poses~\cite{ganhand,mandikal2021learning}. In contrast we argue that a generative approach 
% produces a richer representation, capturing not only \emph{how} an object can move, \emph{where} it can be touched, or \emph{what} forces can be applied, but also the pose of an agent (\eg, a human hand) needed to execute such behavior.  

% As a result, we believe that hallucinating the appearance of HOI is core to affordance extraction and functional understanding.

\noindent\textbf{Visual Affordance from Images. } Affordance is defined as functions that environments could offer \cite{gibson1978ecological}. Although the idea of functional understanding is core to visual understanding, it is not obvious what is the proper representation for object affordances. Some approaches directly map images to categories, like holdable, pushable, liftable, \etc \cite{cai2016understanding,hermans2011affordance, nagarajan2020ego,lee2015predicting}.  Some other approaches ground these action labels to images by predicting heatmaps that indicate interaction possibilities \cite{nagarajan2019iccv:hotspots, liu2022joint, fang2018demo2vec,huang2018predicting,pan2017salgan}. While heatmaps only specify \textit{where} to interact without telling \textit{what} to do, recent approaches predict richer properties such as contact distance \cite{karunratanakul2020grasping}, action trajectory\cite{mo2021iccv:wheretoact,liu2022joint}, grasping categories\cite{goyal2022human,mandikal2022dexvip}, \etc.  
Instead of predicting  more sophisticated interaction states, we
% explore a  generative approach that produces a richer representation, capturing not only \emph{how} an object can move, \emph{where} it can be touched, or \emph{what} forces can be applied, but also the pose of an agent (\eg, a human hand) needed to execute such behavior. 
explore directly synthesizing HOI images for possible interactions because images demonstrate both \textit{where} and \textit{how} to interact comprehensively and in a straightforward manner. 


% Another line of works \cite{} regresses from images the parameters of a predefined human template in a top-down manner. They typically require annotations of the ground truth parameters hence prevent them from scaling up to in-the-wild data. In contrast \todo{this is not  contract to supervisoin.. but contrast to top-down}, we leverage recent success in conditional image generations to first hallucinate pixels before extracting hand poses. \todo{expalin why we do this.} \SL{Maybe we can left sentences describing ours in a separate paragraph? Just focusing on what is the current research and what needs to be resolved.}


\input{fig/fig_pipeline}

\noindent\textbf{3D Affordance. } While 3D prediction from images is common for human-scene interaction \cite{grabner2011makes,gupta20113d,fouhey2012people,li2019putting,PLACE:3DV:2020,PSI:2019}, 3D affordance for hand and object typically takes in \textit{known} object 3D shapes and predicts \textit{stable} grasps \cite{miller2004graspit,jiang2021hand,grady2021contactopt,pas2015using}. In contrast, our work predicts \textit{coarse} 3D grasps from RGB images of \textit{general} objects. The coarse but generalizable 3D hand pose prediction is shown as useful human prior for dexterous manipulation \cite{kokic2020learning,mandikal2022dexvip,dasari2022pgdm,wu2022learning,qin2022dexmv,bahl2022human}. 
While some recent works \cite{karunratanakul2020grasping,ganhand} generate 3D hand poses from images by direct regression, we instead first synthesize HOI images from which 3D hand poses are reconstructed afterwards. 



% \noindent\textbf{Object Placement. } HOI image synthesis is also related to object placement that seeks plausible location to insert new objects and then composites the inserted foreground and background together \cite{}. 
% % https://github.com/bcmi/Awesome-Object-Placement
% In these works, the foreground objects to add are often provided and only lighting and textures needs modification while we aim to hallucinate  highly non-rigid hands with plausible articulations. 
% We are inspired by their common techniques to generate training data by inpainting  


\noindent\textbf{Diffusion Models and Image Editing. } Diffusion models~\cite{ho2020denoising,song2020denoising} have driven significant advances in various domains~\cite{zeng2022lion,zhou20213d,tashiro2021csdi,xu2022geodiff,kong2020diffwave}, including image synthesis~\cite{ramesh2022hierarchical,saharia2022photorealistic,balaji2022ediffi,rombach2022high}. A key advantage of diffusion models over other families of generative models~\cite{kingma2013auto,goodfellow2020generative} is their ability to easily adapt to image editing and re-synthesis tasks without much training~\cite{gal2022image,ruiz2022dreambooth,kawar2022imagic}. 
While recent image-conditioned generative models achieve impressive results on various image translation tasks such as image editing \cite{meng2021sdedit,text2live,pix2pix}, style transfer\cite{li2022diffusion,patashnik2021styleclip}, the edits mostly modify textures and style, but preserve structures, or insert new content to user-specified regions
% . Works for interactive content creation can manipulate image structure and insert new content
\cite{avrahami2022blended,saharia2022palette,ramesh2022hierarchical}. In contrast, we focus on affordance synthesis where both layout (structure) and appearance are automatically reasoned about. 


% \noindent\textbf{Diffusion Models. } Diffusion models~\cite{HoJaiAbb20,SonSohKin20} have driven significant advances in various domains, such as text-to-image generation~\cite{ramesh2022hierarchical,saharia2022photorealistic,balaji2022ediffi}, natural language processing~\cite{li2022diffusion}, text-to-speech synthesis~\cite{kong2020diffwave}, 3d point cloud generation~\cite{zeng2022lion,ye2022first,zhou20213d}, time series modeling~\cite{tashiro2021csdi}, and molecular conformal generation~\cite{xu2022geodiff}. A key advantage of diffusion models over other families of generative models (such as variational autoencoders~\cite{kingma2013auto} and generative adversarial networks~\cite{goodfellow2020generative}) is its ability to easily adapt to image editing and re-synthesis tasks without much training (such as DreamBooth~\cite{ruiz2022dreambooth}, Textural Inversion~\cite{gal2022image}, and Imagic~\cite{kawar2022imagic}). 

% A signature of such methods is SDEdit~\cite{meng2021sdedit}, where the user can guide the synthesis of realistic images via colored strokes or image patches, all with a slightly different initialization to the diffusion process. SDEdit can be used with text-conditioned diffusion models~\cite{avrahami2022blended,avrahami2022blendedlatent} and is the method behind the recently popularized img2img method in Stable Diffusion~\cite{rombach2022high}. DiffEdit~\cite{couairon2022diffedit} is an extension of SDEdit that uses language prompts to automatically find editing masks with a text-conditioned denoiser, yet the mask has to act on existing objects in the image. In our work, we apply a diffusion model to synthesize hand-object-interaction which requires understanding both layout and appearance of HOI. \js{can we say that different from diffedit, our model has to infer the hand orientation independent from language?}
