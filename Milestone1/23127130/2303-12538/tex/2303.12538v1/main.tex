% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}
%\usepackage{todonotes}
% \newcommand{todo}[]{}
\usepackage{cuted}
\usepackage{bm}
\usepackage{tabulary}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

%%%%%%%%% PAPER ID  - PLEaffSE UPDATE
\def\cvprPaperID{2972} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}

\newcommand*{\yy}[1]{\textcolor{purple}{[Judy: #1]}}
\newcommand*{\SL}[1]{\textcolor{blue}{[SL: #1]}}
\newcommand*{\shubham}[1]{\textcolor{green}{[ST: #1]}}
\newcommand*{\XT}[1]{\textcolor{orange}{[XT: #1]}}
\newcommand*{\js}[1]{\textcolor{teal}{[JS: #1]}}
\newcommand*{\SB}[1]{\textcolor{red}{[SB: #1]}}
\newcommand*{\sd}[1]{\textcolor{brown}{[sd: #1]}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\vspace{-1.5ex}Affordance Diffusion: Synthesizing Hand-Object Interactions \vspace{-1.7ex}}
% \vspace{-10mm}
\author{
Yufei Ye\textsuperscript{1}\thanks{Yufei was an intern at NVIDIA during the project.} \qquad Xueting Li\textsuperscript{2} \qquad Abhinav Gupta\textsuperscript{1} \qquad Shalini De Mello\textsuperscript{2} \\ \qquad Stan Birchfield\textsuperscript{2} \qquad Jiaming  Song\textsuperscript{2}  \qquad Shubham Tulsiani\textsuperscript{1} \qquad Sifei Liu\textsuperscript{2}   \\
\textsuperscript{1}Carnegie Mellon University  \qquad \textsuperscript{2}NVIDIA \\
{\tt \small \href{https://judyye.github.io/affordiffusion-www}{https://judyye.github.io/affordiffusion-www}}
}


% }

% \author{Yufei Ye\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Hehehe\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }
\maketitle
\input{fig/teaser}

%%%%%%%%% ABSTRACT
\begin{abstract}
\vspace{-3mm}
Recent successes in image synthesis are powered by large-scale diffusion models.  However, most methods are currently limited to either text- or image-conditioned generation for synthesizing an entire image, texture transfer or inserting objects into a user-specified region. In contrast, in this work we focus on synthesizing complex interactions (\ie, an articulated hand) with a given object.  Given an RGB image of an object, we aim to hallucinate plausible images of a human hand interacting with it. 
We propose a two-step generative approach:  a LayoutNet that samples an articulation-agnostic hand-object-interaction layout, and a ContentNet that synthesizes images of a hand grasping the object given the predicted layout. Both are built on top of a large-scale pretrained diffusion model to make use of its latent representation. 
Compared to baselines, the proposed method is shown to generalize better to novel objects and perform surprisingly well on out-of-distribution in-the-wild scenes of portable-sized objects. The resulting system allows us to predict descriptive affordance information, such as hand articulation and approaching orientation. 


% \SL{see 0-abstract-sf.tex}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
\input{1_intro}

\section{Related Work}
\label{sec:related}
\input{2_related_work}

\section{Method}
\label{sec:method}
\input{3_method}


\section{Experiments}
\label{sec:exp}
\input{4_exp}

\section{Conslusion}
\label{sec:conclusion}
\input{5_conclusion}

\textbf{Acknowledgements.} This work is supported by NVIDIA Graduate Fellowship
to YY. The authors would like to thank Xiaolong Wang, Sudeep Dasari, Zekun Hao and Songwei Ge for helpful discussions. 

% ACK: xiaolong wang ??Sudeep?? sheng-yu? 
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix
\input{appendix}
\end{document}
