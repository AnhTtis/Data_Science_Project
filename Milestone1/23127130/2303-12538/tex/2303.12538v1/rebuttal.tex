\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[dvipsnames]{xcolor}
\usepackage{romannum}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand*{\SL}[1]{\textcolor{blue}{[SL: #1]}}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{2972} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Affordance Diffusion: Synthesizing Hand-Object Interactions}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

\vspace{-8mm}
%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
% We thank the reviewers and we are glad that reviewers think our design reasonable and find our analysis solid  \todo{summarize strength} 
We thank reviewers for valuable feedback.
We are glad that reviewers \Ro~and \Rt~recommend acceptance. 
While reviewer \Riii~finds our analysis solid and using the diffusion model in affordance synthesis new, they are mainly concerned with algorithm novelty and image quality. The latter is a common concern and we address it first. 
% our overall novelty \SL{probably remove 'novelty' since they concerned the architectures' novelty} and solid analysis,
% our novel  overall approach to  HOI synthesis, our in-depth analysis and dataset contribution, 
% their two major concerns are the limited novelty in network architecture along with a misunderstanding of our task as full 3D scene reconstruction of both hands and objects \SL{3D scene understanding? He/she didn't mention ``reconstruction''.}. 
% Before responding to each individual concern, we first address the common concern about image quality. 

\noindent\textbf{Image quality (common).} 
While there is room to improve quality,
we emphasize that image-conditioned HOI synthesis is very challenging.  Prior works that synthesize high-quality  general images like DALLE2 struggle in this task even when we provide prompts and manually drawn detailed masks (\romannum{3}, \romannum{5}~in the figure to the right). It struggles to satisfy hand structure, object affordance, scene physicality, \etc. It sometimes even fails to generate any hands.
% We compare with DALLE2 in Fig 1. Besides requiring manually drawn detailed masks(col3,5), its outputs often violate scene physicality (col6) and sometimes even fail to generate any hand. 
To address {\Riii}'s specific concern around ``blurry images cannot manifest hand structure", we note that as shown in Fig 1,5,7,8, the synthesized hand images are sufficiently sharp to extract correct 3D hand pose. We believe that even the current results could lead to many downstream applications. 
% We respectfully disagree with \Riii~that ``the blurry grasped gesture cannot manifest the hand structure". Instead, the current generation is sharp enough for the 3D hand poses to be extracted (Figs. 1,5,7,8). We believe that even the current results could lead to many downstream applications. 
 

\noindent\textbf{FID (common).} FID's range depends on the intrinsic data diversity and thus can differ very much. Datasets of human or cat faces are tightly cropped and well aligned so FID can be as low as 3.  
But much higher FID are commonly reported on less structured scenes,
\eg 130/143 (mean/median across all methods) in \href{https://infinite-nature.github.io/}{Infinite-Nature}, 128/111 in \href{https://openaccess.thecvf.com/content_ICCV_2019/papers/Henzler_Escaping_Platos_Cave_3D_Shape_From_Adversarial_Rendering_ICCV_2019_paper.pdf}{PlatoGAN} and 115/128 in \href{https://sunshineatnoon.github.io/texture-from-image/}{Texture Scraping}. 
For our task, we estimate the \textit{lower bound} FID on HOI4D to be 83 by computing FID between \emph{real} HOI images from training and test sets.
%We report our FID lower  bound as $83$ by comparing two \textit{real} HOI image sets.
% and it rises to $86$ when auto-encoding one set with the off-the-shelf latent autoencoder(L422). 
While recent techniques can further improve FID,
we also highlight that: 1) FID only evaluates image quality but not affordance or contacts, which is better reflected in CR. The user study shows that our method is \textit{overall} the most preferred one.  
2) FID is primarily determined by ContentNet (CN). VAE+CN and LayoutNet+CN (ours) produce the top two results in Tab 1, indicating that CN produces better image quality than the alternatives. 

% Moreover, FID from ContentNet is expected to further improve when finetuning 
% the off-the-shelf latent autoencoder as  we find autoencoding alone decreases the FID by 3 on real HOI image sets.  

% The two methods differ in layout generation whose key properties like object affordance and contact states cannot be well reflected in FID, but more in the CR.

% Our method is the second best in terms of FID, slightly worse than VAE-ContentNet. 
% However, the latter \textit{uses the same ContentNet as ours},  indicating that the proposed ContentNet synthesizes better image quality compared to other alternatives.   
% The two models differ in layout generation whose key properties like object affordance and contact states cannot be well reflected in FID \SL{but more in the CR.}. While FID and CR only evaluate certain aspects of performance, the user study shows that  overall our method is \textit{the most preferred} one.  

% \noindent\textbf{High FID (\Riii).}


\noindent\textbf{Cropping (\Ro).} We crop all objects with 10\% squared padding before resizing such that objects (hands) appear in similar (different) sizes. The model learns the priors of their relative scales, \eg, a hand to grasp a kettle appears much smaller than that of a mug (Fig 4). 


\noindent\textbf{Diversity (\Ro). } 
Compared to churches or flowers, hands naturally display less variance in shape and texture and their poses are largely constrained by object shape.
Nevertheless, our diversity (Fig 1) mainly manifests in approaching direction, contact points, finger articulations, \etc. Time interval 1:47--1:58 in the supp.\ video gives examples of diverse plausible interactions with the same object.
% various possible ways when interacting with a single object.
% Other properties like viewpoints and skin tones are limited by current egocentric training set but they are expected to improve with heavier efforts to collect more diverse in-the-wild data. 

% Some properties like grasp gestures are less diverse for a \textit{single object} because they highly depend on the object shape. But the method does generate different finger articulation and palm orientation for \textit{different objects}. 
% Other properties like viewpoints and skin tones are limited by current egocentric training set but they are expected to improve with heavier efforts to collect more diverse in-the-wild data. 

% As our method is scalable by only requiring detection and segmentation, 
% We expect the diversity to increase with heavier efforts to collect more diverse in-the-wild data. 

\input{fig/dalle.tex}


\noindent\textbf{Real-world applications (\Rt).}  Recent studies[2,12,50]  in robotics show that dexterous manipulation can be guided by human interaction priors learned by vision systems. We are hopeful that our work provides such a  prior across objects and are in fact exploring its transfer to robotics tasks. 

\noindent \textbf{Layout representation (\Rt).} 
While other layout representation is possible, our current choice is simple and effective 
such that the ContentNet can leverage priors from pretrained models (L424)
while only requiring detection/segmentation to get ground truth layouts.
We did not investigate hand skeleton because 1) obtaining its ground truth will be noisier due to occlusions, 2) PoseDiffusion (L729) that diffuses hand poses suggests that it is harder to generate a valid layout with more complicated parameters.

% As the representation gets more complicated like skeletons, although the ContentNet is more informed about the structure (if valid), it is harder for the LayoutNet to diffuse a valid skeleton. Furthermore, it also becomes harder to get the ground truth layout parameters especially when joints are occluded.
% In contrast, the current design is simple and effective 
% such that the ContentNet is informed enough to generate articulation
% as the generated HOI images manifest articulation to be extracted
% while only requiring detection/segmentation.
% We also hope our work inspires follow-ups on better layout representations. 
% While our work does not provide final words on the best layout representation, we hope our work would inspire followups. 

\noindent \textbf{Generalization (\Rt). } Besides kitchen objects, we show results of generalizing to other common portable objects, \eg, a thermometer and a mouse (supp Fig 10).  Our method currently does not generalize well to larger furniture like sofas, which is not present in our training data. We will elaborate on generalization ability in the revised version.

% \noindent \textbf{Generalization (\Rt). } The method is not limited to kitchen-related stuff but also works for common portable objects like thermometer and mouse (supp Fig10). We do not generalize well to very-large furniture like sofa, table because of training data. We will elaborate on generalization ability in the revised version.

\noindent \textbf{Hand template details (\Rt).} We describe it in supp A.1 and will make the reference clear in the revised version.

\noindent \textbf{Novelty (\Riii). } 
Our work, for the very first time, make innovative use of diffusion models to resolve a challenging yet practical task, \ie affordance synthesis, that has rarely been explored for generic objects. To do so, we propose a two-step pipeline and a dataset construction technique (L153).
While some components borrow network architecture from previous works, we verify in Sec4.1 that a naive diffusion model~(LDM), a simpler LayoutNet design (L676), disabling the proposed loss (Eq 2), or removing the data augmentation (Tab 2) all leads to inferior results, proving the value of our careful designed algorithm.

% Our biggest novelty lies in solving a new task, proposing a two-step pipeline and a dataset construction technique (L153).  While some components reuse networks from previous works, the design of the LayoutNet is not incremental for we propose a new way to diffuse spatial relations with better losses and architecture(L676). 


\noindent \textbf{Other views for scene understanding analysis (\Riii). } While we can reconstruct hands of a common size interacting with multiple objects in a scene (Fig 8), we do not reconstruct the object or the full scene because generic in-hand object reconstruction is extremely challenging and is certainly out of the paper's scope. Therefore we cannot provide the whole scene from another view.  
Yet, the proposed vision task is still useful to provide more informative human interaction priors [50] than prevalent 2D heatmaps.
% and it provides much richer information than prevalent 2D heatmaps. 
% It can provide a reasonable hand initialization to guide dexterous manipulation where depth can be provided by depth sensors when deployed on robots. 

\noindent\textbf{Table 2 (\Riii).} As described in L638, the gap shows that the model would overfit to the inpainting artifact without SDEdit augmentation. 
This also justifies our evaluation on real object images for all experiments.  

\noindent \textbf{Figures and Tables (\Riii). } We will present images in PDF format and provide more details in the image caption. 

% %%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

\end{document}
