% synthesize single object is easy  hand --> synthesize object is easy 

% conditoinal diffusion model --> techiinnial background
% current method's problem without mentioning ours. 

% image synthesizing has achieve progress in object / scene / synthesis --> we sytnhesize object affordance. example of task. affordance is useful... 

% given an object, we synthesize HOI... this is challenging in terms of .... 

% template-based method. --> bottome up method. diffusion model --> layout. specific model

\todo{lame opening .... o(╥﹏╥)o }Image generation has achieved remarkable progresses in synthesizing realistic high-resolution images spanning wide range of object categories \cite{}, from human faces to ImageNet categories. Even highly-articulated structure like human hands can be synthesized with astonishing realism \cite{ganerated_hand}.   While single objects and hands can be individually synthesized, generating their interaction is hard as it requires the model to understand physical constraints between each other. The generated hands cannot be in arbitrary poses since they have to make contact with the objects. It also requires to understand \textit{object affordance} -- the probability of object to be interacted. \js{is it probability or possibility?}
The contact points on the objects follows certain patterns; for example, kettle handles are preferred whereas knife blades are avoided. 
In this work, we focus on image-conditioned generation for hand-object-interaction \js{hand-object-interaction generation (HOI) conditioned on images}, \ie given an image of an object, we aim to synthesize human hands interacting with it. 

% HOI image synthesis is deeply connected to understanding object affordance -- the function that objects affords, as \todo{example}  The generated HOI images can also be considered as affordance prediction which is useful for the downstream robotic tasks. 

Generic image generation has advanced over years and has especially been driven by diffusion models very recently~\cite{ho2020denoising,song2020denoising}. On one hand, highly-realistic HOI\js{is HOI defined before?} images can be synthesized  from text inputs such as ``a hand holding a cup'' \cite{dalle2,stablediffusion}. While this indicates the models are capable of understating general hand-object-interaction's layout and appearance, these text-to-image models cannot condition  on a particular object instance. On the other hand, a body of works for image-conditioned generation are proposed. But they either only modify textures and style while preserving the structures \cite{sdedit, couairon2022diffedit}, or let a user to specify the layout to change (\eg inpaint a user-defined mask region) \cite{avrahami2022blendedlatent}. 
In contrast, HOI image synthesis for a given object image is challenging as both layout and appearance are reasoned. For instance, the model needs to ensure that the added hand has a reasonable layout such as scale, approaching orientation w.r.t the object, pose, contacting region, and a convincing appearance such as illumination and shadow. Furthermore, while certain parts of the original object interacts with the added hand, the non-interaction region should retain its appearance. \js{we should keep the appearance of the remaining parts intact.}

% This is very challenging . First, to supervise the network, we need to obtain pairs of images of object alone and hand interacting with the same object. It is \todo{too expensive? } to manually specify images of human hand grasping the objects. Second, the objects and hands largely engaged. 

% synthesizing HOI is useful (affordance)
% Synthesizing object interacting with hands is closely related to object affordance. 
% Affordance studies possible actions to be applied to an object \cite{gibson}, \eg  a mug affords holding, a button affords poking. 

% \todo{synthesizing HOI is useful --> relation to object affordance?}

% \todo{Prior works: image-conditioned synthesis either just modify textures and preserve structure, or the structure comes from by users. but HOI generation requires reasoning both. (how to motivate diffusion model? }

% \SL{Say sth like: Meanwhile, large-scale diffusion models [] have been substantially advance the text-conditioned image generation. E.g., an authentic HOI image can (very likely) be synthesized by saying ``a hand holding a mug'' (here maybe find some example/link we can cite/refer), indicating the model is capable of (1) understanding interaction layouts and (2) creating plausible texture appearance. However, the model has to synthesize both the object and the hand simultaneously.  In another thread, a body of image-conditioned synthesis methods are proposed, mostly focus on the appearance: they either just modify textures while preserving the structure (e.g., style transfer on the original objects []), or let users to specify the layout (e.g., inpaint a user defined mask region []). Conditional synthesis that reasoning both the structure and the appearance remains unexplored.}
% Image-conditioned generation are well studied and can modify both the style and content of a given image. Style can be transferred ... New object can be inserted into 

% \SL{I felt the 1st and the 2nd paragraphs are two seperate openings, they seem equally strong and can lead two different stories...Maybe need some links in between.}
% Object affordance prediction is challenging due to lack of supervision and heavy mutual occlusion between hand and object. One line of prior works pursue pixel-wise prediction as affordance representations. Early methods output heatmaps to indicate interaction probability at each pixel \cite{}. While heatmaps alone does not specify what to do for a probable location, later approaches equip more properties to read-off from each pixel such as object material \cite{}, action trajectory, grasping categories, \etc.  As one could continue to devise pixel-wise predictions with more and more descriptive properties \SL{but the aforementioned properties are not all pixel-wise?}, we argue that encoding holistic interaction information into each and every pixel is hard and instead pursue to synthesize images of hand-object-interaction for one possible grasp. \SL{I feel this argue is not very strong: there seems no information why it is hard; nor reason why synthesize is a must.}
% Another line of works \cite{} regresses from images the parameters of a predefined human template in a top-down manner. They typically require annotations of the ground truth parameters hence prevent them from scaling up to in-the-wild data. In contrast \todo{this is not  contract to supervisoin.. but contrast to top-down}, we leverage recent success in conditional image generations to first hallucinate pixels before extracting hand poses. \todo{expalin why we do this.} \SL{Maybe we can left sentences describing ours in a separate paragraph? Just focusing on what is the current research and what needs to be resolved.}

% In this work, we task the diffusion model to affordance prediction \SL{need to define affordance (e.g., draw connection with ``interaction'')}, in particular generating hand-object-interaction images. It is a challenging image-conditioned generation tasks as new content (hand) is added to change the original image layout. Furthermore, the location of new content is not specified by users but is reasoned by the model. Lastly, while certain part of the original object interacts with the added hand, the non-interaction region should retain its appearance. \todo{too dry.. }\SL{Draw connection to the previous paragraph.}

% \SL{In this work, we focus on image-conditioned generation for hand-object-interaction, \ie given an image of an object, we aim to synthesize human hands interacting with it. As in contrast to the aforementioned efforts, this task reasons both the layout and the appearance, which is way more challenging. For instance, the model needs to ensure the added hand presenting a reasonable scale, approaching orientation w.r.t the object, pose, contacting region, and appearance such as illumination and shadow. Lastly, while certain part of the original object interacts with the added hand, the non-interaction region should retain its appearance.}

% Image synthesis has advanced over years, being able to synthesize various objects like faces, cars, and even ImageNet categories. However, synthesizing a group of entities interacting with each other is less studied. Most of the prior works 

% An easier way is to begin with a synthesise story. I.e., Synthesizing an object is relatively mature, however, synthesizing a group of object interacting with each other is much more challenging, but rarely studied. Say something specifying what affordance is, what the task need to resolve, and why it is so hard (e.g., need to understand layout, content, interaction, articulation, everything ...); Say something how prior arts did (only human vs. environment setting is studied?), while non-rigid, articulated hand is even more challenging since human-environment interaction is more about locations and scales...

% \SL{Then from another angle, maybe say something about diffusion: on one hand, we found that diffusion models synthesize promising plausible HOI images, indicating the learned representation already knows the layout and the content  \todo{emmm}; on thea other hand, all those models synthesized everything simultaneously. We are the very first one to perform conditional generation via diffusion network...}

% \SL{Then talk about our method.}

% \SL{Say something to contrast to previous work: 1. 2D heatmap; 2. GanHand, in order to highlight 1. more meanningful representation, and 2. in the wild setting. Then say something about applications, e.g., a shortcut for 3D articulation generation in the wild.}

% \SL{Our contributions are xxx.}

% \todo{reviewer: you seem to solve a much harder problem. why don't you directly predict 3D pose?... e.g. object --> 3D pose( -> pix2pix.........) generalziation? }


% One more option for affordance representation, instead of superior to them. 
% generative model, synthesize object is easy, synthesize interaction is hard... 
% conditional diffusion, compared to alyosha's style 


% One of the fundamental goals of  vision is to enable agents to interact with the world. Affordance plays a central role  to bridge perception and action, both for biological agents \cite{gibson} and robots \cite{}. 

% Affordance studies possible actions to be applied to an object, \eg  a mug affords holding, a button affords poking. Affordance serves as a bridge to connect perception and action. It has been studied by computer vision community over years as one of the fundamental goals of computer vision is to enable agents to interact with the world. \todo{modtivae hand-object instead of human-scene}


% Over the past decades, researchers have made great progress to approach affordance problem. Early methods predict  affordable actions to an object, like holdable, pushable, liftable, \etc \cite{}. Besides outputting semantic labels, later approaches study to ground the action labels to corresponding object parts \cite{} and typically predicts a heatmap to indicate possibilities of that location to be interacted with. Such 2D prediction of action likelihood have been shown useful to guide robot manipulation and scale up to novel objects \cite{pinto}.  
% \todo{Setup is tooooo long....}

% However, predicting pixel-wise likelihood is only part of the story. Given a particular object, like the mug shown in Figure \ref{teaser}, we humans can not only predict interaction location, we are also able to hallucinate very specific ways to place our fingers in order hold it. In this work, we aim to synthesize realistic appearance of hand-object-interaction, from which 3D hand pose can be extracted. As affordance heatmaps facilitate robot manipulation with two-finger gripper, this task has potential impact to facilitate dexterous hand manipulation \cite{dexVIP} as human prior. 


% We borrow the classic idea in affordance prediction literature that where and what ... 
In this work, we task the diffusion models with interaction synthesis \js{we perform interaction synthesis with diffusion models}. In contrast to a generic image-conditioned diffusion model, we build upon the classic idea in the affordance literature \cite{} that disentangles where to interact and how to interact. The key insight is that diverse interactions largely come from hand-object layout while hand articulation is highly driven by the local geometry of the object.  A mug can be grasped either by its handle from one side or by its body from the other side. Once the grasping location is determined such as the handle of the mug,  the fingers would pinch much closer to that of mug body with only subtle differences in finger placement.  We operationalize this idea by proposing a two-step stochastic procedure: 
1) a LayoutNet that generates 2D spatial arrangements of hands and objects, and
% but abstracts away hand articulation to save computation; 
2) a ContentNet that is conditioned on the query object image and the sampled HOI layout to synthesize the images of hand-object interactions. These two modules are connected by an articulation-agnostic hand proxy and both of them are implemented as image-conditioned diffusion models. Furthermore, we propose a better architecture to generate layouts via diffusion model and we also introduce an additional loss term to train the model in the layout space. 

To supervise such a system, we create a dataset with pairs of object-only images and HOI images that depict objects before and after hand interaction. We first extract frames of hand in contact with an object.  Their corresponding object-only images are automatically generated by removing detected human from the scene via inpainting techniques. We also introduce a new data augmentation technique to prevent the learned model from overfitting to the inpainting artifacts. 


We evaluate our model on HOI4D and EPIC-KITCHEN datasets. The affordance synthesis method outperforms generic image generation baselines and the extracted hand poses from our HOI synthesis are favored against baselines that is trained to directly predict hand pose manner. We demonstrate the surprising generalization ability to EPIC-KITCHEN datasets. 
We also show that our model can be quickly adapted to new hand-object-interactions with only a few examples. Lastly, we show that our proposed method enables editing and guided generation by partially specifying layout parameters. This allows us to reuse heatmap prediction from prior works while grounding hand sizes of different objects to a consistent physical scale in one scene.  


% Our contributions are summarized as below:
% \begin{itemize}
%     \item We propose a method based on diffusion models that \todo{not only? can we also ?} predicts a heatmap of affordance but also synthesize \todo{articulation-rich affordance.} 
%     \item 
%     \item We collect a large-scale datasets consists of xxK pairs of object-only images and hands interacting with them. 
% \end{itemize}



