Recently, image synthesis and editing have been tremendously advanced by large-scale diffusion models. However, most methods are currently limited to either text or image-conditioned generation that focuses on synthesizing an entire image, texture transfer or inserting objects with a user-specified region. In this work we demonstrate the ability to synthesize complex interactions (\ie, an articulated hand) with a given object. E.g., on top of synthesizing plausible hand appearance, a proper grasping hand pose interacting with a provided mug need to be determined. Our approach, which we call xxx, leverages a LayoutNet that samples an articulation-agnostic hand-object-interaction layout, and a ContentNet that synthesizes images of hand grasping the object given the predicted layout. Both are built on top of a large-scale pretrained diffusion model to make use of its latent representation. We demonstrate that the proposed method generalizes better to novel objects and even out-of-distribution scene. The resulting system allows us to predict affordance more descriptive than 2D heatmap such as hand articulation, approaching orientation, which can be extracted for the downstream tasks.