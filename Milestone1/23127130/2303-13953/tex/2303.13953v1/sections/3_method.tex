% !TEX root = ../main.tex
\section{AssetField}
\label{sec:method}

In this work, we primarily consider a branch of real-world application scenarios that require fast and high-quality rendering of scenes whose configuration is subject to change, such as interior design, urban planning and traffic simulation. 
In these cases, objects are mainly placed on some dominant horizontal plane, and is commonly manipulated with insertion, deletion, translation on the horizontal plane, and rotation around the vetical axis, etc.

We first introduce our ground feature plane representation in Sec.~\ref{subsec:ground_feature_plane} to model each neural field.
Sec.~\ref{subsec:asset_mining} describes the process of assets mining with the inferred the ground feature plane. 
We further leverage the color and semantic feature planes to categorize objects in an unsupervised manner, which is illustrated in Sec.~\ref{subsec:asset_grouping}. 
Finally, Sec.~\ref{subsec:asset_library} demonstrates the construction of a cross-scene asset library that enables versatile scene editing.


\subsection{Ground Feature Plane Representation}
\label{subsec:ground_feature_plane}
Ground plan has been commonly used for indoor and outdoor scene modeling~\cite{sharma2022seeing,devries2021unconstrained,Saha2022TranslatingII}.
% where object arrangement of a scene is mainly reflected on a horizontal plane. 
We adopt a similar representation to parameterize a 3D neural field with a 2D ground feature plane $\mathcal{M}$ of shape $L\times W \times N$, and a globally encoded vertical feature axis $\mathcal{H}$ of shape $H \times N$, where $N$ is the feature dimension. A query point at coordinate $(x,y,z)$ is projected onto $\mathcal{M}$(plane) and $\mathcal{H}$(axis) to retrieve its feature values $m$ and $h$ via bilinear/linear interpolation:
\begin{equation}
		m=\operatorname{Interp}(\mathcal{M},(x,y)), 
		h=\operatorname{Interp}(\mathcal{H},z), 
\end{equation}	
which are then combined and decoded into the 3D scene feature via a MLP decoder.
Concretely, a 3D radiance field is parameterized by a set of ground feature planes $\mathcal{M}$=$(\mathcal{M}_\sigma, \mathcal{M}_{c})$, and vertical feature axes $\mathcal{H}$=$(\mathcal{H}_\sigma, \mathcal{H}_{c})$, for the density and color fields respectively.
The retrieved feature values $m$=$(m_{\sigma},m_{c})$ and $h$=$(h_{\sigma}, h_{c})$ are then combined and decoded into point density $\sigma$ and view-dependent color $c$ values by two decoders $Dec_\sigma$, $Dec_{rgb}$.
Points along a ray $\mathbf{r}$ are volumetrically integrated following~\cite{mildenhall2020nerf}:
\vspace{-5pt}
\begin{equation}
	\vspace{-5pt}
	\hat{C}(\mathbf{r})=\sum_{i=1}^N T_i\left(1-\exp \left(-\sigma_i \delta_i\right)\right) c_i,
	\label{eq:volumn-rendering}
\end{equation}
where $T_i=\exp (-\sum_{j=1}^{i-1} \sigma_j \delta_j)$, and supervised by the 2D image reconstruction loss with $\sum_{\mathbf{r}}(\|\hat{C}(\mathbf{r})-C(\mathbf{r})\|_2^2)$, where $C(\mathbf{r})$ is the ground truth pixel color. 

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{figs/groundplane_comparison.pdf}
	\vspace{-5pt}
	\caption{\small TensoRF with full $3D$ factorization produces noisy feature planes; our ground plane representation yields informative features that clearly illustrated scene contents and layout after discretization, especially in the density field. 
	{\color{red}Red} boxes: two spatially close objects can be clearly separated on the density plane but not the RGB plane. 
	{\color{cyan}Blue} boxes: objects with similar geometry but different appearance can be distinguished on the RGB plane but not the density plane.}
	\vspace{-15pt}
	\label{fig:tensorf_vs_groundplan}
\end{figure}

Such neural representation are beneficial to our scenario. 
Firstly, the ground feature planes are naturally aligned with the BEV of the scene, mirroring the human approach to high-level editing and graphic design, where artists and designers mainly sketch on 2D canvas to reflect a 3D scene. 
Secondly, the globally encoded vertical feature axis encourages the ground feature plane to encode more scene information, which aligns better with scene contents. 
Thirdly, this compact representation is more robust when trained with sparse view images, where the full 3D feature grids are easy to overfit under insufficient supervision, producing noisy values, as depicted in Fig.~\ref{fig:tensorf_vs_groundplan}. 

\begin{figure*}
	\centering
	\includegraphics[width=0.98\linewidth]{figs/pipeline_v6.pdf}
	\vspace{-5pt}
	\caption{\small Overview of \emph{AssetField}. (a) We demonstrate on a scene without background for clearer visuals. (b) The proposed ground feature plane representation factorizes a neural field into a horizontal feature plane and a vertical feature axis. (c) We further integrate color and semantic field into a 2D neural plane, which is decoded into 3D-aware features with the geometry guidance from scene density. The inferred RGB-DINO plane is rich in object appearance and semantic clues whilst being less sensitive to vertical displacement between objects, on which we can (d) detect assets and grouping them into categories. (e) For each category, we select a template object and store its density and color ground feature patches into the asset library. A cross-scene asset library can be construct by letting different scenes fit there own ground feature planes whilst sharing the same vertical feature axes and decoders/renderers.}
	\vspace{-18pt}
	\label{fig:pipeline}
\end{figure*}

\subsection{Assets Mining on Ground Feature Plane}
\label{subsec:asset_mining}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/background_stackobj.pdf}
	\vspace{-15pt}
	\caption{\small Top: Simple scene background can be filtered on the learned ground feature plane in advance using feature clustering. Bottom: (a) Nested structure can be separated by (c) firstly identify the enclosed chair, then set its value to background feature for table patch. (b) Items placed on top of a surface can be detected by (d) another round of filtering that treats table surface as background.}
	\vspace{-20pt}
	\label{fig:hierarchical_structure}
\end{figure}

For the ease of demonstration, let us first consider a simplified case where objects are scattered on an invisible horizontal plane, as in Fig.~\ref{fig:pipeline}~(a).
On scenes with a background, a pre-filtering step can be performed on the learned ground feature plane as illustrated in Fig.~\ref{fig:hierarchical_structure}.
We start from modeling the radiance field,
where a set of ground features planes $\mathcal{M}$=$(\mathcal{M}_\sigma, \mathcal{M}_{c})$ describing scene density and color are inferred following the formulation in Sec.~\ref{subsec:ground_feature_plane}.
It can be observed that $\mathcal{M}_\sigma$ tends to exhibit sharper object boundaries compared to the color feature plane, as shown in the red boxes in Fig.~\ref{fig:tensorf_vs_groundplan}.
This could be attributed to the mechanism of neural rendering (Eq.~\ref{eq:volumn-rendering}), where the model firstly learns a clean and accurate density field to guide the learning of the color field. 
We therefore prefer to use $\mathcal{M}_\sigma$ for assets mining.
%
In the example scene, the feature plane is segmented into two clusters with K-means~\cite{Lloyd1982LeastSQ} to obtain a binary mask of the objects.
Contour detection~\cite{Suzuki1985TopologicalSA,opencv_library} is then applied to locate each object, resulting in a set of bounding box. 
Note that the number of clusters can be customized according to the objects users want to highlight. 
In more complex scenarios where objects are arranged in a hierarchical structure,~(\eg computer - table - floor), the clustering step can be repeated to gradually unpack the scene, as illustrated in Fig.~\ref{fig:hierarchical_structure}. 
With the bounding boxes, a collection of object neural representations $\mathcal{P}$=$\{(p_{\sigma}, p_{c})\}$ can be obtained, which are the enclosed feature patches on $\mathcal{M}_\sigma$ and $\mathcal{M}_{rgb}$. 
To address complex real-world scenes, we take inspiration from previous works ~\cite{kobayashi2022distilledfeaturefields,tschernezki22neural} that models a DINO~\cite{Caron2021EmergingPI} field to guide the learning a semantic-aware radiance field.
Similarly, we can learn a \emph{separate} DINO ground feature plane $\mathcal{M}_{dino}$ to provide more explicit indications of object presence. 
As AssetField models a set of separate fields, object discovery can be conducted on any field that offers the most distinctive features in a scene dependent manner.

At this point, users can intuitively edit the scene with object feature patches $\mathcal{P}$, \eg,``paste" $(p_{\sigma}^{i}$, $p_{c}^{i})\in\mathcal{P}$ to the designated location on $\mathcal{M}_\sigma$ and $\mathcal{M}_{rgb}$ to insert object $i$. The edited ground feature planes $\mathcal{M'}$=$(\mathcal{M'}_\sigma, \mathcal{M'}_{c})$ get paired with the original vertical feature axes $\mathcal{H}$=$(\mathcal{H}_\sigma, \mathcal{H}_{c})$ to decode 3D information using the original $Dec_\sigma$ and $Dec_{rgb}$.

\subsection{Unsupervised Asset Grouping}
\label{subsec:asset_grouping}
Despite being versatile, users can only interact with individual instances in $\mathcal{P}$ from the learned ground planes, whereas group editing is also a desirable feature in real-world applications, especially when objects of the same category need to be altered together. While the definition of object category can be narrow or broad, here we assume that objects with close appearance and semantics are analogues and use RGB and semantic feature plane for assets grouping. A case where the density features fail to distinguish two visually different objects is highlight in Fig.~\ref{fig:tensorf_vs_groundplan}.

\noindent \textbf{Occupancy-Guided RGB-DINO field.} 
As our goal is to ``self-discover" assets from neural scene representation, there is no extra prior on object category to regularize scene features. 
3D voxel-based methods such as those described in ~\cite{liu2020neural,Yu2022PlenoxelsRF}, may learn different sets of features to express the same objects, as grid features are independently optimized. 
Such issue can be alleviated by our proposed neural representation, where the ground feature plane $\mathcal{M}$ is constrained by the globally shared vertical feature axis $\mathcal{H}$. 
Concretely, given two identical objects $i,j$ placed on a horizontal flat surface, the same feature chunk on $\mathcal{H}$ will be queried during training, which constraints their corresponding feature patches $p_i$ and $p_j$ to be as similar as possible so that they can be decoded into the same set of 3D features.
However, such constraint no longer holds when there is a vertical displacement among identical objects~(\eg one the ground and one on the table), where different feature chunks on $\mathcal{H}$ are queried, leading to divergent $p_i$ and $p_j$.

To learn a more object-centric ground feature plane rich in color and semantics clues, 
we propose to integrate the color and semantic fields by letting them share the same set of ground feature planes, denoted by $\mathcal{M}_{rgb{\text -}dino}$. 
Instead of appending a vertical feature axis, here we use scene density features to guide the decoding of $\mathcal{M}_{rgb{\text -}dino}$ into 3D-aware features, as illustrated in Fig.~\ref{fig:pipeline}(c).
It can be interpreted as $\mathcal{M}_\sigma$ and $\mathcal{H}_\sigma$ fully capture the scene geometry, while $\mathcal{M}_{rgb{\text -}dino}$ captures the `floorplan' of scene semantics layouts and appearances. 
For a query point at $(x,y,z)$, its retrieved density feature $m_\sigma$ and $h_\sigma$ are mapped to a color feature $v_{rgb}$ and a semantic feature $v_{dino}$ via two MLPs, 
which are then decoded into scene color $c$ and semantic $f_{dino}$ along with the RGB-DINO plane feature $m_{rgb{\text -}dino}=\operatorname{Interp}(\mathcal{M}_{rgb{\text -}dino},(x,y))$ via $Dec_{rgb}$ and $Dec_{dino}$.


\smallskip
\noindent \textbf{Assets Grouping and Template Matching.} 
On the inferred RGB-DINO ground feature plane, we then categorize the discovered objects by comparing their RGB-DINO feature patches enclosed in bounding boxes. 
However, due to the absence of object pose information, pixel-wise comparison is not ideal. 
Instead, we compare the distributions of color and semantic features among patches. 
To do this, we first discretize $\mathcal{M}_{rgb{\text -}dino}$ with clustering~(\eg K-means), 
which results in a set of labeled object feature patches $\mathcal{K}$. 
The similarity between two object patches $k_i,k_j\in\mathcal{K}$ are measured by the Jensen-Shannon Divergence over the distribution of labels, denoted by $\operatorname{JSD}(k_i||k_j)$.
Agglomerative clustering~\cite{Mllner2011ModernHA} is then performed using JS-divergence as the distance metric. The number of clusters can be set by inspecting training views, and can be flexibly adjusted to fit users' desired categorization granularity. 

With scene assets grouped into categories, a \emph{template} object can be selected from each cluster either randomly or in a user-defined manner.
We can further extract scene layout in BEV by computing the relative pose between the template object and its copies,~\ie to optimize a rotation angle $\theta$ that minimizes the pixel-wise loss between the RGB-DINO feature patches of the template and each copy with
$\theta^{\ast} = \argmin_{\theta} \sum_{i}^{N}||\hat{p}_i - R_\theta(p)_i||^2_2$ for $p\in\mathcal{P}_{rgb{\text -}dino}$,
where $\hat{p}$ is the template RGB-DINO feature patch, $R_\theta$ rotates the input feature patch by $\theta$.

\subsection{Cross-scene Asset Library}
\label{subsec:asset_library}
Following the proposed framework, a scene can be represented with (1) a set of template feature patches $\mathcal{P}$=$\{(p_\sigma,p_{rgb})\}$, (2) a layout describing object position and pose in the BEV, (3) the shared vertical feature axes $\mathcal{H}=(\mathcal{H}_\sigma,\mathcal{H}_{rgb})$, and (4) MLP decoders $Dec_\sigma$, $Dec_{rgb}$, 
which enables versatile scene editing at object-, category-, and scene-level. The newly configured scenes can be directly rendered without retraining. 	
An optional template refinement step is also allowed. Examples are given in Sec.~\ref{sec:exp}.

Previous work~\cite{liu2020neural} demonstrates that voxel-based neural representations support multi-scene modeling by learning different voxel embeddings for each scene whilst sharing the same MLP renderer.
However, it does not support cross-scene analogue discovery due to the aforementioned lack of constraints issue, 
whereas in reality, objects are not exclusive to a scene.
%~\ie the same objects can appear at multiple venues.
Our proposed neural representation has such potential to discover cross-scene analogues by also sharing the vertical feature axes among different scenes. 
Consequently, we can construct a cross-scene asset library storing template feature patches, and continuously expand it to accommodate new ones.



