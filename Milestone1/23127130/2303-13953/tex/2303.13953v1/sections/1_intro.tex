% !TEX root = ../main.tex

\section{Introduction}
\label{sec:intro}
\vspace{-5pt}
The demand for bringing our living environment into a virtual realm continuous to increase these days, with example cases ranging from indoor scenes such as rooms and restaurants, to outdoor ones like streets and neighborhoods.
Apart from the realistic 3D rendering, real-world applications also require flexible and user-friendly editing of the scene.
Use cases can be commonly found in interior design, urban planning etc. To save human labor and expense, users need to frequently visualize different scene configurations before finalizing a plan and bringing it to reality, like shown in Fig.\ref{fig:teaser}.
For their interests, a virtual environment offering versatile editing choices and high rendering quality is always preferable.
In these scenarios, objects are primarily located on a horizontal plane like ground, and can be inserted to/deleted from the scene.
Translation along the plane and rotation around the vertical axis are also common operations. 
Furthermore, group editing becomes essential when scenes are populated with recurring items~(\eg substitute all chairs with stools and remove all sofas in a bar).

While recent advances in neural rendering \cite{mildenhall2020nerf,Barron2021MipNeRF3U,Yu2022PlenoxelsRF,mueller2022instant}
offer promising solutions to producing realistic visuals, they struggle to meet the aforementioned editing demands.
Specifically, traditional neural radiance field (NeRF)-based methods such as~\cite{Zhang2020NeRFAA,MartinBrualla2020NeRFIT,Barron2021MipNeRF3U} encode an entire scene into a single neural network, making it difficult to manipulate and composite due to its implicit nature and limited network capacity.
Some follow-up works~\cite{Yang2022NeuralRI,Guo2020ObjectCentricNS} tackle object-aware scene rendering in a bottom-up fashion by learning one model per object and then performing joint rendering. 
Another branch of  methods learn object radiance fields using instance masks~\cite{yang2021objectnerf}, object motions~\cite{Yuan2021STaRST}, and image features~\cite{tschernezki22neural,kobayashi2022distilledfeaturefields} as clues but are scene-specific, limiting their applicable scenarios.
Recently, some approaches have attempted to combine voxel grids with neural radiance fields~\cite{Liu2020NeuralSV,Yu2022PlenoxelsRF,mueller2022instant} to explicitly model the scene. 
Previous work~\cite{Liu2020NeuralSV} showed local shape editing and scene composition abilities of the hybrid representation. 
However, since the learned scene representation is not object-aware, users must specify which voxels are affected to achieve certain editing requirements, which is cumbersome, especially for group editing.
Traditional graphical workflows build upon an asset library that stores template objects, whose copies are deployed onto a `canvas' by designers, then rendered by some professional software~(\eg interior designers arrange furniture according to floorplans). 
This practice significantly saves memory for large scene development and offers users versatile editing choices,
which inspires us to resemble this characteristic in neural rendering. 

To this end, we present \emph{AssetField}, a novel neural representation that bears the editing flexibility of traditional graphical workflows.
Our method factorizes a 3D neural field into a ground feature plane and a vertical feature axis. As illustrated in Fig.~\ref{fig:teaser}, the learned \textbf{ground feature plane} is a 2D feature plane that is visually aligned with the bird-eye view (BEV) of the scene, allowing intuitive manipulation of individual objects. 
It is also able to embed multiple scenes into scene-specific ground feature planes with a shared vertical feature axis, rendered using a shared MLP.
The learned ground feature planes encode scene density, color and semantics, providing rich clues for object detection and categorization.
We show that assets mining and categorization, and scene layout estimation can be directly performed on the ground feature planes. 
By maintaining a cross-scene \emph{asset library} that stores template objectsâ€™ ground feature patches, our method enables versatile editing at \emph{object-level}, \emph{category-level}, and \emph{scene-level}.

In summary, AssetField
1) learns a set of explicit ground feature planes that are intuitive and user-friendly for scene manipulation;
2) offers a novel way to discover assets and scene layout on the informative ground feature planes, from which one can construct an asset library storing feature patches of object templates from multiple scenes;
3) improves group editing efficiency and enables versatile scene composition and reconfiguration and
4) provides realistic renderings on new scene configurations.
