% !TEX root = ../main.tex

\section{Experiment}
\label{sec:exp}

In this section, we first describe our experiment setup, then evaluate AssetField on novel view synthesis both quantitatively and qualitatively, demonstrating its advantages in asset mining, categorization, and editing flexibility.
More training details and ablating results of hyper-parameters~(\eg the number of clusters, the pairing of plane feature, and axis feature) are provided in supplementary.

\subsection{Experimental Setup}
\noindent \textbf{Dataset.}
A synthetic dataset is created for evaluation. We compose $10$ scenes resembling common man-made environments such as conference room, living room, dining hall and office. Each scene contains objects from 3$\sim$12 categories with a fixed light source. For each scene, we render $50$ views with viewpoints sampled on a half-sphere, among which $40$ are used for training and the rest for testing. 
We demonstrate flexible scene manipulation with AssetField on both the synthetic and real-world data, including scenes from Mip-NeRF 360~\cite{barron2022mip}, DONeRF~\cite{neff2021donerf}, and ObjectNeRF~\cite{yang2021objectnerf}. We also show manipulation results on city scenes collected from Google Earth Studio~\cite{google_earth_studio}.


\noindent \textbf{Implementation.}
We use NeRF~\cite{mildenhall2020nerf} and TensoRF~\cite{Chen2022ECCV} as baselines to evaluate the rendering quality of the original scenes.
For a fair comparison, all methods are implemented to model an additional DINO field. Specifically,
(1) NeRF is extended with an extra head to predict view-independent DINO feature~\cite{amir2021deep} in parallel with density.
(2) For TensoRF, we additionally construct the DINO field which is factorized along $3$ directions the same as its radiance field.
(3) \textbf{S(tandard)-AssetField} separately models the density, RGB, and DINO fields.
(4) \textbf{I(ntegrated)-AssetField }models the density field the same as S-AssetField, and an integrated RGB-DINO ground feature plane. 
Both S-AssetField and I-AssetField adopt outer-product to combine ground plane features and vertical axis features, following~\cite{Chen2022ECCV}.
The resolution of feature planes in TensoRF baseline and AssetField are set to 300$\times$300.
Detailed model adaptation can be found in the supplementary.
We train NeRF for $200k$ iterations, and $50k$ iterations for TensoRF and AssetField using Adam~\cite{Kingma2015AdamAM} optimization with a learning rate set to $5e^{-4}$ for NeRF and $0.02$ for TensoRF and AssetField.

\subsection{Results}
\begin{table}[t!]
	\begin{center}
		\resizebox{\linewidth}{!}{
			\begin{tabular}{l|rrr|rrr|rrr|rrr}
				\toprule
				&   \multicolumn{3}{c|}{\emph{Scene1}}  &   \multicolumn{3}{c|}{\emph{Scene2}} &   \multicolumn{3}{c|}{\emph{Scene3}} & \multicolumn{3}{c}{\emph{Scene4}} \\
				&  PSNR & SSIM & LPIPS  &
				PSNR & SSIM & LPIPS &
				PSNR & SSIM & LPIPS  & PSNR & SSIM  & LPIPS   \\
				\midrule
				NeRF   & 32.977 & 0.969 & 0.067  & 35.743 & 0.967 & 0.051 & 32.521 & 0.959 & 0.058 & 34.212 & 0.964 & 0.072 \\
				TensoRF   & 35.751 & 0.990 & 0.057 & \tb{38.184} & \tb{0.995} & \tb{0.027} & \underline{36.933} & \underline{0.994} & \underline{0.034} & \tb{37.795} & \tb{0.993} & \tb{0.059}\\
				\midrule
				S-AssetField & 36.471 & 0.992 & 0.049 & 36.856 &  0.993  & 0.037 & 36.753 & 0.994 & 0.038 & 37.445 & 0.990 & 0.065 \\
				I-AssetField & \tb{36.526} & \tb{0.992} & \tb{0.047} & \underline{37.271} & \underline{0.994} & \underline{0.035} & \tb{37.249} & \tb{0.995} & \tb{0.032} & \underline{37.716} & \underline{0.991} & \underline{0.060} \\
				\bottomrule
			\end{tabular}		
		}
		\vspace{-15pt}
	\end{center}
	\centering
	\caption{\small Quantitative comparison on test views for the $4$ scenes in Fig.~\ref{fig:editing_results}. We report PSNR($\uparrow$), SSIM($\uparrow$)~\cite{Wang2004ImageQA} and LPIPS($\downarrow$)~\cite{Zhang2018TheUE} for evalution. The \tb{best} and \underline{second best} results are highlighted.}
	\vspace{-15pt}
	\label{tab:recon}
\end{table}



\noindent \textbf{Novel View Rendering.}
We compare S-AssetField and I-AssetField with the adapted NeRF~\cite{mildenhall2020nerf} and TensoRF~\cite{Chen2022ECCV} as described above. 
Quantitative results are provided in Tab.~\ref{tab:recon}. 
It is noticeable that AssetField's ground feature plane representation~(\ie $xy{\text -}z$) achieves comparable performance with TensoRF's $3$-mode factorization~(\ie $xy{\text -}z$, $xz{\text -}y$,$yz{\text -}x$), indicating the suitability of adopting ground plane representations for such scenes. Our method also inherits the merit of efficient training and rendering from grid-based methods. Compared to NeRF, our model converges 40x faster at training and renders 30x faster at inference.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/integrated.pdf}
	\caption{\small The RGB-DINO ground feature plane from I-AssetField yields consistent features for analogues with vertical displacement, whereas S-AssetField infers different set of features due to the lack of constraints.}
	\label{fig:occupancy-guided}
	\vspace{-10pt}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/toydesk-new.pdf}
	\vspace{-15pt}
	\caption{\small Multi-scene learning on the Toydesk dataset~\cite{yang2021objectnerf}. As real-world scenes usually exhibit noisier color and density features, we apply the object mask obtained from the density plane before categorization.
		The common object between scenes ({\color{yellow}yellow}) can be correctly clustered with I-AssetField's occupancy-guided RGB-DINO plane features ({\color{green}green}) whilst the independently modeled neural planes by S-AssetField fails ({\color{red}red}).}
	\label{fig:toydesk}
	\vspace{-15pt}
\end{figure}

\begin{figure*}[t!]
	\centering
	\includegraphics[width=\linewidth]{figs/results.pdf}
	\vspace{-20pt}
	\caption{\small Results of assets mining and scene editing with \emph{I-AssetField} on synthetic scenes. (a) Our approach learns informative density and RGB-DINO ground feature planes that support object detection and categorization. (b) With joint training, an asset library can be constructed by storing ground \emph{feature plane patches} of the radiance field (we show label patches here for easy visualization). (c) The proposed ground plane representation provides an explicit visualization of the scene configuration, which can be directly manipulated by users. The altered ground feature planes are then fed to the global MLP renderer along with the shared vertical feature axes to render the novel scenes. Operations such as object removal, translation, rotation and rescaling are demonstrated on the right. }
	\vspace{-10pt}
	\label{fig:editing_results}
\end{figure*}

\noindent \textbf{Object Detection and Categorization.}
In Fig.~\ref{fig:tensorf_vs_groundplan} we already showed an example of the ground feature planes learned by AssetField compared to the $xy$-planes learned by TensoRF. 
While TensoRF produces noisy and less informative feature planes that is unfriendly for object discovery in the first place, AssetField is able to identify and categorize most of the scene contents, as shown in Fig.~\ref{fig:editing_results}~(b).
Furthermore, I-AssetField is more robust to vertical displacement, as shown in Fig.~\ref{fig:occupancy-guided}. On this scene variation, TensoRF/S-AssetField/I-AssetField achieves 35.873/36.358/36.452 in PSNR metric respectively on the test set.

Recall that I-AssetField is able to identify object analogues \emph{across different scenes}, to demonstrate such ability, we jointly model the two toy desk scenes from~\cite{yang2021objectnerf} by letting them share the same vertical feature axes and MLPs as described in Sec.~\ref{subsec:asset_library}. The inferred feature planes are showed in Fig.~\ref{fig:toydesk}.
Since the coordinate systems of these two scenes are not aligned with the physical world, we perform PCA~\cite{pca} on camera poses such that the $xy$-plane is expanded along the ground/table-top. However, we cannot guarantee their table surfaces are at the same height, meaning that vertical displacement among objects is inevitable. 
I-AssetField is able to infer similar RGB-DINO feature values for the common cube plush (yellow circle), whilst the independently learned RGB/DINO planes in S-AssetField are affected by the height difference.

\begin{figure}
	\centering
	\includegraphics[width=0.95\linewidth]{figs/density-warping-new.pdf}
	\vspace{-8pt}
	\caption{\small Density warping from the blue bottle to the region of the brown one. S-AssetField loses the structure of the brown bottle in terms of part semantics, while I-AssetField gives plausible editing result with appropriate structure transfer.}
	\vspace{-10pt}
	\label{fig:density-warping}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{figs/remove-ceiling-new.pdf}
	\vspace{-5pt}
	\caption{\small Expanding the 2D ground plane back to 3D feature grids, explicit control on full 3D space is allowed. We remove the ceiling light by setting the density grids as zero at the target region.}
	\vspace{-15pt}
	\label{fig:remove-ceiling}
\end{figure}



\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/asset-basic-real-new.pdf}
	\vspace{-18pt}
	\caption{\small Example editings on real-world scenes~\cite{barron2022mip} and indoor scenarios~\cite{neff2021donerf}. We use RGB-DINO plane for assets discovery.
		%	The side table and the slippers are duplicated by placing their templates on nearby ground planes.
	}
	\vspace{-20pt}
	\label{fig:room-basic}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.98\linewidth]{figs/cityscene_editing-new.pdf}
	\vspace{-5pt}
	\caption{\small Editing two city scenes collected from Google Earth~\copyrightgoogle. AssetField is versatile where users can directly operate on the ground feature plane, supporting both within-scene and cross-scene editing with realistic rendering results.}
\vspace{-5pt}
\label{fig:cityscene}
\end{figure}


\noindent\textbf{Scene Editing.}
Techniques on 2D image manipulation can be directly applied to ground feature planes. Fig.~\ref{fig:editing_results} shows that AssetField supports a variety of operations, such as object removal, insertion, translation and rescaling. Scene-level reconfiguration is also intuitive by composing objects' density and color ground feature patches. 
In particular, I-AssetField associates the RGB-DINO field with space occupancy, producing more plausible warping results. Fig.~\ref{fig:density-warping} demonstrates a case of topology deformation, where the blue bottle's density field is warped to the region of the brown bottle, while keeping their RGB(-DINO) feature unchanged. Results show that I-AssetField successfully preserves object structure and part semantics, whereas S-AssetField fails to render the cork correctly.

Despite the convenience of ground feature plane representation, it does not directly support manipulating overlapping/stacked objects. However, one can expand the ground feature plane back to 3D feature grids with its pairing vertical feature axis, and control the scene in the conventional way as described in~\cite{liu2020neural}. An example is given in Fig.~\ref{fig:remove-ceiling}.
%

Fig.~\ref{fig:room-basic} shows AssetField's editing capability on real-world datasets~\cite{barron2022mip,yang2021objectnerf,neff2021donerf}. Additionally, on a self-collected city scene from Google Earth, we find a construction site and complete it with different nearby buildings (within-scene editing), even borrow Colosseum from Rome (cross-scene editing). Results are shown in Fig~\ref{fig:cityscene}. The test view PSNR for the original scene is NeRF/TensoRF/S-AssetField/I-AssetField: 24.55/27.61/ 27.54/ 27.95.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/category-editing.pdf}
	\vspace{-20pt}
	\caption{\small We apply batch-wise color changing for all instances of the chair, by replacing the template RGB feature map solely.}
	\label{fig:category}
	\vspace{-15pt}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{figs/cross-scene.pdf}
	\vspace{-10pt}
	\caption{\small We expand the asset library from the living room with the newly included assets \emph{mics} from~\cite{mildenhall2020nerf}. The template of mics is in the shared latent space with the living room and can thus naturally composed together for rendering.}
	\label{fig:cross-scene}
	\vspace{-10pt}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{figs/template-new.pdf}
	\vspace{-10pt}
	\caption{\small Feature plane refinement. The object template, when trained among all instances within the scene, produces more accurate feature map compared to the isolated ones.}
	\label{fig:template}
	\vspace{-20pt}
\end{figure}


\noindent \textbf{Group Editing and Scene Reconfiguration.}
Recall that a template object can be selected for each asset category to substitute all its instances in the scene (on the ground feature planes). Consequently, we are allowed to perform group editing like changing the color of a specific category as depicted in Fig.~\ref{fig:category}. 
Scene-level reconfiguration is also intuitive, where users can freely compose objects from the \emph{asset library} on a neural `canvas' to obtain a set of new ground feature planes, as demonstrated in Fig.~\ref{fig:cross-scene}.
The environments or containers~(\eg the floor or an empty house) can also be considered as a special asset category, where small objects~(\eg furniture) can be placed into the container to deliver immersive experience. The final scene can be composited with summed density value and weighted color, as has been discussed in~\cite{Tang2022CompressiblecomposableNV}.


\smallskip
\noindent \textbf{Template Refinement.} 
Grid-based neural fields are sensitive to training views with insufficient point supervision, leading to noisy and inaccurate feature values.
Appearance differences caused by lighting variation, occlusion, etc., interferes the obtaining of a clean template feature patch. 
An example can be found in Fig.~\ref{fig:template}.
Due to imbalanced training view distribution, the chair in the corner receives less supervision, resulting in inconsistent object feature patch within a category. 
Such issue can be alleviated with a following-up \emph{template refinement} step.
With the inferred scene layout and the selected object templates (Sec.~\ref{subsec:asset_grouping}).
We propose to replace all instances $p\in\mathcal{P}$ with their representative category template $\hat{p}$ and optimize this set of feature patches to reconstruct the scene instead of the full ground planes. 
Consequently, the template feature patch integrates supervisions from all instances in the scene to overcome appearance variations and sparse views. 
