% !TEX root = ../main.tex

\section{Related Works}
\label{sec:related}
\noindent\textbf{Neural Implicit Representations and Semantic Fields.}
Since the introduction of neural radiance fields~\cite{mildenhall2020nerf}, many advanced scene representations have been proposed\cite{liu2020neural,Yu2022PlenoxelsRF,mueller2022instant,Chen2022ECCV,liu2020neural,Chan2021,mueller2022instant}, 
demonstrating superior performance in terms of quality and speed for neural renderings. 
However, most of these methods are semantic and content agnostic, and many assume sparsity to design a more compact structure for rendering acceleration~\cite{liu2020neural,Chen2022ECCV,mueller2022instant}. We notice that the compositional nature of a scene and the occurrence of repetitive objects within can be further utilized, where we can extract a reusable asset library for more scalable usages, similar to those adopted in the classical modeling pipeline.

A line of recent neural rendering works has explored the jointly learning a semantic fields along with the original radiance field. Earlier works use available semantic labels \cite{zhi2021place} or existing 2D detectors for supervision~\cite{kundu2022panoptic}. The realized semantic field can enable category or object-level control. More recently, \cite{tschernezki22neural,kobayashi2022distilledfeaturefields} explore the potential of distilling self-supervised 2D image feature extractors~\cite{caron2021emerging,amir2021deep,fan2022nerf} into NeRF, and showcasing their usages of support local editing. In this work, we target an orthogonal editing goal where the accurate control of high-level scene configuration and easy editing on object instances is desired.

\smallskip
\noindent\textbf{Object Manipulation and Scene Composition.}
Traditional modeling and rendering pipelines~\cite{Bleyer2011PatchMatchS, Broadhurst2001APF, Schnberger2016StructurefromMotionR, Schnberger2016PixelwiseVS, Seitz1997PhotorealisticSR,Karsch2011RenderingSO} are vastly adopted for scene editing and novel view synthesis in early approaches. For example, Karsch~\etal~\cite{Karsch2011RenderingSO} propose to realistically insert synthetic objects into legacy images by creating a physical model of the scene from user annotations of geometry and lighting conditions, then compose and render the edited scene. Cossairt~\etal~\cite{Cossairt2008LightFT} consider synthetic and real objects compositions from the perspective of light field, where objects are captured by a specific hardware system.
\cite{zheng2012interactive,kim2012acquiring,kholgade20143d} consider the problem of manipulating existing 3D scenes by matching the objects to cuboid proxies/pre-captured 3D models.

These days, several works propose to tackle object-decomposite rendering under the context of newly emerged neural implicit representations~\cite{mildenhall2020nerf}. 
Ost~\etal~\cite{Ost2021NeuralSG} target dynamic scenes and learn a scene graph representation that encodes object transformation and radiance at each node, which further allows rendering novel views and re-arranged scenes. 
Kundu~\etal \cite{kundu2022panoptic} resort to existing 3D object detectors for foreground object extraction.
Sharma~\etal~\cite{sharma2022seeing} disentangles static and movable scene contents, leveraging object motion as a cue.
Guo~\etal~\cite{Guo2020ObjectCentricNS} propose to learn object-centric neural scattering functions to implicitly model per-object light transportation, enabling scene rendering with moving objects and lights. 
Neural Rendering in a Room~\cite{Yang2022NeuralRI} targets indoor scenes by learning a radiance field for each pre-captured object and putting objects into a panoramic image for optimization. 
While these methods need to infer object from motion, or require one model per object, ObjectNeRF~\cite{Yang2021LearningON} learns a decompositional neural radiance field, utilizing semantic masks to separate objects from the background to allow editable scene rendering.
uORF~\cite{yu2022unsupervised} performs unsupervised discovery of object radiance fields without the need for semantic masks, but requires cross-scene training and is only tested on simple synthetic objects without textures.
