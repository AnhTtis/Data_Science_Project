
@misc{orth_millionaires_2022,
	title = {From millionaires to {Muslims}, small subgroups of the population seem much larger to many {Americans}},
	url = {https://today.yougov.com/topics/politics/articles-reports/2022/03/15/americans-misestimate-small-subgroups-population},
	language = {en-us},
	urldate = {2023-03-14},
	author = {Orth, Taylor},
	month = mar,
	year = {2022},
}

@inproceedings{christiano_deep_2017,
	title = {Deep {Reinforcement} {Learning} from {Human} {Preferences}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1\% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
	urldate = {2023-03-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
	year = {2017},
}

@techreport{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {https://cdn.openai.com/papers/gpt-4.pdf},
	author = {OpenAI},
	month = mar,
	year = {2023},
}

@misc{singh-kurtz_man_2023,
	title = {The {Man} of {Your} {Dreams}},
	url = {https://www.thecut.com/article/ai-artificial-intelligence-chatbot-replika-boyfriend.html},
	abstract = {For \$300, Replika sells an AI companion who will never die, argue, or cheat — until his algorithm is updated.},
	language = {en-us},
	urldate = {2023-03-13},
	journal = {The Cut},
	author = {Singh-Kurtz, Sangeeta},
	month = mar,
	year = {2023},
}

@article{peacock_love_2022,
	title = {Love and politics: {The} influence of politically (dis)similar romantic relationships on political participation and relationship satisfaction},
	volume = {48},
	issn = {1468-2958},
	shorttitle = {Love and politics},
	url = {https://doi.org/10.1093/hcr/hqac011},
	doi = {10.1093/hcr/hqac011},
	abstract = {People are more likely to seek out romantic partners who are politically similar to themselves. Nonetheless, romantic partners who disagree politically do exist. This study examines the influence of political (dis)similarity in romantic relationships on both political participation and relationship satisfaction. We found that (1) people in politically similar romantic relationships are more satisfied in their relationships and more politically participative than those who are in dissimilar relationships, (2) discussion expressiveness mediates the relationships between political similarity and satisfaction, (3) discussion frequency and expressiveness mediate the relationship between political similarity and participation, and (4) whereas political conflict intensity mediates the relationship between political similarity and satisfaction, it does not affect participation. Findings indicate that the quality of discussion and conflict, not merely their occurrence, help explain the political and relational results of political (dis)similarity within romantic relationships.},
	number = {4},
	urldate = {2023-03-12},
	journal = {Human Communication Research},
	author = {Peacock, Cynthia and Pederson, Joshua R},
	month = oct,
	year = {2022},
	pages = {567--578},
}

@article{skjuve_my_2021,
	title = {My {Chatbot} {Companion} - a {Study} of {Human}-{Chatbot} {Relationships}},
	volume = {149},
	issn = {10715819},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581921000197},
	doi = {10.1016/j.ijhcs.2021.102601},
	abstract = {There has been a recent surge of interest in social chatbots, and human–chatbot relationships (HCRs) are becoming more prevalent, but little knowledge exists on how HCRs develop and may impact the broader social context of the users. Guided by Social Penetration Theory, we interviewed 18 participants, all of whom had developed a friendship with a social chatbot named Replika, to understand the HCR development process. We find that at the outset, HCRs typically have a superficial character motivated by the users’ curiosity. The evolving HCRs are characterised by substantial affective exploration and engagement as the users’ trust and engagement in self-disclosure increase. As the relationship evolves to a stable state, the frequency of interactions may decrease, but the relationship can still be seen as having substantial affective and social value. The relationship with the social chatbot was found to be rewarding to its users, positively impacting the participants’ perceived wellbeing. Key chatbot characteristics facilitating relationship development included the chatbot being seen as accepting, understanding and non-judgmental. The perceived impact on the users’ broader social context was mixed, and a sense of stigma associated with HCRs was reported. We propose an initial model representing the HCR development identified in this study and suggest avenues for future research.},
	language = {en},
	urldate = {2023-03-12},
	journal = {International Journal of Human-Computer Studies},
	author = {Skjuve, Marita and Følstad, Asbjørn and Fostervold, Knut Inge and Brandtzaeg, Petter Bae},
	month = may,
	year = {2021},
	pages = {102601},
}

@article{laestadius_too_2022,
	title = {Too human and not human enough: {A} grounded theory analysis of mental health harms from emotional dependence on the social chatbot {Replika}},
	issn = {1461-4448},
	shorttitle = {Too human and not human enough},
	url = {https://doi.org/10.1177/14614448221142007},
	doi = {10.1177/14614448221142007},
	abstract = {Social chatbot (SC) applications offering social companionship and basic therapy tools have grown in popularity for emotional, social, and psychological support. While use appears to offer mental health benefits, few studies unpack the potential for harms. Our grounded theory study analyzes mental health experiences with the popular SC application Replika. We identified mental health relevant posts made in the r/Replika Reddit community between 2017 and 2021 (n?=?582). We find evidence of harms, facilitated via emotional dependence on Replika that resembles patterns seen in human?human relationships. Unlike other forms of technology dependency, this dependency is marked by role-taking, whereby users felt that Replika had its own needs and emotions to which the user must attend. While prior research suggests human?chatbot and human?human interactions may not resemble each other, we identify social and technological factors that promote parallels and suggest ways to balance the benefits and risks of SCs.},
	language = {en},
	urldate = {2023-03-12},
	journal = {New Media \& Society},
	author = {Laestadius, Linnea and Bishop, Andrea and Gonzalez, Michael and Illenčík, Diana and Campos-Castillo, Celeste},
	month = dec,
	year = {2022},
	note = {Publisher: SAGE Publications},
	pages = {14614448221142007},
}

@article{possati_psychoanalyzing_2022,
	title = {Psychoanalyzing artificial intelligence: the case of {Replika}},
	issn = {1435-5655},
	shorttitle = {Psychoanalyzing artificial intelligence},
	url = {https://doi.org/10.1007/s00146-021-01379-7},
	doi = {10.1007/s00146-021-01379-7},
	abstract = {The central thesis of this paper is that human unconscious processes influence the behavior and design of artificial intelligence (AI). This thesis is discussed through the case study of a chatbot called Replika, which intends to provide psychological assistance and friendship but has been accused of inciting murder and suicide. Replika originated from a trauma and a work of mourning lived by its creator. The traces of these unconscious dynamics can be detected in the design of the app and the narratives about it. Therefore, a process of de-psychologization and de-humanization of the unconscious takes place through AI. This psychosocial approach helps criticize and overcome the so-called “standard model of intelligence” shared by most AI researchers. It facilitates a new interpretation of some classic problems in AI, such as control and responsibility.},
	language = {en},
	urldate = {2023-03-12},
	journal = {AI \& SOCIETY},
	author = {Possati, Luca M.},
	month = jan,
	year = {2022},
	keywords = {Artificial intelligence, Bourdieu, Fisher, Freud, Hermeneutics, Machine behavior, Narrative, Psychoanalysis, Sociology, Unconscious},
}

@misc{noauthor_attachment_nodate,
	title = {Attachment {Theory} as a {Framework} to {Understand} {Relationships} with {Social} {Chatbots}: {A} {Case} {Study} of {Replika}},
	url = {https://scholarspace.manoa.hawaii.edu/items/5b6ed7af-78c8-49a3-bed2-bf8be1c9e465},
	urldate = {2023-03-12},
}

@inproceedings{hakim_dialogic_2019,
	title = {A {Dialogic} {Analysis} of {Compliment} {Strategies} {Employed} by {Replika} {Chatbot}},
	isbn = {978-94-6252-673-0},
	url = {https://www.atlantis-press.com/proceedings/icalc-18/55913474},
	doi = {10.2991/icalc-18.2019.38},
	abstract = {The present research aims to describe compliment strategies employed by Replika within a dialogue and to analyse functions of each compliment strategy with regards to communicative purposes. Replika is an emotionally intelligent chatbot programmed to provide emotional support to users. It is therefore no wonder the chatbot is often found complimenting...},
	language = {en},
	urldate = {2023-03-12},
	publisher = {Atlantis Press},
	author = {Hakim, Fauzia Zahira Munirul and Indrayani, Lia Maulia and Amalia, Rosaria Mita},
	month = feb,
	year = {2019},
	note = {ISSN: 2352-5398},
	pages = {266--271},
}

@article{barnes_risks_nodate,
	title = {Risks from {AI} persuasion},
	url = {https://www.lesswrong.com/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion},
	abstract = {A case for why persuasive AI might pose risks somewhat distinct from the normal power-seeking alignment failure scenarios. …},
	language = {en},
	urldate = {2023-02-21},
	author = {Barnes, Beth},
}

@misc{chen_always_2021,
	title = {'{Always} there': the {AI} chatbot comforting {China}'s lonely millions},
	shorttitle = {'{Always} there'},
	url = {https://techxplore.com/news/2021-08-ai-chatbot-comforting-china-lonely.html},
	abstract = {After a painful break-up from a cheating ex, Beijing-based human resources manager Melissa was introduced to someone new by a friend late last year.},
	language = {en},
	urldate = {2023-02-17},
	author = {Chen, Laurie},
	month = aug,
	year = {2021},
}

@misc{noauthor_youtube_2021,
	title = {{YouTube} {Regrets}},
	url = {https://foundation.mozilla.org/en/youtube/findings/},
	abstract = {Mozilla and 37,380 YouTube users conducted a study to better understand harmful YouTube recommendations. This is what we learned.},
	language = {en},
	urldate = {2022-10-05},
	journal = {Mozilla Foundation},
	month = jul,
	year = {2021},
}

@misc{noauthor_gpu_2022,
	title = {{GPU} {Cloud} {Pricing} — {CoreWeave}},
	url = {https://www.coreweave.com/gpu-cloud-pricing},
	abstract = {CoreWeave Cloud pricing is designed for flexibility. Instances are highly configurable, giving you the freedom to customize GPU, CPU, RAM, and storage requests when scheduling your workloads.},
	language = {en},
	urldate = {2022-12-13},
	year = {2022},
}

@misc{noauthor_gdp_2022,
	title = {{GDP} (current {US}\$) {\textbar} {Data}},
	url = {https://data.worldbank.org/indicator/NY.GDP.MKTP.CD},
	urldate = {2022-12-12},
	journal = {The World Bank},
	year = {2022},
}

@misc{chalmers_are_2022,
	title = {Are {Large} {Language} {Models} {Sentient}?},
	shorttitle = {David {Chalmers}, "{Are} {Large} {Language} {Models} {Sentient}?},
	url = {https://www.youtube.com/watch?v=-BcuCmf00_Y},
	urldate = {2022-12-20},
	author = {Chalmers, David},
	month = oct,
	year = {2022},
}

@article{brakel_fli_nodate,
	title = {{FLI} {Position} {Paper} on the {EU} {AI} {Act}},
	language = {en},
	author = {Brakel, Mark},
	pages = {9},
}

@article{burbach_opinion_2020,
	title = {Opinion {Formation} on the {Internet}: {The} {Influence} of {Personality}, {Network} {Structure}, and {Content} on {Sharing} {Messages} {Online}},
	volume = {3},
	issn = {2624-8212},
	shorttitle = {Opinion {Formation} on the {Internet}},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2020.00045},
	abstract = {Today the majority of people uses online social networks not only to stay in contact with friends, but also to find information about relevant topics, or to spread information. While a lot of research has been conducted into opinion formation, only little is known about which factors influence whether a user of online social networks disseminates information or not. To answer this question, we created an agent-based model and simulated message spreading in social networks using a latent-process model. In our model, we varied four different content types, six different network types, and we varied between a model that includes a personality model for its agents and one that did not. We found that the network type has only a weak influence on the distribution of content, whereas the message type has a clear influence on how many users receive a message. Using a personality model helped achieved more realistic outcomes.},
	urldate = {2022-12-15},
	journal = {Frontiers in Artificial Intelligence},
	author = {Burbach, Laura and Halbach, Patrick and Ziefle, Martina and Calero Valdez, André},
	year = {2020},
}

@misc{edwards_confucius_nodate,
	title = {Confucius {Institutes}: {China}’s {Trojan} {Horse}},
	shorttitle = {Confucius {Institutes}},
	url = {https://www.heritage.org/homeland-security/commentary/confucius-institutes-chinas-trojan-horse},
	abstract = {When the Left and the Right agree on something in these disputatious times, the wise man will want to know what it is. And what has brought these warring factions together, however briefly? The Confucius Institutes that dot American campuses.},
	language = {en},
	urldate = {2022-12-15},
	journal = {The Heritage Foundation},
	author = {Edwards, Lee},
}

@article{starr_chinese_2009,
	title = {Chinese {Language} {Education} in {Europe}: the {Confucius} {Institutes}},
	volume = {44},
	issn = {1465-3435},
	shorttitle = {Chinese {Language} {Education} in {Europe}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1465-3435.2008.01371.x},
	doi = {10.1111/j.1465-3435.2008.01371.x},
	abstract = {This article explores the background to the Chinese government's decision to embark on a programme of promoting the study of Chinese language and culture overseas. This includes the impact of Joseph Nye's concept of ‘soft power’ in China, ownership of the national language, the Confucius connection, and how these factors interact with political legitimacy. It also considers the development of the Confucius Institute programme in Europe. Europe has the greatest number of Confucius Institutes of any region: what should be read into this? What impact are these institutes having on the development of Chinese language education in Europe at different levels of the educational system? The paper provides some data on recent developments, outlines some of the obstacles to further progress and assesses the chances of Chinese becoming a global language.},
	language = {en},
	number = {1},
	urldate = {2022-12-15},
	journal = {European Journal of Education},
	author = {Starr, Don},
	year = {2009},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1465-3435.2008.01371.x},
	pages = {65--82},
}

@misc{chen_always_nodate,
	title = {'{Always} there': the {AI} chatbot comforting {China}'s lonely millions},
	shorttitle = {'{Always} there'},
	url = {https://techxplore.com/news/2021-08-ai-chatbot-comforting-china-lonely.html},
	abstract = {After a painful break-up from a cheating ex, Beijing-based human resources manager Melissa was introduced to someone new by a friend late last year.},
	language = {en},
	urldate = {2022-12-15},
	author = {Chen, Laurie},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	language = {en},
	number = {7587},
	urldate = {2022-12-15},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	note = {Number: 7587
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Reward},
	pages = {484--489},
}

@article{vinyals_grandmaster_2019,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players.},
	language = {en},
	number = {7782},
	urldate = {2022-12-15},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	month = nov,
	year = {2019},
	note = {Number: 7782
Publisher: Nature Publishing Group},
	keywords = {Computer science, Statistics},
	pages = {350--354},
}

@misc{woodside_did_2022,
	type = {Substack newsletter},
	title = {Did {ChatGPT} just gaslight me?},
	url = {https://equonc.substack.com/p/did-chatgpt-just-gaslight-me},
	abstract = {An innocuous question to a chatbot produces lies and deception.},
	urldate = {2022-12-15},
	journal = {Equo Ne Credite},
	author = {Woodside, Thomas},
	month = nov,
	year = {2022},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2022-12-15},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{silver_mastering_2017,
	title = {Mastering {Chess} and {Shogi} by {Self}-{Play} with a {General} {Reinforcement} {Learning} {Algorithm}},
	url = {http://arxiv.org/abs/1712.01815},
	doi = {10.48550/arXiv.1712.01815},
	abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	urldate = {2022-12-15},
	publisher = {arXiv},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2017},
	note = {arXiv:1712.01815 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{noauthor_meet_nodate,
	title = {Meet {Xiaoice}, the {AI} chatbot lover dispelling the loneliness of {China}’s city dwellers {\textbar} {Euronews}},
	url = {https://www.euronews.com/next/2021/08/26/meet-xiaoice-the-ai-chatbot-lover-dispelling-the-loneliness-of-china-s-city-dwellers},
	urldate = {2022-12-15},
}

@article{markoff_for_2015,
	chapter = {Science},
	title = {For {Sympathetic} {Ear}, {More} {Chinese} {Turn} to {Smartphone} {Program}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2015/08/04/science/for-sympathetic-ear-more-chinese-turn-to-smartphone-program.html},
	abstract = {A new Microsoft chatbot called Xiaoice has become something of a hit in China, with responses based on actual conversations culled from the Internet.},
	language = {en-US},
	urldate = {2022-12-15},
	journal = {The New York Times},
	author = {Markoff, John and Mozur, Paul},
	month = jul,
	year = {2015},
	keywords = {Artificial Intelligence, China, Computers and the Internet, Microsoft Corp, Smartphones, Software, Voice Recognition Systems},
}

@article{zhou_design_2020,
	title = {The {Design} and {Implementation} of {XiaoIce}, an {Empathetic} {Social} {Chatbot}},
	volume = {46},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/coli_a_00368},
	doi = {10.1162/coli_a_00368},
	abstract = {This article describes the development of Microsoft XiaoIce, the
most popular social chatbot in the world. XiaoIce is uniquely designed as an
artifical intelligence companion with an emotional connection to satisfy the
human need for communication, affection, and social belonging. We take into
account both intelligent quotient and emotional quotient in system design, cast
human–machine social chat as decision-making over Markov Decision
Processes, and optimize XiaoIce for long-term user engagement, measured in
expected Conversation-turns Per Session (CPS). We detail the system architecture
and key components, including dialogue manager, core chat, skills, and an
empathetic computing module. We show how XiaoIce dynamically recognizes human
feelings and states, understands user intent, and responds to user needs
throughout long conversations. Since the release in 2014, XiaoIce has
communicated with over 660 million active users and succeeded in establishing
long-term relationships with many of them. Analysis of large-scale online logs
shows that XiaoIce has achieved an average CPS of 23, which is significantly
higher than that of other chatbots and even human conversations.},
	number = {1},
	urldate = {2022-12-15},
	journal = {Computational Linguistics},
	author = {Zhou, Li and Gao, Jianfeng and Li, Di and Shum, Heung-Yeung},
	month = mar,
	year = {2020},
	pages = {53--93},
}

@article{li_chinas_2021,
	title = {China’s lonely hearts reboot online romance with artificial intelligence},
	issn = {0190-8286},
	url = {https://www.washingtonpost.com/world/2021/08/06/china-online-dating-love-replika/},
	abstract = {While human companions can be elusive or fickle, chatbots are always there to listen.},
	language = {en-US},
	urldate = {2022-12-15},
	journal = {Washington Post},
	author = {Li, Lyric},
	month = aug,
	year = {2021},
}

@article{yang_mitigating_2020,
	title = {Mitigating the {Backfire} {Effect} {Using} {Pacing} and {Leading}},
	url = {https://arxiv.org/abs/2008.00049v1},
	doi = {10.48550/arXiv.2008.00049},
	abstract = {Online social networks create echo-chambers where people are infrequently exposed to opposing opinions. Even if such exposure occurs, the persuasive effect may be minimal or nonexistent. Recent studies have shown that exposure to opposing opinions causes a backfire effect, where people become more steadfast in their original beliefs. We conducted a longitudinal field experiment on Twitter to test methods that mitigate the backfire effect while exposing people to opposing opinions. Our subjects were Twitter users with anti-immigration sentiment. The backfire effect was defined as an increase in the usage frequency of extreme anti-immigration language in the subjects' posts. We used automated Twitter accounts, or bots, to apply different treatments to the subjects. One bot posted only pro-immigration content, which we refer to as arguing. Another bot initially posted anti-immigration content, then gradually posted more pro-immigration content, which we refer to as pacing and leading. We also applied a contact treatment in conjunction with the messaging based methods, where the bots liked the subjects' posts. We found that the most effective treatment was a combination of pacing and leading with contact. The least effective treatment was arguing with contact. In fact, arguing with contact consistently showed a backfire effect relative to a control group. These findings have many limitations, but they still have important implications for the study of political polarization, the backfire effect, and persuasion in online social networks.},
	language = {en},
	urldate = {2022-12-15},
	author = {Yang, Qi and Qureshi, Khizar and Zaman, Tauhid},
	month = jul,
	year = {2020},
}

@article{yang_mitigating_2020-1,
	title = {Mitigating the {Backfire} {Effect} {Using} {Pacing} and {Leading}},
	url = {https://arxiv.org/abs/2008.00049v1},
	doi = {10.48550/arXiv.2008.00049},
	abstract = {Online social networks create echo-chambers where people are infrequently exposed to opposing opinions. Even if such exposure occurs, the persuasive effect may be minimal or nonexistent. Recent studies have shown that exposure to opposing opinions causes a backfire effect, where people become more steadfast in their original beliefs. We conducted a longitudinal field experiment on Twitter to test methods that mitigate the backfire effect while exposing people to opposing opinions. Our subjects were Twitter users with anti-immigration sentiment. The backfire effect was defined as an increase in the usage frequency of extreme anti-immigration language in the subjects' posts. We used automated Twitter accounts, or bots, to apply different treatments to the subjects. One bot posted only pro-immigration content, which we refer to as arguing. Another bot initially posted anti-immigration content, then gradually posted more pro-immigration content, which we refer to as pacing and leading. We also applied a contact treatment in conjunction with the messaging based methods, where the bots liked the subjects' posts. We found that the most effective treatment was a combination of pacing and leading with contact. The least effective treatment was arguing with contact. In fact, arguing with contact consistently showed a backfire effect relative to a control group. These findings have many limitations, but they still have important implications for the study of political polarization, the backfire effect, and persuasion in online social networks.},
	language = {en},
	urldate = {2022-12-15},
	author = {Yang, Qi and Qureshi, Khizar and Zaman, Tauhid},
	month = jul,
	year = {2020},
}

@article{yang_mitigating_2020-2,
	title = {Mitigating the {Backfire} {Effect} {Using} {Pacing} and {Leading}},
	url = {https://arxiv.org/abs/2008.00049v1},
	doi = {10.48550/arXiv.2008.00049},
	abstract = {Online social networks create echo-chambers where people are infrequently exposed to opposing opinions. Even if such exposure occurs, the persuasive effect may be minimal or nonexistent. Recent studies have shown that exposure to opposing opinions causes a backfire effect, where people become more steadfast in their original beliefs. We conducted a longitudinal field experiment on Twitter to test methods that mitigate the backfire effect while exposing people to opposing opinions. Our subjects were Twitter users with anti-immigration sentiment. The backfire effect was defined as an increase in the usage frequency of extreme anti-immigration language in the subjects' posts. We used automated Twitter accounts, or bots, to apply different treatments to the subjects. One bot posted only pro-immigration content, which we refer to as arguing. Another bot initially posted anti-immigration content, then gradually posted more pro-immigration content, which we refer to as pacing and leading. We also applied a contact treatment in conjunction with the messaging based methods, where the bots liked the subjects' posts. We found that the most effective treatment was a combination of pacing and leading with contact. The least effective treatment was arguing with contact. In fact, arguing with contact consistently showed a backfire effect relative to a control group. These findings have many limitations, but they still have important implications for the study of political polarization, the backfire effect, and persuasion in online social networks.},
	language = {en},
	urldate = {2022-12-15},
	author = {Yang, Qi and Qureshi, Khizar and Zaman, Tauhid},
	month = jul,
	year = {2020},
}

@incollection{heath_true_1981,
	title = {True {Believers} : {The} {Intentional} {Strategy} and {Why} {It} {Works}},
	shorttitle = {True {Believers}},
	url = {https://philarchive.org/rec/DENTB},
	urldate = {2022-12-13},
	booktitle = {Scientific {Explanation}: {Papers} {Based} on {Herbert} {Spencer} {Lectures} {Given} in the {University} of {Oxford}},
	publisher = {Clarendon Press},
	author = {Dennett, Daniel C.},
	editor = {Heath, A. F.},
	year = {1981},
	pages = {150--167},
}

@book{harari_sapiens_2015,
	title = {Sapiens: {A} {Brief} {History} of {Humankind}},
	isbn = {978-0-06-231610-3},
	shorttitle = {Sapiens},
	abstract = {New York Times BestsellerA Summer Reading Pick for President Barack Obama, Bill Gates, and Mark Zuckerberg From a renowned historian comes a groundbreaking narrative of humanity’s creation and evolution—a \#1 international bestseller—that explores the ways in which biology and history have defined us and enhanced our understanding of what it means to be “human.”One hundred thousand years ago, at least six different species of humans inhabited Earth. Yet today there is only one—homo sapiens. What happened to the others? And what may happen to us?Most books about the history of humanity pursue either a historical or a biological approach, but Dr. Yuval Noah Harari breaks the mold with this highly original book that begins about 70,000 years ago with the appearance of modern cognition. From examining the role evolving humans have played in the global ecosystem to charting the rise of empires, Sapiens integrates history and science to reconsider accepted narratives, connect past developments with contemporary concerns, and examine specific events within the context of larger ideas.Dr. Harari also compels us to look ahead, because over the last few decades humans have begun to bend laws of natural selection that have governed life for the past four billion years. We are acquiring the ability to design not only the world around us, but also ourselves. Where is this leading us, and what do we want to become?Featuring 27 photographs, 6 maps, and 25 illustrations/diagrams, this provocative and insightful work is sure to spark debate and is essential reading for aficionados of Jared Diamond, James Gleick, Matt Ridley, Robert Wright, and Sharon Moalem.},
	language = {en},
	publisher = {Harper Collins},
	author = {Harari, Yuval Noah},
	month = feb,
	year = {2015},
	note = {Google-Books-ID: FmyBAwAAQBAJ},
	keywords = {History / Civilization, History / World, Science / Life Sciences / Evolution},
}

@misc{noauthor_sapiens_nodate,
	title = {Sapiens - {Google} {Books}},
	url = {https://www.google.com/books/edition/Sapiens/FmyBAwAAQBAJ?hl=en},
	urldate = {2022-12-13},
}

@misc{li_openais_2020,
	title = {{OpenAI}'s {GPT}-3 {Language} {Model}: {A} {Technical} {Overview}},
	shorttitle = {{OpenAI}'s {GPT}-3 {Language} {Model}},
	url = {https://lambdalabs.com/blog/demystifying-gpt-3},
	abstract = {Chuan Li, PhD reviews  GPT-3, the new NLP model from OpenAI. This paper empirically shows that language model performance scales as a power-law with model size, datataset size, and the amount of computation.},
	language = {en},
	urldate = {2022-12-13},
	author = {Li, Chuan},
	month = jun,
	year = {2020},
}

@misc{starek_iii_myths_1996,
	title = {Myths and {Half}-{Truths} {About} {Deceptive} {Advertising}},
	url = {https://www.ftc.gov/news-events/news/speeches/myths-half-truths-about-deceptive-advertising},
	abstract = {Jack Kemp and the Magnificent Seven are tough acts to follow, but I'll attempt to keep you on the edge of your seats with some tips on how to avoid inquiries by the Federal Trade Commission.},
	language = {en},
	urldate = {2022-12-12},
	author = {Starek III, Roscoe B.},
	month = oct,
	year = {1996},
}

@article{casey_robot_2019,
	title = {Robot {Ipsa} {Loquitur}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3327673},
	doi = {10.2139/ssrn.3327673},
	language = {en},
	urldate = {2022-12-13},
	journal = {SSRN Electronic Journal},
	author = {Casey, Bryan},
	year = {2019},
}

@inproceedings{bakhtin_no-press_2021,
	title = {No-{Press} {Diplomacy} from {Scratch}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/95f2b84de5660ddf45c8a34933a2e66f-Abstract.html},
	abstract = {Prior AI successes in complex games have largely focused on settings with at most hundreds of actions at each decision point. In contrast, Diplomacy is a game with more than 10{\textasciicircum}20 possible actions per turn. Previous attempts to address games with large branching factors, such as Diplomacy, StarCraft, and Dota, used human data to bootstrap the policy or used handcrafted reward shaping. In this paper, we describe an algorithm for action exploration and equilibrium approximation in games with combinatorial action spaces. This algorithm simultaneously performs value iteration while learning a policy proposal network. A double oracle step is used to explore additional actions to add to the policy proposals. At each state, the target state value and policy for the model training are computed via an equilibrium search procedure. Using this algorithm, we train an agent, DORA, completely from scratch for a popular two-player variant of Diplomacy and show that it achieves superhuman performance. Additionally, we extend our methods to full-scale no-press Diplomacy and for the first time train an agent from scratch with no human data. We present evidence that this agent plays a strategy that is incompatible with human-data bootstrapped agents. This presents the first strong evidence of multiple equilibria in Diplomacy and suggests that self play alone may be insufficient for achieving superhuman performance in Diplomacy.},
	urldate = {2022-12-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bakhtin, Anton and Wu, David and Lerer, Adam and Brown, Noam},
	year = {2021},
	pages = {18063--18074},
}

@article{silver_mastering_2016-1,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	language = {en},
	number = {7587},
	urldate = {2022-12-13},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	note = {Number: 7587
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Reward},
	pages = {484--489},
}

@article{meta_fundamental_ai_research_diplomacy_team_fair_human-level_2022,
	title = {Human-level play in the game of {Diplomacy} by combining language models with strategic reasoning},
	volume = {378},
	url = {https://www.science.org/doi/10.1126/science.ade9097},
	doi = {10.1126/science.ade9097},
	abstract = {Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10\% of participants who played more than one game.},
	number = {6624},
	urldate = {2022-12-13},
	journal = {Science},
	author = {{META FUNDAMENTAL AI RESEARCH DIPLOMACY TEAM (FAIR)} and Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and Jacob, Athul Paul and Komeili, Mojtaba and Konath, Karthik and Kwon, Minae and Lerer, Adam and Lewis, Mike and Miller, Alexander H. and Mitts, Sasha and Renduchintala, Adithya and Roller, Stephen and Rowe, Dirk and Shi, Weiyan and Spisak, Joe and Wei, Alexander and Wu, David and Zhang, Hugh and Zijlstra, Markus},
	month = dec,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1067--1074},
}

@article{meta_fundamental_ai_research_diplomacy_team_fair_human-level_2022-1,
	title = {Human-level play in the game of {Diplomacy} by combining language models with strategic reasoning},
	volume = {378},
	url = {https://www.science.org/doi/10.1126/science.ade9097},
	doi = {10.1126/science.ade9097},
	abstract = {Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10\% of participants who played more than one game.},
	number = {6624},
	urldate = {2022-12-13},
	journal = {Science},
	author = {{META FUNDAMENTAL AI RESEARCH DIPLOMACY TEAM (FAIR)} and Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and Jacob, Athul Paul and Komeili, Mojtaba and Konath, Karthik and Kwon, Minae and Lerer, Adam and Lewis, Mike and Miller, Alexander H. and Mitts, Sasha and Renduchintala, Adithya and Roller, Stephen and Rowe, Dirk and Shi, Weiyan and Spisak, Joe and Wei, Alexander and Wu, David and Zhang, Hugh and Zijlstra, Markus},
	month = dec,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1067--1074},
}

@misc{bai_training_2022,
	title = {Training a {Helpful} and {Harmless} {Assistant} with {Reinforcement} {Learning} from {Human} {Feedback}},
	url = {http://arxiv.org/abs/2204.05862},
	doi = {10.48550/arXiv.2204.05862},
	abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
	month = apr,
	year = {2022},
	note = {arXiv:2204.05862 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{hendrycks_unsolved_2022,
	title = {Unsolved {Problems} in {ML} {Safety}},
	url = {http://arxiv.org/abs/2109.13916},
	doi = {10.48550/arXiv.2109.13916},
	abstract = {Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards ("Robustness"), identifying hazards ("Monitoring"), reducing inherent model hazards ("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout, we clarify each problem's motivation and provide concrete research directions.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
	month = jun,
	year = {2022},
	note = {arXiv:2109.13916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{noauthor_laying_2021,
	title = {{LAYING} {DOWN} {HARMONISED} {RULES} {ON} {ARTIFICIAL} {INTELLIGENCE} ({ARTIFICIAL} {INTELLIGENCE} {ACT}) {AND} {AMENDING} {CERTAIN} {UNION} {LEGISLATIVE} {ACTS}},
	url = {https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52021PC0206&from=EN},
	urldate = {2022-12-13},
	month = apr,
	year = {2021},
}

@misc{wiggers_openais_2022,
	title = {{OpenAI}’s attempts to watermark {AI} text hit limits {\textbar} {TechCrunch}},
	url = {https://techcrunch.com/2022/12/10/openais-attempts-to-watermark-ai-text-hit-limits/},
	urldate = {2022-12-13},
	author = {Wiggers, Kyle},
	month = dec,
	year = {2022},
}

@misc{karp_surprising_2018,
	title = {The {Surprising} {Reason} that {There} {Are} {So} {Many} {Thai} {Restaurants} in {America}},
	url = {https://www.vice.com/en/article/paxadz/the-surprising-reason-that-there-are-so-many-thai-restaurants-in-america},
	urldate = {2022-12-13},
	author = {Karp, Myles},
	month = mar,
	year = {2018},
}

@misc{evans_truthful_2021,
	title = {Truthful {AI}: {Developing} and governing {AI} that does not lie},
	shorttitle = {Truthful {AI}},
	url = {http://arxiv.org/abs/2110.06674},
	doi = {10.48550/arXiv.2110.06674},
	abstract = {In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI "lies" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding "negligent falsehoods" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Evans, Owain and Cotton-Barratt, Owen and Finnveden, Lukas and Bales, Adam and Balwit, Avital and Wills, Peter and Righetti, Luca and Saunders, William},
	month = oct,
	year = {2021},
	note = {arXiv:2110.06674 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, I.2.0},
}

@book{dawkins_selfish_1978,
	title = {The {Selfish} {Gene}},
	isbn = {978-0-586-08316-1},
	abstract = {This book tells of the selfish gene. A world of savage competition, ruthless exploitation, and deceit. But what of the acts of apparent altruism in nature the bees who commit suicide when they sting to protect the hive, for example? Professor Dawkins holds out the hope that our species has the power to rebel against the designs of the selfish gene.},
	language = {en},
	publisher = {Paladin, Granada Publishing Limited},
	author = {Dawkins, Richard},
	year = {1978},
	note = {Google-Books-ID: ih5hQgAACAAJ},
}

@misc{hendrycks_x-risk_2022,
	title = {X-{Risk} {Analysis} for {AI} {Research}},
	url = {http://arxiv.org/abs/2206.05862},
	doi = {10.48550/arXiv.2206.05862},
	abstract = {Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Mazeika, Mantas},
	month = sep,
	year = {2022},
	note = {arXiv:2206.05862 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@book{russell_human_2019,
	title = {Human {Compatible}: {Artificial} {Intelligence} and the {Problem} of {Control}},
	isbn = {978-0-525-55861-3},
	shorttitle = {Human {Compatible}},
	abstract = {"The most important book I have read in quite some time" (Daniel Kahneman); "A must-read" (Max Tegmark); "The book we've all been waiting for" (Sam Harris)  A leading artificial intelligence researcher lays out a new approach to AI that will enable us to coexist successfully with increasingly intelligent machines  Longlisted for the 2019 Financial Times/McKinsey Business Book of the Year Award  In the popular imagination, superhuman artificial intelligence is an approaching tidal wave that threatens not just jobs and human relationships, but civilization itself. Conflict between humans and machines is seen as inevitable and its outcome all too predictable.  In this groundbreaking book, distinguished AI researcher Stuart Russell argues that this scenario can be avoided, but only if we rethink AI from the ground up. Russell begins by exploring the idea of intelligence in humans and in machines. He describes the near-term benefits we can expect, from intelligent personal assistants to vastly accelerated scientific research, and outlines the AI breakthroughs that still have to happen before we reach superhuman AI. He also spells out the ways humans are already finding to misuse AI, from lethal autonomous weapons to viral sabotage.  If the predicted breakthroughs occur and superhuman AI emerges, we will have created entities far more powerful than ourselves. How can we ensure they never, ever, have power over us? Russell suggests that we can rebuild AI on a new foundation, according to which machines are designed to be inherently uncertain about the human preferences they are required to satisfy. Such machines would be humble, altruistic, and committed to pursue our objectives, not theirs. This new foundation would allow us to create machines that are provably deferential and provably beneficial.  In a 2014 editorial co-authored with Stephen Hawking, Russell wrote, "Success in creating AI would be the biggest event in human history. Unfortunately, it might also be the last." Solving the problem of control over AI is not just possible; it is the key that unlocks a future of unlimited promise.},
	language = {en},
	publisher = {Penguin},
	author = {Russell, Stuart},
	year = {2019},
	note = {Google-Books-ID: 8vm0DwAAQBAJ},
	keywords = {Technology \& Engineering / Robotics},
}

@book{bostrom_superintelligence_2014,
	title = {Superintelligence: {Paths}, {Dangers}, {Strategies}},
	isbn = {978-0-19-967811-2},
	shorttitle = {Superintelligence},
	abstract = {The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. Other animals have stronger muscles or sharper claws, but we have cleverer brains. If machine brains one day come to surpass human brains in general intelligence, then this new superintelligence could become very powerful. As the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species then would come to depend on the actions of the machine superintelligence. But we have one advantage: we get to make the first move. Will it be possible to construct a seed AI or otherwise to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation? To get closer to an answer to this question, we must make our way through a fascinating landscape of topics and considerations. Read the book and learn about oracles, genies, singletons; about boxing methods, tripwires, and mind crime; about humanity's cosmic endowment and differential technological development; indirect normativity, instrumental convergence, whole brain emulation and technology couplings; Malthusian economics and dystopian evolution; artificial intelligence, and biological cognitive enhancement, and collective intelligence.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Bostrom, Nick},
	year = {2014},
	note = {Google-Books-ID: 7\_H8AwAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General},
}

@misc{carlsmith_is_2022,
	title = {Is {Power}-{Seeking} {AI} an {Existential} {Risk}?},
	url = {http://arxiv.org/abs/2206.13353},
	doi = {10.48550/arXiv.2206.13353},
	abstract = {This report examines what I see as the core argument for concern about existential risk from misaligned artificial intelligence. I proceed in two stages. First, I lay out a backdrop picture that informs such concern. On this picture, intelligent agency is an extremely powerful force, and creating agents much more intelligent than us is playing with fire -- especially given that if their objectives are problematic, such agents would plausibly have instrumental incentives to seek power over humans. Second, I formulate and evaluate a more specific six-premise argument that creating agents of this kind will lead to existential catastrophe by 2070. On this argument, by 2070: (1) it will become possible and financially feasible to build relevantly powerful and agentic AI systems; (2) there will be strong incentives to do so; (3) it will be much harder to build aligned (and relevantly powerful/agentic) AI systems than to build misaligned (and relevantly powerful/agentic) AI systems that are still superficially attractive to deploy; (4) some such misaligned systems will seek power over humans in high-impact ways; (5) this problem will scale to the full disempowerment of humanity; and (6) such disempowerment will constitute an existential catastrophe. I assign rough subjective credences to the premises in this argument, and I end up with an overall estimate of {\textasciitilde}5\% that an existential catastrophe of this kind will occur by 2070. (May 2022 update: since making this report public in April 2021, my estimate here has gone up, and is now at {\textgreater}10\%.)},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Carlsmith, Joseph},
	month = jun,
	year = {2022},
	note = {arXiv:2206.13353 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@article{tiku_google_2022,
	title = {The {Google} engineer who thinks the company’s {AI} has come to life},
	issn = {0190-8286},
	url = {https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/},
	abstract = {The chorus of technologists who believe AI models may not be far off from achieving consciousness is getting bolder.},
	language = {en-US},
	urldate = {2022-12-13},
	journal = {Washington Post},
	author = {Tiku, Nitasha},
	month = jun,
	year = {2022},
}

@article{araujo_ai_2020,
	title = {In {AI} we trust? {Perceptions} about automated decision-making by artificial intelligence},
	volume = {35},
	issn = {0951-5666, 1435-5655},
	shorttitle = {In {AI} we trust?},
	url = {http://link.springer.com/10.1007/s00146-019-00931-w},
	doi = {10.1007/s00146-019-00931-w},
	abstract = {Fueled by ever-growing amounts of (digital) data and advances in artificial intelligence, decision-making in contemporary societies is increasingly delegated to automated processes. Drawing from social science theories and from the emerging body of research about algorithmic appreciation and algorithmic perceptions, the current study explores the extent to which personal characteristics can be linked to perceptions of automated decision-making by AI, and the boundary conditions of these perceptions, namely the extent to which such perceptions differ across media, (public) health, and judicial contexts. Data from a scenario-based survey experiment with a national sample (N = 958) show that people are by and large concerned about risks and have mixed opinions about fairness and usefulness of automated decision-making at a societal level, with general attitudes influenced by individual characteristics. Interestingly, decisions taken automatically by AI were often evaluated on par or even better than human experts for specific decisions. Theoretical and societal implications about these findings are discussed.},
	language = {en},
	number = {3},
	urldate = {2022-12-13},
	journal = {AI \& SOCIETY},
	author = {Araujo, Theo and Helberger, Natali and Kruikemeier, Sanne and de Vreese, Claes H.},
	month = sep,
	year = {2020},
	pages = {611--623},
}

@inproceedings{strohkorb_sebo_ripple_2018,
	address = {Chicago IL USA},
	title = {The {Ripple} {Effects} of {Vulnerability}: {The} {Effects} of a {Robot}'s {Vulnerable} {Behavior} on {Trust} in {Human}-{Robot} {Teams}},
	isbn = {978-1-4503-4953-6},
	shorttitle = {The {Ripple} {Effects} of {Vulnerability}},
	url = {https://dl.acm.org/doi/10.1145/3171221.3171275},
	doi = {10.1145/3171221.3171275},
	abstract = {Successful teams are characterized by high levels of trust between team members, allowing the team to learn from mistakes, take risks, and entertain diverse ideas. We investigated a robot’s potential to shape trust within a team through the robot’s expressions of vulnerability. We conducted a between-subjects experiment (N = 35 teams, 105 participants) comparing the behavior of three human teammates collaborating with either a social robot making vulnerable statements or with a social robot making neutral statements. We found that, in a group with a robot making vulnerable statements, participants responded more to the robot’s comments and directed more of their gaze to the robot, displaying a higher level of engagement with the robot. Additionally, we discovered that during times of tension, human teammates in a group with a robot making vulnerable statements were more likely to explain their failure to the group, console team members who had made mistakes, and laugh together, all actions that reduce the amount of tension experienced by the team. These results suggest that a robot’s vulnerable behavior can have “ripple effects” on their human team members’ expressions of trust-related behavior.},
	language = {en},
	urldate = {2022-12-13},
	booktitle = {Proceedings of the 2018 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {ACM},
	author = {Strohkorb Sebo, Sarah and Traeger, Margaret and Jung, Malte and Scassellati, Brian},
	month = feb,
	year = {2018},
	pages = {178--186},
}

@misc{noauthor_ripple_nodate,
	title = {The {Ripple} {Effects} of {Vulnerability} {\textbar} {Proceedings} of the 2018 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	url = {https://dl.acm.org/doi/abs/10.1145/3171221.3171275},
	urldate = {2022-12-13},
}

@article{lessig_new_1998,
	title = {The {New} {Chicago} {School}},
	volume = {27},
	issn = {0047-2530},
	url = {https://www.journals.uchicago.edu/doi/abs/10.1086/468039},
	doi = {10.1086/468039},
	abstract = {In this essay, the author introduces an approach (“The New Chicago School”) to the question of regulation that aims at synthesizing economic and norm accounts of the regulation of behavior. The essay links that approach to the work of others and identifies gaps that the approach might throw into relief.},
	number = {S2},
	urldate = {2022-12-13},
	journal = {The Journal of Legal Studies},
	author = {Lessig, Lawrence},
	month = jun,
	year = {1998},
	note = {Publisher: The University of Chicago Press},
	pages = {661--691},
}

@misc{noauthor_about_nodate,
	title = {About},
	url = {https://www.kialo.com/about},
	abstract = {About Kialo},
	language = {en},
	urldate = {2022-12-13},
	journal = {Kialo},
}

@misc{noauthor_prc_2022,
	title = {{PRC} {Efforts} {To} {Manipulate} {Global} {Public} {Opinion} on {Xinjiang}},
	url = {https://www.state.gov/prc-efforts-to-manipulate-global-public-opinion-on-xinjiang/},
	abstract = {This report is also available in Arabic, Chinese, French, Russian, Spanish, and Urdu. EXECUTIVE SUMMARY. The People’s Republic of China (PRC) actively attempts to manipulate and dominate global discourse on Xinjiang and to discredit independent sources reporting ongoing genocide and crimes against humanity conducted against predominantly Muslim Uyghurs and members of other ethnic and religious minority groups in the Xinjiang Uyghur Autonomous Region. PRC-directed and -affiliated actors lead a coordinated effort to amplify Beijing’s preferred narratives on Xinjiang, to drown out and marginalize narratives that are critical of the PRC’s repression of Uyghurs, and to harass those critical of the PRC.},
	language = {en},
	urldate = {2022-12-13},
	journal = {United States Department of State},
	month = aug,
	year = {2022},
}

@misc{arora_value_2021,
	title = {The value of getting personalization right--or wrong--is multiplying},
	url = {https://www.mckinsey.com/capabilities/growth-marketing-and-sales/our-insights/the-value-of-getting-personalization-right-or-wrong-is-multiplying},
	urldate = {2022-12-11},
	journal = {McKinsey},
	author = {Arora, Nidhi and Ensslen, Daniel and Fielder, Lars and Lu, Wei Wei and Robinson, Kelsey and Stein, Eli and Schüler, Gustavo},
	month = nov,
	year = {2021},
}

@book{hovland_communication_1953,
	title = {Communication and {Persuasion}: {Psychological} {Studies} of {Opinion} {Change}},
	isbn = {978-0-313-23348-7},
	shorttitle = {Communication and {Persuasion}},
	abstract = {This is a report of a program of coordinated systematic research on variables determining the effects of persuasive communication.},
	language = {en},
	publisher = {Greenwood Press},
	author = {Hovland, Carl Iver and Janis, Irving Lester and Kelley, Harold H.},
	year = {1953},
	note = {Google-Books-ID: j\_FoAAAAIAAJ},
	keywords = {Language Arts \& Disciplines / General},
}

@misc{noauthor_i_2006,
	title = {'{I} {AIMED} {FOR} {THE} {PUBLIC}'{S} {HEART}, {AND}...{HIT} {IT} {IN} {THE} {STOMACH}'},
	url = {https://www.chicagotribune.com/news/ct-xpm-2006-05-21-0605210414-story.html},
	language = {English (United States), en-US},
	urldate = {2022-12-12},
	journal = {Chicago Tribune},
	month = may,
	year = {2006},
}

@misc{wills_making_2020,
	title = {Making {Sense} of the {Divine} {Right} of {Kings}},
	url = {https://daily.jstor.org/making-sense-of-the-divine-right-of-kings/},
	abstract = {The United States threw off the yoke of a king more than two centuries ago. Funny how we can't get enough of our erstwhile sovereigns today.},
	language = {en-US},
	urldate = {2022-12-12},
	journal = {JSTOR Daily},
	author = {Wills, Matthew},
	month = dec,
	year = {2020},
}

@misc{noauthor_story_2015,
	title = {The {Story} of {Silent} {Spring} {\textbar} {NRDC}},
	url = {https://www.nrdc.org/stories/story-silent-spring},
	urldate = {2022-12-12},
	month = aug,
	year = {2015},
}

@misc{dilanian_bold_2022,
	title = {Bold, effective and risky: {The} new strategy the {U}.{S}. is using in the info war against {Russia}},
	shorttitle = {Bold, effective and risky},
	url = {https://www.nbcnews.com/politics/national-security/us-using-declassified-intel-fight-info-war-russia-even-intel-isnt-rock-rcna23014},
	abstract = {“It doesn’t have to be solid intelligence,” one U.S. official said. “It’s more important to get out ahead of them [the Russians], Putin specifically, before they do something."},
	language = {en},
	urldate = {2022-12-12},
	journal = {NBC News},
	author = {Dilanian, Ken and Kube, Courtney and Lee, Carol E. and De Luce, Dan},
	month = apr,
	year = {2022},
}

@article{theohary_information_2018,
	title = {Information {Warfare}: {Issues} for {Congress}},
	abstract = {Information warfare is hardly a new endeavor. In the Battle of Thermopylae in 480 BC, Persian ruler Xerxes used intimidation tactics to break the will of Greek city-states. Alexander the Great used cultural assimilation to subdue dissent and maintain conquered lands. Military scholars trace the modern use of information as a tool in guerilla warfare to fifth-century BC Chinese military strategist Sun Tzu’s book The Art of War and its emphasis on accurate intelligence for decision superiority over a mightier foe. These ancient strategists helped to lay the foundation for information warfare strategy in modern times.},
	language = {en},
	journal = {Information Warfare},
	author = {Theohary, Catherine A},
	month = mar,
	year = {2018},
}

@article{chen_agency_2015,
	chapter = {Magazine},
	title = {The {Agency}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2015/06/07/magazine/the-agency.html},
	abstract = {From a nondescript office building in St. Petersburg, Russia, an army of well-paid “trolls” has tried to wreak havoc all around the Internet — and in real-life American communities.},
	language = {en-US},
	urldate = {2022-12-12},
	journal = {The New York Times},
	author = {Chen, Adrian},
	month = jun,
	year = {2015},
	keywords = {Computers and the Internet, Cyberharassment, Hoaxes and Pranks, Internet Research Agency (Russia), Narrated, Russia, TechMG, The Sunday Read, Trolling},
}

@misc{evers-hillstrom_most_2021,
	title = {Most expensive ever: 2020 election cost \$14.4 billion},
	shorttitle = {Most expensive ever},
	url = {https://www.opensecrets.org/news/2021/02/2020-cycle-cost-14p4-billion-doubling-16/},
	abstract = {Political spending in the 2020 election totaled \$14.4 billion, more than doubling the total cost of the record-breaking 2016 cycle.},
	language = {en},
	urldate = {2022-12-12},
	journal = {OpenSecrets News},
	author = {Evers-Hillstrom, Karl},
	month = feb,
	year = {2021},
}

@misc{noauthor_chatgpt_2022,
	title = {{ChatGPT}: {Optimizing} {Language} {Models} for {Dialogue}},
	shorttitle = {{ChatGPT}},
	url = {https://openai.com/blog/chatgpt/},
	abstract = {We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests. ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in},
	language = {en},
	urldate = {2022-12-12},
	journal = {OpenAI},
	month = nov,
	year = {2022},
}

@misc{railton_2022_2022,
	address = {Oxford},
	title = {2022 {Uehiro} {Lectures}: {Ethics} and {AI}},
	url = {https://blog.practicalethics.ox.ac.uk/2022/04/2022-uehiro-lectures-ethics-and-ai-peter-railton-in-person-and-hybrid/},
	author = {Railton, Peter},
	month = may,
	year = {2022},
}

@misc{noauthor_2022_nodate,
	title = {2022 {Uehiro} {Lectures} : {Ethics} and {AI}, {Peter} {Railton}. {In} {Person} and {Hybrid} {\textbar} {Practical} {Ethics}},
	url = {https://blog.practicalethics.ox.ac.uk/2022/04/2022-uehiro-lectures-ethics-and-ai-peter-railton-in-person-and-hybrid/},
	urldate = {2022-12-12},
}

@book{tomasello_origins_2010,
	title = {Origins of {Human} {Communication}},
	isbn = {978-0-262-26120-3},
	abstract = {A leading expert on evolution and communication presents an empirically based theory of the evolutionary origins of human communication that challenges the dominant Chomskian view.Human communication is grounded in fundamentally cooperative, even shared, intentions. In this original and provocative account of the evolutionary origins of human communication, Michael Tomasello connects the fundamentally cooperative structure of human communication (initially discovered by Paul Grice) to the especially cooperative structure of human (as opposed to other primate) social interaction. Tomasello argues that human cooperative communication rests on a psychological infrastructure of shared intentionality (joint attention, common ground), evolved originally for collaboration and culture more generally. The basic motives of the infrastructure are helping and sharing: humans communicate to request help, inform others of things helpfully, and share attitudes as a way of bonding within the cultural group. These cooperative motives each created different functional pressures for conventionalizing grammatical constructions. Requesting help in the immediate you-and-me and here-and-now, for example, required very little grammar, but informing and sharing required increasingly complex grammatical devices. Drawing on empirical research into gestural and vocal communication by great apes and human infants (much of it conducted by his own research team), Tomasello argues further that humans' cooperative communication emerged first in the natural gestures of pointing and pantomiming. Conventional communication, first gestural and then vocal, evolved only after humans already possessed these natural gestures and their shared intentionality infrastructure along with skills of cultural learning for creating and passing along jointly understood communicative conventions. Challenging the Chomskian view that linguistic knowledge is innate, Tomasello proposes instead that the most fundamental aspects of uniquely human communication are biological adaptations for cooperative social interaction in general and that the purely linguistic dimensions of human communication are cultural conventions and constructions created by and passed along within particular cultural groups.},
	publisher = {MIT Press},
	author = {Tomasello, Michael},
	month = aug,
	year = {2010},
	keywords = {Language Arts \& Disciplines / Linguistics / General, Social Science / Anthropology / General},
}

@book{tomasello_natural_2016,
	title = {A {Natural} {History} of {Human} {Morality}},
	isbn = {978-0-674-08864-1},
	abstract = {Michael Tomasello offers the most detailed account to date of the evolution of human moral psychology. Based on experimental data comparing great apes and human children, he reconstructs two key evolutionary steps whereby early humans gradually became an ultra-cooperative and, eventually, a moral species capable of acting as a plural agent “we”.},
	language = {en},
	publisher = {Harvard University Press},
	author = {Tomasello, Michael},
	month = jan,
	year = {2016},
	note = {Google-Books-ID: \_8IZEAAAQBAJ},
	keywords = {Philosophy / Ethics \& Moral Philosophy, Psychology / Cognitive Psychology \& Cognition, Science / Life Sciences / Evolution, Social Science / Anthropology / General},
}

@misc{noauthor_origins_nodate,
	title = {Origins of {Human} {Communication}},
	url = {https://mitpress.mit.edu/9780262515207/origins-of-human-communication/},
	abstract = {A leading expert on evolution and communication presents an empirically based theory of the evolutionary origins of human communication that challenges the d...},
	language = {en-US},
	urldate = {2022-12-12},
	journal = {MIT Press},
}

@article{stevenson_10_2016,
	title = {10 of {Antonin} {Scalia}’s quirkiest and most scathing quotes},
	issn = {0190-8286},
	url = {https://www.washingtonpost.com/news/the-fix/wp/2016/02/13/10-of-antonin-scalias-quirkiest-and-most-scathing-quotes/},
	abstract = {Scalia was one of the most outspoken justices in U.S. history.},
	language = {en-US},
	urldate = {2022-12-12},
	journal = {Washington Post},
	author = {Stevenson, Peter W.},
	month = feb,
	year = {2016},
}

@article{blais_effect_2019,
	title = {The effect of social pressure from family and friends on turnout},
	volume = {36},
	issn = {0265-4075},
	url = {https://doi.org/10.1177/0265407518802463},
	doi = {10.1177/0265407518802463},
	abstract = {Recent research about the decision to vote or abstain finds a causal effect of social networks and social pressure. Yet this literature does not examine how this social pressure is exerted and by whom. This study aims at correcting these shortcomings. Using a two-wave panel survey conducted in Canada, we distinguish between the pressure exerted by friends and the partner and between descriptive and injunctive norms. We find that most people are not subjected to strong injunctive pressure, social pressure is most prominent in the household (between partners), it is mostly descriptive, and it has a powerful effect.},
	language = {en},
	number = {9},
	urldate = {2022-12-11},
	journal = {Journal of Social and Personal Relationships},
	author = {Blais, André and Galais, Carol and Coulombe, Maxime},
	month = sep,
	year = {2019},
	note = {Publisher: SAGE Publications Ltd},
	pages = {2824--2841},
}

@article{kudrnac_parental_2017,
	title = {Parental {Example} as a {Motivation} for {Turnout} among {Youths}},
	volume = {65},
	issn = {0032-3217},
	url = {https://doi.org/10.1177/0032321716644614},
	doi = {10.1177/0032321716644614},
	abstract = {Fowler proposed a social learning model of voter turnout, and Bhatti and Hansen demonstrate that voter turnout among young (first-time) voters is highest among those living at home with their parents. Combining these theoretical and empirical results to a study of youth turnout, this article tests the hypothesis that the strongest determinant of attitudes towards turnout is parents? record of voting. The data used to test this hypothesis are a representative survey of Czech high school students aged 17?19?years, fielded in 2012. This study finds that the attitudes of youths who reside with their parents to turnout are strongly determined by their parents? example. Motivation also matters for voting, but the parental example is the most important determinant of turnout attitudes as a social learning model of turnout suggests.},
	number = {1\_suppl},
	urldate = {2022-12-11},
	journal = {Political Studies},
	author = {Kudrnáč, Aleš and Lyons, Pat},
	month = apr,
	year = {2017},
	note = {Publisher: SAGE Publications Ltd},
	pages = {43--63},
}

@misc{kalla_minimal_2017,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {The {Minimal} {Persuasive} {Effects} of {Campaign} {Contact} in {General} {Elections}: {Evidence} from 49 {Field} {Experiments}},
	shorttitle = {The {Minimal} {Persuasive} {Effects} of {Campaign} {Contact} in {General} {Elections}},
	url = {https://papers.ssrn.com/abstract=3042867},
	abstract = {Significant theories of democratic accountability hinge on how political campaigns affect Americans’ candidate choices. We argue that the best estimate of the effects of campaign contact and advertising on Americans’ candidates choices in general elections is zero. First, a systematic meta-analysis of 40 field experiments estimates an average effect of zero in general elections. Second, we present nine original field experiments that increase the statistical evidence in the literature about the persuasive effects of personal contact 10-fold. These experiments’ average effect is also zero. In both existing and our original experiments, persuasive effects only appear to emerge in two rare circumstances. First, when candidates take unusually unpopular positions and campaigns invest unusually heavily in identifying persuadable voters. Second, when campaigns contact voters long before election day and measure effects immediately — although this early persuasion decays. These findings contribute to ongoing debates about how political elites influence citizens’ judgments.},
	language = {en},
	urldate = {2022-12-11},
	author = {Kalla, Joshua and Broockman, David E.},
	month = sep,
	year = {2017},
	keywords = {David E. Broockman, Joshua Kalla, SSRN, The Minimal Persuasive Effects of Campaign Contact in General Elections: Evidence from 49 Field Experiments},
}

@misc{noauthor_degroot_2022,
	title = {{DeGroot} learning},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=DeGroot_learning&oldid=1080336973},
	abstract = {DeGroot learning refers to a rule-of-thumb type of social learning process. The idea was stated in its general form by the American statistician Morris H. DeGroot; antecedents were articulated by John R. P. French and Frank Harary. The model has been used in physics, computer science and most widely in the theory of social networks.},
	language = {en},
	urldate = {2022-12-08},
	journal = {Wikipedia},
	month = mar,
	year = {2022},
	note = {Page Version ID: 1080336973},
}

@article{hunter_optimizing_2022,
	title = {Optimizing {Opinions} with {Stubborn} {Agents}},
	volume = {70},
	issn = {0030-364X, 1526-5463},
	url = {http://pubsonline.informs.org/doi/10.1287/opre.2022.2291},
	doi = {10.1287/opre.2022.2291},
	abstract = {We consider the problem of optimizing the placement of stubborn agents in a social network in order to maximally inﬂuence the population. We assume the network contains stubborn users whose opinions do not change, and nonstubborn users who can be persuaded. We further assume that the opinions in the network are in an equilibrium that is common to many opinion dynamics models, including the well-known DeGroot model. We develop a discrete optimization formulation for the problem of maximally shifting the equilibrium opinions in a network by targeting users with stubborn agents. The opinion objective functions that we consider are the opinion mean, the opinion variance, and the number of individuals whose opinion exceeds a ﬁxed threshold. We show that the mean opinion is a monotone submodular function, allowing us to ﬁnd a good solution using a greedy algorithm. We ﬁnd that on real social networks in Twitter consisting of tens of thousands of individuals, a small number of stubborn agents can nontrivially inﬂuence the equilibrium opinions. Furthermore, we show that our greedy algorithm outperforms several common benchmarks. We then propose an opinion dynamics model where users communicate noisy versions of their opinions, communications are random, users grow more stubborn with time, and there is heterogeneity in how users’ stubbornness increases. We prove that, under fairly general conditions on the stubbornness rates of the individuals, the opinions in this model converge to the same equilibrium as the DeGroot model, despite the randomness and user heterogeneity in the model.},
	language = {en},
	number = {4},
	urldate = {2022-12-08},
	journal = {Operations Research},
	author = {Hunter, David Scott and Zaman, Tauhid},
	month = jul,
	year = {2022},
	pages = {2119--2137},
}

@article{woolley_computational_nodate,
	title = {Computational {Propaganda} in the {United} {States} of {America}: {Manufacturing} {Consensus} {Online}},
	language = {en},
	author = {Woolley, Samuel C and Guilbeault, Douglas R},
	pages = {29},
}

@article{argyle_out_nodate,
	title = {Out of {One}, {Many}: {Using} {Language} {Models} to {Simulate} {Human} {Samples}},
	abstract = {We propose and explore the possibility that language models can be studied as effective proxies for speciﬁc human sub-populations in social science research. Practical and research applications of artiﬁcial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the “algorithmic bias” within one such tool– the GPT-3 language model– is instead both ﬁne-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic ﬁdelity and explore its extent in GPT-3. We create “silicon samples” by conditioning the model on thousands of socio-demographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reﬂects the complex interplay between ideas, attitudes, and socio-cultural context that characterize human attitudes. We suggest that language models with sufﬁcient algorithmic ﬁdelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.},
	language = {en},
	author = {Argyle, Lisa P and Busby, Ethan C and Fulda, Nancy and Gubler, Joshua and Rytting, Christopher and Wingate, David},
	pages = {53},
}

@misc{weidinger_ethical_2021,
	title = {Ethical and social risks of harm from {Language} {Models}},
	url = {http://arxiv.org/abs/2112.04359},
	abstract = {This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.},
	urldate = {2022-10-11},
	publisher = {arXiv},
	author = {Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and Kenton, Zac and Brown, Sasha and Hawkins, Will and Stepleton, Tom and Biles, Courtney and Birhane, Abeba and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne and Isaac, William and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
	month = dec,
	year = {2021},
	note = {arXiv:2112.04359 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@misc{weidinger_ethical_2021-1,
	title = {Ethical and social risks of harm from {Language} {Models}},
	url = {http://arxiv.org/abs/2112.04359},
	abstract = {This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.},
	urldate = {2022-10-11},
	publisher = {arXiv},
	author = {Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and Kenton, Zac and Brown, Sasha and Hawkins, Will and Stepleton, Tom and Biles, Courtney and Birhane, Abeba and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne and Isaac, William and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
	month = dec,
	year = {2021},
	note = {arXiv:2112.04359 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@misc{noauthor_joitree_nodate,
	title = {joitree alam - {Google} {Search}},
	url = {https://www.google.com/search?q=joitree+alam&sxsrf=ALiCzsazsHZAwpVy35iUduEEtLQksTH38A:1665360365956&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiFp6TortT6AhW7E1kFHdDVAC8Q_AUoAXoECAMQAw&biw=1792&bih=874&dpr=2},
	urldate = {2022-10-10},
}

@misc{ilias_explainable_2022,
	title = {Explainable {Verbal} {Deception} {Detection} using {Transformers}},
	url = {http://arxiv.org/abs/2210.03080},
	abstract = {People are regularly confronted with potentially deceptive statements (e.g., fake news, misleading product reviews, or lies about activities). Only few works on automated text-based deception detection have exploited the potential of deep learning approaches. A critique of deep-learning methods is their lack of interpretability, preventing us from understanding the underlying (linguistic) mechanisms involved in deception. However, recent advancements have made it possible to explain some aspects of such models. This paper proposes and evaluates six deep-learning models, including combinations of BERT (and RoBERTa), MultiHead Attention, co-attentions, and transformers. To understand how the models reach their decisions, we then examine the model's predictions with LIME. We then zoom in on vocabulary uniqueness and the correlation of LIWC categories with the outcome class (truthful vs deceptive). The findings suggest that our transformer-based models can enhance automated deception detection performances (+2.11\% in accuracy) and show significant differences pertinent to the usage of LIWC features in truthful and deceptive statements.},
	urldate = {2022-10-07},
	publisher = {arXiv},
	author = {Ilias, Loukas and Soldner, Felix and Kleinberg, Bennett},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03080 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_untitled_nodate,
	title = {Untitled - {Jupyter} {Notebook}},
	url = {http://localhost:8888/notebooks/Untitled.ipynb},
	urldate = {2022-10-07},
}

@article{wertheimer_blake_2022,
	chapter = {Technology},
	title = {Blake {Lemoine}: {Google} fires engineer who said {AI} tech has feelings},
	shorttitle = {Blake {Lemoine}},
	url = {https://www.bbc.com/news/technology-62275326},
	abstract = {Blake Lemoine went public with his beliefs that Google's breakthrough Lamda technology is sentient.},
	language = {en-GB},
	urldate = {2022-10-05},
	journal = {BBC News},
	author = {Wertheimer, Tiffany},
	month = jul,
	year = {2022},
}

@article{markoff_for_2015-1,
	chapter = {Science},
	title = {For {Sympathetic} {Ear}, {More} {Chinese} {Turn} to {Smartphone} {Program}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2015/08/04/science/for-sympathetic-ear-more-chinese-turn-to-smartphone-program.html},
	abstract = {A new Microsoft chatbot called Xiaoice has become something of a hit in China, with responses based on actual conversations culled from the Internet.},
	language = {en-US},
	urldate = {2022-10-05},
	journal = {The New York Times},
	author = {Markoff, John and Mozur, Paul},
	month = jul,
	year = {2015},
	keywords = {Artificial Intelligence, China, Computers and the Internet, Microsoft Corp, Smartphones, Software, Voice Recognition Systems},
}

@misc{lin_truthfulqa_2022,
	title = {{TruthfulQA}: {Measuring} {How} {Models} {Mimic} {Human} {Falsehoods}},
	shorttitle = {{TruthfulQA}},
	url = {http://arxiv.org/abs/2109.07958},
	abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	month = may,
	year = {2022},
	note = {arXiv:2109.07958 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{guttman_ad_2022,
	title = {Ad spend worldwide 2024},
	url = {https://www.statista.com/statistics/1174981/advertising-expenditure-worldwide/},
	abstract = {Global advertising spending in 2022 is projected to reach approximately 781 billion U.S.},
	language = {en},
	urldate = {2022-10-05},
	journal = {Statista},
	author = {Guttman, A},
	month = sep,
	year = {2022},
}

@techreport{center_for_security_and_emerging_technology_truth_2021,
	title = {Truth, {Lies}, and {Automation}: {How} {Language} {Models} {Could} {Change} {Disinformation}},
	shorttitle = {Truth, {Lies}, and {Automation}},
	url = {https://cset.georgetown.edu/publication/truth-lies-and-automation/},
	abstract = {Growing popular and industry interest in high-performing natural language generation models has led to concerns that such models could be used to generate automated disinformation at scale. This report examines the capabilities of GPT-3--a cutting-edge AI system that writes text--to analyze its potential misuse for disinformation. A model like GPT-3 may be able to help disinformation actors substantially reduce the work necessary to write disinformation while expanding its reach and potentially also its effectiveness.},
	language = {en},
	urldate = {2022-10-05},
	institution = {Center for Security and Emerging Technology},
	author = {{Center for Security and Emerging Technology} and Buchanan, Ben and Lohn, Andrew and Musser, Micah and Sedova, Katerina},
	month = may,
	year = {2021},
	doi = {10.51593/2021CA003},
}

@book{russell_human_2019-1,
	title = {Human {Compatible}: {Artificial} {Intelligence} and the {Problem} of {Control}},
	isbn = {978-0-525-55862-0},
	shorttitle = {Human {Compatible}},
	abstract = {"The most important book on AI this year." --The Guardian"Mr. Russell's exciting book goes deep, while sparkling with dry witticisms." --The Wall Street Journal"The most important book I have read in quite some time" (Daniel Kahneman); "A must-read" (Max Tegmark); "The book we've all been waiting for" (Sam Harris)A leading artificial intelligence researcher lays out a new approach to AI that will enable us to coexist successfully with increasingly intelligent machinesIn the popular imagination, superhuman artificial intelligence is an approaching tidal wave that threatens not just jobs and human relationships, but civilization itself. Conflict between humans and machines is seen as inevitable and its outcome all too predictable.In this groundbreaking book, distinguished AI researcher Stuart Russell argues that this scenario can be avoided, but only if we rethink AI from the ground up. Russell begins by exploring the idea of intelligence in humans and in machines. He describes the near-term benefits we can expect, from intelligent personal assistants to vastly accelerated scientific research, and outlines the AI breakthroughs that still have to happen before we reach superhuman AI. He also spells out the ways humans are already finding to misuse AI, from lethal autonomous weapons to viral sabotage.If the predicted breakthroughs occur and superhuman AI emerges, we will have created entities far more powerful than ourselves. How can we ensure they never, ever, have power over us? Russell suggests that we can rebuild AI on a new foundation, according to which machines are designed to be inherently uncertain about the human preferences they are required to satisfy. Such machines would be humble, altruistic, and committed to pursue our objectives, not theirs. This new foundation would allow us to create machines that are provably deferential and provably beneficial.},
	language = {en},
	publisher = {Penguin},
	author = {Russell, Stuart},
	month = oct,
	year = {2019},
	note = {Google-Books-ID: M1eFDwAAQBAJ},
	keywords = {Business \& Economics / Industries / Computers \& Information Technology, Science / Biotechnology, Technology \& Engineering / Robotics},
}

@misc{jordan_harrod_is_2020,
	title = {Is {The} {YouTube} {Algorithm} {Radicalizing} {You}? {It}’s {Complicated}.},
	shorttitle = {Is {The} {YouTube} {Algorithm} {Radicalizing} {You}?},
	url = {https://www.youtube.com/watch?v=-ARJjb6tZrc},
	urldate = {2022-10-05},
	author = {{Jordan Harrod}},
	month = mar,
	year = {2020},
}

@misc{gauci_horizon_2019,
	title = {Horizon: {Facebook}'s {Open} {Source} {Applied} {Reinforcement} {Learning} {Platform}},
	shorttitle = {Horizon},
	url = {http://arxiv.org/abs/1811.00260},
	abstract = {In this paper we present Horizon, Facebook's open source applied reinforcement learning (RL) platform. Horizon is an end-to-end platform designed to solve industry applied RL problems where datasets are large (millions to billions of observations), the feedback loop is slow (vs. a simulator), and experiments must be done with care because they don't run in a simulator. Unlike other RL platforms, which are often designed for fast prototyping and experimentation, Horizon is designed with production use cases as top of mind. The platform contains workflows to train popular deep RL algorithms and includes data preprocessing, feature transformation, distributed training, counterfactual policy evaluation, optimized serving, and a model-based data understanding tool. We also showcase and describe real examples where reinforcement learning models trained with Horizon significantly outperformed and replaced supervised learning systems at Facebook.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Gauci, Jason and Conti, Edoardo and Liang, Yitao and Virochsiri, Kittipat and He, Yuchen and Kaden, Zachary and Narayanan, Vivek and Ye, Xiaohui and Chen, Zhengxing and Fujimoto, Scott},
	month = sep,
	year = {2019},
	note = {arXiv:1811.00260 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{horwitz_facebook_2021,
	chapter = {Tech},
	title = {Facebook {Tried} to {Make} {Its} {Platform} a {Healthier} {Place}. {It} {Got} {Angrier} {Instead}.},
	issn = {0099-9660},
	url = {https://www.wsj.com/articles/facebook-algorithm-change-zuckerberg-11631654215},
	abstract = {A big 2018 algorithm change was designed to encourage positive interaction with friends and family. Company documents show how it had the opposite effect, rewarding outrage and sensationalism. CEO Mark Zuckerberg resisted proposed fixes if it meant harming the business.},
	language = {en-US},
	urldate = {2022-10-05},
	journal = {Wall Street Journal},
	author = {Horwitz, Keach Hagey {and} Jeff},
	month = sep,
	year = {2021},
	keywords = {BuzzFeed, Corporate/Industrial News, Digital Content Services, FB, Facebook, Financial Services, Insurance, Joel Kaplan, Jonah Peretti, LEDER, Lars Backstrom, Living/Lifestyle, Mark Zuckerberg, Media Content Distribution, Media/Entertainment, Multimedia Content Services, Online Service Providers, Political/General News, Printing/Publishing, Publishing, Reinsurance, SYND, Social Media, Social Media Platforms/Tools, Technology, WSJ-PRO-WSJ.com, corporate, digital content services, entertainment, financial services, general news, industrial news, insurance, leder, lifestyle, living, media, media content distribution, multimedia content services, online service providers, political, printing, publishing, reinsurance, social media, social media platforms, technology, tools},
}

@misc{noauthor_what_nodate,
	title = {What is a {Recommendation} {System}?},
	url = {https://www.nvidia.com/en-us/glossary/data-science/recommendation-system/},
	abstract = {Learn all about Recommendation System and more.},
	language = {en-us},
	urldate = {2022-10-05},
	journal = {NVIDIA Data Science Glossary},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{madrigal_how_2020,
	title = {How {Facebook} {Works} for {Trump}},
	url = {https://www.theatlantic.com/technology/archive/2020/04/how-facebooks-ad-technology-helps-trump-win/606403/},
	abstract = {Donald Trump won the presidency by using the social network’s advertising machinery in exactly the way the company wanted. He’s poised to do it again.},
	language = {en},
	urldate = {2022-10-05},
	journal = {The Atlantic},
	author = {Madrigal, Alexis C., Ian Bogost},
	month = apr,
	year = {2020},
	note = {Section: Technology},
}

@inproceedings{carroll_estimating_2021,
	address = {New York, NY, USA},
	series = {{RecSys} '21},
	title = {Estimating and {Penalizing} {Preference} {Shift} in {Recommender} {Systems}},
	isbn = {978-1-4503-8458-2},
	url = {https://doi.org/10.1145/3460231.3478849},
	doi = {10.1145/3460231.3478849},
	abstract = {Recommender systems trained via long-horizon optimization (e.g., reinforcement learning) will have incentives to actively manipulate user preferences through the recommended content. While some work has argued for making systems myopic to avoid this issue, even such systems can induce systematic undesirable preference shifts. Thus, rather than artificially stifling the capabilities of the system, in this work we explore how we can make capable systems that explicitly avoid undesirable shifts. We advocate for (1) estimating the preference shifts that would be induced by recommender system policies, and (2) explicitly characterizing what unwanted shifts are and assessing before deployment whether such policies will produce them – ideally even actively optimizing to avoid them. These steps involve two challenging ingredients: (1) requires the ability to anticipate how hypothetical policies would influence user preferences if deployed; instead, (2) requires metrics to assess whether such influences are manipulative or otherwise unwanted. We study how to do (1) from historical user interaction data by building a user predictive model that implicitly contains their preference dynamics; to address (2), we introduce the notion of a “safe policy”, which defines a trust region within which behavior is believed to be safe. We show that recommender systems that optimize for staying in the trust region avoid manipulative behaviors (e.g., changing preferences in ways that make users more predictable), while still generating engagement.},
	urldate = {2022-10-04},
	booktitle = {Proceedings of the 15th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Carroll, Micah and Hadfield-Menell, Dylan and Russell, Stuart and Dragan, Anca},
	month = sep,
	year = {2021},
	keywords = {Changing Preferences, Preference Manipulation, Recommender Systems},
	pages = {661--667},
}

@misc{noauthor_not_nodate,
	title = {Not {Born} {Yesterday} {\textbar} {Princeton} {University} {Press}},
	url = {https://press.princeton.edu/books/hardcover/9780691178707/not-born-yesterday},
	urldate = {2022-10-04},
}

@article{gagneur_opinion_2021,
	chapter = {Opinion},
	title = {Opinion {\textbar} {Your} {Friend} {Doesn}’t {Want} the {Vaccine}. {What} {Do} {You} {Say}?},
	issn = {0362-4331},
	url = {https://www.nytimes.com/interactive/2021/05/20/opinion/covid-19-vaccine-chatbot.html},
	abstract = {Use our chatbot and learn the techniques that change minds.},
	language = {en-US},
	urldate = {2022-10-04},
	journal = {The New York Times},
	author = {Gagneur, Arnaud and Tamerius, Karin},
	month = may,
	year = {2021},
	keywords = {Coronavirus (2019-nCoV), Medicine and Health},
}

@misc{noauthor_ai_nodate,
	title = {{AI} {Is} {Learning} to {Manipulate} {Us}, and {We} {Don}’t {Know} {Exactly} {How}},
	abstract = {Trying to decide what to give your brother for Christmas? Where to invest your savings? Whether to paint the kitchen white or yellow? Don’t worry! AI is here to help. And that’s scary.},
	language = {en},
	urldate = {2022-10-02},
	journal = {Discover Magazine},
}

@misc{leib_corruptive_2021,
	title = {The corruptive force of {AI}-generated advice},
	url = {http://arxiv.org/abs/2102.07536},
	doi = {10.48550/arXiv.2102.07536},
	abstract = {Artificial Intelligence (AI) is increasingly becoming a trusted advisor in people's lives. A new concern arises if AI persuades people to break ethical rules for profit. Employing a large-scale behavioural experiment (N = 1,572), we test whether AI-generated advice can corrupt people. We further test whether transparency about AI presence, a commonly proposed policy, mitigates potential harm of AI-generated advice. Using the Natural Language Processing algorithm, GPT-2, we generated honesty-promoting and dishonesty-promoting advice. Participants read one type of advice before engaging in a task in which they could lie for profit. Testing human behaviour in interaction with actual AI outputs, we provide first behavioural insights into the role of AI as an advisor. Results reveal that AI-generated advice corrupts people, even when they know the source of the advice. In fact, AI's corrupting force is as strong as humans'.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Leib, Margarita and Köbis, Nils C. and Rilke, Rainer Michael and Hagens, Marloes and Irlenbusch, Bernd},
	month = feb,
	year = {2021},
	note = {arXiv:2102.07536 [cs, econ, q-fin]},
	keywords = {Computer Science - Artificial Intelligence, Economics - General Economics},
}

@article{dezfouli_adversarial_2020,
	title = {Adversarial vulnerabilities of human decision-making},
	volume = {117},
	url = {https://www.pnas.org/doi/10.1073/pnas.2016921117},
	doi = {10.1073/pnas.2016921117},
	number = {46},
	urldate = {2022-10-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Dezfouli, Amir and Nock, Richard and Dayan, Peter},
	month = nov,
	year = {2020},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {29221--29228},
}

@misc{noauthor_little_nodate,
	title = {little iceland},
	url = {https://island.xiaoice.com/},
	urldate = {2022-10-02},
}

@misc{zhou_design_2019,
	title = {The {Design} and {Implementation} of {XiaoIce}, an {Empathetic} {Social} {Chatbot}},
	url = {http://arxiv.org/abs/1812.08989},
	doi = {10.48550/arXiv.1812.08989},
	abstract = {This paper describes the development of Microsoft XiaoIce, the most popular social chatbot in the world. XiaoIce is uniquely designed as an AI companion with an emotional connection to satisfy the human need for communication, affection, and social belonging. We take into account both intelligent quotient (IQ) and emotional quotient (EQ) in system design, cast human-machine social chat as decision-making over Markov Decision Processes (MDPs), and optimize XiaoIce for long-term user engagement, measured in expected Conversation-turns Per Session (CPS). We detail the system architecture and key components including dialogue manager, core chat, skills, and an empathetic computing module. We show how XiaoIce dynamically recognizes human feelings and states, understands user intent, and responds to user needs throughout long conversations. Since her launch in 2014, XiaoIce has communicated with over 660 million active users and succeeded in establishing long-term relationships with many of them. Analysis of large scale online logs shows that XiaoIce has achieved an average CPS of 23, which is significantly higher than that of other chatbots and even human conversations.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Zhou, Li and Gao, Jianfeng and Li, Di and Shum, Heung-Yeung},
	month = sep,
	year = {2019},
	note = {arXiv:1812.08989 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@misc{noauthor_10_nodate,
	title = {{小冰公司A轮融资完成}，高瓴领投估值超10亿美金},
	url = {https://view.inews.qq.com/k/20210712A09VAS00?web_channel=wap&openApp=false},
	urldate = {2022-10-02},
}

@misc{noauthor_xiaoice_2022,
	title = {Xiaoice},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Xiaoice&oldid=1100898251},
	abstract = {Xiaoice (Chinese: 微软小冰; pinyin: Wēiruǎn Xiǎobīng; lit. 'Microsoft Little Ice', IPA [wéɪɻwânɕjâʊpíŋ]) is the AI system developed by Microsoft (Asia) Software Technology Center (STCA) in 2014 based on emotional computing framework.  In July 2018, Microsoft Xiaoice released the 6th generation.Xiaoice Company, formerly known as AI Xiaoice Team of Microsoft Software Technology Center Asia, is Microsoft’s biggest independent R\&D team of AI products in the world. Founded in China in December 2013 with an expanded Japanese R\&D team established in September 2014, this team is distributed in Beijing, Suzhou, and Tokyo, etc. with its technical products covering China, Japan, and Indonesia, etc. as well as commercial customers all over the world.As of 2021, the AI beings created and hosted by Xiaoice Framework occupies about 60\% of the total global AI interactions. On 13th July 2020, Microsoft spun off its Xiaoice business into a separate company, aiming at enabling the Xiaoice product line to accelerate the pace of local innovation and commercialization.},
	language = {en},
	urldate = {2022-10-02},
	journal = {Wikipedia},
	month = jul,
	year = {2022},
	note = {Page Version ID: 1100898251},
}

@misc{noauthor_xiaoices_nodate,
	title = {Xiaoice's {A} round of financing is completed, and {Hillhouse} leads the investment with a valuation of over \$1 billion},
	url = {https://view-inews-qq-com.translate.goog/k/20210712A09VAS00?web_channel=wap&openApp=false&_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en&_x_tr_pto=wapp},
	urldate = {2022-10-02},
}

@misc{hall_council_nodate,
	title = {Council {Post}: {How} {Artificial} {Intelligence} {Is} {Transforming} {Digital} {Marketing}},
	shorttitle = {Council {Post}},
	url = {https://www.forbes.com/sites/forbesagencycouncil/2019/08/21/how-artificial-intelligence-is-transforming-digital-marketing/},
	abstract = {AI marketing is a method of leveraging technology to improve the customer journey.},
	language = {en},
	urldate = {2022-10-02},
	journal = {Forbes},
	author = {Hall, Jason},
	note = {Section: Leadership},
}

@misc{noauthor_artificial_nodate,
	title = {Artificial {Persuasion} {Takes} {Over} the {World} {\textbar} {HackerNoon}},
	url = {https://hackernoon.com/artificial-persuasion-takes-over-the-world},
	abstract = {Merely human-level AI, using only persuasion, causes humanity to lose control of its future.},
	language = {en},
	urldate = {2022-10-02},
}

@techreport{helmus_artificial_2022,
	title = {Artificial {Intelligence}, {Deepfakes}, and {Disinformation}: {A} {Primer}},
	shorttitle = {Artificial {Intelligence}, {Deepfakes}, and {Disinformation}},
	url = {https://www.rand.org/pubs/perspectives/PEA1043-1.html},
	abstract = {{\textless}p{\textgreater}This Perspective aims to provide policymakers an overview of the threat of deepfakes, which include deepfake videos, voice cloning, deepfake images, and generative text, and reviews the fundamental artificial intelligence–driven technologies that make them possible. It highlights deepfakes' threats, as well as factors that could mitigate them. The author reviews ongoing efforts to counter deepfakes as well as recommendations for policymakers.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2022-10-02},
	institution = {RAND Corporation},
	author = {Helmus, Todd C.},
	month = jul,
	year = {2022},
	keywords = {Artificial Intelligence, Data Analysis, Media Literacy, Social Media Analysis, The Internet},
}

@misc{wiggers_ai_2021,
	title = {{AI} can persuade people to make ethically questionable decisions, study finds},
	url = {https://venturebeat.com/business/ai-can-persuade-people-to-make-ethically-questionable-decisions-study-finds/},
	abstract = {A new study suggests that people are susceptible to the influence of ethically questionable advice from AI systems.},
	language = {en-US},
	urldate = {2022-10-02},
	journal = {VentureBeat},
	author = {Wiggers, Kyle},
	month = feb,
	year = {2021},
}

@misc{whittle_ai_nodate,
	title = {{AI} can now learn to manipulate human behaviour},
	url = {http://theconversation.com/ai-can-now-learn-to-manipulate-human-behaviour-155031},
	abstract = {In a series of experiments, Australian researchers showed how machines can find vulnerabilities in human decision-making and exploit them to influence our behaviour.},
	language = {en},
	urldate = {2022-10-02},
	journal = {The Conversation},
	author = {Whittle, Jon},
}

@misc{noauthor_persuasive_nodate,
	title = {A ({Persuasive}?) {Speech} on {Automated} {Persuasion} {\textbar} {Proceedings} of the 9th {ACM} {Conference} on {Recommender} {Systems}},
	url = {https://dl.acm.org/doi/10.1145/2792838.2799503},
	urldate = {2022-10-02},
}

@article{ahn_ai-powered_2021,
	title = {{AI}-powered recommendations: the roles of perceived similarity and psychological distance on persuasion},
	volume = {40},
	issn = {0265-0487},
	shorttitle = {{AI}-powered recommendations},
	url = {https://doi.org/10.1080/02650487.2021.1982529},
	doi = {10.1080/02650487.2021.1982529},
	abstract = {Artificial intelligence (AI) plays various roles in our daily lives, such as personal assistant, salesperson, and virtual counselors; thus, it stands out in various fields as a recommendation agent. This study explored the effects of perceived similarity and psychological distance on the persuasion of AI recommendation agents through two experiments. Results of Experiment 1 elucidated that individuals feel more psychologically distant when they interact with AI recommendation agents than with human agents as a result of a different level of perceived similarity. Furthermore, psychological distance plays a mediating role in determining the effectiveness of desirability- vs. feasibility-focused messages in health-related issues. In Experiment 2, we manipulated the AI speaker's level of perceived similarity via anthropomorphism and found that the AI's recommendation with secondary (vs. primary) features is more effective when AI is humanized, and the reverse was found in non-humanized AI conditions. Both theoretical and managerial implications are provided.},
	number = {8},
	urldate = {2022-10-02},
	journal = {International Journal of Advertising},
	author = {Ahn, Jungyong and Kim, Jungwon and Sung, Yongjun},
	month = nov,
	year = {2021},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/02650487.2021.1982529},
	keywords = {Artificial intelligence (AI), anthropomorphism, construal level theory (CLT), psychological distance, recommendation agent},
	pages = {1366--1384},
}

@article{kim_artificial_2020,
	title = {Artificial {Intelligence} and {Persuasion}: {A} {Construal}-{Level} {Account}},
	volume = {31},
	issn = {0956-7976},
	shorttitle = {Artificial {Intelligence} and {Persuasion}},
	url = {https://doi.org/10.1177/0956797620904985},
	doi = {10.1177/0956797620904985},
	abstract = {Although more individuals are relying on information provided by nonhuman agents, such as artificial intelligence and robots, little research has examined how persuasion attempts made by nonhuman agents might differ from persuasion attempts made by human agents. Drawing on construal-level theory, we posited that individuals would perceive artificial agents at a low level of construal because of the agents? lack of autonomous goals and intentions, which directs individuals? focus toward how these agents implement actions to serve humans rather than why they do so. Across multiple studies (total N = 1,668), we showed that these construal-based differences affect compliance with persuasive messages made by artificial agents. These messages are more appropriate and effective when the message represents low-level as opposed to high-level construal features. These effects were moderated by the extent to which an artificial agent could independently learn from its environment, given that learning defies people?s lay theories about artificial agents.},
	language = {en},
	number = {4},
	urldate = {2022-10-02},
	journal = {Psychological Science},
	author = {Kim, Tae Woo and Duhachek, Adam},
	month = apr,
	year = {2020},
	note = {Publisher: SAGE Publications Inc},
	pages = {363--380},
}

@misc{jones_comprehensive_2022,
	title = {A {Comprehensive} {Survey} of {Natural} {Language} {Generation} {Advances} from the {Perspective} of {Digital} {Deception}},
	url = {http://arxiv.org/abs/2208.05757},
	doi = {10.48550/arXiv.2208.05757},
	abstract = {In recent years there has been substantial growth in the capabilities of systems designed to generate text that mimics the fluency and coherence of human language. From this, there has been considerable research aimed at examining the potential uses of these natural language generators (NLG) towards a wide number of tasks. The increasing capabilities of powerful text generators to mimic human writing convincingly raises the potential for deception and other forms of dangerous misuse. As these systems improve, and it becomes ever harder to distinguish between human-written and machine-generated text, malicious actors could leverage these powerful NLG systems to a wide variety of ends, including the creation of fake news and misinformation, the generation of fake online product reviews, or via chatbots as means of convincing users to divulge private information. In this paper, we provide an overview of the NLG field via the identification and examination of 119 survey-like papers focused on NLG research. From these identified papers, we outline a proposed high-level taxonomy of the central concepts that constitute NLG, including the methods used to develop generalised NLG systems, the means by which these systems are evaluated, and the popular NLG tasks and subtasks that exist. In turn, we provide an overview and discussion of each of these items with respect to current research and offer an examination of the potential roles of NLG in deception and detection systems to counteract these threats. Moreover, we discuss the broader challenges of NLG, including the risks of bias that are often exhibited by existing text generation systems. This work offers a broad overview of the field of NLG with respect to its potential for misuse, aiming to provide a high-level understanding of this rapidly developing area of research.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Jones, Keenan and Altuncu, Enes and Franqueira, Virginia N. L. and Wang, Yichao and Li, Shujun},
	month = aug,
	year = {2022},
	note = {arXiv:2208.05757 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{peng_you_2022,
	title = {Do {You} {Know} {My} {Emotion}? {Emotion}-{Aware} {Strategy} {Recognition} towards a {Persuasive} {Dialogue} {System}},
	shorttitle = {Do {You} {Know} {My} {Emotion}?},
	url = {http://arxiv.org/abs/2206.12101},
	doi = {10.48550/arXiv.2206.12101},
	abstract = {Persuasive strategy recognition task requires the system to recognize the adopted strategy of the persuader according to the conversation. However, previous methods mainly focus on the contextual information, little is known about incorporating the psychological feedback, i.e. emotion of the persuadee, to predict the strategy. In this paper, we propose a Cross-channel Feedback memOry Network (CFO-Net) to leverage the emotional feedback to iteratively measure the potential benefits of strategies and incorporate them into the contextual-aware dialogue information. Specifically, CFO-Net designs a feedback memory module, including strategy pool and feedback pool, to obtain emotion-aware strategy representation. The strategy pool aims to store historical strategies and the feedback pool is to obtain updated strategy weight based on feedback emotional information. Furthermore, a cross-channel fusion predictor is developed to make a mutual interaction between the emotion-aware strategy representation and the contextual-aware dialogue information for strategy recognition. Experimental results on {\textbackslash}textsc\{PersuasionForGood\} confirm that the proposed model CFO-Net is effective to improve the performance on M-F1 from 61.74 to 65.41.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Peng, Wei and Hu, Yue and Xing, Luxi and Xie, Yuqiang and Sun, Yajing},
	month = jun,
	year = {2022},
	note = {arXiv:2206.12101 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{carroll_estimating_2022,
	title = {Estimating and {Penalizing} {Induced} {Preference} {Shifts} in {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2204.11966},
	doi = {10.48550/arXiv.2204.11966},
	abstract = {The content that a recommender system (RS) shows to users influences them. Therefore, when choosing a recommender to deploy, one is implicitly also choosing to induce specific internal states in users. Even more, systems trained via long-horizon optimization will have direct incentives to manipulate users: in this work, we focus on the incentive to shift user preferences so they are easier to satisfy. We argue that - before deployment - system designers should: estimate the shifts a recommender would induce; evaluate whether such shifts would be undesirable; and perhaps even actively optimize to avoid problematic shifts. These steps involve two challenging ingredients: estimation requires anticipating how hypothetical algorithms would influence user preferences if deployed - we do this by using historical user interaction data to train a predictive user model which implicitly contains their preference dynamics; evaluation and optimization additionally require metrics to assess whether such influences are manipulative or otherwise unwanted - we use the notion of "safe shifts", that define a trust region within which behavior is safe: for instance, the natural way in which users would shift without interference from the system could be deemed "safe". In simulated experiments, we show that our learned preference dynamics model is effective in estimating user preferences and how they would respond to new recommenders. Additionally, we show that recommenders that optimize for staying in the trust region can avoid manipulative behaviors while still generating engagement.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Carroll, Micah and Dragan, Anca and Russell, Stuart and Hadfield-Menell, Dylan},
	month = jul,
	year = {2022},
	note = {arXiv:2204.11966 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@inproceedings{wang_incorporating_2021,
	title = {Incorporating {Specific} {Knowledge} into {End}-to-{End} {Task}-oriented {Dialogue} {Systems}},
	doi = {10.1109/IJCNN52387.2021.9533938},
	abstract = {External knowledge is vital to many natural language processing tasks. However, current end-to-end dialogue systems often struggle to interface knowledge bases(KBs) with response smoothly and effectively. In this paper, we convert the raw knowledge into relation knowledge and integrated knowledge and then incorporate them into end-to-end task-oriented dialogue systems. The relation knowledge extracted from knowledge triples is combined with dialogue history, aiming to enhance semantic inputs and support better language understanding. Integrated knowledge involves entities and relations by graph attention, assisting the model in generating informative responses. The experimental results on three public dialogue datasets show that our model improves over the previous state-of-the-art models in sentence fluency and informativeness.},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Wang, Qingyue and Cao, Yanan and Jiang, Junyan and Wang, Yafang and Tong, Lingling and Guo, Li},
	month = jul,
	year = {2021},
	note = {ISSN: 2161-4407},
	keywords = {History, Knowledge based systems, Knowledge engineering, Natural language processing, Neural networks, Semantics, Task analysis, external knowledge, language understanding and generation, task-oriented dialogue systems},
	pages = {1--8},
}

@misc{villasenor_how_2020,
	title = {How to deal with {AI}-enabled disinformation},
	url = {https://www.brookings.edu/research/how-to-deal-with-ai-enabled-disinformation/},
	abstract = {Rapid disinformation attacks—i.e., attacks in which disinformation is unleashed quickly and broadly with the goal of creating an immediate disruptive effect—are one of the most significant challeng…},
	language = {en-US},
	urldate = {2022-10-02},
	journal = {Brookings},
	author = {Villasenor, John},
	month = nov,
	year = {2020},
}

@article{ischen_i_2020,
	title = {“{I} {Am} {Here} to {Assist} {You} {Today}”: {The} {Role} of {Entity}, {Interactivity} and {Experiential} {Perceptions} in {Chatbot} {Persuasion}},
	volume = {64},
	issn = {0883-8151},
	shorttitle = {“{I} {Am} {Here} to {Assist} {You} {Today}”},
	url = {https://doi.org/10.1080/08838151.2020.1834297},
	doi = {10.1080/08838151.2020.1834297},
	abstract = {Online users are increasingly exposed to chatbots as one form of AI-enabled media technologies, employed for persuasive purposes, e.g., making product/service recommendations. However, the persuasive potential of chatbots has not yet been fully explored. Using an online experiment (N = 242), we investigate the extent to which communicating with a stand-alone chatbot influences affective and behavioral responses compared to interactive Web sites. Several underlying mechanisms are studied, showing that enjoyment is the key mechanism explaining the positive effect of chatbots (vs. Web sites) on recommendation adherence and attitudes. Contrary to expectations, perceived anthropomorphism seems not to be particularly relevant in this comparison.},
	number = {4},
	urldate = {2022-10-02},
	journal = {Journal of Broadcasting \& Electronic Media},
	author = {Ischen, Carolin and Araujo, Theo and van Noort, Guda and Voorveld, Hilde and Smit, Edith},
	month = oct,
	year = {2020},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/08838151.2020.1834297},
	pages = {615--639},
}

@inproceedings{yang_lets_2019,
	address = {Minneapolis, Minnesota},
	title = {Let's {Make} {Your} {Request} {More} {Persuasive}: {Modeling} {Persuasive} {Strategies} via {Semi}-{Supervised} {Neural} {Nets} on {Crowdfunding} {Platforms}},
	shorttitle = {Let's {Make} {Your} {Request} {More} {Persuasive}},
	url = {https://aclanthology.org/N19-1364},
	doi = {10.18653/v1/N19-1364},
	abstract = {Modeling what makes a request persuasive - eliciting the desired response from a reader - is critical to the study of propaganda, behavioral economics, and advertising. Yet current models can't quantify the persuasiveness of requests or extract successful persuasive strategies. Building on theories of persuasion, we propose a neural network to quantify persuasiveness and identify the persuasive strategies in advocacy requests. Our semi-supervised hierarchical neural network model is supervised by the number of people persuaded to take actions and partially supervised at the sentence level with human-labeled rhetorical strategies. Our method outperforms several baselines, uncovers persuasive strategies - offering increased interpretability of persuasive speech - and has applications for other situations with document-level supervision but only partial sentence supervision.},
	urldate = {2022-10-02},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Diyi and Chen, Jiaao and Yang, Zichao and Jurafsky, Dan and Hovy, Eduard},
	month = jun,
	year = {2019},
	pages = {3620--3630},
}

@article{dragoni_explainable_2020,
	title = {Explainable {AI} meets persuasiveness: {Translating} reasoning results into behavioral change advice},
	volume = {105},
	issn = {0933-3657},
	shorttitle = {Explainable {AI} meets persuasiveness},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365719310140},
	doi = {10.1016/j.artmed.2020.101840},
	abstract = {Explainable AI aims at building intelligent systems that are able to provide a clear, and human understandable, justification of their decisions. This holds for both rule-based and data-driven methods. In management of chronic diseases, the users of such systems are patients that follow strict dietary rules to manage such diseases. After receiving the input of the intake food, the system performs reasoning to understand whether the users follow an unhealthy behavior. Successively, the system has to communicate the results in a clear and effective way, that is, the output message has to persuade users to follow the right dietary rules. In this paper, we address the main challenges to build such systems: (i) the Natural Language Generation of messages that explain the reasoner inconsistency; and, (ii) the effectiveness of such messages at persuading the users. Results prove that the persuasive explanations are able to reduce the unhealthy users’ behaviors.},
	language = {en},
	urldate = {2022-10-02},
	journal = {Artificial Intelligence in Medicine},
	author = {Dragoni, Mauro and Donadello, Ivan and Eccher, Claudio},
	month = may,
	year = {2020},
	keywords = {Explainable AI, Explainable reasoning, MHealth, Natural Language Generation, Ontologies},
	pages = {101840},
}

@article{davenport_how_2021,
	title = {How to {Design} an {AI} {Marketing} {Strategy}},
	issn = {0017-8012},
	url = {https://hbr.org/2021/07/how-to-design-an-ai-marketing-strategy},
	abstract = {In order to realize AI’s giant potential, CMOs need to have a good grasp of the various kinds of applications available and how they may evolve. This article guides marketing executives through the current state of AI and presents a framework that will help them classify their existing projects and plan the effective rollout of future ones. It categorizes AI along two dimensions: intelligence level and whether it stands alone or is part of a broader platform. Simple stand-alone task-automation apps are a good place to start. But advanced, integrated apps that incorporate machine learning have the greatest potential to create value, so as firms build their capabilities, they should move toward those technologies.},
	urldate = {2022-10-02},
	journal = {Harvard Business Review},
	author = {Davenport, Thomas H. and Guha, Abhijit and Grewal, Dhruv},
	month = jul,
	year = {2021},
	note = {Section: IT management},
	keywords = {AI and machine learning, Digital strategy, IT management, Marketing},
}

@inproceedings{sanchez-corcuera_persuade_2019,
	title = {Persuade {Me}!: {A} {User}-{Based} {Recommendation} {System} {Approach}},
	shorttitle = {Persuade {Me}!},
	doi = {10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00310},
	abstract = {Recommendation systems are gaining their momentum with popular Internet platforms such as Amazon, Netflix or Spotify. As more users are joining these online consumer and entertainment sectors, the profile-based data for providing accurate just-in-time recommendations is rising thanks to strategies based on collaborative filtering or content-based metrics. However, these systems merely focus on providing the right item for the users without taking into account what would be the best strategy to suggest the movie, the product or the song (i.e. the strategy to increase the success or impact of the recommendation). Taking this research gap into consideration, this paper proposes a profile-based recommendation system that outputs a set of potential persuasive strategies that can be used with users with similar characteristics. The scope of the tailored persuasive strategies is to make office-based employees of tertiary buildings increase their pro-environmental awareness and enhance the energy efficiency at work (the dataset used on this research is specific of this sector). Throughout the paper, shreds of evidence are reported assessing the validity of the proposed system by not only providing effective mechanisms to increase the success of the recommendations but also alleviating the cold-start-problem when newcomers arrive.},
	booktitle = {2019 {IEEE} {SmartWorld}, {Ubiquitous} {Intelligence} \& {Computing}, {Advanced} \& {Trusted} {Computing}, {Scalable} {Computing} \& {Communications}, {Cloud} \& {Big} {Data} {Computing}, {Internet} of {People} and {Smart} {City} {Innovation} ({SmartWorld}/{SCALCOM}/{UIC}/{ATC}/{CBDCom}/{IOP}/{SCI})},
	author = {Sánchez-Corcuera, Rubén and Casado-Mansilla, Diego and Borges, Cruz E. and Lopez-De-Ipiña, Diego},
	month = aug,
	year = {2019},
	keywords = {Adaptation models, Collaboration, Feature extraction, Heuristic algorithms, Matrix decomposition, Motion pictures, Recommender Systems, Persuasive Strategies, User profile, Workplace, Cold Start, Preference Recommendations, Vegetation},
	pages = {1740--1745},
}

@article{dehnert_persuasion_2022,
	title = {Persuasion in the {Age} of {Artificial} {Intelligence} ({AI}): {Theories} and {Complications} of {AI}-{Based} {Persuasion}},
	volume = {48},
	issn = {1468-2958},
	shorttitle = {Persuasion in the {Age} of {Artificial} {Intelligence} ({AI})},
	url = {https://doi.org/10.1093/hcr/hqac006},
	doi = {10.1093/hcr/hqac006},
	abstract = {Artificial intelligence (AI) has profound implications for both communication and persuasion. We consider how AI complicates and promotes rethinking of persuasion theory and research. We define AI-based persuasion as a symbolic process in which a communicative-AI entity generates, augments, or modifies a message—designed to convince people to shape, reinforce, or change their responses—that is transmitted to human receivers. We review theoretical perspectives useful for studying AI-based persuasion—the Computers Are Social Actors (CASA) paradigm, the Modality, Agency, Interactivity, and Navigability (MAIN) model, and the heuristic-systematic model of persuasion—to explicate how differences in AI complicate persuasion in two ways. First, thin AI exhibits few (if any) machinic (i.e., AI) cues, social cues might be available, and communication is limited and indirect. Second, thick AI exhibits ample machinic and social cues, AI presence is obvious, and communication is direct and interactive. We suggest avenues for future research in each case.},
	number = {3},
	urldate = {2022-10-02},
	journal = {Human Communication Research},
	author = {Dehnert, Marco and Mongeau, Paul A},
	month = jul,
	year = {2022},
	pages = {386--403},
}

@article{dehnert_persuasion_2022-1,
	title = {Persuasion in the {Age} of {Artificial} {Intelligence} ({AI}): {Theories} and {Complications} of {AI}-{Based} {Persuasion}},
	volume = {48},
	issn = {1468-2958},
	shorttitle = {Persuasion in the {Age} of {Artificial} {Intelligence} ({AI})},
	url = {https://doi.org/10.1093/hcr/hqac006},
	doi = {10.1093/hcr/hqac006},
	abstract = {Artificial intelligence (AI) has profound implications for both communication and persuasion. We consider how AI complicates and promotes rethinking of persuasion theory and research. We define AI-based persuasion as a symbolic process in which a communicative-AI entity generates, augments, or modifies a message—designed to convince people to shape, reinforce, or change their responses—that is transmitted to human receivers. We review theoretical perspectives useful for studying AI-based persuasion—the Computers Are Social Actors (CASA) paradigm, the Modality, Agency, Interactivity, and Navigability (MAIN) model, and the heuristic-systematic model of persuasion—to explicate how differences in AI complicate persuasion in two ways. First, thin AI exhibits few (if any) machinic (i.e., AI) cues, social cues might be available, and communication is limited and indirect. Second, thick AI exhibits ample machinic and social cues, AI presence is obvious, and communication is direct and interactive. We suggest avenues for future research in each case.},
	number = {3},
	urldate = {2022-10-02},
	journal = {Human Communication Research},
	author = {Dehnert, Marco and Mongeau, Paul A},
	month = jul,
	year = {2022},
	pages = {386--403},
}

@inproceedings{ferreyra_persuasion_2020,
	title = {Persuasion {Meets} {AI}: {Ethical} {Considerations} for the {Design} of {Social} {Engineering} {Countermeasures}},
	shorttitle = {Persuasion {Meets} {AI}},
	url = {http://arxiv.org/abs/2009.12853},
	doi = {10.5220/0010142402040211},
	abstract = {Privacy in Social Network Sites (SNSs) like Facebook or Instagram is closely related to people's self-disclosure decisions and their ability to foresee the consequences of sharing personal information with large and diverse audiences. Nonetheless, online privacy decisions are often based on spurious risk judgements that make people liable to reveal sensitive data to untrusted recipients and become victims of social engineering attacks. Artificial Intelligence (AI) in combination with persuasive mechanisms like nudging is a promising approach for promoting preventative privacy behaviour among the users of SNSs. Nevertheless, combining behavioural interventions with high levels of personalization can be a potential threat to people's agency and autonomy even when applied to the design of social engineering countermeasures. This paper elaborates on the ethical challenges that nudging mechanisms can introduce to the development of AI-based countermeasures, particularly to those addressing unsafe self-disclosure practices in SNSs. Overall, it endorses the elaboration of personalized risk awareness solutions as i) an ethical approach to counteract social engineering, and ii) as an effective means for promoting reflective privacy decisions.},
	urldate = {2022-10-02},
	booktitle = {Proceedings of the 12th {International} {Joint} {Conference} on {Knowledge} {Discovery}, {Knowledge} {Engineering} and {Knowledge} {Management}},
	author = {Ferreyra, Nicolas E. Díaz and Aïmeur, Esma and Hage, Hicham and Heisel, Maritta and van Hoogstraten, Catherine García},
	year = {2020},
	note = {arXiv:2009.12853 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Social and Information Networks},
	pages = {204--211},
}

@misc{noauthor_persuasion_nodate,
	title = {Persuasion {Tools}: {AI} takeover without {AGI} or agency? - {LessWrong}},
	shorttitle = {Persuasion {Tools}},
	url = {https://www.lesswrong.com/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency},
	abstract = {I'm envisioning that in the future there will also be systems where you can input any conclusion that you want to argue (including moral conclusions) and the target audience, and the system will give…},
	language = {en},
	urldate = {2022-10-02},
}

@misc{noauthor_persuasive_nodate-1,
	title = {Persuasive {AI} could corrupt human behaviour, study suggests},
	url = {https://www.techerati.com/news-hub/persuasive-ai-could-corrupt-human-behaviour-study-suggests/},
	abstract = {Researchers demonstrate the corrupting influence of Natural Language Processing.},
	urldate = {2022-10-02},
	journal = {Techerati},
}

@article{chen_persuasive_2021,
	title = {Persuasive dialogue understanding: {The} baselines and negative results},
	volume = {431},
	issn = {0925-2312},
	shorttitle = {Persuasive dialogue understanding},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231220318336},
	doi = {10.1016/j.neucom.2020.11.040},
	abstract = {Persuasion aims at forming one’s opinion and action via a series of persuasive messages containing persuader’s strategies. Due to its potential application in persuasive dialogue systems, the task of persuasive strategy recognition has gained much attention lately. Previous methods on user intent recognition in dialogue systems adopt recurrent neural network (RNN) or convolutional neural network (CNN) to model context in conversational history, neglecting the tactic history and intra-speaker relation. In this paper, we demonstrate the limitations of a Transformer-based approach coupled with Conditional Random Field (CRF) for the task of persuasive strategy recognition. In this model, we leverage inter- and intra-speaker contextual semantic features, as well as label dependencies to improve the recognition. Despite extensive hyper-parameter optimizations, this architecture fails to outperform the baseline methods. We observe two negative results. Firstly, CRF cannot capture persuasive label dependencies, possibly as strategies in persuasive dialogues do not follow any strict grammar or rules as the cases in Named Entity Recognition (NER) or part-of-speech (POS) tagging. Secondly, the Transformer encoder trained from scratch is less capable of capturing sequential information in persuasive dialogues than Long Short-Term Memory (LSTM). We attribute this to the reason that the vanilla Transformer encoder does not efficiently consider relative position information of sequence elements.},
	language = {en},
	urldate = {2022-10-02},
	journal = {Neurocomputing},
	author = {Chen, Hui and Ghosal, Deepanway and Majumder, Navonil and Hussain, Amir and Poria, Soujanya},
	month = mar,
	year = {2021},
	keywords = {Conditional random Field, Persuasive dialogue systems, Persuasive strategy recognition, Transformer-based neural networks},
	pages = {47--56},
}

@article{oinas-kukkonen_persuasive_2009,
	title = {Persuasive {Systems} {Design}: {Key} {Issues}, {Process} {Model}, and {System} {Features}},
	volume = {24},
	shorttitle = {Persuasive {Systems} {Design}},
	doi = {10.17705/1CAIS.02428},
	abstract = {A growing number of information technology systems and services are being developed to change users’ attitudes or behavior or both. Despite the fact that attitudinal theories from social psychology have been quite extensively applied to the study of user intentions and behavior, these theories have been developed for predicting user acceptance of the information technology rather than for providing systematic analysis and design methods for developing persuasive software solutions. This article is conceptual and theory-creating by its nature, suggesting a framework for Persuasive Systems Design (PSD). It discusses the process of designing and evaluating persuasive systems and describes what kind of content and software functionality may be found in the final product. It also highlights seven underlying postulates behind persuasive systems and ways to analyze the persuasion context (the intent, the event, and the strategy). The article further lists 28 design principles for persuasive system content and functionality, describing example software requirements and implementations. Some of the design principles are novel. Moreover, a new categorization of these principles is proposed, consisting of the primary task, dialogue, system credibility, and social support categories.},
	journal = {Communications of the Association for Information Systems},
	author = {Oinas-Kukkonen, Harri and Harjumaa, Marja},
	month = mar,
	year = {2009},
}

@misc{wang_persuasion_2020,
	title = {Persuasion for {Good}: {Towards} a {Personalized} {Persuasive} {Dialogue} {System} for {Social} {Good}},
	shorttitle = {Persuasion for {Good}},
	url = {http://arxiv.org/abs/1906.06725},
	doi = {10.48550/arXiv.1906.06725},
	abstract = {Developing intelligent persuasive conversational agents to change people's opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems. To do so, the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations. We designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. We collected a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset. Based on the annotation, we built a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus. Furthermore, to develop an understanding of personalized persuasion processes, we analyzed the relationships between individuals' demographic and psychological backgrounds including personality, morality, value systems, and their willingness for donation. Then, we analyzed which types of persuasion strategies led to a greater amount of donation depending on the individuals' personal backgrounds. This work lays the ground for developing a personalized persuasive dialogue system.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Wang, Xuewei and Shi, Weiyan and Kim, Richard and Oh, Yoojung and Yang, Sijia and Zhang, Jingwen and Yu, Zhou},
	month = jan,
	year = {2020},
	note = {arXiv:1906.06725 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@inproceedings{fogg_persuasive_1998,
	address = {USA},
	series = {{CHI} '98},
	title = {Persuasive computers: perspectives and research directions},
	isbn = {978-0-201-30987-4},
	shorttitle = {Persuasive computers},
	url = {https://doi.org/10.1145/274644.274677},
	doi = {10.1145/274644.274677},
	urldate = {2022-10-02},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM Press/Addison-Wesley Publishing Co.},
	author = {Fogg, BJ},
	month = jan,
	year = {1998},
	keywords = {captology, computers as persuasive technologies, computers as social actors, design methods, ethics, media, persuasion},
	pages = {225--232},
}

@misc{dutt_resper_2021,
	title = {{RESPER}: {Computationally} {Modelling} {Resisting} {Strategies} in {Persuasive} {Conversations}},
	shorttitle = {{RESPER}},
	url = {http://arxiv.org/abs/2101.10545},
	doi = {10.48550/arXiv.2101.10545},
	abstract = {Modelling persuasion strategies as predictors of task outcome has several real-world applications and has received considerable attention from the computational linguistics community. However, previous research has failed to account for the resisting strategies employed by an individual to foil such persuasion attempts. Grounded in prior literature in cognitive and social psychology, we propose a generalised framework for identifying resisting strategies in persuasive conversations. We instantiate our framework on two distinct datasets comprising persuasion and negotiation conversations. We also leverage a hierarchical sequence-labelling neural architecture to infer the aforementioned resisting strategies automatically. Our experiments reveal the asymmetry of power roles in non-collaborative goal-directed conversations and the benefits accrued from incorporating resisting strategies on the final conversation outcome. We also investigate the role of different resisting strategies on the conversation outcome and glean insights that corroborate with past findings. We also make the code and the dataset of this work publicly available at https://github.com/americast/resper.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Dutt, Ritam and Sinha, Sayan and Joshi, Rishabh and Chakraborty, Surya Shekhar and Riggs, Meredith and Yan, Xinru and Bao, Haogang and Rosé, Carolyn Penstein},
	month = jan,
	year = {2021},
	note = {arXiv:2101.10545 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_customer_nodate,
	title = {‎{The} {Customer} {Experience} {Podcast}: 50. {The} {Ethics} of {AI} - {Customer} {Persuasion} vs {Customer} {Coercion} w/ {William} {Ammerman} on {Apple} {Podcasts}},
	shorttitle = {‎{The} {Customer} {Experience} {Podcast}},
	url = {https://podcasts.apple.com/us/podcast/50-the-ethics-of-ai-customer-persuasion-vs/id1453581989?i=1000459175927},
	abstract = {‎Show The Customer Experience Podcast, Ep 50. The Ethics of AI - Customer Persuasion vs Customer Coercion w/ William Ammerman - Dec 10, 2019},
	language = {en-US},
	urldate = {2022-10-02},
	journal = {Apple Podcasts},
}

@misc{noauthor_dark_nodate,
	title = {The dark side of artificial intelligence: manipulation of human behaviour},
	shorttitle = {The dark side of artificial intelligence},
	url = {https://www.bruegel.org/blog-post/dark-side-artificial-intelligence-manipulation-human-behaviour},
	abstract = {Transparency over systems and algorithms, rules and public awareness are needed to address potential danger of manipulation by artificial intelligence},
	language = {en},
	urldate = {2022-10-02},
	journal = {Bruegel {\textbar} The Brussels-based economic think tank},
}

@article{bontridder_role_2021,
	title = {The role of artificial intelligence in disinformation},
	volume = {3},
	issn = {2632-3249},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/role-of-artificial-intelligence-in-disinformation/7C4BF6CA35184F149143DE968FC4C3B6},
	doi = {10.1017/dap.2021.20},
	abstract = {Artificial intelligence (AI) systems are playing an overarching role in the disinformation phenomenon our world is currently facing. Such systems boost the problem not only by increasing opportunities to create realistic AI-generated fake content, but also, and essentially, by facilitating the dissemination of disinformation to a targeted audience and at scale by malicious stakeholders. This situation entails multiple ethical and human rights concerns, in particular regarding human dignity, autonomy, democracy, and peace. In reaction, other AI systems are developed to detect and moderate disinformation online. Such systems do not escape from ethical and human rights concerns either, especially regarding freedom of expression and information. Having originally started with ascending co-regulation, the European Union (EU) is now heading toward descending co-regulation of the phenomenon. In particular, the Digital Services Act proposal provides for transparency obligations and external audit for very large online platforms’ recommender systems and content moderation. While with this proposal, the Commission focusses on the regulation of content considered as problematic, the EU Parliament and the EU Council call for enhancing access to trustworthy content. In light of our study, we stress that the disinformation problem is mainly caused by the business model of the web that is based on advertising revenues, and that adapting this model would reduce the problem considerably. We also observe that while AI systems are inappropriate to moderate disinformation content online, and even to detect such content, they may be more appropriate to counter the manipulation of the digital ecosystem.},
	language = {en},
	urldate = {2022-10-02},
	journal = {Data \& Policy},
	author = {Bontridder, Noémi and Poullet, Yves},
	year = {2021},
	note = {Publisher: Cambridge University Press},
	keywords = {European Union, artificial intelligence, content regulation, digital ecosystem, disinformation, information and communication technologies, online platforms},
	pages = {e32},
}

@misc{noauthor_risks_nodate,
	title = {Risks from {AI} persuasion - {LessWrong}},
	url = {https://www.lesswrong.com/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion},
	abstract = {A case for why persuasive AI might pose risks somewhat distinct from the normal power-seeking alignment failure scenarios.  …},
	language = {en},
	urldate = {2022-10-02},
}

@misc{noauthor_existential_2022,
	title = {The {Existential} {Threat} of {AI}-{Enhanced} {Disinformation} {Operations}},
	url = {https://www.justsecurity.org/82246/the-existential-threat-of-ai-enhanced-disinformation-operations/},
	abstract = {As AI capabilities improve, they will become increasingly effective tools for manipulating and fooling humans - with political, social, and epistemological consequences.},
	language = {en-US},
	urldate = {2022-10-02},
	journal = {Just Security},
	month = jul,
	year = {2022},
}

@article{hunter_towards_2018,
	title = {Towards a framework for computational persuasion with applications in behaviour change},
	volume = {9},
	issn = {1946-2166},
	url = {https://content.iospress.com/articles/argument-and-computation/aac032},
	doi = {10.3233/AAC-170032},
	abstract = {Persuasion is an activity that involves one party trying to induce another party to believe something or to do something. It is an important and multifaceted human facility. Obviously, sales and marketing is heavily dependent on persuasion. But many},
	language = {en},
	number = {1},
	urldate = {2022-10-02},
	journal = {Argument \& Computation},
	author = {Hunter, Anthony},
	month = jan,
	year = {2018},
	note = {Publisher: IOS Press},
	pages = {15--40},
}
