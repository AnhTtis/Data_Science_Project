\section{Discussion}\label{discussion}
\begin{figure}[t]
	\includegraphics[width=\textwidth]{fig/fig_4.jpg}
	\caption{Comparison results of image translation by CycleGAN and QS-Attn.}
	\label{fig4}
\end{figure}

Fig.~\ref{fig4} shows the results of the two separate image translation models utilized in the multi-view image translation framework. For comparison, we randomly picked two ce$\text{T}_1$ images (A\&E), their corresponding translated hr$\text{T}_2$ images (B-C\&F-G), and two unpaired real hr$\text{T}_2$ images (D\&H). We can see that QS-Attn (C) well captured the structure of cochleas with less distortion or blurring compared to CycleGAN (B). Meanwhile, some images translated through QS-Attn (G) have too high intensities for VS, whereas those by CycleGAN (F) have similar intensity and textures to VS in the real hr$\text{T}_2$  image (H). As shown in Fig.~\ref{fig4}, the two constraint models have different strengths. Therefore, in the proposed method, the segmentation model can learn both structures and textures of VS and cochleas through our multi-view image translation framework. It allows the segmentation model to consider various perspectives of VS and cochleas and helps improve the performance of the segmentation model.