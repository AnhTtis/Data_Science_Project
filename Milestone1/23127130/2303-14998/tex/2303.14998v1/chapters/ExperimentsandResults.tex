\section{Experiments and Results}

\subsection{Dataset and preprocessing}
We used the CrossMoDA dataset~\footnote{https://crossmoda-challenge.ml/}~\cite{dorent2022crossmoda} for training, validation. The CrossMoDA dataset consists of data from two different institutions: London and Tilburg. The London data consists of 105 ce$\text{T}_1$ scans and 105 hr$\text{T}_2$ scans. The ce$\text{T}_1$ scans were acquired with the in-plane resolution of 0.4×0.4mm, in-plane matrix of 512×512, and slice thickness of 1.0 to 1.5 mm with an MPRAGE sequence (TR=1900 ms, TE=2.97 ms, TI=1100 ms). Meanwhile, hr$\text{T}_2$ scans were acquired with the in-plane resolution of 0.5×0.5mm, in-plane matrix of 384×384 or 448×448, and slice thickness of 1.0 to 1.5 mm with a 3D CISS or FIESTA sequence (TR=9.4 ms, TE=4.23ms). For the Tilburg data set, ce$\text{T}_1$ scans and hr$\text{T}_2$ scans consist of 105 subjects each. The ce$\text{T}_1$ scans were acquired with the in-plane resolution of 0.8×0.8mm, in-plane matrix of 256×256, and slice thickness of 1.5 mm with a 3D-FFE sequence (TR=25 ms, TE=1.82 ms). The hr$\text{T}_2$ scans were acquired with the in-plane resolution of 0.4×0.4mm, in-plane matrix of 512×512, and slice thickness of 1.0 mm with a 3D-TSE sequence (TR=2700 ms, TE=160 ms, ETL=50)~\cite{dorent2022crossmoda}. The training dataset of the $\text{CrossMoDA2022 Challenge  }^1$ contains a total of 210 ce$\text{T}_1$ scans with annotation labels and 210 hr$\text{T}_2$ scans without annotation labels. In addition, they provide 64 scans of hr$\text{T}_2$ images for validation.

Since the voxel spaces vary across scans, all the images were resampled to [0.41, 0.41, 1.5] voxel sizes. For image translation, the 3D MRI images were sliced into a series of 2D images along the axial plane and the images were center-cropped and resized to 256 × 256. After performing image translation, the translated hr$\text{T}_2$ images were merged into 3D MR imaging and fed into the segmentation model.

\subsection{Implementation details} 
We implement CycleGAN~\cite{zhu2017unpaired}, QS-Attn~\cite{hu2022qs}, and nnUNet~\cite{isensee2021nnu}, following their default parameter settings. We also apply a global attention in QS-Attn~\cite{hu2022qs}, and ensemble selection in nnUNet~\cite{isensee2021nnu} for the final prediction. All the implementations are powered by RTX 3090 24GB GPUs. The training of CycleGAN, QS-Attn, and nnUNet is performed with PyTorch 1.8.0, 1.7.1, and 1.10.2, respectively.

\subsection{Results}
Table~\ref{tab5} and Fig.~\ref{fig2} show the VS and cochlea segmentation results with different image translation methods. The proposed multi-view image translation framework with CycleGAN~\cite{zhu2017unpaired} and QS-Attn~\cite{hu2022qs} shows better performance compared to other methods using each model alone. Moreover, we greatly improved the performance of the segmentation model with self-training. As a result, our proposed method obtained a great achievement with a mean dice score of 0.8504±0.0466 in the validation period. 






\begin{table}[t]
\centering
\caption{Segmentation results with dice and ASSD scores (ST: self-training).}
\label{tab5}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|ccc|cc}
\hline
Translation                                             & \multicolumn{3}{c|}{Dice score}                                                                                                                                                                                                   & \multicolumn{2}{c}{ASSD}                                                                                                                   \\ \cline{2-6} 
model                                                   & \multicolumn{1}{c|}{VS}                                                        & \multicolumn{1}{c|}{Cochlea}                                                         & Mean                                                         & \multicolumn{1}{c|}{VS}                                                         & Cochlea                                                         \\ \hline
\begin{tabular}[c]{@{}c@{}}CycleGAN\\ (\emph{w/o.} ST) \end{tabular}                      & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}0.7798\\ ($\pm{0.1901}$)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}0.8066\\ ($\pm{0.0323}$)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}0.7932\\ ($\pm{0.0972}$)\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}0.8750\\ ($\pm{0.9222}$)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}0.2422\\ ($\pm{0.1608}$)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}QS-Attn\\ (\emph{w/o.} ST) \end{tabular}                       & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}0.7779\\ ($\pm{0.1825}$)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}0.8158\\ ($\pm{0.0287}$)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}0.7968\\ ($\pm{0.0929}$)\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}0.6667\\ ($\pm{0.3891}$)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}0.2365\\ ($\pm{0.1573}$)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}Proposed\\ (\emph{w/o.} ST) \end{tabular}                      & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}0.8043\\ ($\pm{0.1656}$)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}0.8158\\ ($\pm{0.0289}$)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}0.8101\\ ($\pm{0.0863}$)\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}0.5742\\ ($\pm{0.2461}$)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}0.2387\\ ($\pm{0.1581}$)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}Proposed\\ (\emph{w.} ST) \end{tabular}                      & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{0.8520}\\ \textbf{($\pm{0.0889}$)}\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{0.8488}\\ \textbf{($\pm{0.0235}$)}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\textbf{0.8504}\\ \textbf{($\pm{0.0466}$)} \end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{0.4748}\\ \textbf{($\pm{0.2072}$)} \end{tabular}} & \begin{tabular}[c]{@{}c@{}}\textbf{0.1992}\\ \textbf{($\pm{0.1524}$)}\end{tabular} \\ \hline
\end{tabular}%
}
\end{table}



\begin{figure}[hbt!]
	\includegraphics[width=\textwidth]{fig/fig_2.jpg}
	\caption{Qualitative comparison of segmentation results for validation set. We visualize the segmentation results of VS (red) and cochlea (green) (ST: Self-training).}
	\label{fig2}
\end{figure}



\begin{figure}[hbt!]
	\includegraphics[width=\textwidth]{fig/fig_3.jpg}
	\caption{Performance comparison of VS and cochlea segmentation models (ST: Self-training).}
	\label{fig3}
\end{figure}

We conducted paired \emph{t}-test among CycleGAN~\cite{zhu2017unpaired}, QS-Attn~\cite{hu2022qs}, and our proposed method (\emph{w/o.} self-training, ST) to compare the segmentation performance, and the results are plotted in Fig.~\ref{fig3}. CycleGAN, QS-Attn, and our proposed method (\emph{w/o.} ST) show statistical significance with \emph{p} $<$ 0.05 for the dice score of VS and mean values. In addition, our proposed method (\emph{w/o.} ST) is statistically better with \emph{p} $<$ 0.0001 than CycleGAN on the dice score of cochleas. Through this statistical comparison, we proved that our proposed framework achieved better performance compared to other methods that use either of the two models alone.







