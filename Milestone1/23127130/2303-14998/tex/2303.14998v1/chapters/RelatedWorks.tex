\section{Related Work}

Cross-modality unsupervised domain adaptation has drawn a lot of attention in the CrossMoDA challenge~\cite{dorent2022crossmoda}. The goal of this challenge is to construct a VS and cochlea segmentation model on hr$\text{T}_2$ images with unpaired annotated ce$\text{T}_1$ and non-annotated hr$\text{T}_2$ scans. Recent studies~\cite{shin2022cosmos,dong2021unsupervised,choi2021using} first translated the source ce$\text{T}_1$ images to the target hr$\text{T}_2$ images, and then trained their segmentation models with the translated hr$\text{T}_2$ (i.e., pseudo-hr$\text{T}_2$) images. More specifically, Shin et al.~\cite{shin2022cosmos} translated the ce$\text{T}_1$ images to the hr$\text{T}_2$ images by adding an additional decoder to CycleGAN to preserve the structures of  VS and cochleas. Dong et al.~\cite{dong2021unsupervised} conducted image translation using NiceGAN~\cite{chen2020reusing}, which is based on CycleGAN~\cite{zhu2017unpaired}, and Choi et al.~\cite{choi2021using} obtained pseudo-hr$\text{T}_2$ images using CUT~\cite{park2020contrastive}. Of note, they all obtained pseudo-hr$\text{T}_2$ images by taking only one constraint model. Besides, Choi et al.~\cite{choi2021using} performed post-processing to obtain the images with low intensity, similar to the VS in real hr$\text{T}_2$ scans.