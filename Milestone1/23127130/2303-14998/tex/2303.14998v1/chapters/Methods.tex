
\section{Proposed Method}
\subsection{Overview}
Fig.~\ref{fig1} shows an overview of our proposed framework, which consists of three parts; (1) multi-view image translation, (2) segmentation model training, and (3) self-training. Specifically, we first generate the pseudo-hr$\text{T}_2$ images with various characteristics through multi-view image translation. After that, we train the segmentation model using the multi-view pseudo-hr$\text{T}_2$ images and the labels of the ce$\text{T}_1$ images. In the self-training, the trained segmentation model first performs pseudo-labeling of real hr$\text{T}_2$  images, and then is further trained by including the pseudo-labeled real hr$\text{T}_2$  images in the next training phase.

\begin{figure}[t]
	\includegraphics[width=\textwidth]{fig/fig_1.jpg}
	\caption{The overview of our proposed framework.}
	\label{fig1}
\end{figure}

\subsection{Multi-view image translation}
We first translate ce$\text{T}_1$ images into multi-view pseudo-hr$\text{T}_2$ images by adopting CycleGAN~\cite{zhu2017unpaired} and QS-Attn~\cite{hu2022qs} in parallel.

\subsubsection{CycleGAN.} CycleGAN uses cycle-consistency loss to translate the source domain ce$\text{T}_1$ images into the target domain hr$\text{T}_2$ images. Cycle-consistency loss described in Eq.~\ref{eq7} encourages $F(G({x}_s ))$ to be equal to ${x}_s$ and $G(F({x}_t ))$ to be equal to ${x}_t$ in pixel-level when given the $G:{X}_s\to{X}_t$  and $F:{X}_t\to{X}_s$ generators~\cite{zhu2017unpaired}.
\begin{equation}
    {L}_{cycle}=\left\|F(G({x}_s)) - {x}_s\right\| + \left\|G(F({x}_t )) - {x}_t\right\|\label{eq7}
\end{equation}

\subsubsection{QS-Attn.} QS-Attn is an unpaired image translation model that is improved from CUT~\cite{park2020contrastive}. CUT preserves the structural information by constraining the patches from the same location on the source and the translated images to be close, compared to the different locations. CUT maximizes the mutual information between the source and translated images through the Eq.~\ref{eq8} ~\cite{park2020contrastive}, 
\begin{equation}
    {L}_{con}=-\log\Biggl[\cfrac{\exp(q\cdot{k}^{+}/\tau)}{\exp(q\cdot{k}^{+}/\tau)+\sum\nolimits_{i=1}^{N-1}\exp(q\cdot{k}^{-}/\tau)}\Biggr]\label{eq8}
\end{equation}

\noindent{where $q$ is the anchor feature from the translated image and ${k}^+$ is a single positive at the same location in the source image and ${k}^-$ are $(N-1)$ negatives at the other locations, and $\tau$ is a temperature~\cite{hu2022qs}.}

However, CUT~\cite{park2020contrastive} calculates the contrastive loss between the randomly selected  patches, which could have less domain-relevant information. QS-Attn addresses this limitation by adopting the QS-Attn module, which can select domain-relevant patches. The QS-Attn module constructs the attention matrix ${A}_g$ using the features in the source images and then obtains the entropy ${H}_g$ by following Eq.~\ref{eq9} ~\cite{hu2022qs}.

\begin{equation}
    {H}_g(i)=-\sum_{j=1}^{HW}{A}_g(i,j)\log{{A}_g(i,j)}\label{eq9}
\end{equation}

\noindent{Of note, the smaller entropy ${H}_g$ means the more important feature. Thus, ${A}_g$ is sorted in ascending order according to entropy ${H}_g$ to select domain-relevant patches~\cite{hu2022qs}. By calculating the contrastive loss using the selected domain-relevant patches, the structures of the source domain better preserve, and more realistic images are generated compared to CUT~\cite{park2020contrastive}.}

We empirically found that CycleGAN with pixel-level cycle-consistency loss allows the model to better reflect the intensity and the texture of the VS and cochleas in the target images, while QS-Attn takes advantage of preserving the structure of them more clearly via patch-level contrastive loss (refer to Section~\ref{discussion} for more details). By using them together, our multi-view image translation can augment pseudo-hr$\text{T}_2$ images from different perspectives, and it can help improve the performance of the following segmentation model.

\subsection{Segmentation and Self-training}
Motivated by the previous works~\cite{shin2022cosmos,dong2021unsupervised,choi2021using}, we also utilize nnUNet~\cite{isensee2021nnu} and self-training procedure~\cite{xie2020self} to construct the segmentation model. nnUNet is a powerful segmentation framework that automatically performs pre-processing, training, and post-processing with heuristic rules~\cite{isensee2021nnu}. Self-training is carried out to reduce the distribution gap between real hr$\text{T}_2$ and translated hr$\text{T}_2$ images and to improve the robustness of the segmentation model for unseen real hr$\text{T}_2$ scans. The segmentation and self-training procedure consists of four steps; (1) training the segmentation model using the translated hr$\text{T}_2$ scans with labels of the ce$\text{T}_1$ scans. (2) Generating pseudo labels of unlabeled real hr$\text{T}_2$ scans by using the trained segmentation model. (3) Retraining the segmentation model using both the translated hr$\text{T}_2$ scans with labels of the ce$\text{T}_1$ scans and the real hr$\text{T}_2$ scans with pseudo labels. 4) Repeating Steps 2-3 to achieve further performance improvement.