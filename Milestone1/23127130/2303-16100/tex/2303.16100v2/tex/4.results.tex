\section{Hardware optimizations for edge deployment}
%\subsection{Model Performance Evaluation}

% \subsection{Critical Pruning Threshold Exploration}
%\subsection{Cumulative Sparsity Evaluation}

%\subsection{Variable Adapter Size Evaluation}

\subsection{Quantization}

While floating point values are still the most common data representation for DNN training, reduced-precision numerical formats are a good compromise for targeting efficient inference~\cite{hubaraBinarized2016,zhuTrained2017,reagenMinerva2016}, since quantized operands can significantly decrease data storage and movement costs, as well as reduce the complexity of the hardware used for implementing MAC processing units~\cite{carmichaelPerformanceEfficiency2019}. 
%By carefully balancing this trade-off, DNN models can be deployed more efficiently on energy- and storage-limited hardware without losing data accuracy. 
The vanilla ALBERT model has been demonstrated to benefit from quantization down to 16-bit floating-point without negatively impacting its inference performance~\cite{tambeEdgeBERT2021a}. 
For our adapter-ALBERT model, we hypothesize that, given the adapter module's proven ability to compensate for accuracy loss caused by pruning, it may also exhibit similar trends for quantization.

We have designed experiments with quantization configurations in 16-bit and 8-bit fixed-point representations using the CSP-both backbone. Using the conventional notation, $Q_{i,f}$, to represent the quantization scheme using $i$ bits for integer and sign, and $f$ fractional bits, we focus on $Q_{3, 13}$ and $Q_{3, 5}$.  
%Introducing a similar notation as the one used to denote the quantization scheme, we use an adapter size of $A_{64,64}$, where the two numbers in the suffix represent the size of the first and second adapter module in the transformer layer. 
To ensure data consistency, the adapter modules are quantized using the same settings as the backbone. The goal for these experiments is to show to what extent the ability of adapter modules in improving or maintaining the inference accuracy is limited by using lower precision quantization. Moreover, we want to verify if adapter modules can learn reduced data information and recover the loss via retraining of the task-specific parameters. 


\begin{figure}[t]
    \centering
    \includegraphics[width=8.5cm]{fig/qt.pdf}
    \caption{Quantization results on the CSP-all backbone. Although the $Q_{3,13}$ quantization example provides competitive results across all the considered datasets, reducing the number of bits per operand to $Q_{3,5}$ shows a drastic accuracy reduction for some of the QNLI and SST-2 tasks.}
    \label{fig:qt}
\end{figure}

%During the experiments, we have observed unacceptable accuracy degradation when applying quantization methods directly onto the model before inference. 
%As both the model's backbone layers and non-fixed layers are quantified, we speculate that as the information stored in the adapter modules' limited parameter matrices are full-precision data, the adapter modules are very sensitive to any data accuracy deduction caused by quantization if no extra actions are taken.
%We hence re-train the adapter modules while quantifying the entire model to ensure the adapter modules could receive and parameterize information from quantified matrices from the backbone layers.
As shown in Figure~\ref{fig:qt}, the $Q_{3,13}$ configuration provides competitive accuracy results when compared with the single-precision floating point (FP32) baseline. 
On the other hand, the results for $Q_{3, 5}$ quantization are much less consistent, suggesting that the optimal quantization scheme will dependent on the subset of tasks used by an application. 

\input{table/memory-footprints}

\subsection{Bitmask Encoding}
Leveraging pruning to improve storage density requires an additional step in how the sparse matrices are mapped into the storage system. Several sparse encoding techniques have been proposed in the past and previous work has highlighted how critical it is to guarantee the robustness of these data structures against faults~\cite{pentecostMaxNVM2019}.  
Among the existing techniques, bitmask encoding is a lightweight approach that can be implemented with minimal encoding and decoding hardware overhead. The non-zero values from the sparse matrix are saved in an ordered array and their location is mapped to a binary matrix. At this point that we apply pruning exclusively to the backbone layers. For this reason, it would be advantageous to store the sparse layers in RRAM. While easy to implement, this solution is susceptible to large errors if any of the bits in the bitmask is flipped. To address this issue and preserve the advantage of the density of RRAMs, we split the bitmask and non-zero value data structures to SLC and MLC RRAM arrays respectively. 

\subsection{Accelerator architecture modeling}
To verify the expected improvements introduced by the adapter-ALBERT model optimizations and the associated on-chip memory architecture, we extrapolate the overall system area, energy, and latency by combining results from NVMExplorer~\cite{pentecostNVMExplorer2021} with a performance model tailored around the EdgeBERT accelerator specifications. In order to minimize the on-chip area overhead and be able to deploy our solution onto a mobile SoC, we select a combination of memory macros with different capacities so that the overall memory size is at the closest value exceeding the application requirements. 
When multiple combinations of memory macros are possible, we evaluate each proposal according to their latency and energy consumption. Since macros of different sizes will display different bandwidths, we compute the overall bandwidth as the weighted average of the bandwidth based on the macro's individual capacity. Off-chip memory access to DRAM are modeled using a similar approach to the one introduced in TETRIS~\cite{gaoTETRIS2017}. In our model, we ignore the energy contribution for the computational units, noting that, with the exception of the adapter computations, the two models will perform the same operations.


The baseline for our evaluations is based on the same accelerator and memory architecture, but for the latter, the capacity requirements consider a different model partition in which only the embedding parameters are stored in RRAM, while the rest of the data uses SRAM. As with adapter-ALBERT, the vanilla ALBERT case uses bitmask encoding for the sparse embeddings. The corresponding memory footprint for these two design options under different quantization configurations are shown in Table \ref{tab:mem-req}.
The model parameters partition of adapter-ALBERT introduces higher storage requirements for SLC and MLC RRAM compared to vanilla ALBERT. This is due to the fact that a larger number of parameters are shared across tasks and therefore are kept in the non-volatile portion of the memory. As a consequence, even though we introduce a parameter overhead associated with adapters, the SRAM storage is reduced by a larger factor. The overall effect is that we can take advantage of the denser RRAM storage for a larger portion of the model, leading to a smaller memory footprint.


The workload scenario we consider is that of a MTI application. The on-chip memory stores only the parameters required to process the current task, and the system performs a parameter update from DRAM whenever a new task needs to be executed. Therefore, we provision the on-chip memory footprint based on the worst-case scenario, \textit{i.e.}, the largest set of parameters for any of the given tasks. Note that this corresponds to the dataset with the largest number of classification labels for the vanilla ALBERT case, and in addition,  
%must consider the largest adapter size from applying VASE to the adapter-ALBERT case.
must consider the largest adapter size adopted by any of the target tasks when using VASE in the adapter-ALBERT case.

%By calculating the model's total number of parameters, removing parameters that are repeatedly reused, and considering sparsity and data representation formats, we obtain the actual memory capacity requirements for RRAMs and SRAM. We then use NVMExplorer to estimate their area, read and write bandwidth, energy, and latency. 

Under this strategy, we compare adapter-ALBERT and vanilla ALBERT's area, energy per inference, and latency per inference under three quantization configurations~\ref{fig:PPA}.
The results are normalized to the FP32 vanilla ALBERT design.
We can observe that for all configurations, the adapter-ALBERT provides all-round advantages against vanilla ALBERT.
% For 8-bit quantization configuration where the two models have the least gaps, the adapter-ALBERT requires 64.9\% area, 1.52\% energy per inference, and 6.37\% latency compared to the vanilla ALBERT model.
% For 32-bit and 16-bit configurations, the advantage of adapter-ALBERT with the memory architecture is more significant.
Compared to the FP32 implementation under a 3-task MTI scenario, adapter-ALBERT enjoys 2.04$\times$, 146.78$\times$, and 2.46$\times$ reductions in area, energy per inference, and latency per inference. The advantage is even more significant when it comes to 16-bit and 8-bit quantified comparisons. For instance, using the $Q_{3, 5}$ configuration, leads to 5.9$\times$, 682$\times$, and 62$\times$ improvements in area, energy and latency.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.48\textwidth]{fig/ppa.pdf}
    \caption{Area, energy/inference, and latency comparison between adapter-ALBERT and vanilla ALBERT using FP32, $Q_{3,13}$, and $Q_{3, 5}$ data types. The results are normalized to the FP32 vanilla ALBERT design.}
    \label{fig:PPA}
\end{figure}
