\section{Related Work}
% \fixme{More papers to be added later}
% The emergence of BERT model \cite{Devlin18} based on Transformer \cite{Vaswani17} architecture has become a milestone in the NLP domain. Many variants of BERT model have explored and expanded the performance envelope of NLP models in multiple dimensions. ALBERT \cite{Lan19}, as a slim BERT variant, could achieve comparable inference accuracy as its predecessor with only 1/10 numbers of parameters. The concept of transfer learning proposed by \cite{Weiss16} has significantly reduced the training cost of large-scale CV models. Among different transfer learning techniques, the residual adapter modules proposed by \cite{Rebuffi17, Rebuffi18} have drawn researchers' attention by showing the potential of enabling heterogeneous hardware storage platforms of traditional SRAM and eNVM devices for multi-task learning. The adapter-BERT model by \cite{Houlsby19} has shown the feasibility that BERT-variant models for NLP tasks could also enjoy the benefits of the adapter modules. Other works, including \cite{bertnpals} have shown other possible paths of enabling efficient MTL capability in the NLP models.

% Data compression methods, including pruning, clustering, and quantization, have been adequate to reduce the storage requirements of DNN models while maintaining their accuracy. We have utilized magnitude pruning in our model, and \cite{Gordon20} has categorized the effects of magnitude pruning on the NLP models as a cross-check to our pruned model behaviors.
% % Here needs Quantization-related works.
% Many have proposed other DNN model compression methods. \cite{Liu17}, \cite{Wen16} have explored channel-level sparsity on CNN models.


% \textbf{Transfer Learning and Multi-Task Learning in Computer Vision -}
\textbf{Multi-Task Learning in Computer Vision -}
Transfer learning DNN models pre-trained on ImageNet~\cite{dengImageNet2009} has been a common approach for efficiently training new tasks in computer vision. However, preserving inference accuracy requires to fine-tune a large fraction of the model. Residual adapter modules~\cite{rebuffiLearning2017, rebuffiEfficient2018} have been introduced as alternative and more efficient way of achieving multi-task learning. The Squeeze-and-Excite blocks proposed by Hu et al.~\cite{huSqueezeandExcitation2019} presents a similar bottle-neck module as residual adapters for channel-wise feature extraction and adaptation. 

Alternative multi-task inference approaches generate multiple copies of the same model which are aggressively compressed to limit the overall model size. Examples of this approach include network slimming~\cite{liuLearning2017} and structured sparsity learning~\cite{wenLearning2016}. The latter approach learns the sparsity from a complex DNN model to accelerate DNN inference. TinyML-inspired projects such as SqueezeNet~\cite{iandolaSqueezeNet2016}, TinyTL~\cite{caiTinyTL2021}, and MCUNet~\cite{linMCUNet2020}, have sought ways to compress the model size to fit in microcontroller-based platforms.

\textbf{Natural Language Processing -}
The field of natural language processing has been rapidly advancing since the proposal of attention mechanism along with the Transformer model~\cite{vaswaniAttention2017}. Edge-cutting attention-based NLP models, including BERT~\cite{devlinBERT2019} and GPT~\cite{radfordImproving, brownLanguage2020}, have brought significant performance improvement over traditional CNN, RNN, and LSTM-based language models and challenges of local deployments due to massive computational and data movement costs from their uncontrollable sizes. Slimmer BERT variants, like ALBERT~\cite{lanALBERT2020}, TinyBERT~\cite{jiaoTinyBERT2020}, and RoBERTa~\cite{liuRoBERTa2019}, have introduced multiple structural and data compression optimizations to reduce their sizes while keeping acceptable performance. Additionally, Gordon et al.~\cite{gordonCompressing2020} have categorized the weight pruning effects on the BERT model, indicating the potential boundary of pruning our adapter-ALBERT model.

Transfer learning is also closely studied in the NLP domain. The adapter-BERT model proposed by Houlsby et al.~\cite{houlsbyParameterEfficient2019} shows feasibility of transplanting adapter modules from CV domain to NLP domain for efficient training compared to traditional fine-tuning methods. Stickland et al.~\cite{sticklandBERT2019} utilize project attention layers across their BERT-PALs model as an alternative task-adaption approach for a lower parameter overhead compared to adapter-BERT model (1.13$\times$ vs 1.3$\times$). However, this model, like many of the alternatives discussed in this section, requires to update all parameters which imposes a heavy toll on the memory bus.


\textbf{Embedded Non-Volatile Memories -}
To match the needs of data-intensive applications, new memory technologies have been proposed since the memristors was first hypotesized in 1971~\cite{chuaMemristorThe1971}. After the first demonstrated physical implementation in 2008~\cite{strukovMissing2008}, the resistive random access memory (RRAM) gradually became one of the most promising eNVM technologies due to its scalability~\cite{dengDesign2013}, storage density~\cite{dengRRAM2013}, competitive read performance~\cite{shyh-shyuansheuFastWrite2011}, and CMOS compatibility~\cite{tanachutiwatFPGA2011}. Collective studies and reviews of RRAM and other eNVM technologies are performed by Panda et al.~\cite{pandaCollective2018} and Chen et al.~\cite{chenReview2016}.

DNN storage architecture realizations utilizing RRAM have been closely studied. Donato et al. have presented an on-chip memory optimization utilizing SRAM and RRAM for CV multi-task inference \cite{donatoMEMTI2019}, which covers the shortcoming of RRAM's write costs by assigning frequently-updated parameters of an adapter-equipped ResNet model onto the SRAM portion of the design. 
