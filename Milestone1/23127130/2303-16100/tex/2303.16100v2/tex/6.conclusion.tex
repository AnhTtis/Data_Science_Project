\section{Conclusions}
As the deployment of large DNN NLP models on edge devices for  multi-task inference (MTI) workloads becomes more prevalent, there is a critical need to balance on-chip memory demands, accuracy, and reduced data movement costs. The adapter-ALBERT model offers a novel co-design methodology to optimize task adaptation on edge devices.

The proposed adapter-ALBERT model significantly reduces trainable parameters at the cost of minimal parameter overhead, maximizing data reuse and resulting in a more efficient storage plan suitable for MTI scenarios and heterogeneous memory architectures. The model's robustness to various data compression methods, including pruning and quantization, is demonstrated.

By employing a pruning strategy that combines sparsity in both embedding and transformer layers, we explore critical pruning thresholds and show that it is possible to maintain competitive accuracy on the GLUE benchmark with over 50\% cumulative sparsity. We analyze the effects of varying adapter sizes on critically pruned models and conclude that the adapter module can recover the accuracy loss caused by pruning.

Our solution is evaluated on a validated hardware accelerator model which integrates an embedded heterogeneous memory architecture. In its most aggressive optimization, our solution offers almost 6$\times$ area reduction, 682$\times$ energy per inference reduction, and 62$\times$ latency per inference speed-up.

In summary, our proposed adapter-ALBERT model offers a powerful approach for efficient NLP DNN with a low memory footprint, low power requirements, and high accuracy. The model's adaptability to pruning and quantization, and its ability to handle multiple tasks make it a suitable solution for on-chip multi-task inference for resource-constrained edge devices.