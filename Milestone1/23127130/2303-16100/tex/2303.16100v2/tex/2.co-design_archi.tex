\section{Model and Memory Architectures}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{fig/adapter-albert.pdf}
    \caption{The overview of the adapter-ALBERT model (a) and the HMA (b) architectures. The colors of the adapter-ALBERT model indicate the backbone layers (red) and non-fixed layers (blue). The colors of the HMA architecture indicate different roles of components: red and blue are HMA memory blocks and their colors match the parts of the adapter-ALBERT model; Purple DRAM block is off-chip memory; Yellow blocks belong to the EdgeBERT accelerator.}
    \label{fig:adapter-albert}
\end{figure*}

\subsection{The Adapter-ALBERT Model Architecture}
The block diagram of Figure \ref{fig:adapter-albert}(a) shows the structure of the adapter-ALBERT model. At a high level, the model is based on the same Embedding-Transformer-Classifier skeleton as the vanilla ALBERT model. The blue and red colors are used to differentiate between task-specific and backbone layers, respectively. The embedding layer, highlighted in red, can be fully shared across a variety of NLP tasks, while the classifier, highlighted in blue, is specifically trained on the task at hand by design. For traditional BERT-based language models, the transformer layers would need to be entirely fine-tuned to learn new tasks. However, the transformer layers in adapter-ALBERT are defined by a mixture of trainable and fixed layers. 
The transformer layer partition is shown in greater detail in the inset flow diagram in the middle of Figure \ref{fig:adapter-albert}(a). Going through the diagram from bottom to top, the multi-head attention layer and all feed-forward layers are fixed and use parameters inherited from a pre-trained model, while layer normalization parameters and the newly introduced adapters are considered task-specific layers. In particular, layer normalization parameters are made re-trainable to ensure correct normalization of current data, preventing unmatched data alignments to damage the model performance.
The introduction of adapter blocks, allows the model to learn new features at the cost of small parameter overhead.  
The two residual adapter modules have an identical structure as shown on the right end of the flow diagram.
An adapter module consists of an adapter-down layer that reduces the input feature size down to the adapter bottleneck size, an adapter-activation layer, and an adapter-up layer that increases the feature size back up to the output layer size before being added to the input feature values through a shortcut connection.
The input size of the adapter-down layer and output size of the adapter-up layer match the hidden feature size of 768 in the ALBERT model. The bottleneck size is defined in our model as a hyperparameter, and it plays an important role in determining the model inference accuracy. 
In particular, we will show that distinct adapter blocks can use different bottleneck sizes to prevent accuracy loss.

\subsection{The Heterogeneous Memory Architecture}
The block diagram of Figure \ref{fig:adapter-albert}(b) shows the architecture of the targeted edge system, with emphasis on the heterogeneous scratchpad memory. 
We use the same color coding to describe how task-specific and backbone parameters for the adapter-ALBERT model are stored on chip. 
The SRAM memory is used to store both activations and task-specific parameters. In addition, two separate RRAM memory blocks are used for storing the backbone parameters in sparse format using bitmask encoding. We perform bitmask encoding after pruning both the embedding and backbone transformer layers. The resulting sparse matrices are represented by a vector of non-zero values and a corresponding bitmask array to encode their location. 
Many non-volatile memories, including RRAM, are capable of storing multiple bits in a single memory cell. The multi-level cell (MLC) feature in RRAM is often used to increase storage density. However, MLC RRAM suffers from reduced read margins between adjacent resistance levels and therefore is subject to higher bit error rates. While DNN applications have a demonstrated resilience to fluctuations in the value of the model weights, even single bit-flips in the bitmask could have catastrophic effects on the model accuracy. Error correcting codes (ECCs) are a common approach to make fault-prone memories more robust, but they introduce additional complexity in terms of encoding and decoding data stored in memory, which would quash the benefits of implementing a simple sparse encoding scheme. Therefore, we limit the storage of non-zero values to MLC RRAM, while we adopt a more conservative single-level cell (SLC) RRAM array for storing the bitmask arrays. 

The memory architecture described above is designed to support the operation of a NLP hardware accelerator modeled after EdgeBERT~\cite{tambeEdgeBERT2021a}. The main computational units in the accelerator are depicted in the block diagram with the yellow blocks.
The processing unit contains bitmask encoders and decoders, multiply-and-accumulate (MAC) units, and activation units.
Following the original design for the accelerator, we consider a datapath with 256 MAC units operating at a clock frequency of 1 GHz.
The special function units, integrated within the heterogeneous scratchpad memory, are responsible for near-memory data computation, such as element-wise add and layer normalization. 
