\input{table/model-acc.tex}

\section{Model optimizations for efficient adaptation}
\subsection{Model Performance Evaluation}
We evaluate the accuracy performance of our adapter-ALBERT model against three alternatives from the same family, namely BERT \cite{devlinBERT2019}, adapter-BERT \cite{houlsbyParameterEfficient2019}, and ALBERT \cite{lanALBERT2020}, on the GLUE benchmark~\cite{wangGLUE2019}. As a first step, we perform a hyperparameter search across the different GLUE datasets. We begin by sweeping the learning rate selecting between 5E-4 and 5E-5, and consider training epochs from 5 to 10. Training and evaluation batch size are set both to 64. This initial set of experiments is performed in two phases: In the first phase, BERT and ALBERT are fine-tuned for each of the GLUE datasets; 
% In the second phase, we transfer the backbone layers from BERT and ALBERT to the corresponding adapter model and train only the task-specific layers.
In the second phase, we transfer the backbone layers from BERT and ALBERT to their corresponding adapter-enhanced model and train only the task-specific layers. As mentioned in the previous section, the adapter modules are designed to have their size independently adjusted as an hyperparameter. We show results for both the case in which the adapter size is fixed to 64 across all tasks, and the case in which the value of the adapter size is selected between 32 and 128. The results from this training experiments are summarized in Table~\ref{tab:model-acc}. 
and show that our adapter-ALBERT model is capable of maintaining competitive results to the other three models, and that varying the adapter size can compensate for the lost accuracy. In particular, the adapter-ALBERT model with an adapter size of 64 matches, or in some cases bests, the other three models on MNLI, MRPC, STS-B, QQP, QNLI, and RTE datasets. 
For the rest three datasets, adjusting the adapter size ranging from 32 to 128 helps boosting the performance close to the other models. 

In addition to evaluating accuracy, we also compare the size of the models in terms of number of parameters as it gives a indication of the relative memory footprint for each of these variants (Table \ref{tab:model-size}). The comparison of trainable parameters reflects the cost of re-training the model for new downstream tasks and the storage and movement costs for task-specific parameters under the MTI scenario. 
Besides showing a small parameter overhead compared to their traditional counterpart, the adapter models also have a much lower number of trainable parameters due to the partition between backbone and task-specific layers This distinction is the key feature that allows the model to perform multi-task inference more efficiently: In the event of the edge system changing the performed inference task, the traditional models would require an update on the entire set of trainable parameters while, for adapters, that number of parameters that need to be refreshed is kept to a few percent of the entire model. Note that, even in the case of adopting the largest adapters size, the parameter overhead would not exceed 4$\%$, or 400K parameters.






% \subsection{Critical Pruning Threshold Exploration}
\subsection{Model Compression}
After identifying the best set of hyperparameters to train the backbone and adapter layers, we focus on evaluating different compression techniques to make the model more suitable for running on edge devices. We evaluate pruning compression techniques that are ultimately combined to minimize the memory footprint for the adapter-ALBERT model.

\subsubsection{\textbf{Cumulative Sparsity Evaluation}}
Pruning is an effective method for compressing a DNN model and remove redundant parameters to reduce computational complexity and improve generalization capability \cite{liangPruning2021}. However, pruning usually causes accuracy degradation that can be recovered when combined with re-training. Previous work has shown that BERT can endure sparsity between 30\% and 40\% with no detrimental effects on the accuracy of either the pre-trained model or the transferred downstream tasks~\cite{gordonCompressing2020}. However, the new learning approach introduced by the adapter modules requires to re-assess the potential impact of pruning on the model's multi-task learning capabilities. Therefore, we conduct a sensitivity study to ascertain the highest sparsity level that can be achieved by adapter-ALBERT. In particular, we designed a series of experiments to identify the critical sparsity point (CSP) as the limit at which pruning causes the accuracy of the model to drop below the un-pruned baseline accuracy. Note that we apply pruning only to the backbone layers,~\textit{i.e.,} embeddings and fixed transformer layers. The task-specific layers represent only a small portion of the entire model and while pruning them would not affect the model size considerably, 
it would trigger dramatic accuracy degradation. 

%The pruning steps that lead to identifying the CSP are described in Algorithm \ref{alg:CSP}.

%\begin{algorithm}[b]
%    \caption{CSP Identification}
%    \label{alg:CSP}
%    \begin{algorithmic}
%        \STATE {set Diff(i) = $Accuracy_i$ - $Accuracy_{i-1}$}
%        \FOR{$i=0.9$ {\bfseries to} $0.2$}
%            \IF{Diff(i) $>$ Diff(i+1) and Acc(1.0) - Acc(i) $\geq$ 1.0}
%                \STATE {CSP $\Leftarrow$ i}
%            \ENDIF
%        \ENDFOR
%    \end{algorithmic}
%\end{algorithm}

We conduct several initial experiments that prune and re-train the embedding and transformer layers down to 90\% sparisty. We have summarized two observations from the model during these experiments as a guideline for cumulative sparsity evaluation.

\textit{Observation 1} - Task bias induced in the pre-trained backbone can significantly impact the overall performance. % This is the de-bias experiment process
% However, adapter-ALBERT maintains a fixed copy of the 
It is important to highlight that in our initial evaluation of the adapter models (Table~\ref{tab:model-acc}) we transferred the backbone parameters form the pre-trained vanilla ALBERT model and re-trained only the non-fixed layers. Pruning the model however, requires a different approach since at each incremental pruning step, the model needs to be retrained to recover from the loss of information. Retraining on BookCorpus would be extremely expensive. Therefore, we tested the effect of re-training the model during pruning using one of largest datasets with abundant information among the GLUE datasets, namely MNLI. While this version of the model could noticeably improve the accuracy performance on the MNLI task after pruning, it would also suffer a noticeable accuracy loss of 3\% on QNLI, which can be interpreted as having biased the backbone parameters towards MNLI. 
To minimize this biasing effect on the backbone parameters, we explored an iterative training approach by further fine-tuning the backbone parameters on QNLI and identifying the next dataset with the highest accuracy drop. Proceeding with this approach quickly leads to a loss of generalization capabilities in the backbone layers, especially when required to re-train on the smaller datasets, which exposes the model to overfitting. Therefore, we decided to explore an alternative route to recover the accuracy loss resulting from pruning.
%We later modified the experiment to employ a fine-tuned ALBERT model from MNLN dataset of GLUE tasks as the backbone of a second adapter-ALBERT, with the same re-training process on its non-fixed layers as the first model.
% First model -> original vanilla albert
% Second model -> pretrained on MNLI
%For un-pruned accuracy comparison, the second model yields superior result on MNLI than the first model, showing the first sign of pre-train bias.
%WE proceeded to prune the second model and identified the CSP at sparsity level 40\%. At this level, the adapter-ALBERT model shows the capability to delay rapid accuracy degradation similar to a vanilla ALBERT model, showing that the residual adapters have no negative impact to the model's performance from pruning.
%However, the QNLI result of the second model showed an unusual drop of 3\% accuracy. 

\textit{Observation 2} - Embedding layer and transformer layers show different pruning sensitivities.
In \textit{observation 1}, we analyzed the adapter-ALBERT model's response to a common pruning threshold applied to both embedding and transformer layers. However, a more accurate picture of how adapter-ALBERT reacts to pruning can be drawn by considering independent pruning thresholds for embedding and transformers.
The embedding layer accounts for approximately 39.9\% of the total model parameters, while the transformer layers contain about 60\%. We use cumulative sparsity as a way of normalizing the effect of pruning the individual blocks on the overall sparsity of the entire model. 
The cumulative sparsity is calculated as shown in Equation \ref{eq:1}, with $S_{c}$ representing cumulative sparsity, $S_{embd}$ and $S_{tf}$ denoting the sparsity for embedding and transformer layers, respectively, and $P_{embd}$ and $P_{tf}$ indicating the ratio of the embedding layer and transformer layers parameters to the entire model size.

\begin{equation}
    \label{eq:1}
    S_{c} = S_{embd} \times P_{embd} + S_{tf} \times P_{tf}
\end{equation}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{fig/Sparsity.png}
    \caption{Accuracy comparison for different pruning configurations. The blue and orange lines show the performance trends of the embedded layer and the transformer encoder layers with their isolated pruning cumulative sparsity, respectively. The green star represents the optimal result when combining pruning of both embedding and transformer layers. The gray dots show other pruning combinations with worse accuracy.}
    \label{fig:sparsity}
\end{figure}
\begin{figure*}[ht!]
    \centering
    
\vspace{-1em}
    % \hspace{-1.5em}
    \includegraphics[width=\textwidth]{fig/VASE_scatter_inset.pdf}
    % \vspace{-2em}
    \caption{Performance comparison of unpruned model, CSP-embd, CSP-tf, and CSP-all models using fine-tuning and VASE. VASE ($\bigcirc$) outperforms fine-tuning ($\times$) for MRPC, QQP, and RTE, while the two approaches yield similar results for MNLI, QNLI, and SST-2.}
    \label{fig:vase}
\end{figure*}
Based on cumulative sparsity calculated from Eq.~\ref{eq:1}, we show results on accuracy and sparsity level for embedding, transformers, and embedding and transformers together, as summarized in Figure \ref{fig:sparsity}. 
All pruning experiments are executed using the MNLI fine-tuned backbone and adapters.
Pruning only a backbone's embedding layer (blue curve) results in a significant accuracy degradation while providing only moderate sparsity levels around 20\%. These results indicate the high sensitivity of the embedding layer to pruning. 
The transformer layers show a slightly higher resilience to pruning (orange line), with the accuracy dropping below our baseline around 40\% sparsity. This results are in line with findings in literature~\cite{gordonCompressing2020}.
The gray dots show different combinations of embedding-transformer pruning with individual sparsity levels set between 0\% and 90\%. The green star represent the best combination that allows to achieve a cumulative CPS of 50\%, a clear improvement over applying pruning only to embeddings or transformers, which would lead to a CPS of 18.8\% and 41.2\%.
These results indicate that the adapter-ALBERT model is more resilient to embedding layer's sparsity, as the non-fixed layers in the transformer layers are able to learn and compensate these information loss.

\input{table/model-size.tex}

\subsubsection{\textbf{Variable Adapter Size Evaluation}}
While our experiments have shown that the combined pruning of embedding and transformer layers is more conducive to higher CPS values, we still need to verify the multi-task learning capabilities of the pruned backbone. In the next set of experiments, we use four different backbones which we define as follows:
\begin{itemize}
 \item \textbf{Unpruned}: The original un-pruned pre-trained model.
 \item \textbf{CSP-embd}: The model with only embedding layer pruned to the corresponding CSP.
 \item \textbf{CSP-tf}: The model with only transformer layers pruned to the corresponding CSP.
 \item \textbf{CSP-all}:The model with both embedding layer and transformer layers pruned to the corresponding CSPs.
\end{itemize}

We evaluate these backbone variants over six datasets from the GLUE benchmark to highlight three distinct NLP tasks: MRPC and QQP for paraphrasing, SST-2 for sentiment analysis, and MNLI, QNLI, and RTE for natural language inference. To better assist the adapter-ALBERT model in learning this diverse set of tasks we employ a variable adapter size which can be set for each module individually, choosing a value between 32, 64, or 128. This strategy is also compared against the vanilla ALBERT pruned model in which the transformer layers are fine-tuned for each of the sample datasets. The impact of the variable adapter size evaluation (VASE) strategy, can be observed on each model in terms of accuracy performance is presented in Figure \ref{fig:vase}. 

As a general trend, VASE provides significant accuracy improvements on most datasets and the ability to compensate for accuracy loss in pruned models compared to fine-tuning. 

% For the unpruned model, the majority of the datasets considered in the evaluation require a VASE configuration larger than the default size (64+64), which requires to introduce an additional parameter overhead to preserve inference accuracy. 
% For the CSP-embd and CSP-tf models, smaller configuration can be achieved.
% For the CSP-all model, almost all optimal VASE combinations are larger than the default 64+64 configuration, which can be imputed to the need for compensating increased sparsity in both embedding layer and transformer layers.

For the unpruned model, almost all of the datasets considered in the evaluation require an adapter size larger than the default setting (64+64), which requires to introduce an additional parameter overhead to preserve inference accuracy.
For the three pruned backbones (CSP-embd, CSP-tf, and CSP-all) the behavior is influenced by the specific task. Nonetheless, we can observe more tasks requiring larger adapter sizes going from CSP-tf to CSP-embd to CSP-all. Although not conclusive, this trend is in line with the growing sparsity level and the need to implement larger adapter sizes to compensate for increasing information loss.

%Comparing optimal VASE scores with the two baselines of directly fine-tuning pruned and un-pruned vanilla ALBERT models, it can be observed that fine-tuning pruned backbone models could receive unacceptable penalties on certain datasets.
%For example, the baseline model, the CSP-tf model, and the CSP-both model on QQP dataset is reporting zero or extremely low scores.

%Overall, VASE has shown positive results in compensate pruned models.
%As VASE has shown obvious task-specific characteristics, a selection strategy is needed to provide optimal VASE settings for best results and parameter overhead reduction merits.

Figure~\ref{fig:vase} shows the comparison of the unpruned and the three pruned backbones trained using VASE against the finetuned version of the model. The baseline thresholds are derived from the results in Table~\ref{tab:model-acc}. The added flexibility in setting different adapter sizes provides a way to combat the backbone bias introduced by fine-tuning the model using the MNLI dataset during the pruning stage. Notably, VASE can recover accuracy loss even in extreme cases such as the finetuned version of QQP, where the accuracy drops below 15\%. Comparing the different backbones in terms of accuracy, CPS-all shows comparable or better results against CPS-embd. CPS-tf has the best accuracy performance across all pruned backbones. As we will show in more detail in the next section, the choice of backbone does not affect the SRAM memory requirements, however selecting CSP-tf over CSP-all would increase the on-chip RRAM capacity by 29.7\%. 

%Considering the outstanding MTI capability, both adapter-BERT and adapter-ALBERT can keep each and every dataset at competitive accuracy with trivial overhead costs in stead of fine-tuning vanilla models every time for new tasks or store multiple copies of them.
%However, our adapter-ALBERT model has only 11.9M parameters v.s. the adapter-BERT model's 111.2M and only 0.2M parameters more than the smallest vanilla ALBERT model in comparison, under the adapter size setting 64+64.

%The four selected CSP models (baseline, CSP-embd, CSP-tf, and CSP-all) are re-trained on the six GLUE datasets with their adapter sizes sweeped through VASE for each dataset.
%The results of optimal VASE accuracy and baseline comparisons are shown in Figure \ref{fig:vase}.


% As a general picture of all six VASE and GLUE sweep results, we can observe that for the MNLI dataset, re-training the adapters on it always shows worse accuracy than directly fine-tuning on the MNLI dataset. Since the backbone model is already fine-tuned based on the MNLI dataset, it is more straightforward to infer it directly using the backbone model without adding extra adapter modules. This can lead to a more accurate and overhead-free result. For all other datasets, the intervention of the adapters module leads to good results. In many cases, the best combination of adapter sizes can obtain accuracy very close to or beyond those obtained by directly fine-tuning.


% We first use NVMExplorer to estimate the on-chip area requirements of both SRAM and RRAM arrays based on the total number of fixed parameters after pruning with cumulative sparsity evaluation.
% We also consider the worst-case scenario in terms of classifier labels and adapter sizes across all inference tasks to ensure enough storage for any possible cases.
% As a result, the area requirement does not change with the numbers of tasks, and new task-specific parameters are loaded from DRAM.
% We then consider the total number of compute cycles based on the 256 MAC units provided by the accelerator, and NVMEXplorer's memory read and write energy and latency estimates to produce energy and latency results.
% The total energy consumption is the summation of all memory components' own consumption, and the total latency is their long pole.
% Following this rule, multiple possible design choices are generated.
% We perform a memory design exploration study and pick an optimal solution balancing the trade-off between energy and latency while pursuing the smallest area, presented in Figure \ref{fig:PPA}.
% The results are comparison between the adapter-ALBERT and vanilla ALBERT models running 3-task MTI.
% The vanilla ALBERT model is evaluated following the same settings: its embedding layers are identically pruned as the adapter-ALBERT model, and is bit-mask encoded and stored in SLC and MLC RRAM.
% Similarly, we considered its worst-case with the largest number of parameters and non-linearity's across all layers.


% The memory architecture (Fig. 2) consists of a heterogeneous dedicated global buffer for the accelerator and off-chip DRAM. The accelerator (Tambe et al., 2020) can access these memories using DMA through a common memory interface. The sparse shared parameters are stored using bit-masks and non-zero values in single-level and multi-level RRAM arrays, respectively. This is done to prevent accuracy degradation due to faults in multi-level RRAM.  Task-specific parameters and activations are stored in SRAM. For multi-task inference, the accelerator reuses the parameters stored in RRAM and updates the task-specific parameters reloading the values from DRAM. 

% Results in figure 6 compare running 3-task MTI with our proposed solution and an implementation running a vanilla ALBERT model. For the latter, we still store embeddings in RRAM and consider an on-chip SRAM for the largest requirement in terms of model parameters and activations across all layers.  
% As explained in Section IIB, the adapter-ALBERT model is mapped onto three memory devices: non-fixed parts on SRAM, backbone non-zero values on MLC RRAM, and backbone bitmasks on SLC RRAM. 
% Similar to the MEMTI architecture \cite{Donato19} that integrates with NVDLA, NVSim \cite{Dong12}, and DRAM estimates, our HMA collaborates with EdgeBERT \cite{Tambe20} and NVM Explorer \cite{Pentecost21} simulations for power, performance, and area results.
% We develop layer-by-layer dataflow models for both adapter-ALBERT and vanilla ALBERT to generate RRAM, SRAM, and DRAM traffic analysis, memory access cycles, and multiple-and-accumulate operation cycles based on EdgeBERT specifications and NVM Explorer simulations.
% The integration with NVM Explorer also allows us to generate multiple design choices including different combinations of RRAM and SRAM memories that optimized for area, energy, and bandwidths.
% \fixme{The hardware models for adapter-ALBERT and vanilla ALBERT are decoupled with tasks to ensure scalability. 
% The downstream tasks have no influence towards dataflow and traffic between each layer of the model and memories. 
% MTI capability is also considered in the models with two options for SRAM to either expand to store multiple tasks or maintain storage while constantly refreshing for different tasks.
% The former option changes the SRAM area estimates and the latter option changes the energy requirements for data reads and writes.}

% The EdgeBERT framework \cite{Tambe20} proposes a primitive heterogeneous memory architecture that stores the ALBERT model's embedding layer leveraging RRAM device with quantization and bitmask encoding techniques, while the transformer layers and classifier are stored in a normal SRAM device. We expand the architecture in accordance with the MEMTI framework \cite{Donato19} with the open-source NVDLA accelerator design, to develop the novel heterogeneous memory architecture model. By identifying the data flow of the vanilla ALBERT and adapter-ALBERT models, the memory architecture is capable to generate precise layer-by-layer RRAM, SRAM, and DRAM traffic reports and corresponding memory access cycles and multiple-and-accumulate operation cycles from the EdgeBERT framework. By flexible integration with NVM Explorer \cite{Pentecost21}, a simulation tool evolved from NVSim \cite{Dong12}, we could further obtain optimal RRAM and SRAM models with targeted memory capacity, design optimization targets, and traffic types. Utilizing RRAM and SRAM models read and write bandwidth, energy consumption, and latency, we are able to generate energy reports and specific memory designs for vanilla ALBERT and adapter-ALBERT models.

% To keep consistent with the EdgeBERT framework, we implement quantization of 8-bit and 16-bit data representation models and bitmask encoding techniques to our heterogeneous memory architecture. Similar to EdgeBERT, we utilize 2 bit-per-cell RRAM devices to store the non-zero values and 1 bit-per-cell RRAM for bitmask matrix. With the comprehensive hardware models, we are able to identify the energy and latency bottleneck of the adapter-ALBERT model and apply different memory designs accordingly. MIT capability is also integrated with three tasks chosen from the three different categories of the GLUE benchmark, to demonstrate and evaluate the adapter-ALBERT model's MTI characteristics with performance, power, and area reports.
