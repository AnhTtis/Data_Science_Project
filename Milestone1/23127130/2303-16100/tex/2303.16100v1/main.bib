% LLM the bigger the better
@misc{EmergentWei22,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Attention & Transformer
@misc{TransformerVaswani17,
  doi = {10.48550/ARXIV.1706.03762},
  url = {https://arxiv.org/abs/1706.03762},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Attention Is All You Need},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Transformer Encoder
@misc{TransformerLiu18,
  doi = {10.48550/ARXIV.1801.10198},
  url = {https://arxiv.org/abs/1801.10198},
  author = {Liu, Peter J. and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Generating Wikipedia by Summarizing Long Sequences},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% BERT Model
@misc{BERTDevlin18,
  doi = {10.48550/ARXIV.1810.04805},
  url = {https://arxiv.org/abs/1810.04805},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% GPT Model
@misc{GPTRadford18,
  added-at = {2020-07-14T16:37:42.000+0200},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/273ced32c0d4588eb95b6986dc2c8147c/jonaskaiser},
  interhash = {5c343ed9a31ac52fd17a898f72af228f},
  intrahash = {73ced32c0d4588eb95b6986dc2c8147c},
  keywords = {final thema:transformer},
  timestamp = {2020-07-14T16:49:42.000+0200},
  title = {Improving language understanding by generative pre-training},
  year = 2018
}

% GPT-3
@misc{GPTBrown20,
  doi = {10.48550/ARXIV.2005.14165},
  url = {https://arxiv.org/abs/2005.14165},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Language Models are Few-Shot Learners},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% SQuAD
@misc{SQUADRajpurkar16,
  doi = {10.48550/ARXIV.1606.05250},
  url = {https://arxiv.org/abs/1606.05250},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% GLUE
@misc{GLUEWang18,
  doi = {10.48550/ARXIV.1804.07461},
  url = {https://arxiv.org/abs/1804.07461},
  author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Transfer learning
@article{TLWeiss16,
  title={A survey of transfer learning},
  author={Weiss, Karl and Khoshgoftaar, Taghi M and Wang, DingDing},
  journal={Journal of Big data},
  volume={3},
  number={1},
  pages={1--40},
  year={2016},
  publisher={SpringerOpen}
}

% ALBERT
@misc{ALBERTLan19,
  doi = {10.48550/ARXIV.1909.11942},
  url = {https://arxiv.org/abs/1909.11942},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Residual Adapter module
@misc{ResidualRebuffi17,
  doi = {10.48550/ARXIV.1705.08045},
  url = {https://arxiv.org/abs/1705.08045},
  author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Learning multiple visual domains with residual adapters},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ResidualRebuffi18,
  doi = {10.48550/ARXIV.1803.10082},
  url = {https://arxiv.org/abs/1803.10082},
  author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient parametrization of multi-domain deep neural networks},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% ResNet
@misc{ResNetHe15,
  doi = {10.48550/ARXIV.1512.03385},
  url = {https://arxiv.org/abs/1512.03385},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Residual Learning for Image Recognition},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% TFLM
@misc{TFLMdavid21,
      title={TensorFlow Lite Micro: Embedded Machine Learning on TinyML Systems}, 
      author={Robert David and Jared Duke and Advait Jain and Vijay Janapa Reddi and Nat Jeffries and Jian Li and Nick Kreeger and Ian Nappier and Meghna Natraj and Shlomi Regev and Rocky Rhodes and Tiezhen Wang and Pete Warden},
      year={2021},
      eprint={2010.08678},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% TinyBERT
@article{TinyBERTJiao19,
  author    = {Xiaoqi Jiao and
               Yichun Yin and
               Lifeng Shang and
               Xin Jiang and
               Xiao Chen and
               Linlin Li and
               Fang Wang and
               Qun Liu},
  title     = {TinyBERT: Distilling {BERT} for Natural Language Understanding},
  journal   = {CoRR},
  volume    = {abs/1909.10351},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.10351},
  eprinttype = {arXiv},
  eprint    = {1909.10351},
  timestamp = {Wed, 01 Sep 2021 15:40:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-10351.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% RoBERTa
@article{RoBERTaLiu19,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% MEMTI
@ARTICLE{MEMTIDonato19,
  author={Donato, Marco and Pentecost, Lillian and Brooks, David and Wei, Gu-Yeon},
  journal={IEEE Micro}, 
  title={MEMTI: Optimizing On-Chip Nonvolatile Storage for Visual Multitask Inference at the Edge}, 
  year={2019},
  volume={39},
  number={6},
  pages={73-81},
  doi={10.1109/MM.2019.2944782}}

% NVDLA
@INPROCEEDINGS{NVDLAZhou18,
  author={Zhou, Gaofeng and Zhou, Jianyang and Lin, Haijun},
  booktitle={2018 12th IEEE International Conference on Anti-counterfeiting, Security, and Identification (ASID)}, 
  title={Research on NVIDIA Deep Learning Accelerator}, 
  year={2018},
  volume={},
  number={},
  pages={192-195},
  doi={10.1109/ICASID.2018.8693202}}


% EdgeBERT
@misc{EdgeBERTTambe20,
  doi = {10.48550/ARXIV.2011.14203},
  url = {https://arxiv.org/abs/2011.14203},
  author = {Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Jia, Tianyu and Yang, En-Yu and Donato, Marco and Sanh, Victor and Whatmough, Paul N. and Rush, Alexander M. and Brooks, David and Wei, Gu-Yeon},
  keywords = {Hardware Architecture (cs.AR), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Adapter-BERT
@misc{AdapterBERTHoulsby19,
  doi = {10.48550/ARXIV.1902.00751},
  url = {https://arxiv.org/abs/1902.00751},
  author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Parameter-Efficient Transfer Learning for NLP},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% NVSim
@ARTICLE{NVSimDong12,
  author={Dong, Xiangyu and Xu, Cong and Xie, Yuan and Jouppi, Norman P.},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
  title={NVSim: A Circuit-Level Performance, Energy, and Area Model for Emerging Nonvolatile Memory}, 
  year={2012},
  volume={31},
  number={7},
  pages={994-1007},
  doi={10.1109/TCAD.2012.2185930}}

% Pruning BERT
@misc{PruningBERTGordon20,
  doi = {10.48550/ARXIV.2002.08307},
  url = {https://arxiv.org/abs/2002.08307},
  author = {Gordon, Mitchell A. and Duh, Kevin and Andrews, Nicholas},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Pruning in general
@misc{PruningLiang21,
      title={Pruning and Quantization for Deep Neural Network Acceleration: A Survey}, 
      author={Tailin Liang and John Glossner and Lei Wang and Shaobo Shi and Xiaotong Zhang},
      year={2021},
      eprint={2101.09671},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

% NVM EXplorer
@misc{NVMEPentecost21,
  doi = {10.48550/ARXIV.2109.01188},
  url = {https://arxiv.org/abs/2109.01188},
  author = {Pentecost, Lillian and Hankin, Alexander and Donato, Marco and Hempstead, Mark and Wei, Gu-Yeon and Brooks, David},
  keywords = {Emerging Technologies (cs.ET), Hardware Architecture (cs.AR), FOS: Computer and information sciences, FOS: Computer and information sciences, B.3; I.6},
  title = {NVMExplorer: A Framework for Cross-Stack Comparisons of Embedded Non-Volatile Memories},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

% ImageNET
@INPROCEEDINGS{ImageNetDeng09,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  doi={10.1109/CVPR.2009.5206848}}

% Transfer Learning
@article{TransferLearningYosinski14,
  author    = {Jason Yosinski and
               Jeff Clune and
               Yoshua Bengio and
               Hod Lipson},
  title     = {How transferable are features in deep neural networks?},
  journal   = {CoRR},
  volume    = {abs/1411.1792},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.1792},
  eprinttype = {arXiv},
  eprint    = {1411.1792},
  timestamp = {Mon, 13 Aug 2018 16:47:20 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/YosinskiCBL14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Squeeze-and-Excite
@article{SEHu17,
  author    = {Jie Hu and
               Li Shen and
               Gang Sun},
  title     = {Squeeze-and-Excitation Networks},
  journal   = {CoRR},
  volume    = {abs/1709.01507},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.01507},
  eprinttype = {arXiv},
  eprint    = {1709.01507},
  timestamp = {Wed, 11 Aug 2021 09:47:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1709-01507.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Source, FLOPs for liner = (2I - 1) * O w.o. bias or 2I*O with bias
@inproceedings{PruningCNNMolchanov17,
title={Pruning Convolutional Neural Networks for Resource Efficient Inference},
author={Pavlo Molchanov and Stephen Tyree and Tero Karras and Timo Aila and Jan Kautz},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=SJGCiw5gl}
}

% BERT-n-PALs
@misc{BERTPALsStickland19,
  doi = {10.48550/ARXIV.1902.02671},
  url = {https://arxiv.org/abs/1902.02671},
  author = {Stickland, Asa Cooper and Murray, Iain},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Network Slimming
@misc{NetworkSlimmingLiu17,
  doi = {10.48550/ARXIV.1708.06519},
  url = {https://arxiv.org/abs/1708.06519},
  author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Learning Efficient Convolutional Networks through Network Slimming},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Structured Sparsity Learning
@misc{SSLWen16,
  doi = {10.48550/ARXIV.1608.03665},
  url = {https://arxiv.org/abs/1608.03665},
  author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.6; I.5.1},
  title = {Learning Structured Sparsity in Deep Neural Networks},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% MCUNet
@misc{MCUNetLin20,
  doi = {10.48550/ARXIV.2007.10319},
  url = {https://arxiv.org/abs/2007.10319},
  author = {Lin, Ji and Chen, Wei-Ming and Lin, Yujun and Cohn, John and Gan, Chuang and Han, Song},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MCUNet: Tiny Deep Learning on IoT Devices},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% SqueezeNet
@article{SqueezeNetIandola16,
  author    = {Forrest N. Iandola and
               Matthew W. Moskewicz and
               Khalid Ashraf and
               Song Han and
               William J. Dally and
               Kurt Keutzer},
  title     = {SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and {\textless}1MB
               model size},
  journal   = {CoRR},
  volume    = {abs/1602.07360},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.07360},
  eprinttype = {arXiv},
  eprint    = {1602.07360},
  timestamp = {Fri, 20 Nov 2020 16:16:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/IandolaMAHDK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% TinyTL
@misc{TinyTLCai20,
  doi = {10.48550/ARXIV.2007.11622},
  url = {https://arxiv.org/abs/2007.11622}, 
  author = {Cai, Han and Gan, Chuang and Zhu, Ligeng and Han, Song},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning},
  publisher = {arXiv},
  year = {2020},
  copyright = {Creative Commons Attribution 4.0 International}
}

% TETRIS DRAM model
@article{TETRISGao17, author = {Gao, Mingyu and Pu, Jing and Yang, Xuan and Horowitz, Mark and Kozyrakis, Christos}, title = {TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory}, year = {2017}, issue_date = {March 2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {45}, number = {1}, issn = {0163-5964}, url = {https://doi.org/10.1145/3093337.3037702}, doi = {10.1145/3093337.3037702}, abstract = {The high accuracy of deep neural networks (NNs) has led to the development of NN accelerators that improve performance by two orders of magnitude. However, scaling these accelerators for higher performance with increasingly larger NNs exacerbates the cost and energy overheads of their memory systems, including the on-chip SRAM buffers and the off-chip DRAM channels.This paper presents the hardware architecture and software scheduling and partitioning techniques for TETRIS, a scalable NN accelerator using 3D memory. First, we show that the high throughput and low energy characteristics of 3D memory allow us to rebalance the NN accelerator design, using more area for processing elements and less area for SRAM buffers. Second, we move portions of the NN computations close to the DRAM banks to decrease bandwidth pressure and increase performance and energy efficiency. Third, we show that despite the use of small SRAM buffers, the presence of 3D memory simplifies dataflow scheduling for NN computations. We present an analytical scheduling scheme that matches the efficiency of schedules derived through exhaustive search. Finally, we develop a hybrid partitioning scheme that parallelizes the NN computations over multiple accelerators. Overall, we show that TETRIS improves mthe performance by 4.1x and reduces the energy by 1.5x over NN accelerators with conventional, low-power DRAM memory systems.}, journal = {SIGARCH Comput. Archit. News}, month = {apr}, pages = {751–764}, numpages = {14}, keywords = {neural networks, 3d memory, dataflow scheduling, acceleration, partitioning} }

% Collective RRAM Study
@article{CollectiveRRAMPanda18,
  title = {A {{Collective Study}} on {{Modeling}} and {{Simulation}} of {{Resistive Random Access Memory}}},
  author = {Panda, Debashis and Sahu, Paritosh Piyush and Tseng, Tseung Yuen},
  year = {2018},
  month = dec,
  journal = {Nanoscale Research Letters},
  volume = {13},
  number = {1},
  pages = {8},
  issn = {1931-7573, 1556-276X},
  doi = {10.1186/s11671-017-2419-8},
  abstract = {In this work, we provide a comprehensive discussion on the various models proposed for the design and description of resistive random access memory (RRAM), being a nascent technology is heavily reliant on accurate models to develop efficient working designs and standardize its implementation across devices. This review provides detailed information regarding the various physical methodologies considered for developing models for RRAM devices. It covers all the important models reported till now and elucidates their features and limitations. Various additional effects and anomalies arising from memristive system have been addressed, and the solutions provided by the models to these problems have been shown as well. All the fundamental concepts of RRAM model development such as device operation, switching dynamics, and current-voltage relationships are covered in detail in this work. Popular models proposed by Chua, HP Labs, Yakopcic, TEAM, Stanford/ASU, Ielmini, Berco-Tseng, and many others have been compared and analyzed extensively on various parameters. The working and implementations of the window functions like Joglekar, Biolek, Prodromakis, etc. has been presented and compared as well. New well-defined modeling concepts have been discussed which increase the applicability and accuracy of the models. The use of these concepts brings forth several improvements in the existing models, which have been enumerated in this work. Following the template presented, highly accurate models would be developed which will vastly help future model developers and the modeling community.},
  langid = {english},
  file = {/Users/ziruifu/Data/Zotero/storage/WCWRXPQ6/Panda et al. - 2018 - A Collective Study on Modeling and Simulation of R.pdf}
}

% Collective eNVM Study
@article{ENVMReviewChen16,
  title = {A Review of Emerging Non-Volatile Memory ({{NVM}}) Technologies and Applications},
  author = {Chen, An},
  year = {2016},
  month = nov,
  journal = {Solid-State Electronics},
  series = {Extended Papers Selected from {{ESSDERC}} 2015},
  volume = {125},
  pages = {25--38},
  issn = {0038-1101},
  doi = {10.1016/j.sse.2016.07.006},
  abstract = {This paper will review emerging non-volatile memory (NVM) technologies, with the focus on phase change memory (PCM), spin-transfer-torque random-access-memory (STTRAM), resistive random-access-memory (RRAM), and ferroelectric field-effect-transistor (FeFET) memory. These promising NVM devices are evaluated in terms of their advantages, challenges, and applications. Their performance is compared based on reported parameters of major industrial test chips. Memory selector devices and cell structures are discussed. Changing market trends toward low power (e.g., mobile, IoT) and data-centric applications create opportunities for emerging NVMs. High-performance and low-cost emerging NVMs may simplify memory hierarchy, introduce non-volatility in logic gates and circuits, reduce system power, and enable novel architectures. Storage-class memory (SCM) based on high-density NVMs could fill the performance and density gap between memory and storage. Some unique characteristics of emerging NVMs can be utilized for novel applications beyond the memory space, e.g., neuromorphic computing, hardware security, etc. In the beyond-CMOS era, emerging NVMs have the potential to fulfill more important functions and enable more efficient, intelligent, and secure computing systems.},
  langid = {english},
  keywords = {Emerging architecture,FeFET,Hardware security,Memory hierarchy,Neuromorphic computing,Nonvolatile memory,PCM,RRAM,Selector,Storage,STTRAM}
}

% Proposal of Memristor
@ARTICLE{MemristorChua71,
  author={Chua, L.},
  journal={IEEE Transactions on Circuit Theory}, 
  title={Memristor-The missing circuit element}, 
  year={1971},
  volume={18},
  number={5},
  pages={507-519},
  doi={10.1109/TCT.1971.1083337}}

% Memristor Breakthrough
 @article{MemristorBreakthroughStrukov08, title={The missing memristor found}, volume={453}, DOI={10.1038/nature06932}, number={7191}, journal={Nature}, author={Strukov, Dmitri B. and Snider, Gregory S. and Stewart, Duncan R. and Williams, R. Stanley}, year={2008}, pages={80–83}} 

 % RRAM Scalability
 @INPROCEEDINGS{RRAMDesignDeng13,
  author={Deng, Yexin and Chen, Hong-Yu and Gao, Bin and Yu, Shimeng and Wu, Shih-Chieh and Zhao, Liang and Chen, Bing and Jiang, Zizhen and Liu, Xiaoyan and Hou, Tuo-Hung and Nishi, Yoshio and Kang, Jinfeng and Wong, H.-S. Philip},
  booktitle={2013 IEEE International Electron Devices Meeting}, 
  title={Design and optimization methodology for 3D RRAM arrays}, 
  year={2013},
  volume={},
  number={},
  pages={25.7.1-25.7.4},
  doi={10.1109/IEDM.2013.6724693}}

% RRAM Density
@ARTICLE{RRAMDensityDeng13,
  author={Deng, Yexin and Huang, Peng and Chen, Bing and Yang, Xiaolin and Gao, Bin and Wang, Juncheng and Zeng, Lang and Du, Gang and Kang, Jinfeng and Liu, Xiaoyan},
  journal={IEEE Transactions on Electron Devices}, 
  title={RRAM Crossbar Array With Cell Selection Device: A Device and Circuit Interaction Study}, 
  year={2013},
  volume={60},
  number={2},
  pages={719-726},
  doi={10.1109/TED.2012.2231683}}

% RRAM Better Read, Write?
@ARTICLE{RRAMWriteSheu11,
  author={Sheu, Shyh-Shyuan and Cheng, Kuo-Hsing and Chang, Meng-Fan and Chiang, Pei-Chia and Lin, Wen-Pin and Lee, Heng-Yuan and Chen, Pang-Shiu and Chen, Yu-Sheng and Wu, Tai-Yuan and Chen, Frederick T. and Su, Keng-Li and Kao, Ming-Jer and Tsai, Ming-Jinn},
  journal={IEEE Design \& Test of Computers}, 
  title={Fast-Write Resistive RAM (RRAM) for Embedded Applications}, 
  year={2011},
  volume={28},
  number={1},
  pages={64-71},
  doi={10.1109/MDT.2010.96}}


% RRAM CMOS/FPGA Compatibility
@ARTICLE{RRAMCMOSTanachutiwat11,
  author={Tanachutiwat, Sansiri and Liu, Ming and Wang, Wei},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems}, 
  title={FPGA Based on Integration of CMOS and RRAM}, 
  year={2011},
  volume={19},
  number={11},
  pages={2023-2032},
  doi={10.1109/TVLSI.2010.2063444}}

@inproceedings{pentecostMaxNVMMaximizingDNN2019,
  title = {{{MaxNVM}}: {{Maximizing DNN Storage Density}} and {{Inference Efficiency}} with {{Sparse Encoding}} and {{Error Mitigation}}},
  booktitle = {International {{Symposium}} on {{Microarchitecture}}, {{MICRO}}},
  author = {Pentecost, Lillian and Donato, Marco and Reagen, Brandon and Gupta, Udit and Ma, Siming and Wei, Gu-Yeon and Brooks, David},
  year = {2019},
  month = oct,
  pages = {769--781},
  publisher = {{ACM}},
  doi = {10.1145/3352460.3358258},
  copyright = {All rights reserved},
  isbn = {978-1-4503-6938-1}
}

@inproceedings{reagenMinervaEnablingLowPower2016,
  title = {Minerva: {{Enabling Low-Power}}, {{Highly-Accurate Deep Neural Network Accelerators}}},
  booktitle = {International {{Symposium}} on {{Computer Architecture}}, {{ISCA}}},
  author = {Reagen, Brandon and Whatmough, Paul and Adolf, Robert and Rama, Saketh and Lee, Hyunkwang and Kyu, S. and Jos{\'e}, L. and Wei, Gu-Yeon Y. and Brooks, David and Lee, Sae Kyu and {Hernandez-Lobato}, Jose Miguel and Wei, Gu-Yeon Y. and Brooks, David},
  year = {2016},
  pages = {267--278}
}

@misc{zhuTrainedTernaryQuantization2017,
  title = {Trained {{Ternary Quantization}}},
  author = {Zhu, Chenzhuo and Han, Song and Mao, Huizi and Dally, William J.},
  year = {2017},
  month = feb,
  number = {arXiv:1612.01064},
  eprint = {arXiv:1612.01064},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1612.01064},
  urldate = {2023-03-21},
  abstract = {Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it's as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16x smaller than full-precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04\%, 0.16\%, 0.36\%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3\% of Top-1 accuracy and outperforms previous ternary models by 3\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{hubaraBinarizedNeuralNetworks2016,
  title = {Binarized {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and {El-Yaniv}, Ran and Bengio, Yoshua},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-21},
  abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At train-time the binary weights and activations are used for computing the parameter gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to  substantially improve power-efficiency. To validate the effectiveness of BNNs, we conducted two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. We also report our preliminary results on the challenging ImageNet dataset. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster  than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.}
}

@inproceedings{carmichaelPerformanceEfficiencyTradeoffLowPrecision2019,
  title = {Performance-{{Efficiency Trade-off}} of {{Low-Precision Numerical Formats}} in {{Deep Neural Networks}}},
  booktitle = {Proceedings of the {{Conference}} for {{Next Generation Arithmetic}} 2019},
  author = {Carmichael, Zachariah and Langroudi, Hamed F. and Khazanov, Char and Lillie, Jeffrey and Gustafson, John L. and Kudithipudi, Dhireesha},
  year = {2019},
  month = mar,
  pages = {1--9},
  publisher = {{ACM}},
  address = {{Singapore Singapore}},
  doi = {10.1145/3316279.3316282},
  urldate = {2023-03-21},
  isbn = {978-1-4503-7139-1},
  langid = {english}
}
