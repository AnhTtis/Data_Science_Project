\section{Introduction}

\begin{figure}[b!]
    \centering
    \includegraphics[width=8.3cm]{fig/bert-models.png}
    \caption{Comparison between BERT (a) and ALBERT (b) models. The BERT model needs to keep twelve unique layers of different parameters. The ALBERT model shares identical parameters across its twelve transformer layers, resulting a significant reduction of numbers of parameters.}
    \label{fig:bert-comp}
\end{figure}

State-of-the-art deep neural network (DNN) large language models (LLMs) are rapidly increasing in size and complexity to embrace emergent abilities and tackle advanced intelligent computations \cite{EmergentWei22}. This trend poses challenges to power- and resource-limited mobile system-on-chip (SoC) on which the models are stored and executed. In the natural language processing (NLP) domain, for example, after the proposal of the Transformer \cite{TransformerVaswani17, TransformerLiu18} architecture, the BERT and GPT model families have continuously made breakthroughs in real-world applications. 
%and benchmarks such as GLUE \cite{GLUEWang18} and SQuAD \cite{SQUADRajpurkar16}. 
The $\mathrm{BERT_{large}}$ model has 340 million parameters \cite{BERTDevlin18} and the GPT-3 model has reached record-breaking 175 billion parameters \cite{GPTBrown20}, leaving memory footprints of 1.27GB and 652GB using single-precision data representation format correspondingly. Consequently, their enormous memory requirements and data movement costs hinder direct deployment onto mobile SoCs.

Many edge and IoT systems are build in a form factor that blends in with the surrounding environment. For such systems, traditional input modalities are not a viable option. On the other hand, DNN models introducing conversational AI capabilities have the potential to introduce more natural and proactive interactions, especially when running multi-task inference (MTI) operations aimed at combining tasks such as paraphrasing, sentimental analysis, and question-answering. Although existing solutions are effective in alleviating the bottlenecks of running a single NLP task, achieving low-latency MTI requires running computations over multiple variants of the model parameters, which are tailored to each of the targeted tasks. This approach leads to either prohibitive on-chip memory requirements or expensive off-chip memory access. Additionally, the cost of deploying multiple copies of the same model increases with additional tasks.

For example, the ALBERT architecture~\cite{ALBERTLan19} reduces the number of model parameters by 10$\times$ compared to BERT by replacing the twelve distinct Transformer encoder layers with twelve identical layers, achieving a higher-level parameter reuse as shown in the structural comparison in Figure \ref{fig:bert-comp}. Much like its beefier relatives, ALBERT implements a language model that is usually pre-trained on large text corpora to learn both low- and high-level language features and can be used as foundation for transfer learning to easily perform task-switching by fine-tuning the transformer layers. The relatively small footprint and the ability to efficiently learn new tasks through fine-tuning make ALBERT a good candidate for edge deployment. However, while task adaption via fine-tuning is an effective solution to optimize training for a specific task, it suffers from catastrophic forgetting, or losing information on previously learned tasks. This limitation makes the fine-tuning approach inadequate to address many real-world DNN application scenarios that require running inference across a variety of tasks. 
Alternative solutions such as TinyTL \cite{TinyTLCai20}, MCUNet \cite{MCUNetLin20}, and custom models converted using edge-friendly lightweight libraries~\cite{TFLMdavid21}, have been proposed specifically to run on embedded devices with only KBs of memory. However, these solutions are not scalable when the target application requires to run either more complex DNN models or multiple inference modalities. The compressed models also have the downsides of being highly task-specialized, losing generalization capabilities, and requiring considerable training efforts for better accuracy when changing tasks. 
%Home-owned healthcare robots, smart voice assistants, and many virtual reality or augmented reality wearable devices often necessitate 

In contrast to these solutions, we offer a hardware and software co-design addressing MTI challenges of LLMs on-chip storage and execution. We first optimize the ALBERT model to perform more efficient task adaptation by integrating residual adapters in the network. Residual adapters were originally introduced to enable efficient learning of new tasks with minimal parameter overhead in ResNet for computer vision tasks~\cite{ResidualRebuffi17, ResidualRebuffi18}, and their application to NLP was first demonstrated in combination with the BERT model~\cite{AdapterBERTHoulsby19}. In contrast to the adapter-BERT model, for which the primary goal is to enable efficient training for transfer learning \cite{TransferLearningYosinski14}, we modify ALBERT to incorporate residual adapters as a way to address MTI applications by maximizing parameter reuse while minimizing the memory footprint.
The resulting \textbf{adapter-ALBERT} model inherits the majority of its parameters from a pre-trained vanilla ALBERT model as \textbf{backbone layers}, while its \textbf{task-specific layers} can be trained on new inference scenarios.

Our approach of partitioning the model between task-agnostic parameters in the backbone layers and task-specific parameters in the non-fixed layers, provides a viable pathway to both efficiently store the entire set of parameters for a single task on chip, and reduces the costs of DRAM memory access by requiring to update only a small number of task-specific parameters for executing new tasks.
However, the modifications introduced by the adapter-ALBERT model alone are not sufficient without the appropriate hardware support.
Therefore, we explore the design of an heterogeneous embedded scratchpad memory architecture in which two technologies are combined: CMOS-based SRAM and non-volatile resistive RAM (RRAM). 
RRAM, like other embedded non-volatile memories~\cite{ENVMReviewChen16}, provides high memory density, low energy consumption, and comparable read latency when compared to SRAM. However, these emerging devices, while promising, have not yet reached the maturity to fully replace SRAM because of their underwhelming write performance, limited endurance, and lower reliability. These factors make non-volatile memories, including RRAM, a uncompelling for workloads displaying frequent write operations. 
By combining these two technologies, we take advantage of the high storage density and non-volatility in RRAM and mask its non-idealities by using this portion of the heterogeneous memory to store the backbone layers exclusively. On the other hand, non-fixed layers which are more susceptible to updates at runtime are stored in SRAM with minimal added cost.

A similar approach was introduced in MEMTI~\cite{MEMTIDonato19} to implement a weight scratchpad for the NVIDIA Deep Learning Accelerator (NVDLA)~\cite{NVDLAZhou18} to enable the concurrent operation on several computer vision inference tasks. In a similar fashion, our memory architecture is designed as a dedicated scratchpad memory to support the operation of a validated NLP hardware accelerator EdgeBERT~\cite{EdgeBERTTambe20}.

Following the modifications we introduce with the adapter-ALBERT model and its mapping to the heterogeneous scratchpad memory architecture, we perform co-design optimizations including pruning, adapter-size accuracy recovery, quantization, and using a mixture of single-level cell (SLC) and 2-bit multi-level cell (MLC) RRAM to efficiently store bit-mask encoded backbone layers parameters. To the best of our knowledge, these techniques allow us to offer the best performance and energy per inference for NLP multi-task inference running on edge devices. In summary, this paper introduces the following contributions:
\begin{itemize}
    \item We propose a MTI-efficient adapter-ALBERT model that enjoys maximum data reuse and small parameter overhead for multiple tasks while maintaining comparable performance than other similar and base models.
    \item We further optimize the model for deployment in resource-constrained edge devices by running a sensitivity study to evaluate the impact of model compression methods to inference accuracy and multi-task adaptability.
    \item We evaluate the performance of the model when running on an edge system by integrating our proposed heterogeneous embedded memory architecture with an NLP accelerator modeled after EdgeBERT, and report energy consumption, latency, and area for the resulting architecture.
\end{itemize}