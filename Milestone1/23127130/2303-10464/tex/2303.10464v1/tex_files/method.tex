\section{Methodology}
\label{sec:method}

This section presents our method to reduce pre-training FLOPs using unstructured
weight sparsity. We first explain our intuition and hypotheses, followed by our
methodology for the SPDF framework.

\subsection{Intuition and Hypotheses}
\label{sec:hypotheses}
Prior works have shown that overparameterization of neural networks improves
optimization and generalizability~\citep{Soltanolkotabi2019, neyshabur2018the,
pmlr-v97-allen-zhu19a}, but leads to an increase in compute
cost~\citep{brown2020gpt3}. Recent work on the Lottery Ticket
Hypothesis~\cite{frankle2018lottery} demonstrates that overparameterized dense
networks contain sparse subnetworks which can be trained to the same accuracy as
their dense counterparts, as long as one initializes the training with a good
sparsity mask (``lottery ticket''). However, the process of searching for high
quality sparse subnetworks is computationally
expensive~\citep{frankle2018lottery, ma2022effective}. Existing sparse training
methods~\citep{evci2020rigl, mocanu2018, jayakumar2020top} aim to discover the
winning lottery ticket (i.e., optimal sparsity mask) in a single training run,
but often fall short of the dense model's accuracy. In our framework, we
mitigate the loss in representational power due to difficulties in sparse
optimization, by transitioning to fully dense weight matrices during the
fine-tuning phase. Even though we perform dense fine-tuning, the computational
costs associated with fine-tuning are significantly lower than the cost of
pre-training LLMs. Therefore, our method targets the phase which dominates the
training FLOPs (i.e., pre-training). Based on recent theoretical findings and
empirical studies on overparameterization and sparse neural networks, we lay out
a set of hypotheses that we aim to study in our work through extensive
experimental evaluation:

\paragraph*{\normalfont\textit{Hypothesis 1\labeltext{1}{hyp:one}: High degrees
of weight sparsity can be used during the pre-training phase of LLMs while
preserving the downstream accuracy with dense fine-tuning.}} \mbox{}

Inducing sparsity during pre-training may cause a loss in representational power
due to difficulties in sparse optimization and the inability to discover optimal
sparsity masks. To mitigate these challenges, we aim to increase the
representational power by allowing the zeroed weights to grow during fine-tuning
(i.e., dense fine-tuning). Additionally, note the full general learning
capability of the pre-trained model is often not required to generalize on the
simpler downstream task when using sparsity during
pre-training~\citep{Ding2022DeltaTA}. \citet{aghajanyan-etal-2021-intrinsic}
investigate this phenomenon from a different angle and show that pre-trained
language models can learn a large set of NLP tasks with only a few parameters.
% They show that the pre-training minimizes the intrinsic
% dimension~\citep{li2018measuring} when later fine-tuning various downstream
% tasks. 
This indicates that the full parameterization of the model is not needed to
generalize well across downstream fine-tuning tasks. Hence, we can exploit
weight sparsity during pre-training while retaining important textual
representations despite the model's lower representational capacity.

\paragraph*{\normalfont \textit{Hypothesis 2\labeltext{2}{hyp:two}: The
performance of the sparse pre-trained model is correlated with the dataset size
and degree of difficulty in the downstream task.}} \label{hyp:two} \mbox{}

\citet{liu2023sparsity} evaluate sparse networks on a diverse set of tasks with
varying degrees of difficulty and show a strong correlation between a model's
ability to be sparsified and task difficulty. Hence, we hypothesize that
models trained on complex tasks with high sparsity levels can suffer more from
sparse training and experience a greater drop in performance compared to simpler
tasks. Also note, small fine-tuning datasets may trigger over-fitting.
Therefore, we hypothesize that a larger dataset can allow the sparse model to
improve its generalization error on the task, and recover from training with
high sparsity.

\paragraph*{\normalfont\textit{Hypothesis 3\labeltext{3}{hyp:three}: As we
 increase the size of the language model, larger models become more amenable to
 higher levels of sparsity during pre-training.}} \label{hyp:three} \mbox{}

Existing work~\citep{liu2022the} has shown that the quality of a network trained
with random static sparsity (even at high sparsity levels) improves quickly to
match its dense counterpart as the network grows wider and deeper. Also, larger
models tend to have a smaller intrinsic
dimension~\citep{aghajanyan-etal-2021-intrinsic}, which suggests that all
parameters are not required to represent the average NLP task. Therefore, we
expect the gap in downstream performance between the sparse pre-trained model
and its dense counterpart to grow smaller as the size of the model increases.  

\begin{figure}
    \centering
    \includegraphics[keepaspectratio=true, width=0.91\linewidth]{./figures/spdf_framework.pdf}
    \caption{Sparse Pre-training and Dense Fine-tuning (SPDF) framework. In this framework, we sparsify a dense network and perform sparse pre-training followed by dense fine-tuning (green connections indicate newly activated weights). We use SPDF to pre-train large GPT models at a fraction of the training FLOPs using weight sparsity, and still retain the benefits on downstream tasks with dense fine-tuning.  }
    \label{fig:spdf}
\end{figure}

\subsection{Sparse Pre-training and Dense Fine-tuning}
Given a dense neural network $f_{\Theta}({\cdot})$ with parameters $\Theta \in
\mathbb{R}^{N}$. The parameters of the $l^{th}$ layer $\in L$ total layers are
denoted as $\Theta_l$, along with the total number of parameters represented as
$N_l$. We use $s_l$ to refer to the sparsity of layer $l$, where a sparse layer
drops $s_l \in (0, 1)$ of its connections, resulting in a total of $(1-s_l)N_l$
parameters. Finally, the overall sparsity of a sparse subnetwork is defined as
the ratio of zeroes to the total number of parameters in the original dense
network, i.e., $S = \frac{\sum_l^L s_lN_l}{N}$.

In our work, we focus solely on static sparsity, where the weights are randomly
pruned at initialization. Specifically, we remove weights in each layer $l \in
L$ randomly to the target sparsity $s_l$. Although several works have explored
generating different layer-wise sparsity ratios at initialization (e.g.,
Erd\"os-R\'enyi-Kernel~\cite{evci2020rigl}, Ideal Gas
Quota~\citep{chen2022sparsity}, SNIP~\citep{lee2018snip},
GraSP~\citep{Wang2020Picking}, SynFlow~\citep{tanaka2020synflow}, etc.), we
focus on the simplest setup, which is uniform sparsity~\citep{zhu2018toprune}.
In uniform sparsity, each sparsified layer is pruned to the same target sparsity
level. During the dense fine-tuning phase, all newly activated weights are
initialized to 0. We evaluated other initializations like random values, but
this did not lead to better results. Figure~\ref{fig:spdf} illustrates the
Sparse Pre-training and Dense-Finetuning (SPDF) framework, and we describe it as
follows:

\begin{enumerate}
    \item Sparsify a given dense network to some target sparsity level, $s_l$,
    at each sparsifiable layer.
    \item \textit{Pre-train} the sparse model following the same training
    schedule as the original dense model.
    \item \textit{Fine-tune} the pre-trained sparse network on a given
    downstream task in a dense manner by allowing the zeroed weights to learn.
\end{enumerate}
