\section{Introduction}
Large language models have contributed to significant advances in natural
language understanding (NLU) and natural language generation (NLG) due to the
introduction of pre-training methods~\citep{Devlin2019BERTPO,
Radford2018ImprovingLU} on massive unannotated datasets (e.g.,
Pile~\citep{gao2020pile}, MassiveText~\citep{rae2021gopher}, etc.). While scaling
the model and dataset size has improved the quality of
LLMs~\citep{wei2022emergent}, it has also substantially increased the
computational cost of pre-training. For instance, GPT-3
175B~\citep{brown2020gpt3} is estimated to cost millions of dollars to
train~\citep{li_2022}. Various techniques have been proposed to reduce the
computational cost of training LLMs, including sparse
attention~\citep{dao2022flashattention, jaszczur2021sparse}, improved
optimization techniques~\citep{tang20221bitadam}, and sequence-level curriculum
learning~\citep{li2022the}. While these methods can help reduce computation
time, weight sparsity is one promising technique orthogonal to the above
methods. Here, a subset of model parameters are set to zero, reducing the FLOPs
required during training. Despite recent advances in sparse
training~\citep{hoefler2022sparsity}, it has yet to be widely adopted by
practitioners. First, it is difficult and expensive to find the optimal sparsity
pattern~\citep{frankle2018lottery, ma2022effective} that can maintain the same
level of accuracy as dense models. Second, unstructured sparsity can be
difficult to accelerate on hardware architectures optimized for dense
computation~\citep{sara2020lottery}. In this work, we show how we can leverage
weight sparsity to reduce training FLOPs, and then recover the lost
representational capacity by shifting to dense weight matrices when fine-tuning
on downstream tasks. In addition, while specialized software kernels have been
developed to achieve inference acceleration with unstructured
sparsity~\citep{gale2020sparse, neural_magic_2021, elsen2019sparse,
Ashby2019ExploitingUS, Wang2021SparseDNNFS}, recent work has shown that we can
realize the gains of unstructured weight sparsity on specialized hardware (e.g.,
Cerebras CS-2~\citep{lie_2022, lie_2021}) when training LLMs. For
example,~\citet{lie_2021} shows the measured speedup for a matrix multiplication
kernel~w.r.t to the sparsity level on a single GPT-3 layer (see
Appendix~\ref{app:unstructured_sparse_nongpu} for more details). Therefore, as
unstructured sparse training techniques continue to become co-designed with the
hardware, we can expect the FLOP reduction to translate into performance and
wall-clock speedups.


Prior work on sparsifying LLMs focuses on reducing
training~\citep{chen2022pixelated, dao2022monarch} or inference
FLOPs~\citep{chen2020lth} while matching standard dense training.
\citet{chen2022pixelated} and~\citet{dao2022monarch} replace dense matrices with
butterfly-based structured sparse weight matrices to reduce a model's size and
accelerate pre-training on block-oriented hardware (e.g.,
GPUs~\citep{krashinsky_2020}, TPUs~\citep{xin2020sparsetpu}). Training with
structured sparsity requires maintaining a regular sparse structure, which can
reduce expressivity at higher sparsity levels. This is a well-known constraint
observed when imposing structured sparsity in dense weight
matrices~\citep{zhou2021learning, jiang2022exposing}. The recent innovations in
hardware architectures aim to facilitate the widespread use and adoption of
unstructured weight sparsity, enabling the ability to achieve higher compression
ratios while attaining practical speedups~w.r.t wall-clock time. Our work
focuses on pre-training with unstructured weight sparsity to reduce the FLOPs
for training language models.

In the recent NLP literature, it is common to first pre-train, then fine-tune a
language model. Fine-tuning pre-trained LLMs on downstream tasks leads to
significantly better accuracy than the zero or few-shot
settings~\citep{alt-etal-2019-fine, ouyang2022training}. The pre-training phase
takes significantly longer compared to fine-tuning on a much smaller dataset to
learn the domain-specific task. In the standard setup, the model size and
capacity is generally kept the same between the two phases. We propose to break
this assumption and show the benefits of modifying the model capacity between
pre-training and fine-tuning with weight sparsity. First, we pre-train a sparse
GPT model to reduce computational training FLOPs. Then, during the fine-tuning
phase, we densify the GPT model, allowing the zeroed weights to learn and
increase the modeling capacity to more accurately learn the downstream task.
While previous work has explored sparse-to-dense training to mitigate the
difficulties of sparse-to-sparse training~\citep{dao2022monarch} and improve the
accuracy of dense models~\citep{han2017dsd}, we perform fully sparse
pre-training and only transition to dense weight matrices during fine-tuning. We
refer to this framework as Sparse Pre-training and Dense Fine-tuning (SPDF) and
demonstrate the ability of the sparse pre-trained model to transfer effectively
to different downstream tasks (e.g., natural language generation and text
summarization). The main contributions of our work are:

\begin{enumerate}
    \item We propose Sparse Pre-training and Dense Fine-tuning (SPDF) as a new
    framework to reduce the FLOPs required during the pre-training phase while
    maintaining accuracy on downstream tasks.

    \item We demonstrate that we can train GPT-3 XL, at 75\% sparsity, reducing
    the overall training FLOPS by 2.5x, while retaining the benefits of
    pre-trained textual representations in LLMs across a majority of tasks and
    evaluation metrics.

    % \item We show that on the WikiText-103 dataset, GPT-2 Small with 90\%
    % sparsity only sees a 0.36 point increase in perplexity over its dense
    % counterpart, while obtaining a 2.4x FLOP reduction over existing
    % structured sparse setups.
    
    \item We establish a correlation between the optimal sparsity level during
    pre-training and the fine-tuning dataset size and task difficulty.
\end{enumerate}