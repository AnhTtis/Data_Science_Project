\section{Conclusion and Future Work}

In this work, we introduced Sparse Pre-training and Dense Fine-tuning (SPDF) to
reduce the computational FLOPs of training GPT models using weight sparsity. To
the best of our knowledge, this is the first time a large GPT model has been
pre-trained with high sparsity (50\%-75\%) without significant loss in
downstream task metrics. In our work, we only use simple static sparsity, which
is arguably the most na√Øve way to induce sparsity in neural networks. As for
future work, there are several natural directions for improving our results on
even larger models, including dynamic sparsity methods, better optimization
techniques for sparse training, and architectures amenable to sparse training.
Moreover, to limit the computational cost of our study, we trained our GPT
models following the Chinchilla scaling law. Although the Chinchilla
pre-training schedule has been shown to be FLOP-optimal for dense models, we
plan to investigate how well it transfers to sparse models, especially at
extreme sparsity levels. Our future work will also investigate sparse scaling
outside the Chinchilla dense scaling laws. Regardless, we see the tremendous
promise of unstructured weight sparsity to accelerate the training of LLMs,
enabled by the recent advances in deep learning hardware accelerators.

