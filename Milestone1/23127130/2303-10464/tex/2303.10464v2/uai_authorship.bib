@InProceedings{von-o-lopez_969,
    title = {SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models},
    author = {Thangarasa, Vithursan and Gupta, Abhay and Mashall, William and Li, Tianda and Leong, Kevin and DeCoste, Dennis and Lie, Sean and Saxena, Shreyas},
    abstract = {The pre-training and fine-tuning paradigm has contributed to a number of
    breakthroughs in Natural Language Processing (NLP). Instead of directly
    training on a downstream task, language models are first pre-trained on
    large datasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.)
    and then fine-tuned on task-specific data (e.g., natural language
    generation, text summarization, etc.). Scaling the model and dataset size
    has helped improve the performance of LLMs, but unfortunately, this also
    lead to highly prohibitive computational costs. Pre-training LLMs often
    require orders of magnitude more FLOPs than fine-tuning and the model
    capacity often remains the same between the two phases. To achieve training
    efficiency~w.r.t training FLOPs, we propose to decouple the model capacity
    between the two phases and introduce Sparse Pre-training and Dense
    Fine-tuning (SPDF). In this work, we show the benefits of using unstructured
    weight sparsity to train only a subset of weights during pre-training
    (Sparse Pre-training) and then recover the representational capacity by
    allowing the zeroed weights to learn (Dense Fine-tuning). We demonstrate
    that we can induce up to 75\% sparsity into a 1.3B parameter GPT-3 XL model
    resulting in a 2.5x reduction in pre-training FLOPs, without a significant
    loss in accuracy on the downstream tasks relative to the dense baseline. By
    rigorously evaluating multiple downstream tasks, we also establish a
    relationship between sparsity, task complexity and dataset size. Our work
    presents a promising direction to train large GPT models at a fraction of
    the training FLOPs using weight sparsity, while retaining the benefits of
    pre-trained textual representations for downstream tasks.}
        
        
        von O'L\'opez, Jane J. and Bovik, Harry Q. and Coauthor, Further and von V\`ereweg, \'E. \v{S}. \r{A}land},
    abstract = {This is the authorship bibfile for your accepted paper. The paper code `von-o-lopez_969' should be replaced with the one for your paper. This bibfile should contain the title of your paper in sentence case (but without any other change from the paper). It should also contain the author list in BibTeX format as above, so in ‘Lastnames, Firstnames’ format (same authors and order as in the submitted version, as you are not allowed to change them). For accented characters, use LaTeX escaped codes, not UTF-8. Finally, you should copy your paper's abstract here, again using LaTeX escaped codes, not UTF-8. Of course, also the constraints about full citations (if any), no math, and no abbreviations carry over from the paper.}
}