\section{Experimental Setup and Results}
\label{sec:results}
First, we provide details on our pre-training settings for GPT-2 Small (125M)
and GPT-3 XL (1.3B), as well as our setups for the downstream fine-tuning tasks.
Then, we compare sparse pre-training and sparse fine-tuning against sparse
pre-training and dense fine-tuning to highlight the benefits of fine-tuning in a
dense manner. Next, we validate our hypotheses (refer to
Section~\ref{sec:hypotheses}) by evaluating SPDF across several tasks in natural
language generation and text summarization. Following this, we compare the
parameter subspaces between the pre-trained and fine-tuned models. Last, we
present the advantages in training efficiency w.r.t total training FLOPs when
using SPDF versus standard dense pre-training and dense fine-tuning. 

All GPT models are pre-trained and fine-tuned using the Cerebras CS-2, taking
advantage of its ability to accelerate training with unstructured sparsity. At
present, the specialized kernels of Cerebras CS-2 are designed to facilitate
training with static unstructured sparsity. Consequently, the results presented
in this section do not include the utilization of dynamic sparse training
methods (e.g., SET~\citep{mocanu2018}, RigL~\citep{evci2020rigl}, etc). In
Appendix C, we emphasize the possible advantages achieved through unstructured
weight sparsity on the Cerebras CS-2. We provide measured speedup results
compared to theoretical speedup across different sparsity levels for a GPT-3
layer's 12k~$\times$ 12k matrix multiplication (MatMul)~\citep{lie_2022}.

\begin{figure*}[!t]
    \centering
    \includegraphics[keepaspectratio=true, width=0.82\linewidth]{./figures/gpt2_small_sparse_pretrain_dense_ft.pdf}
    \caption{ Comparison of sparse-to-dense vs sparse-to-sparse pre-training and
    fine-tuning with GPT-2 Small on E2E, WebNLG and DART. Across tasks dense
    fine-tuning noticeably outperforms sparse fine-tuning, especially at 75\%
    sparsity. }
    \label{fig:sparseft}
    \vspace{-0.1in}
\end{figure*}

\paragraph{Flop Optimal Pre-training via Chinchilla Scaling Law}
It was previously conventional in the literature to train all large language
models (e.g., GPT-3~\citep{brown2020gpt3}, Gopher~\citep{rae2021gopher},
Jurassic~\citep{J1WhitePaper}, etc.) on approximately 300B tokens of data. More
recently, Chinchilla~\citep{hoffmann2022an} shows how parameters and data should
be scaled equally as compute budget increases, which leads to significant gains
in FLOP efficiency. In our pre-training setup, we follow Chinchilla's scaling
law which suggests that we need approximately 20 tokens per parameter. Thus, for
GPT-2 Small, a model with 125M parameters needs to be pre-trained on 2.5B
tokens. Then, for GPT-3 XL, a model which has 1.3B parameters, needs to be
pre-trained on 26B tokens. Unless stated otherwise, we pre-train our sparse GPT
models from scratch on the Pile dataset~\citep{gao2020pile} across sparsity
levels $S \in \{50\%, 75\%\}$.


\paragraph{Fine-tuning on Downstream Tasks}  We studied dense fine-tuning on
several downstream tasks in natural language generation and text summarization.
We follow~\citet{hu2022lora} in using the three standard natural language
generation benchmark datasets (i.e., E2E~\citep{novikona2017e2e},
WebNLG~\citep{gardent2017webnlg} and DART~\citep{nan2021dart}). In addition, we
fine-tune on Curation Corpus~\citep{curationcorpusbase:2020} according to the
details described in~\citep{rae2021gopher}. We fine-tune all parameters of the
pre-trained GPT models and evaluate the final fine-tuning performance using the
official evaluation scripts. More details on the hyperparameters can be found in
Appendix A.

\subsection{Details on the Fine-tuning Datasets}

Our work uses four fine-tuning datasets to investigate the efficacy of our SPDF
framework. These datasets were chosen for studying the effect of sparse
pre-training on different sizes and types of data, along with the varying degree
of difficulty in the tasks.

\textbf{End-2-End (E2E) NLG challenge} dataset contains approximately 45k
training, 4.6k validation, and 4.6k test examples with 8 distinct fields from
the restaurant domain. The goal of the task is to generate natural language
descriptions in the restaurant domain from meaning representations. We use the
official evaluation script, which reports BLEU~\citep{kishore2002bleu},
NIST~\citep{belz2006comparing}, METEOR~\citep{alon2007meteor},
ROUGE-L~\citep{lin2004rouge}, and CIDEr~\citep{ramakrishna2015cider}.
% It contains multiple test references for one source table, and the average
% output length is 22.9.

\textbf{WebNLG} dataset consists of 18k training, 2.2k validation, and 2.4k test
examples, where the input is a sequence of (subject, property, object) triples.
In the training and validation splits, the input describes entities from 9
distinct DBpedia categories. The test set contains 15 different domains where 10
are present only in the training data. Here, the test data is split into two
parts, where categories seen in the train set are in the first half, while the
second half consists of 5 unseen categories. We use the official evaluation
script, which reports BLEU, METEOR and TER~\citep{snover2006study}. The WebNLG
dataset is the smallest of the three NLG tasks we evaluate on.

\textbf{DART} is an open domain DAta-Record-to-Text (i.e., table-to-text)
dataset, with a similar input format to WebNLG. It consists of 62.6k training,
6.9k validation, and 12.5k test examples from several sources:
WikiSQL~\citep{zhong2018seqsql},
WikiTableQuestions~\citep{pasupat2015compositional}, Cleaned
E2E\footnote{\url{https://github.com/tuetschek/e2e-cleaning}}, and WebNLG
2017\footnote{\url{https://gitlab.com/shimorina/webnlg-dataset/-/tree/master/webnlg_challenge_2017}}
and applies some manual or automated conversion. We use the official evaluation
script and report BLEU, METEOR and TER. The DART dataset is considered to be the
most challenging NLG task out of the three we evaluate.

\textbf{Curation Corpus} is a recently introduced dataset comprised of 40,000
bespoke text summaries of finance articles for the task of text summarization.
We follow the instructions in the Curation Corpus GitHub
repository\footnote{\url{https://github.com/CurationCorp/curation-corpus}} to
download approximately 40k article summary pairs. After filtering examples where
either the article or the summary are empty, we are left with 39,911 examples.
Following~\citet{marfurt2021sentence}, we split them into train/validation/test
sets as 80/10/10 to arrive at split sizes of 31,929/3,991/3,991.


\subsection{Sparse Fine-tuning vs Dense Fine-tuning} In this section, we first
empirically establish the need for dense fine-tuning to help mitigate the
difficulties of sparse-to-sparse training (i.e., sparse pre-training followed by
sparse fine-tuning). In Figure~\ref{fig:sparseft}, we compare dense fine-tuning
against sparse fine-tuning on GPT-2 Small and show that across all three NLG
tasks (i.e., E2E, WebNLG and DART), dense fine-tuning helps reduce the drop in
BLEU score relative to the respective dense baselines. For example, the 75\%
sparse GPT-2 Small model on WebNLG observes a delta of -1.48 and -0.78 in the
BLEU scores, when sparse fine-tuning and dense fine-tuning, respectively. This
suggests that fully sparse end-to-end pre-training and fine-tuning can prevent
the model from generalizing well on downstream tasks. However, we can mitigate
the difficulties of poor generalizability due to sparse-only training by
transitioning from sparse to dense matrices during the fine-tuning phase.
Although dense fine-tuning consumes more FLOPs compared to sparse fine-tuning,
the overall fine-tuning FLOPs relative to pre-training, still remains
insignificant  (discussed further in Section~\ref{sec:spdf_train_eff}). 

% We also note that even though the fine-tuning datasets are small, we perform
% early stopping to ensure dense fine-tuning does not overfit.

\begin{table}[!t]
    \caption{Downstream accuracy of GPT-2 Small and GPT-3 XL across various
    tasks (i.e., E2E, WebNLG, DART and Curation Corpus) at sparsity levels 50\%
    and 75\% during pre-training. In the metric column, the direction of the
    arrow indicates better result (e.g., up indicates higher is better).}
    \label{tab:alltasks}
    % \makebox[\linewidth]{
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc|ccc|c}
        \toprule
        \multirow{3}{*}{Model} &
        \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Pre-Train\\
        Sparsity\end{tabular}} & \multirow{2}{*}{E2E} & \multirow{2}{*}{WebNLG}
        & \multirow{2}{*}{DART} &
        \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Curation \\
        Corpus\end{tabular}}  \\ 
        & &                      &                         & & \\ \cmidrule{3-6}
        & & \multicolumn{3}{c|}{BLEU$\uparrow$} & PPL$\downarrow$ \\ \midrule
        \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}GPT-2 \\ Small\end{tabular}}
        & 0\% & 67.49\textsubscript{$\pm$0.60} & 63.42\textsubscript{$\pm$0.26}
        & 46.30\textsubscript{$\pm$0.16} & 13.38\textsubscript{$\pm$0.02} \\
                              & 50\% &      67.39\textsubscript{$\pm$0.38}     &  
                              63.10\textsubscript{$\pm$0.13}    &
                              45.74\textsubscript{$\pm$0.10}&
                              15.09\textsubscript{$\pm$0.04} \\
                              & 75\% &   66.50\textsubscript{$\pm$0.85}       &
                              62.64\textsubscript{$\pm$0.22}  &
                              44.97\textsubscript{$\pm$0.11} &
                              17.14\textsubscript{$\pm$0.01}            \\
    \midrule \multirow{3}{*}{GPT-3 XL} & 0\% & 68.10\textsubscript{$\pm$0.60} &
        63.62\textsubscript{$\pm$0.23} & 47.71\textsubscript{$\pm$0.11}&
        8.28\textsubscript{$\pm$0.01}   \\
                              & 50\% &    67.98\textsubscript{$\pm$0.63}       &
                              
                              63.47\textsubscript{$\pm$0.21}       &
                              47.10\textsubscript{$\pm$0.13} &
                              9.21\textsubscript{$\pm$0.02}            \\
                              & 75\% &     67.66\textsubscript{$\pm$0.59}      &
                              
                              63.06\textsubscript{$\pm$0.11}        &
                              46.96\textsubscript{$\pm$0.08}&
                              11.03\textsubscript{$\pm$0.02} \\
                              \bottomrule             
    \end{tabular}
    % }
    }
\end{table}

\subsection{SPDF on Natural Language Generation and Text Summarization} 

We perform an extended study on SPDF to further investigate its effectiveness on
a diverse set of fine-tuning tasks, when using sparse pre-trained GPT-2 Small
and GPT-3 XL models. In this section, we focus on natural language generation
(i.e., E2E, WebNLG, and DART) and text summarization (i.e., Curation Corpus)
tasks and refer to Table~\ref{tab:alltasks} for all the discussion points. We
note that in Appendix B, we provide evaluation scores on all the metrics used to
officially evaluate E2E, WebNLG and DART, respectively.

First, we validate Hypothesis~\ref{hyp:one} that high degrees of weight sparsity
can be induced during pre-training. Our results indicate that in most settings,
we can pre-train these GPT models with up to 75\% sparsity without significant
degradation across all NLG tasks. On the 75\% sparse GPT-3 XL model, we observe
deltas of -0.44, -0.56, and -0.75 in the BLEU scores for E2E, WebNLG and DART,
respectively. In addition, the 50\% sparse GPT-2 Small model observes deltas of
-0.10, -0.32, and -0.56 in the BLEU scores for E2E, WebNLG and DART,
respectively. Overall, our findings show that these GPT models can be
pre-trained with 50\%-75\% sparsity without losing significant accuracy on these
downstream tasks.

Second, we validate Hypothesis~\ref{hyp:two} that the performance of the sparse
pre-trained model is correlated with the difficulty of the fine-tuning task.
E2E, WebNLG and DART are NLG tasks which focus on mapping structured data
content to a text describing this content. The Curation Corpus task focuses on
summarizing the text description. While both tasks involve generating
semantically coherent natural language, the summarization tasks are more
difficult, since it require understanding of long sequences and compressing the
sequence without loss of information. On the E2E, WebNLG and DART tasks, GPT-3
XL can be pre-trained up to 75\% sparsity without a significant drop in BLEU
score, as discussed previously. In contrast, on Curation Corpus, GPT-3 XL
pre-trained at 75\% sparsity loses 2.75 perplexity. In general, all data-to-text
NLG tasks obtain a lower degradation compared to the more difficult Curation
Corpus summarization task at higher levels of sparsity.

Finally, we validate Hypothesis~\ref{hyp:three} that as the size of the model
increases, it becomes more amenable to higher sparsity levels. We analyze the
relative drop in performance between the dense baseline and its sparse variants
for GPT-2 Small and GPT-3 XL. This trend is clearly evident on the more
difficult Curation Corpus task at 75\% sparsity, where relative to the dense
baseline, the larger GPT-3 XL model has a perplexity delta of +2.75 compared to
a worse +3.76 delta observed in the smaller GPT-2 Small model. Similarly, on the
DART task, the most challenging NLG task out of the three we evaluated, the
delta in the BLEU score is -1.33 and -0.75 for GPT-2 Small and GPT-3 XL,
respectively. These observations indicate that as the size of the language model
increases, it suffers less on downstream task performance when training with
high sparsity.

\subsection{Pre-training vs Fine-tuning Parameter Subspaces}
In this section we analyze the parameter subspaces of the pre-trained model and
its fine-tuned parameters  across all layers to further understand (a) the
behaviour of dense and spare pre-trained representations when fine-tuned, and
(b) the effect of scaling the model size on parameter subspaces between the two
phases. Inspired by~\citet{RadiyaDixit2020HowFC}, we measure the angular
distance (i.e., cosine distance) between the pre-trained model parameters and
its fine-tuned parameters on a given downstream task. Specifically, in all
layers of the language model, we inspect the four weight matrices in the
self-attention module; $W_Q$ (query), $W_K$ (key), $W_V$ (value) and $W_D$
(attention output projection) and the two in the MLP module; $W_I$
(intermediate) and $W_O$ (MLP output projection). In this analysis we focus on
DART, the most difficult NLG task, and report the cosine distances for all
modules in each layer of the dense and 75\% sparse pre-trained GPT-2 Small and
GPT-3 XL. 

First, we aim to understand the behaviour of the parameter subspaces of the
dense and sparse pre-trained models when fine-tuned. In GPT-2 Small (see
Figure~\ref{fig:gpt2_dart_subspaces}) and GPT-3 XL (see
Figure~\ref{fig:gpt3xl_dart_subspaces}), we observe that the dense pre-trained
parameters and its fine-tuned parameters have very small cosine distances in
almost all modules across each layer, whereas the 75\% sparse model has larger
cosine distances in certain modules (e.g., $W_D$ and $W_O$) across all layers.
Here, the dense model's fine-tuned parameters require less change in the
parameter subspace relative to the pre-trained parameters, while the sparse
model requires more movement in certain modules to learn the downstream task.
This indicates that pre-trained models which learn high quality textual
representations need less movement in the parameter subpsace to adapt to the
downstream task. Although the sparse model has less representational capacity in
its pre-trained parameters, it is capable of adapting certain modules through
dense fine-tuning to learn the downstream task and stay competitive with the
dense model's performance.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.78\linewidth}
       \includegraphics[keepaspectratio=true, width=1\linewidth]{./figures/gpt2small_dart_cos_0.00_all.pdf}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.78\linewidth}
       \includegraphics[keepaspectratio=true, width=1\linewidth]{./figures/gpt2small_dart_cos_0.75_all.pdf}
    \end{subfigure}
    
    \caption[GPT-2 Small Pre-trained vs. DART Weights]{The angular distances in
    parameter subspaces between dense (top) and 75\% sparse (bottom) pre-trained
    and fine-tuned DART weights for GPT-2 Small.}
    \label{fig:gpt2_dart_subspaces} 
\end{figure}

\begin{figure}[!t]
    \centering
    \makebox[\linewidth]{
    \begin{subfigure}[b]{\linewidth}
       \includegraphics[width=1\linewidth]{./figures/gpt3xl_dart_cos_0.00_all.pdf}
       
    \end{subfigure}
    }
    \begin{subfigure}[b]{\linewidth}
       \includegraphics[width=1\linewidth]{./figures/gpt3xl_dart_cos_0.75_all.pdf}
    \end{subfigure}
    
    \caption[GPT-3 XL Pre-trained vs. DART Weights]{The angular distances in
    parameter subspaces between dense (top) and 75\% sparse (bottom) pre-trained
    and fine-tuned DART weights for GPT-3 XL.}
    \label{fig:gpt3xl_dart_subspaces} 
    \vspace{-0.1in}
\end{figure}


Next, we study the effect of model size and the parameter subspaces of the
pre-trained and fine-tuned parameters. Evidently, in
Figure~\ref{fig:gpt3xl_dart_subspaces}, we observe that the dense pre-trained
GPT-3 XL model has very small cosine distances across all modules in almost each
layer, in comparison to GPT-2 Small. This suggests that as we increase the
modeling capacity of the language model, only a few model parameter updates
traverse a very short distance in the parameter space. This results in the
pre-trained and fine-tuned weights being highly close across all modules in
almost each layer. The larger language model is more capable of learning high
quality representations, thus requires less movement in the fine-tuning
parameter subspace. Even at 75\% sparsity, the GPT-3 XL model requires
significantly less change to the pre-trained parameters compared to GPT-2 Small
in order to perform competitively well with the dense model. Given that many
layers experience a very small change in the parameter subspace, we leave the
investigation of freezing these modules during the fine-tuning phase for future
work.

\begin{table*}[!ht]
    \caption{Total FLOPs along with the associated theoretical speedup~w.r.t the
    dense baseline (in brackets) for each of the evaluated fine-tuning tasks on
    GPT-2 Small and GPT-3 XL.  The reported training FLOPs includes both
    pre-training and dense fine-tuning FLOPs. GPT-3 XL 75\% SPDF provides
    $\approx$ 2.5x FLOP reduction over end-to-end dense training.}
    \label{tab:flops_gpt2_gpt3xl}
    % \makebox[\linewidth]{
    \centering \resizebox{0.78\linewidth}{!}{
    \begin{tabular}{cc|cccc}
        \toprule
    \multirow{2}{*}{Model} &
                           \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Pre-Train\\
                           Sparsity\end{tabular}} &
                           \multicolumn{4}{c}{Pre-training + Fine-tuning FLOPs
                           ($\times 10^{18}$)}         \\ \cmidrule{3-6} & & E2E
                           & WebNLG       & DART         & Curation Corpus \\
                           \midrule \multirow{3}{*}{GPT-2 Small}           & 0\%
                           & 2.48 (1.00x) & 2.48 (1.00x) & 2.45 (1.00x) & 2.44
                           (1.00x)  \\
                                      & 50\% & 1.84 (1.34x) & 1.82 (1.35x) &
                           1.84 (1.34x) & 1.81 (1.35x) \\
                & 75\% & 1.52 (1.64x)       & 1.49 (1.65x)       & 1.52 (1.64x)
                & 1.48 (1.65x) \\ 
    \midrule \multirow{3}{*}{GPT-3 XL}           & 0\% & 236.62 (1.00x) & 236.62
    (1.00x) & 236.33 (1.00x) & 236.32 (1.00x) \\
               & 50\% & 142.40 (1.66x)       & 142.10 (1.66x) & 142.01 (1.66x) &
               142.40 (1.66x) \\
            & 75\% & 95.29 (2.48x)       & 94.98 (2.49x)        & 95.29 (2.48x)
            & 94.90 (2.49x) \\
\bottomrule      
    \end{tabular}
    }
    \vspace{-0.1in}
\end{table*}

\subsection{SPDF Training Efficiency}
\label{sec:spdf_train_eff}
We compare the standard dense pre-training followed by dense fine-tuning
framework to SPDF and highlight the potential FLOP reduction we can achieve. In
Table~\ref{tab:flops_gpt2_gpt3xl}, we report the total FLOPs (i.e., both the
forward and backward propagations) needed for pre-training and dense fine-tuning
GPT-2 Small and GPT-3 XL models on each of the tasks we evaluated. We note that
in the GPT-2 Small model, the percentage of attention and vocab embeddings FLOPs
account for approximately 13.3\% and 27\% of the total FLOPs, respectively.
Therefore, at 75\%, we achieve approximately 1.65x FLOP reduction over the dense
baseline. However, in the larger GPT-3 XL, the percentage of attention and vocab
embeddings FLOPs account for 13.3\% and 6.8\%, respectively. As a result, at the
GPT-3 XL scale, SPDF provides almost 2.5x FLOP reduction over the dense baseline
when pre-training with 75\% sparsity. The trend of FLOP reduction relative to
the dense baseline continues to increase with larger models, so the potential
gains from sparse pre-training improves as model size grows. We also emphasize
that the total fine-tuning FLOPs is a small fraction of the total pre-training
FLOPs. In Appendix A.4, we provide details on how the total pre-training and
fine-tuning FLOPs for GPT-2 Small and GPT-3 XL were calculated.
