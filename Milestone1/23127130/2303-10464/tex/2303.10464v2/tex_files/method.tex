\section{Methodology}
\label{sec:method}

This section presents our method to reduce pre-training FLOPs using unstructured
weight sparsity. We first explain our intuition and hypotheses, followed by our
methodology for the SPDF framework.

\subsection{Intuition and Hypotheses}
\label{sec:hypotheses}
Prior works have shown that overparameterization of neural networks improves
optimization and generalizability~\citep{Soltanolkotabi2019, neyshabur2018the,
pmlr-v97-allen-zhu19a}, but leads to an increase in compute
cost~\citep{brown2020gpt3}. Recent work on the Lottery Ticket
Hypothesis~\cite{frankle2018lottery} demonstrates that overparameterized dense
networks contain sparse subnetworks which can be trained to the same accuracy as
their dense counterparts, as long as one initializes the training with a good
sparsity mask (``lottery ticket''). However, the process of searching for highly
quality sparse subnetworks is computationally
expensive~\citep{frankle2018lottery, ma2022effective}. Existing sparse training
methods~\citep{evci2020rigl, mocanu2018, jayakumar2020top} aim to discover the
winning lottery ticket (i.e., optimal sparsity mask) in a single training run,
but often fall short of the dense model's accuracy. In our framework, we
mitigate the loss in representational power due to difficulties in sparse
optimization~\citep{Evci2019TheDO}, by transitioning to fully dense weight
matrices during the fine-tuning phase. Even though we perform dense fine-tuning,
the computational costs associated with fine-tuning are significantly lower than
the cost of pre-training LLMs. Therefore, our method targets the phase which
dominates the training FLOPs (i.e., pre-training). Based on recent theoretical
findings and empirical studies on overparameterization and sparse neural
networks, we lay out a set of hypotheses which we aim to study in our work
through extensive experimental evaluation:

\paragraph*{\normalfont\textit{Hypothesis 1\labeltext{1}{hyp:one}: High degrees
of weight sparsity can be used during the pre-training phase of LLMs while
preserving the downstream accuracy with dense fine-tuning.}} \mbox{}

Inducing sparsity during pre-training may cause a loss in representational power
due to difficulties in sparse optimization and inability to discover optimal
sparsity masks~\citep{Evci2019TheDO}. To mitigate these challenges, we aim to
increase the representational power by allowing the zeroed weights to grow
during fine-tuning (i.e., dense fine-tuning). Additionally, note the full
capacity of the pre-trained model is often not required to generalize on the
simpler downstream task, when using sparsity during
pre-training~\citep{Ding2022DeltaTA}. \citet{aghajanyan-etal-2021-intrinsic}
investigate this phenomenon from a different angle and show pre-trained language
models can learn a large set of NLP tasks with only a few parameters.
% They show that the pre-training minimizes the intrinsic
% dimension~\citep{li2018measuring} when later fine-tuning various downstream
% tasks. 
This indicates that the full parameterization of the model is not needed to
generalize well across downstream fine-tuning tasks. Hence, we can exploit
weight sparsity during pre-training while retaining important textual
representations despite the model's lower representational capacity.

\paragraph*{\normalfont \textit{Hypothesis 2\labeltext{2}{hyp:two}: The
performance of the sparse pre-trained model is correlated with the dataset size
and degree of difficulty in the downstream task.}} \label{hyp:two} \mbox{}

\citet{liu2023sparsity} evaluate sparse networks on a diverse set of tasks with
varying degrees of difficulty and show a strong correlation between a model's
ability to be sparsified and the task difficulty. Hence, we hypothesize that
models trained on complex tasks with high sparsity levels can suffer more from
sparse training and experience a greater drop in performance compared to simpler
tasks. We also note that small fine-tuning datasets may trigger
over-fitting~\citep{Li2021ImprovedRA}. Therefore, we hypothesize that larger
datasets can allow the sparse model to improve its generalization error on the
task, and recover from training with high sparsity.

\paragraph*{\normalfont\textit{Hypothesis 3\labeltext{3}{hyp:three}: As we
 increase the size of the language model, larger models become more amenable to
 higher levels of sparsity during pre-training.}} \label{hyp:three} \mbox{}

Existing work~\citep{liu2022the} has shown that the quality of a network trained
with random static sparsity (even at high sparsity levels) improves quickly to
match its dense counterpart as the network grows wider and deeper. Also, larger
models tend to have a smaller intrinsic
dimension~\citep{aghajanyan-etal-2021-intrinsic}, which suggests that all
parameters are not required to represent the average NLP task. Therefore, we
expect the gap in downstream performance between the sparse pre-trained model
and its dense counterpart to grow smaller as the size of the model increases.  

\subsection{Sparse Pre-training and Dense Fine-tuning}
Our training procedure consists of two phases. The first phase involves
pre-training a sparse language model on a large corpus of text in an
unsupervised manner. Here, we induce unstructured weight sparsity into the
neural network to reduce the pre-training FLOPs. This is followed by a dense
fine-tuning stage, where we expand the representational capacity of the model by
allowing zeroed weights to learn, and adapt to a discriminative task with
labeled data.

\paragraph*{Unsupervised Dense Pre-training} While our proposed framework is
agnostic to the training objective, we focus on autoregressive language modeling
as our motivating use case. In an autoregressive language model, the sequence
generation process is modeled as a Markov chain, where the token to be predicted
depends on all the previous tokens~\citep{bengio2003pretrain}. Hence, the
standard approach is to learn the probability distribution over sequences of
tokens from an unsupervised pre-training corpus. Given an unsupervised
pre-training corpus of tokens~$\mathcal{U} = \{u_1,
u_2,\ldots,u_{|\mathcal{U}|}\}$,  where $|\mathcal{U}|$ is the total number of
tokens. We aim to maximize the likelihood using the language modeling objective
formulated as follows,
\begin{equation}
    \label{eq:gpt}
    \mathcal{L}(\mathcal{U}) = \sum_{i=1}^{|\mathcal{U}|}\log(p(u_i | u_{i-k},\ldots,u_{i-1}, \theta)),\notag
\end{equation}

where $k$ is the size of the context window, and the conditional probability $p$
is modeled using a neural network with parameters $\theta \in \mathbb{R}^{N}$.
The parameters of the $l^{th}$ layer $\in L$ total layers are denoted as
$\theta_l$, along with the total number of parameters represented as $N_l$. We
note that the network parameters $\theta$ are considered to be dense.

\paragraph*{Unsupervised Sparse Pre-training} To induce sparsity into the
$l^{th}$ layer, we drop $s_l \in (0, 1)$ of its connections, where $s_l$ to
refer to the sparsity of layer $l$. This results in a total of $(1-s_l)N_l$
parameters. Finally, the overall sparsity of a sparse subnetwork is defined as
the ratio of zeroes to the total number of parameters in the original dense
network, i.e., $S = \frac{\sum_l^L s_lN_l}{N}$. In our sparse training setup, we
apply a binary sparsity mask $m \in \{0,1\}^{|\theta|}$ on the initial
parameters $\theta^0$, such that its initialization is $m \odot \theta^{0}$.
Here, the values 0 and 1 in the mask denote inactive (i.e., zero) and active
(i.e., non-zero) weights, respectively. As a result, the sparse language model
minimizes the following objective instead,
\begin{equation}
    \label{eq:gpt}
    \mathcal{L}(\mathcal{U}) = \sum_{i=1}^{|\mathcal{U}|}\log(p(u_i | u_{i-k},\ldots,u_{i-1}, m \odot \theta)).
\end{equation}

In our work, we focus solely on static sparsity (i.e., $m$ remains fixed
throughout training) and the weights are randomly pruned at initialization.
Specifically, we remove weights in each layer $l \in L$ randomly to the target
sparsity $s_l$. Although several works have explored generating different
layer-wise sparsity ratios at initialization (e.g.,
Erd\"os-R\'enyi-Kernel~\cite{evci2020rigl}, Ideal Gas
Quota~\citep{chen2022sparsity}, SNIP~\citep{lee2018snip},
GraSP~\citep{Wang2020Picking}, SynFlow~\citep{tanaka2020synflow}, etc.), we
focus on the simplest setup, which is uniform sparsity~\citep{gale2019state}. In
uniform sparsity, each sparsified layer is pruned to the same target sparsity
level. 

For the language model, we use GPT~\citep{radfordGPT2, brown2020gpt3} in our
experiments, which is a variant of the
Transformer~\citep{vaswani2017transformers}. We train the network with objective
shown in Eq.~\ref{eq:gpt} and AdamW~\citep{loshchilov2017decoupled} optimizer on
an unsupervised pre-training dataset for a total of $j$ iterations, arriving at
parameters $\theta^j$. Then, we adapt (i.e., fine-tune) the final pre-trained
autoregressive language model $p_{m \odot \theta}$ to the supervised target
task.

\begin{figure}[!t]
    \centering
    \includegraphics[keepaspectratio=true, width=\linewidth]{./figures/spdf_framework.pdf}
    \caption{Sparse Pre-training and Dense Fine-tuning (SPDF) framework. In this
    framework, we sparsify a dense network and perform sparse pre-training
    followed by dense fine-tuning (green connections indicate newly activated
    weights). We use SPDF to pre-train large GPT models at a fraction of the
    training FLOPs using weight sparsity, and still retain the benefits on
    downstream tasks with dense fine-tuning.  }
    \label{fig:spdf}
    \vspace{-0.1in}
\end{figure}

\paragraph*{Dense Fine-tuning} 
Following~\citet{hu2022lora} and~\citet{Li2021PrefixTuningOC}, each downstream
fine-tuning task is represented by a training dataset consisting of
context-target pairs defined as: $\mathcal{Z} = \{(x_1 , y_1),(x_2 ,
y_2),\ldots,(x_{|x|}, y_{|y|})\}$, where both $x$ and $y$ are sequences of
tokens. For example, in structured data-to-text (e.g.,
E2E~\citep{novikona2017e2e}), $x$ corresponds to a linearized data table and $y$
a textual description; in text summarization (e.g., Curation
Corpus~\citep{curationcorpusbase:2020}), $x$ is the content of an article and
$y$ is its summary. 

We initialize the start of dense fine-tuning to the final pre-trained parameters
$\theta^j$ and during fine-tuning are updated to $\theta^j + \Delta\theta$. For
each downstream task,  we learn a different set of parameters with the
task-specific parameter increment $\Delta\theta$ whose dimension
$|\Delta\theta|$ equals $|\theta|$. Other works have explored more parameter
efficient approaches to reduce the size of the task-specific parameters for the
purpose of deploying fine-tuned models~\citep{ben-zaken-etal-2022-bitfit,
pmlr-v97-houlsby19a,hu2022lora}.  However, in our approach, we focus on reducing
the pre-training FLOPs with unstructured weight sparsity and perform dense
fine-tuning to mitigate the challenges of sparse optimization by increasing
representational power of the network. In the dense fine-tuning phase, we
essentially remove the sparsity mask $m$ to allow the inactive weights to grow.
More specifically, we increase the representational capacity in $\theta^j$ by
reviving all $\sum_{l}^{L}s_l {\cdot} N_l$ inactive weights, where all newly
activated weights are initialized to 0. We evaluated other initializations like
scaled normal distribution, but this did not lead to better results. Finally,
the network is updated in a dense manner with the objective shown below,
\begin{equation}
    \mathcal{L}(\mathcal{Z}) = \sum_{(x,y)\in\mathcal{Z}}\sum_{t=1}^{|\boldsymbol{y}|}\log(p(y_t | (x_{1},\ldots,x_{t-1}), \theta^j + \Delta\theta)).\notag
\end{equation}

The generic Sparse Pre-training and Dense Fine-Tuning (SPDF) framework,
illustrated in Figure~\ref{fig:spdf}, consists of the following three steps:
\begin{enumerate}
    \item Sparsify a given dense network to some target sparsity level, $s_l$,
    at each sparsifiable layer.
    \item \textit{Pre-train} the sparse model following the same training
    schedule as the original dense model.
    \item \textit{Fine-tune} the pre-trained sparse network on a given
    downstream task in a dense manner by allowing the zeroed weights to learn.
\end{enumerate}
