\section{Related Work}

\paragraph{Zero-Shot vs. Fine-tuning}
% The majority of fine-tuning literature has been established on \textit{dense
% fine-tuning} of deep learning models to specific downstream tasks. Fine-tuning
% a pre-trained language model has become the de facto standard for doing
% transfer learning~\citep{sanh2021multitask, ouyang2022training}. The empirical
% success of these methods has led to the development of even larger language
% models~\citep{smith2022using, chowdhery2022palm}. 
Recent works have shown that large language models can achieve reasonable
performance without any parameter
updates~\citep{brown2020gpt3,chowdhery2022palm,rae2021gopher, smith2022using},
often referred to as the zero-shot or few-shot setting. When no parameters are
fine-tuned, framing a target task in terms of the pre-training objective enables
zero-shot or few-shot learning to use a task-specific prompt and a few examples
of a task~\citep{brown2020gpt3}. However, while such few-shot learning is simple
using such large models, there are alternative methods to obtain similar task
accuracy using smaller models~\citep{schick2021smallllm}. In recent
work,~\citet{lamda2022} demonstrate that while scaling the size of LaMDA can
improve quality, combining scaling with fine-tuning can improve the model across
all metrics including quality, safety and groundness.~\citet{irene2021} show
that fine-tuning also helps update language model behaviour to mitigate harmful
outputs, which is highly critical for real-world deployment of LLMs (e.g.,
ChatGPT~\citep{openai_chatgpt_2022}, Bard~\citep{pichai_2023}, etc.). To achieve
the best performance in practice, fine-tuning will continue to be the modus
operandi when using pre-trained LLMs. Hence, our work focuses on pre-training
and fine-tuning language models across a diverse set of tasks, including natural
language generation and text summarization.

\paragraph*{Efficient Fine-tuning} While most large-scale models such as
GPT~\citep{brown2020gpt3, smith2022using} or T5~\citep{raffel2022transfer} are
trained dense, there are works~\citep{houlsby2019parameter, li2021prefix,
zaken2021bitfit, hu2022lora} that explore using limited capacity (tuning a few
layers or subset of parameters) in the pre-trained models to fine-tune on
downstream tasks. These works are indicative that the total modeling capacity is
unnecessary for fine-tuning on downstream tasks. Our work draws some inspiration
from these works for exploiting the limited capacity of models for final tasks.
However, we choose to reduce FLOPs for pre-training (significantly more training
FLOPs than fine-tuning) and then add all the modeling capacity back during
fine-tuning. This allows us to train large models efficiently and yet retain
accuracies comparable to dense baselines. Although we do not explore efficient
fine-tuning in our study, we leave the exploration of using alternative sparsity
schedules~\citep{zhu2018toprune, liu2021sparsegranet}, adapting a subset of
parameters during fine-tuning~\citep{Ding2022DeltaTA} and imposing low-rank
structures~\citep{hu2022lora} for future work.

\paragraph{Weight Sparsification Techniques}
Many unstructured weight sparsification techniques have been proposed in the
literature for training neural networks~\citep{hoefler2022sparsity}, which can
be categorized as static sparsity and dynamic sparsity. Static sparsity methods
have a fixed sparsity structure (i.e., sparsity mask) determined at
initialization~\citep{lee2018snip,Wang2020Picking}. In contrast, dynamic sparse
training (DST) methods iteratively prune (drop) and add (regrow) weights during
training~\citep{mocanu2018, evci2020rigl, jayakumar2020top,
shaoyi2022betterrigl} to find the best possible sparse subnetwork while
retaining accuracy comparable to dense baselines. Although, dynamic sparse
training methods can help achieve Pareto improvements in terms of number of
training FLOPs to accuracy, we leave this for future work. Inspired
by~\citep{li2022the}, which shows that scaling the size of CNNs closes the gap
between a randomly pruned sparse network and its dense counterpart, we focus our
study on language models with static sparsity. 
% In addition, most static sparse methods maintain a constant modeling capacity
% throughout training, but, in our work, we increase the capacity by
% pre-training sparse models and fine-tuning dense. 
While~\citet{dao2022monarch} demonstrate the benefits of sparse-to-dense
training, they mainly apply it during pre-training and instead, focus their
studies on dense-to-sparse fine-tuning similar to other efficient fine-tuning
efforts. In our work, we show that sparse pre-training followed by dense
fine-tuning on downstream tasks can be on par with the accuracy of a dense
pre-trained model on many tasks, while significantly lowering overall training
FLOPS.
