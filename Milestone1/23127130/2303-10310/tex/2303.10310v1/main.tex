% Template for producing ESWA-format journal articles using LaTeX    
% Written by Miha Ravber                
% Programming methodologies laboratory                    
% Faculty of Electrical Engineering and Computer Science 
% University of Maribor                              
% Koroška cesta 46, 2000 Maribor                                       
% E-mail: miha.ravber@um.si                           
% WWW: https://lpm.feri.um.si/en/members/ravber/    
% Created: November 20, 2020 by Miha Ravber                                          
% Modified: November 20, 2020 by Miha Ravber                     
% Use at your own risk :) 
% Please submit your issues on the github page: https://github.com/Ravby/eswa-template


\documentclass[review]{elsarticle}
\graphicspath{ {./figures/} }
\usepackage{hyperref}
\usepackage{float}
\usepackage{verbatim} %comments
\usepackage{apalike}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage[table,xcdraw]{xcolor}
\usepackage{multirow}
\usepackage{array}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{textcomp}

\usepackage[export]{adjustbox}
\usepackage{amsfonts}
% \usepackage{algpseudocode}
% \usepackage{algorithm2e  }


% \usepackage{algpseudocode}
\usepackage{algorithmic}
\usepackage[ruled,vlined,linesnumbered,boxruled]{algorithm2e}
\usepackage{setspace}

\restylefloat{figure}
\restylefloat{table}

\journal{Engineering Applications of Artificial Intelligence}

%% For ESWA journal you need to use APA style
\bibliographystyle{model5-names}\biboptions{authoryear}

\begin{document}
\begin{frontmatter}


\begin{titlepage}
\begin{center}
\vspace*{1cm}

\textbf{ \large Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks}

\vspace{0.5cm}

% Author names and affiliations
Firas Al-Hindawi$^{a}$ (falhinda@asu.edu), Md Mahfuzur Rahman Siddiquee$^a$ (mrahmans@asu.edu), Teresa Wu$^a$ (teresa.wu@asu.edu), Han Hu$^b$ (hanhu@uark.edu),  Ying Sun$^c$ (sunyg@ucmail.uc.edu) \\

% \hspace{10pt}

\section*{Highlights}
% \item Research highlight 1
% \item Research highlight 2
\begin{itemize}

\item New metric for evaluating UI2I translation models using pseudo supervised metrics that is designed specifically to support cross-domain classification.

% \item Gaussian Mixture Model is utilized to generate pseudo labels to enable the use of standard supervised metrics. 

\item The metric not only outperforms unsupervised metrics such as the FID, but is also highly correlated with the true supervised metrics, robust, and explainable.

\item We demonstrate that this metric can be used for future research in this field by applying it to a critical real-world problem (the boiling crisis problem).

\end{itemize}




\begin{flushleft}
\small  
% $^a$ Full address of first author, including the country name \\
% $^b$ Full address of second author, including the country name \\
% $^c$ Full address of last author, including the country name
$^a$ Arizona State University, 699 Mill Avenue, Tempe, AZ 85281, US\\
$^b$ University of Arkansas, 1 University of Arkansas, Fayetteville, AR 72701, US\\
$^c$ University of Cincinnati, 2600 Clifton Ave, Cincinnati, OH 45221, US\\





\begin{comment}
Clearly indicate who will handle correspondence at all stages of refereeing and publication, also post-publication. Ensure that phone numbers (with country and area code) are provided in addition to the e-mail address and the complete postal address. Contact details must be kept up to date by the corresponding author.
\end{comment}

\vspace{1cm}




\textbf{Corresponding Author:} \\
Firas Al-Hindawi \\
Arizona State University, 699 Mill Avenue, Tempe, AZ 85281, US \\
Tel: +1 (602) 837-9820 \\
Email: falhinda@asu.edu

\end{flushleft}        
\end{center}
\end{titlepage}

\title{Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks}

\author[label1]{Firas Al-Hindawi \corref{cor1}}
\ead{falhinda@asu.edu}

\author[label1]{Md Mahfuzur {Rahman Siddiquee}}
    \ead{mrahmans@asu.edu}

\author[label1]{Teresa Wu}
    \ead{teresa.wu@asu.edu}

\author[label2]{Han Hu}
    \ead{hanhu@uark.edu}
    
\author[label3]{Ying Sun}
    \ead{sunyg@ucmail.uc.edu}

\cortext[cor1]{Corresponding author.}
\address[label1]{Arizona State University, 699 Mill Avenue, Tempe, AZ 85281, US}
\address[label2]{University of Arkansas, 1 University of Arkansa, Fayetteville, AR 72701, US}
\address[label3]{University of Cincinnati, 2600 Clifton Ave, Cincinnati, OH 45221, US}



\begin{abstract}
The ability to classify images accurately and efficiently is dependent on having access to large labeled datasets and testing on data from the same domain that the model is trained on. Classification becomes more challenging when dealing with new data from a different domain, where collecting a large labeled dataset and training a new classifier from scratch is time-consuming, expensive, and sometimes infeasible or impossible. Cross-domain classification frameworks were developed to handle this data domain shift problem by utilizing unsupervised image-to-image (UI2I) translation models to translate an input image from the unlabeled domain to the labeled domain. The problem with these unsupervised models lies in their unsupervised nature. For lack of annotations, it is not possible to use the traditional supervised metrics to evaluate these translation models to pick the best-saved checkpoint model. In this paper, we introduce a new method called Pseudo Supervised Metrics that was designed specifically to support cross-domain classification applications contrary to other typically used metrics such as the FID which was designed to evaluate the model in terms of the quality of the generated image from a human-eye perspective. We show that our metric not only outperforms unsupervised metrics such as the FID, but is also highly correlated with the true supervised metrics, robust, and explainable. Furthermore, we demonstrate that it can be used as a standard metric for future research in this field by applying it to a critical real-world problem (the boiling crisis problem).
\end{abstract}


\begin{keyword}
Unsupervised Metrics \sep  Cross-Domain Classification \sep Critical Heat Flux \sep Domain Adaptation \sep Generative Adversarial Networks \sep Image-to-Image Translation \sep Pool Boiling \sep Unsupervised Machine Learning.
\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{introduction}

Classification algorithms have been a rapidly growing field in recent years, particularly with the significant advancements in deep learning and computer vision technologies. These advancements enabled classification algorithms to become highly efficient and accurate, making them applicable to a wide range of applications in various domains \cite{alhindawi2018predicting, altarazi2019machine, rassoulinejad-mousavi2021a, padmapriya2023deep, omeroglu2023novel}. The success of most classification models is contingent on 1) having access to large, labeled datasets that allow for the training of models with high accuracy and 2) that the model is applied (tested) on data coming from the same domain that the model is trained on and is familiar with. The limitations of these classification models become evident when dealing with new data from a different domain. In such cases, collecting a large labeled dataset and training a new classifier from scratch can be time-consuming, expensive, and in some situations infeasible or impossible. This motivated the need for a methodology or framework to overcome these limitations. The branch of machine learning that generally deals with the data domain shift problem in an unsupervised manner is called unsupervised domain adaptation (UDA). The specific classification problem described earlier falls under the UDA umbrella, and is generally referred to in the field of machine learning as unsupervised cross-domain classification, which is the problem of training a classifier on a dataset from one domain and using it to predict a dataset from a different domain without the labeling information. 

% Researchers used different methodologies and frameworks to tackle the cross-domain classification problem. Tzeng et al. \cite{tzeng2017adversarial} introduced Adversarial Discriminative Domain Adaptation (ADDA), where they adapted the features learned from the source domain to the target domain by aligning their distributions. Ghifary et al. \cite{ghifary2015domain} proposed Multi-Task Autoencoder (MTAE), which is a feature learning algorithm that provides good generalization performance for cross-domain object recognition. Instead of reconstructing images from noisy versions, MTAE learns to transform the original image into analogs in multiple related domains. It thereby learns features that are robust to variations across domains. The learned features are then used as inputs to a classifier. Zhu et al. \cite{zhu2019multi} proposed Multi-Representation Adaptation (MRAN) which specially aims to align the distributions of multiple representations which are extracted by a hybrid structure named Inception Adaptation Module (IAM) to capture the information from different aspects. The previous methods handle the domain adaptation of images by transforming them into different representations which might be needed when dealing with domains with big shifts between them, but in the case of domains with small shifts, the lost spatial information during transformation might affect the results negatively. 

Researchers used different methodologies and frameworks to tackle the cross-domain classification problem. \cite{tzeng2017adversarial} introduced Adversarial Discriminative Domain Adaptation (ADDA), where they adapted the features learned from the source domain to the target domain by aligning their distributions. \cite{ghifary2015domain} proposed Multi-Task Autoencoder (MTAE), which is a feature learning algorithm that provides good generalization performance for cross-domain object recognition. Instead of reconstructing images from noisy versions, MTAE learns to transform the original image into analogs in multiple related domains. It thereby learns features that are robust to variations across domains. The learned features are then used as inputs to a classifier.  \cite{zhu2019multi} proposed Multi-Representation Adaptation (MRAN) which specially aims to align the distributions of multiple representations which are extracted by a hybrid structure named Inception Adaptation Module (IAM) to capture the information from different aspects. The previous methods handle the domain adaptation of images by transforming them into different representations which might be needed when dealing with domains with big shifts between them, but in the case of domains with small shifts, the lost spatial information during transformation might affect the results negatively. 

Recently, with the rise of Generative Adversarial Networks (GANs) and Unsupervised Image to Image (UI2I) translation models and their applications to solve computer vision problems such as detection \cite{ben2019cross,zhong2023deeper} and data augmentation \cite{zhu2021novel,yun2023gan}, researchers started investigating their utility in solving the cross-domain classification problem. Image-to-image translation models, in general, are used to convert an input image from one domain (e.g., horse images) to another domain (e.g., zebra images) using a generative model. The main difference between the supervised and unsupervised models is that supervised models are typically trained using a dataset of paired images, where each pair consists of an image in the source domain and its corresponding image in the target domain, while their unsupervised counterpart does not require such constraint. Once trained, I2I translation models can be used to transform an input image from the source domain to the target domain by inputting the image into the model and generating a synthetic output image in the target domain. This ability of I2I translation models inspired a variety of innovative and useful applications such as image colorization \cite{https://doi.org/10.48550/arxiv.1603.08511}, image style transfer \cite{https://doi.org/10.48550/arxiv.1703.06868}, data augmentation \cite{https://doi.org/10.48550/arxiv.1711.04340}, and most recently cross-domain classification. 
\cite{li2021cross} proposed a cross-domain sentiment classification framework based on GANs with the assistance of the attention mechanism to leverage the information available from the source domain to the target domain. \cite{https://doi.org/10.48550/arxiv.2212.09107} developed a framework to support unsupervised cross-domain classification using UI2I translation models. The framework consists of two parts. The first part utilized a typical classification model that is trained and tested on the source dataset, while the second part utilized an Unsupervised Image-to-Image (UI2I) translation model to transform images in the target dataset to look as if they were obtained from the same domain of the source dataset that the classification model is familiar with. This expanded the capability of the classification model that was trained and tested on a dataset from a specific domain and enabled it to classify images from a different foreign domain previously unseen by the classifier.

One of the main challenges in such unsupervised frameworks is knowing which UI2I model to select/save during training (when to stop the training). If the model was not trained long enough, it will underfit the training data and produce poor results. In contrast, if the model was overtrained, it will overfit the training data and generate poor results as well. Because of the unsupervised nature of UI2I models, the standard supervised metrics that are used to validate supervised models during training (e.g., accuracy, AUC) are not available to be used without access to the labeling information. To overcome this issue, such frameworks depend on one of the most used metrics in evaluating unsupervised image-to-image translation models, the Frechet Inception Distance (FID) \cite{heusel2017a}. FID is a popular metric for evaluating the quality of generated images in the context of Generative Adversarial Networks (GANs) and other image synthesis models. However, like any metric, FID has its limitations and may not always be the best choice for evaluating the performance of a particular model \citep{borji2022pros}. Some of the general limitations of FID are that the Gaussian assumption in FID calculation might not hold in practice, the FID has high bias, and the sample size to calculate FID has to be large enough (usually above 50k), otherwise it would lead to an over-estimation of the actual FID \citep{borji2022pros}. Moreover, it is computationally expensive \citep{https://doi.org/10.48550/arxiv.2009.14075}. For the specific application of unsupervised cross-domain classification such as the framework mentioned in \cite{https://doi.org/10.48550/arxiv.2212.09107}, the FID was helpful in selecting a relatively good model, but it had problems. Most of the time it wasn’t able to pick the best possible model; moreover, the FID model ranking was not correlated with the true model ranking based on the true supervised classification metrics. It was stressed in \cite{https://doi.org/10.48550/arxiv.2212.09107} that there is a need for a framework to properly assess the validation datasets in order to improve the UI2I translation model selection criteria in unsupervised cross-domain classification models. The reason why traditional GAN evaluation metrics such as the FID are not suitable to support cross-domain classification is that they were not designed for that purpose, but rather were designed to evaluate the model in terms of the quality of the generated image from a human-eye perspective, and its ability to generate diverse results (not falling into mode collapse). Moreover, they do not take advantage of prior domain knowledge available in classification tasks such as the number of classes expected. 

In this paper, we introduce a framework to evaluate UI2I translation models designed to support cross-domain classification applications using pseudo-supervised metrics. In this proposed approach, we use the Inception model to extract the features from the unlabeled target DS, but unlike other methods such as the Inception Score (IS) or the Frechet Inception Distance (FID), we utilize an unsupervised clustering technique known as the Gaussian Mixture Models (GMM) to cluster the extracted features into $N$ clusters, where $N$ is the number of classes known from prior domain knowledge. The clusters were generated to act as pseudo labels ($y_{true}$) which enables the use of standard supervised metrics. We show that our methodology not only outperforms unsupervised metrics such as the FID, but also is highly correlated with the true supervised metrics and mimics their monotonically decreasing behavior which demonstrates the robustness and explainability of the metric, unlike the FID which is poorly correlated with the true supervised metrics, and has inconsistent ranking order that is neither robust nor explainable. To showcase the efficiency of the methodology, we use the boiling crisis detection problem as an example and we use the same datasets used by the authors in \cite{https://doi.org/10.48550/arxiv.2212.09107}. We conduct two experiments using the two publicly available datasets alternating the target and source datasets for each experiment ($DS1  \rightarrow{} DS2$ and $DS2  \rightarrow{} DS1$).

To summarize the contribution of this manuscript:

\begin{itemize}
  \item We propose a new framework for evaluating UI2I translation models using pseudo-supervised metrics that is designed specifically to support cross-domain classification.
  \item We use an unsupervised clustering technique (GMM) to cluster the extracted features into N clusters, where N is the number of classes known as a prior from domain knowledge, and use these clusters as pseudo labels to enable the use of standard supervised metrics. 
  \item The framework not only outperforms unsupervised metrics such as the FID, but is also highly correlated with the true supervised metrics, robust, and explainable.
\end{itemize}

The manuscript is organized as follows, section 1 provides an introduction and background information on the research topic and explains the research question and objectives of this study. Section 2 discusses relevant I2I translation studies and related evaluation frameworks. Section 3 describes the framework and the analysis procedures used in the study. In section 4 we describe the conducted experiments and then showcase and discuss the results. We conclude the manuscript in section 5 and list the sources cited afterward.

\section{Related Work} \label{RelatedWork}

First introduced by \cite{goodfellow2014a}, GAN consists of two networks, a generator and a discriminator, that are trained together to generate synthetic data that resembles a given dataset. The development of GANs has been the subject of numerous research papers and has led to the introduction of various variations and extensions of the original GAN architecture. For example, \cite{mirza2014conditional} introduced Conditional GANs (cGANs), which allowed the generation of data that is conditioned on some additional input (e.g., generating images of a specific type of object based on a class label). In 2016, \cite{chen2016infogan} introduced InfoGANs which disentangle the latent factors of variation in the data in order to make the generated data more interpretable and controllable. In 2017, \cite{karras2017progressive} introduced Progressive GANs which use a "progressive" training approach in which the resolution of the generated images is gradually increased during training, resulting in higher-quality images. \cite{arjovsky2017wasserstein} introduced Wasserstein GANs (WGANs) which use the Wasserstein distance as the loss function in order to improve the training stability and convergence.

The first work to utilize GANs to solve the I2I translation problem was \cite{isola2017image}. In their pix2pix model, they used conditional adversarial networks to learn the mapping from an input image to an output image, where the networks learn a loss function to train this mapping. This method uses a "U-Net" based architecture for the generator and a "PatchGAN" classifier for the discriminator. Multiple efforts were spent to improve and build upon the pix2pix model and overcome its limitations. \cite{wang2018discriminative} improved upon the pix2pix model to handle translation tasks that require high-quality images. They used Discriminative Region Proposal Adversarial Networks (DRPAN) for high-quality image-to-image translation. \cite{albahar2019guided} addressed the problem of guided image-to-image translation while respecting the constraints provided by an external, user-provided guidance image. They introduced a bi-directional feature transformation (bFT) scheme to utilize the constraints of the guidance image. \cite{wang2018high} Also tackled the low-resolution limitation of pix2pix with a novel adversarial loss, and new multi-scale generator and discriminator architectures. They also extended their framework to interactive visual manipulation with two additional features.

The main limitation of the pix2pix model was that it was supervised. The training process required  paired images in the training set for the model to learn the mapping $G: X \rightarrow Y$, where generated images from $G(X)$ are indistinguishable from real images coming from domain $Y$. Simply using the adversarial loss for this problem makes it heavily under-constrained. Thus, to solve this problem, \cite{zhu2017toward} introduced one of the most famous unsupervised I2I models known as CycleGAN. The authors coupled the adversarial loss with an inverse mapping $F: Y \rightarrow X$ and introduced a cycle consistency loss. The objective of this loss is to enforce $F(G(X)) \approx X$ and $G(F(Y)) \approx Y$. The idea was inspired by the language translation process, where a reverse translation of a sentence that was translated from language A to language B should give the same original sentence in language A. A similar approach was performed by \cite{yi2017dualgan} and \cite{kim2017learning} concurrently with cycleGAN. \cite{li2018unsupervised} proposed the SCANs framework to address the shortcomings of UI2I models in handling problems where there is a marginal difference between the domains or when the images are of high resolution. Their framework works by decomposing a single translation into multi-stage transformations where the information from the previous stage is used in the next stage using an adaptive fusion block. \cite{kim2019u} incorporated a new attention module to guide the model in distinguishing between source and target domains by focusing on the most important features.

\cite{Choi_2018_CVPR} introduced their StarGAN framework that simultaneously trains multiple datasets with different domains using a single generator and discriminator pair. However, StarGAN tends to change the images unnecessarily during image-to-image translation even when no translation is required \citep{Siddiquee_2019_ICCV}. To address this issue, \cite{Siddiquee_2019_ICCV} proposed the Fixed-Point GAN (FP-GAN) framework. This framework focused on identifying a minimal subset of pixels for domain translation and introduced fixed-point translation by supervising same-domain translation through a conditional identity loss, and regularizing cross-domain translation through revised adversarial, domain classification, and cycle consistency loss.

With the rapid rise of GANs and their use in various applications, the need for evaluation metrics to assess these models became increasingly critical. Traditional image evaluation measures were mainly designed to support tasks related to image compression and restoration where the image-to-image similarity was of utmost importance. Thus, they focused on measuring the similarity between images and were not suitable for image synthesis tasks. Currently, the two most common GAN evaluation measures are the Inception Score (IS) \cite{salimans2016improved} and the Fréchet Inception Distance (FID) \cite{heusel2017a}.

The IS in Eq. \eqref{InceptionScore} is a measure of the quality and diversity of generated images, based on the KL divergence between the predicted and true class distributions of a pre-trained Inception network \citep{borji2022pros,treder2022quality}. where $\mathbb{E}_{x \sim p_{\mathrm{gen}}}$ is the expectation over the images sampled from the generator, $\mathbb{K} \mathbb{L}$ refers to the Kullback-Leibler divergence, $p(y)$ is the marginal distribution of class labels, and $(p(y \mid x)$ represents the conditional distribution of class labels $y$ given an input image $x$.

\begin{equation}
\mathrm{IS}=\exp \mathbb{E}_{x \sim p_{\mathrm{gen}}}(\mathbb{K} \mathbb{L}(p(y \mid x) \| p(y)))
\label{InceptionScore}
\end{equation}

Although used commonly, The IS has some limitations. The IS does not capture intra-class diversity and lacks correlation with the human judgment of image quality and may produce inconsistent results when compared to human evaluations\citep{barratt2018note}. Moreover, it is insensitive to the prior distribution over labels (hence is biased towards ImageNet dataset and Inception model), and is very sensitive to model parameters and implementations \citep{borji2022pros}). The IS also requires a large sample size to be reliable. 

Similar to IS, the FID depends on the Inception model to generate its value, the difference, however, is that the FID calculates the Wasserstein-2 (a.k.a Fréchet) distance between multivariate Gaussians fitted to the embedding space of the Inception-v3 network of generated and real images \citep{borji2022pros,treder2022quality}. The FID is Denoted as:

\begin{equation}
\text{FID} = ||\mu_{g} - \mu_{r}||^{2} + Tr(C_{g} + C_{r} - 2(C_{g}C_{r})^{1/2})
\label{FID}
\end{equation}

In this equation, $\mu_g$ and $\mu_r$ represent the means of the feature maps of the generated images and real images, respectively, and $C_g$ and $C_r$ represent the covariance matrices of the feature maps of the generated images and real images, respectively. $Tr$ represents the trace operator, which sums the diagonal elements of a matrix. Unlike IS, the FID is more consistent with human inspection, is sensitive to minimal changes in the real distribution, and can detect intra-class mode collapse \citep{borji2022pros}. That being said, the FID has shortcomings as well. For example, the Gaussian assumption in FID calculation is not always valid, the FID has high bias, and requires a large sample size ($\geq 50k$ images) to be efficient \citep{chong2020effectively}.

% Despite these and the efforts of many others, these metrics either focus solely on the diversity of the results or the image quality from a human-eye perspective. There has yet to emerge a metric or framework that is designed to support the cross-domain classification applications of UI2I translation models.

\cite{siddiquee2023brainomaly} showed that the I2I model selected by FID has a weak correlation with the target classification task; therefore, the I2I model selected by FID performs poorly on the classification task. As a result, they proposed a pseudo-AUC metric for their anomaly detection task. Though we took inspiration from this work, their proposed pseudo-AUC metric cannot be applied directly to our task as they had images partially annotated in their problem setting. 

Despite these efforts and those of others, these metrics either focus solely on the diversity of the results, the image quality from a human-eye perspective, or require a portion of the target domain to be partially annotated (as the case with the pseudo-AUC). There has yet to emerge a metric or framework that is designed to specifically support fully unsupervised cross-domain classification frameworks.

\section{Methodology} \label{Methodology}

This section describes the methodlogy used to obtain our results. The demonstrated figures in this section shows only one of the experiments, which is when $DS1$ is used as a source dataset and $DS2$ as target ($DS1 \xrightarrow{} DS2$). The same logic applies for the other experiment.

\subsection{Source Classification Model Training}
% Figure \ref{fig:CNN-training} summarizes our source classifier training approach. 
The source dataset is split into three subsets, training, validation, and testing. A classification model is then trained on the training set for a pre-set number of iterations and the model is validated after every epoch using the validation dataset. The model that scores the best on the validation dataset is saved. Afterward, the best-saved model is tested on the testing set for final evaluation. For the purpose of our experiments, we used a convolutional neural network (CNN) as our classifier, but the methodology is agnostic to the type of classification model used.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width = \columnwidth, frame]{CNN training.jpg}
%     \caption{Source classifier training process}
%     \label{fig:CNN-training}
% \end{figure}

\subsection{Cross Domain Classification Framework}
Figure \ref{fig:UI2I-training} summarizes the UI2I translation training process. In order for enabling the source classification model to correctly classify images from the target dataset; which is an unlabeled dataset coming from a different domain that the source classification model has not seen before, we utilize an unsupervised Image-to-Image (UI2I) translation Generative Adversarial network (GAN) to translate the images in the target dataset from their domain to the source domain, so that they look like images familiar to the source classification model. For the purpose of demonstrating our methodology we have used Fixed-Point GAN (FP-GAN) \cite{Siddiquee_2019_ICCV} as our UI2I translation model. FP-GAN was designed to support domain adaptation by identifying a minimal subset of pixels for domain translation and has shown superiority over other models in domain translation tasks. This being said, the methodology is agnostic to the type of UI2I translation model used. The UI2I translation model is trained for $I$ number of iterations and a model is saved every $i$ iterations during training, making a total of $I/i$ checkpoint models saved. Both $I$ and $i$ are hyper-parameters that need to be tuned. 

\begin{figure}[H]
    \centering
    \includegraphics[width = \columnwidth, frame]{UI2I_training.jpg}
    \caption{UI2I training process}
    \label{fig:UI2I-training}
\end{figure}

Once the UI2I translation model training is complete, we use each of the saved i/I checkpoint models to translate the target validation set from the target domain to the source domain as shown in Figure \ref{fig:Translation-process}. We do this in order to evaluate which model is the best one to be used in deployment.

\begin{figure}[H]
    \centering
    \includegraphics[width = \columnwidth, frame]{Translation_process.jpg}
    \caption{Image translation process}
    \label{fig:Translation-process}
\end{figure}

\subsection{Pseudo Supervised Metrics}
The problem now is knowing which of these I/i UI2I translation models to use for model deployment. Since the target dataset is unlabeled, we cannot use supervised metrics to evaluate the validation set. The go-to metric in I2I translation is either the Inception Score (IS) or the Frechet Inception Distance (FID). The problem with these metrics is that they do not guarantee the best model. To combat this we came up with a novel framework to generate pseudo-labels for the target dataset before translation, which will eventually allow us to use the traditional supervised metrics to evaluate the best model, as demonstrated in Figure \ref{fig:Pseudo Labels}. The method starts by using the pre-trained inception model to generate features from the validation set of the target dataset prior to translation. After extracting the features, we use a Gaussian Mixture Model with a pre-set number of $N$ clusters, where $N$ is the number of classes known from prior domain knowledge. This will group the images into $N$ clusters which are used as pseudo labels (or pseudo classes) for the unlabeled data. In our experiments, the number of classes set by prior domain knowledge is equal to two ($N = 2$).

\begin{figure}[H]
    \centering
    \includegraphics[width = \columnwidth, frame]{Pseudo_Labels.jpg}
    \caption{Pseudo labels generation}
    \label{fig:Pseudo Labels}
\end{figure}

Now that we have the pseudo labels, we could use them to generate the pseudo-supervised metrics. For each translated validation set of the target dataset, we use the source classification model to generate predictions and then compare the generated predictions with the pseudo labels using the traditional supervised metrics. The model that scores the best pseudo-supervised metric will be the model selected for production. The pseudo metrics evaluation is described in Figure \ref{fig:Pseudo metrics} and the entire framework is described in Algorithm \ref{alg:Pseudo code}.

Note that since we don't know which cluster represents which actual label (class), we run all possible scenarios. The scenario that generates the best pseudo-supervised metrics is assumed to be the correct one. 


\begin{figure}[H]
    \centering
    \includegraphics[width = \columnwidth, frame]{pseudo_metrics.jpg}
    \caption{Pseudo metrics evaluation}
    \label{fig:Pseudo metrics}
\end{figure}


\begin{algorithm}
\scriptsize 
\setstretch{0.6}
\caption{Pseudo Supervised Metrics Algorithm}

\label{alg:Pseudo code}
  \DontPrintSemicolon
    \SetKwFunction{one}{train\_source\_classifier}
    \SetKwFunction{two}{train\_UI2I\_model}
    \SetKwFunction{three}{translate\_target}
    \SetKwFunction{four}{generate\_pseudo\_labels}
    \SetKwFunction{five}{pseudo\_supervised\_metrics}
  
  \SetKwProg{Fn}{Function}{:}{}
  

 \textbf{ Step 1:}
  \Fn{\one{$source\_DS$}}{
        train model\;
        save best model\;
        source-DS classifier = best saved model\;
        \KwRet{source-DS classifier}\;
  }
  
  \textbf{Step 2:}
  \Fn{\two{$source\_DS$,$target\_DS$}}{
        \textbf{initialize} models\_list = [ ]\;
        use source\_DS and target\_DS to train UI2I translation model\;
        run training for $I$ iterations\;
        save a checkpoint model every $i$ iterations\;
        append saved checkpoint model to models\_list\;
        \KwRet{models\_list [$i$-model, $2i$-model, ... , $I$-model]}\;
  }
  \textbf{Step 3:}
    \Fn{\three{$target\_DS$,$models\_list$}}{
        \textbf{initialize} translated\_sets\_list = [ ]\;
        \textbf{For} each model in models\_list:\;
        \Indp translate target\_DS to source domain using model\;
        append translated images set to translated\_sets\_list\;
        \Indm \KwRet{translated\_sets\_list [$source\_DS^*-i$, $source\_DS^*-2i$, ... , $source\_DS^*-I$]}\;
  }
  \textbf{Step 4:}
    \Fn{\four{$target\_DS$,$GMM$,$Inception\_model$}}{
        extract features from the target\_DS\;
        use GMM to cluster the data into 2 clusters\;
        assign labels to clusters covering both possible scenarios:\;
        \Indp \textbf{Scenario 1:} \{cluster 1 = label 1, cluster 2 = label 2 \}\;
        \textbf{Scenario 2:} \{cluster 1 = label 2, cluster 2 = label 1 \}\;
        \Indm \KwRet{ Pseudo\_$y_{true}$(scenario 1), Pseudo\_$y_{true}$ (scenario 2)}\;
  }
  
  \textbf{Step 5:}
  \Fn{\five{$translated\_sets\_list$,Pseudo $y_{true}$,Source-DS classifier}}{
        \textbf{initialize} models\_results\_list = [ ]\;
        \textbf{For} each translated\_set in translated\_sets\_list:\;
        \Indp $y_{pred} =$ Source-DS\_classifier(translated\_set)\;
        generate Pseudo metrics using $y_{pred}$ and pseudo $y_{true}$ from scenario 1\;
        generate Pseudo metrics using $y_{pred}$ and pseudo $y_{true}$ from scenario 2\;
        append best Pseudo metrics results to models\_results\_list\;
        \Indm best\_UI2I\_model = best(models\_results\_list)\;
        \KwRet{best\_UI2I\_model}\;
  }

\end{algorithm}



\section{Experiment}
\subsection{The Boiling Crisis Detection Problem}

Over the past few decades, the study of heat transfer mechanisms has become a focal topic for researchers around the world. Heat transfer mechanisms are critical in various industrial applications, including steam generation in boilers in power plants and solar collectors \cite{dirker2019thermal}, immersion cooling for high-performance electronics and data centers \cite{birbarah2020water,el2012immersion}, integrated cooling for three-dimensional electronic packaging \cite{kandlikar2014review}, cooling the core and used fuel in nuclear reactors \cite{fenech2013heat}, and so on. Thus, it has become increasingly important. One of the widely implemented heat transfer mechanisms is boiling heat transfer, a mechanism that utilizes the latent heat of the working fluid to dissipate a large amount of heat with minimal temperature increase \citep{rassoulinejad-mousavi2021a}. Despite its wide application and the amount of effort spent studying boiling heat transfer, this mechanism comes with a dangerous drawback known as the boiling crisis. The boiling crisis is the phenomenon where the heat flux of boiling reaches a critical bound known as the Critical Heat Flux (CHF), after which the heating surface will be covered by a blanket of continuous vapor layer that adversely affects heat dissipation by depreciating the heat transfer coefficient \citep{https://doi.org/10.48550/arxiv.2212.09107}. This is critically dangerous because the improper heat dissipation will lead to a quick temperature upraise on the heater surface beyond its capability and eventually cause it to break down. Hence, diverse research efforts have been dedicated to the boiling crisis detection problem. With the prominent rise of the application of machine learning algorithms in solving engineering problems \cite{altarazi2019machine,alhindawi2018predicting, zhao2022subdomain,ji2022machine, wang2022simultaneous}, researchers started investigating the applicability of these algorithms in CHF detection using a variety of techniques and data types. Whether it was acoustic emissions \cite{sinha2021a}, optical images \cite{rokoni2022a}, thermographs \cite{ravichandran2021a} or whether it was using a variety of supervised learning algorithms, including support vector machine \cite{hobold2018a}, multilayer perceptron (MLP) neural networks  \cite{hobold2018a}, transfer learning \cite{rassoulinejad-mousavi2021a}, and most recently researchers \cite{https://doi.org/10.48550/arxiv.2212.09107} started using frameworks supported by UI2I translation models to solve the cross-domain classification problem in boiling crisis detection such as the example used in this work to showcase our methodology. 

In this work, two different pool boiling experimental image datasets (DS-1 and DS-2) were prepared, where both DS-1 and DS-2 were generated using publicly available videos \cite{you-a,minseok2014a}. Specifically, the video from which DS-1 was prepared shows a pool boiling experiment performed using a square heater made of high-temperature, thermally-conductive microporous coated copper where the surface was fabricated by sintering copper powder. The square heater had a surface area of $\approx$ $100$ $mm^2$ and the working fluid used was water. All experiments were performed at a steady-state under an ambient pressure of 1 atm. A T-type thermocouple was used for temperature measurements. The resolution of the video frames was $512$ x $480$  pixels. The YouTube video from which DS-2 was prepared shows a pool boiling experiment performed using a circular heater made of microporous-coated copper where the surface was fabricated by sintering copper powder. The circular heater had a diameter of $\approx$ $16$ $mm$ and the working fluid used was DI water. All experiments were performed at a steady state under an ambient pressure of 50 kPa. A T-type thermocouple was used for temperature measurements. The resolution of the video frames was $1280$ x $720$ pixels.

Images for DS-1 and DS-2 were prepared by downloading the videos from YouTube and extracting individual frames using a MATLAB code via the VideoReader and imwrite functions. Recognizing duplicate frames extracted from the YouTube videos, quality control was conducted to remove the repeated images by calculating the relative difference using the Structural Similarity Index (SSIM) value \cite{gao2020a} between two consecutive images where images with a relative difference less than 0.03\% were removed. This pre-processing is important to ensure DL models were not biased by identical image frames. 

The images were categorized into two boiling regimes: (1) The critical heat flux regime (CHF), where a significant drop in the heat transfer coefficient is observed due to a continuous vapor layer blanketing the heater surface and (2) pre-CHF regime, where optimal heat transfer coefficient is obtained and  only discrete bubbles or frequent bubble coalescence is observed before departure. Originally, DS-1 had a total of 6158 images (786 CHF versus 5372 pre-CHF) and DS-2 had a total of 3215 (1233 CHF versus 1982 pre-CHF). As seen, both data sets were unbalanced. In each of the two experiments, only the training data for the source dataset was balanced before use. The target dataset was not balanced since the objective of this study is to introduce a framework that utilizes unsupervised learning, that is, the labeling information of the target datasets are assumed to be unavailable; Thus, impossible to balance using traditional oversampling or undersampling techniques. Table\ref{table1} shows the original number of images in each regime for each dataset and Fig\ref{fig3} shows a visual representation of the images for each dataset. The pixel intensity values in each image were normalized to fit in the range [0,1] to ensure uniformity over multiple datasets during deep learning training.

% \begin{figure}[!t] ----> uncomment this when doing two  column paper format
\begin{figure}[H]
\captionsetup{justification=centering}
\centerline{\includegraphics[width = \columnwidth]{datasets_v2_only_two.jpg}}
\caption{Representative images of bubble dynamics from source videos.}
\label{fig3}
\end{figure}

\begin{table}[H]
\captionsetup{justification=centering}
\caption{Datasets Summary}
\label{table1}
\centering
\scriptsize  % --> You may need to remove this line 
\resizebox{\columnwidth}{!}{%
%\begin{tabular}{lcccc}
\begin{tabular}{>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2cm}}
\toprule

\textbf{DS} & \textbf{Pre-CHF} & \textbf{CHF} \\
\midrule
DS-1 & 5372 & 786  \\
DS-2 & 1982 & 1233   \\ 
\bottomrule             
\end{tabular}
}
\end{table}

% In this work, three different pool boiling experimental image datasets (DS-1, DS-2, and DS-3) were prepared, where DS-1 and DS-2 were generated using publicly available YouTube videos \cite{you-a,minseok2014a} while DS-3 was conducted in-house. Specifically, the video from which DS-1 was prepared shows a pool boiling experiment performed using a square heater made of high-temperature, thermally-conductive microporous coated copper where the surface was fabricated by sintering copper powder. The square heater had a surface area of $\approx$ $100$ $mm^2$ and the working fluid used was water. All experiments were performed at a steady-state under an ambient pressure of 1 atm. A T-type thermocouple was used for temperature measurements. The resolution of the video frames was $512$ x $480$  pixels. The YouTube video from which DS-2 was prepared shows a pool boiling experiment performed using a circular heater made of microporous-coated copper where the surface was fabricated by sintering copper powder. The circular heater had a diameter of $\approx$ $16$ $mm$ and the working fluid used was DI water. All experiments were performed at a steady state under an ambient pressure of 0.5 atm. A T-type thermocouple was used for temperature measurements. The resolution of the video frames was $1280$ x $720$ pixels. DS-3 was obtained from our in-house experiments of water boiling on polished copper surfaces with an area of $100$ $mm^2$. The high-speed videos were captured using Phantom VEO 710L at a frame rate of 3000 fps and a resolution of $1280$ × $800$ pixels.

% Images for DS-1 and DS-2 were prepared by downloading the videos from YouTube and extracting individual frames using a MATLAB code via the VideoReader and imwrite functions. Recognizing duplicate frames extracted from the YouTube videos, quality control was conducted to remove the repeated images by calculating the relative difference using the Structural Similarity Index (SSIM) value \cite{gao2020a} between two consecutive images where images with a relative difference less than 0.03\% were removed. This pre-processing is important to ensure DL models were not biased by identical image frames. Benefiting from the large optical sensor size (25.6 mm × 16 mm) and the high-power backlight (Advanced Illumination BT200100-WHIIC), images of DS-3 have a balanced and homogeneous background. Also, the images are directly saved from the raw video files (.cine) that retain the highest image quality. As such, this pre-processing step was not necessary for DS-3.

% The images were categorized into two boiling regimes: (1) The critical heat flux regime (CHF), where a significant drop in the heat transfer coefficient is observed due to a continuous vapor layer blanketing the heater surface and (2) pre-CHF regime, where optimal heat transfer coefficient is obtained and  only discrete bubbles or frequent bubble coalescence is observed before departure. Originally, DS-1 had a total of 6158 images (786 CHF versus 5372 pre-CHF), DS-2 had a total of 3215 (1233 CHF versus 1982 pre-CHF) and DS-3 had a total of 23890 (12166 CHF versus 11724 pre-CHF). As seen, all data sets were unbalanced. In each of the six experiments, only the training data for the source dataset was balanced before use. The target dataset was not balanced since the objective of this study is to introduce a framework that utilizes unsupervised learning, that is, the labeling information of the target datasets are assumed to be unavailable. Thus, impossible to balance using traditional oversampling or undersampling techniques. Table\ref{table1} shows the original number of images in each regime for each dataset and Fig\ref{fig3} shows a visual representation of the images for each dataset. The pixel intensity values in each image were normalized to fit in the range [0,1] to ensure uniformity over multiple datasets during deep learning training.

% % \begin{figure}[!t] ----> uncomment this when doing two  column paper format
% \begin{figure}[H]
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width = \columnwidth]{datasets_v2.jpg}}
% \caption{Representative images of bubble dynamics from source videos.}
% \label{fig3}
% \end{figure}

% \begin{table}
% \captionsetup{justification=centering}
% \caption{Datasets Summary}
% \label{table1}
% \centering
% \resizebox{\columnwidth}{!}{%
% %\begin{tabular}{lcccc}
% \begin{tabular}{>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2cm}}
% \toprule
%  & \multicolumn{2}{c}{\textbf{Before under-sampling of DS-1}} & \multicolumn{2}{c}{\textbf{After under-sampling of DS-1}} \\
% \cmidrule(lr){2-5}
% \textbf{DS} & \textbf{CHF} & \textbf{Pre-CHF} & \textbf{CHF} & \textbf{Pre-CHF} \\
% \midrule
% DS-1 & 786 & 5372 & 786 & 786 \\
% DS-2 & 1233 & 1982 & 1233 & 1982 \\ 
% DS-3 & 12166 & 11724 & 12166 & 11724\\ 
% \bottomrule             
% \end{tabular}
% }
% \end{table}

% \begin{table}
% \begin{table}[H]
% \captionsetup{justification=centering}
% \caption{Datasets Summary}
% \label{table1}
% \centering
% \scriptsize  % --> You may need to remove this line 
% \resizebox{\columnwidth}{!}{%
% %\begin{tabular}{lcccc}
% \begin{tabular}{>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2cm}}
% \toprule

% \textbf{DS} & \textbf{Pre-CHF} & \textbf{CHF} \\
% \midrule
% DS-1 & 5372 & 786  \\
% DS-2 & 1982 & 1233   \\ 
% DS-3 & 11724& 12166  \\ 
% \bottomrule             
% \end{tabular}
% }
% \end{table}


% \begin{table}
% \captionsetup{justification=centering}
% \caption{Datasets Summary}
% \label{table1}
% \centering
% \resizebox{\columnwidth}{!}{%
% %\begin{tabular}{lcccc}
% \begin{tabular}{>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2cm}}
% \toprule
% & \multicolumn{2}{c}{\textbf{Before under-sampling of DS-1}} \
% \cmidrule(lr){2-3}
% \textbf{DS} & \textbf{CHF} & \textbf{Pre-CHF} \
% \midrule
% DS-1 & 786 & 5372 \
% DS-2 & 1233 & 1982 \
% DS-3 & 12166 & 11724 \
% \bottomrule
% \end{tabular}
% }
% \end{table}


% =========================================================
\subsection{Source Classification Model Training}

To diversify our training, we used different architectures for the classification model in each experiment. For the $DS1 \rightarrow{} DS2$ Experiment, we used the same architecture used in \cite{https://doi.org/10.48550/arxiv.2212.09107} to train the model. The architecture for that model is summarized in figure \ref{CNN_DS1_2_DS2}. For the $DS2 \rightarrow{} DS1$ Experiment, we used the ResNet50 model architecture \cite{He2015}. In both experiments, the data was split into three subsets: a training set (80\%), a validation set (10\%), and a testing set (10\%). The models were trained for a total of 100 epochs and the best model was saved and used in our pipeline. 

\begin{figure}[H]
\captionsetup{justification=centering}
\centerline{\includegraphics[width = \columnwidth]{CNN_arch_v4.jpg}}
\caption{Source classification model for the $DS1 \rightarrow{} DS2$ experiment.}
\label{CNN_DS1_2_DS2}
\end{figure}


\subsection{UI2I translation Model Training}

For the UI2I translation we used the FP-GAN model. We trained the model using the same architectures for both the generator and the discriminator as implemented by the authors \cite{Siddiquee_2019_ICCV}. The following hyperparameters were chosen in training the model: Image size $= 256$ x $256$, c dim $= 1$, batch size $= 8$, number of workers$ = 4$, lambda id$ = 0.1$,  $I$ (number of iterations) $= 300000$ , $i$ (save model after $i$ iterations) $= 10,000$, learning rate for $G$ and $D = 0.0001$, number of $D$ updates per each $G$ update $= 5$, $\beta_1$ for Adam optimizer$ = 0.500$, $\beta_2$ for Adam optimizer $= 0.999$.
% \begin{table}[htbp]
% \centering
% \begin{tabular}{|l|l|}
% \hline
% \textbf{Part} & \textbf{Input $\to$ Output Shape} \ \hline
% Down-sampling & (h, w, 3 + nc) $\to$ (h, w, 64) \
% & CONV-(N64, K7x7, S1, P3), IN, ReLU \
% & (h, w, 64) $\to$ (h/2, w/2, 128) \
% & CONV-(N128, K4x4, S2, P1), IN, ReLU \
% & (h/2, w/2, 128) $\to$ (h/4, w/4, 256) \
% & CONV-(N256, K4x4, S2, P1), IN, ReLU \
% Bottleneck & (h/4, w/4, 256) $\to$ (h/4, w/4, 256) \
% & Residual Block: CONV-(N256, K3x3, S1, P1), IN, ReLU \
% & (h/4, w/4, 256) $\to$ (h/4, w/4, 256) \
% & Residual Block: CONV-(N256, K3x3, S1, P1), IN, ReLU \
% & (h/4, w/4, 256) $\to$ (h/4, w/4, 256) \
% & Residual Block: CONV-(N256, K3x3, S1, P1), IN, ReLU \
% & (h/4, w/4, 256) $\to$ (h/4, w/4, 256) \
% & Residual Block: CONV-(N256, K3x3, S1, P1), IN, ReLU \
% & (h/4, w/4, 256) $\to$ (h/4, w/4, 256) \
% & Residual Block: CONV-(N256, K3x3, S1, P1), IN, ReLU \
% & (h/4, w/4, 256) $\to$ (h/4, w/4, 256) \
% & Residual Block: CONV-(N256, K3x3, S1, P1), IN, ReLU \
% Up-sampling & (h/4, w/4, 256) $\to$ (h/2, w/2, 128) \
% & DECONV-(N128, K4x4, S2, P1), IN, ReLU \
% & (h/2, w/2, 128) $\to$ (h, w, 64) \
% & DECONV-(N64, K4x4, S2, P1), IN, ReLU \
% & (h, w, 64) $\to$ (h, w, 3) \
% & CONV-(N3, K7x7, S1, P3), Tanh \ \hline
% \end{tabular}
% \end{table}


\subsection{Results and Discussion}

Once the pseudo metrics are generated, we can rank the models generated by the FP-GAN training process according to these metrics and pick the best one. Any of the regular metrics used for classification could be used as a pseudo metric once the pseudo labels were generated, but to demonstrate the methodology we will showcase the results using the balanced-accuracy and the AUC metrics. Figure \ref{DS1 vs DS2_all} A) and B) show the models ranking results based on pseudo supervised balanced accuracy and pseudo supervised AUC for the experiments ran with DS1 as a source Dataset ($DS1  \rightarrow{} DS2$). Figure \ref{DS1 vs DS2_all} C) and D) shows the ranking using the same metrics menionted, but using DS2 as a source dataset ($DS2  \rightarrow{} DS1$). The $x-axis$ represents the model ranking according to that metric from best to worst, while the y-axis represents the real metric value of the ranked model. The ranking curve for the actual ranking will show a monotonically decreasing behavior. In all comparisons, we show the ranking provided by the pseudo-supervised metrics and the FID metric vs the actual ranking. The best metric is the one that could mimic the monotonically decreasing behavior of the actual ranking line.

% \begin{figure}[h!]
\begin{figure}[H]
\captionsetup{justification=centering}
\centerline{\includegraphics[width = \columnwidth]{DS1_vs_DS2_all.jpg}}
\caption{Ranking results: A) True balanced accuracy for ranked models when using DS1 as source, B) True AUC for ranked models when using DS1 as source, C) True Balanced Accuracy for ranked models when using DS2 as source, D) True AUC for ranked models when using DS2 as source.}
\label{DS1 vs DS2_all}
\end{figure}

As seen from the figures, the pseudo metric ranking is consistent for both metrics while the FID shows very inconsistent and random behavior. Moreover, the Pearson correlations between the pseudo metrics and the FID vs the actual metric for the $DS1  \rightarrow{} DS2$ and $DS2 \rightarrow{} DS1$ experiments shown in Figures \ref{DS1_2_DS2_Correlations},\ref{DS2_2_DS1_Correlations} respectively, confirm the superiority of the pseudo metrics in comparison to the FID. The robustness of the pseudo metrics is more evident when DS2 was used as a source dataset, even when the pseudo metrics deviate from the original ranking, it still picks a model that is close in terms of the actual metric value and returns to mimic the actual ranking behavior. One thing to note is that the Pearson correlation was $\ge 96\%$ for the pseudo metrics for all experiments except the pseudo AUC in the $DS2  \rightarrow{} DS1$ experiment. What is interesting about that experiment is that the results of the real AUC for all 30 FP-GAN models were very close to one another ($\mu = 0.77$ and $std = 0.035$) resulting in a semi-constant line as shown in Figure \ref{DS1 vs DS2_all} D).

% \begin{figure}[h!]
\begin{figure}[H]
\captionsetup{justification=centering}
\centerline{\includegraphics[width = \columnwidth]{DS1_2_DS2_Correlations.jpg}}
\caption{Correlations results for the $DS1  \rightarrow{} DS2$ experiment:A) FID vs the actual balanced accuracy, B) Pseudo-balanced accuracy vs the actual balanced accuracy, C) FID vs actual AUC score, and D) Pseudo-balanced AUC vs actual AUC score. }
\label{DS1_2_DS2_Correlations}
\end{figure}

% \begin{figure}[h!]
\begin{figure}[H]
\captionsetup{justification=centering}
\centerline{\includegraphics[width = \columnwidth]{DS2_2_DS1_Correlations.jpg}}
\caption{Correlations results for the $DS2  \rightarrow{} DS1$ experiment: a) FID vs the actual balanced accuracy, b) pseudo-balanced accuracy vs the actual balanced accuracy, c) FID vs actual AUC score, and d) pseudo-balanced AUC vs actual AUC score.}
\label{DS2_2_DS1_Correlations}
\end{figure}





% \subsection{Results and Discussion}

% Once the pseudo metrics are generated, we can rank the models generated by the FP-GAN training process according to these metrics and pick the best one. Any of the regular metrics used for classification could be used as a pseudo metric once the pseudo labels were generated, but to demonstrate the methodology we will showcase the results using the balanced-accuracy and the AUC metrics. Figure \ref{DS1_all} shows the results for the experiments ran with DS1 as a source Dataset ($DS1  \rightarrow{} DS2$ and $DS1 \rightarrow{} DS3$). The $x-axis$ represents the model ranking according to that metric from best to worst, while the y-axis represents the real metric value of the ranked model. The ranking curve for the actual ranking will show a monotonically decreasing behavior. In all comparisons, we show the ranking provided by the pseudo-supervised metrics and the FID metric vs the actual ranking. The best metric is the one that could mimic the monotonically decreasing behavior of the actual ranking line.

% % \begin{figure}[h!]
% \begin{figure}[H]
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width = \columnwidth]{DS1_All.jpg}}
% \caption{Ranking results when using DS1 as source.}
% \label{DS1_all}
% \end{figure}

% Figure \ref{DS1_all} A) and B) shows the pseudo-balanced accuracy and the pseudo-AUC respectively for the $DS1  \rightarrow{} DS2$ experiment, while C) and D) show the same for the $DS1 \rightarrow{} DS3$ experiment. As seen from the figures, the pseudo metric ranking is consistent for both metrics while the FID shows very inconsistent and random behavior. Moreover, the Pearson correlations between the pseudo metrics and the FID vs the actual metric for the $DS1  \rightarrow{} DS2$ and $DS1 \rightarrow{} DS3$ experiments shown in Figures \ref{DS1_2_DS2_Correlations},\ref{DS1_2_DS3_Correlations} respectively, confirm the superiority of the pseudo metrics in comparison to the FID. 

% % \begin{figure}[h!]
% \begin{figure}[H]
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width = \columnwidth]{DS1_2_DS2_Correlations.jpg}}
% \caption{Correlations results for the $DS1  \rightarrow{} DS2$ experiment.}
% \label{DS1_2_DS2_Correlations}
% \end{figure}

% % \begin{figure}[h!]
% \begin{figure}[H]
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width = \columnwidth]{DS1_2_DS3_Correlations.jpg}}
% \caption{Correlations results for the $DS1  \rightarrow{} DS3$ experiment.}
% \label{DS1_2_DS3_Correlations}
% \end{figure}

% A similar behavior for the metrics is observed when $DS2$ is used as the sources dataset. Figure \ref{DS2_all} shows the results for the experiments ran with $DS2$ as a source Dataset ($DS2  \rightarrow{} DS1$ and $DS2 \rightarrow{} DS3$). The robustness of the pseudo metrics is shown more clearly in this Figure. Even when the pseudo metrics deviate from the original ranking, it does pick a model that is close in terms of the actual metric value and returns to mimic the actual ranking behavior. The Pearson correlations between the pseudo metrics and the FID vs the actual metric for the $DS2  \rightarrow{} DS1$ and $DS2 \rightarrow{} DS3$ experiments shown in Figures \ref{DS2_2_DS1_Correlations},\ref{DS2_2_DS3_Correlations} respectively, confirm the superiority of the pseudo metrics in comparison to the FID. One thing to note is that the Pearson correlation was $\ge 96\%$ for the pseudo metrics for all experiments except the pseudo AUC in the $DS2  \rightarrow{} DS1$ experiment. What is interesting about that experiment is that the results of the real AUC for the entire 30 FP-GAN models were very close ($\mu = 0.77$ and $std = 0.035$) resulting in a semi-constant line as shown in Figure \ref{DS2_all} B).


% % \begin{figure}[h!]
% \begin{figure}[H]
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width = \columnwidth]{DS2_All.jpg}}
% \caption{Ranking results when using DS2 as source.}
% \label{DS2_all}
% \end{figure}

% % \begin{figure}[h!]
% \begin{figure}[H]
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width = \columnwidth]{DS2_2_DS1_Correlations.jpg}}
% \caption{Correlations results for the $DS2  \rightarrow{} DS1$ experiment.}
% \label{DS2_2_DS1_Correlations}
% \end{figure}

% % \begin{figure}[h!]
% \begin{figure}[H]
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width = \columnwidth]{DS2_2_DS3_Correlations.jpg}}
% \caption{Correlations results for the $DS2  \rightarrow{} DS3$ experiment.}
% \label{DS2_2_DS3_Correlations}
% \end{figure}

% ======================================================================
\section{Conclusion}

In this paper, we introduce a framework to evaluate UI2I translation models that was designed specifically to support cross-domain classification applications using pseudo-supervised metrics. To showcase the efficiency of the framework, we use the boiling crisis detection problem as an example. We demonstrate the consistency of the results by conducting two experiments using two publicly available datasets from different domains.

We show that our methodology not only outperforms unsupervised metrics such as the FID, but also is highly correlated with the true supervised metrics, unlike the FID which is poorly correlated with the true supervised metrics and inconsistent. Moreover, we show that typical GAN evaluation metrics such as the FID which were designed to evaluate models based on their ability to generate images that are both diverse and realistic to the human eye are not suitable to support cross-domain classification tasks As shown in the results section. We compare our method against the FID metric and the true metrics in cross-domain classification tasks and we show that in all comparisons the ranking provided by the pseudo metric is superior to the FID metric and mimics the monotonically decreasing behavior of the true metric. In the future, we want to expand on this work by further testing on several other multi domain datasets; moreover, we want to extend this work to include non-binary classification problems where the GMM clusters label assignment is expected to be more complicated.

% \section{Conclusion}

% In this paper, we introduce a framework to evaluate UI2I translation models that was designed specifically to support cross-domain classification applications using pseudo-supervised metrics. To showcase the efficiency of the framework, we use the boiling crisis detection problem as an example. We demonstrate the consistency of the results by conducting six different experiments using three datasets from different domains.

% We show that our methodology not only outperforms unsupervised metrics such as the FID, but also is highly correlated with the true supervised metrics, unlike the FID which is poorly correlated with the true supervised metrics and inconsistent. Moreover, we show that typical GAN evaluation metrics such as the FID which were designed to evaluate models based on their ability to generate images that are both diverse and realistic to the human eye are not suitable to support cross-domain classification tasks As shown in the results section. We compare our method against the FID metric and the true metrics in cross-domain classification tasks and we show that in all comparisons the ranking provided by the pseudo metric is superior to the FID metric and mimics the monotonically decreasing behavior of the true metric.

\section*{CRediT authorship contribution statement}
\textbf{Firas Al-Hindawi:} Conceptualization, Methodology, Software, Writing - Original Draft, Writing - Review \& Editing. \textbf{Md Mahfuzur Rahman Siddiquee:} Conceptualization, Methodology, Software, Writing - Review \& Editing. \textbf{Teresa Wu:} Conceptualization, Methodology, Writing - Original Draft, Writing - Review \& Editing, Resources, Supervision, Project administration. \textbf{Han Hu:} Conceptualization, Writing - Original Draft, Writing - Review \& Editing, Data Curation. \textbf{Ying Sun} Writing - Review \& Editing, Supervision.


\section*{Declaration of Competing Interest}
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper

% \section*{Acknowledgement}
% Support for this work was provided in part by the US National Science Foundation under Grant No. CBET-1705745.

% \bibliographystyle{ieeetr}
\bibliography{references.bib}

\end{document}
