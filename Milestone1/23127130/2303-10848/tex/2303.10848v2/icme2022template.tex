% Template for ICME 2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP/ICME LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{appendix}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\usepackage[dvipsnames, svgnames, x11names]{xcolor}
\usepackage{multirow}
\usepackage[switch]{lineno}


\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}

\pagestyle{empty}


\begin{document}\sloppy

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}


% Title.
% ------
\title{Weakly-Supervised Text Instance Segmentation}
%
% Single address.
% ---------------
\name{Xinyan Zu, Haiyang Yu, Bin Li, and Xiangyang Xue}
%Address and e-mail should NOT be added in the submission paper. They should be present only in the camera ready paper. 
\address{\{xyzu20, hyyu20, libin, xyxue\}@fudan.edu.cn\\
Shanghai Key Laboratory of Intelligent Information Processing\\
School of Computer Science, Fudan University}


\maketitle


%
\begin{abstract}
Text segmentation is a challenging vision task with many downstream applications. Current text segmentation methods require pixel-level annotations, which are expensive in the cost of human labor and limited in application scenarios. In this paper, we take the first attempt to perform weakly-supervised text instance segmentation by bridging text recognition and text segmentation. The insight is that text recognition methods provide precise attention position of each text instance, and the attention location can feed to both a text adaptive refinement head (TAR) and a text segmentation head. Specifically, the proposed TAR generates pseudo labels by performing two-stage iterative refinement operations on the attention location to fit the accurate boundaries of the corresponding text instance. Meanwhile, the text segmentation head takes the rough attention location to predict segmentation masks which are supervised by the aforementioned pseudo labels. In addition, we design a mask-augmented contrastive learning by treating our segmentation result as an augmented version of the input text image, thus improving the visual representation and further enhancing the performance of both recognition and segmentation. The experimental results demonstrate that the proposed method significantly outperforms weakly-supervised instance segmentation methods on ICDAR13-FST (18.95$\%$ improvement) and TextSeg (17.80$\%$ improvement) benchmarks.
\end{abstract}
%
\begin{keywords}
Text instance segmentation, weakly-supervised method
\end{keywords}
%
\section{Introduction}
In recent years, the text segmentation task has aroused growing research interests because it plays an important role in computer vision research and many practical applications, \textit{e.g.}, font style transfer, scene text removal, and interactive text image editing. Therefore, the text segmentation task can facilitate these downstream tasks and alleviate massive human labor for manually labeling pixel-level text regions.

The early work on text segmentation mainly follows the design of semantic segmentation for generic objects, which uses pixel-level annotations as supervision. Although the existing text segmentation methods have achieved considerable performance on several benchmarks, these methods rely heavily on a large amount of data with pixel-level annotations, \textit{e.g.}, the method proposed in \cite{xu2021rethinking} achieves the state-of-the-art performance on text segmentation with 196K pixel-level labeled training data, which costs hundreds of man-hours but still remains limited in application scenarios (lack of text segmentation data for web text or documentation text). Moreover, existing text segmentation methods only distinguish texts from backgrounds, \textit{i.e.}, instance-level segmentation results are inaccessible to current methods, which usually have limited assistance to downstream tasks.

Recently, generic object segmentation methods like \cite{zhou2018weakly}, which are weakly supervised by categories of the objects in an image rather than pixel-level annotations, have been proposed. However, a few challenging problems exist in text segmentation, \textit{e.g.}, unpredictable fonts, diverse textures, and non-convex contours. Therefore, directly applying weakly-supervised generic segmentation methods to solve the text segmentation task achieves poor performance.

Therefore, our motivation is to propose a text segmentation method that is 1) free from pixel-level annotations, 2) capable of conducting text-instance-wise segmentation. Our key observation is that the rough location of a text instance can be precisely predicted at the decoding stage of an attention-based text recognizer. It sounds reasonable that the text instance segmentation result can thus be obtained by feeding the segmentation head with adequate initial information, \textit{i.e.}, visual features, rough location, and classification of the text instance.


With such prior knowledge as our guideline, we design the proposed method with three modules: text recognition module, text segmentation module, and text adaptive refinement (TAR) module. As a preprocessing step, we employ an off-the-shelf text detector to extract text lines. For each text line image cropped by the detector, the proposed recognition module predicts its text instances and perceives their rough location via the attention mechanism. The insight is to forge the label-free attention maps into segmentation masks by feeding the segmentation module with adequate information: 1) rough attention location as initial seeds, 2) classification results as perceptual knowledge of the text instances 3) visual features as the panorama of the text image. Subsequently, we forge the attention maps again with the proposed TAR module to generate pixel-wise pseudo labels as supervision of the segmentation module. Specifically, TAR is a two-stage refinement module to fit the effective region of each text instance in an iterative manner. Through training, the attention maps shape closer to the text instance, thus further improving the quality of pseudo labels, and it is such iterative mutual enhancement between attention maps and pseudo labels that makes our method work. Additionally, with segmentation masks obtained, it naturally comes to us that mask-augmented contrastive learning between the segmentation results and the input text image can be conducted to enrich the feature representation and suppress the interference caused by complex backgrounds, thus achieving better recognition and segmentation performance. It also benefits existing recognition methods by functioning as a plug-and-play module.

In summary, the contributions of this work are as follows:

\begin{itemize}
    \item This work is the first attempt to propose a weakly-supervised text instance segmentation method. The proposed method couples text segmentation with text recognition, thus bridging the gap between the isolated processes of these two fields.
    \item We propose a text adaptive refinement (TAR) module that provides high-quality pseudo labels through a two-stage iterative procedure with fast inference speed.
    \item We design a mask-augmented contrastive learning strategy to improve the performance of both text recognition and text segmentation. Serving as a plug-and-play solution, it boosts the performance of existing text recognition methods by an average of 2$\%$.
    \item Experimental results show that the proposed method outperforms generic weakly-supervised segmentation methods on text segmentation benchmarks by a considerable margin, specifically by 18.95$\%$ on ICDAR13-FST datasets and 17.80$\%$ on TextSeg datasets.
\end{itemize}
 

\section{Related Works}
\subsection{Generic Weakly-Supervised Segmentation Methods}
In the last decades, several weakly-supervised methods \cite{zhou2018weakly,ahn2019weakly,cholakkal2019object,kuo2019shapemask,lee2021bbam} for instance segmentation, which usually use the bounding boxes or class activation maps (CAMs) as the rough location for each instance and then refine the rough locations to obtain the fine-grained segmentation masks, are proposed. Although these weakly-supervised generic instance segmentation methods can achieve comparable performance with those methods using pixel-level annotations as supervision, these methods cannot reach satisfying performance when applied to the text segmentation task.
  
\subsection{Text Segmentation Methods}
Traditional methods directly set a threshold for grayscale or employ Markov Random Field (MRF) to separate the foreground and background. In recent years, some methods based on deep neural networks have emerged. In \cite{xu2021rethinking}, the authors put forward TextRnet and a new text segmentation dataset, where TextRnet utilizes the unique text prior such as texture diversity and non-convex contours to achieve state-of-the-art performance on text segmentation benchmarks. The authors of \cite{xu2022bts} mainly focus on Bi-Lingual text segmentation, they propose a Bi-Lingual text dataset along with PGTSNet which contains a plug-in text-highlighting module and a text perceptual module to help distinguish between text languages. Most recently, Textformer\cite{wang2023textformer} leverages the similarities between text components by proposing a multi-level transformer framework to enhance the interaction between text components and image features at different granularities. However, the above-mentioned methods only distinguish texts from the background while the proposed text instance segmentation will be more practical for downstream tasks. 



\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{pipeline.png}
  \caption{The overall architecture of the proposed method. The recognition module, the segmentation module and the refinement module are on gray, beige and purple backgrounds, respectively. Best viewed in color.}
  \label{fig:architecture}
\end{figure*}

% \subsection{Weakly-supervised Instance Segmentation}
% Zhou et al. [] first proposes a weakly-supervised instance segmentation method that regards the class activation maps of the images as strong visual clues and \yhy{combines a few object segmentation proposals via spacial-continuity to realize instance segmentation}. Ahn et al. [] leverage the deep edge detection method to generate class boundary maps. [] designs an approach to meet the consistency between the peak map and the density map. Ge et al[] propose a cascaded four-task segmentation network with a share-backbone. [] first models segmentation tasks with graph networks. Shen et al[] sees weakly-supervised instance segmentation as detection-guided weakly-supervised semantic segmentation. Lee et al[] propose a bounding box attribution map for weakly-supervised instance segmentation. [] uses a supervised deep salience method to train a mask-RCNN network for instance segmentation.


% On one hand, the need of fully annotated data demand massive human labour, which makes such method hard to extend to practical senarios, e.g. web text, document text. On the other hand, the rich text prior still remains untouched by existing text segmentation methods. In this work, we leverage the strong prior of text order and number implicitly from text recognition module. By cascading text recognition and text segmentation networks, we thus perform instance-level text segmentation with word-level annotation only.


\section{METHODOLOGY}
\textbf{Overview:} As shown in Fig. \ref{fig:architecture}, the proposed method mainly consists of three modules: 1) attention-based text recognition module. 2) text segmentation module. 3) proposed text adaptive refinement module. More details are in the following.
% \subsection{Design principle} 
%\(4) contrastive learning is conducted between the final segmentation mask and the input image to meet with the consistency in feature level, thus enriching the feature representation of our backbone to further improve the recognition and segmentation performance
% Although remarkable progress have been achieved, existing methods on weakly-supervised segmentation tasks can hardly generalize to the problem setting of text image. To explain, these methods rely on localisation cues such as bounding boxes, scribbles, and points which usually focus on visual textures to discriminate objects. Regular objects like human faces, automobiles, trees, and birds share similar textures from scenes to scenes. However, text in the image can be variously shaped and distinct in fonts. There also exists text characters like “O”, “D”, and “A” which has a close-up structure. Without pixel-level annotation, these methods can barely learn to mimic the exact shape of such character with an inner space.
% \\As mentioned in the SOTA text segmentation method[], another challenge of text segmentation is the arbitrarily scaled text. The number of characters that may occur in a single text image varies from one to dozens, causing the segmentation model struggling in capturing a specific text pattern. Under such circumstances, segmentation masks can be redundant or incomplete. 
% \\Furthermore, despite the large potential value, instance-level image text segmentation remains less studied in this field.
%  \\Motivated by these challenges, we made an inspiring attempt at conducting text recognition and text segmentation collaboratively. We adopt a FPN-based encoder to extract content-rich multi-scaled vision features to tackle with the arbitrarily scaled text. For obtaining high quality segmentation masks of those characters with inner spaces, we propose TCAR to refine our attention maps iteratively in a manner of meeting agreement between the segmentation region and its corresponding chromatic appearance, i.e. the text boundary is within discrimination by human eyes because of the chromatic aberration. Via mapping segmentation masks to their character identities given by recognition module, we thus shed the new light on image text instance segmentation.
 
\subsection{Text Recognition Module}
We build our text recognition module following the fashion of SAR \cite{li2019show}, which provides the required attention location while having a light-weighted framework. It is worth noting that the recognition baseline can be replaced by other attention-based recognizers to pursue better performance.

% an attention-based text recognizer with the encoder-decoder framework. Given an input text image $\mathbf{I}$, the attention-based recognizer intend to predict the character sequence $\textbf{Y} = \{y_1, y_2,..., y_N\}$, where N denotes the length of the text. Meanwhile, the generated attention maps $\mathbf{V}= \{v_1, v_2,..., v_N\}$ during the iterative decoding stage mark the rough location of the corresponding character. To be mentioned, the recognizer can be replaced by other attention-based text recognition methods, we carefully choose [x] because it is strong yet efficient.

% \subsubsection{Encoder with FPN structure}
In the wild, texts are varying in scale. To address this challenge, a customized backbone with FPN feature fusion\cite{liu2019learning} is designed to extract features at three levels. Given the input image $\mathbf{X} \in \mathbb{R}^{H \times W \times C}$, the extracted visual features at different levels are denoted as $\mathbf{F}^1 \in \mathbb{R}^{\frac{H}{2} \times \frac{W}{2} \times C} $, $\mathbf{F}^2 \in \mathbb{R}^{\frac{H}{4} \times \frac{W}{4} \times C} $, and $\mathbf{F}^3 \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{4} \times C} $ from bottom to top, respectively. Subsequently, by training adaptive weights at nearly free cost, the fused features manage to keep the scale-invariance between different levels, represented as:
\begin{equation}
  \mathbf{F}^l_{fuse} = \alpha^l \cdot \mathbf{F}^{1 \rightarrow l} + \beta^l \cdot \mathbf{F}^{2 \rightarrow l} + \gamma^l \cdot \mathbf{F}^{3 \rightarrow l}
\end{equation}
Where $l$ denotes the spatial level of fused feature, $\alpha^l$, $\beta^l$, and $\gamma^l$ are trainable adaptive weights. Symbol $\rightarrow$ denotes the resize transformation. As shown in Fig. \ref{fig:architecture}, these fused feature maps with different scales will be parallelly processed and they are uniformly denoted as $\mathbf{F}$ in the following. Subsequently, a 2-layer BiLSTM encoder is applied to extract the holistic feature $\mathbf{h}_w$, which is used to initialize the hidden state of the LSTM decoder. The attention-based decoder aims to model the conditional distribution $P(\mathbf{y}|\mathbf{X})$ in a recurrent manner, which is denoted as follows:
\begin{equation}
  P(\mathbf{y}|\mathbf{X}) = \prod _{t=0}^{T}{P(\mathbf{y}_{t}|\mathbf{g}_{t},\mathbf{h}_{t})}
\end{equation}
where $\mathbf{h}_t$ represents the hidden state of the LSTM decoder. $\mathbf{g}_{t}$ is known as the glimpse vector, which is computed by the following steps:
\begin{equation}
  \mathbf{e}_{i,j} = \tanh(\mathbf{W}_F \mathbf{F}_{ij} + \sum_{p,q \in \mathcal{N}_{ij}} \Tilde{\mathbf{W}}_{p-i,q-j} \cdot \mathbf{F}_{pq} + \mathbf{W}_h \mathbf{h}_t)
\end{equation}
\begin{equation}
  \alpha_{ij} = \text{softmax}(\mathbf{w}_e^\top \cdot  \mathbf{e}_{ij})
\end{equation}
\begin{equation}
  \mathbf{g}_{t} = \sum_{i,j} \alpha_{ij} \mathbf{F}_{ij}, \ \ \ i = 1,...,H; \ j = 1,...,W
\end{equation}
where $\mathbf{F}_{ij}$ is the feature vector at position $(i, j)$ in $\mathbf{F}$, and $\mathcal{N}_{ij}$ is the eight pixels neighboring the position $(i, j)$; $\mathbf{W}_F$, $\mathbf{W}_h$, and $\Tilde{\mathbf{W}}$ are learnable parameters of linear transformations; and $\alpha_{ij}$ is the attention weight at location $(i, j)$. With the glimpse vector $\mathbf{g}_t$ and the hidden state $\mathbf{h}_t$, the $t$-th text instance is predicted by the following transformation:
\begin{equation}
  y_t = \text{softmax}(\mathbf{W}_p[\mathbf{h}_t; \mathbf{g}_t])
\end{equation}
Where $\mathbf{W}_p$ is a linear transformation. Finally, the loss of the text recognition module is as follows:
\begin{equation}
  \mathcal{L}_\text{text} = -\sum_{t={1}}^{T}{\log{P(\hat{y_t}|y_t)}} 
\end{equation}
where $\hat{y}$ is the ground truth of the text in the input image.

\subsection{Segmentation Module}
Through a convolution layer with $1 \times 1$ kernel size, the fused features $\mathbf{F}$, the attention maps $\mathbf{A}$ and the embeddings of the text instance prediction $y$ are concatenated into a combined feature $\textbf{F}^c$, which is taken as the input of the segmentation module. The segmentation module, aiming to generate the coarse mask $\mathbf{M}_i$ for $i$-th text instance, mainly consists of transposed  convolution layers with skip connections. The coarse masks have two channels (\textit{i.e.}, foreground mask and background mask), and are supervised by the binarized pseudo labels generated by the refinement module, which will be detailed in Sec. \ref{refinement}. The loss of the coarse masks is calculated as follows:
\begin{equation}
  \mathcal{L}_\text{seg} = \sum_{t={1}}^{T}{BCE(\mathbf{m}_t, \mathbf{p}_t)} 
\end{equation}
where $\mathbf{p}_t$ denotes the pixel-level pseudo label for the $t$-th coarse mask, and BCE denotes binary cross entropy loss.
\subsection{Refinement Module}
\label{refinement}
Generic weakly-supervised segmentation method has developed refinement technique \cite{araslanov2020single} based on parameter-free pixel adaptive convolution, which is fast and efficient. On top of that, we propose a two-stage refinement strategy for the text segmentation task. To update each pixel $p^t$ on pseudo label at $t-th$ time step, we have:
\begin{equation}
  p^t = \sum_{a \in \mathcal{A}}{\frac{e^{\overline{k}(\mathcal{V}_p,\mathcal{V}_a)}}{\sum_{n \in \mathcal{A}}{e^{\overline{k}(\mathcal{V}_p,\mathcal{V}_n)}}}} \cdot p^{t-1}
\end{equation}
where $\mathcal{A}$ denotes the collection of all adjacent pixels of $p^t$ in the kernel, and $\mathcal{V}$ denotes the visual feature that actually controls the refinement appearance. k is a kernel function:
\begin{equation}
k(\mathcal{V}_x,\mathcal{V}_y) = \frac{-|\mathcal{V}_x - \mathcal{V}_y|}{\sigma^2_x}
\end{equation}
where $\sigma$ denotes the standard deviation of the
visual intensity inside the kernel. Our key design for text segmentation is that the refinement operation falls into two stages. Please refer to Fig.\ref{fig:dilation} for a vivid explanation: 1) in stage one, we use the backbone feature as visual guidance for TAR, which literally alters the rough location towards the effective receptive field of the text instance, thus making the pseudo label covers more region of the text instance. 2) in stage two, we use the image RGB as the visual guidance for TAR to get the fine-grained pseudo mask of the text instance. It is reasonable that since stage one has covered the major part of the text instance, the text segments of stage two can be more complete.
% Following this principle, namely classification-appearance consistency, we design the refinement module that is free from parameters. In this module, we iteratively calculate the similarity and update the confidence for each pixel in the text image to estimate whether this pixel should be regarded as the foreground of the text image. Specifically, The similarity between two pixels is determined by the RGB distance or feature similarity while the pixel confidence derives from the coarse mask produced by the segmentation module. Through several iterations, coarse masks produced by the segmentation module can be gradually refined into fine-grained masks to fit the boundaries of text instances. By replacing the RGB appearance (input image) with semantic appearance (feature map with rich contextual meaning), the erosion mode will be turned to dilation mode. This is because the semantic appearance is a high-level visual representation that obeys the semantic-integrity, it usually describes a text instance with the whole region that surrounds the instance. Via a number of dilation and erosion, the attention map is transformed to a pseudo ground-truth. An example of generating the pseudo label and more details of the proposed refinement module are shown in Supplementary Material. 


%As depicted in Figure 3, the refinement module serves as an important role in our segmentation approach. TCAR follows the guideline of the segmentation-appearance consistency prior, iteratively adjust the boundary of a segmentation masks based on the observation of image chromatic aberration locally and globally.
% \subsubsection{segmentation-appearance consistency prior}
% The idea of segmentation-appearance consistency comes from a previous 
% research[] that solves human-skin segmentation with a skin color 
% adaptive algorithm. Globally, it first learns the overall visual 
% presentation of the human skins in a given image. Then the method 
% expand or shrink the local segmentation regions based on the 
% similarity of skin color and its learned adaptive threshold. Just like human skins, the text in image can be easily discriminated because of 
% the obvious color difference between text boundaries and the 
% background. As we observing the examples shown in Figure 3(a), if two 
% adjacent pixels vote for the same class in segmentation but appears 
% quite different in RGB space, then this two pixels have violated the segmentation-appearance consistency.
% To estimate the similarity between two adjacent pixels, we design an appearance similarity measurement to map the RGB distance, large in value range, to an interval of (0,1], assume $X$ is the image, $X_{i,j}$ represents the pixel at position$(i,j)$. Let $N$ be the aggregation of 8 nearest neighbour of $X_{i,j}$, the similarity score is as follows:
% \begin{equation}
% %   Sim(X_{i,j},X_{p,q}) = e^{-0.01 \times Dst(X_{i,j},X_{p,q})}
% \end{equation}
% Where $X_{p,q} \in N$, $Dst$ is the RGB distance between the two pixels. Therefore, the similarity Kernel $K_{sim}$ is computed. We empirically find that the borderline similarity to distinguish between two different colors is 0.3. As mentioned in Sec3.2, we perform pixel-wise softmax on the coarse mask to model the pixel-wise classification confidence of foreground(text) and background. Therefore, the confidence kernel $K_{conf}$ can be cropped from the foreground confidence map in the corresponding $3\times3$ area of position $(i,j)$. For classification-appearance alignment, we design a mechanism namely supportive scoring, where each adjacent pixel of $X_{i,j}$ delivers the likeliness that they support pixel $X_{i,j}$ to be a text area. It is logically sounding that nearby pixel whose similarity and confidence are both high or both low should highly support $X_{i,j}$ to be the text part. The supportive score Supp can be computed by:
% \begin{equation}
% %   Supp = 1-(Sim)^2-(Conf)^2 , Sim+Conf<1
% \end{equation}
% \begin{equation}
% %   Supp = 1-(Sim-1)^2-(Conf-1)^2 , Sim+Conf>=1
% \end{equation}
% Where $Sim$ and $Conf$ denote the appearance similarity and confidence score of the nearby pixel. The distribution of supportive mechanism is shown in Figure4. By computiong the supportive score on all the nearby pixels, the supportive kernel $K_{spp}$ is obtained. The mean value of $K_{spp}$ is taken as refinement outcome of input pixel $X_{i,j}$ and updated to the its classification confidence. The refinement is complete when every pixel of input mask is updated by the algorithm above. For better segmentation performance, the refinement operation can be performed iteratively. In this work, we empirically choose 5 iterations to achieve a balance between segmentation performance and training effiency. Note that in the aforementioned sub-task(a), the appearance refers to the feature map which is rich in semantic meaning but poor in textual detail. Eventually, the semantic fidelity guides the refined result to dilate to roughly cover the whole character regions. On the contrary, sub-task(b) takes original image as appearance to generate fine-grained segmentation results.
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{distribution.png}
%   \caption{distribution of supportive score}
%   \label{figure__5}
% \end{figure}
% \subsubsection{color-adaptive refinement}
% As discussed above, the classification-appearance consistency is critical to the completeness of segmentation mask. We encourage that nearby pixels with similar appearance should be assigned to the same class. Illustrated in Figure 3(b), we consider to implement this idea by adding color adaptive convolutions to efficiently refine the segmentation masks in an iterative manner. Notice that the module is parameter-free.
% Given two neighbour pixels p1 and p2, their color distance in RGB space can be computed by
% \begin{equation}
%   Distance(P_1,p_2) = \sqrt{2+(p_{1,R}+p_{2,R})/512 \times (p_{1,R}-p_{2,R})^2 + 4 \times (p_{1,G}-p_{2,G})^2 + (2+(255-(p_{1,R}+p_{2,R})/2)/256) times B^2} 
% \end{equation}
% Where R, G, B denotes the color intensity in the corresponding channels. Although this measurement has been widely used, it doesn’t suit our method because of its varying range. As a result, we further design a normalized measurement namely color similarity to measure chromatic difference, mathematically put:

% Although the classification-segmentation consistency is met, the pseudo ground truth may still contain several errors in training time since the initial seed of segmentation comes from a recognition attention map that is not designed for segmentation in the first place(discussed in Sec4.6). To avoid the strong model from learning to mimic these errors[articles in pamr], we adopt a random strategy to enlarge the exploration space of the pseudo label, thus alleviating such effects. Another threat is the symptom of chromatic distortion, illustrated in Figure3(c). The changing color inside single character slightly violates the classification-appearance consistency. We tackle this by adding an adaptive weight to increase tolerance for the consistency principle. In this case, neighbourhood pixels with high confidence but less similarity also have high supportive score. The detailed random strategy and adaptive weighted is demonstrated in Appendix.
% To further expanding the exploration space of the refinement module, we design a random mechanism for training only, that is, drawing 9 random numbers with independent distribution from the uniform (0,1) distribution, denoted as N1, N2,..., N9 $M_n$. We use the following equation to apply random noise to the initial confidence map:
% \begin{equation}
%   M_{conf} = M_{conf} + 0.2 \times (M_n - 0.5)
% \end{equation}
% More extensive experiments are conducted in Sec 4.3 to prove the effectiveness of this random trick.
% \subsubsection{color-adaptive weight}
% As discussed above, the purpose of our TCAR is to eliminate the disagreement between the segmentation mask and the corresponding appearance. A common priori hypothesis is that within the same text-word, color and font between different characters are unified. However, Figure3(c) has shown an exception of such prior. At the meantime, it also shows the satisfactory performance of TCAR to tackle the image text with inner gradient color by adopting a color-adaptive weight. In the previous setup, we crop nearby pixels using a 3x3 kernel, we define this 3x3 adjacent kernel as a “local kernel”. Subsequently, we crop nearby pixels with a 5x5 kernel, which is times bigger than the 3x3 kernel. We call this as a “global kernel”. a standard deviation of RGB distance can be inferred through the adjacent kernel. Let theta L as the standard RGB deviation of the local kernel, while let theta G as the standard RGB deviation of the global kernel. We thus calculate the support map by adding a weight 
% T =theta L/ theta G to manipulate the intensity of the refinement, mathematically put:
% \begin{equation}
%   M_{supp} = T \times (M_{conf}+M_{rand})
% \end{equation}
% When T>=1 , it shows that the color variation of the local region is larger than that of the global region. It is the common circumstance when the crop box locates at text boundary, the refinement ought to be enhanced.
% When T<1, the standard deviation of the global kernel outmatch that of the local kernel. This will occasionally be caused by the image text with gradient color change, where the refinement should be restricted.
% \subsubsection{pseudo labels}
% The initial attention seeds are comparatively denser than a regular segmentation mask can be, which may not achieve good results if we unfold these attention maps via TCAR. As a matter of fact, we seek to explore the possibility of estimating image visual appearance by the feature map F1 instead of the input image I. As is shown in the right part of Figure 2, performing TCAR with feature map has largely expanded the edges of the attention areas, while no signs of character-shaped boundaries are observed. A reasonable explanation can be that the feature map has a representation of high semantic context but with low texture details. The refinement is thus towards character semantic fidelity, causing a larger attention region with semantic completeness. On top of that, we further refine these enlarged attention maps to obtain fine-grained segmentation masks with high completeness. We treat these fine-grained segmentation masks as our pseudo ground truth for the reconstruction module. The Pseudo label of time step t is as follows:
% \begin{equation}
%   S_{t_{Pseudo}} = TCAR(I,TCAR(F_1,V_t))
% \end{equation}
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{TAR.png}
  \caption{Explanation of the proposed two-stage TAR process.}
  \label{fig:dilation}
\end{figure}

\subsection{Mask-augmented Contrastive Learning}
With the obtained segmentation results, it naturally comes to us that these segmentation masks can function as positive data augmentation for the model to conduct contrastive learning. Through the recognition baseline, projection features of the input image, the corresponding segmentation mask, the set of the whole training batch, and images other than the input in the batch can be obtained, which is denoted as $\mathbf{P}_i$, $\mathbf{P}_p$, $\mathcal{U}$ and $\mathcal{P}_n$ respectively. We have $l_\text{NCE}$ as the contrastive loss:

\begin{equation}
    l_\text{NCE}(\mathbf{P}_i, \mathbf{P}_p, \mathcal{P}_n) = -log \frac{e^{sim(\mathbf{P}_i, \mathbf{P}_p) / \tau}}{\sum_{\mathbf{P} \in \mathcal{P}_n} e^{sim(\mathbf{P}_i, \mathbf{P}) / \tau}  }
\end{equation}
where $\tau$ is a temperature parameter. $sim(\mathbf{a}, \mathbf{b})$ is to measure the feature distance between $\mathbf{a}$ and $\mathbf{b}$, which is a cosine similarity calculated by
$
    sim(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a}^\text{T} \mathbf{b}}{ ||\mathbf{a}|| \cdot ||\mathbf{b}|| }
$. Therefore, the loss of the mask-augmented contrastive learning is computed by:
\begin{equation}
    \mathcal{L}_c = \sum_{\mathbf{P}_i\in \mathcal{U}} l_\text{NCE} (\mathbf{P}_i, \mathbf{P}_p, \mathcal{P}_n) + \sum_{\mathbf{P}_i\in \mathcal{U}} l_\text{NCE} (\mathbf{P}_p, \mathbf{P}_i, \mathcal{P}_n)
\end{equation}
Instead of engaging the main recognition-segmentation process, mask-augmented contrastive learning helps to extract a better and more robust presentation that suppresses the influence of complicated backgrounds of text images in the wild.

\subsection{Training Objective} 
The overall loss of the proposed method is calculated as follows:
\begin{equation}
  \mathcal{L} = \mathcal{L}_{seg} + \lambda_{rec} \mathcal{L}_{rec} + \lambda_{c} \mathcal{L}_{c}
\end{equation}
where and $ \lambda_{c}$ are the hyperparameters to balance losses from three modules in our method.

The inference procedure is demonstrated in the bottom-left region of Fig. \ref{fig:architecture}: with coarse masks in all three levels, \textit{i.e.}, $\mathbf{M}_1$, $\mathbf{M}_2$, and $\mathbf{M}_3$ are binarized and ensembled to generate the fine-grained mask as final segmentation result.

\section{Experiments}
% \yhy{In this section, we first introduce the used datasets and implementation details of our method. We subsequently compare our method with existing methods and perform ablation studies on varying settings. Next, discussion and visual analysis are conducted to further prove the feasibility of our work. Last but not least, we implement two down-to-earth programs as downstream applications. }

\subsection{Datasets and Implementation Details}
\begin{table*}
\centering
\renewcommand\tabcolsep{9.5pt} % 调整表格列间的长度
\renewcommand{\arraystretch}{1.0}
  \caption{Performance comparison with other methods, where $Seg^2$ denotes ICDAR2013-FST and TextSeg that with text label, $Seg^5$ denotes the all five text segmentation datasets. TR, UT denotes text recognition datasets and unlabelled text datasets, respectively. $T$,$S$,$B_c$,$B_w$ denote text label, pixel-wise segmentation label, text-instance-wise bounding box annotation, and word-level bounding box annotation, respectively. `pseudo' indicates that the results are tested with binarized pseudo labels.}
  \label{accuracy_public}
  \scalebox{0.85}{
  \begin{tabular}{lccccccc}
    \hline
    Method & Training Data & Supervision & ICDAR13-FST & TextSeg & MLT-S & COCO-TS & Total-Text\\
    \hline
    \multicolumn{8}{l}{\textbf{Fully-Supervised Text Segmentation Methods (Upper-bounds, Not for Comparison)}}\\
    DeeplabV3 \cite{chen2018encoder} & $Seg^5$ & $T+S+B_c$ & 69.27 & 84.07 & 84.63 & 72.07 & 74.44\\
    HRNetV2-W48 \cite{yuan2020object} & $Seg^5$ & $T+S+B_c$ & 70.98 & 85.03 & 83.26 & 68.93 & 75.29\\
    TexRNet \cite{xu2021rethinking} & $Seg^5$ & $T+S+B_c$ & 73.38 & 86.84 & 86.09 & 72.39 & 78.47\\
    \hline
    \multicolumn{8}{l}{\textbf{Weakly-Supervised Instance Segmentation Methods (For Comparison)}}\\
    Zhou \textit{et al.} \cite{zhou2018weakly}& $Seg^2$ & $T$ & 22.34 & 26.79 & - & - & - \\
    Ahn et al. \cite{ahn2019weakly}& $Seg^2$ & $T$ & 35.66 & 38.40 & - & - & - \\
    Cholakkal et al. \cite{cholakkal2019object}&  $Seg^2$ & $T$ & 37.15 & 42.86 & - & - & - \\
    ShapeMask \cite{kuo2019shapemask}& $Seg^2$ & $T+B_c$ & 37.84 & 49.67 & - & - & -\\
    BBAM \cite{lee2021bbam}& $Seg^2$ & $T+B_c$ & 41.60 & 53.59 & - & - & -\\
    \hline
    Ours& $Seg^2$ & $T+B_w$& 48.09 & 59.75 & - & - & - \\
    Ours& $Seg^5$ & $T+B_w$& 51.30 & 64.47 & 62.91 & 55.38 & 56.67 \\
    Ours& $Seg^5$+$TR$& $T+B_w$& 53.80 & 67.99 & 64.75 & 57.23 & 59.85 \\
    \textbf{Ours}& $Seg^5$+$TR$+$UT$& $T+B_w$ & \textbf{60.55} & 71.24 & 72.44 & \textbf{63.14} & \textbf{66.42} \\
    Ours (pseudo)& $Seg^5$+$TR$+$UT$& $T+B_w$ & 60.49 & \textbf{71.39} & \textbf{72.48} & 63.11 & 66.29 \\
  \hline
  \end{tabular}}
\end{table*}

\textbf{Datasets} The training and validation of our network involve text recognition datasets and text segmentation datasets where only text labels are used. To warm up the recognition module, we have several popular datasets from the text recognition field and unlabeled data, details are introduced in \textbf{Appendix}. The joint training stage involves text segmentation datasets, \textit{e.g.}, ICDAR2013-FST \cite{karatzas2013icdar}, TextSeg \cite{xu2021rethinking}, Total-Text \cite{ch2017total}, MLT\_S \cite{bonechi2020weak}, and COCO\_TS \cite{bonechi2019coco_ts}. Different from previous methods, we only utilize text labels instead of pixel-wise labels. The bi-lingual text-related benchmark \cite{chen2021benchmarking} can also be used for training the recognition module.

\begin{table}
\centering

% \renewcommand\tabcolsep{7.5pt} % 调整表格列间的长度
\renewcommand{\arraystretch}{0.9}
\caption{Ablation on different refinement settings.}
  \label{refinement_compare}
  \scalebox{1.0}{
  \begin{tabular}{c|ccc}
    \hline
    Refinement & Iteration & Time (ms) & fIoU (\%) \\
    \hline
    PRMs \cite{zhou2018weakly} & - & 5.6 & 46.21\\
    FC-CRFs \cite{krahenbuhl2011efficient} & - & 89.4 & 64.95\\
    \textbf{TAR (ours)} & \textbf{2 - 8} & \textbf{2.6} & \textbf{71.24}\\
    \hline
\end{tabular}}
\end{table}
\noindent\textbf{Implementation Details} The size of the input is set to 48$\times$160 and the number of channels for the fused feature is set to 512. Please note that three levels of features are parallelly processed by the subsequent modules. The number of iterations for the proposed two-stage TAR module is set to 2 and 8, respectively. Our method is implemented with PyTorch and trained on a single NVIDIA RTX 3090 GPU with 24GB memory. The batch size is set to 32 with a learning rate of 1.0. We conduct warm-up training for the recognition module alone, then jointly train it with the segmentation module. The hyperparameters $\lambda_{rec}$ and $\lambda_{c}$ are empirically set to 1 and 0.1.


% \\Syn90K[] and SynthText[] are two major synthetic datasets widely adopted by scene text recognition methods for training. These two datasets contains more than 10M synthetic scene text images which lays a solid foundation of generalization ability for recognition models. We leverage these two datasets for model warm-up.
% Since text in real world have extreme diversity in terms of color, size, and font. Undesirable situation such as curved text, low resolution, occlusion and color distortion can make text segmentation a totally tough mission. We consider adding the following real-world data for our joint-training stage to narrow the domain gap between synthetic datasets and real scenarios.
% Real-world scene text datasets, e.g. IC13[], IC15[], IIIT-5k[], svt[], COCO[], CUTE[] and TotalText[] are filmed at real nature scenes like street views, traffic intersections and billboards, where svt[] COCO[], CUTE[] and IC15[] contains irregular text cases as aforementioned.
% We also manage to utilize unlabeled datasets, ranging from BOOK32[], TextVQA[] and ST-VQA[] which contain massive real-world scene text images without annotation. We prevent a semi-supervised training scheme to leverage these unlabeled images, see Figure 5[] for more details.


\subsection{Comparison with Existing Methods}
% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=\linewidth]{result_display.png}
%   \caption{Segmentation visualization on 5 datasets}
%   \label{result display}
% \end{figure*}
We conduct experiments on five aforementioned public text segmentation benchmarks to evaluate our method and listed the performance of the following methods: (a) weakly-supervised generic instance segmentation methods including \cite{zhou2018weakly}, \cite{ahn2019weakly}, \cite{cholakkal2019object}, ShapeMask \cite{kuo2019shapemask}, and BBAM \cite{lee2021bbam}. (b) state-of-the-art text segmentation methods including DeeplabV3 \cite{chen2018encoder}, HRNetV2-W48 \cite{yuan2020object}, and TexRNet \cite{xu2021rethinking}, note that these are fully-supervised methods acting as upper-bound, and our method is not expected to exceed that. Following the fashion of most segmentation manuscripts, we adopt fIoU (foreground Intersection over Union) as the evaluation metric. As shown in Tab. \ref{accuracy_public}, generic weakly-supervised instance segmentation methods with available public sources are retrained with the same training data. Since MLT-S, COCO-TS, and Total-Text do not contain text labels, they are not used to evaluate weakly-supervised methods. Our method, however, is able to train segmentation on the above three datasets by shifting the recognition module to inference mode. Therefore, the proposed method is capable of leveraging text recognition datasets and unlabeled datasets to further enhance performance and suppress overfitting. To sum up, our method significantly outperforms the SOTA weakly-supervised segmentation methods on all available datasets: on ICDAR13-FST, we exceed the former SOTA \cite{lee2021bbam} by 18.95$\%$, while on TextSeg, we take an advantage of 17.80$\%$. Meanwhile, the proposed methods achieve more than 82$\%$ of the fully supervised SOTA's \cite{xu2021rethinking} performance. We also evaluate the performance of the pseudo labels in inference: As shown in the last row of Tab. \ref{accuracy_public}, utilizing the pseudo labels to generate segmentation masks can achieve comparable performance with using the output of the segmentation module.

\subsection{Ablation Studies}
We conduct extensive ablation studies to evaluate the effectiveness of the proposed modules and training strategies. To evaluate the superiority of the proposed TAR, other refinement methods, \textit{i.e.}, FC-CRF \cite{krahenbuhl2011efficient} and PRMs \cite{zhou2018weakly}, which enjoy much popularity among weakly-supervised segmentation methods, are compared. As shown in Tab. \ref{refinement_compare}, the proposed TAR achieves more than 30x speed-up compared to FC-CRFs and 2x speed-up compared to PRMs, and TAR outperforms PRMs and FC-CRFs significantly by 25.03$\%$ and 6.29$\%$ in accuracy.

We also conduct ablation studies to evaluate the effectiveness of mask-augmented contrastive learning, warm-up, and joint training. As shown in Tab. \ref{ablation}, the accuracy of the recognizer in our model will be improved by 1.94\% and fIoU by 0.80\% if our model is equipped with the contrastive learning strategy. Furthermore, the experimental results demonstrate that the warm-up and joint training strategy are necessary to our model, details discussed in \textbf{Appendix}.

\begin{table}
\renewcommand\tabcolsep{7.5pt}
\renewcommand{\arraystretch}{1.0}
  \caption{Ablation studies for contrastive learning, warm-up, and joint training strategies.}
  \label{ablation}
  \scalebox{0.80}{
  \begin{tabular}{ccccc}
    \hline
    Contrastive Learning & Warm-up & Joint Training & Acc &  fIoU \\
    \hline
    & \checkmark & \checkmark  & 91.93 & 70.44\\
    % \midrule
    % \midrule
    \checkmark & & \checkmark & 79.27 & 52.45 \\
    %  \midrule
    \checkmark & \checkmark & & 88.04 & 42.38\\
    %  \midrule
    \checkmark & \checkmark & \checkmark & 93.87 & 71.24\\
    \hline
\end{tabular}}
\end{table}

\begin{table}
\renewcommand\tabcolsep{7.5pt} % 调整表格列间的长度
\renewcommand{\arraystretch}{1.0}

  \caption{Plugable mask-augmented contrastive learning.}
  \label{seghelprec}
  \scalebox{0.83}{
  \begin{tabular}{lcc}
    \hline
    Text Recognition Methods & Baseline & Baseline + Segmentation \\
    \hline
    CRNN \cite{shi2016end}&  83.53 & 87.23 (+ \textcolor{green}{3.70})\\
    SAR \cite{li2019show} &  90.85 & 93.10 (+ \textcolor{green}{2.25})\\
    AutoSTR \cite{zhang2020autostr} &  93.88 & 95.06 (+ \textcolor{green}{1.18})\\
    ABINet \cite{fang2021read} &  93.51 & 94.47 (+ \textcolor{green}{0.96})\\
    ours &  91.93 & 93.87 (+ \textcolor{green}{1.94})\\
    \hline
  \label{boost}
\end{tabular}}
\end{table}

% \subsubsection{Different settings in refinement module}
% We notice that a recent work\cite{araslanov2020single} of generic segmentation designs their refinement module, titled PAMR, following the similar principle as ours. We carry out comparisons between our refinement module and PAMR on performance and time efficiency to investigate the superiority of our module on text images. Another refinement method denseCRF\cite{} which enjoys much popularity among text segmentation methods is added to the comparison as well. As illustrated in table\ref{refinement_compare}, we set different appearance-kernel size and iteration times to observe the time efficiency and segmentation performance, all models in this experiment share the same configuration except the refinement module. Experimental results show that our refinement module not only achieves the best performance on text segmentation, it also speeds up the refinement speed by 3X of PAMR and 30X of dense-CRF.
% \subsubsection{Impact of contrastive learning}
% Although the proposed method achieves considerable performance on both recognition and segmentation, we would like to see the contribution of the contrastive learning strategy. In table\ref{contrastive_table1}, we train another control group without contrastive learning, which suffers a drop of $1.8\%$ in recognition accuracy and $0.5\%$ in segmentation fIoU. We further argue that by adopting our coupled recognition-segmentation paradigm, any existing attention-based text recognition method can benefit from this contrastive learning strategy, we leave this discussion to Section4.4.
% \subsubsection{Influence of warm-up for recognizer} As mentioned above, we perform a 50K iterations warm-up for the recognition module individually. This strategy is of great significance because it ensures the rough localization ability of the attention mechanism. We conduct experiment to see the performance of the model that is trained end-to-end without warm-up. As is illustrated, there is a huge gap between the performance of both recognition and segmentation with or without the warm-up step. This is not surprising because when trained without warm-up, the error of attention localization will accumulate to pseudo label generating. The details are further discussed in Sec4.4.
% \subsubsection{Diversity of multi-scaled segmentation}
% We design the scaled FPN structure in the first place to 1) enrich the contextual meaning for feature representation. 2) add diversity to the segmentation results due to the different receptive fields of three scaled feature. In the proposed method, the final segmentation result is a combination of three different outputs: M1, M2, and M3 from parallel pipelines. By ablating these sub-masks, we demonstrate the effectiveness of the designed multi-scaled structure, shown in tabel\ref{multiscale_table}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{vis0816.png}
  \caption{Visualization of our text segmentation results.}
  \label{fig:quality}
\end{figure}

\subsection{Further Discussions and Analysis}
\textbf{Visualization of Segmentation Results}
As shown in Fig. \ref{fig:quality}, there exist several challenges in the five adopted text segmentation datasets: 1) All the selected images are captured in the wild with complex backgrounds. 2) The sample of TextSeg shows challenging aesthetic font types. 3) Unexpected lighting situation occurs in the example of ICDAR2013-FST. In spite of the above three challenges, the visualization results of our method are satisfying.
% 1) the misleading of the localization cue. 2) segmentation results not accurate enough. 3) Our method though has a strong understanding ability towards characters. However, as discussed above, since the method is not trained with pixel-level labels, even with same semantic meaning, the result can be unfairly judged to have fIoU errors, as in Fig. \ref{fig:misjudge}.

\textbf{Text Segmentation helps Text Recognition}
Theoretically, the strategy of contrastive learning between original inputs and segmentation masks can be equipped to benefit all existing text recognition methods. We select some of the text recognizers \cite{shi2016end,li2019show,zhang2020autostr,fang2021read} with great popularity in recent years to observe the performance boosts when adopting our strategy. These methods are all fairly re-trained and tested on our datasets (any datasets with text labels). According to the experimental results in Tab. \ref{boost}, all selected methods have achieved a marginal boost in accuracy (2$\%$ in average).

\textbf{Down-stream Applications} Interesting down-stream applications (\textit{e.g.}, image text removal, and text instance style transfer) are demonstrated in \textbf{Appendix}.


% After the warm-up of recognition module, we jointly train recognition module and segmentation module from end-to-end to achieve best performance. Since the recognizer have obtained the localization ability after warm-up, training segmentation module separately seems to be an option. However, this idea conflicts the experimental results shown in table\ref{contrastive_table2}. A logical explanation is that we need attention maps that not only with rough localization cues, but also suitable for segmentation tasks. The idea is clearly addressed in Figure\ref{}, where we capture some "incomplete dilation" situations from the early stage of our training. 
% \subsubsection{Utilizing massive unlabeled data}
% \subsubsection{What if we use pseudo masks as segmentation results}
% \subsubsection{Segmentation-aided text recognition}
% In this section, we seek to answer this question: whether and how can text segmentation help the task of text recognition. As mentioned above, the contrastive learning conducted in our method have boosted the performance of both segmentation and recognition. Here, we extend this experiment on three text recognition methods to investigate the feasibility of segmentation-aided text recognition. The chosen text recognition methods are CRNN\cite{}(CTC-based), SAR(our baseline), and ABINet(state-of-the-art), respectively. Since designing customized segmentation networks to fit their models separately are time-consuming and unfair for comparison, we conduct this experiment shown in Table\ref{contrastive_table2} by treating our method as a plug-and-play segmentation agent. Specifically, via the proposed method, we generate a segmentation-mask version of all training data(MJ and ST) used by three recognition methods. Therefore, contrastive learning between origin image and its segmentation version are conducted on aforementioned methods. Note that for CRNN, we test the model on IC15, SVTP, and CUTE80 while the performance are not given in there article. For SAR, we retrain the model on ST and MJ for a fair comparison. The experimental results demonstrate that all three methods achieve considerable improvements more or less. Therefore, we stay positive about our intuition of using text segmentation to enhance text recognition. We point out that it can be a potential research direction in the future, yet to be explored.
% \subsubsection{Qualitative analysis}


% \subsection{Setup}
% \textbf{Datasets:}
% \\By performing text segmentation and text recognition collaboratively, we are able to tap the potential of annotated data or unlabeled data from communities of both Text Recognition and Text Segmentation to the greatest extent. The preview of datasets are shown in Figure 5.
% \\\textbf{text recognition datasets:}
% \\Syn90K[] and SynthText[] are two major synthetic datasets widely adopted by scene text recognition methods for training. These two datasets contains more than 10M synthetic scene text images which lays a solid foundation of generalization ability for recognition models. We leverage these two datasets for model warm-up.
% Since text in real world have extreme diversity in terms of color, size, and font. Undesirable situation such as curved text, low resolution, occlusion and color distortion can make text segmentation a totally tough mission. We consider adding the following real-world data for our joint-training stage to narrow the domain gap between synthetic datasets and real scenarios.
% Real-world scene text datasets, e.g. IC13[], IC15[], IIIT-5k[], svt[], COCO[], CUTE[] and TotalText[] are filmed at real nature scenes like street views, traffic intersections and billboards, where svt[] COCO[], CUTE[] and IC15[] contains irregular text cases as aforementioned.
% We also manage to utilize unlabeled datasets, ranging from BOOK32[], TextVQA[] and ST-VQA[] which contain massive real-world scene text images without annotation. We prevent a semi-supervised training scheme to leverage these unlabeled images, see Figure 5[] for more details. 
% \\\textbf{text segmentation datasets:}
% Five public text segmentation datasets are used for training and test. ICDAR2013 FST[] and a newly released dataset TextSeg[] contain word-level labels, which can be used for end-to-end training in our model. However, the rest three: Total-Text[], MLTS[] and COCOTS[] only consist of pixel-level masks. As a result, we train them with the introduced semi-supervised strategy as mentioned above. The quality comparison between these datasets is demonstrated in Figure 5.
% \\\textbf{Implementation Details}
% \\We adopt a 31-layer customized ResNet as our backbone based on the design in [sar]. The input image will be resized to 48*160 and padding with zeros if lack in width. The FPN heads are set with 512 output channels to modeling each scaled feature maps. By adjusting the pooling layers of FPN, the multi-scaled feature maps are in a size of 6x40 , 12x40, and 24x80 respectively. Please note that the parallel recognition and segmentation procedures for the three separate features do not share weights afterwards. The LSTM encoder and attentional LSTM decoder both adopt a 2-layer architecture with hidden state number set to 512. A total number of 95 classes are taken as the character alphabet, including 26 English character in both capital and lower case, numbers, printable punctuation and the “EOS” token. The 2-dimensional attention maps are calculated by a 3x3 convolution layer with the same spatial size as their corresponding feature maps. The word embedding [FI] and the word prediction [ss] are linear layouts with an output dimension of 95, as the number of classes. The decoder in segmentation module consists of fractional-convolution layers with skip connection which are mirroredly reverse to the backbone encoder. We use the ADADELTA as our optimizer to begin training with learning rate 1.0 and batch size 16. We train the proposed model on 6 GeForce GTX 1080ti GPUs with 11GB memory of each card. The code is implemented with PyTorch. Before end-to-end training, we first warm-up with the recognition module only for 100K iterations. Then we train the recognition module and segmentation module jointly for 320K iterations to achieve the proposed performance. The training objective weights [lambda1] and [lambda2] are empirically set to 1 and 0.1, respectively. In the TCAR module, we let the iteration number of refinement to be 0 at warm-up and 10 at training. We reduce the iteration parameter from 10 to 5 after 50K joint training iterations. The iteration parameter remains 5 for evaluation. Additionally, note that the teacher forcing strategy is adopted for the recognition module at training stage as is illustrated in section3.2.2.
% \begin{table*}
% \renewcommand\tabcolsep{9.5pt} % 调整表格列间的长度
%   \caption{Results compared to Sota methods.}
%   \label{accuracy_public}
%   \begin{tabular}{llccccccc}
%     \toprule
%     \midrule
%     Method & Training Data & ICDAR13-FST & TextSeg & MLT-S* & COCO-TS* & Total-Text*\\
%     \midrule
%     DeeplabV3+ & 4 & 69.27 & 84.07 & 84.63 & 72.07 & 74.44\\
%     HRNetV2-W48 & 4 & 70.98 & 85.03 & 83.26 & 68.93 & 75.29\\
%     HRNetV2-W48 + OCR & 4& 72.45 & 85.98 & 83.49 & 69.54 & 76.23\\
%     TexRNet & 4+TextSeg & 73.38 & 86.84 & 86.09 & 72.39 & 78.47\\
%     \midrule
%     \midrule
%     ${WISE}^{[1]}$& 4+TextSeg & 50.83 & 64.15 & 62.63 & 55.04 & 56.33 \\
%     ${WISE}^{[2]}$& 4+TextSeg+TR& 52.29 & 67.44 & 64.10 & 56.73 & 59.45 \\

%     ${WISE}^{[3]}$(11.21)& 4+TextSeg+TR+U & 59.84 & 71.21 & 70.39 & 62.07 & 64.90 \\
%     \midrule
%   \bottomrule
% \end{tabular}
% \end{table*}
% \subsection{Results and Comparison}
% The comparison between WISE and other methods, including SOTA text segmentation method[] with full supervision and weakly-supervised instance segmentation methods [,,,,] which have released their code in public, is conducted on five segmentation benchmarks. The overall results can be seen in Table 2. The proposed WISE outperforms other weakly-supervised instance segmentation methods by a considerable margin and has reached nearly 82 percents accuracy of the fully supervised SOTA text segmentation method.

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{error.png}
%   \caption{failure case}
%   \label{figure_8}
% \end{figure}
% As is shown in Figure, two main type of segmentation error occurs in our results. One of them can be concluded as positioning error, in which inaccurate attention area causes the segmentation results to be sticky between two adjacent characters. The other failure can be observed in characters like “O”, and “O” which has a close-up structure. When the character is large in visual size, the attention peak is reasonable to appear inside the hole. Under such circumstance, the attention expanding is hard to cover the whole character, it can be hard for the refinement module to capture the actual body of the character.\\\textbf{Visualization of attention calibration}


% \begin{table*}
% \renewcommand\tabcolsep{7.5pt} % 调整表格列间的长度
%   \caption{contrastive study on recognition}
%   \label{contrastive_table2}
%   \begin{tabular}{lccccccc}
%     \toprule
%     \midrule
%     method & contrastive & training data& IIIT-5K & IC13 & IC15 & Svt-p & CUTE80\\
%     \midrule
%     CRNN &  & ST+MJ& 78.2 & 86.7 & 64.8 & 70.9 & 61.4\\
%     CRNN & \checkmark & ST+MJ& 79.6 & 89.0 & 65.4 & 72.3 & 63.5\\
%     SAR(our baseline) &  & ST+MJ& 92.8 & 90.6 & 78.4 & 85.8 & 86.1\\
%     SAR(our baseline) &\checkmark  & ST+MJ& 93.4 & 91.5 & 79.9 & 86.0 & 86.7\\
%     ABINet &  & ST+MJ& 96.4 & 96.9 & 84.8 & 88.5 & 89.8\\
%     ABINet & \checkmark & ST+MJ& 96.7 & 97.1 & 85.3 & 88.7 & 91.2\\
%     \midrule
%   \bottomrule
% \end{tabular}
% \end{table*}

% \begin{table}
% \renewcommand\tabcolsep{7.5pt} % 调整表格列间的长度
%   \caption{warm-up ablation}
%   \label{warmup_table}
%   \begin{tabular}{lcccc}
%     \toprule
%     \midrule
%     method & warmup  & jointly & acc. &  fIoU \\
%     \midrule
%     \midrule
%     Ours &  & \checkmark  & 78.36 & 52.30\\
%     Ours & \checkmark &   & 87.11 & 42.24\\
%     Ours & \checkmark& \checkmark & 93.65 & 70.98\\
%     \midrule
%   \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}
% \renewcommand\tabcolsep{7.5pt} % 调整表格列间的长度
%   \caption{warm-up ablation}
%   \label{warmup_table}
%   \begin{tabular}{lccc}
%     \toprule
%     \midrule
%     method & warmup & acc. &  fIoU \\
%     \midrule
%     \midrule
%     Ours &   & 78.36 & 52.30\\
%     Ours & \checkmark & 93.65 & 70.98\\
%     \midrule
%   \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}
% \renewcommand\tabcolsep{7.5pt} % 调整表格列间的长度
%   \caption{multi-scaled performance}
%   \label{multiscale_table}
%   \begin{tabular}{lcccc}
%     \toprule
%     \midrule
%     M1 & M2 & M3 &  acc. & fIoU \\
%     \midrule
%     \midrule
%     \checkmark &  &  &  91.26 & 69.52 \\
%      & \checkmark &  &  90.78 & 67.01 \\
%      &  & \checkmark &  90.15 & 65.54 \\
%     \checkmark & \checkmark &  &  92.21 & 70.36 \\
%     \checkmark &  & \checkmark &  92.36 & 69.98 \\
%      & \checkmark & \checkmark &  91.44 & 68.40 \\
%     \checkmark & \checkmark & \checkmark & 93.65 & 70.98 \\
%     \midrule
%   \bottomrule
% \end{tabular}
% \end{table}



\section{Conclusion}
In this paper, we make the first attempt to propose a weakly-supervised text instance segmentation method. Moreover, We propose a two-stage text adaptive refinement module TAR to generate high-quality pseudo labels. In addition, we develop a mask-augmented contrastive learning strategy and make it pluggable, boosting the performance of both our method and existing text recognition methods. The experimental results demonstrate that the proposed method significantly outperforms weakly-supervised segmentation methods on ICDAR13-FST by 18.95$\%$ and on TextSeg by 17.80$\%$, and it achieves 82\% of the fully supervised SOTA’s performance. Last but not least, the proposed method drives multiple downstream applications in practical scenarios.
% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
{\small
\bibliographystyle{IEEEbib}
\bibliography{icme2022template}
}
% \newpage
\appendix
\noindent \Large \textbf{Appendix}
\normalsize
\section{Down-stream Applications}
Compared with \cite{xu2021rethinking}, the proposed method, which can produce the text instance segmentation mask, provides more powerful and controllable operations for downstream applications.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{remove.jpg}
  \caption{An example of text instance removal.}
  \label{removal}
\end{figure}

\begin{table*}[ht]
\centering

% \renewcommand\tabcolsep{7.5pt} % 调整表格列间的长度
\renewcommand{\arraystretch}{1.0}
\caption{Runtime of all methods}
  \label{runtime}
  \scalebox{1.0}{
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    Methods & Zhou et al. \cite{zhou2018weakly} & Ahn et al. \cite{ahn2019weakly} & Cholakkal et al. \cite{cholakkal2019object}  & ShapeMask \cite{kuo2019shapemask} & BBAM \cite{lee2021bbam} & Ours\\
    \hline
    Runtime(ms) & 18.9 & 30.4 & 15.9  & 89.5 & 48.0 & 28.7\\
    \hline
\end{tabular}}
\end{table*}

\begin{table*}[ht]
\centering

% \renewcommand\tabcolsep{7.5pt} % 调整表格列间的长度
\renewcommand{\arraystretch}{1.0}
\caption{Adjusting of hyper-parameters $\lambda_{rec}$ and $\lambda_{c}$, the f-IoU performance is tested on TextSeg datasets.}
  \label{param}
  \scalebox{1.0}{
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{2}*{Params.}& $\lambda_{rec}$ $|$ $\lambda_{c}$  & $\lambda_{rec}$ $|$ $\lambda_{c}$ & $\lambda_{rec}$ $|$ $\lambda_{c}$  & $\lambda_{rec}$ $|$ $\lambda_{c}$  & $\lambda_{rec}$  $|$ $\lambda_{c}$ & $\lambda_{rec}$  $|$ $\lambda_{c}$& $\lambda_{rec}$  $|$ $\lambda_{c}$& $\lambda_{rec}$  $|$ $\lambda_{c}$ \\
    \cmidrule(r){2-9}
    ~ &  0.1 $|$  0.1 &  0.1 $|$ 1 &  1 $|$ 0.1  &   1 $|$  1 &  1 $|$  10 &  10 $|$ 10 &  0.01 $|$ 1 &  1 $|$ 100\\
    \hline
    f-IoU & 69.85 & 70.66 & \textbf{71.24} & 71.18 & 70.43 & 70.57 & 70.89& 69.01\\
    \hline
\end{tabular}}
\end{table*}

\textbf{Text Instance Removal} Scene text removal is urgently needed by some industries \textit{e.g.}, advertising, film, television, etc. In these scenarios, it is a challenging task to remove unwanted text in videos or images while keeping other contents unchanged. Although \cite{xu2021rethinking} achieves seemingly satisfactory text removal results, its application scenarios are limited due to its semantic segmentation nature, \textit{i.e.}, this method erases every text in the image, wanted or not. Different from previous methods, our method enables the user to remove specific unwanted text instances while leaving others untouched. The visualization of this application is demonstrated in Fig. \ref{removal}.

\textbf{Text Instance Style Transfer} Text segmentation can help transfer a font style to a given text image, which usually requires accurate text masks. In this work, we upgrade font style transfer to text-instance-level, which means that the user can transfer different styles to different text instances. An example of the text-instance-wise style transfer is illustrated in Fig. \ref{transfer}. By combining the controllable style transfer and text-instance removal, one can easily conduct data augmentation on text images in the wild to produce high-value synthetic datasets with complex real-world background for text-related researches.

\begin{figure}[ht]
  \includegraphics[width=\linewidth]{transfer.jpg}
  \caption{An example of text instance style transfer.}
  \label{transfer}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{HollowTrap.png}
  \caption{Case study of pseudo labels generated by rough attention location with different qualities. Rows titled `Normal' and `Hollow Trap' show normal cases, `Misalignment' denotes the failure case. Note that the visualization results are captured at the beginning of the joint training stage.}
  \label{warm-up}
\end{figure}

\section{Warm-Up and Joint Training}
\textbf{Importance of Warm-Up for Recognizer}
\label{discussion}
As mentioned above, the proposed method should warm up the recognizer in advance. Since pseudo labels for the segmentation module derive from the attention maps generated by the recognizer, the refinement module will not produce effective pseudo labels if the recognizer has a wrong awareness of text instance position, as vividly shown in Fig. \ref{warm-up}. The bottom row shows that the refinement module produces imprecise pseudo labels based on misaligned attention maps, which will further mislead the segmentation module. Therefore, the warm-up strategy is essential for the proposed method to generate accurate attention localization. Additionally, as in the middle row, an interesting observation is that for text instances like `C`, 'O', and 'D', the refinement of their initial attention sometimes gets misled by the hollow structures of their own (inevitable), and it will take a long period of joint training for the model to manage escaping from such hollow traps.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{iters0815.png}
  \caption{The visualization of attention location during joint training. The attention is binarized for better understanding.}
  \label{joint-training}
\end{figure}

\textbf{Necessity of Joint Training}
After warming up the recognition module, the segmentation module will be jointly trained to achieve better performance. At the warm-up stage, the recognizer can generate a rough location for each text instance (Fig. \ref{joint-training}(b)). However, unlike CAMs \cite{zhou2016learning}, the attention mechanisms for text recognizers tend to focus on the center of text instance. Refining such focused attention as initial seeds cannot completely perceive the instance boundary. Therefore, when the recognition module and segmentation module are jointly trained, the attention maps generated by the recognizer gradually learn to be closer to the shape of the corresponding text instance (shown in Fig. \ref{joint-training}(c) and Fig. \ref{joint-training}(d)), which results in pseudo labels with better quality for the segmentation module. In short, the effectiveness of our method is based on such mutual enhancement between attention maps and pseudo labels. Additionally, the ablation experiments also validate the necessity of joint training. It also needs to be mentioned that we observed the recognition loss fluctuates greatly at the beginning of the joint training. The phenomenon, most likely caused by the reshaping of attention maps, will vanish with longer training.
% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------

\textbf{Text Recognition Data for Warm-up Training}
For the warm-up of text recognition datasets, we use Syn90K \cite{jaderberg2014synthetic}, SynthText \cite{gupta2016synthetic}, IC13 \cite{karatzas2013icdar}, etc, which follows the fashion of recent text recognition methods. In addition, unlabelled datasets, \textit{i.e.}, BOOK32 \cite{iwana2016judging}, TextVQA \cite{singh2019towards} and ST-VQA \cite{biten2019scene}, are used through a semi-supervised training strategy. To enable semi-supervised training, all we need is to switch the text recognition module in evaluation mode.



\section{Additional Experiments}
We put the experiment results of the empirical configuration of weighting balance and inference time in Tab. \ref{param} and Tab. \ref{runtime}.
\end{document}
