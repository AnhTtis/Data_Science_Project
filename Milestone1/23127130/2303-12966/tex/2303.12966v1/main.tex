%%%% useful links:
%% logic latek symbols: https://tex.stackexchange.com/questions/85006/writing-linear-temporal-logic-in-latex


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[journal,twoside,web]{ieeecolor}
% \documentclass[12pt,draftcls,onecolumn]{IEEEtran}
\usepackage{lcsys}

% \overrideIEEEmargins
% \let\proof\relax
% \let\endproof\relax
% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%     T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


% The following packages can be found on http:\\www.ctan.org
\usepackage{graphicx} % for pdf, bitmapped graphics files

\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{mathrsfs}

\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{cite}
% \usepackage[noend]{algpseudocode}
\usepackage{algpseudocode}[noend]

\usepackage{soul}
% \usepackage{color}
% \usepackage[dvipsnames]{xcolor}
% \usepackage{sectsty}

% \newcommand{\blue}[1]{{\normalsize{{\color{blue}#1}}}}
% \newcommand{\probs}{\mathbb P}
% \newcommand{\green}[1]{{\normalsize{{\color{ForestGreen}#1}}}}
% \newcommand{\red}[1]{{\normalsize{{\color{red} #1 }}}}
% \newcommand{\golden}[1]{{\normalsize{{\color{Goldenrod} #1 }}}}

% \newcommand\hcancel[2][black]{\setbox0=\hbox{$#2$}%
% \rlap{\raisebox{.45\ht0}{\textcolor{#1}{\rule{\wd0}{1pt}}}}#2} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[linkcolor=blue,colorlinks=true]{hyperref}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{problem}{Problem}
\newtheorem{remark}{Remark}
\newtheorem{conjecture}{Conjecture}
\newtheorem{property}{Property}
\newtheorem{assumption}{Assumption}
\newtheorem{claim}{Claim}
\newtheorem{example}{Example}

\usepackage{subcaption}

\newtheorem{objective}{Objective}

\usepackage{caption}
\DeclareCaptionType{equ}[][]

\graphicspath{figures/} %Setting the graphicspath

\title{\LARGE \bf
Rate-Tunable Control Barrier Functions: \\Methods and Algorithms for Online Adaptation
%Data-driven Robustness Metric for STL specifications based on CBFs
}
\author{Hardik Parwana, Dimitra Panagou
\thanks{This work was partially sponsored by the Office of Naval Research (ONR), under grant number N00014-20-1-2395. The views and conclusions contained herein are those of the authors only and should not be interpreted as representing those of ONR, the U.S. Navy or the U.S. Government.}
\thanks{Hardik Parwana and Dimitra Panagou are with Department of Robotics, University of Michigan, MI 48105, USA (email: hardiksp@umich.edu, dpanagou@umai.edu). }}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\s}{\mathcal{S}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\p}{\mathbb{P}}
\newcommand{\K}{\mathcal{K}}
% \newcommand{\classK}{class-$\K$ }
\newcommand{\I}{\mathcal{I}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\classK}{\mbox{class-$\K$} }
\newcommand{\classKinf}{\mbox{class-$\K_\infty$} }
\newcommand{\Int}{\text{Int} }
\newcommand{\Cl}{\text{Cl} }
\newcommand{\E}{\mathbb{E} }
\newcommand{\eqn}[1]{\begin{align}#1\end{align}}
\newcommand{\D}{\partial}
\newcommand{\Ub}{{\bf U}}
\newcommand{\Fb}{{\bf F}}
\newcommand{\Gb}{{\bf G}}
\newcommand{\vbar}{~ | ~}
\newcommand{\vphi}{\varphi}
% \newcommand{\red}[1]{{\normalsize{{\color{red} #1 }}}}

% \newcommand{\red}[1]{\textcolor{red}{#1}}
% \newcommand{\blue}[1]{\textcolor{blue}{#1}}
% \newcommand{\green}[1]{\textcolor{green}{#1}}

%https://tex.stackexchange.com/questions/5223/command-for-argmin-or-argmax
\newcommand{\argminF}{\mathop{\mathrm{argmin}}\limits}

\begin{document}


\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
Control Barrier Functions offer safety certificates by dictating controllers that enforce safety constraints. However, their response depends on the $\classK$ function that is used to restrict the rate of change of the barrier function along the system trajectories. This paper introduces the notion of Rate Tunable Control Barrier Function (RT-CBF), which allows for online tuning of the response of CBF-based controllers. In contrast to the existing CBF approaches that use a fixed (predefined) $\classK$ function to ensure safety, we parameterize and adapt the $\classK$ function parameters online. Furthermore, we discuss the challenges associated with multiple barrier constraints, namely ensuring that they admit a common control input that satisfies them simultaneously for all time. In practice, RT-CBF enables designing parameter dynamics for (1) a better-performing response, where performance is defined in terms of the cost accumulated over a time horizon, or (2) a less conservative response. We propose a model-predictive framework that computes the sensitivity of the future states with respect to the parameters and uses Sequential Quadratic Programming for deriving an online law to update the parameters in the direction of improving the performance. When prediction is not possible, we also provide point-wise sufficient conditions to be imposed on any user-given parameter dynamics so that multiple CBF constraints continue to admit common control input with time. Finally, we introduce RT-CBFs for decentralized uncooperative multi-agent systems, where a trust factor, computed based on the instantaneous ease of constraint satisfaction, is used to update parameters online for a less conservative response.
\end{abstract}

\section{Introduction \& Previous Works}

% \begin{itemize}
%     \item what CBF means
%     \item drawbacks: input bounds, alpha, multiple time-varying barriers
%     \item adaptive frameworks
%     \item myopicness
% \end{itemize}

The notion of a control barrier function (CBF) has emerged as a tool to ensure constraint satisfaction for dynamical systems. The principle is as follows: Given a constraint function, termed barrier function thereafter, whose zero super-level sets define a constrained set, termed also the safe set, %whose value encodes proximity to constraint violation, 
one restricts the rate of change of the barrier function along the system trajectories using $\classK$ function of the barrier function \cite{ames2016control, ames2019control}. If such a condition can be satisfied everywhere in the constrained set under the given dynamics and control input constraints, then the barrier function is called a CBF, and the constrained set is forward invariant. This method, in conjunction with control Lyapunov functions (CLFs) for stability, has been employed to design safe controllers for several applications.

\subsection{Background and open challenges}
%[Issues with CBFs] 
Despite significant extensions of the CBF concept to address various challenges (e.g., modeling/parameter uncertainty, input constraints) over the last few years, we highlight three unsolved issues. 

\subsubsection{Finding valid CBFs} Given a single constraint function, finding a suitable (valid) CBF reduces in effect to finding both a barrier function and a $\classK$ function so that the CBF condition can be realized for given dynamics and control input bounds. This task is non-trivial and several approaches, mostly offline, have been proposed recently\cite{dawson2023safe,jagtap2020control}. When the safe set is represented as an intersection of the sets defined by several constraints, for example, when there are multiple unsafe regions such as obstacles, the objective becomes finding a controller that simultaneously ensures the invariance of the intersection of the sets. Existing CBF implementations either combine all barriers into a single barrier function \cite{glotfelter2017nonsmooth, stipanovic2012monotone, panagou2013multi} resulting in a single CBF derivative condition, or consider each barrier separately\cite{usevitch2020strong,xiao2020feasibility,wang2022ensuring} thereby imposing multiple CBF derivative conditions simultaneously. Either case only adds to the complexity of finding valid CBFs.

\subsubsection{Feasibility and optimality} CBF controllers are in principle obtained as the result of a Quadratic Program (QP) searching for the point-wise minimum-norm control input, and therefore are myopic, i.e., only point-wise optimal. This can lead to sub-optimal performance in the long term. In other words, the point-wise solutions can be inferior in terms of performance compared to trajectories obtained by the exact solution of the Infinite or Finite Horizon Optimal Control \cite{cohen2020approximate,parwana2022recursive} problem. 
%Another reason is that CBFs are only a sufficient condition and not a necessary condition \textbf{FOR WHAT?} and therefore do not represent all possible trajectories \textbf{FOR WHAT? OF WHAT? YOU HAVE TO BE SPECIFIC}. 
%Both the myopic-ness \textbf{THIS IS NOT EVEN A TECHNICAL TERM. CALL IT POINTWISE OPTIMALITY OR SOMETHING MORE RIGOROUS} and sufficient-but-not-necessary limitations \textbf{THE PREVIOUS REASON IS NOT RELATED TO INFEASIBILITY, REMOVE} 
Furthermore, imposing multiple CBFs and control input bounds in the optimization problem can lead to infeasibility of the solution (controller) at the current or at some future time. %This often happens when there are control input bounds and multiple state constraints in the system. 
We call this problem ``lack of persistent feasibility" of the low-level CBF controller. Note that a good choice of CBF parameters may lead to persistent feasibility, whereas a poor choice may not; however, finding a good choice of parameters is not a trivial problem. 
%Fourth, \textbf{THE REST TWO ARE GIVEN VERY SHORT DESCRIPTIONS, ELABORATE MORE}Fourth, there might be uncontrollable elements in the system. Finally, can we adapt our controllers online? that is, how can we auto-tune online?

% \textbf{INCOMPLETE:} Before embarking on an extensive literature survey, we would like to use an analogy to give an intuition of how we solve this problem. We do prediction and adaptation as well as Now suppose the intention of other agents is not clear. How do we tune our controller.

%[Higher Order]
\subsubsection{High relative degree}
The notion of CBFs as first introduced in \cite{ames2016control} applies to constraints of relative degree $r=1$ with respect to the system dynamics, i.e., to constraints whose first-order derivative is a function of the control input. The concept is extended to constraint functions of higher relative degree $r>1$, i.e., to the case where the control input appears in the $r$-th -order derivative of the constraint function. \cite{nguyen2016exponential} proposed the class of Exponential CBFs when the $\classK$ functions used in CBF derivative condition are linear in their argument. They employ input-output linearization and design controllers via pole placement so that the barrier state is stabilized to zero. \cite{xiao2019control} generalizes Exponential CBFs to generic nonlinear $\classK$ functions in the form of Higher-Order CBFs (HOCBF). \cite{taylor2022safe} proposes an alternate Lyapunov backstepping-based design for higher-order systems that always find a first-order barrier function. 
% TODO: forward completeness
%While the previous approaches assumed \red{(Lipschitz continuous controller??)} forward completeness, i.e., the existence of a solution for all time, \cite{tan2021high} further extends HOBCF to cases when forward completeness is not guaranteed \textbf{HOW DO THEY DO THIS? I AM NOT SUPER SURE ABOUT THIS ARGUMENT} and when the relative degree is not uniform.
%[Multiple]
% \subsubsection{Multiple CBFs}\textbf{THIS IS NOT REALLY A CONTRIBUTION: RESTATE:}
% \textbf{The paragraph above about multiple reads a bit repetitive to the first one. Can you talk about multiple functions only at one place?}
% \textbf{Here you present literature review related to the earlier problems, from what I understand. I would suggest either you move the corresponding solutions to the corresponding problems above, OR have a section like below BUT WITH PROPER SUBSECTIONS -- CHECK THE NAMES I GAVE:}

\subsection{Current approaches and limitations}
\label{section::review_limitations}
\subsubsection{Finding valid CBFs} In recognition of the importance of finding a valid control barrier function, several works have also attempted to learn a barrier function, or to find the domain over which a given barrier function satisfies the CBF condition. \cite{agrawal2021safe} does offline analysis to find the domain over which CBF condition can be satisfied in the presence of input bounds. \cite{robey2020learning, lindemann2021learning} perform offline analysis to find a barrier function that satisfies the CBF derivative condition. A more thorough survey on learning-based methods can be found in \cite{dawson2022safe}. 
%\cite{srinivasan2020synthesis} learns a barrier function from lidar measurements when the safe set is unknown. 
% TODO: mention that people just use same alpha elsewhere
%\cite{wang2021learning} learns the unknown terms in barrier function rates resulting from unknown dynamics. 
%Both of the above works either assume that the learned barrier function is also a control barrier function, i.e., the CBF derivative condition can be satisfied for all control inputs at all states. The learning of valid CBF is done in  

%Still, even if one constraint is considered at a time, the problem of finding a valid CBF for each such constraint remains.
When multiple constraints are present, \cite{stipanovic2012monotone, panagou2013multi} combine barrier functions into a single barrier constraint through smoothed minimum (or maximum) operator. \cite{black2022adaptation} proposes a novel weighted-summation technique, where the weights are tuned for performance and to avoid singular states where the control input has no contribution to barrier function derivatives (usually the latter is only assumed, but not guaranteed, to not happen in the literature). The authors in \cite{glotfelter2017nonsmooth} perform non-smooth analysis to construct a barrier function with the non-smooth minimum (or maximum) operation. The aforementioned approaches thus either involve smoothing approximations or require intricate non-smooth analysis. When multiple CBF derivative conditions are considered simultaneously instead as in 
\cite{usevitch2020strong, xiao2020feasibility,wang2022ensuring},  \cite{tan2022compatibility} proposes an offline sampling-and-grid refinement method to search the domain of state space over which multiple control barrier functions are compatible.
%However, since multiple constraints are not guaranteed to be 
%compatible to allow a solution to exist for all states in the domain, 
%\cite{aali2022multiple} also imposes multiple CBFs in a QP, and analyzes the continuity property of the solution. 
Multiple CBF constraints also arise naturally in multi-agent scenarios. \cite{lindemann2019control,lindemann2020barrier} encode Signal Temporal Logic (STL) tasks and safety specifications as CBF constraints. \cite{borrmann2015control, usevitch2021adversarial} propose CBFs for safe swarm behavior in the presence of uncooperative agents when their identities are known in advance. These uncooperative agents serve as an excellent example of uncontrollable systems, where different CBF $\classK$ functions are suited to controllable and uncontrollable dynamics. The safe sets are also time-varying for the controllable dynamics, which 
%is something that 
most of the aforementioned papers have not considered in their theoretical analysis. \cite{usevitch2021adversarial} designs a robust CBF controller for systems with adversarial agents without detecting malicious agents. \cite{mustafa2022adversary} detects malicious agents by checking for collisions in the predicted future wherein the ego and target agents, forgoing their other objectives, try to only maintain and violate respectively, the CBF derivative constraint. Both the above-mentioned approaches are conservative due to the consideration of worst-case bounds. % and the same $\classK$ function in CBF derivative condition for all their neighbors.

\subsubsection{Feasibility} All of the aforementioned works use a constant $\classK$ function over the whole domain, and its parameters are often pre-tuned manually before deploying for an actual application. However, since it is not possible to test each and every scenario in the offline phase, there is a dire need for online adaptation, especially to maintain the feasibility of the controller in the presence of multiple constraints and input bounds.
%CBFs, being myopic controllers, give suboptimal performance over a time horizon. To achieve acceptable performance, the $\classK$ functions are often manually tuned based on observed behavior before deploying in real applications. 
Several works relax the CBF constraint in the QP to ensure that the QP controller has a solution. \cite{zeng2021safety} proposes an optimal-decay CBF-QP that multiplies the $\classK$ function by a scalar variable that is optimized for (along with the control input) in the QP. \cite{wang2022ensuring}, \cite{garg2019control} introduced similar relaxation strategies for CBF constraints with the subtle difference being that instead of linear multiplicative factor to generic $\classK$ function, they only consider linear $\classK$ functions and treat the $\classK$ function parameter as an optimization variable. \cite{seo2022safety} considers control non-affine dynamics and in light of the resulting loss of linearity of the CBF constraint, chooses to impose it through a penalty term in the cost function, thereby converting it into an unconstrained optimization problem.

%%%%%%%%%

The aforementioned adaptive approaches can be thought of as adapting \textit{on a need basis}; i.e., they relax the CBF constraint whenever the original $\classK$ function leads to infeasibility. However, persistent feasibility is not guaranteed for multiple CBF constraints in the presence of input bounds. Since they also solve for the control input point-wise (via a QP), they are still sub-optimal in the long term. Among the non-myopic approaches, \cite{xiao2020feasibility} multiplies a scalar penalty to the $\classK$ functions, and creates a dataset of persistently feasible and eventually infeasible parameters offline by randomly sampling parameters and simulating the system forward. Then they fit a differentiable classifier that is used to impose the resulting feasibility margin as another HOBCF constraint in the system. \cite{ma2022learning} performs offline learning to learn the best-performing parameters of the $\classK$ function as a function of the state, although without consideration of feasibility. 
%Even with fixed $\classK$ function, some predictive approaches have been able to reduce ``myopic-ness" of CBF controllers when performance is considered over a time horizon.

 %[mention that optimality means over time horizon]
% [Predictive]
\subsubsection{Optimality} In an effort to produce optimal trajectories over a time horizon with the hitherto mentioned approaches, predictive frameworks similar to Model Predictive Control have inspired several developments. \cite{wu2019control}, \cite{zeng2021safetycbf} impose control barrier function constraints for every state and input pair in an MPC formulation. 
%\cite{zeng2021safetycbf} employs CBF constraints in MPC. 
\cite{zeng2021enhancing} employs CBFs with penalty $\classK$ functions from \cite{zeng2021safety} to specify safety constraints at each time step of the MPC formulation to resolve the trade-off between feasibility and safety, and enhance them both at the same time. \cite{grandia2021multi} shows that, for legged robots, using CBF constraints in both the MPC-based high-level planner and the low-level controller leads to the best results in terms of number of missteps. While the aforementioned approaches report improvement in performance, incorporating CBF constraints in MPC usually results in nonlinear optimization that typically cannot be solved fast enough to replace low-level controllers running at, for example, 40Hz or more. The low-level CBF controllers can also be made less myopic by taking incorporating future predictions in their design. \cite{black2022future} propose a future-focused CBF, wherein they construct a CBF based on the minimum distance to the boundary of unsafe set in the future predicted based on zero acceleration policy. \cite{breeden2022predictive} also proposes a predictive control barrier function (PCBF) that, given a nominal trajectory in terms of sequence of control inputs, predicts the future and upon encountering a safety violation, uses the sensitivity of state at the future instant of safety violation w.r.t the first input in the sequence to reduce the violation. 
% TODO: mention Zellinger's predictive CBF??
%The above two approaches \cite{wabersich2022predictive}(Melanie Zellinger)\red{this one not relevant probably}.


% [Limitations]
All of the aforementioned works except for \cite{black2022future} and \cite{breeden2022predictive} perform offline analysis (training or verification) to find a control barrier function. Since it is computationally infeasible to simulate all possible scenarios offline that the robotic agent may be subjected while operating in real-world, there is a need for online learning and adaptation in face of unforeseen environments. This need can result from, e.g., a varying number of agents in a multi-agent scenario, or time-varying safe sets that the robot is subjected to based on the user's whim, such as changing collision radius. Most of the learning and verification-based frameworks \cite{dawson2023safe,jagtap2020control} also focus on learning a barrier function that can satisfy the CBF derivative condition for a fixed $\classK$ function. However, $\classK$ can not only drastically change the realizable safe set, but also affects the performance in the long term. The methods that do adapt $\classK$ function online, do so through a relaxation that does not help improve the optimality of the resulting trajectory or guarantee persistent feasibility in presence of time-varying multiple barrier constraints and input bounds. Therefore, a generic framework to handle multiple CBF constraints and ensure feasibility in an online phase does not exist. 

\subsection{Contributions}
% [Our Contribution]

% \textbf{The contributions have to become more to-the-point and clear. Write the contributions based on the outline of the technical sections. What is each section about? WHAT ARE THE MAIN PROVABLE/RIGOROUS RESULTS IN THE PAPER? DO NOT LIST ANY DISCUSSIONS REMARKS AND THINGS THAT YOU HAVE WITHOUT PROOFS HERE. That will help you make some key points here. Put them in bullet points if needed. Right now the reviewer can not understand what is really the problem you solved and what the paper offers. YOU TALK ABOUT TWO METHODS. Is it two methods for the same problem? OR Do you have many problems and one method per problem? Since the paper is really long, you have to spend some time explaining things clearly otherwise the reviewers will find an easy reason to reject it. }
Building upon our previous work \cite{parwana2022recursive,parwana2022trust}, we introduce a new notion of a Rate-Tunable Control Barrier Function (RT-CBF) in Section \ref{section::RT-CBF} that allows for a time-varying $\classK$ function while ensuring safety.
%Note that a barrier function that does qualify as CBF can still qualify as RT-CBF, and hence be used to ensure the safety of the system. 
In practice, this allows consideration of parametric $\classK$ functions and adaptation of their parameters online. This adaptation can be used to achieve better performance over a finite-time horizon, or make the controller less or more conservative without jeopardizing safety. It is also noteworthy that this adaptation facilitates the consideration and satisfaction of multiple time-varying barrier constraints, %and, similar to our previous work, rather than combining multiple barrier functions into a single barrier function, we choose to formulate multiple CBF derivative constraints and impose them simultaneously. Combining barriers into a single barrier constraint may reduce computational load but either requires the smooth approximation of minimum operation or consideration of non-smooth control theory as mentioned in Section \ref{section::review_limitations}. Furthermore, we believe that a combination of barrier functions is 
by making them easier to tune for performance, especially when they do not represent similar physical quantities (e.g., when imposing constraints on the rotational dynamics, and constraints on the translational dynamics for a quadrotor). Then, in Section \ref{section::problem_formulation}, we pose the problem of guaranteeing the existence of a solution to CBF-QP controller as persistently ensuring \textit{compatibility} between multiple CBF constraints along system trajectories. We then establish the necessity and sufficiency of our notion of safety in Section \ref{section::RT-CBF}, and propose two approaches to design dynamical laws that govern the update of the parameters of the $\classK$ function. The first is a model-predictive approach in Section \ref{section::predictive_framework} that, given a performance metric defined as a summation of stage-wise cost, computes the sensitivity of predicted future states w.r.t. the parameters of the $\classK$ function. We use these sensitivities for a Sequential Quadratic Programming (SQP) \cite{boggs1995sequential,tits2009feasible} inspired gradient descent law to update the parameters online in direction of improving performance metric. The prediction and sensitivity computation is shown to be amenable to the backpropagation type of gradient computation over a computational graph. Infinitesimal improvements in future trajectory is also a common idea in Differential Dynamic Programming (DDP)\cite{tassa2014control} and thus we bring together elements from DDP and SQP to develop an auto-tuning mechanism for CBF controllers. We also address the issue of ensuring compatibility among multiple barrier constraints along the system trajectory. 
When the future prediction is not possible, our second approach in Section \ref{section::pointwise_sufficient} provides point-wise sufficient conditions based on local Lipschitz constants to be imposed on any given parameter update rule to ensure persistent compatibility of the multiple constraints along the system trajectories. Although this does not provide specific update rules for parameters and is only sub-optimal owing to its dependence on current state only, it  
%We provide sufficient conditions on $\classK$ function dynamics for the persistent feasibility of the CBF controller. 
%One immediate advantage is that the 
allows the user to decide on any rule, even heuristically, for updating the $\classK$ function, and project it to a set of feasible update rules using our sufficient conditions to ensure persistent feasibility whenever these conditions are well defined.
%This is useful when future prediction is not possible and our first method in Section \ref{section::predictive_framework} cannot be applied. 
%While our conditions are computed based on the current state only, and do not constitute an optimal update law, they still have practical significance for implementation in very low-cost hardware.
%In the first approach, we propose a predictive framework that uses the sensitivity of future states w.r.t $\classK$ function parameters to improve the long-term objective. When, over the period of interest, persistent feasibility is already observed \textbf{REPHRASE:} in prediction, the objective becomes performance, i.e., the summation of stage-wise cost over a finite time horizon. This sensitivity-based update law provides us with gradients that can be used to update parameters online. Updating parameters in direction of improving performance does not ensure that {\color{blue}new values of the parameters will still satisfy constraints that were compatible with the previous values of the parameters.} Therefore, we propose a constrained gradient-descent (GD) update law, inspired by Sequential Quadratic Programming(SQP). 
%When infeasibility is observed in the prediction, the objective becomes the infeasibility margin of the violated constraint, and the same constrained GD law is used to reduce the infeasibility margin. Our approach shared similarities with differential dynamic programming (DDP), which also finds the optimal trajectory by iteratively linearizing about the current best trajectory and using sensitivity to improve it. 
As part of an illustrative case study, we then introduce RT-CBFs for decentralized uncooperative multi-agent systems in Section \ref{section::multi-agent} based on our previous work in \cite{parwana2022trust}. We design the parameter dynamics based on a trust factor that is computed based on the instantaneous ease of satisfaction of the CBF constraints. We also provide a way to incorporate prior beliefs of how other agents are moving in our trust design. This gives robots reasoning abilities to make their controller more or less conservative against adversarial, uncooperative, or cooperative agents whose identities are unknown a priori. 
%TODO: add terminal cost function??
%Finally, we also use this trust-based value function as the terminal stage cost function in the finite horizon predictive framework of our first approach (or directly in the MPC framework). In time-varying systems, finite horizon approaches can lead to issues because of being myopic and a terminal stage cost function based on belief and trust can help take into account \textit{anticipation} of what might happen in the future.

\subsection{Comparison to Earlier Work}
The work most similar to our notion of CBF is \cite{xiao2021adaptive}, wherein they introduce a time-varying \textit{penalty parameter} that multiplies to the fixed $\classK$ function. The penalty parameter is allowed to change with time but is constrained to be non-negative and hence is itself posed as higher-order CBF. \cite{xiao2021adaptive} then proposes another low-level optimization problem that finds the penalty parameter closest to a nominal parameter value subject to HOCBF constraints. This work does not consider multiple barrier functions simultaneously. And while they do consider control input bounds, the proof for persistent feasibility (existence of a solution to HOCBF QP for all future times) is based on the assumption that the original MPC-like formulation always admits a solution irrespective of the initial state, which is possible only because the environment (and hence the constraints) is time-invariant. If constraints are time-varying then, in general, unless the correct actions are taken from the start, the system trajectory may enter states in the future where no feasible control input exists anymore.

Compared to our own previous work in \cite{parwana2022recursive,parwana2022trust}, we make the following new contributions: (1) Modification of the definition of RT-CBFs to represent a weaker safety condition with more practical importance. (2) Proof of necessity and sufficiency of RT-CBFs, and finally, (3) Consideration of control input bounds and constraints of higher relative degree with respect to the system dynamics and evaluation of our methods on several new simulation case studies, including multi-agent systems with higher-order barrier functions. 

The rest of the paper is structured as follows: our notations and some preliminary information is discussed in Sections \ref{section::notation} and \ref{section::preliminaries}. Section \ref{section::problem_formulation} formulates our problem and Section \ref{section::RT-CBF} introduces our notion of RT-CBFs. We then propose our two methods of designing $\classK$ function dynamics in Section \ref{section::dynamics_design} and introduce RT-CBFs for multi-agent systems in Section \ref{section::multi-agent}. Finally, we present simulation results in Section \ref{section::simulation} and conclude the paper in Section \ref{section::conclusion}.


\section{Notations}
\label{section::notation}
The set of real numbers is denoted as $\reals$ and the non-negative real numbers as $\reals^+$. Given $x\in \reals$, $y \in \reals^{n_i}$, and $z\in \reals^{n_i\times m_i}$, $|x|$ denotes the absolute value of $x$ and $||y||$ denotes $L_2$ norm of $y$. The interior and boundary of a set $\C$ are denoted by $\textrm{Int}(\C)$ and $\partial \C$. For $a\in \reals^+$, a continuous function $\alpha:[0,a)\rightarrow[0,\infty)$ is a $\classK$ function if it is strictly increasing and $\alpha(0)=0$.  A continuous function $\alpha:(-b,a)\rightarrow (-\infty,\infty)$ for $a,b\in \reals^+$ is an extended $\classK$ function if it is strictly increasing and $\alpha(0)=0$. Furthermore, if $a=\infty$ and $\lim_{r\rightarrow \infty} \alpha(r)=\infty$, then it is called extended class-$\mathcal{K}_\infty$. 
%A continuous function $\beta:[0,b)\rightarrow [0,\infty)$ for some $b>0$ is said to belong to class $\mathcal{KL}$ if for each fixed $s$, the mapping $\beta(r,s)$ belongs to class-$\mathcal{K}$ w.r.t $r$ and for each fixed $r$, the mapping $\beta(r,s)$ is decreasing w.r.t $s$ and $\beta(r,s)\rightarrow 0$ as $s\rightarrow \infty$. 
The $k^{th}$ time derivative of a function $h(t,x):\reals^{+}\times \reals^{n}\rightarrow \reals$ is denoted as $h^{(k)}$. 
%The first and second-order derivatives are also denoted as $\dot{h}$ and $\Ddot{h}$ respectively.
For brevity, we will refrain from mentioning explicit arguments whenever the context is clear. For example, $h(x)$ may simply be denoted as $h$. The Lie derivative of a function $h$ w.r.t a function $f$ is denoted as $L_f h = \frac{\partial h}{\partial x}f$. The angle between $\gamma$ two vectors $a,b\in \reals^n$ is given by $\cos \gamma=\frac{a^T b}{||a||~||b||}$ and the inner product between them is denoted as $\langle a, b \rangle$.

\section{Preliminaries}
\label{section::preliminaries}
\subsection{System Description}
Consider a nonlinear dynamical system
\eqn{
\dot{x} = f(x) + g(x)u,
\label{eq::dynamics_general}
}
where $x\in \X \subset \reals^{n}$ and $u \in \U \subset \reals^{m}$ represent the state and control input, and  $f:\X\rightarrow \reals^{n}$ and $g:\X\rightarrow \reals^{m}$ are locally Lipschitz continuous functions. The set $\s(t)$ of allowable states at time $t$ is specified as an intersection of $N$ sets $\mathcal S_i(t),i\in\{1,2,..,N\}$, each of which is defined as the 0-superlevel set of a continuously differentiable function $h_i:\reals^+ \times \mathcal{X} \rightarrow \reals$ as:
\begin{subequations}
    \begin{align}
        \s_i(t) & \triangleq \{ x \in \X : h_i(t,x) \geq 0 \}, \label{eq::safeset1} \\
        \partial \s_i(t) & \triangleq \{ x\in \X: h_i(t,x)=0 \}, \label{eq::safeset2}\\
        \Int (\s_i)(t) & \triangleq \{ x \in \X: h_i(t,x)>0  \} \label{eq::safeset3}
    \end{align}
    \label{eq::safeset}
\vspace{-0.8cm}
\end{subequations}
\subsection{Control Barrier Function}
Definitions \ref{definition::cbf_definition} and \ref{definition::hocbf_definition} provided below were presented in their original papers for the time-invariant safe sets $\s_i$ but we note that an extension to the time-varying case can be proven with Nagumo's theorem applied to non-autonomous systems\cite[Theorem 3.5.2]{carja2007viability} and hence we directly present that. Henceforth, we shall also refer to \eqref{eq::cbf_derivative} as the \textit{CBF derivative condition}.

\begin{definition}
\label{definition::cbf_definition}
(Control Barrier Function) \cite{ames2016control} For the dynamical system (\ref{eq::dynamics_general}), $h_i$ is a control barrier function (CBF) on the set $\s_i$ defined by (\ref{eq::safeset1})-(\ref{eq::safeset3}) if there exists an extended class-$\mathcal{K}$ function $\alpha : \reals \rightarrow \reals^+$ and a set $\mathcal{D}$ with $\mathcal{S}_i\subseteq \mathcal{D} \subset \reals^n$ such that 
\eqn{
 \sup_{u\in \mathcal{U}} \left[ \frac{\partial h_i(t,x)}{\partial t} + L_f h_i(t,x)+ L_g h_i(t,x)u \right] \geq -\alpha_i(h_i(t,x)) \nonumber \\
 \forall x\in \mathcal{D}, \forall t>0.
  \label{eq::cbf_derivative}
}
\end{definition}

\begin{theorem}(Set Invariance) \cite{ames2016control}
Given the dynamical system (\ref{eq::dynamics_general}) and a set $\s_i$ defined by (\ref{eq::safeset1})-(\ref{eq::safeset3}) for some continuously differentiable function $h_i:\reals^+ \times \reals^n\rightarrow \reals$, if $h_i$ is a control barrier function defined on set $\mathcal{D}$ with $\mathcal{C}\subseteq \mathcal{D} \subset \reals^n$, then $\s$ is forward invariant.
\end{theorem}

When $L_g h_i = 0$, the control input $u$ does not appear in $\dot{h}_i$. Suppose the relative degree of the function $h_i$ w.r.t the control input $u$ under the dynamics \eqref{eq::dynamics_general} is $r_i$. We can then define $r_i$ derived barrier functions as follows:
\begin{subequations}
    \eqn{
   \psi_i^0(t,x) &= h_i(t,x), \\
   \psi_i^k(t,x) &= \dot \psi_i^{k-1}(t,x) + \alpha_{i}^k (\psi^{k-1}_i(t,x)), \; k\in \{1,2,..,r_i\},
}
\label{eq::hobcf_barriers}
\end{subequations}
with corresponding sets 
\eqn{
  \mathscr{C}_i(t) = \{x \; | \; \psi_i^t(t,x)\geq 0, \forall k\in {1,..,r_i} \}, \mathscr{C}(t) = \cap_{i=1}^{N} \mathscr{C}_i
  \label{eq::hobcf_intersection}
}
\begin{definition}(HOCBF CBF)\cite{tan2021high}
\label{definition::hocbf_definition}
    The $r^{th}$ order differentiable function $h_i(t,x):\reals^+\times \reals^{n}\rightarrow \reals$ is a Higher-Order CBF (HOCBF) on the set $\mathscr{C}_i$ if there exist extended $\classK$ functions $\alpha_i^k:\reals \rightarrow \reals,  k=1,2,..,r$, and an open set $\mathscr{D}\subset \reals^+ \times \reals^n$ with $\mathscr{C}_i \subset \mathscr{D} \subset$ such that
     \eqn{
      \psi_i^r(t,x) \geq 0, ~ \forall x \in \mathscr{D}, \forall t\geq 0.
      \label{eq::hobcf_cbf_derivatrive}
    }
\end{definition}
In this work, we consider parametric $\classK$ functions $\alpha_i^k$ whose parameters are denoted by vector $\theta_{\alpha_i^k}$. For example, a linear $\classK$ function 
\eqn{
  \alpha_i^k(h) = \nu_i^k h, \; \nu_i \in \reals
  \label{eq::linear_classK_example}
}
is described by its constant parameter $\theta_{\alpha_i^k} = \nu_i^k$. The $\classK$ function may then be described as $\alpha_i^k(\theta_{\alpha_i^k},h)$ where we may skip mentioning $\theta_{\alpha_i^k}$ as an argument at every mention of $\alpha_i^k$ for brevity. The vector concatenation of the parameters of all $\classK$ functions $\alpha_i^k, i\in \{1,..,N\}, k\in \{1,..,r_i\}$ is denoted as $\theta_{\alpha_i}=[\theta_{\alpha_i^1},...,\theta_{\alpha_i^{r_i}}]^T$ and $\theta_{\alpha} = [\theta_{\alpha_1}, ..., \theta_{\alpha_N}]^T$. While the number of parameters depend on the parametrization of the $\classK$ function, we denote the size of vectors $\theta_{\alpha_i}, \theta_{\alpha_i^k}$ and $\theta_{\alpha_i}$ as $n_{\alpha_i}, n_\alpha$ and $n_\alpha$.
%then HOBCF reduces to Exponential CBF[]. Unless stated otherwise, we will only be using linear $\classK$ functions in this work. We will also refer to $\alpha = [ \alpha_1 , ..., \alpha_r ]^T$ as the vector-valued function. 
\subsection{Sequential Quadratic Programming}
\label{section::SQP}
Finally, we briefly describe Sequential Quadratic Programming (SQP)\cite{boggs1995sequential,tits2009feasible} that we use extensively in Section \ref{section::predictive_framework} to derive real-time parameter adaptation scheme. Consider an optimization problem in $z\in \reals^n$ with cost function $J:\reals^n\rightarrow \reals$, and constraint function $c:\reals^n \rightarrow \reals^m$ 
\begin{subequations}
    \begin{align}
    \min_z \quad & J(z), \\
    \textrm{s.t.} \quad & c(z)\leq 0, 
    \end{align}
    \label{eq::sqp_intro}
\end{subequations}  
In SQP, given an estimate $\bar z$ of the solution of the optimization problem \eqref{eq::sqp_intro}, gradients or sensitivities of the cost and constraint functions at $\bar z$ are used to compute infinitesimal update directions $d$ so that new estimate becomes $\bar{z}^+ = \bar z + d$.
The update direction $d$ is computed based on two factors

(1) Improving-cost direction $-\nabla J|_{z=\bar z}$: this is obtained by computing the gradient of the cost function. 

(2) Feasible update directions: these are the directions for which constraints that are satisfied with the current estimate $\bar z$ continue to be satisfied with the new estimate $\bar z$. The set of feasible directions $\mathcal{F}$ up to first-order approximation is given as:
\eqn{
   \mathcal{F}(\bar z) = \{ d~ | ~ c(\bar z) + \nabla \langle c|_{z=\bar z}, d\rangle \leq 0 \}.
}
 The set $\mathcal{F}$ need not be computed explicitly. The update direction $d$ is computed by projecting the improving-cost direction into the set of feasible directions by solving the following linear program
\begin{subequations}
    \begin{align}
\min_d & \quad \langle \nabla J_{z=\bar z}, d  \rangle \\
    \textrm{s.t} & \quad c(\bar z) + \langle \nabla c_{z=\bar z}, d\rangle  \leq 0.
\end{align}
\end{subequations}    
 Note that the notion of feasible directions is needed because moving only in the direction of improving cost does not guarantee that if $c(z)\leq 0$ then $c(\bar z^+)\leq 0$. 


\section{Problem Formulation}
\label{section::problem_formulation}
In this work, we develop online adaptation laws for parameters $\theta_{\alpha_i^k}$ of the $\classK$ functions $\alpha_i^k$. Before stating our objective formally, we introduce the following definitions.

Given the barrier function $h_i$ and the derived barrier functions $\psi_i^k, i\in \{1,..,N\}, k\in \{1,..,r_i\}$ defined in \eqref{eq::hobcf_barriers}, the control set that satisfies the CBF derivative constraint for a given state $x$ at time $t$ is given by
\eqn{
  \mathcal{U}_i(t,x) = \{ u \in \reals^m ~|~ \psi_i^{r_i}(t,x) \geq 0\}.
}
The intersection of the aforementioned sets with the control input domain is given as
\eqn{
    \mathcal{U}_c(t,x) = \bigcap\limits_{i=1}^{N} ~ \mathcal{U}_i(t,x) \bigcap \mathcal{U}.
    \label{eq::cbf_control_intersection}
}
Note that $\psi_i^k, k<r_i$ do not depend on the control input $u$ and therefore do not contribute to the computation of $\mathcal{U}_i$. 
\begin{definition}
    (\textbf{Compatible}) For a given state $x$ at time $t$, the control barrier functions $h_i(t,x)$ are called compatible at time $t$ if $\mathcal{U}_c(t,x)\neq\emptyset$. 
    %equations \eqref{eq::qp_cbf_constraint} are called compatible \textbf{Incomplete:} if a solution to \eqref{eq::CBF-CLF-QP}.
\end{definition}

\begin{definition} (\textbf{Persistently Compatible})
    For a state trajectory $x(t)$, $t\geq 0$ that is a solution to \eqref{eq::dynamics_general} under any Lipschitz continuous control $u(x(t)):\mathcal{X}\rightarrow \mathcal{U}$, the control barrier functions $h_i(t,x(t))$ are called persistently compatible if they are compatible for all $t\geq 0$.
\end{definition}

Next, we introduce the low-level controller we employ in our work. Let $V(t,x)$ be a control Lyapunov function (CLF) encoding stability objectives. The following controller is commonly employed to design a safety-critical controller that satisfies multiple CBF constraints:
\begin{subequations}
        \begin{align}
     u = \arg \min_{u\in \mathcal{U}} \quad &  u^TQu + M \delta^2 \\
         \textrm{s.t.} \quad & \dot{V}(t,x,u) \leq - k V(t,x) + \delta, \label{eq::qp_hoclf_constraint} \\
          & \psi_i^{r_i}(t,x,u;
          \theta_{\alpha_i})  \geq 0, \label{eq::qp_hocbf_constraint} \; i\in \{1,2,..,N\}
        \end{align}
        \label{eq::HOCBF-CLF-QP}
\end{subequations}
% \begin{subequations}
%         \begin{align}
%      u &= \arg \min_{u\in \mathcal{U}} \quad  u^TQu + M \delta^2 \\
%          \textrm{s.t.} \quad & \dot{V}(t,x,u) \leq - k V(t,x) + \delta, \label{eq::qp_clf_constraint} \\
%          & \dot{h}_i(t,x,u) \geq - \alpha_i(t,h_i(t,x)) \label{eq::qp_cbf_constraint}, \; i\in \{1,2,..,N\},
%         \end{align}
%         \label{eq::CBF-CLF-QP}
% \end{subequations}
where $Q\in \reals^{m\times m},M\in \reals$ are weighting matrices, $k$ is the exponential rate of convergence, and $\delta$ is a slack variable used to relax the CLF constraint \eqref{eq::qp_hoclf_constraint} and give priority to CBF constraints \eqref{eq::qp_hocbf_constraint}. The optimization \eqref{eq::HOCBF-CLF-QP} is a QP when the dynamics is control affine as in \eqref{eq::dynamics_general} and $\mathcal{U}$ can be expressed in the form of a polytope $Au\leq b, A\in \reals^{q\times m}, b\in \reals^{q\times 1}, q>0$. 

A solution to \eqref{eq::HOCBF-CLF-QP} exists at a given $(t,x)$ if $h_i(t,x)$ are compatible. Note that even when $\mathcal{U}=\reals^m$, that is, when the control input is unbounded, $h_i(t,x)$ may not be compatible. This is because CBFs reduce the size of the safe set depending on the system dynamics \eqref{eq::dynamics_general} and the functions $\alpha_i^k$. 
%As an example, note the trajectories that result from different choices of the parameter $\nu_i$ in \eqref{eq::linear_classK_example} for a single-integrator agent controlled to avoid an obstacle as shown in \red{Fig.[]}. A smaller $\nu$ value leads to a more conservative response than a larger $\nu$ value. When multiple constraints are present, a poor choice of $\alpha_i^k$ may eventually result in the incompatibility of the CBF constraints as we show in Section \ref{section::simulation}.

% \begin{example}
% \label{example:car}
% Consider the motion of an autonomous car modeled as one-dimensional integrator system $\dot{x}=u$ with state $x$ and velocity input $u$. Now consider a common road scenario in which the car is caught between two vehicles. The vehicle in front is moving slower than the vehicle at back and therefore the safe set, described as no collision zone between the vehicles, is shrinking with time and vanishes in future. The objective is to stay safe for maximum possible time. Let the shrinking set be given by the constraint $x\geq t$, and $x\leq 1+c t$ where $t$ is time and $c<1$ is a constant. We can encode these two constraints with two barrier functions $h_1 = x - t$, $h_2 = 1 + c t-x$ and solve the following optimization problem
% \eqn{
%     \begin{aligned}
%             \max \quad & u  \\
%             \textrm{s.t.} \quad & h_1(t+1,x_{t+1}) \geq (1-a) h_1(t,x_t)   \\
%                           &  h_2(t+1,x_{t+1}) \geq (1-b) h_2(t,x_t) 
%             \label{eq::car_LP}
%     \end{aligned}
% }
% Here, $a,b\in \reals $ are parameters. Linear Programs(LP) satisfy the same assumptions introduced for QPs in Section \ref{section::cvx_der} and therefore are still amenable to our approach. Note that eventually the safe set completely vanishes here but the response of the system before that critical time is still relevant to real scenarios. The closed loop system for different values of parameter is simulated. Fig.\ref{fig:example_path} shows the resulting trajectories and Fig.\ref{fig:1d_example} shows how time to infeasibility changes with parameter choice for two different set shrinkage rates $c$. Both figures are asymmetric w.r.t $a,b$ which is a consequence of difference in rate of change of both constraints. Figs.\ref{fig:example_path},\ref{fig:1d_example} also show why it is important to choose parameter proactively as a wrong parameter choice may lead to infeasibility and possible crash of the system.

% \begin{figure}[th!]
%         \centering
%         \includegraphics[width=0.7\linewidth]{figures/paths_example1.png}
%         \caption{Plot of trajectories for different values of parameter. Trajectories end at point of infeasibility. The values of $a$ and $b$ shown are normalized by sampling time $\Delta t$. 
%         %Note that the controller Eq.(\ref{eq::car_LP}) may become infeasible even if system is far away from unsafe boundaries.
%         }
%     \label{fig:example_path}
% \end{figure}
% \end{example}

% \begin{figure}[th!]
    
%         \centering
%         \includegraphics[width=0.95\linewidth]{figures/paper_example.png}
%         \caption{Time to Infeasibility: X and Y axis represent parameter values(normalized by sampling time $\Delta t$) for which trajectory is simulated. The color gradation represents the time after which problem becomes infeasible. A value of 100 implies it remains feasible for maximum possible time and value of 0 implies it is infeasible from the start. (a)c=0.3, (b)c=0.7 in Example \ref{example:car}.}
%     \label{fig:1d_example}
% \end{figure}

%The functions $\alpha$ are often manually tuned for a suitable response to a specific task however, the presence of control input bounds, time-dependent variations in safe sets, and changes in the application may render the constraints incompatible or when compatible, may result in an unacceptable closed-loop response. 
%Given the inter-dependency of the aforementioned factors, it is not straightforward in practice to choose $\alpha_i$ that completes the task and ensures persistent compatibility of CBF constraints.
Therefore, in our approach, we allow adaptation of the $\theta_\alpha$ so that the controller \eqref{eq::HOCBF-CLF-QP} can be tuned for its response, for example, to make it less or more conservative, or to are persistently compatible along the system trajectories $x(t), \forall t\geq0$. 
% TODO: check if line below needs to be mentioned somewhere
%Note that our notion of persistent compatibility, even in presence of a single barrier function, i.e., $N=1$, does not enforce compatibility $\forall x\in \mathcal{X}$, {\color{blue}i.e., for all points of the state space}, as is required in the definition of CBF \eqref{eq::cbf_derivative},\eqref{eq::hobcf_cbf_derivatrive}. Therefore, the concept of persistent compatibility is \textbf{I wouldn't call it more generic, perhaps you can call it a weaker notion that the CBF condition} more generic than CBF derivative condition. 
We now state our objective.


%From the above equation, we can infer that persistent feasibility implies that $\mathcal{U_c}\neq \Phi~ \forall t>=0$.
%even when infinite control input is available ($\mathcal{U} = \reals^m$), the constraints might not be compatible and $\mathcal{U}_c$ might be empty. This can happen because either $h=\bigcap\limits_{i=1}^{N}h_i$ is non-empty but poor choice of $\alpha_i$ renders $\mathcal{U}_c$ empty or when $\bigcap\limits_{i=1}^{N}h_i$ itself is empty. The latter can again happen due to poor choice of $\alpha_i$ when at $t=0$, $h$ is non-empty but the designed control input steers the system to states where $h$ becomes empty.

\begin{objective}
    Consider the dynamical system \eqref{eq::dynamics_general} subject to $N$ barrier constraints $h_i(t,x)\geq 0, i\in\{1,..,N\}$  defined in \eqref{eq::safeset3}. Suppose each barrier function $h_i$ has relative degree $r_i$ giving rise to $r_i$ derived barrier functions $\psi_i^k, k\in\{1,..,r_i\}$ defined in \eqref{eq::hobcf_barriers} with corresponding $\classK$ functions $\alpha_i^k$ with parameters $\theta_{\alpha^k_i}$. Let the control input be designed as in \eqref{eq::HOCBF-CLF-QP}. If at $t=0$, $\mathscr{C}(t,x(0))\neq \emptyset$, then design an update law function $\dot \theta_{\alpha_i^k}(t,x,\theta_{\alpha_i^k}): \reals^+ \times \reals^n \times \reals^{n_\alpha}\rightarrow n_{\alpha}$ so that 
    \begin{enumerate}
        \item $\mathscr{C}(t,x(t))\neq \emptyset ~ \forall t>0$
        \item $\mathcal{U}_c(t,x(t))\neq \emptyset ~ \forall t>0$
    \end{enumerate}
\end{objective}


\section{Rate-Tunable CBFs}
\label{section::RT-CBF}
In this section, we introduce our notion of RT-CBFs and analyze the resulting safety guarantees. 

\begin{definition}(Rate-Tunable CBF) Consider the system dynamics in (\ref{eq::dynamics_general}) augmented with the state $\theta_\alpha\in\reals^{n_\alpha}$ that obeys the dynamics
\eqn{
\begin{bmatrix} \dot x \\ \dot \theta_\alpha \end{bmatrix} = \begin{bmatrix} f(x) + g(x) u \\ f_\alpha(x, \theta_\alpha) \end{bmatrix},
\label{eq::augmented}
}
 where $f_\alpha:\mathcal{X}\times \reals^{n_\alpha} \rightarrow \reals^{n_\alpha}$ is any locally Lipschitz continuous function w.r.t. $(x,\theta_\alpha)$ that ensures that $\theta_\alpha(t)\in \mathcal{A}$ where $\mathcal{A}\subset \reals^{n_\alpha}$ is a compact set. Let the set of allowable states $\s_i$ be defined as 0-superlevel set of a $C^{r+1}$ function $h_i:\mathcal{X}\rightarrow \reals$ as in \eqref{eq::safeset3}. Suppose $h_i$ has relative degree $r_i$ w.r.t the control input $u$ and gives rise to $r_i$ derived barrier functions $\psi^k_i$, 
    \eqn{
   \psi^0_i(t,x) &= h_i(t,x), \\
   \psi^k_i(t,x) &= \dot \psi^{k-1}_i(t,x) + \alpha^k_i (\psi^k_i(t,x)), k\in \{1,2,..,r_i\}\nonumber
   \label{eq::rt_hobcf_barriers}
}
where $\alpha^k_i:\reals\rightarrow \reals$ is $C^{r-k}$ extended $\classK$ function and 
\eqn{
   \dot \alpha^k = \frac{\partial \alpha^k}{\partial \theta_{\alpha^k}}\dot \theta_{\alpha^k} + \frac{\partial \alpha^k}{\partial \psi^k}\dot \psi^k
}

 Then, $h_i$ is a rate tunable control barrier function for the augmented system \eqref{eq::augmented} starting at initial state $x_0\in \mathcal{S}_i,\theta_{\alpha_0}\in \mathcal{A}$ for the set $\mathcal{S}_i$ and $\mathcal{A}$ if 
 %and a set $D\subset \reals^n$ with $\s \subset D$
 % such that
\eqn{
  \sup_{u\in \reals^{m}} \left[ \psi^r_i(x,u,\dot \theta_{\alpha_i})  \right] \geq 0 
  %~\forall x\in \mathcal{D},
  ~\forall \theta_\alpha \in \mathcal{A}, \forall t>0 
  \label{eq::rtcbf_derivative_condition}
}
\label{definition::RT-CBF}
\end{definition}
The above definition implies the existence of an envelope of safe trajectories resulting from the choice of multiple $\classK$ function parameters at each time $t>0$ thereby giving the user the freedom to tune the response of the system in real-time. Further imposing dynamics for parameter $\theta_\alpha$ as in \eqref{eq::augmented}, instead of allowing discrete jumps in its value, allows us to prove continuity of the control input $u_i$ that is designed using RT-CBF. 
%These $\classK$ functions are dependent on the parameter $\theta_\alpha$ that is allowed to evolve over a compact set 

Note that the derivative condition in \eqref{eq::rtcbf_derivative_condition} is not enforced for $\forall x\in \mathcal{X}$, i.e., for all points of the state space. RT-CBF is thus a weaker notion of safety that allows more freedom to design controllers.  The relationship between RT-CBF and CBF is given by the following theorem.
%, as is required in the definition of CBF \eqref{eq::cbf_derivative},\eqref{eq::hobcf_cbf_derivatrive}. 
%TODO: see if line below is worth mentioning somewhere
%Therefore, the concept of persistent compatibility is more generic than CBF derivative condition. 

\begin{theorem}
 Consider the augmented system dynamics \eqref{eq::augmented} and the barrier function $h_i$ and derived barrier functions $\psi^r_i$ as defined in Definition \ref{definition::RT-CBF}. If there exists $\theta_{\alpha_0} \in \mathcal{A}$ such that $h_i$ is a RT-CBF for all $x_0\in D_i$ where $D_i\subset \reals^n$ with $\s_i \subset D$, then $h_i$ is a CBF on $D_i$.
 \label{theorem::RT-CBF-to-CBF}
\end{theorem}
\begin{proof}
    By definition of RT-CBF, \eqref{eq::rtcbf_derivative_condition} is satisfied for all $x(0)\in \mathcal{D}$. Hence, \eqref{eq::cbf_derivative} (respectively \eqref{eq::hobcf_cbf_derivatrive}) is satisfied trivially and $h_i$ is therefore a CBF (respectively HOCBF).
\end{proof}
\begin{remark}
    While several works employ heuristics to ensure that a solution to CBF-QP exists for all $t>0$\cite{zeng2021safety,wang2022ensuring,garg2019control}, most of these are equivalent to treating the parameter $\nu$ of a linear $\classK$ function $\alpha(h) = \nu h$ as an optimization variable, thereby turning the hard CBF constraint into a soft constraint. However, a formal analysis encompassing all these heuristics and other possible ways to adapt the $\classK$ function has been lacking so far and RT-CBFs aim to bridge this gap in theory and application.
\end{remark}

\begin{remark}
\label{remark::rtcbf_Aset}
    Just like CBF condition in Definition \ref{definition::cbf_definition} assumes the existence of a suitable $\classK$ function $\alpha$ and does not provide a way to find it, Definition \ref{definition::RT-CBF} also does not provide a way to compute the set $\mathcal{A}$ for which we can find a RT-CBF. In fact, our definition emphasizes that the set $\mathcal{A}$ may not be the same for different initial conditions $x_0$ as is usually observed in practice, and is the reason why $\classK$ functions in CBF-QP controllers are usually manually tuned for a particular application. Therefore, when a given parameter $\bar{\theta}_\alpha$ that does not ensure persistent compatibility and hence does not belong to $\mathcal{A}$, our methods in Section \ref{section::dynamics_design} are also devoted to implicitly finding the set $\mathcal{A}$ by designing $\dot \theta_\alpha$ and incrementally improving $\bar{\theta}_\alpha$ in a direction that satisfies compatibility condition for longer time horizon than $\bar{\theta}_\alpha$.
\end{remark}

Before we show that RT-CBF is sufficient to ensure safety, we introduce the following result that will be used to ensure compactness of set over which the state $x$ evolves.

\begin{lemma}
\label{lemma:boundedness}
Consider the system $\dot x = F(t,x)$ with $F$ being locally Lipschitz in $x$ and piecewise continuous in $t$. Further consider a radially unbounded, continuously differentiable Lyapunov function candidate $V(t,x): [0,\infty]\times \reals^n \rightarrow \reals $. Suppose,
\begin{enumerate}
    \item $\exists ~ \classK_\infty$ functions $\alpha_1$ and $\alpha_2$ such that 
    \eqn{
       \alpha_1(||x||) \leq V(t,x) \leq \alpha_2(||x||), \forall t>0, \forall x\in \reals^n
       \label{eq::V_bounds}
    }
    \item for any interval $t\in (0,\mathcal{I}), \mathcal{I}\in [0,\infty]$ over which a solution exists to $\dot x=f(t,x)$, it is known that the following equation is satisfied 
    \eqn{
      \dot V(t,x) \leq -k V(t,x) + \delta(t), \forall t \in [0,\mathcal{I}]
    }
    where $\delta(t) \leq \bar \delta, \bar\delta \in [0,\infty)$
\end{enumerate}
Then, the state trajectory $x(t)$ evolves over a compact set in its domain of existence and this compact set is independent of the initial condition and of the interval of existence.
\end{lemma}

\begin{proof}
Since $\bar \delta$ is bounded, using \eqref{eq::V_bounds} we have that $\exists R>0$ such that $V(t,x)\geq \frac{\delta(t)}{k}$ and hence $\dot V(t,x)\leq 0$ for all $x\in \reals^n\setminus B_r(0)$ where $B_r(0)=\{x~ | ~||x||\leq R\}$. 
Now let 
\eqn{
\beta = \max_{||x||= R} \alpha_2(||x||) %V(t,x)
}
which exists because $V$ is continuous. Choose $\infty > c > \beta$ and consider the time-varying set
\eqn{
  \Omega_{t,c} = \{ x | V(t,x)\leq c \}
}
that, using \eqref{eq::V_bounds}, is contained within the set
\eqn{
  \bar \Omega = \{ ||x|| \leq \alpha_2^{-1}(c) \}
  \label{eq::state_compact_set}
}
then $\bar \Omega$ is compact. It can also be shown that $B_r(0)\subset \Omega_{t,c}$ and hence $B_r(0)\subset \bar\Omega$. 
Since $B_r(0) \subset \bar \Omega$, we have $\dot V(t,x)\leq 0$ for all $x\in \partial \bar \Omega$ (boundary of $\bar \Omega$). Therefore, from \cite[Theorem 3.5.2]{carja2007viability}, $\bar \Omega$ is positively invariant on $t\in [0,\mathcal{I}]$.


\end{proof}

% \textbf{There is specific reason why Khalil takes the minimum; the set should be fully contained in a ball of given radius. So it sounds your approach is not correct?} \\
% \blue{RESPONSE: Khalil uses minimum to show compactness as well as to create contracting sublevel sets for showing stability. It shows compactness of its ball because of the minimum, not because it is contained within the ball of a given radius. the ball in Khalil is used to show boundedness only. minimum helps in making sure it is closed, and therefore compact too. I do not need stability and I can reason about compactness based on \eqref{eq::V_bounds}. My purpose is different, so the proof is different too.}


% \blue{
% \begin{assumption}
%   The functions $f(x), g(x)$ are bounded. Furthermore, there exists $\bar{\mathcal{X}}_U\in \reals^+$ such that $h(t,x)<0$ if $||x_i||>\bar{\mathcal{X}}_U$ and $\frac{\partial h}{\partial x_i}\not \equiv 0$.
% \end{assumption}
% }
% The above assumption will be used in Theorem \ref{theorem::RT_CBF_invariance} to conclude the compactness of the set over which the state $x(t)$ evolves. We believe that the above assumption is not strict and is in fact representative of the real systems where we want autonomous agents to operate in a specific region. 

\begin{theorem}
\label{theorem::RT_CBF_invariance}
Consider the augmented system~(\ref{eq::augmented}) with given initial state $x(0)\in \reals^n$ and any $\theta_\alpha(0) \in \mathcal{A}$, and the set of allowable states $\s_i$ defined as 0-superlevel set of a $C^{r}$ RT-CBF $h_i:\mathcal{X}\rightarrow \reals, i\in \{1,..,N\}$ as in \eqref{eq::safeset3}.
%Suppose $h(x)<\bar h, \forall x \in S $, 
Then under the action of control input designed in \eqref{eq::HOCBF-CLF-QP}, there exists a time interval $[0,\mathcal{I}], \mathcal{I}\in [0,\infty)$ over which the $x(t)$ exists and the set $\s$ is rendered forward invariant for $t\in [0,\mathcal{I}]$ if either of the following conditions are met
\begin{enumerate}
    \item Case 1:  $N=1$
    \item Case 2: $N>1$ and following assumptions holds: (1) $h_i$ are compatible for given compact sets $\mathcal{A}$, (2) Slater's condition and strict duality\cite{boyd2004convex} hold for the optimization \eqref{eq::HOCBF-CLF-QP}, and (3) the constraint functions and their gradients in \eqref{eq::qp_hoclf_constraint} and \eqref{eq::qp_hocbf_constraint} are continuously differentiable in $x,\theta_\alpha$.
\end{enumerate}
Furthermore, if the conditions of Lemma \ref{lemma:boundedness} hold, then $\mathcal{I}=\infty$.


% For a locally Lipschitz continuous reference controller $u^r: \mathcal{X} \to \reals^{m_i}$, let the controller $u = \pi(x, \alpha)$, where $\pi : \mathcal{X} \times \reals^+ \to \mathcal{U}$ be formulated as
% \begin{subequations}\label{eq::theorem_qp}
% \begin{align}
% \begin{split}\label{eq::theorem_qpa} 
%  \pi(x, \alpha) = \underset{u\in \mathcal{U}}{\operatorname{argmin}} \quad & || u - u^r(x) ||^2 
% \end{split}\\ 
% \begin{split} %\label{eq::theorem_qpb}
% \textrm{s.t.} \quad & \psi^r(t,x,\alpha) \geq 0 
% \label{eq::cbf_alpha}
% % \textrm{s.t.} \quad & \frac{\partial h}{\partial x} ( f(x)+g(x)u )  )\geq -\alpha h(x) 
% \end{split}
% \end{align}

% \end{subequations}
% \noindent where $L_f^r h$ and $L_g L_f^{r-1}h$ are also locally lipschitz continuous. 
%1) the solution, u(x,$\alpha$) is locally Lipschitz continuous for $x \in int(\s)$.  2) 
\end{theorem}

\begin{proof}

  Consider case 1, $N=1$. Since $h_1$ is a Rate-Tunable CBF, according to~\eqref{eq::cbf_derivative}, there always exists a $u \in \reals^{m}$ that satisfies the constraint of the QP~\eqref{eq::qp_hocbf_constraint}. The solution $\pi(x,\alpha)$ is also unique as the optimization problem is strictly convex. Since $f,g, h_1, L_g L_f^{r-1}h_1, L_f^k h_1, k\in \{1,..,r_1\} $ are locally Lipschitz, by \cite[Thm. 3]{ames2016control}, it follows that $\pi$ is locally lipschitz continuous.

Now consider case 2. Under the given assumptions, the conditions of \cite{agrawal2019differentiating},\cite{barratt2018differentiability} Theorem3.1 are satisfied and hence the QP \eqref{eq::HOCBF-CLF-QP} solution is a locally single-valued function around its solution $u(t)$ and is continuously differentiable in the neighborhood of ($x(t),\theta_\alpha(t)$). 
    %The remaining arguments are same as \cite[Thm. 3]{ames2016control} that finds this unique solution for \textbf{UNCLEAR: AVOID THESE LONG AMBIGUOUS EXPRESSIONS:} CBF inequality based QP, and shows that the solution is locally Lipschitz continuous w.r.t $\frac{\partial h}{\partial x}f(x),\frac{\partial h}{\partial x} g(x)$ and $\alpha h(x)$. This gives us that $\pi$ is locally Lipschitz w.r.t $x,\alpha$.

We have that $u$ is also locally Lipschitz continuous for both Case 1 and Case 2. Hence, the closed-loop dynamics is locally Lipschitz continuous, and \eqref{eq::augmented} admits a unique solution over its interval of existence \cite[Theorem 3.1]{khalil2002nonlinear}. We know that from \eqref{eq::rt_hobcf_barriers} that
    \eqn{
       %\psi^{k} &= \left(\frac{d}{dt} + \alpha^k \right) \psi^{k-1}\\
    & \implies \dot \psi^{k-1}(t,x) = \psi^k(t,x) - \alpha(\psi^{k-1}(t,x)) \\
        & \hspace{3.2cm} \geq - \alpha(\psi^{k-1}(t,x)) ~ \forall x \in \mathscr{C}  \label{eq::hocbf_derivative_condition}
        % & \geq 0 ~ ~ \forall x \in \mathscr{C} \\
        % \implies &  \frac{\partial \psi_{k-1}}{\partial x}f(x) + \frac{\partial \psi_{k-1}}{\partial t} \geq 0
    }
    Since the set $\mathcal{A}$ is compact, there exists $\classK$ functions  $\gamma^k_i: \reals \rightarrow \reals, ~ k\in\{1,..,r\}$ such that $-\alpha^k_i(\psi^k_i(t,s)) \geq -\gamma^k_i(\psi^k_i(t,s)), \forall (t,s) \in \reals^+ \times \reals$. Now consider the following comparison system
    \eqn{
        \begin{bmatrix}
            \dot c^0 \\
            ... \\
            \dot c^{r-1}
        \end{bmatrix} = \begin{bmatrix}
            -\gamma^{1}_i(c^0)  \\
            -\gamma^{2}_i(c^1)  \\
            ... \\
            -\gamma^r_i(c^{r-1})
        \end{bmatrix}
        \label{eq::comparison_system_rtcbf}
    }
    From \cite[Lemma 4.4]{khalil2002nonlinear}, the system \eqref{eq::comparison_system_rtcbf} admits a unique solution $c(t)$ for all $t>0$ and is lower bounded by 0. Therefore, using comparison lemma \cite[Lemma 3.4]{khalil2002nonlinear}, the systems governed by $\dot \psi^k_i\geq -\gamma^k_i(\psi^k_i)$ and hence $\dot \psi^k_i \geq -\alpha^k_i(\psi^k_i)$, over their respective interval of existence,  are also lower bounded by 0. Therefore, $x(t)\in \mathcal{S} ~ \forall t\in[0,\mathcal{I}]$ where $\mathcal{I}$ is the interval of existence of solution of \eqref{eq::augmented}. Now suppose the conditions of Lemma \ref{lemma:boundedness} hold. Then the state $x$ evolves over a compact set $\bar \Omega$ defined in \eqref{eq::state_compact_set}. Therefore, since the closed-loop dynamics is locally lipschitz continuous and $x(t)$ is known to belong to a compact set, from  \cite[Theorem 3.3]{khalil2002nonlinear}, $\mathcal{I}=\infty$ and the $x(t)$ is defined for all $t>0$. Hence the set $\s(t)$ is forward invariant for all $t>0$.
\end{proof}

\begin{remark}
    We use \cite{agrawal2019differentiating},\cite{barratt2018differentiability} Theorem3.1 to conclude that the solution of the optimization \eqref{eq::HOCBF-CLF-QP} is differentiable w.r.t $x,\theta_\alpha$. In case of QPs, a simple way to ensure that these constraints hold is if the unconstrained solution does not make any of the constraints active. However, if this cannot be guaranteed, then relaxed KKT conditions\cite{biegler2010nonlinear} can be used to get a gradient in the limiting case\cite{gros2019towards}. A more involved analysis for non-differentiable but continuous controller based on generalized gradients is also possible but is outside the scope of this paper and will be addressed in future work.
\end{remark}

\begin{assumption}
    Every safe trajectory $x(t)$ of the system \eqref{eq::dynamics_general} evolves in a fixed compact set $\mathcal{B}\subset \reals^n$. 
    \label{assumption::compactness_safe_trajectories}
\end{assumption}
We make the above assumption to show the necessity of RT-CBF. We consider this a weak assumption as in practice we do not want the states of our system to approach infinity or have finite escape time. 

\begin{theorem}
\label{theorem::necessity-RT-CBF}
Consider the augmented system~(\ref{eq::augmented}), and the sets of allowable states $\s_i$ defined as 0-superlevel sets of a $C^{r}$ functions $h_i:\mathcal{X}\rightarrow \reals$ as in \eqref{eq::safeset3}. Suppose the conditions of Theorem \ref{theorem::RT_CBF_invariance} hold. Then under assumption \ref{assumption::compactness_safe_trajectories}, the sets $\s_i$ are rendered forward invariant if and only if $h_i$ are RT-CBFs. 

\end{theorem}
\begin{proof}
    Sufficiency has been shown in Theorem \ref{theorem::RT_CBF_invariance} and we show necessity here. 

        First, consider the first-order barrier function and a given safe trajectory $x'(t)$. Given that $x'(t)$ is a safe trajectory, we have $h_i(t,x')\geq 0$ for all $t>0$, $i\in \{1,..,N\}$. Now consider $\dot{h}_i = \frac{\partial h_i}{\partial x}\dot x' + \frac{\partial h_i}{\partial t}$. Since $h_i$ is $C^{r_i+1}$ function $\dot h_i$ is $C^{r_i}$ function. Now consider 
        \eqn{
             \alpha_i^{'1}(r) = - \inf_{t>0, 0 \leq h_i(t,x')\leq r} L_f h_i(t,x') 
             \label{eq::construct_alpha}
        }
        Since $x'$ evolves in a compact set under assumption \ref{assumption::compactness_safe_trajectories}, the set $\{x | h_i(t,x) \leq r, \forall t>0\}$ is also compact. Hence, the infimum in \eqref{eq::construct_alpha} exists as $h_i$ is a $C^{r_i+1}$ function. Therefore, $\alpha_i^{1'}$ is a non-decreasing $C^{r_i}$ function. 
        %In contrast to necessity proof in \cite{ames2016control} for first order CBFs, we have not assumed that the barriers $h_i$ define a compact set. Instead, given our assumption that the control input and dynamics is bounded, the state trajectory $x(t)$ cannot have finite escape time and hence belongs to a compact set at any time $t<\infty$. Therefore, the infimum in \eqref{eq::construct_alpha} exists. The function $\alpha'(r)_i^1$ is thus a $C^{r_i}$ non-decreasing function on $\reals^+$ \red{(Needs work to show last statement, maybe assume that given safe trajectory $x(t)$ lies in a compact set? will make things simpler)}. 
        There always exist a $C^{r_i} ~ \classK$ function $\hat \alpha_i^1(r)$ that upper bounds $\alpha_i^{'1}(r)$ yielding $\dot h_i \geq -\hat \alpha_i^1(h_i)$.

        Now consider the higher-order derived barrier functions  $\psi_i^k, k\in \{0,..,r_i-1\}$ and consider 
        \eqn{
          \alpha'^{k+1}_i(r) &= - \inf_{t>0, 0\leq h_i(t,x)\leq r} \dot \psi_i^k(t,x',\dot x', \alpha^{'1}_i,..,\alpha^{'1(k)}_i, ... ,\nonumber \\
          & \hspace{4cm} \alpha^{'k}_i, ,..,\alpha^{'k(1)}_i)
        }
        where $\dot \psi_i^{k+1}$ includes all $k+1-j$ time derivatives of functions $\alpha_i^j, j\in \{1,..,k\}$. Then, following similar arguments, we have that $\alpha'^{k+1}_i(r)$ is non-decreasing $C^{r_i-k}$ function. Therefore, there always exists a $C^{r_i-k} ~ \classK$ function $\hat \alpha^1_i(r)$ that upper bounds $\alpha'^{k+1}_i(r)$ such that $\dot \psi_i^k \geq - \hat \alpha^1_i(r)$.
        % \item Order 1: Consider any arbitrary safe and continuously differentiable trajectory $x(t)$ of the system \eqref{eq::dynamics_general}. Since $x(t)$ is known to be safe, at any time $t$, we have that $h(t)\geq 0$. Furthermore, $\dot{h} = \frac{\partial h}{\partial x}\dot x$ is locally lipschitz continuous $\forall i$. Hence, we can always choose $\alpha^1(h)$ such that $\dot{h}\geq -\alpha_i^1(h)$. Without loss of generality, we choose $-\alpha_i^1(h) = \dot{h}(t)$ at all $t>0$. Finally, since $h$ is $C^{r+1}$ function, $\alpha(h)$ is a well-defined $C^{r}$ function.
        % \item Since $\ddot h = (L_f \dot h) \dot x$, $\ddot h$ is also Lipschitz continuous. Therefore, $\dot \psi^1 = \ddot h + \dot \alpha^1 h +  \alpha^1 \dot h $ is also Lipschitz continuous. Therefore, we can always choose $\alpha^2(\psi^1)$ such that $\dot \psi^1 \geq -\alpha_i^2(\psi^1)$. Without loss of generality, we choose $-\alpha_i^2(\psi^1) = \dot{\psi^1}(t)$. Finally, given that $h$ is $C^r$ function and $\alpha^1$ is $C^{r}$ function,  $\alpha_i^2$ is thus a $C^{r-1}$ function.
        % \item Similarly, we can find candidate functions $\alpha^k, k\in \{3,..,r-1\}$ which are $C^{r-k+1}$ functions. 
        Since for any given safe trajectory $x'(t)$, there exists an augmented system \eqref{eq::augmented} such that $h_i$ can be shown to be RT-CBF, RT-CBF is a necessary condition for safety. 

    
    %Without loss of generality, we choose $-\alpha(t,h) = \dot{h}(t)$ at all $t>0$. By definition, $h_i$ is an RT-CBF. Since the choice of safe trajectory $x(t)$ is arbitrary, there exists an RT-CBF for every possible safe trajectory. Therefore RT-CBF is a necessary condition for safety. From Theorem \ref{theorem::RT_CBF_invariance}, we also know that RT-CBF is a sufficient condition for set invariance. Hence, we have shown the necessity and sufficiency of RT-CBFs.
\end{proof}

Note that in contrast to the necessity proof in \cite{ames2016control} for first-order CBFs, we have not assumed that the barriers $h_i$ define a compact set. This is also a result of RT-CBF being a weaker notion of safety as shown in Theorem \ref{theorem::RT-CBF-to-CBF}.
The new notion of Rate-Tunable CBFs is thus more expressive than CBFs with fixed $\classK$ function. It is also trivial to show that even linear $\classK$ function of the form \eqref{eq::linear_classK_example} with time-varying parameter $\nu$ are equivalent to any fixed differentiable $\classK$ function. Hence, in practice, linear functions usually suffice with properly designed dynamics of the parameter $\nu$. 

While Theorem \ref{theorem::necessity-RT-CBF} and \ref{theorem::RT_CBF_invariance} guarantees that RT-CBF exists, it is not trivial to find barriers $h_i$ that are compatible RT-CBF without performing offline analysis for all possible scenarios that the system may be subjected to. Therefore, the ensuing section is dedicated to finding $\dot \theta_{\alpha}$ in an online fashion that renders given candidate barriers $h_i$ RT-CBF.

% TODO: show other approaches are subset of RT-CBF
% \textbf{The following paragraphs read disconnected, remove or explain what you mean or where these are presented. The Example also doesn't really state anything.}

% Finally, we show how several heuristics proposed in the last several years can also be formalized under the framework of RT-CBFs. 

% \begin{example}
%     Show how formulations previously proposed by other researchers are just a special case of this generic formulation.
% \end{example}

\section{Design of $\classK$ function dynamics}
\label{section::dynamics_design}
In this section, we provide two different approaches to design $\dot\theta_\alpha(x,\theta_\alpha):\reals^n \times \reals^{n_\alpha}\rightarrow \reals^{n_\alpha}$. The first approach is a discrete-time predictive framework that optimizes the cost accumulated over finite time horizons. The framework is implemented in receding horizon and uses sensitivities of the future states w.r.t. $\theta_{\alpha_k^i}$ to find an update direction for $\theta_{\alpha_k^i}$ that improves a performance metric to be defined later. 

Our second approach provides pointwise sufficient conditions on $\dot \theta_i^k$ that, whenever can be imposed, ensure that $h_i$ remains compatible.
%These sufficient conditions can be used to incorporate any ad-hoc, heuristic, or other user-designed update rules for $\theta_\alpha$ by projecting them to the set of $\dot \theta_\alpha$ that ensure persistent compatibility. This has importance when prediction is not possible, for example, when dynamics is partially unknown, or when it is computationally expensive to perform prediction for very low-cost hardware. As a case study, we also introduce RT-CBFs for decentralized and uncooperative multi-agent systems in Section \ref{section::multi-agent}, 
%where we design $\dot \theta_\alpha$ for ego-agents based on a \textit{trust metric} that is computed from the observations of motions of other agents. This trust-based update law is then projected to the set of $\dot \theta_\alpha$ that ensure persistent compatibility and is implemented online to show a less conservative response of the controller of the ego-agent.

\subsection{Predictive Framework for Optimal Tuning of CBFs }
\label{section::predictive_framework}
In this section, we propose a discrete-time predictive framework to be implemented in receding horizon fashion to design design $\dot \theta_\alpha$. Let the state and input at time step $\tau\in \mathbb{Z}^+$ be denoted as $x_\tau,u_\tau$ and let $\tau+1$ denote the next sampling time instant. The controller \eqref{eq::HOCBF-CLF-QP} is given in discrete-time implementation as
\begin{subequations}
        \begin{align}
            \min_{u_t\in \mathcal{U},\delta} \quad & J(u_\tau) = (u_\tau-u_{d_\tau})^TQ(u_\tau-u_{d_\tau}) + M\delta^2,\\
            \textrm{s.t.} \quad &  V(\tau+1,x_{\tau+1}) \leq (1-\alpha_0)V(\tau,x) + \delta, \label{eq::inequality_CLF_QP}\\
                    &       \psi_i^{k}(\tau+1,x_{\tau+1}) \geq (1-\alpha_{i}^k)\psi^{k}_i(\tau,x_\tau), \label{eq::inequality_CBF_QP}\\
                    & \hspace{1cm} \forall i \in \{1,2,..,N\}, k\in \{1,2,..,r-1\}, \nonumber \\
                   & \quad \hspace{1.3cm} A u_\tau \leq b\label{eq::linear_input_bounds}
        \end{align}
        \label{eq::CBF_MDP}
\end{subequations}
and the dynamics \eqref{eq::dynamics_general} in discrete time form are denoted as:
\eqn{
   x_{\tau+1} = f_\tau(x_\tau) + g_\tau(x_\tau) u_\tau. \label{eq::dynamic_discrete}
}
The controller \eqref{eq::CBF_MDP} is henceforth referred to as the policy $\pi_{QP}(x_\tau;\theta_{\alpha_\tau})$. Now suppose the reward of choosing action $u_t$ at state $x_t$ is given by the function $r:\mathcal{X}\times \mathcal{U}\rightarrow \reals$. The reward $r$ can, for example, encode either of stability and safety objectives or try to minimize control input. A finite-horizon optimal control problem to find (fixed) parameters $\theta_\alpha$ that maximize the reward accumulated for $T$ time steps starting at time $\tau$ can be formulated as follows:

\begin{subequations}
        \begin{align}
            \max_{\theta_\alpha} \quad & R(x_\tau,\theta) = \sum_{s=\tau}^{\tau+T}  r(x_s,u_s) + \Phi(x_{T+1}), \label{eq::mpc_reward}\\
            \textrm{s.t.} \quad & u_s = \pi_{QP}(x_s;\theta_\alpha), \label{eq::MPC_u}\\
             & x_{s+1} = f(x_s) + g(x_s)u_s, \\
             & \forall s \in \{\tau,\tau+1,..,\tau+T\}, \nonumber
        \end{align}
        \label{eq::high_level}
\end{subequations}
where $\Phi: \mathcal{X}\rightarrow \reals$ is the terminal cost, which ideally is representative of the residual infinite-horizon cost. The QP controller \eqref{eq::CBF_MDP} is said to be implemented at \textit{low-level}, and the optimization \eqref{eq::high_level} at \textit{high-level}. The optimization \eqref{eq::high_level} is a nonlinear program whose global optimum is difficult to compute in real time. Therefore we only focus on methods that help improve the estimates of \eqref{eq::high_level} but not necessarily lead to convergence to the global optimum. 
%However, since our framework allows for gradient flows or dynamics for $\theta_\alpha$, we resort to gradient-descent-based methods for solving \eqref{eq::high_level}  .
%Therefore, we focus on gradient-descent-based optimization of $\theta_\alpha$ to produce gradient-flows for $\theta_\alpha$ that can be implemented online as our autonomous agent moves in the environment. 

Since we are looking for a real-time receding horizon approach, we resort to gradient descent, specifically SQP, based solutions of optimization problems and propose to use the sensitivity w.r.t $\theta_\alpha$ of cumulative reward \eqref{eq::mpc_reward} that is achieved for a fixed $\theta_\alpha$ and given initial state $x_\tau$. That is, at time $\tau$ we predict the future states by propagating the system \eqref{eq::dynamic_discrete} forward in time with control at each time step chosen in \eqref{eq::MPC_u} as $u_{s} = \pi_{QP}(x_s;\theta_{\alpha_\tau})$ and obtain the sensitivity (gradient) of predicted reward w.r.t $\theta_\alpha$. This sensitivity is then used to design $\dot \theta_\alpha$ at time $\tau$ and the procedure is repeated in receding horizon fashion at every time $\tau'>\tau$. 

% Our objective is to find $\dot \theta_\alpha(t)$ to maximize the accumulated reward of the following finite-horizon optimal control problem
% \begin{subequations}
%     \begin{align}
%         \dot{\alpha} = \arg \min_{f' } \quad & \int_{t=t_0}^{\infty} r(x,u) \label{eq::OC_alpha_objective}\\
%     \textrm{s.t.} \quad & \dot{x} = f(x)+ g(x)u\\
%     & \dot \alpha = \\
%     & \dot{\psi}_i^k \geq -\alpha_{i}^k(\psi_i^k(t,x)) \\
%     & \dot \theta_\alpha = f_\alpha(x,\theta_\alpha)\\
%                 & \dot{V} \leq -kV \\
%                 & u \in \mathcal{U} \\
%                 & \alpha_{i,k}( \psi^k_i(t,x) ) > 0 \\
%                 & \forall i\in \{ 1,2,..,N \}
%                 & \forall k \in \{1,2,..,k\}
%     \end{align}
% \end{subequations}
% \textbf{DOES NOT MAKE SENSE: ALSO DO YOU KNOW THE RIGHT HAND SIDE OF 31d OR NOT? } where the optimization is to find a function $\dot\alpha $. Suppose $\alpha_i$ can be represented as a parametric function \textbf{WHAT PARAMETRIC FUNCTION? HOW IS IT MODELED?} whose parameters are $\theta_i(t) \in \reals^{N_i}$. Let $\theta=[\theta_1 ~, ... , \theta_N]$. Then the optimization can be framed to find $\theta(t)$. 



%  \begin{equation}
%         \begin{aligned}
%             \max_{\pi} \quad & J(\pi) =  \sum_t \gamma^t R(x_t,u_t) \\
%             \textrm{s.t.} \quad & \mathcal{C}_i(\pi) =  \sum_t \gamma^t c_i(x_t,u_t) \geq \bar{\mathcal{C}}_i, i \in \mathcal{I}
%         \end{aligned}
%         \label{eq::CMDP}
%     \end{equation}

%For example, it can be designed based on desired state and distance from safe set boundary or for minimization of total control input over time period.
One challenge that shows up in practice is that the barrier functions $h_i$ may not be persistently compatible for an initial estimate of parameter $\theta_\alpha$ because the set $\mathcal{A}$ is unknown as mentioned in Remark \ref{remark::rtcbf_Aset}. Thus, controller $\pi_{QP}(x_s,\theta_{\alpha_\tau})$ in \eqref{eq::CBF_MDP} may not admit a solution for all $s>\tau$. Therefore we propose different update laws depending on whether the current choice of the parameter $\theta_{\alpha\tau}$ at time $\tau$ leads to a feasible or infeasible control input trajectory at future times $s\in [\tau, \tau+T]$. For this purpose, we first introduce the following definitions. 



\begin{definition}
Consider the system \eqref{eq::dynamic_discrete} evolving under the policy $\pi_{QP}(x;\theta_\alpha)$ given by \eqref{eq::CBF_MDP} with fixed parameter $\theta_\alpha$. 
\begin{itemize}
    \item The parameter $\theta_\alpha$ is called feasible if starting at state $x_\tau$ at time $\tau$, the barrier functions $h_i(s, x_s)$ are compatible for all time $s>\tau$, that is, the controller $\pi_{QP}$ admits a solution for all $s>\tau$.
    \item A parameter $\theta$ is called $T$-feasible for state $x_\tau$ and time $\tau$, if the barrier functions $h_i(s,x_s)$ are compatible for all time $s < \tau+T$, but incompatible at time $\tau + T$.
\end{itemize} 
\end{definition}

In practice, it is sufficient to call a parameter feasible if \eqref{eq::CBF_MDP} remains feasible for the period of interest $T$, which could be the time horizon we expect our autonomous agent to take to finish the task or get away from unsafe regions. Depending on whether we have feasibility or $T$-feasibility, we now specify two objectives to be solved in this section:
\begin{objective}
  Given a feasible $\theta$, design an update rule $\theta^+=F(x,\theta)$, where $F:\reals^n\times \reals^{n_\alpha}$ is a lipschitz continuous function, such that $\theta^+$ is feasible, and $R(x_\tau,\theta^+)\geq R(x_\tau,\theta)$.
\end{objective}

\begin{objective}
    Given a $T$-feasible $\theta$, design an update rule $\theta^+=F(\theta)$ such that $\theta^+$ is $(T+1)$-feasible.
\end{objective}

%TODO: see if text below is required
% The objective is to design an update rule for $\theta$ without losing feasibility of QP over period of interest, i.e., ensuring that a trajectory exists over the same horizon for updated $\theta$. 


%\st{In the subsequent, we refer to \textit{system} to describe evolution of states for a given initial condition and a parameter value.}




% \begin{problem}
% Given the MDP ($\X,\U,r,\gamma$) where $r:\X\times \U \rightarrow \mathbb{R}$ is the reward function and $\gamma$ is the discount factor, solve the CMDP (\ref{eq::CBF_MDP}) so that following is satisfied:
% \begin{enumerate}
%     \item The MDP converges to the optimal stationary policy asymptotically, i.e., $\pi \rightarrow \pi^*$ as $t\rightarrow \infty$.
%     \item For a time-invariant safe set, if the initial policy $\pi_0$ is safe, then all subsequent policies $\pi_t$ are also safe. 
%     \item For a time-varying safe set, find the minimum time $\tau$ over which initial policy $\pi_0$ should be safe so that all subsequent policies $\pi_t$ are also safe over $[0,t']$ where $t'=\text{max}(\tau,t)$.
%     %\item \blue{Minimum prior knowledge (in terms of starting network for example) or minimum volume of state space that initial policy $\pi_0$ should be safe over so that the objective can be achieved asymptotically or in finite time.}
% \end{enumerate}
% \end{problem}




% where the policy $\pi_{QP}(x_t;u_{d_t})$ is given by following CBF-CLF-QP formulation:
% \begin{equation}
%         \begin{aligned}
%             \min_{u} \quad & J(\pi) = (u_t-u_{d_t})^TR(u_t-u_{d_t}) + Q\delta_t^2\\
%             \textrm{s.t.} \quad & V(x_{t+1}) \leq (1-\theta_0)V(x) + \delta \\
%                     &      h_i(x_{t+1}) \leq (1-\theta_i)h_i(x_t) \quad \forall i \in \{1,2,..,N\}
%         \end{aligned}
%         \label{eq::CBF_MDP_policy}
% \end{equation}




\subsubsection{Methodology}
We present two update rules inspired by Sequential Quadratic Programming (SQP) for solving Objectives 1 and 2.

\noindent \textbf{Case 1: Feasible $\theta$}:
First, we design an update rule for improving the performance of a feasible parameter. A gradient-based update rule computes infinitesimal changes in the current estimate of parameter $\theta_{\alpha_\tau}$. We design the update rules based on SQP as introduced in section \ref{section::SQP}. In the context of our problem, the improving-cost direction is simply obtained as the gradient of the reward function $R(x_\tau,\theta)$ in \eqref{eq::mpc_reward} w.r.t $\theta_{\alpha_\tau}$. The set of feasible directions is computed such that if the parameter $\theta_{\alpha_\tau}$ is feasible (or $T$-feasible), then $\theta^+_{\alpha_{\tau}}$ is also feasible (respectively $T$-feasible). Let $e(t,u_t,x_t,\theta_{\alpha_\tau}) \in \reals^{N+1+q} \geq 0$ represent all inequality constraints at time $t$ in \eqref{eq::MPC_u} which are respectively given by \eqref{eq::inequality_CLF_QP}, \eqref{eq::inequality_CBF_QP}, and \eqref{eq::linear_input_bounds}. The update direction $d$, following the SQP rule in Section \ref{section::SQP}, is thus obtained as a result of the following linear program. 
\begin{subequations}
  \begin{align}
      d = \arg \max_{d_\theta} & \quad \langle \nabla_\theta R(x_\tau,\theta_{\alpha_\tau}), d_\theta \rangle \\
    \textrm{s.t.} \quad & e(s,u_s,x_s;\theta_{\alpha_\tau}) + \nabla e(s,u_s,x_s;\theta_{\alpha_\tau}) d_{\theta} \geq 0, \nonumber \\
        & \quad \forall s\in \{\tau,\tau+1,..,\tau+T \}
  \end{align}
  \label{eq::feasible_sqp}
\end{subequations}
\vspace{-0.5cm}
\begin{remark}
    The rule \eqref{eq::feasible_sqp} uses the first-order approximation of the change in the value of function $e(t,u_t,x_t,\theta_\alpha)$ when the parameter $\theta_\alpha$ is perturbed. This is equivalent to linearizing the solution of \eqref{eq::dynamic_discrete} about the current trajectory $(x_s,u_s), s\in \{\tau,\tau+1,..,\tau+T \}$. This rule employing SQP is also inspired by Differential Dynamic Programming literature\cite{tassa2014control} that iteratively improves estimates of the optimal trajectory by using the sensitivity of future trajectory w.r.t a given sequence of control input. However, unlike DPP, we use a parameterized controller rather than considering a sequence of control inputs, and we use sensitivities for the purpose of automated tuning of controller parameters.     
\end{remark}

\noindent \textbf{Case 2: $T$-feasible $\theta$}: Second, we design an update rule to perturb a $T$-feasible parameter with the goal of making it a $(T+1)$-feasible parameter. Given that the QP \eqref{eq::CBF_MDP} is infeasible at $\tau+T$, we add slack variables to all the hard constraints so that the imposed constraint is 
\eqn{
   e(\tau+T,u_{\tau+T+1},x_{\tau+T+1};
   \theta) + \bar \delta \geq 0
}
The slack $\bar \delta>0$ is thus the infeasibility margin of the hard constraints and should be minimized to zero. The high-level objective to find the update direction $d$ thus becomes
\begin{subequations}
    \begin{align}
         d = \min_{d_\theta} \quad & \langle \nabla_\theta  (\bar \delta^T \bar \delta), d_\theta \rangle \\
        & e(s,u_s,x_s;\theta_{\alpha_\tau}) + \nabla e(s,u_s,x_s;\theta_{\alpha_\tau}) d_{\theta} \geq 0, \nonumber \\
        & \quad ~~~~~~~~~~~~~~~~~~  \forall s\in \{\tau,\tau+1,..,\tau+T-1 \}
    \end{align}
    \label{eq::T_feasible_sqp}
\end{subequations}
\noindent Note that the only independent variables in \eqref{eq::feasible_sqp}, \eqref{eq::T_feasible_sqp} are the initial state $x_\tau$ and the parameter $\theta$, and therefore the gradients have to be computed w.r.t. to these independent variables.
% This is done by first identifying the constraint at T+1, 
% %say $C_1(x_T) + C_2^T(x_{T})u>=0$ for some real valued functions $C_1,C_2$ and control input $u$, 
% that needs to be relaxed the least of all other constraints so that the QP at T+1 becomes feasible. A direction that reduces the infeasibility margin of this constraint is then computed 
% %as its positive gradient direction of $C_1(x_T)$ 
% with respect to $\theta$. This direction is then projected to set of feasible directions of QPs from t=1 to t=T and the resulting direction is used for update with learning rate $\beta$. 
The next two subsections discuss how we can compute gradients by backpropagating. These gradients can then be used in \eqref{eq::feasible_sqp}, \eqref{eq::T_feasible_sqp} to find $d$. In the next section, we describe how automatic differentiation can be used to obtain the required gradients.

\subsubsection{Gradient through Backpropagation}
\label{section::backprop}
The dynamical system \eqref{eq::dynamic_discrete} can be visualized as a computational graph (see Fig.\ref{fig:graph}) where the nodes either represent independent variables or functions, and incoming edges represent the arguments to the function and the flow of information. The gradient of each function node can be obtained either from the system dynamics or through the sensitivity analysis of the QP. Since the dynamics is deterministic and known, we can roll-out a trajectory to the future and compute the gradient of the objective function with respect to $\theta$ and initial state $x_\tau$ at time $\tau$. As an example, for $T=2$,
\eqn{
   \frac{\partial R(x_\tau,\theta)}{\partial \theta} = \frac{\partial r(x_\tau,u_\tau)}{\partial \theta} + \frac{\partial r(x_{\tau+1},u_{\tau+1})}{\partial \theta}
   \label{eq::chain_rule1}
}
Here, we have
% \eqn{
%   \frac{\partial r(x_1,u_1)}{\partial \theta} = \underbrace{\frac{\partial r_1 }{\partial x_1}}_{known}\underbrace{\frac{\partial x_1}{\partial \theta}}_{0} + \underbrace{\frac{\partial r_1}{\partial u_1}}_{known}\frac{\partial u_1}{\partial \theta}
% }
% where the last term $\frac{\partial u_1}{\partial \theta}$ is the gradient of QP solution w.r.t the parameters which can be obtained using available libraries\cite{diamond2016cvxpy}. Similarly, we have
\eqn{
   \frac{\partial r(x_{\tau+1},u_{\tau+1})}{\partial \theta} = \underbrace{\frac{\partial r_{\tau+1} }{\partial x_{\tau+1}}}_{known}\frac{\partial x_{\tau+1}}{\partial \theta} + \underbrace{\frac{\partial r_{\tau+1}}{\partial u_{\tau+1}}}_{known}\frac{\partial u_{\tau+1}}{\partial \theta}
   \label{eq::chain_rule2}
}
with $r_{\tau+1}=r(x_{\tau+1}, u_{\tau+1})$.  Since $x_{\tau+1} = f(x_{\tau}) + g(x_{\tau})u_{\tau}$ is a function of $x_{\tau}$ and $u_{\tau}$
\eqn{
  \frac{\partial x_{\tau+1}}{\partial \theta} = \underbrace{\frac{\partial x_{\tau+1}}{\partial x_{\tau} }}_{\textrm{from dynamics}}\underbrace{\frac{\partial x_{\tau} }{\partial \theta}}_{\textrm{$x_{\tau}$ constant}} + \underbrace{\frac{\partial x_{\tau+1}}{\partial u_{\tau}}}_{\textrm{from dynamics}}\frac{\partial u_{\tau}}{\partial \theta}
  \label{eq::chain_rule3}
}
$\frac{\partial u_{\tau}}{\partial \theta}, \frac{\partial u_{\tau+1}}{\partial \theta}$ are gradients of QP's solution w.r.t. the parameters which can be obtained by performing sensitivity analysis of QPs which is readily implemented in libraries such as cvxpy\cite{diamond2016cvxpy}. 
%\st{For $T>2$, all the gradients can be recursively computed in terms of initial state $x_1$ and the parameter $\theta$.}
The chain-rule based recursive analysis that we employ in \eqref{eq::chain_rule1}-\eqref{eq::chain_rule3} to compute gradients of stage-wise reward w.r.t the parameter $\theta$ is amenable to backpropagation-based derivative computation and is also readily implementable in libraries like pytorch and tensorflow, that are capable of constructing computational graphs. The gradient of other quantities of interest, such as barrier function, can be obtained similarly.

% \begin{figure}[htp]
%     \centering
%     \includegraphics[scale=0.45]{figures/MPC_graph_derivative.png}
%     \caption{Red edges represent QP optimization layer, green edges represent dynamics equations, and the brown edges represent the reward function which is designed by user.}
%     \label{fig:graph}
% \end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.3]{figures/computational_graph.png}
    \caption{\small{Nodes represent functions, specified here by dynamics or QP controller, and edges represent arguments to the function. Thick blue arrows show the flow of information when computing gradient of second reward $r_2=R(x_2,u_2)$ with respect to $\theta$ through backpropagation.}}
    \label{fig:graph}
\end{figure}
%, is projected to set of feasible directions in the next section.

%\st{Note that this problem could also be solved if a $T$ horizon MPC problem is formulated for control computation but the safety constraints will be nonlinear and the solution cannot be calculated in real-time with current computation power.}
% \vspace{-5mm}
\subsubsection{Constraint Aware Gradient Descent}
Using the gradients obtained through the backpropagation procedure we can solve the optimization problems \eqref{eq::feasible_sqp} and \eqref{eq::T_feasible_sqp} to obtain the update direction $d$. We now use the following update 
\eqn{
   \theta_{t+1} = \theta_t + \beta d,
   \label{eq::gd_update}
}
where $\beta \in (0,1]$ is the learning rate of gradient descent. Since $d$ was computed based on linearization about the rolled-out trajectory corresponding to the parameter $\theta$, we use a small learning rate to ensure approximation errors due to linearization are suppressed. Note that choosing a proper learning rate to suppress linearization errors has been an active research area due to the popularity of gradient descent-based updates for training highly nonlinear neural network models\cite{bengio2012practical,zhang2018improved} and is outside the scope of this paper. In practice, some manual tuning is required for our approach when applied to an agent with new nonlinear dynamics.

\begin{remark}
    While SQP provides constraint-maintenance guarantees only up to first-order approximation due to linearization, one can employ Feasible SQP\cite{tits2009feasible} algorithms that can guarantee constraint satisfaction for higher-order approximations. However, since these methods also require computation of higher-order gradients such as Hessians, which can also be obtained through backpropagation, they add to the computational complexity of the algorithm.
\end{remark}

\begin{remark}
Since each update is based on a single trajectory roll-out and backpropagation and solving the linear program, \eqref{eq::feasible_sqp} or \eqref{eq::T_feasible_sqp}, it can be performed very fast and is suitable for online implementation. In practice, in our receding horizon framework, for Case 1, a single update based on \eqref{eq::feasible_sqp} can be done at each time $\tau$. For Case 2, we can perform multiple updates at each time $\tau$ based on \eqref{eq::T_feasible_sqp} until we reach a parameter $\theta$ that is $T+1$ feasible. Since SQP in general does not provide guarantees for finite time convergence to local optimum for generic nonlinear optimization problems, a thorough analysis of conditions on system dynamics and initial parameter choice under which we can guarantee convergence is outside the scope of this paper and will be addressed in future work. We show the efficacy of our method through an extensive simulation study in Section \ref{section::simulation}. Our work thus brings together elements from SQP, DDP, and backpropagation based gradient computation for computational graphs to propose an automated tuning framework for controllers.  
\end{remark}
%\st{The framework presented is not meant to generate a valid barrier function directly but to constructively avoid going to states for which the chosen barrier function is not valid.} While the usual CBF conditions lose validity here as changing parameter values do not satisfy the regularity properties such as class-$\mathcal{K}$, for example, of $\alpha_i h$ in Example \ref{example::unicycle}, 
% \begin{remark}
% The dynamics equation is not explicitly considered in computation of feasibility directions as (1)CBFs have conveniently combined the dynamics with state constraint in a single inequality and (2) the backpropagation does take into account dynamics while traversing the computational graph.
% \end{remark}

% \subsubsection{(Will update later): Anticipation-based Terminal Cost Function}
%  Designing a terminal cost function is non-trivial as it is supposed to represent the residual cost of the infinite horizon problem. Since the ability to predict precludes  uncontrollable states with unknown dynamics, we have $x_j\equiv 0, \dot x_j \equiv 0$ and the constraint margin $m_i^k$ is computed based on remaining terms in \eqref{eq::allowed_halfspace}. The terminal cost function is implicitly defined as
% \eqn{
%  \frac{d\Phi(x_T)}{d\alpha_i^0} = f_\alpha (\rho^0_{i})
% }
\vspace{-0.4cm}
 \subsection{Pointwise Sufficient Conditions for Compatibility based on Lipschitz constants}
\label{section::pointwise_sufficient}
In this section, we derive a lower bound based on Lipschitz constants to be imposed on $\dot \theta_{\alpha_i^k}$ at any given time $t$ and state $x,\theta_\alpha$ so that the barriers $h_i, i\in\{1,.., N\}$ remain compatible for as long as the aforementioned lower bound is well-defined.
 
Consider the augmented dynamical system in  \eqref{eq::augmented} subject to multiple barriers functions $h_i, i \in  {1,2..,N}$.
 Let the controller $u$ be formulated as
\begin{subequations}\label{eq::theorem_multi_barriers}
    \begin{align}
        \begin{split}\label{eq::theorem_multi_barriersa} 
         u(x,\theta_\alpha) = \arg \min_{u} \quad & || u - u^r ||^2
         \vspace{-0.1cm}
        \end{split}\\ 
        \begin{split} \label{eq::theorem_multi_barriersb}
        \textrm{s.t.} \quad & \psi^{r_i}_{i}(t,x,u,\theta_{\alpha_i}) \geq 0
        \end{split}
    \end{align}
% \vspace{-0.1cm}
\label{eq::th2_eq}
\end{subequations}
with corresponding parameters $\theta_{\alpha_i}$. Suppose the solution of QP in \eqref{eq::theorem_multi_barriers}, the control $u(x,\alpha) \in \U$, is always locally Lipschitz continuous in $x$ and $\theta_\alpha$ so that the closed-loop dynamics of $x$ denoted as $\dot x=F(x)\triangleq f(x)+g(x)u(x,\theta_\alpha)$ is locally Lipschitz continuous. Let $L_{\psi_i^k},L_{\dot{\psi}_i^k}$, $L_F$, and $L_{f_{\theta_i}}$ be the Lipschitz constants of $\psi_i^k, \dot{\psi}_i^k$, $F(x)$, and $f_{\theta_i}$, $i\in\{1,..,N\}, k\in\{1,..,r_i\}$. 
%Whenever a solution Suppose a solution to (\ref{eq::th2_eq}) exists at given time $t$ 

Let $m_{i}^k$ be the the constraint margin by which CBF derivative condition \eqref{eq::hocbf_derivative_condition} is satisfied, that is, 
\eqn{
  m_{i}^k = \dot\psi_i^k + \alpha_i( \psi_i^k ) 
}
For the system state $x$ evolving according to \eqref{eq::dynamics_general}, a necessary condition for persistent compatibility is
\eqn{
   m_i^k + d m_i^k \geq 0
   \label{eq::necessary_compatibility}
}
where $dm_i^k = \dot m_i^k dt$ is the infinitesimal change in $m_i^k$ for an infinitesimal increment in time $dt$. Here $\dot m_i^k$ depends on the chosen control input $u(t,x,\theta_\alpha)$ which in turn is not only dependent on $m_i^k$ but also on $m_j^{l_j}, j\in \{1,..,N\}\setminus i, l_j\in \{1,..,r_j\}$. This coupling is non-trivial to model, however, the knowledge of local lipschitz constants can be used to decouple the constraints and give a point-wise sufficient condition for compatibility at time $t+dt$. 
Using the following decomposition 
\eqn{
    d m_{i}^k = d\dot  \psi_i^k + \frac{\partial \alpha_i^k(\psi_i^k)}{\partial \psi_i^k}d \psi_i^k + \frac{\partial \alpha_i^k( \psi_i^k )}{\partial \theta_{\alpha_i^k}}d \theta_{\alpha_i^k}
}
the necessary condition \eqref{eq::necessary_compatibility} results in 
\eqn{
d \theta_{\alpha_i^k} \geq \frac{-\left(  m_i^k + d\dot \psi_i^k + \frac{\partial \alpha_i^k(\psi_i^k)}{\partial \psi_i^k}d \psi_i^k \right)  }{ \frac{\partial \alpha_i^k( \psi_i^k )}{\partial \theta_{\alpha_i^k}}} 
}
Using the local Lipschitz constants, we have the following relationships
\eqn{
  d\dot\psi_i(x,\dot x) \leq L_{\dot \psi_i^k} \bigg|\bigg| \begin{bmatrix} d x \\ d \dot{x}  \end{bmatrix} \bigg|\bigg| & \leq L_{\dot \psi_i^k}( || d x || + || d \dot{x} ||) \\
  & \leq L_{\dot \psi_i^k}( || d x || + L_F || d x ||) \\
  & = L_{\dot \psi_i^k}|| d x ||( 1 + L_F )
}
and 
\eqn{
   d\psi_i^k \leq L_{\psi_k^i} ||dx||
}
which gives us
\eqn{
d \theta_{\alpha_i^k} \geq \frac{-\left(  m_i^k + L_{\dot \psi_i^k}|| d x ||( 1 + L_F ) + \frac{\partial \alpha_i^k(\psi_i^k)}{\partial \psi_i^k}L_{\psi_k^i} ||dx|| \right)  }{ \frac{\partial \alpha_i^k( \psi_i^k )}{\partial \theta_{\alpha_i^k}}} 
\label{eq::lower_bound}
}
where $dx = \dot x dt$ and $d \theta_{\alpha_i^k} = \dot \theta_{\alpha_i^k} dt$. Here as $dt\rightarrow 0$, the condition plays an active role only if one of $m^i_k \rightarrow 0$, that is, the right-hand side is $>0$ forcing $\theta_{\alpha}$ to change in direction that makes CBF less conservative. However, since the application of control algorithms are always in discrete time, $dt=\Delta t$ is the sampling time step, $dx$ is obtained based on the change in the state $x$ from the last sampling time, that is, $t-\Delta t$, and $d \theta_{\alpha_i^k} = \dot \theta_{\alpha_i^k} \Delta t$. The above lower bound can then be used to constrain any user-specified $\dot \theta_{\alpha_i^k}$. This bound is also helpful when dynamics is unknown but the lipschitz constants are known. In these cases, the application of the framework proposed in section  \ref{section::predictive_framework} cannot be applied as prediction is not possible. Finally, \eqref{eq::lower_bound} is well-defined only when $\frac{\partial \alpha_i^k( \psi_i^k )}{\partial \theta_{\alpha_i^k}}\neq 0, \forall i,k$ as this may require $d\theta_{\alpha^k_i}\rightarrow\infty$. For example, in the case of the linear $\classK$ function in \eqref{eq::linear_classK_example}, the bound \eqref{eq::lower_bound} is not defined when $h_i=0$.

\section{RT-CBFs for Non-Cooperating Multi-agent systems}
\label{section::multi-agent}
In this section, as an application case study, we introduce RT-CBFs and propose $\classK$ function parameter dynamics for decentralized non-cooperating multi-agent systems based on our work in \cite{parwana2022trust}. We show through simulations that our online parameter adaptation scheme leads to a less conservative response by quantifying the ease of satisfaction of CBF constraints. 
Consider a heterogeneous multi-agent system with $N+1$ agents. The state of each agent $a, a\in \{1,..,N+1\}$is denoted as $x_a \in \reals^{n_a}$ and follows the dynamics 
\eqn{
  \dot x_a = f_a(x_a) + g(x_a)u_a 
  \label{eq::dynamics_multi_agent}
}
where $f_a:\reals^{n_a}\rightarrow \reals^{n_a}, g_a:\reals^{n_a}\times \reals^{n_a\times m_a}$ are locally lipschitz continuous functions and $u_a \in \reals^{m_a}$ is the control input of agent $a$. The set of neighbors of agent $a$ is denoted as $\mathcal{N}_a=\{1,..,N+1\}\setminus a$. We consider a decentralized scenario where each agent decides its own control input under the following assumptions
\begin{assumption}
\label{assumption:multi_agent}
    Each agent $a$ can make perfect observations of $x_b$ and $\dot x_b$ of every other agent $b\in N_a$. Furthermore, $a$ knows a lipschitz bound of the closed-loop dynamics of $b$. 
\end{assumption}
The knowledge of lipschitz bounds under Assumption \ref{assumption:multi_agent} allows us first design a \textit{trust-based} parameter dynamics in this section and then use results from Theorem \ref{section::pointwise_sufficient} to ensure point-wise compatibility for multiple constraints that each agent has to satisfy. 


The safety constraints are defined for every pair of two robots. These could be collision avoidance or connectivity maintenance constraints. We will design a controller for only a subset of all agents, which we shall refer to as ego agents. Each ego agent $i$ thus has to satisfy $N=M$ constraints specified in terms of safe sets $\s_{ij}$ and corresponding barrier functions $h_{ij}, j\in \mathcal{N}_i$ as defined in \eqref{eq::safeset}. The controller employed by an ago agent is given by \eqref{eq::HOCBF-CLF-QP} with barrier function $h_{ij}$ and derived barrier functions $\psi_{ij}^k, j\in \mathcal{N}_i, k\in \{1,..,r_{ij}\}$. An agent $j$ may have one of the following behaviors towards agent $i$:
\begin{itemize}
    \item Cooperative: when $j$ decides it's control input $u_j$ such that $\frac{\partial h_{ij}}{\partial x_j}\dot x_j \geq 0$ 
    \item Uncooperative: when $j$'s control input is not dependent on $x_i$, that is, when $j$ moves disregarding $i$. This could be, for example, an obstacle moving in a fixed direction irrespective of other agents in the environment.
    \item Adversarial: when $j$ decides its control input such that $\frac{\partial h_{ij}}{\partial x_j}\dot x_j \leq 0$
\end{itemize}
The above definitions can be used to classify behaviors when there are only two agents $i$ and $j$. In presence of multiple agents, a cooperative agent $j$ may be perceived as an adversarial agent by agent $i$ under the influence of another agent $k$ that is moving towards $j$. Therefore, if the identities of agents are not known apriori, it is not trivial to infer them based on observations only. Even when identities are known, the controllers still need to be tuned not only for the response to different types of agents but also for the type of motion exhibited by a particular type of agent. For example, a fast and slow adversarial agent will ideally warrant more and less conservative responses respectively from the ego agent.

A dynamic environment with moving agents implies that the world is inherently time-varying from the perspective of an ego agent $i$. Therefore, as the number of agents and their initial location changes, a new environment is created for agent $i$ and it is not possible to consider all possibilities to tune a controller's response in offline simulations. Therefore, an online adaptation scheme to shape controller response to make it less or more conservative based on observations becomes necessary. 
A decentralized system precludes the use of predictive methods developed in Section \ref{section::predictive_framework}. The next section is thus devoted to developing a \textit{trust metric} based on instantaneous observations that is used to modify $\theta_{\alpha_ij}^k$.



\subsubsection{Design of Trust Metric}
\label{section::trust_metric}
Consider the following CBF constraint
\eqn{
    \dot \psi^{k-1}_{ij} & \geq -\alpha_{ij}^{k}( \psi^{k-1}_{ij} )\\
    \implies \frac{\partial \psi^{k-1}_{ij}}{\partial t} + \frac{\partial \psi^{k-1}_{ij}}{\partial x_i}\dot{x}_i + \frac{\partial \psi^{k-1}_{ij}}{\partial x_j}\dot x_j  & \geq -\alpha_{ij}^{k-1}( \psi^k_{ij} ) \label{eq::cbf_two_states}
}
\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.25]{figures/halfspace_journal.png}
    \caption{\small{Illustration of the halfspaces induced by the constraint function. $Av\geq b$ represents the halfspace described by \eqref{eq::allowed_halfspace} with $A = \frac{\partial h_{ij}}{\partial x_j}$, $v=\dot{x}_j$, and $b =  -\alpha h - \max \left\{ \frac{\partial h_{ij}}{\partial x_i}\dot{x}_i \right\}$. The two $x_j$ are possible instances of the actual motion of agent $j$. $\hat{s}$ is the normal to the hyperplane $Av=b$. $\hat n_j$ is the nominal direction of motion for agent $j$.}
    }
    \label{fig:allowed_space}
\end{figure}

 The margin by which the CBF condition (\ref{eq::cbf_two_states}) is satisfied, i.e., the value of $\dot \psi^{k-1}_{ij} +\alpha_{ij}^{k-1}( \psi^{k-1}_{ij} )$ represents of the ease of satisfaction of constraint w.r.t agent $i$. The best case action by agent $i$ thus corresponds to the largest possible margin for a given motion $\dot{x}_j$ of agent $j$ and is an important cue that helps infer the nature of agent $j$. Furthermore, as we will be discussing later, this equation also paves the way to mathematically incorporate our prior belief of how the agent $j$ is supposed to behave. \\
 
% \begin{enumerate}
    % \item 
%   \subsubsection{Safe motions of agent $j$: Constraint Margin based Trust score} %
\noindent \textbf{Constraint margin based trust score:} The motions of agent $j$ that lead to a feasible solution to (\ref{eq::cbf_two_states}) are considered as \textit{safe} by $i$. Consider the following decomposition
   \eqn{
      \dot \psi^{k-1}_{ij} = \frac{\partial \psi^{k-1}_{ij}}{\partial t} + \frac{\partial \psi^{k-1}_{ij}}{\partial x_i}\dot{x}_i + \frac{\partial \psi^{k-1}_{ij}}{\partial x_j}\dot x_j + \frac{\partial \psi^{k-1}_{ij}}{\partial \alpha^{k}_{ij}}\dot \alpha^{k}_{ij}
   }
   Here, we have ignored the higher order derivatives of $\alpha^{m}_{ij}, m<k-1$. The safe motions of $j$ are thus given by
    \eqn{
     \frac{\partial \psi^{k-1}_{ij}}{\partial x_j}\dot x_j & \geq - \alpha_{ij}^{k}( \psi^{k-1}_{ij} ) -  \frac{\partial \psi^{k-1}_{ij}}{\partial \alpha^{k}_{ij}}\dot \alpha^{k}_{ij} - \frac{\partial \psi^{k-1}_{ij}}{\partial t} - \frac{\partial \psi^{k-1}_{ij}}{\partial x_i}\dot{x}_i  \\
     & \geq - \alpha_{ij}^{k}( \psi^{k-1}_{ij} ) - \frac{\partial \psi^{k-1}_{ij}}{\partial \alpha^{k}_{ij}}\dot \alpha^{k}_{ij} -\frac{\partial \psi^{k-1}_{ij}}{\partial t} \nonumber \\
     & \hspace{4cm} - \max_{u_i \in \mathcal{U}} \left\{ \frac{\partial \psi^{k-1}_{ij}}{\partial x_i}\dot{x}_i \right\}
      % \frac{\partial h_{ij}}{\partial x_j}\dot{x}_j &\geq -\alpha_{ij} -\frac{\partial h_{ij}}{\partial x_i}\dot{x}_i, \\
      % &\geq -\alpha h - \max_{\dot{u}_i} \left\{ \frac{\partial h_{ij}}{\partial x_i}\dot{x}_i \right\}.
      \label{eq::allowed_halfspace}
    }
    This gives a lower bound on motions of agent $j$, a violation of which will lead to the infeasibility of QP controller of agent $i$. The design of the trust factor is thus based on a comparison of the contribution of agent $j$ with the aforementioned lower bound. Note that the required maximum value in Eq. (\ref{eq::allowed_halfspace}) can be obtained from the following Linear Program (LP):
    %\eqn{
     %   \max \quad & \frac{\partial h_{ij}}{\partial x_i}\dot{x}_i \\
        %\textrm{s.t.} \quad & \textrm{other barrier constraints hold} 
      %  \textrm{s.t.} \quad & \dot{h}_{ik} \geq -\alpha_{ik} h_{ik} %~~\forall k\in \mathcal{V}\setminus {i,j}
    %}
\begin{subequations}\label{eq::LP}
    \begin{align}
        \begin{split}\label{eq::LPa} 
             \max_{u_i} \quad & \frac{\partial \psi^k_{ij}}{\partial x_i}\dot{x}_i 
             \vspace{-0.1cm}
        \end{split}\\ 
        \begin{split} \label{eq::LPb}
            \textrm{s.t.} \quad & \dot{\psi}^k_{il} \geq -\alpha_{il} \psi^k_{il}, ~~\forall l\in N_i\setminus j.
        \end{split}
    \end{align}
% \vspace{-0.1cm}
\end{subequations}
\begin{remark}
    For $k<r-1$, $\frac{\partial \psi^k_{ij}}{\partial x_i}\dot{x}_i$ will be independent of $u_i$ and the above linear program need not be solved.
\end{remark}

Equation \eqref{eq::allowed_halfspace} represents a halfspace whose separating hyperplane has the normal direction $\hat s_{ij}^k=\frac{\partial \psi^k_{ij}}{\partial x_j}$, and has been visualized in Fig. \ref{fig:allowed_space}.
Let the half-space in \eqref{eq::allowed_halfspace} be represented in the form $A_{ij}^kv \geq B_{ij}^k$ with $v = \dot{x}_j$ and $A_{ij}^k,B_{ij}^k$ defined accordingly. Let $m_{ij}^k$ be the constraint margin by which \eqref{eq::allowed_halfspace} is satisfied
\eqn{
m_{ij}^k = A_{ij}^kv + B_{ij}^k %b - Av,
}
where $m_{ij}^k<0$ is an incompatible vector, corresponding to scenarios that should never happen. The larger the value of $m_{ij}^k$, the greater the ease of satisfaction for agent $i$. Hence, a larger $m_{ij}^k$ allows room for relaxing the constraint. Therefore, the constraint margin-based trust score is designed as
\eqn{
\rho_{m_{ij}^k} = f_d(m_{ij}^k), k\in \{ 1,..,r \}
\label{eq::rho_d}
}
where $f_d:\reals^+\rightarrow[0,1]$ is a monotonically-increasing, Lipschitz continuous function, such that $f_d(0)=0$. A possible choice would be $f_d(d) = \tanh(\beta d)$, with $\beta$ being a scaling parameter. \\

\noindent \textbf{Belief based trust score:}
We now describe a method to incorporate $i$'s prior belief of the motion of agent $j$. Note that infinitely many values of $\dot x_j$ may satisfy the CBF derivative condition \eqref{eq::allowed_halfspace} with the same constraint margin (all $\dot x_j$ lying on the yellow hyperplane in Fig. \ref{fig:allowed_space} will have the same $m_{ij}^k$). This gives us an additional degree of freedom for deciding which motions are more trustworthy. Consider the scenario shown in Fig.\ref{fig::infinite_solutions} where an agent $j_1$ is moving directly towards $i$ with speed $2$ and another agent moving at an angle $\phi$ w.r.t the vector $x_i - x_j$ but with speed $2/\cos45^0$. Both $j_1$ and $j_2$ would thus lead to the same constraint margin but we would like to trust $j_2$ more if we had a belief that it is supposed to be moving in that direction or because it is not moving straight towards $i$. We denote this nominal belief direction of agent $j$ w.r.t agent $i$ as $b_{ij}$. If $\dot x_j$ performs worse than the nominal direction, then it is a cause of concern as it is not consistent with the $i's$ belief for agent $j$. Note that $b_{ij}$ need not be the true direction of motion of agent $j$ for our algorithm to work since if $b_{ij}$ is not known, we can consider the worst-case scenario where $b_{ij}=x_i-x_j$, i.e., that $j$ is an adversary and intends to collide with agent $i$; over time, our algorithm will learn that $j$ is not moving along $b_{ij}$ and therefore increases $i's$ trust of $j$. Knowledge of nominal direction plays an important role in shaping belief as it helps to distinguish between uncooperative and adversarial agents.

\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.15]{figures/infinite_solutions.png}
    \caption{\small{Agents $j_1$ and $j_2$ contribute the same constraint margin for satisfying the CBF derivative condition \eqref{eq::hocbf_derivative_condition}. 
    %Comparing their heading directions w.r.t the gradient direction of the barrier function, pointing towards agent $i$ from their locations in this case, and any belief that $i$ may have on the motion of $j_1,j_2$ offers another degree of freedom to trust one agent more over the other.
    }
    }
    \label{fig::infinite_solutions}
\end{figure}

Suppose the angle between the vectors $b_{ij}$ and $\hat s_{ij}^k$ is denoted by $\gamma_{s_{ij}}^{b_{ij}}$, and the angle between $\dot x_j$ and $\hat s_{ij}$ is denoted by $\gamma_{s_{ij}^k}^{\dot x_{j}}$. The belief-based trust is designed as
% \eqn{
% \rho_\theta = \left\{
% 	\begin{array}{cl}
% 		1  & \mbox{if } \theta_s^a > \theta^n_s, \theta_a^n \geq \bar \theta  \\
% 		\beta_2 + \theta_a^n/\bar \theta & \mbox{if } \theta_s^a > \theta^n_s, \theta_a^n < \bar \theta \\
% 		\beta_2 & \mbox{if } \theta_s^a = \theta^n_s \\
% 		\beta_1 + (\beta_2-\beta_1)\theta_s^a/\theta_s^n & \mbox{if } \theta_s^a < \theta^n_s
% 	\end{array}
% \right.
% }
\eqn{
\rho_{\gamma_{ij}} = f_\gamma( \gamma^{b_{ij}}_{s_{ij}^k}/\gamma^{\dot x_j}_{s_{ij}^k} ),
%\rho_\theta = \beta_2 + \tanh{(\beta_1\theta^n_s/\theta^a_s)}
\label{eq::rho_theta}
}
where $f_\gamma:\reals^+ \rightarrow [0,1]$ is again a monotonically-increasing Lipschitz continuous function with $f_\gamma(0)=0$. Note that even if $\gamma^{b_{ij}}_{s_{ij}^k}=\gamma^{b_{ij}}_{s_{ij}^k}$, i.e., $j$ is perfectly following its nominal direction, we do not design $f_\gamma$ to be 1 as the robot might be uncooperative. We give $f_\gamma$ higher values when $\gamma^{b_{ij}}_{s_{ij}^k}<\gamma^{b_{ij}}_{s_{ij}^k}$, as with $a_{j_2}$ in Fig. \ref{fig:allowed_space}, $j$ seems to be compromising its nominal movement direction for improved safety, thus leading to a higher score. Finally, when ${b_{ij}}_{s_{ij}}<\gamma^{b_{ij}}_{s_{ij}}$, as with $a_{j_1}$ in Fig. \ref{fig:allowed_space}, then $j$'s current motion is performing worse for inter-robot safety than its nominal motion and is therefore either uncooperative/adversarial or under the influence of other robots, both of which lead to lower trust. \\

%$\beta_1,\beta_2$ are chosen such that $\rho_\theta\in [0,1]$ with higher values for a cooperative and cooperative agent.

%Here $\beta_1 \neq 0$ because it does not represent the worst case scenario. It represents the case when system is doing the least for safety. We would like 0 to represent the absolute worst case where the adversary is trying to intentionally collide with other robots. Similarly
%$\beta_1< 1$ because a robot going towards it's goal by exactly following $\hat n$ might be uncooperative and simply ignoring other robots.

\noindent \textbf{Final Trust Score:} The trust metric is now designed based on $\rho_{m_{ij}^k}$ and $\rho_{\gamma_{ij}^k}$. Let $\bar \rho_{m_{ij}^k}\in (0,1)$ be the desired minimum robustness/margin in satisfying the CBF condition \eqref{eq::allowed_halfspace}. Then, the trust metric $\rho_{ij}\in [-1,1]$ is designed as follows:
\eqn{
\rho_{ij}^k = \left\{ \begin{array}{cc}
    (\rho_{m_{ij}^k}-\bar \rho_{m_{ij}^k})\rho_{\gamma_{ij}^k},  & \mbox{if } \rho_{m_{ij}^k} \geq \bar \rho_{m},\\
     (\rho_{m_{ij}^k}-\bar \rho_m)(1-\rho_{\gamma_{ij}^k}),  & \mbox{if } \rho_{m_{ij}^k} < \bar \rho_m.
\end{array} \right.
\label{eq::trust}
}
Here, $\rho_{ij}^k=1$ and $-1$ represent the absolute belief in another agent being cooperative and adversary, respectively. If $\rho_{m_{ij}^k}>\bar \rho_m$, then we would like to have $\rho_{ij}^k>0$, and its magnitude is scaled by $\rho_{\gamma{ij}^k}$ with smaller values of $\rho_{\theta_{ij}^k}$ conveying low trust. Whereas, if $\rho_{m_{ij}^k}<\bar \rho_m$, then we would like the trust factor to be negative. A smaller $\rho_{\gamma_{ij}^k}$, in this case, implies more distrust and should make the magnitude larger, hence the term $1-\rho_{\gamma_{ij}^k}$.
% For a desired minimum robustness in satisfying the safety constraint,  When $\rho_d$ is close to 1, we would like to trust $j$ regardless of $\rho_\theta$ to prevent conservative behavior. Then for some thresholds $\bar \rho_m>0$, the trust is defined as
% \eqn{
%   \rho = \left\{
%         \begin{array}{cl}
%              \rho_\theta \rho_m &  \mbox{if } \rho_m > \bar \rho_m \\
%              \rho_\theta \rho_m &  \mbox{if } \rho_m < \bar \rho_m, \rho_\theta>0.5\\
%              -(1-\rho_\theta)(1-\rho_m) &  \mbox{if } \rho_m < \bar \rho_m, \rho_\theta<0.5
%         \end{array}
%   \right.
%   \label{eq::rho}
% }
% In this design $\rho_\theta$ decides the direction of change and $\rho\theta,\rho_m$ decides
The trust $\rho$ is now used to adapt $\alpha$ with following equation
\eqn{
    \dot{\alpha}_{ij}^k = f_{\alpha}(\rho_{ij}^k),
    \label{eq::alpha_dot}
}
where $f_{\alpha}:[-1,1]\rightarrow \reals$
%, f_\alpha(0)=0$
 is a monotonically increasing function. A positive value of $f_{\alpha_{ij}^k}$ relaxes the CBF condition by increasing $\alpha_{ij}^k$, and a negative value decreases $\alpha_{ij}^k$.
Note that \eqref{eq::allowed_halfspace} needs $\dot \alpha_{ij}^{k-1}$. Therefore, we first design $\dot \alpha_{ij}^1$ and then use it in the design of $\dot \alpha_{ij}^2$ and repeat this procedure for higher order derivatives. This is similar to the cascaded control structures.

% \subsection{Compatible Rate-Tunable CBFs}
% We have discussed the possible non-existence of control law resulting either from multiple conflicting barrier constraints or control input bounds that render the QP infeasible. 

%  Given that $\mathcal{U}_c(0,x(0))$ is non-empty, the objective is to design $\dot{\alpha}_i$ such that for system evolving according to \eqref{eq::dynamics} with $u(t)\in \mathcal{U}_c(t,x)$,  $\mathcal{U}_c(t,x(t))$ is non-empty $\forall t >0$.



\section{Simulation Results}
\label{section::simulation}
Code and videos available at: \href{https://github.com/hardikparwana/RateTunableCBFs}{https://github.com/hardikparwana/RateTunableCBFs}.
In this section, we present three simulation studies that show the efficacy of our proposed algorithms.
\subsection{Predictive Framework vs MPC}
Here we do a comparison of trajectories obtained from the proposed receding horizon approach for tuning CBF controllers and MPC. We validate our approach on two different robots. The first, a single integrator, has following dyamics
\eqn{
    \dot p_x = u_1, \dot p_y = u_2
}
and second, a mobile robot with the following dynamic unicycle dynamics:
\eqn{
  \dot p_x &= v\cos \phi, ~ \dot p_y = v\sin \phi, ~ \dot v = u_1, ~\dot \phi = u_2
}
% and second, a underwater vehicle, with the following dynamics[]
% \eqn{
% \begin{bmatrix}
%     \dot p_x \\
%     \dot p_y \\
%     \dot \phi \\
%     m_{11}\dot w \\
%     m_{22} \dot v \\
%     m_{33} \dot r \\
%     \dot \tau_{w}
% \end{bmatrix} = \begin{bmatrix}
%     w\cos\phi - v\sin\phi \\
%     w\sin\phi + v\cos\phi \\r_i \\
%     m_{22} v r + X_w w + X_{w |w|}w |w| + \tau_{w} \\
%     m_{11} w r + Y_v v + Y_{v|v|}v|v| \\
%     (m_{11}-m_{22}uw) + N_r r + N_{r|r|}r|r| + u_2\\
%     u_1
% \end{bmatrix}
% }
 % where $p_x,p_y$ is the location of the robot in horizontal plane, $\phi$ is the heading angle measured w.r.t X axis, $w,v,r$ are linear and angular velocities of surface vehicle, $u_1,u_2$ are the control inputs, $X_w,X_{w|w|},Y_v,Y_{v|v|},N_r,N_{r|r|}$ are nonlinear drag term coefficients taken from []. Compared to [], instead of treating $\tau_w$ as control input, we have extended the state space to have uniform relative degree w.r.t both the control inputs of the barrier function used for collision avoidance in this section. 
where $p_x,p_y$ are the location of the robot in horizontal plane, $\phi$ is the heading angle measured w.r.t X axis, $u_1,u_2$ are the control inputs. 
The objective is to navigate around two obstacles to reach a goal location $x_g = [g_x ~ g_y]^T$ as shown in Fig.\ref{fig::mpc} with input bounds $|u_1|, |u_2|\leq 3$ for single integrator and $|u_1|\leq 2, |u_2|\leq 5$ for dynamic unicycle. For collision avoidance with the two circular obstacles, the barrier function is designed as $h_i(x) = s_i^2 - s_{min}^2$ where $s_i$ is the distance of the robot from the center of $i^{th}$ circular obstacle and $s_{min}$ is the collision avoidance radius. The stage-wise reward for high-level optimization \eqref{eq::high_level} is chosen as $r(x_t,u_t) =-10 \left(  (p_x-g_x)^2 + (p_y-g_y)^2\right) - ||u||^2 $. The nonlinear MPC problem is also solved to optimize $\sum r(x_t,u_t)$ the same stage-wise cost to find a path that maintains $h_i(x)\geq 0$. The simulation time is $4s$ with time steps of $0.01s$ and the prediction horizon for the proposed method is taken as 20 with an initial learning rate of 0.07.
%The nonlinear MPC is formulated as
% \begin{equation}
%         \begin{aligned}
%             \max_{u_1,...,u_T} \quad & \sum_{t=1}^T  r(x_t,u_t)\\
%             \textrm{s.t.} \quad & x_{t+1} = f(x_t) + g(x_t)u_t \\
%              &  h_i( x_t ) \geq 0\\
%              &  |F_t| \leq F_{max} \\
%              & |M_t| \leq M_{max}\\
%              & \forall t \in \{1,2,..,T\}, i\in \{1,2\}
%         \end{aligned}
%         \label{eq::MPC}
% \end{equation}
and is solved with IPOPT using cyipopt library in Python. Our method always improves the solution resulting from initial choice of parameters. Note that in Fig.\ref{fig::mpc_case2}, Case 1 nominal parameters fail to ensure the existence of a solution to QP and so we relax the CBF constraints by adding a slack variable, similar to \eqref{eq::qp_hoclf_constraint}, with a corresponding large weight in the objective function. The resulting solution thus does not ensure safety but gives minimum violating trajectory. The proposed method, on the other hand, is able to tweak the gains to ensure persistent compatibility.
 \begin{figure}[h]
 \centering
 \begin{subfigure}[b]{0.5\textwidth}
     \centering    
     \includegraphics[width=0.88\linewidth]{figures/si_comparison_new.png}
    \caption{\small{ Single Integrator: Initial parameter: Case 1: $k=1, \nu_i=3$; Case 2: $k=0.5, \nu_i=0.5, i=\{1,2\}$ in \eqref{eq::HOCBF-CLF-QP}} }%
    \label{fig::mpc_case1}%
 \end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
     \centering    
     \includegraphics[width=0.83\linewidth]{figures/bicycle_comparison_new.png}
    \caption{\small{Dynamic Unicycle: Initial parameter: Case 1: $k=1, \nu_i^0=5, \nu_i^1=20$; Case 2: $k=1, \nu_i^0=1, \nu_i^1=3$ in \eqref{eq::HOCBF-CLF-QP}. MPC solution first overshoots but comes back to the goal at (2,2).} }%
    \label{fig::mpc_case2}%
\end{subfigure}
\caption{\small{Comparison of MPC with the proposed method for two different initial parameter values given by blue and green trajectories. Receding horizon(RC) corresponds to gradient descent steps \eqref{eq::gd_update} applied as robots move in the environment. Fixed-tuned trajectories result from performing 20 iterations of \eqref{eq::gd_update} before actually moving the robot in the environment. Goal is at (2,2). Start location is at (-0.5,-0.5).}}
\label{fig::mpc}
\end{figure}
% \vspace{-0.83cm}
\subsection{Predictive Framework for Leader-Follower Problem}

Videos and code available on our website. We consider a robot, called follower, modeled by unicycle kinematics, equipped with a forward-looking sensor (e.g., camera). The follower's objective is to keep another robot, called leader, within its  field-of-view (FoV). The leader robot is modeled by  single-integrator dynamics and is assumed to be moving independently of the follower robot. The FoV constraint specifies a time-varying safe set obtained as the intersection of the following three barrier conditions 
\begin{subequations}
\begin{align}
h_1 &= s^2 - s_{min}^2\geq 0,\label{eq::barrier_colission}\\
h_2 &= s_{max}^2 - s^2 \geq 0,\\ 
h_3 &= b - \cos(\gamma) \geq 0,
\end{align}
\end{subequations}
where $s$ is the distance between the follower and the leader, $s_{min}$ is the minimum allowed distance for collision avoidance, $s_{max}$ is the maximum allowed distance for accurate detection using the onboard camera, $b$ is the bearing vector from the follower to the leader, and $\gamma$ is the FoV angle of the follower's camera. The CBF-CLF-QP \eqref{eq::linear_input_bounds} uses the aforementioned three first-order barrier functions and the Lyapunov function 
\begin{align}
    V(x) = (s - \frac{s_{min}+s_{max}}{2} )^2,
\end{align} 
The linear and angular velocity of the unicycle follower is constrained to be under $2.5$ m/s and $4.0$ rad/s in magnitude. We use linear $\classK$ functions with first-order barriers $h_1,h_2,h_3$ having parameter $\nu_1,\nu_2\nu_3$. The predictive framework from Section \ref{section::predictive_framework} is used to auto-tune the controller gains $\nu_1, \nu_2, \nu_3, k$. The objective is to keep the controller \eqref{eq::linear_input_bounds} feasible for all $t>0$, and to maximize the reward in \eqref{eq::high_level} where $ r(x_t,u_t) = V(x) - 10 h_3$, that is, the reward is maximum when the leader is at the center of field-of-view. The leader is made to move aggressively with a constant horizontal velocity of $\dot p_x = 1$ m/s and sinusoidal vertical velocity of $\dot p_y=12\sin\sin(4\pi t)$ m/s. $s_{min}, s_{max}, \gamma$ are chosen to be $0.3, 2.0, \pi/3$ respectively. 
We simulate for two cases: unbounded and bounded control input and compare the proposed approach with a fixed parameter case, leading to a total of four comparisons. 
The videos and code can be found on our website. The results are shown in Fig.\ref{fig::lf}. We see that unbounded input with online adaptation leads to the lowest cost in Fig. \ref{fig::lf_rewards}. Despite not having input bounds, it demands smaller control inputs compared to fixed parameter case as seen in Fig.\ref{fig::lf_input}. When the control input is bounded, the fixed parameter case leads to infeasible QP and the simulation ends after 0.1 seconds. Our proposed method is able to quickly adapt parameters to maintain input bounds without violating barrier constraints as is evident in Figs.\ref{fig::lf_input} and \ref{fig::lf_barriers}.
% \vspace{-1cm}
 \begin{figure}
    \centering    
    \begin{subfigure}[b]{0.5\textwidth}
    \centering 
        \includegraphics[width=1.0\linewidth]{figures/control_new2.png}
    \caption{\small{Follower's input with time.}}%
    \label{fig::lf_input}%
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering    \includegraphics[width=1.0\linewidth]{figures/rewards_new2.png}
    \caption{\small{Variation of moving horizon reward with time. Adaptation with unbounded input is able to achieve the highest reward as expected.} }%
    \label{fig::lf_rewards}%
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
\includegraphics[width=1.0\linewidth]{figures/barrier_new2.png}
    \caption{\small{Constraint functions $h_1$ (minimum distance), $h_2$ (maximum distance) and $h_3$ (maximum angle w.r.t. camera axis) with time. Constraint maintenance corresponds to $h_i>0, i=1,2,3$.}} 
    \label{fig::lf_barriers}%
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
         \centering
\includegraphics[width=1.0\linewidth]{figures/params_new2_1only.png}
    \caption{\small{Variation in the four parameters for leader-follower problem}} 
    \label{fig::lf_params}%
    \end{subfigure}
    \caption{Leader-Follower Problem}
    \label{fig::lf}
\end{figure}%
%  \begin{figure}[h]
% \end{figure}%
% \begin{figure}[h!]
% \end{figure}
% \begin{figure}[h!]
%     \centering
% \includegraphics[width=1.0\linewidth]{figures/params_new2.png}
%     \caption{\small{Variation in the four parameters for leader-follower problem}} 
%     \label{fig::lf_params}%
% \end{figure}

\subsection{Adversarial Multi-agent System}
Videos and code available on our website.
We consider a decentralized multi-agent system with ego agents, uncooperative agents, and adversarial agents. We provide results for two scenarios as shown in Fig. \ref{fig::multi_agent_scenarios}. The initial locations of ego agents are marked with green circles, uncooperative agents with blue circle, and adversarial agents with red circles. 
For scenario 1, the ego agents are modeled as unicycles and use the first order barrier functions from \cite{wu2016safety}, the adversarial agent is modeled as a single integrator and chases the first ego agent, and the uncooperative agent moves to the left at constant velocity. None of the ego agents know the identities of any other agent in the system, including other ego agents. The starting $\theta_{\alpha_{ij}}$ are all chosen to be 0.8 and the resulting trajectories and variations in barrier function value and $\classK$ function parameters are shown in Figs. \ref{fig::scenario1} and \ref{fig::scenario1_plots}. We also compare our results with fixed parameter case. No solution exists to QP\eqref{eq::HOCBF-CLF-QP} after some time when all $\theta_{alpha_{ij}}$ are initialized to 0.8. Hence, assuming identities are known, we choose $\theta_{\alpha_{ij}}$ 2.0 when $j$ is an ego agent and 0.8 otherwise. The fixed parameter CBF controller fails to lead the robots to their goal locations because of conservative response. The value of parameters in Fig.\ref{fig::scenario1_plots} increases when the trust is positive, i.e., when it is easy to satisfy the CBF constraint and decreases when trust is negative which happens when an agent is moving too fast towards the ego agent or is very close. The beliefs that each ego agent employs for other agent are straight-line motions parallel to $Y$ axis for ego agents and parallel to $X$ axis for other agents.


In scenario 2, shown in Fig. \ref{fig::scenario2}, the ego agents labeled 2 and 3 are modeled as unicycles and use the first-order barrier function from \cite{wu2016safety}, and the agents labeled as 5,6 are modeled as double integrators and use the second-order barrier function in \eqref{eq::barrier_colission}. The adversarial agent chases the unicycle labeled as 2. The uncooperative agent moves to the left at constant velocity but has a visibility cone that must be avoided by ego-agents. We again choose linear $\classK$ functions with $\theta_{\alpha_{ij}}$ initialized to 0.8 for first order barrier function and $\theta_{\alpha_{ij}^0}$ to 0.4 and $\theta_{\alpha_{ij}^1}$ to 0.8 for second order barrier functions.
The response of the fixed parameter case is again to be very conservative compared to the proposed method. 

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[scale=0.4]{figures/path_new.png}
         \caption{\small{Scenario 1}}
         \label{fig::scenario1}
     \end{subfigure}
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[scale=0.35]{figures/compar_paths2.png}
         \caption{\small{Scenario 2}}
         \label{fig::scenario2}
     \end{subfigure}
    \caption{The timestamp is given by the colormap. Paths with bold colors result from the proposed method. Paths with increased transparency result from the application of CBFs with fixed $\classK$ function parameter.}
    \label{fig::multi_agent_scenarios}
    \vspace{-3mm}
\end{figure}

% \begin{figure}[h!]
%     \centering
% %    \includegraphics[scale=0.25]{cdc2022figures/barriers_agent1.png}
%     % \includegraphics[scale=0.20]{figures/new_barriers.png}
%      \includegraphics[scale=0.20]{figures/barriers_scenario1.png}
%     \caption{\small{Scenario 1: Variation of barrier functions of Robot 1 with time. Trust-based relaxation allows agents to go closer to the safe set boundary ($h_{ij}=0$) compared to fixed $\alpha$ case, and hence leads to a less conservative response while still guaranteeing safety. %More plots can be The barriers with uncooperative agents are not shown but can be found along with our videos. 
%     }}
%     \label{fig::scenario1-barriers}
%     \vspace{-2mm}
% \end{figure}

\begin{figure}[h!]
    \centering
%    \includegraphics[scale=0.25]{cdc2022figures/barriers_agent1.png}
    % \includegraphics[scale=0.20]{figures/new_barriers.png}
     \includegraphics[scale=0.36]{figures/combined_barrier_alpha_new.png}
    \caption{\small{Scenario 1: Variation of $\classK$ function parameters(left) and barrier functions (right) of Robot 1 with time. UC1, UC2 are the uncooperative agents starting from left and right respectively in Fig. \ref{fig::scenario1}. Trust-based relaxation allows agents to go closer to the safe set boundary ($h_{ij}=0$) compared to fixed $\alpha$ case, and hence leads to a less conservative response while still guaranteeing safety. %More plots can be The barriers with uncooperative agents are not shown but can be found along with our videos. 
    }}
    \label{fig::scenario1_plots}
    \vspace{-2mm}
\end{figure}

% \begin{figure}[h!]
%     \centering
%     % \includegraphics[scale=0.4]{cdc2022figures/alphas_agent1.png}
%     % \includegraphics[scale=0.35]{figures/1alphas_new.png}
%     \includegraphics[scale=0.35]{figures/alphas_scenario1.png}
%     \caption{\small{Scenario 1: Variation of the parameters $\alpha_{1j}$ of Robot 1 with respect to Robot $j$=2,3 and the adversarial agent. Trust-based adaptation allows robot 1's $\theta$ to increase, thus relaxing the constraints.}}
%     \label{fig::scenario1-alphas}
%     \vspace{-1mm}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[scale=0.35]{figures/compar_paths2.png}
%     \caption{\small{Scenario 2:. the timestamp is given by the colormap. Paths with bold colors result from the proposed method. Paths with increased transparency result from the application of CBFs with fixed $\classK$ function parameter.}}
%     \label{fig::scenario2}
%     \vspace{-5mm}
% \end{figure}

\section{Conclusion}
\label{section::conclusion}
We introduced a new notion of Rate Tunable CBFs that allows for online tuning of CBF-based controllers by adapting the $\classK$ function parameters. We also introduce the notion of persistent compatibility that is required when the system is subject to multiple barrier constraints. Keeping barrier functions separate rather than combining them into a single barrier function allowed us to shape responses to each of them separately. Finally, we proposed a predictive framework and a trust-based framework to design update laws for $\classK$ function parameter. Future work includes safety analysis when the control input is not guaranteed to be lipschitz continuous which is common when multiple constraints are present and a thorough analysis of forward completeness with trust-based design in presence of input bounds and online estimation of beliefs of how other agents are moving.

% \section{Appendix I}

% \begin{claim}
%       $B_r(0)\subset \Omega_{t,c}$
% \end{claim}
% \begin{proof}
%     Note that there are 4 possibilities
%     \begin{enumerate}
%         \item $B_r(0)\cap \Omega_{t,c}=\emptyset$ ($B_r$ and $\Omega_{t,c}$ are disjoint) OR  \\ $\Omega_{t,c}\subset B_r(0)$ ( $\Omega_{t,c}$ completely inside $B_r(0)$ ):\\
%         both of them are not possible as atleast one point $x'$ on boundary of $B_r(0)$ that has $V(t,x')=\beta$ is inside $\Omega_{t,c}$ by construction.
%         \item $\Omega_{t,c}\cap  B_r(0) \neq \emptyset $ and $ \Omega_{t,c}\not\subset   B_r(0) $ and $\Omega_{t,c}\not \subset   B_r(0)$ (they have common regions but neither is completely inside the other): \\
%         in this case $\exists x''$ such that $||x''||=R$ and $x''\not\in \Omega_{t,c}$ that is $V(t,x'')>c > \beta$. But $V(t,x'')\leq \beta$ and therefore this is a contradiction. Therefore, this case is not possible.
%         \item $\Omega_{t,c}\subset B_r(0)$: since other cases are not possible, this is the only possibility.
%     \end{enumerate}
% \end{proof}


% \section*{Acknowledgement}




\bibliographystyle{IEEEtran}
\bibliography{journal1.bib}


\end{document}