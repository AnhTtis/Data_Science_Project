 %% Short data paper template
%% Created by Simon Hengchen and Nilo Pedrazzini for the Journal of Open Humanities Data (https://openhumanitiesdata.metajnl.com)

\documentclass{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{style}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow}
\usepackage{algorithm} 
\usepackage{algpseudocode} 
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepgfplotslibrary{fillbetween}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\red}[1]{\textcolor{red}{#1}}

\newlength\figureheight
\newlength\figurewidth

\usepackage{bm}

\usepackage{amsthm,amsmath}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\title{Black-box Backdoor Defense via Zero-shot
Image Purification}

\author{Yucheng Shi,$^{\dagger}$ Mengnan Du,$^{\ddagger}$ Xuansheng Wu,$^{\dagger}$ Zihan Guan,$^{\dagger}$ Ninghao Liu$^{\dagger}$ \\
        \small $^{\dagger}$School of Computing, University of Georgia, Athens, GA \\
        \small $^{\ddagger}$Department of Data Science, New Jersey Institute of Technology, Newark, NJ \\
        \small \{yucheng.shi, xuansheng.wu, zihan.guan, ninghao.liu\}@uga.edu, mengnan.du@njit.edu
        %\small $^{*}$Corresponding author: \tt{ninghao.liu.uga.edu} \\
}
\date{}

% \author{Yucheng Shi$^{a}$, Mengnan Du$^{b}$, Zihan Guan$^{a}$, Ninghao Liu$^{a}$$^{*}$ \\
%         \small $^{a}$School of Computing, University of Georgia, Athens, GA \\
%         \small $^{b}$Department of Data Science, New Jersey Institute of Technology, Newark, NJ \\\\
%         \small $^{*}$Corresponding author: \tt{ninghao.liu.uga.edu} \\
% }
% \date{}

\begin{document}
\maketitle
\begin{abstract}
Backdoor attacks inject poisoned data into the training set, resulting in misclassification of the poisoned samples during model inference. Defending against such attacks is challenging, especially in real-world black-box settings where only model predictions are available. In this paper, we propose a novel backdoor defense framework that can effectively defend against various attacks through zero-shot image purification (ZIP). Our proposed framework can be applied to black-box models without requiring any internal information about the poisoned model or any prior knowledge of the clean/poisoned samples. Our defense framework involves a two-step process. First, we apply a linear transformation on the poisoned image to destroy the trigger pattern. Then, we use a pre-trained diffusion model to recover the missing semantic information removed by the transformation. In particular, we design a new reverse process using the transformed image to guide the generation of high-fidelity purified images, which can be applied in zero-shot settings. We evaluate our ZIP backdoor defense framework on multiple datasets with different kinds of attacks. Experimental results demonstrate the superiority of our ZIP framework compared to state-of-the-art backdoor defense baselines. We believe that our results will provide valuable insights for future defense methods for black-box models.
%Backdoor attacks inject poisoned data into the training set, causing the model to misclassify poisoned images during inference.  Defending such attacks requires removing the poisoning effect from the model. However, this is challenging, especially for the black-box model, where we can only obtain the model predictions. To defend against attacks on black-box models, we present a novel model-agnostic backdoor defense framework, which does not require any knowledge from the black-box models. In our framework, we defend against various attacks during the model inference stage by purifying the testing poisoned image prior to making predictions, which we conduct in a zero-shot learning manner. Specifically, we utilize the power of a pre-trained off-the-shelf diffusion model as the generative prior. The image purification based defense approach can remove the trigger pattern from the image, allowing the model to make a more accurate prediction. The results show that the proposed method is effective in defending against various types of backdoor attacks with high accuracy.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
%Talk about the background and motivation: 1) why your project topic is exciting? 2) How other people solving this problem now, and what is missing from their approach? 3) How are you going to solve the problem? 4) What will the result look like?

%what are the black box models and why they are important/popular(with today's big model(large pre-trained model)), however, the current defense methods are naive, 1. causing the CA to drop, 2. Can not handle the invisible (not static or local) attacks. Therefore, we aim to tackle such a challenge:
%Backdoor attacks intend to maliciously change the predictions of users' deep neural networks (DNNs) by attacking the input images with specified triggers. When the DNN is trained on the poisoned dataset, it will learn a mapping from the trigger pattern to the target label. These triggers here can be static patterns or dynamic patterns learned by adversarial training. The pair of poisoned images and the wrongly-labeled labels together mislead the DNN, thus attacking the model. To defend against the backdoor attacks, previous defense methods usually require (1) enough clean or poisoned samples as the training set, and (2) the poisoned network for their methods to successfully defend against the attack. However, such prerequisites limit the backdoor defense's application. Specifically, existing methods will fail if there are no available labeled clean or poisoned samples for the defense model to train. And these methods will fail and require re-training if downstream networks are changed.


As machine learning becomes increasingly integrated into high-stakes applications such as healthcare~\cite{obermeyer2016predicting}, finance~\cite{kashyap2017machine}, and autonomous systems~\cite{zhang2017current}, ensuring the security and reliability of these models has become more critical, which includes defending against various backdoor attacks~\cite{gu2017badnets,chen2017targeted, nguyen2021wanet}. The backdoor attack is a type of attack on machine learning models in which an attacker can manipulate the model behavior by poisoning training data with malicious samples that contain a trigger pattern and a target label, or altering the weights of the model~\cite{li2022backdoor}. Although several defense strategies have been proposed in recent years to mitigate the effects of backdoor attacks, many of them require access to the model's internal structure and poisoned training data~\cite{li2022backdoor}. This is not always feasible in real-world black-box settings, as users are unable to check or audit the inner workings of the model. For example, end-users may prefer to use pre-trained models provided by third-party vendors, such as those available on AWS SageMaker~\cite{amazon-sagemaker} or Microsoft Azure Machine Learning~\cite{azure-machine-learning}, to save on computation costs. However, they also lose control over the model training or deploying processes. In such cases, backdoor attacks can be particularly harmful and hard to detect because the end-user can only access prediction results. This lack of transparency poses a significant challenge for detecting and defending against black-box backdoor attacks.

%For instance, nowadays, instead of training their model, many end-users prefer to use models provided by third-party vendors~\cite{amazon-sagemaker,azure-machine-learning}. And these models have the potential to be hijacked by the attacker. In this case, backdoor attacks can be particularly harmful and hard to detect because the end-user can only access prediction results. This lack of transparency poses a significant challenge for detecting and defending against black-box backdoor attacks.

Existing defense methods for black-box backdoor models fall into two categories. The first type trains a detection model with clean/poisoned images as inputs~\cite{zeng2021rethinking, guo2021aeva, li2021anti, dong2021black}. However, this method has limitations such as being dependent on the quality and quantity of collected data, and not being able to use the detected poisoned images for further analysis ~\cite{guo2021aeva, zeng2021rethinking, dong2021black}. The second type applies strong image transformations during the inference stage before feeding poisoned images to the poisoned model~\cite{li2021backdoor, zeng2020deepsweep}. This kind of methods can defend against static pattern-based attacks, but are not effective against advanced attacks with advanced patterns~\cite{li2021backdoor}. Both methods have limitations and are not effective in real-world applications. To overcome the limitations of existing black-box backdoor defense methods, we aim to propose a novel defense framework that does not require (1) any internal information of the classification model or (2) any prior knowledge of the clean/poisoned samples. And our proposed method can help benign end-users utilize those poisoned samples that are maliciously attacked by attackers, rather than discarding them. However, we face two main challenges in designing such a framework. 

(1) \textbf{Trade-off between image transformation and fidelity}. Strong transformation is needed to create a mismatch between advanced trigger patterns and poisoned labels. But it can also remove important semantic information from the images. Since the transformation-based defense method applies the transformation to the entire dataset, it can cause the classification accuracy (CA) to drop on the clean images. To avoid such drop, we need to maintain a high fidelity of the transformed image while we destroy the trigger pattern. One potential solution to solve this is to recover the missing information removed by strong transformation. However, how to recover the semantic information instead of the trigger pattern from transformed images remains a challenge. 

(2) \textbf{Defending against attacks in a zero-shot setting}. In our proposed framework, we define "zero-shot" as the ability to defend against various attacks without relying on any prior knowledge of the clean or poisoned image samples. In other words, our approach does not require access to any clean or poisoned image samples and can be applied directly to unseen attack scenarios. This setting is crucial because real-world users have limited information, while new threats always emerge. Collecting the most recent attacked images for the detection models' training samples may not be feasible. Therefore, effectively defending against poisoned images without any reference samples poses a significant challenge. 


%In this paper, we propose a generative model-based purification framework to remove attack effects from poisoned test images in the zero-shot setting. 
To address the above challenges, we propose a novel backdoor defense framework that can effectively defend against various attacks through Zero-shot Image Purification (ZIP). 
%by implementing a purification operation on the poisoned image. 
The purification in our ZIP framework aims to preserve the original image's semantic information while minimizing the presence of the trigger pattern. To achieve this goal, we first utilize image transformation techniques to destruct the trigger pattern. We then leverage an off-the-shelf, pre-trained diffusion generative model to restore the transformed semantic information. Our defense strategy is based on the motivation that the semantic information in a poisoned image (e.g., human faces, cars) constitutes the majority of the image and typically falls within the pre-training data distribution. Conversely, the trigger pattern (e.g., small white box, colorful spirals) is inconspicuous and not included in the pre-training datasets. Since the diffusion model can only sample images from the training dataset distribution~\cite{ho2020denoising}, purified images generated from the diffusion model can only retain their semantic information instead of the trigger pattern. As a result, our purification approach can effectively defend against various attacks while maintaining high-fidelity in the restored images. Our main contributions are summarized as follows.
\begin{itemize}
    \item We develop a novel defense framework that can be applied to black-box models without requiring any internal information about the model. Our defense system is also model-agnostic, making it versatile and easy to use with various models without retraining.
    \item Our proposed framework is designed to function in a zero-shot setting, meaning it can operate without requiring any prior knowledge 
    of the clean or poisoned images. This feature relieves end-users from the need to collect samples, which enhances the framework's application ability and usability.
    \item Our defense framework is capable of achieving decent classification accuracy on the purified images that are originally poisoned samples, even with an attack model as the classifier. This improvement further enhances the framework's effectiveness and practicality.
\end{itemize}


%1. How to effectively defense the invisible attack? 2. How to improve the CA decrease (information loss in the data augmentation, how to make up the lost information? 3. Training an image generative model is costly, can we do not train a model?


%(1) \textbf{Trigger broken} In this paper, we apply the data augmentation on the poisoned images to break the pattern of poison triggers. And the selection of data augmentation can significantly have an impact on the data. Which kind of data augmentation is the most effective?
%(2) \textbf{Recovering the deleted information from the augmented images} The restored images of poisoned images should maintain the original semantic information. After the restoration with a pre-trained diffusion model, how can we make sure the generated model maintains the same semantic information? 
%In detail, there are two scenarios. The first kind is that the data distribution and pre-trained data align with the downstream tasks dataset. The second is 
%(3) \textbf{Zero-shot setting} The discussed above situation is that the data distribution of pre-trained datasets aligns with the downstream tasks dataset. However, if the pre-training data distribution is different from the downstream tasks datasets. How can we make the pre-trained model quickly adapt to the new data distribution?

\section{Preliminaries}

\subsection{Diffusion Model}


The denoising diffusion probabilistic model (DDPM~\cite{ho2020denoising}) is a generative model that has recently gained attention due to its ability to generate high-quality images. The original DDPM includes two processes: the forward process and the reverse process. In the forward process, the model iteratively adds noise to the input image until it transforms to random Gaussian noise $\mathbf{x}_T$, then in the reverse process, the model iteratively removes the added noise from the Gaussian noise image $\mathbf{x}_T$ to generate a noise-free image $\mathbf{x}_0$. The generated image $\mathbf{x}_0$ fits the distribution of the input images in the forward process. The model details are as follows: 


\vspace{4pt}
\noindent\textbf{Forward Process:}
In the forward process, a noise-free image $\mathbf{x}_0$ is transformed to a noisy image $\mathbf{x}_t$ with controlled noise. Specifically, Gaussian noise $\boldsymbol{\epsilon}$ is gradually added to image $\mathbf{x}_0$ in $T$ steps based on the variance schedule $\beta_t$:
\begin{equation}
    q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right):=\mathcal{N}\left(\mathbf{x}_{t} ; \sqrt{1-\beta_{t}} \mathbf{x}_{t-1}, \beta_{t} \mathbf{I}\right),
\end{equation}
A nice property of the above process is that we can sample noised image at step $t$ using reparameterization trick: 
\begin{equation}
    \label{forward}
    q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)=\mathcal{N}\left(\mathbf{x}_{t} ; \sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0},\left(1-\bar{\alpha}_{t}\right) \mathbf{I}\right), \,\,\, \mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I}), 
\end{equation}
where we have $\alpha_{t}=1-\beta_{t} \text { and } \bar{\alpha}_{t}=\prod_{i=1}^{t} \alpha_{i}$.

\vspace{4pt}
\noindent\textbf{Reverse Process:}
\label{revserse}
In the reverse process, the noisy input image $\mathbf{x}_T$ obtained from the forward stage is transformed into a noise-free output image $\mathbf{x}_0$ step by step. In each step, the diffusion model takes in the current image state $\mathbf{x}_t$ and produces a previous state $\mathbf{x}_{t-1}$. We aim to obtain clean images $\mathbf{x}_0$ by iteratively sampling $\mathbf{x}_{t-1}$ from $p(\mathbf{x}_{t-1}|x_t, x_0)$:

\begin{equation}
    \label{reverse}
    \mathbf{x}_{t-1}=\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \mathbf{x}_{0}+\frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_{t}} \mathbf{x}_{t}+\sigma_{t} \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I}), \quad \sigma_{t}^{2}=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}} \beta_{t},
\end{equation}
Based on Equation~\ref{forward}, we can estimate a noisy input image $\mathbf{x}_{0|t}$ based on the step $t$ observation $\mathbf{x}_t$:
\begin{equation}
\label{x0t}
    \mathbf{x}_{0|t}=\frac{1}{\sqrt{\bar{\alpha}_{t}}} (\mathbf{x}_t - \sqrt{1-\bar{\alpha}_{t}}\boldsymbol{\epsilon}_t),
\end{equation}
where $\boldsymbol{\epsilon_t}$ denotes the estimation of the real $\boldsymbol{\epsilon}$ in time step $t$. In each step $t$, DDPM utilizes a neural network $g_\phi(\cdot)$ to predict the noise $\boldsymbol{\epsilon_t}$, i.e., $\boldsymbol{\epsilon_t} = g_\phi(\mathbf{x}_t, t)$.
With this estimation, we can transform Equation~\ref{reverse} into the following form:
\begin{equation}
    \label{xt-1}
    \mathbf{x}_{t-1}=\frac{1}{\sqrt{\alpha}_{t}} (\mathbf{x}_t - \frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\boldsymbol{\epsilon_t})+\sigma_{t} \boldsymbol{\epsilon}.
\end{equation}



%\subsection{Range-null space decomposition}



%Previous research has demonstrated that the intactness of trigger patterns is the key for the backdoor attack to mislead the model while corrupting the trigger pattern will significantly compromise the attack effect~\cite{li2021backdoor}. Inspired by such observations, we propose to conduct the backdoor defense by eliminating the trigger pattern in the poisoned images. 


%\subsection{Image Purification for Backdoor Defense}

\subsection{Notation and Problem Definition}

This paper addresses the backdoor defense problem in the context of image classification. The goal of image classification is to learn a function $f_{\theta}(\mathbf{x})$ that maps input images $\mathbf{x} \in \mathcal{X}$ to their correct labels $y \in\mathcal{Y} $, where $\mathcal{X}$ denotes the set of images, $\mathcal{Y}$ denotes their corresponding labels, and $\theta$ represents the parameters of the function $f_{\theta}$. When this learning process is under attack, an attacker can alter the classification result by modifying a small portion of the training dataset $\mathcal{X}^{attack}$ to insert a "backdoor" into the model. This backdoor is usually a specific pattern $\mathbf{p}$ (or a set of patterns), such as mosaic pixels arranged on a 2$\times$2 grid. When this pattern is present in an input image, it forces the model to generate a predetermined output, regardless of the true label of the image.

The backdoor attack can be formalized as follows: given an original clean image $\mathbf{x} \in \mathcal{X} $ and a trigger pattern $\mathbf{p} $, simply adding the pattern $\mathbf{p}$ to $\mathbf{x}$ can produce the poisoned image $\mathbf{x}^P =\mathbf{x}+\mathbf{p}$. During the training process, the attackers first obtain poisoned image $\mathbf{x}^P$ to form a subset of the training images $\mathcal{X}^{attack}$. After training on the  $\mathcal{X}^{attack}$, the attacked classification function is denoted as $f_{\theta}^{attack}(\mathbf{x})$. During testing, an attacker can apply the same trigger pattern $\mathbf{p}$ to a clean test image $\mathbf{x}$ to create a poisoned image $\mathbf{x}^P=\mathbf{x}+\mathbf{p}$, which will be classified by the backdoored model as the target label $y^{target} = f_{\theta}^{attack}(\mathbf{x}^P) \ne y$. To defend against the above attack, we formally define our defense problem as follows.

\newtheorem{problem}{Problem}
\begin{problem}
\textbf{Image Purification for Backdoor Defense.}
Our defense will be implemented in the model inference stage.
Let $f_\theta^{attack}$ denote the attacked neural network that has been trained on poisoned dataset $\mathcal{X}^{attack}$. In our study, we consider the challenging black-box setting in real-world scenarios, where we lack access to the parameters of the $f_\theta^{attack}$ model.
Given a poisoned image $\mathbf{x}^P$, our goal is to remove the effect of trigger pattern $\mathbf{p}$ from $\mathbf{x}^P$ to obtain a purified image $\mathbf{x}'$. The purified image should be classified as the same type as the original clean image $\mathbf{x}$, i.e., $f_\theta(\mathbf{x}') = f_\theta(\mathbf{x})\neq f_\theta(\mathbf{x}^P)$. For the purified image $\mathbf{x}'$, ideally we have $\mathbf{x}'=\mathbf{x}$.
\end{problem}

Specifically, we propose to conduct the backdoor defense by eliminating trigger patterns in poisoned images through image purification during the model inference stage. This is inspired by this observation that the integrity of trigger patterns is the key for the backdoor attack to mislead the model, while corrupting the trigger pattern will significantly compromise the effectiveness of the attack~\cite{li2021backdoor}. 


%\subsection{Constrained Diffusion for Image Purification}
\section{Proposed Defense Framework}
\label{sec:method}

\begin{figure}[tp]
    \centering
    \includegraphics[width=0.9\textwidth]{figure/Figure1v3.pdf}
    \caption{The framework of our \textbf{ZIP} backdoor defense. In Stage 1, we use a linear transformation to destruct the trigger pattern in poisoned image $\mathbf{x}^P$. In Stage 2, we make use of a pre-trained diffusion model to generate a purified image. From time step $T$ to $T'$: starting from the Gaussian noise image $\mathbf{x}^T$, we use the transformed image $\mathbf{A}^{\dagger}\mathbf{x}^A$ obtained in Stage 1 to guide the generation of the purified image. From time step $T'$ to 0: we discard $\mathbf{A}^{\dagger}\mathbf{x}^A$ and exclusively use the diffusion model to generate $\mathbf{x}'$. }
    \label{fig1}
\end{figure}

In this section, we present our proposed backdoor defense \textbf{ZIP} (Zero-shot Image Purification) which is performed in the zero-shot setting where we lack any prior knowledge of the trigger pattern. Specifically, we utilize the power of an off-the-shelf pre-trained diffusion model to generate high-quality purified images. 
%First, we utilize a strong image transformation to destroy the trigger pattern. However, this operation will also destruct the semantic information and diminish the fidelity, decreasing the classification accuracy. To tackle this problem, we propose utilizing a diffusion model to generate high-quality purified images, and introduce a novel reverse process that is conditional on poisoned images to guide the generation of the purified images.  %Finally, we introduce a technique to speed up the inference process.

\subsection{Overview of Proposed Framework}
Our proposed defense framework is based on the reverse process of the diffusion model, as illustrated in Figure~\ref{fig1}. To reduce computation and eliminate the need for sample training, we leverage an off-the-shelf pre-trained diffusion model $g_\phi$~\cite{dhariwal2021diffusion}. Our objective is to generate a purified image for a specific image while destroying any trigger patterns. While a vanilla reverse process can generate a noise-free image that fits the distribution of the pre-trained model, the resulting image content cannot be controlled, thus failing to satisfy our high-fidelity requirements. Directly incorporating poisoned images into the reverse process to guide the generation process can produce high-fidelity images but may retain the trigger pattern, thus failing to achieve our purification objectives. 

To address these challenges, we first perform a robust image transformation in the poisoned input image to destroy the trigger pattern. However, this operation will also destruct the semantic information and diminish the fidelity, decreasing the classification accuracy. To tackle this problem, we propose utilizing a diffusion model to generate high-quality purified images, and introduce a novel reverse process that is conditional on poisoned images to guide the generation of the purified images. 
%Next, we use the transformed image as a reference to guide the generation process. 
Specifically, we apply the range-null decomposition~\cite{schwab2019deep} to extract the transformed image's deeper relation to the original image, and use this relation as constraint to ensure the generation of high-fidelity purified images. As the trigger pattern is destroyed in the transformed image, it will not be present in the purified image. Moreover, to further mitigate the trigger effect, our framework can switch to the vanilla reverse process in the final stages, where the semantic information in the image is adequate to guide the generation process. In contrast, since the trigger pattern is usually not included in the distribution of pre-trained model, it will not appear in the purified image.


%Given an original clean image $\mathbf{x} \in \mathcal{X} $, a trigger pattern $\mathbf{p} \in $, the poisoned image $\mathbf{x}^P =\mathbf{x}+\mathbf{p}$.

%Zero-shot Image Restoration refers to the task of restoring an image without having seen similar examples during the training phase of the model. In other words, the model is able to restore the image by only relying on its general understanding of the image content, without having seen any similar examples during the training phase. 

\subsection{Image Transformation Constraints}

%2. 因为有这个公式所以我们知道了可以这样子分解
%3. 这个分解公式说明了两个问题，第一个是data augmentation会从原始图片中去掉很多很多信息, 首先要做一个反操作，然后还有被移除的信息 直接用augmented的图片作为输入不合理. 第二点是这个公式提供了一个有效的constrain， 可以让我们在diffusion model 的reserve过程中使用

As the first step, we apply image transformation on the poisoned image to destruct the trigger pattern, e.g., by using average pooling to blur poisoned images. Formally, we denote the applied image transformation as a liner operator function $\mathbf{A}$, and let the transformed image be $\mathbf{x}^A = \mathbf{A}\mathbf{x}^P =\mathbf{A}(\mathbf{x}+\mathbf{p})$. However, directly using the transformed image as the purification result could lead to poor classification accuracy due to fidelity loss induced by $\mathbf{A}$, which is supported by several previous studies~\cite{qiu2021deepsweep}. Recovering the original image $\mathbf{x}$ from the transformed image $\mathbf{x}^A$ is a linear inverse problem that is difficult to solve~\cite{kawar2022denoising}.

To recover the lost information, an intuitive way is to apply an image generative model, e.g., the diffusion model, to yield a purified image with high fidelity. However, images are generated from random Gaussian noise in the diffusion model, which lacks control over the result and cannot be directly applied to solve our problem. Thus, we derive a constraint for the generation process of diffusion models to precisely recover the original image. Specifically, for an ideally purified image $\mathbf{x}'$, it should satisfy $\mathbf{x}'=\mathbf{x}$, so we establish the constraint as follows:
\begin{equation}
    \label{constr}
      \mathbf{A}(\mathbf{x}'+\mathbf{p}) = \mathbf{A}(\mathbf{x}+\mathbf{p}) = \mathbf{x}^A .
\end{equation}
%where we can observe that the $\mathbf{x}^A \neq \mathbf{x}$ when given non-zero linear augmentation $\mathbf{A}$ and non-zero pattern $\mathbf{p}$. Therefore naively using $\mathbf{x}^A$ as purified images will damage the fidelity.
%To quantify the difference between the ideally purified image and the augmented image, 
% However, the above constraint indicates the potential to extract the original image (the ideal purified image) $\mathbf{x}$ from the augmented image $\mathbf{x}^P$, and this process can be formalized as linear inverse problems. To tackle such problems, 
Then, we leverage the power of range-null space decomposition (RND)~\cite{wang2022zero, schwab2019deep} to extract additional relations between $\mathbf{x}$ and $\mathbf{x}^A$. 
According to the RND theory, it is possible to decompose an image $\mathbf{x}$ into two parts using a linear operator $\mathbf{A}$ (e.g., average pooling) and its pseudo-inverse $\mathbf{A}^{\dagger}$ (e.g., upsampling) that satisfies $\mathbf{A}\mathbf{A}^{\dagger}\mathbf{A}=\mathbf{A}$. The decomposition can be expressed as $\mathbf{x} = \mathbf{A}^{\dagger} \mathbf{A} \mathbf{x}+\left(\mathbf{I}-\mathbf{A}^{\dagger} \mathbf{A}\right)\mathbf{x}$, where the first part represents the observable information in the range-space, and the second part represents the lost information in the null-space removed by transformation\footnote{Usually, we have $\mathbf{A}^{\dagger} \mathbf{A} \neq \mathbf{I}$, which implies that this operation is lossy. When applying $\mathbf{A}^{\dagger} \mathbf{A}$ to an image, certain information will be removed, making this operation irreversible.}. Applying this decomposition to Equation~\ref{constr}, we could have:
\begin{equation}
    \label{RND0}
     (\mathbf{x}' +\mathbf{p}) = \mathbf{A}^{\dagger} \mathbf{A} (\mathbf{x}' +\mathbf{p})+\left(\mathbf{I}-\mathbf{A}^{\dagger} \mathbf{A}\right) (\mathbf{x}' +\mathbf{p}).
\end{equation}
Using this equation, we can derive a constraint as follows for image purification to restore the original $\mathbf{x}$.% The detailed derivation is provided in the appendix. 
\begin{equation}
     \mathbf{x}' = \mathbf{A}^{\dagger}\mathbf{x}^A - \mathbf{A}^{\dagger} \mathbf{A}\mathbf{p} +\left(\mathbf{I}-\mathbf{A}^{\dagger} \mathbf{A}\right) \mathbf{x}',\label{RND}
\end{equation}
where we can observe that the ideally purified images are composed of three parts. The first two parts are in the range space: the observable information stored in the transformed image $\mathbf{A}^{\dagger}\mathbf{x}^A$, as well as the intractable information embedded in the transformed trigger pattern $\mathbf{A}^{\dagger}\mathbf{A}\mathbf{p}$; and the last part is unobservable in the null-space since it is removed by image transformation. To restore the lost information in the null-space, we utilize the observable information in the range-space as references. This approach will be explained in more detail in Section~\ref{3.3.1}. However, it is important to note that the second component mentioned earlier is often difficult to estimate in our zero-shot setting. To address this challenge, we propose an approximation approach that will be introduced in Section~\ref{3.3.2}.




\subsection{Reverse Process Conditional on Poisoned Images}
\label{3.3.1}
The reverse process of the diffusion model takes Gaussian noise $\mathbf{x}_T$ as input (see Figure~\ref{fig1}), and can generate a noise-free image $\mathbf{x}_0$ by iteratively removing noise from the input. However, the vanilla reverse process in Section~\ref{revserse} generates uncontrollable random images without any further constraints, which does not satisfy the requirement of fidelity. To address this issue, we utilize the image transformation constraint discussed in Equation~\ref{RND} to encourage the generated image $\mathbf{x}_0$ to closely match the original image $\mathbf{x}$. In our setting, we consider the generated noise-free image $\mathbf{x}_0$ as the purified image $\mathbf{x}'$, where $\mathbf{x}_0 = \mathbf{x}'$. We also use $\mathbf{x}_t$ to denote the image at time step $t$ in the reverse process of the diffusion model. Specifically, we design a new reverse process conditional on the poisoned image. By combining Equation~\ref{x0t} and Equation~\ref{RND}, we can derive the constraints for the intermediate state $\mathbf{x}_t$ as follows:
\begin{equation}
        \mathbf{x}_t = \sqrt{\bar{\alpha}_{t}}\mathbf{A}^{\dagger}\mathbf{x}^A - \sqrt{\bar{\alpha}_{t}}\mathbf{A}^{\dagger}\mathbf{A}\mathbf{p} +(\mathbf{I}-\mathbf{A}^{\dagger} \mathbf{A}) \mathbf{x}_t + \mathbf{A}^{\dagger}\mathbf{A}\sqrt{1-\bar{\alpha}_{t}}{\boldsymbol{\epsilon_t}},
\end{equation}
where $\boldsymbol{\epsilon_t}$ denotes the estimated noise, which is calculated using the pre-trained diffusion model $g_\phi$: $\boldsymbol{\epsilon_t} = g_\phi(\mathbf{x}_t, t)$. The above constraints indicate that any ideally purified images at each time step $t$ should comply with this constraint.

Next, we modify the original reverse process in Equation~\ref{xt-1} to accommodate this constraint. The modified reverse process can be transformed as:
\begin{equation}
    \label{strict_setting}
    \mathbf{x}_{t-1}= \frac{1}{\sqrt{\alpha}_{t}} (\sqrt{\bar{\alpha}_{t}}\mathbf{A}^{\dagger}\mathbf{x}^A - \sqrt{\bar{\alpha}_{t}}\mathbf{A}^{\dagger}\mathbf{A}\mathbf{p} +(\mathbf{I}-\mathbf{A}^{\dagger} \mathbf{A}) \mathbf{x}_t + \mathbf{A}^{\dagger}\mathbf{A}\sqrt{1-\bar{\alpha}_{t}}\boldsymbol{\epsilon_t} - \frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\boldsymbol{\epsilon_t})+\sigma_{t} \boldsymbol{\epsilon},
\end{equation}
It is worth noting that the above reverse process will be applied to all test images, regardless of whether they are attacked or not. The generated image after this reverse process can preserve the semantic information in the clean image and helps maintain a high classification accuracy.

\subsection{Approximation of $\mathbf{x}_t$ in the Zero-shot Setting}
\label{3.3.2}

In this section, we present an approximation of Equation~\ref{strict_setting} to address the practical challenges of applying it directly in the zero-shot and black-box settings. In these scenarios, the trigger pattern used to poison the image is unknown and designed to be subtle to evade detection by end-users without access to the training data or model internal information. Therefore, to address this issue, we propose to omit the trigger pattern from the decomposition equation when its contribution is negligible compared to other components. Our proposed approximated form is presented below:
\begin{equation}
        \hat{\mathbf{x}}_t = \sqrt{\bar{\alpha}_{t}}\mathbf{A}^{\dagger}\mathbf{x}^A +(\mathbf{I}-\mathbf{A}^{\dagger} \mathbf{A}) \mathbf{x}_{t} + \mathbf{A}^{\dagger}\mathbf{A}\sqrt{1-\bar{\alpha}_{t}}\boldsymbol{\epsilon_t},
\end{equation}
where $\mathbf{x}_{t}$ is the noisy image at time step $t$, which is transformed from the image $\mathbf{x}_{t+1}$ at time step $t+1$. While $\hat{\mathbf{x}}_t$ denotes the approximated image of $\mathbf{x}_t$ based on the range-null decomposition, and we can have $\hat{\mathbf{x}}_t-\mathbf{x}_t  = \sqrt{\bar{\alpha}_{t}}\mathbf{A}^{\dagger}\mathbf{A}\mathbf{p}$. The above approximation of $\mathbf{x}_t$ is based on several known factors: 
\begin{itemize}
    \item Since the reverse process begins from $T$, $t$ is very large at the beginning, and the value of $\sqrt{\bar{\alpha}_{t}}$ will be very small, making $\sqrt{\bar{\alpha}_{t}}\mathbf{A}^{\dagger}\mathbf{A}\mathbf{p}$ negligible compared to other terms like $(\mathbf{I}-\mathbf{A}^{\dagger} \mathbf{A}) \hat{\mathbf{x}}_t$, or $\mathbf{A}^{\dagger}\mathbf{A}\sqrt{1-\bar{\alpha}_{t}}\boldsymbol{\epsilon_t}$.
    \item Backdoor attacks are generally stealthy and have much smaller patterns compared to the original images since the attacker wants to minimize the impact on the model's accuracy on legitimate data while still being able to trigger the backdoor~\cite{nguyen2021wanet}. Thus, compared to $\mathbf{x}^A$, $\mathbf{Ap}$ can be ignored.
    \item $\mathbf{A}^{\dagger}\mathbf{A}\mathbf{p}$ can be further reduced by selecting appropriate image transformation techniques. Since most backdoor attacks are characterized by severe high-frequency artifacts~\cite{zeng2021rethinking}, we select average pooling as our transformation method. Average pooling can effectively blur the input images, resulting in the remove of high-frequency information. 
\end{itemize}

Overall, we can have $\hat{\mathbf{x}}_t  \approx  \mathbf{x}_t$. Therefore, we can modify Equation~\ref{strict_setting} into the following form:
\begin{equation}
    \label{zero-shot}
    \mathbf{x}_{t-1}= \frac{1}{\sqrt{\alpha}_{t}} (\hat{\mathbf{x}}_t - \frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\boldsymbol{\epsilon_t})+\sigma_{t} \boldsymbol{\epsilon} = \frac{1}{\sqrt{\alpha}_{t}} (\sqrt{\bar{\alpha}_{t}}\mathbf{A}^{\dagger}\mathbf{x}^A +(\mathbf{I}-\mathbf{A}^{\dagger} \mathbf{A}) \mathbf{x}_{t} + \mathbf{A}^{\dagger}\mathbf{A}\sqrt{1-\bar{\alpha}_{t}}\boldsymbol{\epsilon_t} - \frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\boldsymbol{\epsilon_t})+\sigma_{t} \boldsymbol{\epsilon},
\end{equation}
As the reverse process progresses and $t$ becomes smaller, the value of $\sqrt{\bar{\alpha}_{t}}$ increases, indicating that the pattern in $\mathbf{A}^{\dagger}\mathbf{A}\mathbf{p}$ becomes increasingly important and cannot be neglected. To address this issue, we revert to the regular reverse process (see the unguided reverse part in Figure~\ref{fig1}) as shown in Equation~\ref{xt-1}. We believe that the semantic information embedded in $\mathbf{x}_t$ is sufficient to guide the model to generate high-quality images when $t$ is relatively small ($t < T' = \lambda T$), where $\lambda$ is a hyperparameter to control model change. Furthermore, since the poisoned pattern is typically outside the pre-trained model's distribution, the last few unguided steps do not generate any trigger patterns on our purified images. We provide our proposed algorithm in Algorithm~\ref{DDPM}.



\begin{algorithm}[t]
\caption{Zero-shot Image Purification (with DDPM)}
\label{DDPM}
\begin{algorithmic}[1]
\Require{Poisoned image $\mathbf{x}^P$; liner transformation $\mathbf{A}$ and its pseudo-inverse $\mathbf{A}^{\dagger}$; pre-trained diffusion model $g$; hyperparameter $\lambda$.}
\Ensure{$\mathbf{x}^A = \mathbf{A}\mathbf{x}^P, \,\,\ T' = \lambda T$}
\State $\mathbf{x}^T \sim \mathcal{N}(\mathbf{0},\mathbf{I})$
%\State $b \gets 1$
\For{$t = T,T-1,...,2,1$}
\State $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$ if $t > 1$, else $\boldsymbol{\epsilon} = \mathbf{0}$
\State $\boldsymbol{\epsilon_t}=g_\phi(\mathbf{x}_t, t)$
\If{$t > T'$}
    \State $\hat{\mathbf{x}}_t = \sqrt{\bar{\alpha}_{t}}\mathbf{A}^{\dagger}\mathbf{x}^A +(\mathbf{I}-\mathbf{A}^{\dagger} \mathbf{A}) \mathbf{x}_{t} + \mathbf{A}^{\dagger}\mathbf{A}\sqrt{1-\bar{\alpha}_{t}}\boldsymbol{\epsilon_t}$
    \State $\mathbf{x}_{t-1}= \frac{1}{\sqrt{\alpha}_{t}} (\hat{\mathbf{x}}_t - \frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\boldsymbol{\epsilon_t})+\sigma_{t} \boldsymbol{\epsilon}$
\Else
    \State $\mathbf{x}_{t-1}= \frac{1}{\sqrt{\alpha}_{t}} (\mathbf{x}_t - \frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\boldsymbol{\epsilon_t})+\sigma_{t} \boldsymbol{\epsilon}$
\EndIf
\EndFor
\State \textbf{return} $\mathbf{x}_0$
\end{algorithmic}
\end{algorithm}


\subsection{Inference Speed-up}

%虽然上面的公式可以生成质量很好的净化图片，但是速度很慢，如果我们希望使用在inference阶段的话，速度太慢了，因此我们在这章讨论如何提升速度

Although Equation~\ref{zero-shot} can generate high-quality purified images in the zero-shot and black-box settings, it is slow during inference, making it difficult to use for real-world applications. The slow inference speed of DDPM is due to the fact that it requires a large number of steps to generate a single sample. Since each step involves computing the estimated noise and diffusion process, which can be computationally expensive and require improvement. 

Therefore, we introduce the denoising diffusion implicit model (DDIM), a recent extension of DDPM that aims to improve its inference speed. Here we modify our reverse process based on the DDIM instead of DDPM. The original DDIM has the reverse process shown below:
\begin{equation}
    \mathbf{x}_{t-1}=\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0|t}+\sqrt{1-\bar{\alpha}_{t-1}-\sigma_{t}^{2}} \boldsymbol{\epsilon}_{t}+\sigma_{t} \boldsymbol{\epsilon},
\end{equation}
where $\mathbf{x}_{0|t}$ is an estimated image based on the step $t$ observation $\mathbf{x}_t$ and can be calculated using Equation~\ref{x0t}.
Our modified speed-up reverse process is shown below:
\begin{equation}
    \mathbf{x}_{t-1}= \sqrt{\bar{\alpha}_{t-1}} \hat{\mathbf{x}}_{0|t}+\sqrt{1-\bar{\alpha}_{t-1}-\sigma_{t}^{2}} \boldsymbol{\epsilon_{t}}+\sigma_{t} \boldsymbol{\epsilon},
\end{equation}
where $\hat{\mathbf{x}}_{0|t}$ is also an estimated image based on $\hat{\mathbf{x}}_t$, where we have $\hat{\mathbf{x}}_{0|t} = \frac{1}{\sqrt{\bar{\alpha}_{t}}} (\hat{\mathbf{x}}_t - \sqrt{1-\bar{\alpha}_{t}}\boldsymbol{\epsilon_t})$.

By applying the above speed-up inference, instead of conducting sampling in thousands of steps, we can sample less than a hundred steps to yield a high-fidelity purified image. The modified algorithm using DDIM is provided in Algorithm~\ref{DDIM}.

\begin{algorithm}[t]
\caption{Zero-shot Image Purification (with DDIM)}
\label{DDIM}
\begin{algorithmic}[1]
\Require{Poisoned image $\mathbf{x}^P$; liner transformation $\mathbf{A}$ and its pseudo-inverse $\mathbf{A}^{\dagger}$; pre-trained diffusion model $g$; hyperparameter $\lambda$; speed-up pace $S$.}
\Ensure{$\mathbf{x}^A = \mathbf{A}\mathbf{x}^P, \,\,\ T' = \lambda T$}
\State $\mathbf{x}^T \sim \mathcal{N}(\mathbf{0},\mathbf{I})$
%\State $b \gets 1$
\For{$t = T,T-S,...,S,1$}
\State $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$ if $t > 1$, else $\boldsymbol{\epsilon} = \mathbf{0}$
\State $\boldsymbol{\epsilon_t}=g_\phi(\mathbf{x}_t, t)$
\If{$t > T'$}
    \State $\hat{\mathbf{x}}_t = \sqrt{\bar{\alpha}_{t}}\mathbf{A}^{\dagger}\mathbf{x}^A +(\mathbf{I}-\mathbf{A}^{\dagger} \mathbf{A}) \mathbf{x}_{t} + \mathbf{A}^{\dagger}\mathbf{A}\sqrt{1-\bar{\alpha}_{t}}\boldsymbol{\epsilon_t}$
    \State $\hat{\mathbf{x}}_{0|t} = \frac{1}{\sqrt{\bar{\alpha}_{t}}} (\hat{\mathbf{x}}_t - \sqrt{1-\bar{\alpha}_{t}}\boldsymbol{\epsilon_t})$
    \State $\mathbf{x}_{t-1}= \sqrt{\bar{\alpha}_{t-1}} \hat{\mathbf{x}}_{0|t}+\sqrt{1-\bar{\alpha}_{t-1}-\sigma_{t}^{2}} \boldsymbol{\epsilon_{t}}+\sigma_{t} \boldsymbol{\epsilon}$
\Else
    \State $\mathbf{x}_{t-1}=\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0|t}+\sqrt{1-\bar{\alpha}_{t-1}-\sigma_{t}^{2}} \boldsymbol{\epsilon_{t}}+\sigma_{t} \boldsymbol{\epsilon}$
\EndIf
\EndFor
\State \textbf{return} $\mathbf{x}_0$
\end{algorithmic}
\end{algorithm}

\section{Experiments}
In this section, we conduct experiments to evaluate the performance of the proposed backdoor defense framework ZIP, in order to answer the following research questions: (1) Does the proposed purification method effectively remove the effects of poisoned triggers? (2) Can the proposed framework successfully defend against various attacks while maintaining high classification accuracy on clean images? and (3) Does our proposed diffusion model recover the semantic information removed by strong transformations?

\subsection{Experimental Settings}
%we apply on our purification on all the test images, including the 
For defense evaluation, we conducted experiments on three types of backdoor attacks: BadNet~\cite{gu2017badnets}, Blended~\cite{chen2017targeted}, and Attack in the physical world (PhysicalBA)~\cite{li2021backdoor}. To configure these attack algorithms, we follow the benchmark paper setting~\cite{li2023backdoorbox} and utilize the open-sourced code provided. We evaluated the effectiveness of our defense framework on three datasets: CIFAR-10~\cite{krizhevsky2009learning}, GTSRB~\cite{stallkamp2012man}, and Imagenette~\cite{howard2019imagenette}. Our poisoned classification network was based on ResNet-34~\cite{he2016deep}. 
%However, we did not include WaNet attacks on Imagenette2 datasets due to a failure to reproduce results from their original papers. 
%For detailed information on the backdoor attacks and training settings, please refer to Appendix 2.
 

To implement our proposed purification method, we select average pooling as the linear transformation $\mathbf{A}$ and up-sampling as the pseudo-inverse operation $\mathbf{A}^{\dagger}$. We also select a pre-trained diffusion model from~\cite{dhariwal2021diffusion} that is designed to process images with a fixed size of 256x256 pixels. To ensure that our proposed purification method is applicable to various datasets, we develop a tiling strategy. For images with dimensions smaller than 256x256 pixels, we combine them into a single larger image by tiling them. For example, by tiling 64 images of size 32x32, we can create a 256x256 image. We then apply our purification method to the resulting tiled image, and split the purified image back into the original sizes. For images with dimensions larger than 256x256 pixels, we resize them to 256x256 before applying our method. This tiling strategy enables our defense approach to handle images of various sizes while also significantly increasing the purification speed for small images by processing multiple images simultaneously. Furthermore, we employ the algorithm described in Algorithm~\ref{DDIM} to further accelerate the inference process. As a result, we can generate high-quality images in just 20 steps.


To assess the effectiveness of our proposed method, we conduct a comparative evaluation against two baseline approaches, namely ShrinkPad~\cite{li2020rethinking} and the Naive Transformation. The former is a state-of-the-art image transformation based defense method that can work on black-box models in the zero-shot setting. While the latter is an image transformation based defense method, where we naively use the transformed images $\mathbf{A}^{\dagger}\mathbf{x}^A$ as purified images. We apply our purification process and baseline defense methods to all test datasets, and then evaluate the purified/defended test set using poisoned models. In addition to using the clean accuracy (CA) and attack success rate (ASR) metrics to assess the effectiveness of our defense, we introduce a new metric called poisoned accuracy (PA). The PA metric measures the classification performance of the purified attacked samples on the poisoned model. A higher PA value indicates a higher likelihood for the original poisoned samples to be correctly classified despite using an attacked classification neural network. 
%This metric provides a more comprehensive evaluation of the effectiveness of our purification method in defending against backdoor attacks and indicates the level of confidence end-users can have in the purified samples.


\subsection{Qualitative Results of Purification}

\begin{figure}[htp]
    \begin{subfigure}[t]{0.99\textwidth}
    \includegraphics[width=0.96\textwidth]{figure/Figure2.pdf}
     \caption{Qualitative results of Blended attack purification.}
    \end{subfigure}
    \begin{subfigure}[t]{0.99\textwidth}
    \includegraphics[width=0.96\textwidth]{figure/Figure3v1.pdf}
    \caption{Qualitative results of BadNet attack purification.}
    \end{subfigure}
    %\hspace{-12pt}
    \caption{Qualitative results of purification on test images.} \label{fig2}
    \hspace{8pt}
\end{figure}

To answer research question (1), we conducted qualitative experiments on poisoned images with trigger patterns selected from the BadNet and Blended attack. We applied our purification method to these images. Figure~\ref{fig2} shows examples of poisoned images $\mathbf{x}^P$ and purified image $\mathbf{x}'$, where the trigger pattern is clearly visible in the poisoned images but has been altered/removed in the purified images. We also demonstrated the transformed image $\mathbf{A}^{\dagger}\mathbf{x}^A$ and the generated images $\mathbf{x}_{T'}$ from the diffusion model at step $T'$. We can observe that the transformed images can destruct the trigger pattern, but they also destruct semantic information. We can also find that the generated image $\mathbf{x}_{T'}$ contains sufficient semantic information to guide the diffusion model. Overall, these results demonstrate the effectiveness of our proposed purification method in removing the effect of trigger patterns from images while maintaining semantic information.



\subsection{Quantitative Results of Defense}

%1. 我们的模型可以在成功地defend各种backdoor的同时, 相比于baseline方法维持了目前模型的classification accuracy。这是因为我们不仅仅使用transformation来制造mismatch of poisoned trigger and poisoned lable, 我们更使用diffusion model recover回来了丢失的语义信息，来保证模型分类的准确。
%2. 可以看出对于我们提出的新指标：poisoned accuracy（PA），我们的模型维持了比较高的PA. 这意味受污染的样本经过我们的方法净化后依旧可以被正常的用于分类任务，即使在使用一个被攻击的黑盒的分类模型。
%3. 我们可以观察到：The ShrinkPad method performs poorly on the PhysicalBA attack, which is consistent with the observation in~\cite{li2021backdoor}. 这是因为这种攻击设计的出发点就是为了逃逸shrinkpad这种defense方法。但我们可以观察到我们提出的模型依旧成功defense了这种攻击。

\begin{table}[htp]
  \renewcommand\arraystretch{1.2}
  \caption{The clean accuracy (CA \%), the attack success rate (ASR \%), and the poisoned accuracy (PA \%) of three backdoor defense methods against three kinds of backdoor attacks, including two visible backdoor attacks and one invisible attacks. $\textit{None}$ means the training data is completely clean.}
  \label{table1}
  \centering
\resizebox{0.99\columnwidth}{!}{
\begin{tabular}{c|l|ccc|ccc|ccc|ccc}
\hline
\multicolumn{1}{c|}{Dataset}           & Types   & \multicolumn{3}{c}{No Defense} & \multicolumn{3}{c}{ShrinkPad} & \multicolumn{3}{c}{Naive Transformation}  &\multicolumn{3}{c}{\textbf{ZIP} (Ours)}       \\ \hline
\multicolumn{1}{l|}{}                  &         & CA  $\uparrow$         & ASR $\downarrow$    &PA $\uparrow$         & CA  $\uparrow$       & ASR $\downarrow$   &PA $\uparrow$      & CA  $\uparrow$        & ASR  $\downarrow$   &PA $\uparrow$         & CA  $\uparrow$                  & ASR $\downarrow$ &PA  $\uparrow$ \\ \hline
\multirow{5}{*}{\tabincell{c}{CIFAR-10 (32 $\times$ 32)\\ (10 classes)}}      & None    & 80.15             & ---                & ---           &---               & ---            &---                 &---  &---   &---  &---  &---   &---    \\
                                       & BadNet &80.57              & 99.98                & 10.00     & 63.67               &9.24            & 62.98      &59.13             & 21.56         & 54.59                      & $\mathbf{72.57}$            &$\mathbf{1.30}$                        &$\mathbf{70.93}$          \\
                                      %& Input-A &              &                 &            &               &             &          &             &       &                       &         \\
                                       & Blended   &80.26              &99.96                 &10.03    & 58.97            &$\mathbf{2.28}$     & 40.22        &55.91    & 3.04                &    49.91                      & $\mathbf{74.84}$            &3.29                        &$\mathbf{65.39}$        \\
                                       %& Refool  &95.82              &85.22                 &98.40            &86.77               &80.14             &20.35      &97.80             &\red{61.93}           &96.26                       &\red{74.29}         \\
                                       %& WaNet &              &                 &             &                &             &     &             &             &                        &        \\
                                       & PhysicalBA  & 94.77             & 99.99                &10.01            &$\mathbf{93.15}$               &  99.95           &10.05     &54.86    & 5.88            & 52.28                           &81.17  &$\mathbf{0.86}$  &$\mathbf{78.90}$  \\
                                       & Average  &85.20	&99.98	&10.01	&71.93	&37.16	&37.75	&56.63	&10.16	&52.26	&$\mathbf{76.19}$	&$\mathbf{1.82}$	&$\mathbf{71.74}$         \\ \hline
\multirow{5}{*}{\tabincell{c}{GTSRB (32 $\times$ 32)\\ (43 classes)}}      & None  &96.95   & ---                & ---           &---               & ---            &---                 &---  &---   &---  &---  &---   &---         \\
                                       & BadNet & 96.53             & 99.99       & 5.70                       &78.33            &$\mathbf{5.81}$    &78.82             &95.98               & 7.33            &95.11          & $\mathbf{96.25}$                       &6.07    &$\mathbf{96.12}$     \\
                                      %& Input-A &              &                 &            &               &             &          &             &       &                       &         \\
                                       & Blended   &96.58             &99.89    & 5.79                           & 76.76  &10.54  &56.41         & 93.68              &   11.07          & 73.91              & $\mathbf{96.19}$                      & $\mathbf{8.78}$ & $\mathbf{82.09}$       \\
                                       %& Refool  &95.82              &85.22                 &98.40            &86.77               &80.14             &20.35      &97.80             &\red{61.93}           &96.26                       &\red{74.29}         \\
                                       %& WaNet  &98.55              &99.33                &6.35  & & &         & 89.31              &54.10             &45.87      &95.70             &1.83           &94.65            \\
                                       & PhysicalBA  & 96.83             & 100.00                 &  5.70          & $\mathbf{97.41}$              &100.00             &5.70       &91.00             &$\mathbf{5.53}$          & 90.53    &95.47 &6.26 &$\mathbf{95.29}$ \\
                                       & Average  &96.65	&99.96	&5.73	&84.17	&38.78	&46.98	&93.55	&7.98	&86.52	&$\mathbf{95.97}$	&$\mathbf{7.04}$	&$\mathbf{91.17}$        \\ \hline
\multirow{5}{*}{\tabincell{c}{Imagenette (256 $\times$ 256) \\ (10 classes)}}      & None    &84.58             & ---                & ---           &---               & ---            &---                 &---  &---   &---  &---  &---   &---         \\
                                       & BadNet & 84.99             & 94.53                & 14.98           &71.23    &8.56               & 70.72          &81.47   &16.45               &79.94                       &$\mathbf{84.48}$                      &$\mathbf{1.30}$ & $\mathbf{84.86}$         \\
                                      %& Input-A &              &                 &            &               &             &          &             &       &                       &         \\
                                       & Blended  &86.14               &99.85                  &10.19            &74.06   &20.63   &36.10               & 78.95            &79.41       &25.57                  &$\mathbf{82.54}$    &$\mathbf{15.43}$          &$\mathbf{46.08}$                      \\
                                       %& Refool  & 64.95             &  \red{42.95}               &            &               &         &    &      &             &60.00           &\red{38.50}                               \\
                                       &PhysicalBA  &90.67              &72.94                 &34.29            &$\mathbf{90.21}$   & 96.81           &13.07              & 84.84            &32.40      &74.87             & 88.58          & $\mathbf{4.78}$                      & $\mathbf{87.94}$         \\
                                       & Average &87.27	&89.11	 &19.82	 &78.50	 &42.00	 &39.96	 &81.75	 &42.75	&60.13	&$\mathbf{85.20}$	&$\mathbf{7.17}$	& $\mathbf{72.96 }$        \\ \hline
\end{tabular}}
\end{table}

We conducted quantitative experiments to answer research questions (2) and (3), the results of which are presented in Table~\ref{table1}. Our experiments demonstrate the effectiveness of our proposed defense methods in reducing the ASR of three types of attacks across three different datasets. Our approach consistently outperformed the other two baseline methods across most datasets and attack types. For example, on the Imagenette dataset, our method reduces the ASR of the BadNet attack from 94.53\% (with no defense) to just 1.3\%, with only a 0.51\% CA drop. Similarly, on the GTSRB dataset, our approach reduces the success rate of the PhysicalBA attack from 100\% (with no defense) to 6.26\%, with only a 1.36\% CA drop.

%However, we also observe that the performance of the defense methods varied depending on the attack type and dataset. For instance, the Naive Transformation method performed well against the PhysicalBA attack on the GTSRB dataset but poorly against the Blend attack on the ImageNet Subset dataset. Conversely, the Shrinkpad method performed well against the Blend attack on the CIFAR-10 dataset but poorly against the BadNets attack on the same dataset. The ShrinkPad method performs poorly on the PhysicalBA attack, which is consistent with the observation in~\cite{li2021backdoor}.

Our observations are as follows: (1) Our model successfully defends against various backdoors while maintaining classification accuracy compared to baseline methods. This is because our approach not only uses transformations to create a mismatch between poisoned triggers and poisoned labels, but also uses a diffusion model to recover lost semantic information to ensure accurate classification. (2) Our proposed purification-based method also outperforms the other two methods in a new evaluation metric that we propose, poisoned accuracy (PA), indicating that contaminated samples can still be used for classification even when using an attacked black-box model as the classifier. This demonstrates the robustness of our approach in real-world scenarios. (3) We find that the ShrinkPad method performs poorly on the PhysicalBA attack, which is consistent with the findings in~\cite{li2021backdoor}. This is because PhysicalBA attack is specifically designed to evade defense methods such as ShrinkPad~\cite{li2021backdoor}. However, our proposed model successfully defends against this attack, demonstrating its superior performance compared to existing defense methods. 


\section{Related Work: Backdoor Defense}
%\label{sec:related-work}
%\subsection{Backdoor Defense}

Existing defense methods for black-box backdoor models can be classified into two types. The first type involves training a proxy detection model using a batch of collected clean/poisoned images as input~\cite{zeng2021rethinking, guo2021aeva, li2021anti}. This detection model distinguishes between clean and poisoned samples by comparing them with other samples. However, this approach has three limitations. First, the success of the defense depends strongly on the quality~\cite{guo2021aeva, zeng2021rethinking} and quantity~\cite{zeng2021rethinking, li2021anti} of the collected data. Second, for any new attacks with new trigger patterns, the detection model requires retraining, which can be time consuming. Lastly, when detecting images that contain the trigger pattern, these methods will usually reject poisoned images and avoid performing any further inference on them~\cite{guo2021aeva, zeng2021rethinking}. Users naturally expect to receive results for all their test samples, so discarding attacked test images is not a viable option, which limits the usefulness of these methods in real-world applications. The second type of defense methods applies strong image transformations on test images before feeding them to the poisoned model for prediction~\cite{li2021backdoor}.
 Strong transformation introduces variations to the input images, creating a mismatch of trigger patterns and poisoned labels, making it more difficult for the poisoned model to recognize the trigger pattern. On the one hand, transformation-based methods can alleviate the limitations of the first type and help users make use of poisoned data. On the other hand, these methods can only defend against static pattern-based attacks. They cannot effectively detect more advanced attacks, such as those with dynamic patterns~\cite{li2021backdoor}, which limits their application.



%\subsection{Diffusion Model}
%Denoising diffusion probabilistic models (DDPM)~\cite{ho2020denoising} is a new generation generative model in the field of computer vision.  It proposes to use a diffusion process to propagate information from pixels that have reliable information to those that have unreliable information. Some extensions to DDPM include conditional probabilistic~\cite{rombach2022high,avrahami2022blended,liu2022diffsinger}, different noisy distributions~\cite{bansal2022cold}, and shorter generation steps~\cite{zhang2022fast,aiello2023fast}. In our research, we focus on exploring the zero-shot abilities of the pre-trained diffusion method~\cite{dhariwal2021diffusion} in terms of backdoor defense.

\section{Conclusion}

In this paper, we propose a novel framework for protecting against backdoor attacks under black-box setting. In detail, we first apply a strong transformation on the poisoned image to destroy the trigger pattern, and then utilize a pre-trained diffusion model to recover the removed semantic information removed by the transformation to maintain the fidelity of the purified images. We have conducted experiments to demonstrate the effectiveness of our proposed methods in defending against various backdoor attacks. Our contributions include the development of an effective poisoned image purification method that does not require model internal information or any training samples. Our method enables end-users to utilize full test samples without discarding any, even when using an attacked classification model. In summary, our framework provides a promising solution for defending against backdoor attacks on black-box models and has the potential to enhance the security and robustness of machine learning systems. Further research and development in this area can build upon our proposed methods and expand the scope of their application.




% \section{References}

{
\bibliographystyle{plain}
\bibliography{bib}
}



\end{document}