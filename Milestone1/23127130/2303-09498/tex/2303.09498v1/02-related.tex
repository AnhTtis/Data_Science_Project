\section{Related work}

Research on explainable recommendation has focused on different dimensions, including (1) explanation goal 
(e.g., transparency, trust, effectiveness, persuasiveness),
(2) style (e.g., content-based, collaborative-based, knowledge/utility-based), 
(3) scope (e.g., user model, process, recommended item), and
(4) format (e.g., textual, visual)~\citep{Tintarev:2015:Book,Nunes:2017:UMUAI,Zhang:2020:FnTIR}.
Another way of categorization is by the method used to generate explanations: self-explainable recommender models vs. post-hoc explanations (i.e., \emph{justifications})~\citep{Biran:2017:IJCAI}.
Our approach can be classified as content-based, post-hoc, natural language explanation generation.
We do not aim to explain \emph{why} a given item was recommended, but rather provide the user with additional information (relevant characteristics of items) with the purpose of aiding them in their decision making. 

Various graphical and textual explanation formats have been considered in the past~\citep{Herlocker:2000:CSCW,Gedikli:2014:IJHCS}, with natural language being the most popular both historically~\citep{Nunes:2017:UMUAI} and in recent years~\citep{Balog:2020:SIGIR,Chang:2016:RecSys,Musto:2019:UMAP,Ni:2019:EMNLP,Penha:2022:CHIIR}.
Text-based explanations range from tags or keywords~\citep{Bilgic:2005:IUI,Vig:2009:IUI} to 
single or multiple sentences~\citep{Chang:2016:RecSys,Balog:2020:SIGIR,Tintarev:2012:UMUAI,Penha:2022:CHIIR,Musto:2019:UMAP}.
In this work, we consider both short descriptive phrases (aspects) and fluent natural language summaries as explanations.  Unique to our approach is that the summaries are generated directly from the aspects, allowing for a direct comparison between the two explanation formats.

Reviews have been exploited for improving recommendations, leading to a line of work on review-aware recommender systems~\citep{Chen:2015:UMUAI,Hernandez-Rubio:2019:UMUAI}, naturally endowing them with a higher degree of explainability and transparency~\citep{He:2015:CIKM}.
In contrast, our work continues the thread of research on leveraging reviews for the purpose of generating post-hoc recommendation justifications, treating the recommender engine as a black box~\citep{Muhammad:2016:IUI,Musto:2019:UMAP,Chang:2016:RecSys,Ni:2019:EMNLP,Penha:2022:CHIIR}.
%
\citet{Muhammad:2016:IUI} highlight the most important features (pros and cons) of an item that are likely to matter to the user (based on their own reviews).
Similarly, \citet{Chen:2014:WWW,Chen:2017:IUI} extract sentiments on specific product attributes and use them to present the user with alternatives to a given recommendation and explaining the trade-offs, i.e., which attributes would be improved and which ones would be compromised.  
\citet{Chang:2016:RecSys} employ a multi-step pipeline for generating natural language explanations using crowdsourcing, by (1) refining algorithmically generated tag clusters,  
(2) writing explanations for these clusters by synthesizing review text, and (3) selecting the best explanations by voting.  
We follow a similar approach, but present crowd workers with a significantly simpler task: given a set of reviews, they only need to extract positive and negative aspects from them.  We then either present these aspects as a list, or turn them into fluent natural language text using state-of-the-art neural language modeling techniques.
\citet{Ni:2019:EMNLP} identify review segments that can serve as justifications and explore the use of neural language models to generate convincing and diverse justifications.
\citet{Penha:2022:CHIIR} choose a helpful sentence for an item from its review, predicted by a classifier, and use that to generate a template-based explanation of an item or a pair of items.
%
Most closely related to ours is the work by~\citet{Musto:2019:UMAP}, which presents a fully automated pipeline for generating natural language justifications by (1) extracting a set of aspects that characterize the item, (2) identifying the most relevant ones, and (3) extracting and aggregating review sentences discussing these aspects.
Though there are similarities in the explanation generation workflow, their focus is on the algorithmic aspects of automation, while ours is on understanding the \emph{impact} of explanations.  
Our aspects are short descriptive phrases, not limited to nouns as in~\citep{Musto:2019:UMAP}, which we extract and curate manually.
Most importantly, we include aspects with negative sentiment as well, not only positive ones, in the explanations.

Evaluation of explanations may target interface-related aspects~\citep{Chen:2017:IUI,Muhammad:2016:IUI,Bilgic:2005:IUI,Herlocker:2000:CSCW,Gedikli:2014:IJHCS}, 
quantifiable properties of generation approaches (e.g., readability of natural language~\citep{Costa:2018:IUI} or fidelity of post-hoc explanations~\citep{Peake:2018:KDD}), 
or subjective perceptions of quality (in terms of transparency, trust, effectiveness, etc.) through questionnaires~\citep{Balog:2020:SIGIR,Musto:2019:UMAP,Nunes:2017:UMUAI,Chang:2016:RecSys,Gedikli:2014:IJHCS}.
Most relevant to our work is the evaluation protocol proposed in \citep{Bilgic:2005:IUI} (also followed in \citep{Gedikli:2014:IJHCS}) for measuring the \emph{persuasiveness} of explanation types, in terms of differences between an initial rating, given based on the explanation, and a second rating, given after the ``consumption'' of the item.
Consumption is approximated by subjects receiving more details (description or user reviews) about the item.  Then, they are asked adjust their rating based on this additional information.
Our experiment design is more realistic in that we do not assume item consumption, but rather measure the impact of explanations via the item selection choices users make, with a prior understanding of what they would most likely pick. 
