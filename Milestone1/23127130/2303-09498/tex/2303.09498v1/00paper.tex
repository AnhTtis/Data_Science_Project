\documentclass[sigconf,natbib=true]{acmart}  % add anonymous for submission

\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{colortbl}

\usepackage[font=small,labelfont=bf]{caption}

\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\thumbsup}[0]{\raisebox{-2pt}{\includegraphics[width=0.3cm]{figures/thumbs_up.jpg}}}
\newcommand{\thumbsdown}[0]{\raisebox{-2pt}{\includegraphics[width=0.3cm]{figures/thumbs_down.jpg}}}

\definecolor{forestgreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{brickred}{rgb}{0.8, 0.25, 0.33}

\newcommand{\greenup}{\textcolor{forestgreen}{$\blacktriangle$}}
\newcommand{\reddown}{\textcolor{brickred}{$\blacktriangledown$}}

\interfootnotelinepenalty=10000

\title{Measuring the Impact of Explanation Bias: A Study of Natural Language Justifications for Recommender Systems}

\author{Krisztian Balog}
  \affiliation{%
    \institution{Google}
    \city{Stavanger}
    \country{Norway}}
  \email{krisztianb@google.com}

\author{Filip Radlinski}
  \affiliation{%
    \institution{Google}
    \city{London}
    \country{UK}}
  \email{filiprad@google.com}

\author{Andrey Petrov}
  \affiliation{%
    \institution{Google}
    \city{London}
    \country{UK}}
  \email{apetrov@google.com}
  

\copyrightyear{2023}
\acmYear{2023}
\setcopyright{rightsretained}
\acmConference[CHI EA '23]{Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems}{April 23--28, 2023}{Hamburg, Germany}
\acmBooktitle{Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems (CHI EA '23), April 23--28, 2023, Hamburg, Germany}\acmDOI{10.1145/3544549.3585748}
\acmISBN{978-1-4503-9422-2/23/04}

\begin{document}

\begin{abstract}
Despite the potential impact of explanations on decision making, there is a lack of research on quantifying their effect on users' choices.
This paper presents an experimental protocol for measuring the degree to which positively or negatively biased explanations can lead to users choosing suboptimal recommendations.
Key elements of this protocol include a preference elicitation stage to allow for personalizing recommendations, manual identification and extraction of item aspects from reviews, and a controlled method for introducing bias through the combination of both positive and negative aspects. We study explanations in two different textual formats: as a list of item aspects and as fluent natural language text.
Through a user study with 129 participants, we demonstrate that explanations can significantly affect users' selections and that these findings generalize across explanation formats. 
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003121.10003122.10003334</concept_id>
       <concept_desc>Human-centered computing~User studies</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10011748</concept_id>
       <concept_desc>Human-centered computing~Empirical studies in HCI</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003347.10003350</concept_id>
       <concept_desc>Information systems~Recommender systems</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~User studies}
\ccsdesc[500]{Human-centered computing~Empirical studies in HCI}
\ccsdesc[500]{Information systems~Recommender systems}

\keywords{Explainable recommendation; natural language justifications; evaluating explanations; explanation types; explanation bias}

\maketitle

\input{01-introduction}
\input{02-related}
\input{03-design}
\input{04-results}
\input{05-conclusion}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\appendix
\input{99-appendix}

\end{document}
