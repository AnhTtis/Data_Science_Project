\section{Introduction}

Recommender systems have become pervasive in modern society, leading to many studies of the degree of trust people place in provided recommendations~\citep{Harman:2014:RecSys,Kunkel:2019:CHI}.  As a result, significant attention has been paid to equip these systems with explanation facilities to help users make informed decisions~\citep{Tintarev:2015:Book,Nunes:2017:UMUAI,Zhang:2020:FnTIR}.
In one of the earliest studies, \citet{Herlocker:2000:CSCW} showed that explanations can make it more likely that people will adopt the recommendations made.  Yet, studies measuring the eventual satisfaction with recommendations found that certain types of explanations can cause users to over- or underestimate the real value of a recommended item~\citep{Bilgic:2005:IUI,Gedikli:2014:IJHCS}. 
Even though explanations are known to be able to significantly affect the decision-making process of users~\citep{Tintarev:2015:Book}, significant gaps remain when it comes to measuring and understanding their effects~\citep{Piscopo:2022:SIGIRForum}.
The bulk of studies of explanations focus on the subjective question of how people perceive the recommendations~\cite{Balog:2020:SIGIR,Musto:2019:UMAP,Nunes:2017:UMUAI,Chang:2016:RecSys,Gedikli:2014:IJHCS}. 
Less attention has been paid to how to objectively quantify the degree to which explanations affect the choices that people make.  This is closely related to the concept of \emph{persuasiveness}, which is the ability of an explanation ``to convince the user to accept or disregard certain items''~\citep{Gedikli:2014:IJHCS}.
Past research has differentiated between over- and underestimate-oriented persuasiveness of \emph{explanation types}~\citep{Bilgic:2005:IUI,Gedikli:2014:IJHCS}. 
Instead, we study in quantitative detail how bias in the \emph{content} of natural language explanations can impact users' choices.

A key difference to past work is that prior research has focused almost exclusively on explanations that highlight positive aspects of items, i.e., endorse a selection, perhaps by drawing similarities to other items the user has previously indicated that they liked.  Explanations can also help users make decisions by highlighting why \emph{not} to select something.  We perform the first study that intentionally biases explanations towards positive or negative aspects of items in a controlled manner in the context of items where we have a prior understanding of which items users should find most relevant. 
We can thus quantify the degree to which positively or negatively biased explanations lead to users choosing suboptimal recommendations.  To achieve this, we are inspired by the work of \citet{Musto:2019:UMAP} who ``\emph{processed and analyzed the reviews in order to obtain a set of characteristics that are often discussed in the reviews (with a positive sentiment, of course) and can induce the user in enjoying the recommended item.}'' However, we identify positive as well as \emph{negative} aspects---as argued by~\citet{Bilgic:2005:IUI}, ``\emph{the goal of an explanation should not be to `sell' the user on the item but rather to help the user to make an informed judgment.}''

The primary research question we address is how to quantify the degree to which explanations influence user decisions, either in the positive or negative direction when presented with a set of recommendations from which to choose.  
Our key contribution is the design of an experiment protocol that allows for a quantitative study of the impact of explanations on users' choices.
Our second contribution is a preliminary analysis of the results obtained with 129 subjects.
We provide baseline statistics showing how often users presented with biased explanations can end up selecting less relevant suggestions. 
Beyond this, we study the effect of the \emph{format} of explanations, directly comparing two common formats, namely lists of item aspects and natural language text. We show that while these formats lead to a similar overall behavior, the effect of explanation bias is more pronounced in case of aspect lists than for natural language text.  These may be considered as factors when designing explainable recommender systems. Overall, this work provides a framework for measuring and optimizing explanations to help guide users to make informed decisions.
