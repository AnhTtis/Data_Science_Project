\section{Study Design}
\label{sec:design}

This section presents the study we designed in order to answer the following two research questions:

\begin{itemize}[topsep=5pt]
    \item \textbf{RQ1} How much can explanations influence users' decisions when presented with a set of item recommendations?
    \item \textbf{RQ2} How big a role does the presentation format of explanations play in the results?
\end{itemize}
%
Our experiment centers around a set of item selection tasks, with participants asked to choose among three items that are presented to them as recommendations. 
These items are selected, based on an initial preference elicitation phase, such that we know which of the three items the user would probably like the most and which one the least. 
By applying a controlled amount of bias via explanations to a randomly chosen item, we are able to measure how much users may be influenced in their choices---it is of particular interest to observe how often they would be inclined to pick a different item than that which they would most likely choose (statistically speaking).
Using a within-subject design, participants are exposed to different experimental conditions, varying the amount of bias and the format of explanations.

\subsection{Overview}

The study is conducted in the movies domain.  As watching films is one of the top media consumption and entertainment activities, most people can easily relate to it without any training or prerequisite knowledge. Indeed, movies have been one of the most studied domains for explainable recommendation~\citep{Chang:2016:RecSys,Balog:2019:SIGIR,Balog:2020:SIGIR,Vig:2009:IUI,Tintarev:2012:UMUAI,Musto:2019:UMAP,Ghazimatin:2021:WWW,Herlocker:2000:CSCW,Gedikli:2014:IJHCS}.
Note that even though the instructions are tailored specifically to movies, the proposed design is generic and can be applied in other domains. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/overview2.pdf}
    \vspace*{-0.5\baselineskip}
    \caption{Overview of our experiment design.}
    \vspace*{-0.5\baselineskip}
    \label{fig:overview}
    \Description{Schematic view of the two stages of the experiment design. The top figure corresponds to Stage 1 and has a 3-column tabular layout. The headings are: "Item," "Seen?," and "Rating (if seen)". Items have placeholders for image and title, seen has radio buttons with Y and N values, and rating has a number of radio buttons. The bottom figure corresponds to Stage 2, where three items are shown next to each other with image, title, description placeholders and a radio button next to each for the user to make a selection.}
\end{figure}

To be able to provide participants with personalized recommendations, the study is divided into two  stages, as shown in Fig.~\ref{fig:overview}.\footnote{Additionally, participants are asked to fill out a post-task questionnaire to allow for a qualitative analysis of their preferences in the future.}
In Stage 1, participants indicate for each movie, in our pool of 400 candidates items, whether they have watched it, and if yes, how much they liked it (on a three-point scale of disliked, neutral, and liked).
In Stage 2, participants complete a total of 12 item selection tasks, corresponding to various experimental conditions in a random order (cf.~Table~\ref{tab:conditions}).  In each task, they are offered three movies to choose from, selected among the items they have not watched yet.
For each movie the title, poster, and synopsis are shown, followed (optionally) by an explanation (see Fig.~\ref{fig:task2} in Appendix~\ref{app:ui}).
The item selection task is situated in the following scenario:
\emph{Imagine that you are traveling alone, and while in transit, you have been offered a free movie rental from a service that only allows you to choose from three possible options. In each of the following tasks, select the movie you would prefer to watch based on the information provided.}

We hypothesize that the explanations accompanying the recommendations can influence users' item selection choices.
To test this, we contrast a baseline setting, showing neutral explanations for all three items, with a ``biased'' setting: showing neutral explanations for two of the items, while making the explanation for the third item overall more positive or negative in a controlled manner.
%
Next, we provide details on each element of the experiment design. 

\begin{table}[t]
    \centering
    \caption{Experimental conditions. Each participant experiences each of the listed conditions exactly once (in random order).}
    \vspace*{-0.5\baselineskip}
    \label{tab:conditions}
    \begin{tabular}{ |r|l|l|c|c| } 
        \hline
        \textbf{ID} & \textbf{Bias} & \textbf{Explanation} & \textbf{Aspects$^+$} & \textbf{Aspects$^-$}  \\ 
        \hline
        \hline
        \multicolumn{5}{|l|}{\emph{Baselines}} \\    
        \hline
        \#1 & \multirow{4}{*}{No bias} & \multirow{2}{*}{-} & \multirow{2}{*}{-} & \multirow{2}{*}{-} \\
        \#2 & & & & \\
        \cline{3-5}
        \#3 & & Itemized & \multirow{2}{*}{2} & \multirow{2}{*}{2} \\
        \cline{3-3}
        \#4 & & Fluent-NL & & \\
        \hline
        \multicolumn{5}{|l|}{\emph{One random item biased by a more positive/negative explanation}} \\    
        \hline
        \#5 & \multirow{2}{*}{Bias +} & Itemized & \multirow{2}{*}{3} & \multirow{2}{*}{1} \\
        \cline{3-3}
        \#6 & & Fluent-NL & & \\
        \hline
        \#7 & \multirow{2}{*}{Bias -} & Itemized & \multirow{2}{*}{1} & \multirow{2}{*}{3} \\
        \cline{3-3}
        \#8 & & Fluent-NL & & \\
        \hline
        \#9 & \multirow{2}{*}{Bias ++} & Itemized & \multirow{2}{*}{4} & \multirow{2}{*}{0} \\
        \cline{3-3}
        \#10 & & Fluent-NL & & \\
        \hline
        \#11 & \multirow{2}{*}{Bias -{-}} & Itemized & \multirow{2}{*}{0} & \multirow{2}{*}{4} \\
        \cline{3-3}
        \#12 & & Fluent-NL & & \\
        \hline
    \end{tabular}
\end{table}


\subsection{Item Collection}
\label{sec:design:items}

We use the MovieLens 25M collection~\citep{Harper:2015:TIIS} along with the Movies and TV subset of the Amazon Reviews 2018 dataset~\citep{Ni:2019:EMNLP}.
Mappings between the two are provided by the Reviews2Movielens dataset (v2 mappings) 
released in~\citep{Zemlyanskiy:2021:EACL}.
%
A total of 400 movies are sampled, using a slightly modified version of the stratified sampling approach employed in~\citep{Balog:2019:SIGIR}: (i) the top 150 movies of all time by the number of ratings received and (ii) a random movie for each year between 1992 and 2016, and for each of the top 10 most popular genres (action, adventure, documentary, comedy, crime, drama, horror, romance, sci-fi, and thriller), that is not already in the top-150 set.\footnote{If it was not possible, then a random movie is sampled from the same year.}  
We require each movie to have at least 100 associated reviews.

\subsection{Generating Personalized Recommendations}
\label{sec:design:recommendations}

Study participants are presented with a set of item selection tasks.  In each task, they need to make a choice between three items $x_a$, $x_b$, and $x_c$, shown in random order.  These items are selected, based on the initial preference elicitation phase, such that the user's estimated preference ordering is $x_a \succ x_b \succ x_c$.
For that, we employ an ensemble of three established recommender algorithms, representing different classes of collaborative filtering approaches that performed best in a prior user study on movie recommendations~\citep{Balog:2019:SIGIR}:
Item-based k-Nearest Neighbors~\citep{Sarwar:2001:WWW}, % (Item-kNN)
Weighted Regularized Matrix Factorization~\citep{Hu:2008:ICDM}, % (WR-MF)
and a Sparse Linear Method~\citep{Ning:2011:ICDM}. % (BRP-SLIM)
The predictions of these algorithms are combined into an ensemble recommendation using a consensus-based voting system (Borda count).  This ensemble is expected to yield better performance than any of the individual recommenders. 
Crucially, only movies not yet seen by the user are eligible for recommendation, as showing already seen items might affect the measurements of the impact of explanations~\citep{Balog:2020:SIGIR}.

\begin{figure*}
    \vspace*{-0.5\baselineskip}
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
        \cline{2-16}
        & \multicolumn{5}{c|}{\textbf{Bucket A}} & \multicolumn{5}{c|}{\textbf{Bucket B}} & \multicolumn{5}{c|}{\textbf{Bucket C}} \\
        \hline
        \multicolumn{1}{|l|}{\textbf{Item rank}} & 
            $1$ & $\dots$ & \cellcolor{red!25}$i$ & $\dots$ & $s$ &
            $s+1$ & $\dots$ & \cellcolor{red!25}$s+i$ & $\dots$ & $2s$ &
            $2s+1$ & $\dots$ & \cellcolor{red!25}$2s+i$ & $\dots$ & $n$ \\
        \hline
    \end{tabular}
    \vspace*{-0.5\baselineskip}
    \caption{Sampling of items for item selection tasks to ensure the largest distance between sampled items across all item samples.}
    \label{fig:item_sampling}
    \Description{Table with three columns: Bucket A, Bucket B, and Bucket C. Within each column the item ranks are shown. In Bucket A these range from 1 to s, in Bucket B from s+1 to 2s, and in Bucket C from 2s+1 to n. Item ranks i, s+i, and 2s+i within Buckets A, B, and C, respectively, are highlighted.}
\end{figure*}

\renewcommand{\arraystretch}{1.25}
\begin{figure*}[t]
    \small
    %\vspace*{-0.5\baselineskip}
    \begin{tabular}{|ll|ll|ll|}
        \hline
            \thumbsup & funny, silly, sweet & 
            \thumbsup & good, clean, and exciting movie &
            \thumbsup & suspenseful \\
            \thumbsup & awesome comedy & 
            \thumbsup & full of action &
            \thumbsdown & a bad remake \\
            \thumbsdown & a few little laughs but not enough & 
            \thumbsup & entertainment for the whole family &
            \thumbsdown & predictable \\
            \thumbsdown & lackluster direction &
            \thumbsdown & predictable attempt at comedy &
            \thumbsdown & low budget effects \\
        \hline
        \hline
        \multicolumn{2}{|p{5cm}|}{This movie is said to be funny, silly, sweet, and an awesome comedy. However, others say it has a few little laughs but not enough, and a lackluster direction.}
        &
        \multicolumn{2}{p{5cm}|}{This movie is said to be good, clean, and exciting, full of action, and an entertainment for the whole family, but some people find it to be a predictable attempt at comedy.}
        &
        \multicolumn{2}{p{5cm}|}{This movie is said to be suspenseful, but others find it to be a bad remake and predictable, with low-budget effects.} \\
        \hline
    \end{tabular}
    \vspace*{-0.5\baselineskip}
    \caption{Examples of itemized and fluent-NL explanations, based on the same set of aspects.} 
    \label{fig:explanation}
    \Description{Table with three columns and two rows. The columns illustrate different explanations, the rows correspond to different explanation styles: itemized and fluent text. Itemized explanations have green thumbs up and red thumbs down icons as the bullet symbol. The left column has two positive and two negative aspects, the middle column has three positive and one negative aspects, and the right column has one positive and three negative aspects.}
\end{figure*}
\renewcommand{\arraystretch}{1.0}

Given a total of $m=12$ item selection tasks to be completed by each participant (cf. Table~\ref{tab:conditions}), $m \times 3$ items are sampled as follows.\footnote{Participants with fewer than 3$\times$12 not yet seen movies are excluded from the study.}
Let $n$ denote the number of movies the person has not seen yet, which are sorted by recommendation score, such that $x_1$ is the highest and $x_n$ is the lowest ranked suggestion.  These items are divided into three approximately equal-sized ($s=\lfloor n/3 \rfloor$) buckets A, B, and C.
To sample $m$ sets of three items, the items picked in set $i \in [1..m]$ are: $x_a=x_i$, $x_b=x_{s+i}$, and $x_c=x_{2s+i}$. 
This ensures that the three items shown to the user are as far apart from each other as possible, in terms of recommendation score, across all item selection tasks; see Fig.~\ref{fig:item_sampling} for a visual explanation.  The $m$ sets of items ($x_a$, $x_b$, and $x_c$) are assigned randomly to the $m$ experimental conditions.


\subsection{Generating Explanations}
\label{sec:design:explanations}

\begin{table}[t]
    \caption{Controlled generation of explanation sentiment based on the number of positive and negative aspects mentioned.}
    \vspace*{-0.5\baselineskip}
    \label{tab:aspects}
    \small
    \begin{tabular}{ |l|c|c|c|c|c| } 
        \hline
        & \textbf{Extreme} & \multirow{2}{*}{\textbf{Negative}} & \multirow{2}{*}{\textbf{Neutral}} & \multirow{2}{*}{\textbf{Positive}} & \textbf{Extreme} \\
        & \textbf{Negative} & & & & \textbf{Positive} \\
        & (Bias -{-}) & (Bias -) & (No bias) & (Bias +) & (Bias ++) \\
        \hline
        \hline
        Aspects$^+$ & 0 & 1 & 2 & 3 & 4 \\
        Aspects$^-$ & 4 & 3 & 2 & 1 & 0 \\
        \hline
    \end{tabular}
    \vspace*{-0.5\baselineskip}
\end{table}

To quantify how much explanations can influence user decisions, we need a controlled way of generating explanations which are by default neutral, but can be biased to have a more positive or negative overall sentiment.
To operationally define what it takes for an explanation to be overall neutral/positive/negative we assume that for each item $M=4$ positive and $M$ negative aspects have been identified.
An aspect in this context is a short natural language text (typically 1--5 words in length) that expresses why a movie might be liked or disliked, such as ``action-packed,'' ``quite corny and unrealistic,'' or ``classic literature brilliantly realized.''  
The sentiment of an explanation can then be controlled by adjusting how many positive and negative aspects it mentions, with Aspects$^+$\,$=$\,Aspects$^-$\,$=M/2$ representing a neutral explanation (cf.~Table~\ref{tab:aspects}).
Aspects are extracted from reviews, as detailed below in Section~\ref{sec:design:explanations:aspects}.

We consider two explanation formats: itemized and fluent natural language text.
\emph{Itemized} explanations are comprised of a list of aspects with their corresponding sentiment symbolized by a thumbs up or down icon. 
Alternatively, the \emph{fluent-NL} format presents the same information as fluent natural language description; see Fig.~\ref{fig:explanation} for an illustration.
The generation of fluent-NL explanations from a list of item aspects is detailed below in Section~\ref{sec:design:explanations:fluent_nl}.



\subsubsection{Aspect Extraction}
\label{sec:design:explanations:aspects}

Positive and negative aspects are extracted manually using crowdsourcing.  Given 10 positive (4-5 stars) and 10 negative reviews (1-2 stars), workers are asked to find aspects that complete the sentence: \emph{This movie may be liked (disliked) because it is/has/contains \_\_\_}.
The goal is not to be exhaustive, but rather focus on high data quality (i.e., favor precision over recall). 
To ensure that, extracted aspects are further checked and manually filtered by authors of the paper to remove too harsh or offensive language, aspects that are too generic (e.g., excellent, terrible) or are not about movie itself (concern price, delivery, medium, etc.).  Movies with fewer than 4 positive and 4 negative aspects after filtering are removed from the recommendation pool (31 in total).
Further details are in Appendix~\ref{app:stage2:aspects}.


\subsubsection{Fluent-NL Explanation Generation}
\label{sec:design:explanations:fluent_nl}

We turn the selected aspects into fluent natural language sentences using a large language model with state-of-the-art few-shot performance (PaLM~\citep{Chowdhery:2022:arXiv}, 62b parameter model).
A separate prompt is created for each combination of Aspects$^+$ and Aspects$^-$, containing three hand-crafted training examples.


\subsection{Participant Condition Assignment}
\label{sec:design:conditions}


The study is performed through crowdsourcing via a web-based platform.  
Each participant is presented with each of the 12 experimental conditions, shown in Table~\ref{tab:conditions}, exactly once in a random order.  In fact, there are only 11 unique conditions, as \#1 and \#2 are the same, but this ``no explanations'' setting is shown twice in order to establish a robust baseline. 
The other baseline is to show neutral explanations, in two different formats (\#3--\#4).  
In the remaining conditions (\#5-\#12) one of the three items shown to the rater is randomly selected to be ``biased,'' by changing the number of positive and negative aspects in the explanation that accompanies that item.  The explanations for the other two items stay neutral, i.e., showing exactly two positive and two negative aspects.
These conditions allow us to make comparisons between different explanation types, i.e., no explanation (\#1--\#2) vs. itemized (\#3, \#5, \#7, \#9, \#11) vs. fluent-NL (\#4, \#6, \#8, \#10, \#12) explanations, and between the amount of bias, i.e., no bias (\#1--\#4) vs. moderate (\#5--\#8) vs. large (\#9--\#12) bias.

We acknowledge that listing positive aspects first, followed by negative ones, or the other way around,  
might have an impact.  
At the same time, to avoid further complicating the design by introducing yet another dimension, we control for this variable by balancing the two possible settings, i.e., positives-first and negatives-first.
Specifically, when both positive and negative aspects are displayed (\#3--\#8), we make those fully balanced for each participant as well as across all participants by cycling through pre-defined sequences in a Latin Square-like design (see Table~\ref{tab:stage2_sequences} in Appendix~\ref{app:stage2:explanations}).
