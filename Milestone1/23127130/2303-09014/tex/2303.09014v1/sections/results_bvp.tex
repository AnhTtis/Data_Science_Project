\label{result:perf_improvement}

\begin{table*}[]
    \centering
    \small
    \begin{tabular}{l|cc|cc|c}
       \toprule
\bf       Task Name (Cluster) & \bf Few Shot & \bf AutoCot & \bf \sys & \bf \sys & \bf GPT-3 \\
       &  &  & \bf w/o Tool Use & & \bf Best\\
       \midrule
       Anachronisms \cellcolor{search}(Search) & 71.3$^5$ & 51.48 & 70.87 & 75.66 & -\\
       % \cellcolor{instructgpt}71.13$^5$\\
       Musique \cellcolor{search}(Search) & 2.03 & 12.88 & 10.04 & 19.19 &  
       15.2$^6$\\
       Hindu Knowledge \cellcolor{search}(Search) & 85.02 $^5$ & 73.03 & 83.42 & 87.98 & -\\
       % \cellcolor{instructgpt}83.65$^5$\\
       Known Unknown \cellcolor{search}(Search) & 68.90 $^5$ & 56.09 & 80.43 & 80.43 & -\\
       \cdashlinelr{1-6}
       \Gape[.5pt][.5pt]{$\Delta$ with \sys(Search)} & \bf +9.0 & \bf +8.9 & \bf +4.6 & & \\
       % + 4.0 \\
       \cdashlinelr{1-6}
       Elementary Math QA \cellcolor{arithmetic}(Arithmetic) & 56.40$^7$ & 74.52 & 58.04 & 68.04 & -\\
       % \cellcolor{codex}57.80$^7$\\
       % \cdashlinelr{1-6}
       Aqua-rat \cellcolor{arithmetic}(Arithmetic) & 20.54$^7$ & 34.41 & 36.29 & 54.20 & 54.1$^8$ \\
       % \hdashline
       GSM8K \cellcolor{arithmetic}(Arithmetic) & 7.79$^7$ & 21.99 & 53.4 & 71.00 & 71.6$^8$\\
       Navigate \cellcolor{arithmetic}(Arithmetic) & 60.7$^7$ & 61.7 & 72.4 & 72.4 & 85.90$^1$\\
       \cdashlinelr{1-6}
       $\Delta$ with \sys(Arithmetic) & \bf +30.0 & \bf +18.25 & \bf +21.85 & & \\
       % -4.6 \\
       \cdashlinelr{1-6}
       % \cellcolor{instructgpt}67.82$^5$\\
       % \textcolor{red}{76.08} & 
       K'th letter concatenation \cellcolor{string}(String) & 3.2$^5$ & 0.64 & 8.19 & 40.00 & 98.0$^3$ \\
       % \cellcolor{instructgpt}33.20$^5$ \\
       Language games \cellcolor{string}(String) & 35.14$^5$ & 18.58 & 11.19 & 23.08 & -\\ 
       Date Understanding \cellcolor{string}(String) & 37.53$^5$ & 38.90 & 52.05 &  - &  70.41$^1$ \\
       Auto Debugging \cellcolor{code}(Code) & 62.94$^5$ & 38.24 & 55.29 & 62.94 & -\\
       % \cellcolor{instructgpt}65.88$^5$\\
       Code Description \cellcolor{code}(Code) & 97.99$^7$ & 88.67 & 84.67 & 88.00 & -\\
       % \cellcolor{codex}95.33$^7$\\
       Formal Fallacies \cellcolor{freeform}(CoT) & 44.84$^5$ & 56.4 & 64.76 & - &  58.4$^1$\\
       Hyperbation  \cellcolor{freeform}(CoT) & 62.72$^5$ & 55.4 & 80.80 & - & 72.4$^1$\\
       \midrule
       $\Delta$ with \sys (Overall) & \bf +14.90 & \bf +17.17 & \bf +7.91 & & \\
       % -9.0  \\
       \bottomrule
    \end{tabular}
    \caption{NLProgrammer performance on task library.($^1$ Human-crafted CoT \citep{wei2022chain, suzgun2022challenging}, $^3$ Decomposed Prompting\citep{khot2022decomposed}, $^5$ InstructGPT \citep{ouyang2022training}, $^6$ Self-Ask \citep{press2022measuring}, $^7$ Code-davinci-002 \citep{chen2021evaluating}, $^8$ PoT \citep{chen2022program}).}
    \label{tab:main_result_table_library}
\end{table*}



\begin{table*}[htb!]
    \centering
    \small
    \begin{tabular}{l|cc|cc|c}
       \toprule
\bf       Task Name (Cluster) & \bf Few Shot & \bf AutoCot & \bf \sys & \bf \sys & \bf GPT-3 \\
       &  &  & \bf w/o Tool Use & & \bf Best\\
    \midrule
        \multicolumn{6}{c}{\bf Test Tasks} \\
       \midrule
        Sentence Ambiguity \cellcolor{search}(Search) & 70.67$^5$ & 51.47 & 71.00 & 73.33 & -\\
        % \cellcolor{instructgpt}71.67$^5$\\
        Strategy QA \cellcolor{search}(Search) & 55.49$^5$ & 27.22 & 59.37 & 66.44 & -\\
        % \cellcolor{instructgpt}62.05$^5$\\
        Physics \cellcolor{search}(Search) & 70.09$^5$ & 68.21 & 59.13 & 67.55 & -\\
        % \cellcolor{instructgpt}72.93$^5$\\

       \cdashlinelr{1-6}
       \Gape[.5pt][.5pt]{$\Delta$ with \sys(Search)} & \bf +.2 & \bf +20.14 & \bf + 3.7 & &  \\
       \cdashlinelr{1-6}
        Physics Questions \cellcolor{arithmetic}(Arithmetic)& 7.02$^5$ & 5.56 & 6.30 & 20.37 & -\\
        % \cellcolor{instructgpt}6.30$^5$\\
        Operators \cellcolor{arithmetic}(Arithmetic)& 71.23$^7$ & 75.52 & 71.80 & 92.00 &-\\
        % \cellcolor{codex}73.80$^7$\\
        Unit interpretation \cellcolor{arithmetic}(Arithmetic)& 58.2$^7$ & 41.20 & 51.4 & 53.99 & -\\
        % \cellcolor{instructgpt}61.8$^5$ \\
        Repeat copy logic \cellcolor{arithmetic}(Arithmetic) & 50.01$^7$ & 15.63 & 31.25 & 44.38 & -\\
        % \cellcolor{codex}50.62$^7$\\
        % \cdashlinelr{1-6}
        % Symbol Interpretation & 30.56 & 23.08 &  &\\
        Object Counting \cellcolor{arithmetic}(Arithmetic) & 39.2$^7$ & 26.80 & 42.2 & 87.00 & 81.20$^1$\\
        Penguins in a table \cellcolor{arithmetic}(Arithmetic) & 58.23$^7$ & 40.40 & 68.86 & 77.85 &  72.34$^1$\\
        Reasoning about objects \cellcolor{arithmetic}(Arithmetic) & 71.00$^7$ & 33.33 & 45.35 & 64.34 & 52.69$^1$ \\
        Tracking shuffled objects \cellcolor{arithmetic}(Arithmetic) & 22.39$^7$ & 19.44 & 18.14 & 37.67 &  36.32$^1$ \\
       \cdashlinelr{1-6}
       \Gape[.5pt][.5pt]{$\Delta$ with \sys (Arithmetic)} & \bf +12.5 & \bf +27.5 & \bf + 17.8 & & +6.1\\
       % -5.40 \\
       \cdashlinelr{1-6}
        Word Unscramble \cellcolor{string}(String) & 40.72$^7$ & 32.44 & 23.03 & 42.7 & -\\
        % \cellcolor{codex}48.20$^7$\\
        Simple Text Editing \cellcolor{code}(Code) & 35.31$^5$ & 30.21 & 20.74 & 27.65 & -\\
        % \cellcolor{instructgpt}36.60$^5$\\
        CS Algorithms \cellcolor{code}(Code) & 73.48$^7$ & 0.0 & 41.59 & 88.11 & -\\
        % \cellcolor{codex}76.82$^7$\\
        Sports Understanding \cellcolor{freeform}(CoT) & 69.74$^5$ & 51.47 & 92.89 & - & 86.59$^1$\\
        Snarks \cellcolor{freeform}(CoT) & 54.58$^5$ & 57.24 & 57.13 & - &  65.2$^1$\\
        Disambiguation QA \cellcolor{freeform}(Free-form) & 55.03$^5$ & 48.45 & 55.89 & - & 60.62$^1$\\
        Temporal sequences \cellcolor{freeform}(CoT) & 55.80$^7$ & 19.70 & 49.5 & - & 81.8$^1$\\
        Ruin names \cellcolor{freeform}(CoT) & 71.01$^5$ & 55.28 & 60.22 & - & -\\
        % \cellcolor{instructgpt}75.51$^5$\\
        \midrule
        $\Delta$ with \sys(Overall) & \bf +6.9 & \bf +24.2 & \bf +12.3 & & \\
        % -7.73  \\
        \midrule
        \multicolumn{6}{c}{\bf MMLU} \\
        \midrule
        College Computer Science \cellcolor{search}(Search)& 41.00 & 43.99 &  63.40 & 67.80 & 63.6$^4$\\
        Astronomy \cellcolor{search}(Search)& 62.10 & 41.48 & 76.71 & 79.1 & 62.5$^4$\\
        Business Ethics \cellcolor{search}(Search)& 61.60 & 48.8 & 77.17 & 81.16 & 72.7$^4$\\
        Virology \cellcolor{search}(Search)&  50.03 & 49.52 & 71.60 & 70.67 & 50.72$^4$\\
        Geography \cellcolor{search}(Search)& 77.67 & 57.07 & 70.30 & 70.67 &  81.8$^4$\\
        Mathematics \cellcolor{arithmetic}(Arithmetic) & 36.67 & 33.77  & 39.50 & 45.66 &  34.5$^4$\\
        \midrule
        $\Delta$ with \sys(MMLU) & +14.3 & +25.71 & +2.04 & & \\
        % +4.6 \\
       \bottomrule
    \end{tabular}
    \caption{NLProgrammer performance on BigBench tasks and MMLU tasks. ($^1$ Human-crafted CoT \citep{wei2022chain, suzgun2022challenging}, $^3$ Decomposed Prompting\citep{khot2022decomposed}, $^4$ Scaled instruction finetuning \citep{chung2022scaling}, $^5$ InstructGPT \citep{ouyang2022training}, $^6$ Self-Ask \citep{press2022measuring}, $^7$ Code-davinci-002 \citep{chen2021evaluating}, $^8$ PoT \citep{chen2022program}).}
    \label{tab:main_result_table_test}
\end{table*}


We report performance of \sys, baselines, and GPT-3 best, on all library tasks (in Table~\ref{tab:main_result_table_library}) and test tasks in BigBench and six MMLU tasks (in Table~\ref{tab:main_result_table_test}). 
Tasks are arranged by the cluster of library tasks used as demonstrations for prompting, which provides us insights about skill-wise performance gains on \sys.
% (approaches with tool use (green), and approaches with supervision for CoT reasoning (yellow). 
We compare and discuss the performance of \sys for library tasks (Section~\ref{sub:results_seed_tasks}), test tasks (Section~\ref{sub:results_test_tasks}), and MMLU tasks (Section~\ref{sub:results_mmlu}).

\subsection{Results on Library tasks}
\label{sub:results_seed_tasks}

% Setting up expectations
For library tasks (training tasks), we use the corresponding "gold" task-cluster programs as prompt demonstrations (e.g. For anachronisms, we create a multi-task prompt containing task programs from anachronisms, Hindu knowledge, and Musique~\ref{appendix:tasklibrary}).
Specifically, the prompt to \sys contains supervision for 2 instances of the task (along with other similar tasks).

% Where we win.
% Cat 1: Search, % Cat 2: Arithmetic, % Overall: 
% Even though \sys uses more labeled examples, its not using as much supervision for reasoning at all. 
\sys improves performance over few-shot prompting by 14.90 (\% points) on average, while also providing intermediate reasoning for its predictions with minimal supervision for reasoning and tool use (for only up to 2 instances of the task). 
In Particular, \sys improves significantly on tasks that require search (+9.0), complex arithmetic calculations (+30), and CoT-style reasoning (+18). 
On the other hand, it lags behind on language games, code description, and auto debugging --- tasks that use code generation and/or code editing models. Code generation errors (i.e. the Codex tool output) can lead to cascading errors in reasoning.


\sys improves or matches the best-known GPT3 performance with decomposition/tool-use supervision 
% on 5 of the 8 tasks across different skill categories, 
on all the tasks where we use the \emph{same} tool or CoT reasoning as prior work.
On k'th letter concatenation, \sys underperforms \citet{khot2022decomposed}, which recursively decomposes the task into finding the k'th letter in every word and prompts the LLM for each subtask with \emph{additional} supervision.
Similarly, for navigation and date-understanding, \citet{suzgun2022challenging} use a different CoT-style reasoning chain, which is more effective than the programs we author. We explore more human feedback to debug our initial reasoning prompt and improve the performance of \sys for these tasks in Section~\ref{appendix:human_feedback}.

% General comments on AutoCOT and Tool use itself (which is common across categories) 
\sys improves performance over automatically generated CoT-style reasoning by 17 \% pts on average, with improvements in all tasks except elementary math QA. Other multi-step arithmetic tasks (GSM8K, Aqua-rat) are more challenging for the CoT baseline, as more complex intermediate calculations are often incorrect.
Notably, \sys with tool-use turned off also gains 8 \% pts over this baseline. We hypothesize that the query language (and PeG grammar) is better at eliciting multi-step reasoning from models than free-form CoT due to the added structure to the reasoning.
Tools are called for 95\% of the test instances on average, and using tools improves performance by 7.9 \% points, averaged over tasks. Gains from tool use are particularly significant for arithmetic tasks that benefit from representing the arithmetic problem as code that can be executed to compute complex arithmetic accurately (+21.85 on average). This has also been noted in prior work \citep{chen2022program, gao2022pal}.


\subsection{Big Bench Test Tasks}
\label{sub:results_test_tasks}

% Setting up expectations
We \emph{select} a few related task programs from the task library as demonstrations. We select tasks from the task cluster with the best performance on a small number($\approx$50) of labeled examples.
This strategy has higher performance and low variance on average compared to using few-shot prompting of LLMs to select ``Similar tasks"(discussed in more detail in Section~\ref{appendix:taskselection}). The resultant best-performing cluster is also specified in Table~\ref{tab:main_result_table_library}. We note that while the task selection strategy still uses some labeled input-output pairs of the task to select related tasks in the prompt, no additional supervision is required for multi-step decompositions or tool use. 
The performance of \sys on test tasks helps us evaluate the cross-task generalizability of the framework.
% The cluster of string tasks for which our programs used limited code generation and execution are not used for string manipulation tasks like simple text editing and repeat copy logic, and code and arithmetic task clusters are used instead.

% Few-shot : (by cat)
\sys improves over the performance of few-shot prompting by 7 \% points on average. 
While \sys uses more input-output pairs for task selection compared to few-shot prompting, \sys provides intermediate reasoning steps at no additional supervision cost, which can be leveraged for further debugging by humans (Section~\ref{human_in_loop})
In particular, \sys has significant improvements on arithmetic tasks (+12.6) and is comparable to the few-shot performance on search tasks.
\sys underperforms on the multiple-choice tasks --- temporal sequences and ruin names. Non-grammatical choices are often incorrect in ruin names and choices not mentioned in the input are often correct in temporal sequences. 
We hypothesize that the few-shot baseline in these tasks may potentially learn to ignore irrelevant choices easily, while \sys attempts to explicitly reason about them. 
String manipulation tasks like simple text editing, word unscrambling, and repeat copy logic also suffer from code generation errors---generated code snippets that are used to edit the string are incorrect in 49\% of the cases. We explore human feedback to debug programs and improve the performance of \sys for these tasks in Section~\ref{sub:human-in-the-loop}.

% Where do we loose (by cat)?
On arithmetic reasoning tasks, \sys improves over the best-known performance with decomposition/tool-use supervision (for GPT3, 175B model size) by 6.1 \% points on average. On these tasks, \citet{suzgun2022challenging} uses human supervision for CoT-style reasoning. \sys requires no such supervision, generates multi-step reasoning containing code snippets, and invokes code execution tools to improve performance. 
\sys lags behind by a large margin on sports understanding and temporal sequences. We hypothesize that the programs generated for these tasks have sub-steps with high error or a missing reasoning step which humans may be able to fix given their supervision helps in \citet{suzgun2022challenging}. We explore human feedback to debug and improve the performance of \sys for these tasks in Section~\ref{sub:human-in-the-loop}.



% General comments on AutoCOT and Tool use itself (which is common across categories) 
\sys improves performance over automatically generated CoT reasoning by 24 \% pts on average, with significant improvements in all tasks except simple text editing. For this task, the code generated to edit text has a high error rate (49\% of instances). \sys with tool-use turned off also gains 12 \% pts over this baseline, once again underscoring the effectiveness of the structured multi-step reasoning of \sys. On average, tools are called for 89\% of the test instances, and using tools improves performance by 12.3 \% points on average. Gains from tool use are once again particularly significant for algorithmic and arithmetic tasks like CS algorithms, word unscrambling, reasoning about colored objects, tracking shuffled objects, and object counting.






 \begin{table*}[htb!]
    \centering
    \small
    \begin{tabular}{l|cccccc}
    \toprule
      & \bf Simple Text  & \bf CS  & \bf Strategy QA &\bf  Physics & \bf Unit  & \bf Reasoning about \\
     & \bf Editing & \bf Algorithms & &\bf  Questions & \bf Interpretation & \bf colored objects \\
     \midrule
    \bf \sys & 27.65 & 88.11 & 66.44 & 20.37 & 53.99 & 64.34 \\
    % \midrule
    % (with tool use) &&&&& \\
    % \bf LLM-based task sim. & 38.30 & 83.71 & 60.39 & 14.06 & 43.56 & 62.00 \\
    \bf Self Consistency & 30.67(+3.0) & 90.99(+2.9) & 70.76(+4.3) & 24.07(+3.7) & 57.20(+3.2) & 69.11(+4.8) \\
    \bottomrule
    % \midrule
    % \multicolumn{7}{c}{Improved instruction finetuned models} \\
    % \midrule
    % % GPT-J &&&&& \\
    % % Toolformer &&&&& \\
    % T5 &&&&& \\
    % FLAN-T5  &&&&& \\
    \end{tabular}
    \caption{Replacing task-cluster based task similarity strategy in \sys with LLM-based similarity. Adding model-ensembling \citep{wang2022self} further boosts performance.}
    \label{tab:self_consistency}
    \begin{tabular}{l|cccccc}
    \midrule
     &  \bf SQuAD &  \bf T-REx & \bf SVAMP & \bf MAWPS & \bf NQ & \bf TriviaQA \\
    \midrule
    \bf GPT3 (175B) & 29.90 & 39.8 & 10.0 & 19.8 & 22.6 & 65.9 \\
    \bf Toolformer & 33.8  & 53.5 & 29.4 & 44.0 & 17.7 & 48.8  \\
    \bf \sys & 39.34(+5.5) & 50.4(-3.1) & 76.2(+46.8) & 71.00(+27) & 33.8(+16.1) & 66.13(+17.33) \\
    \bottomrule
    \end{tabular}
    \caption{Comparing \sys results on GPT3 (175B) model and \citep{schick2023toolformer}, which is a smaller GPT-J model finetuned for tool-use. Results are reported from their paper (their code and models are not publicly available).}
    \label{tab:ablations}
\end{table*}


% \subsection{Benefits from tool use}
\subsection{Performance on other benchmarks}
\label{sub:results_mmlu}
In Table \ref{tab:main_result_table_test}, we report performance on a small randomly chosen subset of tasks from the MMLU \citep{hendrycks2020measuring} benchmark. 
% In Table \ref{tab:ablations}, we also report the performance of \sys on tasks used in the Toolformer approach. 
These results help us understand if composing the task library with BigBench tasks limits the performance of the framework on other benchmarks and tasks.
\sys is better on average than few-shot prompting (14 avg. \% points) and automatic CoT-style reasoning approaches (25.7 avg. \% points) on the chosen MMLU tasks. Search is the most commonly used tool for these tasks since the benchmark is designed to require extensive world knowledge. For instance, the gains of \sys over CoT-style reasoning are especially noteworthy in tasks like astronomy and business ethics --- tasks requiring specialized knowledge that benefit from access to a search engine. 
\sys also improves over the best-known GPT-3 results for these tasks from \cite{chung2022scaling} except for the high school geography task.
% A similar pattern holds for SQuAD, TriviaQA, MAWPS, SVAMP, and NQ in Table~\ref{tab:ablations}. 
These tasks are still "in-domain" with respect to our choice of reasoning skills and tools. Tasks requiring a completely new skill, like machine translation, may prove to be challenging for the current task and tool library catalog. However, we hypothesize that these libraries can be expanded to support additional tool use and reasoning skills in the future.

