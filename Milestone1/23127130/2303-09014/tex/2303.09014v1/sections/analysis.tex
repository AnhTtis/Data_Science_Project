% \subsection{Self Consistency}


\begin{table*}[]
    \centering
    \small
    \begin{tabular}{l|cc|cc|c|l}
    \toprule
    % &  \multicolumn{2}{c}{Human-in-the-loop improvement} \\
    % \midrule
     \bf Task & \multicolumn{2}{c}{\bf CoT}  & \multicolumn{2}{c}{\bf \sys} &  \bf GPT-3 & \bf Human  \\
      % &\multicolumn{2}{c}{CoT} & \bf & \\
      & & \bf +Human & \bf & \bf + Human & \bf Best & \bf Feedback \\
     \midrule
     % Kth letter concatenation* & 59.40 & 100.0 & 59.40 & 100.0 & Code C: k'th letter extraction and merge for a list of words \\
     % Language Games* & 26.08 & 35.38 & 26.08 & 35.38 & Code  C: Eng->Pig Latin and vice-versa\\
     % Anachronisms*   & 49.82 & 82.91 & 49.82 & 82.91 & C: search query constrained to extract time-periods \\
     % & & & of the entities \\
     % Auto Debugging* & 61.18 & 67.05 & 61.18 & 67.05 & Code C: Code edit fixed to print variable asked in input \\
     % & & & & & A: ``[generate answer] What is the final error message?'' \\
     % Navigate & 85.9 & 80.89 & 85.9 & 80.89 & Code C: correct forward, backward, right, left distances\\
     % Date Understanding & 70.4 & 65.45 & 70.4 & 65.45 & A: First find what date is today \\
     % \midrule
     CS Algorithms & 0.0 & 23.0 & 88.11 & 92.73 & 73.48 & C: longest common subsequence code\\
     Colored objects  & 33.33  & 67.75 & 64.34 & 98.90 & 71.00 & C: Define object, color, count data structure\\
     Repeat Copy Logic*   & 15.63 & 45.22 & 44.38 & 80.31 & 50.01 & C: string edit operation \\
     Sentence Ambiguity   & 51.47 & 72.33 & 73.33 & 83.67 & 70.67  & C: Constrain queries to extract relevant info.\\
     % &&&&& A: ``[subquestion] Is the claim true, given this information?''\\
     Simple Text editing*  & 30.21 & 35.31 & 27.65 & 36.11 & 35.31  &  C: string edit operation \\
     Strategy QA*   & 27.22 & 29.19 & 66.44  & 69.15 & 55.49 & C: Constrain queries to extract relevant info.\\
    % colored objects &  &  \\
     Physics* & 61.83  & 68.21 & 67.55 & 77.55 & 70.09   & A: [search] Physics formula that relates mass, ...\\
     Temporal Sequences & 19.70 & 30.22 & 30.22 & 88.00 & 81.8  & A: [subquestion] Is X free Yam to Zam? \\
     % for every option.\\
     Shuffled objects & 19.44 & 36.48 & 37.67 & 99.86 & 36.32  & C: Define object pair data struct and swap logic\\
     Unit Interpretation* & 41.2  & 41.2  & 53.99 & 95.0 & 58.2   & A: [add unit] Add the right unit to the answer\\
     Word Unscrambling*  & 32.44 &  33.40 & 42.70 & 62.11 & 40.72 & T: lookup permutations in dictionary\\
     \midrule
     Average & 30.2 & \bf 43.8 & 56.0 & \bf 80.3 & 58.5  \\
    \bottomrule
    \end{tabular}
    \caption{Improving \sys and free-form CoT via human feedback (*) indicates that human-in-the-loop improvement was done over automatically generated CoT reasoning for these tasks.}
    \label{tab:model_improvements}
\end{table*}

\subsection{Self-consistency}
\label{sub:self-consistency}
% 3.65 pts over \sys
Self-consistency~\cite {wang2022self} --- where the LLM generates N diverse outputs with top-p sampling~\citep{holtzman2019curious}, and the most frequent answer is used --- can help improve performance in prompting approaches that generate multi-step reasoning, as generating multiple outputs can help counter potential cascading errors in reasoning. Prior work on CoT prompting and multi-step reasoning \citep{khot2022decomposed, wang2022self} has noted improvements from using self-consistency. To study the interplay of tool use and self-consistency in \sys, we select a subset of tasks from different clusters that require tool use.
% .to study improvements to \sys from self-consistency.
Table~\ref{tab:self_consistency} reports results on using self-consistency with \sys, with a performance boost of 3.6 \% points on average for the selected subset of tasks. 
% This is in line with observations from prior work that combine CoT prompting with self-consistency \citep{khot2022decomposed, wang2022self}.



\subsection{Human Feedback}
\label{sub:human-in-the-loop}
% 13.11611111 gain over SOTA and only doesnt work in navigate and date understanding.
% 21.33777778 over other auto CoT. Improve out programs instead of autoCot
% Programmatic tasks (arithmetic and algorithmic) have the largest gains (tracking, reasoning abt colored aobjects, repeat copy logic, kth letter, auto debugging)
% Explain how the baseline gets its improvements.

% Humans are provided two affordances with the goal of improving end-task performance --- (a) Program editing to extend the task library; and (b) Improving tool definition and integration to extend the tool library.

Humans are shown 5 random instances of model errors and the corresponding programs generated for these instances. They can edit these programs to correct errors in sub-step command arguments and answers, as well as add additional sub-steps (with arguments and correct answers) to fill-in potentially missing reasoning steps. Humans can also add additional tools in the tool library by specifying its arguments, implementation, and how output is integrated into the incomplete program. Human feedback can be obtained over several rounds, with task performance change at the end of each round informing their feedback in future rounds. 
We study human feedback in a limited setting in this work. Authors either rewrite programs in text files or add additional tools programmatically to integrate them into the parser. Only two rounds of feedback are obtained. 
In Appendix~\ref{appendix:human_feedback}, we describe the different edits and augmentations made by humans for 18 tasks for which human feedback was obtained.


Table~\ref{tab:model_improvements} reports the performance improvements from human feedback on a subset of test tasks from Table~\ref{tab:main_result_table} that underperform the best-known performance on GPT-3, either few-shot prompting or using supervision for reasoning and/or tool use. We also report performance improvements on library tasks in Appendix~\ref{appendix:human_feedback}. 
We briefly describe the changes made by human feedback in Table~\ref{tab:model_improvements}, which include correcting sub-steps in programs (\exinline{C:}), adding additional sub-steps(\exinline{A:}), and defining new tools(\exinline{T:}).


Humans correct sub-step arguments and outputs in 72\% of the chosen tasks and add additional sub-steps in 44\% of the tasks, with three tasks undergoing more than one type of change. On average, humans edit 15.7\% of the tokens in the program, measured as the average edit distance between the original and edited program. Humans add additional tool definitions for two tasks --- dictionary lookup for word unscrambling and Prolog engine for formal fallacies.
% The programs resulting from human-in-the-loop feedback are used for prompting the LLM. 
The edited and/or augmented programs for the five debugging instances are used for prompting the LLM.
This feedback results in a 26.4 average \% point increase in performance from before, and a 19.9 average \% point improvement in performance over the best-known results for GPT-3.
Algorithmic and arithmetic tasks that use the code generation tool especially benefit from human correction of generated code in the program.

We also compare human feedback applied to CoT-style reasoning. \citet{suzgun2022challenging} already provide reference CoT-style reasoning for some tasks. For datasets where  human-authored CoT reasoning in unavailable, we correct the output of the automatic CoT baseline, as indicated in Table~\ref{tab:model_improvements}. As before, we randomly select 5 instances of model errors for this approach. Humans are allowed to edit intermediate reasoning generated by the model for correctness and add more intermediate reasoning that leads to the correct answer. On average, humans edit 35\% of the tokens in the original program. \sys is able to improve performance over \emph{corrected} CoT-style reasoning by 21 \% points on average. We find that \sys enables easy human intervention and improvement of the reasoning process by simply updating the sub-tasks and tool libraries, making it easier to improve performance on any specific task with minor human feedback. Furthermore, CoT reasoning is less amenable to the redefinition of tool use or the addition of new tools.