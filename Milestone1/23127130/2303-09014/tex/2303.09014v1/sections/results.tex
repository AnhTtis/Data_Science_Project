\label{result:perf_improvement}

% \begin{table*}[]
%     \centering
%     \small
%     \begin{tabular}{l|cc|cc|c}
%        \toprule
% \bf       Task Name (Cluster) & \bf Few Shot & \bf AutoCot & \bf \sys & \bf \sys & \bf GPT-3 \\
%        &  &  & \bf w/o Tool Use & & \bf Best\\
%        \midrule
%        Anachronisms \cellcolor{search}(Search) & 71.3$^5$ & 51.48 & 70.87 & 75.66 & -\\
%        % \cellcolor{instructgpt}71.13$^5$\\
%        Musique \cellcolor{search}(Search) & 2.03 & 12.88 & 10.04 & 19.19 &  
%        15.2$^6$\\
%        Hindu Knowledge \cellcolor{search}(Search) & 85.02 $^5$ & 73.03 & 83.42 & 87.98 & -\\
%        % \cellcolor{instructgpt}83.65$^5$\\
%        Known Unknown \cellcolor{search}(Search) & 68.90 $^5$ & 56.09 & 80.43 & 80.43 & -\\
%        \cdashlinelr{1-6}
%        \Gape[.5pt][.5pt]{$\Delta$ with \sys(Search)} & \bf +9.0 & \bf +8.9 & \bf +4.6 & & +4.0\\
%        % + 4.0 \\
%        \cdashlinelr{1-6}
%        Elementary Math QA \cellcolor{arithmetic}(Arithmetic) & 56.40$^7$ & 74.52 & 58.04 & 68.04 & -\\
%        % \cellcolor{codex}57.80$^7$\\
%        % \cdashlinelr{1-6}
%        Aqua-rat \cellcolor{arithmetic}(Arithmetic) & 20.54$^7$ & 34.41 & 36.29 & 54.20 & 54.1$^8$ \\
%        % \hdashline
%        GSM8K \cellcolor{arithmetic}(Arithmetic) & 7.79$^7$ & 21.99 & 53.4 & 71.00 & 71.6$^8$\\
%        Navigate \cellcolor{arithmetic}(Arithmetic) & 60.7$^7$ & 61.7 & 72.4 & 72.4 & 85.90$^1$\\
%        \cdashlinelr{1-6}
%        $\Delta$ with \sys(Arithmetic) & \bf +30.0 & \bf +18.25 & \bf +21.85 & & -4.7 \\
%        % -4.6 \\
%        \cdashlinelr{1-6}
%        % \cellcolor{instructgpt}67.82$^5$\\
%        % \textcolor{red}{76.08} & 
%        K'th letter concatenation \cellcolor{string}(String) & 3.2$^5$ & 0.64 & 8.19 & 40.00 & 98.0$^3$ \\
%        % \cellcolor{instructgpt}33.20$^5$ \\
%        Language games \cellcolor{string}(String) & 35.14$^5$ & 18.58 & 11.19 & 23.08 & -\\ 
%        Date Understanding \cellcolor{string}(String) & 37.53$^5$ & 38.90 & 52.05 &  - &  70.41$^1$ \\
%        Auto Debugging \cellcolor{code}(Code) & 62.94$^5$ & 38.24 & 55.29 & 62.94 & -\\
%        % \cellcolor{instructgpt}65.88$^5$\\
%        Code Description \cellcolor{code}(Code) & 97.99$^7$ & 88.67 & 84.67 & 88.00 & -\\
%        % \cellcolor{codex}95.33$^7$\\
%        Formal Fallacies \cellcolor{freeform}(CoT) & 44.84$^5$ & 56.4 & 64.76 & - &  58.4$^1$\\
%        Hyperbation  \cellcolor{freeform}(CoT) & 62.72$^5$ & 55.4 & 80.80 & - & 72.4$^1$\\
%        \midrule
%        $\Delta$ with \sys (Overall) & \bf +14.90 & \bf +17.17 & \bf +7.91 & & -9\\
%        % -9.0  \\
%        \bottomrule
%     \end{tabular}
%     \caption{NLProgrammer performance on tasks in the task library.($^1$ Human-crafted CoT \citep{wei2022chain, suzgun2022challenging}, $^3$ Decomposed Prompting\citep{khot2022decomposed}, $^5$ InstructGPT \citep{ouyang2022training}, $^6$ Self-Ask \citep{press2022measuring}, $^7$ Code-davinci-002 \citep{chen2021evaluating}, $^8$ PoT \citep{chen2022program}).}
%     \label{tab:main_result_table_library}
% \end{table*}



\begin{table*}[htb!]
    \centering
    \small
    \begin{tabular}{l|cc|cc|c}
       \toprule
\bf       Task Name (Cluster) & \bf Few Shot & \bf AutoCot & \bf \sys & \bf \sys & \bf GPT-3 \\
       &  &  & \bf w/o Tool Use & & \bf Best\\
    \midrule
        \multicolumn{6}{c}{\bf Test Tasks} \\
       \midrule
        Sentence Ambiguity \cellcolor{search}(Search) & 70.67$^5$ & 51.47 & 71.00 & 73.33 & -\\
        % \cellcolor{instructgpt}71.67$^5$\\
        Strategy QA \cellcolor{search}(Search) & 55.49$^5$ & 27.22 & 59.37 & 66.44 & -\\
        % \cellcolor{instructgpt}62.05$^5$\\
        Physics \cellcolor{search}(Search) & 70.09$^5$ & 61.83 & 59.13 & 67.55 & -\\
        % \cellcolor{instructgpt}72.93$^5$\\

       \cdashlinelr{1-6}
       \Gape[.5pt][.5pt]{$\Delta$ with \sys(Search)} & \bf +3.7 & \bf +22.27 & \bf + 5.9 & &  \\
       \cdashlinelr{1-6}
        Physics Questions \cellcolor{arithmetic}(Arithmetic)& 7.02$^5$ & 5.56 & 6.30 & 20.37 & -\\
        % \cellcolor{instructgpt}6.30$^5$\\
        Operators \cellcolor{arithmetic}(Arithmetic)& 71.23$^7$ & 75.52 & 71.80 & 92.00 &-\\
        % \cellcolor{codex}73.80$^7$\\
        Unit interpretation \cellcolor{arithmetic}(Arithmetic)& 58.2$^7$ & 41.20 & 51.4 & 53.99 & -\\
        % \cellcolor{instructgpt}61.8$^5$ \\
        Repeat copy logic \cellcolor{arithmetic}(Arithmetic) & 50.01$^7$ & 15.63 & 31.25 & 44.38 & -\\
        % \cellcolor{codex}50.62$^7$\\
        % \cdashlinelr{1-6}
        % Symbol Interpretation & 30.56 & 23.08 &  &\\
        Object Counting \cellcolor{arithmetic}(Arithmetic) & 39.2$^7$ & 26.80 & 42.2 & 87.00 & 81.20$^1$\\
        Penguins in a table \cellcolor{arithmetic}(Arithmetic) & 58.23$^7$ & 40.40 & 68.86 & 77.85 &  72.34$^1$\\
        Reasoning about objects \cellcolor{arithmetic}(Arithmetic) & 71.00$^7$ & 33.33 & 45.35 & 64.34 & 52.69$^1$ \\
        Tracking shuffled objects \cellcolor{arithmetic}(Arithmetic) & 22.39$^7$ & 19.44 & 18.14 & 37.67 &  36.32$^1$ \\
       \cdashlinelr{1-6}
       \Gape[.5pt][.5pt]{$\Delta$ with \sys (Arithmetic)} & \bf +19.0 & \bf +36.7 & \bf + 23.1 & & \bf +6.1\\
       % -5.40 \\
       \cdashlinelr{1-6}
        Word Unscramble \cellcolor{string}(String) & 40.72$^7$ & 32.44 & 23.03 & 42.7 & -\\
        % \cellcolor{codex}48.20$^7$\\
        Simple Text Editing \cellcolor{code}(Code) & 35.31$^5$ & 30.21 & 20.74 & 27.65 & -\\
        % \cellcolor{instructgpt}36.60$^5$\\
        CS Algorithms \cellcolor{code}(Code) & 73.48$^7$ & 0.0 & 41.59 & 88.11 & -\\
        % \cellcolor{codex}76.82$^7$\\
        Sports Understanding \cellcolor{freeform}(CoT) & 69.74$^5$ & 51.47 & 92.89 & - & 86.59$^1$\\
        Snarks \cellcolor{freeform}(CoT) & 54.58$^5$ & 57.24 & 57.13 & - &  65.2$^1$\\
        Disambiguation QA \cellcolor{freeform}(Free-form) & 55.03$^5$ & 48.45 & 55.89 & - & 60.62$^1$\\
        Temporal sequences \cellcolor{freeform}(CoT) & 55.80$^7$ & 19.70 & 49.5 & - & 81.8$^1$\\
        Ruin names \cellcolor{freeform}(CoT) & 71.01$^5$ & 55.28 & 60.22 & - & -\\
        % \cellcolor{instructgpt}75.51$^5$\\
        \cdashlinelr{1-6}
       \Gape[.5pt][.5pt]{$\Delta$ with \sys (Misc)} & \bf 2.4 & \bf 22.5 & \bf 24.37 &  &  -9.4 \\
       % -5.40 \\
       \cdashlinelr{1-6}
        % \midrule
        $\Delta$ with \sys(Overall) & \bf +6.9 & \bf +24.6 & \bf +16.7 & & -1.7\\
        \midrule
        \multicolumn{6}{c}{\bf MMLU} \\
        \midrule
        College Computer Science \cellcolor{search}(Search)& 41.00 & 43.99 &  63.40 & 67.80 & 63.6$^6$\\
        Astronomy \cellcolor{search}(Search)& 62.10 & 41.48 & 76.71 & 79.1 & 62.5$^6$\\
        Business Ethics \cellcolor{search}(Search)& 61.60 & 48.8 & 77.17 & 81.16 & 72.7$^6$\\
        Virology \cellcolor{search}(Search)&  50.03 & 49.52 & 71.60 & 71.49
 & 50.72$^6$\\
        Geography \cellcolor{search}(Search)& 77.67 & 57.07 & 70.30 & 71.71 &  81.8$^6$\\
        Mathematics \cellcolor{arithmetic}(Arithmetic) & 36.67 & 33.77  & 39.50 & 45.66 &  34.5$^6$\\
        \midrule
        $\Delta$ with \sys(MMLU) & \bf +14.6 & \bf+23.7 & \bf+3.0 & & \bf+8.5\\
        % +4.6 \\
       \bottomrule
    \end{tabular}
    \caption{\sys performance on BigBench tasks and MMLU tasks. ($^1$ Human-crafted CoT \citep{wei2022chain, suzgun2022challenging}, $^5$ InstructGPT \citep{ouyang2022training}, $^6$ Scaled instruction finetuning \citep{chung2022scaling}, $^7$ Code-davinci-002 \citep{chen2021evaluating}).}
    \label{tab:main_result_table_test}
\end{table*}

 \begin{table*}[htb!]
    \centering
    \small
    \begin{tabular}{l|cccccc}
    \midrule
     &  \bf SQuAD &  \bf T-REx & \bf SVAMP & \bf MAWPS & \bf NQ & \bf TriviaQA \\
    \midrule
    \bf GPT3 (175B) & 29.90 & 39.8 & 10.0 & 19.8 & 22.6 & 65.9 \\
    \bf Toolformer & 33.8  & 53.5 & 29.4 & 44.0 & 17.7 & 48.8  \\
    \bf \sys & 39.34(+5.5) & 50.4(-3.1) & 76.2(+46.8) & 71.00(+27.0) & 33.8(+16.1) & 66.13(+17.33) \\
    \bottomrule
    \end{tabular}
    \caption{Comparing \sys results on GPT3 (175B) model and \citep{schick2023toolformer}, which is a smaller GPT-J model finetuned for tool-use. Results are reported from their paper (their code and models are not publicly available).}
    \label{tab:toolformer}
\end{table*}


We evaluate \sys (without human feedback) on tasks in the task library (\ref{sub:results_seed_tasks}), and on a variety of test tasks from BigBench, MMLU, and QA benchmarks (\ref{sub:results_test_tasks}).
Then, we show that \sys can be further improved with more compute (self-consistency) and with human feedback (\ref{sub:improving}).
% (approaches with tool use (green), and approaches with supervision for CoT reasoning (yellow). 

\subsection{Results on the task library}
\label{sub:results_seed_tasks}
% - We beat few shot despite having slightly more annotations, in line with prior work on CoT. 
% - Beat AutoCoT in all tasks except math QA. Better on almost all even without tool use, which indicates our program format is helpful.
% - Tools provide a non trivial gain, especially in arithmetic. Tool use: 95\% of cases. Tools are useful.
For tasks in the task library, demonstrations in the prompt include two instances of the task itself, along with other instances from tasks in the same cluster.
We present results in Table \ref{tab:main_result_table_library}, where tasks are organized by skill cluster.
Even with decomposition demonstrations for only two instances, \sys drastically improves performance over few-shot learning (+14.9 \% points on average), in line with prior work on CoT. It does not do as well on language games, code description, and auto debugging --- tasks that use code generation and/or code editing models. We observe that code generation errors often lead to cascading errors in reasoning.

Similarly, \sys outperforms AutoCoT on most tasks even without any tool use (by 8\% points on average).
We hypothesize that the program format (and PeG grammar) is better at eliciting multi-step reasoning from models than free-form CoT due to the added structure to the reasoning.
% which indicates that our \emph{program} format is helpful.
When tool use is turned on, \sys outperforms AutoCoT on all tasks (+17.7 \% points) minus one.
Tools are called in $\approx 95\%$ of test instances, and significantly improve performance (+7.91 \% points).
Gains from tool use are particularly significant for arithmetic tasks that benefit from representing the arithmetic problem as code that executes complex arithmetic accurately (+21.85 on average). This has also been noted in prior work \citep{chen2022program, gao2022pal}.

Compared to the best published GPT-3 results, \sys is stronger or comparable in 5/8 tasks.
For the others, further investigation indicates that the demonstrations provided by \citet{khot2022decomposed} and \citet{suzgun2022challenging} are just more effective than the two programs we author for these tasks (we explore further human feedback for these in Appendix \ref{appendix:human_feedback}).
In sum, \sys is stronger than few-shot learning and AutoCoT on the library tasks (where we provide $2$ labeled decompositions), and comparable to the best published GPT-3 results.

\vspace{-7pt}
\subsection{Test tasks (cross-task transfer)}
\label{sub:results_test_tasks}
% Setting up expectations
We measure cross-task generalization on test tasks where \sys does not use explicit supervision for decomposition and tool use.
\sys retrieves demonstrations from the task library according to the first strategy in Section \ref{sec:seed_tasks}, which uses a small amount of labeled input-output pairs to pick a task cluster and sample demonstration programs from that cluster.\footnote{We compare both strategies in Appendix~\ref{appendix:taskselection}}

\paragraph{BigBench test tasks} Even though there is no decomposition or tool use supervision, the results in Table \ref{tab:main_result_table_test} are similar to those for tasks in the task library.
\sys outperforms few-shot learning (6.9 \% points). In particular, \sys has significant improvements on arithmetic tasks (+19.0) and is comparable to the few-shot performance on search tasks.
Non-grammatical choices in \emph{ruin names} and choices not in the input in \emph{temporal sequences} are often incorrect, which the few-shot baseline may potentially learn to ignore, while \sys attempts to explicitly reason about them. 
As with library tasks, we observe that string manipulation tasks like simple text editing, word unscrambling, and repeat copy logic suffer from code generation errors.

As observed in the case of library tasks, \sys is better than AutoCoT on almost all tasks (24.6 \% points). Tools are once again called very frequently (89\% of instances), and are responsible for a significant fraction of the gains over baselines.

When compared to the best published GPT-3 results, \sys performs favorably on average, especially on arithmetic tasks (+6.1 \% points).
As before, it does worse in tasks where good human demonstrations of how to decompose \emph{the task itself} (provided by \citet{suzgun2022challenging}) have a big impact.
We re-evaluate \sys with more human feedback on these tasks in \ref{sub:improving}, but even without that we conclude that \sys is competitive on BigBench even when we do not have supervision for decompositions for the task at hand (i.e. there is cross-task generalization).




 \begin{table*}[htb!]
    \centering
    \small
    \begin{tabular}{l|cccccc}
    \toprule
      & \bf Simple Text  & \bf CS  & \bf Strategy QA &\bf  Physics & \bf Unit  & \bf Reasoning about \\
     & \bf Editing & \bf Algorithms & &\bf  Questions & \bf Interpretation & \bf colored objects \\
     \midrule
    \bf \sys & 27.65 & 88.11 & 66.44 & 20.37 & 53.99 & 64.34 \\
    % \midrule
    % (with tool use) &&&&& \\
    % \bf LLM-based task sim. & 38.30 & 83.71 & 60.39 & 14.06 & 43.56 & 62.00 \\
    \bf + Self Consistency & 30.67(+3.0) & 90.99(+2.9) & 70.76(+4.3) & 24.07(+3.7) & 57.20(+3.2) & 69.11(+4.8) \\
    \bottomrule
    % \midrule
    % \multicolumn{7}{c}{Improved instruction finetuned models} \\
    % \midrule
    % % GPT-J &&&&& \\
    % % Toolformer &&&&& \\
    % T5 &&&&& \\
    % FLAN-T5  &&&&& \\
    \end{tabular}
    \caption{Improving \sys via self-consistency \citep{wang2022self}. Ensembling model generations over 15 runs further boosts performance.}
    \label{tab:self_consistency}
\end{table*}

\begin{table*}[htb!]
    \centering
    \small
    \begin{tabular}{l|cc|cc|c|l}
    \toprule
    % &  \multicolumn{2}{c}{Human-in-the-loop improvement} \\
    % \midrule
     \bf Task & \multicolumn{2}{c}{\bf CoT}  & \multicolumn{2}{c}{\bf \sys} &  \bf GPT-3 & \bf Human  \\
      % &\multicolumn{2}{c}{CoT} & \bf & \\
      & & \bf +Human & \bf & \bf + Human & \bf Best & \bf Feedback \\
     \midrule
     % Kth letter concatenation* & 59.40 & 100.0 & 59.40 & 100.0 & Code C: k'th letter extraction and merge for a list of words \\
     % Language Games* & 26.08 & 35.38 & 26.08 & 35.38 & Code  C: Eng->Pig Latin and vice-versa\\
     % Anachronisms*   & 49.82 & 82.91 & 49.82 & 82.91 & C: search query constrained to extract time-periods \\
     % & & & of the entities \\
     % Auto Debugging* & 61.18 & 67.05 & 61.18 & 67.05 & Code C: Code edit fixed to print variable asked in input \\
     % & & & & & A: ``[generate answer] What is the final error message?'' \\
     % Navigate & 85.9 & 80.89 & 85.9 & 80.89 & Code C: correct forward, backward, right, left distances\\
     % Date Understanding & 70.4 & 65.45 & 70.4 & 65.45 & A: First find what date is today \\
     % \midrule
     CS Algorithms & 0.0 & 23.0 & 88.11 & 92.73 & 73.48 & C: longest common subsequence code\\
     Reasong about objs.  & 33.33  & 67.75 & 64.34 & 98.90 & 71.00 & C: Define object, color, count data structure\\
     Repeat Copy Logic*   & 15.63 & 45.22 & 44.38 & 80.31 & 50.01 & C: string edit operation \\
     Sentence Ambiguity   & 51.47 & 72.33 & 73.33 & 83.67 & 70.67  & C: Constrain queries to extract relevant info.\\
     % &&&&& A: ``[subquestion] Is the claim true, given this information?''\\
     Simple Text editing*  & 30.21 & 35.31 & 27.65 & 36.11 & 35.31  &  C: string edit operation \\
     Strategy QA*   & 27.22 & 29.19 & 66.44  & 69.15 & 55.49 & C: Constrain queries to extract relevant info.\\
    % colored objects &  &  \\
     Physics* & 61.83  & 68.21 & 67.55 & 72.55 & 70.09   & A: [search] Formula that connects mass, ...\\
     Temporal Sequences & 19.70 & 30.22 & 49.5 & 88.00 & 81.8  & A: [subquestion] Is X free Yam to Zam? \\
     % for every option.\\
     Track Shuffled objs. & 19.44 & 36.48 & 37.67 & 99.86 & 36.32  & C: Define object pair data struct, swap logic\\
     Unit Interpretation* & 41.2  & 41.2  & 53.99 & 95.0 & 58.2   & A: [add unit] Add the right unit to the answer\\
     Word Unscrambling*  & 32.44 &  33.40 & 42.70 & 62.11 & 40.72 & T: lookup permutations in dictionary\\
     \midrule
     Average & 30.2 & \bf 43.8 & 56.0 & \bf 79.85 & 58.5  \\
    \bottomrule
    \end{tabular}
    \caption{Improving \sys and free-form CoT via self-consistency and human-in-the-loop feedback. (*) indicates that human-in-the-loop improvement was done over automatically generated CoT reasoning for these tasks. Feedback for \sys includes correcting sub-steps in programs (\exinline{C:}), adding additional sub-steps(\exinline{A:}), and defining new tools(\exinline{T:}). Note that only five  examples were edited for each task.}
    \label{tab:model_improvements}
\end{table*}



\paragraph{Other benchmarks}
To make sure \sys does not overfit to BigBench-style tasks, we evaluate performance on additional benchmarks.
We report performance on randomly selected tasks from the MMLU benchmark \cite{hendrycks2020measuring} in Table \ref{tab:main_result_table_test}, where \sys is more effective than all baselines on 5/6 tasks (+8.5 points better than few-shot baseline on average), despite having no supervision for demonstrations or tool use.
MMLU requires extensive world knowledge, and thus most of these tasks benefit the most from the search tool.

In Table \ref{tab:toolformer}, we compare \sys to a random subset of tasks used to evaluate Toolformer \cite{schick2023toolformer}, a model finetuned to use a variety of tools.
The comparison is not exact since Toolformer uses a smaller GPT-J model, but it is informative that \sys outperforms Toolformer by a large margin on 5/6 of these tasks.
To make sure these gains are not simply a result of model scale, we also use vanilla GPT-3 as a baseline, which yields much worse results than \sys on all tasks.
Besides improved performance, we note again that \sys does not require additional fine-tuning when new tools or new base LLMs are introduced, and also is amenable to further improvement at the cost of compute or human feedback.
% (to which we now turn).


\subsection{Improving \sys}
\label{sub:improving}


\paragraph{Self-consistency}
% \label{sub:self-consistency}
% 3.65 pts over \sys
Previous work has noted benefits in generating multiple LLM outputs and taking the most frequent answer (a process known as self-consistency), particularly for settings with multi-step reasoning \cite{khot2022decomposed, wang2022self}.
In Table \ref{tab:self_consistency}, we present self-consistency results (generating $15$ outputs) for \sys on a subset of tasks and see that it consistently improves performance, at the cost of extra computation.

\paragraph{Human feedback}
% \label{sub:human-in-the-loop}
% 13.11611111 gain over SOTA and only doesnt work in navigate and date understanding.
% 21.33777778 over other auto CoT. Improve out programs instead of autoCot
% Programmatic tasks (arithmetic and algorithmic) have the largest gains (tracking, reasoning abt colored aobjects, repeat copy logic, kth letter, auto debugging)
% Explain how the baseline gets its improvements.

% Humans are provided two affordances with the goal of improving end-task performance --- (a) Program editing to extend the task library; and (b) Improving tool definition and integration to extend the tool library.
We also pilot the use of task-specific feedback in Table \ref{tab:model_improvements}, by having one of the authors \emph{edit} $5$ random instances of model-generated programs that resulted in errors for each task.
When editing, we correct errors in sub-steps (denoted as \exinline{C:}), adds missing substeps (\exinline{A:}), or defines a new tool and demonstrates its use (\exinline{T:}).
For example, this involved introducing an ``add unit'' sub-step for the PQA task, and implementing a dictionary lookup function as a tool for the ``Word Unscrambling'' task (both illustrated by Figure \ref{fig:human_feedback}).

We also compare human feedback applied to CoT-style reasoning. \citet{suzgun2022challenging} already provide reference CoT-style reasoning for some tasks. For datasets where  human-authored CoT reasoning is unavailable, we correct the output of the automatic CoT baseline, as indicated in Table~\ref{tab:model_improvements}.
The same author \emph{edits} $5$ random instances of AutoCoT decompositions that lead to errors on the same tasks, correcting errors in sub-steps or adding new sub-steps. As a reference, the  edits included 35\% of tokens in the baseline, and 15.7\% of tokens in the \sys programs. This included correcting sub-step arguments and outputs in 72\% of the chosen tasks and adding additional sub-steps in 44\% of the tasks. New tool definitions were added for two tasks --- dictionary lookup for word unscrambling and a Prolog engine for formal fallacies.

In both cases, editing programs and adding them as demonstrations leads to significant gains in performance on the task at hand.
However, the gain is much more dramatic in \sys, leading it to consistently outperform the best published GPT-3 baseline for the task at hand.
Further, these corrected programs and tools can be added to the task and tool libraries, and our prior results in Table \ref{tab:main_result_table_test} suggest that they potentially help improve \sys on other tasks as well.
This pilot indicates that besides being competitive on cross-task generalization, \sys is very amenable to task-specific improvement with minimal human intervention. We report similar results in the task library in \ref{appendix:human_feedback}.
