Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code).
Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce \textbf{A}utomatic \textbf{R}easoning and \textbf{T}ool-use (\sys), a framework that uses frozen \emph{LLMs} to \emph{automatically} generate intermediate reasoning steps as a program.
Given a new task to solve, \sys selects demonstrations of multi-step reasoning and tool use from a task library. At test time, \sys seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation.
\sys achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks.
\sys is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.