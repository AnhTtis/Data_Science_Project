We briefly describe our experimental setup, including the datasets chosen for evaluation, important implementation details, and baseline approaches and state-of-the-art models we compare with .

\paragraph{Evaluation Datasets}
In Section~\ref{sec:seed_tasks}, we described how a subset of BigBench tasks that require the most common reasoning skills are chosen as part of the task library. To evaluate \sys, we use the remaining tasks that require those skills as test sets. In total, 15 tasks are used to create the library of train task programs and 19 tasks are used for evaluation. In addition to BigBench tasks, we also evaluate \sys on a random subset of six tasks from the MMLU benchmark - Computer science, astronomy, business, virology, High school geography, and high school mathematics. Finally, we also evaluate on a subset of tasks used to evaluate Toolformer~\citep{schick2023toolformer}, in order to compare our gradient-free approach for automatic tool use with a model fine-tuned for tool use. 
Performance on MMLU and additional evaluation tasks can help us gauge if our choice of the task library does not overfit to BigBench.


\paragraph{Metrics and Hyperparameters}
We use InstructGPT (text-Davinci-002) as the main frozen LLM. 
With tool use, various parts of this program may be replaced by generations from other models or API outputs. For example, Codex is used for code generation and editing. We set the number of seed tasks in the prompt to $N=3$ and use $2$ demonstration programs from each task. The temperature for GPT-3 and Codex are set to 0.3. We extract the final answer from the program based on keywords that the language model is
prompted to produce at the end (i.e., ``Ans: "). We measure the preferred scoring metric for each task used in \citet{srivastava2022beyond}. We report performance averaged over 5 runs of \sys. 
% (\ref{appendix:additional} contains standard deviation results).
More details on models, metrics, and hyperparameters are in Appendix~\ref{appendix:hyperparameters}.
% Note that a similar pattern ("The final answer is") is used in the prompts to the Auto-CoT baseline as well. 


\paragraph{Baselines}
\sys proposes an automatic framework to generate multi-step reasoning decompositions and use relevant available external tools within those decompositions. We compare with the following baselines to better understand the source of improvement and where \sys stacks up against other approaches that use supervision and/or finetuning. 
% We compare \sys with AutoCoT~\citep{zhang2022automatic}, which is most similar to our approach in that they generate 
\setlist{nolistsep}
\begin{itemize}[noitemsep]
    \item Few-shot baseline: Prompting LLMs with input-output pairs (no intermediate reasoning)
    \item Auto-CoT baseline: A baseline that generates automatic CoT-style multi-step reasoning in a free-form natural language (as done in AutoCoT~\citep{zhang2022automatic}). A randomly selected subset of examples in the dataset is used to prompt the LLM to elicit CoT-style reasoning (\emph{Input + Let's think step-by-step.}). Since CoT-style generation is free-form and parsing potential tool use symbols is harder, we don't use tools for this baseline. This baseline specifically measures the effectiveness of a custom query language (and PeG grammar) we use to write programs and parse tool calls;
    \item \sys without tools: Tool-use is turned off. This baseline specifically ablates for the advantage of tool use over LLM generation alone.
    \item Comparable Best: Best-known comparable-sized model result for each task that does multi-step decomposition and/or tool use. We survey all works that use GPT-3 and Codex 175B parameter models, along with additional human supervision to decompose reasoning steps and additional external tools to boost performance (with human supervision for what tool to use where and how).
\end{itemize}
% For a fair comparison with our work, the Auto-CoT baseline is implemented without additional clustering and filtering of examples used for demonstrations i.e. a random subset of examples in the dataset are used to prompt the LLM to elicit CoT-style reasoning (\emph{Let's think step-by-step}), similar to how we use a random subset of examples to create demonstration programs.

