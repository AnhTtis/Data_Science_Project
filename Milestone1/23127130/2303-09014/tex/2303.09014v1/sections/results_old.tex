\label{result:perf_improvement}

\begin{table*}[]
    \centering
    \small
    \begin{tabular}{l|cc|cc|c}
       \toprule
\bf       Task Name (Cluster) & \bf Few Shot & \bf AutoCot & \bf \sys & \bf \sys & \bf GPT-3 \\
       &  &  & \bf w/o Tool Use & & \bf Best\\
       \midrule
       Anachronisms \cellcolor{search}(Search) & 71.3$^5$ & 51.48 & 70.87 & 75.66 & -\\
       % \cellcolor{instructgpt}71.13$^5$\\
       Musique \cellcolor{search}(Search) & 2.03 & 12.88 & 10.04 & 19.19 &  
       15.2$^6$\\
       Hindu Knowledge \cellcolor{search}(Search) & 85.02 $^5$ & 73.03 & 83.42 & 87.98 & -\\
       % \cellcolor{instructgpt}83.65$^5$\\
       Known Unknown \cellcolor{search}(Search) & 68.90 $^5$ & 56.09 & 80.43 & 80.43 & -\\
       \cdashlinelr{1-6}
       \Gape[.5pt][.5pt]{$\Delta$ with \sys(Search)} & +9.0 & +8.9 & + 4.6 & & + 6.3 \\
       \cdashlinelr{1-6}
       Elementary Math QA \cellcolor{arithmetic}(Arithmetic) & 56.40$^7$ & 74.52 & 58.04 & 68.04 & -\\
       % \cellcolor{codex}57.80$^7$\\
       % \cdashlinelr{1-6}
       Aqua-rat \cellcolor{arithmetic}(Arithmetic) & 20.17$^7$ & 34.41 & 36.29 & 54.20 & 54.1$^8$ \\
       % \hdashline
       GSM8K \cellcolor{arithmetic}(Arithmetic) & 7.79$^7$ & 21.99 & 53.4 & 71.00 & 71.6$^8$\\
       Navigate \cellcolor{arithmetic}(Arithmetic) & 60.7$^7$ & 61.7 & 72.4 & 72.4 & 85.90$^1$\\
       \cdashlinelr{1-6}
       $\Delta$ with \sys(Arithmetic) & +30.0 & +18.25 & + 21.85 & & -1.0 \\
       \cdashlinelr{1-6}
       % \cellcolor{instructgpt}67.82$^5$\\
       % \textcolor{red}{76.08} & 
       K'th letter concatenation \cellcolor{string}(String) & 3.2$^5$ & 0.64 & 8.19 & 40.00 & 98.0$^3$ \\
       % \cellcolor{instructgpt}33.20$^5$ \\
       Language games \cellcolor{string}(String) & 35.14$^5$ & 18.58 & 11.19 & 23.08 & -\\ 
       Auto Debugging \cellcolor{code}(Code) & 62.94$^5$ & 38.24 & 55.29 & 62.94 & -\\
       % \cellcolor{instructgpt}65.88$^5$\\
       Code Description \cellcolor{code}(Code) & 97.99$^7$ & 88.67 & 84.67 & 88.00 & -\\
       % \cellcolor{codex}95.33$^7$\\
       Date Understanding \cellcolor{freeform}(Free-form) & 37.53$^5$ & 38.90 & 52.05 &  - &  70.41$^1$ \\
       Formal Fallacies \cellcolor{freeform}(Free-form) & 44.84$^5$ & 56.4 & 64.76 & - &  58.4$^1$\\
       Hyperbation  \cellcolor{freeform}(Free-form) & 62.72$^5$ & 55.4 & 80.80 & - & 72.4$^1$\\
       \midrule
       $\Delta$ with \sys (Overall) & +14.90 & +17.17 & +7.91 & & -4.0  \\
       \bottomrule
    \end{tabular}
    \caption{NLProgrammer performance on task library.($^1$ Human-crafted CoT \citep{wei2022chain, suzgun2022challenging}, $^3$ Decomposed Prompting\citep{khot2022decomposed}, $^5$ InstructGPT \citep{ouyang2022training}, $^6$ Self-Ask \citep{press2022measuring}, $^7$ Code-davinci-002 \citep{chen2021evaluating}, $^8$ PoT \citep{chen2022program}).}
    \label{tab:main_result_table_library}
\end{table*}



\begin{table*}[htb!]
    \centering
    \small
    \begin{tabular}{l|cc|cc|c}
       \toprule
\bf       Task Name (Cluster) & \bf Few Shot & \bf AutoCot & \bf \sys & \bf \sys & \bf GPT-3 \\
       &  &  & \bf w/o Tool Use & & \bf Best\\
    \midrule
        \multicolumn{6}{c}{\bf Test Tasks} \\
       \midrule
        Sentence Ambiguity \cellcolor{search}(Search) & 70.67$^5$ & 51.47 & 71.00 & 73.33 & -\\
        % \cellcolor{instructgpt}71.67$^5$\\
        Strategy QA \cellcolor{search}(Search) & 55.49$^5$ & 27.22 & 59.37 & 66.44 & -\\
        % \cellcolor{instructgpt}62.05$^5$\\
        Physics \cellcolor{search}(Search) & 70.09$^5$ & 68.21 & 59.13 & 67.55 & -\\
        % \cellcolor{instructgpt}72.93$^5$\\
        Sports Understanding \cellcolor{search}(Search) & 69.74$^5$ & 51.47 & 62.74 & 64.77 & 86.59$^1$\\
       \cdashlinelr{1-6}
       \Gape[.5pt][.5pt]{$\Delta$ with \sys(Search)} & +1.52 & +18.43 & + 4.9 & & -5.3 \\
       \cdashlinelr{1-6}
        Physics Questions \cellcolor{arithmetic}(Arithmetic)& 7.02$^5$ & 5.56 & 6.30 & 20.37 & -\\
        % \cellcolor{instructgpt}6.30$^5$\\
        Operators \cellcolor{arithmetic}(Arithmetic)& 71.23$^7$ & 75.52 & 71.80 & 92.00 &-\\
        % \cellcolor{codex}73.80$^7$\\
        Unit interpretation \cellcolor{arithmetic}(Arithmetic)& 58.2$^7$ & 41.20 & 51.4 & 53.99 & -\\
        % \cellcolor{instructgpt}61.8$^5$ \\
        Repeat copy logic \cellcolor{arithmetic}(Arithmetic) & 50.01$^7$ & 15.63 & 31.25 & 44.38 & -\\
        % \cellcolor{codex}50.62$^7$\\
        % \cdashlinelr{1-6}
        % Symbol Interpretation & 30.56 & 23.08 &  &\\
        Object Counting \cellcolor{arithmetic}(Arithmetic) & 39.2$^7$ & 26.80 & 42.2 & 87.00 & 81.20$^1$\\
        Penguins in a table \cellcolor{arithmetic}(Arithmetic) & 58.23$^7$ & 40.40 & 68.86 & 77.85 &  72.34$^1$\\
        Reasoning about objects \cellcolor{arithmetic}(Arithmetic) & 71.00$^7$ & 33.33 & 45.35 & 64.34 & 52.69$^1$ \\
        Temporal sequences \cellcolor{arithmetic}(Arithmetic) & 55.80$^7$ & 19.70 & 26.38 & 30.22 & 81.8$^1$\\
        Tracking shuffled objects \cellcolor{arithmetic}(Arithmetic) & 22.39$^7$ & 19.44 & 18.14 & 37.67 &  36.32$^1$ \\
       \cdashlinelr{1-6}
       \Gape[.5pt][.5pt]{$\Delta$ with \sys (Arithmetic)} & +8.3 & +25.58 & + 16.24 & & -1.0 \\
       \cdashlinelr{1-6}
        Word Unscramble \cellcolor{string}(String) & 40.72$^7$ & 32.44 & 23.03 & 42.7 & -\\
        % \cellcolor{codex}48.20$^7$\\
        Simple Text Editing \cellcolor{code}(Code) & 35.31$^5$ & 30.21 & 20.74 & 27.65 & -\\
        % \cellcolor{instructgpt}36.60$^5$\\
        CS Algorithms \cellcolor{code}(Code) & 73.48$^7$ & 0.0 & 41.59 & 88.11 & -\\
        % \cellcolor{codex}76.82$^7$\\
        Ruin names \cellcolor{freeform}(Free-form) & 71.01$^5$ & 55.28 & 60.22 & - & -\\
        % \cellcolor{instructgpt}75.51$^5$\\
        Disambiguation QA \cellcolor{freeform}(Free-form) & 55.03$^5$ & 48.45 & 55.89 & - & 60.62$^1$\\
        Snarks \cellcolor{freeform}(Free-form) & 54.58$^5$ & 57.24 & 57.13 & - &  65.2$^1$\\
        \midrule
        $\Delta$ with \sys(Overall) & +4.33 & +21.68 & +12.58 & & -3.20  \\
        \midrule
        \multicolumn{6}{c}{\bf MMLU} \\
        \midrule
        College Computer Science \cellcolor{search}(Search)& 41.00 & 43.99 &  63.40 & 67.80 & 63.6$^4$\\
        Astronomy \cellcolor{search}(Search)& 62.10 & 41.48 & 76.71 & 79.1 & 62.5$^4$\\
        Business Ethics \cellcolor{search}(Search)& 61.60 & 48.8 & 77.17 & 81.16 & 72.7$^4$\\
        Virology \cellcolor{search}(Search)&  50.03 & 49.52 & 71.60 & 70.67 & 50.72$^4$\\
        Geography \cellcolor{search}(Search)& 77.67 & 57.07 & 70.30 & 70.67 & 95.5$^4$\\
        Mathematics \cellcolor{arithmetic}(Arithmetic) & 36.67 & 33.77  & 39.50 & 45.66 & 51.7$^4$\\
        \midrule
        $\Delta$ with \sys(MMLU) & +15.4 & +25.71 & +2.04 & & +8.5  \\
       \bottomrule
    \end{tabular}
    \caption{NLProgrammer performance on BigBench tasks and MMLU tasks. ($^1$ Human-crafted CoT \citep{wei2022chain, suzgun2022challenging}, $^3$ Decomposed Prompting\citep{khot2022decomposed}, $^4$ Scaled instruction finetuning \citep{chung2022scaling}, $^5$ InstructGPT \citep{ouyang2022training}, $^6$ Self-Ask \citep{press2022measuring}, $^7$ Code-davinci-002 \citep{chen2021evaluating}, $^8$ PoT \citep{chen2022program}).}
    \label{tab:main_result_table_test}
\end{table*}


We report performance of \sys, baselines, and comparable best, on all library tasks (in Table~\ref{tab:main_result_table_library}) and test tasks in BigBench and six MMLU tasks (in Table~\ref{tab:main_result_table_test}). 
Tasks are arranged by the cluster of library tasks used as demonstrations for prompting, which is related to the primary reasoning skill needed for the task and provides insights about skill-wise performance gains on \sys.
% (approaches with tool use (green), and approaches with supervision for CoT reasoning (yellow). 
We compare and discuss the performance of \sys for library tasks (Section~\ref{sub:results_seed_tasks}), test tasks (Section~\ref{sub:results_test_tasks}), and MMLU tasks (Section~\ref{sub:results_mmlu}).

\subsection{Results on Library tasks}
\label{sub:results_seed_tasks}
% Comparison to Few-shot baseline [Outcome, why, what it means ] - lg (string prompt),auto debugging (code errors and variable names are not always available upon execution), code [code generation and execution errors for different choices]. It seems like with all three tasks, my current decomposition supervision is not the best. 
% Comparison to Auto-Cot baseline [Outcome, why, what it means ] Improvement is 16.584 pts, all but elementary math qa, errors in code generation for math word problem. When comparing \sys-tools with auto, 14/19 tasks with average improvement of 8.45\%. PEG grammar format is better than CoT for multi-step decomp. For data understanding, formal and hyperbation, we use free form human authored CoT itself as the substep, but no affordance is called.
% Comparison to No tool use [Outcome, why, what it means ] 8.13\% point improvement with succesful affordance call being made 95.33\% times. Affordance call mostly search, code gen and execution. BUt note that aqua calls sympy.
% Comparison to SOTA [Outcome, why, what it means ] Worse in 6/19 tasks, with worst performance on Algorithmic and Multi-Step Arithmetic Reasoning tasks (auto, code), spatial (navigate) and string manipulation tasks (lg, kth, date). Particularly for human/tool use SOTA, kth, navigate and data significantly underperform, which we explore in hiil setup.

For library tasks (training tasks), we use the corresponding "gold" task-cluster programs as prompt demonstrations (e.g. For anachronisms, we create a multi-task prompt containing task programs from anachronisms, Hindu knowledge, and Musique~\ref{appendix:tasklibrary}).
Specifically, the prompt to \sys contains supervision for the task for multi-step reasoning and tool use for up to 2 instances (along with  other similar tasks).

% What do I want to say here: Even though we get supervision here, its for far fewer examples. Yet in most cases except code gen/edit, we are doing better. What I am not able to explain here is what code description, auto-debugging is better. 
% Why i think the performance is bad on LG: code input is often wrong, code description: generating code for each option and comparing code is not the best strategy maybe. Auto debugging I think its seeing more examples.

% Cat 1: Search

% Cat 2: Arithmetic

% Overall: 

% Where we loose: 
\sys improves performance over few-shot prompting by 12.3 (\% points) on average, despite more than 2 supervised demonstrations used for the latter (\ref{appendix:hyperparameters}).
\sys improves significantly on tasks that require search (), complex arithmetic calculations, and CoT-style reasoning. On the other hand, it lags behind on language games, code description, and auto debugging --- tasks that use code generation and/or editing models. Code generation errors  (in the Codex tool output) can lead to cascading errors in reasoning. 

% The improvement from AutoCOT stems from better program structure and tool-use.
\sys improves performance over automatically generated CoT-style reasoning by 17 \% pts on average, with improvements in all tasks except elementary math QA. Other multi-step arithmetic tasks (GSM8K, Aqua-rat) are more challenging for the CoT, as complex intermediate calculations are often incorrect.
\sys with tool-use turned off also gains 8 \% pts over this baseline. We hypothesize that the query language (and PeG grammar) is better at eliciting multi-step reasoning from models than free-form CoT due to the added structure to the reasoning.

\sys improves or matches the best-known performance with decomposition/tool-use supervision (for GPT3, 175B model size) on 5 of the 8 library tasks.
% Its doing better or similar in 5/8. In the other 3, kth letter: de-comp is not the same as theirs, in navig and date-understanding, the reasoning is better than ours. This is basically better human supervision.
On k'th letter concatenation, \sys performs much worse than \citet{khot2022decomposed}, which recursively decomposes the task to finding the k'th letter in a word and prompts the LLM for the subtask with \emph{more} supervision.  
Similarly, for navigation and date-understanding, \citet{suzgun2022challenging} use a different CoT-style reasoning chain, which may be more effective than what we initially authored. We explore more human feedback to debug the program and improve performance of \sys for these tasks in Section~\ref{appendix:human_feedback}.

Tools are called for 95\% of the test instances on average, and using tools improves performance by 7.9 \% points, averaged over tasks. Gains from tool use are particularly significant for arithmetic tasks that benefit from representing the arithmetic problem as code that can be executed to compute complex arithmetic accurately. This has also been noted in prior work as well \citep{chen2022program, gao2022pal}.





\subsection{Big Bench Test Tasks}
\label{sub:results_test_tasks}

% Comparison to Few-shot baseline [Outcome, why, what it means ] 9/19 (0.5852631579) tasks on BB. On the tasks that it does not do well: code and arithmetic operations (unit intepret, reasoning colored objects), string operations (simple, word unscrambling, repeat), and datasets that used the CoT-cluster (ruin, disambiguation), temporal sequences (biggest loss): Knowledge intensive tasks like sentence ambiguity, strat, sports etc) we do well. 

% Comparison to Auto-Cot baseline [Outcome, why, what it means ] AutoCoT worse almost always than few-shot baseline. \sys beats auto by 21.68631579 avg and even \sys without tool use beats auto by 9.102631579 pts. 

% Comparison to No tool use [Outcome, why, what it means ] Tool use 12.58368421 with tools being used (search, code operations) 89.11\% of the times. The benefits of tool use are particularly large when code gen and exec is involved (pq, word unscrambling, cs, op, repeat copy, obj cnt, reasoning color, temporal). Tools are not used in disamb, ruin and snarks. 

% Comparison to SOTA [Outcome, why, what it means]: Looking at SOTA human/tool use models alone. Worst in sports, disambiguation, temporal. Points to a lacking in the automatically generated sequence. Give eG. We explore in hiil

For test tasks, no explicit supervision is available for decompositions and tool use. Instead, we \emph{select} a few related task programs from the task library as demonstrations. We use the task-cluster-based selection strategy since it has higher performance and low variance on average compared to the LLM-based selection strategy (discussed in more detail  in Section~\ref{appendix:taskselection}). The best-performing task cluster used is also specified in Table~\ref{tab:main_result_table_library}. We note that while the task selection strategy still uses some held-out input-output pairs of the task to select related tasks in the prompt, no additional supervision is required for  multi-step decomposition or tool use. 
The performance of \sys on test tasks helps us evaluate the cross-task generalizability of the framework.
% The cluster of string tasks for which our programs used limited code generation and execution are not used for string manipulation tasks like simple text editing and repeat copy logic, and code and arithmetic task clusters are used instead.

\sys improves or matches the performance of few-shot prompting in 9 of the 19 test tasks. Unlike few-shot prompting, \emph{no} labeled examples of the task are used in the prompt. In particular, it has superior performance on tasks that require search (Sentence ambiguity, strategy QA, Sports understanding) and in some arithmetic and code tasks that require complex calculations (CS algorithms,  Physics Questions, object counting, penguins in a table, tracking shuffled objects).
Few-shot prompting beats out \sys on multiple-choice arithmetic tasks (unit interpretation, reasoning about colored objects, and temporal sequences). We hypothesize that code generation errors and cascading errors in arithmetic calculations can make choosing from options challenging.
String manipulation tasks like simple text editing, word unscrambling, and repeat copy logic also suffer from code generation errors---code snippet that edits the string is often incorrect (in 49\% of the cases).

\sys improves performance over automatically generated CoT reasoning by 21 \% pts on average, with significant improvements in all tasks except simple text editing. For this task, the code generated to edit text has a high error rate (49\% of instances). \sys with tool-use turned off also gains 9 \% pts over this baseline, underscoring the effectiveness of the structured multi-step reasoning of \sys.

% Its doing better or similar in 3/8. This maybe because of sub-step errors that we address in 
\sys improves or matches the best-known performance with decomposition/tool-use supervision (for GPT3, 175B model size) in 4 of the 8 tasks. On these tasks, \citet{suzgun2022challenging} uses human supervision for CoT-style reasoning. \sys requires no such supervision, generates multi-step reasoning containing code snippets, and invokes code execution tools to improve performance. 
\sys lags behind on sports understanding, disambiguation QA, and temporal sequences. We hypothesize that the programs generated for these tasks have sub-steps with high error or a missing reasoning step. We explore human feedback to debug and improve the performance of \sys for these tasks in Section~\ref{sub:human-in-the-loop}.

On average, tools are called for 89\% of the test instances, and using tools improves performance by 13 \% points on average. Gains from tool use are once again particularly significant for algorithmic and arithmetic tasks like CS algorithms, word unscrambling, reasoning about colored objects, tracking shuffled objects, and object counting.



 \begin{table*}[htb!]
    \centering
    \small
    \begin{tabular}{l|cccccc}
    \toprule
      & \bf Simple Text  & \bf CS  & \bf Strategy QA &\bf  Physics & \bf Unit  & \bf Reasoning about \\
     & \bf Editing & \bf Algorithms & &\bf  Questions & \bf Interpretation & \bf colored objects \\
     \midrule
    \bf \sys & 27.65 & 88.11 & 66.44 & 20.37 & 53.99 & 64.34 \\
    % \midrule
    % (with tool use) &&&&& \\
    % \bf LLM-based task sim. & 38.30 & 83.71 & 60.39 & 14.06 & 43.56 & 62.00 \\
    \bf Self Consistency & 30.67 & 90.99 & 70.76 & 24.07 & 57.20 & 69.11\\
    \bottomrule
    % \midrule
    % \multicolumn{7}{c}{Improved instruction finetuned models} \\
    % \midrule
    % % GPT-J &&&&& \\
    % % Toolformer &&&&& \\
    % T5 &&&&& \\
    % FLAN-T5  &&&&& \\
    \end{tabular}
    \caption{Replacing task-cluster based task similarity strategy in \sys with LLM-based similarity. Adding model-ensembling \citep{wang2022self} further boosts performance.}
    \label{tab:self_consistency}
    \begin{tabular}{l|cccccc}
    \midrule
     &  \bf SQuAD &  \bf T-REx & \bf SVAMP & \bf MAWPS & \bf NQ & \bf TriviaQA \\
    \midrule
    \bf GPT3 & 29.90 & 39.8 & 10.0 & 19.8 & 22.6 & 65.9 \\
    \bf Toolformer & 33.8  & 53.5 & 29.4 & 44.0 & 17.7 & 48.8  \\
    \bf \sys & 39.34 & 50.4 & 76.2 & 71.00 & 33.8 & 66.13 \\
    \bottomrule
    \end{tabular}
    \caption{Comparing \sys results on GPT3 (175B) model and \citep{schick2023toolformer}, which is a smaller GPT-J model finetuned for tool-use. Results are reported from their paper (their code and models are not publicly available).}
    \label{tab:ablations}
\end{table*}


% \subsection{Benefits from tool use}
\subsection{Performance on other benchmarks}
\label{sub:results_mmlu}
% Comparison to Few-shot baseline [Outcome, why, what it means ] 7.676666667
% Comparison to Auto-Cot baseline [Outcome, why, what it means ] 10.60 over auto, 23.40 over auto. 
% Comparison to No tool use [Outcome, why, what it means ] 2.73 over no tools. Tools are search in most and code gen/exec for mmlu mathemtics.
% Comparison to SOTA [Outcome, why, what it means ] 3.029333333 (georgraphy is significantly lower)
In Table \ref{tab:main_result_table_test}, we report performance on a small subset of tasks from the MMLU \citep{hendrycks2020measuring} benchmark. 
% In Table \ref{tab:ablations}, we also report the performance of \sys on tasks used in the Toolformer approach. 
These results help us understand if composing the task library with BigBench tasks limits the performance of the framework on other benchmarks and tasks.
\sys is better on average than few-shot prompting (7.6 avg. \% points) and automatic CoT-style reasoning approaches (23.40 avg. \% points) on the MMLU tasks. Search is the most commonly used tool for these tasks. The gains of \sys over CoT-style reasoning are especially noteworthy in tasks like astronomy and business ethics --- tasks requiring specialized knowledge that benefit from access to a search engine. \sys also improves over the best-known GPT-3 results for these tasks from \cite{chowdhery2022palm} except for the high school geography task.
% A similar pattern holds for SQuAD, TriviaQA, MAWPS, SVAMP, and NQ in Table~\ref{tab:ablations}. 
These tasks are still "in-domain" with respect to our choice of reasoning skills and tools. Tasks requiring a completely new skill, like machine translation, may prove to be challenging for the current task and tool library catalog. However, we hypothesize that these libraries can be expanded to support additional tool use and reasoning skills in the future.

