
Large language models (LLMs) have achieved impressive zero- and few-shot results on a variety of natural language processing tasks \citep{brown2020language,chowdhery2022palm} via in-context learning \citep{xie2021explanation} --- providing instructions and examples as prompts. Their ability to tackle complex reasoning tasks by decomposing prompts into multiple intermediate steps has been particularly noteworthy \citep{weichain, zhou2022least}. However, LLMs have inherent limitations in their abilities - mathematical skills \citep{patel-etal-2021-nlp}, up-to-date information on recent events \citep{komeili-etal-2022-internet}, specialized domain knowledge \citep{gozalo2023chatgpt}, and hallucinating facts while ignoring local context \citep{ji2022survey}. Recent work has proposed using external tools and APIs to address these limitations - like code generation and interpreters \citep{gao2022pal, chen2022program}, search engines \citep{press2022measuring, weichain}, and machine translation systems \citep{schick2023toolformer}. Existing approaches that combine CoT in-context learning with tool use rely on hand-crafted prompts (often with task-specific multi-step  reasoning) and explicit human supervision on which tools to use where, and how to use incorporate their results in the prompt. \sys provides a flexible framework to automate both, generating multi-step reasoning \emph{and} decisions about tool use for a multitude of downstream tasks, without any additional training of the LLM.

\begin{figure}
    \centering
    \includegraphics[scale=0.3]{sections/resources/teaser.jpg}
    \caption{\sys Framework}
    \label{fig:teaser}
\end{figure}


The \sys framework automatically generates multi-step reasoning (decompositions) for instances of new tasks, only assuming access to a few input-output pairs. It consists of a library of diverse training tasks (from BigBench) that require multi-step reasoning, a subset of which are used to provide in-context supervision for the new test task. It also consists of a library of external tools, such as a python interpreter, Codex\citep{chen2021evaluating}, or Google search, that can be leveraged to improve performance on individual sub-steps. \sys prompts the \emph{frozen} LLM with multi-step reasoning demonstrations of similar training tasks, that are formatted using a flexible query language \citep{beurer2022prompting}. This allows \sys to parse the generation, identify calls to tools and call them, and integrate their output back into the generation to re-prompt the LLM (see Figure \ref{fig:teaser}). Furthermore, the framework facilitates human-in-the-loop feedback --- humans can observe generation errors, fix syntactic and structural issues in generations, evaluate specific sub-steps, change outputs of sub-steps, and add additional tool use to boost performance.


\sys significantly improves zero-shot multi-step reasoning performance on unseen tasks over previous approaches for automatically generating reasoning chains (\citep{zhang2022automatic} (AutoCoT)).
Using 13 diverse training tasks from BigBench, and experimenting with 21 new test tasks from the benchmarks and 5 additional tasks from MMLU, \sys obtains performance gains on 14 of the test tasks ranging from 8-118\% over AutoCoT.
Automatic tool use significantly improves (by 85\% on average) or matches the performance of AutoCOT on all tasks.  On 8 of the unseen tasks, \sys is better by 68\% on average than vanilla \emph{few-shot} in-context learning approach. Finally, human-in-loop feedback improves performance on the remaining tasks by 37.6\% on average, nearly solving several of the tasks considered. 

\sys makes a significant contribution to the line of work on automatic generation of interpretable language prompts for LLMs. While prior work has focused on automatically generating task instructions \cite{zhou2022large} for simple NLU tasks\citep{honovich2022instruction} and multi-step reasoning chains for a small subset of analytical tasks in BigBench\citep{srivastava2022beyond}, \sys works on larger variety tasks from the BigBench and MMLU \citep{hendrycks2020measuring} benchmarks, while also supporting automatic tool use. 
% Since models are now leveraged for generating multi-step reasoning in in-context learning with the potential use of external APIs and models, 
Recent work on scaling up instruction finetuning  with CoT training data \cite{chung2022scaling} and explicitly finetuning LLMs for tool use \cite{schick2023toolformer} have been very effective strategies to improve performance for in-context multi-step reasoning with tool-use. While \sys does not explicitly use additional finetuning of LLMs on CoT training data or tool use, we hypothesize that such models may improve the performance of the \sys framework if used in place of the frozen LLM. 
% Furthermore, while finetuning on specific reasoning formats or tool-use libraries may potentially make these models rigid. 
\sys is also flexible and extensible - the library of training tasks and tools can be expanded to support a larger array of APIs and downstream tasks in the future. An important contribution of NLprogrammer in the human-in-the-loop feedback literature \citep{bai2022training, wu2022ai} is that it naturally facilitates human feedback that can iteratively improve task performance. We enlist contributions of \sys as follows:

\begin{itemize}
    \item Zero-shot adaptation of multi-step reasoning to new tasks without significant prompt engineering by humans or explicit instruction finetuning on the training tasks.
    \item Prompt query language allows for automatic parsing and calls to tools to improve performance
    \item Extensible framework: Library of tools and training tasks can be expanded to support custom tasks and tools
    \item  Humans can debug generation and extend framework to improve performance.
\end{itemize}

% \paragraph{What problem are we solving and why its important. What do the current approaches lack}
% CoT, instructions and decomposed prompts are subject to manual prompt engineering skills. Moreover, humans have to examine task instances, identify and define task-level strategies to come up with decomposed instructions. Humans have to also identify and define when certain sub-steps can be delegated to external tools or other LLMs. We need to built a framework that works in the same low-data paradigm; and can reduce this human effort, while also facilitating humans to arrive at better decomposed instructions.

% A recent line of work has proposed automatic natural language instruction generation and automatically generating CoT using LLMs themselves. The former still works on relatively simple tasks and the latter has not been extensively evaluated on a large range of language tasks and requires a lot of data to work. Furthermore, we really want to be able to leverage tool use and human-in-the-loop feedback so that our framework is extendable and can always be improved with some additional human effort. This is the gap that \sys fills.

% \paragraph{Results}

