\paragraph{Evaluation Datasets}
In addition to $15$ tasks in the task library (Section \ref{sec:seed_tasks}), we evaluate \sys on $19$ additional test tasks from BigBench which also belong to the five task clusters identified in Section~\ref{sec:seed_tasks}.
% (also grouped by required skills).
To check for cross-benchmark generalization, we further evaluate \sys on a random subset of tasks from the MMLU benchmark \citep{hendrycks2020measuring}.
Finally, we also evaluate on a subset of tasks used to evaluate Toolformer~\citep{schick2023toolformer}, in order to compare \sys to a model fine-tuned for tool use.

\paragraph{Details}
We use InstructGPT (text-davinci-002) as the frozen LLM, and Codex as the code generation tool, with temperature set to $0.3$. 
We set the number of seed tasks in the prompt to $N=3$ and use $2$ demonstration programs from each task.
% We extract the final answer from the program based on keywords that the language model is
% prompted to produce at the end (i.e., ``Ans: ").
We measure the preferred scoring metric for each task as in \citet{srivastava2022beyond}, and report performance averaged over 5 runs.
% Additional details on models, metrics, and hyperparameters are in Appendix~\ref{appendix:hyperparameters}. 


\begin{table*}[htb!]
    \centering
    \small
    \begin{tabular}{l|cc|cc|c}
       \toprule
\bf       Task Name (Cluster) & \bf Few Shot & \bf AutoCot & \bf \sys & \bf \sys & \bf GPT-3 \\
       &  &  & \bf w/o Tool Use & & \bf Best\\
       \midrule
       Anachronisms \cellcolor{search}(Search) & 71.3$^5$ & 51.48 & 70.87 & 75.66 & -\\
       % \cellcolor{instructgpt}71.13$^5$\\
       Musique \cellcolor{search}(Search) & 2.03$^5$  & 12.88 & 10.04 & 19.19 &  
       15.2$^3$\\
       Hindu Knowledge \cellcolor{search}(Search) & 85.02 $^5$ & 73.03 & 83.42 & 87.98 & -\\
       % \cellcolor{instructgpt}83.65$^5$\\
       Known Unknown \cellcolor{search}(Search) & 68.90 $^5$ & 56.09 & 80.43 & 80.43 & -\\
       \cdashlinelr{1-6}
       \Gape[.5pt][.5pt]{$\Delta$ with \sys(Search)} & \bf +9.0 & \bf +17.44 & \bf +4.6 & & +4.0\\
       % + 4.0 \\
       \cdashlinelr{1-6}
       Elementary Math QA \cellcolor{arithmetic}(Arithmetic) & 56.40$^7$ & 74.52 & 58.04 & 68.04 & -\\
       % \cellcolor{codex}57.80$^7$\\
       % \cdashlinelr{1-6}
       Aqua-rat \cellcolor{arithmetic}(Arithmetic) & 20.54$^7$ & 34.41 & 36.29 & 54.20 & 54.1$^4$ \\
       % \hdashline
       GSM8K \cellcolor{arithmetic}(Arithmetic) & 7.79$^7$ & 21.99 & 53.4 & 71.00 & 71.6$^4$\\
       Navigate \cellcolor{arithmetic}(Arithmetic) & 60.7$^7$ & 61.7 & 72.4 & 72.4 & 85.90$^1$\\
       \cdashlinelr{1-6}
       $\Delta$ with \sys(Arithmetic) & \bf +30.0 & \bf +18.25 & \bf +11.4 & & -4.7 \\
       % -4.6 \\
       \cdashlinelr{1-6}
       % \cellcolor{instructgpt}67.82$^5$\\
       % \textcolor{red}{76.08} & 
       K'th letter concatenation \cellcolor{string}(String) & 3.2$^5$ & 0.64 & 8.19 & 40.00 & 98.0$^2$ \\
       % \cellcolor{instructgpt}33.20$^5$ \\
       Language games \cellcolor{string}(String) & 35.14$^5$ & 18.58 & 11.19 & 23.08 & -\\ 
       Date Understanding \cellcolor{string}(String) & 37.53$^5$ & 38.90 & 52.05 &  - &  70.41$^1$ \\
       Auto Debugging \cellcolor{code}(Code) & 62.94$^5$ & 38.24 & 55.29 & 62.94 & -\\
       % \cellcolor{instructgpt}65.88$^5$\\
       Code Description \cellcolor{code}(Code) & 97.99$^7$ & 88.67 & 84.67 & 88.00 & -\\
       % \cellcolor{codex}95.33$^7$\\
       Formal Fallacies \cellcolor{freeform}(CoT) & 44.84$^5$ & 56.4 & 64.76 & - &  58.4$^1$\\
       Hyperbation  \cellcolor{freeform}(CoT) & 62.72$^5$ & 55.4 & 80.80 & - & 72.4$^1$\\
       \cdashlinelr{1-6}
       $\Delta$ with \sys (Misc) & \bf +9.6 & \bf +16.4  & \bf +13.7  &  & -15.4\\
       % \cdashlinelr{1-6}
       \midrule
       $\Delta$ with \sys (Overall) & \bf +14.90 & \bf +17.17 & \bf +7.91 & & -9.0 \\
       % -9.0  \\
       \bottomrule
    \end{tabular}
    \caption{\sys performance on tasks in the task library. ($^1$Human-crafted CoT \citep{wei2022chain, suzgun2022challenging}, $^2$Decomposed Prompting~\citep{khot2022decomposed}, $^3$Self-Ask \citep{press2022measuring}, $^4$PoT \citep{chen2022program}, $^5$InstructGPT \citep{ouyang2022training}, $^7$Code-davinci-002 \citep{chen2021evaluating}). (-) For tasks using CoT reasoning, no tool use is used.}
    \label{tab:main_result_table_library}
\end{table*}


\paragraph{Baselines}
\sys proposes an automatic framework to generate multi-step reasoning decompositions and use relevant available external tools within those decompositions.
We compare with the following baselines:% to better understand the source of improvement and whether \sys stacks up against other approaches that use supervision and/or finetuning. 
\setlist{nolistsep}
\begin{itemize}[noitemsep]
    \item \textbf{Few-shot/Direct}: Prompting LLMs with input-output pairs (but no intermediate reasoning). We use 3 examples for BigBench and 5 examples for MMLU, as done in prior work \citep{suzgun2022challenging}. We evaluate this baseline for both, GPT-3 and Codex, and report the higher of the two. 
    \item \textbf{Auto-CoT}: A baseline that automatically generates multi-step reasoning in natural language. A random subset of 5 examples is first used to elicit CoT-style reasoning (\emph{Input + Let's think step-by-step.}). These examples and their generated output form the prompt for other unseen examples of the task.
    % The LLM is prompted with a random subset of 5 examples in the dataset to elicit CoT-style reasoning (\emph{Input + Let's think step-by-step.}).
    This baseline is free-form and does not include tools, and thus allows us to verify the effectiveness of our query language and task library.  We evaluate this baseline for GPT-3.
    \item \textbf{\sys-tool}: \sys with tool-use turned off, i.e. the LLM generates the output of every substep, to verify the gains from tool use.
    \item \textbf{GPT-3 Best}: Best published GPT-3/Codex (175B) result with multi-step decomposition and/or tool use. These often include additional human supervision to decompose reasoning steps, and external tools to boost performance (with carefully constructed prompts).
\end{itemize}
Additional details about baselines and GPT-3 best models are in Appendix~\ref{appendix:baselines}.

% --------------------------------------

% We briefly describe our experimental setup, including the datasets chosen for evaluation, important implementation details, and baseline approaches and state-of-the-art models we compare with .

% \paragraph{Evaluation Datasets}
% In Section~\ref{sec:seed_tasks}, we described how a subset of BigBench tasks that require the most common reasoning skills are chosen as part of the task library. To evaluate \sys, we use the remaining tasks that require those skills as test sets. In total, 15 tasks are used to create the library of train task programs and 19 tasks are used for evaluation. In addition to BigBench tasks, we also evaluate \sys on a random subset of six tasks from the MMLU benchmark - Computer science, astronomy, business, virology, High school geography, and high school mathematics. Finally, we also evaluate on a subset of tasks used to evaluate Toolformer~\citep{schick2023toolformer}, in order to compare our gradient-free approach for automatic tool use with a model fine-tuned for tool use. Performance on MMLU and additional evaluation tasks can help us gauge if our choice of the task library does not overfit to BigBench.


% \paragraph{Metrics and Hyperparameters}
% We use InstructGPT (text-Davinci-002) as the main frozen LLM. 
% With tool use, various parts of this program may be replaced by generations from other models or API outputs. For example, Codex is used for code generation and editing. We set the number of seed tasks in the prompt to $N=3$ and use $2$ demonstration programs from each task. The temperature for GPT-3 and Codex are set to 0.3. We extract the final answer from the program based on keywords that the language model is
% prompted to produce at the end (i.e., ``Ans: "). We measure the preferred scoring metric for each task used in \citet{srivastava2022beyond}. We report performance averaged over 5 runs of \sys. 
% % (\ref{appendix:additional} contains standard deviation results).
% More details on models, metrics, and hyperparameters are in Appendix~\ref{appendix:hyperparameters}.
% % Note that a similar pattern ("The final answer is") is used in the prompts to the Auto-CoT baseline as well. 


% \paragraph{Baselines}
% \sys proposes an automatic framework to generate multi-step reasoning decompositions and use relevant available external tools within those decompositions. We compare with the following baselines to better understand the source of improvement and where \sys stacks up against other approaches that use supervision and/or finetuning. 
% % We compare \sys with AutoCoT~\citep{zhang2022automatic}, which is most similar to our approach in that they generate 
% \setlist{nolistsep}
% \begin{itemize}[noitemsep]
%     \item Few-shot baseline: Prompting LLMs with input-output pairs (no intermediate reasoning)
%     \item Auto-CoT baseline: A baseline that generates automatic CoT-style multi-step reasoning in a free-form natural language (as done in AutoCoT~\citep{zhang2022automatic}). A randomly selected subset of examples in the dataset is used to prompt the LLM to elicit CoT-style reasoning (\emph{Input + Let's think step-by-step.}). Since CoT-style generation is free-form and parsing potential tool use symbols is harder, we don't use tools for this baseline. This baseline specifically measures the effectiveness of a custom query language (and PeG grammar) we use to write programs and parse tool calls;
%     \item \sys without tools: Tool-use is turned off. This baseline specifically ablates for the advantage of tool use over LLM generation alone.
%     \item Comparable Best: Best-known comparable-sized model result for each task that does multi-step decomposition and/or tool use. We survey all works that use GPT-3 and Codex 175B parameter models, along with additional human supervision to decompose reasoning steps and additional external tools to boost performance (with human supervision for what tool to use where and how).
% \end{itemize}
% % For a fair comparison with our work, the Auto-CoT baseline is implemented without additional clustering and filtering of examples used for demonstrations i.e. a random subset of examples in the dataset are used to prompt the LLM to elicit CoT-style reasoning (\emph{Let's think step-by-step}), similar to how we use a random subset of examples to create demonstration programs.

