We begin by providing an overview of the \sys framework (Figure \ref{fig:teaser}).  The end goal of \sys is to use a frozen LLM to generate accurate multi-step reasoning decompositions combined with useful external tools, without explicit supervision for decomposition or tool use. We consider an example test task, physics question answering (PQA), which involves solving a high-school physics word problem using an appropriate formula. We only assume access to a description of the task (a brief instruction) and some input-output pairs.
\sys first searches an existing \emph{task library} (Section~\ref{sec:seed_tasks}) for similar tasks and uses instances of these tasks as few-shot demonstrations to prompt the LLM. The task library is manually constructed for a small collection of seed tasks. For PQA, \sys finds Anachronisms (a search task) and GSM8K (an arithmetic task) among the most similar library tasks.

A demonstration for a library task is written in a specific format, defined by a custom \emph{parsing expression grammar (PeG)} (Section~\ref{sec:peg_grammar}). 
The grammar is defined such that a task instance is decomposed into a sequence of sub-steps. Some of these sub-steps contain symbols corresponding to tools in a \emph{tool library} (Section~\ref{sec:tool_set}).
% (APIs, interpreters, calculators, and other LLMs). 
We refer to these decompositions as programs, since the sequential reasoning steps and symbolic calls to tools are similar to a conventional program with function calls. For instance, programs for anachronisms include calls to the search engine tool, and programs for GSM8K include calls to code generation and execution tools. Figure \ref{fig:nlprogrammer_architecture} shows programs for these two tasks. As in conventional programming, we can parse the program, identify calls to tools, and stop/restart LLM execution when tools are called. 

Programs of related tasks provide few-shot supervision to the LLM --- related sub-steps and tools in these programs can be used by the LLM for cross-task generalization. The LLM generates a similar program for the new task with calls to shared tools that are maybe useful for the new task as well. For PQA, the LLM generates a program that includes calls to both, search and code operations.
The LLM output (output program) is parsed for tool calls during generation. When a tool call is encountered, LLM generation is interrupted to run the tool. Tool output is integrated back into the program, and LLM execution is resumed until the the next tool is identified or the final answer is generated. For PQA, search is used to find the appropriate physics formula, and code generation and execution are used to substitute given values and compute the answer.

The generated programs can also be used for human-in-the-loop feedback. When shown a few generated programs with inaccurate predictions, they can correct sub-step outputs or add additional sub-steps to augment the task library, as shown in Figure~\ref{fig:human_feedback}-(a). They can also redefine tools, add new tool definitions to augment the tool library,
% and change how tool output is integrated back into the program,
as shown in Figure~\ref{fig:human_feedback}-(b).  For PQA, humans add an additional step for adding the appropriate unit of measure to the computed answer. The resultant programs are used to prompt the LLM in a few-shot setting. 
In the following subsections, we describe the implementation of the these components of \sys --- the PeG grammar, the task library, the tool library, and human-in-the-loop feedback --- in greater detail. In Section~\ref{sec:sys_in_action}, we demonstrate with an example how a program is generated and parsed for a new task.

\subsection{Parsing expression grammar}
\label{sec:peg_grammar}
We considered two objectives in designing a specific format to represent multi-step reasoning - coverage over a wide range of NLP tasks and flexibility in representing input arguments and outputs of a wide range of tools.  
We surveyed existing work on human-authored CoT-style prompting and tool-use to create a format that could represent all these prompts and support all the NLP tasks explored in existing work. 
We define a query language \cite{beurer2022prompting} that extends the decomposed prompting format used in \citet{khot2022decomposed}, since it can represent decomposed reasoning steps sequentially and incorporates function calls to tools (LLMs in the case of \citet{khot2022decomposed}) within the individual sub-steps. 

A program in this language consists of a series of \emph{nodes} --- a task input node, several sub-step nodes, and an answer node. 
The input node contains the task name, a simple instruction describing the task, and the input for an instance of the task:  ``\emph{(Physics QA) Answer the high school Physics question.}"
This is followed by a sequence of sub-tasks that accomplish the task over multiple steps. The $i^{th}$ sub-task is represented as a (query, answer) pair formatted as (\emph{$Qi:\dots, \#i:\dots$}). The sub-task query consists of the sub-task name and its arguments (``\emph{Q1: [search] What is the formula for the horizontal component of tension force?}"). The sub-task answer is simply the output of the sub-task (``\emph{\#1: George H. W. Bush's tenure as the 41st president of the US ...}"). 
The program ends with a dummy sub-task with no answer (``\emph{Q3: [EOQ]}"). The answer node represents the final answer used for evaluation (``\emph{Ans: 59 N}") 
Programs for the arithmetic reasoning task (GSM8K) and anachronism detection task are shown in Figure \ref{fig:nlprogrammer_architecture}. This consistent language is used to construct an expandable \emph{task library} consisting of programs for a small set of seed tasks \ref{sec:seed_tasks}. More examples of programs used in the task library can be found in Appendix \ref{sec:appendix}. Furthermore, the language defines an expandable set of symbols that map to external tools in the tool library \ref{sec:tool_set}.

% Explain syntax borrowed from \citep{khot2022decomposed}. Surveyed several human-authored CoT style promtps and came up with this prompt that generally supports most of these prompts and tasks. This allowed us to choose a subset of tools as fix their syntax for calling and how their arguments and output would be formatted.

% OLD ENDS HERE



\subsection{Task Library}
\label{sec:seed_tasks}
% Why Big bench was chosen to sample from. Sampling diverse tasks and task categorization. Human-authored program were written.
We construct a library of programs for a small seed set of tasks from Big-Bench benchmark (\citep{srivastava2022beyond}, Beyond the Imitation Game Benchmark). BIG-Bench is a collaborative benchmark constructed to quantitatively measure the capabilities and limitations of
language models, with over 200 diverse text-based tasks in categories including traditional NLP, mathematics, commonsense reasoning, and question-answering. A subset of these tasks is selected for the library. Our selection criterion is based on maximizing coverage over a wide range of language understanding and reasoning skills required for BigBench. We identify 5 common skills that are generally useful across more than half of the tasks in BigBench \ref{sec:appendix}.
% the following broad skills that are useful for several tasks in BigBench \ref{sec:appendix} ---  search (18), sub-question decomposition (11), arithmetic and algebra (13), code operations (generation, editing, and execution) (20), string operations (13), free-form step-by-step reasoning (10), multiple-choices classification (14), specialized knowledge lookup (6), text tagging/annotation, text generation/editing,  database operations, natural and propositional Logic, machine translation, and reasoning about long text \footnote{The corresponding numbers in () correspond to the number of tasks in BigBench that may require the reasoning skill}. 
We select 3-4 tasks from each if these five skill categories and write decomposition programs for a few instances in these tasks. These decompositions also demonstrate relevant tool calls, arguments and outputs. 
This gives us siz \emph{clusters} of tasks (with overlap), with each cluster representing a key skill required for the tasks in that cluster.
\setlist{nolistsep}
\begin{itemize}[noitemsep]
\item Arithmetic tasks: solving arithmetic and algebra problems.
\item Code tasks: Generate, debug, execute python code.
\item Search and question decomposition tasks: Single or multi-step questions that require search
% \item Multiple-choice tasks: Choose one or more of N options (overlaps with other categories)
\item Free-form reasoning: Explain step-by-step reasoning in natural language (CoT-style)
\item String Operations: Reformat strings, check string entailment.
\end{itemize}
The specific tasks chosen under each cluster and the programs authored for these tasks are detailed in Appendix~\ref{sec:appendix}. All other tasks in BigBench that belong to these clusters are used to test \sys. 





\subsection{Task retrieval} 
\label{section:train_task_selection}
During execution, \sys constructs a dynamic multi-task prompt consisting of programs from N library tasks that are most similar to the new task. We experiment with two simple strategies to select similar tasks. 
 
 \paragraph{Best task-cluster:} We iteratively use tasks from a particular task-cluster in the library. For example, we only use programs from arithmetic tasks as demonstrations in the prompt. The task cluster with the highest performance on the held-out set of examples (~50) is chosen. This strategy requires as many API calls as there are task clusters, and a held-out set of input-output pairs for the new task. Note that no additional supervision is needed for the new task to generate a decomposed  program. 
 
 \paragraph{LLM-based:} Here, we use the LLM to measure task similarity. The LLM is prompted with pairs of tasks. Some pairs contain two tasks from the same cluster and are labeled "Similar" while some pairs don't and are labeled "Not similar". Additionally, we also provide reasoning for the decision --- \emph{``Elementary math QA and GSM8K are related tasks because they both require solving arithmetic word problems"}. 
A task in this prompt is represented by its name, an instruction, and a few input-output pairs. 
More details about this prompt are in Appendix~\ref{sec:appendix}. The LLM is prompted for a decision for every library task paired with the new task. We choose the top-N tasks ranked by the ratio of log probabilities of "Similar" to "Not similar". This strategy requires fewer held-out examples but is prone to high variance in performance based on the tasks chosen in every experimental run. For PQA, the most similar tasks chosen based on the LLM-based similarity are anachronisms and GSM8K. We explore the trade-off between these strategies in Section~\ref{result:perf_improvement}. 
% More effective task selection strategies can be explored in future work.
 
 % While this strategy requires more API calls and a held-out set of input-output pairs, we find that it is consistently better than the LLM-based task similarity approach.


\begin{figure*}
    \centering
    \includegraphics[scale=0.17]{sections/resources/human_feedback.jpeg}
    \caption{Human feedback to \sys shown for (a) PQA where reasoning steps are added to the program and; (b) Word unscrambling where tool library is augmented with a new lookup tool.  }
    \label{fig:human_feedback}
\end{figure*}

\subsection{Tool library}
\label{sec:tool_set}

% \sys facilitates the use of tools that improve performance on tasks in the task library. 
\sys uses the following tools to improve performance on tasks in the task library --- search engine, generating and editing code, and executing code using arithmetic and symbolic solvers (like python packages like sympy, numpy, etc).
% looking up knowledge bases like dictionaries, etc. 
Tool use requires parsing the program, as it is generated, for tool symbols and their arguments. For every identified tool, language model generation is interrupted and the tool is called with its arguments. Tool output is integrated back into the program, either replacing the LLM's generated output or augmenting it. Language model generation is then resumed to complete the program. Replacing or augmenting tool output possibly generates  a different program than what would have been generated if no tool was used.
We briefly discuss these tools, including the symbols used to represent them and their arguments. We also specify how the tool's output is integrated back into the generated program. 
% This creates a new prompt with an incomplete program, and language model generation is resumed.
More details about tool-specific implementation details and other tools added to \sys during feedback (\ref{human_in_loop}) are in Appendix~\ref{appendix:toolise}.

\paragraph{Search}
We used \url{https://serpapi.com}, which provides an API for Google search. 
The argument to search is the sequence generated by the LLM after the sub-task query symbol ``$Qi: [search]$''. We extract answer box snippets when they are available or combine the top-2 search result snippets together. The API's output is appended to the sub-task answer sequence  ``$\#i:\dots$" generated by the LLM. For PQA in Figure~\ref{fig:nlprogrammer_architecture}, the argument (search query) to the API is the input and the query \emph{What is the formula for the horizontal component of tension force?}. The API output (``\emph{... horizontal component (Fx) can be calculated as Ftens*cosine($\theta$) ...}'') is added to the response generated by the LLM. 


\paragraph{Code generation and editing}
We use the Codex \citep{chen2021evaluating} model for code generation and code editing. Arguments for both include the previous sub-task's answer sequence ``$\#i:\dots$" (or the input if $i=1$) and the sequence generated by the LM after the sub-task query symbol ``$Qi: [generate\; python\; code]$". The first argument is the code snippet and the second argument is a multi-line comment in Python used as the instruction for editing/generation. The resultant code snippet is used to replace the LLM generation. For PQA in Figure~\ref{fig:nlprogrammer_architecture}, the code snippet (``\emph{T = 72.0, theta = 35.0, ..., Fx = T*math.cos(radians)}'') is appended to the incomplete program.
% replaces the original code snippet.

% Furthermore, to encourage executable code with consistent variable usage, we also append the sequence "Store your final answer is variable 'ans'".

\paragraph{Code Execution}
We run python code in a virtual python environment with arithmetic, symbolic, and scientific computing packages pre-installed. The arguments to code execute include the previous sub-task's answer sequence ``$\#i:\dots$", which is the python code snippet that requires executing. The other argument is the  sequence generated by the LM after the sub-task query symbol ``$Qi: [execute\; code]$" (which is prepended to the code snippet as a comment). The results of the execution call are used to replace the answer sequence generated by the language model. For PQA in Figure~\ref{fig:nlprogrammer_architecture}, the value of variable ``\emph{Fx}'' is appended to the incomplete program.
% replaces the original answer. The LLM is once again prompted to complete the program and generate the final answer node that terminates the program.
% Again, to encourage executable code with consistent variable usage, we also append the sequence "Store your final answer is variable 'ans'" as a comment. 
% Finally, we prepend a code snippet consisting of useful module and function imports (see Appendix \ref{sec:appendix}). 
% We use the \emph{exec} native python function to execute the code snippet and access the 'ans' local variable if it exists. 

\subsection{Human-in-the-loop feedback}
\label{human_in_loop}
\sys is specifically designed to be amenable to human-in-the-loop feedback, since the framework does not require additional finetuning and generates multi-step reasoning programs that humans can interpret, debug and refine. Furthermore, 
the library of seed tasks and tools can be extended to work on target downstream tasks and use additional useful tools. In this work, we explore human-in-the-loop feedback in a limited setting. 

Humans are provided two affordances with the goal of improving end-task performance --- (a) Program editing to extend the task library; and (b) Improving tool definition and integration to extend the tool library.

For (a), humans are shown 5 random instances of model errors and the corresponding programs generated for these instances. They can edit these programs to correct errors in sub-step command arguments and answers, as well as add additional sub-steps (with arguments and correct answers) to fill-in reasoning steps that they deem are missing in the automatically generated program. For example, in Figure~\ref{fig:human_feedback}-(a), human feedback on the PQA task results in the addition of another reasoning step for adding an appropriate unit of measurement to the final answer. Humans can therefore add additional tasks to the task library after refining their generated programs.
For (b), humans can redefine or add additional tools in the tool library, including changing the manner in which the tool output is integrated into the program. For example, for the word-unscrambling task in Figure~\ref{fig:human_feedback}-(b), humans define a new tool that involves looking up different permutations of the original word in a standard English dictionary. Humans can therefore add additional tools to the tool library, while simultaneously providing demonstrative task programs for the new tool in the task library.

% Details,
In this work, we study human feedback in a limited setting. Authors either rewrite programs in text files or add additional tools programmatically to integrate them into the parser. Feedback was only solicited twice.
In Appendix~\ref{appendix:human_feedback}, we describe the human-in-the-loop feedback framework in greater detail, and also describe the different edits and augmentations made by humans for 18 tasks for which human feedback results are reported.