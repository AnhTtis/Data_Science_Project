% a. In context learning: no need to retrain, can use an API, can get new task working very quickly
% b. However, LM is often bad at stuff that requires multiple reasoning steps.
% Also, there are things it's just bad at or can't do: anything that requires symbolic manipulation (math, strings) or current information

% Previous
% Large language models (LLMs) have achieved impressive zero- and few-shot results on a variety of natural language processing tasks \citep{brown2020language,chowdhery2022palm} via in-context learning \citep{xie2021explanation} --- providing instructions and a few training examples as prompts. However, in-context learning underperforms smaller finetuned models on NLP tasks that require complex multi-step reasoning \cite{liu2022few}. LLMs also have inherent limitations in their abilities - mathematical skills \citep{patel-etal-2021-nlp}, up-to-date information on recent events \citep{komeili-etal-2022-internet}, specialized domain knowledge \citep{gozalo2023chatgpt}, and hallucinating facts while ignoring local context \citep{ji2022survey}. 
% In this paper, we propose a framework for in-context learning for complex multi-step reasoning tasks --- \sys.
% End of previous

% Marco's version
%In-context learning allows large language models (LLMs) to quickly adapt to new tasks without any additional training, by using natural language instructions and a few demonstrations as a prompt~\citep{xie2021explanation}.While this allows users to get new tasks working very quickly \citep{brown2020language, chowdhery2022palm} without annotating large datasets (or even without hosting the LLM itself, since many are available through APIs), there are severe performance limitations around multi-step reasoning \cite{liu2022few}, math \cite{patel-etal-2021-nlp}, having up-to-date information \cite{komeili-etal-2022-internet}, hallucination \cite{ji2022survey}, and others.
%%%%--- End of marco's version




% 2. Status quo
% a. Chain of Thought & friends help with multiple reasoning steps by \emph{decomposing} it into steps.
% b. Tool use helps with external stuff, paste results back into prompt.
% c. However, tool use right now either requires fine tuning, or expert-crafted prompts for specific tools

%% PRevious
% Decomposing prompts into multiple intermediate steps (commonly referred to as chain-of-thought (CoT) style prompting) has proven to be an effective technique for in-context learning on such tasks \citep{weichain, zhou2022least, wang2022self, press2022measuring, khot2022decomposed, arora2022ask}, especially when LLMs are already finetuned on a collection of datasets phrased as instructions \citep{chung2022scaling}.
% Recent work has also proposed using external tools and APIs to improve performance on intermediate reasoning steps - like code generation and interpreters \citep{gao2022pal, chen2022program}, search engines \citep{press2022measuring, weichain}, and machine translation systems \citep{schick2023toolformer}. 
% % Line to explain how tools are used with in-context learning.
% Existing approaches that combine CoT-style prompting with tool use rely on hand-crafted prompts (often with task-specific multi-step reasoning) and explicit human supervision on which tools to use where, and how to use incorporate their results in the prompt. Scaled fine-tuning of LLMs for tool-use 
% \citep{schick2023toolformer}
% % in MLM/next token prediction 
% is another recent approach for automating the use of a few tools. However, extending the model to more tasks and tools may require additional supervision and finetuning.   
% \sys provides a flexible framework to automate both, generating multi-step reasoning \emph{and} decisions about tool use for a multitude of downstream tasks, without any additional training of the LLM.
%% End of previous

\begin{figure}[htb!]
    \centering
\includegraphics[scale=0.50]{sections/resources/teaser2.jpeg}
    \caption{\sys generates automatic multi-step decompositions for new tasks by retrieving decompositions of related tasks in the \emph{task libray}, and selecting and using tools in the \emph{tool library}. Humans can edit decompositions to improve performance.}
    \label{fig:teaser}
\end{figure}

% Marco's version
Large language models (LLMs) support complex reasoning, especially when prompted to mimic a chain of thought (CoT) for multi-step reasoning \citep{weichain, zhou2022least, wang2022self, press2022measuring, khot2022decomposed, arora2022ask} or provided with access to tools (e.g. a calculator or QA model) to enable more complex reasoning steps \citep{gao2022pal, chen2022program, press2022measuring, weichain, schick2023toolformer}. 
%To address these limitations, recent work has proposed various methods in the line of chain-of-thought prompting (CoT), which encourage the LLM to \emph{decompose} predictions into multiple intermediate steps by providing demonstrations of decompositions \citep{weichain, zhou2022least, wang2022self, press2022measuring, khot2022decomposed, arora2022ask} or using a prompt like ``Letâ€™s think step by step'' (AutoCOT, \cite{zhang2022automatic,kojima2022large}).
%Recent work has also proposed ways to use external tools and APIs (e.g. search engines and code interpreters) to improve these intermediate reasoning steps \citep{gao2022pal, chen2022program, press2022measuring, weichain, schick2023toolformer}.
However, existing methods for combining chained reasoning with tool use are difficult to extend to new tasks and tools. They require fine-tuning or prompt-engineering tailored for each task \cite{parisi2022talm} or tool \cite{schick2023toolformer}.
%both of which limit generalization to new tasks and integration of new tools. 
%%%%--- End of marco's version


% \begin{figure}
%     \centering
%     \includegraphics[scale=0.35]{sections/resources/difference.jpg}
%     \caption{How \sys differs from other approaches.}
%     \label{fig:teaser}
% \end{figure}

% 3. What we did
% a. In this paper, we present \toolname{}, a framework that automaticaly generates multi-step reasoning (decompositions)
% for instances of new tasks, selecting and using the most appropriate available tools.
% b. When given an instance of a new task, \toolname{} uses instances from related tasks retrieved from a \emph{task library} as demonstrations.
% c. These demonstrations follow a flexible but structured query language (to allow for easy parsing), which includes calls to external tools from a \emph{tool library}.
% d. As a result, the LLM is shown how to decompose and use tools, and uses those
% demonstrations to decompose instances of \emph{new tasks} appropriately, calling the right tools in intermediate steps.
% e. This decomposition has the further advantage of allowing for easy human feedback on the reasoning, e.g. by adding additional substeps, or including calls to new tools when needed

%% Previous
% The \sys framework automatically generates multi-step reasoning (decompositions) for instances of new tasks, only assuming access to a few input-output pairs of the task.
% When given an instance of a new task, \sys composes an in-context prompt from instances of related tasks that are retrieved from a \emph{task library} of decomposition demonstrations. These decompositions follow a flexible query language \citep{beurer2022prompting} that enumerates reasoning steps sequentially and has function names that map to external tools like python interpreters, Codex\citep{chen2021evaluating}, or Google search (\emph{library of tools}). This enables \sys to parse the decompositions generated for the new task instance, identify calls to tools and call them, and integrate their output into the prompt to resume generation (see Figure \ref{fig:teaser}). 
% \sys introduces a multi-task prompt setup where the LLM is shown how to decompose and use tools for multiple related tasks, and it uses those demonstrations to decompose instances of \emph{new tasks} appropriately, calling the right tools in intermediate steps. Furthermore, the framework facilitates human feedback --- humans can observe generation errors in the decomposition, fix syntactic and structural issues in generations, change outputs of sub-steps, and add additional tool use to boost performance.
%% End of Previous

%%%---- Marco's version
In this paper, we present \textbf{A}utomatic \textbf{R}easoning and \textbf{T}ool use (\sys), a framework that automatically generates decompositions (multi-step reasoning) for instances of new tasks. The framework also selects and uses the most appropriate available tools (like search engines, and code execution) in individual steps.
Given a new task, \sys retrieves demonstrations of related tasks from a \emph{task library} to enable few-shot decomposition and tool use.
These demonstrations a flexible but structured query language \cite{beurer2022prompting}, such that it is easy to parse intermediate steps, stop generation to call external tools, and resume it after including the output of such tools (Figure \ref{fig:teaser}).
%In other words, \sys provides the LLM with demonstrations of how to decompose instances of several related tasks, and how to select and use any tool from the \emph{library} represented in the demonstrations.
In this way, the model can often generalize from the examples to perform a new task zero shot. Additionally, if it makes mistakes, users can fix any problems in the reasoning chain or add new tools, by simply updating the task and tool libraries, and providing new demonstrations where necessary (e.g. for the task at hand).
%%%%--- End of marco's version

% It consists of a library of diverse training tasks (from BigBench) that require multi-step reasoning, a subset of which are used to provide in-context supervision for the new test task. It also consists of a library of external tools, such as a python interpreter, Codex\citep{chen2021evaluating}, or Google search, that can be leveraged to improve performance on individual sub-steps. \sys prompts the \emph{frozen} LLM with multi-step reasoning demonstrations of similar training tasks, that are formatted using a flexible query language \citep{beurer2022prompting}. This allows \sys to parse the generation, identify calls to tools and call them, and integrate their output back into the generation to re-prompt the LLM (see Figure \ref{fig:teaser}). Furthermore, the framework facilitates human-in-the-loop feedback --- humans can observe generation errors, fix syntactic and structural issues in generations, evaluate specific sub-steps, change outputs of sub-steps, and add additional tool use to boost performance.

%% Previous
% We use 15 diverse seed tasks from BigBench~\citep{srivastava2022beyond} to construct the library of demonstrations and evaluate on 21 new test tasks from the benchmark, 5 additional tasks from MMLU, and tasks used in related work on generating automatic decompositions (SQUAD, TriviaQA, SVAMP, MAWPS). \sys significantly improves in-context learning performance on new test tasks over approaches for automatically generating reasoning chains that rely on CoT-style prompting (\citep{zhang2022automatic}AutoCoT) --- improving on 24 of the 26 test tasks from BigBench and MMLU  by 8-118\%. 
% Tool use in particular improves performance by 44\% on average compared to when it is turned off for these tasks. When compared to in-context learning performance on comparable-sized LLMs with additional supervision for prompt construction and/or tool use, \sys improves by 5\% on average across test tasks with no explicit supervision for multi-step reasoning or tool use.
% % For tasks where \sys underperforms the best-known in-context learning performance, 
% Human-in-loop feedback further helps boost performance on tasks where \sys underperform comparable supervised approaches, improving on these tasks by 37.6\% on average.
%% end of Previous

%%% --- Marco's version
We construct a task library for 15 diverse BigBench~\citep{srivastava2022beyond} tasks, and evaluate \sys on 19 unseen \emph{test tasks} from BigBench, 6 MMLU tasks, and various tasks used by related work on tool use (SQUAD, TriviaQA, SVAMP, MAWPS).
\sys consistently outperforms automatically generated CoT reasoning chains (AutoCoT) on 23 / 25 BigBench and all MMLU tasks (by an average of over 22 percentage points).
% When compared to state-of-the-art work using a similar-sized LLM and with additional supervision in the form of human labels or expert-crafted prompts, \sys still presents an improvement (5\% on average), despite using no additional supervision. 
Tool-use in particular improves performance by an average of over 12.5 percentage points, as compared to when no tools are allowed.
% For 12/25 tasks, \sys overperforms by ~10 \% points and for 12/25 it underperforms by ~14 \% pts.
% For 13 / 25 tasks \sys underperforms the comparable best-known results for GPT3 in approaches that use tools or human supervision for decompositions, especially on tasks that require algorithmic and arithmetic reasoning with higher rates of code generation errors. However, a small amount of human feedback brings accuracy up to or higher than these approaches.
\sys improves over direct few-shot prompting by 10.8\% points on average across BigBench and MMLU tasks. Improvements are particularly notable on tasks requiring arithmetic and algorithmic reasoning, where \sys improves on average over direct few-shot prompting by 12.5\% points and previous best-known results for GPT3 that use supervision for decomposition and/or tool use by 6.1\% points. 
% \sys underperforms on text editing tasks due to higher error rates in editing code.
 

%Overall, \sys is able to generate high-quality decompositions (with tool use) for new tasks without any retraining or fine-tuning of the LLM, yielding accuracies much stronger than automatic baselines, and comparable to baselines that rely on further supervision.
Moreover, \sys enables easy human intervention and improvement of the reasoning process by simply updating the task and tool libraries with new demonstrations, making it very easy to improve performance on any specific task with minor human feedback.
On 12 test tasks for which we provide additional human feedback, \sys surpasses the best-known results for GPT3 by an average of over \emph{20\% points}.
%%%%--- End of marco's version

% \sys makes a significant contribution to the line of work on automatic generation of interpretable language prompts for LLMs. While prior work has focused on automatically generating task instructions \cite{zhou2022large} for simple NLU tasks\citep{honovich2022instruction} and multi-step reasoning chains for a small subset of analytical tasks in BigBench\citep{srivastava2022beyond}, \sys works on larger variety tasks from the BigBench and MMLU \citep{hendrycks2020measuring} benchmarks, while also supporting automatic tool use. 
% % Since models are now leveraged for generating multi-step reasoning in in-context learning with the potential use of external APIs and models, 
% Recent work on scaling up instruction finetuning  with CoT training data \cite{chung2022scaling} and explicitly finetuning LLMs for tool use \cite{schick2023toolformer} have been very effective strategies to improve performance for in-context multi-step reasoning with tool-use. While \sys does not explicitly use additional finetuning of LLMs on CoT training data or tool use, we hypothesize that such models may improve the performance of the \sys framework if used in place of the frozen LLM. 
% % Furthermore, while finetuning on specific reasoning formats or tool-use libraries may potentially make these models rigid. 
% \sys is also flexible and extensible - the library of training tasks and tools can be expanded to support a larger array of APIs and downstream tasks in the future. An important contribution of NLprogrammer in the human-in-the-loop feedback literature \citep{bai2022training, wu2022ai} is that it naturally facilitates human feedback that can iteratively improve task performance. We enlist contributions of \sys as follows:

% \begin{itemize}
%     \item Zero-shot adaptation of multi-step reasoning to new tasks without significant prompt engineering by humans or explicit instruction finetuning on the training tasks.
%     \item Prompt query language allows for automatic parsing and calls to tools to improve performance
%     \item Extensible framework: Library of tools and training tasks can be expanded to support custom tasks and tools
%     \item  Humans can debug generation and extend framework to improve performance.
% \end{itemize}

% \paragraph{What problem are we solving and why its important. What do the current approaches lack}
% CoT, instructions and decomposed prompts are subject to manual prompt engineering skills. Moreover, humans have to examine task instances, identify and define task-level strategies to come up with decomposed instructions. Humans have to also identify and define when certain sub-steps can be delegated to external tools or other LLMs. We need to built a framework that works in the same low-data paradigm; and can reduce this human effort, while also facilitating humans to arrive at better decomposed instructions.

% A recent line of work has proposed automatic natural language instruction generation and automatically generating CoT using LLMs themselves. The former still works on relatively simple tasks and the latter has not been extensively evaluated on a large range of language tasks and requires a lot of data to work. Furthermore, we really want to be able to leverage tool use and human-in-the-loop feedback so that our framework is extendable and can always be improved with some additional human effort. This is the gap that \sys fills.

% \paragraph{Results}

