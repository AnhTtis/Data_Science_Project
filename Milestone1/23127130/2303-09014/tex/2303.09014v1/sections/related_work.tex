
\paragraph{Scaled finetuning for low-resource adaptation}
% Instruction finetuning and in-context finetuning, all human prompt engineering papers.
Recent work has shown that finetuning LLMs on a broad range of public NLP datasets (with prefixed instructions) is an effective technique for cross-task generalization \citep{mishra2021cross, sanh2021multitask, khashabi2020unifiedqa,wei2021finetuned} in both the zero-shot and few-shot settings.
\citet{ouyang2022training} show that aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback for desired model behavior (InstructGPT) further improves in-context learning performance on complex NLP tasks.
\citet{chung2022scaling} show that finetuning on an aggregated mixture of tasks (T0, CoT, dialog, and code datasets) together with scaling models to 540B parameters achieves state-of-the-art in-context learning performance on several benchmarks such as BigBench and MMLU.
\sys uses API access to InstructGPT and Codex (LLM finetuned on code \citep{chen2021evaluating}) to leverage their emergent in-context learning abilities.
Future improvements in scaled finetuning in LLMs will likely improve the performance on \sys.
 

\paragraph{Prompting with intermediate reasoning steps}
% APE, AutoCOT
Chain-of-thought (CoT) prompting \citep{wei2022chain,suzgun2022challenging} is a popular gradient-free technique that encourages LLMs to generate intermediate reasoning steps prior to the final answer, with multiple task-specific variants (e.g. Least-to-most prompting \citep{zhou2022least}, Self-Ask \citep{press2022measuring}, Ask-me-anything \citep{arora2022ask}, Successive prompting \citep{dua2022successive}, decomposed prompting \citep{khot2022decomposed}).
% This is useless here
% Combining CoT-style prompting with sampling of multiple outputs (Self-consistency \citep{wang2022self}) has shown to be the most effective in-context learning approach for complex NLP tasks.
While such prompts were initially hand-crafted, recent work~\citep{kojima2022large} showed that LLMs can generate CoT-style multi-step reasoning in a zero-shot manner, when prompted with the prefix ``Let's think step-by-step". \citet{zhang2022automatic} use LLMs to automatically generate such CoT-style prompts---AutoCoT---which are competitive with hand-crafted prompts in their performance on arithmetic and commonsense reasoning tasks. We compare \sys, CoT and AutoCoT in Table~\ref{tab:related_work_comparison}. 
% Similarly, \citet{zhou2022large} have shown that LLM-generated single-step instructions are competitive with human-authored ones for relatively simple NLP tasks. 
\sys builds on this line of work, introducing a common language that enables cross-task demonstrations and flexible and extensible tool use, improving accuracy of intermediate reasoning steps.
% While LLMs can potentially generate complex reasoning chains without explicit supervision, models are often inaccurate on the intermediate reasoning steps, resulting in cascading errors.  
% \sys aims to automatically generate more \emph{accurate} multi-step reasoning for complex tasks.

% \begin{figure}[]
%     \centering
% \includegraphics[scale=0.50]{sections/resources/related_work_comparison.jpeg}
%     \caption{Comparing \sys with related approaches for mult-step reasoning and tool-use.}
%     \label{fig:related_work}
% \end{figure}

\begin{table}[]
    \centering
    \small
    \caption{Comparing \sys with related approaches for multi-step reasoning and tool-use}
    \begin{tabular}{l|cccc}
    \toprule
    Feature   &  CoT & Auto & Tool- & \sys \\
       &   & CoT & former &  \\
    \midrule
     Multi-step reasoning   & \checkmark & \checkmark & &  \checkmark\\
     Limited supervision   & & \checkmark & \checkmark & \checkmark \\
     Tool use   & & & \checkmark &  \checkmark \\
     Extendable libraries   & & & &  \checkmark \\
     Cross-task transfer & & \checkmark & \checkmark & \checkmark \\
     Human feedback & \checkmark & & & \checkmark \\
    \bottomrule
    \end{tabular}
    \label{tab:related_work_comparison}
\end{table}


\paragraph{Tool Use}
% PAL, ProgPrompt, SelfAsk, and Toolformer
% There is growing interest in teaching LLMs to use external tools such as search engines \cite{komeili-etal-2022-internet, thoppilan2022lamda,lazaridou2022internet,shuster2022language}, web browsers \citep{nakano2021webgpt}, calculators \citep{thoppilan2022lamda,cobbe2021training}, translation systems \citep{thoppilan2022lamda} and python interpreters \citep{gao2022pal,chen2022program}.
There is growing interest in overcoming LLM limitations with external tools such as search engines, web browsers, calculators, translation systems, and python interpreters \citep{komeili-etal-2022-internet, thoppilan2022lamda,lazaridou2022internet,shuster2022language, nakano2021webgpt, thoppilan2022lamda,cobbe2021training, 
 thoppilan2022lamda, gao2022pal,chen2022program}.
% Tools are particularly helpful in improving the accuracy of sub-steps in a multi-step reasoning \emph{decomposition} for the task.
Most of these approaches either require large amounts of human supervision \citep{thoppilan2022lamda,komeili-etal-2022-internet} or carefully constructed prompts tailored to specific tasks and particular tools.
An alternative line of recent work uses self-supervision to teach LLMs to use search, translation, and a calculator \cite{schick2023toolformer}---Toolformer.
In contrast, since \sys does not require any additional training or tool-specific prompts, it allows users flexibility both in terms of replacing the underlying LLM (e.g. when a new version of GPT-3 is released), and in replacing or adding \emph{new tools} (either general-purpose tools or tools that are important for a specific task of interest). We compare \sys and Toolformer in Table~\ref{tab:related_work_comparison}.
% Comparison of \sys with \citet{schick2023toolformer} in Section~\ref{sub:toolformer_comparison} suggest that replacing the frozen LLM in \sys with such models can further improve out-of-the-box performance of the framework. 
In Section~\ref{human_in_loop}, we show how human-in-the-loop feedback --- analyzing and debugging LLM generations and extending tool-use --- can provide a large boost in the performance of \sys while also extending it with new tools. This built-in feedback loop and adaptive capability of \sys extends the capabilities of LLMs that are finetuning to follow instructions and use tools.

% Recently, \citep{schick2023toolformer} have introduced a self-supervised approach to learn how and when to
% use tools without requiring a task-specific prompt.
% They show that a much smaller pretrained LLM (GPT-J with 6.7B parameters) can achieve stronger zero-shot results with tool-use than the larger GPT-3 on tasks that require search and translation. Similarly, \cite{parisi2022talm} use a self-supervised objective to teach a model to use a calculator and a search engine, in settings where the model is finetuned for downstream tasks.
% On the other hand, \sys works in the gradient-free setting to learn how and when to use tools, making it more flexible to adapt to new tools and tasks without additional finetuning. Our experimental results from Section~\ref{sub:toolformer_comparison} suggest that replacing the frozen LLM in \sys with such models can further improve out-of-the-box performance of the framework. In Section~\ref{sub:human-in-the-loop}, we show how human-in-the-loop feedback --- analyzing and debugging LLM generations and extending tool-use --- can provide a large boost in performance of \sys while also extending it with new tools. This built-in feedback loop and adaptive capability of \sys extends the capabilities of LLMs that are finetuning to follow instructions and use tools.

\begin{figure*}[]
    \centering
    \includegraphics[scale=0.24]{sections/resources/model_architecture_part1.pdf}
    % \includegraphics[scale=0.17]{sections/resources/human_feedback.jpeg}
    \caption{A run-through of \sys on a new task, Physics QA. (A) Programs of related tasks like anachronisms and Math QA provide few-shot supervision to the LLM --- related sub-steps and tools in these programs can be used by the LLM for cross-task generalization (shown in purple). (B) Tool use: Search is used to find the appropriate physics formula, and code generation and execution are used to substitute given values and compute the answer (shown in orange).}
    % (C) Human feedback to edit steps in the program (shown in blue)}
    \label{fig:nlprogrammer_architecture}
\end{figure*}


% \paragraph{Human-in-loop feedback}
% Another line of work closely related to \sys is human-LLM interactions, where interfaces are designed to facilitate humans to accomplish complex tasks using LLMs. \citet{wu2022ai} introduce AI chains ---  chaining multiple LLM runs together (with the
% output of one step is the input to the next). In follow-up work, \citet{wu2022promptchainer} design an interactive e-interface for visually programming AI 
% chains. Users can debug sub-steps in the chain, reducing cascading errors, and augment the LLM-generated chain with additional functionality. 
% \sys similarly facilitates humans to iteratively improve downstream task performance by analyzing and debugging LLM generations and extending tool-use.
% TBD : AIChains, PromptChainer, Prompt Interpreter (Query language)