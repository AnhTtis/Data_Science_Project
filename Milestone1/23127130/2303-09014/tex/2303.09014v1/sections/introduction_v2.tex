% Marco's version
% In-context learning allows large language models (LLMs) to quickly adapt to new tasks without any additional training, by using natural language instructions and a few demonstrations as a prompt~\citep{xie2021explanation}. While this allows users to get new tasks working very quickly \citep{brown2020language, chowdhery2022palm} without annotating large datasets (or even without hosting the LLM itself, since many are available through APIs), there are severe performance limitations around multi-step reasoning \cite{liu2022few}, math \cite{patel-etal-2021-nlp}, having up-to-date information \cite{komeili-etal-2022-internet}, hallucination \cite{ji2022survey}, and others.
%%%%--- End of marco's version


\begin{figure}[htb!]
    \centering
\includegraphics[scale=0.78]{sections/resources/teaser2.pdf}
    \caption{\sys generates automatic multi-step decompositions for new tasks by selecting decompositions of related tasks in the \emph{task libray}~(A) and selecting and using tools in the \emph{tool library} alongside LLM generation~(B). Humans can optionally edit decompositions (eg. correcting and editing code) to improve performance~(C).}
    \label{fig:teaser}
\end{figure}

% Marco's version

In-context learning allows large language models (LLMs) to quickly adapt to new tasks simply by using natural language instructions and a few demonstrations as a prompt to the LLM~\citep{xie2021explanation, brown2020language, chowdhery2022palm}. 
While this circumvents annotating large datasets or even hosting the LLM itself (since many are available through APIs), there are severe performance limitations around multi-step reasoning \cite{liu2022few}, math \cite{patel-etal-2021-nlp}, having up-to-date information \cite{komeili-etal-2022-internet}, and others.
To address these limitations, recent work proposes prompting LLMs 
% Large language models (LLMs) support complex reasoning, especially when prompted 
to mimic a chain of thought (CoT) for multi-step reasoning \citep{weichain, zhou2022least, wang2022self, press2022measuring, khot2022decomposed, arora2022ask} or providing them with access to tools (e.g. a calculator or QA model) to enable more complex reasoning steps \citep{gao2022pal, chen2022program, press2022measuring, weichain, schick2023toolformer}.
%To address these limitations, recent work has proposed various methods in the line of chain-of-thought prompting (CoT), which encourage the LLM to \emph{decompose} predictions into multiple intermediate steps by providing demonstrations of decompositions \citep{weichain, zhou2022least, wang2022self, press2022measuring, khot2022decomposed, arora2022ask} or using a prompt like ``Letâ€™s think step by step'' (AutoCOT, \cite{zhang2022automatic,kojima2022large}).
%Recent work has also proposed ways to use external tools and APIs (e.g. search engines and code interpreters) to improve these intermediate reasoning steps \citep{gao2022pal, chen2022program, press2022measuring, weichain, schick2023toolformer}.
However, existing methods for chained reasoning with tool use are difficult to extend to new tasks and tools, requiring fine-tuning or prompt-engineering tailored for a specific task \cite{parisi2022talm} or tool \cite{schick2023toolformer}.
%both of which limit generalization to new tasks and integration of new tools. 
%%%%--- End of marco's version


%%%---- Marco's version
In this paper, we present \textbf{A}utomatic \textbf{R}easoning and \textbf{T}ool use (\sys), a framework that automatically generates decompositions (multi-step reasoning) for instances of new tasks. The framework also selects and uses the most appropriate available tools (like search engines, and code execution) in individual steps.
Given a new task, \sys retrieves demonstrations of related tasks from a \emph{task library} to enable few-shot decomposition and tool use.
These demonstrations follow a flexible but structured query language \cite{beurer2022prompting}, such that it is easy to parse intermediate steps, stop generation to call external tools, and resume it after including the output of such tools (Figure \ref{fig:teaser}).
%In other words, 
\sys provides the LLM with demonstrations of how to decompose instances of several related tasks, and how to select and use any tool from the \emph{tool library} that is represented in these demonstrations.
This encourages the model to generalize from demonstrations to decompose a new task and use tools in appropriate places, zero-shot.
It also enables users to fix any mistakes in the reasoning chain or add new tools by simply updating the task and tool libraries, providing new demonstrations where necessary (e.g. for the task at hand).
%%%%--- End of marco's version

%%% --- Marco's version
We construct a task library for 15 diverse BigBench~\citep{srivastava2022beyond} tasks, and evaluate \sys on 19 unseen \emph{test tasks} from BigBench, 6 MMLU tasks, and various tasks used by related work on tool use (SQUAD, TriviaQA, SVAMP, MAWPS).
\sys consistently matches or outperforms automatically generated CoT reasoning chains on 32 / 34 BigBench and all MMLU tasks, by an average of over 22 percentage points. 
% When compared to state-of-the-art work using a similar-sized LLM and with additional supervision in the form of human labels or expert-crafted prompts, \sys still presents an improvement (5\% on average), despite using no additional supervision. 
Tool-use in particular improves performance on test tasks by an average of over 12.3 percentage points, as compared to when no tools are allowed (Table~\ref{tab:main_result_table_test}).
% For 12/25 tasks, \sys overperforms by ~10 \% points and for 12/25 it underperforms by ~14 \% pts.
% For 13 / 25 tasks \sys underperforms the comparable best-known results for GPT3 in approaches that use tools or human supervision for decompositions, especially on tasks that require algorithmic and arithmetic reasoning with higher rates of code generation errors. However, a small amount of human feedback brings accuracy up to or higher than these approaches.
\sys improves over direct few-shot prompting by 10.8\% percentage points on average across unseen BigBench and MMLU tasks. 
Improvements are particularly notable on \emph{unseen} tasks requiring arithmetic and algorithmic reasoning, where \sys improves over direct few-shot prompting by 12.5\% and previous best-known results for GPT3 that use supervision for decomposition and/or tool use by 6.1\% percentage points (Table~\ref{tab:main_result_table_test}).  
% \sys underperforms on text editing tasks due to higher error rates in editing code.
 % \sameer{consider putting one of these numbers in the abstract}

%Overall, \sys is able to generate high-quality decompositions (with tool use) for new tasks without any retraining or fine-tuning of the LLM, yielding accuracies much stronger than automatic baselines, and comparable to baselines that rely on further supervision.
Finally, \sys enables human intervention and improvement of the reasoning process by simply updating the task and tool libraries with new demonstrations, making it very easy to improve performance on any specific task with minor human feedback.
On 12 test tasks, \sys with additional human feedback surpasses the best-known results for GPT3 by an average of over \emph{20\% points} (Table~\ref{tab:model_improvements}).\footnote{Code is available at \url{https://github.com/bhargaviparanjape/language-programmes/}}
%%%%--- End of marco's version

