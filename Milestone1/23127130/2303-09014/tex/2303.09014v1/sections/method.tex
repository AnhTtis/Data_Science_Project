

% \subsection{Overview of \sys Framework}
% We begin by providing an overview of the \sys framework (Figure \ref{fig:nlprogrammer_architecture}). \sys operates in the gradient-free in-context setting setup. The end goal is to prompt the LLM $\mathcal{L}$ to generate accurate  multi-step decompositions with calls to relevant and useful tools for a set of test tasks $\mathcal{D} = \{D_1, D_2,\dots,\}$. We assume a seed set of tasks  $\mathcal{S} = \{S_1,S_2,\dots,\}$ which are used to compose the prompt to $\mathcal{L}$. Additionally, \sys also supports a set of tools $\mathcal{T} = \{T_1,T_2\dots,\}$ (APIs, interpreters, calculators, and other LLMs) that can improve intermediate sub-step accuracy. Both $\mathcal{S}$ and $\mathcal{T}$ can be extended to include additional seed tasks and 
% specialized tools that may be needed to improve accuracy on $\mathcal{D}$ or other downstream tasks. In order to encourage cross-task generalization of sub-steps and tools used in $S$ to $D$, we introduce a simple yet flexible query language \cite{beurer2022prompting} based on a custom parsing expression grammar (PEG). The query language is formatted to sequentially enumerate sub-steps and represent tool names, their arguments, and outputs. A string belonging to this query language is a \emph{program} that can be parsed and interpreted.  
% For tasks in $\mathcal{S}$, we hand-craft programs for a few instances per task. These constitute a  \emph{libary} of demonstration programs. Similarly, the parsed program consists of symbols that can be mapped to tools in the \emph{Tool library} --- a small subset of external APIs, models, and programs. When prompted with a subset of similar seed task programs (multi-task prompt as opposed to a single-task prompt), \sys generates a program for a new task in $\mathcal{D}$. The generated programs are parsed for tools (like search, code generation, etc) and tool arguments, tools are executed, and their outputs are integrated back into the program. The LLM is reprompted to generate the remaining program, conditioned on the outputs of used tools. Each part of the framework is discussed in greater detail in the following sections.

%% Marco's version starts here
% End goal: use a frozen LM to decompose without explicit supervision, with tool use
% Example task: Physics question answering.
With \sys, a frozen LLM decomposes instances of a new task into multiple steps (using external tools whenever appropriate), despite not having explicit supervision for decomposition or tool use.
In this section, we present an overview of \sys, followed by more thorough descriptions of each individual component.
We use the Physics Question Answering (PQA) task as a running example, which consists of high-school physics problems.


\subsection{Overview}
\label{sec:overview}
% Input: description and a few input output pairs (without decomposition!)
In Figure \ref{fig:nlprogrammer_architecture}, \sys is presented with a new task description and input instance. We also assume access to a few input-output pairs (not shown), with no decomposition or tool use supervision.


% Prompt building: demonstration retrieval from a \emph{task library}
\paragraph{Prompt building.} \sys retrieves similar tasks from a \emph{task library} (Figure \ref{fig:nlprogrammer_architecture}(A); Section \ref{sec:seed_tasks}), and adds instances of those tasks as demonstrations in the prompt.

A demonstration in the task library is written in a specific format, defined by a custom \emph{parsing expression grammar (PeG)} (Section~\ref{sec:seed_tasks}). 
The grammar is defined such that each task instance is decomposed into a sequence of sub-steps. Some of these sub-steps contain symbols corresponding to tools in a \emph{tool library} (Section~\ref{sec:tool_set}).
% (APIs, interpreters, calculators, and other LLMs). 
We refer to these decompositions as programs, since the sequential reasoning steps and symbolic calls to tools are similar to a conventional program with function calls. 

% Marco start
% The demonstrations in the task library follow a specific format, where each instance is decomposed into a sequence of sub-steps that can be parsed by a \emph{parsing expression grammar} (PeG). They also include calls to external tools (e.g. search) in a \emph{tool library} (\ref{sec:tool_set}).
% % Note that some steps are generated by the LLM itself rather than external tools.
% Since reasoning steps and tool calls are similar to a conventional program, we refer to each full decomposition as a \emph{program}.
% Marco end
The resultant prompt consists of programs from related tasks and teaches the LLM how to effectively decompose instances of a new task---related sub-steps and tools in these programs can be used by the LLM for cross-task generalization.
% into step-by-step programs, with tool calls in intermediate steps.

In Figure \ref{fig:nlprogrammer_architecture}(A), the demonstrations include calls to both search and code tools.

% Generation: stop generation, call tools, etc. If a tool is not in the tool library, gpt-3 juts does it
\paragraph{Generation.} At generation time (Figure \ref{fig:nlprogrammer_architecture}(B)), the LLM writes its own program. \sys parses the program as it is generated, and pauses generation whenever a tool call is encountered in the generated text, resuming generation after the tool is called and its output is integrated back into the program. As illustrated in the figure, a search engine is used to find the appropriate physics formula, and then the LLM uses code generation and execution to substitute the given values and compute the answer.



\paragraph{Human feedback (optional).} Humans can add new decomposition demonstrations to the task library, or add/edit tools in the tool library in order to improve performance on a particular task of interest, or in general. In Figure \ref{fig:human_feedback}(C) a user corrects a specific program by including a step that adds the unit of measurement, and adds this (modified) program to the task library.
While most of our experiments do not use such feedback, we show that it is very effective at drastically improving performance when task generalization does not happen automatically.
Further, it gives users flexibility to add custom tools without  retraining of the LLM.

\subsection{Task Library}
\label{sec:seed_tasks}
We construct a library of programs for a small seed set of tasks from Big-Bench~\citep{srivastava2022beyond}, a collaborative benchmark that measures the capabilities and limitations of language models. Big-Bench tasks span categories of traditional NLP, mathematics, commonsense reasoning, and question-answering. 
\paragraph{Constructing the task library.}

We identify \emph{five} skills that are useful across more than half of the tasks in BigBench that encompass text classification or generation of short answers in English (see \ref{appendix:tasklibrary}). We group tasks in the benchmark by these skills into the following clusters:
\begin{itemize}[noitemsep]
\item Arithmetic: arithmetic and algebra problems.
\item Code: Generating and executing python code.
\item Search and question decomposition: Single or multi-step questions that require search
% \item Multiple-choice tasks: Choose one or more of N options (overlaps with other categories)
\item Free-form reasoning: Explaining step-by-step reasoning in natural language
\item String Operations: Reformatting/editing strings, checking string entailment, etc.
\end{itemize}
We then select 2-4 tasks from each cluster and write programs (decompositions) for a few instances of each task, including calls to external tools and real outputs of those tools. Examples of programs in each cluster are in Appendix \ref{appendix:tasklibrary}. These programs follow a specific grammar, as outlined below.

\paragraph{Program grammar}
% \marco{TODO: make these examples follow the running example. I didn't really edit this much yet.}
The program format must be flexible in terms of task inputs
% in terms of input arguments
, steps, and tool calls, such that a wide variety of NLP tasks can be covered.
To do so, we define a query language \cite{beurer2022prompting} that extends the decomposed prompting format of \citet{khot2022decomposed}, since it can represent decomposed reasoning steps sequentially and incorporates function calls to external tools (like other LLMs).
Each program consists of a series of \emph{nodes} --- a task input node, several sub-step nodes, and an answer node. 
The \emph{input node} contains the task name, a simple instruction describing the task, and the input for an instance of the task:  
\exinline{Answer this high-school Physics question.\\\textbf{Input:} Hector yanks...}.
The input node is followed by a sequence of sub-task nodes, represented as a (query, answer) pair \exinline{$Q_i:..., \#i: ...$}. The sub-task query $Q_i$ has a sub-task name and sub-task input (\exinline{Q1: [search] What is the formula...}), while the sub-task answer $\#i$ is simply the output of the sub-task (\exinline{\#1: The horizontal component (Fx) can be calculated...}).
The program ends with a dummy sub-task (\exinline{Q3: [EOQ]}), followed by a final answer node (\exinline{Ans: 59N}).
All examples in Figures \ref{fig:teaser} and \ref{fig:nlprogrammer_architecture} follow this format.

\begin{figure*}
    \centering
    \includegraphics[scale=0.28]{sections/resources/human_feedback.pdf}
    \caption{Human feedback to \sys shown for (a) PQA where reasoning steps are added to the program and; (b) Word unscrambling where tool library is augmented with a new lookup tool.  }
    \label{fig:human_feedback}
\end{figure*}

\paragraph{Task Retrieval}
Given a new task, \sys retrieves $N$ tasks from the task library to construct a dynamic multi-task prompt.
We explore two strategies to retrieve similar tasks, depending on what data is available.
If 
% the task is classification and 
a small number of labeled examples for the new task is available ($\approx$50), 
we iterate over all five task clusters and select a few task programs from each cluster to compose the prompt. Ultimately, the task cluster with the highest performance on the held-out set of examples is chosen when predicting on all unlabeled examples from the task. While this strategy requires a held-out set of input-output pairs, no additional supervision is needed to generate a decomposed program. 

In the second strategy, we craft a few-shot prompt (Appendix \ref{appendix:taskselection}) with task pairs, where each task includes a name, instructions, and a few input-output examples. For each pair, we provide a label of ``Similar'' or ``Not similar'', and reasoning (e.g. ``These are related because they require solving arithmetic word problems''). At run time, we pair the test task with every task in the task library, and choose the highest-ranked ones based on the log probability ratio between ``Similar'' and ``Not similar''. We explore both strategies in Section~\ref{appendix:taskselection}.


\subsection{Tool Library}
\label{sec:tool_set}
Whenever a sub-task query name matches a tool name in the task library (e.g. \exinline{$Q_i$: [search]}), generation is stopped and resumed after the tool is called and its output is incorporated into the partially completed program.
We seed the tool library with the following tools (all of which have demonstrations in the task library). In particular, we describe the symbols used to represent these tools and their inputs. We also specify how the tool output is incorporated back into the program. Tool-specific implementation details and other tools added to \sys during feedback (\ref{human_in_loop}) are in Appendix~\ref{appendix:toolise}.

\paragraph{Search}
We use SerpAPI\footnote{\url{https://serpapi.com}}, which provides an API for Google search. 
The input to search is the sequence generated by the LLM after \exinline{$Q_i$: [search]}.
We extract answer box snippets when they are available or combine the top-2 search result snippets together. 
For PQA in Figure~\ref{fig:nlprogrammer_architecture}(B), the search query is the original input followed by  \exinline{What is the formula for the horizontal component of tension force?}, and the output is ``\exinline{... horizontal component (Fx) can be calculated as Ftens*cosine($\theta$) ...}''.

\paragraph{Code Generation}
% \marco{should edit this, very hard to follow}
We use the Codex \citep{chen2021evaluating} model for code generation. Input to 
 code generation is the sequence generated by the LM after the sub-task query symbol \exinline{$Qi: [generate\; python\; code]$}.
This argument is an instruction for code generation and is prompted to Codex as a multi-line comment in Python. For example, in Figure~\ref{fig:nlprogrammer_architecture}, Codex is prompted the instruction \exinline{``Use the formula Fx = Ftens * cosine($\theta$) to solve...''} as a comment and generates \exinline{T = 72.0, theta = 35.0, ..., Fx = T*math.cos(radians)}, which is appended to the incomplete program.

% \paragraph{Code Editing}
% \marco{should edit this, very hard to follow}
% \bvp{Shifting to appendix since we dont have a concrete example here}
% We use the Codex \citep{chen2021evaluating} model for code generation and code editing. Arguments for both include the previous sub-task's answer sequence \exinline{$\#i:\dots$} (or the input if $i=1$), and the sequence generated by the LM after the sub-task query symbol \exinline{$Qi: [generate\; python\; code]$}.
% The first argument is the code snippet and the second argument is a multi-line comment in Python used as the instruction for editing/generation.
% For example, in Figure~\ref{fig:nlprogrammer_architecture}, the code snippet \exinline{T = 72.0, theta = 35.0, ..., Fx = T*math.cos(radians)} is appended to the incomplete program.

\paragraph{Code Execution}
We run Python code in a virtual Python environment with arithmetic, symbolic, and scientific computing packages pre-installed. The argument to code execute is the previous sub-task's answer sequence \exinline{$\#(i-1):\dots$}, i.e. the python code snippet to be executed. For $i=1$, the task input is used as the argument since it potentially contains the code snippet to be executed.
% \marco{Not clear what incomplete program means here}
In Figure~\ref{fig:nlprogrammer_architecture}, the code snippet generated in the previous step is executed and the value of variable \exinline{Fx} is added to the incomplete program.


\subsection{Human feedback}
\label{human_in_loop}
\sys is specifically designed to be amenable to human feedback since it does not require additional finetuning.
Consequently, users can incorporate feedback immediately into \sys, by editing the task library and/or the tool library.
Since \sys generates multi-step reasoning programs that are interpretable, we explore feedback in the form of debugging, i.e. users \emph{edit} existing programs rather than creating programs from scratch.
These edits can be in the form of correcting sub-step outputs, adding/removing sub-steps (with appropriate inputs and answers), adding calls to new tools, etc.

For example, in Figure \ref{fig:human_feedback}(a) the user edits a program by adding two sub-steps, in order to round the answer to the nearest integer and include the appropriate unit of measurement to the answer.
This feedback demonstrates appropriate \emph{decompositions} for the task, as these operations are still performed by the LLM (the tool library does not have \exinline{[arithmetic]} or \exinline{[add unit]} APIs).
In contrast, in Figure \ref{fig:human_feedback}(b) the user demonstrates the use of a dictionary \exinline{[lookup]} \emph{and} implements it as a tool in the tool library.
While most of our experiments do not rely on such feedback (and thus measure ``zero shot'' task transfer with no supervision for reasoning/tool-use), we show that simple operations like these can drastically improve performance on target tasks.
