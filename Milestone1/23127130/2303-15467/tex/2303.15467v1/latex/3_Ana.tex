\section{Information Analysis in OSAR}
\label{sec:ana}

\subsection{Prototypical Learning}
Let $f$ be the encoder to extract the information for an input video sample $x$ and output the feature representation $z = f(x), z \in \mathbb{R}^d$. We first define a prototypical learning (PL) loss~\cite{yang2018robust}, which is a general version of the cross-entropy (C.E.) loss:
\begin{equation}
\vspace{-0.2cm}
    \mathcal{L}_{PL} = - \log \frac{\exp (\frac{z^T k_{i}} {\tau}) }{ \exp (\frac{z^T k_{i}}{ \tau}) + \sum\limits_{n \in K_i^{-}} \exp (\frac{z^T  n}{ \tau})},
    \label{eq:PL}
    \vspace{-0.2cm}
\end{equation}
where $i$ is the ground truth label of $x$, $k_i \in \mathbb{R}^d$ is the prototype for class $i$, $\tau$ is a temperature parameter, $K_i^{-}=\left \{ k_j| j \in \left \{ 1,2,...,N \right \}, j\ne i\right \}$ is the negative prototype set, and $N$ is the number of InD classes. Note that $z$ and $k_i$ are normalized by L2 norm, so that $z^T k_{i}$ is the cosine similarity. If we regard prototypes as the row vector of the linear classifier $W \in \mathbb{R}^{N \times d}$, and do not normalize $z$ and $k$ as well as remove $\tau$, $\mathcal{L}_{PL}$ will degenerate to the C.E. loss. We introduce the $\mathcal{L}_{PL}$ so that we can directly manipulate the feature representation $z$.

\subsection{Information Analysis of OSAR}
Let $x_{InD}, z_{InD}$, and $Y$ be the random variables of InD sample, extracted representation of InD sample, and the task to predict the label of $x_{InD}$, where $z_{InD}=f(x_{InD})$. Given the joint distribution of $p(x_{InD},Y)$, the relevant information between $x_{InD}$ and $Y$ is defined as $I(x_{InD},Y)$, where $I$ denotes the mutual information~\cite{tishby2000information}. The learned representation $z_{InD}$ satisfies:
\vspace{-0.2cm}
\begin{equation}
% \vspace{-0.2cm}
    I(x_{InD}; z_{InD}) = \underbrace{I(x_{InD}; z_{InD}|Y)}_{IS} + \underbrace{I(z_{InD};Y)}_{CS},
    \label{eq:sum}
    \vspace{-0.2cm}
\end{equation}
in which $I(x_{InD}; z_{InD}|Y)$ and $I(z_{InD};Y)$ denote the \emph{Instance-Specific (IS)} and \emph{Class-Specific (CS) information} respectively. In \cref{fig:ana}, IS information is blue and orange areas, and CS information is yellow and green areas. CS information is for the closed-set label prediction task $Y$, while IS information is the special information of each sample that is not related to $Y$.

To analyze the information about OSAR, we let $T$ be a random variable that represents the task to distinguish OoD samples from InD samples, then we divide the information contained in $z_{InD}$ about $T$ into two parts~\cite{wang2022rethinking}:
\vspace{-0.2cm}
 \begin{equation}
I(z_{InD};T)=\underbrace{I(z_{InD}|Y; T)}_{IS \; \text{about} \; T} + \underbrace{I(z_{InD};Y;T)}_{CS \; \text{about} \; T},
     \label{eq:OSAR}
     \vspace{-0.2cm}
 \end{equation}
where $I(z_{InD}|Y; T)$ and $I(z_{InD};Y;T)$ are the information about the OoD detection task $T$ in IS and CS information (orange and green areas in \cref{fig:ana} respectively). We can see that larger IS and CS information are helpful for OSAR.

In this paper, we aim to enlarge the information about $T$ contained in CS and IS information for better OSAR performance, as illustrated in \cref{fig:1} (b) and the enlarged green and orange areas in \cref{fig:ana}. We first analyze the CS and IS information behaviors under the classical C.E. loss, and find that CS information is encouraged to be maximized but IS information tends to be eliminated in \cref{sec:CS_IS_CE}. Then we explain this conclusion from the IB theory view in \cref{sec:ib_analy}.

\begin{figure}[t]
    \centering
\includegraphics[width=0.99\linewidth]{docu/figs/fig_2.pdf}
    \vspace{-0.4cm}
    \caption{The neural network (NN) can only extract limited representations $z_{InD}$ of the InD sample $x_{InD}$ for the current task $Y$ (predict the closed-set label), which is not diverse enough for the task $T$ (distinguish OoD samples), as green and orange areas are small in (a). In our PSL, we encourage the NN to learn a more diverse representation so that more IS and CS information about $T$ are contained.}
    \label{fig:ana}
\end{figure}


\subsection{CS and IS Information Behavior under C.E.}
\label{sec:CS_IS_CE}
CS information is for closed-set classification task $Y$, so it is similar for the same class sample, but distinct for the different class sample ($s_1,s_2/s_3$ in \cref{fig:1}). In contrast, IS information is not related to $Y$ and it is distinct for samples in the same class ($s_1, s_2$ in \cref{fig:1}). Therefore, we have the following proposition which describe the relation between CS/IS information and feature representation similarity.
\begin{proposition}
\vspace{-0.2cm}
For two feature representations of samples in the same class, more CS information means these two feature representations are more similar, and more IS information decreases their feature similarity.
\label{prop:cs_is}
\vspace{-0.2cm}
\end{proposition}

CS information is for the closed-set label prediction task $Y$, which is fully supervised by C.E. loss, so it is maximized during training.  In contrast, Eq.~\ref{eq:PL} shows that C.E. encourages representations of the same class to be exactly same with the corresponding prototype, and such high similarity eliminates the IS information according to Proposition~\ref{prop:cs_is}. Therefore, \textbf{C.E. loss tends to maximize the CS information and eliminate the IS information in the feature representation}. We analyze this conclusion based on Information Bottleneck (IB) theory in next~\cref{sec:ib_analy}.

\subsection{IB Theory Analysis for CS and IS Information}
\label{sec:ib_analy}

Applying the Data Processing Inequality~\cite{cover1999elements} to the Markov chain $Y \to x_{inD} \to z_{InD}$, we have
\vspace{-0.2cm}
\begin{equation}
    I(z_{InD};Y) \le I(x_{InD};Y).
    \label{eq:mar}
    \vspace{-0.2cm}
\end{equation}
It means that the compressed representation $z_{InD}$ cannot contain more information of $Y$ compared to the original data $x_{InD}$. 

According to the IB theory~\cite{tishby2000information,tishby2015deep}, the NN is to find the optimal solution of $z_{InD}$ with minimizing the following Lagrange:
\vspace{-0.2cm}
\begin{equation}
    \mathcal{L}[p(z_{InD}|x_{InD})]=I(z_{InD}; x_{InD})-\beta I(z_{InD}; Y),
    \label{eq:lar}
    \vspace{-0.2cm}
\end{equation}
where $\beta$ is the Lagrange multiplier attached to the constrained meaningful condition. Eq.~\ref{eq:lar} demonstrates the NN is solving a trade-off problem, as the first term tends to keep the information of $x_{InD}$ as less as possible while the second term tends to maximize the information of $Y$.

Inspired by \cite{wang2022rethinking,achille2018emergence}, the sufficient and minimum sufficient representation of $x_{InD}$ about $Y$ can be defined as:
\begin{definition}(Sufficient Representation) A feature representation $z_{InD}^{suf}$ of $x_{InD}$ is sufficient for $Y$ if and only if $I(z_{InD}^{suf};Y)=I(x_{InD}; Y)$.
\label{def:suf}
\end{definition}
\begin{definition}(Minimum Sufficient Representation) A sufficient representation $z_{InD}^{min}$ of $x_{InD}$ is minimum if and only if $I(z_{InD}^{min};x_{InD}) \le I(z_{InD}^{suf}; x_{InD})$, ${\forall} z_{InD}^{suf}$ that is sufficient for $Y$.
\label{def:mini}
\end{definition}

\noindent \textbf{CS Information Maximization.} The goal of training is to optimize $f$ so that $I(z_{InD};Y)$ (CS information) can approximate $I(x_{InD};Y)$, which stays unchanged as data distribution is fixed during training. Therefore, CS information is supposed to be maximized to the upper bound $I(x_{InD};Y)$ because of Eq.~\ref{eq:mar}. In this way, the closed-set classification task pushes the NN to learn the sufficient representation $z_{InD}^{suf}$ according to definition~\ref{def:suf}~\cite{federici2020learning}. 

\noindent \textbf{IS Information Elimination.}
When $z_{InD}$ is close to the sufficient representation $z_{InD}^{suf}$, the second term in Eq.~\ref{eq:lar} will be the fix value $I(x_{InD}; Y)$ based on the definition~\ref{def:suf}. So the key to minimize Eq.~\ref{eq:lar} is to minimize the first term $I(z_{InD}^{suf}; x_{InD})$. Based on the definition~\ref{def:mini}, the lower bound of $I(z_{InD}^{suf}; x_{InD})$ is $I(z_{InD}^{min}; x_{InD})$, so we can conclude that the learned representation is supposed to be the minimum sufficient representation $z_{InD}^{min}$~\cite{wang2022rethinking}. We substitute $I(z_{InD}^{suf}; x_{InD})$ and $I(z_{InD}^{min}; x_{InD})$ in definition~\ref{def:mini} with Eq.~\ref{eq:sum} and we have
\vspace{-0.2cm}
\begin{align}
    & I(x_{InD}; z_{InD}^{min}|Y) + I(z_{InD}^{min};Y) \nonumber \\
    \le &  
    I(x_{InD}; z_{InD}^{suf}|Y) + I(z_{InD}^{suf};Y).
    \label{eq:ine}
\end{align}
As both $z_{InD}^{min}$ and $z_{InD}^{suf}$ are sufficient, the second term of both sides in Eq.~\ref{eq:ine} is $I(x_{InD}; Y)$, so we have
\begin{equation}
    0 \le I(x_{InD}; z_{InD}^{min}|Y)
    \le
    I(x_{InD}; z_{InD}^{suf}|Y).
\end{equation}
Therefore, the learned IS information in $z_{InD}^{min}$ is smaller than any IS information in $z_{InD}^{suf}$, which could be eliminated to 0~\cite{wang2022rethinking} (no blue and orange areas in $z_{InD}^{min}$ in \cref{fig:ana}).

\subsection{Enlarge CS and IS Information for OSAR}
\label{sec:lar_cs_is}
Based on the analysis in \cref{sec:CS_IS_CE} and \cref{sec:ib_analy}, we show that C.E. tends to maximize the CS information and eliminate the IS information in the feature representation. Both larger IS and CS information are crucial for OSAR according to Eq.~\ref{eq:OSAR}, but C.E. does not bring the optimal information. On the one hand, IS information is eliminated so we lose a part of information which is beneficial for the OSAR. On the other hand, the learned representation is not sufficient and does not contain enough CS information in practice due to the model capacity and data distribution shift between training and test sets, which can be supported by the fact that test accuracy cannot reach 100\%. Therefore, we propose our method to enlarge the CS and IS information for better OSAR performance in next~\cref{sec:method}.

