\section{Experiments}
\label{sec:exp}

\noindent \textbf{Datasets.} Following~\cite{bao2021evidential}, we use UCF101~\cite{soomro2012ucf101} as the InD dataset for training and closed-set evaluation, and use HMDB51~\cite{kuehne2011hmdb} and MiT-v2~\cite{monfort2021multi} as OoD data for open-set evaluation. Different from~\cite{bao2021evidential} which does not clean the OoD data that may contains InD classes, we remove the overlapping classes between InD and OoD dataset during evaluation. See Appendix A for more details.

\noindent \textbf{Evaluation protocols.} For closed-set performance, we evaluate like the traditional way to calculate the top-1 accuracy Acc. (\%). For open-set performance, we follow the classical open-set recognition protocol~\cite{hendrycks2016baseline,hendrycks2018deep} to use the obtained uncertainty score Eq.~\ref{eq:unce} to calculate AUROC (\%), AUPR (\%) and FPR95(\%).\footnote{We find AUROC in~\cite{bao2021evidential} only considers one specific threshold based on their code, and after discussion and agreement they provide the modified correct score in our Tab.~\ref{tab:bench_hmdb}. See Appendix B for details.}

\begin{table*}[t]
\centering
\tablestyle{6pt}{0.6}
\begin{tabular}{llcccccccc}
\toprule[1pt]
& & \multicolumn{4}{c}{\bf{w/o K400 Pretrain}} & \multicolumn{4}{c}{\bf{w/ K400 Pretrain}} \\ \cmidrule{3-10}
   \bf{Datasets} &\bf{Methods}                     & \bf{AUROC$\uparrow$}    & \bf{AUPR$\uparrow$}  &\bf{FPR95$\downarrow$}  & \bf{Acc.$\uparrow$}  & \bf{AUROC$\uparrow$}   & \bf{AUPR$\uparrow$} &\bf{FPR95$\downarrow$}   &  \bf{Acc.$\uparrow$}  \\ \midrule
  \multirow{7}{*}{\makecell[l]{UCF101 (InD) \\ HMDB51 (OoD)}}
&OpenMax~\cite{bendale2016towards}     & \textit{82.28}    & \textit{54.59}   & \textit{50.69} & \textit{73.92} & 90.89  & 73.16   &38.77               & 95.32        \\
&MC Dropout~\cite{gal2016dropout}     & 75.75    & 41.21   & 54.78 & 73.63                              & 88.23   & 67.62   &38.12            & 95.06           \\
&BNN SVI~\cite{krishnan2018bar}      & 80.10    & 53.43  & 52.33  & 71.51                                & \textit{91.81}   & \textit{79.65}  &31.43             & 94.71           \\
&SoftMax~\cite{hendrycks2016baseline}    & 79.72    & 52.13  & 53.22  &\textit{73.92}                                   & 91.75   & 77.69  &\textit{28.60}              & 95.03           \\
&RPL~\cite{chen2020learning}        & 79.67    & 51.85  &56.40  & 71.46                                  & 90.53   & 77.86  &37.09               &\textit{95.59}           \\
&DEAR~\cite{bao2021evidential}      & 80.00         &49.23         &53.28 & 71.33                                    &84.16   & 75.54  &89.40                   & 94.48           \\
&PSL(ours)   & \bf{86.43}    & \bf{65.54}   & \bf{41.67} & \bf{76.53}                                  & \bf{94.05}   & \bf{86.55}   &\bf{23.18}   & \bf{95.62}           \\
&$\mathbf{\Delta}$ &\bf \textcolor{themeblue}{(+4.15)} &\bf \textcolor{themeblue}{(+10.95)} &\bf \textcolor{themeblue}{(-9.02)} &\bf \textcolor{themeblue}{(+2.61)} &\bf \textcolor{themeblue}{(+2.24)} &\bf \textcolor{themeblue}{(+6.90)} &\bf \textcolor{themeblue}{(-5.42)} &\bf \textcolor{themeblue}{(+0.03)}\\
\midrule
\multirow{7}{*}{\makecell[l]{UCF101 (InD) \\ MiTv2 (OoD)}} 
&OpenMax~\cite{bendale2016towards}       & \textit{84.43}   & \textit{76.69} &\textit{47.74}  & \textit{73.92}                 & \textit{93.34}   & 88.14   & \textit{28.95} & 95.32           \\
&MC Dropout~\cite{gal2016dropout}                                   & 75.66    & 62.20   & 51.57 & 73.63            & 88.71   & 83.36   & 39.46 & 95.06           \\
&BNN SVI~\cite{krishnan2018bar}                                      & 79.48    & 71.73   & 52.52 & 71.51            & 91.86   & \textit{90.12}   & 36.21 & 94.71           \\
&SoftMax~\cite{hendrycks2016baseline}                                      & 80.55    & 73.17   & 50.49 & \textit{73.92}            & 91.95   & 89.16   & 32.00 & 95.03           \\
&RPL~\cite{chen2020learning}                                          & 80.21    & 72.04   & 52.83 & 71.46            & 90.64   & 88.79   & 38.43 & \textit{95.59}           \\
&DEAR~\cite{bao2021evidential}                                         & 79.00          & 67.10        &52.44   &  71.33             & 86.04   & 87.38   & 87.40 & 94.48           \\
&PSL(ours)            & \bf{86.53}   & \bf{79.95}  & \bf{40.99} &  \bf{76.53}                & \bf{95.75}        & \bf{94.96}        & \bf{18.96} & \bf{95.62}                \\ 
&$\mathbf{\Delta}$ &\bf \textcolor{themeblue}{(+2.10)} &\bf \textcolor{themeblue}{(+3.26)} &\bf \textcolor{themeblue}{(-6.75)} &\bf \textcolor{themeblue}{(+2.61)} &\bf \textcolor{themeblue}{(+2.41)} &\bf \textcolor{themeblue}{(+4.84)} &\bf \textcolor{themeblue}{(-9.99)} &\bf \textcolor{themeblue}{(+0.03)} \\
\bottomrule[1pt]
\end{tabular}
\vspace{-0.3cm}
\caption{Comparison with state-of-the-art methods on {\bf HMDB51 and MiTv2 (OoD)} using TSM backbone. Acc. refers to closed-set accuracy. AUROC, AUPR and FPR95 are open-set metrics. Best results are in {\bf bold} and second best results in \textit{italic}. The gap between best and second best is in {\bf \textcolor{themeblue}{blue}}. DEAR and our methods contain video-specific operation.}
\label{tab:bench_hmdb}
\vspace{-0.6cm}
\end{table*}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{docu/figs/MiT_2.pdf}
\vspace{-8mm}
\caption{
The uncertainty distribution of InD and OoD samples of (a) Softmax, (b) DEAR, (c) BNN SVI and (d) our PSL method.
}
\label{fig:uncer_method}
\end{figure}

\noindent \textbf{Implementation details.} For Kinetics400 (K400)~\cite{i3d} pretrained model, our implementation setting is the same with~\cite{bao2021evidential}. The base learning rate is 0.001 and step-wisely decayed every 20 epochs with total of 50 epochs. We argue that as K400 is extremely large, the K400 pretrained model may already have seen the OoD data used in inference, so we conduct experiments from scratch (no ImageNet pretrained) to ensure that OoD data is absolutely unavailable during training. We use the LARS optimizer~\cite{you2017large} and set the base learning rate and momentum as 0.6 and 0.9 with total of 400 epochs. The experiments are conducted on TSM~\cite{lin2019tsm}, I3D~\cite{i3d} and SlowFast~\cite{feichtenhofer2019slowfast}. The batch size for all methods is 256. More details are in Appendix C.

\subsection{Evaluation Results}
\label{sec:res}

\noindent \textbf{Comparison with state-of-the-art.} We report the results on HMDB51 (OoD) and MiT-v2 (OoD) in Table~\ref{tab:bench_hmdb} using TSM backbone~\cite{lin2019tsm}. The evaluation results of other backbones including I3D and SlowFast are in the Appendix D. We can see that for w/ or w/o K400 pretrain, our PSL method has significantly better open-set and closed-set performance than all baselines. The uncertainty distribution of InD and OoD samples are depicted in Fig.~\ref{fig:uncer_method} for MiT-v2 (OoD) with K400 pretrained. Three baseline methods have a clear over confidence problem, \textit{i.e.}, the far left column is extremely high (red circles in Fig.~\ref{fig:uncer_method}), which means a large number of OoD samples have almost 0 uncertainty, while our method significantly alleviates this problem through the distinct representation of OoD samples, illustrated in Fig.~\ref{fig:tsne}. Besides, we can find that the open-set performance w/ K400 pretrain is higher than w/o pretrain for almost all methods in Table~\ref{tab:bench_hmdb} and \cref{fig:1} (a), which can testify the importance of richer semantic representation for OSAR.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{docu/figs/tsne.pdf}
\vspace{-10mm}
\caption{
Feature representation visualization of cross-entropy and our PSL method. OoD samples are in black and InD samples are in other colors. In the red, blue and green circles, it is clear that OoD samples distribute at the edge of InD samples in our PSL, while greatly overlap with each other in the cross-entropy method.
}
\label{fig:tsne}
\end{figure}

\begin{table*}[t!]
\tablestyle{6pt}{0.3}
\centering
\begin{tabular}{lcccccccccccc}
\toprule[1pt]
&&&&& \multicolumn{2}{c}{\bf{InD}} &\multicolumn{2}{c}{\bf{OoD}} \\ \cmidrule{6-9}
                        & \bf{$s$} & \bf{$Q_{ns}$} & \bf{$Q_{sc}$} & $Q_{shuf}$ &\bf{Mean} & \bf{Variance} &\bf{Mean} & \bf{Variance} & \bf{AUROC$\uparrow$} & \bf{AUPR$\uparrow$} &\bf{FPR95$\downarrow$}  & \bf{Acc.$\uparrow$} \\ \midrule
$\mathcal{L}_{PL}$                   & \XSolidBrush  & \XSolidBrush      & \XSolidBrush      & \XSolidBrush        &0.81 &0.0015 &0.63 &0.0029 & 80.95 & 52.79 & 52.51 & 72.36          \\ \midrule
$\mathcal{L}_{PSL}$ & \Checkmark  & \XSolidBrush      &\XSolidBrush       & \XSolidBrush        & 0.79     &0.0016  & 0.62 & 0.0028 & 81.79 & 54.16 & 52.33 & 72.33          \\ \midrule
 \multirow{3}{*}{$\mathcal{L}_{PSL}^{CT}$}                       & \Checkmark  & \Checkmark      & \XSolidBrush      & \XSolidBrush        &0.71 &0.0022 &0.61 &0.0036 & 82.60 & 57.36 & 50.03 & 72.17          \\ 
                        & \Checkmark  & \Checkmark      & \Checkmark      & \XSolidBrush        & 0.71 &0.0023 &0.49 &0.0035 & 83.42 & 59.05 & 51.32 & 72.28          \\
                        & \Checkmark  & \Checkmark      & \Checkmark      & \Checkmark        & 0.74 &0.0016 &0.63 &0.0029 & 86.43 & 65.58 & 41.75 & 77.19     \\ 
                        \bottomrule[1pt]  
\end{tabular}
\vspace{-0.3cm}
\caption{Abaltion results of different components in $\mathcal{L}_{PSL}^{CT}$.}
\label{tab:abla}
\vspace{-0.5cm}
\end{table*}


\noindent \textbf{Comparison with metric learning methods.} Our method concentrates on the feature representation aspect for the OSAR problem, so we also implement several well-known metric learning methods and show the result in Table~\ref{tab:metric_learning}. The evaluation is conducted using TSM model and OoD dataset is HMDB51. We do not use video shuffling in our method for fair comparison. We can see that our method still achieves the best open-set performance. The most important difference between our method and all other metric learning methods is that they aim to push the features of one class as tight as possible like C.E., while our method aims to keep the feature variance within a class to retain IS information. We calculate the mean similarity between the sample feature and the corresponding class center. The mean similarity ranges from 0.77 to 0.82 for other metric learning methods, while mean similarity is 0.71 ($s=0.8$) and 0.6 ($s=0.6$) for our PSL. So our method has looser feature distribution within a class, as shown in \cref{fig:tsne}.

\begin{table}[t!]
\tablestyle{5pt}{0.8}
\centering
\begin{tabular}{lcccccccccc}
\toprule[1pt]
& \bf{AUROC$\uparrow$} & \bf{AUPR$\uparrow$} &\bf{FPR95$\downarrow$}  & \bf{Acc.$\uparrow$} \\ \midrule
SoftMax    &80.95	&52.79	&52.51   &72.36	        \\
 Triplet \cite{triplet} &81.02 &54.75	&53.88 &75.50\\
Normface \cite{normface} &80.99	&54.90	&53.19 &73.34            \\
Circle  \cite{circle} 	&78.76	&51.65	&55.27 &72.15 	\\
Arcface \cite{arcface} &81.23	&55.03	&53.67 &\bf{75.95}	\\
LSoftMax \cite{lsoftmax} &80.87 &54.01	&52.29 &73.05 \\ \midrule
PSL($s=0.8$)   & \bf{83.42} & \bf{59.05} & \bf{51.32} & 72.28           \\
PSL($s=0.6$)    &82.75 & 58.57 & 52.27 & 73.26           \\
                        \bottomrule[1pt]  
\end{tabular}
\vspace{-0.3cm}
\caption{Comparison with different metric learning methods.}
\label{tab:metric_learning}
\vspace{-0.4cm}
\end{table}


\subsection{Ablation Study}
\label{sec:abl_stu}

\noindent \textbf{Contrastive terms in $\mathcal{L}_{PSL}^{CT}$ for IS information.} The intuition of PSL is to keep the intra-class variance to retain the IS information which is helpful for OSAR. We expect that the representation $z$ within a class has a similarity $s<1$ with the prototype $k_i$, so each sample can keep its own IS information. However, we find that the loss $\mathcal{L}_{PSL}$ may lead the network to find the trivial representation of samples $z$ which is similar to using loss $\mathcal{L}_{PL}$, where only $k_i$ shifts and $z$ does not. We calculate the mean of similarity $sim(z, \bar z_i)$, where $\bar z_i$ denotes the mean representation of all samples in the same class $i$, and the mean of similarity with the corresponding prototype $sim(z, k_i)$, as well as the feature variance in all dimensions. Fig.~\ref{fig:abl_contra} (a) and (b) show that with the hyper-parameter $s$ decreasing, the $sim(z, k_i)$ decreases as expected by $\mathcal{L}_{PSL}$ (green curves), but the $sim(z, \bar z_i)$ and variance stay unchanged (blue curves), meaning that the representation of samples are still similar with using $\mathcal{L}_{PL}$, and only the prototypes are pushed away by the sample representations. In contrast, with CT in $\mathcal{L}_{PSL}^{CT}$, the $sim(z, \bar z_i)$ decreases and variance increases with $s$ decreases (red curves), indicating that CT is significantly effective to keep the intra-class variance.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{docu/figs/abl_contra_term.pdf}
    \vspace{-0.5cm}
    \caption{Mean similarity and variance analysis for CT terms.}
    \label{fig:abl_contra}
    \vspace{-0.3cm}
\end{figure}


To individually study the effectiveness of $Q_{ns}$ and $Q_{sc}$ in $\mathcal{L}_{PSL}^{CT}$, we provide the ablation results in Table~\ref{tab:abla}. For OoD samples, we calculate the similarity with the mean representation of its predicted class. Table~\ref{tab:abla} shows that using $Q_{ns}$ alone can significantly increase the intra-class variance for both InD and OoD samples, meaning the pushing effect of representations in other classes can implicitly help retain the IS information. On top of that, $Q_{sc}$ can further learn more IS information that is helpful to distinguish OoD samples, as the mean similarity of InD samples stay unchanged, but OoD samples are smaller which means OoD samples are far away from InD samples.

\noindent \textbf{Shuffled videos for CS information.} \cref{tab:abla} shows that $Q_{shuf}$ can improve both closed-set and open-set performance, which proves introducing shuffled videos in PSL can enlarge CS information. Smaller intra-class variance brought by $Q_{shuf}$ testify Proposition~\ref{prop:cs_is} that more CS information means more similar features within the same class.

We draw the uncertainty of all classes in HMDB51, as shown in Fig.~\ref{fig:ood_samples}. Note that some classes in HMDB51 are actually InD as they appear in the UCF101, like the class 3 \emph{golf} and 4 \emph{shoot bow} in Fig.~\ref{fig:ood_samples}. We find that in C.E. some OoD classes have extremely low uncertainty, such as class 1 \emph{chew} and 2 \emph{smile}, because they are spatially similar to some InD classes like \emph{ApplyEyeMakeup} and \emph{ApplyLipstick} in Fig.~\ref{fig:ood_samples} (a). Comparing (b) and (c) shows that our PSL can increase the average uncertainty of OoD classes (higher yellow points), and some OoD classes which are similar to InD classes like 1 and 2 have much higher uncertainty in our PSL method. After shuffled samples are involved, some InD classes whose uncertainty are increased in (c) like 3 and 4 have lower uncertainty in (d), and the uncertainty of some OoD classes sharing similar appearance with InD classes like class 1 is further improved.

$Q_{sp}$ in Eq.~\ref{eq:PSL_CT} contains $Q_{shuf}$ and $Q_{sc}$, so we analyze whether should we assign the same $s$ for the shuffled video $Q_{shuf}$ and other videos in the same class $Q_{sc}$. \cref{tab:s_q_shu} shows that the same $s$ have good enough performance. So we set the same $s$ for $Q_{shuf}$ and $Q_{sc}$ in the default setting to reduce the number of hyper-parameters.

\subsection{Discussion}
\label{sec:discussion}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{docu/figs/fig_ood_samples.pdf}
    \vspace{-0.5cm}
    \caption{(a) \emph{chew} and \emph{smile} are OoD samples from HMDB51, and \emph{ApplyEyeMakeup} and \emph{ApplyLipstick} are InD samples from UCF101. (b-d) Uncertainty distribution of each class in HMDB51. Class 1: \emph{chew}, 2: \emph{smile}, 3: \emph{golf}, 4: \emph{shoot bow}. Classes 1 and 2 are OoD while 3 and 4 are InD.}
    \label{fig:ood_samples}
    \vspace{-0.4cm}
\end{figure}


\begin{table}[t]
\tablestyle{4pt}{0.8}
\centering
\begin{tabular}{ccccccc}
\toprule[1pt]
 $s(Q_{shuf})$ &$s(Q_{sc})$  &\bf{AUROC$\uparrow$} & \bf{AUPR$\uparrow$} &\bf{FPR95$\downarrow$}  & \bf{Acc.$\uparrow$} \\ \midrule
0.7 &\multirow{4}{*}{0.7} & 85.25	&63.91	&48.34 &76.98\\
0.5	& &86.03	&64.36	&43.70 & 76.53\\
0.3	& &83.80	&60.42	&48.76 & 75.50\\
0 & &79.54&50.59&54.43& 72.59\\ \midrule
0.8 &0.8 &86.43	&65.58	&41.75 &76.53\\
0.9 &0.9 &83.12	&57.04	&46.84 &73.31\\
1 &1 &82.04	&53.82&51.82 &72.89	\\
\bottomrule[1pt]
\end{tabular}
\vspace{-0.3cm}
\caption{Ablation study of similarity $s$ for $Q_{shuf}$ and $Q_{sc}$.}
\label{tab:s_q_shu}
\vspace{-0.3cm}
\end{table}

\noindent \textbf{Both CS and IS information are useful.} We provide the closed-set and open-set performance under different hyper-parameter $s$ and feature dimension $d$ in \cref{fig:dis_s}. (a) shows that $s=0.8$ has better open-set performance than $s=1$ and has comparable closed-set accuracy, which illustrates that retaining the IS information which is eliminated by C.E. ($s=1$) is beneficial. When $s<0.8$, the NN cannot learn enough CS information, so both closed-set and open-set performance drops. Therefore, a proper mixture of CS and IS information is ideal. (b) shows that when $d$ grows from 4 to 16, more CS information is contained so that both closed-set and open-set performance improves. When $d$ grows from 16 to 128, the feature does not include more CS information as closed-set accuracy is comparable. However, open-set performance keeps increasing which means more IS information is contained based on more feature dimensions. This interesting experiment shows that enough information for closed-set recognition is not enough for open-set recognition because IS information is not related to the closed-set task but useful for the open-set task.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{docu/figs/dis_s.pdf}
    \vspace{-0.5cm}
    \caption{Ablation study of similarity $s$ and feature dimension $d$.}
    \label{fig:dis_s}
    \vspace{-0.4cm}
\end{figure}
\begin{table}[t]
\tablestyle{2pt}{0.8}
\centering
\begin{tabular}{lcccccccccccc}
\toprule[1pt]
   \bf{Epoch}                      &\bf{Mean} & \bf{Variance} & \bf{AUROC$\uparrow$} & \bf{Acc-Test.$\uparrow$} & \bf{Acc-Train.$\uparrow$}\\ \midrule
200 &0.577 &3.3e-3 &75.08	&68.39 &99.85\\
400 &0.602 &3.1e-3 &82.92	&73.26 &100\\
800 &0.613 &3.0e-3 &82.54	&73.29 &100\\
                        \bottomrule[1pt]  
\end{tabular}
\vspace{-0.3cm}
\caption{Training process analysis when $s=0.6$ w/o $Q_{shuf}$.}
\label{tab:continual_train}
\vspace{-0.3cm}
\end{table}


\noindent \textbf{Feature variance and open-set performance analysis.} \cref{fig:dis_s} (a) shows that when features get looser ($s=1-0.8$), the open-set performance is improved, but if features get continually looser ($s=0.8-0.1$), the open-set performance drops. So there is no strict relation between the feature variance and open-set performance. One may argue that continual training can benefit the open-set performance~\cite{vaze2021open}, which is alongside with smaller feature variance~\cite{han2021neural}. We show that the benefit of continual training comes from better closed-set performance, not tighter features. \cref{tab:continual_train} shows that when we train the model from 200 to 400 epochs, the closed-set accuracy is higher, and feature is tighter (larger mean similarity and smaller variance), and the open-set performance is better. But from epoch 400 to 800 we find the model is already overfitted to the training set, as the accuracy of test set remains unchanged. So although the features get tighter in the 800 epoch, both the closed-set and open-set performance remain same.

\section{Conclusion}
We analyze the OSAR problem from the information perspective, and show that cross-entropy tends to eliminate IS information and cannot fully learns CS information which are both useful for the open-set task. So we propose PSL to retain IS information and introduce shuffle videos into PSL to enlarge CS information. Comprehensive experiments demonstrate the effectiveness of our PSL and the importance of IS and CS information in the OSAR task.

\noindent {\bf Acknowledgements} This work is supported by Alibaba Group through Alibaba Research Intern Program.