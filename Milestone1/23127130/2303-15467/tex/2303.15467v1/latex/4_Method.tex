\section{Methods}
\label{sec:method}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{docu/figs/fig_3_3.pdf}
    \vspace{-0.5cm}
    \caption{(a) C.E. encourages the sample feature $z$ to be exactly same with the corresponding prototype $k_i$. (b) Our PSL encourages the similarity between $z$ and $k_i$, features of shuffled sample $Q_{shuf}$ and other samples in the same class $Q_{sc}$ to have a similarity less than 1.}
    \label{fig:psl}
\end{figure*}

\subsection{Prototypical Similarity Learning}
\label{subsec:psl}

According to \cref{sec:CS_IS_CE}, we notice that IS information is suppressed by the C.E. loss and a key reason is C.E. encourages feature representations of the same class to be exactly same. Therefore, we argue that the feature representation of the same class samples should have a similarity $s < 1$. In other words, we aim to \textbf{keep the intra-class variance which prevents intra-class collapse to retain IS information}. Based on the classical PL loss Eq.~\ref{eq:PL}, we develop prototypical similarity learning (PSL):
\vspace{-0.2cm}
\begin{equation}
    \mathcal{L}_{PSL} = - \log \frac{\exp (\frac{1 - \left | z^T k_{i} - s \right |} {\tau}) }{ \exp (\frac{1 - \left | z^T k_{i} - s \right |} {\tau}) + \sum\limits_{n \in K_i^{-}} \exp (\frac{z^T  n} {\tau})},
    \label{eq:PSL}
    \vspace{-0.2cm}
\end{equation}
where $s$ and $\tau$ are fixed hyperparameters. In this way, we expect the prototype $k_i$ to act as the CS information for the InD class $i$, which is used to predict the label, and the dissimilarity between the $z$ and $k_i$ represents the IS information. Traditional PL loss (or C.E. loss) encourages the features of samples in the same classes to be as tight as possible, while our PSL aims to keep the variance within the same class. 

However, we find Eq.~\ref{eq:PSL} will converge to the trivial solution, where the $z$ converges to the training result of Eq.~\ref{eq:PL} and only $k_i$ shifts. To solve this problem, we introduce the similarity between different samples within a mini-batch into the denominator of Eq.~\ref{eq:PSL}. In this way, we directly constrain the relationship between sample features instead of only supervising the similarity between the sample feature and its prototype. We name the modified loss as PSL with contrastive terms (CT): 
\vspace{-0.2cm}
\begin{align}
\vspace{-0.2cm}
&\mathcal{L}_{PSL}^{CT} = \nonumber \\
&\frac{\exp (\frac{1 - \left | z^T k_{i} - s \right |} {\tau}) }{ \exp (\frac{1 - \left | z^T k_{i} - s \right |} {\tau}) + \sum\limits_{n \in Q_n} \exp (\frac{z^T  n} {\tau})+\sum\limits_{p \in Q_{sp} } \exp (\frac{\left | z^T p - s \right |} {\tau})},
    \label{eq:PSL_CT}
\end{align}
where $Q_n=K_i^{-} \cup Q_{ns}$. $Q_{ns}$ refers to the negative samples, \textit{i.e.}, samples in other classes, and $Q_{sp}$ refers to the soft positive samples which contains samples in the same class $Q_{sc}$ here. The reason we call soft positive samples is that we think samples in the same class share CS information but have distinct IS information.

\subsection{Video Shuffling for PSL}
\label{sec:shuf_PSL}

PSL aims to keep IS information during training, and in this section we introduce how to enlarge CS information through video shuffling. The appearance bias is a significant problem in the OSAR. For instance, the OoD classes \emph{Smile} and \emph{Chew} are easily classified as InD classes \emph{ApplyEyeMakeup} and \emph{ApplyLipstick}, as the majority area of all these classes are occupied by a face, as shown in Fig.~\ref{fig:ood_samples}. The NN is confused by the extremely similar spatial information and neglects the minor different temporal information. This phenomenon encourages us to strengthen the temporal information extraction ability of the NN to distinguish classes with very similar appearances but different actions. We find that introducing a simple yet effective way, \textit{i.e.}, to regard the shuffled video $Q_{shuf}$ as the soft positive sample in Eq.~\ref{eq:PSL_CT}, is extremely suitable and useful in our PSL framework. In this case, $Q_{sp}=Q_{sc} \cup Q_{shuf}$. Shuffled video means shuffling the frames within a single video. As the appearance information of the shuffled video is almost the same as the original video, a smaller than 1 similarity forces the NN to learn the distinct temporal information between them. Unlike existing works which predict the sequence or the type of the shuffled video~\cite{shi2022shuffle, jenni2020video, fernando2017self, lee2017unsupervised}, we regard the shuffle video as a whole sample and directly compare its feature representation with the original video in our PSL. \textbf{We find this technique can improve the closed-set accuracy which indicates more CS information is learned}. We summarize the difference between our PSL and classical C.E. in \cref{fig:psl}.

\subsection{Uncertainty Score}
\label{sec:uncer_score}
As our PSL aims to learn richer CS and IS information in the feature representation, we use the Mahalanobis distance to measure the uncertainty as it can be calculated from the feature representation perspective~\cite{sehwag2021ssd,lee2018simple}:
\begin{equation}
    u=(z-\mu_m)^{T}{\textstyle \sum_{m}^{-1}} (z-\mu_m),
    \label{eq:unce}
\end{equation}
where $\mu_m$ and ${\textstyle \sum_{m}}$ denote the mean and covariance of the whole training set features, and $z$ is the test sample feature.