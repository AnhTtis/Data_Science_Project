
\label{sec:intro}
%%
%\memo{Background}
%%
Frameworks for big data management are widely used in many applications, such as Web services, IoT applications, and scientific analysis.
These applications need to manage petabyte-scale data.
In particular, NoSQL databases are one of the important frameworks for such large-scale big data management.
Historically, many NoSQL databases have their roots in systems such as BigTable~\cite{DBLP:journals/tocs/ChangDGHWBCFG08}, Amazon Dynamo~\cite{DBLP:conf/sosp/DeCandiaHJKLPSVV07}, and Yahoo! PNUTS~\cite{DBLP:journals/pvldb/CooperRSSBJPWY08}. Some recent examples of NoSQL databases are Google F1~\cite{DBLP:conf/sigmod/ShuteOEHRSVWCJLT12} and Spanner~\cite{DBLP:journals/tocs/CorbettDEFFFGGHHHKKLLMMNQRRSSTWW13}: most of these systems are classified as wide-column store (or extensible record store), a general type of NoSQL databases.

%%
%\memo{Background: clarify the importance of predictable cyclic patterns on workload}
%%
In most of the above applications, workloads follow common time-dependent patterns, such as cycles, growth and spikes, or workload evolution~\cite{Ma2018}.
We focus on predictable or pre-scheduled patterns that are common features in automated IoT applications and scientific analysis.
Specifically, we describe two such examples:
\begin{itemize}
\item {\bf IoT applications.} An electric power company utilizes an analytic pipeline to collect power usage from 7.5 million smart meters, put them into a distributed database system, aggregate the power usage, compute its total cost, and then notify electrical power retailers in a timely fashion~\cite{Sasaki2017}.
\item {\bf Astronomical data analysis.} Astronomers use an analytic pipeline to capture images, transform data, calibrate parameters, identify objects, and detect transients and variables~\cite{Takata2020}.
The National Astronomical Observatory of Japan provides a database service on the Web to support such pipelines for astronomers.
The database stores 436 million objects and works on a distributed database system~\cite{PDR2}.
A large number of different queries are executed on the database depending on the type of analysis tasks. 
\end{itemize}
% Toward fast search and real-time inputs of big astronomical catalogs by the new generation relational database, https://www.adass2019.nl/wp-content/uploads/adass-pdf/Poster391.pdf
% Innovative astronomical applications with a new-generation relational database
% https://ui.adsabs.harvard.edu/abs/2020SPIE11452E..26F/abstract
An important feature of those automated analysis pipelines is that they form predictable patterns: workloads are scheduled in advance and they are repeated for a certain time period, such as every day or every week.
So, we can significantly improve the performance of database systems by appropriately designing database schema based on such predictable patterns.

\subsubsection*{Technical trend and Major issues}
Schema design on NoSQL is crucial for achieving high performance for large-scale big data management.
There has been significant research on automated schema design on relational databases~\cite{Agrawal2000,Kimura2010,Rafi2020,DBLP:journals/pvldb/TangSEKF20}, NoSQL~\cite{Vajk2013,Mior2017, CONST2020}, and cloud-scale environment~\cite{Jindal2018,DBLP:conf/socc/JindalPRQYSK19}.
Since our target is a large-scale big data management, we focus on the technical trend of schema design techniques for NoSQL (see Section~\ref{sec:related} for relational databases).
Most existing work assumes that the workload is static (does not change dynamically).
That is, they use an average workload aggregated from a dynamic time-dependent workload. 
As an example, NoSQL Schema Evaluator (NoSE)~\cite{Mior2017} leverages integer linear programming (ILP) for schema design to optimize the execution cost of static workload. 
%% The experiments validated the schema suggested by NoSE outperformed the design manually optimized by database experts. 
However, NoSE is not effective for a time-dependent workload because its average workload may not be a good approximation for the whole workload. 
Also, there is another type of research that adaptively changes schemas as workload changes.
CONST~\cite{CONST2020} is one such example, which heuristically changes schema design over time.
However, it ignores the cost of database migration when schema changes so it does not generate optimal schema designs for time-depended workloads.

\subsubsection*{Technical challenges}
To tackle the above weaknesses of existing techniques, we take an approach for optimizing time-series schema of taking both the execution cost of time-dependent workload and database migrations into account.
However, if we naively extend existing techniques designed for static workloads to dynamic time-dependent ones, we have three obstacles for finding optimal answers using ILP: 1) we need to handle the trade-off between the cost of time-dependent workload execution and database migration, 2) the number of schema candidates blows up depending on the number of time steps, and 3) the number of migration plan candidates also blows up depending on the number of schema candidates.
Here, a migration plan indicates a database transformation from old schema to new schema.

\subsubsection*{Contributions}
We propose new techniques for optimizing time-series schema by effectively reducing the number of schema candidates and the number of migration plan candidates.
The novelty of our proposal is three-fold.

%\memo{Proposal: 1st novelty}
First, we formulate the optimization problem of time-series schema with a single integer linear program for minimizing the total cost of time-dependent workload execution and database migration. 
%The formulation with a single integer linear programming is indispensable, since we need to handle the trade-off between the cost of time-dependent workload and database migrations.
%There is a trade-off between cost of time-dependent workload and database migrations that choosing the best schema at every time step require additional cost of database migration.
% Therefore, we formulate the optimization problem as single integer linear programming to consider the trade-off.
%\memo{Proposal: 2nd novelty}
Second, we propose an efficient schema candidate pruning technique by introducing a new data structure which we call a workload summary tree. % that approximates the ILP for original time-dependent workload.
We decompose the ILP of the original time-dependent workload via approximation into hierarchical local ILPs of smaller sub-workloads. This technique is scalable by effectively pruning uninteresting schema candidates, since each local ILP works efficiently with a small number of time steps while capturing the global feature of its parent workload.
%We design the workload summary tree to have the following features, 1) every child node represents a sub-workload split in time-scale from the workload of its parent node and 2) parent node's workload is more summarized than its child's sub-workload.
\begin{comment}
In detail, 
we recursively construct ILP for a summarized workload assigned to each node starting from the root node and translate its answer as the ILP constraint of its child sub-workloads in order to take the global features into account at local ILPs.
We identify uninteresting schema candidates that are never chosen as ILP answers at any nodes in the tree.
%This approach is efficient, because the time step size of (sub-)workload at every node is kept significantly smaller than that of the original workload.
\end{comment}
%\memo{Proposal: 3rd novelty}
Finally, we propose an effective technique that reduces the number of migration plan candidates.
We notice that workload changes cause optimized query plan changes, which necessitate a database migration. 
So, we can effectively reduce the number of migration plan candidates by restricting them according to how optimized query plans are changed instead of considering arbitrary migration plans.
% among possible query plan candidates. 
\begin{comment}
We first enumerate migration plan candidates query-wisely by ignoring the correlation between queries.
Then, we enumerate additional migration plan candidates that capture the correlation between queries, but we limit only to efficient ones: the key order in an old schema (partition key and clustering key in column families) is kept in the new schema after the migration.
\end{comment}
%We validate the effectiveness of our proposal through intensive experiments by adapting the TPC-H benchmark to generate a time-dependent workload.

\subsubsection*{Paper organization}
The rest of this paper is organized as follows.
We describe the problem statement and the challenges of schema design problem for time-dependent workload (Section~\ref{sec:problem}) and then formulate it with a single integer linear program (Section~\ref{Sec:Optimization}).
We describe the detail of our approach %including a system overview, a column family pruning using workload summary tree, migration plan enumeration, and cost estimation 
(Section~\ref{sec:proposal}). 
\extended{
We validate the effectiveness of our approach through intensive experiments using an extended TPC-H benchmark to various time-dependent workloads (Section~\ref{sec:experiments}).}
We finally position our proposal with respect to the state of the art (Section~\ref{sec:related}) and give concluding remarks (Section~\ref{sec:conclusion}).