\section{Related Work}

In this section, we first review the literature related to the GDSR task, divided into \textit{conventional} and \textit{learning} methods, as well as to vision transformers.

\begin{figure*}
	\centering
	\includegraphics[width=4.5in]{./figs/framework/architecture.pdf}
	\vspace{-0.3cm}
	\caption{\textbf{\netname \ architecture.} Rectangles with different colors depict different stages and functions in each stage.}
	\label{framework}
\end{figure*}

\textbf{Conventional Methods.}
Initially, hand-craft models were developed for GDSR, using the edge co-occurrence between the LR depth map and its HR color counterpart as prior. \cite{kopf2007joint} first utilizes a joint bilateral filter, taking guidance cues from color images. The so-called \textit{local} methods followed this pivotal work: \cite{yang2007spatial} enhances the LR depth maps by exploiting registered HR color images, \cite{riemens2009multistep} uses anti-alias image prefiltering built on the multi-stage joint bilateral filter, while graph-based joint bilateral upsampling \cite{wang2014graph} casts GDSR as a regularization problem. 

More accurate solutions, although slower, are represented by \textit{global} methods. The first work in this direction is \cite{diebel2005application}, which employs Markov random fields (MRF) to integrate multi-modal data for LR depth map upsampling. Using the non-local mean filtering method, \cite{park2011high} recovers noisy LR images from a ToF camera to a high-quality image. To be more efficient, \cite{ferstl2013image} exploits Total Generalized Variation (TGV) regularization for GDSR, enabling a high frame rate. \cite{li2016fast} uses fast global smoothing (FGS) 
to make guided depth interpolation more robust. 


\textbf{Learning Methods.}
Earlier methods from this category exploit MRF~\cite{mac2012patch, kiechle2013joint, kwon2015data}.
However, these techniques rely on manually created dictionaries, whose limited content restricts the capacity of generalizing. More recently, deep learning-based approaches achieved remarkable results and became the preferred choice for GDSR. 
\cite{hui2016depth} designs a multi-scale guided CNN using hierarchical feature extraction to gradually restores blurred edges. To reconstruct sharp edges, the works by Li et al. \cite{li2016deep,li2019joint} learn salient features from color images using an encoder-decoder structure. In contrast, \cite{lutio2019guided} casts GDSR as a pixel-to-pixel mapping from the HR RGB image to the domain of the LR source image, learned by a multi-layer perceptron. In \cite{ye2020pmbanet}, a multi-branch network with progressive refinement 
performs adaptive information fusion to restore depth details. \cite{wang2020depth} can quickly upsample depth maps by learning Canny edges, while \cite{zuo2020frequency} proposes a depth-guided affine transformation where the feature refinement is carried out iteratively. 
\cite{tang2021joint} makes use of implicit neural interpolation, \cite{kim2021deformable} develops a deformable kernel network whose outputs are per-pixel kernels, and \cite{zhao2022discrete} proposes a Discrete Cosine Transform Network (DCTNet) to extract multi-modal features effectively. Through graph optimization, \cite{de2022learning} combines the advantages of model-driven and deep learning-based methods. 
Concurrent works exploit recurrent structure attention \cite{yuan2023recurrent} or combine deep learning with anisotropic diffusion \cite{metzger2022guided}.

Despite substantial advancements, these networks are not effective enough at extracting HF guidance from RGB images. 
Inspired by \cite{liu2021multi}, this paper tackles GDSR leveraging both explicit and implicit HF features guidance. 

\textbf{Vision Transformers.}
\label{Transformer Attention Mechanism}
Transformers, initially designed for natural language processing \cite{vaswani2017attention}, recently gained popularity in computer vision, for tasks such as image recognition \cite{2021An, touvron2021training}, object detection \cite{carion2020end} and semantic segmentation \cite{wang2021pyramid}. 
Vision Transformers (ViTs) learn long-range dependencies across image tokens through self-attention \cite{han2022survey}.
Given the natural advantages of such a mechanism, ViTs targeting low-level vision tasks emerged more recently \cite{zamir2022restormer,lee2022mpvit,pu2022edter}, although requiring much larger amounts of parameters and computing resources. 
