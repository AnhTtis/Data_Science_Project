\section{Introduction}

With the rise of consumer-grade depth cameras, depth maps are employed in various scenarios such as 3D reconstruction \cite{chen20203d, chen2020real}, recognition \cite{cai20103d} and more. Time-of-Flight (ToF) is one of the leading technologies involved in depth sensing, measuring the distance traveled by emitted rays until they reach points in the scenes. However, due to the limitations of physical fabrication, power consumption and costs \cite{bamji2022review}, the resolution of depth maps usually is often insufficient to fulfill the demand of the downstream applications, such as object detection~\cite{chen2021dpanet} and pose estimation~\cite{ge2019real}. 
In contrast, collecting RGB images at much higher resolution is cheaper. As a result, the guided depth super-resolution task, known as GDSR, has emerged as a crucial solution to this technological limitation, allowing to obtain an accurate high-resolution (HR) depth map from a low-resolution (LR) one, guided by an HR image.

Initially, algorithms addressing this problem were classified into local \cite{kopf2007joint, yang2007spatial, riemens2009multistep, wang2014graph} and global \cite{diebel2005application, park2011high, ferstl2013image, li2016fast}, with the former family being faster, yet suffering in low-textured regions and the latter resulting more robust, at the expense of processing time.
More recently, deep neural networks have become the preferred choice for depth super-resolution \cite{hui2016depth, li2016deep, li2019joint, lutio2019guided, tang2021bridgenet}, although they still struggle to restore sharp and precise edges from LR depth maps reliably, especially when dealing with large upsampling factors. 
This is mainly due to the inadequate guidance provided by High-Frequency (HF) features, implicitly modeled by deep networks, which frequently cause texture copying effects in the upsampled depth maps.
In addition, single-stage multi-scale architectures for this task \cite{ye2020pmbanet, wang2020depth, zuo2020frequency, tang2021joint}, at any given scale, cannot fully leverage fine details encoded at the higher ones, as they are lost due to down-sampling and only partially recovered through skip connections.


In light of the two weaknesses highlighted so far, we aim to improve GDSR by explicitly countering them. For the former, we argue that explicit extraction of HF features, supported by edge detection algorithms such as the Canny operator, can play a crucial role \cite{wang2020depth}. Concerning the latter, multi-stage network design -- which outperforms single-stage counterparts in high-level visual tasks like action segmentation \cite{farha2019ms} and pose estimation \cite{chen2018cascaded}, as well as for low-level vision problems such as image restoration \cite{zamir2021multi, kim2022mssnet} -- can mitigate the information loss issue. However, since features extracted from RGB images need to be considered in addition to depth features, existing multi-stage networks are inadequate for GDSR and should be revised to fuse features from the two domains.

In this paper, we present a \underline{D}epth \underline{S}uper-\underline{R}esolution method leveraging both \underline{E}xplicit and \underline{I}mplicit HF information (\underline{\netname}), which contains two branches: the High-Frequency Extraction Branch (HFEB) and the Guided Depth Restoration Branch (GDRB). The former is designed to model \textbf{explicit} HF features by exploiting dynamic self-calibrated convolutions (DSP) and the power of vision transformers blocks.
The latter effectively fuses the guidance from RGB features with depth features to obtain HR depth maps. This is achieved by deploying two novel modules: 1) the Adaptive Feature Fusion Module (AFFM), which counters the HF information loss due to downsampling, and 2) the Low-Cut Filtering (LCF) module, which acts in the frequency domain to improve \textbf{implicit} extraction of HF features.
Exhaustive experiments on several standard datasets show the superiority of \netname. In summary, the main contributions of this paper are:

\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
\item{The proposed architecture employs a novel efficient transformer for explicit, HF feature extraction. The transformer can accurately capture image details and structures from depth maps.}
\item{In the guided depth restoration branch, we propose a low-cut filtering module that can obtain accurate, {implicit} HF information.}
\item{To counter the information loss issue, we propose an Adaptive Feature Fusion Module located in the middle of the guided depth restoration branch.}
\item{Quantitative and qualitative experimental results demonstrate that our approach establishes a new state-of-the-art in the field of guided depth super-resolution.}
\end{itemize}
Fig. \hyperref[fig:teaser]{1} provides a high-level view of our framework, followed by examples that anticipate the superior accuracy achieved by \netname{} compared to existing methods \cite{zhao2022discrete,de2022learning}. 