\section{Experimental Results}
In this section, we validate the effectiveness of our proposal. We first introduce datasets, metrics and implementation details involved in our evaluation. Then, we compare \netname{} with state-of-the-art methods, conduct an ablation study on our model and, finally, discuss its limitations.


\begin{table*}[htbp] \scriptsize
	\renewcommand\tabcolsep{2.3pt} 
	\centering
	\scalebox{0.85}{
	\begin{tabular}{@{}ccccccccccccccccc@{}}
		\toprule
		 Dataset & Scale & Metrics & GF~\cite{he2010guided} & SD~\cite{ham2017robust}  & GSRPT~\cite{lutio2019guided} & MSG~\cite{hui2016depth} & DKN~\cite{kim2021deformable} & FDKN~\cite{kim2021deformable} & PMBANet~\cite{ye2020pmbanet} & FDSR~\cite{he2021towards} & JIIF~\cite{tang2021joint} & DCTNet~\cite{zhao2022discrete} & LGR~\cite{de2022learning} & DADA~\cite{metzger2022guided} & DSR-EI & DSR-EI$^+$ \\ \midrule
		\multirow{6}{*}{\rotatebox[origin=l]{90}{\scriptsize \textbf{Middlebury}}} & \multirow{2}{*}{$4\times$} 
		& MSE & 33.3 & 24.9 & 39.8 & 4.13 & 4.29 & 3.60 & 4.72 & 7.72 & 2.70 & 5.00 & 3.04 & \bronze{2.58} & \gold{2.46} & \silver{2.56} \\
		& & MAE & 1.27 & 0.46 & 0.79 & 0.22 & 0.18 & 0.16 & 0.25 & 0.35 & \bronze{0.11} & 0.24 & 0.13 & \bronze{0.11} & \silver{0.08} & \gold{0.07} \\ \cline{2-17}
		& \multirow{2}{*}{$8\times$} 
		& MSE & 40.5 & 82.5 & 32.7 & 10.5 & 11.2 & 10.4 & 9.48 & 23.2 & 8.01 & 15.1 & 7.26 & \silver{5.68} & \bronze{6.20} & \gold{5.13} \\
		& & MAE & 1.49 & 0.86 & 0.82 & 0.43 & 0.38 & 0.37 & 0.38 & 0.69 & 0.27 & 0.57 & 0.24 & \bronze{0.20} & \gold{0.18} & \gold{0.18} \\ \cline{2-17}
		& \multirow{2}{*}{$16\times$} 
		& MSE & 67.4 & 511 & 41.5 & 34.2 & 47.6 & 38.5 & 30.6 & 55.4 & 37.5 & 52.3 & 24.7 & \silver{16.3} & \gold{15.8} & \bronze{16.6}  \\
		& & MAE & 2.21 & 1.73 & 1.24 & 1.06 & 1.42 & 1.18 & 0.89 & 1.51 & 0.98 & 1.50 & 0.67 & \bronze{0.48} & \silver{0.47} & \gold{0.40} \\ \hline\hline
	    % middlebury end
		\multirow{6}{*}{\rotatebox[origin=l]{90}{\scriptsize \textbf{NYUv2}}} & \multirow{2}{*}{$4\times$}
		& MSE & 114 & 36.0 & 112 & 6.85 & 11.4 & 9.07 & 10.8 & 10.1 & \bronze{3.28} & 3.63 & 6.45 & 4.83 & \silver{2.82} & \gold{2.75}\\
		& & MAE & 3.91 & 1.31 & 3.61 & 0.81 & 1.03 & 0.85 & 0.93 & 0.94 & \bronze{0.52} & 0.68 & 0.73 & 0.64 & \silver{0.49} & \gold{0.47}\\ \cline{2-17}
		& \multirow{2}{*}{$8\times$} 
		& MSE & 142 & 105 & 122 & 24.1 & 29.8 & 29.9 & 17.2 & 19.5 & \bronze{15.2} & 20.9 & 19.6 & 16.6 & \gold{11.8} & \gold{11.8}\\
		& & MAE & 4.47 & 2.57 & 3.86 & 1.66 & 1.82 & 1.80 & 1.38 & 1.38 & \bronze{1.29} & 1.79 & 1.42 & 1.30 & \silver{1.12} & \gold{1.09}\\ \cline{2-17}
		& \multirow{2}{*}{$16\times$} 
		& MSE & 249 & 533 & 219 & 84.5 & 115 & 113 & 84.9 & 86.4 & 59.9 & 77.0 & 67.5 & \bronze{59.0} & \silver{47.8} & \gold{47.1} \\
		& & MAE & 6.34 & 5.07 & 5.40 & 3.35 & 4.01 & 3.95 & 3.26 & 3.35 & 2.81 & 3.61 & 2.90 & \bronze{2.64} & \silver{2.48} & \gold{2.40}\\ \hline\hline
		% NYU end
		\multirow{6}{*}{\rotatebox[origin=l]{90}{\scriptsize \textbf{DIML}}} & \multirow{2}{*}{$4\times$}
		& MSE & 25.6 & 10.5 & 20.7 & 1.73 & 3.47 & 2.20 & 3.05 & 2.75 & \bronze{1.19} & 2.09 & 1.68 & 1.33 & \silver{0.70} & \gold{0.65} \\
		& & MAE & 1.45 & 0.40 & 1.15 & 0.22 & 0.33 & 0.23 & 0.31 & 0.29 & \bronze{0.16} & 0.31 & 0.20 & 0.17 & \silver{0.13} & \gold{0.12} \\ \cline{2-17}
		& \multirow{2}{*}{$8\times$} 
		& MSE & 34.1 & 44.9 & 23.0 & 4.13 & 5.47 & 5.95 & 5.87 & 8.40 & 3.65 & 7.08 & 3.51 & \bronze{2.93} & \silver{2.12} & \gold{2.09} \\
		& & MAE & 1.77 & 0.83 & 1.26 & 0.40 & 0.45 & 0.47 & 0.47 & 0.66 & 0.32 & 0.65 & 0.31 & \bronze{0.28} & \gold{0.22} & \gold{0.22} \\ \cline{2-17}
		& \multirow{2}{*}{$16\times$} 
		& MSE & 66.3 & 41.1 & 39.3 & 13.0 & 19.3 & 20.8 & 13.8 & 32.9 & 11.7 & 23.4 & 9.45 & \bronze{7.61} & \gold{6.29} & \silver{6.31} \\
		& & MAE & 2.74 & 1.91 & 1.78 & 0.93 & 1.20 & 1.24 & 0.87 & 1.66 & 0.81 & 1.75 & 0.68 & \bronze{0.59} & \silver{0.52} & \gold{0.50} \\
		% DIML end
    \bottomrule
	\end{tabular}}
    \vspace{-0.3cm}
	\caption{\textbf{Results on Middlebury, NYUv2 and DIML datasets.} The lower the MSE and MAE, the better.}
	\label{sota_comparison_mid_nyu_diml}
\end{table*}



\begin{table*}[t] \footnotesize
	\renewcommand\tabcolsep{1.5pt} 
	\centering
	\scalebox{0.85}{
	\begin{tabular}{@{}ccccccccccccccccc@{}}
		\toprule
		 Scale & SDF~\cite{li2016deep} & SVLRM~\cite{pan2019spatially} & DJF~\cite{li2016deep} & DJFR~\cite{li2019joint} & PAC~\cite{su2019pixel} & CUNet~\cite{deng2020deep} & FDKN~\cite{kim2021deformable} & DKN~\cite{kim2021deformable} & FDSR~\cite{he2021towards} & DCTNet~\cite{zhao2022discrete} & RSAG~\cite{yuan2023recurrent} & DSR-EI & DSR-EI$^+$ \\ \midrule
		$4\times$ & 2.00 & 3.39 & 3.41 & 3.35 & 1.25 & 1.18 & 1.18 & 1.30 & 1.16 & \bronze{1.07} & 1.14 & \gold{0.91} & \gold{0.91} \\
		$8\times$ & 3.23 & 5.59 & 5.57 & 5.57 & 1.98 & 1.95 & 1.91 & 1.96 & 1.82 & 1.78 & \bronze{1.75} & \gold{1.37} & \silver{1.38} \\
		$16\times$ & 5.16 & 8.28 & 8.15 & 7.99 & 3.49 & 3.45 & 3.41 & 3.42 & 3.06 & 3.18 & \bronze{2.96} & \gold{2.10} & \gold{2.10}  \\
    \bottomrule
	\end{tabular}}
	\vspace{-0.3cm}
	\caption{\textbf{Results on the RGBDD dataset.} We report RMSE, the lower the better.}
	\label{sota_comparison_rgbdd}
\end{table*}



\subsection{Datasets and Metrics}
We evaluate \netname{} on four datasets, compared with existing methods when super-solving depth maps by three different upsampling factors: $4\times,\ 8\times$, and $16\times$. 

\textbf{Middlebury}\cite{scharstein2003high,scharstein2007learning,hirschmuller2007evaluation,scharstein2014high}. We train all learning-based methods using 50 RGB-D images with ground truth from Middlebury 2005, 2006 and 2014 datasets. As in~\cite{de2022learning}, we retain 5 for validation and 5 for testing. 

\textbf{NYUv2}\cite{silberman2012indoor}. It contains 1449 RGB-D images in total. Following \cite{de2022learning}, we randomly split it into 849 RGB-D images for the training set, 300 for the validation set and 300 for the test set. Compared to \cite{ye2020pmbanet,liu2022pdr}, it comes with a validation set to make the comparison fairer.

\textbf{DIML}\cite{kim2016structure,kim2017deep,kim2018deep,cho2021deep} consists of 2 million color images and corresponding depth maps from indoor and outdoor scenes. We adopt the same strategy outlined in \cite{de2022learning}, i.e., considering only the indoor data subset, and use 1440 for training, 169 for validation, and 503 for testing.

\textbf{RGBDD}\cite{he2021towards} is a new real-world dataset for GDSR, which consists of 4811 image pairs. For evaluation, we follow the protocol described in \cite{he2021towards}, using 2215 images (1586 portraits, 380 plants, 249 models) as the training set and 405 images (297 portraits, 68 plants, 40 models) as the test set. 

\textbf{Metrics.} Following \cite{de2022learning}, we compute mean square error (MSE / $cm^2$) and mean absolute error (MAE / $cm$) as metrics on Middlebury, NYUv2 and DIML. For RGBDD, we use root mean square error (RMSE / $cm$) as in \cite{he2021towards}. 

\subsection{Implementation Details}
During training, the HR depth maps and the color images are randomly cropped into $256\times 256$ patches. LR depth patches are generated by bicubic interpolation at $64\times 64$, $32\times 32$, $16\times 16$ resolution for $4\times$, $8\times$ and $16\times$ factors, respectively. We randomly extract about 75K, 168K, 223K and 232K patches from Middlebury, NYUv2, DIML and RGBDD for training. Before being fed to the network, depth maps and images are normalized in the [0, 1] range.

We use Pytorch \cite{paszke2019pytorch} to implement and train \netname{}, on a single Nvidia RTX 3090 GPU. The batch size is set to 4, using Adam as the optimizer. The learning rate is initialized to $1\times 10^{-4}$, then performing a 5-epoch warm-up and cosine annealing. We use random rotation, horizontal/vertical flipping as data augmentation. According to the size of the four datasets, we train our network for 1505, 198, 155 and 109 epochs on Middlebury, NYUv2, DIML and RGBDD, respectively. 
When evaluating results on a specific dataset, we do not perform any pre-training on the others. Following \cite{de2022learning}, testing is performed by processing $256\times256$ patches at a time on Middlebury, NYUv2 and DIML for fairness, while full-resolution images are processed for RGBDD.

\begin{figure*}[t] 
	\centering
	\renewcommand\tabcolsep{1.5pt} 
	\begin{tabular}{cccccccccccc}
	\vspace{-0.1cm}
    \rotatebox[origin=l]{90}{\scriptsize \quad \textbf{Middlebury}} & \includegraphics[height=0.6in]{./figs/sota_comp_middlebury/389/Middlebury_389_img.pdf}
        \hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_middlebury/389/Middlebury_389_source.pdf}
	\hspace{-1.8mm} &  \includegraphics[height=0.6in]{./figs/sota_comp_middlebury/389/Middlebury_389_GT.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_middlebury/389/Middlebury_389_PMBA.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_middlebury/389/Middlebury_389_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_middlebury/389/Middlebury_389_JIIF.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_middlebury/389/Middlebury_389_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_middlebury/389/Middlebury_389_LGR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_middlebury/389/Middlebury_389_MSS.pdf}
        
        \hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_middlebury/389/Middlebury_389_ours.pdf}
    \\ \vspace{-0.1cm}
    
    \rotatebox[origin=l]{90}{\scriptsize \quad \textbf{NYUv2}} & \includegraphics[height=0.6in]{./figs/sota_comp_nyu/357/NYU_357_img.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_nyu/357/NYU_357_source.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_nyu/357/NYU_357_GT.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_nyu/357/NYU_357_PMBA.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_nyu/357/NYU_357_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_nyu/357/NYU_357_JIIF.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_nyu/357/NYU_357_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_nyu/357/NYU_357_LGR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_nyu/357/NYU_357_MSS.pdf}
 
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_nyu/357/NYU_357_ours.pdf}
	\\ 
	
    \rotatebox[origin=l]{90}{\scriptsize \quad \textbf{DIML}} & \includegraphics[height=0.6in]{./figs/sota_comp_diml/856/DIML_856_img.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_diml/856/DIML_856_source.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_diml/856/DIML_856_GT.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_diml/856/DIML_856_PMBA.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_diml/856/DIML_856_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_diml/856/DIML_856_JIIF.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_diml/856/DIML_856_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_diml/856/DIML_856_LGR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_diml/856/DIML_856_MSS.pdf}
 
	\hspace{-1.8mm} & \includegraphics[height=0.6in]{./figs/sota_comp_diml/856/DIML_856_ours.pdf}
 \\
	& \scriptsize \textbf{(a)} RGB & \scriptsize \textbf{(b)} Bicubic & \scriptsize \textbf{(c)} GT & \scriptsize \textbf{(d)} PMBA & \scriptsize \textbf{(e)} FDSR & \scriptsize \textbf{(f)} JIIF & \scriptsize \textbf{(g)} DCTNet & \scriptsize \textbf{(h)} LGR & \scriptsize \textbf{(i)} \netname{} & \scriptsize \textbf{(j)} \netname{} (depth)
	\end{tabular}
    \vspace{-0.3cm}
	\caption{\textbf{Qualitative comparison on Middlebury, NYUv2 and DIML datasets (scaling factor $8\times$).} From left to right: (a) RGB image, (b) Bicubic upsampled depth map, (c) GT; then, error maps achieved by selected methods: (d) PMBA~\cite{ye2020pmbanet}, (e) FDSR~\cite{he2021towards}, (f) JIIF~\cite{tang2021joint}, (g) DCTNet~\cite{zhao2022discrete}, (h) LGR~\cite{de2022learning}; finally, (i) error maps and (j) predictions by \netname.} 
	\label{qualitative}
\end{figure*}


\begin{table*}[htbp] \footnotesize
	\renewcommand\tabcolsep{1.5pt} 
	\centering
	\scalebox{0.85}{
	\begin{tabular}{@{}ccccccccccccccccc@{}}
		\toprule
		 Testing Dataset & Metric & GF\cite{he2010guided} & SD~\cite{ham2017robust}  & GSRPT~\cite{lutio2019guided} & MSG~\cite{hui2016depth} & FDKN~\cite{kim2021deformable} & PMBANet~\cite{ye2020pmbanet} & FDSR~\cite{he2021towards} & JIIF~\cite{tang2021joint} & DCTNet~\cite{zhao2022discrete} & LGR~\cite{de2022learning} & \netname$^+$ \\ \midrule
		\multirow{2}{*}{DIML}
		& MSE & 34.1 & 44.9 & 23.0 & 5.76 & 6.74 & 7.35 & 7.73 & \silver{4.10} & 5.64 & \bronze{4.95} & \gold{3.72} \\
		& MAE & 1.77 & 0.83 & 1.26 & 0.51 & 0.53 & 0.59 & 0.74 & \silver{0.38} & 0.77 & \bronze{0.40} & \gold{0.36} \\ \hline
		\multirow{2}{*}{Middlebury\textit{-HR}}
		& MSE & 40.5 & 82.5 & 32.7 & 11.0 & \bronze{10.0} & \silver{9.62} & 18.4 & 19.3 & 17.5 & \gold{8.25} & 14.6 \\
		& MAE & 1.49 & 0.86 & 0.82 & 0.54 & \silver{0.43} & \bronze{0.46} & 0.73 & 0.74 & 0.77 & \gold{0.35} & 0.54  \\ \hline
		\multirow{2}{*}{Middlebury\textit{-LR}}
		& MSE & 25.6 & 28.8 & 15.8 & 8.89 & 5.54 & 4.16 & 6.92 & 4.40 & 6.96 & 5.94 & \gold{3.44} \\
		& MAE & 2.31 & 2.07 & 1.73 & 1.62 & 0.99 & \silver{0.91} & 1.09 & \bronze{0.92} & 1.15 & 1.11 & \gold{0.87}  \\
        \bottomrule
	\end{tabular}}
	\vspace{-0.3cm}
	\caption{\textbf{Cross-dataset generalization.} All methods are trained on NYUv2 and tested on DIML/Middlebury with factor $8\times$. Middlebury\textit{-HR} is the test set defined in \cite{de2022learning}, Middlebury\textit{-LR} is the one from \cite{tang2021joint}. The lower MSE and MAE, the better. }
	\label{cross-data_comparison}
\end{table*}

\subsection{Comparison with State-of-the-Art}
We compare \netname{} to GF \cite{he2010guided}, SD \cite{ham2017robust}, GSRPT \cite{lutio2019guided}, MSG \cite{hui2016depth}, DKN and its fast implementation FDKN \cite{kim2021deformable}, PMBANet \cite{ye2020pmbanet}, FDSR \cite{he2021towards}, JIIF \cite{tang2021joint}, DCTNet \cite{zhao2022discrete}, LGR \cite{de2022learning}, and finally to DADA~\cite{metzger2022guided} on Middlebury, NYUv2 and DIML datasets. We could not compare with PDRNet \cite{liu2022pdr} under the same setting because the source code is unavailable at the time of writing. For the other methods, we use the results from \cite{de2022learning} or the officially published codes, and results from \cite{yuan2023recurrent,metzger2022guided} for concurrent works. On the RGBDD dataset, the proposed network is compared to SDF~\cite{li2016deep}, SVLRM \cite{pan2019spatially}, DJF~\cite{li2016deep}, DJFR~\cite{li2019joint}, PAC~\cite{su2019pixel}, CUNet~\cite{deng2020deep}, FDKN~\cite{kim2021deformable}, DKN~\cite{kim2021deformable}, FDSR~\cite{he2021towards}, DCTNet~\cite{zhao2022discrete} and RASG~\cite{yuan2023recurrent}. To be fair with DCTNet~\cite{zhao2022discrete}, we downsample depth maps as the LR input.  
When reporting results, we highlight \gold{absolute}, \silver{second} and \bronze{third} best methods for each metric on each dataset.

\textbf{Quantitative Comparison.} Tabs. \ref{sota_comparison_mid_nyu_diml} and \ref{sota_comparison_rgbdd} report the accuracy of super-solved depth maps at factors $4\times$, $8\times$ and $16\times$ on the four datasets. As expected, learning-based methods show a significant improvement over traditional methods \cite{he2010guided,ham2017robust,lutio2019guided}. \netname{} vastly outperforms any existing network, with larger gaps in accuracy with the increasing of the upsampling factor. This can be attributed to the limitations affecting existing methods, i.e., 1) the guidance of either explicit or implicit RGB features alone being insufficient; 2) multi-modal information fusion on a single scale being not flexible enough to deal with complex scenes. Both limitations are fully addressed by \netname, which consistently outperforms concurrent works \cite{metzger2022guided,yuan2023recurrent}. 


The margin is consistent both on perfect (Middlebury) and noisy datasets (NYUv2, DIML, RGBDD), with the latter being a more challenging, realistic benchmark. Although \netname$^+$ is definitely the absolute best, its margin over \netname{} is negligible, with tiny gains yielded by NLSPN with respect to our main modules. Indeed, \netname{} alone consistently outperforms any other approach already.

       
\textbf{Qualitative Comparison.}
Fig. \ref{qualitative} shows qualitative comparisons of $8\times$ super-solved depth maps on Middlebury, NYUv2 and DIML datasets, respectively. From left to right, we show, the RGB image and LR depth map, followed by the ground truth HR depth and error maps obtained by several state-of-the-art frameworks, concluding with ours in the second-to-last columns. In each of the three examples, the lower error magnitude produced by \netname{}$^+$ further demonstrates its superior accuracy. 

\textbf{Cross-dataset Generalization.}
We conclude the comparison with existing methods by conducting cross-dataset experiments with $8\times$ factor. All methods are trained on the NYUv2 dataset and directly evaluated on DIML and Middlebury. Table \ref{cross-data_comparison} collects quantitative results for the 11 selected methods. Again, CNN-based methods attain better performance than traditional approaches, despite the domain gap playing a significant role in performance -- as evident by comparing results with Table \ref{cross-data_comparison}. Nonetheless, \netname{} outperforms any other framework on DIML. 


\begin{figure}	
	\centering	
	\captionsetup[subfigure]{font=footnotesize,textfont=footnotesize}
	\subfloat[RGB]{	
		\centering	
		\label{cross_dataset} 
		\includegraphics[height=0.8in]{./figs/ablation_figure/cross_dataset/receptive_field/cross_dataset.pdf}}	
	\hspace{-2mm}
	\subfloat[$D_{hr}$]{	
		\centering	
		\label{HR}
		\includegraphics[width=0.8in]{./figs/ablation_figure/cross_dataset/receptive_field/HR.pdf}}
	\hspace{-2mm}
	\subfloat[$D_{lr}$]{	
		\centering	
		\label{LR}
		\includegraphics[width=0.8in]{./figs/ablation_figure/cross_dataset/receptive_field/LR.pdf}}
		\vspace{-0.3cm}
	\caption{\textbf{Image context processed on Middlebury -- HR vs LR.} (a) RGB image and depth patches $D$ processed when testing on (b) Middlebury\textit{-HR} and (c) Middlebury\textit{-LR}. }	
	\label{hr-lr} 
\end{figure}

When considering the Middlebury dataset, we evaluate using the setting proposed in \cite{de2022learning} -- Middlebury\textit{-HR} in the table. In this case, our results are slightly less accurate compared to a few existing methods. However, given the very high resolution of Middlebury images, we argue that this testing protocol -- i.e., consisting of processing $256\times 256$ crops at a time -- penalizes our network's ability to leverage the global context in the input that results irremediably reduced to a very local area in these images. Therefore, we also evaluate on Middlebury test set defined by~\cite{tang2021joint} -- Middlebury-\textit{LR} in the table. Note that different subsets of images are used in Middlebury\textit{-HR} and Middlebury-\textit{LR} splits. Besides, Middlebury-\textit{LR} images are resized and processed without cropping, i.e., used at full-size after resizing, allowing to fully exploit global context, while this is not feasible with Middlebury-\textit{HR} due to memory constraints. In this case, \netname{} attains the best performance again, confirming our previous analysis, as shown in Tab. \ref{cross-data_comparison}. Such a difference in terms of context is highlighted in Fig. \ref{hr-lr}.

\begin{table}[t]
    \centering
	\renewcommand\tabcolsep{3pt} 
    \scalebox{0.5}{
    \begin{tabular}{ccc}

    \begin{tabular}{@{}ccccccc@{}} %\label{hf_infomation}
		\toprule
		\textbf{No.} & \textbf{Gradient} & \tabincell{c}{\textbf{Shallow} \\ \textbf{Feature}} & \textbf{LCF} & \textbf{ResBlock} & \textbf{MSE} & \textbf{MAE}\\
		\midrule
		(\uppercase\expandafter{\romannumeral1}) & \XSolidBrush &  \Checkmark     &  \Checkmark &  & 13.1 & 1.19 \\
		(\uppercase\expandafter{\romannumeral2}) & \Checkmark &    \XSolidBrush   &   &  & 12.4 & 1.14 \\
		(\uppercase\expandafter{\romannumeral3}) & \Checkmark &    \Checkmark     &   & \Checkmark & 12.3 & 1.15 \\
		\rowcolor{LightYellow}
		(\uppercase\expandafter{\romannumeral4}) & \Checkmark &    \Checkmark     & \Checkmark  &  & \gold{11.8} & \gold{1.12} \\
		\bottomrule
	\end{tabular}
	
	& \quad &
	
	\begin{tabular}{@{}clcc@{}} %\label{edge_types}
		\toprule
		\specialrule{0em}{3pt}{3pt}
		\multicolumn{1}{c}{\textbf{No.}} & 
		\tabincell{l}{\textbf{HF Information} \textbf{ \quad\quad\quad\quad}} & \textbf{MSE} & \textbf{MAE}\\
		\specialrule{0em}{3pt}{2pt}
		\midrule
		(\uppercase\expandafter{\romannumeral1}) & 
		{Canny Edge} & 12.0 & 1.13 \\
		(\uppercase\expandafter{\romannumeral2}) & 
		{Gaussian Edge} & 12.1 & 1.16 \\
		(\uppercase\expandafter{\romannumeral3}) & 
		{DCT} & 12.1 & 1.15 \\
		(\uppercase\expandafter{\romannumeral4}) & 
		{Wavelet Transform} & 12.1 & 1.15  \\
		\rowcolor{LightYellow}
		(\uppercase\expandafter{\romannumeral5}) & 
		{Gradient Map} & \gold{11.8} & \gold{1.12} \\
		\bottomrule
	\end{tabular}
	
	\\
	\textbf{(a)} & \quad & \textbf{(b)} 
	\\
	\\
	
	
	\begin{tabular}{@{}clcccc@{}} %\label{dsp_ablation}
		\toprule
		\textbf{No.} & \textbf{Config.} & \textbf{Params (M)} & \textbf{Flops (G)} & \textbf{MSE} & \textbf{MAE}\\
		\midrule
		(\uppercase\expandafter{\romannumeral1}) & EdgeNet \cite{liu2021multi}    & 5.78 &  95.6  & 12.0 & \gold{1.12} \\
		(\uppercase\expandafter{\romannumeral2}) & SCPA \cite{zhao2020efficient}  & 0.29 &  13.1  & 12.5 & 1.16 \\
		\rowcolor{LightYellow}
		(\uppercase\expandafter{\romannumeral3}) & HFEB       & \gold{0.27} & \gold{11.6}  & \gold{1.18} & \gold{1.12} \\
		\rowcolor{white}
		\bottomrule
		\multicolumn{4}{c}{\quad\quad\textbf{(c)}} \\
		\\
		\toprule
		\textbf{No.} & \textbf{Config.} & \textbf{Params (M)} & \textbf{MSE} & \textbf{MAE}\\
		\midrule
		(\uppercase\expandafter{\romannumeral1}) & 
		w/o AFFM        & -   & 12.7 & 1.16 \\
		(\uppercase\expandafter{\romannumeral2}) & 
		w/o att         & 1.3 & 12.2 & 1.13 \\
		(\uppercase\expandafter{\romannumeral3}) & 
		Concat.  & 4.5 & 12.2 & 1.13 \\
		\rowcolor{LightYellow}
		(\uppercase\expandafter{\romannumeral4}) & 
		AFFM & 3.0 & \gold{11.8} & \gold{1.12} & \\
		\rowcolor{white}
		\bottomrule
		\multicolumn{4}{c}{\quad\quad\textbf{(e)}} \\
	\end{tabular}
	
	
	& \quad &
	
	
	\begin{tabular}{@{}clccc@{}} %\label{affm_setting}
		\toprule
		\textbf{No.} & \textbf{Scales} & \textbf{Params (M)} & \textbf{MSE} & \textbf{MAE}\\
		\midrule
		(\uppercase\expandafter{\romannumeral1}) & 
		H1              & 1.5 & 12.3 & 1.14 \\
		\rowcolor{LightYellow}
		(\uppercase\expandafter{\romannumeral2}) & 
		H1, H2       & 3.0 & \gold{11.8} & \gold{1.12} \\
		\rowcolor{white}
		(\uppercase\expandafter{\romannumeral3}) & 
		H1, H2, H3              & 4.5 & \gold{11.8} & \gold{1.12} \\
		\bottomrule
		\multicolumn{4}{c}{\textbf{(d)}} \\
% 		\\
% 		\\
        \specialrule{0em}{5.4pt}{5.4pt} %
		\toprule
		\specialrule{0em}{1.7pt}{1.7pt} %
		\textbf{No.} & \textbf{Stages} & \textbf{Params (M)} & \textbf{MSE} & \textbf{MAE}\\
		\specialrule{0em}{1.7pt}{1.7pt} %
		\midrule
		\specialrule{0em}{1.8pt}{1.8pt} %
		(\uppercase\expandafter{\romannumeral1}) & 
		$1$   & 14.2 & 13.3 & 1.19 \\
		\specialrule{0em}{1.8pt}{1.8pt} %
		\rowcolor{LightYellow}
		(\uppercase\expandafter{\romannumeral2}) & 
		$2$   & 25.0 & 11.8 & 1.12 \\
		\specialrule{0em}{1.8pt}{1.8pt} %
		\rowcolor{white}
		(\uppercase\expandafter{\romannumeral3}) & 
		$3$   & 37.5 & \gold{11.6} & \gold{1.10} \\
		\specialrule{0em}{1.8pt}{1.8pt} %
		\bottomrule
		\multicolumn{4}{c}{\quad\quad\textbf{(f)}} \\
	\end{tabular}
	
    \end{tabular}}
    \vspace{-0.3cm}
    \caption{\textbf{Ablation study (NYUv2 test set, $8\times$ factor).} We measure the impact of (a) explicit vs implicit HR features, (b) different kinds of HF supervision, (c) different sub-networks for explicit HF features extraction, (d) scales at which AFFM is applied, (e) modules building AFFM, (f) number of stages in GDRB. In yellow, configurations corresponding to our final model without NLSPN.}
    \label{tab:ablations}
\end{table}


\subsection{Ablation Study}
We now perform a series of ablation experiments to measure the impact of key components and parameters in \netname. Tab. \ref{tab:ablations} collects the outcome of these studies, conducted on NYUv2 test set with $8\times$ factor. Without loss of fairness, NLSPN is never used here -- to fully focus on the impact of single components. 

\textbf{(a) Implicit vs Explicit High-Frequency Features.}
To measure the impact of both implicit and explicit HR features, we compare the performance of the proposed network and its variants when extracting either only one of the two. The quantitative results are collected in Tab.~\ref{tab:ablations}(a). Without the help of gradient maps (I), the performance of the network significantly degrades. We believe this is caused by the difficulty in effectively extracting fine structures or salient edges required for LR depth maps from implicit HF features alone. Moreover, explicit features highlight regions in the image that need to be focused on, avoiding \netname{} to learn to localize them and easing its task. 


Nonetheless, explicit HF features alone as guidance (II) are insufficient as well. We argue that the explicit information might neglect some RGB features, whereas implicit HF feature extraction can recover them. Furthermore, to verify the effectiveness of LCF, we replace it with ResBlock~\cite{he2016deep} (III) to extract shallow features from RGB images, highlighting a negative impact on implicit features extraction -- i.e., it results less accurate than (II). 

\textbf{(b) Ablation on Explicit High-Frequency Features.}
We now investigate which kind of HF information is more effective for our framework. Purposely, we train HFEB with supervision coming from five different HF features used as ground truth edge maps $E_{gt}$. Tab.~\ref{tab:ablations}(b) collects results from this experiment, highlighting that Canny edges (I) and Gradient maps (V) lead to slightly better results. 


\textbf{(c) Impact of HFEB.}
To verify the effectiveness of HFEB, we replace it with EdgeNet~\cite{liu2021multi} -- based on the widely-used U-net structure -- and SCPA~\cite{zhao2020efficient}, which inspires our scaling strategy. As shown in Tab.~\ref{tab:ablations}(c), EdgeNet (I) achieves lower MSE and MAE than SCPA (II), yet needs more parameters -- 5.78M vs. 0.29M. HFEB (III) yields the same accuracy as EdgeNet, with fewer parameters than SCPA, thus being both more accurate and efficient. 



\textbf{(d -- e) Impact of AFFM.}
We now measure the effectiveness of AFFM. Tab.~\ref{tab:ablations}(d) shows results obtained by deploying AFFM at different scales, respectively the highest (I), the first two (II) and all of the three scales. We can notice how performing fusion at the highest scale alone results insufficient, whereas using multi-scale features for fusion yields improvements, despite saturating already when using two scales, with the lowest one not providing additional, meaningful details to be taken into account.

Furthermore, we ablate AFFM in its single components. Tab.~\ref{tab:ablations}(e) resumes the outcome of this evaluation. 
We first test the performance of \netname{} without AFFM (I), highlighting a large drop in accuracy. By adding dynamic fusion, yet without using attention (II) vastly improves the results already, while replacing the weighted sum in the upper of Fig.~\ref{affm} with concatenation and a ResBlock~\cite{he2016deep} (III) yields worse results compared to our full AFFM (IV). 

\textbf{(f) Impact of Stages Number.}
To conclude, we evaluate the impact of the multi-stage design.
As shown in Tab.~\ref{tab:ablations}(f), a single-stage architecture (I) is vastly outperformed by deploying two stages (II), yet at the expense of doubling the number of parameters. Furthermore, while the three-stage architecture (III) still yields some improvement, the benefit is minor in comparison to the significant increase in parameters. Hence, we choose two stages as the default configuration to balance accuracy and efficiency.


\begin{table}[t] \footnotesize
	\renewcommand\tabcolsep{1.5pt} 
	\centering
	\scalebox{0.8}{
	\begin{tabular}{@{}lcccccc@{}}
		\toprule
		 & PMBANet~\cite{ye2020pmbanet} & FDSR~\cite{he2021towards} & JIIF~\cite{tang2021joint} & DCTNet~\cite{zhao2022discrete} & LGR~\cite{de2022learning} & Ours \\ 
		 \midrule
		 Runtime (ms)
		 & 26.9 & 1.03 & 89.8 & 9.03 & 26.4 & 51.5\\
		 Memory Peak (GB)
		 & 3.07 & 2.05 & 2.36 & 0.26 & 0.19 & 18.6 \\ 
		\bottomrule
	\end{tabular}}
	\vspace{-0.3cm}
	\caption{\textbf{Computational requirements}. Experiments on Nvidia RTX 3090 GPU, with $256\times256$ input and $8\times$ factor.}
	\label{runtime_memory}
    
\end{table}

\subsection{Limitations}
We conclude by listing a few limitations of \netname. As previously pointed out, global context is crucial for it to achieve the best performance. When this is unavailable, some accuracy is lost when generalizing across datasets. Moreover, the significant improvements over existing methods are paid for in terms of time/memory requirements. Tab. \ref{runtime_memory} highlights the higher runtime and, more evidently, peak memory usage. Future work will aim at reducing the overhead, while minimizing the drop in accuracy.

