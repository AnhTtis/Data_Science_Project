%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t] \scriptsize
	\renewcommand\tabcolsep{8pt} 
	\centering
        \caption{\textbf{Results on Middlebury, NYUv2 and DIML datasets.} The lower the MSE and MAE, the better.}
	\begin{tabular}{@{}lccccccccc@{}}
		\toprule
		  \textbf{Dataset} & \multicolumn{3}{c}{\textbf{Middlebury}} & \multicolumn{3}{c}{\textbf{NYUv2}} & \multicolumn{3}{c}{\textbf{DIML}} \\
            \cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10}
            \textbf{Methods} & $4\times$ & $8\times$ & $16\times$ & $4\times$ & $8\times$ & $16\times$ &  $4\times$ & $8\times$ & $16\times$    \\
            \midrule
		GF~\citep{he2010guided}          & 33.3 / 1.27 & 40.5 / 1.49 & 67.4 / 2.21 & 114 / 3.91 & 142 / 4.47 & 249 / 6.34 & 25.6 / 1.45 & 34.1 / 1.77 & 66.3 / 2.74   \\
		SD~\citep{ham2017robust}         & 24.9 / 0.46 & 82.5 / 0.86 & 511 / 1.73 & 36.0 / 1.31 & 105 / 2.57 & 533 / 5.07 & 10.5 / 0.40 & 44.9 / 0.83 & 41.1 / 1.91    \\
		P2P~\citep{lutio2019guided}    & 39.8 / 0.79 & 32.7 / 0.82 & 41.5 / 1.24 & 112 / 3.61 & 122 / 3.86 & 219 / 5.40 & 20.7 / 1.15 & 23.0 / 1.26 & 39.3 / 1.78    \\
		MSG~\citep{hui2016depth}         & 4.13 / 0.22 & 10.5 / 0.43 & 34.2 / 1.06 & 6.85 / 0.81 & 24.1 / 1.66 & 84.5 / 3.35 & 1.73 / 0.22 & 4.13 / 0.40 & 13.0 / 0.93  \\
		DKN~\citep{kim2021deformable}    & 4.29 / 0.18 & 11.2 / 0.38 & 47.6 / 1.42 & 11.4 / 1.03 & 29.8 / 1.82 & 115 / 4.01 & 3.47 / 0.33 & 5.47 / 0.45 & 19.3 / 1.20  \\
		FDKN~\citep{kim2021deformable}   & 3.60 / 0.16 & 10.4 / 0.37 & 38.5 / 1.18 & 9.07 / 0.85 & 29.9 / 1.80 & 113 / 3.95 & 2.20 / 0.23 & 5.95 / 0.47 & 20.8 / 1.24  \\
		PMBANet~\citep{ye2020pmbanet}    & 4.72 / 0.25 & 9.48 / 0.38 & 30.6 / 0.89 & 10.8 / 0.93 & 17.2 / 1.38 & 84.9 / 3.26 & 3.05 / 0.31 & 5.87 / 0.47 & 13.8 / 0.87    \\
		  FDSR~\citep{he2021towards}       & 7.72 / 0.35 & 23.2 / 0.69 & 55.4 / 1.51 & 10.1 / 0.94 & 19.5 / 1.38 & 86.4 / 3.35 & 2.75 / 0.29 & 8.40 / 0.66 & 32.9 / 1.66    \\
            JIIF~\citep{tang2021joint}       & 2.70 / 0.11 & 8.01 / 0.27 & 37.5 / 0.98 & 3.28 / 0.52 & 15.2 / 1.29 & 59.9 / 2.81 & 1.19 / 0.16 & 3.65 / 0.32 & 11.7 / 0.81    \\
		DCTNet~\citep{zhao2022discrete}  & 5.00 / 0.24 & 15.1 / 0.57 & 52.3 / 1.50 & 3.63 / 0.68 & 20.9 / 1.79 & 77.0 / 3.61 & 2.09 / 0.31 & 7.08 / 0.65 & 23.4 / 1.75    \\
		LGR~\citep{de2022learning}         & 3.04 / 0.13 & 7.26 / 0.24 & 24.7 / 0.67 & 6.45 / 0.73 & 19.6 / 1.42 & 67.5 / 2.90 & 1.68 / 0.20 & 3.51 / 0.31 & 9.45 / 0.68    \\
		DADA~\citep{metzger2022guided}     & 2.58 / 0.11 & \underline{5.68} / 0.20 & \underline{16.3} / 0.48 & 4.83 / 0.64 & 16.6 / 1.30 & 59.0 / 2.64 & 1.33 / 0.17 & 2.93 / 0.28 & 7.61 / 0.59    \\
		DSR-EI                            & \textbf{2.46} / \underline{0.08} & 6.20 / \textbf{0.18} & \textbf{15.8} / \underline{0.47} & \underline{2.82} / \underline{0.49} & \textbf{11.8} / \underline{1.12} & \underline{47.8} / \underline{2.48} & \underline{0.70} / \underline{0.13} & \underline{2.12} / \textbf{0.22} & \textbf{6.29} / \underline{0.52}    \\
		DSR-EI$^+$                        & \underline{2.56} / \textbf{0.07} & \textbf{5.13} / \textbf{0.18} & 16.6 / \textbf{0.40} & \textbf{2.75} / \textbf{0.47} & \textbf{11.8} / \textbf{1.09} & \textbf{47.14} / \textbf{2.40} & \textbf{0.65} / \textbf{0.12} & \textbf{2.09} / \textbf{0.22} & \underline{6.31} / \textbf{0.50}    \\
            \bottomrule
	\end{tabular}
    \vspace{-0.3cm}
	\label{sota_comparison_mid_nyu_diml}
\end{table*}


\section{Experimental Results}
In this section, we validate the effectiveness of our proposal. We first introduce datasets, metrics and implementation details involved in our evaluation. Then, we compare \netname{} with state-of-the-art methods, conduct an ablation study on our model and, finally, discuss its limitations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t] \scriptsize
	\renewcommand\tabcolsep{14pt} 
	\centering
        \caption{\textbf{Results on RGBDD dataset.} We report RMSE, the lower the better.}
	\begin{tabular}{@{}lccc@{}}
	\toprule                
	\textbf{Methods} & $4\times$ & $8\times$ & $16\times$  \\ \midrule
        SDF~\citep{li2016deep}             & 2.00 & 3.23 & 5.16  \\
        SVLRM~\citep{pan2019spatially}     & 3.39 & 5.59 & 8.28  \\
        DJF~\citep{li2016deep}             & 3.41 & 5.57 & 8.15  \\
        DJFR~\citep{li2019joint}           & 3.35 & 5.57 & 7.99  \\
        PAC~\citep{su2019pixel}            & 1.25 & 1.98 & 3.49  \\
        CUNet~\citep{deng2020deep}         & 1.18 & 1.95 & 3.45  \\
        DKN~\citep{kim2021deformable}      & 1.30 & 1.96 & 3.42  \\
        FDKN~\citep{kim2021deformable}     & 1.18 & 1.91 & 3.41  \\
        FDSR~\citep{he2021towards}         & 1.16 & 1.82 & 3.06  \\
        DCTNet~\citep{zhao2022discrete}    & 1.07 & 1.78 & 3.18  \\
        RSAG~\citep{yuan2023recurrent}     & 1.14 & 1.75 & 2.96  \\
        DSR-EI                              & \textbf{0.91} & \textbf{1.37} & \textbf{2.10}  \\
        DSR-EI$^+$                          & \textbf{0.91} & \underline{1.38} & \textbf{2.10}  \\
    \bottomrule
	\end{tabular}
	\vspace{-0.3cm}
	\label{sota_comparison_rgbdd}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%
\begin{figure*}[t] 
	\centering
	\renewcommand\tabcolsep{1.5pt} 
	\begin{tabular}{cccccccccccc}
	\vspace{-0.1cm}
        \rotatebox[origin=l]{90}{\scriptsize \quad \textbf{4$\times$}} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/122/Middlebury_x4_122_img.pdf}
        \hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/122/Middlebury_x4_122_source.pdf}
	\hspace{-1.8mm} &  \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/122/Middlebury_x4_122_GT.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/122/Middlebury_x4_122_PMBA.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/122/Middlebury_x4_122_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/122/Middlebury_x4_122_JIIF.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/122/Middlebury_x4_122_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/122/Middlebury_x4_122_LGR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/122/Middlebury_x4_122_MSS.pdf}
        
        \hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/122/Middlebury_x4_122_ours.pdf}
    \\ \vspace{-0.cm}
    
        \rotatebox[origin=l]{90}{\scriptsize \quad \textbf{Middlebury}} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/138/Middlebury_x4_138_img.pdf}
        \hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/138/Middlebury_x4_138_source.pdf}
	\hspace{-1.8mm} &  \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/138/Middlebury_x4_138_GT.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/138/Middlebury_x4_138_PMBA.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/138/Middlebury_x4_138_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/138/Middlebury_x4_138_JIIF.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/138/Middlebury_x4_138_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/138/Middlebury_x4_138_LGR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/138/Middlebury_x4_138_MSS.pdf}
        
        \hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/Middlebury_4/138/Middlebury_x4_138_ours.pdf}
        \\ \vspace{-0.1cm}
    
        \rotatebox[origin=l]{90}{\scriptsize \quad \textbf{8$\times$}} & \includegraphics[height=0.65in]{./figs/sota_comp_nyu/357/NYU_357_img.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/sota_comp_nyu/357/NYU_357_source.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/sota_comp_nyu/357/NYU_357_GT.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/sota_comp_nyu/357/NYU_357_PMBA.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/sota_comp_nyu/357/NYU_357_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/sota_comp_nyu/357/NYU_357_JIIF.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/sota_comp_nyu/357/NYU_357_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/sota_comp_nyu/357/NYU_357_LGR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/sota_comp_nyu/357/NYU_357_MSS.pdf}
 
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/sota_comp_nyu/357/NYU_357_ours.pdf}
     \\ \vspace{-0.cm}
    
        \rotatebox[origin=l]{90}{\scriptsize \quad \textbf{NYUv2}} & \includegraphics[height=0.65in]{./figs/supplementary/NYU_8/400/NYU_400_img.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/NYU_8/400/NYU_400_source.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/NYU_8/400/NYU_400_GT.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/NYU_8/400/NYU_400_PMBA.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/NYU_8/400/NYU_400_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/NYU_8/400/NYU_400_JIIF.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/NYU_8/400/NYU_400_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/NYU_8/400/NYU_400_LGR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/NYU_8/400/NYU_400_MSS.pdf}
 
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/NYU_8/400/NYU_400_ours.pdf}
	\\ \vspace{-0.1cm}
	
        \rotatebox[origin=l]{90}{\scriptsize \quad \textbf{16$\times$}} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/15/DIML_x16_15_img.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/15/DIML_x16_15_source.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/15/DIML_x16_15_GT.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/15/DIML_x16_15_PMBA.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/15/DIML_x16_15_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/15/DIML_x16_15_JIIF.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/15/DIML_x16_15_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/15/DIML_x16_15_LGR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/15/DIML_x16_15_MSS.pdf}
 
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/15/DIML_x16_15_ours.pdf}
        \\
        
        \rotatebox[origin=l]{90}{\scriptsize \quad \textbf{DIML}} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/127/DIML_x16_127_img.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/127/DIML_x16_127_source.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/127/DIML_x16_127_GT.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/127/DIML_x16_127_PMBA.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/127/DIML_x16_127_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/127/DIML_x16_127_JIIF.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/127/DIML_x16_127_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/127/DIML_x16_127_LGR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/127/DIML_x16_127_MSS.pdf}
 
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/supplementary/DIML_16/127/DIML_x16_127_ours.pdf}
 \\
	& \scriptsize \textbf{(a)} RGB & \scriptsize \textbf{(b)} Bicubic & \scriptsize \textbf{(c)} GT & \scriptsize \textbf{(d)} PMBA & \scriptsize \textbf{(e)} FDSR & \scriptsize \textbf{(f)} JIIF & \scriptsize \textbf{(g)} DCTNet & \scriptsize \textbf{(h)} LGR & \scriptsize \textbf{(i)} \netname{} & \scriptsize \textbf{(j)} \netname{} (depth)
	\end{tabular}
    \vspace{-0.3cm}
	\caption{\textbf{Qualitative comparison on the Middlebury, NYUv2, and DIML.} From left to right: (a) RGB image, (b) Bicubic upsampled depth map, (c) GT; then, error maps achieved by selected methods: (d) PMBA~\cite{ye2020pmbanet}, (e) FDSR~\cite{he2021towards}, (f) JIIF~\cite{tang2021joint}, (g) DCTNet~\cite{zhao2022discrete}, (h) LGR~\cite{de2022learning}; finally, (i) error maps and (j) predictions by \netname.} 
	\label{fig:sota_comp1}
\end{figure*}
%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%% 
\begin{figure*}[t] 
	\centering
	\renewcommand\tabcolsep{1.5pt} 
	\begin{tabular}{ccccccccc}
	\vspace{-0.1cm}
    
    \rotatebox[origin=l]{90}{\scriptsize \quad \textbf{8$\times$}} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/20/img_20.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/20/bicubic_20.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/20/GT_20.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/20/RGBDD_20_FDKN.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/20/RGBDD_20_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/20/RGBDD_20_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/20/RGBDD_20_MSS.pdf}
 
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/20/20.pdf}
	\\ 
	
    \rotatebox[origin=l]{90}{\scriptsize \quad \textbf{RGBDD}} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/101/img_101.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/101/bicubic_101.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/101/GT_101.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/101/RGBDD_101_FDKN.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/101/RGBDD_101_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/101/RGBDD_101_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/101/RGBDD_101_MSS.pdf}
 
	\hspace{-1.8mm} & \includegraphics[height=0.62in]{./figs/supplementary/RGBDD/101/101.pdf}
 \\
	& \scriptsize \textbf{(a)} RGB & \scriptsize \textbf{(b)} Bicubic & \scriptsize \textbf{(c)} GT & \scriptsize \textbf{(d)} FDKN & \scriptsize \textbf{(e)} FDSR & \scriptsize \textbf{(f)} DCTnet & \scriptsize \textbf{(g)} \netname{} & \scriptsize \textbf{(h)} \netname{} (depth)
	\end{tabular}
    \vspace{-0.3cm}
	\caption{\textbf{Qualitative comparison on the RGBDD dataset.} From left to right: (a) RGB image, (b) Bicubic upsampled depth map, (c) GT; then, error maps achieved by selected methods: (d) FDKN~\citep{kim2021deformable}, (e) FDSR~\citep{he2021towards}, (f) DCTNet~\citep{zhao2022discrete}; finally, (g) error maps and (h) predictions by \netname.} 
	\label{fig:rgbdd_comp}
\end{figure*}
%%%%%%%%%%%%%%%%%%


\subsection{Datasets and Metrics}
We evaluate \netname{} on four datasets, compared with existing methods when super-solving depth maps by three different upsampling factors: $4\times,\ 8\times$, and $16\times$. 

\textbf{Middlebury}~\citep{scharstein2003high,scharstein2007learning,hirschmuller2007evaluation,scharstein2014high}. We train all learning-based methods using 50 RGB-D images with ground truth from Middlebury 2005, 2006 and 2014 datasets. As in~\cite{de2022learning}, we retain 5 for validation and 5 for testing. 

\textbf{NYUv2}~\citep{silberman2012indoor}. It contains 1449 RGB-D images in total. Following \cite{de2022learning}, we randomly split it into 849 RGB-D images for the training set, 300 for the validation set and 300 for the test set. Compared to \cite{ye2020pmbanet,liu2022pdr}, it comes with a validation set to make the comparison fairer.

\textbf{DIML}~\citep{kim2016structure,kim2017deep,kim2018deep,cho2021deep} consists of 2 million color images and corresponding depth maps from indoor and outdoor scenes. We adopt the same strategy outlined in \cite{de2022learning}, i.e., considering only the indoor data subset, and use 1440 for training, 169 for validation, and 503 for testing.

\textbf{RGBDD}~\citep{he2021towards} is a new real-world dataset for GDSR, which consists of 4811 image pairs. For evaluation, we follow the protocol described in \cite{he2021towards}, using 2215 images (1586 portraits, 380 plants, 249 models) as the training set and 405 images (297 portraits, 68 plants, 40 models) as the test set. 

\textbf{Metrics.} Following \cite{de2022learning}, we compute mean square error (MSE / $cm^2$) and mean absolute error (MAE / $cm$) as metrics on Middlebury, NYUv2 and DIML. For RGBDD, we use root mean square error (RMSE / $cm$) as in \cite{he2021towards}. 


\subsection{Implementation Details}
During training, the HR depth maps and the color images are randomly cropped into $256\times 256$ patches. LR depth patches are generated by bicubic interpolation at $64\times 64$, $32\times 32$, $16\times 16$ resolution for $4\times$, $8\times$ and $16\times$ factors, respectively. We randomly extract about 75K, 168K, 223K and 232K patches from Middlebury, NYUv2, DIML and RGBDD for training. Before being fed to the network, depth maps and images are normalized in the [0, 1] range.

We use Pytorch \citep{paszke2019pytorch} to implement and train \netname{}, on a single Nvidia RTX 3090 GPU. The batch size is set to 4, using Adam as the optimizer. The learning rate is initialized to $1\times 10^{-4}$, then performing a 5-epoch warm-up and cosine annealing. We use random rotation, horizontal/vertical flipping as data augmentation. According to the size of the four datasets, we train our network for 1505, 198, 155 and 109 epochs on Middlebury, NYUv2, DIML and RGBDD, respectively. When evaluating results on a specific dataset, we do not perform any pre-training on the others. Following \cite{de2022learning}, testing is performed by processing $256\times256$ patches at a time on Middlebury, NYUv2 and DIML for fairness, while full-resolution images are processed for RGBDD.


%%%%%%%%%%%%%%%%%%
\begin{table}[t] \scriptsize
	\renewcommand\tabcolsep{4.3pt} 
	\centering
        \caption{\textbf{Cross-dataset generalization.} All methods are trained on NYUv2 and tested on DIML/Middlebury with factor $8\times$. Middlebury\textit{-HR} is the test set defined in \cite{de2022learning}, Middlebury\textit{-LR} is the one from \cite{tang2021joint}. The lower MSE and MAE, the better. }
	\begin{tabular}{@{}lccc@{}}
		\toprule
		 \textbf{Methods} & DIML & Middlebury\textit{-HR} & Middlebury\textit{-LR}  \\ \midrule
		GF~\citep{he2010guided} & 34.1 \ 1.77 & 40.5 \ 1.49 & 25.6 \ 2.31  \\
		SD~\citep{ham2017robust} & 44.9 \ 0.83 & 82.5 \ 0.86 & 28.8 \ 2.07  \\
		P2P~\citep{lutio2019guided} & 23.0 \ 1.26 & 32.7 \ 0.82 & 15.8 \ 1.73  \\
		MSG~\citep{hui2016depth}  & 5.76 \ 0.51 & 11.0 \ 0.54 & 8.89 \ 1.62  \\
		FDKN~\citep{kim2021deformable} & 6.74 \ 0.53 & 10.0 \ \underline{0.43} & 5.54 \ 0.99  \\
		PMBANet~\citep{ye2020pmbanet} & 7.35 \ 0.59 & \underline{9.62} \ 0.46 & 4.16 \ \underline{0.91}  \\
            FDSR~\citep{he2021towards} & 7.73 \ 0.74 & 18.4 \ 0.73 & 6.92 \ 1.09  \\
            JIIF~\citep{tang2021joint} & \underline{4.10} \ \underline{0.38} & 19.3 \ 0.74 & 4.40 \ 0.92  \\
            DCTNet~\citep{zhao2022discrete} & 5.64 \ 0.77 & 17.5 \ 0.77 & 6.96 \ 1.15  \\ 
            LGR~\citep{de2022learning} & 4.95 \ 0.40 & \textbf{8.25} \ \textbf{0.35} & 5.94 \ 1.11  \\
            \netname$^+$  & \textbf{3.72} \ \textbf{0.36} & 14.6 \ 0.54 & \textbf{3.44} \ \textbf{0.87}  \\         
        \bottomrule
	\end{tabular}
	\vspace{-0.cm}
	\label{cross-data_comparison}
\end{table}
%%%%%%%%%%%%%%%%%%


\subsection{Comparison with State-of-the-Art}
We compare \netname{} to GF \citep{he2010guided}, SD \citep{ham2017robust}, P2P \citep{lutio2019guided}, MSG \citep{hui2016depth}, DKN and its fast implementation FDKN \citep{kim2021deformable}, PMBANet \citep{ye2020pmbanet}, FDSR \citep{he2021towards}, JIIF \citep{tang2021joint}, DCTNet \citep{zhao2022discrete}, LGR \citep{de2022learning}, and finally to DADA~\citep{metzger2022guided} on Middlebury, NYUv2 and DIML datasets. We could not compare with PDRNet \citep{liu2022pdr} under the same setting because the source code is unavailable at the time of writing. For the other methods, we use the results from \citep{de2022learning} or the officially published codes, and results from \citep{yuan2023recurrent,metzger2022guided} for concurrent works. On the RGBDD dataset, the proposed network is compared to SDF~\citep{li2016deep}, SVLRM \citep{pan2019spatially}, DJF~\citep{li2016deep}, DJFR~\citep{li2019joint}, PAC~\citep{su2019pixel}, CUNet~\citep{deng2020deep}, FDKN~\citep{kim2021deformable}, DKN~\citep{kim2021deformable}, FDSR~\citep{he2021towards}, DCTNet~\citep{zhao2022discrete} and RASG~\citep{yuan2023recurrent}. To be fair with DCTNet~\citep{zhao2022discrete}, we downsample depth maps as the LR input. 
{When reporting results, we highlight \textbf{absolute} and \underline{second} best methods for each metric on each dataset.}

%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht] 
	\centering
	\renewcommand\tabcolsep{1.5pt} 
	\begin{tabular}{cccccccccccc}
	\vspace{-0.cm}
        \rotatebox[origin=l]{90}{\scriptsize \quad \textbf{DIML}} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/DIML_1511/DIML_1511_img.pdf}
        \hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/DIML_1511/DIML_1511_source.pdf}
	\hspace{-1.8mm} &  \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/DIML_1511/DIML_1511_GT.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/DIML_1511/DIML_1511_PMBA.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/DIML_1511/DIML_1511_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/DIML_1511/DIML_1511_JIIF.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/DIML_1511/DIML_1511_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/DIML_1511/DIML_1511_LGR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/DIML_1511/DIML_1511_MSS.pdf}
        
        \hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/DIML_1511/DIML_1511_ours.pdf}
    \\ \vspace{-0.cm}
    
        \rotatebox[origin=l]{90}{\scriptsize  \textbf{\textit{Middlebury-HR}}} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_6/Middlebury_6_img.pdf}
        \hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_6/Middlebury_6_source.pdf}
	\hspace{-1.8mm} &  \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_6/Middlebury_6_GT.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_6/Middlebury_6_PMBA.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_6/Middlebury_6_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_6/Middlebury_6_JIIF.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_6/Middlebury_6_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_6/Middlebury_6_LGR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_6/Middlebury_6_MSS.pdf}
        
        \hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_6/Middlebury_6_ours.pdf}
        \\ \vspace{-0.cm}
    
        \rotatebox[origin=l]{90}{\scriptsize \textbf{\textit{Middlebury-LR}}} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_5_lr/Middlebury_5_img.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_5_lr/Middlebury_5_source.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_5_lr/Middlebury_5_GT.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_5_lr/Middlebury_5_PMBA.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_5_lr/Middlebury_5_FDSR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_5_lr/Middlebury_5_JIIF.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_5_lr/Middlebury_5_DCTnet.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_5_lr/Middlebury_5_LGR.pdf}
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_5_lr/Middlebury_5_MSS.pdf}
 
	\hspace{-1.8mm} & \includegraphics[height=0.65in]{./figs/ablation_figure/cross_dataset/middlebury_5_lr/Middlebury_5_ours.pdf}
 \\
	& \scriptsize \textbf{(a)} RGB & \scriptsize \textbf{(b)} Bicubic & \scriptsize \textbf{(c)} GT & \scriptsize \textbf{(d)} PMBA & \scriptsize \textbf{(e)} FDSR & \scriptsize \textbf{(f)} JIIF & \scriptsize \textbf{(g)} DCTNet & \scriptsize \textbf{(h)} LGR & \scriptsize \textbf{(i)} \netname{} & \scriptsize \textbf{(j)} \netname{} (depth)
	\end{tabular}
    \vspace{-0.3cm}
	\caption{\textbf{Visual comparison on cross-dataset generalization (scaling factor $8\times$).} The top, middle and last row show the error maps on the DIML dataset, the \textit{Middlebury-HR} dataset and the \textit{Middlebury-LR} dataset, respectively. From left to right: (a) RGB image, (b) Bicubic upsampled depth map, (c) GT; then, error maps achieved by selected methods: (d) PMBA~\cite{ye2020pmbanet}, (e) FDSR~\cite{he2021towards}, (f) JIIF~\cite{tang2021joint}, (g) DCTNet~\cite{zhao2022discrete}, (h) LGR~\cite{de2022learning}; finally, (i) error maps and (j) predictions by \netname.} 
	\label{fig:cross_dataset}
\end{figure*}
%%%%%%%%%%%%%%%%%%




\textbf{Quantitative Comparison.} Tabs. \ref{sota_comparison_mid_nyu_diml} and \ref{sota_comparison_rgbdd} report the accuracy of super-solved depth maps at factors $4\times$, $8\times$ and $16\times$ on the four datasets. As expected, learning-based methods show a significant improvement over traditional methods \citep{he2010guided,ham2017robust,lutio2019guided}. \netname{} vastly outperforms any existing network, with larger gaps in accuracy with the increasing of the upsampling factor. This can be attributed to the limitations affecting existing methods, i.e., 1) the guidance of either explicit or implicit RGB features alone being insufficient; 2) multi-modal information fusion on a single scale being not flexible enough to deal with complex scenes. Both limitations are fully addressed by \netname, which consistently outperforms concurrent works \citep{metzger2022guided,yuan2023recurrent}. 

The margin is consistent both on perfect (Middlebury) and noisy datasets (NYUv2, DIML, RGBDD), with the latter being a more challenging, realistic benchmark. Although \netname$^+$ is definitely the absolute best, its margin over \netname{} is negligible, with tiny gains yielded by NLSPN with respect to our main modules. Indeed, \netname{} alone consistently outperforms any other approach already.

       
\textbf{Qualitative Comparison.}
Fig.~\ref{fig:sota_comp1} provides qualitative comparisons of the GDSR results across multiple datasets, i.e., NYUv2, Middlebury, and DIML, which cover various types of scenarios and noise levels. We can notice that our model can extract boundaries and details from the RGB image more accurately. Specifically, on the depth discontinuities in the two topmost rows, \netname{}$^+$ introduces fewer artifacts around the edges of objects where specular reflections occur, which means that our network is more robust in removing texture-copy effects from RGB images compared with other methods. On the two samples selected from NYUv2, our network produces fewer errors in recovering fine structures and details. For example, in the fourth row of this figure, there are many tiny objects whose shape and structure are degraded due to downsampling. Other methods may produce artifacts and inaccurate depth boundaries, while our method has a clear advantage in recovering fine-grained depth details. Fig.~\ref{fig:rgbdd_comp} also reports two examples on the RGBDD dataset. In this case, we notice fewer errors in the background, e.g., on the curtain. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]	
	\centering	
	\subfloat[RGB]{	
		\centering	
		\label{cross_dataset} 
		\includegraphics[height=0.8in]{./figs/ablation_figure/cross_dataset/receptive_field/cross_dataset.pdf}}	
	\hspace{-2mm}
	\subfloat[$D_{hr}$]{	
		\centering	
		\label{HR}
		\includegraphics[width=0.8in]{./figs/ablation_figure/cross_dataset/receptive_field/HR.pdf}}
	\hspace{-2mm}
	\subfloat[$D_{lr}$]{	
		\centering	
		\label{LR}
		\includegraphics[width=0.8in]{./figs/ablation_figure/cross_dataset/receptive_field/LR.pdf}}
		\vspace{-0.3cm}
	\caption{\textbf{Image context processed on Middlebury -- HR vs LR.} (a) RGB image and depth patches $D$ processed when testing on (b) Middlebury\textit{-HR} and (c) Middlebury\textit{-LR}. }	
	\label{hr-lr} 
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%% 


\textbf{Cross-dataset Generalization.}
We conclude the comparison with existing methods by conducting cross-dataset experiments with $8\times$ factor. All methods are trained on the NYUv2 dataset and directly evaluated on DIML and Middlebury. Table \ref{cross-data_comparison} collects quantitative results for the 11 selected methods. Again, CNN-based methods attain better performance than traditional approaches, despite the domain gap playing a significant role in performance -- as evident by comparing results with Table \ref{cross-data_comparison}. Nonetheless, \netname{} outperforms any other framework on DIML. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]	    \scriptsize
	\centering
	\renewcommand\tabcolsep{6pt} 
	\caption{\textbf{Abaltion study -- high-frequency information.} Scale $8\times$.}
	\begin{tabular}{@{}ccccccc@{}} 
		\toprule
		\textbf{No.} & \textbf{Gradient} & \tabincell{c}{\textbf{Shallow} \\ \textbf{Feature}} & \textbf{LCF} & \textbf{ResBlock} & \textbf{MSE} & \textbf{MAE}\\
		\midrule
		(\uppercase\expandafter{\romannumeral1}) & \XSolidBrush &  \Checkmark     &  \Checkmark &  & 13.1 & 1.19 \\
		(\uppercase\expandafter{\romannumeral2}) & \Checkmark &    \XSolidBrush   &   &  & 12.4 & 1.14 \\
		(\uppercase\expandafter{\romannumeral3}) & \Checkmark &    \Checkmark     &   & \Checkmark & 12.3 & 1.15 \\
		\gray{(\uppercase\expandafter{\romannumeral4})} & \Checkmark &    \Checkmark     & \Checkmark  &  & \textbf{11.8} & \textbf{1.12} \\
		\bottomrule
	\end{tabular}
	\label{hf_infomation}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

When considering the Middlebury dataset, we evaluate using the setting proposed in \cite{de2022learning} -- Middlebury\textit{-HR} in the table. In this case, our results are slightly less accurate compared to a few existing methods. However, given the very high resolution of Middlebury images, we argue that this testing protocol -- i.e., consisting of processing $256\times 256$ crops at a time -- penalizes our network's ability to leverage the global context in the input that results irremediably reduced to a very local area in these images. Therefore, we also evaluate on Middlebury test set defined by~\cite{tang2021joint} -- Middlebury-\textit{LR} in the table. Note that different subsets of images are used in Middlebury\textit{-HR} and Middlebury-\textit{LR} splits. Besides, Middlebury-\textit{LR} images are resized and processed without cropping, i.e., used at full-size after resizing, allowing to fully exploit global context, while this is not feasible with Middlebury-\textit{HR} due to memory constraints. In this case, \netname{} attains the best performance again, confirming our previous analysis, as shown in Tab. \ref{cross-data_comparison}. Such a difference in terms of context is highlighted in Fig. \ref{hr-lr}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]	
	\centering	
	\captionsetup[subfigure]{font=footnotesize,textfont=footnotesize}
	\subfloat[]{	
		\centering	
		\label{img_diml_7} 
		\includegraphics[width=1.03in]{./figs/ablation_figure/hfeb_feat/DIML_0_img.pdf}}	
	\hspace{-2.mm}
	\subfloat[]{	
		\centering	
		\label{gt_diml_7}
		\includegraphics[width=1.03in]{./figs/ablation_figure/hfeb_feat/DIML_0_gt.pdf}}
	\hspace{-2.mm}
	\subfloat[]{	
		\centering	
		\label{bicubic_diml_7}
		\includegraphics[width=1.03in]{./figs/ablation_figure/hfeb_feat/DIML_0_dep_lr.pdf}}
	
	\vspace{-0.0cm}
	\subfloat[]{	
		\centering	
		\label{feat0_diml_7}
		\includegraphics[width=1.03in]{./figs/ablation_figure/hfeb_feat/DIML_0_feat.pdf}}	
	\hspace{-2.mm}
	\subfloat[]{	
		\centering	
		\label{feat1_diml_7}
		\includegraphics[width=1.03in]{./figs/ablation_figure/hfeb_feat/DIML_1_feat.pdf}}
	\hspace{-2.mm}
	\subfloat[]{	
		\centering	
		\label{feat2_diml_7}
		\includegraphics[width=1.03in]{./figs/ablation_figure/hfeb_feat/DIML_2_feat.pdf}}
	\caption{\textbf{Visual exhibition of high-frequency features generated from HFEB.} (a) RGB image, (b) GT, (c) Bicubic, (d)-(f) high-frequency features.}	
	\label{hfeb_feat} 
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]	
	\centering	
	\captionsetup[subfigure]{font=footnotesize,textfont=footnotesize}
	\subfloat[]{	
		\centering	
		\label{img_diml_1} 
		\includegraphics[width=1.03in]{./figs/ablation_figure/lcf_feat/DIML_0_img.pdf}}	
	\hspace{-2mm}
	\subfloat[]{	
		\centering	
		\label{gt_diml_1}
		\includegraphics[width=1.03in]{./figs/ablation_figure/lcf_feat/DIML_0_gt.pdf}}
	\hspace{-2mm}
	\subfloat[]{	
		\centering	
		\label{bicubic_diml_1}
		\includegraphics[width=1.03in]{./figs/ablation_figure/lcf_feat/DIML_0_dep_lr.pdf}}
	
	\vspace{-0.cm}
	\subfloat[]{	
		\centering	
		\label{feat0_diml_1}
		\includegraphics[width=1.03in]{./figs/ablation_figure/lcf_feat/DIML_0_feat.pdf}}	
	\hspace{-2mm}
	\subfloat[]{	
		\centering	
		\label{feat1_diml_1}
		\includegraphics[width=1.03in]{./figs/ablation_figure/lcf_feat/DIML_1_feat.pdf}}
	\hspace{-2mm}
	\subfloat[]{	
		\centering	
		\label{feat2_diml_7}
		\includegraphics[width=1.03in]{./figs/ablation_figure/lcf_feat/DIML_2_feat.pdf}}
	\caption{\textbf{Visual exhibition of shallow high-frequency features generated from LCF.} (a) RGB image, (b) GT, (c) Bicubic, (d)-(f) high-frequency features.}	
	\label{lcf_feat} 
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%% 



%%%%%%%%%%%%%%%
\begin{table}[t]	\scriptsize
	\centering
	\renewcommand\tabcolsep{15pt} 
        \caption{\textbf{Different configurations for HR information.} Scale $8\times$.}
        \begin{tabular}{@{}clcc@{}} 
		\toprule
		\textbf{No.} & \textbf{HF Information} & \textbf{MSE} & \textbf{MAE}\\
		\midrule
		(\uppercase\expandafter{\romannumeral1}) & {Canny Edge} & 12.0 & 1.13 \\
		(\uppercase\expandafter{\romannumeral2}) & {Gaussian Edge} & 12.1 & 1.16 \\
		(\uppercase\expandafter{\romannumeral3}) & {DCT} & 12.1 & 1.15 \\
		(\uppercase\expandafter{\romannumeral4}) & {Wavelet Transform} & 12.1 & 1.15  \\
		\gray{(\uppercase\expandafter{\romannumeral5})} & {Gradient Map} & \textbf{11.8} & \textbf{1.12} \\
		\bottomrule
	\end{tabular}
	\label{edge_types}
\end{table}
%%%%%%%%%%%%%% 


%%%%%%%%%%%%%%%%%%%%%%%%    %%%%%%%%%%%%%%%%%% 
\begin{table}[t]	\scriptsize
	\centering
	\renewcommand\tabcolsep{8pt} 
	\caption{\textbf{Effectiveness of HFEB.} Scale $8\times$.}
	\begin{tabular}{@{}clcccc@{}}
		\toprule
		\textbf{No.} & \textbf{Config.} & \textbf{Params (M)} & \textbf{Flops (G)} & \textbf{MSE} & \textbf{MAE}\\
		\midrule
		(\uppercase\expandafter{\romannumeral1}) & EdgeNet    & 5.78 &  95.6  & 12.0 & \textbf{1.12} \\
		(\uppercase\expandafter{\romannumeral2}) & SCPA  & 0.29 &  13.1  & 12.5 & 1.16 \\
		\gray{(\uppercase\expandafter{\romannumeral3})} & HFEB       & \textbf{0.27} & \textbf{11.6}  & \textbf{11.8} & \textbf{1.12} \\
		\bottomrule
	\end{tabular}
	\label{dsp_ablation}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%   %%%%%%%%%%%%%%%%%%


\subsection{Ablation Study}
We now perform a series of ablation experiments to measure the impact of key components and parameters in \netname. We collects the outcome of these studies, conducted on NYUv2 test set with $8\times$ factor. Without loss of fairness, NLSPN is never used here -- to fully focus on the impact of single components. The configurations marked in gray in Tab.~\ref{hf_infomation}-Tab.~\ref{stage_num} correspond to our final model without NLSPN. 

\textbf{(a) Implicit vs Explicit High-Frequency Features.}
To measure the impact of both implicit and explicit HR features, we compare the performance of the proposed network and its variants when extracting either only one of the two. The quantitative results are collected in Tab.~\ref{hf_infomation}. Without the help of gradient maps (I), the performance of the network significantly degrades. We believe this is caused by the difficulty in effectively extracting fine structures or salient edges required for LR depth maps from implicit HF features alone. Moreover, explicit features highlight regions in the image that need to be focused on, avoiding \netname{} to learn to localize them and easing its task. 
{Fig.~\ref{hfeb_feat} shows three among the high-frequency features $F_{edge}$ from a representative sample. We can notice how each of the three mainly emphasizes object boundaries, confirming the effectiveness of HFEB at extracting gradient information. At the same time, we can notice how the input RGB images expose very low texture, further confirming the effectiveness of HFEB at localizing high-frequency information.}

Nonetheless, explicit HF features alone as guidance (II) are insufficient as well. We argue that the explicit information might neglect some RGB features, whereas implicit HF feature extraction can recover them. Furthermore, to verify the effectiveness of LCF, we replace it with ResBlock~\citep{he2016deep} (III) to extract shallow features from RGB images, highlighting a negative impact on implicit features extraction -- i.e., it results less accurate than (II). Fig.~\ref{lcf_feat} shows some of the features extracted by LCF. We can notice how, in addition to the primary high-frequency information, other information is encoded, such as semantics, which can further provide support for the explicit high-frequency information extracted in parallel by HFEB and improve the guidance for the final, depth super-resolution task.


\textbf{(b) Ablation on Explicit High-Frequency Features.}
Based on the previous analysis, HFEB can significantly improve the network. To determine which high-frequency information is more suitable as guidance for GDSR, we experiment with five kinds of edge maps used as ground truth $E_{gt}$ to train HFEB: (1) the Canny edge map, (2) the Gaussian high-frequency map, (3) the high-frequency map generated by discrete cosine transform, (4) the high-frequency wavelet map and (5) the gradient map, as shown in Tab.~\ref{edge_types}. The Gaussian high-frequency map is obtained using a Gaussian filter, as detailed in~\cite{wang2020depth}. Table~\ref{edge_types} reports the outcome of the evaluation. From it, we can see that the Canny edge and the gradient map allow for better performance. Although DSR-EI with the gradient map attains the best results in terms of MSE and MAE, the different types of high-frequency maps do not significantly affect the final upsampling result.


\textbf{(c) Impact of HFEB.}
To verify the effectiveness of HFEB, we replace it with EdgeNet~\citep{liu2021multi} -- based on the widely-used U-net structure -- and SCPA~\citep{zhao2020efficient}, which inspires our scaling strategy. As shown in Table~\ref{dsp_ablation}, although the parameter size of EdgeNet is 5.6M, its performance is almost the same as our HFEB, while the parameter size of our network is only 0.7M, i.e. only $\frac{1}{8}$ of it. This fact highlights that our network based on a transformer is more efficient at feature extraction. 

Besides, unlike previous works that employ fixed feature scaling rules, we adopt a dynamic scaling strategy to extract high-frequency features from depth maps. Table~\ref{dsp_ablation} also shows that our DSP with the dynamic scale strategy decreases the number of parameters while simultaneously enhancing the performance of GDSR. Compared to the original SCPA~\citep{zhao2020efficient}, DSP can perform dynamic scaling according to the characteristics of the feature map to get a more effective receptive field.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{table}[t]	\scriptsize
	\centering
	\renewcommand\tabcolsep{10pt}
        \caption{\textbf{The impact of scales at which AFFM is applied.}}
	\begin{tabular}{@{}clccc@{}} %\label{affm_setting}
		\toprule
		\textbf{No.} & \textbf{Scales} & \textbf{Params (M)} & \textbf{MSE} & \textbf{MAE}\\
		\midrule
		(\uppercase\expandafter{\romannumeral1}) & H1              & 1.5 & 12.3 & 1.14 \\
		\gray{(\uppercase\expandafter{\romannumeral2})} & H1, H2       & 3.0 & \textbf{11.8} & \textbf{1.12} \\
		(\uppercase\expandafter{\romannumeral3}) & H1, H2, H3              & 4.5 & \textbf{11.8} & \textbf{1.12} \\
		\bottomrule
	\end{tabular}
	\label{affm_scale}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


\textbf{(d) Impact of AFFM.}
We now measure the effectiveness of AFFM. Tab.~\ref{affm_setting} shows results obtained by deploying AFFM at different scales, respectively the highest (I), the first two (II) and all of the three scales. We can notice how performing fusion at the highest scale alone results insufficient, whereas using multi-scale features for fusion yields improvements, despite saturating already when using two scales, with the lowest one not providing additional, meaningful details to be taken into account.

Furthermore, we ablate AFFM in its single components. Tab.~\ref{affm_setting} resumes the outcome of this evaluation. 
We first test the performance of \netname{} without AFFM (I), highlighting a large drop in accuracy. By adding dynamic fusion, yet without using attention (II) vastly improves the results already, while replacing the weighted sum in the upper of Fig.~\ref{affm} with concatenation and a ResBlock~\citep{he2016deep} (III) yields worse results compared to our full AFFM (IV). 

Fig.~\ref{affm_feat} visualizes the attention maps produced by AFFM, highlighting how sharp and accurate they are in correspondence with depth discontinuities, tiny objects, and fine details. Thus, thanks to them AFFM can better focus on reconstructing depth boundaries and details more accurately.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{table}[t]	\scriptsize
	\centering
	\renewcommand\tabcolsep{10pt}
	\caption{\textbf{Ablation study of AFFM.} Scale $8\times$.}
	\begin{tabular}{@{}clccc@{}} 
		\toprule
		\textbf{No.} & \textbf{Config.} & \textbf{Params (M)} & \textbf{MSE} & \textbf{MAE}\\
		\midrule
		(\uppercase\expandafter{\romannumeral1}) & w/o AFFM        & -   & 12.7 & 1.16 \\
		(\uppercase\expandafter{\romannumeral2}) & w/o att         & 1.3 & 12.2 & 1.13 \\
		(\uppercase\expandafter{\romannumeral3}) & Concat.  & 4.5 & 12.2 & 1.13 \\
		\gray{(\uppercase\expandafter{\romannumeral4})} & AFFM & 3.0 & \textbf{11.8} & \textbf{1.12} \\
		\bottomrule
	\end{tabular}
	\label{affm_setting}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]	
	\centering	
	\captionsetup[subfigure]{font=footnotesize,textfont=footnotesize}
	\subfloat[]{	
		\centering	
		\label{img_diml_9} 
		\includegraphics[width=0.8in]{./figs/ablation_figure/affm_feat/DIML_0_img.pdf}}	
	\hspace{-2mm}
	\subfloat[]{	
		\centering	
		\label{gt_diml_9}
		\includegraphics[width=0.8in]{./figs/ablation_figure/affm_feat/DIML_0_gt.pdf}}
	\hspace{-2mm}
	\subfloat[]{	
		\centering	
		\label{bicubic_diml_9}
		\includegraphics[width=0.8in]{./figs/ablation_figure/affm_feat/DIML_0_dep_lr.pdf}}
		\hspace{-2mm}
	\subfloat[]{	
		\centering	
		\label{feat1_diml_9}
		\includegraphics[width=0.8in]{./figs/ablation_figure/affm_feat/DIML_0_feat.pdf}}
		
	\vspace{-0.cm}
	\subfloat[]{	
		\centering	
		\label{feat0_diml_9}
		\includegraphics[width=0.8in]{./figs/ablation_figure/affm_feat/DIML_1_feat.pdf}}	
	\hspace{-2mm}
	\subfloat[]{	
		\centering	
		\label{feat1_diml_9}
		\includegraphics[width=0.8in]{./figs/ablation_figure/affm_feat/DIML_5_feat.pdf}}
	\hspace{-2mm}
	\subfloat[]{	
		\centering	
		\label{feat2_diml_7}
		\includegraphics[width=0.8in]{./figs/ablation_figure/affm_feat/DIML_6_feat.pdf}}
	\hspace{-2mm}
	\subfloat[]{	
		\centering	
		\label{feat1_diml_9}
		\includegraphics[width=0.8in]{./figs/ablation_figure/affm_feat/DIML_7_feat.pdf}}
	\caption{\textbf{Visual exhibition of attention maps generated from AFFM.} (a) RGB image, (b) GT, (c) Bicubic, (d)-(h) attention maps.}	
	\label{affm_feat} 
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]	\scriptsize
	\centering
	\renewcommand\tabcolsep{10pt} 
        \caption{\textbf{Comparisons with different stage numbers.} Scale $8\times$.}
	\begin{tabular}{@{}clccc@{}}
            \toprule
		\textbf{No.} & \textbf{Stages} & \textbf{Params (M)} & \textbf{MSE} & \textbf{MAE} \\
		\midrule
		(\uppercase\expandafter{\romannumeral1}) &  $1$   & 14.2 & 13.3 & 1.19 \\
		\gray{(\uppercase\expandafter{\romannumeral2})} & 	$2$   & 25.0 & 11.8 & 1.12 \\
		(\uppercase\expandafter{\romannumeral3}) &  $3$   & 37.5 & \textbf{11.6} & \textbf{1.10} \\
		\bottomrule
	\end{tabular}
	\label{stage_num}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{(e) Impact of Stages Number.}
To conclude, we evaluate the impact of the multi-stage design.
As shown in Tab.~\ref{stage_num}, a single-stage architecture (I) is vastly outperformed by deploying two stages (II), yet at the expense of doubling the number of parameters. Furthermore, while the three-stage architecture (III) still yields some improvement, the benefit is minor in comparison to the significant increase in parameters. Hence, we choose two stages as the default configuration to balance accuracy and efficiency.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t] \scriptsize
	\setlength{\tabcolsep}{5pt}
	\centering
	\label{input_size}
        \caption{\textbf{Results on NYUv2 and DIML dataset -- different input sizes.} We report MSE (cm$^2$) / MAE (cm), the lower the better.}
	\begin{tabular}{@{}ccccccccc@{}}
		\toprule
		  \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Size}} & \multicolumn{3}{c}{\textbf{DIML}} & \multirow{2}{*}{\textbf{Size}} & \multicolumn{3}{c}{\textbf{NYUv2}}  \\ 
            \cmidrule(r){3-5} \cmidrule(r){7-9}
                &  & \textbf{4$\times$} & \textbf{8$\times$} & \textbf{16$\times$} &  & \textbf{4$\times$} & \textbf{8$\times$} & \textbf{16$\times$}     \\
            \midrule
		\multirow{2}{*}{\textbf{DSR-EI$^+$}} & 256$\times$256 & 0.65 / \textbf{0.12} & 2.09 / 0.22 & 6.31 / 0.50 & 256$\times$256 & 2.75 / 0.47 & 11.8 / 1.09 & 47.1 / 2.40       \\ 
                                                 & 1344$\times$756 & \textbf{0.58} / \textbf{0.12} & \textbf{1.91} / \textbf{0.20} & \textbf{5.15} / \textbf{0.45} & 640$\times$480 & \textbf{1.93} / \textbf{0.39} & \textbf{8.14} / \textbf{0.89} & \textbf{33.0} / \textbf{2.02}                      \\
 \bottomrule
	\end{tabular}
	\label{tab:fullsize}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%
\begin{table*}[t]    \scriptsize
	\renewcommand\tabcolsep{5mm} 
	\centering
         \caption{\textbf{\textbf{Computational requirements at inference}}. Experiments on Nvidia RTX 3090 GPU, with $256\times256$ input and $8\times$ factor.}
	\begin{tabular}{@{}lcccccc@{}}
		\toprule
		 & \textbf{PMBANet} & \textbf{FDSR} & \textbf{JIIF} & \textbf{DCTNet} & \textbf{LGR} & \textbf{Ours} \\ 
		 \midrule
		 \textbf{Runtime (ms)}
		 & 26.9 & 1.03 & 89.8 & 9.03 & 26.4 & 51.5\\
		 \textbf{Memory Peak (GB)}
		 & 3.07 & 2.05 & 2.36 & 0.26 & 0.19 & 18.6 \\ 
		\bottomrule
	\end{tabular}
	\vspace{-0.cm}
	\label{runtime_memory}    
\end{table*}
%%%%%%%%%%%%%%%%%%



{\textbf{(f) Results on full-size images.}
In Tab.~\ref{sota_comparison_mid_nyu_diml} and \ref{sota_comparison_rgbdd}, we reported the results achieved by our model when processing $256\times256$ patches, to allow for a fair comparison with LGR~\citep{de2022learning} and DADA~\citep{metzger2022guided}. However, this irremediably reduces the global context processed by \netname{}, hindering its capacity to exploit it enabled by the transformer blocks similar to what was observed in the generalization experiment on Middlebury (Tab. 4). 
In this section, we demonstrate how processing larger images allows \netname{} to further improve its performance. 
Tab. \ref{tab:fullsize} compares the results achieved when switching from $256\times256$ patches to the full resolution images of DIML and NYUv2 -- i.e., $1344\times756$ and $640\times480$, respectively. We can notice consistent improvements, particularly when dealing with larger upsampling factors.}


\subsection{Limitations}
We conclude by listing a few limitations of \netname. As previously pointed out, global context is crucial for it to achieve the best performance. When this is unavailable, some accuracy is lost when generalizing across datasets. Moreover, the significant improvements over existing methods are paid for in terms of time/memory requirements. Tab. \ref{runtime_memory} highlights the higher runtime and, more evidently, peak memory usage. Future work will aim at reducing the overhead, while minimizing the drop in accuracy.
