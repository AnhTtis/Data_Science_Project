\section{\netname{} Framework}

In GDSR, HF information in color images -- complementary to depth maps -- is essential for achieving high performance, which motivates us to seek an efficient method to extract it. In this section, we present our framework that exploits explicit and implicit HF information for depth super-resolution. Then, we introduce the two branches in our network: the High-Frequency Extraction Branch (HFEB) and the Guided Depth Restoration Branch (GDRB). 

\begin{figure}	
	\centering	
	\captionsetup[subfigure]{font=footnotesize,textfont=footnotesize}
	\subfloat{	
		\centering	
		\includegraphics[width=3.2in]{./figs/hf_compare/hf_compare.pdf}}
	\vspace{-0.cm}
	\caption{\textbf{High-frequency information loss (factor $4\times$).} From left to right, HR depth map and its corresponding gradient map, followed by the gradient map from bicubic upsampled LR depth map and LR depth map itself. HF information is mostly lost in the second gradient map.}
	\label{hf_compare}
\end{figure}


Fig.~\ref{framework} shows an overview of our architecture. Given the LR depth map $D_{lr} \in \mathbb{R}^{h\times w\times 1}$ and the corresponding HR color image $I_{hr} \in \mathbb{R}^{H\times W\times 3}$, we aim at restoring HR depth map $\tilde{D}_{sr}$. Note that $H=s\times h$ and $W=s\times w$, where $s$ denotes the upsampling factor -- e.g., $4\times, 8\times$ or even $16\times$. In our proposed network, the input depth map is firstly upsampled with bicubic interpolation to the same size as $I_{hr}$. At different scales, we denote the corresponding depth maps and color images as $D_{lr}^{i}$ and $I_{hr}^{i}$, respectively, with $s=2^i$. Then, according to the above notation, the input images $D_{lr}^{0}$ and $I_{hr}^{0}$ are fed into the two branches, respectively.  Before being sent to GDRB, both the RGB and depth images are processed by a channel-attention block (CAB) \citep{zhang2018image} and a low-cut filtering (LCF) module, which will be explained in detail in Sec. \ref{GDRB}.


\subsection{High-Frequency Extraction Branch (HFEB)}
We argue HF information is crucial for effective super-solving depth and is often lost by upsampling. The primary goal of HFEB is to produce an accurate gradient map from an LR depth map, with the support of a color image jointly processed with it. 

Indeed, as pointed out in \cite{wang2020depth}, networks for GDSR tend to focus more on depth discontinuities or object boundaries. However, from Fig.~\ref{hf_compare}, we can notice that even with a $4\times$ factor, most high-frequency information vanishes, as shown by the gradient maps extracted from HR and upsampled LR depth maps, leading to severe degradation of the super-solved depth map. Traditional methods tend to transfer texture to depth maps rather than structural details, failing to extract accurate edges. Moreover, methods extracting binary edges \citep{wang2020depth} gather insufficient high-frequency information, yielding sub-optimal results. 

\begin{figure}	
	\centering	
	\captionsetup[subfigure]{font=footnotesize,textfont=footnotesize}
	\subfloat{	
		\centering	
		\includegraphics[width=3.2in]{./figs/framework/DSP.pdf}}	
    \vspace{-0.cm}		
	\caption{\textbf{DSP architecture.} Differently from SCPA \citep{zhao2020efficient}, our module processes features at different scales, allowing to extract explicit HF information more effectively.}	
	\label{dsp} 
\end{figure}

The work~\citep{pu2022edter} has shown that transformer-based networks can extract clear and meaningful edges by leveraging both global and local features simultaneously. Considering the sparsity of edge maps, we design an efficient transformer, inspired by dynamic scale policy \citep{wang2019elastic} and self-attention \citep{vaswani2017attention}, to obtain strong HF priors for guiding depth super-resolution. Specifically, our transformer consists of a stack of blocks called dynamic self-calibrated convolution with pixel attention (DSP) and one LightViT block~\citep{huang2022lightvit}. 
To better extract HF features, we design the DSP block, which is inspired by SCPA \citep{zhao2020efficient} and performs self-calibrated convolution with two branches at a single scale. However, unlike SCPA, our DSP block includes an additional branch that enables the processing of features at different scales without incurring extra computational burden, as we will demonstrate empirically in our experiments. Specifically, stacked DSP blocks can be expressed as:
\begin{equation}
    \label{eq_dsp1}
    \Phi_{M}=\mathcal{F}_{DSP}^{M}(\mathcal{F}_{DSP}^{M-1}(\cdot\cdot\cdot\mathcal{F}_{DSP}^{1}(\Phi_0)\cdot\cdot\cdot))
\end{equation}
where $\mathcal{F}_{DSP}^{m}$ denotes the mapping of the $m$-th DSP block, $m\in[1, M]$, $\Phi_0$ and $\Phi_M$ are the input/output features, respectively. As shown in Fig.~\ref{dsp}, each DSP block includes three branches: the upper is the dynamic scale branch, the middle is the flat convolution branch, and the lower is the pixel attention branch. Specifically, we employ three convolutions with $1\times1$ kernel to split the channels, which are further processed by each branch. Note that the dynamic scale branch needs to be downsampled before $1\times1$ convolution. 
{Given the input $\Phi_{m-1}$, we obtain:
\begin{gather}
    \Phi_{m-1}^{1}=Conv_{1\times1}((\Phi_{m-1})\downarrow)  \label{eq_dsp2_1}
    \\
    \Phi_{m-1}^{k}=Conv_{1\times1}(\Phi_{m-1})  \label{eq_dsp2_2}
\end{gather}
where $\Phi_{m-1}^{1}$ is the output from the upper dynamic scale branch, $k=2,3$ denotes the features of the other two branches, $Conv_{1\times1}$ is $1\times1$ convolution, and $\downarrow$ is the downsampling operation.} 
Except for the pixel attention branch, which has features with half the total channels, the other two branches process features with $\frac{1}{4}$ of the channels each. Next, the pixel attention branch obtains features through the pixel attention scheme~\citep{zhao2020efficient}. In contrast, the other two branches extract spatial information with a $3\times3$ flat convolution, followed by a $1\times1$ convolution to restore the number of channels to be the same as the pixel attention branch. Note that the dynamic scale branch needs upsampling after $1\times1$ convolution. Then, the features from the dynamic scale and the flat convolution branches can be fused by summation. After concatenation of the features followed by a $1\times1$ convolution, the DSP finally generates the output features $\Phi_{m}$ in a residual learning fashion. {It can be written as follows:
\begin{gather}
    \Phi_{m}^{1}=Conv_{1\times1}(Conv_{3\times3}(\Phi_{m-1}^{1}))\uparrow     \label{eq_dsp3_1}
    \\
    \Phi_{m}^{2}=Conv_{1\times1}(Conv_{3\times3}(\Phi_{m-1}^{2}))               \label{eq_dsp3_2}
    \\ 
    \Phi_{m}^{3}=Conv_{3\times3}(\Phi_{m-1}^{3}) \odot \sigma(Conv_{1\times1}(\Phi_{m-1}^{3}))
    \label{eq_dsp3_3}
    \\
    \Phi_{m}^{'}=Conv_{3\times3}(\Phi_{m}^{3})
    \label{eq_dsp3_5}
    \\
    \Phi_{m}^{''}=Conv_{3\times3}(\Phi_{m}^{1}\oplus\Phi_{m}^{2})  \label{eq_dsp3_4}
\end{gather}
where $\sigma$ is the sigmoid function, $\odot$ and $\oplus$ are element-wise multiplication and element-wise summation, respectively, and $\uparrow$ denotes the upsampling operation. After concatenation of the features $\Phi_{m}^{'}$ and $\Phi_{m}^{''}$ followed by a $1\times1$ convolution, the DSP finally generates the output features $\Phi_{m}$ in a residual learning manner. This process can be expressed as follows:
\begin{equation}
    \label{eq_dsp1_4}
    \Phi_{m}=Conv_{1\times1}([\Phi_{m}^{'}, \Phi_{m}^{''}]) \oplus \Phi_{m-1}
\end{equation}
where $[\cdot]$ perform concatenation.}

To further enhance the feature representation of the subnetwork, we incorporate LightViT \citep{huang2022lightvit} as the tail module, which utilizes local-global attention broadcast to aggregate information from all tokens, allowing for the efficient integration of global dependencies of local tokens into each image token. Finally, considering that the supervised attention module (SAM) \citep{zamir2021multi} can restore information progressively between stages/branches, we employ it to output the gradient map $E\in \mathbb{R}^{H\times W\times 1}$ and high-frequency features $F_{edge}\in \mathbb{R}^{H\times W\times C}$, used respectively as intermediate output -- allowing for explicit supervision over edges -- and as guidance for GDRB. Under this lightweight design, HFEB can effectively still extract meaningful structural information with different scale receptive fields.


\subsection{Guided Depth Restoration Branch (GDRB)}
\label{GDRB}
As shown in Fig.~\ref{framework}, GDRB is composed of two stages, and each one processes features at three scales, following a coarse-to-fine strategy \citep{gao2019dynamic,sarlin2019coarse}. The two stages are implemented with standard U-net architectures~\citep{ronneberger2015u}. More specifically, a cross-stage feature fusion module~\citep{zamir2021multi} is deployed between the two, which proved to be effective in image restoration and, in our design, allows GDRB to benefit from the intermediate features extracted by HFEB. To prevent aliasing in downsampling, we employ content-aware filtering layers~\citep{zou2022delving} in the encoders. Besides, GDRB deploys some further SAM blocks \citep{zamir2021multi}, allowing valuable features to propagate to the next stage. In addition to depth features, the SAMs of the two stages also output depth maps $\tilde{D}_{sr}^{'}$ and $\tilde{D}_{sr}^{''}$, to which intermediate supervision is provided. Note that input images are downsampled to the lower stage using pixel unshuffling to prevent information loss. Subsequently, the depth map output of this stage is restored at high resolution by employing pixel shuffling.

Based on the above structure, we propose two novel modules: AFFM and LCF. The former fuses gradient features between each encoder/decoder, while the latter supplements additional HF information in an implicit manner. 

\begin{figure}	
	\centering	
	\captionsetup[subfigure]{font=footnotesize,textfont=footnotesize}
	\subfloat{	
		\centering	
		\includegraphics[width=2.9in]{./figs/framework/AFFM.pdf}}
	\vspace{-0.cm}
	\caption{\textbf{AFFM architecture, operating at middle scale.} AFFMs for the remaining scales follow the same design.}
	\label{affm} 
\end{figure}

\textbf{Adaptive feature fusion module.} Recent networks such as \cite{ye2020pmbanet,tang2021joint} typically concatenate RGB and depth features directly during feature fusion,  followed by additional operations such as channel attention~\citep{zhang2018image} to capture useful information. In contrast, inspired by \cite{liu2021multi}, we run adaptive feature fusion through AFFM in two steps to strengthen the reconstruction of HF cues, as illustrated in Fig.~\ref{affm}. We differentiate from \cite{liu2021multi} by using dynamic convolution~\citep{chen2020dynamic} to better aggregate depth and HR features. In the first step, we generate dynamic weights $\pi_i, i=0,1,2$, which are then assigned to features from different scales within the current stage. Finally, we perform element-wise summation to obtain the feature maps $F^{'}$. For clarity, the figure shows the module working at the middle scale of the network as an example, with the others sharing the same design. 
{The process is defined as follows:
\begin{gather}
    F_{cat}=Conv_{3\times3}(Conv_{1\times1}([F^0\downarrow,F^1,F^2\uparrow]))       \label{eq_affm1_0}
    \\
    \{\pi_0,\pi_1,\pi_2\}=\sigma(Avgpool(F_{cat})) \label{eq_affm1_1}
    \\
    F^{'}=\pi_0\cdot F^0\downarrow+\pi_1\cdot F^1+\pi_2\cdot F^2\uparrow  \label{eq_affm1_2}
\end{gather}
where $F^{i}, i=0,1,2$ denotes the feature maps from the three scales, and $\downarrow$, $\uparrow$ are respectively downsampling and upsampling operators.}

In the second step, gradient features $F_{edge}$ from HFEB are concatenated with $F^{'}$. Then, per-pixel attention maps $F_{att}$ are generated by a ResBlock \citep{he2016deep} followed by an average pooling operation. These attention maps are then applied directly to the adaptively fused features $F^{'}$ through element-wise multiplication operation. Finally, after $1\times 1$ convolution, the attention-guided features $F_{out}^{i}$ are delivered to the corresponding scale of the current stage. In Fig. \ref{affm}, the output is passed to the middle scale of the decoder. AFFMs working at the other scales send their output to the corresponding scale in the decoder.  
{This step can be formalized as follows:
\begin{gather}
    F^{''}=[Avgpool(Conv_{3\times3}(F_{edge})),F^{'}]  \label{eq_affm2_0}
    \\
    F_{att}=\sigma(Avgpool(ResBlock(F^{''}))) \label{eq_affm2_1}
    \\
    F_{out}^{1}=con_{1\times1}(F^{'}\otimes F_{att})  \label{eq_affm2_2}
\end{gather}
where $\otimes$ is an element-wise multiplication operation and $F_{out}^{1}$ denotes the output features at the middle scale.}


\begin{figure}	
	\centering	
    \includegraphics[width=0.45\textwidth]{./figs/framework/LCF.pdf}
    \vspace{-0.cm}
	\caption{\textbf{Low-cut filtering module (LCF).} LF features are extracted through DCT and multi-spectral channel attention, and subtracted from the input to retain HF features.}	
	\label{lcf}
\end{figure}

\textbf{Low-cut filtering module.} 
The performance of our method greatly benefits from the explicit gradient information, but some valuable high-frequency information still vanishes. This fact motivates us to consider extracting complementary information in the frequency domain. As a common practice \citep{chang2007reversible,lin2010improving}, we use the low-frequency information of the discrete cosine transform (DCT) to compress images. Based on the design approach proposed in \cite{qin2021fcanet}, we develop a filtering module utilizing feature decomposition in the frequency domain to extract low-frequency components from the input. Specifically, we apply a $1\times1$ convolution followed by a channel split to the input color image $I_{hr}^{0}$. Then, we can obtain assigned frequency components from the output features $[{f}_0, {f}_1, \cdot\cdot\cdot, {f}_{n-1}]$ after DCT. Thus, the multi-spectral channel attention maps are generated by a fully connected layer and sigmoid activation. According to \cite{qin2021fcanet}, the low-frequency information is first assured to pass. Thus, we subtract such a low-frequency component from the input features producing the complementary high-frequency features $F_{rgb}$. Fig.~\ref{lcf} illustrates LCF in detail.  The high-frequency cues extracted from these features enable GDRB to progressively super-resolve LR depth maps into HR ones.


\textbf{Refinement.} To enhance the depth quality further, we optionally feed our final output into NLSPN \citep{park2020non} for refinement. This variant of the method is referred to as \netname$^+$.


\subsection{Training Loss}
Our network is trained in an end-to-end fashion using two loss terms: depth loss $L_d$ and gradient loss $L_g$. The depth loss is defined as:
\begin{equation}
\begin{aligned}
\label{lossd}
    L_d\!=\!\parallel\!(\!\tilde{D}_{sr}\!-\!D_{gt})\!\odot\!\mathbb{I}\!\parallel_1\!&+\lambda_d\cdot\!\parallel\!(\!\tilde{D}_{sr}^{'}\!-\!D_{gt})\!\odot\!\mathbb{I}\!\parallel_1\! \\
        &+\lambda_d\cdot\!\parallel\!(\!\tilde{D}_{sr}^{''}\!-\!D_{gt})\!\odot\!\mathbb{I}\!\parallel_1
\end{aligned}
\end{equation}
where $D_{gt}$ is the ground truth depth, $\tilde{D}_{sr}$, $\tilde{D}_{sr}^{'}$ and $\tilde{D}_{sr}^{''}$ are predicted depth maps from different stages, and $\mathbb{I}$ is pixel validity, as defined in \cite{de2022learning}. We empirically set $\lambda_d=0.2$.
Gradient loss $L_g$ is computed on HEFB output, as:
\begin{equation}
    L_g=\parallel\tilde{E}-E_{gt}\parallel_1
\end{equation}
where $\tilde{E}$ is the predicted gradient map and $E_{gt}$ is the ground truth one, extracted according to \cite{liu2021multi}. Thus, the total loss can be defined as:
\begin{equation}
    L_{total}=L_d+\lambda_g\cdot L_g
\end{equation}
with $\lambda_g$ empirically set to 0.01.