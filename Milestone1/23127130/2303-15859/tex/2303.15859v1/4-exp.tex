We have conducted adequate experiments in this section to reveal what kinds of objectness is beneficial for query-based methods in open-world instance segmentation task. 

\begin{table*}[htbp]
% \renewcommand\arraystretch{1.2}
\centering
\caption{Results of cross-category generalizability evaluation on VOC$\to$Non-VOC scenario. $\mathrm{AR^{box}}$ denotes the box AR performance at a budget of 100. $\mathrm{AR}$ without superscript denotes mask AR performance at a budget of 100. \textbf{Bold} scores denote the best performance of each metric. OpenInst$^*$ denotes OpenInst trained with pseudo annotations produced by GGN.}
\label{tab:exp:voc}
\resizebox{\textwidth}{!} 
{
\begin{tabular}{lcccccccc}
\toprule
Methods & Ref & $\mathrm{AR^{box}}$ & $\mathrm{AR}$ & $\mathrm{AR_{0.5}}$ & $\mathrm{AR_{0.75}}$  & $\mathrm{AR_{small}}$ & $\mathrm{AR_{med}}$ & $\mathrm{AR_{large}}$ \\
\midrule
OLN~\cite{oln}        & ICRA22      & \textbf{33.0}          & 26.9          & -             & -                & -              & -             & -       \\
LDET~\cite{ldet}      & ECCV22      & 30.8          & 27.4          & -             & -                & -              & -             & -       \\
GGN~\cite{ggn}        & CVPR22      & 31.5          & 28.7          & -             & -                & -              & -             & -       \\
SOIS~\cite{sois}      & ARXIV22     &  -            & 11.0          &  -            &  -               &  4.9           &  9.2          & 24.8    \\
\textbf{OpenInst}     & -             & 32.0          & 28.2          & 44.8          & 29.4             & \textbf{11.6}           & 33.4          & 54.8    \\
\textbf{OpenInst$^*$} & -             & 33.0          & \textbf{30.1}          & \textbf{47.5}          & \textbf{31.9}             &  7.4           & \textbf{38.5}          & \textbf{64.2}    \\
\bottomrule
\end{tabular}
}
\end{table*}

\subsection{Datasets and Evaluation}
We conduct our experiments on six popular datasets: COCO~\cite{coco}, UVO~\cite{uvo}, Objects365~\cite{objects365}, Mapillary Vistas~\cite{mapillary}, LVIS~\cite{lvis}, and Cityscapes~\cite{cityscapes}, which are widely used in the closed-world instance segmentation task. LVIS, Mapillary Vistas, and Objects365 are only used for evaluation. Cityscapes is only used for training. COCO and UVO are used for both training and evaluation. 

\textbf{COCO} is a widely-used dataset in object detection and instance segmentation.
Following OLN and LDET, we use the train2017 split of COCO for training and the val2017 split for evaluation, which contain 117k and 5k images respectively. COCO covers 80 object categories, which are a superset of categories in the PASCAL VOC dataset.
We use COCO in both cross-category and cross-dataset generalizability evaluations. 
\textbf{UVO} is a class-agnostic and exhaustively labeled dataset. It is specially designed for the open-world instance segmentation task. We use UVO v0.5 for a fair comparison with other methods. UVO v0.5 contains 15315 images for training and 7356 images for validation. Following OLN, GGN, and LDET, we mainly use the validation split for cross-dataset generalizability evaluations. Following SOIS, we also conduct the so-called inner-dataset evaluation, which means using the training set of UVO v0.5 to train the model, and using the validation set of UVO v0.5 for evaluation. 
\textbf{Objects365} is large scale dataset for object detection and has 80k images for validation. Following LDET, we sample 5k images from the validation split of Objects365 for cross-dataset generalizability evaluation. Since Objects365 only has box-level annotations. We only apply open-world object detection evaluation on Objects365.
\textbf{Mapillary Vistas} is a street-centric dataset.
We use the validation split of Mapillary Vistas on version v1.2 for cross-dataset generalizability evaluation.
\textbf{LVIS} is a large-vocabulary dataset for instance segmentation. It contains more than 1200 categories. Following SOIS, we use the validation split (20k images) of LVIS for cross-dataset generalizability evaluation.
\textbf{Cityscapes} consists of urban scene images from 50 different cities. We use the 8 foreground classes from Cityscapes for training, and test the model on the validation split of Mapillary Vistas.


We use the average recall (AR) as the indicator to quantitatively measure the generalizability of different models. Since the AR metric is commonly used in many open-world instance segmentation works~\cite{oln,oln-fcos,ggn,ldet,sois}. Following OLN~\cite{oln}, GGN~\cite{ggn}, and LDET~\cite{ldet}, we also conduct experiments in two main settings: cross-category and cross-dataset.
AR@k denotes the average recall at a budget of k, which means only top k predictions of each image are used for calculating the average recall.

\textbf{Cross-category.}
We conduct cross-category generalizability evaluation only on one scenario: VOC$\to$Non-VOC. We only use annotations belonging to PASCAL VOC~\cite{pascalvoc} categories for supervision in the training phase. And we use annotations belonging to other categories for evaluation. In this experiment, we set the query number to 150. Because predictions belong to $C_{base}$ categories are excluded from the budget of the recall in evaluation.

\textbf{Cross-dataset.}
Cross-dataset generalizability evaluation uses two different datasets for training and testing respectively. The testing dataset contains both $C_{base}$ and $C_{novel}$ categories. We conduct cross-dataset generalizability evaluation on four scenarios: COCO$\to$UVO, COOC$\to$Objects365, COCO$\to$LVIS, and Cityscapes$\to$Mapillary Vistas.

\subsection{Implementations Details}
We build OpenInst on the powerful MMDetection~\cite{mmdet} library. We benchmark our method against the advanced QueryInst~\cite{queryinst} method. We use ResNet-50~\cite{resnet} as the backbone of our model, and leverage BiFPN~\cite{bifpn} instead of vanilla FPN~\cite{fpn} as the neck module. We use box IoU to displace the classification label as the learning objective unless otherwise specified. The box head is trained with L1 loss and GIoU loss~\cite{giou}, whose loss weights are set to 5.0 and 2.0 respectively. The mask head is trained with the dice loss. The number of decoder layer is set to 6 in all experiments.
We use the Adam~\cite{adam} optimizer as our solver with the learning rate set to 1e-4 and weight decay set to 5e-4. For \textbf{1x} configuration, we set the total epoch to 12, and make it decayed at epoch 8 and epoch 11 by 0.1 respectively. For \textbf{3x} configuration, the decay point is set to epoch 27 and epoch 33 respectively. We adopt RandomFlip as the only data augmentation method in our data pipeline for \textbf{1x} configuration and apply \textbf{L}arge \textbf{S}cale \textbf{J}ittor (LSJ) for \textbf{3x} configuration for fair comparison.

% \subsection{Main Results}
\begin{table*}[htbp]
% \renewcommand\arraystretch{1.2}
\centering
\caption{Results of cross-category generalizability evaluation on COCO$\to$UVO scenario. $\mathrm{AR^{box}}$ denotes the box AR performance at a budget of 100. $\mathrm{AR}$ without superscript denotes the mask AR performance at a budget of 100. \textbf{Bold} scores denote the best performance of each metric. Aux. denotes that the method contains an auxiliary model and pseudo annotations.}
\label{tab:exp:uvo}
\resizebox{\textwidth}{!} 
{
\begin{tabular}{lcccccccccc}
\toprule
Methods & Ref & Aux. & Epochs & $\mathrm{AR^{box}}$ & $\mathrm{AR}$ & $\mathrm{AR_{0.5}}$ & $\mathrm{AR_{0.75}}$  & $\mathrm{AR_{small}}$ & $\mathrm{AR_{med}}$ & $\mathrm{AR_{large}}$ \\
\midrule
%OLN       &             & 8 & -             & -             & -             & -                & -              & -     & -      \\
LDET~\cite{ldet}    & ECCV22  &             & 8  & 47.5          & 40.7          & -             & -                & 26.8           & 40.0           & 45.7   \\
GOOD~\cite{good}  & ICLR23    &  \checkmark & 8 & 50.3  & - & - & - & - & - & - \\
GGN~\cite{ggn}    & CVPR22    & \checkmark  & 8  & 52.8          & 43.4          & 71.7          & 44.5             & 23.3           & 44.4           & 50.0   \\
SOIS~\cite{sois}  & ARXIV22    & \checkmark  & 36 & -             & 51.3          & -             & -                & -              & -              & -      \\
\textbf{OpenInst} & -          &             & 12 & 59.1          & 48.7          & 72.6          & 51.4             & 26.4           & 44.3           & 60.4   \\
\textbf{OpenInst} & -          &             & 36 & \textbf{63.0} & \textbf{53.3} & \textbf{76.6} & \textbf{56.8}    & \textbf{31.8}  & \textbf{49.4}  & \textbf{64.3}   \\
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[htbp]
% \renewcommand\arraystretch{1.2}
\centering
\caption{Results of cross-category generalizability evaluation on COCO$\to$Objects365 scenario. $\mathrm{AR^{box}}$ denotes the box AR performance at a budget of 100. \textbf{Bold} scores denote the best performance of each metric.}
\label{tab:exp:obj365}
\begin{tabular}{lcccccc}
\toprule
Methods & $\mathrm{AR^{box}}$ & $\mathrm{AR_{0.5}^{box}}$ & $\mathrm{AR_{0.75}^{box}}$  & $\mathrm{AR_{small}^{box}}$ & $\mathrm{AR_{med}^{box}}$ & $\mathrm{AR_{large}^{box}}$ \\
\midrule
Mask R-CNN~\cite{mask-rcnn}  & 38.5          & -             & -             & 24.0             & 40.1           & 50.2   \\
LDET~\cite{ldet}      & 41.1          & -             & -             & 26.1             & 43.8           & 52.8   \\
% GGN       & 00.0          & 00.0          & 52.9          & 33.5             & 50.7           & 61.9   \\
% SOIS      & 00.0          & 00.0          & \textbf{53.6} & 34.3             & \textbf{51.4}  & \textbf{62.6}   \\
\textbf{OpenInst} & \textbf{50.1}          & 64.1          & 53.0          & \textbf{29.8}             & \textbf{51.8}           & \textbf{66.6}   \\
\bottomrule
\end{tabular}
\end{table*}


\begin{table}[htbp]
% \renewcommand\arraystretch{1.2}
\centering
\caption{Results of cross-category generalizability evaluation on COCO$\to$LVIS scenario. $\mathrm{AR}$ denotes the mask AR performance at a budget of 100. \textbf{Bold} scores denote the best performance of each metric.}
\label{tab:exp:lvis}
\begin{tabular}{ccccccc}
\toprule
Methods & $\mathrm{AR}$ \\
\midrule
Mask R-CNN~\cite{mask-rcnn}    & 22.4  \\
LDET~\cite{ldet}        & 25.1  \\
Mask2Former~\cite{mask2former} & 24.5  \\
SOIS~\cite{sois}        & 25.2  \\
\textbf{OpenInst}   & \textbf{29.3}  \\
\bottomrule
\end{tabular}
\end{table}

\begin{table*}[htbp]
% \renewcommand\arraystretch{1.2}
\centering
\caption{Results of cross-category generalizability evaluation on Cityscapes$\to$Mapillary Vistas scenario. $\mathrm{AR}$ denotes the mask AR performance at a budget of 100. \textbf{Bold} scores denote the best performance of each metric.}
\label{tab:exp:mapillary}
\begin{tabular}{ccccccc}
\toprule
Methods & $\mathrm{AR}$ & $\mathrm{AR_{0.5}}$ & $\mathrm{AR_{0.75}}$  & $\mathrm{AR_{small}}$ & $\mathrm{AR_{med}}$ & $\mathrm{AR_{large}}$ \\
\midrule
% OLN       & 00.0          & 00.0          & 50.8          & \textbf{35.4}    & 49.5           & 57.7   \\
Mask R-CNN~\cite{mask-rcnn}      &  8.4          & 16.3          & -             & -                & -              & -      \\
LDET~\cite{ldet}      & 10.6          & \textbf{21.8}          & -             & -                & -              & -      \\
% GGN       & 00.0          & 00.0          & 52.9          & 33.5             & 50.7           & 61.9   \\
% SOIS      & 00.0          & 00.0          & \textbf{53.6} & 34.3             & \textbf{51.4}  & \textbf{62.6}   \\
\textbf{OpenInst} & \textbf{11.6}          & 18.1          & 11.9          &  3.0             & 12.9           & 33.6   \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Cross-category Generalizability Evaluation}

\textbf{VOC}$\to$\textbf{Non-VOC.}
In the VOC$\to$Non-VOC scenario, we only concern with the performance of predictions matched with $C_{novel}$. Predictions matched with $C_{base}$ categories are not taken into account when calculating the recall scores. Therefore, we set the query number to 150 for a fair comparison with fellow works. The initialization is also changed from "Image Initialization" to "Random Initialization". Because "Image Initialization" focus more on $C_{base}$ objects in the refining process of the decoder, and thus weakens the localization performance on $C_{novel}$ objects. 
As shown in Fig.~\ref{tab:exp:voc}, OpenInst achieves comparable results against dense proposal-based methods~\cite{oln,ldet,ggn}. When compared with the query-based method SOIS, OpenInst obtains significant advantages. Powered by pseudo annotations, OpenInst$^*$ achieves state-of-the-art results on both boxes as well as mask AR and suppresses other methods by a notable margin. Besides, we have noticed that pseudo annotations mainly help improve the performance of medium and large-size novel objects, while downgrading the performance of small-size objects.


\subsection{Cross-dataset Evaluation}

\textbf{COCO}$\to$\textbf{UVO} is the most important scenario of cross-dataset generalizability evaluation. As shown in Tab.~\ref{tab:exp:uvo}, OpenInst achieves state-of-the-art results when trained with 12 epochs. OpenInst suppresses all dense proposal-based and query-based methods by a large margin. The mask AR score of OpenInst reaches 53.3, which is nearly 10 points higher than the advanced GGN~\cite{ggn}. When compared with query-based methods, OpenInst exceeds SOIS~\cite{sois} by 2 points on the mask AR score. Note that both GGN and SOIS have an auxiliary model and leverage pseudo annotations. Though an auxiliary model and pseudo annotations can enhance the detector, they also make the detector much heavier and the training pipeline more complicated. OpenInst possesses two advantages: better results and a simpler structure. LDET~\cite{ldet} is a dense proposal based without auxiliary models and pseudo annotation. Compared with LDET, we can find that the improvement of OpenInst trained with 12 epochs mainly comes from large objects. LDET suppress OpenInst trained with 12 epochs on $\mathrm{AR_{small}}$ by 0.4. We presume that query-based methods like OpenInst work better on large objects. Because the initialization of query boxes in this scenario is the size of the full images, which means query boxes and features are more likely to notice large objects. LDET leverages densely spread proposals of various sizes, which enables LDET to take objects of all sizes into account. When trained with 36 epochs with LSJ data augmentation, OpenInst obtains significant improvements on all size objects.

\textbf{COCO}$\to$\textbf{Objects365.}
Objects365 only has box-level annotations. We only use the box AR as the metric for evaluation. The original validation split of Objects365 contains 80k images. Following LDET~\cite{ldet}, we use the same subset of the original validation split for evaluation. The evaluation subset consists of 5k images. The taxonomy of Objects365 contains 365 categories and is a superset of the COCO taxonomy (80 categories). Objects365 contains all $C_{base}$ objects and lots of $C_{novel}$ objects. As shown in Tab.~\ref{tab:exp:obj365}, OpenInst outperforms LDET by a large margin. The box AR score of OpenInst reaches 50.1 and suppresses the score of LDET by 9.0 points. From the performance of the three object sizes, we can observe that the principal gap between the performance of LDET and OpenInst comes from large objects. This observation is consistent with that of the COCO$\to$UVO scenario.

\textbf{COCO}$\to$\textbf{LVIS} scenario is introduced by SOIS~\cite{sois}. LVIS is built upon COCO images but has more granular annotations. LVIS has 1203 categories while COCO has only 80. The taxonomy of LVIS is far bigger than that of COCO. Therefore, despite the overlapped images in the training and testing split, annotations of the overlapped images have a huge difference between the training split and the testing split. For those overlapped images, annotations from the training split do not help them cheat on the testing split. Results from COCO to LVIS can reveal the generalizability of the model all the same. As shown in Tab.~\ref{tab:exp:lvis}, OpenInst outperforms all available methods by at least 4.1 AR and achieves state-of-the-art results on the COCO$\to$LVIS scenario.

\textbf{Cityscapes}$\to$\textbf{Mapillary Vistas.}
Following LDET~\cite{ldet}, We train the detector using 8 object-level categories of Cityscapes: \textit{car, bicycles, motorcycle, train, bus, truck, person, rider}. For evaluation, we use the 35 foreground object-level categories of Mapillary Vistas. The number of categories in evaluation is four times as many as in training. As shown in Tab.~\ref{tab:exp:mapillary}, OpenInst achieves 29.3 mask AR@100, which promotes the performance of the query-based method SOIS by 4.1 AR.

\subsection{Ablation Study}

\begin{table}[htbp]
% \renewcommand\arraystretch{1.2}
\centering
\caption{Results on class-agnostic dataset UVO. AR@100 and AR@10 denote mask AR performance at a budget of 100 and 10 respectively. \textbf{Bold} scores denote the best performance of each metric.}
\label{tab:exp:uvo2uvo}
\begin{tabular}{ccc}
\toprule
Methods & $\mathrm{AR@100}$ & $\mathrm{AR@10}$ \\
\midrule
Mask R-CNN~\cite{mask-rcnn}         & 22.8          & 20.0             \\
LDET~\cite{ldet}             & 35.6          & 23.7             \\
SOIS~\cite{sois}             & 41.9          & \textbf{29.2}             \\
\textbf{OpenInst}        & \textbf{43.1} & 20.8             \\
\bottomrule
\end{tabular}
\end{table}


\begin{table*}[htbp]
% \renewcommand\arraystretch{1.2}
\centering
\caption{Effect of different geometric cues on COCO$\to$UVO scenario. $\mathrm{AR}^{box}$ denotes the box AR performance at a budget of 100. $\mathrm{AR}$ without superscript denotes the mask AR performance at a budget of 100. \textbf{Bold} scores denote the best performance of each metric. OpenInst-cls denotes the vanilla QueryInst trained in a class-agnostic way.}
\label{tab:exp:geocue}
\begin{tabular}{lccccccc}
\toprule
Methods & $\mathrm{AR^{box}}$ & $\mathrm{AR}$ & $\mathrm{AR_{0.5}}$ & $\mathrm{AR_{0.75}}$  & $\mathrm{AR_{small}}$ & $\mathrm{AR_{med}}$ & $\mathrm{AR_{large}}$ \\
\midrule
OpenInst-void   & 58.4          & 48.5          & 71.3          & 51.3          & 25.1             & 43.7           & \textbf{60.8}   \\
OpenInst-cls    & 55.7          & 44.9          & 72.1          & 46.7          & 24.0             & 41.8           & 55.2   \\
OpenInst-box    & \textbf{59.1} & \textbf{48.7} & \textbf{72.6} & \textbf{51.4} & \textbf{26.4}    & \textbf{44.3}  & 60.4   \\
OpenInst-mask   & 58.1          & 47.8          & 71.2          & 50.6          & 24.4             & 43.3           & 60.1 \\
OpenInst-fusion & 58.6          & 48.1          & 72.0          & 50.8          & 25.6             & 43.9           & 59.8   \\
\bottomrule
\end{tabular}
\end{table*}


\begin{table*}[htbp]
% \renewcommand\arraystretch{1.2}
\centering
\caption{Effect of different modules on COCO$\to$UVO scenario. $\mathrm{AR}^{box}$ denotes the box AR performance at a budget of 100. $\mathrm{AR}$ without superscript denotes the mask AR performance at a budget of 100. \textbf{Bold} scores denote the best performance of each metric}
\label{tab:exp:module}
\resizebox{\textwidth}{!} 
{
\begin{tabular}{lcccccccc}
\toprule
DCN~\cite{dcnv2} & BiFPN~\cite{bifpn} & $\mathrm{AR^{box}}$ & $\mathrm{AR}$ & $\mathrm{AR_{0.5}}$ & $\mathrm{AR_{0.75}}$  & $\mathrm{AR_{small}}$ & $\mathrm{AR_{med}}$ & $\mathrm{AR_{large}}$ \\
\midrule
           &            & 56.3          & 46.4          & 71.3          & 48.9          & 24.3             & 42.2           & 57.8   \\
\checkmark &            & 57.9          & 47.9          & \textbf{72.8} & 50.5             & 25.3          & 43.8           & 59.4   \\
           & \checkmark & 58.4          & 47.8          & 71.4          & 50.5             & 25.8           & 43.4          & 59.3   \\
\checkmark & \checkmark & \textbf{59.1} & \textbf{48.7} & 72.6          & \textbf{51.4} & \textbf{26.4}    & \textbf{44.3}  & \textbf{60.4}   \\
% OpenInst & &  & 00.0          & 00.0          & 52.9          & 33.5             & 50.7           & 61.9   \\
\bottomrule
\end{tabular}
}
\end{table*}

\textbf{Open-world dataset evaluation.} Following SOIS~\cite{sois}, we conduct extra experiments on the open-world dataset UVO. We use the training split (15k images) and validation split (7856 images) of UVO for training and evaluation respectively. For a fair comparison, we set the image size to 640, to be consistent with SOIS. As shown in Tab.~\ref{tab:exp:uvo2uvo}, OpenInst achieves the best performance on the mask AR@100. OpenInst suppresses the prior query-based SOIS by 1.2 points. Whereas, OpenInst performs poorly on the mask AR@10 metric. The result of OpenInst on AR@10 indicates that using the box IoU as a ranking indicator is inadequate. Box IoU is not a good ranking indicator.

\textbf{Effect of geometric cues.} We use OpenInst-void as the baseline model. Based on this baseline model, we add the classification, box IoU, mask IoU, and the geometric mean of box and mask IoU as the learning objective respectively for training. The combination of OpenInst and classification is a vanilla QueryInst trained in a class-agnostic way. The remaining combinations are illustrated in Sec.~\ref{sec:method}. As shown in Tab.~\ref{tab:exp:geocue}, OpenInst-void has achieved impressive results. These results effectively demonstrate that \textbf{explicitly learning objectness is not crucial in the open-world localization and instance segmentation problem}, which is a highly insightful discovery. When classification is applied as a learning objective, the performances of box and mask AR drop by 2.7 and 3.6 respectively. The performance on $\mathrm{AR_{0.75}}$ experienced a noticeable decrease of 4.3 points. It can be inferred that the classification primarily resulted in insufficient fineness instead of accuracy in the predictions. The impacts of the box and mask IoU is minor than that of the classification. The box IoU has a positive effect on the generalizability while the mask IoU has a negative effect. We presume that learning the mask IoU is more difficult than learning box IoU. Because masks always have irregular shapes and boxes are always a rectangle. Being biased to the difficult mask IoU learning objective degrades the generalizability of the model. From Tab.~\ref{tab:exp:geocue} we can observe that only setting box IoU as the learning objective can slightly improve the performance. Query-based methods without learning any objectness have already been good detectors for the open-world instance segmentation task.

\textbf{Effect of DCN and BiFPN}. As shown in Tab.~\ref{tab:exp:module}, both DCN~\cite{dcnv2} and BiFPN~\cite{bifpn} have a positive effect on all metrics. The deformable mechanism of DCN expands the receptive field of the detector, making it larger and irregular. The detector can find more objects from a larger receptive field and output more precise predictions through irregular shapes. As can be seen in the second row of Tab.~\ref{tab:exp:module}, DCN improves the performance of $\mathrm{AR_{0.5}}$ by an increase of 1.5 points. In comparison, BiFPN brings even greater improvements to small objects. BiFPN designs a dedicated structure that makes full use of multi-scale feature maps. Feature maps with high resolution are enhanced, which makes it easier to locate small objects. After combining the two modules, the mask AR performance is further boosted to 48.7. This shows that DCN and BiFPN are complementary to each other in the open-world instance segmentation task.
% \subsection{Visualization}
% % 
