
% intro for object detection and closed-world assumption
Object detection, which entails localizing and identifying objects within an image or a video sequence, is a crucial task in computer vision. Extensive researches have been conducted in this field, leading to the development of numerous works~\cite{ss,edgebox,rcnn,fast-rcnn,detr,owdetr,towod}. However, the majority of these works operate under the closed-world assumption, limiting their applicability to a pre-defined set of categories. Consequently, these models are inadequate when it comes to detecting novel objects that do not belong to the pre-defined categories, and their performance in recognizing such objects is limited. Direct deployment of these closed-world models in real-world scenarios can lead to serious consequences, such as false negatives that may cause accidents when applied to autonomous driving systems.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.95\linewidth]{fig/SpareseOLN.pdf}
%   \centering
%   \caption{(a) OLN is a dense proposal-based method built on Faster R-CNN that utilizes geometric objectness as both a learning objective and a ranking and filtering indicator, achieved by replacing the classification branch with a geometric cue branch. (b) In contrast, QueryInst is a query-based method that focuses on utilizing classification as the learning objective. Unlike Faster R-CNN, QueryInst does not have a ranking/filtering process. (c) Our proposed method, OpenInst, is built on QueryInst and aims to investigate the impact of utilizing geometric cues (box IoU or mask IoU or both, or none) solely as the learning objective. The box and mask predictions have been omitted in the figure to emphasize the objectness branches. \textbf{Bold} lines are also used to emphasize the objectness branch further. Dashed lines indicate that these components are optional. }
%   \label{fig:OpenInst}
% \end{figure}

% bring in the open-world proposal and instance segmentation task
Open-world instance segmentation~\cite{oln,ggn,ldet,sois} has recently gained increasing attention for improving real-world applications. This task involves localizing and segmenting both seen and novel objects in an image, without necessarily recognizing them. This concept is in line with the occupancy network~\cite{occupancy} implemented in Tesla AI, which only considers the presence of objects and not their categories. It can be applied to downstream task open-vocabulary object detection and instance segmentation~\cite{ovvild,ovcm}, which aims to localize and segment both seen and unseen objects with recognizing their categories. The proposal process such as localize and segment can be realized by open-world instance segmentation methods. The recognition process is usually realized by vision-language models (VLMs) such as CLIP~\cite{clip} and ALIGN~\cite{align}.


% Several approaches have addressed this issue by utilizing the assumption that some public datasets are not exhaustively annotated. For instance, GGN~\cite{ggn} leverages novel objects in training images using an auxiliary model by designing a pairwise affinity model to generate pseudo annotations of potential novel objects. Similarly, LDET~\cite{ldet} improves the approach by using a simple copy-and-paste data augmentation method to move annotated objects onto a clean background image, thereby avoiding the issue of misclassifying novel objects as background.

% problems that open-world proposals want to address
Despite the recent advancements in open-world instance segmentation, it remains a challenging task, mainly due to the unlabeled objects in the training dataset. Traditional closed-world methods treat unlabeled objects as background, leading to overfitting of the labeled categories and poor generalization. LDET~\cite{ldet} proposes a simple-yet-effective data augmentation method to address the issue. It simply copies and pastes labeled objects onto background images. OLN~\cite{oln} takes a different approach by replacing the classification branch with a localization cues branch, which is not trained on negative (background) samples, thus improving the model's ability to generalize. Whereas, the performance of these methods is limited. Other methods, such as GGN~\cite{ggn}, GOOD~\cite{good}, and SOIS~\cite{sois}, have achieved better results but rely on complex systems and paradigms, making it difficult to compare their performance fairly. For example, GGN uses an auxiliary model to pseudo-annotate potential novel objects in the training images based on pairwise affinity. GOOD utilizes off-the-shelf models to produce depth and normal images of the original training images, which are then used to generate pseudo annotations. SOIS introduces an auxiliary branch to predict the foreground of an image and minimizes the difference between the outputs of the foreground branch and mask branch as a consistent loss during training.

The previous methods have failed to strike a balance between simplicity and high-performance. We seek an alternative approach that would allow us to achieve excellent results while maintaining a straightforward methodology. Recently, query-based detectors, such as DETR~\cite{detr}, have emerged as a promising option. Unlike dense proposal-based methods, query-based detectors employ N (e.g. 100) learnable queries to replace hundreds of thousands of pre-defined proposals, eliminating the need for many-to-one matching and post-processing.
We are drawn to query-based approach for two reasons. First, it offers a more concise structure than dense proposal-based methods. Second, it delivers better open-world instance segmentation performance than dense propose-based methods. In particular, we conducted a fair comparison of the class-agnostic QueryInst~\cite{queryinst} and Mask R-CNN~\cite{mask-rcnn} models in the COCO$\to$UVO scenarios and found that QueryInst outperformed Mask R-CNN by 4.9 mask AR.
Based on these findings, we develop our model, which we name OpenInst, using a query-based approach. We selected the advanced QueryInst~\cite{queryinst} as our baseline model for open-world instance segmentation, allowing OpenInst to maintain the simplicity of the base model while avoiding the complicated processing associated with dense proposals.

% Additionally, its structure is similar to two-stage detectors such as Faster R-CNN and Cascade R-CNN, except for the proposal component, which allows for a fair comparison with dense proposal-based approaches. The comparison between OLN, QueryInst, and OpenInst is demonstrated in Fig.~\ref{fig:OpenInst}. Our modification to QueryInst is minimal and focused on advancing our research goals. 

IoU-Net~\cite{iounet}, Mask Scoring R-CNN~\cite{msrcnn}, and FCOS~\cite{fcos} have demonstrated the effectiveness of geometric cues such as centerness, box IoU, mask IoU) in the closed-world tasks. OLN~\cite{oln} has further extended geometric cues to open-world proposal in dense proposal-based model. Those observations make it promising to extend geometric cues to query-based methods in terms of simplicity and performance. Therefore, OpenInst focuses solely on geometric cues in training. Going one step further, the geometric cues are derived from ground-truth boxes and masks without producing any extra information. Is it possible for the model to only learn to predict boxes and masks? Our study offers an affirmative answer.


Following prior works~\cite{oln,ggn,ldet}, we evaluate the generalizability of OpenInst through experiments in two major settings: cross-category and cross-dataset. 
The cross-category setting involves training the model on a pre-defined set of classes from a dataset and testing it on the remaining classes of the same dataset. The cross-dataset setting involves training the model on one dataset and testing it on a different dataset.

For cross-category generalizability evaluation, we use the COCO~\cite{coco} dataset. In this experiment, we train the model on the VOC~\cite{pascalvoc} classes of the training split and evaluate it on the remaining classes of the validation split. OpenInst achieves a mask AR of 28.2 in the VOC$\to$Non-VOC scenario, outperforming OLN by 1.3 AR. Additionally, OpenInst is compatible with the GGN method. And when powered by pseudo labels produced by GGN~\cite{ggn}, it achieves state-of-the-art results of 33.0 box AR and 30.1 mask AR in the VOC$\to$Non-VOC generalizability evaluation.
As for the cross-dataset evaluation, we conduct four experiments. We use COCO as the training set and test OpenInst on UVO~\cite{uvo}, Objects365~\cite{objects365}, and LVIS~\cite{lvis} datasets respectively. We have also trained OpenInst on Cityscapes~\cite{cityscapes} and evaluated it on Mapillary Vistas~\cite{mapillary}. OpenInst achieves state-of-the-art results in mask AR in all four scenarios.
In the COCO$\to$UVO scenario, In the COCO$\to$UVO scenario, OpenInst outperforms all dense proposal-based methods by a significant margin, achieving a mask AR of 53.3. When OpenInst is trained for 36 epochs, the mask AR of OpenInst can be further boosted to 53.3 AR, which outperforms SOIS~\cite{sois} by 2.0 AR. 
In the COCO$\to$Objects365, COCO$\to$LVIS, and Cityscapes$\to$Mapillary Vistas scenarios, OpenInst outperforms all other methods by a remarkable margin.
When trained without using geometric cues as the learning objective, OpenInst still performs well with only a 0.2 AR decrease from the state-of-the-art result in COCO$\to$UVO scenario. As the old saying goes, "Great Truths Are Always Simple". It also holds true here.


%This discovery indicates that objectness branches (classification, box IoU, or mask IoU) are not crucial components of the open-world instance segmentation task and brings the current paradigm to a new era. 



To summarize, our contributions can be concluded as follows:
\begin{itemize}
    \item OpenInst achieves state-of-the-art results on multiple datasets and significantly outperforms previous methods. Our study confirms that incorporating box IoU into query-based detectors improves their generalizability.
    \item To the best of our knowledge, OpenInst is the first work that achieves both simplicity and high performance, serving as a solid baseline for future research in the open-world community.
    \item We conduct extensive ablation studies to analyze each component used in our model. We hope those observations benefit future research in this area.
\end{itemize}
