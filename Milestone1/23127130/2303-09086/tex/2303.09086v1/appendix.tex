\section{Complete Proofs}\label{sec_proof}

This section lays out the proofs for our statements in Section \ref{sec_alg}.
For ease of reading, we include a list of notations needed in the proofs below.

\begin{table}[h]
\centering
\caption{A table of notations used in our paper for reference.}\label{table_notations}
{\footnotesize \begin{tabular}{@{}ll@{}}
        \toprule
        Symbol & Definition \\ \midrule
$\cG = (\cV, \cE)$     &  Weighted and possibly directed graph      \\ 
$W$             &    A nonnegative weight matrix of $\cG$    \\
$W_{i,j}$       & The $(i, j)$-th entry of $W$ \\
$\lambda_k(W)$  &  The $k$-th largest singular value of a matrix $W$ \\
$\vec u_k$ & The left singular vector of a weight matrix $W$ corresponding to $\lambda_k(W)$ \\
$\vec v_k$ & The right singular vector of a weight matrix $W$ corresponding to $\lambda_k(W)$ \\
$\vec v(i)$ & The $i$-th coordinate of the vector $\vec v$ \\
$\tilde X_r$    & The best rank-$r$ approximation of $X$ \\
$\cW$          &  A sequence of weight matrices from timestamp $1$ to $s$ \\
$\cE^{(t)}$     &   The set of edges in the $t$-th graph of the sequence \\
$W^{(t)}$       &   A nonnegative square weight matrix for the graph at timestamp $t$ \\
$\norm{\cdot}$    & The $\ell_2$ norm of a vector or the spectral norm of a matrix \\
$\bignormFro{\cdot \ }$   & The Frobenius norm of a matrix  \\
$\langle \cdot , \cdot \rangle$  & The matrix inner product between two matrices 
            \\\bottomrule
\end{tabular}}
\end{table}


\subsection{Proof for the iterative greedy algorithm}
First, we prove the connection between generalized edge centrality and gradients.

\medskip
\noindent\textbf{Proof of Lemma \ref{prop_grad}.}
    Consider a singular value $\lambda_k$ of $X$, for any $k$.
    Let $\vec u_k$ and $\vec v_k$ be the left and right singular vectors of $X$ corresponding to $\lambda_k$, respectively.
    By the chain rule, it suffices to show that $\frac{\partial \lambda_k(X)}{\partial X_{i,j}} = \vec u_k(i) \cdot \vec v_k(j)$.
    First, we have
    $\vec u_k^{\top} X = \lambda_k  \vec v_k^{\top}.$
    We differentiate over $X$ on both sides of the above equation:
    \begin{align}
        \der(\vec u_k^{\top}) X + \vec u_k^{\top} \der(X) = \der(\lambda_k) \vec v_k^{\top} + \lambda_k \der(\vec v_k^{\top}). \label{eq_partial}
    \end{align}
    Since $\vec v_k$ is a unit length vector,
    \begin{align}
        \der(\norm{\vec v_k}^2) = 2\inner{\vec v_k}{\der(\vec v_k)} = 2\der(\vec v_k^{\top}) \vec v_k = 0. \label{eq_orth}
    \end{align}
    Thus, by multiplying both sides of equation \eqref{eq_partial} with $\vec v_k$, we  get
    \begin{align}
        \der(\vec u_k^{\top}) X \vec v_k + \vec u_k^{\top} \der(X) \vec v_k
        = \der(\lambda_k) \vec v_k^{\top} \vec v_k + \lambda_k \der(\vec v_k^{\top}) \vec v_k, \label{eq_multiply}
    \end{align}
    which is equal to $\der(\lambda_k)$ since equation \eqref{eq_orth} holds and $v_k$ is a unit length vector.
    Looking at equation \eqref{eq_multiply}, we observe
    \begin{align}
        \der(\vec u_k^{\top}) X \vec v_k = \der(\vec u_k^{\top}) \lambda_k \vec u_k = \lambda_k \der(\vec u_k^{\top}) \vec u_k = 0, \label{eq_i_j}
    \end{align}
    where the last step follows similarly to equation \eqref{eq_orth}, since $\vec u_k$ is also a unit length vector.
    In summary, we have shown
    $\vec u_k^{\top} \der(X) \vec v_k = \der(\lambda_k)$.
    This implies that the derivative of $\lambda_k$ over $X_{i,j}$ is equal to $\vec u_k(i) \cdot \vec v_k(j)$.
    Since this holds for any $k$, we thus conclude that equations \eqref{eq_edge_cen_1} and \eqref{eq_edge_cen_r} are both true.\hfill$\square$






\subsection{Proof for the running guarantee}

Next, we derive the convergence guarantee of Algorithm \ref{alg_edgecen}.

\medskip
\noindent\textbf{Proof of Theorem \ref{prop_continuous}.}
    We complete the convergence analysis of our algorithm.
    First, we show that the objective function $f(M)$ is convex in $M$.
    Second, we invoke the result of \citet{jaggi2013revisiting}, specifically Lemma 7 and Theorem 1, which show that as long as the gradient $\nabla f(M)$ is Lipschitz-continuous and the constraint set has bounded diameter, the Frank-Wolfe algorithm will converge to the optimum at a rate of $O(\frac 1 t)$ after $t$ iterations.

    We first show that the sum of top singular values $g(M) = \sum_{k=1}^r \lambda_k(M)$ is convex.
    With the variational characterization of singular values, $g(M)$ is equal to
    \begin{align}\label{eq_char}
        g(M) = \max_{U^{\top}U = V^{\top} V = \id_r:~U\in\real^{n\times r}, V\in\real^{m\times r}} \inner{UV^{\top}}{M}.
    \end{align}
    Thus, for any $n$ by $m$ matrix $M_1,M_2$, and any $\alpha \in [0, 1]$, let $\tilde U$ and $\tilde{V}$ be the maximizer of the above for $f\big(\alpha M_1 + (1 - \alpha) M_2\big)$.
    Therefore,
    \begin{align*}
        g\big(\alpha M_1 + (1 - \alpha) M_2\big)
        &= \inner{\tilde U \tilde V^{\top}}{\alpha M_1 + (1- \alpha) M_2} \\
        &\le \alpha \inner{\tilde U \tilde V^{\top}}{M_1} + (1-\alpha) \inner{\tilde U \tilde V^{\top}}{M_2} \\
        &\le \alpha g(M_1) + (1 - \alpha) g(M_2),
    \end{align*}
    which implies that $g(M)$ is convex. Next, we show that $f(M)$ is convex.
    For any $\alpha \in [0, 1]$,
    \begin{align*}
        &f(\alpha M_1 + (1- \alpha) M_2)
        = g\Big( (\alpha M_1 + (1- \alpha) M_2)^T(\alpha M_1 + (1- \alpha) M_2) \Big)\\
        &\le \alpha^2 g(M_1^{\top}M_1) + (1-\alpha)^2 g(M_2^{\top}M_2)
        + 2\alpha(1-\alpha) g(M_1^{\top}M_2).
    \end{align*}
    Let $\tilde U$ and $\tilde V$ be the maximizer of \eqref{eq_char} for $M_1^{\top} M_2$. We have
    \begin{align*}
        &2g(M_1^{\top} M_2)
        = 2\inner{\tilde U \tilde V^{\top}}{M_1^{\top} M_2}
        = 2\inner{M_1 \tilde U}{M_2 \tilde V} \\
        \le& \bignormFro{M_1 \tilde U}^2 + \bignormFro{M_2 \tilde V}^2 = \inner{M_1^{\top} M_1}{\tilde U \tilde U^{\top}} + \inner{M_2^{\top} M_2}{\tilde V \tilde V^{\top}} \\
        \le& g(M_1^{\top} M_1) + g(M_2^{\top} M_2).
    \end{align*}
    Therefore, $f(\alpha M_1 + (1-\alpha)M_2)$ is less than $\alpha \cdot g(M_1^{\top} M_1) = \alpha \cdot f(M_1)$ plus $(1-\alpha)\cdot g(M_2^{\top} M_2) = (1-\alpha) \cdot f(M_2)$.

    Second, we verify that $\nabla f(M)$ is $\alpha_2$ Lipschitz continuous in the Frobenius norm.
    The proof is based on matrix perturbation bounds.
    Let $\tilde M = M + E$ be a perturbation of $M$.
    Let $M_r = U_r D_r V_r^{\top}$ be the top-$r$ SVD of $M$.
    Let $\mu_1$ be the largest singular value of $M$.
    Let $\tilde M_r = \tilde U_r \tilde D_r \tilde V_r^{\top}$ be the top-$r$ SVD of $\tilde M$.
    First, consider $\norm{E}_2 \le \kappa / 2$.
    By matrix perturbation bounds on the truncated SVD of a matrix (e.g., Theorem 1 of \citet{vu2021perturbation}; the condition is satisfied since $\kappa$ is the spectral gap between the $r$-th and $(r+1)$-th largest singular values), we have
    \begin{align*}
        \normFro{M_r - \tilde M_r}^2
        \le 2\normFro{E}^2 + \frac{4\lambda_1^2}{\kappa^2}\normFro{E}^2 + C\normFro{E}^2.
    \end{align*}

     When $\norm{E}_2 \ge \kappa/2$, notice that
     \begin{align*}
         \normFro{M_r - \tilde M_r}^2 &= \normFro{U_r D_r V_r^{\top} - \tilde U_r \tilde D_r \tilde V_r^{\top}}^2 \\
         &\le 2\normFro{D_r}^2 + 2\normFro{\tilde D_r}^2 \\
         &\le 2r \lambda_1^2 + 2r(\lambda_1 + \norm{E}_2)^2,
     \end{align*}
     which is at most $2r(3\lambda_1^2 +  2\norm{E}_2^2)$.
     The step above uses the Weyl's Theorem that $\norm{D_r - \tilde D_r}_2 \le \norm{E}_2$.
     Taken together, we conclude that $\nabla f(M)$ must be
     \[ \sqrt{\max\Big(2 + \frac{4 \lambda_1^2}{\kappa^2} + C, \frac{24r \cdot \lambda_1^2}{\kappa^2} + 4r \Big)} \]
     Lipschitz-continuous.
     Lastly, the diameter of the constraint set is at most $\sqrt{\sum_{(i,j)\in\cE} W_{i,j}^2}$, since for every $(i,j)\in\cE$, the search space is bounded between $0$ and $W_{i,j}$.
     Taken together, we have proved that:
     $f(M)$ is convex, $\nabla f(M)$ is $\alpha_2$ Lipschitz continuous, and the diameter of the constrained space of problem \eqref{eq_convex} is $\sqrt{\alpha_1/8}$.
     Using Lemma 7 and Theorem 1 of \citet{jaggi2013revisiting}, the proof is complete.\hfill$\square$

\medskip
\noindent\textbf{Extension to time-varying networks.} Notice that the time-varying extension is a special case of the above result.
Therefore, the same convergence rate of $O(T^{-1})$ holds for Algorithm \ref{alg_edgecen_temporal} towards the global optimum of problem \eqref{eq_convex_temporal}.

\medskip
Lastly, we derive the gradient of the largest $r$ eigenvalues of $X^{\top}X$ where $X$ is the product of the weight matrices in the sequence of time-varying networks (cf. Section \ref{sec_tv}).

\medskip
\subsection{Derivation of the time-varying case: Equation \ref{eq_tv_ec}.} Let $\set{M^{(1)}, M^{(2)}, \dots, M^{(s)}}$ be a sequence of modified weight matrices and $X = \prod_{t=1}^{s}M^{(t)}$. Following Lemma \ref{prop_grad}, we derive the gradient of the largest $r$ eigenvalues of $X^{\top}X$ with respect to $M^{(t)}_{i, j}$, for any $1\le i,j\le n$.
By the chain rule, we have:
{\small\begin{align}
     \frac{\partial f(\cM)}{\partial M_{i,j}^{(t)}} 
    = \Big\langle{\frac{\partial\Big(\sum_{k=1}^r \big(\lambda_k(X)\big)^2 \Big)}{\partial X}}, {\frac{\partial X}{\partial M_{i,j}^{(t)}}}\Big\rangle. \label{eq_tv}
\end{align}}

\noindent Notice that the first term above on the right is precisely the edge centrality scores we have derived in Lemma \ref{prop_grad}.
The second term is essentially the product of the rest of the weight matrices in $\cW$ except that $M^{(t)}$ is replaced by an indicator matrix, which is the derivative of $M^{(t)}$ with respect to its $(i, j)$-th entry.

Let $\tilde X_r = U_r D_r V_r^{\top}$ be the rank-$r$ SVD of $X$. Let the product of weight matrices from $1$ to $t-1$ as $A = \prod_{k=1}^{t-1} M^{(k)}$ and the product of weight matrices from $t+1$ to $s$ as $B = \prod_{k=t+1}^{s} M^{(k)}$. $A$ is equal to identity matrix when $t = 1$, and $B$ is equal to identity matrix when $t = s$. Let $J^{i, j}$ as a single-entry indicator matrix where its $(i, j)$-th entry is $1$, and the rest of the entries are equal to 0. Then, we can rewrite the gradient as follows:
{\small\begin{align}
     \frac{\partial f(\cM)}{\partial M_{i,j}^{(t)}} 
    = 2 \Big\langle{\tilde X_r}, {AJ^{i,j}B}\Big\rangle 
    = 2 \sum_{1 \leq p,q \leq n} {\big( \tilde X_r \big )}_{p,q} {\big( AJ^{i,j}B \big)}_{p,q}
    = 2 \sum_{1 \leq p,q \leq n} {\big( \tilde X_r \big )}_{p,q} A_{p, i} B_{j, q}  
    = 2 \Big( A^{\top} {\tilde X_r } B^{\top} \Big)_{i, j}
\end{align}}

\noindent Thus, we get the gradient of $f(\cM)$ with respect to the weight matrix $M^{(t)}$ as follows:
{\small\begin{align}
    \frac{\partial f(\cM)}{\partial M^{(t)}}
    =2 A^{\top} {\tilde X_r } B^{\top}
    =2\Big(\prod\nolimits_{k=1}^{t-1} M^{(k)}\Big)^{\top} \tilde{X}_r \Big(\prod\nolimits_{k=t+1}^s M^{(k)}\Big)^{\top}.
\end{align}}%
The derivation of statement \eqref{eq_tv_ec} is now completed. \hfill$\square$


\section{Epidemic Models}\label{sec_epi_models}

We describe the epidemic models that are considered in our experiments. 
One widely used model of epidemic spread is the SEIR compartmental model.
An SEIR model uses four compartments to capture a spreading process: Susceptible (S), Exposed (E), Infected (I), and Recovered (R).
Every node must belong to one of the four states during the process.
At every time $t$,
\begin{itemize}[leftmargin=1cm]
    \setlength\itemsep{0.00em}
	\item $S^{(t)}$ denotes the set of susceptible nodes at time $t$. A node may get exposed if its incoming neighbors are infectious. The probability depends on the edge weights and the virus transmission rate.
	\item $E^{(t)}$ denotes the nodes exposed to the virus but are not infectious at time $t$. In expectation, a node remains exposed for $\delta_E$ periods.
	\item $I^{(t)}$ denotes the nodes who are infectious at time $t$. Each node remains infectious for $\delta_I$ periods in expectation.
	\item $R^{(t)}$ denotes the nodes who have recovered at time $t$.
\end{itemize}

For weighted graphs, we simulate an SEIR model. At each time $t$, we calculate the infection probability for node $i$ based on the edge weights and transmission rate $\beta_{\text{Base}}$:
{\small
$$
p_i = 1 - \prod_{(i,j)\in E: j \in I^{(t)}} \max\Big(1 - W_{i,j}\beta_{\text{base}}, 0\Big).
$$
}

We follow the procedure in \citet{chang2021mobility} for mobility networks to simulate the metapopulation SEIR model.
At time $t$, the transitions between the four states (for $c_i$) are sampled as follows:
\begin{small}
\begin{align}
	N^{(t)}_{S_{c_i} \rightarrow E_{c_i}} &\sim \pois\Big( \frac{S_{c_i}^{(t)}}{N_{c_i}} \lambda^{(t)} \Big) + \Binom\Big(S_{c_i}^{(t)}, \lambda_{c_i}^{(t)}\Big). \label{eq_S_E} \\
	N_{E_{c_i} \rightarrow I_{c_i}}^{(t)} &\sim \Binom\Big(E_{c_i}^{(t)}, \frac{1}{\delta_E}\Big). \label{eq_E_I} \\
	N_{I_{c_i} \rightarrow R_{c_i}}^{(t)} &\sim \Binom\Big(I_{c_i}^{(t)}, \frac{1}{\delta_I}\Big). \label{eq_I_R}
\end{align}
\end{small}

\noindent where $\lambda^{(t)}$ is the aggregate transmission rate over the points of interest; $\lambda_{c_i}^{(t)}$ is the base transmission rate within $c_i$; $\latent$ represents the mean latency period; $\recover$ is the mean infectious period.

In equation \eqref{eq_S_E},
$\lambda_{c_i}^{(t)}$ is given by the product of the base transmission rate $\beta_{\base}$ and the proportion of infectious individuals in CGB $c_i$: 
    $\lambda_{c_i}^{(t)} = \beta_{\base} \frac{I_{c_i}^{(t)}}{N_{c_i}}.$
The infection rate across all the POIs is 
{\small
$$
    \lambda^{(t)} = \sum_{j=1}^n \lambda_{p_j}^{(t)} W_{i,j}^{(t)}; \ \lambda_{p_j}^{(t)} = \beta_{p_j}^{(t)} \frac{ I_{p_j}^{(t)}}{\sum_{i=1}^m W_{i, j}^{(t)} }.
$$}

\noindent where $\lambda_{p_j}^{(t)}$ is the infection rate for POI $p_j$  at time $t$.
$\beta_{p_j}^{(t)}$ is the transmission rate at POI $p_j$ and ${I_{p_j}^{(t)}}$ is the number of infectious individuals in $p_j$ at time $t$. The parameters are estimated as follows.
(i) $\beta_{p_j}^{(t)}$ is estimated by the physical area of $p_j$: 
$
\beta_{p_j}^{(t)} = \psi \cdot \cdot d_{p_j}^2 \cdot \frac{V_{p_j}^{(t)}}{a_{p_j}}$
in which $\psi$ is a transmission constant;
$a_{p_j}$ is the physical area of $p_j$;
$V_{p_j}^{(t)} = \sum_{i=1}^m W_{i,j}^{(t)}$ represents the number of visitors to $p_j$ at time $t$.
(ii) $I_{p_j}^{(t)}$ is estimated in proportion to the infectious population from each CBG and their number of visits to $p_j$: 
$
I_{p_j}^{(t)} = \sum_{k=1}^m \frac{I_{c_k}^{(t)}}{N_{c_k}} W_{k,j}^{(t)}.
$

The SEIR model has many variants (cf. \citet{prakash2012threshold}). We consider SIR and SIS that share similar spreading processes as the SEIR model. We describe their differences as follows.
The SIR model uses three compartments as the SEIR model except for the exposed state. It assumes that there is no latent period of the disease. Nodes are capable of infecting susceptible nodes directly after being infected.
The SIS model uses two states (Susceptible and Infectious) in a spreading process. It assumes that recovery does not bring immunity and nodes who have recovered will become susceptible again. 

\section{Experiment Details}\label{sec_add_setup}

\medskip
\noindent\textbf{Simulation setup.} 
For the weighted graphs, we simulate an SEIR model on each graph.
We use a transmission rate $\beta_{\text{Base}} = 0.05$ and a initial exposed ratio $p_0 = 0.01$.
To avoid infecting all the graph nodes, we simulate for 50 epochs. We use a slightly higher edge-weight reduction budget as 20\% of the total edge weights because the average edge weight in these three graphs is smaller than the mobility networks.

For the experiments concerning mobility networks, we follow the procedures of \citet{chang2021mobility} to simulate a metapopulation SEIR model in each network.
We calibrate the parameters of the SEIR model following their method.
We simulate 100 epochs on static mobility networks to be consistent with the simulation of \citet{chang2021mobility}.
The results are consistent throughout the simulation.
We compare the \LPshort{} algorithm with baseline methods using an edge-weight reduction budget as 5\% of the total edge weights. The results of using other budget amounts are consistent. We use the same set of parameters for SIR and SIS model simulations.

In time-varying mobility networks experiments, we simulate the metapopulation SEIR model on a sequence of ten networks for 70 epochs for every network. We set the edge-weight reduction budget as 5\% of the total edge weights of the sequence. 


\medskip
\noindent\textbf{Model validation.} 
We calibrate the following parameters for the metapopulation SEIR model on mobility networks: (i) the transmission constant in POIs, $\psi$; (ii) the base transmission rate, $\beta_{\text{base}}$; and (iii) the ratio of initially exposed individuals, $p_0$.
We use grid search to find the parameters with the smallest root mean square error compared to the reported number of infected cases.  
We calibrate an SEIR model for every MSA independently.
We compare the predicted cases of our simulated SEIR model with the reported cases from New York Times COVID-19 data.
The root mean squared error of all the epochs is 295.17, averaged over eight mobility networks. The error is within $3\%$ compared to the overall infected population at $10^4$. These results reaffirm the finding of \citet{chang2021mobility}.

 
\medskip
\noindent\textbf{Data availability.}
The three weighted graphs are available in the following sources: Airport\footnote{\url{http://opsahl.co.uk/tnet/datasets/openflights.txt}}, Adavogato\footnote{\url{https://downloads.skewed.de/mirror/konect.cc/files/download.tsv.advogato.tar.bz2}}, and Bitcoin\footnote{\url{http://snap.stanford.edu/data/soc-sign-bitcoinalpha.html}}.
The two weighted time-varying graphs are available in the following sources: Bitcoin-Alpha\footnote{\url{https://snap.stanford.edu/data/soc-sign-bitcoinalpha.csv.gz}} and Bitcoin-OTC\footnote{\url{https://snap.stanford.edu/data/soc-sign-bitcoinotc.csv.gz}}. We report the network statistics in Table \ref{table_web}. 
The mobility network data is freely available to researchers, non-profit organizations, and governments through the SafeGraph COVID-19 Data Consortium.\footnote{\url{https://www.safegraph.com/covid-19-data-consortium}}
The construction of mobility networks requires the following data sources:
(i) Mobility patterns from the Monthly Pattern\footnote{\url{https://docs.safegraph.com/docs/monthly-patterns}} and Weekly Pattern datasets, \footnote{\url{https://docs.safegraph.com/docs/weekly-patterns}}
(ii) The geometry dataset,\footnote{\url{https://docs.safegraph.com/docs/geometry-data}},
(iii) The Open Census Dataset\footnote{\url{https://docs.safegraph.com/docs/open-census-data}}, and
(iv) The New York Times COVID-19 data.\footnote{\url{https://github.com/nytimes/covid-19-data}} 

\begin{table}[t!]
\caption{\textbf{Left:} Dataset statistics for three weighted graphs. \textbf{Right:} Dataset statistics for four time-varying networks. Each time-varying network sequence has ten networks.}\label{table_web}
\begin{minipage}[b]{0.43\textwidth}
\centering
{\footnotesize\begin{tabular}{@{}lccc@{}}
        \toprule
                & Airport & Advogato & Bitcoin       \\ \midrule
            Nodes     & 7,977 & 6,541     & 3,783                 \\
            Edges     & 30,501 & 51,127    & 24,186    \\
            Avg. edge weight & 1.45 & 0.83 & 1.46      \\ \bottomrule
\end{tabular}}
\end{minipage}
\begin{minipage}[b]{0.55\textwidth}
\centering
{\footnotesize\begin{tabular}{@{}lcccc@{}}
        \toprule
                 & Bitcoin-Alpha & Bitcoin-OTC & Chicago & Houston  \\\midrule
Nodes            & 3,783 & 5,881 & 32,390 & 38,895                \\
Edges            & 24,186 & 35,591 & 975,569  & 1,586,683  \\
Avg. edge weight & 1.46 & 1.51 & 4.27 & 4.42   \\ \bottomrule
\end{tabular}}
\end{minipage}
\end{table}



