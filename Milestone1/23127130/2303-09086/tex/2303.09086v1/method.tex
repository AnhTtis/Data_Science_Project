\section{Spectral Optimization with Frank-Wolfe}\label{sec_alg}

We present a new algorithm to optimize problem \eqref{eq_convex}.
We observe that the gradient of $f(M)$ is equal to the edge centrality scores.
Then, we develop an iterative algorithm with an efficient inner loop that reduces edges with the highest edge centrality.
Lastly, we extend our algorithm to time-varying networks. %


\begin{figure*}[!ht]
	\begin{subfigure}[b]{0.33\textwidth}
		\centering
		\includegraphics[width=0.98\textwidth]{figures/compare_static_network_1percent-eps-converted-to.pdf}
		\caption{\footnotesize Budget is 1\% of total edge weights.}
		\label{fig_static_comparison1}
	\end{subfigure}\hfill%
	\begin{subfigure}[b]{0.33\textwidth}
		\centering
		\includegraphics[width=0.98\textwidth]{figures/compare_static_network_20percent-eps-converted-to.pdf}
		\caption{\footnotesize Budget is 20\% of total edge weights.}
		\label{fig_static_comparison20}
	\end{subfigure}\hfill%
	\begin{subfigure}[b]{0.33\textwidth}
		\centering
		\includegraphics[width=0.98\textwidth]{figures/degrees_ccdf-eps-converted-to.pdf}
		\caption{\footnotesize CCDF of nodes.}
		\label{fig_poi_deg}
	\end{subfigure}
	\caption{Comparison of greedy selection and uniform edge-weight reduction on a mobility network.
	Top-K-EC is more effective in reducing the infected proportion throughout the SEIR model simulation.
	Moreover, the groups and the points of interest in the graph follow heavy-tailed degree distributions, supporting our selection using edge centrality scores.}
	\label{fig_sec31}
\end{figure*}

\vspace{-0.04in}
\subsection{Edge centrality as gradient}\label{sec_edge_centrality}

To motivate our approach, we begin by reviewing the approach of \citet{tong2012gelling}, which introduces edge centrality to reduce $f(W)$ for the case of $r = 1$.
The edge centrality score is defined as the product of the eigenvector scores from both ends of an edge.
Let $X$ be any matrix. Let $\vec u_1$ and $\vec v_1$ be the left and right singular vector of $X$, corresponding to $\lambda_1(X)$.
Then, for any edge $(i, j) \in \cE$, its edge centrality score is given by $\vec u_1(i) \cdot \vec v_1(j)$, where $\vec u_1(i)$ denotes the $i$-th coordinate of $\vec u_1$ and $\vec v_1(j)$ denotes the $j$-th coordinate of $\vec v_1$.

The edge-weight reduction can be viewed as a continuous relaxation of edge removal since the weight of an edge can be reduced by a fraction.
Interestingly, we show that the edge centrality scores are equal to the gradient of $\lambda_1(XX^{\top})$ concerning the edge weights up to scaling.
As a result, we generalize edge centrality scores as the gradient of the largest $r$ singular values of $X$.


\begin{lemma}\label{prop_grad}%
    Assume that the singular values of $X$ are all distinct.
    Then, for any $1\le i, j \le n$, the partial derivative of  $(\lambda_1(X))^2$ with respect to $X_{i,j}$ satisfies
    {\small \begin{align}
        \frac{\partial \big((\lambda_1(X))^2\big)}{\partial X_{i, j}} = 2 \lambda_1(X) \cdot \vec u_1(i) \cdot \vec v_1(j). \label{eq_edge_cen_1}
    \end{align}}%
    More generally, for any $r = 1,2,\dots, n$, we have
    {\small
    \begin{align}
        \frac{\partial \big(\sum_{k=1}^r (\lambda_k(X))^2 \big)}{\partial X_{i,j}} = 2 \sum_{k=1}^r \lambda_k(X) \cdot \vec u_k(i) \cdot \vec v_k(j). \label{eq_edge_cen_r}
    \end{align}}
\end{lemma}




Above, $\vec u_k$ and $\vec v_k$ are the left and right singular vectors of $X$ corresponding to $\lambda_k(X)$, and the indices correspond to entries of the vectors.
The proof of Lemma \ref{prop_grad} is presented in Appendix \ref{sec_proof}.
Given a weight matrix $W$ of a network, we compute the edge centrality scores via the best rank-$r$ approximation of $W$ as $\tilde W_r$. %
Let $\big(\tilde W_r\big)_{i, j}$ be the edge centrality score of edge $(i, j) \in \cE$.
We validate that removing edges via top edge centrality scores effectively reduces infections.
Figure \ref{fig_sec31} shows the benefit compared with uniform reduction.



\subsection{Global optimization via iterative greedy}

We now develop the {Frank-Wolfe edge centrality} minimization algorithm, or Frank-Wolfe-EC, specified in Algorithm \ref{alg_edgecen}.
The high-level idea is iteratively applying a greedy selection of edges with the highest generalized edge centrality scores while recomputing the scores:
\begin{itemize}[leftmargin=0.15in]
\setlength\itemsep{0.0em}
\item \textbf{Input:} The primary inputs are graph $\cG$ with weight matrix $W$, an arbitrary budgeted reduction amount $B$, and an arbitrary rank $r \le n$.
\item \textbf{Output:} An $n$ by $n$ weight matrix $M$ with reduced edge weights from $W$.
\end{itemize}

\noindent\textbf{Derivation of the algorithm:} At every iteration $t$ from $1$ to $T$, let $M_t$ be the currently modified weight matrix.
Let $\nabla f(M_t)$ be the gradient of $f(M_t)$.
The Frank-Wolfe algorithm \cite{frank1956algorithm,nocedal2006numerical} computes a descent direction of $M_t$ by minimizing the correlation between the gradient and the iterate subject to the same constraints as problem \eqref{eq_convex}:
{\small
\begin{align}
    G_t^{\star} \leftarrow \arg\min_{X}~~&\quad \inner{X}{\nabla f(M_t)} = 
    \bigtr{\nabla f(M_t)^{\top} X} \label{eq_g_t} \\
    \mbox{s.t.}                 \quad& \sum_{(i,j)\in \cE} \Big(W_{i, j} - X_{i,j}\Big) \le B  \nonumber \\
                                ~~&\quad 0 \le X_{i, j} \le W_{i, j},\,  \forall (i, j)\in \cE, \nonumber \\
                                ~~&\quad X_{i,j}=0,\quad\quad\quad\,\,\, \forall (i, j)\notin \cE. \nonumber
\end{align}}%

The core of our approach is to prove that the optimal descent direction for problem \eqref{eq_g_t} is essentially by removing edges via top edge centrality scores.
Let $X$ be the best rank-$r$ approximation of $M_t$.
Let ${(i_1, j_1), (i_2, j_2), \dots, (i_{m}, j_{m})}$ be the edges in descending order of their generalized edge centrality scores, where $m$ is the number of edges in the graph.
Consider the first $k$ edges whose total weight exceeds the reduction budget $B$.
Then, the weight of the first $k-1$ edges is reduced to zero.
The weight of the last edge decreases with the remaining budget.

Let us call this procedure Top-K-EdgeCentrality (cf. Alg. \ref{alg_edgecen}).
The following result proves that this greedy procedure yields an optimal solution to problem \eqref{eq_g_t}!

\begin{lemma}\label{thm_optimal_descent}
    The optimal solution  $G_t^{\star}$ (cf. \ref{eq_g_t}) is equal to the output of Top-K-EdgeCentrality($W, B; M_t$).
\end{lemma}

\begin{proof}
    By Lemma \ref{prop_grad}, for every edge $(i, j) \in \cE$, the gradient of $f(M_t)$ over this edge is given by the \textit{generalized edge centrality} scores.
    Since $X_{i, j} = 0$ for any $(i, j) \notin \cE$, the optimization objective is:
    {\small \begin{align}
       \inner{X}{\nabla f(M)} = \sum_{(i, j) \in \cE}  2 X_{i, j} \Big( \sum_{k=1}^r  \lambda_k \cdot \vec  u_k(i) \cdot \vec v_k(j) \Big). \label{eq_obj_linear}
    \end{align}}%
    Above, each variable $X_{i,j}$ is multiplied precisely by the generalized edge centrality of the edge $(i, j)$ (cf. line \eqref{alg_ec}).
    Consider minimizing the equivalent objective \eqref{eq_obj_linear} with the constraints of Problem \eqref{eq_g_t}.
    The minimizer, $G_t^{\star}$, is achieved by reducing the weight of the edges with the highest edge centrality to zero until the budget $B$ gets exhausted.
    This is precisely the procedure of Top-K-EC from lines \eqref{alg_ec}-\eqref{alg_ec_red}.
    Thus, we have proved this result.
\end{proof}

After finding the descent direction $G_t^{\star}$, the next step of the Frank-Wolfe algorithm is setting a learning rate $\eta_t$ in a range between $0$ and $1$.
This follows standard procedures from the Frank-Wolfe algorithm \cite{nocedal2006numerical}.
See Algorithm \ref{alg_edgecen} for the complete pseudo-code.

\medskip
\noindent\textbf{Running time analysis:}
Next, we examine the number of iterations needed for Alg. \ref{alg_edgecen} to converge to the global optimum of problem \eqref{eq_convex}.
A well-established result is that the Frank-Wolfe algorithm will converge to the global minimum for convex objectives under mild conditions \cite{nocedal2006numerical}.
Note that objective \eqref{eq_convex} is indeed convex.
Therefore, our algorithm will provably converge to the global minimum of problem \eqref{eq_convex}, denoted as $f^{\textup{OPT}}$.

\begin{theorem}\label{prop_continuous}
    Let $\kappa$ be the minimum of $\lambda_r({M_t}) - \lambda_{r+1}({M_{t}})$ over $t = 0, 1,\dots, T-1$.
    Assume that $\kappa$ is strictly positive.
    Then, the following holds for $M_T$:
    {\small \begin{align}
        f(M_T) - f^{\textup{OPT}} \le \frac{40\Big(\sum_{(i,j)\in\cE} W_{i,j}^2\Big)\alpha_2}{T},\label{eq_converge}
    \end{align}}%
    where $\alpha_2 = \kappa^{-1}{r}^{1/2}\big(\max_{t=1}^T \lambda_1(M_t) \big) + r + C$, for a fixed value $C > 0$.
\end{theorem}

The convergence rate of $O(T^{-1})$ in statement \eqref{eq_converge} is obtained following recent literature (e.g., \cite{jaggi2013revisiting}).
This result guarantees that our algorithm will converge to the global minimum solution under mild conditions.
See Appendix \ref{sec_proof} for the proof.
The constants inherited from the previous guarantee in statement \eqref{eq_converge} can be quite large. However, in our experiments, we observe that less than $30$ iterations are sufficient for the algorithm to converge (at the global optimum).

\medskip
To recap, the running time of our algorithm is $T$ times the running time of each iteration, including:
\begin{itemize}[leftmargin=0.15in]
\setlength\itemsep{0.0em}
    \item Computing a truncated rank-$r$ SVD of a sparse matrix with $m$ nonzeros;
    this requires a time complexity of $O(m r \log (m))$  \cite{musco2015randomized}.
    \item Sorting an array of size $m$; this requires $O(m\log(m))$ time complexity.
\end{itemize}
By comparison, running a linear program solver for problem \eqref{eq_g_t} requires at least $O(m n)$ time complexity \cite{nocedal2006numerical}.
Thus, our approach is most efficient for small $r$.




\subsection{Optimization on time-varying networks}\label{sec_tv}
Our study has focused on mitigating the spread in a static network.
Another consideration is that the network topology evolves over time.
Therefore, an important question is how to tackle such temporal evolution.
Next, we show how to extend our optimization algorithm to time-varying networks.



\medskip
\noindent\textbf{Derivation of the algorithm:} 
Let the weight matrices of a sequence of graphs be denoted as $\cW = \set{W^{(1)}, W^{(2)}, \dots, W^{(s)}}$.
Motivated by the work of \citet{prakash2010virus} which shows the epidemic threshold of time-varying networks, we extend the eigenvalue minimization problem on time-varying networks.
Let $\cM = \set{M^{(1)}, M^{(2)}, \dots, M^{(s)}}$ be a sequence of modified weight matrices.
We aim to find $\cM$ that shrinks the largest eigenvalues of a product matrix:
{\small
\begin{align}
     \min_{\cM}~~&\quad f\big(\cM\big) = \sum_{k=1}^r \Big(\lambda_k\Big(\prod_{t=1}^s M^{(t)}\Big)\Big)^2 \label{eq_convex_temporal} \\
    \mbox{s.t.}              
    ~~&\,\, \sum_{t=1}^s \sum_{(i,j)\in \cE^{(t)}} \big(W_{i, j}^{(t)} - M_{i,j}^{(t)}\big) \le B \nonumber \\
    \quad&\quad 0 \le M_{i, j}^{(t)} \le W_{i, j}^{(t)},\, \forall (i, j)\in \cE^{(t)}, t = 1, \ldots, s,  \nonumber \\
    \quad~~&\quad M_{i,j}^{(t)}=0, \quad\quad\quad\,\,\, \forall (i, j)\notin \cE^{(t)}, t = 1, \ldots, s. \nonumber
\end{align}}%
Above, $\cE^{(t)}$ represents the set of edges in the $t$-th graph of the sequence.
Based on \citet[Theorem 2]{prakash2010virus}, the weight matrix that determines the epidemic threshold process in time-varying networks is the joint product of each weight matrix in the sequence:
$X = \prod_{t=1}^s M^{(t)}$. 
This is why we minimize the largest eigenvalues of the product matrix in $f(\cM)$.

Following Lemma \ref{prop_grad}, we derive the gradient of the largest $r$ eigenvalues of $X^{\top}X$ with respect to $M^{(t)}_{i, j}$, for any $1\le i,j\le n$.
By the chain rule, we have:%
{\small\begin{align}
     \frac{\partial f(\cM)}{\partial M_{i,j}^{(t)}} 
    = \Big\langle{\frac{\partial\Big(\sum_{k=1}^r \big(\lambda_k(X)\big)^2 \Big)}{\partial X}}, {\frac{\partial X}{\partial M_{i,j}^{(t)}}}\Big\rangle. \label{eq_tv}
\end{align}}%
Notice that the first term above on the right is precisely the edge centrality scores we have derived in Lemma \ref{prop_grad}. %
The second term is the product of the rest of the weight matrices in $\cW$ except that $M^{(t)}$ is replaced by an indicator matrix, which is the derivative of $M^{(t)}$ with respect to its $(i, j)$-th entry.
Let $\tilde X_r = U_r D_r V_r^{\top}$ be the rank-$r$ SVD of $X$.
We get (cf. Appendix \ref{sec_proof}):
{\small\begin{align}
    \frac{\partial f(\cM)}{\partial M^{(t)}}
    =2\Big(\prod\nolimits_{k=1}^{t-1} M^{(k)}\Big)^{\top} \tilde{X}_r \Big(\prod\nolimits_{k=t+1}^s M^{(k)}\Big)^{\top}. \label{eq_tv_ec}
\end{align}}%
Matrix \eqref{eq_tv_ec} encodes the edge centrality scores for every edge of $\cE^{(t)}$, at any step $t$.
Thus, we can develop an algorithm for time-varying networks as the static case.
The complete procedure is described in Algorithm \ref{alg_edgecen_temporal}.

\begin{algorithm}[!t]
\caption{Frank-Wolfe for Static Networks}\label{alg_edgecen}
	\begin{footnotesize}
		\begin{algorithmic}[1] %
		    \Input A graph $\cG = (\cV, \cE)$ with weight matrix $W$; Budget $B$.
		    \Param Rank $r$; Iterations $T$; Range of learning rate $H$.
		    \Output A weight matrix $M$ modified from $W$.
			\Procedure{\LP}{$W, B; T, H$}
			    \State Let $M_0 = W$
			    \For {$t = 0, 1, \dots, T-1$}
			     %
			     %
			     %
			        \State $G_t^{\star}$ = \edgecen($W, B; M_t$)
			        \State Set $\eta_t$ by minimizing $f\big((1 - \eta_t) M_t + \eta_t G_t^{\star}\big)$ for $\eta_t \in H$
			        \State $M_{t+1} = (1 - \eta_t) M_t + \eta_t  G_t^{\star}$
			    \EndFor
			    \If {there is unused budget in $M_T$}
			        \State $B' = B - \textup{sum}(W - M_T)$
			        \State $M^{\star}$ = \edgecen($M_T, B'; M_T$)
			    \EndIf
			    \State \Return $M^{\star}$
			\EndProcedure
			\vspace{0.1in}
			\Procedure{\edgecen}{$W, B; M$}
			    \State Let $\tilde M_r$ be the rank-$r$ SVD of $M$ \label{alg_ec}
			    \State Sort the edges in $\cE$ by their edge centrality scores from $\tilde M_r$; let $k$ be the first value such that the total top-$k$ edge weights in $W$ exceed $B$
			    \State Reduce the first $k-1$ edges' weight to zero and the last edge's weight by the remaining budget \label{alg_ec_red}
			    %
			    \State \Return the updated $W$
			\EndProcedure	
		\end{algorithmic}
	\end{footnotesize}
\end{algorithm}



\medskip
\noindent\textbf{Running time analysis:}
Similar to Theorem \ref{prop_continuous}, one can then prove that Algorithm \ref{alg_edgecen_temporal} is guaranteed to converge to the optimum solution of problem \eqref{eq_convex_temporal} at the rate of $O(T^{-1})$ after $T$ iterations.
The details of this extension can be found in Appendix \ref{sec_proof}.





