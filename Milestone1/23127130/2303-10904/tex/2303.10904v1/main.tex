% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[twocolumn]{article}
\usepackage[accsupp]{axessibility}
%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{algorithm}
\usepackage{algorithmic}

% \usepackage{tikz}
\usepackage{comment}
\usepackage{amssymb} % define this before the line numbering.

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{multicol}
\usepackage{wrapfig}
% \usepackage{amsthm}
% \usepackage{subfigure}
\usepackage[english]{babel}
\usepackage{bm}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{amsmath}
\usepackage[switch]{lineno}
\usepackage{bbm}

\usepackage{soul}
\usepackage{color}
\usepackage{xcolor}
\usepackage{colortbl} 

\newcommand{\wh}[1]{\textcolor{black}{#1}}
\newcommand{\zjh}[1]{\textcolor{black}{#1}}
\newcommand{\whhl}[2]{\hl{#1} \textcolor{black}{(WH: #2)}}
\newcommand{\lin}[1]{\textcolor{black}{#1}}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{6459} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based Action Recognition}

\author{Lilang Lin, Jiahang Zhang, Jiaying Liu\thanks{Corresponding author. This work is supported by the National Natural Science Foundation of China under contract No.62172020.}\\
Wangxuan Institute of Computer Technology, Peking University, Beijing, China\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
% \begin{abstract}
%    The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, below the author and affiliation information.
%    Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized.
%    The abstract is to be in 10-point, single-spaced type.
%    Leave two blank lines after the Abstract, then begin the main text.
%    Look at previous CVPR abstracts to get a feel for style and length.
% \end{abstract}

\begin{abstract}
The self-supervised pretraining paradigm has achieved great success in skeleton-based action recognition. However, these methods treat the motion and static parts equally, and lack an adaptive design for different parts, which has a negative impact on the accuracy of action recognition. To realize the adaptive action modeling of both parts, we propose an \textbf{Act}ionlet-Dependent \textbf{C}ontrastive \textbf{L}ea\textbf{r}ning method (ActCLR). The actionlet, defined as the discriminative subset of the human skeleton, effectively decomposes motion regions for better action modeling. In detail, by contrasting with the static anchor without motion, we extract the motion region of the skeleton data, which serves as the actionlet, in an unsupervised manner. Then, centering on actionlet, a motion-adaptive data transformation method is built. Different data transformations are applied to actionlet and non-actionlet regions to introduce more diversity while maintaining their own characteristics. Meanwhile, we propose a semantic-aware feature pooling method to build feature representations among motion and static regions in a distinguished manner. Extensive experiments on NTU RGB+D and PKUMMD show that the proposed method achieves remarkable action recognition performance. More visualization and quantitative experiments demonstrate the effectiveness of our method. Our project website is available at \url{https://langlandslin.github.io/projects/ActCLR/}

% The pretraining paradigm has achieved great success. However, these methods treat each part of the skeleton data equally. Thus, the information between motion and non-motion parts is coupled. It has led to the current dilemma of self-supervised action recognition.
%
% \wh{However, as the pretraining stage has to include a diversity constraint of the extracted features to prevent model collapse, it is difficult to further improve the performance of downstream tasks that heavily rely on the semantic consistency of features extracted from different views of the same data.}
% However, to prevent model collapse, the pretraining stage has to include a diversity constraint of the extracted features. The diversity constraint makes it difficult to further improve the performance of downstream tasks, which heavily relies on the semantic consistency of features extracted from different views of the same data.
%
% Therefore, we propose an actionlet-dependent contrastive learning method (ActCLR). This approach decouples the motion and non-motion regions to obtain better action modeling. By contrasting with the static anchor without motion, we unsupervised extract the motion region of the skeleton data, i.e., actionlet. Using actionlet, we design a motion-aware data transformation method. Different data transformations are applied to actionlet and non-actionlet regions to introduce more diversity while maintaining action semantic information.
% Meanwhile, we propose an actionlet-based feature pooling method that decouples the information of actionlet and non-actionlet regions. It makes the motion feature without the effect of the motionless regions. During training, a regularization constraint based on entropy minimization is added to the loss. We reduce the entropy of the feature space to obtain more confident output features.
%
% The feature space obtained by pretraining is \wh{augmented} through a self-distillation stage. 
% %
% \wh{In self-distillation, we optimize without the diversity constraint and reduce the cross entropy between the features of the teacher and the student network, which improves the semantic consistency on the features extracted by the student network.}
% %
% Besides, two new metrics are proposed to quantitatively evaluate the consistency and diversity of the feature space.
% %
% These metrics show that our method improves the \wh{semantic} consistency of the feature space of the previously pretrained network while maintaining \wh{feature} diversity.
% %
% In the finetuning stage, a gradient-based parameter-efficient finetuning method is applied \wh{to only optimize the important model parameters,} obtaining more satisfactory performance with fewer trainable parameters.
%
% Extensive experiments on NTU RGB+D and PKUMMD show that the proposed method achieves remarkable action recognition performance. Meanwhile, more visualization and quantitative experiments demonstrate the effectiveness of our method.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Skeletons represent human joints using 3D coordinate locations. 
%
Compared with RGB videos and depth data, skeletons are lightweight, \wh{privacy-preserving}, and compact to represent human motion. 
%
On account of being easier and more discriminative for analysis, skeletons have been widely used in action recognition task~\cite{zhang2020context,liu2020disentangling,zhang2020semantics,song2020stronger,peng2020learning,su2020predict}.

\begin{figure}[tb]
\includegraphics[width=\linewidth]{fig/teaser.pdf}
\caption{
% 
Our proposed approach (ActCLR) locates the motion regions as actionlet to guide contrastive learning.
}
\label{fig:teaser}
\end{figure}

Supervised skeleton-based action recognition methods~\cite{si2019attention,shi2019two,chen2021channel} \wh{have achieved impressive performance}.
%
However, \wh{their success highly depends} on \wh{a large amount of} labeled training data, which \wh{is} expensive to obtain. 
%
To get rid of the reliance on full supervision, self-supervised learning~\cite{zheng2018unsupervised,lin2020ms2l,su2020predict,thoker2021skeleton} has been introduced into skeleton-based action recognition. 
%
It adopts a two-stage paradigm, \wh{\textit{i.e.}} first \wh{applying} pretext tasks for unsupervised pretraining and then \wh{employing} downstream tasks for finetuning.

\wh{According to learning paradigms, all} methods can be classified into two categories: reconstruction-based~\cite{su2020predict,yang2021skeleton,kim2022global} and contrastive \wh{learning-based}.
%
Reconstruction-based methods \wh{capture} the \wh{spatial-temporal} correlation by predicting masked skeleton data. 
%
Zheng \wh{\textit{et al.}}~\cite{zheng2018unsupervised} first proposed \wh{reconstructing} masked skeletons for \wh{long-term} global motion dynamics.
%
 Besides, the contrastive learning-based methods have shown remarkable potential recently. These methods employ skeleton transformation to generate positive/negative samples.
%
Rao \wh{\textit{et al.}}~\cite{rao2021augmented} applied Shear and Crop as data augmentation.
%
Guo \wh{\textit{et al.}}~\cite{guo2021contrastive} further proposed to use more augmentations, \wh{\textit{i.e.}} rotation, masking\wh{,} and flippling, to improve the consistency of contrastive learning.

%
\wh{These} contrastive learning works treat different regions of the skeleton sequences \wh{uniformly}.
\wh{However, the motion regions contain richer action information and contribute more to action modeling.}
%
Therefore, it is sub-optimal to directly apply data transformations to all regions in the previous works, \wh{which may degrade the motion-correlated information too much}. 
For example, if the mask transformation is applied to the hand joints in the hand raising action, the motion information of the hand raising is \wh{totally} impaired.
%
It will give rise to the false positive problem, \textit{i.e.}, the semantic inconsistency due to the information loss between positive pairs.
Thus, \wh{it is necessary to adopt a distinguishable design} for motion and static regions in the data sequences.
%
% And the reconstruction task reconstructs random areas. 
% %
% But these regions may not be the region where the motion occurs. 
% %
% Therefore, it encodes a lot of motion-independent information in the model.
%
% Thus, a sequence of actions exists motion-correlated regions and static regions. It is not reasonable to treat these regions equally.

To tackle these problems, we propose a new actionlet-dependent contrastive learning method (ActCLR) by \wh{treating} motion and static regions \wh{differently}, as shown in Fig.~\ref{fig:teaser}.
An \textit{actionlet}~\cite{wang2012mining} is \wh{defined as} a conjunctive structure of skeleton joints. \wh{It is expected to be highly representative of one action and highly discriminative to distinguish the action from others.}
%
\wh{The actionlet in previous works is defined in a supervised way, which relies on action labels and has a gap with the self-supervised pretext tasks.}
%
\wh{To this end, in the unsupervised learning context, we propose to obtain actionlet by comparing the action sequence with the average motion to guide contrastive learning. In detail,} the average motion is \wh{defined as} the average of all the series in the dataset.
%
Therefore, this average motion is employed as the static anchor without motion. 
We contrast the action sequence with the average motion to get the area with the largest difference. 
This region is considered \wh{to be} the region where the motion \wh{takes place}, \textit{i.e.}, actionlet.


Based on this actionlet, we design \wh{a} motion-adaptive transformation strategy. 
The actionlet region is transformed by performing the proposed semantically \wh{preserving} data transformation.
%
\wh{Specifically,} we \wh{only} apply \wh{stronger data} transformations to non-actionlet \wh{regions}. \wh{With less interference in the motion regions, this} motion-adaptive transformation strategy makes \wh{the model learn} better semantic consistency and obtain stronger generalization performance.
%
\wh{Similarly,} we utilize a semantic-aware feature pooling method\wh{.} By extracting the features in the actionlet region, \wh{the features can be more representative of the motion without the interference of the semantics in static regions.}
%
% Finally, we utilize a regularization constraint based on entropy minimization for training. The entropy of the feature space is reduced during the optimization process to obtain a more \wh{definitive} output.


%
We provide thorough experiments and detailed analysis on NTU RGB+D~\cite{shahroudy2016ntu,liu2019ntu} and PKUMMD~\cite{liu2020pku} datasets to prove the superiority of our method. 
%
Compared to the state-of-the-art methods, our model achieves remarkable results with self-supervised learning.


In summary, our contributions \wh{are summarized as follows}:
\begin{itemize}%[leftmargin=*]
\setlength\itemsep{.3em}
\item We propose a novel unsupervised actionlet-based contrastive learning method. Unsupervised actionlets are mined as skeletal regions that are the most discriminative compared with the static anchor, \textit{i.e.}, the average motion of \wh{all} training data.
%
\item A motion-adaptive transformation strategy is designed for contrastive learning. In the actionlet region, we employ semantics-preserving data transformations to learn semantic consistency. And in non-actionlet regions, we apply \wh{stronger} data transformations to obtain stronger generalization performance.
%
\item We utilize \wh{semantic-aware} feature pooling to extract motion features of the actionlet regions. It makes features to be more focused on motion \wh{joints} without being distracted by motionless joints.
\end{itemize}

\section{Related Work}
In this section, we first introduce the related work of skeleton-based action recognition, and then briefly review contrastive learning.

\subsection{Skeleton-Based Action Recognition}
Skeleton-based action recognition is a fundamental yet challenging field in computer vision research. Previous skeleton-based motion recognition methods are usually realized with the geometric relationship of skeleton joints~\cite{vemulapalli2014human,vemulapalli2016rolling,goutsu2015motion}. The latest methods pay more attention to deep networks. Du \etal~\cite{du2015hierarchical} applied a hierarchical RNN to process body keypoints. 
% To learn the mappings between co-occurrence of joints and the human action, \wh{in} \cite{zhu2015co}, a model utilizing co-occurring joints \wh{is designed} as a strong discriminative feature and using a connection matrix. 
Attention-based methods are proposed to automatically select important skeleton joints~\cite{song2017end,zhang2018adding,song2018spatio,si2019attention} and video frames~\cite{song2017end,song2018spatio} to learn more adaptively about the simultaneous appearance of skeleton joints. However, recurrent neural networks often suffer from gradient vanishing~\cite{hochreiter2001gradient}, which may cause optimization problems. Recently, graph convolution networks attract more attention for skeleton-based action recognition. To extract both the spatial and temporal structural features from skeleton data, Yan \etal~\cite{yan2018spatial} proposed spatial-temporal graph convolution networks. To make the graphic representation more flexible, the attention mechanisms are applied in~\cite{si2019attention,shi2019two,chen2021channel} to adaptively capture discriminative features based on spatial composition and temporal dynamics.



% These supervised models have achieved excellent performance on skeleton-based action recognition. However, they rely heavily on massive data with annotated action labels. To relieve the data limitation, self-supervised methods are developed, which only require a significant number of unlabeled samples. Our work is also inspired to apply self-supervised learning for action recognition.

\subsection{Contrastive Learning}
\label{sec:contra}
Contrastive representation learning can date back to \cite{hadsell2006dimensionality}. The following approaches~\cite{tian2019contrastive,wu2018unsupervised,bachman2019learning,ye2019unsupervised,isola2015learning} learn representations by contrasting positive pairs against negative pairs to make the representations between positive pairs more similar than those between negative pairs. Researchers mainly focus on how to construct pairs to learn robust representations. SimCLR proposed by Chen \etal~\cite{chen2020simple} uses a series of data augmentation methods, such as random cropping, Gaussian blur and color distortion to generate positive samples. He \etal~\cite{he2020momentum} applied a memory module that adopts a queue to store negative samples, and the queue is constantly updated with training. In self-supervised skeleton-based action recognition, contrastive learning has also attracted the attention of numerous researchers. Rao \etal~\cite{rao2021augmented} applied MoCo for contrastive learning with a single stream. To utilize cross-stream knowledge, Li \etal~\cite{li20213d} proposed a multi-view contrastive learning method and Thoker \etal~\cite{thoker2021skeleton} employed multiple models to learn from different skeleton representations. Guo \etal~\cite{guo2021contrastive} proposed to use more extreme augmentations, which greatly improve the effect of contrastive learning. Su \etal~\cite{su2021self} proposed novel representation learning by perceiving motion consistency and continuity. 
Following MoCo v2~\cite{he2020momentum}, they exploit InfoNCE loss to optimize contrastive learning:
\begin{equation}
\label{equ:info}
\begin{aligned}
\mathcal{L}_{\text{CL}} = - \log \frac{\exp(\text{sim}(\mathbf{z}^i_q, \mathbf{z}^i_k) / \tau)}{\exp(\text{sim}(\mathbf{z}^i_q, \mathbf{z}^i_k)/ \tau) + K},
\end{aligned}
\end{equation}
where $\mathbf{z}^i_q = g_q(f_q(\mathbf{X}^i_q))$ and $\mathbf{z}^i_k = g_k(f_k(\mathbf{X}^i_k))$. $K = \sum_{j=1}^M \exp(\text{sim}(\mathbf{z}^i_q, \mathbf{m}^j)/ \tau)$ and $\tau$ is a temperature hyper-parameter. $f_q(\cdot)$ is an online encoder and $f_k(\cdot)$ is an offline encoder. $g_q(\cdot)$ is an online projector and $g_k(\cdot)$ is an offline projector. The offline encoder $f_k(\cdot)$ is updated by the momentum of the online encoder $f_q(\cdot)$ by $f_k \leftarrow \alpha f_k + (1 - \alpha) f_q$, where $\alpha$ is a momentum coefficient. $\mathbf{m}^j$ is the negative sample, stored in memory bank $\mathbf{M}$. $\text{sim}(\cdot,\cdot)$ is the cosine similarity.

\section{Actionlet-Based Unsupervised Learning}
In this section, we introduce unsupervised actionlet for contrastive representation learning, which is based on MoCo v2 described in Sec.~\ref{sec:contra}.
First, we describe the unsupervised actionlet extraction method. Then, the motion-adaptive data transformation and the semantic-aware feature pooling are introduced.

% \input{cvpr2023-author_kit-v1_1-1/latex/fig/actionlet.tex}

% we employ the unsupervised actionlet selection module to generate actionlet regions. And the data is enhanced with the motion-adaptive data transformation module. In contrastive learning, the offline network $f_k(\cdot)$ is to provide a stable and accurate anchor feature for the online network $f_q(\cdot)$. Thus, in feature pooling, we utilize the semantic-aware feature pooling method to generate offline features. 

% The feature extraction can be expressed as follows:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathbf{z}^i_q &= (g_q \circ \text{GAP} \circ f_q \circ \text{ADT})(\mathbf{X}^i_q),\\
% \mathbf{z}^i_k &= (g_k \circ \text{AFP} \circ f_k)(\mathbf{X}^i_k).
% \end{aligned}
% \end{equation}
% where $\mathbf{z}^i_q = (g_q \circ \text{GAP} \circ f_q \circ \text{ADT})(\mathbf{X}^i_q)$ and $\mathbf{z}^i_k = (g_k \circ \text{AFP} \circ f_k)(\mathbf{X}^i_k)$.

% We exploit cross-entropy loss to reduce the entropy of the feature space to constrain the features.:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{L}_{\text{KL}}(\mathbf{p}_q^i, \mathbf{p}_k^i) = -\mathbf{p}_k^i \log \mathbf{p}_q^i,
% \end{aligned}
% \end{equation}
% where $\mathbf{p}_q^i = \text{SoftMax}(\text{Linear}(\mathbf{z}^i_q)/\tau_q)$ and $\mathbf{p}_k^i = \text{SoftMax}(\text{Linear}(\mathbf{z}^i_k)/\tau_k)$. $\text{Linear}(\cdot)$ is a linear projection. Since $\tau_k$ is much smaller than $\tau_q$, the entropy of $\mathbf{p}_k^i$ is smaller than the entropy of $\mathbf{p}_q^i$. By using $\mathbf{p}_k^i$ as a guide, the entropy of $\mathbf{p}_q^i$ can be reduced. In turn, the feature space is subject to an entropy constraint.

% The total loss function can be expressed as:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{L} = \mathcal{L}_{\text{CL}} + \lambda \mathcal{L}_{\text{KL}},
% \end{aligned}
% \end{equation}
% where $\lambda$ is a hyperparameter.

%
% Following ~\cite{li20213d,rao2021augmented}, we employ \textit{MoCo} as our backbone for skeleton data:
% This algorithm adopts momentum mechanism~\cite{he2020momentum}. There are three branches, optimized based on \textit{MoCo}. And joint, motion and bone data are input respectively to assist in distinguishing positive pairs from similar negative samples. We introduce the steps of \textit{MoCo} in detail:
% \begin{itemize}
%     \item[1)] The network inputs the original data $\mathbf{X}^i$ and performs data transformation $\mathcal{T}$ to obtain two different views $\mathbf{X}^i_q$ and $\mathbf{X}^i_k$.
%     \item[2)] Two encoders are employed, an online encoder $f_q(\cdot)$ and an offline encoder $f_k(\cdot)$ to extract features: $\mathbf{z}^i_q = g_q(f_q(\mathbf{X}^i_q))$ and $\mathbf{z}^i_k = g_k(f_k(\mathbf{X}^i_k))$, where $g_q(\cdot)$ is an online projector and $g_k(\cdot)$ is an offline projector. The offline networks $f_k(\cdot)$ and $g_k(\cdot)$ are updated by the momentum of the online networks $f_q(\cdot)$ and $g_q(\cdot)$. by $\hat{f} \leftarrow \alpha \hat{f} + (1 - \alpha) f$, where $\alpha$ is a momentum coefficient. 
%     \item[3)]  A memory bank $\mathbf{M} = \{\mathbf{m}^i\}^M_{i=1}$ is utilized to store negative features. The features extracted from the data in each batch are stored in the memory bank, and the bank is continuously updated using a first-in first-out strategy.
%     \item[4)] Following recent works~\cite{chen2020simple}, \textit{MoCo} exploits InfoNCE loss to optimize:
%     \begin{equation}
%     \label{equ:info}
%     \begin{aligned}
%     \mathcal{L}_{\text{CL}} = - \log \frac{\exp(\text{sim}(\mathbf{z}^i_q, \mathbf{z}^i_k) / \tau)}{\exp(\text{sim}(\mathbf{z}^i_q, \mathbf{z}^i_k)/ \tau) + K},
%     \end{aligned}
%     \end{equation}
%     where $K = \sum_{j=1}^M \exp(\text{sim}(\mathbf{z}^i_q, \mathbf{m}^j)/ \tau)$ and $\tau$ is a temperature hyper-parameter.
% \end{itemize}



% \subsection{Preliminary}
% An \textit{actionlet} is defined as a conjunctive structure of skeleton joints. It should be highly representative of one action and highly discriminative compared to other actions. Therefore, the actionlet is considered as representing the main motion region.

% Formally, supposing the $i^{th}$ skeleton sequence is $\mathbf{X}^i = \{\mathbf{x}^i_1, \dots, \mathbf{x}^i_T\}$, where $\mathbf{x}^i_t \in \mathbb{R}^{C \times V}$ represents the $t^{th}$ frame. $C$ is the number of channels and $V$ is number of joints. An actionlet is denoted as a subset of joints $\mathcal{S} \subseteq \{1,2,\dots,V\}$. In the previous work, the actionlet is selected by supervision. It is related to the action category. For an action category $c$, its actionlet is chosen according to this probability $P_{\mathcal{S}} (\mathbf{y}^i = c | \mathbf{x}^i)$, where $\mathbf{y}^i$ is the label of $\mathbf{x}^i$. This probability should be large for the data in $\mathcal{X}_c$ and be small for the data not in $\mathcal{X}_c$. $\mathcal{X}_c$ is the set of sequences which labels are $c$. 

% The actionlet makes features focus on joints in movement. It allows action information to be highlighted. However, the selection of supervised actionlets is action class dependent. This makes it inapplicable to the field of unsupervised learning. And actionlets are not generated adaptively based on action data. Actionlets are shared between sequences in the same class of skeletons. Consequently, the choice of actionlet will be less accurate. For example, the action of raising a hand may be raising the left hand or the right hand, rather than different people raising the same hand. Additionally, these actionlets ignore the temporal dynamics and focus only on spatial information. However, the motion region of the skeleton data may change over time. Therefore, it is important to design an unsupervised spatio-temporal actionlet selection method. And, using this unsupervised actionlet benefits contrastive learning.

\subsection{Unsupervised Actionlet Selection}
\label{sec:actionlet}
Traditional actionlet mining methods rely on the action label to identify the motion region, which cannot be employed in the unsupervised learning context.
%
Inspired by contrastive learning, we propose an unsupervised spatio-temporal actionlet selection \wh{method} to mine the motion region as shown in Fig.~\ref{fig:overview}.
%
The actionlet is \wh{obtained} by comparing the differences between \wh{an} action sequence and the static sequence where we assume no motion takes place.

Specifically, we introduce the average motion as the static anchor, which is regarded as the sequence without motion. Resort to this, we contrast the action sequences between the static anchor to realize actionlet localization. The details of the proposed method are described below.
\vspace{1mm}

\noindent\textbf{Average Motion as Static Anchor.}
In the process of obtaining the sequence without action occurrence, we observe that most of the action sequences have no action in most of the regions. The motion usually occurs in a small localized area, such as the hand or head.
%
\wh{Therefore, as shown in Fig.~\ref{fig:mean}, we can easily obtain the static anchor via average all the actions in the dataset, since most of the sequence has no motion in most of the regions and this average is a relatively static sequence.} 
It is formalized as:
\begin{equation}
    \label{equ:info}
    \begin{aligned}
    \Bar{\mathbf{X}} = \frac{1}{N}\sum_{i=1}^N (\mathbf{X}^i),
    \end{aligned}
\end{equation}
where $\mathbf{X}^i$ is the $i^{th}$ skeleton sequence and $N$ is the size of the dataset.
\vspace{1mm}
%It can also be seen in Fig.~\ref{fig:mean} that the average motion is a relatively static sequence of motions. In this case, the average motion can be viewed as a static anchor without any movement.

\begin{figure*}[tb]
\begin{center}
\includegraphics[width=\textwidth]{fig/pipeline.pdf}
\end{center}
\caption{
The pipeline of actionlet-dependent contrastive learning. In unsupervised actionlet selection, we employ the difference from the average motion to obtain the region of motion. For contrastive learning, we employ two streams, \textit{i.e.}, the online stream and the offline stream. The above stream is the online stream, which is updated by gradient. The below is the offline stream, which is updated by momentum. We get the augmented data $\mathbf{X}^i_{\text{trans}}$ by performing motion-adaptive data transformation (MATS) on the input data $\mathbf{X}^i_q$ with the obtained actionlet. In offline feature extraction, we employ semantic-aware feature pooling (SAFP) to obtain the accurate feature anchor. Finally, utilizing similarity mining, we increase the similarity between positives and decrease the similarity between negatives.
}
\label{fig:overview}
\end{figure*}

\noindent\textbf{Difference Activation Mapping for Actionlet Localization.}
To obtain the region where the motion \wh{takes place}, we input the skeleton sequence $\mathbf{X}^i$ with the average motion $\Bar{\mathbf{X}}$ into the offline encoder $f_k(\cdot)$ to obtain its corresponding dense features $\mathbf{h}^i_{ctv} = f_k(\mathbf{X}^i)$ and $\Bar{\mathbf{h}}_{ctv} = f_k(\Bar{\mathbf{X}})$, where $c$ means channel dimension, $t$ temporal dimension\wh{,} and $v$ joint dimension.
%
After global average pooling (GAP), we then apply the offline projector $g_k(\cdot)$ to obtain global features $\mathbf{z}^i = g_k(\text{GAP}(\mathbf{h}^i_{ctv}))$ and $\Bar{\mathbf{z}} = g_k(\text{GAP}(\Bar{\mathbf{h}}_{ctv}))$.
%
Then we calculate the cosine similarity of these two features. 
%
This can be formalized as:
\begin{equation}
    \label{equ:cos}
    \begin{aligned}
    \text{sim}(\mathbf{z}^i, \Bar{\mathbf{z}}) = \frac{\langle \mathbf{z}^i, \Bar{\mathbf{z}} \rangle}{\|\mathbf{z}^i\|_2\|\Bar{\mathbf{z}}\|_2},
    \end{aligned}
\end{equation}
where $\langle \cdot, \cdot \rangle$ is the inner product. 

To find the region where this similarity can be reduced, we back-propagate and reverse the gradient of this similarity to the dense feature $\mathbf{h}^i_{ctv}$. 
%
These gradients then are global average pooled over the temporal and joint dimensions to obtain the neuron importance weights $\alpha^i_c$:
\begin{equation}
    \label{equ:sim}
    \begin{aligned}
    \Delta \mathbf{h}^i_{ctv} & = \frac{\partial (-\text{sim}(\mathbf{z}^i, \Bar{\mathbf{z}}))}{\partial \mathbf{h}^i_{ctv}},\\
    \alpha^i_c &= \frac{1}{T \times V} \sum_{t=1}^T \sum_{v=1}^V \sigma (\Delta \mathbf{h}^i_{ctv}),
    \end{aligned}
\end{equation}
where $\sigma(\cdot)$ is the activation function.

These importance weights capture the magnitude of the effect of each channel dimension on the final difference. Therefore, these weights $\alpha^i_c$ are considered difference activation mapping. 
%
We perform a weighted combination of the difference activation mapping and dense features as follows:
\begin{equation}
    \label{equ:wc}
    \begin{aligned}
    \mathbf{A}^i_{tv} = \sigma\left(\sum_{c=1}^C \alpha^i_c \mathbf{h}^i_{ctv}\right) \mathbf{G}_{vv},
    \end{aligned}
\end{equation}
where $\sigma(\cdot)$ is the activation function and $\mathbf{G}_{vv}$ is the adjacency matrix of skeleton data for importance smoothing. The linear combination of maps selects features that have a negative influence on the similarity. The actionlet region is the area where the value of the
generated actionlet $\mathbf{A}^i_{tv}$ exceeds a certain threshold, while the
non-actionlet region is the remaining part.
%
% Therefore, the large value in $\mathbf{H}^i_{tv}$ corresponds to the region with the large difference from the average motion. We select the areas that exceed a certain threshold as actionlets:
% \begin{equation}
%     \label{equ:actionlet}
%     \begin{aligned}
%     \mathbf{A}^i_{tv} = \frac{\text{Clip}(\mathbf{H}^i_{tv}, [0, k])}{k},
%     \end{aligned}
% \end{equation}
% where $\text{Clip}$ is employed to resize $\mathbf{H}^i_{tv}$ into $[0,1]$ and $k$ is the threshold of activation. 

% Grad-CAM~\cite{selvaraju2017grad} also applies gradient back-propagation. In Grad-CAM, the activation regions are visualized according to the categories. 
%
% Instead, we mine the region that differs most from the average motion without any label reliance. The region that has a large difference compared to it can be considered as the region where the motion occurs, \textit{i.e.}, actionlet.

\subsection{Actionlet-Guided Contrastive Learning}

To take full advantage of the actionlet, we propose an actionlet-dependent contrastive learning method, shown in Fig.~\ref{fig:overview}.
%
We impose different data transformations for different regions by a motion-adaptive data transformation strategy module (MATS). 
%
Moreover, the semantic-aware feature pooling module (SAFP) is proposed to aggregate the features of actionlet region for \wh{better} action modeling.
\vspace{1mm}

\noindent\textbf{Motion-Adaptive Transformation Strategy (MATS).} 
\label{sec:mats}
In contrastive learning, data transformation $\mathcal{T}$ is crucial for semantic information extraction and generalization capacity. 
%
How to design more diverse data transformations while maintaining relevant information for downstream tasks is still a challenge.
%
Too simple data transformation is limited in numbers and modes and cannot obtain rich augmented patterns. However, data transformations that are too difficult may result in loss of motion information.
%
\zjh{To this end}, we propose motion-adaptive data transformations for skeleton data based on actionlet.
%
For different regions, we propose two transformations, actionlet transformation and non-actionlet transformation.

\vspace{1mm}

$\bullet$ \textbf{Actionlet Transformation $\mathcal{T}_{\text{act}}$}: 
%
Actionlet data transformations are performed within the actionlet regions. Inspired by the previous work~\cite{guo2021contrastive}, we adopt four spatial data transformations \{Shear, Spatial Flip, Rotate, Axis Mask\}; two temporal data transformations \{Crop, Temporal Flip\}; and two spatio-temporal data transformations \{Gaussian Noise, Gaussian Blur\}.

Besides, Skeleton AdaIN is proposed as a mixing method of global statistics. 
%
We randomly select two skeleton sequences and then swap the spatial mean and temporal variance of the two sequences. 
%
This transformation is widely used in style transfer~\cite{huang2017arbitrary}.
%
Here, we are inspired by the idea of style and content decomposition in style transfer and regard the motion-independent information as style and the motion-related information as content. Therefore, we use Skeleton AdaIN to transfer this motion independent noise between different data.
%
The noisy pattern of the data is thus augmented by this transfer method.
%
This transformation can be formalized as:
\begin{equation}
    \label{equ:info}
    \begin{aligned}
    \mathbf{X}^i_{\text{adain}} = \sigma(\mathbf{X}^j) \left(\frac{\mathbf{X}^i - \mu(\mathbf{X}^i)}{\sigma(\mathbf{X}^i)}\right) + \mu(\mathbf{X}^j),
    \end{aligned}
\end{equation}
where $\sigma(\cdot)$ is the temporal variance\wh{,} $\mu(\cdot)$ is the spatial mean, and $\mathbf{X}^j$ is a randomly selected sequence. All these data transformations maintain the action information.

\vspace{1mm}

$\bullet$ \textbf{Non-Actionlet Transformation $\mathcal{T}_{\text{non}}$}: 
To obtain stronger generalization, several extra data transformations are applied to the non-actionlet regions in addition to the above data transformation.

We apply an intra-instance data transformation \{Random Noise\} and an inter-instance data transformation \{Skeleton Mix\}. The random Noise has larger variance. Skeleton Mix is an element-wise data mixing method, including Mixup~\cite{zhang2017mixup}, CutMix~\cite{yun2019cutmix}, and ResizeMix~\cite{ren2022simple}. Because these transformations are performed on non-actionlet regions, they do not change the action semantics. Therefore, the transformed data are used as positive samples with the original data.

% In adversarial noise, the transformed data $\mathbf{X}^i_\text{ad}$ can be expressed as:
% \begin{equation}
% \label{equ:trans}
% \mathbf{X}^i_\text{ad} = \mathbf{X^i} + \mathbf{N},
% \end{equation}
% where $\mathbf{N}$ is the adversarial noise initialized to $\mathbf{0}$.

% We update the parameters of $\mathbf{N}$ by adversarial attack. Following the Fast Gradient Sign Method (FGSM)~\cite{goodfellow2014explaining}, we hope to make the data transformation to attack the similarity to fool the encoder $f(\cdot)$. Therefore, the parameters of the three matrices are updated as follows:
% \begin{equation}
% \label{equ:at}
% \mathbf{A} = \mathbf{A} + \epsilon \cdot \text{sgn} (\nabla_{\mathbf{A}} \mathcal{L}_{V}),
% \end{equation}
% where $\epsilon$ is the learning rate, and we employ the same formula to update $\mathbf{R}$ and $\mathbf{N}$.
% \input{tab/loss}

% Mixup mixes two samples through a hyperparameter $\alpha$ from a uniform distribution $\mathbf{U}[0,1]$ as follows:
% \begin{equation}
%     \label{equ:info}
%     \begin{aligned}
%     \mathbf{X}^i_{\text{mixup}} = \alpha \mathbf{X}^i + (1 - \alpha) \mathbf{X}^j,
%     \end{aligned}
% \end{equation}
% where $\mathbf{X}^i$ and $\mathbf{X}^j$ are two randomly selected samples.

% CutMix utilizes the partial random mixing of two skeleton sequences in spatio-temporal dimensions.
% %
% On the temporal axis, we randomly sample a start frame $t_s$ and an end frame $t_e$. 
% %
% Meanwhile, we randomly sample several skeleton joints $\mathcal{N}$. 
% %
% We crop out this corresponding region of the sequence $\mathbf{x}^j$ and paste it into the sequence $\mathbf{x}^i$ as follows:
% \begin{equation}
%     \label{equ:info}
%     \begin{aligned}
%     \mathbf{X}^i_{\text{cutmix}} = \mathbf{M} \odot \mathbf{X}^i + (1 - \mathbf{M}) \odot \mathbf{X}^j,
%     \end{aligned}
% \end{equation}
% where $\mathbf{M}$ is a $[0,1]$ binary mask whose elements are 0 in the cropping region and 1 in other areas.

% ResizeMix resizes the action sequence $\mathbf{X}^j$ to the length of the cropping region, then inserts it as a patch into the cropping region of the sequence $\mathbf{X}^i$. 
% %
% It is formalized as:
% \begin{equation}
%     \label{equ:info}
%     \begin{aligned}
%     \mathbf{X}^i_{\text{resizemix}} = \text{Paste}(\mathbf{M} \odot \mathbf{X}^i, \text{Resize}(\mathbf{X}^j)),
%     \end{aligned}
% \end{equation}
% where $\text{Resize}(\cdot)$ indicates the resize function and $\text{Paste}(\cdot)$ indicates pasting the patch onto the cropping region.

$\bullet$ \textbf{Actionlet-Dependent Combination}:
To merge the data transformations of the two regions, we utilize actionlets to combine.
It is formalized as:
\begin{equation}
    \label{equ:info}
    \begin{aligned}
    \mathbf{X}^i_{\text{trans}} = \mathbf{A}^i_{tv} \odot \mathbf{X}^i_{\text{act}} + (1 - \mathbf{A}^i_{tv}) \odot \mathbf{X}^i_{\text{non}},
    \end{aligned}
\end{equation}
where $\mathbf{X}^i_{\text{trans}}$ is the final transformed data, $\mathbf{X}^i_{\text{act}}$ and $\mathbf{X}^i_{\text{non}}$ are transformed with actionlet transformations $\mathcal{T}_{\text{act}}$. Then, $\mathbf{X}^i_{\text{non}}$ is performed non-actionlet transformations $\mathcal{T}_{\text{non}}$. $\mathbf{A}^i_{tv}$ represents the actionlet.
\vspace{1mm}

\noindent\textbf{Semantic-Aware Feature Pooling (SAFP).} 
\label{sec:safp}
To extract the motion information more accurately, we propose \wh{a} semantic-aware feature pooling method along the spatial-temporal dimension. This method focuses only on the feature representation of the actionlet region, thus reducing the interference of other static regions for motion feature extraction. It is formalized as:
\begin{equation}
    \label{equ:abfp}
    \begin{aligned}
     \text{SAFP}(\mathbf{h}^i_{ctv})= \sum_{t=1}^T \sum_{v=1}^V \mathbf{h}^i_{ctv} \left(\frac{\mathbf{A}^i_{tv}}{\sum_{t=1}^T \sum_{v=1}^V \mathbf{A}^i_{tv}}\right).
    \end{aligned}
\end{equation}
% where $\kappa$ is a hyper-parameter. 

This semantic-aware feature aggregation approach effectively extracts motion information and makes the features more distinguishable. We utilize this semantic-aware feature pooling operation in the offline stream to provide accurate anchor features.

% \vspace{1mm}

\begin{table*}[tb]
\small
\centering
\caption{Comparison of action recognition results with unsupervised learning approaches on NTU dataset.}
\begin{tabular}{l|c|c|c|c|c}
    \toprule
    % \hline
    Models& Stream &NTU 60 xview&NTU 60 xsub&NTU 120 xset&NTU 120 xsub\\
    \midrule
    AimCLR~\cite{guo2021contrastive} & joint & 79.7 & 74.3 & 63.4 & 63.4\\
    % CMD$^\dag$~\cite{mao2022cmd} & joint & 81.3 & 76.8 & 66.0 & 65.4 \\
    \textbf{ActCLR} & joint & \textbf{86.7} &\textbf{80.9}&\textbf{70.5} &\textbf{69.0}\\
    \midrule
    AimCLR~\cite{guo2021contrastive} & motion & 70.6 & 66.8 & 54.4 & 57.3\\
    % CMD$^\dag$~\cite{mao2022cmd} & motion &  80.4 & 75.8 & 64.8 & 64.2 \\
    \textbf{ActCLR} & motion & \textbf{84.4} &\textbf{78.6}&\textbf{67.8} &\textbf{68.3}\\
    \midrule
    AimCLR~\cite{guo2021contrastive} & bone & 77.0 & 73.2 & 63.4& 62.9\\
    % CMD$^\dag$~\cite{mao2022cmd} & bone & 80.7 & 75.2 & 65.0 & 64.7 \\
    \textbf{ActCLR} & bone & \textbf{85.0} &\textbf{80.1}&\textbf{68.2} &\textbf{67.8}\\
    \midrule
    3s-AimCLR~\cite{guo2021contrastive} & \quad joint+motion+bone \quad & 83.8 & 78.9 & 68.8 & 68.2\\
    % 3s-CMD$^\dag$~\cite{mao2022cmd} & \quad joint+motion+bone & 85.0 & 79.9 & 69.6 & 69.1\\
    \textbf{3s-ActCLR} & \quad joint+motion+bone \quad & \textbf{88.8} &\textbf{84.3}&\textbf{75.7} &\textbf{74.3}\\
    \bottomrule
\end{tabular}
\label{tab:unsupervised_ntu}
\end{table*}

\vspace{1mm}

\noindent\textbf{Training Overview.}
In this part, we conclude our framework of contrastive learning in detail: 
\begin{itemize}%[itemsep=2pt]
\setlength{\itemsep}{1.5pt} 
    \item[1)] Two encoders are pre-trained using MoCo v2~\cite{he2020momentum}, an online encoder $f_q(\cdot)$ and an offline encoder $f_k(\cdot)$.  \zjh{The online encoder is updated via back-propagation gradients, while the offline encoder is a momentum-updated version of the online encoder as described} in Sec.~\ref{sec:contra}.
    \item[2)] The offline network $f_k(\cdot)$ inputs the original data $\mathbf{X}^i$ and we employ the unsupervised actionlet selection module to generate actionlet regions $\mathbf{A}^i_{tv}$ in the offline stream in Sec.~\ref{sec:actionlet}.
    \item[3)] We perform data transformation $\mathcal{T}$ to obtain two different views $\mathbf{X}^i_q$ and $\mathbf{X}^i_k$. And we apply motion-adaptive transformation strategy (MATS) to enhance the diversity of $\mathbf{X}^i_q$ in Sec.~\ref{sec:mats}.
    \item[4)] For feature extraction, in online stream, $\mathbf{z}^i_q = (g_q \circ \text{GAP} \circ f_q \circ \text{MATS})(\mathbf{X}^i_q)$, where $g_q(\cdot)$ is an online projector and GAP is the global average pooling. To provide a stable and accurate anchor feature, we utilize the semantic-aware feature pooling (SAFP) method in Sec.~\ref{sec:safp} to generate offline features $\mathbf{z}^i_k = (g_k \circ \text{SAFP} \circ f_k)(\mathbf{X}^i_k)$, where $g_k(\cdot)$ is an offline projector. 
    % Two encoders are employed, an online encoder $f_q(\cdot)$ and an offline encoder $f_k(\cdot)$ to extract features: $\mathbf{z}^i_q = g_q(f_q(\mathbf{X}^i_q))$ and $\mathbf{z}^i_k = g_k(f_k(\mathbf{X}^i_k))$, where $g_q(\cdot)$ is an online projector and $g_k(\cdot)$ is an offline projector. The offline networks $f_k(\cdot)$ and $g_k(\cdot)$ are updated by the momentum of the online networks $f_q(\cdot)$ and $g_q(\cdot)$. by $\hat{f} \leftarrow \alpha \hat{f} + (1 - \alpha) f$, where $\alpha$ is a momentum coefficient. 
    \item[5)]  A memory bank $\mathbf{M} = \{\mathbf{m}^i\}^M_{i=1}$ is utilized to store offline features. The offline features extracted from the offline data in each batch are stored in the memory bank, and the bank is continuously updated using a first-in first-out strategy.
    \item[6)] Following recent works~\cite{mao2022cmd,zhang2022contrastive}, we exploit similarity mining to optimize:
    \begin{equation}
    \label{equ:info}
    \begin{aligned}
    &\mathcal{L}_{\text{KL}}(\mathbf{p}_q^i, \mathbf{p}_k^i) = -\mathbf{p}_k^i \log \mathbf{p}_q^i,\\
    &\mathbf{p}_q^i = \text{SoftMax}(\text{sim}(\mathbf{z}^i_q, \mathbf{M})/\tau_q),\\
    &\mathbf{p}_k^i = \text{SoftMax}(\text{sim}(\mathbf{z}^i_k, \mathbf{M})/\tau_k),
    \end{aligned}
    \end{equation}
    where $\text{sim}(\mathbf{z}^i_q, \mathbf{M}) = [\text{sim}(\mathbf{z}^i_q, \mathbf{m}^j)]^M_{j=1}$, which indicates the similarity distribution between feature $\mathbf{z}^i_q$ and other samples in $\mathbf{M}$. For the elements $\mathbf{p}_k^{ij}$ of $\mathbf{p}_k^i$ greater than the elements $\mathbf{p}_q^{ij}$ of $\mathbf{p}_q^i$, these corresponding features $\mathbf{m}^j$ in the memory bank are positive samples. This is because the network increases the similarity of the output with these features.
\end{itemize}


% \subsection{Training Paradigm}
% Our method training follows the following steps. To obtain more accurate actionlets, our network is pre-trained using \textit{MoCo v2}~\cite{guo2021contrastive}. Then we employ the unsupervised actionlet selection module to generate actionlet regions. And the data is enhanced with the motion-adaptive data transformation module. In contrastive learning, the offline network $f_k(\cdot)$ is to provide a stable and accurate anchor feature for the online network $f_q(\cdot)$. Thus, in feature pooling, we utilize the semantic-aware feature pooling method to generate offline features. 

% The feature extraction can be expressed as follows:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathbf{z}^i_q &= (g_q \circ \text{GAP} \circ f_q \circ \text{ADT})(\mathbf{X}^i_q),\\
% \mathbf{z}^i_k &= (g_k \circ \text{AFP} \circ f_k)(\mathbf{X}^i_k).
% \end{aligned}
% \end{equation}
% % where $\mathbf{z}^i_q = (g_q \circ \text{GAP} \circ f_q \circ \text{ADT})(\mathbf{X}^i_q)$ and $\mathbf{z}^i_k = (g_k \circ \text{AFP} \circ f_k)(\mathbf{X}^i_k)$.

% We exploit cross-entropy loss to reduce the entropy of the feature space to constrain the features.:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{L}_{\text{KL}}(\mathbf{p}_q^i, \mathbf{p}_k^i) = -\mathbf{p}_k^i \log \mathbf{p}_q^i,
% \end{aligned}
% \end{equation}
% where $\mathbf{p}_q^i = \text{SoftMax}(\text{Linear}(\mathbf{z}^i_q)/\tau_q)$ and $\mathbf{p}_k^i = \text{SoftMax}(\text{Linear}(\mathbf{z}^i_k)/\tau_k)$. $\text{Linear}(\cdot)$ is a linear projection. Since $\tau_k$ is much smaller than $\tau_q$, the entropy of $\mathbf{p}_k^i$ is smaller than the entropy of $\mathbf{p}_q^i$. By using $\mathbf{p}_k^i$ as a guide, the entropy of $\mathbf{p}_q^i$ can be reduced. In turn, the feature space is subject to an entropy constraint.

% The total loss function can be expressed as:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{L} = \mathcal{L}_{\text{CL}} + \lambda \mathcal{L}_{\text{KL}},
% \end{aligned}
% \end{equation}
% where $\lambda$ is a hyperparameter.

% \noindent\textbf{Random Adversarial Training.} In previous work on self-distillation, student networks are usually learned by stochastic gradient descent. However, this approach easily leads to overfitting of the student network to the teacher network. To enhance the generalization performance of the student network, we employ random adversarial learning for consistency regularization. Random adversarial training randomly reverses the gradient direction with a given probability during backpropagation.

% Specifically, during gradient backpropagation, we randomly reverse the gradient direction of the current layer with probability $p$. This makes layers with reversed gradients to increase the self-distillation loss, while layers that are not reversed remain trained normally. Therefore, this is equivalent to a kind of adversarial training between different layers. With this adversarial training, the network tends to converge to a flat parameter region. This enables the model to have better generalization performance and its robustness to disturbances is enhanced.

% Meanwhile, we adopt a two-stage training strategy. At the beginning of training, we employ normal stochastic gradient descent to learn the parameters of the student network. When the loss is small and the network starts to overfit, we apply random adversarial training to regularize the network. This training strategy can effectively alleviate the overfitting phenomenon of the network and enhance the generalization ability of the student network.

% \subsection{Consistency and Diversity Analysis of Self-Supervised Representations}
% To further quantitatively analyze the feature space of the pretrained model, in this section, we analyze the previous \wh{loss functions of self-supervised learning} and propose \wh{two new metrics to measure the consistency and diversity in the feature space}.

% The losses of the previous self-supervised tasks can be decomposed into two parts, consistency loss (alignment loss) and diversity loss (uniform loss):
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{L} = \mathcal{L}_{\text{con}} + \mathcal{L}_{\text{div}}.
% \end{aligned}
% \end{equation}
% Consistency loss $\mathcal{L}_{\text{con}}$ constrains \wh{different views of data or data with the same semantic labels} to extract \wh{similar} features, whereas the diversity loss $\mathcal{L}_{\text{div}}$ \wh{enlarges the distance of features extracted from different data/semantics, to prevent the feature space from collapsing.} 

% %
% The loss decomposition of canonical pretrained models is listed in Table~\ref{tab:loss}.
% %
% Previous works~\cite{huang2021towards, wang2022chaos} point out that the consistency loss is an upper bound on the loss of downstream tasks when the diversity loss is in a range.
% %
% Therefore, consistency and diversity are the two most important metrics for evaluating \wh{the feature spaces of self-supervised learning}.

% \wh{However, directly using the loss function as the metric to measure these two dimensions may be inaccurate as they are adopted as the constraint in the training and may incur overfitting.}
% %
% Thus, we propose two \wh{novel} metrics based on the attention mechanism. 
% %
% For the skeleton data $\mathbf{x}^i$, the representation is defined as $\mathbf{h}^i = f(\mathbf{x}^i) \in \mathbb{R}^d$.
% %
% We choose coordinates far from the mean in the feature space to generate the attention map:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \alpha^i_j = \mathbbm{1}\left[\mathbf{h}^i_j > \mu^i + \lambda \sigma^i\right],
% \end{aligned}
% \end{equation}
% where $\mathbbm{1}[\cdot]$ is an indicator function which outputs 1 if we meet the condition, and 0 otherwise, $\mu^i = \frac{1}{d} \sum_{j=1}^d \mathbf{h}^i_j$, and $\sigma^i = \sqrt{\frac{1}{d-1} \sum_{j=1}^d (\mathbf{h}^i_j - \mu^i)^2}$. 
% %
% We then exploit \wh{this} attention map to obtain measures of consistency and diversity. 

% For \wh{consistency}, we mainly consider the overlap of attention maps between different two views.
% %
% We use mIoU to calculate the overlap ratio\wh{:}
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{C} = \frac{1}{N} \sum_{i=1}^N \frac{|\alpha^i \cap \hat{\alpha}^i|}{|\alpha^i \cup \hat{\alpha}^i|},
% \end{aligned}
% \end{equation}
% where $\alpha^i$ and $\hat{\alpha}^i$ are \wh{views} from different data transformations of the same data.
% %
% Note that to prevent overfitting, the data transformation used here is not \wh{the one used} in the pretraining.

% For \wh{diversity}, we first obtain the mean attention map of each data. 
% %
% This mean attention map reflects the average importance of different features. 
% %
% If a feature is always important, then it may extract some general information. 
% %
% \wh{In constract, unimportant features may not extract any critical information.}
% %
% On this basis, we compute the entropy of the mean feature map to infer the diversity of this feature space:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{D} = -\frac{1}{d} \sum_{j=1}^d \alpha_j \log \alpha_j.
% \end{aligned}
% \end{equation}
% If $\mathcal{D}$ is small, it means that some features are always important while some are always unimportant, which indicates that a model collapsing has occurred. 

% Table~\ref{tab:cd} shows the consistency and diversity of a series of pretraining methods and the accuracy of downstream tasks.
% %
% \wh{It is observed} that larger \wh{semantic} consistency \wh{of different views} does lead to better downstream task performance. 
% %
% % \wh{Contrastive} learning, \wh{\textit{e.g.} SkeletonCLR and AimCLR}, cannot achieve optimal consistency due to the false negative samples problem~\cite{arora2019theoretical, chen2021incremental} \wh{when the diversity constraint is calculated}. 
% % %
% % Reconstruction tasks, \textit{e.g.} MAE, usually only apply masking and lack strong data transformations.
% $\mathcal{C}$ of SkeletonCLR and AimCLR are less than 0.2, according their accuracy is below 80, which is in line with the false negative sample assumption in~\cite{arora2019theoretical, chen2021incremental}.
% %
% Unlike them, our method can \wh{successfully} improve the consistency of pretrained models while maintaining diversity.
% %
% This is because our method employs enhanced data transformation and does not apply negative data, \wh{\textit{i.e.}, not including a diversity loss}, but only \wh{narrows} the feature distance between positive samples.

% \subsection{Gradient-Guided Parameter-Efficient Finetuning}
% In the finetuning of downstream tasks, previous works usually employ full-model finetuning or linear evaluation \wh{ finetuning, \textit{i.e.}, \wh{only finetuning} the last MLP layer}.
% %
% However, the full model update leads to \wh{huge} consumption of space-time resources.
% %
% It not only requires a lot of training time, but also requires a lot of memory usage.
% %
% While linear evaluation \wh{finetuning} is fast to train but the performance is sub-optimal. 
% %
% To achieve \wh{optimal} finetuning parameter efficiency while maintaining downstream effectiveness, we aim to design a parameter efficient finetuning strategy, which \wh{makes} the trade-off between efficiency and effectiveness.
% %
% We propose a gradient-guided parameter update strategy.
% %
% \wh{Via only updating the important parameters, the strategy} enables better performance while keeping fewer training parameters.

% Specifically, in each gradient \wh{back-propagation}, we calculate an indicator $\mathcal{I}$ from the modulus length of the current gradient and the number of parameters of the gradient. 
% %
% When the indicator $\mathcal{I}$ exceeds a threshold, this parameter is added to the optimizer. We formalize it as:
% \begin{equation}
%     \label{equ:info}
%     \begin{aligned}
%     \mathcal{I} = \sqrt{\frac{\text{tr}(\mathbf{\Delta W}^T \mathbf{\Delta W})}{M}},
%     \end{aligned}
% \end{equation}
% where $\mathbf{\Delta W} = \frac{\partial \mathcal{L}}{\partial \mathbf{W}}$ and $M$ is the number of parameters in $\mathbf{\Delta W}$. 
% %
% This indicator can quantitatively measure whether the parameters are important to the final loss function.
% %
% Finally, we update the parameters in the optimizer using standard stochastic gradient descent. 
% %
% Through the choice of thresholds, we control the amount of finetuning parameters. 
% \input{tab/sup}


\begin{table}[tb]
    \small
    \centering
    \caption{Comparison of action recognition results with unsupervised learning approaches on NTU 60 dataset. $^\dag$ indicates that results reproduced on our settings of feature dimension size.}
    \begin{tabular}{l|c|c|c}
      \toprule
      % \hline
      Models&Architecture&xview&xsub\\
      \midrule
      \rowcolor{gray!10} \multicolumn{4}{l}{\textit{Single-stream:}}\\
      LongT GAN~\cite{zheng2018unsupervised} & GRU & 48.1 & 39.1\\
      MS$^2$L~\cite{lin2020ms2l} & GRU & - & 52.5\\
      AS-CAL~\cite{rao2021augmented} & LSTM & 64.8 & 58.5 \\
      P$\&$C~\cite{su2020predict} & GRU & 59.3 & 56.1 \\
      SeBiReNet~\cite{nie2020unsupervised} & SeBiReNet & 79.7 & -\\
      ISC~\cite{thoker2021skeleton}& GCN $\&$ GRU & 78.6 & 76.3 \\
      AimCLR~\cite{guo2021contrastive}& GCN & 79.7 & 74.3\\
      CMD$^\dag$~\cite{mao2022cmd} & GRU & 81.3 & 76.8 \\
      GL-Transformer~\cite{kim2022global} & Transformer & 83.8 & 76.3\\
      CPM~\cite{zhang2022contrastive} & GCN & 84.9 & 78.7\\
      \textbf{ActCLR} & GCN & \textbf{86.7} &\textbf{80.9}\\
      \midrule
      \rowcolor{gray!10} \multicolumn{4}{l}{\textit{Three-stream:}}\\
      3s-Colorization~\cite{yang2021skeleton} & DGCNN & 83.1 & 75.2 \\
      3s-CrosSCLR~\cite{li20213d} & GCN & 83.4 & 77.8\\
      3s-AimCLR~\cite{guo2021contrastive}& GCN & 83.8 & 78.9\\
      3s-CMD$^\dag$~\cite{mao2022cmd} & GRU & 85.0 & 79.9 \\
      3s-SkeleMixCLR~\cite{chen2022contrastive} & GCN & 87.1 & 82.7\\
      3s-CPM~\cite{zhang2022contrastive} & GCN & 87.0 & 83.2\\
      \textbf{3s-ActCLR} & GCN & \textbf{88.8} &\textbf{84.3}\\
      \bottomrule
    \end{tabular}
    \label{tab:unsupervised_ntu_60}
  \end{table}
  
  % \begin{table*}[tb]
  %   \caption{Comparison of action recognition results with unsupervised learning approaches on NTU 120 dataset.}
  %   \label{tab:unsupervised_ntu}
  %   \centering
  %   \begin{tabular}{l|c|c|c|c}
  %     \toprule
  %     % \hline
  %     Models&NTU 60 xview&NTU 60 xsub&NTU 120 xset&NTU 120 xsub\\
  %     \midrule
  %     MS$^2$L~\cite{lin2020ms2l} & - & 52.5 & - & -\\
  %     AS-CAL~\cite{rao2021augmented} & 64.8 & 58.5 & 49.2 & 48.6 \\
  %     P$\&$C~\cite{su2020predict} & 59.3 & 56.1 & 44.1 & 41.4 \\
  %     SeBiReNet~\cite{nie2020unsupervised} & 79.7 & - & - & -\\
  %     AimCLR~\cite{guo2021contrastive} & 79.7 & 74.3 & - & -\\
  %     3s-Colorization~\cite{yang2021skeleton} & 83.1 & 75.2 & - & -\\
  %     ISC~\cite{thoker2021skeleton} & 78.6 & 76.3 & \textbf{67.1} & \textbf{67.9}\\
  %     3s-CrosSCLR~\cite{li20213d} & 83.4 & \textbf{77.8} & 66.7 & \textbf{67.9}\\
  %     BCLR & \textbf{83.9} &\textbf{77.8}&66.8& 67.5\\
  %     \bottomrule
  % \end{tabular}
  % \end{table*}

\section{Experiment Results}

For evaluation, we conduct our experiments on the following two datasets: the NTU RGB+D dataset~\cite{shahroudy2016ntu,liu2019ntu} and the PKUMMD dataset~\cite{liu2020pku}. 
% Our goal is to obtain the feature encoder $f(\cdot)$ which can generate good feature representations for action recognition. 

\subsection{Datasets and Settings}
\noindent$\bullet$ \textbf{NTU RGB+D Dataset 60 (NTU 60)}~\cite{shahroudy2016ntu} is a large-scale dataset which contains 56,578 videos with 60 action labels and 25 joints for each body, including interactions with pairs and individual activities.

\vspace{1mm}

\noindent$\bullet$ \textbf{NTU RGB+D Dataset 120 (NTU 120)}~\cite{liu2019ntu} is an extension to NTU 60 and the largest dataset for action recognition, which contains 114,480 videos with 120 action labels. Actions are captured with 106 subjects with multiple settings using 32 different setups.


\vspace{1mm}

\noindent$\bullet$ \textbf{PKU Multi-Modality Dataset (PKUMMD)}~\cite{liu2020pku} covers a multi-modality 3D understanding of human actions. The actions are organized into 52 categories and include almost 20,000 instances. There are 25 joints in each sample. The PKUMMD is divided into part I and part II. Part II provides more challenging data, because the large view variation causes more skeleton noise.

To train the network, all the skeleton sequences are temporally down-sampled to 50 frames. The encoder $f(\cdot)$ is based on ST-GCN~\cite{yan2018spatial} with hidden channels of size 16, which is a quarter the size of the original model. The projection heads for contrastive learning and auxiliary tasks are all multilayer perceptrons, projecting features from 256 dimensions to 128 dimensions. $\tau_q$ is $0.1$ and $\tau_k$ is $0.04$. We employ a fully connected layer $\phi(\cdot)$ for evaluation.

\begin{table}[tb]
  \small
  \centering
  \caption{Comparison of action recognition results with unsupervised learning approaches on NTU 120 dataset. $^\dag$ indicates that results reproduced on our settings of feature dimension size.}
  \begin{tabular}{l|c|c|c}
    \toprule
    % \hline
    Models&Architecture&xset&xsub\\
    \midrule
    \rowcolor{gray!10} \multicolumn{4}{l}{\textit{Single-stream:}}\\
    AS-CAL~\cite{rao2021augmented} & LSTM & 49.2 & 48.6 \\
    AimCLR~\cite{guo2021contrastive} & GCN & 63.4 & 63.4\\
    CMD$^\dag$~\cite{mao2022cmd} & GRU & 66.0 & 65.4 \\
    GL-Transformer~\cite{kim2022global} & Transformer & 68.7 & 66.0\\
    CPM~\cite{zhang2022contrastive} & GCN & 69.6 & 68.7\\
    \textbf{ActCLR} & GCN & \textbf{70.5} &\textbf{69.0}\\
    \midrule
    \rowcolor{gray!10} \multicolumn{4}{l}{\textit{Three-stream:}}\\
    3s-CrosSCLR~\cite{li20213d} & GCN & 66.7 & 67.9\\
    3s-AimCLR~\cite{guo2021contrastive} & GCN & 68.8 & 68.2\\
    3s-CMD$^\dag$~\cite{mao2022cmd} & GRU & 69.6 & 69.1 \\
    3s-SkeleMixCLR~\cite{chen2022contrastive} & GCN & 70.7 & 70.5\\
    3s-CPM~\cite{zhang2022contrastive} & GCN & 74.0 & 73.0\\
    \textbf{3s-ActCLR} & GCN & \textbf{75.7} &\textbf{74.3}\\
    \bottomrule
  \end{tabular}
  \label{tab:unsupervised_ntu_120}
\end{table}

To optimize our network, Adam optimizer~\cite{newey1988adaptive} is applied, and we train the network on one NVIDIA TitanX GPU with a batch size of 128 for 300 epochs.

\begin{table*}[tb]
    \small
    \centering
    \caption{Comparison of action recognition results with supervised learning approaches on NTU dataset.}
    \begin{tabular}{l|c|c|c|c|c}
      \toprule
     Models&Params&NTU 60 xview&NTU 60 xsub&NTU 120 xset&NTU 120 xsub\\
      \midrule
      \rowcolor{gray!10} \multicolumn{6}{l}{\textit{Single-stream:}}\\
      % \textit{Single-stream:}\\
      ST-GCN~\cite{yan2018spatial} & 0.83M & 88.3 & 81.5 & 73.2 & 70.7 \\
      SkeletonCLR~\cite{li20213d} & 0.85M & 88.9 & 82.2 & 75.3 & 73.6\\
      AimCLR~\cite{guo2021contrastive} & 0.85M & 89.2 & 83.0 & 76.1 & 77.2\\
      CPM~\cite{zhang2022contrastive} & 0.84M & 91.1 & 84.8 & 78.9 & 78.4\\
      % \textbf{F4F (Parameter-Efficient)} & 0.23M & 90.1 & 84.7 & 79.3 & 76.6\\
      \textbf{ActCLR} & 0.84M & \textbf{91.2} &\textbf{85.8}&\textbf{80.9}& \textbf{79.4}\\
      \midrule
      \rowcolor{gray!10} \multicolumn{6}{l}{\textit{Three-stream:}}\\
      3s-ST-GCN~\cite{yan2018spatial} & 2.49M & 91.4 & 85.2 & 77.1 & 77.2 \\
      3s-CrosSCLR~\cite{li20213d}& 2.55M & 92.5 & 86.2 & 80.4 & 80.5 \\
      3s-AimCLR~\cite{guo2021contrastive}& 2.55M & 92.8 & 86.9 & 80.9 & 80.1\\
      3s-SkeleMixCLR~\cite{chen2022contrastive}& 2.55M & 93.9 & 87.8 & 81.2 & 81.6\\
      % \textbf{3s-F4F (Parameter-Efficient)} & 0.69M & 92.7 & 87.5 & 83.2 & 81.0\\
      \textbf{3s-ActCLR} & 2.52M & \textbf{93.9} &\textbf{88.2}&\textbf{84.6}& \textbf{82.1}\\
      \bottomrule
  \end{tabular}
    \label{tab:supervised_ntu}
  \end{table*}

\subsection{Evaluation and Comparison}
To make a comprehensive evaluation, we compare our method with other methods under variable settings.
\vspace{1mm}

\begin{table}[tb]
    \small
    \centering
    \caption{Comparison of the transfer learning performance on PKUMMD dataset with linear evaluation pretrained on NTU 60.}
    \begin{tabular}{l|c|c}
        \toprule
    Models&PKU I xview& PKU II xview\\
      \midrule
      3s-AimCLR~\cite{guo2021contrastive} & 85.3 & 42.4 \\
      \textbf{3s-ActCLR} & \textbf{91.6} &\textbf{44.5}\\
      \midrule
      \midrule
     Models& PKU I xsub& PKU II xsub\\
      \midrule
      LongT GAN~\cite{zheng2018unsupervised} & - & 44.8 \\
      MS$^2$L~\cite{lin2020ms2l} & - & 45.8\\
      ISC~\cite{thoker2021skeleton} & - & 51.1\\
      Hi-TRS~\cite{chen2022hierarchically} & - & 55.0\\
      3s-CrosSCLR~\cite{li20213d} & - & 51.3\\
      3s-AimCLR~\cite{guo2021contrastive} & 85.6 & 51.6 \\
      \textbf{3s-ActCLR} & \textbf{90.0} &\textbf{55.9}\\
      \bottomrule
  \end{tabular}
    \label{tab:trans_pku}
  \end{table}

\noindent\textbf{1) Linear Evaluation.}
In the linear evaluation mechanism, a linear classifier $\phi(\cdot)$ is applied to the fixed encoder $f(\cdot)$ to classify the extracted features. We adopt action recognition accuracy as a measurement. Note that this encoder $f(\cdot)$ is fixed in the linear evaluation protocol.

Compared with other methods in Tables~\ref{tab:unsupervised_ntu},~\ref{tab:unsupervised_ntu_60} and~\ref{tab:unsupervised_ntu_120}, our model shows superiority on these datasets. We find that the transformation that 3s-CrosSCLR~\cite{li20213d} and 3s-AimCLR~\cite{guo2021contrastive} design in the contrastive learning task is unified for different regions, which makes the data transformation interfere with the motion information. On the contrary, our method adopts MATS for semantic-aware motion-adaptive data transformation. Thus, the features extracted by our method maintain better action information which is more suitable for downstream tasks. 

\vspace{1mm}


\noindent\textbf{2) Supervised Finetuning.}
We first pretrain the encoder $f(\cdot)$ in the self-supervised learning setting, and then finetune the entire network. We train the encoder $f(\cdot)$ and classifier $\phi(\cdot)$ using complete training data.

Table~\ref{tab:supervised_ntu} displays the action recognition accuracy on the NTU datasets. This result confirms that our method extracts the information demanded by downstream tasks and can better benefit action recognition. 
In comparison with state-of-the-art supervised learning methods, our model achieves better performance.

\vspace{1mm}

% \noindent\textbf{\wh{3)} Semi-Supervised Approaches.}
% In semi-supervised learning, the training process utilizes both labeled data and unlabeled data. Generally, the encoder $f(\cdot)$ is pretrained with the contrastive learning task with full data, and then fine-tuned with the classifier $\phi(\cdot)$ with labeled data. To give a comprehensive and thorough evaluation, we conduct experiments under different settings, including $1\%$, $5\%$, $10\%$ and $20\%$ of labeled skeleton sequences. We also show the comparison results with the state-of-the-art methods. 

% In Table~\ref{tab:semi_supervised_pku} and~\ref{tab:semi_supervised_ntu}, we notice that with small subsets of the datasets, our method improves the accuracy considerably and performs better than the state-of-the-art methods. Especially with smaller training data, our method outperforms the state-of-the-art by large margins.

% \noindent\textbf{\wh{3)} Parameter-Efficient Finetuning.}
% We also first pretrain the encoder $f(\cdot)$, and after self-distillating the encoder $f(\cdot)$, employ the gradient-guided parameter-efficient finetuning.

% Table~\ref{tab:supervised_ntu} shows the action recognition accuracy on the NTU datasets. Our method achieves comparable performance to full model fine-tuning with fewer trainable parameters. Moreover, we can adjust the \wh{number} of parameters that need to be finetuned through the threshold, which makes the finetuning more controllable, as shown in Table~\ref{tab:effi}.

\vspace{1mm}

\noindent\textbf{3) Transfer Learning.} 
To explore the generalization ability, we evaluate the performance of transfer learning. In transfer learning, we exploit self-supervised task pretraining on the source data. Then we utilize the linear evaluation mechanism to evaluate on the target dataset. In linear evaluation, the encoder $f(\cdot)$ has fixed parameters without fine-tuning.

As shown in Table~\ref{tab:trans_pku}, our method achieves significant performance. Our method employs MATS to remove irrelevant information, and SAFP to retain information related to downstream tasks. This allows our encoder $f(\cdot)$ to obtain stronger generalization performance.

\vspace{1mm}

\noindent\textbf{4) Unsupervised Action Segmentation.} 
To explore the extraction of local features by our method, we used unsupervised action segmentation as an evaluation metric. We pre-train the encoder $f(\cdot)$ on the NTU 60 dataset. Then we utilize the linear evaluation mechanism to evaluate \wh{the results} on the PKUMMD dataset. In linear evaluation, the encoder $f(\cdot)$ has fixed parameters without fine-tuning.

As shown in Table~\ref{tab:seg_pkuII}, our method achieves significant performance. Because our method focuses on the main occurrence region of the action, it is possible to locate the actions out of the long sequence.

\begin{table}[tb]
    \small
    \begin{center}
    \caption{Comparison of the action segmentation performance on PKUMMD II xview dataset with linear evaluation pretrained on NTU 60 xview dataset.}
    \label{tab:seg_pkuII}
    \begin{tabular}{l|c|c|c|c|c}
    \toprule
    \multirow{2.5}*{Models}&\multirow{2.5}*{Stream}&\multicolumn{4}{c}{PKUMMD II xview}\\
    \cmidrule(lr){3-6}
    &&ACC & MACC & FWIoU & mIoU\\
    \midrule
    AimCLR~\cite{guo2021contrastive} & joint & 39.77 & 28.68 & 26.79 & 15.67\\
    \textbf{ActCLR} & joint & \textbf{51.29}  & \textbf{31.97} & \textbf{35.24} & \textbf{21.38}\\
    \midrule
    AimCLR~\cite{guo2021contrastive}& motion& 42.32 & 26.65 & 29.92 & 15.92 \\
    \textbf{ActCLR} & motion& \textbf{56.69} & \textbf{39.45} & \textbf{41.34} & \textbf{27.73}\\
    \midrule
    AimCLR~\cite{guo2021contrastive}& bone& 54.22 & 39.52 & 39.41 & 27.36\\
    \textbf{ActCLR} &bone& \textbf{59.09} & \textbf{41.14} & \textbf{41.54} & \textbf{28.89}\\
    \bottomrule
    \end{tabular}
    \end{center}
    \end{table}


\subsection{Ablation Study}
Next, we conduct ablation experiments to give a more detailed analysis of our proposed approach.

\vspace{1mm}

\noindent\textbf{1) Analysis of Motion-Adaptive Data Transformation.} 
Data transformation is very important for consistency learning. To explore the influence of motion-adaptive data transformations, we test the action recognition accuracy under different data transformations. 
%
As shown in Table~\ref{tab:data}, the motion-adaptive transformation can obtain better performance than full region (the whole skeleton data) in different noise settings. It is also observed that when the noise strength increases, our performance degradation is much smaller than that of full region. This indicates that the design is more robust to data transformation.
% the consistency of the feature space is further enhanced with actionlet-dependent data transformations. And under stronger data transformations, our method still maintains the motion semantic information. Accordingly, the performance of downstream tasks is improved. 

To explore the influence of different data transformations on the contrastive learning effect, we test the action recognition accuracy under different data transformation combinations. As shown in Table~\ref{tab:com}, the consistency of the feature space is further enhanced with more data transformations. Thus, the performance of the downstream task is improved.

% \begin{table}[tb]
% \small
% \begin{center}
% \caption{Analysis of actionlet-dependent data transformation on NTU 60 dataset with the joint stream.}
% \label{tab:data}
% \begin{tabular}{l|c|c|c|c}
% \toprule
% \multirow{2.5}*{Transformation}& \multicolumn{2}{c}{Region}&\multicolumn{2}{|c}{xview}\\
% \cmidrule(lr){2-5}
% & Non-Actionlet & Full Area & KNN & Linear \\
% \midrule
% \multirow{2}*{Noise 0.01} & \checkmark &  & 77.63 & 86.46 \\
% &  & \checkmark & 76.51 & 85.91 \\
% \midrule
% \multirow{2}*{Noise 0.05} & \checkmark &  & \textbf{78.04} & \textbf{86.79} \\
% &  & \checkmark & 75.28 & 84.20 \\
% \midrule
% \multirow{2}*{Noise 0.1} & \checkmark &  & 77.31 & 86.12\\
% &  & \checkmark & 74.19 & 83.69\\
% \midrule
% \multirow{2}*{Skeleton Mix} & \checkmark &  & \textbf{78.04} & \textbf{86.79}\\
% &  & \checkmark & 73.24 & 83.05\\
% \bottomrule
% \end{tabular}
% \end{center}
% \end{table}

\begin{table}[tb]
    \small
    \begin{center}
    \caption{Analysis of motion-adaptive data transformation on NTU 60 xview dataset with the joint stream.}
    \label{tab:data}
    \begin{tabular}{l|c|c|c}
    \toprule
    Transformation & Region & KNN & Linear\\
    \midrule
    \multirow{2}*{Noise 0.01} & Non-Actionlet & 77.63 & 86.46 \\
    &  Full Area & 76.51 & 85.91 \\
    \midrule
    \multirow{2}*{Noise 0.05} & Non-Actionlet  & \textbf{78.04} & \textbf{86.79} \\
    &  Full Area & 75.28 & 84.20 \\
    \midrule
    \multirow{2}*{Noise 0.1} & Non-Actionlet  & 77.31 & 86.12\\
    &  Full Area & 74.19 & 83.69\\
    \midrule
    \multirow{2}*{Skeleton Mix} & Non-Actionlet  & \textbf{78.04} & \textbf{86.79}\\
    &  Full Area & 73.24 & 83.05\\
    \bottomrule
    \end{tabular}
    \end{center}
    \end{table}
    
    \begin{table}[tb]
    \small
    \begin{center}
    \caption{Analysis of data transformation combinations on NTU 60 xview dataset with the joint stream. $\mathcal{T}$ is all the transformations. $\mathcal{T}_{\text{act}}$ is actionlet transformations. $\mathcal{T}_{\text{non}}$ is non-actionlet transformations. AdaIN refers to Skeleton AdaIN.}
    \label{tab:com}
    \begin{tabular}{l|c|c}
    \toprule
    % \multirow{2.5}*{Proportion}&\multicolumn{2}{|c}{xview}\\
    % \cmidrule(lr){2-3}
    Modules& KNN & Linear\\
    \midrule
    w/o $\mathcal{T}$ & 67.50 & 79.98\\
    w/o (AdaIN + $\mathcal{T}_{\text{non}}$) & 69.75 & 81.80\\
    w/o $\mathcal{T}_{\text{non}}$ & 73.63 & 83.27\\
    Full Version & \textbf{78.04} & \textbf{86.79}\\
    \bottomrule
    \end{tabular}
    \end{center}
    \end{table}

\vspace{1mm}

\noindent\textbf{2) Analysis of Semantic-Aware Feature Pooling.} 
To explore the semantic-aware feature pooling, we perform this pooling on different streams. Table~\ref{tab:effi} shows the results of accuracy of action recognition under different settings. We note that better performance is obtained with offline, as it makes offline to generate better positive sample features for contrastive learning. Using this module in online reduces the benefits exposed by the non-actionlet transformation.
% In offline network feature extraction, $\kappa$ controls the attention of the actionlet. A smaller $\kappa$ makes the feature pooling close to the global average pooling, while a larger $\kappa$ makes the features extracted only from the actionlet region.
% %
% Table~\ref{tab:effi} shows the results of different $\kappa$. 
% We notice that using only SAFP can get better performance than using GAP. And the combination of both is used to obtain more informative features.

\begin{table}[tb]
    \small
    \begin{center}
    \caption{Analysis of semantic-aware feature pooling on NTU 60 xview dataset with the joint stream.}
    \label{tab:effi}
    \begin{tabular}{l|c|c}
    \toprule
    % \multirow{2.5}*{Proportion}&\multicolumn{2}{|c}{xview}\\
    % \cmidrule(lr){2-3}
    Modules& KNN & Linear\\
    \midrule
    w/o SAFP & 76.38 & 85.69\\
    % 1.0 & 75.89 & 86.28\\
    offline w/ SAFP  & \textbf{78.04} & \textbf{86.79}\\
    online w/ SAFP & 76.02 & 85.25\\
    online + offline w/ SAFP & 76.71 & 85.92\\
    \bottomrule
    \end{tabular}
    \end{center}
    \vspace{-1em}
    \end{table}
    

\vspace{1mm}

% \noindent\textbf{3) Analysis of Unsupervised Actionlet Selection.} 
% We explored the effect of thresholding on actionlet selection for feature learning. As shown in Table~\ref{tab:threshold}, when the threshold is small, many joints are selected causing the actionlet to contain many static regions. When the threshold value is relatively large, the selected area is relatively small, making the motion information lost.
% \input{cvpr2023-author_kit-v1_1-1/latex/tab/thresholds.tex}

% \vspace{1mm}

\noindent\textbf{3) Analysis of Actionlet and Non-Actionlet Semantic Decoupling.}   
In Fig.~\ref{fig:acc}, we show the performance of extracting only actionlet region information and non-actionlet region information for action recognition. The accuracy of the actionlet region for action recognition is comparable to the accuracy of the whole skeleton data. In contrast, the performance of the features of non-actionlet regions for action recognition is much lower. This shows that the actionlet area does contain the main motion information. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/acc.png}
    \caption{
    % 
    Action recognition accuracy of actionlet regions and non-actionlet regions. 
    }
    \label{fig:acc}
    \end{figure}

\vspace{1mm}

\noindent\textbf{4) Visualization of Average Motion and Actionlet.} 
\wh{Fig}.~\ref{fig:mean} shows a visualization of the average motion and actionlet respectively. The average motion has no significant motion information and serves as a background. The actionlet, shown in Fig.~\ref{fig:act}, selects the joints where the motion mainly occurs. Our actionlet is spatio-temporal, because the joints with motion may change when the action is performed.

\begin{figure}[tb]
    \includegraphics[width=\linewidth]{fig/mean.png}
    \caption{Visualization of the average motion. No obvious action takes place in the average motion sequence and can therefore be considered as a static anchor.}
    \label{fig:mean}
    \end{figure}
    
    \begin{figure}[tb]
    \includegraphics[width=\linewidth]{fig/actionlet.png}
    % \includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/latex/fig/actionlet2.png}
    \caption{
    Visualization of the actionlet for a ``throw" sequence. The yellow joints are the actionlet. Note that hand movements are mainly selected, indicating that the actionlet is reasonable.
    }
    \label{fig:act}
    \end{figure}

\section{Conclusions}
% \lorem{3}
In this work, we propose a novel actionlet-dependent contrastive learning method. Using actionlets, we design motion-adaptive data transformation and semantic-aware feature pooling to decouple action and non-action regions. These modules make the motion information of the sequence to be attended to while reducing the interference of static regions in feature extraction. In addition, the similarity mining loss further regularizes the feature space. Experimental results show that our method can achieve remarkable performance and verify the effectiveness of our designs.

% \bibliography{ref}

% %%%%%%%%% BODY TEXT
% \section{Introduction}
% \label{sec:intro}

% Please follow the steps outlined below when submitting your manuscript to the IEEE Computer Society Press.
% This style guide now has several important modifications (for example, you are no longer warned against the use of sticky tape to attach your artwork to the paper), so all authors should read this new version.

% %-------------------------------------------------------------------------
% \subsection{Language}

% All manuscripts must be in English.

% \subsection{Dual submission}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a
% discussion of the policy on dual submissions.

% \subsection{Paper length}
% Papers, excluding the references section, must be no longer than eight pages in length.
% The references section will not be included in the page count, and there is no limit on the length of the references section.
% For example, a paper of eight pages with two pages of references would have a total length of 10 pages.
% {\bf There will be no extra page charges for \confName\ \confYear.}

% Overlength papers will simply not be reviewed.
% This includes papers where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide.
% Note that this \LaTeX\ guide already sets figure captions and references in a smaller font.
% The reason such papers will not be reviewed is that there is no provision for supervised revisions of manuscripts.
% The reviewing process cannot determine the suitability of the paper for presentation in eight pages if it is reviewed in eleven.

% %-------------------------------------------------------------------------
% \subsection{The ruler}
% The \LaTeX\ style defines a printed ruler which should be present in the version submitted for review.
% The ruler is provided in order that reviewers may comment on particular lines in the paper without circumlocution.
% If you are preparing a document using a non-\LaTeX\ document preparation system, please arrange for an equivalent ruler to appear on the final output pages.
% The presence or absence of the ruler should not change the appearance of any other content on the page.
% The camera-ready copy should not contain a ruler.
% (\LaTeX\ users may use options of cvpr.sty to switch between different versions.)

% Reviewers:
% note that the ruler measurements do not align well with lines in the paper --- this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly.
% Just use fractional references (\eg, this line is $087.5$), although in most cases one would expect that the approximate location will be adequate.


% \subsection{Paper ID}
% Make sure that the Paper ID from the submission system is visible in the version submitted for review (replacing the ``*****'' you see in this document).
% If you are using the \LaTeX\ template, \textbf{make sure to update paper ID in the appropriate place in the tex file}.


% \subsection{Mathematics}

% Please number all of your sections and displayed equations as in these examples:
% \begin{equation}
%   E = m\cdot c^2
%   \label{eq:important}
% \end{equation}
% and
% \begin{equation}
%   v = a\cdot t.
%   \label{eq:also-important}
% \end{equation}
% It is important for readers to be able to refer to any particular equation.
% Just because you did not refer to it in the text does not mean some future reader might not need to refer to it.
% It is cumbersome to have to use circumlocutions like ``the equation second from the top of page 3 column 1''.
% (Note that the ruler will not be present in the final copy, so is not an alternative to equation numbers).
% All authors will benefit from reading Mermin's description of how to write mathematics:
% \url{http://www.pamitc.org/documents/mermin.pdf}.

% \subsection{Blind review}

% Many authors misunderstand the concept of anonymizing for blind review.
% Blind review does not mean that one must remove citations to one's own work---in fact it is often impossible to review a paper unless the previous citations are known and available.

% Blind review means that you do not use the words ``my'' or ``our'' when citing previous work.
% That is all.
% (But see below for tech reports.)

% Saying ``this builds on the work of Lucy Smith [1]'' does not say that you are Lucy Smith;
% it says that you are building on her work.
% If you are Smith and Jones, do not say ``as we show in [7]'', say ``as Smith and Jones show in [7]'' and at the end of the paper, include reference 7 as you would any other cited work.

% An example of a bad paper just asking to be rejected:
% \begin{quote}
% \begin{center}
%     An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of our previous paper [1], and show it to be inferior to all previously known methods.
%    Why the previous paper was accepted without this analysis is beyond me.

%    [1] Removed for blind review
% \end{quote}


% An example of an acceptable paper:
% \begin{quote}
% \begin{center}
%      An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of the  paper of Smith \etal [1], and show it to be inferior to all previously known methods.
%    Why the previous paper was accepted without this analysis is beyond me.

%    [1] Smith, L and Jones, C. ``The frobnicatable foo filter, a fundamental contribution to human knowledge''. Nature 381(12), 1-213.
% \end{quote}

% If you are making a submission to another conference at the same time, which covers similar or overlapping material, you may need to refer to that submission in order to explain the differences, just as you would if you had previously published related work.
% In such cases, include the anonymized parallel submission~\cite{Authors14} as supplemental material and cite it as
% \begin{quote}
% [1] Authors. ``The frobnicatable foo filter'', F\&G 2014 Submission ID 324, Supplied as supplemental material {\tt fg324.pdf}.
% \end{quote}

% Finally, you may feel you need to tell the reader that more details can be found elsewhere, and refer them to a technical report.
% For conference submissions, the paper must stand on its own, and not {\em require} the reviewer to go to a tech report for further details.
% Thus, you may say in the body of the paper ``further details may be found in~\cite{Authors14b}''.
% Then submit the tech report as supplemental material.
% Again, you may not assume the reviewers will read this material.

% Sometimes your paper is about a problem which you tested using a tool that is widely known to be restricted to a single institution.
% For example, let's say it's 1969, you have solved a key problem on the Apollo lander, and you believe that the CVPR70 audience would like to hear about your
% solution.
% The work is a development of your celebrated 1968 paper entitled ``Zero-g frobnication: How being the only people in the world with access to the Apollo lander source code makes us a wow at parties'', by Zeus \etal.

% You can handle this paper like any other.
% Do not write ``We show how to improve our previous work [Anonymous, 1968].
% This time we tested the algorithm on a lunar lander [name of lander removed for blind review]''.
% That would be silly, and would immediately identify the authors.
% Instead write the following:
% \begin{quotation}
% \noindent
%    We describe a system for zero-g frobnication.
%    This system is new because it handles the following cases:
%    A, B.  Previous systems [Zeus et al. 1968] did not  handle case B properly.
%    Ours handles it by including a foo term in the bar integral.

%    ...

%    The proposed system was integrated with the Apollo lunar lander, and went all the way to the moon, don't you know.
%    It displayed the following behaviours, which show how well we solved cases A and B: ...
% \end{quotation}
% As you can see, the above text follows standard scientific convention, reads better than the first version, and does not explicitly name you as the authors.
% A reviewer might think it likely that the new paper was written by Zeus \etal, but cannot make any decision based on that guess.
% He or she would have to be sure that no other authors could have been contracted to solve problem B.
% \medskip

% \noindent
% FAQ\medskip\\
% {\bf Q:} Are acknowledgements OK?\\
% {\bf A:} No.  Leave them for the final copy.\medskip\\
% {\bf Q:} How do I cite my results reported in open challenges?
% {\bf A:} To conform with the double-blind review policy, you can report results of other challenge participants together with your results in your paper.
% For your results, however, you should not identify yourself and should not mention your participation in the challenge.
% Instead present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results.\medskip\\

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}

%    \caption{Example of caption.
%    It is set in Roman so that mathematics (always set in Roman: $B \sin A = A \sin B$) may be included without an ugly clash.}
%    \label{fig:onecol}
% \end{figure}

% \subsection{Miscellaneous}

% \noindent
% Compare the following:\\
% \begin{tabular}{ll}
%  \verb'$conf_a$' &  $conf_a$ \\
%  \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
% \end{tabular}\\
% See The \TeX book, p165.

% The space after \eg, meaning ``for example'', should not be a sentence-ending space.
% So \eg is correct, {\em e.g.} is not.
% The provided \verb'\eg' macro takes care of this.

% When citing a multi-author paper, you may save space by using ``et alia'', shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word).
% If you use the \verb'\etal' macro provided, then you need not worry about double periods when used at the end of a sentence as in Alpher \etal.
% However, use it only when there are three or more authors.
% Thus, the following is correct:
%    ``Frobnication has been trendy lately.
%    It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
%    Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

% This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...'' because reference~\cite{Alpher03} has just two authors.


% % Update the cvpr.cls to do the following automatically.
% % For this citation style, keep multiple citations in numerical (not
% % chronological) order, so prefer \cite{Alpher03,Alpher02,Authors14} to
% % \cite{Alpher02,Alpher03,Authors14}.


% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.68\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{An example of a subfigure.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{Another example of a subfigure.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
% \end{figure*}

% %------------------------------------------------------------------------
% \section{Formatting your paper}
% \label{sec:formatting}

% All text must be in a two-column format.
% The total allowable size of the text area is $6\frac78$ inches (17.46 cm) wide by $8\frac78$ inches (22.54 cm) high.
% Columns are to be $3\frac14$ inches (8.25 cm) wide, with a $\frac{5}{16}$ inch (0.8 cm) space between them.
% The main title (on the first page) should begin 1 inch (2.54 cm) from the top edge of the page.
% The second and following pages should begin 1 inch (2.54 cm) from the top edge.
% On all pages, the bottom margin should be $1\frac{1}{8}$ inches (2.86 cm) from the bottom edge of the page for $8.5 \times 11$-inch paper;
% for A4 paper, approximately $1\frac{5}{8}$ inches (4.13 cm) from the bottom edge of the
% page.

% %-------------------------------------------------------------------------
% \subsection{Margins and page numbering}

% All printed material, including text, illustrations, and charts, must be kept
% within a print area $6\frac{7}{8}$ inches (17.46 cm) wide by $8\frac{7}{8}$ inches (22.54 cm)
% high.
% %
% Page numbers should be in the footer, centered and $\frac{3}{4}$ inches from the bottom of the page.
% The review version should have page numbers, yet the final version submitted as camera ready should not show any page numbers.
% The \LaTeX\ template takes care of this when used properly.



% %-------------------------------------------------------------------------
% \subsection{Type style and fonts}

% Wherever Times is specified, Times Roman may also be used.
% If neither is available on your word processor, please use the font closest in
% appearance to Times to which you have access.

% MAIN TITLE.
% Center the title $1\frac{3}{8}$ inches (3.49 cm) from the top edge of the first page.
% The title should be in Times 14-point, boldface type.
% Capitalize the first letter of nouns, pronouns, verbs, adjectives, and adverbs;
% do not capitalize articles, coordinate conjunctions, or prepositions (unless the title begins with such a word).
% Leave two blank lines after the title.

% AUTHOR NAME(s) and AFFILIATION(s) are to be centered beneath the title
% and printed in Times 12-point, non-boldface type.
% This information is to be followed by two blank lines.

% The ABSTRACT and MAIN TEXT are to be in a two-column format.

% MAIN TEXT.
% Type main text in 10-point Times, single-spaced.
% Do NOT use double-spacing.
% All paragraphs should be indented 1 pica (approx.~$\frac{1}{6}$ inch or 0.422 cm).
% Make sure your text is fully justified---that is, flush left and flush right.
% Please do not place any additional blank lines between paragraphs.

% Figure and table captions should be 9-point Roman type as in \cref{fig:onecol,fig:short}.
% Short captions should be centred.

% \noindent Callouts should be 9-point Helvetica, non-boldface type.
% Initially capitalize only the first word of section titles and first-, second-, and third-order headings.

% FIRST-ORDER HEADINGS.
% (For example, {\large \bf 1. Introduction}) should be Times 12-point boldface, initially capitalized, flush left, with one blank line before, and one blank line after.

% SECOND-ORDER HEADINGS.
% (For example, { \bf 1.1. Database elements}) should be Times 11-point boldface, initially capitalized, flush left, with one blank line before, and one after.
% If you require a third-order heading (we discourage it), use 10-point Times, boldface, initially capitalized, flush left, preceded by one blank line, followed by a period and your text on the same line.

% %-------------------------------------------------------------------------
% \subsection{Footnotes}

% Please use footnotes\footnote{This is what a footnote looks like.
% It often distracts the reader from the main flow of the argument.} sparingly.
% Indeed, try to avoid footnotes altogether and include necessary peripheral observations in the text (within parentheses, if you prefer, as in this sentence).
% If you wish to use a footnote, place it at the bottom of the column on the page on which it is referenced.
% Use Times 8-point type, single-spaced.


% %-------------------------------------------------------------------------
% \subsection{Cross-references}

% For the benefit of author(s) and readers, please use the
% {\small\begin{verbatim}
%   \cref{...}
% \end{verbatim}}  command for cross-referencing to figures, tables, equations, or sections.
% This will automatically insert the appropriate label alongside the cross-reference as in this example:
% \begin{quotation}
%   To see how our method outperforms previous work, please see \cref{fig:onecol} and \cref{tab:example}.
%   It is also possible to refer to multiple targets as once, \eg~to \cref{fig:onecol,fig:short-a}.
%   You may also return to \cref{sec:formatting} or look at \cref{eq:also-important}.
% \end{quotation}
% If you do not wish to abbreviate the label, for example at the beginning of the sentence, you can use the
% {\small\begin{verbatim}
%   \Cref{...}
% \end{verbatim}}
% command. Here is an example:
% \begin{quotation}
%   \Cref{fig:onecol} is also quite important.
% \end{quotation}

% %-------------------------------------------------------------------------
% \subsection{References}

% List and number all bibliographical references in 9-point Times, single-spaced, at the end of your paper.
% When referenced in the text, enclose the citation number in square brackets, for
% example~\cite{Authors14}.
% Where appropriate, include page numbers and the name(s) of editors of referenced books.
% When you cite multiple papers at once, please make sure that you cite them in numerical order like this \cite{Alpher02,Alpher03,Alpher05,Authors14b,Authors14}.
% If you use the template as advised, this will be taken care of automatically.

% \begin{table}
%   \centering
%   \begin{tabular}{@{}lc@{}}
%     \toprule
%     Method & Frobnability \\
%     \midrule
%     Theirs & Frumpy \\
%     Yours & Frobbly \\
%     Ours & Makes one's heart Frob\\
%     \bottomrule
%   \end{tabular}
%   \caption{Results.   Ours is better.}
%   \label{tab:example}
% \end{table}

% %-------------------------------------------------------------------------
% \subsection{Illustrations, graphs, and photographs}

% All graphics should be centered.
% In \LaTeX, avoid using the \texttt{center} environment for this purpose, as this adds potentially unwanted whitespace.
% Instead use
% {\small\begin{verbatim}
%   \centering
% \end{verbatim}}
% at the beginning of your figure.
% Please ensure that any point you wish to make is resolvable in a printed copy of the paper.
% Resize fonts in figures to match the font in the body text, and choose line widths that render effectively in print.
% Readers (and reviewers), even of an electronic copy, may choose to print your paper in order to read it.
% You cannot insist that they do otherwise, and therefore must not assume that they can zoom in to see tiny details on a graphic.

% When placing figures in \LaTeX, it's almost always best to use \verb+\includegraphics+, and to specify the figure width as a multiple of the line width as in the example below
% {\small\begin{verbatim}
%    \usepackage{graphicx} ...
%    \includegraphics[width=0.8\linewidth]
%                    {myfile.pdf}
% \end{verbatim}
% }


% %-------------------------------------------------------------------------
% \subsection{Color}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a discussion of the use of color in your document.

% If you use color in your plots, please keep in mind that a significant subset of reviewers and readers may have a color vision deficiency; red-green blindness is the most frequent kind.
% Hence avoid relying only on color as the discriminative feature in plots (such as red \vs green lines), but add a second discriminative feature to ease disambiguation.

% %------------------------------------------------------------------------
% \section{Final copy}

% You must include your signed IEEE copyright release form when you submit your finished paper.
% We MUST have this form before your paper can be published in the proceedings.

% Please direct any questions to the production editor in charge of these proceedings at the IEEE Computer Society Press:
% \url{https://www.computer.org/about/contact}.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{ref}
}

\end{document}
