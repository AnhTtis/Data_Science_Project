\begin{abstract}
The self-supervised pretraining paradigm has achieved great success in skeleton-based action recognition. However, these methods treat the motion and static parts equally, and lack an adaptive design for different parts, which has a negative impact on the accuracy of action recognition. To realize the adaptive action modeling of both parts, we propose an \textbf{Act}ionlet-Dependent \textbf{C}ontrastive \textbf{L}ea\textbf{r}ning method (ActCLR). The actionlet, defined as the discriminative subset of the human skeleton, effectively decomposes motion regions for better action modeling. In detail, by contrasting with the static anchor without motion, we extract the motion region of the skeleton data, which serves as the actionlet, in an unsupervised manner. Then, centering on actionlet, a motion-adaptive data transformation method is built. Different data transformations are applied to actionlet and non-actionlet regions to introduce more diversity while maintaining their own characteristics. Meanwhile, we propose a semantic-aware feature pooling method to build feature representations among motion and static regions in a distinguished manner. Extensive experiments on NTU RGB+D and PKUMMD show that the proposed method achieves remarkable action recognition performance. More visualization and quantitative experiments demonstrate the effectiveness of our method. Our project website is available at \url{https://langlandslin.github.io/projects/ActCLR/}

% The pretraining paradigm has achieved great success. However, these methods treat each part of the skeleton data equally. Thus, the information between motion and non-motion parts is coupled. It has led to the current dilemma of self-supervised action recognition.
%
% \wh{However, as the pretraining stage has to include a diversity constraint of the extracted features to prevent model collapse, it is difficult to further improve the performance of downstream tasks that heavily rely on the semantic consistency of features extracted from different views of the same data.}
% However, to prevent model collapse, the pretraining stage has to include a diversity constraint of the extracted features. The diversity constraint makes it difficult to further improve the performance of downstream tasks, which heavily relies on the semantic consistency of features extracted from different views of the same data.
%
% Therefore, we propose an actionlet-dependent contrastive learning method (ActCLR). This approach decouples the motion and non-motion regions to obtain better action modeling. By contrasting with the static anchor without motion, we unsupervised extract the motion region of the skeleton data, i.e., actionlet. Using actionlet, we design a motion-aware data transformation method. Different data transformations are applied to actionlet and non-actionlet regions to introduce more diversity while maintaining action semantic information.
% Meanwhile, we propose an actionlet-based feature pooling method that decouples the information of actionlet and non-actionlet regions. It makes the motion feature without the effect of the motionless regions. During training, a regularization constraint based on entropy minimization is added to the loss. We reduce the entropy of the feature space to obtain more confident output features.
%
% The feature space obtained by pretraining is \wh{augmented} through a self-distillation stage. 
% %
% \wh{In self-distillation, we optimize without the diversity constraint and reduce the cross entropy between the features of the teacher and the student network, which improves the semantic consistency on the features extracted by the student network.}
% %
% Besides, two new metrics are proposed to quantitatively evaluate the consistency and diversity of the feature space.
% %
% These metrics show that our method improves the \wh{semantic} consistency of the feature space of the previously pretrained network while maintaining \wh{feature} diversity.
% %
% In the finetuning stage, a gradient-based parameter-efficient finetuning method is applied \wh{to only optimize the important model parameters,} obtaining more satisfactory performance with fewer trainable parameters.
%
% Extensive experiments on NTU RGB+D and PKUMMD show that the proposed method achieves remarkable action recognition performance. Meanwhile, more visualization and quantitative experiments demonstrate the effectiveness of our method.
\end{abstract}