\input{tab/unsup}
\section{Experiment Results}

For evaluation, we conduct our experiments on the following two datasets: the NTU RGB+D dataset~\cite{shahroudy2016ntu,liu2019ntu} and the PKUMMD dataset~\cite{liu2020pku}. 
% Our goal is to obtain the feature encoder $f(\cdot)$ which can generate good feature representations for action recognition. 

\subsection{Datasets and Settings}
\noindent$\bullet$ \textbf{NTU RGB+D Dataset 60 (NTU 60)}~\cite{shahroudy2016ntu} is a large-scale dataset which contains 56,578 videos with 60 action labels and 25 joints for each body, including interactions with pairs and individual activities.

\vspace{1mm}

\noindent$\bullet$ \textbf{NTU RGB+D Dataset 120 (NTU 120)}~\cite{liu2019ntu} is an extension to NTU 60 and the largest dataset for action recognition, which contains 114,480 videos with 120 action labels. Actions are captured with 106 subjects with multiple settings using 32 different setups.


\vspace{1mm}

\noindent$\bullet$ \textbf{PKU Multi-Modality Dataset (PKUMMD)}~\cite{liu2020pku} covers a multi-modality 3D understanding of human actions. The actions are organized into 52 categories and include almost 20,000 instances. There are 25 joints in each sample. The PKUMMD is divided into part I and part II. Part II provides more challenging data, because the large view variation causes more skeleton noise.

To train the network, all the skeleton sequences are temporally down-sampled to 50 frames. The encoder $f(\cdot)$ is based on ST-GCN~\cite{yan2018spatial} with hidden channels of size 16, which is a quarter the size of the original model. The projection heads for contrastive learning and auxiliary tasks are all multilayer perceptrons, projecting features from 256 dimensions to 128 dimensions. $\tau_q$ is $0.1$ and $\tau_k$ is $0.04$. We employ a fully connected layer $\phi(\cdot)$ for evaluation.

\input{tab/unsup_ntu120.tex}

To optimize our network, Adam optimizer~\cite{newey1988adaptive} is applied, and we train the network on one NVIDIA TitanX GPU with a batch size of 128 for 300 epochs.

\input{tab/sup.tex}

\subsection{Evaluation and Comparison}
To make a comprehensive evaluation, we compare our method with other methods under variable settings.
\vspace{1mm}

\input{tab/trans}

\noindent\textbf{1) Linear Evaluation.}
In the linear evaluation mechanism, a linear classifier $\phi(\cdot)$ is applied to the fixed encoder $f(\cdot)$ to classify the extracted features. We adopt action recognition accuracy as a measurement. Note that this encoder $f(\cdot)$ is fixed in the linear evaluation protocol.

Compared with other methods in Tables~\ref{tab:unsupervised_ntu},~\ref{tab:unsupervised_ntu_60} and~\ref{tab:unsupervised_ntu_120}, our model shows superiority on these datasets. We find that the transformation that 3s-CrosSCLR~\cite{li20213d} and 3s-AimCLR~\cite{guo2021contrastive} design in the contrastive learning task is unified for different regions, which makes the data transformation interfere with the motion information. On the contrary, our method adopts MATS for semantic-aware motion-adaptive data transformation. Thus, the features extracted by our method maintain better action information which is more suitable for downstream tasks. 

\vspace{1mm}


\noindent\textbf{2) Supervised Finetuning.}
We first pretrain the encoder $f(\cdot)$ in the self-supervised learning setting, and then finetune the entire network. We train the encoder $f(\cdot)$ and classifier $\phi(\cdot)$ using complete training data.

Table~\ref{tab:supervised_ntu} displays the action recognition accuracy on the NTU datasets. This result confirms that our method extracts the information demanded by downstream tasks and can better benefit action recognition. 
In comparison with state-of-the-art supervised learning methods, our model achieves better performance.

\vspace{1mm}

% \noindent\textbf{\wh{3)} Semi-Supervised Approaches.}
% In semi-supervised learning, the training process utilizes both labeled data and unlabeled data. Generally, the encoder $f(\cdot)$ is pretrained with the contrastive learning task with full data, and then fine-tuned with the classifier $\phi(\cdot)$ with labeled data. To give a comprehensive and thorough evaluation, we conduct experiments under different settings, including $1\%$, $5\%$, $10\%$ and $20\%$ of labeled skeleton sequences. We also show the comparison results with the state-of-the-art methods. 

% In Table~\ref{tab:semi_supervised_pku} and~\ref{tab:semi_supervised_ntu}, we notice that with small subsets of the datasets, our method improves the accuracy considerably and performs better than the state-of-the-art methods. Especially with smaller training data, our method outperforms the state-of-the-art by large margins.

% \noindent\textbf{\wh{3)} Parameter-Efficient Finetuning.}
% We also first pretrain the encoder $f(\cdot)$, and after self-distillating the encoder $f(\cdot)$, employ the gradient-guided parameter-efficient finetuning.

% Table~\ref{tab:supervised_ntu} shows the action recognition accuracy on the NTU datasets. Our method achieves comparable performance to full model fine-tuning with fewer trainable parameters. Moreover, we can adjust the \wh{number} of parameters that need to be finetuned through the threshold, which makes the finetuning more controllable, as shown in Table~\ref{tab:effi}.

\vspace{1mm}

\noindent\textbf{3) Transfer Learning.} 
To explore the generalization ability, we evaluate the performance of transfer learning. In transfer learning, we exploit self-supervised task pretraining on the source data. Then we utilize the linear evaluation mechanism to evaluate on the target dataset. In linear evaluation, the encoder $f(\cdot)$ has fixed parameters without fine-tuning.

As shown in Table~\ref{tab:trans_pku}, our method achieves significant performance. Our method employs MATS to remove irrelevant information, and SAFP to retain information related to downstream tasks. This allows our encoder $f(\cdot)$ to obtain stronger generalization performance.

\vspace{1mm}

\noindent\textbf{4) Unsupervised Action Segmentation.} 
To explore the extraction of local features by our method, we used unsupervised action segmentation as an evaluation metric. We pre-train the encoder $f(\cdot)$ on the NTU 60 dataset. Then we utilize the linear evaluation mechanism to evaluate \wh{the results} on the PKUMMD dataset. In linear evaluation, the encoder $f(\cdot)$ has fixed parameters without fine-tuning.

As shown in Table~\ref{tab:seg_pkuII}, our method achieves significant performance. Because our method focuses on the main occurrence region of the action, it is possible to locate the actions out of the long sequence.

\input{tab/seg.tex}


\subsection{Ablation Study}
Next, we conduct ablation experiments to give a more detailed analysis of our proposed approach.

\vspace{1mm}

\noindent\textbf{1) Analysis of Motion-Adaptive Data Transformation.} 
Data transformation is very important for consistency learning. To explore the influence of motion-adaptive data transformations, we test the action recognition accuracy under different data transformations. 
%
As shown in Table~\ref{tab:data}, the motion-adaptive transformation can obtain better performance than full region (the whole skeleton data) in different noise settings. It is also observed that when the noise strength increases, our performance degradation is much smaller than that of full region. This indicates that the design is more robust to data transformation.
% the consistency of the feature space is further enhanced with actionlet-dependent data transformations. And under stronger data transformations, our method still maintains the motion semantic information. Accordingly, the performance of downstream tasks is improved. 

To explore the influence of different data transformations on the contrastive learning effect, we test the action recognition accuracy under different data transformation combinations. As shown in Table~\ref{tab:com}, the consistency of the feature space is further enhanced with more data transformations. Thus, the performance of the downstream task is improved.

\input{tab/data.tex}

\vspace{1mm}

\noindent\textbf{2) Analysis of Semantic-Aware Feature Pooling.} 
To explore the semantic-aware feature pooling, we perform this pooling on different streams. Table~\ref{tab:effi} shows the results of accuracy of action recognition under different settings. We note that better performance is obtained with offline, as it makes offline to generate better positive sample features for contrastive learning. Using this module in online reduces the benefits exposed by the non-actionlet transformation.
% In offline network feature extraction, $\kappa$ controls the attention of the actionlet. A smaller $\kappa$ makes the feature pooling close to the global average pooling, while a larger $\kappa$ makes the features extracted only from the actionlet region.
% %
% Table~\ref{tab:effi} shows the results of different $\kappa$. 
% We notice that using only SAFP can get better performance than using GAP. And the combination of both is used to obtain more informative features.

\input{tab/effi}

\vspace{1mm}

% \noindent\textbf{3) Analysis of Unsupervised Actionlet Selection.} 
% We explored the effect of thresholding on actionlet selection for feature learning. As shown in Table~\ref{tab:threshold}, when the threshold is small, many joints are selected causing the actionlet to contain many static regions. When the threshold value is relatively large, the selected area is relatively small, making the motion information lost.
% \input{cvpr2023-author_kit-v1_1-1/latex/tab/thresholds.tex}

% \vspace{1mm}

\noindent\textbf{3) Analysis of Actionlet and Non-Actionlet Semantic Decoupling.}   
In Fig.~\ref{fig:acc}, we show the performance of extracting only actionlet region information and non-actionlet region information for action recognition. The accuracy of the actionlet region for action recognition is comparable to the accuracy of the whole skeleton data. In contrast, the performance of the features of non-actionlet regions for action recognition is much lower. This shows that the actionlet area does contain the main motion information. 
\input{fig/acc.tex}

\vspace{1mm}

\noindent\textbf{4) Visualization of Average Motion and Actionlet.} 
\wh{Fig}.~\ref{fig:mean} shows a visualization of the average motion and actionlet respectively. The average motion has no significant motion information and serves as a background. The actionlet, shown in Fig.~\ref{fig:act}, selects the joints where the motion mainly occurs. Our actionlet is spatio-temporal, because the joints with motion may change when the action is performed.
\input{fig/mean.tex}