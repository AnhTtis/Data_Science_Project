\section{Actionlet-Based Unsupervised Learning}
In this section, we introduce unsupervised actionlet for contrastive representation learning, which is based on MoCo v2 described in Sec.~\ref{sec:contra}.
First, we describe the unsupervised actionlet extraction method. Then, the motion-adaptive data transformation and the semantic-aware feature pooling are introduced.

% \input{cvpr2023-author_kit-v1_1-1/latex/fig/actionlet.tex}

% we employ the unsupervised actionlet selection module to generate actionlet regions. And the data is enhanced with the motion-adaptive data transformation module. In contrastive learning, the offline network $f_k(\cdot)$ is to provide a stable and accurate anchor feature for the online network $f_q(\cdot)$. Thus, in feature pooling, we utilize the semantic-aware feature pooling method to generate offline features. 

% The feature extraction can be expressed as follows:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathbf{z}^i_q &= (g_q \circ \text{GAP} \circ f_q \circ \text{ADT})(\mathbf{X}^i_q),\\
% \mathbf{z}^i_k &= (g_k \circ \text{AFP} \circ f_k)(\mathbf{X}^i_k).
% \end{aligned}
% \end{equation}
% where $\mathbf{z}^i_q = (g_q \circ \text{GAP} \circ f_q \circ \text{ADT})(\mathbf{X}^i_q)$ and $\mathbf{z}^i_k = (g_k \circ \text{AFP} \circ f_k)(\mathbf{X}^i_k)$.

% We exploit cross-entropy loss to reduce the entropy of the feature space to constrain the features.:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{L}_{\text{KL}}(\mathbf{p}_q^i, \mathbf{p}_k^i) = -\mathbf{p}_k^i \log \mathbf{p}_q^i,
% \end{aligned}
% \end{equation}
% where $\mathbf{p}_q^i = \text{SoftMax}(\text{Linear}(\mathbf{z}^i_q)/\tau_q)$ and $\mathbf{p}_k^i = \text{SoftMax}(\text{Linear}(\mathbf{z}^i_k)/\tau_k)$. $\text{Linear}(\cdot)$ is a linear projection. Since $\tau_k$ is much smaller than $\tau_q$, the entropy of $\mathbf{p}_k^i$ is smaller than the entropy of $\mathbf{p}_q^i$. By using $\mathbf{p}_k^i$ as a guide, the entropy of $\mathbf{p}_q^i$ can be reduced. In turn, the feature space is subject to an entropy constraint.

% The total loss function can be expressed as:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{L} = \mathcal{L}_{\text{CL}} + \lambda \mathcal{L}_{\text{KL}},
% \end{aligned}
% \end{equation}
% where $\lambda$ is a hyperparameter.

%
% Following ~\cite{li20213d,rao2021augmented}, we employ \textit{MoCo} as our backbone for skeleton data:
% This algorithm adopts momentum mechanism~\cite{he2020momentum}. There are three branches, optimized based on \textit{MoCo}. And joint, motion and bone data are input respectively to assist in distinguishing positive pairs from similar negative samples. We introduce the steps of \textit{MoCo} in detail:
% \begin{itemize}
%     \item[1)] The network inputs the original data $\mathbf{X}^i$ and performs data transformation $\mathcal{T}$ to obtain two different views $\mathbf{X}^i_q$ and $\mathbf{X}^i_k$.
%     \item[2)] Two encoders are employed, an online encoder $f_q(\cdot)$ and an offline encoder $f_k(\cdot)$ to extract features: $\mathbf{z}^i_q = g_q(f_q(\mathbf{X}^i_q))$ and $\mathbf{z}^i_k = g_k(f_k(\mathbf{X}^i_k))$, where $g_q(\cdot)$ is an online projector and $g_k(\cdot)$ is an offline projector. The offline networks $f_k(\cdot)$ and $g_k(\cdot)$ are updated by the momentum of the online networks $f_q(\cdot)$ and $g_q(\cdot)$. by $\hat{f} \leftarrow \alpha \hat{f} + (1 - \alpha) f$, where $\alpha$ is a momentum coefficient. 
%     \item[3)]  A memory bank $\mathbf{M} = \{\mathbf{m}^i\}^M_{i=1}$ is utilized to store negative features. The features extracted from the data in each batch are stored in the memory bank, and the bank is continuously updated using a first-in first-out strategy.
%     \item[4)] Following recent works~\cite{chen2020simple}, \textit{MoCo} exploits InfoNCE loss to optimize:
%     \begin{equation}
%     \label{equ:info}
%     \begin{aligned}
%     \mathcal{L}_{\text{CL}} = - \log \frac{\exp(\text{sim}(\mathbf{z}^i_q, \mathbf{z}^i_k) / \tau)}{\exp(\text{sim}(\mathbf{z}^i_q, \mathbf{z}^i_k)/ \tau) + K},
%     \end{aligned}
%     \end{equation}
%     where $K = \sum_{j=1}^M \exp(\text{sim}(\mathbf{z}^i_q, \mathbf{m}^j)/ \tau)$ and $\tau$ is a temperature hyper-parameter.
% \end{itemize}



% \subsection{Preliminary}
% An \textit{actionlet} is defined as a conjunctive structure of skeleton joints. It should be highly representative of one action and highly discriminative compared to other actions. Therefore, the actionlet is considered as representing the main motion region.

% Formally, supposing the $i^{th}$ skeleton sequence is $\mathbf{X}^i = \{\mathbf{x}^i_1, \dots, \mathbf{x}^i_T\}$, where $\mathbf{x}^i_t \in \mathbb{R}^{C \times V}$ represents the $t^{th}$ frame. $C$ is the number of channels and $V$ is number of joints. An actionlet is denoted as a subset of joints $\mathcal{S} \subseteq \{1,2,\dots,V\}$. In the previous work, the actionlet is selected by supervision. It is related to the action category. For an action category $c$, its actionlet is chosen according to this probability $P_{\mathcal{S}} (\mathbf{y}^i = c | \mathbf{x}^i)$, where $\mathbf{y}^i$ is the label of $\mathbf{x}^i$. This probability should be large for the data in $\mathcal{X}_c$ and be small for the data not in $\mathcal{X}_c$. $\mathcal{X}_c$ is the set of sequences which labels are $c$. 

% The actionlet makes features focus on joints in movement. It allows action information to be highlighted. However, the selection of supervised actionlets is action class dependent. This makes it inapplicable to the field of unsupervised learning. And actionlets are not generated adaptively based on action data. Actionlets are shared between sequences in the same class of skeletons. Consequently, the choice of actionlet will be less accurate. For example, the action of raising a hand may be raising the left hand or the right hand, rather than different people raising the same hand. Additionally, these actionlets ignore the temporal dynamics and focus only on spatial information. However, the motion region of the skeleton data may change over time. Therefore, it is important to design an unsupervised spatio-temporal actionlet selection method. And, using this unsupervised actionlet benefits contrastive learning.

\subsection{Unsupervised Actionlet Selection}
\label{sec:actionlet}
Traditional actionlet mining methods rely on the action label to identify the motion region, which cannot be employed in the unsupervised learning context.
%
Inspired by contrastive learning, we propose an unsupervised spatio-temporal actionlet selection \wh{method} to mine the motion region as shown in Fig.~\ref{fig:overview}.
%
The actionlet is \wh{obtained} by comparing the differences between \wh{an} action sequence and the static sequence where we assume no motion takes place.

Specifically, we introduce the average motion as the static anchor, which is regarded as the sequence without motion. Resort to this, we contrast the action sequences between the static anchor to realize actionlet localization. The details of the proposed method are described below.
\vspace{1mm}

\noindent\textbf{Average Motion as Static Anchor.}
In the process of obtaining the sequence without action occurrence, we observe that most of the action sequences have no action in most of the regions. The motion usually occurs in a small localized area, such as the hand or head.
%
\wh{Therefore, as shown in Fig.~\ref{fig:mean}, we can easily obtain the static anchor via average all the actions in the dataset, since most of the sequence has no motion in most of the regions and this average is a relatively static sequence.} 
It is formalized as:
\begin{equation}
    \label{equ:info}
    \begin{aligned}
    \Bar{\mathbf{X}} = \frac{1}{N}\sum_{i=1}^N (\mathbf{X}^i),
    \end{aligned}
\end{equation}
where $\mathbf{X}^i$ is the $i^{th}$ skeleton sequence and $N$ is the size of the dataset.
\vspace{1mm}
%It can also be seen in Fig.~\ref{fig:mean} that the average motion is a relatively static sequence of motions. In this case, the average motion can be viewed as a static anchor without any movement.

\input{fig/pipeline}

\noindent\textbf{Difference Activation Mapping for Actionlet Localization.}
To obtain the region where the motion \wh{takes place}, we input the skeleton sequence $\mathbf{X}^i$ with the average motion $\Bar{\mathbf{X}}$ into the offline encoder $f_k(\cdot)$ to obtain its corresponding dense features $\mathbf{h}^i_{ctv} = f_k(\mathbf{X}^i)$ and $\Bar{\mathbf{h}}_{ctv} = f_k(\Bar{\mathbf{X}})$, where $c$ means channel dimension, $t$ temporal dimension\wh{,} and $v$ joint dimension.
%
After global average pooling (GAP), we then apply the offline projector $g_k(\cdot)$ to obtain global features $\mathbf{z}^i = g_k(\text{GAP}(\mathbf{h}^i_{ctv}))$ and $\Bar{\mathbf{z}} = g_k(\text{GAP}(\Bar{\mathbf{h}}_{ctv}))$.
%
Then we calculate the cosine similarity of these two features. 
%
This can be formalized as:
\begin{equation}
    \label{equ:cos}
    \begin{aligned}
    \text{sim}(\mathbf{z}^i, \Bar{\mathbf{z}}) = \frac{\langle \mathbf{z}^i, \Bar{\mathbf{z}} \rangle}{\|\mathbf{z}^i\|_2\|\Bar{\mathbf{z}}\|_2},
    \end{aligned}
\end{equation}
where $\langle \cdot, \cdot \rangle$ is the inner product. 

To find the region where this similarity can be reduced, we back-propagate and reverse the gradient of this similarity to the dense feature $\mathbf{h}^i_{ctv}$. 
%
These gradients then are global average pooled over the temporal and joint dimensions to obtain the neuron importance weights $\alpha^i_c$:
\begin{equation}
    \label{equ:sim}
    \begin{aligned}
    \Delta \mathbf{h}^i_{ctv} & = \frac{\partial (-\text{sim}(\mathbf{z}^i, \Bar{\mathbf{z}}))}{\partial \mathbf{h}^i_{ctv}},\\
    \alpha^i_c &= \frac{1}{T \times V} \sum_{t=1}^T \sum_{v=1}^V \sigma (\Delta \mathbf{h}^i_{ctv}),
    \end{aligned}
\end{equation}
where $\sigma(\cdot)$ is the activation function.

These importance weights capture the magnitude of the effect of each channel dimension on the final difference. Therefore, these weights $\alpha^i_c$ are considered difference activation mapping. 
%
We perform a weighted combination of the difference activation mapping and dense features as follows:
\begin{equation}
    \label{equ:wc}
    \begin{aligned}
    \mathbf{A}^i_{tv} = \sigma\left(\sum_{c=1}^C \alpha^i_c \mathbf{h}^i_{ctv}\right) \mathbf{G}_{vv},
    \end{aligned}
\end{equation}
where $\sigma(\cdot)$ is the activation function and $\mathbf{G}_{vv}$ is the adjacency matrix of skeleton data for importance smoothing. The linear combination of maps selects features that have a negative influence on the similarity. The actionlet region is the area where the value of the
generated actionlet $\mathbf{A}^i_{tv}$ exceeds a certain threshold, while the
non-actionlet region is the remaining part.
%
% Therefore, the large value in $\mathbf{H}^i_{tv}$ corresponds to the region with the large difference from the average motion. We select the areas that exceed a certain threshold as actionlets:
% \begin{equation}
%     \label{equ:actionlet}
%     \begin{aligned}
%     \mathbf{A}^i_{tv} = \frac{\text{Clip}(\mathbf{H}^i_{tv}, [0, k])}{k},
%     \end{aligned}
% \end{equation}
% where $\text{Clip}$ is employed to resize $\mathbf{H}^i_{tv}$ into $[0,1]$ and $k$ is the threshold of activation. 

% Grad-CAM~\cite{selvaraju2017grad} also applies gradient back-propagation. In Grad-CAM, the activation regions are visualized according to the categories. 
%
% Instead, we mine the region that differs most from the average motion without any label reliance. The region that has a large difference compared to it can be considered as the region where the motion occurs, \textit{i.e.}, actionlet.

\subsection{Actionlet-Guided Contrastive Learning}

To take full advantage of the actionlet, we propose an actionlet-dependent contrastive learning method, shown in Fig.~\ref{fig:overview}.
%
We impose different data transformations for different regions by a motion-adaptive data transformation strategy module (MATS). 
%
Moreover, the semantic-aware feature pooling module (SAFP) is proposed to aggregate the features of actionlet region for \wh{better} action modeling.
\vspace{1mm}

\noindent\textbf{Motion-Adaptive Transformation Strategy (MATS).} 
\label{sec:mats}
In contrastive learning, data transformation $\mathcal{T}$ is crucial for semantic information extraction and generalization capacity. 
%
How to design more diverse data transformations while maintaining relevant information for downstream tasks is still a challenge.
%
Too simple data transformation is limited in numbers and modes and cannot obtain rich augmented patterns. However, data transformations that are too difficult may result in loss of motion information.
%
\zjh{To this end}, we propose motion-adaptive data transformations for skeleton data based on actionlet.
%
For different regions, we propose two transformations, actionlet transformation and non-actionlet transformation.

\vspace{1mm}

$\bullet$ \textbf{Actionlet Transformation $\mathcal{T}_{\text{act}}$}: 
%
Actionlet data transformations are performed within the actionlet regions. Inspired by the previous work~\cite{guo2021contrastive}, we adopt four spatial data transformations \{Shear, Spatial Flip, Rotate, Axis Mask\}; two temporal data transformations \{Crop, Temporal Flip\}; and two spatio-temporal data transformations \{Gaussian Noise, Gaussian Blur\}.

Besides, Skeleton AdaIN is proposed as a mixing method of global statistics. 
%
We randomly select two skeleton sequences and then swap the spatial mean and temporal variance of the two sequences. 
%
This transformation is widely used in style transfer~\cite{huang2017arbitrary}.
%
Here, we are inspired by the idea of style and content decomposition in style transfer and regard the motion-independent information as style and the motion-related information as content. Therefore, we use Skeleton AdaIN to transfer this motion independent noise between different data.
%
The noisy pattern of the data is thus augmented by this transfer method.
%
This transformation can be formalized as:
\begin{equation}
    \label{equ:info}
    \begin{aligned}
    \mathbf{X}^i_{\text{adain}} = \sigma(\mathbf{X}^j) \left(\frac{\mathbf{X}^i - \mu(\mathbf{X}^i)}{\sigma(\mathbf{X}^i)}\right) + \mu(\mathbf{X}^j),
    \end{aligned}
\end{equation}
where $\sigma(\cdot)$ is the temporal variance\wh{,} $\mu(\cdot)$ is the spatial mean, and $\mathbf{X}^j$ is a randomly selected sequence. All these data transformations maintain the action information.

\vspace{1mm}

$\bullet$ \textbf{Non-Actionlet Transformation $\mathcal{T}_{\text{non}}$}: 
To obtain stronger generalization, several extra data transformations are applied to the non-actionlet regions in addition to the above data transformation.

We apply an intra-instance data transformation \{Random Noise\} and an inter-instance data transformation \{Skeleton Mix\}. The random Noise has larger variance. Skeleton Mix is an element-wise data mixing method, including Mixup~\cite{zhang2017mixup}, CutMix~\cite{yun2019cutmix}, and ResizeMix~\cite{ren2022simple}. Because these transformations are performed on non-actionlet regions, they do not change the action semantics. Therefore, the transformed data are used as positive samples with the original data.

% In adversarial noise, the transformed data $\mathbf{X}^i_\text{ad}$ can be expressed as:
% \begin{equation}
% \label{equ:trans}
% \mathbf{X}^i_\text{ad} = \mathbf{X^i} + \mathbf{N},
% \end{equation}
% where $\mathbf{N}$ is the adversarial noise initialized to $\mathbf{0}$.

% We update the parameters of $\mathbf{N}$ by adversarial attack. Following the Fast Gradient Sign Method (FGSM)~\cite{goodfellow2014explaining}, we hope to make the data transformation to attack the similarity to fool the encoder $f(\cdot)$. Therefore, the parameters of the three matrices are updated as follows:
% \begin{equation}
% \label{equ:at}
% \mathbf{A} = \mathbf{A} + \epsilon \cdot \text{sgn} (\nabla_{\mathbf{A}} \mathcal{L}_{V}),
% \end{equation}
% where $\epsilon$ is the learning rate, and we employ the same formula to update $\mathbf{R}$ and $\mathbf{N}$.
% \input{tab/loss}

% Mixup mixes two samples through a hyperparameter $\alpha$ from a uniform distribution $\mathbf{U}[0,1]$ as follows:
% \begin{equation}
%     \label{equ:info}
%     \begin{aligned}
%     \mathbf{X}^i_{\text{mixup}} = \alpha \mathbf{X}^i + (1 - \alpha) \mathbf{X}^j,
%     \end{aligned}
% \end{equation}
% where $\mathbf{X}^i$ and $\mathbf{X}^j$ are two randomly selected samples.

% CutMix utilizes the partial random mixing of two skeleton sequences in spatio-temporal dimensions.
% %
% On the temporal axis, we randomly sample a start frame $t_s$ and an end frame $t_e$. 
% %
% Meanwhile, we randomly sample several skeleton joints $\mathcal{N}$. 
% %
% We crop out this corresponding region of the sequence $\mathbf{x}^j$ and paste it into the sequence $\mathbf{x}^i$ as follows:
% \begin{equation}
%     \label{equ:info}
%     \begin{aligned}
%     \mathbf{X}^i_{\text{cutmix}} = \mathbf{M} \odot \mathbf{X}^i + (1 - \mathbf{M}) \odot \mathbf{X}^j,
%     \end{aligned}
% \end{equation}
% where $\mathbf{M}$ is a $[0,1]$ binary mask whose elements are 0 in the cropping region and 1 in other areas.

% ResizeMix resizes the action sequence $\mathbf{X}^j$ to the length of the cropping region, then inserts it as a patch into the cropping region of the sequence $\mathbf{X}^i$. 
% %
% It is formalized as:
% \begin{equation}
%     \label{equ:info}
%     \begin{aligned}
%     \mathbf{X}^i_{\text{resizemix}} = \text{Paste}(\mathbf{M} \odot \mathbf{X}^i, \text{Resize}(\mathbf{X}^j)),
%     \end{aligned}
% \end{equation}
% where $\text{Resize}(\cdot)$ indicates the resize function and $\text{Paste}(\cdot)$ indicates pasting the patch onto the cropping region.

$\bullet$ \textbf{Actionlet-Dependent Combination}:
To merge the data transformations of the two regions, we utilize actionlets to combine.
It is formalized as:
\begin{equation}
    \label{equ:info}
    \begin{aligned}
    \mathbf{X}^i_{\text{trans}} = \mathbf{A}^i_{tv} \odot \mathbf{X}^i_{\text{act}} + (1 - \mathbf{A}^i_{tv}) \odot \mathbf{X}^i_{\text{non}},
    \end{aligned}
\end{equation}
where $\mathbf{X}^i_{\text{trans}}$ is the final transformed data, $\mathbf{X}^i_{\text{act}}$ and $\mathbf{X}^i_{\text{non}}$ are transformed with actionlet transformations $\mathcal{T}_{\text{act}}$. Then, $\mathbf{X}^i_{\text{non}}$ is performed non-actionlet transformations $\mathcal{T}_{\text{non}}$. $\mathbf{A}^i_{tv}$ represents the actionlet.
\vspace{1mm}

\noindent\textbf{Semantic-Aware Feature Pooling (SAFP).} 
\label{sec:safp}
To extract the motion information more accurately, we propose \wh{a} semantic-aware feature pooling method along the spatial-temporal dimension. This method focuses only on the feature representation of the actionlet region, thus reducing the interference of other static regions for motion feature extraction. It is formalized as:
\begin{equation}
    \label{equ:abfp}
    \begin{aligned}
     \text{SAFP}(\mathbf{h}^i_{ctv})= \sum_{t=1}^T \sum_{v=1}^V \mathbf{h}^i_{ctv} \left(\frac{\mathbf{A}^i_{tv}}{\sum_{t=1}^T \sum_{v=1}^V \mathbf{A}^i_{tv}}\right).
    \end{aligned}
\end{equation}
% where $\kappa$ is a hyper-parameter. 

This semantic-aware feature aggregation approach effectively extracts motion information and makes the features more distinguishable. We utilize this semantic-aware feature pooling operation in the offline stream to provide accurate anchor features.

% \vspace{1mm}

\input{tab/sota}

\vspace{1mm}

\noindent\textbf{Training Overview.}
In this part, we conclude our framework of contrastive learning in detail: 
\begin{itemize}%[itemsep=2pt]
\setlength{\itemsep}{1.5pt} 
    \item[1)] Two encoders are pre-trained using MoCo v2~\cite{he2020momentum}, an online encoder $f_q(\cdot)$ and an offline encoder $f_k(\cdot)$.  \zjh{The online encoder is updated via back-propagation gradients, while the offline encoder is a momentum-updated version of the online encoder as described} in Sec.~\ref{sec:contra}.
    \item[2)] The offline network $f_k(\cdot)$ inputs the original data $\mathbf{X}^i$ and we employ the unsupervised actionlet selection module to generate actionlet regions $\mathbf{A}^i_{tv}$ in the offline stream in Sec.~\ref{sec:actionlet}.
    \item[3)] We perform data transformation $\mathcal{T}$ to obtain two different views $\mathbf{X}^i_q$ and $\mathbf{X}^i_k$. And we apply motion-adaptive transformation strategy (MATS) to enhance the diversity of $\mathbf{X}^i_q$ in Sec.~\ref{sec:mats}.
    \item[4)] For feature extraction, in online stream, $\mathbf{z}^i_q = (g_q \circ \text{GAP} \circ f_q \circ \text{MATS})(\mathbf{X}^i_q)$, where $g_q(\cdot)$ is an online projector and GAP is the global average pooling. To provide a stable and accurate anchor feature, we utilize the semantic-aware feature pooling (SAFP) method in Sec.~\ref{sec:safp} to generate offline features $\mathbf{z}^i_k = (g_k \circ \text{SAFP} \circ f_k)(\mathbf{X}^i_k)$, where $g_k(\cdot)$ is an offline projector. 
    % Two encoders are employed, an online encoder $f_q(\cdot)$ and an offline encoder $f_k(\cdot)$ to extract features: $\mathbf{z}^i_q = g_q(f_q(\mathbf{X}^i_q))$ and $\mathbf{z}^i_k = g_k(f_k(\mathbf{X}^i_k))$, where $g_q(\cdot)$ is an online projector and $g_k(\cdot)$ is an offline projector. The offline networks $f_k(\cdot)$ and $g_k(\cdot)$ are updated by the momentum of the online networks $f_q(\cdot)$ and $g_q(\cdot)$. by $\hat{f} \leftarrow \alpha \hat{f} + (1 - \alpha) f$, where $\alpha$ is a momentum coefficient. 
    \item[5)]  A memory bank $\mathbf{M} = \{\mathbf{m}^i\}^M_{i=1}$ is utilized to store offline features. The offline features extracted from the offline data in each batch are stored in the memory bank, and the bank is continuously updated using a first-in first-out strategy.
    \item[6)] Following recent works~\cite{mao2022cmd,zhang2022contrastive}, we exploit similarity mining to optimize:
    \begin{equation}
    \label{equ:info}
    \begin{aligned}
    &\mathcal{L}_{\text{KL}}(\mathbf{p}_q^i, \mathbf{p}_k^i) = -\mathbf{p}_k^i \log \mathbf{p}_q^i,\\
    &\mathbf{p}_q^i = \text{SoftMax}(\text{sim}(\mathbf{z}^i_q, \mathbf{M})/\tau_q),\\
    &\mathbf{p}_k^i = \text{SoftMax}(\text{sim}(\mathbf{z}^i_k, \mathbf{M})/\tau_k),
    \end{aligned}
    \end{equation}
    where $\text{sim}(\mathbf{z}^i_q, \mathbf{M}) = [\text{sim}(\mathbf{z}^i_q, \mathbf{m}^j)]^M_{j=1}$, which indicates the similarity distribution between feature $\mathbf{z}^i_q$ and other samples in $\mathbf{M}$. For the elements $\mathbf{p}_k^{ij}$ of $\mathbf{p}_k^i$ greater than the elements $\mathbf{p}_q^{ij}$ of $\mathbf{p}_q^i$, these corresponding features $\mathbf{m}^j$ in the memory bank are positive samples. This is because the network increases the similarity of the output with these features.
\end{itemize}


% \subsection{Training Paradigm}
% Our method training follows the following steps. To obtain more accurate actionlets, our network is pre-trained using \textit{MoCo v2}~\cite{guo2021contrastive}. Then we employ the unsupervised actionlet selection module to generate actionlet regions. And the data is enhanced with the motion-adaptive data transformation module. In contrastive learning, the offline network $f_k(\cdot)$ is to provide a stable and accurate anchor feature for the online network $f_q(\cdot)$. Thus, in feature pooling, we utilize the semantic-aware feature pooling method to generate offline features. 

% The feature extraction can be expressed as follows:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathbf{z}^i_q &= (g_q \circ \text{GAP} \circ f_q \circ \text{ADT})(\mathbf{X}^i_q),\\
% \mathbf{z}^i_k &= (g_k \circ \text{AFP} \circ f_k)(\mathbf{X}^i_k).
% \end{aligned}
% \end{equation}
% % where $\mathbf{z}^i_q = (g_q \circ \text{GAP} \circ f_q \circ \text{ADT})(\mathbf{X}^i_q)$ and $\mathbf{z}^i_k = (g_k \circ \text{AFP} \circ f_k)(\mathbf{X}^i_k)$.

% We exploit cross-entropy loss to reduce the entropy of the feature space to constrain the features.:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{L}_{\text{KL}}(\mathbf{p}_q^i, \mathbf{p}_k^i) = -\mathbf{p}_k^i \log \mathbf{p}_q^i,
% \end{aligned}
% \end{equation}
% where $\mathbf{p}_q^i = \text{SoftMax}(\text{Linear}(\mathbf{z}^i_q)/\tau_q)$ and $\mathbf{p}_k^i = \text{SoftMax}(\text{Linear}(\mathbf{z}^i_k)/\tau_k)$. $\text{Linear}(\cdot)$ is a linear projection. Since $\tau_k$ is much smaller than $\tau_q$, the entropy of $\mathbf{p}_k^i$ is smaller than the entropy of $\mathbf{p}_q^i$. By using $\mathbf{p}_k^i$ as a guide, the entropy of $\mathbf{p}_q^i$ can be reduced. In turn, the feature space is subject to an entropy constraint.

% The total loss function can be expressed as:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{L} = \mathcal{L}_{\text{CL}} + \lambda \mathcal{L}_{\text{KL}},
% \end{aligned}
% \end{equation}
% where $\lambda$ is a hyperparameter.

% \noindent\textbf{Random Adversarial Training.} In previous work on self-distillation, student networks are usually learned by stochastic gradient descent. However, this approach easily leads to overfitting of the student network to the teacher network. To enhance the generalization performance of the student network, we employ random adversarial learning for consistency regularization. Random adversarial training randomly reverses the gradient direction with a given probability during backpropagation.

% Specifically, during gradient backpropagation, we randomly reverse the gradient direction of the current layer with probability $p$. This makes layers with reversed gradients to increase the self-distillation loss, while layers that are not reversed remain trained normally. Therefore, this is equivalent to a kind of adversarial training between different layers. With this adversarial training, the network tends to converge to a flat parameter region. This enables the model to have better generalization performance and its robustness to disturbances is enhanced.

% Meanwhile, we adopt a two-stage training strategy. At the beginning of training, we employ normal stochastic gradient descent to learn the parameters of the student network. When the loss is small and the network starts to overfit, we apply random adversarial training to regularize the network. This training strategy can effectively alleviate the overfitting phenomenon of the network and enhance the generalization ability of the student network.

% \subsection{Consistency and Diversity Analysis of Self-Supervised Representations}
% To further quantitatively analyze the feature space of the pretrained model, in this section, we analyze the previous \wh{loss functions of self-supervised learning} and propose \wh{two new metrics to measure the consistency and diversity in the feature space}.

% The losses of the previous self-supervised tasks can be decomposed into two parts, consistency loss (alignment loss) and diversity loss (uniform loss):
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{L} = \mathcal{L}_{\text{con}} + \mathcal{L}_{\text{div}}.
% \end{aligned}
% \end{equation}
% Consistency loss $\mathcal{L}_{\text{con}}$ constrains \wh{different views of data or data with the same semantic labels} to extract \wh{similar} features, whereas the diversity loss $\mathcal{L}_{\text{div}}$ \wh{enlarges the distance of features extracted from different data/semantics, to prevent the feature space from collapsing.} 

% %
% The loss decomposition of canonical pretrained models is listed in Table~\ref{tab:loss}.
% %
% Previous works~\cite{huang2021towards, wang2022chaos} point out that the consistency loss is an upper bound on the loss of downstream tasks when the diversity loss is in a range.
% %
% Therefore, consistency and diversity are the two most important metrics for evaluating \wh{the feature spaces of self-supervised learning}.

% \wh{However, directly using the loss function as the metric to measure these two dimensions may be inaccurate as they are adopted as the constraint in the training and may incur overfitting.}
% %
% Thus, we propose two \wh{novel} metrics based on the attention mechanism. 
% %
% For the skeleton data $\mathbf{x}^i$, the representation is defined as $\mathbf{h}^i = f(\mathbf{x}^i) \in \mathbb{R}^d$.
% %
% We choose coordinates far from the mean in the feature space to generate the attention map:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \alpha^i_j = \mathbbm{1}\left[\mathbf{h}^i_j > \mu^i + \lambda \sigma^i\right],
% \end{aligned}
% \end{equation}
% where $\mathbbm{1}[\cdot]$ is an indicator function which outputs 1 if we meet the condition, and 0 otherwise, $\mu^i = \frac{1}{d} \sum_{j=1}^d \mathbf{h}^i_j$, and $\sigma^i = \sqrt{\frac{1}{d-1} \sum_{j=1}^d (\mathbf{h}^i_j - \mu^i)^2}$. 
% %
% We then exploit \wh{this} attention map to obtain measures of consistency and diversity. 

% For \wh{consistency}, we mainly consider the overlap of attention maps between different two views.
% %
% We use mIoU to calculate the overlap ratio\wh{:}
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{C} = \frac{1}{N} \sum_{i=1}^N \frac{|\alpha^i \cap \hat{\alpha}^i|}{|\alpha^i \cup \hat{\alpha}^i|},
% \end{aligned}
% \end{equation}
% where $\alpha^i$ and $\hat{\alpha}^i$ are \wh{views} from different data transformations of the same data.
% %
% Note that to prevent overfitting, the data transformation used here is not \wh{the one used} in the pretraining.

% For \wh{diversity}, we first obtain the mean attention map of each data. 
% %
% This mean attention map reflects the average importance of different features. 
% %
% If a feature is always important, then it may extract some general information. 
% %
% \wh{In constract, unimportant features may not extract any critical information.}
% %
% On this basis, we compute the entropy of the mean feature map to infer the diversity of this feature space:
% \begin{equation}
% \label{equ:info}
% \begin{aligned}
% \mathcal{D} = -\frac{1}{d} \sum_{j=1}^d \alpha_j \log \alpha_j.
% \end{aligned}
% \end{equation}
% If $\mathcal{D}$ is small, it means that some features are always important while some are always unimportant, which indicates that a model collapsing has occurred. 

% Table~\ref{tab:cd} shows the consistency and diversity of a series of pretraining methods and the accuracy of downstream tasks.
% %
% \wh{It is observed} that larger \wh{semantic} consistency \wh{of different views} does lead to better downstream task performance. 
% %
% % \wh{Contrastive} learning, \wh{\textit{e.g.} SkeletonCLR and AimCLR}, cannot achieve optimal consistency due to the false negative samples problem~\cite{arora2019theoretical, chen2021incremental} \wh{when the diversity constraint is calculated}. 
% % %
% % Reconstruction tasks, \textit{e.g.} MAE, usually only apply masking and lack strong data transformations.
% $\mathcal{C}$ of SkeletonCLR and AimCLR are less than 0.2, according their accuracy is below 80, which is in line with the false negative sample assumption in~\cite{arora2019theoretical, chen2021incremental}.
% %
% Unlike them, our method can \wh{successfully} improve the consistency of pretrained models while maintaining diversity.
% %
% This is because our method employs enhanced data transformation and does not apply negative data, \wh{\textit{i.e.}, not including a diversity loss}, but only \wh{narrows} the feature distance between positive samples.

% \subsection{Gradient-Guided Parameter-Efficient Finetuning}
% In the finetuning of downstream tasks, previous works usually employ full-model finetuning or linear evaluation \wh{ finetuning, \textit{i.e.}, \wh{only finetuning} the last MLP layer}.
% %
% However, the full model update leads to \wh{huge} consumption of space-time resources.
% %
% It not only requires a lot of training time, but also requires a lot of memory usage.
% %
% While linear evaluation \wh{finetuning} is fast to train but the performance is sub-optimal. 
% %
% To achieve \wh{optimal} finetuning parameter efficiency while maintaining downstream effectiveness, we aim to design a parameter efficient finetuning strategy, which \wh{makes} the trade-off between efficiency and effectiveness.
% %
% We propose a gradient-guided parameter update strategy.
% %
% \wh{Via only updating the important parameters, the strategy} enables better performance while keeping fewer training parameters.

% Specifically, in each gradient \wh{back-propagation}, we calculate an indicator $\mathcal{I}$ from the modulus length of the current gradient and the number of parameters of the gradient. 
% %
% When the indicator $\mathcal{I}$ exceeds a threshold, this parameter is added to the optimizer. We formalize it as:
% \begin{equation}
%     \label{equ:info}
%     \begin{aligned}
%     \mathcal{I} = \sqrt{\frac{\text{tr}(\mathbf{\Delta W}^T \mathbf{\Delta W})}{M}},
%     \end{aligned}
% \end{equation}
% where $\mathbf{\Delta W} = \frac{\partial \mathcal{L}}{\partial \mathbf{W}}$ and $M$ is the number of parameters in $\mathbf{\Delta W}$. 
% %
% This indicator can quantitatively measure whether the parameters are important to the final loss function.
% %
% Finally, we update the parameters in the optimizer using standard stochastic gradient descent. 
% %
% Through the choice of thresholds, we control the amount of finetuning parameters. 
% \input{tab/sup}


