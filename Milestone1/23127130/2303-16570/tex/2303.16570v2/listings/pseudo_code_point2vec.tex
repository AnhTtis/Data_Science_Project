\begin{lstlisting}[language=Python, float=*t, caption=\textbf{PyTorch-inspired pseudocode for \name{} pre-training.}, label={lst:point2vec}, escapechar=!]
# N: batch size (512)
# S: number of groups/embeddings (64)
# E: embedding feature dimension (384)

for point_cloud in data_loader:
    # point cloud embedding
    center_points = self.FPS(point_cloud)  # (N, S, 3)
    # (N, S, 32, 3)
    point_patches = self.KNN(point_cloud, center_points)  
    # (N, S, E) (Fig. 2, !\colorsquare{m_pointnet}!)
    patch_embeddings = self.mini_pointnet(point_patches)  
    # (N, S, E)
    pos_embeddings = self.pos_encoder(center_points)  
  
    # masking
    # (N, S, E)
    mask_embeddings = self.mask_embedding.expand(N, S, E)  
    mask = generate_mask(center_points)  # (N, S) boolean
  
    # targets
    with torch.no_grad():
        # (12, N, S, E) (Fig. 2, !\colorsquare{m_green}!)
        teacher_states = self.teacher(patch_embeddings, 
                                      pos_embeddings)  
        target_layers = [F.layer_norm(x, (E,)) for x in 
                         teacher_states[6:]]  # [(N, S, E)]
        targets = torch.stack(target_layers).mean(0) # (N, S, E)
        targets = F.layer_norm(targets, (E,))  # (N, S, E)
        
    # predictions
    last_student_state = self.student(  # (N, S, E) (Fig. 2, !\colorsquare{m_blue}!)
        patch_embeddings[~mask].reshape(N, -1, E),
        pos_embeddings[~mask].reshape(N, -1, E)
    )[-1]
    predictions = self.decoder(  # (N, S, E) (Fig. 2, !\colorsquare{m_red}!)
        mask_embeddings.index_put([~mask], 
        last_student_state.reshape(-1, E)),
        pos_embeddings
    )[-1]
  
    # optimization
    loss = F.smooth_l1_loss(predictions[mask], targets[mask])
    loss.backward()
    optimizer.step()
  
    # update teacher weights
    ema_update(student, teacher)
\end{lstlisting}