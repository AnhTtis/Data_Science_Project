\begin{table}
	\centering
	\caption{
		\textbf{Hyperparameters for Classification.}
            We use the same hyperparameters when fine-tuning \name{} and \datavec{} on ModelNet40\,\cite{wu2015modelnet40} and ScanObjectNN\,\cite{uy2019scanobjectnn}.
            When training from scratch, we increase the learning rate to $1 \times 10^{-3}$ and do not freeze the encoder.
	}
	\label{tab:hyperparameters_classification}
	\begin{tabular}{lll}
		\toprule
		Epochs                   & $150$ \\
		Batch size              & $32$                  \\
		Optimizer               & AdamW               \\
		Learning rate           & $3 \times 10^{-4}$  \\
		Weight decay            & $0.05$                \\
		Learning rate schedule  & cosine              \\
		Learning rate warm-up   & $10$ epochs           \\
  \arrayrulecolor{black!10}\midrule\arrayrulecolor{black}
            points & $1024$ \tiny($2048$ for ScanObjNN) \\
            $n$ (center points) & $64$ \tiny($128$ for ScanObjNN) \\
            $k$ ($k$-NN grouping) & $32$ \\
		mini-PointNet 1st MLP dim          & $128$, $256$                  \\
		mini-PointNet 2nd MLP dim          & $512$, $384$                  \\
  \arrayrulecolor{black!10}\midrule\arrayrulecolor{black}
		Encoder layers          & $12$                  \\
		Encoder dimension       & $384$                 \\
		Encoder heads           & $6$                 \\
		Encoder drop path           & $0\%,\ldots,20\%$                 \\
		Encoder frozen & $100$ epochs \\
  \arrayrulecolor{black!10}\midrule\arrayrulecolor{black}
            Feature aggregation & \footnotesize{mean-,max-pool.} \\
            Classification head dim & \footnotesize{$256$, $256$, \#classes} \\
            Classification head dropout & $50\%$ \\
            Label smoothing & $0.2$ \\
		\bottomrule
	\end{tabular}
\end{table}
\begin{table}
	\centering
	\caption{
		\textbf{Hyperparameters for Part Segmentation.}
            We use the same hyperparameters when fine-tuning \name{} and \datavec{} on ShapeNetPart\,\cite{yi16siggraph}.
	}
	\label{tab:hyperparameters_part_segmentation}
	\begin{tabular}{lll}
		\toprule
		Epochs                   & $300$ \\
		Batch size              & $16$                  \\
		Optimizer               & AdamW               \\
		Learning rate           & $2 \times 10^{-4}$  \\
		Weight decay            & $0.05$                \\
		Learning rate schedule  & cosine              \\
		Learning rate warm-up   & $10$ epochs           \\
  \arrayrulecolor{black!10}\midrule\arrayrulecolor{black}
            points & $2048$ \\
            $n$ (center points) & $128$ \\
            $k$ ($k$-NN grouping) & $32$ \\
		mini-PointNet 1st MLP dim          & $128$, $256$                  \\
		mini-PointNet 2nd MLP dim          & $512$, $384$                  \\
  \arrayrulecolor{black!10}\midrule\arrayrulecolor{black}
		Encoder layers          & $12$                  \\
		Encoder dimension       & $384$                 \\
		Encoder heads           & $6$                 \\
		Encoder drop path           & $0\%, \ldots, 20\%$                 \\
  \arrayrulecolor{black!10}\midrule\arrayrulecolor{black}
            Feature propagation & \tiny{described in main paper} \\
            Segmentation head dim & \footnotesize{$512$, $256$, \#classes} \\
            Segmentation head dropout & $50\%$ \\
		\bottomrule
	\end{tabular}
\end{table}
