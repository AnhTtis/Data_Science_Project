\section{Related Work}

\parag{Self-Supervised Learning.}
Recently, self-supervised learning gained much attention due to its promise to learn meaningful data representations without any human annotations.
At the heart of self-supervised learning is the \emph{pretext} task, offering a vast range of diverse options.
One such line of work investigates contrastive learning objectives\,\cite{chen2020simclr, he2020moco, oord2018cpc, tian2020cmc, tian2020makes}, \ie maximizing feature similarity across multiple views of the same training sample while simultaneously minimizing the similarity to other training samples.
Contrastive learning approaches typically rely on a careful choice of data augmentations, negative sample mining, or large batch sizes\,\cite{grill2020BYOL}.
Addressing these limitations, student--teacher approaches\,\cite{grill2020BYOL, chen2021simsiam, bardes2022vicreg, baevski2022data2vec, caron2021dino} follow a \emph{joint-embedding} architecture, \ie two copies of the same network are trained to produce similar latent representations for two views of the identical input. 
Among them and most important to our work is data2vec\,\cite{baevski2022data2vec}, which relies on a teacher first generating targets by predicting latent representations using the complete view of the input and a student which predicts these targets using only a \emph{masked} view of the same input.
Inspired by data2vec's %
flexibility across a wide range of modalities, in this paper, we seek to unlock the full potential of data2vec-like pre-training for point clouds by specifically taking the unique characteristics of point clouds into account.

\parag{Self-Supervised Learning on Point Clouds.}
The success of self-supervised learning in 2D vision\,\cite{grill2020BYOL, caron2021dino, he2022mae, chen2020simclr, baevski2022data2vec,bao2021beit, bardes2022vicreg,oord2018cpc}, natural language processing\,\cite{devlin2018bert, baevski2022data2vec}, and speech\,\cite{baevski2020wav2vec, baevski2022data2vec} has inspired a number of recent works proposing self-supervised learning frameworks for point cloud understanding tasks.
Among them, contrastive self-supervised frameworks are typically deployed for room-scale pre-training.
The pioneering work of Xie \etal\cite{xie2020pointcontrast} contrasts corresponding 3D points from multiple partial views of a reconstructed static scene, showing impressive improvements when fine-tuned on several scene-level downstream tasks.
Extending upon this, Hou \etal\cite{hou2021contrastivescene} propose to leverage both point-level correspondences and spatial contexts of 3D scenes.
In contrast to room-scale pre-training, we see a line of work developing self-supervised methods tailored towards single object understanding tasks\,\cite{wang2021occo, huang2021strl, sharma2020ssl, yu2021pointbert, pang2022pointmae, zhang2022pointm2ae, xue2022ulip, liu2022maskpoint, eckart2021parae, rao2020pointglr}.
They typically use the inherent structure and geometry of 3D point clouds to learn meaningful representations, \eg by %
explicitly reconstructing point cloud patches using the Chamfer distance\,\cite{zhang2022pointm2ae, pang2022pointmae}, discriminating masked points from noise\,\cite{liu2022maskpoint}, or performing point cloud completion for occluded regions\,\cite{wang2021occo}.
Another line of work additionally leverages multi-modal information to improve the latent representation of 3D point clouds, \ie incorporating knowledge from models on 2D images\,\cite{dong2023act,xue2022ulip,zhang2022pointclip,zhang2023i2pmae} or text descriptions\,\cite{xue2022ulip,zhang2022pointclip}.
The advances of above methods are orthogonal to our approach \name{} as it operates on point clouds only.
Most relevant to our work are Transformer-based self-supervised learning approaches on point clouds.
Due to the sucess of pre-trained Transformer architectures in various domains\,\cite{devlin2018bert, baevski2022data2vec, he2022mae, caron2021dino}, we recently see a shift towards pre-training Transformer-based approaches for point clouds\,\cite{liu2022maskpoint, yu2021pointbert, ma2022pointmlp, zhang2022pointm2ae}.
Among them, Point-BERT\,\cite{yu2021pointbert} introduces a standard ViT-like\,\cite{dosovitskiy2020vit} backbone to point clouds and 
extends BERT pre-training to point clouds\,\cite{devlin2018bert}.
Point-MAE\,\cite{pang2022pointmae} and Point-M2AE\,\cite{zhang2022pointm2ae} follow the masked autoencoder approach proposed by He \etal\cite{he2022mae}.
In contrast to these methods, we do not explicitly reconstruct masked point cloud patches but predict contextualized targets in the latent feature space, circumventing the need to define sophisticated distance metrics to compare point cloud patches.








