\section{Introduction}
In this work, we address the task of self-supervised representation learning on 3D point clouds.
With the ever increasing availability of affordable consumer-grade 3D sensors, point clouds are becoming a widely adopted data representation for capturing real-world objects and environments\,\cite{dai2017scannet, dehghan2021arkitscenes, Armeni16CVPR, behley2019semantickitti, caesar2020nuscenes}.
They provide accurate 3D geometry information, making them a valuable input for many applications in the field of robotics, autonomous driving\,\cite{behley2019semantickitti, caesar2020nuscenes}, and AR/VR applications.
The 3D computer vision community has made impressive progress by developing 3D-centric approaches which directly process 3D point clouds to semantically understand 3D objects and environments\,\cite{qi2016pointnet, qi2017pointnetplusplus, choy2019minkowskinet, schult23mask3d}.
However, these approaches typically rely on fully-supervised training \emph{from scratch}\,\cite{xie2020pointcontrast}, requiring time-consuming and labor-intensive human annotations.
For example, semantically annotating a single room-scale scene of the ScanNet dataset takes about $22$ minutes \cite{dai2017scannet}.
This results in a lack of large-scale annotated point cloud datasets, making it challenging to learn strong representations from limited data.

At the same time, self-supervised training has shown impressive results in natural language processing\,\cite{zhilin2019xlnet,devlin2018bert}, speech\,\cite{baevski2020wav2vec, hsu2021hubert}, and 2D vision\,\cite{he2022mae,grill2020BYOL,baevski2022data2vec,caron2021dino,chen2020simclr}, enabling learning of meaningful representations from massive unlabeled datasets without any human annotations.
Only recently, we have seen self-supervised methods being successfully applied to Transformer architectures for 2D vision\,\cite{caron2021dino, baevski2022data2vec, he2022mae} and 3D point clouds\,\cite{pang2022pointmae, yu2021pointbert, zhang2022pointm2ae}.
Baevski \etal propose data2vec\,\cite{baevski2022data2vec}, a modality-agnostic self-supervised learning framework showing competitive performance in speech recognition, image classification, and natural language understanding.
Data2vec uses a joint-embedding architecture\,\cite{grill2020BYOL, baevski2022data2vec, caron2021dino} with a \emph{student} Transformer encoder and a \emph{teacher} network parameterized as the exponential moving average of the student weights.
Specifically, the teacher first predicts latent representations using an uncorrupted view of the input, which the student network then predicts from a masked view of the same input.


\input{figures/teaser/teaser_small_gcpr.tex}
In this paper, our aim is to apply data2vec-like pre-training to point clouds. %
The key difference to top-performing approaches for point cloud representation learning such as Point-MAE\,\cite{pang2022pointmae}, Point-M2AE\,\cite{zhang2022pointm2ae} and Point-BERT\,\cite{yu2021pointbert} is the target representation.
The self-attention in the student Transformer encoder of data2vec generates \emph{contextualized} feature targets that contain \emph{global} information of the entire input.
In contrast, Point-MAE\,\cite{pang2022pointmae} and Point-M2AE\,\cite{zhang2022pointm2ae} explicitly reconstruct only \emph{local} point cloud patches, and Point-BERT\,\cite{yu2021pointbert} is restricted to a fixed-sized vocabulary of token representations.
To apply data2vec\,\cite{baevski2022data2vec} on point clouds, we use the same underlying 3D-specific Transformer model as Point-BERT\,\cite{yu2021pointbert} and Point-MAE\,\cite{pang2022pointmae}.
In experiments, we show that these modality-specific adaptations to data2vec already enable competitive performance compared to highly 3D-specific self-supervised approaches\,\cite{yu2021pointbert, pang2022pointmae, zhang2022pointm2ae, liu2022maskpoint, wang2021occo}.
Encouraged by these promising results, we perform a subsequent analysis that reveals a crucial and point cloud specific shortcoming that restricts data2vec's representation learning capabilities:
data2vec uses masked embeddings in the student network which carry positional information.
Unlike images, text, and speech, the positional information in point clouds contains semantic meaning, namely 3D point locations (\reffig{early_leakage}).
Feeding masked embeddings with positional information into the student network therefore reveals the overall object shape to the student which makes the masking operation far less effective, as also reported by Pang \etal \cite{pang2022pointmae} in the context of masked autoencoders for point clouds.
Based on this analysis, we propose \name{} that effectively addresses the leakage of positional information to the student and thus unleashes the full potential of data2vec-like pre-training for point clouds.
To this end, we exclude masked embeddings from the student network.
This prevents the overall object shape from being revealed, while also decreasing the computational cost.
Instead, we introduce a shallow decoder which processes masked embeddings together with the student's outputs and which is trained to regress the representations of the teacher (\reffig{teaser}).

Evaluating the quality of the learned representations on downstream tasks is a crucial step for analyzing self-supervised methods.
After pre-training on the ShapeNet dataset\,\cite{chang2015shapenet}, our experiments demonstrate that \name{} outperforms other self-super-vised methods on both the ModelNet40\,\cite{wu2015modelnet40} and ScanObjectNN\,\cite{uy2019scanobjectnn} shape classification benchmarks.
Additionally, \name{} achieves state-of-the-art performance on few-shot classification on ModelNet40 and competitive results on Part Segmentation on ShapeNetPart\,\cite{yi16siggraph}.
These findings suggest that the learned representations are strong and transferable, indicating that \name{} is a promising approach for self-supervised point cloud representation learning.

To summarize, our contributions are:
\textbf{(1)} We extend the seminal work data2vec\,\cite{baevski2022data2vec} to the point cloud domain.~\textbf{(2)}~In our experiments, we discover a crucial shortcoming of data2vec that hampers its representation learning capabilities for point clouds: Masked embeddings leak positional information to the student, revealing the overall object shapes even under heavy masking.~\textbf{(3)} We propose \name{} which unleashes the full potential of data2vec-like pre-training for self-supervised representation learning by addressing the aforementioned shortcomings. %
\emakefirstuc{\name{}} learns strong and transferable features in a self-supervised manner, outperforming self-supervised approaches on several downstream tasks.

