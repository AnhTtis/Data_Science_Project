\begin{figure*}[t!]
\includegraphics[width=1.0\linewidth, trim={0 0.3cm 0 0.1cm}, clip]{figures/architecture/architecture.pdf}
\vspace{-15pt}
\caption{
\textbf{Point2Vec pre-training.}
Our model divides the input point cloud into %
point patches using farthest point sampling (FPS) and $k$-NN aggregation.
We obtain patch embeddings by applying a mini-PointNet\,\colorsquare{m_pointnet} to each point patch (\emph{right}).
The teacher Transformer encoder\,\colorsquare{m_green} infers a contextualized %
representation for all patch embeddings which, after normalization and averaging over the last $K$ Transformer layers, serve as training targets.
The student's input is a masked view on the input data, \ie we randomly mask out a ratio of patch embeddings and only pass the remaining embeddings into the student Transformer encoder\,\colorsquare{m_blue}.
After applying a shallow decoder\,\colorsquare{m_red} on the outputs of the student, padded with learned mask embeddings\,\protect\maskembedding{}, we train the student and decoder to predict the latent teacher representation of the patch embeddings.
\vspace{-10pt}
}
\label{fig:model}
\end{figure*}