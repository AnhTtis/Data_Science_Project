\begin{figure*}[t!]
\includegraphics[width=1.0\linewidth, trim={0 0.3cm 0 0.1cm}, clip]{figures/architecture/architecture.pdf}
\caption{
\textbf{Illustration of our \name{} pre-training method.}
Our model divides the input point cloud into spatially contiguous point patches using farthest point sampling (FPS) and $k$-NN aggregation.
We obtain patch embeddings by applying a mini-PointNet\,\colorsquare{m_pointnet} to each point patch (\emph{right}).
The teacher Transformer encoder\,\colorsquare{m_green} infers a contextualized latent representation for all patch embeddings which, after normalization and averaging over the last $K$ Transformer layers, serve as training targets.
The input for the student is a masked view on the input data, \ie we randomly mask out a given ratio of patch embeddings and only pass the remaining patch embeddings into the student Transformer encoder\,\colorsquare{m_blue}.
After applying a shallow decoder\,\colorsquare{m_red} on the outputs of the student, padded with learned mask embeddings\,\protect\maskembedding{}, we train the student and decoder to predict the latent teacher representation of the patch embeddings.
}
\label{fig:model}
\end{figure*}