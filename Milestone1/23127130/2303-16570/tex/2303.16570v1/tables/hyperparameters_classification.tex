\begin{table}
	\centering
	\caption{
		\textbf{Hyperparameters for Classification.}
            We use the same hyperparameters when fine-tuning \name{} and \datavec{} on ModelNet40\,\cite{wu2015modelnet40} and ScanObjectNN\,\cite{uy2019scanobjectnn}.
            When training from scratch, we increase the learning rate to $1 \times 10^{-3}$ and do not freeze the Transformer encoder.
	}
	\label{tab:hyperparameters_classification}
	\begin{tabular}{lll}
		\toprule
		Epochs                   & $150$ \\
		Batch size              & $32$                  \\
		Optimizer               & AdamW               \\
		Learning rate           & $3 \times 10^{-4}$  \\
		Weight decay            & $0.05$                \\
		Learning rate schedule  & cosine              \\
		Learning rate warm-up   & $10$ epochs           \\
  \arrayrulecolor{black!10}\midrule\arrayrulecolor{black}
            points & $1024$ \footnotesize($2048$ for ScanObjNN) \\
            $n$ (center points) & $64$ \footnotesize($128$ for ScanObjNN) \\
            $k$ ($k$-NN grouping) & $32$ \\
		mini-PointNet 1st MLP dim          & $128$, $256$                  \\
		mini-PointNet 2nd MLP dim          & $512$, $384$                  \\
  \arrayrulecolor{black!10}\midrule\arrayrulecolor{black}
		Encoder layers          & $12$                  \\
		Encoder dimension       & $384$                 \\
		Encoder heads           & $6$                 \\
		Encoder drop path           & $0\%,\ldots,20\%$                 \\
		Encoder frozen & $100$ epochs \\
  \arrayrulecolor{black!10}\midrule\arrayrulecolor{black}
            Feature aggregation & mean- \& max-pooling \\
            Classification head dim & $256$, $256$, \#classes \\
            Classification head dropout & $50\%$ \\
            Label smoothing & $0.2$ \\
		\bottomrule
	\end{tabular}
\end{table}
