\section{Experiments}\label{sec:experiments}
In this section, we describe the self-supervised pre-training of \name{} on ShapeNet\,\cite{chang2015shapenet} (\refsec{pretraining}).
Next, we compare \name{} with top-performing self-supervised approaches and our baseline method \datavec{} on three well-established datasets and four downstream tasks (\refsec{main_results}).
Finally, we put the spotlight on the architectural changes from our data2vec adaptation for point clouds to our proposed model \name{} which address the unique challenges of 3D point clouds (\refsec{analysis}).
In the supplementary material, we provide detailed hyperparameters of our model.
Code and checkpoints will be made available.


\subsection{Self-Supervised Pre-training}
\label{sec:pretraining}
Following the pre-training protocol propagated by Point-BERT\,\cite{yu2021pointbert}, Point-MAE\,\cite{pang2022pointmae} and Point-M2AE\,\cite{zhang2022pointm2ae}, we pre-train \name{} on the training split of ShapeNet\,\cite{chang2015shapenet} consisting of \num{41952} synthetic 3D meshes of $55$ categories, \eg `\emph{chair}', `\emph{guitar}', `\emph{airplane}'.
We set the number of Transformer blocks to $12$ with an internal dimension of $384$. %
To pre-train our point-based approach, we uniformly sample \num{8192} points from the surfaces of the objects and then resample \num{1024} points using farthest point sampling\,\cite{qi2017pointnetplusplus}.
During the point cloud embedding step we sample $n$$=$$64$ center points and $k$$=$$32$ nearest neighbors.
We train \name{} with a batch size of $512$ for $800$ epochs using the AdamW\,\cite{loshchilov2018adamw} optimizer and a cosine learning rate decay\,\cite{loshchilov2017ICLR} with a maximal learning rate of $10^{-3}$ after $80$ epochs of linear warm-up.
For \datavec{}, we increase the batch size and learning rate to $2048$ and $2$$\times$$10^{-3}$, respectively, as this empirically led to better results.
Following data2vec\,\cite{baevski2022data2vec}, we set $\beta$$=$$2$ for the Smooth L1 loss and average the last $K$$=$$6$ blocks of the teacher.
We use minimal data augmentations during pre-training: we randomly scale the input with a factor between $[0.8, 1.2]$ and rotate around the gravity axis.
Pre-training takes roughly $18$\,hours on a single V100 GPU.
\input{tables/classification_modelnet40.tex}
\input{tables/part_segmentation_shapenet.tex}
\subsection{Main Results on Downstream Tasks}
\label{sec:main_results}
In order to evaluate the effectiveness of \name{}'s self-supervised learning capabilities, we test \name{} against top-performing self-supervised methods on four different downstream tasks on well-established benchmarks.
To that end, we discard the teacher network as well as the decoder and append a task-specific head to the student network.
We then fine-tune the full network end-to-end for the specific task.
We provide detailed hyperparameters for all downstream tasks in the supplementary material.

\paragraph{Synthetic Shape Classification.}
\input{figures/oa_learning_curve_modelnet40/oa_learning_curve_modelnet40.tex}
After pretraining on ShapeNet, we finetune our model for shape classification on ModelNet40\,\cite{wu2015modelnet40} consisting of \num{12311} \emph{synthetic} 3D models of $40$ semantic categories.
We obtain the semantic class label by passing the concatenated mean- and max-pooled output of the Transformer encoder into a $3$-layer MLP and finetune the whole network end-to-end.
We use minimal data augmentations consisting only of resampling $1024$ points with farthest point sampling, applying random anisotropic scaling of up to $40\%$, centering at the origin, and rescaling to the unit sphere.
Other commonly used augmentations did not improve performance, \eg random rotations around the axis of gravity and random translations are detrimental as ModelNet40 instances are canonically oriented.
During the point cloud embedding step we sample $n$$=$$64$ center points and $k$$=$$32$ nearest neighbors.
In~\reftab{modelnet_results}, we report a new state-of-the-art for shape classification on ModelNet40\,\cite{wu2015modelnet40} among self-supervised methods by a large margin of $+1.3\%$ without voting\,\cite{zhang2022pointm2ae, pang2022pointmae, yu2021pointbert}.
Interestingly, pre-training with \datavec{} results only in marginal improvements ($+0.3\%$ without voting) over the same model trained \emph{from scratch} on ModelNet40.
Unlike \datavec{}, we observe that \name{} unleashes the full potential of data2vec-like pre-training on ModelNet40 by achieving substantial performance gains of $+1.7\%$ over the baseline trained from scratch.
In~\reffig{oa_learning_curve_modelnet40}, 
we plot the accuracy per training epoch of \name{}, \datavec{}, as well as our baseline trained \emph{from scratch} on ModelNet40.
We observe that \name{} outperforms our strong baselines by a consistent margin throughout the entire training.
We conclude that \name{} effectively learns strong feature representations on ShapeNet, resulting in a significantly accelerated adaptation to the fine-tuning task (\reffig{oa_learning_curve_modelnet40}) as well as strong performance gains of $+2.2\%$\,mAcc over the baseline only trained on ModelNet40 (\reftab{modelnet_results}).

\paragraph{Real-World Shape Classification.}
\input{tables/classification_scanobjectnn.tex}
Next, we fine-tune \name{} on ScanObjectNN\,\cite{uy2019scanobjectnn} containing \num{2902} \emph{real-world} object scans of $15$ semantic classes.
In contrast to shape classification on ModelNet40, we do not resample points but use all \num{2048} points and sample $n$$=$$128$ center points for the point cloud embedding step. %
We found more aggressive scaling to be detrimental and use random anisotropic scaling of up to $10\%$. %
Although pre-trained on synthetic data, \reftab{scanobjectnn_results} shows that \name{} generalizes well to cluttered real-world data and achieves state-of-the-art performance among self-supervised methods by a significant margin of $+1.1\%$ on \texttt{PB-T50-RS}, the most difficult variant of the dataset.
Furthermore, we observe that pre-training \name{} on ShapeNet plays a crucial role to its strong performance.
Compared to the baseline trained from scratch on ScanObjectNN, pre-training with \name{} achieves an impressive performance gain of $+3.2\%$.
We again report significant improvements of \name{} over \datavec{} of up to $+2.3\%$.


\paragraph{Few-Shot Classification.}
\input{tables/few_shot_modelnet40.tex}
Following the standard evaluation protocol proposed by Sharma \etal\,\cite{sharma2020ssl}, we test the few-shot capabilities of \name{} in a $m$-way, $n$-shot setting.
To this end, we randomly sample $m$ classes and select $n$ instances for training at random for each of these classes. 
For testing, we randomly pick $20$ unseen instances from each of the $m$ support classes.
We provide the standard deviation over $10$ independent runs.
In \reftab{modelnet_fewshot_results}, we report a new state-of-the-art by improvements up to $+1.3\%$ in the most difficult $10$-way $10$-shot setting.
\emakefirstuc{\name{}} clearly outperforms the \datavec{} baseline in all settings.
We conclude that \name{} learns rich feature representations which are also well suited for transfer learning in a low-data regime.

\paragraph{Part Segmentation.}
Finally, we address the task of part segmentation, which assigns a semantic part label to each point in a 3D point cloud of a single object.
For this purpose, we employ a simple segmentation head that is similar to the segmentation head in Point-MAE\,\cite{pang2022pointmae}.
First, we average the outputs of the 4th, 8th, and 12th Transformer blocks to incorporate features from multiple levels of abstraction.
We then concatenate the mean- and max-pooling of the $n$ averaged token outputs, along with the one-hot encoded class label of the object, to obtain a global feature vector.
At the same time, we up-sample the $n$ averaged outputs from the corresponding center points to all points using a PointNet\texttt{++}\,\cite{qi2017pointnetplusplus} \emph{feature propagation layer}, which uses inverse distance weighting and a shared MLP to produce a local feature vector for each point.
Finally, we concatenate the global feature vector with each local feature vector and a shared MLP predicts a part label for each point.
In~\reftab{ShapeNetPart}, we report competitive results on ShapeNetPart\,\cite{yi16siggraph} which consists of \num{16881} 3D models of $16$ semantic categories.
Apart from Point-M2AE\,\cite{zhang2022pointm2ae}, \name{} outperforms all other self-supervised methods.
We hypothesize that Point-M2AE's multi-scale U-Net like architecture\,\cite{ronneberger2015unet} enables to learn more expressive spatially localized features which results in slightly better scores ($+0.2$\,mIoU$_I$).
Since \name{} relies on a standard single-scale Transformer backbone, we see multi-scale Transformers for 3D point clouds as an interesting orthogonal improvement, similar to the advances in 2D vision\,\cite{zhang2021longformer,fan2021msvit,li2022msvit2,chen2021crossvit} extending vision Transformers\,\cite{dosovitskiy2020vit} with multi-scale capabilities.
\subsection{Analysis}
\label{sec:analysis}
\input{tables/ablation_architecture.tex}

\paragraph{Leakage of Positional Information.}
\input{figures/early_leakage/early_leakage_fig.tex}
The main limitation of \datavec{} is that it directly feeds masked embeddings, along with their positional information, to the student network, which undermines the effectiveness of masking.
To visualize this problem, we show a representative example in \reffig{early_leakage}(a).
Revealing the positions of masked patches %
of the chair inadvertently weakens the learning objective because it allows the student to rely on the positional information instead of truly learning to predict the teacher's representations of the corresponding masked-out patches.
To mitigate this issue, \name{} excludes masked embeddings from the student and only subsequently feeds them to the decoder.
As a result, %
several sections of the chair in \reffig{early_leakage}(b) are effectively concealed from the student network, leading to a more resilient learning framework.
In \reftab{architecture_ablation}, we report that \name{} outperforms our baseline \datavec{} by a significant margin of up to $+2.0\%$.
In particular, we observe that the decoder itself provides consistent improvements, but the key contribution of \name{} is to conceal positional information from the student. 
Complementary to our findings, He \etal\,\cite{he2022mae} show that moving masked embeddings to a deferred shallow decoder reduces memory requirements and training time significantly.
Our findings align with those of Pang \etal\,\cite{pang2022pointmae}, who found similar benefits for masked autoencoders on point clouds.





\input{figures/representation_qualitative/representation_qualitative.tex}
\parag{Masking Strategy.}
\input{tables/ablation_masking.tex}
The masking strategy defines which of the student's input embeddings are masked (\reffig{model}, \protect\inlinegraphics{tables/misc/masked_token.pdf}).
In this study, we investigate two variants of masking strategies with different masking ratios: \emph{random} masking and \emph{block} masking.
For random masking, we mask out a specified ratio of embeddings for the student.
In contrast, block masking masks out a random embedding and its nearest neighbors such that the specified masking ratio is achieved.
This strategy puts focus on masking out spatially contiguous regions of the point cloud whereas random masking is independent of position.
Our findings, summarized in \reftab{ablation_masking_ratio}, reveal that random masking with a $65\%$ masking ratio performs best for both ModelNet40 and ScanObjectNN, while block masking lags behind.
We attribute this to the high level of ambiguity that arises when masking a spatially contiguous region, resulting in several potential point clouds that could have given rise to the masked input.
While we seek a challenging pretext task to learn rich representations, ambiguity should not be the primary source of difficulty.

\parag{Visualization of representations learned by \name{}.}
In \reffig{quali_representations}, we show qualitative examples of representations of ModelNet40 instances after pre-training on ShapeNet.
Both a random initialization and \datavec{} pre-training show a strong positional bias, whereas \name{} exhibits a stronger semantic grouping without being trained on downstream dense prediction tasks.
Unlike \datavec{}, \name{} conceals positional information from the student, forcing it to learn more about the semantics of the data, resulting in more semantically meaningful representations.%
