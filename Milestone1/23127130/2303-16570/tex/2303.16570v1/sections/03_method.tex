\input{figures/architecture/architecture_fig.tex}
\section{Method}

The aim of this work is to unlock the full potential of data2vec-like\,\cite{baevski2022data2vec} pre-training on point clouds by addressing point cloud specific challenges.
To achieve this, we first summarize the technical concepts of data2vec (\refsec{method_d2v}) and show how to learn rich representations on point clouds using data2vec pre-training (\refsec{method_d2v_pcl}).
Finally, we propose \name{}, which accounts for the point cloud specific limitations of data2vec (\refsec{method_p2v}).

\subsection{Data2vec}\label{sec:method_d2v}
Data2vec\,\cite{baevski2022data2vec} is designed to pre-train Transformer-based models, which involve a feature encoder that maps the input data to a sequence of embeddings.
These embeddings are subsequently passed to a standard Transformer encoder to generate the final latent representations.
During pre-training, two versions of the Transformer encoder are kept: a \emph{student} and a \emph{teacher}.
The teacher is a momentum encoder, \ie its parameters $\Delta$ track the student's parameters $\theta$ by being updated after each training step according to an exponential moving average (EMA) rule\,\cite{caron2021dino, baevski2022data2vec, grill2020BYOL, he2020moco}: $\Delta \leftarrow \tau \Delta + (1-\tau)\theta$,
where $\tau \in [0,1]$ is the EMA decay rate.
The teacher provides the training targets, which the student predicts given a corrupted version of the same input.

In a first step, the teacher encodes the uncorrupted input sequence.
The training targets are then constructed by averaging the outputs of the last $K$ blocks of the teacher, which are normalized beforehand to prevent a single block from dominating the sum.
Due to the self-attention layers, these targets are \emph{contextualized}, \ie they incorporate global information from the whole input sequence.
This is an important difference to other masked-prediction methods such as BERT\,\cite{devlin2018bert} and MAE\,\cite{he2022mae}, where the targets only comprise local information, \eg a word or an image patch. %

The student is given a masked version of the same input, where some of the embeddings in the input sequence are substituted by a special learned \emph{mask embedding}. %
The student's task is to predict the targets corresponding to the masked parts of the input.
The model is trained by optimizing a Smooth L1 loss on the regressed targets. %







\subsection{Data2vec for Point Clouds}\label{sec:method_d2v_pcl}

To apply data2vec to point clouds, we utilize the same underlying model as Point-BERT\,\cite{yu2021pointbert} and Point-MAE\,\cite{pang2022pointmae}.
This model is well suited for data2vec pre-training: it extracts a sequence of patch embeddings from the input point cloud and feeds it to a standard Transformer encoder.
For downstream tasks, we append a task-specific head to the Transformer encoder (\refsec{experiments}).
Next, we describe the point cloud embedding and the Transformer in detail and conclude with a summary of data2vec for point clouds.


\parag{Point Cloud Embedding.}
First, we sample $n$ center points from the input point cloud using farthest point sampling (FPS)\,\cite{qi2017pointnetplusplus}.
Grouping the center points' $k$-nearest neighbors ($k$-NN) in the point cloud yields $n$ contiguous \emph{point patches}, \ie sub-clouds of $k$ elements.
Next, we normalize the point patches by subtracting the corresponding center point from the patch's points.
This untangles the positional and the structural information.
To account for the permutation-invariant property of point clouds, we employ a mini-PointNet\,\cite{qi2016pointnet} (\reffig{model}, \emph{right}) that maps each normalized point patch to a \emph{patch embedding}.

The mini-PointNet involves the following steps:
First, we map each point of a patch to a feature vector using a shared MLP.
Then, we concatenate max-pooled features to each feature vector.
The resulting feature vectors are then passed through a second shared MLP and a final max-pooling layer to obtain the patch embedding.

\paragraph{Transformer Encoder.}
The central component of the model is a standard Transformer encoder.
The patch embeddings form the input sequence to the Transformer encoder.
Since the point patches are normalized, the patch embeddings carry no positional information;
therefore, a two-layer MLP maps each center point to a position embedding, which is then added to the corresponding patch embedding.
Due to the special importance of positional information in point clouds, the position embeddings are added again before each subsequent Transformer block to ensure that the positional information is incorporated at every step of the encoding process.

\paragraph{\emakefirstuc{\datavec{}}.}

To establish a baseline, we apply the unmodified data2vec approach to the previously described underlying model of Point-BERT and Point-MAE.
Going forward, we will refer to this approach as \datavec{}.


\subsection{\emakefirstuc{\name{}}}\label{sec:method_p2v}
In \reffig{model}, we present the complete pipeline of our \name{} model.
Directly applying data2vec to point cloud data without modifications is not optimal, as the position embeddings are also added to the mask embeddings, revealing the overall shape of the point cloud to the student.
As positions are the only features for point clouds, this makes the masking far less effective, as noted by Pang \etal \cite{pang2022pointmae} in the context of masked autoencoders.

To solve this issue, we adopt an approach inspired by MAE\,\cite{he2022mae}, where we only feed the non-masked embeddings to the student\,\colorsquare{m_blue}.
A separate decoder\,\colorsquare{m_red}, implemented as a shallow Transformer encoder, takes the output of the student and the previously held-back masked embeddings\,\maskembedding{} as input and predicts the training targets.
In contrast to \datavec{}, this approach does not suffer from leaking positional information from the masked-out point patches to the student.
Moreover, utilizing an MAE-inspired setup provides additional benefits:
First, the student is more computationally efficient, as it only needs to process the non-masked embeddings.
Second, the model's inputs during fine-tuning are more similar to those during pre-training because the inputs during pre-training are no longer dominated by masked embeddings which are absent during fine-tuning.
This likely makes the learned representations more transferable to downstream tasks.
