\begin{abstract}
This supplementary material contains further ablation studies on the efficiency of pre-training data and the selection of hyperparameters during pre-training and fine-tuning on downstream tasks.
Our code and model will be made publicly available for research purposes.
\end{abstract}

\section{Further Ablation Studies}

\paragraph{Pretext Task.}
In the main paper, we have only explored self-supervised pre-training on the ShapeNet dataset.
However, ShapeNet also contains class labels which could instead be used for a fully supervised classification-based pre-training.
As can be seen in \reftab{pretext_tasks}, this yields a significantly worse performance than using \name{} or even than directly training \emph{from scratch}.
We can also pre-train using \name{} directly on ModelNet40, which constitutes roughly a quarter of ShapeNet's size.
Still, we see improved downstream performances, indicating that the \name{} pretext task is meaningful for pre-training.

\paragraph{Warm-Up EMA Rate.}
During pre-training, we linearly warm-up the EMA rate $\tau$ over the first $\tau_n$ epochs from its initial value $\tau_0$ to its final value $\tau_e$ \,\cite{baevski2022data2vec}.
This approach is based on the idea that we should update the teacher network more frequently at the start of training since the feature representations are not yet well-established.
In\,\reftab{ema_tau_ablation}, we show overall accuracy scores on ModelNet40 and the \texttt{PB-T50-RS} variant of ScanObjectNN using various values for $\tau_n$.
Our findings suggest that $\tau_n$ is a crucial hyperparameter for achieving effective pre-training with \name{}.
In\,\reftab{hyperparameters}, we provide the EMA rates employed by our baseline \datavec{} and \name{}, respectively.
\input{tables/ablation_pretext_task_and_dataset}
\input{tables/ablation_ema_tau}
\input{figures/confusion_matrix/confusion_matrix.tex}
\paragraph{Target Layer Aggregation.}
During training of \datavec{}, as well as \name{}, we need to specify which layers of the teacher should be defined as the target.
The target is constructed by averaging the last $K$ layers, where Baevski \etal \cite{baevski2022data2vec} recommend to use half the number of total layers in absence of additional experiments.
We ablate this hyperparameter and report results in \reftab{number_of_target_layers}.
Although all tested values lead to usable results, indeed $K=6$ overall leads to the best performance.
\input{tables/ablation_target_layers}

\vspace{10pt}
\paragraph{Pre-Training Data Efficiency.}
\input{figures/pretraining_efficiency/pretraing_efficiency_figure.tex}
We evaluate the efficiency of self-supervised pre-training with \name{}.
To this end, we partition the ShapeNet training dataset into subsets containing $25\%$, $50\%$, $75\%$ and $100\%$ of the data.
We then fine-tuned our model for shape classification on ModelNet40 and ScanObjectNN, respectively.
As shown in\,\reffig{pretraining_data_efficiency}, \name{} achieves consistent improvements on both datasets.
Notably, pre-training \name{} with only $25\%$ of the training data yields superior results compared to Point-MAE pre-trained with $100\%$ of the training data.

\paragraph{Class Confusions on ModelNet40.}
Given the very high overall accuracies on the ModelNet40 dataset, we further analyze the remaining errors.
\reffig{modelnet_confusion} shows the confusion matrix on the ModelNet40 test split, clearly showing that most remaining mistakes are made for a few classes with very similar appearances, which might also be difficult for humans to distinguish.


\section{Architecture Details.}


In \reftab{hyperparameters}, we provide detailed hyperparameters for pre-training \datavec{} and \name{} on ShapeNet.
We, furthermore, report the hyperparameters for fine-tuning \name{} for the shape classification (\reftab{hyperparameters_classification}) and part segmentation task (\reftab{hyperparameters_part_segmentation}).
In Listing \ref{lst:point2vec}, we provide the PyTorch-inspired pseudocode for \name{} pre-training.

\section{Qualitative Results for Part Segmentation.}
\input{figures/shapenetpart_qualitative/shapenetpart_qualitative}
In \reffig{shapenetpart_qualitative}, we show qualitative results for part segmentation on the ShapeNetPart dataset.
\emakefirstuc{\name{}} achieves remarkable results, as the boundaries between parts are accurately localized with minimal semantic errors.
In the majority of instances, there is no perceivable difference between the results produced by \name{} and the ground truth.




\input{tables/hyperparameters.tex}
\input{tables/hyperparameters_classification.tex}
\input{tables/hyperparameters_segmentation.tex}
\input{listings/pseudo_code_point2vec.tex}
