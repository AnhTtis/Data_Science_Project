\section{Duty Cycle Selection}
\label{sec:algo}
Existing duty cycle algorithms use a fixed duty cycle for all the nodes in a system. When the available energy is constant over time and is equal to all the system nodes, a fixed duty cycle is feasible for amalgamated intermittent computing systems. Even without communication, the nodes can keep track of the energy budget of the other nodes as they have the same available energy. However, a node needs to know when the other nodes are active. Thus, by scheduling with a pre-defined duty cycle and staggered start time, an amalgamated system can ensure at least one active node at any given point in time without the additional computational overhead.

However, this work focuses on a more diverse scenario where the available energy varies at each node. We observe that \textit{a standard pre-defined duty cycle is not optimal even if the available energy is invariant over time but deviates at each node}. We formally proof it below. 

\itparlabel{Proof}
We prove this statement using contradiction. Let us assume that a standard pre-defined duty cycle is optimal when the energy source is constant over time but variable at each node. In such a scenario, the duty cycle is calculated with the lowest available energy to a node to ensure that at least one node is active. Taking the highest available energy to a node is not suitable due to the lack of guarantee and possible lack of sufficient energy to obey the defined duty cycle. However, the nodes with higher available energy will waste energy as their energy storage will get charged faster. If these nodes have small duty cycles, the total number of nodes required to ensure one active node at any time can be reduced. It contradicts our earlier assumption that a common pre-defined duty cycle is optimal. Thus, the common and pre-defined duty cycle is not optimal when available energy is consistent over time but differs across nodes. 
\proved

\subsection{Tailored Duty Cycle}
Thus, we explore the opportunity to define a tailored duty cycle for each node in the system by proposing a \textit{Prime-Co-Prime (PCP)} duty cycle selection algorithm. PCP provides the duty cycle based on the prime and co-prime numbers. Prime numbers refer to numbers greater than 1 and only have two factors, one and the number itself. Therefore, we will require at least one node for each prime number to have an active node at all times. If the lowest allowed duty cycle is 2 having additional nodes will increase the redundant active time where more than one is active at any time. However, in the real world, the lowest allowed duty cycle varies based on the available energy and capacitor size. To accommodate this, we exploit co-prime numbers of the allowed prime numbers. Two positive integers are called co-prime if and only if they have 1 as their only common factor. This algorithm is optimal and can ensure that at least one node is active at any time using the minimum number of nodes. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.45\textwidth]{fig/pcp.pdf}
    \caption{Prime-Co-Prime (PCP) with the lowest allowed duty cycle of 3 and hyper-period 15.}
    \label{fig:primecoprime}
\end{figure}


The Prime-Co-Prime duty cycle is determined offline during the compile time and is calculated in three steps. 

\begin{enumerate}
    \item First, we calculate the hyperperiod, T. The hyperperiod is the smallest interval after which the periodic patterns of all the tasks are repeated. In Figure~\ref{fig:primecoprime}, the hyperperiod is 15. 
    \item Next, we take all the prime numbers, P, where P<Q and T is the lowest duty cycle possible at any location. The example in Figure~\ref{fig:primecoprime} has the lowest allowed duty cycle of 3, and the prime numbers are 3, 5, 7, 11, and 13. 
    \item Finally, we use the Sieve of Eratosthenes to determine the rest of the duty cycles larger than T and are not divisible by P. The Sieve of Eratosthenes is an iterative process that is often used to find the prime numbers until a limit. We use it for now only to identify the prime and co-prime numbers of the prime list. In Figure~\ref{fig:primecoprime}, we observe that the co-prime to the prime list of the previous step is 4, which ensures that at least one of the 6 nodes are active at any time. 
\end{enumerate}


We formally prove that -- 
\textit{when the energy is constant in time and variable at each node, the minimum number of intermittent nodes required to have at least one node active at any time within a hyperperiod (T) can be given by a total number of primes smaller than or equal to T.} 
Here, the smallest allowed duty cycle is 2.

\itparlabel{Proof}
We prove this theorem by contradiction. Let us assume that for every prime number smaller than or equal to hyperperiod if there exists an intermittent node with a prime duty cycle. Thus, there exist time instances where no node is active.
Each number smaller than or equal to T can be either prime or non-prime. Moreover, as the nodes are co-prime, each duty cycle will be unique. If the time instance represents a prime number, then there is an intermittent node with a prime duty cycle, and thus precisely one node will wake up at those points. 

For a non-prime number, we must have $q=mn$, where $1<m,n<q$. By induction, as $m$ and $n$ are smaller than $q$, they must each be a product of primes. Therefore, $q$ is also a product of prime. Therefore there will be $p$ actives nodes at $q$ time where $p$ is the number of unique primes factors of $q$. 

It contradicts our assumption.
\proved

\subsection{Online Tailored Duty Cycle}
When the available energy varies with time and node, having a pre-defined tailored duty cycle is not sufficient. As the available energy changes over time, the possible duty cycle for each node also changes, and the pre-defined duty cycles can not be achieved. To address this, we form this problem as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP), where there is no communication between the agents (nodes), and the state (energy status) is partially known (each node knows its harvestable energy). A markovian decision process  (MDP) formalizes sequential decision-making where a state's actions depend on immediate rewards and subsequent states. In our scenario, each node's decision depends not only on whether it successfully captured and processed an event but also on the subsequent energy status to allow future success in the duty cycle. 

An agent cannot directly observe the underlying state in a Partially Observable Markov Decision Process (POMDP), which is suitable for our scenario as a node can not directly observe the energy status of the other nodes. Finally, in a Decentralized Partially Observable Markov Decision Process (Dec-POMDP), there is no communication among the nodes, and decentralized decisions are made by each node individually. In the amalgamated intermittent computing system, there is no communication among the nodes; thus, we formulate the problem as a Dec-POMDP. Each node will determine its duty cycle online at the available energy change to this node. 

A Dec-POMDP is represented by a tuple $\{N, S, A, T, R, \Omega, O, h, b_0\}$. Here, $N = \{ 1, .., n\}$ is the set of n agents, $S$ is the finite set of states $s$, $A$ is the set of joint actions $a = \{a_1, .., a_n\}$, $T$ is the transition function that specifies $Pr(s_{t+1}|s_t, a_t)$, $R(s, a)$ is the immediate reward function, $\Omega$ is the set of joint observations $o = \{o_1, ..., o_n\}$, $O$ is the observation function: $Pr(o_{t+1} | a_t, s_{t+1})$, $h$ is the horizon of the problem, and $b_0 \in \Delta(S)$ is the initial state distribution at time $t=0$.

Dec-POMDP aims to find an optimal joint policy $\pi*$ that maximizes rewards' expected sum (over time-step). We do not use the discounted summation of the rewards as performing a task sooner does not benefit our goal. Instead, having at least one intermittent node active all the time is more critical. 


The main difference between multiagent MDP and Dec-POMDP frameworks is that the joint policy is decentralized. $\pi*$ is a tuple $\{\pi_i, ..., pi_n\}$ where the individual policy $\pi_i$ of every agent $i$ maps individual observations histories $o_{i, t} = \{o_{i, 1}, ..., o_{i, t}$ to action $\pi_i(o_{i,t} = a_{i,t})$.


\parlabel{Reward Function, R} The immediate reward function is crucial for solving a Dec-POMDP. A reward function can be positive, negative, or even a combination. The positive reward is more straightforward to calculate as any time a node successfully captures and processes an event, we assign a positive reward. However, a negative reward is more challenging to determine as the ground truth of whether an event went missing is not available to a node. We exploit application-specific characteristics to define the negative reward. To illustrate, in the audio event detection example mentioned above, if a node captures an event from the first time after waking up, it can assume that the audio event started when the node was asleep before waking up and missed the event. Thus, we will assign a negative reward. 

When the application involves monitoring and maintaining a continuous variable, we can exploit that variable to assign a negative reward. For example, if a bell ringing indicates the end of a manufacturing cycle, the manufacturing machines will reach a steady state for cool-down. An intermittent node can monitor the movement and temperature of the machines during each wake-up to identify whether it has missed an event and assign a negative reward. 


Despite being a suitable solution, DEC-POMDP is unsuitable for an intermittent node due to its \textit{NEXP completeness}. 
As the energy at each node is non-uniform, the number of states can be hard to bound. To reduce the complexity, we use the duty cycles derived from the Prime-Co-Prime algorithm to deduce states. To illustrate, in the example of Figure~\ref{fig:primecoprime} the states-set includes all permutations of $\{3, 4, 5, 7, 11, 13\}$. Therefore, all nodes in the same state will have at least one active node. Next, we design the state diagram using knowledge from an approximate offline solution of the DEC-POMDP. However, along with additional computational overhead, using the DEC-POMDP transition function requires observation history, which introduces additional memory overhead and is unsuitable for intermittent nodes. 

Thus we provide two lightweight heuristics to transition between states. As a node does not know the state of other nodes, these heuristics are suboptimal. The following are some proposed heuristics:

\vspace{5pt}
\parlabel{Randomized Binary Search (RBS)}
Upon experiencing changing available energy, a node randomly chooses a state using a variant of the binary search method. In this method, instead of selecting the middle point, a node selects a random position between the range.
We use randomization instead of the traditional binary search method to ensure that intermittent nodes can recover from scenarios where all of them have selected the wrong state sets. On the contrary, this solution may also force nodes to choose the wrong state sets when they were already at the correct one. 

\vspace{5pt}
\parlabel{Suboptimal Reinforcement Learning (SRL)}
In this heuristic, each node maintains a local transition matrix where sleep or wake are the actions and uses it to make individual decisions. A node exploits feedback to update the offline transition table whenever feedback is available. Here, each node maintains a local Q-Table where choosing the new duty cycle is the action. Each node makes individual decisions from the local Q-table and updates it using the feedback from the continuous variable. This heuristic is suitable for systems that monitor and maintain a continuous variable.


% \subsubsection{Randomized Binary Search (RBS)}
% Upon experiencing changing available energy, a node randomly chooses a state using a variant of the binary search method. In this method, instead of selecting the middle point, a node selects a random position between the range.
% We use randomization instead of the traditional binary search method to ensure that intermittent nodes can recover from scenarios where all of them have selected the wrong state sets. On the contrary, this solution may also force nodes to choose the wrong state sets when they were already at the correct one. 

% \subsubsection{Suboptimal Reinforcement Learning (SRL)}
% In this heuristic, each node maintains a local transition matrix where sleep or wake are the actions and uses it to make individual decisions. A node exploits feedback to update the offline transition table whenever feedback is available. Here, each node maintains a local Q-Table where choosing the new duty cycle is the action. Each node makes individual decisions from the local Q-table and updates it using the feedback from the continuous variable. This heuristic is suitable for systems that monitor and maintain a continuous variable.