%%
\documentclass{amsart}

\usepackage{multirow}
\usepackage{enumitem}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=20mm,
	top=20mm,
}
%\usepackage{bibliography}
\usepackage[ruled]{algorithm2e}
\usepackage[noend]{algpseudocode}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\usepackage{subfig}

\usepackage{xr}
\externaldocument[m-]{main}
% Theorem environments-------------------------------------------

%\theoremstyle{sltheorem}
\newtheorem{theorem}{Theorem}

\newtheorem{thm}{Theorem} %If you put '[subsection]' or '[section]' at the end, it means the numbering include the annotations for that.
\newtheorem{cor}[thm]{Corollary} %'[thm]' is for numbering in an uniform way 
\newtheorem{lem}[thm]{Lemma}
\newtheorem{ex}[thm]{Example}
\newtheorem{prop}[thm]{Proposition}

\theoremstyle{definition}
\newtheorem{defn}{Definition}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}




\numberwithin{equation}{section}
%color commands--------
\newcommand*{\red}[1]{\textcolor{red}{#1}}
\newcommand*{\blue}[1]{\textcolor{blue}{#1}}
\newcommand*{\green}[1]{\textcolor{teal}{#1}}
% simple commands---------------------------------------------
\newcommand*{\abs}[1]{\left\vert#1\right\vert}
\newcommand*{\set}[1]{\left\{#1\right\}}
\newcommand*{\seq}[1]{\left<#1\right>}
\newcommand*{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand*{\opnorm}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
		\right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}
\newcommand*{\nucnorm}[1]{\left\Vert#1\right\Vert_{\text{nuc}}}
\newcommand*{\adj}[1]{\tilde{#1}}

\newcommand*{\defeq}{\stackrel{\mathrm{def}}{=}}
\newcommand*{\probref}[1]{\hyperref[#1]{Problem \ref{#1}}}
\newcommand*{\lemref}[1]{\hyperref[#1]{Lemma \ref{#1}}}
\newcommand*{\thmref}[1]{\hyperref[#1]{Theorem \ref{#1}}}
\newcommand*{\exref}[1]{\hyperref[#1]{Example \ref{#1}}}
\newcommand*{\defnref}[1]{\hyperref[#1]{Definition \ref{#1}}}
\newcommand*{\algoref}[1]{\hyperref[#1]{Algorithm \ref{#1}}}
\newcommand*{\figref}[1]{\hyperref[#1]{Figure \ref{#1}}}
\newcommand*{\tableref}[1]{\hyperref[#1]{Table \ref{#1}}}

\newcommand*{\ind}{\mathbf{1}}
\newcommand*{\EE}{\mathbb{E}}
\newcommand*{\NN}{\mathbb{N}}
\newcommand*{\PP}{\mathbb{P}}
\newcommand*{\KL}{\mathrm{KL}}
\newcommand*{\dd}{\mathrm{d}}
\newcommand*{\Leb}{\mathrm{Leb}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\vol}{vol}
\DeclareMathOperator*{\VC}{VC}

\newcommand*{\N}{\mathbb{N}}
\newcommand*{\Z}{\mathbb{Z}}
\newcommand*{\Q}{\mathbb{Q}}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\C}{\mathbb{C}}
\newcommand*{\Rp}{\mathbb{R}_+}
\newcommand*{\cC}{\mathcal{C}}
\newcommand*{\cF}{\mathcal{F}}
\newcommand*{\cH}{\mathcal{H}}
\newcommand*{\cN}{\mathcal{N}}
\newcommand*{\cR}{\mathcal{R}}
\newcommand*{\cA}{\mathcal{A}}
\newcommand*{\cS}{\mathcal{S}}


%comments-------------------------------------------------------
\usepackage{todonotes}
\newcounter{todocounter}
\newcommand{\dl}[1]{\stepcounter{todocounter}{\color{purple} [\textbf{DL \thetodocounter:} #1] } }
\newcommand{\jb}[1]{\stepcounter{todocounter}{\color{green} [\textbf{JB \thetodocounter:} #1] } }

%additional-------------------------------------------------------
\newcommand*{\eval}[2]{\hat{#1}(#2)}

%------------------------------------------------------
\begin{document}

\title{Supplementary Information}

\maketitle

\section{Model hyper-parameters}

\subsection{CNN}
We used the same hyper-parameters as \cite{DeepSurf} for the 3D BottleNeck ResNet model. 

\subsection{Attention}

For individual attention layers, we used the same default hyper-parameters as Alphafold:
\begin{itemize}
	\item hidden dimension: 128
	\item number of attention heads: 12
	\item dimension of query and key vectors: 16,
	\item dimension of value vectors: 16
	\item number of attention points: 4
	\item dropout rate: 0.1
\end{itemize}
At the end of each layer, we used a 2-layer point-wise feed-forward network with the intermediate hidden dimension 512. 

For the part of the architecture that BSD and BRI modules share, we used 3 attention layers. For the BSD module, we used one additional attention layer. 

\section{Details of training}

We will summarize the details such as hyper-parameters for each training stage, 

\subsection{Pre-training BSD module}\label{subsubsection:BSD_pre-training_details}

\begin{itemize}
\item Minibatch size: 16 
\item Batch sampling: balanced BSD labels (See \ref{m-subsection:BSD_training})
\item Loss function: balanced binary cross entropy (See \ref{m-subsection:BSD_training}) 
\item Reduction of loss terms from individual datapoints: mean 
\item Peak learning rate: 1e-4 
\item Learning rate schedule: one-cycle cosine scheduling (\cite{cosine_annealing}) with 6000 warmup steps and 60000 total steps 
\item Optimizer: AdamW with $\beta_1=0.9$, $\beta_2=0.999$ and weight decay rate 0.01
\end{itemize}

%We used minibatch size 16, and peak learning rate 1e-4, with the loss terms from individual datapoints mean-reducted. We used AdamW optimizer with $\beta_1=0.9$, $\beta_2=0.999$ and weight decay of rate 0.01 and the cosine learning rate scheduler (the most simple form of cosine annealing \cite{cosine_annealing}) with 6000 warmup steps and 60000 total steps. 

\subsection{Fine-tuning BSD module}\label{subsubsection:BSD_fine-tuning_details}

\begin{itemize}
\item Minibatch size: 16 
\item Batch sampling: balanced BSD labels (See \ref{m-subsection:BSD_training})
\item Loss function: binary cross entropy  
\item Reduction of loss terms from individual datapoints: mean 
\item Peak learning rate: 1e-6 
\item Learning rate schedule: one-cycle cosine scheduling (\cite{cosine_annealing}) with 4000 warmup steps and 20000 total steps 
\item Weight freezing: Up to 8000 steps, the parameters from the part of the architecture shared with the BRI module are frozen (See \ref{m-subsection:BSD_training})
\item Optimizer: AdamW with $\beta_1=0.9$, $\beta_2=0.999$ and weight decay rate 0.01
\end{itemize}

%For a batch, the resulting weighted sums are mean-reduced to obtain the loss.  
%We use minibatch size 16, and peak learning rate 1e-4. We use AdamW optimizer with $\beta_1=0.9$, $\beta_2=0.999$ and weight decay of rate 0.01 and the cosine learning rate scheduler (the most simple form of cosine annealing \cite{cosine_annealing}) with 6000 warmup steps and 60000 total steps.   

%In the second stage, we use mean-reduced binary cross entropy loss, minibatch size 16, peak learning rate 1e-6, AdamW optimizer with $\beta_1=0.9$, $\beta_2=0.999$ and weight decay of rate 0.01, and the cosine learning rate scheduler with 4000 warmup steps and 20000 total steps. We freeze the model parameters of the parts shared by the BRI module architecture until 8000 steps, to promote smoother fine-tuning of the parameters.   

\subsection{Training BRI module}\label{subsubsection:BRI_training_details}

We used the same details as in \ref{m-subsubsection:BSD_pre-training_details}, except the balanced batch sampling. When training BRI module, as explained in \ref{m-subsection:BRI_training}, we used only the binding sites with positive BSD labels. 

\section{Datasets}

In our experiments, we used the datasets scPDB, COACH420, HOLO4K and CHEN. As explained in \ref{m-subsection:datasets},  We used scPDB for a held-out cross validation, and used the other ones as external test datasets.

\subsection{Characteristics of the datasets}
ScPDB (\cite{scPDB}) is a large database of binding sites from the Protein Data Bank. COACH420 (\cite{tmsite_ssite}) consists of 420 single-chain structures from the COACH dataset. HOLO4K (\cite{holo4k_origin}) consists of 4009 larger multi-chain structures. The CHEN dataset (\cite{chen_dataset}) is smaller, but covers a wide range of non-homologous structures. Also, it has 104 holo structures with apo counterparts, where corresponding holo and apo structures have the same SCOP family, 80\% sequence similarity, and TM score above 0.5.  These holo and apo structures resulted in ``CHEN-holo" and ``CHEN-apo" datasets used in our experiments.  

\subsection{Ligands used for evaluations} 

Evaluation of binding sites depends on what ``ligands" are considered to constitute binding sites. ``Biologically irrelevant" ligands such as additives are usually excluded in this calculation. Therefore, we will clarify what ligands we considered for each dataset. For scPDB and CHEN, we used the ones the original databases provided. For COACH420 and HOLO4K, we used the ligands in the PDB files provided by the p2rank (\cite{p2rank}) repository (\cite{p2rank-dataset_repo}), which are those with PDB ligand code listed in ``biologically relevant ligands" curated by MOAD 2013 database (\cite{moad2013_paper}). 


\subsection{Structural alignments of apo structures and holo structures}

To evaluate BSP algorithm's performance on the CHEN-apo dataset, we projected the ligands in CHEN-holo to the corresponding protiens of CHEN-apo. We will describe the projection procedure. 

Let $H$ be a protein structure from CHEN-holo, $L_1,\cdots,L_k$ be ligands within $H$, and $P$ be the structure of the corresponding protein in CHEN-apo. We positioned the ligands $L_i$ onto $P$ as follows.  
\begin{itemize}
\item Find a sequence alignment of $H$ and $P$, using the software \textsc{mmseqs}. 
\item  Find a SE(3) transformation $\phi_i$ from the space of $H$ onto the space of $P$ that minimizes the mean square alpha carbon distance between the pairs of residues $(R, R')$ such that
\begin{itemize}
    \item $R$ is from $H$ and $R'$ is from $P$
	\item $R$ and $R'$ are aligned 
	\item $R$ or one of its (two) neighboring residues is within $4\AA$ from $L_i$.  
\end{itemize} 
(If there was no such pair, the ligand was ignored, and not projected to the apo structure)
\item Consider $L_i'=\phi_i(L_i)$ as a ligand within the structure $P$ 
\end{itemize}


\subsection{dataset statistics} 
The procedures described in the previous two sections resulted in the following dataset statistics: 

\begin{table}[h]
	\tiny
	\centering
	\label{dataset_stats}
	\begin{tabular}{|c|c|c|c|c|}
	    \hline 
		dataset & number of proteins & number of binding sites & average number of binding sites & maximum number of binding sites \\
		\hline 
		scPDB & 16612 & 17594 & 1.06 & 4\\ 
		\hline
	    COACH420 & 299 & 494 & 1.65 & 12 \\ 
	    \hline
	    HOLO4K & 3378 & 7141 & 2.11 & 24 \\ 
	    \hline
	    CHEN-holo & 104 & 244 & 2.35 & 11 \\ 
	    \hline
	    CHEN-apo & 104 & 243 & 2.34 & 11 \\
		\hline
	\end{tabular}
\end{table}

\section{Comparison of our evaluation metrics with those of baseline methods}\label{supplementary:metric_pecularities}

As explained in \ref{m-subsubsection:evaluation_metrics}, we used three evaluation metrics for BSP --- \textit{success rate}, \textit{IOU} and \textit{conditional IOU}. Each of these is inspired by previous works, but not exactly same as theirs. 

\subsection{success rate}
This metric is similar to those used in the baseline methods (\cite{Deeppocket}, \cite{Kalasanty} and \cite{DeepSurf}). Compared to their definitions, our definition makes two specific choices. First, as in \cite{Kalasanty}, our definition uses F1 score to compare predicted binding sites and actual ligands in each protein. This differs from the \textit{precision} version used in \cite{Deeppocket} and \cite{DeepSurf}.  These two may differ when more than one \textit{ligand of interest} is bound to the protein. In that case, the \textit{precision} version may give the \textit{perfect} score when all predicted binding sites are nearby a single ligand. %Therefore, we believe that using F1 score is more reasonable for fair evaluations. 
Secondly, in our definition, the definition of ``detection" is based on DCA (used in \cite{Deeppocket} and \cite{DeepSurf}), instead of Distance from Center to Center (DCC, used in \cite{Deeppocket} and \cite{Kalasanty}). DCC, unlike DCA, computes the distance between the predicted binding site center and the center of the binding pocket associated to the ligand. Although this may be semantically more adequate than DCA, DCC has a critical limitation---it can be measured only when the binding pocket associated to the ligand is manually annotated or extracted successfully by a computational means. Indeed, in \cite{Kalasanty}, this fact limited the evaluation on a non-annotated dataset to the ligands whose associated binding pockets could be successfully extracted by an external software. 

\subsection{IOU} 
This metric is analogous to a metric used in \cite{Kalasanty} to evaluate the \textit{segmentation} ability of the model, but differs from it in two aspects. Firstly, we defined the "closest ligands" in terms of DCA, instead of DCC. Secondly, we compared the shape of the predicted binding site and the actual binding site in terms of the protein residues, instead of the segmented 3D volume as in \cite{Kalasanty}. The last choice reflects our emphasis on the task of BRI, which has its own applications that cannot be accomplished by volume segmentation, as explained in the introduction. 

\subsection{conditional IOU}
This metric is analogous to a metric used in \cite{Deeppocket}. However, their version computed the IOU out of predictions made from Fpocket-generated binding sites that are closest to the ligands, instead of binding sites that their BSD model predicted. Although this is a reasonable approach, other baseline methods that are not based on Fpocket cannot be assessed in this way. 


\bibliographystyle{alpha}
\bibliography{suppl_info}
	
%------------------------------------------------------
\end{document}