\documentclass[journal,twocolumn,letterpaper]{IEEEJERM}

\ifCLASSINFOpdf
 
\else

\fi



\usepackage{times,amsmath,epsfig}
\renewcommand\IEEEkeywordsname{Index Terms}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage[latin1]{inputenc}
\usepackage[utf8]{inputenc}
\usepackage{array}
\usepackage{graphicx}
\usepackage{url}
\usepackage{subfigure}
\usepackage{bm}
\usepackage{breqn}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{amssymb}
\usepackage{flushend}
\usepackage[noadjust]{cite}


\hyphenation{}
\IEEEoverridecommandlockouts

\begin{document}

%\title{Towards Predictive Model on Clinical Narratives Classification - A Transformer Approach}
\title{A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification}
%A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification
%Enhancing Clinical Narratives Classification with a Small-Scale Switch Transformer Model
%Transformer-based for Clinical Narratives Classification- A Inpretaba Analysis
\author{Thanh-Dung Le,~\IEEEmembership{Member,~IEEE,},  Philippe Jouvet M.D., Ph.D., and Rita Noumeir Ph.D.,~\IEEEmembership{Member,~IEEE} 
 }     
      


% The paper headers
\markboth{IEEE Journal of Biomedical and Health Informatics}
{T.D. Le \MakeLowercase{\textit{et al.}}: IEEE Journal of Biomedical and Health Informatics}


\twocolumn[
\begin{@twocolumnfalse}
  
% make the title area
\maketitle

\begin{abstract}

In recent years, Transformer-based models such as the Switch Transformer have achieved remarkable results in natural language processing tasks. However, these models are often too complex and require extensive pre-training, which limits their effectiveness for small clinical text classification tasks with limited data. In this study, we propose a simplified Switch Transformer framework and train it from scratch on a small French clinical text classification dataset at CHU Sainte-Justine hospital. Our results demonstrate that the simplified small-scale Transformer models outperform pre-trained BERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT. Additionally, using a mixture of expert mechanisms from the Switch Transformer helps capture diverse patterns; hence, the proposed approach achieves better results than a conventional Transformer with the self-attention mechanism. Finally, our proposed framework achieves an accuracy of 87\%, precision at 87\%, and recall at 85\%, compared to the third-best pre-trained BERT-based model, FlauBERT, which achieved an accuracy of 84\%, precision at 84\%, and recall at 84\%. However, Switch Transformers have limitations, including a generalization gap and sharp minima. We compare it with a multi-layer perceptron neural network for small French clinical narratives classification and show that the latter outperforms all other models.


\vspace{9pt}
\begin{IEEEkeywords}
Clinical natural language processing, cardiac failure, BERT, Transformer.
\end{IEEEkeywords}

\vspace{9pt}
\textbf{\textit{Clinical and Translational Impact Statement---} The application of Switch Transformer to clinical text classification represents a promising avenue for improved performance over pre-trained BERT-based models. While it does not outperform a small MLP-NN neural network, the framework has the potential to enhance accuracy on small French clinical narrative classification.}
\end{abstract}

\end{@twocolumnfalse}]

% Put footnotes here
{
  \renewcommand{\thefootnote}{}%
  \footnotetext[1]{This work was supported in part by the Natural Sciences and Engineering Research Council (NSERC), in part by the Institut de Valorisation des donnees de l'Universite de Montreal (IVADO), in part by the Fonds de la recherche en sante du Quebec (FRQS), and in part by the Fonds de recherche du Quebec-Nature et technologies (FRQNT). }
  
  \footnotetext[2]{Thanh-Dung Le is with the Biomedical Information Processing Lab, Ecole de Technologie Superieure, University of Quebec, Canada, and also with the Research Center at CHU Sainte-Justine, University of Montreal, Canada (Email: thanh-dung.le.1@ens.etsmtl.ca).}

  \footnotetext[3]{Rita Noumeir is with the Biomedical Information Processing Lab, Ecole de Technologie Superieure, University of Quebec, Canada. }

  \footnotetext[3] {Philippe Jouvet are with the Research Center at CHU Sainte-Justine, University of Montreal, Canada.}
}
 
\IEEEpeerreviewmaketitle


\section{Introduction}
\IEEEPARstart{R}ecent advancements in deep learning have led to the development of Transformer models \cite{vaswani2017attention}, which have shown remarkable performance in various natural language processing (NLP) tasks \cite{tripathy2021comprehensive}. As a result, there is a growing interest in applying Transformer-based models to clinical applications, such as predicting disease risk \cite{huang2022assessing}, identifying disease \cite{ilias2022explainable}, and improving clinical decision-making \cite{meng2021bidirectional}. These models can be trained on various data sources, including electronic health records (EHRs) \cite{meng2021bidirectional, blanco2021exploiting, li2022hi}, medical imaging \cite{deng2022rformer, li2022ddpTransformer, mondal2021xvitcos}, electrogram \cite{phan2022sleepTransformer, lu2022improving}, and genome \cite{clauwaert2020novel, huang2020sacall}  to extract clinically relevant information and provide accurate predictions. Overall, Transformer models present a powerful tool for clinical applications and can potentially play an increasingly important role in healthcare.

In clinical NLP, Transformers-based models have shown great promise in clinical narrative classification. In this context, clinical narrative refers to patient encounters in EHRs or other clinical documentation. Using Transformers-based models, researchers and clinicians can develop algorithms that automatically classify these narratives based on different criteria, such as diagnosis, treatment, or patient outcomes. This can help streamline clinical workflows and improve patient care by providing more accurate and efficient clinical data processing. Some examples of successful applications of Transformers-based models for clinical narrative classification include identifying clinical coding \cite{lopez2021Transformers, lopez2023explainable}, diagnosing health conditions \cite{roitero2021dilbert, mugisha2022comparison, rizwan2022depression, kjell2022natural}, and detecting clinical events \cite{althari2022exploring, kim2023identifying, yang2020clinical}. As such, Transformers-based models have become an increasingly important tool in clinical NLP and are likely to continue playing a significant role in this field \cite{zhou2022natural}.

Despite their many benefits, Transformers-based models for clinical text classification have some limitations that must be considered. One major challenge is the need for large amounts of annotated clinical data to train these models effectively. Clinical data is often scarce and sensitive, which makes it challenging to obtain and annotate in a way that preserves patient privacy \cite{gao2021limitations}. Additionally, clinical language is highly specialized and can vary significantly across different specialties and regions, making it difficult to develop models that generalize well across different contexts \cite{bear2021clinically}. There is a risk of bias in the data used to train these models, leading to errors or disparities in the predictions made \cite{alimova2021cross}. Furthermore, the computational requirements of Transformer-based models can be pretty high, which can limit their use in resource-constrained settings where computational resources are limited \cite{gillioz2020overview}. Finally, the interpretability of these models can be limited, making it difficult for clinicians to understand how they make their predictions and trust their outputs \cite{rudin2019stop, tonekaboni2019clinicians}. While Transformers-based models have great potential for clinical text classification, they also require careful attention to their limitations and the potential biases that can arise.

\begin{figure*}[!ht]
	\centering
	\vspace{2pt}
	\includegraphics[scale=0.606]{photo/CliNoteVisualization.png}
	\vspace{-2pt}
	\caption{French clinical note at CHUSJ illustration by using Scattertext visualization.}
	\vspace{-8pt}
	\label{fig:clinicalnoteCHUSJ}
\end{figure*}

\begin{itemize}
    \item Computational requirements: If a model lacks the necessary computational capacity, its training efforts will fail, regardless of the learning algorithm's sophistication or the training data's quality \cite{bhattamishra2020computational}. This can be a limiting factor for smaller clinical text or resource-constrained settings. 
    
    \item  Data requirements: Transformer-based models require large amounts of labeled data for training, which may not be available for some clinical text classification tasks, especially for rare or low-frequency conditions \cite{zeng2022pretrained}.

    \item Domain-specific language: Clinical text is highly domain-specific and contains jargon and abbreviations that may not be covered by general language models such as Transformers. This can lead to suboptimal performance on clinical text classification tasks \cite{gu2021domain}.

    \item  Interpretability: Transformer-based models are highly complex and difficult to interpret, making it challenging to understand how the model makes predictions, which is essential for clinical decision-making \cite{zafar2021lack}.

\end{itemize}


Another significant limitation of using Transformer-based models for clinical text classification is that they may not perform as well for languages other than English and that are in limited availability. Most Transformer-based models have been developed and trained on English-language text, and their performance may suffer when applied to other languages \cite{alshuweihi2021biomedical}. This is particularly important in the clinical context, where patient data can be collected in many languages. Another challenge is that clinical datasets are often small and imbalanced, making it difficult to train accurate models using Transformer-based  \cite{neveol2018clinical}. Small datasets can also lead to overfitting, where the model performs well on the training data but fails to generalize to new data. When there is insufficient data, the Transformer model does not learn to focus on local features in the lower layers of the network. This may result in reduced model performance, as it cannot effectively capture relevant information from the input data \cite{raghu2021vision}. Overall, while Transformer-based models offer many advantages for clinical text classification, their effectiveness is influenced by the data's language and the training dataset's size and quality.


This study aims to overcome the challenges of using Transformer-based models for clinical text classification for a small French clinical note by employing the Mixture-of-expert (MoE) framework from the recent Switch Transformer model developed by Google \cite{fedus2021switch}. Switch Transformer is an extension of the Transformer architecture motivated by the original model's self-attention mechanisms. Still, it uses an MoE mechanism to address the limitations of the conventional Transformer \cite{vaswani2017attention}. A key technical difference between Switch Transformers with an MoE mechanism and Transformers with self-attention is how they handle the modeling of complex input-output relationships. An example of the effectiveness of MoE has been proven by \cite{xue2022go}; that study shows that the approach of using parameter sharing to compress along the depth of the model, which is used in existing works, is limited in terms of performance. To improve the model's capacity, the authors propose scaling along the model's width by replacing the feed-forward network with an MoE. This allows for better modeling capacity and potentially better performance. 

Additionally, the study \cite{lazaridou2021mind} suggests that simply increasing the model's size is insufficient to address the issue of performance degradation over time from neural language models. However, the researchers found that using models that continuously update their knowledge with new information can help alleviate this problem. While Transformers with self-attention model these relationships through a single attention mechanism that captures dependencies between all input and output positions, Switch Transformers with an MoE mechanism decompose the problem into smaller, simpler sub-problems, each handled by a different ``expert" model. In other words, instead of using a single global attention mechanism, Switch Transformers employ multiple local attention mechanisms focusing on different input aspects. The gating mechanism used in Switch Transformers selects which expert model to use for a given input, depending on the context. Therefore, this approach can potentially improve the modeling of complex input-output relationships and increase the model's efficiency, especially when dealing with complex data from the clinical domain. This is particularly important in clinical data, where information is often conveyed through complex and nuanced language. By employing this approach, our study aims to improve the accuracy and generalizability of clinical text classification models for small datasets in languages other than English. We have made several significant contributions to clinical text classification using Transformer-based models. 
\begin{itemize}
    \item First, our study demonstrates a comprehensive implementation of a simplified Switch Transformer model from scratch. This would allow other researchers to understand and replicate the methodology used in the study, which is essential for building on and advancing this work.
    \item Second, our study provides experimental evidence showing the limitations of Transformer-based models in terms of generalization gap and sharp minima. This highlights the importance of carefully selecting and preprocessing the data used to train these models to avoid overfitting and improve generalization performance.
    \item Finally, our study illustrates the interpretable output of the model by adapting the Integrated Gradients (IG) \cite{sundararajan2017axiomatic}. It provides a way to attribute importance to the input features of a model, allowing clinicians and researchers to gain insight into how the model is making its predictions.
    
\end{itemize}

%Overall, the study contributes to developing more accurate and interpretable clinical text classification models and provides essential insights into the limitations and challenges of using Transformer-based models in this context. This approach offers a promising solution to the challenge of small datasets in clinical text classification by allowing for the practical adaptation of Transformer-based models to the real-world clinical dataset. The MoE enables the model to learn from multiple experts, each specialized in different aspects of the data, and combine their outputs to achieve better performance. Additionally, by using a Transformer-based model, the study takes advantage of its ability to capture the complex relationships between words and phrases in the clinical text.

This study significantly contributes to developing accurate and interpretable clinical text classification models and sheds light on the limitations and challenges of using Transformer-based models in this context. By leveraging the MoE technique, this approach offers a promising solution to the problem of small datasets in clinical text classification, enabling the practical adaptation of Transformer-based models to real-world clinical data. The MoE allows the model to learn from multiple experts, each specialized in different aspects of the data, and to combine their outputs to achieve improved performance. Furthermore, a Transformer-based model provides a powerful tool for capturing the complex relationships between words and phrases in clinical text.  However, our proposed method underperforms compared to a smaller and simpler framework that combines statistical representation learning with term frequency-inverse document frequency and multilayer perceptron network. Despite this limitation, our work demonstrates the potential of combining MoE with Transformer-based models to overcome data limitations and improve the accuracy and interpretability of clinical text classification models, which could have a significant impact on clinical decision-making. 

%Overall, this work demonstrates the potential of combining MoE with Transformer-based models to overcome data limitations and improve the accuracy and interpretability of clinical text classification models.


%This paper is organized as follows. Section \ref{sec:autoencoder} describes the materials and methods used in this study, including French clinical notes and the model's architecture. In section \ref{sec:exp_result}, the experimental results and discuss their implications for the model's performance are presented. We also examine the strengths and limitations of the proposed approach in section \ref{sec:dis}. To gain insights into the misclassifications, we analyze some of these cases in section \ref{sec:misclass}. Finally, we conclude our study with remarks in section \ref{sec:conclusion}.
%on the essential findings and future directions
%cases where the model made

This paper is organized as follows. Section \ref{sec:autoencoder} will discuss the materials and methods. Then, the experimental results and discussion will be discussed in section \ref{sec:exp_result}, and \ref{sec:dis}, respectively. Misclassification cases will be discussed in section \ref{sec:misclass}. Finally, section \ref{sec:conclusion} provides concluding remarks.

\section{MATERIALS AND METHODS}
\label{sec:autoencoder}

\subsection{French Clinical Data at CHUSJ}

The clinical decision support system (CDSS) system in the CHU Sainte Justine (CHUSJ) hospital aims to improve the diagnosis and management of acute respiratory distress syndromes (ARDS) in real-time by automatically screening data from electronic medical records, chest X-rays, and other sources. Previous studies have found that the diagnosis of ARDS is often delayed or missed in  many patients \cite{bellani2016epidemiology}, emphasizing the need for more effective diagnostic tools. Three main conditions must be detected to diagnose ARDS: hypoxemia, chest X-ray infiltrates, and absence of cardiac failure \cite{pediatric2015pediatric}. The research team at CHUSJ has developed algorithms for detecting hypoxemia \cite{sauthier2021estimated}, analyzing chest X-rays \cite{zaglam2014computer, yahyatabar2020dense}, and identifying the absence of cardiac failure. In addition, the team has performed extensive analyses of machine learning algorithms for detecting cardiac failure from clinical narratives using natural language processing \cite{le2021detecting, le2023adaptation}. Implementing these algorithms could increase ARDS diagnosis rates and improve patient outcomes.

This study was conducted following ethical approval from the research ethics board at CHUSJ; and, the study's design focused on identifying cardiac failure in patients within the first 24 hours of admission by analyzing admission and evolution notes during this initial period. Therefore, we conducted a retrospective analysis of EHRs from the Research Center of CHUSJ in this study. The dataset consisted of 580,000 unigrams extracted from 5,444 single lines of short clinical narratives. Of these, 1,941 cases were positive (36\% of the total), and 3,503 cases were negative. ScatterText \cite{kessler2017scattertext} was utilized to visualize the notes and identified over 580,000 unigrams (n-grams), as depicted in Fig. \ref{fig:clinicalnoteCHUSJ}. The visualization showcases the most frequent words for positive cases in the upper right corner, negative cases in the lower-left corner, and less frequent words for both cases in the center. The top terms for positive and negative cases are also presented on the right-hand side. Upon inspection, we observed that most top terms for positive cases were positively related to cardiac malfunction, such as milrinone or milri (milrinone), and aorte or aortique valve (aortic valve). In contrast, terms like respiratoire (respiratory), d√©tresse respiratoire (distress respiratory), and 02 (oxygen) indicated respiratory syndromes in negative cases. While the longest n-gram was over 400 words, most n-grams had a length distribution between 50 and 125 words. The average length of the number of characters was 601 and 704, and the average size of the number of digits was 25 and 26 for the positive and negative cases, respectively. We pre-processed the data by removing stop-words and accounting for negation in medical expressions. Numeric values for vital signs (heart rate, blood pressure, etc.) were also included and decoded to account for nearly 4\% of the notes that contained these values. All the notes are short narratives; detailed characteristics can be found in the Supplementary Materials from \cite{le2021detecting}.  

\begin{figure}[t]
	\centering
	\vspace{2pt}
	\includegraphics[scale=0.6]{photo/workflow.png}
	\vspace{-2pt}
	\caption{Workflow demonstration of the proposed methodology to classify French clinical narratives at CHUSJ hospital.}
	\vspace{-8pt}
	\label{fig:CHUSJ_workflow}
\end{figure}


\subsection{Language Models for Clinical Narratives}

This manuscript thoroughly analyzes the present state of pre-trained BERT-based models and Transformer models for clinical narrative classification, with a particular emphasis on limited datasets. Various pre-trained BERT-based models for the French language are leveraged, such as FlauBERT, FrALBERT, CamemBERT, and DistilBERT, as depicted in Fig. \ref{fig:CHUSJ_workflow}. Moreover, conventional and Switch Transformer models are constructed from scratch to perform the same task. Finally, we compare the performance of all models based on various evaluation metrics for binary classification, including accuracy, precision, recall, F1-score, and area under the curve (AUC). This study endeavors to offer insights into the efficacy of these models on limited datasets, which is a critical aspect in real-world clinical settings for non-English notes.

\begin{figure*}[!ht]
	\centering
	\vspace{2pt}
	\includegraphics[scale=0.625]{photo/trans_vs_switchtrans.png}
	\vspace{-2pt}
	\caption{Illustration of a Conventional Transformer \cite{alammar2018illustrated} (left), and a Switch Transformer \cite{fedus2021switch} (right) encoder block.}
	\vspace{-8pt}
	\label{fig:trans_vs_switch}
\end{figure*}


\subsubsection{Transformer-based Models}
Transformer-based models have been highly effective for various NLP tasks, including text classification. The conventional Transformer model \cite{vaswani2017attention} with multi-head self-attention is a widely used architecture for this task. Shown in Fig. \ref{fig:trans_vs_switch} (left), its architecture comprises an encoder consisting of multiple layers of multi-head self-attention and feedforward neural networks (FFN). The multi-head self-attention mechanism allows the model to weigh the importance of different words in a sequence based on their semantic relationships, while the FFNs transform the output of the self-attention layer into a more helpful representation. The Transformer's core is the self-attention mechanism based on mathematical expressions \cite{lin2022survey}. Given a sequence of input embeddings $x_1, ..., x_n,$ the self-attention mechanism computes a set of context-aware embeddings $h_1, ..., h_n$ as follows:

\begin{align}
    {h}_i = \text{Attention}(QW_i^Q,KW_i^K,VW_i^V)
    \label{eq:head_i}
\end{align}
    
where $\text{Attention}$ is the scaled dot-product attention function:
\begin{align}
    \text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
    \label{eq: Attetion}
\end{align}
Then, the multi-head attention is a concatenation of all head of $h_i$, as follows:
\begin{align}
    \text{MultiHead}(Q,K,V) = \text{Concat}(h_1,\ldots,h_n)W^O
    \label{eq: concat}
\end{align}

Additionally, the position-wise FFNs are multi-layer perceptrons applied independently to each position in the sequence, which provide a nonlinear transformation of the attention outputs. FNNs are calculated as follows:
\begin{align}
    \text{FFN}(x) = \text{ReLU}(xW_1+b_1)W_2+b_2
    \label{eq: ffn}
\end{align}

For each layer, there is a Layer Normalization which normalizes the inputs to a layer in a neural network to improve training speed and stability.
\begin{align}
    \text{LayerNorm}(x) = \gamma\frac{x-\mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
\end{align}


\noindent where $Q$, $K$, and $V$ are the query, key, and value matrices, $W_i^Q$, $W_i^K$, and $W_i^V$ are the learned weight matrices for the $i$-th head of the multi-head attention, $W_1$ and $W_2$ are the weight matrices for the position-wise FFNs, $\gamma$ and $\beta$ are learned scaling and shifting parameters for layer normalization, and $\mu$ and $\sigma$ are the mean and standard deviation of the input feature activations. The working mechanism in the Transformer architecture can be summarized into the following steps:
\begin{enumerate}
    \item Linear Transformation: The input sequence is projected into three vectors, query $Q$, key $K$, and value $V$, by applying a linear transformation to the input embedding.
    \item Splitting: The $Q$, $K$, and $V$ vectors are then split into multiple heads $h_i$, allowing the model to simultaneously attend to different aspects of the input sequence Eq. \ref{eq:head_i}.
    \item Scaled Dot-Product Attention: For each $h_i$, the model calculates the attention weights between the $Q$ and $K$ vectors by scaling their dot product by the square root of the vector dimension. It calculates each $K$ vector's importance to the corresponding $Q$ vector.
    \item Softmax: The resulting attention weights are normalized using a softmax function, ensuring that they sum to 1.
    \item Weighted Sum: The attention weights are then used to weigh the $V$ vectors, producing an attention output for each head $h_i$ Eq. \ref{eq: Attetion}.
    \item Concatenation: The attention outputs from each head are concatenated and projected back to the original vector dimension through another linear transformation Eq. \ref{eq: concat}.
    \item Feed Forward Network: The resulting output is passed through a feedforward network, which introduces non-linearity and allows the model to capture more complex relationships between the input and output Eq. \ref{eq: ffn}.

\end{enumerate}

By performing these steps for each layer in the encoder and decoder, the multi-head self-attention mechanism allows the Transformer architecture to capture rich semantic relationships between different words in a sequence and is highly effective for a wide range of NLP tasks. However, the conventional Transformer architecture has some limitations. One of the main issues is that the self-attention mechanism requires quadratic computation time concerning the input sequence length, making it difficult to scale the model to very long sequences \cite{raffel2020exploring}, and lower generalizability for a short sequence \cite{gao2021limitations}. Additionally, the self-attention mechanism treats all positions in the input sequence equally, which may not be optimal for certain types of inputs where some positions are more critical than others. While the Transformer model has shown state-of-the-art performance on many NLP tasks, it can still struggle to capture complex input-output relationships requiring more specialized models. 

Switch Transformers \cite{fedus2021switch}  attempt to address these limitations by introducing a mixture of expert (MoE) mechanisms that decompose the problem into smaller, simpler sub-problems, allowing the model to handle long sequences and complex input-output relationships better. As mentioned above, the multi-head self-attention mechanism in the Transformer model is motivated by the need to capture semantic relationships between words in a sequence, but it has limitations when dealing with short sequences \cite{gao2021limitations}. The MoE mechanisms allow the model to divide the sequence into smaller, more manageable segments and apply different experts to each segment. This approach has improved the model's performance on short sequence tasks and has achieved state-of-the-art results on several benchmarks \cite{xue2022go, lazaridou2021mind, fan2021beyond}. %; hence, it can overcome the limitations by increasing the capacity of the FFNs of conventional Transformers.

The critical difference in the mathematical equation of the Switch Transformer compared to the conventional Transformer is replacing the FFN with the MoE mechanism, shown in Fig. \ref{fig:trans_vs_switch} (right). In the conventional Transformer, the FFN consists of two linear layers with a ReLU activation function in between. The MoE mechanism, on the other hand, uses a set of expert networks to learn different aspects of the input data and then combines their outputs with a gating network. It allows the model to dynamically choose between multiple sets of parameters (i.e., expert modules) based on the input. This contrasts the original Transformer model in Eq. \ref{eq: ffn}, which uses a fixed set of parameters for all inputs. Formally, the MoE mechanism in the Switch Transformer can be represented by the following equation:

\begin{align}
    z_t = \sum_j g_j(x_t) * e_j(x_t)
\end{align}

\noindent where $g_j(x_t)$ is a gating function that determines the importance of expert module j for input $x_t$, and $e_j(x_t)$ is the output of expert module j for input $x_t$. The switch mechanism is implemented by learning the parameters of the gating functions, which are used to select the expert modules dynamically. This allows the model to adapt to different input distributions and perform better on various tasks. Here is a summary of how the MoE mechanism works in the Switch Transformer:

\begin{enumerate}
    \item  The input is split into multiple subspaces, and each subspace is processed by a separate expert. Each expert is a separate neural network trained to specialize in a specific subset of the input space.

    \item The output of each expert is a vector that represents its prediction for the given input subspace.

    \item A gating mechanism selects the most relevant expert for a given input. This gating mechanism takes the input and produces a set of weights that determine the importance of each expert's prediction.

    \item The final output is a weighted combination of the experts' predictions. The weights used in the combination are determined by the gating mechanism.

    %\item This final output is then used as input to the next network layer.

\end{enumerate}

Overall, the MoE allows the Switch Transformer to learn complex patterns in the input space by leveraging the specialized knowledge of multiple experts. The MoE framework enables the model to learn from multiple experts, each specialized in different aspects of the data, and combine their outputs to achieve better performance. This can lead to better performance on tasks requiring understanding inputs and offers a promising solution to the challenge of small datasets in clinical text classification. Consequently, the study uses its ability to capture the complex relationships between words and phrases in the clinical text.


\subsubsection{Pre-trained BERT-based Models for French}
Pre-trained BERT-based models have become increasingly popular, enabling researchers and practitioners to perform various language-processing tasks with unprecedented accuracy. While BERT \cite{devlin2019bert} was initially developed for English language processing, it has since been adapted to several other languages, including French. In this context, we will explore some of the most popular pre-trained BERT-based models for French language processing available from Huggingface.

%Pre-trained BERT-based models for the French language have opened up new possibilities for analyzing French text, providing a powerful tool for various applications, from sentiment analysis to machine translation.

CamemBERT \cite{martin2020camembert}: This is a pre-trained Transformer-based language model designed explicitly for processing French text. It is based on the Roberta architecture and was trained on a large corpus of French text that was filtered and pre-processed to improve the data quality. Its pre-training objective is a masked language model, where some input tokens are masked, and the model is trained to predict the missing tokens. Overall, CamemBERT is a highly effective tool for processing French language text and can be fine-tuned for specific downstream tasks or used for transfer learning in multilingual settings.


FlauBERT \cite{le2020flaubert}: It is based on the original BERT architecture and was trained on a large corpus of the French text. It has been shown to perform strongly on several natural language processing tasks in French, including named entity recognition and sentiment analysis. It also performs well on tasks related to French morphosyntaxes, such as part-of-speech tagging and dependency parsing. It was trained using a masked language model objective, where a portion of the input tokens are masked, and the model is trained to predict the missing tokens. FlauBERT is a powerful language model for processing French text that can be fine-tuned for specific downstream tasks.


FrALBERT \cite{CattanSR21} is a Transformer-based language model designed explicitly for text classification tasks in French. It is based on the ALBERT architecture and was trained on a large corpus of the French text. It has achieved state-of-the-art performance on several text classification tasks in French, including sentiment analysis, news categorization, and toxic comment classification. The model was fine-tuned using a supervised learning approach, where the model was trained on labeled data to predict the correct class label for a given input text. FrALBERT is available for download and can be fine-tuned on specific text classification tasks in French or used for transfer learning in multilingual settings.

DistillBERT \cite{sanh2019distilbert} is a smaller and more efficient version of the BERT architecture designed to reduce the computational and storage requirements of the model while maintaining its performance. It was trained on a large corpus of French text and has been shown to perform strongly on various natural language processing tasks, including text classification. It is particularly useful for text classification tasks in French, such as sentiment analysis and news categorization. DistillBERT is much smaller than the original BERT model, making it more suitable for deployment on resource-constrained devices or in applications where speed and efficiency are a concern. 


\section{Experimental Implementation}
\label{sec:exp_result}

\begin{table*}[t]
\centering
\caption{Models Hyperparameters}
\label{tab:models-hyperpara}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Hyperparameters  & CamemBERT & DistillBERT & FlauBERT & FrALBERT & Transformer & Switch Transformer \\ \hline
Hidden Layers    & 12        & 6           & 6        & 12       & 4           & 4                  \\ \hline
Total Parameters & 111 M     & 66.7 M      & 54.6 M   & 12.3 M   & 2.3 M       & 5.7 M              \\ \hline
\end{tabular}
\end{table*}

Table \ref{tab:models-hyperpara} shows the hyperparameters of different Transformer-based models used in this study, including CamemBERT, DistillBERT, FlauBERT, FrALBERT, Transformer, and Switch Transformer. The hyperparameters compared include hidden layers and total parameters. CamemBERT and FrALBERT have 12 hidden layers, whereas DistillBERT, FlauBERT, Transformer, and Switch Transformer have 6, 6, 4, and 4 hidden layers, respectively. Regarding to total parameters, CamemBERT has the highest number of parameters, with 111 million, followed by DistillBERT with 66.7 million parameters, and FlauBERT with 54.6 million parameters. FrALBERT, Transformer, and Switch Transformer have significantly fewer parameters, with 12.3 million, 2.3 million, and 5.7 million, respectively. The variation in hyperparameters across different models reflects the differences in the architecture and design of the models. This information is crucial for understanding each model's computational complexity and efficiency and helps select the most suitable model.

%Therefore, the machine specifications used for training are vital information to include when describing the models.

When training a machine learning model, the hardware and software specifications used for the training process can significantly impact the model's performance and efficiency. In this case, the models were trained on a local machine with a Quadro P620 GPU and CUDA library version 12. Including these specifications when describing the trained models can provide important context for others looking to replicate or build upon the work.

%The Quadro P620 is a mid-range graphics card for professional use, including machine learning applications. It has 512 CUDA cores and 2GB of GDDR5 memory. CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) created by NVIDIA, allowing NVIDIA GPUs to perform high-performance computing tasks. The version used for training, in this case, was CUDA 12.


% \begin{figure}[t]
% 	\centering
% 	\vspace{2pt}
% 	\includegraphics[scale=0.6]{photo/GPU.png}
% 	\vspace{-2pt}
% 	\caption{GPU Specification Summarization.}
% 	\vspace{-8pt}
% 	\label{fig:GPU}
% \end{figure}

\begin{table*}[t]
\centering
\caption{Hyperparameters of the fine-tuned models}
\label{tab:models-hyperpara-finetune}
\begin{tabular}{|l|l|l|l|}
\hline
Hyperparameters                & Pretrained BERT-based & Transformer  & Switch Transformer \\ \hline
Number of multi-head attention & N/A                     & 4            & 4                  \\ \hline
Number of Experts              & N/A                     & N/A            & 4                  \\ \hline
Batch size                     & 16                    & 16           & 16                 \\ \hline
%Epochs                         & 20                    & 40           & 40                 \\ \hline
Dropout                        & 0.5                   & 0.35         & 0.35               \\ \hline
Learning rate                  & Cosine annealed          & Cosine annealed & Cosine annealed      \\ \hline
Optimizer                      & Adam                  & AdamW        & AdamW              \\ \hline
Adam\_$\epsilon$                  & N/A                     & 5*1e-06      & 5*1e-06            \\ \hline
Maximum sequence length        & 256                   & 256          & 256                \\ \hline
\end{tabular}
\end{table*}

Defining the hyperparameters during the training process of Transformers is a critical step in achieving good performance. Hyperparameters are the settings that control the behavior of the training algorithm, and they can significantly impact the final performance of the model. Here are some of the critical hyperparameters that are tuned during the training process of BERT-based and Transformer models in this study:

\begin{itemize}
    \item \textbf{Maximum sequence length}: This is the maximum number of tokens that can be inputted into the model simultaneously. Setting an appropriate maximum sequence length can affect the performance and memory usage of the model. Due to computational constraints, the maximum sequence length varies from 128 to 256.

    \item \textbf{Batch size}: Choosing an appropriate batch size can affect the speed and stability of the training process. We varied the training batch size for each trial, ranging from 4 to 32 (with gradient accumulation as 4), based on the knowledge that training with smaller batches is more effective for highly low-resource language training \cite{atrio2021small}.

    %This determines the number of samples processed at once during each training iteration.
    
    \item \textbf{Drop-out}: This regularization technique randomly drops out some of the neurons during training to prevent overfitting. The dropout rate determines the proportion of neurons to drop out during each iteration \cite{srivastava2014dropout}.

   \item \textbf{Optimizers}: These algorithms update the model weights during training to minimize the loss function. Different optimizers have different strengths and weaknesses, and choosing the right one can impact the final performance of the model. Adaptive Moment Estimation (Adam) \cite{KingmaB14}, AdamW (Adam with weight decay) \cite{LoshchilovH19} were used.

    \item \textbf{Learning rate}: Consine annealed learning rate with warmup can help prevent training instability in the deeper layers of a neural network; its primary purpose is to help the model converge more quickly and effectively to a better solution overall \cite{otmareKXS19}.
    
    % This controls how much the model weights are updated during each iteration. Choosing an appropriate learning rate can affect the speed and accuracy of the training process. 
    
    \item \textbf{Number of multi-head attention}: This determines the number of attention heads used in the multi-head attention layer of the Transformer. Increasing the number of attention heads can improve the model's ability to attend to different input parts.

    \item \textbf{Number of experts}: This determines the number of experts used in the MoE layer of the Transformer. Increasing the number of experts can improve the model's ability to handle diverse inputs.
    
\end{itemize}
%Defining these hyperparameters can significantly impact the performance and efficiency of the model during training.

Choosing appropriate values for these hyperparameters requires careful experimentation and tuning to achieve the best possible results. Additionally, optimizing hyper-parameters is essential for achieving high performance in machine learning models, but this process comes with a tradeoff between the quality of the final solution and the time required for computation. However, not all hyperparameters significantly impact model accuracy, and only a few parameters require careful tuning. As reported in \cite{popel2018training}, the model size, learning rate, batch size, and maximum sequence length are the three critical hyper-parameters for Transformer model training. For this reason, grid search can be an efficient approach for optimizing these parameters by simultaneously exploring all possible combinations of intervals. Compared to Bayesian optimization, grid search has advantages in parallelization and flexibility of resource allocation \cite{yu2020hyper}. In this study, we used grid search to optimize hyper-parameters for model training. The combination with the highest estimated performance was considered the optimal solution, and this approach balances computational efficiency and models' accuracy.


Finally, table \ref{tab:models-hyperpara-finetune} presents the hyperparameters used to fine-tune three models. For the pre-trained BERT-based model, the number of multi-head attention and the number of experts are not applicable (N/A), as this model is already trained and does not require further customization. The batch size, epochs, dropout rate, learning rate, and optimizer for all models are specified. The trained BERT-based model uses an Adam optimizer with a dropout rate of 0.5 and a cosine decay learning rate. The Transformer and Switch Transformer models use an AdamW optimizer with a dropout rate of 0.35 and a cosine decay learning rate. The Adam\_$\epsilon$ is only specified for the Transformer and Switch Transformer models and is set to 5*1e-06. The maximum sequence length for all models is set to 256. The fine-tuning process for the pre-trained BERT-based model was performed for 40 epochs, while the Transformer and Switch Transformer models were fine-tuned for 70 epochs. Additionally, the GlorotNormal initializer \cite{glorot2010understanding}, batch normalization \cite{ioffe2015batch, bjorck2018understanding} are employed for models' stability, and balancing the classes by using the Bayes Imbalance Impact Index \cite{lu2019bayes} to deal with the imbalanced classes. Then, these hyperparameters were carefully chosen to achieve optimal performance and prevent overfitting. 

\begin{figure*}[t]
	\centering
	\vspace{2pt}
	\includegraphics[scale=0.625]{photo/Switch_results.png}
	\vspace{-2pt}
	\caption{Training and validation performance results from Switch Transformer model.}
	\vspace{-8pt}
	\label{fig:switch_results}
\end{figure*}

The data was divided into 80\% training and 10\% validation and 10\% testing. To assess the performance of our method, metrics including accuracy, precision, recall, and F1 score were used \cite{goutte2005probabilistic}. These metrics are defined as follows. 
\begin{align}
&\text {Accuracy}=\frac{\mathrm{TP}+\mathrm{TN}}{\mathrm{TP}+\mathrm{TN}+\mathrm{FP}+\mathrm{FN}} \nonumber \\ 
&\text {Precision}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}} \nonumber \\
&\text {Recall/Sensitivity}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}} \nonumber \\ 
&\text {F1-Score} =\frac{2^{\star} \text {Precision}^{\star} \text {Recall}}{\text {Precision }+\text {Recall}} \nonumber
\end{align}

\noindent where TN and TP stand for true negative and true positive, respectively, and are the number of negative and positive patients correctly classified. FP and FN represent false positives and false negatives and the number of incorrectly predicted positive and negative patients.


\section{Results and Discussion}
\label{sec:dis}

During training and validation shown in Fig. \ref{fig:switch_results}, the Switch Transformer model showed a gradual decrease in loss with increasing epochs. The loss started to converge after around 20 epochs and reached its minimum at the 30th epoch. Applying the early stopping at this point helped prevent the model's overfitting. The accuracy and precision of the model showed a smooth convergence to their optimal values for both the training and validation phases. However, the recall values for the two phases were observed to be quite fluctuating. The model's overall performance was good, with high accuracy, precision, and recall. The model's ability to reach its optimal values with smooth convergence and with the help of early stopping indicates the model's effectiveness in the given task.

%Despite this, t

The results presented in the table \ref{tab:model_comparisions} indicate that careful hyperparameter tuning can result in better performance of Transformer models over pre-trained BERT-based models for the given task. The table compares the performance of six classifiers with metrics such as accuracy, precision, recall, F1, and AUC. The classifiers include DistillBERT, CamemBERT, FlauBERT, FrALBERT, Transformer, and Switch Transformer. The results show that the best-performing classifier in accuracy, precision, recall, F1, and AUC is Switch Transformer, with an accuracy score of 0.87, precision of 0.87, recall of 0.85, F1 score of 0.86, and AUC of 0.92. The Transformer model has the second-best performance with an accuracy score of 0.85. DistillBERT, CamemBERT, and FrALBERT perform comparably well, with accuracy scores ranging from 0.80 to 0.83. The Switch Transformer and Transformer models achieved the best accuracy, precision, recall, F1 score, and AUC. These models demonstrated faster training and evaluation times than others, making them the most suitable options for the given task. However, it is essential to note that FlauBERT achieved the best precision, recall, F1 score, and AUC among all models, although it required longer training and evaluation times. Compared to other methods (excluding fine-tuning), mixture-of-experts (MoEs) is more efficient regarding the computational resources required \cite{artetxe2021efficient}. The study suggests that Switch Transformer and Transformer models are the most suitable for the given task, given their high performance and faster training and evaluation times. Overall, these findings suggest that careful selection of Transformer-based models and hyperparameter tuning can significantly improve the performance of small clinical narrative classification. 


\begin{table*}[]
\centering
\caption{A comparison performance of different classifiers}
\label{tab:model_comparisions}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
Model & Accuracy & Precision & Recall & F1 & AUC  & Training Time  & Evaluation Time  \\ \hline %& Validation Loss
DistillBERT         & 0.80     & 0.79      & 0.78   & 0.78  & 0.84    & 109   & 5\\ \hline % & 0.7432
CamemBERT           & 0.83     & 0.82      & 0.83   & 0.82  & 0.89    & 212   & 19\\ \hline %& 0.6904
FlauBERT            & 0.84     & 0.84      & 0.84   & 0.84  & 0.91    & 51    & 6\\ \hline  %& 1.6759
FrALBERT            & 0.83     & 0.82      & 0.81   & 0.81  & 0.89    & 196   & 19\\ \hline  %& 0.7338
Transformer         & 0.85     & 0.85      & 0.83   & 0.84  & 0.91  & \textbf{4}  & \textbf{1}\\ \hline  %& 0.5818    
Switch Transformer  & \textbf{0.87}     & \textbf{0.87}      & \textbf{0.85}   & \textbf{0.86}  & \textbf{0.92}    & 34    & 2\\ \hline % & 1.3658 
\end{tabular}
\end{table*}

\begin{figure*}[t]
	\centering
	\vspace{2pt}
	\includegraphics[scale=0.625]{photo/confusion_matrix.png}
	\vspace{-2pt}
	\caption{Confusion matrix comparison for all classifiers.}
	\vspace{-8pt}
	\label{fig:confu_matrix}
\end{figure*}


Fig. \ref{fig:confu_matrix} compares the confusion matrices obtained from six models. Each confusion matrix presents the number of true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) for binary classification tasks. This study labels the classes `0' for `Negative' and `1' for `Positive.' The Switch Transformer model obtained the highest number of TP and TN, with 253 and 219, respectively. It misclassified 34 instances as false positives and 38 instances as false negatives. DistillBERT, on the other hand, obtained 253 TP and 201 TN, with 54 instances misclassified as false positives and 56 instances as false negatives. FlauBERT and FrALBERT models had similar results with 246 TP and 215 TN and 241 TP and 209 TN, respectively. Both models misclassified around 15\% of instances. CamemBERT model obtained 239 TP and 214 TN, with 48 and 43 instances misclassified as false positives and false negatives, respectively. Finally, the Transformer model obtained 250 TP and 213 TN, with 37 and 44 instances misclassified as false positives and false negatives, respectively. In summary, the Switch Transformer model achieved the highest number of correct classifications and the lowest number of misclassifications, followed closely by the DistilBERT and Transformer models. The FlauBERT and FrALBERT models performed similarly, with slightly higher misclassifications. However, the CamemBERT model had the lowest number of correct classifications and a relatively high number of misclassifications. These results can guide the selection of models for future classification tasks. Particularly, it suggests that simpler models (in terms of the number of parameters) may perform better for non-English and limited clinical narrative datasets.

%In summary, the Switch Transformer model obtained the highest number of correct classifications and the lowest number of misclassifications, followed closely by DistillBERT and Transformer models. FlauBERT and FrALBERT models performed similarly with a slightly higher number of misclassifications. CamemBERT model obtained the lowest number of correct classifications and a relatively high number of misclassifications. These results can inform the choice of the model for future binary classification tasks.

\begin{figure}[t]
	\centering
	\vspace{2pt}
	\includegraphics[scale=0.75]{photo/Sharp_minima.png}
	\vspace{-2pt}
	\caption{Generalization gap and sharp minima during training Switch Transformer without early stopping.}
	\vspace{-8pt}
	\label{fig:sharp_minima}
\end{figure}


\begin{figure*}[t]
	\centering
	\vspace{2pt}
    \includegraphics[scale=0.627]{photo/Train_20_min.png}
    \includegraphics[scale=0.6275]{photo/Val_20_min.png}
	\vspace{-2pt}
	\caption{Hidden embedding visualization during training (top 4 figures) and validation (bottom 4 figures) for the Switch Transformer at the 30th epoch.}
	\vspace{-8pt}
	\label{fig:hiden_train_20}
\end{figure*}


Although the Switch Transformer outperforms several other models, including DistillBERT, CamemBERT, FlauBERT, FrALBERT, and the conventional Transformer model, its performance falls short when compared to two of our previous studies \cite{le2021detecting, le2023adaptation} that extensively analyzed a conceptual framework for detecting a patient's health condition from contextual input to output. The proposed framework in those studies utilized a combination of TF-IDF (term frequency-inverse document frequency) and MLP-NN (multilayer perceptron neural network), achieving an overall classification performance of 89\% accuracy, 88\% recall, and 89\% precision. Moreover, sparsity reduction significantly affected classifier performance in downstream tasks, and a generative AE (autoencoder) learning algorithm effectively leveraged sparsity reduction to help the MLP-NN classifier achieve 92\% accuracy, 91\% recall, 91\% precision, and 91\% F1-score. These findings suggest that the simpler frameworks are effective for this specific context and highlight the limitations of the Switch Transformer model.

While the Switch Transformer model has demonstrated promising results in clinical text classification, there is still room for further improvement of its performance. One possible area of investigation is the training methodology, as suggested by previous research \cite{hoffer2017train, nakkiran2021deep}. Specifically, the model was trained for 500 epochs without early stopping, which resulted in three distinctive phases in the learning curves of training and validation losses in Fig. \ref{fig:sharp_minima}. Initially, the model underwent the learning phase, where the loss gradually decreased and reached its minimum at epoch 30. Subsequently, the model entered the second phase, where overfitting occurred, and the loss increased sharply, reaching its maximum at epoch 120. Interestingly, the model experienced double descent, and the loss started decreasing again in the third phase and remained flat until nearly the end of the 400 epochs. During this phase, the classifier was confined to a sharp minimum and failed to improve further. Regarding accuracy, after achieving the optimal value, both learning curves from training and validation remained flat, which is expected. These are typical phenomena in deep learning models trained on small datasets, as the model tends to overfit the data and struggles with generalization. The classifier could not bridge the generalization gap caused by the sharp minima effect due to insufficient data explained in \cite{KeskarMNST17}.

Furthermore, we propose a novel perspective on this behavior and find a better illustration, viewing them through hidden embedding visualization for each layer during training and validation to explain their behavior. To illustrate this perspective, we present detailed visualizations of the Switch Transformer embedding for each layer (from 1 to 4) in Figure \ref{fig:hiden_train_20}. We utilize t-SNE, a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data into lower-dimensional data (2 dimensions in our case). By analyzing the hidden embedding from the model, we successfully observe the difference between the training and validation processes. The four top figures illustrate that after the 30th epoch, the model successfully separates the two classes (1: positive, 0: negative) in each hidden layer. Remarkably, the last hidden layer (4th layer) achieves perfect classification accuracy of 98\% on the training set. However, this level of performance does not carry over to the validation set at the same epoch. The four bottom figures demonstrate that the two classes overlap, and the model cannot learn a clear boundary between them, resulting in only 87\% validation accuracy. Therefore, we observe a generalization gap between the training and validation for a large model with small data.


%As shown in the four top figures, after the 30th epoch of training, for each hidden layer, the model learns to separate the two classes (1: positive, 0: negative), particularly at the last hidden layer (4th layer), where the model perfectly classifies the two labels. However, this process does not work well for the validation process at the same 30th epoch. The four bottom figures demonstrate that two classes overlap, and the model fails to learn the separable boundary for the two classes. Therefore, we observe a generalization gap between the training and validation for a large model with small data.

\section{Misclassification Interpretability}
\label{sec:misclass}

Interpretability of misclassifications is essential to model evaluation, particularly in critical applications such as medical diagnosis. In this study, we analyze the misclassification cases of the Switch Transformer model by visualizing the results from the misclassification. Totally, there are  72 cases of misclassification from the results of the Switch Transformer. Our focus has been primarily on the false negatives, where the true label indicates the presence of cardiac failure (True label is 1); however, our classifiers predict the opposite. We have referred to the labeled data to understand the reasons behind these misclassifications better. The clinician  analyzes and confirms which information was inferred to label the data. 

Technically, Integrated Gradients (IG) \cite{sundararajan2017axiomatic} are a powerful interpretability technique for explaining the predictions of deep learning models, including the Transformers model used in clinical text classification. IG provides a way to attribute importance to the input features of a model, allowing clinicians and researchers to gain insight into how the model is making its predictions. Then, we compared this information with the information from the classifier based on the IG methods. This helped us identify misclassification sources and improve our classifiers' accuracy in detecting cardiac failure. 


\begin{figure*}[!ht]
	\centering
	\vspace{2pt}
	\includegraphics[scale=0.6]{photo/Misclassification.png}
	\vspace{-2pt}
	\caption{The highlighted misclassification cases from the Switch Transformer model.}
	\vspace{-8pt}
	\label{fig:Misclassification}
\end{figure*}

%This tutorial showcases the application of Integrated Gradients (IG), an Explainable AI method introduced in the paper Axiomatic Attribution for Deep Networks, for clinical text classification. IG aims to explain the correlation between the features of a model and its predictions. It has various applications, such as identifying data skew, debugging model performance, and understanding feature importances.

%IG has become a favored interpretability technique due to its broad applicability to any differentiable model (e.g., text, images, structured data), ease of implementation, theoretical justifications, and computational efficiency relative to alternative approaches, enabling it to scale to large networks and feature spaces such as text.

%This tutorial presents a step-by-step implementation of IG to determine the feature importance of a clinical text classifier. For instance, when classifying a report about a patient with chest pain, the model may highlight the presence of specific symptoms or medical conditions as being crucial to the prediction. The tutorial will examine if the model highlights the same features as necessary when explaining its decision.



%One benefit of using IG for explaining the Transformers model in clinical text classification is that it can help identify which words or phrases in a clinical report are most influential in the model's decision-making process. This can aid in understanding how the model processes information and makes predictions, which can be especially important in the medical domain, where decisions can have life-or-death consequences.

%Another benefit of IG is that it can be applied to any different model, making it a versatile technique for explaining a wide range of deep learning models, including those used in clinical text classification. Additionally, IG has theoretical justifications and computational efficiency relative to alternative approaches, allowing it to scale to large networks and feature spaces such as clinical reports.

The results in Fig. \ref{fig:Misclassification} demonstrate the Transformer model's ability to calculate attribution scores to predict output based on input features. The sign of the attribution score indicates the direction of the feature's influence on the output: a positive score means that the feature positively influences the output, while a negative score indicates a negative influence. However, the model did not perform well on the task at hand. The correct labeling of the data requires clinical expertise and professional knowledge. For example, in the first original note, the absence of data on cardiac failure was compensated for by the presence of other clinical signs such as `Souffle 3/6,' `tr√®s faible pouls f√©moral mais pas de pouls p√©dieux (very weak femoral pulse but no pedal pulse),' and `Pieds ti√®des (warm feet).' Similarly, in the second note, no data on cardiac failure was present, but `st√©nose sous pulmonaire et CIV large (subpulmonary stenosis and wide CIV)' suggested its presence. These examples highlight the significant gap in the Transformer model's contextual learning and understanding of real clinical datasets. There are two possible reasons for this limitation. First, while Transformer models have shown promising performance in new tasks, it remains unclear if they can generalize across the differences in settings within the clinical domain \cite{bear2021clinically}. Second, the tasks in the clinical domain often have a low signal-to-noise ratio, where the presence of a few essential keywords may suffice to determine a specific label. In contrast, Transformer's training process involves learning intricate and nuanced relations between all words in the pretraining corpus, which may not be relevant for the classification task and may shift attention away from the critical keywords \cite{gao2021limitations}. 


%The clinical domain generates extensive information through clinical notes, which can be utilized for various clinical applications. However, the heterogeneity of clinical institutions and settings poses challenges to processing this information. To address this challenge, the clinical natural language processing field has made significant strides in overcoming domain heterogeneity by employing pre-trained deep learning models, which can transfer knowledge from one task to another. 

%The tasks in the clinical domain often have a low signal-to-noise ratio, where the presence of a few essential keywords may suffice to determine a specific label. In contrast, BERT's pretraining process involves learning intricate and nuanced relations between all words in the pretraining corpus, which may not be relevant for the classification task and may shift attention away from the critical keywords \cite{gao2021limitations}. 



\section{Conclusion}
\label{sec:conclusion}

We compared the performance of 6 classifiers on a binary classification task: CamemBERT, DistillBERT, FlauBERT, FrALBERT, Transformer, and Switch Transformer. The results indicated that careful hyperparameter tuning could significantly improve the performance of Transformer models over pre-trained BERT-based models. The Switch Transformer model achieved the highest performance in Accuracy, Precision, Recall, F1, and AUC, with an accuracy score of 0.87, precision of 0.87, recall of 0.85, F1 score of 0.86, and AUC of 0.92. The Transformer model achieved the second-best performance, with an accuracy score of 0.85.

Furthermore, we presented the confusion matrices obtained from six models. The results indicated that the Switch Transformer model obtained the highest number of correct classifications and the lowest number of misclassifications, followed closely by the DistillBERT and Transformer models. FlauBERT and FrALBERT models performed similarly, with slightly higher misclassifications. Finally, the CamemBERT model obtained the lowest number of correct classifications and a relatively high number of misclassification.

The study used attribution scores to demonstrate the Transformer model's ability to predict output based on input features. However, the model did not perform very well on the clinical dataset due to its inability to contextualize and understand real-world data. The clinical tasks have a low signal-to-noise ratio, and the Transformer's training process may shift attention away from critical keywords. Additionally, it remains unclear whether Transformer models can generalize across different settings in the clinical domain. Overall, the results suggest the need for further research to improve the Transformer model's performance in clinical settings.


These findings suggest that careful selection of Transformer-based models and hyperparameter tuning can significantly improve the performance of clinical narrative classification tasks. Especially the CDSS at CHUSJ is currently under development. By combining this NLP algorithm to detect the absence of heart failure with the two other algorithms already developed on hypoxemia detection \cite{sauthier2021estimated} and chest, X-ray analysis \cite{zaglam2014computer, yahyatabar2020dense}, the next step of our study is to implement the resulting CDSS (integration of the three algorithms) within the cyberinfrastructure of the pediatric intensive care unit (PICU) at Sainte-Justine Hospital to diagnose ARDS early. We will then verify the ability of the CDSS to detect ARDS prospectively once the integration with the PICU e-Medical infrastructure is completed.

\section{Future Works}

The study only considers binary classification tasks and does not examine the performance of Transformer-based models on multiclass classification tasks. The dataset used for the study is relatively small, with almost more than 5000 instances, which may limit the generalizability of the findings to larger datasets. The study did not examine the impact of fine-tuning on the performance of the Transformer-based models. To improve the performance of this study, some potential solutions would be 1) including multiclass classification tasks to examine the performance of Transformer-based models on more complex classification tasks; 2) expanding the dataset to increase the generalizability of the findings. The impact of fine-tuning could be examined to determine if it improves the performance of the Transformer-based models. In summary, potential future directions could be explored as follows:
\begin{enumerate}
    \item Model optimization:  Transformer-based models can be optimized to reduce their computational requirements while maintaining accuracy, such as using distillation or pruning methods to reduce the number of parameters.

    \item Data augmentation: Data augmentation techniques can be used to increase the amount of labeled data available for training Transformer-based models, such as using synthetic data generation methods or unsupervised learning techniques to leverage unlabeled data.

    \item Domain-specific pre-training: pre-trained Transformer-based models on clinical text data can be employed to improve their understanding of domain-specific language and performance on clinical text classification.

   % \item Explainable AI: Researchers can develop techniques to make Transformer-based models more interpretable, such as using attention visualization or sensitivity analysis to understand which parts of the input text the model focuses on during prediction.

\end{enumerate}

%In summary, Transformer-based models have shown promising results for small clinical text classification, but they also have limitations and potential challenges that must be addressed. We can optimize Transformer-based models, use data augmentation techniques, perform domain-specific pretraining, and develop techniques for explainable AI to improve their performance and interpretability on clinical text classification.


\section*{Acknowledgment}

Clinical data were provided by the Research Center of CHU Sainte-Justine hospital, University of Montreal. The authors thank Dr. Sally Al Omar, Dr. Michael Sauthier, Dr. Rambaud Jerome and Dr. Sans Guillaume for their data support of this research. This work was supported by a scholarship from the Fonds de recherche du Quebec-Nature et technologies (FRQNT) to Thanh-Dung Le, and the grants from the Natural Sciences and Engineering Research Council (NSERC), the Institut de valorisation des donnees (IVADO), and the Fonds de la recherche en sante du Quebec (FRQS).

%The authors thank Dr. Sally Al Omar, Dr. Rambaud Jerome and Dr. Sans Guillaume for their data support of this research

\bibliographystyle{IEEEtran}
%\IEEEtriggeratref{42}
\flushend
\bibliography{IEEEabrv,Bibliography}
\end{document}
