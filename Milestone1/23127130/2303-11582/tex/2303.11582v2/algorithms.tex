\section{Bayesian Adaptive Experimentation}
\label{section:algorithms}

It is natural for the experimenter to have a prior distribution $h \sim \nu$
over the relative gap between average rewards. For example, modern online
platforms run thousands of experiments from which a rich reservoir of
previously collected data is available~\cite{KohaviLoSoHe09,
  KohaviDeFrLoWaXu12, KohaviDeFrWaXuPo13}. In this work, we focus on
minimizing the \emph{Bayes simple regret} at the end of the experiment
\begin{equation}
  \label{eqn:bsr}
  \bsr_{T}(\pi,\nu,\bar{R})
  \defeq \E_{h \sim \nu} \E[h_{a\opt} - h_{\what{a}}]
  ~~~\mbox{where}~~\what{a} \sim \pi_T~\mbox{and}~a\opt \in \argmax_a h_a,
\end{equation}
the scaled optimality gap between the final selection $\what{a}$ and the
optimal arm $a\opt$ averaged over the prior.  The notation
$\bsr_{T}(\pi,\nu,\bar{R})$ considers adaptive policies
$\pi = \{\pi_t\}_{t=0}^T$, prior $\nu$ over $h$, and observation process
$\bar{R} = (\bar{R}_0,\ldots, \bar{R}_{T-1} )$, which is the set of aggregate
rewards used by the policy to determine the sampling allocations.

Instead of optimizing the Bayes simple regret for each finite batch size, we
use Theorem~\ref{theorem:limit} to derive an asymptotic approximation under
the Gaussian sequential experiment.  
\begin{corollary}
  \label{cor:bsr-limit}
  Let $\nu$ be a prior over average rewards $h$ satisfying
  $\E_{h \sim \nu} \lone{h} < \infty$ and let
  Assumptions~\ref{assumption:reward},~\ref{assumption:policy} hold. Then, the
  Bayes simple regret under $\bar{R}^n$ can be approximated by that under the
  Gaussian sequential experiment $G$ in Definition~\ref{def:gse}:
  \begin{equation}
    \label{eqn:bsr-limit}
    \bsr_{T}(\pi,\nu, \sqrt{n} \bar{R}^{n}) \to \bsr_{T}(\pi,\nu, G).
  \end{equation}
\end{corollary}
\noindent See Section~\ref{section:proof-bsr-limit}
for a proof of the corollary.

Using the approximation of Bayes simple regret, we optimize the asymptotic
Bayesian objective
\begin{equation}
  \label{eqn:gse-bsr}
  \minimize_{\pi}~~\left\{
    \bsr_{T}(\pi,\nu, G) = \E_{h \sim \nu} \E[h_{a\opt} - h_{\what{a}}]
    \right\}
\end{equation}
over policies $\pi = \{\pi_t(G_0, \ldots, G_{t-1})\}_{t=0}^T$ adapted to the
sequential observations $G_0, \ldots, G_{T-1}$ of the Gaussian sequential
experiment.  Policies derived from the optimization
problem~\eqref{eqn:gse-bsr} has marked modeling flexibility and computational
advantages compared to typical Bayesian sequential sampling algorithms (e.g.,
variants of Thompson sampling~\cite{Russo20}). Although reward distributions
are unknown in general, Bayesian sequential sampling approaches require a
distributional model of individual rewards comprising of prior and likelihood
function~\cite{RussoVaKaOsWe18}.  In contrast, our framework does not assume
restrictive distributional assumptions on rewards and allows naturally
incorporating prior information over average rewards $h_a$. As a result, our
limiting Gaussian sequential experiment coincides with typical frequentist
inferential paradigms for calculating confidence intervals and statistical
power.

Computationally, policies derived from the optimization
problem~\eqref{eqn:gse-bsr} can be efficiently updated \emph{offline}. As
these offline updates give a fixed sampling probability over the $\numarm$
arms at each epoch, the policies we propose in subsequent sections can be
deployed to millions of units at ease. This is in stark contrast to Bayesian
sequential sampling algorithms designed for unit-level feedback. While their
policy updates are also often performed offline in batches in
practice~\cite{OfferWestortCoGr20, EspositoSa22}, when deployed these policies
require \emph{real-time} posterior inference and action optimization to
generate treatment assignments for each unit. Implementing such methods at
scale is highly challenging even for the largest online platforms with mature
engineering infrastructure~\cite{AgarwalBiCoHoLaLeLiMeOsRi16,
  NamkoongDaBa20}. Methods derived from our formulation provides a scalable
alternative as deploying the resulting adaptive policies only involves
sampling from a fixed sampling probability $\pi_t$ for every unit in a batch,
\emph{regardless} of the batch size.

% Although reward distributions are unknown in
% general, Bayesian sequential sampling approaches require a distributional
% model of individual rewards comprising of prior and likelihood
% function~\cite{RussoVaKaOsWe18}.  In contrast, our framework does not assume
% restrictive distributional assumptions on rewards and allows naturally
% incorporating prior information over average rewards $h_a$. In our approach,
% the likelihood function of the aggregate rewards
% $\sqrt{n} \bar{R}_{t, a}^{n} \mid h$ is derived from the normal
% approximation~\eqref{eqn:weak-convergence}. As a result, our limiting Gaussian
% sequential experiment coincides with typical frequentist inferential paradigms
% for calculating confidence intervals and statistical power.

% Computationally, policies derived from our asymptotic optimization
% problem~\eqref{eqn:gse-bsr} can be efficiently updated offline. As these
% offline updates give a fixed sampling probability over the $\numarm$ arms at
% each epoch, the policies we propose in subsequent sections can be deployed to
% millions of units at ease. This is in stark contrast to Bayesian sequential
% sampling algorithms designed for unit-level feedback. While their policy
% updates are also often performed offline in batches in
% practice~\cite{OfferWestortCoGr20, EspositoSa22}, when deployed these policies
% require \emph{real-time} posterior inference and action optimization to
% generate treatment assignments for each unit. Implementing such methods at
% scale is highly challenging even for the largest online platforms with mature
% engineering infrastructure~\cite{AgarwalBiCoHoLaLeLiMeOsRi16,
%   NamkoongDaBa20}. Methods derived from our formulation provides a scalable
% alternative as deploying the resulting adaptive policies only involves
% sampling from a fixed sampling probability $\pi_t$ for every unit in a batch,
% \emph{regardless} of the batch size.

Using the Gaussian sequential experiment as an asymptotic approximation, we
derive a dynamic program that solves for the adaptive allocation minimizing
the Bayes simple regret~\eqref{eqn:gse-bsr}. Policies derived from this
dynamic program---which we call the (limiting) Bayesian adaptive
experiment---are tailored to the signal-to-noise ratio in each problem
instance and the number of reallocation opportunities $T$. In the Bayesian
adaptive experiment, we observe a Gaussian draw $G_t$ at each epoch and use it
to perform posterior updates over the experimenter's belief on the average
rewards $h$. In Section~\ref{section:algorithms-mdp}, we formulate the
sequential updates using a Markov decision process where state transitions are
governed by changes in the posterior mean and variance. Crucially, the dynamics of the MDP
and its value functions are differentiable with respect to sampling allocations, 
allowing the application of gradient-based methods for developing dynamic policies.
In Section~\ref{section:algorithms-adp}, we introduce a range of adaptive
sampling algorithms that (approximately) solve the Bayesian dynamic
program. In particular, our main proposed method, $\algofull$, solves an
open-loop problem that optimize future sampling allocations that only depend
on currently available information.

\subsection{Markov decision process}
\label{section:algorithms-mdp}

The normal approximations in the previous section gives Gaussian likelihood
functions in our Bayesian adaptive experiment
\begin{equation*}
  \mbox{Likelihood function:}~~~~
  G \mid h \sim N\left(\pi h, \diag\left( \frac{\pi s^2}{b} \right)\right),
\end{equation*}
where we abuse notation to describe elementwise algebraic operations over
$\numarm$-dimensional vectors. (Recall that $s^2 \in \R^{\numarm}$ is the
measurement variance on the raw rewards~\eqref{eqn:rewards}.) To achieve
conjugacy, we assume that the experimenter has a Gaussian prior over the
average reward
\begin{equation}
  \label{eqn:prior}
 \mbox{Prior:}~~~~ h \sim N\left(\mu_0, \diag(\sigma_0^2)\right) \eqdef \nu,
\end{equation}
where $\mu_0, \sigma_0^2 \in \R^{\numarm}$ are the prior mean and
variance. Standard posterior updates for Gaussian conjugate priors give the
following recursive formula for the posterior mean and variance
\begin{subequations}
  \label{eqn:posterior}
  \begin{align}
    \mbox{Posterior variance:}~~~~ \sigma_{t+1, a}^{-2}
    & \defeq \sigma_{t, a}^{-2} + s_a^{-2} b_t \pi_{t, a}
      \label{eqn:posterior-var} \\
    \mbox{Posterior mean:}~~~~\mu_{t+1, a}
    & \defeq   \sigma_{t+1, a}^{2}
      \left( \sigma_{t, a}^{-2} \mu_{t, a} + s_a^{-2}b_t G_{t, a} \right).
      \label{eqn:posterior-mean}
  \end{align}
\end{subequations}
The posterior variance decreases as a deterministic function of the sampling
allocation $\pi_t$, and in particular does not depend on the observation
$G_t$. 

Under the posterior updates~\eqref{eqn:posterior}, our goal is to optimize the
Bayes simple regret~\eqref{eqn:gse-bsr} at the end of the experiment (time
$t = T$). We consider the final allocation that simply selects the arm with
the highest posterior mean; formally, if the $\argmax_a \mu_{T, a}$ is unique,
this is equivalent to 
\ifdefined\msom
$\pi_{T, a} = 1\{a = \argmax_{a} \mu_{T, a}\}$
\else
\begin{equation*}
  \pi_{T, a} = \begin{cases}
    1 ~~&~\mbox{if}~a = \argmax_{a} \mu_{T, a} \\
    0 ~~&~\mbox{if}~a \neq\argmax_{a} \mu_{T, a}. 
  \end{cases}
\end{equation*}
\fi
(More generally, we choose an arm randomly from $\argmax_a \mu_{T, a}$; this
does not change any of the results below.)  Then, minimizing the Bayes simple
regret under the asymptotic Gaussian sequential experiment~\eqref{eqn:gse-bsr}
is equivalent to the following reward maximization problem
\begin{equation}
  \label{eqn:bayesian-dp}
  \maximize_{\pi_0, \ldots, \pi_{T-1}}~~\E^{\pi} \left[ \max_a \mu_{T, a} \right].
\end{equation}
Here, we write $\E^\pi$ to denote the expectation under the stochastic
transitions~\eqref{eqn:posterior} induced by the policy
$\pi_0, \ldots, \pi_{T-1}$ adapted to the observation sequence
$G_0, \ldots, G_{T-1}$. Although we focus on the Bayes simple regret in this
work, our MDP formulation can accomodate any objective function or constraint
that depends on the posterior states, such as the cumulative regret,
probability of correct selection, simple regret among the top-$k$ arms, and
constraints on the sampling allocations.

Noting that the preceding optimization problem is entirely characterized by
the prior~\eqref{eqn:prior} and the posterior updates~\eqref{eqn:posterior},
we view $(\mu_t, \sigma_t)$ as the \emph{states} of a Markov decision
process. The state transitions are governed by posterior
updates~\eqref{eqn:posterior} which depend on the continuous actions
$\pi_t \in \Delta^\numarm$. Given this Markovian structure in the limit
adaptive experiment, we restrict attention to sampling allocations
$\pi_t(\mu_t, \sigma_t)$ that only depend on the current state
$(\mu_t, \sigma_t)$. We present algorithms for (approximately) solving the
Bayesian dynamic program~\eqref{eqn:bayesian-dp} in the next subsection.

The state transitions~\eqref{eqn:posterior} are derived under the idealized
the Gaussian sequential experiment. In practice, any policy $\pi$ derived in
this asymptotic regime will be applied to a finite batch problem. Recalling
the asymptotic approximation~\eqref{eqn:weak-convergence}, the finite batch
problem will involve states $(\mu_{n,t}, \sigma_{n,t})$ updated according to
the same dynamics~\eqref{eqn:posterior}, but using the sample mean estimator
$\sqrt{n}\bar{R}^{n}_{t}$ instead of the Gaussian approximation $G_t$
\begin{subequations}
  \label{eqn:pre-limit-posterior}
  \begin{align}
    \mbox{Pre-limit posterior variance:}~~~~ \sigma_{n,t+1, a}^{-2}
    & \defeq \sigma_{n,t, a}^{-2} + s_a^{-2} b_t \pi_{t, a}(\mu_{n,t}, \sigma_{n,t})
      \label{eqn:pre-limit-posterior-var} \\
    \mbox{Pre-limit posterior mean:}~~~~\mu_{n,t+1, a}
    & \defeq   \sigma_{n,t+1, a}^{2}
      \left( \sigma_{n,t, a}^{-2} \mu_{n,t, a} + s_a^{-2}b_t \sqrt{n}\bar{R}_{t,a}^{n} \right).
      \label{eqn:pre-limit-posterior-mean}
  \end{align}
\end{subequations}
Given any policy $\pi_t(\mu_t, \sigma_t)$ derived from the Bayesian dynamic
program~\eqref{eqn:bayesian-dp} and average rewards $h$, the pre-limit
posterior state $(\mu_{n,t}, \sigma_{n,t})$ evolves as a Markov chain and each
adaptive allocation to be used in the original finite batch problem is given
by $\pi_{t}(\mu_{n,t}, \sigma_{n,t})$.


For policies $\pi$ that is continuous in the states---satisfying conditions in
Assumption~\ref{assumption:policy}---Corollary~\ref{cor:bsr-limit} implies
that the Bayesian dynamic program~\eqref{eqn:bayesian-dp} is an accurate
approximation for measuring and optimizing the performance of the policy as
$n$ grows large. Moreover, we can show that the trajectories of pre-limit
posterior beliefs $(\mu_{n,t}, \sigma_{n,t})$~\eqref{eqn:pre-limit-posterior}
can be approximated by trajectories of the Markov decision process derived
using the Gaussian sequential experiment~\eqref{eqn:dynamics}.
\begin{corollary}
  \label{cor:bayes-limit}
  Let $\pi_t(\mu_t, \sigma_t)$ be a policy derived under the limit Bayesian
  dynamic program~\eqref{eqn:bayesian-dp} that is continuous as a function of
  $(\mu_t, \sigma_t)$.  Let Assumption~\ref{assumption:reward} hold and
  consider any fixed average reward $h$ and prior $(\mu_{0}, \sigma_{0})$. The
  posterior states~\eqref{eqn:pre-limit-posterior} converge to the
  states~\eqref{eqn:posterior} as $n\to\infty$
  \begin{equation*}
    (\mu_{n,t}, \sigma_{n,t},\ldots,\mu_{n,T-1}, \sigma_{n,T-1})
    \cd (\mu_{t}, \sigma_{t},\ldots,\mu_{T-1}, \sigma_{T-1})
  \end{equation*}
\end{corollary}
\noindent See Section~\ref{section:proof-bayes-limit} for the proof.


% Theorem~\ref{theorem:limit} guarantees that as $n$ grows large
% $(\mu_{n,t}, \sigma_{n,t})$ will converge to $(\mu_{t}, \sigma_{t})$, the
% Markov chain of posterior states in the Gaussian Sequential Experiment.


\subsection{Adaptive designs as approximate DP methods}
\label{section:algorithms-adp}

We now derive algorithms for solving the limit Bayesian dynamic
program~\eqref{eqn:bayesian-dp}. To simplify things further, we use a change
of variable to reparameterize state transitions~\eqref{eqn:posterior} as a
random walk.
\begin{lemma}
  \label{lemma:mdp}
  Let $h \sim N(\mu_0, \sigma_0^2)$ and
  $Z_0, \ldots, Z_{T-1} \simiid N(0, I_{\numarm})$ be standard normal
  variables. The system governed by the posterior
  updates~\eqref{eqn:posterior} has the same joint distribution as that with
  the following state transitions
  \begin{subequations}
    \label{eqn:dynamics}
    \begin{align}
      \sigma_{t+1, a}^{-2}
      & \defeq \sigma_{t, a}^{-2} + s_a^{-2} b_t \pi_{t, a}(\mu_t, \sigma_t)
        \label{eqn:dynamics-var} \\
        %\defeq   \mu_{t, a} + \left( \sigma_{t, a}^2 - \sigma_{t+1, a}^2\right)^{\half} Z_{t, a}
      \mu_{t+1, a}
      &:= \mu_{t, a} + \sigma_{t, a}\sqrt{\frac{b_t \pi_{t, a}(\mu_t, \sigma_t)
        \sigma_{t, a}^{2}}{s_a^{2}+b_t\pi_{t, a}(\mu_t, \sigma_t)\sigma_{t, a}^{2}}} Z_{t, a}.
        \label{eqn:dynamics-mean}
    \end{align}
  \end{subequations}
\end{lemma}
\noindent We defer derivation details to Section~\ref{section:proof-mdp}.

For the Markov decision process defined by the reparameterized state
transitions~\eqref{eqn:dynamics}, the value function for the Bayes simple regret
$\E^{\pi} \left[ \max_a \mu_{T, a} \right]$ has the following succinct
representation
\begin{align}
  V_{t}^{\pi}(\mu_{t},\sigma_{t})
  & = \E^{\pi} \left[\max_a \mu_{T,a} \mid \mu_t, \sigma_t \right] \nonumber \\
  & = \E^{\pi} \left[ \max_{a} \left\{ \mu_{t,a} + \sum_{v=t}^{T-1} \sigma_{v,a}\sqrt{\frac{b_v \pi_{v, a}(\mu_{v},\sigma_{v})\sigma_{v, a}^{2}}{s_a^{2}+b_v\pi_{v, a}(\mu_{v},\sigma_{v})\sigma_{v, a}^{2}}}Z_{v, a}
    \right\} ~\Bigg|~ \mu_t, \sigma_t \right].   \label{eqn:q_fn}
\end{align}
In what follows, we use the notational shorthand
$\E_t^{\pi}[\cdot] = \E^{\pi}[\cdot \mid \mu_t, \sigma_t^2]$ so that
$V_{t}^{\pi}(\mu_{t},\sigma_{t}) = \E^{\pi}_t \left[\max_a \mu_{T,a} \right]$.
Since the states $(\mu_t, \sigma_t)$ and actions $\pi_t$ are both continuous,
using dynamic programming to directly solve the policy optimization problem
\begin{equation}
  \label{eqn:opt-dp}
  \maximize_{\pi_0, \ldots, \pi_{T-1}}~
  \left\{ V_{0}^{\pi}(\mu_{0},\sigma_{0}) =
    \E^{\pi}_0 \left[\max_a \mu_{T,a} \right]
    \right\}
\end{equation}
is computationally intractable even for a moderate number of arms and
reallocation epochs. 

A key property of this MDP is that the state transitions are differentiable
with respect to the sampling allocations along any sample path
$Z_0,...,Z_{T-1}$.  As a result, standard Monte Carlo approximations of the
value function are differentiable with respect to the sampling allocations,
allowing the use of gradient-based methods for planning and (approximately)
solving the DP.

\begin{algorithm}[t]
  \caption{\label{alg:rho} $\algofull$}
  \begin{algorithmic}[1]
    \State \textsc{Input: prior mean and variance on average rewards $(\mu_0, \sigma_0)$}
    \State Initialize pre-limit states $(\mu_{n, 0}, \sigma_{n, 0}) = (\mu_0, \sigma_0)$
    \For{each epoch $t \in 0, \ldots, T-1$}
    \State Letting $\bar{b}_t \defeq \sum_{v=t}^{T-1} b_v$, solve the following (e.g., using stochastic gradient methods)
    \begin{equation}
      \label{eqn:rho}
      \rho_{t}(\mu_t,\sigma_t) \in
      \argmax_{\bar{\rho} \in \Delta_\numarm}
      \left\{ V^{\bar{\rho}}_{t}(\mu_{t},\sigma_{t})
        \defeq \E_{t}  \left[ \max_{a} \left\{ \mu_{t,a}
            + \sqrt{\frac{\sigma_{t, a}^4 \bar{\rho}_{a} \bar{b}_{t}}
              {s_a^2 + \sigma_{t, a}^2 \bar{\rho}_{a} \bar{b}_{t}}} Z_{t, a}
          \right\} \right] \right\}
    \end{equation}
    \State For each unit $j =1, \ldots, b_t n$,
    sample arms according to $\rho_{t}(\mu_t,\sigma_t)$ 
    and observe reward $R^t_{a, j}$ if  arm $a$ was sampled ($\xi_{a, j}^t = 1$)
    \State Use aggregate rewards 
    $\sqrt{n} \bar{R}_{t, a}^{n} = \frac{1}{b_{t}\sqrt{n}}
    \sum_{j=1}^{b_{t}n}\xi_{a,j}^{t} R^{t}_{a,j}$ and  the
    formula~\eqref{eqn:pre-limit-posterior} to compute the next
    state transition $(\mu_{n, t+1}, \sigma_{n, t+1})$ 
    \EndFor 
    \State \Return $\argmax_{a \in [\numarm]} \mu_{n, T, a}$
  \end{algorithmic}
\end{algorithm}


\paragraph{Residual Horizon Optimization (RHO)} We propose a simple yet
effective method that solves a computationally cheaper approximation of the
dynamic program by assuming future allocations are non-adaptive. Instead of
maximizing the value function $\E_{t}^{\pi} [\max_a \mu_{T,a} ]$ over all
fully adaptive allocations, $\algo$ iteratively solves the open-loop planning
problem that considers future sampling allocations that only depend on the
available information $(\mu_t, \sigma_t)$ at time $t$. At each epoch $t$, the
allocation $\rho_t(\mu_t, \sigma_t)$ is derived by assuming that a fixed
sequence of allocations will be used for the remaining periods horizon
regardless of new information obtained in the future
\begin{align}
  \label{eqn:rho-planning}
  \rho_{t}(\mu_t,\sigma_t)
  \in  \argmax_{\bar{\rho}_{t},\ldots,\bar{\rho}_{T-1} \in \Delta_\numarm}
    ~\E_{t} \left[ \max_{a} \left\{ \mu_{t,a}
    + \sum_{v=t}^{T-1} \sigma_{v,a}\sqrt{\frac{b_v \bar{\rho}_{v,a}\sigma_{v, a}^{2}}
    {s_a^{2}+b_v \bar{\rho}_{v,a}\sigma_{v, a}^{2}}}Z_{v, a}
    \right\}~~\Bigg|~~ \mu_{t},\sigma_{t} \right],
\end{align}
where $\rho_{t}(\mu_{t}, \sigma_{t}) = \rho_{t}^{*}$ for the sequence $\rho_{t}^{*},\ldots,\rho_{T-1}^{*}$
that maximizes the planning objective.

The planning problem~\eqref{eqn:rho-planning} can be simplified to consider a
constant allocation $\bar{\rho}\in \Delta_{K}$ to be deployed for every
remaining period.  For any fixed sequence of allocations that only depend on
currently available information, the value function over remaining horizon is
equivalent to a modified one-step lookahead value function.  Intuitively,
since the allocation does not change as the experiment progresses, we can
accumulate updates to the posterior beliefs at each epoch and think of it as a
single update to the current belief, resulting in the terminal posterior
belief.  We summarize how our procedure will be applied to finite batch
problems in Algorithm~\ref{alg:rho}, and formalize this insight in the below
result. See Section~\ref{section:proof-rho-reduction} for its proof. 
\begin{lemma}
  \label{lemma:rho-reduction}
  Let $\bar{b}_t \defeq \sum_{v=t}^{T-1} b_v$. For any sequence of future
  allocations $\bar{\rho}_{t},\cdots,\bar{\rho}_{T-1} \in \Delta_\numarm$ that only
  depends on $(\mu_t,\sigma_t)$, there is a constant allocation
  $\bar{\rho}(\mu_t, \sigma_t)$ achieving the same Bayes simple regret 
  \begin{equation*}
    V^{\bar{\rho}_{t:T-1}}_{t}(\mu_{t},\sigma_{t}) = V^{\bar{\rho}}_{t}(\mu_{t},\sigma_{t})
    ~~\mbox{where}~~V^{\bar{\rho}}_{t}(\mu_{t},\sigma_{t})~\mbox{is defined in Eq.~}\eqref{eqn:rho}.
  \end{equation*}
  Thus, it is sufficient to solve the constant allocation planning
  problem~\eqref{eqn:rho} to achieve the same optimal value as the original
  problem~\eqref{eqn:rho-planning}.
  % \begin{equation}
  %   \label{eqn:rho}
  %   \maximize_{\bar{\rho} \in \Delta_\numarm}
  %   \left\{ V^{\bar{\rho}}_{t}(\mu_{t},\sigma_{t})
  %     =  \E_{t} \left[ \max_{a} \left\{ \mu_{t,a}
  %         + \sqrt{\frac{\sigma_{t, a}^4 \bar{\rho}_{a} \bar{b}_{t}}
  %           {s_a^2 + \sigma_{t, a}^2 \bar{\rho}_{a} \bar{b}_{t}}} Z_{t, a}
  %       \right\} \right] \right\}.
  % \end{equation}
\end{lemma}


In the pre-limit problem, state transitions~\eqref{eqn:pre-limit-posterior}
are based on observed aggregate rewards $\sqrt{n} \bar{R}_{t, a}^{n}$. The
$\algofull$ algorithm enjoys several desirable properties. First, by solving
the optimization problem~\eqref{eqn:rho} at every epoch $t$, the policy
remains adaptive without involving heavy computation. Implementing this policy
only requires solving the above optimization problem~\eqref{eqn:rho} at the
currently observed state $(\mu_{n, t}, \sigma_{n, t})$; we use stochastic
gradient methods for this purpose. Second, the objective~\eqref{eqn:rho}
encourages the policy to explore more when the remaining horizon is long,
which leads it to explore aggressively early on in the experiment while
focusing on strong alternatives as the experiment winds down (see Figure~\ref{fig:rho_explore}).

Finally, since $\algo$ iteratively computes at each epoch the optimal
allocation among those that assume future allocations will only use currently
available information, it is guaranteed to outperform any open-loop allocation
including static designs. In particular, $\algo$ is guaranteed to achieve a
smaller Bayes simple regret than the uniform allocation across any time
horizon. This gives a practical performance guarantee even for small $T$, 
the regime which is relevant for most real-world experimental settings,
for which the uniform allocation is a highly competitive policy.
\begin{proposition}
  \label{prop:rho-vs-static}
  Let $\rho_t$ be given by Algorithm~\ref{alg:rho}.  For any policy
  $\bar{\pi}_{t:T} = (\bar{\pi}_t(\mu_t, \sigma_t), \ldots,
  \bar{\pi}_{T-1}(\mu_t, \sigma_t))$ that only depends on currently available
  information $(\mu_t,\sigma_t)$, we have
  $V_{t}^{\rho}(\mu_t,\sigma_t) \geq V_{t}^{\bar{\pi}}(\mu_t,\sigma_t)$.
\end{proposition}
\noindent See Section~\ref{section:proof-rho-vs-static} for the proof.

\begin{figure}[t]
  \vspace{-.4cm}
  \centering
  \vspace{-.4cm}
  %\hspace{-1cm}
  %\includegraphics[height=9cm]{./fig/prob_comparison.png}
  \includegraphics[height=3.8cm]{./fig/prob_comparison_alt.png}


  \vspace{.2cm}
  \caption{\label{fig:rho_explore} Comparison of sampling allocations.  
  (a) displays posterior means $\mu$ and standard deviations
    $\sigma$ (the length of each whisker) for $K = 5$ arms. The other graphs
    illustrate the sampling allocation computed by different policies given
    posterior belief $(\mu, \sigma)$. (b) shows the sampling
    allocation produced by the Gaussian Thompson sampling policy, (c)
    is $\algo$ when there is only 1 epoch left, and (d) is $\algo$ when
    the residual horizon is $T - t = 10$.  $\algo$ calibrates exploration to
    the length of the remaining horizon; when there is only 1 epoch left, the
    policy gives the Bayes optimal allocation focusing solely on the top two
    arms. When there are many reallocation epochs left, the policy explores
    other arms more and resembles the Thompson sampling allocation.  }
\end{figure}

Empirically, we demonstrate in Section~\ref{section:experiments} that this
iterative planning method (Algorithm~\ref{alg:rho}) provides a highly
effective heuristic. Motivated by its practical performance, we analyze the
theoretical properties of Algorithm~\ref{alg:rho} in
Section~\ref{section:asymptotics}.  We show that the optimization
problem~\eqref{eqn:rho} becomes strongly concave for a large enough residual
horizon $T-t$.
By explicitly using the KKT conditions of the optimization
problem~\eqref{eqn:rho}, we also characterize the asymptotic sampling policy
$\rho_t$ converges to as $T-t$ grows large. This allows us show that the
open-loop planning problem~\eqref{eqn:rho} assuming future policies only
depend on current information still achieves good performance even when the
horizons $T-t$ is long.

Instead of computing $\rho_t(\mu_t, \sigma_t)$ online as the experiment
progresses, we can pre-compute the $\rho_t(\cdot, \cdot)$ so that it can be
readily applied for any observed current state. Our next result reformulates
the optimization problem~\eqref{eqn:rho} as a stochastic optimization problem
over functions of $(\mu_t, \sigma_t)$.
\begin{proposition}
  \label{prop:loss-min}
  The solution to the $\algofull$ problem~\eqref{eqn:rho} coincides with the
  maximizer of the following reward maximization problem over measurable
  functions $\rho(\mu_t, \sigma_t) \in \Delta^{\numarm}$
  \begin{equation}
    \label{eqn:loss-min}
    \maximize_{\rho(\cdot)~\mbox{\scriptsize measurable}}
    ~\E_{(\mu_t, \sigma_t) \sim P_t} \left[ \max_{a} \left\{ \mu_{t,a}
        + \sqrt{\frac{\sigma_{t, a}^4 \rho(\mu_t, \sigma_t) \bar{b}_t}
        {s_a^2 + \sigma_{t, a}^2 \rho(\mu_t, \sigma_t) \bar{b}_t}} Z_{t, a}
      \right\} \right],
  \end{equation}
  where $P_t$ is any probability measure with support
  $\R^\numarm \times (0, \sigma_0^2]^\numarm$ and
  $\bar{b}_t \defeq \sum_{v=t}^{T-1} b_v$.
\end{proposition}
\noindent See Section~\ref{section:proof-loss-min} for the proof.
Proposition~\ref{prop:loss-min} allows us to use standard ML best practices to
(approximately) solve the reward maximization problem~\eqref{eqn:rho}.
By parameterizing the policy using black-box ML models,
one can use stochastic gradient-based optimization algorithms to learn the mapping $\rho_t(\cdot,\cdot)$ in a
fully offline manner.
In particular, the model
training problem~\eqref{eqn:loss-min} can be approximated by neural networks
and optimized through standard auto-differentiation
frameworks~\cite{TensorFlow15, PyTorch19} for computing stochastic
(sub)gradients. 




% \begin{algorithm}[t]
%   \caption{\label{alg:distilled-rho} Distilled $\algofull$}
%   \begin{algorithmic}[]
%     \State \textsc{Input: Policy parameterization
%       $\{\rho_{\theta,t}(\mu_t, \sigma_t): \theta \in \Theta_t\}$, State distribution $P_{t}$} 
%     \For{each epoch $t = 0, \ldots, T-1$}
%     \State Use stochastic gradient-based optimization methods to optimize
%     \begin{equation}
%       \label{eqn:loss-min}
%       \maximize_{\theta \in \Theta_t}
%       ~\E_{(\mu_t, \sigma_t) \sim P_t} \left[ \max_{a} \left\{ \mu_{t,a}
%           + \sqrt{\frac{\sigma_{t, a}^4 \rho_{\theta,t}(\mu_t, \sigma_t) \bar{b}_t}
%           {s_a^2 + \sigma_{t, a}^2 \rho_{\theta,t}(\mu_t, \sigma_t) \bar{b}_t}} Z_{t, a}
%         \right\} \right].
%     \end{equation}
%     \EndFor
%   \end{algorithmic}
% \end{algorithm}

% \paragraph{Distilled Residual Horizon Optimization} Instead of computing
% $\rho_t(\mu_t, \sigma_t)$ as the experiment progresses, it can sometimes be
% convenient to pre-compute the mapping $\rho_t(\cdot, \cdot)$ so that it can be
% readily applied for any observed current state.  By reformulating the
% optimization problem~\eqref{eqn:rho} as a stochastic optimization problem over
% \emph{functions of $(\mu_t, \sigma_t)$}, we propose a \emph{distilled} variant
% of the $\algo$ policy that can learn the mapping $\rho_t(\cdot,\cdot)$ in a
% fully offline manner.
% \begin{proposition}
%   \label{prop:loss-min}
%   The solution to the $\algofull$ problem~\eqref{eqn:rho} coincides with the
%   maximizer of the following reward maximization problem over measurable
%   functions $\rho(\mu_t, \sigma_t) \in \Delta^{\numarm}$
%   \begin{equation}
%     \label{eqn:loss-min}
%     \maximize_{\rho(\cdot)~\mbox{\scriptsize measurable}}
%     ~\E_{(\mu_t, \sigma_t) \sim P_t} \left[ \max_{a} \left\{ \mu_{t,a}
%         + \sqrt{\frac{\sigma_{t, a}^4 \rho(\mu_t, \sigma_t) \bar{b}_t}
%         {s_a^2 + \sigma_{t, a}^2 \rho(\mu_t, \sigma_t) \bar{b}_t}} Z_{t, a}
%       \right\} \right],
%   \end{equation}
%   where $P_t$ is any probability measure with support
%   $\R^\numarm \times (0, \sigma_0^2]^\numarm$ and
%   $\bar{b}_t \defeq \sum_{v=t}^{T-1} b_v$.
% \end{proposition}
% \noindent See Section~\ref{section:proof-loss-min} for the proof. 


% Proposition~\ref{prop:loss-min} allows us to use standard ML best practices to
% (approximately) solve the reward maximization problem~\eqref{eqn:rho}. We
% parameterize the policy using black-box ML models
% \begin{equation*}
%   \left\{\rho_{\theta,t}(\mu_t, \sigma_t): \theta \in \Theta_t\right\},
% \end{equation*}
% and use stochastic gradient-based optimization algorithms to solve for the
% optimal parameterization (Algorithm~\ref{alg:rho}). In particular, the model
% training problem~\eqref{eqn:loss-min} can be approximated by neural networks
% and optimized through standard auto-differentiation
% frameworks~\cite{TensorFlow15, PyTorch19} for computing stochastic
% (sub)gradients. 
% By the envelope theorem, the stochastic (sub)gradient at the
% sample $(\mu_t, \sigma_t) \sim P_t$ is given by
% \begin{equation*}
%   Z_{t, a\opt} ~\partial_{\theta} \left\{
%     \sqrt{\frac{\sigma_{t, a\opt}^4  \rho_{\theta,t} (\mu_t, \sigma_t) \bar{b}_t}
%     {s_{a\opt}^2 + \sigma_{t, a\opt}^2 \rho_{\theta,t}(\mu_t, \sigma_t) \bar{b}_t}}
%   \right\},
% \end{equation*}
% where
% $a\opt \in \argmax_{a} \left\{ \mu_{t,a} + \sqrt{\frac{\sigma_{t, a}^4
%       \rho_{\theta,t}(\mu_t, \sigma_t) \bar{b}_t} {s_a^2 + \sigma_{t, a}^2
%       \rho_{\theta,t}(\mu_t, \sigma_t) \bar{b}_t}} Z_{t, a} \right\}$. 


% We provide concrete implementation details for Algorithm~\ref{alg:rho} in
% Section~\ref{section:implementation}. \hn{@Ethan: Fill in details in this
%   section. Discuss model architecture, training method etc.}



% (Proposition \ref{prop:dts}, Section \ref{section:proof-dts})



% We again solve for the $Q$-myopic policy through stochastic approximation
% methods.  
% \begin{equation}
% \label{eqn:q-myopic}
% Q_{t}^{\bar{\pi}}(\mu_{t},\sigma_{t})
% = \E_{t}^{\bar{\pi}} [\max_a \mu_{T,a} ]
% = \E_t^{\pi} \left[ \max_{a} \left\{ \mu_{t,a}
%     + \frac{\sigma_{t, a}^4 \bar{\pi}_{t, a} B_t}{s_a^2 + \sigma_{t, a}^2 \bar{\pi}_{t, a} B_t} Z_t
%   \right\} \right]
% \end{equation}
% where $B_t = \sum_{v=t}^{T-1} b_v$.


\paragraph{Pathwise Policy Gradient} We also consider reinforcement
learning/approximate DP methods for solving the full horizon
problem~\eqref{eqn:bayesian-dp}. We apply policy gradients (PG) computed through
black-box ML models, as well as their limited lookahead
variants~\cite{SuttonBa18}. We consider an differentiable parameterized
policy $\pi_\theta = \{\pi_{t}^{\theta} \}_{t=0}^{T-1}$ (e.g., neural
networks) and aim to directly optimize the value function~\eqref{eqn:q_fn} using
stochastic approximation methods. We use stochastic gradient ascent over
sample paths $Z_0, \ldots, Z_{T-1}$ to update policy parameters $\theta$:
\[
  \theta \leftarrow \theta + \alpha \nabla_{\theta} V_{0}^{\pi_{\theta}}(\mu_{0},\sigma_{0}),
\]
for step-size $\alpha$ and prior $(\mu_{0}, \sigma_{0})$.
Note that the gradients of the value function can be computed \emph{exactly}
through auto-differentiation along every sample path, which is enabled
by the reparameterization trick for Gaussian random variables \cite{KingmaWe14}. 
This is in contrast with standard RL methods 
for continuous state and action spaces,
such as Deep Deterministic Policy Gradient (DDPG) \cite{LillicrapEtAl16},
that require function approximation for the Q-functions, a step
which can introduce significant approximation errors.

For long horizons, training the policy becomes more difficult due to vanishing
gradients. Policies are evaluated solely on the simple regret incurred at the
end of the experiment, which makes it difficult to do credit assignment for
sampling allocations near the beginning of the experiment.  We hence consider
$m$-lookahead policies (denoted PG-$m$) trained to optimize the shorter time
horizon objective $V_{T-m}^{\pi_\theta}(\mu_{0},\sigma_{0})$. In the next
section, we empirically find that policy gradient achieves very strong
performance, equivalent to that of $\algo$, for small problems involving a
limited number of treatments and reallocation epochs. Yet training the policy
becomes more difficult in larger-scale experiments, and performance degrades.
We expect though that policy gradient methods will improve in more complex
settings, i.e. those involving utility functions that depend on performance
during the experiment or settings that allow adaptive batch sizes.  In this
sense, $\algo$ strikes a good balance between incorporating planning
capabilities of the Gaussian sequential experiment while maintaining low
operational and optimization complexity.


% even carefully tuned variants of these RL approaches
% frequently perform worse than $\algo$ consistently across problem instances.



% We rewrite the asymptotic sequential experiment as a Markov decision process
% to find the optimal adaptive design. Our formulation is based on
% reparameterizing the distribution of sequential observations
% $G_0, \ldots, G_{T-1}$ using posterior mean and variances defined over
% standard normal variables.  Formally, consider an independent Gaussian prior
% over the local parameters, $h_a \sim N(\mu_{0, a},\sigma_{0, a}^{2})$, under
% which the trajectory of posterior beliefs $(\mu_{t, a},\sigma_{t, a}^{2})$
% follows a Markov Decision Process.  Using standard normal variables
% $Z_{t, a} \simiid N(0, 1)$, the ``states'' $\mu_t,\sigma_t^{2}$ and
% ``actions'' $\pi_t \in\Delta_{k}$ characterize the asymptotic sequential
% experiment through the following transitions
% \begin{equation}
%   \label{eqn:dynamics}
%   \mu_{t+1, a} 
%   =\mu_{t, a}+\sigma_{t, a}\sqrt{\frac{b_t \pi_{t, a}\sigma_{t, a}^{2}}{s_a^{2}+b_t\pi_{t, a}\sigma_{t, a}^{2}}}Z_{t, a}
%   ~~~\mbox{and}~~~
%   \sigma_{t+1, a}^{2}
%   =\left(\frac{1}{\sigma_{t, a}^{2}}+\frac{b_t \pi_{t, a}}{s_a^{2}}\right)^{-1}.
% \end{equation}




% Other well-established approaches for adaptive experimentation are Bayesian
% optimization (BO) and Bayesian optimal experimental design (BOED). Relevant to
% our work, there have been many recent works in the Bayesian optimization
% literature on batch and non-myopic adaptive designs in order to exploit
% parallelism and alleviate the myopia of standard acquisition functions.  Our
% setting departs from this literature in that we focus on the multi-armed
% bandit setting with a finite (although possibly large) number of arms. The
% core challenge in our setting is less so the size of the design space and
% rather how to allocate sampling effort under limited statistical power,
% limited extrapolation between treatments, and a finite exploration horizon.



% The dynamic programming (DP) policy $\pi^*$ solves for $\pi^{*}_{t}=\argmax_{\pi} Q_{t}^{\pi}(\mu_{t},\sigma_{t})$ at every time $t$. The $m$-step lookahead policy $\pi^m$ instead solves for $\pi^{*}_{T-m}(\mu_{t},\sigma_{t})$ at every $t$. Our method approximates these policies with parameterized policies $\pi_\theta = \{\pi_{t}^{\theta_t}\}_{t=1}^{T-1}$ (e.g. neural networks).
% We train the policies to maximize sample average approximations of the Q functions. Emulating backward induction, we begin from $t = T-1$ and train policy $\pi_{t}^{\theta_t}$ using stochastic gradient ascent, fixing parameters $\theta_s$ for all $s > t$ corresponding to shallower lookahead policies. Explicitly, at each $t$ we perform the update:

% \begin{equation}
% \label{eqn:update}
% \theta_t \leftarrow \theta_t + \alpha \nabla_{\theta_t}\E[Q_{t}^{\pi_\theta}(\mu_{t},\sigma_{t}) ]
% \end{equation}

% Since the dynamics of the MDP are known explicitly, we can compute these gradients through standard auto-differentiation frameworks. After training these policies, the approximate DP policy $\pi_\theta^*$ selects allocation $\pi_{t}^{\theta_t}(\mu_t,\sigma_t)$ at every $t$ and the approximate $n$-step lookahead $\pi_\theta^m$ selects $\pi_{T-m}^{\theta_{T-m}}(\mu_t,\sigma_t)$.


% Our algorithm is based on a Monte Carlo approximation of the
% $Q$-function and is tailored to allow efficient computation of gradients
% through standard auto-differentiation frameworks. We also propose myopic
% policies with lookahead horizons and empirically validate their performance.
% As we detail below, our asymptotic normal approximation can provide effective
% adaptive designs even when $n$ is small.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
