\section{Large Horizon Analysis}
\label{section:asymptotics}


Given the strong empirical performance of $\algofull$, we take a closer look
at the behavior of the planning problem~\eqref{eqn:rho} as the residual
horizon $T - t$ grows large, and derive asymptotic expressions for the
gradient and curvature of the objective function.  Although practical
instances typically involve only a handful of reallocation epochs, the large
horizon regime is of interest from an approximate dynamic programming (ADP)
perspective. Since it does not plan for future feedback, we expect the
open-loop approximation~\eqref{eqn:rho} to diverge from the true value
function when the remaining horizon is large. By quantifying the behavior of
$\algo$ in the (somewhat adversarial) large horizon limit, we show how it
calibrates exploration in an intuitive manner despite the open-loop planning.


Recall the $\algo$ planning problem~\eqref{eqn:rho}:
\[
  \maximize_{\bar{\rho} \in \Delta_\numarm} \left\{
    V^{\bar{\rho}}_{t}(\mu_{t},\sigma_{t}) = \E_{t} \left[ \max_{a} \left\{
        \mu_{t,a} + \sqrt{\frac{\sigma_{t, a}^4 \bar{\rho}_{a} \bar{b}_{t}}
          {s_a^2 + \sigma_{t, a}^2 \bar{\rho}_{a} \bar{b}_{t}}} Z_{t, a}
      \right\} \right] \right\}.
\]
The main results in this section show that when the sampling budget is large,
the above planning problem gives rise to a posterior sampling procedure.  This
illustrate a novel connection between posterior sampling and DP-based
approaches to sampling: as $T - t \to \infty$, the solution to the planning
problem converges to a limiting sampling allocation which we denote as Density
Thompson Sampling (DTS), as it samples arms proportionally to the partial
derivatives of the Gaussian Thompson Sampling policy. DTS is highly exploring
and does not over-sample the best arm, which makes it better suited for
best-arm identification tasks than TS.


% In particular, we show the gradient and the Hessian vanish at rate
% $s_{a}^{2}/\bar{b}_{t}$, the effective sample variance, as the residual
% sampling budget $\bar{b}_{t}$ grows large. Our theoretical result explains the
% empirical observation in Section~\ref{section:experiments} that differences in
% the raw regret incurred by adaptive policies are small when the measurement
% variance is small. We find the limiting policy is penalized less for diverting
% sampling effort away from the best arms and is able to explore more, and the
% gradient of the limiting objective shows DTS discourages sampling allocations
% that under-sample arms which are close to the best arm.


% We summarize the main observations as follows:
% \begin{itemize}
% \item In general, the gradient of the objective penalizes sampling allocations
% that under-sample arms which are close to the best arm.
% \item As the residual sampling budget $\bar{b}_{t}$ grows large,
% the gradient and the Hessian vanish at rate $s_{a}^{2}/\bar{b}_{t}$, the effective sample variance. 
% This means that when the residual sampling budget is large,
% the policy is penalized less for diverting sampling effort away from the best arms
% and is able to explore more.
% \item This result coincides with the empirical observation in Section~\ref{section:experiments}
% that differences in the raw regret incurred by adaptive policies are small
% when the measurement variance is small. 
% \item As $\bar{b}_{t} \to \infty$, the Hessian of the objective
% converges to a negative-definite diagonal matrix, and the problem
% becomes strongly concave. This means that it is easier to optimize
% the planning problem for longer horizons, which is usually not the case
% for DP-based methods.
% \item As $T - t \to \infty$, the solution to the planning problem 
% converges to a limiting sampling allocation which we denote as Density Thompson Sampling (DTS),
% as it samples arms proportionally to the partial derivatives of the Gaussian Thompson Sampling policy.
% This policy is highly exploring and does not over-sample the best arm, which makes it better suited 
% for best-arm identification tasks than TS.
% \item This illustrates a novel connection between posterior sampling
% and DP-based approaches to sampling and and shows that even for long horizons, the planning problem leads to 
% a sensible policy despite the open-loop approximation.
% \end{itemize}

% \hn{Instead of motivating this from Frazier Powell and saying we do this, first give the overall motivation and significance of the results to come. Why is this analytical regime meaningful? What does it give you? Then, to the extent it helps the overall story (if at all), mention and connect to prior work.}
% \hn{Explain and acknowledge the gap between the theoretical setting studied here and the practical instances we motivate our problem with. Discuss intractability, and why the large horizon analysis is still meaningful (``adversarial'' regime)}

%In section~\ref{section:algorithms},\hn{capitalize section} we showed that
%the general open-loop planning problem is equivalent to
%a constant allocation planning problem. The constant allocation problem
%can be seen as a modified
%single-batch allocation problem, whose theoretical properties
%were studied in~\citet{FrazierPo10}.
%While they highlight the non-concavity of the single-batch problem,
%we instead analyze the behavior of the problem as the residual horizon goes to infinity,
%in order to study the behavior
%of the planning problem in an adaptive experimental setting. 
%We extend this analysis
%by characterizing the limiting sampling allocation as the residual
%time horizon goes to infinity, which provides a novel connection
%with the Gaussian Thompson sampling policy.

First, we characterize the asymptotic behavior of the gradient and the Hessian
of the planning objective with respect to the sampling allocation.  As the
residual horizon increases, the gradient and Hessian vanish at the rate of
$s_{a}^{2}/\bar{b}_{t}$, which means that when the residual sampling budget is
large the policy is punished less for diverting sampling effort away from the
best arms and explores more as a result. Our characterization shows that the
gradient of the limiting objective encourages the policy to sample arms which
are close to the optimal arm.
\begin{proposition}
  \label{prop:grad-hess}
  Consider any fixed epoch $t$ and state $(\mu, \sigma)$.
  Let $\theta_{a} := \mu_a + \sigma_{a}Z_{a}$ be the random variable
  representing the posterior belief of the average reward for arm $a$,
  and let $\theta_{a}^{*} := \max_{a' \neq a} \theta_{a'}$.
  Suppose that as $T-t$ grows large, 
  the residual sample budget $\bar{b}_{t} = \sum_{s=t}^{T-1} b_{s}\to \infty$.
  Then gradient and Hessian of the planning objective $V^{\bar{\rho}}_{t}(\mu,\sigma)$
  with respect to the allocation $\bar{\rho}$ converge as follows:
    \begin{align*}
    \lim_{T \to \infty} \bar{b}_{t} \nabla V^{\bar{\rho}}_{t}(\mu,\sigma)
     &= \textup{diag} \left( \frac{s_{a}^{2}}{2\bar{\rho}_{a}^{2}}
     \E \left[\frac{1}{\sigma_{a}} \phi \left( \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right] \right) \\
     \lim_{T \to \infty} \bar{b}_{t} \nabla^{2} V^{\bar{\rho}}_{t}(\mu,\sigma)
     &= - \textup{diag} \left(\frac{s_{a}^{2}}{2\bar{\rho}_{a}^{3}}
     \E \left[\frac{1}{\sigma_{a}} \phi \left( \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right] \right)
    \end{align*}
  where $\phi(\cdot)$ is the standard normal density.
\end{proposition}
\noindent See Section~\ref{section:proof-grad-hess} for the proof. 

The $(s_{a}^{2}/\bar{b}_{t})$-scaling explains the empirical observation in
Section~\ref{section:experiments} that differences in the raw regret incurred
by policies are small when the measurement variance $s_a^2$ is small. It also
lends support to the empirical observation that policy gradient suffers from
vanishing gradients for long horizons and small measurement noise.  The product of
$\E \left[\frac{1}{\sigma_{a}} \phi \left(
    \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right]$ and
$1/\bar{\rho}_{a}^2$ is large for arms which are under-sampled and are close
to the best arm under the posterior distribution.  In addition, this
dependence of the gradient on $1/\bar{\rho}_{a}^{2}$ means that in general the
gradient blows up for small sampling probabilities, which can pose a challenge
when there are many treatment arms.  This explains why approximate
second-order methods such as Adam \cite{KingmaBa15} or L-BFGS \cite{LiuNo89}
are empirically more effective than vanilla SGD at optimizing the planning
problem as they precondition the gradient with second-order information that
also scales inversely proportional to $\bar{\rho}_{a}$, which removes this
instability.  Proposition~\ref{prop:grad-hess} also suggests that adding an
artificial scaling term to the remaining sampling budget, i.e., replacing
$\bar{b}_{t}$ with $\alpha \bar{b}_{t}$ for $\alpha > 1$, can enhance
optimization performance of gradient methods; empirically, this improves the
quality of solutions when there are many arms.


Proposition~\ref{prop:grad-hess} implies that the Hessian of the $\algo$
objective converges to a negative-definite diagonal matrix. As the problem
becomes strongly concave, it is easier to optimize the planning problem for
longer horizons. This observation was originally made by~\citet{FrazierPo10} in
the context of a single-batch problem.
\begin{corollary}
  \label{cor:strong-concavity}
  Let $\Delta_K^\epsilon = \Delta_K \cap \{p: p \ge \epsilon\}$ be the
  truncated simplex. For a fixed epoch $t$ and state $(\mu, \sigma)$, there
  exists $T_0(\epsilon) > 0$ such that $\forall T - t > T_0$,
  $\bar{\rho} \mapsto \bar{b}_{t}V^{\bar{\rho}}_{t}(\mu,\sigma)$ is strongly
  concave on $\Delta_K^\epsilon$.
\end{corollary}
\noindent See Section~\ref{section:proof-strong-concavity} for the proof.
Corollary~\ref{cor:strong-concavity} suggests that gradient-based methods will
quickly converge to the solution to the problem.  Unlike most approximate
dynamic programming methods, not only does the complexity of computing the
$\algo$ planning problem~\eqref{eqn:rho} remain the same as the single-batch
or myopic policy as the residual horizon increases, it actually becomes more
amenable for optimization under longer horizons.

Finally, we can characterize the solution of the planning problem
as the residual horizon grows large $T-t \to \infty$. The
key observation is that the expectation that appears in the gradient
in Proposition~\ref{prop:grad-hess} can be 
written as the partial derivative of the Thompson sampling probabilities
$\pi_{a}^{\text{TS}}(\mu, \sigma):= \P(a^{*} = a|\mu, \sigma)$ with respect to the means
\[
  \E \left[\frac{1}{\sigma_{a}} \phi \left(
      \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right]
  =\frac{\partial}{\partial \mu_a} \left(1 -\Phi\left(\frac{\theta^{*}_{a}-
      \mu_{a}}{\sigma_{a}}\right)\right) = \frac{\partial}{\partial \mu_a}
  \pi_{a}^{\text{TS}}(\mu, \sigma).
\]
We denote the limiting policy as
Density Thompson Sampling (DTS) or $\pi^\text{DTS}$.
\begin{proposition}
  \label{prop:dts}
  Consider a fixed epoch $t$ and state $(\mu, \sigma)$. Suppose there exists
  $T_1$ such that for $\forall T - t > T_1$, the $\algo$ allocation satisfies
  $\rho_{t,a}(\mu,\sigma) > \epsilon$. Then as $T-t \to \infty$,
  \begin{equation*}
    \rho_{t,a}(\mu,\sigma) \to \pi_{a}^{\textup{DTS}}(\mu, \sigma),~\mbox{where}~
    \pi^{\rm DTS}_a(\mu, \sigma) \propto s_a \left[ \frac{\partial}{\partial
        \mu_a}\pi_{a}^{\textup{TS}}(\mu,\sigma) \right]^{1/2}
  \end{equation*}
  %if $\pi^{\rm DTS}_a (\mu_t,\sigma_t) > \epsilon$.
\end{proposition}
\noindent See Section~\ref{section:proof-dts} for the proof.  The hypothesis
is necessary as we prove convergence of the solutions through the KKT
conditions.

% Proposition~\ref{prop:dts} provides a novel connection between
% optimization-based approaches for sampling such as expected
% improvement~\cite{Mockus05}, often used in Bayesian Optimization, and
% posterior sampling methods such as Thompson sampling~\cite{RussoVaKaOsWe18}.
% The Density Thompson Sampling policy is similar to Thompson sampling in that
% the sampling allocation is determined by sampling from the posterior, while
% ignoring the time horizon and measurement variance.  The partial derivatives
% of Thompson sampling probabilities is appropriate for best-arm identification
% tasks: they capture the marginal probability arm $a$ will be the arm selected
% at the end of the experiment (i.e. the highest posterior mean) after updating
% with a large number of samples.  Specifically, the DTS allocations
% $\phi(\frac{\theta^{*}_{a}- \mu_{a}}{\sigma_{a}})$ are symmetric with respect
% to the normalized `gap' $\frac{\theta^{*}_{a}- \mu_{a}}{\sigma_{a}}$ between
% the arm and the best arm, involving the \emph{magnitude} of the gaps rather
% than the direction.  As a result, unlike TS which asymptotically assigns
% almost all sampling effort to the best arm, DTS samples the best arm based on
% the gap with the next best arm and does not oversample.  In this sense, DTS
% can be viewed as a variant of posterior sampling that is better suited for
% pure exploration.


Similar to TS, DTS samples from the posterior distribution; while TS
asymptotically assigns all sampling effort to the best arm, DTS does not
oversample the best arm, as it is based on the gap between the second best
arm. Concretely, the relative sampling proportions
$\pi^{\text{DTS}}_{a}/\pi^{\text{DTS}}_{a'}$ can be expressed as ratios of the
`index'
$\E \left[\frac{1}{\sigma_{a}} \phi \left(
    \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right]^{1/2}$; the
higher the index, the more sampling effort allocated to that arm during the
epoch.  This index decreases at an exponential rate for \emph{all} arms as the
experiment progresses and the posterior uncertainty shrinks
$\sigma_{a} \to 0$.  
\begin{proposition}
  \label{prop:log-dts}
  Consider a fixed state $(\mu, \sigma)$ and suppose arms are sorted in decreasing order of their means $\mu_{1} > \mu_{2} > \ldots > \mu_{K}$.
  Then we have that:
  \[
    \begin{cases}
      -\frac{(\mu_{1}-\mu_{2})^{2}}{2(\sigma_{1}^{2}+\sigma_{2}^{2})}
      & ~~\mbox{top two arms}~~a\in{1,2} \\
      -\frac{(\mu_{1}-\mu_{a})^{2}}{2\sigma_{a}^{2}}
      & ~~\mbox{otherwise} \\
    \end{cases}
  \lesssim \log \E \left[\frac{1}{\sigma_{a}} \phi \left( \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right]
  \lesssim -\min_{a'\neq a}\frac{(\mu_{a'}-\mu_{a})^{2}}{2(\sigma_{a}^{2}+\sigma_{a'}^{2})}
  \]
  where $\lesssim$ contains additive terms that are logarithmic in $\min_{a} \sigma_{a}$.
  % For the remaining arms:
  % \[
  % -\frac{(\mu_{1}-\mu_{a})^{2}}{2(\sigma_{a}^{2})}
  % \leq \log \E \left[\frac{1}{\sigma_{a}} \phi \left( \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right]
  % \leq -\min_{a'\neq a}-\frac{(\mu_{a'}-\mu_{a})^{2}}{2(\sigma_{a}^{2}+\sigma_{a'}^{2})}
  % \]
\end{proposition}
\noindent See Section~\ref{section:proof-log-dts} for a proof.  If an arm is
excessively under-sampled, its index will eventually be larger than others and
so sampling effort will be spread out across arms rather than concentrating on
the best one.  This makes DTS better suited for best-arm identification and
closer to Top-Two Thompson Sampling~\citep{Russo20}.

% \begin{lemma}
% \[
%   \begin{aligned}
%   \E \left[\frac{1}{\sigma_{a}} \phi \left( \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right] 
%   &= \frac{1}{\sigma_{a}}\E \left[Z_{a}1\{\theta_{a} > \max_{a'\neq a}\theta_{a} \}\right] \\
%   &\leq \frac{1}{\sigma_{a}}\E \left[Z_{a} \sum_{a' \neq a} 1\{\theta_{a} > \theta_{a'} \}\right] \\
%   &\leq \frac{1}{\sigma_{a}}\E (\numarm - 1)\max_{a'\neq a} \left[Z_{a} 1\{\theta_{a} > \theta_{a'} \}\right] \\
%   &= \frac{1}{\sigma_{a}}\E (\numarm - 1)\max_{a'\neq a} \left[\phi \left( \frac{\theta_{a'}^{*}-\mu_{a}}{\sigma_{a}} \right) \right] \\
%   &= \E (\numarm - 1)\max_{a'\neq a} e^{-\frac{(\mu_{a'} - \mu_{a})^{2}}{2(\sigma_{a'}^{2} + \sigma_{a}^{2})}} \\
%   \end{aligned}
% \]
% \end{lemma}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
