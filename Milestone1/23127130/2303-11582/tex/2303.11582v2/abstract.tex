% In typical experimentation paradigms, continual reallocation of measurement
% effort is expensive due to infrastructural and organizational difficulties,
% and is sometimes infeasible due to delayed feedback. Challenges in
% reallocation lead practitioners to employ a handful of reallocation epochs in
% which outcomes are measured in large batches. However, standard adaptive
% methods are specifically designed to do well when the number of reallocation
% epochs grows large. We develop a new adaptive experimentation framework that
% can flexibly handle any batch size and learns near-optimal designs when
% reallocation opportunities are few. By deriving an asymptotic sequential
% experiment based on normal approximations, we formulate a Bayesian dynamic
% program that can leverage prior information based on previous experiments. We
% propose an iterative method, $\algofull$, and demonstrate that despite relying
% on approximations it significantly improves statistical power over standard
% adaptive policies. Overall, our work shows normal approximations that are
% universal in statistical inference can also serve as powerful tools for
% adaptive experimental design.


Standard bandit algorithms that assume continual reallocation of measurement
effort are challenging to implement due to delayed feedback and
infrastructural/organizational difficulties. Motivated by practical instances
involving a handful of reallocation epochs in which outcomes are measured in
batches, we develop a new adaptive experimentation framework that can flexibly
handle any batch size.  Our main observation is that normal approximations,
which are universal in statistical inference, can also guide the design of
scalable adaptive designs. By deriving an asymptotic sequential experiment, we
formulate a dynamic program that can leverage prior information on
\emph{average rewards}. We propose a simple iterative planning method,
$\algofull$, which selects sampling allocations by optimizing a planning
objective with stochastic gradient descent.  Our method significantly improves
statistical power over standard methods, even when compared to Bayesian bandit
algorithms (e.g., Thompson sampling) that require full distributional
knowledge of \emph{individual rewards}.  Overall, we expand the scope of
adaptive experimentation to settings that are difficult for standard methods,
involving a small number of reallocation epochs, low signal-to-noise ratio,
and unknown reward distributions.


%%% TeX-master: "algorithms"
%%% Local Variables: %%% mode: latex %%% TeX-master: "main" %%% End:
