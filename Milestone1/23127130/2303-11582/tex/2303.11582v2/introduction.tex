\section{Introduction}
\label{section:introduction}

Experimentation is the basis of scientific decision-making for medical
treatments, engineering solutions, policy-making, and business products
alike. Since experimenting is typically expensive or risky (e.g., clinical
trials), the cost of collecting data poses a central operational
constraint. Simultaneously, as policy interventions and engineering solutions
become more sophisticated, modern experiments increasingly involve many
treatment arms. When the differences in average rewards across arms (average
treatment effects) are small relative to the sample size, statistical power is
of fundamental concern~\citep{KohaviLoSoHe09, ButtonIoMoNoFlRoMu13,
  CziborJiLi19}. Even for online platforms that can automatically deploy
experiments to millions to billions of users, typical A/B tests are
underpowered as they involve incremental changes to a product that have a
small relative impact on key business metrics such as revenue or user
satisfaction~\cite{KohaviLoSoHe09, KohaviDeFrLoWaXu12,
  KohaviDeFrWaXuPo13}. When there is interference across individuals,
treatments may be randomized over entire markets or geographic regions
severely limiting statistical power~\cite{ImbensRu15}.


Adaptive allocation of measurement effort can significantly improve
statistical power and allow reliable identification of the optimal
decision/treatment.  Accordingly, adaptive methods---dubbed pure-exploration
multi-armed bandit (MAB) algorithms---have received tremendous attention since
the foundational works of Thompson, Chernoff, Robbins, and
Lai~\cite{Thompson33, Chernoff59, Robbins52, LaiRo85}. Most of these
algorithms are specifically designed to enjoy strong theoretical guarantees as
the number of reallocation epochs grows to infinity~\cite{BubeckCe12,
  LattimoreSz19, Slivkins19}. However, standard frameworks cannot model
typical experimentation paradigms where adaptive reallocation incurs high
operational cost. Although a universal assumption in the MAB literature,
unit-level continual reallocation of sampling effort is often expensive or
infeasible due to organizational cost and delayed feedback. Even in online
platforms with advanced experimentation infrastructures, engineering
difficulties and lack of organizational incentives deter continual
reallocation at the unit level~\cite{SculleyEtAl15, AgarwalEtAl16,
  BakshyEtAl18, NamkoongDaBa20}.

Due to the challenges associated with reallocating measurement effort, typical
real-world adaptive experiments employ a few reallocation epochs in which
outcomes are measured for many units in parallel (``batches'')
\cite{BakshyEtAl18, OfferWestortCoGr20, JobjornssonScMuFr22, ChowCh08,
  KasySa21, EspositoSa22}.
% ~\hn{Cite
% real-world experimentation papers here: classical online ads papers by
% Langford et al., Kasy's recent applied papers, Molly Offer-Westort, applied
% papers from Eytan's team, Dan's TS tutorial also has some references that
% may be potentially relevant}. 
Motivated by these operational considerations, we develop and analyze batched
adaptive experimentation methods tailored to a handful of reallocation
opportunities. Our main conceptual contribution is the formulation of a
dynamic program that allows designing adaptive methods that are near-optimal
for the fixed number of reallocation epochs. Algorithms designed from our
framework can flexibly handle any batch size, and are automatically tailored
to the instance-specific measurement noise and statistical
power. Specifically, we use a normal approximation for aggregate rewards to
formulate a \emph{Gaussian sequential experiment} where each experiment epoch
consists of draws from a Gaussian distribution for each arm. The dynamic
program solves for the best adaptive allocation where noisy Gaussian draws
become more accurate with sampling allocation (see
Figure~\ref{fig:arm-diagram}).

\begin{figure}[t]

  \vspace{-0.5cm}
  \centering
  \includegraphics[height=6cm]{./fig/arm_diagram.png}%
  \vspace{0.4cm}
  \caption{\label{fig:arm-diagram} Gaussian sequential experiment with $T=3$
    reallocation epochs. Bar visualizes sampling allocations at each epoch
    and bell curves depict normal approximations of the aggregated reward
    distribution.
    %\hn{Let's delete the final $\cdots$ to indicate a short horizon problem}
    }
    % \vspace{-1cm}
\end{figure}

Although we use (frequentist) central limit-based normal approximations to
derive the Gaussian sequential experiment, our proposed dynamic programming
framework solves for adaptive allocations using Bayesian approaches. Unlike
standard Bayesian bandit formulations (e.g.,~\cite{FrazierPoDa08, KasySa21,
  Russo20}) that require distributional knowledge of individual rewards, we
only use a prior over the \emph{average rewards} and our likelihood functions
over average rewards are Gaussian from the CLT.  Our formulation thus allows
leveraging prior information on average rewards constructed using the rich
reservoir of previous experiments, but retains some of the advantages of
model-free frequentist methods. Computationally, sampling allocations derived
from our Gaussian sequential experiment can be computed offline after each
epoch, in contrast to typical Bayesian bandit algorithms that require
real-time posterior inference (e.g., top-two Thompson
sampling~\cite{Russo20}).  In our formulation, standard Monte Carlo
approximations of value functions are \emph{differentiable} with respect to
the sampling allocations, allowing the application of stochastic
gradient-based methods. While standard bandit formulations rely on theoretical
insights due to their complexity, our formulation provides a computational
framework for algorithmic development, leading to policies that are effective
even for small time horizons.

Since the dynamic program for the optimal adaptive allocation is intractable,
we propose an iterative planning method, $\algofull$, which solves for the
optimal allocation among those that only use currently available observations.
The allocation is used in the corresponding period and newly collected
observations are used to update the planning problem in the next period. Our
method is optimal among those that do not optimize for potential future
feedback and in particular, is always better than the uniform
allocation. (This algorithmic design principle is often referred to as the
open-loop feedback control~\cite[Chapter 6]{Bertsekas12a}.)  Through extensive
empirical validation, we demonstrate in Section~\ref{section:experiments} that
$\algo$ consistently provides major gains in decision-making performance over
a rich set of benchmark methods.  We present a summary of our main empirical
findings in Figure~\ref{fig:bern-bsr-batch}, which compares the performance of
$\algo$ against standard batch policies such as oracle Thompson sampling-based
policies that have full distributional knowledge of individual rewards.
% we provide a complete empirical validation of our proposed algorithm
% against a wide range of different adaptive sampling methods.
% Across different problem scenarios, we find that
% $\algo$ outperforms even specialized algorithms that require complete
% knowledge of the reward distribution such as top-two Thompson
% sampling~\cite{Russo20}.  
As we discuss later, we find our approach achieves particularly large gains on
harder/underpowered problems with high effective measurement noise.  Overall,
our proposed methods expand the scope of adaptive experimentation to settings
which are difficult for standard adaptive policies, including problems with a
low signal-to-noise ratio, small number of reallocation epochs relative to
sample size, and unknown reward distributions.

\begin{figure}[t]

  \vspace{-1cm}
  
  \centering
  \hspace{-.9cm}
 \subfloat[\centering  Batch size $= 100$ samples.]{\label{fig:bern_bsr_100}{\includegraphics[height=5.7cm]{./fig/reg_Bernoulli_100_100_1_Flat.png}} }
 \hspace{.3cm}
 \subfloat[\centering Batch size $= 10,000$ samples.]{\label{fig:bern_bsr_10000}{\includegraphics[height=5.7cm]{./fig/reg_Bernoulli_10000_100_1_Flat.png} }}%
 \vspace{.4cm}

  \caption{\label{fig:bern-bsr-batch} Relative gains over the uniform
    allocation as measured by the Bayes simple regret for the finite batch
    problem with $K = 100$ treatment arms.  Individual rewards are
    Bernoulli with a $\mbox{Beta}$ prior. Despite relying on normal
    approximation of aggregate rewards over a batch, $\algo$ delivers
    substantial performance gains even over oracle algorithms that know
    the true reward model.  These gains persist even for small batch sizes
    in Figure~\ref{fig:bern_bsr_100} where batch size is equal to the
    number of arms.}  
\end{figure}

We begin our discussion by showing that the Gaussian sequential experiment as
a valid approximation as the batch size becomes large
(Section~\ref{section:formulation}). We study the admissible regime where
differences in average rewards scale as $1/\sqrt{n}$, where $n$ is the batch
size at each epoch.  This asymptotic regime has long been used to formalize
statistical power in inferential scenarios~\cite{VanDerVaart98, LeCamYa00}. By
extending this conceptual framework to sequential experiments, we show that
normal approximations universal in statistical inference is also useful for
designing adaptive experimentation methods.

In Section~\ref{section:algorithms}, we observe that the Gaussian sequential
experiment can be represented by a Markov decision process over prior means
and variances on average rewards.  At every epoch, state transitions in the
MDP are governed by posterior updates based on new observations. Our MDP
formulation allows us to develop our iterative planning algorithm,
$\algofull$, which solve an open-loop problem that optimize future sampling
allocations that only depend on currently available information.  Our simple
iterative planning approach almost always outperforms carefully tuned
reinforcement learning algorithms for (approximately) solving the MDP.
Although our exposition focuses on best-arm identification, our MDP framework
and iterative planning policy can accommodate a myriad of alternative
objectives and constraints. This flexibility is in stark contrast to standard
adaptive policies that require ad-hoc adjustments tailored for each setting.

The strong empirical performance of $\algo$
(Section~\ref{section:experiments}) impels us to analyze its theoretical
properties (Section~\ref{section:asymptotics}). Even in the asymptotic
Gaussian sequential experiment, the performance of $\algo$ is hard to quantify
when epochs are few, a well-recognized difficulty in the
literature~\cite{FrazierPo10}.  Instead, we focus on a tractable yet
adversarial regime for our policy: since $\algo$ does not optimize over
possible future observations, their suboptimality will grow with the number of
available reallocation epochs (horizon $T$). We characterizes the behavior of
$\algo$ in the large horizon limit, and provide an understanding of how it
calibrates exploration when the remaining sampling budget is large.

% We characterize the frequentist suboptimality gap of $\algo$ in the limit
% $T \to \infty$, when observations are governed by a single unknown
% distribution.  Although our primary interests lie with small horizon
% problems in practice, our theoretical results provide assurance that $\algo$
% has bounded suboptimality even when one can adapt the allocation infinitely
% many times.

% Case study paragraph Finally, we demonstrate the potential practical impact
% of our method through a case study with real data
% (Section~\ref{section:case}). Since machine learning models require large
% supervised datasets, industrial models are largely trained to predict
% abundant yet frivolous short-term outcomes such as user clicks. To assess
% the operational impact of a new ML model, online platforms run experiments
% to measure whether the model improves key business metrics like long-term
% customer satisfaction, e.g., a platform may assess many configurations of a
% ML-driven recommendation system. To illustrate practical experimentation
% scenarios, we study an advertising auction example based on a dataset from
% Criteo~\cite{DiemertMeynetGaLe17} to demonstrate that our adaptive
% experimentation algorithms can guide sampling effort to efficiently identify
% the optimal ML-driven bidding strategy.

% In the rest of the paper, we first derive our asymptotic formulation and
% algorithms in Sections~\ref{section:formulation}
% and~\ref{section:algorithms}. Then, we present empirical and theoretical
% analysis of our main proposed adaptive algorithm, $\algo$, in
% Sections~\ref{section:experiments} and~\ref{section:asymptotics}.
%, alongside a case study in Section~\ref{section:case}. 

We defer a full discussion of the literature to the end of the paper to
maximize clarity.  Our asymptotic formulation and resulting algorithms are
new, but intimately connected to the vast body of work on adaptive
allocation of measurement effort that spans multiple fields. In
Section~\ref{section:discussion}, we situate the current paper against
previous works on pure-exploration problems, batched bandits, ranking and
selection in simulation optimization, diffusion limits for MABs, as well as
various works studying Gaussian environments related to the one we outline in
Figure~\ref{fig:arm-diagram}.



% although any single product improvement may yield small relative increase in
% revenue, absolute gains are nevertheless substantial. The relative
% performance differential between arms can thus be difficult to discern even
% with a large sampling budget, particularly when the population is stratified
% into groups~\cite{WernerfeltTuShMo22}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
