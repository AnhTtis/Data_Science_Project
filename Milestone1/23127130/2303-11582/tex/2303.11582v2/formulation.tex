\section{Gaussian Sequential Experiment}
\label{section:formulation}

% Discuss outline for the section. Talk about why it's hard to formulate the
% prelimit problem. Batch size is often difficult to control.


Our goal is to select the best treatment arm out of $\numarm$ alternatives,
using a small (known) number of reallocation epochs ($\horizon$).  Each
experimentation epoch $t=0,...,\horizon - 1$ involves a batch of $b_{t}n$
samples, where $b_{t}>0$ is a fixed and known constant and $n$ is a scaling
parameter. The constants $b_t$ can be thought of as the length of time between
experiment epochs, which may vary across epochs $t$. Solving for the optimal
adaptive allocation is challenging as it involves a dynamic program over a
combinatorially large action space that depends on unknown reward
distributions. Any solution to such ``finite batch'' problem will depend
heavily on the batch size and need to be re-solved if the batch size changes,
that is if more/less samples are collected than expected.


To circumvent such complexity, we formulate an asymptotic approximation of the
adaptive experimentation problem using normal approximation of aggregate
rewards. The \emph{sequential Gaussian experiment} we derive in this section
provides tractable policies that can deal with any \emph{flexible batch
  size}. To derive this CLT-based large batch limit, we must first choose a
natural scaling for the average rewards. If the gap in the average reward of
each arm is $\gg1/\sqrt{n}$, adaptive experimentation is unnecessary as the
best arm can be found after only a single epoch. Conversely, if the gaps are
$\ll1/\sqrt{n}$, then we cannot reliably learn even after many experimentation
epochs; one cannot improve upon the uniform allocation (a.k.a. randomized
design or A/B testing). We thus focus on the admissible regime where the gap
between average rewards are $\Theta(1/\sqrt{n})$.

Using this scaling for average rewards, we observe i.i.d. rewards for each
unit allocated to a treatment arm $a$
\begin{equation}
  \label{eqn:rewards}
  R_a = \frac{h_a}{\sqrt{n}} + \epsilon_a,
\end{equation}
where $h_a$ is an unknown ``local parameter'' that determines the difference
in average rewards across arms. Without loss of generality, we set the
baseline reward to zero; since we assume the noise $\epsilon_a$ has mean zero,
henceforth we abuse notation and refer to $h$ as both the average rewards and
the gaps in average rewards.  We assume that the variance of the noise
$\text{Var}(\epsilon_{a})=s_a^{2}$ is known and constant. In particular,
$s_a^2$ does not scale with $n$, so it is crucial to sample arms many times to
discern differences between their means $h_a/\sqrt{n}$. Although reward
variances are typically unknown, they can be estimated from a small initial
batch in practice; empirically, the policies we consider are robust to
estimation error in $s_a^2$ and a rough estimate suffices.
%\hn{@Ethan: Point  to a plot on this when you add it.}

\subsection{Gaussian sequential experiment as a limit adaptive experiment}
\label{section:formulation-gse}

Our goal is to use the information collected until the beginning of epoch $t$
to select an adaptive allocation (policy) $\pi_{t} \in\Delta_{\numarm}$, the
fraction of samples allocated to each of the $\numarm$ treatment arms. Let
$\{R_{a, j}^t\}_{a=1}^\numarm$ denote the \emph{potential rewards} for unit
$j \in [b_t n]$ at time $t$.  We use $\xi_{a, j}^t$ to denote an indicator for
whether arm $a$ was pulled for unit $j$ at time $t$.  Our starting point is
the observation that the (scaled) estimator for average rewards converges in
distribution from the central limit theorem
\begin{equation}
  \label{eqn:limit}
  \sqrt{n} \bar{R}_{t, a}^{n} \defeq
%\frac{1}{b_{t}\sqrt{n}} \sum_{j=1}^{b_{t}n}\frac{\xi_{a,j}^{t}}{\pi_{t, a}} R^{t}_{a,j}
  \frac{1}{b_{t}\sqrt{n}} \sum_{j=1}^{b_{t}n}\xi_{a,j}^{t} R^{t}_{a,j}
  \cd N\left(\pi_{t,a}h_{a},\pi_{t,a}s_a^{2}\right).
\end{equation}
From the normal approximation~\eqref{eqn:limit},
$\sqrt{n} \bar{R}_{t, a}^n / \pi_{t, a}$ can be seen as an approximate draw
from the distribution $N(h_a, s_a^2/\pi_{t, a})$, giving a noisy observation
of the average reward $h_a$. The allocation $\pi_{t}$ controls the effective
sample size and the ability to distinguish signal from noise,
a.k.a. statistical power.


Using successive normal approximations~\eqref{eqn:limit} at each epoch, we
arrive at a \emph{Gaussian sequential experiment} that provides an asymptotic
approximation to the adaptive experimentation problem.
\begin{definition}
  \label{def:gse}
  A Gaussian sequential experiment is characterized by observations
  $G_0, \ldots, G_{T-1}$ with conditional distributions
  \begin{equation*}
    G_t | G_0, \ldots, G_{t-1} \sim
    N\left(\left\{\pi_{t,a}h_{a}\right\}_{a=1}^\numarm,
    \diag\left(\left\{\frac{\pi_{t, a}s_a^{2}}{b_t }\right\}_{a=1}^\numarm\right)\right).
  \end{equation*}
\end{definition}
\noindent In this asymptotic experiment, the experimenter chooses $\pi_t$ at
each epoch $t$ and observes an independent Gaussian measurement distributed as
$N\left(\pi_{t,a}h_a,\pi_{t, a} s_a^{2} \right)$ for each arm $a$.  
%$N\left(h_a,\frac{s_a^{2}}{b_t \pi_{t, a}}\right)$ for each arm $a$.  
We use the asymptotic Gaussian sequential experiment as an approximation to the
original batched adaptive epochs and derive near-optimal adaptive
experimentation methods for this asymptotic problem.


% assumptions, point to proof
Building on the observation~\eqref{eqn:limit}, our first theoretical result
shows that the Gaussian sequential experiment (Definition~\ref{def:gse})
provides an accurate approximation in the large batch limit $n \to \infty$.
Our limit sequential experiment extends classical local asymptotic normality
results in statistics~\cite{VanDerVaart98, LeCamYa00}. Our result relies on
the following basic conditions on the reward and policy $\pi_t$'s.
\begin{assumption}
  \label{assumption:reward}
\begin{enumerate}


\item  \label{item:ignorability}
  \emph{Ignorability}: rewards are independent from sampling decisions,
  conditional on past observations: \ifdefined\msom
$\xi^{t+1}_j \perp R^{t+1}_j \mid \bar{R}^n_0, \ldots, \bar{R}^n_t.$
\else
\begin{equation*}
  \xi^{t+1}_j \perp R^{t+1}_j \mid \bar{R}^n_0, \ldots, \bar{R}^n_t.
\end{equation*}
\fi

\item \label{item:moment} \emph{Moment restriction}: there exists $C>0$
  such that $\E\norm{\epsilon}_{2}^{4}\leq C$.
  \end{enumerate}
\end{assumption}
\noindent For the sequential Gaussian experiment to provide a valid
approximation of the objective~\eqref{eqn:bsr}, we further need regularity conditions
on the policy.
\begin{assumption}
  \label{assumption:policy}
  \begin{enumerate}
  \item \label{item:sufficiency} \emph{Sufficiency}: the policy depends on the
    data only through aggregate rewards: \ifdefined\msom
$\pi_{t}=\pi_{t}\left(\sqrt{n} \bar{R}^{n}_{0},\ldots,\sqrt{n}
\bar{R}^{n}_{t-1}\right)$ for all $t= 0, \cdots, T-1$.
\else
\begin{equation*}
  \pi_{t}=\pi_{t}\left(\sqrt{n} \bar{R}^{n}_{0},\ldots,\sqrt{n}
  \bar{R}^{n}_{t-1}\right)~~~\mbox{for all}~t= 0, \cdots, T-1
  \end{equation*}
\fi
  \item \label{item:continuity} \emph{Continuity}: discontinuity points of
    $\pi_{t}$ are measure zero with respect to $(G_{0},...,G_{t-1})$ for all
    $t$
  \end{enumerate}
\end{assumption}

% \begin{assumption}
%   \label{assumption:policy}
%   \begin{enumerate}
%   \item \label{item:sufficiency}
%     \emph{Sufficiency}: the policy $\pi$ depends on
%     the data only through the importance sampling estimator, i.e.,
%     $\pi_{t}=\pi_{t}(\sqrt{n} \bar{R}^{n}_{0},\ldots,\sqrt{n}
%     \bar{R}^{n}_{t-1})$ for all $t$.
%   \item  \label{item:overlap}
%     \emph{Overlap}: for the same $\delta$ in
%     Assumption~\ref{assumption:reward}\ref{item:moment}, for all $t$ there exists $C_{t}>0$
%     such that
%     \begin{equation*}
%       \sup_{n} \E \left[ \frac{1}{\min_{a} \pi_{a,t}(\sqrt{n}
%           \bar{R}^{n}_{0},\ldots,\sqrt{n} \bar{R}^{n}_{t-1})^{3.5+\delta}}
%       \right] \leq C_{t}.
%     \end{equation*}
%   \item \label{item:continuity} \emph{Continuity}: for all $t$, the
%     discontinuity points of $\pi_{t}$ are of measure zero with respect to
%     $(G_{0},...,G_{t-1})$.
%   \end{enumerate}
% \end{assumption}
\noindent Assumption~\ref{assumption:policy}\ref{item:sufficiency} is
unavoidable in our setup as we restrict attention to coarser information
represented by the aggregate rewards~\eqref{eqn:limit}.  Note however, that we
do not require sampling probabilities to be bounded from below, as is commonly
assumed for inferential procedures.
% Assumption~\ref{assumption:policy}\ref{item:overlap}
% requires the policies to sample every arm frequently enough, so that the
% inverse probability weighting estimator has sufficient moments for the central
% limit theorem~\eqref{eqn:limit} to be valid.

We are now ready to give our main asymptotic result, which we prove in
Section~\ref{section:proof-limit}. The key difficulty of this result is
to show convergence of the batch sample means to their Gaussian limit under
sampling probabilities that are themselves stochastic, 
as they are selected by the policy which is influenced by previous measurements.
The sampling probabilities are arbitrary and are allowed to even be zero, so we 
require showing uniform convergence across all possible sampling probabilities.
\begin{theorem}
  \label{theorem:limit}
  Let Assumptions~\ref{assumption:reward},~\ref{assumption:policy} hold. Then,
  the Gaussian sequential experiment in Definition~\ref{def:gse} provides a
  valid asymptotic approximation as $n \to \infty$
  \begin{equation}
    \label{eqn:weak-convergence}
    (\sqrt{n} \bar{R}^{n}_{0},\ldots,\sqrt{n} \bar{R}^{n}_{T-1}) \cd
    (G_{0},...,G_{T-1}).
  \end{equation}
\end{theorem}

\ifdefined\msom
\else
\noindent Our proof also provides a bound for the rate of convergence in
Theorem~\ref{theorem:limit}.  To metrize weak convergence, consider the
bounded Lipschitz distance $d_{\text{BL}}$
\begin{equation*}
  d_{\text{BL}}(\mu, \nu) \defeq \sup\left\{
    |\mathbb{E}_{R \sim \mu}\bar{f}(R)-\mathbb{E}_{R \sim \nu}\bar{f}(R)|:
    \bar{f}\in\text{Lip}(\mathbb{R}^{\numarm}),
    \sup_{x,y\in\mathbb{R}^{\numarm}}|\bar{f}(x)-\bar{f}(y)|\leq1\right\}.
\end{equation*}
Since we measure convergence through bounded Lipschitz test functions, we require smoothness of
the policy with respect to observations
\begin{equation}
  \label{eqn:lip-policy}
  \bar{L} := \max_{0\leq t \leq T -1} \max_{a} \max \{ \norm{\pi_{t,a}}_{\textup{Lip}}, \norm{\sqrt{\pi_{t,a}}}_{\textup{Lip}} \}<\infty.
  % ~~\mbox{and}~~ \bar{L} := \max_{0\leq t \leq T - 1} \max_{a} \norm{\sqrt{\pi_{t,a}}}_{\textup{Lip}} < \infty.
\end{equation}
\vspace{-.4cm}
\begin{corollary}
  \label{cor:proof-limit-rate}
  Let Assumptions~\ref{assumption:reward},~\ref{assumption:policy} and the bound~\eqref{eqn:lip-policy} hold.
  Let $M := \left(1 + \bar{L} ( s_*^2/b_* \E \ltwo{Z} + \max_{a} |h_{a}|) \right)$ with
  $s_{*}^{2} := \max_{a} s_{a}^{2}$, $b_{*} := \min_{0\leq t\leq T-1} b_{t}$.
 Then,
 \begin{equation}
   \label{eqn:bl-rate}
   d_{\textup{BL}}((\sqrt{n} \bar{R}^{n}_{0},\ldots,\sqrt{n} \bar{R}^{n}_{T-1}), (G_{0},\ldots,G_{T-1})) 
   \leq C \frac{M^{T} - 1}{M - 1} n^{-1/6}
  \end{equation}
  where $C\in (0,\infty)$ is a constant that depends polynomially 
  on $K$, $h$, $s_{*}^{2}$, $\E \norm{\epsilon}_{2}^{3}$, $\E \norm{\epsilon}_{2}^{4}$
  and $b_{*}^{-1}$.
\end{corollary}
Unlike other asymptotic normality results for adaptive sampling policies, we
do not assume anything beyond continuity of the sampling probabilities.  As a
result, we obtain a slower rate of convergence than the standard $n^{-1/2}$
rate expected for the CLT (e.g., Berry-Esseen bound) as the batch size grows
large; a minor modification to our proof gives the usual $O(n^{-1/2})$ rate if
the sampling probabilities are bounded from below. 
%\hn{Discuss in 1-2 sentences why we get this rate; smoothing multivariate Stein's method.}
We obtain an $O(n^{-1/6})$ rate through a multivariate Stein's method; since 
we do not assume a lower bound on sampling probabilities the corresponding Stein's operator
has weaker smoothing properties than in standard cases
and we require an additional Gaussian smoothing argument
to apply the result to Lipschitz test functions.
The bound~\eqref{eqn:bl-rate} suffers an exponential dependence on the time
horizon $T$, which is perhaps unsurprising as the result involves \emph{joint}
weak convergence of correlated random variables.  The smoothness of the policy
also enters, which suggests that convergence may be slower for policies that
are very sensitive to changes in the measurement.  Nevertheless, $T$ is very
small compared to the batch size for the settings we consider. Empirically, we
observe below that the asymptotic limit offer an accurate approximation even
when the batch sizes are small.

% \subsection{Bayesian formulation}
% \label{section:formulation-bayes}

% It is natural for the experimenter to have a prior distribution $h \sim \nu$
% over the relative gap between average rewards. For example, modern online
% platforms run thousands of experiments from which a rich reservoir of
% previously collected data is available~\cite{KohaviLoSoHe09,
%   KohaviDeFrLoWaXu12, KohaviDeFrWaXuPo13}. In this work, we focus on
% minimizing the \emph{Bayes simple regret} at the end of the experiment
% \begin{equation}
%   \label{eqn:bsr}
%   \bsr_{T}(\pi,\nu,\bar{R})
%   \defeq \E_{h \sim \nu} \E[h_{a\opt} - h_{\what{a}}]
%   ~~~\mbox{where}~~\what{a} \sim \pi_T~\mbox{and}~a\opt \in \argmax_a h_a,
% \end{equation}
% the scaled optimality gap between the final selection $\what{a}$ and the
% optimal arm $a\opt$ averaged over the prior.  The notation
% $\bsr_{T}(\pi,\nu,\bar{R})$ considers adaptive policies
% $\pi = \{\pi_t\}_{t=0}^T$, prior $\nu$ over $h$, and observation process
% $\bar{R} = (\bar{R}_0,\ldots, \bar{R}_{T-1} )$, which is the set of aggregate
% rewards used by the policy to determine the sampling allocations.

% Instead of optimizing the Bayes simple regret for each finite batch size, we
% use Theorem~\ref{theorem:limit} to derive an asymptotic approximation under
% the Gaussian sequential experiment.  
% \begin{corollary}
%   \label{cor:bsr-limit}
%   Let $\nu$ be a prior over average rewards $h$ satisfying
%   $\E_{h \sim \nu} \lone{h} < \infty$ and let
%   Assumptions~\ref{assumption:reward},~\ref{assumption:policy} hold. Then, the
%   Bayes simple regret under $\bar{R}^n$ can be approximated by that under the
%   Gaussian sequential experiment $V$ in Definition~\ref{def:gse}:
%   \ifdefined\msom
%   $\bsr_{T}(\pi,\nu, \sqrt{n} \bar{R}^{n}) \to \bsr_{T}(\pi,\nu, V)$.
%   \else
%   \begin{equation}
%     \label{eqn:bsr-limit}
%     \bsr_{T}(\pi,\nu, \sqrt{n} \bar{R}^{n}) \to \bsr_{T}(\pi,\nu, V).
%   \end{equation}
%   \fi
% \end{corollary}
% \noindent See Section~\ref{section:proof-bsr-limit}
% for a proof of the corollary.

% Using the approximation of Bayes simple regret, we optimize the asymptotic
% Bayesian objective
% \begin{equation}
%   \label{eqn:gse-bsr}
%   \minimize_{\pi}~~\left\{
%     \bsr_{T}(\pi,\nu, G) = \E_{h \sim \nu} \E[h_{a\opt} - h_{\what{a}}]
%     \right\}
% \end{equation}
% over policies $\pi = \{\pi_t(G_0, \ldots, G_{t-1})\}_{t=0}^T$ adapted to the
% sequential observations $G_0, \ldots, G_{T-1}$ of the Gaussian sequential
% experiment. Policies derived from the optimization problem~\eqref{eqn:gse-bsr}
% has marked modeling flexibility and computational advantages compared to
% typical Bayesian sequential sampling algorithms (e.g., variants of Thompson
% sampling~\cite{Russo20}).  Although reward distributions are unknown in
% general, Bayesian sequential sampling approaches require a distributional
% model of individual rewards comprising of prior and likelihood
% function~\cite{RussoVaKaOsWe18}.  In contrast, our framework does not assume
% restrictive distributional assumptions on rewards and allows naturally
% incorporating prior information over average rewards $h_a$. In our approach,
% the likelihood function of the aggregate rewards
% $\sqrt{n} \bar{R}_{t, a}^{n} \mid h$ is derived from the normal
% approximation~\eqref{eqn:weak-convergence}. As a result, our limiting Gaussian
% sequential experiment coincides with typical frequentist inferential paradigms
% for calculating confidence intervals and statistical power.

% Computationally, policies derived from our asymptotic optimization
% problem~\eqref{eqn:gse-bsr} can be efficiently updated offline. As these
% offline updates give a fixed sampling probability over the $\numarm$ arms at
% each epoch, the policies we propose in subsequent sections can be deployed to
% millions of units at ease. This is in stark contrast to Bayesian sequential
% sampling algorithms designed for unit-level feedback. While their policy
% updates are also often performed offline in batches in
% practice~\cite{OfferWestortCoGr20, EspositoSa22}, when deployed these policies
% require \emph{real-time} posterior inference and action optimization to
% generate treatment assignments for each unit. Implementing such methods at
% scale is highly challenging even for the largest online platforms with mature
% engineering infrastructure~\cite{AgarwalBiCoHoLaLeLiMeOsRi16,
%   NamkoongDaBa20}. Methods derived from our formulation provides a scalable
% alternative as deploying the resulting adaptive policies only involves
% sampling from a fixed sampling probability $\pi_t$ for every unit in a batch,
% \emph{regardless} of the batch size.

\begin{figure}[t]
  \vspace{-.4cm}
  \centering
  \vspace{-.4cm}
  \hspace{-.9cm}

  \ifdefined\msom
  \subfloat[\centering Simple regret for Gumbel rewards (Gamma prior)]{\label{fig:scaling_gumbel}{\includegraphics[height=5cm]{./fig/scaling_gumbel_ts.jpg}} }%
  \subfloat[\centering Simple regret for Bernoulli rewards (Beta
  prior)]{\label{fig:scaling_bern}{\includegraphics[height=5cm]{./fig/scaling_bern_ts.jpg}
    }}%
  \else
  \subfloat[\centering Simple regret for Gumbel rewards (Gamma prior)]{\label{fig:scaling_gumbel}{\includegraphics[height=5.7cm]{./fig/scaling_gumbel_ts.jpg}} }%
  \subfloat[\centering Simple regret for Bernoulli rewards (Beta
  prior)]{\label{fig:scaling_bern}{\includegraphics[height=5.7cm]{./fig/scaling_bern_ts.jpg}
    }}%
  \fi


  \vspace{.2cm}
  \caption{\label{fig:scaling-bsr} Histograms of simple regret after $T=10$
    epochs involving $\numarm = 100$ arms, across instances $h$ drawn from the
    prior $\nu$. Each histogram corresponds to a different batch size, with
    the green histogram corresponding to the Gaussian sequential experiment
    ($n=\infty$).  Even when the batch size is small ($b_t n = 100$), the
    performance of the policy in these non-Gaussian environments closely
    matches the performance of the policy in the Gaussian sequential
    experiment. 
    %\hn{@Ethan: Can you make the legend and x- and y-labels a lot bigger? They're hard to read as-is.}
      }
\end{figure}

\subsection{Validity of normal approximations even for small batch sizes}
\label{section:formulation-validity}

Empirically, we observe that the Gaussian sequential experiment is a good
approximation even when the batch sizes are exceedingly small. While
Theorem~\ref{theorem:limit} and Corollary~\ref{cor:bsr-limit} only guarantees
validity of our approximation in the large batch limit, this observation
impels us to apply policies derived from the problem~\eqref{eqn:gse-bsr} to
problems with small batches.  Overall, our observations are consistent with
the longstanding folklore in statistics that the central limit theorem often
provides a practical approximation even for small samples.

We illustrate that the sequential Gaussian experiment provides an accurate
prediction of performance in finite batch systems. Our primary metric of performance
is simple regret, the difference in the average reward between the best arm
and the arm selected by the policy at the end of experiment (according to a policy denoted as $\pi_{T}$). 
Since modern experimental platforms typically run many experiments, it is natural for the experimenter
to have a prior $\nu$ over the average rewards $h$ of the treatment arms, so we focus on simple regret among
instances drawn from this prior. For different batch
scalings $n$, we compare the \emph{distribution} of the simple regret
\begin{equation}
  \label{eqn:sr}
  \E_{\what{a}} [h_{a\opt} - h_{\what{a}}]~~~\mbox{where}~~
  \what{a} \sim \pi_T\left( \sqrt{n}\bar{R}_0,\ldots, \sqrt{n}\bar{R}_{T-1} \right)
\end{equation}
over the prior $h \sim \nu$, to its limiting object
$\E_{\what{a} \sim \pi_T(G_0, \ldots, G_{T-1})} [h_{a\opt} - h_{\what{a}}]$.

For simplicity, we consider a large batch limit version of the Thompson
sampling policy; we observe similar trends for different policies $\pi$.  As
we detail in Section~\ref{section:algorithms}, this Bayesian policy maintains
a Gaussian belief over the average rewards $h$ and updates posteriors using
observed aggregate rewards $\sqrt{n}\bar{R}^{n}$. Figure~\ref{fig:scaling-bsr}
displays histograms of the simple regret~\eqref{eqn:sr} incurred by this
policy across experiments with $T = 10$ reallocation epochs and
$\numarm = 100$ treatment arms.  Each histogram depicts the distribution of
the simple regret~\eqref{eqn:sr} over the prior $h \sim \nu$ for a particular
batch scaling $n$, including under the Gaussian sequential experiment
corresponding to $n=\infty$. Even for exceptionally small batch sizes relative
to the number of arms (batch size of $b_t n = 100$ across $K=100$ arms), the
Bayes simple regret closely matches that predicted by the Gaussian sequential
experiment.

% This allows us to present a more complete description than simply comparing
% the means, i.e., comparing the Bayes simple regret
% $\bsr_{T}(\pi,\nu, \sqrt{n} \bar{R}^{n})$ to its limiting object
% $\bsr_{T}(\pi,\nu, G)$.

% We use the asymptotic Gaussian sequential experiment as an approximation to
% the original batched adaptive epochs and derive near-optimal adaptive
% experimentation methods for the asymptotic problem. While our framework allows
% naturally incorporating prior information, we do not assume restrictive
% distributional assumptions on rewards that are required in typical Bayesian
% sequential sampling approaches~\cite{RussoVaKaOsWe18}. Instead, the likelihood
% function of the aggregate rewards $\sqrt{n} \bar{R}_{t, a}^{n} \mid h$ is
% derived from the normal approximation~\eqref{eqn:limit}. Our limiting normal
% approximation coincides with typical (frequentist) inferential paradigms for
% confidence intervals and power calculations.


% ~\citet{JiangBINO} studied an open loop approximation for a batched MAB
% setting with a different $Q$-function.
% \ec{We note that while this method uses a similar non-adaptive approximation
% of the $Q$-function as proposed in [BINOCULARS], the $Q$ function itself is entirely
% different and is designed for the batch multi-armed bandit setting.}



% A number of works in Bayesian optimization \cite{GinsbourgerRiCa08, 
% WuFr19, GonzalezOsLa16, JiangChGoGa20, LamWiWo16, JiangJiBaKaGaGa19} 
% and experimental design study batched adaptive designs that alleviate the myopia of standard acquisition
% functions.  In contrast to this literature studying continuous design spaces,
% we focus on allocation of measurement effort over a finite number of arms
% under limited statistical power. Our setting is characterized by limited
% extrapolation between arms and a fixed, finite exploration horizon. As our
% approach maximizes an expected utility function, it is
% intimately connected to Bayesian experimental design methods \cite{ChalonerVe95, RyanDrMcPe16, FosterIvMaRa21}. Instead of
% optimizing expected information gain (EIG), we minimize expected simple regret
% at the terminal period, a more tractable objective.
% While our approach to improving statistical power is new, the corresponding
% Gaussian sequential experiment~\eqref{eqn:dynamics} was previously studied
% in the context of robust control~\cite{MullerVaPrRo17} and attention
% allocation~\cite{LiangMuSy22}, and \citet{FrazierPo10} studied a single
% epoch variant.


% Moreover, the effective measurement variance $s_{t}^{2}$ not only involves
% the base variance of the arm rewards but also the relative scaling of the
% sample size with the gap size -- in essence measuring the statistical power
% within each batch.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
