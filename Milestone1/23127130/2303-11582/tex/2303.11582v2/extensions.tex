\subsection{Extensions}
\label{section:extensions}


In practice, the experimenter often needs to trade multiple objectives and
constraints beyond identifying the treatment with the highest average reward.
For example, the experimenter may want to identify the top two or three
treatments and then later run a static A/B test to obtain statistical
significance.  Alternatively, she may want to minimize the regret incurred
during the experiment and quickly eliminate treatment arms that are
detrimental to sample participants.  In some cases, she may constrain sampling
probabilities to be above a threshold for downstream inferential procedures.
Treatments typically impact multiple rewards/outcomes, in which case there may
no longer be a `best' treatment arm and the experimenter may instead be interested
in identifying treatments that perform well across all dimensions.

Since our framework is optimization-based,
these objectives and constraints can be easily incorporated
into the policy by modifying the planning problem~\eqref{eqn:rho-planning}.
Adding such modifications guarantees that 
performance will always improve over any static design 
across any time horizon as in Proposition~\ref{prop:rho-vs-static}.
One can contrast this with standard adaptive policies such as Thompson Sampling
or Successive Elimination, for which one may have to make
drastic and ad-hoc changes to the policy to accomodate additional features,
without any guarantees of performance.

To illustrate, consider an experimenter 
who wants to minimize a combination of
Bayesian simple regret among the top-5 arms and a negative entropy penalty (to encourage
sampling all arms), while imposing a linear constraint on the sampling probabilities.
For example, in a pricing experiment where each arm corresponds to a different price,
this could be a constraint on the average price shown to users.
Rather than optimizing the value function $\E_{t}[\max_{a}\mu_{T,a}]$,
through similar arguments as in Lemma~\ref{lemma:rho-reduction} one can show that the
$\algo$ problem can be reduced to:
\[
    \maximize_{\bar{\rho} \in \Delta_\numarm}
    \left\{ V^{\bar{\rho}}_{t}(\mu_{t},\sigma_{t})
     = \E_{t}\left[\text{top-}5(\mu_{T}) \right]   + \lambda H(\bar{\rho})
    : r^{\top}\bar{\rho} \leq \bar{r} \right\},
\]
where $r \in \R^{\numarm}$, $\bar{r}\in \R$, $\text{top-}k$ sums the values of the top $k$ entries of an input vector,
$H(\bar{\rho}) = \sum_{a} \bar{\rho}_{a} \log \bar{\rho}_{a}$ is the entropy function,
and $\lambda$ is a parameter calibrating the tradeoff between simple regret and the entropy penalty.
Although $\text{top-}k$ is non-differentiable, there exist reliable differentiable
surrogates~\cite{AmosKoKo19, BlondelTeBeDj20} that allow the use of stochastic gradient methods 
to solve the planning problem. 
The strong empirical performance of our iterative planning approach
suggests that our framework can reliably guide
adaptive experimental design across the varied demands
of modern large-scale experiments.











%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
