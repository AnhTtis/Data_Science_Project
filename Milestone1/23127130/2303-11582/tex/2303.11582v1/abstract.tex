% In typical experimentation paradigms, continual reallocation of measurement
% effort is expensive due to infrastructural and organizational difficulties,
% and is sometimes infeasible due to delayed feedback. Challenges in
% reallocation lead practitioners to employ a handful of reallocation epochs in
% which outcomes are measured in large batches. However, standard adaptive
% methods are specifically designed to do well when the number of reallocation
% epochs grows large. We develop a new adaptive experimentation framework that
% can flexibly handle any batch size and learns near-optimal designs when
% reallocation opportunities are few. By deriving an asymptotic sequential
% experiment based on normal approximations, we formulate a Bayesian dynamic
% program that can leverage prior information based on previous experiments. We
% propose an iterative method, $\algofull$, and demonstrate that despite relying
% on approximations it significantly improves statistical power over standard
% adaptive policies. Overall, our work shows normal approximations that are
% universal in statistical inference can also serve as powerful tools for
% adaptive experimental design.


Standard bandit algorithms that assume continual reallocation of measurement
effort are challenging to implement due to delayed feedback and
infrastructural/organizational difficulties. Motivated by practical instances
involving a handful of reallocation epochs in which outcomes are measured in
batches, we develop a new adaptive experimentation framework that can flexibly
handle any batch size.  Our main observation is that normal approximations
universal in statistical inference can also guide the design of scalable
adaptive designs. By deriving an asymptotic sequential experiment, we
formulate a dynamic program that can leverage prior information on
\emph{average rewards}. State transitions of the dynamic program are \emph{differentiable} with
respect to the sampling allocations, allowing the use
of gradient-based methods for planning and policy optimization.
We propose a simple iterative planning
method, $\algofull$, which selects sampling allocations by optimizing 
a planning objective via stochastic gradient-based methods. 
Our method significantly improves statistical power over
standard adaptive policies, even when compared to Bayesian bandit algorithms
(e.g., Thompson sampling) that require full distributional knowledge of
\emph{individual rewards}.  Overall, we expand the scope of adaptive
experimentation to settings which are difficult for standard adaptive
policies, including problems with a small number of reallocation epochs, low
signal-to-noise ratio, and unknown reward distributions.


%%% Local Variables: %%% mode: latex %%% TeX-master: "main" %%% End:
