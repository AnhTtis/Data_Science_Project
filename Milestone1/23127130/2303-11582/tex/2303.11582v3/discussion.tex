\section{Discussion}
\label{section:discussion}

In this work, we use Gaussian approximations to derive an MDP framework that
describes how posterior beliefs evolve under the experimenter's sampling
policy in batch experiments.  With tools from optimal control and
reinforcement learning, this framework guides the development of policies that
can plan ahead and calibrate exploration to the size of the remaining sampling
budget.

\subsection{Related work}
\label{section:related-work}

This paper is situated in the extensive body of work studying adaptive methods
for instance-dependent pure-exploration problems. Since this literature spans
multiple communities including operations research, statistics, and machine
learning, we provide a necessarily abridged review centered around the
following characteristic aspects of our approach.
\begin{enumerate}[itemsep=-4pt, leftmargin=.7cm]
\item Our proposed adaptive framework focuses on a small number of
  reallocation epochs (horizon $T$). The algorithms we derive optimize
  instance-dependent factors that do not grow with $T$, which are often
  ignored as ``constants'' in the literature.
  \label{item:small-horizon}
\item Using sequential Gaussian approximations, our algorithms can handle
  flexible batch sizes.
  \label{item:batch}
\item The Gaussian sequential experiment we derive is based on the admissible
  regime where the gaps in average rewards scale as $\Theta(1/\sqrt{n})$.
  \label{item:diffusion}
\item When endowed with a Gaussian prior over average rewards, the Gaussian
  sequential experiment gives rise to a Markov decision process with
  fractional measurement allocations. Our MDP is connected to previous works
  in simulation optimization that study problems with Gaussian observations,
  as well as (adaptive) Bayesian experimental design methods.
  \label{item:gaussian}
\end{enumerate}
 

\paragraph{Pure-exploration MABs and ranking \& selection}

The problem of identifying the top-$k$ arms out of a set of alternatives has
been extensively studied in the simulation optimization and multi-armed bandit
literatures. As a detailed review of this vast literature is outside the scope
of the work, we refer the reader
to~\citet{HongNeXu15},~\citet{ChenChLePu15},~\citet[Ch
33.5]{LattimoreSz19},~\citet[Section 1.2]{Russo20}, and references therein for
a complete review. In light of the small-horizon perspective
(Point~\ref{item:small-horizon}), we discuss these works by dividing them into
two broad categories.


The first category studies fully sequential procedures and study the behavior
of the problem when the number of reallocation epochs (horizon $T$) is
expected to be large. Several authors explicitly study the limiting regime
$T \to \infty$~\cite{Chernoff59, Chernoff73, ChenLiYuCh00, GlynnJu04,
  Russo20}, though the multi-armed bandit (MAB) literature primarily focuses
on finite horizon guarantees. These works are often classified into two
regimes: in the fixed-confidence setting \cite{MannorTs04,EvenDarMaMa06,
  KarninKoSo13, JamiesonMaNoBu14, JamiesonNo14, KaufmannKa13, GarivierKa16,
  KaufmannCaGa16}, the objective is to find the best arm with a fixed level of
confidence using the smallest number of samples, where as the fixed budget
setting \cite{BubeckMuSt09, AudibertBuMu10,
  GabillonGhLa12,KarninKoSo13,CarpentierLo16} aims to find the best arm under
a fixed sampling budget.  Our setting can be viewed as a Bayesian fixed-budget
problem where the sampling budget is split into fixed batches. The main
conceptual difference between this paper and the above body of work is that we
explicitly optimize performance for each problem instance and a fixed horizon,
where ``constant terms'' that do not depend on the horizon $T$ play an outsize
role in determining statistical performance.


From an algorithmic viewpoint, the majority of the works in this category
study sequential elimination schemes or modifications of the upper confidence
bound algorithm (UCB). Our Bayesian adaptive algorithms are mostly closely
related to Bayesian algorithms for best-arm
identification~\cite{QinKlRu17,RussoVaKaOsWe18, ShangHeMeKaVa20,
  Russo20, KasySa21}. When viewed as algorithms for the original pre-limit problem,
these algorithms require a full distributional model of individual rewards, in
contrast to our framework that only requires priors over average rewards as we
belabored in Section~\ref{section:algorithms}. Empirically, we observe in
Figure~\ref{fig:bern-bsr-batch} that a simple algorithm derived from the
\emph{limiting Gaussian sequential experiment} significantly outperforms
these oracle Bayesian sampling methods and these gains hold even when the batch size
is small.

Most of the aforementioned works study the probability of correct selection,
with notable recent exceptions that consider the Bayes simple
regret~\cite{KomiyamaArKaQi21, AtsidakouKaSaKv22}. In this paper, we focus on
the Bayes simple regret for ease of exposition, and as we believe this is the
more relevant objective in practice. However, our asymptotic formulation can
incorporate alternative objectives such as the probability of correct
selection, and the adaptive algorithms we develop can be appropriately
modified.

The second category of works develops one-step or two-step heuristics for
selecting the next alternative, such as Knowledge Gradient or expected
opportunity cost~\cite{GuptaMi96, FrazierPoDa08, ChickBrSc10, ChickIn01,
  HeChCh07} or probability of correct selection~\cite{ChickBrSc10}. Of
particular relevance to our work is \citet{FrazierPo10}, who consider the
problem of allocating a \emph{single} batch of samples across several
alternatives with normally distributed rewards to minimize the Bayes simple
regret. Their setting is equivalent to a single-stage version of the Gaussian
sequential experiment we consider. Empirically, we observe in
Section~\ref{section:experiments} that our proposed planning-based policies
can perform much better than these single-batch or one-step lookahead
procedures.


\paragraph{Batching and delayed feedback}

From the perspective of handling flexible batch sizes
(Point~\ref{item:batch}), several authors have adapted bandit algorithms to
problems with batch evaluations and delayed feedback. In the standard
exploration-exploitation MAB setting with fixed batch sizes,
\citet{PerchetRiChSn16, GaoHaReZh19} find that even when the number of
reallocation epochs scales logarithmically or even sub-lograthmically in the
total sample size, one can recover the same rates for cumulative regret as in
the fully-sequential setting.~\citet{EsfandiariKaMeMi21} show these rates can
be made tighter under adaptive batch sizes, and~\citet{KalkanhOz21, KarbasiMiSh21} show that
Thompson sampling with an adaptive batch scheme also achieves rates equivalent
to the fully sequential case up to similar logarithmic terms.  The growing
literature on bandits with delayed feedback can also be seen as a bandit
problem with stochastic batch sizes \cite{JoulaniGySz13, GroverEtAl18,
  Pike-BurkeAgSzGr18, VernadeCaLaZaErBr20}.  
  % \hn{Should we also cite
  % \citet{DesautelsKrBu14, KandasamyKrScPo18} here? Especially since the latter
  % studies TS?}

We focus on pure-exploration when the number of reallocation epochs is small
compared to the total sample size.  In this regard, \citet{JunJaNoZh16} propose
fixed confidence and fixed budget policies that obtain the top-k arms and find
that the batch complexity scales logarithmically in the total sample size.
\citet{AgarwalEtAl17} show that a sequential elimination policy obtains the
top-k arms with fixed confidence with a number of reallocation epochs that
only grows iterated logarithmically in the total sample size. Most of the
policies proposed in the batched bandit literature are either sequential
elimination based policies, or batch variants of Thompson Sampling that know
the true reward distribution. Unlike these works, we study a limiting regime
and propose Bayesian algorithms that use Gaussian approximations for the
likelihoods.  As we further detail in the next discussion point, the policies
we derive for the Gaussian sequential experiment can be directly applied to
the original pre-limit problem, and can thus handle any batch size
flexibly. Empirically, our methods outperform standard batch methods across a
range of settings, as we outline in Figure~\ref{fig:bern-bsr-batch} and
further expand in Section~\ref{section:experiments}.



% \begin{itemize}
% \item Best arm identification in bandits (Bayesian, fixed confidence, fixed budget). Qin and Russo and~\citet{Russo20} have a rather extensive literature review
% \item Batched bandits, e.g.,~\citet{DesautelsKrBu14, KandasamyKrScPo18}
% \item BayesOpt and experimental design
% \item Connections to Heavy traffic and diffusion limits, as well as more recent bandit works (Wager \& Xu, Fan \& Glynn, Kalvit \& Zeevi)
% \end{itemize}

\paragraph{Gaussian approximations and asymptotics}

\citet{BhatFaMoSi20} study optimal experimental design for binary treatments,
under a linear specification for treatments and covariates. Their approach has
philosophical similarities to ours, which optimizes a final stage outcome by
using DP methods to sequentially assign treatments to subjects.  They analyze
an fully online version of the problem where subjects arrive stochastically
and the experimenter sequentially allocates them to the treatment or control
group. They balance covariates in the two groups to minimize the variance of
the OLS estimator at the end of the experiment; when covariates are drawn
i.i.d.  from an elliptical distribution, they show the dimensionality of the
DP collapses. Recently,~\citet{XiongAtBaImb23} consider a similar problem for
panel data, maximizing the precision of a generalized least squares (GLS)
estimator of the treatment effect.
% In our work, the final stage outcome is the simple regret compared to the
% optimal arm rather than the variance of treatment estimates since we consider
% multiple treatments. 
On the other hand, our DP crystallizes how the adaptive experimentation
problem simplifies in the large-batch approximation; our formulation relies on
Bayesian posterior beliefs over \emph{average rewards}, rather than an
estimate of the uncertainty. We consider multiple treatment arms where an
effective adaptive design must sample strategically to reduce uncertainty for
treatments which are likely to be the best one.
% In this two-armed setting, they consider uncertainty estimates for the
% sample variance of the GLS estimator and derive adaptive designs using
% Gaussian approximations.  In
% addition, our work does not feature a panel data setting so there is no
% temporal dimension of the sampling decision.

Our work is also connected to recent works that use Gaussian approximations
for statistical inference on data produced by bandit
policies~\cite{HadadHiZhAt21, LuedtkeVa16, LuedtkeVa18}. Our work is most
related to~\citet{ZhangJaMu20} who derive inferential procedures based on
Gaussian approximations in a batched bandit setting.  They construct a
modified ordinary least squares (OLS) estimator and show asymptotic normality
as the batch size grows large. Although our setting does not feature any
contextual information, we similarly consider a fixed horizon experiment with
batches and our main asymptotic result considers the regime where the batch
size grows large while the horizon remains fixed. In this regard, we show that
asymptotic approximations used for inference are also highly effective in
guiding adaptive experimentation procedures. However, unlike their work, we
are able to derive our result without assuming that the propensity scores are
bounded from below or clipped.

Concurrent and independent to the present paper,~\citet{HiranoPo23} consider
data produced from a parametric model and derive Gaussian approximations for
batched data produced by adaptive sampling policies. Their work extends the
classical framework of local asymptotic normality of parametric
models~\cite{VanDerVaart98} to adaptive data.  On the other hand, we do not
assume a parametric model over the reward distribution and instead derive
Gaussian limits over \emph{average rewards}. The two approaches are
complementary: we focus on deriving a formulation on which modern
computational tools can be used to derive new Bayesian batch adaptive sampling
policies, whereas \citet{HiranoPo23} study power calculations and inferential
questions.

There is a nascent literature on deriving diffusion limits for bandit
policies~\cite{KalvitZe21, WagerXu21, FanGl21, Adusumilli21,
  AramanCa22}. These works typically let the number of reallocation epochs
grow to infinity, while scaling down the gaps in average rewards like in our
setting (Point~\ref{item:diffusion}). \citet{WagerXu21} consider any Markovian
policy, and~\citet{FanGl21} study Thompson sampling with extensions to batched
settings where the batch size is small compared to the number of reallocation
epochs. \citet{KalvitZe21} find a sharp discrepancy between the behavior of
UCB and Thompson Sampling in a related regime. \citet{Adusumilli21} considers
an arbitrary Bayesian policy and derive an HJB equation for the optimal policy
in the limit, which can be solved by PDE methods. \citet{AramanCa22} consider
a sequential testing environment with two hypotheses involving experiments
that arrive according to an exogenous Poisson process.  They derive diffusion
process approximations in the limit as intensity of arrivals grows large and
the informativeness of experiments shrinks.

In contrast, our diffusion limit is motivated by the practical difficulty of
reallocating sampling effort and we consider a \emph{fixed} number of
reallocation epochs. Our formulation is derived by letting the batch size $n$
grow large, while scaling down the difference in average rewards as
$\Theta(1/\sqrt{n})$.  This setting is particularly appropriate for modeling
experimentation on online platforms that deal with many units and
interventions with small treatment effects.  Unlike the above line of work
that study limiting diffusion processes, our limiting process is a discrete
time MDP which allows us to use approximate DP methods to develop new adaptive
experimentation policies.


\paragraph{Gaussian observations and priors}

Several research communities have taken interest in sequential decision-making
problems with Gaussian observations and priors due to its tractability. While
the algorithmic connections are salient as we discuss shortly, these
literatures do not provide formal justifications of normality unlike the
present work, although we have found some nice heuristic discussions (e.g.,
see~\citet[Section 3.1]{KimNe07}).  Another key difference between our
asymptotic framework and others is that we consider normal distributions over
the gaps between \emph{average rewards}, rather than individual rewards
(Point~\ref{item:gaussian}).

Taking a decision-theoretic framework in problems with Gaussian observations,
a number of authors~\cite{GuptaMi96, ChickBrSc10, FrazierPo10} have studied
simple settings that yield an exact solution, e.g., when there is a single
measurement. While the formal settings are different, the spirit of our
algorithmic development is similar to these approaches which heuristically
extend the solution derived in the simple setting to build adaptive policies.
Our use of the Gaussian sequential experiment to improve statistical power
appears new; a similar Gaussian MDP problem has been recently studied in the
context of robust control~\cite{MullerVaPrRo17} and attention
allocation~\cite{LiangMuSy22}.  Most recently, \citet{LiuDeVaXu22} provide
bounds on the cumulative regret of a Bayesian agent that maintains a
misspecified Gaussian posterior state in a (fully sequential) Bernoulli reward
environment.

The Bayesian optimization literature uses Gaussian processes to model the
world. Many authors have proposed acquisition functions for batch
evaluations~\cite{GinsbourgerRiCa08, ShahGh15, WangGeKoJe18, GonzalezDaHeLa16,
  WuFr19} and some derive regret bounds for batched versions UCB and Thompson
sampling in Gaussian process environments\cite{ContalBuRoVa13,
  DesautelsKrBu14, KandasamyKrScPo18}. The primary focus of this literature is
problems with a continuous set of arms. In contrast, we focus on the
allocation of measurement effort over a finite number of arms under limited
statistical power, and our setting is characterized by limited extrapolation
between arms and a fixed, finite exploration horizon. Our approach of using a
MDP for posterior beliefs to design non-myopic adaptive experimentation
policies conceptually builds on works that design non-myopic acquisition
functions for Bayesian optimization~\cite{GonzalezOsLa16, JiangChGoGa20,
  LamWiWo16, JiangJiBaKaGaGa19,AstudilloJiBaBaFr21}.

As our methods maximize an expected utility function, it is conceptually
related to Bayesian experimental design methods~\cite{ChalonerVe95,
  RyanDrMcPe16, FosterIvMaRa21}. Instead of optimizing expected information
gain (EIG), we minimize expected simple regret at the terminal period, a more
tractable objective. In particular, we consider policy gradient based methods
to guide the experimental design similar to the works~\cite{FosterIvMaRa21,
  JorkeLeBr22}. Our work is also similar to~\citep{MinMoRu2020}, who propose a policy gradient algorithm
  for shaping the posterior used by Thompson Sampling as a computational tool to improve its performance. But unlike these works, the methods we consider at the end of
Section~\ref{section:algorithms-mdp} use \emph{pathwise} policy gradients of the
value function enabled by the 
reparameterization trick for Gaussian random vectors~\cite{KingmaWe14}.
This is in contrast with the score function or REINFORCE gradient estimator~\cite{Williams92}
commonly used in reinforcement learning, which is known to have higher variance
and is difficult to apply in continuous state and action settings.
Our method allows us to use standard auto-differentiation frameworks to compute exact 
gradients of the value function without having to
fit the value function or the model separately, as is required for
other RL methods such as Deep Deterministic Policy Gradient~\cite{LillicrapEtAl16}. 
While the reparameterization trick
has been used before to maximize one-step or two-step 
acquisition functions in Bayesian optimization
(e.g. ~\cite{WilsonHuDe18, WuFr19, BalandatEtAl20}), our work differs in that we compute pathwise gradients of entire
sample paths with respect to sampling \emph{allocations},
and use these gradients to optimize sampling policies.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

