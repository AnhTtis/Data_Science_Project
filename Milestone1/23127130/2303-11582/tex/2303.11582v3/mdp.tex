\section{Bayesian Adaptive Experimentation}
\label{section:algorithms}

It is natural for the experimenter to have a prior distribution $h \sim \nu$
over the relative gap between average rewards. For example, modern online
platforms run thousands of experiments from which a rich reservoir of
previously collected data is available~\citep{KohaviLoSoHe09,
  KohaviDeFrLoWaXu12, KohaviDeFrWaXuPo13}. In this work, we focus on
minimizing the \emph{Bayes simple regret} at the end of the experiment
\begin{equation}
  \label{eqn:bsr}
  \bsr_{T}(\pi,\nu,\bar{R})
  \defeq \E_{h \sim \nu} \E[h_{a\opt} - h_{\what{a}}]
  ~~~\mbox{where}~~\what{a} \sim \pi_T~\mbox{and}~a\opt \in \argmax_a h_a,
\end{equation}
the scaled optimality gap between the final selection $\what{a}$ and the
optimal arm $a\opt$ averaged over the prior.  The notation
$\bsr_{T}(\pi,\nu,\bar{R})$ considers adaptive policies
$\pi = \{\pi_t\}_{t=0}^T$, prior $\nu$ over $h$, and observation process
$\bar{R} = (\bar{R}_0,\ldots, \bar{R}_{T-1} )$, which is the set of aggregate
rewards used by the policy to determine the sampling allocations.

Instead of optimizing the Bayes simple regret for each finite batch size, we
use Theorem~\ref{theorem:limit} to derive an asymptotic approximation under
the Gaussian sequential experiment.  
\begin{corollary}
  \label{cor:bsr-limit}
  Let $\nu$ be a prior over average rewards $h$ satisfying
  $\E_{h \sim \nu} \lone{h} < \infty$ and let
  Assumptions~\ref{assumption:reward},~\ref{assumption:policy} hold. Then, the
  Bayes simple regret under $\bar{R}^n$ can be approximated by that under the
  Gaussian sequential experiment $G$ in Definition~\ref{def:gse}:
  \begin{equation}
    \label{eqn:bsr-limit}
    \bsr_{T}(\pi,\nu, \sqrt{n} \bar{R}^{n}) \to \bsr_{T}(\pi,\nu, G).
  \end{equation}
\end{corollary}
\noindent See Section~\ref{section:proof-bsr-limit}
for a proof of the corollary.

Using the approximation of Bayes simple regret, we optimize the asymptotic
Bayesian objective
\begin{equation}
  \label{eqn:gse-bsr}
  \minimize_{\pi}~~\left\{
    \bsr_{T}(\pi,\nu, G) = \E_{h \sim \nu} \E[h_{a\opt} - h_{\what{a}}]
    \right\}
\end{equation}
over policies $\pi = \{\pi_t(G_0, \ldots, G_{t-1})\}_{t=0}^T$ adapted to the
sequential observations $G_0, \ldots, G_{T-1}$ of the Gaussian sequential
experiment.  Policies derived from the optimization
problem~\eqref{eqn:gse-bsr} has marked modeling flexibility and computational
advantages compared to typical Bayesian sequential sampling algorithms (e.g.,
variants of Thompson sampling~\citep{Russo20}). Although reward distributions
are unknown in general, Bayesian sequential sampling approaches require a
distributional model of individual rewards comprising of prior and likelihood
function~\citep{RussoVaKaOsWe18}.  In contrast, our framework does not assume
restrictive distributional assumptions on rewards and allows naturally
incorporating prior information over average rewards $h_a$. 
% As a result, our limiting Gaussian sequential experiment coincides with typical frequentist
% inferential paradigms for calculating confidence intervals and statistical
% power.

Computationally, policies derived from the optimization
problem~\eqref{eqn:gse-bsr} can be efficiently updated \emph{offline}. As
these offline updates give a fixed sampling probability over the $\numarm$
arms at each epoch, the policies we propose in subsequent sections can be
deployed to millions of units at ease. This is in stark contrast to Bayesian
sequential sampling algorithms designed for unit-level feedback. While their
policy updates are also often performed offline in batches in
practice~\citep{OfferWestortCoGr20, EspositoSa22}, when deployed these policies
require \emph{real-time} posterior inference and action optimization to
generate treatment assignments for each unit. Implementing such methods at
scale is highly challenging even for the largest online platforms with mature
engineering infrastructure~\citep{AgarwalBiCoHoLaLeLiMeOsRi16,
  NamkoongDaBa20}. Methods derived from our formulation provides a scalable
alternative as deploying the resulting adaptive policies only involves
sampling from a fixed sampling probability $\pi_t$ for every unit in a batch,
\emph{regardless} of the batch size.

% Although reward distributions are unknown in
% general, Bayesian sequential sampling approaches require a distributional
% model of individual rewards comprising of prior and likelihood
% function~\cite{RussoVaKaOsWe18}.  In contrast, our framework does not assume
% restrictive distributional assumptions on rewards and allows naturally
% incorporating prior information over average rewards $h_a$. In our approach,
% the likelihood function of the aggregate rewards
% $\sqrt{n} \bar{R}_{t, a}^{n} \mid h$ is derived from the normal
% approximation~\eqref{eqn:weak-convergence}. As a result, our limiting Gaussian
% sequential experiment coincides with typical frequentist inferential paradigms
% for calculating confidence intervals and statistical power.

% Computationally, policies derived from our asymptotic optimization
% problem~\eqref{eqn:gse-bsr} can be efficiently updated offline. As these
% offline updates give a fixed sampling probability over the $\numarm$ arms at
% each epoch, the policies we propose in subsequent sections can be deployed to
% millions of units at ease. This is in stark contrast to Bayesian sequential
% sampling algorithms designed for unit-level feedback. While their policy
% updates are also often performed offline in batches in
% practice~\cite{OfferWestortCoGr20, EspositoSa22}, when deployed these policies
% require \emph{real-time} posterior inference and action optimization to
% generate treatment assignments for each unit. Implementing such methods at
% scale is highly challenging even for the largest online platforms with mature
% engineering infrastructure~\cite{AgarwalBiCoHoLaLeLiMeOsRi16,
%   NamkoongDaBa20}. Methods derived from our formulation provides a scalable
% alternative as deploying the resulting adaptive policies only involves
% sampling from a fixed sampling probability $\pi_t$ for every unit in a batch,
% \emph{regardless} of the batch size.


\subsection{Markov decision process}
\label{section:algorithms-mdp}

Using the Gaussian sequential experiment as an asymptotic approximation, we
derive a dynamic program that solves for the adaptive allocation minimizing
the Bayes simple regret~\eqref{eqn:gse-bsr}. Policies derived from this
dynamic program---which we call the (limiting) Bayesian adaptive
experiment---are tailored to the signal-to-noise ratio in each problem
instance and the number of reallocation opportunities $T$. In the Bayesian
adaptive experiment, we observe a Gaussian draw $G_t$ at each epoch and use it
to perform posterior updates over the experimenter's belief on the average
rewards $h$. We formulate the sequential updates using a Markov decision
process where state transitions are governed by changes in the posterior mean
and variance.
% Crucially, the dynamics of the MDP
% and its value functions are differentiable with respect to sampling allocations, 
% allowing the application of gradient-based methods for developing dynamic policies.
In Section~\ref{section:algorithms-adp}, we introduce a range of adaptive
sampling algorithms that (approximately) solve the Bayesian dynamic program,
and benchmark them empirically. Our empirical analysis highlights a
particularly effective policy, $\algofull$, which solves an open-loop problem
over future allocations that only use currently available information.


The normal approximations in the previous section gives Gaussian likelihood
functions in our Bayesian adaptive experiment
\begin{equation*}
  \mbox{Likelihood function:}~~~~
  G \mid h \sim N\left(\pi h, \diag\left( \frac{\pi s^2}{b} \right)\right),
\end{equation*}
where we abuse notation to describe elementwise algebraic operations over
$\numarm$-dimensional vectors. (Recall that $s^2 \in \R^{\numarm}$ is the
measurement variance on the raw rewards~\eqref{eqn:rewards}.) To achieve
conjugacy, we assume that the experimenter has a Gaussian prior over the
average reward
\begin{equation}
  \label{eqn:prior}
 \mbox{Prior:}~~~~ h \sim N\left(\mu_0, \diag(\sigma_0^2)\right) \eqdef \nu,
\end{equation}
where $\mu_0, \sigma_0^2 \in \R^{\numarm}$ are the prior mean and
variance. Standard posterior updates for Gaussian conjugate priors give the
following recursive formula for the posterior mean and variance
\begin{subequations}
  \label{eqn:posterior}
  \begin{align}
    \mbox{Posterior variance:}~~~~ \sigma_{t+1, a}^{-2}
    & \defeq \sigma_{t, a}^{-2} + s_a^{-2} b_t \pi_{t, a}
      \label{eqn:posterior-var} \\
    \mbox{Posterior mean:}~~~~\mu_{t+1, a}
    & \defeq   \sigma_{t+1, a}^{2}
      \left( \sigma_{t, a}^{-2} \mu_{t, a} + s_a^{-2}b_t G_{t, a} \right).
      \label{eqn:posterior-mean}
  \end{align}
\end{subequations}
The posterior variance decreases as a deterministic function of the sampling
allocation $\pi_t$, and in particular does not depend on the observation
$G_t$.

Under the posterior updates~\eqref{eqn:posterior}, our goal is to optimize the
Bayes simple regret~\eqref{eqn:gse-bsr} at the end of the experiment (time
$t = T$). We consider the final allocation that simply selects the arm with
the highest posterior mean; formally, if the $\argmax_a \mu_{T, a}$ is unique,
this is equivalent to 
\begin{equation*}
  \pi_{T, a} = \begin{cases}
    1 ~~&~\mbox{if}~a = \argmax_{a} \mu_{T, a} \\
    0 ~~&~\mbox{if}~a \neq\argmax_{a} \mu_{T, a}. 
  \end{cases}
\end{equation*}
(More generally, we choose an arm randomly from $\argmax_a \mu_{T, a}$; this
does not change any of the results below.)  Then, minimizing the Bayes simple
regret under the asymptotic Gaussian sequential experiment~\eqref{eqn:gse-bsr}
is equivalent to the following reward maximization problem
\begin{equation}
  \label{eqn:bayesian-dp}
  \maximize_{\pi_0, \ldots, \pi_{T-1}}~~
  \left\{V_{0}^{\pi}(\mu_{0},\sigma_{0}) = \E^{\pi} \left[ \max_a \mu_{T, a} \right]\right\}.
\end{equation}
Here, we write $\E^\pi$ to denote the expectation under the stochastic
transitions~\eqref{eqn:posterior} induced by the policy
$\pi_0, \ldots, \pi_{T-1}$ adapted to the observation sequence
$G_0, \ldots, G_{T-1}$. Although we focus on the Bayes simple regret in this
work, our MDP formulation can accomodate any objective function or constraint
that depends on the posterior states, such as the cumulative regret,
probability of correct selection, simple regret among the top-$k$ arms, and
constraints on the sampling allocations.

% Noting that the preceding optimization problem is entirely characterized by
% the prior~\eqref{eqn:prior} and the posterior updates~\eqref{eqn:posterior},
% we view $(\mu_t, \sigma_t)$ as the \emph{states} of a Markov decision
% process. The state transitions are governed by posterior
% updates~\eqref{eqn:posterior} which depend on the continuous actions
% $\pi_t \in \Delta^\numarm$. Given this Markovian structure in the limit
% adaptive experiment, we restrict attention to sampling allocations
% $\pi_t(\mu_t, \sigma_t)$ that only depend on the current state
% $(\mu_t, \sigma_t)$. We present algorithms for (approximately) solving the
% Bayesian dynamic program~\eqref{eqn:bayesian-dp} in the next subsection.


To simplify things further, we use a change of variable to reparameterize
state transitions~\eqref{eqn:posterior} as a random walk.
\begin{lemma}
  \label{lemma:mdp}
  Let $h \sim N(\mu_0, \sigma_0^2)$ and
  $Z_0, \ldots, Z_{T-1} \simiid N(0, I_{\numarm})$ be standard normal
  variables. The system governed by the posterior
  updates~\eqref{eqn:posterior} has the same joint distribution as that with
  the following state transitions
  \begin{subequations}
    \label{eqn:dynamics}
    \begin{align}
      \sigma_{t+1, a}^{-2}
      & \defeq \sigma_{t, a}^{-2} + s_a^{-2} b_t \pi_{t, a}(\mu_t, \sigma_t)
        \label{eqn:dynamics-var} \\
        %\defeq   \mu_{t, a} + \left( \sigma_{t, a}^2 - \sigma_{t+1, a}^2\right)^{\half} Z_{t, a}
      \mu_{t+1, a}
      &:= \mu_{t, a} + \sigma_{t, a}\sqrt{\frac{b_t \pi_{t, a}(\mu_t, \sigma_t)
        \sigma_{t, a}^{2}}{s_a^{2}+b_t\pi_{t, a}(\mu_t, \sigma_t)\sigma_{t, a}^{2}}} Z_{t, a}.
        \label{eqn:dynamics-mean}
    \end{align}
  \end{subequations}
\end{lemma}
\noindent We defer derivation details to Section~\ref{section:proof-mdp}.

For the Markov decision process defined by the reparameterized state
transitions~\eqref{eqn:dynamics}, the value function for the Bayes simple regret
$\E^{\pi} \left[ \max_a \mu_{T, a} \right]$ has the following succinct
representation
\begin{align}
  V_{t}^{\pi}(\mu_{t},\sigma_{t})
  & = \E^{\pi} \left[\max_a \mu_{T,a} \mid \mu_t, \sigma_t \right] \nonumber \\
  & = \E^{\pi} \left[ \max_{a} \left\{ \mu_{t,a} + \sum_{v=t}^{T-1} \sigma_{v,a}\sqrt{\frac{b_v \pi_{v, a}(\mu_{v},\sigma_{v})\sigma_{v, a}^{2}}{s_a^{2}+b_v\pi_{v, a}(\mu_{v},\sigma_{v})\sigma_{v, a}^{2}}}Z_{v, a}
    \right\} ~\Bigg|~ \mu_t, \sigma_t \right].   \label{eqn:q_fn}
\end{align}
In what follows, we use the notational shorthand
$\E_t^{\pi}[\cdot] = \E^{\pi}[\cdot \mid \mu_t, \sigma_t^2]$ so that
$V_{t}^{\pi}(\mu_{t},\sigma_{t}) = \E^{\pi}_t \left[\max_a \mu_{T,a} \right]$.
% \begin{equation}
%   \label{eqn:bayesian-dp}
%   \maximize_{\pi_0, \ldots, \pi_{T-1}}~
%   \left\{ V_{0}^{\pi}(\mu_{0},\sigma_{0}) =
%     \E^{\pi}_0 \left[\max_a \mu_{T,a} \right]
%     \right\}
% \end{equation}


\subsection{Asymptotic validity}

The state transitions~\eqref{eqn:posterior} are derived under the idealized
the Gaussian sequential experiment. In practice, any policy $\pi$ derived in
this asymptotic regime will be applied to a finite batch problem.  Recalling
the asymptotic approximation~\eqref{eqn:weak-convergence}, the finite batch
problem will involve states $(\mu_{n,t}, \sigma_{n,t})$ updated according to
the same dynamics~\eqref{eqn:posterior}, but using the sample mean estimator
$\sqrt{n}\bar{R}^{n}_{t}$ instead of the Gaussian approximation $G_t$
\begin{subequations}
  \label{eqn:pre-limit-posterior}
  \begin{align}
    \mbox{Pre-limit posterior variance:}~~~~ \sigma_{n,t+1, a}^{-2}
    & \defeq \sigma_{n,t, a}^{-2} + s_a^{-2} b_t \pi_{t, a}(\mu_{n,t}, \sigma_{n,t})
      \label{eqn:pre-limit-posterior-var} \\
    \mbox{Pre-limit posterior mean:}~~~~\mu_{n,t+1, a}
    & \defeq   \sigma_{n,t+1, a}^{2}
      \left( \sigma_{n,t, a}^{-2} \mu_{n,t, a} + s_a^{-2}b_t \sqrt{n}\bar{R}_{t,a}^{n} \right).
      \label{eqn:pre-limit-posterior-mean}
  \end{align}
\end{subequations}
Given any policy $\pi_t(\mu_t, \sigma_t)$ derived from the Bayesian dynamic
program~\eqref{eqn:bayesian-dp} and average rewards $h$, the pre-limit
posterior state $(\mu_{n,t}, \sigma_{n,t})$ evolves as a Markov chain and each
adaptive allocation to be used in the original finite batch problem is given
by $\pi_{t}(\mu_{n,t}, \sigma_{n,t})$.


For policies $\pi$ that is continuous in the states---satisfying conditions in
Assumption~\ref{assumption:policy}---Corollary~\ref{cor:bsr-limit} implies
that the Bayesian dynamic program~\eqref{eqn:bayesian-dp} is an accurate
approximation for measuring and optimizing the performance of the policy as
$n$ grows large. Moreover, we can show that the trajectories of pre-limit
posterior beliefs $(\mu_{n,t}, \sigma_{n,t})$~\eqref{eqn:pre-limit-posterior}
can be approximated by trajectories of the Markov decision process derived
using the Gaussian sequential experiment~\eqref{eqn:dynamics}.
\begin{corollary}
  \label{cor:bayes-limit}
  Let $\pi_t(\mu_t, \sigma_t)$ be a policy derived under the limit Bayesian
  dynamic program~\eqref{eqn:bayesian-dp} that is continuous as a function of
  $(\mu_t, \sigma_t)$.  Let Assumption~\ref{assumption:reward} hold and
  consider any fixed average reward $h$ and prior $(\mu_{0}, \sigma_{0})$. The
  posterior states~\eqref{eqn:pre-limit-posterior} converge to the
  states~\eqref{eqn:posterior} as $n\to\infty$
  \begin{equation*}
    (\mu_{n,t}, \sigma_{n,t},\ldots,\mu_{n,T-1}, \sigma_{n,T-1})
    \cd (\mu_{t}, \sigma_{t},\ldots,\mu_{T-1}, \sigma_{T-1})
  \end{equation*}
\end{corollary}
\noindent See Section~\ref{section:proof-bayes-limit} for the proof.


% Theorem~\ref{theorem:limit} guarantees that as $n$ grows large
% $(\mu_{n,t}, \sigma_{n,t})$ will converge to $(\mu_{t}, \sigma_{t})$, the
% Markov chain of posterior states in the Gaussian Sequential Experiment.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
