\section{Proofs of large horizon results}
\label{section:proof-asymptotic}

\subsection{Proof of Proposition~\ref{prop:asymptotic-rho}}
\label{section:proof-asymptotic-rho}

The proof of Proposition~\ref{prop:asymptotic-rho} first relies on a characterization of the gradient of 
the objective as $T - t \to \infty$.

\begin{proposition}
  \label{prop:grad-hess}
  Consider any fixed epoch $t$ and state $(\mu, \sigma)$.
  Let $\theta_{a} := \mu_a + \sigma_{a}Z_{a}$ be the random variable
  representing the posterior belief of the average reward for arm $a$,
  and let $\theta_{a}^{*} := \max_{a' \neq a} \theta_{a'}$.
  Suppose that as $T-t$ grows large, 
  the residual sample budget $\bar{b}_{t} = \sum_{s=t}^{T-1} b_{s}\to \infty$.
  Then gradient and Hessian of the planning objective $V^{\bar{\rho}}_{t}(\mu,\sigma)$
  with respect to the allocation $\bar{\rho}$ converge as follows:
    \begin{align*}
    \lim_{T \to \infty} \bar{b}_{t} \nabla V^{\bar{\rho}}_{t}(\mu,\sigma)
     &= \textup{diag} \left( \frac{s_{a}^{2}}{2\bar{\rho}_{a}^{2}}
     \E \left[\frac{1}{\sigma_{a}} \phi \left( \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right] \right) \\
     \lim_{T \to \infty} \bar{b}_{t} \nabla^{2} V^{\bar{\rho}}_{t}(\mu,\sigma)
     &= - \textup{diag} \left(\frac{s_{a}^{2}}{2\bar{\rho}_{a}^{3}}
     \E \left[\frac{1}{\sigma_{a}} \phi \left( \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right] \right)
    \end{align*}
  where $\phi(\cdot)$ is the standard normal density.
\end{proposition}

\begin{proof}
We characterize the asymptotic behavior of the gradient and hessian of
$V_{t}^{\bar{\rho}}(\mu,\sigma) =
\mathbb{E}_{t}\left[\max_{a}\mu_{T,a}\right]$ with respect to $\bar{\rho}$ as
the residual horizon $T-t$ grows large.  The residual horizon affects the
planning problem through the residual batch size $\bar{b}_{t}$, which is
assumed to converge to infinity as $T-t\to \infty$.  Rewrite the planning
objective $V_{t}^{\bar{\rho}}(\mu,\sigma)$ with a rescaled measurement
variance $s_{a}^{2}/\bar{b}_{t}$
\begin{align*}
  V_{t}^{\bar{\rho}}(\mu,\sigma) =\E_{t} \left[ \max_{a} \left\{ \mu_{t,a}
  + \sqrt{\frac{\sigma_{t, a}^4 \bar{\rho}_{a}}
  {s_a^2/\bar{b}_{t} + \sigma_{t, a}^2 \bar{\rho}_{a} }} Z_{t, a}  \right\} \right],
\end{align*}
It is sufficient to understand the behavior of
$V_{t}^{\bar{\rho}}(\mu,\sigma)$ as the measurement variance goes to zero.
For ease of notation we define $\alpha := 1/\bar{b}_{t}$, and consider
$\alpha$ as a scaling parameter of the measurement variance that converges to
zero.

Observe $V_{t}^{\bar{\rho}}(\mu,\sigma)$ can be expressed as the composition
$V_{t}^{\bar{\rho}}(\mu,\sigma)=g\left(\varphi\left(\bar{\rho},\sigma^{2},\alpha
    s^{2} \right)\right)$ where 
\[
\begin{aligned}
  g(v) &\defeq \E[\max_{a}\mu_{a}+v_{a}Z_{a}] \\
  \varphi_{a}(\bar{\rho}_{a},\sigma_{a}^{2},s_{a}^{2})&\defeq\sigma_{a}\sqrt{\frac{\bar{\rho}_{a}\sigma_{a}^{2}}{s_{a}^{2}+\bar{\rho}_{a}\sigma_{a}^{2}}}
\end{aligned}
\]
and $\varphi, \sigma^{2}, s^{2}$ refer to the vectorized versions. The
function $g(v)$ describes the behavior of the max of Gaussian random variables
under standard deviations $v$, and $\varphi$ characterizes how the standard
deviation of the update changes with the sampling probabilities $\bar{\rho}$.

Through the chain rule, it is sufficient to characterize the gradient and
hessian of $g,\varphi$.  \citet{FrazierPo10} derived expressions for the
gradient and hessian of $g(v)$.
\begin{lemma}[{\citet{FrazierPo10}}]
  \label{lemma:frazier} Letting
  $W_{a}\defeq \max_{a'\neq a}\mu_{a'}+v_{a'}Z_{a'}$,
  \begin{align*}
    \frac{\partial}{\partial v_{a}}g(v)
    & = \mathbb{E}\left[Z_{a}\indic{a=\arg\max_{a'}\mu_{a'}+v_{a'}Z_{a'}}\right]
      = \mathbb{E}\left[\phi\left(\frac{W_{a}-\mu_{a}}{v_{a}}\right)\right] \\
    \frac{\partial^{2}}{\partial v_{a}^{2}}g(v)
    & = \mathbb{E}\left[\left(\frac{W_{a}-\mu_{a}}{v_{a}^{3}}\right)
      \phi\left(\frac{W_{a}-\mu_{a}}{v_{a}}\right)\right] \\
    \frac{\partial^{2}}{\partial v_{a}\partial v_{a'}}g(v)
    & = \mathbb{E}\left[-\left(\frac{W_{a}-\mu_{a}}{v_{a}}\right)
      \phi\left(\frac{W_{a}-\mu_{a}}{v_{a}}\right)Z_{a'}
      \indic{a'=\argmax_{\hat{a}\neq a}
      \mu_{\hat{a}}+v_{\hat{a}}Z_{\hat{a}}}\right]
  \end{align*}
\end{lemma}
\noindent The first and second derivatives of $\varphi$ are given by
\[ \frac{\partial}{\partial\bar{\rho}_{a}}\varphi_{a}(\bar{\rho}_{a},\sigma_{a}^{2},\alpha s_{a}^{2}) 
  = \frac{\alpha s_{a}^{2} \sigma_{a}^{2}}{2\bar{\rho}_{a}^{1/2}(\alpha s_{a}^{2} + \bar{\rho}_{a}\sigma_{a}^{2})^{3/2}},
  \qquad \frac{\partial^{2}}{\partial\bar{\rho}_{a}^{2}}\varphi_{a}(\bar{\rho}_{a},\sigma_{a}^{2},\alpha s_{a}^{2}) = -\frac{\sigma_{a}^{2}(\alpha^2 s_{a}^{4} + 4\bar{\rho}_{a} \alpha s_{a}^{2}\sigma_{a}^{2})}{4\bar{\rho}_{a}^{3/2}(\alpha s_{a}^{2} + \bar{\rho}_{a}\sigma_{a}^{2})^{5/2}}.
\]

We first study the behavior of $\varphi$ and its derivatives as
$\alpha\to0$
\[
\varphi_{a}(\bar{\rho}_{a},\sigma_{a}^{2},\alpha s_{a}^{2}) \to \sigma_{a},
\qquad \frac{\partial}{\partial\bar{\rho}_{a}}\varphi_{a}(\bar{\rho}_{a},\sigma_{a}^{2},\alpha s_{a}^{2})\sim\frac{\alpha s_{a}^{2}}{2\bar{\rho}_{a}^{2}\sigma_{a}},
\qquad \frac{\partial^{2}}{\partial\bar{\rho}_{a}^{2}}\varphi_{a}(\bar{\rho}_{a},\sigma_{a}^{2}, \alpha s_{a}^{2})\sim-\frac{\alpha s_{a}^{2}}{\bar{\rho}_{a}^{3}\sigma_{a}}.
\]
To analyze $g(\varphi (\bar{\rho},\sigma^{2},\alpha s^{2}))$, recall that
$\theta_{a} = \mu_{a} + \sigma_{a}Z_{a}$ is the random variable representing
the posterior belief of the average reward of arm $a$.  For any realization of
$Z\sim N(0,I)$, we have
\[
  W_{a} = \max_{a'\neq a}\left\{ \mu_{a'}+\varphi_{a'}(\bar{\rho}_{a'},\sigma_{a'}^{2},\alpha s_{a'}^{2})Z_{a'} \right\} \to \theta_{a}^{*},
\]
so dominated convergence implies
\begin{align*}
  \lim_{\alpha \to 0}\frac{\partial}{\partial v_{a}}
  g(\varphi(\bar{\rho}, \sigma^{2}, \alpha s^2))
  & = \E \left[
    \phi\left(\frac{\theta_{a}^{*} - \mu_{a}}{\sigma_{a}}\right)
    \right] \\
  \lim_{\alpha \to 0}
  \frac{\partial^{2}}{\partial v_{a}^{2}}
  g(\varphi(\bar{\rho}, \sigma^{2}, \alpha s^2))
  & = \E \left[
    \frac{\theta_{a}^{*} - \mu_{a}}{\sigma_{a}^{3}} 
    \cdot \phi\left(\frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}}\right)
    \right] \\
  \lim_{\alpha \to 0}
  \frac{\partial^{2}}{\partial v_{a}\partial v_{a'}}
  g(\varphi(\bar{\rho}, \sigma^{2}, \alpha s^2))
  & = \E \left[
    - \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \cdot
    \phi\left(\frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}}\right)
    Z_{a'} \indic{a'=\arg\max_{\hat{a}\neq a}\theta_{\hat{a}}}
    \right]
\end{align*}


By the chain rule,
$\nabla_{\bar{\rho}} V_{t}^{\bar{\rho}}(\mu, \sigma) =
\nabla_{v}g(\varphi(\mu, \sigma^{2}, \alpha s^{2})) \nabla_{\bar{\rho}}
\varphi (\bar{\rho},\sigma^{2},\alpha s^{2}) $ where the Jacobian
$\nabla_{\bar{\rho}} \varphi$ is
\[
  \nabla_{\bar{\rho}} \varphi (\bar{\rho},\sigma^{2},\alpha s^{2}) := \text{diag} \left( \frac{\partial}{\partial\bar{\rho}_{a}}\varphi_{a}(\bar{\rho}_{a},\sigma_{a}^{2},\alpha s_{a}^{2}) \right).
\]
Rescaling the planning objective $V_{t}^{\bar{\rho}}$ by $1/\alpha$, we obtain
the following limit
\[
  \lim_{\alpha \to \infty}  \alpha^{-1}
  \nabla_{\bar{\rho}} V^{\bar{\rho}}_{t}(\mu, \sigma)
  = \frac{s_{a}^{2}}{2\bar{\rho}_{a}^{2}\sigma_{a}}
  \E \left[\phi\left(\frac{\theta_{a}^{*} - \mu_{a}}{\sigma_{a}}\right)\right]
\]
As for the Hessian, the chain rule implies
\begin{align*}
  \frac{\partial^{2} V_{t}^{\bar{\rho}}}{\partial \bar{\rho}_{a}\partial \bar{\rho}_{a'}}
  &= \sum_{i} \frac{\partial g(\varphi(\bar{\rho}))}{\partial v_{i}}
    \frac{\partial^{2} \varphi_{i}}{\partial \bar{\rho}_{a} \partial \bar{\rho}_{a'}}
    + \sum_{i,j} \frac{\partial g(\varphi(\bar{\rho})) }{\partial v_{i}\partial v_{j}} 
    \frac{\partial \varphi_{i}}{\partial \bar{\rho}_{a}}
    \frac{\partial \varphi_{j}}{\partial \bar{\rho}_{a'}} \\
  &= \frac{\partial g(\varphi(\bar{\rho}))}{\partial v_{a}}
    \frac{\partial^{2} \varphi_{a}}{\partial \bar{\rho}_{a}^{2}} 1\{ a = a' \}
    + \frac{\partial g(\varphi(\bar{\rho})) }{\partial v_{a}\partial v_{a'}} 
    \frac{\partial \varphi_{a}}{\partial \bar{\rho}_{a}}
    \frac{\partial \varphi_{a'}}{\partial \bar{\rho}_{a'}},
\end{align*}
where we suppress the dependence of $\varphi$ on other arguments for
succinctness. Rewriting this using
$\nabla_{\bar{\rho}}^{2}\varphi := \diag \left( \frac{\partial^{2}
    \varphi_{a}}{\partial \bar{\rho}_{a}^{2}} \right)$, we get
\[  
  \nabla_{\bar{\rho}}^{2}V^{\bar{\rho}}_{t}(\mu,\sigma)
 = \text{diag}(\nabla_{v} g(\varphi(\bar{\rho}))) \nabla_{\bar{\rho}}^{2}\varphi + \nabla_{\bar{\rho}}\varphi \nabla_{v}^{2}g(\varphi(\bar{\rho}))\nabla_{\bar{\rho}}\varphi.
\]
Collecting the above results, we have
\begin{align*}
  \lim_{\alpha \to 0} \alpha^{-1} 
  \text{diag}(\nabla_{v} g(\varphi(\bar{\rho}))) \nabla_{\bar{\rho}}^{2}\varphi
  &= -\text{diag} \left( \frac{ s_{a}^{2}}{\bar{\rho}_{a}^{3}\sigma_{a}} 
      \E \left[\phi\left(\frac{\theta_{a}^{*} - \mu_{a}}{\sigma_{a}}\right)\right] \right)\\
  \lim_{\alpha \to 0} \alpha^{-1}  \nabla_{\bar{\rho}}\varphi \nabla_{v}^{2}g(\varphi(\bar{\rho}))\nabla_{\bar{\rho}}\varphi
  &= \lim_{\alpha \to 0} \alpha^{-1}
  \text{diag} \left(  \frac{\alpha s_{a}^{2}}{2 \bar{\rho_{a}}^{2} \sigma_{a}} \right)
  \nabla_{v}^{2} g(\varphi(\bar{\rho}))
  \text{diag} \left( \frac{ \alpha s_{a}^{2}}{2 \bar{\rho_{a}}^{2} \sigma_{a}} \right)
  = 0,
\end{align*}
which gives the final result.

\end{proof}

This leads us to the following corollary, which uses the characterization of the Hessian to show that 
the objective becomes strongly concave as $T-t \to \infty$.

\begin{corollary}
  \label{cor:strong-concavity}
  Let $\Delta_K^\epsilon = \Delta_K \cap \{p: p \ge \epsilon\}$ be the
  truncated simplex. For a fixed epoch $t$ and state $(\mu, \sigma)$, there
  exists $T_0(\epsilon) > 0$ such that $\forall T - t > T_0$,
  $\bar{\rho} \mapsto \bar{b}_{t}V^{\bar{\rho}}_{t}(\mu,\sigma)$ is strongly
  concave on $\Delta_K^\epsilon$.
\end{corollary}

% \subsection{Proof of Corollary~\ref{cor:strong-concavity}}
% \label{section:proof-strong-concavity}

\begin{proof}
In the proof of Proposition~\ref{prop:grad-hess}, we showed that as the
(scaled) measurement variance $\alpha s_{a}^{2}$ converges to zero, the
Hessian $\alpha^{-1} \nabla_{\bar{\rho}}^{2}V^{\bar{\rho}}_{t}(\mu,\sigma)$
can be expressed as a negative-definite diagonal matrix plus a matrix that
converges to zero. As long as the sampling probabilities are bounded below, the
convergence is uniform. We have the following bound for the largest eigenvalue
of the Hessian
\begin{align*}
  \lambda_{\max}\left(
  \nabla_{\bar{\rho}}^{2} \alpha^{-1}
  V^{\bar{\rho}}_{t}(\mu,\sigma) \right)
  & \leq \lambda_{\max}\left( \alpha^{-1}
    \text{diag}(\nabla_{v} g(\varphi(\bar{\rho})))
    \nabla_{\bar{\rho}}^{2}\varphi \right)
    +\lambda_{\max}\left(\alpha^{-1} \nabla_{\bar{\rho}}\varphi
    \nabla_{v}^{2}g(\varphi(\bar{\rho}))\nabla_{\bar{\rho}}\varphi
    \right) \\
  & \leq -\min_{a} \frac{s_{a}^{2}}{2\sigma_{a}\epsilon^3}
    \E \left[\phi\left(\frac{\theta_{a}^{*} - \mu_{a}}{\sigma_{a}}\right)\right]
    + \alpha C/\epsilon^{4},
\end{align*}
where $C$ is a constant that depends only on $\mu,\sigma,s$. So as
$\alpha \to 0$, the Hessian will be negative-definite for all
$\bar{\rho}\in \Delta_{K}^{\epsilon}$. Thus, for a large enough residual batch
size $\bar{b}_{t}$ the planning objective will be strongly concave on the
truncated simplex $\Delta_{K}^{\epsilon}$ for some threshold $\epsilon$.
\end{proof}

% \hn{I think the final bound be something like
% $-\min_{a} \frac{s_{a}^{2}}{\epsilon^3 \sigma_{a}}
%     \E \left[\phi\left(\frac{\theta_{a}^{*} - \mu_{a}}{\sigma_{a}}\right)\right]
%     + \alpha C/\epsilon^{4}$?}

% \subsection{Proof of Proposition~\ref{prop:dts}}
% \label{section:proof-dts}

% By assumption, there exists some threshold $\epsilon > 0$ so that
% for $T-t$ large enough, the solution to $\max_{\rho\in\Delta_{K}^{\epsilon}} V^{\bar{\rho}}_{t}(\mu,\sigma)$
% is unique and the solution is in the interior of $\Delta_{K}^{\epsilon}$.
% We require this assumption as we prove convergence of the solutions through the
% KKT conditions, but we only obtain strong concavity, which guarantees uniqueness
% of solutions, on the truncated simplex
% $\Delta_{K}^{\epsilon}$ rather than the entire simplex. 
% However, this is not a strong assumption,
% as we are free to choose the threshold $\epsilon$.
% Although there are other techniques for proving the convergence of maxima
% without this assumption
% (e.g. Berge's Maximum Principle), a technical difficulty is that while
% the gradient and hessian of the rescaled objective $\alpha^{-1}V^{\bar{\rho}}_{t}(\mu,\sigma)$
% have well-defined limits, the function itself blows up as $\alpha \to 0$.

Finally, by strong concavity, the KKT conditions of the planning problem have a unique solution.
By the implicit function theorem, we can characterize the limit of this solution.

\begin{proposition}
  \label{prop:dts}
  Consider a fixed epoch $t$ and state $(\mu, \sigma)$. Suppose there exists
  $T_1$ such that for $\forall T - t > T_1$, the $\algo$ allocation satisfies
  $\rho_{t,a}(\mu,\sigma) > \epsilon$. Then as $T-t \to \infty$,
  \begin{equation*}
    \rho_{t,a}(\mu,\sigma) \to \pi_{a}^{\textup{DTS}}(\mu, \sigma),~\mbox{where}~
    \pi^{\rm DTS}_a(\mu, \sigma) \propto s_a \left[ \frac{\partial}{\partial
        \mu_a}\pi_{a}^{\textup{TS}}(\mu,\sigma) \right]^{1/2}
  \end{equation*}
  %if $\pi^{\rm DTS}_a (\mu_t,\sigma_t) > \epsilon$.
\end{proposition}

\begin{proof}
As in the proof of Proposition~\ref{prop:grad-hess}, we let
$\alpha = \bar{b}_{t}^{-1}$. Let $\alpha_{0}$ to be the threshold such that
for $\alpha < \alpha_{0}$ the objective is strongly concave (such a threshold
exists from Corollary~\ref{cor:strong-concavity}), and let
$\alpha_{1} < \alpha_{0}$ to be the threshold such that for
$\alpha < \alpha_{1}$ the unique solution is in the interior of
$\Delta_{K}^{\epsilon}$ (such a threshold exists by hypothesis).

For $\alpha$ small enough, the (scaled) KKT conditions give
\begin{equation}
  \label{eqn:opt_foc}
  \alpha^{-1} \nabla_{\bar{\rho}} V^{\bar{\rho}}_{t}(\mu,\sigma)
  - \lambda \onevec = 0, \quad \onevec^\top \rho_t = 1,
\end{equation}
where $\lambda$ is the optimal dual variable for the equality constraint.  The
Jacobian of this equation with respect to $(\bar{\rho}, \lambda)$ is the
following block matrix
\[
\begin{bmatrix}
  \nabla^{2}_{\bar{\rho}} \alpha^{-1}V^{\bar{\rho}}_{t}(\mu,\sigma) & -\onevec\\
  \onevec^{\top} & 0
\end{bmatrix}
\]
Since $\alpha < \alpha_{0}$, the Hessian of
$\alpha^{-1}V^{\bar{\rho}}_{t}(\mu,\sigma)$ is negative definite and
invertible in a neighborhood of $\alpha = 0$.  The Jacobian is also invertible
since the Schur complement
\begin{equation*}
  0 - \onevec^\top \alpha^{-1} \nabla^{2}_{\bar{\rho}}\E\left[\max_{a}\mu_{T,a}\right] (-\onevec) < 0
\end{equation*}
is invertible.  Moreover,
$\alpha^{-1} \nabla_{\bar{\rho}} V^{\bar{\rho}}_{t}(\mu,\sigma)$ is
continuously differentiable in $\alpha$ at $\alpha = 0$.  So by the Implicit
Function Theorem, there exists a $\alpha_2$ such that for all
$\alpha < \alpha_2$, the solution satisfying the KKT
conditions~\eqref{eqn:opt_foc}, $\bar{\rho}(\alpha)$ and $\lambda(\alpha))$,
is continuous in $\alpha$.  Thus, as $\alpha \to 0$, the unique maximizer of
$\alpha^{-1}V^{\bar{\rho}}_{t}(\mu,\sigma)$ converges to $\bar{\rho}\opt$
satisfying the limiting version of the KKT conditions~\eqref{eqn:opt_foc}.
Using the explicit expression for
$\alpha^{-1} \nabla_{\bar{\rho}} V^{\bar{\rho}}_{t}(\mu,\sigma)$ obtained in
Proposition~\ref{prop:grad-hess}, we conclude
\begin{equation}
  \label{eqn:kkt_dts}
  \bar{\rho}_{a}\opt \propto s_{a} \E \left[\frac{1}{\sigma_{a}}
    \phi \left( \frac{\theta_{a}\opt-\mu_{a}}{\sigma_{a}} \right)
  \right]^{1/2}
\end{equation}



% Using the expression for the gradient in the proof of
% Proposition~\ref{prop:grad-hess}, we have that the following expression for
% the KKT conditions of the rescaled objective
% $\alpha^{-1}V^{\bar{\rho}}_{t}(\mu,\sigma)$ for $\alpha < \alpha_{1}$.  We can
% omit the inequality constraints $\bar{\rho} > \epsilon$ as by assumption at
% the solution the inequality constraints do not bind:
% At $\alpha = 0$, using the limit of $\nabla_{\bar{\rho}} \alpha^{-1}V^{\bar{\rho}}_{t}(\mu,\sigma)$
% obtained in Proposition~\ref{prop:grad-hess}, we have that the solution to the KKT conditions is:
% \begin{equation} \label{eqn:kkt_dts}
%   \bar{\rho}_{a}^{*} \propto s_{a} \E \left[\frac{1}{\sigma_{a}} \phi \left( \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right]^{1/2},
%   \quad \lambda^{*} = \sum_{a} 2\bar{\rho}_{a}^{*}
% \end{equation}



To show the equivalence with the partial derivatives of the Thompson sampling probabilities, observe
\begin{align*}
  \pi_{a}^{\text{TS}}(\mu,\sigma)
  & =\mathbb{P}(\theta_{a}>\theta_{a}^{*})
    =\mathbb{P}\left(Z_{a}>\frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}}\right) \\
  & = \E\left[\mathbb{P}\left(
    Z_{a}>\frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}}
    \mid \theta_{a}^{*} \right) \right] 
   = \E\left[1-\Phi\left(
    \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}}\right)\right],
\end{align*}
where the final equality is a result of the independence prior and posterior
distributions across the treatment arms. Interchanging the derivative
and expectation, 
\[ \frac{\partial}{\partial\mu_{a}}
  \pi_{a}^{\text{TS}}(\mu,\sigma)
  = \frac{\partial}{\partial\mu_{a}}
  \E\left[
    1-\Phi\left(\frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}}\right)
  \right]
  = \E\left[\frac{1}{\sigma_{a}}
    \phi\left(\frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}}\right)\right].
\]
\end{proof}

\subsection{More details on Density Thompson Sampling}
\label{section:proof-log-dts}

Similar to TS, DTS samples from the posterior distribution; while TS
asymptotically assigns all sampling effort to the best arm, DTS does not
oversample the best arm, as it is based on the gap between the second best
arm. Concretely, the relative sampling proportions
$\pi^{\text{DTS}}_{a}/\pi^{\text{DTS}}_{a'}$ can be expressed as ratios of the
`index'
$\E \left[\frac{1}{\sigma_{a}} \phi \left(
    \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right]^{1/2}$; the
higher the index, the more sampling effort allocated to that arm during the
epoch.  This index decreases at an exponential rate for \emph{all} arms as the
experiment progresses and the posterior uncertainty shrinks
$\sigma_{a} \to 0$.  
\begin{proposition}
  \label{prop:log-dts}
  Consider a fixed state $(\mu, \sigma)$ and suppose arms are sorted in decreasing order of their means $\mu_{1} > \mu_{2} > \ldots > \mu_{K}$.
  Then we have that:
  \[
    \begin{cases}
      -\frac{(\mu_{1}-\mu_{2})^{2}}{2(\sigma_{1}^{2}+\sigma_{2}^{2})}
      & ~~\mbox{top two arms}~~a\in{1,2} \\
      -\frac{(\mu_{1}-\mu_{a})^{2}}{2\sigma_{a}^{2}}
      & ~~\mbox{otherwise} \\
    \end{cases}
  \lesssim \log \E \left[\frac{1}{\sigma_{a}} \phi \left( \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right]
  \lesssim -\min_{a'\neq a}\frac{(\mu_{a'}-\mu_{a})^{2}}{2(\sigma_{a}^{2}+\sigma_{a'}^{2})}
  \]
  where $\lesssim$ contains additive terms that are logarithmic in $\min_{a} \sigma_{a}$.
  % For the remaining arms:
  % \[
  % -\frac{(\mu_{1}-\mu_{a})^{2}}{2(\sigma_{a}^{2})}
  % \leq \log \E \left[\frac{1}{\sigma_{a}} \phi \left( \frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}} \right) \right]
  % \leq -\min_{a'\neq a}-\frac{(\mu_{a'}-\mu_{a})^{2}}{2(\sigma_{a}^{2}+\sigma_{a'}^{2})}
  % \]
\end{proposition}
%\noindent See Section~\ref{section:proof-log-dts} for a proof. 
If an arm is excessively under-sampled, its index will eventually be larger than others and
so sampling effort will be spread out across arms rather than concentrating on
the best one.  This makes DTS better suited for best-arm identification and
closer to Top-Two Thompson Sampling~\citep{Russo20}.

\begin{proof}
Suppose that the arms are sorted so that $\mu_{1}>...>\mu_{K}$. Since
$\theta_{a}^{*}$ is the max of independent random variables, $\mathbb{P}(\theta_{a}^{*}\leq x)=\prod_{a'\neq a}\mathbb{P}(\theta_{a'}\leq x)=\prod_{a'\neq a}\Phi\left(\frac{x-\mu_{a'}}{\sigma_{a'}}\right)$.
This means that the density of $\theta_{a}^{*}$ , denoted as $f_{a}^{*}(x)$,
can be expressed as (by the Leibniz product rule)
\begin{align*}
  f_{a}^{*}(x)
  & =\frac{d}{dx}\prod_{a'\neq a}\Phi\left(\frac{x-\mu_{a'}}{\sigma_{a'}}\right)\\
  & =\sum_{a'\neq a}
    \left[\frac{d}{dx}\Phi\left(\frac{x-\mu_{a'}}{\sigma_{a'}}\right)\right]
    \prod_{\hat{a}\notin\{a,a'\}}\Phi\left(\frac{x-\mu_{\hat{a}}}{\sigma_{\hat{a}}}\right)\\
  & =\sum_{a'\neq a}\left[\frac{1}{\sigma_{a'}}
    \phi\left(\frac{x-\mu_{a'}}{\sigma_{a'}}\right)\right]
    \prod_{\hat{a}\notin\{a,a'\}}\Phi\left(\frac{x-\mu_{\hat{a}}}{\sigma_{\hat{a}}}\right).
\end{align*}
Thus, we can write the expectation as
\begin{align*}
  \mathbb{E}\left[\frac{1}{\sigma_{a}}\phi\left(\frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}}\right)\right]
  & =\int_{-\infty}^{\infty}\frac{1}{\sigma_{a}}
    \phi\left(\frac{x-\mu_{a}}{\sigma_{a}}\right)f_{a}^{*}(x)dx\\
  & =\int_{-\infty}^{\infty}\frac{1}{\sigma_{a}}
    \phi\left(\frac{x-\mu_{a}}{\sigma_{a}}\right)\sum_{a'\neq a}
    \left[\frac{1}{\sigma_{a'}}
    \phi\left(\frac{x-\mu_{a'}}{\sigma_{a'}}\right)\prod_{\hat{a}\notin\{a,a'\}}
    \Phi\left(\frac{x-\mu_{\hat{a}}}{\sigma_{\hat{a}}}\right)\right]dx\\
  & =\sum_{a'\neq a}\int_{-\infty}^{\infty}\frac{1}{\sigma_{a}}
    \phi\left(\frac{x-\mu_{a}}{\sigma_{a}}\right)
    \frac{1}{\sigma_{a'}}\phi\left(\frac{x-\mu_{a'}}{\sigma_{a'}}\right)
    \prod_{\hat{a}\notin\{a,a'\}}\Phi\left(\frac{x-\mu_{\hat{a}}}{\sigma_{\hat{a}}}\right)dx.
\end{align*}

For any arm $a$, we can obtain an upper bound for each term in the
sum by bounding $\Phi\left(\frac{x-\mu_{\hat{a}}}{\sigma_{\hat{a}}}\right)\leq1$
and directly integrating
\begin{align*}
  & \int_{-\infty}^{\infty}\frac{1}{\sigma_{a}}
    \phi\left(\frac{x-\mu_{a}}{\sigma_{a}}\right)
    \frac{1}{\sigma_{a'}}\phi\left(\frac{x-\mu_{a'}}{\sigma_{a'}}\right)
    \prod_{\hat{a}\notin\{a,a'\}}\Phi\left(\frac{x-\mu_{\hat{a}}}{\sigma_{\hat{a}}}\right)dx\\
  & \leq\int_{-\infty}^{\infty}\frac{1}{\sigma_{a}}
    \phi\left(\frac{x-\mu_{a}}{\sigma_{a}}\right)
    \frac{1}{\sigma_{a'}}\phi\left(\frac{x-\mu_{a'}}{\sigma_{a'}}\right)dx\\
  & =\frac{1}{\sqrt{2\pi(\sigma_{a'}^{2}+\sigma_{a}^{2})}}
    e^{-\frac{(\mu_{a'}-\mu_{a})^{2}}{2(\sigma_{a'}^{2}+\sigma_{a}^{2})}}.
\end{align*}
Finally, taking a max and summing up the terms gives
\[
  \mathbb{E}\left[\frac{1}{\sigma_{a}}
    \phi\left(\frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}}\right)\right]
  \leq(K-1)\max_{a'\neq a}\frac{1}{\sqrt{2\pi(\sigma_{a'}^{2}+\sigma_{a}^{2})}}
  e^{-\frac{(\mu_{a'}-\mu_{a})^{2}}{2(\sigma_{a'}^{2}+\sigma_{a}^{2})}}
\]

To get the lower bound, it suffices to lower bound a single $a'$ term in the
sum, since all terms are positive. We select the term involving $a'=1$ ($a'=2$
if $a=1$). Since the integrand is positive, we obtain a further lower bound by
truncating the integration from $\mu_{1}$ to $\infty$
\begin{align*}
  \mathbb{E}\left[\frac{1}{\sigma_{a}}\phi\left(\frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}}\right)\right]
  & \geq\int_{-\infty}^{\infty}
    \frac{1}{\sigma_{a}}\phi\left(\frac{x-\mu_{a}}{\sigma_{a}}\right)
    \frac{1}{\sigma_{1}}\phi\left(\frac{x-\mu_{1}}{\sigma_{1}}\right)
    \prod_{\hat{a}\notin\{a,1\}}\Phi\left(\frac{x-\mu_{\hat{a}}}{\sigma_{\hat{a}}}\right)dx \\
  & \geq\int_{\mu_{1}}^{\infty}\frac{1}{\sigma_{a}}
    \phi\left(\frac{x-\mu_{a}}{\sigma_{a}}\right)
    \frac{1}{\sigma_{1}}\phi\left(\frac{x-\mu_{1}}{\sigma_{1}}\right)
    \prod_{\hat{a}\notin\{a,1\}}\Phi\left(\frac{x-\mu_{\hat{a}}}{\sigma_{\hat{a}}}\right)dx.
\end{align*}
On the domain of integration, $x\geq\mu_{1}$, so
$\Phi\left(\frac{x-\mu_{\hat{a}}}{\sigma_{\hat{a}}}\right)\geq\frac{1}{2}$ for
all $\hat{a}\notin\{a,1\}$. Conclude
\begin{align*}
  & \int_{\mu_{1}}^{\infty}\frac{1}{\sigma_{a}}
    \phi\left(\frac{x-\mu_{a}}{\sigma_{a}}\right)
    \frac{1}{\sigma_{1}}\phi\left(\frac{x-\mu_{1}}{\sigma_{1}}\right)
    \prod_{\hat{a}\notin\{a,1\}}\Phi\left(\frac{x-\mu_{\hat{a}}}{\sigma_{\hat{a}}}\right)dx\\
  & \geq\left(\frac{1}{2}\right)^{K-2}
    \int_{\mu_{1}}^{\infty}\frac{1}{\sigma_{a}}
    \phi\left(\frac{x-\mu_{a}}{\sigma_{a}}\right)
    \frac{1}{\sigma_{1}}\phi\left(\frac{x-\mu_{1}}{\sigma_{1}}\right)dx.
\end{align*}
Evaluating the integral explicitly gives
\begin{align*}
   \int_{\mu_{1}}^{\infty}\frac{1}{\sigma_{a}}
    \phi\left(\frac{x-\mu_{a}}{\sigma_{a}}\right)
    \frac{1}{\sigma_{1}}\phi\left(\frac{x-\mu_{1}}{\sigma_{1}}\right)dx
  = \frac{e^{-\frac{(\mu_{a}-\mu_{1})^{2}}{2(\sigma_{a}^{2}+\sigma_{1}^{2})}}}{\sqrt{2\pi (\sigma_{a}^{2}+ \sigma_{1}^{2})}}
    \bar{\Phi} \left( \frac{\sigma_{1}}{\sigma_{a}}\frac{\mu_{1} - \mu_{a}}{\sqrt{\sigma_{a}^{2}+ \sigma_{1}^{2}}} \right)
  % & =\frac{1}{\sigma_{a}\sigma_{1}}
  %   e^{-\frac{(\mu_{a}-\mu_{1})^{2}}{2(\sigma_{a}^{2}+\sigma_{1}^{2})}}
  %   \int_{\mu_{1}}^{\infty}e^{-\frac{1}{2}\left(\sigma_{1}^{-2}+\sigma_{a}^{-2}\right)
  %   \left(x-\frac{\sigma_{1}^{-2}\mu_{1}+\sigma_{a}^{-2}\mu_{a}}{\sigma_{1}^{-2}+\sigma_{a}^{-2}}
  %   \right)^{2}}dx\\
  % & =\frac{1}{\sigma_{a}\sigma_{1}}
  %   e^{-\frac{(\mu_{a}-\mu_{1})^{2}}{2(\sigma_{a}^{2}+\sigma_{1}^{2})}}
  %   \sqrt{2\pi(\sigma_{1}^{-2}+\sigma_{a}^{-2})^{-1}}
  %   \bar{\Phi}\left(\frac{(\mu_{1}-\mu_{a})}
  %   {\sigma_{a}^{2}\sqrt{\sigma_{1}^{-2}+\sigma_{a}^{-2}}}\right).
\end{align*}
Using the Gaussian tail bound $\bar{\Phi}(x)\geq\frac{\phi(x)}{2x}$ for
$x > 0$, 
\begin{align*}
  \mathbb{E}\left[\frac{1}{\sigma_{a}}
  \phi\left(\frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}}\right)\right]
  & \geq\left(\frac{1}{2}\right)^{K-2}
    \int_{\mu_{1}}^{\infty}\frac{1}{\sigma_{a}}
    \phi\left(\frac{x-\mu_{a}}{\sigma_{a}}\right)
    \frac{1}{\sigma_{1}}\phi\left(\frac{x-\mu_{1}}{\sigma_{1}}\right)dx\\
  & \geq\frac{1}{\sqrt{2\pi(\sigma_{a}^{2}+\sigma_{1}^{2})}}
    e^{-\frac{(\mu_{a}-\mu_{1})^{2}}{2(\sigma_{a}^{2}+\sigma_{1}^{2})}}
    \frac{1}{2\left(\frac{\sigma_{1}}{\sigma_{a}}\frac{(\mu_{1}-\mu_{a})}{\sqrt{\sigma_{1}^{2}+\sigma_{a}^{2}}}\right)}
    e^{-\frac{(\mu_{1}-\mu_{a})^{2}}{2\sigma_{a}^{4}(\sigma_{1}^{-2}+\sigma_{a}^{-2})}}\\
  & =\frac{\sigma_{a}}{2\sigma_{1}\sqrt{2\pi}(\mu_{1}-\mu_{a})}
    e^{-\frac{1}{2}\frac{(\mu_{a}-\mu_{1})^{2}}{\sigma_{a}^{2}}}.
\end{align*}

We can get tighter lower bounds for the top two arms $a\in\{1,2\}$.  As in the
above lower bound, we focus on lower bounding the term corresponding with
$a'=1$ if $a=2$ or $a'=2$ if $a=1$ (for both we obtain the same lower bound).
But instead of integrating from $\mu_{1}$, we integrate from $\mu_{2}$.  For
illustration, we focus on $a=2$
\begin{align*}
  \mathbb{E}\left[\frac{1}{\sigma_{2}}
  \phi\left(\frac{\theta_{2}^{*}-\mu_{2}}{\sigma_{2}}\right)\right]
  & \geq\int_{-\infty}^{\infty}\frac{1}{\sigma_{2}}
    \phi\left(\frac{x-\mu_{2}}{\sigma_{2}}\right)
    \frac{1}{\sigma_{1}}\phi\left(\frac{x-\mu_{1}}{\sigma_{1}}\right)
    \prod_{\hat{a}\notin\{a,1\}}
    \Phi\left(\frac{x-\mu_{\hat{a}}}{\sigma_{\hat{a}}}\right)dx\\
  & \geq\int_{\mu_{2}}^{\infty}\frac{1}{\sigma_{2}}
    \phi\left(\frac{x-\mu_{2}}{\sigma_{2}}\right)
    \frac{1}{\sigma_{1}}\phi\left(\frac{x-\mu_{1}}{\sigma_{1}}\right)
    \prod_{\hat{a}\notin\{a,1\}}
    \Phi\left(\frac{x-\mu_{\hat{a}}}{\sigma_{\hat{a}}}\right)dx.
\end{align*}
On this range, we have $x\geq\mu_{2}$. This implies that for every
$\hat{a}\neq\{1,2\}$, $x-\mu_{\hat{a}}\geq\mu_{2}-\mu_{\hat{a}}\geq0$. In
turn, this means
$\Phi\left(\frac{x-\mu_{\hat{a}}}{\sigma_{\hat{a}}}\right)\geq\frac{1}{2}$ as
we had before
\[
  \mathbb{E}\left[\frac{1}{\sigma_{a}}
    \phi\left(\frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}}\right)\right]
  \geq\left(\frac{1}{2}\right)^{K-2}
  \int_{\mu_{2}}^{\infty}\frac{1}{\sigma_{2}}
  \phi\left(\frac{x-\mu_{2}}{\sigma_{2}}\right)
  \frac{1}{\sigma_{1}}\phi\left(\frac{x-\mu_{1}}{\sigma_{1}}\right)dx
\]
For $a=2$, we can evaluate this integral explicitly
\begin{align*}
  & \int_{\mu_{2}}^{\infty}\frac{1}{\sigma_{2}}
    \phi\left(\frac{x-\mu_{2}}{\sigma_{2}}\right)\frac{1}{\sigma_{1}}
    \phi\left(\frac{x-\mu_{1}}{\sigma_{1}}\right)dx\\
  & =\frac{1}{2\sqrt{2\pi(\sigma_{1}^{2}+\sigma_{2}^{2})}}
    e^{-\frac{(\mu_{1}-\mu_{2})^{2}}{2(\sigma_{1}^{2}+\sigma_{2}^{2})}}
    \left(1+\text{erf}\left(\frac{(\mu_{1}-\mu_{2})\sigma_{2}}
    {\sqrt{2}\sigma_{1}\sqrt{\sigma_{1}^{2}+\sigma_{2}^{2}}}
    \right)\right).
\end{align*}
Note that for positive values of the error function it is bounded below by zero. This gives 
\[
  \mathbb{E}\left[\frac{1}{\sigma_{a}}
    \phi\left(\frac{\theta_{a}^{*}-\mu_{a}}{\sigma_{a}}\right)\right]
  \geq\left(\frac{1}{2}\right)^{K-2}
  \frac{1}{2\sqrt{2\pi(\sigma_{1}^{2}+\sigma_{2}^{2})}}
  e^{-\frac{(\mu_{1}-\mu_{2})^{2}}{2(\sigma_{1}^{2}+\sigma_{2}^{2})}}.
\]
Repeating these steps for $a=1$ (taking $a'=2$) results in the same lower
bound.
\end{proof}


%\subsection{Proof of Theorem~\ref{theorem:dts-asymptotic}}
%\label{section:proof-dts-asymptotic}
%
%We consider the improper prior setting, in which the experimenter begins with a flat prior $\mu_{a}=0, \sigma_{a}=\infty$. 
%Recall that the Density Thompson Sampling at state $(\mu_{t},\sigma_{t})$ samples arms with probabilities:
%\[
%\pi_{a}^{\text{DTS}}(\mu_{t},\sigma_{t})=
%\frac{s_{a}\left(\E \left[\frac{1}{\sigma_{t,a}}\phi\left(\frac{\theta_{t,a}^{*}-\mu_{t,a}}{\sigma_{t,a}}\right)\right]\right)^{1/2}}
%{\sum_{b}s_{b}\left(\E\left[\frac{1}{\sigma_{t,b}}\phi\left(\frac{\theta_{t,b}^{*}-\mu_{t,b}}{\sigma_{t,b}}\right)\right]\right)^{1/2}}
%\]
%where $\theta_{t,a}^{*}=\max_{a'\neq a}\mu_{'t,a}+\sigma_{t,a'}Z_{a'}$.
%As a shorthand, let $\pi_{t,a} := \pi^{\text{DTS}}(\mu_{t}, \sigma_{t})$, 
%and let $\Psi_{t,a}=\sum_{s=1}^{t}\pi_{s,a}$ be the total sampling effort allocated to arm $a$ by time $t$.
%
%By the Gumbel-max trick, the sampling policy is equivalent to:
%\[
%\pi_{a}^{\text{DTS}}(\mu_{t},\sigma_{t})=\mathbb{P}\left(a=\arg\max_{b}\left\{ \log s_{b}\left(\mathbb{E}\left[\frac{1}{\sigma_{t,b}}\phi\left(\frac{\theta_{t,b}^{*}-\mu_{t,b}}{\sigma_{t,b}}\right)\right]\right)^{1/2}+\xi_{b}\right\} \right)
%\]
%where $\xi_{b}$ are independent $Gumbel(0,1)$ random variables. 
%Now, we consider the modified DTS policy that randomly draws an arm from $\pi^{\text{DTS}}$ and assigns all sampling effort
%to that arm.
%\[
%I_{a}^{\text{DTS}}(\mu,\sigma)=1\left\{ a=\arg\max_{b}\left\{ \log s_{b}\left(\mathbb{E}\left[\frac{1}{\sigma_{t,b}}\phi\left(\frac{\theta_{t,b}^{*}-\mu_{t,b}}{\sigma_{t,b}}\right)\right]\right)^{1/2}+\xi_{b}\right\} \right\}.
%\]
%As a shorthand let $I_{t,a} := I_{a}^{\text{DTS}}(\mu_{t}, \sigma_{t})$, 
%and we denote $N_{t,a}=\sum_{s=1}^{t}I_{s,a}$ as the realized total sampling effort to arm $a$ by time $t$
%under this alternative policy. Under this policy, we have by Bayes rule:
%\[
%\mu_{t,a}=\frac{1}{N_{t,a}}\sum_{s=1}^{t}I_{s,a}X_{s,a},\quad\sigma_{t,a}^{2}=\frac{s_{a}^{2}}{N_{t,a}}
%\]
%where $X_{s,a} \sim N(h_{a}, s_{a}^{2})$.
%
%We use a key result from~\citet{Russo20}, which establishes appropriate law of large numbers for martingale sums.
%Crucially, it implies that  $N_{t,a}$ and $\Psi_{t,a}$ will be close together as long as $\Psi_{t,a}\to\infty$. 
%For concreteness, let $\mathcal{F}_{t} = \sigma(\mu_{t}, \sigma_{t}, I_{t-1}, \ldots, \mu_{0}, \sigma_{0})$
%denote the filtration up to $t$.
%
%\begin{lemma}[\citet{Russo20}]
%  \label{lemma:mrtngle-conv}
%Let $Y_{t}$ be an iid sequence of real-valued random variables with finite variance 
%and let $\{X_{t}\}$ be a sequence of binary random variables with 
%$\E[X_{t}]=Z_{t}$. 
%If conditioning on the filtration $\mathcal{F}_{t-1}$, each $Y_{t}$ is independent from $X_{t}$, then with probability 1:
%\[
%  \begin{aligned}
%\lim_{t\to\infty}\sum_{s=1}^{t}Z_{s}	&=\infty\implies\lim_{t\to\infty}\frac{\sum_{s=1}^{t}X_{s}Y_{s}}{\sum_{s=1}^{t}Z_{s}}=\mathbb{E}[Y_{1}] \\
%\lim_{t\to\infty}\sum_{s=1}^{t}Z_{s}	&<\infty\implies\sup_{t\in\mathbb{N}}\left|\sum_{s=1}^{t}X_{s}Y_{s}\right|<\infty
%  \end{aligned}
%\]
%\end{lemma}
%
%Notably this implies that:
%\[
%  \begin{aligned}
%  \lim_{t\to \infty} \Psi_{t,a} = \infty &\implies \lim_{t\to \infty} N_{t,a} = \infty 
%  \text{ and } \lim_{t\to \infty} \frac{N_{t,a}}{\Psi_{t,a}} = 1 \\
%  \lim_{t\to \infty} \Psi_{t,a} < \infty &\implies \sup_{t,a} N_{t,a} < \infty 
%  \text{ and } \sup_{t,a}|\mu_{t,a}| < \infty \\
%  \end{aligned}
%\]
%
%We can use this to prove the following:
%\begin{lemma}
%  For all arms $a$, $\lim_{t\to\infty} \Psi_{t,a} = \infty$.
%\end{lemma}
%
%\begin{proof-of-lemma}
%Consider a sample path $\omega$, and suppose this does not hold for some $a$. 
%  Since $\Psi_{t,a}(\omega)$ is monotonic, there exists a limit $\Psi_{\infty,a}(\omega)=\lim_{t\to\infty}\Psi_{t,a}$.
%  By the pigeonhole principle, since $\sum_{a}\Psi_{t,a}=1$ at every $t$, 
%  there must be at least 1 arm $b$ such that $\Psi_{t,b}\to\infty$. Note that
%\[
%\frac{\pi_{t,a}}{\pi_{t,b}}=\frac{s_{a}\left(\mathbb{E}\left[\frac{1}{\sigma_{t,a}}\phi\left(\frac{\theta_{t,a}^{*}-\mu_{t,a}}{\sigma_{t,a}}\right)\right]\right)^{1/2}}
%{s_{b}\left(\mathbb{E}\left[\frac{1}{\sigma_{t,b}}\phi\left(\frac{\theta_{t,b}^{*}-\mu_{t,b}}{\sigma_{t,b}}\right)\right]\right)^{1/2}}
%\]
%Lemma~\ref{lemma:mrtngle-conv} implies that as $t\to\infty, \mu_{t,b}\to h_{b}$ almost surely, which implies 
%$\theta_{t,b}^{*}\to h_{b}^{*}$. Since $\Psi_{t,b}\to\infty$, $N_{t,b}\to\infty$. 
%Thus,
%\[
%s_{b}\left(\mathbb{E}\left[\frac{1}{\sigma_{t,b}}\phi\left(\frac{\theta_{t,b}^{*}-\mu_{t,b}}{\sigma_{t,b}}\right)\right]\right)^{1/2}=s_{b}\left(\mathbb{E}\left[\sqrt{N_{t,b}}e^{-N_{t,b}\left(\frac{\theta_{t,b}^{*}-\mu_{t,b}}{s_{b}}\right)^{2}}\right]\right)^{1/2}\to0
%\]
%almost surely as well. 
%On the other hand, since $\lim_{t\to\infty}\Psi_{t,a}<\infty$, then Lemma~\ref{lemma:mrtngle-conv} implies that 
%$N_{t,a},\mu_{t,a}$ and $\theta_{t,a}^{*}$ will be bounded for all $t$ almost surely, which means that for $\omega$, there exists $\epsilon$ such that:
%
%\[
%\inf_{t \in \mathbb{N}}s_{a}\left(\mathbb{E}\left[\sqrt{N_{t,a}}e^{-N_{t,a}\left(\frac{\theta_{t,a}^{*}-\mu_{t,a}}{s_{a}}\right)^{2}}\right]\right)^{1/2}
%>\epsilon
%\]
%This means there exists $T_{M}$ large enough such that for all $t>T_{M}$, $\frac{\pi_{t,a}}{\pi_{t,a'}}>M$, which is a contradiction of $\lim_{t\to\infty}\Psi_{t,a}<\infty$.
%\end{proof-of-lemma}
%
%This implies that it is sufficient to track 
%$N_{t,a}$ in order to measure $\Psi_{t,a}$. 
%We define the index  $\nu_{t,a}:=\nu_{a}(N_{t,a})$ to be the quantity
%$I_{t,a} := 1\{a = \argmax_{b} \nu_{b}(N_{t,b})\}$.
%
%\[
%\nu_{a}(N_{t,a})=\log s_{a}+\frac{1}{4}\log N_{t,a}+\frac{1}{2}\log\mathbb{E}\left[e^{-N_{t,a}\left(\frac{\theta_{t,a}^{*}-\mu_{t,a}}{s_{a}}\right)^{2}}\right]+\xi_{a}
%\]



% %%% Local Variables:
% %%% mode: latex
% %%% TeX-master: "main"
% %%% End:
