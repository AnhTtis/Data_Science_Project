\section{Algorithm design through empirical benchmarking}
\label{section:algorithms-adp}


We now derive algorithms for solving the limit Bayesian dynamic
program~\eqref{eqn:bayesian-dp}, which arise naturally from reinforcement
learning (RL) and approximate dynamic programming (ADP).  Breaking from the
typical theory-driven paradigm in bandit algorithms that compare regret
bounds, we empirically benchmark adaptive experimentation methodology to
assess their performance. To ensure \emph{empirical rigor}, we present a
comprehensive set of experiments in the following Streamlit app:
% \vspace{0.3cm}
% \centerline{\begin{minipage}[t]{10cm}
%   \url{https://ethche-aes-view-kfyu7r.streamlit.app/}
% \end{minipage}}
\begin{equation}
  \label{eqn:streamlit}
    \mbox{\url{https://aes-batch.streamlit.app/}}.
\end{equation}
Our empirical approach allows us to study performance among a large class of
algorithms, and analyze factors that impact performance (e.g.  measurement
noise $s_a^2$, horizons length $T$). Our focus on empirical analysis reveals
practical aspects of implementation that are critical for performance, but are
difficult to observe from a purely theoretical approach.

\subsection{Adaptive experimentation as approximate dynamic programming}

 Since the states
$(\mu_t, \sigma_t)$ and actions $\pi_t$ are both continuous, using dynamic
programming to directly solve the policy optimization
problem~\eqref{eqn:bayesian-dp} is computationally intractable even for a
moderate number of arms and reallocation epochs. However, a key property of
this MDP is that the state transitions are differentiable with respect to the
sampling allocations along any sample path $Z_0,...,Z_{T-1}$, which is enabled
by the reparameterization trick for Gaussian random
variables~\cite{KingmaBa15}. As a result, standard Monte Carlo approximations
of the value function are differentiable with respect to the sampling
allocations, allowing the use of gradient-based methods for planning and
(approximately) solving the DP. In this section, we explore algorithms that
utilize this auto-differentiability~\cite{TensorFlow15, PyTorch19}.

\paragraph{Residual Horizon Optimization (RHO)} We propose a simple yet
effective method that solves a computationally cheaper approximation of the
dynamic program based on model predictive control (MPC). $\algo$ iteratively solves an open-loop planning
problem, optimizing over a sequence of future sampling allocations based on currently
available information $(\mu_t, \sigma_t)$ at time $t$. At each epoch $t$, the
allocation $\rho_t(\mu_t, \sigma_t)$ is derived by assuming that a fixed
sequence of allocations will be used for the remaining periods horizon
regardless of new information obtained in the future
\begin{align}
  \label{eqn:rho-planning}
  \rho_{t}(\mu_t,\sigma_t)
  \in  \argmax_{\bar{\rho}_{t},\ldots,\bar{\rho}_{T-1} \in \Delta_\numarm}
    ~\E_{t} \left[ \max_{a} \left\{ \mu_{t,a}
    + \sum_{v=t}^{T-1} \sigma_{v,a}\sqrt{\frac{b_v \bar{\rho}_{v,a}\sigma_{v, a}^{2}}
    {s_a^{2}+b_v \bar{\rho}_{v,a}\sigma_{v, a}^{2}}}Z_{v, a}
    \right\}~~\Bigg|~~ \mu_{t},\sigma_{t} \right],
\end{align}
where $\rho_{t}(\mu_{t}, \sigma_{t}) = \rho_{t}^{*}$ for the sequence $\rho_{t}^{*},\ldots,\rho_{T-1}^{*}$
that maximizes the planning objective.



\begin{algorithm}[t]
  \caption{\label{alg:rho} $\algofull$}
  \begin{algorithmic}[1]
    \State \textsc{Input: prior mean and variance on average rewards $(\mu_0, \sigma_0)$}
    \State Initialize pre-limit states $(\mu_{n, 0}, \sigma_{n, 0}) = (\mu_0, \sigma_0)$
    \For{each epoch $t \in 0, \ldots, T-1$}
    \State Letting $\bar{b}_t \defeq \sum_{v=t}^{T-1} b_v$, solve the following (e.g., using stochastic gradient methods)
    \begin{equation}
      \label{eqn:rho}
      \rho_{t}(\mu_t,\sigma_t) \in
      \argmax_{\bar{\rho} \in \Delta_\numarm}
      \left\{ V^{\bar{\rho}}_{t}(\mu_{t},\sigma_{t})
        \defeq \E_{t}  \left[ \max_{a} \left\{ \mu_{t,a}
            + \sqrt{\frac{\sigma_{t, a}^4 \bar{\rho}_{a} \bar{b}_{t}}
              {s_a^2 + \sigma_{t, a}^2 \bar{\rho}_{a} \bar{b}_{t}}} Z_{t, a}
          \right\} \right] \right\}
    \end{equation}
    \State For each unit $j =1, \ldots, b_t n$,
    sample arms according to $\rho_{t}(\mu_t,\sigma_t)$ 
    and observe reward $R^t_{a, j}$ if  arm $a$ was sampled ($\xi_{a, j}^t = 1$)
    \State Use aggregate rewards 
    $\sqrt{n} \bar{R}_{t, a}^{n} = \frac{1}{b_{t}\sqrt{n}}
    \sum_{j=1}^{b_{t}n}\xi_{a,j}^{t} R^{t}_{a,j}$ and  the
    formula~\eqref{eqn:pre-limit-posterior} to compute the next
    state transition $(\mu_{n, t+1}, \sigma_{n, t+1})$ 
    \EndFor 
    \State \Return $\argmax_{a \in [\numarm]} \mu_{n, T, a}$
  \end{algorithmic}
\end{algorithm}

The planning problem~\eqref{eqn:rho-planning} can be simplified to consider a
constant allocation $\bar{\rho}\in \Delta_{K}$ to be deployed for every
remaining period.  
% For any fixed sequence of allocations that only depend on
% currently available information, the value function over remaining horizon is
% equivalent to a modified one-step lookahead value function.  
Intuitively,
since the allocation does not change as the experiment progresses, we can
accumulate updates to the posterior beliefs at each epoch and think of it as a
single update to the current belief, resulting in the terminal posterior
belief.  In this sense, the policy can be seen as a dynamic extension
of the single-batch problem studied in~\cite{FrazierPo10}. We summarize how our procedure will be applied to finite batch
problems in Algorithm~\ref{alg:rho}, and formalize this insight in the below
result. See Section~\ref{section:proof-rho-reduction} for its proof. 
\begin{lemma}
  \label{lemma:rho-reduction}
  Let $\bar{b}_t \defeq \sum_{v=t}^{T-1} b_v$. For any sequence of future
  allocations $\bar{\rho}_{t},\cdots,\bar{\rho}_{T-1} \in \Delta_\numarm$ that only
  depends on $(\mu_t,\sigma_t)$, there is a constant allocation
  $\bar{\rho}(\mu_t, \sigma_t)$ achieving the same Bayes simple regret 
  \begin{equation*}
    V^{\bar{\rho}_{t:T-1}}_{t}(\mu_{t},\sigma_{t}) = V^{\bar{\rho}}_{t}(\mu_{t},\sigma_{t})
    ~~\mbox{where}~~V^{\bar{\rho}}_{t}(\mu_{t},\sigma_{t})~\mbox{is defined in Eq.~}\eqref{eqn:rho}.
  \end{equation*}
  Thus, it is sufficient to solve the constant allocation planning
  problem~\eqref{eqn:rho} to achieve the same optimal value as the original
  problem~\eqref{eqn:rho-planning}.
  % \begin{equation}
  %   \label{eqn:rho}
  %   \maximize_{\bar{\rho} \in \Delta_\numarm}
  %   \left\{ V^{\bar{\rho}}_{t}(\mu_{t},\sigma_{t})
  %     =  \E_{t} \left[ \max_{a} \left\{ \mu_{t,a}
  %         + \sqrt{\frac{\sigma_{t, a}^4 \bar{\rho}_{a} \bar{b}_{t}}
  %           {s_a^2 + \sigma_{t, a}^2 \bar{\rho}_{a} \bar{b}_{t}}} Z_{t, a}
  %       \right\} \right] \right\}.
  % \end{equation}
\end{lemma}

In the empirical benchmarks to come, we find that among Bayesian policies that
utilize batch Gaussian approximations, $\algofull$ achieves the largest
performance gain across a wide variety of settings. When the number of
reallocation epochs is small, $\algo$ calibrates the level of exploration to
the time horizon by iteratively planning with the Gaussian sequential
experiment~\eqref{eqn:rho}.


% Instead of computing $\rho_t(\mu_t, \sigma_t)$ online as the experiment
% progresses, we can pre-compute the $\rho_t(\cdot, \cdot)$ so that it can be
% readily applied for any observed current state. Our next result reformulates
% the optimization problem~\eqref{eqn:rho} as a stochastic optimization problem
% over functions of $(\mu_t, \sigma_t)$.
% \begin{proposition}
%   \label{prop:loss-min}
%   The solution to the $\algofull$ problem~\eqref{eqn:rho} coincides with the
%   maximizer of the following reward maximization problem over measurable
%   functions $\rho(\mu_t, \sigma_t) \in \Delta^{\numarm}$
%   \begin{equation}
%     \label{eqn:loss-min}
%     \maximize_{\rho(\cdot)~\mbox{\scriptsize measurable}}
%     ~\E_{(\mu_t, \sigma_t) \sim P_t} \left[ \max_{a} \left\{ \mu_{t,a}
%         + \sqrt{\frac{\sigma_{t, a}^4 \rho(\mu_t, \sigma_t) \bar{b}_t}
%         {s_a^2 + \sigma_{t, a}^2 \rho(\mu_t, \sigma_t) \bar{b}_t}} Z_{t, a}
%       \right\} \right],
%   \end{equation}
%   where $P_t$ is any probability measure with support
%   $\R^\numarm \times (0, \sigma_0^2]^\numarm$ and
%   $\bar{b}_t \defeq \sum_{v=t}^{T-1} b_v$.
% \end{proposition}
% \noindent See Section~\ref{section:proof-loss-min} for the proof.
% Proposition~\ref{prop:loss-min} allows us to use standard ML best practices to
% (approximately) solve the reward maximization problem~\eqref{eqn:rho}.
% By parameterizing the policy using black-box ML models,
% one can use stochastic gradient-based optimization algorithms to learn the mapping $\rho_t(\cdot,\cdot)$ in a
% fully offline manner.
% In particular, the model
% training problem~\eqref{eqn:loss-min} can be approximated by neural networks
% and optimized through standard auto-differentiation
% frameworks~\citep{TensorFlow15, PyTorch19} for computing stochastic
% (sub)gradients. 




% \begin{algorithm}[t]
%   \caption{\label{alg:distilled-rho} Distilled $\algofull$}
%   \begin{algorithmic}[]
%     \State \textsc{Input: Policy parameterization
%       $\{\rho_{\theta,t}(\mu_t, \sigma_t): \theta \in \Theta_t\}$, State distribution $P_{t}$} 
%     \For{each epoch $t = 0, \ldots, T-1$}
%     \State Use stochastic gradient-based optimization methods to optimize
%     \begin{equation}
%       \label{eqn:loss-min}
%       \maximize_{\theta \in \Theta_t}
%       ~\E_{(\mu_t, \sigma_t) \sim P_t} \left[ \max_{a} \left\{ \mu_{t,a}
%           + \sqrt{\frac{\sigma_{t, a}^4 \rho_{\theta,t}(\mu_t, \sigma_t) \bar{b}_t}
%           {s_a^2 + \sigma_{t, a}^2 \rho_{\theta,t}(\mu_t, \sigma_t) \bar{b}_t}} Z_{t, a}
%         \right\} \right].
%     \end{equation}
%     \EndFor
%   \end{algorithmic}
% \end{algorithm}

% \paragraph{Distilled Residual Horizon Optimization} Instead of computing
% $\rho_t(\mu_t, \sigma_t)$ as the experiment progresses, it can sometimes be
% convenient to pre-compute the mapping $\rho_t(\cdot, \cdot)$ so that it can be
% readily applied for any observed current state.  By reformulating the
% optimization problem~\eqref{eqn:rho} as a stochastic optimization problem over
% \emph{functions of $(\mu_t, \sigma_t)$}, we propose a \emph{distilled} variant
% of the $\algo$ policy that can learn the mapping $\rho_t(\cdot,\cdot)$ in a
% fully offline manner.
% \begin{proposition}
%   \label{prop:loss-min}
%   The solution to the $\algofull$ problem~\eqref{eqn:rho} coincides with the
%   maximizer of the following reward maximization problem over measurable
%   functions $\rho(\mu_t, \sigma_t) \in \Delta^{\numarm}$
%   \begin{equation}
%     \label{eqn:loss-min}
%     \maximize_{\rho(\cdot)~\mbox{\scriptsize measurable}}
%     ~\E_{(\mu_t, \sigma_t) \sim P_t} \left[ \max_{a} \left\{ \mu_{t,a}
%         + \sqrt{\frac{\sigma_{t, a}^4 \rho(\mu_t, \sigma_t) \bar{b}_t}
%         {s_a^2 + \sigma_{t, a}^2 \rho(\mu_t, \sigma_t) \bar{b}_t}} Z_{t, a}
%       \right\} \right],
%   \end{equation}
%   where $P_t$ is any probability measure with support
%   $\R^\numarm \times (0, \sigma_0^2]^\numarm$ and
%   $\bar{b}_t \defeq \sum_{v=t}^{T-1} b_v$.
% \end{proposition}
% \noindent See Section~\ref{section:proof-loss-min} for the proof. 


% Proposition~\ref{prop:loss-min} allows us to use standard ML best practices to
% (approximately) solve the reward maximization problem~\eqref{eqn:rho}. We
% parameterize the policy using black-box ML models
% \begin{equation*}
%   \left\{\rho_{\theta,t}(\mu_t, \sigma_t): \theta \in \Theta_t\right\},
% \end{equation*}
% and use stochastic gradient-based optimization algorithms to solve for the
% optimal parameterization (Algorithm~\ref{alg:rho}). In particular, the model
% training problem~\eqref{eqn:loss-min} can be approximated by neural networks
% and optimized through standard auto-differentiation
% frameworks~\cite{TensorFlow15, PyTorch19} for computing stochastic
% (sub)gradients. 
% By the envelope theorem, the stochastic (sub)gradient at the
% sample $(\mu_t, \sigma_t) \sim P_t$ is given by
% \begin{equation*}
%   Z_{t, a\opt} ~\partial_{\theta} \left\{
%     \sqrt{\frac{\sigma_{t, a\opt}^4  \rho_{\theta,t} (\mu_t, \sigma_t) \bar{b}_t}
%     {s_{a\opt}^2 + \sigma_{t, a\opt}^2 \rho_{\theta,t}(\mu_t, \sigma_t) \bar{b}_t}}
%   \right\},
% \end{equation*}
% where
% $a\opt \in \argmax_{a} \left\{ \mu_{t,a} + \sqrt{\frac{\sigma_{t, a}^4
%       \rho_{\theta,t}(\mu_t, \sigma_t) \bar{b}_t} {s_a^2 + \sigma_{t, a}^2
%       \rho_{\theta,t}(\mu_t, \sigma_t) \bar{b}_t}} Z_{t, a} \right\}$. 


% We provide concrete implementation details for Algorithm~\ref{alg:rho} in
% Section~\ref{section:implementation}. \hn{@Ethan: Fill in details in this
%   section. Discuss model architecture, training method etc.}



% (Proposition \ref{prop:dts}, Section \ref{section:proof-dts})



% We again solve for the $Q$-myopic policy through stochastic approximation
% methods.  
% \begin{equation}
% \label{eqn:q-myopic}
% Q_{t}^{\bar{\pi}}(\mu_{t},\sigma_{t})
% = \E_{t}^{\bar{\pi}} [\max_a \mu_{T,a} ]
% = \E_t^{\pi} \left[ \max_{a} \left\{ \mu_{t,a}
%     + \frac{\sigma_{t, a}^4 \bar{\pi}_{t, a} B_t}{s_a^2 + \sigma_{t, a}^2 \bar{\pi}_{t, a} B_t} Z_t
%   \right\} \right]
% \end{equation}
% where $B_t = \sum_{v=t}^{T-1} b_v$.

\paragraph{Pathwise Policy Iteration} We also consider reinforcement
learning/approximate DP methods for solving the full horizon
problem~\eqref{eqn:bayesian-dp}. 
Our framework enables the use of policy iteration~\citep{SuttonBa18}
to improve upon a standard sampling policy, such as Thompson Sampling or Top-Two Thompson Sampling.
Policy iteration (also referred to as `Rollout') takes a base policy $\pi$
and returns an improved policy $\pi'$ that selects the sampling allocation at each epoch $t$ that maximizes
the $Q$-function of $\pi$
\begin{equation}
  \label{eqn:policy-iteration}
  \pi'_{t}(\mu_{t}, \sigma_{t}) \in \argmax_{\bar{\rho}\in \Delta_{K}} Q_{t}^{\pi}(\bar{\rho}; \mu_{t}, \sigma_{t})
\end{equation}
where the $Q$-function gives the expected reward of selecting sampling allocation $\bar{\rho}$ at state $(\mu_{t}, \sigma_{t})$
and epoch $t$ given that all future sampling allocations are determined by the base policy,
\[
  Q_{t}^{\pi}(\bar{\rho}; \mu_{t}, \sigma_{t})
  = \E_{t} \left[ V_{t+1}^{\pi}\left(\mu_{t} + \sigma_{t}\sqrt{\frac{b_t \bar{\rho}
  \sigma_{t}^{2}}{s^{2}+b_t\bar{\rho}\sigma_{t}^{2}}} Z_{t}, 
  (\sigma_{t}^{-2} + s^{-2} b_t \bar{\rho})^{-1} \right) \right].
\]


Solving this inner problem would typically require discretizing the state and
action spaces or using function approximation for the Q-function. However,
since the dynamics of the MDP are differentiable, one can simply draw a Monte
Carlo sample of the $Q$-function and directly compute the gradient with
respect to $\bar{\rho}$ through auto-differentiation~\cite{TensorFlow15, PyTorch19}, as long
as the base policy $\pi$ is differentiable with respect to the state
$(\mu, \sigma)$.  One can then use stochastic gradient ascent to solve
\eqref{eqn:policy-iteration}.  Although differentiability is not guaranteed
for all policies, we observe there is often reliable differentiable
surrogates. For example, for Thompson Sampling we have the following
approximation
\[
  \begin{aligned}
\pi^{\text{TS}}(\mu, \sigma) 
= \E\left[\textsf{onehot}(\argmax_{a} \mu_{a} + \sigma_{a}Z_{a})\right]
\approx \E\left[\textsf{softmax}(\mu_{a} + \sigma_{a}Z_{a})\right] =: \hat{\pi}^{\text{TS}}(\mu, \sigma)
\end{aligned}
\]
where $Z_{a} \simiid N(0, 1)$, $\textsf{onehot}$ maps an index to its
corresponding one-hot vector, and
$\textsf{softmax}(v) \defeq \exp(v_{a})/\sum_{a'} \exp(v_{a'})$. We define the
policy TS$+$ to be policy iteration on the approximate Thompson sampling
policy, in which at every state $(\mu_{t}, \sigma_{t})$ the allocation is
determined by solving
\begin{equation}
  \label{eqn:TS-plus}
  \pi^{\text{TS}+}_{t}(\mu_{t}, \sigma_{t}) \in \argmax_{\bar{\rho}\in \Delta_{K}} Q_{t}^{\hat{\pi}^{\text{TS}}}(\bar{\rho}; \mu_{t}, \sigma_{t})
\end{equation}
with stochastic gradient ascent.

\paragraph{Pathwise Policy Gradient} We apply policy gradient (PG) computed through
black-box ML models, as well as their limited lookahead
variants~\citep{SuttonBa18}. We consider an differentiable parameterized
policy $\pi_\theta = \{\pi_{t}^{\theta} \}_{t=0}^{T-1}$ (e.g., neural
networks) and aim to directly optimize the value function~\eqref{eqn:q_fn} using
stochastic approximation methods. We use stochastic gradient ascent over
sample paths $Z_0, \ldots, Z_{T-1}$ to update policy parameters $\theta$:
\begin{equation}
  \label{eqn:policy-gradient}
  \theta \leftarrow \theta + \alpha \nabla_{\theta} V_{0}^{\pi_{\theta}}(\mu_{0},\sigma_{0}),
\end{equation}
for step-size $\alpha$ and prior $(\mu_{0}, \sigma_{0})$.
As for policy iteration, that the gradients of the value function can be computed \emph{exactly}
through auto-differentiation along every sample path.
% , which is enabled
% by the reparameterization trick for Gaussian random variables \citep{KingmaWe14}. 
This is in contrast with standard RL methods 
for continuous state and action spaces,
such as Deep Deterministic Policy Gradient (DDPG) \citep{LillicrapEtAl16},
that require function approximation for the Q-functions, a step
which can introduce significant approximation errors.

For long horizons, both policy iteration and training a policy with gradient descent are more
difficult due to vanishing
gradients. Policies are evaluated solely on the simple regret incurred at the
end of the experiment, which makes it difficult to do credit assignment for
sampling allocations near the beginning of the experiment.  For policy gradient, we consider
$m$-lookahead policies (denoted PG-$m$) trained to optimize the shorter time
horizon objective $V_{T-m}^{\pi_\theta}(\mu_{0},\sigma_{0})$. 

%In the next
% section, we empirically find that policy gradient and policy iteration achieve very strong
% performance, equivalent to that of $\algo$, for small problems involving a
% limited number of treatments and reallocation epochs. Yet training the policy
% becomes more difficult in larger-scale experiments, and performance degrades.
% We expect though that these methods will improve in more complex
% settings, i.e. those involving utility functions that depend on performance
% during the experiment or settings that allow adaptive batch sizes.  In this
% sense, $\algo$ strikes a good balance between incorporating planning
% capabilities of the Gaussian sequential experiment while maintaining low
% operational and optimization complexity.


\subsection{Empirical comparison of Gaussian batch policies}

There are many policies that use the Gaussian sequential experiment
(Definition~\ref{def:gse}) as an approximation for batch rewards.  To identify
performant policies among them, we turn to empirical benchmarking on realistic
examples.  

\paragraph{Setup}
Since we are interested in the performance of these policies in the pre-limit
problem, we simulate state transitions~\eqref{eqn:pre-limit-posterior} based
on observed aggregate rewards $\sqrt{n} \bar{R}_{t, a}^{n}$.  As a concrete
illustration of our main findings, consider a setting with $K = \{10, 100 \}$
arms with up to $T=10$ batches. All batches have the same number of
observations in each epoch and we consider two batch sizes: $b_{t}n = 100$
samples and $b_{t}n = 10,000$ observations per epoch. We evaluate each policy
using the Bayes simple regret~\eqref{eqn:bsr} under the true prior. For
policies that use the Gaussian sequential experiment, we train them by
approximating the true prior with a Gaussian prior with the same mean and
standard deviation in order to preserve conjugacy and utilize the MDP
formulation~\eqref{eqn:dynamics}. All policies are evaluated according to the
pre-limit finite batch problem under the true prior.

To test the finite batch performance of Bayesian policies that use Gaussian
approximations, we consider two data-generating distributions. We use the
$\textsf{Beta-Bernoulli experiment}$ as our primary vehicle, where rewards for
each arm are drawn from independent $\textsf{Bernoulli}(\theta_{a})$
distributions. Here, the parameters $\theta_{a}$ are drawn independently from
a known $\textsf{Beta}(\alpha,\beta)$ prior.  For each batch size
$b_{t}n = 100$ or $10,000$, we scale the prior parameters
$\alpha = \beta = b_{t}n$ to preserve the difficulty. Concretely, when
$b_{t}n = 100$, the prior mean for each parameter $\theta_{a}$ is 0.5 and the
prior standard deviation is $\approx 0.03$, so typically each treatment arm
has an average reward of $0.5 \pm 0.03$. We focus on small differences in
average rewards as this is often the case in real-world
experiments~\cite{KohaviLoSoHe09, KohaviDeFrLoWaXu12, KohaviDeFrWaXuPo13}.


Since we cannot vary the measurement noise
$s^{2}_a = \var(\epsilon_a)$~\eqref{eqn:rewards} under the
$\textsf{Beta-Bernoulli experiment}$, we also consider an alternative setting
which we call the $\textsf{Gamma-Gumbel experiment}$.  Here, rewards for each
arm are drawn from independent $\textsf{Gumbel}(\mu_{a}, \beta)$
distributions, with a known, fixed scale parameter $\beta$ that determines the
measurement variance $s^2_a = \frac{\pi^2}{6}\beta$.  The location parameters
$\mu_{a}$ are drawn independently from a known $\textsf{Gamma}(\alpha,\beta)$
prior, which determines the differences in the average reward between the
arms.  For each batch size $b_{t}n$, we set $\alpha = b_{t}n$ and
$\beta = 1/b_{t}n$ to preserve the difficulty of the problem as measured by
the gap between average rewards.




\paragraph{Algorithms} In addition to the policies mentioned in
Section~\ref{section:algorithms-mdp} that use the dynamic programming
structure of the model, we also implement policies such as Gaussian Thompson
Sampling which are natural adaptations of standard bandit policies to the
Gaussian sequential experiment. We summarize the the list of methods we
compare below.
% Our discussion so far highlights $\algo$ as an effective iterative planning
% policy. Since $\algo$ is but one policy that use the Gaussian sequential
% experiment (Definition~\ref{def:gse}) as an approximation for batch rewards,
% we now consider alternative Bayesian policies that also maintain Gaussian posterior
% states over the \emph{average rewards}.
\begin{itemize}[itemsep=0pt]
\item $\algofull$: Solves the planning problem \eqref{eqn:rho} as in
  Algorithm \ref{alg:rho}
\item Gaussian Limit Thompson Sampling: TS policy for the Gaussian sequential
  experiment
\item Gaussian Limit Top-Two Thompson Sampling: Top-Two TS for the Gaussian
  sequential experiment.
\item Myopic: Maximizes one-step lookahead value function for the
  problem~\eqref{eqn:bayesian-dp}; a randomized version of the Knowledge
  Gradient method
\item TS$+$: Policy iteration on the approximate TS
  policy~\eqref{eqn:TS-plus}
\item Policy Gradient: Heuristically solves the dynamic
  program~\eqref{eqn:bayesian-dp} using a policy parameterized by a
  feed-forward neural network; trained through policy
  gradient~\eqref{eqn:policy-gradient} with episode length 5
\end{itemize}

\begin{figure}[t]
  \vspace{-1.6cm}
  \centering
  \hspace{-1.6cm}
  \subfloat[\centering Measurement variance $s_{a}^{2} = 1$.]{\label{fig:gse_bsr}{\includegraphics[height=5.2cm]{./fig/alt_reg_Gumbel_10000_100_1.0_Flat.png} }}%
  % \hspace{.05cm}
  \subfloat[\centering Measurement variance $s_{a}^{2} \in \{0.2, 1, 5\}$ for $T = 10$]{\label{fig:gse_var}{\includegraphics[height=5.2cm]{./fig/alt_gse_bar_var_Gumbel_10000_100_Flat.png}} }%
  \vspace{.4cm}
  \caption{\label{fig:gse_compare} Comparison of Gaussian batch policies.
    Relative gains over the uniform allocation as measured by the Bayes simple
    regret for the finite batch problem with $K = 100$ treatment arms and
    batches of size $b_{t} n = 10,000$.  $\textsf{Gamma-Gumbel experiment}$
    where individual rewards are Gumbel with a $\mbox{Gamma}$ prior. $\algo$
    maintains strong performance for small and long horizon experiments, as
    well as for low and high noise levels.  Myopic performs well in short
    horizon experiments, but worsens in longer horizon experiments due to
    insufficient exploration. The Thompson sampling policies are effective for
    low measurement noise but their performance degrades in underpowered
    settings. 
    % \hn{@Ethan: Align the plot more to the left. Right plot should
    % have similar x- and y-label font sizes as the left one. Nit:
    % can you make the x-axis ticks horizontal in panel (b)?}
  }
\end{figure}

\paragraph{Overview of empirical results} We refer the reader to the
interactive plot~\eqref{eqn:streamlit} for a comprehensive presentation on our
benchmarking results.  In Figure~\ref{fig:gse_compare}, we provide
representative results where we consider $\numarm = 100$ arms and batches of
size $b_{t} n = 10,000$.  Although all of these policies use Gaussian batch
approximations, we evaluate them in a pre-limit problem with Gumbel
distributed rewards and a Gamma prior on the mean rewards, which we refer to
as the $\textsf{Gamma-Gumbel experiment}$.  Figure~\ref{fig:gse_bsr} focuses
on a fixed measurement variance $(s_{a}^{2} = 1)$ across a range of
reallaction epochs. Figure~\ref{fig:gse_var} compares performance across
different measurement noise levels for a fixed number of reallocations
$T = 10$.


We observe that $\algo$, which uses the Gaussian sequential experiment for
planning, exhibits consistently strong performance across a wide array of
settings; other policies are effective in some instances and less effective in
others. For example, the myopic policy performs well for short horizon
experiments as expected, but worsens in longer horizon settings due to
insufficient exploration.  Thompson sampling, which tends to explore more than
other policies, is an effective heuristic when the noise level is low, but
suffers in more underpowered experiments. In the low noise regime, the
differences in raw performance between adaptive policies tends to be small as
all policies perform well, whereas the gap in raw performance between policies
is wider in underpowered experiments. The policy gradient method achieves
equivalent performance to $\algo$ when there are a small number of
alternatives ($\numarm = 10$) and measurement noise is high.  TS$+$ improves
upon Thompson Sampling in similar settings.  Yet, although these methods solve
a more sophisticated planning problem, which accounts for future adaptivity,
the performance of these methods degrades compared to $\algo$ in settings with
more arms, lower measurement noise, and longer horizon experiments.


\subsection{Discussion of $\algofull$}


\begin{figure}[t]
  \vspace{-.4cm}
  \centering
  %\hspace{-1cm}
  %\includegraphics[height=9cm]{./fig/prob_comparison.png}
  \includegraphics[height=3.8cm]{./fig/prob_comparison_alt.png}


  \vspace{.2cm}
  \caption{\label{fig:rho_explore} Comparison of sampling allocations.  
  (a) displays posterior means $\mu$ and standard deviations
    $\sigma$ (the length of each whisker) for $K = 5$ arms. The other graphs
    illustrate the sampling allocation computed by different policies given
    posterior belief $(\mu, \sigma)$. (b) shows the sampling
    allocation produced by the Gaussian Thompson sampling policy, (c)
    is $\algo$ when there is only 1 epoch left, and (d) is $\algo$ when
    the residual horizon is $T - t = 10$.  $\algo$ calibrates exploration to
    the length of the remaining horizon; when there is only 1 epoch left, the
    policy gives the Bayes optimal allocation focusing solely on the top two
    arms. When there are many reallocation epochs left, the policy explores
    other arms more and resembles the Thompson sampling allocation.  }
\end{figure}

The strong empirical performance of $\algofull$ impels us to carefully study
the advantages of solving the planning problem~\eqref{eqn:rho}.  In this
subsection, we show that $\algo$ enjoys several desirable properties. First
and foremost, implementing this policy only requires solving the above
optimization problem~\eqref{eqn:rho} at the currently observed state, allowing
it to remain adaptive while being computationally efficient; we use stochastic
gradient methods for this purpose. Second, the objective~\eqref{eqn:rho}
encourages the policy to explore more when the remaining horizon is long,
which leads it to explore aggressively early on in the experiment while
focusing on strong alternatives as the experiment winds down (see
Figure~\ref{fig:rho_explore}).

Third, $\algo$ iteratively computes at each epoch the optimal allocation among
those that assume future allocations will only use currently available
information, it is guaranteed to outperform any open-loop allocation including
static designs. In particular, $\algo$ is guaranteed to achieve a smaller
Bayes simple regret than the uniform allocation across any time horizon. This
gives a practical performance guarantee even for small $T$, the regime which
is relevant for most real-world experimental settings, for which the uniform
allocation is a highly competitive policy.
\begin{proposition}
  \label{prop:rho-vs-static}
  Let $\rho_t$ be given by Algorithm~\ref{alg:rho}.  For any policy
  $\bar{\pi}_{t:T} = (\bar{\pi}_t(\mu_t, \sigma_t), \ldots,
  \bar{\pi}_{T-1}(\mu_t, \sigma_t))$ that only depends on currently available
  information $(\mu_t,\sigma_t)$, we have
  $V_{t}^{\rho}(\mu_t,\sigma_t) \geq V_{t}^{\bar{\pi}}(\mu_t,\sigma_t)$.
\end{proposition}
\noindent See Section~\ref{section:proof-rho-vs-static} for the proof.
In cases where it is computationally expensive to compute the allocation 
$\rho_t(\mu_t, \sigma_t)$ as the experiment progresses, one can learn the policy
offline via standard ML methodologies (see Section~\ref{section:distilled}).


% Empirically, we demonstrate in Section~\ref{section:experiments} that this
% iterative planning method (Algorithm~\ref{alg:rho}) provides a highly
% effective heuristic. Motivated by its practical performance, we analyze the
% theoretical properties of Algorithm~\ref{alg:rho} in
% Section~\ref{section:asymptotics}.


Although we are primarily interested in short horizons $T$ in practice, we can
theoretically characterize the behavior of $\algo$ in the infinite horizon
limit $T - t\to \infty$. We show that the optimization problem~\eqref{eqn:rho}
becomes strongly concave for a large enough residual horizon $T-t$, which was
observed for the single-batch problem in~\cite{FrazierPo10}.  Then, by using
the KKT conditions of the optimization problem~\eqref{eqn:rho}, we also
characterize the asymptotic sampling policy $\rho_t$ converges to as $T-t$
grows large. We find that it converges to a novel posterior sampling policy we
denote as Density Thompson Sampling (DTS), as it samples arms proportionally
to the partial derivatives of the Gaussian Thompson Sampling policy.
\begin{proposition}
  \label{prop:asymptotic-rho}
  Consider any fixed epoch $t$ and state $(\mu, \sigma)$.
  Let $\Delta_K^\epsilon = \Delta_K \cap \{p: p \ge \epsilon\}$ be the truncated simplex. 
  Suppose that as $T-t$ grows large, 
  the residual sample budget $\bar{b}_{t} = \sum_{s=t}^{T-1} b_{s}\to \infty$.
  \begin{enumerate}
  \item As $\bar{b}_{t}\to \infty$, the gradient scales as $\nabla_{\bar{\rho}} V^{\bar{\rho}}_{t}(\mu,\sigma)
     \sim c\frac{s_{a}^{2}}{\bar{b}_{t}\bar{\rho}_{a}^{2}}$, where $c$ depends only on $(\mu, \sigma)$.
  \item There exists $T_0$ such that $\forall T - t > T_0$,
  $\bar{\rho} \mapsto \bar{b}_{t}V^{\bar{\rho}}_{t}(\mu,\sigma)$ is strongly
  concave on $\Delta_K^\epsilon$.
  \item Suppose there exists
  $T_1$ such that for $\forall T - t > T_1$, the $\algo$ allocation satisfies
  $\rho_{t,a}(\mu,\sigma) > \epsilon$. Then as $T-t\to \infty$
  \begin{equation*}
    \rho_{t,a}(\mu,\sigma) \to \pi_{a}^{\textup{DTS}}(\mu, \sigma),~\mbox{where}~
    \pi^{\rm DTS}_a(\mu, \sigma) \propto s_a \left[ \frac{\partial}{\partial
        \mu_a}\pi_{a}^{\textup{TS}}(\mu,\sigma) \right]^{1/2}
  \end{equation*}
  where $\pi_{a}^{\text{TS}}(\mu, \sigma):= \P(a^{*} = a|\mu, \sigma)$ is the Thompson Sampling probability.
\end{enumerate}
\end{proposition}
\noindent See Section~\ref{section:proof-asymptotic-rho} for the proof. 

This shows that the planning problem becomes more amenable to optimization as
the residual horizon increases, although the gradient of the objective decays
at rate $1/\bar{b}_{t}$ as $T-t\to \infty$.  Moreover, although we would
expect the open-loop planning problem to be better calibrated under short time
horizons, even in the infinite horizon limit it gives rise to a natural,
stationary posterior sampling policy, establishing a novel connection between
posterior sampling and model predictive control.  The limiting policy DTS is
highly exploring and does not over-sample the best arm, which makes it better
suited for best-arm identification tasks than standard Gaussian Thompson
Sampling (see Section~\ref{section:proof-log-dts} for more details).

In contrast, we empirically observe that the more sophisticated approaches to
planning---policy iteration~\eqref{eqn:TS-plus} and policy gradient---suffer
optimization difficulties due to vanishing gradients. This is alluded to in
Proposition~\ref{prop:asymptotic-rho}: the gradient of the static allocation
value function scales as $(s_{a}^{2}/\bar{b}_{t})$ and thus vanishes under
small measurement variance and long residual horizon.



% \paragraph{Policy Iteration} As further illustration of our MDP framework as a source of algorithmic 
% development for batch adaptive experimentation, we observe that it enables the use of policy iteration
% to improve upon a standard sampling policy, such as Thompson Sampling or Top-Two Thompson Sampling.
% Recall that policy iteration (also referred to as rollout) takes a base policy $\pi$
% and returns an improved policy $\pi'$ that selects the sampling allocation at each epoch $t$ that maximizes
% the $Q$-function of $\pi$
% \begin{equation}
%   \label{eqn:policy-iteration}
%   \pi'_{t}(\mu_{t}, \sigma_{t}) \in \argmax_{\bar{\rho}\in \Delta_{K}} Q_{t}^{\pi}(\bar{\rho}; \mu_{t}, \sigma_{t})
% \end{equation}
% where the $Q$-function gives the expected reward of selecting sampling allocation $\bar{\rho}$ at state $(\mu_{t}, \sigma_{t})$
% and epoch $t$ given that all future sampling allocations are determined by the base policy,
% \[
%   Q_{t}^{\pi}(\bar{\rho}; \mu_{t}, \sigma_{t})
%   = \E_{t} \left[ V_{t+1}^{\pi}\left(\mu_{t} + \sigma_{t}\sqrt{\frac{b_t \bar{\rho}
%   \sigma_{t}^{2}}{s^{2}+b_t\bar{\rho}\sigma_{t}^{2}}} Z_{t}, 
%   (\sigma_{t}^{-2} + s^{-2} b_t \bar{\rho})^{-1} \right) \right].
% \]
% The policy $\pi'$ is guaranteed to have a higher expected reward then the base policy.
% Since the dynamics of the MDP are differentiable, one can obtain gradients of standard Monte Carlo approximations of the $Q$-function
% as long as the base policy $\pi$ is differentiable with respect to the state $(\mu, \sigma)$.
% Although this condition may not directly hold for all standard policies,
% there often exists reliable differentiable surrogates, e.g. for Thompson Sampling we have the following
%  approximation:
% \[
%   \begin{aligned}
% \pi^{\text{TS}}(\mu, \sigma) 
% = \E[\textsf{onehot}(\argmax_{a} \mu_{a} + \sigma_{a}Z_{a})]
% \approx \E[\textsf{softmax}(\mu_{a} + \sigma_{a}Z_{a})]
%   \end{aligned}
% \]
% where $\textsf{onehot}$ maps an index to its corresponding one-hot vector and 
% $\textsf{softmax}(v) \defeq \exp(v_{a})/\sum_{a'} \exp(v_{a'})$.
% Even if a policy is non-differentiable, it is enough to be able to train a differentiable approximation
% (e.g. Distilled $\algo$) in order to have a differentiable $Q$-function.


% With this, the objective in the policy improvement step~\eqref{eqn:policy-iteration} can be  
% directly optimized by standard stochastic gradient-based methods, with gradients computed
% through auto-differentiation~\cite{TensorFlow15, PyTorch19}. This allows policy
% iteration to be performed efficiently despite continuous state and action spaces.


% even carefully tuned variants of these RL approaches
% frequently perform worse than $\algo$ consistently across problem instances.



% We rewrite the asymptotic sequential experiment as a Markov decision process
% to find the optimal adaptive design. Our formulation is based on
% reparameterizing the distribution of sequential observations
% $G_0, \ldots, G_{T-1}$ using posterior mean and variances defined over
% standard normal variables.  Formally, consider an independent Gaussian prior
% over the local parameters, $h_a \sim N(\mu_{0, a},\sigma_{0, a}^{2})$, under
% which the trajectory of posterior beliefs $(\mu_{t, a},\sigma_{t, a}^{2})$
% follows a Markov Decision Process.  Using standard normal variables
% $Z_{t, a} \simiid N(0, 1)$, the ``states'' $\mu_t,\sigma_t^{2}$ and
% ``actions'' $\pi_t \in\Delta_{k}$ characterize the asymptotic sequential
% experiment through the following transitions
% \begin{equation}
%   \label{eqn:dynamics}
%   \mu_{t+1, a} 
%   =\mu_{t, a}+\sigma_{t, a}\sqrt{\frac{b_t \pi_{t, a}\sigma_{t, a}^{2}}{s_a^{2}+b_t\pi_{t, a}\sigma_{t, a}^{2}}}Z_{t, a}
%   ~~~\mbox{and}~~~
%   \sigma_{t+1, a}^{2}
%   =\left(\frac{1}{\sigma_{t, a}^{2}}+\frac{b_t \pi_{t, a}}{s_a^{2}}\right)^{-1}.
% \end{equation}




% Other well-established approaches for adaptive experimentation are Bayesian
% optimization (BO) and Bayesian optimal experimental design (BOED). Relevant to
% our work, there have been many recent works in the Bayesian optimization
% literature on batch and non-myopic adaptive designs in order to exploit
% parallelism and alleviate the myopia of standard acquisition functions.  Our
% setting departs from this literature in that we focus on the multi-armed
% bandit setting with a finite (although possibly large) number of arms. The
% core challenge in our setting is less so the size of the design space and
% rather how to allocate sampling effort under limited statistical power,
% limited extrapolation between treatments, and a finite exploration horizon.



% The dynamic programming (DP) policy $\pi^*$ solves for $\pi^{*}_{t}=\argmax_{\pi} Q_{t}^{\pi}(\mu_{t},\sigma_{t})$ at every time $t$. The $m$-step lookahead policy $\pi^m$ instead solves for $\pi^{*}_{T-m}(\mu_{t},\sigma_{t})$ at every $t$. Our method approximates these policies with parameterized policies $\pi_\theta = \{\pi_{t}^{\theta_t}\}_{t=1}^{T-1}$ (e.g. neural networks).
% We train the policies to maximize sample average approximations of the Q functions. Emulating backward induction, we begin from $t = T-1$ and train policy $\pi_{t}^{\theta_t}$ using stochastic gradient ascent, fixing parameters $\theta_s$ for all $s > t$ corresponding to shallower lookahead policies. Explicitly, at each $t$ we perform the update:

% \begin{equation}
% \label{eqn:update}
% \theta_t \leftarrow \theta_t + \alpha \nabla_{\theta_t}\E[Q_{t}^{\pi_\theta}(\mu_{t},\sigma_{t}) ]
% \end{equation}

% Since the dynamics of the MDP are known explicitly, we can compute these gradients through standard auto-differentiation frameworks. After training these policies, the approximate DP policy $\pi_\theta^*$ selects allocation $\pi_{t}^{\theta_t}(\mu_t,\sigma_t)$ at every $t$ and the approximate $n$-step lookahead $\pi_\theta^m$ selects $\pi_{T-m}^{\theta_{T-m}}(\mu_t,\sigma_t)$.


% Our algorithm is based on a Monte Carlo approximation of the
% $Q$-function and is tailored to allow efficient computation of gradients
% through standard auto-differentiation frameworks. We also propose myopic
% policies with lookahead horizons and empirically validate their performance.
% As we detail below, our asymptotic normal approximation can provide effective
% adaptive designs even when $n$ is small.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
