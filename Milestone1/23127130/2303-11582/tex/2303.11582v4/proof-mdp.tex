\section{Derivations for Bayesian adaptive experiment}
\label{section:proof-bae}
\subsection{Proof of Lemma~\ref{lemma:mdp}}
\label{section:proof-mdp}

In the posterior updates~\eqref{eqn:posterior}, the
result~\eqref{eqn:dynamics} follows by noting
\begin{equation*}
  G_{t, a} \mid \mu_t, \sigma_t \eqd
  \E[G_{t, a} \mid \mu_t, \sigma_t] + \sqrt{\var(G_{t, a} \mid \mu_t, \sigma_t)} Z_{t, a},
\end{equation*}
and plugging in expressions for the conditional mean and variance of
$G_{t, a}$. Instead of this change of variables argument, we give a more
evocative derivation below that shows
\begin{align}
  \label{eqn:marginal-normality}
   \mu_{t+1} \mid \mu_t, \sigma_t
  \sim N\left(\mu_t, \diag(\sigma_t^2 - \sigma_{t+1}^2)\right).
\end{align}

Gaussianity of $ \mu_{t+1} \mid \mu_t, \sigma_t$ follows since $\mu_{t+1}$ is
a linear function of $G_{t, a}$ given $\mu_t, \sigma_t$
\begin{align*}
  \mu_{t+1, a}
  & = \sigma_{t+1, a}^{2}
  \left( \sigma_{t, a}^{-2} \mu_{t, a} + s_a^{-2}b_t G_{t, a} \right) \\
  & = \left(\sigma_{t, a}^{-2} + s_a^{-2} b_t \pi_{t, a}(\mu_t, \sigma_t)\right)^{-1}
  \left( \sigma_{t, a}^{-2} \mu_{t, a} + s_a^{-2}b_t G_{t, a} \right).
\end{align*}
We now derive expressions for the conditional mean and variance of
$\mu_{t+1}$. Noting that
\begin{equation*}
  \E[G_{t, a} \mid \mu_t, \sigma_t]
  = \E[ \E[G_{t, a} \mid h, \mu_t, \sigma_t] \mid \mu_t, \sigma_t]
  = \E[\pi_{t, a}(\mu_t, \sigma_t)h_a \mid \mu_t, \sigma_t] = 
  \pi_{t, a}(\mu_t, \sigma_t)\mu_{t, a},
\end{equation*}
we conclude
\begin{equation*}
  \E[\mu_{t+1, a} \mid \mu_t,\sigma_t] = \sigma_{t+1, a}^2
  \left(\sigma_{t, a}^{-2} \mu_{t, a} + s_a^{-2} b_t \pi_{t, a}(\mu_t, \sigma_t) \mu_{t, a}\right)
  = \mu_{t, a}.
\end{equation*}

Next, use the law of total variance to derive
\begin{align*}
  \var(\mu_{t+1, a} \mid \mu_t, \sigma_t)
  & = \left( \sigma_{t+1, a}^2 s_{a}^{-2}b_{t}\right)^2
  \var(G_{t, a} \mid \mu_t, \sigma_t) \\
  & = \left( \sigma_{t+1, a}^2 s_{a}^{-2}b_{t} \right)^2
  \Big(
  \E[\var(G_{t, a} \mid h, \mu_t, \sigma_t) \mid \mu_t, \sigma_t]
  + \var(\E[G_{t, a} \mid h, \mu_t, \sigma_t] \mid \mu_t, \sigma_t)
  \Big) \\
  & = \left( \sigma_{t+1, a}^2 s_{a}^{-2}b_{t} \right)^2
    \left( \frac{\pi_{t, a}(\mu_t, \sigma_t)s_a^2}{b_t} +\pi_{t, a}(\mu_t, \sigma_t)^{2} \sigma_{t, a}^2\right) 
   = \frac{\sigma_{t, a}^4 b_t \pi_{t, a}(\mu_t, \sigma_t) }
    {s_a^2 + \sigma_{t, a}^2 b_t \pi_{t, a}(\mu_t, \sigma_t)}.
\end{align*}
% \begin{align*}
%   \var(\mu_{t+1, a} \mid \mu_t, \sigma_t)
%   & = \left( \sigma_{t+1, a}^2 \frac{b_t \pi_{t, a}(\mu_t, \sigma_t)}{s_a^2}\right)^2
%   \var(G_{t, a} \mid \mu_t, \sigma_t) \\
%   & = \left( \sigma_{t+1, a}^2 \frac{b_t \pi_{t, a}(\mu_t, \sigma_t)}{s_a^2}\right)^2
%   \Big(
%   \E[\var(G_{t, a} \mid h, \mu_t, \sigma_t) \mid \mu_t, \sigma_t]
%   + \var(\E[G_{t, a} \mid h, \mu_t, \sigma_t] \mid \mu_t, \sigma_t)
%   \Big) \\
%   & = \left( \sigma_{t+1, a}^2 \frac{b_t \pi_{t, a}(\mu_t, \sigma_t)}{s_a^2}\right)^2
%     \left( \frac{s_a^2}{b_t \pi_{t, a}(\mu_t, \sigma_t)} + \sigma_{t, a}^2\right) 
%    = \frac{\sigma_{t, a}^4 b_t \pi_{t, a}(\mu_t, \sigma_t) }
%     {s_a^2 + \sigma_{t, a}^2 b_t \pi_{t, a}(\mu_t, \sigma_t)}.
% \end{align*}
We arrive at the desired result~\eqref{eqn:marginal-normality} since
\begin{equation}
  \label{eqn:var-decrease}
  \sigma_{t, a}^2 - \sigma_{t+1,a}^2 = \sigma_{t, a}^2 - \left(\sigma_{t,
      a}^{-2} + s_a^{-2} b_t \pi_{t, a}(\mu_t, \sigma_t)\right)^{-1}
    = \frac{\sigma_{t, a}^4 b_t \pi_{t, a}(\mu_t, \sigma_t) } {s_a^2 + \sigma_{t,
      a}^2 b_t \pi_{t, a}(\mu_t, \sigma_t)}.
\end{equation}

\subsection{Proof of Corollary~\ref{cor:bayes-limit}}
\label{section:proof-bayes-limit}

The posterior states $(\mu_{n,t}, \sigma_{n,t})$ can be expressed as a
function of the sample mean estimators and the propensity scores
\[
\begin{aligned}
  \sigma_{n,t+1}^{-2} &= \sigma_{0}^{-2} + \sum_{v=0}^{t} b_{v}\pi_{v}(\mu_{n,v}, \sigma_{n,v})s^{-2} \\
  \mu_{n,t+1} &= \sigma_{n,t+1}^{2} \left( \sigma_{0}^{-2}\mu_{0} + \sum_{v=0}^{t} b_{v}s^{-2} \Pstate_{v} \right).
\end{aligned}
\]
where the operations are vector-wise.

Since the allocations $\pi_v$ are assumed to be continuous in the posterior
state $(\mu_{n,v}, \sigma_{n,v})$, the states
$(\mu_{n, t+1}, \sigma_{n, t+1})$ are continuous functions of the sample mean
estimators $\Pstate_v$. By the continuous mapping theorem, we conclude
\[
  (\mu_{n,0}, \sigma_{n,0}, \ldots, \mu_{n,T-1},\sigma_{n,T-1}) \cd (\mu_{0}, \sigma_{0}, \ldots, \mu_{T-1},\sigma_{T-1}).
\]

% Suppose that the policy $\pi_{t}$ is Lipschitz in the posterior state $(\mu, \sigma)$.
% We proceed to prove by induction that $(\mu_{n,t+1}, \sigma_{n, t+1})$
% are Lipschitz wrt to the estimators $\Pstate_0,\ldots,\Pstate_t$.
% For the base case, note that $\sigma_{n,1}^{-2}$ is constant wrt $\Pstate_{0}$
% and $\mu_{n,1}(\Pstate_{0})$ is linear in $\Pstate_{0}$ with coefficient $< 1$. 
% In addition, $\sigma_{n,1}^{-2}(\Pstate_{0})$ is 

\subsection{Proof of Lemma~\ref{lemma:rho-reduction}}
\label{section:proof-rho-reduction}

At each fixed epoch $t \in [T]$, we consider imagined \emph{counterfactual
  state transitions} if one follows future allocations
$\bar{\rho}_{t, a},\ldots,\bar{\rho}_{T-1,a}$ that only depend on currently
available information $(\mu_t, \sigma_t)$. Using the same notation for
counterfactual states to simplify the exposition, Lemma~\ref{lemma:mdp} shows
that subsequent counterfactual states will be governed by
\begin{subequations}
  \label{eqn:counterfactual-dynamics}
  \begin{align}
    \sigma_{v+1, a}^{-2}
    & \defeq \sigma_{v, a}^{-2} + s_a^{-2} b_v \bar{\rho}_{v, a}
      \label{eqn:counterfactual-dynamics-var} \\
    \mu_{v+1, a}
    & \defeq   \mu_{v, a} + \left( \sigma_{v, a}^2 - \sigma_{v+1, a}^2\right)^{\half} Z_{v, a}
      = \mu_{v, a} + \sigma_{v, a}\sqrt{\frac{b_v \bar{\rho}_{v, a}(\mu_t, \sigma_t)
      \sigma_{v, a}^{2}}{s_a^{2}+b_v\bar{\rho}_{v, a}(\mu_t, \sigma_t)\sigma_{v, a}^{2}}} Z_{v, a}.
      \label{eqn:counterfactual-dynamics-mean}
  \end{align}
\end{subequations}

First, we rewrite the objective~\eqref{eqn:rho} as a function of a single
Gaussian variable $\bar{Z}$. Recalling the identity~\eqref{eqn:var-decrease}
\begin{align*}
 \left( \sigma_{v, a}^2 - \sigma_{v+1, a}^2\right)^{\half} Z_{v, a}
  = \sigma_{v, a}\sqrt{\frac{b_v \bar{\rho}_{v, a}(\mu_t, \sigma_t)
  \sigma_{v, a}^{2}}{s_a^{2}+b_v\bar{\rho}_{v, a}(\mu_t, \sigma_t)\sigma_{v, a}^{2}}} Z_{v, a},
\end{align*}
use the recursive relation~\eqref{eqn:counterfactual-dynamics-var} to write
\begin{align*}
  \var\left( \sum_{v=t}^{T-1} \sigma_{v,a}\sqrt{\frac{b_v \bar{\rho}_{v, a}(\mu_t, \sigma_t)\sigma_{v, a}^{2}}
      {s_a^{2}+b_v\bar{\rho}_{v, a}(\mu_t, \sigma_t)\sigma_{v, a}^{2}}}Z_{v, a}\right)
  & = \sum_{v=t}^{T-1} \sigma_{v, a}^2 - \sigma_{v+1, a}^2
    = \sigma_{t, a}^2 - \sigma_{T, a}^2 \\
  & = \sigma_{t, a}^2 -  \left( \sigma_{t,a}^{-2}
    + \sum_{v=t}^{T-1} \frac{b_v \bar{\rho}_{v, a}(\mu_t, \sigma_t)}{s_a^2}  \right)^{-1} \\
  & =  \frac{\sigma_{t,a}^4 \sum_{v=t}^{T-1} b_{v}\bar{\rho}_{v, a}(\mu_t, \sigma_t)}
  {s_a^2 + \sigma_{t,a}^2 \sum_{v=t}^{T-1} b_{v}\bar{\rho}_{v, a}(\mu_t, \sigma_t)}.
\end{align*}
Thus, we conclude
\begin{align*}
  & \E \left[ \max_{a} \left\{ \mu_{t,a}
  + \sum_{v=t}^{T-1} \sigma_{v,a}\sqrt{\frac{\sigma_{v, a}^{2} b_v \bar{\rho}_{v, a}(\mu_t, \sigma_t)}
  {s_a^{2}+\sigma_{v, a}^{2} b_v \bar{\rho}_{v, a}(\mu_t, \sigma_t)}}Z_{v, a}
  \right\}~\Bigg|~ \mu_{t},\sigma_{t} \right] \\
  & = \E \left[ \max_{a} \left\{ \mu_{t,a}
  + \sqrt{\frac{\sigma_{t, a}^4 \sum_{v=t}^{T-1} \bar{\rho}_{v, a}(\mu_t, \sigma_t) b_{v}}
  {s_a^2 + \sigma_{t, a}^2 \sum_{v=t}^{T-1} \bar{\rho}_{v, a}(\mu_t, \sigma_t) b_{v}}} \bar{Z}_{a}
  \right\} ~\Bigg|~ \mu_{t},\sigma_{t} \right].
\end{align*}
Abusing notation, we replace $\bar{Z}$ with $Z_t$ in the final expectation.

Note that for any sequence of future allocations
$\bar{\rho}_{t},\ldots,\bar{\rho}_{T-1}$ that only depend on
$(\mu_t,\sigma_t)$, the change of variables
$\bar{\rho}_{a} = \frac{\sum_{v=t}^{T-1} \bar{\rho}_{v,a} b_{v}
}{\sum_{v=t}^{T-1} b_{v}}$ give
\[
  \begin{split}
  V^{\bar{\rho}_{t:T-1}}_{t}(\mu_{t},\sigma_{t})
  &= \E \left[ \max_{a} \left\{ \mu_{t,a}
  + \sqrt{\frac{\sigma_{t, a}^4 \sum_{v=t}^{T-1} \bar{\rho}_{v, a} b_{v}}
  {s_a^2 + \sigma_{t, a}^2 \sum_{v=t}^{T-1} \bar{\rho}_{v, a} b_{v}}} \bar{Z}_{a}
  \right\} ~\Bigg|~ \mu_{t},\sigma_{t} \right] \\
  &= \E \left[ \max_{a} \left\{ \mu_{t,a}
  + \sqrt{\frac{\sigma_{t, a}^4 \bar{\rho}_{a} \bar{b}_{t}}
  {s_a^2 + \sigma_{t, a}^2 \bar{\rho}_{a} \bar{b}_{t}}} \bar{Z}_{a}
  \right\} ~\Bigg|~ \mu_{t},\sigma_{t} \right] \\
  &= V^{\bar{\rho}}_{t}(\mu_{t},\sigma_{t})
  \end{split}
\]
where $\bar{b}_{t} = \sum_{v=t}^{T-1} b_{v}$ and
$V^{\bar{\rho}}_{t}(\mu_{t},\sigma_{t})$ is the value function of the constant
allocation $\bar{\rho}$.  Thus, for any sequence of future allocations that
only depend on $(\mu_t,\sigma_t)$, there exists a constant allocation
$\bar{\rho}(\mu_t, \sigma_t)$ that achieves the same performance.

\subsection{Proof of Proposition~\ref{prop:rho-vs-static}}
\label{section:proof-rho-vs-static}

By Lemma~\ref{lemma:rho-reduction}, for any future allocation
$\bar{\pi}_{t:T} = (\bar{\pi}_t(\mu_{t}, \sigma_{t}), \ldots,
\bar{\pi}_{T-1}(\mu_{t}, \sigma_{t}))$ that only depends on currently
available information $(\mu_{t},\sigma_{t}))$, there is a constant
allocation that matches the same performance.  Thus, it is sufficient to show
that the value function for $\rho$ dominates the value function of any constant
allocation
$\bar{\pi}_{v}(\mu_{t}, \sigma_{t}) \equiv \bar{\rho}(\mu_{t}, \sigma_{t})$ for $v \ge t$.  Proceeding by induction, observe for the base case that
when $t = T-1$, the definition~\eqref{eqn:rho} of the policy $\rho_t$ implies
\begin{equation*}
  V^{\rho}_{T-1}(\mu_{T-1},\sigma_{T-1})
  = \max_{\bar{\rho} \in \Delta_{\numarm}} V^{\bar{\rho}}_{T-1}(\mu_{T-1},\sigma_{T-1})
\end{equation*}
for all $(\mu_{T-1}, \sigma_{T-1})$.

Next, as an inductive hypothesis, suppose
\begin{equation*}
V^{\rho}_{t+1}(\mu_{t+1},\sigma_{t+1}) \geq \max_{\bar{\rho} \in
  \Delta_{\numarm}} V^{\bar{\rho}}_{t+1}(\mu_{t+1},\sigma_{t+1})
~~~\mbox{for all}~~(\mu_{t+1}, \sigma_{t+1}).
\end{equation*}
 Then, for any
$(\mu_{t}, \sigma_{t})$
\begin{align}
  V^{\rho}_{t}(\mu_{t}, \sigma_{t}) 
  = \E_{t} \left[V^{\rho}_{t+1}\left(\mu_{t+1}, \sigma_{t+1}\right)\right] 
  & \geq \E_{t} \left[ \max_{\bar{\rho} \in \Delta_{\numarm}} V^{\bar{\rho}}_{t+1}
  \left(\mu_{t+1}, \sigma_{t+1}\right)\right] \nonumber \\
  & \ge \max_{\bar{\rho} \in \Delta_{\numarm}} \E_{t} \left[  V^{\bar{\rho}}_{t+1}
    \left(\mu_{t+1}, \sigma_{t+1}\right)\right]
      \label{eqn:exp-max-interchange}
\end{align}
where we abuse notation to denote by $(\mu_{t+1}, \sigma_{t+1})$ the state
transition under the policy $\rho_t$~\eqref{eqn:dynamics}
\begin{subequations}
  \label{eqn:dynamics-rho-t}
  \begin{align}
    \sigma_{t+1, a}^{-2}
    & \defeq \sigma_{t, a}^{-2} + s_a^{-2} b_t \rho_{t, a}(\mu_t, \sigma_t)
    \label{eqn:dynamics-rho-t-var} \\
    \mu_{t+1, a}  
    & \defeq   \mu_{t, a} + \left( \sigma_{t, a}^2 - \sigma_{t+1, a}^2\right)^{\half} Z_{t, a}
      = \mu_{t, a} + \sigma_{t, a}\sqrt{\frac{b_t \rho_{t, a}(\mu_t, \sigma_t)
      \sigma_{t, a}^{2}}{s_a^{2}+b_t\rho_{t, a}(\mu_t, \sigma_t)\sigma_{t, a}^{2}}} Z_{t, a}.
              \label{eqn:dynamics-rho-t-mean}
  \end{align}
\end{subequations}

We now derive a more explicit representation for
$\E_{t} \left[ V^{\bar{\rho}}_{t+1} \left(\mu_{t+1},
    \sigma_{t+1}\right)\right]$, which is the value function for a policy that
follows $\rho_t$ at time $t$ and the constant policy $\bar{\rho}$ onwards.  By
the expression~\eqref{eqn:rho} in Lemma~\ref{lemma:rho-reduction}, we have
\begin{align*}
  \E_{t} \left[ V^{\bar{\rho}}_{t+1} \left(\mu_{t+1}, \sigma_{t+1}\right)\right]
  =  \E_{t} \left[ \max_{a} \left\{ \mu_{t+1,a}
          + \sqrt{\frac{\sigma_{t+1, a}^4 \bar{\rho}_{a} \bar{b}_{t+1}}
            {s_a^2 + \sigma_{t+1, a}^2 \bar{\rho}_{a} \bar{b}_{t+1}}} Z_{t+1, a}
        \right\} \right].
\end{align*}
Plugging in the state transition~\eqref{eqn:dynamics-rho-t-mean}, we have
\begin{align*}
  \E_{t} \left[ V^{\bar{\rho}}_{t+1} \left(\mu_{t+1}, \sigma_{t+1}\right)\right]
  =  \E_{t} \left[ \max_{a} \left\{ \mu_{t,a} 
  + \left( \sigma_{t, a}^2 - \sigma_{t+1, a}^2\right)^{\half} Z_{t, a}
          + \sqrt{\frac{\sigma_{t+1, a}^4 \bar{\rho}_{a} \bar{b}_{t+1}}
            {s_a^2 + \sigma_{t+1, a}^2 \bar{\rho}_{a} \bar{b}_{t+1}}} Z_{t+1, a}
        \right\} \right].
\end{align*}
Noting that
\begin{align*}
  \var_t \left(
  \left( \sigma_{t, a}^2 - \sigma_{t+1, a}^2\right)^{\half} Z_{t, a}
          + \sqrt{\frac{\sigma_{t+1, a}^4 \bar{\rho}_{a} \bar{b}_{t+1}}
            {s_a^2 + \sigma_{t+1, a}^2 \bar{\rho}_{a} \bar{b}_{t+1}}} Z_{t+1, a}
  \right) =
  \frac{\sigma_{t, a}^4 \left( \bar{\rho}_{a} \bar{b}_{t+1}
  + \rho_{t, a}(\mu_{t}, \sigma_{t}) b_{t} \right)}
  {s_a^2 + \sigma_{t, a}^2 \left(
  \bar{\rho}_{a} \bar{b}_{t+1} +\rho_{t, a}(\mu_{t}, \sigma_{t}) b_{t} \right)},
\end{align*}
we arrive at the identity
\begin{align*}
  \E_{t} \left[ V^{\bar{\rho}}_{t+1} \left(\mu_{t+1}, \sigma_{t+1}\right)\right]
  =  \E_{t} \left[ \max_{a} \left\{ \mu_{t,a} 
  + \sqrt{\frac{\sigma_{t, a}^4 \left( \bar{\rho}_{a} \bar{b}_{t+1}
  + \rho_{t}(\mu_{t}, \sigma_{t}) b_{t} \right)}
  {s_a^2 + \sigma_{t, a}^2 \left(
  \bar{\rho}_{a} \bar{b}_{t+1} +\rho_{t}(\mu_{t}, \sigma_{t}) b_{t} \right)}}
  Z_{t, a} \right\} \right].
\end{align*}

Recalling the bound~\eqref{eqn:exp-max-interchange}, use
$\bar{\rho} = \rho_{t}(\mu_{t}, \sigma_{t})$ to conclude
\begin{align*}
  V^{\rho}_{t}(\mu_{t}, \sigma_{t})
  & \ge \max_{\bar{\rho} \in \Delta_{\numarm}}
  \E_{t} \left[ \max_{a} \left\{ \mu_{t,a} 
  + \sqrt{\frac{\sigma_{t, a}^4 \left( \bar{\rho}_{a} \bar{b}_{t+1}
  + \rho_{t}(\mu_{t}, \sigma_{t}) b_{t} \right)}
  {s_a^2 + \sigma_{t, a}^2 \left(
  \bar{\rho}_{a} \bar{b}_{t+1} +\rho_{t}(\mu_{t}, \sigma_{t}) b_{t} \right)}}
  Z_{t, a} \right\} \right] \\
  & \ge \E_{t} \left[ \max_{a} \left\{ \mu_{t,a} 
  + \sqrt{\frac{\sigma_{t, a}^4 \rho_{t, a}(\mu_{t}, \sigma_{t}) \bar{b}_{t} }
  {s_a^2 + \sigma_{t, a}^2  \rho_{t, a}(\mu_{t}, \sigma_{t}) \bar{b}_{t}  }}
    Z_{t, a} \right\} \right]  \\
  & = \max_{\bar{\rho} \in \Delta^\numarm} V_t^{\bar{\rho}} (\mu_t, \sigma_t),
\end{align*}
where we used the definition of $\rho_t$ in expression~\eqref{eqn:rho} in
the final line.


\subsection{Distilled Residual Horizon Optimization}
\label{section:distilled}


Instead of computing
$\rho_t(\mu_t, \sigma_t)$ as the experiment progresses, it can sometimes be
convenient to pre-compute the mapping $\rho_t(\cdot, \cdot)$ so that it can be
readily applied for any observed current state.  By reformulating the
optimization problem~\eqref{eqn:rho} as a stochastic optimization problem over
\emph{functions of $(\mu_t, \sigma_t)$}, we propose a \emph{distilled} variant
of the $\algo$ policy that can learn the mapping $\rho_t(\cdot,\cdot)$ in a
fully offline manner.
\begin{proposition}
  \label{prop:loss-min}
  The solution to the $\algofull$ problem~\eqref{eqn:rho} coincides with the
  maximizer of the following reward maximization problem over measurable
  functions $\rho(\mu_t, \sigma_t) \in \Delta^{\numarm}$
  \begin{equation}
    \label{eqn:loss-min}
    \maximize_{\rho(\cdot)~\mbox{\scriptsize measurable}}
    ~\E_{(\mu_t, \sigma_t) \sim P_t} \left[ \max_{a} \left\{ \mu_{t,a}
        + \sqrt{\frac{\sigma_{t, a}^4 \rho(\mu_t, \sigma_t) \bar{b}_t}
        {s_a^2 + \sigma_{t, a}^2 \rho(\mu_t, \sigma_t) \bar{b}_t}} Z_{t, a}
      \right\} \right],
  \end{equation}
  where $P_t$ is any probability measure with support
  $\R^\numarm \times (0, \sigma_0^2]^\numarm$ and
  $\bar{b}_t \defeq \sum_{v=t}^{T-1} b_v$.
\end{proposition}
%% \noindent See Section~\ref{section:proof-loss-min} for the proof. 


Proposition~\ref{prop:loss-min} allows us to use standard ML best practices to
(approximately) solve the reward maximization problem~\eqref{eqn:rho}. We
parameterize the policy using black-box ML models
\begin{equation*}
  \left\{\rho_{\theta,t}(\mu_t, \sigma_t): \theta \in \Theta_t\right\},
\end{equation*}
and use stochastic gradient-based optimization algorithms to solve for the
optimal parameterization (Algorithm~\ref{alg:rho}). In particular, the model
training problem~\eqref{eqn:loss-min} can be approximated by neural networks
and optimized through standard auto-differentiation
frameworks~\cite{TensorFlow15, PyTorch19} for computing stochastic
(sub)gradients. 
By the envelope theorem, the stochastic (sub)gradient at the
sample $(\mu_t, \sigma_t) \sim P_t$ is given by
\begin{equation*}
  Z_{t, a\opt} ~\partial_{\theta} \left\{
    \sqrt{\frac{\sigma_{t, a\opt}^4  \rho_{\theta,t} (\mu_t, \sigma_t) \bar{b}_t}
    {s_{a\opt}^2 + \sigma_{t, a\opt}^2 \rho_{\theta,t}(\mu_t, \sigma_t) \bar{b}_t}}
  \right\},
\end{equation*}
where
$a\opt \in \argmax_{a} \left\{ \mu_{t,a} + \sqrt{\frac{\sigma_{t, a}^4
      \rho_{\theta,t}(\mu_t, \sigma_t) \bar{b}_t} {s_a^2 + \sigma_{t, a}^2
      \rho_{\theta,t}(\mu_t, \sigma_t) \bar{b}_t}} Z_{t, a} \right\}$. 


% We provide concrete implementation details for Algorithm~\ref{alg:rho} in
% Section~\ref{section:implementation}. \hn{@Ethan: Fill in details in this
%   section. Discuss model architecture, training method etc.}

\begin{proof}
It remains to show that minimizing over all measurable functions
$\rho(\mu_t, \sigma_t) \in \Delta^{\numarm}$ gives pointwise suprema for each
$(\mu_t, \sigma_t)$ almost surely
\begin{align*}
  & \sup_{\rho(\cdot)~\mbox{\scriptsize measurable}} ~\E_{(\mu_t, \sigma_t) \sim P_t} \left[ \max_{a} \left\{ \mu_{t,a}
        + \sqrt{\frac{\sigma_{t, a}^4 \rho(\mu_t, \sigma_t) \bar{b}_t}
        {s_a^2 + \sigma_{t, a}^2 \rho(\mu_t, \sigma_t) \bar{b}_t}} Z_{t, a}
    \right\} \right] \\
  & = \E_{(\mu_t, \sigma_t) \sim P_t} \left[ \sup_{\rho}
    \E\left[ \max_{a} \left\{ \mu_{t,a}
        + \sqrt{\frac{\sigma_{t, a}^4 \rho \bar{b}_t}
        {s_a^2 + \sigma_{t, a}^2 \rho \bar{b}_t}} Z_{t, a}
      \right\} ~\Bigg|~ \mu_t, \sigma_t \right] \right].
\end{align*}
We use normal integrand theory~\cite[Section 14.D]{RockafellarWe98} to
interchange the integral and supremum over measurable mappings. Recall that a
map
$f: \Delta^{\numarm} \times (\R^{\numarm} \times (0, \sigma_0]^\numarm) \to
\bar{\R}$ is a normal integrand if its epigraphical mapping---viewed as a
set-valued
mapping---$S_f: \R^{\numarm} \times (0, \sigma_0]^\numarm \to \R \times
\R,~(\mu_t, \sigma_t) \mapsto \epi f(\cdot; \mu_t, \sigma_t) = \{ (\rho,
\alpha) \in \Delta^{\numarm} \times \R: f(\rho; \mu_t, \sigma_t) \le \alpha\}$ is
closed-valued. That is, $S_f(\mu_t, \sigma_t)$ is closed for all
$(\mu_t, \sigma_t) \in \R^{\numarm} \times (0, \sigma_0]^\numarm$ and
measurable (i.e., for any open set
$O \subseteq \R^{\numarm} \times (0, \sigma_0]^\numarm$,
$S_f^{-1}(O) \defeq \cup_{o \in O}~ S_f^{-1}(o)$ is measurable).
\begin{lemma}[{\citet[Theorem 14.60]{RockafellarWe98}}]
  \label{lemma:inf-int-interchange}
  If
  $f: \Delta^{\numarm} \times (\R^{\numarm} \times (0, \sigma_0]^\numarm) \to
  \bar{\R}$ is a normal integrand, and
  $ \int_{\R^{\numarm} \times (0, \sigma_0]^\numarm} f(\rho(\mu_t, \sigma_t);
  \mu_t, \sigma_t) ~ dP_t(\mu_t, \sigma_t) < \infty$ for some measurable
  $\rho(\cdot)$, then
  \begin{align*}
    & \inf_{\rho(\cdot)} \left\{
      \int_{\R^{\numarm} \times (0, \sigma_0]^\numarm} f(\rho(\mu_t, \sigma_t); \mu_t, \sigma_t)
      ~d P_t(\mu_t, \sigma_t) ~\Big|~
      \rho: \R^{\numarm} \times (0, \sigma_0]^\numarm \to \R~\mbox{measurable}
      \right\} \\
    & = \int_{\R^{\numarm} \times (0, \sigma_0]^\numarm}
      \inf_{\rho \in \Delta^\numarm} f(\rho; \mu_t, \sigma_t) ~ dP_t(\mu_t, \sigma_t).
  \end{align*}
  If this common value is not $-\infty$, a measurable function
  $ \rho^*: \R^{\numarm} \times (0, \sigma_0]^\numarm \to \R$ attains the
  minimum of the left-hand side iff
  $ \rho^*(\mu_t, \sigma_t) \in \argmin_{\rho \in \Delta^\numarm} f(\rho; \mu_t,
  \sigma_t)$ for $P_t$-almost every
  $(\mu_t, \sigma_t) \in \R^{\numarm} \times (0, \sigma_0]^\numarm$.
\end{lemma}
\noindent Define the function
\begin{equation*}
  f(\rho; \mu_t, \sigma_t) \defeq - \E_{Z_t \sim N(0,I)}\left[ \max_{a} \left\{ \mu_{t,a} +
    \sqrt{\frac{\sigma_{t, a}^4 \rho_a \bar{b}_t} {s_a^2 + \sigma_{t, a}^2 \rho_a
        \bar{b}_t}} Z_{t, a} \right\} \right].
\end{equation*}
Since $f$ is continuous in $(\rho, \mu_t, \sigma_t)$ by dominated convergence,
$f$ is a normal integrand~\cite[Examples 14.31]{RockafellarWe98}.  Applying
Lemma~\ref{lemma:inf-int-interchange}, we obtain the desired result.
\end{proof}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
