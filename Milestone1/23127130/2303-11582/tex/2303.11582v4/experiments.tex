\section{Comparison with standard bandit approaches}
\label{section:experiments}

%  \begin{figure}[t]
% \vspace{-1.4cm}
%   \centering
%   \subfloat{{\includegraphics[height=3.6cm]{../fig/gumbel_10_correct_pct_regret_1.jpg} }}%
%   \subfloat{{\includegraphics[height=3.6cm]{../fig/bar_gumbel_correct_0.2_1_5_5.jpg} }}%
%   \vspace{-.4cm}
%   \setcounter{subfigure}{0}
%    \subfloat[\centering Relative gains over uniform allocation for $s_a^2 \equiv 1$ (above: $K = 10$; below: $K = 100$).]{\label{fig:bsr_time_100}{\includegraphics[height=3.6cm]{../fig/gumbel_100_correct_pct_regret_1.jpg} }}%
%    \subfloat[\centering Relative gains over uniform allocation for $s_a^2  \in \{\frac{1}{5}, 1, 5\}$ with $T = 5$ (above: $K = 10$; below: $K = 100$).]{\label{fig:bsr_noise_100}{\includegraphics[height=3.6cm]{../fig/bar_100_gumbel_correct_0.2_1_5_5.jpg} }}%
%    \caption{\label{fig:bsr} Pre-limit Bayes simple regret for Gumbel rewards
%      and $\mbox{Gamma}(100,100)$ prior}
%   \vspace{-.4cm}
% \end{figure}

Our empirical and theoretical analysis in the previous section shows that
$\algo$ is an exceedingly simple yet effective heuristic. Impelled by these
benefits, we now provide a careful empirical comparison between $\algo$ and
other standard multi-armed bandit algorithms. Overall, we find that although
$\algo$ relies on Gaussian approximations for the rewards and the prior
distribution, performance gains from planning with the Gaussian sequential
experiment greatly outweigh approximation errors. In total, we find that
across 640 settings, $\algo$ outperforms Gaussian policies discussed in Section~\ref{section:algorithms-adp} and
standard batch bandit policies in 493 (77.0\%) of them.
Our empirical results
suggest that $\algo$ particularly excels in settings that are difficult for standard
adaptive policies, including those a limited number of reallocation epochs,
unknown reward distributions, low signal-to-noise ratios, and a large number
of treatment arms.  Its versatile performance across different horizon and
noise levels shows the utility of calibrating exploration using the Gaussian
sequential experiment.  In addition to the key dimensions we highlight in
this section, we provide a comprehensive set of experiments in the Streamlit
app~\eqref{eqn:streamlit}.

% \begin{itemize}[itemsep=-2pt, leftmargin=.7cm]
% \item For general reward distributions, the asymptotic Gaussian sequential
%   experiment problem~\eqref{eqn:dynamics} provides a useful framework for
%   experimental design, even when batch sizes are small. $\algo$ outperforms 
%   standard batch experimentation policies, including algorithms (e.g.  variants of Thompson
%   sampling~\citep{Russo20}) that require knowledge of the reward
%   distribution.
% \item Performance gains from $\algo$ are larger in underpowered experiments
%   with many treatment arms or high measurement noise, settings in
%   which standard adaptive policies struggle to learn.
% \item $\algo$ effectively incorporates prior information on average rewards,
%   achieving a larger reduction in the Bayes simple regret when the prior is
%   more informative.
% \end{itemize}



% For general reward distributions, the asymptotic Gaussian
% sequential experiment problem~\eqref{eqn:dynamics} provides a valuable
% framework for experimental design, even when the scaling parameter $n$ (batch
% size) is not large.  Our approximate dynamic programming policies outperform
% uniform allocation and even specialized algorithms (e.g., variants of Thompson
% sampling~\citep{Russo20}) that require complete knowledge of the reward
% distribution. We observe the policy gradient-based policies can generalize to
% longer horizons than initially trained on. The $Q$-myopic policy enjoys strong
% performance despite optimizing a lower bound of the $Q$ function.  Both policy
% gradient and $Q$-myopic achieve more significant performance gains in
% harder/underpowered experiments with high effective measurement noise
% $s_a^{2} / b_t$.



\begin{figure}[t]

    \centering
    \hspace{-.9cm}
      \subfloat[\centering Number of arms $\numarm = 10$.]{\label{fig:num_arm_bsr_10}{\includegraphics[height=5cm]{./fig/reg_Bernoulli_100_10_1_Flat.png}} }%
      \hspace{.5cm}
      \subfloat[\centering Number of arms $\numarm = 100$.]{\label{fig:num_arm_bsr_100}{\includegraphics[height=5cm]{./fig/reg_Bernoulli_100_100_1_Flat.png} }}%
      \vspace{.4cm}
      \caption{\label{fig:num_arm_bsr} Comparison of performance across
        different number of treatment arms.  Relative gains over the uniform
        allocation as measured by the Bayes simple regret for the finite batch
        problem with a batch size of $b_{t}n = 100$.
        $\textsf{Beta-Bernoulli experiment}$ where individual rewards are
        Bernoulli with a $\mbox{Beta}$ prior.  
        % $\algo$ delivers larger
        % performance gains when $K = 100$, even though the overall sample size
        % is small when there are many treatment arms.  This is not necessarily
        % true for other adaptive policies: the relative performance of the
        % Thompson sampling policies diminishes when there are more treatment
        % arms. 
        %\hn{@Ethan: FOR EVERY PLOT, make x/y-labels and legends AS LARGE AS possible. }
          }
    
  \end{figure}

\paragraph{Algorithms} To benchmark performance, we consider a suite of standard batch bandit
policies proposed in the literature.
\begin{itemize}[itemsep=-2pt, leftmargin=.7cm]
  \item Uniform: For each batch, assign samples uniformly across treatment arms.
  \item Successive Elimination \citep{EvenDarMaMa06, GaoHaReZh19,
      PerchetRiChSn16}: For each batch, assign samples uniformly across
    treatment arms that are not eliminated. Eliminate all arms whose upper
    confidence bound is less than the lower confidence bound of some other
    arm. Given $\numarm$ arms, $n_{a}$ samples drawn for arm $a$, measurement
    variance $s_{a}^{2}$, the confidence interval is
    $\hat{\mu}_{a} \pm \beta_{a}(n_{a})$, where $\hat{\mu}_{a}$ is the
    empirical mean reward of arm $a$ and the width of the confidence bound
    $\beta_{a}(n_{a})$ is
  $$ \beta_{a}(n_{a}) = cs_{a} \sqrt{\frac{\log(n_{a}^{2}K/\delta)}{n_{a}}},$$
  where $c,\delta$ are constants that are chosen via grid search to minimize regret in each instance.
  \item Oracle Thompson Sampling \citep{KalkanhOz21, KandasamyKrScPo18}: Beta-Bernoulli TS with batch updates.
  \item Oracle Top-Two Thompson Sampling \citep{Russo20}: Beta-Bernoulli Top-Two TS with batch updates.
  \item $\algofull$: Selects the allocation by solving the planning problem \eqref{eqn:rho}, as in Algorithm \ref{alg:rho}.
\end{itemize}

For the $\textsf{Gamma-Gumbel experiment}$, the Gumbel distribution does not
have a known conjugate prior distribution and updating the posterior after
each batch is more involved. For this reason, when in this environment we
restrict our focus to successive elimination as the main baseline. This
captures settings in which the reward model is unknown to the experimenter,
where it is difficult to use Thompson sampling or other Bayesian policies that
require the exact reward distribution.



\begin{figure}[t]
  \centering
  \includegraphics[height=5cm]{./fig/bar_var_Gumbel_100_100_Flat.png}
    \caption{\label{fig:bar_var} Comparison of performance across different
      measurement noise levels.  Relative gains over the uniform allocation as
      measured by the Bayes simple regret. $\textsf{Gamma-Gumbel experiment}$
      where rewards follow Gumbel distributions with measurement variances
      $s_{a}^{2} \in \{0.2, 1, 5 \}$.  There are $K = 100$ treatment arms with
      $T = 10$ batches of size $b_{t}n = 100$.  
      % We observe that $\algo$ is
      % versatile across different signal-to-noise ratio regimes.  It still
      % delivers strong performance gains when the measurement noise is large,
      % even though other adaptive methods struggle to improve upon the uniform
      % allocation. 
      %\hn{@Ethan, Nit: can you make the x-axis ticks horizontal?}
    }
\end{figure}

\paragraph{Number of alternative $\numarm$ (treatment arms)}
In order to study how performance changes with the number of alternatives, we
fix the batch size to be $b_{t}n = 100$ and evaluate our policies for
$\numarm = 10$ and $\numarm = 100$ alternatives. Figure \ref{fig:num_arm_bsr}
shows that $\algo$ achieves strong performance for large and small number of
alternatives alike, and the performance gains are larger when there is a large
number of arms.  
% This is not the case for all adaptive policies: the relative
% performance of the Thompson sampling policies is slightly worse when there are
% more alternatives. 
In these experiments and others presented in the
interactive app~\eqref{eqn:streamlit}, we observe that the performance gains
of $\algo$ persist across all time horizons, particularly for short
horizons. 
% This highlights the effectiveness of planning with the Gaussian
% approximation for calibrating exploration under a limited
% number of reallocations.

\paragraph{Measurement variance $s_a^2$}
Next, we study how the signal-to-noise of the problem instance affects the
performance of adaptive policies. To study a scenario where we can flexibly
control different measurement variances
$s^{2}_a = \var \epsilon_a$~\eqref{eqn:rewards}, we move away from the
$\textsf{Beta-Bernoulli experiment}$ and instead consider the
$\textsf{Gamma-Gumbel experiment}$. In Figure \ref{fig:bar_var}, we observe
that $\algo$ outperforms uniform allocation and successive elimination in both
high and low signal-to-noise regimes. When the signal-to-noise ratio is high
(i.e., measurement noise $s_a^2$ is small), $\algo$ is able to rapidly hone in
on the highest reward treatment arms. Even when the signal-to-noise is low,
$\algo$ is able to make substantial progress over uniform while other adaptive
policies struggle to learn enough to improve sampling efficiency.  
% We note
% that when the measurement variance is low, the differences in \emph{raw}
% performance between policies is small as all policies are able to hone in on
% high reward treatment arms. 
%  In the high noise regime, the raw performance
% gains of $\algo$ are much larger, especially compared to alternative adaptive
% policies.  Altogether, this illustrates that iterative planning is particularly useful
% in underpowered experiments by calibrating exploration to the statistical
% power of the experimental setting.

% \hn{@Ethan: Discuss what's not
%   in the figure. The raw scales are also interesting here---would be nice to
%   mention this.}

\begin{figure}[t]
  \centering
  \hspace{-.9cm}
  \subfloat[\centering Batch size $b_{t}n = 100$.]{\label{fig:gumbel_batch_bsr_10}{\includegraphics[height=5cm]{./fig/reg_Gumbel_100_100_1_Flat.png}} }%
  \hspace{.5cm}
  \subfloat[\centering Batch size $b_{t}n = 10,000$.]{\label{fig:gumbel_batch_bsr_100}{\includegraphics[height=5cm]{./fig/reg_Gumbel_10000_100_1_Flat.png} }}%
  \vspace{.4cm}
  \caption{\label{fig:gumbel_batch_bsr} Comparison of performance across
    batch sizes.  Relative gains over the uniform allocation as measured
    by the Bayes simple regret for the finite batch problem with $K = 100$
    treatment arms. $\textsf{Gamma-Gumbel experiment}$ where individual
    rewards are Gumbel with a $\mbox{Gamma}$ prior. 
    % Despite relying on
    % Gaussian approximations, $\algo$ delivers substantial performance
    % improvements over uniform even though the reward distribution has a
    % high excess kurtosis compared to Gaussian data. Moreover, $\algo$
    % performs well for large and small batches alike, while successive
    % elimination struggles when the batch size is small.
    }    
\end{figure}

\paragraph{Batch size $b_t n$}
In Figure~\ref{fig:bern-bsr-batch} we presented in
Section~\ref{section:introduction}, we compared the regret incurred by
different policies for large and small batches in the
$\textsf{Beta-Bernoulli experiment}$.  We use the
$\textsf{Gamma-Gumbel experiment}$ as an additional test of whether the
Gaussian approximations hold for batched rewards. Although the Gumbel
distribution has a high excess kurtosis, we still observe that the Gaussian
sequential experiment serves as a useful approximation even for small
batches. In Figure \ref{fig:gumbel_batch_bsr}, we see that $\algo$ greatly
outperforms uniform allocation and successive elimination for small and large
batches alike.  In particular, successive elimination especially struggles
when the batch size is small ($b_{t}n = 100$).

\ifdefined\msom
\else

\begin{figure}[t]
  \centering
  \includegraphics[height=5cm]{./fig/bar_prior_Bernoulli_100_100_0.25.png}
    \vspace{.4cm}
    \caption{\label{fig:bar_prior} Comparison of performance across  different
      priors.  Relative gains over the uniform allocation as measured by the
      Bayes simple regret for the finite batch problem for $\numarm = 100$
      arms and $T = 10$ batches of size $b_{t}n = 100$ samples.
      $\textsf{Beta-Bernoulli experiment}$ where individual rewards are
      Bernoulli with various $\mbox{Beta}$ priors.  
      % When the prior is
      % informative (e.g. Top Half and Descending), Bayesian policies such as
      % Thompson sampling and $\algo$ improve upon policies which do not
      % incorporate prior information; performance
      % gains from $\algo$ are much larger.
      %\hn{Can we have diagnoal or horizontal x-labels? The verticals take up a lot of space and it's hard to read.}
    }
  
\end{figure}


\paragraph{Non-uniform prior distributions}

The previous numerical experiments focused on the case in which the
experimenter's prior distribution is identical across treatment arms. Yet,
modern experimental platforms typically run many experiments with similar
treatments, so the experimenter may have different prior beliefs across
treatments. These beliefs can be incorporated into the experimental design to
improve sampling efficiency.  To study the effect of non-uniform priors, we
consider the $\textsf{Beta-Bernoulli experiment}$ under different Beta priors
for each arm. Fixing the batch size to be $b_{t}n = 100$ and the number of
arms to be $ \numarm = 100$, we consider the following prior distributions.
\begin{itemize}[itemsep=-2pt, leftmargin=.7cm]
  \item Flat: All arms have an identical prior of Beta$(100,100)$.
  \item Top One: There is a single treatment with a Beta$(110,100)$ prior such
    that it has a higher prior mean than other arms. All other arms have
    Beta$(100,100)$ priors.
  \item Top Half: Half of the arms have Beta$(110,100)$ priors so that they
    have a higher prior mean than the rest which have Beta$(100,100)$ priors.
  \item Descending: The first arm has the highest prior mean (Beta$(100,100)$)
    and the prior means decrease for each arm $i \in [\numarm]$. Each arm has
    a Beta$(100 - (i - 1),100)$ prior.
\end{itemize}

Figure \ref{fig:bar_prior} compares the performance of the sampling policies
across different prior distributions. Unsurprisingly, policies which use prior
information outperform non-Bayesian policies when the prior is more
informative. We observe that despite using a Gaussian prior to approximate the
true prior distribution, $\algo$ obtains significantly larger performance
gains compared to Thompson sampling policies that use the true prior.  
% This
% illustrates that by planning with the Gaussian sequential experiment, $\algo$
% is able to effectively incorporate prior information into the experimental
% design despite approximation errors.

% \begin{figure}[t]
%  \ifdefined\msom
%  \else
%   \vspace{-1.6cm}
%   \fi
%   \centering
%   \hspace{-1.6cm}
%   \subfloat[\centering Measurement variance $s_{a}^{2} = 1$.]{\label{fig:gse_bsr}{\includegraphics[height=5.2cm]{./fig/alt_reg_Gumbel_10000_100_1.0_Flat.png} }}%
%   %\hspace{.05cm}
%   \subfloat[\centering Measurement variance $s_{a}^{2} \in \{0.2, 1, 5\}$ for $T = 10$]{\label{fig:gse_var}{\includegraphics[height=5.2cm]{./fig/alt_gse_bar_var_Gumbel_10000_100_Flat.png}} }%
%   \vspace{.4cm}
%   \caption{\label{fig:gse_compare} Comparison of Gaussian batch policies.
%     Relative gains over the uniform allocation as measured by the Bayes simple
%     regret for the finite batch problem with $K = 100$ treatment arms and
%     batches of size $b_{t} n = 10,000$.  $\textsf{Gamma-Gumbel experiment}$
%     where individual rewards are Gumbel with a $\mbox{Gamma}$ prior. $\algo$
%     maintains strong performance for small and long horizon experiments, as
%     well as for low and high noise levels.  Myopic performs well in short
%     horizon experiments, but worsens in longer horizon experiments due to
%     insufficient exploration. The Thompson sampling policies are effective for
%     low measurement noise but their performance degrades in underpowered
%     settings. 
%     %\hn{@Ethan: Align the plot more to the left. Right plot should
%       %have similar x- and y-label font sizes as the left one. Nit:
%         %can you make the x-axis ticks horizontal in panel (b)?}
%         }
% \end{figure}


% \paragraph{Gaussian batch policies}
% Our discussion so far highlights $\algo$ as an effective iterative planning
% policy. Since $\algo$ is but one policy that use the Gaussian sequential
% experiment (Definition~\ref{def:gse}) as an approximation for batch rewards,
% we now consider alternative Bayesian policies that also maintain Gaussian posterior
% states over the \emph{average rewards}. We consider the following benchmark
% approaches that rely Gaussian batch approximations.
% \begin{itemize}
% \item Gaussian Limit Thompson Sampling: Thompson Sampling policy for the Gaussian sequential experiment.
% \item Gaussian Limit Top-Two Thompson Sampling: Top-Two Thompson Sampling
%   policy for the Gaussian sequential experiment.
% \item Myopic: Selects the sampling allocation that maximizes the one-step
%   lookahead value function for the problem~\eqref{eqn:opt-dp}. This is a
%   randomized version of the Knowledge Gradient method.
% \item TS$+$: Policy iteration on the approximate Thompson Sampling policy~\eqref{eqn:TS-plus}.
% \item Policy Gradient: Alternative to $\algo$ for (heuristically) solving the
%   dynamic program~\eqref{eqn:opt-dp}.  Allocates samples according to a policy
%   parameterized by a feed-forward neural network, which is trained through
%   policy gradient with an episode length of 5 batches.
% \end{itemize}


% In Figure~\ref{fig:gse_compare}, we compare the performance of the above
% policies in the $\textsf{Gamma-Gumbel experiment}$ with $\numarm = 100$ arms
% and batches of size $b_{t} n = 10,000$. Figure~\ref{fig:gse_bsr} focuses on a
% fixed measurement variance $(s_{a}^{2} = 1)$ across a range of reallaction
% epochs. Figure~\ref{fig:gse_var} compares performance across different
% measurement noise levels for a fixed number of reallocations $T = 10$. These
% results show that $\algo$ exhibits consistently strong performance across a
% wide array of settings; other policies are effective in some instances and
% less effective in others. For example, the myopic policy performs well for
% short horizon experiments as expected, but worsens in longer horizon settings
% due to insufficient exploration.  Thompson sampling, which tends to explore
% more than other policies, is an effective heuristic when the noise level is
% low, but suffers in more underpowered experiments. It is worth noting though that
% in the low noise regime the differences in raw performance
% between adaptive policies tends to be small as all policies perform well,
% whereas the gap in raw performance between policies is wider in underpowered experiments.

% We find the policy gradient method achieves equivalent performance to $\algo$
% when there are a small number of alternatives ($\numarm = 10$) and measurement noise is high. 
% TS$+$ improves upon Thompson Sampling in similar settings.
% However, these methods become more challenging to use when there are many treatment
% arms and when the measurement variance is low, as the gradients vanish in
% these settings. For low measurement variance, TS$+$ is unable to improve upon Thompson Sampling
% due to difficulties in the optimization of~\eqref{eqn:TS-plus}. 
% The versatile performance of $\algo$ across short and long
% horizon experiments as well as low and high measurement noise levels shows
% that the planning aspects of the Gaussian sequential experiment
% are useful for calibrating exploration in different experimental settings, and
% that these benefits can be obtained by the simple and computationally
% efficient procedure~\eqref{eqn:rho}.

% As for the lackluster performance of policy gradient,
% in general we observe that while policy gradient
% performs equally as well as $\algo$ when there are fewer alternatives (e.g. $\numarm = 10$)
% it becomes more challenging to train the policy due to noisy gradients when
% there are many treatment arms and when the measurement variance is low.

% Each of these policies varies in terms of how they use Markov Decision Process
% structure of the Gaussian sequential experiment to select the sampling allocation. 
% For example, while $\algo$ and policy gradient are directly optimizing the Q-function,
% Thompson sampling only uses the Gaussian batch approximation for updating beliefs;
% the sampling allocation is generated by posterior sampling. Thus, these benchmarks
% allow us to study when and to what degree utilizing the MDP improves statistical power
% of the experiment.





% rewards follow a Gumbel distribution with a fixed scale
% parameter $\beta > 0$ across all arms, implying the measurement variance
% $s^2_a = \frac{\pi^2}{6}\beta$.  The location parameters $\mu_{a}$ are drawn
% from an independent $Gamma(100,100)$ prior, which is known to the
% experimenter.  The Gaussian sequential approximation for each batch gives
% $\bar{R}^n_{t,a} \sim N\left(\mu_a,s^2_a/(10\pi_{t})\right)$.  To maintain
% conjugacy, we also approximate the prior with a normal distribution with the
% same mean ($\mu_{0,a} = 1$) and standard deviation ($\sigma_{0,a} = 0.1$).  We
% use an adapted version of the successive elimination (SE)
% algorithm~---a popular heuristic in practice---as a key
% benchmark.  In Figure~\ref{fig:bsr}a), policy gradient and $Q$-myopic achieve
% substantial performance gains over uniform allocation, despite relying on
% normal approximations of the true reward distribution and the prior
% distribution. Gains are significant even with few reallocation epochs, and
% grow with more adaptivity (larger $T$). In Figure~\ref{fig:bsr}b), these gains
% persist when the noise level is high, even though standard adaptive procedures
% (SE) struggle to eliminate non-performant arms.




% Main result
%\hn{May change to Gumbel stuff; need to include a plot} As a concrete illustration of our main
%findings, consider a Gumbel model with $k=10$ arms with unknown mean parameters $\{\mu_i\}^{k}_{i=1}$ and a common $\beta$ parameter. There are up to $T=10$
%batches and $n = 100$ samples in each batch. The experimenter has an
%independent $Gamma(100,100)$ prior on $\mu_i$. The mean of the prior is 1 and the standard
%deviation is $0.1$. We can approximate the prior
%distribution as $1+\frac{Z_{i}}{\sqrt{n}} = 1 + (0.1)Z_i$ where $Z_i  \sim N(0,1)$. We consider various measurement variances $s^2$
%by adjusting the $\beta$ parameter accordingly. 


% \subsection*{Additional Numerical Experiments}

% % \begin{figure}[b]
% %   %\vspace{-1.4cm}
% %     \centering
% %     \subfloat{{\includegraphics[height=3.6cm]{../fig/gumbel_10_correct_pct_regret_1_other.jpg} }}%
% %     \subfloat{{\includegraphics[height=3.6cm]{../fig/gumbel_10_correct_0.2_1_5_5.jpg} }}%
% %     \vspace{-.4cm}
% %     \setcounter{subfigure}{0}
% %      \subfloat[\centering Relative gains over uniform allocation for $s_a^2 \equiv 1$ (above: $K = 10$; below: $K = 100$).]{\label{fig:bsr_time_100_more}{\includegraphics[height=3.6cm]{../fig/gumbel_100_correct_pct_regret_1_other.jpg} }}%
% %      \subfloat[\centering Relative gains over uniform allocation for $s_a^2  \in \{\frac{1}{5}, 1, 5\}$ with $T = 5$ (above: $K = 10$; below: $K = 100$).]{\label{fig:bsr_noise_100_more}{\includegraphics[height=3.6cm]{../fig/gumbel_100_correct_0.2_1_5_5.jpg} }}%
% %      \caption{\label{fig:bsr_more} Pre-limit Bayes simple regret for Gumbel rewards
% %        and $\mbox{Gamma}(100,100)$ prior}
% %     \vspace{-.4cm}
% %   \end{figure}

%   We consider additional numerical experiments for the Gumbel arm setting in
%   Section~\ref{section:experiments} with further benchmarking across other
%   standard adaptive sampling policies.

% We compare our methods with Top-Two Thompson Sampling (TTTS) \citep{Russo20}, adapted to the batch-limit setting. That is,
% as with our our Bayesian batch policies, the policy maintains Gaussian posterior beliefs for the arm means that are updated
% with Gaussian approximations for the arm rewards. We denote this policy as ''Batch-Limit TTTS''. We also compare with
% the batch-limit Thompson sampling policy, but since it performs almost identically to TTTS in our experiments, we omit these results.

% We also consider the myopic policy, which selects the sampling allocation that maximizes the one-step lookahead $Q$-function
% (for the limiting Gaussian sequential experiment), and denote it as ''KG'', due to its similarity with the Knowledge Gradient (KG) policy \citep{FrazierPoDa08}. 
% Explicitly the sampling allocation at state $(\mu_{t},\sigma_{t})$ is

% \[
%   \pi^{\text{KG}}(\mu_{t},\sigma_{t}) = \argmax_{\bar{\pi}\in \Delta_{K}} Q_{T-1}^{\bar{\pi}} (\mu_{t}, \sigma_{t})
%   = \argmax_{\bar{\pi}\in \Delta_{K}}\E_{T-1}[\max_{a} \mu_{T,a}]
% \]

% Note that this is not identical to the KG policy in \citep{FrazierPoDa08}, which only samples arms one at a time. 
% Rather this can be thought of as a ''randomized'' KG policy.

% In Figure~\ref{fig:bsr_more}a) and b), policy gradient and $Q$-myopic are able to outperform other adaptive policies. In particular,
% when the measurement variance is small and as the number of reallocations grows, our non-myopic methods achieve a 
% larger performance gap compared to the myopic policy.



% We also consider an alternate setting with Bernoulli rewards. The purpose of these experiments
% is to compare our batch policies with Bayesian policies that know the true reward distribution.
% Specifically, we compare our methods with Bernoulli Top-Two Thompson Sampling (Bern-TTTS), which updates
% Beta posterior beliefs after each batch using the cumulative rewards.

% As in Section~\ref{section:experiments}, we consider $K = 10, 100$ arms with
% up to $T=10$ batches with $100$ samples in each batch.  The rewards of each
% treatment arm are given by $R_{a} \sim Bernoulli(\theta_{a})$ and each
% parameter $\theta_{a}$ is drawn from independent $Beta(100,100)$
% distributions.  The Gaussian sequential approximation for each batch gives
% $\bar{R}^n_{t,a} \sim N\left(\mu_a,s^{2}_{a}/(10\pi_{t})\right)$, with
% $s^{2}_{a}\approx 1/4$ as under the prior $\theta_{a}$ will be close to $1/2$.
% To maintain conjugacy, we also approximate the prior with a normal
% distribution with the same mean ($\mu_{0,a} = 1/2$) and standard deviation
% ($\sigma_{0,a} \approx 0.03$).

% In Figure \ref{fig:bsr_more}, we observe that despite our methods relying on approximations for the reward distribution and the prior,
% they are able to outperform Bern-TTTS, which knows the true reward distribution and prior.

% \begin{figure}[t]
%   %\vspace{-1.4cm}
%     \centering
%     \subfloat[\centering Relative gains over uniform allocation ($K = 10$).]{{\includegraphics[height=3.8cm]{../fig/bern_10_correct_pct_regret_0.25.jpg} }}%
%     %\subfloat{{\includegraphics[height=3.6cm]{../fig/gumbel_10_correct_0.2_1_5_5.jpg} }}%
%     \vspace{-.4cm}
%     \setcounter{subfigure}{0}
%      \subfloat[\centering Relative gains over uniform allocation ($K = 100$).]{\label{fig:bern}{\includegraphics[height=3.8cm]{../fig/bern_100_correct_pct_regret_0.25.jpg} }}%
%      %\subfloat[\centering Relative gains over uniform allocation for $s_a^2  \in \{\frac{1}{5}, 1, 5\}$ with $T = 5$ (above: $K = 10$; below: $K = 100$).]{\label{fig:bsr_noise_100_more}{\includegraphics[height=3.6cm]{../fig/gumbel_100_correct_0.2_1_5_5.jpg} }}%
%      \vspace{.4cm}
%      \caption{\label{fig:bsr_more} Pre-limit Bayes simple regret for Bernoulli rewards
%        and $\mbox{Beta}(100,100)$ prior}
%     %vspace{-.4cm}
%   \end{figure}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
