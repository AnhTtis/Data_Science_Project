\section{Introduction}
\label{section:introduction}

Experimentation is the basis of scientific decision-making for medical
treatments, engineering solutions, policy-making, and business products
alike. Since experimenting is typically expensive or risky (e.g., clinical
trials), the cost of collecting data poses a central operational
constraint. Simultaneously, as policy interventions and engineering solutions
become more sophisticated, modern experiments increasingly involve many
treatment arms. When the differences in average rewards across arms (average
treatment effects) are small relative to the sample size, statistical power is
of fundamental concern~\citep{KohaviLoSoHe09, ButtonIoMoNoFlRoMu13,
  CziborJiLi19}. Even for online platforms that can automatically deploy
experiments to millions to billions of users, typical A/B tests are
underpowered as they involve incremental changes to a product that have a
small relative impact on key business metrics such as revenue or user
satisfaction~\citep{KohaviLoSoHe09, KohaviDeFrLoWaXu12,
  KohaviDeFrWaXuPo13}. When there is interference across individuals,
treatments may be randomized over entire markets or geographic regions
severely limiting statistical power~\citep{ImbensRu15}.

Adaptive allocation of measurement effort can significantly improve
statistical power and allow reliable identification of the optimal
decision/treatment.  Accordingly, adaptive methods---dubbed pure-exploration
multi-armed bandit (MAB) algorithms---have received tremendous attention since
the foundational works of Thompson, Chernoff, Robbins, and
Lai~\citep{Thompson33, Chernoff59, Robbins52, LaiRo85}. Most of these
algorithms are specifically designed to enjoy strong theoretical guarantees as
the number of reallocation epochs grows to infinity~\citep{BubeckCe12,
  LattimoreSz19, Slivkins19}. However, standard frameworks cannot model
typical experimentation paradigms where adaptive reallocation incurs high
operational cost. Although a universal assumption in the MAB literature,
unit-level continual reallocation of sampling effort is often expensive or
infeasible due to organizational cost and delayed feedback. Even in online
platforms with advanced experimentation infrastructures, engineering
difficulties and lack of organizational incentives deter continual
reallocation at the unit level~\citep{SculleyEtAl15, AgarwalEtAl16,
  BakshyEtAl18, NamkoongDaBa20}.

Due to the challenges associated with reallocating measurement effort, typical
real-world adaptive experiments employ a few reallocation epochs in which
outcomes are measured for many units in parallel (``batches'')
\citep{BakshyEtAl18, OfferWestortCoGr20, JobjornssonScMuFr22, ChowCh08,
  KasySa21, EspositoSa22}.
% ~\hn{Cite
% real-world experimentation papers here: classical online ads papers by
% Langford et al., Kasy's recent applied papers, Molly Offer-Westort, applied
% papers from Eytan's team, Dan's TS tutorial also has some references that
% may be potentially relevant}. 
Motivated by these operational considerations, we develop and analyze batched
adaptive experimentation methods tailored to a handful of reallocation
opportunities. Our main conceptual contribution is the formulation of a
dynamic program that allows designing adaptive methods that are near-optimal
for the fixed number of reallocation epochs. Algorithms designed from our
framework can flexibly handle any batch size, and are automatically tailored
to the instance-specific measurement noise and statistical
power. Specifically, we use a normal approximation for aggregate rewards to
formulate a \emph{Gaussian sequential experiment} where each experiment epoch
consists of draws from a Gaussian distribution for each arm. The dynamic
program solves for the best adaptive allocation where noisy Gaussian draws
become more accurate with increased sampling allocation (see
Figure~\ref{fig:arm-diagram}).

\begin{figure}[t]

  \centering
  \includegraphics[height=5cm]{./fig/arm_diagram.png}%
  \vspace{0.4cm}
  \caption{\label{fig:arm-diagram} Gaussian sequential experiment with $T=3$
    reallocation epochs. Bar visualizes sampling allocations at each epoch
    and bell curves depict normal approximations of the aggregated reward (sample mean)
    distribution.
    %\hn{Let's delete the final $\cdots$ to indicate a short horizon problem}
  }
    % \vspace{-1cm}
\end{figure}

Although we use (frequentist) central limit-based normal approximations to
derive the Gaussian sequential experiment, our proposed dynamic programming
framework solves for adaptive allocations using Bayesian approaches. Unlike
standard Bayesian bandit formulations (e.g.,~\citep{FrazierPoDa08, KasySa21,
  Russo20}) that require distributional knowledge of individual rewards, we
only use a prior over the \emph{average rewards} and our likelihood functions
over average rewards are Gaussian from the CLT.  Our
formulation thus allows leveraging prior information on average rewards
constructed using the rich reservoir of previous experiments, but retains some
of the advantages of model-free frequentist methods. Computationally, sampling
allocations derived from our Gaussian sequential experiment can be computed
offline after each epoch, in contrast to typical Bayesian bandit algorithms
that require real-time posterior inference (e.g., top-two Thompson
sampling~\citep{Russo20}).

Our formulation provides a computation-driven framework for algorithm
design. Despite continuous and high-dimensional state/action spaces, our
limiting dynamic program offers a novel computational advantage: the Gaussian 
sequential experiment provides a \emph{smoothed} model of partial feedback
for which sample paths are fully \emph{differentiable} with respect to the sampling
probabilities chosen by the experimenter.  By simulating a trajectories of the
experiment, we can calculate performance metrics (e.g., cumulative or simple
regret) and compute policy gradients through modern auto-differentiation
softwares such as Tensorflow~\citep{TensorFlow15} or PyTorch~\citep{PyTorch19}.
These gradients can be used to directly optimize a planning problem over
allocations, e.g., policy gradient or policy iteration. We demonstrate our
framework by providing efficient implementations of approximate dynamic
programming (ADP) and reinforcement learning (RL) methods, and evaluating them
through \emph{empirical benchmarking}. Our algorithm development and
evaluation approach is in stark contrast to the bandit literature that largely
operates on mathematical insight and researcher ingenuity~\citep{MinMoRu2020}.


% In our formulation, standard Monte Carlo
% approximations of value functions are \emph{differentiable} with respect to
% the sampling allocations, allowing the application of stochastic
% gradient-based methods. While standard bandit formulations rely on theoretical
% insights due to their complexity, our formulation provides a computational
% framework for algorithmic development, leading to policies that are effective
% even for small time horizons.

Our empirical analysis highlights a simple yet effective heuristic,
$\algofull$, which iteratively solves for the optimal static allocation that
only uses currently available information. Similar to model predictive control
(MPC), the allocation is used in the corresponding period and newly collected
observations are used to update the planning problem in the next
period. Through extensive empirical validation, we demonstrate $\algo$
consistently provides major gains in decision-making performance over a rich
set of benchmark methods. Out of 640 problem instances we consider---varying
across number of reallocation opportunities, number of arms, reward
distributions, priors, etc.---$\algo$ achieves the best performance in 493
(77\%) of them despite its simplicity.  As a summary of our findings,
Figure~\ref{fig:bern-bsr-batch} compares the performance of $\algo$ against
standard batch policies such as oracle Thompson sampling-based policies that
have full distributional knowledge of individual rewards.  Despite relying on
Gaussian approximations, $\algo$ provides significant power gains in hard
instances with high measurement noise.  Overall, our approach expands the
scope of adaptive experimentation to settings standard adaptive policies
struggle with, involving few reallocation epochs, low signal-to-noise ratio,
and unknown reward distributions.




\begin{figure}[t]
  
  \centering \hspace{-.9cm} \subfloat[\centering Batch size $= 100$
samples.]{\label{fig:bern_bsr_100}{\includegraphics[height=5cm]{./fig/reg_Bernoulli_100_100_1_Flat.png}}
} \hspace{.3cm} \subfloat[\centering Batch size $= 10,000$
samples.]{\label{fig:bern_bsr_10000}{\includegraphics[height=5cm]{./fig/reg_Bernoulli_10000_100_1_Flat.png}
}}% \vspace{.4cm}

  \caption{\label{fig:bern-bsr-batch} Relative gains over the uniform
allocation as measured by the Bayes simple regret for the finite batch problem
with $K = 100$ treatment arms.  Individual rewards are Bernoulli with a
$\mbox{Beta}$ prior. Despite relying on normal approximation of aggregate
rewards over a batch, $\algo$ delivers substantial performance gains even over
oracle algorithms that know the true reward model.  These gains persist even
for small batch sizes in Figure~\ref{fig:bern_bsr_100} where batch size is
equal to the number of arms.}
\end{figure}


\paragraph{Paper outline} We begin our discussion by showing that the Gaussian
sequential experiment is a valid approximation as the batch size becomes large
(Section~\ref{section:formulation}). We study the admissible regime where
differences in average rewards scale as $1/\sqrt{n}$, where $n$ is the batch
size at each epoch.  The scaling is standard in formalizing statistical power
in inferential scenarios~\citep{VanDerVaart98, LeCamYa00}. By extending this
conceptual framework to sequential experiments, we show that normal
approximations universal in statistical inference is also useful for designing
adaptive experimentation methods. In Section~\ref{section:algorithms}, we
observe the Gaussian sequential experiment can be represented by a Markov
decision process over the experimenter's beliefs on average rewards.  At every
reallocation epoch, state transitions in the MDP are governed by posterior
updates based on new observations.

We illustrate our computation-driven paradigm to algorithm development in
Section~\ref{section:algorithms-adp} and~\ref{section:experiments}.  Our MDP
formulation gives rise to several ADP and RL-based policies, which we detail
in Section~\ref{section:adp}.  Through empirical benchmarking, we find that a
simple MPC-style policy, $\algo$, consistently outperforms even carefully
tuned RL methods (Section~\ref{section:benchmarking}).  Motivated by its
strong empirical performance, we provide basic theoretical insights on $\algo$
in Section~\ref{section:rho-analysis}. We show $\algo$ is optimal among those
that do not optimize for potential future feedback and in particular, is
always better than the uniform allocation---the de facto standard in practice.
By noting that $\algo$ can be seen as a dynamic extension of the single-batch
problem analyzed by~\citet{FrazierPo10}, we also characterize how $\algo$
calibrates exploration when the remaining sampling budget is large.  In
Section~\ref{section:experiments}, we perform a thorough empirical comparison
with standard batched bandit policies under diverse settings, e.g., low vs
high measurement noise, large vs small batches, flat vs concentrated priors.
We summarize our empirical benchmarking in the interactive
app~\eqref{eqn:streamlit}.

% Although our exposition focuses on best-arm identification, our MDP framework
% and model predictive control policy can accommodate a myriad of alternative
% objectives and constraints. This flexibility is in stark contrast to standard
% adaptive policies that require ad-hoc adjustments tailored for each setting.

% The strong empirical performance of $\algo$
% (Section~\ref{section:experiments}) impels us to analyze its theoretical
% properties (Section~\ref{section:asymptotics}). Even in the asymptotic
% Gaussian sequential experiment, the performance of $\algo$ is hard to quantify
% when epochs are few, a well-recognized difficulty in the
% literature~\citep{FrazierPo10}.  Instead, we focus on a tractable yet
% adversarial regime for our policy: since $\algo$ does not optimize over
% possible future observations, their suboptimality will grow with the number of
% available reallocation epochs (horizon $T$). We characterizes the behavior of
% $\algo$ in the large horizon limit, and provide an understanding of how it
% calibrates exploration when the remaining sampling budget is large.

% We characterize the frequentist suboptimality gap of $\algo$ in the limit
% $T \to \infty$, when observations are governed by a single unknown
% distribution.  Although our primary interests lie with small horizon
% problems in practice, our theoretical results provide assurance that $\algo$
% has bounded suboptimality even when one can adapt the allocation infinitely
% many times.

% Case study paragraph Finally, we demonstrate the potential practical impact
% of our method through a case study with real data
% (Section~\ref{section:case}). Since machine learning models require large
% supervised datasets, industrial models are largely trained to predict
% abundant yet frivolous short-term outcomes such as user clicks. To assess
% the operational impact of a new ML model, online platforms run experiments
% to measure whether the model improves key business metrics like long-term
% customer satisfaction, e.g., a platform may assess many configurations of a
% ML-driven recommendation system. To illustrate practical experimentation
% scenarios, we study an advertising auction example based on a dataset from
% Criteo~\citep{DiemertMeynetGaLe17} to demonstrate that our adaptive
% experimentation algorithms can guide sampling effort to efficiently identify
% the optimal ML-driven bidding strategy.

% In the rest of the paper, we first derive our asymptotic formulation and
% algorithms in Sections~\ref{section:formulation}
% and~\ref{section:algorithms}. Then, we present empirical and theoretical
% analysis of our main proposed adaptive algorithm, $\algo$, in
% Sections~\ref{section:experiments} and~\ref{section:asymptotics}.
%, alongside a case study in Section~\ref{section:case}. 

We defer a full discussion of the literature to the end of the paper to
maximize clarity.  Our asymptotic formulation and resulting algorithms are
new, but intimately connected to the vast body of work on adaptive allocation
of measurement effort. In Section~\ref{section:discussion}, we situate the
current paper across several fields, such as pure-exploration MABs, batched
MABs, ranking \& selection in simulation optimization, diffusion limits for
MABs, as well as various works studying Gaussian environments related to the
one we outline in Figure~\ref{fig:arm-diagram}.



% although any single product improvement may yield small relative increase in
% revenue, absolute gains are nevertheless substantial. The relative
% performance differential between arms can thus be difficult to discern even
% with a large sampling budget, particularly when the population is stratified
% into groups~\citep{WernerfeltTuShMo22}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
