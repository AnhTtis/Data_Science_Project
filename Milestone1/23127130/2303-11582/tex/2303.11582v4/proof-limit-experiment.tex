\section{Derivations of Gaussian sequential experiment}


\subsection{Proof of Theorem \ref{theorem:limit}}
\label{section:proof-limit}


Let $s_{t, a}^{2} \defeq s_a^{2}/b_{t}$ be the rescaled measurement variance.
Recall the Gaussian sequential experiment given in Definition~\ref{def:gse}:
for all $0< t \le T-1$
\begin{equation*}
  G_{0}\sim N(h\pi_{0},\Sigma_{0}),
  ~~G_{t}|G_{t-1},\ldots,G_{0} \sim N(h\pi_{t}(G_{1},...,G_{t-1}),\Sigma_{t})
\end{equation*}
where $\Sigma_{t}=\text{diag}(s_{t,a}^{2}\pi_{t,a}(G_{1},...,G_{t-1}))$ and
$\Sigma_{0}=\text{diag}(s_{t,a}^{2}\pi_{0,a})$.

\paragraph{Induction} We use an inductive argument to prove the weak
convergence~\eqref{eqn:weak-convergence} for a policy $\pi$ and reward process
$R_{a,t}$ satisfying
Assumptions~\ref{assumption:reward},~\ref{assumption:policy}.  For the base
case, let $I_{0} \defeq \{ a \in [K]: \pi_{0,a} > 0 \}$ denote the arms with a
positive sampling probability (this is a deterministic set).  For the arms not in
$I_{0}$,
$\sqrt{n}\bar{R}^{n}_{0} = 0 = N(h_{a}\pi_{0,a}, s_{t,a}^{2}\pi_{0,a})$.  For
the remaining arms, by the Lindeberg CLT, we have that
$\Pstate_{0} \cd N(\pi_{0}h, \Sigma_{0})$.

Next, suppose
\begin{equation}
  \label{eqn:inductive-hypothesis}
  (\sqrt{n}\bar{R}^{n}_{0},\ldots,\sqrt{n} \bar{R}^{n}_{t})
  \cd (G_{0},...,G_{t}).
\end{equation}
To show weak convergence to $G_{t+1}$, it is sufficient to show 
\begin{equation}
  \label{eqn:inductive-weak-convergence}
  \E[f(\sqrt{n}\bar{R}^{n}_{0},\ldots,\sqrt{n} \bar{R}^{n}_{t+1})] \to
  \E[f(G_{0},...,G_{t+1})]~~\mbox{for any
bounded Lipschitz function}~f
\end{equation}
Fixing a bounded Lipschitz $f$, assume without loss of generality that
\begin{equation*}
  \sup_{x,y\in\mathbb{R}^{(t+1)\times k}}|f(x)-f(y)|\leq 1
  ~~~\mbox{and}~~~f\in\text{Lip}(\R^{(t+1)\times K}).
\end{equation*}

We first set up some basic notation. For any
$r_{0:t} \in \R^{(t+1) \times \numarm}$, define the conditional expectation
operator on a random variable $W$
\begin{equation*}
  \E_{r_{0:t}}[W] \defeq \E\left[ W \Bigg| \sqrt{n} \left\{\bar{R}^n_s\right\}_{s=0}^t = r_{0:t}\right].
\end{equation*}
Then, conditional on realizations of previous estimators up to time $t$, we
define a shorthand for the conditional expectation of $f$ and its limiting
counterpart.  Suppressing the dependence on $f$, let
$g_{n},g:\R^{t\times K} \to \R$ be
\begin{equation}
  \begin{aligned}
    \label{eqn:gs}
  g_{n}(r_{0:t}) &\defeq
             \E_{r_{0:t}}\left[f\left(\sqrt{n}\bar{R}_{0}^{n}, \ldots,
             \sqrt{n}\bar{R}_{t}^{n},\sqrt{n}\bar{R}_{t+1}^{n}\right)\right]
             = \E\left[f\left(r_{0:t}, \sqrt{n}\bar{R}_{t+1}^{n}\right)\right] \\
  g(r_{0:t}) & \defeq \E_{r_{0:t}}\left[f\left(\sqrt{n}\bar{R}_{0}^{n}, \ldots,
         \sqrt{n}\bar{R}_{t}^{n},\Sigma_{t+1}^{1/2}(r_{0:t})Z + h\pi_{t+1}(r_{0:t})\right)\right] \\
         & = \E\left[f\left(r_{0:t}, \Sigma_{t+1}^{1/2}(r_{0:t})Z + h\pi_{t+1}(r_{0:t})\right)\right] 
\end{aligned}
\end{equation}
where $Z\sim N(0,I)$ and the conditional covariance is determined by the
allocation $\pi_{t+1,a}(r_{0:t})$
\begin{equation}
  \label{eqn:cov}
  \Sigma_{t+1}(r_{0:t}) \defeq \text{diag}\left(\pi_{t+1,a}(r_{0:t})s_{t+1,a}^{2}\right).
\end{equation}
Conditional on the history $r_{0:t}$, $\sqrt{n}\bar{R}_{t+1}^{n}$ depends on
$r_{0:t}$ only through the sampling probabilities $\pi_{t+1,a}(r_{0:t})$.

To show the weak convergence~\eqref{eqn:inductive-weak-convergence}, decompose the
difference between
$\E[f(\sqrt{n}\bar{R}^{n}_{0},\ldots,\sqrt{n} \bar{R}^{n}_{t})]$ and
$\E[f(G_{0},\ldots,G_{t})]$ in terms of $g_{n}$ and $g$
\begin{align}
  &|\mathbb{E}[f(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t+1}^{n})]
     - \mathbb{E}[f(G_{0},...,G_{t+1})]| \label{eqn:start} \\ 
  & =|\mathbb{E}[g_{n}(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]
    -\mathbb{E}[g(G_{0},...,G_{t})]|  \nonumber  \\
  & =|\mathbb{E}[g_{n}(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]
    -\mathbb{E}[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]|
    +|\E[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]-\mathbb{E}[g(G_{0},...,G_{t})]| \nonumber
\end{align}
By Assumption~\ref{assumption:policy}\ref{item:continuity} and dominated
convergence, $g$ is continuous almost surely under $(G_0, \ldots, G_{t})$.
From the inductive hypothesis~\eqref{eqn:inductive-hypothesis}, the continuous mapping theorem implies
\begin{equation*}
  \E[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})] \to \mathbb{E}[g(G_{1},...,G_{t})].
\end{equation*}

\paragraph{Uniform convergence of $g_n \to g$}
It remains to show the convergence
$\E[g_{n}(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})] \to
\E[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]$.  While one would
expect pointwise convergence of $g_n(r_{0:t})$ to $g(r_{0:t})$ by the CLT,
proving the above requires controlling the convergence across random
realizations of the sampling probabilities
$\pi_{t+1,a}(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})$.  This is
complicated by the fact that we allow the sampling probabilities to be zero.  To
this end, we use the Stein's method for multivariate distributions to provide
rates of convergence for the CLT that are uniform across realizations
$r_{0:t}$. We use the bounded Lipschitz distance $d_{\text{BL}}$ to metrize
weak convergence.
% \begin{equation*}
%   d_{\text{BL}}(\mu, \nu) \defeq \sup\left\{
%     |\mathbb{E}_{R \sim \mu}\bar{f}(R)-\mathbb{E}_{R \sim \nu}\bar{f}(R)|:
%     \bar{f}\in\text{Lip}(\mathbb{R}^{\numarm}),
%     \sup_{x,y\in\mathbb{R}^{\numarm}}|\bar{f}(x)-\bar{f}(y)|\leq1\right\}.
% \end{equation*}
% and the 1-Wasserstein distance $d_{W}$
% \begin{equation*}
%   d_{W}(\mu, \nu) \defeq \sup\left\{
%     |\mathbb{E}_{R \sim \mu}\bar{f}(R)-\mathbb{E}_{R \sim \nu}\bar{f}(R)|:
%     \bar{f}\in\text{Lip}(\mathbb{R}^{\numarm})\right\}.
% \end{equation*}
% in order to metrize weak convergence.
                               
To bound $|\E[g_{n}(\Pstate_{0}, \ldots, \Pstate_{t})] - g(\Pstate_{0}, \ldots, \Pstate_{t})|$,
decompose
\begin{align}
  & |\E[g_{n}(\Pstate_{0}, \ldots, \Pstate_{t}) - g(\Pstate_{0}, \ldots, \Pstate_{t})]| \nonumber  \\
  & = |\E[\E_{\pstate_{0:t}}[f(\pstate_{0:t}, \Pstate_{n,t+1})
    - f(\pstate_{0:t},
    \Sigma_{t+1}(\pstate_{0:t})^{1/2}Z + \pi_{t+1}(\pstate_{0:t})h)]]|
    \nonumber \\
    & \leq \E[\E_{\pstate_{0:t}}[d_{\text{BL}}(Q_{n,t+1}(\pstate_{0:t}),
    \Sigma_{t+1}^{1/2}(\pstate_{0:t})Z)]] \nonumber \\
  & \leq \E[\E_{\pstate_{0:t}}[d_{\text{BL}}(Q_{n,t+1}(\pstate_{0:t}),
    \Sigma_{n,t+1}^{1/2}(\pstate_{0:t})Z)]
    + \E_{\pstate_{0:t}}[d_{\text{BL}}(
    \Sigma_{n,t+1}^{1/2}(\pstate_{0:t})Z,
    \Sigma_{t+1}^{1/2}(\pstate_{0:t})Z)]]
\label{eqn:bl-triangle}
\end{align}
where $Q_{n,t+1}(\pstate_{0:t})$ is the demeaned estimator
$$Q_{n,t+1}(\pstate_{0:t}) := \Pstate_{t+1}(\pstate_{0:t}) - \pi_{t+1}(\pstate_{0:t})h 
= \frac{1}{\sqrt{b_{t+1}n}}\sum_{j=1}^{b_{t+1}n} \left( \frac{\xi_{j}^{t+1}R_{j}^{t+1}}{\sqrt{b_{t+1}}} - \frac{\pi_{t+1}(\pstate_{0:t})h}{\sqrt{b_{t+1}n}} \right)$$
and the covariance $\Sigma_{n,t+1}(\pstate_{0:t})$ is
\begin{align}
  \label{eqn:cov-n}
  \Sigma_{n,t+1}(\pstate_{0:t})
  & \defeq \text{Cov}
  \left(\frac{\xi_{j}^{t+1}R_{j}^{t+1}}{\sqrt{b_{t+1}}}\right) \\
  & = \Sigma_{t+1}(\pstate_{0:t}) 
  + \frac{1}{b_{t+1}n}\text{diag}(\pi_{t+1,a}(\pstate_{0:t})h_{a}^{2})
  - \frac{1}{b_{t+1}n}(\pi_{t+1}(\pstate_{0:t})h)
  (\pi_{t+1}(\pstate_{0:t})h)^{\top}. \nonumber
\end{align}
To ease notation, we often omit the dependence on $\pstate_{0:t}$. Note that
$\Sigma_{n,t+1}$ is not quite equal to the covariance matrix $\Sigma_{t+1}$
appearing in the limiting Gaussian sequential experiment~\eqref{eqn:cov}.
This is because for any finite $n$, the covariance of this Gaussian
$\Sigma_{n,t+1}(r_{0:t})$ not only includes the variance of the arm rewards
but also correlations that emerge from sampling arms according to a random
multinomial vector (rather than a deterministic allocation).  This gap
vanishes as $n\to\infty$ and the covariance matrix converges to the diagonal
covariance matrix $\Sigma_{t+1}(r_{0:t})$.

\paragraph{Bounding the first term in inequality~\eqref{eqn:bl-triangle}}
The first term in inequality~\eqref{eqn:bl-triangle} measures the distance
between the sample mean $Q_{n,t+1}$ and its Gaussian limit.  Before we
proceed, it is helpful to define the following quantities, which describe
smoothness of the derivatives of any function $f\in\mathcal{C}^{3}$
\[
  M_{1}(f) = \sup_{x} \norm{\nabla f}_{2} 
  ~~~~M_{2}(f) = \sup_{x} \norm{\nabla^{2}f}_{op} 
  ~~~~M_{3}(f) = \sup_{x} \norm{\nabla^{3}f}_{op}
\]
The following bound, which we prove in
Section~\ref{section:proof-clt-sample-mean}, quantifies the rate of
convergence for the CLT using the multivariate Stein's
method~\citet{Meckes2009stein}. Recall that $\epsilon$ is the noise in the
rewards~\eqref{eqn:rewards}.
\begin{proposition}
  \label{prop:clt-sample-mean}
  For any $f \in \mathcal{C}^{3}$,
  \[
    |\E f(Q_{n,t+1}) - \E f(\Sigma_{n,t+1}^{1/2} Z) |
    \leq 
    % \frac{\sqrt{\numarm}}{4(b_{t+1}n)^{1/2}} M_{2}(f) \sqrt{\frac{\E \norm{\epsilon}_{2}^{4}}{b_{t+1}^{2}}}
    % + \frac{8}{9(b_{t+1}n)^{1/2}} M_{3}(f) \frac{\E \norm{\epsilon}_{2}^{3}}{b_{t+1}^{3/2}}
    C_{1} n^{-1/2} M_{2}(f) + C_{2}n^{-1/2} M_{3}(f)
  \]
  where $C_{1}$ and $C_{2}$ depend polynomially on $K,b_{t+1},h,s^{2},\norm{\epsilon}_{2}^{3}, \norm{\epsilon}_{2}^{4}$.
\end{proposition}
%\hn{Can we consistently use $f$ for test functions instead of $f$?}

It remains to show convergence of $Q_{n,t+1}$ to $\Sigma_{n,t+1}Z$ for
Lipschitz test functions, which is required to control the bounded Lipschitz
distance.  We use standard Gaussian smoothing arguments found in
\citet{Meckes2009gauss}: by convolving the test function with a Gaussian
density, one obtains a smoother function for which the result of
Proposition~\ref{prop:clt-sample-mean} is applicable. At the same time, the
amount of smoothing is controlled to ensure the bias with the original test
function is small.

%\hn{I assume you meant $\phi = \varphi$}
\begin{lemma}[{\citet[Corollary 3.5]{Meckes2009gauss}}]
\label{lemma:convolution}
For any 1-Lipschitz function $f$, consider the Gaussian convolution 
$(f * \phi_{\delta})(x) := \E[f(x + \delta Z)]$, where $Z\sim N(0,I)$.
\[
  \begin{aligned}
  M_{2}(f * \phi_{\delta}) 
  &\leq M_{1}(f) \sup_{\theta:\norm{\theta}_{2} = 1} \norm{\nabla \phi_{\delta}^{\top} \theta}
  \leq \sqrt{\frac{2}{\pi}} \frac{1}{\delta} \\
  M_{3}(f * \phi_{\delta}) 
  &\leq M_{1}(f) \sup_{\theta:\norm{\theta}_{2} = 1} \norm{\theta^{\top} \nabla^{2} \phi_{\delta} \theta}
  \leq \frac{\sqrt{2}}{\delta^{2}}
  \end{aligned}
\]
Moreover, for any random vector $X$,
$$ \E[(f * \phi_{\delta})(X) - f(X)] \leq \E[\delta\norm{Z}_{2}] \leq \delta \sqrt{\numarm} $$
\end{lemma}

Thus, for any 1-Lipschitz function $f$, we have the bound
\[
  \begin{aligned}
    |\E f(Q_{n,t+1}) - \E f(\Sigma_{n,t+1}^{1/2} Z) |
    &\leq |\E f(Q_{n,t+1}) - \E (f * \phi_{\delta})(Q_{n,t+1}) |
    + |\E (f * \phi_{\delta})(Q_{n,t+1}) - \E (f * \phi_{\delta})(\Sigma_{n,t+1}^{1/2} Z) | \\
    &\qquad + |\E (f * \phi_{\delta})(\Sigma_{n,t+1}^{1/2} Z)
    - \E f(\Sigma_{n,t+1}^{1/2} Z)| \\
    &\leq 2\delta \sqrt{\numarm}
       +  \sqrt{\frac{2}{\pi}} \frac{1}{\delta} C_{1} n^{-1/2}
        + \frac{\sqrt{2}}{\delta^{2}} C_{2} n^{-1/2}
  \end{aligned}
\]
Optimizing over $\delta$, we obtain
\[
  d_{\text{BL}}(Q_{n,t+1}, \Sigma_{n,t+1}^{1/2} Z)
  \leq C_{t+1} n^{-1/6}
\]
for some constant $C_{t+1}$ that depends only on $K, b_{t+1}, \E \norm{\epsilon}_{2}^{3}, \E \norm{\epsilon}_{2}^{4}$
but not on $n$ or $\pstate_{0:t}$. 

\paragraph{Bounding the second term in inequality~\eqref{eqn:bl-triangle}}
Finally, it remains to show uniform convergence of the second term in the
bound~\eqref{eqn:bl-triangle}. That is, the limiting Gaussian
$N(0, \Sigma_{n,t+1}^{1/2}(r_{0:t}))$ converges uniformly to
$N(0, \Sigma_{t+1}^{1/2}(r_{0:t}))$ as $n\to \infty$:
\begin{equation*}
  \E\left[\E_{r_{0:t}}\left[d_{\text{BL}}\left(\Sigma_{n,t+1}^{1/2}(r_{0:t})Z,
        \Sigma_{t+1}^{1/2}(r_{0:t})Z\right) \right]\right] \to 0.
\end{equation*}
First, for any two measures $(\mu,\nu)$, $d_{\text{BL}}(\mu,\nu)$ is upper
bounded by the total variation distnace $2d_{\text{TV}}(\mu,\nu)$, which is in
turn bounded above by KL divergence $\sqrt{\frac{1}{2}d_{\text{KL}}(\mu,\nu)}$
by Pinsker's inequality. We derive the following concrete bound on the KL
divergence, whose proof we defer to Section~\ref{section:proof-kl-cov}.
\begin{lemma}\label{lemma:kl-cov} The KL-divergence between 
  $N(0, \Sigma_{n,t+1}(\pstate_{0:t}))$ and
  $N(0, \Sigma_{t+1}(\pstate_{0:t}) )$
  is bounded above as follows
\[
 \dkl{N \left( 0,\Sigma_{n,t+1}(\pstate_{0:t}) \right)}{N \left( 0, \Sigma_{t+1}(\pstate_{0:t}) \right)}
  \leq \frac{1}{b_{t+1}n} \left[\frac{\max_{a} h_{a}^{2}/s_{t+1,a}^{2}}{1 - \max_{a} h_{a}^{2}/(s_{a}^{2}+h_{a}^{2})}+\sum_{a}\left(\frac{h_{a}^{2}}{s_{t+1,a}^{2}}\right)\right]
\]
\end{lemma}
\noindent This guarantees that
\[
  \begin{split}
  &\E\left[\E_{r_{0:t}}\left[d_{\text{BL}}\left(\Sigma_{n,t+1}^{1/2}(r_{0:t})Z,
  \Sigma_{t+1}^{1/2}(r_{0:t})Z\right) \right]\right] \\
  &\leq (b_{t+1}n)^{-1/2} \sqrt{2}\left(\frac{\max_{a} h_{a}^{2}/s_{t+1,a}^{2}}{1 - \max_{a} h_{a}^{2}/(s_{a}^{2}+h_{a}^{2})}+\sum_{a}\left(\frac{h_{a}^{2}}{s_{t+1,a}^{2}}\right)\right)^{1/2}
  \end{split}
\]

\paragraph{Conclusion} Altogether, we have shown
\[
  |\E[g_{n}(\Pstate_{0},\ldots,\Pstate_{t}) -
  g(\Pstate_{0},\ldots,\Pstate_{t})]| \leq C_{t+1}n^{-1/6} + D_{t+1}n^{-1/2}
\]
for constants $C_{t+1}, D_{t+1}$ that depend polynomially on $h_{a},s_{a}^{2}$
and higher order moments of $\epsilon_{a}$ as well as on $K$ and $b_{t+1}$.
Thus, this shows that
$\E[f(\Pstate_{0},\ldots,\Pstate_{t+1})] \to \E[f(G_{0},\ldots,G_{t+1})]$ as
$n\to \infty$ for any bounded Lipschitz function $f$, which implies the
desired weak convergence.

\subsubsection{Proof of Proposition~\ref{prop:clt-sample-mean}}
\label{section:proof-clt-sample-mean}

In order to quantify the rate of the CLT, we use the following result by
\citet{Meckes2009stein} which provides a characterization of Stein's method
for random vectors with arbitrary covariance matrices.
\begin{lemma}[{\citet[Theorem 3]{Meckes2009stein}}]
  \label{theorem:meckes}
  Let $(W, W')$ be an exchangeable pair of random vectors in $\R^{K}$. 
  Suppose that there exists $\lambda > 0$, a positive semi-definite matrix $\Sigma$,
  a random matrix $E$ such that
  \[
    \begin{aligned}
      \E[W' - W | W] &= -\lambda W \\
      \E[(W' - W)(W' - W)^{\top}|W] &= 2\lambda \Sigma + \E[E|W]
    \end{aligned}
  \]
  Then for any $f \in \mathcal{C}^{3}$,
  \[
    |\E f(W) - \E f(\Sigma^{1/2} Z) |
    \leq \frac{1}{\lambda}
    \left[ 
        \frac{\sqrt{K}}{4} M_{2}(f) \E \norm{E}_{H.S.}
        + \frac{1}{9} M_{3}(f) \E \norm{W' - W}^{3}
    \right]
  \]
where $\norm{\cdot}_{H.S.}$ is the Hilbert-Schmidt norm.
\end{lemma}


The rest of the proof is similar to that of~\citet[Theorem 7]{ChatterjeeMe08},
with slight modifications due to the fact that we have a non-identity
covariance.  For simplicity, define
$X_{j} := \frac{\xi_{j}^{t+1}R_{j}^{t+1}}{\sqrt{b_{t+1}}} -
\frac{\pi h}{\sqrt{b_{t+1}n}}$.
% \hn{Missing $\pi$ in $\frac{h}{\sqrt{b_{t+1}n}}$}
For any index $j$, we construct an independent copy $Y_j$ of $X_j$.  We
construct an exchangeable pair $(Q_{n,t+1},Q'_{n,t+1})$ by selecting an random
index $I\in \{1,...,b_{t+1}n\}$ chosen uniformly and independently from
$Q_{n,t+1}$ and letting
$$ Q'_{n,t+1} = Q_{n,t+1} - \frac{X_{I}}{\sqrt{b_{t+1}n}} + \frac{Y_{I}}{\sqrt{b_{t+1}n}}. $$

We can observe then that
\[
  \begin{aligned}
    \E [Q'_{n,t+1} - Q_{n,t+1}|Q_{n,t+1}]
    &= \frac{1}{\sqrt{b_{t+1}n}} \E[Y_{I} - X_{I}|Q_{n,t+1}] \\
    &= \frac{1}{(b_{t+1}n)^{3/2}} \sum_{j=1}^{b_{t+1}n} \E[Y_{j} - X_{j}|Q_{n,t+1}] \\
    &= -\frac{1}{b_{t+1}n} Q_{n,t+1}
  \end{aligned}
\]
by independence of $Y_j$ and $Q_{n,t+1}$. This pair satisfies the first condition of 
Theorem~\ref{theorem:meckes} with $\lambda = 1/(b_{t+1}n)$.

% \hn{get rid of transposes}
It also satisfies the second condition of Theorem~\ref{theorem:meckes} with:
% \[
%   \begin{aligned}
%     E_{a,a'} &= \E[(Q'_{n,t+1,a} - Q_{n,t+1,a})(Q'_{n,t+1, a'} - Q_{n,t+1,a'})|Q_{n,t+1}] - \frac{2}{b_{t+1}n} (\Sigma_{n,t+1})_{a,a'} \\
%     &= \frac{1}{b_{t+1}n}  \E[(Y_{I,a} - X_{I,a})(Y_{I,a'} - X_{I,a'})|Q_{n,t+1}] - \frac{2}{b_{t+1}n} (\Sigma_{n,t+1})_{a,a'} \\
%     &= \frac{1}{(b_{t+1}n)^{2}} \sum_{j=1}^{b_{t+1}n} \E[Y_{j,a}Y_{j,a'} + Y_{j,a}X_{j,a'} + Y_{j,a'}X_{j,a} + X_{j,a}X_{j,a'}|Q_{n,t+1}] 
%     - \frac{2}{b_{t+1}n} (\Sigma_{n,t+1})_{a,a'}\\
%     & = \frac{1}{(b_{t+1}n)^{2}} \sum_{j=1}^{b_{t+1}n} \E[X_{j,a}X_{j,a'} - (\Sigma_{n,t+1})_{a,a'}|Q_{n,t+1}] 
%   \end{aligned}
% \]
\begin{align*}
  E_{a,a'} 
& = \frac{1}{(b_{t+1}n)^{2}} \sum_{j=1}^{b_{t+1}n} X_{j,a}X_{j,a'} - (\Sigma_{n,t+1})_{a,a'}
\end{align*}
by independence of $X_{i}$ and $Y_{i}$.
% \hnlong{Can't we just define
%   \begin{align*}
%         E_{a,a'} 
%     & = \frac{1}{(b_{t+1}n)^{2}} \sum_{j=1}^{b_{t+1}n} X_{j,a}X_{j,a'} - (\Sigma_{n,t+1})_{a,a'}
%   \end{align*}
%   }
Thus, we have that
\[
  \begin{aligned}
    \E E_{a,a'}^{2} &= \frac{1}{(b_{t+1}n)^{4}} \sum_{j=1}^{b_{t+1}n} \E \left( \E[X_{j,a}X_{j,a'} - (\Sigma_{n,t+1})_{a,a'}|Q_{n,t+1}] \right)^{2} \\
    &\leq  \frac{1}{(b_{t+1}n)^{4}} \sum_{j=1}^{b_{t+1}n} \E\left[ \left( X_{j,a}X_{j,a'} - (\Sigma_{n,t+1})_{a,a'} \right)^{2} \right]  \\
    &= \frac{1}{(b_{t+1}n)^{3}} \E\left[ \left( X_{1,a}X_{1,a'}  \right)^{2} - (\Sigma_{n,t+1})_{a,a'}^{2}\right]  \\
  \end{aligned}
\]
This gives us a bound on the Hilbert-Schmidt norm:
\[
  \begin{aligned}
    \E \norm{E}_{H.S.} &\leq \sqrt{\sum_{a,a'} \E E_{a,a'}^{2}} 
    \leq \frac{1}{(b_{t+1}n)^{3/2}} \sqrt{\sum_{a,a'}\E \left( X_{1,a}X_{1,a'}  \right)^{2} - (\Sigma_{n,t+1})_{a,a'}^{2}}
    \leq \frac{1}{(b_{t+1}n)^{3/2}} \sqrt{\E\norm{X_{1}}_{2}^{4}}
  \end{aligned}
\]
We can further bound $\E\norm{X_{1}}_{2}^{4}$ by
a constant $C_{1}$ that is polynomial in $h, b_{t+1}^{-1}, s^{2}, \E\norm{\epsilon}_{2}^{3}$, and $\E\norm{\epsilon}_{2}^{4}$.
% \hn{How sure are you
%   about these constants? FYI, it may be easier to just use a universal
%   constant whose value changes line by line. I got something like
%   $\E[\ltwo{\epsilon}^4] + C$ for a universal constant $C$.} 
Finally, we can
bound $\E \norm{Q'_{n,t+1} - Q_{n,t+1}}_{2}^{3}$ as follows:
\[
  \begin{aligned}
    \E \norm{Q'_{n,t+1} - Q_{n,t+1}}_{2}^{3}
    =  \frac{1}{(b_{t+1}n)^{3/2}} \E \norm{Y_{I} - X_{I}}_{2}^{3}
    \leq \frac{1}{(b_{t+1}n)^{3/2}} 8\E \norm{X_{1}}_{2}^{3}
  \end{aligned}
\]
where the final inequality uses independence of $Y_{j}$ and $X_{j}$ as well as Holder's inequality.
We can bound $\E \norm{X_{1}}_{2}^{3}$ by another constant $C_{2}$ that is $h, b_{t+1}^{-1}, s^{2}$, and $\E\norm{\epsilon}_{2}^{4}$.
Plugging these bounds into the statement of Theorem~\ref{theorem:meckes}, we obtain the stated result.



\subsubsection{Proof of Lemma~\ref{lemma:kl-cov}}
\label{section:proof-kl-cov}

We use the expression for the KL-divergence between two multivariate normal distributions
\[
  \begin{aligned}
	& \dkl{N(0,  \Sigma_{n,t+1}(\pstate_{0:t}))}{N(0,\Sigma_{t+1}(\pstate_{0:t}))} \\
	&=\text{tr}\left(( \Sigma_{t+1}(\pstate_{0:t}))^{-1}  \Sigma_{n,t+1}(\pstate_{0:t})\right)
   - m +\log\frac{\det \Sigma_{t+1}(\pstate_{0:t})}{\det \Sigma_{n,t+1}(\pstate_{0:t})} \\
  &=\sum_{a} \left( s_{t+1,a}^{2}\pi_{t+1,a}(\pstate_{0:t}) \right)^{-1} 
  \left(s_{t+1,a}^{2}\pi_{t+1,a}(\pstate_{0:t}) + \pi_{t+1,a}(\pstate_{0:t}) (1 - \pi_{t+1,a}(\pstate_{0:t})) \frac{h_{a}^{2}}{b_{t+1}n} \right)-m \\
  &\qquad +\log\det  \Sigma_{t+1}(\pstate_{0:t}) -\log\det  \Sigma_{n,t+1}(\pstate_{0:t}) \\
	&\leq \sum_{a} \frac{1}{b_{t+1}n} \frac{\left(1-\pi_{t+1,a}(\pstate_{0:t})\right)h_{a}^{2}}{s_{t+1, a}^{2}}
  +\sum_{a}\log\left( s_{t+1,a}^{2}\pi_{t+1,a}(\pstate_{0:t}) \right) \\
  & \qquad-\sum_{a}\log\left(s_{t+1,a}^{2}\pi_{t+1,a}(\pstate_{0:t})+\frac{1}{b_{t+1}n}h_{a}^{2}\pi_{t+1,a}(\pstate_{0:t})\right) 
  + \left(\frac{1}{b_{t+1}n}\frac{\max_{a} h_{a}^{2}/s_{t+1,a}^{2}}{1 - \max_{a} h_{a}^{2}/(s_{t+1,a}^{2}+h_{a}^{2})} \right) \\
	&\leq\frac{1}{b_{t+1}n}\left[\sum_{a}\left(\frac{h_{a}^{2}}{s_{t+1,a}^{2}}\right) + \frac{\max_{a} h_{a}^{2}/s_{t+1,a}^{2}}{1 - \max_{a} h_{a}^{2}/(s_{a}^{2}+h_{a}^{2})}\right]
\end{aligned}
\]

We obtain $\det{\Sigma_{n,t+1}(\pstate_{0:t})^{\top}}$ using the fact that $\Sigma_{n,t+1}(\pstate_{0:t})$ is a rank-one perturbation of a diagonal matrix
\[
  \begin{aligned}
  &\log\det \Sigma_{n,t+1}	\\
  &=\log\det\left( \left( \Sigma_{t+1}+\frac{1}{b_{t+1}n}\text{diag}\left(\pi_{t+1,a}h_{a}^{2}\right)\right)\right) \\
  &\qquad+\log\det\Bigl(I + \left(  \left(\Sigma_{\epsilon,t}+\frac{1}{b_{t+1}n}\text{diag}\left(\pi_{t+1,a}h_{a}^{2}\right)\right)^{\top}\right)^{-1}
   \left(-\frac{1}{b_{t+1}n}(\pi_{t+1}h)(\pi_{t+1}h)^{\top}\right)\Bigr) \\
	&=\sum_{a} \log\left(s_{t+1,a}^{2}\pi_{t+1,a}+\frac{1}{b_{t+1}n}h_{a}^{2}\pi_{t+1,a}\right)  \\
  &\qquad+\log\det\left(I-\frac{1}{b_{t+1}n}( \pi_{t+1}h)^{\top}\left( \left(\Sigma_{t+1}
  +\frac{1}{b_{t+1}n}\text{diag}\left(\pi_{t+1,a}h_{a}^{2}\right)\right)^{\top}\right)^{-1}(\pi_{t+1}h)\right) \\
	&=\sum_{a} \log\left(\pi_{t+1,a}s_{t+1,a}^{2}
  +\frac{1}{b_{t+1}n}\pi_{t+1,a}h_{a}^{2}\right)
  +\log\left(1-\frac{1}{b_{t+1}n} \sum_{a} \pi_{t+1,a}\frac{h_{a}^{2}}{s_{t+1,a}^{2}+(1/b_{t+1}n)h_{a}^{2}}\right) \\
  &\geq \sum_{a} \log\left(\pi_{t+1,a}s_{t+1,a}^{2}+\frac{1}{b_{t+1}n}\pi_{t+1,a}h_{a}^{2}\right)
   + \log\left(1- \frac{1}{b_{t+1}n}\max_{a} \frac{h_{a}^{2}}{s_{t+1,a}^{2}+(1/b_{t+1}n)h_{a}^{2}}\right)
  \end{aligned}
\]

Finally, we use the bound $\log(1-x)\geq \frac{-x}{1-x}$ to obtain
\[
  \begin{aligned}
    \log\det\Sigma_{n,t+1}&\geq \sum_{a}
        \log\left(\pi_{t+1,a}s_{t+1,a}^{2}+\frac{1}{b_{t+1}n}\pi_{t+1,a}h_{a}^{2}\right)
    -  \frac{1}{b_{t+1}n}\frac{\max_{a} \frac{h_{a}^{2}}{s_{t+1,a}^{2}+(1/b_{t+1}n)h_{a}^{2}}}{1 - \max_{a} \frac{h_{a}^{2}}{b_{t+1}ns_{t+1,a}^{2}+h_{a}^{2}}} \\
  &\geq \sum_{a} \log\left(\pi_{t+1,a}s_{t+1,a}^{2}+\frac{1}{b_{t+1}n}\pi_{t+1,a}h_{a}^{2}\right)
   -  \frac{1}{b_{t+1}n}\frac{\max_{a} h_{a}^{2}/s_{t+1,a}^{2}}{1 - \max_{a} h_{a}^{2}/(s_{a}^{2}+h_{a}^{2})} \\
  \end{aligned}
\]






\subsection{Proof of Corollary~\ref{cor:proof-limit-rate}}
\label{section:proof-limit-rate}

We show that for any horizon $T$
\[
  d_{\text{BL}} \left( (\Pstate_{0},\ldots,\Pstate_{T-1} ),(G_{0},\ldots,G_{T-1} ) \right) \leq C \frac{M^{T} - 1}{M - 1} n^{-1/6},
\]
% where $M = \sqrt{(1 + \ltwo{h}^2L^{2} + (s_{*}^{2}/b_{*})\bar{L}^{2}K)}$
where $M = \left(1 + \bar{L} ( s_*^2/b_* \sqrt{K} + \max_{a} |h_{a}|) \right)$, 
$\bar{L}$ is the bound on the Lipschitz constant for $\pi^{1/2}$ and $\pi$, and $s_{*}^{2} = \max_{a} s_{a}^{2}$
and $b_{*} = \min_{0\leq t\leq T-1} b_{t}$.

We prove the statement through induction. For the base case, we have from Proposition~\ref{prop:clt-sample-mean},
Lemma~\ref{lemma:convolution}, and Lemma~\ref{lemma:kl-cov} that $d_{BL}(\Pstate_{0}, N(\pi_{0} h, \Sigma_{0}))$ is bounded by 
$C_{0}n^{-1/6}$, for some constant $C_{0}$ that depends on $\numarm$, $b_{0}$, $h$, $s^{2}$,
and higher moments of $\norm{\epsilon}_{2}$.
Suppose now that this holds up to $t$
\[
  d_{\text{BL}} \left( (\Pstate_{0},\ldots,\Pstate_{t} ),(G_{0},\ldots,G_{t} ) \right) \leq C \sum_{s=0}^{t} M^{s} n^{-1/6}.
\]
We proceed to show that this inequality then holds for $t+1$.

Recall the decomposition~\eqref{eqn:start} for any bounded Lipschitz function
$f$ and corresponding $g, g_n$ defined as in Eq.~\eqref{eqn:gs}.
% \begin{align}
%   &|\mathbb{E}[f(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t+1}^{n})]
%      - \mathbb{E}[f(G_{0},...,G_{t+1})]| \label{eqn:start} \\ 
%   & =|\mathbb{E}[g_{n}(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]
%     -\mathbb{E}[g(G_{0},...,G_{t})]|  \nonumber  \\
%   & =|\mathbb{E}[g_{n}(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]
%     -\mathbb{E}[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]|
%     +|\E[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]-\mathbb{E}[g(G_{0},...,G_{t})]| \nonumber
% \end{align}
% where $g,g_{n}$ are defined in \eqref{eqn:gs}.
From Proposition~\ref{prop:clt-sample-mean}, Lemma~\ref{lemma:convolution}, and Lemma~\ref{lemma:kl-cov}
\[
  |\mathbb{E}[g_{n}(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]
    -\mathbb{E}[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]| \leq C_{t+1} n^{-1/6} + D_{t+1}n^{-1/2}
\]
Thus, there exists some $C'_{t+1}$ such that the above is bounded by $C'_{t+1}n^{-1/6}$. Note that the dependence
of $C'_{t}$ on $t$ is only through a polynomial on $1/b_{t+1}$. Thus we can bound this by a constant $C$ which
will depends on $T$ only through polynomial factors of $b_{*} = \min_{0\leq t\leq T-1} b_{t}$.

To bound the second term in the decomposition~\eqref{eqn:start}, we use the
fact that
\[
  g(r_{0:t}) = \E\left[f\left(r_{0:t}, \Sigma_{t+1}^{1/2}(r_{0:t})Z +
      h\pi_{t+1}(r_{0:t})\right)\right]
\]
is a bounded Lipschitz function.  We seek to identify the Lipschitz
constant for $g$, as we can then utilize the rate for bounded Lipschitz
functions in the induction hypothesis.  For any two $r_{0:t}$, $r'_{0:t}$,
% \[
% \begin{aligned}  
%   |g(r_{0:t}) - g(r'_{0:t})| & \leq \E \ltwo{(r_{0:t}, \Sigma_{t+1}^{1/2}(r_{0:t})Z + h\pi_{t+1}(r_{0:t})) - (r'_{0:t}, \Sigma_{t+1}^{1/2}(r'_{0:t})Z + h\pi_{t+1}(r'_{0:t}))}\\
%   &\leq \E \sqrt{\ltwo{r_{0:t} - r'_{0:t}}^{2} 
%   + \ltwo{\Sigma_{t+1}^{1/2}(r_{0:t})Z - \Sigma_{t+1}^{1/2}(r'_{0:t})Z}^{2}+ \ltwo{h\pi_{t+1}(r_{0:t}) - h\pi_{t+1}(r'_{0:t})}^2} \\
%   &\leq \E \sqrt{\ltwo{r_{0:t} - r'_{0:t}}^{2} 
%   + \ltwo{\Sigma_{t+1}^{1/2}(r_{0:t}) - \Sigma_{t+1}^{1/2}(r'_{0:t})}^{2}\ltwo{Z}^{2}+ \max_{a}(\pi_{t+1,a}(r_{0:t}) - \pi_{t+1,a}(r'_{0:t}))^{2} \ltwo{h}^2} \\
%   &\leq \E \sqrt{(1 + \ltwo{h}^2L^{2})\ltwo{r_{0:t} - r'_{0:t}}^{2} 
%   + (\max_{a} s_{t+1,a}^{2}) \max_{a} (\pi_{t+1,a}^{1/2}(r_{0:t}) - \pi_{t+1,a}^{1/2}(r'_{0:t}))^{2} \ltwo{Z}^{2}} \\
%   & \leq \E \sqrt{(1 + \ltwo{h}^2L^{2} + \max_{t} \max_{a} s_{t+1,a}^{2}\bar{L}^{2}\ltwo{Z}^{2})\ltwo{r_{0:t} - r'_{0:t}}^{2} } \\
%   & \leq \sqrt{(1 + \ltwo{h}^2L^{2} + (s_{*}^{2}/b_{*})\bar{L}^{2}K)}\ltwo{r_{0:t} - r'_{0:t}} \\
% \end{aligned}
% \]
\[
\begin{aligned}  
  |g(r_{0:t}) - g(r'_{0:t})| & \leq \E \ltwo{(r_{0:t}, \Sigma_{t+1}^{1/2}(r_{0:t})Z + h\pi_{t+1}(r_{0:t})) - (r'_{0:t}, \Sigma_{t+1}^{1/2}(r'_{0:t})Z + h\pi_{t+1}(r'_{0:t}))}\\
  &\leq \E \left(\ltwo{r_{0:t} - r'_{0:t}}^{2} 
    + \ltwo{\Sigma_{t+1}^{1/2}(r_{0:t})Z - \Sigma_{t+1}^{1/2}(r'_{0:t})Z}^{2}+ \ltwo{h\pi_{t+1}(r_{0:t}) - h\pi_{t+1}(r'_{0:t})}^2\right)^{\half} \\
  &\leq \ltwo{r_{0:t} - r'_{0:t}} 
  + \E\ltwo{\Sigma_{t+1}^{1/2}(r_{0:t})Z - \Sigma_{t+1}^{1/2}(r'_{0:t})Z}
  + \ltwo{h\pi_{t+1}(r_{0:t}) - h\pi_{t+1}(r'_{0:t})} \\
  &\leq \left(1 + \bar{L} ( s_*^2/b_* \E \ltwo{Z} + \max_{a} |h_{a}|) \right)\ltwo{r_{0:t} - r'_{0:t}}
  % & \leq \E \sqrt{(1 + \ltwo{h}^2L^{2} + \max_{t} \max_{a} s_{t+1,a}^{2}\bar{L}^{2}\ltwo{Z}^{2})\ltwo{r_{0:t} - r'_{0:t}}^{2} } \\
  % & \leq \sqrt{(1 + \ltwo{h}^2L^{2} + (s_{*}^{2}/b_{*})\bar{L}^{2}K)}\ltwo{r_{0:t} - r'_{0:t}} \\
\end{aligned}
\]
using $\sqrt{a+b} \le \sqrt{a} + \sqrt{b}$ and $a - b = (\sqrt{a} + \sqrt{b})(\sqrt{a}-\sqrt{b})$.
% \hnlong{I think this can be simplified:
% \[
% \begin{aligned}  
%   |g(r_{0:t}) - g(r'_{0:t})| & \leq \E \ltwo{(r_{0:t}, \Sigma_{t+1}^{1/2}(r_{0:t})Z + h\pi_{t+1}(r_{0:t})) - (r'_{0:t}, \Sigma_{t+1}^{1/2}(r'_{0:t})Z + h\pi_{t+1}(r'_{0:t}))}\\
%   &\leq \E \left(\ltwo{r_{0:t} - r'_{0:t}}^{2} 
%     + \ltwo{\Sigma_{t+1}^{1/2}(r_{0:t})Z - \Sigma_{t+1}^{1/2}(r'_{0:t})Z}^{2}+ \ltwo{h\pi_{t+1}(r_{0:t}) - h\pi_{t+1}(r'_{0:t})}^2\right)^{\half} \\
%   &\leq \ltwo{r_{0:t} - r'_{0:t}} 
%   + \E\ltwo{\Sigma_{t+1}^{1/2}(r_{0:t})Z - \Sigma_{t+1}^{1/2}(r'_{0:t})Z}
%   + \ltwo{h\pi_{t+1}(r_{0:t}) - h\pi_{t+1}(r'_{0:t})} \\
%   &\leq \left(1 + \bar{L} ( s_*^2/b_* \E \ltwo{Z} + 2 \ltwo{h}) \right)\ltwo{r_{0:t} - r'_{0:t}}
%   % & \leq \E \sqrt{(1 + \ltwo{h}^2L^{2} + \max_{t} \max_{a} s_{t+1,a}^{2}\bar{L}^{2}\ltwo{Z}^{2})\ltwo{r_{0:t} - r'_{0:t}}^{2} } \\
%   % & \leq \sqrt{(1 + \ltwo{h}^2L^{2} + (s_{*}^{2}/b_{*})\bar{L}^{2}K)}\ltwo{r_{0:t} - r'_{0:t}} \\
% \end{aligned}
% \]
% using $\sqrt{a+b} \le \sqrt{a} + \sqrt{b}$ and $a - b = (\sqrt{a} + \sqrt{b})(\sqrt{a}-\sqrt{b})$.
% }
This implies
\[
  \begin{aligned}
  & |\mathbb{E}[f(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t+1}^{n})]
     - \mathbb{E}[f(G_{0},...,G_{t+1})]| \\
  & \leq |\mathbb{E}[g_{n}(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]
  -\mathbb{E}[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]|
  +|\E[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]-\mathbb{E}[g(G_{0},...,G_{t})]| \\
  & \leq C n^{-1/6}
  +  M \cdot C \sum_{s=0}^{t} M^{s} n^{-1/6} 
  = C \sum_{s=0}^{t+1} M^{s} n^{-1/6}
  \end{aligned}
\]
Noting that $\sum_{s=0}^{T-1} M^{s} = \frac{M^{T} - 1}{M - 1}$,
we have the desired inequality.

\subsection{Proof of Corollary~\ref{cor:root-n-rate}}
\label{section:proof-root-n-rate}

The proof proceeds as in the proof of Corollary~\ref{cor:proof-limit-rate}. We show that for any horizon $T$
\[
  d_{\text{BL}} \left( (\Pstate_{0},\ldots,\Pstate_{T-1} ),(G_{0},\ldots,G_{T-1} ) \right) \leq \tilde{C} \frac{M^{T} - 1}{M - 1} n^{-1/2},
\]
where $M = \left(1 + \bar{L} ( s_*^2/b_* K^{1/2} + \max_{a} |h_{a}|) \right)$, 
$\bar{L}$ is the bound on the Lipschitz constant for $\pi^{1/2}$ and $\pi$, and $s_{*}^{2} = \max_{a} s_{a}^{2}$
and $b_{*} = \min_{0\leq t\leq T-1} b_{t}$.

For the base case, note that by Assumption~\ref{assumption:overlap},
$ \max_{a} \pi_{0,a}^{-3}(\mu_{0}, \sigma_{0}) \leq C_{o}$. Applying standard
results for rates of weak convergence of the central limit theorem (see
Lemma~\ref{lemma:clt-rate}), we have that
$d_{BL}(\Pstate_{0}, N(\pi_{0} h, \Sigma_{0}))$ is bounded by $C_{0}n^{-1/2}$,
for some constant $C_{0}$ that depends on $\numarm$ and polynomially on
$C_{o}$, $b_{0}$, $h$, and $s^{2}$.  For the induction step, assume that up to
$t$, the following inequality holds
\[
  d_{\text{BL}} \left( (\Pstate_{0},\ldots,\Pstate_{t} ),(G_{0},\ldots,G_{t} ) \right) \leq \tilde{C} \sum_{s=0}^{t} M^{s} n^{-1/2}.
\]
for $M = \left(1 + \bar{L} ( s_*^2/b_* K^{1/2} + \max_{a} |h_{a}|) \right)$ and a constant $\tilde{C}$ that depends on $\numarm$, $s_{a}^{2}$, $b_{t+1}$, and $h_{a}$.
We proceed to show that this inequality then holds for $t+1$.

Under Assumption~\ref{assumption:overlap}, we can obtain a $n^{-1/2}$ rate for
the central limit theorem specialized for bounded Lipschitz distance.  First,
we establish some facts about the covariance matrix $\Sigma_{n,t+1}(r_{0:t})$
defined in Eq.~\eqref{eqn:cov-n} for the sample average $Q_{n, t+1}(r_{0:t})$.
We omit the dependence on $r_{0:t}$ to ease notation.
\begin{lemma} \label{lemma:min-max-eig} Let $\lambda_{\max}(\cdot)$ and
  $\lambda_{\min}(\cdot)$ denote the maximum and minimum eigenvalues
  respectively. For any realization of $\pstate_{0:t}$, we have 
  \[
    \begin{aligned}
    \lambda_{\max} (\Sigma_{n,t+1} ) &\leq \lambda_{\max}
    \left( \Sigma_{t+1} 
    +\frac{1}{b_{t+1}n}  \diag \left( \pi_{t+1,a}h_{a}^{2} \right) \right)
    \leq \max_{a}\left( \frac{s_{a}^{2} + h_{a}^{2}}{b_{t+1}} \right) \\
    \lambda_{\min} ( \Sigma_{n,t+1} ) 
    & \geq \lambda_{\min}(  \Sigma_{t+1} )
    +\frac{1}{b_{t+1}n}  \lambda_{\min}\left( \diag(\pi_{t+1,a}h_{a}^{2})
    -  (\pi_{t+1}h)(\pi_{t+1}h)^{\top}  \right)\\
    &\geq  \left( \min_{a}\pi_{t+1,a}(\pstate_{0:t}) \right) \left( \min_{a}s_{t+1,a}^{2}  \right).
    \end{aligned}
  \]
\end{lemma}

\begin{proof-of-lemma}
  The first inequality follows since
  $\lambda_{\max}(A + B) \le \lambda_{\max}(A) + \lambda_{\max}(B)$ for
  symmetric matrices $A,B \in \R^{m\times m}$. The second inequality uses
  $\lambda_{\min}(A + B) \geq \lambda_{\min}(A) + \lambda_{\min}(B)$.
  Moreover, for any $b\in \R^{m}$
  \[
  \begin{split}
  & b^{\top}  \left( \text{diag}(\pi_{t+1,a}(\pstate_{0:t})h_{a}^{2}) - (\pi_{t+1}(\pstate_{0:t})h)(\pi_{t+1}(\pstate_{0:t})h)^{\top} \right) b \\
  &= \sum_{a} \pi_{t+1,a}(\pstate_{0:t})h_{a}^{2}b_{a}^{2} - \left( \sum_{a} \pi_{t+1,a}(\pstate_{0:t})h_{a}b_{a} \right)^{2} \geq 0
  \end{split}
  \]
  as by Cauchy-Schwartz:
  \[
   \left( \sum_{a} \pi_{t+1,a}(\pstate_{0:t})h_{a}b_{a} \right)^{2} \leq \left(\sum_{a} \pi_{t+1,a}(\pstate_{0:t}) \right)
   \left(\sum_{a} \pi_{t+1,a}(\pstate_{0:t})h_{a}^{2}b_{a}^{2} \right) \leq \left(\sum_{a} \pi_{t+1,a}(\pstate_{0:t})h_{a}^{2}b_{a}^{2} \right)
  \]
so the matrix is positive semi-definite
\end{proof-of-lemma}

Next, we show convergence of $Q_{n, t+1}(r_{0:t})$ to a standard normal
random vector $Z \sim N(0,I_{m})$ via a classical result, which is a corollary
of~\citet[Theorem 1]{Bhattacharya70}.  The following result explicitly
quantifies how the rate of convergence depends on the sampling probabilities,
allowing us to guarantee uniform convergence.
\begin{lemma}[{\citet[Theorem 1]{Bhattacharya70}}]. 
  \label{lemma:clt-rate}
  Let $\{X_i\}_{i =1}^n$ be a sequence of $n$ independent random vectors
  taking values in $\R^{\numarm}$ with covariance matrix $\Sigma_n$. Assume
  that for some $\delta > 0$
  \begin{equation*}
    M \defeq \sup_{n} \frac{1}{n} \sum_{i=1}^{n}
    \E \ltwo{\Sigma_{n}^{-1/2} X_{i}}^{3+\delta} < \infty.
  \end{equation*}
  Then, the normalized partial sum
  $\bar{Z}_{n} = \frac{1}{\sqrt{n}} \sum_{i=1}^{n} \Sigma_{n}^{-1/2} X_{i}$
  satisfies
  \begin{equation*}
    d_{\text{BL}}(\bar{Z}_n, N(0, I))
    \leq A_{K,\delta} M^{\frac{3+3\delta}{3+\delta}}n^{-1/2} + B_{K,\delta} M^{\frac{3}{3+\delta}}n^{-1/2}
  \end{equation*}
  where $A_{K,\delta}, B_{K,\delta}$ are constants that only depends on $K$ and
  $\delta$. 
\end{lemma}

\noindent  In order to guarantee uniform convergence over $r_{0:t}$ using
Lemma~\ref{lemma:clt-rate}, we use
Assumptions~\ref{assumption:reward}\ref{item:moment}
to control the moment term $M$. Since we assume the existence of a fourth moment,
we let $\delta = 1$.
Using $C_{\numarm}$ to denote a constant that only depends on
$\numarm$ and polynomially on $s^{2}$, $b_{t+1}$, $h$ (that may differ line by line), use lower bound on the
minimum eigenvalue of $\Sigma_{n,t+1}$ given in Lemma~\ref{lemma:min-max-eig}
to get
\begin{equation*}
  \begin{split}
  M(\pstate_{0:t})
  & \defeq \sup_{n} \frac{1}{b_{t+1}n}\sum_{j=1}^{b_{t+1}n} \mathbb{E}
    \ltwo{\left(  \Sigma_{n,t+1}(\pstate_{0:t})  \right)^{-1/2} 
     \left(
      \xi_{j}^{t+1}R_{j}^{t+1} -\frac{\pi_{t+1,a}(\pstate_{0:t})h_{a}}{\sqrt{n}}
    \right)}^{4} \\
  & \leq \left( \min_{a}\pi_{t+1,a}(\pstate_{0:t}) \min_{a} s^{2}_{t+1,a} \right)^{-2} 
  \sup_{n} \frac{1}{b_{t+1}n}\sum_{j=1}^{b_{t+1}n} \mathbb{E}
  \ltwo{ \left( \xi_{j}^{t+1}R_{j}^{t+1} -\frac{\pi_{t+1}(\pstate_{0:t})h}{\sqrt{n}}\right)}^{4} \\
      &\leq C_{K} \left( \max_{a}\pi_{t+1,a}^{-2}(\pstate_{0:t}) \right)
      \max_{a} \E \left|\xi_{a,j}^{t+1}R_{a,j} - \frac{\pi_{t+1,a}(\pstate_{0:t})h_{a}}{\sqrt{n}}\right|^{4} < \infty\\
  \end{split}
\end{equation*}
Using Assumption~\ref{assumption:reward}\ref{item:moment}, conclude
\[
  M(\pstate_{0:t})\leq C_{K} \left( \max_{a}\pi_{t+1,a}^{-2}(\pstate_{0:t}) \right)
  \left( C + \left(\frac{\max_{a} |h_{a}|}{\sqrt{n}} \right)^{4} \right) < \infty 
\]
where $C$ is the moment bound in $\epsilon_{a}$ given in Assumption~\ref{assumption:reward}\ref{item:moment}.

Apply Lemma~\ref{lemma:clt-rate} to get
\[
  \begin{split}
  &\E[\E_{\pstate_{0:t}} \left[ d_{\text{BL}} ( Q_{n, t+1}, \Sigma^{1/2}_{n,t+1}(\pstate_{0:t})Z )\right]] \\
  & \leq \max_{a} \left(\frac{s_{a}^{2} + h_{a}^{2}}{b_{t+1}}\right)^{1/2} 
  \E[\E_{\pstate_{0:t}} \left[ d_{\text{BL}} ( \Sigma_{n,t+1}^{-1/2}Q_{n, t+1}, Z )\right]] \\
  & \leq  \max_{a}\left(\frac{s_{a}^{2} + h_{a}^{2}}{b_{t+1}}\right)^{1/2} 
  \E[\E_{\pstate_{0:t}} 
  \left[ A_{K,\delta} M(\pstate_{0:t})^{\frac{3}{2}} (b_{t+1}n)^{-1/2}
  + B_{K,\delta} M(\pstate_{0:t})^{\frac{3}{4}} (b_{t+1}n)^{-1/2} \right]].
  \end{split}
\]
Taking expectation over $\pstate_{0:t}$ and using the preceding bound on $M(\pstate_{0:t})$, we have that
\[
  \begin{split}
  & \E[\E_{\pstate_{0:t}} \left[ d_{\text{BL}} (Q_{n, t+1}, \Sigma^{1/2}_{n,t+1}(\pstate_{0:t})Z )\right]] \\
  & \leq \max_{a} \left(\frac{s_{a}^{2} + h_{a}^{2}}{b_{t+1}}\right)^{1/2} \cdot 
  \E[\E_{\pstate_{0:t}} 
  \left[ A_{K,\delta} M(\pstate_{0:t})^{\frac{3}{2}} (b_{t+1}n)^{-1/2}
  + B_{K,\delta} M(\pstate_{0:t})^{\frac{3}{4}} (b_{t+1}n)^{-1/2} \right]] \\
  & \leq n^{-1/2} C_{K} \E \left[ \max_{a}\pi_{t+1,a}^{-3}(\Pstate_{0:t}) \right]
  \left( C^{\frac{3}{2}} + \frac{\max_{a} |h_{a}|^{6} }{n^{3}} \right) \\
  &\qquad +n^{-1/2} C_{K} \E \left[\max_{a}\pi_{t+1,a}^{-3/2}(\Pstate_{0:t}) \right]
  \left( C^{\frac{3}{4}} + \frac{\max_{a} |h_{a}|^{3} }{n^{3/2}} \right).
  \end{split}
\]
Due to Assumption~\ref{assumption:overlap}, the moment $\E[\max_{a}\pi_{t+1,a}^{-3}(\Pstate_{0:t})]$ is bounded by $C_{o}$,
which gives us a $n^{-1/2}$ upper bound on $\E[\E_{\pstate_{0:t}} \left[ d_{\text{BL}} (Q_{n}^{t+1}, \Sigma^{1/2}_{n,t+1}(\pstate_{0:t})Z )\right]]$.
Finally, combining with the result in Lemma~\ref{lemma:kl-cov}, there exists a constant $\tilde{C}_{t+1}$ depending on
$\numarm$ and polynomially on $C_{o}$, $s^{2}$, $b_{t+1}$, and $h$
\[
  \E[\E_{\pstate_{0:t}} \left[ d_{\text{BL}} (\Pstate_{n, t+1}, \Sigma^{1/2}_{t+1}(\pstate_{0:t})Z + h\pi_{t+1}(\pstate_{0:t}))\right]] \leq \tilde{C}_{t+1} n^{-1/2}
\]
uniformly over all $\pstate_{0:t}$.

Recall the decomposition~\eqref{eqn:start} for any bounded Lipschitz function
$f$ and corresponding $g, g_n$ defined as in Eq.~\eqref{eqn:gs}. The above result implies that
\[
  |\E[g_{n}(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})] - \E[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})] | \leq \tilde{C} n^{-1/2}
\]
for some constant $\tilde{C} =\max_{t} \tilde{C}_{t}$.

It remains to show convergence of $\E[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]$ to $\E[g(G_{1}, \ldots, G_{t})]$.
Recall that
\[
  g(r_{0:t}) = \E\left[f\left(r_{0:t}, \Sigma_{t+1}^{1/2}(r_{0:t})Z +
      h\pi_{t+1}(r_{0:t})\right)\right]
\]
is a bounded Lipschitz function. From the proof of Corollary~\ref{section:proof-limit-rate}
we have that the Lipschitz constant for $g$ is bounded by $M = \left(1 + \bar{L} ( s_*^2/b_* \sqrt{K} + \max_{a} |h_{a}|) \right)$
where $\bar{L}$ is the upper bound of the Lipschitz constants for $\pi^{1/2}$ and $\pi$, $s_{*} := \max_{a} s_{a}$, and $b_{*} := \min_{t} b_{t}$.
We can then use the induction hypothesis, which gives us
\[
  |\E[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})] - \E[g(G_{0},...,G_{t})]| \leq M \cdot \tilde{C} \sum_{s=0}^{t} M^{s} n^{-1/2}.
\]
This implies
\[
  \begin{aligned}
  & |\mathbb{E}[f(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t+1}^{n})]
     - \mathbb{E}[f(G_{0},...,G_{t+1})]| \\
  & \leq |\mathbb{E}[g_{n}(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]
  -\mathbb{E}[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]|
  +|\E[g(\sqrt{n}\bar{R}_{0}^{n},...,\sqrt{n}\bar{R}_{t}^{n})]-\mathbb{E}[g(G_{0},...,G_{t})]| \\
  & \leq \tilde{C} n^{-1/2}
  +  M \cdot \tilde{C} \sum_{s=0}^{t} M^{s} n^{-1/2} 
  = \tilde{C} \sum_{s=0}^{t+1} M^{s} n^{-1/2}
  \end{aligned}
\]
Noting that $\sum_{s=0}^{T-1} M^{s} = \frac{M^{T} - 1}{M - 1}$,
we have the desired inequality.

\subsection{Proof of Corollary~\ref{cor:bsr-limit}}
\label{section:proof-bsr-limit}

From the proof of Theorem~\ref{theorem:limit}, the convergence of the Bayes
simple regret immediately follows from the fact that the weak
convergence~\eqref{eqn:weak-convergence} is uniform in $h$ under the
hypothesized moment condition on $h$.  The discontinuity points of $\pi_T$ are
of measure zero with respect to $(G_0,\dots,G_{T-1})$, the
convergence~\eqref{eqn:weak-convergence} and the Portmanteau theorem implies
that for any fixed $h$, as $n \to \infty$
\begin{equation*}
  \E[\pi_{T}(\sqrt{n}\bar{R}_{0}^{n},\ldots,\sqrt{n}\bar{R}_{T-1}^{n})|h] \to
  \E[\pi_{T}(G_{0},\ldots,G_{T-1})|h]
\end{equation*}
For any prior distribution $\nu$ over $h$ such that $\E_{\nu}||h|| < \infty$,
dominated convergence gives 
\begin{align*}
  \bsr_{T}(\pi, \nu, \sqrt{n}\bar{R}^{n})
  &= \E_{h\sim\nu}[h^{\top}\pi_{T}(\sqrt{n}\bar{R}_{0}^{n},\ldots,\sqrt{n}\bar{R}_{t+1}^{n})] 
  = \E_{h\sim\nu}[h^{\top}\E[\pi_{T}(\sqrt{n}\bar{R}_{0}^{n},\ldots,\sqrt{n}\bar{R}_{t+1}^{n})|h]] \\
  &\to \E_{h\sim\mu}[h^{\top}\E[\pi_{T}(G_{0},\ldots,G_{T-1})|h]] 
  = \bsr_{T}(\pi, \nu, G).
\end{align*}




% %%% Local Variables:
% %%% mode: latex
% %%% TeX-master: "main"
% %%% End:
