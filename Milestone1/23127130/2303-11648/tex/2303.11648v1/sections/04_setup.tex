\input{tables/dataset_stats}
\section{Experimental Setup}

In this section, we first describe the data used to test our hypotheses, followed by the implementation details of the methods and baselines as well as how we evaluate different approaches.

\subsection{Datasets}
In order to test our hypothesis and compare different methods to generate queries we rely on three datasets: \tracks{}, \podcasts{}, and \books{}. For each dataset, we have a set of entities (>600k entities), a set of 100k queries, and a set of relevance judgements. Table~\ref{table:dataset_statistics} describes the statistics of the datasets and examples of entities.

While the queries and entities from \textbf{\tracks{}} and \textbf{\podcasts{}} were extracted from a large-scale online platform the \textbf{\books{}} dataset is a subset of the Goodreads public dataset from~\cite{DBLP:conf/recsys/WanM18}~\footnote{\href{https://github.com/MengtingWan/goodreads}{https://github.com/MengtingWan/goodreads}}. The query sets from \tracks{} and \podcasts{} are a unique subset of randomly sampled entities and queries from the logs of a large scale online audio platform, where clicks for a given entity after issuing the query are considered to be the relevance signal in our experiments. 

We also use the number of distinct users for which the query was issued by, and use them as $o_{q}$ to calculate retrievability scores (see Section 4.3). Regarding the columns, as seen in Table~\ref{table:dataset_statistics}, the ones with numbers 1--3 are considered to be \narrowcols{}, whereas 4--9 are \broadcols{}. The broad columns which are free-text (\broadcolsft{}) are: \textit{Episode \& show description} and \textit{Transcript} for \podcasts{}, and \textit{User reviews} and \textit{Description} for \books{}.

Since the Goodreads dataset does not have any set of queries available, we generate a set of queries automatically: 75\% of the queries are narrow, generated by sampling words from the \narrowcols{}, and 25\% of them are from \broadcols{} and consider that as the relevance labels. This specific split of narrow and broad queries was chosen to simulate actual user behavior observed in the other two datasets (\tracks{} and \podcasts{}) where narrow queries are the majority but in a less extreme fashion. We use the number of ratings the entities from \books{} have as a proxy for the number of users that would issue such queries ($o_{q}$). 
% In the next subsection we explore ways of getting a dataset of broad queries for the music and podcast domains.

\subsubsection{Broad queries datasets}

Since the majority of the queries from \tracks{}, \podcasts{} and \books{} are \narrow{}, we also employ two smaller additional sets of queries and relevance labels that have an underlying \broad{} intent. They are $\tracks{}_{\broad{}}$ and $\podcasts{}_{\broad{}}$, containing a total of 1309 and 500 queries. The $\tracks{}_{\broad{}}$ is a sample of queries from the logs that have a high predicted probability of being broad based on the interaction signals the user had after issuing the query. If the user interacts with entities such as playlists and hubs more than tracks and albums they are more likely to be issuing a broad query. Based on this set, we get the clicked entities where the query does not match the title, artist, or album of the entity, avoiding cases where the query seems broad but is in fact a narrow interaction, e.g. query ``\textit{pop}'' and clicking a track with the title ``\textit{POP!}''. For $\podcasts{}_{\broad{}}$, there is no parallel for the \tracks{} playlists so we employ a set of manually curated pairs of broad queries and entities. Annotators were instructed to write a query relevant to the podcast episode while avoiding exact matches and matching diverse metadata fields.

\subsection{Implementation Details}

\subsubsection{Query generation models}
As baselines for generating synthetic queries, we first use \textbf{\qgen{}}, a common approach to generate queries from documents used in this manner in different previous work~\cite{nogueira2019doc2query,liang2020embedding,ma2020zero,wang2021gpl}. We rely on fine-tuning T5~\cite{raffel2019exploring} (\textit{t5-base}) on a subset of \clicks{} train set with 10k pairs query-entities. The second baseline for generating queries requires very little supervision signal: \textbf{\inpars{}}~\cite{bonifacio2022inpars}. The model uses in-context learning, i.e. few examples in the prompt of the document and expected query, and large language models. For a fair comparison, we randomly sample examples to use in the prompt every time we are generating the output queries, this way \inpars{} has access to the same amount of training pairs of query and entities as \qgen{}\footnote{This was shown to be effective for the validation sets of \podcasts{} and \books{}. For \tracks{} we did not observe the same, so we used a fixed prompt with the same two examples randomly selected from the dataset.}. We rely on the open \textit{bigscience/bloom-760m}\footnote{\href{https://bigscience.huggingface.co/blog/bloom}{https://bigscience.huggingface.co/blog/bloom}} release to do so\footnote{We explore larger GPT-3 models on the appendix and see that the larger 175B parameter one does not significantly improve over smaller models.}. For the \textbf{\cqg{}} implementation we also rely on the T5 (\textit{t5-base}) model. When generating the queries with T5, for both \qgen{} and \cqg{} we employ $do\_sample$=True and $top\_k$=10.

\subsubsection{Retrieval models}
For \textbf{\bm{}}~\cite{robertson1994some} we resort to the default hyperparameters and implementation provided by the PyTerrier toolkit~\cite{pyterrier2020ictir}. For the zero-shot \textbf{\biencoder{}} models, we rely on the SentenceTransformers~\cite{reimers-2019-sentence-bert} model releases\footnote{\url{https://www.sbert.net/docs/pretrained_models.html}}. The library uses Hugginface transformers for the pre-trained models such as BERT~\cite{devlin2018bert} and MPNet~\cite{song2020mpnet}. Specifically, we employ the pre-trained model \textit{all-mpnet-base-v2}. When fine-tuning the \biencoder{} models on the \clicks{} or synthetic datasets, we rely on the \textit{MultipleNegativesRankingLoss}, which uses in-batch random negatives to train the model. We fine-tune the dense models for a total of 10k steps. \textbf{Thus, all dense models were trained on the same amount of (synthetic or not) queries}. We use a batch size of 8, with 10\% of the training steps as warmup steps. The learning rate is 2e-5 and the weight decay is 0.01. We refer to the \biencoder{} model trained on \clicks{} data as \textbf{$\biencoder{}_{\clicks{}}$} and a \biencoder{} model trained on the queries from \cqg{} as \textbf{$\biencoder{}_{\cqg{}}$}.

\subsection{Evaluation Procedure}

To evaluate the effectiveness of the retrieval systems we use the recall at 100, $R@100$. The choice for R@100 is due to the objective of increasing the retrievability of items considering the first 100 options\footnote{A second stage re-ranker in this pipeline could be precision-focused if the retriever is able to find enough relevant and diverse options.}. We perform Students t-tests at the confidence level of 0.95 with Bonferroni correction to compare the difference between models with statistical significance. 

To evaluate how biased the retrieval system is in terms of retrievability, we first estimate the retrievability of an entity $e$ as defined by~\cite{azzopardi2008retrievability}: $r(\mathbf{e})=\sum_{\mathbf{q} \in \mathbf{Q}} o_{q} \cdot f\left(k_{e q}, c\right)$, where $\mathbf{Q}$ is the set of queries\footnote{The size of $\mathbf{Q}$ is 100k for all computations.}, $o_{q}$ is the weight of each query---here we use the number of users that issued the query---and $f\left(k_{e q}, c\right)$ is 1 if the entity $e$ is ranked above $c$ by the search system (in our experiments we set c=100) and 0 otherwise. In order to get a number that summarizes how concentrated or biased the retrievability scores are we calculate the Gini score~\cite{gastwirth1972estimation}: $G=\frac{\sum_{i=1}^{N}(2 * i-N-1) * r\left(\mathbf{e}_{\mathbf{i}}\right)}{N \sum_{j=1}^{N} r\left(\mathbf{e}_{\mathbf{j}}\right)}$, where G=1 means only one entity concentrates all the retrievability, and G=0 means every entity in the collection has the same retrievability score. In order to perform statistical testing for the Gini scores we follow~\cite{gamboa2010statistical}.