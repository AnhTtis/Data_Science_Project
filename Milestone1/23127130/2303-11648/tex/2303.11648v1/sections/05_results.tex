\input{tables/results_narrow}

\section{Results}
In this section, we first describe the experimental results on \textbf{H1}---training dense retrieval models on synthetic queries leads to less retrievability bias than training on real queries and clicked entities---followed by the results for \textbf{H2}---suggesting broad queries generated by our proposed method \cqg{} leads to less retrievability bias when compared to the set of queries from the logs.

\subsection{H1: Modifying the Ranker with Generated Queries as Training Data}
\subsubsection*{\textbf{Evaluation with narrow intent queries}} 
Table~\ref{table:results_narrow} displays R@100 and Gini scores for different retrieval models on the three datasets which contain mostly narrow intent queries. Zero-shot models do not have access to any \clicks{} relevance labels for training. As expected, a \biencoder{} that has no access to the target domain queries and entities does not perform well, and it has worse effectiveness than \bm{} (30\% less R@100 on average, as seen row \textit{a} vs row \textit{b}). When using the target training data to fine-tune the dense retrieval model (\biencoder{}$_\clicks{}$) we observe that it outperforms zero-shot models significantly, with absolute gains of R@100 up to 158\% (row \textit{h} vs row \textit{b}). However, both the model trained on the \clicks{} data (row \textit{h}) and the pre-trained \biencoder{} (row \textit{c}) have significantly more bias than \bm{}, as seen by the Gini scores increases of 9.2\% and 10\% respectively.

When using the synthetic queries created by any of the query generation models to train the dense retrieval methods (\inpars{}, \qgen{}, \cqg{}) as described in Section 3.2.1, we observe significant drops of 10\% Gini on average (rows \textit{d},\textit{e},\textit{f} vs row \textit{h}), \emph{\textbf{indicating positive evidence for our first hypothesis}} that a model trained on the synthetic queries lead to less retrievability bias than the model trained on the \clicks{} data. Specifically, with \cqg{}\footnote{We employ here \cqg{}$_{narrow}$ which sets ($P_{narrow}$, $P_{broad}$) as ($100\%$, $0\%$) and \weaklabelsip{} as found to be optimal in the validation experiments (c.f. Table~\ref{table:ablation_study}).} we show that we can get statistically significant better effectiveness and retrievability for \tracks{} and \books{} than the query generation baselines with 24\% more R@100 and 3\% less Gini on average over all datasets and baselines (row \textit{f} vs rows \textit{d}, \textit{e}). We also show that we improve the retrievability over the model trained on \clicks{} data (row \textit{f} vs row \textit{h}) by 10\% Gini. This effectively makes more than 62k (9\%) entities in the \tracks{} dataset retrievable compared to $\biencoder{}_{\clicks{}}$, i.e. the entity goes from zero to a non-zero value.

We see also that with a combination of synthetic queries from \cqg{} and queries from the \clicks{} dataset~\footnote{We set the percentage as the optimal one in the validation set: 10\% synthetic queries and 90\% queries and clicks from \clicks{}.} (row \textit{g}) we can achieve similar effectiveness to the model training on the \clicks{} dataset (no statistical difference) while having less retrievability bias for both \tracks{} and \podcasts{} datasets with statistical significance, being Pareto optimal when considering both objectives.
% R@100 and Gini as objectives.

% \begin{figure}[]
%     \centering
%     \includegraphics[width=.45\textwidth]{img/broad_queries.pdf}
%     \caption{Recall@100 and confidence intervals at 95\% of \biencoder{} models trained on different sets of synthetic queries and evaluated for \broad{} queries. A model trained with broad queries generated by our method \cqg{}$_{\broad{}}$ outperforms a dense retriever trained on \clicks{} data and a dense retriever trained on queries from the baselines \qgen{} and \inpars{}.}
%     \label{fig:broad_queries_effectiveness} 
% \end{figure}

\begin{table*}[]
% \footnotesize
\caption{Retrieval effectiveness (R@100$\uparrow{}$ the higher the better) and retrievability bias (Gini $\downarrow{}$ the lower the better) of dense models trained on different training data for a subset of \broad{} queries. Bold indicates the best model for each category with statistical significance and superscripts indicate statistically significant improvements over the respective model using Students t-test at 0.95 confidence with Bonferoni correction. When the set of queries $\set{Q'}$ were generated with ($P_{narrow}$, $P_{broad}$) as ($100\%$, $0\%$), we refer to it as $\cqg{}_{narrow}$, with ($0\%$, $100\%$) we call it $\cqg{}_{broad}$ and with ($50\%$, $50\%$) as $\cqg{}_{both}$.}
\label{table:gini_broad}
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Method} & \multicolumn{2}{c}{\textbf{\textbf{R@100 $\uparrow$}}} & \multicolumn{2}{c}{\textbf{\textbf{Gini $\downarrow$}}} \\
& \tracksbroad{} & \podcastbroad{} & \tracksbroad{} & \podcastbroad{} \\\midrule
(\textit{a}) \biencoder{}$_{\cqg{}_{broad}}$  & \textbf{0.074}$^{bcdef}$ & 0.800$^{ef}$ & 0.596$^{b}$ & 0.831$^{bf}$\\
(\textit{b}) \biencoder{}$_{\clicks{}}$ & 0.035$^{def}$ & 0.756$^{f}$ & 0.878 & 0.846\\
(\textit{c}) \biencoder{}$_{\cqg{}_{both}}$ & 0.033$^{def}$ & 0.780$^{f}$ & 0.492 $^{abef}$ & 0.831$^{bf}$\\
(\textit{d}) \biencoder{}$_{\inpars{}}$  \cite{bonifacio2022inpars} & 0.010 & \textbf{0.827}$^{cef}$ & \textbf{0.489}$^{abcef}$ & \textbf{0.816}$^{abcf}$\\
(\textit{e}) \biencoder{}$_{\qgen{}}$ \cite{ma2020zero} & 0.009 & 0.744$^{f}$ & 0.540$^{ab}$ & 0.820$^{abcf}$\\
(\textit{f}) \biencoder{}$_{\cqg{}_{narrow}}$ & 0.003 & 0.609 & 0.517$^{abe}$ & 0.835$^{b}$ \\ \bottomrule
\end{tabular}
\end{table*}

\subsubsection*{\textbf{Evaluation with broad intent queries}} 
In order to understand how the models perform for exploratory and complex information needs, we take a closer look at the effectiveness and retrievability of the models in a set containing only broad intent queries. Table~\ref{table:gini_broad} shows that a model trained on synthetic queries from \cqg{} gets significantly better when we include broader queries in the training (going from 0, 50 and 100\% on rows \textit{f}, \textit{c} and \textit{a}). 

A model trained only on synthetic broad queries outperforms a model trained on \clicks{} data by 111\% of R@100 for \tracksbroad{} with statistical significance (row \textit{a} vs row \textit{b}). We also observe significant drops in the retrievability bias when we compare models trained with \cqg{} queries with models trained with \clicks{} data, going from 0.878 to 0.596 and from 0.846 to 0.831 as seen in Table~\ref{table:gini_broad} (row \textit{a} vs row \textit{b}). Baseline models to generate queries (rows \textit{d} and \textit{e}) also have significantly less retrievability bias than the model trained on click data (row \textit{b}) \textbf{\emph{showing again positive evidence for our first hypothesis}} that models trained on synthetic queries lead to less retrievability bias.
% \inpars{} is performing better for the \podcastbroad{} dataset compared to \podcasts{} in relation to other approaches. We hypothesize that this is due to the fact that the topics and knowledge required to match podcast queries and their respective episodes is far closer to web search than for \tracksbroad{}. Thus the pre-training procedure of models such as GPT-3 and BLOOM capture this complete query generation process better than relying on generating queries that resemble user instant queries for the platform (queries from \clicks{}).

\input{tables/ablation_study}
\subsubsection*{\textbf{Contribution of each module of \cqg{}}}
When using the set of queries generated by \weaklabelsun{}(no supervision available) to train the dense retriever, we get statistically significant improvements over the model that is not fine-tuned (row \textit{c} vs row \textit{b}, going from 0.142 to 0.222 R@100 and from 0.323 to 0.465 R@100 for the \tracks{} and \podcasts{} datasets as seen in Table~\ref{table:results_narrow}. Using the remaining components of \cqg{} we obtain significant improvements over using solely \weaklabelsun{}. A natural question is from which modules the improvements are coming. To answer this question Table~\ref{table:ablation_study} displays an ablation study on the components of \cqg{}. If we remove all components of \cqg{} we end up with the baseline \qgen{} (first line of the table). We incrementally add each component of the model in the following rows. 

The main findings are that (I) the serialization component, where we indicate which the metadata columns and their respective values as opposed to values only, is beneficial for both R@100 and Gini for both the narrow evaluation set of queries and broad set of queries and (II) the \weaklabelsun{} is only beneficial for the broad set of queries as \weaklabelsip{} cover narrow queries well (they are the majority of the available existing queries). 

% \vspace{1cm}

\subsection{H2: Modifying the Set of Queries by Suggesting Generated Queries}
% In order to gather initial evidence to the retrievability bias when using a set of queries $\mathbf{Q}$ that is composed of mostly narrow versus a set composed of mostly broad queries, we display at Figure~\ref{fig:narrow_broad_lorenz} the Lorenz curves of both distributions when using a zero-shot \biencoder{} model (same findings were found when using other retrieval model). The results show that when using $\mathbf{Q}_{\broad{}}$ we obtain less retrievability bias than when using $\mathbf{Q}_{\narrow{}}$. Thus, we show positive evidence to the intuition that broad queries should match with a higher number of distinct entities when compared to narrow ones and thus have less retrievability bias.

% This motivates suggesting broader queries to users that engage in search sessions with the system. 

In order to test our second hypothesis that suggesting broad queries with \cqg{} leads to less retrievability bias compared to the queries found in the logs, we rely on a simulation where a percentage of suggested queries are accepted and added into the set of queries as described in Section 3.2.2. For each entity in the top-5 ranked list for the log queries we create 3 query suggestions.

Figure~\ref{fig:query_suggestion} displays the results of this simulation. We see that if the \biencoder{} is trained on a set of broad queries, the retrievability of the system drops significantly as higher percentages of suggested broad queries by \cqg{} are accepted, with decreases of Gini up to 11\% and 7\% for \tracks{} and \podcasts{} \textbf{\emph{showing positive evidence for our second hypothesis}}. If we consider that all queries are accepted by the users a total of 78k (11\%) entities for the \tracks{} dataset would become retrievable, i.e. retrievability different than zero, compared to using \cqg{}$_{both}$ with the log queries. We also see that only modifying the set of queries is not enough, as a Bi-Encoder trained on the \clicks{} data does not achieve the same effect, showing that it is also necessary to employ a model that was trained for both narrow and broad queries.

% \begin{figure}[]
%     \centering
%     \includegraphics[width=.27\textwidth]{img/narrow_broad_lorenz.pdf}
%     \caption{Lorenz curve of retrievability scores. The diagonal line corresponds to a distribution where each entity has the same retrievability (Gini=0). Narrow queries---users have a specific entity in mind, e.g. Kate Bush---lead to higher retrievability bias than broad queries, e.g. 90s music for workout.}
%     \label{fig:narrow_broad_lorenz}
% \end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=.45\textwidth]{img/query_suggestion.pdf}
    \caption{Suggesting broad queries with \cqg{} reduces the retrievability of a model trained on synthetic data.}
    % , whereas the model trained on \clicks{} data displays the same retrievability bias.}
    \label{fig:query_suggestion}
    \vspace{-0.5cm} 
\end{figure}

% \begin{figure}
% \centering
% \begin{subfigure}[b]{.23\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{img/narrow_broad_lorenz.pdf}
%   \label{fig:narrow_broad_lorenz}
% \end{subfigure}
% \begin{subfigure}[b]{.23\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{img/query_suggestion.pdf}
%   \label{fig:query_suggestion}
% \end{subfigure}
% \caption{\textcolor{Brown}{Left}: Lorenz curve of retrievability scores. The diagonal line corresponds to a distribution where each entity has the same retrievability (Gini=0). Narrow queries---users have a specific entity in mind, e.g. Kate Bush---lead to higher retrievability bias than broad queries, e.g. 90s music for workout. \textcolor{Brown}{Right}: Suggesting broad queries using \cqg{} reduces the retrievability of a \biencoder{} model trained on synthetic data, whereas the model trained on \clicks{} data displays the same retrievability bias.}
% \end{figure}


