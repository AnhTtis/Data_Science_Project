% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table*}[ht!]
\caption{Retrieval effectiveness (R@100$\uparrow{}$ the higher the better) and retrievability bias (Gini $\downarrow{}$ the lower the better) of dense retrieval models trained on different training data for predominantly \narrow{} queries (\clicks{} test sets). Bold indicate the best model for each category with statistical significance and superscripts indicate statistically significant improvements over the respective model using students t-test at 0.95 confidence with Bonferoni correction for multiple comparisons. The values for the \books{} dataset on row ($c$) are not included as they are already a synthetic set of queries.
% Models fine-tuned on synthetic data have less retrievability bias, and models trained on the click data (or combinations of click and synthetic data) have the best effectiveness.
}
\label{table:results_narrow}
\begin{tabular}{@{}lp{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}@{}}
\toprule
 & \multicolumn{3}{c}{\textbf{R@100$\uparrow{}$}} & \multicolumn{3}{c}{\textbf{Gini$\downarrow{}$}} \\ \midrule
\begin{tabular}[c]{@{}l@{}}\textbf{Zero-shot}\\ \textit{(no target domain \clicks{} training data)}\end{tabular} & \tracks{} & \podcasts{} & \books{} & \tracks{} & \podcasts{} & \books{} \\ \midrule
(\textit{a}) \bm{} & 0.182$^{b}$ & 0.436$^{b}$ & \textbf{0.721}$^{bd}$ & 0.752$^{bh}$ & \textbf{0.666}$^{bcdefh}$ & \textbf{0.779}$^{b}$ \\
(\textit{b}) \biencoder{} & 0.142 & 0.323 & 0.415 & 0.818$^{h}$	& 0.765 & 0.836$^{d}$ \\
(\textit{c}) \biencoder{}$_{\weaklabelsun{}}$ (Ours) & \textbf{0.222} $^{abd}$ & \textbf{0.465}$^{b}$ & - & \textbf{0.748}$^{abh}$ & 0.730$^{bh}$ & - \\ \midrule
\multicolumn{7}{l}{\begin{tabular}[c]{@{}l@{}}\textbf{Fine-tuned on synthetic data} \\ \textit{(target domain \clicks{} training data to train query generators)}\end{tabular}} \\ \midrule
(\textit{d}) \biencoder{}$_{\inpars{}}$ \cite{bonifacio2022inpars} & 0.202$^{ab}$ & 0.474$^{ab}$ & 0.492$^{b}$ & 0.712$^{abch}$ & 0.677$^{bch}$ & 0.842 \\
(\textit{e}) \biencoder{}$_{\qgen{}}$ \cite{ma2020zero} & 0.296$^{abcd}$  & \textbf{0.503}$^{abc}$ & 0.755$^{abd}$ & 0.701$^{abcdh}$ & \textbf{0.674$^{bcdfh}$} & 0.766$^{abgh}$\\
(\textit{f}) \biencoder{}$_{\cqg{}}$ (Ours) & \textbf{0.333}$^{abcde}$ & 0.500$^{abc}$ & \textbf{0.770}$^{abde}$ & \textbf{0.693}$^{abcdeh}$ & 0.676$^{bdch}$ & \textbf{0.762}$^{abegh}$\\ \midrule
\multicolumn{7}{l}{\begin{tabular}[c]{@{}l@{}}\textbf{Fine-tuned on target data or in combination with synthetic data}\\ \textit{(access to target domain \clicks{} training data)}\end{tabular}} \\ \midrule
% \biencoder{}$_{\clicks{} + \inpars{}}$ & 0.353 & \textbf{0.639} & \textbf{0.781}$^{\dagger}$ & 0.865 & 0.753 & 0.775 \\
% \biencoder{}$_{\clicks{} + \qgen{}}$  & 0.360 & 0.616 & 0.779 & 0.830 & 0.753 & \textbf{0.767} \\
(\textit{g}) \biencoder{}$_{\clicks{} + \cqg{}}$ (Ours) & 0.361$^{abcdef}$ & 0.622$^{abcdef}$ & \textbf{0.775}$^{abde}$ &\textbf{0.817}$^{bh}$ & \textbf{0.741}$^{bh}$ & 0.768 $^{ab}$\\ %\midrule
% \multicolumn{7}{l}{\begin{tabular}[c]{@{}l@{}}\textbf{Fine-tuned on target data} \\ \textit{(access to target domain \clicks{} training data)}\end{tabular}} \\ \midrule
(\textit{h}) \biencoder{}$_{\clicks{}}$ & \textbf{0.366}$^{abcdef}$ & \textbf{0.634}$^{abcdef}$ & 0.769$^{abde}$ & 0.856 & 0.763$^{b}$ & \textbf{0.767}$^{abg}$ \\ \bottomrule
\end{tabular}
\end{table*}