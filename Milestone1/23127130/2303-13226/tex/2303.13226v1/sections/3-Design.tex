\section{LearnedFTL}\label{design}

\subsection{System Overview}
The main idea of LearnedFTL is to use learned index models to predict the PPNs when I/O requests miss from the CMT, thus reducing the number of double reads induced by random read requests. Figure~\ref{fig-overall} illustrates the system overview of LearnedFTL, which adopts a demand-based FTL framework \cite{gupta2009dftl,zhou2015efficient}. Based on the analysis in Section~\ref{two-four}, a model layer is introduced into the Global Translation Directory (GTD) for mapping prediction. Each GTD entry has a specific learned index model. For LPNs that miss the CMT, the learned index models can predict the PPNs. Each time a PPN is correctly predicted, the corresponding double read is eliminated. 

\begin{figure}[t]
    \centering
    \setlength{\abovecaptionskip}{5pt}
    \includegraphics[scale=0.9]{photos/design/overall.pdf}
    \caption{The system overview of LearnedFTL.}
    \label{fig-overall}\vspace{-7pt}
\end{figure}

Each GTD entry is associated with a dedicated learned model. An entry in GTD indexes a translation page (TransPage in Figure~\ref{fig-overall}, indexed by translation PPN in GTD, denoted as TPPN) that stores hundreds of LPN-PPN mappings (e.g., 512 for an SSD with a page size of 4KB and PPN size of 8 Bytes). It is worth noting that such a number of mappings is compatible with the state-of-the-art Learned Indexes~\cite{ALEX,li2021finedex,kraska2018case} that usually build a learned model for hundreds of key-value pairs, making a GTD entry a sensible basic unit to build a learned model.

To solve the challenge of model selection, LearnedFTL uses a Dynamic Piece-wise Linear Regression model to achieve high accuracy and high space efficiency (cf. Section~\ref{three-two} for details). To solve the challenge that how to obtain contiguous LPN-PPN mappings, LearnedFTL uses a method of Low-cost Model Training via Garbage Collection (GC) by Group-based Allocation and Virtual PPN Representation (cf. Section~\ref{three-three} for details). To solve the challenge of how to determine the correctness of model prediction, LearnedFTL uses a Bitmap Filter method to filter the correct model predictions (detailed in Section~\ref{three-four}). With Bitmap Filter, LearnedFTL solves the challenge of how to determine the correctness of the predicted PPN.


\subsection{Dynamic Piece-wise Linear Regression}

\label{three-two}

The knowledge about the data to be trained is important for the model selection. To this end, we collect several GTD entries' valid LPN-PPN mappings in DFTL with stable state. To make the mappings meet the requirement(2) in Section~\ref{two-four}, we reallocate contiguous PPNs. Figure~\ref{fig-model-image} shows the typical image of LPN-PPN mappings, where we label different segments of contiguous LPN-PPN mappings (i.e., both LPN and PPN are contiguous) with different colors. These disconnected segments (i.e., short straight lines) collectively form a roughly straight line. This suggests that a proper choice is the piece-wise linear regression (PLR) model if all contiguous segments of a GTD entry can be fitted with linear regression.


However, the number of locally contiguous LPN-PPN mappings may be too large (36 segments in Figure~\ref{fig-model-image}) to assign a model for each segment. Considering space footprint, the best solution is to preferentially fit an individual model for the longest continuous segments, thus ensuring high accuracy.

\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{5pt}
\includegraphics[scale=0.8]{photos/design/model_image.pdf}
\caption{An example for LPN-PPN translations. Each color represents a contiguous segment.}
\label{fig-model-image}\vspace{-10pt}
\end{figure}


\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{5pt}
\includegraphics[scale=0.75]{photos/design/DPLR_process.pdf}
\caption{The process of DPLR building.}
\label{fig-model-build}\vspace{-15pt}
\end{figure}


To this end, we propose a \textbf{dynamic piece-wise linear regression (DPLR)} to achieve high model accuracy with fewer segments. In DPLR, the maximum number of segments is set as \emph{N} (typically 8 but can be adjusted according to the environment), which means that there are at most \emph{N} linear regressions. Supposing there are \emph{M} LPN-PPN mappings, DPLR fitting needs three steps, as shown in Figure~\ref{fig-model-build}: (1) Set Parameters: LearnedFTL first sets the average length of each segment by \emph{M/N}. If the length of one continuous LPN-PPN mapping exceeds this average value, these mappings can form one individual segment with a linear model (called \textbf{Large Segment}). (2) Count continuation: LearnedFTL traverses all mappings and records all large segments. Supposing there are \emph{T} large segments, the remaining mappings are combined into \emph{N-T} segments as close as possible. (3) Build segments: LearnedFTL performs the Least Squares algorithm~\cite{bjorck1990least} to calculate the parameters of linear models.

After all the segments are processed, their metadata are combined, including the slope \emph{k}, intercept \emph{b}, and the offset \emph{off} of each segment from the start LPN. Then these metadata are updated to the model layer of the corresponding GTD entry. Figure~\ref{fig-model-layer} shows the data structure of an entry's model layer in GTD. Besides the segment metadata, DPLR models also store the \emph{start\_PPN} to indicate the start PPN of the LPN-PPN mappings in the group, a bitmap with \emph{M} bits to filter predictions (cf. Section~\ref{three-four}). Since each GTD entry uses the start LPN as an index, it is not stored in the model layer.

LearnedFTL proposes \textbf{sequential initialization} to make the DPLR model efficient before GC. The sequential initialization happens when GTD is built. For each GTD entry, the model is initialized to \emph{\textbf{y = x}}, where all bits in the bitmap are set to \emph{\textbf{0}}, and the \emph{start\_PPN} is set to \emph{\textbf{NULL}}. When the first write request with contiguous LPNs arrives in the SSD, \emph{start\_PPN} is set to the first assigned PPN of these LPNs. Then LearnedFTL sets the remaining bits corresponding to these contiguous LPNs to 1.


\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{5pt}
\includegraphics[scale=0.9]{photos/design/model_layer.pdf}
\caption{The data structure of a model in a GTD entry.}
\label{fig-model-layer}\vspace{-15pt}
\end{figure}


\vspace{-3pt}
\subsection{Low-Cost Model Training}
\vspace{-5pt}
\label{three-three}

We propose a new technique, called low-cost model training, to address the PPNs-reordering issue (i.e., how to reorder mapped PPNs of contiguous LPNs). Since the flash-based SSDs need to perform garbage collection (GC) to relocate the valid pages, it provides us an opportunity to rearrange the physical location of the data to make them in the desired order. As a result, it is possible to collect the valid pages with LPNs of one GTD entry, sort these LPNs in order, and write them back in contiguous PPNs. The sorted LPNs in increasing order and contiguous PPNs are then used to train the DPLR models detailed in Section~\ref{three-two}. This, however, hinges on whether LearnedFTL can address the two challenges described in Section~\ref{two-four}, i.e., (1) How to reorganize the random PPNs caused by the allocation strategy? and (2) How to ensure the PPNs are contiguous without affecting the access parallelism?


\subsubsection{Group-based Space Allocation}

The high complexity of PPNs-reordering stems from the SSD's space allocation strategy, that is, it assigns LPNs their corresponding PPNs~\cite{hu2011performance,li2018case}. The mainstream page allocation strategy is dynamic allocation~\cite{hu2011performance,li2018case}. In this strategy, when an LPN needs a physical page, the FTL will select the least busy flash chip (the parallel unit in SSD to execute requests) to allocate pages so as to achieve the best parallelism and write efficiency. 
%A dynamic allocation strategy can provide better write performance as they are more flexible and can adjust the write request's location based on the runtime state of flash chips  . 

However, with dynamic allocation, the PPNs of a GTD entry will be scattered in various locations. When building a learned model over this GTD entry, it is necessary for the GC process to collect the valid pages across multiple flash blocks, and these blocks also contain data (PPNs) belonging to multiple GTD entries. As a result, the GC process needs a large amount of data movement, which significantly increases the complexity and overhead of the model training process. 

To address this issue, we propose a group-based allocation strategy in our LearnedFTL to reduce the GC overhead and simplify the model training~\cite{ZNSwap}. The basic idea is to divide GTD into groups of consecutive entries, referred to as \emph{GTD entry group}. Each group is allocated with an exact number of contiguous flash blocks to accommodate all the LPNs of the group. When the flash blocks allocated to a GTD entry group are full, these used flash blocks are replaced by the same number of contiguous empty flash blocks. When there are no empty flash blocks or the cumulative number of flash blocks allocated to this GTD entry group reaches a threshold, GC is performed on the GTD entry group with the most invalid data pages. During GC, LearnedFTL reclaims data blocks by relocating the valid data pages and retrains the learned models for all GTD entries in this group. 
%To facilitate GC, each GTD entry group will record all flash blocks belonging to this group.

%In some cases, some groups contain cold data, and GC will not occur for a long time. At this time, LearnedFTL will allow other groups that hot data to occupy the free pages of this cold data group. This can not only achieve efficient space utilization, but also allow cold data to be trained by the model.

\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{5pt}
\includegraphics[scale=0.85]{photos/design/group_allocation.pdf}
\caption{An example of group-based allocation.}
\label{fig-group-allocation}\vspace{-5pt}
\end{figure}

Figure~\ref{fig-group-allocation} illustrates an example of group-based allocation. In this instance, for the convenience of presentation, each GTD entry group contains two entries and needs two contiguous flash blocks to accommodate all its LPNs. Therefore, LPNs \emph{0-1023} belong to group 0 and LPNs \emph{4096-5119} belong to group 4. When a request for data with LPN belonging to group 0 arrives, two contiguous blocks, \emph{blk1} and \emph{blk2}, are allocated to group 0. When a request for data with LPN belonging to group 4 arrives, another two contiguous blocks, \emph{blk110} and \emph{blk111}, are allocated to group 4 to accommodate the required data pages. When group 0 has no free physical pages, another two contiguous blocks, \emph{blk3} and \emph{blk4}, are allocated to this group. If group 0 is selected for garbage collection, all four blocks are collected directly.
 




% A serious write-amplification concern arises with this group-based allocation strategy when all GTD entry groups have been written at least once while a small number of hot GTD entry groups are frequently written. When this happens, all GTD entry groups have been allocated sufficient flash blocks to accommodate all their LPNs, exhausting allocable empty flash blocks. At this point, if any of the hot groups run out of free pages in their allocated flash blocks, these full blocks will be replaced by empty flash blocks from the over-provisioning space, which triggers GC on the full blocks. With each GC on a group, all the group’s valid pages are relocated (written) elsewhere, resulting in write amplification. In fact, the hotter (more frequently written) a GTD entry group is, the more frequently GC will be triggered on it, and thus the higher the GC-induced write amplification. 

% A serious write-amplification concern arises with this group-based allocation strategy: When all GTD entry groups have been written at least once, a small number of hot GTD entry groups are frequently written will cause giant write amplification. To solve the problem, a global counter is associated with each GTD entry group to identify the hot groups by counting available free pages in the group. For those hot GTD entry groups with few or no free pages, LearnedFTL uses an \emph{opportunistic cross-group allocation} approach that allows them to encroach into the free-page spaces of flash blocks of ``cold'' GTD entry groups that have an abundance of free pages (thus without GC yet) and untrained models, to avoid or postpone GC. When the amount of encroachment reaches a threshold, GC will be triggered on both the encroaching (hot) group and the encroached (cold) group, and their corresponding models will be retrained and trained respectively thereafter. As a result, this opportunistic cross-group allocation approach not only reduces the GC frequency and the GC-induced write amplification of hot groups but also ensures the early training of the cold groups’ models. 

\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{5pt}
\includegraphics[scale=0.85]{photos/design/virtual_address_principle.pdf}
\caption{The principle of virtual PPN translation.}
\label{fig-virtual}\vspace{-15pt}
\end{figure}


\subsubsection{Virtual PPN representation}


The second challenge of non-contiguous PPNs stems from the write-back stage in the GC process. When the valid sorted pages collected by GC need to be written back, they must be written back in contiguous PPNs. However, the pages in multi-channel SSDs may be written back to different flash chips, leading to non-contiguous PPNs.

To tackle this problem, LearnedFTL uses a virtual PPN (VPPN) representation that aims to transform the non-contiguous PPNs scattered across different chips into contiguous ones. This translation thus allows LearnedFTL to build a model based on contiguous LPN-VPPN pairs. When predicting the PPN of an LPN, LearnedFTL first uses this LPN to predict the VPPN and then translates this VPPN to PPN to get the physical address to complete the prediction.

Figure~\ref{fig-virtual} shows the principle of translation from PPN to virtual PPN. Since the total number of physical flash pages is fixed in an SSD, the PPN is formed in such a way that it represents the hierarchical tree structure of an SSD by the concatenation of address fields representing different levels of the hierarchy from the highest (channel) to the lowest (page) granularity. Because of the commutative law of multiplication, the order of these address fields in PPN can be changed to obey the allocation order. After the change, each physical page retains its unique number, and the new page number will become contiguous according to the allocation order.



\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{5pt}
\includegraphics[scale=0.85]{photos/design/virtual_address_sample.pdf}
\caption{An example of PPN-to-VPPN translation.}
\label{fig-virtual-example}\vspace{-15pt}
\end{figure}



Figure~\ref{fig-virtual-example} gives an example of the PPN-to-VPPN translation. In LearnedFTL, the allocation order is \emph{channel, chip, plane, page, and block}, which is the fastest allocation order based on the previous study~\cite{hu2011performance}. For requests with LPNs \emph{1001, 1002, 1003} that are already written to flash-based SSD, their PPNs are \emph{5013631, 6062207, 7110783}, which are not contiguous. However, after the PPN-to-VPPN translation by changing the order of the fields in the address appropriately, LearnedFTL obtains contiguous VPPNs \emph{2105388, 2015389, 2105390} for these LPNs.


The virtual PPN representation allows LearnedFTL to generate contiguous VPPNs for model training when valid pages are written back to the flash-based SSDs concurrently. Since the training model is built based on LPN-VPPN mappings, the predicted VPPN needs to be translated back to PPN to obtain the physical flash page.


\subsubsection{Model Training Process with GC}

\label{gc-steps}

When a GTD entry group needs to perform GC, the whole model training process via GC is divided into three steps:

\textbf{\ding {172} Regulate valid translations.} First read all the translation pages of this GTD entry group and only keep the valid translations in memory. Then sort the valid translations by their LPNs to make them ordered. 

\textbf{\ding {173} Write valid data back and obtain VPPNs.} First allocate another group of flash blocks to this GTD entry group. Then write the valid pages back to the newly allocated flash blocks. For each GTD entry in this GTD entry group, calculate the VPPNs corresponding to the new PPNs, and combine the VPPNs and LPNs for the training dataset.

\textbf{\ding {174} Train the learned model.} In this step, each GTD entry in this group will train its own DPLR model. For each GTD entry, calculate the offset of VPPNs/LPNs from this GTD entry’s starting VPPN/LPN. Then, perform DPLR model fitting to get the <slope, intercept, offset> model arrays. Finally, evaluate the accuracy of the model and update the bitmap filter (detailed in Section~\ref{three-four}).


\subsubsection{Model Training within Rewrite}

For some scenarios where GC rarely happens, the model training can be integrated into the SSD rewrite process~\cite{cai2015data,maneas2022operational}. The rewrite is a widely used reliability mechanism to reduce retention errors in modern SSDs by periodically reading, correcting, and reprogramming the flash memory. Rewrite happens frequently and is the most significant factor for write amplification~\cite{maneas2022operational}. During SSD rewrite, LPNs of flash pages can be sorted in order so that these pages are written back in contiguous PPNs, which then enables a model to be built and trained on them by LearnedFTL.

\subsection{Bitmap Prediction Filter}

\label{three-four}

The bitmap prediction filter is designed to address the third challenge (i.e., how to determine the correctness of a model PPN prediction). Since it is impossible to achieve 100\% accuracy without an error interval, a fail-safe solution is to perform double reads to fetch the correct data page on the unmatched PPNs. As a result, a prediction filter is needed to confirm whether a requested LPN can predict accurately. 

LearnedFTL uses a bitmap data structure to filter the correct predictions called the bitmap prediction filter. For each LPN in an SSD, LearnedFTL assigns one bit to indicate whether it can predict the real PPN for this LPN. As illustrated in Figure~\ref{fig-model-layer}, for a GTD entry with \emph{M} LPNs, LearnedFTL assigns a bitmap with M bits for all LPNs. During the LPN-PPN translation, when an LPN cannot hit in CMT, LearnedFTL will check this LPN's bit in the bitmap, that is, check whether the corresponding bit is \emph{\textbf{1}}. If yes, it means the model prediction with this LPN is trusted. Then LearnedFTL can directly use the corresponding linear model to calculate the VPPN, translate the VPPN to PPN, and use the PPN to access the data page. If the bit is \emph{\textbf{0}}, LearnedFTL will perform a double read for this LPN and will not use the model to make predictions. With the bitmap prediction filter, LearnedFTL can make only the correct model predictions and avoid miss penalty caused by wrong model predictions. Figure~\ref{fig-model-predict} illustrates two different instances of the bitmap prediction filter.

The bitmap prediction filter is built in the model evaluation process (step 3 in Section~\ref{three-three}). After one DPLR model is trained, all valid LPNs of this GTD entry are input to the trained model to get the predicted PPN. If the predicted PPN is consistent with the real PPN, this LPN corresponding bit in the bitmap is marked as \emph{\textbf{1}}. Otherwise, it is marked as \emph{\textbf{0}}. 

The data consistency of the bitmap is guaranteed upon each update. For each write request with an LPN, LearnedFTL will first check if the corresponding bit of this LPN in the bitmap is \emph{\textbf{1}}. If yes, LearnedFTL will set this bit to \emph{\textbf{0}}. Since persisting the bitmap in NAND flash with each update will incur significant performance overhead, LearnedFTL chooses not to persist them in SSD until a normal power off or rebuild the bitmap by scanning on a power failure as TPFTL and DFTL do.

\begin{figure}[t]
\centering
%\setlength{\abovecaptionip}{5pt}
\includegraphics[scale=0.85]{photos/design/model_predictor.pdf}
\caption{The workflow of bitmap prediction filter.}
\label{fig-model-predict}\vspace{-15pt}
\end{figure}



\subsection{Cost Analysis}

\label{three-cost}
Though LearnedFTL introduces multiple new components to apply the learned index in the FTL, it only introduces minor computational overhead compared to DFTL and TPFTL. We discuss the details as follows.

% \textbf{(1) Write}: For each write request with an LPN, LearnedFTL incurs one additional \textbf{bitmap check} operation. LearnedFTL will check whether this LPN's bit in the corresponding bitmap is \textbf{\emph{1}}. If it is, LearnedFTL will modify this LPN's bit to \textbf{\emph{0}}.

\textbf{(1) Read}: For each read request with an LPN, LearnedFTL incurs two additional operations when this LPN cannot hit in the CMT. The first operation is a \textbf{bitmap check} to check if this LPN can predict a real PPN. The second operation is a \textbf{model prediction} when the answer of the bitmap check is yes. For these LPNs, LearnedFTL will use the model to predict the real PPN instead of an extra flash read. The model prediction includes calculating the VPPN with the \emph{y=kx+b} model and translating the predicted VPPN to PPN. 

\textbf{(2) GC}: The model training incurs two computational overheads during the GC period. The first one is \textbf{sorting} all the LPNs within each GTD entry (Step \ding {172} in Section~\ref{gc-steps}). The second one is \textbf{training} each GTD entry's model (Step \ding{174} in Section~\ref{gc-steps}).

Our experiments in Section~\ref{overhead-analysis} have detailed evaluations to quantitatively analyze these overheads.\vspace{-10pt}