\section{Introduction}\vspace{-5pt} 

Since the last decade, SSDs have replaced traditional hard disk drives (HDDs) as the dominant storage media in diverse computing domains, including personal computers, enterprise servers, and large-scale data centers~\cite{Migrating,BCW}. The prevalence of SSDs comes from their performance superiority thanks to a series of software and hardware technology shifts. To be specific, SSDs replace the platter and spinner structures of HDDs \cite{cornwell2012anatomy,kasavajhala2011solid} with a massive number of CMOS-based NAND flash memory \cite{kasavajhala2011solid,cornwell2012anatomy}, which can deliver a high level of internal I/O parallelism. Moreover, with sophisticated buffer designs and log-structured techniques~\cite{F2FS,ParaFS,IPLFS,grupp2013harey,yoo2020reinforcement}, the SSD write performance has been significantly improved. 

%In particular, \cite{grupp2013harey} schedules flash pages based on operations' performance needs and pages' performance characteristics to improve write performance meanwhile, \cite{yoo2020reinforcement} employs reinforcement learning to SLC cache to enhance SSD write performance. 



% since random write in SSD is slower than sequential write, many works, like log-structured file systems~\cite{F2FS,ParaFS,IPLFS} and log-structured merge tree designs~\cite{RocksDB,LSMTSSD,p2KVS} convert random writes into sequential ones, thus considerably improving the random write performance of SSD-based applications. 
%leaving the random-read a critical performance bottleneck in flash-based SSDs. Moreover, most applications are bonded by the read accesses, making the read performance critically important for SSD-based storage systems.


While the prior works have effectively addressed the write issues in SSDs, the impacts of read behaviors, especially the random reads, are overlooked. Unfortunately, random reads have become a serious  performance bottleneck in SSDs. It is worth noting that most applications are bonded by the read accesses~\cite{IPC}, making the read performance critically important for SSD-based storage systems. To quantitatively analyze the problem of random reads in SSDs, we set up an experiment that executes a set of microbenchmarks on an FEMU SSD simulator~\cite{li2018case} (cf. Section \ref{two-one} for experiment details). Figure~\ref{seqrand} shows the performance of random and sequential reads, indicating that the bandwidth of random reads only reaches up to 60\% of that of sequential reads. To find out the root cause behind the performance degradation of random reads, we collect the statistics of I/O accesses and present the I/O access breakdown in Figure \ref{breakdown}. Random read requests nearly double the number of flash read accesses compared to sequential read requests, in which the extra read I/Os are all used for address translation.
%The number of random-read I/Os is nearly twice that of sequential-read I/Os, and the extra read I/Os are all used for address translation.


\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{5pt}
\includegraphics[scale=0.55]{photos/Overview.pdf}
\caption{Performance \& memory of different FTL designs.}
\label{fig-overview}\vspace{-15pt}
\end{figure}

Address translation is a main function of flash translation layer (FTL). To be specific, modern FTL maintains page mapping information from logical pages to physical pages, that is, for a given logical page number (LPN) from the host, FTL will return its physical page number (PPN) to access the data~\cite{Tradeoffs,FTLSurvey,SSDManagement}. However, page-level mapping requires huge memory space to store the LPN-PPN mappings. To reduce memory usage while gaining high performance, modern SSDs employ demand-based page mapping methods such as the ones in DFTL~\cite{gupta2009dftl} and TPFTL~\cite{zhou2015efficient}. Specifically, DFTL stores all LPN-PPN mappings in flash, and uses a small mapping cache in the SSD memory to store frequently accessed mapping information. With this mapping cache, DFTL and TPFTL can achieve excellent performance with low memory cost under workloads with high spatial and temporal locality. However, as the random reads exhibit poor data locality, the mapping cache designs fail in accommodating the desired mapping information. As a result, almost all random read requests miss in the mapping cache. To handle a cache miss, DFTL and TPFTL need an extra flash read to bring the missing LPN-PPN mapping to the cache, which is followed by another flash read to access the data. 
This phenomenon is referred to \emph{double read problem}, which is observed in Figure~\ref{first-test}. 


%To accelerate the speed of translation, SSD will put the mapping table in internal memory. However, the size of the mapping table keeps expanding as SSD capacity grows, which raises the need for memory and drives up the cost. Some studies have tried to utilize both the spatial and temporal locality of workloads to address this issue, such as the demand-based page mapping method in the DFTL~\cite{gupta2009dftl} and TPFTL~\cite{zhou2015efficient}. The demand-based page mapping is also the FTL implementation in our Figure~\ref{first-test} experiment. To be specific, DFTL stores all LPA-PPA mappings in flash, and uses a small mapping cache in memory to store frequently accessed mappings. With this memory cache, DFTL can achieve excellent performance with low memory cost. 
% Unexpectedly, the demand-based page mapping performs poorly under random reads. Since the cache cannot predict the PPAs for random reads, every time a random access request arrives in the SSD, it will introduce a cache miss and thus generate an extra flash read to bring in the missing address mapping information before accessing the data itself, leading to \emph{the double-read problem} that appears in Figure~\ref{first-test}. 

For further analysis, it is the locality-based cache replacement policy (e.g., LRU policy) supports the mapping cache to obtain high performance with small space. The LRU strategy needs to summarize the frequently accessed mappings from recent visits, which works well under workloads with spatial and temporal localities. However, since there is no regularity for random reads, prediction through recent visits is not feasible. As a result, caching a small part of the mapping with replacement will not work under random reads.

% The double read problem is brought on by the locality-based cache replacement policy (e.g., LRU policy). For instance, under workloads with high localities, LRU policy in DFTL can retain the recently accessed mappings in the mapping cache, thereby achieving high performance with very few mappings. However, due to the uniform access pattern of random reads, the LRU policy fails and such few mappings retained are hard to hit again.
% %the cache replacement policy fails to retain the mappings that will be accessed next.
% As a result, the hit ratio of the mapping cache become very low, incurring double reads.

To solve the problem in random reads, one solution is abandoning the replacement policy and indexing almost all the mappings in cache-sized memory space, so that most random reads can be hit. Fortunately, recent studies on learned index~\cite{ALEX,APEX,kraska2018case,wei2020fast} have proved such a possibility. Learned index builds machine learning models based on the key-position mappings, thus enabling indexing hundreds of mappings with a few parameters and saving a lot of space.
%that replacing traditional cache policy (cache with locality-based replacement policy) with machine learning models can improve data access performance while significantly reducing memory footprint~\cite{kraska2018case, ALEX}. 
Ideally, by adopting the learned index to all LPN-PPNs, one can potentially calculate the PPN of an LPN directly from learned models without double reads.


% Unlike the traditional HDDs, flash memory has unique device characteristics, such as out-of-place update and erase-before-write~\cite{Tradeoffs,FTLSurvey,SSDManagement}. To hide the intrinsic of flash memory and expose a block device interface to the host, flash-based SSDs employ a software indirection layer, called Flash Translation Layer (FTL), which maps the logical page addresses (LPAs) to physical page addresses (PPAs) within the device. 
%In order to represent the flash page more clearly, we use the logical page number (LPN) to represent the LPA and use the physical page number (PPN) to represent the PPA.
%The address mapping scheme is a key component within the FTL because it will directly and significantly determine the SSD performance. As a result, the random read performance of flash-based SSDs is highly dependent on the effectiveness of the page-level mapping scheme in the FTL design.
% The address mapping scheme is a key component of FTL design. Its effectiveness directly and significantly determines SSD random read performance.


% Address mapping is performed by referring to a table that pairs any given LPN with its corresponding PPN. This mapping table is buffered at the SSD internal memory for instant lookup while periodically flushing back to flash memory in case of a power failure. The simplest organization uses page-level mapping which maps any logical page from the host to a physical page in flash. Page-level mapping policy offers the highest flexibility whereas it costs huge DRAM space to store the table, which significantly increases the manufacturing costs. To minimize the DRAM cost, one common solution would be block-level mapping, which maps data at the granule of blocks rather than pages. While the block-level mapping policy is space-efficient, it introduces significant performance issues, especially when write request sizes are smaller than that of a flash block. Figure~\ref{fig-overview} shows the tradeoff between page-level mapping and block-level mapping is one of performance versus memory space. Some studies have tried to utilize the locality of workload in page-level mappings, such as the demand-based page mapping method in the DFTL~\cite{gupta2009dftl} and TPFTL~\cite{zhou2015efficient}. 

% The demand-based page-level mapping method (DFTL) works well under workloads with good spatial and temporal locality. To be specific, DFTL stores all mappings in flash, and uses a small memory cache in DRAM to store frequently accessed mappings. With this memory cache, DFTL can achieve excellent performance with low memory cost. 

However, the limitations of the learned index and the unique characteristics of flash SSD pose three challenges to its direct adoption in FTL. First, the output of any learned model is not completely accurate, what are the learned models most appropriate for FTL? Second, how to handle mispredictions? Furthermore, the learned index requires that the LPN-PPN mappings be sorted. The unique append-only and out-of-place-update characteristics of flash make such sorting impossible. Thus, the third challenge is how to obtain ordered LPN-PPN mappings in flash-based SSD.

% However, the limitations of the learned index make it very difficult, if not impossible, to be applied to FTL directly. One such limitation is that the output of the learned model is not accurate. They produce error intervals that can lead to a page read, incurring multiple page accesses to get the correct flash page, thus causing a very serious read amplification problem. Another limitation of learned models is that you cannot generate them unless both the LPNs and the PPNs are sorted into order. Unfortunately, with the out-of-place-update property of flash memory, such sorting or ordering is impossible. To combat the first limitation, it would require reading multiple flash pages within the error interval to obtain the correct flash page, severely worsening the read performance. It is not possible to combat the second. This is because the unique append-only and out-of-place-update characteristics of flash-based SSD, in which LPNs are distributed in random and incontiguous order across the contiguous PPN space, make it impossible for both LPNs and PPNs to be sorted into order. What all these mean are that applying learned indexing to FTL requires addressing the following challenges: what are the most suitable learned models? how to generate sorted LPN-PPN mappings for model building? And how to ensure the correctness of the model without incurring read amplification?


To tackle these challenges and improve the random read performance of flash-based SSDs, we propose LearnedFTL, a learning-based page-level FTL. To solve the first challenge, LearnedFTL divides the global flash-page space into multiple partitions and fits the LPN-PPN mappings for each partition with a dynamic piece-wise linear regression (DPLR) model to achieve model accuracy. To get ordered LPN-PPN mappings, LearnedFTL utilizes the page relocation characteristics of GC to rearrange flash pages, thus generating sorted LPN-PPN mappings. Furthermore, it also proposes a novel virtual PPN representation to make the PPNs scattered under parallel units continuous. To handle the mispredictions of models, it proposes a bitmap filter to ensure the model’s correctness. 

With the key challenges addressed, LearnedFTL is able to significantly accelerate address translation and reduce translation-induced double-read accesses. Figure~\ref{fig-overview} compares various types of FTL designs in terms of memory consumption and performance. LearnedFTL achieves a performance that is close to the page-level mapping (i.e., storing the entire mapping table in memory) while preserving low memory space overhead. 
%thanks to its state-of-the-art demand-based page-level mapping method.
To the best of our knowledge, this is the first study that explores the feasibility of applying the learned index to address translation in FTL designs.
%We implement a LearnedFTL prototype in the SSD emulator FEMU~\cite{li2018case}, on which we perform experiments with various benchmarks and storage traces. The trace-driven experimental results show that the 99th percentile tail latency of LearnedFTL is on average 6.4$\times$ lower than that of the state-of-the-art TPFTL scheme. Moreover, LearnedFTL also provides much better performance under real-world applications and traces.


%With LearnedFTL, we make the following contributions:

%\begin{itemize}
%\vspace{-5pt}
%\item Our experimental analysis of the random reads in flash-based SSDs reveals that the address-translation-induced double-read accesses to flash storage are the root cause of SSD’s poor random-read performance.\vspace{-5pt}

%\item We present the first study on applying the learned index methodology to the address mapping schemes in the FTL design in flash-based SSDs and propose LearnedFTL. By exploiting features of garbage collection and internal parallelism of flash-based SSDs, LearnedFTL is able to effectively construct the learned models to efficiently translate LPNs to their PPNs. \vspace{-5pt}

%\item We study the efficiency of LearnedFTL on the SSD emulator FEMU. The extensive evaluations validate LearnedFTL's efficacy and advantages over the state-of-the-art FTL designs.
%\end{itemize}\vspace{-5pt}

%The rest of this paper is organized as follows. Section~\ref{background} describes the background and motivation, while Section~\ref{design} presents the design of LearnedFTL. Section~\ref{performance} evaluates the performance of different FTL designs. 
%Section~\ref{related} discusses the related work. Finally, 
%Section~\ref{conclusion} concludes the paper. 

%the double-read problem in flash-based SSDs, followed by the learned index methodology and the challenge analysis of applying the learned index to FTL designs to motivate the LearnedFTL study.