 \section{Performance Evaluation}
%  \vspace{-5pt}
 \label{performance}
%In this section, we first describe the prototype implementation and experiment setup, followed by the extensive evaluation driven by different benchmarks and workloads. Finally, we discuss the overhead in LearnedFTL.

\begin{figure}[t]
    \setlength{\abovecaptionskip}{0em}
    \setlength{\abovecaptionskip}{-0em}
    \subfigure[Memory consumption]{
        \begin{minipage}[t]{0.45\linewidth}
            \centering
            \includegraphics[scale=0.8]{photos/evaluation/space_occupation.pdf}
            \label{fig-space-overhead}
        \end{minipage}
    }
    \subfigure[Computing cost]{
        \begin{minipage}[t]{0.48\linewidth}
            \centering
            \includegraphics[scale=0.8]{photos/evaluation/compute_power.pdf}
            \label{fig-compute-simulation}
        \end{minipage}
    }
    \caption{The memory consumption and computing cost of the additional operations between ARM and X86 processors.}
    
    \label{fig-simu}
\end{figure}

\subsection{Implementation and Experiment Setup}

\textbf{Experiment Setup}: The experiments are conducted on FEMU~\cite{li2018case}. FEMU is a QEMU-based and DRAM-backend SSD emulator that is widely used in recent studies~\cite{li2021loda, zhou2021remap, han2021zns+}. It runs in a machine with two Intel(R) Xeon(R) Gold 5318Y 2.10GHz CPUs and 128GB DRAM. The operating system is Linux with kernel version 5.4.0. The emulated SSD is configured with 32GB logical capacity plus 2GB over-provisioning space and has 64 parallel chips (8 channels and 8 ways per channel). Each flash chip has 256 flash blocks and each flash block has 512 flash pages. The size of a flash page is set to 4KB. The latency of NVMe SSD is 40 $\mu$s for NAND read, 200$\mu$s for NAND write, and 2ms for NAND erase, which are the default settings in FEMU and widely used in the recent flash-based studies~\cite{li2018case,li2021loda,han2021zns+}. Since the SSD rewrite for retention errors is not implemented in FEMU~\cite{maneas2022operational}, we only train models in GC.

LearnedFTL is compared against two representative page-level FTL designs, DFTL~\cite{gupta2009dftl} and TPFTL~\cite{zhou2015efficient}. We also use full-page mapping as a control (denoted as \emph{\textbf{ideal}}, which is considered a performance upper bound). In the experiments, we use both FIO benchmark~\cite{FIO} and real-world applications/traces to evaluate different FTL designs.

\textbf{Prototype implementation}: We implement LearnedFTL by modifying the blackbox mode of the FEMU based on the TPFTL scheme. The CMT has 8192 slots which are about 0.1\% of the total flash pages. According to the allocation strategy and internal parallelism of the SSDs, we group each 64 consecutive GTD entries as a \emph{GTD entry group}. Since the size of a flash page is 4KB and each translation page has 512 LPN-PPN mappings, the GTD has 16384 entries. Each GTD entry group is allocated 64 flash blocks at a time, one for each of the 64 translation pages. For parameter setting in the piecewise linear model, 8 pieces are set by default. 



\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0em}
    \setlength{\abovecaptionskip}{-0em}
    \subfigure[Throughput]{
        \begin{minipage}[t]{0.35\linewidth}
            \centering
            \includegraphics[scale=0.76]{photos/evaluation/ReadAfterRandom1.pdf}
            \label{fig-read-after-read}
        \end{minipage}%
    }
    \subfigure[Model and CMT hit ratio]{
        \begin{minipage}[t]{0.37\linewidth}
            \centering
            \includegraphics[scale=0.76]{photos/evaluation/ReadRandHit.pdf}
            \label{fig-read-rand-hit}
        \end{minipage}
        \vspace{5pt}
    }
    \subfigure[Write amplification]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[scale=0.76]{photos/evaluation/WA.pdf}
            \label{fig-wa}
        \end{minipage}
    }
    
    \caption{The performance results of FIO benchmark under 64 threads (D: DFTL, TP: TPFTL, LD: LearnedFTL, I: ideal FTL).}
    
    \label{double-read}
\end{figure*}

Since the previous demand-based FTLs, such as DFTL and TPFTL, are implemented on trace-driven simulators, such as SSDsim~\cite{hu2011performance} and Flashsim~\cite{kim2009flashsim}, we incorporate them into the FEMU emulator according to their designs in the papers. For their allocation strategy, we use FEMU's default greedy dynamic allocation strategy. We added and modified about 4,000 LoC to implement these baselines and the LearnedFTL in FEMU. The source code of these prototype implementations, along with our LearnedFTL on the FEMU platform, will be released upon the publication of the paper.

\textbf{Memory consumption}: LearnedFTL adds a model layer in GTD on the basis of TPFTL, which brings additional memory space overhead. In LearnedFTL, each model has three parameters, \emph{start\_PPN}, \emph{<k,b,off>[N]} and \emph{bitmap}. For \emph{start\_PPN}, LearnedFTL stores it as a \emph{uint64\_t} integer. For \emph{<k,b,off>[N]}, each integer is a type of \emph{uint8\_t} to save space. For \emph{bitmap}, each slot is a bit, which makes the bitmap the biggest memory space overhead. To sum up, a model in a GTD entry requires 96 Bytes. For a fair comparison, we increase the CMT size in DFTL and TPFTL to ensure the same memory space overhead as that of LearnedFTL. Figure~\ref{fig-space-overhead} illustrates the total memory consumption of DFTL, TPFTL, and LearnedFTL.


\textbf{Controller computing}: Since LearnedFTL adds some additional computing operations mentioned in Section~\ref{three-cost}, it is necessary to correctly simulate the computing power of the SSD controller. The mainstream SSD controller CPUs are ARM's Cortex A series and Cortex R series. we compared the time consumption of executing the additional operations on the FEMU simulated CPU (X86) and a low-end embedded processor (ARM Cortex-A72) and each operation is at the maximum complexity. Figure~\ref{fig-compute-simulation} shows that the ARM A72 processor even performs better than the X86, which shows that we can use the X86 FEMU simulator to simulate LearnedFTL's computing power.



\subsection{FIO Benchmark}
\label{fio-sec}

We first use the FIO benchmark~\cite{FIO} to evaluate the performance of sequential writes, random writes, sequential reads, and random reads for different FTL designs.  


\emph{\textbf{(1) Read}}: 
For random-read and sequential-read evaluations, we first perform 10 minutes of FIO random writes to warm up the whole SSD, then we perform a corresponding FIO read benchmark for 5 minutes. All the above benchmarks use 4KB I/O size and \emph{psync} I/O engine with 64 threads.

Figure~\ref{fig-read-after-read} illustrates the throughput results for different FTL designs under different access patterns. For random read, LearnedFTL outperforms DFTL and TPFTL by 1.5$\times$ and 1.4$\times$, respectively. For sequential read, LearnedFTL outperforms DFTL and TPFTL by 1.1$\times$ and 1.1$\times$, respectively. Moreover, the performance of LearnedFTL is very close to that of the ideal FTL, achieving about 89.2\% and 96.8\% of the performance of the ideal FTL under random reads and sequential reads, respectively.

% Under random reads, there is no locality to be exploited by DFTL and TPFTL. As a result, the random read incurs two flash reads, one for the LPN-PPN translation and another for the data page. By contrast, LearnedFTL uses the learned models built based on the data location. Some requests can get the needed PPNs by model predictions. Those requests that can get correct PPNs through learned models can directly get the data pages based on the predicted PPNs, thus reducing the LPN-PPN translation-induced flash reads.

To explore the behind reasons, we also recorded the percentage of requests that hit the CMT and the learned models during random and sequential reads. The ideal FTL is used as a control which can be considered as an upper bound since its CMT has a hit ratio of 100\% and infinite space.

Figure~\ref{fig-read-rand-hit} shows that the CMT hit ratios of DFTL and TPFTL designs are almost 0 under random reads. The reason is that random reads show no locality, which makes the cache replacement policy fails to capture the access pattern. By contrast, LearnedFTL utilizes the low-cost learned models to index most of the mappings, achieving 55.5\% hit ratios in the models, which reduceing 55.5\% extra flash translation reads. As a result, LearnedFTL significantly improves the random-read performance over DFTL and TPFTL. 

Under sequential reads, LearnedFTL still outperforms DFTL and TPFTL. Although DFTL and TPFTL respectively achieve 61\% and 80\% hit ratios on CMT, their CMT hit ratios are quite different in a single-threaded environment shown in Figure~\ref{hit_ratio}. The reason is that the throughput of DFTL and TPFTL can be affected by parallel threads which in turn affect the locality of sequential reads (i.e., increasing temporal locality but decreasing spatial locality). Driven by parallel threads, the space contention in the CMT may cause some misses, resulting in a lower CMT hit ratio. 


By contrast, LearnedFTL can resolve contentions effectively. Compared to the mapping entries in the CMT, the learned models have a much longer life cycle because they are only rebuilt during GC periods and will not be evicted as the mapping entries in the CMT. As a result, LearnedFTL achieves a combined CMT-Model hit ratio of up to 90\%, eliminating 90\% of the LPN-PPN double reads. Thus, LearnedFTL achieves the best performance among all FTLs and approaches that of the ideal FTL, which is the upper bound.


% \emph{\textbf{(2) Model accuracy}}:
% Another important metric is the model accuracy under sequential reads and random reads. Figure~\ref{fig-diff-model} shows that LearnedFTL achieves a model accuracy of 55.6\% and 80.0\% in random reads and sequential reads, respectively. The reason for the higher accuracy of sequential reads is that when a read request fails to hit in both the CMT and the learned model, the corresponding LPN-PPN translation flash page will be fetched to the CMT, effectively prefetching LPN-PPN mappings of the subsequent requests in the sequential reads. This reduces the model prediction failures and makes the model more accurate for sequential reads than for random reads.

% During experiments, we also find that LearnedFTL has a much lower CMT hit ratio than DFTL and TPFTL under sequential reads. The reason is that when an LPN does not hit in the CMT but hits in the learned model prediction, LearnedFTL does not insert the corresponding PPN to the CMT for space efficiency consideration because a relatively very small CMT space is allocated to LearnedFTL to compensate for its very large learned-model space. As a result, subsequent requests for this LPN will not hit in the CMT but will hit in the learned models in LearnedFTL.



\emph{\textbf{(3) Write}}:
For the random-write and sequential-write evaluations, we perform a corresponding write benchmark in FIO for 10 minutes from the empty state of SSD, and all the evaluations use 4KB I/O size and \emph{psync} I/O engine with 64 threads. 

Figure~\ref{fig-read-after-read} shows that under random writes, LearnedFTL outperforms DFTL and TPFTL by 1.4$\times$ and 1.2$\times$, respectively, because of LearnedFTL's group-based allocation strategy. Since LearnedFTL selects a GTD entry group for each GC, only the translation pages of this GTD entry group need to be updated. That is, a maximum of 64 translation pages are updated per GC. However, for the dynamic allocation strategies used in DFTL and TPFTL, when the same number of data blocks are collected, the LPN range of flash pages written back may be more than 64 translation pages, which causes additional write amplification. 

Owing to the spatial locality of sequential writes, LearnedFTL performs almost the same as DFTL and TPFTL, by less than 2\%. Unlike the dynamic allocation strategy which selects the blocks with the fewest valid pages in each GC, the group-based allocation strategy performs GC on a group-by-group basis, which may result in more valid pages being written back. But the result shows that this does not have a large impact on sequential write performance.

% Fortunately, the opportunistic cross-group allocation allows the hot GTD entry group in sequential writes to use free pages of cold GTD entry groups, reducing the number of valid pages being written back. As a result, the sequential write performance of LearnedFTL is the same as that of DFTL and TPFTL. 


% \emph{\textbf{(4) Effect of queue depth}}:
% To evaluate the read performance under different queue depths in open-loop workloads, we conduct the performance evaluations under the asynchronous engine \emph{libaio} with different I/O depths. 

% Figure~\ref{fig-iodepth} shows the average read latency of the four FTLs under different I/O depths. We can see that LearnedFTL achieves much lower read latency than both DFTL and TPFTL. As the queue depth increases, LearnedFTL outperforms DFTL and TPFTL by up to 41.5\% and approaches that of the ideal FTL at the I/O depth of 64. This indicates that LearnedFTL performs much better under I/O-intensive workloads and approaches the ideal FTL’s performance. 


% \emph{\textbf{(3) Model accuracy}}:
% The model accuracy is the most important metric that affects the performance of LearnedFTL. During experiments (1), we find that whether the requests are random or not, they significantly impact the accuracy of the learned model. In this experiment, we will further explore the factors that affect the accuracy of the learned models.

% \textbf{Load factor of SSD}: Load factor denotes how much valid data is in SSD. We build the learned model over the LPN-PPN translations of a range represented by a GTD entry. A high load factor means more contiguous segments in this range of LPN-PPN mappings. In random writes of the FIO benchmark, the load factor is influenced by the execution time of random writes. To explore the effects of the load factor, we record the hit ratios of the learned models after random writes at different times. Figure~\ref{fig-exec-accu} shows that as the loading factor in the SSD gets higher and higher, the accuracy of the piecewise function becomes much more accurate.


% \begin{figure}[t]
% \centering
% \setlength{\abovecaptionskip}{5pt}
% \includegraphics[scale=0.82]{photos/evaluation/accu_growth.pdf}
% \caption{The model accuracy and accuracy's growth rate under different number of pieces in piecewise linear model.}
% \label{fig-piece}\vspace{-10pt}
% \end{figure}


% \textbf{Number of pieces in piecewise linear model}: Another factor that affects the model accuracy is the number of pieces of the piecewise linear model. The more pieces in the piecewise function, the higher the accuracy with the higher the space overhead. In order to better measure the accuracy and the space occupation under different numbers of pieces in the piecewise linear model, we record the model accuracy and the growth rate of model accuracy as the number of pieces increases. Figure~\ref{fig-piece} shows that the accuracy of the model increases rapidly at the beginning and then gradually flattens. It is also reflected by the growth rate of model accuracy. The growth rate peaks at 7 pieces and then gradually declines. These results validate that selecting 7 pieces for each piecewise linear regression is the most appropriate.


% \emph{\textbf{(4) Training overhead}}: 
% LearnedFTL integrates model training into the GC process. Therefore, it is important to investigate whether the model training process incurs additional overhead to GC. As mentioned in Section~\ref{three-three}, the model training process mainly adds two steps to GC: (1) sort the valid LPNs of the GTD entry group and (2) train the dynamic piece-wise linear regression models. To obtain the time overhead of these two steps, we record the proportion of the time used in these two steps relative to the total runtime consumed by GC at regular intervals. Figure~\ref{fig-time-overhead} illustrates that the proportion of the total time for sorting and model calculations is very low and does not exceed 2\% of total GC time as the run time grows. As a result, the training impact on the GC is minimal.

\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{1pt}
% \setlength{\abovecaptionskip}{-0em}
\includegraphics[scale=0.9]{photos/evaluation/gc_frequency.pdf}
\caption{The GC frequency of all FTL designs under FIO random and sequential write benchmarks.}
\label{fig-gc-frequency}
\end{figure}


% \vspace{-4pt}
\subsection{Overhead Analysis}

\label{overhead-analysis}
LearnedFTL added additional operations to SSD. In this subsection, we evaluate the overhead induced by these operations.

\textbf{(1) GC frequency and write amplification}: In LearnedFTL, model training happens in GC, and LearnedFTL proposes group-based allocation to assist model training. Therefore, the GC frequency and write amplification are critical indicators. Figure~\ref{fig-gc-frequency} illustrates the GC frequency of various FTLs in the FIO write evaluations. Although the GC frequency of LearnedFTL fluctuates, the total number of GCs triggered under random writes and sequential writes of LearnedFTL (4188 and 4285) are less than DFTL (4335 and 4572) and TPFTL (4335 and 4304).
Figure~\ref{fig-wa} also shows that the write amplifications of DFTL and TPFTL are larger than LearneFTL in random writes because the group-based allocation requires fewer translation page writes. For sequential writes, although the group-based allocation may write more valid pages, the write amplification of LearnedFTL is comparable to DFTL and TPFTL.
Overall, our group-based allocation can effectively assist the learning models without causing additional GC and write amplification.

\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{2pt}
    
\includegraphics[scale=0.85]{photos/evaluation/gc_overhead.pdf}
\caption{The time overhead of sorting and training under different running times of FIO random writes (MAX means almost all pages are valid during GC).}
\label{fig-gc-overhead}
\end{figure}

\begin{figure}[t]
    \setlength{\abovecaptionskip}{0em}
    \setlength{\abovecaptionskip}{-0em}
    \subfigure[Write and GC]{
        \begin{minipage}[t]{0.52\linewidth}
            \centering
            \includegraphics[scale=0.8]{photos/evaluation/write_overhead.pdf}
            \label{fig-write-overhead}
        \end{minipage}%
    }
    \subfigure[Read]{
        \begin{minipage}[t]{0.43\linewidth}
            \centering
            \includegraphics[scale=0.8]{photos/evaluation/read_overhead.pdf}
            \label{fig-read-overhead}
        \end{minipage}
    }
    
    \caption{Performance of LearnedFTL with and without additional computing operations (LD: LearnedFTL, ideal LD: ideal LearnedFTL  }
    \label{overhead-two}\vspace{-12pt}
\end{figure}


\textbf{(2) Overhead of training and sorting}: The \textbf{model training} (denoted as training) and \textbf{PPNs-sorting} (denoted as sorting) are two additional operations added to GC. In our implementation, we group 64 GTD entries into one group. In one GC, a maximum of 64 PPN-sorting and model training operations will be triggered for one GTD entry group. As mentioned in Figure~\ref{fig-compute-simulation}, each GTD entry needs about 50$\mu$s for sorting and training in ARM Cortex-A72. The maximum additional overhead incurred by sorting and training are equivalent to about 80 SSD reads ($\mu$s per read), which is negligible since GC for one GTD entry group will incur tens of thousands of SSD reads and writes. Figure~\ref{fig-gc-overhead} show that the time consumption of training and sorting only accounts for up to 3.2\% of the GC execution time.


% Compared to TPFTL and DFTL, as mentioned in Section~\ref{three-cost}, for normal write operations, there are three additional operations in LearnedFTL: \textbf{bitmap check} in SSD write, \textbf{sorting} and \textbf{training} in GC.
GC and write requests do not block each other. To further explore whether they will introduce additional latency, we compare FIO random write performance of LearnedFTL with and without these additional operations. Figure~\ref{fig-write-overhead} shows that their performance difference is nearly negligible (less than 0.7\%), further verifying that the computing overhead of training and sorting is minimal in LearnedFTL.


\textbf{(3) Overhead in read operations}: As mentioned in Section~\ref{three-cost}, only LPNs that can be correctly predicted will perform \textbf{model prediction} (0.65$\mu$s in Figure~\ref{fig-compute-simulation}). This means there is no miss penalty in model predictions. Although there is no miss penalty, if the model prediction takes too long, it will reduce the advantage of reducing double reads. We implement ideal LearnedFTL which put all mappings in memory. For ideal LearnedFTL, each time the bitmap check is yes, it can directly get the PPN through mapping table without model prediction. Figure~\ref{fig-read-overhead} shows that the FIO read performance gap between LearnedFTL and ideal LearnedFTL does not exceed 1\%, demonstrate that the model predictions are lightweight.

% LearnedFTL with model prediction outperforms the one without model by 1.4$\times$ and 1.1$\times$ in random reads and sequential reads, respectively. This result shows that it is worthwhile to use a model prediction instead of an additional translation read.



\subsection{Real-World Applications}
We use Filebench~\cite{Filebench} and RocksDB~\cite{RocksDB} as two real-world applications to evaluate the efficacy of different FTL designs. 
%Before each experiment, we first warm up the SSD with FIO's random write to reach a stable performance state.

\begin{figure}[t]
    \setlength{\abovecaptionskip}{0em}
    \setlength{\abovecaptionskip}{-0em}
    \subfigure[Normalized throughput]{
        \begin{minipage}[t]{0.47\linewidth}
            \centering
            \includegraphics[scale=0.77]{photos/evaluation/rocksdb_all_per.pdf}
            \label{rocksdb_throughput}
        \end{minipage}%
    }
    \subfigure[CMT and model hit ratio]{
        \begin{minipage}[t]{0.48\linewidth}
            \centering
            \includegraphics[scale=0.77]{photos/evaluation/rocksdb_hit.pdf}
            \label{rocksdb_hit_ratio}
        \end{minipage}
    }
    % \vspace{-10pt}
    \caption{Performance results of RocksDB with one thread (D: DFTL, TP: TPFTL, LD: LearnedFTL, I: ideal FTL). }
    
    \label{rocksdb_performance}\vspace{-15pt}
\end{figure}



\textbf{RocksDB}: RocksDB~\cite{RocksDB} is a widely used LSM-Tree-based KV store designed to exploit the parallelism of flash-based SSDs. As we mentioned before, LSM-Trees can merge random writes into sequential ones, but at the cost of relatively poor services to random reads. We deploy RocksDB with EXT4 file system on top of each FTL design and use the \emph{db\_bench} tool of RocksDB with one thread, which is consistent with the previous studies~\cite{yao2020matrixkv, kannan2018redesigning, raju2017pebblesdb}. To evaluate the read performance, we first use the \emph{fillseq} and \emph{overwrite} in $db\_bench$ to write the DB to 80\% full, then we perform \emph{readrandom} and \emph{readseq} in $db\_bench$ to evaluate the read performance in RocksDB. 

In terms of throughput, Figure~\ref{rocksdb_throughput} illustrates that LearnedFTL outperforms DFTL and TPFTL by 1.3$\times$ and 1.3$\times$ in random reads. LearnedFTL also outperforms DFTL and TPFTL by 1.7$\times$ and 1.02$\times$ in sequential reads.


To better understand these results, Figure~\ref{rocksdb_hit_ratio} shows the model and CMT hit ratios recorded in these evaluations. In a single-threaded environment, DFTL does not exploit and thus fails to benefit from the spatial locality, so its CMT hit ratio is zero. TPFTL can achieve an 81\% CMT hit ratio by exploiting the spatial locality. By contrast, since LearnedFTL exploits both the spatial locality and the learned model, it achieves 0.3\% and 46\% CMT hit ratio, 55\% and 41\% model hit ratio in random reads and sequential reads, respectively. 


\begin{table}[!t]   
\small
\begin{center}   
\setlength{\abovecaptionskip}{0pt}
\caption{Filebench configurations.} 
\label{table_filebench_configuration} 
\begin{tabular}{|c|c|c|c|}   
\hline   \textbf{Name} & \textbf{Fileset} & \textbf{Feature} & \textbf{Threads} \\   
\hline   fileserver & 22,500 $\times$ 128KB & write heavy & 50 \\
\hline   webserver & 82,500 $\times$ 16KB & read heavy & 64 \\ 
\hline   randomread & 24,000 $\times$ 1MB & all read & 64  \\
\hline
\end{tabular}   
\end{center}  
\end{table}


\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{5pt}
\includegraphics[scale=0.9]{photos/evaluation/filebench.pdf}
\caption{The normalized throughput of Filebench.}
\label{fig-filebench}\vspace{-15pt}
\end{figure}

\textbf{Filebench}: \emph{Filebench}~\cite{Filebench} is a highly flexible storage benchmark. We select three workloads that are most widely used in previous studies~\cite{zhou2021remap, han2021zns+, bjorling2021zns}: 
\emph{fileserver} (write heavy), \emph{webserver} (read heavy, less random write), and \emph{randomread} (all random reads). Their configurations, consistent with previous studies~\cite{han2021zns+, zhou2021remap}, are summarized in Table~\ref{table_filebench_configuration}.

Figure~\ref{fig-filebench} shows that LearnedFTL outperforms DFTL by 2.4$\times$, 1.5$\times$, and 1.2$\times$ under the three workloads, respectively. LearnedFTL outperforms TPFTL by 1.05$\times$, 1.1$\times$, and 1.2$\times$ under the three workloads, respectively. 
%Moreover, LearnedFTL's performance reaches 97.3\%, 93.4\%, and 91.3\% of the ideal FTL under the three workloads, respectively. 





\begin{table}[t]   
\small
\begin{center}   
\setlength{\abovecaptionskip}{0em}
\setlength{\abovecaptionskip}{-0em}
\caption{Workload characteristics of 4 traces.}
\label{table_trace_configuration} 
\begin{tabular}{|c|c|c|c|}   
\hline   \textbf{Traces} & \textbf{\# of I/O} & \textbf{avg. I/O size} & \textbf{Read ratio} \\   
\hline   WebSearch1 & 1,055,235 & 15.5KB & 100\%  \\
\hline   Websearch2 & 1,200,964 & 15.3KB & 99.98\%  \\ 
\hline   Websearch3 & 793,073 & 15.7KB & 99.96\%  \\
\hline   Systor17 & 1,253,423 & 10.25KB & 61.6\% \\
\hline
\end{tabular}   
\end{center}\vspace{-7pt}    
\end{table}


\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{5pt}
\includegraphics[scale=1]{photos/evaluation/tail_latency.pdf}
\caption{The tail latency results under 3 WebSearch traces (WS\# demotes WebSearch\#).}
\label{fig-tail}\vspace{-15pt}
\end{figure}


\subsection{Real-world Traces}
\vspace{-3pt}

We select four traces (Three WebSearch traces and one Systor trace) to evaluate the efficacy of different FTL designs. The three WebSearch traces are read-intensive workloads that are generated from a popular search engine~\cite{Oltp}. The Systor trace is the enterprise storage traffic on modern commercial office VDI for 28 days~\cite{snia-trace-block-io-4928,Lee2017Understanding}. The four traces all have strong locality. For these traces, we pick the busiest periods (20 minutes to 2 hours). Since the WebSearch traces is relatively old, we re-rated 64 times more intense to reflect modern SSD workloads~\cite{li2021loda}. The workload characteristics of the four traces are summarized in Table~\ref{table_trace_configuration}. Before we replay the four traces, we warm up the whole SSD to a steady state with FIO random writes which is consistent with the previous works~\cite{li2021loda}. Since TPFTL exploits both spatial locality and temporal locality, we choose TPFTL as the baseline for the tail latency evaluation.

% We select three WebSearch traces with the strong temporal and spatial locality to evaluate the efficacy of different FTL designs. WebSearch1, WebSearch2, and WebSearch3 are read-intensive workloads generated from a popular search engine~\cite{Oltp}. We use the 1-hour busiest period of the three traces and scale them up to reflect modern SSD workloads, particularly at the cache layer in datacenters~\cite{yadgar2021ssd}. The workload characteristics of the three traces are summarized in Table~\ref{table_trace_configuration}. Before we replay the three traces, we warm up the whole SSD to a steady state with FIO random writes, which is consistent with the previous study~\cite{li2021loda}. Since TPFTL exploits both spatial locality and temporal locality, we choose TPFTL as the baseline for the tail latency evaluation.


Figure~\ref{fig-tail} shows the \emph{P99}, \emph{P99.9} tail latency of TPFTL, LearnedFTL, and ideal FTL driven by the four traces. Under the four traces, compared to TPFTL, LearnedFTL reduces the \emph{P99} tail latency by 5.3$\times$, 7.4$\times$, 6.5$\times$, and 3.0$\times$ respectively, with an average of 4.8$\times$. Moreover, LearnedFTL also significantly reduces the P99.9 tail latency of TPFTL, by up to 13.9$\times$. LearnedFTL’s tail latency in WebSearch2 and WebSearch3 is extremely close to that of the ideal FTL. Although TPFTL can maintain high CMT hit ratios on workloads with strong locality, sporadic double reads still induce high tail latency. By contrast, LearnedFTL's learned model can further reduce these sporadic double reads by accurate PPN prediction, thus reducing tail latency.
