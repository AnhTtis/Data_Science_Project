
\section{Background and Motivation}
\label{background}
%In this section, we first present the FTL basics in flash-based SSDs, followed by an analysis of the performance impact of random reads. Then we present the learned index basics and describe the challenges of applying the learned index to the FTL design to motivate the LearnedFTL solution. 

%\begin{figure}[t]
%\centering
%\setlength{\abovecaptionskip}{5pt}
%\includegraphics[scale=0.75]{photos/motivation/DFTL.pdf}
%\caption{The structure of DFTL.}
%\label{fig-dftl}\vspace{-15pt}
%\end{figure}

\begin{figure}[t]
    \subfigure[Read throughput]{
        \begin{minipage}[t]{0.46\linewidth}
            \centering
            \includegraphics[scale=0.82]{photos/seq_rand.pdf}
            \label{seqrand}
        \end{minipage}%
    }
    \subfigure[number of I/O]{
        \begin{minipage}[t]{0.46\linewidth}
            \centering
            \includegraphics[scale=0.80]{photos/breakdown.pdf}
            \label{breakdown}
        \end{minipage}
    }
    \vspace{-10pt}
    \caption{The performance of a FEMU-simulated SSD under sequential reads and random reads.}
    \vspace{-10pt}
    \label{first-test}
\end{figure}

\subsection{FTL and Address Translation }
\label{two-one}



Flash memory exhibits unique characteristics, such as out-of-place updates, the erase-before-write feature, and a limited endurance~\cite{hu2011performance,zhou2015efficient,gupta2009dftl}. To hide the intrinsic of flash memory and expose a block device interface to the host, flash translation layer (FTL) is employed to manage the flash memory. For incoming data, FTL simply writes the data to a free page, which is the basic unit of flash read and write, and invalidates the stale page. As a result, address mapping is required to translate a host logical page address (logical page number, LPN) to a physical flash page address (physical page number, PPN), which is called address translation. A flash block that contains several pages, is the basic unit of erase operations. When the number of free blocks decreases to a threshold, garbage collection (GC) is invoked to relocate the valid pages and erase the victim blocks. 
%To manage the flash memory and enable its use as traditional block storage, the SSD controller usually implements FTL as critical firmware~\cite{FTLSurvey} to serve two main purposes: address mapping and garbage collection. Address mapping is required to present a block storage interface because flash memory cannot be updated in place, leading to a mapping table that records the mapping from logical addresses to physical addresses~\cite{yoo2013vssim, han2019wal}. The need for garbage collection stems from flash memory’s unique features of out-of-place updates, page write-granularity and block erase-granularity. That is, updates use free pages and render old pages invalid, which causes erase operations to be triggered to reclaim invalid page space when free page space is exhausted. However, when erasing a block, the valid pages of the block must be first preserved by copying them to a new location before erasing the block. In other words, the garbage collection process of FTL entails erasing blocks to generate new free space from invalid pages while preserving valid pages of these blocks by relocating them in flash memory ~\cite{bux2010performance}.

There exist two basic address translation schemes in FTL designs~\cite{Janus-FTL, FTLSurvey,arpaci2018operating} including page-level and block-level mappings. In page-level mapping, FTL maintains a fine-granule mapping table that records LPN-PPN mappings. With this fine-grained mapping, a page-level mapping scheme achieves excellent performance but requires huge memory capacity to accommodate its mapping table. For example, given an SSD of 10TB with a 4KB page size and an 8B entry for an LPN-PPN mapping, it takes 20GB memory space to store the entire 2.5 billion mapping entries in the mapping table, which is impractical in enterprise SSDs.
Block-level mapping, on the other hand, records the mapping entry at the granularity of a flash block, which consumes significantly lower space overhead than page-level mapping. However, as pages can only go to fixed locations within blocks, page writing operations are limited and the performance is very poor.
% when updating a data page in block-level mapping, it requires transferring all of the valid data pages from a victim block to a clean block so as to enforce mapping regularity. Moreover, block-level mapping requires storing consecutive logical addresses in the same physical block, making it difficult to take advantage of the abundant access parallelism offered by modern NAND flash~\cite{jung2014sprinkler}.

%In the address mapping management of FTL, there are mainly two basic mapping methods: page-level mapping and block-level mapping~\cite{arpaci2018operating}. Page-level mapping maintains a page-level mapping table that records the physical page number (PPN) corresponding to each logical page number (LPN). When writing or updating a page, we can directly modify the related item in the mapping table. Thus, the address translation using a page-level mapping scheme has excellent performance. However, it requires expensive, high-capacity memory space. For example, given an SSD of 1TB with a 4KB page size and an 8B mapping entry, it needs 2GB memory to store the entire 256M mapping entries in a mapping table, which is impractical in enterprise SSDs.

%A solution on the other extreme of the memory space requirement is block-level mapping. Block-level mapping records only the mapping from the logical block number (LBN) to the physical block number (PBN). The address of a data page needs to be addressed by the offset in the block. The space overhead of block-level mapping is significantly lower than that of page-level mapping. However, updating a data page in block-level mapping needs to transfer all valid data pages to another block to enforce the mapping regularity. This characteristic causes a significant performance problem. In addition, the block-level mapping requirement of consecutive logical addresses being stored in the same physical block makes it difficult to take advantage of the abundant access parallelism offered by modern NAND flash~\cite{jung2014sprinkler}.

%\begin{figure}[t]
%\centering
%\setlength{\abovecaptionskip}{5pt}
%    \subfigure[Sequential reads]{
%        \begin{minipage}[t]{0.48\linewidth}
%            \centering
%            \includegraphics[scale=0.7]{photos/motivation/double_read_seq.pdf}
%            \label{sequential-double}
%        \end{minipage}%
%    }
%    \subfigure[Random reads]{
%        \begin{minipage}[t]{0.47\linewidth}
%            \centering
%            \includegraphics[scale=0.7]{photos/motivation/double_read_rand.pdf}
%            \label{random-double}
%        \end{minipage}
%    }
%    \vspace{-5pt}
%    \caption{The double-read in different read scenarios.}
%    \vspace{-10pt}
%    \label{double-read-example}
%\end{figure}

To strike a good balance between performance and memory space, the state-of-the-art approaches store the entire mapping table in flash and cache only parts of the mapping table in SSD controller memory. This is referred to as demand-based page-level mapping~\cite{arpaci2018operating}. The first such solution, DFTL~\cite{gupta2009dftl}, was proposed to exploit the temporal locality of workloads. It stores the whole mapping table in multiple flash pages, called \textbf{translation pages}, and  allocates a small memory space of the controller, called \textbf{Cached Mapping Table (CMT)}, to accommodate the mapping information of several frequently accessed flash pages. The cache is usually a hash-based index with LRU-related replacement strategies. DFTL employs a \textbf{Global Translation Directory (GTD)} in the controller memory to record the physical location of the translation pages within the flash memory. However, each time the SSD controller fails in getting the mapping information of a read request from CMT, the SSD controller has to fetch the missing translation page from the flash memory by referring to GTD. This extra flash read operation is called \textbf{translation read}. Since a read request may generate two flash reads for both data and metadata, it introduces the \textbf{double-read} problem. 
%That is, for an SSD read with an LPN that misses the CMT, this SSD read needs an extra flash read, referred to as \textbf{translation read}, to get the translation page containing the missing mapping information before accessing the data page. 
The double-read problem significantly degrades the SSD read performance. 

A series of demand-based FTLs~\cite{qin2011two, jiang2011s, chen2019hcftl, hu2010achieving, wang2012zftl, zhou2015efficient} have been proposed to alleviate the aforementioned double-read problem by further exploiting the locality characteristics of the workloads. One of the most famous representatives, TPFTL~\cite{zhou2015efficient} proposes two prefetch schemes including request-level prefetch and selective prefetch. With these two prefetch techniques, TPFTL can predictively prefetch several consecutive LPN-PPN mapping entries when TPFTL reads an LPN-PPN mapping.
TPFTL makes full use of the spatial locality within the workload, which greatly alleviates the double-read problem in sequential reads. However, it is non-trivial to resolve the double-read problem for random reads, which will be explained shortly. 
%the double-read problem still exists in random reads, which is non-trivial as explained in the rest of the paper.



\subsection{Performance Impact of Double Reads}\vspace{-5pt}
% Due to the buffer effect and the log-structured techniques, such as F2FS~\cite{F2FS} and RocksDB~\cite{RocksDB}, writes to SSDs are significantly alleviated. As a result, the reads become critically important for the performance of flash-based SSDs. It is primarily for this reason that we focus our attention on the random-read performance of flash SSDs in this paper.

To investigate how the double read degrades the performance of random reads, we conduct an evaluation on TPFTL-based FEMU SSD~\cite{li2018case} driven by microbenmark (i.e., random and sequential reads) under different numbers of threads. As shown in Figure~\ref{seqrand}, no matter the number of threads varies, the performance of random reads is always lower than that of sequential reads (i.e., up to 60\% degradation). We also collect the number of flash I/Os used for data reads (denoted as \emph{data access}) and translation reads (denoted as \emph{addr translation}) under 64 threads. As shown in Figure~\ref{breakdown}, almost all random reads require extra translation reads, that is, random reads in demand-based FTL frequently invoke double reads, which worsens the performance.

%the high \$/GB, SSDs are usually deployed in enterprise and large-scale data centers as a cache layer to absorb random accesses for HDD-based storage~\cite{Migrating, BCW}. However, log-structured file systems (e.g., F2FS~\cite{F2FS}) and log-structured merge tree designs (e.g., RocksDB~\cite{RocksDB}),  convert the random writes into sequential writes for the SSD-based cache layer, making the random reads critically important for the performance of flash-based SSDs. It is primarily for this reason that we focus our attention on the random-read performance of flash SSDs in this paper.

%Although the demand-based FTLs have reduced the double reads under workloads with strong locality, they do not address the double-read problem under workloads full of random accesses. The randomness of such workloads renders it difficult, if not impossible, to prefetch mapping information of future accesses to the CMT in advance, making double reads likely unpreventable. 

% Figure~\ref{double-read-example} shows an example. After processing the read requests with LPN 1 ($L_1$), the mapping information of LPNs 2-10 is also prefetched to the CMT due to leverage spatial locality. For a subsequent sequential read request with LPN 2 ($L_2$), it directly hits in the CMT, thus avoiding the double-read problem. However, for a random read request with LPN 108 ($L_{108}$), as shown in Figure~\ref{random-double}, it has no associations with $L_1$ in terms of the temporal and spatial locality, so the demand-based strategy fails. As a result, $L_{108}$ misses in the CMT and incurs the double-read problem.

\begin{figure}[t]
    \subfigure[Read throughput]{
        \begin{minipage}[t]{0.30\linewidth}
            \centering
            \includegraphics[scale=0.82]{photos/motivation/bandwidth.pdf}
            \label{bandwidth}
        \end{minipage}%
    }
    \subfigure[CMT hit ratio]{
        \begin{minipage}[t]{0.3\linewidth}
            \centering
            \includegraphics[scale=0.82]{photos/motivation/hit_ratio.pdf}
            \label{hit_ratio}
        \end{minipage}
    }
    \subfigure[Different ratio]{
        \begin{minipage}[t]{0.3\linewidth}
            \centering
            \includegraphics[scale=0.82]{photos/motivation/different_ratio.pdf}
            \label{space_ratio}
        \end{minipage}
    }
    \vspace{-10pt}
    \caption{The performance of demand-based FTLs and ideal page-level FTL under sequential reads and random reads.}
    \vspace{-15pt}
    \label{conflict}
\end{figure}




To figure out the reasons behind these performance results, we also conduct the same experiments with various FTL designs: DFTL, TPFTL and ideal FTL. We assume that ideal FTL stores the entire page mapping table in memory. Figures ~\ref{bandwidth} and ~\ref{hit_ratio} show the read throughput and the CMT hit ratio in a single-threaded environment.
%To investigate the performance impact of random reads with various FTL designs, we conduct experiments driven by workloads with random reads and sequential reads on DFTL, TPFTL, and ideal FTL which assumes that the entire page mapping table is stored in memory. The experiments are performed in the FEMU emulator driven by the flexible I/O (FIO) benchmark \cite{FIO}, which is described in detail in Section~\ref{performance}. Figures ~\ref{bandwidth} and ~\ref{hit_ratio} show the read throughput and the CMT hit ratio in a single-threaded environment to better investigate the reasons behind these performance results.
First, Figure~\ref{bandwidth} shows that TPFTL achieves similar throughput as the ideal FTL under sequential reads. This is because TPFTL exploits the spatial locality in sequential reads to significantly reduce the number of translation reads, which in turn achieves a 93\% CMT hit ratio. By contrast, DFTL exploits only the temporal locality in sequential reads, which is ineffective with a 0\% CMT hit ratio. This enforces DFTL to fetch the mapping information from the underlying flash for each read I/O, thus incurring more double reads. 
Second, both DFTL and TPFTL perform poorly in random reads, which is at a level much lower than the ideal FTL. As shown in Figure~\ref{hit_ratio}, the CMT hit ratios are almost 0 (0.1\% for DFTL and 0.09\% for TPFTL), which means that the read amplification is almost 2. These results indicate that double reads occur for nearly all random read requests.

% The above experiments show that the hit ratio of CMT is directly related to the double reads. Under sequential reads, the temporal and spatial localities of the workload allow CMT to maintain a high hit ratio. However, under random reads, CMT completely fails to fit the workloads full of randomness, inducing more double reads in random reads as a result.

% all the demand-based FTLs only fit to the workloads that have a high temporal and/or spatial localities. This unique characteristic, thus, limits the applicability of these demand-based FTLs. Under workloads dominated by random reads, the locality-aware mapping designs, such as demand-based FTLs, become ineffective because of the double read problem. This performance degradation of random reads is much worse in multi-threaded, I/O-intensive environments as the queuing of access requests will lead to read amplification, as detailed in Section~\ref{performance}.

One simple way to improve random read performance is to increase the size of the CMT. However, this solution still performs poorly due to the contention of the memory cache. Figure~\ref{space_ratio} illustrates the changes in TPFTL’s CMT hit ratio when increasing CMT space. The hit ratios of random reads are only 1\%, 5\%, and 25.9\% when the CMT sizes are set as 2\%, 10\%, and 50\% of the total mapping table size, respectively. It is evident that, as long as CMT cannot accommodate the vast majority of the mappings, the contention for CMT will not disappear. As a result, for any practical capacity of the CMT, the prefetched mappings will be replaced frequently and cause a low CMT hit ratio. 

The root cause of why CMT cannot handle random reads comes from its locality-based cache replacement policy, which employs least recently used (LRU) strategies. This policy can identify the locality of workload, and retain the frequently accessed mappings in the cache. That's why it can handle the sequential reads well. However, as there is no frequently accessed mappings under random reads, the LRU policy fails to retain the correct mappings. As a result, the very few mappings that reside in the cache are rarely hit, thus incurring more double reads. What's worse, this policy can also incur fierce competition for cache space, which makes its hit ratio even lower. The above analysis demonstrates that small-capacity caches relying on efficient replacement strategies cannot handle the address translation in random reads.




\subsection{Learned Index and Challenges of Applying It to FTL Designs}
\label{two-four}\vspace{-5pt}

Since a random read request may access any LPN in the entire logical address space, an efficient solution is to place as many mappings as possible in the cache-sized SSD memory space. For example, if the FTL can index 50\% of the mappings in the cache-sized space, it can reduce double reads by 50\% in the random read requests. 
Fortunately, the space-efficiency \textbf{learned index}~\cite{kraska2018case,ALEX,ferragina2020pgm, li2021finedex,Tsunami} has proved such a possibility. 
% The learned index learns several lightweight machine-learning models over the existing data locations. Through these models, learned index can achieve both space efficiency and high performance. Furthermore, recent achievements in the learned index suggest that it may help alleviate the double-read problem if it can somehow be applied to the address-mapping designs in SSDs.
Learned index builds lightweight models for key-position mappings. Hundreds of data locations can be calculated by a model with several parameters, thus reducing memory consumption.
% Multiple studies claim that learned index is a promising solution to solve the problems of enormous space consumption and query performance of existing indexing schemes, such as B+-Tree, hashing-based indexes, and bloom filters~\cite{kraska2018case, ferragina2020pgm}. 
Figure~\ref{fig-principle} illustrates the workflow of the learned index. The main target of the learned index is to train an approximate model (usually a straight line) over the key-position mappings with an error band to make sure that all data are in the interval [$y_{predict}-error$, $y_{predict}+error$]. The workflow consists of the following three steps: sort the keys in increasing order and place them in sequential physical locations (pos) (\ding{182}); train predefined model over the <key, pos> pairs (\ding{183}); use all the <key, pos> pairs as the input of the model to calculate the maximum error band of both the predicted value and the true value (\ding{184}). Moreover, since the positions of keys may be updated, the model must be periodically retrained.

\begin{figure}
\centering
\setlength{\abovecaptionskip}{5pt}
\includegraphics[scale=0.9]{photos/motivation/WorkFlow.pdf}
\caption{The workflow of the learned index.}
\label{fig-principle}\vspace{-15pt}
\end{figure}

%The learned index replaces traditional index structures (B+-Tree node and Hash table) with a certain number of machine learning models. By using machine learning models with fewer parameters, the learned index can significantly reduce space consumption. The reported experiments~\cite{kraska2018case} demonstrate that the learned index can achieve a speed similar to the state-of-art traditional Hash-based indexes while reducing the space consumption by up to 80\%.

The aforementioned workflow shows three essential requirements for applying the learned index in FTL. Firstly, it is essential to group the training data and select a proper model. Secondly, the $key$ in <key, pos> pairs must be sorted, and the $pos$ in <key, pos> pairs must be contiguous to guarantee the accuracy of the approximate model. Lastly, an error interval is in need to guarantee 100\% correctness.


%\subsection{Applying Learned Index to FTL}

Ideally, applying learned index to the demand-based FTL would be a solution to accelerate random reads. By applying the learned index to all LPN-PPN mappings, FTL can calculate the PPNs with the learned models for the LPNs missed from the CMT, thus avoiding the extra flash read for the missing mapping information. However, before applying the learned index to the page-level mapping FTL, we need to examine whether the flash-based SSDs and DFTL meet the aforementioned three requirements of the learned index. 

% While Demand-based FTLs are well optimized for temporal/spatial locality, they perform poorly in random reads that trigger frequent double reads. The learned index is a location-based design that builds index models based on the already stored data. It provides fast mapping translations, which can potentially perfectly accommodate random reads. Therefore, it makes good sense to consider the possibility of applying the learned index to demand-based FTL. By calculating the corresponding PPNs for LPNs that miss the CMT, extra flash accesses for the missing mapping information can be avoided. In other words, if this is possible, it would benefit all workloads, regardless of locality. However, before applying the learned index to the page-level mapping FTL, we need to examine whether flash-based SSDs meet the aforementioned three essential requirements of the learned index. 

%Requirement (1) means that one must decide how to group LPNs-PPNs mappings for learned models and what model should be used within each LPNs-PPNs group in flash-based SSDs. Requirement (2) means that both LPNs and PPNs in each LPNs-PPNs group must be sorted and in order. Requirement (3) means that an error interval must be used to make the model output accurate.

%For requirement (1), a Global Translation Directory (GTD) entry in demand-based FTLs, including the LPNs-PPNs mappings contained in the Translation Page it points to, would be the best unit to establish a model, as shown in Figure~\ref{fig-overall}. The reason is that each entry in GTD is linked to a flash translation page containing a range of contiguous LPNs, recording the mapped PPNs of these LPNs. Therefore, the first challenge is the choice of an appropriate model, that is, \textbf{What kind of learned model is the most suitable?}

To satisfy requirement (1) in flash-based SSDs means deciding how to group LPN-PPN mappings for learned models and what model should be used within each LPN-PPN group. 
%The best way of doing the first part would be to employ a GTD in the demand-based FTL, as this would include the translation pages where the LPN-PPN mappings are stored (cf. Figure~\ref{fig-overall}). 
Since each entry in the GTD is linked to a translation page containing a range of contiguous LPNs and their mapped PPNs (cf. Figure~\ref{fig-overall}). The best way of employing the model is to build learned models for each GTD entry. Therefore, the first challenge is to choose an appropriate model, that is, \textbf{what kind of learned model is the most suitable?} 

%\begin{figure}[t]
%\centering
%\setlength{\abovecaptionskip}{5pt}
%\includegraphics[scale=0.80]{photos/design/multi-channel.pdf}
%\caption{The internal parallelism within flash-based SSDs.}
%\label{fig-multi-channel}\vspace{-15pt}
%\end{figure}

%For requirement (2), due to the unique append-only and out-of-place update characteristics of flash-based SSD, the mapped PPNs of contiguous LPNs are usually distributed in random and incontiguous PPN space. Since DFTL's GTD is organized based on orderly and contiguous LPNs, their mapped PPNs covered by each GTD entry become unordered and incontiguous after SSD writes. To make things worse, the inherent internal parallelism of flash-based SSDs forces pages in different parallel channels to be written in parallel, which tends to scatter the PPNs further. This PPN scattering is worsened still, due to the dynamical allocation strategies of flash-based SSDs. Therefore, the second challenge is, \textbf{How to reorder mapped PPNs of contiguous LPNs?}

To satisfy requirement (2) means that both LPNs and PPNs in each LPN-PPN group must be sorted in order. However, due to the unique characteristics of flash-based SSDs (i.e., append-only and out-of-place update), the mapped PPNs of contiguous LPNs are usually distributed in random and non-contiguous physical space. Even though GTD is organized based on orderly and contiguous LPNs, the mapped PPNs of each GTD entry may be unordered and incontiguous after SSD writes. To make things worse, the SSD internal parallelism and dynamic allocation strategies force flash pages in different flash channels to be scattered further. Therefore, the second challenge is, \textbf{how can we reorder the mapped PPNs of contiguous LPNs?}


%For requirement (3), using the error interval to guarantee 100\% correctness incurs read amplification in flash-based SSDs. The error interval of model predictions will require multiple flash pages in the error interval to be read to obtain the correct flash page, severely worsening read performance. This result is contrary to our design objective of improving read performance. Therefore, the third challenge is, \textbf{How to determine the correctness of a predicted PPN of the learned model?}

To satisfy requirement (3) means using an error interval to make the model output accurate. However, using an error interval to guarantee 100\% correctness generates more flash reads. Specifically, since there is an error interval in the model predictions, multiple pages in the error interval need to be read from NAND flash so as to obtain the correct translation page, which severely worsens the read performance. As the goal of this work is to improve random read performance, therefore, the third challenge is, \textbf{how can we determine the correctness of a predicted PPN in the learned model?}


%Recent advances in the learned index have shown that it can achieve significantly faster lookup times and index space savings~\cite{kraska2018case, ferragina2020pgm}. While there have been many follow-up studies, the question of whether learned indexes can have an impact on page-level FTL designs in flash-based SSDs remains unaddressed. Motivated by the urgent need to resolve the double-read problem caused by random reads in flash-based SSDs, we understand that we cannot apply learned indexing to page-level FTL design without overcoming the aforementioned three main challenges. To this end, we propose LearnedFTL, which uses lightweight learned models, low-cost model training via GC, and a precise prediction filter, to accelerate the random-read performance in flash-based SSDs.
%While there have been many follow-up studies, the question of whether learned indexes can have an impact on page-level FTL designs in flash-based SSDs remains unaddressed.  we understand that we cannot apply learned indexing to page-level FTL design without overcoming the aforementioned three main challenges. To this end, 


Recent advances in the learned index have shown that it can achieve significantly faster lookup times and index space savings~\cite{kraska2018case, ferragina2020pgm}. Motivated by the urgent need to resolve the double-read problem caused by random reads in flash-based SSDs, we propose LearnedFTL, which uses lightweight learned models, low-cost model training via GC, and a precise prediction filter, to accelerate the random-read performance in flash-based SSDs.
