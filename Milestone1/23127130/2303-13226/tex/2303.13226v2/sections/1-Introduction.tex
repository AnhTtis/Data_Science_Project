\section{Introduction}
With the emergence of 3D NAND and NVMe techniques~\cite{GSSA-HPCA21,MGC-HPCA23,AstriFlash-HPCA23}, the capacity and performance of flash-based SSDs have increased significantly. Accordingly, the mapping table size for those using page-level Flash Translation Layer (FTL) cannot be practically deployed in SSDs since it consumes a prohibitively large memory. Moreover, storing the mapping table in SSDs causes the double-read problem of constantly accessing the flash for the address translation, which directly degrades the read performance of SSDs.

To solve this problem, the mainstream demand-based FTL (DFTL) proposes a mapping cache method \cite{gupta2009dftl,zhou2015efficient,chae2020dsftl}. Specifically, to reduce the memory overhead of the page-level mapping table, DFTL exploits the locality of workloads and sets a small-sized mapping cache in SSD internal memory to cache frequently accessed mappings. If an LPA cannot be found in this mapping cache, DFTL needs an extra flash read to fetch the corresponding PPA, followed by another flash read to access the data. This phenomenon is referred to as \emph{double reads}. Since the mapping cache can capture locality well, DFTL generates only a small number of double reads under workloads with high locality, which in turn has minor impacts on SSD performance.

However, the double-read problem in DFTL becomes very severe under random reads. Since there is no obvious data locality in random reads, and the LPNs of continuous requests may separate far away, it is difficult for the mapping cache to buffer the mappings in need. As a result, almost all read requests require double reads~\cite{gupta2009dftl,zhou2015efficient}, which incurs poor random read performance as also validated in the subsection~\ref{two-one}. Unfortunately, in many SSD-based modern applications, the proportion of random access is gradually increasing and even occupies a dominant position. The ability to handle random access becomes critical for FTL. 


How to index the most mappings in cache-sized DRAM memory is the key to increasing the hit rate of random reads. Recent studies on learned index~\cite{ALEX,APEX,kraska2018case,wei2020fast} have demonstrated the feasibility of realizing this goal. Learned index builds machine learning models based on the key-position mappings. With these models, the learned index can index hundreds of mappings with a few parameters. Ideally, by adopting the learned index to all LPN-PPN mappings, one can calculate the PPN of an LPN directly from learned models without double reads.


Unfortunately, the limitations of the learned index pose several challenges. First, the accuracy of the learned index cannot reach 100\%. When the PPN prediction is incorrect, the neighboring flash pages to the predicted PPN need to be probed in order to identify the correct PPN. However, this process results in degraded performance. Second, learned indexes require assigning consecutive PPNs to consecutive LPNs, which conflicts with the internal access parallelism of SSDs. Third, the complex model training of learned indexes will cause additional performance and space overheads. Specifically, training a learned index needs multiple time-consuming computational operations on the critical write path. Moreover, training random write requests requires more space to store the learning models.


\begin{figure*}[t]
    % \setlength{\abovecaptionskip}{0pt}
    \centering
    \includegraphics[scale=0.75]{photos/Figure2_all_structures.pdf}

    \caption{Representative mapping schemes in flash-based SSDs.}
    \label{three_schemes}
    % \vspace{-18pt}
\end{figure*}

To tackle these challenges, we propose LearnedFTL, a learning-based page-level FTL. LearnedFTL combines the learned indexes with the existing demand-based page-level FTL scheme, TPFTL~\cite{zhou2015efficient}, where TPFTL handles the locality workloads, and learned indexes handle the random workloads. Each learned index in LearnedFTL is a piece-wise linear model with adjustable parameters, called the in-place-update linear model. To ensure the accuracy of the model prediction, LearnedFTL equips each in-place-update linear model with a bitmap filter where each bit indicates whether the prediction of an LPN is accurate. To obtain contiguous PPNs for sorted LPNs with SSD's internal parallelism, LearnedFTL proposes a virtual PPN representation to convert the incontiguous PPNs to sequential ones. To reduce the space overhead of model training under random writes, a group-based allocation strategy is proposed to replace the current dynamic allocation strategy. Lastly, LearnedFTL proposes two model training strategies to minimize the performance overhead: first, initializing the model based on sequential write requests, and second, training the model through garbage collection. Overall, this paper makes the following contributions:


\begin{itemize}
\item Our experimental analysis of the random reads in flash-based SSDs reveals that the address-translation-induced double-read accesses to flash storage are the root cause of SSDâ€™s poor random-read performance.  


\item We propose a novel FTL design, LearnedFTL, to improve the read performance of SSDs.
The innovation of LearnedFTL lies in its ability to effectively combine Learned Index and address mapping in FTL that maximizes performance while minimizing modifications. By incorporating learned indexes into the currently popular demand-based FTL, LearnedFTL can enhance random read performance without compromising the ability to handle workloads with locality.
To the best of our knowledge, LearnedFTL is the first FTL design that optimizes for random read performance.
    
\item We propose several optimizations in LearnedFTL to facilitate the learned indexes, including an in-place-update linear model equipped with a bitmap filter to guarantee the accuracy of predictions, a virtual PPN representation to convert the unordered PPNs from different parallel units into contiguous ones, a group-based allocation strategy and two model training strategies to reduce training overheads.
    
\item We implement the prototype of LearnedFTL on the SSD emulator FEMU~\cite{li2018case}. The extensive evaluations validate the efficacy of LearnedFTL over the state-of-the-art FTLs.
\end{itemize}
