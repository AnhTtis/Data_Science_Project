\section{Discussion}
\label{discussion}
% under modification
\textbf{Linearity in page-level mappings}. 
One common concern is why simple linear models can effectively fit page-level mappings. In real-world workloads, write requests often consist of multiple consecutive LPNs. When these LPNs are written to consecutive PPNs, we can observe a linear relationship like \emph{y=x+b}. Furthermore, GC can also contribute to the linearity. During the GC process, the FTL can collect LPNs, sort them, and write them back to consecutive physical pages, resulting in a linear relationship such as \emph{y=kx+b}. The existence of this linearity enables good fitting results of simple linear models. Moreover, these models are easy to train and require less training time. Consequently, employing linear models for fitting purposes emerges as the optimal choice.

% under modification
\textbf{Efficiency in random access}. The superior performance of LearnedFTL in random access comes from its unique learning pattern. Unlike traditional machine learning methods that try to fit access patterns~\cite{yoo2020reinforcement,zhang2019reinforcement}, the learned index models in LearnedFTL fit the relationship between LPN-PPN, that is, the relationship between data and actual locations. In this way, whether it is random access or regular access, the model can stably calculate the PPN corresponding to the LPN, thereby significantly improving the performance of random access.

\textbf{Model's Space Overhead}. The space overhead of learned index models is an additional consideration introduced in FTL. Therefore, how to further reduce the space consumption of the model is a future work. Since LearnedFTL uses rounding mode to calculate PPN and bitmap filter to ensure the accuracy of predictions, the computational precision requirement is not high. As a result, it is possible to consider using lower-precision data types, such as Float8, for model parameters. Apart from that, we can further reduce the space overhead by compressing each model' bitmap through compression or encoding techniques.
