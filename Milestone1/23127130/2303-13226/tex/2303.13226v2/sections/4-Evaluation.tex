\section{Performance Evaluation}
 \label{performance}
%In this section, we first describe the prototype implementation and experiment setup, followed by the extensive evaluation driven by different benchmarks and workloads. Finally, we discuss the overhead in LearnedFTL.

\subsection{Implementation and Experiment Setup}
\label{implementation}

\textbf{Experiment Setup}: The experiments are conducted on FEMU~\cite{li2018case}. FEMU is a QEMU-based and DRAM-backed SSD emulator that is widely used in recent studies~\cite{li2021loda, zhou2021remap, han2021zns+}. It runs in a machine with two Intel(R) Xeon(R) Gold 5318Y 2.10GHz CPUs and 128GB DRAM. The operating system is Linux with kernel version 5.4.0. The emulated SSD is configured with 32GB logical capacity plus 2GB over-provisioning space and has 64 parallel chips (8 channels and 8 ways per channel). Each flash chip has 256 flash blocks and each flash block has 512 flash pages. The size of a flash page is set to 4KB. The latency of NVMe SSD is 40 $\mu$s for NAND read, 200$\mu$s for NAND write, and 2ms for NAND erase, which are the default settings in FEMU and widely used in the recent flash-based studies~\cite{li2018case,li2021loda,han2021zns+}. Since the SSD rewrite for retention errors is not implemented in FEMU~\cite{maneas2022operational}, we only train models in GC.

LearnedFTL is compared against three representative page-level FTL designs, DFTL~\cite{gupta2009dftl}, LeaFTL~\cite{sun2023leaftl}, and TPFTL~\cite{zhou2015efficient}. We also use full-page mapping as a control (denoted as \emph{\textbf{ideal}}, which is considered a performance upper bound). In the experiments, we use both FIO benchmark~\cite{FIOmisc} and real-world applications/traces to evaluate different FTL designs.

\textbf{Prototype implementation}: We implement LearnedFTL by modifying the blackbox mode of the FEMU based on the TPFTL scheme. According to the allocation strategy and internal parallelism of the SSDs, we group each 64 consecutive GTD entries as a \emph{GTD entry group}. Since the size of a flash page is 4KB and each translation page has 512 LPN-PPN mappings, the GTD has 16384 entries. Each GTD entry group is allocated 64 flash blocks at a time, one for each of the 64 translation pages. For parameter setting in the piecewise linear model, 8 pieces are set by default. 

\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0pt}
    \subfigure[Throughput]{
        \begin{minipage}[t]{0.35\linewidth}
            \centering
            \includegraphics[scale=0.76]{photos/evaluation/ReadAfterRandom1.pdf}
            \label{fig-read-after-read}
        \end{minipage}%
    }
    \subfigure[Model and CMT hit ratio]{
        \begin{minipage}[t]{0.37\linewidth}
            \centering
            \includegraphics[scale=0.76]{photos/evaluation/ReadRandHit.pdf}
            \label{fig-read-rand-hit}
        \end{minipage}
        
    }
    \subfigure[Write amplification]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[scale=0.76]{photos/evaluation/WA.pdf}
            \label{fig-wa}
        \end{minipage}
    }    
    \caption{The FIO performance under 64 threads (D: DFTL, TP: TPFTL, LF: LeaFTL, LD: LearnedFTL, I: ideal FTL).}
    % \vspace{-15pt}    
    \label{double-read}
\end{figure*}


Since the previous demand-based FTLs, such as DFTL and TPFTL, and the recent LeaFTL are all implemented on trace-driven simulators, such as SSDsim~\cite{hu2011performance} and Flashsim~\cite{kim2009flashsim}, we incorporate them into the FEMU emulator according to their designs in the papers. We use FEMU's default greedy dynamic allocation strategy for their allocation strategy. Since LeaFTL's paper does not explain how the data in the model buffer is written to the SSD, in this article, the data writing strategy of LeaFTL will be consistent with that of TPFTL. Besides, we also added VPPN representation to LeaFTL to obtain continuous training data. We added and modified about 5,000 LoC to implement these baselines and the LearnedFTL in FEMU. The source code of these prototype implementations can be found in our Github repository~\cite{LearnedFTL}.

\textbf{Memory consumption}: Previous studies on demand-based FTLs usually set the capacity of CMT to about 3\% of the total number of page mappings~\cite{gupta2009dftl,zhou2015efficient}. For a fair comparison, we set the capacity of LeaFTL's model cache to have the same space overhead as the CMT of DFTL/TPFTL. For LearnedFTL, each model in the GTD entry has two parameters, \emph{$<$k,b,off$>$[N]} and \emph{bitmap}. For \emph{$<$k,b,off$>$[N]}, both \emph{k} and \emph{b} are set to a 2B float value (float16), and \emph{off} is set to a 2B integer value. For \emph{bitmap}, each slot is a bit, and there are 512 bits in total. To sum up, an in-place-update linear model requires 128 Bytes. After aggregating the space overhead of all the models, it can be calculated that the total overhead of the models in LearnedFTL is approximately half of the CMT space overhead in TPFTL and DFTL. Therefore, we set the CMT size of LearnedFTL to only accommodate 1.5\% of the total number of mappings to maintain the same memory overhead as other FTLs.


\begin{figure}[t]
    \centering
    % \setlength{\abovecaptionskip}{0pt}
    \includegraphics[scale=0.9]{photos/evaluation/compute_power_1.pdf}
    \caption{The computing overhead of the training operations between ARM and X86 processors.}
    \label{fig-compute-simulation}
    % \vspace{-15pt}
\end{figure}


\textbf{Controller computing}: Since LearnedFTL adds some additional computing operations, it is necessary to correctly simulate the computing power of the SSD controller. The mainstream SSD controller CPUs are ARM's Cortex-A series and Cortex-R series. we compared the time consumption of executing the additional operations on the FEMU simulated CPU (X86) and a low-end embedded processor (ARM Cortex-A72), and each operation is at the maximum complexity. Figure~\ref{fig-compute-simulation} shows that the ARM A72 processor even performs better than the X86, which shows that we can use the X86 FEMU simulator to simulate LearnedFTL's computing power.



\subsection{FIO Benchmark}
\label{fio-sec}

We use the FIO benchmark~\cite{FIOmisc} to evaluate the performance of sequential writes, random writes, sequential reads, and random reads for different FTL designs. For each experiment, we ran at least three times to get the average results.


\emph{\textbf{(1) Read}}: 
For random-read and sequential-read evaluations, we first perform FIO random write and sequential write to warm up the whole SSD. Data is continuously written until the SSD is written over about 6 times to reach a stable state.  Since LeaFTL cannot handle 4KB random writes, the I/O size in the warm-up is 512KB (128 flash pages), which allowed the learned index of LeaFTL to be built normally. Then we perform a corresponding FIO read benchmark for evaluations. All the evaluations use 4KB I/O size and \emph{psync} I/O engine with 64 threads.

Figure~\ref{fig-read-after-read} illustrates the throughput results for d
ifferent FTL designs under different access patterns. For random read, LearnedFTL outperforms DFTL, TPFTL, and LeaFTL by 1.5$\times$, 1.4$\times$, and 1.6$\times$, respectively. For sequential read, LearnedFTL outperforms DFTL, TPFTL, and LeaFTL by 1.1$\times$, 1.1$\times$, and 1.1$\times$, respectively. Moreover, the performance of LearnedFTL is very close to that of the ideal FTL, achieving about 89.2\% and 96.8\% of the performance of the ideal FTL under random and sequential reads, respectively.


To explore the behind reasons, we also recorded the percentage of requests that hit the CMT and the learned models during random and sequential reads. The ideal FTL is used as a control which can be considered as an upper bound since its CMT has a hit ratio of 100\% and infinite space. For LeaFTL, we only count the situation that requires a single flash read (cache hit and model prediction is accurate), which is also marked as a model hit.

Figure~\ref{fig-read-rand-hit} shows that the CMT hit ratios of DFTL and TPFTL designs are almost 0 under random reads. The reason is that random reads show no locality, which makes the cache replacement policy fail to capture the access pattern. In LeaFTL, only 5\% of requests can perform a single flash read. Of the remaining requests, 43\% were triple reads and 52\% were double reads, so LeaFTL performs worst in random reads. By contrast, all learned index models in LearnedFTL can be stored in SSD's memory with 55.5\% accuracy, which means 55.5\% extra flash translation reads can be reduced. Thus, it can significantly improve the random-read performance over other FTL schemes. 


Under sequential reads, LearnedFTL still outperforms DFTL, LeaFTL, and TPFTL. Since each thread competes for cache space, DFTL, TPFTL, and LeaFTL only achieve 61\%, 80\%, and 76\% hit ratios on CMT and models. By contrast, because all models of LearnedFTL can be stored in SSD memory, LearnedFTL can resolve contentions effectively. LPN misses in CMT can be hit in the model, and no cache replacement will occur. As a result, LearnedFTL achieves a combined CMT-Model hit ratio of up to 90\%, eliminating 90\% of the LPN-PPN double reads. Thus, LearnedFTL achieves the best performance among all FTLs and approaches that of the ideal FTL, which is the upper bound.


\begin{figure}[t]
    \centering
    % \setlength{\abovecaptionskip}{0pt}
% \setlength{\abovecaptionskip}{-0em}
    \includegraphics[scale=0.85]{photos/evaluation/gc_frequency.pdf}
    \caption{The GC frequency of all FTL designs under FIO random and sequential write benchmarks.}
    \label{fig-gc-frequency}
    % \vspace{-15pt}
\end{figure}

\emph{\textbf{(2) Write}}:
We perform FIO write for the random-write and sequential-write evaluations, and all the evaluations use 4KB I/O size and \emph{psync} I/O engine with 64 threads. 

Figure~\ref{fig-read-after-read} shows that under random writes, LearnedFTL outperforms other schemes by 1.2$\times$ to 1.4$\times$, respectively, because of LearnedFTL's group-based allocation strategy. With the group-based allocation strategy, LearnedFTL selects one GTD entry group for each GC, only the translation pages of this GTD entry group need to be updated. That is, a maximum of 64 translation pages are updated per GC. However, for the dynamic allocation used in other schemes, when the same number of data blocks are collected, the LPN range of flash pages written back may be more than 64 translation pages, incurring additional write amplification. 

Owing to the spatial locality of sequential writes, Learned FTL performs almost the same as DFTL and TPFTL, by less than 2\%. Unlike the dynamic allocation strategy which selects the blocks with the fewest valid pages in each GC, the group-based allocation strategy performs GC on a group-by-group basis, which may result in more valid pages being written back. 
Fortunately, the opportunistic cross-group allocation allows the hot GTD entry group in sequential writes to use free pages of cold GTD entry groups, reducing the number of valid pages being written back. Thus, LearnedFTL's sequential write performance is the same as other FTLs. 


\begin{figure}[t]
    \centering
    % \setlength{\abovecaptionskip}{0pt}   
    \includegraphics[scale=0.85]{photos/evaluation/gc_overhead.pdf}
    \caption{The time overhead of sorting and training under different running times of FIO random writes (MAX means almost all pages are valid during GC).}
    \label{fig-gc-overhead}
    % \vspace{-10pt}
\end{figure}




\subsection{Overhead Analysis}

\label{overhead-analysis}
We evaluate the overhead induced by additional operations that LearnedFTL brings as mentioned in Section~\ref{three-cost}.

\textbf{(1) GC frequency and write amplification}: In LearnedFTL, model training happens in GC, and LearnedFTL proposes group-based allocation to assist model training. Therefore, the GC frequency and write amplification are critical indicators. Figure~\ref{fig-gc-frequency} illustrates the GC frequency of various FTLs in the FIO write evaluations. Although the GC frequency of LearnedFTL fluctuates, the total number of GCs triggered under random writes and sequential writes of LearnedFTL (4188 and 4285) are less than DFTL (4335 and 4572), LeaFTL(4395 and 4473) and TPFTL (4335 and 4304).
Figure~\ref{fig-wa} shows that the write amplifications of DFTL and LeaFTL are larger than LearneFTL in random writes because the group-based allocation requires fewer translation page writes. For sequential writes, with the assistance of opportunistic cross-group allocation, the write amplification of LearnedFTL is comparable to other FTLs.
In summary, our group-based allocation can effectively assist the model training without inducing additional GC and write amplifications.


\textbf{(2) Overhead of training and sorting}: The \textbf{model training} (denoted as training) and \textbf{LPNs-sorting} (denoted as sorting) are two additional operations added to GC in LearnedFTL. In our implementation, we group 64 GTD entries into one group. During each GC, a maximum of 64 LPNs-sorting and model training operations will be triggered for each GTD entry group. Figure~\ref{fig-compute-simulation} shows each GTD entry needs about 50$\mu$s for sorting and training in ARM Cortex-A72. The maximum additional overhead incurred by sorting and training is equivalent to about 80 SSD reads (40 $\mu$s per read), which is negligible since GC for each GTD entry group will incur tens of thousands of SSD reads and writes. Figure~\ref{fig-gc-overhead} shows that the time overhead of sorting and training only accounts for up to 3.2\% of the GC execution time.


To further explore whether they will introduce additional latency, we compare the FIO random write performance of LearnedFTL with and without training and sorting operations. Figure~\ref{fig-write-overhead} shows that their performance difference is nearly negligible (less than 0.7\%), further verifying that the computing overhead of training and sorting is minimal in LearnedFTL.


\begin{figure}[t]
    % \setlength{\abovecaptionskip}{0em}
    % \setlength{\abovecaptionskip}{-0em}
    \subfigure[Write and GC]{
        \begin{minipage}[t]{0.5\linewidth}
            \centering
            \includegraphics[scale=0.8]{photos/evaluation/write_overhead.pdf}
            \label{fig-write-overhead}
        \end{minipage}%
    }
    \subfigure[Read]{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[scale=0.8]{photos/evaluation/read_overhead.pdf}
            \label{fig-read-overhead}
        \end{minipage}
    }    
    \caption{LearnedFTL with and without additional computing operations (LD: LearnedFTL, ideal LD: ideal LearnedFTL.}
    \label{overhead-two}
    % \vspace{-10pt}
\end{figure}

%As mentioned in Section~\ref{three-cost}, 
\textbf{(3) Overhead in read operations}: Only LPNs that can be correctly predicted will perform \textbf{model prediction} (0.65$\mu$s in Figure~\ref{fig-compute-simulation}). This means there is no miss penalty in model predictions. Although there is no miss penalty, if the model prediction takes too long, it will reduce the advantage of reducing double reads. We implement the ideal LearnedFTL, which puts all mappings in memory. For ideal LearnedFTL, each time the bitmap check is yes, it can directly get the PPN through the mapping table without model prediction. Figure~\ref{fig-read-overhead} shows that the FIO read performance gap between LearnedFTL and ideal LearnedFTL does not exceed 1\%, demonstrating that the model predictions are lightweight.


\subsection{Real-World Applications}


\begin{figure}[t]
    % \setlength{\abovecaptionskip}{0em}
    % \setlength{\abovecaptionskip}{-0em}
    \subfigure[Normalized throughput]{
        \begin{minipage}[t]{0.45\linewidth}
            \centering
            \includegraphics[scale=0.77]{photos/evaluation/rocksdb_all_per.pdf}
            \label{rocksdb_throughput}
        \end{minipage}%
    }
    \subfigure[CMT and model hit ratio]{
        \begin{minipage}[t]{0.46\linewidth}
            \centering
            \includegraphics[scale=0.75]{photos/evaluation/rocksdb_hit.pdf}
            \label{rocksdb_hit_ratio}
        \end{minipage}
    }
    \caption{RocksDB performance with one thread (D: DFTL, TP: TPFTL, LF: LeaFTL LD: LearnedFTL, I: ideal FTL). }    
    \label{rocksdb_performance}
    % \vspace{-15pt}
\end{figure}


\textbf{RocksDB}: RocksDB\cite{RocksDB} is a widely used LSM-Tree-based KV store designed to exploit the parallelism of flash-based SSDs. As we mentioned before, LSM-Trees can merge random writes into sequential ones, but at the cost of relatively poor services to random reads. We deploy RocksDB with EXT4 file system on top of each FTL design and use the \emph{db\_bench} tool of RocksDB with one thread, which is consistent with the previous studies~\cite{ kannan2018redesigning, raju2017pebblesdb}. To evaluate the read performance, we first use the \emph{fillseq} and \emph{overwrite} in $db\_bench$ to write the DB to 80\% full, then we perform \emph{readrandom} and \emph{readseq} in $db\_bench$ to evaluate the read performance in RocksDB. 

In terms of throughput, Figure~\ref{rocksdb_throughput} shows that LearnedFTL outperforms other FTLs by 1.3$\times$ $\sim$ 1.4$\times$ in random reads. LearnedFTL also outperforms other FTLs by 1.02$\times$ $\sim$ 4.0$\times$ in sequential reads. To better understand these results, Figure~\ref{rocksdb_hit_ratio} shows the model and CMT hit ratios recorded in these evaluations. In a single-threaded environment, DFTL does not exploit and thus fails to benefit from the spatial locality, so its CMT hit ratio is zero. TPFTL and LeaFTL can achieve an 81\% CMT hit ratio and 83\% model hit ratio by exploiting the spatial locality. By contrast, since LearnedFTL exploits both the spatial locality and the learned model, it achieves 0.3\% and 46\% CMT hit ratio, 55\% and 41\% model hit ratio in random reads and sequential reads, respectively. 

\begin{table}[t] 
\small
\begin{center}   
% \setlength{\abovecaptionskip}{0pt}
\caption{Filebench configurations.} 
\label{table_filebench_configuration} 
\begin{tabular}{|c|c|c|c|}   
\hline   \textbf{Name} & \textbf{Fileset} & \textbf{Feature} & \textbf{Threads} \\   
\hline   fileserver & 225,000 $\times$ 128KB & write heavy & 50 \\
\hline   webserver & 825,000 $\times$ 16KB & read heavy & 64 \\ 
\hline   varmail & 475,000 $\times$ 16KB & all read & 64  \\
\hline
\end{tabular}   
\end{center}
% \vspace{-15pt}  
\end{table}


\begin{figure}[t]
\centering
% \setlength{\abovecaptionskip}{5pt}
\includegraphics[scale=0.9]{photos/evaluation/filebench.pdf}
\caption{The normalized throughput of Filebench.}
\label{fig-filebench}
% \vspace{-15pt}
\end{figure}

\textbf{Filebench}: Filebench~\cite{Filebench} is a highly flexible storage benchmark. We select three workloads that are most widely used in previous studies~\cite{zhou2021remap, han2021zns+, bjorling2021zns}: 
\emph{fileserver} (write heavy), \emph{webserver} (read heavy, less random write), and \emph{varmail} (read:write=1:1). Their configurations, consistent with previous studies~\cite{han2021zns+, zhou2021remap}, are summarized in Table~\ref{table_filebench_configuration}.


Figure~\ref{fig-filebench} shows the normalized performance of the four FTLs. LearnedFTL outperforms other schemes by 1.1$\times$ to 2.3$\times$. As we mentioned in Challenge \#1 in Section~\ref{two-four}, the inaccuracy of LeaFTL's learned models makes LeaFTL still require many double reads under these workloads with high locality. As a result, LeaFTL's performance is lower than TPFTL and LearnedFTL. Since LearnedFTL preserves the CMT, so most requests with high locality can be hit directly through the CMT. In addition, the learned index models can also make predictions for requests that cannot be hit in the CMT, further improving performance.


\subsection{Real-world Traces}

We select four traces (three WebSearch traces and one Systor trace) to evaluate the efficacy of different FTL designs. The three WebSearch traces are read-intensive workloads that are generated from a popular search engine~\cite{Oltp}. The Systor trace is the enterprise storage traffic on modern commercial office VDI for 28 days~\cite{Lee2017Understanding}. The four traces all have strong locality. For these traces, we pick the busiest periods (20 minutes to 2 hours). Since the WebSearch traces are relatively old, we scale up them to reflect modern SSD workloads~\cite{li2021loda}. The workload characteristics of the four traces are summarized in Table~\ref{table_trace_configuration}. Before replaying the four traces, we warm up the whole SSD to a steady state with the same warm-up method mentioned in Section~\ref{fio-sec}. We choose TPFTL and LeaFTL as the baselines for the tail latency evaluation.

\begin{table}[t]   
\small
\begin{center}   
% \setlength{\abovecaptionskip}{0em}
% \setlength{\abovecaptionskip}{-0em}
\caption{Workload characteristics of four traces.}
\label{table_trace_configuration} 
\begin{tabular}{|c|c|c|c|}   
\hline   \textbf{Traces} & \textbf{\# of I/O} & \textbf{Average I/O size} & \textbf{Read ratio} \\   
\hline   WebSearch1 & 1,055,235 & 15.5KB & 100\%  \\
\hline   Websearch2 & 1,200,964 & 15.3KB & 99.98\%  \\ 
\hline   Websearch3 & 793,073 & 15.7KB & 99.96\%  \\
\hline   Systor17 & 1,253,423 & 10.25KB & 61.6\% \\
\hline
\end{tabular}   
\end{center}  
% \vspace{-15pt}
\end{table}

\begin{figure}[t]
\centering
% \setlength{\abovecaptionskip}{0pt}
\includegraphics[scale=0.9]{photos/evaluation/Learned_tail_latency.pdf}
\caption{The P99 and P999 tail latencies results under four traces (WS\# denotes WebSearch\#).}
\label{fig-tail}
% \vspace{-15pt}
\end{figure}


\begin{figure}[t]
% \vspace{-15pt}
\centering
%\setlength{\abovecaptionskip}{10pt}
\includegraphics[scale=0.76]{photos/evaluation/Trace_Energy_Cost.pdf}
\caption{The energy cost under four traces (WS\# denotes WebSearch\#).}
\label{energy-cost}
% \vspace{-10pt}
\end{figure}

Figure~\ref{fig-tail} shows the \emph{P99} and \emph{P99.9} tail latencies of TPFTL, LearnedFTL, and ideal FTL driven by the four traces. Under these four traces, compared to TPFTL, LearnedFTL reduces the \emph{P99} tail latency by 5.3$\times$, 7.4$\times$, 6.5$\times$, and 2.9$\times$, respectively, with an average of 5.5$\times$. Compared with LeaFTL, LearnedFTL reduces the \emph{P99} tail latency by 7.8$\times$, 12.2$\times$, 9.7$\times$, and 3.0$\times$, respectively, with an average of 8.2$\times$. Moreover, compared with TPFTL and LeaFTL, LearnedFT reduces the \emph{P99.9} tail latency by up to 13.9$\times$ and 21.4$\times$, respectively. LearnedFTLâ€™s tail latency in WebSearch2 and WebSearch3 are extremely close to that of the ideal FTL. Although TPFTL and LeaFTL can maintain high CMT hit ratios or model hit ratios on workloads with strong locality, sporadic double reads and triple reads still induce high tail latency. By contrast, LearnedFTL's learned model can further reduce these sporadic double reads by accurate PPN prediction, thus reducing tail latency.

\subsection{Energy Cost}
We established a basic power/energy model based on NANDFlashSim\cite{NANDFlashSim-TOS16} and conducted tests. Figure~\ref{energy-cost} provides a comparison of energy consumption for each FTL in four real traces. In the three WebSearch traces, LearnedFTL reduces energy consumption by 1.09$\times$ to 1.2$\times$ than TPFTL and LeaFTL, respectively. They perform similar under Systor trace. The reason is that the energy consumption of flash write and erase overwhelms that of flash read. In workloads that are not read-intensive, the reduction in reading energy consumption has limited impact on the total energy consumption. In read-intensive workloads, LearnedFTL can reduce energy consumption compared to other FTLs.
