\subsection{Reinforcement Learning}

IL methods require large amounts of manually labeled data, and diverse drivers may arrive at entirely distinct decisions when presented with identical situations, which leads to uncertainty quandaries during training. In order to obviate the hunger for labeled data, some researchers have endeavored to utilize reinforcement learning (RL) algorithms for autonomous decision planning. The agent can obtain some rewards by interacting with the environment. The objective of RL is to optimize cumulative numerical rewards via trial-and-error. By consistently interacting with the environment, the agent gradually acquires knowledge of the optimal policy to attain the target endpoint. 

With the advancement of artificial intelligence, deep reinforcement learning (DRL) integrates the feature-extracting capabilities of deep learning with the decision-making abilities of conventional reinforcement learning. This facilitates the resolution of dilemmas arising from high dimensionality state and extensive action space, and culminating in end-to-end autonomous driving from state input to action output. 
In this survey, we classify the main RL methods into four parts: Value-based Reinforcement Learning, Policy-based Reinforcement Learning, Hierarchical Reinforcement Learning (HRL), and Multi-Agent Reinforcement Learning (MARL), the reviewed methods are listed in Table \ref{tab:reinforcementlearning}.


% in autonomous driving as follows: a) according to the way the optimal strategy is generated, many approaches can be divided into value-based and policy-based RL methods and b) addition to single-agent reinforcement learning, according to the structure of the model, we survey hierarchical reinforcement learning and multi-agent reinforcement learning.
%Deep reinforcement learning has been more widely used in the field of end-to-end autonomous driving planning. 

% In earlier studies, value-based reinforcement learning approaches are used to generate discrete driving actions. With numerous policy-based reinforcement learning methods proposed and successfully applied to autonomous driving, the output control signal space changes to continuous and more consistent with the real situation. Due to the exponential growth of model parameters in reinforcement learning and the difficulty of setting the reward function artificially, hierarchical reinforcement learning and other methods have been proposed and successfully applied to autonomous driving. There is often more than one vehicle on real roads, and multi-agent reinforcement learning can naturally model real traffic scenarios, therefore learning policies and controllers for multi-agent systems.

\subsubsection{Value-based Reinforcement Learning}

Value-based methods try to estimate the value of different actions in a given state and learn to assign a value to each action based on the expected reward that can be obtained by taking that action in that state. The agent learns to associate the rewards with the states and actions taken in the environment and leverages this information to make optimal decisions \cite{JAS_planning, TVT10}.

Among value-based methods, Q-Learning \cite{Qlearning} stands out as the most prominent. The framework for implementing Q-Learning in end-to-end planning is illustrated in Fig. \ref{fig:DQN}. 
Mnih et al. \cite{VEL1} propose the first deep learning method by a Q-learning based approach that learns directly from screenshots to control signals. Furthermore, Wolf et al. \cite{RL-wolf2017} introduce the Q-learning method into the intelligent vehicle field, they define five different driving maneuvers in the Gazebo simulator \cite{RL-Gazebo}, and the vehicle chooses a corresponding maneuver based on the image information. For the purpose of alleviating the problem of poor stability  with high-dimensional perception input. The Conditional DQN \cite{ chen2020conditional} method is proposed, which leverages a defuzzification algorithm to enhance the predictive stability of distinct motion commands. The proposed model achieves a performance comparable to human driving in specific scenarios


In order to perform high-level decision-making for IVs on specific scenarios, Alizadeh et al. \cite{Alizadeh2019} train a DQN agent combined with DNN which outputs two discrete actions. The safety and agility of the ego vehicle can be balanced on-the-go, indicating that the RL agent can learn an adaptive behavior. Furthermore, Ronecker et al. \cite{Ronecker2019} propose a safer navigating method for IVs in highway scenarios by combining Deep Q-Networks from control theory. The proposed network is trained in simulation for central decision-making by proposing targets for a trajectory planner, which shows that the value-based RL can produce efficient and safe driving behavior in highway traffic scenarios.

The security of end-to-end autonomous driving also raises significant apprehension. Constrained Policy Optimization (CPO) \cite{SafeRL1} is a pioneering general-purpose policy exploit algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Building on this, \cite{SafeRL2} and \cite{SafeRL3} present the Safety Gym benchmark suite and validate several constrained deep RL algorithms under constrained conditions. Li et al. \cite{RLTsy7} introduce a risk awareness algorithm into DRL frameworks to learn a risk-aware driving decision policy for lane-changing tasks with the minimum expected risk. Chow et al. \cite{SafeRL4} propose safe policy optimization algorithms that employ a Lyapunov-based approach \cite{Lya} to address CMDP problems. Furthermore, Yang et al. \cite{SafeRL5} construct a model-free safe RL algorithm that integrates policy and neural barrier certificate learning in a stepwise state constraint scenario.  Mo et al. \cite{RLTsy9} leverage Monte Carlo Tree Search to reduce unsafe behaviors on overtaking subtasks at highway scenarios.

%The proposed safe DRL framework is mainly composed of two components: the risk state estimation module and the safe policy search module. Once the forthcoming state will be riskily predicted by the risk state estimation module using current state information and the action outputted by the RL agent, the MCTS-based safe policy search module will activate to guarantee a safer exploration by adding an additional reward for risk actions.



\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{Pictures/Qlearning.png}
    \caption{The architecture of DQN-based end-to-end  autonomous driving method.}
    \label{fig:DQN}
\end{figure}











\begin{figure}[b]
    \centering
    \includegraphics[width=0.8\linewidth]{Pictures/RL-Kendall2019.png}
    \caption{The actor-critic algorithm used to learn a policy and value function for driving proposed in \cite{RL-kendall2019}.}
    \label{fig:RL-kendall2019}
\end{figure}



\subsubsection{Policy-based Reinforcement Learning}

The value-based approach is limited to providing discrete commands. However, autonomous driving is a continuous process, continuous commands within an uninterrupted span can be controlled at a fine-grained level \cite{TVT4}. Therefore, the continuous approach is better for vehicle control. Policy-based methods hold the potential for high ceilings in high-dimensional action spaces with continuous control commands. These methods exhibit superior convergence and exploration than value-based methods.

The execution of RL on real-world IVs is a challenging assignment. Kendall et al. \cite{RL-kendall2019} implement the Deep Deterministic Policy Gradient (DDPG) \cite{RL-ddpg-lillicrap2015} algorithm on an actual intelligent vehicle, performing all exploration and optimization on-board, as shown in Fig. \ref{fig:RL-kendall2019}. Monocular images are the only input, the agent learns the lane-following policy and achieves human-level performance in a 250m road test. This work marks the first application of implementing deep reinforcement learning on a full-sized autonomous vehicle. To further enhance driving safety and comfort, Wang et al. \cite{RLTsy6} introduce an innovative method for IVs based on the lane-change policy of human experts. This method can be executed on single or multiple vehicles, facilitating smooth lane changes without the need for V2X communication support.

To alleviate the challenge of autonomous driving on congested roads, Saxena et al. \cite{RL-saxena2020} employ the proximal policy optimization (PPO) algorithm \cite{rl-ppo-schulman2017} to learn a control policy in a continuous motion planning space. Their model implicitly simulates interaction with other vehicles to avoid collisions and enhance passenger comfort.  Building on this work, Ye et al. \cite{ye2020} leverage PPO to learn an automated lane change policy on real highway scenarios. Taking the ego vehicle and its surrounding vehicle states as input, the agent learns to avoid collisions and to drive in a smooth manner. Several other studies \cite{guan2020, wuyuanqing2021} have also demonstrated the efficacy of PPO-based RL algorithms in end-to-end autonomous driving policy learning


Training a policy from scratch in RL is frequently time-consuming and difficult. Combining RL with other methods such as imitation learning (IL) and curriculum learning may serve as a viable solution. Liang et al. \cite{RL-liang2018cirl} combine IL and DDPG together to alleviate the problem of low efficiency in exploring the continuous space, an adjustable gating mechanism is introduced to selectively activate four different control signals, which allows the model to be controlled by a central one.  Tian et al. \cite{RLTsy2} leverage an RL method of learning from expert experience to implement trajectory-tracking tasks, which are trained in two steps, an IL method adopted in \cite{IL7} and a continuous, deterministic, model-free RL algorithm to further fine-turn the method. 

To address the learning efficiency limitations of RL methods, Huang et al. \cite{RLTsy3} devise a novel method, which incorporates human prior knowledge in RL methods. When confronted with the long-tail problem of autonomous driving, many researchers have turned their perspective to the exploitation of expert human experience. Wu et al. \cite{RLTsy4} propose a human guidance-based RL method which leverages a novel prioritized experience replay mechanism to improve the efficiency and performance of the RL algorithm in extreme scenarios, the framework of the proposed method is shown in Fig \ref{fig:LvChen}. This method is validated in two challenging autonomous driving tasks and achieves a competitive result.




\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{Pictures/RL_LvChen.png}
    \caption{Framework of the proposed human guidance-based RL algorithm \cite{RLTsy4}.}
    \label{fig:LvChen}
\end{figure}
% \subsubsection{Policy-based Reinforcement Learning}




\subsubsection{Hierarchical Reinforcement Learning}

RL methods have shown great promise in various domains, however, these methods are often criticized for difficult training. Especially in the autonomous driving field, non-stationary scenarios and high-dimensional input data cause intolerable training hours and resource usage \cite{JAS3}. Hierarchical reinforcement learning (HRL) decomposes the total problem into a hierarchy of subtasks, and each subtask has its own goal and policy. The subtasks are organized in a hierarchical manner, with higher-level subtasks providing context and guidance for lower-level ones. This hierarchical organization allows the agent to focus on smaller subproblems, reducing the complexity of the learning problem and making it more tractable. 

Forcing the lane-changing task, Chen et al. \cite{chen_hrl_2019} propose a two-level method. The high-level network learns policies for deciding whether to execute a lane change action, while the low-level network learns policies for executing the chosen commands. \cite{HRL-shi2019} and \cite{li_hrl_2021} also present a two-stage HRL methodology based on \cite{chen_hrl_2019}, where \cite{HRL-shi2019} needs to employ the pure pursuit to track the output trajectory points, and \cite{li_hrl_2021} integrates position, velocity and heading of ego-vehicle to further improve the performance of the low-level controller.  All these proposed methods provide a promising solution for developing robust and safe autonomous driving systems.





\begin{figure}[b]
    \centering
    \includegraphics[width=0.98\linewidth]{Pictures/HRL-Duan2020.png}
    \caption{The framework of hierarchical reinforcement learning (HRL) for self-driving decision-making proposed in \cite{duan2020hierarchical}.}
    \label{fig:HRL-Duan2020}
\end{figure}



The generalizability of HRL is a hot research point. Lu et al. \cite{HRL-Lu2020} propose an HRL approach for autonomous decision-making and motion planning in complex dynamic traffic scenarios, as shown in Fig. \ref{fig:HRL-Duan2020}. The approach consists of a high-level layer and a low-level planning layer, the high-level layer leverages a kernel-based least-squares policy iteration algorithm with uneven sampling and pooling strategy (USP-KLSPI) to solve the decision-making problems. Duan et al. \cite{duan2020hierarchical} divide the whole navigation task into three models. The master policy network is trained to select the appropriate driving task, this policy greatly enhances the generalizability and effectiveness of the model. For the purpose of further improving decision quality in complex scenarios, Cola-HRL \cite{HRLTsy2_2022} is presented based on \cite{duan2020hierarchical}, this method consists of three main components: a high-level planner, a low-level controller, and a continuous-lattice representation of the state space. Both the planner and controller use the state space to generate high-quality decisions. The results show that the Cola-HRL outperforms the other SOTA methods in most scenarios.






% \begin{figure}
%     \centering
%     \includegraphics[width=0.68\linewidth]{Pictures/HRL-Lu2020.png}
%     \caption{The approach proposed in \cite{HRL-Lu2020}: $s_{t}$ is the state perceived by the ego vehicle. $v_{expect}$ is longitudinal speed and $\delta$ is the front steering angle. The thick arrows indicate the transfer of batch data, and the thin arrows represent the transfer of a matrix or several states.}
%     \label{fig:HierarchicalRL}
% \end{figure}




\subsubsection{Multi-Agent Reinforcement Learning}
In real scenarios,  diverse traffic participants are commonly present, and their interactions can have a significant impact on the policy of each other \cite{xu2022bridging}. In the single-agent system, the behavior of other participants is usually controlled based on pre-defined rules, and the predicted behavior of the agent may overfit the other participants, thus leading to a more deterministic policy other than in a multi-agent one \cite{JAS2, TVT7}. Multi-Agent Reinforcement Learning (MARL) is designed to learn the decision-making policies of multiple agents in the environment. One popular modeling method for MARL is the decentralized partially observable Markov decision process (DEC-POMDP). However, the state space expands exponentially with the number of agents, making it more challenging and slow to train a multi-agent system (MAS) \cite{TVT6, xu2022model}.

To reduce the impact of ``the dimension explosion", some effective learning schemes are proposed. Kaushik et al. \cite{Kaushik19} use a simple parameter-sharing DDPG to train the agent for two distinct tasks. By injecting the task into the observation space as a command, the same agent can act both competitively or cooperatively. Wang et al. \cite{wang2020multi} train autonomous agents in three scenarios: a ring network, a figure-of-eight network, and a mini city with various scenarios. Graph information sharing between each agent is integrated in the approach with PPO for continuous action generation, and vehicle communication is permitted within a certain range.

Although RL has been widely studied for lane-changing decision makings, those studies are mainly focused on a single-agent system. MARL methods provide a global perspective on multi-vehicle control. Zhou et al. \cite{zhou2022multi} formulate the lane-changing decision-making of multiple autonomous vehicles coexisting with human-driven vehicles in a mixed-traffic highway scenario. Beyond simple tasks, MARL approaches have great potential to solve decision and planning problems in complex scenarios. Chen et al. \cite{chen2021deep} train agents to evade collisions in a time-critical merging highway scenario. The agents observe the locations and the velocities of the surrounding vehicles and then select corresponding actions. 


Credit assignment is vital for policy learning in cooperative multi-agent scenarios. Han et al. \cite{han2022} introduce an effective reward reallocating mechanism to motivate stable cooperation among IVs using a cooperative policy learning algorithm with Shapley value reward reallocation. The experimental results of this mechanism demonstrate significant improvement of the mean episode system reward in connected autonomous vehicles. Instead of reallocating rewards between agents, Peng et al. \cite{peng2021learning} incorporate the ring measure of social value orientation into the Self-Driven Particles (SDP) system which is a category of MAS. As each constituent agent in an SDP system is self-interested and the relationship between agents is constantly changing. The proposed method, Coordinated Policy Optimization (CoPO), performs local coordination between the agent and its neighbor vehicles within a certain distance, as shown in Fig \ref{fig:CoPO}. Experiments demonstrate that the proposed method outperforms MARL baselines across three main metrics: success rate, safety, and efficiency.



\begin{figure}
    \centering
    \includegraphics[width=0.88\linewidth]{Pictures/MARL_CoPO.png}
    \caption{The framework of the CoPO method proposed in \cite{peng2021learning}: the Local Coordination Factor (LCF) describes an agent’s preference of being selfish, cooperative, or competitive. A Local Coordination for each policy and a Global Coordination to update global LCF are both performed during training.}
    \label{fig:CoPO}
\end{figure}





% \begin{table*}
% \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}  %导言区表格内换行
% \centering
% \caption{MAIN APPROACHES FOR MOTION PLANNING IN AUTONOMOUS DRIVING BASED ON DEEP REINFORCEMENT LEARNING.}
% \label{tab:reinforcementlearning}
% \begin{tabular}{m{2.25cm} m{1.8cm}<{\centering} m{2.5cm}<{\centering}  m{4cm}<{\centering}  m{2.4cm}<{\centering}  m{3cm}<{\centering}}
% \hline
% \rowcolor[rgb]{0.937,0.937,0.937} \multicolumn{1}{c}{\textbf{Article}} & \textbf{Method} & \multicolumn{1}{c}{\textbf{Observation}}      & \multicolumn{1}{c}{\textbf{Output}} & \multicolumn{1}{c}{\textbf{Scenario}}         & \multicolumn{1}{c}{\textbf{\textbf{Simulator}}}        \\ 

% %m{1cm}<{\centering}

% \hline

% Wolf et al. \cite{RL-wolf2017}    & \textbf{Value-based}, DQN  & Front Cam    & Discrete Steering angle    & Lane keeping       & Gazebo \\

% Alizadeh  et al. \cite{Alizadeh2019}  & \textbf{Value-based}, DQN  & Relative distance \& velocity  & Trajectory points  & Lane change     & \tabincell{c}{A self-made environment}\\

% Ronecker  et al. \cite{Ronecker2019} & \textbf{Value-based}, DQN, CNN  & Relative distance \& velocity   & Trajectory points  & \tabincell{c}{Lane change,\\Highway strategy}   & \tabincell{c}{A self-made environment}\\

% Li et al. \cite{RLTsy7} & \textbf{Value-based}, DQN, CNN  & Front Cam  & Discrete lane change action   & \tabincell{c}{Lane change,\\City strategy}    & CARLA \\

% Mo et al. \cite{RLTsy9} & \textbf{Value-based}, DQN, CNN  & Front Cam  & Discrete acceleration \&  lane change action   & \tabincell{c}{Overtakeing,\\Highway strategy}     & SUMO \\

% Kendall  et al. \cite{RL-kendall2019}   & \textbf{Policy-based}, DDPG    & Front Cam    & Continuous steering angle \& speed setpoint    & Lane keeping        & Unreal Engine 4    \\

% Wang et al. \cite{RLTsy6}   & \textbf{Policy-based}, DDPG, DQN, DNN    & Front Cam    & Discrete lane change action   & Lane change      & A self-made environment   \\

% Saxen  et al. \cite{RL-saxena2020}   & \textbf{Policy-based}, PPO   & Lane based grid   & Continuous acceleration \& steering angle   & Highway kinematic	    & An open source simulator \\

% Ye et al. \cite{ye2020}  & \textbf{Policy-based}, PPO   & Relative distance \& velocity   & Discrete lane change action   & Lane change	    & SUMO \\

% Liang  et al. \cite{RL-liang2018cirl}	& \textbf{Policy-based}, DDPG  	& Front Cam, Speed	  & Continuous steering angle, acceleration, braking	 & \tabincell{c}{Straight, One-turn, \\Navigation}	      & CARLA \\

% Tian  et al. \cite{RLTsy2} 	  & \textbf{Policy-based}, BC, DDPG    & Vehicle kinematic	  & Continuous steering angle \& vehicle speed  & Path tracking      & Carsim/Simulink \\

% Huang  et al. \cite{RLTsy3}   & \textbf{Policy-based}, BC, Actor-Critic     & BEV images    &    Continuous target speed \& Discrete lane change action	  & \tabincell{c}{Unprotected left turn, \\Roundabout}      & SMARTS \\

% Wu  et al. \cite{RLTsy4}   & \textbf{Policy-based}, PHIL-TD3	& BEV semantic graph	& Continuous steering angle \& accelerating  & Left-turn, Congestion	   	  & CARLA \\

% Chen et al. \cite{chen_hrl_2019}  & \tabincell{c}{\textbf{HRL}, \\AC, DQN}     & Front Cam  & Trajectory points  & Lane change    & TORCS \\

% Shi	 et al. \cite{HRL-shi2019}  & \tabincell{c}{\textbf{HRL}, \\DQN, DNN}      & Relative distance \& velocity  & Discrete lane change action \& Continuous acceleration  & Lane change   & A self-made environment\\

% Li et al. \cite{li_hrl_2021}  & \tabincell{c}{\textbf{HRL},\\ DQN, DNN}  & Scenario state  & Discrete speed \& steering angle & Navigation    & INTERACTION Dataset \& OpenAI GYM toolkit\\

% Duan  et al. \cite{duan2020hierarchical}  &\textbf{HRL}  & Policy-specific dynamics  & Discrete speed \& steering increment	 &\tabincell{c}{Straight, \\Lane change}	 	& A highway environment  \\


% Lu  et al. \cite{HRL-Lu2020}  & \textbf{HRL}, USP-KLSPI  & 14-DOF dynamics	 & Discrete speed \& steering action   & \tabincell{c}{Left-turn,\\Lane merging}	  & Matlab \\


% Gao et al. \cite{HRLTsy2_2022}  &\tabincell{c}{\textbf{HRL}, \\DDPG, CNN}  & BEV perception data, HD-Map  & Continuous speed \& steering angle 	 &Navigation 	  	& Real-world HD-maps  \\



% Kaushik et al. \cite{Kaushik19}  & \textbf{MARL}, Parameter Sharing DDPG  & \tabincell{c}{Continuous ego state,\\LIDAR}  & Continuous  speed \& steering angle   & Highway navigation    &TORCS  \\

% Wang et al. \cite{wang2020multi}  &\tabincell{c}{ \textbf{MARL}, \\GCN, PPO}    & Relative distance \& velocity   & Continuous acceleration & Road networks   &Flow  \\

% Zhou et al. \cite{zhou2022multi}    & \tabincell{c}{\textbf{MARL},  \\MA2C}    & Relative distance \& velocity   &  Discrete acceleration \& lane change action  & Lane changing    & Highway-env  \\

% Chen et al. \cite{chen2021deep}  & \tabincell{c}{\textbf{MARL}, \\MA2C}    & Relative distance \& velocity   & Discrete acceleration \& lane change action  & Lane merging     & Highway-env  \\

% Han et al. \cite{han2022}    & \tabincell{c}{\textbf{MARL},\\Reward\\Reallocation}    & \tabincell{c}{Front Cam, LIDAR,\\Vehicle kinematic}   & Discrete lane change action  & Mixed traffic  & CARLA  \\

% Peng et al \cite{peng2021learning}      & \tabincell{c}{\textbf{MARL},  \\CoPO}   & \tabincell{c}{Continuous ego state,\\LIDAR}   & Continuous acceleration \& steering angle & Multi scenarios   & MetaDrive  \\



% \hline
% \end{tabular}
% \end{table*}


\begin{table*}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}  %导言区表格内换行
\centering
\caption{MAIN APPROACHES FOR MOTION PLANNING IN AUTONOMOUS DRIVING BASED ON DEEP REINFORCEMENT LEARNING.}
\label{tab:reinforcementlearning}
\begin{tabular}{m{2.25cm} m{1.8cm}<{\centering} m{2.5cm}<{\centering}  m{4cm}<{\centering}  m{2.4cm}<{\centering}  m{3cm}<{\centering}}
\hline
\rowcolor[rgb]{0.937,0.937,0.937} \multicolumn{1}{c}{\textbf{Article}} & \textbf{Method} & \multicolumn{1}{c}{\textbf{Observation}}      & \multicolumn{1}{c}{\textbf{Output}} & \multicolumn{1}{c}{\textbf{Scenario}}         & \multicolumn{1}{c}{\textbf{\textbf{Simulator}}}        \\ 

%m{1cm}<{\centering}

\hline

Wolf et al. \cite{RL-wolf2017}    & \textbf{Value-based}, DQN  & front cam    & discrete Steering angle    & lane keeping       & Gazebo \\

Alizadeh  et al. \cite{Alizadeh2019}  & \textbf{Value-based}, DQN  & \tabincell{c}{ relative distance \& \\ velocity value} & trajectory points  & lane change     & \tabincell{c}{Self-made environment}\\

Ronecker  et al. \cite{Ronecker2019} & \textbf{Value-based}, DQN  & \tabincell{c}{ relative distance \& \\ velocity value}   & trajectory points  & \tabincell{c}{lane change,\\highway strategy}   & \tabincell{c}{Self-made environment}\\

Li et al. \cite{RLTsy7} & \textbf{Value-based}, DQN  & front cam  & discrete lane change action   & \tabincell{c}{lane change \& \\city strategy}    & CARLA \\

Mo et al. \cite{RLTsy9} & \textbf{Value-based}, DQN  & front cam  & \tabincell{c}{discrete acceleration \& \\ lane change action}   & \tabincell{c}{overtakeing \& \\highway strategy}     & SUMO \\

Kendall  et al. \cite{RL-kendall2019}   & \textbf{Policy-based}, DDPG    & front cam    & \tabincell{c}{continuous steering angle \& \\ speed setpoint}    & lane keeping        & Unreal Engine 4    \\

Wang et al. \cite{RLTsy6}   & \textbf{Policy-based}, DDPG, DQN    & front cam    & discrete lane change action   & lane change      & Self-made environment   \\

Saxen  et al. \cite{RL-saxena2020}   & \textbf{Policy-based}, PPO   & lane based grid   & \tabincell{c}{continuous acceleration \& \\steering angle}   & highway kinematic	    & Open source simulator \\

Ye et al. \cite{ye2020}  & \textbf{Policy-based}, PPO   & \tabincell{c}{relative distance \& \\ velocity}   & discrete lane change action   & lane change	    & SUMO \\

Liang  et al. \cite{RL-liang2018cirl}	& \textbf{Policy-based}, DDPG  	& front cam, Speed	  & \tabincell{c}{continuous steering angle, \\ acceleration, braking}	 & \tabincell{c}{navigation}	      & CARLA \\

Tian  et al. \cite{RLTsy2} 	  & \textbf{Policy-based}, BC, DDPG    & vehicle kinematic	  & \tabincell{c}{continuous steering angle \& \\vehicle speed}  & path tracking      & Carsim/Simulink \\

Huang  et al. \cite{RLTsy3}   & \textbf{Policy-based}, BC, AC     & BEV images    &    \tabincell{c}{continuous target speed \& \\ discrete lane change action}	  & \tabincell{c}{unprotected left turn, \\roundabout}      & SMARTS \\

Wu  et al. \cite{RLTsy4}   & \textbf{Policy-based}, PHIL-TD3	& BEV semantic graph	& \tabincell{c}{ continuous steering angle \& \\accelerating}  & left-turn, congestion	   	  & CARLA \\

Chen et al. \cite{chen_hrl_2019}  & \tabincell{c}{\textbf{HRL}, \\AC, DQN}     & front cam  & trajectory points  & lane change    & TORCS \\

Shi	 et al. \cite{HRL-shi2019}  & \tabincell{c}{\textbf{HRL}, \\DQN}      & relative distance \& velocity  & \tabincell{c}{discrete lane change action \& \\ continuous acceleration}  & lane change   & Self-made environment\\

Li et al. \cite{li_hrl_2021}  & \tabincell{c}{\textbf{HRL},\\ DQN}  & scenario state  & \tabincell{c}{discrete speed \& \\ steering angle}    & INTERACTION dataset & OpenAI GYM toolkit\\

Duan  et al. \cite{duan2020hierarchical}  &\textbf{HRL}  & policy-specific dynamics  & \tabincell{c}{discrete speed \& \\ steering increment}	 &\tabincell{c}{lane change}	 	& Highway environment  \\


Lu  et al. \cite{HRL-Lu2020}  & \textbf{HRL}, USP-KLSPI  & 14-DOF dynamics	 & \tabincell{c}{discrete speed \& \\steering action}   & \tabincell{c}{lane merging}	  & Matlab \\


Gao et al. \cite{HRLTsy2_2022}  &\tabincell{c}{\textbf{HRL}, \\DDPG, CNN}  & BEV perception data, HD-Map  & \tabincell{c}{ continuous speed \& \\ steering angle} 	 &navigation 	  	& Real-world HD-maps  \\



Kaushik et al. \cite{Kaushik19}  & \textbf{MARL}, DDPG  & \tabincell{c}{continuous ego state,\\LIDAR}  & \tabincell{c}{continuous speed \& \\ steering angle}   & highway navigation    &TORCS  \\

Wang et al. \cite{wang2020multi}  &\tabincell{c}{ \textbf{MARL}, \\PPO}    & relative distance \& velocity   & continuous acceleration & road networks   &Flow  \\

Zhou et al. \cite{zhou2022multi}    & \tabincell{c}{\textbf{MARL},  \\MA2C}    & relative distance \& velocity   &  discrete acceleration & lane change action    & Highway-env  \\

Chen et al. \cite{chen2021deep}  & \tabincell{c}{\textbf{MARL}, \\MA2C}    & relative distance \& velocity   & \tabincell{c}{discrete acceleration \& \\ lane change action}  & lane merging     & Highway-env  \\

Han et al. \cite{han2022}    & \tabincell{c}{\textbf{MARL},\\Reward\\Reallocation}    & \tabincell{c}{front cam, LIDAR,\\vehicle kinematic}   & discrete lane change action  & mixed traffic  & CARLA  \\

Peng et al \cite{peng2021learning}      & \tabincell{c}{\textbf{MARL},  \\CoPO}   & \tabincell{c}{continuous ego state \& \\LIDAR}   & \tabincell{c}{continuous acceleration \& \\ steering angle values} & multi scenarios   & MetaDrive  \\



\hline
\end{tabular}
\end{table*}