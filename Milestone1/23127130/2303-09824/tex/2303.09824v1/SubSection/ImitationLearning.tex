\subsection{Imitation Learning}
Imitation learning (IL) refers to the agent learning policy based on expert trajectory, which generally provides expert decisions and control information \cite{TIV221}. Each expert trajectory contains a sequence of states and actions, and all ``state-action" pairs are extracted to construct datasets. In the IL task, the model leverages constructed dataset to learn the latent relationship between state and action, the state stands for a feature and the action demonstrates labels. Thus, the specific objective of IL is to appraise the most fitness mapping between state and action, so that the agent achieves the expert trajectories as much as possible. Table \ref{tab:imitationlearning} presents all famous imitation learning methods reviewed in this part.




% % Please add the following required packages to your document preamble:
% % \usepackage[table,xcdraw]{xcolor}
% % If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
% \begin{table*}[]
% \caption{THE CRUCIAL REVIEWS AND RELATIVE INFORMATIONS OF EACH FAMOUS END-TO-END MODELS IN AUTONOMOUS DRIVING.}
% %\begin{tabular}{ m{2.3cm}  m{1cm}  m{2cm}  m{2cm}  m{3cm}  m{6cm}}
% \begin{tabular}{m{2.25cm} c m{2cm}  m{2.5cm}  m{3cm}  m{5.2cm}} 
% \hline
% \rowcolor[rgb]{0.937,0.937,0.937} \multicolumn{1}{c}{\textbf{Article}} & \textbf{Category} & \multicolumn{1}{c}{\textbf{Raw Data}}                          & \multicolumn{1}{c}{\textbf{Dataset}}                                    & \multicolumn{1}{c}{\textbf{Implement Tasks}}                    & \multicolumn{1}{c}{\textbf{Detail}}                                                                                                                                                                       \\ 
% \hline
% Codevilla et al. \cite{CIL}    & \textbf{BC}       & Monocular image                                               & Carla                                            & Simulation navigation task                                           & Use high-level commands as a switch to select the appropriate branch                                                                                                                                     \\
% Chen et al. \cite{IL_Inter1}   & \textbf{BC}       & Monocular image                                               & TORCS Dataset and KITTI                                     & Simulation navigation task                                           & Embedded an affordance into model, the affordance learns from perception data and is used to predict control actions                                                                                     \\
% Sauer et al. \cite{IL_Inter2}  & \textbf{BC}       & Monocular video and directional input                         & Carla                                            & Physical navigation task, handle traffic light and speed signs       & Conditional affordance is trained to calculate low-dimension intermediate representations from video and high-level command                                                                              \\
% Zeng et al. \cite{IL_Inter4}   & \textbf{BC}       & Lidar data and HD Map                                         & Physical Dataset collected in North America                 & Physical Navigation Task with trajectory-3D Detection as output      & Propose the first learnable and interpretable end-to-end motion planner, leverage intermediate representations to improve the interpretability of the end-to-end model                                   \\
% Sadat et al. \cite{IL_Inter5}  & \textbf{BC}       & Lidar data and HD Map                                         & Physical Dataset collected in North America                 & Physical navigation task                                             & Propose a joint system for self-driving vehicles including perception, prediction, and motion planning, which can produce interpretable intermediate representations.                                    \\
% Ross et al. \cite{IL8}         & \textbf{DPL}      & Monocular image                                               & A 3D racing game and Super Mario Bros                       & Autonomous racing competition and character control                  & Propose an iterative algorithm, which learns a stationary deterministic policy guaranteed to perform well under its induced distribution of states                                                       \\
% Zhang et al. \cite{IL10}       & \textbf{DPL}      & Monocular image                                               & A racing car simulator, named TORCS and its dataset         & Autonomous racing competition                                        & Embedded query-efficient model to further reduce the requirement for the label in expert trajectories.                                                                                                   \\
% Yan et al. \cite{IL12}         & \textbf{DPL}      & LiDAR, ego-vehicle speed, Sub-goal               & Physical and Simulate robot platform                        & Physical and simulation navigation task                              & Fuse the novice policy and the expert policy to control the robot with a quantified safe criterion.                                                                                                      \\
% Li et al. \cite{OIL}           & \textbf{DPL}      & Monocular image and sub-goal                                  & Sim4CV                                              & Autonomous racing                                        & Provide a reward-based online learning method for multiple experts which allows learning optimal control policies from sub-optimal expert trajectories.                                                  \\
% Ohn-Bar et al. \cite{LSD}      & \textbf{IRL}      & Monocular image                                               & Carla                                            & Simulation navigation task                                           & Embedded scenario context network into policy learning network, and both networks are optimized by online iterative training strategy                                                                    \\
% Levine et al. \cite{BIRL2}     & \textbf{IRL}      & BEV image for simulation environment                          & Highway driving simulator        & Keep the lane, change lanes and takeover                             & Leverage the Gaussian algorithm to learn the reward function and determine the relevance of features which is presented in the expert trajectory.                                                        \\
% Brown et al. \cite{BIRL3}      & \textbf{IRL}      & Monocular image                                               & Highway driving simulator                                           & Keep the lane, change lanes and takeover                             & Propose a sampling method based on Bayesian inverse reinforcement learning and calculate the practical high-confidence upper bounds on the Î±-worst-case difference by expert   trajectories.             \\
% Palan et al. \cite{BIRL4}      & \textbf{IRL}      & Monocular image                                               & Lunar lander simulator                                      & Keep the lane, change lanes and takeover                             & Propose a probabilistic method based on maximum entropy, and provides a well-defined, globally normalized reward function for decision sequences problem                                                 \\
% Ziebart et al. \cite{MEIRL}    & \textbf{IRL}      & Road network, Sub-goal, and GPS Data                          & Driver route modeling                                       & Long range autonomous navigation task                                & Develop a probabilistic approach based on the principle of maximum entropy and provide a well-defined, globally normalized distribution over decision sequences.                                         \\
% Lee et al. \cite{MEIRL4}       & \textbf{IRL}      & Monocular image                                               & NGSIM and Carla & Keep the lane, change lanes and takeover                             & Leverage the expert trajectory to learn a coarse reward function and use the demonstrations to ground the query generation process.                                                                      \\
% Ho et al. \cite{MEIRL5}        & \textbf{IRL}      & Monocular image                                               & Carla                                            & Keep the lane, change lanes and takeover                             & Integrate Generative Adversarial Network (GAN) to IRL, treat the expert trajectory as a true label, and then approximate the expert policy by generating adversarial training                            \\
% Phan et al. \cite{Extend} & \textbf{IRL}      & BEV image, HD map, obstacle information & Autonomous driving dataset from the Las Vegas Strip         & Autonomous driving in real-life state with heavy traffic             & Propose an IRL planner with three steps, generate a set of proposed trajectories, filter trajectories with an interpretable safety filter, then leverage a network to score each   remaining trajectory. \\ \hline

% \end{tabular}
% \label{tab:imitationlearning}

% \end{table*}

\begin{table*}
\centering
\caption{THE CRUCIAL REVIEWS AND RELATIVE INFORMATION OF EACH FAMOUS END-TO-END MODELS IN AUTONOMOUS DRIVING.}
\label{tab:imitationlearning}

\begin{tabular}{m{2.25cm} c m{2cm}<{\centering}  m{2.5cm}<{\centering}  m{2.4cm}<{\centering} m{3.5cm} m{2cm}<{\centering}}
\hline
\rowcolor[rgb]{0.937,0.937,0.937} \multicolumn{1}{c}{\textbf{Article}} & \textbf{Category} & \multicolumn{1}{c}{\textbf{Input}}      & \multicolumn{1}{c}{\textbf{Output}} & \multicolumn{1}{c}{\textbf{Implement Tasks}} & \multicolumn{1}{c}{\textbf{Auxiliary Method}}                                              & \multicolumn{1}{c}{\textbf{\textbf{Dataset}}}        \\ 
\hline
Bojarski et al.\cite{IL2}                                                        & \textbf{BC}       & monocular image                         & steering angle                      & lane Keeping                                 & CNN is the only component of end-to-end model                                              & physical $\&$ simulate platform    \\
Codevilla  et al.\cite{CIL}                                                       & \textbf{BC}       & monocular image                         & control information                 & simulation navigation task                   & High-level commands as a switch to select the branch                                       & Carla                                                \\
Chen et  al.\cite{IL_Inter1}                                                           & \textbf{BC}       & monocular image                         & control information                 & simulation navigation task                   & Affordance is used to predict control actions                                              & TORCS Dataset $\&$ KITTI                              \\
Sauer et  al.\cite{IL_Inter2}                                                          & \textbf{BC}       & monocular video $\&$ directional input   & control information                 & physical navigation task                     & Conditional affordance is trained to calculate intermediate representations                & Carla                                                \\
Zeng et al.\cite{IL_Inter4}                                                            & \textbf{BC}       & Lidar data $\&$ HD Map                   & trajectory, scenario representation & physical navigation task                     & The intermediate representation is used to improve the model's interpretability            & physical dataset collected in North America          \\
Sadat  et al.\cite{IL_Inter5}                                                          & \textbf{BC}       & Lidar data $\&$ HD Map                   & trajectory, scenario representation & physical navigation task                     & A joint system with~interpretable intermediate representations for interpretable planner   & physical dataset collected in North America          \\
Ross et  al.\cite{IL8}                                                            & \textbf{DPL}      & monocular image                         & control information                 & autonomous racing competition                & An iterative algorithm is proposed to guarantee the performance in corner cases            & 3D racing simulator                                     \\
Zhang et  al.\cite{IL10}                                                          & \textbf{DPL}      & monocular image                         & control information                 & autonomous racing competition                & Embedded query-efficient model to reduce the requirement for expert trajectories           & racing car simulator               \\
Yan et  al.\cite{IL12}                                                            & \textbf{DPL}      & LiDAR, ego-vehicle speed, Sub-goal      & control information                 & physical $\&$ simulation navigation task      & The novice and the expert policy is fused to control the robot                             & physical and simulate platform    \\
Li et  al.\cite{OIL}                                                              & \textbf{DPL}      & monocular image $\&$ sub-goal            & waypoint, control information       & autonomous racing                            & A reward-based online method learns from multiple experts                                  & Sim4CV                                               \\
Ohn-Bar  et al.\cite{LSD}                                                        & \textbf{IRL}      & monocular image                         & control information                 & simulation navigation task                   & Scenario context is embedded into the policy learning network                              & Carla                                                \\
Levine  et al.\cite{BIRL2}                                                         & \textbf{IRL}      & BEV image    & control information                 & keep the lane, change lanes $\&$ takeover     & The gaussian algorithm is used to learn the relevance of features in expert trajectories.  & Highway driving simulator                            \\
Brown  et al.\cite{BIRL3}                                                          & \textbf{IRL}      & monocular image                         & control information                 & keep the lane, change lanes $\&$ takeover     & The high-confidence upper bounds on the $alpha$-worst-case are embedded into the policy network. & Highway driving simulator                            \\
Palan et  al.\cite{BIRL4}                                                          & \textbf{IRL}      & monocular image                         & control information                 & keep the lane, change lanes $\&$ takeover     & A globally normalized reward function is constructed.                                      & Lunar lander simulator                               \\
Ziebart  et al.\cite{MEIRL}                                                        & \textbf{IRL}      & Road network, Sub-goal, $\&$ GPS Data    & control information                 & long range autonomous navigation task        & A probabilistic approach is proposed for maximum entropy                                   & Driver route modeling                                \\
Lee et  al.\cite{MEIRL4}                                                            & \textbf{IRL}      & monocular image                         & control information, costmap        & keep the lane, change lanes $\&$ takeover     & The query generation process is used to improve the generalization                         & NGSIM $\&$ Carla                                      \\
Ho et  al.\cite{MEIRL5}                                                             & \textbf{IRL}      & monocular image                         & control information                 & keep the lane, change lanes $\&$ takeover     & GAN is integrated into the end-to-end model                                                & Carla                                                \\
Phan  et al.\cite{Extend}                                                           & \textbf{IRL}      & BEV image, HD map, obstacle information & Control information                 & physical navigation task                      & A three-step IRL planner is proposed                                                       & physical dataset from the Las Vegas Strip  \\
\hline
\end{tabular}
\end{table*}

%There are three widely used training approaches \cite{IL1}, first manifests as a negative approach, named Behavioral Cloning (BC), in which an agent is provided with a training dataset that performs the full trajectory according to the optimal policy; The second builds on the first approach, named Direct Policy Learning (DPL), which leverages expert trajectory to overcome convergence problems in the training stage; The last is a task-dependent approach, named Inverse Reinforcement Learning (IRL) approach, which assumes a reward function to fitting input and output. In this section, Imitation Learning is reviewed in these three categories.

There are three widely used training methods \cite{IL1}, first manifests as a negative method, named behavioral cloning (BC); The second builds on BC, named direct policy learning (DPL); The last is a task-dependent method, named inverse reinforcement learning (IRL) method.

\subsubsection{Behavioral Cloning}
%interpretable in this part
Behavioral Cloning (BC) manifests as the primary method of IL in autonomous driving \cite{IL2,zhu2022multi}. The agent leverages the state-action pairs from the expert to the training model and then replicates the strategy using a classifier/regressor. BC is a passive method, where the objective is to learn the target strategy by passively observing the complete execution of commands, however, this requires the premise that the state-action pairs in all trajectories are independent.
Bojarski et al. \cite{IL2} construct a pioneering framework for BC, which trains a convolutional neural network to only compute steering from a front-view monocular camera. This method exclusively outputs lateral control while ignoring longitudinal commands, rendering it can only be implemented in a limited number of uncomplicated scenarios.
%Although with little expert trajectories, this framework achieves a lane-keeping function at relatively short distances.
Codevilla et al. \cite{CIL} proposed a famous IL model, named conditional imitation learning (CIL), which contains both lateral and longitudinal control, as shown in Fig. \ref{fig:cil}. Monocular images, velocity measurement of ego-vehicle, and high-level commands (straight, left, right and lane following) are used as input to CIL, and both predicted longitude and latitude control commands as output. Each command acts as a switch to select a specialized sub-module. CIL is a milestone for the CL method in autonomous driving and demonstrates that the  convolutional neural network (CNN) can learn to perform lane and road tracking tasks autonomously.

\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{Pictures/CIL.png}
    \caption{The model proposed in \cite{CIL}. Measurements stand for the velocity of ego-vehicle. The high-level command, including straight, left, right, and lane following. Actions are control signals including steering, accretion, and brake.}
    \label{fig:cil}
\end{figure}


Based on CIL \cite{CIL}, many researchers include additional information such as global route, location information, or point cloud in the input stage \cite{IL6,IL7,T1}. These methods demonstrate strong generalization and robustness in various conditions, because of sufficient perception data input.

Because of its novel structure, IL methods exclude uncertainty estimation among different sub-systems and lead to fewer feedback milliseconds. However, this characteristic  leads to a significant drawback, lack of interpretability, which does not provide sufficient reasons to explain the decisions. Many researchers try to address this pain point by inserting the intermediate representation layer. Chen et al. \cite{IL_Inter1} propose a novel paradigm, named direct perception method, to predict an affordance for urban autonomous driving scenarios. The affordance represents a BEV format that clearly displays features about the surrounding environment and then is fed to a low-level controller to generate steering and acceleration. Sauer et al. \cite{IL_Inter2} further propose an advanced direct perception model, which leverages video and high-level commands  to intermediate representations and computes control signals as output. Compared with \cite{IL_Inter1}, this model can handle complex scenarios in urban traffic scenarios. Urtasun and her team also propose two interpretable end-to-end planners \cite{IL_Inter4, IL_Inter5}, both planners leverage raw LiDAR data and High-Definition Map (HD Map) to predict safe trajectories and intermediate representations, which are utilized to present how policy responds to surrounding scenarios.

%In addition, this model is able to handle traffic markers (speed signs, traffic lights, and so on) by image-level object labels as well as smooth car-following.

% The University of Toronto also make great contributions to the interpretable of end-to-end autonomous driving, Zeng et al. \cite{IL_Inter4} propose a holistic motion planner, named Neural Motion Planner (NMP), to drive autonomously in complex urban scenarios, including traffic sign detection, yielding at intersections, and interactions with multiple traffic participants. This model leverages raw LiDAR data and HD map as input and predicts interpretable intermediate representations and the trajectory of dynamic obstacles. the interpretable representation is able to demonstrate how the policy understands the surrounding environments and reasons why failure cases occur in some extreme conditions. Recently, Sadat et al. \cite{IL_Inter5} construct an multi-task interpretable imitation learning model that integrates perception, prediction, and planning for navigation tasks and produces interpretable intermediate representations and . This model leverages a novel differentiable semantic occupancy representation to align planning costs and estimates of perceptions and predictions. It significantly outperforms NMP by consistency of perception and prediction outputs in motion planning.  



The main feature of the BC method is that only experts can generate training examples, which directly leads to the training set being a subset of the states accessed during the execution of the learned policy \cite{hu2020learning}. Therefore, when the dataset is biased or overfitted, the method is limited to generalize. Moreover, when the agent is guided to an unknown state, it is hard to learn the correct recovery behavior.

\subsubsection{Direct Policy Learning} Direct Policy Learning (DPL), a training method based on BC, evaluates the current policy and then obtains more suitable training data for self-optimization. Compared with BC, the main advantage of DPL leverages expert trajectories to instruct the agent how to recover from current errors \cite{IL1}. In this way, DPL alleviates the limitation of BC due to insufficient data. In this section, we summarize a series of DPL methods.

Ross et al. \cite{IL8} construct a classical online IL method named Dataset Aggregation (DAgger) method. This is an active method based on the Follow-the-Leader algorithm \cite{IL1}, each validation iteration is an online learning example. The method modifies the main classifier or regressor on all state$-$action pairs experienced by the agent. DAgger is a novel solution for sequential prediction problems, however, its learning efficiency might be suppressed by the far distance between policy space and learning space. In reply, He et al. \cite{IL9} propose a DAgger by coaching algorithm which employs a coach to demonstrate easy-to-learn policies for the learner and the demonstrated policies gradually converge to label. To better instruct the agent, the coach establishes a compromised policy which is not much worse than a ground truth control signal and much better than novice predicted action. %As shown in Fig. \ref{fig:DAgger}, $\mathbold{\pi}$ is the predicted command, $\mathbold{\pi^{*}}$ shows the expert trajectory, and $\mathbold{\pi^{'}}$ presents the compromised trajectory. $\mathbold{\pi^{'}}$ is much easier than $\mathbold{\pi^{*}}$ for agent to learn sub-optimal policy in each iteration. The policy is asymptotically optimal.

% DAgger in this section

\begin{figure} [t]
    \centering
	  \subfloat[DAgger]{
       \includegraphics[width=0.48\linewidth]{Pictures/TwoPolicies.png}}
    \label{1a}\hfill
	  \subfloat[DAgger by coaching]{
        \includegraphics[width=0.48\linewidth]{Pictures/ThreePolicies.png}}
    \label{1b}\hfill
	  \caption{The DAgger method \cite{IL8} for Autonomous Driving Navigation Task.}
	  \label{fig:DAgger} 
\end{figure}

% Based on the DAgger algorithm, Zhang et al. \cite{IL10} propose the SafeDAgger algorithm, which intends to improve the query efficiency of DAgger and can further reduce the dependence on label accuracy by reducing expert involvement by calculating the steps that need to be instructed by the autonomous driving model, the algorithm automatically recovers from extreme cases, which can further reduce the need for labels. In the training stage,  Safe DAgger \cite{IL10} is tested in a racing car simulator, named TORCS, SafeDAgger showed even better performance than DAgger and supervised approaches in terms of safe operating milages, number of crashes, and steering angle mean square error. DAgger and its extensions depend on frequent interaction with experts, which will result in considerable human costs, Hoque et al. \cite{IL11} propose a ThriftyDAgger model, which requires human feedback only on novel or risky scenarios. The algorithm is implemented in simulation and physical platform, which has shown promising results on navigation tasks. Yan et al. \cite{IL12} propose a novel DPL training scheme for Navigation tasks in mapless scenarios, the output of control command is fused with expert label to control the mobile platform. The scheme presents a remarkable performance in both simulation and physical scenarios.


Other researchers also point out some drawbacks of DAgger methods \cite{IL8,IL9}: inefficient query, inaccurate data collector, and poor generalization. In reply, Zhang et al. \cite{IL10} propose the SafeDAgger algorithm, which intends to improve the query efficiency of DAgger and can further reduce the dependence on label accuracy. Hoque et al. \cite{IL11} propose a ThriftyDAgger model, which integrates human feedback on corner cases, Yan et al. \cite{IL12} propose a novel DPL training scheme for navigation tasks in mapless scenarios, both of them improve the generalization and robustness of the model.

DAgger-based methods reduce dataset dependency and improve learning efficiency, however, these methods cannot distinguish between good or bad expert trajectories and ignore the learning opportunity from unfitness behaviors. In reply, Li et al. \cite{OIL} propose the observational imitation learning (OIL) method, which predicts the control commands from the monocular image and embeds waypoints as intermediate representations. OIL manifests as an online learning policy based on a reward function, it could learn from multi-experts and abandon the wrong policies.

To fine-tune the agent policy in perception-to-action methods, Ohn-Bar et al. \cite{LSD} propose a method for optimizing situational driving policies which effectively captures reasoning in different scenarios, shown in Fig. \ref{fig:LSD}. The training policy is divided into three parts. First, the model learns sub-optimal policies by the BC method. Second, context embedding is trained to learn scenario features. Third, refined the integrated model by online interaction with the simulation and collect better data by a DAgger-based method. 

\begin{figure} [b]
    \centering
    \includegraphics[width=0.9\linewidth]{Pictures/test14.png}
    \caption{Training policies propose in \cite{LSD}. Export policies learn sub-optimal policy. Context Embedding is trained to learn scenarios. Both Context Embedding and Export Policies are fine-tuning in Task-Driven Optimization by Online method.}
    \label{fig:LSD}
\end{figure}


DPL is an iterative online learning policy that alleviates the requirements for the volume and distribution of dataset, while facilitating the continuous improvement of policies by effectively eliminating incorrect ones.
\subsubsection{Inverse Reinforcement Learning}
% feature base -> Maximum Entropy Inverse Reinforcement Learning -> GAIL -> GCL -> AIRL -> MEDIRL
Inverse reinforcement learning (IRL) is designed to circumvent the drawbacks of the aforementioned methods by inferring the latent reasons between input and output \cite{TVT2}. Similar to the prior methods, IRL needs to collect a set of expert trajectories at the beginning. However, instead of simply learning a state-action mapping, these expert trajectories are first inferred and then the behavioral policy is optimized based on the elaborate reward function.  IRL method can be classified into three distinct categories, max-margin methods, Bayesian methods, and maximum entropy methods. 


%Inverse Reinforcement Learning, Optimal Control, and Apprenticeship Learning are three similar forms of learning from expert trajectories, so we don't distinguish them, all of them are recognized as Inverse Reinforcement Learning (IRL). 


The max-margin method leverages expert trajectories to evaluate a reward function that maximizes the margin between the optimal policy and estimates sub-optimal policies. these methods represent reward functions with a group of features utilizing a linear combination algorithm, where all features are considered independent %It is often easier to enumerate all potentially relevant component features than to manually specify a set of features that is both complete and fully relevant. The difficulty of performing IRL given only such components is that many of them may have important logical relationships that make it impossible to represent the reward function as their linear combination while enumerating all possible relationships is intractable.

Andrew Wu \cite{IRL1} is a pioneer in this field, he introduces the first max-margin IRL method, which puts forward three algorithms for computing the refined reward function. Furthermore, Pieter et al. \cite{IRL2} devise an optimized algorithm based on \cite{IRL1}, which assumes that an expert reward function can be expressed as a manually crafted linear combination of known features, with the objective of uncovering the latent relationships between weights and features.

%The first two algorithms are suited for that both the transition probabilities and the expert policy are known, and the last algorithm address the more realistic case, which requires relaxed requirements that the expert policy can be estimated, and the simulated trajectories from the Markov decision process (MDP) can be inferred in the training stage. 


The limitation of prior methods is that the quality and distribution of the expert trajectories sets an upper bound on the performance of the method. In reply, Umar et al. \cite{IRL3} propose a game-theoretic-based IRL method named multiplicative weights for apprenticeship learning, it has the capability to import prior policy to the agent about the weight of each feature and leverages a linear programming algorithm to modify the reward function so that its policy is stationary.

% Chestnutt et al. \cite{IRL7} propose a maximum margin planning model, which estimates a reward function to make predicted behavior close to the label control signal in the expert trajectories. This model utilizes a simple binary classification/regression method to improve performance and reduce negative affect by sub-optimal influence.

In addition, Phan-Minh et al. \cite{Extend} propose an interpretable planning system, as shown in Fig. \ref{fig:threepart}. The trajectory generation module leverages perception information to compute a set of future trajectories. The safety filter is used to guarantee basic safety with an interpretable method. DeepIRL trajectory scoring the predicted trajectories, which is the core contribution of this system. Furthermore, \cite{IRL4} and \cite{IRL5} propose preference-inference formulation, users can choose actions according to their personal preferences, which indeed improves the performance of the model.


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Pictures/2022arxiv.png}
    \caption{The system proposed in \cite{Extend} is divided into three stages: trajectory generation, safety filtering, and trajectory scoring.}
    \label{fig:threepart}
\end{figure}

% The goal of MWAL is to maximize the performance of the agent even if the trajectories are sub-optimal. To achieve this goal, MWAL has the capability to import prior policy to the agent about the weight of each feature and leverages a linear programming algorithm to modify the reward function, so that its policy is stationary. \cite{IRL4} and \cite{IRL5} propose preference-inference formulation, users can choose actions according to their personal preferences, which indeed the performance of the model.

%Michal et al. \cite{IRL4} leverage semi-supervised to train IRL model, the expert trajectory is assumed to be the label data, and the sub-optimal trajectory is shown as unlabeled examples. Bryce et al. \cite{IRL5} introduce a novel preference-inference formulation and a candidate inference algorithm to IRL model. The model is able to infer many of predict actions based on the partial observing information, and then users choose a set of actions according to their personal preferences. 

% Another limitation of the IRL model is extremely sensitive to the scale features of traffic participants. In response, Chestnutt et al. \cite{IRL7} propose a maximum margin planning model, which estimates a reward function to make predicted behavior close to the label control signal in the expert trajectories. This model utilizes a simple binary classification/regression method to improve performance and reduce negative affect by sub-optimal influence.



%Due to the complexity of the problem-solving dimension in autonomous robots, IRL often struggles. One problem is that high-quality demonstrations are difficult to collect in all driving scenarios. For this problem, Some researchers focus on the generalizability of the IRL model and validate their model in a driving simulator. 
The second part of IRL is Bayesian methods, which often leverage the optimized trajectory or the prior distribution of the reward to maximize the posterior distribution of the reward. 
The first Bayesian IRL is proposed by Ramachandran et al. \cite{BIRL1}, which references the IRL model from a Bayesian perspective and inferences a posterior distribution of the estimated reward function from a prior distribution. Levine et al. \cite{BIRL2} integrate a kernel function into the Bayesian IRL model \cite{BIRL1} to improve the accuracy of estimating reward and promote the performance in unseen driving. 
%When new candidate rewards are selected at random, more weight is given to states that are similar to those visited by the expert. This method improves time to convergence and allows the user to trade-off computation time for solution accuracy.

Furthermore, Brown et al. \cite{BIRL3} construct a sampling-based Bayesian IRL model, which utilizes expert trajectories to calculate practical high-confidence upper bounds on the $\alpha$-worst-case difference in expected return under the unseen scenarios without a reward function.
Palan et al. \cite{BIRL4} propose DemPref model, which utilizes the expert trajectory to learn a coarse reward function, the trajectory is used to ground the (active) query generation process, to improve the quality of the generated queries. DemPref alleviates the efficiency problems faced by standard preference-based learning methods and does not exclusively depend on high-quality expert trajectories.
%The model demonstrates tighter and more accurate confidence error bounds than previous work in the autonomous driving task.
 
% Another problem is IRL method attempts to learn a continuous, high-dimensional function, and it leads to inefficient learning. In response, Palan et al. \cite{BIRL4} propose an extension of Bayesian IRL, named DemPref, which utilizes the expert trajectory to learn a coarse reward function, and then, the trajectory is used to ground the (active) query generation process, to improve the quality of the generated queries. DemPref alleviates the efficiency issues faced by standard preference-based learning methods and does not exclusively depend on high-quality expert trajectories.

The third part of IRL is the maximum entropy method, which is defined by using maximum entropy in the optimization routine to estimate the reward function. Compared with the previous IRL method, Maximum entropy methods are preferable for continuous spaces and have the potential ability to address the sub-optimal impact of expert trajectories. The first Maximum Entropy IRL model is proposed by Ziebart \cite{MEIRL}, which leverages the same method as \cite{IRL1} and could alleviate both noises and imperfect behavior in the expert trajectory. The agent attempts to optimize the reward function under supervision by linearly mapping features to rewards.
%This model forms the fundamental groundwork for the application of Maximum Entropy IRL in autonomous driving.

And then, many researchers \cite{MEIRL3,MEIRL4,MEIRL5} implement the maximum entropy IRL to physical end-to-end autonomous driving. Among them, \cite{MEIRL5} propose Generative Adversarial Imitation Learning (GAIL), which has become a classical algorithm in this field. GAIL leverages a generative adversarial network (GAN) to generate the distribution of expert trajectories with a model-free method in order to alleviate the problem of state drift caused by insufficient datasets. Because of sufficient reconstruction expert trajectories and competitive policies, GAIL achieves performance comparable to that of human drivers in specific scenarios. Based on \cite{MEIRL5}, many works have been proposed, such as InfoGAIL \cite{GIAL1}, Directed-InfoGAIL \cite{GAIL3}, Co-GAIL \cite{GIAL2}, all of them achieve competitive results in their implement fields. 

IRL provides several excellent works for autonomous driving, however, like the aforementioned methods, it also has long tail problems in corner cases. How to effectively improve the robustness and interpretability of IRL is also a future direction.



%Wulfmeier et al. \cite{MEIRL3} establish a general framework for IRL with maximum entropy paradigm. This model leverages the state feature as input and establish a nonlinear reward function to efficiently represent complex scenario features. Then, they construct a non-linear, maximum entropy based end-to-end IRL model \cite{MEIRL3} based on the model proposed in \cite{MEIRL2}. This model calculates cost maps directly form raw perception data, saves time for manual adjustments of hyper-parameters of cost map and features. In the validation stage, the predicted trajectories form the learned cost maps not only imitate the expert trajectory control signal but are also demonstrably robust against systematic errors in putative robot configuration.

%Lee et al. \cite{MEIRL4} construct an IRL model based on the previous model proposed in \cite{MEIRL}, which could autonomously compute driver behavior by an underlying reward function from the expert trajectory. This model 

%Lee et al. \cite{MEIRL4} integrate additional information to improve performance in lane-keeping and lane-changing tasks. The goal-conditioned policy is proposed in this model to restrict agent reach states, and the zero-learning method is used to solve the noise and artifacts problem without any human labeling or pretraining the model.

%Ho et al. \cite{MEIRL5} propose Generative Adversarial Imitation Learning (GAIL), which is a model-free method to IRL. GAIL leverages a generative adversarial network (GAN) to generate the distribution of expert trajectories in order to alleviate the problem of state drift caused by insufficient datasets. Because of sufficient reconstruct expert trajectories and competitive policies, GAIL achieves performance comparable to that of human drivers in terms of any reward function.


% Since the expert's decisions are often optimal or near-optimal for a given task. The cumulative payoff expectation from all strategies is less than or equal to the cumulative payoff expectation from the expert's strategy.



