\section{Solutions}
\label{sec:solutions}
\begin{table}[t]
    \centering
    \begin{tabular}{lcc}
    \toprule 
         & 2021 & 2022  \\
    \midrule
     Number of Teams & 37 & \textbf{63} \\
     Number of Individuals & 358 & \textbf{446} \\ 
     Number of Submissions & 271 & \textbf{504} \\
     Number of Teams that Scored Higher than Baselines & 10 & \textbf{11} \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Participation statistics.} The 2022 competition saw more teams, individuals, and submissions than 2021, demonstrating an increased interest.\vspace{-8pt}}
    \label{tab:basalt_participation}
\end{table}

\smchange{
Compared to last year, this year we saw a larger number of individuals (and teams) competing and nearly double the submissions (see \Cref{tab:basalt_participation}).
%This data is summarized in \Cref{tab:basalt_participation}.
We believe that the number of submissions increased in part due to the introductory track. 
Interestingly, the scores corresponding to submissions that achieved higher than the lowest scores (e.g. approaches that did not just submit the baselines) remained the same as last year.
This similarity indicates that the increased popularity likely stemmed from less experienced teams.
}

\smchange{
In the rest of this section, we describe approaches taken by the competition winners and the teams who were selected for research prizes.\footnote{To see example videos of each of the agents trained using the algorithms described performing each of the tasks, please see: \url{https://www.youtube.com/playlist?list=PL7H6ODSaA0Y-yyJDXLOJQQcThg7_SBoqU}.}
}
%\kr{The higher interest can also be seen in the MineRL Discord server statistics in Table \ref{tab:discord_stats}. Despite there being two competitions (Diamond and BASALT) last year, and only the less popular one being held this year, there was only a modest reduction in the community activity and new members.}

\begin{table}[t]
\centering
\floatconts
  {tab:leaderboard}
  {\begin{tabular}{lSSSSS} \label{table:winners}
  \\ \toprule
  \bfseries Team & {\bfseries \cavetasknospace} & {\bfseries \waterfalltasknospace} & {\bfseries \pentasknospace} & {\bfseries \housetasknospace} & {\bfseries Average}\\
  \midrule
   GoUp & 0.31 & {\bfseries 1.21} & {\bfseries 0.28} & {\bfseries 1.11} & {\bfseries 0.73} \\
   UniTeam & {\bfseries 0.56} & -0.10 & 0.02 & 0.04 & {\bfseries 0.13} \\
   voggite & 0.21 & 0.43 & -0.20 & -0.18 & {\bfseries 0.06} \\
   JustATry & -0.31 & -0.02 & -0.15 & -0.14 & {\bfseries -0.15} \\
   TheRealMiners & 0.07 & -0.03 & -0.28 & -0.38 & {\bfseries -0.16} \\
   yamato.kataoka & -0.33 & -0.20 & -0.27 & -0.18 & {\bfseries -0.25} \\
   corianas & -0.05 & -0.26 & -0.45 & -0.24 & {\bfseries -0.25} \\
   Li\_and\_Ivan & -0.15 & -0.72 & -0.14 & -0.22 & {\bfseries -0.31} \\
   KAIROS & -0.35 & -0.32 & -0.41 & -0.36 & {\bfseries -0.36} \\
   Miner007 & -0.07 & -0.76 & -0.12 & -0.52 & {\bfseries -0.37} \\
   KABasalt & -0.57 & -0.23 & -0.41 & -0.31 & {\bfseries -0.38} \\
   %pm & -0.52 & -0.98 & -0.45 & -0.41 & {\bfseries -0.59} \\
   %Dopamind & -1.03 & -0.84 & -1.09 & -1.25 & {\bfseries -1.05} \\
   \midrule
   Human2 & 2.52 & 2.42 & 2.46 & 2.34 & {\bfseries 2.43} \\
   Human1 & 1.94 & 1.94 & 2.52 & 2.28 & {\bfseries 2.17} \\
   BC-Baseline & -0.43 & -0.23 & -0.19 & -0.42 & {\bfseries -0.32} \\
   Random & -1.80 & -1.29 & -1.14 & -1.16 & {\bfseries -1.35} \\
  \bottomrule
  \end{tabular}}
  {\caption{{\bfseries Leaderboard: normalized TrueSkill scores.} \smchange{The top three teams were GoUp, UniTeam, and voggite. GoUp achieved higher performance on all tasks but \cavetasknospace. We include scores for BC-Baseline (organizer-provided baseline), two expert humans, and a random agent.}\vspace{-8pt}}\label{tab:leaderboard}}
\end{table}

\subsection{Approaches of Competition Winners}
\label{subsec:winner_approaches}
%\smchange{We now turn our attention to the approaches taken by the competition winners.}
\smchange{The scores of the top teams are captured in \Cref{tab:leaderboard}.}
\smchange{GoUp achieved the highest overall score of $2.09$, with UniTeam and voggite taking second- and third-place, respectively.}
\smchange{GoUp achieved the highest scores on all tasks except for \cavetasknospace. 
In this case, the UniTeam scored higher.
We now describe each team's solution in turn (more details in \Cref{sec:solutions}).} 

\paragraph{First Place: GoUp}
\smchange{This solution utilized the power of machine learning and human knowledge by dividing each task into two parts: one that can be solved by transforming human knowledge into code (i.e., scripts) and the other that requires machine learning to solve.\footnote{Open-source code for GoUp: \url{https://github.com/gomiss/neurips-2022-minerl-basalt-competition}.} % for completing which machine learning approaches should be adopted. 
The team found that all of the four tasks consist of the same flow.
The agent walks around, searches for a target (e.g., a cave), then solves the task. 
They identified the targets in each task by training several classifiers and object detection models. 
To source data for training their models, they manually labeled images collected from the expert videos provided by the competition with the corresponding task. 
\Cref{Fig:Solution} shows the framework of the solution. 
Taking the AnimalPen task as an example, the solution for this task contains a fine-tuned VPT model for moving the agent, a fine-tuned YOLOv5 \citep{yolov5} detector for detecting the types and locations of animals, a fine-tuned MobileNet~\citep{howard2019searching} detector for identifying the location of fence placement, and a finite state machine that controls the executing flow of all the components. 
}

\paragraph{Second Place: UniTeam} %, \textit{Search-Based Behavioral Cloning}}
\begin{figure*}[t]
\centering
  \includegraphics[scale=0.4]{media/go-up-solution.pdf}
  \caption{\smchange{\textbf{Decomposition of subtasks by GoUp.} To solve tasks that cannot be precisely described by a person, they leverage a variety of machine learning techniques. To solve the remaining tasks, they use human knowledge through scripting. \vspace{-8pt}} \label{Fig:Solution} }
\end{figure*}

%\kb{Is this search-based BC or just K-NN on trajectories using BC features?}
\smchange{UniTeam proposed a search-based behavioral cloning approach, which aims to reproduce an expert's behavior by copying relevant actions from relevant situations in the demonstration dataset.\footnote{Open-source code for UniTeam: \url{https://github.com/fmalato/basalt_2022_submission}.} 
\Cref{fig:uniteam_approach} shows their approach.
They defined a situation as a subset of consecutive frames and actions within a recorded trajectory, which they encoded using the VPT network.
To find the most similar situations to the current one, they used a pre-trained VPT model to produce latent representations.
They searched for the nearest embedding point in the VPT latent space to find the reference situation.
They assumed that retrieved situations represent an optimal solution to a specific past situation. 
As a result, they copy the corresponding actions.
%Such solutions can be re-used to address similar situations happening in the present~\citep{beohar2022solving}.
}
\smchange{ 
After each timestep, they updated both the current and reference situations. 
Because the reference and current situations diverge over time, they measured the L1 similarity between the situations at each timestep. 
Once the distance between trajectories was greater than some fixed threshold (or after 128 timesteps), they performed a new search.
See~\cite{malato2022behavioral} for more details.}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{media/uniteam_figure.png}
    \caption{\smchange{\textbf{Generation of past-situation points in the VPT latent space by UniTeam.} (A) They encode situations from the BC dataset through a pre-trained VPT model. (B) They compute the L1 distance between their embedded current situation and the embedded situations from the expert’s dataset. Then, they copy actions from the best-matching reference situation. (C) Examples of visual (left) and numerical (right) similarity between current and reference situations. In the right figures, the blue vertical lines denote new time-based searches and red lines correspond to new threshold-based searches.
}\vspace{-8pt}}
    \label{fig:uniteam_approach}
\end{figure}

\paragraph{Third Place: voggite} 
\smchange{This team focused on improving the behavioral cloning baseline.\footnote{Open-source code for voggite: \url{https://github.com/shuishida/minerl_2022}.}
To enable quick iterations of solutions, they precomputed the VPT state embeddings for all of the expert demonstrations, then trained a lightweight policy model with PyTorch Lightning. 
To focus on rarer but significant actions, they introduced action reweighting based on how frequently they were encountered. 
They observed that certain actions serve as signals that trigger higher-level changes in state. 
For example, the use of a bucket to pour water to complete the waterfall task signals that the agent should begin to climb down the mountain to take a scenic picture. 
They manually encoded these trigger actions along with the change in action distribution (e.g., decreasing the probability of moving forward once the agent starts building something); however, they plan to incorporate this idea into a hierarchical reinforcement learning framework, such as option-critic~\citep{bacon2017option}. 
}

\subsection{Approaches of Research Prize Winners}
\smchange{In addition to evaluating teams on the performance of their algorithms using human evaluations, we awarded additional prizes for research contributions.
To select the winners, each advisor read through a description of the approach and watched a video demonstration of the agent behavior. 
They had the option to view the agent's score. 
Each advisor then awarded a prize to the approach for their research contribution.
In general, the advisors preferred elegant, intuitive approaches that were ambitious, even if the final scores were relatively low. 
Advisors independently chose the following teams for the research prize: UniTeam (2 votes), KABasalt (2 votes), and KAIROS (1 vote). 
We now describe the remaining approaches in turn.
} 

\paragraph{KABasalt} 
\smchange{This team aimed for a reward modeling approach based on preference feedback after a pre-training phase through imitation and preference learning with demonstrations.\footnote{Open-source code for KABasalt: \url{https://github.com/BASALT-2022-Karlsruhe/ka-basalt-2022}.}
The main contribution was integrating the VPT models into the \textit{Imitation} library~\citep{gleave2022imitation} and {Stable-Baselines3}~\citep{stable-baselines3}, which required the ability to create compatible policy and reward model objects from the VPT backbone.
After tackling the myriad challenges of making reinforcement learning work with foundation models, such as incompatible interfaces, they are now further developing their solution and adding support for more features, like off-policy reinforcement learning algorithms.
}
%Although we have produced a working training pipeline, we couldn't carry out and optimize the full training including human preferences because of time constraints.
%We underestimated the difficulty of making RL work with foundation models like the provided VPT models. For example, integrating them into existing libraries was not straightforward because of initially incompatible interfaces and missing features like support for recurrent policies.
%We are in the process of polishing our solution and to contribute our code, packaged into new features of the libraries we utilized. In the future we plan to add support for off-policy RL algorithms like PEBBLE.
%Overall, we believe that our work represents an important step towards large scale reward modeling.
%\smnote{todo for kabasalt: add missing refs, link to open source, and update language/description}

\paragraph{KAIROS}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{media/PIQL_no_state_classifier.png}
    \caption{\smchange{\textbf{Preference-Based IQ-Learning (PIQL) algorithm proposed by KAIROS.} Their algorithm incorporates imitation learning with reinforcement learning using a reward model trained using pairwise human preferences over videos agents completing the tasks.}\vspace{-8pt}}
    \label{fig:piql_diagram}
\end{figure}

\smchange{This team proposed Preference-Based IQ-Learning (PIQL), a novel algorithm that extends a state-of-the-art actor-critic imitation learning algorithm (IQ-Learn~\citep{garg2021iq}) to additionally leverage the VPT model and online pairwise preferences over trajectories~\citep{christiano2017deep}.\footnote{Open-source code for KAIROS: \url{https://github.com/nwayt001/preference-IQL.git}.}
\Cref{fig:piql_diagram} shows their approach. 
PIQL uses pairwise preferences over videos of agents completing the task to learn a reward function via the Bradley-Terry model~\citep{christiano2017deep} through a reward head attached to the VPT network.
They define a novel critic loss that includes an IQ-learn term that prioritizes imitating the demonstrations and a reinforcement learning term~\citep{haarnoja2018soft} that aims to maximize the reward learned from human preferences. 
They include regularization terms that penalize the behavioral cloning loss on the demonstrations and the KL divergence between policies in successive learning iterations.
Thus, while performing imitation learning via IQ-Learn, PIQL obtains human pairwise preferences and learns a reward model that assists Q-function learning alongside the demonstrations.
}

%We propose Preference-Based IQ-Learning (PIQL, pronounced “pickle”), a novel algorithmic approach that extends IQ-Learn~\citep{garg2021iq}---a state-of-the-art actor-critic imitation learning algorithm---to not only learn from expert demonstrations, but to also leverage both the VPT model and online human pairwise preferences over trajectories of agent-environment interactions~\citep{christiano2017deep}, as seen in Figure \ref{fig:piql_diagram}. Each preference is collected by a human comparing two side-by-side video segments of the Minecraft agent interacting with the BASALT environment. PIQL utilizes these pairwise preferences to learn a reward function via the Bradley-Terry model~\citep{christiano2017deep} through a reward head attached to the VPT network. We define a novel Q loss that sums a) an IQ-learn term that prioritizes imitating the demonstrations and b) a reinforcement learning (RL) term as in the Soft Actor Critic (SAC) RL algorithm~\citep{haarnoja2018soft}, which aims to maximize the reward learned from human preferences. Simultaneously, we train the policy as in SAC to maximize the learned Q-values, with regularization terms penalizing the behavioral cloning loss on the demonstrations and the KL divergence between policies in successive learning iterations. 
%Thus, while performing imitation learning via IQ-Learn, PIQL obtains human pairwise preferences and learns a reward model that assists Q-function learning alongside the demonstrations. 
%Algorithm~\ref{alg:PIQL} describes the complete PIQL algorithm.

\begin{comment}
\begin{algorithm}[h]
    \label{alg:PIQL}
    Load pre-trained VPT weights \\
    \For{each iteration}{
    Optimize policy via SAC given current Q weights \\
    Collect pairwise preferences \\
    Train reward to maximize pairwise preference likelihood \\
    Optimize Q-function via: a) IQ-learn loss (from demonstrations) and b) SAC RL loss to encourage high human preference reward
    }
    \caption{PIQL algorithm.}
\end{algorithm}
\end{comment}
