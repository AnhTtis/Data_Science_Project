\section{Competition Overview}
\label{sec:overview}

\smchange{We now present important details about the competition (more in \Cref{apd:comp_details}).} % , including the tasks, provided resources, rules, validation technique, evaluation, and prizes.}
%\kr{Figure~\ref{fig:overview} shows the overall competition structure.}
%\smchange{For more information, please see \Cref{apd:comp_details}.}

\paragraph{Tasks}
\smchange{
We provided a set of tasks in Minecraft that consist of a simple English-language task description and a Gym~\citep{brockman2016openai} environment. 
They were: \cavetasknospace, \waterfalltasknospace, \pentaskfull (\pentasknospace), and \housetaskfull (\housetasknospace).}
\smchange{
%We describe the tasks in more detail in \Cref{apd:comp_details}.
Crucially, the Gym environments lacked any associated reward functions.}
For each task, we provided a dataset of human demonstrations consisting of a sequence of state-action pairs.
\smchange{To help participants familiarize themselves with the code pipeline, we included an introductory track with a task with an explicit reward function, \texttt{ObtainDiamondShovel}.}

\paragraph{Resources}
\smchange{Through our partnership with AIcrowd, we provided competitors with a unified interface to register for the competition, submit trained agents, ask questions, and monitor their progress on a public leaderboard.}\footnote{\url{https://www.aicrowd.com/challenges/neurips-2022-minerl-basalt-competition}}
We partnered with OpenAI to release over 600h of human demonstrations in the four tasks and baselines built on the VPT model~\citep{baker2022video}. 
The baseline solution fine-tunes VPT model with behavioral cloning using the collected dataset.\footnote{The pre-trained models and human demonstration data are available from the following GitHub pages: \url{https://github.com/openai/Video-Pre-Training} and \url{https://github.com/minerllabs/basalt-2022-behavioural-cloning-baseline}.}
\ak{To provide mentorship and foster an active community, we continued maintaining the MineRL Discord server.} %, as we previously found it to increase engagement and provide support to participants.}
%\smnote{mention Aicrowd website, baselines, mentorship, computing and eval resources, and tutorial/documentation}

\paragraph{Rules and Validation}
\smchange{We required methods to use only the specified Gym API. 
For reproducibility, we mandated that participants submit their \textit{training} code.
We limited the size of data and public models (30MB) that could be included in their submissions. 
The provided resources did not count toward this limit. 
The AIcrowd page contains the full rules.\footnote{\url{https://www.aicrowd.com/challenges/neurips-2022-minerl-basalt-competition/challenge_rules}}}
%\paragraph{Validation}
\ak{
To ensure reproducibility, we retrained the finalists' submissions with their provided training code for up to 4 days (on all four tasks) using at most 10 hours of human feedback. 
We fixed the compute to 12 CPU cores, 56GB of RAM, and an NVIDIA Tesla K80 GPU. The training code could query humans for input with traditional desktop UIs. 
We provided human contractors who connected to the training instances to provide input. We also manually inspected the code to ensure compliance with the 30MB upload limit.
}

\paragraph{Evaluation}
\smchange{
%\ak{We describe evaluation in more detail in \Cref{apd:comp_details} but explain it at a high level here.}
\ak{Upon submission,} we deployed the agents on fixed world seeds of each task to generate multiple example videos.
\ak{After the submission deadline,} we asked human judges recruited through Amazon Mechanical Turk (MTurk) to choose which agent better completed the task through pairwise comparisons of the agents.
Given a dataset of these comparisons, we computed each agent's scores using the TrueSkill system \citep{herbrich2006trueskill}.
We determined the winners by normalizing and aggregating these scores across tasks.}
%We describe this metric in more detail in \Cref{apd:comp_details} but explain it at a high level here.
%We first normalize the TrueSkill scores across submissions to the tasks to ensure that no one task dominates the final score.
%We then average the normalized score to obtain an ovethe rall score.
%}
%\kb{Could you shed a little light on how the MTurk participants were preped? What information was given to them? How were the told to judge the results?}

\paragraph{Prizes}
\ak{We awarded the top three solutions as ranked by the human evaluation 7000, 4000, and 3000 USD, respectively. 
\smchange{To encourage the exploration of creative solutions, we asked each advisor to select a single team to award a research prize of 1000 USD.} 
To drive community engagement, we gave a total of 1000 USD to participants who helped others or otherwise contributed to the competition.}


\paragraph{Related Competitions}
\smchange{
Minecraft is a popular platform for various research benchmarks~\citep{grbic2021evocraft,gray2019craftassist,johnson2016malmo,hafner2021benchmarking,fan2022minedojo} and competitions. 
%These benchmarks include searching for in-game artifacts in an open-ended manner~\citep{grbic2021evocraft}, developing dialogue-enabled interactive agents~\citep{gray2019craftassist}, and building generally-capable embodied agents~\citep{johnson2016malmo,hafner2021benchmarking,fan2022minedojo}. 
Their diversity demonstrates Minecraft's flexibility to both instantiate and evaluate a variety of interesting problems. 
Research competitions have focused on multi-agent learning of cooperative and competitive tasks~\citep{perez2019multi}, generating functional and believable settlements~\citep{salge2018generative}, and learning to build structures based on natural language descriptions~\citep{kiseleva2021neurips}.
However, none of these competitions are particularly relevant to \fthf{} for hard-to-specify sequential decision-making tasks. 
The most related competitions are the previous MineRL Diamond competitions \smchange{that focused on} learning from a reward function and demonstrations to solve a crisply-defined task~\citep{guss2019neurips}. 
In contrast, we emphasize the use of fine-tuning techniques and utilize tasks that do not have an easy-to-define reward function.
}