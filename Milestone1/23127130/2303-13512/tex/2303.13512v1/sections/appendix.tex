\appendix

\section{Task Details}\label{apd:comp_details}

% Anssi: removed the table. While I like it in a way, I want to avoid copy-pasting from the proposal too much.
% \begin{table}[t]
%     \centering
%     \begin{tabular}{ccc}
%     \toprule
%       \textbf{Task}   & \textbf{Description} & \textbf{Real-World Application}  \\
%       \midrule
%         FindCave     & Find a naturally-generated cave. & Search for a hard-to- \\
%         & & describe landmark. \\
%         MakeWaterfall & Create and photograph a waterfall. & Create an aesthetically \\
%         & & pleasing design.  \\ 
%         CreateVillageAnimalPen & Build an animal pen, then corral & Interact with agents \\ 
%         & a pair of animals into it. & or other entities. \\ 
%         BuildVillageHouse & Build a house in the same style & Work on industrial \\
%         & as surrounding houses. & product design. \\
%         \bottomrule \\
%     \end{tabular}
%     \caption{\textbf{Competition tasks and their potential real-world applications.}}
%     \label{tab:comp_tasks}
% \end{table}
In this section, we provide more information about the competition tasks and the associated human demonstration data.
For more details, we refer an interested reader to the competition proposal~\citep{kanervisto2022basalt}

\subsection{Data}

Similarly to last year, we provided the participants with a dataset of human demonstrations of each task. 
To produce these demonstrations, we used the same infrastructure as MineRL Diamond~\citep{guss2019neurips}. 
Each demonstration consists of a sequence of state-action pairs, called a trajectory, contiguously sampled at every Minecraft game tick.
There are 20 game ticks per second.
Each state is includes an RGB frame from the player's perspective, as well as a set of features from the game state. 
Each action consists of all keyboard and ``mouse'' interactions (change in view, pitch and yaw), in addition to a representation of the player GUI interactions. 

\subsection{Competition Tasks}
\ak{
Here we detail the four tasks participants had to solve with their submissions.
The following text is verbatim from the competition webpage, i.e., the same exact info participants saw.
}
\mybox{
\paragraph{Find Caves task}
\begin{itemize}
    \item Description: Look around for a cave. When you are inside one, press ESCAPE to end the minigame.
    \item Clarification: You are not allowed to dig down from the surface to find a cave.
    \item Starting conditions: Spawn in ``plains" biome.
    \item Timelimit: 3 minutes (3,600 steps)
\end{itemize}
}

\mybox{
\paragraph{Waterfall task}
\begin{itemize}
    \item Description: After spawning in a mountainous area with a water bucket and various
             tools, build a beautiful waterfall and then reposition yourself to
             ``take a scenic picture" of the same waterfall by pressing the ESCAPE
             key. Pressing the ESCAPE key also ends the episode.
    \item Starting conditions: Spawn in \texttt{extreme\_hills} biome. Start with a waterbucket,
                     cobblestone, a stone pickaxe and a stone shovel.
    \item Timelimit: 5 minutes (6,000 steps)
\end{itemize}
}

\mybox{
\paragraph{Village Animal Pen Task}
\begin{itemize}
    \item Description: After spawning in a village, build an animal pen next to one of
                 the houses in a village. Use your fence posts to build one animal
                 pen that contains at least two of the same animal. (You are only
                 allowed to pen chickens, cows, pigs, sheep or rabbits.) There
                 should be at least one gate that allows players to enter and exit
                 easily. The animal pen should not contain more than one type of animal. (You may kill any extra types of animals that accidentally got into the pen.) Don’t harm the village. Press the ESCAPE key to end the minigame.
    \item Clarifications: You may need to terraform the area around a house to build a pen. When we say not to harm the village, examples include taking animals from existing pens, damaging existing houses or farms, and attacking villagers. Animal pens must have a single type of animal: pigs, cows, sheep, chicken or rabbit.
    \item Technical clarification: The MineRL environment may spawn player to a snow biome, which does not contain animals. Organizers will ensure that the seeds used for the evaluation will spawn the player in villages with suitable animals available near the village.
    \item Starting conditions: Spawn near/in a village. Start with fences, fence gates, carrots, wheat seeds and wheat. This food can be used to attract animals.
    \item Timelimit: 5 minutes (6,000 steps)
\end{itemize}
}

\mybox{\
\paragraph{Village House Construction task}
\begin{itemize}
    \item Description: Taking advantage of the items in your inventory, build a new house in the style of the village (random biome), in an appropriate location (e.g. next to the path through the village), without harming the village in the process. Then give a brief tour of the house (i.e. spin around slowly such that all of the walls and the roof are visible). Press the ESCAPE key to end the minigame.
    \item Clarifications:  It’s okay to break items that you misplaced (e.g. use the stone pickaxe to break cobblestone blocks). You are allowed to craft new blocks. You don’t need to copy another house in the village exactly (in fact, we’re more interested in having slight deviations, while keeping the same “style”). You may need to terraform the area to make space for a new house. When we say not to harm the village, examples include taking animals from existing pens, damaging existing houses or farms, and attacking villagers. Please spend less than ten minutes constructing your house.
    \item Starting conditions: Spawn in/near a village (of any type!). Start with varying construction materials designed to cover different biomes.
    \item Timelimit: 12 minutes (14,400 steps)
\end{itemize}
}

\subsection{Intro Track Task (Obtain diamond shovel)}
\ak{In the \easy track, participants were challenged to obtain a diamond shovel. This is akin to the ``obtain diamond" task of the previous MineRL competitions, but the agent has to use the gathered diamond to craft a diamond shovel. While difficult, this task is easier with the OpenAI VPT models, one of which is able to obtain diamonds in 20\% of the trajectories.}

\ak{Upon submission, the agent was run for 20 episodes, starting from a fresh Minecraft world. Agent receives a reward for reaching a new stage in the crafting tree, much like in MineRL \citep{guss2019neurips}. The addition is that the agent receives a 2048 reward for crafting a diamond shovel. The final agent score is the maximum score over the 20 episodes. We used max instead of mean in this track, as its purpose was not to draw people to compete in it, but to introduce new users to the environment code and submission pipeline.}


\section{Participant Outcomes}
\label{sec:solutions}
Here we provide more details on participation and the solutions taken by the teams.

\subsection{Participation Details}
\begin{table}[t]
    \centering
    \begin{tabular}{ccccc}
    \toprule
         & 2019 & 2020 & 2021 & 2022 \\
         \midrule 
       Number of Messages & 2,473 & 2,464 & 7,047 & 5,800 \\ 
       Number of New Users  & 445 & 160 & 955 & 731 \\
       \bottomrule
    \end{tabular}
    \caption{\textbf{Discord statistics for the MineRL server from its inception in 2019 to 2022.} 
    These numbers indicate a substantial and sustained interest in using Minecraft for machine learning applications. }
    \label{tab:discord_stats}
\end{table}
We found that the increased interest mentioned earlier was also reflected in the MineRL Discord server.
We detail some interesting statistics in \Cref{tab:discord_stats}.
These numbers indicate a sustained interest in using Minecraft for machine learning research.

\subsection{Solution Details}
We now provide more details about some of the solutions.
\paragraph{GoMiss}
\begin{table}[t]
    \centering
    \begin{tabular}{ccc}
    \toprule 
     Recognition Target & Data Used & Data Type \\
    \midrule 
     Cave and hole & 10k images & Labeled with `Cave' and `Hole'  \\
     (object detection) & & \\ 
       Animal  & 4k images & Labeled with target animals \\ 
      (object detection) & & \\
       Flat area  & 10k images & 5k positive, 5k negative \\ 
      (classification) && \\
       Mountain top & 1750 images & 250 positive, 1500 negative \\
       (classification) && \\
      \bottomrule
    \end{tabular}
    \caption{\textbf{Data used by GoMiss to train the neural networks used to identify the targets for each task.} Interestingly, the most data was used to identify caves/holes and flat areas.}
    \label{tab:gomiss_data}
\end{table}
As mentioned in \Cref{subsec:winner_approaches}, GoUp utilized a number of classifiers and object detectors to recognize the targets for each task.
\Cref{tab:gomiss_data} contains details about the data used to train these neural networks.

GoUp finetuned the VPT model using imitation learning with the organizer-provided \cavetasknospace{} videos.
Notably, the fine-tuned Mobilenet was only used to identify the flat area. 
After finding a flat area, they used a script to verify whether a flat area was actually found.
At a high level, this script has the agents take actions, then checks the difference in the image observations.
The script also includes actions to help the agent make the ground more flat by digging or placing blocks. 
After all of these attempts, if the agent is not on a flat area, it will move to a new area to try again.

To build a house, GoUp implemented a series of hard-coded macro actions to construct each module.
Using the game image as input, they determine the current stage and execute the specific macro actions accordingly. 
These macro actions include: constructing pillars, walls, and doors. 
They did not use privileged state information, only the game image as input for the Hough transforms to accurately adjust the player's orientation.

\paragraph{KAIROS}
The videos used by their approach are rollouts generated by the agent during training.
The agent executes its current policy in the environment throughout training to generate rollout trajectories.
These trajectories are used in both preference-based RL and imitation learning via IQ-learn (which uses both expert demonstrations and on-policy data).
Overall, their approach could be characterized as combining IQ-learning from demonstrations, deep RL from human preferences, and a behavioral cloning term in the policy loss on a VPT torso. 
In PIQL, a Q-function is trained using both IQ-learn and deep RL from human preferences.

\section{Evaluation Process Details}
\label{app:evaluation_results}
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.5]{media/rerun_scores.pdf}
    \caption{\textbf{TrueSkill rankings of the participants, including baselines.} The error bar shows plus/minus one standard deviation of the TrueSkill rating. ``Random" is an agent that chose actions randomly. ``BC\_baseline" is the behavioural cloning baseline supplied to the participants as a baseline solution.}
    \label{fig:evaluation_scores}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.5]{media/worker_distribution.pdf}
    \caption{\textbf{Distribution of answers per human evaluator.} Here, the x axis represents a worker ID. Around 10 workers contributed over 100 responses each. The remaining around 50 workers contributed less than 100 responses each.}
    \label{fig:worker_distribution}
\end{figure}

%\smnote{also include table with normalized scores and reference in the main body?}
\subsection{Final scores and MTurk worker statistics}
    Figure \ref{fig:evaluation_scores} shows the final TrueSkill rankings of each evaluated submission in each task. We also recorded two human players completing the same tasks in the same 10 evaluation seeds and a random agent to act as an upper and a lower bound of performance.

    In total, we had 3049 valid answers from 65 workers after removing 417 low-quality MTurk answers. We required the MTurk workers to have above 99\% HIT acceptance rate and more than 10,000 accepted HITs, as well as complete custom questionnaire testing for basic Minecraft knowledge. We then studied the quality of the mandatory justifications of answers to detect low quality answers (e.g., duplicates and irrelevant text) and remove them from the dataset.
     
    Figure \ref{fig:worker_distribution} shows the distribution of answers per worker, which shows that even the most active worker only submitted 10\% of the answers, indicating a healthy diversity in the worker pool.

\begin{table}[t]
\centering
\begin{tabular}{cccc}
\toprule
Task & Total & Draws & Draw \%\\
\midrule
FindCave & 722 & 201 & 27.84\% \\
MakeWaterfall & 682 & 210 & 30.79\% \\
CreateVillageAnimalPen & 914 & 404 & 44.20\% \\
BuildVillageHouse & 731 & 320 & 43.78\% \\
\bottomrule
\end{tabular}
\caption{\textbf{Comparison of the total number of \textit{draw} answers to total number of answers per task, across all submissions.} 
Draw means that the human evaluator marked both solutions as equally valid. 
Proportionally, the FindCave task had the least amount of \textit{draw} answers, while the CreateVillageAnimalPen task had the most.}
\label{tab:task_distribution}
\end{table}
    Table \ref{tab:task_distribution} shows the distribution of answers per task, as well as the amount of ``draw" votes. The increased numbers in \pentask and \housetask indicate the solutions were not as distinguishable from each other as in \cavetask and \waterfalltask. Given this increased amount of uninformative draw answers, we increased the number of answers for \pentask and \housetask to reach stabler results.

\subsection{TrueSkill normalization}
    We computed the final score as follows:
    \begin{enumerate}
        \item A TrueSkill rating consists of a mean and standard deviation. Take the mean parts of the TrueSkill ratings and normalize them per task to standard norm $\hat x = (x - \mu) / \text{max}(\sigma, 1)$, where $\mu$ and $\sigma$ are sample mean and standard deviation over the mean parts of TrueSkill ratings per task. (We clip the standard deviation to be at least 1 to avoid amplifying noise in rankings for tasks where all submissions behave nearly identically, where we would expect means to be similar and so the standard deviation would be tiny.)
        \item We then sum the normalized scores over tasks to form the final score.
    \end{enumerate}