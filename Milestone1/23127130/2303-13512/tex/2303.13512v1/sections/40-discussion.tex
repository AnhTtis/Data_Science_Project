\section{Discussion}
\label{sec:discussion}
\smchange{The results and outcomes of the competition indicate that there are parts that worked well and many opportunities for improvement.}
%\smnote{lessons learned, future challenges, and improvement.}
%\smnote{my general comment here is that it would be good to have concrete takeaways. for example, if i want to make a competition next year, what should i concretely consider when designing tasks? what should i do to evaluate submissions?}

\paragraph{Task Design}
\kr{
While the four tasks stayed the same this year, the intended way to solve them changed. Fine-tuning the VPT model from human feedback meant that participants started with an agent already capable of traversing and exploring the Minecraft world. Compared to the last year, the solutions of the 2022 edition were indeed all able to navigate around the world, and they were mainly separable by the ability to place blocks down and coherency in their actions.
}
\smchange{We were again pleased to see that teams used significantly different approaches in tackling the tasks, which serves the research goals of the competition.}

\paragraph{Evaluation Methodology and Results}
This year, we used Amazon Mechanical MTurk to crowdsource the evaluation across multiple workers (see \Cref{app:evaluation_results} for details).
By using a scalable platform, we increased the number of human evaluators over the 2021 edition, which relied on a small pool of hired contractors.
To ensure high-quality responses, we set high qualification limits. 
We also asked the human judges to justify their choice of higher-performing agent, which we manually vetted to filter out low-quality answers. 
We found that asking for these responses helped substantially with filtering out low-quality answers.
We recommend utilizing this setup in future competitions that include human evaluations.

Compared to last year, we saw an increase in the performance of the submitted algorithms. 
Meanwhile, we also saw an increase in informative answers, where evaluators chose one of the two pairwise videos as better instead of answering ``draw". 
In 2021, we had a ``draw" rate of 80\%~\citep{shah2022retrospective}, while this year we had a 27-44\% ``draw" rate, depending on the task (see \Cref{app:evaluation_results}). 
\smchange{However, we note that this difference may be attributed to the change in crowd-sourcing platform from contract workers to MTurk. 
Regardless, this finding affirms the utility of this setup for assessing agents.
}



%\paragraph{Organizational Details}
%\kr{(1. Evaluation late again (AK: could you give some color here pls?). 2. (maybe) Young Minecrafters excited about AI flooding discord - more signposting, beginner content/videos would help save organizer time.)}
%\smnote{we could consider cutting this part. depending on the benchmark solution, could cite this discussion as motivation for automated eval and reward modeling.}

%\subsection{Participation}
%\kr{(possibly move to Winning Solutions part?) The competition this year had significantly more teams competing, a higher number of individuals and nearly double the submissions. Non-lowest score submissions stayed roughly the same however. This could indicate that most of the increased interest came from less experienced teams, that had a lower conversion rate to full participation. The number of submissions was increased by the addition of the introductory track and possibly due to the lower reliability of the submission system which led to more re-submissions (AK: was it less reliable?). Full participation details can be seen in Table \ref{tab:basalt_participation}.}

%\kr{The higher interest can also be seen in the MineRL Discord server statistics in Table \ref{tab:discord_stats}. Despite there being two competitions (Diamond and BASALT) last year, and only the less popular one being held this year, there was only a modest reduction in the community activity and new members.}

\paragraph{Intent of the Competition Compared with Evaluation Metrics}
\kr{When organizing an AI competition, one of the biggest questions is how to align the intent of the competition with the evaluation metrics. Especially in reinforcement or imitation learning competitions, there is usually a way to place high on the leaderboard without meaningfully following the intent. One way to solve this is through stringent rules on what methods are allowed, like in past MineRL Diamond competitions \citep{guss2019neurips}. The downside is the extra work required to check rule compliance and to continuously clarify the fine line between allowed and disallowed methods. The way we attempted to solve this tension was by allowing any methods but awarding extra prizes for interesting research contributions. The balance between the more objective evaluation metrics and the more subjective interestingness is one knob that could be tuned to achieve the goals of such competitions.} 

{An illustrative example of this tension is how team KAIROS decided to follow the intent of BASALT 2022.} %However, using these methods they couldn't place high on the leaderboard.)}
\smchange{Although they did not score highly on the leaderboard, they received a research reward by closely following the intent of the competition.}
\smchange{In the future, to encourage creativity of solutions that adhere more closely to the intent of the competition, we recommend providing a diverse set of baselines for participants to build off of. 
Then, participants have examples of what the competition organizers are looking for and preliminary working code to further refine. }
%\smnote{karolis, that's a great point. i'd like to think of a tactful way to discuss this without implying that the top solution is "less interesting" than others. } 
%\smnote{i think a concrete suggestion would be good here. do we think having a separate research prize is sufficient? what if instead of a single pmlr submission comp tracks were treated more like actual research conf tracks? what is the point of competitions in ml? }
%\kb{This is a good point. There is a tradeoff between solutions and creativity. The ideal solution should be creative and  potential shed light on new ideas but also impactful in the sense that it solves the problem. Maybe in the future the best way to get around this is to train simple baselines like: BC, KNN, LWL, etc  and provide these as methods on the leaderboard.}