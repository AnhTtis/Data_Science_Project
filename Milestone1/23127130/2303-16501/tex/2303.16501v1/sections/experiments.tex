% !TEX root = ../main.tex
\section{Experiments}
\label{sec:experiments}

% \xxman{where are the results on the pure ASR dataset to prove that this model does not suffer from catastrophic forgetting? It will be more convincing if you try finetuning everything and show that this is worse than only training the adaption layers on the pure ASR datasets.}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/adapter_type_bottleneck.pdf}
    \vspace{-0.6cm}
    \caption{\textbf{Effects of different architectures (feed-forward (FF) vs feed-forward + self-attention (FF+SA)) and the bottleneck dimensionality $B$ of adaptor layers on performance.} Results are for audiovisual models trained with our curriculum learning, and are shown on 3 datasets in the zero-shot setting (lower WER\% is better). We show that a bottleneck dimension of 64 with FF layers achieves the best or almost the best performance (a,b,c) with the least number of additional parameters (d). Best viewed in color.}
    \label{fig:type_bottleneck}
    \vspace{-0.1cm}
\end{figure*}

% \begin{table}[t]
%     \centering
%     \caption{Caption}
%     \label{tab:bottleneck}
%     \begin{tabular}{c|c|c|ccc}
%         \hline
%         Type & Dim & \#Params & How2 & VisSpeech & Ego4D \\ \hline\hline
%         \multirow{4}{*}{FF} & 32 & 1.65M & 14.90 & 17.89 & 65.52 \\
%         & 64 & 3.22M & \bf14.66 & \bf17.18 & \bf65.45 \\
%         & 128 & 6.37M & 14.73 & 17.29 & 66.17 \\
%         & 256 & 12.66M & 14.86 & 17.38 & 66.07 \\ \hdashline
%         \multirow{4}{*}{+SA} & 32 & 4.87M & 14.78 & 17.70 & 65.44 \\
%         & 64 & 9.59M & 14.73 & \bf17.14 & \bf65.05 \\
%         & 128 & 19.82M & 14.81 & 17.28 & 65.31 \\
%         & 256 & 37.92M & \bf14.71 & 17.29 & 65.99 \\ \hline
%     \end{tabular}
% \end{table}


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/curriculum_vp.pdf}
    \vspace{-0.6cm}
    \caption{\textbf{Effects of curriculum learning and the number of visual tokens $M$ on performance.} Red and blue lines are for audiovisual models and are shown on 3 datasets in the zero-shot setting (lower WER\% is better). Using the curriculum helps on all 3 datasets (for How2 (a) and Ego4D (c) it is crucial for outperforming audio-only performance). Performance improves up until 4 visual tokens, at which point it saturates. Best viewed in color.}
    \label{fig:curriculum_vp}
    \vspace{-1em}
\end{figure*}

% \begin{table}[t]
%     \centering
%     \caption{Caption}
%     \label{tab:bottleneck}
%     \begin{tabular}{c|c|cc|c}
%         \hline
%         Dataset & \#VP & w/o CL & w/ CL & Rel. $\Delta$\\ \hline\hline
%         \multirow{5}{*}{How2} & 0 & 14.66 & -- & -- \\
%         & 1 & 14.95 & 14.09 & 5.75\% \\
%         & 2 & 15.07 & 13.83 & 8.24\% \\
%         & 4 & 14.97 & \bf 13.63 & 8.90\% \\
%         & 8 & 15.38 & 14.08 & 8.42\% \\ \hline
%         \multirow{5}{*}{VisSpeech} & 0 & 17.18 & -- & -- \\
%         & 1 & 17.12 & 16.79 & 1.92\% \\
%         & 2 & 16.82 & 16.61 & 1.21\% \\
%         & 4 & 17.16 & \bf 16.39 & 4.49\% \\
%         & 8 & 17.49 & 16.51 & 5.65\% \\ \hline
%         \multirow{5}{*}{Ego4D} & 0 & 65.45 & -- & -- \\
%         & 1 & 65.74 & 64.97 & 1.17\% \\
%         & 2 & 65.49 & 65.19 & 0.46\% \\
%         & 4 & 65.71 & \bf 64.63 & 1.64\% \\
%         & 8 & 66.40 & 65.49 & 1.37\% \\ \hline
%     \end{tabular}
% \end{table}

\subsection{Experimental Settings}
\paragraph{Implementation Details.}
As mentioned earlier, we use BEST-RQ~\cite{chiu2022self} as the frozen ASR model.
Since it has 24 conformer blocks, we add 24 adapters (one in each layer) in all experiments.
When added, all adapters and visual projectors are randomly initialized.
The decoder predicts WordPiece tokenized graphemes with a vocabulary size of 1,024.
In the adapters, we apply layer norm~\cite{ba2016layer} at every residual connection.
For both phases of training, we use standard SGD with momentum with a moving average coefficient of 0.9 and a cosine learning rate schedule; the initial learning rate is set to 0.4.
We train for 40K and 30K iterations in phase 1 and 2 respectively, with a batch size of 256 on 32 TPU v4 chips.
We run 5 independent experiments and report the mean scores for ablation studies. 
When testing Audiovisual models on audio-only benchmarks, we feed dummy visual inputs (zero tensors).

\noindent\textbf{Metrics.}
We use word error rate (WER) for all evaluation (lower is better).
The alignment between predicted words and ground truth is computed using dynamic programming.
The WER is then computed by the number of errors (deletions, substitutions and insertions) across the whole test set divided by the number of ground truth words. 

\noindent\textbf{Baselines.}
We compare \model~to two strong baselines proposed this year - (i) the state-of-the art AV-ASR model AVATAR~\cite{gabeur2022avatar} and (ii) the state-of-the-art ASR (audio only) model BEST-RQ~\cite{chiu2022self}. We apply both models to the same settings as \model~ for a fair comparison.

\subsection{Datasets}
The additional parameters in our model are finetuned on the HowTo100M dataset, which contains instructional videos from YouTube.
In order to assess generalization, we evaluate across different domains -- LibriSpeech (audiobooks), How2 and VisSpeech (YouTube instructional videos) and Ego4D (egocentric video of daily-life activities). Note that VisSpeech consists of more unconstrained video (background noise, challenging accents etc) than How2. More details for each dataset are provided below. \\
\noindent\textbf{LibriLight~\cite{kahn2020libri} and LibriSpeech~\cite{panayotov2015librispeech}.}
LibriLight is an unlabelled speech dataset that is used to pretrain BEST-RQ.
The model is then finetuned for ASR on LibriSpeech containing 960 hours audio with manually annotated GT transcripts.
For a fair comparison, we also use LibriSpeech for pretraining some of our baselines in the ablations. \\
\noindent\textbf{HowTo100M~\cite{miech19howto100m}.}
This dataset contains 1.2M instructional videos without manual annotations.
ASR is used to obtain pseudo-GT transcripts  
% as they are not manually labelled but a model output) and used 
for training our adapters and visual projector. We remove videos present in VisSpeech and How2 (described next). \\
\noindent\textbf{How2~\cite{sanabria2018how2}.}
We use the 300hr version of How2, which consists of instructional videos with automatically collected user uploaded captions.
The videos are segmented into 5.8s short clips with 20-word long transcripts in average.
We use the validation (2,022 clips) and test (2,305 clips) splits to evaluate our model in a zero-shot setting. \\
\noindent\textbf{VisSpeech~\cite{gabeur2022avatar}.}
VisSpeech is an AV-ASR test benchmark that consists of 503 video clips with manually annotated transcripts, which are sampled from HowTo100M. The dataset curation process focuses on samples where an audio-only ASR model fails and where strong visual correlations are observed. \\
\noindent\textbf{Ego4D~\cite{grauman2022ego4d}.}
Ego4D consists of egocentric video from 74 worldwide locations and 9 countries, with over 3,670 hours of daily-life activity video. We use the audiovisual diarization benchmark in the Ego4D challenge\footnote{\url{https://ego4d-data.org/docs/challenge/}}. 
It consists of 585 5-minutes long egocentric video clips splited into train (397 clips), validation (51 clips) and test (137 clips) sets.
We report zero-shot results on the validation set as the test annotations are not released.
We evaluate transcripts on segmented clips based on GT boundaries.
% For evaluation, we segment input clips using GT boundaries and generate transcripts for each segmented chunk.
%Note that the domain of this dataset is very different from HowTo100M unlike the other two test benchmarks.
% \textcolor{red}{segmentation scheme}

\subsection{Results}
In this section, we show ablations of the various design choices in our model (adapter architecture and bottleneck dimension), and then discuss the impact of curriculum learning and the benefit of adding visual tokens (including the impact of the number of visual tokens). We then show an ablation discussing the impact of adding both adapters and visual tokens, and the impact of finetuning dataset size. Finally, we show zero-shot performance of our model compared to state of the art baselines. Note that all ablations and results are provided on all $3$ downstream datasets in a zero-shot setting -- How2, VisSpeech and Ego4D.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/qualitative_examples/qualitative_how2.pdf}
    \includegraphics[width=\linewidth]{figures/qualitative_examples/qualitative_visspeech.pdf}
    \includegraphics[width=\linewidth]{figures/qualitative_examples/qualitative_ego4d.pdf}
    \vspace{-0.7cm}
    \caption{\textbf{Qualitative Results on  How2 (top), VisSpeech (middle) and Ego4D (bottom).} We show the ground truth (GT), and predictions from the audio only BEST-RQ model (B-RQ) and our audiovisual \model~(Ours) in the zero-shot setting. For each clip we show a single visual frame. Note how the visual context helps with visual objects (tuxedos, veil, scooter, bowl, cake, carrot \textit{etc}), as well as actions (exhale, drive over) and works well even in the ego-centric domain (learns driving from input of road in row 3, column 3). Errors in the predicted words compared to the GT are highlighted in red. Faces are blurred for privacy.}
    \label{fig:qualitative_results}
    \vspace{-1em}
\end{figure*}


\noindent\textbf{Adapter Architecture and Bottleneck Dimensionality.}
Figure~\ref{fig:type_bottleneck} compares results with feed-forward adapters (FF) only vs adapters with both feed-forward and self-attention (FF+SA). We also vary the bottleneck dimension from 32 to 256.
We observe that on How2 (Figure~\ref{fig:type_bottleneck}a) and VisSpeech (Figure~\ref{fig:type_bottleneck}b), both adapter types perform similarly although FF+SA uses significantly more parameters than FF (Figure~\ref{fig:type_bottleneck}d), indicating that a simple projection is enough for strong adaptation. 
% These results indicate that simple projection of individual features could already perform strong multimodal fusion within a frozen ASR model without direct feature aggregation through the self-attention process.
On Ego4D (Figure~\ref{fig:type_bottleneck}c), simple FF outperforms FF+SA by a large margin, potentially because of the larger domain gap (instructional edited videos online to egocentric daily activity videos). The greater number of parameters in FF+SA may result in a larger shift to the instructional video domain and away from Ego4D. 
% Note that Ego4D has a larger domain gap from the training dataset, \ie HowTo100M, than How2 and VisSpeech; HowTo100M, How2 and VisSpeech are all instructional videos whereas Ego4D contains egocentric videos of daily activities.
% FF+SA introduces more parameters resulting in a larger domain shift towards instructional video domain.
% We believe this is the main cause of the significant drops on Ego4D.
Figure~\ref{fig:type_bottleneck} also shows the effect of different bottleneck dimensions. In general the WER comes down from $32$ to $64$, but saturates at $B=64$ across all datasets with FF, while introducing only few additional parameters (0.6\% of the number of parameters in BEST-RQ).
Hence in the rest of experiments, we adopt FF adapters with $B=64$.
\begin{table}[t]
    \centering
    \caption{\textbf{Effect of visual tokens (VT) and adapter layers.}
    Results on 3 datasets are obtained in the zero-shot setting (lower WER\% is better). 
    The first row corresponds to the vanilla pretrained BEST-RQ. 
    Visual projector is added only when feeding VT. 
    The gains from both VT and adapters are complementary.}
    \label{tab:model_ablations}
    \vspace{-0.1cm}
    \scalebox{0.85}{
    \begin{tabular}{cc|ccc}
        \toprule
        \bf VT & \bf Adapters & \bf How2 & \bf VisSpeech & \bf Ego4D \\ \midrule
         & & 21.90 & 31.61 & 77.98 \\
        $\checkmark$ & & 19.74 \small{$\pm$ 0.04} & 31.13 \small{$\pm$ 0.06} & 76.50 \small{$\pm$ 0.11}\\
        & $\checkmark$ &  14.66 \small{$\pm$ 0.03} & 17.18 \small{$\pm$ 0.15} & 65.45 \small{$\pm$ 0.14} \\
        $\checkmark$ & $\checkmark$ & 13.63 \small{$\pm$ 0.10} & 16.39 \small{$\pm$ 0.11} & 64.63 \small{$\pm$ 0.79} \\\bottomrule
    \end{tabular}
    }
    \vspace{-1em}
\end{table}
\begin{table}[t]
    \centering
    \caption{\textbf{Effect of training dataset size.} Results are for audiovisual models trained with our curriculum learning, and are shown on 3 datasets in the zero-shot setting (lower WER\% is better).
    Only 5\% of HowTo100M is required.}
    \label{tab:dataset_size}
    \vspace{-0.1cm}
    \scalebox{0.85}{
    \begin{tabular}{c|ccc}
        \toprule
        \bf Training-set size & \bf How2 & \bf VisSpeech & \bf Ego4D \\\midrule
        5\% & 13.69 \small{$\pm$ 0.17} & 16.60 \small{$\pm$ 0.17} & 64.75 \small{$\pm$ 1.05} \\
        100\% & 13.63 \small{$\pm$ 0.10} & 16.39 \small{$\pm$ 0.11} & 64.63 \small{$\pm$ 0.79} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-1em}
\end{table}
\begin{table*}[t]
    \centering
    \caption{\textbf{Comparison to state-of-the-art methods for zero-shot performance across different AV-ASR datasets.} We also show performance on LibriSpeech which is audio-only. Results are reported as WER \% (lower is better). Note that AVATAR and BEST-RQ are finetuned end-to-end (all parameters) on HowTo100M, whereas for our model, only the visual projectors (VP) and adapters are finetuned on 5\% of the dataset. PT means pretraining. When a model is marked with both LibriSpeech and HowTo100M pretraining, we first train the model on LibriSpeech and then on HowTo100M next. For LibriSpeech evaluation, we report numbers on test-clean set. *LibriSpeech trained model is evaluated directly on LibriSpeech test set. }
    \vspace{-0.2cm}
    \label{tab:zeroshot}
    \scalebox{0.85}{
    \begin{tabular}{c|c|ccc|c|ccc}
        \toprule
         &   &   & \multicolumn{2}{c|}{\textbf{HowTo100M PT}} & &  &  &  \\ 
       \textbf{Method} & \textbf{Modality} & \textbf{LibriSpeech PT} & Pretrained params & Data \% & \textbf{LibriSpeech} & \textbf{How2} & \textbf{VisSpeech} & \textbf{Ego4D} \\ \midrule
        % AVATAR~\cite{gabeur2022avatar} & A & -- &  all & 100 & 27.18 & 19.92 & 37.56 & 95.25 \\      
        AVATAR~\cite{gabeur2022avatar} & A & $\checkmark$  & -- & -- & 8.85 & 39.43 & 65.33 & 110.86 \\
        AVATAR~\cite{gabeur2022avatar} & A+V & -- & All & 100 & 24.65 & 17.23 & 35.66 & 92.03 \\
        AVATAR~\cite{gabeur2022avatar} & A+V & $\checkmark$ & All & 100 & 24.08 & 18.37 & 35.59 & 71.97 \\ \midrule %\hdashline
        BEST-RQ~\cite{chiu2022self} & A & $\checkmark$ & -- & -- & 1.60* & 21.90 & 28.62 & 77.98 \\
        BEST-RQ~\cite{chiu2022self} & A & $\checkmark$ & All & 100 & 5.60 & 15.32 & 16.69 & 68.34 \\ \midrule
        % BEST-RQ~\cite{chiu2022self}+V & A+V & $\checkmark$ & $\checkmark$ & 3.29 & 19.74 & 31.13 & 76.50 \\ 
        %\hdashline
        % \model~(Ours) & A & $\checkmark$ &  Adapters only & 5 & 4.39 & 14.71 & 17.39 & 65.55 \\ 
        \bf \model~(Ours) & \bf A+V & \bf $\checkmark$ &  VP + Adapters & 5 & 4.36 & \bf 13.69 & \bf 16.60 & \bf 64.75 \\ \bottomrule
    \end{tabular}
    }
    \vspace{-0.2cm}
\end{table*}

\noindent\textbf{Curriculum Learning and Visual Tokens.}
We show the results of \model~with and without the proposed two-stage curriculum in Figure~\ref{fig:curriculum_vp}, and also compare to an audio-only baseline which had only FF adapters with $B=64$ and no visual information.
Without curriculum learning, our AV-ASR model is worse than the audio-only baseline across all datasets, with the gap increasing as more visual tokens are added. 
% Joint training of both adapters and visual-audio projecter layers disturbs not only the visual processing but also the domain adaptation.
% The results also confirm the difficulty of learning to leverage visual information in audio-dominant AV-ASR, which is already argued in~\cite{gabeur2022avatar}.
In contrast, when the proposed two-phase curriculum is applied, our AV-ASR model performs significantly better than the baseline audio-only model.
We also test our model with different number of visual input tokens (where one token corresponds to one frame).
More visual tokens improves the model up until $M=4$ with up to 7.0\% relative improvement, after which performance begins to degrade. Hence we set $M=4$ in all experiments. \\
\noindent\textbf{Complementary Gain of Additional Components.}
Table~\ref{tab:model_ablations} shows the effect of our additional lightweight components (projection layer for visual tokens and adapter layers) for zero-shot AV-ASR. The first row is simply the vanilla baseline (frozen BEST-RQ).
We observe that adding projected visual tokens and adapters bring individual gains to the baseline (the former adding visual information and the latter aiding with audio-domain adaptation), and when  combined with our curriculum learning, are complementary to performance, achieving the lowest WER. \\
\noindent\textbf{Training Dataset Size}.
Given our additional components are so lightweight, we test whether adaptation can be done with a small amount of weakly labelled data. 
The results in Table~\ref{tab:dataset_size} show only 5\% of HowTo100M training data performs on par with the full dataset -- \ie\ the pretrained knowledge in BEST-RQ and CLIP yields considerable data efficiency to the model. Ablation results with more data fractions are provided in the appendix. \\
\noindent\textbf{Comparisons to Zero-shot Baselines on AV-ASR.}
We compare our model to baselines in Table~\ref{tab:zeroshot}, for zero-shot performance on all 3 AV-ASR benchmarks\footnote{Note that the original AVATAR and BEST-RQ papers do not report this. We apply these models in the same setting as ours for a fair comparison.}
\model~outperforms AVATAR and BEST-RQ on all, even outperforming both AVATAR and BEST-RQ when they are fully finetuned on LibriSpeech and then 100\% of HowTo100M (3rd and 5th row). Note for BEST-RQ, this involves finetuning 0.6B params. Our model, in contrast only finetunes 4M params on 5\% of HowTo100M. 

\noindent\textbf{Comparisons to Zero-shot Baselines on LibriSpeech.}
Even though this is not the main goal of this work, we also investigate performance on LibriSpeech, which is audio-only (Table~\ref{tab:zeroshot}). Note other AV-ASR works do not do this, but we believe it is important for deployment of AV-ASR models. We first note that AVATAR pretrained on LibriSpeech and then finetuned on HowTo100M performs poorly when re-evaluated on LibriSpeech (showing severe catastrophic forgetting between rows 1 and 3). We believe this is because all parameters are trained end-to-end. On the other hand, AVFormer performs much better on LibriSpeech (4.36 vs 24.08), and is much closer to BEST-RQ's 1.60 which is a model tuned only for LibriSpeech and incapable of AV-ASR, while \model~achieves SOTA on AV-ASR as well. \\
\noindent\textbf{Qualitative Results.} Qualitative examples are provided in Fig. \ref{fig:qualitative_results} comparing our method to audio-only BEST-RQ for zero-shot ASR. We show that for all 3 downstream AV-ASR datasets, visual context improves mistakes that are made on objects (\textit{eg.} tuxedos, veil and scooter from the top row), actions (exhale - top row, second column), and even corrects a homophone \footnote{same pronunciation, different spelling} (colonels to kernals, row 2 column 4). \\
%We also show a failure example are provided in the supp. mat.\\
% \begin{table}[t]
%     \centering
%     \caption{\textbf{Comparing to the state-of-the-art on How2.}
%     % \textbf{Comparing to the state-of-the-art across all three AV-ASR datasets.}
%     The models are tested in the supervised setting.
%     AVFormer and AVATAR trained on How2 are also evaluated on the other datasets to see the model's generalizability. \textcolor{red}{abbriviations will be modefied and clarified in the caption.}}
%     \label{tab:sota_comp}
%     \begin{tabular}{l|c|c|ccc}
%         \toprule
%         \bf Method & \bf \#TP & \bf H2 & \bf VS & \bf E4D & \bf LS\\\midrule
%         VAT~\cite{Caglayan2019VAT} & -- &  18.0 & -- & -- \\
%         MultiRes~\cite{paraskevopoulos2020multires} & -- &  20.5 & -- & --\\
%         LLD~\cite{Ghorbani2021LLD} & -- & 16.7 & -- & -- \\
%         AVATAR \cite{gabeur2022avatar} & 271M & 9.11 & 11.28 & 57.84 &   \\ \midrule
%         \model~(Ours) & 4M & 10.24 & 16.30 & 60.54 & 4.21 \\
%         \bottomrule
%     \end{tabular}
% \end{table}
\begin{table}[t]
    \centering
    \caption{\textbf{Finetuning performance on How2 and Ego4D.} We outperform all previous works on How2 that use frozen visual features. AVATAR is trained end-to-end, with all visual parameters finetuned. Scores are in WER~\%.
    % , other than for VisSpeech where they are finetuned on How2 (as is standard~\cite{gabeur2022avatar}).
    }
    \label{tab:sota_comp}
    \scalebox{0.85}{
    \begin{tabular}{lc|cc}
        \toprule
        \bf Method & \bf Frozen visual\ feats & \bf How2 
        % & \bf VisSpeech 
        & \bf Ego4D \\\midrule
        VAT~\cite{Caglayan2019VAT} &$\checkmark$ &  18.0 & -- \\
        MultiRes~\cite{paraskevopoulos2020multires} & $\checkmark$ & 20.5 & --\\
        LLD~\cite{Ghorbani2021LLD} &$\checkmark$ &  16.7 & -- \\
        AVATAR \cite{gabeur2022avatar} & & 9.11  & 55.27 \\ \midrule
        \model~(Ours) & $\checkmark$ & 10.22 & \textbf{55.23} \\
        \bottomrule
     \vspace{-2em}
    \end{tabular}
    }
\end{table}
\noindent\textbf{Comparisons to SOTA after Finetuning.}
For completeness, we also show finetuning results on two domains - instructional (How2) and egocentric (Ego4D) videos in Table \ref{tab:sota_comp}. We outperform all previous works on How2 that use frozen visual features. Our model is also not too much worse (How2) or on par (Ego4D) with AVATAR, even though AVATAR is trained end-to-end, and all parameters (including a large visual encoder) are finetuned. 