% !TEX root = ../main.tex
\section{Introduction}
\label{sec:intro}
Robustness or adaptation to new, unconstrained domains is a key challenge for automatic speech recognition (ASR) systems. In multimodal video (\eg, TV, online edited videos), the visual stream can
provide strong cues for improving the robustness of ASR systems, particularly in cases
where the audio is noisy -- this is called audiovisual ASR (AV-ASR). Unlike works that simply focus on lip motion~\cite{Noda2014AudiovisualSR,Tamura2015AudiovisualSR,Chung2017LipRS,Afouras2018DeepAS,Petridis2018EndtoEndAS,Makino2019RecurrentNN,Ma2021EndToEndAS,Serdyuk2021AudioVisualSR}, we investigate the contribution of entire visual frames.
This is particularly useful for videos `in the wild', where the
mouth is not necessarily visible (\eg, egocentric viewpoints, face coverings, and low resolution etc.)~\cite{gabeur2022avatar}. 
The task is illustrated in Figure~\ref{fig:av-asr}.

Building audiovisual datasets for training AV-ASR models, however, is challenging. Datasets such as How2~\cite{sanabria2018how2} and VisSpeech~\cite{gabeur2022avatar} have been created from instructional videos online,
%however labels in How2 are noisy due to its automatic data collection process whereas VisSpeech only contains 500 \textit{test} samples.
but they are small in size. % (in the order of thousands of samples) and hence are more appropriate as \textit{test} sets rather than training sets. 
Not only are datasets for this task small, but models are typically large and consist of both visual and audio encoders. For example the latest AV-ASR model AVATAR~\cite{gabeur2022avatar} shows impressive performance on both datasets, but requires the end-to-end training of visual and audio components in tandem, and consequently a large amount of compute. Like other AV-ASR works~\cite{Miao2016OpenDomainAS,Gupta2017VisualFeatures,Palaskar2018EndtoendMS,Caglayan2019VAT,Srinivasan2020LookingEL}, it is also only trained and tested on instructional videos, and as we show in the experiments, generalizes poorly to new domains in the zero-shot setting. 
% AVATAR solves this issue by large-scale weakly supervised training on unlabelled audiovisual datasets such as HowTo100M~\cite{miech19howto100m}. 

On the other hand, there have been a number of recently released large-scale audio-only models~\cite{chiu2022self,chung2021w2v,hsu2021hubert} that are heavily optimised via self-supervised pretraining and large-scale supervised training on \textit{audio-only} data obtained from audio books such as LibriLight~\cite{kahn2020libri} and LibriSpeech~\cite{panayotov2015librispeech}. These models contain billions of parameters, are readily available, and show strong \textit{generalization across domains}. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/avasr.pdf}
    \caption{\textbf{Unconstrained audiovisual speech recognition.} We inject vision into a frozen speech model (BEST-RQ, in grey) for zero-shot audiovisual ASR via lightweight modules to create a parameter and data efficient model called AVFormer (blue).  
    The visual context can provide helpful clues for robust speech recognition especially when the audio signal is noisy (the visual loaf of bread helps correct the audio-only mistake \textcolor[RGB]{155,0,0}{clove} to \textcolor[RGB]{0,110,51}{loaf} in the generated transcript). } 
    \label{fig:av-asr}
\end{figure}

Our goal is to reuse the extensive expertise and training time that has been invested in such models, by using their weights. We are inspired by recent works adapting \textit{frozen} foundation models for multi-modal tasks. A popular example is~\cite{alayrac2022flamingo} that injects visual information into large language models (LLMs) for vision-text tasks. The benefit of building on strong frozen LLMs for these tasks is the hope that this will enable the visual-text model to retain powerful \textit{language-only} abilities such as few-shot language adaptation or external knowledge retrieval. Our goal is simple - we wish to do the same for AV-ASR, using strong audio-only ASR models. We add visual inputs to these models in a lightweight manner to enable AV-ASR, but still maintain the benefits of audio-only pretraining for zero-shot generalization. 

Our framework is called \model{}, and injects visual information into a frozen ASR model using lightweight projection layers and trainable adaptors. We show that these can be trained on a small amount of weakly labelled video data (only 5\% of the data used by existing state of the art methods~\cite{gabeur2022avatar}) with minimum additional training time and parameters, minimizing the domain shift and catastrophic forgetting that can accompany end-to-end finetuning. In order to further ensure stability during the finetuning of these adapters, we also introduce a simple curriculum scheme during training which we show is crucial to enable the model to jointly process audio and visual information effectively. Finally,  we show that our model outperforms existing state of the art zero-shot methods on three different AV-ASR benchmarks (How2, VisSpeech and Ego4D) across different domains, while also crucially preserving decent performance on traditional audio-only speech recognition benchmarks (LibriSpeech). 

% We are the first to show zero-shot performance for AV-ASR across multiple domains.