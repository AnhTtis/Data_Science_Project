% !TEX root = ../main.tex
\section{Related Works}
\label{sec:related}
\noindent\textbf{State-of-the-Art Speech Recognition}
Recent state-of-the-art ASR models~\cite{chiu2022self,hsu2021hubert,chung2021w2v,zhang2020pushing,xu2021self} almost all adopt transformer based audio encoders~\cite{gulati2020conformer,synnaeve2019end,hsu2021hubert} embedding input audio signals into a set of token features thereby extracting local information within a temporal window.
Encoders are trained end-to-end using losses such as CTC~\cite{graves2006connectionist}, RNN-T~\cite{graves2012sequence} and LAS~\cite{chan2016listen}.
In many cases, these encoders are pretrained~\cite{chiu2022self,hsu2021hubert,chung2021w2v,zhang2020pushing,xu2021self} on large-scale unannotated datasets such as LibriLight~\cite{kahn2020libri}, and then finetuned for downstream ASR. Consequently, such models incorporate a number of highly-engineered training tricks and techniques suitable for ASR, which we want to reuse for multimodal inference.
Rebuilding a multimodal model from scratch incorporating these learnings is expensive and must be redone for each new model. As models get larger and larger~\cite{hsu2021hubert,chung2021w2v,zhang2020pushing}, this requires a prohibitive amount of compute. Our goal is to reuse this knowledge in a lightweight manner by injecting visual understanding capability into a readily available state-of-the-art ASR model. \\
\noindent\textbf{Audiovisual Speech Recognition }
% For AV-ASR focused on lip motion~\cite{Afouras2018DeepAS,Ma2021EndToEndAS}, the two most popular losses are CTC~\cite{Graves2006ConnectionistTC} and Seq2Seq~\cite{Sutskever2014SequenceTS,Cho2014LearningPR}.
Most AV-ASR works are focused on lip motion, right from early works that use pre-extracted features~\cite{Noda2014AudiovisualSR,Tamura2015AudiovisualSR} to more recent end-to-end approaches that work on pixels directly~\cite{Chung2017LipRS,Afouras2018DeepAS,Petridis2018EndtoEndAS,Makino2019RecurrentNN,Ma2021EndToEndAS,Serdyuk2021AudioVisualSR}. In contrast, the setting explored in this work is full frame AV-ASR beyond the speaker's mouth movements (also known as `context-aware' speech recognition). Here the defacto strategy is to use pre-extracted visual context features (due to the high dimensionality of full frame video) -- either action features~\cite{sanabria2018how2,Caglayan2019VAT,Ghorbani2021LLD,paraskevopoulos2020multires}, or place and object features~\cite{Miao2016OpenDomainAS,Gupta2017VisualFeatures,Palaskar2018EndtoendMS,Caglayan2019VAT,Srinivasan2020LookingEL}. Unlike these works which all use visual features from classification models trained on a closed-set of pre-defined objects, places or actions, we use features from CLIP~\cite{Radford2021CLIP}, which is trained on image and text paired data, and known to have strong generalization and zero-shot capabilities. This makes our features more suited to unconstrained videos `in the wild'. An outlier is the recently proposed AVATAR~\cite{gabeur2022avatar}, which uses full frame pixels and trains end-to-end on HowTo100. It is the state of the art for this task, achieving good performance on How2 and introducing a new dataset called VisSpeech. Unlike AVATAR, our method reuses strong frozen pretrained models, thereby requiring only 5\% of the audiovisual data used in AVATAR, and generalises much better across different domains in the zero-shot setting. \\ 
\noindent\textbf{Adapting Large Frozen Pretrained Models} There has been a recent flurry of works that adapt frozen foundation models for multi-modal tasks, most notably for injecting visual information to large language models (LLMs)~\cite{alayrac2022flamingo}.  Architectural details vary: for example MAGMA~\cite{eichenberg2021magma} and Frozen-BiLM~\cite{yang2022frozenbilm} add bottleneck adapters~\cite{houlsby2019parameter,sung2022vl} to the frozen LLM injecting some visual information; ClipCap~\cite{mokady2021clipcap} learns a vision-to-prefix bridging transformer to map vision features into a prefix for GPT-2, while VC-GPT~\cite{luo2022vc} adds new learnt layers to the frozen LLM. In the AV-ASR domain specifically, multiple works use pre-extracted visual features to improve audio-only ASR~\cite{Miao2016OpenDomainAS}. Early work~\cite{Miao2016OpenDomainAS}
leverages objects and places features from visual classifiers by 
projecting them to the same space as the audio features in a process known as Visual Adaptive Training (VAT).~\cite{Gupta2017VisualFeatures} also uses similar features, but adopts them as the beginning token of each sentence in a language modelling framework.~\cite{Caglayan2019VAT} also uses VAT, but for a sequence to sequence model. Unlike these works which use a single visual feature, we show that having multiple visual features improves performance. The closest to our work is LLD~\cite{Ghorbani2021LLD}, which also uses a stream of visual features extracted from the MIL-NCE model~\cite{miech2020end}. Their fusion method, however, consists of a complicated deliberation decoder, and while they initialize their model with audio-only pretraining, they then finetune the entire audiovisual model end-to-end. In contrast, most of our model remains frozen, and only lightweight adapters are tuned on a small amount of audio-visual data.    
All previous works are also only focused on the instructional video domain, reporting results either on internally collected datasets or the publicly released How2~\cite{sanabria2018how2}. Our focus instead is on zero-shot generalisation across multiple domains, including audio-only Librispeech~\cite{panayotov2015librispeech} (from audiobooks) and Ego4D~\cite{grauman2022ego4d} (egocentric video). We believe this is a more useful setting for actual deployment of such models. 
