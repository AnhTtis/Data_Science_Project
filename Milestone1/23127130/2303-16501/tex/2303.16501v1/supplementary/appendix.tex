
\appendix

\part{Appendix} 
In this appendix we provide additional ablations with varying the number of adapter layers and with more complex visual projectors in Section~\ref{sec:num_layers} and \ref{sec:complex_vp}, respectively.
In Section~\ref{sec:iter_train}, we investigate the effects of iterative training.
Then we supplement Table 2 of the main paper by providing results with more training dataset fractions in Section~\ref{sec:data_size},
and show additional experiments replacing the RNN-T decoder with a cross-attentional transformer decoder in Section~\ref{sec:decoder}.
Finally, we present a failure case in Section~\ref{sec:failure}. 



\section{Number of Adapter Layers}
\label{sec:num_layers}

Figure~\ref{fig:num_layers} shows the word error rate (WER) when varying the number of adapter layers from 4 to 24.
Note that the number of the conformer blocks in BEST-RQ is 24 and therefore, the model with 24 adapters means that an adapter is added to every conformer block in the model.
We also note that we add adapter layers to the last conformer blocks (before the decoder) when fewer than 24 layers are added to achieve the best performance.

WER for How2 (Figure~\ref{fig:num_l_how2}) and VisSpeech (Figure~\ref{fig:num_l_visspeech}) monotonically decreases as we add more adapter layers to the model.
For Ego4D (Figure~\ref{fig:num_l_ego4d}), the performance saturates at 20 layers.
These results suggest that it is critical to inject an adapter into every conformer block.




\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.75\linewidth]{supplementary/figures/num_layers_how2.pdf}
        \caption{How2}
        \label{fig:num_l_how2}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.75\linewidth]{supplementary/figures/num_layers_visspeech.pdf}
        \caption{VisSpeech}
        \label{fig:num_l_visspeech}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.75\linewidth]{supplementary/figures/num_layers_ego4d.pdf}
        \caption{Ego4D}
        \label{fig:num_l_ego4d}
    \end{subfigure}
    \caption{\textbf{Effect of the number of adapter layers.} Models are trained with 4 visual tokens using our curriculum learning strategy. Performance improve on all datasets as we increase the number of adapter layers. Lower WER is better.}
    \label{fig:num_layers}
\end{figure}

\begin{figure*}[t]
    \centering
    % \vspace{-0.4cm}
    % \scalebox{0.8}{
    % \begin{tabular}{cc|ccc}
    %     \toprule
    %     \bf \# iters & \bf 2nd curr. & \bf How2 & \bf VisSpeech & \bf Ego4D \\ \midrule
    %     1 & N/A & \bf 13.63 \small{$\pm$ 0.10} & \bf 16.39 \small{$\pm$ 0.11} & \bf 64.63 \small{$\pm$ 0.79} \\\midrule
    %     2 & & 14.80 \small{$\pm$ 0.48} & 17.27 \small{$\pm$ 0.10} & 65.70 \small{$\pm$ 0.55} \\
    %     2 & $\checkmark$ & 14.74 \small{$\pm$ 0.18} & 17.28 \small{$\pm$ 0.16} & 65.71 \small{$\pm$ 0.28} \\\bottomrule
    % \end{tabular}
    % }
    \includegraphics[width=\linewidth]{supplementary/figures/iter_train.pdf}
    % \vspace{-0.8cm}
    \caption{\textbf{Effects of iterative training on How2 and Ego4D.} Our model is trained for the second time with both or without our proposed curriculum using different learning rates.
    }
    \label{fig:iter_train}
\end{figure*}
\section{More Complex Visual Projector}
\label{sec:complex_vp}
We also test a more complex visual projector in the form of a multi-layer perceptron (MLP) with varying number of layers (Table~\ref{supptab:vp_layers}).
The results consistently show on all three datasets that a single linear layer is sufficient for good performance (lower is better), and adding more layers makes a marginal impact (within error bars). Note that similar results are observed in \cite{eichenberg2021magma} for prefix matching tasks.


\section{Iterative Training}
\label{sec:iter_train}
In this section, we investigate iterative applications of our curriculum. We train our model for the second time, both with or without our proposed curriculum. The results in Figure~\ref{fig:iter_train} present performance degradation compared to our model with single iteration (gray dotted lines) in both cases on all three benchmarks.
%How2 and Ego4D (same trends hold on VisSpeech).
We observe that a larger learning rate increases the WER. We believe that this phenomenon is due to over-adaptation to HowTo100M.



\section{Effect of Dataset Size}
\label{sec:data_size}
We extend the ablation presented in Table 2 of the main paper in Table~\ref{supptab:dataset_size}.
Due to the strong pretrained knowledge in BEST-RQ, we show that only a small fraction (5\%) of the HowTo100M training dataset is enough to achieve comparable performance with training on the full dataset. This shows that our adapted model is extremely data efficient. 


\begin{table}[t]
    \centering
    \scalebox{0.85}{
    \begin{tabular}{c|ccc}
        \toprule
        \bf \# layers & \bf How2 & \bf VisSpeech & \bf Ego4D \\ \midrule
         \bf 1 & \bf 13.63 \small{$\pm$ 0.10} & \bf 16.39 \small{$\pm$ 0.11} & \bf 64.63 \small{$\pm$ 0.79} \\
         2 & 13.77 \small{$\pm$ 0.09} & 16.47 \small{$\pm$ 0.34} & 64.75 \small{$\pm$ 0.81} \\
         3 & 13.93 \small{$\pm$ 0.21} & 16.49 \small{$\pm$ 0.14} & 65.20 \small{$\pm$ 0.56} \\
         4 & 13.72 \small{$\pm$ 0.11} & 16.49 \small{$\pm$ 0.25} & 65.04 \small{$\pm$ 0.50} \\\bottomrule
    \end{tabular}
    }
        \caption{\textbf{Effect of the number of MLP layers in the visual projector.} ReLU is used as the intermediate activation function.}
    \label{supptab:vp_layers}
    \vspace{-0.3cm}
\end{table}

\begin{table}[t]
    \centering
    \scalebox{0.85}{
    \begin{tabular}{c|ccc}
        \toprule
        \bf Dataset Size & \bf How2 & \bf VisSpeech & \bf Ego4D \\ \midrule
5\%&	13.69 & 16.60 & 64.75\\
10\%&	13.79 & 16.56 & 65.37\\
25\%&	13.60 & 16.57 & 64.29\\
50\%&	13.63 & 16.53 & 64.63\\
75\%&	13.66 & 16.69 & 65.11\\
100\%&	13.63 & 16.39 & 64.63\\
        \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Effect of training dataset size.} Models are trained with 4 visual tokens using our curriculum strategy. Models are trained with varying fractions of HowTo100M. All scores are in WER\% (lower is better). The results show that 5\% of dataset is enough to achieve state-of-the-art performance.}
    \label{supptab:dataset_size}
\end{table}



\section{Autoregressive Decoder}
\label{sec:decoder}
Finally, we test our method with different decoders: an RNN-Transducer (RNN-T) and a transformer decoder using cross-attention (Cross-attention) introduced in \cite{vaswani2017attention}.
RNN-T is the decoder used in the pretrained BEST-RQ model, we keep the weights frozen when training for AV-ASR.
The cross-attention decoder performs autoregressive decoding while cross-attending to all input tokens.
We stack 8 decoder transformer blocks; the weights are randomly initialized and tuned during the AV-ASR training.

Table~\ref{tab:decoder} shows the results of these models on the four datasets (LibriSpeech and the three AV-ASR benchmarks). 
% We observe the original RNN-T decoder significantly outperforms Cross-attention.
% The cross-attention decoder does significantly worse on LibriSpeech even though it uses the full pretrained encoder. 

The cross-attention decoder performs worse than RNN-T on the three AV-ASR benchmarks, while performing \textit{significantly} worse on Librispeech.
Note that Cross-attention uses the entire set of input encodings for generating each output token whereas every output token is generated from a single input encoding with RNN-T.
% Note that each output token is generated from a single input encoding (with a local temporal window) whereas the new transformer decoder cross-attends on the entire set of input encodings.
However, the results show that maintaining the pretrained decoding knowledge in RNN-T is more important than introducing larger flexibility in a finetuned decoder.



\section{Failure Analysis}
\label{sec:failure}
% Finally, we test our method with different decoders: an RNN-Transducer (RNN-T) and a transformer decoder using cross-attention (Cross-attention) introduced in \cite{vaswani2017attention}.
% RNN-T is the decoder used in the pretrained BEST-RQ model, we keep the weights frozen when training for AV-ASR.
Figure~\ref{fig:failure} shows a failure case with an erroneous word `hands' introduced by the visual input. However, we find this case very rare in our extensive qualitative exploration.
% Although visual inputs improves the robustness of the model and allows to induce correct words by incorporating visual cues, in rare cases, it c


\begin{table}[t]
    \centering
    \scalebox{0.85}{
    \begin{tabular}{ccccc}
        \toprule
        \bf Decoder & \bf LibriSpeech & \bf How2 & \bf VisSpeech & \bf Ego4D \\
        \midrule
        Cross-attention & 13.79 & 16.67 & 20.21 & 70.47 \\
        RNN-T & 4.40 & 13.63 & 16.39 & 64.63\\
        \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Results with different decoders.} All scores are in WER\% (lower is better). RNN-T represents that an RNN-T decoder is initialized with the pretrained BEST-RQ weights and frozen. Cross-attention means that we replace the original RNN-T decoder with a autoregressive transformer decoder using cross-attention on the input token embeddings. Results are reported on all three AV-ASR benchmarks as well as on LibriSpeech.}
    \label{tab:decoder}
\end{table}



\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{rebuttal_figures/RsPpGEJjLCU-297611999-305104000.jpeg}

    \vspace{0.1cm}
    \begin{tabular}{rl}
        GT: & and tie up both ends with a simple knot \\
        Ours: & and tie up both \textcolor{red}{hands}  with a simple knot
    \end{tabular}
  \caption{\textbf{A failure example on VisSpeech.}}
  \label{fig:failure}
\end{figure}
%
