\documentclass[12pt]{article} %***
\usepackage[sectionbib]{natbib}
\usepackage{array,epsfig,fancyheadings,rotating}
\usepackage[]{hyperref}  %<----modified by Ivan
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{sectsty, secdot}
%\sectionfont{\fontsize{12}{15}\selectfont}
\sectionfont{\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont}
\renewcommand{\theequation}{\thesection\arabic{equation}}
\subsectionfont{\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textwidth=31.9pc
\textheight=46.5pc
\oddsidemargin=1pc
\evensidemargin=1pc
\headsep=15pt
%\headheight=.2cm
\topmargin=.6cm
\parindent=1.7pc
\parskip=0pt

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{amsthm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{xcolor}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Comandos introducidos para simplificar escritura
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\lc}{\left[}
\newcommand{\rc}{\right]}
\newcommand{\lb}{\left\{}
\newcommand{\rb}{\right\}}
\newcommand{\Hb}{\mathbb{H}}
\newcommand{\ind}[1]{\mathbbm{1}_{\lrb{#1}}}
\newcommand{\inprod}[2]{\langle#1,#2\rangle_\Hb}
\newcommand{\lrp}[1]{\left(#1\right)}
\newcommand{\lrc}[1]{\left[#1\right]}
\newcommand{\lrb}[1]{\left\{#1\right\}}
\newcommand{\Esp}[1]{\mathbb{E}\lc #1\rc}



\setcounter{page}{1}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\newtheorem{proof}{Proof}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\def\n{\noindent}
\lhead[\fancyplain{} \leftmark]{}
\chead[]{}
\rhead[]{\fancyplain{}\rightmark}
\cfoot{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
	\sf
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\renewcommand{\baselinestretch}{2}
	
	\markright{ \hbox{\footnotesize\rm
			%{\footnotesize\bf 24} (201?), 000-000
		}\hfill\\[-13pt]
		\hbox{\footnotesize\rm
			%\href{http://dx.doi.org/10.5705/ss.20??.???}{doi:http://dx.doi.org/10.5705/ss.20??.???}
		}\hfill }
	
	\markboth{\hfill{\footnotesize\rm W. Gonz\'alez--Manteiga$^{1}$, M.D. Ruiz--Medina$^{2}$ and M. Febrero--Bande$^{1}$} \hfill}
	{\hfill {\footnotesize\rm ARH(1) GOODNESS--OF--FIT } \hfill}
	
	\renewcommand{\thefootnote}{}
	$\ $\par
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont \vspace{0.8pc}
	\centerline{\Large\bf Testing the goodness-of-ﬁt of }
	\vspace{2pt}
	\centerline{\Large\bf a functional autoregressive model}
	\vspace{.4cm}
	\centerline{W. Gonz\'alez--Manteiga$^1$,  M.D. Ruiz--Medina$^2$,  M.   Febrero--Bande$^1$}
	\vspace{.4cm}
	\centerline{\it Universidad de Santiago de Compostela$^1$  and Universidad de Granada$^2$}
	\vspace{.55cm} \fontsize{9}{11.5pt plus.8pt minus.6pt}\selectfont
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\begin{quotation}
		\noindent {\it Abstract:}
%% Text of abstract
The proposed Goodness--of--Fit  (GoF) test for checking the linear autocorrelation model in a functional time series is based on
an  empirical process, whose residual marks and covariate index set are in a separable Hilbert space $\mathbb{H}.$  A functional central limit theorem is derived providing the convergence of the  empirical process to a time-changed   Wiener process evaluated in a separable Hilbert space $\mathbb{H},$ with subordinator given by the marginal probability of the involved Autoregressive Hilbertian process  (AR$\mathbb{H}$(1) process).  The  large sample behavior of the test statistics is  obtained under  simple and composite  null hypotheses. The consistency of the test is addressed under simple null hypothesis.
The simulation study provided in  the Appendix illustrates the finite--sample performance of the testing procedure under different families of alternatives.


\vspace{9pt}


\noindent {\it Key words and phrases:} AR$\mathbb{H}$(1) process;
generalized functional empirical processes;    goodness--of--fit test.

\par
\end{quotation}\par



\def\thefigure{\arabic{figure}}
\def\thetable{\arabic{table}}

\renewcommand{\theequation}{\thesection.\arabic{equation}}


\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont






\section{Introduction}
\label{secintro}
Functional time series  models have been extensively analyzed in the last few decades, supporting inference on stochastic processes.
Special attention has been paid to functional time series defined in terms of random functions observed through time in regular periods.
One can refer the reader to  the monograph by \cite{Bosq2000} for linear functional time series in a state space framework.
There are several motivating examples arising in Demography, Finance, Environmental Sciences and many other contexts. Weakly dependent functional times series analysis (see, e.g.,  \cite{Kokoszka10}) constitutes a first methodological  attempt in the literature for the extension of the classical vector--valued time series framework. Also it provides  a more flexible analysis than the one based on independent and identically distributed functional random variables (see  \cite{Horvath2012a} on functional data analysis, and \cite{Ferraty06} on nonparametric functional  statistics).

Long run covariance estimation has played a crucial role in functional time series (see \cite{Berkes16}), particularly, in  recent developments in  long range dependence analysis  (see, e.g.,  \cite{Li2020}; \cite{RuizMedina2022}).
In a more general context, one can mention  linear functional regression from correlated in time bivariate functional (surface) data (see, e.g.,  \cite{OvalleRM24};  \cite{RuizMD}).

In the last few years, there exists a vast literature on testing  different hypothesis  on the dependence structure  in functional time series. In particular, testing independence  constitutes a key subject in this research area (see, e.g., \cite{Hlavka21}; \cite{Horvath2013}; 	\cite{Zhang16}). On the other hand, second--order properties like separability, stationarity and linear correlation range  are usually tested with the objective of the specification of the covariance operator family adapted for these models (see \cite{Constantinou18}; \cite{Horvath2014}; \cite{Kokoszka2013}; \cite{Kokoszka2017}; \cite{Zhang15}, among others). Alternative contributions can be found in testing  the assumption of normality and periodicity, as given in
\cite{Gorecki18}  and \cite{Horman18}. The issue of change point testing of the mean, of the variance or of the autocorrelation operator also leads to important contributions, as it can be seen in the already mentioned book by \cite{Horvath2012a}.  In all the described procedures, the calibration of the
distributions of the associated statistical tests has been achieved from the asymptotic theory or resampling techniques, as for example the bootstrap.

A very recent topic, of high interest, in the context of functional time series, is  the \emph{goodness of fit} (GoF) of the structure of the models to be used in one omnibus way. A clear motivation is given by the recent availability of high frequency data in several fields, as for example in finance, where a simple structure for the model can be useful for the forecasting. The recent papers by \cite{Amato24} and \cite{Elias22} constitute some examples where statistical analysis based on  FAR (Functional Autoregressive) and AR$\mathbb{H}$ (Autoregressive Hilbertian) models is considered,  providing a generalization of the well--known AR (autoregressive) time  series setting widely used in  forecasting.

The GoF setup  for regression models in the Euclidean context is a well--established field with several contributions in the last decades. For a comprehensive review, see \cite{Manteiga13}. A good review for the GoF of regression models with functional data, under
the scenario of independent and identically distributed functional random variables, can be found in \cite{GM22}. However, there are few papers devoted to specification tests for functional time series. They mainly cover with noise testing, i.e., the case of functional white noise null hypothesis (see, for example, \cite{Zhang16}, \cite{Kim23} or more recently \cite{Kim24}), with generalizations to the GoF for functional autoregressive models (FAR), using different approximations based on the periodogram operator in the spectral domain, or the autocovariance estimates constructed from principal components projections.



The  present paper extends in the linear setting  the methodological GoF  approach in \cite{KoulStute1999}, based on empirical processes, to the context of functional autoregressive models. Specifically, our test statistics involves an  empirical process   marked by functional residuals, and indexed by a Hilbert--valued  covariate.  This approach has also been adopted in
\cite{Cuesta}  in the context  of the functional linear model with scalar response. See also the computational approaches implemented in
\cite{Alvarez25}  and \cite{GPotugues21} for the case of funtional response and covariate.

A challenging topic in the above referred GoF functional  testing framework   is the derivation of the   limiting process characterizing the asymptotic properties of the involved generalized $\mathbb{H}$--valued  empirical process.
Theorem \ref{th3} of the present  paper solves this open  problem. Specifically, applying an invariance principle based on Robbins-Monro procedure (see  Theorem 2 in  \cite{Walk77}), we prove the convergence in distribution, under simple and composite  null hypothesis $H_{0},$  of our test statistics to an $\mathbb{H}$--valued Wiener process, whose index set is also $\mathbb{H}.$ The consistency of the test is proved under simple null hypothesis in   Section \ref{s4}.

The large sample analysis  of the test, performed  under composite $H_{0},$ is based on the strong--consistency results derived in Chapter 8  in  \cite{Bosq2000} when the  eigenvalues of the autocovariance operator of an AR$\mathbb{H}$(1) process are unknown,  covering the cases of known and unknown eigenfunctions. These  results allow us to prove the asymptotic equivalence in probability of the test statistics under simple and composite  $H_{0}$ (see Theorem \ref{th4}).


The asymptotic power analysis deserves further attention and will be addressed in a subsequent paper. The finite--sample power properties are illustrated in the simulation study undertaken, under simple and composite null hypothesis, considering   two different alternative hypothesis scenarios. Specifically, Section \ref{illb} of the Appendix
considers nonlinearities in the functional covariates defining the alternative, assuming the eigenvalues and eigenfunctions of the autocovariance operator are unknown under composite $H_{0}.$ While  Section   \ref{sph1} in the Appendix  illustrates the performance of the GoF testing procedure to discriminate between  two families of linear autocorrelation operators, in the framework of SP$\mathbb{H}$AR(1) models. Here,  under composite $H_{0},$ assuming  invariance of the involved operators,  the eigenfunctions are  known and  the eigenvalues are unknown.
Under both scenarios, for relatively small functional sample sizes,  robust empirical  test sizes, and  good  empirical power  properties are observed  under simple $H_{0}.$   The impact of the misspecification level of the   autocorrelation operator   is also illustrated  under composite $H_{0}.$ As expected, when the misspecification level is lower, that is the case of invariant  SP$\mathbb{H}$AR(1) model scenario, where the eigenfunctions are known, but  the eigenvalues are unknown,  better performance is observed for small sample sizes than in the case of unknown eigenfunctions and eigenvalues analyzed in Section \ref{illb} of the Appendix.




The outline of the paper is the following. Preliminaries are provided in Section \ref{sec1}. The formulation of the GoF testing procedure and its implementation in practice are given in Section \ref{test}.
  The asymptotic distribution of the marginals of the generalized $\mathbb{H}$--valued empirical process is provided in the  Central Limit Theorem derived in Section \ref{s2}.The Functional Central Limit Theorem  in    Section \ref{s3} introduces the limiting  generalized $\mathbb{H}$--valued  Gaussian process, which is identified, in probability distribution, with time--changed $\mathbb{H}$--valued Wiener process  under $H_{0}.$ From Continuous Mapping Theorem, the critical value for the supremum norm of the  empirical standardized random projected test statistics  is obtained from the standard Gaussian distribution, by applying reflection principle  given in terms of  the boundary crossing probabilities of   Brownian motion.
Consistency of the GoF test is addressed in  Section \ref{s4}.  The   composite $H_{0}$ scenario is analyzed  in Section \ref{s6}.
Final discussion is presented in Section \ref{fc}. Auxiliary information is  provided in the Appendix. Also, illustration of the finite sample performance of the proposed testing procedure is provided in the simulation study undertaken in this Appendix.



\section{Preliminaries}\label{sec1}

In what follows we denote by $\mathcal{L}^{2}_{\mathbb{H}}(\Omega,\mathcal{A},P)$ the space of zero--mean $\mathbb{H}$--valued random variables $X$ on the basic probability space $(\Omega,\mathcal{A},P)$  with $E\|X\|_{\mathbb{H}}^{2}<\infty.$
Let $Y=\{Y_{t},\ t\in \mathbb{Z}\}$  be a zero--mean  AR$\mathbb{H}$(1)  process  on the basic probability space $(\Omega,\mathcal{A},P)$  satisfying
\begin{equation}Y_{t}=\Gamma (Y_{t-1})+\varepsilon_{t},\quad t\in \mathbb{Z},\label{eq1}\end{equation}
\noindent where $\Gamma \in \mathcal{L}(\mathbb{H})$ denotes the autocorrelation operator of AR$\mathbb{H}$(1)  process $Y,$ with $\mathcal{L}(\mathbb{H})$  being the space of linear bounded operators on $\mathbb{H}.$ Along the paper,
we assume that  the conditions formulated   in Chapter 3 in  \cite{Bosq2000} hold, ensuring  the existence of a unique stationary solution to equation (\ref{eq1}).
Hence,  $C_{0}^{Y}=\mathbb{E}[Y_{0}\otimes Y_{0}]=\mathbb{E}[Y_{t}\otimes Y_{t}],$
$t\in \mathbb{Z},$  denotes  the autocovariance operator of $Y,$ and
$C_{1}^{Y}=\mathbb{E}\left[Y_{0}\otimes Y_{1}\right]=  \mathbb{E}\left[Y_{t}\otimes Y_{t+1}\right],$ $t\in \mathbb{Z},$  its cross--covariance operator.

The  $\mathbb{H}$--valued innovation process $\varepsilon$   is   assumed to be $\mathbb{H}$--valued Strong White Noise ($\mathbb{H}$--SWN) (see Definition 3.1 in \cite{Bosq2000}). That is, $\varepsilon=\{\varepsilon_{t},\ t\in \mathbb{Z}\}$ is a sequence of independent and identically distributed zero--mean  $\mathbb{H}$--valued random variables with $C_{0}^{\varepsilon}:=\mathbb{E}\left[\varepsilon_{t} \otimes \varepsilon_{t}\right]=\mathbb{E}\left[\varepsilon_{0} \otimes \varepsilon_{0}\right],$ for all $t\in \mathbb{Z},$ and functional variance
$\mathbb{E}[\|\varepsilon_{t}\|^{2}_{\mathbb{H}}]=\mathbb{E}[\|\varepsilon_{0}\|^{2}_{
	\mathbb{H}}]=\|C_{0}^{\varepsilon}\|_{L^{1}(\mathbb{H})}=
	\sigma^{2}_{\varepsilon},$ that coincides with the trace norm $\|C_{0}^{\varepsilon}\|_{L^{1}(\mathbb{H})}$ of the autocovariance operator $C_{0}^{\varepsilon}$ of $\varepsilon.$ We restrict our attention to AR$\mathbb{H}$(1) processes    satisfying
the following condition:

\medskip

\noindent \textbf{Assumption A1}.  $Y$ is a  strictly stationary  AR$\mathbb{H}$(1) process  with $\mathbb{H}$--SWN innovation $\varepsilon $ such that $\mathbb{E}\|\varepsilon_{1}\|^{4}_{\mathbb{H}}<\infty.$


\medskip


Under model (\ref{eq1}), keeping in mind  \textbf{Assumption A1}, the following orthogonality condition \begin{equation}\mathbb{E}\left[(Y_{i}-\Gamma (Y_{i-1}))/Y_{i-1}\right]=\mathbb{E}[\varepsilon_{1}/Y_{0}]\underset{\mbox{a.s.}}{=}0\label{oc}\end{equation} \noindent holds  for $i\geq 1,$ with $\underset{\mbox{a.s.}}{=}$   denoting  the almost surely (a.s.)  equality.

In what follows, we will use the notation $E^{i}(x)=\{\omega \in \Omega ; \ \left\langle Y_{i}(\omega ),\phi_{j}\right\rangle_{\mathbb{H}}\leq \left\langle  x,\phi_{j}\right\rangle_{\mathbb{H}},\ j\geq 1\},$ and $\ind{E^{i}(x)}$ for its indicator function,  $x\in \mathbb{H},$ and $i\geq 0.$

Let us consider a centered non--degenerated Gaussian probability measure $\mu $ on $\mathbb{H}.$ From Theorem 4.1 in  \cite{Cuestalbertos}, under  \textbf{Assumption A1},    if    $Y_{0}$ satisfies Carleman condition, a class in $\mathcal{B}(\mathbb{H})$ of Borel sets of $\mathbb{H}$ defines a separating class of $\mathbb{H}$ if it contains a set  of  positive $\mu$--measure. Thus, Lemma 1(d) in   \cite{Escanciano06} can be reformulated in an infinite--dimensional framework by considering, for each $x\in \mathbb{H},$ and for a given orthonormal basis $\{ \phi_{j},\ j\geq 1\}$ of $\mathbb{H},$
$$B_{x}=\{h\in \mathbb{H};\ \left\langle h,\phi_{j}\right\rangle_{\mathbb{H}}=h(\phi_{j})\leq \left\langle x,\phi_{j}\right\rangle_{\mathbb{H}}=x(\phi_{j}),\ \forall j\geq 1 \},$$
\noindent leading to the definition of the separating class  of sets $\{B_{x},\ x\in \mathbb{H}\}$ (see Theorem 1.2.1 in \cite{DaPrato2002}).
Hence,  under  \textbf{Assumption A1}, one can equivalently express the orthogonality condition (\ref{oc}) as
\begin{eqnarray}&&\mathbb{E}\left[\varepsilon_{1}\ind{\{ \omega \in \Omega;\ \left\langle Y_{0}(\omega ),\phi_{j}\right\rangle_{\mathbb{H}}\leq \left\langle  x,\phi_{j}\right\rangle_{\mathbb{H}},\ j\geq 1\}}\right]=\mathbb{E}\left[\varepsilon_{1}\ind{E^{0}(x)}\right]
	=0, \ \mbox{a.e.} \  x \in  \mathbb{H},\nonumber\\ \label{occ2}
\end{eqnarray}
\noindent for a given orthonormal basis $\left\{ \phi_{j},\ j\geq 1\right\}$ of $\mathbb{H}.$ It is straightforward that  the sequence $\{X_{i}(x),\ i\geq 1\}=\left\{\varepsilon_{i}\ind{E^{i-1}(x)}\right\}$ is an $\mathbb{H}$--valued martingale difference  with respect to  the filtration  $\mathcal{M}_{0}^{Y}\subset \mathcal{M}_{1}^{Y}\dots \subset \mathcal{M}_{n}^{Y}\subset \dots,$  where   $\mathcal{M}_{i-1}^{Y}=\sigma (Y_{t},\ t\leq i-1),$
for $ i\geq 1,$ as given in the following lemma:

\begin{lemma}
	\label{lem1a} For each $x\in \mathbb{H},$ the sequence
	$$\{X_{i}(x),\ i\geq 1\}=\{(Y_{i}-\Gamma (Y_{i-1}))\ind{E^{(i-1)}(x)},\ i\geq 1\}$$  \noindent  is an $\mathbb{H}$--valued martingale difference  with respect to  the filtration  $\mathcal{M}_{0}^{Y}\subset \mathcal{M}_{1}^{Y}\dots \subset \mathcal{M}_{n}^{Y}\subset \dots,$  where   $\mathcal{M}_{i-1}^{Y}=\sigma (Y_{t},\ t\leq i-1),$
	for $ i\geq 1.$
	
\end{lemma}
The proof is straightforward from  $\varepsilon_{n}= Y_{n}-\Pi^{\mathcal{M}_{n-1}}( Y_{n}),$ for every $n\geq 1,$ with
$\Pi^{\mathcal{M}_{n-1}}$ being the orthogonal projector into $\mathcal{M}_{n-1}$  (see equation (2.55), and  pp. 72--73 in  \cite{Bosq2000}).


  Along the paper, from (\ref{eq1}), the following identity will be applied several times  (see equation (3.11) in \cite{Bosq2000}), for every $i\geq 1,$
\begin{eqnarray}
	Y_{i-1}=\sum_{t=0}^{i-2}\Gamma^{t}(\varepsilon_{i-1-t})+\Gamma^{i-1}(Y_{0}).
	\label{eq311b2000}
\end{eqnarray}
\subsection{Testing the autocorrelation model}
\label{test}
We first address the functional parametric testing  problem
\begin{eqnarray}H_{0}:\Gamma &=&\Gamma_{0}\nonumber\\
	H_{1}:\Gamma &\neq &\Gamma_{0}\nonumber
\end{eqnarray}
\noindent where  $\Gamma_{0}$ is a known operator. A Kolmogorov–Smirnov (K–S) test based on
the following  Hilbert--valued   marked empirical process indexed by  $x\in \mathbb{H}$ is adopted
\begin{eqnarray}V_{n}(x)&=&\frac{1}{\sqrt{n}}\sum_{i=1}^{n}X_{i}(x)=
	\frac{1}{\sqrt{n}}\sum_{i=1}^{n}(Y_{i}-\Gamma (Y_{i-1}))\ind{E^{i-1}(x)},\label{efp}
\end{eqnarray}
\noindent where $\{\phi_{j},\ j\geq 1\}$ is an orthonormal basis of the separable Hilbert space $\mathbb{H},$ and  $Y=\{Y_{i},\ i\in \mathbb{Z} \}$ denotes the autoregressive process introduced in equation (\ref{eq1}). As before, for each $x\in \mathbb{H},$ $X_{i}(x)=(Y_{i}-\Gamma (Y_{i-1}))\ind{E^{i-1}(x)}=\varepsilon_{i}\ind{E^{i-1}(x)},$ $i\geq 1.$


From Theorem \ref{th3} below (see Section \ref{s3}), $\left\{V_{n}(x),\ x\in \mathbb{H}\right\}$ weak converges  to an  $\mathbb{H}$--valued  Gaussian process indexed by $x\in \mathbb{H},$ $\left\{ W_{\infty }(x),\ x\in \mathbb{H}\right\},$
with covariance operator $C_{\min(x,y)}^{W_{\infty}}=C_{0}^{\varepsilon}P\left[E^{0}(\min(x,y))\right],$ for every $x,y\in \mathbb{H},$ where $C_{0}^{\varepsilon}=\mathbb{E}[\varepsilon_{t} \otimes \varepsilon_{t}],$ for  $t\in \mathbb{Z},$ and
 \begin{eqnarray}&& P\left[E^{0}(\min(x,y))\right]\nonumber\\ &&=P\left( \omega \in \Omega ; \ \left\langle Y_{0}(\omega ),\phi_{j}\right\rangle_{\mathbb{H}}\leq \min\left(\left\langle  x,\phi_{j}\right\rangle_{\mathbb{H}},\left\langle  y,\phi_{j}\right\rangle_{\mathbb{H}}\right),\ j\geq 1\right).\end{eqnarray}
 \noindent Thus,  $E^{0}(\min(x,y))=\{\omega \in \Omega ; \ \left\langle Y_{0}(\omega ),\phi_{j}\right\rangle_{\mathbb{H}}\leq \min\left(\left\langle  x,\phi_{j}\right\rangle_{\mathbb{H}},\left\langle  y,\phi_{j}\right\rangle_{\mathbb{H}}\right),\ j\geq 1\}.$   Here,
 $\min(x,y)$  denotes the infinite--dimensional vector
\begin{equation}\min(x,y)=\left(\min\left(\left\langle  x,\phi_{j}\right\rangle_{\mathbb{H}},\left\langle  y,\phi_{j}\right\rangle_{\mathbb{H}}\right),\  j\geq 1\right).\label{eqmin}
\end{equation}
 Then, $\left\{ W_{\infty }(x),\ x\in \mathbb{H}\right\}$ can be identified in distribution sense with time--changed $\mathbb{H}$--valued Wiener process  with subordinator $\left\{P_{Y_{0}}(x),\ x\in \mathbb{H}\right\}$  (see Remark \ref{limitproc} below).



In practice, applying random projection methodology, assuming  $Y_{0}$ satisfies Carleman condition (see Theorem 4.1 in  \cite{Cuestalbertos}), the null hypothesis will be rejected if conditionally to the observation of an  $\mathbf{h}\in \mathbb{H}$ value of a functional non--degenerated Gaussian random variable, the test statistic
\begin{equation}\mathcal{T}(\mathbf{h})= \sup_{t\in [0,1]} \left|s_{n}^{-1}(\mathbf{h})\left\langle V_{n}(P_{Y_{0}}^{-1}(t)), \mathbf{h}\right\rangle_{\mathbb{H}}\right|,\label{statistic}
\end{equation}
\noindent exceeds a  critical value of the probability distribution of the supremum norm of   Brownian motion $W_{t}$ on the interval $[0,1],$  which is computed
by Reflection Principle from the standard normal probability distribution $\Phi$ as $P\left[\sup_{t\in [0,1]}W_{t}\leq a\right]=2\Phi(a)-1.$
In equation (\ref{statistic}),
\begin{eqnarray}&&
	s_{n}^{-1}(\mathbf{h})=\left(\frac{1}{n}\sum_{i=1}^{n}\left[\left\langle (Y_{i}-\Gamma (Y_{i-1})),\mathbf{h}\right\rangle_{\mathbb{H}} \right]^{2} \right)^{-1/2}\nonumber\\
	&&= \left(\frac{1}{n}\sum_{i=1}^{n}[\varepsilon_{i}(\mathbf{h})]^{2}\right)^{-1/2},
	\label{rpci}
\end{eqnarray}

\noindent and $P_{Y_{0}}$  denotes the infinite--dimensional marginal probability  measure induced by $Y_{0},$  with
\begin{equation}P_{Y_{0}}^{-1}(t):= x_{0}\in \mathbb{H} \ \mbox{such that} \ \left\langle x_{0}, \mathbf{h}\right\rangle_{\mathbb{H}}\leq \left\langle x,  \mathbf{h}\right\rangle_{\mathbb{H}},\ \mbox{if} \  P_{Y_{0}}(x)\geq t, \ t\in [0,1].\label{eqinf}
\end{equation}



An alternative subordinator scenario is discussed in Section \ref{subsuppm} of the Appendix.

In the computation of the second--order moments of $\mathbb{H}$--valued martingale difference sequence $\{X_{i}(x),\ i\geq 1\},$ $x\in \mathbb{H}$ (see Section \ref{sop} of the Appendix),
the following assumption  will  be considered:


\medskip

\noindent \textbf{Assumption A2}.  Assume  that $Y_{0}$ is independent of $ \varepsilon_{i},$ for all  $i\geq 1.$






\section{Asymptotic properties for simple hypothesis ($\Gamma=\Gamma_0$)}

The asymptotic  distribution of the generalized $\mathbb{H}$--valued  empirical process  $\left\{V_{n}(x),\ x\in \mathbb{H}\right\}$
in equation  (\ref{efp}) when $\Gamma=\Gamma_0$ (simple hypothesis) is analyzed in this section.   Specifically,  Theorem \ref{th1} in Section \ref{s2} provides the convergence in distribution  $V_{n}(x)\to_{D}V_{\infty }(x),$  as $n\to \infty,$ where, for each $x\in \mathbb{H},$   $V_{\infty }(x)\sim \mathcal{N}(0,C_{0}^{\varepsilon}P(E^{0}(x))).$ Here, $\mathcal{N}(0,C_{0}^{\varepsilon}P(E^{0}(x)))$ denotes the zero--mean Gaussian distribution on $\mathbb{H}$ with autocovariance operator $C_{0}^{\varepsilon}P(E^{0}(x)),$  $x\in \mathbb{H}.$
Theorem \ref{th3} in Section \ref{s3} then obtains  the weak convergence of  $\left\{V_{n}(x),\ x\in \mathbb{H}\right\}$   to a generalized $\mathbb{H}$--valued  Gaussian process $\left\{ W_{\infty }(x),\ x\in \mathbb{H}\right\},$    identified in distribution sense with time--changed $\mathbb{H}$--valued  Wiener process $\left\{W_{C_{0}^{\varepsilon}}\circ P_{Y_{0}}(x),\ x\in \mathbb{H}\right\}$ with subordinator  $\left\{ P_{Y_{0}}(x),\ x\in \mathbb{H}\right\},$ having  covariance operator \linebreak $\mathbb{E}\left[W_{C_{0}^{\varepsilon}}\circ P_{Y_{0}}(x)\otimes W_{C_{0}^{\varepsilon}}\circ P_{Y_{0}}(y)\right]=P\left[E^{0}(\min(x,y))\right]C_{0}^{\varepsilon}.$


\subsection{Central Limit Theorem}
\label{s2}

Theorem 2.16 in \cite{Bosq2000}  is now applied  in the derivation of  Theorem \ref{th1}, providing the Gaussian  limit distribution of the marginals of the generalized $\mathbb{H}$--valued  marked empirical process  $\{ V_{n}(x),\ x\in \mathbb{H}\}$ in (\ref{efp}).
\begin{theorem}\label{th1} Let $\{Y_{t},\ t\in \mathbb{Z}\}$ be a   centered AR$\mathbb{H}$(1) process.
	Under \textbf{Assumptions A1--A2},
	for each $x\in \mathbb{H},$  the generalized $\mathbb{H}$--valued  empirical process (\ref{efp})
	satisfies $V_{n}(x)\to_{D}V_{\infty }(x)\sim\mathcal{N}(0,C_{x}),$ as $n\to \infty,$ with $\to_{D}$ denoting the convergence in distribution.  Here, for each $x\in \mathbb{H},$ the autocovariance operator $C_{x}=
	C_{0}^{\varepsilon}P(E^{0}(x)).$
	
	If, for every $j\geq 1,$  $\left\langle  x,\phi_{j}\right\rangle_{\mathbb{H}}\to \infty,$
	$$V_{n}(x)\to_{D}\mathcal{Z}\sim
	\mathcal{N}(0,C_{0}^{\varepsilon}),\quad n\to \infty.$$
	
\end{theorem}
\begin{proof}
The proof is based on the verification of the conditions assumed  in the CLT for Hilbert--valued martingale difference sequences given  in Theorem 2.16 in \cite{Bosq2000}. Let us first verify  condition (2.59) in Theorem 2.16.
For every   $i\geq 1,$ $n\geq 1,$   and  $x\in \mathbb{H},$
the following events  are considered:
\begin{eqnarray}
	A(x) &=&\left\{ \omega \in \Omega; \ \max_{1\leq i\leq n}\|X_{i}(x,\omega )\|_{\mathbb{H}}>\sqrt{n}\eta \right\}\in \mathcal{A}\nonumber\\
	B_{i}(x)& =&\left\{ \omega \in \Omega; \ \|X_{i}(x,\omega )\|_{\mathbb{H}}>\sqrt{n}\eta \right\}\in \mathcal{A},\ i=1,\dots,n\nonumber\\
	B_{n}(x)&=&\bigcup_{i=1}^{n}\left\{ \omega \in \Omega; \ \|X_{i}(x,\omega )\|_{\mathbb{H}}>\sqrt{n}\eta \right\}\in \mathcal{A}.
	\label{eqevents}
\end{eqnarray}
\noindent Clearly, $A(x)\subset B_{n}(x),$  and for every $x\in \mathbb{H},$  \begin{eqnarray}
	&&
	P(A(x))=P\left( \omega \in \Omega; \ \max_{1\leq i\leq n}\|X_{i}(x,\omega)\|_{\mathbb{H}}>\sqrt{n}\eta \right)
	\leq P\left( B_{n}(x)\right)\nonumber\\
	&&
	\leq \sum_{i=1}^{n}P\left( B_{i}(x)\right)=\sum_{i=1}^{n}P\left(  \omega \in \Omega; \ \|X_{i}(x,\omega)\|_{\mathbb{H}}>\sqrt{n}\eta \right).\label{eqevents2}\end{eqnarray}
From equation  (\ref{eqevents2}),   applying Chebyshev inequality and stationarity, keeping in mind that the events $D_{i}^{(1)}(x)=\{ \omega \in \Omega; \ \|X_{i}(x,\omega)\|_{\mathbb{H}}>\sqrt{n}\eta \}$ and $D_{i}^{(2)}(x)=\{ \omega \in \Omega; \ \|X_{i}(x,\omega)\|_{\mathbb{H}}\ind{\|X_{i}(x,\omega)\|_{\mathbb{H}}>\sqrt{n}\eta}>\sqrt{n}\eta \}$ coincide for $i=1,\dots,n,$   and $x\in \mathbb{H},$ we obtain
\begin{eqnarray}&& P\left( \omega \in \Omega; \ \max_{1\leq i\leq n}\|X_{i}(x,\omega)\|_{\mathbb{H}}>\sqrt{n}\eta \right)\nonumber\\ &&\leq \frac{1}{n\eta^{2}}\sum_{i=1}^{n}\Esp{\|X_{i}(x)\|_{\mathbb{H}}^{2}\ind{\|X_{i}(x)\|_{\mathbb{H}}>\sqrt{n}\eta}}=
	\frac{1}{n\eta^{2}}\sum_{i=1}^{n}\Esp{\|X_{i}(x)\|_{\mathbb{H}}^{2}\ind{B_{i}(x)}}
	\nonumber\\ &&=\frac{1}{n\eta^{2}}\sum_{i=1}^{n}\Esp{\|X_{1}(x)\|_{H}^{2}\ind{B_{1}(x)}}=\frac{1}{\eta^{2}}
	\Esp{\|X_{1}(x)\|_{\mathbb{H}}^{2}\ind{B_{1}(x)}}.
	\label{eqeventa}
\end{eqnarray}

Dominated Convergence Theorem yields to
\begin{equation}
	\lim_{n\to \infty}P\left[ \max_{1\leq i\leq n}\left\|X_{i}(x)\right\|_{\mathbb{H}}>\sqrt{n}\eta  \right]=0,\quad \forall x\in \mathbb{H}.
	\label{mcth}
\end{equation}

On the other hand, for any $x\in \mathbb{H},$ and  $n\geq 1,$ applying SWN property of the innovation process $\varepsilon,$ under \textbf{Assumptions A1--A2}, from equation (\ref{eq311b2000}),
\begin{eqnarray}
	&&\Esp{\max_{1\leq i\leq n}\left\|\frac{X_{i}(x)}{\sqrt{n}}\right\|_{\mathbb{H}}^{2}}\nonumber\\ &&\leq  \sum_{i=1}^{n}\Esp{\frac{\|Y_{i}-\Gamma (Y_{i-1})\|_{\mathbb{H}}^{2}\ind{E^{(i-1)}(x)}^{2}}{n}}\nonumber\\
	&&=\frac{1}{n}\sum_{i=1}^{n}\Esp{\ind{E^{(i-1)}(x)}\Esp{\|\varepsilon_{i}\|_{H}^{2}|Y_{i-1}}}\nonumber\\
	&& \leq \frac{1}{n}\sum_{i=1}^{n}\Esp{\Esp{\|\varepsilon_{i}\|_{\mathbb{H}}^{2}/Y_{i-1}}} = \Esp{\|\varepsilon_{1}\|_{\mathbb{H}}^{2}}=\|C_{0}^{\varepsilon}\|_{L^{1}(\mathbb{H})}<\infty.
	\label{eqaddfc}
\end{eqnarray}
\noindent Thus, $\left\{\max_{1\leq i\leq n}\left\|\frac{X_{i}(x)}{\sqrt{n}}\right\|_{\mathbb{H}}^{2},\ n\geq 1\right\}$ is uniformly integrable, and the desired result holds.

Let us now prove that  condition  (2.61) in Theorem 2.16 in \cite{Bosq2000} holds.
Specifically, considering again \textbf{Assumptions A1--A2}, from equation (\ref{eq311b2000}), and  from the SWN property of $\varepsilon,$ applying Markov Theorem, for any $n\geq 1,$
\begin{eqnarray}&&P\lrc{\sum_{i=1}^{n}r_{N}^{2}\lrp{\frac{X_{i}}{\sqrt{n}}}>\eta}=P\lrc{\sum_{i=1}^{n}\sum_{l=N}^{\infty}
		\left\langle \frac{X_{i}}{\sqrt{n}},\phi_{l}\right\rangle^{2}_{\mathbb{H}}>\eta}\nonumber\\
	&&\leq \sum_{i=1}^{n}P\lrc{r_{N}^{2}\lrp{\frac{X_{i}}{\sqrt{n}}}>\eta}\leq \frac{1}{\eta}\sum_{i=1}^{n}\Esp{r_{N}^{2}\lrp{\frac{X_{i}}{\sqrt{n}}}}\nonumber\\
	&&\leq
	\frac{1}{n\eta}\sum_{i=1}^{n}\Esp{r_{N}^{2}\lrp{\varepsilon_{i}}}=\frac{1}{\eta}\Esp{r_{N}^{2}\lrp{\varepsilon_{1}}}=\frac{1}{\eta}\sum_{l=N}^{\infty}\lambda_{l}(C^{\varepsilon}_{0}).
	\label{sc}\end{eqnarray}
From (\ref{sc}),  in view of the trace property of operator $C_{0}^{\varepsilon},$ $$\lim_{N\to \infty} \lim_{n\to \infty}
P\left[\sum_{i=1}^{n}r_{N}^{2}\left(\frac{X_{i}}{\sqrt{n}}\right)>\eta\right]\leq  \lim_{N\to \infty} \frac{1}{\eta}\sum_{l=N}^{\infty}\lambda_{l}(C^{\varepsilon}_{0})  =0,$$
\noindent as we wanted to prove.


Finally, we prove  condition (2.60)  in Theorem 2.16 in \cite{Bosq2000} holds.
Specifically, we prove that  condition (2.36) in  Corollary 2.3 in \cite{Bosq2000} holds.
Under \textbf{Assumptions A1--A2}, considering, as before,
\begin{eqnarray}
	C_{0}^{X_{i}(x)}&:=&\Esp{ X_{i}(x)\otimes X_{i}(x)}=\Esp{\varepsilon_{i}\otimes \varepsilon_{i}\ind{E^{(i-1)}(x)}^{2}}\nonumber\\
	&=&\Esp{ \ind{E^{(i-1)}(x)}\Esp{\varepsilon_{i}\otimes \varepsilon_{i}|Y_{i-1}}}
	=\Esp{\varepsilon_{i}\otimes \varepsilon_{i}}
	P\lrp{E^{(i-1)}(x)}\nonumber \\
	&=& C_{0}^{\varepsilon}P(E^{(i-1)}(x))= C_{0}^{\varepsilon}P(E^{0}(x))=C_{0}^{X_{1}(x)},\quad \forall  i\geq 1,
	\label{eqmp2b}
\end{eqnarray}
\noindent   	with
\begin{eqnarray}
	&&P(E^{(i-1)}(x))=P\left[\omega \in \Omega ; \ \left\langle Y_{i-1}(\omega ),\phi_{j}\right\rangle_{\mathbb{H}}\leq \left\langle  x,\phi_{j}\right\rangle_{\mathbb{H}},\ j\geq 1\right]\label{stt}\\
	&&=P\left[\omega \in \Omega ; \ \left\langle Y_{0}(\omega ),\phi_{j}\right\rangle_{\mathbb{H}}\leq \left\langle  x,\phi_{j}\right\rangle_{\mathbb{H}},\ j\geq 1\right]=P(E^{0}(x)),\ \ i\geq 1,\nonumber
\end{eqnarray}
\noindent  we have
$$\Esp{X_{n+i}(x)\otimes X_{n+i}(x)}=C_{0}^{\varepsilon}P\left( E^{(n+i-1)}(x)\right)=C_{0}^{\varepsilon}P\left( E^{0}(x)\right),\quad x\in \mathbb{H}.$$
Let us now denote
$W_{i}(x)=X_{n+i}(x)\otimes X_{n+i}(x)-C_{0}^{\varepsilon}P\left( E^{0}(x)\right),$ for $i=0,\dots,p-1,$ with, as before, for every $x\in \mathbb{H},$
\begin{equation*}
	X_{n+i}(x)=\lrc{Y_{n+i}-\Gamma (Y_{n+i-1})}\ind{E^{(n+i-1)}(x)}
	=\varepsilon_{n+i}\ind{E^{(n+i-1)}(x)},\ i=0,\ldots,p-1.
\end{equation*}
Under  \textbf{Assumptions A1--A2}, applying strictly stationarity, and that  $\varepsilon $ is strong white noise
\vspace{-12pt}
\begin{eqnarray}
	&&\Esp{\left\|W_{0}(x)+\dots +W_{p-1}(x)\right\|^{2}_{\mathcal{S}(\mathbb{H})}}  \nonumber\\
	&& =\sum_{i,k=0}^{p-1}\Esp{ \left\langle W_{i}(x),W_{k}(x)\right\rangle_{\mathcal{S}(\mathbb{H})}} \nonumber\\
	&& =\sum_{i,k=0}^{p-1}\Esp{ \left\langle X_{n+i}(x)\otimes X_{n+i}(x),X_{n+k}(x)\otimes X_{n+k}(x)\right\rangle_{\mathcal{S}(\mathbb{H})}}\nonumber \\
	&& -\sum_{i,k=0}^{p-1}\lrc{P\lrp{ E^{0}(x)}}^{2}\left\langle C_{0}^{\varepsilon}, C_{0}^{\varepsilon}\right\rangle_{\mathcal{S}(\mathbb{H})} -\sum_{i,k=0}^{p-1}\lrc{P\lrp{ E^{0}(x)}}^{2}\left\langle C_{0}^{\varepsilon}, C_{0}^{\varepsilon}\right\rangle_{\mathcal{S}(\mathbb{H})} \nonumber \\
	&& +\sum_{i,k=0}^{p-1}[P\left( E^{0}(x)\right)]^{2}\left\langle C_{0}^{\varepsilon}, C_{0}^{\varepsilon}\right\rangle_{\mathcal{S}(\mathbb{H})} \nonumber \\
	&& =\sum_{i,k=0}^{p-1}\Esp{  \left\langle X_{n+i}(x)\otimes X_{n+i}(x),X_{n+k}(x)\otimes X_{n+k}(x)\right\rangle_{\mathcal{S}(\mathbb{H})}}\nonumber \\
	&& -\sum_{i,k=0}^{p-1}\lrc{P\lrp{ E^{0}(x)}}^{2}\|C_{0}^{\varepsilon}\|_{\mathcal{S}(\mathbb{H})}^{2}\leq  \sum_{i,k=0}^{p-1}\Esp{\inprod{X_{n+i}(x)}{X_{n+k}(x)}^{2}}
	\nonumber  \\ 	
	&&= \sum_{\substack{i,k=0 \\ i\neq k}}^{p-1} \Esp{  \inprod{X_{n+i}(x)}{X_{n+k}(x)}^{2}}+p\lrc{\Esp{\|\varepsilon_{1}\|^{4}_{\mathbb{H}}}}P(E^{0}(x)) \nonumber \\
	&& = p\lrc{\sum_{\substack{u=-(p-1) \\ u\neq 0}}^{p-1} \lrp{1-\frac{|u|}{p}}\Esp{\ind{E^{0}(x)}^{2}\ind{E^{u}(x)}^{2} \inprod{\varepsilon_{1}}{\varepsilon_{u+1}}^{2}}}\nonumber\\
	&&+p\Esp{\|\varepsilon_{1}\|^{4}_{\mathbb{H}}}P(E^{0}(x))
	\nonumber \end{eqnarray}\begin{eqnarray}
	&&\hspace*{-0.7cm} = p\lrc{\sum_{\substack{u=-(p-1) \\ u\neq 0}}^{p-1}\lrp{1-\frac{|u|}{p}}\Esp{\ind{E^{0}(x)}\ind{E^{u}(x)}
			\sum_{k,l\geq 1}\varepsilon_{1}(\phi_{k})\varepsilon_{1}(\phi_{l})\varepsilon_{u+1}(\phi_{k})
			\varepsilon_{u+1}(\phi_{l})}}\nonumber \\
	&& +p\Esp{\|\varepsilon_{1}\|^{4}_{\mathbb{H}}}P(E^{0}(x)) \leq p\left[\left\|C_{0}^{\varepsilon}\right\|_{\mathcal{S}(\mathbb{H})}^{2}+\Esp{\|\varepsilon_{1}
		\|^{4}_{\mathbb{H}}}\right]<\infty.	 \end{eqnarray}

Note that under \textbf{Assumption A1}, $\Esp{\|\varepsilon_{1}\|^{4}_{\mathbb{H}}}<\infty,$ and, since
$C_{0}^{\varepsilon}$ is a trace operator, its Hilbert--Schmidt operator norm $\left\|C_{0}^{\varepsilon}\right\|_{\mathcal{S}(\mathbb{H})}$ is finite.
Thus,  we can apply Corollary 2.3 in  \cite{Bosq2000} with $\gamma =1,$ and,  for all $\beta >1/2,$ and for each  $x\in \mathbb{H},$
$$\frac{n^{1/4}}{(\log(n))^{\beta }}\left\|\frac{S_{n}^{W(x)}}{n}\right\|_{\mathcal{S}(\mathbb{H})}=\frac{n^{1/4}}{(\log(n))^{\beta }}\left\|\sum_{i=1}^{n}\frac{ W_{i}(x)}{n}\right\|_{\mathcal{S}(\mathbb{H})}\to_{\textrm{a.s.}} 0,\ n\to \infty,$$
\noindent leading to condition (2.60)   in Theorem 2.16 in \cite{Bosq2000}, as we wanted to prove. Thus, the weak convergence to the indicated  $\mathbb{H}$--valued Gaussian random variable holds.


\end{proof}

\subsection{Functional Central Limit Theorem}
\label{s3}

This section provides the convergence in law of the generalized  (indexed  by $\mathbb{H}$) functional  empirical process (\ref{efp}) to a generalized  $\mathbb{H}$-valued     Gaussian process.
Continuous Mapping Theorem, and  Theorem 4.1 in  \cite{Cuestalbertos} will then  be considered  to formulate  the distribution free test (\ref{statistic}) (see Section \ref{test}).

The following lemma (see Theorem 2 in  \cite{Walk77}), providing
an invariance principle based on Robbins-Monro procedure, will be applied in the proof of Theorem \ref{th3}  below.
\begin{lemma}
	\label{th2}
	Let $\{X_{n},\ n\in \mathbb{N}\}$ be a martingale difference sequence of $\mathbb{H}$--valued random variables,  with respect to  the filtration  $\mathcal{M}_{0}^{Y}\subset \mathcal{M}_{1}^{Y}\dots \subset \mathcal{M}_{n}^{Y}\subset \dots,$
	satisfying $\Esp{\|X_{n}\|_{\mathbb{H}}^{2}}<\infty.$  Let $S:\mathbb{H}\to \mathbb{H}$ be a trace operator. For each $n\in \mathbb{N},$ denote as $S^{n}$ the autocovariance operator of $X_{n},$ given $Y_{1},\dots,Y_{n-1}.$  That is,
	$$S^{n}=\Esp{X_{n}\otimes X_{n}|Y_{1},\dots,Y_{n-1}}, \quad n\in \mathbb{N}.$$
	
	Assume that
	\begin{itemize}
		\item[(i)] $\Esp{\left\|\frac{1}{n}\sum_{j=1}^{n}S^{j}-S\right\|_{L^{1}(\mathbb{H})}}\to 0,$ $n\to \infty.$
		\item[(ii)] $\frac{1}{n}\sum_{j=1}^{n}\Esp{\|X_{j}\|^{2}_{\mathbb{H}}}\to \mathrm{trace}(S),$ $n\to \infty.$
		\item[(iii)] For $r>0,$  $$\frac{1}{n}\sum_{j=1}^{n}\Esp{ \|X_{j}\|_{\mathbb{H}}^{2}\chi(\|X_{j}\|^{2}_{\mathbb{H}}\geq rj)|Y_{1},\dots, Y_{j-1}}\to_{P} 0,\ n\to \infty.$$
	\end{itemize}
	
	Then, the sequence of random elements $\{Z_{n}\}$  in $C_{\mathbb{H}}([0,1])$ with the supremum norm, which are defined by
	\begin{equation}
		Z_{n}(t)=\frac{1}{\sqrt{n}}\sum_{j=1}^{[nt]}X_{j}+\left(nt-[nt]\right)\frac{1}{\sqrt{n}}X_{[nt]+1},\quad t\in [0,1],
		\label{eqrech01}
	\end{equation}
	\noindent converges in  distribution to a Brownian motion $W$ in $\mathbb{H},$  with $W(0)=0,$ a.s.,  $\Esp{W(1)}=0,$ and covariance operator $S$ of $W(1).$
\end{lemma}


\noindent The  following additional condition is required in the derivation of the next result.

\medskip

\noindent \textbf{Assumption A3}. $\sup_{j\in \mathbb{Z}; \ \omega \in \Omega } \|\varepsilon_{j,\omega}\|_{\mathbb{H}}^{2}<\infty,$ where
$\varepsilon_{j,\omega}$ denotes  the sample  functional value in $\mathbb{H}$ corresponding to $\omega \in \Omega $ of   $\varepsilon $ at time $j,$ $j\in \mathbb{Z}.$

\medskip

In what follows $C_{\mathbb{H}}([0,1])$ will denote   the separable  Banach space  of $\mathbb{H}$--valued continuous functions on $[0,1],$ with respect to the $\mathbb{H}$ norm,  under the supremum norm $\|g\|_{\infty}=\sup_{t\in [0,1]}\|g(t)\|_{\mathbb{H}}.$



	

\begin{theorem}
	\label{th3}
	Under \textbf{Assumptions A1--A3},
		   process  $\left\{V_{n}(x),\ x\in \mathbb{H}\right\}$ in  (\ref{efp}) weak converges, as $n\to \infty,$ to a  generalized $\mathbb{H}$--valued  Gaussian process \linebreak $\left\{ W_{\infty} (x),\ x\in \mathbb{H}\right\}$ with covariance operator
	\begin{equation}C_{\min(x,y)}^{W_{\infty}}
		=P[E^{0}(\min(x,y))]C_{0}^{\varepsilon},\quad \forall x,y\in \mathbb{H},
		\label{covop}
		\end{equation}
	 \noindent where $\min(x,y)$ has been introduced in equation (\ref{eqmin}).
\end{theorem}
\begin{remark}\label{limitproc}  The limiting process $\{W_{\infty}(x),\ x\in \mathbb{H}\}$ can be identified in probability distribution sense with  time--changed  $\mathbb{H}$--valued  Brownian  motion $\left\{W_{C_{0}^{\varepsilon}}\circ P_{Y_{0}}(x),\ x\in \mathbb{H}
	\right\}$  with subordinator $\left\{P_{Y_{0}}(x),\ x\in \mathbb{H}\right\},$
 since, from equation (\ref{covop}),
	$C_{\min(x,y)}^{W_{\infty}}=
	\mathbb{E}\left[W_{C_{0}^{\varepsilon}}\circ P_{Y_{0}}(x)\otimes W_{C_{0}^{\varepsilon}}\circ P_{Y_{0}}(y)\right],$ for every  $x,y \in \mathbb{H}.$
	Here,
	process $W_{C_{0}^{\varepsilon}}=\left\{ W_{C_{0}^{\varepsilon}}(t),\  t\in [0,1]\right\}$ is an infinite--dimensional ($\mathbb{H}$--valued) Wiener process on the interval  $[0,1]$ in the sense introduced in Definition 2 of \cite{Dedecker}. In particular, $W_{C_{0}^{\varepsilon}}$ induces a probability measure on $C_{\mathbb{H}}([0,1]).$
 The identity $
	\sup_{x\in \mathbb{H}}\left\|W_{C_{0}^{\varepsilon}}\circ P_{Y_{0}}(x)\right\|_{\mathbb{H}}\underset{D}{=}\sup_{t\in [0,1]}\left\|W_{C_{0}^{\varepsilon}}(t)\right\|_{\mathbb{H}}$
	  has been considered  in the determination of the critical value  of the test statistics
(\ref{statistic}) when we apply random projection methodology. An alternative subordinator scheme has been considered in Section \ref{subsuppm} of the Appendix.

\end{remark}
\begin{proof}
Let us define   the sequence   $\{Y_{n,P_{Y_{0}}^{-1}(t)},\ n\geq 2\}$ satisfying
\begin{eqnarray}\hspace*{-1cm} Y_{n, P_{Y_{0}}^{-1}(t)}(t)&=&
	\frac{\sqrt{[nt]}}{\sqrt{n}}	V_{[nt]}(P_{Y_{0}}^{-1}(t))+\frac{(nt-[nt])}{\sqrt{n}}X_{[nt]+1}(P_{Y_{0}}^{-1}(t)),
	\label{fclt}
\end{eqnarray}
\noindent for each   $t\in [0,1],$ where $V_{n}$ is the $\mathbb{H}$--valued empirical process introduced in equation (\ref{efp}).

The proof is based on verifying that, for each fixed $x\in \mathbb{H},$  the $\mathbb{H}$--valued martingale difference sequence $\{X_{i}(x),\ i\geq 1\}$ satisfies  Lemma \ref{th2}(i)--(iii), after proving that, for every $t\in [0,1],$  $$\lim_{\substack{\left\langle P_{Y_{0}}^{-1}(t),\phi_{j}\right\rangle_{\mathbb{H}} \to \infty \ j\geq 1}}\lim_{n\to \infty}
P\left(\|Y_{n, P_{Y_{0}}^{-1}(t)}(t)-V_{n}(P_{Y_{0}}^{-1}(t))\|_{\mathbb{H}}>\epsilon\right)=0,\quad \epsilon>0.$$
\noindent Note that, applying Chebyshev's inequality, for $t\in [0,1],$
\begin{eqnarray}
	&&P\left(\|Y_{n, P_{Y_{0}}^{-1}(t)}(t)-V_{n}(P_{Y_{0}}^{-1}(t))\|_{\mathbb{H}}>\epsilon\right)\nonumber\\&&\leq \frac{\epsilon^{-2}}{n}\Esp{\left\|\sum_{i=n\wedge [nt]}^{n\vee [nt]}X_{i}(P_{Y_{0}}^{-1}(t))+(nt-[nt])X_{[nt]+1}(P_{Y_{0}}^{-1}(t))\right\|_{\mathbb{H}}^{2}}\nonumber\\
	&&\leq \frac{\epsilon^{-2}}{n}\left\|C^{R_{[nt]}^{n}}_{0}\right\|_{L^{1}(\mathbb{H})}\to 0,\ n\to \infty,\nonumber
\end{eqnarray}
\noindent where $R_{[nt]}^{n}=\sum_{i=n\wedge [nt]}^{n\vee [nt]}X_{i}(P_{Y_{0}}^{-1}(t))+(nt-[nt])X_{[nt]+1}(P_{Y_{0}}^{-1}(t))$, $n\wedge [nt]=\min\{n,  [nt]\}$ and $n\vee [nt]=\max\{n,  [nt]\}.$
We now  verify conditions (i)-(iii) of Lemma   \ref{th2}, ensuring ergodicity of the involved    centered stationary martingale difference functional sequence. We begin considering  (i) in Lemma  \ref{th2}. Thus, under \textbf{Assumption A1},
keeping in mind  (\ref{eq311b2000}),  for $x\in \mathbb{H},$ and $n\geq 1,$
\begin{eqnarray}
	&&\Esp{\left\|\frac{1}{n}\sum_{j=1}^{n}\Esp{X_{j}(x)\otimes X_{j}(x)|Y_{1},\dots,Y_{j-1}}-S_{x,x}\right\|_{L^{1}(\mathbb{H})}}  \nonumber \\
	&&= \Esp{\left\|\frac{1}{n}\sum_{j=1}^{n}\Esp{X_{j}(x)\otimes X_{j}(x)|Y_{1},\dots,Y_{j-1}}-P(E^{0}(x))C_{0}^{\varepsilon}
		\right\|_{L^{1}(\mathbb{H})}} \nonumber \\
	&&=\Esp{\left\|\frac{1}{n}\sum_{j=1}^{n}\Esp{X_{j}(x)\otimes X_{j}(x)|Y_{j-1}}-P(E^{0}(x))C_{0}^{\varepsilon}
		\right\|_{L^{1}(\mathbb{H})}} 	\nonumber \\
	&&=\Esp{\left\|\frac{1}{n}\sum_{j=1}^{n}
		\Esp{\varepsilon_{j}\otimes \varepsilon_{j}\ind{E^{(j-1)}(x)}^{2}|Y_{j-1}}-P(E^{0}(x))C_{0}^{\varepsilon}
		\right\|_{L^{1}(\mathbb{H})}} \nonumber \\
	&&=  \frac{1}{n}\sum_{j=1}^{n}\mathbb{E}_{P_{Y_{j-1}}}\lrc{\ind{E^{(j-1)}(x)}^{2}}
	\sum_{l=1}^{\infty }\Esp{\varepsilon_{j}\otimes \varepsilon_{j}(\phi_{l})(\phi_{l})}  \nonumber\\
	&&
	-\sum_{l=1}^{\infty }P(E^{0}(x))C_{0}^{\varepsilon}(\phi_{l})(\phi_{l}) \nonumber\\
	&&= \frac{1}{n}\sum_{j=1}^{n}P\lrp{E^{(j-1)}(x)}\sum_{l=1}^{\infty} \Esp{\varepsilon_{1}\otimes \varepsilon_{1}(\phi_{l})(\phi_{l})}
	-\sum_{l=1}^{\infty }P(E^{0}(x))C_{0}^{\varepsilon}(\phi_{l})(\phi_{l}) \nonumber\\
	&&= P(E^{0}(x))\lrc{\sum_{l=1}^{\infty} C_{0}^{\varepsilon}(\phi_{l})(\phi_{l})-C_{0}^{\varepsilon}(\phi_{l})(\phi_{l})}=0. \label{zerlim}
\end{eqnarray}

\noindent Lemma  \ref{th2}(ii) holds, under \textbf{Assumptions A1--A2},  from equation (\ref{eqmp2b}), since, for each $x\in \mathbb{H}$ and every $n\geq 1,$
\begin{align}
	&\frac{1}{n}\sum_{j=1}^{n}\Esp{\|X_{j}(x)\|^{2}_{\mathbb{H}}}=\frac{1}{n}
	\sum_{j=1}^{n}\|\Esp{X_{j}(x)\otimes X_{j}(x)}\|_{L^{1}(\mathbb{H})}
	\nonumber\\
	&=\frac{1}{n}\sum_{j=1}^{n}\|C_{0}^{X_{1}(x)}\|_{L^{1}(\mathbb{H})}=
	P(E^{0}(x))\sum_{l=1}^{\infty}C_{0}^{\varepsilon}(\phi_{l})(\phi_{l})
	=\mathrm{trace}(S).\nonumber
\end{align}
\noindent Finally,   Lemma  \ref{th2}(iii)  also holds under \textbf{Assumptions A1--A3}. Specifically, we prove convergence in $L^{1}_{\mathbb{H}}(\Omega,\mathcal{A},P).$  Thus, from equation (\ref{eq311b2000}),   for each $x\in \mathbb{H},$ and $r>0,$ applying    Chebyshev's  inequality, we obtain as $n\to \infty,$
\begin{align}
	& \Esp{\frac{1}{n}\sum_{j=1}^{n}\Esp{ \|X_{j}(x)\|_{\mathbb{H}}^{2}\chi(\|X_{j}(x)\|^{2}_{\mathbb{H}}\geq rj)|Y_{1},\dots, Y_{j-1}}} \nonumber\\
	=& \Esp{\frac{1}{n}\sum_{j=1}^{n}\Esp{\|X_{j}(x)\|_{\mathbb{H}}^{2}\chi(\|X_{j}(x)\|^{2}_{\mathbb{H}}\geq rj)|Y_{j-1}}}  \nonumber\\
	\leq &  \frac{1}{n}\sum_{j=1}^{n}\int_{\mathbb{H}}\ind{E^{(j-1)}(x)}(y_{j-1})\int_{\|\varepsilon_{j}\|_{
			\mathbb{H}}^{2}>rj}\|\varepsilon_{j}\|_{\mathbb{H}}^{2}P\lrp{d \epsilon_{j}/y_{j-1}}dy_{j-1} \nonumber\\
	=& \frac{1}{n}\sum_{j=1}^{n}\lrc{\int_{\mathbb{H}}\ind{E^{j-1}(x)}(y_{j-1})dy_{j-1}}\lrc{\int_{\|\varepsilon_{j}\|_{\mathbb{H}}^{2}>rj}\|\varepsilon_{j}\|_{\mathbb{H}}^{2}P\lrp{d \epsilon_{j}}}  \nonumber\\
	\leq & \lrp{\frac{1}{n}}P(E^{0}(x))\sup_{j\in \mathbb{Z}; \omega \in \Omega } \|\varepsilon_{j,\omega}\|_{\mathbb{H}}^{2}\sum_{j=1}^{n}P\lrp{\|\varepsilon_{j}\|_{\mathbb{H}}^{2}>rj}  \nonumber\\
	\leq &\lrp{\frac{1}{n}} P(E^{0}(x))\sup_{j\in \mathbb{Z}; \omega \in \Omega} \|\varepsilon_{j,\omega }\|_{H}^{2}\sum_{j=1}^{n}\frac{\Esp{\|\varepsilon_{j}\|_{\mathbb{H}}^{4}}}{(rj)^{2}}
	\nonumber\\
	\leq & \left(\frac{1}{n}\right)P(E^{0}(x))\sup_{j\in \mathbb{Z}; \ \omega \in \Omega } \|\varepsilon_{j,\omega}\|_{\mathbb{H}}^{2}\frac{\Esp{\|\varepsilon_{1}\|_{\mathbb{H}}^{4}}}{r^{2}}
	\sum_{j=1}^{\infty }(1/j)^{2}\to 0,
	\label{i2}
\end{align}
\noindent  under {\bf Assumption A1} and {\bf A3}, since $\sum_{j=1}^{\infty}\left(\frac{1}{j}\right)^{2}<\infty.$
\end{proof}





\subsection{Consistency}\label{s4}


This section shows the consistency under \textbf{Assumption A1} of the proposed GoF procedure  based on $V_{n}(x)$ for testing $H_{0}:\Gamma =\Gamma_{0}$  versus the  alternative $H_{1}:\Gamma \neq\Gamma_{0}$  when  $\Gamma_{0}$ is a known operator. By $\Gamma \neq\Gamma_{0}$ it should be understood that $P_{Y_{0}}\{ y\in \mathbb{H}:\  \Gamma (y) \neq\Gamma_{0}(y)\}>0.$  Let $\lambda (y,z):=\mathbb{E}_{H_{1}}\left[ Y_{1}-\Gamma (Y_{0})+z/Y_{0}=y\right],$ for $y,z\in \mathbb{H}.$  Assume that for every $y\in \mathbb{H},$
$\lambda (y,z)=0,$ if and only if $\|z\|_{\mathbb{H}}=0.$ Let $d(x):=\Gamma (x)-\Gamma_{0}(x),$ for every $x\in \mathbb{H},$ and consider $\mathcal{D}_{n}(x):= \frac{1}{\sqrt{n}} \sum_{i=1}^{n}\lambda (Y_{i-1},d(Y_{i-1}))\ind{E^{i-1}(x)},\ x\in \mathbb{H}.$
 For  $i=1,\dots,p-1,$ and  $x\in \mathbb{H},$ let also $Z_{i}(x)=\lambda (Y_{i-1},d(Y_{i-1}))\ind{E^{i-1}(x)}$ \linebreak $-\mathbb{E}_{H_{1}}[\ind{E^{0}(x)} \lambda (Y_{0},d(Y_{0}))],$ where  $\lambda (Y_{i-1},d(Y_{i-1}))=[\Gamma-\Gamma_{0}](Y_{i-1}),$
$i\geq 1,$ and  $\mathbb{E}_{H_{1}}$ means expectation is computed under the alternative ($\Gamma\neq \Gamma_{0}$). Then, under \textbf{Assumption A1},  applying stationarity, for every  $x\in \mathbb{H},$
\begin{eqnarray}&&
	\mathbb{E}_{H_{1}}\left[\left\|Z_{1}(x)+\dots +Z_{p-1}(x)\right\|_{\mathbb{H}}^{2}\right]
	\nonumber\\
	&&	\leq p\sum_{u\in \{-(p-1),\dots,(p-1)\}}\left[1-\frac{|u|}{p}\right]\mathbb{E}_{H_{1}}\left[\left\langle \lambda (Y_{0},d(Y_{0})),\lambda (Y_{u},d(Y_{u}))\right\rangle_{\mathbb{H}}\right]\nonumber\\
	&&\hspace*{-0.5cm}\leq p\sum_{u\in \mathbb{Z}}\mathbb{E}_{H_{1}}\left[\left\langle \lambda (Y_{0},d(Y_{0})),\lambda (Y_{u},d(Y_{u}))\right\rangle_{\mathbb{H}}\right]\leq p\sum_{u\in \mathbb{Z}}\|\mathbb{E}\left[Y_{0}\otimes Y_{u}\right]\|_{L^{1}(\mathbb{H})}<\infty,\nonumber\\
	\label{eqc23bb}
\end{eqnarray}
\noindent  in view of the short  range dependence displayed by AR$\mathbb{H}$(1) process $Y.$
 From
Corollary 2.3 in  \cite{Bosq2000},   equation (\ref{eqc23bb})  leads to the almost surely convergence
\begin{equation}\sup_{x\in \mathbb{H}}\left\|n^{-1/2}\mathcal{D}_{n}(x)-\mathbb{E}_{H_{1}}\left[\lambda (Y_{0},d(Y_{0}))\ind{E^{(0)}(x)}\right]\right\|_{\mathbb{H}}\to_{\mbox{a.s}} 0,\label{eqc23b}
\end{equation}
\noindent   yielding the consistency of the test.

\section{Asymptotic properties for composite hypothesis}\label{s6}
%	 Misspecified $\Gamma $}\label{s6}

 This section addresses the asymptotic analysis under composite null hypothesis. Specifically, we consider that the probabilistic conditions, determining
 the scenario $\Theta_{0}$ of the null hypothesis, ensure consistency of an estimator of the unknown  $\Gamma .$
  Two scenarios are considered respectively corresponding  to the  cases where  the eigenfunctions  $\left\{ \phi_{j},\ j\geq 1\right\}$ of the autocovariance operator $C_{0}^{Y}$ of $Y$  are known and unknown.

Let us first consider the composite null hypothesis
$$\widetilde{H_{0}}: \Gamma =\Gamma_{0},\ \mbox{for some} \ \Gamma_{0}\in \Theta_{0},$$
\noindent where the family of autocorrelation operators in $\Theta_{0}$ satisfies the  conditions in Lemma 8.1, and Theorems 8.5--8.6 in \cite{Bosq2000}. Thus, we   derive  the asymptotic properties of the generalized $\mathbb{H}$--valued plug--in empirical process
\begin{equation}\widetilde{V}_{n}^{\phi }(x)=
\frac{1}{\sqrt{n}}\sum_{i=1}^{n}(Y_{i}- \widehat{\Gamma}_{n} (Y_{i-1}))\ind{E^{i-1}(x)},
\label{depbb}
\end{equation}
\noindent where $\widehat{\Gamma}_{n}$ is a consistent estimator of $\Gamma $ under $\widetilde{H_{0}}$ in the norm of the space of bounded linear operators $\mathcal{L}(\mathbb{H})$ on $\mathbb{H}$
(see \cite{Bosq2000}).

In this first scenario, when the eigenfunctions $\left\{ \phi_{j},\ j\geq 1\right\}$ of the autocovariance operator $C_{0}^{Y}$ of $Y$  are known,   the estimator $\widehat{\Gamma}_{n}$
of $\Gamma $ is given by
\begin{equation}\widehat{\Gamma}_{n}(\varphi )=\sum_{l=1}^{k_{n}}\gamma_{n,l}(\varphi )\phi_{l},\quad \varphi \in \mathbb{H}, \ n\geq 2,\label{eqestgamma}
\end{equation}
\noindent where $k_{n}\to \infty,$ and  $k_{n}/n\to 0,$ $n\to \infty,$ and
\begin{eqnarray}\gamma_{n,l}(\varphi )&=&\frac {1}{n-1}\sum_{i=1}^{n-1}\sum_{j=1}^{k_{n}}\widehat{\lambda }_{j,n}^{-1}\left\langle \varphi ,\phi_{j}\right\rangle_{\mathbb{H}}
\left\langle  Y_{i},\phi_{j}\right\rangle_{\mathbb{H}}
\left\langle Y_{i+1},\phi_{l} \right\rangle_{\mathbb{H}}\label{rhocoef}\\
\widehat{\lambda }_{k,n} &=&  \frac{1}{n}\sum_{i=1}^{n}\left(\left\langle Y_{i}, \phi_{k} \right\rangle_{\mathbb{H}}\right)^{2},\  k\geq 1,\  n\geq 2.
\nonumber
\end{eqnarray}

The following assumption is also considered  providing the strong consistency of  $\widehat{\Gamma}_{n}$ in
$\mathcal{L}(\mathbb{H})$ (see \cite{Bosq2000}).

\medskip

\noindent \textbf{Assumption A4}. Assume the following conditions:
\begin{itemize}
\item[(i)] $Y$  is a standard AR$\mathbb{H}$(1) process with $\mathbb{H}$--SWN innovations, and \linebreak  $E\|Y_{0}\|_{\mathbb{H}}^{4}<\infty.$
\item[(ii)] The eigenvalues $\{\lambda_{k}(C_{0}^{Y}),\ k\geq 1\}$ of $C_{0}^{Y}$ satisfying $C_{0}^{Y}(\phi_{k})=\lambda_{k}\phi_{k},$  for every $k\geq 1,$ are strictly positive (i.e., $\lambda_{k}>0,$  $k\geq 1$), and the eigenfunctions $\{\phi_{k},\ k\geq 1\}$ are known.
\item[(iii)] $P\left(\left\langle Y_{0},\phi_{k}\right\rangle_{\mathbb{H}}=0\right)=0,$ for every $k\geq 1.$
\end{itemize}

\noindent Under \textbf{Assumption A4},  $\widehat{\Gamma}_{n}$ is bounded satisfying:
$\|\widehat{\Gamma}_{n}\|_{\mathcal{L}(\mathbb{H})}\leq \|\widehat{C}_{1,n}^{Y}\|_{\mathcal{L}(\mathbb{H})}$\linebreak $\max_{1\leq j\leq k_{n}}\widehat{\lambda }_{j,n}^{-1},$ with
$\widehat{C}_{1,n}^{Y}=\frac{1}{n-1}\sum_{i=1}^{n-1} Y_{i}\otimes Y_{i+1}.$

\medskip


Considering,  additionally, the conditions assumed in Lemma 8.1, and Theorems 8.5--8.6 in \cite{Bosq2000}, we obtain the  the strong consistency of $\widehat{\Gamma}_{n}$  in the space $\mathcal{L}(\mathbb{H}).$


\medskip

Let us now consider the composite null hypothesis
$$\widetilde{H_{0}}: \Gamma =\Gamma_{0},\ \mbox{for some} \ \Gamma_{0}\in \Theta_{0},$$
\noindent where the family of autocorrelation operators in $\Theta_{0}$  satisfies the  conditions in Theorems 8.7--8.8 in  \cite{Bosq2000}. The estimator $\widetilde{\Gamma}_{n}$ of $\Gamma $ is formulated  in terms of the empirical eigenfunctions
$\{ \phi_{k,n},\ k\geq 1\}$ and eigenvalues $\{ \widetilde{\lambda }_{k,n},\ k\geq 1\},$ given by
$$\widehat{C}_{0,n}^{Y}(\phi_{k,n})=\frac{1}{n}\sum_{i=1}^{n}Y_{i}\left\langle Y_{i},\phi_{k,n}\right\rangle_{\mathbb{H}}=
\widetilde{\lambda }_{k,n}\phi_{k,n},\ k\geq 1,$$
\noindent where $\widehat{C}_{0,n}^{Y}=\frac{1}{n}\sum_{i=1}^{n}Y_{i}\otimes Y_{i}.$
 Specifically, $\widetilde{\Gamma}_{n}$ is defined as
\begin{equation}\widetilde{\Gamma}_{n}(\varphi )=\sum_{l=1}^{k_{n}}\widetilde{\gamma}_{n,l}(\varphi)\phi_{l,n},\quad \varphi\in \mathbb{H}, \ n\geq 2,
\label{est2}
\end{equation}
\noindent with, as before,  $k_{n}\to \infty,$ and  $k_{n}/n\to 0,$ $n\to \infty,$ and
for $n\geq 1,$ and $l\geq 1,$  $$\widetilde{\gamma}_{n,l}(\varphi )=
	\frac{1}{n-1}\sum_{i=1}^{n-1}\sum_{j=1}^{k_{n}}\widetilde{\lambda }_{j,n}^{-1}\left\langle \varphi ,\phi_{j,n}\right\rangle_{\mathbb{H}}
\left\langle  Y_{i},\phi_{j,n}\right\rangle_{\mathbb{H}}
\left\langle Y_{i+1},\phi_{l,n} \right\rangle_{\mathbb{H}}, \ \varphi\in \mathbb{H}.$$



The  following additional condition is  assumed for strong--consistency (see Theorems 8.7--8.8 in  \cite{Bosq2000}).

\medskip

\noindent \textbf{Assumption A5}. The AR$\mathbb{H}$(1) process $Y$ is such that
\begin{itemize}
\item[(i)] The eigenvalues $\{\lambda_{k}(C_{0}^{Y}),\ k\geq 1\}$ of $C_{0}^{Y}$ satisfy
$$\lambda_{1}(C_{0}^{Y})> \lambda_{2}(C_{0}^{Y})>\dots  >\lambda_{j}(C_{0}^{Y})>\dots >0,$$
\noindent where as before,  $C_{0}^{Y}(\phi_{k})=\lambda_{k}(C_{0}^{Y})\phi_{k},$  for every $k\geq 1.$
\item[(ii)]   For every $n\geq 2$ and $k\geq 1,$   $\widetilde{\lambda }_{k,n}>0$ a.s.
\end{itemize}



Under the assumed conditions (see \textbf{Assumption A5}), Theorems 8.7--8.8 in  \cite{Bosq2000} establishe the strong consistency of  $\widetilde{\Gamma}_{n}$ in $\mathcal{L}(\mathbb{H}),$ in the case of unknown  eigenfunctions of the autocovariance operator $C_{0}^{Y}.$ Thiese theorems are  applied in the derivation of the next result providing the asymptotic equivalence in probability  of  $\left\{V_{n}(x),\ x\in \mathbb{H}\right\}$ and $\left\{\widetilde{V}_{n}(x)=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}(Y_{i}-\widetilde{\Gamma}_{n} (Y_{i-1}))\ind{E^{i-1}(x)},\ x\in \mathbb{H}\right\}.$

\begin{theorem}
\label{th4}
Under conditions of Theorems 8.7--8.8 in  \cite{Bosq2000}, the following identity holds:
\begin{eqnarray}
	\sup_{x\in \mathbb{H}}\|\widetilde{V}_{n}(x)-V_{n}(x)\|_{\mathbb{H}}=o_{P}(1),\quad n\to \infty.
	\label{eqconstest}
\end{eqnarray}
\end{theorem}
\begin{proof}
	Let us consider, for every $x\in \mathbb{H},$
	\begin{eqnarray}&&
		P\left[\|\widetilde{V}_{n}(x)-V_{n}(x)\|_{\mathbb{H}}^{2}>\eta \right]\nonumber\\ &&=
		P\left[\left[\frac{1}{n}\sum_{k\geq 1}\sum_{i,j=1}^{n}[\widetilde{\Gamma}_{n}-\Gamma](Y_{i-1})(\phi_{k})[\widetilde{\Gamma}_{n}-\Gamma](Y_{j-1})(\phi_{k})
		1_{E_{i-1}(x)}1_{E_{j-1}(x)}\right]>\eta \right]
		\nonumber\\
		&&\leq  P\left[\left\|\frac{1}{\sqrt{n}}\sum_{i=1}^{n}[\widetilde{\Gamma}_{n}-\Gamma](Y_{i-1})\right\|^{2}_{\mathbb{H}}>\eta \right]
		\nonumber\\
		&&
		\leq  P\left[\left\|\widetilde{\Gamma}_{n}-\Gamma\right\|_{\mathcal{L}(\mathbb{H})}^{2}
		\left\|\frac{1}{\sqrt{n}}\sum_{i=1}^{n}Y_{i-1}\right\|_{\mathbb{H}}^{2}>\eta\right].
		\label{eq1proofth4}
	\end{eqnarray}
	From Theorems 8.7--8.8 in  \cite{Bosq2000},  $\left\|\widetilde{\Gamma}_{n}-\Gamma\right\|_{\mathcal{L}(\mathbb{H})}=o_{P}(1).$ Also,  from equation (2.21) in Theorem 2.5(2) in \cite{Bosq2000}, in a similar way to the proof of  Theorem 3.9 (see, in particular,  equations (3.36)--(3.37)) in \cite{Bosq2000},
	$\left\|\frac{1}{\sqrt{n}}\sum_{i=1}^{n}Y_{i-1}\right\|_{\mathbb{H}}
	=o_{P}(1).$ Thus, (\ref{eq1proofth4}) implies equation (\ref{eqconstest}).
	
	
	
	\end{proof}
\begin{remark}
\label{efsh0}
 The asymptotic equivalence in probability of $\left\{\widetilde{V}_{n}^{\phi },\ x\in \mathbb{H}\right\}$ and
$\left\{V_{n}(x),\ x\in \mathbb{H}\right\}$ also follows from the  strong--consistency in $\mathcal{L}(\mathbb{H})$ of
$\widehat{\Gamma}_{n}.$
\end{remark}




As direct consequence  of Theorem \ref{th4},
 under  \textbf{Assumptions A1--A5}, and conditions of  Theorems 8.7--8.8 in  \cite{Bosq2000}, as $n\to \infty,$  $\widetilde{V}_{n}\to_{D} W_{C_{0}^{\varepsilon}}\circ P_{Y_{0}},$ and
$\sup_{x\in \mathbb{H}}\|\widetilde{V}_{n}(x)\|_{\mathbb{H}}\to_{D} \sup_{x\in  \mathbb{H}}\left\|W_{C_{0}^{\varepsilon}}\circ P_{Y_{0}}(x)\right\|_{\mathbb{H}}.$
From Remark \ref{efsh0}, similar assertions hold regarding the limiting generalized $\mathbb{H}$--valued  Gaussian process of the empirical process
$\widetilde{V}_{n}^{\phi }.$
 The remaining steps in the implementation of the testing procedure via random projection follows in a similar way to  Section \ref{test},
where
$s_{n}$ can be defined as  in equation (\ref{rpci}) in terms of the plug--in empirical process $\widetilde{V}_{n}$  (respectively,
 the plug--in  empirical process
$\widetilde{V}_{n}^{\phi }$ under  the first $\widetilde{H}_{0}$ scenario).


\section{Discussion}
\label{fc}

Up to our knowledge, this paper constitutes the first attempt to derive asymptotic theory of  GoF test of the AR$\mathbb{H}$(1) time series model,  based on the empirical process methodology. The main contribution of this paper is the functional central limit result derived. The analysis under composite null hypothesis  is supported by the strong--consistency results in Chapter 8 of \cite{Bosq2000}. Note that the asymptotic analysis performed here  is  different from the asymptotic GoF analysis of the classical  functional linear model under independent functional data (see Section \ref{secrevsm} of the Appendix).


The  performance of our  approach is  also illustrated in the context of   SP$\mathbb{H}$AR(1) processes in  Section \ref{sph1} (see, e.g., \cite{CaponeraMarinucci}). The assumed  invariance property of the kernels defining the autoregression, and covariance operators  of  SP$\mathbb{H}$AR(1) process  leads to an important dimension reduction in the implementation.
Similar results hold under this invariance property in the case of autoregressive processes on compact and connected two point homogeneous spaces (see, e.g., \cite{MaMalyarenko}).


\section*{Acknowledgements}
\noindent This work has been supported in part by projects PID2022--142900NB-I00 and PID2020-116587GB-I00, financed by
MCIU/AEI/10.13039/\linebreak 501100011033 and by FEDER UE,  and CEX2020-001105-M MCIN/AEI/\linebreak 10.13039/501100011033), as well as supported by grant ED431C 2021/24 (Grupos competitivos) financed by Xunta de Galicia through European Regional Development Funds (ERDF).


\bibliographystyle{chicago}
\bibliography{GMRMFB}

% {\small
%\bibliographystyle{elsarticle-num}
%\bibliography{GMRMFBJUKY2024}
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Appendix}
Auxiliary information about differences arising in our setting with respect to the asymptotic theory of GoF in the $\mathbb{H}$--valued linear model, under independent functional data, are discussed. The second--order properties of $\mathbb{H}$--valued martingale difference sequence  $\{X_{i}(x),\ i\geq 1\},$ $x\in \mathbb{H},$  are provided. An alternative subordination scheme,  in the spirit of the real--valued case, is  introduced.  The finite sample performance of the GoF testing procedure proposed  is finally illustrated in the simulation study undertaken.
\setcounter{section}{0}
\section{Auxiliary information}

Additional supporting information to facilitate the  reading of  the paper is provided in this section.
\subsection{Differences with asymptotic GoF under independent data}
\label{secrevsm}
Theorem 1 in  \cite{Cardot07} focuses on functional regression  with scalar response, and $\mathbb{H}$--valued covariate $X.$ The projection estimator of the regression function does not satisfy a Central Limit Theorem (CLT)  with convergence to a non--degenerated random element in $\mathbb{H}.$  Weak--convergence results hold for the predictor, under  suitable truncation and convexity of the eigenvalues of the autocovariance operator $C_{0}^{X}$ of the covariate $X.$   In the AR$\mathbb{H}$(1)  framework, the situation is quite different as follows from the convergence results enumerated  below (see \cite{Ma99}):


\begin{itemize}
	\item[(i)] The convergence to zero, in probability, of $\sqrt{n}[C_{0}^{Y}]^{-1}\pi^{k_{n}}\left( C_{0}^{Y}-\widehat{C}_{0,n}^{Y}\right),$ in the norm of the space of Hilbert--Schmidt operators on $\mathbb{H}$,  $\mathcal{S}(\mathbb{H}),$ under suitable conditions like the ones assumed  in Theorem 4.1, p. 98,  in  \cite{Bosq2000}. Here, $\pi^{k_{n}}$ denotes the projection operator into the subspace of $\mathbb{H}$ generated by the eigenfunctions $\{\phi_{1},\dots,\phi_{k_{n}}\}$
	of $C_{0}^{Y}.$
	In particular, under the conditions assumed  in Theorem 4.1 in  \cite{Bosq2000}, our choice of the truncation order  $k_{n}$ must be  such that, as $n\to \infty,$ $\sqrt{n}\lambda_{k_{n}}^{-1}=\mathcal{O}\left(n^{1/4}(\log(n))^{-\beta }\right),$ $\beta >1/2,$ ensuring, in particular, Proposition 4 in  \cite{Ma99}  holds.    Note that Theorem 4.1 in \cite{Bosq2000} is proved by applying the strong law of large number for weakly dependent sequences of $\mathbb{H}$-valued random variables, that leads to the strong law of large numbers for AR$\mathbb{H}$(1) processes (see, e.g., Theorem 3.7, p.86, in  \cite{Bosq2000}).  Specifically, from Lemma 4.1, p.96, in  \cite{Bosq2000},  on the AR$\mathcal{S}(\mathbb{H})(1)$ representation of the diagonal self--tensorial product of an AR$\mathbb{H}$(1) process,  the strong consistency in the norm of $\mathcal{S}(\mathbb{H})$ of the empirical autocovariance operator $\widehat{C}^{Y}_{0,n}$ of $Y$ is obtained by applying Theorem 3.7 in  \cite{Bosq2000}.
	\item[(ii)] The convergence in distribution of $\sqrt{n}[C_{0}^{Y}]^{-1}\pi^{k_{n}}U_{n},$  with \linebreak $U_{n}=\frac{1}{n}\sum_{i=1}^{n}Y_{i}\otimes \varepsilon_{i+1},$
	to a centered Gaussian random Hilbert-Schmidt operator $\widetilde{\Gamma },$ under  the key condition $\Esp{\left\|[C_{0}^{Y}]^{-1}\varepsilon_{0}\right\|_{\mathbb{H}}^{2}}<\infty.$  This limit result is obtained by applying a CLT for an array of $\mathcal{S}(\mathbb{H})$--valued martingale differences. The limit centered Gaussian random Hilbert-Schmidt operator $\widetilde{\Gamma }$ has
	covariance operator $\Sigma $ defined by, for $T_{kl}=\phi_{k}\otimes \phi_{l},$ $k,l\geq 1,$
	\begin{eqnarray}
		\left\langle \Sigma T_{ii^{\prime}}, T_{jj^{\prime }}\right\rangle_{\mathcal{S}(\mathbb{H})}=
		\left\{\begin{array}{lc}
			0 & \ i\neq  j\\
			\frac{\lambda_{i}(C_{0}^{Y})\left\langle C_{0}^{\varepsilon}(\phi_{i^{\prime }}), \phi_{j^{\prime }}\right\rangle_{\mathbb{H}}}{\lambda_{i^{\prime}}(C_{0}^{Y})\lambda_{j^{\prime}}(C_{0}^{Y})} & i=j. \\
		\end{array}
		\right.
		\nonumber\end{eqnarray}
	
	Note that, in the functional linear regression model with scalar response and functional covariate, the AR$\mathbb{H}$(1) condition $\Esp{\left\|[C_{0}^{Y}]^{-1}\varepsilon_{0}\right\|^{2}_{\mathbb{H}}}<\infty$ can not be considered since  the innovation process  is real--valued.  Hence, the convergence to a non--degenerated random element in the norm of $\mathbb{H}$ does not hold (see Theorem 1 in \cite{Cardot07}).   A similar assertion can be made for the functional linear regression model with functional response and covariate (see Theorem 8 in \cite{Crambes13}), since the  above--referred  CLT for an array of $\mathcal{S}(\mathbb{H})$--valued martingale differences can not be applied under independent functional data.
	
	
	
	Condition $\Esp{\left\|[C_{0}^{Y}]^{-1}\varepsilon_{0}\right\|^{2}}_{\mathbb{H}}<\infty$
	means that $\|[C_{0}^{Y}]^{-1}C_{0}^{\varepsilon} [C_{0}^{Y}]^{-1}\|_{L^{1}(\mathbb{H})}<\infty.$  Thus, $P[\varepsilon_{0}\in C_{0}^{Y}(\mathbb{H}) ]=1,$ or, equivalently, $\varepsilon_{0}$ belongs to the Reproducing Kernel Hilbert Space (RKHS) generated by the integral  operator $[C_{0}^{Y}]^{2}.$  In the case where $C_{0}^{Y}$ and $C_{0}^{\varepsilon}$
	have a common system of eigenfunctions, this condition can be equivalently expressed as $$\sum_{k\geq 1}\frac{\lambda_{k}(C_{0}^{\varepsilon})}{[\lambda_{k}(C_{0}^{Y})]^{2}}<\infty.$$
	\noindent Hence, a faster decay of the eigenvalues of the autocovariance operator  $C_{0}^{\varepsilon}$  of the innovation process $\varepsilon $ than the eigenvalues of the square autocovariance operator $[C_{0}^{Y}]^{2}$ of the AR$\mathbb{H}$(1) process $Y$   is required. In particular, a  mild  sufficient condition is given by   $$\frac{\lambda_{k}(C_{0}^{\varepsilon})}{[\lambda_{k}(C_{0}^{Y})]^{2}}= \mathcal{O}\left(k^{-\gamma }\right),\quad \gamma >1, \quad k\to \infty.$$
	
	Section 2.3 in \cite{Ma99} provides some rules to compute the truncation order $k_{n},$  depending on the functional sample size $n,$ in order to ensure  the derived asymptotic results hold. Specifically,
	one can consider $k_{n}=o\left( n^{1/(2\alpha )}\right),$ $n\to \infty,$ if $\lambda_{k}(C_{0}^{Y})$ obeys the following asymptotic behavior as $k\to \infty$ $\lambda_{k}(C_{0}^{Y})=\mathcal{O}\left(k^{-\alpha }\right),$  $\alpha > 1.$ On the other hand, for $\lambda_{k}(C_{0}^{Y})= \mathcal{O}\left(\lambda^{k}\right),$ $k\to \infty,$ one can choose $k_{n}=\log(n)$ if $\log(\lambda )>-1/2,$ while $k_{n}= o\left(\log(n)\right),$ if $\log(\lambda )\leq -1/2.$
	
	In the case where the eigenfunctions of the autocovariance operator  $C_{0}^{Y}$ are unknown, projection $\widetilde{\pi}^{k_{n}}$  into the empirical eigenfunctions of $\widehat{C}_{0,n}$ can be  considered, and a CLT is obtained in Theorem 8.9 in  \cite{Bosq2000}.
	Indeed,   a similar decomposition to equation (11) in  \cite{Cardot07} can be obtained. The strong consistency of the empirical eigenvalues and eigenfunctions of $\widehat{C}_{0,n}$  is then applied (see Theorem 4.4, Lemma 4.3, Theorem 4.5, and Corollary 4.3  in
	\cite{Bosq2000}).
	
	The decay  velocity of the eigenvalues of $C_{0}^{Y},$  and   the distance between such  eigenvalues, characterized by their distribution, mainly in the interval $(0,1),$ play a key role in the conditions assumed for the uniform   asymptotic equivalence in probability in the norm of  $\mathbb{H}$ of the totally specified (under simple $H_{0}$), and plug--in empirical processes (under the two composite null hypotheses  considered).
	
	
\end{itemize}
\subsection{Second--order properties of involved $\mathbb{H}$--valued martingale difference sequence $\{X_{i}(x),\ i\geq 1\}$}
\label{sop}
Note that, for each $x\in \mathbb{H},$ the strongly integrability of the marginals of  the $\mathbb{H}$--valued martingale difference sequence   $\{X_{i}(x),\ i\geq 1\}$     allows the computation of the means and conditional means of the elements of this sequence from  their weak counterparts (Section 1.3 in  \cite{Bosq2000}).

Under \textbf{Assumptions A1--A2},
$\mathbb{E}[X_{i}(x)]=0,$ for any $i\geq 1,$ and $x\in \mathbb{H},$ and  $\mathcal{T}(x)=\mbox{Var}(X_{1}(x))$ can be computed as follows:
\begin{eqnarray}&&\mathcal{T}(x)= \mathbb{E}\left[\|X_{1}(x)\|_{\mathbb{H}}^{2}\right]\nonumber\\
	&&=
	\mathbb{E}\left[\left\|\varepsilon_{1}\ind{E^{0}(x)}\right\|_{\mathbb{H}}^{2}\right]\nonumber\\
	&&=\mathbb{E}\left[\mathbb{E}\left[\left\|[Y_{1}-\Gamma (Y_{0})]\ind{E^{0}(x)}\right\|_{\mathbb{H}}^{2}/Y_{0}\right]\right]\nonumber\\
	&&=\int_{\mathbb{H}}\ind{u\in \mathbb{H};\ \left\langle u,\phi_{j}\right\rangle_{\mathbb{H}}\leq \left\langle  x,\phi_{j}\right\rangle_{\mathbb{H}},\ j\geq 1}^{2}\mbox{Var}\left([Y_{1}-\Gamma (Y_{0})]/Y_{0}=u\right)P_{Y_{0}}(du)\nonumber\\
	&&=\left\|C_{0}^{\varepsilon}\right\|_{L^{1}(\mathbb{H})}
	P\left( E^{0}(x)\right),
	\label{eqmp2}
\end{eqnarray}
\noindent where we  have  applied  $\mbox{Var}\left([Y_{1}-\Gamma (Y_{0})]/Y_{0}=u\right)=\mbox{Var}\left(Y_{1}-\Gamma (Y_{0})\right)$ \linebreak  $=\|C_{0}^{\varepsilon}\|_{L^{1}(\mathbb{H})},$  for all $u\in \mathbb{H},$ under \textbf{Assumptions A1--A2}.
As before, $P_{Y_{0}}$ denotes the infinite--dimensional marginal probability  measure induced by $Y_{0},$ and  $E^{0}(x)$ is the event
$E^{0}(x)=\{\omega \in \Omega ; \ \left\langle Y_{0}(\omega ),\phi_{j}\right\rangle_{\mathbb{H}}\leq \left\langle  x,\phi_{j}\right\rangle_{\mathbb{H}},\ j\geq 1\},$  for each  $x\in \mathbb{H}.$ Keeping in mind  equation (\ref{eq311b2000}),  applying  the strictly stationarity of  $Y,$ for each $x\in \mathbb{H},$ the autocovariance operator  $C_{0}^{X_{i}(x)}:=\mathbb{E}\left[ X_{i}(x)\otimes X_{i}(x)\right]$ of
$\{X_{i}(x),\ i\geq 1\}$ is given by
\begin{eqnarray}&&
	C_{0}^{X_{i}(x)}:=\mathbb{E}\left[ X_{i}(x)\otimes X_{i}(x)\right]=\mathbb{E}\left[\varepsilon_{i}\otimes \varepsilon_{i}\ind{E^{i-1}(x)}^{2}\right]\nonumber\\
	&&
	=\mathbb{E}\left[ \ind{E^{i-1}(x)}^{2}
	\mathbb{E}\left[\varepsilon_{i}\otimes \varepsilon_{i}/Y_{i-1}\right]\right]=\mathbb{E}\left[\varepsilon_{i}\otimes \varepsilon_{i}\right]
	P\left( E^{i-1}(x)\right)\nonumber\\
	&&= C_{0}^{\varepsilon}P(E^{(i-1)}(x))= C_{0}^{\varepsilon}P(E^{(0)}(x)),\quad \forall  i\geq 1.
	\label{eqmp2b}
\end{eqnarray}

In a similar way,
the covariance operator $C^{X_{i}(x),X_{k}(y)}_{0}:=\mathbb{E}\left[ X_{i}(x)\otimes X_{k}(y)\right]$ can be computed  for every $x,y\in \mathbb{H},$
\begin{eqnarray}&&
	C^{X_{i}(x),X_{k}(y)}_{i,k}:=\mathbb{E}\left[ X_{i}(x)\otimes X_{k}(y)\right]\nonumber\\ &&=\delta_{i,k}C_{0}^{\varepsilon}P\left( \omega \in \Omega ; \ \left\langle Y_{i-1}(\omega ),\phi_{j}\right\rangle_{\mathbb{H}}\leq \min\left(\left\langle  x,\phi_{j}\right\rangle_{\mathbb{H}},\left\langle  y,\phi_{j}\right\rangle_{\mathbb{H}}\right),\ j\geq 1\right)
	\nonumber\\   &&=\delta_{i,k}C_{0}^{\varepsilon}P\left( \omega \in \Omega ; \ \left\langle Y_{0}(\omega ),\phi_{j}\right\rangle_{\mathbb{H}}\leq \min\left(\left\langle  x,\phi_{j}\right\rangle_{\mathbb{H}},\left\langle  y,\phi_{j}\right\rangle_{\mathbb{H}}\right),\ j\geq 1\right)\nonumber\\   &&=
	\delta_{i,k}C_{0}^{\varepsilon}P\left(E^{0}(\min(x,y))\right),\ i,k\geq 1,
	\label{sttb}
\end{eqnarray}
\noindent where $\min(x,y)$ is interpreted  as in equation (\ref{eqmin}) of the paper. Here, for $i,k\in \mathbb{Z},$
$\delta_{i,k}=0$  if $i\neq k,$ and $\delta_{i,k}=1$ if $i=k.$
\subsection{Subordinators in the probability distribution identification of $W_{\infty}$}
\label{subsuppm}
This section provides an  alternative subordinator scenario to work in the design of the test statistics based on random projection methodology  (see Section \ref{test} of the paper). Specifically, when indentifiability is performed in the norm of the space of nuclear operators $L^{1}(\mathbb{H}),$ the spectral properties of the autocovariance  operator of the innovation process $\varepsilon$ are summarized in terms of its trace norm, providing the functional variance of the residual marks of the empirical process in equation (\ref{efp}) of the paper, in the spirit of the real--valued case (see \cite{KoulStute1999}).    One can then consider for every $x\in \mathbb{H},$ \begin{equation}\sigma^{2}(x)=\|C_{0}^{\varepsilon}\|_{L^{1}(\mathbb{H})}P\left[E^{0}(x)\right]
	\label{eqtrace}
\end{equation}
\noindent with, as in the paper,  $E^{0}(x)=\{ \omega \in \Omega;\ \left\langle Y_{0}(\omega ),\phi_{j}\right\rangle_{\mathbb{H}}\leq \left\langle  x,\phi_{j}\right\rangle_{\mathbb{H}},\ j\geq 1\}.$

Note that $\sigma^{2}(x)$ is a nondecreasing and nonnegative function, whose values are in the interval $[0,\sigma^{2}(\infty )],$ with
$\sigma^{2}(\infty )=\|C_{0}^{\varepsilon}\|_{L^{1}(\mathbb{H})}.$
One can then  consider the  identification in probability distribution of the limiting process with time--changed  $\mathbb{H}$--valued Wiener process with subordinator $\left\{\sigma^{2}(x),\ x\in \mathbb{H}\right\}.$ In particular,
$W_{\infty}([\sigma^{2}]^{-1}(t))-W_{\infty}([\sigma^{2}]^{-1}(s))$ is independent of $W_{\infty}([\sigma^{2}]^{-1}(s)),$ $0\leq s< t\leq \sigma^{2}(\infty ),$ and $W_{\infty}([\sigma^{2}]^{-1}(t+s))-W_{\infty}([\sigma^{2}]^{-1}(t))$ defines a Gaussian distribution on $\mathbb{H}$ with zero mean
and covariance operator \begin{equation*}C_{[\sigma^{2}]^{-1}(s)}=C^{\varepsilon}_{0}P(E^{0}([\sigma^{2}]^{-1}(s))).	
\end{equation*}
\noindent Here, \begin{equation}[\sigma^{2}]^{-1}(t):= x_{0}\in \mathbb{H} \ \mbox{such that} \ \left\langle x_{0}, \mathbf{h}\right\rangle_{\mathbb{H}}\leq \left\langle x, \mathbf{h}\right\rangle_{\mathbb{H}},\  \mbox{if} \  \sigma^{2}(x)\geq t, \ t\in [0,\sigma^{2}(\infty)].\label{eqinf}
\end{equation}



\section{Illustration of the performance of GoF}
\label{ill}
This section presents two illustrations of the proposed GoF test in the context of AR$\mathbb{H}$(1) ($\mathbb{H}=L^{2}([0,1])$), and SP$\mathbb{H}$AR(1) models in Sections  \ref{illb} and \ref{sph1}, respectively.

.
\subsection{Detecting independence and nonlinearities}
\label{illb}

In this section, an illustration of the performance of the   proposed  GoF  procedure is given by simulation in both scenarios, simple and composite hypothesis.
For simple hypothesis, we consider the particular case of testing $ H_0: \Gamma=\Gamma_0=\mathbf{0},$ versus  $H_1: \Gamma\ne\Gamma_0,$
in the  AR$\mathbb{H}$(1) model  (\ref{eq1}).
Under simple $H_{0},$ the empirical type I error  and power, based on $R=500$ repetitions of the procedure implemented, for functional samples sizes  $n=50,100,200,$ are computed. The critical values of  the test statistics  are approximated from \emph{Fast Bootstrap} based on $B=2000$ bootstrap replicates.  As given in Section \ref{test}  (see also Section \ref{s3}), the random projection methodology is implemented in such  computations    (see   \cite{Cuesta}).


Under a Gaussian scenario, the functional values of an AR$\mathbb{H}$(1) process are generated with support  in the interval $[0,1],$ evaluated at $71$ temporal nodes.  A  Gaussian zero--mean $\mathbb{H}$--SWN noise innovation process having integral  autocovariance operator with exponential covariance kernel
\begin{eqnarray}
	&&  C_{0}^{\varepsilon}(u,v)=\mathbb{E}\left[\varepsilon_{t}(u)\otimes \varepsilon_{t}(v)\right]=
	\sigma^{2}_{\varepsilon}\exp\left(\frac{-|u-v|}{\theta}\right)\nonumber\\ &&  u,v= (i-1)/70, \  i=1,\ldots,71, \ \sigma_{\varepsilon}=0.10, \ \theta=0.6,\nonumber
\end{eqnarray}
\noindent is generated.
A Gaussian  random initial condition $Y_{0}$ is also generated,  independently of the innovation process $\varepsilon$. $Y_{0}$  has  exponential  autocovariance kernel having the same scale parameter values, $\sigma_{\varepsilon}=0.10,  \theta=0.6, $ as  the innovation process  $\varepsilon$. Equation  (\ref{eq1}) is then recursively generated involving the numerical approximation of the integral $\Gamma(Y_{t-1})(u):=\int \Gamma(u,v)Y_{t-1}(v)dv,$
where, under the alternative hypothesis  $H_1$,  the kernel of the integral autocorrelation operator   $\Gamma$  is given by $\Gamma(u,v)=\frac{0.7}{71}\exp\left(\frac{-(u^2+v^2)}{0.7468}\right),$ for $u,v \in [0,1]$. The constants are ensuring that the process is stationary.
To remove dependence from the random initial condition,  a  burn-in period of  $n_{0}=500$ observations is considered. That is, for each functional sample size $n,$  the above--described recursive approximation of the values $Y_0,Y_1,\dots,Y_{n_{0}},\dots,Y_{n_{0}+n}$ is computed removing the values  $Y_0,\dots,Y_{n_{0}}.$

The random projection of the functional covariate and marks is obtained. The empirical process in equation (\ref{efp})
is then expressed in terms of such random projections as
$$V_{n}(x)=
\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\left\langle Y_{i}-\Gamma (Y_{i-1}),\gamma_{\varepsilon }\right\rangle_{\mathbb{H}}\ind{ \omega \in \Omega;\ \left\langle Y_{i-1}(\omega ),\gamma_{Y}\right\rangle_{\mathbb{H}}\leq \left\langle  x,\gamma_{Y}\right\rangle_{\mathbb{H}}}$$


\noindent where $\left\{\gamma_{\varepsilon }(u),\  u\in [0,1]\right\}$ and $\left\{\gamma_{Y}(u),\ u\in [0,1]\right\}$
are independent  realizations of a zero--mean Gaussian process with  covariance kernel given by the autocovariance kernel of  $\varepsilon $ and $Y,$ being those processes,  $\left\{\gamma_{\varepsilon }(u),\ u\in [0,1]\right\}$
and $\left\{\gamma_{Y}(u),\ u\in [0,1]\right\},$ approximated by their truncated, at term $M=5,$  Karhunen--Lo\'eve expansions. Note that here we have considered two independent realizations of an $\mathbb{H}$--valued  Gaussian random variable  for random projection,  but, in our  particular AR$\mathbb{H}$(1) scenario, it is suffcient to consider for random projection a common realization $\mathbf{h}$ of a non--degenerated $\mathbb{H}$--valued  Gaussian random variable.   \emph{Fast Bootstrap} is implemented from the equation
\begin{equation}
	V_{n}^{*b}(x)=
	\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\eta_i \left\langle Y_{i}-\Gamma (Y_{i-1}),\gamma_{\varepsilon }\right\rangle_{\mathbb{H}}\ind{ \omega \in \Omega;\ \left\langle Y_{i-1}(\omega ),\gamma_{Y}\right\rangle_{\mathbb{H}}\leq \left\langle  x,\gamma_{Y}\right\rangle_{\mathbb{H}}},\label{eq:FBSH}
\end{equation}
\noindent where $\eta_{i},$ $i=1,\dots,n,$ are independent and identically distributed standard normal random variables. The computation of the $p$--value is obtained for each projection from
$$p_v(\gamma_\epsilon,\gamma_Y)=\#\left\lbrace \max_{x\in\mathbb{H}}|V_N^{*b}(x)|\geq\max_{x\in\mathbb{H}}|V_N(x)| \right\rbrace/B,$$ \noindent where, as commented,   $B=2000$  bootstrap replicates have been considered. The numbers  of projections tested is  $NP= 1, 2, 3, 4, 5, 10 ,15.$ To obtain only one   $p$--value  the   False Discovery Rate is computed (i.e.,  the expected proportion of false positives among the rejected hypotheses).

For the case of composite null hypothesis  $\widetilde{H}_{0},$ the process generated  is an AR$\mathbb{H}$(1) process, that uses the same $\Gamma$ employed to the alternative hypothesis in the case of simple null hypothesis. For the alternative, we have also considered  an AR$\mathbb{H}$(1) process with the same $\Gamma$ but evaluated in the square process, i.e.
$$Y_{t}(u)=\int \Gamma(u,v)\lrp{\frac{Y_{t-1}}{a_{t-1}}}^2(v)dv+\varepsilon_t(u),$$
where the parameter $a_{t-1}\in\{1, 2\}$ avoids that the process escapes to nonstationarity, being $a_{t-1}=2$ only when $\|Y_{t-1}\|>2$. Although, this is a clearly non AR$\mathbb{H}$(1) process, it is pretty close to an AR$\mathbb{H}$H(1) due to the scale of $Y_t$.
Now, the statistics in equation (\ref{eq:FBSH}) must change to incorporate the variability due to the estimation of parameter $\Gamma$. So, following \cite{Escanciano06}, $V_{n}^{*b}(x)$ is changed by $\widetilde{V}_{n}^{*b}(x)$ given by

\begin{equation}
	\widetilde{V}_{n}^{*b}(x)=
	\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\eta_i \left\langle Y^{*b}_{i}-\widehat{\Gamma}^{*b}_{n} (Y_{i-1}),\gamma_{\varepsilon }\right\rangle_{\mathbb{H}}\ind{\omega \in \Omega;\ \left\langle Y_{i-1}(\omega ),\gamma_{Y}\right\rangle_{\mathbb{H}}\leq \left\langle  x,\gamma_{Y}\right\rangle_{\mathbb{H}}},\label{eq:FBCH}
\end{equation}
\noindent where $\widehat{\Gamma}^{*b}_{n}$ is the estimation of the functional parameter and $Y^{*b}_{i}=\widehat{\Gamma}^{*b}_{n}(Y_{i-1})+\varepsilon^{*b}_i$ with $\varepsilon^{*b}_i$ being the resampled errors from the estimated model.

\begin{table}[ht]
	\centering
	\begin{tabular}{|lccccccc|}
		\hline
		$NP$ & 1 & 2 & 3 & 4 & 5 & 10 & 15 \\
		\hline
		$n=50$ & 0.050 & 0.054 & 0.044 & 0.038 & 0.034 & 0.034 & 0.042 \\
		$n=100$ & 0.060 & 0.056 & 0.054 & 0.062 & 0.058 & 0.060 & 0.056 \\
		$n=200$ & 0.050 & 0.054 & 0.056 & 0.052 & 0.048 & 0.052 & 0.040 \\
		\hline
	\end{tabular}
	\caption{Simple Hypothesis. Empirical test size based on $R=500$ repetitions. Number of projections by column, and functional sample size by rows.}
	\label{tabH0}
\end{table}




% latex table generated in R 4.4.0 by xtable 1.8-4 package
% Mon Jun  3 18:52:20 2024
\begin{table}[ht]
	\centering
	\begin{tabular}{|lccccccc|}
		\hline
		$NP$ & 1 & 2 & 3 & 4 & 5 & 10 & 15 \\
		\hline
		$n=50$ & 0.434 & 0.492 & 0.526 & 0.524 & 0.530 & 0.562 & 0.552 \\
		$n=100$ & 0.486 & 0.630 & 0.678 & 0.728 & 0.764 & 0.836 & 0.848 \\
		$n =200$ & 0.650 & 0.808 & 0.896 & 0.922 & 0.932 & 0.984 & 0.994 \\
		\hline
	\end{tabular}
	\caption{Simple Hypothesis. Empirical test  power based on $R=500$ repetitions. Number of projections by column, and functional sample size by rows.}
	\label{tabH1}
\end{table}


\begin{table}[ht]
	\centering
	\begin{tabular}{|lccccccc|}
		\hline
		$NP$ & 1 & 2 & 3 & 4 & 5 & 10 & 15 \\
		\hline
		$n=50$ & 0.054 & 0.034 & 0.034 & 0.048 & 0.056 & 0.038 & 0.032 \\
		$n=100$ & 0.056 & 0.062 & 0.058 & 0.050 & 0.068 & 0.046 & 0.046 \\
		$n=200$ & 0.050 & 0.050 & 0.052 & 0.046 & 0.054 & 0.048 & 0.048 \\
		\hline
	\end{tabular}
	\caption{Composite Hypothesis. Empirical test size based on $R=500$ repetitions. Number of projections by column, and functional sample size by rows.}
	\label{tabHC0}
\end{table}


\begin{table}[ht]
	\centering
	\begin{tabular}{|lccccccc|}
		\hline
		$NP$ & 1 & 2 & 3 & 4 & 5 & 10 & 15 \\
		\hline
		$n=50$ & 0.114 & 0.096 & 0.084 & 0.062 & 0.064 & 0.062 & 0.066 \\
		$n=100$ & 0.152 & 0.182 & 0.196 & 0.204 & 0.214 & 0.232 & 0.264 \\
		$n=200$ & 0.242 & 0.340 & 0.378 & 0.400 & 0.432 & 0.520 & 0.508 \\
		$n=300$ &0.302  & 0.438   & 0.528  & 0.562  & 0.584  & 0.666   & 0.706\\
		$n=500$ &  0.448 &  0.610 & 0.680 &  0.756 & 0.806 &  0.892 & 0.938\\
		$n=750$ & 0.474 & 0.682 &  0.774  &  0.830  &  0.888  & 0.970 & 0.992\\
		\hline
	\end{tabular}
	\caption{Composite Hypothesis. Empirical test power based on $R=500$ repetitions. Number of projections by column, and functional sample size by rows.}
	\label{tabHC1}
\end{table}




In Tables~\ref{tabH0} and ~\ref{tabH1}, one can respectively find the   $p$--values and power  approximations computed,  based on  $R=500$ repetitions of the implemented testing procedure,  for functional sample sizes $n=50,100,200.$
The Confidence Interval, based on $R=500$ repetitions, obtained for $p=0.05$ is $[0.031, 0.069].$ Specifically,  Table~\ref{tabH0} shows the rejection probability when  $H_{0}$ is true. One can observe  in Table~\ref{tabH0}  that the computed estimates of the test size are slightly below the true parameter  value  $p=0.05$   for the functional sample size $n=50$.
While, for  functional sample sizes $n=100, 200,$ an improvement is observed in the computed estimates of $p=0.05$ for all number of random projections.  For the functional sample sizes analyzed, no patterns are observed regarding computed estimates and the number of random projections.  The  empirical power of the test, for the same functional sample sizes $n=50,100,200,$ and number of repetitions, are displayed in Table~\ref{tabH1}. As  expected,  the  performance of the test is improved  when  the functional sample size $n$ increases. Tables~\ref{tabHC0} and ~\ref{tabHC1} show sizes and powers for the composite hypothesis. The test seems well calibrated as every element in Table~\ref{tabHC0} is inside the Confidence Interval for $p=0.05$ although, for $n=50,$ the test shows a tendency to be below $p=0.05$ similar to what happens in simple null hypothesis. Given the rate of convergence of the empirical eigenfunctions to the theoretical ones  (see Lemma 4.3 and Theorem 4.5 in \cite{Bosq2000}),  larger  functional sample sizes are required to obtain competitive empirical power values.
In Table  \ref{tabHC1}, we illustrate this fact considering additional  functional sample size values $n= 300, 500, 750.$




\subsection{Spherical functional time series SP$\mathbb{H}$AR(1)}
\label{sph1}
In this section we analyze the performance of the proposed GoF procedure in the special case where $\mathbb{H}=L^{2}(\mathbb{S}_{d},d\nu ,\mathbb{R})$ is  the space   of real--valued square integrable functions  on  the $d$--dimensional sphere in $\mathbb{R}^{d+1}.$
Here, $d\nu$  denotes the normalized Riemannian measure on $\mathbb{S}_{d}.$ In what follows, we will denote by
$\{S_{k,j}^{d}, \ j=1,\dots, \Lambda  (k,d),\ k\in \mathbb{N}_{0}\}$
the  orthonormal basis of   eigenfunctions of the Laplace--Beltrami operator $\Delta_{d}$ on $L^{2}\left(\mathbb{S}_{d},d\nu , \mathbb{R}\right),$ with $\Lambda (k,d)$ denoting the dimension of the $k$th eigenspace of the Laplace--Beltrami operator.  Here, we consider  $d+1=3$ and $d=2.$

The time--varying random projections of the SP$\mathbb{H}$AR(1) process, with repect to the eigenfunctions of the Laplace Beltrami operator,  have been generated by using the MatLab function \emph{arima}. They are evaluated at  $n$ time instants,  where $n$ denotes the functional sample size.  The  functional values of the  innovation process are also  generated from its time--varying projections with respect to such a basis at  $n$ times.

In the generations of SP$\mathbb{H}$AR(1) model we have considered the spherical harmonics $\{S^{2}_{0,0},S^{2}_{1,0}, S_{1,1}^{2},S_{2,1}^{2},S_{2,2}^{2},S_{3,1}^{2},S_{3,2}^{2},
S_{3,3}^{2}\},$ which are localized in the first four eigenspaces ($k=4$) of  the Laplace Beltrami operator $\Delta_{2}$  on $L^{2}\left(\mathbb{S}_{2},d\nu , \mathbb{R}\right)$ (see
left--hand--side of Figure \ref{f6app2ex3BS62}).
Under the simple null hypothesis $H_{0}=\Gamma_{0},$  one realization (evaluated at $100$ spherical spatial  nodes) of the SP$\mathbb{H}$AR(1) process projected into the corresponding direct sum  $\bigoplus_{k=0}^{3}\mathcal{H}_{k}$ of Laplace Beltrami eigenspaces   is   displayed at the right--hand--side of Figure \ref{f6app2ex3BS62}. Here, $\mathcal{H}_{k}$ denotes the $k$th eigenspace of the Laplace Beltrami operator on $L^{2}(\mathbb{S}_{2},d\nu ,\mathbb{R}),$ $k\in \mathbb{N}_{0}.$ The eigenvalues of the invariant kernel defining the autoregressive operator $\Gamma_{0}$ under simple  $H_{0},$ associated with the Laplace Beltrami  eigenfunctions,  are given by $\{\lambda_{k-1}(\Gamma_{0})=0.7\left(\frac{k+1}{k}\right)^{-3/2},\ k\geq 1\}.$


\begin{figure}[!h]
	\begin{center}
		\includegraphics[width=6.6cm,height=9cm]{Eigenfunctionbasis}
		\includegraphics[width=6.6cm,height=9cm]{SPHAR1} 		
	\end{center}
	\caption{Elements of the truncated orthonormal eigenfunction spherical basis (left--hand--side), and one realization of the functional values of the  generated SP$\mathbb{H}$AR(1) process, projected into  $\bigoplus_{k=0}^{3}\mathcal{H}_{k},$ at times $t=1,51, 101,151, 202,251, 301,351, 401,451,$ from a functional sample of size $n=500$ (right--hand--side)}
	\label{f6app2ex3BS62}
\end{figure}


We first consider  $H_{0}: \Gamma =\Gamma_{0}$ versus  $H_{1},$ where we  assume that the response is of the form
$Y_{t}^{H_{1}}=[\Gamma_{0}+\widetilde{\Gamma }_{\mathbb{S}_{2}}](Y^{H_{1}}_{t-1})+\varepsilon_{t}^{H_{1}},$
with $\varepsilon_{t}^{H_{1}}=\widetilde{\Gamma }_{\mathbb{S}_{2}}(\varepsilon_{t-1}^{H_{1}})+\eta_{t},$ for  $t\in \mathbb{Z},$  and $\left\{\eta _{t},\ t\in \mathbb{Z}\right\}$  being an $\mathbb{H}$--SWN. Thus,
the random components of the innovation process $\varepsilon $ still display significative correlations induced by $\widetilde{\Gamma }_{\mathbb{S}_{2}}.$
Both operators, $\Gamma_{0}$ and $\widetilde{\Gamma }_{\mathbb{S}_{2}},$ are   invariant bounded linear operators against the group of rotations in the sphere.  The test statistics (\ref{statistic}) is evaluated at different  random projections  generated from Fractional Brownian Motion (FBM).  In the generation of FBM we have used MatLab function \emph{wfbm}. The critical value corresponding to $\alpha =0.05$ is $SW_{\alpha}= 2.2414,$ for the supremum norm $SW$ of   Brownian motion $W_{t}$ on the interval $[0,1].$  As indicated, this value  is computed,
by applying  Reflection Principle, from the standard normal probability distribution (see Section \ref{test}).


Under simple null hypothesis (totally specified autocorrelation operator),  Table \ref{tabH0SPH}
displays the empirical test size for the functional sample sizes   $n=50, 100, 200.$ Again, no patterns are displayed. In particular,   no tendency to be below $\alpha =0.05$ is observed
for $n=50.$
In Table \ref{tabH1SPH},  one can find  the empirical test  power values, based on $R=500$ repetitions,  for the functional sample sizes   $n=50, 100, 200, 300, 500, 750.$  As in the previous  $\mathbb{H}=L^{2}([0,1])$ scenario analyzed in Table  \ref{tabH1}, one can observe in Table \ref{tabH1SPH} a better  performance of the test statistics when  the  functional sample size increases.

\begin{table}[ht]
	\centering
	\begin{tabular}{|lccccccc|}
		\hline
		$NP$ & 1 & 2 & 3 & 4 & 5 & 10 & 15 \\
		\hline
		$n=50$ & 0.060 & 0.040  & 0.040 & 0.060 & 0.040 & 0.040 & 0.060 \\
		$n=100$ & 0.040 & 0.040 & 0.040 & 0.060 & 0.060 & 0.060 &  0.040 \\
		$n=200$ & 0.050 & 0.054 & 0.056 & 0.052 & 0.048 & 0.052 & 0.040 \\
		\hline
	\end{tabular}
	\caption{Simple Hypothesis SP$\mathbb{H}$AR(1). Empirical test size based on $R=500$ repetitions. Number of projections by column, and functional sample size by rows.}
	\label{tabH0SPH}
\end{table}




%$n =600$ & 0.940 & 0.960 & 0.950 & 0.980 &  1.000 & 1.000 &   0.995 \\
\begin{table}[ht]
	\centering
	\begin{tabular}{|lccccccc|}
		\hline
		$NP$ & 1 & 2 & 3 & 4 & 5 & 10 & 15 \\
		\hline
		$n=50$ & 0.376   & 0.336 &0.718 &0.454 & 0.542 &  0.594&  0.584 \\
		$n=100$ &0.566 & 0.464 & 0.658  & 0.700    & 0.428   &   0.450  & 0.762 \\
		$n=200$  &   0.562 & 0.772 &0.806 & 0.772 &   0.744 &  0.882 & 0.732\\
		$n=300$ & 0.838 & 0.546 & 0.682&  0.944 &   0.864 & 0.912 & 0.892 \\
		$n=500$ & 0.890 & 0.998 & 0.914& 0.994 & 0.978 &   0.984 &   0.984\\	
		$n=750$ & 0.851 & 0.926 & 0.974 & 0.938 & 0.850 & 0.996 & 0.992 \\
		\hline
	\end{tabular}
	\caption{Simple Hypothesis SP$\mathbb{H}$AR(1). Empirical test  power based on $R=500$ repetitions. Number of projections by column, and functional sample size by rows.}
	\label{tabH1SPH}
\end{table}

\begin{table}[ht]
	\centering
	\begin{tabular}{|lccccccc|}
		\hline
		$NP$ & 1 & 2 & 3 & 4 & 5 & 10 & 15 \\
		\hline
		$n=50$ &   0.018 &   0.014 &  0.014   & 0.022       &    0.028 &  0.022 & 0.022\\
		$n=100$ &  0.032  &  0.034  &  0.026  &  0.022 &  0.034  &   0.030 &  0.024\\
		$n=200$ &  0.038 & 0.052 & 0.032 & 0.042 & 0.040 & 0.038 & 0.040 \\
		$n=300$ &
		0.042 &   0.054   &  0.032   & 0.026   & 0.056  &  0.040  & 0.042\\	
		$n=500$ & 0.046 & 0.046 & 0.050 & 0.054 & 0.046 & 0.044 & 0.044 \\
		$n=750$ &0.050 & 0.045 & 0.050 & 0.045 & 0.045&  0.045& 0.055\\
		\hline
	\end{tabular}
	\caption{Composite Hypothesis  SP$\mathbb{H}$AR(1). Empirical test size based on $R=500$ repetitions. Number of projections by column, and functional sample size by rows.}
	\label{tabHC0SPH1}
\end{table}


\begin{table}[ht]
	\centering
	\begin{tabular}{|lccccccc|}
		\hline
		$NP$ & 1 & 2 & 3 & 4 & 5 & 10 & 15 \\
		\hline
		$n=50$ & 0.636 & 0.706 & 0.522 & 0.506 &  0.484 &0.584 &  0.576\\
		$n=100$ &   0.668 & 0.736 &  0.630 & 0.652 & 0.556 & 0.806 & 0.678 \\
		$n=200$ & 0.824 & 0.870 &0.646& 0.912 & 0.612 &  0.062 &  0.638\\
		$n=300$ &  0.962 &0.826  &   0.670 & 0.902  & 0.882  & 0.914 &  0.764 \\
		$n=500$ & 0.994 &  0.966 & 0.998 & 0.978 & 0.986 & 0.918
		&   0.840 \\
		$n=750$ &   0.998     &  0.986  & 0.996 & 0.826 & 0.786 & 0.976   &  0.960 \\
		\hline
	\end{tabular}
	\caption{Composite Hypothesis  SP$\mathbb{H}$AR(1). Empirical test power based on $R=500$ repetitions. Number of projections by column, and functional sample size by rows.}
	\label{tabHC1SPH1}
\end{table}

Let us now consider, under the general  scenario given by condition (c$_{0}$) in \cite{Bosq2000}, ensuring existence and uniqueness of a stationary solution to the AR$\mathbb{H}$(1) equation (\ref{eq1}) (see \cite{Bosq2000}, p.74, Chapter 3),   the following testing problem:
\begin{eqnarray}
	&&\widetilde{H}_{0}: \|\Gamma_{0}\|_{\mathcal{L}(\mathbb{H})}\leq 1/4  \nonumber\\
	&&\widetilde{H}_{1}: \|\Gamma_{0}\|_{\mathcal{L}(\mathbb{H})}> 1/4,\nonumber\end{eqnarray}
\noindent where generations under $\widetilde{H}_{1}$ have been achieved for $\Gamma_{\mathbb{S}_{2}}$ with eigenvalues $\{\lambda_{k-1}(\Gamma_{\mathbb{S}_{2}})=0.5\left(\frac{k+1}{k}\right)^{-3/2},\ k \geq 1\}.$ Hence, under $\widetilde{H}_{1},$
$\|\Gamma_{1}\|_{\mathcal{L}(\mathbb{H})}$ \linebreak $= (0.7+0.5)[1/2]^{3/2}=0.4243>1/4.$
Generations under $\widetilde{H}_{0}$ have  been performed, as before,  under the pure point spectrum  $\left\{\lambda_{k-1}(\Gamma_{0})\right.$ \linebreak $\left.=0.7\left(\frac{k+1}{k} \right)^{-3/2},\ k\geq 1\right\}$ ensuring $\widetilde{H}_{0}$ is satisfied, since  $\|\Gamma_{0}\|_{\mathcal{L}(\mathbb{H})}=0.7[1/2]^{3/2}$ \linebreak $= 0.2475<1/4.$ Note that Lemma 8.1(3) in \cite{Bosq2000}  holds  under the generated scenario, since for $\alpha =2.01$ and $k_{n}=\left[\log(\log(n))+2.2\right],$  $\underline{\lim}\widehat{\frac{n\lambda_{k_{n}}^{8}}{(\log(n))^{\alpha }}}= 0.0038,$ computed from $n=10^{7}$  terms of the empirical sequence $\frac{n\widehat{\lambda}_{k_{n}}^{8}}{(\log(n))^{\alpha }}.$ Here, $[\cdot]$ denotes the nearest positive integer.

Tables~\ref{tabHC0SPH1} and ~\ref{tabHC1SPH1} show empirical test sizes and powers for  composite null hypothesis under a SP$\mathbb{H}$AR(1) scenario.
For the number of random projections, functional sample sizes and number of repetitions reflected in Tables~\ref{tabHC0SPH1} and ~\ref{tabHC1SPH1}, the values of the test statistics (\ref{statistic}) based on the plug--in empirical process (\ref{depbb}) are respectively computed under $\widetilde{H}_{0}$ and under $\widetilde{H}_{1}.$ The eigenvalues $\left\{\lambda_{k}(\Gamma_{0} ),\ k\in \mathbb{N}_{0}\right\}$ of $\Gamma_{0} $ are estimated from the following identity
(see equation (3.13) in  \cite{Bosq2000}):
\begin{eqnarray}
	&&C_{0}^{\varepsilon}=C_{0}^{Y}-\Gamma_{0}C_{0}^{Y}\Gamma_{0}^{\star},
	\label{eqemp}
\end{eqnarray}
\noindent where $\Gamma_{0} ^{\star}$ denotes the adjoint of $\Gamma_{0} ,$ with
$\Gamma_{0}=\Gamma_{0}^{\star}$ in the generations.
Given the diagonal spectral factorization in terms of the common resolution of the identity
$\sum_{k\in \mathbb{N}_{0}}\sum_{j=1}^{\Lambda (k,d)} S_{k,j}^{d}\otimes S_{k,j}^{d}$
of operators in   equation  (\ref{eqemp}), the associated pure point spectra satisfy
\begin{equation}\lambda_{k}(\Gamma_{0})=
	\left[1-\lambda_{k}(C_{0}^{\varepsilon})[\lambda_{k}(C_{0}^{Y})]^{-1}
	\right]^{1/2},\quad k\in \mathbb{N}_{0}.\label{pps}
\end{equation}

\noindent  Equation  (\ref{pps})  is then approximated  in terms of the following empirical pure point spectra:
\begin{eqnarray}&&
	\widehat{\lambda}_{k}(C_{0}^{\varepsilon})=\frac{1}{\Lambda  (k,d)}\sum_{j=1}^{\Lambda  (k,d)}
	\frac{1}{n}\sum_{i=1}^{n}\frac{1}{R_{2}}\sum_{l=1}^{R_{2}}\left[\left\langle \varepsilon_{i,l},S_{k,j}^{d}\right\rangle_{L^{2}(\mathbb{S}_{2}, d\nu)}\right]^{2},\ k\in \mathbb{N}_{0},\nonumber \\
	&&\widehat{\lambda_{k}}(C_{0}^{Y})=\frac{1}{\Lambda  (k,d)}\sum_{j=1}^{\Lambda (k,d)}
	\frac{1}{n}\sum_{i=1}^{n}\frac{1}{R_{2}}\sum_{l=1}^{R_{2}}\left[\left\langle Y_{i,l},S_{k,j}^{d}\right\rangle_{L^{2}(\mathbb{S}_{2}, d\nu)}\right]^{2},\ k\in \mathbb{N}_{0},\nonumber \\
	\label{epps}
\end{eqnarray}
\noindent where, for $i=1,\dots,n,$   $\{Y_{i,l},\ l=1,\dots,R_{2}\}$ and $\{\varepsilon_{i,l},\ l=1,\dots,R_{2}\}$ denote the generated  $R_{2}=100$ repetitions  of $Y_{i}$ and  $\varepsilon_{i}$ under $\widetilde{H}_{0},$ respectively, for the estimation of $\Gamma $  under $\widetilde{H}_{0},$ in the SP$\mathbb{H}$AR(1) model. Note that, from equations (\ref{pps})--(\ref{epps}),  considering  $\varphi =S_{k,j}^{d},$ $j=1,\dots,\Lambda  (k,d),$ $k\in \mathbb{N}_{0},$ in
the a.s. convergence in $\mathbb{H}$--norm   in Lemma 8.1(3) in \cite{Bosq2000}, the a.s. convergence  of the eigenvalues of $\widehat{\Gamma}_{n}$ (computed from   (\ref{pps})--(\ref{epps})) to the eigenvalues of $\Gamma_{0}$ satisfying (\ref{pps}) is obtained.






In Table~\ref{tabHC0SPH1}, we observe empirical test sizes close to the theoretical value $\alpha =0.05$ for the sample sizes $n=200, 300,500,750.$   The empirical test powers showed in Table~\ref{tabHC1SPH1} are also  computed for the functional sample sizes   $n=50,100,200, 300, 500, 750,$ considering the same number of repetitions $R=500.$  As expected, the empirical test powers are improved when   the functional sample size increases. A better performance in the case of composite  hypothesis is observed under a lower level of misspecification of the autocorrelation operator $\Gamma $ for small sample sizes ($n=50,100, 200$). That is the case of the SP$\mathbb{H}$AR(1) scenario, where the eigenvalues are unknown but the eigenfunctions are known, in contrast with the scenario where the eigenfunctions and eigenvalues are unknown  analyzed  in Table \ref{tabHC1} for  $\mathbb{H}=L^{2}([0,1]).$ Under simple $H_{0}$ the reverse  situation is observed in Tables \ref{tabH1}  and \ref{tabH1SPH}. Although, given the variability displayed
by columns in these two tables, for the sample sizes $n=50,100, 200,$ one can not conclude that these numerical results establish any significative performance difference between the two testing problems.


\end{document}


