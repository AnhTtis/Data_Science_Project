\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\inprod}[2]{\langle#1,#2\rangle}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage[capitalize]{cleveref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{10960} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

% Support for easy cross-referencing
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

%%%%%%%%% TITLE
\title{4D Panoptic Segmentation as Invariant and Equivariant Field Prediction}

\author{
Minghan Zhu$^{1,}$\thanks{Work done at Qualcomm AI Research during an internship.}
\quad\quad
Shizong Han$^2$
\quad\quad
Hong Cai$^2$
\quad\quad
Shubhankar Borse$^2$
\and
Maani Ghaffari Jadidi$^{1,*}$
\quad\quad
Fatih Porikli$^2$\\\\
{$^1$University of Michigan, Ann Arbor
\quad\quad
$^2$Qualcomm AI Research\thanks{Qualcomm AI Research, an initiative of Qualcomm Technologies, Inc.} }
}
\maketitle
% Remove page # from the first page of camera-ready.
%\ificcvfinal\thispagestyle{empty}\fi


\newcommand{\hc}[1]{\textcolor{magenta}{[HC: #1]}}
\newcommand{\mhz}[1]{\textcolor{blue}{[Minghan: #1]}}


%%%% Let's see if we can fit it without squeezing the space. 
% \setlength{\belowdisplayskip}{5pt} 
% \setlength{\abovedisplayskip}{5pt} 

%%%%%%%%% ABSTRACT
\begin{abstract}\vspace{-3pt}
In this paper, we develop rotation-equivariant neural networks for 4D panoptic segmentation. 4D panoptic segmentation is a recently established benchmark task for autonomous driving, which requires recognizing semantic classes and object instances on the road based on LiDAR scans, as well as assigning temporally consistent IDs to instances across time. We observe that the driving scenario is symmetric to rotations on the ground plane. Therefore, rotation-equivariance could provide better generalization and more robust feature learning. Specifically, we review the object instance clustering strategies, and restate the centerness-based approach and the offset-based approach as the prediction of invariant scalar fields and equivariant vector fields. Other sub-tasks are also unified from this perspective, and different invariant and equivariant layers are designed to facilitate their predictions. Through evaluation on the standard 4D panoptic segmentation benchmark of SemanticKITTI, we show that our equivariant models achieve higher accuracy with lower computational costs compared to their non-equivariant counterparts. Moreover, our method sets the new state-of-the-art performance and achieves 1st place on the SemanticKITTI 4D Panoptic Segmentation leaderboard. 

% \hc{Any other highlights that we should mention in abstract?}

% In this paper, we propose to conduct 4D panoptic segmentation task using rotation-equivariant neural networks. In this task, we retrieve the semantic and instance information from Lidar scans and assign temporally consistent IDs to instances. Giving the diverse environmental variations in driving scenarios and the captured Lidar data, we believe that rotational equivariance can bring better generalization and more robust feature learning to this task. More specifically, we enable the equivariance to a cyclic group, which is a discretization of SO(2), the rotations in the ground plane. To maximize the benefit of using equivariant models, we found that it is important to formulate the outputs as equivariant vector fields rather than invariant scalar fields. When scalar fields are unavoidable, we also investigated different designs on the invariant pooling layer to optimize the performance. Our equivariant models can achieve higher accuracy with lower computational costs compared with their non-equivariant counterparts. We reached the 1st place in the SemanticKITTI 4D Panoptic Segmentation benchmark. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}\label{sec:intro}
\vspace{-3pt}
Perception with LiDAR point clouds is an important part of building real-world autonomous systems, for example, self-driving cars. As the computer vision community gradually builds more capable neural networks, the tasks also become more complex. 4D panoptic segmentation~\cite{aygun20214d} is an emerging task that combines several previously independent tasks: semantic segmentation, instance segmentation, and object tracking, in a unified framework, given sequential LiDAR scans. As the output provides abundant useful information for understanding the dynamic driving environment, solving this task has significant practical value.  

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{title_fig2.png}
    \caption{Predicting centerness or offset is a critical step in estimating object centers, as part of the overall panoptic segmentation. A centerness heatmap can be viewed as an invariant field, while the offset can be viewed as an equivariant vector field. In our proposed approach, we devise different, corresponding methods to predict invariant and equivariant fields, respectively.}
    % {Centerness and offset prediction are two methods to estimate object centers, which is a key step in panoptic segmentation. They can be viewed as invariant and equivariant fields respectively, resulting in different prediction strategies and performance when applying equivariant models. }
    \label{fig:ctr_vs_off}
    % \vspace{-10pt}
\end{figure}

While there are a few existing methods to solve 4D panoptic segmentation~\cite{aygun20214d, kreuzberg20234d}, they ignore the rich, inherent symmetries present in this task. For example, for the point cloud of an object instance, its center is invariant to rotations, and the offset vector from any point on the object to the center is attached to and thus rotates along with the body frame. See Fig.~\ref{fig:ctr_vs_off} for a visual illustration.

As such, in this paper, we propose to develop equivariant neural networks to solve 4D panoptic segmentation. Equivariant networks~\cite{cohen2016group} are deep learning models that are guaranteed to render outputs that respect the symmetries in data. For the 4D panoptic segmentation task, rotational equivariance can help the model perform consistently and generalize over rotations in the input data. 

% While there are a few existing methods to solve 4D panoptic segmentation~\cite{aygun20214d, kreuzberg20234d}, they ignore the rich, inherent equivariance and invariance (which is a special case of equivariance) properties present in this prediction task. For instance, the semantic class label of a 3D point is invariant to any transformation applied to this point. For the point cloud of an object instance, its center is invariant to rotations and the offset of a 3D point in this point cloud to the object center is equivariant to rotations; see Fig.~\ref{fig:ctr_vs_off} for a visual illustration.

% As such, in this paper, we propose equivariant neural networks to solve 4D panoptic segmentation. Equivariant networks~\cite{cohen2016group} are deep learning models that preserves certain types of transformations. In other words, when the input undergoes a transformation of such a type, the network output is guaranteed to transform accordingly. 
% This also implies that the fitting of one input-output pair generalizes to all input-output pairs that are transformed versions of the fitted pair. 
% For the 4D panoptic segmentation task, the real-world outdoor environment can contain vast variations of the data. Therefore, equivariant models allow us to reduce the sampling complexity and make it easier for the network to fit the varying inputs. 

% In this paper, we consider incorporating equivariant networks to solve this task. Equivariant networks \cite{cohen2016group} are deep learning models that preserves certain types of transformations. In other words, when the input undergoes a transformation of such type, the network output is guaranteed to transform accordingly. 
% % It implies that the network generalizes over such transformations. 
% It also implies that the fitting of one input-output pair generalizes to all input-output pairs that are transformations of the fitted pair. 
% % fitting one data point is equivalent to fitting all data points that are generated by the one data point through this type of transformations. 
% For the 4D panoptic segmentation task, the real-world outdoor environment can present a lot of variations. Therefore, equivariant models allow us to reduce the sampling complexity and make it easier for the network to fit the varying inputs. 

While equivariance is a nice property, equivariant models can be complex and incur high computational costs~\cite{zhu20222, yu2022rotationally, wu2022transformation}. As a result, most existing equivariant models are only applied to small-scale problems, such as molecular analysis and single-object perception \cite{fuchs2020se}. 
Recent works have looked into more efficient equivariant networks~\cite{zhu20222} and applications in larger problems \cite{yu2022rotationally}, but significant performance improvement without extra computational cost has not been achieved in large-scale equivariant perception solutions. 

% , but have not used them for large problems.    
% In this work, we propose an efficient equivariant approach, for the first time, to tackle a complex large-scale, real-world perception task, which is 4D panoptic segmentation. 

In this work, SO(2)-equivariance is incorporated in the 4D segmentation model. 
% In particular, rotations in the ground plane are ubiquitous in outdoor driving scenarios. As such, we propose to incorporate SO(2)-equivariance in the 4D segmentation model and investigate the optimal strategy to leverage the equivariant property for this specific task. 
We find that the equivariance brings consistent improvements in different performance metrics and that formulating the output as equivariant vector fields helps maximizing the benefits of equivariant models, as compared to restricting to invariant scalar fields (see \cref{fig:ctr_vs_off}). 
Furthermore, our equivariant networks can improve the segmentation performance while, at the same time, reducing computational costs. With our proposed design, we outperform the non-equivariant models and, notably, achieve the \textbf{top-1} ranking position in the SemanticKITTI benchmark (at the time of this submission).  

% Particularly, rotations in the ground plane are ubiquitous in the outdoor driving scenario. Therefore, we propose to incorporate SO(2)-equivariance in the 4D segmentation model. We investigate the optimal strategy to leverage the equivariant property to serve this specific task. 
% We find that equivariance brings improvements in different metrics universally, and that formulating the output as equivariant vector fields helps maximizing the benefits of equivariant models, compared with invariant scalar fields. 
% % We find that it is more beneficial to formulate the output as equivariant vector fields than invariant scalar field. However, scalar outputs are unavoidable due to the nature of the problem, in which case we found that a simple average-pooling layer yields the best results among various invariant layers. 
% We also show that equivariant networks can improve the segmentation performance while saving computational cost at the same time. As a result, we achieved the \textbf{1st} place in the SemanticKITTI benchmark at the time of our submission. 

Our main contributions in this paper are as follows:
\begin{itemize}
    \vspace{-3pt}
    \item We develop the first rotation-equivariant model for 4D panoptic segmentation. Our proposed model enables more generalizable and robust feature learning, while being computationally efficient.
    \vspace{-3pt}
    \item We investigate different strategies and propose careful designs to construct the equivariant architecture, including the encoder, the decoder, and prediction heads. Specifically, we discover the advantage of formulating prediction targets as equivariant vector fields, as compared to only invariant fields.
    \vspace{-3pt}
    \item By evaluating on the SemanticKITTI benchmark, we show that our equivariant models significantly outperform 
    % their non-equivariant counterparts as well as other 
    existing methods, validating the value of equivariance in this large-scale perception task. %This validates our proposed equivariant solution for the large-scale complex perception task of 4D panoptic segmentation.
\end{itemize}


\section{Related work} \label{sec:literature}\vspace{-3pt}

\subsection{LiDAR 3D and 4D Panoptic segmentation}\label{sec:lit_seg}\vspace{-3pt}
\paragraph{3D Panoptic segmentation}
The task of panoptic segmentation was first proposed in the image domain, and was later extended to LiDAR point clouds after a large-scale outdoor LiDAR point cloud dataset, SemanticKITTI, was published with panoptic labels \cite{behley2021benchmark}. Similar to the semantic segmentation~\cite{borse2021inverseform, borse2021hs3, hu2022learning, borse2023dejavu, borse2023x} and panoptic segmentation techniques in the image domain \cite{mohan2021efficientps, yang2019deeperlab, borse2022panoptic}, their 3D counterparts can be classified into proposal-based and proposal-free methods. Proposal-based methods \cite{behley2021benchmark, sirohi2021efficientlps} require an object detection module to locate the objects first and then predict the instance mask for each bounding box and conduct semantic segmentation on the background pixels. This strategy needs to deal with potential conflicts among the segmentations. 
% Early baseline methods \cite{behley2021benchmark} are in this category. The performance was largely improved in \cite{sirohi2021efficientlps} with distance-aware convolution on range images. 
More methods fall into the other category, proposal-free methods, which conduct semantic segmentation first and then cluster the points belonging to different instances. A lot of research efforts focused on clustering strategies, as it impacts overall efficiency and performance. Offset prediction and central heatmap prediction are two major approaches for this. \textit{Offset prediction} \cite{hong2021lidar, xu2022sparse} means that each point predicts the offset to the center position of the instance it belongs to, and the clustering is conducted on the predicted centers. \textit{Heatmap prediction} \cite{aygun20214d} is to regress the closeness to the center of instances at any spatial location. Then the local maximums are used to cluster the points nearby. These two strategies can also be combined \cite{zhou2021panoptic, li2022panoptic}. Other clustering methods also exist. For example, \cite{gasperini2021panoster} proposed an end-to-end clustering layer. \cite{razani2021gp} used a graph network to cluster over-segmented points. % into instances. 

\vspace{-3pt}
\paragraph{4D Panoptic segmentation}
The 4D task is to provide temporally consistent object IDs on top of 3D panoptic segmentation. MOPT \cite{hurtado2020mopt} is an early attempt to provide tracking ID for panoptic outputs. 4D-PLS \cite{aygun20214d} proposed a strong baseline method and evaluation metrics for this task. It accumulates point clouds in sequential timestamps to a common frame and applies segmentation on the aggregated point clouds. 4D-DS-Net \cite{hong2022lidar} and 4D-StOP \cite{kreuzberg20234d} also followed this pipeline but used different clustering strategies. 4D-PLS predicted the closeness to the instance centers for each point. 4D-DS-Net \cite{hong2022lidar} and 4D-StOP \cite{kreuzberg20234d} predicted the offset to the instance center from each point. CA-Net \cite{marcuzzi2022contrastive} directly used an off-the-shelf 3D panoptic segmentation network and learned the temporal instances association through contrastive learning. 

\subsection{Equivariant Learning}
\vspace{-3pt}
\paragraph{Equivariant neural networks}
% Equivariant networks are a type of geometric deep learning that can efficiently handle data with underlying symmetries. 
The equivariance to translations of CNNs enabled the generalization to image content translations with much fewer parameters compared with fully connected networks. Equivariant networks generalize the symmetries to rotations, reflections, permutations, etc. G-CNN \cite{cohen2016group} was proposed to enable equivariance to 90-degree rotations of images. Steerable CNNs extend the symmetry to continuous rotations \cite{weiler2018learning}. The input is also extended from 2D images to spherical \cite{cohen2018spherical, esteves2018learning} and 3D data \cite{weiler20183d}. To deal with infinite groups (e.g., continuous rotations), generalized Fourier transforms and irreducible representations are adopted to formulate convolutions in the generalized domains \cite{worrall2017harmonic, thomas2018tensor}. Equivariant graph networks \cite{satorras2021n} and equivariant transformers \cite{fuchs2020se} were also proposed as non-convolutional equivariant layers. Equivariant models have applications in various areas such as physics, chemistry, and bioimaging \cite{thomas2018tensor, fuchs2020se}, where symmetries have an important role. They also attracted research interest in robotic applications. 
% For example, 3D geometric data naturally inherit symmetry to rigid body transformations. 
For example, equivariant networks with SE(3)-equivariance are developed to process 3D data such as point clouds \cite{chen2021equivariant}, meshes \cite{de2020gauge}, and voxels \cite{weiler20183d}. However, due to the added complexity, most equivariant networks for 3D perception are restricted to relatively simple tasks with small-scale inputs, such as object-wise classification, registration, part segmentation, and reconstruction \cite{sajnani2022condor, zhu2022correspondence, chatzipantazis2022se}. In the following, we will review recent progress in extending equivariance to large-scale outdoor 3D perception tasks. 

\vspace{-3pt}
\paragraph{Equivariant networks in LiDAR perception}
LiDAR perception incurs two requirements on the equivariant models. First, outdoor scene perception requires sophisticated network design. As existing models are mostly non-equivariant, it is beneficial to have equivariant layers with similar or compatible structures to the conventional network layers to leverage the successful design of existing work. Second, the sizes of LiDAR point clouds are large. Therefore, it is important to have an expressive equivariant model with memory footprints under the limit of a regular GPU. 

Existing work mainly follows two strategies. With the first strategy, inputs are projected to rotation invariant features using local reference frames \cite{li2023improving, xie2023general}. In this way, the changes are mainly at the first layer of the network, causing limited memory overhead. The main drawback is that the invariant feature could cause information loss and limit performance. The second strategy adopts group convolution, by augmenting the domain of feature maps to include rotations \cite{yu2022rotationally, wu2022transformation}. While achieving improved performance with the help of equivariance, they consumed twice \cite{wu2022transformation} or four times \cite{yu2022rotationally} of memory as their non-equivariant counterparts due to the augmented dimension of the feature maps and convolutions. With the two strategies, equivariant networks have been applied in 3D object detection \cite{yu2022rotationally, wu2022transformation, xie2023general} and semantic segmentation \cite{li2023improving}.

% In this work, we show that by leveraging more efficient 
% While being more expensive, the models are more expressive, and the complexity can be reduced with special designs in the equivariant networks, including separable convolution \cite{chen2021equivariant}, symmetric kernels, and quotient representations \cite{zhu20222}. 


% \textbf{4D Panoptic LiDAR Segmentation:}
% 4D Panoptic point cloud Segmentation~\cite{aygun20214d, kreuzberg20234d} is a recent topic of research which unifies semantic segmentation, instance segmentation and tracking of LiDAR point clouds into a single framework. Traditional works used feature extractors and classifiers~\cite{agrawal2009mmm} to tackle this problem. More recent works propose an architecture consisting of three heads that tackle semantic segmentation, instance segmentation and tracking
% tasks individually. The current SOTA method~\cite{kreuzberg20234d} produces spatio-temporal proposals in a bottom-up approach to jointly assign semantic labels and associate instances over time.
% copied a few lines from 4D Stop, please rewrite.

% \textbf{Equivariant Neural Network:}



% \section{Overview of the idea}
\section{Equivariance for 4D Panoptic Segmentation}
\vspace{-3pt}
In this section, we provide preliminaries on equivariance and formulate the dense prediction task from the perspective of equivariant learning. We discuss how to learn equivariant features and fit equivariant prediction target fields, which leads to the design choices in our proposed architecture.

% We introduce how we formulate a dense prediction task from the perspective of equivariant learning, and how it leads to the design choices in our network. We abstract the network details and directly build connections between the equivariance property and the dense prediction targets, which were usually skipped or taken for granted in literature. We found this point of view beneficial for investigating the optimal design for incorporating equivariance with our task. 

\subsection{Preliminaries on Equivariance}
\vspace{-3pt}
\paragraph{Feature maps as fields}
A feature map $f_0$ is a \textit{field}, assigning a value to each point in some space. In the context of point cloud perception, we have $f_0: \mathbb{R}^3\rightarrow V$, where $V$ is some vector space. 
This map can represent the geometry of the point cloud, i.e., $f_0(x)=1$ for every point $x$ in the point cloud and $f_0(x)=0$ otherwise. It can also represent point properties or arbitrary learned point features. For example, the feature map $f_0: x \mapsto SC(x)$, where $SC(x)$ is the semantic class label of the point $x$, represents the semantic segmentation of a point cloud. We denote the space of all such feature maps as $\mathcal{F}_0$. 

% It could represent the geometry of the point cloud, i.e., $f_0(x)=1$ for every point $x$ in the point cloud. It could also represent point properties or arbitrary learned point features. For example, the feature map $f_0: x \mapsto SC(x)$ where $SC(x)$ is the semantic class label of the point $x$ represents the semantic segmentation of a point cloud. We denote the space of all such feature maps $\mathcal{F}_0$.

\vspace{-3pt}
\paragraph{Invariant and equivariant fields}
For the \textit{group} of transformations $G$, we consider the rotation group in this paper. In this section, we use $\mathrm{SO}(3)$ for the formulation, for which $\mathrm{SO}(2)$ is also valid. 

% \paragraph{Invariant and equivariant features}
A rotation $R\in \mathrm{SO}(3)$ can be applied to a point or to a feature map. A \textit{rotated feature map} $[Rf_0]$ is simply the feature map of the rotated point cloud. A point at $x$ becomes $Rx$ after the rotation, thus $f_0(x)$ and $[Rf_0](Rx)$ are the features of the same point before and after rotating the point cloud. The relation between them depends on the property of $V$. For instance, if $V=SC$, then we know 
% Now we introduce the \textit{group} of transformations $G$. 
% % A group is a set with a binary operator that is associative and closed in the set. The set must have an inverse element for each element and must have an identity element. 
% In this paper, we consider the rotation group. In this section we use $\mathrm{SO}(3)$ for the formulation, which is also valid for $\mathrm{SO}(2)$. 
% % \paragraph{Invariant and equivariant features}
% A rotation $R\in \mathrm{SO}(3)$ can be applied on a point or on a feature map. A \textit{rotated feature map} $[Rf_0]$ is simply the feature map of the rotated point cloud. A point at $x$ goes to $Rx$ after the rotation, thus $f_0(x)$ and $[Rf_0](Rx)$ are the features of the same point before and after rotating the point cloud. The relation between them depends on the property of $V$. For example, if $V=SC$, then we know 
\begin{equation}\label{eq:inv}
    [Rf_0](Rx)=f_0(x),\quad \forall R\in\mathrm{SO}(3),
\end{equation}
i.e., the rotation does not change the semantic class.

As another example, if $V$ represents the surface normal vector, then we have 
\begin{equation}\label{eq:equiv}
    [Rf_0](Rx)=Rf_0(x),\quad \forall R\in\mathrm{SO}(3),
\end{equation}
where on the right-hand side, $R$ is applied to $f_0(x)\in\mathbb{R}^3$, meaning that the normal vector of a given point rotates along with the point cloud.

In both cases, the feature map of a rotated point cloud, $[Rf_0]$, can be generated by the feature map of the point cloud before rotation, $f_0$. This property is called \textit{equivariance}. In the case of \cref{eq:inv}, we call $f_0$ an \textit{invariant scalar field}. In the case of \cref{eq:equiv}, we call $f_0$ an \textit{equivariant vector field}.  

\vspace{-3pt}
\paragraph{Learning equivariant features}
To conduct a dense prediction task on a point cloud is to reproduce the target field (e.g., $x\mapsto SC(x)$ where semantic class labels are the targets) using a feature map realized by a neural network. Naturally, it would be helpful to equip the network with the same invariant and/or equivariant properties as the target field. However, a general feature map learned by a network is typically neither invariant nor equivariant to rotations. To fix this, we can augment the space of feature maps to $\mathcal{F}=\{f: \mathbb{R}^3\times \mathrm{SO}(3)\rightarrow V\}$, defined by
\begin{equation}\label{eq:def}
    f(x, R) := [R^{-1}f_0](R^{-1}x)
\end{equation}
for some $f_0\in \mathcal{F}_0$, i.e., the augmented feature map at rotation $R$ equals the original feature map of the point cloud rotated by the inverse of $R$. In this way, we have $\forall R, R'\in \mathrm{SO}(3)$, 
\begin{equation}\label{eq:eq}
    [Rf](Rx, R')\!=\![{R'}^{-1}\!R f_0]({R'}^{-1\!}Rx)\!=\!f(x, R^{-1}\!R'),
\end{equation}
which means that the augmented feature map of a rotated point cloud, $[Rf]$, can be generated by the augmented feature map of the original point cloud $f$, indicating that $f$ is equivariant. As has been shown in the literature, equivariant feature maps satisfying \cref{eq:def} can be constructed using group convolutions~\cite{cohen2016group, chen2021equivariant, zhu20222}.

% \subsection{Generalizability from equivariance} 
\subsection{Fitting Equivariant Targets} \label{sec:fitinvequiv}
\vspace{-3pt}
Now we can use the learned equivariant feature map $f$ to fit the target field $f_{gt}\in \mathcal{F}_0$, which is invariant or equivariant. 
For example, in semantic segmentation, the target field $f_{gt}(x)=SC(x)$ is an invariant scalar field. 
Suppose that we can fit $f_{gt}$ using $f$, then $[Rf]$, the feature map of a rotated point cloud, should automatically fit $[Rf_{gt}]$ which is the target field of the rotated point cloud. In other words, equivariance enables the \textit{generalization} over rotations. Next, we introduce two strategies to achieve this.

% The goal is that if we can fit $f_{gt}$ using $f$, then $[Rf]$, the feature map of a rotated point cloud, should automatically fit $[Rf_{gt}]$, the target field of the rotated point cloud. In other words, we desire \textit{generalization} over rotations enabled by the equivariance. Here we introduce two strategies to achieve this goal.

\vspace{-3pt}
\paragraph{Rotational coordinate selection}
Assuming that we can fit the target field of a point cloud without rotation: $f(x, I) = f_{gt}(x)$, where $I$ is identity rotation. We want to show that the fitting generalizes to the rotated point clouds. 

When $f_{gt}$ is an invariant scalar field, from \cref{eq:eq}, we have
\begin{equation}
    [Rf](Rx, R) = f(x, I) = f_{gt}(x) = [Rf_{gt}](Rx),
\end{equation}
meaning that the feature map $[Rf]$ at the rotational coordinate $R$, $[Rf](\cdot, R)$, fits the target $[Rf_{gt}]$ for the rotated point cloud. 

When $f_{gt}$ is an equivariant vector field, we have
\begin{equation}
    [Rf](Rx, R)\! =\! f(x, I)\! =\! f_{gt}(x)\! =\! R^{-1}[Rf_{gt}](Rx),
\end{equation}
which means that we need to apply a rotational matrix multiplication on features in $[Rf](\cdot, R)$ to fit $[Rf_{gt}]$, i.e., $[Rf_{gt}](Rx) = R[Rf](Rx, R), \forall x\in \mathbb{R}^3$.

The analysis above shows that the fitting of $f\in\mathcal{F}$ to $f_{gt}\in\mathcal{F}_0$ generalizes over rotations, if the rotational coordinate (i.e., the second argument in $f$) is the same as the actual rotation of the point cloud. 
However, the actual rotation $R$ is usually \textit{unknown} during inference; thus needs to be learned. In practice, this is formulated as a \textit{rotation classification} task to select the best rotational channel in a feature map $f$. We will discuss how we instantiate this in the architecture in \cref{sec:head}. 

% The analysis above shows that the fitting between $f\in\mathcal{F}$ and $f_{gt}\in\mathcal{F}_0$ generalizes over rotations, if we pick the rotational coordinate (the second argument in $f$) corresponding to the actual rotation of the point cloud. However, the actual rotation $R$ is usually \textit{unknown} during inference, thus needs to be learned. In practice, it corresponds to a \textit{rotation classification} task to pick the best rotational channel in a feature map $f$. More details are explained in \cref{sec:head}. 

% Ideally, we should learn $f$ to fit some $f_{gt}\in \mathcal{F}_0$. If $f_{gt}$ is invariant, plugging \cref{eq:inv} into \cref{eq:def}, we get 
% \begin{equation}
%     f_{gt}(x) = f(x, I) = f(x, R), \forall R\in\mathrm{SO}(3)
% \end{equation}
% where $I$ is the identity. If $f_{gt}$ is equivariant, then we have 
% \begin{equation}
%     f_{gt}(x) = f(x, I) = Rf(x, R), \forall R\in\mathrm{SO}(3)
% \end{equation}

\vspace{-6pt}
\paragraph{Invariant pooling layer}
% For an invariant target field, $f_{gt}$, there is also another fitting strategy. For equivariant feature maps $f$ satisfying \cref{eq:def}, we can build an rotation-invariant layer by marginalizing the second argument of $f$: 
When the target field $f_{gt}$ is invariant, there is another fitting strategy. For equivariant feature map $f$ that satisfies \cref{eq:def}, we can build a rotation-invariant layer by marginalizing over the second argument of $f$, i.e.,
\begin{equation}
    f_{inv}(x) = \prod_R f(x, R),
\end{equation}
where $\prod$ denotes any summarizing operator symmetric to all its arguments, e.g., sum, average, and max.

If we can fit $f_{inv}(x)=f_{gt}(x)$, then based on \cref{eq:eq}, we have
\begin{equation}
\begin{split}
    [Rf_{inv}](Rx) &= \prod_{R'} [Rf](Rx, R') = \prod_{R'} f(x, R^{-1}R') \\
    &= f_{inv}(x) = f_{gt}(x) = [Rf_{gt}](Rx),
\end{split}
\end{equation}
which indicates that generalization over rotations holds. In \cref{sec:head}, we discuss different options of implementing the invariant layer in the network. 

\subsection{Equivariant Instance Segmentation}
\vspace{-3pt}
In instance segmentation, the intermediate and final prediction targets can be modeled as invariant or equivariant fields. Centerness regression and offset regression are two predominately used approaches to estimate the object centers. Centerness represents the closeness to the instance center, with a point closer to the center having a larger centerness value. The offset represents the displacement vector from a point to the instance center. \cref{fig:ctr_vs_off} illustrates both approaches in the context of equivariant learning. Specifically, we can see that \textit{centerness is an invariant scalar field} and \textit{offset is an equivariant vector field}. As a result, we propose different prediction layers, as we will discuss in \cref{sec:head}, resulting in different performance shown in \cref{sec:exp_res}.

% For our task, the invariant and equivariant target fields have specific correspondences in the instance segmentation. As discussed in \cref{sec:lit_seg}, centerness regression and offset regression are two major approaches to estimate the object centers in instance segmentation. The centerness represents the clossness to the instance center. A point closer to the center has larger centerness value. The offset represents the displacement vector from a point to the instance center. \cref{fig:ctr_vs_off} illustrates both methods in the context of equivariant learning. We can see that \textit{the centerness is an invariant scalar field}, and \textit{the offset is an equivariant vector field}, thus requiring different prediction layers as discussed in \cref{sec:fitinvequiv}, resulting in different performance as shown in \cref{sec:exp_res}.
% More details are introduced in \cref{sec:head}, and we can see their performance difference in \cref{sec:exp_res}.









\section{Proposed Network Architecture}\label{sec:method}
\vspace{-3pt}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{overview3_small.png}
    \caption{Overview of the network structure. All prediction targets can be classified as equivariant (offsets) and invariant (others). Invariant predictions use features from the invariant pooling layer. Equivariant predictions use features from the rotational coordinate selection. }
    \label{fig:overview}
\end{figure*}

\subsection{Network Overview}\vspace{-3pt}
\paragraph{Discretized SO(2)-equivariance} 
The group equivariance for the model should cover the actual transformations presented in the data, but a group too large or too general can incur significant computational overhead with limited performance gain. For the outdoor driving scenario, we choose the $\mathrm{SO}(2)$ group as the designed equivariance space, representing planar rotations around the gravity axis. 

We discretize $\mathrm{SO}(2)$ to a finite group and implement a group CNN equivariant to the discretized rotations instead of continuous rotations. 
This is because equivariant models for finite groups have simpler structures similar to conventional deep learning models. As a result, they can be built upon existing SOTA task-specific networks. Since deep networks are capable of approximating nonlinear maps locally, the equivariance gaps due to discretization are expected to be interpolated by the network through training. For this reason, we still use rotational data augmentation in training. 

% In this work, we choose to discretize $\mathrm{SO}(2)$ to a finite group and implement a group CNN equivariant to the discretized rotations instead of continuous rotations, because 
% % diff to cylinder networks
% % \paragraph{Discretized group equivariance} As discussed in \cref{sec:literature}, we can either implement a steerable CNN to realize the continuous SO(2)-equivariance, or discretize SO(2) to a finite group and implement a group CNN. We choose the latter approach. 
% equivariant models for finite groups have simpler structure similar to conventional deep learning models, thus can be built upon existing SOTA task-specific networks. Deep networks are able to approximate nonlinear maps locally, thus the equivariance gap caused by the discretization is expected to be interpolated by the network through training.
% % Second, nonlinear layers in group CNNs are found more expressive and efficient than those in steerable CNNs, contributing to overall more capable models for group CNNs. Third, deep neural networks are able to approximate nonlinear maps locally, thus the equivariance gap caused by the discretization is expected to be interpolated by the network through training. 

Specifically, we discretize $\mathrm{SO}(2)$ into cyclic groups $C_n$, where $n$ denotes the number of discretized rotations. For example, $C_3$ is the group of 120-degree rotations. We refer to the discretized rotation set as the \textit{rotation anchors}. 

% The continuous rotation group $\mathrm{SO}(2)$ can be discretizd into cyclic groups $C_n$, where $n$ denotes the number of discretized rotations. For example, $C_3$ can be regarded as the group of 120-degree rotations. We call the discretized rotation set \textit{rotation anchors}. 

\vspace{-3pt}
\paragraph{Network structure}
We utilize the point-convolution style equivariant network E2PN~\cite{zhu20222} as our backbone, which is an equivariant version of KPConv \cite{thomas2019kpconv}. We describe necessary adaptations to E2PN in \cref{sec:encdec}, which allow us to build equivariant models on top of SOTA 4D panoptic segmentation network 4D-PLS~\cite{aygun20214d} and 4D-StOP~\cite{kreuzberg20234d}, both of which are based on KPConv. We refer to our equivariant models as \textit{Eq-4D-PLS} and \textit{Eq-4D-StOP}, respectively. 
% We utilize the point-convolution style equivariant network design of E2PN~\cite{zhu20222} as our backbone, which is an equivariant version of KPConv, and build our equivariant models on top of SOTA 4D panoptic segmentation network 4D-PLS~\cite{aygun20214d} and 4D-StOP~\cite{kreuzberg20234d}, both of which are based on KPConv~\cite{thomas2019kpconv}. We describe necessary adaptations to E2PN in \cref{sec:encdec}, which allow us to apply the equivariant design to 4D-PLS and 4D-StOP. Our proposed models can be regarded as the equivariant versions of 4D-PLS and 4D-StOP, to which we refer as \textit{Eq-4D-PLS} and \textit{Eq-4D-StOP} respectively. 

On a high level, both models first stack the point clouds from several sequential time instances within a common reference frame so that the temporal association becomes part of the instance segmentation. Each network consists of an encoder, a decoder, and prediction heads. 
The encoders and decoders are very similar for the two models, while the main differences lie in their prediction heads, which formulate the targets as invariant scalar fields and equivariant vector fields, respectively. 
% As we will show in the experimental results, this has a big impact on the final performance. 
% In \cref{sec:encdec}, we introduce the adaptations needed for applying E2PN as our backbone, and our equivariant encoder and decoder designs. 
In \cref{sec:head}, we discuss how we accomplish the 4D panoptic segmentation task by fitting the targets as invariant and equivariant fields. An overview of our network structure is shown in \cref{fig:overview}. 

% We apply the point-convolution-style equivariant network E2PN  \cite{zhu20222} as our backbone, which is an equivariant version of KPConv. With some adaptations introduced in \cref{sec:encdec}, it is compatible with the state-of-the-art 4D panoptic segmentation networks 4D-PLS \cite{aygun20214d} and 4D-StOP \cite{kreuzberg20234d}, both of which are based on KPConv \cite{thomas2019kpconv}. Our proposed models can be regarded as the equivariant versions of 4D-PLS and 4D-StOP, and we name them Eq-4D-PLS and Eq-4D-StOP respectively. On the high level, both methods first stack the point clouds at several sequential timestamps in a common reference frame, so that the temporal association is part of the instance segmentation. The networks are composed of an encoder, an decoder, and prediction heads. Their encoders and decoders are very similar, while differences lie in their prediction heads, corresponding to learning targets as invariant scalar fields and equivariant vector fields respectively, which has a big impact on the final performance, as shown in \cref{sec:exp_res}. In \cref{sec:encdec}, we introduce the adaptations needed for applying E2PN as our backbone. In \cref{sec:head}, we introduce how we accomplish the 4D panoptic segmentation task by fitting the targets as invariant and equivariant fields. 

\subsection{Equivariant Encoder and Decoder}\label{sec:encdec}
\vspace{-3pt}
\paragraph{Equivariant encoder}
The encoder of the 4D panoptic segmentation networks can be made equivariance by simply swapping the KPConv \cite{thomas2019kpconv} layers with E2PN \cite{zhu20222} convolution layers. However, E2PN is originally designed for $\mathrm{SO}(3)$ equivariance, and uses quotient representations and efficient feature gathering to improve the efficiency. Using it for $\mathrm{SO}(2)$ equivariance requires two adaptations. 

First, we use the regular representation instead of the quotient representation in~\cite{zhu20222}. This is because $\mathrm{SO}(2)$ is abelian, in which case quotient representations cause loss of information (see appendix for details). %\hc{What do we use instead? Need some description here.}

Second, to leverage the efficient feature gathering in E2PN, the spatial position of the convolution kernel needs to be symmetric to the rotation anchors. In this way, the rotation of kernel points can be implemented as a permutation of their indices. This implies that different $n$'s in $C_n$ impose different constraints on the number and distribution of the kernel points. For example, the default KPConv kernel with 15 points is symmetric to 60-degree rotations, and thus can be used to realize $C_2$, $C_3$, $C_6$ equivariance. However, $C_4$ requires a different kernel (for symmetry to 90-degree rotations), for which the 19-point KPConv kernel works. 

\vspace{-3pt}
\paragraph{Equivariant decoder}
The segmentation task requires dense prediction. The original E2PN only provides an encoder part for predicting a single output for each input point cloud. As such, we need to devise an equivariant decoder for our 4D panoptic segmentation model. Similar to conventional non-equivariant networks, our equivariant decoder is composed of upsampling layers and 1-by-1 convolution layers. The upsampling layers simply assign the features of the coarser point cloud to the finer point cloud via nearest neighbor interpolation. The 1-by-1 convolution processes the feature at each point and each rotation independently. It is straightforward to show such a decoder preserves the equivariance (see the appendix for more details). 


\subsection{Equivariant Prediction Heads}\label{sec:head}
\vspace{-3pt}
Our baseline models 4D-PLS~\cite{aygun20214d} and 4D-StOP~\cite{kreuzberg20234d} have different prediction heads. Their semantic segmentation heads are similar, but they employ different clustering approaches for instance segmentation. Correspondingly, we propose different equivariant prediction designs for them.

% \subsubsection{Eq-4D-PLS: segmentation as invariant scalar fields prediction}
\subsubsection{Eq-4D-PLS: Segmentation as Invariant Scalar Field Prediction}\label{sec:seg_inv}
\vspace{-3pt}
In 4D-PLS~\cite{aygun20214d}, instance segmentation is done by clustering the point embeddings, assuming a Gaussian distribution for the embeddings of each instance. It also predicts a point-wise centerness score, measuring the closeness of a point to its instance center, which is used to initialize the cluster centers. Both the point embeddings and the centerness scores can be viewed as \textit{invariant scalar fields}. Note that while these targets can appear like a vector, they are in fact a stack of scalars invariant to rotations. Similarly, the semantic segmentation target is also an invariant scalar field. 

As discussed in \cref{sec:fitinvequiv}, there are two strategies to fit invariant targets, i.e., rotation coordinate selection and invariant pooling. For invariant pooling layers, max pooling and average pooling over the rotational dimension are two obvious choices. We also experiment with the group attentive pooling layer proposed in EPN~\cite{chen2021equivariant}. 

For the rotational coordinate selection strategy, a key challenge is that the ground truth for the rotational coordinate $R$ is unavailable, or even undefined (as there is no canonical orientation for a LiDAR scan). In addition, existing dataset, e.g., SemanticKITTI, does not provide object bounding box annotations, hence the object orientations are unknown. In order to address this, we use an unsupervised strategy. Instead of picking the best rotational dimension, we perform a weighted sum of all rotational dimensions, which is differentiable and allows the model to learn the weight for different rotations. This is equivalent to the group attentive pooling layer. 

In summary, we study three designs, i.e., max pooling, average pooling, and group attentive pooling, for the prediction of the invariant targets including semantic classes, point embeddings, embedding variances, and centerness scores. 

% The major differences between our baseline models 4D-PLS and 4D-StOP are in their prediction heads. Without reiterating the design details in each of them, here we categorize the prediction targets into two types: \textit{invariant predictions} and \textit{equivariant predictions}, depending on whether the variable changes when the input point cloud is rotated. Prediction heads in charge of invariant and equivariant target variables need different designs. 

% For example. the semantic label of a point does not change with rotations, thus is invariant. The instance segmentation of 4D-PLS and 4D-StOP follows different approaches. In 4D-PLS, instance segmentation is by clustering the point embeddings assuming a Gaussian distribution for each instance. Therefore, it is a rotation-invariant prediction. In 4D-StOP, instance segmentation is by voting for the offset of instance center from each point, and clustering on the voted instance centers. In this case, the offset voting is actually a rotation-equivariant prediction, because the vector from a point to the instance center \textit{is} subject to rotations. 

% \subsubsection{Invariant predictions}
% \paragraph{Early v.s. late fusion}
% Conducting invariant predictions through an equivariant network can be done in two ways. First, the model may predict the output from every rotation channels, and average the predictions at the end, which is called late fusion. Second, the features from every rotation channel may be fused to marginalize the rotational information, and then used to provide a single output, which is called early fusion. We tested on both strategies in our experiments. 

% \paragraph{Feature pooling layer}
% The early fusion involves fusing features on different rotations into a rotation-invariant feature. There are many possible ways. In this paper, we explored six ways: max pooling, mean pooling, soft attentive selection, hard attentive selection, soft attentive permutation, hard attentive permutation. Max and mean poolings are self-explainable. The rest four fall in two classes: selection and permutation. 

% Denote the feature for point $x$ at rotation index $i$ as $f_i(x)\in \mathbb{R}^m$, then the soft and hard attentive selection layer can be written as:
% \begin{align}
%     f_{soft\text{-}sel}(x) & = \sum_i w_i(x)f_i(x) \\
%     f_{hard\text{-}sel}(x) & = f_{i^*}(x), \text{where } i^*=\arg \max_i w_i(x)
% \end{align}
% where $w_i(x)\in \mathbb{R}$ is a attention function assigning weights on each rotation index. 

% With the same notation of $i, i^*, w, \text{and } f$, he soft and hard permutation layer can be written as:
% \begin{align}
%     f_{soft\text{-}perm}(x) & = \sum_i w_i(x)[f_{\sigma(i)1}(x),...,f_{\sigma(i)n}(x)] \\
%     f_{hard\text{-}perm}(x) & = [f_{\sigma(i^*)1}(x),...,f_{\sigma(i^*)n}(x)]
% \end{align}
% where $[\cdot, ..., \cdot]$ is the concatenation operator, and $\sigma$ represents the group action of the cyclic group $C_n$ on itself. Ideally, if the attention function predicts the highest weight for the ground truth rotation, the output of the hard permutation layer corresponds to the feature of the point cloud before rotation. The hard permutation layer is very similar to the permutation layer introduced in E2PN for pose estimation. Here we repurpose this layer for invariant feature generation, and the soft version is introduced to enable differentiability when direct supervision on $w$ is unavailable. 

% We test the various feature pooling layers in the experiments. 

% \subsubsection{Eq-4D-StOP: segmentation as equivariant and invariant fields prediction}
\subsubsection{Eq-4D-StOP: Segmentation as Equivariant and Invariant Field Prediction}\label{sec:seg_equiv}
\vspace{-3pt}
4D-StOP~\cite{kreuzberg20234d} follows an offset-based clustering approach for instance segmentation. The network predicts the offset vector to the instance center for each input point, which forms an \textit{equivariant vector field}. These predicted instance center locations are then clustered into instances. The points clustered into one instance are aggregated to predict instance properties.

As discussed in \cref{sec:fitinvequiv}, we fit equivariant vector fields through rotational coordinate selection. While we do not have ground-truth rotations, the vector field of offsets naturally defines orientations. Denote an offset vector at point $x$ as $v(x) = x_{ctr}-x\in V=\mathbb{R}^3$, where $x_{ctr}$ is the center of the instance that $x$ belongs to. We can define its rotation in $\mathrm{SO}(2)$ as $\theta(v)=\mathrm{atan2}(v_Y, v_X)$, where $X,Y$ are axes in the horizontal plane and $Z$ is the vertical axis. $\theta$ can be assigned to a discretized rotation coordinate (anchor) $R_{i}^{gt}$ by nearest neighbor.
In this way, each point has a rotation label based on its relative position to the instance center, which is well defined and equivariant to the point cloud rotations. 

% As discussed in \cref{sec:fitinvequiv}, we fit equivariant vector fields through rotational coordinate selection. Same as in \cref{sec:seg_inv}, we face the challenge of not having a ground truth rotation. However, the vector field of offsets naturally defines orientations for us. Denote an offset vector at point $x$ as $v(x) = x_{ctr}-x\in V=\mathbb{R}^3$, where $x_{ctr}$ is the center of the instance that $x$ belongs to. We can define its rotation in $\mathrm{SO}(2)$ as $\theta(v)=\mathrm{atan2}(v_Y, v_X)$, where $X,Y$ are axes in the horizontal plane and $Z$ is the vertical axis. $\theta$ can be assigned to a discretized rotation coordinate (anchor) $R_{i_{gt}}$ by nearest neighbor. In this way, each point has a rotation label based on its relative position to the instance center, which is well defined and equivariant to the point cloud rotations. 

Given the rotation label, we train the network to predict it as a classification task. Given the feature map at each rotational coordinate, a rotation score is predicted: 
\begin{equation}
    s_x(R_i) = \phi(f(x, R_i)),\quad \forall R_i\in \mathrm{SO}(2)',
\end{equation}
where $x\in \mathbb{R}^3$, $\mathrm{SO}(2)'$ is the set of rotation anchors, i.e., $\mathrm{SO}(2)'\cong C_n$, $i=1, ..., n$, and $\phi$ is a scoring function. We concatenate $s_x(R_i)$'s as $S_x=[s_x(R_1), ..., s_x(R_n)]$ and apply a cross-entropy loss function on $S_x$ with label $R_i^{gt}$.  

The semantic segmentation, object size, and radius regression in 4D-StOP are invariant fields. However, note that a key difference from the prediction in Eq-4D-PLS (c.f.~\cref{sec:seg_inv}) is that we now have the rotation labels. As such, the rotational coordinate selection strategy can be applied to predicting the invariant targets. Therefore, for Eq-4D-StOP, we study four options for the invariant target prediction, including max pooling, average pooling, group attentive pooling, and rotational coordinate selection. 




\section{Experiments}\vspace{-3pt}

% We perform comprehensive analysis on our developed models and benchmark them against existing methods. %compare and show experiments using our proposed approach.
% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{vis.PNG}
%     \caption{Qualitative result}
% \end{figure*}

\subsection{Experimental setup}\vspace{-3pt}
\textbf{Dataset:} We conduct our experiments on the SemanticKITTI dataset \cite{behley2019semantickitti}. The SemanticKITTI dataset establishes a benchmark for LiDAR panoptic segmentation \cite{behley2021benchmark}. It consists of 22 sequences from the KITTI dataset. 10 sequences are used for training, 1 for validation, and 11 for testing. In total there are 43,552 frames. The dataset includes 28 annotated semantic classes, which are reorganized into 19 classes for the panoptic segmentation task. Of the 19 classes, 8 are classified as \textit{things}, while the remaining 11 are categorized as \textit{stuff}. Each point in the dataset is assigned a semantic label and for points belonging to things an instance ID that is temporally consistent. 
% Points belonging to stuff classes are assigned an instance ID of 0.

\textbf{Metrics:} The core metric for 4D panoptic segmentation is $LSTQ=\sqrt{S_{cls}\times S_{assoc}}$, which is the geometric mean of the semantic segmentation metric $S_{cls}$ and the instance segmentation and tracking metric $S_{assoc}$. $S_{cls}=\frac{1}{C}\sum_{c=1}^C IoU(c)$ is the segmentation IoU averaged over all semantic classes. The average IoU for points belonging to things and stuff are denoted $IoU^{Th}$ and $IoU^{St}$ respectively. $S_{assoc}$ measures the spatial and temporal accuracy of segmenting object instances, more details of which are explained in \cite{aygun20214d}. 

\textbf{Architecture:} For our Eq-4D-PLS and Eq-4D-StOP models, we keep the architectures unchanged from their 4D-PLS~\cite{aygun20214d} and 4D-StOP~\cite{kreuzberg20234d} baselines except for the added rotation classification layers and invariant pooling layers necessary for equivariant and invariant field predictions. The input size, the batch size, and the learning rate also follow the baselines respectively. 

As the efficiency (especially the memory consumption) is a pain point for equivariant learning in large-scale LiDAR perception tasks, we specify the number of channels (network width) $c$ and the rotation anchor size $n$ in our analysis. The size of an equivariant feature map $f$ is $\abs{f}=mcn$ for a point cloud with $m$ points. Non-equivariant networks can be viewed as $n=1$. We use the width of the first layer to denote the network width $c$, since the width of the following layers scales with the first layer proportionally. As feature maps play a major role in memory consumption, $c\times n$ gives a rough idea of the memory cost of a model. 
% Non-equivariant networks can be viewed as $n=1$. We will use the notation $(c\times n)$ to specify each network, where $c$ is the width of the first layer. The width of following layers scales proportionally. 
% As the feature maps dominate the memory consumption, $(c\times n)$ gives a rough idea of the memory cost of a model. More detailed efficiency analysis is in \cref{sec:abl_eff}. 
% Most experiments can be trained on a single Nvidia A40 GPU, with a few exceptions on a Nvidia A100 GPU, marked with an asterisk (*). 

\begin{table}[]
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
Method                                                         & $LSTQ$        & $S_{assoc}$   & $S_{cls}$     & $IOU^{St}$    & $IOU^{Th}$    \\ \midrule
RangeNet++\cite{milioto2019rangenet++}+PP+MOT & 43.8          & 36.3          & 52.8          & 60.5          & 42.2          \\
KPConv\cite{thomas2019kpconv}+PP+MOT          & 46.3          & 37.6          & 57.0          & 64.2          & 54.1          \\
RangeNet++\cite{milioto2019rangenet++}+PP+SFP & 43.4          & 35.7          & 52.8          & 60.5          & 42.2          \\
KPConv\cite{thomas2019kpconv}+PP+SFP          & 46.0          & 37.1          & 57.0          & 64.2          & 54.1          \\
4D-DS-Net\cite{hong2022lidar}                 & 68.0          & 71.3          & \textbf{64.8}          & 64.5          & 65.3          \\
4D-PLS\cite{aygun20214d}                      & 62.7          & 65.1          & 60.5          & 65.4          & 61.3          \\
4D-StOP\cite{kreuzberg20234d}                 & 67.0          & 74.4          & 60.3          & 65.3          & 60.9          \\ \midrule
Eq-4D-PLS \textit{(ours)}                     & 65.0          & 67.7          & 62.3          & 66.4          & 64.6          \\
Eq-4D-StOP \textit{(ours)}                   & \textbf{70.1} & \textbf{77.6} & 63.4 & \textbf{66.4} & \textbf{67.1} \\ \bottomrule
\end{tabular}
}
\vspace{0.5pt}
\caption{SemanticKITTI validation set result. PP: PointPillars \cite{lang2019pointpillars}. MOT: tracking-by-detection by \cite{weng20203d}. SFP: tracking-by-detection with scene flow \cite{mittal2020just}. The best is highlighted in \textbf{bold}. }
\label{tab:val}
\end{table}

\begin{table}[]
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
Method            & $LSTQ$ & $S_{assoc}$ & $S_{cls}$ & $IoU^{St}$ & $IoU^{Th}$ \\ \midrule
RangeNet++\cite{milioto2019rangenet++}+PP+MOT & 35.5 & 24.1     & 52.4   & 64.5       & 35.8        \\
KPConv\cite{thomas2019kpconv}+PP+MOT     & 38.0 & 25.9     & 55.9   & 66.9       & 47.7        \\
RangeNet++\cite{milioto2019rangenet++}+PP+SFP & 34.9 & 23.3     & 52.4   & 64.5       & 35.8        \\
KPConv\cite{thomas2019kpconv}+PP+SFP     & 38.5 & 26.6     & 55.9   & 66.9       & 47.7        \\
4D-PLS\cite{aygun20214d} & 56.9 & 56.4     & 57.4   & 66.9       & 51.6        \\
4D-DS-Net\cite{hong2022lidar}         & 62.3 & 65.8     & 58.9   & 65.6       & 49.8        \\
CA-Net\cite{marcuzzi2022contrastive}            & 63.1 & 65.7     & 60.6   & 66.9       & 52.0        \\
4D-StOP\cite{kreuzberg20234d} & 63.9 & 69.5     & 58.8   & 67.7       & 53.8        \\ \midrule
Eq-4D-StOP \textit{(ours)} & \textbf{67.0} & \textbf{72.0}     & \textbf{62.4}   & \textbf{69.1}       & \textbf{60.9}        \\ \bottomrule
\end{tabular}
}
% \vspace{-5pt}
\caption{SemanticKITTI test set result. }
\label{tab:test}
\end{table}

\subsection{Quantitative Results}\label{sec:exp_res}\vspace{-3pt}
The evaluation results of our equivariant models on the SemanticKITTI validation set are shown in \cref{tab:val}. 
% and results on the test set are shown in~\cref{tab:test}. 
Compared with 4D-PLS, our equivariant model improved by \textbf{2.3} points on $LSTQ$. Our Eq-4D-StOP model outperforms its non-equivariant baseline by \textbf{3.1} $LSTQ$ points. The Eq-4D-StOP model achieves state-of-the-art performance among published methods. From these experiments, we have the following observations:
\begin{itemize}
    \item The improvements on $IoU^{St}$ are similar for both Eq-4D-StOP and Eq-4D-PLS compared with their non-equivariant baselines.
    \item The improvements on $IoU^{Th}$ and $S_{assoc}$ are larger than on $IoU^{St}$. 
    \item Eq-4D-StOP gains larger improvements on $IoU^{Th}$ and $S_{assoc}$ than Eq-4D-PLS does over their non-equivariant baselines. 
\end{itemize}
The observations indicate the following. First, the introduction of equivariance brings improvements to both models across all metrics consistently. Second, objects (the \textit{things} classes) enjoy more benefits from the equivariant models compared with the background (the \textit{stuff} classes). We hypothesize that this is because objects present more rotational symmetry as compared to background classes. Third, the fact that more significant improvements are observed in Eq-4D-StOP shows the benefit of formulating the equivariant vector field regression problem induced from the offset-based clustering strategy. Improved clustering directly benefits the instance segmentation, i.e., $S_{assoc}$, and it also improves the semantic segmentation of objects ($IoU^{Th}$), since 4D-StOP unifies the semantic class prediction of all points belonging to a single instance by majority voting. 

The evaluation results on the SemanticKITTI test set are shown in \cref{tab:test}. Eq-4D-StOP achieved \textbf{3.1} points improvement in $LSTQ$ over the non-equivariant model, and ranked 1st in the leaderboard at the time of submission. Consistent with our observations in the validation set, major improvements of Eq-4D-StOP are from $S_{assoc}$ and $IoU^{Th}$. We only tested our best model on the test set, thus the result of Eq-4D-PLS is not available.

We use max-pooling in Eq-4D-PLS and average pooling in Eq-4D-StOP as the invariant pooling layer. The ablation study is in \cref{sec:abl_layer}. The Eq-4D-PLS model is with $c=128, n=6$, and the Eq-4D-StOP model is with $c=128, n=4$. These parameters are selected based on the network scaling analysis in \cref{sec:abl_eff}. 
% The network width and anchor size selection is analyzed in \cref{sec:abl_eff}. 

\begin{table}[]
\centering
\resizebox{0.85\columnwidth}{!}{
\begin{tabular}{c|c|cc}
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Target \\ type\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Layer \\ type\end{tabular}} & \multicolumn{2}{c}{$LSTQ$}    \\ \cmidrule{3-4} 
                                                                        &                                                                        & Eq-4D-StOP    & Eq-4D-PLS     \\ \midrule
\multirow{4}{*}{Invariant}                                              & Max                                                                    & 68.2          & \textbf{63.7} \\
                                                                        & Average                                                                & \textbf{69.2} & 61.4          \\
                                                                        & Attentive                                                              & 68.5          & 63.1          \\
                                                                        & RCS                                                                    & 68.4          & n/a             \\ \midrule
\multirow{2}{*}{Equivariant}                                            & RCS                                                                    & \textbf{69.2} & n/a             \\
                                                                        & Average                                                                & 61.6          & n/a             \\ \bottomrule
\end{tabular}
}
\vspace{1pt}
\caption{Ablation study on the invariant pooling layers and rotational coordinate selection (RCS). All comparisons used $c=128, n=3$. Some options are n/a for Eq-4D-PLS (see \cref{sec:seg_inv}). }
\label{tab:abl}
\end{table}

\subsection{Ablation Study}\label{sec:abl_layer}\vspace{-3pt}
In \cref{tab:abl}, we show the segmentation performance with different choices of the invariant pooling layers and the effect of rotational coordinate selection (RCS). In terms of the invariant pooling layer, average-pooling performs the best in Eq-4D-StOP, while max-pooling performs the best in Eq-4D-PLS. A possible reason for Eq-4D-PLS to strongly favor max-pooling over average-pooling is that the point embeddings learned from averaging over all rotational directions could be less discriminative, creating difficulties in instance association. For Eq-4D-StOP, while rotational coordinate selection can be used to predict invariant targets as well, average-pooling still performs the best. The reason could be that average-pooling gathered information from all orientations. 
For the equivariant field (offset) prediction, we compare the rotational coordinate selection with an average-pooling layer that treats the offsets as rotation-invariant targets. The performance drastically decreases when RCS is replaced with average-pooling, showing that it is of vital importance to respect the equivariant nature of the targets. 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{const_width2.png}
    \caption{Effect of anchor size $n$ on performance and memory usage in training, given constant network width $c$. Note that $n=1$ corresponds to the non-equivariant baselines. }
    \label{fig:const_width}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{const_size2.png}
    \caption{Effect of anchor size $n$ on performance and memory usage in training, given constant size of feature maps $c\times n$. }
    \label{fig:const_size}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{scaling2.png}
    \caption{Comparison between equivariant and non-equivariant models at different network sizes. }
    \label{fig:scaling}
\end{figure}

\begin{table}[]
\centering
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{c|ccccc}
\toprule
Anchor size $n$           & 1    & 2    & 3    & 4    & 6    \\ \midrule
Performance ($LSTQ$)        & 67.1 & 67.3 & 69.3 & 69.8 & 69.0 \\
Inference speed (fps) & 0.73 & 1.06 & 1.14 & 1.27 & 1.39 \\ \bottomrule
\end{tabular}}
\vspace{1pt}
\caption{Running speed analysis of Eq-4D-StOP, given constant feature map size $c\times n = 256$. Note that $n=1$ corresponds to the non-equivariant baseline. }
\label{tab:fps}
\vspace{-5pt}
\end{table}

\subsection{Network Scaling and Computational Cost}\label{sec:abl_eff}\vspace{-3pt}
We experimented on different network widths $c$ and anchor sizes $n$ to investigate the effect of network sizes on the model performance and efficiency. The $LSTQ$ is evaluated on the SemanticKITTI validation set. 

In \cref{fig:const_width}, we keep $c=128$ and try different anchor size $n$'s. A larger $n$ better approximates the continuous $\mathrm{SO}(2)$ equivariance, but also increases the size of feature maps and the overall memory consumption proportionally. We can see that the performance of both Eq-4D-PLS and Eq-4D-StOP improves as $n$ gets larger, validating the effectiveness of equivariance in this task. 

To rule out the factor of varying sizes of feature maps, we keep $c\times n=256$ with different combinations of $c$ and $n$ (for $n=3$, we use $c=85$ as an approximation). As shown in \cref{fig:const_size}, Eq-4D-StOP significantly outperforms the non-equivariant version ($n=1$) when $n>=3$. The memory consumption even decreases when we increase $n$. There are two reasons. First, the size of the weight matrix in convolution layers gets smaller. Denote the number of kernel points as $k$, for a convolution layer with input and output width $c$, the size of the weight matrix is $knc^2$. Given constant $c\times n$, this number decreases with larger $n$. Second, the feature maps after the invariant pooling layer are smaller (of size $m\times c$). The small bump-up of memory usage at $n=4$ is due to the larger convolution kernel ($k=19$ v.s. $k=15$ as discussed in \cref{sec:encdec}), but it is still lower than the memory usage of the non-equivariant baseline. A similar trend can also be observed in Eq-4D-PLS, but with a smaller performance margin. 

We also report the running time of our models in \cref{tab:fps}. The equivariant models run faster with higher accuracy. 

We further investigate whether the advantage of equivariant models only occurs at a specific network size. In \cref{fig:scaling}, we scale the feature map size of the networks. We use $n=4$ for equivariant models in this comparison. The equivariant models outperform the non-equivariant models at all network sizes. The memory consumption increases faster for the non-equivariant models, because the sizes of convolution kernel weight matrices grow quadratically with the network width $c$, while equivariant models have a smaller $c$ given the same feature map size. 

% The running time is another aspect of computational cost
% Above we mainly focus on the memory aspect of computational cost. In fact, our equivariant models also have faster inference than the non-equivariant baselines. 

In summary, we found that the equivariant models have better performance than their non-equivariant counterparts with lower computational costs at different network sizes. 

% \begin{table}[]
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{c|l|lllll}
% \hline
% \multirow{2}{*}{Method}     & \multicolumn{1}{c|}{\multirow{2}{*}{Channels  c}} & \multicolumn{5}{c}{Rotation   discretization n} \\
%                             & \multicolumn{1}{c|}{}                             & 1        & 2        & 3        & 4        & 6   \\ \hline
% \multirow{4}{*}{Eq-4D-StOP} & 256                                               & 67.14    & 67.52    & 0        &          &     \\
%                             & 128                                               & 66.36    & 67.28    & 69.15    & 70.14    & 0   \\
%                             & 64                                                &          &          & 0        & 0        &     \\
%                             & 256/n                                             & 67.14    & 67.28    & 69.25    & 0        & 69.01   \\ \hline
% \multirow{4}{*}{Eq-4D-PLS}  & 256                                               & 0        & 0        & 0        & 0        & 0   \\
%                             & 128                                               & 59.85        & 60.01        & 62.76        & 61.86        & 65.50   \\
%                             & 64                                                & 0        & 0        & 0        & 0        & 0   \\
%                             & 256/n                                             & 0        & 0        & 0        & 0        & 0   \\ \hline
% \end{tabular}
% }
% \caption{Performance with respect to network width and rotation group size. }
% \end{table}

% \begin{table}[]
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{c|l|lllll}
% \hline
% \multirow{2}{*}{Method}     & \multicolumn{1}{c|}{\multirow{2}{*}{Channels  c}} & \multicolumn{5}{c}{Rotation   discretization n} \\
%                             & \multicolumn{1}{c|}{}                             & 1        & 2        & 3        & 4        & 6   \\ \hline
% \multirow{4}{*}{Eq-4D-StOP} & 256                                               & 0    & 0    & 0        &          &     \\
%                             & 128                                               & 0    & 0    & 0    & 0    & 0   \\
%                             & 64                                                &          &          & 0        & 0        &     \\
%                             & 256/n                                             & 0    & 0    & 0    & 0        & 0   \\ \hline
% \multirow{4}{*}{Eq-4D-PLS}  & 256                                               & 0        & 0        & 0        & 0        & 0   \\
%                             & 128                                               & 0        & 0        & 0        & 0        & 0   \\
%                             & 64                                                & 0        & 0        & 0        & 0        & 0   \\
%                             & 256/n                                             & 0        & 0        & 0        & 0        & 0   \\ \hline
% \end{tabular}
% }
% \caption{Training memory consumption with respect to network width and rotation group size. }
% \end{table}



\section{Conclusion}
In this paper, we use equivariant learning to tackle a complicated large-scale perception problem, the 4D panoptic segmentation of sequential point clouds. While equivariant models were generally perceived as more expensive and complex than conventional non-equivariant models, we showed that our method can bring performance improvements and lower computational costs at the same time. We also showed that the advantage of equivariant models can be better leveraged if we formulate the learning targets as equivariant vector fields, compared with invariant scalar fields. A limitation of this work is that we did not propose drastically new designs on the overall structure of the 4D panoptic segmentation network under the equivariant setup, but it also allowed us to conduct apple-to-apple comparisons regarding equivariance on this task, and so that our contribution is orthogonal to the improvements in the specific network design. We hope our work could inspire wider incorporation of equivariant networks in practical robotic perception problems. 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}