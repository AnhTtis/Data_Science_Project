
% \vspace{-.5em}
\subsection{Comparison to zero-shot open-vocab baseline} \label{sec:groupvit}
% \vspace{-.3em}
As a reference, we provide a comparison with GroupVit \cite{xu2022groupvit}, which solves a related but different segmentation problem, \emph{open-vocabulary segmentation}. 
%This task requires a text query from the test set to find each segment in the input image. 
This task requires text queries to specify which objects to segment, although the queries can be arbitrary or unseen during training. 
%Comparing scores between such methods using the same predefined test labels is thus straightforward and justified. 
Our method, on the contrary, predicts arbitrary text labels at inference time and is not directly comparable using the same standard benchmarks. Nonetheless, our proposed relabeling procedure can allow useful comparative analysis against open-vocabulary baselines on the same benchmarks. 
% For GroupVit's architecture, the number of segments is fixed to 8 per image.

Table \ref{tab:groupvit} shows that GroupVit obtains a better IoU on PAS-20 with 20 classes. However, our method is significantly narrowing the gap on PC-59, especially with human-threshold IoU, and our IoU with constant-threshold even surpasses GroupVit's on challenging PC-459, which has much more classes (459).
%, despite using no text queries to help find semantic segments.
%, \todo{ espicailly with IoU scores with human thresholding (IoU$_\text{h}$). 
Figure \ref{fig:groupvit} shows that our method can discover more objects and provide more fine-grained labeling, while GroupVit labels only a few objects and does not label every part of the image.
%.can label objects more specifically.


% \begin{table}[]
% \centering
% \caption{Comparison to GroupVit \cite{xu2022groupvit}, which solves a related but different segmentation problem and requires input text queries. *denotes scores computed on 1,000 random test images. IoU$_\text{c}$ and IoU$_\text{h}$ are IoU with constant thresholding or human verification.}
% % \vspace{-0.5em} IoU
% \label{tab:groupvit}
% \resizebox{\columnwidth}{!}{
% \setlength{\tabcolsep}{10pt}
% \begin{tabular}{lcc@{\extracolsep{6pt}}cc}
% \toprule
%                             & \multicolumn{3}{c}{\textbf{PC-59}}                               & \multicolumn{1}{c}{\textbf{PAS-20}}                                  \\ \cline{2-4} \cline{5-5} \\[-1em]
%  \multicolumn{1}{c}{Method} &  IoU$_\text{c}$  & IoU$_{\text{h } \geq 2}$  & IoU$_{\text{h } \geq 1}$ & $\text{IoU}_\text{c}$  \\ \midrule
% GroupVit \cite{xu2022groupvit} & 22.4      & -     & -      & 52.3      \\
% Ours                           & 19.6      & 20.9*      & 22.7*     & 20.1     \\ \bottomrule
% \end{tabular}}
% \vspace{-0.5em}
% \end{table}

\begin{table}[]
\centering
\caption{Comparison to GroupVit \cite{xu2022groupvit}, which solves a related but different segmentation problem and requires input text queries. *denotes scores computed on 1,000 random test images. IoU$_\text{c}$ and IoU$_\text{h}$ are IoU with constant thresholding or human verification. GroupVit's numbers have been updated according to \cite{xu2023open}}
\vspace{-0.5em}
% \vspace{-0.5em} IoU
\label{tab:groupvit}
\resizebox{\columnwidth}{!}{
\setlength{\tabcolsep}{5pt}
\begin{tabular}{lcc@{\extracolsep{6pt}}ccc}
\toprule
                            & \multicolumn{3}{c}{\textbf{PC-59}}                               & \multicolumn{1}{c}{\textbf{PC-459}}  &   \multicolumn{1}{c}{\textbf{PAS-20}}   \\ \cline{2-4} \cline{5-5} \cline{6-6}\\[-1em]
 \multicolumn{1}{c}{Method} &  IoU$_\text{c}$  & IoU$_{\text{h } \geq 2}$  & IoU$_{\text{h } \geq 1}$ & $\text{IoU}_\text{c}$ & $\text{IoU}_\text{c}$  \\ \midrule
GroupVit \cite{xu2022groupvit} & 25.9      & -     & -      & 4.9     & 50.7 \\
Ours                           & 19.6      & 20.9*      & 22.7*     & 11.3   & 20.1  \\ \bottomrule
\end{tabular}}
\vspace{-1em}
\end{table}




% \begin{figure}
% \centering
% % ablation_4 1.1
%   % \includegraphics[scale=0.43]{./figs/groupvit.pdf}
%   % \includegraphics[scale=0.425]{./figs/groupvit_lt.pdf}
%   \includegraphics[scale=0.44]{./figs/groupvit_tp.pdf}
% %   \includegraphics[scale=1.1]{./figs/ablation_4.pdf}
% % \vspace{-1.7em}
%   \caption{\textbf{Qualitative comparison to GroupVit \cite{xu2022groupvit}.} 
%   Despite lower IoU score, our method discovers more objects in input images, including objects not shown in the dataset (hay, mirror). Our method can offer more specific labels, such as `stool'.
%   }
%   % \vspace{-1.em}
%   \label{fig:groupvit}
%   % \vspace{-.5em}
% \end{figure}

\begin{figure*}
% \centering
% ablation_4 1.1
  % \includegraphics[scale=0.43]{./figs/groupvit.pdf}
  % \includegraphics[scale=0.425]{./figs/groupvit_lt.pdf}
  \includegraphics[scale=1.04]{./figs/groupvit_big.pdf}
%   \includegraphics[scale=1.1]{./figs/ablation_4.pdf}
\vspace{-.5em}
  \caption{\textbf{Qualitative comparison to GroupVit \cite{xu2022groupvit}.} 
  Despite achieving lower IoU scores, our method can discover objects beyond the labels in the dataset, such as `hay' and `mirror', and can provide more fine-grained labels, such as `stool'.
  }
  \label{fig:groupvit}
  \vspace{-.5em}
\end{figure*}

\subsection{Comparison to supervised baselines}
\label{apx:baselines}
Table \ref{tab:baselines} presents an IoU comparison with existing supervised open-vocabulary baselines on three datasets based on the numbers presented in \cite{liang2022open}.
Unlike our approach, these methods require segmentation annotations (or pretrained segmentation models) during training and text queries to guide the segmentation. 
% Note that ``open-vocabulary'' means that the labels can be arbitrary and unseen during training, but the labels still need to be provided to these methods to specify which object to find or segment at inference time.
%and ground-truth labels, which specify which object to find or segment, at inference time. 
%Unlike our approach, these methods require ground-truth labels, which specify which object to find or segment, at inference time. 
%Their final outputs are segments, each of which corresponds to a \emph{given} ground-truth label. 
% Comparing scores between such methods using the same predefined test labels is thus straightforward and justified.
%As these baselines take text labels as input, the matching between visual and text is a non-issue. 
% They can use different matching criteria/thresholds, and their IoUs can still be compared fairly. 
% All the details about how IoU scores are computed (such as thresholds/matching criteria) may varied but the IoUs can be compared fairly. 
% Our method, on the contrary, predicts arbitrary text labels at inference time and is not directly comparable using the same standard benchmarks. Nonetheless, our proposed label reassignment procedure can provide useful comparative analysis against these supervised baselines on the same benchmarks. 
%As we discussed in Section \ref{apx:thresholds}, for future methods that solve zero-guidance segmentation, 
%and the matching in the evaluation step is merely for assessing the results with the standard metric. 
Our IoU scores in Table \ref{tab:baselines} are computed using segment-to-text IoU with a constant threshold $\tau_\text{CLIP} = 0.1$ and human score $\geq 1$.
%but if future work wants to compare the results with us, they may need to consider to use the same matching or justify how to compare different matching approaches with fairness.
There is still a gap in performance between our unsupervised method and these supervised baselines, though our method performs only slightly worse on the more challenging PC-459.

\begin{table}[]
\centering
\caption{IOU scores comparison between supervised open-vocabulary segmentation baselines (trained with segmentation labels) and our unsupervised method.}
\label{tab:baselines}
\vspace{-0.5em}
\resizebox{\columnwidth}{!}{
\setlength{\tabcolsep}{12pt}
\begin{tabular}{lccc}
\toprule
Method   & PAS-20 & PC-59  & PC-459 \\ \midrule
% \multicolumn{4}{c}{\emph{Supervised open-vocab method}} \\
Lseg \cite{li2022language}          & 47.4   & -     &  -     \\
SimBaseline \cite{xu2021simple}     & 74.5   & -     &  -   \\
ZegFormer \cite{ding2022decoupling} & 80.7   & -     &  -     \\
OpenSeg \cite{ghiasi2021open}       & -      & 42.1  &  9.0   \\
OVSeg \cite{liang2022open}          & 94.5   & 55.7  &  12.4   \\  \midrule
% GroupVit \cite{xu2022groupvit}    & 52.3   & 22.4        \\
Ours - $\text{IoU}_\text{c}$       & 20.1  &   19.6  & 11.3 \\ 
Ours - $\text{IoU}_{\text{h } \geq 1}$  & - &   22.7  & - \\ \bottomrule
\end{tabular}}
% \vspace{-.7em}
\end{table}