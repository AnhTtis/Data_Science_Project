\subsection{\todo{User study}} \label{sec:userstudy}
We conduct a user study for a more comprehensive evaluation. The previously presented metrics in Section \ref{sec:ablation} can directly evaluate the quality of predicted labels with respect to the predicted segments. Evaluating predicted texts comes with the challenge of text granularity. How can one correctly associate the text label `wheel' to the ground truth label `car', and also determine that both `wheel' and `car' are the correct label of the corresponding segment? In this section, we rely on human judgment to solve this complex task.

% To evaluate whether a segment can be correctly describe by its predicted label, and the closest ground truth label. For each segment, We ask human subjects to give ratings to [predicted label, segment] and [closest ground truth label, segment] separately. 
% To evaluate our results, we design a system that can assess how well a given text can describe a segment. We present to human subjects pairs of [text description, segment] and ask them to rate the pairs.  

% The proposed task's natural-language outputs make the task subjective; the output labels could describe segments in various ways. Thus, existing segmentation test sets with inflexible predefined classes may not be the most suitable way to evaluate our results. In this section, we conduct a user study using Amazon Mechanical Turk to learn about human impressions of our results.

% ness's draft ( not finish/ un refined ) 
% To evaluate the full pipeline of our proposed task, generated texts and segments must be considered. However, current automated metrics are incapable of capturing the quality of generated texts in in-the-wild settings, which we further discuss in (Section \ref{sec:analysis}), i.e., TLS could evaluate solely the text generation given that a ground truth label must be known, whereas IoU and Recall focus on segment quality and ignore the text outputs entirely. In this section, we conduct a user study using Amazon Mechanical Turk to incorporate human judgment into the interpretation of our results in both textual and visual aspects.



% To evaluate the full pipeline of our proposed task, both generated texts and segments have to be considered. However, current automated metrics are incapable of capturing such an overall quality, which we further discuss in (Section \ref{sec:analysis}), i.e., IoU and Recall focus on segment quality and ignore the text outputs entirely, whereas TLS could evaluate solely the text generation only given that ground truth label must be known. In this section, we conduct a user study using Amazon Mechanical Turk to incorporate human judgment into the interpretation of our results in both textual and visual aspects.






%(discussion about meronymy and asymmetry)%


% The proposed task's natural-language outputs make the task subjective; the output label could describe the segment in various ways. Thus, existing segmentation test sets with inflexible predefined classes may not be the most suitable way to evaluate our results.
% In this section, we conduct a user study using Amazon Mechanical Turk to learn about human impressions of our results.  


\subsubsection{Setup}
% We present to human subjects pairs of text descriptions and images with highlighted segments.
We let human subjects evaluate whether a segment can be correctly described by its predicted label and the closest ground truth label. For each segment, We ask human subjects to give ratings to [predicted label, segment] and [closest ground truth label, segment] separately. 

The subject is shown a text and an image with a highlighted region representing a segment and then instructed to rate the quality of the text descriptions on a scale of 0-4: 0 (incorrect), 1 (partially correct), 2 (correct but too generic/specific), and 3 (correct). 
We randomly sample 1,000 images from PC-59 for the user study. 
% There are two sets of questions: 1) pairs of [predicted label, segment], 2) pairs of [closest ground truth label, segment]. The two sets are shuffled together, and the subjects do not know whether a text description is a prediction or a ground truth.
The detailed instruction is in Appendix \ref{apx:userstudy}.

% \subsubsection{Human-judgment Rating}
% There are two human-judgment rating scores:
% 1) \textbf{Segment-predicted text Human Rating: HR$_{\text{s}-\text{pred}}$} measures human rating of a segment $S_i$ and its predicted label $T_i$.
% 2) \textbf{Segment-predicted text Human Rating: HR$_{\text{s}-\text{gt}}$} measures human rating of a segment $S_i$ and its its closest ground-truth label $T^*_i$.

\subsubsection{Experiments and results}
This experiment aims to measure the result with IoU scores that use the human decision to control CLIP-based reassignment (Section \ref{seg2text}) instead of using a constant threshold. 
To assess the whole pipeline of our method, we relabel only segments whose both (segment, predicted label) pairs and (segment, ground truth label) pairs receive human scores greater than 1. We denote this metric IoU$_\text{human}$
We also evaluate the results independent of text generation by only considering human scores of (segment, predicted label) pairs. We denote this metric IoU$_\text{human}^\text{?}$




% IoU$_{\text{h}}^{\text{s}-\text{pred}}$
% \subsubsection{Human-reassigned IoU}
% Instead of using $\tau_\text{CLIP}$ to determine whether a segment $S_i$ should be reassigned with a ground-truth label $T^*_i$ (Section \ref{seg2text}), we use human ratings to determine the reassignment. 

% \textbf{IoU$_{\text{h}}^{\text{gt},\text{pred}}$}
% This score evaluates the IoU of segments that match with both their corresponding predicted labels and closest ground truth labels according to human judgment.
% Given a segment $S_i$ and its closest ground-truth labels $T^*_i$, 
% $S_i$ is relabeled with $T^*_i$ if HR$_{\text{s}-\text{pred}} \geq  \tau_\text{HR}$ 
% and HR$_{\text{s}-\text{gt}} \geq \tau_\text{HR}$.
% Then, IoU is computed as usual.

% \textbf{IoU$_{\text{h}}^{\text{gt}}$}
% This score evaluates the IoU of segments that  match their closest ground truth labels, disregarding the predicted labels.
% Given a $S_i$ and its $T^*_i$,
% $S_i$ is relabeled with $T^*_i$ if HR$_{\text{s}-\text{gt}} \geq  \tau_\text{HR}$.
% Then, IoU is computed as usual.

% Table \ref{tab:userstudy} presents human reassigned IoU with varying $\tau_\text{HR}$.
% whose HR$_{\text{s}-\text{pred}} \geq  \tau_\text{HR}$ and HR$_{\text{s}-\text{gt}} \geq  \tau_\text{HR}$.


% $\tau_\text{pred\_h}$ if human rating of a segment $S_i$ and its predicted label $T_i$ is than $\tau_\text{pred\_h}$, we reassign.
% \item $\tau_\text{st\_h}$ measures if humans find a segment $S_i$ and its closet ground-truth label $T^*_i$ match.

\begin{table}[]
\centering
\caption{\textbf{Human-reassigned IoU}}
% \vspace{-0.5em}
\label{tab:userstudy}
% \resizebox{0.85\columnwidth}{!}{
\begin{tabular}{ccc}
\toprule
 Human Rating &  IoU$_{\text{h}}^{\text{gt},\text{pred}}$  & IoU$_{\text{h}}^{\text{gt}}$ \\ \midrule
$\geq 0$        & 19.3  & 19.3 \\
$\geq 1$        & 18.7  & 24.1 \\
$ \geq 2$        & 14.8  & 23.0 \\
$ \geq 3$        &  6.0  & 15.5 \\ \bottomrule
\end{tabular}%}
% \vspace{-1em}
\end{table}






\subsection{Comparison to zero-shot open-vocab baseline} \label{sec:groupvit}
As a reference, we provide a comparison with GroupVit \cite{xu2022groupvit}, which solves a related but different segmentation problem. GroupVit \cite{xu2022groupvit} requires a text query from the test set to find each segment in the input image, and the number of segments is fixed to 8 per image by its architecture. Table \ref{tab:grouovit} shows that  GroupVit obtains a much better $\text{IoU}_\text{st}$ on PAS VOC with 20 classes; however, our method is significantly closing the gap on the more challenging PAS Context, despite using no text queries to help find semantic segments. 
\todo{Figure \ref{fig:groupvit} shows that our results can discover more objects in the input image and can label objects more specifically.}


\begin{table}[]
\centering
\caption{Comparison to GroupVit \cite{xu2022groupvit}, which solves a related but different segmentation problem and requires input text queries.}
% \vspace{-0.5em}
\label{tab:grouovit}
\resizebox{0.85\columnwidth}{!}{
\begin{tabular}{lcc@{\extracolsep{6pt}}cc}
\toprule
                            & \multicolumn{3}{c}{\textbf{PC-59}}                               & \multicolumn{1}{c}{\textbf{PAS-20}}                                  \\ \cline{2-4} \cline{5-5} 
\multicolumn{1}{c}{Method} & \rule{0pt}{2ex}$\text{IoU}_\text{st}$ & $\text{IoU}_\text{h1}$  & $\text{IoU}_\text{h2}$ & $\text{IoU}_\text{st}$  \\ \midrule
GroupVit \cite{xu2022groupvit} & 22.4      & -     & -      & 52.3      \\
Ours                           & 19.6      & 14.8      & 23.0      & 20.1     \\ \bottomrule
\end{tabular}}
% \vspace{-1em}
\end{table}\



\begin{figure}
\centering
% ablation_4 1.1
  \includegraphics[scale=0.43]{./figs/groupvit.pdf}
%   \includegraphics[scale=1.1]{./figs/ablation_4.pdf}
% \vspace{-1.7em}
  \caption{\textbf{Qualitative comparison to GroupVit \cite{xu2022groupvit}.} 
  Despite lower IoU score, our method discovers more objects in input images, including objects not shown in the dataset (hay, mirror). Our method can offer more specific labels, such as `stool'.
  }
  \vspace{-1em}
  \label{fig:groupvit}
%   \vspace{-1.5em}
\end{figure}