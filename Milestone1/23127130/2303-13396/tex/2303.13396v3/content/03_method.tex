\section{Approach}
\label{sec:method}
\begin{figure*}
\centering
  \includegraphics[scale=0.965]{./figs/overview.pdf}
  \vspace{-.2em}
  \caption{\textbf{Pipeline Overview.} Our method first segments an input image by clustering learned per-pixel features extracted from DINO. The input image is then fed into CLIP's image encoder. In this step, the produced segmentation masks are used to modify CLIP's attention to provide embeddings that are more focused to each segment. The resulting embeddings are then used to optimize a trained language model to generate texts closest to these embeddings. Lastly, segments with similar embeddings and text outputs are merged together to form more coherent segmentation results.}
  \label{fig:overview}
  \vspace{-1.em}
\end{figure*}
% This work introduces a novel open-vocabulary segmentation setup with labeling-text generation. Given an input image, we aim to segment this image into regions and give each region a text that indicates what it is. 
% problem setup involves vision understanding, as well as the ability to generate text from visual information.
% Instead of using image-caption pairs or image-segment pairs of datasets to train a new network to solve this problem, we distill vision-language information from three existing pretrained networks and solve this problem with no additional training.
% To segment and generate text that describes the segment, one needs the abilities to 1) understand visual cues, 2) map the visual cues into a language domain, and 3) generate texts describing the visual cues.   
% There are many existing datasets that can teach a network to solve this problem.
% However, with remarkable existing pretrained networks proposed recently, we believe the problem can be solve by merely distilling knowledge from these networks with no additional training.
% However, training such a model would surely require high computation and resources. 
% We propose to instead take advantage of public pretrained models that already contain information needed for these tasks. 
% we distill vision-language information from three existing pretrained networks and solve this problem with no additional training.
% We leverage three pretrained networks: 1) DINO-ViT for unsupervised image segmentation, 2) GPT-2 for text generation, and 3) CLIP for bridging the gap beteen image and text.

Given an input image, our goal is to partition this image into semantic segments and label each segment using words in natural language.
Our framework consists of four stages: 1) we identify segment candidates based on clustering deep per-pixel features of DINO-ViT \cite{caron2021emerging}, 2) we map each segment to an embedding in the CLIP's visual-language space using our proposed attention masking technique, 3) we translate each CLIP embedding into words by optimizing a generative language model with an existing technique, ZeroCap \cite{tewel2022zerocap}, and 4) we merge segments with similar semantics.
%This work involves partitioning images into semantic segments and translating these segments into words. The pipeline has four main stages: 1) clustering deep per-pixel feature of DINO-ViT into segments, 2) encoding visual segments into CLIP's space with our newly proposed attention masking technique, 3) translating segment embeddings to word labels by optimizing a generative language model, and 4) merge segments with similar semantic.


% \textbf{Overview}
% Zero-shot open-vocabulary segmentation with zero-shot text labeling involves dividing an image into segments and translating these segments into words. The challenge of this work is doing all this with no direct supervision from any region-based annotation. A recent method presents part co-segmentation using deep features of a self-supervised vision model, DINO-Vit. We find that DINO's deep features in its learned feature space are very discriminative, and simple clustering on these per-pixel features gives fairly reasonable segments. These segments need not to be perfect. Due to the lack of supervision, it is not probable that such a method can provide segments that are aligned with human expectation of segmentation. Instead, we let the clustering produce oversegments; one can think of them as super-pixels from classical segmentation approaches. 
% After this, we map all visual segments to labeling texts. This requires a network that learns joint text-image embedding, i.e. CLIP. One main novelty of ours is how we encode a segment, a partial image, into CLIP's text-image embedding space with no fine-tuing.
% With this ability and the resulting segment embedding, an existing or off-the-shelf text captioning can translate segments into words.
% Lastly, we simply merge segments with similar semantic into bigger segments.

% \textbf{Overviewest}
% Our key idea is to use existing trained models specialized in distilling visual information to perform zero-shot open-vocabulary segmentation with text labels generation. We segment an image by clustering per-pixel features extracted from DINO, whose self-distillation training provides discriminative features suitable for segmentation. Then, we encoder segmented regions into CLIP embedding space using attention masking to make the produced embeddings focus on targeted regions. Lastly, we use existing language model optimization method to find texts that are closest to region embeddings.


% Our key idea is to cluster pixels of input image into segments and use mask attention to encode each segment into CLIP's space, then use language model optimization to find labeling texts that are closest to the segments.

% \textbf{Overviewer}
% Our pipeline first clusters DINO's per-pixel features of an input image into segmented regions. Next, we encode these segmented regions into CLIP's space using attention masking and a novel technique called global subtraction which makes the produced embedding more focus on targeted regions. Then, language model optimization is used to create labeling texts for all segments.

% \textbf{Overview}
% Our pipeline takes as input an image, which is passed into pretrained DINO-ViT in order to obtain discriminative per-pixel features. These features are then clustered into segmented regions, representing regions of interest in the input image.
% Next, we create CLIP embeddings for each of these segments.
% Binary masks is computed for all segments and downsampled into CLIP's attention resolution.
% These masks is used to mask attention of CLIP in the latter layers to make the embedding more focus on targeted regions instead of the whole image, followed with a technique we introduced, global subtraction, to decrease the influence of global context in targerted region. After obtaining the region embeddings, language model optimization is used to products text for each embedding with objective that a generated text are close to its corresponding region embedding in CLIP space.

\subsection{Finding segment candidates with DINO} \label{sec:finding_segments}
%First, we partition images into parts. There are many ways to segment and image and our main contribution `attention masking' can be applied with segmentation masks. 
The goal of this step is to partition the input image into small over-segments, which will be merged in the final step. To do so, we first extract spatial features of the input image from DINO-ViT. In particular, we use the ``key'' values from the last attention layer as the features (following \cite{amir2021deep}), which have a total dimension of (\#patch$\times C$). Unlike in standard use of ViT, we use a small stride of two instead of the patch size, resulting in a dense feature map ($\frac{H}{2}$$\times$$\frac{W}{2}$$\times$$C$).

% To do this, we feed the input image into DINO-ViT and use the ``key'' values from the last attention layer as per-patch features, which have a total dimension of (\#patch$\times C$). 
% Following \cite{amir2021deep}, we slide a patch over the input image using a small stride (2) and feed each patch into DINO-ViT to compute a per-patch feature vector of size $C$, resulting in a dense feature map ($\frac{H}{2}$$\times$$\frac{W}{2}$$\times$$C$).
% Following \cite{amir2021deep}, these features are then upsampled and refined with DenseCRF \cite{li2018referring} to the original input resolution, resulting in a \todo{dense patch-wise} feature map ($H$$\times$$W$$\times$$C$). 
%These per-patch features are then refined using multi-label CRF \cite{krahenbuhl2011efficient}.
%more specifically the `key' matrix in the last attention
%Inspired by Amir et al.\cite{amir2021deep}, we first extract pixel-wise features from 
%We adopt this oversegment-then-merge design because merging is
%we can leverage the joint 

Given this dense feature map, we initially assign each feature vector ($1\times$$C$) its own cluster and perform agglomerative clustering by repeatedly merging any two clusters with the smallest combined feature variance. We stop this process when the target number of clusters $n = 20$ is reached, and we additionally merge clusters with similar feature vectors based on their cosine similarity, detailed in Appendix \ref{apx:clustering}. The output segments from this step may break single objects into small parts, which lack semantic meanings by themselves. However, our decision to oversegment first allows merging in the semantic space of CLIP later on, which takes into account both vision and language semantics and can be done with simple thresholding.
%, compared to splitting clusters.
%The merging history produces a binary tree where each node represents a segment and the root represents the entire image.
%This hierarchical clustering produces a tree structure 

%Here we use deep per-pixel features extracted from DINO-ViT, a pretrained self-supervised network.
%We derived this method from Amir et %al.\cite{amir2021deep}. Their experiment shows that DINO-Vit's features give reasonable segmentation results without any segmentation labels required 
% we find segments in an image using deep per-pixel features from a pretrained self-supervised network, DINO-ViT.
% Our segmentation method is closely similar to Amir et al.\cite{amir2021deep}'s.
%Based on Amir et al.'s work, we pass an input image into DINO to extract per-pixel features from the deep attention module, more specifically the `key' matrix in the last attention block and then cluster the extract features into segments with k-mean clustering. Lastly, the segment masks are refined using multi-label CRF \cite{krahenbuhl2011efficient}.
%We follow most of this pipeline except for the clustering part.
% The clustering of this pipeline is our own design.
%As the desired number of partitions across images can be different, we select an algorithm that does not restrict the number of clusters.
%We hierarchically partition an image and stop when the new partitions have average DINO features with cosine similarity over a threshold $T_{Dino}$.
%We use a binary tree structure to represent the hierarchical partition of the input image. 

%To construct the binary tree, we use agglomerative clustering, a bottom-up algorithm that first regards each pixel as a cluster and then merges pairs of nearest clusters until a stop condition is met. We keep merging the cluster until there is only one cluster left (the whole image). The tree records merging of the last $n$ clusters. The last cluster is assigned as the root of the binary tree and two clusters that form the root as its children. Children nodes are added to the tree in the same manner until their cosine similarity is over $T_{Dino}$. 
% Each sibling pairs of this tree are two segments that can be merge to reconstruct their parent.

% We use AgglomerativeClustering from sklearn to form a binary tree where the root is the whole in put image. each sibling pairs of this tree are two segments that can be merge to reconstruct their parent.
% We recursively partition a segment into two new segments and stop when the cosine similarity of the two new segments' average feature vectors is less than a threshold $T_{Dino}$.
% with a condition that the cosine similarity of the two new segments' average feature vectors must be higher than a threshold $T_{Dino}$.

% This image partition method requires no additional training nor annotations. 
%The segments produce from this partition need not be perfect.
% Without human supervision, it is rational that segments from this method are not in alignment with human-labeled segments. Instead, 
%We let the clustering produce oversegments; one can think of them as super-pixels from classical segmentation approaches. These segments will be merged later in the pipeline.
% We find that DINO's features are very discriminative, and simple clustering on these per-pixel features gives fairly reasonable segments. 

% introduced a method to perform part co-segmentation by clustering `key' in the last self-attention modules. From their results, DINO's features show strong association between same object parts across different images. Amir et al. use k-mean clustering to group features from multiple images into a predefined number of clusters. We use a similar method to segment input images one by one. Unlike Amir et al., we use hierarchical clustering to branch a segment into two new segments with a condition that the cosine similarity of the two new segments' average feature vectors must be higher than a threshold $T_{Dino}$.

\subsection{Transforming segment candidates into CLIP's vision-language embeddings}
\label{sec:region-emb}
%\subsection{Transforming segment candidates into CLIP's vision-language embeddings}
To map a given segment to CLIP's vision-language space, our idea is to feed the entire input image into CLIP's image encoder while masking some of the encoder's attention layers with an alpha mask corresponding to the given segment. 
%
One major consideration is which layers should the masking be applied to properly balance global and local contexts. This turns out to be challenging: masking in earlier layers destroys global contexts, whereas masking in later layers eliminates local contexts. This finding agrees with several studies \cite{zhou2021deepvit, xue2022deeper, gong2021vision} showing that vision transformers trained for classification suffer from an ``attention collapse,'' where the attention in deeper layers becomes near uniform and all tokens converge to the same value. 
Another study \cite{shen2021much} also suggests that CLIP may lack the ability to maintain local information. In Appendix \ref{apx:clip-visualize}, we show how CLIP's attention maps become less localized in later layers.
%across layers and show how the attention maps are less localized in later layers.
%in Appendix \ref{apx:clip-visualize}.
% We also visualize this phenomenon in Figure \ref{fig:self_attn}. 
%As shown in Figure \ref{fig:self_attn}, the attention maps in earlier layers focus on local individual objects but converge to the same 
%, but in deeper layers, attention maps of both patch tokens and global token appear more uniform, suggesting non-local context is mixed into all patches.

Masking in the middle layers also performs poorly because different objects still require different degrees of context balancing depending on how salient they are in the scene. For example, the embedding of a small, obscure object in the background can be dominated by global contexts, which describe other prominent objects in the scene. As a result, these small objects may require more de-emphasis of the global contexts for their semantics to emerge.
%which require more de-emphasis of the global contexts for the object's semantic to emerge.
%, compared to those of a prominent object.

Based on this observation, we propose a simple technique to estimate the saliency of each segment and use it to modulate how much global information should be removed or subtracted from individual tokens during attention masking. We next explain how we apply masking to the attention module, and then our \emph{global subtraction} technique.

%Adding to this challenge is the fact that different objects may require different degrees of balancing. E.g., a salient object  
%Masking near the middle layers also does not provide satisfactory results.
%resulting in all embeddings converging to the same primary object. Masking  This finding agrees with several studies \cite{zhou2021deepvit, xue2022deeper, gong2021vision}, which show that vision transformers trained for classification results in an ``attention collapse,'' where attention in deeper layers becomes near uniform and all tokens are nearly identical. 


%As discussed earlier, both local and global contexts are important for segmentation
%masking in earlier layers reduce global contexts, which are important for recognition, whereas masking in later layers fails to properly focus on a given segment, resulting in all embeddings and labels describing the same dominant object in the image. This finding agrees with several studies \cite{zhou2021deepvit, xue2022deeper, gong2021vision}, which show that vision transformers trained for classification results in an `attention collapse', where attention in the later layers become near uniform, and all tokens are nearly identical. 

%In this step, we map all visual segments into a joint text-image space.
%CLIP image encoder can easily encode any input image into an embedding of joint space. However, obtaining an embedding that represents a specific region in an image is not as straightforward. 
%CLIP is trained on natural images, and using masked images with black regions that are not presented in the training data as input may cause unexpected behaviors.
% Simply masking the input images causes a domain gap between the masked images full of black regions and CLIP natural training images.

%Alternatively, we propose applying segmentation masks on CLIP's self-attention, but doing this straightforwardly brings on two challenges. 
%1) Masking self-attention in all layers removes all context of the segmented region's surroundings. For example, in an image that shows the underside of a bridge, masking out all but the bridge can lead to the conclusion that the region is some other steel arch structures like tunnels. Showing the surrounding context, like the water underneath, can be beneficial to the prediction.
%2) Transformer's patch tokens are not reliable in preserving the local information of each patch. 
%Several publications \cite{zhou2021deepvit, xue2022deeper, gong2021vision} have similar findings that vision transformers trained for classification results in `attention collapse', where attention in the latter layers become uniform, and all tokens are nearly identical. 
%As CLIP is trained on an image-caption dataset, the network is biased on foreground objects mentioned in captions, and the context of these foreground objects, or the global context, often dominates the embedding of unmasked regions that belong to minor objects and background making the information about those specific regions (local context) disappear. 
%As shown in Figure \ref{fig:self_attn}, attention maps of patch tokens in earlier layers focus on themselves and patches with similar appearance, but in deeper layers, attention maps of both patch tokens and global token appear more uniform, suggesting non-local context is mixed into all patches.

% To overcome attention collapse, a method add training objectives to promote diversity in patch tokens \cite{gong2021vision}, the other one adds an extra module to regenerate attention maps \cite{zhou2021deepvit}. Our work introduces a novel technique, `global subtraction', that can get reduce of global context from each patch token without having to retrain the whole pipeline.

% masking
\vspace{-.7em}
\subsubsection{Masking in self-attention module}\vspace{-.2em}
Given a logit vector $x \in \mathbb{R}^n$ and a flattened mask $M \in [0, 1]^n$, we first define the masked softmax operator as:
\begin{equation}
\mathrm{MaskedSoftmax}(x, M) = \frac{e^{x} \odot M}{\sum_{j=1}^{n} \left ( e^{x_j} \times M_j \right )},
\end{equation}
where $\odot$ denotes the element-wise multiplication.
%To create CLIP embedding that represents a region in an image, we propose to use masking inside the attention module so that the encoding process can put more focus on the targeted region.
%In CLIP's image encoder, there are $\#patch+1$ tokens. The first token stores global information of the whole image while the rest encapsulates the information of the $\#patch$ non-overlapping patches.
%Transformer's self-attention module allows all tokens to put varying amounts of focus on each other. Our approach leverages this mechanism to enforce the focus onto the target tokens by applying a mask on the attention matrix.
%Our method masks self-attention from a layer $l$ to the last layer of CLIP image encoder. The starting layer $l$ is selected empirically.
%As defined in \cite{vaswani2017attention}, attention $\text{Attn}$ is computed as
To mask a standard attention module, we compute $A_i^\text{masked}=\mathrm{MaskedSoftmax}\left(Q_{i}K^{T} / \sqrt{d_{k}}, M\right)V$ for \emph{every} token $i$.
%compute the attention output for each token $i$ as follows:
%\begin{equation}
%    A_i^\text{masked} = \mathrm{MaskedSoftmax}\left(\frac{Q_{i}K^{T}}{\sqrt{d_{k}}}, M\right)V.
%\end{equation}
In practice, when the mask size is larger than the visual patch grid, we first downsample $M$ to the same size using area interpolation.
%before applying the masked softmax. 
We also prepend one extra element to the flattened $M$ for the global token, which is always set to one in our algorithm, and thus $\text{\#tokens} =\text{\#patches} + 1$.
%\todo{We apply the same mask for all tokens.}
%are query, keys and values 
%\begin{equation}
%    Attn = \text{softmax}(\frac{QK^{T}}{\sqrt{d_{k}}})V
%\end{equation}
%where $Q$, $K$, and $V$ are query, keys and values respectively, and $\frac{1}{\sqrt{d_{k}}}$ is a scaling factor based on the dimension of keys $d_{k}$.
% In a transformer, there are $\#patch+1$ tokens. The first token stores global information of the whole image while the rest encapsulates the information of non-overlapping patches.
%$\text{softmax}(\frac{QK^{T}}{\sqrt{d_{k}}})$ represents the distribution over $V$. for each token $token_{i}$, the dot product of vector $Q_{i}$ and $K$ determines how much of value $V_{i}$ of all tokens is retrieved. 
%We first create $Mask_{S}$ of dimension $\#patch \times  \#patch$ by flattening the down-sampled binary segmentation mask.
%Then, $Mask_{A}$ is define as a one-dimension of size $\#patch^{2}+1$ to account for the global token, which is set to $1$ and the rest is $Mask_{S}$.
%We define a custom softmax function that simultaneously masks and normalizes an input vector $x$ of size $n$ given $mask\, \epsilon \left [ 0,1 \right ]$ of the same size:
%\begin{equation}
%\text{masked\_softmax}(x, mask) = \frac{e^{x} \odot mask}{\sum_{i=1}^{n} \left ( e^{x_i} \cdot mask_i \right )}
%\end{equation}
%where $\odot$ denotes Hadamard operation.
%Then, each $Q_{i}K^{T}$ is masked as follow
%\begin{equation}
%    MaskedAttn_i = \text{masked\_softmax}(\frac{Q_{i}K^{T}}{\sqrt{d_{k}}}, Mask_S)V.
%\end{equation}
% where 
% \begin{equation}\label{eq:mask-fn}
%     \mathbb{M}(\frac{QK^{T}}{\sqrt{d_{k}}}, Mask_S) = \frac{Q_{i}K_{i}^{T}}{\sqrt{d_{k}}} + (Mask_S-1) \times inf.
% \end{equation}
% The second term of Equation \ref{eq:mask-fn} turns 1s in $Mask_S$ into 0s and 0s into $-inf$s. After applying softmax function, $-inf$ values will turn to 0s, thus masking those values out from $V$.


% \begin{figure}
% \centering
%   \includegraphics[scale=0.45]{./figs/self_attn.pdf}
%   \vspace{-.5em}
%   \caption{\textbf{Visualization of self-attention in CLIP's image encoder.} Each row shows the attention of the token of the pink patch across layers. The last row shows global token's attention.}
%   \label{fig:self_attn}
%   \vspace{-1.2em}
% \end{figure}

\begin{figure}
\centering
  \includegraphics[scale=0.82]{./figs/attention_masking.pdf}
  \caption{\textbf{Attention masking and global subtraction.} To encode a segment into CLIP's space, we pass the input image into CLIP's image encoder and mask self-attention map in some layers with the segment's mask. We apply this masking inside masked softmax function while still computing normal softmax. Cosine similarity between masked and unmasked output is used to estimate the saliency of the region. This similarity determines how much global context needs to be reduced in global subtraction.}
  \label{fig:attention_masking}
  % \vspace{-1.2em}
\end{figure}

% \vspace{-.7em}
\subsubsection{Global subtraction}
% \vspace{-.2em}
To balance global and local contexts in our output embedding, we design a proxy function that estimates the ``saliency'' of each segment or the segment's presence in the global contexts. This value will be used to determine how much global contexts should be removed from the attention output. Note that our saliency value is only defined with respect to the attention mechanism in CLIP and is unrelated to other uses of ``saliency'' in the literature \cite{4270292, 6112774}. 
%the input and is unrelated to the term saliency.
%The less salient an object is, the more global contexts should be removed. 

%Given an input segment that is fed to CLIP's image encoder, 
We perform the following operations separately for each attention layer $l$ using its own saliency value $\mathcal{S}_l$. We compute $\mathcal{S}_l$ as the cosine similarity between the masked attention output $A^\text{masked}$ and unmasked attention output $A= \mathrm{Softmax}(QK^T/\sqrt{d_k})V$ at layer $l$, averaged over all tokens (we omit the subscript $l$ from $A$ and $A^\text{masked}$ for simplicity):
%The computed saliency for attention layer $l$ is given by:
\begin{equation}
    \mathcal{S}_l = \frac{1}{\text{\#tokens}}\sum_{i = 1}^{\text{\#tokens}}\mathrm{cossim}(A_i, A_i^\text{masked}).
\end{equation}

The output of the attention layer $l$ is computed by subtracting the unmasked attention output of the global token $A_\text{0}$ from the masked attention $A^\text{masked}$:
\begin{align}
    A^\text{out} &= A^\text{masked} - w A_0,\\
    \label{eqn:salience_est}
    \text{where } w &= \exp(-(\mathcal{S}_l+1)^2 / 2\sigma^2).
\end{align}
This global subtraction weight $w$ is computed by applying a Gaussian function with a standard deviation $\sigma$ to $(\mathcal{S}_l+1)$, making $w$ highest when $\mathcal{S}_l=-1$. In other words, when an object is \emph{not} salient, 
%$w$ becomes higher so that 
we remove \emph{more} global contexts from its attention output. 
% [which layer do we start?] \todo{21}
We apply these masking operations starting from attention layer 21, which is chosen empirically, to the last layer 24. Finally, the embedding of our segment is the output from our masked CLIP's encoder, which additionally applies linear projection to the global token value from the last attention layer.
%which also contains a linear projection layer applied to the global token value from the last attention layer.
%Finally, to compute the embedding of our segment, we mask attention from layer 21 to layer 24 (the last layer) with the segmentation map then use the embbeding produced by projecting the global token of the last layer into CLIP's space.
%which determines how much global contexts should be attenuated.

%To lessen the degree of the global context in each region, we introduce a new technique called \emph{global subtraction}. 
%This technique assumes that attention of CLIP is linearly separable.
%ZeroCap \cite{tewel2022zerocap} performs CLIP's embedding arithmetic which works for both image and text. For instance, for two embedding vectors of daytime and nighttime view images of the same place, subtracting  daytime embedding from nighttime embedding will produce an embedding vector that is closely associated with the word `night'. We perform similar arithmetic in attention space to reduce some global context from each segmented region.
% This indicates that CLIP's latent space is linearly separable, i.e., we can subtract a certain context from an embedding and the other remaining context in that embedding is maintained.
% We follow this assumption to reduce some global context from each segmented region.
%Global attention, or the attention of the global token, is computed as follows:
%\begin{equation}
%Attn_{global} = \text{softmax}(\frac{Q_{global}K^{T}}{\sqrt{d_{k}}})V
%\end{equation}
%where $Q_{global}$ is the query of the global token.

%As the contribution of the global context on each segment varies, we weigh the degree of global subtraction for each segment with a different weight $w$. 
%A minor region that has little relation to the global context needs a larger $w$, whereas a foreground region that is highly related to the global context needs a smaller $w$.
%We define the similarity of $Attn$ and $MaskedAttn$ with cosine similarity
%\begin{equation}
%    sim = \frac{1}{\#patch+1}\sum_{i = 1}^{\#patch+1}\text{cossim}(Attn_i, MaskedAttn_i).
%\end{equation}
%$w$ is compute with a Gaussian function of $sim$ with standard deviation $\sigma$.
%\begin{equation}
%    w = e^{-\frac{(sim+1)^2}{2\sigma^2}}
%\end{equation}
%Then we perform global subtraction as follow:
%\begin{equation}
%    \hat{MaskedAttn_i} = MaskedAttn_i - w * Attn_{global}.
%\end{equation}

\subsection{Text generation from CLIP's embedding}\label{sec:textgeneration}
To translate our segment embedding in CLIP's joint space into words, we use an existing image-to-text generation algorithm, ZeroCap \cite{tewel2022zerocap}.
%, to convert the embedding into words. ZeroCap uses 
%In CLIP's space, texts and images that are semantically similar lie close to each other. We leverage this quality to find a text close to the targeted region using simple optimization on a pretrained language model.
%We use a CLIP-based text generation method proposed by ZeroCap \cite{tewel2022zerocap}. 
% Here we describe their method.
This method uses a pretrained language model GPT-2 \cite{radford2019language} along with CLIP to optimize for a sentence that describes an input image. This is done by optimizing specific activations of GPT-2 (K and V matrices) to complete an initial prompt of  ``Image of a ...'' and minimizing the difference between the output sentence and the input image in CLIP's joint space.


%autoregressively produce a new word from an initial text prompt.
%They optimize the language model so that it would generate text the lies close to the targeted image in CLIP's space.
% Instead of letting the model sample a new word freely from GPT-2's space, an objective function of minimizing the distance of the generated text sequence and the targeted visual in CLIP's space is used to optimize the context cache (which stores key and value from attention module of the previous words).
%We use this process to guide the output texts toward our segment embeddings with the same initial prompts as ZeroCap: ``Image of a''.

% to generate text from CLIP embedding of each region. This work optimizes cache context (from previous words) of a generative language model, namely GPT-2 \cite{radford2019language}, to autoregressively produce the next words that bring CLIP embedding of the generated text sequence closer to the segment embedding.


\subsection{Merging segment candidates} \label{sec:merge}
In this step, we merge segments that are semantically similar or small segments that may not be so meaningful by themselves from our oversegmentation. 
We compute the similarity score between two segments using the average of two measures: 1. the cosine similarity between their visual embeddings (Section \ref{sec:region-emb}) and 2. the cosine similarity between their predicted texts' embeddings computed from CLIP's text encoder. 
In our implementation, we also reduce the number of merging combinations by limiting the pairing option. In particular, we first continue the agglomorative clustering in the first step (Section \ref{sec:finding_segments}) until there is a single cluster representing the entire image. By keeping track of the merging history, we obtain a binary tree where each node represents a segment and each parent is the merged segment of its children. We limit the pairing to only between siblings in this tree and recursively merge segments up the tree when their similarity score is at least $\tau_\text{merge}$. The final embedding of a merged segment is computed by passing the corresponding merged mask through the embedding pipeline (Section \ref{sec:region-emb}) and is then used to generate the final predicted text.
%We recompute new visual and text embeddings after merging.
%We do this by computing the cosine similarity between the embeddings of any two segments.
%and merge them if the value is above a threshold. 
%In particular, we compute the cosine similarity between two segments in both image and text spaces  and take the mean.

%we compute the cosine similarity between two segments by encoding both visual segments and their predicted texts to CLIP's space and computing the mean cosine similarity over both spaces.

%The desired semantic level of segments is task-dependent, hence segments from unsupervised algorithm are often not semantically aligned with one's objective. For example, the clustering might separate cars and their wheels into two segments. Due to that reason, we propose a novel technique to control the semantic level of the desire segments by merging semantically similar segments.
                                                                                                          