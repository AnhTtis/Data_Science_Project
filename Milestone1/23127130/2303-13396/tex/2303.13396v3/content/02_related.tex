\section{Related Work}
\label{sec:related}
% \vspace{-.3em}
\textbf{Open-vocabulary segmentation.}
This problem aims to predict segments in an input image that correspond to a set of input texts not necessarily seen during training. %without having to see these text during training.
Prior solutions often involve a shared latent space between the image and text domains. 
OpenSeg \cite{ghiasi2021open} uses datasets of images, captions, and class-agnostic segmentation masks to train a mask proposal network before matching the predicted masks with nouns in the captions using a shared latent space. 
OVSeg \cite{liang2022open} built a segmentation pipeline that fine-tunes CLIP on masked images to make it more suitable for masked image classification. 
% in a mask proposal network.
% to take as input mask images and replace meaningless tokens from black region with learnable tokens to adapt CLIP to mask image while preserving generalization that is lost during fine-tuning.
Xu et al. \cite{xu2021simple} proposed a zero-shot segmentation baseline by matching CLIP's embeddings of masked images to text embeddings of classes.
ZegFormer \cite{ding2022decoupling} performs class-agnostic pixel grouping to create segments and uses CLIP to classify them. 
Lseg \cite{li2022language} trains an image pixel encoder that encodes each pixel into an embedding that is close to the corresponding text labels' embeddings in CLIP's space.
These methods show impressive results but still demand expensive segmentation labels.

To avoid the use of segmentation datasets, GroupViT \cite{xu2022groupvit} proposes a new method based on hierarchical vision transformers where the visual tokens represent arbitrary regions instead of patches in a square grid. By using only image-caption pairs, GroupVit can match each region from its visual tokens to input text prompts.
% of pre-defined classes. 
Zhou et al. \cite{zhou2021denseclip} modifies CLIP for text-guided segmentation and employs self-training to improve the results.
% Several methods first acquires class-agnostic masks then match each text prompts to a masked region.
% Unlike these approaches, not only our method works without additional training, we also do not need the input text prompts
In contrast, our work requires neither additional training nor text prompts but can discover semantic segments and label them automatically.
%not only does our method work without additional training, but we also do not require input text prompts as our work masks CLIP's attention to produce embedding focus on only a segment in the input image and generate texts that represent the segments with zero supervision.

\textbf{Attention masking in transformer.}
Masking self-attention is a common practice in NLP to input a word sequence more efficiently \cite{vaswani2017attention}.
% prevent a word token from looking at its following tokens in an input sequence \cite{vaswani2017attention}. 
In computer vision, few explorations exist:
%masking in transformer's attention module is still underexplored.
Mask2Former \cite{cheng2022masked} solves supervised segmentation %given predefined classes 
by masking self-attention layers of a transformer decoder, achieving state-of-the-art results. % in several benchmarks.
%This decoder takes a multi-scale feature pyramid of an input image computed from a pretrained backbone \cite{he2016deep, liu2021swin}, then uses the feature in each layer/scale to predict an attention mask for the subsequent layer. 
%Their network is then trained 
%. These image features are used to compute predicted segment masks by using their dot product with object query. 
% into segmentation masks and their corresponding segment features.
%In each layer, Mask2Former simultaneously predicts segmentation masks and masks the attention of the image features with predicted masks from the previous layer.
%Masking the attention of the image features produces segment features focus only on the masked regions.
%This segment features are used to classify each segment into a class label instead of doing per-pixel classification..
%Mask2Former achieves state-of-the-art results in several segmentation benchmarks.
% proposes a novel pipeline designed specifically for the conventional segmentation task that predicts segment masks of trained classes from an input image. 
% Their idea is to mask attention of a transformer decoder to extract features of predicted segment regions and use that features to classify each segment into a class label instead of doing per-pixel classification.
% Mask2Former takes per-pixel image features extracted from a network trained on ImageNet \cite{}.
% In each layer, Mask2Former simultaneously predicts segmentation masks and masks the attention of the image features with predicted masks from the previous layer.
% As the layer goes deeper, the predicted masks become more refined.
% The last layer outputs both segmentation masks and their corresponding segment features, which is use for segment classification.
% Mask2Former first extracts low-resolution image features from a backbone network trained on ImageNet \cite{}.
% Then, a pixel decoder is used to gradually upsample these features and produces pyramid feature maps of higher resolutions.
% Lastly, the transformer encoder attend to the image features in increasing resolution across the layers, as well as predicting segmentation maps in increasing size.
% The predicted masks are used to mask the attention of the next layer to restrict the attention within the predicted regions and create region features that are use for classification. 
% The key idea is to extract the feature of each segmented region by masking the attention to focus only on the region.
Unlike Mask2Former, which is trained on specific segmentation datasets, our method and the base models we used (CLIP and DINO-ViT) do not have any explicit segmentation supervision. We found that using the masking mechanism of Mask2Former yields noisy CLIP embeddings, which are often heavily biased toward the foreground objects. This can be solved by our proposed global subtraction technique.

%Our work uses a similar method to enforce focus on the targeted region of a trained network. This poses a new challenge because, unlike Mask2Former, CLIP is trained without any region-based information and does not have any explicit supervision to maintain spatial information of an image. As such, masking attention yields noisy embeddings which may be overly dominated by the foreground objects. Our method also introduces a technique to lessen the degree of the foreground context in the segment embeddings.

\textbf{Image segmentation with DINO.}
DINO \cite{caron2021emerging} is a model that uses self-distillation to learn rich features of an input image with no supervision and has been used as a pre-trained network or representation extractor in many tasks \cite{vaze2022gcd, wang2022tokencut, Simeoni2021LocalizingOW, hamilton2022unsupervised}.
%DINO-ViT \cite{caron2021emerging} uses self-distillation to learn rich features with no supervision and is used as a pre-trained network in numerous tasks \cite{vaze2022gcd, wang2022tokencut, Simeoni2021LocalizingOW, hamilton2022unsupervised}. 
%DINO-ViT's features have been shown to capture object boundaries and the scene layout, and encode association between the same object parts across different images \cite{amir2021deep}.
Caron et al. \cite{caron2021emerging} demonstrated that DINO's features effectively capture object boundaries and scene layout \cite{caron2021emerging}, and Hamilton et al. \cite{hamilton2022unsupervised} further showed that these features can perform segmentation of not only foreground objects but also other elements in the background, such as the sky.
%other components present in the scene, including background elements like the sky.
%and can be used to locate object parts across different images (part co-segmentation) through clustering techniques \cite{amir2021deep}. Recent work 
%Their studies have shown that DINO-ViT's features explicitly capture object boundaries and the scene layout. Performing k-NN directly on DINO-ViT features can achieve 78.3\% top-1 accuracy on ImageNet.
%DINO-Vit self-attention is shown to focus on the foreground image without being taught to. 
%This property leads to DINO-Vit being more amiable to segmentation-related tasks than other pretrained visual encoders trained on image-level annotations.
%These DINO's properties are appealing to segmentation-related tasks than other pretrained visual encoders trained on image-level annotations.
%DINO-ViT has been explored in several interesting applications.
% This property leads to some interesting applications.
% Amir et al.\cite{amir2021deep} does zero-shot part co-segmentation by clustering DINO's deep features. The results shown that DINO features are discriminative and can be used to diffirentiate the same object parts. 
%Amir et al. \cite{amir2021deep} introduced a method to perform part co-segmentation by clustering DINO's deep features, which encode a strong relationship between identical object parts across different images.
% Splice-ViT \cite{tumanyan2022splicing} uses structure information in the patch tokens and style information in the class tokens to perform image translation by training on only one image pair.
% \todo{Splice-ViT \cite{tumanyan2022splicing} leverages DINO's rich information to perform image style transfer by extracting appearance from DINO's global information of the target appearance image and structure from DINO's patch information of the source structure image, then combining the two into a new image.}
% Given two input images, Splice-ViT \cite{tumanyan2022splicing}generates a new image with the appearance of one input and the structure of the other by  leveraging DINO to extract rich appearance and structure information from the two inputs. Splice-ViT produces high-quality results without any additional training data other than the input pairs.
% information from DINO's visual tokens and global [CLS] token structure information from DINO's 
%Unsupervised features from DINO have been successfully used to segment not just foreground objects, but also other components that appear in the scene or the background, such as the sky \cite{hamilton2022unsupervised}. 
%DINO also provides competitive results on unsupervised part segmentation [1].
Our method uses a simple DINO-based clustering, inspired by Amir et al. \cite{amir2021deep}, which requires no training and offers reasonable results.
%Our simple DINO-based clustering inspired by \cite{amir2021deep} requires no training, is easy to adopt, and offers good results. 
Note that our key contribution in attention masking is orthogonal to this clustering choice.
%and our framework supports any clustering model.
%Our pipeline leverages Amir et al.'s findings and partitions an image into initial segments also by clustering DINO features.
%We also partition an image into initial segments by clustering DINO features like \cite{amir2021deep} with an additional merging step.

\textbf{Image-to-text generation with CLIP.}
The recent advent of CLIP leads to new approaches in text-image tasks, including generating text from an input image.
ClipCap \cite{mokady2021clipcap} trains a mapping network that joins CLIP with a pretrained language model, GPT-2, and performs image captioning with faster training.
ZeroCap \cite{tewel2022zerocap} performs zero-shot image caption by optimizing the value matrix $V$ in each attention module in GPT-2 to guide the embedding of the output text toward the target image's embedding. The output texts display knowledge learned from CLIP's vast and diverse training set, such as names of celebrities and pop culture references. This is a new ability unseen in older image captioning methods. Note that our contribution is not directly in text generation, rather we focus on inferring semantic segments and mapping them to CLIP's latent space.



% \textbf{Joint applications between Vision and Language}
% Recently, the advent of CLIP (Contrastive Language-Image Pretraining) \cite{radford2021learning}, a contrastive model trained on a large-scale dataset of image-caption pairs, has led to big improvement in tasks that require both vision and language understanding. DALL-E\cite{ramesh2021zero}, DALL-E2\cite{ramesh2022hierarchical}, Imagen\cite{saharia2022photorealistic}, and Stable Diffusion\cite{Rombach_2022_CVPR} show remarkable results in text-to-image generation using CLIP shared embedding space to transfer information between the two domains. ClipCap\cite{mokady2021clipcap} trains a mapping network joining CLIP to a pretrained language model, GPT-2, and performs image captioning with relatively fast training.
% ZeroCap \cite{tewel2022zerocap} perform zero-shot image caption by optimizing values in GPT-2 to guide embedding of the output text toward target image's embedding.
% These approaches demonstrate that CLIP can map semantic between vision and language.

% \textbf{DINO-ViT}
% Vision Transformer (ViT)\cite{dosovitskiy2020image} has replaced CNNs as the dominant architecture for vision networks. One of many interesting properties of Vit is that resolution of the network remains the same throughout all layers, unlike general CNNs whose resolution decreases across the network. In general cases, CNNs' deep features lack spatial information and contain only high-level features, which is useful for image-level tasks like classification. There are variation of CNNs that produce high-resolution deep features (such as UNet\cite{ronneberger2015u}), but the global information is lacking in this case. On the other hand, ViT's deep feature contains both global information in the class token and local spatial information in the patch tokens. 
% DINO-ViT\cite{caron2021emerging} use self-distillation method to learn rich features with no supervision and is used as pre-trained network in numerous tasks. DINO-Vit self-attention is shown to focus on the foreground image without being taught to. This property mixed with Vit unique deep feature leads to some interesting applications.
% Amir et al.\cite{amir2021deep} does zero-shot part co-segmentation by clustering DINO's deep features. 
% Splice-ViT \cite{tumanyan2022splicing} uses structure information in patch tokens and style information in the class tokens to perform image translation by training on only one input image pair.