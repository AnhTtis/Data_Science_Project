
\section{New Evaluation Protocol}  \label{sec:eval}
This section introduces new metrics to evaluate the quality of the output segments and their corresponding text labels.
%for our new problem setup. 
%In this section, we introduce new evaluation metrics that can quantify the performance of our newly proposed problem setup. 
%The setup takes as input an image and output segmentation masks and text labels for all the masks. 
To overcome the evaluation challenges due to the use of synonyms or the difference in label granularity, such as ``car'' vs ``wheel,'' we first map the predicted labels to the predefined ones in the test set (Section \ref{label-re}) and verify the reassignment using thresholding or human evaluation (Section \ref{sim-thres}) before applying standard metrics such as IoU.
%This section describes metrics we used to evaluate our results. There are two main parts to consider: the accuracy of output segments and the quality of corresponding output labels.

% This section introduces new metrics to evaluate the accuracy of the output segments and their corresponding text labels.
% To overcome the evaluation challenges due to the use of synonyms or the variation in label granularity, such as ``car'' vs ``wheel,'' we first map the predicted semantic labels to the predefined ones in the test set.

\subsection{Label reassignment} \label{label-re}
Given a predicted segment $S_i$ and its predicted text $T_i$, our goal is to relabel $S_i$ with $T^*_i$, which should be one of the test labels. We describe two reassignment techniques based on text-to-text and segment-to-text similarity.

\myparagraph{Text-to-text similarity (TT).} 
%\subsubsection{Text-to-text similarity}  \label{text2text}
This technique relabels $S_i$ with the ground-truth label that is closest to $T_i$ in the embedding space of Sentence-BERT \cite{reimers-2019-sentence-bert}, a pre-trained text encoder widely used in NLP for computing text similarity \cite{gao2021simcse,li2020sentence,choi2021evaluation}.
%However, the relabelling only happens if $T_j^\text{gt}$ is sufficiently similar to $T_i$
%To measure the semantic similarity, we use cosine similarity in the embedding space of Sentence-BERT \cite{reimers-2019-sentence-bert}, a pre-trained text encoder widely used in NLP for computing text similarity \cite{gao2021simcse,li2020sentence,choi2021evaluation}. 
Formally, the new label $T^*_i$ is computed by
%, and a threshold $\tau_\text{SBERT}$ to test whether two words are sufficiently similar. Formally, the new label of $S_i$ is given by:
\begin{align}
    T^*_i &= \argmax_{t \in T^\text{gt}} \left[ \mathrm{cossim}^\text{SBERT}(T_i, t)\right],
\end{align}


%Note that using CLIP's text encoder for this purpose is not ideal for two reasons. 
%First, since our method directly uses CLIP, matching text based on the same CLIP model may be unfair to others that train their embedding spaces from scratch or use other pre-trained text models. Second, our experiment in Appendix \ref{apx:text_sim} shows that there are numerous occasions where the similarity in CLIP's embedding space highly contradicts human judgement. For example, in CLIP's space, the word ``dog'' is closer to ``car'' than ``labrador.''

\myparagraph{Segment-to-text similarity (ST).}
%\subsubsection{CLIP-based label reassignment}
%\label{seg2text}
% \todo{Given a predicted segment $S_i$ and its predicted text $T_i$, our goal is to relabel $S_i$ with the ground-truth label $T^*_i$ closest to $S_i$ in the CLIP's joint image-text space \cite{hessel2021clipscore, kawar2022imagic}.} 
This technique relabels $S_i$ with the ground-truth label that is closest to $S_i$ in the CLIP's joint image-text space \cite{hessel2021clipscore, kawar2022imagic}. That is, 
%Formally, the new label $T^*_i$ is computed by
%Formally, let $j$ indicates the closest the new label $T^*_i$ is computed by:
\begin{align}
    T^*_i &= \argmax_{t \in T^\text{gt}} \left[ \mathrm{cossim}^\text{CLIP}(S_i, t)\right],
\end{align}
where $\text{cossim}^\text{CLIP}(s, t)$ uses CLIP's image encoder for $s$ and text encoder for $t$. Note that this relabeling is commonly used in open-vocabulary settings \cite{liang2022open, ghiasi2021open}, but it does not consider our predicted label $T_i$ during relabeling. Nonetheless, this technique is still valuable as it offers a complementary assessment that does not involve text generation, which is based on prior work (Section \ref{sec:textgeneration}), or text-to-text mapping, which can be challenging and ambiguous even for human evaluators (Section \ref{sec:analysis}).
%, it provides a complementary evaluation that does not involve text generation or text-to-text mapping, which can be challenging and ambiguous even for human evaluators.
% but it is helpful for evaluating our pipeline \emph{independent} of the text generation which is borrowed from prior work (Section \ref{sec:textgeneration}). 

%Note that this mapping technique is commonly used in open-vocabulary settings \cite{liang2022open, ghiasi2021open}.

\subsection{Reassignment verification}\label{sim-thres}
For evaluation, we need to verify that the reassigned label $T^*_i$ is sufficiently close to the original label  $T_i$ or its segment $S_i$. We provide two kinds of verification. The first is based on simple thresholding on the cosine similarity using $\tau_\text{SBERT}$ and $\tau_\text{CLIP}$ for TT and ST reassignments, respectively (Appendix \ref{apx:thresholds}). The second involves human judgement, in which we ask human evaluators to rate how well the reassigned label describes its segment on a scale of 0-3, ranging from 0: incorrect, 1: partially correct, 2: correct but too general/specific, 3: correct. The full definitions are in Appendix \ref{apx:userstudy}. Multiple thresholds will be used to report scores.

%the reassignment only takes place when the similarity between $T^*_i$ and $S_i$ or between $T^*_i$ and $T_i$ is high enough. We threshold this similarity using human verification. Human subjects are instructed to rate the quality of $T^*_i$ as a label of $S_i$ on a scale of 0-3 (0: incorrect, 1: partially correct, 2: correct but too general/specific, 3: correct). See the details in Appendix \ref{apx:userstudy}.
%We also use simple constants $\tau_\text{CLIP}$ and $\tau_\text{SBERT}$ on the segment-to-text or text-to-text cosine similarity as an alternative thresholds (See Appendix \ref{apx:thresholds}).

% The relabelling only takes place when $\mathrm{cossim}^\text{CLIP}(S_i, T^*_i) \geq \tau_\text{CLIP}$, otherwise $T^*_i=T_i$.
% \todo{Alternatively, we can use human judgment to do thresholding. $S_i$ is relabeled only if humans deem $T^*_i$ a correct label for $S_i$.}
%However, the relabelling only happens if the closest ground-truth label is sufficiently similar to $S_i$.
%is based on matching a predicted segment $S_i$ to its closest ground-truth text. 


% , but it is only applicable for comparing between CLIP-based methods. 



%\subsection{Segmentation IoU}
\subsection{Metrics} \label{sec:metrics}
\vspace{-0.5em}
\myparagraph{Segmentation IoU} evaluates the quality of the output segments in terms of Intersection-over-Union (IoU) against the ground-truth segments in each test image. 
Given a set of predicted segments with reassigned labels $T^*$, segments with the same label $T^*$ are merged to form a single segment for the label. Then, IoU for each image can be computed  using a standard protocol \cite{everingham2010pascal}.
%IoU for each image is then computed. 
%In our experiments, 
%\todo{we use IoU$_\text{CLIP}$ and IoU$_\text{human}$ to denote the mean IoU using $T^*$ from CLIP-based label reassignments with $\tau_\text{CLIP}$ and with human decision respectively.}


%\subsection{Segment Recall}
\myparagraph{Segment Recall}
%Segment Recall 
measures how many objects labeled in the ground truth are discovered. This metric disregards any extra labels predicted by our method that are not part of the ground truth labels. We consider each merged segment of the same reassigned label a True Positive if its IoU against the corresponding ground-truth segment is greater than $\tau_\text{IoU}$.
%, which can occur when our method discovers more objects than the provided ground truth. 
%We merge predicted segments with the same reassigned label and consider the merged segment a True Positive if the IoU between their merged segment and the corresponding ground-truth segment is greater than $\tau_\text{IoU}$. 
%After the reassignment, any predicted segments with the same reassigned label are merged and considered a True Positive if the IoU between their merged segment and the corresponding ground-truth segment is greater than $\tau_\text{IoU}$. 
%. Then, the merged segment is considered as a True Positive if the IoU between it and the corresponding ground truth segment is greater than  $\tau_\text{IoU}$. 
Segment Recall is the rate of True Positive over the number of grounding segments. 
%As with the IoU metrics, we use $\text{Recall}_\text{CLIP}$ to $T^*$ reassignment method.


 
%\subsection{Labelling Quality}
%\myparagraph{Labeling Quality} measures the quality of predicted labels using human evaluation. Three human evaluators are shown a segment $S_i$ (as a highlighted region within a full input image) and our predicted label $T_i$, and are asked to rate how well the label describes the segment, similar to the process in Section \ref{sim-thres} except $T_i$ is shown instead of $T_i^*$.

%given a segment using with human judgement. Human are asked to rate predicted label $T_i$ as a label of its corresponding $S_i$ with the same scoring described in Section \ref{label-re}.

%\subsection{Text Generation Quality} 
\myparagraph{Text Generation Quality} 
%This metric measures the similarity between the predicted label and the ground-truth label when given an oracle segmentation. In particular, we feed each ground-truth segment into our 
measures the quality of text generation given an oracle segmentation. 
That is, we feed each \emph{ground-truth} segment into our model and compute the cosine similarity between our predicted label and the ground-truth label.
%This score computes the similarity between the predicted label and the ground-truth label given perfect segmentation.
%The ground-truth masks are used instead of the proposed image segments.
If the value is higher than $\tau_\text{SBERT}$, it is considered a True Positive. The score is the True Positive rate over the entire test set. This metric evaluates our attention-masking and text generation components independent of the segment generation process (Section \ref{sec:finding_segments}). 
