% \vspace{-1em}
\section{Introduction}
\label{sec:intro}
Semantic segmentation is a core computer vision problem that seeks to partition an image into semantic regions. 
Traditionally, the semantic classes of interest need to be predefined and are limited in number \cite{9356353}. Earlier methods thus cannot generalize beyond the training classes. 
%seen during training. 
With recent advances in joint vision-language representation learning, e.g., CLIP \cite{radford2021learning}, newer methods \cite{liang2022open, li2022language, xu2021simple} can successfully predict segments corresponding to arbitrary text queries in a novel task called open-vocabulary segmentation. These segmentation methods are guided by a text query, which describes what already exists in the image and must be provided by the user.
%One practical issue with this setup is that the user still needs to provide a text query describing what already exists in the image.
%These methods allow segmentation of unseen classes or even a specific class instance that fits the natural language query.
%This query needs to be provided by the user, 
Another meaningful milestone, however, is how we can segment an image \emph{without} user input or guidance like text queries or predefined classes, and label such segments automatically using natural language. Our work provides the first baseline for this novel problem, referred to as \emph{zero-guidance segmentation\footnote{We avoid calling this \emph{zero-shot} segmentation because 
it has been used in the literature for setups that \emph{require} a text query but allow it to be unseen during training, whereas our problem does not require any text query.}.}   

\begin{figure}
%   \includegraphics[scale=0.59]{./figs/teasor_mini.pdf}
  \includegraphics[scale=0.59]{./figs/new_teaser.pdf}
%   \caption{\textbf{Zero-guidance Segmentation} Our method produces these segmentation maps and texts using only pretrained networks with no fine-tuning nor annotations}
  \caption{\textbf{Zero-guidance Segmentation} segments input images and generated text labels for all segments without any guidance or prompting. Our method produces these results using only pretrained networks with no fine-tuning or annotations.}
  \label{fig:teasor}

  \vspace{-1.em}
\end{figure}

Our work is inspired by a recent research direction that solves segmentation by leveraging CLIP \cite{xu2021simple, zhou2021denseclip};
%or self-supervised learning\cite{caron2021emerging, amir2021deep}
however, our key distinction is that we require no segmentation datasets, no text query guidance, and no additional training or fine-tuning. This problem is challenging partly because CLIP has been trained with image captions that globally describe the scenes and provide no spatially specific information for learning segmentation.
%, typically required to train a segmentation network. 
Surprisingly, we show that it is possible to distill the learned knowledge from two generalist models: a self-supervised visual model, DINO \cite{caron2021emerging}, and a visual-language model, CLIP \cite{radford2021learning},
%both trained on tasks unrelated to segmentation---to solve segmentation without further training.
to solve zero-guidance segmentation without further training.



% In more details, our method firstly extracts semantic regions by clustering on deep features of DINO-ViT, and then expresses each region in natural semantic class with the help of visual-language model (CLIP) and language models. To this end, we could obtain partitioned regions along with their corresponding natural entity class without the need of explicitly training and annotation. That is, our framework could be directly scaled with the generalist pretrained modules in which their dataset is much more scalable. Moreover, we achieve comparable results with baselines. 
The overall idea is to first over-segment an image into small segment candidates, then translate each segment into words, and finally join semantically similar segments to form the output segments. 
In particular, we identify segment candidates by clustering deep pixel-wise features from a DINO that takes as input our image. Despite using no training labels, DINO has been shown to produce class-discriminative features, allowing unsupervised segmentation of the primary object in an image \cite{caron2021emerging} or part co-segmentation across images \cite{amir2021deep}. However, our main challenge lies in the next step, which maps each segment into a meaningful representation that can be later translated to words. We leverage CLIP's joint space to model this intermediate representation.
%Each initial segment produced by our clustering, however, may not always be semantically meaningful and will be later merged with other segments based on their inferred semantic labels. 
%One of our key contributions is in the next step, which converts a given segment to an embedding in the CLIP latent space and later to a semantic label.
%inferred in the the next step. 
%and needs to be merged with other segments. Our key component lies in the next step that assigns each segment a semantic label .
%Our key contribution lies in the next step where these in

%We leverage this useful property to provide initial over-segments, which may not be meaningful by themselves but conform to the semantic boundaries.
%We leverage this  feature space of DINO and 
%We group DINO's features using agglomerative clustering%Our clustering is done 

%In particular, our pipeline first identifies segment candidates by 
%This 
%The segment candidates from our clustering form a complete partition of the image, but they may over-segment objects into parts that are not very meaningful by themselves. Our next step is to 
A naive way to project a segment to CLIP's joint space is to input the segment directly into CLIP's image encoder, but this entirely ignores the surrounding context needed for object recognition and disambiguation \cite{zhao2017pyramid,chen2017rethinking,article}.
% Alternatively, masking can be applied inside the attention layers, as done in NLP tasks \cite{vaswani2017attention} or with a transformer-based segmentation network \cite{cheng2022masked}.
Alternatively, masking can be applied inside the attention layers, as done with a transformer-based segmentation network \cite{cheng2022masked}.
%or inside a transformer trained specifically for segmentation \cite{cheng2022masked}. 
However, when applied to CLIP's encoder, which was not trained for segmentation,
%not specifically trained for segmentation, 
these techniques struggle to produce segment-specific embeddings due to the domination of global context information.
%in the embeddings. 
%We found that the main difficulty in masking CLIP's attention is balancing global and local contexts, which are both important in the output embedding.
Evidence in \cite{zhou2021deepvit, xue2022deeper, gong2021vision} also suggests that a transformer trained with image-level annotations, such as CLIP, may lose local information in its tokens in later layers. 
We discovered similar issues: masking in earlier layers removes global contexts and hurts recognition, whereas masking in later layers fails to focus the embedding on a given segment, resulting in all embeddings describing the same dominant object in the image.
%describing the same dominant object in the scene.


%This means that apply masking 
%, which is not trained specifically for segmentation. The main challenge comes from 

%After the segmentation, we encode segments using CLIP by masking in the attention module. Transformer's self-attention mechanism allows the network to attend each image patch in varying degrees and aggregate information to form the global representation.

% In this work, we introduce a method of masking inside the attention module to selectively process only some part of an input image. 
%However, evidence suggests that transformers trained with image-level annotations lose local information in image tokens in deep layers \cite{}.


Another difficulty in balancing global and local contexts
%in the masked embedding 
is that different objects may require different degrees of context balancing.
%depending on how salient they are. 
For small objects, their CLIP embeddings can be dominated by global contexts, which describe other prominent objects in the scene. This phenomenon matches the characteristics of CLIP's training captions, which often ignore unimportant objects in the image. As a result, less prominent objects may require less of the global contexts to highlight their semantics and local contexts.

%The domination of prominent objects in the CLIP embedding is related to 
%similar to the image captions used to train CLIP.
%the embedding of a small object can be dominated by global contexts, which describe some 


%We encountered a similar problem while masking CLIP's image encoder, which is trained on an image-caption dataset. Masked and unmasked embeddings are still very similar and output the same text labels later in the pipeline. 
To solve this, we introduce a novel attention-masking technique called \emph{global subtraction}, which helps adjust the influence of global contexts in the output embedding. The key idea is to first estimate the saliency or the presence of a given segment in the global contexts by analyzing CLIP's attention values. Then, this saliency value will be used to determine how much global contexts should be attenuated in the segment's embedding. The resulting embedding in CLIP's joint vision-language space allows us to readily translate it to text labels with an existing image-to-text generation algorithm \cite{tewel2022zerocap}. And finally, we merge semantically similar segments with simple thresholding by considering both their visual and text similarities.
%by comparing the scenario where CLIP sees the entire image against the scenario where CLIP only sees the given segment. A large difference in how CLIP processes the two scenarios means that a given segment is less salient and requires more 
%the given segment. With this information, we can adjust the influence of the global contexts accordingly.
%with and without masking. A large difference means that a given segment is not salient and requires 
%effectively reduce the dominant global context from attention-masked embeddings and allow local contexts to emerge.
%These segment embeddings allow us to find text labels using a CLIP-based zero-shot caption method \cite{tewel2022zerocap} that optimizes a language model \cite{radford2019language} to generate word sequences whose embeddings are closet to the segments'.
% that local context tends to disappear in deep layers of transformers trained on image-level annotations. [] find that in deep layers of these transformers all patch tokens are nearly identical, thus losing all local information of their respective regions. 

%Our unique problem setup 
%Evaluating an algorithm under this unique setup
%As this work proposes a unique and new problem setup, suitable evaluation methods are also discussed.
%Our method describes a visual segment using natural language without any direct supervision. As such, the predicted text labels may not be in alignment with human-made classes in a segmentation dataset.

To evaluate our algorithm that can output arbitrary text labels, we also propose new evaluation metrics. Evaluating an algorithm under this setup is not straightforward as predicted labels may not necessarily match predefined labels in the test set but can still be correct.
%because the predicted labels may not use the same predefined words in the test set but can still be correct.
%can be correct while not using the same words defined in a test set. 
This may result from the use of synonyms, such as ``cat'' vs. ``feline,'' or differences in label granularity, such as ``cat'' vs. ``orange cat'' or ``cat's nose'' or ``kitten.'' Generally, there is no single correct level of granularity, and each dataset may arbitrarily adopt any level. To address this, we propose to first map the predicted semantic labels to the existing ones in a given test set. After that, we can use standard measurements, such as segment IoU, to evaluate the results as if the algorithm performs segmentation with the predefined test classes. We also introduce Segment Recall, which measures how often ground-truth objects are discovered, and Text Generation Quality, which tests the quality of our embedding technique given ground-truth oracle segmentation.

Our technique can automatically segment an image into meaningful segments as shown in Figure \ref{fig:teasor} without any supervision or text guidance. There are still performance gaps between our technique and other supervised methods or methods fine-tuned on segmentation datasets---but none can specifically solve our problem that lacks user guidance. Nonetheless, we provide a detailed analysis on obstacles that lie ahead as well as ablation studies for the first approach to this problem. In summary, our contributions are:




%For instance, given an image segment of a car, our method may instead output text of car brands or car models, like ‘camry’ or ‘mercedes’. 
%Furthermore, our approach can discover objects not annotated in datasets, which is more practical but penalized by conventional evaluation metrics.
%We propose a systematic evaluation framework that takes into account both visual and semantic perspectives of our proposed task. 
%In addition to matching predicted segmentation masks to ground truth class with predicted texts or segments' visual embeddings, We also introduce a metric that evaluates text outputs from segment embeddings of ground truth segmentation masks to see how close the text outputs are to the corresponding ground truth texts given the perfect segment maps. 



% Natural Entity Semantic Segmentation (NESS), unlike conventional approaches, could directly partitioned regions along with their corresponding natural entity class. The system offers both visual and semantic information without the need of zero-shot transfer. Moreover, ​​the predicted natural entity class is not necessary to be well aligned with the predefined classes of the test set. 

\begin{itemize}
\vspace{-.2em}
\item We introduce the first baseline to a novel problem, \emph{zero-guidance segmentation}, which aims to segment and label an input image in natural language without predefined classes or text query guidance. Our method does not require a segmentation dataset or fine-tuning.
%whose goal is to segment and label an input image in natural language automatically without predefined classes or text query guidance.
% \item We are the first open-vocab segmentation that is solely based visual and visual-language models without further training or data require, hence scalable 
%\vspace{-.2em}%\vspace{-1.5em}
\item We propose a novel attention-masking technique to convert a segment into an embedding in CLIP's joint space by balancing global and local contexts.
%to balance  to infer a segment of an image into a trained transformer images encoder trained on natural image without any fine-tuning.
% \item We introduce a new evaluation framework for our setup.
%\vspace{-.2em}
\item We present evaluation metrics for the proposed setup.

\end{itemize}

