% \begin{figure*}
% \centering
%   \includegraphics[scale=0.89]{./figs/ablation.pdf}
%   \caption{ablation}
%   \label{fig:ablation}
% %   \vspace{-1.5em}
% \end{figure*}
\begin{figure*}
\centering
%   \includegraphics[scale=0.92]{./figs/results.pdf}
%   \includegraphics[scale=0.92]{./figs/results_long.pdf}
 \includegraphics[scale=0.92]{./figs/qualitative_results_new.pdf}
  % \vspace{-1.7em}
  \caption{\textbf{Qualitative results.} Our method gives reasonable segments and outputs free-language text labels representing all regions. The labels can describe regions by different kinds of descriptions, such as object names, facial expressions, locations, car models, or even animal breeds.}
  \label{fig:results}
  \vspace{-1.5em}
\end{figure*}

\begin{figure*}
\centering
% ablation_4 1.1
  % \includegraphics[scale=1.04]{./figs/ablation.pdf}
  \includegraphics[scale=1.04]{./figs/ablation_2.pdf}
%   \includegraphics[scale=1.1]{./figs/ablation_4.pdf}
% \vspace{-1.7em}
  \caption{\textbf{Ablation results.} Comparison between the results from different segment encoding methods. 
  The crop-and-mask baseline often outputs text labels that is not relevant to segments/input images.
  Our method without global subtraction suffers from global leak and often mislabels non-salient objects.
  Without semantic merging, text outputs look good, but it tends to over-segment.
  %Our method can mostly provide reasonable segments output texts that describe corresponding segments correctly. Note that we omit some predicted text labels due to limited space.
  }
  % \vspace{-1em}
  \label{fig:ablation}
%   \vspace{-1.5em}
\end{figure*}
% \begin{figure*}
% \centering
%   \includegraphics[scale=0.35]{./figs/glob_sub.png}
%   \caption{glob sub}
%   \label{fig:glob_sub}
% %   \vspace{-1.5em}
% \end{figure*}

% \vspace{-0.3em}
\section{Experiments}\vspace{-.3em}
% MIGHT NEED REWRITE HERE
%This section is structured as follows:
%In this section, we show
% 1) we describe our experiment setup and datasets used to evaluate our method, 
%1) our qualitative and quantitative results, 
%2) a user study,
%3) an ablation study,
%the difference in masking the input image and in masking attention module, and 
%4) a comparison with an open-vocabulary baseline.

\vspace{-.3em}
% \subsection{Dataset}
\myparagraph{Datasets.}
We evaluate our results on two commonly used segmentation datasets: Pascal Context \cite{mottaghi2014role} and Pascal VOC 2012 \cite{everingham2010pascal}. %, and COCO Stuff \cite{caesar2018coco}. 
Pascal Context contains 5,000 validation images with segmentation ground truths of 459 object classes for scene segmentation task. 
We use PC-59, the commonly used subset with 59 most common objects, following \cite{ghiasi2021open,xu2022groupvit,liang2022open}, as well as the full PC-459.
% We use the commonly used subset with 59 most common object classes, following \cite{ghiasi2021open,xu2022groupvit,liang2022open}.
Pascal VOC (PAS-20) is a segmentation dataset with 1,500 image-segment validation pairs of 20 object classes.
% COCO Stuff is a detailed screen segmentation dataset comprising 5,000 validation images with 80 `thing' or object classes and 91 `stuff' classes for background components like `sky' and `grass'.
%For both datasets, we fine-tune our parameters on the train splits and report our results on the validation splits, as the test split is not available publicly. 
For both datasets, we report our results on the validation splits, as the test splits are not publicly available, but the validation splits were never used for hypertuning.
For comparison with our own variations and a crop-and-mask baseline, we test on the first 1,000 images of Pascal Context dataset and full 1,500 image for Pascal VOC dataset (Section \ref{sec:ablation}). We use the full datasets when compared to prior work (Section \ref{sec:groupvit}). 

% \textbf{Text Generation} 
% We use the official code released by ZeroCap\cite{tewel2022zerocap} for text generation from CLIP's embedding using GPT-2. 
% We shorten the output sequence by increase the sampling rate of the end-of-sentence token.  
% When comparing a text to an image with CLIP similarity score, we also use the same prompt before embedding the text, but we do not use any initial prompt when comparing two texts with S-BERT.

% \textbf{Thresholds used to compute the metrics} 
% We use $\tau_\text{SBERT} = 0.5$, $\tau_\text{CLIP} = 0.1$, and $\tau_\text{IoU} = 0.5$. TODO We provide analysis and justifications for these thresholds in Appendix XXX.

% \vspace{-.3em}
\subsection{Zero-guidance segmentation results} 
% \vspace{-.3em}
We present our qualitative results in Figure \ref{fig:teasor}, \ref{fig:results}.
%on zero-guidance segmentation.
Our method can discover semantic segments and densely label them with diverse types of labels, including names of objects, animal breeds, facial expressions, and places. More results are in Appendix \ref{apx:results}. 
In Table \ref{tab:quan results}, we report IoU and Recall scores using different reassignment and verification techniques (Section \ref{sim-thres}), computed on 1,000 randomly sampled images from PC-59. We observe that ST tends to perform reassignment better than TT, as evident by its higher scores. 
Reassigning words like `leg' to the correct animal class in the ground-truth set can be challenging when relying solely on text (TT), as it lacks any additional context. But ST can access other visual information within the segment, which better facilitates reassignment. 
%We further discuss evaluation challenges in Section \ref{sec:analysis}.
%Our method produce 
%When utilizing text-only (TT) reassignment, it can be challenging to reassign words like 'leg' to a ground-truth animal class, as it has no other context. On the other hand, ST has access to other visual information in the segment, which better facilitates reassignment.
%Reassignment using text alone (TT) can be difficult to map words such as `leg' to a ground-truth animal class.
%as it is much more challenging for TT to map `leg' to 


%ST has significantly higher scores than TT across all metrics. 
%Mapping to the correct ground-truth label may be easier for ST as a segment naturally hold more information than a label does. From a predicted label `leg', it is impossible to infer which animal in ground-truth labels it is, but `leg' segment contains more information, like color, shape, or even information of adjacent regions, making it more recognizable. 
%As For varying human verification, using strict threshold of only `correct' makes the IoU drops to 14.2, but more flexible thresholds `correct but too general/specific' and `partial correct' can boost the IoU up to 20.9 and 22.7 respectively. 

% For label reassignment, we find that segment-to-text reassignment works better than text-to-text. The predicted label can only hold some aspects of the segment, while a segment naturally contains more information. For example, a predicted label `leg' has no indication of what animal its belong to, but the segment contains more information, like color, shape, or even information of adjacent regions, making it more recognizable. 
% We believe this is one reason of why segment-to-text reassignment performs better in Table \ref{tab:quan results}.
% Our method achieves IoU scores of 11.2 and 19.3 for constant thresholding with text-to-text reassignment (t-t) and 19.3 reassignment respectively. With human verification threshold, IoU$_\text{segment-text}$ increase by 3.4 points.
%``const.'' represents using a constant threshold 
%We report IoU and Recall scores in Table \ref{tab:quan results} computed on 1,000 randomly sampled images of PC-59 using different reassignment techniques. For each reassignment technique, we also compute the scores for 
% The segment-to-text reassignment has significantly higher scores across all metrics (more discussion in Section \ref{sec:analysis}). 
% We also compare the results with constant thresholding and varying human verification thresholds. Overall, the scores are better when we reassign only text labels that humans find at least `partially correct'. 


% \begin{table}[]
% % \centering
% \caption{Quantitative results}
% \label{tab:quan results}
% \resizebox{\columnwidth}{!}{
% \setlength{\tabcolsep}{3pt}
% \begin{tabular}{cc@{\extracolsep{6pt}}cc@{\extracolsep{6pt}}cc}
% \toprule
%  & & \multicolumn{2}{c}{\textbf{IoU}}  & \multicolumn{2}{c}{\textbf{Recall}}                                 \\ \cline{3-4} \cline{5-6} 
%  % & & \multicolumn{2}{c}{Threshoding}  & \multicolumn{2}{c}{Threshoding} \\
%  % \cline{3-4} \cline{5-6}  
% \multicolumn{2}{c}{Threshoding} & \rule{0pt}{2ex}  human rating &  constant & human rating &  constant  \\ \midrule
% \multirow{2}{*}{Reassignment}    & text-text     & x     & x      & x    & x    \\
%      & segment-text     & x     & x      & x    & x    \\ \bottomrule
% \end{tabular}}
% % \vspace{-0.5em}
% \end{table}

\begin{table}[]
% \centering
\caption{Quantitative results on 1,000 random images from PAS-59's validation split. We use constants ($\tau_\text{SBERT}$ and $\tau_\text{CLIP}$) and multiple human verification scores (h) for thresholding.}
\vspace{-0.5em}
\label{tab:quan results}
\resizebox{\columnwidth}{!}{
\setlength{\tabcolsep}{0pt}
\begin{tabular}{l@{\extracolsep{6pt}}ccc@{\extracolsep{6pt}}cccc}
% {\columnwidth}{c *{8}{Y}}%{l@{\extracolsep{6pt}}cc@{\extracolsep{6pt}}ccccc}
\toprule
 & \multicolumn{3}{c}{\textbf{Text-text reassign.}}  & \multicolumn{4}{c}{\textbf{Segment-text reassign.}}                                \\ \cline{2-4} \cline{5-8}
 % & & \multicolumn{2}{c}{Threshoding}  & \multicolumn{2}{c}{Threshoding} \\
 % \cline{3-4} \cline{5-6}  z
 \\[-1em]
\textbf{Threshold:} && const. & h $ \geq 1$ & const. &  h $ = 3$ &  h $ \geq 2$ & h $ \geq 1$   \\  \midrule
IoU    && 11.2     & 11.0     & 19.3      & 14.2    & 20.9      & 22.7        \\
Recall    && 10.3     & 9.8     & 18.0      & 13.2    & 18.0      & 19.4        \\
 \bottomrule
\end{tabular}}
% \vspace{-1em}
\end{table}

\begin{table}[]
\centering
\caption{Distribution of human rating scores on the quality of the predicted labels (0: incorrect, 1: partially correct, 2: correct but too general/specific, 3: correct).}
\vspace{-0.5em}
\label{tab:labelq}
\resizebox{\columnwidth}{!}{
\setlength{\tabcolsep}{12pt}
\begin{tabular}{ccccc}
\toprule
\textbf{Human rating} & 0 & 1 & 2 & 3  \\ \midrule
\% of labels    & 36.0 & 20.8 & 23.9  & 19.3 \\ \bottomrule
\end{tabular}}
\vspace{-1em}
\end{table}

% \begin{table}[]
% % \centering
% \caption{Quantitative results on 1,000 random images from PAS-59's validation split. We use constants and varying human verification scores (h) for thresholding.}
% \label{tab:quan results}
% \resizebox{\columnwidth}{!}{
% \setlength{\tabcolsep}{0pt}
% \begin{tabular}{l@{\extracolsep{6pt}}ccc@{\extracolsep{6pt}}ccccc}
% % {\columnwidth}{c *{8}{Y}}%{l@{\extracolsep{6pt}}cc@{\extracolsep{6pt}}ccccc}
% \toprule
%  & \multicolumn{3}{c}{\textbf{Text-text reassign.}}  & \multicolumn{5}{c}{\textbf{Segment-text reassign.}}                                \\ \cline{2-4} \cline{5-9}
%  % & & \multicolumn{2}{c}{Threshoding}  & \multicolumn{2}{c}{Threshoding} \\
%  % \cline{3-4} \cline{5-6}  z
%  \\[-1em]
% \textbf{Threshold:} && const. & h $ \geq 1$ & const. &  h $ = 3$ &  h $ \geq 2$ & h $ \geq 1$ & h $ \geq 0$  \\  \midrule
% IoU    && 11.2     & 11.0     & 19.3      & 14.2    & 20.9      & 22.7    & 19.3    \\
% Recall    && 10.3     & 9.8     & 18.0      & 13.2    & 18.0      & 19.4    & 14.2    \\
%  \bottomrule
% \end{tabular}}
% % \vspace{-1em}
% \end{table}

\subsection{User study}
We evaluate the quality of predicted labels using human evaluation. %Label Quality and report the score in Table \ref{tab:labelq}.
%measures the quality of predicted labels using human evaluation. 
Each segment and its predicted label were shown to three distinct human evaluators, who were asked to rate how well the label describes the segment on a scale of 0-3, similar to the process in Section \ref{sim-thres} except we show the predicted label $T_i$ instead of the reassigned label $T_i^*$. Full details and the score definitions are in Appendix \ref{apx:userstudy}.

%For each Three human evaluators are shown a segment $S_i$ (as a highlighted region within a full input image) and our predicted label $T_i$, and are asked to rate how well the label describes the segment, similar to the process in Section \ref{sim-thres} except $T_i$ is shown instead of $T_i^*$. 
%To evaluate Label Quality, given a pair of a segment and the corresponding predicted label, we ask three human subjects to score the label on 0-3 scale described in Appendix \ref{apx:userstudy}.

Table \ref{tab:labelq} shows that human evaluators found about 43\% of our results to be `correct' or `correct but too generic/specific' and 64\% to be at least `partially correct.' 

We provide example images and their scores given by the human evaluators in Figure \ref{fig:user_study_adj_failure}. According to the result, most of our score-0 labels are single-word adjectives, such as `black', or collective nouns, such as `group'. Another kind of score-0 labels is caused by biases toward stereotypical appearances of objects, such as when a pet dog was mislabeled as `stray' due to its shabby appearance (row 4). Some of score-1 labels correspond to descriptions or abstract nouns that are related to their segments but may not fully describe them, such as `reflection', `dining', and `sunny', and some other labels describe specific but incorrect types of objects, such as `uber' or `military'. Most of our score-2 labels are nearly accurate, but the segments may incompletely or excessively cover the referred objects, such as `lush moss' and `few puppies' (row 2). Most of our score-3 labels accurately represent their segments, such as `plane', and they can be descriptive even on background objects, such as `sandy beach' and `crowd observing', unlike labels from traditional segmentation methods.

\begin{figure}
\centering
 \includegraphics[scale=0.77]{./figs/userstudy_all_example_small.pdf}
  \vspace{-1.8em}
  % \caption{Examples of correct (top) and incorrect (below) labels \todo{ TODO: add correct labels}} 
  \caption{Examples of segmentation results that were evaluated by human evaluators with scores ranging from 3 (correct) to 0 (incorrect). The definitions of the scores can be found in Appendix \ref{apx:userstudy}.} 
  \vspace{-.5em}
  \label{fig:user_study_adj_failure}
\end{figure}

% Most correct labels predicted by our method refer to common objects, such as `table' and `horse,' while incorrect labels are usually adjectives, such as `small', and `black.' 
% An additional analysis can be found in Appendix \ref{apx:userstudy_analysis}.


%We provide further analysis in Appendix \ref{apx:userstudy_analysis}.
%According to the study's result in Table \ref{tab:labelq}, human subjects find over 40 \% of our results `correct' (3) or `correct but too generic/specific' (2). 
%The results further suggest that most successful labels refer to common objects, i.e., `table' and `horse', while failure labels are usually mere adjectives, i.e., `small', and `black'. We provide further analysis in Appendix \ref{apx:userstudy_analysis}.
% which are often not sufficiently informative to represent a segment

% The results further suggest that failure labels are usually a mere adjective, i.e., `small', and `black' or a collective noun, i.e., `group', and `herd', which is often not sufficiently informative to represent a segment. % Additionally, 73.5 \% of the given pairs were agreed upon by at least two subjects and 13.5 \% by all three subjects.

% To evaluate the label quality, we conduct a user study. In particular, for each pair of a segment and a predicted label, we ask three human subjects to score the label on 0-3 scale described in \ref{apx:userstudy}.
% According to Table \ref{tab:labelq}, human subjects find over 40 \% of our results `correct'(3) or `correct but too generic/specific'(2). Additionally, 73.5 \% were agreed upon by at least two subjects and 13.5 \% by all three subjects.



% To further evaluate our predicted labels, we conducted a user study, which is described in \ref{apx:userstudy}.

% Table \ref{tab:labelq} presents the distribution of Labeling Quality.
% For each pairs of a segment and a predicted label, we ask three human subjects to score the label on 0-3 scale.
% Human subjects find over 40 \% of our results `correct' (3) or `correct but too generic/specific' (2). 
% Only 13.5 \% of these pair receive the same score from all three subjects, and around three-fourths have at least two agreeing subjects.



% , only a third of our results are deemed `incorrect'  The average Labeling Quality of the whole test set is 1.265. 



\subsection{Ablation study} \label{sec:ablation} 
% \vspace{-.5em}
%\subsubsection{Zero-guidance segmentation evaluation}
% We discuss the effectiveness of our masking method in encoding visual information of an image segment. 

We compare our method with alternative attention-masking methods, which include 1) cropping and masking the input image to fit the segment region \cite{xu2021simple}, 2) our method without global subtraction, and 3) our method without the merging step.
Note that we also apply the same merging in the crop-and-mask baseline for a fair comparison. We show the results in Figure \ref{fig:ablation} and in Table \ref{tab:ablation}.

In Figure \ref{fig:ablation}, the crop-and-mask baseline often returns text labels that are unrelated to the segments, like `video' in column 2-4.  
Without global subtraction, our method often fails to recognize objects in the background due to the leak of global contexts. For example, the sky in column 1 is labeled `tower', and almost everything in column 2 is labeled `room'.
Our full pipeline yields reasonable results, and can label `ladder', `lamp', `floor', and `reader' correctly.% in column 2.
%in order to compare our attention masking and the RGB masking fairly. 
% before passing it to the CLIP image encoder.
%The results in Figure \ref{fig:ablation} show that the crop-and-mask baseline often returns text labels that are completely unrelated to the segments.  %, most likely due to CLIP's encoder having never seen masked images during training.
% crop and mask baseline is not good at encoding background feature because cropping and masking background image often results in images with black regions, which is not within the distribution of CLIP's natural image training data, thus leading to unrelated output texts.
%Without global subtraction, our method often fails to recognize objects in the background due to the leak of global contexts. %, and often returns texts describing the salient objects for these regions. 
% same texts for background and foreground region due to \emph{global leak} discussed in Section \ref{sec:region-emb}.

In Table \ref{tab:ablation}, we report results based on ST reassignment and constant thresholding. Our method outperforms all alternative masking techniques in terms of IoU, Recall, and Text Generation Quality scores on PC-59. Global subtraction also helps improve both $\text{IoU}_\text{CLIP}$ by 3.0-3.1 points. On PAS-20, our method achieves a slightly lower IoU than not using global subtraction (1.1 lower). Upon inspection, we observe that the 20-class PAS-20 tends to label only a few foreground objects while ignoring much of the background (see Appendix \ref{apx:voc}), and not using global subtraction may preserve the embedding of these few objects better. This bias toward primary objects, however, would not be beneficial if the goal is to discover \emph{all} semantic objects.
% \todo{Note that all metrics in Table \ref{tab:ablation} use segment-to-text reassignment and constant thresholding.}

\begin{table}[]
% \centering
\caption{Ablation study of our CLIP's mask attention technique. IoU and Recall are computed with ST and constant thresholding.
}
\vspace{-0.5em}
\label{tab:ablation}
\resizebox{\columnwidth}{!}{
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lccc@{\extracolsep{6pt}}c@{\extracolsep{6pt}}c}
\toprule
 & \multicolumn{3}{c}{\textbf{PC-59}}  & \textbf{PC-459}& \textbf{PAS-20}                         \\ \cline{2-4} \cline{5-5} \cline{6-6}\\[-1em]
\multicolumn{1}{c}{Method} & \rule{0pt}{2ex}$\text{IoU$_\text{c}$}$ & $\text{Recall$_\text{c}$}$ &  TGQ  & $\text{IoU$_\text{c}$}$ &  $\text{IoU$_\text{c}$}$  \\ \midrule
% Crop and Mask w/o merge     & 11.7      & 5.6   & 8.9 & 4.2   & 12.1      & 16.8 \\
Crop and Mask               & 12.1       & 10.2 & 16.9 & 5.4 & 14.0        \\
Ours w/o glob sub.        & 14.5         & 15.0 & 11.8 & 7.2 & 21.2          \\
Ours w/o merge              & 16.4       & 11.8 & - & 10.2  & 18.3         \\
Ours                        & 17.5       & 15.0 & 19.0 & 11.3  & 20.1       \\ \bottomrule
\end{tabular}}
% \vspace{-1em}
\end{table}

% \begin{table}[]
% % \centering
% \caption{Ablation study of our CLIP's mask attention technique.
% }
% \label{tab:ablation}
% \resizebox{\columnwidth}{!}{
% \setlength{\tabcolsep}{1pt}
% \begin{tabular}{lccccc@{\extracolsep{6pt}}cc@{\extracolsep{6pt}}cc}
% \hline
%                             & \multicolumn{5}{c}{\textbf{PC-59}}                            & \multicolumn{2}{c}{\textbf{PC-459}}                                & \multicolumn{2}{c}{\textbf{PAS-20}}                                  \\ \cline{2-6} \cline{7-8} \cline{9-10}
% \multicolumn{1}{c}{Method} & \rule{0pt}{2ex}$\text{IoU}_\text{st}$ & $\text{IoU}_\text{tt}$ & $\text{Recall}_\text{st}$ & $\text{Recall}_\text{tt}$  & OVCS  & $\text{IoU}_\text{st}$ & $\text{IoU}_\text{tt}$& $\text{IoU}_\text{st}$ & $\text{IoU}_\text{tt}$  \\ \hline
% % Crop and Mask w/o merge     & 11.7      & 5.6   & 8.9 & 4.2   & 12.1      & 16.8 \\
% Crop and Mask               & 12.1      & 5.6   & 10.2 & 4.9   & 16.9 & 5.4 & 3.6 & 14.0      & 17.9   \\
% Ours w/o glob sub.        & 14.5      & 4.9   & 15.0 & 5.4   & 11.8 & 7.2 & 4.0 & 21.2      & 18.8    \\
% Ours w/o merge              & 16.4      & 7.7   & 11.8 & 5.4   & - & 10.2 & 8.2 & 18.3      & 24.9     \\
% Ours                        & 17.5      & 8.0   & 15.0 & 7.2   & 19.0 & 11.3 & 9.0 & 20.1      & 27.3     \\ \hline
% \end{tabular}}
% % \vspace{-0.5em}
% \end{table}

% \begin{table}[]
% % \centering
% \caption{Ablation study of our CLIP's mask attention technique.
% }
% \label{tab:ablation}
% \resizebox{\columnwidth}{!}{
% \setlength{\tabcolsep}{1pt}
% \begin{tabular}{lccccc@{\extracolsep{6pt}}cc}
% \hline
%                             & \multicolumn{5}{c}{\textbf{PAS Context 59}}                               & \multicolumn{2}{c}{\textbf{PAS VOC}}                                  \\ \cline{2-6} \cline{7-8} 
% \multicolumn{1}{c}{Method} & \rule{0pt}{2ex}$\text{IoU}_\text{st}$ & $\text{IoU}_\text{tt}$ & $\text{Recall}_\text{st}$ & $\text{Recall}_\text{tt}$  & OVCS  & $\text{IoU}_\text{st}$ & $\text{IoU}_\text{tt}$  \\ \hline
% % Crop and Mask w/o merge     & 11.7      & 5.6   & 8.9 & 4.2   & 12.1      & 16.8 \\
% Crop and Mask               & 12.1      & 5.6   & 10.2 & 4.9   & 16.9 & 14.0      & 17.9   \\
% Ours w/o glob sub.        & 14.5      & 4.9   & 15.0 & 5.4   & 11.8 & 21.2      & 18.8    \\
% Ours w/o merge              & 16.4      & 7.7   & 11.8 & 5.4   & - & 18.3      & 24.9     \\
% Ours                        & 17.5      & 8.0   & 15.0 & 7.2   & 19.0 & 20.1      & 27.3     \\ \hline
% \end{tabular}}
% % \vspace{-0.5em}
% \end{table}

% \subsubsection{Open-Vocab Class Similarity}
% OVCS (Open-Vocab Class Similarity Score) shows how well generated  text labels can represent `perfect' segments from human-labeled annotation. As all methods presented here use the same text generation method (Section \ref{sec:textgeneration}), this experiment demonstrates the performance of our segment embedding methods compared to simply encoding cropped masked images.
% We omit segment merging step from this experiment.

% Table \ref{tab:text_score} shows OVCS of our method and the baseline on Pascal Context dataset. Segments embeddings from our attention masking and global subtraction technique achieve better performance compared to using cropped masked images, but our method with no global subtraction has a low score due to global leaking.

% \begin{table}[]
% \centering
% \caption{Open-Vocab Class Similarity on Pascal Context Dataset}
% \label{tab:text_score}
% \resizebox{0.8\columnwidth}{!}{
% \begin{tabular}{lc}
% \toprule
% \multicolumn{1}{c}{Method} & Open-Vocab Class Similarity \\ \midrule
% Crop and Mask                 & 16.9 \\
% Ours w/o global subtraction  & 11.8   \\
% Ours                         & 19.0     \\ \bottomrule
% \end{tabular}}
% \end{table}

