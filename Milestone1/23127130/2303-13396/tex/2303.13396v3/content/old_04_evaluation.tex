
\section{New Evaluation Metrics}  \label{sec:eval}
This section introduces new metrics to evaluate the accuracy of the output segments and their corresponding text labels.
%for our new problem setup. 
%In this section, we introduce new evaluation metrics that can quantify the performance of our newly proposed problem setup. 
%The setup takes as input an image and output segmentation masks and text labels for all the masks. 
To overcome the evaluation challenges due to the use of synonyms or the variation in label granularity, such as ``car'' vs ``wheel,'' we first map the predicted semantic labels to the predefined ones in the test set.

%The main challenge of evaluating an algorithm under our new setup is that the predicted labels may not use the same predefined words in the test set but can still be correct.
%This can be due to the use of synonyms, such as ``cat'' vs. ``feline,'' or the difference in fine-grained categorization, such as ``cat'' vs. ``orange cat'' or ``cat's nose'' or ``kitten.'' Generally, there is no single correct level of fine-grained categorization, and each dataset may arbitrarily adopt any level. To address this, we propose to first map the predicted semantic labels to the existing ones in a given test set. After that, we can use standard protocols to evaluate the results as if the algorithm performs segmentation with the predefined test classes.

\subsection{Label reassignment}
Given a predicted segment $S_i$ and its predicted text $T_i$, our goal is to relabel $S_i$ with $T^*_i$, which should be one of the test labels. We describe two reassignment techniques based on text-to-text or segment-to-text similarity.

\subsubsection{Text-to-text similarity}  \label{text2text}
This technique relabels $S_i$ with the ground-truth label $T_j^\text{gt}$ that is semantically closest to $T_i$. However, the relabelling only happens if $T_j^\text{gt}$ is sufficiently similar to $T_i$.
%, if and only if $T_i$ is sufficiently close to $T_j^\text{gt}$.
%Given a predicted segment $S_i$ with its predicted text $T_i$, we want to find the semantically closest ground-truth text $T_j^\text{gt}$ in a given test set and map $T_i$ to $T_j^\text{gt}$, i.e., declare that they are the same, if and only if $T_i$ is sufficiently close to $T_j^\text{gt}$.
%The evaluation needs to consider two aspects: the quality of predicted segmentation mask and how well the text labels represent the corresponding segments.
%Our method produces text labels in natural language from a CLIP embedding, which is trained on a large corpus of internet-crawled image-caption pairs. Unlike conventional datasets for segmentation where each segment class is predefined by humans in a definitive way, ours segments are expressed more freely due to the influence from external knowledge from CLIP dataset.
%For a segment class defined by humans as `cat', our method may output its synonyms, like `kitten', or more descriptive labels, like `orange cat', `sleeping cat' or `small kitten'. A good evaluation metric should be able to match these different labels to the ground truth class without penalizing our method for being more expressive but still knows not to falsely reward unrelated labels.
To measure the semantic similarity, we use cosine similarity in the embedding space of Sentence-BERT \cite{reimers-2019-sentence-bert}, a pre-trained text encoder widely used in NLP for computing text similarity \cite{gao2021simcse,li2020sentence,choi2021evaluation}, and a threshold $\tau_\text{SBERT}$ to test whether two words are sufficiently similar. Formally, the new label of $S_i$ is given by:
%Specifically, we use the cosine similarity of SBERT embeddings and if  a threshold $\tau_\text{SBert}$ to test if they are sufficiently close. 
%Formally, we map $T_i$ to the new $T^*_i$,,,:
\begin{align}
&T^*_i =
    \begin{cases}
       T_j^\text{gt} & \text{if } \mathrm{cossim}^\text{SBERT}(T_i, T_j^\text{gt}) \geq \tau_\text{SBERT} \\
       T_i & \text{otherwise}
    \end{cases},  \\
&\text{where } j = \argmax_{k} \left[ \mathrm{cossim}^\text{SBERT}(T_i, T^\text{gt}_k) \right].
\end{align}
Note that using CLIP's text encoder for this purpose is not ideal for two reasons. First, since our method directly uses CLIP, matching text based on the same CLIP model may be unfair to others that train their embedding spaces from scratch or use other pre-trained text models. Second, our experiment in Appendix \ref{apx:text_sim} shows that there are occasions where the similarity in CLIP's embedding space highly contradicts human judgement. For example, in CLIP's space, the word ``dog'' is closer to ``car'' than ``labrador.''
%One way to determine whether two texts are semantically similar is to use the cosine similarity of their embedding.
%However, we avoid using CLIP embeddings in metrics calculation for two main reasons. 
%1) while our method used CLIP as a big part of our pipeline, there are others that build their own joint space from scratch. Using CLIP as an evaluator may be unfair to those methods.
%2) In Figure \ref{fig:text_sim}, we test the cosine similarity  between embeddings of a set of text. While all texts describing a car are quite similar, the similarity score between `dog' and `car' is even higher, and 'dog' are more similar to 'car' than 'labrador', a dog breed.
%For these two reasons, we instead use Sentence-BERT \cite{reimers-2019-sentence-bert}, a trained text encoders widely used in NLP for calculating the similarity between two texts \cite{gao2021simcse,li2020sentence,choi2021evaluation}.
%Since our method can discover unlabeled objects, we propose to use similarity thresholding to filter such unlabeled objects.
%Sentence-Bert achieves decent quantitative score for text embedding similarity and also provide strong evidence on user study \cite{gao2021simcse}. Based on their user study, since our text is simply a single noun or noun phrase, we use similarity threshold $T_{sbert}$ of 0.5 as default. 
%Here, we propose multiple metrics that consider different aspects of our problem setup. Proposed metrics can be divided into two categories: metrics that evaluate the whole pipeline and metrics that evaluate only a part of our pipeline.
% Existing open-vocabulary methods have two main steps in their evaluation process: 1) aligning their prediction results with test set's class labels 2) Following a standard protocol for semantic segmentation, mean intersection over union (mIoU) is then computed.  

%\subsection{Whole Pipeline Evaluation}
%The metrics in this section reflect the performance of the whole pipeline, which means the metrics take into account both segmentation masks and output labels. 

\subsubsection{Segment-to-text similarity} \label{seg2text}
The second technique relabels $S_i$ with the ground-truth label $T_j^\text{gt}$ that is semantically closest to $S_i$ in CLIP's joint image-text space \cite{hessel2021clipscore, kawar2022imagic}. 
%is based on matching a predicted segment $S_i$ to its closest ground-truth text. 
This relabelling ignores the predicted label $T_i$ in finding the closest $T_j^\text{gt}$ entirely, but it is helpful for partially evaluating our pipeline \emph{independent} of the text generation component borrowed from prior work (Section \ref{sec:textgeneration}). We note that this relabelling or mapping technique is commonly used in open-vocabulary settings \cite{liang2022open, ghiasi2021open, xu2022groupvit}, but it is only applicable for comparing between CLIP-based methods. 
The new label $T^*_i$ is computed by:

\begin{align}
&T^*_i =
    \begin{cases}
       T_j^\text{gt} & \text{if } \mathrm{cossim}^\text{CLIP}(S_i, T_j^\text{gt}) \geq \tau_\text{CLIP} \\
       T_i & \text{otherwise}
    \end{cases},  \\
&\text{where } j = \argmax_{k} \left[ \mathrm{cossim}^\text{CLIP}(S_i, T^\text{gt}_k)\right],
\end{align}
%To do this, we use CLIP's text and image encoders to map $S_i$ and grount-truth texts into the same embedding space. Then, the remapped $T^*_i$ is computed by:
and $\text{cossim}^\text{CLIP}(s, t)$ uses CLIP's image encoder for $s$ and text encoder for $t$.

\subsubsection{CLIP-based reassignment} \label{seg2text}
This technique relabels $S_i$ with the ground-truth label that is closest to $S_i$ in the CLIP's joint image-text space \cite{hessel2021clipscore, kawar2022imagic}. Formally, the new label $T^*_i$ is computed by
%Formally, let $j$ indicates the closest the new label $T^*_i$ is computed by:
\begin{align}
    T^*_i &= \argmax_{t \in T^\text{gt}} \left[ \mathrm{cossim}^\text{CLIP}(S_i, t)\right],
\end{align}
where $\text{cossim}^\text{CLIP}(s, t)$ uses CLIP's image encoder for $s$ and text encoder for $t$. The relabelling only takes place when $\mathrm{cossim}^\text{CLIP}(S_i, T^*_i) \geq \tau_\text{CLIP}$, otherwise $T^*_i=T_i$.
%However, the relabelling only happens if the closest ground-truth label is sufficiently similar to $S_i$.
%is based on matching a predicted segment $S_i$ to its closest ground-truth text. 
This relabelling does not consider the predicted label $T_i$, but it is helpful for evaluating our pipeline \emph{independent} of the text generation component borrowed from prior work (Section \ref{sec:textgeneration}). Note that this mapping technique is commonly used in open-vocabulary settings \cite{liang2022open, ghiasi2021open, xu2022groupvit}, but it is only applicable for comparing between CLIP-based methods. 

%\begin{align}
%&T^*_i =
%    \begin{cases}
%       T_j^\text{gt} & \text{if } \mathrm{cossim}^\text{CLIP}(S_i, %T_j^\text{gt}) \geq \tau_\text{CLIP} \\
%       T_i & \text{otherwise}
%    \end{cases},  \\
%&\text{where } T_j^\text{gt}  = \argmax_{t \in T^\text{gt}} \left[ %\mathrm{cossim}^\text{CLIP}(S_i, t)\right],
%\end{align}
%To do this, we use CLIP's text and image encoders to map $S_i$ and grount-truth texts into the same embedding space. Then, the remapped $T^*_i$ is computed by:
%and $\text{cossim}^\text{CLIP}(s, t)$ uses CLIP's image encoder for $s$ and text encoder for $t$.


\subsection{Segmentation IoU}
This metric evaluates the quality of the output segments in terms of Intersection-over-Union (IoU) with respect to the ground-truth segments in each test image. 
Given a set of predicted segments with reassigned labels $T^*$, segments with the same label $T^*$ are merged to form a single segment for the label. Then, IoU for each image can be computed  using the standard protocol \cite{everingham2010pascal}.
%IoU for each image is then computed. 
In our experiments, we use IoU$_\text{tt}$ and IoU$_\text{st}$ to denote the mean IoU using $T^*$ from text-to-text and segment-to-text reassignments, respectively.

% This metric evaluates the quality of the output segments in terms of Intersection-over-Union with respect to the ground-truth segments in each test image. We call the ground-truth segments and labels in \emph{each} test image \emph{grounding} and denote their sets by $S^\text{gd}$ and $T^\text{gd}$. Given a set of predicted segments $S$ with reassigned labels $T^*$, segments with the same label $T^*$ is then merged by union operation. Following the standard protocol \cite{everingham2010pascal}, IoU for each image is then computed. In our experiments, we use IoU$_\text{TT}$ and IoU$_\text{ST}$ to denote the mean IoU using $T^*$ from text-to-text and segment-to-text reassignments, respectively.


% Given a set of predicted segments $S$ with reassigned labels $T^*$, the IoU for each test image is computed by:

% \begin{equation} 
%  \text{IoU}^\text{per-img} = \frac{1}{|S^\text{gd}\cup S|}\sum_{k = 1}^{|S^\text{gd}\cup S|} \text{IoU}(\cup\{ S_i \in S \mid T^*_i = T_k^\text{gd}\}, S_k^\text{gd}),
% \end{equation}
% where the union operator $\cup\{V_1, V_2, ...,V_n\} = V_1 \cup ...\cup V_n$ is used to merge all predicted segments with the same reassigned label $T_k^\text{gd}$.
% %Since our segments are designed to be ``over-segments,'' 
% %We merge all segments with the same label before computing the IoU. 
% In our experiments, we use IoU$_\text{TT}$ and IoU$_\text{ST}$ to denote the mean IoU using $T^*$ from text-to-text and segment-to-text reassignments, respectively.

\iffalse
\subsubsection{Text-text Paring (IoU$_\text{TT}$)}
This metric is designed for comparison between this setup and any other segmentation methods. 

We define $S_\text{gt}$ as the set of ground-truth classes in the test set and $S_\text{pred}$ as the set of predicted classes.
For each image in the test set, we want to match each predicted mask $M_i^{gt}$ to the a ground-truth masks $M_j^{pred}$ using output text $T_i^{pred}$ and ground-truth text $T_j^{gt}$, for $i \in S_{pred}$ and $j \in S_{gt}$.
% output texts to the predefined text classes in segmentation datasets and calculate the IoU score between matched predicted masks and ground truth mask.

Text similarity score between two texts $T_1$ and $T_2$ is defined as
\begin{equation} 
\text{text\_sim}(T_1, T_2) = \text{cossim}(E_t( T_1),E_t(T_2))
\end{equation}
where $E_t$ is the text encoder (S-BERT).

First, we assign one of the ground-truth classes to each output segment mask $M_i^{pred}$ by finding the max text\_sim.
% best match using cosine similarity between the output text and ground truth texts.
\begin{equation} 
 \text{pred\_class}_{i} = \underset{j \in S_{gt}}{\argmax}\  \text{text\_sim}(T_i^{pred}, T_j^{gt})
\end{equation}
Note that if the max similarity score of a segment is less than $\tau_{sbert}$, that segment is regarded as having no match (or belongs to a class outside of $S_{gt}$).

Next, we create a predicted mask $M_j^{\text{pred}}$ for each ground-truth class $j \in S_{gt}$ and calculate the IoU.
\begin{equation} 
 M_j^{pred} = \text{Union}(M_{i}\ \forall \text{pred\_class}_i = j)
\end{equation}
\begin{equation} 
 \text{IoU}_{t-t} = \frac{1}{|S_{gt}|}\sum_{j \in S_{gt}} \text{IoU}(M_j^{pred}, M_j^{gt}).
\end{equation}
% where $gt\_m_{j}$ is the ground truth segmentation mask of class $j$.

Then, $\text{mIoU}_{t-t}$  is computed as the mean of $\text{IoU}_{t-t}$ of all images in the test set.
% Then, for all $N_{im}$ images in the test set, $\text{mIoU}_{t-t}$ is computed as
% \begin{equation} 
%  \text{mIoU}_{t-t} = \frac{1}{N_{im}}\sum_{n=1}^{N_{im}} \text{IoU}_{t-t\_n}
% \end{equation}

% Since OST could discover unlabeled objects, we propose to use similarity thresholding to filter such unlabeled objects. We use Sentence-BERT [] as our text encoder as they achieve decent quantitative score for text embedding similarity and also provide strong evidence on user study []. Base on their user study, since our text is simply a single noun or noun phrase, we use similarity threshold of 0.5 as default. 

\subsubsection{Segment-text Paring (IoU$_\text{ST}$)}
This is the metric commonly used in open-vocabulary setting, although it is not explicitly described as such in other works.
To compute this metric, the joint space between image and text is needed to match the embeddings of segments and predefined text classes. After that, IoU scores between predicted masks and their corresponding matched ground-truth masks are calculated. The text generation stage is not considered in this metric.

The concept is similar to $\text{IoU}_{t-t}$, but instead of matching output texts and ground-truth texts with S-BERT, we match the visual segment embeddings to ground-truth text embeddings using image-text joint space. The formulation is as follows:

\begin{equation} 
 \text{pred\_class}_{i} = \underset{j \in S_{gt} V}{\text{argmax}}\ \text{cossim}(E_v(M_i^{pred}),E_t( T_j^{gt}) )
\end{equation}
\begin{equation} 
 M_j^{pred}= \text{Union}((M_i^{pred}\ \forall\ \text{pred\_class}_{i} = j)
\end{equation}
\begin{equation} 
 \text{IoU}_{i-t} = \frac{1}{|S_{gt}|}\sum_{j=1}^{|S_{gt}|} \text{IoU}(M_j^{pred} M_j^{gt})
\end{equation}
where $E_v$ and $E_t$ are visual encoder and text encoder of the same visual-language space, which is CLIP's in our case. 
Note that if the max similarity score of a segment is less than $\tau_{CLIP}$, that segment is regarded as having no match (or belongs to a class outside of $S_{gt}$).
% Due to the success of visual-language models [], current approaches [] often use image to text zero-shot transfer to assign a class to an image segment. To elaborate, they first compute image embedding of image segments and text embedding of all classes in a test set. Each image segment is assigned to the class that maximizes its embedding cosine similarity.
% \begin{equation} 
% pred\_class_{i-t}( m_i) = argmax_{t_j \epsilon V}\ similarity(E_v( m_i),E_t( t_j) ) 
% \end{equation}

% \begin{equation} 
% pred\_class_{i-t}( m_i) =  \underset{t_j \epsilon V}{\text{argmax}}\  similarity(E_v( m_i),E_t( t_j) ) 
% \end{equation}


\begin{figure}
\centering
  \includegraphics[scale=0.5]{./figs/text_sim.png}
  \caption{empirical experiment on text similarity using CLIP's text encoder and Sentence-BERT}
  \label{fig:text_sim}
\end{figure}
\fi

\subsection{Open-Vocab Recall ($\text{Recall}_{ov}$) } 
Open-Vocab Recall measures how many objects labeled in the ground truth are discovered.
This metric disregards any superfluous predictions, which can occur when our method discovers more objects in the background. The assignment of True Positive takes into account both the segment and label.
%The assignment of true positives takes into account both the segment and the label.
%However, it disregards any superfluous predictions, which can occur when our method over-segments and discovers objects in the background.
%This metric disregards any superfluous predictions, which can occur when our method discovers more objects in the background.
%This metric takes into account both the location and the label.
For each ground-truth segment, all predicted segments with the same label are merged. Then, if the IoU of the merged segment and the ground-truth segment is greater than $\tau_\text{IoU}$, we consider it a True Positive. 
%as with segmentation IoU. 
%Then, if the IoU of the segment and the ground-truth is greater than $\tau_\text{IoU}$, we consider it a True Positive. 
Open-Vocab Recall is the rate of True Positive over the number of grounding segments. As with the IoU metrics, we use $\text{Recall}_\text{tt}$ and $\text{Recall}_\text{st}$ to denote the two methods to acquire $T^*$.

%For each segment of the grounding class
% We only consider ground truth object classes that appear in the image, also known \emph{grounding classes}.
% For each test image, we first define $S_{ground}$ as a set of grounding classes in the test set and $S_{pred}$ as a set of predicted class.
% We define `Postitive' ($TP + FN$) as the number of grounding classes $n\_gc$.

%For each segment of grounding class $S_j^\text{gd}$, we check if the segments of the reassigned label 
%For each segment of grounding class $S_j^\text{gd}$, we calculate the IoU between $S_j^\text{gd}$ and the union of $S_i$ whose reassigned label is $T^*_i = T_j^{gd}$. If the IoU is greater than $\tau_\text{IoU}$, we consider it a True Positive. Open-vocab recall is the rate of true positive over the number of grounding segments.

\iffalse
\begin{align}
&S^*_i =
    \begin{cases}
      S_j^\text{gd} & \text{if } \text{IoU}(\cup\{ S_i \in S \mid T^*_i = T_k^\text{gd}\}, S_k^\text{gd}) \geq \tau_\text{IoU} \\
      S_i & \text{otherwise}
    \end{cases},  %\\
% &\text{where } j = \argmax_{k} \{ \text{IoU}(S_k, S_j^\text{gd}\},
\end{align}



\begin{align}
TP = 
    \begin{cases}
    %   \mathbbm{1} & \text{if } \exists( \text{IoU}(S_i, S_j^\text{gd})> \tau_\text{IoU} \land T^*_i = T_j^\text{gd}) \\
       1 & \text{if } \exists(S^*_i = S_j^\text{gd} \land T^*_i = T_j^\text{gd}) \\
       0 & \text{otherwise}
    \end{cases},  
% &\text{where } j = \argmax_{k} \{ \text{IoU}(S_k, S_j^\text{gd}\},
\end{align}
\begin{align}
    \text{Recall}_{ov} = \frac{TP}{|S^{gd}|}
\end{align}
\fi

% \begin{equation} 
%     \text{max}\_\text{IoU}_j(j) = \underset{i \in S_{pred}}{\text{argmax}}\ \text{IoU}(M_i^{pred}, M_j^{gt}) 
% \end{equation} 
% \begin{equation} 
%     k = \text{max}\_\text{IoU}_j(j)
% \end{equation} 
% % and $T_j^{pred}$ is predicted text label of $M_j^{pred}$.

% For True Positive ($TP$), $M_k^{gt}$ must meets two conditions $C_1$ and $C_2$:
% \begin{equation} 
%     C_1 = \text{IoU}(M_k^{pred}, M_j^{gt}) \geq \tau_{\text{IoU}}
% \end{equation}
% \begin{equation} 
%     C_2 = \text{text\_sim}(T_k^{pred}, T_j^{gt}) \geq \tau_{\text{sbert}}.
% \end{equation}
% % \begin{equation} 
% % TP = \text{IoU}(pred\_m_i, gt\_m_j) \geq Th_{\text{IoU}} \cap \text{text\_sim}(pred\_t_i, gt\_t_j) \geq Th_{sbert}
% % \end{equation} 
% % \begin{equation} 
% % FN = \text{IoU}(pred\_m_i, gt\_m_j) \geq Th_{\text{IoU}} \cap \text{text\_sim}(pred\_t_i, gt\_t_j) < Th_{sbert}
% % \end{equation} 

% Then 
% \begin{equation}
%     TP_j(C_1, C_2)  =
%         \begin{cases}
%             1 & \text{if } C_1 \land C_2 \\
%             0 & \text{otherwise}
%         \end{cases}       
% \end{equation}
% % \begin{equation}
% %     \text{Recall}_{ov}= \frac{TP}{TP+FN}
% % \end{equation}
% \begin{equation}
%     \text{Recall}_{ov}= \frac{\sum_{j} TP_j}{|S_{ground}|}
% \end{equation}



%\subsection{Partial Pipeline Evaluation}
%The metrics proposed here evaluate only a part of the pipeline. 
%\subsubsection{Image-Text IoU ($\text{IoU}_{i-t}$)}

\subsection{Open-Vocab Class Similarity (OVCS) } 
OVCS measures only the quality of text generation given an oracle segmentation. 
This score is based on how similar generated texts are to the ground-truth texts given perfect segmentation. The ground-truth masks are used instead of the proposed image segments. If the cosine similarity between the generated text and the ground-truth text is higher than $\tau_\text{SBERT}$, it is considered a True Positive. The OVCS score is the rate of True Positive over the entire test set.

\iffalse
The ground-truth mask $M_j^{gt}$ is fed into the text generation process along with the input image. Then, the similarity score of predicted text $T_j^{pred}$ and the corresponding ground-truth text $T_j^{gt}$ is computed by Sentence-BERT cosine similarity score of the two embeddings.
for each ground-truth class $j$ presented in the image, 
\begin{equation}
    OVCS_j =
        \begin{cases}
            1 & \text{if text\_sim}(T_j^{pred}, T_j^{gt}) \geq Th_{sbert}\\
            0 & \text{if text\_sim}(T_j^{pred}, T_j^{gt}) < Th_{sbert}
        \end{cases}       
\end{equation}
then $OVCS$ is the average of all $OVCS_j$ where $j \in S_{ground}$.
% \begin{equation} 
% OVCS =  \frac{1}{n\_class}\sum_{j=1}^{n\_class}\text{text\_sim}(pred\_t_j, gt\_t_j)
% \end{equation}
\fi

% \subsection{Open Vocab Semantic Visual Score (OVSV) } 
% We also propose to quantify how well the predicted open-vocab texts semantically represent their corresponding regions. That is, we compute the similarity between the predicted text embedding and visual embedding of the corresponding region. Note that we also directly use ground truth mask as an input as in OVCS. 
% \begin{equation} 
% OVSV(m_{gt_i}) = similarity(E_t( t_{pred_i}),E_v( m_{gt}_i ) )
% \end{equation}
% Open-vocabulary segmentation with text generation needs to be evaluate on two main aspects: 1) the quality of output segmentation masks, and 2) how well the output texts represent the regions from segmentation mask.
% Normally, CLIP score can be used to evaluate how close a text and an image are, but our method relied on CLIP heavily, so we also propose metrics that are not completely dependent on CLIP for fairness 
% % avoid calculating score with CLIP in our evaluation process for fairness.

% As the text our method generate are free language that can be influenced by factors like popular culture and inherent information from CLIP's image-caption training set, the output texts are often not in alignment with predefined classes in segmentation dataset. For instance, given a segment that contains a car, our method may output brand of car models or car brands like `camry' or `mercedes' instead. To align these free language output text to the predefined text classes, we use Sentence-BERT \cite{reimers-2019-sentence-bert} to embed both sets of texts and calculate the similarity between them.


#########################


\section{Evaluation}  \label{sec:eval}
This section describes metrics we used to evaluate our results. There are two main parts to consider: the accuracy of output segments and the quality of corresponding output labels.


% This section introduces new metrics to evaluate the accuracy of the output segments and their corresponding text labels.
% To overcome the evaluation challenges due to the use of synonyms or the variation in label granularity, such as ``car'' vs ``wheel,'' we first map the predicted semantic labels to the predefined ones in the test set.


\subsection{Label reassignment}
\todo{We first need to map each predicted segment to one of the predefined ground truth labels in the test set; however, mapping between predicted labels and ground truth labels is challenging because our predicted labels may use synonyms or different variation in label granularity, such as ``car'' vs ``wheel,'' 
To map from predicted labels to ground truth labels correctly, a text encoder with knowledge of open-vocabulary meronymy relation is needed.
% we first map the predicted semantic labels to the predefined ones in the test set. 
As such encoders do not yet exist, we instead map the predicted segment directly to ground truth labels without using predicted labels. 
}

Given a predicted segment $S_i$ and its predicted text $T_i$, our goal is to relabel $S_i$ with $T^*_i$, which should be one of the test labels. 
% There are several ways 
% We describe two reassignment techniques based on text-to-text or segment-to-text similarity.

\subsubsection{CLIP-based label reassignment}
\label{seg2text}
% \todo{Given a predicted segment $S_i$ and its predicted text $T_i$, our goal is to relabel $S_i$ with the ground-truth label $T^*_i$ closest to $S_i$ in the CLIP's joint image-text space \cite{hessel2021clipscore, kawar2022imagic}.} 
This technique relabels $S_i$ with the ground-truth label that is closest to $S_i$ in the CLIP's joint image-text space \cite{hessel2021clipscore, kawar2022imagic} Formally, the new label $T^*_i$ is computed by
%Formally, let $j$ indicates the closest the new label $T^*_i$ is computed by:
\begin{align}
    T^*_i &= \argmax_{t \in T^\text{gt}} \left[ \mathrm{cossim}^\text{CLIP}(S_i, t)\right],
\end{align}
where $\text{cossim}^\text{CLIP}(s, t)$ uses CLIP's image encoder for $s$ and text encoder for $t$. 
The relabelling only takes place when $\mathrm{cossim}^\text{CLIP}(S_i, T^*_i) \geq \tau_\text{CLIP}$, otherwise $T^*_i=T_i$.
\todo{Alternatively, we can use human judgment to do thresholding. $S_i$ is relabeled only if humans deem $T^*_i$ a correct label for $S_i$.}
%However, the relabelling only happens if the closest ground-truth label is sufficiently similar to $S_i$.
%is based on matching a predicted segment $S_i$ to its closest ground-truth text. 

The relabelling does not consider the predicted label $T_i$, but it is helpful for evaluating our pipeline \emph{independent} of the text generation component borrowed from prior work (Section \ref{sec:textgeneration}). Note that this mapping technique is commonly used in open-vocabulary settings \cite{liang2022open, ghiasi2021open}.
% , but it is only applicable for comparing between CLIP-based methods. 



\subsection{Segmentation IoU}
This metric evaluates the quality of the output segments in terms of Intersection-over-Union (IoU) with respect to the ground-truth segments in each test image. 
Given a set of predicted segments with reassigned labels $T^*$, segments with the same label $T^*$ are merged to form a single segment for the label. Then, IoU for each image can be computed  using the standard protocol \cite{everingham2010pascal}.
%IoU for each image is then computed. 
In our experiments, 
\todo{we use IoU$_\text{CLIP}$ and IoU$_\text{human}$ to denote the mean IoU using $T^*$ from CLIP-based label reassignments with $\tau_\text{CLIP}$ and with human decision respectively.}



\subsection{Segment Recall}
% ($\text{Recall}_{ov}$)  
% \todo{Segment Recall measures how many segments labeled in the ground truth are discovered.
% This metric disregards any superfluous predictions, which can occur when our method discovers more objects in the background. The assignment of True Positive takes into account both the segment and label.
% % For each ground-truth segment, all predicted segments with the same label are merged.
% All predicted segments with the same reassigned label are merged. Then, if the IoU of the merged segment and the ground-truth segment is greater than $\tau_\text{IoU}$, we consider it a True Positive. 
% Segment Recall is the rate of True Positive over the number of grounding segments. As with the IoU metrics, we use $\text{Recall}_\text{CLIP}$ to $T^*$ reassignment method.}


\todo{Segment Recall measures how many objects labeled in the ground truth are discovered.
This metric disregards any superfluous predictions, which can occur when our method discovers more objects than the provided ground truth. After the reassignment, predicted segments with the same reassigned label are merged. Then, the merged segment is considered as a True Positive if the IoU between it and the corresponding ground truth segment is greater than  $\tau_\text{IoU}$. 
Segment Recall is simply the rate of True Positive over the number of grounding segments. As with the IoU metrics, we use $\text{Recall}_\text{CLIP}$ to $T^*$ reassignment method.}


% If the IoU between a merged segment and the corresponding ground truth segment is greater than  $\tau_\text{IoU}$, the merged segment is then considered a true positive.

% The merged segment is regarded as true positive if the IoU between it and the corresponding ground truth segment is greater than  $\tau_\text{IoU}$. 
% \todo{Segment Recall measures how many objects labeled in the ground truth are discovered.
% This metric disregards any superfluous predictions, which can occur when our method discovers more objects than the provided ground truth. Predicted segments with the same reassigned label are merged. Then, the merged segments are regarded as true positives if the IoU between them and the corresponding ground truth segment is greater than  $\tau_\text{IoU}$. 
% Segment Recall is the rate of true positives over the number of grounding segments. As with the IoU metrics, we use $\text{Recall}_\text{CLIP}$ to $T^*$ reassignment method.}


 % If the IoU between a merged segment and the corresponding ground truth segment is greater than IoU, the merged segment is then considered a true positive.

\subsection{\todo{Text Label Similarity (TLS) }} 
TLS measures only the quality of text generation given an oracle segmentation. 
This score is based on how similar generated texts are to the ground-truth texts given perfect segmentation. The ground-truth masks are used instead of the proposed image segments. If the cosine similarity between the generated text and the ground-truth text is higher than $\tau_\text{SBERT}$, it is considered a True Positive. The TLS score is the rate of True Positive over the entire test set.
