\begin{figure}
\centering
  % \includegraphics[scale=0.62]{./figs/result_analysis.pdf}
  \includegraphics[scale=0.615]{./figs/result_analysis_2.pdf}
  \vspace{-0.7em}
  \caption{
  % Our prediction and ground truth on PASCAL Context. 
  \textbf{Label reassignment issue.} Our predicted labels `roof' and `pub' are correct but are not matched to the ground-truth class `building' during label reassignment.}
  \label{fig:result_analysis}
  % \vspace{-1.em}
\end{figure}

% \vspace{-.5em}
\section{Discussion and Analysis} \label{sec:analysis}
% \vspace{-.5em}
% \myparagraph{Evaluation metrics recommendation} 
% % We introduce 2 label reassignment methods, 2 thresholding alternatives, and several metrics. 
% \todo{
% % Among numerous evaluation metrics, some is better than others. 
% % Here we discuss which evaluation options should be the priority for evaluating your results. 
% For label reassignment, we find that segment-to-text reassignment works better than text-to-text. The predicted label can only hold some aspects of the segment, while a segment naturally contains more information. For example, a predicted label `leg' has no indication of what animal its belong to, but the segment contains more information, like color, shape, or even information of adjacent regions, making it more recognizable. 
% We believe this is one reason of why segment-to-text reassignment performs better in Table \ref{tab:quan results}.
% As for thresholding method, constant thresholding is suitable for tuning parameters and ablation study. Human verification can better filter out incorrect reassignment and should be use when compare to other work.
% As for thresholding method, human verification works better but also costs more. Constant thresholding is more suitable for tuning parameters and ablation study. Human verification is beneficial for a potential showcase.
% Finally, regarding metrics, labelling quality and text generation quality are optional but useful for evaluation.
%}

\myparagraph{Mismatched text labels during evaluation.} Evaluation in our new setup is still challenging, despite using label reassignment.
%Our label reassignment using SBERT or CLIP in our evaluation can still miss correct predictions . 
For example, in Figure \ref{fig:result_analysis}, our algorithm breaks down the `building' ground-truth segment into `roof' and `pub', which are correct.
% But during SBERT label reassignment, `roof' is mapped to `ceiling' and `pub' finds no matches. As a result, our algorithm obtains zero $\text{IoU}_\text{tt}$ for `building'. 
But ST reassignment assigns `pub' to `sign', which is still technically correct but not counted toward our IoU score for `building'. 
Another problematic class is `person' whose parts like `face', `hair', `shirt' appear distinct in CLIP's space and may not be mapped to `person' (Figure \ref{fig:failure_case}).
%Another problematic class is `person', which consists of semantic parts, such %as `face', `hair', `shirt'. These parts can be distinct in CLIP's spaces, so they may not be mapped to `person' (Figure \ref{fig:failure_case}).
% While our segments in Figure \ref{fig:failure_case} are qualitatively reasonable, they receive zero $\text{IoU}_\text{CLIP}$ for class `person'. 
To overcome this challenge, we may need a new kind of embedding space that understands the hierarchical nature of object parts.
%may be required. 
% For some ground truth classes, the segments are almost always are mismatched. For example, t
% The class `person' is a challenge for our method in terms of label reassignment.
%Humans have parts that are visually different like, face, hair, body, and clothes. These part are semantically meaningful on their own, so they often have their own segments and text labels. % that are not mapped with `person'
% Moreover, labels describing people are often descriptive words the show characteristic, expression, etc., like `smiling' or `walking'.
%For text-text mapping, text labels like `hair' or `shirt' cannot be mapped to `person'. And for image-text mapping, although body parts can be mapped to `person', the clothes are often reassigned to the class `cloth'. % despite them being labelled as `person'.
%Examples are shown in Figure \ref{fig:failure_case}. 
%Qualitatively, these results seem good, but all of them receive a zero for the $\text{IoU}_\text{tt}$ of class `person'.

%\textbf{Challenge of evaluating on Pascal VOC 2012.}
%As there are only 20 classes, many objects in the scene are left unlabeled. 
%Zero-guidance segmentation however may discover and segment the unlabeled regions. 
%Figure \ref{fig:2012} shows an example where only two objects are labeled. Our method discovers `macbook', `desk', and `paper'
%, our method segments unlabeled object `macbook', `desk', and `paper'. To alleviate the mistaken penalty, segments in unlabeled regions that reassigned to a ground truth class not within an image are omitted in IoU score computation. The mistaken penalty however remains for the case that unlabeled object resigned to a grounding class. For example, `macbook' that is resigned to a ground truth class `monitor'.

% and then reassign them to the ground truth class `monitor' which would mistakenly penalize by IoU score. To alleviate this, segments in unlabeled regions that reassigned to a ground truth class not within an image are omitted in IoU score computation. 

% Pascal VOC 2012 contains only 20 classes. Each label is strictly specific such `potted plant' and `dining table'. Zero-guidance segmentation however may discover and segment objects in unlabeled region. For example, in Figure \ref{fig:2012}, our method segments the unlabeled object `plant' and then reassign them to the ground truth class `potted plant' which would mistakenly penalize by IoU score. To alleviate this, segments in unlabeled regions that reassigned to a ground truth class not within an image are omitted in IoU score computation. 

% Pascal VOC 2012 contains only 20 classes. Each label is strictly specific such 'plotted plant' and 'dining table'. Zero-guidance segmentation however may discover and segment objects in unlabeled region. For example, in Figure \ref{fig:2012}, our method segments the unlabeled tables and then reassign them to the ground truth class 'dinning table' which would mistakenly penalize by IoU score. To alleviate this, segments in unlabeled regions that reassigned to a ground truth class not within an image are omitted in IoU score computation. 



% \begin{figure}
%   \includegraphics[scale=1.1]{./figs/failure_case.pdf}
%   \vspace{-1.em}
%   \caption{\textbf{Failure Cases} 1) As our method segments images without any prompting, classes like `person' that has a lot of visually different parts may be difficult to reassign labels correctly for all parts. 
%   2) Background regions that share boundary with salient objects are still prone to global leak
%   3) Semantic merging may fail when text outputs of the same object give different descriptions.}
%   \label{fig:failure_case}
%   \vspace{-1.em}
% \end{figure}

\begin{figure}
\centering
%   \includegraphics[scale=0.512]{./figs/failure_case_long.pdf}
    % \includegraphics[scale=0.512]{./figs/analysis_new.pdf}
    \includegraphics[scale=0.9]{./figs/analysis_2.pdf}
  \vspace{-1.5em}
  \caption{\textbf{Failure cases.} 
  1) Classes that have many visually distinct parts, such as `person', are difficult to reassign labels correctly. 
  2) Background regions that share boundaries with salient objects are still prone to \emph{global context leakage}. 
  3) Semantic merging may fail when the text outputs of the same object give different descriptions.
  }
  \label{fig:failure_case}
  \vspace{-1.em}
\end{figure}


% \subsection{Failure cases} 
\myparagraph{Global context leakage.} Some background segments that share boundaries with primary objects can be mislabeled due to the influence of global contexts as shown in Figure \ref{fig:failure_case}. 
% \todo{The leakage may also result from the loss of mask precision when downsampling the segment masks to fit CLIP's image grid (24x24) during attention masking.}
Another problem that can cause context leakage is the low-resolution 24x24 image grid of CLIP visual tokens. As we downsample our segment masks to fit this grid, we lose masking precision and information can leak between neighboring segments.


%the low resolution image grid used in CLIP to define its visual tokens.
%We mask CLIP's attention with downsampled segment masks, which have a lower resolution than the original image and can , and the information from nearby segments can thus leak to each other.
%and these segments can still see the boundary of the other segments
%regions, and some objects with distinct shapes/appearance, such as planes or bicycles, are still recognizable although masked (see Figure \ref{fig:failure_case}).

\myparagraph{Merge fail due to different labels of the same object.}
Our over-segment outputs may use a wide variety of descriptions for the same object, such as car model and car color.
%For example, there are several ways to describe a cat, including by color or action, and 
These segments may fail to merge into a single object during the merging step (see Figure \ref{fig:failure_case}).
%There are several ways to describe a cat, including by color or action. 
%  This can lead to failure during semantic merging.% as the semantics these over-segment labels are too different.

\section{Conclusion} We have presented the first framework for \emph{zero-guidance segmentation}, a novel problem that seeks to segment and label an image using natural language automatically. We leverage two generalist models, DINO and CLIP, and propose a technique to map a given segment to CLIP's joint space by masking CLIP's attention, allowing zero-shot segmentation without the need for any segmentation dataset or fine-tuning. We also introduce a new evaluation protocol for this problem and will release our code.

% and balancing global and local context information in the output embedding. 
% By leveraing CLIP and DINO, our framework can solve 
 %We also introduce new evaluation metrics for this new setup. 
 % We next discuss current challenges in zero-guidance segmentation.


% \subsection{Limitations}
% \textbf{Attention size} Attention masking and global subtraction operate on attention space of the pretrianed ViT-based CLIP, hence our method's resolution capacity is restricted by the attention size of the pretrained CLIP.

% Also, our framework might inherits bias from the pretrained generalist which could impact the performance of segmentation. For examples, .....
% and since our modules give free-language class output, the output could appear impolitic and offensive discussed in [potential Negative Societal impact]

