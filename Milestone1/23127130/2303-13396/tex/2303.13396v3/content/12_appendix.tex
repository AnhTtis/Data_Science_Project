% \newpage
% \clearpage
\appendix
% \section*{Appendix}
\section*{Appendix: Zero-guidance Segmentation Using Zero Segment Labels}
% \label{sec:appendix}
\vspace{10pt}
In this Appendix, we provide additional details and experiments:
\begin{itemize}
    \item Section~\ref{apx:clip-visualize}: CLIP's self-attention visualization
    \item Section~\ref{apx:clustering}: Implementation details of our segment candidate finding method
    \item Section~\ref{apx:thresholds}: Thresholds used in metrics
    \item Section~\ref{apx:voc}: Limitations of Pascal VOC dataset for evaluation.
    \item Section~\ref{apx:hypertune}: Details on hyperparameters tuning
    \item Section~\ref{apx:userstudy}: User study
    % \item Section~\ref{apx:userstudy_analysis}: User study analysis
    \item Section~\ref{apx:results}: Additional results
    \item Section~\ref{apx:negimpact}: Potential negative societal impacts
\end{itemize}

\section{CLIP's Self-attention Visualization} \label{apx:clip-visualize}
Figure \ref{fig:self_attn} visualizes the self-attention maps of CLIP's image encoder across different layers. The self-attention maps appear to be meaningful in the earlier layers, i.e., the patch tokens mostly attend to regions that contain semantically similar pixels, and the global token attends to regions with prominent objects. However, the self-attention map appear more random and uninterpretable in the later layers.


\section{Finding Segment Candidates with DINO: Implementation Details} \label{apx:clustering}
We provide more implementation details for Section 3.1. 
%the first paragraph is Amir's work, the second is ours.
We adopt DINO feature extraction method from Amir et al. \cite{amir2021deep}. The method first feeds an input image into DINO and extracts ``key'' values from the last attention layer as dense spatial features. %This results in $111\times111$ patches.
% The input image is pre-processed by resizing so that the shorter size is 224 pixels. Dino is modified to extract features from overlapping patches during inference to increase feature resolution, and the resulting per-patch feature dimension is 111 for the shorter side in our implementation. 

% Following this is the detail of clustering part, which is our own design.
After extracting the features, we partition the image into segments by clustering DINO's features.
We perform bottom-up clustering starting from each feature vector. The merging is done recursively by combining two clusters with the least combined variance. After this initial clustering, we end up with a binary tree where the root is the cluster of all the feature vectors. This binary tree structure is used as a heuristic to perform divisive clustering. Each node in the tree is represented by the average feature of its members. We prune the siblings whose cosine similarity score is over  $T_{Dino} = 0.9$. This yields a segmentation map with all leaf nodes of the binary tree as segments. The two-stage clustering algorithm is chosen to lessen the computation requirement since we start from a large number of spatial features ($111\times111$).
%We first assign each per-patch feature as one cluster, then recursively merge two clusters with the least combined variance.
%We record the merging using a binary tree.
%The last cluster is assigned as the root of the binary tree, and the two clusters that form the root as its children. Each sibling pair of this tree is two segments that can be merged to reconstruct their parent, which will be used in merging step of our pipeline. Each node in the binary tree is represented by the average per-patch feature of its members. 
%We prune the siblings whose similarity scores is over  $T_{Dino} = 0.9$, then compute for segmentation map with all leave nodes of the binary tree as segments.

% We hierarchically partition an image and stop when the new partitions have average DINO features with cosine similarity over a threshold $T_{Dino}$.
% We use a binary tree structure to represent the hierarchical partition of the input image. 
% To construct the binary tree, we use agglomerative clustering, a bottom-up algorithm that first regards each pixel as a cluster and then merges pairs of nearest clusters until a stop condition is met. We keep merging clusters until there is only one cluster left (the whole image). The tree records merging of the last $n$ clusters. The last cluster is assigned as the root of the binary tree and two clusters that form the root as its children. Children nodes are added to the tree in the same manner until their cosine similarity is over $T_{Dino} = 0.9$. 
% Each sibling pair of this tree is two segments that can be merged to reconstruct their parent, which will be used in merging step of our pipeline.

Following Amir et al, the segmentation map is then upsampled to input resolution and refined using DenseCRF as described in \cite{krahenbuhl2011efficient}. The Unary Energy is set as the normalized distance of each feature vector to all $k$ centroids, and the pairwise connection is fully-connected. 
Pairwise edge potentials are Gaussian kernels with location (pixel coordinates) as feature and Bilateral kernels with location and RGB values as features. 
Our implementation can be founded in the provided source code.
% We use AgglomerativeClustering from sklearn to form a binary tree where the root is the whole in put image. each sibling pairs of this tree are two segments that can be merge to reconstruct their parent.
% We recursively partition a segment into two new segments and stop when the cosine similarity of the two new segments' average feature vectors is less than a threshold $T_{Dino}$.
% with a condition that the cosine similarity of the two new segments' average feature vectors must be higher than a threshold $T_{Dino}$.

\begin{figure}
\centering
  \includegraphics[scale=0.45]{./figs/self_attn.pdf}
  \vspace{-.5em}
  \caption{\textbf{Visualization of self-attention in CLIP's image encoder.} Each row shows the attention of the token of the pink patch across layers. The last row shows global token's attention.}
  \label{fig:self_attn}
  \vspace{-1.2em}
\end{figure}

\section{Thresholds Used in Metrics}
\label{apx:thresholds}
% We use $\tau_\text{SBERT} = 0.5$ according to a user study \cite{gao2021simcse}, $\tau_\text{CLIP} = 0.1$, and $\tau_\text{IoU} = 0.5$. 
\textbf{S-BERT text-to-text similarity threshold ($\tau_\text{SBERT}$).} 
\label{apx:text_sim}
% We empirically show that SBERT's text representation similarity is more semantically meaningful than CLIP's in Figure \ref{fig:text_sim}. CLIP's text encoder does well among car-related terms. However, the similarity score between `dog' and `car' is 0.85, which is higher than any car-related pairs. Moreover, `dog' is more similar to `car' than `labrador', a dog breed. On the contrary, SBERT's similarity scores show clear differences between the two groups. SBERT has been shown to correlate well with human evaluation \cite{gao2021simcse}. Thus, we use SBERT's text representation when computing similarity between texts in the proposed evaluation metrics (Section \ref{sec:eval}). 
% \todo{We provide Text-to-text IoU (IoU$_\text{tt}$) scores with several $\tau_\text{SBERT}$ threshold values in Figure \ref{fig:sbert_thres} and Table \ref{tab:thres_sbert}. In the main experiment, we select $\tau_\text{SBERT}=0.5$ because it corresponds to a match between topics based on human judgement \cite{cer2017semantic, gao2021simcse}, which is the main goal of our evaluation metric. }
We provide Text-to-text IoU (IoU$_\text{tt}$) scores with several $\tau_\text{SBERT}$ threshold values in Figure \ref{fig:sbert_thres} and Table \ref{tab:thres_sbert}. In the main experiment, when referred to a constant threshold, we select $\tau_\text{SBERT}=0.5$ as it represents an approximate minimum threshold that human evaluators use to determine if two sentences share a common topic, based on a user study \cite{cer2017semantic, gao2021simcse}. 
%corresponds to the minimum value that indicates that two sentences match their topics based on human judgement \cite{cer2017semantic, gao2021simcse}}. 



% For possible future explorations and comparisons with future studies, we also provide \todo{Text-to-text IoU} (IoU$_\text{tt}$) scores for other $\tau_\text{SBERT}$ thresholds in Figure \ref{fig:sbert_thres} and Table \ref{tab:thres_sbert}.



%We also provide IoU$_\text{tt}$ with several values of $\tau_\text{SBERT}$ in Figure \ref{fig:sbert_thres}. Optionally, we further propose the simplify version of the metric by using a representative threshold supported by the user study \cite{gao2021simcse}. To elaborate, our task mainly focuses on similarity between on a single noun or noun phrase corresponding to group2 based on human judgement \cite{cer2017semantic, gao2021simcse}, hence we use $\tau_\text{SBERT} = 0.5$ to report IoU$_\text{tt}$ in our main experiments.   




\textbf{CLIP segment-to-text similarity threshold ($\tau_\text{CLIP}$).} 
We provide Segment-to-text IoU (IoU$_\text{st}$) scores with several $\tau_\text{CLIP}$ threshold values in Figure \ref{fig:clip_thres} and Table \ref{tab:thres_clip}.
Selecting the threshold $\tau_\text{CLIP}$ is more challenging, since there is no established consensus or user studies to rely on. Figure \ref{fig:clip_hist} shows histograms of CLIP similarity scores between ground-truth image segments and their corresponding ground-truth labels in Pascal Context and Pascal VOC datasets. Given the distributions, we select $\tau_\text{CLIP} = 0.1$ to be on the safe side to report Segment-to-text IoU scores in the main experiment. 

It is important to note that for our zero-guidance segmentation problem, the thresholds $\tau_\text{CLIP}$ and $\tau_\text{SBERT}$ are used in the label reassignment verification process (Section 4.2), which is part of the evaluation not the segmentation algorithm itself.
For a given algorithm, varying the threshold values can result in distinct performance profiles, e.g., a precision-recall curve, and several thresholds may be used together for the purposes of evaluation and comparison, as is common practice in the object detection literature \cite{zou2019object}.
% Alternatively, this threshold might be updated or several thresholds might be used together just like the case of object detection metrics \cite{zou2019object}. It is important to note that this threshold $\tau_\text{CLIP}$ is part of the metric, and different threshold values can produce different performance profiles, e.g., a precision-recall curve, of a given algorithm. To facilitate future discussion, 



% Selecting the threshold $\tau_\text{CLIP}$ is more challenging, since there is no established consensus or user study to rely on. Figure \ref{fig:clip_hist} shows the histograms of the CLIP similarity scores between ground truth image segments and their corresponding ground truth texts in the two datasets. Given the distributions, we select $\tau_\text{CLIP} = 0.1$ to be on the safe side. Alternatively, this threshold might be updated or several thresholds might be used together just like the case of object detection metrics \cite{zou2019object}. It is important to note that this threshold $\tau_\text{CLIP}$ is part of the metric, and different threshold values can produce different performance profiles, e.g., a precision-recall curve, of a given algorithm. To facilitate future discussion, we include  segmentation results using several threshold values in Figure \ref{fig:clip_thres} and Table \ref{tab:thres_clip}.


\iffalse
Selecting the threshold $\tau_\text{CLIP}$ is more challenging, since there is no established consensus or user study to rely on. Figure \ref{fig:clip_hist} shows histograms of the CLIP similarity scores after the label reassignment (Section \ref{seg2text}) in the two datasets. Given the distributions, we select $\tau_\text{CLIP} = 0.1$, essentially allowing all reassignments. Alternatively, this threshold might be updated or several thresholds might be used together just like the case of object detection metrics \cite{zou2019object}. It is important to note that this threshold $\tau_\text{CLIP}$ is part of the metric, and a particular threshold value can produce different performance profiles, e.g., precision-recall curves, of a given algorithm.
To facilitate future discussion, we include results on several threshold values in Figure \ref{fig:clip_thres} and Table \ref{tab:thres_clip}.
\fi

%We report IoU$_\text{st}$ with several threshold values in Figure \ref{fig:clip_thres} and Table \ref{tab:thres_clip}. We also provide the distribution of clip score between each segment and its corresponding ground truth class that maximizes the score in Figure \ref{fig:clip_hist}. Alternatively, the community may design a convention for competition purposes by using particular thresholds (as in the case of $\tau_\text{IoU}$ during the early era of object detection \cite{zou2019object}). In our main experiments, we report IoU$_\text{st}$ with  $\tau_\text{CLIP} = 0.1$. However, in order to assess the overall performance, multiple threshold values should be considered.
 
 
%   we do not use only a particular threshold value to represent the metric.
%  We further suggest that a decent threshold should be consistent with community's convention (as in the case of $\tau_\text{IoU}$ in object detection community \cite{zou2019object}  ) or based on a user study. We also provide the distribution of clip score between each segment and its corresponding ground truth class that maximizes the score in Figure \ref{fig:clip_hist}. 
 
 
% We further suggest that a decent threshold should be consistent with community's convention (as in the case of $\tau_\text{IoU}$ in object detection community \cite{zou2019object}  ) or based on a user study. We also provide the distribution of clip score between each segment and its corresponding ground truth class that maximizes the score in Figure \ref{fig:clip_hist}. In our main experiments, we report IoU$_\text{st}$ with  $\tau_\text{CLIP} = 0.1$. However, in order to interpret the overall score, other threshold values must be also considered.
 
%  we emphasize that $\tau_\text{CLIP} = 0.1$ could not represent the overall score or replace other threshold values. 
% We leave the discussion to the community and future works
% A user study would be required in order to select a justifiable threshold.

% e Selecting a particular value of $\tau_\text{CLIP}$ should be decided by community's agreement as in the case of IoU threshold \cite{zou2019object} in object detection task.



\textbf{IoU threshold ($\tau_\text{IoU}$).}   \label{sec:iou_thres}
We use $\tau_\text{IoU} = 0.5$, which is commonly used in object detection tasks to determine if a predicted bounding box is `correct' compared to the ground truth \cite{zou2019object}. 


% \begin{figure}
% \centering
% %   \includegraphics[scale=1.2]{./figs/text_sim.pdf}
%   \includegraphics[scale=0.4]{./figs/text_embedding.pdf}
%   \caption{Empirical experiment on text similarity using CLIP's text encoder and Sentence-BERT.}
%   \label{fig:text_sim}
% \end{figure}



\begin{figure}
\centering
  \includegraphics[scale=0.47]{./figs/iou_tt.pdf}
%   \caption{\textbf{Segment to text IoU (IoU$_\text{st}$) and clip threshold ($\tau_\text{CLIP}$) }}
  \caption{\textbf{Text to text IoU and SBERT threshold }}
  \label{fig:sbert_thres}
%   \vspace{-1.5em}
\end{figure}



\begin{figure}
\centering
  \includegraphics[scale=0.47]{./figs/iou_st.pdf}
%   \caption{\textbf{Segment to text IoU (IoU$_\text{st}$) and clip threshold ($\tau_\text{CLIP}$) }}
  \caption{\textbf{Segment to text IoU and CLIP threshold }}
  \label{fig:clip_thres}
%   \vspace{-1.5em}
\end{figure}


% Please add the following required packages to your document preamble:
% \usepackage{multirow}




\begin{figure}
\centering
%   \includegraphics[scale=0.33]{./figs/clip_dist.pdf}
 \includegraphics[scale=0.33]{./figs/gt_clip_dist.pdf}
%   \caption{\textbf{Segment to text IoU (IoU$_\text{st}$) and clip threshold ($\tau_\text{CLIP}$) }}
  \caption{\textbf{CLIP similarity score distribution between the ground truth segment and the ground truth label}}
  \label{fig:clip_hist}
%   \vspace{-1.5em}
\end{figure}


\begin{figure}
\centering
  \includegraphics[scale=0.47]{./figs/start_ly.pdf}
  \caption{\textbf{Start masking layer selection}}
  \label{fig:start_ly}
%   \vspace{-1.5em}
\end{figure}

\begin{figure}
\centering
  \includegraphics[scale=0.47]{./figs/gs_sd.pdf}
  \caption{\textbf{Global subtraction's variance selection}}
  \label{fig:gs_sd}
%   \vspace{-1.5em}
\end{figure}

\begin{figure}
\centering
  \includegraphics[scale=0.47]{./figs/merge_ablation.pdf}
  \caption{\textbf{Merging threshold selection }}
  \label{fig:merge_design}
%   \vspace{-1.5em}
\end{figure}







%For these two reasons, we instead use Sentence-BERT
% \section{SBERT VS CLIP text sim} \label{apx:text_sim}
% \subsection{Text-to-text similarity threshold } \label{apx:text_sim}

% Our task mainly focuses the semantic similarity between object's names, hence, based on the user study \cite{gao2021simcse}, we decided to use $ \tau_\text{SBERT}$ of 0.5. 


%  \subsection{Segment-to-text similarity threshold }
% CLIP


% \subsection{Merging segment candidates ablation}
% In our main experiments, we merge semantically similar segments based on their the mean of visual and text similarity and the threshold $\tau_\text{merge}$ of 0.8. 

% label only a few foreground objects while ignoring much of the background (see Figure \ref{fig:2012})
\begin{figure}
    \centering
%   \includegraphics[scale=0.51]{./figs/pascal2012.pdf}
%   \includegraphics[scale=0.37]{./figs/cat_teen_plant.pdf}
    % \includegraphics[scale=0.37]{./figs/macbook_office_new.pdf}
    \includegraphics[scale=0.38]{./figs/pascal_voc_gt.pdf}
\vspace{-.5em}
  \caption{An example of Pascal VOC segmentation dataset}%. As there are only 20 classes, many objects in the scene are left unlabeled.  which may not be suitable for zero-guidance segmentation.
  %which that has no instruction of what to segment.}
  \label{fig:2012}
\end{figure}

% \section{An Example of Pascal VOC Dataset} 
% \todo{
% Figure \ref{fig:2012} shows an example of Pascal VOC's ground truth and our result. As Pascal VOC only has 20 classes, many objects in the scene are left unlabeled, which may not be suitable for zero-guidance segmentation. Our method discovers various objects not presented in the ground truth, such as `paper', `macbook', and `poster'.
% }

\section{Limitations of Pascal VOC for evaluating zero-guidance segmentation.} \label{apx:voc}
Evaluating zero-guidance segmentation performance using Pascal VOC (PAS-20) may not be ideal because PAS-20 has a very small number of labeled classes. In this dataset, many objects or sometimes the vast majority of regions in the images are left unlabeled as shown in Figure \ref{fig:2012}.
%As a result, this dataset cannot be used to evaluate the ability to discover all semantic objects. 
%Figure \ref{fig:2012} shows an example of PAS-20's ground truth and our result. 
% As Pascal VOC only has 20 classes, many objects in the scene are left unlabeled. 
Our method can discover various objects not presented in the ground truth labels, such as `paper', `macbook', and `poster', but these are never counted towards any IoU scores.
  %which that has no instruction of what to segment.

% \section{}
\section{Hyperparameters Tuning} % this should end up in appendix
\label{apx:hypertune}
% We present how hyperparameters used in our pipeline are selected. These hyperparameters are selected based on 100 randomly selected images from the Pascal Context's training split. We would like to note here that there is no training involved in our pipeline, and the hyperparameters selected for Pascal Context works fine on other datasets.

We present how our hyperparameters, which are the layer to start attention-masking, the global subtraction variance, and the merging threshold, are tuned.
%, are tuned. 
Our tuning metrics are the Text-to-text IoU (IoU$_\text{tt}$) and Segment-to-text IoU (IoU$_\text{st}$) with the constant thresholds. The data used in this process are 100 randomly selected images from the Pascal Context's training split, which is never used for evaluation. Note here that there is no training involved in our pipeline.

The first parameter is the layer where attention masking starts. 
We found that masking from early layers erases all global context, resulting in poor results as context can be crucial for recognizing objects. Masking only the last layer also has poor results due to global leak. We found that masking attention of the last four layers (21-24) gives the best scores (see Figure \ref{fig:start_ly}).

Another important hyperparameter is the variance ($\sigma^2$) in the saliency estimation, which is used to determine the degree of global context subtracted from a region (see Equation 4). The higher the variance, the more global context is reduced. As seen in Figure \ref{fig:gs_sd}, the optimal spot is at $\sigma^2 = 2.5$. 
 %The graph shows that over reduction of global context can hurt the performance.

The last parameter is the merging threshold $\tau_{merge}$ used to decide which segment candidates to merge (Section 3.4). We found that $\tau_{merge} = 0.8$ returns the best scores on both IoU$_\text{tt}$ and IoU$_\text{st}$ on the tuning set (see Figure \ref{fig:merge_design}).

% Note that we consider both of the visual and text similarity as the criteria because it could reflect the overall performance our framework.
\begin{table}[]
\caption{Text-to-text IoU with several SBERT thresholds}
\vspace{-0.5em}
\label{tab:thres_sbert}
\resizebox{\columnwidth}{!}{
\setlength{\tabcolsep}{1.5pt}
\begin{tabular}{lccccccccrrr}
\toprule
               & \multicolumn{11}{c}{IoU$_\text{tt}$}                                       \\ \midrule
$\tau_\text{SBERT}$ & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8  & 0.9  & 1.0    \\ \midrule
Pascal Context & 8.1  & 8.1  & 8.2  & 9.4  & 11.2 & 11.4 & 9.8  & 8.6  & 6.5  & 5.6  & 5.3  \\
Pascal VOC     & 11.2 & 11.2 & 11.6 & 16.0 & 23.9 & 27.3 & 24.2 & 21.5 & 15.3 & 12.0 & 11.2 \\ \bottomrule
\end{tabular}}
\end{table}


\begin{table}[]
\caption{Segment-to-text IoU with several CLIP thresholds}
\vspace{-0.5em}
\label{tab:thres_clip}
\resizebox{\columnwidth}{!}{
% \setlength{\tabcolsep}{1pt}
\begin{tabular}{lcccccccc}
\toprule
                              & \multicolumn{8}{c}{IoU$_\text{st}$}                   \\ \midrule
$\tau_\text{CLIP}$  & 0.00 & 0.05 & 0.10 & 0.15 & 0.20 & 0.25 & 0.30 & 0.35 \\ \midrule
Pascal Context & 19.6 & 19.6 & 19.6 & 19.5 & 16.0 & 2.2  & 0.0  & 0.0  \\
Pascal VOC     & 20.1 & 20.1 & 20.1 & 20.8 & 24.8 & 4.2  & 0.0  & 0.0  \\ \bottomrule
\end{tabular}}
\end{table}

\section{User Study Implementation Details}
\label{apx:userstudy}
We conducted a user study using Amazon Mechanical Turk. 
% For each set of questions, a human evaluator was given a detailed instruction and 30 questions. 
Each evaluation task contains a detailed instruction and 30 questions. We did not limit the number of tasks per human evaluator.
For each question, the evaluators were shown a predicted segment, in the form of a highlighted region, overlaid on an input image, and its predicted text label. The evaluators were then asked to rate how well the label describes the segment on a scale of 0-3, defined in the provided instruction as shown in Figure \ref{fig:user_study_ui}. %\ref{tab:userstudy_option}.
There were a total of 23,076 questions, each evaluated by three different evaluators. The total number of unique evaluators was 429, and the average number of questions answered by the evaluators was about 155.
%The maximum number of questions answered by a single evaluator was 5,730
We calculated the scores (Section 4.3) for each of the three batches separately then reported the average.
We include the full instruction and a task example in Figure \ref{fig:user_study_ui}.
% The human subjects were given predicted segments in the form of highlighted regions overlaid on input images along with their corresponding text labels. Then, the subjects are instructed to carefully choose from four options as described in Table \ref{tab:userstudy_option}. Examples of each option are also presented.
% Each segment-label pair was evaluated by 3 different subjects. 


% \begin{figure}
% \centering
%  \includegraphics[scale=1.05]{./figs/correct.pdf}
% %   \vspace{-1.em}
%   % \caption{Examples of correct (top) and incorrect (below) labels \todo{ TODO: add correct labels}} 
%   \caption{Examples of correct labels} 
  
%   \label{fig:user_study_adj_correct}
% \end{figure}

% \begin{figure}
% \centering
%  \includegraphics[scale=1.05]{./figs/incorrect.pdf}
% %   \vspace{-1.em}
%   % \caption{Examples of correct (top) and incorrect (below) labels \todo{ TODO: add correct labels}} 
%   \caption{Examples of incorrect labels} 
  
%   \label{fig:user_study_adj_failure}
% \end{figure}

% \begin{figure*}
% \centering
%  \includegraphics[scale=1.1]{./figs/userstudy_all_example.pdf}
% %   \vspace{-1.em}
%   % \caption{Examples of correct (top) and incorrect (below) labels \todo{ TODO: add correct labels}} 
%   \caption{Examples of segmentation results that were evaluated by human evaluators with scores ranging from 3 (correct) to 0 (incorrect). The full definitions of the scores can be found in Figure \ref{fig:user_study_ui}.} 
  
%   \label{fig:user_study_adj_failure}
% \end{figure*}







% \section{User Study Analysis}
% % \todo{
% We provide examples of the scores given by human evaluators in Figure \ref{fig:user_study_adj_failure}. According to the result, most of our score-0 labels are single-word adjectives, such as `small' and `black', or collective nouns, such as `group' and `herd'. Another kind of score-0 labels is caused by biases toward stereotypical appearances of objects, such as when a pet dog was mislabeled as `stray' due to its shabby appearance in Figure \ref{fig:user_study_adj_failure} (row 4). Some of score-1 labels correspond to descriptions or abstract nouns that are related to their segments but may not fully describe them, such as `reflection', `dining', and `sunny', and some other labels describe specific but incorrect types of objects, such as `uber' or `military'. Most of our score-2 labels are nearly accurate, but the segments may incompletely or excessively cover the referred objects, such as `lush moss', `jet engine', and `few puppies' in the second row in Figure \ref{fig:user_study_adj_failure}. Most of our score-3 labels accurately represent their segments, such as `horse' and `new cruiser' shown in the first row, and they can be descriptive even on background objects, such as `sandy beach' and `crowd observing', unlike labels from traditional segmentation methods.

% We provide additional analysis on the failure cases based on the human evaluation. According to the study's result, most of our ``incorrect'' (score 0) labels are adjectives, i.e., `small' and `black', or collective nouns, i.e., `group' and `herd'  (see Figure \ref{fig:user_study_adj_failure}). Another type of failure corresponds to the bias toward the stereotypical appearance of objects, as in Figure \ref{fig:user_study_adj_failure}, where a pet dog is  mistakenly labeled as `stray' due to its shabby appearance. }


% #  These types of words are commonly followed by a noun, for example, `black horse' and `group of people' , otherwise they are considered insufficiently informative to describe objects.
% stereo
%  trade-off 
% The main reason for this failure is that the text generation process ends too early, i.e, before a corresponding noun is generated. 

% To mitigate this problem, plausible solutions are to decrease the multiplicative factor of the ZeroCAP's \emph{end-token} probability (currently set to 1.1 by default), or by using a simple heuristic method to prevent the generation of \emph{end-token} before a noun has been generated.

% percentage 
% 

% This problem could be handled by set `ending factor' parameter of ZeroCAP or by performing a heuristic method to contro
% that stop the text generation only after a noun has been generated.



% which is often insufficiently informative to represent a segment. Another type of word that is often considered incorrect is a collective noun, i.e., `group' and `herd'. To illustrate, our method labels a group of persons or animal with a word `group'. The main reason for these failures is that the text generation process ends too early, i.e, before a corresponding noun is generated. This problem could be handled by tuning `ending factor' parameter of ZeroCAP or by performing a heuristic method to stop the text generation only after a noun has been generated.
% \label{apx:userstudy_analysis}


\section{Additional Results}
\label{apx:results}
We present more qualitative results in this section. In Figure \ref{fig:ablation_apx}, we include more results from the ablation experiment in Section 5.3.
We show random results of our method in Figure \ref{fig:ran_context} for the Pascal Context dataset and Figure \ref{fig:ran_2012} for the Pascal VOC 2012 dataset.

\begin{figure*}
\centering
 \includegraphics[scale=0.85]{./figs/ablation_apx.pdf}
%   \vspace{-1.em}
  \caption{Qualitative ablation analysis} 
  \label{fig:ablation_apx}
\end{figure*}

\begin{figure*}
\centering
 \includegraphics[scale=0.82]{./figs/ran_context.pdf}
%   \vspace{-1.em}
  \vspace{.5em}
  \caption{Randomly sampled results from Pascal Context} 
  \label{fig:ran_context}
\end{figure*}

\begin{figure*}
\centering
 \includegraphics[scale=0.82]{./figs/rand_2012.pdf}
  \vspace{.5em}
  \caption{Randomly sampled results from Pascal VOC 2012}
  \vspace{8em}
  \label{fig:ran_2012}
\end{figure*}

% \section{Comparison to Supervised Baselines}
% \label{apx:baselines}
% Table \ref{tab:baselines} presents an IoU comparison with existing supervised open-vocabulary baselines based on the numbers presented in \cite{liang2022open} on three datasets, Pascal VOC (PAS-20), Pascal Context with 59 most common classes (PC-59), and the full version of Pascal Context with 459 classes (PC-459). 
% Unlike our approach, these methods require segmentation annotations (or pretrained segmentation models) during training and text queries or labels to guide the segmentation. Note that ``open-vocabulary'' means that the labels can be arbitrary and unseen during training, but the labels still need to be provided to these methods to specify which object to find or segment at inference time.
% %and ground-truth labels, which specify which object to find or segment, at inference time. 
% %Unlike our approach, these methods require ground-truth labels, which specify which object to find or segment, at inference time. 
% %Their final outputs are segments, each of which corresponds to a \emph{given} ground-truth label. 
% Comparing scores between such methods using the same predefined test labels is thus straightforward and justified.
% %As these baselines take text labels as input, the matching between visual and text is a non-issue. 
% % They can use different matching criteria/thresholds, and their IoUs can still be compared fairly. 
% % All the details about how IoU scores are computed (such as thresholds/matching criteria) may varied but the IoUs can be compared fairly. 
% Our method, on the contrary, predicts arbitrary text labels at inference time and is not directly comparable using the same standard benchmarks. Nonetheless, our proposed label reassignment procedure can provide useful comparative analysis against these supervised baselines on the same benchmarks. 
% %As we discussed in Section \ref{apx:thresholds}, for future methods that solve zero-guidance segmentation, 
% %and the matching in the evaluation step is merely for assessing the results with the standard metric. 
% In Table \ref{tab:baselines}, we report our IoU scores from the setups with the best performance using segment-to-text IoU with a constant threshold $\tau_\text{CLIP} = 0.1$ and human score $\geq 1$.
% %but if future work wants to compare the results with us, they may need to consider to use the same matching or justify how to compare different matching approaches with fairness.
% There is still a gap in performance between our unsupervised method and these supervised baselines, but our method performs only slightly worse on the more challenging PC-459.



% \begin{table}[]
% \centering
% \caption{IOU score comparision with open-vocabulary segmentation baselines}
% \label{tab:baselines}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{lcccc}
% \toprule
% Method & Segmentation label   & PAS-20 & PC-59  & PC-459 \\ \midrule
% Lseg \cite{li2022language}          & \cmark & 47.4   & -     &  -     \\
% SimBaseline \cite{xu2021simple}     & \cmark & 74.5   & -     &  -   \\
% ZegFormer \cite{ding2022decoupling} & \cmark & 80.7   & -     &  -     \\
% OpenSeg \cite{ghiasi2021open}       & \cmark & -      & 42.1  &  9.0   \\
% OVSeg \cite{liang2022open}          & \cmark & 94.5   & 55.7  &  12.4   \\  \midrule
% % GroupVit \cite{xu2022groupvit}      & \xmark & 52.3   & 22.4        \\
% Ours ($\text{IoU}_\text{st}$)       & \xmark  & 20.1  &   19.6  & 11.3 \\ \bottomrule
% \end{tabular}}
% \end{table}

% \begin{table}[]
% \centering
% \caption{IOU scores comparison between supervised open-vocabulary segmentation baselines (trained with segmentation labels) and our unsupervised method.}
% \label{tab:baselines}
% % \resizebox{\columnwidth}{!}{
% \begin{tabular}{lccc}
% \toprule
% Method   & PAS-20 & PC-59  & PC-459 \\ \midrule
% % \multicolumn{4}{c}{\emph{Supervised open-vocab method}} \\
% Lseg \cite{li2022language}          & 47.4   & -     &  -     \\
% SimBaseline \cite{xu2021simple}     & 74.5   & -     &  -   \\
% ZegFormer \cite{ding2022decoupling} & 80.7   & -     &  -     \\
% OpenSeg \cite{ghiasi2021open}       & -      & 42.1  &  9.0   \\
% OVSeg \cite{liang2022open}          & 94.5   & 55.7  &  12.4   \\  \midrule
% % GroupVit \cite{xu2022groupvit}    & 52.3   & 22.4        \\
% Ours - $\text{IoU}_\text{c}$       & 20.1  &   19.6  & 11.3 \\ 
% Ours - $\text{IoU}_{\text{h } \geq 1}$  & - &   22.7  & - \\ \bottomrule
% \end{tabular}
% \end{table}


\section{Potential Negative Societal Impacts}
\label{apx:negimpact}

%Given a segmented image, our method generate free-language texts that identify or describe all segment with no direct supervison.
% can depict objects and their associated free-form text description. 
%That is, unlike traditional methods, the outputs are not constrained and can freely label a segment in many different ways. 
Unlike traditional segmentation methods, our method outputs arbitrary text labels and may describe people with incorrect assumptions or discriminatory characteristics based on their stereotypical appearances, such as body shape, clothes, nationality, and sexual orientation. For example, we found `Asian woman' or `homeless' in some generated, which can be offensive in some scenarios. Some characteristics, such as beauty and politics, are rather subjective and challenging to filter without human intervention. Due to the data-driven nature of the pre-trained models we use, our model would also be biased toward the culture, preferences, and characteristics of the training sets and may pose controversial issues. 





% \begin{table*}[]
% \centering
% \caption{User study's description on the criteria of each option.}
% \label{tab:userstudy_option}
% \resizebox{\textwidth}{!}{
% \setlength{\tabcolsep}{10pt}
% \begin{tabular}{cp{0.95\textwidth}}
% \toprule
% Score       &   \multicolumn{1}{c}{Description} \\ \midrule
% 3           &  The label correctly describes the region as an object or object part with or without detailed attributes. Slightly inaccurate regions are allowed. \\ \midrule
% 2           &   The label is technically correct but is too generic for the specific region, only describes the action, shape, or color without mentioning the kind of object, or is a reasonable description but not entirely intuitive or natural.\\ \midrule
% 1           &   The label is partially correct but contains wrong details, or correctly describes the majority of region that contains multiple objects or has inaccurate boundaries.\\ \midrule
% 0           &   The label does not describe the region.\\
% \bottomrule
% \end{tabular}}
% \end{table*}


\begin{figure*}
\centering
 \includegraphics[scale=0.4]{./figs/user_study_ui.pdf}
  \vspace{-7.em}
  \caption{User interface for the user study with a full instruction, definitions, and examples of each score.} 
  \label{fig:user_study_ui}
\end{figure*}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table*}[]
% \centering
% \caption{}
% \label{tab:all_score_thres}
% \resizebox{\textwidth}{!}{
% \setlength{\tabcolsep}{1pt}
% \begin{tabular}{lccccccccccccccccrrr}
% \toprule
% \multicolumn{1}{c}{\multirow{2}{*}{Dataset}} &
%   \multicolumn{8}{c}{IoU$_\text{st}$} &
%   \multicolumn{11}{c}{IoU$_\text{tt}$} \\ \cline{2-20} 
% \multicolumn{1}{c}{} &
%   0.00 &
%   0.05 &
%   0.10 &
%   0.15 &
%   0.20 &
%   0.25 &
%   0.30 &
%   0.35 &
%   0.00 &
%   0.10 &
%   0.20 &
%   0.30 &
%   0.40 &
%   0.50 &
%   0.60 &
%   0.70 &
%   0.80 &
%   0.90 &
%   1.00 \\ \hline
% Pascal context &
%   0.1962 &
%   0.1962 &
%   0.1962 &
%   0.1951 &
%   0.1593 &
%   0.0224 &
%   0.0000 &
%   0.0000 &
%   0.0814 &
%   0.0814 &
%   0.0818 &
%   0.0938 &
%   0.1119 &
%   0.1137 &
%   0.0976 &
%   0.0856 &
%   0.0646 &
%   0.0560 &
%   0.0528 &
% Pascal VOC 2012 &
%   0.2011 &
%   0.2012 &
%   0.2012 &
%   0.2075 &
%   0.2477 &
%   0.0425 &
%   0.0000 &
%   0.0000 &
%   0.1117 &
%   0.1118 &
%   0.1162 &
%   0.1599 &
%   0.2389 &
%   0.2731 &
%   0.2416 &
%   0.2145 &
%   0.1528 &
%   0.1202 &
%   0.1122 \\ \hline
% \end{tabular}}
% \end{table*}
% % ( maybe in limitation ) 
% % Our framework relies directly on existing generalist modules, hence bias from the modules also inherits. To illustrate, CLIP is trained using noise image-caption dataset, in which the caption is often bias toward the narrator. For example,



% \begin{figure*}
% \centering
%   \includegraphics[scale=0.88]{./figs/all_graph.pdf}
%   \caption{\textbf{Parameters Fine-tuning}}
%   \label{fig:para_tune}
% %   \vspace{-1.5em}
% \end{figure*}

% \begin{table*}[]
% \centering
% \caption{IOU score comparision with open-vocabulary segmentation baselines}
% \label{tab:baselines}
% % \resizebox{\textwidth}{!}{
% \begin{tabular}{cccc}
% \toprule
% \multicolumn Method & Segmentation label   & PAS-20 & PC-59  \\ \midrule
% Lseg \cite{li2022language}          & \cmark & 47.4   & -            \\
% SimBaseline \cite{xu2021simple}     & \cmark & 74.5   & -          \\
% ZegFormer \cite{ding2022decoupling} & \cmark & 80.7   & -            \\
% OpenSeg \cite{ghiasi2021open}       & \cmark & -      & 42.1       \\
% OVSeg \cite{liang2022open}          & \cmark & 94.5   & 55.7       \\
% GroupVit \cite{xu2022groupvit}      & \xmark & 52.3   & 22.4        \\
% Ours                        & \xmark  & 20.1  &   19.6   \\ \bottomrule
% \end{tabular}
% \end{table*}

\clearpage