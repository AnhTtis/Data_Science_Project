\input{_constants}
\rebuttal

\documentclass[10pt,twocolumn,letterpaper]{article}
\input{cvpr_header}
\myexternaldocument{_main}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
% Include other packages here, before hyperref.
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{booktabs}
% \usepackage[dvipsnames]{xcolor}

% Support for easy cross-referencing
% \usepackage[capitalize]{cleveref}
% \crefname{section}{Sec.}{Secs.}
% \Crefname{section}{Section}{Sections}
% \Crefname{table}{Table}{Tables}
% \crefname{table}{Tab.}{Tabs.}

\newcommand{\RONE}{{\color{ForestGreen} \textbf{R1}}}
\newcommand{\RTWO}{{\color{BrickRed} \textbf{R2}}}
\newcommand{\RTHREE}{{\color{BlueViolet} \textbf{R3}}}
\newcommand{\RFOUR}{{\color{Magenta} \textbf{R4}}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\begin{document}
%% TITLE
\title{\paperTitle}
\maketitle
\thispagestyle{empty}
\appendix
%%

We thank the reviewers (\RONE-oRR7, \RTWO-atoX, \RTHREE-MSCc, \RFOUR-3Ydc) for their valuable comments.
We are glad that the reviewers appreciate the ``novelty''(\RONE) and potential of our proposed task, which can inspire followup research
and that our method achieves ``plausible and reasonable'' (\RTHREE) results with ``nice qualitative results'' (\RFOUR) and ``quite accurate and detailed mask''(\RTHREE) despite the limited setup with no additional datasets or fine-tuning. 
Our paper is ``well-written and easy to follow'' (\RONE), but we will further improve the writing as kindly suggested by \RTWO.
% However, we acknowledge some issues with our exposition, as kindly mentioned in the review. We would like to address and clarify these issues here.
% We will refine our explanations, add details and equations, and re-position the figures. 

\textbf{\RONE: Unclear motivation of using DINO. DINO is known for segmenting foreground from background, not all pixels.
%Motivations of using DINO for Can DINO segment objects beyond foreground-background segmentation? What is the movtivation of the current segmentation method} 
}
Unsupervised features from DINO have been successfully used to segment not just foreground objects, but also other components that appear in the scene or the background, such as the sky [13]. DINO also provides competitive results on unsupervised part segmentation [1]. Our simple DINO-based clustering inspired by [1] requires no training, is easy to adopt, and offers good results. Note that our key contribution in attention masking is orthogonal to this choice and our framework supports any clustering model.

%Evidence from [1] shows that DINO's features can be used to segment not just foreground objects, but other components that appear in the background, such as sky.
%, as well as unsupervised part segmentation [1].
%We chose the simple clustering method from [1] as it requires no training, is easily adopted, and offers competitive performance.

\textbf{\RTHREE: How about using CLIP's features for clustering?
%for a more unified framework?} 
}
% Though, we agree that DINO is not the only model that works under our framework. 
In our earlier study, we found that clustering using CLIP was not as successful and observed that its features derived for semantic parts were not as discriminative as DINO's.
%Using CLIP's features for clustering was not as successful according to our earlier test, and we observe that CLIP's features of different objects are not as discriminative compared to those from DINO.
% While it seems that DINO may be only capable of rough segmentation due to low-resolution attention shown in [2]\footnote{\label{mainpaper}Refers to the numbered reference in our main paper}, [1]\footref{mainpaper} modifies DINO to extract features from non -overlapping patches to increase the feature resolution and achieves competitive segmentation performance.
%Our choice was inspired by existing findings that show DINO's ability to segment foreground objects \cite{caron2021emerging} or perform unsupervised segmentation with competitive performance \cite{amir2021deep}. Though, we agree that DINO is not the onlyyou model that works under our framework. Using CLIP's features for clustering was not as successful according to our earlier test, and we observe that its features are not as spatially discriminative compared to those from DINO.
%DINO is shown to have innate ability to segment foreground object \cite{caron2021emerging}.
%The unsupervised segmentation method we use offers competitive results with state-of-the-art supervised methods on part co-segmentation and co-segmentation tasks at the time \cite{amir2021deep}.
%We did not emphasize this motivation as the core techniques that make our pipeline work are the proposed self-attention masking and global subtraction.
%Our early exploration also suggests that DINO's features are more semantically discriminative compared to CLIP's.

% Our DINO-based unsupervised segmentation is derived from \cite{amir2021deep}, which offers competitive results with state-of-the-art supervised methods. 
% We did not emphasize this motivation as the segmentation step is derivative and not a part of our technical contributions.

% \textbf{\RTHREE: How about clustering with CLIP's features for a more unified framework?}
% Clustering with CLIP would yield significantly worse results.
% CLIP is not trained to preserve the local information in its features. Having good global features is sufficient for CLIP's objective of embedding whole images into its latent space.

\textbf{\RTHREE: Compared to $\text{IoU}_\text{st}$, why is our $\text{IoU}_\text{tt}$ higher on PAS VOC but lower on PAS Context?}
PAS Context contains more classes (59 vs 20) and some classes are quite similar such as `tree' and `potted plant.' These similar labels tend to produce more mismatched text-to-text reassignments (Sec 4.1.1). E.g., a tree segment can be predicted as `plant' and mistakenly reassigned to `potted plant' instead of `tree.' On the contrary, $\text{IoU}_\text{st}$ directly compares the visual feature to the ground-truth text, making it less prone to this problem.
% converting the feature to a predicted text

%than PAS VOC (20) may make it more difficult to map a segment to its corresponding class using the predicted text. For example, PAS Context has classes `tree' and `potted plant'. The predicted texts of these two classes tend to be closely similar, resulting in mismatching during text-to-text reassignment. 
% These two classes can have very similar predicted texts. It is possibly easier to distinguish between the visual embeddings than the text embeddings of the two classes.

% \textbf{\RONE: The results are not good enough to be practical}
% While we agree that our results may not be good enough for real-world applications, we believe it gives reasonable results for our limited setup with no additional dataset and fine-tuning. As R1 mentioned, future work can be built and improved on this idea. 


\textbf{\RONE: This work should also be compared with guided segmentation methods with less limited setups (use training/dataset) to suggest an upperbound.}
We will add a comparison with supervised open-vocabulary segmentation baselines based on [20] to our appendix. Unlike GroupVit [33] or our method, those baselines all require segmentation annotations. The IoU scores of Lseg [17], SimBaseline [34], ZegFormer [7], and OVSeg [20] on PAS VOC are 47.4, 74.5, 80.7, and 94.5, respectively. And the IoU scores of OpenSeg [10] and OVSeg [20] on PAS Context are 42.1 and 55.7. %There is still a gap in performance of our work and the supervised baselines.
% We present the comparison between $\text{IoU}$ scores of open-vocabualary methods and our $\text{IoU}_\text{st}$ scores in Table \ref{tab:baselines}.
% We will add it to our appendix.


\textbf{\RFOUR: The evaluations use too limited numbers of categories for open-vocabulary segmentation.}
Our paper follows the evaluation protocol in GroupVit [33], the closest existing work that requires no segmentation labels, and uses PAS Context-59 and PAS VOC.
% We used PAS Context with the highest number of classes (59) used in the open-vocab literature, following [7,17,33,34]\footref{mainpaper}.
Nonetheless, we tested our method on the 459 classes PAS Context and will add this to the appendix for future reference. Our $\text{IoU}_\text{st}$ and $\text{IoU}_\text{tt}$ are 11.3 and 9.0, which is close to OVSeg [20]'s $\text{IoU}_\text{st}$ of 12.4.
%We follow \cite{xu2022groupvit,li2022language,xu2021simple,ding2022decoupling} and use the version of PAS Context that contain 59 classes. Here we include the results on the full version with 459 classes, and we will add this to our appendix for future reference. Our $\text{IoU}_\text{st}$ and $\text{IoU}_\text{tt}$ are XX and XX respectively.

% We also have the results for PAS Context 459 (the full version with 459 classes). We did not include this in the main paper because the dataset is not prevalently used in existing open-vocabualry segmentation work due to the excessive number of classes.
% % The big number of classes makes this dataset very challenging. For example a segment `wooden floor' may be mapped to `wood' or `floor'. 
% We include the results here and will consider adding them to our appendix for future reference.



\textbf{\RONE: Constrain text query set to match the guidance level with GroupVit [33].}
Our $\text{IoU}_\text{st}$ and GroupVit's IoU are computed in the same way: finding the best match for visual embedding of each segment among text embeddings of class labels.
Thus, they have the same level of guidance.

\textbf{\RTHREE: Eq. 2 and Fig. 4 are inconsistent.} Thank you. Fig. 4 is indeed inaccurate, while Eq. 2 is correct. We will fix this. 
%We thank R3 for kindly pointing out this mistake. Figure 4 is mislabeled. We will correct it accordingly.

\textbf{\RTHREE: How is an attention mask applied?}
We flatten the object mask, add one element for the global token, and use this mask for all tokens (via tiling). We will revise the text.
% As R3 stated, The object mask's shape is ($H,W$), but the mask we used in self-attention is ($H \times W+1,H\times W+1$). We first reshape the object mask to ($H \times W$), then prepend one extra element (1) for the global token. Lastly, the mask is tile $H \times W+1$ times into the ($H \times W+1,H\times W+1$) mask.
% We will rewrite lines 371-377 of our paper to be more detailed.

\textbf{\RTWO: There are many ``mathematically sophisticated'' metrics, which hurt reproducibility and seem specific to the proposed method.
%that look `which may hurt reproducibility and and  Many complicated metrics may hurt reproducibility and seem specific to our proposed technique.
%The proposed evaluation metrics look complicated and many, specific only to our proposed technique, and challenging to reproduce.
}
Zero-guidance segmentation is a novel and much trickier problem to evaluate (see line 154), which requires multifaceted metrics, not just a single IOU. Our metrics are not specific to our method and were designed with the simple idea of label reassignment (Sec 4.1), which involves straightforward equations (Eq. 6-9), can be implemented in 3 lines of code, and allows standard metrics to be used afterward. We will try to simplify the explanations and will release the code for reproducibility.

%The proposed metrics are designed to evaluate semantic segmentation with natural language labels and are not limited to our proposed technique. We agree that there are several metrics; however, each metric serves a different and specific purpose, i.e., $\text{IoU}_\text{tt}$ measures segmentation performance in which label assignment is based on the generated texts, whereas $\text{IoU}_\text{st}$ completely ignores the texts and OVCS solely evaluates the text generation. As a result, they provide comprehensive detail about the performance from various perspectives. Additionally, the metrics are simple, and the code will be publicly released to facilitate reproducibility.

% The proposed evaluation metrics are designed to evaluate general open-vocabulary semantic segmentation systems that could output natural language labels and are not limited to our proposed technique. The proposed metrics are mostly derived from conventional metrics and require only a slight modification. Each metric serves a different and specific purpose; as a result, they are simple to interpret and provide a fine detail on the performance from a variety of perspectives. Plus, the metrics need only a few lines of code and will be publicly available.
% to enable reproducibility
% example 
% no derive , diff label, contribution , many is good  

\textbf{\RFOUR: No metrics to evaluate ``quality'' and diversity of generated texts.}
Qualities such as diversity and naturalness are interesting aspects that can be further measured using ideas from the NLP community, e.g. Alihosseini et al., 2019, and will be added to the discussion. Our method uses prior work, ZeroCAP, for text generation and these metrics will be mostly affected by its performance. Our OVCS metric is designed to test our contribution, independent of ZeroCAP.

%We will discuss 
%and complementary indicators.
%While our OCVS metric measures the quality of texts in terms of whether they match the ground-truth texts, we agree that the diversity and other qualities such as whether the texts sound natural and grammatical are useful and complementary indicators.
%We believe that such quality is subjective and should incorporate human judgment into the measurement. One relevant and simple idea for measuring diversity, used in the prior work, ZeroCAP, is to count the distinct vocabulary. 
%We appreciate the suggestion.

\textbf{\RTWO: Unclear what `zero' and `guidance' mean.}
Prior work requires users to pre-specify the classes of interest or the text labels of the existing objects in the input image to ``guide'' the model to find the corresponding segments [7,10,17,20,33,34]. Our work requires no such guidance.

%natural language indicators that complement our primary metrics. 
% Plus, we believe that relevant metrics for measuring both quality and diversity (Zhu et al.,2018; Alihosseini et al.,2019) already exist and will consider incorporating them into our work. We appreciate the suggestion.



% While our OCVS metric measures the ``quality'' of texts in terms of whether they match the ground-truth texts, we agree that the diversity and other qualities such as whether the texts sound natural and grammatical are interesting natural language indicators that complement our primary metrics. Plus, we believe that relevant metrics for measuring both quality and diversity [2]\footnote{Zhu, Yaoming, et al. "Texygen: A benchmarking platform for text generation models." The 41st international ACM SIGIR conference on research \& development in information retrieval. 2018.} [3]\footnote{Alihosseini, Danial, Ehsan Montahaei, and Mahdieh Soleymani Baghshah. "Jointly measuring diversity and quality in text generation models." Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation. 2019.}  already exist and will consider incorporating them into our work. We appreciate the suggestion.

% We agree that quality, such as naturalness, and diversity of the generated texts are interesting indicators for pure language aspect that would compliment our to proposed metrics that already cover the key components of the task.

%  and are not implied by the proposed metrics. We suggest that the naturalness of texts are subjective, hence requires human judgement to evaluate. 
% However, we believe that our proposed metrics already cover the key components of the task.

% Open-Vocab Class Similarity (OVCS) (section 4.4) is specifically designed to measure exclusively the quality of the generated text labels, where our proposed technique outperforms the baseline. We appreciate the suggestion for the diversity measurement; that would be an interesting indicator. However, we believe that the diversity is not a primary concern for the performance of the task. 


% we agree, ... that ZeroCap is not our technical contribution. zoom, the grad. quality to user study. NLP on top diversity on experiment. key component is already .... quality such as naturalness of words.

\textbf{\RTHREE: How to control the granularity of the text labels?} 
We have not explored this and delegate the text generation to ZeroCAP, but it is an interesting direction. One plausible solution is to vary the multiplicative factor of the ZeroCAP's \emph{end-token} probability (currently set to 1.1 by default), which controls the length / descriptiveness of the output texts.
%adjust the scaling factor of the end token's probability during text generation.
% We have not explored this and delegate the text generation to prior work, ZeroCAP, but it is an interesting direction. One simple solution is to utilize a Hypernymy or Hyponymy classifier's confidence score (Wang et al.,KEML, 2021)  as an additional objective to ZeroCAP's to guide the model to the preferred lexical granularity. 
% We have not explored this and delegate the text generation to prior work, ZeroCAP, but it is an interesting direction. One simple solution is to ..
%We do not control the abstract granularity of text labels in our work. 
% We suggest that the abstract granularity of text could be controlled by incorporating an abstract preference as an additional objective to ZeroCAP's objective function. 
% We suggest that the abstract granularity of text could be controlled by incorporating an abstract preference as an additional objective to ZeroCAP's objective function. 

% We have not explored this and delegate the text generation to prior work, ZeroCAP, but it is an interesting direction. One simple solution is to integrate lexical relations such as Hypernymy and Hyponymy, which could be distilled from a language model, as an additional objective to ZeroCAP's. 


% We have not explored this and delegate the text generation to prior work, ZeroCAP, but it is an interesting direction. One simple solution is to utilize a Hypernymy or Hyponymy classifier's confidence score [4]\footnote{Wang, Chengyu, et al. "KEML: A knowledge-enriched meta-learning framework for lexical relation classification." Proceedings of the AAAI conference on artificial intelligence. Vol. 35. No. 15. 2021.} as an additional objective to ZeroCAP's to guide the model to the preferred lexical granularity. 


 % \footnote{Wang, Chengyu, et al. "KEML: A knowledge-enriched meta-learning framework for lexical relation classification." Proceedings of the AAAI conference on artificial intelligence. Vol. 35. No. 15. 2021.} 

 
% ; rather, we let the model output natural language labels that best describe the corresponding segments according to ZeroCap's objective function. H

% he part-whole granularity of text labels could be adjusted in the merging step through the merging threshold $\tau_{merge}$. However, we do not directly control the abstract granularity; rather, we let the model output natural language labels that best describe the corresponding segments.
% % beamsearch ..... cost 


\textbf{\RTHREE: What happens if there are multiple instances of the same class? Does it do semantic or instance segmentation?}
%Whether the model is doing semantic segmentation for the images containing multiple instances of the same category?}
Our method does not differentiate different instances of the same class. E.g., all face regions will have similar visual features and will be treated as a single semantic ``face'' segment regardless of the number of people or faces in the image---our work focuses on semantic segmentation. 
%Regardless of their numbers, our framework will treat the same object categories, the same as semantic segments. 


% Instance segmentation is beyond our scope; however, we suggest that the proposed global subtraction would also be applicable and beneficial to instance-level segmentation.


% Our framework will give semantic segmentation results since the framework is based on CLIP, which is a visual-semantic alignment module, with no instance level being considered. Additionally, the proposed merging approach would automatically merge semantically similar segments. Instance segmentation is beyond the scope of our work. However, instance-level segments would also profit from the proposed global subtraction module. Thus, we suggest a straightforward idea to enable instance segmentation in our work: simply change the segment discovery module from DINO, which provides semantic-level segments, to the modules that could provide instance-level segments.

% Our model will treat  instances as same semantic. hand waive.

\textbf{\RTHREE: Computation of crop-and-mask vs our proposed masking? }
The average per-segment embedding time (RTX 3090) are 31.8ms for crop-and-mask and 38.4ms for ours. 
% Crop-and-mask computes masking once on the input image while attention masking and global subtraction are computed 4 times in the last four layers.
% % The difference between the two methods is that crop-and-mask applies mask one times to the input image while our method performs these steps in the last four layers: mask the attention matrix, compute cosine similarity between mask and unmask attention matrix, and perform global subtraction.
% Both methods need to make a pass through CLIP for each segment.
% % The computational complexity of crop-and-mask and our proposed attention masking are $\text{O}(\text{img}\_\text{size})$ and $\text{O}(12*\text{attn}\_\text{size})$, respectively. 



% Crop-and-mask apply the mask one time on input RGB while our masking apply the mask on attention matrices of the 4 last layers. 
% Therefore, the computational complexity are O(img_size) and O(4*attention_size), respective.



% The masking is only elemental-wise multiplication, and both method need to run inference for every segment.

% The difference is insignificant.
% since they yield similar computational complexity.


%The term `zero-guidance' means refers to the novel proposed segmentation task that requires no instruction or example of what it should segment, contrary to the existing setups that require segmentation mask examples or text queries to `guide' the segmentation process.

% \textbf{\RTWO: The writing needs improvement.}
% While we are sorry to hear that R2 finds our writing unclear, we are glad R1 thinks our paper is ``well-written and easy to follow''. We thank all comments and will further refine our manuscript.

% \begin{table}[]
% \centering
% \caption{IOU score comparision with open-vocabulary segmentation baselines}
% \label{tab:baselines}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{lccc}
% \toprule
% Method & Segmentation label   & PAS-20 & PC-59  \\ \midrule
% Lseg \cite{li2022language}          & \cmark & 47.4   & -            \\
% SimBaseline \cite{xu2021simple}     & \cmark & 74.5   & -          \\
% ZegFormer \cite{ding2022decoupling} & \cmark & 80.7   & -            \\
% OpenSeg \cite{ghiasi2021open}       & \cmark & -      & 42.1       \\
% OVSeg \cite{liang2022open}          & \cmark & 94.5   & 55.7       \\
% % GroupVit \cite{xu2022groupvit}      & \xmark & 52.3   & 22.4        \\
% Ours ($\text{IoU}_\text{st}$)                      & \xmark  & 20.1  &   19.6   \\ \bottomrule
% \end{tabular}}
% \end{table}

% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{11_references}
% }

\end{document}
