
\section{Evaluation Metrics}


Zero-Shot Semantic Segmentation approaches [] have two main steps in their evaluation process: 1) aligning their prediction results with test set's class labels 2) Following a standard protocol for semantic segmentation, mean intersection over union (mIoU) [] is then computed. 
However, in open-vocabulary semantic segmentation with text generation (OST), unlike conventional approaches, each image segment has already got its corresponding open-vocab class through inferencing. Moreover, The predicted open-vocab class is not necessary to be well aligned with the predefined classes. For instance, given an image segment that contains a car, our method may output text of car brands or car models, like `camry' or `mercedes', instead. Furthermore, our approach can discover objects not limited to the annotation in test sets. That is, the output is not dependent on the annotation of the dataset, which is more practical in real-world scenarios. To the best of our knowledge, our approach is the first open-vocabulary segmentation with text generation (OST), and there is no suitable evaluation framework for the task. Using the conventional evaluation approaches \cite{} would simply disregard all of the semantic information and mistakenly penalize unlabeled objects; therefore, the system's capability could not be precisely captured.

To address this issue, we further purpose a new systematic evaluation metrics for open-vocabulary semantic segmentation with text generation by taking into account both semantic and visual perspectives.

\subsection{Image-Text IoU ($\text{IoU}_{i-t}$)}
Due to the success of visual-language models [], current approaches [] often use image to text zero-shot transfer to assign a class to an image segment. To elaborate, they first compute image embedding of image segments and text embedding of all classes in a test set. Each image segment is assigned to the class that maximizes its embedding cosine similarity.
% \begin{equation} 
% pred\_class_{i-t}( m_i) = argmax_{t_j \epsilon V}\ similarity(E_v( m_i),E_t( t_j) ) 
% \end{equation}

\begin{equation} 
pred\_class_{i-t}( m_i) =  \underset{t_j \epsilon V}{\text{argmax}}\  similarity(E_v( m_i),E_t( t_j) ) 
\end{equation}


% : 1) the quality of output segmentation masks and 2) how well the text semantically represent the corresponding regions.

\begin{figure}
\centering
  \includegraphics[scale=0.5]{./figs/text_sim.png}
  \caption{empirical experiment on text similarity using CLIP's text encoder and Sentence-Bert}
  \label{fig:text_sim}
  \vspace{-1.5em}
\end{figure}

\subsection{Text-Text IoU ($\text{IoU}_{t-t}$) } To capture both semantic and localization performance, we use text to text zero-shot transfer to assign a class to a region. To elaborate, we compute text embedding cosine similarity between predicted open-vocab text, which is already assigned to a region, and all classes in a test set. 
% \begin{equation} 
% pred\_class_{t-t}( m_i) = argmax_{t_j \epsilon V}\ similarity(E_t( t_i),E_t( t_j) )
% \end{equation}
\begin{equation} 
 pred\_class_{t-t}( m_i) = \underset{t_j \epsilon V}{\text{argmax}}\  similarity(E_t( t_i),E_t( t_j) )
 \end{equation}
Since OST could discover unlabeled objects, we propose to use similarity thresholding to filter such unlabeled objects. We use Sentence-BERT [] as our text encoder as they achieve decent quantitative score for text embedding similarity and also provide strong evidence on user study []. Base on their user study, since our text is simply a single noun or noun phrase, we use similarity threshold of 0.5 as default. 

\subsection{Open Vocab Class Semantic Score (OVCS) } 
To measure solely on the text results, we measure how semantically close the predicted open-vocab texts and the ground truth class are. That is, we directly feed ground truth segmentation masks, instead of predicted regions, to the model and then measure the similarity between the output text and the corresponding ground truth class embedding
\begin{equation} 
OVCS(m_{gt_i}) = similarity(E_t( t_{pred_i}),E_t( t_{gt_i} ) )
\end{equation}


% \subsection{Open Vocab Semantic Visual Score (OVSV) } 
% We also propose to quantify how well the predicted open-vocab texts semantically represent their corresponding regions. That is, we compute the similarity between the predicted text embedding and visual embedding of the corresponding region. Note that we also directly use ground truth mask as an input as in OVCS. 
% \begin{equation} 
% OVSV(m_{gt_i}) = similarity(E_t( t_{pred_i}),E_v( m_{gt}_i ) )
% \end{equation}
% Open-vocabulary segmentation with text generation needs to be evaluate on two main aspects: 1) the quality of output segmentation masks, and 2) how well the output texts represent the regions from segmentation mask.
% Normally, CLIP score can be used to evaluate how close a text and an image are, but our method relied on CLIP heavily, so we also propose metrics that are not completely dependent on CLIP for fairness 
% % avoid calculating score with CLIP in our evaluation process for fairness.

% As the text our method generate are free language that can be influenced by factors like popular culture and inherent information from CLIP's image-caption training set, the output texts are often not in alignment with predefined classes in segmentation dataset. For instance, given a segment that contains a car, our method may output brand of car models or car brands like `camry' or `mercedes' instead. To align these free language output text to the predefined text classes, we use Sentence-BERT \cite{reimers-2019-sentence-bert} to embed both sets of texts and calculate the similarity between them.
