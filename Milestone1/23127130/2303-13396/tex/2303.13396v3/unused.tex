% \begin{table*}[]
% \centering
% \caption{Ablation:pascal context 59. We compare results of variation of our Attention Masking (AM) method with and without Globa subtraction (GS) and Semantics Merging (SM), as well as Crop and Mask baseline}
% \label{tab:ablation}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lcccc@{\extracolsep{6pt}}cccc}
% \toprule
%                             & \multicolumn{4}{c}{\textbf{Pascal Context 59}}                               & \multicolumn{4}{c}{\textbf{Pascal Voc 2012}}                                  \\ \cline{2-5} \cline{6-9} 
% \multicolumn{1}{c}{Method} & \rule{0pt}{2ex}$\text{IoU}_{i-t}$ & $\text{IoU}_{t-t}$ & Text-score & $\text{Recall}_{OV}$ & $\text{IoU}_{i-t}$ & $\text{IoU}_{t-t}$ & $\text{Recall}_{OV}$ & Text-score \\ \midrule
% Crop and Mask                 & 11.7              & 5.6               & -             & x          & 9.2               & 15.4               & x             & x          \\
% Crop and Mask + SM            & 12.1              & 5.6                & 16.9             & x          & 11.6               & 17.1               & x             & x          \\
% AM + SM                     & 14.5               & 4.9                & 0.1             & x          & 16.8                 & 18.2                & x             & x          \\
% AM + GS                     & 16.4               & 7.5               & -             & x          & 20.6               & 23.5              & x             & x          \\
% AM + GS + SM (Ours)         & 17.5               & 7.8                  & 19.0             & x          & 22.8                  & 26.2                  & x             & x          \\ \bottomrule
% \end{tabular}}
% \end{table*}


% \begin{table}[]
% \centering
% \caption{Ablation:pascal context 59. We compare results of variation of our Attention Masking (AM) method with and without Globa subtraction (GS) and Semantics Merging (SM), as well as Crop and Mask baseline}
% \label{tab:ablation_context59}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccc}
% \toprule
% \multicolumn{1}{c|}{method}                      & $\text{IoU}_{i-t}$& $\text{IoU}_{t-t}$ & $Recall_{OV}$ & Text-score \\ \midrule
% Crop and mask                                    &            13.72   &       7.9        &        x         &     x             \\
% AM                                     &          12.9      &    6.8           &                 &                  \\
% AM + GS                &          19.8      &     10.5          &                 &                  \\
% AM + GS + SM &           x      &     x          &                 &                  \\ \bottomrule
% \end{tabular}}
% \end{table}

% \begin{table*}[]
% \centering
% \caption{Ablation:pascal voc 2012}
% \label{tab:ablation_2012}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{l|cccc}
% \toprule
% \multicolumn{1}{c|}{method}                      & $\text{IoU}_{i-t}$& $\text{IoU}_{t-t}$ & CLIP text-score & SBERT text-score \\ \midrule
% Crop and mask  + merge                         &            19.9    &       20.1        &        x         &     x             \\
% Attention mask  + merge                          &          20.0      &    17.4           &                 &                  \\
% Attention mask + global subtraction                &         26.5      &    30.6          &                 &                  \\
% Attention mask + global subtraction + merge (Ours) &         x      &     x          &                 &                  \\ \bottomrule
% \end{tabular}}
% \end{table*}

% \begin{table}[]
% \centering
% % '../results_eval/context_val_i-t_pascal_context_59/i-t_c0.10_pascal_context_59'
% % '../results_eval/context_val_t-t_pascal_context_59/t-t_t0.50_pascal_context_59'
% % iou_D0.1_i-t_U
% \caption{Ablation}
% \label{tab:globalsub}
% % \resizebox{\columnwidth}{!}{
% \begin{tabular}{c|cc}
% \hline
% \multicolumn{1}{l|}{global subtraction sd} & $\text{IoU}_{i-t}$ & $\text{IoU}_{t-t}$ \\ \hline
% \multicolumn{1}{l|}{no global subtraction} &      12.0          &  3.8             \\
% 1                                          &      12.4         &    4.1           \\
% 2                                          &      12.1         &    5.0           \\
% 5                                          &      17.2         &    6.6           \\
% 10                                          &      18.0          &   7.1            \\
% 15                                          &      18.0          &    7.1           \\
% 30                                          &      17.5          &    6.3           \\ 
% 50                                          &      17.1          &    6.2           \\ \hline
% \end{tabular}
% \end{table}


% \subsection{Comparison to Open-vocabulary Baseline}
% We evaluate segmentation performance in conventional open-vocabulary segmentation setting. 
% We report our results with text-text IoU score as described in Section \ref{}.
% % We match text labels predicted by our method to a set of texts of classes predefined by each dataset and calculate the IoU of our predicted region and ground truth of matched class.

% We present IOU scores of our method and baselines in Table \ref{tab:baselines}. Our method is the only method that output text labels of each segment.

% \begin{table*}[]
% \centering
% \caption{IOU score comparision with open-vocabulary segmentation baselines}
% \label{tab:baselines}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{l|ccc|cccc}
% \toprule
% \multicolumn{1}{c|}{Method} & Pretrained network & Training dataset               & Training required     & PAS-20 & PC-59 & coco-stuff \\ \midrule
% Lseg \cite{li2022language}                       & CLIP               & image-segment                & \cmark & 47.4   & -     & -          \\
% SimBaseline \cite{xu2021simple}                & CLIP               & image-segment                & \cmark & 74.5   & -     & -          \\
% ZegFormer \cite{ding2022decoupling}                  & CLIP               & image-segment                & \cmark & 80.7   & -     & -          \\
% OpenSeg \cite{ghiasi2021open}                    & -                  & image-caption, image-segment & \cmark & -      & 42.1  & 36.1       \\
% GroupVit \cite{xu2022groupvit}                   & -                  & image-caption                & \cmark & 52.3   & 22.4  & -          \\
% OVSeg \cite{liang2022open}                      & CLIP, MaskFormer   & image-caption, image-segment & \cmark & 94.5   & 55.7  & -          \\
% Ours                        & CLIP, DINO, GPT-2         & -                            & \xmark &        &       &            \\ \bottomrule
% \end{tabular}}
% \end{table*}



%Solving this 
%the segment `building' from the ground truth is correctly predicted as two segments, `pub' and `roof'. The text `roof' is mapped to the ground truth class `ceiling', and `pub' finds no match. As a result, $\text{IoU}_\text{tt}$ for class ‘building’ is zero. 
%Using label matching algorithms based on SBERT or CLIP for evaluation still 
%for matching texts during evaluation 
%Our method outputs free text labels, which can be tricky to matched with the correct ground truth class. 
% However, there are different ways to describe a segment, and sometimes, even thought the predicted labels are correct, they can not be aligned with the ground truth labels.
%In Figure \ref{fig:result_analysis}, the segment `building' from the ground truth is correctly predicted as two segments, `pub' and `roof'. The text `roof' is mapped to the ground truth class `ceiling', and `pub' finds no match. As a result, $\text{IoU}_\text{tt}$ for class ‘building’ is zero. 
% (plus the penalty for incorrectly predicted `ceiling'). 
%For image-text mapping, while the segment embedding of `roof' is reassigned to the text `building', `pub' is matched to the class `sign'.
% , which is not wrong but still hurts $\text{IoU}_\text{st}$ score. 



\textbf{Mismatched text labels during evaluation.} Evaluating an algorithm under this new setup is still challenging, despite using label reassignment.
%Our label reassignment using SBERT or CLIP in our evaluation can still miss correct predictions . 
For example, in Figure \ref{fig:result_analysis}, our algorithm breaks down the `building' ground-truth segment into `roof' and `pub', which are correct. But during SBERT label reassignment, `roof' is mapped to `ceiling' and `pub' finds no matches. As a result, our algorithm obtains zero $\text{IoU}_\text{tt}$ for `building'. CLIP label reassignment assigns `pub' to `sign', which is still technically correct but also not counted toward our $\text{IoU}_\text{st}$ score for `building'.
Another problematic class is `person', which consists of small semantic parts, such as `face', `hair', `shirt'. These parts can be quite distinct in both SBERT's and CLIP's spaces, so they may not be mapped to `person' during evaluation.
%. Examples of these are shown in Figure \ref{fig:failure_case}. 
While our segments in Figure \ref{fig:failure_case} are qualitatively reasonable, they receive zero $\text{IoU}_\text{tt}$ for class `person'. To overcome this evaluation challenge, a new kind of embedding that understands the relationships or hierarchy of object parts may be required. 