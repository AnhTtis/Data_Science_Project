\section*{Acknowledgements} 
% \textbf{Acknowledgements:}
This research was supported by PTT public company limited and SCB public company limited. 

% \section{Limitations}
% Our work heavily relies on pretrained generalist modules:  ViT-based CLIP and DINO. In particular, our proposed a method to guide CLIP by directly masking on attention space, hence the resolution of our segments is restricted by the attention size of the pretrained generalists. Also, our framework might inherits bias from the pretrained generalist which could impact the performance of segmentation. For examples, .....
% and since our modules give free-language class output, the output could appear impolitic and offensive discussed in [potential Negative Societal impact]


% Our work introduces a new problem setup, open-vocabulary segmentation with text generation. 
% While our text generation has no additional training and can distill knowledge from CLIP directly, it can output text in free language that may describe the object in many different ways, like object name, appearance, how it is used, etc. However, the downside is that there is no control of what it output, which may make it difficult to be put to use in some settings.  
% - patch resolution 

% As the work is heavily dependent on CLIP, whose dataset is a large image-caption collection aggregated from all over the internet, it has some biases that may be offensive.


% \section{Conclusion}
% \label{sec:conclusion}
% We presented a novel problem setup that segments an input image and generate text labels all segment without any guidance. 
% Together with the new setup, we also introduce the first baseline that solve this task without fine-tuning CLIP or using any additional dataset.
% Our novelty lies in attention masking and global subtraction techniques that balance global context and local context to retain the right amount of both information in CLIP's embedding of a segment.