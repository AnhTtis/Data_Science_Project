\begin{abstract}
CLIP has enabled new and exciting joint vision-language applications, one of which is open-vocabulary segmentation, which can locate any segment given an arbitrary text query. 
In our research, we ask whether it is possible to discover semantic segments without any user guidance in the form of text queries or predefined classes, and label them using natural language automatically?
%Our paper asks the next question: can we discover semantic segments without user guidance in the form of text queries or predefined classes and label them in natural language all automatically?
%Unlike traditional segmentation, the user is not restricted to predefined classes
%is far beyond the traditional segmentation with predefined classes.
%CLIP has enabled some new joint vision-language applications, one of them being open-vocabulary segmentation. 
%The task locates segments of objects with guidance from input text queries, which must be provided by the user.
% The text queries can be either predefined object classes from a dataset or arbitrary classes inputted by the user. 
We propose a novel problem \textbf{zero-guidance segmentation} and the first baseline that leverages two pre-trained generalist models, DINO and CLIP, to solve this problem without any fine-tuning or segmentation dataset.
%In this paper, we propose a novel problem setup: \textbf{zero-guidance segmentation}. Given an input image, the goal is to segment the image and give text labels for all segments without any prompting of what the segments should be. 
%Along with the new setup, we present the first baseline that tackles this problem using only pretrained networks without any fine-tuning or segmentation dataset.
The general idea is to first segment an image into small over-segments, encode them into CLIP's visual-language space, translate them into text labels, and merge semantically similar segments together. The key challenge, however, is how to encode a visual segment into a segment-specific embedding that balances global and local context information, both useful for recognition.
Our main contribution is a novel attention-masking technique that balances the two contexts by analyzing the attention layers inside CLIP. We also introduce several metrics for the evaluation of this new task. 
% Our method can label segments using only CLIP's innate knowledge. These text labels can describe varied aspects of the segments, such as object names, appearances, places.
With CLIP's innate knowledge, our method can precisely locate the Mona Lisa painting among a museum crowd (Figure \ref{fig:teasor}).
%encodes a segment into CLIP's space by balancing global and local context. 
%Our method can label segments using only CLIP's innate knowledge.
%We also introduce several metrics for the evaluation of this new task.
Project page: \emph{\small \url{https://zero-guide-seg.github.io/}}.
\end{abstract}