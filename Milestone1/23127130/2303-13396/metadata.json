{
    "arxiv_id": "2303.13396",
    "paper_title": "Zero-guidance Segmentation Using Zero Segment Labels",
    "authors": [
        "Pitchaporn Rewatbowornwong",
        "Nattanat Chatthee",
        "Ekapol Chuangsuwanich",
        "Supasorn Suwajanakorn"
    ],
    "submission_date": "2023-03-23",
    "revised_dates": [
        "2023-03-27"
    ],
    "latest_version": 2,
    "categories": [
        "cs.CV"
    ],
    "abstract": "CLIP has enabled new and exciting joint vision-language applications, one of which is open-vocabulary segmentation, which can locate any segment given an arbitrary text query. In our research, we ask whether it is possible to discover semantic segments without any user guidance in the form of text queries or predefined classes, and label them using natural language automatically? We propose a novel problem zero-guidance segmentation and the first baseline that leverages two pre-trained generalist models, DINO and CLIP, to solve this problem without any fine-tuning or segmentation dataset. The general idea is to first segment an image into small over-segments, encode them into CLIP's visual-language space, translate them into text labels, and merge semantically similar segments together. The key challenge, however, is how to encode a visual segment into a segment-specific embedding that balances global and local context information, both useful for recognition. Our main contribution is a novel attention-masking technique that balances the two contexts by analyzing the attention layers inside CLIP. We also introduce several metrics for the evaluation of this new task. With CLIP's innate knowledge, our method can precisely locate the Mona Lisa painting among a museum crowd. Project page: https://zero-guide-seg.github.io/.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13396v1",
        "http://arxiv.org/pdf/2303.13396v2"
    ],
    "publication_venue": null
}