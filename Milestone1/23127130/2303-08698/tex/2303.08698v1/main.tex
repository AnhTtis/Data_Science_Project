% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}
%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{tikz}
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{diagbox}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{bbding}
\usepackage{booktabs}
\usepackage{utfsym}
\usepackage{makecell}
\usepackage{bm}
\usepackage[accsupp]{axessibility} 
% \usepackage{epsfig,epstopdf,subfigure}
% \usepackage{pifont}
% % \usepackage{multirow}
% \usepackage{tabularx,arydshln}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage{multirow}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}



% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}

\usepackage{amsthm}
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}


\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{3847} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}
\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Bi-directional Distribution Alignment for Transductive Zero-Shot Learning}

\author{Zhicai Wang$^1$, Yanbin Hao$^{1*}$, Tingting Mu$^2$, Ouxiang Li$^1$, Shuo Wang$^1$, Xiangnan He$^{1}$\thanks{Yanbin Hao and Xiangnan He are both the corresponding authors.}\\
$^1$University of Science and Technology of China, $^2$The University of Manchester\\
{\tt\small wangzhic@mail.ustc.edu.cn, haoyanbin@hotmail.com, tingtingmu@manchester.ac.uk,}\\
{\tt\small  lioox@mail.ustc.edu.cn, \{shuowang.hfut,xiangnanhe\}@gmail.com}
}
% \author{Zhicai Wang\\
% University of Science and Technology of China\\
% {\tt\small wangzhic@mail.ustc.edu.cn}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Yanbin Hao\\
% University of Science and Technology of China\\
% \and
% Tingting Mu\\
% University of Manchester\\
% \and
% Ouxiang Li\\
% University of Science and Technology of China\\
% \and 
% Shuo Wang\\
% University of Science and Technology of China\\
% \and
% Xiangnan He\\
% University of Science and Technology of China\\
% }
\maketitle
%%%%%%%%% ABSTRACT
\begin{abstract}
 
    
 It is well-known that zero-shot learning (ZSL) can suffer severely from the problem of domain shift, where the true and learned data distributions for the unseen classes do not match. Although transductive ZSL (TZSL) attempts to improve this by allowing the use of unlabelled examples from the unseen classes, there is still a high level of distribution shift. We propose a novel TZSL model (named as Bi-VAEGAN), which largely improves the shift by a strengthened distribution alignment between the visual and auxiliary spaces. The key proposal of the model design includes (1) a bi-directional distribution alignment, (2) a simple but effective $L_2$-norm based feature normalization approach, and (3) a more sophisticated unseen class prior estimation approach. In benchmark evaluation using four datasets,  Bi-VAEGAN achieves the new state of the arts under both the standard and generalized TZSL settings. Code could be found at \href{https://github.com/Zhicaiwww/Bi-VAEGAN}{https://github.com/Zhicaiwww/Bi-VAEGAN}.
 
  % Zero-shot learning (ZSL) is a cross-modal knowledge transfer task, which aims to classify the unseen classes with training on seen classes and some auxiliary information. It is challenging since the domain shift generally exists because of the untouchable unseen data assumption.  To alleviate the domain shift problem introduced by untouchable unseen data, transductive ZSL (TZSL) concedes that unseen data is accessible in the training. Generative models have shown their superiority in the TZSL since their distribution modeling nature.  However, how to conditionally align the cross-modal distribution using the unlabeled target data is still an open question.

    % In this work, we propose a competitive bi-directional cross-modal generative framework, Bi-VAEGAN, for the challenging TZSL. We find a transductive regressor, the cycle-consistent module that maps visual feature back to auxiliary space, is playing an important role in the visual feature alignment.
    % % and help with a more compact and realistic generation. 
    % We also introduce the free-lunch feature $L_2$ normalization and attribute perturbation for generative distribution alignment. 
    % The experiments show that our Bi-VAEGAN opens a significant performance gap with other generative approaches under the given class frequency prior assumption.  Meanwhile, we put a discussion on how the class prior affects the conditional alignment for the unseen data and propose to utilize the cluster property to estimate the class frequency given the unknown prior assumptions. 

\vspace{-0.2cm}
\end{abstract}
\vspace{-0.3cm}

%%%%%%%%% BODY TEXT

% \section{INTRODUCTION}

% \begin{figure}[thp]
% \begin{center}
%     \includegraphics[width = 1\linewidth]{figures/visualization/abstract.pdf}
% \end{center}    
% \caption[]{Bidirectional distribution aligned transductive zero-shot method. Our approach targets better knowledge transfer from the open-world seen data to unseen data using the auxiliary (e.g., attribute) information. The accessible unseen data is unlabeled and thus only the overall distribution is known. Our model successfully aligns the real unseen conditional distribution with the synthesized in visual space and meanwhile aligned the conditional distribution in the auxiliary space by the regressor. The bottom middle figure shows our aligned unseen visual space of AwA2 and its top right shows the aligned attribute space with regressed real/synthesized unseen feature and conditioned attribute\label{fig.1}}
% \end{figure}

% The challenge of zero-shot learning (ZSL) is to transfer knowledge from readily accessible seen classes to those novel unseen classes \cite{norouzi2013zero,zhang2016zero,xian2018zero}. Auxiliary data, such as attribute information that is human-annoated discrete vectors\cite{wah2011caltech}, text description embedded vectors\cite{reed2016learning} and et-al, is typically provided to bridge the knowledge transfer from seen classes to the unseen. Conventional ZSL is also known as inductive ZSL that no unseen data is provided, and new categories are classified by relying only on the auxiliary data.
% It usually suffer from the poor knowledge transferability because of the domain shift problem. 
% The transferability is also extremely depending on the quality of auxiliary information. In fact, it turns out to be simple to gather the vast amounts of unlabeled data in the real world, which could alleviate the domain shift problem to a great extent. As a workaround to inductive ZSL, Transducitve ZSL (TZSL) is proposed to circumvent the unconstraited unseen visual domain issue by assuming the accessibility of unlabeled unseen test data. 

% Generative methods like generative adversarial networks (GAN) have gained great attention in TZSL recently \cite{xian2018feature}. Given the real data set, generative models could be trained to fit the distribution and its conditional generation mecahnism could further realize the intra-class distribution alignment. Thus conditional generative models could be treated as a fairly well tool to transfer the ZSL knowledge by setting the auxiliary information as the condition. F-VAEGAN \cite{xian2019f}, which is one of the most performing approaches in TZSL, utilizes a shared weight VAE \cite{doersch2016tutorial} decoder and GAN \cite{creswell2018generative} generator to map the attribute space to visual space. Specifically, they use an unconditional unseen discriminator to capture the overal unseen data distribution. However, F-VAEGAN is only working with the alignment in the visual space and this causes the unconstrained generation and weak conditional distribution matching for the unseen classes. This is because the attribute information is only implemented in the forward generation process and no adaptive information from the attribute is extracted (i.e., missing from the gradient backpropagation), which results in a weak constrained generation and alignment becomes more sensitive to the auxiliary inforamtion quality. 
% % The later TF-VAEGAN uses an attribute regressor to force the synthesized feature map back to attribute space and achieve a better synthesis. Its feedback module is another modification that makes the back-propagation interrogated with attribute knowledge by enhancing the generation, effectively showing the importance of the \textit{\textbf{direct}} participation of auxiliary information in the TZSL. 

% Figure \ref{fig.1} shows the core idea of our work. We use a transductive generator and transductive regressor to realize a bidirectionally conditional distribution alignment between visual space and auxiliary space.  For example, The attribute vectors of 'horse' and 'sheep' on AWA2 \cite{xian2018zero} provide limited discriminative information and the inductive generation process result in the extremely adjacent synthesized domain in the visual space, while there is a huge gap with the real target domain. Our approach successfully aligns the fake data with the real one in both visual and auxiliary spaces, and even for the confusing pair of 'walrus' and 'seal', our model could decouple the mixed cluster to be correctly separated. In this work, we conduct a deeper comprehension of the regressor module and make a series of modifications such that the generator can provide a more realistic conditional generation.  We find the regressor module could help with the intra-clustering of the unseen data and improve the models' tolerance to the attribute quality. Unlike TF-VAEGAN \cite{narayan2020latent}, which utilizes the inductive regressor that only trains with the seen feature, we propose the transducitve regressor which reveals the regressor with the information of unseen feature to realize a stronger constraint for visual generation. Meanwhile, we find the (unseen) class frequency prior is important for the conditional distribution alignment, especially for those extremely unbalanced datasets. An unknown class prior could easily damage the convergence guarantee and lead to a terrific alignment. To alleviate the above problem, we propose to use a simple class prior estimation strategy to approximate the class prior and epochs update it until convergence. Experimental result shows our simple strategy guarantee our model the competitive result even under the unknown prior case. Our approach could be summarized as,
% \begin{itemize}
%     \item We propose the bi-directional distribution alignment training strategy in TZSL and achieve a new state-of-the-art performance over multiple datasets.
%     \item We first utilize the data augmentation methods including $L_2$ feature normalization and attribute perterbation to ease the bi-directional alignment.
%     \item We first discuss the importance of class prior for generative approaches in TZSL and propose a simple and effective class prior estimation moudle to effective deal with the unknown class prior datasets.
% \end{itemize}


% \section{RELATED WORKS}
% \noindent{\textbf{Inductive Zero-Shot Learning}}
% To realize knowledge transfer from seen classes to unseen classes, early works of inducitve ZSL learn a compatible projection function from semantic space to visual space \cite{changpinyo2017predicting,zhang2017learning}. However, due to existence of distribution gap between seen and unseen data, those methods suffer from the well known \textbf{domain shift} problem. The other apporach, generative method (e.g., VAEs and GANs), utilizes generative models to synthesize unseen instance and train those fake data with an additional classifier. The noise-sampled generation increases the intersection possibility between synthesized and real unseen domain, but fundamentally generative approachs still suffer from the domain shift problem. Some works try to alleviate the problem by introduce auxiliary moudle. f-CLSWGAN \cite{xian2018feature} utilizes conditional Wasserstein Generative Adversarial Network (WGAN) to model conditional seen distribution and adds a classification obejctive function to enhance the realistic generation. A related work that is close to our approach, Cycle-WGAN \cite{felix2018multi}, implements the cycle consistency loss \cite{zhu2017unpaired} with a semantic regressor and achieves a stronger constraint. However, with out knowing any unseen data, the inductive ZSL is severely relying on the richness and quality of semantic information.

% \noindent{\textbf{Transductive Zero-Shot Learning}}
% As a concession from inductive ZSL, transductive ZSL allow model accesses the test-time unseen data in the training \cite{fu2015transductive}. VSC \cite{wan2019transductive} utilize the clustering property of unseen data and propose the visual structure constraint to align the projection center with the cluster center. More recently, the distribution modeling ability of generative models show its superiority in transductive ZSL. F-VAEGAN introduces the cooperation training strategy of VAE and GAN and implements a additional unconditional discriminator to capture the unseen distribution. STHS-WGAN \cite{bo2021hardness} enhances unseen conditional generation by iterative adding the easy classified classes to the training seen set. However, they only concentrate on the one-way generation from semantic to visual space and give limited constrait for the unseen generation. Though TF-VAEGAN  \cite{narayan2020latent} adopts the semantic regressor in the transductive setting and uses a feedback moudle to recurrently generate the features, it does not fully utilize the transductive data for the training of regressor. Our method is different in that we also focus on the genrattion from visual to semantic and argue the cycle-consistant constrait is beneficial for the synthesis quality. Moreover, we discuss the importance of class frequency prior for the generative transductive ZSL approach and propose to estimate the frequency with its cluster property.


%-----------------------------------------------------------------------------------------------------------------------------------------------------

\section{INTRODUCTION}



Zero-shot learning (ZSL) was originally proposed  as zero-data learning in computer vision  \cite{larochelle2008zero}, tackling challenging training setups that largely restrict the example and (or) label availability \cite{chen2022msdn}. 
%
For instance, in conventional ZSL,  no training example is provided for the targeted classes, and they are therefore referred to as the unseen classes.   
%
Instead,  a large number of training examples paired with their class labels are provided for a different set of classes, which are referred to as the seen classes. 
%
This setup is called the inductive ZSL. 
%
Its core challenge is to enable the classifier to extract knowledge from the seen classes and transfer it to the unseen classes, assuming the existence of such relevant knowledge \cite{norouzi2013zero,zhang2016zero,xian2018zero}. 
%
For instance,  a ZSL classifier can be constructed to recognize \textit{leopard} images after feeding it the Felinae images like \textit{wildcat}, knowing  \textit{leopard} is relevant to Felinae.  
%
Information on class relevance is typically provided as auxiliary data,  bridging knowledge transfer from the seen to unseen classes.
% 
The auxiliary data can be human-annotated attribute information  \cite{wah2011caltech}, text description \cite{reed2016learning}, knowledge graph \cite{lee2018multi} or a formal description of knowledge (e.g., ontology) \cite{geng2021ontozsl}, etc., which are encoded as (set of) embedding vectors. 
%
Learning solely from auxiliary data to capture class relevance is challenging, resulting in a discrepancy between the true and modeled distributions for the unseen classes, known as the domain shift problem.
%
To ease the learning, another ZSL setup called transductive ZSL (TZSL) is proposed. 
%
It allows to additionally include in the training unlabelled examples collected for the target classes.
%
Since it does not require any extra annotation effort to pair the examples from the unseen classes with their class labels, this setup is still practical in real-world applications.

\begin{figure}[t]
\begin{center}
    \includegraphics[width = 1\linewidth,height = 5.3cm]{figures/visualization/abstract.pdf}
\end{center}    
\caption[]{The top figure illustrates the proposed bi-directional generation between the visual and auxiliary spaces. The bottom figure compares the aligned visual space obtained by our method with the unaligned one obtained by inductive ZSL for the unseen classes using the AWA2 data. The bottom right figure shows our approximately aligned attributes in auxiliary space.   \label{fig.1}}
\vspace*{-0.4cm}
\end{figure}

Benefiting from good example availability, generative models have become popular for synthesizing examples to enhance classifier training \cite{xian2018feature,elhoseiny2019creativity,li2019leveraging} and for learning the unseen data distribution \cite{wu2020self,paul2019semantically,xian2019f} under the TZSL setup. 
%
Depending on the data availability, they can be formulated as unconditional generation i.e., $p(\bm v)$, or conditional generation i.e., $p(\bm v|\bm y)$.
%
When being conditioned on the auxiliary information, which is a more informative form of class labels, a data-auxiliary (data-label) joint distribution could be learned.
% Zhicai: Added following
If proper supervision is added, intra-class data distribution alignment (with the real data) could be realized, e.g, using a conditional discriminator to discriminate whether the generation is a realistic \textit{wildcat} image. 
The data-auxiliary joint distribution modeling nature bridges the knowledge between two spaces and enables the generator a proper knowledge transfer tool.
As for TZSL, the challenging problem is how to transfer the joint distribution knowledge of seen classes to the unlabelled unseen data and have a realistic generation for the unseen classes.
%
f-VAEGAN \cite{xian2019f},  a representative generative approach,  which enhances the unseen generation using an unconditional discriminator and learns the overall unseen data distribution.
%
This simple strategy turns out to be effective in approximating the conditional distribution of unseen classes.
%
However, the majority of existing works \cite{wu2020self,bo2021hardness} only use auxiliary data in the forward generation process, i.e.,  to generate images from the auxiliary data as by $p(\bm v|\bm y)$. 
%
It results in weakly guided conditional generation for the unseen classes and the alignment is extremely sensitive to the auxiliary information quality.
%
To bridge better between visual and auxiliary data, particularly for the unseen classes, which is equivalent to enhancing the alignment with the true unseen conditional distribution, we propose a novel bi-directional generation process. 
%
It couples the forward generation process with a backward one, i.e., to generate auxiliary data from the images as by $p(\bm y|\bm x)$. 
%
We build our bi-directional generation based on the feature-generating framework as shown in  Figure \ref{fig.1}, and name the proposed model Bi-VAEGAN. 
%
% Its core architecture is presented in Figure \ref{fig.1}. 

Overall, the proposed design covers three aspects.  (1) A transductive regressor is added to form the backward generation, synthesizing pseudo auxiliary features conditioned on visual features of an image. 
%
This, together with the forward generation as used in  f-VAEGAN, provides more constraints to learn the unseen conditional distribution, expecting to achieve better alignments between the visual and auxiliary spaces.
%
(2) We introduce  $L_2$-feature normalization, a free-lunch data pre-processing method,  to further support the conditional alignment. 
%
(3) Besides, we note that the (unseen) class prior plays a crucial role in distribution alignment, particularly for those datasets that have extremely unbalanced label distribution. 
%
A poor choice of the class prior can easily lead to a poor alignment. 
%
To address this issue, we propose a simple but effective class prior estimation approach based on a cluster structure contained by the examples from the unseen classes.  
%
The proposed Bi-VAEGAN is compared against various advanced ZSL techniques using four benchmark datasets and achieves a significant TZSL classification accuracy boost compared with the other generative benchmark models.
% \section{INTRODUCTION}

% \begin{figure}[thp]
% \begin{center}
%     \includegraphics[width = 1\linewidth,height = 5.3cm]{figures/visualization/abstract.pdf}
% \end{center}    
% \caption[]{Our approach aims to improve knowledge transfer using auxiliary information from open-world seen data to unseen data.  We utilize transductive generator to synthesize auxiliary information into visual space, and introduce cycle-consistent transductive regressor to further restrict the generation. Our model successfully aligned the real 
% feature with the synthesized in visual space and aligned the pseudo attribute that converted from synthesized feature with the real attribute in the auxiliary space.
% The bottom middle figure shows our aligned unseen visual space of AWA2 and its top right depicts the aligned auxiliary space. \label{fig.1}}
% \end{figure}


% Zero-shot learning (ZSL) was first named zero-data learning in the computer vision community \cite{larochelle2008zero}, which aims to classify those unseen classes that have no labeled data provided in training. For the specific domain of interests to conduct ZSL, there is usually a large amount of labeled data provided which we call those of seen classes.  
% The challenge of ZSL is to transfer knowledge from readily accessible seen classes to those novel unseen classes \cite{norouzi2013zero,zhang2016zero,xian2018zero}. 
% % For example, we have seen amounts of animals from Felinae like wildcat, and we hope the machine to classify the in-the-wild collected leopard figures.
% Auxiliary data, such as attribute information that is human-annotated discrete vectors\cite{wah2011caltech}, text description embedded vectors\cite{reed2016learning} and et al, is typically provided to bridge the knowledge transfer from seen classes to the unseen. 
% % So apart from the similar characters between wildcat and leopards, the text description like 'short legs', 'long body', and 'rosettes marked fur' could help with the classification of unseen leopards.
% Conventional ZSL, also referred to as inductive ZSL, classifies new categories solely based on auxiliary data with no unseen information provided. Due to the \textbf{domain shift} problem, which means there is a discrepancy between the algorithms' modeled unseen distribution and the real one, it typically has poor knowledge transferability.
% Thus, the quality of auxiliary information has a huge impact on transferability.
% In reality, gathering enormous amounts of unlabeled data in the real world isn't difficult at all, which could greatly reduce the domain shift issue. Transductive ZSL (TZSL), a solution to inductive ZSL, thus is proposed to get around the problem of the unconstrained unseen visual domain by assuming the availability of unlabeled test data.

% Generative models in ZSL is to use the generated fake data to train the classifier \cite{xian2018feature,elhoseiny2019creativity,li2019leveraging}, and it has gained great attention in TZSL recently since we can access the real unseen data distribution\cite{wu2020self,paul2019semantically,xian2019f}. 
% Given the data distribution or data-label joint distribution, generative models could be trained to fit the distribution and its conditional generation mechanism could also realize the intra-class distribution alignment (i.e., $p(\bm x|\bm y)$). Thus conditional generative models could be treated as a fairly effective tool to transfer knowledge in the ZSL scenarios by setting the auxiliary information as the condition. 
% In order to capture the overall unseen distribution, generative approaches like f-VAEGAN \cite{xian2019f} typically use an unconditional unseen discriminator, which could achieve a fairly good conditional distribution alignment of the most unseen classes. However, the majority of them only focus on one-directional visual space alignment, which results in a weak, unguided conditional alignment for those unlabeled unseen classes. Because no adaptive information is extracted from the auxiliary and only the forward generation process uses the auxiliary information (i.e., missing from the gradient backward-propagation). The alignment continues to be sensitive to the quality of the auxiliary information.
% % Consequently, F-VAEGAN fails to generate correct feature for the unseen 'walrus' and 'seal' on AWA2 dataset due to their resemble appearrance. 


% To address the constraint generation and enhance the conditional alignment in the unseen domain, we propose our generative approach, Bi-VAEGAN.
% The central concept of our work, where we attempt to align the features in both visual and auxiliary spaces, is shown in Figure \ref{fig.1}.
% % Figure \ref{fig.1} shows the core idea of our work where we try to align the features in both visual and auxiliary spaces.
% % Specifically, unlike f-VAEGAN,  we use an additional transductive regressor to realize the bidirectionally conditional distribution alignment between visual space and auxiliary space.
% In particular, unlike F-VAEGAN, we employ an additional transductive regressor
% to achieve the alignment of the bidirectional conditional distribution between the auxiliary space and the visual space.
% For example, for the appearance-resemble 'horse' and 'sheep' pair on AWA2,
% our approach successfully aligns the synthesized visual feature with the real one while also aligning the pseudo attribute (regressed from the synthesized visual feature) with the real attribute. Benefiting from the bidirectional alignment constraint, even for the visually perplexing pair of 'walrus' and 'seal', our model can decouple the mixed visual cluster and produce a proper synthesis. 

% In this work, we follow previous works that condition knowledge transfer on auxiliary information using a generative adversary network. 
% The difference is that we investigate the regressor module more thoroughly and discover that it is useful for providing supervised information, which enables the model to be more roubust to auxiliary information quality. To achieve a stronger constraint (bidirectional alignment) for the visual generation, we suggest the transductive regressor, which reveals the regressor with the information of unseen features. A better conditional alignment is made possible by a number of free-lunch data augmentation techniques that we also introduce, such as $L_2$ feature normalization and attribute perturbation. 
% % Lastly, we find the (unseen) class frequency prior is important for the conditional distribution alignment, especially for those extremely unbalanced datasets. An unknown class prior could easily damage the convergence guarantee and lead to a terrific alignment. To alleviate the above problem, we propose to use a simple class prior estimation strategy to approximate the class prior. Experimental result shows our simple strategy guarantee our model the competitive result even under the unknown prior case.
% Besides, we find that the (unseen) class frequency prior is crucial for the conditional distribution alignment, particularly for those datasets that have extremely unbalanced label distribution. Through the fusion of knowledge of class prior and auxiliary information model can achieve a better conditional alignment,  whereas an unknown class prior will easily lead to a poor alignment. To address TZSL under the prior unknown scenarios, we propose to approximate the class prior using a simple class prior estimation strategy, which is based on the observation of the cluster property of unseen classes.  Experimental result shows our simple strategy guarantee our model the competitive result even under the unknown prior case.



%Zhicai (TM): Check very carefully whether it makes sense.
\section{RELATED WORKS}
\noindent{\textbf{Inductive Zero-Shot Learning }}
%
Previous works on inductive ZSL learn simple projections from the auxiliary (e.g. semantic)  to visual spaces to enable knowledge transfer from the seen to the unseen classes \cite{changpinyo2017predicting,zhang2017learning,DBLP:conf/nips/YuJFGPZ18,chou2020adaptive}. They suffer from the well-known domain shift problem due to the distribution gap between the seen and unseen data. Relation-Net \cite{sung2018learning} utilizes two embedding modules to align visual and semantic information respectively and compute the relation scores in the embedded space.
Generative approaches, e.g., variational auto-encoder (VAE) \cite{doersch2016tutorial} and generative adversarial network (GAN) \cite{creswell2018generative},  synthesize unseen examples and train an additional classifier using the generated examples. This can enhance alignment between synthesized and true unseen domains, although it still suffers from domain shift. Some works, on the other hand, aim to improve by introducing auxiliary modules. For instance, f-CLSWGAN \cite{xian2018feature}  uses a Wasserstein GAN (WGAN) to model the conditional distribution of seen data,  while also introducing a classification loss to improve the generation. Cycle-WGAN \cite{felix2018multi} employs a semantic regressor with a cycle-consistency loss \cite{zhu2017unpaired} to reconstruct the original semantic features, which provides stronger generation constraints and shares some similar spirit with our work. However, because there is no knowledge of the unseen data, the inductive ZSL heavily relies on the quality of the auxiliary information, making it difficult to solve the performance bottleneck.

\noindent{\textbf{Transductive Zero-Shot Learning }}
As a concession of inductive ZSL, TZSL allows to use of the test-time unseen data in the training \cite{fu2015transductive,wu2020self,yang2022iterative}. 
One of the most representative approaches is visual structure constraint (VSC) \cite{wan2019transductive} which exploits the cluster structure of the unseen data and proposes to align the projection centers with the cluster centers. Recently, generative models have been actively explored and demonstrate their superiority in TZSL. For instance, f-VAEGAN combines the VAE and GAN, and additionally includes an unconditional discriminator to capture the unseen distribution. SDGN \cite{wu2020self} introduces a self-supervised objective for the discriminability mining between seen and unseen domains. STHS-WGAN \cite{bo2021hardness} iteratively adds easily distinguishable unseen classes to the training seen examples to improve the unseen generation. However, previous approaches mostly work with a uni-directional generation from the auxiliary space to the visual one, which could potentially result in limited constraints when learning the unseen distribution. 
TF-VAEGAN \cite{narayan2020latent} enhances the generated visual features by utilizing a inductive regressor that trained with seen data and a feedback module. Our work is different from TF-VAEGAN as we additionally consider the information exploration for unseen data in the regressor.




\section{METHOD}

\subsection{Notations}
We use $V^s=\{\bm v^s_i\}_{i = 1}^{n_s}$ and $V^u=\{\bm v^u_i\}_{i = 1}^{n_u}$ to denote the collections of examples from the seen and unseen classes, where each example is characterized by visual feature extracted by a pre-trained network.  
%
For examples from the seen classes, their class labels are provided and denoted by  $Y^s = \{y_i \}_{i = 1}^{n_s}$. 
%
Attribute (we set as the default choice of auxiliary information) is provided to describe both the seen and unseen classes, represented by vector sets $A^s=\{\bm a^s_i\}_{i = 1}^{N_s}$ and $A^u=\{\bm a^u_i\}_{i = 1}^{N_u}$ where $N_s$ and $N_u$ are the number of seen and unseen classes. 
%
Under the TZSL setting, a classifier $f(\bm v): \mathcal{V}^u \rightarrow \mathcal{Y}^u$ is trained to conduct inference on unseen data, where we use $\mathcal{V}$ to denote the encoded visual space, and  $\mathcal{Y}$ the label space.
%
The pipeline in the training stage uses the information provided by $D^{tr}=\{\langle V^s,Y^s\rangle,V^u,\{A^s, A^u\}\}$, where $\langle \cdot, \cdot \rangle$ means 
paired data.

\subsection{$L_2$-Feature Normalization}

%
Feature normalization is an important data preprocessing method, which guarantees the model training and accelerates the model convergence\cite{li2021feature}. 
%
A common practice in TZSL is to normalize the visual features by the Min-Max approach i.e.,$\bm v^\prime = \frac{\bm v -\text{min}(\bm v)}{\text{max}(\bm v)-\text{min}(\bm v)}$\cite{narayan2020latent,xian2019f,wu2020self}. 
%
However, we find that it suffers from the distribution skew of synthesized feature value and it is more beneficial to normalize the visual features by their $L_2$-norm. 
%
For a visual feature vector $\bm v  \in {V}^s \bigcup {V}^u $, it has
\vspace{-0.3cm}
\begin{equation}
\label{V_norm}
\bm v ^\prime = L_2(\bm v, r) = \frac{r\bm v}{||\bm v||_2},
\end{equation}
where the hyperparameter $r>0$ controls the norm of a normalized visual feature vector.  
%
In the generator, the key modification is to replace the last $sigmoid$  layer accompanying  the Min-Max approach with an $L_2$-normalization layer. 
%
We discuss further its effect in Section \ref{sec:norm}.

 
\begin{figure*}[thp]
    \small
    \centering
        \includegraphics[width=0.9\linewidth,height = 7.6cm]{figures/model/BiT-VAEGAN.pdf} 
        \caption{The proposed Bi-VAEGAN model architecture under the TZSL setup.}
        \vspace{-0.5cm}
        \label{fig.3}
\end{figure*}


\subsection{Bi-directional Alignment Model}
\label{sec:BA}

We propose a modular model architecture composed of six modules: (1) a conditional VAE encoder $\bm E: \mathcal{V} \times 
\mathcal{A} \rightarrow \mathbb{R}^k$ mapping the visual features  to a $k$-dimensional hidden representation vector conditioned on the class attributes,  (2) a  conditional visual generator $\bm G: \mathcal{A} \times \mathbb{R}^k  \rightarrow \mathcal{V}$ synthesizing visual features from  a $k$-dimensional random  vector that is usually sampled from a normal distribution $\mathcal{N}(\bm 0, \bm 1)$, conditioned on the class attributes, 
(3) a conditional  visual Wasserstein GAN (WGAN)  critic $D: \mathcal{V}^s\times \mathcal{A } \rightarrow \mathbb{R}$ for the seen classes,  (4) a visual WGAN critic $ D^{u}: \mathcal{V}^u\rightarrow \mathbb{R}$ for the unseen classes, (5) a regressor mapping the visual space to the attribute space  $\bm R: \mathcal{V}\rightarrow \mathcal{A }$, and (6) an attribute WGAN critic $D^{a}: \mathcal{A}\rightarrow \mathbb{R}$. 
%
% Later on, we slightly abuse the notation ${\mathcal{A}}$ to denote the pre-processed attribute space (attribute perturbation) .


The proposed workflow consists of two levels. In \textbf{Level-1}, the regressor $\bm R$ is adversarially trained using the critic $D^{a}$ so that the pseudo attributes converted from the visual features align with the true attributes. 
In \textbf{Level-2}, the visual generator $\bm G$ is adversarially trained using the two critics $D$ and $D^{u}$ so that the generated visual features align with the true visual features. 
%
Additionally, the training of $\bm G$ depends on the regressor $\bm R$. 
%
This encourages the pseudo attributes converted from the synthesized visual features align better with the true attributes.
%
To highlight the core innovation of aligning true and fake data in both the visual and attribute spaces, we name the proposal as bi-directional alignment.

\subsubsection{Level 1: Regressor Training}
\label{sec:3.3.1}
The regressor $\bm R$ is trained transductively and adversarially.  It is constructed by performing supervised learning using the labeled examples from the seen classes, additionally enhanced by unsupervised learning from the visual features and class attributes of the unseen classes. By  ``unsupervised", we mean that the features and classes are unpaired for examples from the unseen classes.
For each example from the seen classes, $\bm R$ learns to map its visual features close to its corresponding class attributes via minimizing
\vspace{-0.3cm}
\begin{align}
    \begin{split}
        \small
         L_R^s({\mathcal{A}}^s, \mathcal{V}^s ) =&  \mathbb{E}[\| \bm R (\bm v^s) - {\bm a}^s \|_1].\\  
    \end{split}
\end{align}
Simultaneously, for examples from the unseen classes, it learns to distinguish their true attributes from the pseudo ones computed from the real unseen visual features by maximizing the adversary objective
\begin{align}
\label{adv_obj}
    \begin{split}
    \small
         L^{u}_{D^a\text{-WGAN}} ({\mathcal{A}}^u, \mathcal{V}^u ) =&  \mathbb{E} \left [D^{a}({\bm a}^u) \right ]  - \mathbb{E} \left [D^{a}(\hat{\bm a}^u) \right ]+\\  
         & \mathbb{E}_{}  [(\|\nabla_{\bar{\bm a}^u} D^{a}(\bar{\bm a}^u)\|_2-1)^2 ],
    \end{split}
\end{align}
where $\hat{\bm a}^u =\bm R (\bm v^u)$,  $\bm a^u \sim p^u_{\bm G}( y)$ and $\bar{\bm a}^u\sim \mathcal {P}_{t}({\bm a}^u,\hat{\bm a}^u)$\footnote{$\mathcal {P}_{t}(\bm a,\bm b)$ is an interpolated distribution in the $L_2$ hypersphere. An example sampled from this distribution is computed by $c = L_2(t\bm a+(1-t)\bm b,r)$ with $t\sim \mathcal{U}(0,1)$ where $\|\bm a\|_2=\|\bm b\|_2=r$.}. 
%
Note that the original  attribute vectors are sampled from  the unseen class prior $p^u_{\bm G}( y)$ explained in Section \ref{sec:prior}, and we refer to this as the \textbf{prior sample} process.
%
The third term in Eq. (\ref{adv_obj}) is known as the gradient penalty \cite{gulrajani2017improved},
which enables the Lipschitz restriction in the original WGAN \cite{arjovsky2017wasserstein}.
%

The regressor $\bm R$ aims at learning a mapping from the visual to the attribute features for the seen classes in a supervised manner, and meanwhile learning the distribution of the overall feature domain for the unseen classes in an unsupervised manner. The  level-1 training is formulated by
\begin{align}
\label{level1_training}
    \begin{split}
        \operatorname*{min}_{\bm R}\operatorname*{max}_{D^a}\; L_{R}^{s}+\lambda L_{D^a\text{-WGAN}}^{u},
    \end{split}
\end{align}
%
where $\lambda$ is a hyper-parameter. It enables knowledge transfer from the seen to  unseen classes in the attribute space, where the feature discriminability  is however limited by the hubness problem \cite{zhang2017learning}. 
% 
But it serves as a good auxiliary module to provide ``approximate supervision'' for the unseen distribution alignment later in the visual space.

\subsubsection{Level 2: Generator and Encoder Training}  

The generator $\bm G$ is also trained transductively and adversarially. It aims at aligning the synthesized and true features,   using the visual critics $D$ and $D^u$ in the visual space, while  using the frozen regressor $\bm R$ in the attribute space.

The two visual critics are trained to get better at distinguishing the true visual features from the synthesized ones computed by the conditional generator, i.e. $ \hat{\bm v} \sim  \bm G(\bm a,\bm z) $ where $\bm z \sim \mathcal{N}(\bm 0,\bm 1)$ and $\bm a \sim p_{\bm G}(y)$. 
%
For the seen classes, their class prior, denoted by $p_{\bm G}^{s}(y)$\footnote{For the intra-class alignment, i.e, we know the paired seen labels, the choice of $p_{\bm G}^{s}(y)$ will not affect much of the training.}, is simply estimated from the number of examples collected for each class. 
%
For the unseen classes, the estimated class prior $p_{\bm G}^u(y)$ is used.
%
The synthesized visual feature $\hat{\bm v}$ is already $L_2$-normalized. 
%
For the seen classes, the critic is conditioned on their class attributes, i.e., $D(\hat{\bm v}^s, {\bm a}^s)$, while  for the unseen classes, the critic is unconditional, i.e., $D^u(\hat{\bm v}^u)$. 
%
The two critics $D$ and $D^u$ are trained based on the adversarial objectives:
\begin{align}
\label{Du_training1}
    \begin{split}
    \small
     L^s_{D\text{-WGAN}}(A^s, V^s) =& \mathbb{E}[D( {\bm v}^s,  {\bm a}^s)]- \mathbb{E}[D(\hat{\bm v}^s, {\bm a}^s)] +\\ 
     & \mathbb{E}[(\|\nabla_{\bar{\bm v}^s} D(\bar{\bm v}^s, {\bm a}^s)\|_2-1)^2],
    \end{split}
\end{align}
and 
\begin{align}
    \label{Du_training}
        \begin{split}
        \small
            L^{u}_{D^u\text{-WGAN}}(A^u, V^u)  = &\mathbb{E}[D^{u}({\bm v}^u )]- \mathbb{E}[D^{u}( \hat{\bm v}^u )]+ \\ 
         & \mathbb{E}[(\|\nabla_{\bar{\bm v}^u} D^u(\bar{\bm v}^u)\|_2-1)^2],
        \end{split}
    \end{align}
where $\bar{\bm v}^s$ and $\bar{\bm v}^u$ are sampled from the interpolated distribution as explained in footnote 1. Here, $\hat{\bm v}^u$ is computed from the unseen attributes sampled by $\bm a^{u} \sim p_{\bm G}^{u}(y)$.
%
The critic  $D^{u}$ in Eq. (\ref{Du_training}) captures the Earth-Mover distance over the unseen data distribution.
% , as  $W(p^u( \bm v),\int p_{\bm G}^u(\hat{\bm v}|\bm a)p^u_{\bm G}(y)dy)$. 


Eqs (\ref{Du_training1}) and (\ref{Du_training}) weakly align the conditional distribution of the unseen classes. It suffers from the absence of any supervised constraints. 
%
To further  strengthen the alignment, we introduce another training loss, as
\begin{align}
\label{vae_loss}
    \begin{split}
         L_R^u(A^u ) =&  \mathbb{E}[\| \bm R (\bm G({\bm a}^u,\bm z)) - {\bm a}^u \|_1].\\  
    \end{split}
\end{align}
It employs the regressor $\bm R$ trained in level-1 to  enforce supervised constraints.
%
As shown in f-VAEGAN, feature-VAE has the potential of preventing model collapse, and it could serve as a suitable complement to GAN training. Similarly, we adopt the VAE objective function to enhance the adversarial training over the seen classes:
\begin{align}
    \begin{split}
    \small
        L^s_{\text{VAE}}(A^s,& V^s)
    =   \mathbb{E} [\text{KL}( \bm E({\bm v}^s,{\bm a}^s)  \|\mathcal{N}(\bm 0, \bm 1))] + \\
        &\mathbb{E}_{\bm z^s \sim \bm E({\bm v}^s,{\bm a}^s)} [ \left(\|\bm G({\bm a}^s,\bm z^s)-{\bm v}^s\|^2_2 \right)].  \label{loss:VAE}
    \end{split}
\end{align}
The first term is the Kullback-Leibler divergence and the second is the mean-squared-error (MSE) reconstruction loss using the $L_2$ normalized feature. 
%
Finally,  the  level-2 training is formulated by,
\begin{align}
\label{level2_training}
        \operatorname*{min}_{\bm E,\bm G}\operatorname*{max}_{D,D^u}\; L^s_{\text{VAE}} + \alpha L_{D\text{-WGAN}}^{s} + \beta L_{R}^{u}+ \gamma L_{D^u\text{-WGAN}}^{u},
\end{align}
%
where $\alpha$, $\beta$ and $\gamma$ are hyper-parameters. It  transfers the knowledge of \textit{ paired visual features and attributes} of the seen classes and the estimated \textit{class prior} of the unseen classes, 
%
and   is enhanced by  the attribute regressor $\bm R$ to constrain further the visual feature generation for unseen classes. 
%
The proposed Bi-VAEGAN architecture can be easily modified to accommodate the inductive ZSL setup, by removing all the loss functions using the unseen data $V^u$:
\begin{align}
\label{inductive_training}
    \begin{split}
        \textmd{For level-1: } &\min_{\bm R} L_{R}^{s} \\
         \textmd{For level-2: }&\min_{\bm E,\bm G}\operatorname*{max}_{D} L^s_{\text{VAE}} + \alpha L_{D\text{-WGAN}}^{s} + \beta L_{R}^{u}.
    \end{split}
\end{align}

\subsubsection{Unseen Class Prior Estimation}
\label{sec:prior}

When training based on the objective functions in Eqs. (\ref{adv_obj}) and (\ref{Du_training}), the attributes for the unseen classes are sampled from the class prior:  $\bm a^u \sim p^u_{\bm{G}}( y)$. Since there is no label information provided for the unseen classes, it is not possible to sample from the real class prior $p^u(y)$. An alternative way to estimate $p^u_{\bm{G}}( y)$ is needed.
%
We have observed that examples from the unseen classes possess fairly separable cluster structures in the visual space, and benefited from the strong backbone network. Therefore, we propose to estimate the unseen class prior based on such cluster structure.
%
We employ the K-means clustering algorithm and carefully design the initialization of its cluster centers since the prior estimation is extremely sensitive to the initialization.
%
The estimated prior $p_{\bm G}^u(y)$ is iteratively updated and in each epoch cluster centers are re-initialized by pseudo class centers calculated from an extra classifier $f$. 
%
For the very first estimation of $p_{\bm G}^u(y)$, rather than use the naive but sometimes (if it differs greatly from the real prior) harmful uniform prior assumption,  we use the inductively trained generator to transfer the seen paired knowledge to have a mild estimation for the unseen.
%
We refer to the estimation approach as the cluster prior estimation (CPE), and its implementation is shown in Algorithm \ref{tioncpe} (lines 1-12).


 
 
 	

\IncMargin{1em}
\begin{algorithm}[tp]\SetAlgoLined
     \SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up} \SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress} \SetKwInOut{Data}{Data}\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
    \small
    \Input{$\langle V^s,Y^s \rangle$,  $V^u$,  $\{A^s, A^u\}$, unseen class number $N_u$, epoch numbers $T_1$ and $T_2$.}
    \Output{$\bm E$, $\bm G$, $\bm R$, $D$, $D^u$, $D^a$.}
    \BlankLine
    \For{ $i = 1$  to $T_1$}{
         Inductive training with $\langle V^s,Y^s \rangle,\{A^s, A^u\}$ by  \\Eq. (\ref{inductive_training})\;
}
      
    \For{ $i = 1$  to $T_2$}{
        Define uniform distribution label set  $Y^u_{\bm G}$\;
        Synthesize paired unseen set  $\langle \hat V^u_{\bm G}, Y^u_{\bm G}\rangle$ using $\bm G$\;
        Train a classifier $f$ using $\langle \hat V^u_{\bm G}, Y^u_{\bm G}\rangle$\; 
        Assign pseudo class labels by $\hat Y^u = f(V^u)$\;
        Pseudo class centers $C^u \leftarrow  \langle V^u, \hat Y^u\rangle$\;
    %    Obtain cluster labels $\hat Y_{C}^u$ by   kmeans  for  $V^u$  with cluster centers initialized by $C^u$.
     
    %   Estimate $p_G^u(y)$ from $\hat Y_{C}^u$ by class size.
      \emph{$\hat Y_{kmeans}^u$ = Kmeans($V^u$,$N_u$, InitCenter = $C^u$)}\; 
      $p_{\bm G}^u(y) \leftarrow  \hat Y_{kmeans}^u$\tcc*{\footnotesize{Update prior}}
      Transductive training with $\langle V^s,Y^s \rangle$, $V^u$, $\{A^s, A^u\}$ and $p_{\bm G}^u(y)$ by Eqs (\ref{level1_training}) and (\ref{level2_training})\;
      }
    \caption{Bi-VAEGAN (CPE) }
           \label{tioncpe} 
      \end{algorithm}
 \DecMargin{1em} 
 

\noindent \textbf{Discussion:} We attempt to explain the importance of estimating $p^u_{\bm{G}}( y)$ based on the following corollary, which is a natural result of Theorem 3.4 of \cite{tachet2020domain}.
\begin{cor}
Under the generative TZSL setup, for the unseen classes, the total variation distance between the true conditional visual feature distribution $p^u( \bm{v}|y)$ and the estimated one by the generator $p_{\bm{G}}^u(\hat{\bm{v}}|y)$ is upper bounded by
    \begin{align}
        \begin{split}
            &\operatorname*{max}_{y \in Y^u}d_{\mathrm{TV}}\left(p^u_{\bm G}\left(\hat{\bm v}\mid y\right),p^u\left(\bm v\mid y\right) \right)\\
            \leq& \frac{1}{\min_{y \in Y^u} p^u(y)}\left( \max_{y\in Y^u}\left(\frac{p^u(y)}{p^u_{\bm{G}}(y)}\right) e^u_f(\hat{\bm v})+e^u_f(\bm v)\right. \\
            & \left.+\sqrt{8{d}_{\textmd{JS}}\left( \sum_{ y \in Y^u } p^u(y)p^u_{\bm G}(\hat{\bm v}|y), p^u(\bm v)\right)} \right),
    \end{split}
    \label{gls}
    \end{align}
    where ${d}_{\textmd{JS}}(\cdot, \cdot)$ is the Jensen-Shanon divergence between two distributions, and $e^u_f(\bm x)$ denotes the error probability that the classification  of the input feature vector disagrees with its ground truth using hypothesis $f$.

    \end{cor} 




\noindent    
The above result is a straightforward application of the domain adaptation result in Theorem 3.4 of \cite{tachet2020domain}, obtained by treating $p^u_{\bm G}\left(\hat{\bm v}\mid y\right)$ as the source domain distribution while $p^u\left(\bm v\mid y\right)$ as the target domain distribution. When the estimated  and ground truth class priors are equal, i.e., $p^u(y) = p^u_{\bm{G}}(y)$, Eq. (\ref{gls}) reduces to 
\vspace*{-0.08cm}
  \begin{align}
        \begin{split}
            &\operatorname*{max}_{y \in Y^u}d_{\mathrm{TV}}\left(p^u_{\bm G}\left(\hat{\bm v}\mid y\right),p^u\left(\bm v\mid y\right) \right)\\
            \leq& \frac{1}{\gamma}\left( e^u_f(\hat{\bm v})+e^u_f(\bm v) 
            +\sqrt{8{d}_{\textmd{JS}}\left(   p^u_{\bm G}(\hat{\bm v}), p^u(\bm v)\right)} \right),
    \end{split}
    \end{align}
where $\gamma=\min_{y \in Y^u} p^u(y)$. The effect of the class information is completely removed from the bound. As a result, the success of the conditional distribution alignment is dominated by the unconditional distribution matching ($D^u$). This is important  for our model because of the unsupervised learning nature for the unseen classes. 
\vspace*{-0.2cm}




\subsubsection{Predictive Model and Feature Augmentation}
\label{sec:3.3.4}

After completing the training of the six modules, a predictive model for classifying unseen examples is trained. This results in a classifier $f: \mathcal{V}^u (\mathcal{\hat{V}}^u ) \times \mathcal{H}^u \times \hat{\mathcal{A}}^u \rightarrow \mathcal{Y}^u$ working in an augmented multi-modal  feature space \cite{narayan2020latent}. Specifically, the used feature vector $\bm x^u $  concatenates the visual features $\bm v^u$ (or $\hat{\bm v}^u $),  the pseudo attribute vector computed by the  regressor $\hat{\bm a}^u=\bm R\left(\bm v^u\right)$ and the hidden representation vector $\bm h^u$ returned by the first fully-conected layer of regressor, which gives  $\bm x^u =\left[\bm v^u, \bm h^u,  \hat{\bm a}^u \right]$.  
%
It integrates knowledge of the generator and regressor that is both transductive, and presents stronger discriminability.



\begin{table*}[thp]
  \centering
  \footnotesize
   \renewcommand{\multirowsetup}{\centering}

   \resizebox{\textwidth}{!}{\begin{tabular}{l|l|cccc|ccc|ccc|ccc|ccc}
      \hline
      \multicolumn{2}{c|}{\multirow{3}{*}{\textbf{Method}}}& \multicolumn{4}{c}{Zero-Shot Learning}& \multicolumn{12}{|c}{Generalized Zero-Shot Learning}\\\cline{3-18}
      \multicolumn{2}{l|}{}&AWA1&AWA2&CUB&SUN&\multicolumn{3}{c}{AWA1}&\multicolumn{3}{|c}{AWA2}&\multicolumn{3}{|c|}{CUB}&\multicolumn{3}{c}{SUN}\\\cline{3-18}
      \multicolumn{2}{l|}{}&T1&T1&T1&T1&S&U&H&S&U&H&S&U&H&S&U&H\\\hline
      \multirow{5}{*}{I}&F-CLSWGAN\cite{xian2018feature}&59.9&62.5&58.1 &54.9& 76.1&16.8&27.5&81.8&14.0&23.9&33.1&21.8&26.3&63.8&23.7&34.4\\
      &SP-AEN\cite{chen2018zero}&-&58.5&59.2&55.4&-&-&-&\underline{90.9}&23.3&37.1&38.6&24.9&30.3&\textbf{70.6}&34.7&46.6\\
      &DEM\cite{zhang2017learning}&68.4& 67.2& 61.9 &51.7& 32.8&84.7&47.3&86.4&30.5&45.1&25.6&34.3&20.5&54.0&19.6&13.6\\
      &ALE\cite{akata2013label}&68.2& -& 60.8 &57.3& 61.4&57.9&59.6&68.9&52.1&59.4&36.6&42.6&39.4&57.7&43.7&49.7\\
      &LisGAN\cite{li2019leveraging}&70.6& -& 61.7 &58.8& 76.3&52.6&62.3&-&-&-&37.8&42.9&40.2&57.9&46.5&51.6 \\\hline
      \multirow{13}{*}{T}&GMN \cite{sariyildiz2019gradient}&82.5&-&64.6&64.3&79.2&70.8&74.8&-&-&-&70.6&60.2&65.0&40.7&57.1&47.5\\
      &DSRL\cite{ye2017zero}&74.7&72.8&56.8&48.7&74.7&20.8&32.6&-&-&-&25.0&17.7&20.7&39.0&17.3&24.0\\
      % &DecGAN&-&-&-&-&-&-&-&-&-&-&68.4&59.1&63.4&44.3&57.2&49.9\\
      &GFZSL\cite{verma2017simple}&48.1&78.6&50.0&64.0&67.2&31.7&43.1&-&-&-&45.8&24.9&32.2&-&-&-\\
      &ALE\_trans\cite{akata2013label}&-&70.7&54.5&55.7&-&-&-&73.0&12.6&21.5&45.1&23.5&30.9&22.6&19.9&21.2\\
      &PREN\cite{ye2019progressive}&-&78.6&66.4&62.8&-&-&-&88.6&32.4&47.4&55.8&35.2&43.1&27.2&35.4&30.8\\
      &f-VAEGAN$^\dagger$ \cite{xian2019f}&-&89.8&71.1/74.2$^*$&70.1&-&-&-&88.6&84.8&86.7&65.1&61.4&63.2&41.9&60.6&49.6\\
      &SABR-T$^\dagger$ \cite{paul2019semantically}&-&88.9&74.0&67.5&-&-&-&\textbf{91.0}&79.7&85.0&\textbf{73.7}&67.2&70.3&41.5&58.8&48.6\\
      &TF-VAEGAN$^\dagger$ \cite{narayan2020latent}&-&92.6&74.7/77.2$^*$&\underline{70.9}&-&-&-&89.6&87.3&88.4&\underline{72.1}&\underline{69.9}&\underline{71.0}&47.1&\underline{62.4}&\underline{53.7}\\
      &GXE\cite{li2019rethinking}&89.8&83.2&61.3&63.5&\textbf{89.0}&\underline{87.7}&\underline{88.4}&{90.0}&80.2&84.8&68.7&57.0&62.3&58.1&45.4&51.0\\
      &LSA$^\dagger$\cite{hanouti2022learning}&-&92.8&\;\;-\;\;/\underline{80.6}$^*$&71.7&-&-&-&86.7&88.5&87.6&-&-&-&\underline{59.5}&46.0&51.8\\
      &SDGN$^\dagger$ \cite{wu2020self}&\underline{92.3}&93.4&74.9&68.4&88.1&87.3&87.7&89.3&\underline{88.8}&\underline{89.1}&70.2&\underline{69.9}&70.1&46.0&62.0&52.8\\
      &STHS-WGAN$^\dagger$\cite{bo2021hardness}&-&\underline{94.9}&\textbf{77.4}&67.5&-&-&-&-&-&-&-&-&-&-&-&-\\\hline
      % TF-VAEGAN+FN+PreTune&-&77.0&71.7&-\\\hline
     T &Bi-VAEGAN$^\dagger$(ours) &\textbf{93.9}&\textbf{95.8}&\underline{76.8}/\textbf{82.8}$^*$&\textbf{74.2}&\underline{88.3}&\textbf{89.8}&\textbf{89.1}&\textbf{91.0}&\textbf{90.0}&\textbf{90.4}&71.7&\textbf{71.2}&\textbf{71.5}&45.4&\textbf{66.8}&\textbf{54.1}\\\hline
          \end{tabular}}\\
  \caption{TZSL performance comparison where the unseen class prior is provided when needed.``$\dagger$" denotes the method that adopts the known unseen class prior assumption. `*' denotes the result is obtained using fine-grained  visual descriptions (AK2 in Section \ref{sec:attribute}) for CUB and the competing results marked by `*' are cited from \cite{hanouti2022learning}. \label{tab:2}}
  \vspace*{-0.3cm}

\end{table*}

\section{EXPERIMENT}

We conduct experiments using four datasets, including  AWA1 \cite{lampert2013attribute}, AWA2 \cite{xian2018zero}, CUB \cite{welinder2010caltech} and  SUN \cite{patterson2012sun}.  Visual features are extracted by the pretrained ResNet-101 \cite{he2016deep}. 
% For the fine-grained CUB dataset, there is a noticeable gap between the pretrained and the desired visual features. To improve this, we pre-tune the pretrained ResNet-101 features using a simple neural network for the CUB dataset, which gives a more distinct cluster structure. 
%
Details on the datasets and implementation details are provided in the supplementary material.
%
We conduct experiments at the feature level following  \cite{xian2018zero,narayan2020latent}. Under the TZSL setup, testing performance of the unseen classes is of interest, for which the average per class top-1 (T1)  accuracy is used, denoted by $\textmd{ACC}^u$ (U).   The ZSL community is also interested in the generalized TZSL performance, i.e., testing performance for both the seen and unseen classes \cite{kong2022compactness,pourpanah2022review,li2022siamese,feng2022non}.  We use the harmonic mean of the average per class top-1 accuracies of the seen and unseen classes to assess it, as  $H = \frac{2 \textmd{ACC}^u\times \textmd{ACC}^s}{\textmd{ACC}^u+ \textmd{ACC}^s}$.
%
Also, all existing results reported in the tables come from their published papers, and ``-" means such result is missing. 





  % \begin{figure}[htp]
  %     \centering
  %     \begin{subfigure}{0.24\linewidth}
  %     \centering
  %     \includegraphics[width=1.15\linewidth]{figures/visualization/AWA1.pdf}
  %     \caption{AWA1} 
  %     \label{fig.3.4}
  %     \end{subfigure}
  %     \begin{subfigure}{0.24\linewidth}
  %     \centering
  %     \includegraphics[width=1.15\linewidth]{figures/visualization/AWA2.pdf}
  %     \caption{AWA2} 
  %     \label{fig.3.1}
  %     \end{subfigure}
  %     \begin{subfigure}{0.24\linewidth}
  %     \centering
  %     \includegraphics[width=1.15\linewidth]{figures/visualization/cub.pdf}
  %     \caption{CUB} 
  %     \label{fig.3.2}
  %     \end{subfigure}
  %     \begin{subfigure}{0.24\linewidth}
  %     \centering
  %     \includegraphics[width=1.15\linewidth]{figures/visualization/sun.pdf}
  %     \caption{SUN} 
  %     \label{fig.3.3}
  %     \end{subfigure}
  %     % \begin{subfigure}{0.24\linewidth}
  %     % \centering
  %     % \includegraphics[width=1.15\linewidth]{figures/visualization/flo.pdf}
  %     % \caption{FLO} 
  %     % \label{fig.3.4}
  %     % \end{subfigure}
  %     \caption{The unseen classes frequency.\label{fig:att}}
  %     \end{figure}
   
\subsection{ Comparison and Result Analysis }

\subsubsection{Known Unseen Class Prior}
We compare performance with both inductive  (I) and transductive (T) state of the arts under the same setting for fair comparison. For TZSL approaches, when class prior of the unseen classes is required, the compared existing techniques assume such information is provided. Therefore,  we first apply the same setting for the proposed Bi-VAEGAN. Table \ref{tab:2} reports the results.  To distinguish from our later results obtained by the proposed prior estimation approach, methods using the provided unseen prior are marked by ``$\dagger$".  Note that we do not report the generalized TZSL performance for STHS-WGAN here. This is because it uses a  harder evaluation setting different from the other approaches by assuming that the unseen and seen data are indistinguishable during training. 
%

It can be observed from Table \ref{tab:2} that in general, the transductive approaches outperform the inductive approaches with a large gap. The proposed Bi-VAEGAN outperforms the transductive state of the arts, particularly the two baseline frameworks F-VAEGAN and TF-VAEGAN that  Bi-VAEGAN is built on, on almost all the datasets. The new state-of-the-art performance that we have achieved is 93.9\% (AWA1), 95.8\% (AWA2),  and 74.2\% (SUN) for TZSL, while 89.1\% (AWA1), 90.4\% (AWA2), and 54.1\% (SUN) for generalized TZSL. Note that for the CUB dataset that has less intra-class clustering property, we find a simple feature pre-tuning network will further boost the performance from 76.8\% to 78.0\% and we put the discussion in the supplementary.
%
It is worth mentioning that for the SUN dataset that is intra-class sample scarce, Bi-VAEGAN achieves satisfactory performance. Learning from the SUN dataset is challenging. This is because the low sample number of each class inherently makes the conditional generation less discriminative. Bi-VAEGAN provides more discriminative features benefitting from its bidirectional alignment generation 
%
and feature augmentation in Section \ref{sec:3.3.4}.
%
% We observe that the unseen accuracy (U) dominates more of the performance gain among methods, as compared to the seen accuracy (S).
\vspace*{-0.2cm}

\begin{table}[th]
  \centering
  \small
   \renewcommand{\multirowsetup}{\centering}
   \begin{tabular}{lcccc}
      \hline
      \multirow{1}{*}{\textbf{Method}}&AWA1&AWA2&CUB&SUN\\\hline
      \multicolumn{5}{l}{\textit{{Non-generative}}} \\

      
      DSRL\cite{ye2017zero}&74.7&72.8&56.8&48.7\\
      GXE\cite{li2019rethinking}&89.8&\underline{83.2}&61.3&63.5\\
      VSC \cite{wan2019transductive}&- &81.7&71.0&62.2\\\hline
      
      \multicolumn{5}{l}{\textit{{Generative with uniform prior}}} \\
      f-VAEGAN$^\ddagger $ \cite{xian2019f}&62.1&56.5&72.1&69.8\\
      TF-VAEGAN$^\ddagger$\cite{narayan2020latent}&63.0&58.6&\underline{74.5}&71.1\\
      % TF-VAEGAN+FN+PreTune&-&77.0&71.7&-\\\hline
      Bi-VAEGAN &66.3&60.3&\textbf{76.8}&\textbf{74.2}\\\hline
      \multicolumn{5}{l}{\textit{Generative with prior estimation}} \\
      Bi-VAEGAN (BBSE) &\underline{90.9}&83.1&72.5&68.4\\
      Bi-VAEGAN (CPE) &\textbf{91.5}&\textbf{85.6}&{74.0}&\underline{71.3}\\\hline
      %   T-VAEGAN+FN+PreTune&88.9 &77.3&70.3&-\\
      \end{tabular}\\
      \caption{Performance comparison in $\textmd{ACC}^u$ for both generative and non-generative techniques using different  unseen class priors. `$^\ddagger$' means our reproduced result.\label{tab:3}}
      \vspace*{-0.6cm}
  \end{table}    
\subsubsection{Unknown Unseen Class Prior}
In this experiment, the unseen class prior is not provided. We compare the proposed prior estimation approach for our method with a naive assumption of uniform class prior and a different approach that treats the prior estimation as a label shift problem and solves it by the black box shift estimation (BBSE) method \cite{lipton2018detecting}. BBSE builds upon the strong assumption that $p_G^u(\hat{\bm v}\mid y) = p^u({\bm v}\mid y)$, while our CPE assumes the cluster structure plays an important role in class prior estimation. 
%
Details on BBSE estimation are provided in the supplementary material.
%
In Table \ref{tab:3}, we compare our results with the existing ones, where, for methods that need unseen class prior, a uniform prior is used. 
%
By comparing Table \ref{tab:3} with Table \ref{tab:2} for the generative methods, it can be seen that, when the used unseen class prior differs significantly from the one computed from the real class sizes, there are significant performance drops, e.g.,  over $30\%$ for the extremely unbalanced AWA2 dataset.  
%
Both BBSE and CPE could provide a satisfactory prior estimation, while our CPE demonstrates consistently better performance.
%
It can be seen from Figure \ref{fig:att_est} that the CPE prior and the one computed from the real class sizes match pretty well for most classes, and for both the unbalanced and balanced datasets. 


\begin{figure}[th]
  \centering
  \begin{subfigure}{0.32\linewidth}
  \centering
  \includegraphics[width=1.1\linewidth]{figures/visualization/AWA1_estimate.pdf}
  \caption{AWA1} 
  \label{fig.5.4}
  \end{subfigure}
  \begin{subfigure}{0.32\linewidth}
  \centering
  \includegraphics[width=1.1\linewidth]{figures/visualization/AWA2_estimate.pdf}
  \caption{AWA2} 
  \label{fig.5.1}
  \end{subfigure}
  \begin{subfigure}{0.32\linewidth}
      \centering
      \includegraphics[width=1.1\linewidth]{figures/visualization/SUN_estimate.pdf}
      \caption{SUN} 
      \label{fig.5.3}
      \end{subfigure}
  \caption{Comparison between the estimated unseen class distribution prior by CPE (orange) and the provided prior computed from the class sizes (gray). \label{fig:att_est}}
  \vspace*{-0.4cm}
\end{figure}



\subsection{Ablation Study}



We perform an ablation study to examine the effect of the proposed  $L_2$-feature normalization (FN), 
the use of an inductive regressor (R) that is trained only with the paired seen data, and the use of the adversarial module (D$^{a}$) in the level-1 transductive regressor training. 
%
We implement a Min-Max normalized f-VAEGAN as the baseline.   
%
Results are reported in Table \ref{tab:4}.
%
One observation is that FN and D$^{a}$ consistently improve performance over four datasets, respectively. 
%
We conclude the following from Table \ref{tab:4}: 
%
(1) The $L_2$-feature normalization is a free-lunch setting, requiring minimal effort but resulting in a satisfactory performance gain.
%
(2) A naive implementation of the inductive regressor is beneficial but somehow limited. The regressor trained solely with the seen attributes provides weak constraints to the unseen generation.
%
(3) The adversarially (D$^{a}$) trained regressor integrates the unseen attribute information and exhibits superiority in the bi-directional synthesis. 


  \begin{table}[t]
      \vspace*{-0.1cm}
      \centering
       \renewcommand{\multirowsetup}{\centering}
  
       \begin{tabular}{ccc|ccc}
          \hline
          \multicolumn{3}{c|}{\textbf{Method}}&\multirow{2}{*}{AWA2}&\multirow{2}{*}{CUB}&\multirow{2}{*}{SUN}\\\cline{1-3}
          FN&$R$&$D^{a}$&&&\\\hline
          \xmark&\xmark&\xmark&91.6&72.1&69.8\\
          \xmark&\cmark&\xmark&92.3(+0.7)&74.3(+2.1)&70.8(+1.0)\\
          \xmark&\cmark&\cmark&95.5(+3.2)&75.8(+1.5)&72.2(+1.4)\\
          \cmark&\cmark&\cmark&\textbf{95.8}(+0.3)&\textbf{76.8}(+1.0)&\textbf{74.2}(+2.0)\\\hline
          \end{tabular}\\
  
      \caption{Ablation study of transductive ZSL results.}
  \label{tab:4}.
  \vspace*{-0.9cm}
  \end{table}
  
  \begin{figure}[th]
      \vspace*{-0.4cm}
      \centering
      \begin{subfigure}{0.48\linewidth}
          \centering
          \includegraphics[width=1.1\linewidth]{figures/visualization/norm.pdf}
          \caption{$L_2$ normalization} 
          \label{fig.4.1}
      \end{subfigure}
      \begin{subfigure}{0.48\linewidth}
          \centering
          \includegraphics[width=1.1\linewidth]{figures/visualization/sigmoid.pdf}
          \caption{Min-Max normalization} 
          \label{fig.4.2}
      \end{subfigure}
  
      \begin{subfigure}{0.48\linewidth}
          \centering
          \includegraphics[width=1.1\linewidth,height=3cm]{figures/visualization/acc.pdf}
          \caption{Training accuracy} 
          \label{fig.4.3}
      \end{subfigure} 
      \begin{subfigure}{0.48\linewidth}
          \centering
          \includegraphics[width=1.1\linewidth,height=3cm]{figures/visualization/f.pdf}
          \caption{Effect of $r$} 
          \label{fig.4.4}
      \end{subfigure}
      \caption{(a) and (b) compare the real and synthesized feature value distributions of AWA2 after $L_2$ and Min-Max normalizations, respectively. (c) compares the TZSL performance observed during the training for the two normalization approaches using the simplified model on AWA2 and CUB. (d) displays the TZSL performance for different radius values used in $L_2$-normalization. \label{fig:norm}}
      \vspace*{-0.5cm}
      \end{figure}    

\subsection{Further Examinations and Discussion}

\paragraph{On $L_2$-Feature Normalization}
\label{sec:norm}
The difference between using the proposed $L_2$-normalization  and the standard Min-Max normalization is the use of Eq. (\ref{V_norm}) or the sigmoid activation in the last normalization layer of the generator $\bm G$.
To demonstrate the difference between the two approaches, we perform a simple experiment, where the network structure is compressed to contain only three core modules $\bm G$, $D$, and $D^u$. The value distributions of real and synthesized visual features after two normalizations are compared in Figures \ref{fig.4.1} and \ref{fig.4.2}. It can be seen that the $L_2$-normalization aligns better with the two feature value distributions, while Min-Max results in a quite significant gap between the two.
%
We investigate further the two approaches by looking into their partial derivatives with respect to each dimension, i.e., for the $L_2$ norm, $\frac{d v_i^\prime}{d v_i} =  \frac{r}{\|v_i\|_2}$, and for the Min-Max, $\frac{d \sigma(v_i)}{d v_i}= \sigma(v_i)(1-\sigma(v_i))$.
  % \vspace*{-0.2cm}
% \begin{align}
%    L_2: &\;  ,\\
%     \textmd{Min-Max:} &\; {\frac{d \sigma(v_i)}{d v_i}}= \sigma(v_i)(1-\sigma(v_i)).
% \vspace{-0.4cm}
% \end{align}
%
The sigmoid feature has a smaller derivative with a larger input magnitude and vice versa. 
%
This causes the sigmoid to skew the activated output towards the middle value e.g., 0.5, and this is not suitable when the feature value distribution is skewed to one side, especially for those features last activated by ReLU.
%
It can be seen in Figure \ref{fig.4.3} that   $L_2$ normalization performs better than Min-Max in terms of higher accuracy and faster convergence in early training. 
%
We examine further the effect of the radius parameter $r$ on different datasets in Figure \ref{fig.4.4}. It is observed that a smaller  $r$ could lead to a more stable performance, while a larger $r$ results in an increased gradient that could potentially cause instability in the training. 


  
  
  \begin{figure}[tp]
      \centering  
  
      \begin{subfigure}{0.49\linewidth}
          \centering
          \includegraphics[width=1\linewidth]{figures/visualization/acc_with_epochs.pdf}
      \end{subfigure}
      \begin{subfigure}{0.49\linewidth}
          \centering
          \includegraphics[width=1\linewidth]{figures/visualization/acc_with_sup_num.pdf}
          \end{subfigure}
      \caption{(a) compares the training accuracies for different auxiliary knowledge. (b) compares TZSL performance with and without the transductive regressor when knowledge AK3 is used.\label{fig:att}}
      \vspace*{-0.58cm}
      \end{figure}


\vspace*{-0.28cm}
     
\paragraph{On Auxiliary Knowledge of CUB} 
\label{sec:attribute}

Auxiliary knowledge plays an overly important role in the success of ZSL, and the motivation of TZSL is to reduce such dependency by learning from unlabelled examples from unseen classes. To examine the effectiveness of our regressor proposed to improve transductive learning and reduce dependency, we conduct experiments using different types of auxiliary knowledge on the CUB dataset.
These include the original 312-dim attribute vectors (AK1), the 1024-dim CNN-RNN embedding from fine-grained visual description \cite{reed2016learning} (AK2), and the 2048-dim averaged visual prototype features of n-shot support (unseen) dataset with label information as assumed in few-shot learning (AK3\#$n$) \cite{li2020adversarial}. 
%
A ground-truth prototype (AK3\#all) is the strongest auxiliary information where the arrival of a conditional alignment is easier. The effectiveness of the prototypes becomes weaker as the number of support examples decreases, where the outliers tend to dominate the generation. AK2 performs similarly well to the ground-truth prototypes, and this indicates that the embeddings learned from a large-scale language model can serve as a good approximation to the ground-truth visual prototypes. 
%
The training accuracies are compared in Figure \ref{fig:att}.(a). 
%
Figure \ref{fig:att}.(b) is produced by removing our transductive regressor (TR) when the model conditions on AK3. It is observed that when TR is not employed, a considerable gap opens up for the less informative prototypes. This demonstrates that TR can improve the model's robustness to auxiliary quality. It is important to note that when the quality is extremely low (as in the AK3\#1 case), alignment in the unseen domain is hard to realize no matter whether the TR module is presented or not.



\begin{figure}[tp]
  \centering
  \begin{subfigure}{0.55\linewidth}
      \centering
      \includegraphics[width=1.04\linewidth]{figures/visualization/column.pdf}
  \end{subfigure}
  \begin{subfigure}{0.43\linewidth}
      \centering
      \includegraphics[width=1.05\linewidth]{figures/visualization/cluster_FA.pdf}
  \end{subfigure}
  \caption{(a) The classification accuracy of different classes in AWA2. `FA' denotes feature augmentation. (b) T-SNE visualization of augmented real/synthesized feature. \label{fig.fa}}
      \vspace*{-0.45cm}
  \end{figure}


\vspace*{-0.3cm}
%Zhicai: Check. I changed the augmented features to concatenated.  If not, you have adjust Sec 3.3.4 to mention augmentation.
\paragraph{On Feature Augmentation} 

The transductive regressor supports distinguishing difficult examples, while the concatenated features based on cross-modal knowledge (see Section \ref{sec:3.3.4}) improve the feature discriminability. Figure \ref{fig.fa}.(a) shows that the regressor leads to a better alignment for the less discriminative classes, such as `bat' and `walrus', and the concatenated features contribute significantly to hardness-aware alignment. Figure \ref{fig.fa}.(b) shows that for the resemble (hard) pairs, such as `walrus' and `seal', the concatenated features are better decoupled in this multi-modal space and the synthesis becomes more discriminative (compare with the visualization in Figure \ref{fig.1}).
  
  



%------------------------------------------------------------------------
\vspace*{-0.17cm}
\section{CONCLUSION}
We have presented a novel bi-directional cross-modal generative model for TZSL. By generating domain-aligned features for the unseen classes from both the forward and backward directions,  the distribution alignment between the visual and auxiliary space has been significantly enhanced. By conducting extensive experiments, we have discovered that $L_2$ normalization can result in a more stable training than the commonly used Min-Max normalization. We have also conducted a thorough examination of how the unseen class prior can affect the model performance and proposed a more effective prior estimation approach, which guarantees the generative approach could still be effective under challenging unknown class prior scenarios. 
 
\section{ACKNOWLEDGEMENT}
This work is mainly supported by the National Key Research and Development Program of China (2021ZD0111802). It was also supported in part by the National Key
Research and Development Program of China under Grant
2020YFB1406703, and by the National Natural Science Foundation of China (Grants No.62101524 and No.U21B2026).
% In this paper, we propose Bi-VAEGAN, which incorporates bidirectional alignment knowledge for the difficult TZSL problem. We propose a transductive regressor that maps visual features back to auxiliary space and improves classification for unpaired unseen data. Furthermore, we discuss the significance of feature preprocessing and class distribution prior for TZSL, and we propose $L_2$ normalization and CPE to address the issues. However, it is still unclear how to understand the underlying mechanism of this cross-modal domain adaptation problem and solve it in the hard prior-unknown case. We hope that this work will inspire other researchers working in this field. 
% \vspace{0.2cm}
%  We achieved competitive results of the image classification task in ImageNet1K compared with other state-of-the-art models. The key element of PosMLP is the relative positional encoding based token-mixing layer which achieves a high parameter efficiency and sample efficiency by explicitly modeling token relations, and we hope this could inspire more works of positional encoding for MLPs.  
% \newpage
% \newpage
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\newpage
\end{document}
