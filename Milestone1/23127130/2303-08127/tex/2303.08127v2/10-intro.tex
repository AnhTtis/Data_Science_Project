\section{Introduction}\label{sec:intro}

Collaborative grounded natural language interactions involve multiple agents, either human or machine, working together to complete tasks while coordinating using natural language.
A key obstacle in studying such scenarios is building the research interaction platform,  a significant design and engineering undertaking. 
This requires building and designing the interaction environment, the task the agents collaborate on, an interface for both machine learning models and human agents, and a process to onboard human agents. 
Each aspect dramatically influences the interaction and language elicited, and is critical to get right. 






We introduce \gamename, a platform for the study of collaborative grounded natural language interaction, and demonstrate its use through the deployment of a learned collaborative natural language agent. 
\gamename largely instantiates the \cerealbar scenario~\citep{Suhr2019:cerealbar},\footnote{\gamename introduces several optional modifications to \cerealbar aimed at richer language and tighter collaboration.} but is implemented from scratch to emphasize research accessibility. 
\gamename is a customizable, scalable, and complete research platform, including server and clients for multi-agent human-machine interactions, tools for real-time data management, and processes to onboard crowdsourcing workers. 








The \gamename scenario poses learning and reasoning challenges, as well as opportunities. 
Comprehending and producing instructions in \gamename requires addressing the symbol grounding problem~\cite{Harnad1990:symbol-grounding-problem}, which is studied extensively in the instruction following~\cite[e.g.,][]{Chen:11, Artzi:13,Misra:17instructions,Fried:17pragmatic-models} and generation~\cite[e.g.,][]{Mei:16generation,Wang2021:generatingInstructions} literature. 
However, the collaborative scenario remains relatively understudied. 
Collaboration is not simply an added complication, but dramatically alters both interaction and learning through joint presence and action. 
It allows the instructor to ad-hoc modify the tasks they delegate based on the follower behavior, potentially recovering from system failures. 
At the same time, this adaptation creates constant distribution shift, a significant generalization challenge. 
Learning is also drastically transformed through collaboration.
The constant engagement of other agents (including humans), the ability to modify delegation strategies, and the shared task-based incentives  bring about within-interaction signals that can be used for continual learning, reducing the dependency on annotated data and enabling model adaptation.



We deploy a demonstration of \gamename with a learned baseline instruction following agent (\autoref{sec:deployment}). 
Players can connect to \gamename and collaborate with our agent or other human agents at \url{https://cb2.ai/}.\footnote{Our deployment has received IRB exemption. All recorded data is anonymized.}
The \gamename platform is available at \url{https://github.com/lil-lab/cb2}.
A video demonstration of \gamename is available at \url{https://youtu.be/tALpX_KKmIw}. 






