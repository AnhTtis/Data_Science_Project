

\section{CB2 Demonstration Deployment}\label{sec:deployment}

We demonstrate the functionality and potential of \gamename via deployment, including collecting a corpus of human-human interaction that we release, training a follower baseline model, and evaluating it in interaction with human leaders. 


\paragraph{Human Games Data}
We follow the crowdsourcing process outlined in Section \ref{sec:crowdsourcing} to collect games between human leaders and followers. 
We collect 185 games containing 3{,}439 instructions.
\autoref{tab:game_stats} provides data statistics. 


\paragraph{Model and Learning}
We train an instruction following model with a behavior cloning objective using the collected human-human data. 
We filter out poor games to improve training data quality, applying heuristics such as removing games where over $20\%$ of instructions are cancelled. 
Our model architecture is based on the Decision Transformer~\cite{chen2021decision, putterman2022pretraining}. 
Follower observations are embedded using \textsc{HexaConv}~\cite{hoogeboom2018hexaconv} because of the hexagonal structure of the map. 
The observations are centered on the follower's position and rotated such that the follower is always facing the same direction. 
This baseline model conditions only on the current instruction for simplicity, similar to the model in \citet{Suhr2019:cerealbar}. In contrast though, it does not assume full observability. 


\paragraph{Results}
We deploy our baseline model as a system demonstration. 
We evaluate it with 188 human-model interactions, conducted side-by-side in a randomized experiment with 187 human-human interactions. 
\autoref{tab:game_stats} shows data and interaction statistics for our training data and final deployments. 
Overall, our models enable effective human-model collaboration in \gamename, but at significantly lower performance than observed in human-human games. 
This is similar to the results of \citet{Suhr2019:cerealbar}, although the numbers are not comparable because of the different environment. 
We also observe the importance of evaluating through deployment, because the behavior of human leaders changes significantly when interacting with our model. 
The vocabulary human leaders use in interactions with the model is smaller compared to when interacting with human followers and the instructions are shorter. 
Our results illustrate the challenge posed by \gamename, and the importance of the kind of deployment \gamename enables. 











