

\section{CB2 Demonstration Deployment}\label{sec:deployment}

We demonstrate the functionality and potential of \gamename via deployment, including collecting a corpus of human-human interaction that we release, training a follower baseline model, and evaluating it in interaction with human leaders. 


\paragraph{Human Games Data}
We follow the crowdsourcing process outlined in Section \ref{sec:crowdsourcing} to collect games between human leaders and followers. 
We collect 185 games containing 3{,}439 instructions.
\autoref{tab:game_stats} provides data statistics. 


\paragraph{Model and Learning}
We train an instruction following model with a behavior cloning objective using the collected human-human data. 
We filter out poor games to improve training data quality, applying heuristics such as removing games where over $20\%$ of instructions are cancelled. 
Our model architecture is based on the Decision Transformer~\cite{chen2021decision, putterman2022pretraining}. 
Follower observations are embedded using \textsc{HexaConv}~\cite{hoogeboom2018hexaconv} because of the hexagonal structure of the map. 
The observations are centered on the follower's position and rotated such that the follower is always facing the same direction. 
This baseline model conditions only on the current instruction for simplicity, similar to the model in \citet{Suhr2019:cerealbar}. In contrast though, it does not assume full observability. 


\paragraph{Results}
We deploy our baseline model as a system demonstration on Amazon Mechanical Turk. 
We evaluate it with 188 human-model interactions, conducted side-by-side in a randomized experiment with 187 human-human interactions. Human leaders are told that they can be matched with a human or a bot follower in the task description, but are not made aware of who they are interacting with in a specific interaction.
\autoref{tab:game_stats} shows data and interaction statistics for our training data and final deployments. 
Overall, our models enable effective human-model collaboration in \gamename, but at significantly lower performance than observed in human-human games. 
This is similar to the results of \citet{Suhr2019:cerealbar}, although the numbers are not comparable because of the different environment. 

Human leaders were able to infer relatively consistently the type of their partner in each interaction. 
This is indicated by differences in the human leader behavior when comparing human-human and human-model interactions. 
For instance, the vocabulary human leaders use in interactions with the model is smaller compared to when interacting with human followers and the instructions are shorter. 
Qualitatively, we observe that instructions in human-human interactions more often use exclamations (e.g., ``oh,'' ``shoot,'' and ``oops'') and informal speech, with abbreviations such as ``btw'' and ``lol'' or words such as ``chill'' and ``kay.'' 
We also found that human leaders in human-human games tend to praise their partners, with words such as ``awesome,'' ``wonderful,'' ``perfect'' or ``great'' appearing uniquely in instructions from human-human games. 
The difference is also seen in game statistics. 
For instance, $16.54\%$ and $12.70\%$ of the times followers and leaders selected a card in human-model games, it was to deselect an already selected card, compared to $8.68\%$ and $8.78\%$ for human-human games. Our results illustrate the challenge posed by \gamename, and the importance of the kind of deployment \gamename enables. 












