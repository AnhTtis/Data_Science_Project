\documentclass{aircc}
\usepackage{mathpartir}
\usepackage{hyperref}
\usepackage{listings}
%\documentclass[12pt]{article}
\usepackage{epsfig,fullpage,multirow,amsmath,amsfonts,latexsym,mathrsfs} % ,enumitem}
\usepackage{times}
\usepackage{graphicx}
\usepackage{float}
\usepackage{bm}
\usepackage{nopageno}
%\bibliographystyle{jasa}
%\usepackage{natbib}

\usepackage{mwe} % for blindtext and example-image-a in example
\usepackage{wrapfig}

%\parindent=0pt\parskip=7pt
%\renewcommand{\baselinestretch}{1.5}
%\renewcommand{\thefootnote}{\fnsymbol{footnote}}
%\renewcommand{\arraystretch}{0.8}
%\newcommand{\todo}[1]{\textbf{[#1]}}
%\newtheorem{defn}{Definition}[section]

%\pdfminorversion=4



\begin{document}

\title{
	%{\sffamily
		%J\fontsize{17}{17}\textbf{AVA MODULAR EXTENSION FOR \\ OPERATOR OVERLOADING
A Deep Learning System for \\
Domain-specific Speech Recognition
			%}}
}

\author{Yanan Jia}
\affiliation{Businessolver \\ \email{\url{yjia@businessolver.com}}}

\maketitle


 
%\footnotetext[1]{Businessolver, Bellevue, WA } 
%\footnotetext[2]{Department of Sociology, The Ohio State University, Columbus, OH, USA} 
%\footnotetext[3]{Email:  calder@stat.osu.edu}

\begin{abstract}   
As human-machine voice interfaces provide easy access to increasingly intelligent machines, many state-of-the-art automatic speech recognition (ASR) systems are proposed. However, commercial ASR systems usually have poor performance on domain-specific speech especially under low-resource settings. The author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models to develop benefit-specific ASR systems. The domain-specific data are collected using proposed semi-supervised learning annotation with little human intervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60 acoustic model with an external KenLM, which surpasses the Google and AWS ASR systems on  benefit-specific speech. The viability of using error prone ASR transcriptions as part of spoken language understanding (SLU) is also investigated. Results of a benefit-specific natural language understanding (NLU) task show that the domain-specific fine-tuned ASR system can outperform the commercial ASR systems even when its transcriptions have higher word error rate (WER), and the results between fine-tuned ASR and human transcriptions are similar.
\end{abstract}

%Motivated by recent advances in transfer learning,
%develop semi-supervised learning annotation to collect domain-specific data, then fine-tune pre-trained DeepSpeech and Wav2Vec acoustic models, develop domain-specific language models to improve the ASR performance for domain-specific speeches.
%benefit-specific acoustic and language models by fine-tuning pre-trained DeepSpeech and Wav2vec acoustic models.
%can be significantly improved using different fine-tuning/transfer-learning strategies.
%. works with

\begin{keywords}
Automatic Speech Recognition,  DeepSpeech2,  Wav2Vec2,   Semi-supervised learning annotation, Spoken language understanding 
\end{keywords}

	
\section{Introduction}
\label{se:intro}
Speech input industrial applications, such as smart voice assistants and customer service voice chat-bots, offer obvious benefits to users in terms of immediacy, touch-free interaction, and convenience via language learning.
Automatic speech recognition (ASR) systems have been deployed as an input method in many successful commercial products and have become a popular human-machine interaction modality.

Building ASR systems typically requires a large volume of training data to cover all possible factors contributing to the creation of speech signals, including but not limited to demographic variety, noise conditions, emotional state, topics under discussion, and the language used in communication. Data has been central to the success of end-to-end speech recognition. At higher-resource conditions, many state-of-the-art ASR pipelines approach or exceed the accuracy of human workers on several benchmarks. However, the ASR performance gap between different datasets remains \cite{Ragni2014DataAF}. Most acoustic models transfer poorly to domain-specific speech especially under low-resource settings \cite{hsu}.
 
%it remains challenging for domain-specific speeches especially under low-resource settings \cite{hsu}.
%  For  commercial ASR systems  
 
DeepSpeech2 (DS2) and Wav2Vec2  are popular pre-trained acoustic models (AMs).  ASR performance on domain-specific speech can be significantly improved by fine-tuning pre-trained AMs on the in-domain data along with external in-domain language models (LMs). 
%ASR performance can be significantly improved using different fine-tuning or transfer-learning strategies.
However, even for fine-tuning, to achieve improved accuracy, large volumes of high-quality annotated data are required.  %Data augmentation is a particular form of approach that can be applied to train speech recognition systems with limited resources by increasing the quantity and diversity of the available data.

The data resources used for this experiment consist of an artificial dataset which is created in laboratory settings by human domain experts  with clean transcripts and real-world call data collected from a benefit service center that needs pre-processing and annotation. To annotate this large amount of unsupervised call data, a semi-supervised annotation method is proposed.  

 
 %Semi-supervised learning annotation %and various synthetic data augmentation schemas expand training data is used to annotate the call dataset. 
Working with the DS2 and Wav2Vec2 applications, we develop employee benefit-specific ASR systems, and compare their  speech transcription performance with AWS and Google commercial ASR systems. Finally, we evaluate the viability of using error prone ASR transcriptions as part of spoken language understanding (SLU) via one downstream  benefit-specific natural language understanding task - intent classification based on outputs generated from different ASR systems. 
%and in the process observe that public validation and test sets performance is predictive of the transfer performance of
%a model to real data.


%The performance gains are observed from fine-tuning/transfer-learning, the use of semi-supervised annotation for a large amount of unsupervised call data.  %, and appropriate augmentation schemas. 
%It is also compared to the results carried out by AWS and Google APIs.


This paper is organized as follows: Section~\ref{se:data} introduces the domain-specific dataset used to build ASR systems along with a semi-supervised annotation method. %, along with data augmentation schemas to expend the training data. 
Section \ref{se:models} presents the methodologies for acoustic, language and punctuation models.  Section \ref{se:experiment} evaluates experimental results across different types of speech. Section \ref{se:discussion} concludes the paper and outlines future work. 






\section{Training Data \label{se:data} }
In this paper, we target and use data collected from a health care benefit call center (named BSCD) which are focused on customers looking for help or support with company provided benefits such as health insurance. 

%We used the Kaldi toolkit [21] to extract 80-dim log Mel-filter bank plus 3-dim pitch features and normalized them. The training data contained around 120k ut- terances, and the exhaustive enumeration process described in Sec- tion 3.1 resulted in 1.7M training samples.
 
 
%Benefits service calls tend to be harder to recognize than lab recording data. 


%In this paper, we target and use  real world data -  benefits service calls, which tends to be harder to recognize than artificial datasets which are recorded in laboratory settings.

%A common issue that arises from the use of limited resources in deep learning systems is robust parameter estimation. 
%A range of approaches can be applied to address robustness issues. 

\subsection{Dataset \label{subse:data} }
To gain significant improvement from fine-tuning AMs, large volumes of high-quality annotated data are needed.  Unfortunately, manual transcription of large datasets is a time-consuming and expensive process, requiring trained human annotators and substantial amounts of supervision. In addition, this manual process cannot guarantee 100\% error-free transcription and the calls that are selected to be annotated may not contain benefit terms, and thus are not guaranteed to be useful for the domain-specific fine-tuning. 

Based on real users' chat-bot and call data, a set of representative utterances are created as  a `script' by experts. 13 people with a variety of backgrounds recorded data in quiet environments. However, this process is still time consuming and labor-intensive. In addition, the recorded artificial datasets lack  real-world variability. % such as different speakers, background noise, environment, and microphone hardware.

%still need a lot of time and human effort to get enough data. In addition, it misses the real-world variability and noises. 
%However, annotating speeches can be a very time consuming and labor-intensive process. 
 
The BSCD call center is a rich resource of benefit-specific acoustic data which covers diverse topics, all demographic varieties and noise conditions. 
It is feasible to collect vast amounts of unsupervised acoustic datasets which lack  correct transcriptions. To label unsupervised datasets with little  human intervention, a semi-supervised annotation method is proposed in section \ref{subse:annot}. 

%This process can bootstrap larger datasets, and reduce system development cost. 


%with prone to error transcripts
 

%  by labeling this type of unsupervised data with  little human intervention. %,  . 
%500 calls are collected from the call center database covering diverse topics, such as insurance plan information, insurance id card, dependent coverage, etc. The call dataset has female and male speakers randomly selected with their age ranging approximately from 16-80. The dataset contains 160 hours of telephone speech.
%Rather than relying on small, supervised artificial recording speeches, to bootstrap larger datasets, and reduce system development cost. 
 


%\subsubsection*{BSCD: Benefits Service Call Dataset}\label{subsubse:calldata}

%chatbot data usually are short and clean. 

%The main dataset is the service calls collected from a health care benefits call center (named BSCD). Calls are focused on customers looking for help or support with company provided benefits such as health insurance. 






 

%section \ref{subse:annot} introduced semi-supervised learning annotation for unsupervised data.
%A robust computational model of ASR needs to be able to handle real-world variability and noises. 


%Calls involving translators are eliminated to keep only speakers expressing themselves in English. 
%All calls are pre-processed to eliminate repetitive introductions. The beginning of each call contains an introduction of the users' company name by a robot. To address this issue, the segment before the first pause (silence duration $>$ 1 second) is removed from each call. 

%The splitting approach can generate a training set with shorter utterances and few erroneous transcriptions.  It splits long audio into shorter segments, aligns the segments with the corresponding transcripts, and filters out segments that have a high likelihood of inaccurate aligned transcripts. 

%\subsubsection*{Recording Data} \label {subsubse:recorddata}



%The amount of supervised data is 12 hours. 

\subsection{Data Pre-processing \label{subse:dataprep} }
In practice, most acoustic model training procedures expect that the training data comes in the form of relatively short utterances paired with associated transcripts, since long utterances have higher cost than short utterances in speech recognition\cite{pmlr-v48-amodei16}. 

All the recording audio files have duration less than 15 seconds.  However,  the call dataset ranges from several minutes to more than hours. The lengths of the call audio files make it impractical to train,  % a sequence model.
so all the calls are split into shorter segments based on silence. Silence is defined as anything under -43 dBFS with duration longer than 800ms. 
%Filter out too short or too long segments. 
%, and keep all the recording files with duration varying from 1.5 seconds  to 15 seconds. 

All the audio files including call segments and recording data are presented in wav format, resampled to 16k Hz. We only keep audio files with duration in the range of 1.5 seconds  to 15 seconds, since short audios usually do not contain any useful information, while long audios have much higher cost during training. %Half seconds silence are added in front of each call segment audio files.
 
%The median numbers of word length are 7  and 20  in each recoring script and call transcripts seperatedly. 
%The median numbers of character length are 40  and 20  in each recoring script and call transcripts seperatedly. 

All the utterance scripts that are used to fine-tune the acoustic models should have the same character inventory as the pre-trained acoustic models which contains only alphabetic letters. The following filtering rules are applied to all the utterance scripts: 

%The vocabulary of pre-trained acoustic models DS2 and Wav2Vec2 contains only alpha-beta letters, hence 



\begin{itemize}
   % \item Utterances contain only numbers are removed. 
	\item All target text is converted to lowercase.
    %\item All partial words ending in ’-’ are marked as non- lexical items.	
    %\item Words joined with hyphen '-' are marked as non - lexical items. 
    \item Punctuation markers not pronounced in speech are eliminated (e.g. Words joined with hyphen '-' are marked as non hyphen lexical items).	
    %A hyphen is a bit of punctuation used to join two (or more) different words.
	%(keep .   since \$10.50)	
	\item Abbreviations are replaced with corresponding full-word forms based on the content (e.g. Carla Dr Athens becomes carla drive athens, Dr Pepper becomes doctor pepper). %Take 'Dr' for example, if the utterance includes locations, 'Dr' is replaced with drive, otherwise, it is replaced with doctor 

	\item Numeric strings and symbols that \textit{are} pronounced in speech are replaced with orthographic strings and words:
	%such as dollar amounts, dates, numeric quantities, street numbers, phone numbers
	\begin{itemize}
		\item Specific terms with numbers or symbols are replaced with specific orthographic strings (e.g. 401k becomes four o one k,  ad\&d becomes a d n d).  %1095c becomes ten ninety five c,
		\item Numeric strings starting with punctuation marker \$ are dollar amounts (e.g. \$50 becomes fifty dollars, \$20.45 becomes twenty dollars forty five cents).
		\item Numeric strings ending with punctuation marker \% are numeric quantities (e.g.  50\% becomes fifty percent).
		\item Ordinal strings usually are dates (e.g. 21st becomes twenty first).
			\item Numbers with one, two or three digits are usually read as  cardinal numbers (e.g.  22 becomes twenty two; 156 becomes one hundred fifty six).
		\item   Four-digit numbers between 1930 and 2030 are considered as years (e.g. 2022 becomes two thousand twenty two).
		\item  Numbers with four or more digits (outside of the year range)  are usually street numbers, phone numbers, or social security numbers (e.g.  4680 becomes four six eight zero).
	\end{itemize}
	\end{itemize}
	%\item All the utterances that still includes numeric strings are removed 

%    \item Numeric straings such as dates, dollar amonts, numeric quantities, street number, phone numbers are replaced with orthographic strings. (e.g. 401k becomes four o one k, 1095c becomes ten ninety five c)
%   \item Punctuation markers \%, \& , \$ are converted as 'percentage', 'and', 'dollor'. (e.g. \$50 becomes fifty dollars, 100\% becomes one hundred percent, )

% All partial words ending in ’-’ were marked as non- lexical items.
%All punctuation marks such as ’.’, ’,’, ’!’ and ’?’ were eliminated
%all the specidic terms contain numbers such as 401k, 1095c are converted to thewords 'four o one k', 'ten niety five c'.
%all the alphabeta numbers such as year, street number, phone numbers  are converted to letter. 
%Some terms have different ways to say it， 

Mistakes can occur when converting scripts to alphabetic letters. For example, year 2022 can be said as 'two thousand twenty two'  or 'twenty twenty two'. Utterances with mistakes can be eliminated via the semi-supervised learning annotation method described in section \ref{subse:annot}. %For test data, human transcribers help correct the mistakes. For those mistakes caused by script converting,
 

% Speech to Text  engines are used to transcribe the recordings. 
 

%All three corpora were conditioned in the same manner, replacing numeric strings (dates, dollar amounts, numeric quantities) with orthographic strings (e.g. June 1996 becomes June nineteen ninety six, \$250 becomes two hundred and fifty dollars), replacing abbreviations with corresponding full-word forms (e.g. Dr. Roberts becomes Doctor Roberts, Pivet Dr. becomes Pivet Drive), and 

%Here only about 10 hours of transcribed acoustic data are available.  The training set for our acoustic models consists of 262 hours of Switchboard 1 audio with transcripts provided by Mis- sissippi State University,  contains 1000 hours of speech sampled at 16 kHz containing million utterances. Audio files are resampled to 16kHz prior to the featurization. 

%An audio augmentation technique with low implementation cost. Speed perturbation, which emulates both VTLP and tempo perturbation, is shown to give more WER improvement than either of those methods. 
%The amount of supervised data is 12 and 14 hours for Assamese and Zulu respectively.

%A large amount of transcribed training data is usually needed to enable accurate speech recognition.  For our system we need many recorded utterances and corresponding English transcriptions, but there are few public datasets in the related domain % sufficient scale. 


\subsection{Semi-supervised learning annotation \label{subse:annot} }
%The call center is a rich resource of audio data.  A large number of calls are recorded daily in order to assess the quality of interactions between representatives and customers. It is feasible to collect vast amounts of unsupervised acoustic datasets covering all demographic variety and noise conditions. However, acoustic model development for ASR systems relies on the availability of large amounts of supervised data manually transcribed by humans. %Even for Wav2vec which trained on the unsupervised audios, 
%Even for fine-tuning, to achieve improved accuracy, large volumes of high-quality annotated data are required.  Unfortunately, manual transcription of large datasets is both time-consuming and expensive, needs trained human annotators and substantial amounts of supervision.
%
%
%In our experiments, recording audios are supervised data with clean transcription, however, the amount of the recording data is relatively small comparing to the amount of the unsupervised call data we can get from the call center.
%Rather than relying on small, supervised training sets, semi-supervised annotation is proposed to bootstrap larger datasets. It can adopt massive quantities of new data with minimum human annotation effort.  Semi-supervised annotation can be achieved with different components. Figure\ref{fig:dataprep} illustrates the main components of semi-supervised annotation process in ASR system that can be implemented to bootstrap new data collection. 


%%%%%%%%%%%%%
%Traditionally, acoustic model development for ASR systems relies on the availability of large amounts of supervised data manually transcribed by humans. Even for fine-tuning, to achieve improved accuracy, large volumes of high-quality annotated data are required. Unfortunately, manual transcription of large data sets is both time-consuming and expensive, requiring trained human annotators and substantial amounts of supervision. 



%The unsupervised data refers to data that could have incorrect transcriptions. The rapid increase in the amount of multimedia content on the Internet makes it feasible to collect vast amounts of unsupervised acoustic datasets covering all demographic variety and noise conditions.  



Rather than relying on small, supervised training sets, a semi-supervised annotation method is proposed to bootstrap larger datasets, and reduce system development cost.  Semi-supervised annotation can be achieved with different components.  Figure \ref{fig:dataprep} illustrates the main components of a semi-supervised annotation process in an ASR system that can be implemented to bootstrap new data collection.  


 The process takes as input a set of labeled examples $D_{L} = \{ X_L, Y_L\}$ and produces recognizer committee  $\boldsymbol{C}$ which is used to generate the rough transcriptions $Y^{\prime}_{U}$   % $\boldsymbol{Y}_U$ 
  for a larger set of unlabeled examples $X_{U}$. After a few filters, a relatively small set of newly labeled data $D_{U}(f)= \{X_{U}(f), Y^{\prime}_{U}(f) \} $ are adopted as part of  $D_{L}$ to retrain the system. The process is cyclical and iterative as every step is repeated to continuously improve the accuracy of the system and achieve a successful algorithm.  



Recognizer committee $\boldsymbol{C}$ can include existing ASR systems or offline recognizers trained or fine-tuned using labeled audio data $D_{L}$. We employ DS2 and Wav2Vec2 models fine-tuned on $D_{L}$ along with AWS and Google commercial ASR systems to form recognizer  committee  $\boldsymbol{C}$ which automatically generates rough transcriptions $\boldsymbol{Y}_U  =  \{ Y_U^{\text{AWS} }, Y_U^{\text{Google}}, Y_U^{\text{DS2}} , Y_U^{\text{Wav2Vec2}}    \} $ for a massive amount of unannotated audio data $X_U$.   



To cull salient utterances $X_U(f)$ with associated transcript $Y_U^{\prime}(f)$ from large unsupervised call data $X_U$,  we first need to find the most accurate transcript $Y_U^{\prime}$ from $\boldsymbol{Y}_U$.
In this step, a new metric `relative  error rate' is proposed.


 

\begin{figure*}
	\centering
	\includegraphics[scale=0.38]{images/semi_sup_v2.pdf}
	\caption{Semi-supervised learning annotation pipeline}
	\label{fig:dataprep}
\end{figure*}


For each unlabeled audio file $x_{u}(i)$  and associate transcript $y_{u}^{j}(i)$, the relative word error rate $wer^{jk}(i)$ and relative character error rate $cer^{jk}(i)$ are calculated by setting $y_{u}^{j}(i)$ as the target text and the transcript  $y_{u}^{k}(i)$ generated from  recognizer $k$  as the prediction transcript, where  $j$=$\{$AWS, Google, DS2, Wav2Vec2$\}$, and $k$ can be any one of the other three recognizers in the committee $\boldsymbol{C}$. 
 %where $k$ = $\{AWS, Google, DS2, Wav2Vec2\} - j $. 
The average $\bar{wer}^j(i) = \frac{1}{3} \sum_{k} wer^{jk}(i) $ and    $\bar{cer}^j(i) = \frac{1}{3} \sum_{k} cer^{jk}(i) $ are defined as  relative word error rate and relative character error rate  of  $y_{u}^{j}(i)$.  


We normalize relative $\bar{wer}$ and  $\bar{cer}$ separately, %get $norm(\bar{wer}^j(i))$ and  $norm(\bar{cer}^j(i))$,  
then calculate the weighted arithmetic mean to get relative error rate $\bar{er}^j$ for all the four ASR systems separately. % For audio file  $x_{u}(i)$,  
The `true' transcript for  $x_{u}(i)$ is set to be $y_{u}^{\prime}(i)$ with minimal relative error rate  $ \bar{er}^{\prime}(i) =min\{ \bar{er}^{\text{AWS} }(i), \bar{er}^{\text{Google} }(i), \bar{er}^{\text{DS2} }(i), \bar{er}^{\text{Wav2Vec2} }(i)  \} $. The label $Y_U^{\prime}$ are mixed transcripts produced from all the four different recognizers. 

% $min( \bar{er}^j(i))=$ 
% $\boldsymbol{y}_u^{\prime} $
%Then select top N utterances with the smallest $\bar{er}$ to fine-tune the acoustic model.  The first approach is based on the error rate measure $\bar{er} $.

A subset $D_U(f_1) = \{X_U(f_1), Y^{\prime}_U(f_1)\}$ is selected from $D_U = \{X_U, Y^{\prime}_U\}$ using an empirically determined threshold. An audio file  $x_{u}(i)$ with large relative error rate usually has loud background noise, overlap, heavy accent, or slurring which tends to lead to incorrect transcription ${y^{\prime}_u(i)}$.   Upper thresholds for  $\bar{wer}^{\prime}(i) $, $\bar{cer}^{\prime}(i)$ and  $\bar{er}^{\prime}(i) $ are empirically determined to make sure $x_{u}(i)$ has  accurate transcript. While $x_{u}(i)$ with relatively large $\bar{wer}^{\text{DS2}}(i) $, $\bar{cer}^{\text{DS2}}(i)$ and $\bar{wer}^{\text{Wav2Vec2}}(i) $, $\bar{cer}^{\text{Wav2Vec2}}(i)$ is useful to correct the wrong transcriptions generated by DS2 and Wav2Vec2 separately. So, lower thresholds for $\bar{wer}^{\text{DS2}}(i) $, $\bar{cer}^{\text{DS2}}(i)$ and $\bar{wer}^{\text{Wav2Vec2}}(i) $, $\bar{cer}^{\text{Wav2Vec2}}(i)$ are also determined. 
%the lower threshold are set to be  $\bar{wer}^j(i) > 0.15$ and $\bar{cer}^j(i) > 0.1$, $\bar{er}^j(i)> 0.006$. The upper bound threshold are set to be   $\bar{wer}^j(i) > 0.5$ and $\bar{cer}^j(i) > 0.4$, $\bar{er}^j(i)> 0.015$ to move awhere $j$=$\{\text{DS2, Wav2Vec2} \}$
Utterances that fall into the determined thresholds are kept  in $D_U(f_1) = \{X_U(f_1), Y^{\prime}_U(f_1)\} $.  

The determined thresholds and the recognizers in   $\boldsymbol{C}$ may vary for each iteration. When the error rates of fine-tuned ASR outputs are low enough, we can remove the commercial ASR systems from  $\boldsymbol{C}$  or replace them with free ASR systems to reduce system development cost.


 

%Among the top N selected utterances, the label $y_u^{\prime}(i)$ come from all the four different recognizers and the AWS has the most of them. By checking the results of a set of samples manually , AWS recognizer generates the most accurate transcripts for different difficulty levels of the input audios $X$,  so for the second approach, we set $Y^{\prime}_U$ =$Y^{\text{AWS}}_U$ directly. Then compare $Y^{\prime}_U$ to the transcripts generated from the other three recognizers to produce  $wer^{\prime}(i)$ and $cer^{\prime}(i)$. Normalize $wer^{\prime}$ and $cer^{\prime}$.


Selecting utterances by error rate threshold alone may lead to a disproportionate training dataset with frequent tokens over represented. To alleviate this problem, frequency detection can be implemented as one component of the pipeline. A threshold is determined to limit the number of  similar utterances which contain a set of the same key words in the datasets and make sure the filtered dataset $D_U(f_2)$  is diverse and has a larger vocabulary. % In this experiment, an intent classifier and name entity model are used to balance the frequent tokens.}  



Another constraint on data selection is the audio duration or character lengths. In this experiment, we only take audios with duration of 1.5 to 15 seconds as training data.  After transcription, utterances with character lengths less than 18 are also removed. For those eliminated utterances that are either too short or too long, reconstruction techniques can be used to reconstruct new audio segments to meet the length requirements. 


The semi-supervised annotation approach directly relies on the transcripts generated by speech recognizers and the thresholds determined by experiments to cull sufficient quality data without any reliance on manual transcription \cite{semi}.


%\begin{table*}
%	\centering
%	\resizebox{0.98\textwidth}{!}{
%		\begin{tabular}{l|cc|ccccc} 
%			\hline
%			Dataset & Type & Duration & Augmentation & Hours & Total Words & Unique Words & Nb. Speaker/call\\ \hline
%			\multirow{3}{*}{Recording} &   Clean  &   \multirow{3}{*}{1.8h} & 
%			VTLP & 1.8h & 13k  & 1.2k & 13\\ \cline{4-8} 
%			& Read & & Noise & 1.8h & 13k  & 1.2k & 13 \\ \cline{4-8} 
%			& Supervised & & speed  & 1.8h & 13k  & 1.2k & 13 \\ \hline
%			\multirow{2}{*}{Call} &  Real-world   &   \multirow{2}{*}{57.6h} & 
%			Semi1-Top11k&  15.4h & 176.7k  & 4.4k & 814 \\ \cline{4-8} 
%			&  Unsupervised &    & 
%			Semi2-AWS01& 16.4h & 181.5k & 4.3k  & 836  \\  \hline
%	\end{tabular}}
%	\caption{Data Summary}
%	\label{table:fusresults}
%\end{table*}

%recording total\left( 
%1556  files 
%12 speakers
%chracter length,  mean 45 median 40
%word length, mean 8.3 median 7
%total word 13019    1195 unique word 
%hard test:  accent , noise,  not clear 


%%%%%%%%%
%The process takes as input a set of labeled examples $D_{L} = \{ X_L, Y_L\}$ and produces committee recognizers $C$ which are used to generate the rough transcriptions $\boldsymbol{Y}_U$  for a larger set of unlabeled examples $X_U$. After a few filters, a relatively small set of newly labeled data $D_{U(c)}= \{X_{U(c)}, Y^{\prime}_{U(c)} \} $ are adopted as part of labeled data $D_{L}$ to retrain the system. The process is cyclical and iterative as every step is repeated to continuously improve the accuracy of the system and achieve a successful algorithm. 
%
%Committee recognizers $C$ can include offline recognizers trained or fine-tuned using labeled audio data $D_{L}$ or existing speech recognizers. In our experiemtns,  fine-tuned DS2  and Wav2Vec2 models based on $D_{L}$ along with AWS and Google Recognizer APIs form committee recognizers $C$ which automatically generate rough transcriptions  $\{\boldsymbol{Y}_U \} =  \{ Y_U^{\text{AWS}}, Y_U^{\text{Google}}, Y_U^{\text{DS2}} , Y_U^{\text{Wav2vec2}}    \} $ for a massive amount of unannotated audio data $X_U$. 
%%One call segment $x_i$ has four associated transcripts ${y_i^{J\prime}}$ where $j$= \{AWS, Google, Wav2Vec2, DS2\} generated from committee recognizers $C$. 
%
%To cull salient utterances $X_U(c1)$ with their assosiated transcipt $Y_U^{\prime}(c1)$ from large unsupervised call data $X_U$ to fine-tune acoustic models, two approchs are proposed. 
%



%the 'true' transcript ${y_u(i)}^{\prime}$ = $y_{u}^{j}(i)$ whith $min( \bar{wer}_i^j) $. Thresholds are empirically determined, 


% $\boldsymbol{y}_{u}(i) $, 
%Set where . Then caculate the wer and cer of the transcripts generated from the other three recognizers.  Then take average get, $\bar{wer}^j(i)$ and $\bar{cer}^j(i)$,  then take normalization of them   ,  caculated a Weighted arithmetic mean $0.4 \bar{wer}^j(i) + 0.6\bar{cer}^j(i)$.    $Norm(\bar{wer}^j(i) )$


%the 'true' transcript ${y_u(i)}^{\prime}$ = $y_{u}^{j}(i)$ whith $min( \bar{wer}_i^j) $. Thresholds are empirically determined,  utterances with $\bar{wer}_i^j > 0.2$  and $\bar{cer}_i^j > 0.1$ are eliminited.  ${y_i^{j\prime}}$  and get
%Utterances $D_{U(c1)}^{\prime}$ meeting the confidence threshold are accepted with corresponding labels.  


%${y_i}^\prime$(DeepSpeech), ${y_i}^\prime$(Wav2Vec), ${y_i}^\prime$(AWS), ${y_i}^\prime$(Google).

%
%\begin{figure}
%	%  \centering
%	\includegraphics[scale=0.4]{images/p_aws_trans.pdf}
%	\caption{Percentage of AWS transcripts retained in the top N utterances with the smallest error rate}
%	\label{fig:paws}
%\end{figure}
%
%\begin{figure}
%	%  \centering
%	\includegraphics[scale=0.4]{images/n_er_trans.pdf}
%	\caption{Number of  utterances retained in the training data}
%	\label{fig:ner}
%\end{figure}
%
%AWS has the most accurate results .  so we propose the other approach to get target text. 



%Figure:  percentage of unsupervised data retained for 

%Thresholds are empirically determined,

%wav_all_sub = wav_all[ (wav_all['AWS_ds_cer']<0.4) | (wav_all['AWS_ds_wer']<0.4) |  (wav_all['AWS_google_cer']<0.4) | (wav_all['AWS_google_wer']<0.4) ]


%
%However, selecting utterances by $wer_i^{\prime}(j)$ and $cer_i^{\prime}(j)$ alone may lead to a disproportionate training dataset with frequent tokens overrepresented. To alleviate this problem, frequency detection are implemented as one component of the pipeline. The utterances with the same transcripts are limited to one. The duplicated utterances are queries that easy to recognize, such as 'okey', 'thank you', 'bye' and auto replies such as 'your call is important to please send the line and the agent will be with you shortly'.
%This process will limit the number of the same utterances in the dataset and make sure the filtered dataset $D_{U(c2)})$  is diverse and has a larger vocabulary.  
%
%Another constraint on the data selection is the audio duration or character lengths. This step is done in data-preprocession.  Utterance $x_i$ with dutation between 1 second and 10 seconds are kept in $D_{U(c3)})$.

%Short audios usually do not contain any useful information. Long audios have much higher cost during training. At Businessolver, we only take audios with duration between 1s and 15s as training data. After transcription, utterances with too short or too long character lengths are also removed.  At Businessolver, we keep utterances with the length of characters between 3 and 150. 

%Figure  shows the percentage of unsupervised data retained for semi-supervised training at different error rate threshold values. With threshold 0.2,   half of the unsupervised data  retained. 






%Some instrumental models \cite{6356906} have been developed to predict the speech quality automatically. Those models can be applied to filter audio data based on the overall speech quality. 


%Rather than relying on small supervised training sets, semi-supervised  annotation are proposed to   bootstrap larger datasets,  and reduce system development cost.  The semi-supervised approach is used in many areas\cite{Lamel2002LightlySA} \cite{jia-2020-deep} to increase the effective amount of labeled data. 

%In ASR system, semi-supervised approach employes existing speech systems to bootstrap new data collection. 
%Multiple speech recognizers are used to generate rough transcriptions for a massive amount of unannotated audio data. Then filtering out those utterances that fail to pass confidence threshold and retraining the system on supervised and newly adopted unsupervised data. This approach directly rely on the transcripts generated by speech recognizers and the threshold the author set to filter sufficient quality.data. 

%Based on the study purpose. The range of the audio duration should be set. Remove audios too short or too long. 

%Our average utterance duration is 3.3 seconds, and we start by randomly selecting 50M utterances from our log stream. We filter this set by removing utterances with transcripts shorter than a threshold which is usually 10 characters, but is adjusted on a per-language basis to account for e.g., Asian language character sets. This filtering is based on our empirical observation that confidence scores are less reliable in these cases. After that, we keep the 2M top utterances by confidence.
%Our LMs are trained on corpora composed of different sources, such as books, web documents, search queries and automatically transcribed speech logs.


%Figure: Main components of the proposed method
%Fig. 2 shows the components of the proposed method. We start by extracting 300M utterances, or more than 30 years of speech data, randomly selected from the logs, and redecode them with an offline recognizer. We improve transcription quality in this recognizer in two ways. First we relax many of the decoder pa- rameters such as beam width and number of active arcs during search to reduce expected word error rate at the cost of increas- ing runtime. Secondly we use a larger language model. We replace the usual production 15M n-gram model by a 100M n- gram language model. In our experiments we have explored even larger sized n-gram language models (up to 1B n-grams) but haven’t observed any significant improvements over the 100M n-gram model.

% However, selecting training corpora by confidence score alone may bias the data selection towards frequent and easy to recognize queries. We observed that this can lead to a dispro- portionately large fraction of the training set having the same transcription. Very popular queries such as ’facebook’, ’mi- crosoft’, ’google’, or ’youtube’ can dominate the resulting train- ing set resulting in a negative feedback loop.

% To alleviate this problem we have experimented with a number of techniques. For example in [12] an additional constraint is introduced to minimize the cross entropy between the CD state distribution of the selected training corpora and a manually transcribed testing set. In the present work we opt for a simpler approach that enforces data diversity and is much more straightforward to implement. We limit the number of utter- ances in the corpus with the same transcription to 20, an empir- ically determined threshold, an approach we refer to as “tran- scription flattening”. This simple idea enforces a more uniform triphone coverage and a larger vocabulary, while also ensuring that frequent tokens are not over represented.
%We also apply additional constraints on the data selection, such as removing utterances with short transcripts as described in Section 2.1. Finally, we retain the top 20M utterances with the highest confidence scores. 


%Figure 2: Percentage of unsupervised data retained for semi- supervised training at different confidence threshold values


%Using confidence scores, transcript length and transcript flattening heuristics designed to cull salient utterances from three decades of speech per language, we then carefully select training data sets consisting of up to 15K hours of speech to be used to train acoustic models without any reliance on manual transcription. 

%This redecoding pass results in a new corpus of transcripts with a substantial reduction in expected word error rate. 
%Our next step is to select 20M utterances within this 300M set, a ten-fold increase in training set size over the baseline ap- proach. In this selection we try to guarantee correct transcrip- tions while preserving the acoustic and linguistic diversity of the data. To guarantee good quality in the transcriptions we rely on utterance level confidence metrics derived from lattice poste- rior word probabilities [11]. Experimentally we have found that utterance confidence measures correlate extremely well with word error rate.
%Typically utterances in the 90th percentage confidence bin exhibit word error rates below 10\%. In our studies we have found this to be close to the performance of human transcribers.



%
%\subsection{Data Synthesis}\label{subse:augdata}
%
%%Large-scale deep learning systems require an abundance of labeled training datasets to achieve good and robust performance, but it may not always be feasible. 
%Data synthesis  is a common strategy adopted to increase both the quantity and diversity of available training data.  %Synthesis datasets by transforming inputs in a way that does not change the label is crucial.
%%Synthesis data is useful for small datasets where the number of examples is low.
%%Speech synthesis schemes such as noise superposition, vocal tract length perturbation (VTLP), speed perturbation have been proposed as effective techniques 
%The synthesized data is obtained by perturbing the original data via different techniques, such as corrupting clean training speech with noise, so the correctness of the corresponding transcripts is generally guaranteed. In addition, like unsupervised case, it is feasible to collect vast amounts of synthesized data, but a major disadvantage of synthetic data is its quality.
%
%%have been proposed as effective techniques 
%
%In our experiment, we tried three schemes: noise superposition, vocal tract length perturbation (VTLP), and speed perturbation on the recording data. The three schemes are easy to adopt and have a low implementation cost. 
%
%Since the call data already contains variate intrinsic noise, and perturbation may generate unrealistic distortions, the synthesized schema only implemented on the clean recording data which are recorded in library setting. 
%
%\begin{itemize}
% \item  \textit{Noise superposition}\cite{ds1}: Each noisy audio $\sigma_i$ superposed to the clean recording data $x_i$ should be unique, since the acoustic model may memorize repeated noise and ‘subtract’ it out of the synthetic data. 
%
% \item \textit{VTLP}\cite{Jaitly2013VocalTL}:  Using a  randomly generated warping factor  {$\alpha_i$} ranged between 0.9 and 1.1 to warp the frequency dimension of the spectrograms of $x_i$.
%
% \item \textit{Speed perturbation}\cite{Ko2015AudioAF}: Using a  randomly generated warping factor  {$\alpha_i$} ranged between 0.9 and 1.1 to
%creates a change in the duration and  the number of frames.  
%\end{itemize}
%
%
%
%%
%%Note, the noisy audio $\sigma(t)$ needs to be unique, since the acoustic model may memorize
%%repeated noise and ‘subtract’ it out of the synthetic data.  The best proportion of noise augmentation utterances are dataset specific. Too much noise augmentation can lead to worse results. 
%
%%There are numerous techniques how data can be perturbed. 
%
%
%%The data augmentation is a common strategy adopted to increase the quantity of training data, avoid overfitting and improve robustness of the models. This approach has an important theoretical advantage of being able to produce data when real examples are not available [8]. It is very useful for small datasets where the number of examples is low.
%
%%n vision systems, some image processing operations, such as translation, rescaling, rotation, or color normalization, do not affect the class label of the object in the image but are yet able to create new related data examples. Augmenting datasets by transforming inputs in a way that does not change the label is crucial.
%
%%Data augmentation has been highly effective in improving the performance of deep learning in computer vision. The same strategy has also been shown to improve automatic speech recognition (ASR) systems.
%
%%Speech synthesis schemes, such as noise superposition, vocal tract length perturbation (VTLP), speed perturbation are  popular approaches to expand the potential training data even further.
%
%
%% corrupting clean training speech with noise was found to improve the robustness of the speech recognizer against noisy speech. With deep neural network (DNN) based acoustic modeling, vocal tract length perturbation (VTLP) [3], has shown gains on the TIMIT phoneme recognition task. 
%%In [5, 6] the use of data augmentation on low resource languages, where the amount of training data is comparatively small (∼ 10 hrs), was investigated. In [5] multiple data augmentation schemes were combined.
%
%%Data augmentation has been highly effective in improving the performance of deep learning in computer vision [39, 56, 14]. This has also been shown to improve speech systems [21, 26]. 
%
%
%%\subsubsection*{Variable Noise Superposition}
%%Superimposing clean audio $x(t)$ with a noisy audio $\sigma(i)$ can synthesize noisy audio $ \hat{x} (t) = x(t) + \sigma(t)$. 
%%Although the original data may contain some intrinsic noise, corrupting clean training speech with variable noise can increase variety of noise and improve the robustness of the speech recognizer against noisy speech.
%%
%%Note, the noisy audio $\sigma(t)$ needs to be unique, since the acoustic model may memorize
%%repeated noise and ‘subtract’ it out of the synthetic data.  The best proportion of noise augmentation utterances are dataset specific. Too much noise augmentation can lead to worse results. 
%
%%\subsubsection*{Vocal Tract Length Perturbation (VTLP)}
%%One way of perturbing data is to transform spectrograms, using a randomly generated warp factor $\aleph$ to warp the frequency dimension, such that a frequency $f$ is mapped to a new frequency $f{\prime }$ via a function of  $\aleph$.
%%
%%Traditionally, the warp factor is assumed to lie between 0.8 and 1.2. Since the goal is to augment the training database and high warps or low warps may generate unrealistic distortions, a smaller range of 0.9 and 1.1 are recommended.  
%
%
%%to 90\% and 110\% of the original rate.
%%producing 3 versions of the original signal with speed factors of 0.9, 1.0 and 1.1.
%%modifying the speed to 90\% and 110\% of the original rate.
%%The synthesised data 
%
%%We augmented speech data by changing the speed with factors of 0.9, 1.0, and 1.1, which results in 3-fold data augmentation. We found this is important to stabilize E2E-ST train- iSnpge.cAugment Time and frequency masking blocks are randomly applied to log mel-filterbank features. This has been originally proposed to improve the ASR performance and shown to be effective for E2E-ST as well (Bahar et al., 2019b).
%
%
%%Two sets of warping factors, {0.9, 1.0, 1.1} and {0.9, 0.95, 1.0, 1.05, 1.1}, are used to create 3 and 5 copies of the original feature vectors, respectively. These two sets of training data were used to train two different DNN systems, which are tagged as 3-fold and 5-fold systems in the comparison.
%
%
%%\subsubsection*{Speed Perturbation}
%%One proposed data augmentation technique is to change the speed rate of the audio. It is easy to adopt and has a low implementation cost. [4] shows speed perturbation give more WER improvement than VTLP method.
%%
%%Multiple versions of the original audio can be produced with different warping factors  {$\alpha_i$}, which usually ranged between 0.9 and 1.1. Given an audio $x(t)$, speed perturbation by a factor α gives a new audio x(αt) which creates a change in the duration and  the number of frames. 
%
%
%%Semi-supervised learning annotation can construct large high quality unsupervised sets to train ASR systems. A variety of schemas can be easily used to generate synthetic data, including but not limited to noise superposition, vocal tract length perturbation (VTLP), speed perturbation, and audio reconstruction. 
%It should be noted that the synthetic data generated via perturbations can only create examples of the same phoneme contexts as the seed audios. In low resource settings, semi-supervised learning annotation would be more useful since it can adopt examples of different phoneme contexts.  



\section{Models \label{se:models}}

In this section, we explain the acoustic models, language models and punctuation models used in the ASR systems. 
%Pre-trained acoustic models, language models, punctuation models and the      format are introduced. 
\subsection{Acoustic Model \label{subse: acousitc} }

Compared to training a powerful AM from scratch, fine-tuning a pre-trained AM can be cheap in terms of data and training efforts. Since our experiment focuses on the employee benefit domain, we can improve the ASR performance through fine-tuning   pre-trained AMs on domain-specific datasets. Two popular open source speech-to-text engines: DS2 and Wav2Vec2 are employed. % Both are employed as pre-trained AMs.
%

The core of DS2 is a recurrent neural network (RNN) trained with the Connectionist Temporal Classification (CTC) loss \cite{ctc}. It directly maps spectrograms to graphemes, and consists of 2 convolutional layers, 5 bidirectional RNN layers and a fully connected layer. 
DS2 is trained on 11,940 hours of labeled speech data containing 8 million utterances of English speech obtained from public sources and internal Baidu corpora. 
Noise superposition augmentation is employed to expand the training data even further.  Hundreds of hours of noise are added to 40\% of the utterances that are chosen at random to improve  robustness to noisy speech.

%widely-used architecture that directly maps speech features to graphemes and is trained using the Connectionist Temporal Classification (CTC) loss 

%Comparing to conventional ASR systems that train a powerful acoustic model (AM) from scratch, Fine-tuning can be cheap in terms of data and training efforts. 

%We chose DeepSpeech2 as our end-to-end ASR model. This is a widely-used architecture that directly maps speech features to graphemes and is trained using the Connectionist Temporal Classification (CTC) loss (Graves et al.,2006)
%This model is trained on 960 hours of US-accented speech obtained from the Librispeech corpus (Panayotov et al., 2015). All subsequent experiments use this pretrained model, which we will refer to as DS2.


%The core of DS2 is a recurrent nerual network (RNN) trained to ingest speech spectrograms and generate text transcriptions. 

%After learning to read and write, most humans can transcribe speech with robustness to variation in environment, speaker accent and noise, without additional training for the transcription task. To meet the expectations of speech recognition users, we believe that a single engine must learn to be similarly competent; able to handle most applications with only minor modifications and able to learn new languages from scratch without dramatic changes. 

% Our goal is to eventually reach human-level performance not only on specific benchmarks, where it is possible to improve through dataset-specific tuning, but on a range of benchmarks that reflects a diverse set of scenarios.

%We use the standard technique of data-parallelism to train   Our most common configuration uses a minibatch of 512 on 8 GPUs. 
%We use layer dropout with p = 0.2 and target embed- ding dropout with p = 0.1 
%we set the peak learning rate to 0.001 and total batch size to 512, and train using 32 Google Cloud TPU chips. 
%During training we apply a dropout [19] rate between 5\% - 10\%. We apply dropout in the feed- forward layers but not to the recurrent hidden activations.
%
%These models provide state of the art performance on various LVCSR tasks. Hence they provide a strong baseline to verify the gains due to the proposed data augmentation technique. 


Wav2Vec2 is a fully convolutional model which is trained on large amounts of unlabeled audio data. It takes raw audio  as input and outputs latent speech representations which are input to the transformer.
%$\mathbf{X}$
%$\mathbf{Z}$ 
%computes a general representation that can be input to acoustic models to improve the model training.
%speech recognition system to improve acoustic model training.  
After pre-training on unlabeled speech, the model can be fine-tuned on labeled data with a CTC loss to be used for downstream speech recognition tasks.  
Wav2Vec2 applies self-supervised pre-training to improve supervised speech recognition. It reduces the supervised data needed in real-world scenarios.

Three pre-trained Wav2Vec2 models are used in this experiment: BASE, LARGE, and LARGE-LV60. The
BASE and LARGE models have different transformer configurations setup. 
The BASE model contains 12 transformer blocks with 8 attention heads each, while the LARGE model contains 24 transformer blocks with 16 attention heads each. The BASE and LARGE pre-trained AMs are trained on  960 hours of unlabeled Librispeech audio (LS-960) \cite{7178964}. The LARGE-LV60 pre-trained AM uses audio data from LibriVox (LV-60k) and follows the pre-processing of \cite{libri} resulting in 53.2k hours of audio. 


%It is trained by predicting speech units for masked parts of the audio. 
%They are then fed to a Transformer g : Z 7→ C to build representations c1, . . . , cT capturing information from  the entire sequence [9, 5, 4]. 
%which takes as input raw audio X and outputs latent speech representations z  which are input to the Transformer (



%in addition to semi- and weakly-supervised learning techniques to 

\subsection{Language Model \label{subse:language}}
%Language Models (LMs) play an essential role in ASR. 
%Even the end to end (E2E) models that implicitly integrate LM into optimization can benefit from LM fusion and achieves higher performance.
Acoustic models can learn to produce readable character-level transcriptions; however, the acoustic model outputs tend to contain errors which occur on phonetically plausible renderings of English words including substitution of phonetically similar words, or misspellings due to irregularities in a language’s orthography.  Thus, integrating ASR systems with an external language model trained from  domain-specific text can further improve the ASR performance. 
A 5-gram KenLM  is trained on 10 million words of cleaned domain-specific documents  to decode the emissions from the acoustic model. A scorer is also trained as an external language model for DS2. The scorer is composed of a KenLM  and a trie data structure containing all words in the vocabulary.

%The labeled training data is small compared to the size of unlabeled text corpora that are available.
%Common errors include substitution of phonetically similar words, or misspellings due to irregularities in a language’s orthography, the latter of which may be addressed by using phone labels in place of ASR output. 


%For  Domain-specific terms are not common in general. Domain-specific language model can further improve the WER. 

 
%WER improves when we supplement our system with a language model trained from external text.
% WER can be improved when we supplement our systme with a language model trained from external text. 

%we integrate our system with an N-gram language model since these models are easily trained from huge unlabeled text corpora. For comparison, while our speech datasets typically include up to 3 million utterances, the N-gram language model used for the experiments in Section 5.2 is trained from a corpus of 220 million phrases, supporting a vocabulary of 495,000 words.4

%I%ndeed for many of the transcriptions, the most likely char- acter sequence predicted by the RNN is exactly correct without external language constraints. The errors made by the RNN in this case tend to be phonetically plausible renderings of English words— Table 1 shows some examples. 

%Many of the errors occur on words that rarely or never appear in our training set. In practice, this is hard to avoid: training from enough speech data to hear all of the words or language constructions we might need to know is impractical. Therefore, we integrate our system with an N-gram language model since these models are easily trained from huge unlabeled text corpora. For comparison, while our speech datasets typically include up to 3 million utterances, the N-gram language model used for the experiments in Section 5.2 is trained from a corpus of 220 million phrases, supporting a vocabulary of 495,000 words.4

%Language model probability can be used as a feature to assist in choosing between hypotheses. We consider a 5-gram KenLM language model (cite )  as decoder /scorer. 



%is considered to decode the emissions from the acoustic model. It is trained on 10 million words of cleaned domain-specific documents. 

%the vocabulary formed with 
%the most frequently used 30,000 words from  
%words of cleaned training transcripts and       words   lines   domain-specific documents.  
%cleaned text ( Sofia training, call transcription, chatbot data) 
%The most frequently used 30,000 words from 10 million words forms the vocabulary, which produces a language model with about 850 million n-grams.  

 





%a Kneser-Ney smoothed 5-gram model with pruning is trained using the KenLM toolkit [28] 

%For decoding the emissions from the acoustic model we use a lexicon as well as a separate language model trained on the WSJ language modeling data only. We consider a 4-gram KenLM language model (Heafield et al., 2013), 


%All decodings are done with an 85K word vocabulary and a 4-gram language model with 36M n-grams. 

%For English, our language model is a Kneser-Ney smoothed 5-gram model with pruning that is trained using the KenLM toolkit [28] on cleaned text from the Common Crawl Repository3. The vocabulary is the most frequently used 400,000 words from 250 million lines of text, which produces a language model with about 850 million n-grams.  

%The language model can and should have additional variations beyond what you have recorded audio for. 

%A 4-gram language model (LM) is trained2 on 3M words of the training transcripts,  A trigram LM is trained3 on 700K words of the training transcripts.


%During inference we search for the transcription y that maximizes Q(y) shown in Equation 12. This is a linear combination of log probabilities from the CTC trained network and language model, along with a word insertion term [26].
%Q(y) = log(pctc(y|x)) + α log(plm(y)) + β word_count(y) (12)
%The weight α controls the relative contributions of the language model and the CTC network. The weight β encourages more words in the transcription. These parameters are tuned on a development set. We use a beam search to find the optimal transcription %[27].

%Parameter Tuning. We find cascaded model perfor- mance can be impacted significantly by model settings such as beam size and choice of ASR target preprocessing.

%Q(c) = log(P(c|x)) + α log(Plm(c)) + β word count(c)
%where α and β are tunable parameters (set by cross-validation) that control the trade-off between the RNN, the language model constraint and the length of the sentence. 

%we use a 5-gram language model for the decoding. Only the most common 495,000 words are kept, the rest remapped to an UNKNOWN token.

\subsection{Punctuation Model \label{subse:punctuation}}


State-of-the-art speech recognition models
%automatic speech recognition (ASR) systems 
still produce raw word streams which  do not contain punctuation marks and capitalization.  Unformatted text is often difficult to read — not only for humans, but also for natural language processing tools. 
The presence of punctuation and capitalization can greatly improve the readability of automatic speech transcripts. 

A bidirectional recurrent neural network model with attention mechanism trained on 40M words of unsegmented text is used to restore 
missing inter-word punctuation marks: comma, period, question mark, exclamation mark, dash, colon, and semicolon \cite{tilk2016}. %These are by far the most frequent punctuation marks, so predicting these correctly will have a significant impact on the readability of the generated text. Other symbols are either be ignored or collapsed them with these three. 
%. The punctuation model focuses on the prediction of punctuation

After inserting punctuation symbols and correctly capitalizing all words in ASR output, numbers and symbols addressed in section \ref{subse:dataprep} are also reversed to remove dysfluencies. 
We limit the problem scope to specific terms with numbers and symbols (e.g. `w two' becomes `w2', `covid nineteen' becomes `covid-19', `a d n d' becomes `ad\&d'). 
%and   easily identified numbers (e.g. dates, digits). 
%The chatbot text and commercial ASR outputs usually includes symbols besides alpha-beta letters for easy reading. 



%estoring punctuation symbols and capitalization in ASR output in English.
%Specific terms    401K，1095C,   company names with specific symbols. 
%Words to number 


%During training, replacing punctuation characters with corresponding word tokens (e.g. ‘,’ becomes ‘,COMMA’).  %After training,  analyze the transcription mistakes and look for patterns — are mistakes focused on a particular word/phrase, an accent type, an environment, or a device? You can add additional recordings to train acoustic models to attempt fixing any accuracy gaps.


%nvolve cascading automatic speech recognition (ASR), text normalization (e.g., punctuation insertion, case restoration), 


%State-of-the-art automatic speech recognition (ASR) systems still produce raw word streams that, even with no recognition errors, are often difficult to read—not only for humans, but also for natural language processing tools, which usually ex- pect formatted text as input. Issues that need to be addressed include formatting numbers, dates and places, removing dis- fluencies, inserting punctuation symbols, and correctly cap- italizing all words. 
%In this work, we limit the problem scope to restoring punctu- ation symbols and capitalization in ASR output in English.
%Our objective is to insert both punctuation and capitalization at once, in one single pass. 

%We have presented an approach to punctuation and capitalization restoration using purely text-based n-gram language models. 

%The output of automatic speech recognition (ASR) systems typically does not contain punctuation marks. However, the presence of punctuation marks can largely increase readability of the generated text and help downstream language processing applications such as natural language understanding or machine translation. In dictation systems, the user experience can be improved if punctuation marks get automatically inserted and do not need to be dictated.





\section{Experiment \label{se:experiment}}

In this section, we empirically investigate the performance of the fine-tuned AMs along with external LMs on domain-specific audio data which contains high quality recorded artificial data and real telephone calls.   %different combinations of data. 
To better assess the real-world applicability of our speech system, we also evaluate a natural language understanding task -  intent detection  - on error prone transcripts generated by different speech recognition systems. 



%We performed two sets of experiments to evaluate our system. In both cases we use the model described in Section 2 trained from a selection of the datasets in Table 2 to predict character-level transcriptions. The predicted probability vectors and language model are then fed into our decoder to yield a word-level transcription, which is compared with the ground truth transcription to yield the word error rate (WER).

%We report Word Error Rate (WER) for the English system and Character Error Rate (CER) for the Mandarin system. In both cases we integrate a language model in a beam search decoding step as described in Section 3.8.
%we show the performance of our Mandarin system on internal datasets that reflect real-world product scenarios.
 

 
\subsection{Experiment Setup \label{subse:setup}}
In this experiment, we used 57.6 hours of company internal call center data along with 1.8 hours of recording data  discussed in section \ref{subse:data}. The call center data is filtered down to  21.1 hours of call segments using the semi-supervised annotation method proposed in section \ref{subse:annot}. The training data statistics are summarized in Table 1. Compared to the recording data, the call segment data are easier to collect, and cover more unique words, diverse speakers, and more numbers of words/characters in each audio file. 

%. In general, each recording data have less words/characters than call segments. % However, since recording has longer silence  at the beginning and end of the audio files. 

The test set contains  95 recording and 907 call segment audio files with approximate size 10 minutes and 75 minutes respectively. The call test data are randomly selected from company internal call segments, which covers  general utterances such as greetings and farewell. To better evaluate the speech system for benefit-specific audios, we  also created a subset of the call test data by selecting only those call segments  containing benefit specific terms, which yielded 565 audio files with  approximate size 48 minutes. 

All the test audios are manually  annotated by two annotators. They transcribe the same audio clip, that is typically about 6s (seconds) long with a range from 0.8s to 27.8s. A third annotator selects the better of the two transcriptions as ground truth to produce WER and CER. 

%for the final WER/CER calculation. They are free to listen to the audio clip as many times as they like. The hand-transcribed results are treated as ground truth to produce a WER/CER.  
 
  \begin{table*}
 	\centering
 	\resizebox{.9\textwidth}{!}{
 		\begin{tabular}{l|cccccccc} 
 			\hline
 			\multirow{2}{*}{Data} &  Total & Training &  Total  & Unique  & Nb. Speakers  & Median &   Median   &  Median  \\  
 			&  Duration &  Duration   & Words & Words & or calls & Words & Characters & Duration \\ \hline
 			Recording &    1.8h & 1.8h & 13k  & 1.2k & 13 &  7  &   40 &   3.9s\\ 
 			Call &    57.6h &  21.1h &  246k & 5.8k & 864  &    11  &  56 & 3.7s \\    \hline
 	\end{tabular}}
 	\caption{Recording  and call segments training data statistics. }
 	\label{table:fusresults}
 \end{table*}  
 

The four pre-trained acoustic models discussed in section \ref{subse: acousitc} are fine-tuned on the 23 hours of labeled speech data described in Table \ref{table:fusresults} with different numbers of epochs to reach a stable validation error rate and  different minibatch numbers to avoid out of memory issues.
The learning rate is chosen from  $ [ 1 \times 10^{-4}, 6 \times 10^{-4}  ] $ to yield fastest convergence. %and annealed by a constant factor of 1.2 after each epoch. We use a momentum of 0.99 for all models.
The external KenLM and scorer described in section \ref{subse:language} are trained for Wav2Vec2 and DS2 acoustic models separately on the same domain-specific text corpus.  


% \footnote{  Wav2Vec2-Base, DS2: 30 epochs and 64 minibatch; Wav2Vec2-Large: 20 epochs and 32 minibatch; Wav2Vec2-Large-LV60: 10 epochs and 8 minibatch }.  

%The external KenLM language models described in section \ref{subse:language} are trained for DS2 and Wav2Vec acoustic models seperately with the same 
%The language models used are those described in Section 3.8.
% The decoding parameters from Equation 12 are tuned on a held-out development set. We use a beam size of 500 for the English decoder


 
 

  
% \begin{table*}
% 	\centering
% 	\resizebox{1.2\textwidth}{!}{
% 		\begin{tabular}{l|cccccccc} 
% 			\hline
% 			Dataset &   Duration & Duration  &  Total Words & Unique Words & Nb. Speaker  & Med Word &   Med Character  &  Med Duration\\ \hline
% 			Recording &    1.8h & 1.8h & 13k  & 1.2k & 13 &  7  &   40 &   3.9s\\ 
% 			Call &    57.6h &  21.1h &  246k & 5.8k & 864  &    11  &  56 & 3.7s \\    \hline
% 	\end{tabular}}
% 	\caption{Data statistics of domain-specific speech datasets. }
% 	\label{table:fusresults}
% \end{table*}
  
  

  
   
  	
  	
  	
 %, split into 138K ut- terances, which were translated via crowdsourcing by Post et al. 
 
% 
% \begin{table}
% 	\begin{center}
% 		\begin{tabular}{l|c|c|c}   
% 			\hline
% 			Data Type &  Ave/Med Word &  Ave/Med Character  & Ave/Med Duration
% 			\\ \hline
% 			Recording & 8.3/7  &  45/40 &  4.2s/3.9s \\ \hline
% 			Call &   14.7/11  & 72.8/56 & 4.8s/3.7s\\
% 			\hline
% 		\end{tabular}
% 	\end{center}
% 	\caption{Data statistics of domain-specific speech datasets.}
% 	\label{tab:table1}
% \end{table}
 

%The relative improvements gained from different pre-trained acoustic models and external language models on different test datasets are shown in table \ref{table: systemresults}

%The model was finally tested on a new, live-talking test dataset that contains recordings of real telephone calls. 

 

%and the increased training corpus size are presented, and different pre-trained models. 



%\subsection{Data Summary}

%Since we are particularly interested in how our fine-tuned models affect the performance on the domain-specific audios, the main part of the test data 



%The speakers in the corpus were ranked according to the WER of the WSJ model’s transcripts, and were divided roughly in the middle, with the lower-WER speakers designated as “clean” and the higher- WER speakers designated as “other”. From the “clean” pool, 20 male and 20 female speakers were drawn at random and assigned to a development set. The same was repeated to form a test set. For each dev or test set speaker, approximately eight minutes of speech are used, for total of approximately 5 hours and 20 minutes each. Note that, as mentioned in Section 2.4, we use a different segmentation procedure for development and test data, than for training data.+
%The rest of the audio in the “clean” pool was randomly split into two training sets with approximate size 100 and 360 hours respec- tively. For each speaker in these training sets the amount of speech was limited to 25 minutes, in order to avoid major imbalances in per-speaker audio duration.


%We evaluate the proposed method on the AISHELL-1 dataset [20], which contains 170 hr Mandarin speech.



%
%%methods will affect training across differently-resourced conditions, we compare results us- ing randomly selected 40 hour and 20 hour subsets of the data.
%%Chat audio data . Known talked to a robot, so usually short, clean, more like the recording ones. no chitchats.   

\subsection{Results Evaluation \label{subse:result} }
The relative improvements gained from four pre-trained AMs and external LMs across three audio test sets are shown in table \ref{table: systemresults}. Speech recognition performance gains are observed by switching the  use of pre-trained model from DS2 to Wav2Vec2. These improvements are achieved without increasing fine-tuning data size. For Wav2Vec2 models, we observe that the use of larger pre-trained model leads to reductions in the WER and CER of the system.  For all the four fine-tuned acoustic models, an external LM improves the WER across all the test sets, with the largest relative WER improvement for the DS2 and Wav2Vec2-Base models and the smallest gain for Wav2Vec2-LARGE model.   
 
\begin{table}
	\centering
	\resizebox{0.75\textwidth}{!}{
		\begin{tabular}{l|cc|cc|cc} 
			\hline
			\multirow{2}{*}{System} &  \multicolumn{2}{c|}{Clean Recording} & \multicolumn{2}{c|}{Call} & \multicolumn{2}{c}{Call-Benefit}   \\  \cline{2-7} 
			&  WER & CER  & WER & CER & WER & CER   \\ \hline
			 DeepSpeech2  & 0.292  & 0.092 &  0.390    & 0.183 & 0.353 & 0.154 \\ \hline   
			Wav2Vec2-BASE  &     0.096 & 0.030    & 0.257  &0.129 & 0.205&  0.094\\ \hline
			Wav2Vec2-LARGE  &    \textbf{0.056} & \textbf{0.023}   & 0.194  &0.102 &0.152 & 0.075\\ \hline
			Wav2Vec2-LARG-LV60  & 0.069  & \textbf{0.023}  &  \textbf{0.189}  & \textbf{0.094}  &   \textbf{0.150}& \textbf{0.068}  \\ \hline
			DeepSpeech2 +LM 	& 0.063  & 0.028  &  0.262& 0.171  & 0.210 &  0.130\\ \hline 
			Wav2Vec2-BASE + LM & 0.057  & 0.024 & 0.165    & 0.098   &0.124 & 0.069 \\ \hline
			Wav2Vec2-LARGE + LM  & 0.043  & \textbf{0.021}& 0.154   & 0.093  &0.120 & 0.069 \\ \hline
			Wav2Vec2-LARG-LV60 + LM  &  \textbf{0.039}  & \textbf{0.021}  &  \textbf{0.138} & \textbf{0.083}  & \textbf{0.105} & \textbf{0.059} \\ \hline
			%AWS API& 0.1446  & 0.0356 &  0.129  &   0.0814 & 0.113& 0.060 \\ \hline
			%Google API & 0.075 & 0.026  &   0.260  &0.183  & 0.219 & 0.146  \\ \hline
	\end{tabular}}
	\caption{Comparison of WER/CER of  fine-tuned ASR systems across three  audio test sets. Best performance  bolded by column. }
	\label{table: systemresults}
\end{table}

% wav2vec2-xls-r-300m  wer: 0.206  cer: 0.098
 
%\begin{table}
%	\centering
%	\resizebox{0.8\textwidth}{!}{
%		\begin{tabular}{l|cc|cc|cc} 
%			\hline
%			\multirow{2}{*}{System} &  \multicolumn{2}{c|}{Clean Recording} & \multicolumn{2}{c|}{Call} & \multicolumn{2}{c}{Call-Benefit}   \\  \cline{2-7} 
%			& no LM & LM   & no LM & LM & no LM & LM   \\ \hline
%			\multirow{2}{*}{DeepSpeech2} & 0.292  & 0.092 &  0.390    & 0.183 & 0.353 & 0.154 \\ \cline{2-7}
%			& 0.063  & 0.028  &  0.262& 0.171  & 0.210 &  0.130\\ \hline  
%			Wav2Vec2-BASE  &     0.096 & 0.057   & 0.257  &0.165 & 0.205&  0.124\\ \hline
%			Wav2Vec2-LARGE  &    0.056 & 0.043   & 0.194  &0.154 &0.152 & 0.120\\ \hline
%			Wav2Vec2-LARG-LV60E  & 0.069  & \textbf{0.039}   &  0.1897 & 0.138 &   0.150&  0.105 \\ \hline
%			AWS API& 0.1446  &  &  0.129  &     & 0.113&  \\ \hline
%			Google API & 0.075 &   &   0.260  &   & 0.219 &    \\ \hline
%	\end{tabular}}
%	\caption{Comparison of WER for 6 speech systems across three  audio testsets. Best performance bolded by column. }
%	\label{table: systemresults}
%\end{table}


\begin{table}
	\centering
	\resizebox{0.51\textwidth}{!}{
		\begin{tabular}{l|ccc } 
			\hline
			 System  &  Clean Recording  & Call  &  Call-Benefit  \\ \hline
			AWS API& 0.1446  &    \textbf{ 0.129}  &      0.113  \\ \hline
			 Google API & 0.075 &     0.260  &     0.219     \\ \hline
			% DeepSpeech2 + LM  &  0.063 &  0.262 &  0.210 \\ \hline
		%	Wav2Vec2-BASE + LM  &      0.057   & 0.165 &  0.124\\ \hline
		%	Wav2Vec2-LARGE + LM  &     0.043   &0.154  & 0.120\\ \hline
			Wav2Vec2    & \textbf{0.039}   & 0.138 &   \textbf{ 0.105} \\ \hline
	\end{tabular}}
	\caption{Comparison of WER  of three ASR systems across three audio test sets. Best performance bolded by column. Error rates are reported only for utterances with predictions given by all systems. }
	\label{table: systemresults3}
\end{table}

%Error rates are reported only for utterances with predictions given by all systems.
 
 
The transcripts carried out by  AWS and Google ASR systems are used as reference for  comparison. The commercial ASR outputs usually include symbols besides alphabetic letters for easy reading. To calculate WER, one human annotator follows the filtering rules in section \ref{subse:dataprep} and changes the commercial ASR outputs to  alphabetic letters only transcripts.   Three call segments with Google output `UnknownValueError' are removed from the test data. %Error rates in table \ref{table: systemresults3} are reported only for utterances with transcriptions given by all systems.
WER of the best fine-tuned ASR outputs (Wav2Vec2-LARG-LV60 + LM) along with two commercial ASR outputs across three audio test sets are reported in table \ref{table: systemresults3}.


For the general call test set, the AWS ASR system produces the lowest WER, but for the domain-specific audios including the recording test set and call-benefit test set, the best fine-tuned Wav2Vec2 model  outperforms the AWS ASR system, especially for recording data which contains many benefit specific terms. 


%There is little room for a generic speech system to further improve on clean read speech without further domain adaption

%we are able to better our per- formance (2.5\% WER on test-clean and 5.8\% WER on test- other), improving the current state of the art on test-other by 22% relatively. 


%. The relative WER improvement given by the language model is small for the Wav2Vec2-LARGE model and large for the DeepSpeech and Wav2Vec2-Base models. 

%Significant performance gains  are observed when we switch the pre-trained model from DeepSpeech to Wav2vec2. 


%drops from 48\% to 36% 

%We can observe that the use of an improved auto generated reference transcript leads to reductions in the word error rate of the system.

%resulting in significant performance gains.
%a gain of 1.0 \% was observed. These improvements were achieved without increasing the number of training epochs, and suggest that data transformations should be an important component of training neural networks for speech, especially for data limited projects.


%This paper also discussed various ways to exploit this data in tandem and hybrid architectures. 
%Speech recognition performance gains were observed from the use of both scheme, with the combined scheme yielding largest gain only for Zulu. Keyword search results showed that gains are also possible, however, the use of semi-supervised training yielded mixed results in this case suggesting sensitivity of the approach to the accuracy of training data transcriptions.











			%Wav2Vec2-LARGE +LM2  & 0.171/0.167 & 0.083/0.058 & 0.196/0.152    & 0.119/0.091  & &  \\ \hline
			%Wav2Vec2-LARGE +LM3  & 0.175/0.167 & 0.081/0.056 & 0.196/0.154    & 0.119/0.091  & &  \\ \hline
			
% The number in parentheses next to each dataset, e.g. Clean (94), is the number of utterances scored.

%\begin{table*}
%	\centering
%	\resizebox{0.8\textwidth}{!}{
%		\begin{tabular}{l|cc|cc|cc} 
%			\hline
%			\multirow{2}{*}{Text} & Nb. Words &  5-gram Unique words & \multicolumn{2}{c|}{Wav2Vec2}   & \multicolumn{2}{c}{DeepSpeech2}         \\ \cline{2-7} 
%			& &    &WER    & CER     & WER      &  CER           \\ \hline
%			None   & &   & 0.586& 0.846   & 0.858 & 0.871\\ \hline
%			Specific Doc.    & &   & 0.525  &0.846 &   0.858  & 0.871        \\ \hline
%			Doc. +  bigtext  & &   &0.531& 0.800  &0.790 &0.818\\ \hline
%			bigtext    & & &0.927  &0.896  &0.946  & 0.933      \\ \hline
%	\end{tabular}}
%	\caption{Binary classification of sentiment polarity on both linguistic and acoustic modalities}
%	\label{table:lmresults}
%\end{table*}





%the dataset is much smaller than the one we present here, with around 100 hours of English speech, and suffers from major gender and per- speaker duration imbalances.
%Data table:   Utterances (train test  number)   Duration (train test  hours)  Error  WER CER 
%Table 1: Data statistics of accented speech datasets. Duration is approximated to hours and WER/CER refer to the test error rates for each accent using DS2.



%data selection and corpus partitions
%The size of the corpus makes it impractical, or at least inconvenient for some users, to distribute it as a single large archive. Thus the training portion of the corpus is split into three subsets, with approximate size 100, 360 and 500 hours respectively. A simple automatic procedure was used to select the audio in the first two sets to be, on average, of higher recording quality and with accents closer to US English. An acoustic model was trained on WSJ’s si-84 data subset and was used to recognize the audio in the corpus, using a bigram LM estimated on the text of the respective books. We computed the Word Error Rate (WER) of this automatic transcript relative to our reference transcripts obtained from the book texts.




%
%\begin{table*}
%	\centering
%	\resizebox{0.9\textwidth}{!}{
%		\begin{tabular}{l|cc|cc|cc|cc} 
%			\hline
%			\multirow{3}{*}{} & \multicolumn{4}{c|}{Wav2Vec2}   & \multicolumn{4}{c}{DeepSpeech2}         \\ \cline{2-9} 
%			&\multicolumn{2}{c|}{Recording}  &  \multicolumn{2}{c|}{Call}        &  \multicolumn{2}{c|}{Recording}  &  \multicolumn{2}{c}{Call}             \\ \hline
%			& WER &CER & WER &CER  & WER &CER  & WER &CER  \\ \hline
%			Sup        & 0.586     & 0.846  & 0.586     & 0.846 & 0.586     & 0.846  & 0.858 & 0.871\\ \hline
%			Sup+Noise       & 0.525  &0.846 & 0.586     & 0.846 & 0.586     & 0.846 &   0.858  & 0.871        \\ \hline
%			Sup+VTLP     &0.531& 0.586     & 0.846 & 0.586     & 0.846 & 0.800  &0.790 &0.818\\ \hline
%			Sup+Speed   &0.927& 0.586     & 0.846 & 0.586     & 0.846   &0.896  &0.946  & 0.933      \\ \hline
%			Sup+Semi1  & 0.979& 0.586     & 0.846 & 0.586     & 0.846 &0.894 &0.950   &0.933 \\ \hline
%			Sup+Semi2  & 0.240& 0.586     & 0.846 & 0.586     & 0.846  & 0.804 &0.777 &0.817 \\ \hline
%	\end{tabular}}
%	\caption{Binary classification of sentiment polarity on both linguistic and acoustic modalities}
%	\label{table:finetuneresults}
%\end{table*}




%Table testsets that are used to report experimental results.
%Testset   Duration (hours)  Nb. speakers  Nb. words (thousands 


 

%For these five LMs, the training data and training proce- dures are common and described below:
%• Weusedthesamevocabularyof85Kwordsfrom[5].
%• We first train the LM with a corpus of 560M words consisting of publicly available text data from LDC, in- cluding Switchboard, Fisher, Gigaword, and Brodcast News and Conversations. Then, starting from the trained model, we further train the LM with only the transcripts of the 1975 hours audio data used to train the acoustic model, consisting of 24M words.
%• WecontrolledtheleaningratebyADAM[29]andintro- duced a self-stabilization term to coordinate the layer- wise learning rates [30].
%• For all models, we tuned the hyper-parameters based on the perplexity of the heldout data which is a subset of the acoustic transcripts. The approximate number of param- eters for each model was 90M to 130M.    
%
%
%Table 9: WER for the different LM rescroign steps across all testsets. Last line shows WERs after ’.’ removal from the references and system outputs.
%
%
%WER is 25.2 on test-other and 16.3 on test-clean.
%




%On the other hand, previous work suggests that models trained solely on ASR text-based features are competitive with those using only acoustic fea- tures or a combination of the two (Loukina and Cahill, 2016). Their interpretation of these results was that the transcription offers some proxy information for prosodic and phonological performance
%– for instance the presence of hesitation and silence markers, the number of word tokens in the transcription, and the transcription errors which might arise from mispronunciations.
%
%We compare cascaded and end-to-end models across high, medium, and low-resource condi- tions,


%Table:  Results in BLEU comparing our proposed phone featured models to baselines. We compare three resource  conditions, and show average improvement for dev and test (∆). Best performance bolded by column
%Model   High (160hr)  Mid (40hr) low (20hr)
%
%dev test 


%Training time:  this model offers a better trade-off between time and performance.
%On the WSJ data, the CRF models outperform the hidden- event baseline significantly. 





% results


%In Table 10, we show the effect of increasing the amount of labeled training data on WER. This is done by randomly sampling the full dataset before training. 
%In English we use 11,940 hours of labeled speech data containing 8 million utterances summarized in Table 9.

%Dataset  Speech Type (read, conversation mixed)  Hpurs       / words 
%Table 9: Summary of the datasets used to train DS2 in English. The Wall Street Journal (WSJ), Switchboard and Fisher [13] corpora are all published by the Linguistic Data Consortium. The LibriSpeech dataset [46] is available free on-line. The other datasets are internal Baidu corpora.

%ervised data and MLP trained on perturbed supervised data. The use of standard and perturbed semi-supervised data for training MLP yields a slightly lower MTWV. However, in this case re-training GMM on the semi-supervised data may hurt performance. This indicates that the approach is also sensitive to the accuracy of training transcriptions.
%In our experiments on the Switchboard (SWB) benchmark task, a 6.7\% relative improvement in WER was obtained using the proposed data augmentation method over a state of the art DNN setup [7]. 

%Average likelihood of training and cross-validation data across iterations

%############################## Augmentatiion 
%Performance monitoring Attention weights and all kinds of training/validation scores and losses for ASR, MT, and ST tasks can be collectively monitored through TensorBoard.

%We found that using speed perturbed training data led to better generalization, 
%we trained the speed-perturbed system for few more epochs. This was found to improve the results.
%Table 3 compares the performance improvement from speed perturbation across a variety of LVCSR tasks with a varying amount of training data. 
%The gains due to data augmentation were consistent. Speed perturbation of the training data led to a relative improvement of 2% on this task.
%The test dataset consists of 21 calls with 1,890 utterances, which are manually  annotated  for negative (848)  and nonnegative (1,042). 
%#####
%We compared the Deep Speech system to several commercial speech systems: (1) wit.ai, (2) Google Speech API, (3) Bing Speech and (4) Apple Dictation.7

%For comparison, our latest ASR system achieves 5.5%/10.3% WER on SWB/CH. 


%The final model was trained on a set of 59 training and 18 vali- dation datasets with a batch size of 160, learning rate of 0.001, Adam optimiser and bias-aware loss according to [35].

Speech input industrial applications fulfill incoming user requests through the use of Spoken Language Understanding (SLU) \cite{7078634} which  applies NLU  tasks after ASR. The ASR system errors would propagate to the downstream NLU and degrade the performance \cite{jia-2020-deep}. 
In this experiment, we also focused on evaluating one of the NLU tasks - intent detection - using error prone ASR transcriptions. 

% instead of text WER along. 
In general,  transcripts with lower WER produce  more accurate  intent prediction. 
 However, even if the ASR system does not correctly transcribe the input speech into text, the final intent result could be correct if the output of the recognition part preserves sufficient semantic information for intent prediction.
 %However, even though the recognition part does not correctly transcribe the input speech into text, the final intent result would be correct if the output of the recognition part preserves sufficient semantic information for translation. %Therefore, we explore to evaluate the intent prediction instead of text WER.

Table \ref{table:intent} presents the intent classification results based on three ASR outputs: AWS, Google, and Wav2Vec2 (Wav2Vec2-large-LV60 + LM).  Since Wav2Vec2 outputs are alphabetic letters only and the intent classification model is trained on regular text data, the punctuation model and reformatting described in section \ref{subse:punctuation} are applied to generate the Wav2Vec2-F (Formatting) transcripts, then feed them to the downstream intent classifier.



 The test utterance intents are labeled by two annotators based on the transcripts. Without knowing context, some of the utterance intents are not clear. Among 904 call utterances, 626 have meaningful intent. Among 95 recording utterances, 83 have meaningful intent. Table \ref{table:intent} shows the WER for the test data and the number of correct intent predictions based on different speech recognition system outputs. The intent predictions based on the manual transcripts are also calculated as reference. The intent classifier model works good for recording transcripts, but for call segment transcripts the accuracy is around 77.6\%.
%\begin{table*}
%	\centering
%	\resizebox{0.6\textwidth}{!}{
%		\begin{tabular}{lc|cccc} 
%			\hline
%			Type & Error & AWS & Google & Wav2V2ec & Wav2Vec2-F \\ \hline
%					\multirow{2}{*}{Record} & WER (83) & 0.136 & 0.070  &0.037 & 0.037 \\ \cline{2-6} 
%			& Intent (83) & 75 & 78 & 81 & 81 \\ \hline
%				\multirow{2}{*}{Call} & WER (626) & 0.107 &0.243  & 0.119 & 0.119\\ \cline{2-6} 
%		     &Intent (626)  & 481 & 388& 481 & 488 \\ \hline
%	\end{tabular}}
%	\caption{WER and intent model outputs based on three speech recognition system outputs. }
%	\label{table:intent}
%\end{table*}

\begin{table*} 
	\centering
	\resizebox{0.72\textwidth}{!}{
		\begin{tabular}{lc|ccccc} 
			\hline
			Type & Error & \textit{Transcripts} & AWS & Google & Wav2Vec2 & Wav2Vec2-F   \\ \hline
			\multirow{2}{*}{Recording} &  WER  &\textit{0.000} & 0.136 & 0.070  &  \textbf{0.037} &  \textbf{0.037} \\ \cline{2-7} 
			& Intent  & \textit{83} & 75 & 78  &  \textbf{81}  &  \textbf{81}   \\ \hline
			\multirow{2}{*}{Call} &   WER & \textit{0.000}  &  \textbf{0.109} &0.243  & 0.119 & 0.119 \\ \cline{2-7} 
			&Intent  & \textit{486} & 481 & 388 & 481 &  \textbf{488} \\ \hline
	\end{tabular}}
	\caption{WER and the number of correct intent predictions based on different speech recognition system outputs. Best performance bolded by column.}
	\label{table:intent}
\end{table*}


For the recording test set, the intent prediction based on fine-tuned Wav2Vec2 outputs performs best. For the call segment test set,  AWS ASR output has the lowest WER but its intent classification performance is the same as  Wav2Vec2. The number of correct intent predictions based on  Wav2Vec2-F is even higher than AWS and  Wav2Vec2 direct alphabetic letters only transcript.  In addition, the intent classification results between fine-tuned ASR and human transcriptions are very similar. 
Therefore, although the intent classifier model can suffer from  erroneous speech systems output,  the domain-specific fine-tuned ASR transcriptions often contain enough accurate terms which are crucial to the meaning of the utterance that the intent classifier can still produce the  correct intents. 


%Since Wav2Vec-F is more accurate for benefit specific terms which are more important when apply the intent prediction. 
%The Wav2Vec-F is better than Wav2Vec alpha-beta only transcript. 

%Experimental results show that the proposed approach obtains improvement to the ST model.

%95 recordings, 90   related one. 
%wav2vec  87 correct one
%AWS   80  
%google 82




\section{Discussion \label{se:discussion}}

%Fine tuned variate types of pre-trained acoustic models on . Then incorporate an domain-specific LM  to further improve performance. 

This paper presents a thorough analysis of how to build an end-to-end ASR system using company internal audio data. The insights we gleaned from this investigation provide hints on how companies could potentially adapt such pre-trained acoustic models and semi-supervised annotation to build their own end-to-end ASR system which is able to outperform the commercial ASR systems for domain-specific speech. 

However, building an ASR system through fine-tuning  pre-trained acoustic models still requires large enough supervised data to achieve reliable performance, and acoustic model performance is sensitive to the accuracy of training transcriptions. At the beginning of the process, humans are needed to get high quality training corpus.  
%the high quality training corpus are small and human may be needed to improve the transcript quality. 
In this experiment, recording data is used to initiate the process. It  takes time  and effort at the beginning, but the acoustic models can be easily refreshed  and improved as the training corpus size and the ASR system transcript quality increased with more iterations.  For  domain-specific SLU tasks, the use of domain-specific fine-tuned ASR output can outperform the use of commercial ASR transcriptions even when the fine-tuned model has higher WER, and it can even reach a similar performance with the use of  human transcriptions.

Building their own ASR system   can better protect personal identifiable information such as social security numbers and birth date which are commonly occurred in call center data. In addition, the cost spent on ASR system can be significantly reduced compared to employing commercial ASR systems especially for call centers with high call volumes daily.
 

\section{Conclusion \label{se:conclusion}}
In this paper, we introduce a fully automated procedure to select and annotate unsupervised audio data. This process allows us to easily improve transcript quality and increase  training corpus size,  therefore updating the ASR systems  with as little human intervention and as often as needed.
%This process allows us to easily update acoustic models
%More iteration can improve  transcript quality and increase training corpus size. 

Four pre-trained acoustic models are fine-tuned on internal company corpora, and KenLM  is incorporated to further improve performance on domain-specific speech. The fine-tuned Wav2Vec2-LARG-LV60 + LM has lower WER for domain-specific speech and more accurate intent predictions compared to AWS and Google commercial speech recognition systems.  

For future work, we would like to explore and evaluate ASR system on more languages such as Chinese and Spanish, investigate ASR error robust downstream NLP systems, and compare with an end-to-end SLU system performance.  
%explore more powerful acoustic models such as WAV2VEC2-XLS-R models which are pre-trained on 436k hours of unlabeled multilingual speech and requires more powerful GPU to run, and  
  
  
%with more labeled training data 

%the training courpus size and the transcript quality will be increase as more iteration goes, and  with more labeled training data.     

%As more iteration, the speech system will continue to improve with more labeled training data. 

%But after the a couple of iteration, the acoustic models can be refreshed with with as little human intervention and as often as possible.
% We hypothesize that equally as important as increasing raw number of hours is increasing the number of speech contexts that are captured in the dataset. A context can be any property that makes speech unique including different speakers, background noise, environment, and microphone hardware.
%and increased training corpus size
 

%Then    the accuracy of training data transcriptions.

% Since the demographics and device channel characteristics of the users of Google’s speech products are constantly changing, it is important to refresh our acoustic models with as little human intervention and as often as possible.

%the use of semi-supervised training yielded mixed results in this case suggesting sensitivity of the approach to the accuracy of training data transcriptions.
%improved transcript quality and increased training corpus size. 
%This indicates that the approach is also sensitive to the accuracy of training transcriptions.

%In all cases word error rate reductions of up to 14\% relative have been obtained using this fully automated and unsupervised procedure. This process allows us to easily update acoustic models in a matter of days. Since the demographics and device channel characteristics of the users of Google’s speech products are constantly changing, it is important to refresh our acoustic models with as little human intervention and as often as possible.

%sensitive to the accuracy of training transcriptions.

%The WER decreases by ∼40\% relative for each factor of 10 increase in training set size. 
%This implies that a speech system will continue to improve with more labeled training data. We hypothesize that equally as important as increasing raw number of hours is increasing the number of speech contexts that are captured in the dataset. A context can be any property that makes speech unique including different speakers, background noise, environment, and microphone hardware.


%\section*{Acknowledgements}
%Support for this work was provided by Businessolver.



\bibliography{Speech2Text}		
\bibliographystyle{ieeetr}
 
%\vspace{10pt} 
%
%\begin{minipage}[b]{0.8\linewidth}
%	\textbf{Authors} \\
%	Yanan Jia is a research scientist at Businessolver, where she is working on a variety of NLP research and development projects of benefit-specific intelligent assistant. She received a PhD degree from the Ohio State University. Her research interests include speech recognition, spoken language understanding, sentiment analysis,  low-resource NLP,  text generation/summarization etc.
%	
%\end{minipage}
%\hfill
%\begin{minipage}[b]{0.2\linewidth}
%	\centering
%	\includegraphics[height=5\baselineskip]{images/yjia.png}
%\end{minipage}




%\begin{wrapfigure}{r}{0.2\textwidth}
%	\centering
%	\includegraphics[width=.6 \linewidth]{images/yjia.png}
%	\caption{A caption}
%\end{wrapfigure}
%\bigskip
%\vspace{3cm} 
%\textbf{Authors} \\
%Yanan Jia is a research scientist at Businessolver, where she is working on a variety of NLP research and development projects that are tightly aligned with the globalization of Alibaba in the Southeast Asia region. She received a PhD degree from The Ohio State University. Her research interests include  low-resource NLP,  sentiment analysis, text generation/summarization, argument mining etc



\end{document}