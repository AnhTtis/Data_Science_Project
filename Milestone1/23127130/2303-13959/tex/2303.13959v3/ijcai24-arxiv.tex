%%%% ijcai24.tex

\typeout{IJCAI--24 Instructions for Authors}

% These are the instructions for authors for IJCAI-24.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai24.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai24}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.



\usepackage{arydshln} %%# 负责画虚线的包
\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{xcolor}
% \usepackage{url}
\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}
\usepackage{booktabs, multirow} % for borders and merged ranges
\definecolor{roadcolor}{RGB}{234,51,246}
\definecolor{sidewalkcolor}{RGB}{68,8,72}
\definecolor{parkingcolor}{RGB}{241,156,249}
\definecolor{othergroundcolor}{RGB}{160,32,76}
\definecolor{buildingcolor}{RGB}{246,202,69}
\definecolor{carcolor}{RGB}{111,149,238}
\definecolor{truckcolor}{RGB}{74,32,172}
\definecolor{bicyclecolor}{RGB}{136,227,242}
\definecolor{motorcyclecolor}{RGB}{37,59,146}
\definecolor{othervehiclecolor}{RGB}{96,81,242}
\definecolor{vegetationcolor}{RGB}{79, 173, 50}
\definecolor{trunkcolor}{RGB}{126, 65, 22}
\definecolor{terraincolor}{RGB}{171, 238, 105}
\definecolor{personcolor}{RGB}{234, 60, 49}
\definecolor{bicyclistcolor}{RGB}{234, 66, 195}
\definecolor{motorcyclistcolor}{RGB}{138, 42, 90}
\definecolor{fencecolor}{RGB}{238, 128, 69}
\definecolor{polecolor}{RGB}{252, 241, 161}
\definecolor{trafficsigncolor}{RGB}{233, 51, 35}
\definecolor{color1}{RGB}{176, 36, 24}
\definecolor{color2}{RGB}{119,185,0}
\definecolor{color3}{RGB}{0, 0, 200}
\definecolor{colorofteaser}{RGB}{176, 36, 24}
\newcommand{\tbr}[1]{\textbf{\textcolor{color1}{#1}}}
\newcommand{\tbg}[1]{\textbf{\textcolor{color2}{#1}}}
\newcommand{\tbb}[1]{\textbf{\textcolor{color3}{#1}}}
\newcommand{\teaser}[1]{\textbf{\textcolor{colorofteaser}{#1}}}
\newcommand{\chaowei}[1]{\textbf{\textcolor{color3}{Chaowei: #1}}}
%%--------------mono
\definecolor{LightGrey}{rgb}{.9,.9,.9}
\definecolor{White}{rgb}{1.,0.,1.}
\definecolor{first}{rgb}{.8,.0,.0}
\definecolor{second}{rgb}{.0,.6,.0}
\definecolor{third}{rgb}{.0,.0,.8}
\definecolor{ceiling}{RGB}{214,  38, 40}   %
\definecolor{floor}{RGB}{43, 160, 4}     %
\definecolor{wall}{RGB}{158, 216, 229}  %
\definecolor{window}{RGB}{114, 158, 206}  %
\definecolor{chair}{RGB}{204, 204, 91}   %
\definecolor{bed}{RGB}{255, 186, 119}  %
\definecolor{sofa}{RGB}{147, 102, 188}  %
\definecolor{table}{RGB}{30, 119, 181}   %
\definecolor{tvs}{RGB}{160, 188, 33}   %
\definecolor{furniture}{RGB}{255, 127, 12}  %
\definecolor{objects}{RGB}{196, 175, 214} %
\definecolor{car}{rgb}{0.39215686, 0.58823529, 0.96078431}
\definecolor{bicycle}{rgb}{0.39215686, 0.90196078, 0.96078431}
\definecolor{motorcycle}{rgb}{0.11764706, 0.23529412, 0.58823529}
\definecolor{truck}{rgb}{0.31372549, 0.11764706, 0.70588235}
\definecolor{other-vehicle}{rgb}{0.39215686, 0.31372549, 0.98039216}
\definecolor{person}{rgb}{1.        , 0.11764706, 0.11764706}
\definecolor{bicyclist}{rgb}{1.        , 0.15686275, 0.78431373}
\definecolor{motorcyclist}{rgb}{0.58823529, 0.11764706, 0.35294118}
\definecolor{road}{rgb}{1.        , 0.        , 1.        }
\definecolor{parking}{rgb}{1.        , 0.58823529, 1.        }
\definecolor{sidewalk}{rgb}{0.29411765, 0.        , 0.29411765}
\definecolor{other-ground}{rgb}{0.68627451, 0.        , 0.29411765}
\definecolor{building}{rgb}{1.        , 0.78431373, 0.        }
\definecolor{fence}{rgb}{1.        , 0.47058824, 0.19607843}
\definecolor{vegetation}{rgb}{0.        , 0.68627451, 0.        }
\definecolor{trunk}{rgb}{0.52941176, 0.23529412, 0.        }
\definecolor{terrain}{rgb}{0.58823529, 0.94117647, 0.31372549}
\definecolor{pole}{rgb}{1.        , 0.94117647, 0.58823529}
\definecolor{traffic-sign}{rgb}{1.        , 0.        , 0.    } 


\definecolor{barrier1}{RGB}{112,128,144}
\definecolor{bicycle1}{RGB}{220,20,60}
\definecolor{bus1}{RGB}{255, 127, 80}
\definecolor{car1}{RGB}{255, 158, 0}
\definecolor{const. veh.1}{RGB}{233, 150, 70}
\definecolor{motorcycle1}{RGB}{255,61,99}
\definecolor{pedestrian1}{RGB}{0,0,230}
\definecolor{traffic cone1}{RGB}{47,79,79}
\definecolor{trailer1}{RGB}{255,140,0}
\definecolor{truck1}{RGB}{255,99,71}
\definecolor{drive. suf.1}{RGB}{0,207,191}
\definecolor{other flat1}{RGB}{175,0,75}
\definecolor{sidewalk1}{RGB}{75,0,75}
\definecolor{terrain1}{RGB}{112,180,60}
\definecolor{manmade1}{RGB}{222,184,135}
\definecolor{vegetation1}{RGB}{0,175,0}

% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2024.0)
}

\title{Bridging Stereo Geometry and BEV Representation with\\ Reliable Mutual Interaction for Semantic Scene Completion}


% % Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Bohan Li$^{1,2}$
\and
Yasheng SUN $^3$\and
Zhujin Liang $^{4}$\and 
Dalong Du $^4$\and  \\
Zhuanghui Zhang $^4$ \and  
 Xiaofeng Wang $^5$ \and Yunnan Wang $^{1,2}$ \and Xin Jin $^{2,*}$ \And Wenjun Zeng $^2$ \\
\affiliations
$^1$Shanghai Jiao Tong University,
$^2$ Eastern Institute of Technology, \\
$^3$Tokyo Institute of Technology, 
$^4$PhiGent Robotics, $^5$University of Chinese Academy of Sciences \\
\emails
\{bohan\_li,wangyunnan\}@sjtu.edu.cn, sun.y.aj@m.titech.ac.jp,  \\
\{zhujin.liang,  dalong.du, zhuanghui.zhang\}@phigent.ai, \\
wangxiaofeng2020@ia.ac.cn, {jinxin,wenjunzengvp}@eias.ac.cn
}
% \fi

\begin{document}

% \maketitle
 

\twocolumn[{
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\vspace{-10pt}
\begin{center}
 % \centering
 % \includegraphics[width=0.95\textwidth]{figs/teaser.pdf}
% \vspace{-10pt}
% \begin{figure*}[!h]
	% \begin{subfigure}{\linewidth}	
	% 	\centering
	\begin{center}
		\includegraphics[width=15cm]{teaser.png}   
  
		\begin{tabular}{cccccc}	
			\multicolumn{6}{c}{
				\scriptsize
				\textcolor{bicycle}{$\blacksquare$}bicycle~
				\textcolor{car}{$\blacksquare$}car~
				\textcolor{motorcycle}{$\blacksquare$}motorcycle~
				\textcolor{truck}{$\blacksquare$}truck~
				\textcolor{other-vehicle}{$\blacksquare$}other vehicle~
				\textcolor{person}{$\blacksquare$}person~
				\textcolor{bicyclist}{$\blacksquare$}bicyclist~
				\textcolor{motorcyclist}{$\blacksquare$}motorcyclist~
				\textcolor{road}{$\blacksquare$}road~
				
}
    \\
    
			\multicolumn{6}{c}{
				\scriptsize
                    \textcolor{parking}{$\blacksquare$}parking~
				\textcolor{sidewalk}{$\blacksquare$}sidewalk~
				\textcolor{other-ground}{$\blacksquare$}other ground~
				\textcolor{building}{$\blacksquare$}building~
				\textcolor{fence}{$\blacksquare$}fence~
				\textcolor{vegetation}{$\blacksquare$}vegetation~
				\textcolor{trunk}{$\blacksquare$}trunk~
				\textcolor{terrain}{$\blacksquare$}terrain~
				\textcolor{pole}{$\blacksquare$}pole~
				\textcolor{traffic-sign}{$\blacksquare$}traffic sign			
			}
		\end{tabular}	
 	\end{center}
        \vspace{-0pt}
	\captionof{figure}{ \textbf{Overview of the proposed BRGScene}. The figure illustrates the stereo inputs, SSC prediction results and ground truth from left to right. We can see that our method shows promising performance in completing semantic scenes, especially for those challenging distant small objects, as indicated by the car highlighted with a red box.
        } 
% \end{figure*} 
\label{fig:teaser}
\end{center}
}]



\begin{abstract}
\vspace{-0pt}
3D semantic scene completion (SSC) is an ill-posed perception task that requires inferring a dense 3D scene from limited observations. Previous camera-based methods struggle to predict accurate semantic scenes due to inherent geometric ambiguity and incomplete observations. In this paper, we resort to \emph{stereo matching} technique and \emph{bird’s-eye-view (BEV)} representation learning to address such issues in SSC. Complementary to each other, stereo matching mitigates geometric ambiguity with epipolar constraint while BEV representation enhances the hallucination ability for invisible regions with global semantic context. However, due to the inherent representation gap between stereo geometry and BEV features, it is non-trivial to bridge them for \emph{dense prediction task} of SSC. Therefore, we further develop a unified occupancy-based framework dubbed \textbf{BRGScene}, which effectively \textbf{br}id\textbf{g}es these two representations with dense 3D volumes for reliable semantic \textbf{scene} completion. Specifically, we design a novel Mutual Interactive Ensemble (MIE) block for pixel-level reliable aggregation of stereo geometry and BEV features. Within the MIE block, a Bi-directional Reliable Interaction (BRI) module, enhanced with confidence re-weighting, is employed to encourage fine-grained interaction through mutual guidance. Besides, a Dual Volume Ensemble (DVE) module is introduced to facilitate complementary aggregation through channel-wise recalibration and multi-group voting.
Our method outperforms all published camera-based methods on SemanticKITTI for semantic scene completion.\looseness=-1
\vspace{-5pt}
\end{abstract}


 
\section{Introduction}
3D scene understanding is a fundamental task in computer vision~\cite{roberts1963machine}, facilitating a variety of applications such as autonomous driving, robotic navigation and augmented reality. Due to the limitations of real-world sensors such as restricted field of view, measurement noise, or sparse results, this task remains a challenging problem. To address this problem, 3D Semantic Scene Completion (SSC)~\cite{roldao20223d} is introduced to jointly predict the geometry and semantic segmentation of a scene. Given its inherent 3D nature, most existing SSC solutions~\cite{garbade2019two,roldao2020lmscnet,wu2020scfusion} employ 3D geometric signals, in the form of occupancy grids, point clouds, or distance fields, as their model inputs. Although they provide insightful geometric cues, it requires costly sensors (e.g. LiDAR) alongside considerable manual labor entailed in their deployment. Hence, it is worth exploring an efficient and effective approach for high-fidelity SSC solely with cost-friendly portable cameras. 

However, the absence of explicit 3D geometric information and incomplete observation pose large challenges to accurate geometry acquisition and reasonable hallucination in invisible regions
% , the core challenges of camera-based methods are \textbf{accurate geometry acquisition and the hallucination ability in invisible regions}
~\cite{cao2022monoscene,huang2023tri,li2023voxformer}. Thus, previous camera-based SSC solutions~\cite{cao2022monoscene,huang2023tri} tend to utilize learning-based projection techniques to convert 2D image features into a 3D dense space, but their predictions inevitably fall short of capturing accurate geometry without explicit constraints.
Later studies~\cite{li2023voxformer} attempt to introduce depth information to augment query for reliable geometry prediction. But their results still struggle to hallucinate reasonable invisible regions without ensembling global semantic context.


As a pivotal technique in 3D vision applications, stereo matching leverages explicit epipolar constraint to establish pixel-level correspondence, which is suitable for reconstructing dense 3D scene geometry~\cite{guo2019group}. On the other hand, the remarkable global robustness and hallucination ability of bird's-eye-view (BEV) representation, coupled with rich context and global semantic information have led to its widespread utilization in the 3D object detection community~\cite{philion2020lift,li2022bevformer,roddick2020predicting}. Inspired by the above two aspects, recent works have begun to simultaneously use stereo matching and BEV features for 3D perception. For instance, in the 3D object detection area, BEVStereo~\cite{li2022bevstereo} fuses monocular and temporal stereo depth maps before generating BEV features. BEVDepth~\cite{li2023bevdepth} improves geometry modeling with BVE representations by introducing extra depth supervision.
% \textcolor{red}{Recently, there have been attempts in 3D object detection to improve geometry modeling in BVE representation} by introducing explicit depth supervision in BEV pipeline~\cite{li2023bevdepth} or fusing monocular and temporal stereo depth maps~\cite{li2022bevstereo} before generating BEV features.
These strategies significantly enhance the effectiveness of sparse perception tasks (e.g. 3D object detection) by focusing on coarse-grained predictions at the region level for common visual objects.\looseness=-1


% Despite unprecedented advances in 3D object detection, it's non-trivial in the semantic scene completion (SSC) task to bridge the representation gap between stereo and BEV features within a unified framework for pixel-level reliable prediction. Because those similar objects vary in 3D structures and the coarse region-level information is not sufficient to describe the pixel-level semantics and geometry in more complex realistic scenarios

Despite remarkable advances in 3D object detection, it's non-trivial in the semantic scene completion (SSC) task to bridge the representation gap between stereo geometry and BEV features within a unified framework for pixel-level reliable prediction. This difficulty arises due to the structural variations among similar objects and the insufficient region-level coarse information for pixel-level semantics and geometry in complex real-world scenarios.

% Despite remarkable advances in 3D object detection, bridging the representation gap between stereo and BEV features for reliable pixel-level predictions in the semantic scene completion (SSC) task is challenging.
% This difficulty arises due to the structural variations among similar objects and the insufficient region-level coarse information for pixel-level semantics and geometry in complex real-world scenarios.





Given these concerns, we propose \textbf{BRGScene}, a framework that bridges stereo matching technique and BEV representation for fine-grained reliable SSC, and the results are shown in Figure~\ref{fig:teaser}.
Our framework aims to fully exploit the potential of vision inputs with explicit geometric constraint of stereo matching~\cite{guo2019group,jiankun2022crestereo} and global semantic context of BEV representation~\cite{philion2020lift,li2023bevdepth}.
Different from previous methods that focus on 2D features or depth maps~\cite{li2023voxformer,li2023bevdepth}, we propose to employ the {dense 3D volumes} of stereo and BEV representations for SSC.

Given the distinct nature of the two volumes, we devise a \emph{Mutual Interactive Ensemble (MIE)} block to bridge the gap for fine-grained reliable perception. Specifically, a \emph{Bi-directional Reliable Interaction (BRI)} module is designed to guide each volume to retrieve pixel-level reliable information. A confidence re-weighting strategy inspired by MVS~\cite{chen2020mvsnet++} is incorporated on top of the BRI module to further enhance the performance. Furthermore, a \emph{Dual Volume Ensemble (DVE)} module is introduced to facilitate complementary aggregation with channel-wise recalibration and multi-group feature voting.
% exhibits exceptional proficiency in producing precise predictions on matched positions. 
Our contributions are summarized as follows: \textbf{1)} We propose a novel framework that resorts to both stereo and BEV representations with dense 3D volumes for precise geometry modeling and hallucination ability enhancement in SSC. \textbf{2)} To bridge the representation gap for fine-grained reliable perception, a \emph{Mutual Interactive Ensemble} block is designed to take advantage of the complementary merits of the volumes in the two representations. \textbf{3)} Our proposed BRGScene outperforms state-of-the-art VoxFormer-T with a 14.5\% relative improvement on the SemanticKITTI leaderboard.\looseness=-1
% For semantic scene completion on the SemanticKITTI test set, BRGScene outperforms state-of-the-art VoxFormer-T with a 22.2\% relative improvement. 
 


% with a relative improvement of 26.9$\%$ in \textbf{geometry} and 38.6$\%$ in \textbf{semantic} on SemanticKITTI.



\section{Related Works}

\subsection{Semantic Scene Completion}

% Earlier works on scene completion (SC) used sophisticated interpolation~\cite{davis2002filling} or energy minimization~\cite{kazhdan2006poisson,newcombe2011kinectfusion,sorkine2004least} techniques to estimate 3D dense geometry given one or more 2D/3D observations. With the large-scale datasets~\cite{behley2019semantickitti,silberman2012indoor,song2017semantic,straub2019replica} released in recent years, recent works~\cite{cai2021semantic,rist2021semantic,yan2021sparse,cheng2021s3cnet,song2017semantic,wu2020scfusion,roldao2020lmscnet,li2020anisotropic} have increasingly relied on deep learning techniques to learn 3D priors and address ambiguity in the scene. Song et al.~\cite{song2017semantic} recognized the intertwined relationship between semantic segmentation and scene completion and proposed a joint estimation of semantic segmentation and scene completion. 
Semantic scene completion is a dense 3D perception task that jointly estimates semantic segmentation and scene completion~\cite{behley2019semantickitti,cai2021semantic}. 
% Given the 3D nature of SSC, many studies~\cite{zhang2018efficient,rist2021semantic,garbade2019two} directly use 3D inputs to take advantage of its accompanying geometrical insights. 
To provide additional texture or geometry information, some works~\cite{cai2021semantic,li2019rgbd} exploit multi-modal inputs, such as RGB images coupled with geometric cues. 
% IPF-SPCNet~\cite{zhong2020semantic} performs semantic segmentation on the image inputs and uses RGB features to augment the 3D point cloud. 
Another slew of studies~\cite{cao2022monoscene,li2023voxformer} aims to achieve semantic scene completion solely with camera-only inputs. 
For instance, MonoScene~\cite{cao2022monoscene} lifts a monocular image using 2D-3D projections and leverages 2D and 3D UNets for semantic scene completion.
TPVFormer~\cite{huang2023tri} utilizes a tri-perspective view representation and attention mechanism for 3D scene understanding.
VoxFormer~\cite{li2023voxformer} employs a transformer-based framework where a sparse set of depth-based voxel queries are devised for scene structure reconstruction.\looseness=-1
% OccDepth~\cite{miao2023occdepth} attempts to improve geometry awareness with a pre-trained depth network of LEAStereo~\cite{cheng2020hierarchical}. However, they rely on knowledge distillation of the pre-trained depth net, without taking into consideration the proper hallucination in invisible regions. 




\subsection{Stereo Matching Based 3D Perception}
With the advances of deep convolution neural networks, the quality of depth predictions from stereo images~\cite{poggi2022stereodepth} has steadily improved and led to a remarkable improvement in downstream 3D vision applications such as object detection, surface reconstruction and augmented reality. 
GC-Net \cite{kendall2017end} first proposes to employ 3D CNNs in the stereo matching framework, where 2D corresponding features are mapped to a 3D cost volume through a concatenation operation. 
% The promise of this method inspired GwcNet \cite{guo2019group}, which proposes group-wise correlation to improve feature similarity measurement.
RAFTStereo~\cite{Lahav2021raftstereo} and CREStereo\cite{jiankun2022crestereo} utilize feature correlation to produce matching cost volume, which is subsequently optimized through sequential refinement modules for depth prediction. 
However, in conditions such as occlusion and large textureless regions, stereo matching prediction performance drops significantly. 


\begin{figure*}[!ht]
\vspace{-0pt}
\hsize=\textwidth %
\centering
\includegraphics[width=1\textwidth]{overall2.png}
\caption{ \textbf{Overall framework of our proposed BRGScene}. Given input stereo images, we employ 2D UNet to extract image features. The BEV latent volume and stereo geometric volume are constructed by a \emph{BEV Constructor} and a \emph{Stereo Constructor}, respectively. To bridge their representation gap for fine-grained
reliable perception, a \emph{Mutual Interactive Ensemble} block is proposed to take advantage of complementary merits of the volumes.}
\label{figoverall}
 \vspace{-10pt}
\end{figure*}


\subsection{Bird's-Eye-View Representation }
The bird's-eye-view is a widely used representation in 3D object detection since it provides a clear depiction of the layout and strong hallucination ability from a top-down perspective~\cite{philion2020lift,hu2021fiery}. 
% In camera-based scene understanding frameworks, researchers lift 2D image features to 3D BEV representations coupled with estimated depth distribution~\cite{reading2021categorical,philion2020lift,li2022bevformer}. Mani et.al.~\cite{mani2020monolayout} utilize a ResNet to reason about bird's-eye-view layout, whereas Roddick et.al.~\cite{roddick2020predicting} devise a transformer architecture to predict map semantic segmentation with converted BEV representations. 
Lift-Splat~\cite{philion2020lift} extracts BEV representations from an arbitrary number of cameras by implicitly unprojecting 2D visual inputs based on estimated depth distribution. 
% To circumvent compounding errors~\cite{wang2022detr3d}, later approaches attempt to dynamically learn reliable geometric insights rather than relying on structural 3D prior. 
% BEVFormer~\cite{li2022bevformer} designs a spatial-temporal transformer to adaptively aggregate BEV features from images at different timestamps. 
BEVDepth~\cite{li2023bevdepth} leverages a camera-aware monocular depth estimation module to improve depth perception in BEV-based 3D detection.  
% BEVStereo~\cite{li2022bevstereo} combines temporal stereo depth maps and monocular depth maps to improve depth prediction quality in multi-view 3D detection. 
However, region-level coarse perception in the object detection task is not effective enough for the dense prediction task of SSC.
In this work, our objective is to investigate reliable pixel-level prediction for high-quality semantic scene completion. 




\section{Methodology}
We present our proposed BRGScene that aims to jointly infer dense 3D geometry and semantics solely from RGB images. In this section, we first introduce a hybrid occupancy-based SSC framework (Sec.~\ref{sec:preliminary}), including problem formulation and architecture overview. Then we provide detailed construction of \emph{dual volume} representations (Sec.~\ref{sec32}). To bridge their representation gap for fine-grained
reliable perception, we depict our devised ensemble block (Sec.~\ref{sec:integration}). Finally, we introduce our SSC generator and training paradigm (Sec.~\ref{sec:ssc}). \looseness=-1

\subsection{The Proposed Framework of BRGScene}

We propose BRGScene, a framework that bridges the stereo matching technique and the BEV representation for fine-grained reliable SSC.


\label{sec:preliminary}
\noindent \textbf{Problem Formulation.} Given a set of stereo RGB images $I^{rgb}_{Stereo}=\{I^{rgb}_l, I^{rgb}_r\}$, our goal is to jointly infer geometry and semantics of a 3D scene. The scene is represented as a voxel grid $\textbf{Y} \in \mathbb{R}^{H\times W \times Z}$, where $H,W,Z$ denote the height, width and depth in 3D space. Regarding each voxel, it will be assigned to a unique semantic label belonging to $C \in \{ c_0, c_1, \cdots, c_M \}$, which either occupies empty space $c_0$ or falls on a specific semantic class $\{c_1, c_2, \cdots, c_M \}$. Here $M$ denotes the total number of semantic classes. We would like to learn a transformation $\hat{\textbf{Y}}=\Theta(I^{rgb}_{Stereo})$ to approach ground truth 3D semantics $\textbf{Y}$.





\noindent \textbf{Architecture Overview}. The overall architecture of our proposed framework is illustrated in Figure~\ref{figoverall}. We follow a common paradigm~\cite{cao2022monoscene} that employs successive 2D and 3D UNets as backbones. The input stereo images $I^{rgb}_{Stereo}$ are separately encoded by a 2D UNet into paired context-aware features ${\textbf{F}_l}$ and ${\textbf{F}_r}$. Then we leverage a \emph{Stereo Constructor} to convert these features into a dense 3D volume $\textbf{V}_{Stereo} \in \mathbb{R}^{D \times H \times W}$. In parallel, a \emph{BEV Constructor} lift 2D features ${\textbf{F}_l}$ of the left image to a latent BEV volume $\textbf{V}_{BEV} \in \mathbb{R}^{D \times H \times W}$ alongside its context feature $\textbf{C}_{BEV} \in \mathbb{R}^{C_b \times H \times W}$ following standard protocol of ~\cite{philion2020lift}. Sequentially, the two-stream built volumes are bridged and aggregated to a new volume $\textbf{V}_{ens}$ by a \emph{Mutual Interactive Ensemble} block. Finally, the context features $\textbf{C}_{BEV}$ splat along volume $\textbf{V}_{ens}$ by outer-product, which will be fed to a 3D UNet for semantic segmentation and completion. \looseness=-1



\subsection{Dual Volume Construction} 
\label{sec32}





Unlike previous studies~\cite{li2023voxformer,li2023bevdepth} which focus on 2D representations, we employ \emph{3D volumetric representation} to resolve dense scene understanding. Specially, we introduce hybrid volumetric representations with stereo and BEV volumes to take full advantage of camera inputs. \looseness=-1


\noindent \textbf{2D Feature Extraction Backbone}. For image feature extraction, 2D UNet with pre-trained EfficientNetB7~\cite{tan2019efficientnet} is leveraged to separately process left and right input images. Note that we utilize shared weights to encourage efficient correspondence feature learning.


\noindent \textbf{Stereo Geometric Volume Constructor}. With the obtained unary features ${\textbf{F}_l}$ and ${\textbf{F}_r}$ from the left and right images, \emph{Stereo Constructor} targets to build a stereo depth volume $\textbf{V}_{Stereo}$ by matching them with epipolar constraint. Specifically, group-wise correlation~\cite{guo2019group} is first adopted to generate disparity cost volume. Formally,
\begin{equation}\label{a}
D_{gwc}(d,x,y,g)=\frac{1}{N_{c}/N_{g}} \left< f_{g}^{l}(x,y), f_{g}^{r}(x-d,y)\right>,
\end{equation}
where $ \left< \cdot,\cdot\right>$represents the inner product, $N_{c}$ is the channels of input features, $N_{g}$ is the number of groups, $f_{g}^{l}$ and $f_{g}^{r}$ represent $g^{th}$ left and right feature group, respectively. 
Afterward, the disparity volume is converted into a depth volume following~\cite{you2019pseudo}, which is formulated as:
\begin{equation} \label{b}
z_{(u,v)}=\frac{f_{u}\times b}{D_{(u,v)}}, x=\frac{(u-c_{u})\times z}{f_{u}}, y=\frac{(v-c_{v})\times z}{f_{v}},
\end{equation}
where $f_{u}$ and $f_{v}$ represent the horizontal and vertical focal length, ($c_{u}, c_{v}$) is the camera center. 
Next, we employ 3D CNNs following~\cite{guo2019group} for dimension reduction and finally squeeze the channel dimension to construct the dense stereo depth volume $\textbf{V}_{Stereo} \in \mathbb{R}^{ D \times H \times W} $. 




\noindent \textbf{BEV Latent Volume Constructor}. Although the stereo constructor provides accurate estimation in matched regions, it struggles in extreme conditions where severe occlusion or high reflection happens. Unlike the stereo-based approach relying on strict geometric matching, BEV representations
%features 
are obtained by lifting an image $I^{rgb}$ to a shared bird's eye space through 3D prior. Following \cite{philion2020lift,li2023bevdepth}, we feed visual features $\textbf{F}_l$ to a neural network and obtain a latent depth distribution $\textbf{V}_{BEV} \in \mathbb{R}^{D \times H \times W}$ with its associated context features $\textbf{C}_{BEV} \in \mathbb{R}^{C_b \times H \times W}$. Since this distribution is essentially a voxel grid that stores the probability of all possible depths, we denote it as BEV latent volume for the sake of clarity. 





\subsection{Mutual Interactive Ensemble}\label{sec34}
\label{sec:integration}
To achieve fine-grained reliable perception, the Mutual Interactive Ensemble (MIE) block is introduced to bridge the representation gap between stereo volume $\textbf{V}_{Stereo}$ and BEV volume $\textbf{V}_{BEV}$ by mutually reinforcing each other and integrating their respective potentials at pixel level.




\begin{figure}[!ht]
\vspace{0pt}
	\begin{center}
		\includegraphics[width=7.0cm]{block12.png}  
 	\end{center}
        \vspace{-5pt}
	\caption{ The structure of the proposed Bi-directional Reliable Interaction module, which is designed for pixel-level reliable geometry information interaction.} 
	\label{figblock}
 \vspace{-10pt}
\end{figure} 


\subsubsection{Bi-directional Reliable Interaction.}
For pixel-level reliable interaction, we propose an initial interactive stage that selectively retrieves dependable information alongside its counterpart volume. More specifically, a Bi-directional Reliable Interaction (BRI) module as shown in Figure~\ref{figblock} is devised to interactively guide reliable predictions of its contrary side through a cross-attention mechanism. For stereo volume $\textbf{V}_{Stereo}$, we first obtain its query $Q_S$, key $K_S$ and value $V_S$ by flattening in spatial and depth dimensions following standard protocol~\cite{wang2018non,liao2022wt}.
%, where the $Q_s , K_s , V_s \in \mathbb{R}^{1 \times DHW}$. 
Similarly, the BEV volume $\textbf{V}_{BEV}$ is forwarded and its query, key and value are denoted as $Q_B, K_B, V_B$, respectively.\looseness=-1 

Then we construct the interacted volume with the cross-attention operation. To reduce computational and memory consumption, we follow~\cite{katharopoulos2020transformers,kitaev2020reformer} to compute linear cross-attention.
Specially, the interacted BEV volume $\textbf{V}_{BEV}'$ is obtained by:

\begin{equation} \label{eqca}
\begin{split}
\textbf{V}_{BEV}' &= CrossAtt(Q_{S},K_{B},V_{B} )  \\
                    & = \phi_q(Q_{S}) \textbf{G}_{B} = \phi_q(Q_{S}) (\phi_k{ (K_{B}) }^{T} V_{B} ),
\end{split}
\end{equation}


where $\phi_q$ and $\phi_k$ denote the softmax function along each row and column of the input matrix, respectively. $\textbf{G}_{B}$ represents global contextual vectors of the BEV representation.
In this way, $\textbf{V}_{BEV}'$ retrieves the relevant aspects of the stereo sides, thereby providing an alternative perspective on the feature importance of the BEV side. 
Likewise, its opposite interacted volume $\textbf{V}_{Stereo}'$ is computed by $CrossAtt(Q_{B},K_{S},V_{S} )$ to encourage reliable geometry information exchange.





\emph{Depth Confidence Filtering}. In order to further retrieve pixel-level reliable information for dense prediction, we develop a depth confidence filtering strategy, which explicitly takes advantage of the involved reliable geometry information behind the volume. We aim to utilize its depth confidence information to enforce the cross-attention operation similar to \cite{chen2020mvsnet++}. Particularly,  
to project the volume to a confidence map $\textbf{C}_S$ % \in \mathbb{R}^{H \times W}$
, we first adopt $softmax$ to convert depth cost value $d_i$ into a probability form, and then take out the highest probability value among all depth hypothesis planes along the depth dimension as the prediction confidence. The process is formally written as:
\begin{equation}
{\textbf{C}_S= WTA(\phi ( \textbf{V}_{Stereo} )) = WTA \left\{   \frac{\exp(d_i)} 
 {\sum_{j=1}^{D_{max}}\exp(d_j)}  \right\},}
\end{equation}
where the $softmax$ is applied across the depth dimension and $WTA$ represents winner-takes-all operation. $D_{max}$ denotes the length of the depth dimension. 

Next, we revisit the cross-attention operation in Equation~\ref{eqca} and construct pixel-level reliable retrieval with the confidence map $\textbf{C}_S$ to identify the criteria for an optimized formulation:

\begin{equation} \label{}
{ CrossAtt(Q_{S},K_{B},V_{B} ) =  \phi_q(Q_{S}) \odot \textbf{C}_S ( \phi_k{ (K_{B}) }^{T} V_{B} ),}
\end{equation}


where $\odot$ represents the element-wise product, through which the reliable geometry information is preserved while low-confidence information is suppressed.





\begin{figure}[!ht]
\vspace{0pt}
\includegraphics[width=8.3cm]{block22.png}  
\caption{The structure of the proposed Dual Volume Ensemble module, which is devised for mutually beneficial aggregation.} 

\label{figblock2}
\vspace{-10pt}
\end{figure} 

\subsubsection{Dual Volume Ensemble.}
With the interacted volume representations $\textbf{V}_{Stereo}'\in\mathbb{R}^{1 \times D \times H \times W}$ and $\textbf{V}_{BEV}'\in\mathbb{R}^{1 \times D \times H \times W}$, the primary objective of this module is to leverage their strengths and facilitate mutually beneficial complementation. 

As illustrated in Figure~\ref{figblock2},
the DVE takes as input concatenated features $\textbf{V}'_{cat}=[\textbf{V}_{Stereo}', \textbf{V}_{BEV}']\in \mathbb{R}^{2 \times D \times H \times W}$ and outputs ensembled volume $\textbf{V}_{ens}$. 
Especially, the input $\textbf{V}'_{cat}$ is first fed into residual 3D CNNs for regularization and channel adjustment, which generates transformed representation $\textbf{V}_f \in \mathbb{R}^{C_f \times D \times H \times W}$. The transformed representation $\textbf{V}_f$ is further processed by Channel-wise Recalibration and Multi-group Voting, which are described in detail below.

 \emph{Channel-wise Recalibration.}
To fully exploit its contextual information~\cite{hu2018squeeze}, we utilize average pooling to squeeze the information into a channel descriptor $\textbf{z}_c$. More specifically, we shrink $\textbf{V}_f$ along both the depth dimension $D$ and spatial dimension $ H \times W$:
\begin{equation}
    \textbf{z}_c = \frac{1}{D\times H\times W} \sum_{d=1}^D \sum_{i=1,j=1}^{H,W} \textbf{V}_f(d,i,j),
\end{equation}


Subsequently, an excitation block~\cite{hu2018squeeze} is leveraged to capture its channel-wise dependencies. Formally, the channel descriptor $\textbf{z}_c$ is updated by two stacked bottleneck-shape convolutions with non-linear activation. Finally, the updated channel descriptor is employed to re-weight the previous transformed feature $\textbf{V}_f$ along the channel dimension: 
\begin{equation}
     \textbf{V}_f ' = \sigma(\textbf{W}_2\delta(\textbf{W}_1 \textbf{z}_c))\cdot  \textbf{V}_f ,
\end{equation}
where the ${W}_1$ and ${W}_2$ represent $ 1\times1\times1 $ convolutions with dimensionality-reduction. The $\delta$ denotes standard GELU activation and the $\sigma$ indicates the sigmoid gate. 
 




 \emph{Multi-group Voting.}
 To further capture multi-scale context, we split $\textbf{V}_f'$ into four groups and employ 3D atrous convolutions~\cite{li2019rgbd,cao2022monoscene} with different dilation rates. 
The contextual information in different groups exhibits distinct receptive fields, staying non-interfering with each other.
Finally, we ensemble the multi-scale context between different voting groups to construct $\textbf{V}_{ens}$:\looseness=-1
\begin{equation}
\textbf{V}_{ens} = \mathbb{P} \left\{ \mathrm{Concat} \left( A_{g}^{i}(\mathrm{Split}_{channel}^{g1\sim g4}( \textbf{V}_f' )) \right) \right\},
\end{equation}
where $A_{g}^{i}$ denotes 3D atrous convolution employed in the $i^{th}$ context group. 
$\mathbb{P}$ is composed of point-wise convolution with GELU activation and group normalization. The point-wise convolution encourages channel-aware mixing by $1\times1\times1$ kernel size, through which the ensembled volume $\textbf{V}_{ens}$ takes into consideration voting features of different aspects.




\subsection{Semantic Scene Completion}
\label{sec:ssc}
To make use of this high-quality volume $\textbf{V}_{ens}$ for semantic scene completion, we augment it with its associated context information $\textbf{C}_{BEV}$. The extracted context information from input images is placed in a specific location of the bird-eye's representation by an outer product operation similar to~\cite{philion2020lift,li2023bevdepth}. Formally, the ensembled voxel features $\textbf{F}_{vox} \in \mathbb{R}^{C_b \times D \times H \times W}$ is computed by:
\begin{equation}
\textbf{F}_{vox} = \textbf{C}_{BEV} \otimes \textbf{V}_{ens},
\end{equation}
% where the $\textbf{V}_{ens}$ and $\textbf{C}_{BEV}$ are firstly unsqueezed on channel dimension and depth dimension, respectively.

In this way, we are able to seamlessly blend the complementary benefits of stereo representation of precise geometry and BEV features of rich semantic context.

\noindent \textbf{Semantic Segmentation Learning.} Following~\cite{cao2022monoscene}, we leverage 3D UNet to regularize the ensembled voxel features. Its output features are fed to a SSC head holding upsampling and a softmax layer for semantic occupancy prediction $\hat{\textbf{Y}}$.

\noindent \textbf{Network Training.} 
We follow the basic learning objective of MonoScene~\cite{cao2022monoscene} for semantic scene completion. 
Standard semantic loss $\mathcal{L}_{\text{sem}}$ and geometry loss $\mathcal{L}_{\text{geo}}$ are leveraged for semantic and geometry supervision, while an extra class weighting loss $\mathcal{L}_{ce}$ is also added.
To further enforce the ensembled volume, we adopt a binary cross entropy loss $\mathcal{L}_{depth}$ to encourage the sparse depth distribution. The overall learning objective of this framework is formulated as:
\begin{equation}
   { \mathcal{L} = \mathcal{L}_{depth} + \mathcal{\lambda}_{ce} \mathcal{L}_{ce} + \mathcal{\lambda}_{sem} \mathcal{L}_{\text{sem}}+ \mathcal{\lambda}_{geo} \mathcal{L}_{\text{geo}} . }
\end{equation}
where several $\lambda$s are balancing coefficients. 



\begin{table*}[!ht]
\vspace{-0pt}
\begin{center}
\small
\renewcommand\tabcolsep{5.2pt}
\begin{tabular}{l|ccccccc}
\toprule
\textbf{Methods}   & BRGScene(ours)  & VoxFormer-T & VoxFormer-S  & OccFormer & SurroundOcc & TPVFormer & MonoScene  \\ \midrule
\textbf{Input}  &  Stereo &  Stereo-T & Stereo &  Mono &  Mono  &  Mono &  Mono   \\ \midrule

\textbf{IoU (\%)} $\uparrow$      & \textbf{43.34}  & {43.21} & {42.95}  & 34.53  & 34.72 &34.25 &34.16    \\ \midrule
\textbf{mIoU (\%)} $\uparrow$     & \textbf{15.36}    & {13.41}  & 12.20 &12.32 & 11.86 &11.26   &11.08     \\ \midrule
\crule[carcolor]{0.13cm}{0.13cm} \textbf{car} (3.92\%)     & \textbf{22.80} & {21.70} & 20.80  & 21.60 &20.60 & 19.20   &  18.80  \\

\crule[bicyclecolor]{0.13cm}{0.13cm} \textbf{bicycle} (0.03\%)    & \textbf{3.40}  &{1.90} & 1.00& 1.50 &1.60 & 1.00 & {0.50}  \\

\crule[motorcyclecolor]{0.13cm}{0.13cm} \textbf{motorcycle} (0.03\%)   & \textbf{2.40} &{1.60} &0.70 & {1.70} &1.20 & 0.50  &  {0.70}    \\

\crule[truckcolor]{0.13cm}{0.13cm} \textbf{truck} (0.16\%)        & {2.80} & 3.60 &3.50 & {1.20} &1.40  & \textbf{3.70}  &  {3.30}  \\

\crule[othervehiclecolor]{0.13cm}{0.13cm} \textbf{other-veh.} (0.20\%)   & \textbf{6.10}  &4.10 & 3.70& 3.20 &{4.40} & 2.30  &{4.40} \\

\crule[personcolor]{0.13cm}{0.13cm} \textbf{person} (0.07\%)       & \textbf{2.90} &{1.60} &1.40 & {2.20} &1.40 &{1.10}  & {1.00}  \\

\crule[bicyclistcolor]{0.13cm}{0.13cm} \textbf{bicyclist} (0.07\%)    & {2.20} & 1.10 & \textbf{2.60} &1.10 &2.00  & {2.40} & {1.40}    \\

\crule[motorcyclistcolor]{0.13cm}{0.13cm} \textbf{motorcyclist} (0.05\%) & \textbf{0.50} & 0.00 &0.20 & 0.20 &0.10 & 0.30   &  {0.40}  \\

\crule[roadcolor]{0.13cm}{0.13cm} \textbf{road} (15.30\%)        & \textbf{61.90} &54.10 &53.90 & 55.90 &{56.90} &{55.10}    &{54.70}  \\

\crule[parkingcolor]{0.13cm}{0.13cm} \textbf{parking} (1.12\%)      & \textbf{30.70}  & 25.10& 21.10 &{31.50} &30.20 & {27.40}   & {24.80}   \\

\crule[sidewalkcolor]{0.13cm}{0.13cm} \textbf{sidewalk} (11.13\%)    & \textbf{31.20}  &26.90 &25.30 & {30.30} &28.30 & {27.20} & {27.10}  \\

\crule[othergroundcolor]{0.13cm}{0.13cm} \textbf{other-grnd} (0.56\%)    & \textbf{10.70} & {7.30} &5.60 &6.50 &6.80 & 6.50 & {5.70}  \\

\crule[buildingcolor]{0.13cm}{0.13cm} \textbf{building} (14.10\%)    & \textbf{24.20} &{23.50} & 19.80 &15.70 &15.20 & 14.80 & {14.40}     \\

\crule[fencecolor]{0.13cm}{0.13cm} \textbf{fence} (3.90\%)        & \textbf{16.50}  &{13.10} & 11.10 & 11.90 & 11.30 & {11.00} &  {11.10} \\

\crule[vegetationcolor]{0.13cm}{0.13cm} \textbf{vegetation} (39.3\%)   & {23.80}  & \textbf{24.40} & 22.40& 16.80 &14.90 & 13.90 & {14.90} \\

\crule[trunkcolor]{0.13cm}{0.13cm} \textbf{trunk} (0.51\%)        & \textbf{8.40} & {8.10} & 7.50 & 3.90 & 3.40 & 2.60   &2.40 \\

\crule[terraincolor]{0.13cm}{0.13cm} \textbf{terrain} (9.17\%)   & \textbf{27.00} &{24.20} &21.30 & 21.30 &19.30  &20.40 & {19.50}   \\

\crule[polecolor]{0.13cm}{0.13cm} \textbf{pole} (0.29\%)         & \textbf{7.00}  & {6.60} & 5.10 & 3.80 &3.90 & 2.90 &3.30  \\

\crule[trafficsigncolor]{0.13cm}{0.13cm} \textbf{traf.-sign} (0.08\%)   & \textbf{7.20} &{5.70} & 4.90 & 3.70 &2.40 & 1.50 &2.10  \\  \bottomrule

\end{tabular}
\vspace{-5pt}
\caption{\textbf{Quantitative results} on the SemanticKITTI test set. The top two performers are marked \textbf{bold}. Our method outperforms temporal stereo-based (Stereo-T) VoxFormer-T in terms of mIoU.\looseness=-1 }
\label{tabq1}        
\vspace{-10pt}
\end{center}
\end{table*}

% \begin{table*}[!ht]
% \vspace{-19pt}
% \begin{center}
% \small
% \renewcommand\tabcolsep{1.2pt}
% \begin{tabular}{l|cccccc}
% \toprule
% \textbf{Methods}   & BRGScene(ours)  & VoxFormer-T(2023) & VoxFormer-S(2023)  & TPVFormer(2023) & MonoScene(2022) & SSCNet(2017) \\ \midrule
% \textbf{Input}  &  Stereo &  Stereo-T & Stereo &  Mono &  Mono  & LiDAR  \\ \midrule

% \textbf{IoU (\%)} $\uparrow$      & \underline{43.34}  & {43.21} & {42.95} & {34.25}  & {34.16}    &   \textbf{49.98}   \\ \midrule
% \textbf{mIoU (\%)} $\uparrow$     & \textbf{15.36}    & {13.41}  & 12.20 & 11.26   & {11.08}     &  \underline{16.14}  \\ \midrule
% \crule[carcolor]{0.13cm}{0.13cm} \textbf{car} (3.92\%)     & \textbf{24.90} & {21.70} & 20.80 & 19.20 &  18.80 & \underline{24.26}  \\

% \crule[bicyclecolor]{0.13cm}{0.13cm} \textbf{bicycle} (0.03\%)    & \textbf{2.60}  &\underline{1.90} & 1.00& 1.00 & {0.50} &0.54   \\

% \crule[motorcyclecolor]{0.13cm}{0.13cm} \textbf{motorcycle} (0.03\%)   & \textbf{1.80} &\underline{1.60} &0.70 & 0.50  &  {0.70} &0.78  \\

% \crule[truckcolor]{0.13cm}{0.13cm} \textbf{truck} (0.16\%)        & \textbf{5.30} & 3.60 &3.50 &\underline{3.70}  &  {3.30}  &1.18 \\

% \crule[othervehiclecolor]{0.13cm}{0.13cm} \textbf{other-veh.} (0.20\%)   & \textbf{8.10}  &4.10 & 3.70& 2.30 &\underline{4.40} &4.43  \\

% \crule[personcolor]{0.13cm}{0.13cm} \textbf{person} (0.07\%)       & \textbf{2.00} &\underline{1.60} &1.40 &{1.10}  & {1.00}   &0.25\\

% \crule[bicyclistcolor]{0.13cm}{0.13cm} \textbf{bicyclist} (0.07\%)    & \underline{2.30} & 1.10 & \textbf{2.60} & {2.40} & {1.40}  &0.25 \\

% \crule[motorcyclistcolor]{0.13cm}{0.13cm} \textbf{motorcyclist} (0.05\%) & \textbf{0.90} & 0.00 &0.20 & 0.30   &  \underline{0.40} &0.03  \\

% \crule[roadcolor]{0.13cm}{0.13cm} \textbf{road} (15.30\%)        & \textbf{62.10} &54.10 &53.90 &\underline{55.10}   &{54.70}  &51.15 \\

% \crule[parkingcolor]{0.13cm}{0.13cm} \textbf{parking} (1.12\%)      & \textbf{32.70}  & 25.10& 21.10&\underline{27.40}  & {24.80} &27.12  \\

% \crule[sidewalkcolor]{0.13cm}{0.13cm} \textbf{sidewalk} (11.13\%)    & \textbf{32.60}  &26.90 &25.30 & {27.20} & {27.10} &\underline{30.76}  \\

% \crule[othergroundcolor]{0.13cm}{0.13cm} \textbf{other-grnd} (0.56\%)    & \textbf{11.00} & \underline{7.30} &5.60 & 6.50 & {5.70}  &6.44 \\

% \crule[buildingcolor]{0.13cm}{0.13cm} \textbf{building} (14.10\%)    & \underline{26.00} &{23.50} & 19.80 & 14.80 & {14.40}  &\textbf{34.53}   \\

% \crule[fencecolor]{0.13cm}{0.13cm} \textbf{fence} (3.90\%)        & \underline{18.40}  &{13.10} & 11.10 & {11.00} &  {11.10} &\textbf{19.87}  \\

% \crule[vegetationcolor]{0.13cm}{0.13cm} \textbf{vegetation} (39.3\%)   & \underline{24.90}  & {24.40} & 22.40& 13.90 & {14.90} & \textbf{35.25}\\

% \crule[trunkcolor]{0.13cm}{0.13cm} \textbf{trunk} (0.51\%)        & \textbf{10.60} & \underline{8.10} & 7.50 & 3.70    &  3.30 &1.18 \\

% \crule[terraincolor]{0.13cm}{0.13cm} \textbf{terrain} (9.17\%)      & \underline{28.00} &{24.20} &21.30 &20.40 & {19.50} & \textbf{29.01}  \\

% \crule[polecolor]{0.13cm}{0.13cm} \textbf{pole} (0.29\%)         & \underline{8.90}  & {6.60} & 5.10 & 2.90 &3.30  &\textbf{13.10} \\

% \crule[trafficsigncolor]{0.13cm}{0.13cm} \textbf{traf.-sign} (0.08\%)   & \textbf{8.20} &{5.70} & 4.90 & 1.50 &2.10 &\underline{6.73}   \\  \bottomrule

% \end{tabular}
% \vspace{-5pt}
% \caption{\textbf{Quantitative results} on the SemanticKITTI test set. The top two performers are marked \textbf{bold} and \underline{underlined}, respectively. Our method outperforms temporal stereo-based (Stereo-T) VoxFormer-T and LiDAR-based SSCNet in terms of mIoU.\looseness=-1 }
% \label{tabq1}        
% \vspace{-0pt}
% \end{center}
% \end{table*}





% \begin{table}[!ht]
% \begin{center}
% \small
% \renewcommand\tabcolsep{3.0pt}
% \begin{tabular}{l|ccc}
% \toprule
%  \textbf{Methods}& \textbf{Input}& \textbf{IoU (\%)} $\uparrow$      & \textbf{mIoU (\%)} $\uparrow$      \\  \midrule
% MonoScene (2022)%~\cite{cao2022monoscene}   
% &  Mono   & 36.80            &    11.30             \\
% TPVFormer (2023)%~\cite{huang2023tri} 
% &  Mono   & 35.61 & 11.36    \\
% VoxFormer-S (2023)%~\cite{li2023voxformer} 
% &  Stereo &{44.02}     &   {12.35}     \\ 
% % VoxFormer-T (2023)%~\cite{li2023voxformer} 
% % &  Stereo-T &{44.15}   &   {13.35}     \\ 
% \rowcolor{gray!10} BRGScene (ours) & Stereo &\textbf{44.22} & \textbf{16.02}  \\  
%   \bottomrule
% \end{tabular}
% \vspace{-5pt}
% \caption{\textbf{Quantitative results} on the SemanticKITTI validation set against state-of-the-art SSC methods.  }
% \vspace{-0pt}
% \label{tabq2}
% \end{center}
% \end{table}



\begin{table}[!ht]
\begin{center}
\small
\renewcommand\tabcolsep{3.0pt}
\begin{tabular}{l|ccc}
\toprule
 \textbf{Methods}& \textbf{Input}   & \textbf{mIoU (\%)} $\uparrow$ & \textbf{Time (s)} $\downarrow$      \\  \midrule
 SSCNet${}^{*}$ (2017) &  Stereo-PTS & 10.31 &\textbf{0.187}\\
 LMSCNet${}^{*}$ (2020) &  Stereo-PTS & 10.45 & 0.214 \\
MonoScene${}^{*}$  (2022)  & Stereo    &    12.82  &   {0.274}            \\
TPVFormer${}^{*}$  (2023)  &  Stereo   & 13.06 & 0.313   \\
OccFormer${}^{*}$  (2023)  & Stereo  &  13.57 & 0.338   \\

VoxFormer-S (2023)    &  Stereo   &   {12.35} &  {0.256}    \\ 
VoxFormer-T (2023)  &  Stereo-T  &   { 13.35 }  & 0.307   \\ 
\rowcolor{gray!10} BRGScene (ours) & Stereo & \textbf{15.43}  & 0.285 \\  
 \bottomrule
\end{tabular}
\vspace{-5pt}
\caption{ \textbf{Evaluation results of stereo variants.} For MonoScene${}^{*}$ and TPVFormer${}^{*}$, We employ stereo images as inputs. For SSCNet${}^{*}$ and LMSCNet${}^{*}$, we leverage stereo depth net to generate pseudo point clouds (Stereo-PTS). }
\vspace{-5pt}
\label{tabq3}
\end{center}
\end{table}



\section{Experiments}
\subsection{Experimental Settings}
\noindent \textbf{Datasets.}
We evaluate the proposed \textbf{BRGScene} on SemanticKITTI~\cite{behley2019semantickitti} that is popularly used in a great number of previous studies. There are 22 driving outdoor scenes from the KITTI Odometry Benchmark~\cite{geiger2012we}, covering diverse and challenging autonomous driving situations. SemanticKITTI holds semantic annotations of LiDAR sweeps that are registered, aggregated and voxelized as 256$\times$256$\times$32 grid of 0.2m voxels. The target ground truth of each voxel grid is annotated as one of 21 classes (1 unknown, 1 free and 19 semantics). The SemanticKITTI benchmark provides both voxelized LiDAR scans and RGB images as model input options. We solely utilize RGB images since our main focus is to explore the portable camera-only signals as did in MonoScene~\cite{cao2022monoscene}. 

\noindent \textbf{Implementation Details.} We set the 3D UNet input to 128$\times$128$\times$16 (1:2) for efficient memory usage, whose feature will be upscaled to 1:1 at completion head by deconvolution operation. The $\lambda$s are empirically set to 1. Our model is implemented on PyTorch. The model is trained for 30 epochs using the AdamW optimizer~\cite{loshchilov2017decoupled} with a learning rate of $1\times10^{-4}$ and batch size set to 8.\looseness=-1




\noindent \textbf{Evaluation Metrics}.
Regarding quantitative evaluations, we conduct experiments on the typical metrics~\cite{li2023voxformer,cao2022monoscene} that have been widely employed in the field of SSC. Specially, we leverage \textbf{IoU} (Intersection over Union) to account for the scene completion (SC) task and \textbf{mIoU} (mean Intersection over Union) to measure the performance of the semantic scene completion (SSC) task, respectively. For both of these two metrics, higher values are desirable, where high IoU indicates accurate geometric prediction and high mIoU implies precise semantic segmentation.\looseness=-1



\begin{figure*}[!ht]
   \vspace{-0pt}
	\begin{center}
		\includegraphics[width=16.0
  cm]{vis_all2.png}   
        \vspace{0pt}
		\begin{tabular}{cccccc}
			\multicolumn{6}{c}{
				\scriptsize
				\textcolor{bicycle}{$\blacksquare$}bicycle~
				\textcolor{car}{$\blacksquare$}car~
				\textcolor{motorcycle}{$\blacksquare$}motorcycle~
				\textcolor{truck}{$\blacksquare$}truck~
				\textcolor{other-vehicle}{$\blacksquare$}other vehicle~
				\textcolor{person}{$\blacksquare$}person~
				\textcolor{bicyclist}{$\blacksquare$}bicyclist~
				\textcolor{motorcyclist}{$\blacksquare$}motorcyclist~
				\textcolor{road}{$\blacksquare$}road~
				}
    \\
    
			\multicolumn{6}{c}{
				\scriptsize
                    \textcolor{parking}{$\blacksquare$}parking~
				\textcolor{sidewalk}{$\blacksquare$}sidewalk~
				\textcolor{other-ground}{$\blacksquare$}other ground~
				\textcolor{building}{$\blacksquare$}building~
				\textcolor{fence}{$\blacksquare$}fence~
				\textcolor{vegetation}{$\blacksquare$}vegetation~
				\textcolor{trunk}{$\blacksquare$}trunk~
				\textcolor{terrain}{$\blacksquare$}terrain~
				\textcolor{pole}{$\blacksquare$}pole~
				\textcolor{traffic-sign}{$\blacksquare$}traffic sign			
			}
		\end{tabular}	
 	\end{center}
        \vspace{-10pt}
	\caption{ \textbf{Qualitative results} on the SemanticKITTI validation set. The overlay shadow areas at the bottom of semantic predictions denote unseen scenery out of the camera's field of view (FOV). } 
	\label{fig_q}
 \vspace{-10pt}
\end{figure*} 



\begin{table}[!ht]\centering
\small
\renewcommand\tabcolsep{5.5pt}
\begin{tabular}{ l |ccc}
\toprule
 \textbf{Methods}& \textbf{Resolution}& \textbf{mAP} $\uparrow$  & \textbf{NDS} $\uparrow$   \\  \midrule
% FCOS3D \\
% DETR3D\\

BEVDet-Base (2021) & 1600$\times$ 640 & 0.397 & 0.477 \\

BEVDet4D-Base (2021) & 1600$\times$ 640 & 0.426 & 0.552 \\

PETR-R101 (2022) & 1408$\times$ 512 & 0.357 & 0.421\\

BEVDepth-R101 (2023) & 512$\times$ 1408 & 0.412 & 0.535 \\

\rowcolor{gray!10} BRGScene (ours) & 1600$\times$ 640  &\textbf{0.451} &\textbf{0.563} \\  
  \bottomrule
\end{tabular}
\vspace{-5pt}
\caption{ \textbf{Quantitative results} of BEV Detection on the nuScenes validation set. We conduct preliminary experiments by employing the detection head. }
\vspace{-5pt}
\label{tabcom2}
\end{table}



\begin{figure}[!ht]
   \vspace{-0pt}
	\begin{center}
        \vspace{-0pt}
        \includegraphics[width=0.8\linewidth]{detection.pdf}
        \end{center}
        \vspace{-5pt}
	\caption{\textbf{Visualization results} of BEV detection on the nuScenes validation set. } 
	\label{fgvisdet2}
 \vspace{-15pt}
\end{figure} 


% \begin{figure}[!ht] 
% \vspace{-12pt}
% \begin{minipage}[t]{0.43\linewidth}
% \vspace{-0pt}
% \begin{center}
% \captionsetup{type=table}
% \caption{Quantitative results on nuScenes validation set.  }
% \label{tabcom2}
% \vspace{-10pt}
% \renewcommand\tabcolsep{0.2pt}
% \scriptsize

% \begin{tabular}{ l |ccc}
% \toprule
%  \textbf{Methods}& \textbf{Resolution}& \textbf{mAP} $\uparrow$  & \textbf{NDS} $\uparrow$   \\  \midrule
% % FCOS3D \\
% % DETR3D\\
% PETR-R101 & \\
% BEVDet-Base~\cite{huang2021bevdet} & 1600$\times$ 640 & 0.397 & 0.477 \\
% BEVDepth-R101~\cite{li2023bevdepth} & 512$\times$ 1408 & 0.412 & 0.535 \\
% BEVDet4D-Base~\cite{huang2021bevdet} & 1600$\times$ 640 & 0.426 & 0.552 \\
% \rowcolor{gray!10} BRGScene & 1600$\times$ 640  &\textbf{0.451} &\textbf{0.563} \\  
%   \bottomrule
% \end{tabular}
% \vspace{-10pt}

% \vspace{0pt}
% \label{fgvisdet2}
% \end{center}
% \end{minipage}
% \hfill
% \begin{minipage}[t]{0.5\linewidth}
% \vspace{-0pt}
% \begin{center}
% \caption{Visualization on nuScenes validation set.
% % of the semantic scene nuScenes-Occupancy results.
% }
% \vspace{-10pt}
% \includegraphics[width=0.8\linewidth]{detection.pdf}
% \end{center}
% % \label{fgvisdet}
% \end{minipage}
% \vspace{-12pt}
% \end{figure}
% % \vspace{-5pt}




\subsection{Performance}
\noindent \textbf{Quantitative Comparison.}
Table \ref{tabq1} reports the performance of our BRGScene and other baselines on the SemanticKITTI test set. 
% \noindent \textbf{Comparison Methods}.
We compare our method with the best models~\cite{li2023voxformer,zhang2023occformer,huang2023tri,wei2023surroundocc,cao2022monoscene} for semantic scene completion. 
BRGScene surpasses temporal stereo-based (Stereo-T) VoxFormer-T~\cite{li2023voxformer} in terms of mIoU.
% \textcolor{blue}{
% \textbf{MonoScene}~\cite{cao2022monoscene} is a very-first camera-based SSC method, relying on feature projection for 2D-to-3D scene reconstruction.  \textbf{VoxFormer-T}~\cite{li2023voxformer} is the temporary version of VoxForme-S that leverages a sequential of stereo images \textcolor{red}{to get depth-based query proposal.} 
% }
 % Other baselines such as \textbf{LMSCNet}~\cite{roldao2020lmscnet} and \textbf{SSCNet}~\cite{song2017semantic} require 3D geometric input. \textcolor{red}{For a fair comparison, we adapt these baselines with pseudo 3D inputs generated by stereo depth following Voxfomer~\cite{li2023voxformer}.  } 
Additionally, our approach outperforms stereo-based VoxFormer-S by a large margin in terms of geometric completion (42.95$\rightarrow$43.34) and semantic segmentation (12.20$\rightarrow$15.36).
It's worth noting that our method demonstrates significant superiority in the prediction of small moving objects compared to VoxFormer-S, including bicycle (1.00$\rightarrow$3.40), motorcycle (0.70$\rightarrow$2.40), pole (5.10$\rightarrow$7.00), etc. We ascribe such improvements to the ensemble of dual volume, which is critical for 3D geometry awareness. 
% As shown in Table~\ref{tabq2}, we also report the quantitative results on the SemanticKITTI validation set. Our method surpasses VoxFormer-S by 4.19 in semantics.
Besides, although VoxFormer-T employs up to 4 temporal stereo image pairs as inputs, our method has a significant advantage in mIoU, with a 14.5\% relative improvement in terms of semantics.\looseness=-1


\noindent \textbf{Evaluation of Stereo Variants.}
To ensure fair comparisons, we also implement stereo variants of the baselines as shown in Table \ref{tabq3}. For MonoScene${}^{*}$~\cite{cao2022monoscene} and TPVFormer${}^{*}$~\cite{huang2023tri}, we employ left and right images to generate stereo-based predictions. 
Other LiDAR-based baselines including LMSCNet~\cite{roldao2020lmscnet} and SSCNet~\cite{song2017semantic} require 3D geometric inputs, so we adapt them with pseudo-3D inputs leveraging the same GwcNet~\cite{guo2019group} used in our framework.  
Our proposed BRGScene efficiently outperforms all the other methods. For more details on model complexity, please refer to the Supplementary Material.\looseness=-1



\noindent \textbf{Qualitative Comparison. }
 As shown in Figure~\ref{fig_q}, we compare the visualization results of BRGScene and VoxFormer on the SemanticKITTI validation set. 
 Due to the complexity of the real-world scenes and the sparsity of the labels, it is challenging to reconstruct the scenes accurately and completely. 
Compared to VoxFormer-T and VoxFormer-S, our method evidently captures better geometric representations for more complete and precise scene reconstruction (e.g. crossroads in rows 1,2) and generates more proper hallucinations outside FOV (e.g. shadow regions in rows 2,3).



\noindent \textbf{BEV Detection Evaluation.} We further conduct preliminary experimental results for BEV 3D detection on nuScenes validation set. Specifically, we adopt \textit{BEVDet}~\cite{huang2021bevdet} as the baseline setting, and replace the \textit{BEVDet} model with our proposed \textit{BRGScene} while maintaining the detection head. 
Note that we adopt temporal inputs from current and previous images to construct the temporal volume, which replaces the original stereo volume.
The preliminary results are in Table~\ref{tabcom2} and Figure~\ref{fgvisdet2}, which show that our proposed method can also be applied to a more wide range of downstream tasks.





% \begin{table}[!ht]\centering
% \small
% \begin{tabular}{l|cc}
% \toprule
%   \textbf{Volumes}   & \textbf{IoU (\%)} $\uparrow$      & \textbf{mIoU (\%)} $\uparrow$      \\ \midrule
  
% \rowcolor{gray!10} Ours      & \textbf{44.22} & \textbf{16.02} \\
% Ours w/o Stereo volume    &      40.81     &   12.91          \\
% Ours w/o BEV volume &   40.75          &   12.98         \\
% % Ours w/o Confidence Filtering&    42.19   &   14.67             \\ 
% \bottomrule
% \end{tabular}
% \vspace{-10pt}
% \caption{ \textbf{Ablation study for the dual volume.} We conduct the experiment with an individual volume and maintain the DVE module.  
% % The best performance is achieved by aggregating the both volumes with confidence filtering. 
% }
% \vspace{-10pt}
% \label{tab_volume}
% \end{table}




% \begin{table}[!ht]\centering
% \small
% \begin{tabular}{cccc}\toprule
% \multicolumn{2}{c}{\textbf{Mutual Interactive Ensemble}} &\multirow{2}{*}{\textbf{IoU (\%)} $\uparrow$ } &\multirow{2}{*}{\textbf{mIoU (\%)} $\uparrow$} \\
%  BRI & DVE   \\ \midrule
%   & & 41.05 & 13.12  \\
% \checkmark& & 43.41 &15.23   \\
%  &\checkmark &43.39 & 15.18  \\
% \rowcolor{gray!10} \checkmark &\checkmark & \textbf{44.22} & \textbf{16.02}  \\
% \bottomrule
% \end{tabular}
%  \vspace{-10pt}
% \caption{\textbf{Ablation study for Mutual Interactive Ensemble.} The BRI and DVE in the MIE block enhance the performance of geometry and semantics significantly.}
% \label{tab_ar}
% \end{table}





\begin{table}[!ht]\centering
\small
% \renewcommand\tabcolsep{3.0pt}
\begin{tabular}{cc|cc|cc}\toprule
\multicolumn{2}{c|}{\textbf{Dual Volume}} & \multicolumn{2}{c|}{\textbf{MIE}} &\multirow{2}{*}{\textbf{IoU (\%)} $\uparrow$ } &\multirow{2}{*}{\textbf{mIoU (\%)} $\uparrow$} \\
 Stereo & BEV & BRI & DVE   \\ \midrule
  &        & &  &     33.87    &   9.92   \\
\checkmark& & &  &   37.38          &   11.49   \\
          &\checkmark & & & 36.34     &   11.06  \\ \midrule
\checkmark &\checkmark & & & 39.68 & 12.53  \\
\checkmark &\checkmark & \checkmark& & 43.04 &14.64   \\
\checkmark &\checkmark &           &\checkmark &42.92 & 14.59  \\
\rowcolor{gray!10} \checkmark &\checkmark & \checkmark &\checkmark & \textbf{43.85} & \textbf{15.43}  \\
\bottomrule
\end{tabular}
\vspace{-5pt}
\caption{ \textbf{Ablation study for architectural components.} }
\vspace{-15pt}
\label{tab_ar}
\end{table}





\subsection{Ablation Study}
We ablate our BRGScene on the SemanticKITTI validation set for semantic scene completion. 
% \subsubsection{Effect of Architectural Components.}
The ablation study for the architectural components is shown in Table~\ref{tab_ar}, which includes the Dual Volume and the Mutual Interactive Ensemble block. The baseline in the first row of the table is built by removing the Dual Volume and the MIE block.

\noindent \textbf{Effect of the Dual Volume.}
% The ablation study for the dual volume is shown in Table~\ref{tab_volume}. 
For the ablation study on the dual volume, we build the framework with an individual volume 
% and maintain the DVE module 
to verify the effect.
We find that after adding the stereo volume, the geometric perception ability of the framework improves obviously (+3.51 IoU), while the prediction performance of semantic scene segmentation is enhanced as well (+1.57 mIoU). 
The introduction of BEV volume also has an obvious impact on the semantic and the geometric aspects
, boosting IoU by 2.47 and mIoU by 1.14, respectively.\looseness=-1

% For "w/o Confidence Filtering", we remove the confidence filter and directly multiply the guidance attention matrix with the value vector, while both of the volumes are utilized and aggregated. The confidence filtering further improves the IoU (+1.66) and mIoU (+0.76) as expected.

% Figure~\ref{fig_q} visualizes the impact of the volumes. 
% As the figure shows, the prediction results of scene semantics (e.g. sidewalk, column 2) and geometry (e.g. distant truck, column 2) are significantly improved with the introduction of BEV volume and stereo volume, respectively. 
% It validates the effectiveness of our confidence filtering on retaining the high confidence regions for better aggregation (e.g. trunks, column1 and column 3). 


\noindent \textbf{Effect of the Mutual Interactive Ensemble.}
We further conduct architectural ablation to evaluate the impact of the Mutual Interactive Ensemble (MIE) block as shown in Table~\ref{tab_ar}. The BRI module can significantly improve the geometric and semantic estimations (+3.36 IoU, +2.11 mIoU) with efficient mutual interaction. Furthermore, we evaluate the DVE module by replacing the alternative naive concatenation, which leads to significant improvements in performance (+3.24 IoU, +2.06 mIoU).
The aforementioned results validate that our complementary mutual interaction has a significant performance improvement compared to naive aggregation.

% \section{Model Complexity with Different 2D Feature Scales}
% Following previous works~\cite{li2023voxformer}, we also study the effect of 2D image feature scales on model complexity, which is shown in Table~\ref{tab_feature}. We construct image features with different scales by adjusting the convolution strides in the 2D UNet and evaluate the running time and memory consumption during model inference. As we can see from the table, using the feature scale of $1/8$ of the input image size achieves a compelling balance between the performance and the model complexity.






% \noindent \textbf{Effect of 2D Feature Scales. }
% Following previous works~\cite{cao2022monoscene,li2023voxformer}, we also study the effect of 2D image feature scales, which is shown in Table~\ref{tab_feature}. We construct image features with different scales by adjusting the convolution strides in the 2D UNet. As we can see from the table, using feature scale of $1/8$ of the input image size achieves an compelling balance between the performance and the inference speed.

% % the results can be obviously improved by combining features with different scales, increasing the IoU by 1.97 and mIoU by 1.01, respectively. 






\section{Conclusion}
In this work, we propose BRGScene, a 3D Semantic Scene Completion framework that leverages both stereo and BEV representations to produce reliable 3D scene understanding results. 
To bridge the representation gap between the stereo volume and BEV volume for fine-grained 3D perception, a {Mutual Interactive Ensemble} block is proposed to incorporate complementary merits of the two dense volumes.
Our BRGScene outperforms existing camera-based state-of-the-arts on the challenging SemanticKITTI dataset.
We hope BRGScene could inspire further research in camera-based SSC and its applications in 3D scene understanding.



\bibliographystyle{named}
\bibliography{ijcai24}

\end{document}

