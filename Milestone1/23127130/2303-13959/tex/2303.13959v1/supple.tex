\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{float}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{indentfirst}
% \usepackage{parskip}
\usepackage{caption} 


% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
 % \usepackage[sorting=none]{biblatex}
\usepackage{xcolor}
\usepackage{url}
\usepackage{balance}
\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}

\newcommand{\jx}{\textcolor{red}}

\usepackage{booktabs, multirow} % for borders and merged ranges
\definecolor{roadcolor}{RGB}{234,51,246}
\definecolor{sidewalkcolor}{RGB}{68,8,72}
\definecolor{parkingcolor}{RGB}{241,156,249}
\definecolor{othergroundcolor}{RGB}{160,32,76}
\definecolor{buildingcolor}{RGB}{246,202,69}
\definecolor{carcolor}{RGB}{111,149,238}
\definecolor{truckcolor}{RGB}{74,32,172}
\definecolor{bicyclecolor}{RGB}{136,227,242}
\definecolor{motorcyclecolor}{RGB}{37,59,146}
\definecolor{othervehiclecolor}{RGB}{96,81,242}
\definecolor{vegetationcolor}{RGB}{79, 173, 50}
\definecolor{trunkcolor}{RGB}{126, 65, 22}
\definecolor{terraincolor}{RGB}{171, 238, 105}
\definecolor{personcolor}{RGB}{234, 60, 49}
\definecolor{bicyclistcolor}{RGB}{234, 66, 195}
\definecolor{motorcyclistcolor}{RGB}{138, 42, 90}
\definecolor{fencecolor}{RGB}{238, 128, 69}
\definecolor{polecolor}{RGB}{252, 241, 161}
\definecolor{trafficsigncolor}{RGB}{233, 51, 35}
\definecolor{color1}{RGB}{176, 36, 24}
% \definecolor{color2}{RGB}{0, 176, 80}
\definecolor{color2}{RGB}{119,185,0}
\definecolor{color3}{RGB}{0, 0, 200}
\definecolor{colorofteaser}{RGB}{176, 36, 24}


\newcommand{\tbr}[1]{\textbf{\textcolor{color1}{#1}}}
\newcommand{\tbg}[1]{\textbf{\textcolor{color2}{#1}}}
\newcommand{\tbb}[1]{\textbf{\textcolor{color3}{#1}}}
\newcommand{\teaser}[1]{\textbf{\textcolor{colorofteaser}{#1}}}

\newcommand{\chaowei}[1]{\textbf{\textcolor{color3}{Chaowei: #1}}}


%%--------------mono
\definecolor{LightGrey}{rgb}{.9,.9,.9}
\definecolor{White}{rgb}{1.,0.,1.}
\definecolor{first}{rgb}{.8,.0,.0}
\definecolor{second}{rgb}{.0,.6,.0}
\definecolor{third}{rgb}{.0,.0,.8}
% \newcolumntype{g}{>{\columncolor{White}}c}


\definecolor{ceiling}{RGB}{214,  38, 40}   %
\definecolor{floor}{RGB}{43, 160, 4}     %
\definecolor{wall}{RGB}{158, 216, 229}  %
\definecolor{window}{RGB}{114, 158, 206}  %
\definecolor{chair}{RGB}{204, 204, 91}   %
\definecolor{bed}{RGB}{255, 186, 119}  %
\definecolor{sofa}{RGB}{147, 102, 188}  %
\definecolor{table}{RGB}{30, 119, 181}   %
\definecolor{tvs}{RGB}{160, 188, 33}   %
\definecolor{furniture}{RGB}{255, 127, 12}  %
\definecolor{objects}{RGB}{196, 175, 214} %

\definecolor{car}{rgb}{0.39215686, 0.58823529, 0.96078431}
\definecolor{bicycle}{rgb}{0.39215686, 0.90196078, 0.96078431}
\definecolor{motorcycle}{rgb}{0.11764706, 0.23529412, 0.58823529}
\definecolor{truck}{rgb}{0.31372549, 0.11764706, 0.70588235}
\definecolor{other-vehicle}{rgb}{0.39215686, 0.31372549, 0.98039216}
\definecolor{person}{rgb}{1.        , 0.11764706, 0.11764706}
\definecolor{bicyclist}{rgb}{1.        , 0.15686275, 0.78431373}
\definecolor{motorcyclist}{rgb}{0.58823529, 0.11764706, 0.35294118}
\definecolor{road}{rgb}{1.        , 0.        , 1.        }
\definecolor{parking}{rgb}{1.        , 0.58823529, 1.        }
\definecolor{sidewalk}{rgb}{0.29411765, 0.        , 0.29411765}
\definecolor{other-ground}{rgb}{0.68627451, 0.        , 0.29411765}
\definecolor{building}{rgb}{1.        , 0.78431373, 0.        }
\definecolor{fence}{rgb}{1.        , 0.47058824, 0.19607843}
\definecolor{vegetation}{rgb}{0.        , 0.68627451, 0.        }
\definecolor{trunk}{rgb}{0.52941176, 0.23529412, 0.        }
\definecolor{terrain}{rgb}{0.58823529, 0.94117647, 0.31372549}
\definecolor{pole}{rgb}{1.        , 0.94117647, 0.58823529}
\definecolor{traffic-sign}{rgb}{1.        , 0.        , 0.    }   

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission


\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi



\begin{document}

%%%%%%%%% TITLE

\title{Supplementary Material for BEV-Assisted Stereo Matching Empowers 3D Semantic Scene Completion }

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


\begin{figure*}[tH]
	
	\begin{center}
		\includegraphics[width=16cm]{demo_video.png}   
        \vspace{0pt}
		\begin{tabular}{cccccc}
			\multicolumn{6}{c}{
				\scriptsize
				\textcolor{bicycle}{$\blacksquare$}bicycle~
				\textcolor{car}{$\blacksquare$}car~
				\textcolor{motorcycle}{$\blacksquare$}motorcycle~
				\textcolor{truck}{$\blacksquare$}truck~
				\textcolor{other-vehicle}{$\blacksquare$}other vehicle~
				\textcolor{person}{$\blacksquare$}person~
				\textcolor{bicyclist}{$\blacksquare$}bicyclist~
				\textcolor{motorcyclist}{$\blacksquare$}motorcyclist~
				\textcolor{road}{$\blacksquare$}road~
				}
    \\
    
			\multicolumn{6}{c}{
				\scriptsize
                    \textcolor{parking}{$\blacksquare$}parking~
				\textcolor{sidewalk}{$\blacksquare$}sidewalk~
				\textcolor{other-ground}{$\blacksquare$}other ground~
				\textcolor{building}{$\blacksquare$}building~
				\textcolor{fence}{$\blacksquare$}fence~
				\textcolor{vegetation}{$\blacksquare$}vegetation~
				\textcolor{trunk}{$\blacksquare$}trunk~
				\textcolor{terrain}{$\blacksquare$}terrain~
				\textcolor{pole}{$\blacksquare$}pole~
				\textcolor{traffic-sign}{$\blacksquare$}traffic sign			
			}
		\end{tabular}	
	% 	\label{fig:qualitative_kitti}
	% \end{subfigure}
 	\end{center}
        % \vspace{1mm}
        \vspace{0pt}
	\caption{ More visualization results on the SemanticKITTI~\cite{behley2021towards} validation set. From top to below demonstrate 3 cases. For each one, the first row illustrates stereo input images while the second row depicts comparison results of MonoScene and \textbf{StereoScene}.  
 } 
	\label{fig_video}
\end{figure*} 

%%%%%%%%% 1.mono对比的左右的视频  2. occdepth的比较  3. 更多可视化 


\begin{figure*}[!ht]
\vspace{0pt}
\hsize=\textwidth %
\centering
\includegraphics[width=1\textwidth]{stereo_constructor.png}
\caption{Implementation details of the stereo representation construction. The left and right features are encoded with camera parameters before forming the disparity volume.
}
\label{figstereo}
 \vspace{0pt}
\end{figure*}

\section{More Visualization Results}
% \textbf{Precision of Semantic Classes}
% \textbf{Completion of Overall Scenes}
Below we provide qualitative comparisons on SemanticKITTI validation set and show visualization examples in Figure~\ref{fig_video}.
Compared to MonoScene~\cite{cao2022monoscene}, our \textbf{StereoScene} generates more precise predictions (e.g. truck, example 1; car, example 3) and more complete scenes with proper hallucinations (e.g. example 2).

Specifically, in example 1, MonoScene predicts wrong shape for truck instance prediction (see yellow circle).
In example 2, compared to MonoScene, our method successfully hallucinates a more complete and reasonable scene outside the camera FOV. Moreover, the semantics generated by StereoScene is consistent with the holistic scenes in most regions.
In example 3, StereoScene precisely capture the semantic and geometry of car while MonoScene faces challenges in correct recognition.
% Due to the lack of image information, predictions outside the field of view (FOV) are challenging for camera-based methods. 





\section{Implementation Details on Dual Volume Construction}

In this section, we describe implementation details for our dual volume construction. The \emph{Stereo Representation Construction} module targets to match 2D extracted features and outputs a structural 3D representation, regularized depth volume. The \emph{BEV Representation Construction} module constructs a 3D latent volume coupled with context features from 2D image features.

\noindent \textbf{More Details on Stereo Representation Construction. }
As mentioned in Sec. 3.2 of the main paper, we construct stereo geometric volume from the left and right image features. 
% In our implementation, in order to 
% filter the noise introduced in binocular image acquisition and calibration, we 
For the purpose of being aware of distinct camera inputs, 
% we model the projection relationship between 2D and 3D representations explicitly, 
we also feed the camera parameters $P_{i}$ (concatenated intrinsic and extrinsic) to a \textbf{Parameter Encoder} as shown in Figure~\ref{figstereo}.
% and fuse them with the unary features obtained from 2D unet extraction. 
% The \textbf{Parameter Encoder} (as shown in Figure~\ref{figstereo}) is formulated as follows:
Formally, 
% \begin{equation} \label{eq1}
% f_{c}=f_{i} \otimes \left(  \sigma \left (\mathbb{C}  (\Re (FC(Para))) \right ) \right )
% \end{equation}
\begin{equation} \label{eq1}
P_{e}= \sigma \left ( Conv  (Reshape (FC(P_{i}))) \right )   
\end{equation}

 where $Conv$ and $FC$ are convolutions and fully-connected layers, whereas $\sigma$ and $Reshape$ represent sigmoid function and reshape operation, respectively. 
The encoded camera parameters $P_{e}$ are multiplied with the extracted features to generate the camera-aware features.



\begin{table}[!tH]\small
\begin{center}
\begin{tabular}{c|c|c|c}
\hline
Name          & Layer Properties & Dim & Input              \\ \hline
Conv1   & $3\times3\times3$, 2, 1  & $32/64$  & Input Volume    \\ \hline
Conv2   & $3\times3\times3$, 1, 1  & $64/64$  & Conv1        \\ \hline
Conv3   & $3\times3\times3$, 2, 1  & $64/128$  & Conv2      \\ \hline
Conv4   & $3\times3\times3$, 1, 1  & $128/128$  & Conv3        \\ \hline
Deconv5   & $3\times3\times3$, 2, 1  & $128/64$  & Conv4       \\ \hline
Conv6   & $1\times1\times1$, 1, 0  & $64/64$  & Conv2       \\ \hline
Shortcut  & Addition \& ReLU  & $64/64$  &\begin{tabular}[c]{@{}c@{}}Deconv5, \\ Conv6 \end{tabular}   \\ \hline

Deconv7   & $3\times3\times3$, 2, 1  & $64/32$  & Shortcut       \\ \hline
Conv8  & $1\times1\times1$, 1, 0  & $32/32$  & Input Volume       \\ \hline
Output Volume  & Addition \& ReLU  & $32/32$  & \begin{tabular}[c]{@{}c@{}}Deconv7,\\ Conv8 \end{tabular}  \\ \hline
\end{tabular}
\caption{Structure details of the 3D hourglass. The $"$Layer Properties$"$ indicates kernel size, stride and padding, respectively.}
\label{tab:1}
\end{center}
\end{table}



The left and right camera-aware unary features are correlated to form the group-wise disparity volume, which is then converted to depth volume. 
To regularize the depth volume, we adopt 3D CNNs with hourglass (encoder-decoder) structures to process it~\cite{guo2019group, cfnet}.
As shown in Figure~\ref{figstereo}, we first feed the depth volume  to four 3$\times$3$\times$3 convolutions with stride 1, the last two of which are implemented with residual connection. 

With the depth volume, we employ three stacked 3D hourglasses to further aggregate the volumetric information. The structure details of the 3D hourglass are shown in Table~\ref{tab:1}, where two residual connections are adopted to aggregate contexts from different semantic levels. In each 3D hourglass, the input volume is first downsampled and then upsampled with deconvolution, thus aggregating information along the spatial and depth dimensions. Finally, two 3$\times$3$\times$3 convolutions are used to generate the regularized depth volume with dimension of $ \mathbb{R}^{1 \times D\times H \times W}$.



\begin{figure}[!ht]
\vspace{0pt}
\begin{center}		\includegraphics[width=8.5cm]{bev_constructor.png}  
 	\end{center}
	\caption{ Implementation details of the BEV representation construction. We employ two branches in parallel to generate the BEV context features and BEV latent volume.} 
	\label{figbev}
 \vspace{0pt}
\end{figure} 




\noindent \textbf{More Details on BEV Representation Construction.}
As mentioned in Sec. 3.2 of the main paper, we construct BEV context features and BEV latent volume as BEV representations~\cite{philion2020lift}. In our implementation, we adopt left image features and camera parameters as inputs. 



As shown in Figure~\ref{figbev}, two branches are employed in parallel, where the upper branch is dedicated to generating contextual features while the lower branch is dedicated to generating latent volume. For both branches, we use camera parameter encoder similar to Equation~\ref{eq1} for camera-awareness. In the upper branch, a 3$\times$3 convolution is adopted to generate the context features. 
In the lower branch, we adopt two stacked residual blocks and an ASPP~\cite{chen2017rethinking, chen2018encoder} module to refine the depth distribution of the latent volume. The ASPP module is used to expand the receptive field in depth perception process, which contains four atrous convolutions with dilation rates of 1, 6, 12, 18, respectively.






%-----------------------------------------------
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}