\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{float}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{indentfirst}
% \usepackage{parskip}
\usepackage{caption} 



% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
 % \usepackage[sorting=none]{biblatex}
\usepackage{xcolor}
\usepackage{url}
\usepackage{balance}
\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}

\newcommand{\jx}{\textcolor{red}}

\usepackage{booktabs, multirow} % for borders and merged ranges
\definecolor{roadcolor}{RGB}{234,51,246}
\definecolor{sidewalkcolor}{RGB}{68,8,72}
\definecolor{parkingcolor}{RGB}{241,156,249}
\definecolor{othergroundcolor}{RGB}{160,32,76}
\definecolor{buildingcolor}{RGB}{246,202,69}
\definecolor{carcolor}{RGB}{111,149,238}
\definecolor{truckcolor}{RGB}{74,32,172}
\definecolor{bicyclecolor}{RGB}{136,227,242}
\definecolor{motorcyclecolor}{RGB}{37,59,146}
\definecolor{othervehiclecolor}{RGB}{96,81,242}
\definecolor{vegetationcolor}{RGB}{79, 173, 50}
\definecolor{trunkcolor}{RGB}{126, 65, 22}
\definecolor{terraincolor}{RGB}{171, 238, 105}
\definecolor{personcolor}{RGB}{234, 60, 49}
\definecolor{bicyclistcolor}{RGB}{234, 66, 195}
\definecolor{motorcyclistcolor}{RGB}{138, 42, 90}
\definecolor{fencecolor}{RGB}{238, 128, 69}
\definecolor{polecolor}{RGB}{252, 241, 161}
\definecolor{trafficsigncolor}{RGB}{233, 51, 35}
\definecolor{color1}{RGB}{176, 36, 24}
% \definecolor{color2}{RGB}{0, 176, 80}
\definecolor{color2}{RGB}{119,185,0}
\definecolor{color3}{RGB}{0, 0, 200}
\definecolor{colorofteaser}{RGB}{176, 36, 24}


\newcommand{\tbr}[1]{\textbf{\textcolor{color1}{#1}}}
\newcommand{\tbg}[1]{\textbf{\textcolor{color2}{#1}}}
\newcommand{\tbb}[1]{\textbf{\textcolor{color3}{#1}}}
\newcommand{\teaser}[1]{\textbf{\textcolor{colorofteaser}{#1}}}

\newcommand{\chaowei}[1]{\textbf{\textcolor{color3}{Chaowei: #1}}}


%%--------------mono
\definecolor{LightGrey}{rgb}{.9,.9,.9}
\definecolor{White}{rgb}{1.,0.,1.}
\definecolor{first}{rgb}{.8,.0,.0}
\definecolor{second}{rgb}{.0,.6,.0}
\definecolor{third}{rgb}{.0,.0,.8}
% \newcolumntype{g}{>{\columncolor{White}}c}


\definecolor{ceiling}{RGB}{214,  38, 40}   %
\definecolor{floor}{RGB}{43, 160, 4}     %
\definecolor{wall}{RGB}{158, 216, 229}  %
\definecolor{window}{RGB}{114, 158, 206}  %
\definecolor{chair}{RGB}{204, 204, 91}   %
\definecolor{bed}{RGB}{255, 186, 119}  %
\definecolor{sofa}{RGB}{147, 102, 188}  %
\definecolor{table}{RGB}{30, 119, 181}   %
\definecolor{tvs}{RGB}{160, 188, 33}   %
\definecolor{furniture}{RGB}{255, 127, 12}  %
\definecolor{objects}{RGB}{196, 175, 214} %

\definecolor{car}{rgb}{0.39215686, 0.58823529, 0.96078431}
\definecolor{bicycle}{rgb}{0.39215686, 0.90196078, 0.96078431}
\definecolor{motorcycle}{rgb}{0.11764706, 0.23529412, 0.58823529}
\definecolor{truck}{rgb}{0.31372549, 0.11764706, 0.70588235}
\definecolor{other-vehicle}{rgb}{0.39215686, 0.31372549, 0.98039216}
\definecolor{person}{rgb}{1.        , 0.11764706, 0.11764706}
\definecolor{bicyclist}{rgb}{1.        , 0.15686275, 0.78431373}
\definecolor{motorcyclist}{rgb}{0.58823529, 0.11764706, 0.35294118}
\definecolor{road}{rgb}{1.        , 0.        , 1.        }
\definecolor{parking}{rgb}{1.        , 0.58823529, 1.        }
\definecolor{sidewalk}{rgb}{0.29411765, 0.        , 0.29411765}
\definecolor{other-ground}{rgb}{0.68627451, 0.        , 0.29411765}
\definecolor{building}{rgb}{1.        , 0.78431373, 0.        }
\definecolor{fence}{rgb}{1.        , 0.47058824, 0.19607843}
\definecolor{vegetation}{rgb}{0.        , 0.68627451, 0.        }
\definecolor{trunk}{rgb}{0.52941176, 0.23529412, 0.        }
\definecolor{terrain}{rgb}{0.58823529, 0.94117647, 0.31372549}
\definecolor{pole}{rgb}{1.        , 0.94117647, 0.58823529}
\definecolor{traffic-sign}{rgb}{1.        , 0.        , 0.    }   

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

 % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi



\begin{document}

%%%%%%%%% TITLE
% \title{StereoScene: Explicit Stereo Matching with Implicit BEV Interaction for 3D Semantic Scene Completion }
\title{StereoScene: BEV-Assisted Stereo Matching Empowers 3D Semantic Scene Completion }



\author{Bohan Li$^{1,3}$ \quad Yasheng Sun$^{2}$ \quad  Xin Jin$^{3}$\footnotemark[2] \quad Wenjun Zeng$^{3}$ \\ Zheng Zhu$^4$   \quad Xiaoefeng Wang$^{4}$ \quad Yunpeng Zhang$^{4}$ \quad James Okae$^{5}$ \quad Hang Xiao$^{4}$ \quad Dalong Du$^{4}$\\
$^{1}$Shanghai Jiao Tong University \quad  $^{2}$ Tokyo Institute of Technology  \quad \\ $^{3}$Eastern Institute for Advanced Study \quad   $^{4}$PhiGent Robotics \quad $^{5}$South China University of Technology 
\\
% {\tt\small yimingli@nyu.edu, cfeng@nyu.edu}
% {\tt\small \url{github}}
% {\tt\small yimingli@nyu.edu, , cfeng@nyu.edu}
}

 

% \maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


\twocolumn[{
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\vspace{-30pt}
\begin{center}
 % \centering
 % \includegraphics[width=0.95\textwidth]{figs/teaser.pdf}
% \vspace{-10pt}


% \begin{figure*}[!h]
	
	% \begin{subfigure}{\linewidth}	
	% 	\centering

	\begin{center}
		\includegraphics[width=17cm]{teaser.png}   
  
		\begin{tabular}{cccccc}	
			\multicolumn{6}{c}{
				\scriptsize
				\textcolor{bicycle}{$\blacksquare$}bicycle~
				\textcolor{car}{$\blacksquare$}car~
				\textcolor{motorcycle}{$\blacksquare$}motorcycle~
				\textcolor{truck}{$\blacksquare$}truck~
				\textcolor{other-vehicle}{$\blacksquare$}other vehicle~
				\textcolor{person}{$\blacksquare$}person~
				\textcolor{bicyclist}{$\blacksquare$}bicyclist~
				\textcolor{motorcyclist}{$\blacksquare$}motorcyclist~
				\textcolor{road}{$\blacksquare$}road~
				}
    \\
    
			\multicolumn{6}{c}{
				\scriptsize
                    \textcolor{parking}{$\blacksquare$}parking~
				\textcolor{sidewalk}{$\blacksquare$}sidewalk~
				\textcolor{other-ground}{$\blacksquare$}other ground~
				\textcolor{building}{$\blacksquare$}building~
				\textcolor{fence}{$\blacksquare$}fence~
				\textcolor{vegetation}{$\blacksquare$}vegetation~
				\textcolor{trunk}{$\blacksquare$}trunk~
				\textcolor{terrain}{$\blacksquare$}terrain~
				\textcolor{pole}{$\blacksquare$}pole~
				\textcolor{traffic-sign}{$\blacksquare$}traffic sign			
			}
		\end{tabular}	
 	\end{center}
        \vspace{1mm}
	\captionof{figure}{ \textbf{Overview of StereoScene Framework}. From left to right list stereo input, SSC prediction and ground truth. 
    Our method is able to precisely produce semantic scenery and capture small distant objects (see the car marked by red box).} 
	\label{fig_q}
% \end{figure*} 

\label{fig:teaser}
\end{center}
}]

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[2]{Corresponding author, \url{jinxin@eias.ac.cn}}
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
\vspace{-10pt}
3D semantic scene completion (SSC) is an ill-posed task that requires inferring a dense 3D scene from incomplete observations.
Previous methods either explicitly incorporate 3D geometric input or rely on learnt 3D prior behind monocular RGB images.
However, 3D sensors such as LiDAR are expensive and intrusive while monocular cameras face challenges in modeling precise geometry due to the inherent ambiguity. 
In this work, we propose \textbf{StereoScene} for 3D Semantic Scene Completion (SSC), which explores taking full advantage of light-weight camera inputs without resorting to any external 3D sensors.
Our key insight is to leverage stereo matching to resolve geometric ambiguity. 
To improve its robustness in unmatched areas, we introduce bird's-eye-view (BEV) representation to inspire hallucination ability with rich context information. 
On top of the stereo and BEV representations, a mutual interactive aggregation (MIA) module is carefully devised to fully unleash their power. 
Specifically, a Bi-directional Interaction Transformer (BIT) augmented with confidence re-weighting is used to encourage reliable prediction through mutual guidance while a Dual Volume Aggregation (DVA) module is designed to facilitate complementary aggregation.
Experimental results on SemanticKITTI  demonstrate that the proposed StereoScene outperforms the state-of-the-art camera-based methods by a large margin with a relative improvement of \textbf{$26.9\%$} in \textbf{geometry} and \textbf{$38.6\%$} in \textbf{semantic}. Our code is available on \url{https://github.com/Arlo0o/StereoScene}.
\vspace{-10pt}
\end{abstract}


 
\section{Introduction}
3D scene understanding is a fundamental task in computer vision~\cite{roberts1963machine}, facilitating a variety of applications such as autonomous driving, robotic navigation and augmented reality. Due to the limitations of real-world sensors such as restricted field of view, measurement noise, or sparse results, this task remains a challenging problem. To address this problem, 3D Semantic Scene Completion (SSC)~\cite{roldao20223d} is introduced to jointly predict the geometry and semantic segmentation of a scene. Given its inherent 3D nature, most existing SSC solutions~\cite{garbade2019two,roldao2020lmscnet,wu2020scfusion} employ 3D geometric signals, in the form of occupancy grids, point clouds, or distance fields, as their model inputs. Although they provide insightful geometric cues, it requires costly sensors (e.g. LiDAR) alongside considerable manual labor entailed in their deployment. Hence, it is worth exploring an efficient and effective approach for high-fidelity SSC solely with portable cameras.



However, lifting 2D image features to 3D is a notoriously ill-posed task due to its inherent uncertainty of incomplete observation and scale ambiguity at single viewpoint~\cite{fahim2021single}. Previous camera-based SSC solutions~\cite{cao2022monoscene} attempt to tackle this challenge by converting 2D image features to 3D dense space with a 2D-3D projection UNet.
Solely relying on 3D prior without geometric constraint, the predicted voxel occupancy inevitably falls short of capturing accurate geometric features. 


In this paper, we propose \textbf{StereoScene}, a framework that targets to fully exploit the potential of vision inputs with stereo matching as Figure~\ref{fig:teaser}. With explicit geometric constraint, stereo matching exhibits exceptional proficiency in producing precise predictions on matched positions. 
However, stereo matching is sensitive to calibration settings and susceptible to failure in challenging circumstances such as high-reflectance illumination or occluded regions, since it heavily relies on feature correspondence between camera rigs.
By contrast, BEV based approaches implicitly convert image features to a canonical space independent of camera setups. Given that this latent representation is directly induced from image features, it encodes rich context and semantic information, demonstrating strong global robustness and hallucination capability~\cite{li2022bevformer}. Based on the above observation, our primary objective is to \emph{devise an integration mechanism to dynamically incorporate dependable BEV predictions without compromising the stereo construction results}.



To this end, we thus propose a BEV-assisted stereo matching SSC framework to make the best of both representations: \textbf{1)} After carefully examining the BEV and stereo construction procedure, we identify that stereo geometric volume and BEV latent volume are suitable for effective mutual reinforcement. \textbf{2)} Given these two volumes withholding distinct natures, we devise a \emph{Mutual Interactive Aggregation Module} that follows bootstrap-before-fuse paradigm. Particularly, \emph{A Bi-directional Interaction Transformer (BIT)} is designed to guide each other to screen reliable information, on top of which a confidence re-weighting strategy inspired by MVS~\cite{chen2020mvsnet++} is introduced to further enhance the boost performance. Besides, a \emph{Dual Volume Aggregation Module} is 
delicately designed to facilitate complementary aggregation.

Our contributions are summarized as follows: \textbf{1)} We propose a BEV-assisted stereo matching framework for 3D semantic scene completion. \textbf{2)} To bridge the representation gap, a Transformer-based \emph{Mutual Interactive Aggregation Module} is designed for combining their complementary merits. \textbf{3)} Extensive experiments demonstrate that our StereoScene
approach outperforms all the camera-based state-of-the-arts by a large margin, with a relative improvement of 26.9$\%$ in \textbf{geometry} and 38.6$\%$ in \textbf{semantic} on SemanticKITT.



\section{Related Works}

\subsection{Semantic Scene Completion}

Earlier works on scene completion (SC) used sophisticated interpolation~\cite{davis2002filling} or energy minimization~\cite{kazhdan2006poisson,newcombe2011kinectfusion,sorkine2004least} techniques to estimate 3D dense geometry given one or more 2D/3D observations. With the large-scale datasets~\cite{behley2019semantickitti,silberman2012indoor,song2017semantic,straub2019replica} released in recent years, recent works~\cite{cai2021semantic,rist2021semantic,yan2021sparse,cheng2021s3cnet,song2017semantic,wu2020scfusion,roldao2020lmscnet,li2020anisotropic} have increasingly relied on deep learning techniques to learn 3D priors and address ambiguity in the scene. Song et al.~\cite{song2017semantic} recognized the intertwined relationship between semantic segmentation and scene completion and proposed a joint estimation of semantic segmentation and scene completion. 
 

Given the 3D nature of semantic scene completion task, many studies~\cite{zhang2018efficient,rist2021semantic,garbade2019two,wu2020scfusion} directly use 3D inputs to take advantage of its accompanying geometrical insights. To provide additional texture or geometry information, a vast majority of works~\cite{cai2021semantic,li2019rgbd,dourado2021edgenet,li2020anisotropic,cherabier2018learning} exploit multi-modal inputs, such as RGB images coupled with various geometric cues. IPF-SPCNet~\cite{zhong2020semantic} performs semantic segmentation on the image inputs and uses RGB features to augment the 3D point cloud. Another slew of studies~\cite{cao2022monoscene,li2023voxformer} aim to achieve semantic scene completion solely with camera-only vision inputs. For instance, MonoScene~\cite{cao2022monoscene} lifts a monocular image using 2D-3D projections and leverages 2D and 3D UNets for semantic scene completion. VoxFormer~\cite{li2023voxformer} takes multiple temporal images as inputs and employs a transformer-based framework where a sparse set of visible and occupied voxel queries are devised for reliable scene structure reconstruction.
Another work that also attempts to exploit geometry constraint by vision inputs is OccDepth~\cite{miao2023occdepth}. However,
they rely on a pre-trained depth network for 3D knowledge distillation in the training process, causing heavy computation overhead. 




\subsection{Stereo Matching Based 3D Perception}

With the advances of deep convolutional neural networks, the quality of depth predicted from stereo images~\cite{poggi2022stereodepth} has steadily improved and has led to a remarkable improvement in downstream 3D vision applications such as object detection, surface reconstruction and semantic scene completion. 
GC-Net \cite{kendall2017end} was the first to propose 3D CNNs based stereo depth prediction framework, where potential stereo corresponding 2D features are mapped to a 3D cost volume through a concatenation operation. The promise of this method inspired GwcNet \cite{guo2019group}, which proposes group-wise correlation to improve feature similarity measurement.
Recent state-of-the-art stereo matching methods can be broadly categorized into 2D CNN based approaches \cite{Mayer2016dispcstereo,pang2017crlstereo,haofei2020aanet,tan2021hitnet} and 3D CNN based approaches \cite{kendall2017end,chang2018pyramid,guo2019group,okae2021robust}. More recent 2D CNN methods such as RAFT-Stereo \cite{Lahav2021raftstereo} and CREStereo \cite{jiankun2022crestereo} utilize correlation to produce matching cost volume which is subsequently optimized through sequential refinement modules for depth prediction. 
However, in conditions such as occlusion and large textureless regions, stereo depth prediction performance drops significantly. 


\subsection{Bird's-Eye-View Representation }
The bird's-eye-view is a widely used representation of a surrounding scene since it provides a clear depiction of the layout and shape of objects from a top-down perspective. Due to its effectiveness on dense representations, BEV has been broadly employed to map segmentation approaches~\cite{philion2020lift,hu2021fiery,ng2020bev,pan2020cross,mani2020monolayout,roddick2020predicting}, benefiting perception and motion planning in autonomous driving scenarios. In camera-based scene understanding frameworks, researchers lift 2D image features to 3D BEV representations coupled with estimated depth distribution~\cite{reading2021categorical,philion2020lift,li2022bevformer}. Mani et.al.~\cite{mani2020monolayout} utilize a ResNet to reason about bird's-eye-view layout, whereas Roddick et.al.~\cite{roddick2020predicting} devise a transformer architecture to predict map semantic segmentation with converted BEV representations. Lift-Splat~\cite{philion2020lift} extracts BEV representations from an arbitrary number of cameras by implicitly unprojecting 2D visual inputs based on estimated depth distribution. To circumvent compounding errors~\cite{wang2022detr3d}, later approaches attempt to dynamically learn reliable geometric insights rather than relying on structural 3D prior. BEVFormer~\cite{li2022bevformer} design a spatial-temporal transformer to adaptively extract and aggregate BEV features from images at different timestamps. However, current BEV approaches still struggle to capture accurate geometric information in complicated scenarios due to the absence of explicit geometric constraints. In this work, we seek to explore more reliable geometric cues by exploiting stereo geometric cues and BEV semantic representations. We demonstrate that this idea provides more reliable geometric cues for improved semantic scene completion performance.


\section{Methodology}
We present our \textbf{StereoScene} framework that aims to jointly infer dense 3D geometry and semantics from solely camera-based stereo RGB images. The whole pipeline is depicted in Figure~\ref{figoverall}. In this section, we first explore a hybrid occupancy based SSC learning formulation (Sec.~\ref{sec:preliminary}), then we provide detailed construction of \emph{dual volume} representation (Sec.~\ref{sec32}). To bridge their representation gap, we depict our devised aggregation module (Sec.~\ref{sec:integration}). Finally, we introduce our SSC generator and training paradigm (Sec.~\ref{sec:ssc}). 

\subsection{Preliminary}
\label{sec:preliminary}
\noindent \textbf{Problem Formulation.} Given a set of stereo RGB images $I^{rgb}_{Stereo}=\{I^{rgb}_l, I^{rgb}_r\}$, our goal is to jointly infer geometry and semantics of a 3D scene. The scene is represented as a voxel grid $\textbf{Y} \in \mathbb{R}^{H\times W \times Z}$, where $H,W,Z$ denote the height, width and depth in 3D space. Regarding each voxel, it will be assigned to an unique semantic label belonging to $C \in \{ c_0, c_1, \cdots, c_M \}$, which either occupies empty space $c_0$ or falls on a specific semantic class $\{c_1, c_2, \cdots, c_M \}$. Here $M$ denotes the total number of semantic classes. We would like to learn a transformation $\hat{\textbf{Y}}=\Theta(I^{rgb}_{Stereo})$ to approach ground truth 3D semantics $\textbf{Y}$.


\noindent \textbf{Architecture Overview}. The overall architecture of our StereoScene framework is illustrated in Figure~\ref{figoverall}. We follow common paradigm~\cite{cao2022monoscene} that employs successive 2D and 3D UNets as backbones. The input stereo images $I^{rgb}_{Stereo}$ are separately encoded by a 2D UNet into paired context-aware features ${\textbf{F}_l}$ and ${\textbf{F}_r} \in \mathbb{R}^{C \times H\times W }$. Then we leverage a \emph{Stereo Constructor} to convert these features into a dense 3D volume $\textbf{V}_{Stereo} \in \mathbb{R}^{D_{s} \times H \times W}$. In parallel, a \emph{BEV Constructor} lift 2D features ${\textbf{F}_l}$ of left image to a latent BEV volume $\textbf{V}_{BEV} \in \mathbb{R}^{D_{b} \times H \times W}$ alongside its context feature $\textbf{C}_{BEV} \in \mathbb{R}^{C_b \times H \times W}$ following standard protocol of ~\cite{philion2020lift}. Sequentially, the two-stream built volumes are bridged and aggregated to a new volume $\textbf{V}_{agg}$ by a \emph{Mutual Interactive Aggregation Module}. Finally, the context feature $\textbf{C}_{BEV}$ splat along volume $\textbf{V}_{agg}$ by outer-product, which will be fed to a 3D UNet for semantic segmentation and completion. \looseness=-1



\subsection{Dual Volume Construction} 
\label{sec32}



\begin{figure*}
\vspace{-10pt}
\hsize=\textwidth %
\centering
\includegraphics[width=1\textwidth]{overall.png}
\caption{Overall framework of our proposed \textbf{StereoScene}. Given input stereo images, we employ 2D UNet to extract multi-scale features. The BEV latent volume and stereo geometric volume are constructed by a \emph{BEV Constructor} and a \emph{Stereo Constructor}, respectively. To fully exploit the complementary potential of the two volumes, a \emph{Mutual Interactive Aggregation Module} is proposed to mutually bootstrap and aggregate them. 
}
\label{figoverall}
 \vspace{-10pt}
\end{figure*}


\begin{figure}
\vspace{-5pt}
	\begin{center}
		\includegraphics[width=8.5cm]{block.png}  
 	\end{center}
	\caption{ The structure of proposed \emph{Mutual Interactive Aggregation} module. We employ \emph{Bi-directional Interactive Transformer} for reliable geometry information interaction. Afterwards, the interacted volumes are concatenated together and fed into \emph{Dual Volume Aggregation} module to generate the aggregated volume. } 
	\label{figblock}
 \vspace{-10pt}
\end{figure} 




Unlike previous studies~\cite{garbade2019two,roldao2020lmscnet,wu2020scfusion}, we aim to resolve holistic 3D scene understanding without resorting to any 3D geometric inputs. To encode 2D image features to 3D space, we introduce a hybrid volume representation, stereo volume and BEV volume, withholding two distinct natures to take full advantage of camera inputs.

\noindent \textbf{Shared 2D Feature Extraction Backbone}. For the purpose of image feature extraction, 2D UNet with pre-trained EfficientNetB7~\cite{tan2019efficientnet} is leveraged to separately process left and right input images. Note that we utilize shared weights to encourage efficient context learning in canonical space.


\noindent \textbf{Stereo Geometric Volume Constructor}. With the obtained unary features ${\textbf{F}_l}$ and ${\textbf{F}_r}$ from left and right images, \emph{Stereo Constructor} targets to build a voxel depth volume $\textbf{V}_{Stereo}$ by matching them with epipolar constraint. Specially, group-wise correlation~\cite{guo2019group} is adopted to generate disparity cost volume. Formally, 
\begin{equation}
D_{gwc}(d,x,y,g)=\frac{1}{N_{c}/N_{g}} \left< f_{g}^{l}(x,y), f_{g}^{r}(x-d,y)\right>,
\end{equation}
where $ \left< \cdot,\cdot\right>$represents the inner product, $N_{c}$ is the channels of input features, $N_{g}$ is the number of groups, $f_{g}^{l}$ and $f_{g}^{r}$ represent $g^{th}$ left and right feature group, respectively. 
Afterwards, the disparity volume is converted into a depth volume following~\cite{you2019pseudo}, which is formulated as:
\begin{equation} \label{ }
z_{(u,v)}=\frac{f_{u}\times b}{D_{(u,v)}}, x=\frac{(u-c_{u})\times z}{f_{u}}, y=\frac{(v-c_{v})\times z}{f_{v}},
\end{equation}
where $f_{u}$ and $f_{v}$ represent the horizontal and vertical focal length, ($c_{u}, c_{v}$) is the camera center. So far, the dense stereo volume $\textbf{V}_{Stereo} $ involving spatial depth is constructed.



\noindent \textbf{BEV Latent Volume Constructor}. Although stereo constructor provides accurate estimation in aligned locations, it struggles in extreme conditions where severe occlusion or high reflection happens. Unlike stereo-based approach relying on strict geometric matching, BEV representations
%features 
are obtained by lifting an image $I^{rgb}$ to a shared bird's eye space through 3D prior. Following \cite{philion2020lift}, we feed visual features $\textbf{F}_l$ to a convolution neural network and obtain a latent depth distribution $\textbf{V}_{BEV} \in \mathbb{R}^{D_b \times H \times W}$ with its associated context features $\textbf{C}_{BEV} \in \mathbb{R}^{C_b \times H \times W}$. Since this distribution is essentially a voxel grid that stores the probability of all possible depths, we denote it as BEV latent volume for the sake of clarity. 




\subsection{Mutual Interactive Aggregation Module}\label{sec34}
\label{sec:integration}
With the obtained stereo volume $\textbf{V}_{Stereo}$ and BEV volume $\textbf{V}_{BEV}$, this module aims to mutually reinforce each other and integrate their respective potentials to produce a new volume $\textbf{V}_{agg}$.




\noindent \textbf{Bi-directional Interactive Transformer}. For superior aggregation, we propose an initial bootstrapping stage that selectively screens dependable information alongside its counterpart volume. Concretely, a Bi-directional Interactive Transformer (BIT) as Figure~\ref{figblock} is devised to interactively guide reliable predictions of its contrary side through cross-attention mechanism. For stereo volume $\textbf{V}_{Stereo}$, we first obtain its query $Q_S$, key $K_S$ and value $V_S$ following standard protocol~\cite{wang2018non, wang2022mvster}.
%, where the $Q_s \in \mathbb{R}^{HW \times CD }, K_s \in \mathbb{R}^{CD \times HW}, V_s \in \mathbb{R}^{HW \times CD}$. 
Similarly, the BEV volume $\textbf{V}_{BEV}$ is forwarded and its query, key and value are denoted as $Q_B, K_B, V_B$, respectively. Then we construct a guidance attention matrix $\textbf{M}_S \in \mathbb{R}^{HW \times HW}$ indicating the interactive guidance, which is implemented by a cross-attention operation as: 
\begin{equation}
\textbf{M}_S = {CrossAtt(\textbf{V}_{Stereo}, \textbf{V}_{BEV}) = softmax ({K_{B}}^{T} Q_{S}),}
\end{equation}
In this way,
% $\textbf{M}_S$ provides an alternative perspective on spatial importance of the stereo side, which accounts for the relevant aspects of the BEV sides. 
$\textbf{M}_S$ accounts for the relevant aspects of the stereo sides, thereby providing an alternative perspective on spatial importance of the BEV side. 
Likewise, $\textbf{M}_B$ is computed by $softmax({K_{S}}^{T} Q_{B})$ to encourage reliable geometry information transmission.




\noindent \textbf{Depth Confidence Filtering}. In order to further enhance the guidance matrix described above, we develop a depth confidence filtering strategy, which explicitly takes advantage of the involved geometry information behind the volume. We aim to utilize its depth distribution information to enforce the guidance matrix similar to \cite{chen2020mvsnet++}. Particularly,  
to project the volume to a confidence map $\textbf{C}_S \in \mathbb{R}^{H \times W}$, we first adopt $softmax$ to convert depth cost value $d_i$ into probability form, and then take out the highest probability value among all depth hypothesis planes along the depth dimension as the prediction confidence. The process is formally written as:
\begin{equation}
{\textbf{C}_S= WTA \left\{   \frac{\exp(d_i)} 
 {\sum_{j=1}^{D_{max}}\exp(d_j)}  \right\},}
\end{equation}
where the $softmax$ is applied across the depth dimension and $WTA$ represents winner-takes-all operation. $D_{max}$ denotes the length of depth dimension. To this end, we update the previous attention matrix with reshaped confidence map $\textbf{C}_S \in \mathbb{R}^{1 \times HW}$ as follows:
\begin{equation}
{\textbf{M}_S'= \textbf{M}_S \odot \textbf{C}_S, }
\end{equation}
where $\odot$ represents element-wise multiplication, through which the geometry information of reliable areas is preserved while low-confidence areas are suppressed.

Finally, the filtered inter-attention matrix $\textbf{M}_S'$ is leveraged to multiply the BEV value vector $V_{B}$ to obtain the interacted BEV volume $\textbf{V}_{BEV}'$.
\begin{equation}
{
\textbf{V}_{BEV}' = \textbf{M}_S' \odot V_{B},}
\end{equation}

Note that its opposite interacted volume $\textbf{V}_{Stereo}'$ is calculated in a symmetric manner.

\noindent \textbf{Dual Volume Aggregation}. With the bootstrapped volume representations $\textbf{V}_{Stereo}'$ and $\textbf{V}_{BEV}'$, the primary objective of this module is to leverage their strengths and facilitate a mutually beneficial complementation and aggregation. Specially, the volume aggregation module takes as input concatenated features $\textbf{V}'_{cat}=[\textbf{V}_{Stereo}', \textbf{V}_{BEV}']\in \mathbb{R}^{2 \times D \times H \times W}$ and outputs aggregated volume $\textbf{V}_{agg}$. As illustrated in  Figure~\ref{figblock}, the input is first fed into residual 3D CNNs for regularization and channel adjustment, which generates transformed representation $\textbf{V}_f \in \mathbb{R}^{C \times D \times H \times W}$. To fully exploit its contextual information~\cite{hu2018squeeze}, we utilize average pooling to squeeze the information into a channel descriptor $\textbf{z}_c \in \mathbb{R}^C$. We shrink $\textbf{V}_f$ along both the depth dimension $D$ and spatial dimension $ H \times W$, 
\begin{equation}
    \textbf{z}_c = \frac{1}{D\times H\times W} \sum_{d=1}^D \sum_{i=1,j=1}^{H,W} \textbf{V}_f(d,i,j),
\end{equation}



Subsequently, an excitation block~\cite{hu2018squeeze} is leveraged to capture its channel-wise dependencies. Formally, the channel descriptor is updated to $\textbf{z}_c'$ by two stacked bottleneck-shape convolutions with non-linear activation as: 
\begin{equation}
    \textbf{z}_c' = \sigma(\textbf{W}_2\delta(\textbf{W}_1 \textbf{z}_c)),
\end{equation}
where the ${W}_1$ and ${W}_2$ represent $ 1\times1\times1 $ convolutions with dimensionality-reduction. The $\delta$ denotes standard GELU and the $\sigma$ indicates sigmoid gate. Then $\textbf{z}_c'$ is employed to re-weight the previous transformed feature $\textbf{V}_f$ along the channel dimension. 
After re-weighting, we aggregate information between different channels. Formally,


\begin{equation}
\textbf{V}_{agg} = \mathbb{P} (\textbf{z}_c' \odot \textbf{V}_f),
\end{equation}
where $\mathbb{P}$ is composed of point-wise convolution with GELU activation and group normalization. The point-wise convolution encourages channel-aware mixing by $1\times1\times1$ kernel size, through which the aggregated volume $\textbf{V}_{agg}$ takes into consideration features of different aspects.




\subsection{Semantic Scene Completion}
\label{sec:ssc}
To make use of this high-quality occupancy grid $\textbf{V}_{agg}$ for semantic scene completion, we augment it with its associated context information $\textbf{C}_{BEV}$. The extracted context information from input images is placed to a specific location of the bird-eye's representation by an outer product operation similar to~\cite{philion2020lift}. Formally, the aggregated 3D feature $\textbf{F}_{agg} \in \mathbb{R}^{C_b \times D_b \times H \times W}$ is computed by:
\begin{equation}
\textbf{F}_{agg} = \textbf{C}_{BEV} \otimes \textbf{V}_{agg},
\end{equation}
where the $\textbf{V}_{agg}$ and $\textbf{C}_{BEV}$ is firstly unsqueezed on channel dimension and depth dimension, respectively.
In this way, we are able to seamlessly blend the complementary benefits of stereo representation for precise geometry and BEV features for rich semantic context.

\noindent \textbf{Semantic Segmentation Learning.} Following~\cite{cao2022monoscene}, we leverage an encoder-decoder 3D UNet to process the aggregated 3D voxel features. Its output features are fed to a SSC head for semantic occupancy prediction $\hat{\textbf{Y}}$.
The SSC head is composed of a 3D ASPP~\cite{chen2017deeplab} block and a softmax layer.

\noindent \textbf{Network Training.} 
We follow the basic learning objective of MonoScene~\cite{cao2022monoscene} for semantic scene completion. 
Standard semantic loss $\mathcal{L}_{\text{sem}}$ and geometry loss $\mathcal{L}_{\text{geo}}$ are leveraged for semantic and geometry supervision, while an extra class weighting loss $\mathcal{L}_{ce}$ is also added as~\cite{cao2022monoscene}.
To further enforce the aggregated volume $\textbf{V}_{agg}$, we adopt a binary cross entropy loss $\mathcal{L}_{depth}$ to encourage the sparse depth distribution. The overall learning objective of this framework is formulated as follows:
\begin{equation}
   { \mathcal{L} = \mathcal{L}_{depth} + \mathcal{\lambda}_{ce} \mathcal{L}_{ce} + \mathcal{\lambda}_{sem} \mathcal{L}_{\text{sem}}+ \mathcal{\lambda}_{geo} \mathcal{L}_{\text{geo}} . }
\end{equation}
where $\lambda$s are balancing coefficients. 



\begin{table*}[!ht]
\begin{center}
\renewcommand\tabcolsep{3.0pt}
\begin{tabular}{l|cccccc}
\hline
\textbf{Methods}              & StereoScene (ours)    & MonoScene~\cite{cao2022monoscene} & JS3CNet~\cite{yan2021sparse} & LMSCNet~\cite{roldao2020lmscnet}  & AICNet~\cite{li2020anisotropic}  & 3DSketch~\cite{3d-sketch}  \\ \hline
\textbf{IoU (\%)}              & \textbf{43.34} &          \underline{34.16}     &    34.00       &    31.38     &    23.93    &    26.85      \\ \hline
\textbf{mIoU}                  & \textbf{15.36}      & \underline{11.08}     & 8.97      & 7.07    & 7.09   & 6.23     \\ \hline

\crule[carcolor]{0.13cm}{0.13cm} \textbf{car} (3.92\%)          & \textbf{22.8}       &  18.80 & \underline{20.10} & 14.30 & 15.30 & 17.10 \\

\crule[bicyclecolor]{0.13cm}{0.13cm} 
\textbf{bicycle} (0.03\%)      & \textbf{3.40}      &  \underline{0.50} & 0.00 & 0.00 & 0.00 & 0.00 \\

\crule[motorcyclecolor]{0.13cm}{0.13cm} \textbf{motorcycle} (0.03\%)   & \textbf{2.40}      &  \underline{0.70} & 0.00 & 0.00 & 0.00 & 0.00 \\

\crule[truckcolor]{0.13cm}{0.13cm} \textbf{truck} (0.16\%)        & \underline{2.80}         &  \textbf{3.30} & 0.80 & 0.30 & 0.70 & 0.00 \\

\crule[othervehiclecolor]{0.13cm}{0.13cm} \textbf{other-veh.} (0.20\%)   & \textbf{6.10}                 &  \underline{4.40} & 4.10 & 0.00 & 0.00 & 0.00 \\

\crule[personcolor]{0.13cm}{0.13cm} \textbf{person} (0.07\%)       & \textbf{2.90}         &  \underline{1.00} & 0.00 & 0.00 & 0.00 & 0.00 \\

\crule[bicyclistcolor]{0.13cm}{0.13cm} \textbf{bicyclist} (0.07\%)    & \textbf{2.20}             &  \underline{1.40} & 0.20 & 0.00 & 0.00 & 0.00 \\

\crule[motorcyclistcolor]{0.13cm}{0.13cm} \textbf{motorcyclist} (0.05\%) & \textbf{0.50}       &  \underline{0.40} & 0.20 & 0.00 & 0.00 & 0.00 \\

\crule[roadcolor]{0.13cm}{0.13cm} \textbf{road} (15.30\%)        & \textbf{61.90}       &  \underline{54.70} & 47.30 & 46.70 & 39.30 & 37.70 \\

\crule[parkingcolor]{0.13cm}{0.13cm} \textbf{parking} (1.12\%)      & \textbf{30.70}       &  \underline{24.80} & 19.90 & 13.50 & 19.80 & 0.00 \\

\crule[sidewalkcolor]{0.13cm}{0.13cm} \textbf{sidewalk} (11.13\%)    & \textbf{31.20}       &  \underline{27.10}  & 21.70 & 19.50 & 18.30 & 19.80 \\

\crule[othergroundcolor]{0.13cm}{0.13cm} \textbf{other-grnd} (0.56\%)    & \textbf{10.70}        &  \underline{5.70} & 2.80 & 3.10 & 1.60 & 0.00 \\

\crule[buildingcolor]{0.13cm}{0.13cm} \textbf{building} (14.10\%)    & \textbf{24.20}        &  \underline{14.40} & 12.70 & 10.30 & 9.60 & 12.10 \\

\crule[fencecolor]{0.13cm}{0.13cm} \textbf{fence} (3.90\%)        & \textbf{16.50}       &  \underline{11.10} & 8.70 & 5.40 & 5.00 & 3.40 \\

\crule[vegetationcolor]{0.13cm}{0.13cm} \textbf{vegetation} (39.3\%)   & \textbf{23.80}      &  \underline{14.90} & 14.20 & 10.80 & 9.60 & 12.10 \\

\crule[trunkcolor]{0.13cm}{0.13cm} \textbf{trunk} (0.51\%)        & \textbf{8.40}         &  2.40 & \underline{3.10} & 0.00 & 1.90 & 0.00 \\

\crule[terraincolor]{0.13cm}{0.13cm} \textbf{terrain} (9.17\%)      & \textbf{27.00}      &  \underline{19.50} & 12.40 & 10.40 & 13.50 & 16.10 \\

\crule[polecolor]{0.13cm}{0.13cm} \textbf{pole} (0.29\%)         & \textbf{7.00}         &  \underline{3.30} & 1.90 & 0.00 & 0.10 & 0.00 \\

\crule[trafficsigncolor]{0.13cm}{0.13cm} \textbf{traf.-sign} (0.08\%)   & \textbf{7.20}      &   \underline{2.10}  & 0.30 & 0.00 & 0.00 & 0.00 \\  \hline

\end{tabular}
\vspace{1mm}
\caption{\textbf{Quantitative results} on the SemanticKITTI hidden test set with the state-of-the-art \textbf{camera-based} SSC methods. The top two performers are marked \textbf{bold} and \underline{underlined}, respectively.
}
\label{tabq1}        
\vspace{-10pt}
\end{center}
\end{table*}




\begin{table}[!ht]
\begin{center}
\begin{tabular}{cccc}
\hline
 \textbf{Methods}& \textbf{Input}& \textbf{IoU(\%)}$\uparrow$      & \textbf{mIoU($\%$)}$\uparrow$      \\  \hline
StereoScene (ours) & Stereo & \underline{43.85} & \textbf{15.43}  \\  
MonoScene~\cite{cao2022monoscene}   &  Mono   & 36.80            &    11.30             \\
Voxformer-T~\cite{li2023voxformer} &  Stereo &\textbf{44.15 }           &   \underline{ 13.35 }             \\
LMSCNet~\cite{roldao2020lmscnet}  &  Mono  & 38.36         &    9.94            \\
SSCNet~\cite{song2017semantic}   &  Mono  & 40.93         &  10.27         \\  \hline
\end{tabular}
% \vspace{1mm}

\caption{ \textbf{Quantitative results} on the SemanticKITTI validation set against the state-of-the-art camera-based SSC methods. Voxformer-T adopts multiple temporal stereo images as inputs. }
\vspace{-15pt}

\label{tabq2}
\end{center}
\end{table}


\section{Experiments}
\subsection{Experimental Settings}
\noindent \textbf{Datasets.}
We evaluate the proposed \textbf{StereoScene} on SemanticKITTI~\cite{behley2019semantickitti} that is popularly used in a great number of previous studies. There are 22 driving outdoor scenes from the KITTI Odometry Benchmark~\cite{geiger2012we}, covering diverse and challenging autonomous driving situations. SemanticKITTI holds semantic annotations of LiDAR sweeps that are registered, aggregated and voxelized as 256$\times$256$\times$32 grid of 0.2m voxels. The target ground truth of each voxel grid is annotated as one of 21 classes (1 unknown, 1 free and 19 semantics). The SemanticKITTI benchmark provides both voxelized LiDAR scans and RGB images as model input options. We solely utilize RGB images since our main focus is to explore the portable camera-only signals as MonoScene~\cite{cao2022monoscene}. 

\noindent \textbf{Implementation Details.} We set the 3D UNet input to 128$\times$128$\times$16 (1:2) for efficient memory usage, whose feature will be upscaled to 1:1 at completion head by deconvolution operation. The $\lambda$s are empirically set to 1. Our models are implemented on PyTorch~\cite{paszke2019pytorch} with two Tesla A100 GPUs. We train 30 epochs employing an AdamW~\cite{loshchilov2017decoupled} optimizer with a learning rate of 1e-4 and batch size set to 10.


\noindent \textbf{Comparison Methods}.
We compare our method with the best models~\cite{li2023voxformer, cao2022monoscene, yan2021sparse, roldao2020lmscnet, li2020anisotropic, 3d-sketch} currently available that support 3D semantic scene completion(SSC).
\textbf{MonoScene}~\cite{cao2022monoscene} is a very-first camera-based SSC method, relying on feature projection for 2D-to-3D scene reconstruction.
\textbf{VoxFormer-T}~\cite{li2023voxformer} is the temporary version of VoxForme-S that leverages a sequential of stereo images. 
 Other baselines such as \textbf{JS3CNet}~\cite{yan2021sparse}, \textbf{LMSCNet}~\cite{roldao2020lmscnet}, \textbf{SSCNet}~\cite{song2017semantic}, \textbf{AICNet}~\cite{li2020anisotropic} and \textbf{3DSketch}~\cite{3d-sketch} require 3D geometric input. For fair comparison, we adapt these baselines with pseudo 3D inputs inferred from images following similar protocols as previous methods~\cite{li2023voxformer,cao2022monoscene}.

\noindent \textbf{Evaluation Metrics}.
Regarding quantitative evaluations, we conduct experiments on metrics~\cite{li2023voxformer,cao2022monoscene} that have been widely employed in the field of SSC. We leverage \textbf{IoU} (Intersection over Union) to account for SC task and \textbf{mIoU} (mean Intersection over Union) to measure the performance of SSC task, respectively. For both of these two metrics, higher values are desirable, where high IoU indicates accurate geometric prediction and high mIoU implies precise semantic segmentation.

\subsection{Performance}
\noindent \textbf{Quantitative Comparison. }
Table \ref{tabq1} reports the performance of our StereoScene and other camera-based baselines on the SemanticKITTI hidden test set. Our approach outperforms MonoScene by a large margin in terms of geometric completion and semantic segmentation. Significantly, the IoU is improved by 26.9\% (34.16\%$\rightarrow$43.34\%)
while the mIoU is improved by 38.6\% (11.08\%$\rightarrow$15.36\%).
Except for the truck category (0.5 lower than MonoScene), StereoScene has surpassed other baselines in terms of individual category prediction.

It's worth noting that our method demonstrates significant superiority in the prediction of small moving objects compared to MonoScene, including bicycle (0.50\%$\rightarrow$3.40\%), motorcycle (0.70\%$\rightarrow$2.40\%), person (1.00\%$\rightarrow$2.90\%), trunk (2.40\%$\rightarrow$8.40\%), pole (3.30\%$\rightarrow$7.00\%), etc. We ascribe such improvements to the introduction of dual volume, which is critical for 3D geometry awareness. 

As shown in Table \ref{tabq2}, we also report the quantitative results on the SemanticKITTI validation set. 
Although Voxformer-T employs up to 4 temporal stereo image pairs as inputs, our method has only a negligible difference in IoU but a significant advantage in mIoU, with a 36.5\% improvement over MonoScene and a 15.6\% improvement over Voxformer-T.



\noindent \textbf{Qualitative Comparison. }
 As shown in Figure~\ref{fig_q}, we compare the visualization results of our StereoScene and MonoScene on the SemanticKITTI validation set. 
 Due to the complexity of the outdoor scenes and the sparsity of the labels, it is challenging to reconstruct the geometry of the scenes accurately and completely. 
Compared to MonoScene, our StereoScene evidently captures better geometric representations to reconstruct more complete scenes with correct classes (e.g. vegetation and terrain beside the road).
It is worth noting that our StereoScene can generate accurate 3D scene layout even in cluttered scenarios, which is attributed to explicit geometry awareness. For example, in the second column, StereoScene not only reconstructs the scene on the right side of the road, but also distinguishes  the sidewalk more properly compared to MonoScene.

Our method shows obvious advancement on small moving objects compared to MonoScene (e.g. cars in column 1 and column 4). Moreover, for regions outside the camera's field of view (voxels in darker regions), StereoScene is capable of hallucinating a more reasonable and complete scene compared to MonoScene (e.g. darker regions in column 1, column3).










\begin{figure*}
   \vspace{-5pt}
	\begin{center}
		\includegraphics[width=16cm]{vis_all.png}   
        \vspace{-10pt}
		\begin{tabular}{cccccc}
			\multicolumn{6}{c}{
				\scriptsize
				\textcolor{bicycle}{$\blacksquare$}bicycle~
				\textcolor{car}{$\blacksquare$}car~
				\textcolor{motorcycle}{$\blacksquare$}motorcycle~
				\textcolor{truck}{$\blacksquare$}truck~
				\textcolor{other-vehicle}{$\blacksquare$}other vehicle~
				\textcolor{person}{$\blacksquare$}person~
				\textcolor{bicyclist}{$\blacksquare$}bicyclist~
				\textcolor{motorcyclist}{$\blacksquare$}motorcyclist~
				\textcolor{road}{$\blacksquare$}road~
				}
    \\
    
			\multicolumn{6}{c}{
				\scriptsize
                    \textcolor{parking}{$\blacksquare$}parking~
				\textcolor{sidewalk}{$\blacksquare$}sidewalk~
				\textcolor{other-ground}{$\blacksquare$}other ground~
				\textcolor{building}{$\blacksquare$}building~
				\textcolor{fence}{$\blacksquare$}fence~
				\textcolor{vegetation}{$\blacksquare$}vegetation~
				\textcolor{trunk}{$\blacksquare$}trunk~
				\textcolor{terrain}{$\blacksquare$}terrain~
				\textcolor{pole}{$\blacksquare$}pole~
				\textcolor{traffic-sign}{$\blacksquare$}traffic sign			
			}
		\end{tabular}	
 	\end{center}
        \vspace{-10pt}
	\caption{ Qualitative results on the SemanticKITTI~\cite{behley2021towards} validation set. The overlay shadow area at bottom of semantic prediction denotes unseen scenery out of camera's field of view (FOV). StereoScene shows obvious advancement on small moving objects and proper hallucinations outside FOV.
 } 
	\label{fig_q}
\end{figure*} 



\begin{table}[!ht]
\begin{tabular}{cccc}
\hline
 \textbf{Architecture Components}& \textbf{IoU(\%)}$\uparrow$      & \textbf{mIoU(\%)}$\uparrow$      \\  \hline
Ours                      & \textbf{42.85} & \textbf{15.29}  \\  
Ours w/o Depth Conversion &  41.94          &    15.12             \\
Ours w/o BiT Module       &  41.60            &   14.19             \\
Ours w/o DVA Module       & 42.28         &  15.16            \\  \hline
\end{tabular}
% \vspace{-5pt}
\caption{ \textbf{Ablation study for architecture components.}  }
\label{tab_ar}
\end{table}


\begin{table}[!ht]\centering
% \scriptsize
\begin{tabular}{lccccc}\toprule
\multicolumn{4}{c}{\textbf{Feature Scales}} &\multirow{2}{*}{\textbf{IoU (\%)}$\uparrow$ } &\multirow{2}{*}{\textbf{mIoU (\%)}$\uparrow$} \\
$\frac{1}{32}$ & $\frac{1}{16}$ & $\frac{1}{8}$ &$\frac{1}{4}$    \\ \midrule
\checkmark & & & & 40.08 & 13.40  \\
\checkmark &\checkmark & & & 41.28 &14.29   \\
\checkmark &\checkmark &\checkmark & & 42.68 &14.93  \\
\checkmark &\checkmark &\checkmark &\checkmark & \textbf{42.85} & \textbf{15.29}  \\
\bottomrule
\end{tabular}
 
\caption{\textbf{Ablation study for feature scales.} Feature scales are related to the resolution of the input image.} 
\label{tab_feature}
\end{table}




\begin{table}[!ht]
\begin{tabular}{ccc}
\hline
  \textbf{Volumes}   & \textbf{IoU(\%)}$\uparrow$      & \textbf{mIoU(\%)}$\uparrow$      \\ \hline
Ours      & \textbf{42.85} & \textbf{15.29} \\
Ours w/o Stereo Volume    &      38.97          &   13.18             \\
Ours w/o BEV Volume &   41.71          &    13.71            \\
Ours w/o Confidence Filtering&    42.19   &   14.67             \\ \hline
% Ours w/o BEV Volume Guidance   &                &                \\
% Ours w/o Stereo Volume Guidance&                &   14.95             \\ \hline
\end{tabular}
\vspace{1mm}
\caption{ \textbf{Ablation study for the dual volume.} The best performance is achieved by aggregating the both volumes with confidence filtering. }
\vspace{-10pt}

\end{table}




\subsection{Ablation Study}
We ablate our StereoScene on the SemanticKITTI validation set for scene completion and semantic scene completion. For both tasks, we use only RGB images as inputs.



\noindent \textbf{Effect of Architecture Components. }
We conduct architectural ablation to evaluate the impact of different components of our network as shown in Table~\ref{tab_ar}. The BIT module can significantly improve the geometric and semantic estimations (+1.25 IoU, +1.10 mIoU). Compared with the DVA module, the depth conversion can boost geometric prediction by a relatively larger margin (+0.91 IoU), while having less impact on semantic prediction (+0.17 mIoU).
 


\noindent \textbf{Effect of 2D Feature Scales. }
The ablation study for the 2D image feature scales is shown in Table~\ref{tab_feature}. We construct image features with different scales by adjusting the convolution strides in the 2D UNet. As we can see from the table, the results can be significantly improved by combining features with different scales, increasing the IoU from 40.08 to 42.85 and mIoU from 13.40 to 15.29, respectively. 
 


\noindent \textbf{Effect of Dual Volume. }
The ablation study for the dual volume is shown in Table~\ref{tab_feature}. 
We first build the framework pipeline with an individual volume and maintain the DVA module to verify the effect.
We find that after removing the stereo volume, the geometric perception ability of the framework decreases dramatically (-3.88 IoU), while the prediction performance of semantic scene segmentation is affected as well (-2.11 mIoU). 
The introduction of BEV volume has a relatively greater impact on the semantic aspect compared to the geometric aspect, boosting IoU by 1.14 and mIoU by 1.58, respectively.
For 'w/o Confidence Filtering', we remove the confidence filter and directly multiply the guidance attention matrix with the value vector, while both of the volumes are utilized and aggregated. The confidence filtering further improves the IoU (+0.66) and mIoU (+0.62) as expected.

Figure~\ref{fig_q} visualizes the impact of the volumes. As the figure shows, the prediction results of scene semantics (e.g. sidewalk, column 2) and geometry (e.g. distant truck, column 2) are significantly improved with the introduction of BEV volume and stereo volume, respectively. 
It validates the effectiveness of our confidence filtering on retaining the high confidence regions for better aggregation (e.g. trunks, column1 and column 3). 



\section{Conclusion}
        \vspace{-10pt}
In this work, we propose \textbf{StereoScene}, a BEV-assisted stereo matching 3D Semantic Scene Completion (SSC) framework that generates high-fidelity 3D scene understanding results without resorting to any external 3D geometric sensor inputs. 
We emphasize serveral appealing properties of our work: \textbf{1)} Without utilizing any pretrained 3D teacher network or explicit geometric inputs, we implicitly formulate a structured space through geometric contraint.  \textbf{2)} A transformer-based \emph{Mutual Interactive
Aggregation Module} is delicately devised to take full advantage of complementary nature of BEV representation and Stereo Matching. \textbf{3)} Our model demonstrates great robustness at extreme conditions such as tiny objects recognition or scenery hallucination outside camera's field of view. 


%-----------------------------------------------
{\small
\bibliographystyle{ieee_fullname}
\bibliography{stereoscene}
}



\appendix
\twocolumn[{
\centering
 \vspace{20pt}
\section*{\Large \centering Supplementary Material of \\BEV-Assisted Stereo Matching Empowers 3D Semantic Scene Completion}
 \vspace{30pt}
 }]

 
\begin{figure*}[!ht]
	
	\begin{center}
		\includegraphics[width=15cm]{demo_video.png}   
        \vspace{0pt}
		\begin{tabular}{cccccc}
			\multicolumn{6}{c}{
				\scriptsize
				\textcolor{bicycle}{$\blacksquare$}bicycle~
				\textcolor{car}{$\blacksquare$}car~
				\textcolor{motorcycle}{$\blacksquare$}motorcycle~
				\textcolor{truck}{$\blacksquare$}truck~
				\textcolor{other-vehicle}{$\blacksquare$}other vehicle~
				\textcolor{person}{$\blacksquare$}person~
				\textcolor{bicyclist}{$\blacksquare$}bicyclist~
				\textcolor{motorcyclist}{$\blacksquare$}motorcyclist~
				\textcolor{road}{$\blacksquare$}road~
				}
    \\
    
			\multicolumn{6}{c}{
				\scriptsize
                    \textcolor{parking}{$\blacksquare$}parking~
				\textcolor{sidewalk}{$\blacksquare$}sidewalk~
				\textcolor{other-ground}{$\blacksquare$}other ground~
				\textcolor{building}{$\blacksquare$}building~
				\textcolor{fence}{$\blacksquare$}fence~
				\textcolor{vegetation}{$\blacksquare$}vegetation~
				\textcolor{trunk}{$\blacksquare$}trunk~
				\textcolor{terrain}{$\blacksquare$}terrain~
				\textcolor{pole}{$\blacksquare$}pole~
				\textcolor{traffic-sign}{$\blacksquare$}traffic sign			
			}
		\end{tabular}	
	% 	\label{fig:qualitative_kitti}
	% \end{subfigure}
 	\end{center}
        % \vspace{1mm}
        \vspace{20pt}
	\caption{ More visualization results on the SemanticKITTI~\cite{behley2021towards} validation set. From top to below demonstrate 3 cases. For each one, the first row illustrates stereo input images while the second row depicts comparison results of MonoScene and \textbf{StereoScene}.  
 } 
	\label{fig_video}
\end{figure*} 

%%%%%%%%% 1.mono  2. occdepth  3.  


\begin{figure*}[!ht]
\vspace{2pt}
\hsize=\textwidth %
\centering
\includegraphics[width=1\textwidth]{stereo_constructor.png}
\caption{Implementation details of the stereo representation construction. The left and right features are encoded with camera parameters before forming the disparity volume.
}
\label{figstereo}
 \vspace{0pt}
\end{figure*}

\vspace{30pt}
\section{More Visualization Results}
% \textbf{Precision of Semantic Classes}
% \textbf{Completion of Overall Scenes}
Below we provide qualitative comparisons on SemanticKITTI validation set and show visualization examples in Figure~\ref{fig_video}.
Compared to MonoScene~\cite{cao2022monoscene}, our \textbf{StereoScene} generates more precise predictions (e.g. truck, example 1; car, example 3) and more complete scenes with proper hallucinations (e.g. example 2).

Specifically, in example 1, MonoScene predicts wrong shape for truck instance prediction (see yellow circle).
In example 2, compared to MonoScene, our method successfully hallucinates a more complete and reasonable scene outside the camera FOV. Moreover, the semantics generated by StereoScene is consistent with the holistic scenes in most regions.
In example 3, StereoScene precisely capture the semantic and geometry of car while MonoScene faces challenges in correct recognition.
% Due to the lack of image information, predictions outside the field of view (FOV) are challenging for camera-based methods. 





\section{Implementation Details on Dual Volume Construction}

In this section, we describe implementation details for our dual volume construction. The \emph{Stereo Representation Construction} module targets to match 2D extracted features and outputs a structural 3D representation, regularized depth volume. The \emph{BEV Representation Construction} module constructs a 3D latent volume coupled with context features from 2D image features.

\noindent \textbf{More Details on Stereo Representation Construction. }
As mentioned in Sec. 3.2 of the main paper, we construct stereo geometric volume from the left and right image features. 
% In our implementation, in order to 
% filter the noise introduced in binocular image acquisition and calibration, we 
For the purpose of being aware of distinct camera inputs, 
% we model the projection relationship between 2D and 3D representations explicitly, 
we also feed the camera parameters $P_{i}$ (concatenated intrinsic and extrinsic) to a \textbf{Parameter Encoder} as shown in Figure~\ref{figstereo}.
% and fuse them with the unary features obtained from 2D unet extraction. 
% The \textbf{Parameter Encoder} (as shown in Figure~\ref{figstereo}) is formulated as follows:
Formally, 
% \begin{equation} \label{eq1}
% f_{c}=f_{i} \otimes \left(  \sigma \left (\mathbb{C}  (\Re (FC(Para))) \right ) \right )
% \end{equation}
\begin{equation} \label{eq1}
P_{e}= \sigma \left ( Conv  (Reshape (FC(P_{i}))) \right )   
\end{equation}

 where $Conv$ and $FC$ are convolutions and fully-connected layers, whereas $\sigma$ and $Reshape$ represent sigmoid function and reshape operation, respectively. 
The encoded camera parameters $P_{e}$ are multiplied with the extracted features to generate the camera-aware features.



\begin{table}[!ht]\small
\begin{center}
\begin{tabular}{c|c|c|c}
\hline
Name          & Layer Properties & Dim & Input              \\ \hline
Conv1   & $3\times3\times3$, 2, 1  & $32/64$  & Input Volume    \\ \hline
Conv2   & $3\times3\times3$, 1, 1  & $64/64$  & Conv1        \\ \hline
Conv3   & $3\times3\times3$, 2, 1  & $64/128$  & Conv2      \\ \hline
Conv4   & $3\times3\times3$, 1, 1  & $128/128$  & Conv3        \\ \hline
Deconv5   & $3\times3\times3$, 2, 1  & $128/64$  & Conv4       \\ \hline
Conv6   & $1\times1\times1$, 1, 0  & $64/64$  & Conv2       \\ \hline
Shortcut  & Addition \& ReLU  & $64/64$  &\begin{tabular}[c]{@{}c@{}}Deconv5, \\ Conv6 \end{tabular}   \\ \hline

Deconv7   & $3\times3\times3$, 2, 1  & $64/32$  & Shortcut       \\ \hline
Conv8  & $1\times1\times1$, 1, 0  & $32/32$  & Input Volume       \\ \hline
Output Volume  & Addition \& ReLU  & $32/32$  & \begin{tabular}[c]{@{}c@{}}Deconv7,\\ Conv8 \end{tabular}  \\ \hline
\end{tabular}
\caption{Structure details of the 3D hourglass. The $"$Layer Properties$"$ indicates kernel size, stride and padding, respectively.}
\label{tab:1}
\end{center}
\end{table}



The left and right camera-aware unary features are correlated to form the group-wise disparity volume, which is then converted to depth volume. 
To regularize the depth volume, we adopt 3D CNNs with hourglass (encoder-decoder) structures to process it~\cite{guo2019group, cfnet}.
As shown in Figure~\ref{figstereo}, we first feed the depth volume  to four 3$\times$3$\times$3 convolutions with stride 1, the last two of which are implemented with residual connection. 

With the depth volume, we employ three stacked 3D hourglasses to further aggregate the volumetric information. The structure details of the 3D hourglass are shown in Table~\ref{tab:1}, where two residual connections are adopted to aggregate contexts from different semantic levels. In each 3D hourglass, the input volume is first downsampled and then upsampled with deconvolution, thus aggregating information along the spatial and depth dimensions. Finally, two 3$\times$3$\times$3 convolutions are used to generate the regularized depth volume with dimension of $ \mathbb{R}^{1 \times D\times H \times W}$.



\begin{figure}[!ht]
\vspace{0pt}
\begin{center}		\includegraphics[width=8.5cm]{bev_constructor.png}  
 	\end{center}
	\caption{ Implementation details of the BEV representation construction. We employ two branches in parallel to generate the BEV context features and BEV latent volume.} 
	\label{figbev}
 \vspace{0pt}
\end{figure} 




\noindent \textbf{More Details on BEV Representation Construction.}
As mentioned in Sec. 3.2 of the main paper, we construct BEV context features and BEV latent volume as BEV representations~\cite{philion2020lift}. In our implementation, we adopt left image features and camera parameters as inputs. 



As shown in Figure~\ref{figbev}, two branches are employed in parallel, where the upper branch is dedicated to generating contextual features while the lower branch is dedicated to generating latent volume. For both branches, we use camera parameter encoder similar to Equation~\ref{eq1} for camera-awareness. In the upper branch, a 3$\times$3 convolution is adopted to generate the context features. 
In the lower branch, we adopt two stacked residual blocks and an ASPP~\cite{chen2017rethinking, chen2018encoder} module to refine the depth distribution of the latent volume. The ASPP module is used to expand the receptive field in depth perception process, which contains four atrous convolutions with dilation rates of 1, 6, 12, 18, respectively.


 
\end{document}