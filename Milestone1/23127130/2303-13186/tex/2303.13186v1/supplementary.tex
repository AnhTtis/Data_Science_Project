% \documentclass[10pt,twocolumn,letterpaper]{article}
\documentclass[10pt,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{eucal} 


% \usepackage{booktabs}
% \usepackage{multicol}
% \usepackage{rotating}
% \usepackage{tabularx}
% \usepackage{multirow}
% \usepackage{makecell}
% \usepackage{hyperref}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE

\title{ScanERU: Interactive 3D Visual Grounding based on \\ Embodied Reference Understanding \\ \large{\textmd{Supplementary Material}}}
\

\author{Ziyang Lu
% Institution1 address\\
% {\tt\small gqwang0420@hotmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Yunqiang Pei
\and 
Guoqing Wang*
\and
Yang Yang
\and
Zheng Wang
\and 
Heng Tao Shen
\\
School of Computer Science and Engineering, \\ University of Electronic Science and Technology of China\\
{\tt\small498358329@qq.com simon1059770342@foxmail.com gqwang0420@hotmail.com}
}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\renewcommand{\thesection}{\Alph{section}}
\section{Validity Test of ScanERU Dataset}
To evaluate the effectiveness of the ScanERU dataset, we conducted an experiment on a real-world test set. This section outlines the test set creation process and provides details about the experiment.



\begin{figure*}[tb]
 \centering
  \includegraphics[width=\textwidth]{sup-imgs-Fig1.pdf}
 \caption{The data collection process. we employed the Azure Kinect DK and the official SDK, as well as the Open3D reconstruction system, to reconstruct 3D scenes. Subsequently, the 3D meshes underwent manual processing in MeshLab to eliminate distorted areas and align the point cloud to a z-axis up coordinate system.}
 \label{fig:collection}
\end{figure*}

\subsection{Data collection}

In our study, we employ the Azure Kinect DK, a Time-of-Flight (ToF) RGB-D camera with an inertial measurement unit (IMU), as our 3D sensor for reconstructing the scene. The RGB sensor operate at a resolution of 1280*540, while the depth sensor function at a resolution of 512*512. We utilize the official SDK of the Azure Kinect DK and the Open3D reconstruction system as our software. Following data acquisition, we manually process the 3D mesh data in MeshLab, by deleting distorted areas in the point cloud, especially in the edge regions, and by applying down-sampling to align the original ScanNet \cite{AngelaDai2017ScanNetR3} point cloud. Additionally, we rotate and translate the point cloud to align it to a z-axis up coordinate system. The data collection process is illustrated in ~\autoref{fig:collection}.

\subsection{Data Annotation}

\begin{figure}[tb]
 \centering
  \includegraphics[width=\columnwidth]{sup-imgs-Fig4-small.pdf}
 \caption{Dataset Statistics of test set for ScanERU.}
 \label{fig:statistics}
\end{figure}

Since these data are only used to test the validity of ScanERU, we do not perform a complete semantic segmentation annotation on the point cloud. We develope a Blender script to annotate the referred object and the human agent. For the descriptions, we apply the similar tool as ScanERU data annotation to help our workers to describe the referred object. The whole task is assigned to four workers with the request of each description (e.g. the length of description, the way to describe the referred object etc). In addition, each referred object is described by all four workers.

\subsection{Data Statistics}
In this test set, we have 100 samples. We collect 5 different scenes, consisting of 1 office, 2 lounges, and 2 classrooms, with 5 referred objects in each scene. For each referred object, we write 4 descriptions to localize them. The average length of description is 19.67 words. Following the ScanRefer \cite{chen2020scanrefer}, 100$\%$ description uses spatial language, 75$\%$ description uses color, 60$\%$ description uses shape terms, and 10$\%$ description use size information. Our human agentâ€™s height is 176cm, following the average height of the synthetic human agents in ScanERU dataset. ~\autoref{fig:statistics} lists the distribution of the referred objects.

\subsection{Experiment}

\begin{figure*}[tb]
 \centering
  \includegraphics[width=\textwidth]{sup-imgs-Fig3.pdf}
 \caption{The localization results of 3DVG-Transformer \cite{LichenZhao20213DVGTransformerRM} and ScanERU on real-world scenes. Ground-truth bounding boxes are shown in blue, while predicted boxes are green if their IoU score with ground truth is above 50$\%$, and red otherwise. The study found their method to handle real-world environments better than 3DVG-Transformer \cite{LichenZhao20213DVGTransformerRM}, which produced inaccurate bounding boxes or failed to locate objects.}
 \label{fig:localization}
\end{figure*}

\begin{table}[b]
\caption{Comparasion of visual grounding performances on real-world test set.}

	\centering%
% \resizebox{\linewidth}{!}{ 
\begin{tabular}{cc|cc}
\hline
Methods        & Modality & Acc@0.25 & Acc@0.5 \\ \hline
ScanRefer \cite{chen2020scanrefer}  & 3D       & 34.6    & 31.0    \\
3DVG-Transformer \cite{LichenZhao20213DVGTransformerRM} & 3D       & 35.3    & 33.6   \\
Ours & 3D       & 38.0    & 34.6   \\ \hline
\end{tabular}
% }
\label{lab:quantitive_study}
\end{table}

\textbf{Quantitive study.}
\autoref{lab:quantitive_study} shows the performance comparison of ScanRefer \cite{chen2020scanrefer}, 3DVG-Transformer \cite{LichenZhao20213DVGTransformerRM}, and ScanERU on the task of 3D object localization. We train ScanRefer \cite{chen2020scanrefer} and 3DVG-Transformer \cite{LichenZhao20213DVGTransformerRM} on the ScanRefer dataset \cite{chen2020scanrefer}, while we train our method on ScanERU dataset. Our method achieves higher accuracy than ScanRefer \cite{chen2020scanrefer} and 3DVG-Transformer \cite{LichenZhao20213DVGTransformerRM} by 3.3$\%$ and 2.6$\%$ for Acc@0.25, and 3.6$\%$ and 1.0$\%$ for Acc@0.5, respectively. The results indicate that our method is generalizable to real-world environments.

\textbf{Qualitive study.}
~\autoref{fig:localization} illustrates examples of the localization results of 3DVG-Transformer \cite{LichenZhao20213DVGTransformerRM} and ScanERU on real-world scenes. The ground-truth bounding boxes are denoted in blue, while the predicted boxes are highlighted in green if their intersection-over-union (IoU) score with the ground truth is above 50$\%$, and in red otherwise. The figure demonstrates that our method can handle complex situations in real-world environments better than 3DVG-Transformer \cite{LichenZhao20213DVGTransformerRM}, which fails to locate the correct objects or produces inaccurate bounding boxes.


\begin{figure*}[tb]
 \centering
  \includegraphics[width=\textwidth]{sup-imgs-Fig5.pdf}
 \caption{The localization results of the ablation study.
}
 \label{fig:ablation}
\end{figure*}

\section{Additional Qualitative Experiment for Ablation Study}
~\autoref{fig:ablation} provides a visualization of the performance of three models: Ours$_{full}$, Ours$_{lang-only}$, and Ours$_{ges-only}$. The ground-truth boxes are marked in blue, while predicted boxes with an IoU score greater than 0.5 are marked in green, otherwise they are marked in red. The results demonstrate that combining textual and gestural information is crucial for localizing objects accurately. As shown in ~\autoref{fig:ablation}(a), the description ``polka dotted chair" is challenging to understand, especially in the point cloud environment. In such cases, gesture provides easily understandable information to aid in localizing the referred object. However, as depicted in ~\autoref{fig:ablation}(b), when multiple objects, such as a sofa and table, are in the same direction pointed by the human agent, the information becomes highly ambiguous, leading to incorrect localization results. Furthermore, ~\autoref{fig:ablation}(c) demonstrates that when the textual and gestural information is ambiguous simultaneously, Ours$_{lang-only}$ and Ours$_{ges-only}$ fail to localize the referred object. In this scenario, Ours$_{full}$ can leverage both textual information, such as the word ``corner" and gestural information to disambiguate and localize the correct object and avoid the problem of potentially confusing the current perspective with the actual shooting perspective. In summary, the findings indicate that the combination of textual and gestural information is crucial for accurate object localization.


\newpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{ours}
}

\end{document}