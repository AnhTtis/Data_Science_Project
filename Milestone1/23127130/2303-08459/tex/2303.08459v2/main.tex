\documentclass[sn-nature,Numbered]{sn-jnl}% Style for submissions to Nature Portfolio journals


%\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage[table]{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%


\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%
\newtheorem{proposition}[theorem]{Proposition}% 
\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%
\raggedbottom


\begin{document}

\title[Forecasting Intraday Power Output by a Set of PV Systems using Recurrent Neural Networks and Physical Covariates]{Forecasting Intraday Power Output by a Set of PV Systems using Recurrent Neural Networks and Physical Covariates}


\author*{\fnm{Pierrick} \sur{Bruneau}*}\email{pierrick.bruneau@list.lu}
\author{\fnm{David} \sur{Fiorelli}}\email{david.fiorelli@list.lu}
\author{\fnm{Christian} \sur{Braun}}\email{christian.braun@list.lu}
\author{\fnm{Daniel} \sur{Koster}}\email{daniel.koster@list.lu}

\affil{\orgname{Luxembourg Institute of Science and Technology}, \orgaddress{\street{5 Avenue des Hauts-Fourneaux}, \city{Esch-sur-Alzette}, \postcode{L-4362}, \country{Luxembourg}}}



\abstract{
Accurate intraday forecasts of the power output by PhotoVoltaic (PV) systems are critical to improve the operation of energy distribution grids. We describe a neural autoregressive model which aims at performing such intraday forecasts.
We build upon a physical, deterministic PV performance model, the output of which being used as covariates in the context of the neural model. In addition, our application data relates to a geographically distributed set of PV systems. We address all PV sites with a single neural model, which embeds the information about the PV site in specific covariates. We use a scale-free approach which does rely on explicit modelling of seasonal effects.
Our proposal repurposes a model initially used in the retail sector, and discloses a novel truncated Gaussian output distribution. 
An ablation study and a comparison to alternative architectures from the literature shows that the components in the best performing proposed model variant work synergistically to reach a skill score of 15.72\% with respect to the physical model, used as a baseline.
}

\keywords{Autoregressive models, Time series forecasting, Solar Energy, Application}


\maketitle

\section{Introduction} \label{sec:intro}


Grids of PV systems have become an inevitable component in the modern and future energy distribution systems. However, due to weather conditions, the magnitude of PV power production is fluctuating, while the supply to consumers requires to be adapted to the demand at each point in time. Distribution system operators (DSOs) have increasing and specific requirements for PV power forecasts. Indeed, fluctuating renewables could cause operational issues (e.g. grid congestion), which call for active grid operation. 
In this context, fine-grained forecasts of PV power expected during the day to come are critical in view to facilitate operations. Also, many forecasting models issue point forecasts, but hardly characterize the uncertainty attached to their forecasts, when such information can be critical for a DSO in order to quantify and mitigate risks in the context of a trading strategy.

PV power production can be forecasted using a deterministic PV performance model \cite{koster_short-term_2019}. It models the internals of a PV system, from the solar irradiance received to the electrical power produced using a set of physical equations (referred to by the shorthand term \emph{physical model} in the remainder of this paper). Therefore, for each time step for which PV power has to be predicted, it needs a solar irradiance forecast, provided by a Numerical Weather Prediction (NWP) service, as inputs. The underlying hypothesis is that solar irradiance is fairly smooth over limited regional areas, and the production curve specific to a PV system will be mainly influenced by how it converts this solar energy to PV power according to its specifications.
In \cite{koster_single-site_nodate}, authors of the present paper introduced a model which performs intraday probabilistic forecasts of PV power production. It combines the physical model referred to above with a model based on Long-Short-Term Memory (LSTM) cells \cite{hochreiter97}. This kind of combination of a model based on a set of physical equations to a statistical model is sometimes coined as \emph{hybrid-physical} in the solar energy literature \cite{antonanzas2016}. For training and evaluation, it uses real data provided by Electris, a DSO in Luxembourg. Results show that this new model improves the baseline performance, while coping with local effects such as PV system shading. The former paper rather targets solar energy specialists, with little details unvealed about how the Machine Learning model acting as cornerstone to the approach has been designed and trained. The present paper aims at filling this gap, by providing entirely new material focusing on this complementary view. Specifically, the purpose of the present paper is to focus on neural time series forecasting aspects in the context of this application.


The specific contributions of the present work mainly focus on the design of a model architecture and a training procedure which meets the operational needs expressed by the DSO. The contributions of our applicative paper are:

\begin{itemize}
    \item Repurposing an existing LSTM-based model \cite{salinas_deepar_2020}, by casting the hybrid-physical approach in terms of statistical model covariates,
    \item Using PV system specifications as covariates in a univariate model,
    \item Designing a novel truncated Gaussian output component, which we plug in to the LSTM-based model.
\end{itemize}

In Section \ref{sec:related}, we give a structured survey of the related work which positions the addressed applicative problem, and motivates which existing work could be reused or repurposed to suit our needs. After describing our model proposal in Section \ref{sec:model}, we provide a thorough experimental evaluation in Section \ref{sec:experiments}. Several variants of our model proposal are compared to alternative models from the literature, and an ablation study allows to emphasize the specific contribution of each of its components. Finally we recall some qualitative results to underline how local effects, tainting the PV performance model, are mitigated using our approach.



\section{Related Work} \label{sec:related}

In Section \ref{sec:pvpower}, we review seminal PV power forecasting methods. Then in Section \ref{sec:ml}, we survey time series forecasting as addressed in the literature on Machine Learning (ML), from the perspective of their repurposing potential to the PV power forecasting application. Finally, Section \ref{sec:validation} focuses on the peculiarities in terms of forecasting structure and validation which come with ML approaches applied to time series forecasting.


\subsection{PV Power Forecasting} \label{sec:pvpower}

Most approaches in PV power forecasting model the conversion chain of solar irradiance to electrical power in a PV system. Thus they follow a two-step approach: first, forecasting the solar irradiance, then converting this irradiance to PV power forecasts \cite{blaga2019, koster_short-term_2019}. 
The most common way to forecast solar irradiance relies on NWP systems such as the European Centre for Medium-Range Weather Forecasts (ECWMF) Ensemble Prediction System \cite{molteni_ecmwf_1996}. Everyday, it issues hourly regional forecasts for a range of meteorological variables (including solar irradiance) on the 10 days to come, thus including the intraday range. 
Intraday may be defined as up to 6h forecast horizons in the literature \cite{antonanzas2016}. In this paper, for experimental simplicity, we deviate from this definition by considering intraday as the next 24h, starting at midnight of the same day.
The PV power data described in Section \ref{sec:data} is collected each day at 6:00 CET by Electris, and twilight time is generally before 22:00 CET in Luxembourg. As we exclude night time slots from the metrics presented in Section \ref{sec:loss}, our intraday definition is roughly equivalent to that used in the context of the Nord Pool power exchange market\footnote{\url{https://www.nordpoolgroup.com/}}.
In view to improve NWP system outputs, or to avoid having to rely on such systems, solar irradiance can also be forecasted using statistical methods and ML models. The simplest include persistence models, which are often adjusted using clear sky models \cite{yang_choice_2020}. \cite{inman_solar_2013} also review various ML techniques which have been employed to this purpose, e.g., AutoRegressive (AR) models, Feed-Forward Networks (FFN) and k-Nearest Neighbors. In this range of contributions, \cite{bruneau_bayesian_2012} address the intraday hourly prediction of solar irradiance using an ensemble of FFN models. Specifically, they implement rolling forecasts by specializing each model to a current time and a prediction horizon.


PV power forecasting is reviewed in detail by \cite{antonanzas2016}. In this domain, several approaches aim at modelling directly the series of PV power values, without having to rely on solar irradiance forecasts. \cite{elsinga_short-term_2017} propose short-term forecasts ($<$30mn) which exploit cross-correlation of PV measurements in a grid of PV systems. They hypothesize that clouds casting over a given PV system have lagged influence on other systems downwind. They optimize associated time lag and cloud motion vector.
\cite{lonij_intra-hour_2013} also consider a spatially distributed set of PV panels. They directly use PV power values, without converting proxy information such as solar irradiance or cloud density. Similarly to \cite{elsinga_short-term_2017}, they focus on correlations among stations to help accounting for intermittency due to clouds.

\cite{pedro_assessment_2012} present AR approaches to PV power forecasting. They focus on forecasting one and two hours ahead, where NWP models tend to under-perform. Several models are compared, among which are persistence, linear models such as AutoRegressive Integrated Moving Average (ARIMA) \cite{hyndman_automatic_2008}, and FFN. 
They found out that FFN perform the best, with improvements brought by the optimization of FFN parameters, input selection and structure using a Genetic Algorithm. 
%They conjecture that binning data according to the associated season, and learning per-bin models should improve forecasting ability overall, even if this approach has been recently criticized \cite{salinas_deepar_2020}.



\subsection{ML approaches for time series forecasting} \label{sec:ml}

Among other related work, Section \ref{sec:pvpower} surveyed some contributions which involved ML methods in view to forecast solar irradiance and PV power production. In this section, we generalize this view, by surveying recent work in time series forecasting at large. Methods in this section were generally not applied to the applicative context considered in the present paper, but could be repurposed \emph{a priori}.
Besides neural and ARIMA models, seminal ways to forecast time series include the Croston method, which is an exponential smoothing model dedicated to intermittent demand forecasting \cite{shenstone_stochastic_2005}. It is notably used as baseline method in \cite{salinas_deepar_2020}, along with the Innovation State-Space Model (ISSM) \cite{seeger_bayesian_2016}.

Modern, so-called \emph{deep} neural network architectures, such as the Long-Short-Term Memory (LSTM) \cite{hochreiter97} and the Gated Recurrent Unit (GRU) \cite{cho14}, exploit the sequential structure of data.
Even though recurrent models such as the LSTM would appear as outdated, they are still popular in the recent literature thanks to improvements to training and inference procedures carried by modern toolboxes such as Tensorflow \cite{tensorflow2015-whitepaper} or MXNet \cite{chen15}, as well as the encoder-decoder mechanism, in which a context sequence is encoded and conditions the prediction of the target sequence. It has been initially codified for the GRU, and transferred to the LSTM, leading to continued usage in recent contributions \cite{shi_convolutional_2015, salinas_deepar_2020}.
These models contrast with the seminal Feed-Forward Network (FFN), in which all layers are fully connected to previous and next layers, up to the activation layer \cite{bebis_feed-forward_1994}.

Salinas et al. propose DeepAR, which implements flexible forecasting for univariate time series \cite{salinas_deepar_2020}. Formally, it defines a \emph{context} interval (a chunk of past values) and a \emph{forecasting} interval (the set of values to be forecasted), and the model is optimized end-to-end w.r.t. the whole forecasting range. This contrasts with linear models such as ARIMA, which are optimized for one time step ahead forecasts.
Also, instead of point forecasts, DeepAR predicts model parameters, which can be used to compute sample paths and empirical quantiles, then potentially used by the DSO to adjust its trading strategy in the context of this paper. In this case, a family of probability distributions has to be chosen so as to fit the time series at hand. The model was initially aimed at retail business applications, but it can be adapted to other types of data just by changing the family of the output probability distribution. It is based on the LSTM model architecture. 
The model supports the adjunction of covariates, i.e., time series which are available for both context and forecasting intervals at inference time. By repeating the same value for the whole intervals, static covariates are also supported. 
%Such flexibility meets all the requirements of our hybrid-physical approach (i.e., NWP covariates and PV system descriptive features).

In retail applications, input data may have highly variable magnitude (e.g., depending on item popularity or time in the year). The authors observe an approximate power law between item magnitude and frequency (i.e., values are less likely as they get large, and reciprocally). They claim that grouping items to learn group-specific models or performing group-specific normalizations, as previously done in the solar and PV power forecasting literature \cite{pedro_assessment_2012,bruneau_bayesian_2012} are not good strategies in such case. They propose a simple alternative scheme, where samples are scaled by a factor computed using the context interval.


%\cite{fawaz_inceptiontime_2019} propose a time series classification model inspired by Inception-v4 \cite{szegedy_inception-v4_2016}. Basically, they are  transferring the multi-scale pattern extraction capability of convolutional neural networks to 1D data such as time series. 
A convolutional encoder is tested by Wen et al. in the context of their multi-horizon quantile forecaster \cite{wen_multi-horizon_2018}. Instead of forecasting probabilistic model parameters, this model directly forecasts quantiles in a non-parametric fashion. However, it suffers from the quantile crossing problem: forecasted values may have ranks inconsistent with the quantile they are attached too. 
\cite{gasthaus_probabilistic_2019} is another alternative to DeepAR. Similarly to \cite{wen_multi-horizon_2018}, it does not rely on probability distribution outputs, and implements conditional quantile functions using regression splines instead. Spline parameters are fit using a neural network directly minimizing the Continuous Ranked Probability Score (CRPS) \cite{gneiting_calibrated_2005}, which is then used as a loss function. This results in a more flexible output distribution, and an alternative to other flexible schemes (e.g. mixture of distributions in the context of \cite{salinas_deepar_2020}). However, it currently\footnote{Checked on 21 June 2023} lacks a publicly available implementation.

Multivariate forecasting consists in modelling and forecasting multiple time series simultaneously, by contrast to univariate forecasting. The seminal way to achieve this, is with the Vector AutoRegression (VAR) model, which is an extension of the linear autoregressive model to multiple variables \cite{stock2016}. As this model has hard time dealing with many variables (e.g., items in the retail domain), neural network-based models such as DeepVAR were designed as an alternative \cite{salinas_high-dimensional_2019}. It can be thought of as a multivariate extension to \cite{salinas_deepar_2020}.
DeepVAR models interaction between time series, e.g., as resulting from causality or combined effects. It uses a Copula model, which models interactions between time series, and elegantly copes with time series of varying magnitude, alleviating the need for an explicit scaling mechanism. A single multivariate state variable underlying a LSTM model is used for all time series. 
They also define a low-rank parametrization, which opens the possibility to deal with a very large number of time series.

Some prior work involved deep learning in the context of PV power forecasting. For example, \cite{abdel-nasser_accurate_2019} consider one hour ahead forecasts (instead of intraday as aimed at in this paper). They used a single LSTM layer without the encoder-decoder mechanism, limited to point forecasts. Data for two PV sites is used for the experiments, with roughly the same power magnitude for both sites (approx. 3.5kW). Models are trained for each site separately. Alternatively, in this paper we address all sites with a single model in a scale free approach, dealing with an arbitrary number of sites with little to no model size overhead. Finally, the locations associated to the datasets are subject to a dry climate, which is simpler to predict \cite{pedro_assessment_2012}. Our application testbed is a temperate area, subject to frequent and abrupt changes on a daily basis, therefore much more challenging to predict.
\cite{wang_day-ahead_2020} also address PV power forecasting, by decorrelating scale free forecasts using LSTM, from seasonal effects modelled separately using time correlation features and partial daily pattern prediction. However, they focus on forecasts aggregated at a daily scale, when we consider hourly data in this paper. In addition, our approach is end-to-end, with seasonal effects modelled as time covariates, as allowed by the DeepAR model \cite{salinas_deepar_2020}.


%\subsection{Hybrid-physical approaches} \label{sec:hybrid-physical}

%\cite{barbounis06} focus on wind speed and power forecasting. They consider hourly forecasts for 72 hours ahead, using Numerical Weather Prediction (NWP) forecasts as additional inputs, thus proposing an early combination of observation data with NWP covariates. The recurrent model used then (diagonal recurrent neural networks \cite{ku95}) was superseded by alternative models such as LSTM \cite{hochreiter97} and GRU \cite{cho14} in the recent literature as seen in the previous section. 

\cite{thaker_comparative_2022} present a regional PV power forecasting system, with hourly resolution up to 72h ahead. Their approach combines clustering and numerical optimization, and it is compared to regression methods such as ElasticNet \cite{zou2005}, SARIMAX \cite{suhartono2017}, or Random Forests \cite{suhartono2017}.
Their approach is not autoregressive, rather they directly predict future PV power from solar irradiance and temperature forecasts obtained from a proprietary system which refines NWP forecasts according to local conditions. Alternatively, our approach tries to combine the benefits of using physical model forecasts (fed periodically by a NWP system) as covariates in an autoregressive model of the PV power time series. We observe that this effectively implements the \emph{hybrid-physical} approach, which combines the output of a physical model to a machine learning model \cite{antonanzas2016}. In an early work, Cao and Lin \cite{cao_study_2008} used NWP covariates in the context of a neural network to perform next hour and aggregated daily solar irradiance forecasting. More recently, several models allow to combine continuous covariates to each time slot in a forecasting interval \cite{wen_multi-horizon_2018,salinas_deepar_2020,rasul_autoregressive_2021,park_learning_2022}. The model which we describe in Section \ref{sec:model}, and the experiments in Section \ref{sec:experiments}, involve models from this family.



\subsection{Forecast structure and validation} \label{sec:validation}


Figure \ref{fig:rolling} distinguishes \emph{regular} forecasts from \emph{rolling} forecasts, which are the two main strategies to consider for extracting fixed sized blocks from time series \cite{benidis_deep_2023}. For simplicity, the figure considers hourly forecasts and 24-hour context and forecasting intervals, but the definition is straightforward to generalize.
In brief, two consecutive regular forecasts are offset by the size of the forecasting interval, when rolling forecasts are offset by the frequency of the time series (hourly, in Figure \ref{fig:rolling}). In other words, with 24-hour forecasting and context intervals, regular forecasts happen on the same time every day, whereas rolling forecasts are issued every hour for the whole forecasting interval. In this process, forecasts beyond the next hour are refreshed every hour.

\begin{figure}[h]
\centering
\includegraphics[width=.9\linewidth]{rolling_forecast.png}
\caption[Caption]{Distinction between regular and rolling forecasts. $t_0$ denotes the present time step in the context of a given data sample.}
\label{fig:rolling}
\end{figure}

Works such as \cite{thaker_comparative_2022} consider regular forecasts, as the forecast time is tied to availability of NWP data. 
As the physical covariates we use in our our work are also tied to forecasts issued from a NWP service, and the data we use in our experiments in Section \ref{sec:experiments} is only collected daily by the DSO, regular forecasts are a requirement in our work too. Alternatively, \cite{bruneau_bayesian_2012} address rolling forecasts by having a distinct model for each possible starting time in the day. 
Let us note that some models (e.g., \cite{salinas_deepar_2020}) allow to encode seasonal information on the predicted time steps (e.g. hour in day, day in week) as covariates (referred to as \emph{time covariates} later on). Therefore, they can be used indistinctively with regular and rolling forecasts, provided an adapted training set is available. Also, contrasting to \cite{wang_day-ahead_2020}, this means that modelling periodicity and seasonality explicitly is not needed, as the model then directly combines this additional input to time series data.


\cite{bergmeir_use_2012} discuss the problem of cross-validation, and more generally validation, in the context of time series forecasting. Original formulations of cross-validation methods often assume that data items are independent. They cannot be used out of the box with time series, as the sequential structure of the latter invalidates the underlying hypotheses. 
They recognize that the seminal way to validate models with time series is to train a model using the first $t_0$ values, and validate using the last $T-t_0$ values. However, this way is hardly compatible with cross-validation schemes, and yields weak test error estimations. 
In virtue of the bias-variance tradeoff \cite{geman_neural_1992}, this issue has moderate impact on models with strong bias such as linear models. However, over-parametrized models, such as most neural models presented in Section \ref{sec:ml} (e.g. \cite{salinas_deepar_2020,salinas_high-dimensional_2019,gasthaus_probabilistic_2019,wen_multi-horizon_2018}) can be significantly affected, and exhibit a strong tendency to overfit. 
For mitigation, \cite{bergmeir_use_2012} recommend blocked cross-validation, in which time series segments of size $\tau << T$ are used for independent training and validation for model selection and test error computation. As we also use deep learning as a building block in our approach, we carefully consider these recommendations in our experimental design (see Section \ref{sec:experiments}).



\section{Model Description} \label{sec:model}

The survey of related works in Section \ref{sec:related} led us to choose DeepAR \cite{salinas_deepar_2020} as a framework to develop our implementation. We adapted the official implementation of the model \cite{gluonts_jmlr} to suit our needs. Another model which offers the relevant flexibility as well as a public implementation is the model by Wen et al. \cite{wen_multi-horizon_2018}. We will compare to this model in our experimental section.

Alternatively, PV sites could have been considered as dimensions in a multivariate forecasting problem, thus possibly forecasting all sites at a given time at once using DeepVAR \cite{salinas_high-dimensional_2019}. However, its limitation to static covariates prevents us from implementing the projected approach, which uses forecasts from the physical model as covariates as means to build upon them.
Also, it is unclear how new PV sites, as well as missing values, which are pretty common as PV sites may witness independent breakdowns and interruptions of measurements or data communication (see Section \ref{sec:scheme} for a quantitative account), can be handled with such multivariate modelling. Alternatively, we choose to model the PV site using covariates in the context of a univariate model. We will test the value of these covariates in our ablation study in Section \ref{sec:experiments}.


\subsection{DeepAR model} \label{sec:deepar}

In the remainder of the paper, for clarity of the derivations, scalar variables are represented in normal font, vector variables in bold font, and matrix variables in capital bold font. This section \ref{sec:deepar} essentially paraphrases \cite{salinas_deepar_2020}, but ensures the present paper is self-contained, while introducing the necessary formalism in general terms.

Let us assume we have a data set of $N$ univariate time series, each with fixed size $T$. Each observed time series in $\{ \bold z_n \}_{n \in 1, \dots, N}$ may relate to an item in store in the retail context, or to a distinct PV system in the context addressed in this paper. 
$t_0 \in [1, \dots, T]$ denotes the present time, i.e. the latest time point for which we assume $z_{n,t}$ is known when issuing the forecast. $[1, \dots, t_0]$ is then the \emph{context} interval, and $[t_0+1, \dots, T]$ is the \emph{forecasting} interval. 
Let us note that each $t \in [1, \dots, T]$ is merely associated to the position of values within each sample $\bold z_n$; this is not inconsistent with the fact that $t$, in the context of two different samples $n$ and $n'$, may be tied with different \emph{absolute} points in time, e.g. referring to two different days if chunking the data as described in Section \ref{sec:scheme}.
The goal of the model is then to forecast $\bold z_{n, t_0+1:T} = [z_{n,t_0+1}, \dots, z_{n,T}]$ with the knowledge of $\bold z_{n, 0:t_0} = [z_{n,0}, \dots, z_{n,t_0}]$. We also consider a set of covariates $\bold X_{n,1:T} = [\bold x_{n,1}, \dots, \bold x_{n,T}]$ which are known for $t \in [1, \dots, T]$, even at time $t_0$.

\begin{figure}[h]
\centering
\includegraphics[width=.9\linewidth]{deepar-model.png}
\caption[Caption]{Illustration of the DeepAR model. Observed variables are represented as shaded boxes, and latent variables as blank boxes. For the context interval, $z$ variables are always known. For the forecasting interval, the model behaves differently at training and test time. At test time, $\tilde z$ variables are sampled according to $p$, forming sample paths. Plain lines represent dependencies between random variables, and the dashed line highlights the reinjected sample.}
\label{fig:deepar}
\end{figure}


In this context, the model is defined by the following product of likelihood factors, also summarized in Figure \ref{fig:deepar}:

\begin{align}
    Q_\Theta &= \prod_{n=1}^N \prod_{t=t_0+1}^T {q_\Theta(z_{n,t} \vert \bold z_{n,1:t-1}, \bold X_{n,1:T})} \nonumber \\
     &= \prod_{n=1}^N \prod_{t=t_0+1}^T { p(z_{n,t} \vert \theta(\bold h_{n,t}, \Theta)) }
    \label{eq:likelihood_model}
\end{align}

The model is both autoregressive and recurrent, as state variable:

\begin{equation}
    \bold h_{n,t} = \Theta(\bold h_{n,t-1}, z_{n,t-1}, \bold x_{n,t}) \label{eq:recursion}
\end{equation}

is obtained from LSTM model $\Theta$ in which the state variable and observation of the previous time step are both reinjected. The model also depends on parametrized function $\theta$, which learns the mapping between the state variable $\bold h$ and parameters of the probability distribution $p$. 

In effect, as seen in Figure \ref{fig:deepar}, at training time, $\bold z_n$ is known for all $t \in [1 \dots T]$, even the forecasting interval. The $z_{n,t}$ boxes on top of the diagram serve to compute losses then backpropagated to the model.
However at test time actual observations are not available for the forecasting interval, so we sample $\tilde z_{n,t} \sim p$, and inject them as proxy observations. Doing so yields sample paths, which can be repeated and serve to compute empirical quantiles in the forecasting interval, instead of simple point estimates. In this paper, when point estimates $\hat z_{n,t}$ are needed, we take them as the empirical median of a set of sample paths. In Figure \ref{fig:deepar}, we can see that the LSTM \emph{encodes} the context interval into $\bold h$, which is then \emph{decoded} for the forecasting interval. The same LSTM model $\Theta$ is used for encoding and decoding.

The negative log of expression \eqref{eq:likelihood_model} is used as the loss function for training all parameters in the model in an end-to-end fashion. The form of function $\theta$ depends on the probabilistic model in expression \eqref{eq:likelihood_model}: for example, if $p$ is chosen as a Gaussian, appropriate functions would be:

\begin{align}
    \theta_\mu(\bold h_{n,t}) &= \bold w_\mu \bold h_{n,t} + b_\mu \nonumber \\
    \theta_\sigma(\bold h_{n,t}) &= \log(1+\exp(\bold w_\sigma \bold h_{n,t} + b_\sigma)) \label{eq:sigmamap} 
\end{align}

We note that the softplus function in \eqref{eq:sigmamap} ensures $\sigma$ is mapped as a positive real number. Among possible probabilistic models and mapping functions, the official DeepAR implementation \cite{gluonts_jmlr}\footnote{\url{https://github.com/awslabs/gluonts}}, used in the experiments for this paper, features Gaussian, Student, negative binomial, and mixture distributions. The mixture distribution composes several distributions from the same nature using mixture weights, which have their dedicated $\theta$ function.

\subsection{Positive Gaussian likelihood model}

As PV power measurements are bound to be non-negative real numbers, a contribution of this paper is to allow for the Gaussian distribution to be truncated from below at 0 (the upper limit remaining $+\infty$), referred to as the \emph{positive Gaussian} distribution in the remainder of this paper. Formally this yields:

\begin{equation}
    p(z_{n,t} \vert \theta_\mu, \theta_\sigma) = \frac 1 {\sigma \sqrt{2\pi}} \frac {\exp{(-\frac 1 2 \frac{(z_{n,t} - \theta_\mu)^2}{{\theta_\sigma}^2})}}{1 - \Phi(-\frac {\theta_\mu} {\theta_\sigma})}
\end{equation}

With $\Phi$ the cumulative distribution function of the standard Gaussian (i.e., with mean 0 and standard deviation 1). Besides adapting the loss function (see Equation \eqref{eq:likelihood_model}) to this new probability distribution function, the same $\theta_\sigma$ function as the Gaussian distribution can be used. To make sure the range of $\theta_\mu$ is also positive, for the positive Gaussian we use:

\begin{equation}
    \theta_\mu(\bold h_{n,t}) = \log(1+\exp(\bold w_\mu \bold h_{n,t} + b_\mu)) \nonumber
\end{equation}

From an application of the Smirnov transformation \cite{devroye_nonuniform_2006} to the case at hand, samples from a positive Gaussian distribution can be obtained as:

\begin{equation}
    \tilde z = \Phi^{-1} \Biggl(\Phi \bigl(-\frac {\theta_\mu} {\theta_\sigma} \bigr) + \tilde u \biggl (1 - \Phi \bigl (-\frac {\theta_\mu} {\theta_\sigma} \bigr) \biggr )\Biggr) \theta_\sigma + \theta_\mu
\end{equation}

where $\tilde u$ is a uniform sample in $[0,1]$.

\section{Experiments} \label{sec:experiments}

\subsection{Data} \label{sec:data}


Section \ref{sec:model} presented the forecasting model underlying our experiments in general terms, but here we recall that we focus on a specific application and its peculiarities: forecasting the power output of a set of PV systems.

The variable to forecast ($z$ in Section \ref{sec:model}) is the average power output of a PV system during the hour to come in Watts. As hypothesized in Section \ref{sec:validation}, it is thus a hourly time series. For our experiments, we used data recorded by 119 PV systems located in Luxembourg between 01/01/2020 and 31/12/2021. They are dispatched in a relatively small (4 $\times$ 4 km) area. These PV systems are managed by Electris, a DSO in Luxembourg which collaborated with the authors of this paper in the context of a funded research project. Besides PV power measurements, each time step is associated to intraday, day-ahead, and 2 days ahead forecasts by the physical model described in \cite{koster_short-term_2019}. 
We add these tier forecasts to the set of covariates used by the model ($\bold X$ in Section \ref{sec:model}) along with time covariates mentioned in Section \ref{sec:validation}, as they will be available beforehand for the forecasting interval.

The model also supports the adjunction of \emph{static} covariates, which are constant for a given sample. Relating to Section \ref{sec:model}, we note that this simply amounts to set associated $\bold x_{n,1:T}$ to a constant. In the context of the present work, we consider a \emph{system ID} categorical feature, which is simply the system ID converted to a categorical feature with 119 modalities. We also consider \emph{system description} continuous features. Among the set of descriptors provided by system vendors and characteristics of their setup, we retain the following features as they are expected to influence PV power curves and magnitude: the \emph{exposition} of the system (in degrees), its \emph{inclination} (in degrees), its nominal \emph{power} (in Watts) and its \emph{calibration factor} (unitless, tied to the system on-site setup). As the DeepAR implementation expects normally distributed features, we standardize features so that they have zero mean and unit standard deviation.

As the nomimal power of our PV systems varies over a large range (from 1.4kW to 247kW), a scaling scheme is necessary to properly handle measured values. As mentioned in Section \ref{sec:related}, we address this as implemented in DeepAR by dividing all measurements in a given sample by $\frac 1 {t_0} \sum_1^{t_0} {\vert z_t \vert}$. Also, as physical model outputs are expected to be distributed similarly to their associated measurements, they are normalized likewise.



\subsection{Loss function and metrics} \label{sec:loss}


The sum of negative log-likelihoods of observations $z_{n,t}$ (on top of Figure \ref{fig:deepar}) is used as a loss function to fit all model parameters $\{\bold w_\mu, \bold w_\sigma, \Theta \}$ in an end-to-end fashion. 
As commonly done in the literature (see Section \ref{sec:ml}), we use a fixed size for the context and forecasting intervals in our experiments. As we are interested in intraday regular forecasts (see Section \ref{sec:validation}), with hourly data this means that the forecasting interval has size 24. 
Being tied to the ECMWF NWP service, the physical model forecasts for the 3 days to come are computed each day early in the morning, but before the sunrise, so while PV power is still obviously zero. For simplicity, we thus choose midnight as the reference time in day for the regular forecasts (i.e., $t_0$ in Section \ref{sec:model}). In this context, 24h, 48h and 72h physical covariates associated to predicted time step $t_0 + h$ will have been issued at time steps $t_0$, $t_0 - 24$ and $t_0 - 48$, respectively.

As the maximal horizon of the collected NWP forecasts is 72h, and we previously set the intraday forecasting interval as 24h, as a rule of thumb we used 48h as the context interval so that a training sample covers 72h. In practice, preliminary tests showed that using a larger context interval would not bring visible improvements, and using a multiple of the forecasting interval size facilitates the creation of train and test data sets.




To measure model performance, we consider metrics based on RMSE (Root-Mean-Square Error) and MAE (Mean Absolute Error).
The former are common in energy utility companies, notably as they penalize large errors \cite{lorenz2016}, and the latter is consistent with our model, as forecasts are obtained by taking the median of sample paths \cite{yang_summarizing_2022}. We define normalized versions of these metrics as:

\begin{align}
    \text{nRMSE}(\hat{\bold Z}, \bold Z) & = \sqrt{\sum_{n=1}^N {\frac 1 N \frac{\frac 1 {T-t_0} \sum_{t_0+1}^T (\hat z_{nt} - z_{nt})^2}{P_n^2}}} \label{eq:rmse} \\ 
    \text{nMAE}(\hat{\bold Z}, \bold Z) & = \sum_{n=1}^N {\frac 1 N \frac{\frac 1 {T-t_0} \sum_{t_0+1}^T | \hat z_{nt} - z_{nt} | }{P_n}} \label{eq:mae}
\end{align}


with $P_n$ the nominal power of PV system $n$, $\hat {\bold Z} = \{ \hat z_{nt} \}$ the estimated point forecast, and $\bold Z = \{ z_{nt} \}$ the observed power. The normalization in Equations \eqref{eq:rmse} and \eqref{eq:mae} allows to measure the performance of a point estimate forecast in such way that PV systems with larger nominal power do not dominate the error metric. This is a field requirement, as PV systems have private owners, who have to be treated equally, irrespective of the nominal power of their system. In practice, nMAE can be interpreted as a percentage of the PV system nominal power. For the sake of consistency with our model, we focus primarily on this metric, while also reporting nRMSE in Section \ref{sec:results}. To evaluate the performance of a proposed system w.r.t. a reference, the \emph{skill score} is derived from nMAE as:

\begin{equation}
    \text{Skill score} = 1 - \frac{\text{nMAE}(\hat{\bold Z}, \bold Z)}{\text{nMAE}(\hat{\bold Z}_\text{ref}, \bold Z)} \label{eq:skill}
\end{equation}


As presented in Section \ref{sec:model}, the models trained in the context of this work output prediction quantiles. We use the median as the point estimate forecast; in addition, we compute the CRPS metric \cite{gneiting_calibrated_2005}, commonly used in related work \cite{gasthaus_probabilistic_2019, salinas_deepar_2020, thaker_comparative_2022}, which rates the quality of prediction quantiles as a whole:

\begin{align}
    \text{CRPS}(F^{-1}, \bold Z) &= \frac 1 {N(T-t_0)} \sum_{n=1}^N \sum_{t=t_0+1}^T \int_0^1 {2 \Lambda_\alpha (F^{-1}(\alpha), z_{nt}) d\alpha} \label{eq:crps} \\
    \Lambda_\alpha(F^{-1}(\alpha), z_{nt})& = (\alpha - \mathcal I_{[z_{nt}<F^{-1}(\alpha)]})(z_{nt} - F^{-1}(\alpha)) \nonumber
\end{align}

with $F^{-1}$ the quantile function of the predictor (which returns the quantile level in Watts associated to a probability $\alpha \in ]0,1[$), $\Lambda_\alpha$ the \emph{quantile loss}, and $\mathcal I_{[\text{c}]}$ the indicator function associated to logical clause $c$. As discussed in Section \ref{sec:model}, the quantile function is estimated empirically using a set of sample paths $\{ \hat {\bold z}_n \}$. 
CRPS of course penalizes when observations are far from the median output by the model, but the penalty is increased if this difference is large while the model is excessively confident, as reflected, for example, by a narrow 90\% prediction interval (i.e., the interval between the 5\% and the 95\% quantiles). In other words, this allows to penalize models which are excessively confident, and somehow reward models which are able to better estimate the expected accuracy of their point forecast. 
In our experiments, we use 100 paths per sample, which can be used as input to return empirical quantiles. 

PV power is naturally zero at night time. Therefore, including these time steps in metric computation is likely to bias nMAE and CRPS towards 0. To prevent this, we exclude time steps associated to a solar zenith angle greater than $85^{o}$ \cite{lauret_benchmarking_2015} in Equation \eqref{eq:rmse}, \eqref{eq:mae} and \eqref{eq:crps}.



\subsection{Validation scheme} \label{sec:scheme}

Following recommendations by \cite{bergmeir_use_2012}, we create training samples by cutting the data set in fixed size 72h segments, with $t_0$ in each segment being midnight 24h before the end of the segment. Assuming we extracted the segment at the beginning of the available data, we then shift the offset 24h forward, so that the next segment includes the previous forecasting interval in its context interval (see, e.g., top of Figure \ref{fig:rolling}). As we treat all PV systems as independent time series, this results in $O(CD)$ series, with $C$ the number of PV systems and $D$ the number of values $t_0$ can take in the original time series according to the segmentation method defined above.

\begin{figure}[h]
\centering
\includegraphics[width=.9\linewidth]{missing_values.png}
\caption[Caption]{\emph{l.h.s.}: Proportion of missing values per system. \emph{r.h.s.}: Proportion of missing data per associated month.}
\label{fig:missing}
\end{figure}

Our number of PV systems and temporal collection bounds would yield 86989 samples. However, PV systems may exhibit missing or erroneous measurements due to several reasons (e.g., power outage, bad manipulation, faulty sensor). Figure \ref{fig:missing} summarizes how missing values are distributed in the data set. The l.h.s. of Figure \ref{fig:missing} shows that missing values are not uniformly distributed across PV systems. Approximately one third has no missing value, another third has a bit less than 20\% of missing values, and the last third between 25\% and 50\%. The under-representation of this last third can be problematic. The r.h.s. of Figure \ref{fig:missing} shows that these missing values are not evenly distributed in time: this indicates that a group of systems may have been offline for a contiguous time frame during late winter and spring. Actually, most missing values are linked to systems started later than the others in year 2020. The associated periods are therefore under-represented, but we note that any month has at most 30\% missing data.
In the remainder, we consider that this bias remains in a range which makes uniform sampling w.r.t. time acceptable for building training batches. In order to facilitate processing, and as samples cuts are aligned with day frames, we detect and exclude days matching one of the following patterns: more than two consecutive missing values, measurements blocked to a constant value, visual inspection for aberrant values. This results in 67666 valid day frames.

The PV systems are distributed in a relatively small area in Luxembourg: therefore, it is expected that forecasting intervals for different systems but same absolute time attached to $t_0$ will be highly correlated. In order to validate this intuition, we computed all $D$ intraday correlation matrices between systems in our data set. Specifically, we defined intraday time steps (excluding nightly time steps) as observations and PV systems as variables, resulting in $D \frac {C(C-1)} 2$ distinct correlation values. We observe that the median of the distribution of these correlation values is 0.95, which confirms a very high correlation between systems for a given day. As a consequence, sampling uniformly training, validation and test sets in the $O(CD)$ series would result in \emph{data leakage}, i.e., the model will be able to overfit without harming test error as identical samples (up to scale) will be scattered in training, validation and test sets. Let us note an unexpected positive benefit of this strong intraday correlation: the under-representation of some systems is then much less problematic. The only remaining issue would pertain to estimating the parameters associated to the static categorical modalities of these systems, if using system ID static covariates. We hypothesise that at least 50\% of represented day frames is sufficient to perform this estimation. 


To prevent the data leakage problem, we first group the samples by the absolute time attached to their respective $t_0$, and sample 60\% of the $D$ time steps as the training set. We use a static temporal pattern, in order to ensure that each month and season is represented fairly evenly. Validation and test sets are uniformly sampled as half of the remaining 40\% irrespective of the PV system, which is not an issue as the goal of validation error is to be an estimate of the test error, provided parameters are not explicitly fitted on the former. The validation set is used to implement early stopping, and select the model before it starts to overfit. The test set serves to compute the metrics described in Section \ref{sec:loss}. To choose the cut between validation and test, and ensure validation error is a fair proxy of the test error, we resample cuts until the validation and test nMAE between ground truth and intraday physical model forecasts (which are considered as constant and known in advance, and are the most relevant baseline to compare to) are equal up to a small threshold. Using the cutting procedure defined so far, we obtain 40670 training, 13498 validation and 13498 test samples.

\subsection{Hyper-parameters} \label{sec:hyperparams}

The models were trained using the Adam optimizer with learning rate $10^{-3}$, batch size 64, for 200 epochs. Samples are reshuffled at the beginning of each epoch. In the end, we implement a form of early stopping, by selecting the model with best validation nMAE. DeepAR uses 2 LSTM layers by default, we stick to this parametrization. Two free parameters remain then: the LSTM hidden layer size, and the number of components when a mixture distribution output is used. Figure \ref{fig:hyperparameters} shows the results of a hyper-parameter search of these parameters using the positive Gaussian distribution as mixture components, as it is the distribution used in the model yielding the best results in Table \ref{tab:results}. Median results from 6 independently trained models are displayed in the graphs, along with $\pm 1$ standard deviation error bars. 
We first use 100 as an initial guess of the hidden layer size, and vary the number of components used by model 1 (see Table \ref{tab:results}, and Figure \ref{fig:hyperparameters}a). Except with only one component, the performance is good for all number of components, and best for 5 components. Then, using 5 components, we test hidden layer sizes between 20 and 500. Results are significantly better with size 40 (Figure \ref{fig:hyperparameters}b). Finally, we try to refine the number of components using hidden layer size 40, and excluding models with only one component as this setting was visibly irrelevant. Using 5 or 6 components stands out (Figure \ref{fig:hyperparameters}c); to give a bonus to parsimony, we retain 5 components for the remainder of the experiments.


\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{hyperparameters2.png}
\caption[Caption]{\emph{a)} nMAE results for variable numbers of positive Gaussian components with hidden layer size 100. \emph{b)} nMAE results for variable hidden layer sizes with 5 components. \emph{c)} Refinement of the number of components with hidden layer size 40.}
\label{fig:hyperparameters}
\end{figure}


\subsection{Ablation study}

Our best model configuration (model 1 in Table \ref{tab:results}) uses the positive Gaussian distribution combined to the physical model forecasts and the PV system ID as covariates. 
We first tested replacing the positive Gaussian distribution with the standard Gaussian (model 2) and Student distributions (model 3). 
We also considered continuous PV system description covariates, instead of the categorical system ID (model 4). The advantage of continuous PV system description covariates is to open the possibility to include new systems without having to learn the model again, which can be highly valuable in production systems. We also test using no PV system covariates at all (model 5), and using only one positive Gaussian component instead of a mixture (model 6). We also consider not using physical model covariates (model 7). This can be of practical interest, as access to ECMWF solar irradiance forecasts, used by the physical model as inputs, is free for research purpose, but requires a subscription for industrial applications.
Finally, we tested alternative models: MQCNN \cite{wen_multi-horizon_2018} (model 8) and FFN \cite{bebis_feed-forward_1994} (model 9). Physical and system ID covariates were used when possible (with MQCNN, thus comparing to model 1), and ignored otherwise (with FFN, thus rather comparing to model 7).



\subsection{Results and interpretation} \label{sec:results}

Results are given in Table \ref{tab:results}. 
In \cite{koster_single-site_nodate}, skill scores are computed using a 24h persistence model, adjusted according to the clear sky PV power for the day under consideration. This is a common baseline in the solar energy domain \cite{pedro_assessment_2012}. In this paper, we rather consider that the physical model is the baseline against which our results have to be evaluated. So we use 24h physical model covariates as $\hat{\bold Z}_\text{ref}$ in Equation \eqref{eq:skill} for computing skill scores presented in Table \ref{tab:results}. This is a stronger baseline for skill score computation, as it was shown to significantly outperform persistence forecasts \cite{koster_short-term_2019}. In this experimental section, this means that a model with a skill score lower than 0 is not able to beat the covariates it is given among its inputs.

\begin{table}[t]
%\scriptsize
    \centering
    \begin{tabular}{ l l l l l l }
    \textbf{ID} & \textbf{Description} & \textbf{nRMSE (\%)} & \textbf{nMAE (\%)} & \textbf{Skill (\%)} & \textbf{CRPS (-)} \\
    \hline
    \multicolumn{6}{c}{\emph{Baseline}} \\
    \hline
    \cellcolor{gray!10}- & \cellcolor{gray!10}Physical model & \cellcolor{gray!10}11.396 & \cellcolor{gray!10} 7.958 & \cellcolor{gray!10}- & \cellcolor{gray!10}- \\
    \hline
    \multicolumn{6}{c}{\emph{Models}} \\
    \hline
        \cellcolor{gray!10}1 & \cellcolor{gray!10}\textbf{Best model} & \cellcolor{gray!10}$\bold{10.808(\pm 0.082)}$ & \cellcolor{gray!10}$\bold{6.707(\pm 0.069)}$ & \cellcolor{gray!10}\textbf{15.72} & \cellcolor{gray!10}$\bold{4.816(\pm 0.048)}$ \\
        2 & w/ Gaussian & $10.950(\pm 0.031)$ & $6.844(\pm 0.070)$ & 14.00 & $4.897(\pm 0.048)$ \\ 
        \cellcolor{gray!10}3 & \cellcolor{gray!10}w/ Student & \cellcolor{gray!10}$10.928(\pm 0.082)$ & \cellcolor{gray!10}$6.841(\pm 0.047)$ & \cellcolor{gray!10}14.04 & \cellcolor{gray!10}$4.916(\pm 0.039)$ \\
        4 & wo/ system ID & $11.127(\pm 0.102)$ & $6.966(\pm 0.049)$ & 12.47 & $5.007(\pm 0.038)$ \\
        \cellcolor{gray!10}5 & \cellcolor{gray!10}wo/ system & \cellcolor{gray!10}$11.461(\pm 0.100)$ & \cellcolor{gray!10}$7.292(\pm 0.067)$ & \cellcolor{gray!10}8.37 &  \cellcolor{gray!10}$5.278(\pm 0.050)$ \\
        & wo/ mixture, &  &  &  &  \\
          \multirow{-2}{*}{6} & wo/ system & \multirow{-2}{*}{$11.547(\pm 0.042)$} & \multirow{-2}{*}{$7.711(\pm 0.017)$} & \multirow{-2}{*}{3.10} & \multirow{-2}{*}{$5.676(\pm 0.024)$} \\
        \cellcolor{gray!10}7 & \cellcolor{gray!10}wo/ physical & \cellcolor{gray!10}$14.203(\pm 0.106)$ & \cellcolor{gray!10}$9.333(\pm 0.053)$ & \cellcolor{gray!10}-17.28 & \cellcolor{gray!10}$6.577(\pm 0.032)$ \\
		8 & MQCNN \cite{wen_multi-horizon_2018} & $11.040(\pm 0.101)$ & $7.249(\pm 0.031)$ & 8.91 & $5.259(\pm 0.024)$ \\
        \cellcolor{gray!10}& \cellcolor{gray!10}FFN \cite{bebis_feed-forward_1994} & \cellcolor{gray!10} & \cellcolor{gray!10} & \cellcolor{gray!10}& \cellcolor{gray!10} \\
          \cellcolor{gray!10} & \cellcolor{gray!10}wo/ system, & \cellcolor{gray!10} & \cellcolor{gray!10} & \cellcolor{gray!10} & \cellcolor{gray!10} \\
          \multirow{-3}{*}{\cellcolor{gray!10}9} & \cellcolor{gray!10}wo/ physical & \multirow{-3}{*}{\cellcolor{gray!10}$14.687(\pm 0.069)$} & \multirow{-3}{*}{\cellcolor{gray!10}$10.087(\pm 0.033)$} & \multirow{-3}{*}{\cellcolor{gray!10}-26.75} & \multirow{-3}{*}{\cellcolor{gray!10}$7.522(\pm 0.037)$} \\
    \end{tabular}
    \caption{nRMSE, nMAE and CRPS test metrics for the range of compared models. The results of the best performing model are bold-faced. Median results and standard deviations are estimated from 6 models trained independently. \emph{w/} and \emph{wo/} stand for \emph{with} and \emph{without}, respectively.}
    \label{tab:results}
\end{table}


% noting that variance is much lower with proposed solution
% Raw FFN not much better than LSTM, but the latter allows to use covariates effectively
% but significant CRPS degradation


First focusing on the nMAE metric through skill scores, we see that using the positive Gaussian component has a small but significant influence, with skill scores improved by 1.72 and 1.68 points with respect to the Gaussian and Student components, respectively. The performance of model 4 using continuous PV system description features also yields solid performance, with 2.95 points score degradation compared to model 1. 
We note that the relatively better performance brought by using the system ID as a covariate provides anecdotal evidence supporting the hypothesis formulated in Section \ref{sec:scheme} regarding the imbalance of systems representation in the dataset.
The first significant performance gap comes with model 5, which ignores all PV system covariates. Its score is 4.10 points lower than model 4. Then, as could be anticipated from Section \ref{sec:hyperparams} on hyper-parameters optimization, using a single positive Gaussian component degrades the performance of model 5 by 5.27 additional points. Finally, the largest gap is observed when ignoring the physical model covariates, while keeping the same configuration as model 1 otherwise, with a negative skill score of -17.28\%. This shows that the neural model alone is not able to compete with the physical model, and implicitly validates the relevance of the \emph{hybrid-physical} approach, which aims at building upon physical models with machine learning techniques \cite{antonanzas2016}. 

Eventually, using MQCNN yields a positive skill score, but degraded by 6.81 points with respect to model 1. Also, using FFN degrades the results of model 7, its closest variant in our ablation study in terms of available input data, by 9.47 points. This validates our design based on DeepAR and our positive Gaussian component.



\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{mae-vs-rmse.png}
\caption[Caption]{Illustration of the relationship between nMAE and nRMSE metrics \emph{(a)}, also magnified after excluding outlying models 7 and 9 from the representation \emph{(b)}. \emph{B} stands for the baseline physical model. Red lines are the result of a linear regression of the points in the graph, reported along with Pearson and Spearman correlation coefficients.}
\label{fig:mae-vs-rmse}
\end{figure}

Figure \ref{fig:mae-vs-rmse} illustrates the relationships between nMAE and nRMSE. First we see that the two models which do not use physical model covariates are outlying (Figure \ref{fig:mae-vs-rmse}a), and thus have an important leverage on any correlation analysis. Excluding them indeed has a significant effect on the regression line and computed coefficients (Figure \ref{fig:mae-vs-rmse}b), even if the correlation remains high and significant. Models below the regression line tend to have their nRMSE lower than would be expected according to their nMAE. All models using the system ID (1, 2, 3 and 8) are in this situation, as well as the physical model baseline. This means that in some production context which would favor nRMSE as prime metric, using the system ID as a covariate is preferable. As a side note, we also computed the analysis shown in Figure \ref{fig:mae-vs-rmse} between nMAE and CRPS metrics; all correlation coefficients were then highly significant and beyond 0.96, even when excluding outlying models 7 and 9. This means that in our experiments, nMAE and CRPS were almost perfectly correlated. Reporting specifically about CRPS would therefore not bring significant added value.


As alternative designs, we considered using unscaled physical model covariates (i.e. not scaling them along with $\bold z$ values as described in Section \ref{sec:data}), and using the weighted sampling scheme described in \cite{salinas_deepar_2020}, which samples training examples with frequency proportional to the magnitude of $\bold z$ values. We did not report results with these alternative designs as they brought systematic degradation to the performance. We also tried to use both types of system description covariates (i.e., system ID and continuous description features) simultaneously, but this led to a slight degradation compared to the respective model using only the system ID covariate. This is expected, as the system ID alone already encodes system diversity. In addition, as we will see in the next section, continuous system description features may reflect the system setup in a biased way, which could explain for the slight degradation in performance observed in Table \ref{tab:results}.


\section{Discussion and qualitative examples}

We first emphasize that the contributions highlighted in the introduction are distributed throughout the paper, not only in terms of theoretical contributions in Section \ref{sec:model}, which are arguably modest, as we position our work as an application, but also in our analysis of the related work, notably positioning hybrid-physical approaches as a special case in the context of neural-network based time series forecasting in Section \ref{sec:ml}. 
We also highlight practical contributions on data preprocessing, training and validation procedures in Sections \ref{sec:ml} and \ref{sec:scheme} which will enable the transfer of the work to the DSO.


In the previous section we evaluated our proposed models using global metrics. In this section, we aim at providing more detailed insight into our results by analyzing model performance at the system level. The displayed examples were obtained using the best performing model (i.e. model 1 in Table \ref{tab:results}). First we compute per-sample nMAE metrics for the test set, group them according to their associated system ID, and rank the systems according to the difference between model 1 and the physical model nMAE. In other words, the higher a system is in this ranking, the model 1 outperforms the physical model for this system. For all but 1 system over 119, model 1 performs better than the baseline. We first consider the worst case system 68.


\begin{figure}[h]
\centering
\includegraphics[width=.9\linewidth]{worst_case.png}
\caption[Caption]{Two test samples associated to system 68. Prediction intervals are displayed using green shades. The observations and 24h physical model forecasts of the context interval are prepended.}
\label{fig:worst-case}
\end{figure}


\begin{figure}[h]
\centering
\includegraphics[width=.9\linewidth]{best_case.png}
\caption[Caption]{Two test samples associated to system 44. Prediction intervals are displayed using green shades. The observations and 24h physical model forecasts of the context interval are prepended.}
\label{fig:best-case}
\end{figure}

On the l.h.s. of Figure \ref{fig:worst-case}, we display the sample of this system among the lowest nMAE with the DeepAR model. This is a typical clear day example, where the prediction is fairly easy for the neural model. We note that for this instance, forecasts stick more closely to the observations curve than the baseline. The prediction intervals are naturally tight, reflecting the high confidence of the neural model in this case. On the r.h.s. of Figure \ref{fig:worst-case}, for the same system, we display a sample for which the difference between the two models is among the largest. In this case, DeepAR is not able to keep up with the sudden peak of PV power. The 24h physical model covariate somehow informed on this peak, but this information was not used by model 1, which acted conservatively regarding the observations in the context interval.

In Figure \ref{fig:best-case}, we consider samples from system 44. This system is the third best of our ranking, and has been identified as problematic for the physical model because of a double-pitched roof, not reflected by the system description features \cite{koster_single-site_nodate}. On both sides of Figure \ref{fig:best-case}, the systematic shift of the PV performance model is clearly visible. We also see that model 1 is able to completely ignore this shift, and come up with sensible forecasts. The figure also shows how the prediction intervals are tighter when the PV production is stable through days (l.h.s.), and broader when the weather seems more unstable (r.h.s.).


\section{Conclusion}

Eventually, we are able to improve power forecasts obtained from an already strong physical PV performance model. With an ablation study and model comparisons, our experiments highlight the best working configuration, which uses the PV performance model forecasts as covariates, a mixture of positive Gaussians as output distribution, and a static categorical covariate reflecting the associated system ID. The positive Gaussian output allows to deal effectively with the \emph{bell-shaped} data profile typical of solar energy applications, and the system ID feature allows to model local effects which went previously unnoticed with the physical PV performance model alone. 

In future work, we plan to refine and explore novel neural model designs. For example, quantile regression methods more recent than \cite{wen_multi-horizon_2018} will be explored. Also, we will further investigate how to deal with novel systems being added to the grid without having to retrain the full model. We saw that using system description features is an effective fallback, but these features do not account for local effects such as a double-pitched roof, so they remain suboptimal. We will also consider longer forecasting intervals (e.g. day-ahead and 2 days ahead).

Despite visible success, models trained for this work tended to overfit, and relied critically on early stopping. This is mostly due to our measures taken to prevent data leakage: when segmenting 2 years of data at the day scale, despite all our measures, training and test sets are unlikely to be identically distributed. We addressed this problem in the most straightforward and conservative way, but it seems related to the domain shift problem characterized by the domain adaptation literature \cite{redko2019}. Adapting contributions from this area to the peculiarities of our application is left for future work.


\subsection{Acknowledgements}

This research was funded by the Luxembourg National Research Fund (FNR) in the framework of the FNR BRIDGES Project \emph{CombiCast} with grant number \newline (BRIDGES18/IS/12705349/Combi-Cast). Furthermore, the authors would like to thank our partner Electris (a brand of Hoffmann Frres Energie et Bois s. r.l.), for their trust, the very supportive partnership throughout the whole project duration, and their contribution to the common project, financially as well as in terms of manpower and data.



\subsection{Data Availability}

The datasets analysed during the current study are not publicly available due to being the property of Electris. A sample may be provided by contacting the corresponding author on reasonable request, without any guarantees.

\subsection{Declarations}

\subsubsection{Conflicts of interest} 

All authors declare that they have no conflicts of interest.


\bibliography{refs}

\end{document}
