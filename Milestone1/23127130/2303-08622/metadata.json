{
    "arxiv_id": "2303.08622",
    "paper_title": "Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer",
    "authors": [
        "Serin Yang",
        "Hyunmin Hwang",
        "Jong Chul Ye"
    ],
    "submission_date": "2023-03-15",
    "revised_dates": [
        "2023-03-16"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "stat.ML"
    ],
    "abstract": "Diffusion models have shown great promise in text-guided image style transfer, but there is a trade-off between style transformation and content preservation due to their stochastic nature. Existing methods require computationally expensive fine-tuning of diffusion models or additional neural network. To address this, here we propose a zero-shot contrastive loss for diffusion models that doesn't require additional fine-tuning or auxiliary networks. By leveraging patch-wise contrastive loss between generated samples and original image embeddings in the pre-trained diffusion model, our method can generate images with the same semantic content as the source image in a zero-shot manner. Our approach outperforms existing methods while preserving content and requiring no additional training, not only for image style transfer but also for image-to-image translation and manipulation. Our experimental results validate the effectiveness of our proposed method.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08622v1"
    ],
    "publication_venue": null
}