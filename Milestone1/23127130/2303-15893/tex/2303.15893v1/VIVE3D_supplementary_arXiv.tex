% CVPR 2022 Paper Template
\documentclass[10pt,twocolumn,letterpaper,table]{article}
\pdfoutput=1

\usepackage[table]{xcolor}
%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version #[review]
\usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage[accsupp]{axessibility}
\usepackage{xr}

\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
% Include other packages here, before hyperref.

\usepackage{comment}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{xurl} % allow line breaks anywhere in URL string
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{color}
\usepackage{bm}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{array}
\usepackage{booktabs}
\usepackage{nth}
\usepackage{nicefrac}
\usepackage{relsize}
\usepackage{xspace} 
\usepackage{bm}
\usepackage{enumerate}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{array}
\usepackage{cuted}
\usepackage{capt-of}
\usepackage[accsupp]{axessibility}
\usepackage{booktabs}
\usepackage{overpic}
\usepackage{cellspace, tabularx}
\usepackage[T1]{fontenc}
\usepackage{dblfloatfix}
%\usepackage[svgnames,table]{xcolor}
	
%\definecolor{mediumgray}{gray}{0.4}
\definecolor{superlightgray}{gray}{0.92}
\definecolor{lightgray}{gray}{0.8}
\definecolor{mediumgray}{RGB}{160,170,180}
\definecolor{azul}{RGB}{230,240,250}
\newcommand{\cc}{\cellcolor{azul}}
\newcommand{\gr}{\cellcolor{superlightgray}}

\usepackage{tcolorbox}
\newtcbox{\inlinecode}{on line, boxrule=0pt, boxsep=0pt, top=2pt, left=2.5pt, bottom=2pt, right=2.5pt, colback=lightgray!15, colframe=gray, fontupper={\ttfamily \footnotesize}}

\newcommand{\anna}[1]{\textcolor{violet}{{\scriptsize~[\textbf{Anna}:~#1]}}} 
\newcommand{\nikos}[1]{\textcolor{orange}{{\scriptsize~[\textbf{Nikos}:~#1]}}} 
\newcommand{\yuanlu}[1]{\textcolor{blue}{{\scriptsize~[\textbf{Yuanlu}:~#1]}}} 
\newcommand{\tony}[1]{\textcolor{red}{{\scriptsize~[\textbf{Tony}:~#1]}}} 
\newcommand{\peter}[1]{\textcolor{green}{{\scriptsize~[\textbf{Peter}:~#1]}}} 


%reduces the height of equation lines
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}

%stretches cell height of table cells because padding was removed
\renewcommand{\arraystretch}{1.15}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


% TODO insert Paper ID!
\def\cvprPaperID{6370}
\def\confName{CVPR}
\def\confYear{2023}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DEFINE MACROS FOR MATHEMATICAL SYMBOLS FOR CONSISTENT USE IN PAPER
\newcommand{\name}{VIVE3D\xspace}
\newcommand{\SIT}{StiiT\xspace}
\newcommand{\VEG}{VEG\xspace}
\renewcommand{\vec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\generator}[1]{\ensuremath{\mathcal{G}_{#1}}} %generator function
\newcommand{\gtuned}{\ensuremath{\generator{\mtxt{ID}}}\xspace} %fine-tuned generator
\newcommand{\mtxt}[1]{\text{\textit{#1}}} % output text nicely in math mode
\newcommand{\w}{\vec{w}\xspace} % latent w
\newcommand{\wperson}{\ensuremath{\vec{w}_\mtxt{ID}}\xspace} % default person W
\newcommand{\idxS}{\mtxt{n}\xspace} % index used to indicate n start faces
\newcommand{\idxF}{\mtxt{f}\xspace} % index used to indicate frame-by-frame indices
\newcommand{\source}{\ensuremath{S}\xspace} % source frame
\newcommand{\face}[1]{\ensuremath{\vec{F}_{#1}}\xspace} % faces 
\newcommand{\facelr}[1]{\ensuremath{{\vec{F}_{128}}_{#1}}\xspace} % low resolution faces 
\newcommand{\seg}[2]{\ensuremath{\vec{S}_{\mtxt{#1}}({#2})}\xspace} % segmentation
\newcommand{\down}[2]{\ensuremath{\vec{D}_{\mtxt{#1}}({#2})}\xspace} % segmentation
\newcommand{\offset}[1]{\ensuremath{\vec{o}_{#1}}\xspace} % offset per person
\newcommand{\loss}[1]{\ensuremath{\mathcal{L}_{\mtxt{#1}}}\xspace}
\newcommand{\weight}[1]{\ensuremath{\lambda_{\mtxt{#1}}}\xspace}
\newcommand{\y}[1]{\ensuremath{\mtxt{yaw}_{#1}}\xspace} %yaw
\newcommand{\cam}[1]{\ensuremath{c_{#1}}\xspace} %yaw
\newcommand{\p}[1]{\ensuremath{\mtxt{pitch}_{#1}}\xspace} %pitch
\newcommand{\edit}{\mtxt{edit}\xspace} % edit, used to denote edited frames

\newcommand{\figlabel}[1]{\textbf{(#1)}}

\newcommand\rot{\rotatebox{90}}
\newcommand\mr[2]{\multirow{#1}{*}{\scriptsize{#2}}}
\newcommand\mc[2]{\multicolumn{#1}{c}{#2}}
\newcommand\tss[1]{\scriptsize\textsc{#1}}

\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}


%----Helper code for dealing with external references----
% (by cyberSingularity at http://tex.stackexchange.com/a/69832/226)

\usepackage{xr}
\makeatletter

\newcommand*{\addFileDependency}[1]{% argument=file name and extension
\typeout{(#1)}% latexmk will find this if $recorder=0
% however, in that case, it will ignore #1 if it is a .aux or 
% .pdf file etc and it exists! If it doesn't exist, it will appear 
% in the list of dependents regardless)
%
% Write the following if you want it to appear in \listfiles 
% --- although not really necessary and latexmk doesn't use this
%
\@addtofilelist{#1}
%
% latexmk will find this message if #1 doesn't exist (yet)
\IfFileExists{#1}{}{\typeout{No file #1.}}
}\makeatother

\newcommand*{\myexternaldocument}[1]{%
\externaldocument{#1}%
\addFileDependency{#1.tex}%
\addFileDependency{#1.aux}%
}
%------------End of helper code--------------

% put all the external documents here!
%\myexternaldocument{VIVE3D_main_arXiv}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE
\title{\name: Viewpoint-Independent Video Editing using 3D-Aware GANs \\ \textsc{Supplementary Materials}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AUTHOR LIST
\author{
    Anna Fr\"{u}hst\"{u}ck\textsuperscript{{2}}, 
    Nikolaos Sarafianos\textsuperscript{1},
    Yuanlu Xu\textsuperscript{1}, 
    Peter Wonka\textsuperscript{2}, 
    Tony Tung\textsuperscript{1}
\\[0.6ex]
	\textsuperscript{1~}Meta Reality Labs Research, Sausalito\quad
        \textsuperscript{2~}KAUST \quad \\ 
        \small{\tt\href{https://afruehstueck.github.io/vive3D}{afruehstueck.github.io/vive3D}}
}

% TEASER FIGURE 
% Workaround to place teaser figure on top of CVPR template
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\vspace*{-.5in}
\begin{center}
  \centering
  \captionsetup{type=figure}
    \includegraphics[width=\linewidth]{figures/suppl_face_change}
   \caption{\textbf{\name generalization to new identities}. The benefits from decomposing the inversion of the input into an identity latent and a set of offsets unlock applications of face/motion re-targeting with minimal effort. This is possible due to our novel personalized generator that can be trained on a specific person's identity and then applied to edit an unseen video. We show two examples: In example \figlabel{a}, we use a person \textit{(top row)} to invert and fine-tune the generator, and we determine the video offsets based on this video sequence. The bottom row determines the target frames as well as the face location and angles. For example \figlabel{b}, we use a personalized generator \textit{(top left)}, but the target frames, angles, as well as motion, stem from a distinct video, driving the motion of the target person.
    }
   \label{fig:head-change}
\end{center}%
}]

\begin{figure*}[t]
\centering

\end{figure*}

\section{Additional Results}
\subsection{Supplementary Video}
Please see our supplementary video (on the \href{https://afruehstueck.github.io/vive3D}{project webpage}) for video sequences illustrating our proposed method and a set of results demonstrating the unique capabilities of our technique as well as comparisons to related methods.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/suppl_ablation}
   \caption{\textbf{\name Ablation Study.} Our proposed approach \name \textit{(\(2^{nd}\) column)} with all the proposed components demonstrates better identity preservation, fixes the spatial misalignment between the face and neck when rendered from a novel view and better captures the fine-level details of the face and results in high-fidelity faithful renders of the person from new views. 
    }
   \label{fig:ablation-images}
   \vspace{-1.5em}
\end{figure*}


\subsection{Experimental Edits}
We showcase some additional experimental edits to illustrate the generalization abilities of our approach for general-purpose video editing. For example, we are able to use two completely disjoint videos of different subjects and achieve reasonable results at compositing them.
We show in \Cref{fig:head-change} two instances of such applications: On the left (\Cref{fig:head-change} (a)), we use one personalized Generator with its "default" person latent \wperson, and a stack of video offsets encoding a sequence of face motions. We then compose these with a different body by running our inset optimization, using the target video frames and head angles from that particular video.
On the right (\Cref{fig:head-change} (b)), we use the encoded face motion from the target video, projecting the motion onto a different person's face, thereby essentially replacing the head in the target video. 
Note that in order to achieve plausible results for these instances, we need to copy head and neck due to the slight differences in lighting between the source and target faces. Further, the segmentation masks need to be considered carefully and be big enough, \eg in the right result, the hair sticking out from the source person's head needs to be covered by inpainting with background color during the inset optimization in order to achieve a reasonable result. Otherwise, the optimization will add extra hair and change the hairstyle.
We observe that \name generates realistic results of placing the first person's head on the second person's body that are temporally and spatially consistent and follow the target frame motion. This is possible because of our proposed design (personalized generator, separate identity, and offset latents) which makes \name unique compared to prior work. It is worth noting that when the source and target videos have different light conditions then the results might have different lighting between the body and the face. 
This is because we do not explicitly tackle this problem in our architecture and hence lighting is baked in the final generated/edited head before it is placed on top of the new body. We identify the problem of better lighting transfer as an avenue for future work.



\subsection{Ablation Study}
To evaluate the impact of each module we conduct ablation studies and report our quantitative and qualitative results in Table 4 of the main paper and \Cref{fig:ablation-images} of the supplementary respectively. Given a video of a person talking \textit{(\(1^{st}\) column) }, we demonstrate our complete approach when rendering the output video from a new viewpoint \textit{(\(1^{st}\) column)}. 
In the next 4 columns of results, we strip one component at a time and observe different performance quality drops. For example, if we do not fine-tune the generator it is clear that the identity of the individual is not properly preserved \textit{(\(3^{rd}\) column)}. If we remove the flow correction module which is a key contribution of our approach, we observe that the face and the neck are not well aligned which makes the results seem unnatural \textit{(\(4^{th}\) column)}. The impact of the flow correction module is demonstrated also in \Cref{fig:camera-problem_sup} and discussed in detail in the supplementary video. 
If we strip the regularization \textit{(\(5^{th}\) column)}, we remove the joint latent \wperson and treat each target face in the initial inversion separately. This means we don't constrain the individual latents to stay close to the common latent. We can see that this leads to a deterioration in the inversion quality of the video frames and produced artifacts, as the inverted latents no longer share information, and there is no constraint on the projected location in latent space. Finally, if we were to only perform single-frame inversion rather than multi-frame, we also observe a significant drop in fidelity \textit{(\(6^{th}\) column)}, which indicates that the proposed approach of performing multi-frame fidelity is beneficial as it better captures the identity and the fine details of the face. 


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/flow_illustration_2}
   \vspace{-6mm}
   \caption{\textbf{View Adjustment Additional Results.} 
   We illustrate the problem arising when attempting to composite a changed view of a person's head on top of the original body. 
   After cropping \figlabel{a} and inversion \figlabel{b}, we perform face editing \figlabel{c} and change the camera viewpoint to an unseen angle \figlabel{d}. Replacing the face in the original frame with this edit yields poor quality \textit{(bottom center)} even for small angular changes because the rotated face is in the wrong location with respect to the body. We address this by estimating the optical flow \figlabel{e} between the face crop and the edit and use the flow direction to correct the location of the reference face based on the prospective inset \figlabel{f}. This allows us to composite the edited face into the frame in a natural-looking fashion \textit{(bottom right)}.
   }
   \label{fig:camera-problem_sup}
   \vspace{-4mm}
\end{figure}

\subsection{Qualitative Results of Method Comparisons}
We provide a qualitative comparison to related methods of GAN-based video editing, Stitch it in Time (\SIT)~\cite{Tzaban2022STIT} and VideoEditGAN (\VEG)~\cite{Xu2022VideoEditGAN}. 
First, we discuss some of the differences between our proposed method and the related work to establish the parameters of our comparison.
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/suppl_method_comparison_ages}
    \vspace{-1.6em}
   \caption{\textbf{Comparison to related work on age editing.} Due to the distinctly different InterfaceGAN editing directions, we handpicked the edit strength for the respective latent space edits to showcase a similar effect in aging the target person. Our technique yields results that are at least on par with the previous StyleGAN2-based editing techniques \textit{(bottom two rows)}. %All frames are center cropped. Not sure why we need that (Nikos)
   \label{fig:methods-age-comparison_sup}
   }
   \vspace{-1.3em}
\end{figure}

\begin{itemize}[leftmargin=*]
\itemsep0em 
    \item \SIT and \VEG, which is closely related to \SIT, both use a StyleGAN2 backbone which outputs high quality images at 1024$\times$1024px resolution, whereas our backbone's (EG3D) output resolution is 512$\times$512px (obtained by super-resolution given 128$\times$128px inputs), providing 3D-awareness at the expense of slightly inferior image quality to classic StyleGAN2. In order to compare quantitatively, we downsample all results generated by \SIT and \VEG to 512$\times$512px, unless we compare at the full video resolution, in which case the output of the respective generator is already resampled to fit the resolution of the original face crop in the video frame.
    \item \SIT and \VEG rely on prior work for a reliable encoding framework, e4e~\cite{richardson2021encoding}, to yield good and coherent inversions, whereas we implemented an optimization strategy to obtain per-frame inversions. Implementing an encoding strategy for EG3D was outside the scope of our project, but would be an interesting topic for future endeavors. We expect that using an encoder would lower the embedding quality, but improve computation speeds.
    \item \SIT and \VEG fine-tune their generator on all video frames simultaneously, thus achieving very good coherence to the input. In contrast, we fine-tune on a select few target faces, yielding a more generalizable generator, which, in consequence, is not optimized to replicate the video frame-by-frame. 
    \item Like our approach, \SIT relies on InterfaceGAN~\cite{Shen2020InterFaceGAN} for discovering and applying latent space editing directions for many of their results. \VEG shows their results using edits based on StyleClip~\cite{Patashnik2021StyleClip}. To compare with their method, we adapted their code to also allow edits with InterfaceGAN -- analogous to \SIT -- before applying their temporal consistency strategy. Note that the discovered directions in StyleGAN2 and EG3D latent space do not yield identical results for the same attribute type and the strength needed to apply the direction vectors is different. We empirically chose weights to approximate the same edit strength when comparing results.
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/suppl_method_comparison_angles}
    \vspace{-1.6em}
   \caption{\textbf{Comparison to related work on angle editing.} A slight change in angle is achieved for the related methods by applying a yaw-changing latent space direction. Both methods fail at producing a reasonable composition given these edits \textit{(bottom two rows)}. We implement a similar angle change and demonstrate that our method creates a natural-looking composition. 
   }
   \label{fig:methods-angle-comparison}
   \vspace{-1.3em}
\end{figure}

\begin{figure}[b]
  \captionsetup{type=figure}
  \centering
  \vspace{-0.6em}
  \caption{\textbf{Comparison to another 3D inversion technique.} To demonstrate the effectiveness of our Personalized Generator inversion, we compare the quality of our inversion with a recent technique of 3D GAN inversion~\cite{Ko20233D}. Note how the head shape and identity correspondence deteriorate when rotating the head away from the original pose.}
  \vspace{-0.6em}
  \includegraphics[width=\linewidth]{figures/inversion_comparison_john}
  \label{fig:inversion-comparison}
   \vspace{-1.3em}
\end{figure}

We demonstrate in \Cref{fig:methods-age-comparison_sup} that all methods provide plausible results for classic semantic editing problems such as aging the target person. However, both related methods fail to yield plausible results for angle editing. In order to compare this task, we utilize a latent space direction discovered in StyleGAN2 that allows for slight angle changes. The comparison for these strategies is illustrated in \Cref{fig:methods-angle-comparison}.
The artifacts present in these qualitative results mirror the deterioration in quantitative scores indicated in Table 2 in the main paper.


In \Cref{fig:inversion-comparison}, we show a result of our multi-target inversion strategy (row \figlabel{(a)}) compared to another single-image 3D GAN inversion\cite{Ko20233D} (row \figlabel{(b)}). Please zoom in to observe the degradation of head shape and loss of identity when the head rotation diverges from the source image.

\begin{table}[t]
\arrayrulecolor{mediumgray}
\footnotesize
\setlength{\tabcolsep}{4pt}
\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}
\centering
\caption{\textbf{Reconstruction Metrics.} We compare the quality of our inversion with \SIT using reconstruction metrics on a subset of videos. We also evaluate the Fr\'echet Inception Distance~(FID) of inversion and edits with respect to the source video.}
\resizebox{\columnwidth}{!}{
\arrayrulecolor{mediumgray}
\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}
\begin{tabular}{l|l|cc|ccc}
      \multicolumn{2}{c|}{~}          &\multicolumn{2}{c|}{\gr\textbf{Reconstruction Quality}}&\mc{3}{\gr\textbf{Editing Quality}}                                     \\
    %\toprule
      \multicolumn{2}{c|}{~}          &\textbf{PSNR}~$\uparrow$&\textbf{SSIM}~$\uparrow$ & \mc{3}{\textbf{Fr\'echet Inception Distance (FID)}~$\downarrow$}            \\    
                      &\textbf{Method}&\multicolumn{2}{c|}{\tss{Inversion}}              & \tss{Inversion}         & \tss{Age Edit}          & \tss{Angle Edit}        \\
    \midrule
    \arrayrulecolor{lightgray}
    \mr{2}{\textbf{Marques}}
                          & \SIT      &        \textbf{36.477}  &                  0.965 &                  10.11 &                   18.44 &                 21.58 \\
                          & \cc \name &\cc             33.791   &\cc       \textbf{0.987}& \cc     \textbf{~~7.25}&\cc        \textbf{12.73}&\cc      \textbf{11.63}\\  
    \cmidrule{1-7}
    %  \rot{\rlap{~Marques}}
    \mr{2}{\textbf{Obama}}
                          & \SIT      &                34.969  &           \textbf{0.976} &        \textbf{~~3.80}&                   16.49 &                 17.12 \\
                          & \cc \name &\cc     \textbf{36.282} &\cc                0.969  & \cc            ~~3.88 &\cc       \textbf{~~8.67}&\cc     \textbf{~~7.22}\\ 
    \cmidrule{1-7}
    \mr{2}{\textbf{Dennis}}
                          & \SIT      &                40.708  &           \textbf{0.993} &                ~~6.32 &                   12.88 &                 16.96 \\
                          & \cc \name &\cc     \textbf{40.804} &\cc                0.990  & \cc    \textbf{~~4.07}&\cc       \textbf{~~8.36}&\cc     \textbf{~~8.07}\\
    \arrayrulecolor{mediumgray}
    \bottomrule
\end{tabular}
}
\label{tab:face-metrics-suppl}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/editing_directions_900dpi}
   %\vspace{-6mm}
   \caption{\textbf{InterfaceGAN Edits Additional Results.} 
   We show InterfaceGAN editing directions discovered in the latent space of the pretrained 3D GAN by applying them to our personalized generator. The attribute edits are plausible and consistent in 3D.
   }
   \label{fig:editing-directions_sup}
   % \vspace{-2mm}
\end{figure}

\subsection{Additional Quantitative and Qualitative Results}
We provide some additional quantitative metrics on individual videos shown throughout this supplementary material. In \Cref{tab:face-metrics-suppl}, we analyze the reconstruction quality and editing quality for three individual videos, showing that our reconstruction and editing capabilities are on par with our main competitor technique \SIT for video inversion and editing tasks. We also analyze the inversion and editing performance of VIVE3D and related methods for two distinct videos in more detail in \Cref{tab:face-similarity-suppl}. We use the ArcFace~\cite{Deng2019ArcFace} metric to calculate the minimum, maximum, and average similarity to the source video as well as the temporal difference by evaluating the metric on adjacent video frames. The quantitative scores show that our method is superior for both attribute and angular edits, the latter being a task at which the previous 2D-GAN-based methods fail.

\begin{table}[t]
\arrayrulecolor{mediumgray}
\scriptsize
\setlength{\tabcolsep}{4pt}
\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}
\centering
\caption{\textbf{Face Similarity Metrics.} We evaluate the identity preservation of inversion and edits based on the cosine similarity of ArcFace features extracted from generated face crops with respect to the face crops of the source video. To evaluate coherence over time, we measure the dissimilarity between consecutive frames.
}
\vspace{-1em}
\resizebox{\columnwidth}{!}{
    \begin{tabular}{@{}c|r|l|ccc|ccc}
    \toprule         
     &                 &               &\multicolumn{3}{c|}{\gr\textbf{Similarity to Source}~$\uparrow$}& \mc{3}{\gr\textbf{Temporal Diff}~$\downarrow$}     \\   
     &                 &\textbf{Method}&\tss{min} &\tss{max} &        \tss{mean} &\tss{min}&\tss{max}&      \tss{mean} \\
    \midrule
    \arrayrulecolor{lightgray}
      \mr{9}{\rotatebox{90}{\textbf{Marques}}}
     & \mr{3}{Inversion}  & e4e        &    0.487 &    0.816 &            0.663  &    0.1 &    36.3 &            5.6   \\
     &                    & \SIT       &    0.720 &    0.877 &            0.820  &    0.1 &   ~~8.6 &    \textbf{1.3}  \\
     &                    & \cc \name  &\cc 0.820 &\cc 0.932 &\cc \textbf{0.894} &\cc 0.1 &\cc~~7.5 &\cc         1.4   \\
      \cmidrule{2-9}
     & \mr{3}{Age Edit}   & \SIT       &    0.720 &    0.877 &            0.820  &    0.1 &   ~~8.8 &            1.3   \\
     &                    & \VEG       &    0.430 &    0.668 &            0.551  &    0.1 &    11.5 &            1.9   \\
     &                    & \cc \name  &\cc 0.730 &\cc 0.923 &\cc \textbf{0.857} &\cc 0.1 &\cc~~5.2 &\cc \textbf{1.1}  \\
      \cmidrule{2-9}
     & \mr{3}{Angle Edit} & \SIT       &    0.654 &    0.801 &            0.740  &    0.1 &   ~~8.9 &            1.4   \\
     &                    & \VEG       &    0.424 &    0.685 &            0.568  &    0.1 &    12.1 &            1.7   \\
     &                    & \cc \name  &\cc 0.762 &\cc 0.899 &\cc \textbf{0.849} &\cc 0.1 &\cc~~8.1 &\cc \textbf{1.1}  \\
      \midrule
      \mr{9}{\rotatebox{90}{\textbf{Obama}}} 
     & \mr{3}{Inversion}  & e4e        &    0.469 &    0.801 &            0.665  &    0.1 &    44.8 &            5.9   \\
     &                    & \SIT       &    0.935 &    0.982 &    \textbf{0.968} &    0.0 &   ~~7.6 &            1.0   \\
     &                    & \cc \name  &\cc 0.882 &\cc 0.961 &\cc         0.930  &\cc 0.0 &\cc~~2.1 &\cc \textbf{0.5}  \\
      \cmidrule{2-9}
     & \mr{3}{Age Edit}   & \SIT       &    0.717 &    0.862 &            0.781  &    0.1 &   ~~6.1 &            1.0   \\
     &                    & \VEG       &    0.522 &    0.763 &            0.671  &    0.2 &   ~~7.1 &            1.5   \\
     &                    & \cc \name  &\cc 0.758 &\cc 0.903 &\cc \textbf{0.850} &\cc 0.1 &\cc~~4.0 &\cc \textbf{0.8}  \\
      \cmidrule{2-9}
     & \mr{3}{Angle Edit} & \SIT       &    0.668 &    0.827 &            0.753  &    0.0 &   ~~7.8 &            1.3   \\
     &                    & \VEG       &    0.771 &    0.874 &            0.840  &    1.0 &   ~~5.9 &            1.2   \\
     &                    & \cc \name  &\cc 0.782 &\cc 0.916 &\cc \textbf{0.868} &\cc 0.1 &\cc~~4.2 &\cc \textbf{0.9}  \\
    \arrayrulecolor{mediumgray}
    \bottomrule
    \end{tabular}
}
\vspace{-0.4cm}
\label{tab:face-similarity-suppl}
\end{table}

We showcase some supplemental qualitative results that demonstrate in further examples that VIVE3D is able to (1) apply existing latent space editing techniques such as InterfaceGAN~\cite{Shen2020InterFaceGAN} to generate natural-looking results (\Cref{fig:editing-directions_sup}) with performance comparable to previous 2D techniques, (2) create plausible image compositions for diverging from the original head angle, generalizing to various camera viewpoints given a source frame (\Cref{fig:view-directions_sup}), (3) generate high-quality results that are temporally consistent for combined angle and attribute editing (\Cref{fig:teaser_sup}), and (4) synthesize spatially consistent results (\Cref{fig:stitch-boundaries_sup}) even for challenging boundary cases.


\begin{figure}[b]
\centering
\includegraphics[width=\linewidth]{figures/directions}
   \vspace{-6mm}
   \caption{\textbf{Changing Camera Poses Additional Results.} Our method can freely alter the camera pose and composite the result back with the source frame by fixing the divergence between the source and target pose using our optical flow correction. The generated results look natural despite the static body pose.
   }
   \label{fig:view-directions_sup}
\end{figure}

\begin{figure*}[t]
  \captionsetup{type=figure}
  \centering
  \caption{\textbf{Additional Qualitative Results.} Given a video sequence, we process individual frames, cropping the face region to correspond with our generator's field of view. VIVE3D faithfully modifies several facial attributes as well as the camera viewpoint of the head crop. Finally, we seamlessly composite the edited face with the source frame in a temporally and spatially consistent manner, while retaining a plausible composition with the static components of the frame outside of the generator's region. The dotted squares in the center frame denote the reference regions for the three different camera poses in the column below.}
  \vspace{-0.5em}
  \includegraphics[width=\linewidth]{figures/vivian_teaser_suppl} 
  \label{fig:teaser_sup}
  \vspace{-1em}
\end{figure*}


\section{Optimization Details and Parameter Settings}
Our approach is implemented in Python 3.8 and uses PyTorch. We build our approach on the pretrained models and the publicly available codebase of EG3D~\cite{eg3d_code, Chan2021EG3D}. During our pipeline, we propose several optimization steps. Each of them is relying on ADAM as an optimizer. We run all experiments on a single NVIDIA A100 GPU and provide timings and hyperparameters for our various pipeline steps.
\begin{enumerate}[leftmargin=*]
\itemsep-0.1em 
\item{
    \textbf{Generator Inversion}: 
    For the initial inversion of the generator, we use a standard learning rate scheduler. We also ramp down the regularization weight of \offset{\idxF} from \inlinecode{\weight{wdist}} to \inlinecode{\weight{wdist\_target}}.\\
    \textit{Optimization hyperparameters}\\ 
    \inlinecode{\weight{L1} = 0.05},
    \inlinecode{\weight{face} = 1.0},
    \inlinecode{\weight{LPIPS} = 0.75},\\
    \inlinecode{\weight{wdist} = 0.05},
    \inlinecode{\weight{wdist\_target} = 0.005},\\
    \inlinecode{initial\_learning\_rate = 1e-2},
    \inlinecode{num\_steps = 600}\\
    \textit{Duration} 3~min 33~sec (5 target images)
}
\item{
    \textbf{Generator Fine-Tuning}: 
    We fine-tune the weights of the StyleGAN2 backbone of EG3D as well as the neural renderer, leaving learned weights of the Upsampling module untouched.\\
    \textit{Optimization hyperparameters}\\ 
    \inlinecode{\weight{L1} = 1.0}, 
    \inlinecode{\weight{LPIPS} = 0.3}, 
    \inlinecode{learning\_rate = 1e-3},\\ 
    \inlinecode{num\_steps = 300}\\
    \textit{Duration} 3~min 27~sec (5 target images)
}

\item{
    \textbf{Video Inversion}:
    During this optimization, we run a frame-per-frame inversion, starting from the average offset of all offsets \offset{\idxF} discovered in step 1. Using this strategy, we invert the first frame for \inlinecode{init\_num\_steps}. Each consecutive frame is started from the previous offset and optimized for \inlinecode{num\_steps}. We provide an early stopping criterion \inlinecode{loss\_threshold} and finish the current frame optimization in case the total loss falls below this threshold.\\
    \textit{Optimization hyperparameters}\\ 
    \inlinecode{\weight{L1} = 0.25},
    \inlinecode{\weight{face} = 1.2},
    \inlinecode{\weight{LPIPS} = 1.0},\\
    \inlinecode{\weight{wdist} = 0.01},
    \inlinecode{learning\_rate = 1e-2},\\
    \inlinecode{loss\_threshold = 0.25},\\
    \inlinecode{init\_num\_steps = 200}, 
    \inlinecode{num\_steps = 50}\\
    \textit{Duration} 19~sec (first frame), \mytilde4~sec/frame (consecutive frames)
}
\item{
    \textbf{Optical Flow Evaluation}:
    During this step, we evaluate the flow between the source face crop and the (angle-edited) target face to estimate the correction of the source crop needed to achieve a plausible inset composition.\\
    To estimate the flow, we use Farneb\"{a}ck optical flow with the following parameters:
    \inlinecode{pyr\_scale = 0.5},
    \inlinecode{levels = 8},
    \inlinecode{winsize = 25},\\
    \inlinecode{iterations = 7}, 
    \inlinecode{poly\_n = 5}, 
    \inlinecode{poly\_sigma = 1.2}\\
    \textit{Duration} \mytilde0.4~sec/frame
}
\item{
    \textbf{Inset Optimization}:
    In this optimization step, we can specify sizes \inlinecode{border\_size} for the width of the segmentation boundary region that is optimized and \inlinecode{edge\_size} to provide an offset distance for the border from the image boundary. We can again specify an early stopping criterion \inlinecode{border\_loss\_threshold} to stop when the border loss falls under this threshold, which in practice provides significant speedup.\\
    \textit{Optimization hyperparameters}\\ 
    \inlinecode{weight\_foreground = 1.0},
    \inlinecode{weight\_border = 2.0},\\
    \inlinecode{edge\_size = 50},
    \inlinecode{border\_size = 50},\\
    \inlinecode{num\_steps = 150},
    \inlinecode{learning\_rate = 1e-2},\\
    \inlinecode{border\_loss\_threshold = 0.05}\\
    \textit{Duration} \mytilde4~sec/frame (\mytilde16~sec/frame w/o early stopping)
}
\end{enumerate}


\section{Social Impact}
The ability to provide editability/customization in videos of humans has been an active area of research over the past few years. On one hand, it can have key applications in providing people the ability to express themselves in different ways (\eg, the ability to change hair color, add glasses, etc) during video calls, or more broadly in how they interact in the digital world. At the same time, such techniques introduce use cases for potentially malicious use that are worth discussing. For example, the ability to replace someone's face in a video from another person resembles deepfakes and could be used by bad actors. While the results such as what is shown in Figure~\ref{fig:head-change} are still not at the level that would be perceived as indistinguishable from an original video this is an important conversation to be had regardless. We encourage the interested reader to refer to concurrent work~\cite{dolhansky2020deepfake, guarnera2020deepfake, nguyen2022deep, agarwal2019protecting} for deep fake detection to discover edited videos. 


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/border_cases_2} 
   \vspace{-6mm}
   \caption{\textbf{Spatial Consistency Additional Results.} \name composites images with challenging boundaries such as long hair \textit{(left)}, yielding faithful hair color change results. For hard boundary cases, such as matching with a static piece of hair outside the boundary crop \textit{(right)}, it plausibly connects the contents of the two images.}
   \label{fig:stitch-boundaries_sup}
   \vspace{-4mm}
\end{figure}


\bibliographystyle{ieee_fullname}
\bibliography{bibliography}
\end{document}
