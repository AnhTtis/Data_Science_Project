% CVPR 2022 Paper Template
\documentclass[10pt,twocolumn,letterpaper,table]{article}
\pdfoutput=1

\usepackage[table]{xcolor}
%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[pagenumbers]{cvpr}              % To produce the CAMERA-READY version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage[accsupp]{axessibility}

\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
% Include other packages here, before hyperref.

\usepackage{comment}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{xurl} % allow line breaks anywhere in URL string
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{color}
\usepackage{bm}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{array}
\usepackage{booktabs}
\usepackage{nth}
\usepackage{nicefrac}
\usepackage{relsize}
\usepackage{xspace} 
\usepackage{bm}
\usepackage{enumerate}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{array}
\usepackage{cuted}
\usepackage{capt-of}
\usepackage[accsupp]{axessibility}
\usepackage{booktabs}
\usepackage{overpic}
\usepackage{cellspace, tabularx}
\usepackage{dcolumn}
\usepackage{xr}
\newcolumntype{G}{D{,}{,}{2.1}}

\definecolor{superlightgray}{gray}{0.92}
\definecolor{lightgray}{gray}{0.8}
\definecolor{mediumgray}{RGB}{160,170,180}
\definecolor{azul}{RGB}{230,240,250}
\newcommand{\cc}{\cellcolor{azul}}
\newcommand{\gr}{\cellcolor{superlightgray}}

\usepackage{tcolorbox}
\newtcbox{\inlinecode}{on line, boxrule=0pt, boxsep=0pt, top=1pt, left=2pt, bottom=1pt, right=2pt, colback=lightgray!15, colframe=gray, fontupper={\ttfamily \footnotesize}}

% \newcommand{\anna}[1]{\textcolor{violet}{{\scriptsize~[\textbf{Anna}:~#1]}}} 
% \newcommand{\nikos}[1]{\textcolor{orange}{{\scriptsize~[\textbf{Nikos}:~#1]}}} 
% \newcommand{\yuanlu}[1]{\textcolor{blue}{{\scriptsize~[\textbf{Yuanlu}:~#1]}}} 
% \newcommand{\tony}[1]{\textcolor{red}{{\scriptsize~[\textbf{Tony}:~#1]}}} 
% \newcommand{\peter}[1]{\textcolor{green}{{\scriptsize~[\textbf{Peter}:~#1]}}} 

%reduces the height of equation lines
\setlength{\abovedisplayskip}{4pt} \setlength{\abovedisplayshortskip}{4pt}
\setlength{\belowdisplayskip}{4pt} \setlength{\belowdisplayshortskip}{4pt}

%stretches cell height of table cells because padding was removed
\renewcommand{\arraystretch}{1.15}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

% TODO insert Paper ID!
\def\cvprPaperID{6370}
\def\confName{CVPR}
\def\confYear{2023}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DEFINE MACROS FOR MATHEMATICAL SYMBOLS FOR CONSISTENT USE IN PAPER
\newcommand{\name}{VIVE3D\xspace}
\newcommand{\SIT}{StiiT\xspace}
\newcommand{\VEG}{VEG\xspace}
\renewcommand{\vec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\generator}[1]{\ensuremath{\mathcal{G}_{#1}}} %generator function
\newcommand{\gtuned}{\ensuremath{\generator{\mtxt{ID}}}\xspace} %fine-tuned generator
\newcommand{\mtxt}[1]{\text{\textit{#1}}} % output text nicely in math mode
\newcommand{\w}{\vec{w}\xspace} % latent w
\newcommand{\wperson}{\ensuremath{\vec{w}_\mtxt{ID}}\xspace} % default person W
\newcommand{\idxS}{\mtxt{n}\xspace} % index used to indicate n start faces
\newcommand{\idxF}{\mtxt{f}\xspace} % index used to indicate frame-by-frame indices
\newcommand{\source}{\ensuremath{S}\xspace} % source frame
\newcommand{\face}[1]{\ensuremath{\vec{F}_{#1}}\xspace} % faces 
\newcommand{\facelr}[1]{\ensuremath{{\vec{F}_{128}}_{#1}}\xspace} % low resolution faces 
\newcommand{\seg}[2]{\ensuremath{\vec{S}_{\mtxt{#1}}({#2})}\xspace} % segmentation
\newcommand{\down}[2]{\ensuremath{\vec{D}_{\mtxt{#1}}({#2})}\xspace} % segmentation
\newcommand{\offset}[1]{\ensuremath{\vec{o}_{#1}}\xspace} % offset per person
\newcommand{\loss}[1]{\ensuremath{\mathcal{L}_{\mtxt{#1}}}\xspace}
\newcommand{\weight}[1]{\ensuremath{\lambda_{\mtxt{#1}}}\xspace}
\newcommand{\y}[1]{\ensuremath{\mtxt{yaw}_{#1}}\xspace} %yaw
\newcommand{\cam}[1]{\ensuremath{c_{#1}}\xspace} %yaw
\newcommand{\p}[1]{\ensuremath{\mtxt{pitch}_{#1}}\xspace} %pitch
\newcommand{\edit}{\mtxt{edit}\xspace} % edit, used to denote edited frames

\newcommand{\figlabel}[1]{\textbf{(#1)}}

\newcommand\rot{\rotatebox{90}}
\newcommand\mr[2]{\multirow{#1}{*}{#2}}
\newcommand\mc[2]{\multicolumn{#1}{c}{#2}}
\newcommand\tss[1]{\textsc{#1}}
\newcommand\mins{{\scriptsize m~}}
\newcommand\secs{{\scriptsize s}}
\newcommand\pgraph[1]{\noindent\textbf{{#1}.}}

\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE
\title{\name: Viewpoint-Independent Video Editing using 3D-Aware GANs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AUTHOR LIST
\author{
    Anna Fr\"{u}hst\"{u}ck\textsuperscript{{2*}}, 
    Nikolaos Sarafianos\textsuperscript{1},
    Yuanlu Xu\textsuperscript{1}, 
    Peter Wonka\textsuperscript{2}, 
    Tony Tung\textsuperscript{1}
\\[0.4ex]
	\textsuperscript{1~}Meta Reality Labs Research, Sausalito \quad
        \textsuperscript{2~}KAUST \quad \\ 
        \small{\tt\href{https://afruehstueck.github.io/vive3D}{afruehstueck.github.io/vive3D}}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
\setlength{\belowcaptionskip}{-5pt}

% TEASER FIGURE 
% Workaround to place teaser figure on top of CVPR template
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\vspace*{-.5in}
\maketitle
\vspace*{-.4in}
\begin{center}
  \centering
  \captionsetup{type=figure}
  \includegraphics[width=0.85\textwidth]{figures/teaser}
  \vspace*{-.07in}
  \caption{We propose \textbf{\name}, a novel method that creates a powerful personalized 3D-aware generator using a low number of selected images of a target person. 
  Given a new video of that person, we can faithfully modify several facial attributes as well as the camera viewpoint of the head crop. Finally, we seamlessly composite the edited face with the source frame in a temporally and spatially consistent manner, while retaining a plausible composition with the static components of the frame outside of the generator's region. The dotted squares in the center frame denote the reference regions for the three different camera poses in the column below.}
  \label{fig:teaser}
\end{center}%
}]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
\begin{abstract}
We introduce \name, a novel approach that extends the capabilities of image-based 3D GANs to video editing and is able to represent the input video in an identity-preserving and temporally consistent way.
We propose two new building blocks.
First, we introduce a novel GAN inversion technique specifically tailored to 3D GANs by jointly embedding multiple frames and optimizing for the camera parameters.
Second, besides traditional semantic face edits (\eg for age and expression), we are the first to demonstrate edits that show novel views of the head enabled by the inherent properties of 3D GANs and our optical flow-guided compositing technique to combine the head with the background video.
Our experiments demonstrate that \name generates high-fidelity face edits at consistent quality from a range of camera viewpoints which are composited with the original video in a temporally and spatially consistent manner. 

\blfootnote{*This work was conducted during an internship at Meta RL Research.}
\end{abstract}
% END ABSTRACT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION

\section{Introduction}
\vspace{-1mm}
\label{sec:intro}
Semantic image editing has been an active research topic for the past few years. Previous work~\cite{Goodfellow2014GAN} uses Generative Adversarial Networks (GANs) to produce high-fidelity results in the image space. The most popular backbone is StyleGAN~\cite{Karras2020StyleGAN2, Karras2019StyleGAN, Karras2020StyleGAN2ADA, Karras2021StyleGAN3} as it generates high-resolution domain-specific images while providing a disentangled latent space that can be utilized for editing operations.
To edit real photographs, there are typically two steps: The first step maps the input image to the latent space of a pre-trained generator. This is usually accomplished either through encoder-based embedding or through optimization, such that generator can accurately reconstruct the image from the latent code~\cite{xia2022gan}. The second step is semantic image manipulation, where one latent input representation is mapped to another to obtain a certain attribute edit, (\eg changing age, facial expression, glasses, or hairstyle).
While existing approaches produce impressive results on single images, extending them to videos is far from straightforward. Among the challenges that arise are: (1) people tend to move their heads freely in videos (instead of assuming frontal image inputs), (2) the inversion of multiple frames should be coordinated, (3) the inverted face and edits need to be temporally consistent and (4) the compositing of the edited face with the original frame must maintain boundary consistency. 

A recent set of approaches has focused on 3D-aware GANs where a 2D face generator is combined with a neural renderer. Given a latent code, a 2D image and the underlying 3D geometry are generated, thus allowing for some camera movement while rendering the head of the person.

In this paper, we tackle the problem of viewpoint-independent face editing in videos. The edited face is rendered from novel views in a temporally-consistent manner. Specifically, we use a 3D-aware GAN in the temporal domain and apply facial image editing techniques per frame that are temporally smooth regardless of the rendered view.
Compared with other GAN-based video editing approaches~\cite{Tzaban2022STIT, Alaluf2022Times}, our method is the first to perform viewpoint-independent video editing while showing the full upper body of the person in the video with high fidelity.

\name takes a video of a person captured from a monocular camera as input. The captured person can move freely across time, talk, and make facial expressions while their body can be visible. Unlike all prior work that learns a generator and performs edits on the exact same video, we disentangle these steps. Hence the output of our approach can be a different video of the same person or the same video. In both cases, the face has undergone one or more attribute edits and is rendered from a novel view. To accomplish this challenging task, we introduce several novel components, each addressing one challenge of the problem at hand. Specifically, we first propose a simple yet effective technique to create a personalized generator by inverting multiple frames at the same time. The simultaneous inversion of \(N\) frames exposes the generator to a variety of facial poses and expressions, which results in a larger capacity that we can then utilize. Our generator can generalize to new unseen videos of the same identity where the person might be wearing a different shirt, a result not demonstrated in the literature so far. In addition, we propose to optimize the camera pose of the 3D-aware GAN during inversion to obtain an accurate estimate which angle the face was captured from. Finally, we introduce an optical flow-based compositing method to properly place the novel view of the edited face back into the original frame while ensuring that the end result is temporally and spatially consistent. 
Our experimental work provides a wide range of qualitative and quantitative results to demonstrate that \name accomplishes semantic video editing with changing camera poses in a faithful way. In summary, our contributions are: 

\begin{itemize}[leftmargin=*]
\itemsep-0.1em 
    \item A new 3D GAN inversion technique that jointly embeds multiple images while optimizing for their camera poses.
    \item A complete attribute editing framework and an optical flow-based compositing technique to replace the edited face in the original video.
    \item \name is the first 3D GAN-based video editing method and the first that can change the camera pose of the face.
\end{itemize}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/pipeline_900dpi}
   \vspace{-6mm}
   \caption{\textbf{\name Pipeline.} 
    To create an edited video, we first need to create a personalized generator by jointly inverting selected faces and fine-tuning a pre-trained generator. We then invert the cropped face regions from a source video (which could be the same or a different video) into our personalized generator and recover the latent codes and camera poses for each target frame. We are able to perform semantic editing on the inverted stack of latent codes using previously discovered latent space directions and we can freely change the camera path around the face region. In order to composite the face with the source frame in a consistent fashion, we use optical flow to correct the position of the inset within the frame, which allows us to composite the result in a seamless and temporally consistent fashion. 
    }
   \label{fig:pipeline}
   \vspace{-4mm}
\end{figure*}
% END INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RELATED WORK
\vspace{-0.45cm}
\section{Related Work}\label{sec:related_work}
\vspace{-0.1cm}
\pgraph{GAN Inversion}
GANs are a powerful tool for semantic editing. Most editing techniques are tailored to StyleGAN, the state-of-the-art of {2D} GANs~\cite{Karras2019StyleGAN, Karras2020StyleGAN2, Karras2020StyleGAN2ADA, Karras2021StyleGAN3}. Several editing techniques~\cite{Fruehstueck2022InsetGAN, Fruehstueck2019TileGAN,karakas2022fairstyle, chaudhuri2021semi, parmar2022spatially} build upon StyleGAN as it uses an intermediate disentangled latent space, usually referred to as \w-space.
Before editing, a latent space representation of the input image has to be recovered using a process typically referred to as Inversion or Projection~\cite{Creswell2018Inverting, Abdal2019Image2StyleGAN, Abdal2020Image2StyleGAN++, Wang2022CVPR}. Refer to~\cite{Xia2022InversionSurvey} for a survey of inversion techniques. In contrast to optimization-based inversion techniques, learning-based approaches attempt to obtain faster latent space correspondences by training encoders~\cite{richardson2021encoding, tov2021designing, alaluf2021restyle}. 
In order to retain the generalization ability of the \w-space while providing a high-quality inversion, Pivotal Tuning~\cite{Roich2022PTI} has successfully shown that trained generators can overfit to target images while still maintaining a navigable latent space.
Recent works study 3D GAN inversion~\cite{Lin20223DGANInversion, Ko20233D}, attempting to infer a 3D representation for a reference image.

\pgraph{GAN-based Latent Space Editing}
Once an appropriate latent space representation of an input image has been recovered, semantic edits can be applied by navigating the latent space manifold surrounding the inverted latent code. Unsupervised techniques attempt to find interesting edits without labeled data~\cite{Jahanian2019Steerability, Harkonen2020GANSpace, Shen2021SeFa, Voynov2020Unsupervised}.
InterfaceGAN~\cite{Shen2020Interpreting, Shen2020InterFaceGAN} is a simple and robust supervised technique that is highly recommended for practical applications, and as such we also employ it in our work.
While there is a plethora of other techniques~\cite{Zhuang2021ICLREditing, Yao2021LatentTransformer, Cherepkov2021Navigating, Abdal2021StyleFlow, Tewari2020StyleRig, Tewari2020Pie} the development of related latent space manipulations itself is not the focus of our work. Another line of work is text-based editing which gained immense popularity during the last year~\cite{Gal2022StyleGANNADA, Patashnik2021StyleClip}.

\pgraph{3D-aware GANs}
Recent GAN papers attempt to discover 3D information from large collections of 2D images using Neural Radiance Fields (NeRFs) as shape representations~\cite{Chan2021EG3D,Orel2022StyleSDF,Deng2022GRAM,Hong2022EVA3D,Zhao2022GMPI, Schwarz2022NEURIPS,Cai2022pix2nerf}. While most of these papers share similar architectural ideas, EG3D~\cite{Chan2021EG3D} has emerged as a popular basis for follow-up work (\eg integration of a segmentation branch~\cite{Sun2022IDE3D}). We chose to build upon EG3D, but our work is also applicable to other generators with a similar latent space.
For more information on 3D GAN architectures, we refer the interested reader to a recent survey~\cite{Xia20223DSurvey}.

\pgraph{Video Synthesis and Editing}
One branch of work attempts to leverage 2D GANs to generate video sequences ~\cite{Fox2021StyleVideoGAN, Tian2021Video, Yu2022Digan, Skorokhodov2022StyleGANV}. These ideas can be extended to create 3D videos~\cite{Bahmani20223DAware}, which also rely on 3D NeRFs.

\pgraph{GAN-based Video Editing}
GAN-based video editing is the core topic of this paper.
Duong \etal~\cite{Duong2019Aging} employ deep reinforcement learning for automatic face aging.
LatentTransformer~\cite{Yao2021LatentTransformer} encodes frames into the StyleGAN latent space using an encoder. They train a transformer to do attribute editing on single frames and blend the result with Poisson blending.
The main competitor to our work is Stitch it in Time (\SIT)~\cite{Tzaban2022STIT}, which crops the faces from a video, edits them with 2D GAN techniques, and merges the edited result back to the video with some blending. However, \SIT does not learn a 3D model of the human head, overfits to a particular video, and is unable to provide edits to the viewpoint of the human head.
Recently, VideoEditGAN (\VEG)~\cite{Xu2022VideoEditGAN} attempted to improve the temporal consistency of \SIT by running a two-step optimization approach focused on localized temporal coherence. 
Alaluf \etal~\cite{Alaluf2022Times} use StyleGAN3 for video editing, to leverage its inherent alignment capabilities and reduce texture sticking artifacts. 
Since this is an active area of research, all these techniques are concurrent work to our method, yet we do provide comparisons to showcase the benefits of our proposed approach. 


% END RELATED WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METHOD SECTION
\vspace{-1mm}
\section{Method}
\vspace{-1mm}
\label{sec:method}
In this section, we introduce the key components of \name to perform frame-by-frame video editing while allowing for rendering the edited face from new views. We leverage a 3D-aware generator that infers 3D geometry and camera positions while being trained solely on 2D images. We build a personalized 3D-aware generator by performing joint inversion on multiple frames and then use it to perform attribute editing, apply camera viewpoint changes, and finally composite the edited face rendered from a new view back into the original frame. An overview of our proposed approach is depicted in \cref{fig:pipeline} while the personalized generator architecture is shown in \cref{fig:generator_inversion}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/inversion_example_900dpi}
   \vspace{-7mm}
   \caption{\textbf{Personalized Generator.} 
   First, we run a joint inversion on $N$ selected target faces, where we optimize a shared target person latent $\wperson$ and an offset $\offset{\idxS}$ for each face.
   This ensures the inversions share information about the target. Simultaneously, we jointly optimize for the camera pose $\cam{\idxS}$. 
   We then fine-tune the generator to ensure it captures the fine details of the target identity. 
   Note that the ``default'' latent \textit{(left column)} implicitly captures the identity of the target person without being explicitly optimized.}
   \label{fig:generator_inversion}
   \vspace{-5mm}
\end{figure}


\subsection{Personalized 3D-Aware Generator}\label{sec:generator}
\pgraph{Face Selection and Cropping} To create a personalized 3D-aware GAN model, we start by processing a short range from the input video where \(N\) frames are selected such that they cover a range of orientations and facial expressions of the target person. We detect the facial keypoints within these frames using an off-the-shelf facial keypoint detector~\cite{Bulat2017Align} and use them to determine the face bounding box within the frame. This is achieved by calculating a rigid transformation from the facial keypoints in the frame to the facial keypoints in a generated example image, thereby aligning the keypoints at the center of the crop in the same way as the generator's original training data. We pick a specific field of view for cropping the faces and optimizing the generator, but the field of view remains a flexible parameter that can be adapted during any later stage in the pipeline.

\pgraph{Simultaneous Inversion} We propose to perform multiple inversions simultaneously.
EG3D has two major components in its generator. The first component uses a mapping network to map random vectors into a semantically meaningful space, called $\w$-space. Vectors in this space control a 3D neural field that defines a 3D representation that is rendered using volumetric rendering. The second component is a 2D upsampler that performs a \(4\times\) super-resolution on the original output.
We invert all selected faces simultaneously into the $\w$-space following a strategy similar to \cite{Roich2022PTI} that we discuss in detail below.

In order to find a representation in $\w$-space, we define a ``global'' \wperson aiming at capturing the global identity features of the target person, and a ``local'' offset vector \offset{\idxS} for each input expression \face{\idxS} that encodes the differences of each individual facial expression and position from the default \wperson.
The length of each \offset{\idxS} is regularized using an $\loss{L2}$ loss, aiming to keep the difference as small as possible, and capturing all similarities between the input images within the default person latent \wperson. 
We use a combination of a perceptual loss $\loss{LPIPS}$ and a pixel loss $\loss{L1}$ for the inversion. Note that during this stage, we calculate these losses on the raw output $\generator{\mtxt{raw}}(\wperson+\offset{\idxS})$ of the EG3D neural renderer at 128$\times$128 resolution because we observed that it yields sharper result quality rather than evaluating the loss at the output of the super-resolution network.
%
We downsample our target images to the same resolution \down{128}{\face{\idxS}} to compare. 
To ensure that we can faithfully capture the target person's identity and expression, we use BiSeNet~\cite{Yu2018BiSeNet} to obtain a segmentation \seg{exp}{\face{\idxS}} of the facial regions encoding the expression (eyes, mouth, eyebrows, and nose) and add an additional feature loss on this area to encourage consistent facial expressions (\eg closed eyes).
To obtain the inversion, in each optimization step, we sum up the losses for each face image \face{\idxS}, therefore jointly optimizing all targets simultaneously, yielding a total loss \loss{inv}.
\begin{align*}
\textstyle
\loss{inv} = \sum\nolimits_{\idxS=0}^{N}\weight{LPIPS} \, &\loss{LPIPS}(\generator{\mtxt{raw}}(\wperson+\offset{\idxS}), \down{128}{\face{\idxS}}) + \\
\weight{L1} \, &\loss{L1}(\generator{\mtxt{raw}}(\wperson+\offset{\idxS}), \down{128}{\face{\idxS}}) + \\
\weight{seg} \, &\loss{LPIPS}(\seg{exp}{\generator{}(\wperson+\offset{\idxS})}, \seg{exp}{\face{\idxS}}) + \\
\weight{reg} \, &\loss{L2}(\offset{\idxS})
\end{align*}
Due to the 3D awareness of the EG3D generator, the quality of the inversion into the latent space is highly sensitive to the camera parameter settings. Hence, in addition to optimizing for \wperson and \offset{\idxS}, we propose to also allow the inversion to optimize for the camera parameters \cam{\idxS} (\y{\idxS} and \p{\idxS}) for each input expression \face{\idxS}, which reliably estimates the camera position that the face is captured from.

A key advantage of this joint optimization is that the facial characteristics of the person preserve their high fidelity even when seen from novel views. When inverting a single image of a side-facing person into the EG3D latent space, exploring other viewpoints of the inverted latent can lead to significant distortions. Often, unseen features (\eg hidden ears) can be blurry or distorted, and the identity no longer resembles the input from a different viewpoint. The joint inversion, however, ensures that the different views are embedded closely enough in latent space such that even unseen views yield consistently identity-preserving outputs.

\pgraph{Generator Fine-tuning}  We propose a variant of Pivotal Tuning~\cite{Roich2022PTI} to jointly fine-tune the weights of the generator \generator{\mtxt{EG3D}} on all input faces \face{\idxS}, while keeping the detected \wperson, \offset{\idxS} and camera poses \cam{\idxS} fixed. Here, we do not allow the weights of the upsampler of the generator to be updated as we want to preserve the generalization capabilities of the super-resolution network and prevent it from overfitting to our target images. During this fine-tuning stage, we employ perceptual and pixel losses described as follows:
\begin{align*}
\textstyle
\loss{tune} = \sum\nolimits_{n=0}^{N}\weight{LPIPS} \, &\loss{LPIPS}(\gtuned(\wperson+\offset{\idxS}), \face{\idxS}) + \\
\weight{L1} \, &\loss{L1}(\gtuned(\wperson+\offset{\idxS}), \face{\idxS})
\end{align*}

Finally, we obtain a personalized EG3D generator \gtuned, fine-tuned to a set of facial expressions of the target person.
%
We verify that the fine-tuned generator indeed provides a good generalized latent space for the target person even though it was inverted and tuned based on a low number of frames by exploring the person created by the ``global'' latent code, which was not explicitly fine-tuned for, as well as through a latent space walk in the fine-tuned latent space.

\subsection{Frame-by-frame Video Inversion}
With the personalized 3D-aware generator in hand, we are now given a video of the same person as input which can be different from the one the generator was trained on.  
To process our new target video, we extract the facial keypoints from each frame \idxF to determine the location of the box to indicate the face crop within the frame. In order to stabilize the crop over time, which supports the temporal coherence of the inversion, we perform a Gaussian smoothing on the extracted facial keypoints along the temporal axis after extraction. However, it is important to not over-smooth, because fast motions in the video would yield distorted keypoint locations, deteriorating the inversion quality.

We then perform a frame-by-frame inversion of the extracted face regions \face{\idxF} into the space of the fine-tuned generator \gtuned. Like before, we optimize for an offset \offset{\idxF} for each input frame \face{\idxF}, as well as regularizing the offset length. After inverting the first frame, each consecutive frame \face{\idxF+1} is inverted starting from the previous inversion and only needs a low number (\mytilde50) of optimization steps. 
\vspace{-5mm}
\begin{align*}
\textstyle
\loss{vid} = \weight{LPIPS} \, &\loss{LPIPS}(\gtuned(\wperson+\offset{\idxF}), \face{\idxF}) +\\
             \weight{L1} \, &\loss{L1}(\gtuned(\wperson+\offset{\idxF}), \face{\idxF}) + \weight{reg} \, \loss{L2}(\offset{\idxF})
\end{align*}
This inversion yields a stack of offsets \offset{\idxF} from the identity latent \wperson as well as camera parameters \cam{\idxF} (\y{\idxF}, \p{\idxF}) encoding the expression and camera position for each frame.

\setlength{\belowcaptionskip}{-12pt}
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/editing_directions} 
   \vspace{-6mm}
   \caption{\textbf{InterfaceGAN edits.} We show InterfaceGAN editing directions discovered in the latent space by applying them on our personalized generator. The attribute edits are consistent in 3D.
   }
   \label{fig:editing-directions}
\end{figure}

\subsection{Attribute Editing and Novel View Synthesis}
Since EG3D is built on top of StyleGAN2, we can leverage existing latent space editing techniques in order to discover semantic editing directions in the latent space of EG3D. As a proof of concept, we implemented InterfaceGAN~\cite{Shen2020InterFaceGAN} to find meaningful latent space direction vectors. We re-trained classifiers on the CelebA dataset for several facial attributes such as age, smile, gender, glasses, beard, and hair color and use these classifiers to classify the reference outputs of a set of randomized latent codes from our generator. Finally, we used an SVM to recover editing boundaries from these classified latent codes, which allows us to perform attribute editing in the EG3D latent space, as shown in \cref{fig:editing-directions}.
For a given latent space direction $\w_{\mtxt{dir}}$, we apply an edit as a linear combination of the person latent \wperson with the direction, multiplied by an empirically chosen weight $\alpha_{\mtxt{dir}}$, which can be positive or negative. For our video sequence, we use the edited person latent ${\wperson}'$ as the new identity to which we apply our video offsets $\offset{\idxF}$.
\begin{align*}
{\wperson}' = \wperson + \alpha_{\mtxt{dir}} \times \w_{\mtxt{dir}}
\end{align*}
In addition, we explore our temporal stack of latent encodings from novel views, diverging from the input views the inversion discovered. This allows us, to generate frontalized videos of the person by fixing the camera position or to define arbitrary camera trajectories around the subject.

\setlength{\belowcaptionskip}{-5pt}
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/border_illustration}
   \vspace{-6mm}
   \caption{\textbf{Border Composition.} 
   We calculate the composition border based on face segmentations of the target image \figlabel{a} and the edited inset \figlabel{c}. We unite the masks and dilate the resulting joint mask to obtain a boundary around the face regions \figlabel{d} that should be optimized, which allows us to create the final composition \figlabel{e}. 
   }
   \label{fig:stitching-border}
   \vspace{-4mm}
\end{figure}

\subsection{Compositing with Source Video} 
After editing the inverted video, we want to recomposite the edited faces back with the source frames such that the edited video is temporally and spatially consistent. We accomplish this by running an optimization to ensure that the boundaries between the compositing of the edited face and the background of the source frame are smooth. 
Since cluttered video backgrounds are hard to reproduce consistently and without artifacts -- especially for novel views -- we define a compositing boundary region in a similar manner to \cite{Tzaban2022STIT}. To accomplish this, we need an accurate segmentation of the face and hair for both the original frame as well as the edited face. Hence, we use BiSeNet~\cite{Yu2018BiSeNet}, a semantic segmentation technique, that accurately provides such face semantics. We use the semantic regions for both the original and edited face to form a union of their respective masks, obtaining a boundary region around the relevant face region, as illustrated in \cref{fig:stitching-border}. 
We run a small number of optimization steps, optimizing for the boundary region of the edited image to appear as close as possible to the boundary region of the input frame while retaining the appearance of the edit. Finally, we use an affine transformation to re-insert the cropped face region back into the original frame and we alpha blend along the optimized boundary region to seamlessly composite the edit with the source frame.


\subsubsection{Flow-based View Adjustment}
During the process of re-inserting the edited face \face{\edit} back into the source frame \source, a major challenge arises when the camera pose has been modified and the face is rendered from a novel view. 
This is because the face is oriented within the bounding box based on the facial keypoints as described in \cref{sec:generator} while upon a camera pose change, the face pivots around the keypoints. When, for instance, attempting to replace a face viewed at an angle with a frontalized face while retaining the original crop boundary of the inset, the keypoints are still roughly in the same location, yet the mass of the head, and crucially, the neck is shifted according to the face rotation, as seen in \cref{fig:camera-problem}, which results in the inset being disconnected from the rest of the body even when using a boundary stitching technique. 

\setlength{\belowcaptionskip}{-5pt}
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/flow_illustration}
   \vspace{-6mm}
   \caption{\textbf{View adjustment.} 
   After cropping \figlabel{a} and inverting \figlabel{b} a face, we perform face editing \figlabel{c} and change the camera viewpoint to an unseen angle \figlabel{d}. When replacing the face in the original frame with this edit, it yields poor quality \textit{(bottom center)} even for small angular changes, because the rotated face is in the wrong location with respect to the body. We address this by estimating the optical flow \figlabel{e} between the face crop and the edit and use the flow direction to correct the location of the reference face based on the prospective inset \figlabel{f}. This allows us to composite the edited face into the frame in a natural-looking fashion \textit{(bottom right)}.
   }
   \label{fig:camera-problem}
   \vspace{-4mm}
\end{figure}

To alleviate this problem, we introduce a simple yet effective technique to reposition the reference face region within the source frame.
We discover the optimal position of the updated inset with respect to the source frame by estimating the optical flow between the face segmentation in the source frame \source and the face segmentation in the inset region \face{\edit} after camera rotation. We convert the images to grayscale and use Farneb\"{a}ck optical flow~\cite{Farnebaeck2003OpticalFlow} to evaluate a dense flow field of the displacement between the edited and target faces. 
The optical flow is defined as a 2D displacement vector field $\vec{d}$ with the displacement vector at image position $(x, y)$ given by \(\vec{d}(x, y) = (u(x, y), v(x, y))\)
where the correspondence between the two images \face{\idxF} and \face{\edit} is:
\begin{align*}
\face{\edit}( x + u(x, y), y + v(x, y) ) = \face{\idxF}( x,  y ).
\end{align*}
We then compute vector magnitudes $\|\vec{d}\| = \sqrt{ u^2+v^2}$ and directions $\phi=\mtxt{atan2}{(v, u)}$, respectively. After eliminating all vectors with a magnitude smaller than a threshold $\epsilon$, we create a histogram of all remaining  directions. We define a dominant displacement vector $\vec{d}_{\mtxt{dom}}$ from the median direction of the histogram bin with the largest count and the maximum vector length within that histogram bin. 
This ensures that erroneous flow directions from features that are present within one of the two images but not the other are not contributing to the final output.

The displacement vector $\vec{d}_{\mtxt{dom}}$ is reprojected from inset space into frame space and is used to correct the location of the reference face crop.
To ensure a smooth transition between adjacent frames, we perform temporal Gaussian smoothing of the recovered displacement vectors.
We then apply our inset optimization using the updated reference areas and obtain a significantly more faithful result, allowing for large camera changes with natural-looking results.

% END METHOD SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RESULTS SECTION
\begin{table}[t]
\arrayrulecolor{mediumgray}
\small
\centering
\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}
\caption{\textbf{Video Quality Metrics.} We compare the quality of our inversion with \SIT using reconstruction metrics on a subset of the \textit{VoxCeleb} dataset. We also evaluate the Fr\'echet Inception Distance~(FID) of inversion and edits with respect to the source video.}
\vspace{-0.7em}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc|ccc}
                 &\multicolumn{2}{c|}{\textbf{Reconstruction Quality}}&\mc{3}{\textbf{Editing Quality}}                                        \\
                 &\gr\textbf{PSNR}~$\uparrow$&\gr\textbf{SSIM}~$\uparrow$ & \mc{3}{\gr\textbf{FID}~$\downarrow$}                               \\    
    \tss{Method} &\multicolumn{2}{c|}{\tss{Inversion}}             & \tss{Inversion}         & \tss{Age Edit}          & \tss{Angle Edit}      \\
    \midrule
    \arrayrulecolor{lightgray}
       \SIT      &            38.0134  &            \textbf{0.9798}&                  6.8329 &                 15.8021 &             19.0371   \\
       \cc \name &\cc \textbf{38.1259} &\cc                 0.9704 & \cc      \textbf{4.9852}&\cc     \textbf{~~9.3410}&\cc   \textbf{~~8.9953}\\ 
    \bottomrule
\end{tabular}
}
\vspace{-5mm}
\label{tab:reconstruction}
\end{table}


\section{Experiments}
We conduct a wide range of quantitative and qualitative comparisons to demonstrate the key contributions of our work along with ablation studies against baselines and simplified variants where proposed modules are removed. We showcase that \name is on par with \SIT in terms of reconstruction quality while it also greatly outperforms prior work in editing quality and identity preservation. However, a key novelty afforded by \name is the ability to render the edited face from novel viewpoints within the existing frame, a task for which comparisons are hard due to the absence of ground truth. We showcase this with qualitative results and videos provided in the supplementary material.


% EVALUATION SECTION
\subsection{Quantitative Evaluation}\label{sec:evaluation}
We establish comparisons with \SIT~\cite{Tzaban2022STIT}, the key competitor to our work in the field of GAN video editing, and with \VEG~\cite{Xu2022VideoEditGAN} for which many components (\eg their stitching technique) are identical to \SIT, so we only compare facial similarity metrics. 
Please see the supplementary material for a detailed description of the comparison setup between our method and the competitors.

\pgraph{Inversion Quality} We provide a quantitative comparison of our inversion quality by measuring the reconstruction quality with respect to the input video in \Cref{tab:reconstruction}. We evaluate PSNR and SSIM for our method and for \SIT on a set of 16 videos from the \textit{VoxCeleb}~\cite{Nagrani2017VoxCeleb_OG} dataset, inverting the face region and recompositing it with the source video without edits. Both methods perform well on the reconstruction of the input signal and the final reconstruction quality of our technique is on par with \SIT.

\begin{table}[t]
\arrayrulecolor{mediumgray}
\footnotesize
\setlength{\tabcolsep}{4pt}
\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}
\centering
\caption{\textbf{Face Similarity Metrics.} We evaluate the identity preservation of inversion and edits based on the cosine similarity of Arc\-Face features extracted from generated face crops with respect to the face crops of the source video. To evaluate coherence over time, we measure the dissimilarity between consecutive frames.
}
\vspace{-0.8em}
\resizebox{\columnwidth}{!}{
\begin{tabular}{r|l|c|c}
                       &\tss{Method}&\gr{\textbf{Similarity to Source}}~$\uparrow$ &\gr{\textbf{Temporal Difference}}~$\downarrow$ \\
\midrule
\arrayrulecolor{lightgray}
 \mr{3}{Inversion}     & e4e        &           0.6923 &   6.2851             \\
                       & \SIT       &  \textbf{0.9261} &  1.2361              \\
                       & \cc \name  &       \cc 0.9203 &\cc \textbf{1.0444}   \\
  \cmidrule{1-4}
 \mr{3}{Age Editing}   & \SIT       &           0.7891 &    1.4126            \\
                       & \VEG       &           0.6004 &    1.7159            \\
                       & \cc \name  &\cc \textbf{0.8381}&\cc \textbf{1.2257}  \\
  \cmidrule{1-4}
 \mr{3}{Angle Editing} & \SIT       &            0.6695 &    1.3102           \\
                       & \VEG       &            0.4955 &    1.4502           \\
                       & \cc \name  &\cc \textbf{0.8694}&\cc \textbf{1.2761}  \\
\bottomrule
\end{tabular}
}
\vspace{-5mm}
\label{tab:face-similarity}
\end{table}

\pgraph{Image Quality} To evaluate the image quality produced by the respective techniques, we compute the Fr\'echet Inception Distance (FID), which is a commonly used quality metric for GANs. To obtain the FID for each video, we compare the set of all frames of the inverted video, as well as selected edits, with all frames of the source video. Our method is able to score very good FID scores overall (see \Cref{tab:reconstruction} right), confirming the quality of our results.

\pgraph{Face Fidelity} We calculate the fidelity of our inversion and edits based on a facial similarity metric, ArcFace~\cite{Deng2019ArcFace}, which extracts a 512-D feature vector capturing facial characteristics from a face region. We compute the metrics based on the respective face crops of the final inset region, scaled to 512$\times$512px, for our method, e4e encoding, \SIT, and \VEG in \Cref{tab:face-similarity} and average across a set of 16 videos from the \textit{VoxCeleb}~\cite{Nagrani2017VoxCeleb_OG} dataset.
The facial similarity is evaluated both with respect to the input video (frame-by-frame) as well as temporally by calculating the dissimilarity of adjacent frames in order to survey the temporal coherence of facial characteristics. In all cases, we use the cosine similarity between the extracted ArcFace deep features.
We observe that the quality of the inversion is good for both \SIT and \name, both significantly improving upon e4e encoding. \VEG uses the same PTI-based inversion as \SIT and is therefore not listed.
For latent space editing, our proposed \name leads the competition. \VEG exhibits good temporal coherence, however, the edits contain artifacts, resulting in a deterioration in the similarity to the source video. Additionally, our technique reconstructs the facial identity faithfully even for angle edits, a task in which \SIT and \VEG fail to produce plausible results due to their methods' inability to accommodate changes in the head rotation.

\begin{table}[b]
\arrayrulecolor{lightgray}
\scriptsize
\centering
\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}
\caption{\textbf{Timings and Memory Requirements.} We provide runtimes and Memory requirements for ours and competing methods.}
\vspace{-0.8em}
\begin{tabular}{r|c|c|c|c|c}
    \tss{Method}  & \gr{\textbf{Total}}      & \gr{\textbf{Precompute}} & \gr{\textbf{Main}}   & \gr{\textbf{Postprocess}} & \gr{\textbf{GPU}}  \\
    \toprule
    \SIT          & ~~58\mins53\secs         & ~~28\mins21\secs         & 30\mins19\secs       & ---                       & 22GB               \\
    \VEG          & 159\mins54\secs          & 107\mins~~9\secs         & 34\mins41\secs       &  18\mins~~3\secs          & 19GB               \\
    \cc\name      & \cc ~~35\mins43\secs     & \cc~~~~6\mins58\secs     & \cc 14\mins54\secs   & \cc 13\mins51\secs        & \cc 21GB           \\
\bottomrule
\end{tabular}
\label{tab:timings}
\vspace{-2.5mm}
\end{table}

\pgraph{Resource Usage} We compare runtimes and memory requirements for the default pipeline of related methods and our method, respectively, in \Cref{tab:timings}. We split each method, wherever applicable, into precomputation, main method, and postprocessing steps and used the hyperparameter settings according to the authors' suggestions. Note that the precomputation step in our method has to only be run once per identity and can then be applied to multiple videos. All experiments are performed on an example video consisting of 200 frames at a resolution of 1920$\times$1080px, using a single NVIDIA A100 GPU with 40GB memory.

\pgraph{Ablation Study} We quantitatively verify the effectiveness of different architecture choices we made, as shown in \Cref{tab:ablation}. We compare several metrics with respect to the source video for our default implementation, and the ablation experiments, respectively. We run four experiments on a set of 5 videos: (1) \name without generator fine-tuning, (2) \name without flow-based adjustment, (3) \name without the joint \wperson latent and offset regularization, (4) using only a single input face for the generator inversion and fine-tuning, which in practice is almost identical to only frame-by-frame inversion in EG3D without any personalized generator. We demonstrate that all experiments deteriorate the facial fidelity as well as the reconstruction quality.

\begin{table}[t]
\arrayrulecolor{lightgray}
\scriptsize
\centering
\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}
\caption{\textbf{Ablation Study.} We demonstrate the effect of removing various components of our pipeline on several quality and reconstruction metrics. We measure the face similarity using the cosine similarity of ArcFace features of the generated face crop, and the reconstruction metrics at the target video resolution.}
\vspace{-0.8em}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|c|c}
            \textbf{Ablation Type}&\gr\textbf{Face Similarity~$\uparrow$}& \gr\textbf{PSNR~$\uparrow$}   & \gr\textbf{SSIM~$\uparrow$}    \\ 
    \midrule
     \cc \name, full              & \cc      \textbf{0.9101}&\cc\textbf{35.5367}&\cc\textbf{0.9763} \\
     no generator fine-tuning     &                  0.7191 &           33.1202 &           0.9616  \\
     no flow correction           &                  0.8198 &           24.8845 &           0.9350  \\
     no regularization            &                  0.8007 &           25.6537 &           0.9162  \\
     single input for inversion   &                  0.7382 &           25.1950 &           0.9137  \\
     \bottomrule
\end{tabular}
}
\label{tab:ablation}
\vspace{-5mm}
\end{table}

\begin{figure}[b]
\centering
\vspace{-6mm}
\includegraphics[width=\linewidth]{figures/comparison_ageedit} 
   \vspace{-6mm}
   \caption{\textbf{Comparisons with \SIT on attribute editing.} 
   We show an example of our method and \SIT editing the subject's age in the video frame \textit{(center column)}. Both methods yield plausible but distinctly different results. Our results \textit{(columns \figlabel{a}, age=--1.4 and \figlabel{c}, age=+2.3)} vs \SIT \textit{(columns \figlabel{b}, age=--8 and \figlabel{d}, age=+12)}. 
   }
   \label{fig:comparison-age}
\end{figure}


\label{sec:results}
\subsection{Qualitative Evaluation}
\pgraph{Semantic Edits} First, we demonstrate that the quality of semantic edits using \name, adapting well-established latent space editing techniques for 3D GANs, is on par with the editing quality of \SIT, as shown in \cref{fig:comparison-age}. Note that while both approaches discover latent space directions using InterfaceGAN~\cite{Shen2020InterFaceGAN}, the latent spaces and discovered directions are dissimilar, yet both plausible.

\setlength{\belowcaptionskip}{-5pt}
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/directions_900dpi} %
   \vspace{-6mm}
   \caption{\textbf{Changing camera poses.} Our method can freely alter the camera pose and composite the result back with the source frame by fixing the divergence between the source and target pose using our optical flow correction strategy. The generated results look natural despite the static body pose.
   }
   \label{fig:view-directions}
   \vspace{-2mm}
\end{figure}

\pgraph{Synthesizing novel views} We show that \name can accommodate changes in the camera view with natural-looking results for a wide range of views regardless of the input face orientation, as shown in \cref{fig:view-directions}. This is nontrivial as we need to ensure that the person's identity is consistent from multiple viewpoints while the body in the source frame also defines a rigid constraint to which the head alignment must be adjusted to.
%
While \SIT produces good results for attribute edits, it cannot composite images with angular changes, despite the fact that a limited head pose change can be achieved by applying latent space manipulations. In \cref{fig:comparison-fail}, we show a comparison where \SIT is unable to generate a reasonable composition, whereas we can achieve natural-looking results for a similar head rotation.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/comparison_angleedit_900dpi}
   \vspace{-6mm}
   \caption{\textbf{Comparisons with \SIT on viewpoint changes.} 
   \name \textit{(\figlabel{a} and \figlabel{c})} produces plausible results for both positive and negative yaw changes, whereas \SIT \textit{(\figlabel{b} and \figlabel{d})} is unable to create natural compositions of the edit with the target frame.
   }
   \vspace{-0.3cm}
   \label{fig:comparison-fail}
\end{figure}

\pgraph{Compositing with challenging boundaries} When parts of the head or hair are visible both inside and outside the face bounding box then compositing the edited frame back into the original input is a challenging task. In most cases, we address these scenarios by adjusting our camera parameters (\eg, choosing a wider field of view for our generator) but some configurations can still be challenging, especially when different textures need to be matched. We show in \cref{fig:stitch-boundaries} that our technique attempts to produce plausible inset optimizations even for such instances.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/border_cases} 
   \vspace{-6mm}
   \caption{\textbf{Spatial Consistency.} \name composites images with challenging boundaries such as long hair \textit{(right)}, yielding faithful hair color change results. For hard boundary cases, such as matching with a static piece of hair outside the boundary crop \textit{(left)}, it plausibly connects the contents of the two images.}
   \label{fig:stitch-boundaries}
   \vspace{-2mm}
\end{figure}

% END RESULTS SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pgraph{Limitations} Changing the camera parameters for the head in videos with fast motion or discontinuities results in artifacts because the flow estimation becomes unstable. Furthermore, we inherit the shortcomings of EG3D (see \cref{fig:limitations}): stronger entanglement of attribute edits compared to StyleGAN2, extreme camera poses are not captured in the training set, and texture sticking. Finally, since the video outside the face region is immovable, the range of possible changes is constricted to plausible compositions.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/limitations}
   \vspace{-6mm}
   \caption{\textbf{Limitations.} 
   \name inherits some limitations from the frameworks we rely on. EG3D cannot capture extreme poses well \textit{(left)}. Large angle changes cannot be composited naturally with the static body in the source frame. For extreme edits (gender edits that change the hair structure \textit{(right)}), it is difficult to yield temporally consistent results, both due to the entanglement of the latent space editing and the challenges of frame compositing.
   }
   \vspace{-0.3cm}
   \label{fig:limitations}
\end{figure}

\vspace{-0.2cm}
\section{Conclusion}\label{sec:conclusion}

In this paper, we introduced \name, a novel framework that uses prior information encoded in 3D GANs for video editing. Our edits are identity-preserving and temporally consistent. While we enable standard semantic edits, such as age, or expressions, a distinguishing feature of our work is that we facilitate edits that alter the view of the head. This capability is not available in any 2D GAN-based prior work. %
The key building blocks of our work are a new embedding algorithm that jointly embeds multiple frames and optimizes for camera pose as well as flow-guided video compositing.
In future work, we aim to extend our framework to include a 3D GAN for head details and another 3D GAN for the body. We also plan to investigate performance speedups by replacing various optimizations with encoders.

{
\small
\bibliographystyle{ieee_fullname}
\bibliography{bibliography}
}
\end{document}