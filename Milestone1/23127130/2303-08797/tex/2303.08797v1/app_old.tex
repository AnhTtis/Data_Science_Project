

\section{Bridging two Gaussian mixture densities}
\label{sec:Gauss:mixt}
In this appendix, we consider the case where $\rho_0$ and $\rho_1$ are both Gaussian mixture densities. We denote by
\begin{equation}
\label{eq:NmC}
\begin{aligned}
    N(x|m,C) &= (2\pi)^{-d/2} [\det C]^{-1/2} \exp\left(-\tfrac12 (x-m)^\T C^{-1} (x-m)\right),\\
    & = (2\pi)^{-d} \int_{\RR^d} e^{ik\cdot (x-m) -\frac12 k^\T C k} dk,
\end{aligned}
\end{equation}
the Gaussian probability density with mean vector $m \in \RR^d$ and positive-definite symmetric covariance matrix $C=C^\T\in \RR^{d\times d}$. We assume that
\begin{equation}
    \label{eq:rho0:mixt}
    \rho_0(x) = \sum_{i=1}^{N_0} p^0_i N(x|m^0_i,C^0_i), \qquad
    \rho_1(x) = \sum_{i=1}^{N_1} p^1_i N(x|m^1_i,C^1_i)
\end{equation}
where $N_0,N_1\in \NN$,  $p^0_i>0$ with $\sum_{i=1}^{N_0} p_i^0 = 1$, $m_i^0\in \RR^d$, $C_i^0= (C_i^0)^\T \in \RR^{d\times d}$, positive-definite, and similarly for $p_i^1$, $m_i^1$, and $C_i^1$. 
% Note that if all the covariance matrices are the same, $C_{i}^0=C_{j}^1= C$, with the trigonometric interpolant in~\eqref{eq:interp:trig} we have $C^{ij}(t) = C$, which justifies this choice of interpolant.
We have:
\begin{restatable}{proposition}{gaussmixt}
\label{th:Gauss:mixt}
Consider the process $x_t$ defined in~\eqref{eq:stochinterp} using the probability densities in~\eqref{eq:rho0:mixt} and the interpolant in~\eqref{eq:lin:interp}, i.e.
\begin{equation}
    \tag{\ref{eq:lin:interp}}
    I(t,x_0,x_1)= \alpha(t) x_0+ \beta(t) x_1  
\end{equation}  
with $\alpha(t)$ and $\beta(t)$ satisfying~\eqref{eq:lin:interp:a:b}. Denote \begin{equation}
    \label{eq:mij:Cij}
    m_{ij}(t) = \alpha(t) m^0_i+\beta(t) m^1_j, \quad C_{ij}(t) = |\alpha(t)|^2 C^0_i+|\beta(t)|^2 C^1_j\quad C_{i,j}^\eps(t) = C_{i,j}(t) +2\eps t(1-t)I, 
\end{equation}
where $i=1,\ldots, N_0,$ $j=1,\ldots, N_1$.
Then the probability density $\rho$ of $x_t$ is the Gaussian mixture density
\begin{equation}
    \label{eq:rhot:Gaussmixt}
    \rho(t,x) = \sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j N(x| m_{ij}(t),C_{ij}^\eps(t))
\end{equation}
and  the velocities $v$ and  $s$ defined in~\eqref{eq:v:def} and \eqref{eq:s:def} are 
\begin{equation}
    \label{eq:vt:Gausmixt}
    v(t,x) = \frac{\sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j \left(\dot m_{ij}(t) + \tfrac12\dot C_{ij}(t) [C^\eps_{ij}(t)]^{-1}(t)(x-m^{ij}(t))  \right) N(x| m_{ij}(t),C^\eps_{ij}(t))}{\sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j N(x| m_{ij}(t),C^\eps_{ij}(t))},
\end{equation}
and
\begin{equation}
    \label{eq:st:Gausmixt}
    s(t,x) = -\frac{\sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  [C_{ij}^\eps(t)]^{-1}(t)(x-m_{ij}(t))  N(x| m_{ij}(t),C^\eps_{ij}(t))}{\sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_i p^1_j N(x| m_{ij}(t),C_{ij}^\eps(t))}.
\end{equation}
\end{restatable}
This proposition is proven it App.~\ref{app:gausmixt}. It implies that $v$ and $s$ grow at most linearly in $x$, and are approximately linear in regions where the modes of $\rho(t,x)$ remain well-separated. In particular, if $\rho_0$ and $\rho_1$ are both Gaussian densities, $\rho_0=N(m_0,C_0)$ and $\rho_1=N(m_1,C_1)$, we have  
\begin{equation}
    \label{eq:vt:mode:Gaussmixt}
    v(t,x)=\dot m(t) + \tfrac12 \dot C(t) [C^\eps(t)]^{-1}(x-m(t)),
\end{equation}
and
\begin{equation}
    \label{eq:st:mode:Gaussmixt}
    s(t,x)= - [C^\eps(t)]^{-1}(x-m(t))  ,
\end{equation}
where
\begin{equation}
    \label{eq:mt:Ct}
    m(t) = \alpha(t) m_0+ \beta(t) m_1, \quad C(t) = |\alpha(t)|^2 C_0+ |\beta(t)|^2 C_1,\quad C^\eps(t) = C(t) + 2\eps t(1-t).
\end{equation}



\section{Proofs}
In this appendix, we provide the details for proofs omitted from the main text. For ease of reading, a copy of the original theorem statement is provided with the proof.
\subsection{Proof of Theorem~\ref{prop:interpolate}}
\label{app:proof:interpolate}
\interpolation*
\begin{proof}
We first observe that~\eqref{eq:fpe:tr} and~\eqref{eq:transport} are direct consequence of~\eqref{eq:fpe} and~\eqref{eq:s:score}, since the equality
\begin{equation}
    \label{eq:score}
\eps\Delta \rho =  \eps\nabla \cdot (\rho \nabla \log \rho ) = \eps\nabla \cdot (s \rho ) 
\end{equation}
 can be used to convert between the FPE, the reversed-FPE, and the TE. It is thus sufficient to prove~\eqref{eq:fpe} and~\eqref{eq:s:score}, and to establish the regularity properties of $\rho$, $v$, and $s$ as well as~\eqref{eq:vt:wt:bounded}. To this end,  let $g(t,k) = \EE e^{ik \cdot x_t}$,  $k \in \RR^d $, be the characteristic function of $\rho(t,x)$. By the definition of $x_t$ in~\eqref{eq:stochinterp},
\begin{equation}
\label{eq:charact}
 g(t,k) = \EE e^{ik\cdot (I(t,x_0,x_1) + \seps  B_t)}.
\end{equation}
 Using the independence between $x_0$, $x_1$, and $B_t$ as well as $\EE B_tB_t^\T = t(1-t) \Id $, we have
\begin{equation}
    \label{eq:gt:k}
    g(t,k) = \EE \left(e^{ik\cdot I(t,x_0,x_1) }\right) \EE \left( e^{i\seps  k\cdot B_t}\right) \equiv g_0(t,k)  e^{-\eps  t(1-t) |k|^2 }
\end{equation}
where we defined
\begin{equation}
    \label{eq:G0}
    g_0(t,k) = \EE \left(e^{ik\cdot I(t,x_0,x_1) }\right)
\end{equation}
The function~$g_0(t,k) $ is the characteristic function of $I(t,x_0,x_1)$ with $x_0\sim\rho_0$ and $x_1\sim\rho_1$. From~\eqref{eq:gt:k}, we have
\begin{equation}
    \label{eq:bound:g}
    |g(t,k)| = |g_0(t,k)|  e^{-\eps  t(1-t) |k|^2 } \le e^{-\eps  t(1-t) |k|^2 }
\end{equation} 
This shows that 
\begin{equation}
    \label{eq:int:g}
    \forall p \in \NN\ \ \text{and} \ \ t\in(0,1) \quad : \quad \int_{\RR^d} |k|^p |g(t,k)|dk<\infty 
\end{equation} 
implying that $\rho(t,\cdot)$ is in $C^p(\RR^d)$ for any $p\in \NN$ and all $t\in (0,1)$.  From~\eqref{eq:gt:k}, we also have
\begin{equation}
    \label{eq:bound:dtg}
    \begin{aligned}
        |\partial_t g(t,k)|^2 & = \left|\EE[ (ik\cdot \partial_t I_t(x_0,x_1) -\eps (1-2t) |k|^2)  e^{ik\cdot I_t(x_0,x_1)}]\right|^2 e^{-2\eps  t(1-t) |k|^2 }  \\ 
        &\le 2\left( |k|^2 \EE\big[|\partial_t I_t(x_0,x_1)|^2] +\eps^2 (1-2t)^2 |k|^4\right) e^{-2\eps  t(1-t) |k|^2 }\\
        &\le 2\left( |k|^2 M_1 +\eps^2 (1-2t)^2 |k|^4\right) e^{-2\eps  t(1-t) |k|^2 }
    \end{aligned}
\end{equation}
and
\begin{equation}
    \label{eq:bound:dttg}
    \begin{aligned}
        |\partial^2_t g(t,k)|^2 & = \left|\EE[ (ik\cdot \partial_t I_t(x_0,x_1) -\eps (1-2t) |k|^2)^2  e^{ik\cdot I_t(x_0,x_1)}]\right|^2 e^{-2\eps  t(1-t) |k|^2 }  \\ 
        & \quad + \left|\EE[ (ik\cdot \partial^2_t I_t(x_0,x_1) +2\eps  |k|^2)^2  e^{ik\cdot I_t(x_0,x_1)}]\right|^2 e^{-2\eps  t(1-t) |k|^2 }  \\ 
        &\le 4\left( |k|^2 \EE\big[|\partial_t I_t(x_0,x_1)|^2] +\eps^2 (1-2t)^2 |k|^4\right)^2 e^{-2\eps  t(1-t) |k|^2 }\\
        & \quad + 2\left( |k|^2 \EE\big[|\partial^2_t I_t(x_0,x_1)|^2] + 4 \eps^2  |k|^4\right)^2 e^{-2\eps  t(1-t) |k|^2 }  \\ 
        &\le 4\left( |k|^2 M_1^2 +\eps^2 (1-2t)^2 |k|^4\right)^2 e^{-2\eps  t(1-t) |k|^2 }\\
        & \quad + 2\left( |k|^2 M_2 + 4 \eps^2  |k|^4\right)^2 e^{-2\eps  t(1-t) |k|^2 } 
    \end{aligned}
\end{equation}
where in both cases we used~\eqref{eq:It:L2} in Assumption~\ref{as:rho:I} to get the last inequalities. These imply that
\begin{equation}
    \label{eq:int:dg}
    \forall p \in \NN\ \ \text{and} \ \ t\in(0,1) \quad : \quad \int_{\RR^d} |k|^p |\partial_t g(t,k)|dk<\infty; \quad \int_{\RR^d} |k|^p |\partial^2_t g(t,k)|dk<\infty
\end{equation} 
indicating that $\partial_t \rho(t,\cdot)$ and $\partial^2_t \rho(t,\cdot)$ are in $C^p(\RR^d)$ for any $p\in \NN$, i.e. $\rho\in C^1((0,1); C^p(\RR^d))$ as claimed. To show that $\rho$ is also positive, denote by $\mu_0(t,dx)$ the unique (by the Fourier inversion theorem) probability measure associated with $g_0(t,k)$, i.e. the measure such that
\begin{equation}
    \label{eq:meas}
    g_0(t,k) = \int_{\RR^d} e^{ik\cdot x} \mu_0(t,dx).
\end{equation}
From~\eqref{eq:gt:k} and the convolution theorem it follows that we can express $\rho$ as
\begin{equation}
    \label{eq:rhot:invF}
    \rho(t,x) = \int_{\RR^d } \frac{e^{-|x-y|^2/(4\eps t(1-t))}}{(4\pi \eps t(1-t))^{d/2}} \mu_0(t,dy),
\end{equation}
This shows that  $\rho>0$ for all $(t,x)\in (0,1)\times \RR^d$. Since $x_{t=0} = x_0$ and  $x_{t=1} = x_1$ by definition of the interpolant, we also have $\rho(0) = \rho_0$ and $\rho(1)=\rho_1$, which shows that $\rho$ is also positive and in $C^p(\RR^d)$  at $t=0,1$ by Assumption~\ref{as:rho:I}. Note that since $\rho \in C^1((0,1); C^p(\RR^d))$ and is positive, we also immediately deduce that $s = \nabla \log \rho = \nabla \rho /\rho \in C^1((0,1); (C^p(\RR^d))^d)$.

To show that $\rho$ satisfies the FPE~\eqref{eq:fpe}, recall that $B_t$ solves the SDE
\begin{equation}
\label{eq:bb:sde}
 dB_t = - \frac{B_t}{(1-t)} dt + dW_t
\end{equation}
where $W_t$ is a Wiener process. The It\^o formula then implies that
\begin{equation}
    \label{eq:ito1}
    d e^{ik\cdot x_t} = ik\cdot\Big( \partial_t I(t,x_0,x_1)dt - \frac{\seps B_t}{(1-t)} dt + \seps  dW_t\Big) e^{ik\cdot x_t} - \eps  |k|^2 e^{ik\cdot x_t}dt.
\end{equation}
Taking the expectation of this equation and applying the It\^o isometry gives
\begin{equation}
    \label{eq:ito2}
    \partial_t g(t,k) = ik\cdot \left(m(t,k) + n(t,k)\right)- \eps  |k|^2  g(t,k)
\end{equation}
where $m: [0,1]\times \RR^d\to\CC^d$ and $n:(0,1) \times \RR^d\to\CC^d$ are vector-valued functions defined as
\begin{equation}
    \label{eq:F12k}
     m(t,k) = \EE\left(\partial_t I_t(x_0,x_1) e^{ik\cdot x_t} \right), \qquad  
     n(t,k) = - \frac{\seps }{(1-t)}\EE\left( B_t e^{ik\cdot x_t}\right).
\end{equation}
Let us evaluate these two functions. By definition of the conditional expectation, $m(t,k)$ can be expressed as
\begin{equation}
    \label{eq:F1k:c}
    \begin{aligned}
     m(t,k) & = \int_{\RR^d} \EE\left(\partial_t I(t,x_0,x_1) e^{ik\cdot x_t} |x_t=x \right) \rho(t,x) dx\\
     & = \int_{\RR^d} e^{ik\cdot x} \EE\left(\partial_t I(t,x_0,x_1) |x_t=x \right) \rho(t,x) dx\\
     & = \int_{\RR^d} e^{ik\cdot x} v(t,x) \rho(t,x) dx
     \end{aligned}
\end{equation}
where the last equality follows from the definition of $v$ in~\eqref{eq:v:def}. Similarly, 
\begin{equation}
    \label{eq:F2k:c}
    \begin{aligned}
    n(t,k) &= -\frac{\seps }{(1-t)} \EE \left( B_t e^{ik\cdot I(t,x_0,x_1) }\right) \EE \left( B_t e^{i\seps  k\cdot B_t}\right)\\
    & = -\frac{\seps }{(1-t)}\int_{\RR^d} e^{ik\cdot x} \EE\left(B_t |x_t=x \right) \rho(t,x) dx\\
     & = 2\eps t \int_{\RR^d} e^{ik\cdot x} s(t,x) \rho(t,x) dx
    \end{aligned}
\end{equation}
where the last equality follows from the definition of $s$ in~\eqref{eq:s:def}. Inserting~\eqref{eq:F1k:c} and~\eqref{eq:jt2} in~\eqref{eq:ito2}, we deduce that this equation can be written in real space as the FPE~\eqref{eq:fpe}.

Using the independence between $x_0$, $x_1$, and $B_t$ as well as $\EE B_tB_t^\T = t(1-t) \Id $, $n(t,k)$ is also given by
\begin{equation}
    \label{eq:F2k:c:b}
    \begin{aligned}
    n(t,k) &= -\frac{\seps }{(1-t)} \EE \left(e^{ik\cdot I(t,x_0,x_1) }\right) \EE \left( B_t e^{i\seps  k\cdot B_t}\right)\\
    & = \frac1{(1-t)} \EE\left(e^{ik\cdot I(t,x_0,x_1) }\right) (i\partial_k) \EE \left(  e^{i\seps  k\cdot B_t}\right)\\
    & = \frac1{(1-t)} \EE \left(e^{ik\cdot I(t,x_0,x_1) }\right) (i\partial_k)  \left(  e^{-\eps  t(1-t) |k|^2 }\right)\\
    & = -2i\eps  t k \EE \left(e^{ik\cdot I(t,x_0,x_1) }\right)  \left(  e^{-\eps  t(1-t) |k|^2 }\right)\\
    & = -2i\eps t k g(t,k)
    \end{aligned}
\end{equation}
where we used Gaussian integration as well as~\eqref{eq:gt:k}. This means that 
\begin{equation}
    \label{eq:jt2}
    n(t,k) =  2\eps t \int_{\RR^d} e^{ik\cdot x}  \nabla \rho(t,x) dx = 2\eps t \int_{\RR^d} e^{ik\cdot x}    \nabla \log \rho(t,x) \rho(t,x) dx
\end{equation}
 Comparing the expressions for $n$ in \eqref{eq:F12k} and~\eqref{eq:jt2} we  obtain~\eqref{eq:s:score}. Comparing the expressions for $n$ in \eqref{eq:F12k} and~\eqref{eq:jt2} we  obtain~\eqref{eq:s:score} for all $t\in(0,1)$. Since \eqref{eq:jt2} shows that $n$ is also well-defined and continuous in time at the end-points $t=0,1$, we deduce that and ~\eqref{eq:s:score} also holds in the limits as $t\to 0,1$. As a result we deduce that $s\in C^1([0,1];(C^p(\RR^d))^d)$ for any $p\in \NN$ since $\rho\in C^1([0,1];C^p(\RR^d))$ and $\rho>0$.
 


Let us now investigate the regularity of $v$. To that end, we go back to $m$, and note that the independence  between $x_0$, $x_1$, and $B_t$ implies that
\begin{equation}
    \label{eq:F1:1}
    m(t,k) = \EE\left(\partial_t I(t,x_0,x_1) e^{ik\cdot I(t,x_1,x_0)}\right) e^{-\eps  t(1-t) |k|^2 },
\end{equation}
As a result
\begin{equation}
    \label{eq:F1:int}
    \begin{aligned}
        |m(t,k)|^2 &= \left|\EE\left(\partial_t I(t,x_0,x_1) e^{ik\cdot I(t,x_1,x_0)}\right)\right|^2 e^{-2\eps  t(1-t) |k|^2 } \\
        & \le  \EE\big[|\partial_t I(t,x_0,x_1)|^2\big] e^{-2\eps  t(1-t) |k|^2 } \\
        & \le M_1 e^{-2\eps  t(1-t) |k|^2 },
    \end{aligned}
\end{equation}
and
\begin{equation}
    \label{eq:F1:dint}
    \begin{aligned}
        &|\partial_t m(t,k)|^2\\
        &= \left|\EE\left(\left(\partial^2_t I(t,x_0,x_1) +\partial_t I(t,x_0,x_1) \left(ik\cdot \partial_t I(t,x_1,x_0) - \eps (1-2t) |k|^2 \right) \right)e^{ik\cdot I(t,x_1,x_0)}\right)\right|^2\\
        & \quad \times  e^{-2\eps  t(1-t) |k|^2 } \\
        & \le  2 \left(\EE\big[|\partial^2_t I(t,x_0,x_1)|^2\big] + |k|^2 \EE\big[|\partial_t I(t,x_0,x_1)|^4\big] + \eps^2 (1-2t)^2 |k|^4  \EE\big[|\partial_t I(t,x_0,x_1)|^2\big]\right)\\
        & \quad \times   e^{-2\eps  t(1-t) |k|^2 } \\
        & \le 2\left(M_2 +|k|^2 M_1 + \eps^2 (1-2t)^2 |k|^4 M_1^{1/2} \right)e^{-2\eps  t(1-t) |k|^2 },
    \end{aligned}
\end{equation}
where in both cases the last inequalities follow from~\eqref{eq:It:L2}. Therefore 
\begin{equation}
    \label{eq:int:f}
    \forall p \in \NN \ \ \text{and} \ \ t\in(0,1) \quad : \quad \int_{\RR^d} |k|^p |m(t,k)|dk<\infty, \quad \int_{\RR^d} |k|^p |\partial_t m(t,k)|dk<\infty 
\end{equation} 
which implies that the inverse Fourier transform of $m$ is a function $j : [0,1]\times \RR^d \to \RR^d$ that is in $(C^p(\RR^d))^d$ for any $p\in \NN$ and can be expressed as
\begin{equation}
    \label{eq:j1}
    j(t,x) = (2\pi)^{-d} \int_{\RR^d} e^{-ik\cdot x} m(t,k) dk = \EE\left(\partial_t I(t,x_0,x_1)|x_t = x\right) \rho(t,x) \equiv v(t,x) \rho_t(x)
\end{equation}
where the last equality follows from the definition of $v_t$ in~\eqref{eq:v:def}. We deduce that $v\in C^0([0,1];(C^p(\RR^d))^d)$ for any $p\in \NN$ since $j\in C^0([0,1];(C^p(\RR^d))^d)$ and $\rho\in C^1([0,1];C^p(\RR^d))$, and $\rho>0$. 


Finally, let us establish~\eqref{eq:vt:wt:bounded}. By~\eqref{eq:It:L2} we  have
\begin{equation}
    \label{eq:v:L2}
    \begin{aligned}
        \int_{\RR^d} |v(t,x)|^2 \rho(t,x) dx & = \int_{\RR^d}|\EE\left(\partial_t I(t,x_0,x_1)|x_t = x\right)|^2 \rho(t,x)dx \\
        &\le \int_{\RR^d} \EE\left(|\partial_t I(t,x_0,x_1)|^2|x_t = x\right) \rho(t,x)dx\\
        &  = \EE\big[|\partial_t I(t,x_0,x_1)|^2\big] < M_1^{1/2}
    \end{aligned} 
\end{equation} 
which establishes the first equation in~\eqref{eq:vt:wt:bounded}. To obtain the second one involving $s$,  notice  that~\eqref{eq:s:def} implies that
\begin{equation}
    \label{eq:s:L2}
    \begin{aligned}
        \forall t\in(0,1) \quad : \quad \int_{\RR^d} |s(t,x)|^2\rho(t,x) dx  & = \frac{2\eps}{t^2(1-t)^2}\int_{\RR^d}|\EE\left(B_t  |x_t=x \right)|^2 \rho(t,x) dx\\
    & \le \frac{2\eps}{t^2(1-t)^2}\int_{\RR^d}\EE[ |B_t|^2] \rho(t,x) dx \\
& = \frac{2\eps }{t(1-t)}
    \end{aligned}
\end{equation}
This shows that $\int_{\RR^d} |s(t,x)|^2\rho(t,x) dx$ is bounded for $t\in(0,1)$ and since it is also continuous at $t=0$ and $t=1$ by~\eqref{eq:rho0:1:sc}, it is bounded for $t\in [0,1]$ and therefore integrable on this interval and the second equation~\eqref{eq:vt:wt:bounded} holds.
\end{proof}

\subsection{Proof of Theorem~\ref{prop:interpolate_losses}}
\label{app:proof:interpolate_losses}
\interpolatelosses*
\begin{proof}
    By definition of $\rho$, the objective $\mathcal{L}_v$ defined in~\eqref{eq:obj:v} 
% is quadratic in $\hat v$ and bounded below by
% \begin{equation}
% \label{eq:lower:b}
%     - \tfrac12 \int_0^1 \EE \big [ |\partial_t I_t(x_0,x_1)|^2 \big] > -\infty
% \end{equation}
% where we used~\eqref{eq:It:L2}. Therefore its minimum exists. To investigate its minimizers, notice that , this objective 
can also be written as 
\begin{equation}
    \label{eq:EL:v}
    \begin{aligned}
    \mathcal{L}_v[\hat v] &= \int_{\RR^d} \left( \tfrac12|\hat v(t,x)|^2 - \EE\left(\partial_t I(t,x_0,x_1)|x_t=x\right)\cdot \hat v(t,x)\right) \rho(t,x) dx\\
    &= \int_{\RR^d} \left( \tfrac12 |\hat v(t,x)|^2 - v(t,x)\cdot \hat v(t,x) \right) \rho(t,x) dx
    \end{aligned}
\end{equation}
where we used the definition of $v$ in~\eqref{eq:v:def}. This quadratic objective is bounded from below since
\begin{equation}
    \label{eq:EL:v:b}
    \begin{aligned}
    \mathcal{L}_v[\hat v] &= \tfrac12 \int_{\RR^d} \left|\hat v(t,x) - v(t,x) \right|^2 \rho(t,x) dx - \tfrac12  \int_{\RR^d} \left|v(t,x) \right|^2 \rho(t,x) dx\\
    & \ge - \tfrac12  \int_{\RR^d} \left|v(t,x) \right|^2 \rho(t,x) dx >-\infty
    \end{aligned}
\end{equation}
where the last inequality follows from the first equation in~\eqref{eq:vt:wt:bounded}. Since $\rho_t$ is positive the  minimizer of~\eqref{eq:EL:v} is unique and given by $\hat v= v$. Similarly, the objective $\mathcal{L}_s$ defined in~\eqref{eq:obj:s}  
can also be written as 
\begin{equation}
    \label{eq:EL:s}
    \begin{aligned}
     \mathcal{L}_s[\hat s] &= \int_{\RR^d} \left( \tfrac12|\hat s(t,x)|^2 + \frac{\eps}{t(1-t)}\EE\left(B_t|x_t=x\right)\cdot \hat s(t,x)\right) \rho(t,x) dx\\
    &= \int_{\RR^d} \left( \tfrac12 |\hat s(t,x)|^2 - s(t,x)\cdot \hat s(t,x) \right) \rho(t,x) dx
    \end{aligned}
\end{equation}
where we used the definition of $s$ in~\eqref{eq:s:def}. This quadratic objective is bounded from below since
\begin{equation}
    \label{eq:EL:s:b}
    \begin{aligned}
    \mathcal{L}_s[\hat s] &= \tfrac12 \int_{\RR^d} \left|\hat s(t,x) - s(t,x) \right|^2 \rho(t,x) dx - \tfrac12  \int_{\RR^d} \left|s(t,x) \right|^2 \rho(t,x) dx\\
    & \ge - \tfrac12  \int_{\RR^d} \left|s(t,x) \right|^2 \rho(t,x) dx >-\infty
    \end{aligned}
\end{equation}
where the last inequality follows from the second equation in~\eqref{eq:vt:wt:bounded}. Since $\rho_t$ is positive the  minimizer of \eqref{eq:EL:s} is unique and given by $\hat s = s$. 

% To establish that the objective~\eqref{eq:EL:v} can also be expressed by the second expectation in this equation, use the SDE~\eqref{eq:bb:sde} for $B_t$ to write
% \begin{equation}
%     \label{eq:equiv}
%      \int_0^1 \EE\frac{B_t\cdot \hat s(t,x_t)}{t(1-t)} dt=-\EE\int_0^1 t^{-1} \hat s(t,x_t)(dB_t - dW_t) =  -\EE\int_0^1 t^{-1}\hat s(t,x_t)dB_t
% \end{equation}
% where the last equality follows from the fact that $\int_0^1 t^{-1} \hat s(t,x_t) dW_t) $ is a martingale.


 

\end{proof}

\subsection{Proof of Corollary~\ref{prop:generative}}
\label{app:proof:generative}

\generative*
\begin{proof}
    The SDE~\eqref{eq:sde:1} and the ODE~\eqref{eq:ode:1} are the evolution equations for the processes whose PDF solve~\eqref{eq:fpe} and~\eqref{eq:transport}. The one equation that requires some explaining in the backward SDE~\eqref{eq:sde:R}.  This backward SDE be solved backward in time from $t=1$ to $t=0$, and by definition its solution is $X^\rev_{t}=Y_{1-t} $ where $Y_t$ solves the forward SDE
\begin{equation}
    \label{eq:sde:generic:rev:Y}
    dY^\fwd_t = -b_\rev(1-t,Y^\fwd_t)dt + \seps d W_t
\end{equation}
to be solved forward in time.
To see how to write the backward It\^o formula~\eqref{eq:ito:formula}  note that given any  $f\in C^1([0,1];C_0^2(\RR^d))$, we have 
\begin{equation}
    \label{eq:Ito:rev:Y}
    \begin{aligned}
    df(1-t,Y^\fwd_t) &= -\partial_t f(1-t,Y^\fwd_t)dt+\nabla f(1-t,Y^\fwd_t) \cdot dY_t + \eps \Delta f(1-t,Y^\fwd_t) dt\\
    &= \left( - b_\rev(1-t,Y^\fwd_t) \cdot \nabla f(1-t,Y^\fwd_t)  + \eps \Delta f(1-t,Y^\fwd_t)\right) dt\\
    & \quad+ \seps \nabla f(1-t,Y^\fwd_t) \cdot dW_t
     \end{aligned}
\end{equation}
In integral form, this equation can be written as
\begin{equation}
    \label{eq:int:Ito:rev:Y}
    \begin{aligned}
    f(1,Y^\fwd_{1}) &= f(1-t,Y^\fwd_{1-t}) \\
    &\quad- \int_{1-t}^1 \left( \partial_t f(1-s,Y^\fwd_s)+ b_\rev(1-s,Y^\fwd_s) \cdot \nabla f(1-s,Y^\fwd_s)  - \eps \Delta f(1-s,Y^\fwd_s)\right) ds \\
    & \quad - \seps \int_{1-t}^1 \nabla f(1-s,Y^\fwd_s) \cdot dW_s. 
     \end{aligned}
\end{equation}
Using $X^\rev_t = Y^\fwd_{1-t}$ and $W^\rev_t = - W_{1-t}$ and changing integration variable from $s$ to $1-s$, this is 
\begin{equation}
    \label{eq:int:Ito:rev:X}
    \begin{aligned}
    f(0,X^\rev_0) & = f(t,X^\rev_t) + \int_t^1 \left( - b_\rev(s,X^\rev_s) \cdot \nabla f(s,X^\rev_s)  + \eps \Delta f(s,X^\rev_s)\right) ds \\
    & \quad - \seps \int_0^t \nabla f(s,X^\rev_s) \cdot dW^\rev_s, 
     \end{aligned}
\end{equation}
In differential form, this is equivalent to saying that 
\begin{equation}
    \label{eq:Ito:rev}
    \begin{aligned}
    df(t,X^\rev_t) &= \partial_t f(t,X^\rev_t)dt + \nabla f(t,X^\rev_t) \cdot dX^\rev_t - \eps \Delta f(X^\rev_t) dt\\
    &= \left( \partial_t f(t,X^\rev_t) +b_\rev(t,X^\rev_t) \cdot \nabla f(t,X^\rev_t)  - \eps \Delta f(t,X^\rev_t)\right) dt + \seps \nabla f(t,X^\rev_t) \cdot dW_t
     \end{aligned}
\end{equation}
which is the backward It\^o formula~\eqref{eq:ito:formula}. Similarly, by the It\^o isometries we have: for any $g\in C^0([0,1];(C_0^0(\RR_d))^d)$ and $t\in[0,1]$
\begin{equation}
\label{eq:ito:iso:Y}
\EE^x \int_{1-t}^1 g(s,Y^\fwd_s) \cdot dW_s = 0, \qquad \EE^x \left|\int_{1-t}^1 g(s,Y^\fwd_s) \cdot dW_s\right|^2 =   \int_{1-t}^1 \EE^x|g (s,Y^\fwd_s) |^2ds.
\end{equation}
Written in terms of $X^\rev_t$, these are~\eqref{eq:ito:iso}.

\end{proof}



\subsection{Proofs of Lemmas~\ref{lemma:kl_transport} and~\ref{lemma:kl_fpe}, and Theorem~\ref{thm:kl:bound}.}
\label{app:proof:kl}
\kltransport*
\begin{proof}
Using~\eqref{eq:2:te}, we compute analytically
\begin{align*}
    \frac{d}{dt}\KL{\rho(t)}{\hat\rho(t)} &= \frac{d}{dt}\int_{\RR^d} \log\left(\frac{\rho}{\hat \rho}\right)\rho dx\\
    %
    %
    &= \int_{\RR^d} \hat\rho\left(\frac{\partial_t \rho}{\hat\rho} - \frac{\rho}{(\hat\rho)^2}\partial_t\hat\rho\right)dx + \int\log\left(\frac{\rho}{\hat\rho}\right)\partial_t\rho dx\\
    %
    %
    &= -\int_{\RR^d} \left(\frac{\rho}{\hat\rho}\right)\partial_t\hat\rho dx + \int_{\RR^d}\log\left(\frac{\rho}{\hat\rho}\right)\partial_t\rho dx\\
    %
    %
    &= \int_{\RR^d} \left(\frac{\rho}{\hat\rho}\right)\nabla\cdot\left(\hat b\hat\rho\right)dx - \int\log\left(\frac{\rho}{\hat\rho}\right)\nabla\cdot\left(b\rho\right)dx\\
    %
    %
    &= -\int_{\RR^d} \nabla\left(\frac{\rho}{\hat\rho}\right)\cdot \hat b\hat\rho dx + \int\left(\nabla\log\rho - \nabla\log\hat\rho\right)\cdot b \rho dx\\
    %
    %
    &= -\int_{\RR^d} \left(\frac{\nabla\rho}{\hat\rho} - \frac{\rho\nabla\hat\rho}{\hat\rho^2}\right)\cdot \hat b\hat\rho dx 
    + \int_{\RR^d}\left(\nabla\log\rho - \nabla\log\hat\rho\right)\cdot b \rho dx\\
    %
    %
    &= \int_{\RR^d} \left(\nabla\log\hat\rho - \nabla\log\rho\right)\cdot \hat b \rho dx + \int_{\RR^d}\left(\nabla\log\rho - \nabla\log\hat\rho\right)\cdot b \rho dx\\
    %
    %
    &= \int_{\RR^d} \left(\nabla\log\hat\rho - \nabla\log\rho\right)\cdot (\hat b - b) \rho dx.
\end{align*}
where we omitted the argument $(t,x)$ of all functions for simplicity of notation. Integrating both sides from $0$ to $1$ completes the proof.
\end{proof}

\klfpe*
\begin{proof}
Similar to the proof of Lemma~\ref{lemma:kl_transport}, we can use the FPE in~\eqref{eq:2:fpe} to compute $\frac{d}{dt}\KL{\rho(t)}{\hat\rho(t)}$, which leads to a proof of the main result. Instead, we take a simpler approach, leveraging the result in Lemma~\ref{lemma:kl_transport}. We re-write the Fokker-Planck equations in~\eqref{eq:2:fpe} as the (score-dependent) transport equations
\begin{equation}
    \label{eqn:sde_transports}
    \begin{aligned}
        \partial_t \rho &= -\nabla\cdot((b_\fwd - \eps\nabla\log\rho)\rho),\\ 
        %
        \partial_t \hat\rho &= -\nabla\cdot((\hat b_\fwd - \eps\nabla\log\hat\rho)\hat\rho).
    \end{aligned}
\end{equation}
Applying Lemma~\ref{lemma:kl_transport} directly, we find that
\begin{equation}
\begin{aligned}
    \KL{\rho(1)}{\hat \rho(1)} = \int_0^1 \int_{\RR^d} \left[\left(\nabla\log\hat\rho  - \nabla\log\rho \right)\cdot \left(\left[\hat b_\fwd - \eps\nabla\log\hat\rho \right] - \left[b_\fwd - \eps\nabla\log\rho \right]\right)\right] \rho dx dt.
\end{aligned}
\end{equation}
Expanding the above,
\begin{equation}
\begin{aligned}
    \KL{\rho(1) }{\hat\rho(1) } &= \int_0^1 \int_{\RR^d}\left[\left(\nabla\log\hat\rho  - \nabla\log\rho \right)\cdot (\hat b_\fwd - b_\fwd)\right]\rho dx dt\\
    &\qquad  - \eps\int_0^1 \int_{\RR^d} \left[\norm{\nabla\log\rho  - \nabla\log\hat\rho }^2\right]\rho dx dt,
\end{aligned}
\end{equation}
which proves the lemma.
\end{proof}

\likelihoodbound*
\begin{proof}
Observe that by Proposition~\ref{prop:generative}, the target density $\rho_1 =\rho(1, \cdot)$ is the density of the process $X_t$ that evolves according to SDE~\eqref{eq:sde:1}. By Lemma~\ref{lemma:kl_fpe}, we then have that 
\begin{equation}
\begin{aligned}
    \KL{\rho_1}{\hat\rho(1)} &= \int_0^1 \int_{\RR^d} \left(\nabla\log\hat{\rho} - \nabla\log\rho\right)\cdot \left(\left[\hat{v} - 2\epsilon t \hat{s}\right] - \left[v - 2\epsilon t s\right]\right)\rho_tdx dt\\
    &\qquad  - \eps\int_0^1 \int_{\RR^d} \norm{\nabla\log\rho_t - \nabla\log\hat{\rho}_t}^2\rho_tdxdt.
\end{aligned}
\end{equation}
By Young's inequality, it holds for any fixed $\eta > 0$ that
\begin{align}
    \KL{\rho_1}{\hat\rho(1)} &\leq \int_0^1 \int_{\RR^d}\left(\frac{\eta}{2}\norm{\nabla\log\hat{\rho} - \nabla\log\rho}^2 + \frac{1}{2\eta}\norm{\hat{v} - v + 2\epsilon t (s - \hat{s})}^2\right)\rho dx dt\nonumber\\
    &\qquad  - \eps\int_0^1 \int_{\RR^d} \norm{\nabla\log\rho - \nabla\log\hat{\rho}}^2\rho dx dt,\\
    %
    %
    &= \int_0^1 \frac{1}{2\eta}\int_{\RR^d}\norm{\hat{v} - v + 2 \epsilon t (s - \hat{s})}^2\rho_tdx dt\nonumber\\
    &\qquad  + \left(\tfrac{1}{2}\eta - \eps\right)\int_0^1 \int_{\RR^d} \norm{\nabla\log\rho_t - \nabla\log\hat{\rho}_t}^2\rho_tdxdt.
\end{align}
Hence, for $\eta =2\eps$, 
\begin{align}
    \KL{\rho_1}{\hat\rho(1)} &\leq \frac{1}{4\eps} \int_0^1 \int_{\RR^d}\norm{\hat{v} - v + 2 \epsilon t (s - \hat{s})}^2\rho dxdt.
\end{align}
Again by Young's inequality, it holds that
\begin{align}
    \KL{\rho_1}{\hat\rho(1)} &\leq \frac{1}{2\eps}\int_0^1\int_{\RR^d}\left(\norm{\hat{v} - v}^2 + 4\epsilon^2 t^2\norm{s - \hat{s}}^2\right)\rho_tdxdt\nonumber\\
    %
    %
    &\leq \frac{1}{2\eps}\int_0^1\int_{\RR^d}\left(\norm{\hat{v} - v}^2 + 4\epsilon^2\norm{s - \hat{s}}^2\right)\rho_tdxdt
\end{align}
where the last line follows because $t \leq 1$. Applying~\eqref{eq:pdf:def:test},~\eqref{eq:v:def}, and~\eqref{eq:s:def}, we conclude that
\begin{equation}
    \KL{\rho_1}{\hat{\rho}(1)} \leq \frac{1}{2\eps}\left(\mathcal{L}_v[\hat{v}] - \min_{\hat{v}}\mathcal{L}_v[\hat{v}]\right)  + 2\eps\left(\mathcal{L}_s[\hat{s}] - \min_{\hat{s}}\mathcal{L}_s[\hat{s}]\right),
\end{equation}
which is~\eqref{eq:bound:kl}.

\end{proof}


\subsection{Derivation of~\eqref{eq:te:sol} and proof of Theorem~\ref{prop:sde:rho}}


We begin by deriving \eqref{eq:te:sol}. If $\rho$ solves the TE~\eqref{eq:transport} and $ X_t$ solves the ODE~\eqref{eq:ode:1}, we have
\begin{equation}
    \label{eq:te:sol:1}
    \begin{aligned}
        \frac{d}{dt} \rho(t,X_t) & = \partial_t  \rho(t, X_t) + \frac{dX_t}{dt} \cdot \nabla  \rho(t,X_t)\\
        & = \partial_t  \rho(t,X_t) + b_\ODE(t,X_t)  \cdot \nabla  \rho(t,X_t)\\
        &= - \nabla \cdot b_\ODE(t, X_t) \rho(t,X_t)
    \end{aligned}
\end{equation}
This implies that
\begin{equation}
    \label{eq:te:sol:2}
    \begin{aligned}
        \frac{d}{dt} \left( \exp\left( \int_0^t\nabla \cdot b_\ODE(\tau,X_\tau) d\tau \right) \rho(t,X_t) \right) = 0.
    \end{aligned}
\end{equation}
Integrating this equation from $t=0$ to $t=1$ gives~\eqref{eq:te:sol}.


\FK*
\begin{proof}
From the definition of the forward and backward drifts in~\eqref{eq:b:def} and~\eqref{eq:b:r:def}, and the definition of $s$ in~\eqref{eq:s:score} we have:
\begin{equation}
    \label{eq:2:id}
      \qquad b_\fwd(t, x) - b_\rev(t, x) = 2\epsilon s(t, x), \qquad \nabla\rho(t, x) = s(t, x)\rho(t, x).
\end{equation}
We will use these identities repeatedly below.
\paragraph{Equation~\eqref{eq:fk}.}  The identies in~\eqref{eq:2:id} enable us to write the FPE~\eqref{eq:fpe} as
\begin{equation}
    \label{eq:pde:r}
    \begin{aligned}
        \partial_t \rho + b_\rev \cdot \nabla \rho - \eps \Delta \rho(t, x) & = -\left(\nabla\cdot b_\fwd + 2\epsilon |s|^2\right)\rho.
    \end{aligned}
\end{equation}
Now, we compute $\rho(t, X^\rev_t)$ via the backward It\^o formula~\eqref{eq:ito:formula} to obtain
\begin{equation}
    \label{eq:ito:rho:1}
    \begin{aligned}
    d\rho(t, X_t^\rev) &= \partial_t \rho(t, X_t^\rev) dt + \nabla \rho(t, X_t^\rev) \cdot dX^\rev_t - \eps \Delta \rho_t(X_t^\rev) dt\\
    &= \partial_t \rho(t, X_t^\rev) dt + \nabla \rho(t, X_t^\rev) \cdot b_\rev(t, X^\rev_t)dt + \seps \nabla \rho(t, X_t^\rev) \cdot dW^\rev_t - \eps \Delta \rho(t, X_t^\rev) dt\\
    & = -\left(\nabla \cdot b_\fwd(t, X^\rev_t) + 2\epsilon |s(t, X_t^\rev)|^2\right)  \rho(t, X^\rev_t) dt + \seps \nabla \rho(t, X_t^\rev) \cdot dW^\rev_t.
    \end{aligned}
\end{equation}
where we used~\eqref{eq:pde:r} in the last step. This equation can also be written as
\begin{equation}
    \label{eq:ito:rho:2}
    \begin{aligned}
    & d \left(\exp\left(- \int_t^1\left(\nabla \cdot b_\fwd(\tau, X^\rev_\tau) + 2 \epsilon |s(\tau, X_\tau^\rev)|^2\right) d\tau\right)  \rho(t, X_t^\rev) \right) \\
    & =  \seps \exp\left(-\int_t^1\left(\nabla \cdot b_\fwd(\tau, X^\rev_\tau) + 2\epsilon|s(\tau, X_\tau^\rev)|^2\right) d\tau\right) \nabla \rho(t, X_t^\rev) \cdot dW^\rev_t,
    \end{aligned}
\end{equation}
which after integration on $t\in[0,1]$ becomes
\begin{equation}
    \label{eq:ito:rho:3}
    \begin{aligned}
    & \rho(1, X_{t=1}^\rev) - \exp\left(- \int_0^1\left(\nabla \cdot b_\fwd(t, X^\rev_t) + 2\epsilon|s(t, X_t^\rev)|^2\right) dt\right)  \rho_0(X_{t=0}^\rev) \\
    %
    %
    & =  \seps \int_0^1 \exp\left(-\int_t^1\left(\nabla \cdot b_\fwd(\tau, X^\rev_\tau) + 2\epsilon|s(\tau, X_\tau^\rev)|^2\right) d\tau\right) \nabla \rho(t, X_t^\rev) \cdot dW^\rev_t.
    \end{aligned}
\end{equation}
Taking an expectation conditioned on the event $X_{t=1}^\rev=x$ and using that the term on the right-hand side has mean zero, we find that
\begin{equation}
    \label{eq:ito:rho:4}
    \rho(1, x) = \EE_\rev^x \left(\exp\left(- \int_0^1\left(\nabla \cdot b_\fwd(t, X^\rev_t) + 2\epsilon|s(t, X_t^\rev)|^2\right) dt\right)  \rho(0, X_{t=0}^\rev)\right).
\end{equation}
Since $\rho(0)=\rho_0$ and $\rho(1)=\rho_1$, we arrive at ~\eqref{eq:fk}.

\paragraph{Equation~\eqref{eq:ito:rho:1:log:int}.} We notice by an analogous procedure, applying $s(t, x) = \nabla\log\rho(t, x)$, that
\begin{equation}
    \label{eq:ito:rho:1:log}
    \begin{aligned}
    d \log \rho(t, X_t^\rev) &= \rho(t, X_t^\rev)^{-1}\left(\partial_t \rho(t, X_t^\rev) dt +  \nabla \rho(t, X_t^\rev) \cdot dX^\rev_t - \eps \Delta \rho(t, X_t^\rev) \right)dt + \epsilon|s(t, X_t^\rev)|^2 dt\\
    %
    %
    &= \rho(t, X_t^\rev)^{-1}\left(\partial_t \rho(t, X_t^\rev) dt + \nabla \rho(t, X_t^\rev) \cdot b_\rev(t, X^\rev_t)dt - \eps \Delta \rho(t, X_t^\rev) dt \right) \\
    &\quad + \epsilon |s(t, X_t^\rev)|^2 dt + \sqrt{2\epsilon} s(t, X_t^\rev) \cdot dW^\rev_t\\
    %
    %
    &= -\left(\nabla \cdot b_\fwd(t, X^\rev_t) + \epsilon|s(t, X_t^\rev)|^2\right)dt + \sqrt{2\epsilon} s(t, X_t^\rev) \cdot dW^\rev_t,
    \end{aligned}
\end{equation}
where we applied~\eqref{eq:pde:r} to obtain the last line. Integrating this equation over $t\in[0,1]$, taking the expectation of the result conditional on $X_{t=1}^\rev=x$, and using $\rho(0)=\rho_0$ and $\rho(1)=\rho_1$ gives~\eqref{eq:ito:rho:1:log:int}.

\paragraph{Equation~\eqref{eq:fk:rev}.} We now proceed similarly, making use of the forward-time SDE and backward FPE, rather than the backward-time SDE and forward-time FPE. We write the backward FPE~\eqref{eq:fpe:tr} as 
\begin{equation}
    \label{eq:pde}
    \begin{aligned}
        \partial_t \rho(t, x) + b_\fwd(t, x)\cdot\nabla\rho(t, x) + \epsilon \Delta \rho(t, x) = -\left(\nabla\cdot b_\rev(t, x) - 2\epsilon |s(t, x)|^2\right)\rho(t, x).
    \end{aligned}
\end{equation}
Evaluating $\rho(t, X^\fwd_t)$ via the It\^o formula, we obtain
\begin{equation}
    \label{eq:ito:rho:r:1}
    \begin{aligned}
    d \rho(t, X^\fwd_t) &= \partial_t \rho(t, X^\fwd_t) dt + \nabla \rho(t, X^\fwd_t) \cdot dX^\fwd_t + \epsilon \Delta \rho(t, X^\fwd_t) dt\\
    %
    %
    &= \partial_t \rho(t, X^\fwd_t) dt + \nabla \rho(t, X^\fwd_t) \cdot b_\fwd(t, X^\fwd_t)dt + \seps \nabla \rho(t, X^\fwd_t) \cdot dW_t + \eps \Delta \rho(t, X^\fwd_t) dt\\
    %
    %
    & = -\left(\nabla \cdot b_\rev(t, X^\fwd_t) - 2\epsilon|s(t, X^\fwd_t)|^2\right)  \rho_t(X^\fwd_t) dt + \seps \nabla \rho_t(X^\fwd_t) \cdot dW_t.
    \end{aligned}
\end{equation}
where we used~\eqref{eq:pde} in the last step. This equation can be written as a total differential in the form
\begin{equation}
    \label{eq:ito:rho:r:2}
    \begin{aligned}
    & d\left(\exp\left( \int_0^t\left(\nabla \cdot b_\rev(\tau, X^\fwd_\tau) - 2\epsilon|s(\tau, X^\fwd_\tau)|^2\right) d\tau\right)  \rho(t,X^\fwd_t) \right) \\
    %
    %
    & =  \seps \exp\left( \int_0^t\left(\nabla \cdot b_\rev(\tau, X^\fwd_\tau) - 2\epsilon|s(\tau, X^\fwd_\tau)|^2\right) d\tau\right) \nabla \rho(t, X^\fwd_t) \cdot dW_t.
    \end{aligned}
\end{equation}
Integrating the above on $t\in[0,1]$, we find that
\begin{equation}
    \label{eq:ito:rho:r:3}
    \begin{aligned}
    & \exp\left( \int_0^1\left(\nabla \cdot b_\rev(t, X^\fwd_t) - 2\epsilon|s(t, X^\fwd_t)|^2\right) dt\right)\rho(1, X_1) -   \rho(0, X_{t=0})\\
    %
    %
    &= \seps \int_0^1 \exp\left( \int_0^t\left(\nabla \cdot b_\rev(\tau, X^\fwd_\tau) - 2\epsilon|s(\tau, X^\fwd_\tau)|^2\right) d\tau\right) \nabla \rho(t,X^\fwd_t) \cdot dW_t.
    \end{aligned}
\end{equation}
Taking an expectation conditioned on the event $X_{t=0} = x$ and applying the It\^o isometry, we deduce that
\begin{equation}
    \label{eq:ito:rho:r:4}
    \rho(0, x) = \EE_\fwd^x \left(\exp\left( \int_0^1\left(\nabla \cdot b_\rev(t, X^\fwd_t) - 2\epsilon|s(t, X^\fwd_t)|^2\right) dt\right)\rho(1, X_{t=1})\right).
\end{equation}
Since $\rho(0)=\rho_0$ and $\rho(1)=\rho_1$, we arrive at ~\eqref{eq:fk:rev}.

\paragraph{Equation~\eqref{eq:ito:rho:r:1:log:int}.} As for the previous result, we now notice that
\begin{equation}
    \label{eq:ito:rho:r:1:log}
    \begin{aligned}
    d \log \rho(t, X^\fwd_t) &= \rho(t, X^\fwd_t)^{-1} \left(\partial_t \rho(t, X^\fwd_t) dt + \nabla \rho(t, X^\fwd_t) \cdot dX^\fwd_t + \eps \Delta \rho(t, X^\fwd_t) \right) dt - \epsilon|s(t, X^\fwd_t)|^2 dt\\
    %
    %
    &=  \rho(t, X^\fwd_t)^{-1} \left( \partial_t \rho(t, X^\fwd_t) dt + \nabla \rho(t, X^\fwd_t) \cdot b_\fwd(t, X^\fwd_t)dt + \eps \Delta \rho(t, X^\fwd_t) \right) dt \\
    &\quad - \epsilon|s(t, X^\fwd_t)|^2 dt+ \sqrt{2\epsilon}s(t, X^\fwd_t)\cdot dW_t\\
    %
    %
    & = -\left(\nabla \cdot b_\rev(t, X^\fwd_t) - \epsilon |s(t, X^\fwd_t)|^2\right) dt + \sqrt{2\epsilon}s(t, X^\fwd_t)\cdot dW_t.
    \end{aligned}
\end{equation}
Integrating this equation over $t \in [0,1]$, taking the expectation of the result conditioned on the event $X^\fwd_{t=0} = x$, and using $\rho(0)=\rho_0$ and $\rho(1)=\rho_1$ gives~\eqref{eq:ito:rho:r:1:log:int}.

\end{proof}

\subsection{Proof of Theorems~\ref{prop:ce:sde} and~\ref{prop:ce:ode}}
\label{app:ce}

\crossentsde*
\begin{proof}
    By definition we have
\begin{equation}
    \label{eq:ce:1}
    H(\rho_1|\hat \rho(1) ) = - \EE_1 \log\hat \rho(1,x_1)
\end{equation}
where $\EE_1$ denotes expecation on $x_1\sim\rho_1$. 
Proceeding as in the proof of Theorem~\ref{prop:sde:rho} we know that we can express the solution to the FPE~\eqref{eq:fpe:hat} at time $t=1$ and any $x\in \RR^d$ in terms of the solution to the backward SDE~\eqref{eq:sde:R:hat} as
\begin{equation}
    \label{eq:fpe:hat:sol}
    \log \hat \rho(1,x) = \EE^{x}_\rev\log \rho_0(\hat X_{t=0}^\rev)+ \int_0^1 \EE^{x}_\rev \left(\nabla \cdot \hat b_\fwd(t, \hat X^\rev_t) + \epsilon|s(t, \hat X_t^\rev)|^2\right)dt,
\end{equation}
where $\EE_\rev^x$ denotes the expectation on $\hat X_t^\rev$ conditional on $\hat X_{t=1}^\rev=x$ and $\hat b = \hat v+2\eps t \hat s$. Inserting~\eqref{eq:fpe:hat:sol} in~\eqref{eq:ce:1} gives~\eqref{eq:ce:sde}.
\end{proof}

\crossentode*
\begin{proof}
    By definition we have
\begin{equation}
    \label{eq:ce:2}
    H(\rho_1|\check \rho(1) ) = - \EE_1 \log\check \rho(1,x_1)
\end{equation}
where $\EE_1$ denotes expectation on $x_1\sim\rho_1$. 
Proceeding as in the proof of Theorem~\ref{prop:sde:rho} we know that we can express the solution to the TE~\eqref{eq:te:hat} at time $t=1$ and any $x_1\in \RR^d$ in terms of the solution to the ODE~\eqref{eq:ode:hat} as
\begin{equation}
    \label{eq:te:hat:sol}
    \log \check \rho(1,x_1) = \log \rho_0(\hat X^{x_1}_{t=0})- \int_0^1  \nabla \cdot \hat b_{\ODE}(t, \hat X^{x_1}_t) dt,
\end{equation}
where  $\hat X^{x_1}_t$ denotes the solution to~\eqref{eq:ode:hat}  with the final condition $\hat X^{x_1}_{t=1} = x_1$. Inserting~\eqref{eq:te:hat:sol} in~\eqref{eq:ce:2} gives~\eqref{eq:ce:ode}.
\end{proof}

\subsection{Proof of Theorem~\ref{prop:sb}.}
\label{app:sb}

\sb*
\begin{proof}
Denoting by $\hat\rho(t,x) $ be the PDF of $\hat x_t$ and define the current$\hat\jmath : [0,1]\times \RR^d \to \RR^d$ as
\begin{equation}
    \label{eq:def:rho:j}
        \hat \jmath(t,x) = \EE \big(\partial_t \hat I_t - \seps (1-t)^{-1}B_t| x=x_t \big) \rho(t,x)
\end{equation}
In terms of this density and this current, the max-min problem~\eqref{eq:max:min} can be formulated as the constrained optimization problem (this assume that we can represent any $\rho$ with the stochastic interpolant, i.e. the density is interpolable in the sense of Ref.~\cite{albergo2023building}) \nb{In~\eqref{eq:max:min:rho:j}, should it be $\hat{u}_t\cdot\hat\jmath_t$?}
\begin{equation}
    \label{eq:max:min:rho:j}
    \begin{aligned}
        &\max_{\hat \rho,\hat \jmath} \min_{\hat u} \int_0^1 \int_{\RR^d}\left( \tfrac12|\hat u(t,x)|^2 \hat \rho(t,x)-  \hat u(t,x)\cdot \hat \jmath(t,x) \right) dx dt\\
        \text{subject to:} \quad & \partial_t \hat \rho + \nabla \cdot \hat \jmath= \eps  \Delta \hat\rho, \quad \hat\rho(t=0) = \rho_0, \quad \hat \rho(t=1)= \rho_1
    \end{aligned}
\end{equation}
To solve this problem we can use the extended objective
\begin{equation}
    \label{eq:max:min:rho:j:2}
    \begin{aligned}
        \max_{\hat \rho,\hat \jmath} \min_{\hat u} &\Bigg(\int_0^1 \int_{\RR^d}\left( \tfrac12|\hat u(t,x)|^2 \hat \rho(t,x)-  \hat u(t,x)\cdot \hat \jmath(t,x) \right) dx dt\\
        & -\int_0^1 \int_{\RR^d}\lambda(t,x) \left(\partial_t \hat \rho(t,x) + \nabla \cdot \hat \jmath(t,x) - \eps  \Delta \hat\rho(t,x)\right) dx dt \\
        & +\int_{\RR^d} \eta_0(x)\left(\hat\rho(0,x)-\rho_0(x)\right) dx-\int_{\RR^d} \eta_1(x)\left(\hat\rho(1,x)-\rho_1(x)\right) dx \Bigg)
    \end{aligned}
\end{equation}
where $\lambda(t,x)$, $\eta_0(x)$, and $\eta_1(x)$ are Lagrange multipliers used to enforce the constraints. The unique minimizer $(\rho,j,\lambda)$ of this optimization problem solves the Euler-Lagrange equations:
\begin{equation}
    \label{eq:max:min:rho:j:el}
    \begin{aligned}
        & \partial_t \rho + \nabla \cdot j = \eps  \Delta \rho, \quad \rho(t=0) = \rho_0\quad \rho(t=1) = \rho_1\\
        & \partial_t \lambda + \tfrac12 |u|^2 = -\eps \Delta \lambda,\\
        & j = u \rho \\
        & u = \nabla \lambda
    \end{aligned}
\end{equation}
We can use the last two equations to write the first two as \eqref{eq:max:min:rho:j:el:2}, with $u=\nabla \lambda$.
\end{proof}

\subsection{Proof of Theorems~\ref{thm:interp:os} and~\ref{prop:generative:os}, and Corollary~\ref{prop:generative:os}} 

\interpos*
\begin{proof}
The proof is similar to that of Theorem~\ref{prop:interpolate}, so we only give a few main steps. 
The generating function of the process $y_t = J_t(x_0) + \sigma_1 W_t$ reads
\begin{equation}
    \label{eq:gen:os}
    g(t,k) = \EE e^{ik\cdot y_t} =  g_0(t,k) \EE e^{ik\cdot \sigma_1 W_t} = g_0(t,k) e^{-\tfrac12 t k^T C_1 k }
\end{equation}
where we used the independence of $x_0\sim \rho_0$ and $W_t$ and we defined
\begin{equation}
    \label{eq:gen:0:os}
    g_0(t,k) = \EE e^{ik\cdot J_t(x_0)}
\end{equation}
Taking the expectation of the differential of $e^{ik\cdot y_t}$ computed using It\^o formula,  we arrive as
\begin{equation}
    \label{eq:gtk:os}
    \partial_t g = ik \cdot \EE \left(\partial J_t(x_0) e^{ik\cdot y_t} \right) - \tfrac12 k^T C_1 k g.
\end{equation}
Writing this equation in real-space using the property of the conditional expectation gives the forward FPE~\eqref{eq:fpe:os} with the drift defined in~\eqref{eq:b:def:os} in terms of the velocity $w$ in~\eqref{eq:w:def:os}. The backward FPE~\eqref{eq:fpe:tr:os} and the TE~\eqref{eq:transport:os} follows from~\eqref{eq:fpe:os}, as long as~\eqref{eq:score:os} holds. To prove this last equation,   notice that 
\begin{equation}
    \label{eq:exp:W}
     \sigma_1 \EE \left(W_t e^{ik\cdot \sigma_1 W_t}\right) =  -i\partial_k  \EE e^{ik\cdot \sigma_1 W_t} = -i\partial_k e^{-\tfrac12 t k^T C_1 k } 
     = -i t C_1k e^{-\tfrac12 t k^T C_1 k }
\end{equation}
This means that
\begin{equation}
    \label{eq:exp:g}
     \sigma_1 \EE \left(W_t e^{ik\cdot y_t}\right) 
     = -i t C_1k g(t,k)
\end{equation}
Now use
\begin{equation}
    \label{eq:exp:g:2}
     \begin{aligned}
         \sigma_1 \EE \left(W_t e^{ik\cdot y_t}\right) &=\sigma_1 \int_{\RR^d} \EE \left(W_t e^{ik\cdot y_t}|y_t = x\right) \rho(t,x) dx\\ 
     & = \sigma_1 \int_{\RR^d} \EE \left(W_t |y_t = x\right)  e^{ik\cdot x}\rho(t,x) dx
     \end{aligned} 
\end{equation}
and
\begin{equation}
    \label{eq:exp:g:3}
    -i t C_1k g(t,k) = \int_{\RR^d}  e^{ik\cdot x} t C_1 \nabla \rho(t,x) dx  = \int_{\RR^d}  e^{ik\cdot x} C_1 \nabla \log \rho(t,x) \, \rho(t,x) dx 
\end{equation}
Inserting~\eqref{eq:exp:g:2} and \eqref{eq:exp:g:3} in~\eqref{eq:exp:g} we deduce that 
\begin{equation}
    \label{eq:exp:g:real}
     \sigma_1 \EE \left(W_t e^{ik\cdot y_t}\right) = t C_1 \nabla \log \rho(t,x).
\end{equation}
Using $C_1^{-1} \sigma_1 = \sigma_1^{-\T}$ this equation implies that the velocity $s$ defined in \eqref{eq:s:def:os} is the score, i.e. \eqref{eq:score:os} holds.To prove the regularity statements about $\rho$, $w$, and $s$, as well as \eqref{eq:vt:wt:bounded:os}, we can proceed as in the proof of Theorem~~\ref{prop:interpolate}
\end{proof}

\objos*
\begin{proof}
The proof is similar to that of Theorem~\ref{prop:interpolate_losses} and uses the fact that the objectives in~\eqref{eq:obj:w:os} can be wriiten as
\begin{equation}
    \label{eq:obj:w:os:2}
    \mathcal{L}_w[\hat{w}] =\int_0^1   \int_{\RR^d}  \left( \tfrac12|\hat w(t,x)|^2 - w(t,x)\cdot \hat w(t,x) \right) \rho(t,x) dx dt
\end{equation}
and
\begin{equation}
    \label{eq:obj:s:os:2}
    \mathcal{L}_s[\hat{s}] = \int_0^1 \int_{\RR^d} \EE\left( \tfrac12|\hat s(t,x)|^2 - s(t,x) \cdot \hat s(t,x) \right)\rho(t,x) dx  dt
% =\EE\int_0^1\left( \tfrac12|\hat s(t,x_t)|^2 dt -(\seps \, t)^{-1} \hat s(t,x_t)dB_t \right)
\end{equation}
\end{proof}

\genos*
\begin{proof}
    The proof is similar to that of Corollary~\ref{prop:generative}, with the same rules for interpreting the backward SDE~\eqref{eq:sde:R:os}.
\end{proof}

\subsection{Proof of Theorem~\ref{th:Gauss:mixt}}
\label{app:gausmixt}

\gaussmixt*
\begin{proof} The characteristic function of $\rho(t,x)$ is given by
\begin{equation}
    \label{eq:rhot:Gaussmixt:k}
    g(t,k)  = \EE e^{ik\cdot x_t} = \sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  e^{ik\cdot m_{ij}(t) - \tfrac12 k^\T C_{ij}^\eps(t) k } .
\end{equation}
whose inverse Fourier transform is~\eqref{eq:rhot:Gaussmixt}. This automatically implies~\eqref{eq:st:Gausmixt} since we know from~\ref{eq:s:score} that  $s= \nabla \log \rho$, but here we will re-derive this result using the functions $m$ and $n$ defined in~\eqref{eq:F12k}. They are
\begin{equation}
    \label{eq:m:Gaussmixt:k}
    m(t,k)  = \sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  (\dot m_{ij}(t) +\tfrac12 i \dot C_{ij}(t) k)e^{ik\cdot m_{ij}(t) - \tfrac12 k^\T C_{ij}^\eps(t) k } .
\end{equation}
and
\begin{equation}
    \label{eq:n:Gaussmixt:k}
    n(t,k)  = 2\eps t ik\sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  e^{ik\cdot m_{ij}(t) - \tfrac12 k^\T C_{ij}^\eps(t) k } .
\end{equation}
From~\eqref{eq:F1k:c} and~\eqref{eq:jt2}, we know that the inverse Fourier transform of these functions are $v\rho$ and $2\eps t s \rho$, respectively, so that we obtain
\begin{equation}
    \label{eq:m:Gaussmixt:x}
    v(t,x) \rho(t,x) = \sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  \left(\dot m_{ij}(t) + \tfrac12\dot C_{ij}(t) [C_{ij}^\eps]^{-1}(t)(x-m_{ij}(t))\right) N(x| m_{ij}(t),C^\eps_{ij}(t)).
\end{equation}
and
\begin{equation}
    \label{eq:n:Gaussmixt:x}
    2\eps t s(t,x) \rho(t,x) = -2\eps t \sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  [C_{ij}^\eps]^{-1}(t)(x-m_{ij}(t))N(x| m_{ij}(t),C^\eps_{ij}(t)).
\end{equation}
This gives~\eqref{eq:vt:Gausmixt} and \eqref{eq:st:Gausmixt}.
\end{proof}



% \section{Description of the Generalized Interpolant}
% We again consider the task of learning a velocity field $v_t$ whose integration pushes an arbitrary base measure $\rho_0$ onto a target measure $\rho_1$ while satisfying the continuity equation. However we promote the contiuity equation to include a stochastic term. In this case, we seek to learn a velocity field which satisfies
% \begin{align}
% \label{eq:continuity}
%     \partial_t \rho_t + \nabla \cdot (v_t\rho_t) = \frac{\epsilon^2}{2} \Delta \rho_t (x)  \\
%     \text{with } \quad \rho_{t=0} = \rho_0 \quad  \text{ and } \quad \rho_{t=1} = \rho_1
% \end{align}
% When the right hand side of \eqref{eq:continuity} is 0, the pushforward of $\rho_0$ on to $\rho_1$  by the map $X_t(x) \sim \rho_t$ is given by the ODE:
% \begin{align*}
%     \dot X_t(x) = v_t(X_t(x)), \quad X_{t=0} = x.
% \end{align*}
% This describes the scenario of \cite{albergo2023building}, where an interpolant function $x_t = I_t (x_0, x_1) \sim \rho_t$ satisfying $I_{t=0} = x_0, I_{t=1} = x_1$ is used to instantiate a probability current connecting the two densities.

% We seek to extend this framework to the form given in \eqref{eq:continuity}, where the interpolant $I_t$ must now be adapted to accommodate the extra term. 

% We fulfill this by the inclusion of a Brownian motion that is conditioned to arrive at 0 at fixed time $t=1$, known as the standard Brownian bridge.

% \section{Some relevant facts about the Brownian Bridge}

% \begin{lemma}
% Let $W_t$ be the standard Wiener process defined up to fixed time $t=1$, then the Brownian bridge process $B_t$ given by
% \begin{align*}
%     B_t = W_t - tW_1
% \end{align*}
% is independent from $\{W_t\}_{t\geq 1}$.
% \end{lemma}

% \begin{lemma}
%     A stochastic process $\{B_t\}_{t\in [0,1]}$ is a Brownian bridge iff it is joint normal with zero mean and covariances given by
%     \begin{align}
%         \mathbb E [B_t B_t] = t(1-t)
%     \end{align}
%     for $0 \leq t \leq 1$.
% \end{lemma}
% \textit{Proof.} Because $B_t$ is given in terms of standard Wiener process, it is likewise joint normal and zero mean. The covariance follows directly via
% \begin{align}
%     \mathbb E [B_tB_t] &= \mathbb E[ (W_t - t W_1)(W_t - tW_1)] \\
%     & = \mathbb E [W_t^2 - 2tW_1W_t - t^2 W_1 ] \\
%     &= t - t^2 - t^2 + t^2 \\
%     &= t(1-t)
% \end{align}



