\subsection{Definition and Assumptions}
\label{sec:si:gm}

We begin by defining the stochastic processes that are central to our approach:
\begin{definition}[Stochastic interpolant] 
\label{def:interp}
 Given two probability density functions $\rho_0, \rho_1 : {\RR^d} \rightarrow \RR_{\geq 0}$, a \textit{stochastic interpolant} between these densities is a stochastic process $x_t$ defined as
\begin{equation}
    \label{eq:stochinterp}
    x_t = I(t,x_0,x_1) + \seps B_t,  \qquad t\in [0, 1],
\end{equation}
where: 
\begin{enumerate}[leftmargin=0.15in]
\item $I: [0,1]\times \RR^d\times \RR^d \to \RR^d$ and satisfies the boundary conditions $I(0,x_0,x_1) = x_0$ and $I(1,x_0,x_1) = x_1$;
\item $x_0$ and $x_1$ are random variables drawn independently from $\rho_0$ and $\rho_1$, respectively;
\item $B_t$ is the $d$-dimensional Brownian bridge, realizable as $B_t = W_t - tW_1$ in terms of the $d$-dimensional Wiener process $W_t$, and assumed independent of $x_0$ and $x_1$;
\item $\eps \ge0$ is a fixed parameter setting the strength of the stochastic term;
\end{enumerate}
\end{definition}
Later, we will find it useful to consider spatially nonlinear interpolating function $I$, which we show can recover the solution to the Schr\"odinger bridge problem; nevertheless, a simple example that serves as a valid $I$ in the sense of Definition~\ref{def:interp} is given by the 
\begin{equation}
    \label{eq:lin:interp}
    I(t,x_0,x_1)= \alpha(t) x_0+ \beta(t) x_1  
\end{equation} 
where $\alpha,\beta\in C^1([0,1])$ satisfy 
\begin{equation}
    \label{eq:lin:interp:a:b}
    \alpha(0)=\beta(1)=1; \quad \alpha(1)=\beta(0)=0; \quad \forall t\in[0,1] \ : \ \alpha(t)\ge 0, \ \dot \alpha(t)\le 0, \beta(t)\ge0, \ \dot \beta(t)\ge 0,
\end{equation}
e.g. $\alpha(t) = 1-t$ and $\beta(t)=t$ or $\alpha(t) = \cos(\frac12\pi t)$ and $\beta(t) = \sin(\tfrac12 \pi t)$. In Sec.~\ref{app:coll} we list a collection of possible choice of $\alpha$ and $\beta$ in~\eqref{eq:lin:interp} and discuss some of the properties of the corresponding process~$x_t$. In Sec.~\ref{app:onesided} we also discuss a simpler interpolant construction that is possible when $\rho_1$ is a Gaussian PDF.
% \begin{equation}
%     \label{eq:interp:trig}
%     I(t,x_0,x_1) = \cos(\tfrac12\pi t) x_0 + \sin(\tfrac12\pi t) x_1.
% \end{equation}

The stochastic interpolant $x_t$ specified in Definition~\ref{def:interp} is a continuous-time stochastic process whose realizations are samples of $\rho_0$ at time $t=0$ and from $\rho_1$ at time $t=1$ by construction. As a result, it offers a way to bridge these two densities and  we will be interested in characterizing the law of $x_t$ in the full interval $[0,1]$ as it will allow us to design generative models. We show below in Theorem~\ref{prop:interpolate} that at any time $t\in[0,1]$ the probability distribution of $x_t$ is absolutely continuous with respect to the Lebesgue measure on~$\RR^d$, i.e. there is a unique time-dependent probability density function $\rho:[0,1]\times {\RR^d}\rightarrow\RR_{\geq 0}$ such that
\begin{equation}
    \label{eq:pdf:def:test}
    \forall t \in [0,1] \quad : \quad  \int_{\RR^d} \phi(x) \rho(t,x)dx  = \EE \phi(x_t) \quad \text{for any test function} \quad \phi \in C^\infty_0(\RR^d),
\end{equation}
where $x_t$ is defined in~\eqref{eq:stochinterp} and the expectation  is taken independently over $x_0\sim\rho_0$, $x_1\sim\rho_1$, and the Brownian bridge $B_t$.  Given samples from $\rho_0$ and $\rho_1$, we can easily generate samples $x_t$ from $\rho(t)$  at any time $t\in[0,1]$. We are, however, interested to  learn how to generate samples from $\rho(t)$ at any $t\in[0,1]$ by propagating either forward or backward samples drawn either from $\rho_0$ at $t=0$ or from $\rho_1$ at $t=1$, but not both: this would give us the possibility to generate new samples at one end if we can sample the PDF at the other end (e.g. if $\rho_0$, say, is a standard Gaussian density, and we want to get new samples from a complex $\rho_1$ known only through data). 

To this end, in Sec.~\ref{sec:cont:eq} we will derive evolution equations for $\rho(t)$; these equations involve the two
  velocity fields  $v :[0,1]\times{\RR^d} \to \RR^d$ and $s: (0,1)\times{\RR^d} \to \RR^d$ defined  as
\begin{equation}
    \label{eq:v:def}
    v(t,x) = \EE\left( \partial_t I(t,x_0,x_1)| x_t=x\right), \qquad (t,x)\in [0,1]\times\RR^d,
\end{equation}
and
\begin{equation}
    \label{eq:s:def}
    s(t,x) = -\frac{\EE\left( B_t| x_t=x\right)}{\seps t(1-t)}, \qquad (t,x)\in (0,1)\times\RR^d,
\end{equation}
where in both equations $x_t$ is defined in~\eqref{eq:stochinterp}, and $\EE(\cdot | x_t = x)$ denotes the expectation taken independently over $x_0\sim \rho_0$, $x_1\sim\rho_1$, and $B_t$, and conditionally on $x_t = x$: that is, given any $f:[0,1]\times \RR^d\times \RR^d\times\RR^d\to \RR$, its conditional expectation $\EE\left(f(t,x_0,x_1,B_t)| x_t=x\right)$ is by definition the function of~$x$ such that
\begin{equation}
    \label{eq:cond:e}
    \int_{\RR^d} \EE\left(f(t,x_0,x_1,B_t)| x_t=x\right) \rho(t,x) dx = \EE f(t,x_0,x_1,B_t)
\end{equation}
where the expectation at the right hand side is now taken independently over $x_0\sim \rho_0$, $x_1\sim\rho_1$, and the Brownian bridge $B_t$ (and similarly for the conditional expectation of vector-valued functions)\footnote{Formally, in terms of the Dirac delta distribution, we can write
$$
\EE\left(f(t,x_0,x_1,B_t)| x_t=x\right) = \frac{\EE\left(f(t,x_0,x_1,B_t)\delta(x-x_t)\right)}{\EE\delta(x-x_t)}
$$
and in these notations we also have $\rho(t,x) = \EE\delta(x-x_t)$.}. 

We will also show below in Theorem~\ref{prop:interpolate} that the velocity $s$ defined in~\eqref{eq:s:def} coincides with the score of $\rho$, i.e. $s=\nabla \log\rho$, so we supplement~\eqref{eq:s:def} at $t=0,1$ with
\begin{equation}
    \tag{\ref{eq:s:def}'}
    s(0,x) = \nabla \log \rho_0(x), \qquad s(1,x) = \nabla \log \rho_1(x), \qquad \forall x \in \RR^d.
\end{equation}


To proceed we make the following assumption on the PDFs $\rho_0$, $\rho_1$, and the interpolant $I$:
\begin{assumption}
\label{as:rho:I}
The PDF $\rho_0$ and $\rho_1$ are both strictly positive, in $C^2(\RR^d)$, and such that
\begin{equation}
    \label{eq:rho0:1:sc}
     \int_{\RR^d} |\nabla \log \rho_0(x)|^2 \rho_0(x) dx < \infty \quad\text{and} \quad \int_{\RR^d} |\nabla \log \rho_1(x)|^2 \rho_1(x) dx < \infty.
\end{equation}
The interpolant $I$ is in $C^2([0,1],(C^2(\RR^d\times\RR^d))^d)$ and such that
\begin{equation}
    \label{eq:bound:dI}
    \begin{aligned}
        &\exists C_1<\infty  \   : \ 
        &&|\partial_t I(t,x_0,x_1)|\le C_1|x_0-x_1|
        \quad  &&\forall (t,x_0,x_1) \in [0,1]\times \RR^d \times \RR^d,
        \end{aligned}
\end{equation}
and
\begin{equation}
    \label{eq:It:L2}
    \exists M_1,M_2 < \infty  \ \ : \ \  \EE\big[ |\partial_t I(t,x_0,x_1)|^4\big] \le M_1; \quad \EE\big[ |\partial^2_t I(t,x_0,x_1)|^2\big] \le M_2, \quad  \forall t\in [0,1],
\end{equation}
where the expectation is taken independently over $x_0\sim \rho_0$ and $x_1\sim\rho_1$.
\end{assumption}
Eq.~\eqref{eq:bound:dI} simply requires that $I(t,x_0,x_1)$ does does not move too fast along the way from $x_0$ at $t=0$ to $x_1$ at $t=1$, and as a result does no wander too far from these endpoints---this assumption is not necessary for most arguments below, but it is also not limiting in any way.
Note that for the  interpolant~\eqref{eq:lin:interp}, Assumption~\ref{as:rho:I} holds if $\rho_0$ and $\rho_1$ both have finite fourth moments and as well as $|\dot \alpha(t) x_0+\dot \beta(t) x1| \le C_1|x_0-x_1|$. 




\subsection{Continuity equations and quadratic objectives}
\label{sec:cont:eq}


We can now state a result specifying some important properties of the PDF $\rho$ and the velocities $v$ and $s$, as well as various evolution equations for $\rho$:

\begin{restatable}[Continuity equations]{theorem}{interpolation}
\label{prop:interpolate}
The probability density function $\rho$ defined in~\eqref{eq:pdf:def:test} satisfies $\rho(0) = \rho_0$ and $\rho(1)=\rho_1$, is in $C^1([0,1];C^p(\RR^d))$ for any $p\in\NN$, and is positive everywhere on $\RR^d$ for all $t\in [0,1]$. The velocities~$v$ and $s$ defined in~\eqref{eq:v:def} and~\eqref{eq:s:def}  are both in $C^0([0,1];(C^p(\RR^d))^d)$ for any $p\in \NN$, and these functions are such that
\begin{equation}
    \label{eq:vt:wt:bounded}
    \forall t\in [0,1] \quad : \quad \int_{\RR^d} |v(t,x)|^2 \rho(t,x) dx < \infty, \qquad \int_{\RR^d} |s(t,x)|^2 \rho(t,x) dx < \infty.
\end{equation}
In addition, the density $\rho$ satisfies the following three dynamical equations:
\begin{enumerate}[leftmargin=0.15in]
\item The forward Fokker-Planck equation
\begin{equation}
    \label{eq:fpe}
    \partial_t \rho + \nabla \cdot \left(b_{\fwd}\rho\right) = \eps  \Delta \rho, \qquad \rho(0) = \rho_0,
\end{equation}
where we defined the forward drift
\begin{equation}
    \label{eq:b:def}
    b_\fwd(t,x) = v(t,x) + 2\eps t s(t,x).
\end{equation}
Equation~\eqref{eq:fpe} is well-posed when solved forward in time from $t=0$ to $t=1$, and its solution for the initial condition $\rho(t=0) = \rho_0$ satisfies $\rho(t=1) = \rho_1$. 

\item  The backward Fokker-Planck equation
\begin{equation}
    \label{eq:fpe:tr}
    \partial_t \rho + \nabla \cdot \left(b_\rev\rho\right) = -\eps  \Delta \rho, \qquad \rho(1) = \rho_1,
\end{equation}
where we defined the backward drift
\begin{equation}
    \label{eq:b:r:def}
    b_\rev(t,x) = v(t,x) - 2\eps(1-t) s(t,x).
\end{equation}
Equation~\eqref{eq:fpe:tr} is well-posed when solved backward in time from $t=1$ to $t=0$, and its solution for the final condition $\rho(1) = \rho_1$ satisfies $\rho(0) = \rho_0$. 
\item The transport equation
\begin{equation}
    \label{eq:transport}
    \partial_t \rho + \nabla \cdot \left(b_\ODE\rho\right) = 0, 
\end{equation}
where we defined the forward-backward drift
\begin{equation}
    \label{eq:b:ode:def}
    b_\ODE(t,x) = v(t,x) - \eps(1-2t) s(t,x).
\end{equation}
Equation~\eqref{eq:transport} can be solved either forward in time from the initial condition $\rho(0)=\rho_0$, in which case $\rho(1)=\rho_1$, or backward in time from the final condition $\rho(1)=\rho_1$, in which case $\rho(0)=\rho_0$. 
\end{enumerate}
Finally, the velocity $s$ defined in~\eqref{eq:s:def} is the score of $\rho$, i.e.
\begin{equation}
    \label{eq:s:score}
    \nabla \log \rho(t,x) = s(t,x), \qquad [t,x]\in (0,1)\times\RR^d
\end{equation}

\end{restatable}


The proof of Theorem~\ref{prop:interpolate} is given in Appendix~\ref{app:proof:interpolate}; it mostly relies on manipulations involving the characteristic function of  the stochastic interpolant $x_t$. Equations~\eqref{eq:fpe}, \eqref{eq:fpe:tr}, and \eqref{eq:transport} for $\rho$ lead to methods for generative modeling and density estimation, as explained in Secs.~\ref{sec:generative} and \ref{sec:density}, provided that we can estimate the velocity~$v$ and the score~$s$.  These velocities are explicitly available only in special cases, for example when $\rho_0$ and $\rho_1$ are both Gaussian mixture densities: this case is treated in Sec.~\ref{sec:Gauss:mixt}.  In general $s$ and $v$ must be calculated numerically. In practice, this can be performed via empirical risk minimization of two quadratic objective functions, as characterized by our next result:

\begin{restatable}[Objectives]{theorem}{interpolatelosses}
\label{prop:interpolate_losses}

The velocity $v$ and the score $s$ defined in~\eqref{eq:v:def} and~\eqref{eq:s:def} are the unique minimizers in $C^1([0,1];(C^1(\RR^d))^d)$ of the quadratic objectives
\begin{equation}
    \label{eq:obj:v}
    \mathcal{L}_v[\hat{v}] =\int_0^1   \EE \left( \tfrac12|\hat v(t,x_t)|^2 - \partial_t I(t,x_0,x_1) \cdot \hat v(t,x_t) \right) dt
\end{equation}
and
\begin{equation}
    \label{eq:obj:s}
    \mathcal{L}_s[\hat{s}] = \int_0^1 \EE\left( \tfrac12|\hat s(t,x_t)|^2 +\frac{ B_t\cdot \hat s(t,x_t)}{\seps \, t(1-t)} \right) dt
% =\EE\int_0^1\left( \tfrac12|\hat s(t,x_t)|^2 dt -(\seps \, t)^{-1} \hat s(t,x_t)dB_t \right)
\end{equation}
where $x_t$ is defined in~\eqref{eq:stochinterp} and the expectation $\EE$ is taken independently over $x_0\sim \rho_0$, $x_1\sim\rho_1$, and the Brownian bridge $B_t$.
 % in the last equation, the integral  involving $dB_t$ should be interpreted in It\^o sense.

\end{restatable}
The proof of Theorem~\ref{prop:interpolate_losses}  is given in Appendix~\ref{app:proof:interpolate_losses}: it relies on the definitions of $v$ and $s$ in~\eqref{eq:v:def} and~\eqref{eq:s:def}, as well as the definition of $\rho$ in~\eqref{eq:pdf:def:test} and some elementary properties of the conditional expectation. Let us make a few remarks about the statements in the two theorems above:


\begin{remark} Since $x_t = I(t,x_0,x_1)$ when $\eps\to0$, it is easy to see that  \eqref{eq:fpe}, \eqref{eq:fpe:tr}, and~\eqref{eq:transport} all collapse to the same transport $\partial_t \rho_t + \nabla \cdot \left(v_t\rho_t\right) = 0$ in that limit, and the formulas above reduce to those in Ref.~\cite{albergo2023building}. Note that, working with $\eps>0$ not only changes the transport equations: it also changes the velocity $v$ and $s$ entering these equations, and makes them smoother in space, which is an advantage in terms of both their estimation and their usage in the ODE/SDE used to design generative models (see Sec.~\ref{sec:generative}).
\end{remark}

 \begin{remark}
 For $\eps>0$, the FPE~\eqref{eq:fpe}, the backward~\eqref{eq:fpe:tr}, and the TE~\eqref{eq:transport} are well-posed when solved in the directions specified in Theorem~\ref{prop:interpolate} because the score $s$ is \textit{externally} specified by~\eqref{eq:s:def}. In contrast, if we were to use $s=\nabla \log \rho$  in~\eqref{eq:fpe}, \eqref{eq:fpe:tr}, and~\eqref{eq:transport}, all three equations would become
\begin{equation}
    \label{eq:fpe:ill}
    \partial_t \rho_t + \nabla \cdot \left(v_t\rho_t\right) = \eps  (1-2t)\Delta \rho_t, 
\end{equation}
Even though this equation is formally exact, it is ill-posed for $t > \frac12$ if solved forward in time, and for $t<1/2$ if solved backward in time; as a result, it is useless in practice. Therefore we cannot avoid estimating $s$ first, e.g. via minimization of~\eqref{eq:obj:s}.
 \end{remark}  

\begin{remark}
    We could also obtain $v(t,\cdot)$ at any $t\in[0,1]$  by minimizing
\begin{equation}
    \label{eq:obj:vt}
    \EE \left( \tfrac12|\hat v(t,x_t)|^2 - \partial_t I(t,x_0,x_1) \cdot \hat v(t,x_t) \right) \qquad t\in [0,1]
\end{equation}
and $s(t,\cdot)$ at any $t\in(0,1)$ by minimizing 
\begin{equation}
    \label{eq:obj:wt}
    \EE \left( \tfrac12|\hat s(t,x_t)|^2 +\frac{B_t\cdot \hat s(t,x_t)}{\seps t(1-t)} \right) \qquad t\in (0,1)
\end{equation}
Using the time-integrated versions of these objectives given in~\eqref{eq:obj:v} and~\eqref{eq:obj:s} is more convenient numerically as it allows one to parameterize  $v$ and $s$ globally for $(t,x)\in [0,1]\times \RR^d$.
\end{remark}
\begin{remark}
    Since $s$ is the score of $\rho$, an alternative objective to estimate it is~\cite{} 
\begin{equation}
    \label{eq:ob:w:alt}
     \int_0^1 \EE\left(|s(t,x_t)|^2 +2\nabla \cdot s(t,x_t) \right)dt.
\end{equation}
The derivation of~\eqref{eq:ob:w:alt} is standard:
for the reader's convenience we recall it at the end of this section. The advantage of using~\eqref{eq:obj:s} over~\eqref{eq:ob:w:alt} is that it does not requires us to take the divergence of $s$.
\end{remark}

\begin{remark}
    The objectives in~\eqref{eq:obj:v} and \eqref{eq:obj:s} are esaily amenable to empiracl estimation if we have samples from $\rho_0$ and $\rho_1$, since in that case we can easily generate samples of $x_t= I(t,x_0,x_1) + \seps B_t$ at any time $t\in[0,1]$. We will use this feature in the numerical experiments presented below.
\end{remark}

\paragraph{Derivation of \eqref{eq:ob:w:alt}.} For any $t\in[0,1]$ the score $s(t,\cdot)$ is the minimizer of
\begin{equation}
    \begin{aligned}
        &\int_{\RR^d} |s(t,x) - \nabla \log \rho(t,x)|^2 \rho(t,x) dx\\
        &= \int_{\RR^d} \left(|s(t,x)|^2 - 2s(t,x) \cdot\nabla\log \rho(t,x)+ | \nabla \log \rho(t,x)|^2\right) \rho(t,x) dx \\
        &= \int_{\RR^d} \left(|s(t,x)|^2 + 2\nabla \cdot s(t,x) + | \nabla \log \rho(t,x)|^2\right) \rho(t,x) dx,
    \end{aligned}
\end{equation}
where we used the identity $s \cdot \nabla \log \rho \,\rho = s \cdot \nabla \rho$ and integration by part to get the second equality. The last term involving  $|\nabla \log \rho|^2$ is a constant in $s$ that can therefore be neglected. Expressing the remaining terms as an expectation over $x_t$ and integrating the result in time gives~\eqref{eq:ob:w:alt}.


\subsection{Generative models}
\label{sec:generative}

Our next result is a direct consequence of Theorem~\ref{prop:interpolate} that shows how to design generative models using the stochastic processes associated with the forward FPE~\eqref{eq:fpe}, the backward FPE~\eqref{eq:fpe:tr}, and the TE~\eqref{eq:transport}:

\begin{restatable}[Generative models]{corollary}{generative}
\label{prop:generative}
At any time $t\in[0,1]$, the law of the stochastic interpolant~$x_t$ coincides with the law of the three processes $X^\fwd_t$, $X^\rev_t$, and $X_t$ respectively defined as:
\begin{enumerate}[leftmargin=0.15in]
\item The solutions of the forward SDE associated with the FPE~\eqref{eq:fpe}
\begin{equation}
    \label{eq:sde:1}
    dX^\fwd_t = b_\fwd(t,X^\fwd_t)dt  + \seps  dW_t,
\end{equation}
solved forward in time from initial data~$X^\fwd_{t=0}\sim\rho_0$ independent of $W$.

\item  The solutions of the backward SDE associated with the backward FPE~\eqref{eq:fpe:tr}
\begin{equation}
    \label{eq:sde:R}
    dX^\rev_t = b_\rev(t,X^\rev_t)dt  + \seps  dW^\rev_t, \quad W_t^\rev = -W_{1-t},
\end{equation}
solved backward in time from final data~$X^\rev_{t=1}\sim\rho_1$ independent of $W^\rev$.
\item The solutions of the ordinary differential equation (ODE) (aka probability flow) associated with the transport equation~\eqref{eq:transport}
\begin{equation}
    \label{eq:ode:1}
    \frac{d}{dt}  X_t = b_\ODE(t, X_t),
\end{equation}
solved either forward in time from initial data $X_{t=0}  \sim\rho_0$ or backward in time from final data  $X_{t=1} = x_1\sim\rho_1$. 
\end{enumerate}
\end{restatable}

Here the solution of the reversed-time SDE~\eqref{eq:sde:R} is by definition $X^\rev_t= Y^\fwd_{1-t}$ where $Y^\fwd_t$ satisfies
\begin{equation}
    \label{eq:sde:R:Y}
        dY^\fwd_t = -b_\rev(1-t,Y^\fwd_t)dt+ \seps  dW_t.
\end{equation}
solved forward in time from initial data~$Y^\fwd_{t=0}\sim \rho_1$ independent of $W$. To not have to change $t\to1-t$ when reversing the time, it is convenient to work with \eqref{eq:sde:R} directly using the rules: for any $f\in C^1([0,1];C_0^2(\RR_d))$ and $t\in[0,1]$, we have the backward It\^o formula:
\begin{equation}
    \label{eq:ito:formula}
    df(t,X^\rev_t) = \partial_t f(t,X^\rev_t) dt+ \nabla f(X^\rev_t) dX^\rev_t - \eps \Delta f(t,X^\rev_t) dt 
\end{equation}
and for any $g\in C^0([0,1];(C_0^0(\RR_d))^d)$ and $t\in[0,1]$,  we have the backward It\^o isometries
\begin{equation}
    \label{eq:ito:iso}
    \begin{aligned}
    \EE^x_\rev\int_t^1 g(t,X^\rev_t) \cdot dW^\rev_t = 0; \qquad \EE^x_\rev\left|\int_t^1 g(t,X^\rev_t)\cdot  dW^\rev_t\right|^2 &= \int_t^1 \EE^x_\rev\left|g(t,X^\rev_t)\right|^2 dt,
    \end{aligned}
\end{equation}
where $\EE^x_\rev$ denotes expectation conditional on $X_{t=1}^\rev = x$. The rules are elucidated in the proof of Corollary~\ref{prop:generative} given in Appendix~\ref{app:proof:generative}. 

The relevance of Corollary~\ref{prop:generative} for generative modeling is clear. Assuming, for example, that $\rho_0$ is a simple density that can be sampled easily (e.g. a Gaussian or a Gaussian mixture density), we can use the SDE~\eqref{eq:sde:1} or the ODE~\eqref{eq:ode:1} to push these samples forward in time and generate samples from a complex target density~$\rho_1$.  In Sec.~\ref{sec:density} we will also show how to use the reverse SDE~\eqref{eq:sde:R} or the ODE~\eqref{eq:ode:1} to estimate $\rho_1$ and $\log\rho_1$ at any $x\in\RR^d$ assuming that we can evaluate $\rho_0$ at any $x\in\RR^d$, or how to use~\eqref{eq:sde:1} and~\eqref{eq:sde:R}, or~\eqref{eq:ode:1}, to perform calculations of the cross entropies between $\rho_0$ and $\rho_1$ or the Kullback-Leibler divergence from $\rho_0$ to $\rho_1$ and vice-versa. Of course, these calculations will typically involve velocities $v$ and $s$ that are imperfectly estimated via minimization of~\eqref{eq:obj:v} and~\eqref{eq:obj:s}. It is therefore important to estimate the error such imperfections induces, which is the object of our next section.

\subsection{Simulation-free maximum likelihood training}
\label{sec:likelihood_bounds}

In this section, we demonstrate that jointly minimizing the objective functions~\eqref{eq:obj:v} and~\eqref{eq:obj:s} defined in theorem~\ref{prop:interpolate} minimizes the $\mathsf{KL}$-divergence from the target density $\rho_1$ to the model density $\hat{\rho}_1$. The derivation is based on a simple and exact characterization of the $\mathsf{KL}$-divergence between two transport equations or two Fokker-Planck equations with differing drifts. Remarkably, we find that the presence of a diffusive term determines whether or not it is sufficient to learn the drift to control $\mathsf{KL}$; this leads to simple maximum likelihood training for SDE-based sampling models. \nb{TODO: cite and relate to the Yang Song MLE paper.} The proofs of the staement in this section are provided in Appendix~\ref{app:proof:kl}.

We first characterize the $\mathsf{KL}$ divergence between PDF transported by two continuity equations:
\begin{restatable}{lemma}{kltransport}
\label{lemma:kl_transport}
Let $\rho_0: \RR^d\rightarrow\RR_{\geq 0}$ denote a fixed base probability density function. Given  two velocity fields $b_\ODE, \hat{b}_\ODE \in C^0([0,1], (C^1(\RR^d))^d))$, let the time-dependent densities $\rho: [0,1]\times \RR^d \to \RR_{\ge0}$ and $\hat \rho: [0,1]\times \RR^d \to \RR_{\ge0}$ denote the solutions to the transport equations
\begin{equation}
    \label{eq:2:te}
    \begin{aligned}
       &\partial_t\rho + \nabla \cdot(b_\ODE \rho) = 0,\qquad &&\rho(0)=\rho_0,\\ 
       %
       %
       &\partial_t\hat\rho + \nabla \cdot(\hat b_\ODE\hat \rho) = 0,\qquad &&\hat \rho(0)=\rho_0.
    \end{aligned} 
\end{equation}
 Then, the Kullback-Leibler divergence of $\rho(1)$ from $\hat\rho(1)$ is given by
\begin{equation}
    \KL{\rho(1)}{\hat\rho(1)} = \int_0^1 \int_{\RR^d} \left(\nabla\log\hat\rho(t,x) - \nabla\log\rho(t,x)\right)\cdot\big(\hat b_\ODE(t,x) - b_\ODE(t,x)\big)\rho(t,x) dxdt.
\end{equation}
\end{restatable}

% \begin{remark}
% The PDF $\rho(t) = X_t\sharp \rho_0$ and $\hat \rho(t) = \hat X_t\sharp\rho_0$ denote the push-forward PDF generated by the flows $X_t$ and $\hat X_t$, respectively
%     \begin{equation}
%     \begin{aligned}
%        &\frac{d}{dt}X_t(x) = b(t,X_t(x)),\qquad &&\hat X_{t=0}(x)=x\\ 
%        &\frac{d}{dt}\hat X_t(y) = \hat b(t,\hat X_t(y)) && \hat X_{t=0}(x) = x.
%     \end{aligned} 
% \end{equation}
% \end{remark}

Lemma~\ref{lemma:kl_transport} shows that it is insufficient in general to match $\hat b$ with $b$ to obtain control on the $\mathsf{KL}$ divergence. The essence of the problem is that a small error in $\hat b - b$ does not ensure control on the Fisher divergence $\fisher{\rho(t)}{\hat\rho(t)} = \int_{\RR^d}\norm{\nabla\log\rho(t,x) - \nabla\log\hat \rho(t,y)}^2\rho(t,x)dx$, which is necessary due to the presence of $\left(\nabla\log\hat\rho - \nabla\log\rho\right)$. 
% While surprising at first glance, this result is perhaps quite natural: if the error term $g_t - f_t$ drives the densities so that $\supp(\rho_t^Y) \subset \supp(\rho_t^X)$, then $\KL{\rho_t^X}{\rho_t^Y}$ must diverge. This is captured by the fact that $\nabla\log\rho_t^Y$ becomes undefined outside of $\supp(\rho_t^Y)$, but this region is included in the spatial integral.

In our next lemma, we consider the case for two Fokker-Planck equations generated by stochastic dynamics with different drifts, and highlight that the situation becomes quite different.
\begin{restatable}{lemma}{klfpe}
\label{lemma:kl_fpe}
Let $\rho_0: \RR^d\rightarrow\RR_{\geq 0}$ denote a fixed base probability density function. Given  two velocity fields $b_\fwd, \hat{b}_\fwd \in C^0([0,1], (C^1(\RR^d))^d))$,  let the time-dependent densities $\rho: [0,1]\times \RR^d \to \RR_{\ge0}$ and  $\hat \rho: [0,1]\times \RR^d \to \RR_{\ge0}$ denote the solutions to the Fokker-Planck equations
\begin{equation}
\label{eq:2:fpe}
    \begin{aligned}
       &\partial_t\rho + \nabla \cdot(b_\fwd \rho) = \eps \Delta \rho,\qquad &&\rho(0)=\rho_0,\\ 
       %
       %
       &\partial_t\hat\rho + \nabla \cdot(\hat b_\fwd\hat \rho) = \eps \Delta \hat \rho,\qquad &&\hat \rho(0)=\rho_0.
    \end{aligned} 
\end{equation}
Then, the Kullback-Leibler divergence of $\rho(1)$ from $\hat\rho(1)$ is given by
\begin{equation}
\begin{aligned}
    \KL{\rho(1)}{\hat\rho(1)} &= \int_0^1\int_{\RR^d} \left(\nabla\log\hat\rho(t,x) - \nabla\log\rho(t,x)\right)\cdot \left(\hat b_\fwd(t,x) - b_\fwd(t,x)\right)\rho(t,x) dxdt\\
    &\qquad -\eps\int_0^1 \int_{\RR^d} \norm{\nabla\log\rho(t,x) - \nabla\log\hat \rho(t,x)}^2\rho(t,x)dx dt,
\end{aligned}
\end{equation}
and as a result
\begin{equation}
\begin{aligned}
    \KL{\rho(1)}{\hat\rho(1)} &\leq \frac1\eps\int_0^1 \int_{\RR^d} \norm{\hat b_\fwd(t,x) - b_\fwd(t,x)}^2\rho(t,x) dxdt.
\end{aligned}
\end{equation}
\end{restatable}
Lemma~\ref{lemma:kl_fpe} shows that, unlike the case for transport equations associated with deterministic flows, the $\mathsf{KL}$-divergence between the solutions of two Fokker-Planck equations associated with stochastic flows \textit{is} controlled by the error in their drifts. The diffusive term in these Fokker-Planck equations provides an additional negative term in the $\mathsf{KL}$-divergence, which eliminates the need for explicit control on the Fisher divergence. 

Putting this all together, we can state the following result, which demonstrates that the losses~\eqref{eq:obj:v} and~\eqref{eq:obj:s} provide simulation-free maximum likelihood training for learned approximations to the FPE~\eqref{eq:fpe} and the associated SDE~\eqref{eq:sde:1}.

\begin{restatable}{theorem}{likelihoodbound}
\label{thm:kl:bound}
Let $\rho$ denote the solution of the Fokker-Planck equation~\eqref{eq:fpe}. Given two velocity fields $\hat{v}, \hat{s} \in C^0([0,1], C^1(\RR^d, \RR^d))$, define $\hat b_\fwd = \hat v_\fwd + 2\eps t \hat s_\fwd$ and let $\hat \rho$ denote the solution to the Fokker-Planck equation
\begin{equation}
     \partial_t \hat \rho + \nabla \cdot (\hat b_\fwd\hat \rho) = \eps  \Delta \hat \rho, \qquad \hat \rho(0) = \rho_0.
\end{equation}
Then,
\begin{equation}
\label{eq:bound:kl}
    \KL{\rho_1}{\hat{\rho}(1)} \leq \frac{1}{2\eps}\left(\mathcal{L}_v[\hat{v}] - \min_{\hat{v}}\mathcal{L}_v[\hat{v}]\right)  + 2\eps\left(\mathcal{L}_s[\hat{s}] - \min_{\hat{s}}\mathcal{L}_s[\hat{s}]\right).
\end{equation}
\end{restatable}

\nb{TODO: Cite relevant literature here}

\subsection{Density estimation and likelihood evaluation}
\label{sec:density}

It is well-known that the probability flow ODE can be used to solve the TE~\eqref{eq:transport}. Specifically we have
\begin{equation}
    \label{eq:te:sol}
    \rho_1(X_{t=1}) =  \exp\left( - \int_0^1 \nabla \cdot b_{\ODE}(t, X_t) dt \right) \rho_0( X_{t=0})
\end{equation}
For completeness, this equation is derived in Appendix~\ref{app:proof:generative}. If we solve  \eqref{eq:ode:1} backward in time from the final condition $\ X_{t=1}=x$, \eqref{eq:te:sol} allows us to express $\rho_1(x)$ at location $x$ in terms of $\rho_0(X_{t=0})$; conversely, if we solve  \eqref{eq:ode:1} forward in time from the initial condition $X_{t=0}=x$, \eqref{eq:te:sol} allows us to express $\rho_0(x)$ at location $x$ in terms of $\rho_1(X_{t=1})$.


Interestingly, we have  similar results using the SDE~\eqref{eq:sde:1} or the backward SDE~\eqref{eq:sde:R}, which show how to evaluate $\rho_0$ or $\rho_1$ at any $x\in {\RR^d}$ if we can evaluate  the other density:

\begin{restatable}{theorem}{FK}
    \label{prop:sde:rho}
    Let $\rho:$ be the PDF defined in~\eqref{eq:pdf:def:test}, and let $X^\fwd_t$ and $X^\rev_t$ be the solution of the SDE~\eqref{eq:sde:1} and the backward~\eqref{eq:sde:R}, respectively. Then:
    \begin{enumerate}[leftmargin=0.15in]
\item Assuming we can evaluate $\rho_0(x)$ at any any $x\in {\RR^d}$, $\rho_1(x)$ and its logarithm can also be estimated at any $x\in {\RR^d}$ using
\begin{equation}
    \label{eq:fk}
    \rho_1(x) = \EE_\rev^x \left(\exp\left(- \int_0^1\left(\nabla \cdot b_\fwd(t, X^\rev_t) + 2\epsilon|s(t, X_t^\rev)|^2\right) dt\right)  \rho_0(X_{t=0}^\rev)\right),
\end{equation}
and
\begin{equation}
    \label{eq:ito:rho:1:log:int}
    \begin{aligned}
    \log \rho_1(x) &= \EE^x_\rev\log \rho_0(X_{t=0}^\rev)- \int_0^1 \EE^x_\rev \left(\nabla \cdot b_\fwd(t, X^\rev_t) + \epsilon|s(t, X_t^\rev)|^2\right)dt,
    \end{aligned}
\end{equation}
where $\EE_\rev^x$ denotes expectation on the path of $X_t^\rev$ conditional on $X_{t=1}^\rev = x$.

\item Assuming we can evaluate $\rho_1(x)$ at any any $x\in {\RR^d}$, $\rho_0(x)$ and its logarithm can also be estimated at any $x\in {\RR^d}$ using
\begin{equation}
    \label{eq:fk:rev}
    \rho_0( x) = \EE_\fwd^x \left(\exp\left( \int_0^1\left(\nabla \cdot b_\rev(t, X^\fwd_t) - 2\epsilon|s(t, X^\fwd_t)|^2\right) dt\right)\rho_1(X^\fwd_{t=1})\right),
\end{equation}
and
\begin{equation}
    \label{eq:ito:rho:r:1:log:int}
    \begin{aligned}
    \log \rho_0(x) &= \EE_\fwd^x \log \rho_1(X^\fwd_{t=1}) + \int_0^1 \EE^x \left(\nabla \cdot b_\rev(t, X^\fwd_t) - \epsilon |s(t, X^\fwd_t)|^2\right) dt,
    \end{aligned}
\end{equation}
where $\EE_\fwd^x$ denotes expectation on the path of $X^\fwd_t$ conditional on $X^\fwd_{t=0} = x$.
\end{enumerate}
\end{restatable}

% Theorem~\ref{prop:sde:rho} can be used to estimate the Kullback-Leibler divergences from $\rho_0$ to $\rho_1$ and $\rho_1$ to $\rho_0$:
% \begin{corollary}
% \label{prop:kl}
% We have
% \begin{equation}
%  \label{eq:kl}
% D_{\text{KL}} (\rho_1|\rho_0)  = \EE_0 \EE^{x_0} \left(\log\rho_1(X_{t=1}) -\log \rho_0(X_{t=1})\right)
% \end{equation} 
% where $X_t$ is the solution to the SDE~\eqref{eq:sde:1}, $\EE^x$ denotes the expectation on $X_t$ conditional on $X_{t=0}=x_0$, and $\EE_0$ denotes the expectation on $x_0\sim\rho_0$. In~\eqref{eq:kl} we can use (from \eqref{eq:fk:rev})
% \begin{equation}
%     \label{eq:fk:rev:X1}
%     \log \rho_1(X_{t=1}) = \EE_\rev^{X_{t=1}} \log \rho_0(X_{t=0}^\rev)- \int_0^1 \EE_\rev^{X_{t=1}}\left(\nabla \cdot b(t, X^\rev_t) + \epsilon|s(t, X_t^\rev)|^2\right)dt, 
% \end{equation}
% where $\EE_\rev^{X_{t=1}}$ denotes expectation on the path of $X_t^\rev$ conditional on $X_{t=1}^\rev = X_{t=1}$. Similarly, we have
% \begin{equation}
%  \label{eq:kl:rev}
% D_{\text{KL}} (\rho_0|\rho_1)  = \EE_1 \EE_\rev^{x_1} \left(\log \rho_0(X^\rev_{t=0}) - \log\rho_1(X^\rev_{t=0})\right)
% \end{equation} 
% where $X^\rev_t$ is the solution to the SDE~\eqref{eq:sde:1}, $\EE_\rev^x$ denotes the expectation on $X^\rev_t$ conditional on $X^\rev_{t=1}=x_1$, and $\EE_1$ denotes the expectation on $x_1\sim\rho_1$. In~\eqref{eq:kl} we can use (from \eqref{eq:fk})
% \begin{equation}
%     \label{eq:fk:X0}
%     \rho_0(X^\rev_{t=0}) = \EE^{X^\rev_{t=0}}  \log \rho_1(X_{t=1}) + \int_0^1  \EE^{X^\rev_{t=0}}\left(\nabla \cdot b^\rev(t, X_t) - \epsilon |s(t, X_t)|^2\right) dt,
% \end{equation}
% where $\EE_\rev^{X^\rev_{t=0}}$ denotes expectation on the path of $X_t$ conditional on $X_{t=0}= X^\rev_{t=0}$.

% \end{corollary}


% Equations~\eqref{eq:kl} and \eqref{eq:fk:rev:X1} show that, if we can evaluate $\rho_0$ at any $x\in {\RR^d}$ and sample this density, we can estimate $D_{\text{KL}} (\rho_1|\rho_0)$ by the following algorithm:
% \begin{enumerate}[leftmargin=0.15in]
% \item generating $x_0\sim \rho_0$;
% \item propagating each of these $x_0$ forward in time using the SDE~\eqref{eq:sde:1} to generate $X_{t=1}$;
% \item propagating each of these $X_{t=1}$ backward in time using the backward SDE~\eqref{eq:sde:R} to generate $X^\rev_{t=0}$;
% \item averaging these data in~\eqref{eq:kl} and \eqref{eq:fk:rev:X1}.
% \end{enumerate}
% Similarly, equations~\eqref{eq:kl:rev} and \eqref{eq:fk:X0} show that, if we can evaluate $\rho_1$ at any $x\in {\RR^d}$ and sample this density, we can estimate $D_{\text{KL}} (\rho_0|\rho_1)$ by:
% \begin{enumerate}[leftmargin=0.15in]
% \item generating $x_1\sim \rho_1$;
% \item propagating each of these $x_1$ backward in time using the backward SDE~\eqref{eq:sde:R} to generate $X^\rev_{t=0}$;
% \item propagating each of these $X^\rev_{t=0}$ forward in time using the  SDE~\eqref{eq:sde:1} to generate $X_{t=1}$;
% \item averaging these data in~\eqref{eq:kl:rev} and \eqref{eq:fk:X0}.
% \end{enumerate}

\subsection{Cross-entropy calculations}
\label{sec:ce}

The results above can also be used if we replace the velocity $v$ and the score $s$ by approximate versions learned via minimization of~\eqref{eq:obj:v} and~\eqref{eq:obj:s}. Assuming that we can evaluate $\rho_0$ at any $x\in \RR^d$ and sample $\rho_1$ empirically (e.g. via an existing data set), this allows us to evaluate  the cross entropy of the density calculated with these approximate $v$ and $s$ relative to $\rho_1$, as stated in the following result

\begin{restatable}{theorem}{crossentsde}
    \label{prop:ce:sde}
    Given the PDFs $\rho_0$ and $\rho_1$, and the two velocity fields $\hat v: [0,1]\times\RR^d\to\RR^d$ and $\hat s: [0,1]\times\RR^d\to\RR^d$, both in $C^0([0,1];(C_b^1(\RR^d))^d)$
 let the time-dependent PDF $\hat \rho: [0,1]\times \RR^d \to \RR_{\ge0} $ be the solution to the FPE
\begin{equation}
    \label{eq:fpe:hat}
    \begin{aligned}
       \partial_t \hat \rho + \nabla \cdot (\hat b_\rev\hat \rho) = \eps  \Delta \hat \rho, \qquad \hat \rho(0) = \rho_0,
    \end{aligned} 
\end{equation}
and let $\hat X^\rev_t$ solve the backward SDE
\begin{equation}
    \label{eq:sde:R:hat}
    d\hat X^\rev_t = \hat b_\rev(t,\hat X^\rev_t)dt + \seps  dW^\rev_t, \quad W_t^\rev = -W_{1-t}.
\end{equation}
where
\begin{equation}
\label{eq:hat:b:r}
 \hat b_\rev(t,x) = \hat v(t,x) - 2\eps (1-t) \hat s(t,x).
\end{equation}
Denote
\begin{equation}
    \label{eq:b:fwd:hat}
    \hat b_\fwd (t,x) = \hat v(t,x) + 2\eps t \hat s(t,x).
\end{equation}
Then the cross-entropy of $\hat \rho(1)$ relative to $\rho_1$ is given by
\begin{equation}
    \label{eq:ce:sde}
    \begin{aligned}
    H(\rho_1|\hat \rho(1)) &= - \int_{\RR^d} \log \hat \rho(1,x) \rho_1(x) dx\\
   & = -\EE_1 \EE^{x_1}_\rev\log \rho_0(\hat X_{t=0}^\rev)+ \int_0^1 \EE_1 \EE^{x_1}_\rev \left(\nabla \cdot \hat b_\fwd(t, \hat X^\rev_t) + \epsilon|s(t, \hat X_t^\rev)|^2\right)dt,
   \end{aligned}
\end{equation}
where $\EE_\rev^x$ denotes the expectation on $\hat X^\rev_t$ conditional on $\hat X^\rev_{t=1}=x_1$, and $\EE_1$ denotes the expectation on $x_1\sim\rho_1$.
  
\end{restatable}

This theorem is proven in Appendix~\ref{app:ce}. If in~\eqref{eq:ce:sde} we approximate the expectation over $\rho_1$ by an empirical expectation over a data set from this density, this equation allows us to cross-validate different approximations of $v$ and $s$. It also allows to to compare the cross-entropy calculated with the backward SDE with the one calculated with the probability flow ODE, since we also have
\begin{restatable}{theorem}{crossentode}
    \label{prop:ce:ode}
    Given the PDFs $\rho_0$ and $\rho_1$, and the two velocity fields $\hat v: [0,1]\times\RR^d\to\RR^d$ and $\hat s: [0,1]\times\RR^d\to\RR^d$, both in $C^0([0,1];(C_b^1(\RR^d))^d)$, let the time-dependent PDF $\check \rho: [0,1]\times \RR^d \to \RR_{\ge0} $ be the solution to the TE
\begin{equation}
    \label{eq:te:hat}
    \begin{aligned}
       \partial_t \check \rho + \nabla \cdot (\hat b_\ODE\check \rho) = 0, \qquad \check \rho(0) = \rho_0,
    \end{aligned} 
\end{equation}
and let $\hat{X}^{x}_t$ solve the ODE
\begin{equation}
    \label{eq:ode:hat}
    \frac{d}{dt}\hat{X}^{x_1}_t = \hat b_\ODE(t,\hat{X}^{x_1}_t), \qquad \hat{X}^{x_1}_{t=1} = x_1,
\end{equation}
where 
\begin{equation}
\label{eq:hat:b:ode}
 \hat b(t,x) = \hat v(t,x) - \eps (1-2t) \hat s(t,x).
\end{equation}
Then the cross-entropy of $\check\rho(1)$ relative to $\rho_1$ is given by
\begin{equation}
    \label{eq:ce:ode}
    \begin{aligned}
    H(\rho_1|\check\rho(1)) &= - \int_{\RR^d} \log \check\rho(1,x) \rho_1(x) dx\\
   & = \EE_1 \log \rho_0(\hat{X}^{x_1}_{t=0})- \int_0^1 \EE_1  \nabla \cdot \hat b_{\ODE}(t, \hat{X}^{x_1}_t)dt,
   \end{aligned}
\end{equation}
where $\EE_1$ denotes the expectation on $x_1\sim\rho_1$.

 
\end{restatable} 

This theorem is also proven in Appendix~\ref{app:ce}.















