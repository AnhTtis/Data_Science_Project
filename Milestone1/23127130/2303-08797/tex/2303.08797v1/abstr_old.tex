% We introduce a new class of generative models based on the framework of stochastic interpolants that generalizes and extends both flow-based and diffusion-based methods.
% %
% To this end, we design a stochastic process that transports a fixed base distribution into a desired target distribution according to a Fokker-Planck equation whose dynamics can be learned by minimizing a simple quadratic loss function that describes the square error on an unknown velocity field.

% %
% For generative modeling, the process can be realized at the level of individual samples through a generic class of stochastic differential equations and deterministic probability flow equations, and we present expressions for the likelihood of both classes of models.
% %
% Remarkably, we show that our approach leads to maximum likelihood training for models built upon stochastic dynamics; as a counter-result, we show that deterministic models also must control the Fisher divergence from the target to the model, which is costly in general.
% %
% % Unlike the original formulation of score-based diffusion, our method forms a connection between arbitrary distributions -- allowing for the seamless incorporation of prior knowledge and fully generalized data-to-data translation -- reaches the target distribution on a finite time interval, and remains bias-free for any choice of noise strength, which can be tuned as a model hyper-parameter.
% The proposed method conveniently houses many modeling goals under one roof: it forms a connection between arbitrary distributions -- allowing for the incorporation of prior knowledge and to directly perform data-to-data translation -- reaches the target distribution on a finite time interval, and remains bias-free for any choice of noise strength, which can be tuned as a model hyper-parameter.
% %
% In particular, by tuning the level of noise, our framework allows us to directly probe the tradeoffs between generative models based on deterministic and stochastic dynamics.
% %
% In line with our theory, we find that stochastic generative models provide the highest-quality samples, at the expense of a greater number of function evaluations. 
% %
% Surprisingly, we show that probability flow equations of stochastic processes lead to higher-quality samples than fundamentally deterministic dynamics.

% We introduce a new class of generative models based on the stochastic interpolant framework proposed by [Albergo \& V.-E. 2023] that unifies flow-based and diffusion-based methods. 
% %
% Each model is built upon an interpolant that pairs samples from the base and target densities to form a connection in time that is smoothed by the addition of a latent Gaussian random variable.
% %
% The resulting construction yields a family of continuous-time stochastic processes that exactly bridge the base and target on the finite time interval $[0,T]$.
% %
% We show that these stochastic processes can be realized at the level of individual samples through either a deterministic probability flow described by an ordinary differential equation or a diffusive dynamics described by forward and backward stochastic differential equations with an arbitrary level of noise.
% %
% The drift coefficients in these equations can be learned by minimizing two computationally simple quadratic loss functions that also yield access to the time-dependent score of the bridging probability density function. 
% %
% Remarkably, we show that minimization of these quadratic losses leads to maximum likelihood training for generative models built upon stochastic dynamics; by contrast, we show that generative models based upon deterministic dynamics must also control the Fisher divergence from the target to the model.
% %
% We find that with an additional layer of optimization over the deterministic pairing, our approach constructs a Schrodinger bridge between the base and target.

% The proposed method conveniently houses many modeling goals under one roof: it forms a connection between arbitrary distributions (allowing for the incorporation of prior knowledge and to directly perform data-to-data translation), reaches the target distribution on a finite time interval, is versatile and can be adapted to various tasks by exploiting an inherent flexibility in the choice of interpolant, and remains bias-free for any choice of noise strength, which can be tuned as a model hyper-parameter after training. 
% %
% In particular, by tuning the level of noise, our framework allows us to directly probe the trade-offs between generative models based on deterministic and stochastic dynamics for a broad class of transport problems. 
% %
% In line with our theory, we find that stochastic generative models provide the highest-quality samples (at the expense of a greater number of function evaluations) -- surprisingly, we also find that deterministic probability flows trained with a finite amount of noise lead to higher-quality samples than those learned without noise.
%





%We find that use of the latent variable leads to drifts that are smoother in space but remain bias-free, which facilitates efficient numerical integration at sample generation time with either choice of dynamics. 
%




% %
% We introduce a new class of generative models based on the stochastic interpolant framework proposed by [Albergo \& V.-E. 2023] that unifies flow-based and diffusion-based methods. 
% %
% The proposed method is a strict generalization of both score-based diffusions and flows, in that it makes possible defining a learnable probability flow for a generic class of stochastic processes (1) between arbitrary densities and (2) does so exactly over a finite time interval.
% %
% The drift coefficients in these equations can be learned by minimizing two computationally simple quadratic loss functions that also yield access to the time-dependent score of the bridging probability density function. 
% %
% Remarkably, we show that minimization of these quadratic losses leads to maximum likelihood training for generative models built upon stochastic dynamics; by contrast, we show that generative models based upon deterministic dynamics must also control the Fisher divergence from the target to the model.

% The proposed method conveniently houses many modeling goals under one roof: it forms a connection between arbitrary distributions (allowing for the incorporation of prior knowledge and to directly perform data-to-data translation), reaches the target distribution on a finite time interval, is versatile and can be adapted to various tasks by exploiting the flexibility in the choice of interpolant, and remains bias-free for any choice of noise strength, which can be tuned as a model hyper-parameter after training. 


% making accessible control on both the deterministic transport and the score for a generic class of stochastic processes




% The proposed method assembles a set of ingredients that allow for explicit design choices 

% The proposed method assembles a set of ingredients that make possible both theoretically analyzing the tradeoffs between ODEs vs SDEs for generative modeling as well as


% that allows one to probe theoretically and experimentally 


% We introduce a new class of generative models based on the stochastic interpolant framework proposed by [Albergo \& V.-E. 2023] that unifies flow-based and diffusion based methods. 
% %
% %The interpolant framework, which devises a direct means of regressing the drift in a continuity equation describing the map between arbitrary base and target densities, is extended  with the inclusion of a Gaussian random variable. 
% We augment the stochastic interpolant framework -- which devises a direct method to regress the drift of a continuity equation that maps an arbitrary base density to a desired target density -- with an additive Gaussian random variable. \nb{Should we say something else about this random variable, like that it smooths the process? Otherwise I think this makes it sound a little incremental.}
% %
% \nb{I think this next sentence is a little confusing -- do we mean that a broad class of stochastic processes (i.e., other than the OU process) can be used to perform the bridging?}
% We show this construction ensures that a) a broad family of continuous-time stochastic processes can bridged exactly and b) the bridging happens on the finite time interval $[0, T]$.
% %
% %The resulting set of stochastic processes, realizable through \textit{either} a deterministic probability flow described by an ordinary differential equation \textit{or} or a diffusive dynamics described by forward and backward stochastic differential equations, can be assembled with a simple set of ingredients.
% After learning estimates for a velocity field and the time-dependent score function of the bridging probability density function, the resulting stochastic processes can be realized at the level of individual samples through either a deterministic probability flow given by an ordinary differential equation or a diffusive dynamics described by forward and backward stochastic differential equations.
% %The drift coefficients in these equations can be learned by minimizing two computationally simple quadratic loss functions that also yield access to the time-dependent score of the bridging probability density function. 
% %
% Remarkably, we show that minimization of two computationally simple quadratic losses leads to maximum likelihood training for generative models built upon stochastic dynamics; by contrast, we show that generative models based upon deterministic dynamics must, in addition, control the Fisher divergence between the target and the model. We also present two exact paradigms for constructing Schrodinger bridges. Because the recipe for diffusive maps between arbitrary densities is reduced to choosing an interpolant and noise process, this method opens a wide class of possible bridges between densities, and precise knobs to vary the effects of noise. This allows us to theoretically and empirically explore the best design choices for learnable diffusive processes, as well as the resultant tradeoff between ODE and SDE methods. 