\subsection{Background and motivation}
\label{sec:back:mot}

Dynamical approaches for deterministic and stochastic transport have become a central theme in contemporary generative modeling research. At the heart of progress is the idea to apply ordinary or stochastic differential equations (ODE/SDE) to continuously transform samples from a base probability density function (PDF) $\rho_0$ into samples from a target density $\rho_1$ (or vice-versa), and the realization that inference over the velocity field in these equations can be formulated as empirical risk minimization over a parametric class of functions \cite{grathwohl2018scalable,Song2019,ho2020,song2021scorebased,benhamu2022,albergo2023building,liu2022,lipman2022}.

A major milestone was the introduction of score-based diffusion methods (SBDM) \cite{song2021scorebased}, which map an arbitrary density into a Gaussian by passing samples through an Ornstein-Uhlenbeck (OU) process. The key insight of SBDM is that this process can be reversed by introducing a backwards SDE whose drift coefficient depends on the score of the time-dependent density of the process. By learning this score -- which can be done by minimization of a quadratic objective function known as the denoising loss~\cite{vincent_connection_2011} -- the backwards SDE can be used as a generative model that maps Gaussian noise into data from the target. Though theoretically exact, the mapping takes infinite time in both directions, and hence must be truncated in practice.

While diffusion-based methods have become state-of-the-art for tasks such as image generation, there remains considerable interest in developing methods that bridge two \textit{arbitrary} densities (rather than requiring one to be Gaussian), that accomplish the transport \textit{exactly}, and that do so on a \textit{finite} time interval.
%
Moreover, while highest quality results were originally obtained using stochastic sampling techniques for score-based diffusion methods based on SDEs~\cite{song2021scorebased}, this has been challenged by recent works that find equivalent or better performance with deterministic sampling techniques based on ODEs if the score is learned sufficiently well~\cite{Karras2022edm}.
%
If made to match the performance of their stochastic counterparts, ODE-based methods exhibit a number of desirable characteristics that are absent for SDEs, such as an exact, computationally tractable formula for the likelihood and easy application of well-developed adaptive integration schemes for sampling. 
%
It is an open question of significant practical importance to understand if there exists a separation in sample quality between generative models based on deterministic dynamics and those based on stochastic dynamics. 



In order to satisfy the desirable characteristics outlined in the previous paragraph, here we develop a framework for generative modeling based on  the method proposed in~\cite{albergo2023building}, with two  modifications -- one that improves the baseline performance, and one that enables us to quantify explicitly and explore empirically the tradeoffs between stochastic and deterministic models.
The approach is built on the notion of a \textit{stochastic interpolant}~$x_t$ used to bridge  two arbitrary densities $\rho_0$ and $\rho_1$.  We will consider more general designs below, but to fix idea the reader can keep in mind:
\begin{equation}
    \label{eq:stoch:interp:lin}
    x_t = (1-t) x_0 + t x_1 + \sqrt{2t(1-t)} z, \quad t \in [0,1],
\end{equation}
where $x_0$, $x_1$, and $z$ are random variables drawn independently from $\rho_0$, $\rho_1$, and the standard Gaussian density $\mathsf{N}(0,\Id)$, respectively. The stochastic interpolant~$x_t$ defined in~\eqref{eq:stoch:interp:lin} is a continuous-time stochastic process which, by construction, satisfies $x_{t=0} = x_1\sim \rho_1$ and $x_{t=1} = x_1\sim \rho_1$. Its paths therefore exactly bridge between samples  from $\rho_0$ at $t=0$ and from $\rho_1$ at $t=1$, without any bias. A key observation is that:
\begin{quote}
    \textit{The law of the interpolant $x_t$ at any time $t\in[0,1]$ can be realized by many different processes, including an ODE and forward and backward SDEs whose drift coefficients can be learned from data.}
\end{quote} 
To see why this is the case, one must consider the probability distribution of the interpolant $x_t$; as shown below, for a large class of densities $\rho_0$ and $\rho_1$ supported on $\RR^d$, this distribution is absolutely continuous with respect to the Lebesgue measure, and its time-dependent density~$\rho(t)$ satisfies a first-order transport equation, as well as forward and backward Fokker-Planck equations in which the diffusion coefficient can be varied at will. Out of these equations, we can readily derive deterministic and stochastic processes satisfying ODEs and SDEs, respectively, whose densities at time~$t$ are given by $\rho(t)$ and hence coincide in law with the original~$x_t$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/front_figure-final.pdf}
    \caption{A generative model based on the proposed stochastic interpolant framework connecting two densities with no analytic form. Designing the time-dependent probability density bridging these densities  and learning the drift coefficients in its evolution equations is independent of choosing how to sample this density via deterministic or stochastic generative models. \textit{Left panel:} sampling with the probability flow ODE. \textit{Right panel:} Sampling with the SDE with arbitrary noise amplitude, using the same drift as the ODE.}
    \label{fig:my_label}
\end{figure}

Interestingly, the drift coefficients entering these ODE/SDE are the unique minimizers of qua\-dra\-tic objective functions that can be  estimated empirically using data from $\rho_0$, $\rho_1$, and $\mathsf{N}(0,\Id)$. The resulting least-squares regression problem allows us to estimate the drift coefficients of the ODE/SDE, which can then be used to push samples from $\rho_0$ onto new samples from $\rho_1$ and vice-versa. 

\subsection{Main contributions and organization}
\input{contrib}

\subsection{Related work}
\input{related}

\subsection{Notations}
\label{sec:notations}

Thorough we denote probability density functions as $\rho_0(x)$, $\rho_1(x)$, $\rho(t,x)$, with $t\in[0,1]$ and $x\in\RR^d$, omitting the function arguments when this leads to no confusion. We proceed similarly for other functions of time and space, such as $b(t,x)$ or $I(t,x_0,x_1)$. We use the subscript $t$ to denote the time-dependency of stochastic processes, like e.g. the stochastic interpolant $x_t$ or the Wiener process $W_t$. To specify that the random variable $x_0$ is drawn for the probability distribution with density $\rho_0$, say, with a slight abuse of notations we use $x_0\sim\rho_0$. Similarly, we use ${\sf N}(0,\Id)$ to denote both the density and the distribution of the Gaussian random variable with mean zero and covariance identity. We denote expectation by $\EE$, and usually specify what are the random variables this expectation is taken over. With a slight abuse of terminology, we say that the law of the process $x_t$ is $\rho(t)$ if $\rho(t)$ is the density of the probability distribution of $x_t$ at time $t$.

We use standard notations for function spaces, e.g. $C^1([0,1])$ is the space of continuously differentiable functions from $[0,1]$ to $\RR$, $(C^2([0,1])^d$ the space of twice continuously differentiable functions from $\RR^d$ to $\RR^d$, and $C^\infty_0(\RR^d)$ the space of compactly supported, infinitely differentiable function from $\RR^d$ to $\RR$. Given a function $b:[0,1]\times \RR^d \to \RR^d$ with value $b(t,x)$ at $(t,x)$, we  use e.g. $b \in C^1([0,1]; (C^2(\RR^d))^d)$ to indicate that $b$  is continuously differentiable in $t$ for all $(t,x)\in[0,1]\times \RR^d$, and that $b(t,\cdot)$ is an element of $(C^2(\RR^d))^d$ for all $t\in[0,1]$. 
