\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/various_interpolants_t8_table-doc.pdf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}


In this section we discuss how to chose the function $I(t,x_0,x_1)$ entering the definition of the stochastic interpolant~$x_t$ given in~\eqref{eq:stochinterp}. We focus on situations where $I$ is specified \textit{a~priori}, i.e. it is not optimized upon as could be done following the procedure explained in Sec.~\ref{sec:si:schb}. We also focus on functions $I$ that are linear in $x_0$ and $x_1$, i.e. we use~\eqref{eq:lin:interp} which we recall for the reader's convenience:
\begin{equation}
    \tag{\ref{eq:lin:interp}}
    I(t,x_0,x_1)= \alpha(t) x_0+ \beta(t) x_1  
\end{equation} 
where $\alpha,\beta\in C^1([0,1])$ satisfy 
\begin{equation}
    \tag{\ref{eq:lin:interp:a:b}}
    \alpha(0)=\beta(1)=1; \quad \alpha(1)=\beta(0)=0; \quad \forall t\in[0,1] \ : \ \alpha(t)\ge 0, \ \dot \alpha(t)\le 0, \beta(t)\ge0, \ \dot \beta(t)\ge 0.
\end{equation}
Our aim therefore is to propose  some possible functions $\alpha$ and $\beta$ and elucidate the rationale behind these choices. We do this  by first studying the impact of $\alpha$ and $\beta$ if we do not take into account the added influence of the noise, then investigating what can be done if we want to give a more active role to the noise and use it to build an intermediate Gaussian density between $\rho_0$ and $\rho_1$.

\subsection{Direct bridging}
\label{eq:reg:noise}

Since 
\begin{equation}
    \label{eq:min:I}
    \begin{aligned}
        \int_0^1 \int_{\RR^d} |v(t,x)|^2 \rho(t,x) dx dt &= \int_0^1 \int_{\RR^d} \EE\big[  |\partial_t I(t,x_0,x_1)|^2|x_t=x\big] \rho(t,x) dx dt\\
        & \le \int_0^1 \EE \big [ |\partial_t I(t,x_0,x_1)|^2\big] dt
    \end{aligned}
\end{equation}
one possible criterion to pick $I$ is to minimize this upper bound over all $I$ subject to the boundary conditions $I(t=0,x_0,x_1) = x_0$ and $I(t=1,x_0,x_1) = x_1$.  Since $x_0\sim\rho_0$ and $x_1\sim\rho_1$ are independent, performing this  minimization is straightforward and gives the minimizer
\begin{equation}
    \label{eq:min:I:1}
    I(t,x_0,x_1) = (1-t) x_0 + t x_1.
\end{equation}
corresponding to $\alpha(t) = 1-t$ and $\beta (t) = t$. The function $I$ in~\eqref{eq:min:I:1} is the one that was also proposed in~\cite{liu2022}.  It is a natural choice, but it is important to realize that the upper bound in~\eqref{eq:min:I} is no tight of general, and other choices of $I$ may lead to smaller value of $\int_0^1 \int_{\RR^d} |v(t,x)|^2 \rho(t,x) dx dt$. 

Another route to define $I$ is therefore to tailor its choice to some structural properties of the PDF $\rho_0$ and $\rho_1$. Our next result, which we state as a Lemma, indicates that if $\rho_0$ and $\rho_1$ both have zero mean and unit covariance (which could e.g. be achieved by rescaling the data using  PCA), then the trigonometric interpolant used in~\cite{albergo2023building} arises naturally:
\begin{lemma}
\label{lem:trig}
Assume that $\rho_0$ and $\rho_1$ both have zero mean and unit covariance, i.e.
\begin{equation}
    \label{eq:z:m:1:c}
    \int_{\RR^d} x \rho_0(x) dx = \int_{\RR^d} x\rho_1(x) dx = 0, \qquad \int_{\RR^d} xx^\T \rho_0(x) dx = \int_{\RR^d} xx^\T\rho_1(x) dx = \text{\it Id}
\end{equation}
Then the optimization problem
\begin{equation}
    \label{eq:I2:in}
    \min_{I} \int_0^1 \EE \big [ |\partial_t I(t,x_0,x_1)|^2\big] dt \quad \text{subject to} \quad \EE I(t,x_0,x_1) = 0, \quad \EE  \big[I(t,x_0,x_1)I^\T(t,x_0,x_1)\big] = \text{\it Id}
\end{equation}
as well as $I(t=0,x_0,x_1) = x_0$ and $I(t=1,x_0,x_1) = x_1$ has a unique minimizer given by
\begin{equation}
    \label{eq:alpha:beta:1}
    I(t,x_0,x_1)= \cos(\tfrac12 \pi t) x_0+ \sin(\tfrac12 \pi t) x_1.
\end{equation}
\end{lemma}
This corresponds to choosing $ \alpha(t) = \cos(\tfrac12 \pi t)$ and $\beta(t) = \sin(\tfrac12 \pi t)$. Other structural properties of $\rho_0$ and $\rho_1$ can be incorporated into the design of $I$ by generalizing the approach above. To do so with more leeway it is possible to generalize the interpolant in~\eqref{eq:lin:interp} and use instead
\begin{equation}
    \label{eq:lin:interp:gen}
    I(t,x_0,x_1)= A(t) x_0+ B(t) x_1  + c(t),
\end{equation} 
where $A$ and $B$ are time dependent matrices whose coefficients are in $C^1[0,1]$, $c$ is a time-dependent vector whose coefficients are in $C^1[0,1]$, and they satisfy
\begin{equation}
    \label{eq:A:B:c}
    A(0) = B(1) =  \text{\it Id}, \qquad A(1)=B(0) = 0, \qquad c(0)=c(1) = 0.
\end{equation}
We will leave exploration of such $I$ to future work. 



\subsection{Intermediate Gaussianization and shared latent representations}
\label{eq:interp:noise}

Setting $\eps>0$ in~\eqref{eq:stochinterp} makes both the density $\rho$ and the velocities $v$ and $s$ spatially smoother. This is an advantage for the numerical integrators used to simulate the forward SDE~\eqref{eq:sde:1}, the backward SDE~\eqref{eq:sde:R}, and the ODE~\eqref{eq:ode:1}. In view of Theorem~\ref{thm:kl:bound}, it also guarantees better control on the Kullback-Leibler if we use the forward and backward SDE.

Adding the noise in the interpolant $x_t$ defined in~\eqref{eq:stochinterp} has another potential advantage over working with no noise ($\eps=0$), namely we can use the density of the Brownian bridge as intermediate to interpolate between $\rho_0$ and $\rho_1$. More precisely, we can devolve the data from $\rho_0$ completely into noise by the mid-interval at $t=\tfrac12$, and reconstruct the data from $\rho_1$ from this noise starting from $t=\frac12$. One choice of $I$ that allows us to do this is~\eqref{eq:lin:interp} with
\begin{equation}
    \label{eq:alpha:beta:2}
    \alpha(t) = \left\{
    \begin{aligned}
        &\cos^2(\pi t), \quad &t\in[0,\tfrac 12]\\
        &0, \quad &t\in(\tfrac 12,1]      
    \end{aligned} \right., \qquad \beta(t) = \left\{
    \begin{aligned}
        &0, \quad &t\in[0,\tfrac 12]\\
        &\cos^2(\pi t), \quad &t\in(\tfrac 12,1]      
    \end{aligned} \right..
\end{equation}
With this choice it is easy to see that $x_{t=\tfrac12}= \sqrt{2\eps} B_{t=\tfrac12} \sim N(0,\tfrac12 \eps)$, which is akin to seamlessly glue together two one-sided interpolants at midinterval, with one important difference: Even though we encode into noise the data from $\rho_0$ on the interval $[0,\tfrac12]$ and decode this noise into data from $\rho_1$ on the interval $[\tfrac12,1]$ (and vice-versa if we proceed backward in time), the resulting velocities $v$ and $s$ still encode some continuity between the data from $\rho_0$ and $\rho_1$. This is most clearly seen at the level of the ODE~\eqref{eq:ode:1}, since the solution $X_t$ to this equation is a bijection between initial and final conditions $X_{t=0}$ and $X_{t=1}$,  but a similar pairing can also be observed in the solutions to the forward and backward SDE~\eqref{eq:sde:1} and~\eqref{eq:sde:R} whose solution at time $t=1$ or $0$ are correlated with the initial or final condition used. 

\begin{remark}
We stress that it is key to have $\eps>0$ in order to use the interpolant with the coefficients in~\eqref{eq:alpha:beta:2}. Indeed, without the diffusive part in the definition of $x_t$, the density $\rho(t)$ would collapse at $t=\frac12$ if $\eps=0$, but it does not if $\eps>0$. Not that this example also shows that \textit{the inclusion of the diffusive part in the definition of the interpolant matters even if we  the ODE~\eqref{eq:ode:1} rather than the SDEs~\eqref{eq:sde:1} and \eqref{eq:sde:R} in the generative models} (which is clear since all three process have the same density $\rho(t)$ which is always affected by inclusion of the diffusive part). 
\end{remark}


\subsection{One-sided interpolants for generation from a Gaussian density}
\label{app:onesided}
\michael{I think from here on, this section should be titled "Interpolant Design" , and the one-sided interpolant becomes one of those in the section.}
In this appendix, we consider the special case when $\rho_1 = N(0,C_1)$, where $C_1$ is a positive definite matrix; without loss of generality, we assume that $C_1$ which can be represented as $C_1 = \sigma_1^\T \sigma_1$ where $\sigma_1$ is a lower triangular matrix. Because of the special structure of $\rho_1$, we can introduce a new type of stochastic interpolant tailored to the situation:

\begin{definition}[One-side stochastic interpolant]
\label{def:interp:os}
 Given a probability density function $\rho_0: {\RR^d} \rightarrow \RR_{\geq 0}$, a \textit{one-side stochastic interpolant} between $\rho_0$ and $N(0,C_1)$ is a stochastic process $y_t$ defined as
\begin{equation}
    \label{eq:stochinterp:os}
    y_t = J(t,x_0) + \sigma_1 W_t,  \qquad t\in [0, 1],
\end{equation}
where: 
\begin{enumerate}[leftmargin=0.15in]
\item $J: [0,1]\times \RR^d \to \RR^d$ and satisfies the boundary conditions $J(0,x_0) = x_0$ and $J(1,x_0) = 0$;
\item $x_0$ is a random variables drawn from $\rho_0$;
\item $W_t$ is the $d$-dimensional Wiener process $W_t$, assumed independent of $x_0$.
\item $\sigma_1$ is such that $\sigma_1^\T\sigma_1=C_1$.
\end{enumerate}
\end{definition}

By construction, we then have $y_{t=0} \sim \rho_0$ and $y_{t=1} \sim N(0,C_1)$ since $W_{t=1} \sim N(0,\text{\it Id})$. Therefore, the  distribution of the stochastic process~$y_t$ bridges $\rho_0$ and $N(0,C_1)$ and we can analyze its properties as we did in main text for the stochastic interpolant~$x_t$. To this end, we define the two velocities $w$ and $s$ (the second velocity will again be the score of the PDF of $y_t$, hence the notation~$s$):
\begin{equation}
    \label{eq:w:def:os}
    w(t,x) = \EE\left( \partial_t J(t,x_0)| y_t = x\right), \qquad (t,x) \in [0,1]\times \RR^d,
\end{equation}
and
\begin{equation}
    \label{eq:s:def:os}
    s(t,x) = t^{-1} \sigma^{-\T}\EE\left( W_t | y_t = x\right), \qquad \forall(t,x) \in (0,1]\times \RR^d,
\end{equation}
supplemented at $t=0$ by
\begin{equation}
    \tag{\ref{eq:s:def:os}'}
    s(0,x) = \nabla \log \rho_0(x), \qquad \forall x \in \RR^d.
\end{equation}
In~\eqref{eq:w:def:os} and~\eqref{eq:s:def:os}, $\EE\left( \cdot | y_t = x\right)$ denotes expectation on $x_0\sim\rho_0$ and $W_t$ conditional on $y_t=x$.


We also make the counterpart of Assumption~\ref{as:rho:I}:
\begin{assumption}
\label{as:rho:J}
The PDF $\rho_0$ is  strictly positive, in $C^2(\RR^d)$, and such that
\begin{equation}
    \label{eq:rho0:sc}
     \int_{\RR^d} |\nabla \log \rho_0(x)|^2 \rho_0(x) dx < \infty.
\end{equation}
The interpolant $J$ is in $C^2([0,1],C^2(\RR^d))^d)$ and such that
\begin{equation}
    \label{eq:bound:dJ}
    \begin{aligned}
        &\exists C_1<\infty  \   : \ 
        &&|\partial_t J(t,x_0)|\le C_1|x_0|
        \quad  &&\forall (t,x_0) \in [0,1]\times \RR^d,
        \end{aligned}
\end{equation}
and
\begin{equation}
    \label{eq:Jt:L2}
    \exists M_1,M_2 < \infty  \ \ : \ \  \EE\big[ |\partial_t J(t,x_0)|^4\big] \le M_1; \quad \EE\big[ |\partial^2_t J(t,x_0)|^2\big] \le M_2, \quad  \forall t\in [0,1],
\end{equation}
where the expectation is taken over $x_0\sim \rho_0$.
\end{assumption}

We then have the equivalent of Theorems~\ref{prop:interpolate} and~\ref{prop:interpolate_losses}, and Corollary~\ref{prop:generative}, all proven in Appendix~\ref{app:onesided}:

\begin{restatable}[Continuity equations for one-sided interpolants]{theorem}{interpos}
    \label{thm:interp:os}
    The probability distrbution of the one-sided stochastic interpolant $y_t$ defined in~\eqref{eq:transport:os} has a density $\rho(t)$, which satisfies  $\rho(0) = \rho_0$ and $\rho(1)=N(0,C_1)$, is in $C^1([0,1];C^p(\RR^d))$ for any $p\in\NN$, and is positive everywhere on $\RR^d$ for all $t\in [0,1]$. The velocities~$w$ and $s$ defined in~\eqref{eq:w:def:os} and~\eqref{eq:s:def:os}  are both in $C^0([0,1];(C^p(\RR^d))^d)$ for any $p\in \NN$, and these functions are such that
\begin{equation}
    \label{eq:vt:wt:bounded:os}
    \forall t\in [0,1] \quad : \quad \int_{\RR^d} |w(t,x)|^2 \rho(t,x) dx < \infty, \qquad \int_{\RR^d} |s(t,x)|^2 \rho(t,x) dx < \infty.
\end{equation}
In addition, the density $\rho$ satisfies the following three dynamical equations:
\begin{enumerate}[leftmargin=0.15in]
\item The Fokker-Planck equation
\begin{equation}
    \label{eq:fpe:os}
    \partial_t \rho + \nabla \cdot \left(c_\fwd\rho\right) = \tfrac12 C_1  :\nabla \nabla \rho, \qquad \rho(0) = \rho_0,
\end{equation}
where we defined the forward drift
\begin{equation}
    \label{eq:b:def:os}
    c_\fwd(t,x) = w(t,x).
\end{equation}
Equation~\eqref{eq:fpe:os} is well-posed when solved forward in time from $t=0$ to $t=1$, and its solution for the initial condition $\rho(t=0) = \rho_0$ satisfies $\rho(t=1) = N(0,C_1)$. 

\item  The backward Fokker-Planck equation
\begin{equation}
    \label{eq:fpe:tr:os}
    \partial_t \rho + \nabla \cdot \left(c_\rev\rho\right) = -\tfrac12 C_1:\nabla \nabla \rho, \qquad \rho(1) = N(0,C_1),
\end{equation}
where we defined
\begin{equation}
    \label{eq:b:r:def:os}
    c_\rev(t,x) = w(t,x) - C_1 s(t,x).
\end{equation}
Equation~\eqref{eq:fpe:tr:os} is well-posed when solved backward in time from $t=1$ to $t=0$, and its solution for the final condition $\rho(1) = N(0,C_1)$ satisfies $\rho(0) = \rho_0$. 
\item The transport equation
\begin{equation}
    \label{eq:transport:os}
    \partial_t \rho + \nabla \cdot \left(c_\ODE\rho\right) = 0, 
\end{equation}
where we defined the forward-backward drift
\begin{equation}
    \label{eq:b:ode:def:os}
    c_\ODE(t,x) = w(t,x) - \tfrac12 C_1s(t,x).
\end{equation}
Equation~\eqref{eq:transport:os} can be solved either forward in time from the initial condition $\rho(0)=\rho_0$, in which case $\rho(1)=N(0,C_1)$, or backward in time from the final condition $\rho(1)=N(0,C_1)$, in which case $\rho(0)=\rho_0$. 
\end{enumerate}
Finally, $s$ is the score of $\rho$
\begin{equation}
    \label{eq:score:os}
    s(t,x) = \nabla \log \rho(t,x), \qquad \forall (t,x) \in [0,1]\times\RR^d.
\end{equation}
\end{restatable}

\begin{restatable}[One-sided objectives]{theorem}{objos}
\label{prop:interpolate_losses:os}
The velocity $w$ and the score $s$ defined in~\eqref{eq:w:def:os} and~\eqref{eq:s:def:os} are the unique minimizers in $C^1([0,1];(C^1(\RR^d))^d)$ of the quadratic objectives
\begin{equation}
    \label{eq:obj:w:os}
    \mathcal{L}_w[\hat{w}] =\int_0^1   \EE \left( \tfrac12|\hat w(t,y_t)|^2 - \partial_t J(t,x_0) \cdot \hat w(t,y_t) \right) dt
\end{equation}
and
\begin{equation}
    \label{eq:obj:s:os}
    \mathcal{L}_s[\hat{s}] = \int_0^1 \EE\left( \tfrac12|\hat s(t,y_t)|^2 +t^{-1} W_t\cdot \hat s(t,y_t) \right) dt
% =\EE\int_0^1\left( \tfrac12|\hat s(t,x_t)|^2 dt -(\seps \, t)^{-1} \hat s(t,x_t)dB_t \right)
\end{equation}
where $y_t$ is defined in~\eqref{eq:stochinterp:os} and the expectation $\EE$ is taken independently over $x_0\sim \rho_0$ and the Wiener process $W_t$.
 % in the last equation, the integral  involving $dB_t$ should be interpreted in It\^o sense.
\end{restatable}

\begin{restatable}[Generative models for one-sided interpolants]{corollary}{genos}
\label{prop:generative:os}
At any time $t\in[0,1]$, the law of the stochastic interpolant~$y_t$ coincides with the law of the three processes $X^\fwd_t$, $X^\rev_t$, and $X_t$ respectively defined as:
\begin{enumerate}[leftmargin=0.15in]
\item The solutions of the stochastic differential equation (SDE) associated with the FPE~\eqref{eq:fpe}
\begin{equation}
    \label{eq:sde:1:os}
    dX^\fwd_t = c_\fwd(t,X^\fwd_t)dt  + \sigma_1  dW_t,
\end{equation}
solved forward in time from initial data~$X^\fwd_{t=0}\sim\rho_0$ independent of $W$.

\item  The solutions of the backward SDE associated with the backward FPE~\eqref{eq:fpe:tr}
\begin{equation}
    \label{eq:sde:R:os}
    dX^\rev_t = c_\rev(t,X^\rev_t)dt  + \sigma_1  dW^\rev_t, \quad W_t^\rev = -W_{1-t},
\end{equation}
solved backward in time from final data~$X^\rev_{t=1}\sim N(0,C_1)$ independent of $W^\rev$.
\item The solutions of the ordinary differential equation (ODE) (aka probability flow) associated with the transport equation~\eqref{eq:transport}
\begin{equation}
    \label{eq:ode:1:os}
    \frac{d}{dt}  X_t = c_\ODE(t, X_t),
\end{equation}
solved either forward in time from initial data $X_{t=0}  \sim\rho_0$ or backward in time from final data  $X_{t=1} = x_1\sim N(0,C_1)$. 
\end{enumerate}
\end{restatable}

The likelihood bounds obtained in Sec.~\ref{sec:likelihood_bounds} can esaily be generalized to the models above, and one-sided interpolants can also be used for generative modeling, density estimation, likelihood evaluation, and cross-entropy calculations, as explained in Secs.~\ref{sec:density}, \ref{sec:ce}. For the sake of brevity, we will not repeat these results but rather focus on some features specific to one-sided interpolants, in particular in their connections to score-based diffusion models and rectified flows.

\subsubsection{Comparison with score-based diffusion models}
\label{sec:SBDM}

The one-sided interpolant $y_t$ satisfies the SDE
\begin{equation}
    \label{eq:os:sde}
    d y_t = \partial_t J(t,x_0) dt + \sigma_1 dW_t, \qquad y_{t=0} = J(t=0,x_0) = x_0 \sim \rho_0 
\end{equation}
This SDE bears similarity with the OU process used in SBDM, namely
\begin{equation}
    \label{eq:sbdm:sde}
    d Z_t = - Z_t dt + \sigma_1 dW_t, \qquad Z_{t=0} \sim \rho_0 
\end{equation}
The main differences between~\eqref{eq:os:sde} and \eqref{eq:sbdm:sde} is that the latter is closed. In fact, to write~\eqref{eq:os:sde} as a closed SDE, we need to take the  expectation of its drift term $\partial_t J(t,x_0)$ conditional on $y_t=x$, to write it as the forward SDE in~\eqref{eq:sde:1:os}. That is, the use this SDE, and its backward version~\eqref{eq:sde:R:os}, we need to learn both the velocity field $w$ defined in~\eqref{eq:w:def:os} and the score $s$ defined~\eqref{eq:s:def:os}.  The advantage of doing so is that we then get a generative model working on the time interval $[0,1]$. In contrast, the SDE~\eqref{eq:sbdm:sde} used in SBDM only pushes the initial  PDF $\rho_0$ unto the final $\rho_1 = N(0,C_1)$ if we use it on the infinite time interval $[0,\infty)$. In practice, learning must be capped to some finite-time interval $[0,T]$ with $T<\infty$, and the backward SDE used in SBDM must also be restricted to $[0,T]$. Since the final conditions used in the backward SDE are drawn from $\rho_1 = N(0,C_1)$ but the PDF of the process~\eqref{eq:sbdm:sde} is not exactly this Gaussian density at time $T$, i.e. $\rho(T) \not = N(0,C_1)$, this introduces a small bias absent with one-sided interpolants. In addition, the final time $T$ becomes an additional meta-parameter, and learning of the score must be done on the full interval $[0,T]$, rather than $[0,1]$. Of course, using stochastic interpolants rather than their one-sided version offers the additional advantage that $\rho_1$ can be any PDF, not just $N(0,C_1)$. 




\subsubsection{Rectifying one-sided interpolants}
\label{sec:rect}

One-sided stochastic interpolants offer a way to apply the rectifying procedure proposed in~\cite{}, but without introducing any bias. To see why, suppose that we started with some $J_t(x_0)$ and have used it to learn $v$ and $s$ via minimization of~\eqref{eq:obj:w:os} and~\eqref{eq:obj:s:os}. Denote by $X_t(x_0)$ the solution to the ODE  \eqref{eq:ode:1:os} for the initial condition $X_{t=0}(x_0)=x_0$, i.e.
\begin{equation}
    \label{eq:ode:1:os:x0}
    \frac{d}{dt}  X_t(x_0) = c_\ODE(t, X_t(x_0)), \qquad X_{t=0}(x_0)=x_0.
\end{equation}
If we pick a function $\alpha:[0,1]\to [0,1]$ such that $\alpha(0)=1$, $\alpha(1) =0$, we can then define a new one-side interpolant using
\begin{equation}
    \label{eq:new:os}
    y'_t = \alpha(t) X_t(x_0) + \sigma_1 W_t
\end{equation}
Clearly, $y'_{t=0} = x_0$ since $\alpha(0) =1$, $X_{t=0}(x_0)=x_0$, and $W_{t=0} =0$, and $y'_{t=0} = \sigma_1 W_{t=1} \sim C(0,C_1)$ since $\alpha(1)=0$. We can then learn a new $w'$ and $s'$ via minimization of~\eqref{eq:obj:w:os} and~\eqref{eq:obj:s:os} using the new interpolant, i.e. by substituting $y_t'$ for $y_t$ and $\partial_t [\alpha(t) X_t(x_0)] = \dot \alpha (t) X_t(x_0) + \alpha(t) C_\ODE(t,X_t(x_0))$  for $\partial_t J_t(x_0)$ in these objectives. This rectification procedure is similar to the one proposed in~\cite{}, except that it is unbiased, since~\eqref{eq:new:os} is a valid one-sided interpolant that satisfies all the conditions of Definition~\ref{def:interp:os}. The procedure could also be iterated upon. While it adds cost to the learning, the benefit is that it will lead to simpler flows, which was the original motivation of~\cite{}.   