\section{Bridging two Gaussian mixture densities}
\label{app:Gauss:mixt}
In this appendix, we consider the case where $\rho_0$ and $\rho_1$ are both Gaussian mixture densities. We denote by
\begin{equation}
\label{eq:NmC}
\begin{aligned}
    {\sf N}(x,|m,C) &= (2\pi)^{-d/2} [\det C]^{-1/2} \exp\left(-\tfrac12 (x-m)^\T C^{-1} (x-m)\right),\\
    & = (2\pi)^{-d} \int_{\RR^d} e^{ik\cdot (x-m) -\frac12 k^\T C k} dk,
\end{aligned}
\end{equation}
the Gaussian probability density with mean vector $m \in \RR^d$ and positive-definite symmetric covariance matrix $C=C^\T\in \RR^{d\times d}$. We assume that
\begin{equation}
    \label{eq:rho0:mixt}
    \rho_0(x) = \sum_{i=1}^{N_0} p^0_i {\sf N}(x,|m^0_i,C^0_i), \qquad
    \rho_1(x) = \sum_{i=1}^{N_1} p^1_i {\sf N}(x,|m^1_i,C^1_i)
\end{equation}
where $N_0,N_1\in \NN$,  $p^0_i>0$ with $\sum_{i=1}^{N_0} p_i^0 = 1$, $m_i^0\in \RR^d$, $C_i^0= (C_i^0)^\T \in \RR^{d\times d}$, positive-definite, and similarly for $p_i^1$, $m_i^1$, and $C_i^1$. 
We have:
\begin{restatable}{proposition}{gaussmixt}
\label{th:Gauss:mixt}
Consider the process $x_t$ defined in~\eqref{eq:stochinterp} using the probability densities in~\eqref{eq:rho0:mixt} and the interpolant in~\eqref{eq:lin:interp}, i.e.
\begin{equation}
    \tag{\ref{eq:lin:interp}}
    I(t,x_0,x_1)= \alpha(t) x_0+ \beta(t) x_1  
\end{equation}  
with $\alpha(t)$ and $\beta(t)$ satisfying~\eqref{eq:lin:interp:a:b}. Denote \begin{equation}
    \label{eq:mij:Cij}
    m_{ij}(t) = \alpha(t) m^0_i+\beta(t) m^1_j, \quad C_{ij}(t) = \alpha^2(t) C^0_i+\beta^2(t) C^1_j +\gamma^2(t) \text{\it Id}, 
\end{equation}
where $i=1,\ldots, N_0,$ $j=1,\ldots, N_1$.
Then the probability density $\rho$ of $x_t$ is the Gaussian mixture density
\begin{equation}
    \label{eq:rhot:Gaussmixt}
    \rho(t,x) = \sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j {\sf N}(x,| m_{ij}(t),C_{ij}(t))
\end{equation}
and  the velocity $b$ and the score $s$ defined in~\eqref{eq:b:ode:def} and \eqref{eq:s:def} are 
\begin{equation}
    \label{eq:vt:Gausmixt}
    b(t,x) = \frac{\sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j \left(\dot m_{ij}(t) + \tfrac12\dot C_{ij}(t) C^{-1}_{ij}(t)(x-m^{ij}(t))  \right) {\sf N}(x,| m_{ij}(t),C_{ij}(t))}{\sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j {\sf N}(x,| m_{ij}(t),C_{ij}(t))},
\end{equation}
and
\begin{equation}
    \label{eq:st:Gausmixt}
    s(t,x) = -\frac{\sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  C^{-1}_{ij}(t)(x-m_{ij}(t))  {\sf N}(x,| m_{ij}(t),C_{ij}(t))}{\sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_i p^1_j {\sf N}(x,| m_{ij}(t),C_{ij}(t))}.
\end{equation}
\end{restatable}
This proposition implies that $b$ and $s$ grow at most linearly in $x$, and are approximately linear in regions where the modes of $\rho(t,x)$ remain well-separated. In particular, if $\rho_0$ and $\rho_1$ are both Gaussian densities, $\rho_0=N(m_0,C_0)$ and $\rho_1=N(m_1,C_1)$, we have  
\begin{equation}
    \label{eq:vt:mode:Gaussmixt}
    b(t,x)=\dot m(t) + \tfrac12 \dot C(t) C^{-1}(t)(x-m(t)),
\end{equation}
and
\begin{equation}
    \label{eq:st:mode:Gaussmixt}
    s(t,x)= - C^{-1}(t)(x-m(t))  ,
\end{equation}
where
\begin{equation}
    \label{eq:mt:Ct}
    m(t) = \alpha(t) m_0+ \beta(t) m_1, \quad C(t) = \alpha^2(t) C_0+ \beta^2(t) C_1 +\gamma^2(t) \text{\it Id}.
\end{equation}
Note that the probability flow ODE~\eqref{eq:ode:1} associated with the velocity~\eqref{eq:vt:mode:Gaussmixt} is the linear ODE
\begin{equation}
    \label{eq:ode:gm}
    \frac{d}{dt} X_t = \dot m(t) + \tfrac12 \dot C(t) C^{-1}(t)(X_t-m(t)).
\end{equation}
This equation can only be solved analytically if $\dot C(t)$ and $C(t)$ commute (which is the case e.g. if $C_0 = \Id$), but it is easy to see that it always guarantees that 
\begin{equation}
    \label{eq:ode:gm:sol}
    \EE_0 X_t(x_0) =  m(t), \qquad \EE_0 \big[(X_t(x_0)-m(t) (X_t(x_0)-m(t))^\T\big]  = C(t),
\end{equation}
where $X_t(x_0)$ denotes the solution to~\eqref{eq:ode:gm} for the initial condition $X_{t=0}(x_0) = x_0$ and $\EE_0$ denotes expectation over $x_0\sim \rho_0$. A similar statement is true if we solve \eqref{eq:ode:gm} with final conditions at $t=1$ drawn from $\rho_1$. Similarly, the forward SDE~\eqref{eq:sde:1} associated with the velocity~\eqref{eq:vt:mode:Gaussmixt}  and the score~\eqref{eq:st:mode:Gaussmixt} is the linear SDE
\begin{equation}
    \label{eq:sde:gm}
    d X^\fwd_t = \dot m(t)dt + \big(\tfrac12 \dot C(t) - \eps\big) C^{-1}(t)(X^\fwd_t-m(t)) dt + \sqrt{2\eps} dW_t.
\end{equation}
and its solutions are such that  
\begin{equation}
    \label{eq:sde:gm:sol}
    \EE_0 \EE^{x_0}_\fwd X_t^\fwd =  m(t), \qquad \EE_0 \EE^{x_0}_\fwd\big[(X^\fwd_t-m(t) (X_t^\fwd-m(t))^\T\big]  = C(t),
\end{equation}
where $\EE_\fwd^{x_0}$ denotes expectation over the solution of \eqref{eq:sde:gm} conditional on the event $X^\fwd_{t=0}=x_0$ and $\EE_0$ denotes expectation over $x_0\sim \rho_0$. A similar statement also holds for the backward SDE~\eqref{eq:sde:R}.

\begin{proof} The characteristic function of $\rho(t,x)$ is given by
\begin{equation}
    \label{eq:rhot:Gaussmixt:k}
    g(t,k)  = \EE e^{ik\cdot x_t} = \sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  e^{ik\cdot m_{ij}(t) - \tfrac12 k^\T C_{ij}(t) k } .
\end{equation}
whose inverse Fourier transform is~\eqref{eq:rhot:Gaussmixt}. This automatically implies~\eqref{eq:st:Gausmixt} since we know from~\ref{eq:s:def} that  $s= \nabla \log \rho$. To derive \eqref{eq:vt:mode:Gaussmixt} use the function $m$ defined in~\eqref{eq:F12k}:
\begin{equation}
    \label{eq:m:Gaussmixt:k}
    m(t,k)  = \sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  (\dot m_{ij}(t) +\tfrac12 i \dot C_{ij}(t) k)e^{ik\cdot m_{ij}(t) - \tfrac12 k^\T C_{ij}^\gamma(t) k } .
\end{equation}
From~\eqref{eq:F1k:c}, we know that the inverse Fourier transform of this function is $b\rho$, so that we obtain
\begin{equation}
    \label{eq:m:Gaussmixt:x}
    b(t,x) \rho(t,x) = \sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  \left(\dot m_{ij}(t) + \tfrac12\dot C_{ij}(t) C_{ij}^{-1}(t)(x-m_{ij}(t))\right) {\sf N}(x,| m_{ij}(t),C^\eps_{ij}(t)).
\end{equation}
This gives~\eqref{eq:vt:Gausmixt}.
\end{proof}


\section{Proofs}
In this appendix, we provide the details for proofs omitted from the main text. For ease of reading, a copy of the original theorem statement is provided with the proof.


\subsection{Proof of Theorems~\ref{prop:interpolate}, ~\ref{prop:interpolate_losses}, and~\ref{thm:score}, and Corollary~\ref{prop:interpolate_fpe}.}
\label{app:proof:interpolate}
\interpolation*
\begin{proof}
 Let $g(t,k) = \EE e^{ik \cdot x_t}$,  $k \in \RR^d $, be the characteristic function of $\rho(t,x)$. By the definition of $x_t$ in~\eqref{eq:stochinterp},
\begin{equation}
\label{eq:charact}
 g(t,k) = \EE e^{ik\cdot (I(t,x_0,x_1) + \gamma(t) z)}.
\end{equation}
 Using the independence between $x_0$, $x_1$, and $z$ , we have
\begin{equation}
    \label{eq:gt:k}
    g(t,k) = \EE \left(e^{ik\cdot I(t,x_0,x_1) }\right) \EE \left( e^{i\gamma(t)  k\cdot z}\right) \equiv g_0(t,k)  e^{-\tfrac12 \gamma^2(t) |k|^2 }
\end{equation}
where we defined
\begin{equation}
    \label{eq:G0}
    g_0(t,k) = \EE \left(e^{ik\cdot I(t,x_0,x_1) }\right)
\end{equation}
The function~$g_0(t,k) $ is the characteristic function of $I(t,x_0,x_1)$ with $x_0\sim\rho_0$ and $x_1\sim\rho_1$. From~\eqref{eq:gt:k}, we have
\begin{equation}
    \label{eq:bound:g}
    |g(t,k)| = |g_0(t,k)|  e^{-\tfrac12 \gamma^2(t) |k|^2  } \le e^{-\tfrac12 \gamma^2(t) |k|^2  }
\end{equation} 
Since $\gamma(t)>0$ for all $t\in(0,1)$ by assumption, this shows that 
\begin{equation}
    \label{eq:int:g}
    \forall p \in \NN\ \ \text{and} \ \ t\in(0,1) \quad : \quad \int_{\RR^d} |k|^p |g(t,k)|dk<\infty 
\end{equation} 
implying that $\rho(t,\cdot)$ is in $C^p(\RR^d)$ for any $p\in \NN$ and all $t\in (0,1)$.  From~\eqref{eq:gt:k}, we also have 
\begin{equation}
    \label{eq:bound:dtg}
    \begin{aligned}
        |\partial_t g(t,k)|^2 & = \left|\EE[ (ik\cdot \partial_t I_t(x_0,x_1) - \gamma(t) \dot \gamma(t) |k|^2)  e^{ik\cdot I_t(x_0,x_1)}]\right|^2 e^{-\gamma^2(t) |k|^2 }  \\ 
        &\le 2\left( |k|^2 \EE\big[|\partial_t I_t(x_0,x_1)|^2] + |\gamma(t) \dot \gamma(t)|^2 |k|^4\right) e^{-\gamma^2(t) |k|^2 }\\
        &\le 2\left( |k|^2 M_1 + 4|\gamma(t) \dot \gamma(t)|^2 |k|^4\right) e^{-\gamma^2(t)  |k|^2 }
    \end{aligned}
\end{equation}
and 
\begin{equation}
    \label{eq:bound:dttg}
    \begin{aligned}
        |\partial^2_t g(t,k)|^2 & \le 4\left( |k|^2 \EE\big[|\partial^2_t I_t(x_0,x_1)|^2] +(|\dot \gamma(t)|^2 + \gamma(t) \ddot \gamma(t))^2 |k|^4\right) e^{-\gamma^2(t) |k|^2 }\\
        & \quad + 8\left( |k|^2 \EE\big[|\partial^2_t I_t(x_0,x_1)|^4] +  (\gamma(t) \dot \gamma(t))^4  |k|^8\right) e^{-\gamma^2(t)  |k|^2 }  \\ 
        &\le 4\left( |k|^2 M_2 +(|\dot \gamma(t)|^2 + \gamma(t) \ddot \gamma(t))^2 |k|^4\right) e^{-\gamma^2(t) |k|^2 }\\
        & \quad + 8\left( |k|^2 M_1\EE\big[|\partial^2_t I_t(x_0,x_1)|^4] +  (\gamma(t) \dot \gamma(t))^4  |k|^8\right) e^{-\gamma^2(t)  |k|^2 } 
    \end{aligned}
\end{equation}
where in both cases we used~\eqref{eq:It:L2} in Assumption~\ref{as:rho:I} to get the last inequalities. These imply that
\begin{equation}
    \label{eq:int:dg}
    \forall p \in \NN\ \ \text{and} \ \ t\in(0,1) \quad : \quad \int_{\RR^d} |k|^p |\partial_t g(t,k)|dk<\infty; \quad \int_{\RR^d} |k|^p |\partial^2_t g(t,k)|dk<\infty
\end{equation} 
indicating that $\partial_t \rho(t,\cdot)$ and $\partial^2_t \rho(t,\cdot)$ are in $C^p(\RR^d)$ for any $p\in \NN$, i.e. $\rho\in C^1((0,1); C^p(\RR^d))$ as claimed. To show that $\rho$ is also positive, denote by $\mu_0(t,dx)$ the unique (by the Fourier inversion theorem) probability measure associated with $g_0(t,k)$, i.e. the measure such that
\begin{equation}
    \label{eq:meas}
    g_0(t,k) = \int_{\RR^d} e^{ik\cdot x} \mu_0(t,dx).
\end{equation}
From~\eqref{eq:gt:k} and the convolution theorem it follows that we can express $\rho$ as
\begin{equation}
    \label{eq:rhot:invF}
    \rho(t,x) = \int_{\RR^d } \frac{e^{-|x-y|^2/(2\gamma^2(t))}}{(2\pi \gamma^2(t))^{d/2}} \mu_0(t,dy),
\end{equation}
This shows that  $\rho>0$ for all $(t,x)\in (0,1)\times \RR^d$. Since $x_{t=0} = x_0$ and  $x_{t=1} = x_1$ by definition of the interpolant, we also have $\rho(0) = \rho_0$ and $\rho(1)=\rho_1$, which shows that $\rho$ is also positive and in $C^p(\RR^d)$  at $t=0,1$ by Assumption~\ref{as:rho:I}. Note that since $\rho \in C^1((0,1); C^p(\RR^d))$ and is positive, we also immediately deduce that $s = \nabla \log \rho = \nabla \rho /\rho \in C^1((0,1); (C^p(\RR^d))^d)$.

To show that $\rho$ satisfies the TE~\eqref{eq:transport}, take the time derivative of~\eqref{eq:charact} to deduce that
\begin{equation}
    \label{eq:ito2}
    \partial_t g(t,k) = ik\cdot m(t,k)
\end{equation}
where $m: [0,1]\times \RR^d\to\CC^d$ is the vector-valued function defined as
\begin{equation}
    \label{eq:F12k}
     m(t,k) = \EE\left(( \partial_t I_t(x_0,x_1) +\dot \gamma(t) z)e^{ik\cdot x_t} \right).
\end{equation}
By definition of the conditional expectation, $m(t,k)$ can be expressed as
\begin{equation}
    \label{eq:F1k:c}
    \begin{aligned}
     m(t,k) & = \int_{\RR^d} \EE\left(( \partial_t I_t(x_0,x_1) +\dot \gamma(t) z) e^{ik\cdot x_t} |x_t=x \right) \rho(t,x) dx\\
     & = \int_{\RR^d} e^{ik\cdot x} \EE\left(( \partial_t I_t(x_0,x_1) +\dot \gamma(t) z) |x_t=x \right) \rho(t,x) dx\\
     & = \int_{\RR^d} e^{ik\cdot x} b(t,x) \rho(t,x) dx
     \end{aligned}
\end{equation}
where the last equality follows from the definition of $b$ in~\eqref{eq:b:ode:def}. Inserting~\eqref{eq:F1k:c}in~\eqref{eq:ito2}, we deduce that this equation can be written in real space as the TE~\eqref{eq:transport}.



Let us now investigate the regularity of $b$. To that end, we go back to $m$, and use the independence  between $x_0$, $x_1$, and $z$, as well as Gaussian integration by part to deduce that
\begin{equation}
    \label{eq:F1:1}
    m(t,k) = \EE\left((\partial_t I(t,x_0,x_1) -i \gamma(t) \dot\gamma(t) k) e^{ik\cdot I(t,x_1,x_0)}\right) e^{-\tfrac12 \gamma^2(t) |k|^2 },
\end{equation}
As a result
\begin{equation}
    \label{eq:F1:int}
    \begin{aligned}
        |m(t,k)|^2 &= \left|\EE\left((\partial_t I(t,x_0,x_1) -i \gamma(t) \dot\gamma(t) k) e^{ik\cdot I(t,x_1,x_0)}\right)\right|^2 e^{-\gamma^2(t)  |k|^2 } \\
        & \le  2\left(\EE\big[|\partial_t I(t,x_0,x_1)|^2\big] + |\gamma(t) \dot \gamma(t)|^2 |k|^2\right) e^{-\gamma^2(t)  |k|^2 } \\
        & \le 2M_1 e^{-\gamma^2(t)  |k|^2 },
    \end{aligned}
\end{equation}
and
\begin{equation}
    \label{eq:F1:dint}
    \begin{aligned}
        |\partial_t m(t,k)|^2
        &\le 4\left(\EE\big[|\partial^2_t I(t,x_0,x_1)|^2 + (\gamma(t) \ddot \gamma(t) + \dot \gamma^2(t))^2 \right) e^{-\gamma^2(t) |k|^2 } \\
        & \quad + 8|k|^2 \left( \EE \big |\partial_t I(t,x_0,x_1)|^4\big] + (\gamma(t) \dot \gamma(t))^4 |k|^4 \right) e^{-\gamma^2(t) |k|^2 } \\
        & \le 4\left(M_1 + (\gamma(t) \ddot \gamma(t) + \dot \gamma^2(t))^2 \right) e^{-\gamma^2(t) |k|^2 } \\
        & \quad + 8|k|^2 \left( M_2 + (\gamma(t) \dot \gamma(t))^4 |k|^4 \right) e^{-\gamma^2(t) |k|^2 } ,
    \end{aligned}
\end{equation}
where in both cases the last inequalities follow from~\eqref{eq:It:L2}. Therefore 
\begin{equation}
    \label{eq:int:f}
    \forall p \in \NN \ \ \text{and} \ \ t\in(0,1) \quad : \quad \int_{\RR^d} |k|^p |m(t,k)|dk<\infty, \quad \int_{\RR^d} |k|^p |\partial_t m(t,k)|dk<\infty 
\end{equation} 
which implies that the inverse Fourier transform of $m$ is a function $j : [0,1]\times \RR^d \to \RR^d$ that is in $(C^p(\RR^d))^d$ for any $p\in \NN$ and can be expressed as
\begin{equation}
    \label{eq:j1}
    j(t,x) = (2\pi)^{-d} \int_{\RR^d} e^{-ik\cdot x} m(t,k) dk = \EE\left(\partial_t I(t,x_0,x_1)+ \dot \gamma (t) z |x_t = x\right) \rho(t,x) \equiv b(t,x) \rho_t(x)
\end{equation}
where the last equality follows from the definition of $b$ in~\eqref{eq:b:ode:def}. We deduce that $b\in C^0([0,1];(C^p(\RR^d))^d)$ for any $p\in \NN$ since $j\in C^0([0,1];(C^p(\RR^d))^d)$ and $\rho\in C^1([0,1];C^p(\RR^d))$, and $\rho>0$. 


Finally, let us establish~\eqref{eq:bt:bounded}. By~\eqref{eq:It:L2} we  have
\begin{equation}
    \label{eq:v:L2}
    \begin{aligned}
        \int_{\RR^d} |b(t,x)|^2 \rho(t,x) dx & = \int_{\RR^d}|\EE\left((\partial_t I(t,x_0,x_1)+\dot \gamma(t) z|x_t = x\right)|^2 \rho(t,x)dx \\
        &\le 2 \int_{\RR^d} \EE\left(|\partial_t I(t,x_0,x_1)|^2+|\dot \gamma(t)|^2 |z|^2|x_t = x\right) \rho(t,x)dx\\
        &  \le 2 \EE\big[|\partial_t I(t,x_0,x_1)|^2 +|\dot \gamma(t)|^2|z|^2\big]\\
        & < M_1^{1/2} + d |\dot \gamma(t)|^2.
    \end{aligned} 
\end{equation} 
Therefore this integral is bounded for all $t\in (0,1)$. To analyze it behavior at the end points, notice that the decomposition \eqref{eq:b:decomp} implies that 
\begin{equation}
\label{eq:b:lim:o=0:1}
\begin{aligned}
    b_0(x) &\equiv \lim_{t\to0} b(t,x) = \EE_1 [\partial_t I(0,x,x_1) ] - \lim_{t\to0} \dot \gamma(t) \gamma(t) s_0(x), \\
    b_1(x) & \equiv \lim_{t\to1} b(t,x) = \EE_0 [\partial_t I(0,x_0,x) ] - \lim_{t\to1} \dot \gamma(t) \gamma(t) s_1(x), 
\end{aligned}
\end{equation}
where $s_0= \nabla \log\rho_0$, $s_1= \nabla \log\rho_1$., $EE_0$ and $EE_1$ denotes expectation s over $x_0\sim\rho_0$ and $x_1\sim \rho_1$ respectively, and we used the property that $x_{t=0}=x_0$ and $x_{t=1} = x_1$. Since $\lim_{t\to0,1} \dot \gamma(t) \gamma(t)$ exist by our assumption that $\gamma^2\in C^1([0,1])$, it means that $b_0$ and $b_1$ are well defined, and 
\begin{equation}
    \label{eq:b0:1:int}
    \int_{\RR^d} |b_0(x)|^2 \rho_0(x) dx < \infty, \qquad \int_{\RR^d} |b_1(x)|^2 \rho_1(x) dx < \infty
\end{equation}
by Assumption~\ref{as:rho:I}.  As a result the integral in~\eqref{eq:v:L2} it is also continuous at $t=0,1$, and as result it must be integrable on $[0,1]$ and~\eqref{eq:bt:bounded} holds. 
\end{proof}


\interpolatelosses*
\begin{proof}
    By definition of $\rho$, the objective $\mathcal{L}_b$ defined in~\eqref{eq:obj:v} can also be written as 
\begin{equation}
    \label{eq:EL:v}
    \begin{aligned}
    \mathcal{L}_b[\hat b] &= \int_{\RR^d} \left( \tfrac12|\hat b(t,x)|^2 - \EE\left((\partial_t I(t,x_0,x_1)+\dot\gamma(t) z|x_t=x\right)\cdot \hat b(t,x)\right) \rho(t,x) dx\\
    &= \int_{\RR^d} \left( \tfrac12 |\hat b(t,x)|^2 - b(t,x)\cdot \hat b(t,x) \right) \rho(t,x) dx
    \end{aligned}
\end{equation}
where we used the definition of $b$ in~\eqref{eq:b:ode:def}. This quadratic objective is bounded from below since
\begin{equation}
    \label{eq:EL:v:b}
    \begin{aligned}
    \mathcal{L}_b[\hat b] &= \tfrac12 \int_{\RR^d} \left|\hat b(t,x) - b(t,x) \right|^2 \rho(t,x) dx - \tfrac12  \int_{\RR^d} \left|b(t,x) \right|^2 \rho(t,x) dx\\
    & \ge - \tfrac12  \int_{\RR^d} \left|b(t,x) \right|^2 \rho(t,x) dx >-\infty
    \end{aligned}
\end{equation}
where the last inequality follows from~\eqref{eq:bt:bounded}. Since $\rho_t$ is positive the  minimizer of~\eqref{eq:EL:v} is unique and given by $\hat b= b$. 

\end{proof}

\score*
\begin{proof}
Since $\rho \in C^1((0,1); C^p(\RR^d))$ and is positive by Theorem~\ref{prop:interpolate}, we already  know that $s = \nabla \log \rho = \nabla \rho /\rho \in C^1((0,1); (C^p(\RR^d))^d)$.
To establish~\eqref{eq:s:def}, note that, for $t\in(0,1)$ where $\gamma(t)>0)$, we have (this is Gaussian integration by part)
 \begin{equation}
 \label{eq:gbp}
 \EE\left(z  e^{i\gamma(t) k \cdot z }\right) = -\gamma^{-1}(t)(i\partial_k)  \EE e^{i\gamma(t) k \cdot z } = -\gamma^{-1}(t)(i\partial_k)  e^{-\tfrac12 \gamma^2(t) |k|^2} = \tfrac12 i \gamma(t)k e^{-\tfrac12 \gamma^2(t) |k|^2}.
 \end{equation}
 As a result, using the independence between  $x_0$, $x_1$, and $z$ , we have
\begin{equation}
 \label{eq:gbp:2}
 \EE\left(z  e^{i k \cdot x_t }\right) =\tfrac12 i \gamma(t)k g(t,k)
 \end{equation}
 where $g$ is the characteristic function of $x_t$ defined in~\eqref{eq:charact}. Using the properties of the conditional expectation, the left hande-side of this equation  can be written as
begin{equation}
\begin{equation}
 \label{eq:gbp:3}
 \EE\left(z  e^{i\gamma(t) k \cdot z }\right) = \int_{\RR^d} \EE\left(z  e^{i k \cdot x_t }|x_t=x\right) \rho(t,x) dx = \int_{\RR^d} \EE\left(z  |x_t=x\right) e^{i k x_t } \rho(t,x) dx 
 \end{equation}
 Since the left hand side of~\eqref{eq:gbp:2} is the Fourier transform of $-\gamma(t) \nabla \rho(t,x)$, we deduce that
 \begin{equation}
 \label{eq:gbp:4}
 \EE\left(z  |x_t=x\right)  \rho(t,x) = \gamma(t) \nabla \rho(t,x) = \gamma(t) s(t,x) \rho(t,x).
 \end{equation}
 Since $\rho(t,x)>0$, this implies~\eqref{eq:s:def} for $t\in(0,1)$ where $\gamma(t)>0$. 


To establish~\eqref{eq:st:bounded}, notice that
\begin{equation}
    \label{eq:s:L2}
    \begin{aligned}
        \int_{\RR^d} |s(t,x)|^2 \rho(t,x) dx & = \int_{\RR^d}|\EE\left((\gamma^{-1}(t) z|x_t = x\right)|^2 \rho(t,x)dx \\
        &\le \int_{\RR^d} \gamma^{-2}(t) \EE\left(|z|^2|x_t = x\right) \rho(t,x)dx\\
        &  \le \gamma^{-2}(t).
    \end{aligned} 
\end{equation} 
This means that this integral is bounded for all $t\in(0,1)$. Since it is also continuous at $t=0,1$, with values given by~\eqref{eq:rho0:1:sc}, it must be integrable on $[0,1]$ and~\eqref{eq:st:bounded} holds.

The objective $\mathcal{L}_s$ defined in~\eqref{eq:obj:s}  
can also be written as 
\begin{equation}
    \label{eq:EL:s}
    \begin{aligned}
     \mathcal{L}_s[\hat s] &= \int_{\RR^d} \left( \tfrac12|\hat s(t,x)|^2 + \frac{\eps}{t(1-t)}\EE\left(B_t|x_t=x\right)\cdot \hat s(t,x)\right) \rho(t,x) dx\\
    &= \int_{\RR^d} \left( \tfrac12 |\hat s(t,x)|^2 - s(t,x)\cdot \hat s(t,x) \right) \rho(t,x) dx
    \end{aligned}
\end{equation}
where we used the definition of $s$ in~\eqref{eq:s:def}. This quadratic objective is bounded from below since
\begin{equation}
    \label{eq:EL:s:b}
    \begin{aligned}
    \mathcal{L}_s[\hat s] &= \tfrac12 \int_{\RR^d} \left|\hat s(t,x) - s(t,x) \right|^2 \rho(t,x) dx - \tfrac12  \int_{\RR^d} \left|s(t,x) \right|^2 \rho(t,x) dx\\
    & \ge - \tfrac12  \int_{\RR^d} \left|s(t,x) \right|^2 \rho(t,x) dx >-\infty
    \end{aligned}
\end{equation}
where the last inequality follows from~\eqref{eq:st:bounded}. Since $\rho$ is positive the  minimizer of \eqref{eq:EL:s} is unique and given by $\hat s = s$. 
\end{proof}


\interpolationfpe*
\begin{proof}
The forward FPE~~\eqref{eq:fpe} and the backward FPE~\eqref{eq:fpe:tr} are direct consequence of the TE~\eqref{eq:transport} and~\eqref{eq:s:def}, since the equality
\begin{equation}
    \label{eq:score}
\eps\Delta \rho =  \eps\nabla \cdot (\rho \nabla \log \rho ) = \eps\nabla \cdot (s \rho ) 
\end{equation}
 can be used to convert between these equations.

\end{proof}

\subsection{Proof of Corollary~\ref{prop:generative}}
\label{app:proof:generative}

\generative*
\begin{proof}
    The SDE~\eqref{eq:sde:1} and the ODE~\eqref{eq:ode:1} are the evolution equations for the processes whose PDF solve~\eqref{eq:fpe} and~\eqref{eq:transport}. The one equation that requires some explaining in the backward SDE~\eqref{eq:sde:R}.  This backward SDE be solved backward in time from $t=1$ to $t=0$, and by definition its solution is $X^\rev_{t}=Z^\fwd_{1-t} $ where $Z^\fwd_t$ solves the forward SDE
\begin{equation}
    \label{eq:sde:generic:rev:Y}
    dZ^\fwd_t = -b_\rev(1-t,Z^\fwd_t)dt + \seps d W_t
\end{equation}
to be solved forward in time.
To see how to write the backward It\^o formula~\eqref{eq:ito:formula}  note that given any  $f\in C^1([0,1];C_0^2(\RR^d))$, we have 
\begin{equation}
    \label{eq:Ito:rev:Y}
    \begin{aligned}
    df(1-t,Z^\fwd_t) &= -\partial_t f(1-t,Z^\fwd_t)dt+\nabla f(1-t,Z^\fwd_t) \cdot dY_t + \eps \Delta f(1-t,Z^\fwd_t) dt\\
    &= \left( - b_\rev(1-t,Z^\fwd_t) \cdot \nabla f(1-t,Z^\fwd_t)  + \eps \Delta f(1-t,Z^\fwd_t)\right) dt\\
    & \quad+ \seps \nabla f(1-t,Z^\fwd_t) \cdot dW_t
     \end{aligned}
\end{equation}
In integral form, this equation can be written as
\begin{equation}
    \label{eq:int:Ito:rev:Y}
    \begin{aligned}
    f(1,Z^\fwd_{1}) &= f(1-t,Z^\fwd_{1-t}) \\
    &\quad- \int_{1-t}^1 \left( \partial_t f(1-s,Z^\fwd_s)+ b_\rev(1-s,Z^\fwd_s) \cdot \nabla f(1-s,Z^\fwd_s)  - \eps \Delta f(1-s,Z^\fwd_s)\right) ds \\
    & \quad - \seps \int_{1-t}^1 \nabla f(1-s,Z^\fwd_s) \cdot dW_s. 
     \end{aligned}
\end{equation}
Using $X^\rev_t = Z^\fwd_{1-t}$ and $W^\rev_t = - W_{1-t}$ and changing integration variable from $s$ to $1-s$, this is 
\begin{equation}
    \label{eq:int:Ito:rev:X}
    \begin{aligned}
    f(0,X^\rev_0) & = f(t,X^\rev_t) + \int_t^1 \left( - b_\rev(s,X^\rev_s) \cdot \nabla f(s,X^\rev_s)  + \eps \Delta f(s,X^\rev_s)\right) ds \\
    & \quad - \seps \int_0^t \nabla f(s,X^\rev_s) \cdot dW^\rev_s, 
     \end{aligned}
\end{equation}
In differential form, this is equivalent to saying that 
\begin{equation}
    \label{eq:Ito:rev}
    \begin{aligned}
    df(t,X^\rev_t) &= \partial_t f(t,X^\rev_t)dt + \nabla f(t,X^\rev_t) \cdot dX^\rev_t - \eps \Delta f(X^\rev_t) dt\\
    &= \left( \partial_t f(t,X^\rev_t) +b_\rev(t,X^\rev_t) \cdot \nabla f(t,X^\rev_t)  - \eps \Delta f(t,X^\rev_t)\right) dt + \seps \nabla f(t,X^\rev_t) \cdot dW_t
     \end{aligned}
\end{equation}
which is the backward It\^o formula~\eqref{eq:ito:formula}. Similarly, by the It\^o isometries we have: for any $g\in C^0([0,1];(C_0^0(\RR_d))^d)$ and $t\in[0,1]$
\begin{equation}
\label{eq:ito:iso:Y}
\EE^x \int_{1-t}^1 g(s,Z^\fwd_s) \cdot dW_s = 0, \qquad \EE^x \left|\int_{1-t}^1 g(s,Z^\fwd_s) \cdot dW_s\right|^2 =   \int_{1-t}^1 \EE^x|g (s,Z^\fwd_s) |^2ds.
\end{equation}
Written in terms of $X^\rev_t$, these are~\eqref{eq:ito:iso}.

\end{proof}


\paragraph{Derivation of \eqref{eq:ob:w:alt}.} For any $t\in[0,1]$ the score is the minimizer of
\begin{equation}
    \begin{aligned}
        &\int_{\RR^d} |\hat s(t,x) - \nabla \log \rho(t,x)|^2 \rho(t,x) dx\\
        &= \int_{\RR^d} \left(|\hat s(t,x)|^2 - 2\hat s(t,x) \cdot\nabla\log \rho(t,x)+ | \nabla \log \rho(t,x)|^2\right) \rho(t,x) dx \\
        &= \int_{\RR^d} \left(|\hat s(t,x)|^2 + 2\nabla \cdot \hat s(t,x) + | \nabla \log \rho(t,x)|^2\right) \rho(t,x) dx,
    \end{aligned}
\end{equation}
where we used the identity $\hat s \cdot \nabla \log \rho \,\rho = \hat s \cdot \nabla \rho$ and integration by parts to obtain the second equality. The last term involving $|\nabla \log \rho|^2$ is a constant in $\hat s$ that can be neglected for optimization. Expressing the remaining terms as an expectation over $x_t$ and integrating the result in time gives~\eqref{eq:ob:w:alt}.



\subsection{Proofs of Lemmas~\ref{lemma:kl_transport} and~\ref{lemma:kl_fpe}, and Theorem~\ref{thm:kl:bound}.}
\label{app:proof:kl}
\kltransport*
\begin{proof}
Using~\eqref{eq:2:te}, we compute analytically
\begin{align*}
    \frac{d}{dt}\KL{\rho(t)}{\hat\rho(t)} &= \frac{d}{dt}\int_{\RR^d} \log\left(\frac{\rho}{\hat \rho}\right)\rho dx\\
    %
    %
    &= \int_{\RR^d} \hat\rho\left(\frac{\partial_t \rho}{\hat\rho} - \frac{\rho}{(\hat\rho)^2}\partial_t\hat\rho\right)dx + \int\log\left(\frac{\rho}{\hat\rho}\right)\partial_t\rho dx\\
    %
    %
    &= -\int_{\RR^d} \left(\frac{\rho}{\hat\rho}\right)\partial_t\hat\rho dx + \int_{\RR^d}\log\left(\frac{\rho}{\hat\rho}\right)\partial_t\rho dx\\
    %
    %
    &= \int_{\RR^d} \left(\frac{\rho}{\hat\rho}\right)\nabla\cdot\left(\hat b\hat\rho\right)dx - \int\log\left(\frac{\rho}{\hat\rho}\right)\nabla\cdot\left(b\rho\right)dx\\
    %
    %
    &= -\int_{\RR^d} \nabla\left(\frac{\rho}{\hat\rho}\right)\cdot \hat b\hat\rho dx + \int\left(\nabla\log\rho - \nabla\log\hat\rho\right)\cdot b \rho dx\\
    %
    %
    &= -\int_{\RR^d} \left(\frac{\nabla\rho}{\hat\rho} - \frac{\rho\nabla\hat\rho}{\hat\rho^2}\right)\cdot \hat b\hat\rho dx 
    + \int_{\RR^d}\left(\nabla\log\rho - \nabla\log\hat\rho\right)\cdot b \rho dx\\
    %
    %
    &= \int_{\RR^d} \left(\nabla\log\hat\rho - \nabla\log\rho\right)\cdot \hat b \rho dx + \int_{\RR^d}\left(\nabla\log\rho - \nabla\log\hat\rho\right)\cdot b \rho dx\\
    %
    %
    &= \int_{\RR^d} \left(\nabla\log\hat\rho - \nabla\log\rho\right)\cdot (\hat b - b) \rho dx.
\end{align*}
where we omitted the argument $(t,x)$ of all functions for simplicity of notation. Integrating both sides from $0$ to $1$ completes the proof.
\end{proof}

\klfpe*
\begin{proof}
Similar to the proof of Lemma~\ref{lemma:kl_transport}, we can use the FPE in~\eqref{eq:2:fpe} to compute $\frac{d}{dt}\KL{\rho(t)}{\hat\rho(t)}$, which leads to a proof of the main result. Instead, we take a simpler approach, leveraging the result in Lemma~\ref{lemma:kl_transport}. We re-write the Fokker-Planck equations in~\eqref{eq:2:fpe} as the (score-dependent) transport equations
\begin{equation}
    \label{eqn:sde_transports}
    \begin{aligned}
        \partial_t \rho &= -\nabla\cdot((b_\fwd - \eps\nabla\log\rho)\rho),\\ 
        %
        \partial_t \hat\rho &= -\nabla\cdot((\hat b_\fwd - \eps\nabla\log\hat\rho)\hat\rho).
    \end{aligned}
\end{equation}
Applying Lemma~\ref{lemma:kl_transport} directly, we find that
\begin{equation}
\begin{aligned}
    \KL{\rho(1)}{\hat \rho(1)} = \int_0^1 \int_{\RR^d} \left[\left(\nabla\log\hat\rho  - \nabla\log\rho \right)\cdot \left(\left[\hat b_\fwd - \eps\nabla\log\hat\rho \right] - \left[b_\fwd - \eps\nabla\log\rho \right]\right)\right] \rho dx dt.
\end{aligned}
\end{equation}
Expanding the above,
\begin{equation}
\begin{aligned}
    \KL{\rho(1) }{\hat\rho(1) } &= \int_0^1 \int_{\RR^d}\left[\left(\nabla\log\hat\rho  - \nabla\log\rho \right)\cdot (\hat b_\fwd - b_\fwd)\right]\rho dx dt\\
    &\qquad  - \eps\int_0^1 \int_{\RR^d} \left[\norm{\nabla\log\rho  - \nabla\log\hat\rho }^2\right]\rho dx dt,
\end{aligned}
\end{equation}
which proves the lemma.
\end{proof}

\likelihoodbound*
\begin{proof}
Observe that by Proposition~\ref{prop:generative}, the target density $\rho_1 =\rho(1, \cdot)$ is the density of the process $X_t$ that evolves according to SDE~\eqref{eq:sde:1}. By Lemma~\ref{lemma:kl_fpe}, we then have that 
\begin{equation}
\begin{aligned}
    \KL{\rho_1}{\hat\rho(1)} &= \int_0^1 \int_{\RR^d} \left(\nabla\log\hat{\rho} - \nabla\log\rho\right)\cdot \left([\hat{b} + \epsilon \hat{s}] - [b+\epsilon  s]\right)\rho dx dt\\
    &\qquad  - \eps\int_0^1 \int_{\RR^d} \norm{\nabla\log\rho - \nabla\log\hat{\rho}}^2\rho_tdxdt.
\end{aligned}
\end{equation}
By Young's inequality, it holds for any fixed $\eta > 0$ that
\begin{align}
    \KL{\rho_1}{\hat\rho(1)} &\leq \int_0^1 \int_{\RR^d}\left(\frac{\eta}{2}\norm{\nabla\log\hat{\rho} - \nabla\log\rho}^2 + \frac{1}{2\eta}\norm{\hat{b} - b + \epsilon (s - \hat{s})}^2\right)\rho dx dt\nonumber\\
    &\qquad  - \eps\int_0^1 \int_{\RR^d} \norm{\nabla\log\rho - \nabla\log\hat{\rho}}^2\rho dx dt,\\
    %
    %
    &= \int_0^1 \frac{1}{2\eta}\int_{\RR^d}\norm{\hat{b} - b +  \epsilon  (s - \hat{s})}^2\rho_tdx dt\nonumber\\
    &\qquad  + \left(\tfrac{1}{2}\eta - \eps\right)\int_0^1 \int_{\RR^d} \norm{\nabla\log\rho_t - \nabla\log\hat{\rho}_t}^2\rho_tdxdt.
\end{align}
Hence, for $\eta =2\eps$, 
\begin{align}
    \label{eqn:kl_step}
    \KL{\rho_1}{\hat\rho(1)} &\leq \frac{1}{4\eps} \int_0^1 \int_{\RR^d}\norm{\hat{b} - b +  \epsilon  (s - \hat{s})}^2\rho dxdt.
\end{align}
Again by Young's inequality, it holds that
\begin{align}
    \KL{\rho_1}{\hat\rho(1)} &\leq \frac{1}{2\eps}\int_0^1\int_{\RR^d}\left(|\hat{b} - b|^2 +\epsilon^2 \norm{s - \hat{s}}^2\right)\rho_tdxdt.
\end{align}
Using the definition of $\mathcal{L}_b[\hat{b}]$ and $\mathcal{L}_s[\hat{s}] $ in~\eqref{eq:obj:v}, and~\eqref{eq:obj:s}, we conclude that
\begin{equation}
    \KL{\rho_1}{\hat{\rho}(1)} \leq \frac{1}{2\eps}\left(\mathcal{L}_b[\hat{b}] - \min_{\hat{b}}\mathcal{L}_b[\hat{b}]\right)  + \frac{\eps}{2}\left(\mathcal{L}_s[\hat{s}] - \min_{\hat{s}}\mathcal{L}_s[\hat{s}]\right),
\end{equation}
which is~\eqref{eq:bound:kl}. Using that $b = v - \gamma\dot{\gamma} s$ and $\hat{b} = \hat{v} - \gamma\dot{\gamma}\hat{s}$, we can write~\eqref{eqn:kl_step} as 
\begin{align}
    \KL{\rho_1}{\hat\rho(1)} &\leq \frac{1}{4\eps} \int_0^1 \int_{\RR^d}\norm{\hat{v} - v - (\gamma(t)\dot\gamma(t) + \epsilon)(\hat{s} - s)}^2\rho dxdt,\\
    %
    %
    &\leq \frac{1}{2\eps} \int_0^1 \int_{\RR^d}\left(\norm{\hat{v} - v}^2 + (\gamma(t)\dot\gamma(t) + \epsilon)^2\norm{\hat{s} - s}^2\right)\rho dxdt,\\
    %
    %
    &\leq \frac{1}{2\eps}\left(\mathcal{L}_{v}[\hat{v}] - \min_{\hat{v}}\mathcal{L}_{v}[\hat{v}]\right) + \frac{\max_{t\in [0 ,1]}(\gamma(t)\dot\gamma(t) + \epsilon)^2}{2\eps}\left(\mathcal{L}_{s}[\hat{s}] - \min_{\hat{v}}\mathcal{L}_{s}[\hat{s}]\right).
\end{align}

\end{proof}


\subsection{Proofs of Lemma~\ref{lem:tesol} and Theorem~\ref{prop:sde:rho}}
\label{app:prrof:de}

\TEs*
\begin{proof}
    If $\hat \rho$ solves the TE~\eqref{eq:TE:hat} and $ X_{s,t}$ solves the ODE~\eqref{eq:ode:st}, we have
\begin{equation}
    \label{eq:te:sol:1}
    \begin{aligned}
        \frac{d}{dt} \hat\rho(t,X_{s,t}(x)) & = \partial_t  \hat\rho(t, X_{s,t}(x))  + b_\ODE(t,X_{s,t}(x))  \cdot \nabla  \hat\rho(t,X_{s,t}(x))\\
        &= - \nabla \cdot b_\ODE(t, X_{s,t}(x)) \hat\rho(t,X_{s,t}(x))
    \end{aligned}
\end{equation}
This equation implies that
\begin{equation}
    \label{eq:te:sol:2}
    \begin{aligned}
        \frac{d}{dt} \left( \exp\left( \int_{s}^t\nabla \cdot b_\ODE(\tau,X_{s,\tau}(x)) d\tau \right) \hat\rho(t,X_{s,t}(x)) \right) = 0.
    \end{aligned}
\end{equation}
Integrating~\eqref{eq:te:sol:2} on $[0,t]$ and setting $s=t $ in the result gives
\begin{equation}
    \label{eq:te:sol:3:f}
    \begin{aligned}
          \hat\rho(t,x)  = \exp\left( -\int_0^{t}\nabla \cdot b_\ODE(\tau,X_{t,\tau}(x)) d\tau \right) \hat\rho(0,X_{t,0}(x))
    \end{aligned}
\end{equation}
If we use the initial condition $\hat \rho(0) = \rho_0$, this gives~\eqref{eq:TE:hat:s:f}.
Similarly, Integrating ~\eqref{eq:te:sol:2}  on $[t,1]$ and setting $s=t $ in the result gives
\begin{equation}
    \label{eq:te:sol:3:r}
    \begin{aligned}
         \hat\rho(t,x)  = \exp\left( \int_{t}^1\nabla \cdot b_\ODE(\tau,X_{t,\tau}(x)) d\tau \right) \hat\rho(1,X_{t,1}(x))
    \end{aligned}
\end{equation}
If we use the final condition $\hat \rho(1) = \rho_1$, this gives~\eqref{eq:TE:hat:s:b}.

\end{proof}
 

\FK*
\begin{proof}
% \paragraph{Equation~\eqref{eq:fk}.}  
Evaluating $d\hat \rho_\fwd(t, Y^\rev_t)$ via the backward It\^o formula~\eqref{eq:ito:formula} we obtain
\begin{equation}
    \label{eq:ito:rho:1}
    \begin{aligned}
    d\hat\rho_\fwd(t, Y_t^\rev) &= \partial_t \hat \rho_\fwd(t, Y_t^\rev) dt + \nabla \hat \rho_\fwd(t, Y_t^\rev) \cdot dY^\rev_t - \eps \Delta \hat \rho_\fwd(t,Y_t^\rev) dt\\
    &= \partial_t \hat \rho_\fwd(t, Y_t^\rev) dt + \nabla \hat \rho_\fwd(t, Y_t^\rev) \cdot \hat b_\fwd(t, Y^\rev_t)dt + \seps \nabla \hat \rho_\fwd(t, Y_t^\rev) \cdot dW^\rev_t - \eps \Delta \hat \rho_\fwd(t, Y_t^\rev) dt\\
    & = -\nabla \cdot \hat b_\fwd(t, Y^\rev_t)  \hat \rho_\fwd(t, Y^\rev_t) dt + \seps \nabla \hat\rho_\fwd(t, Y_t^\rev) \cdot dW^\rev_t.
    \end{aligned}
\end{equation}
where we used~\eqref{eq:sde:y:R} in the second step and~\eqref{eq:fpe:f:hat} in the last one. This equation can be written as a total differential in the form
\begin{equation}
    \label{eq:ito:rho:2}
    \begin{aligned}
    & d \left(\exp\left(- \int_t^1\nabla \cdot \hat b_\fwd(\tau, Y^\rev_\tau) d\tau\right)  \hat \rho_\fwd(t, Y_t^\rev) \right) \\
    & =  \seps \exp\left(-\int_t^1\nabla \cdot \hat b_\fwd(\tau, Y^\rev_\tau) d\tau\right) \nabla \hat \rho_\fwd(t, Y_t^\rev) \cdot dW^\rev_t,
    \end{aligned}
\end{equation}
which after integration on $t\in[0,1]$ becomes
\begin{equation}
    \label{eq:ito:rho:3}
    \begin{aligned}
    & \hat \rho_\fwd(1, Y_{t=1}^\rev) - \exp\left(- \int_0^1\nabla \cdot \hat b_\fwd(t, Y^\rev_t)  dt\right)  \rho_0(Y^\rev_{t=0}) \\
    %
    %
    & =  \seps \int_0^1 \exp\left(-\int_t^1\nabla \cdot \hat b_\fwd(\tau, Y^\rev_\tau) d\tau\right) \nabla \hat \rho(t, Y_t^\rev) \cdot dW^\rev_t.
    \end{aligned}
\end{equation}
where we used $\hat \rho(0)=\rho_0$.
Taking an expectation conditioned on the event $Y_{t=1}^\rev=x$ and using that the term on the right-hand side has mean zero, we find that
\begin{equation}
    \label{eq:ito:rho:4}
    \hat \rho_\fwd(1, Y_{t=1}^\rev) - \EE^x_\rev\exp\left(- \int_0^1\nabla \cdot \hat b_\fwd(t, Y^\rev_t)  dt\right)  \rho_0(Y^\rev_{t=0}) = 0.
\end{equation}
This gives~\eqref{eq:fk}.


Similarly, evaluating $d\hat \rho(t, Y^\fwd_t)$ via the It\^o formula, we obtain
\begin{equation}
    \label{eq:ito:rho:r:1}
    \begin{aligned}
    d \hat \rho_\rev(t, Y^\fwd_t) &= \partial_t \hat \rho_\rev(t, Y^\fwd_t) dt + \nabla \hat \rho_\rev(t, Y^\fwd_t) \cdot dY^\fwd_t + \epsilon \Delta \hat \rho_\rev(t, Y^\fwd_t) dt\\
    %
    %
    &= \partial_t \hat \rho_\rev(t, Y^\fwd_t) dt + \nabla \hat \rho_\rev(t, Y^\fwd_t) \cdot \hat b_\rev(t, Y^\fwd_t)dt + \seps \nabla \hat \rho_\rev(t, Y^\fwd_t) \cdot dW_t + \eps \Delta \hat \rho_\rev(t, Y^\fwd_t) dt\\
    %
    %
    & = -\nabla \cdot \hat b_\rev(t, Y^\fwd_t)   \hat \rho_\rev(t,Y^\fwd_t) dt + \seps \nabla \hat \rho_\rev(t,Y^\fwd_t) \cdot dW_t.
    \end{aligned}
\end{equation}
where we used~\eqref{eq:sde:y:1} in the second step and~\eqref{eq:fpe:r:hat} in the last one.  This equation can be written as a total differential in the form
\begin{equation}
    \label{eq:ito:rho:r:2}
    \begin{aligned}
    & d\left(\exp\left( \int_0^t\nabla \cdot \hat b_\rev(\tau, Y^\fwd_\tau)  d\tau\right)  \hat \rho_\rev(t,Y^\fwd_t) \right) \\
    %
    %
    & =  \seps \exp\left( \int_0^t\nabla \cdot \hat b_\rev(\tau, Y^\fwd_\tau)  d\tau\right) \nabla \hat \rho_\rev(t, Y^\fwd_t) \cdot dW_t.
    \end{aligned}
\end{equation}
Integrating the above on $t\in[0,1]$, we find that
\begin{equation}
    \label{eq:ito:rho:r:3}
    \begin{aligned}
    & \exp\left( \int_0^1\nabla \cdot \hat b_\rev(t, Y^\fwd_t)  dt\right)\rho_1( Y^\fwd_1) -   \hat \rho_\rev(0, Y^\fwd_{t=0})\\
    %
    %
    &= \seps \int_0^1 \exp\left( \int_0^t\nabla \cdot \hat b_\rev(\tau, Y^\fwd_\tau)  d\tau\right) \nabla \hat \rho_\rev(t,Y^\fwd_t) \cdot dW_t.
    \end{aligned}
\end{equation}
where we used $\hat \rho(1) = \rho_1$.
Taking an expectation conditioned on the event $Y^\fwd_{t=0} = x$ and applying the It\^o isometry, we deduce that
\begin{equation}
    \label{eq:ito:rho:r:4}
    \EE_\fwd^x \left(\exp\left( \int_0^1\nabla \cdot \hat b_\rev(t, Y^\fwd_t) dt\right)\rho_1(Y^\fwd_{t=1})\right) - \hat \rho_\rev(0, y) =0.
\end{equation}
This gives~\eqref{eq:fk:rev}.

\end{proof}

\subsection{Proof of Theorem~\ref{thm:sdeos:ou}}
\label{app:sdeos:ou}

\sdeos*
\begin{proof}
    For the interpolant $y_t$ defined in~\eqref{eq:os:lin:x0}, the velocity $b$ in~\eqref{eq:b:ode:def:os} reduces to
\begin{equation}
    \label{eq:b:ode:def:os:lin}
    b_\ODE(t,x) = \EE ( \dot \alpha(t) x_0 + \dot \beta(t) z| y_t = x).
\end{equation}
Since $y_t = \alpha(t) x_0 + \beta(t) z$, this can be written as
\begin{equation}
    \label{eq:b:ode:def:os:lin:2}
    \begin{aligned}
        b_\ODE(t,x) &= \EE ( \alpha^{-1}(t)\dot \alpha(t) y_t + (\dot \beta(t) - \beta(t)\alpha^{-1}(t)\dot\alpha(t)) z| y_t = x)\\
    & = \alpha^{-1}(t)\dot \alpha(t) x + (\dot \beta(t) - \beta(t)\alpha^{-1}(t)\dot\alpha(t))  \EE (z| y_t = x)\\
    & = \alpha^{-1}(t)\dot \alpha(t) x - D(t) s(t,x)
    \end{aligned}
\end{equation}
where we used $\EE (z| y_t = x) = - \beta(t) s(t,x) $ from \eqref{eq:obj:v:os} as well as the definition of~$D(t)$ in~\eqref{eq:sig:def}. This means that the PDF of $y_t$ satisfies
\begin{equation}
    \label{eq:pdf:lin:os}
    0= \partial_t \rho + \nabla \cdot ( [\alpha^{-1}(t)\dot \alpha(t) x - D(t) s(t,x)]\rho(t,x)) = \partial_t \rho + \nabla \cdot ( \alpha^{-1}(t)\dot \alpha(t) x\rho(t,x)) - D(t) \Delta \rho.
\end{equation}
where we used $s \rho = \nabla \log \rho \rho = \nabla \rho$.
The forward SDE associated with this forward FPE is~\eqref{eq:sde:os:lin}. 

\end{proof}

\subsection{Proof of Lemma~\ref{lem:interp} and Theorem~\ref{prop:sb}.}
\label{app:sb}

\interppdf*
\begin{proof}
By the definition of the map~$T$ in~\eqref{eq:interpol}, if $x_0\sim \rho_0$ and $x_1\sim\rho_1$, then $T^{-1}(0,x_0)\sim {\sf N}(0,\Id)$ and $T^{-1}(1,x_1)\sim {\sf N}(0,\Id)$. As a result, since $x_0$, $x_1$, and $z$ are independent, and $z\sim {\sf N}(0,\Id)$,  we have
\begin{equation}
    \label{eq:gauss:sum}
    \alpha(t) T^{-1}(0,x_0) + \beta(t) T^{-1}(1,x_1) +\gamma(t) z \sim {\sf N}(0,\alpha^2(t)\Id+\beta^2(t)\Id+\gamma^2(t)\Id) = {\sf N}(0,\Id)
\end{equation}
where the second equality follows from the condition $\alpha^2(t)+\beta^2(t)+\gamma^2(t)=1$. Therfore, using again the definition of the map~$T$
\begin{equation}
    \label{eq:rho:map}
    T(t,\alpha(t) T^{-1}(0,x_0) + \beta(t) T^{-1}(1,x_1) +\gamma(t) z) \sim \rho(t),
\end{equation}
and we are done.
\end{proof}

\sb*
\begin{proof}
Denote by $\hat\rho(t,x) $ be the PDF of $\hat x_t$ and define the current $\hat\jmath : [0,1]\times \RR^d \to \RR^d$ as
\begin{equation}
    \label{eq:def:rho:j}
        \hat \jmath(t,x) = \EE \big(\partial_t \hat I_t +\gamma(t) z| x=\hat x_t \big) \hat \rho(t,x)
\end{equation}
In terms of this density and this current, the max-min problem~\eqref{eq:max:min} can be formulated as the constrained optimization problem:
\begin{equation}
    \label{eq:max:min:rho:j}
    \begin{aligned}
        \max_{\hat \rho,\hat \jmath} \min_{\hat u} &\int_0^1 \int_{\RR^d}\left( \tfrac12|\hat u(t,x)|^2 \hat \rho(t,x)-  \hat u(t,x)\cdot \hat \jmath(t,x) \right) dx dt\\
        \text{subject to:} \quad & \partial_t \hat \rho + \nabla \cdot \hat \jmath= \eps  \Delta \hat\rho, \quad \hat\rho(t=0) = \rho_0, \quad \hat \rho(t=1)= \rho_1
    \end{aligned}
\end{equation}
To solve this problem we can use the extended objective
\begin{equation}
    \label{eq:max:min:rho:j:2}
    \begin{aligned}
        \max_{\hat \rho,\hat \jmath} \min_{\hat u} &\Bigg(\int_0^1 \int_{\RR^d}\left( \tfrac12|\hat u(t,x)|^2 \hat \rho(t,x)-  \hat u(t,x)\cdot \hat \jmath(t,x) \right) dx dt\\
        & -\int_0^1 \int_{\RR^d}\lambda(t,x) \left(\partial_t \hat \rho(t,x) + \nabla \cdot \hat \jmath(t,x) - \eps  \Delta \hat\rho(t,x)\right) dx dt \\
        & +\int_{\RR^d} \eta_0(x)\left(\hat\rho(0,x)-\rho_0(x)\right) dx-\int_{\RR^d} \eta_1(x)\left(\hat\rho(1,x)-\rho_1(x)\right) dx \Bigg)
    \end{aligned}
\end{equation}
where $\lambda(t,x)$, $\eta_0(x)$, and $\eta_1(x)$ are Lagrange multipliers used to enforce the constraints. The unique minimizer $(\rho,j,\lambda)$ of this optimization problem solves the Euler-Lagrange equations:
\begin{equation}
    \label{eq:max:min:rho:j:el}
    \begin{aligned}
        & \partial_t \rho + \nabla \cdot j = \eps  \Delta \rho, \quad \rho(t=0) = \rho_0\quad \rho(t=1) = \rho_1\\
        & \partial_t \lambda + \tfrac12 |u|^2 = -\eps \Delta \lambda,\\
        & j = u \rho \\
        & u = \nabla \lambda
    \end{aligned}
\end{equation}
We can use the last two equations to write the first two as \eqref{eq:max:min:rho:j:el:2}, with $u=\nabla \lambda$. Since under Assumption~\ref{as:sb:2} that there is an interpolant that realizes the PDF $\rho(t)$ solution to~\eqref{eq:max:min:rho:j:el:2}, we conclude that a optimizer $(I,u)$ of the the max-min problem~\eqref{eq:max:min} exists, and for any optimizer, $I$ will be such $\rho(t)$ is the PDF of $x_t= I(t,x_0,x_1)+\gamma(t) z$ and $u=\nabla \lambda$.
\end{proof}

\section{Experimental Specifications}
\label{app:exp}

Details for the experiments regarding learning to sample under the checkerboard density are provided here. Feed forward neural networks of depth $4$ and width $512$ are used for each model of the velocity $b$. Training was done for $7000$ iterations on batches comprised of $25$ draws from the base, $400$ draws from the target, and $100$ time slices. At each iteration, a variance reduction technique based on antithetic sampling, as in \cite{song2021mle}. The objectives given in \eqref{eq:obj:v} and \eqref{eq:obj:s} were optimized using the Adam optimizer. The learning rate was set to $.002$ and was dropped by a factor of $2$ every $1500$ iterations of training. To integrate the ODE/SDE when drawing samples, we used the Heun-based integrator as suggested in \cite{Karras2022edm}.

