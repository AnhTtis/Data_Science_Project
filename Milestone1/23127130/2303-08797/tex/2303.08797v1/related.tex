\label{sec:related}

\paragraph{Deterministic Transport and Normalizing Flows.}

Transport-based sampling and density estimation has its contemporary roots in Gaussianizing data via maximum entropy methods \cite{Friedman1987,Chen2000, tabak2010, tabak2013}. The change of measure under such transformation is the backbone of normalizing flow models. The first neural network realizations of these methods arose through imposing clever structure on the transformation to make the change of measure tractable in discrete, sequential steps \cite{rezende2015, dinh2017density, papamakarios2017, huang2018, durkan2019}. A continuous time version of this procedure was made possible by viewing the map $T= X_t(x)$ as the solution of an ODE  \cite{chen2018, grathwohl2018scalable}, whose parametric drift defining the transport is learned via maximum likelihood. Training this way is intractable at scale, as it requires simulating the ODE. As such, various methods introduced regularization on the path taken between the two densities to make the ODE solves more efficient \cite{finlay2020,wu2021,tong_trajectorynet_2020}, though the inherent obstacle still remains. We also work in the continuous time setting; however, our approach allows for the learning the drift without needing to simulate the dynamics, and can additionally handle diffusive processes.

\paragraph{Stochastic Transport and Score-Based Diffusions (SBDMs).}

Recent works complementary to the deterministic map approach have realized that connecting a data distribution to a Gaussian density can be viewed as the evolution of an Ornstein-Ulhenbeck (OU) process which gradually degrades samples from the distribution of interest to Gaussian noise \cite{dickstein2015, ho2020, Song2019, song2021scorebased}. The OU process endows a specific path in the space of probability density functions, and its reverse process likewise creates a path that is dependent on the score of the time-dependent density, $\nabla \log \rho(t)$. The fixed path and ability to sample $\rho(t)$ means that optimization to learn the backward drift amounts to a least-square regression problem under the score-matching framework \cite{hyvarinen05a}. Once the score is learned, sampling from the target amounts to evolving the reverse diffusions according to its bacward SDE. Moreover, the dynamics of these stochastic processes also emit a probability flow equation at the level of the distribution, first noted \cite{Bakry1985, OTTO2000361, Kim2010AGO} and then applied in \cite{maoutsa2020,song2021mle, kingma2021on, boffi2022}. This probability flow is used for density estimation and cross-entropy calculations, even though the quantities calculated this are not readily connected to those associated withb the SDE.  The SDE framework, as it has been originally presented, introduces a number of complexities which are not \textit{a~priori} well motivated, including the dependence on mapping to a normal density and the complicated tuning of the time parameterization and noise scheduling \cite{xiao2022tackling, hoogeboom2023simple} as well as the underlying stochastic dynamics \cite{dockhorn2022score, Karras2022edm}. While related efforts have also tried to remove the dependency on the OU backbone of SBDMs \cite{peluchetti2022nondenoising}, these procedures can be overly complicated, relying on inexact mixtures of diffusions with limited expressivity and no accessible probability flow formulation. Our method avoids these complexities by observing that the  key idea behind SBDMs, namely the bridging of densities via a time-dependent PDF whose evolution equations is avialble, can be straightforwardly generalized to a much wider class of processes. This allows us to simplify the picture, working on a finite interval exactly between arbitrary densities. 


\paragraph{Stochastic Interpolants, Rectified Flows, and Flow matching.}
Variants of the stochastic interpolant method presented in \cite{albergo2023building} were also presented in \cite{liu2022, lipman2022}. In \cite{liu2022}, a linear interpolant was proposed with a focus on straight paths. This was employed as a step toward rectifying the transport paths \cite{liu2022-ot} through a procedure which improves sampling efficiency, but introduces a bias. Here, in Section~\ref{sec:rect}, we present an alternative form of the rectification that is bias-free. In \cite{lipman2022}, the interpolant picture was assembled from the perspective of conditional probability paths connecting to a Gaussian, where a noise convolution was used to improve the learning, at the cost of biasing the method. In the method proposed here, we introduce an unbiased means for incorporating noise into the process, both via the introduction of a latent variable into the stochastic interpolant and the inclusion of a tunable diffusion coefficient in the asociated stochastic generative models. We also provide  theoretical and practical motivation for the presence of these noises.


\paragraph{Optimal Transport and Schr√∂dinger Bridges.}

There is both theoretical and practical interest in minimizing the transport cost of connecting $\rho_0$ and $\rho_1$, which, in the case of deterministic maps, is characterized by the optimal transport problem and, in the case of diffusive maps,  by the Schr\"odinger Bridge problem \cite{villani2009optimal, chen2021}. Optimal transport perspectives in flow-based methods have primarily been employed as a regularization penalty \cite{zhang2018, wu2021, finlay2020, tong_trajectorynet_2020} or via imposing structure on the parameterization itself \cite{huang2021convex, Yang2022}. A variety of recent works have formulated the Schr\"odinger problem in the context of a learnable diffusion \cite{bortoli2021diffusion, su2023dual}.  In the interpolant framework, \cite{albergo2023building, liu2022, lipman2022} all propose optimal transport extensions to the learning procedure. The method of \cite{liu2022, liu2022-ot} allows one to sequentially lower the transport cost through rectification, at the cost of introducing a bias unless the velocity field is perfectly learned. The method of \cite{albergo2023building} proposes an unbiased framework at the cost of solving an additional optimization problem of the interpolant function.  The statement of optimal transport in \cite{lipman2022} only applies to Gaussians, but is shown to be practically useful in experimental demonstrations. 

In the method proposed below, we provide two approaches for optimizing the transport under stochastic dynamics. Our primary approach, based on that of \cite{albergo2023building}, is presented in Section~\ref{sec:si:schb}. It offers an alternative route to solving Benamou-Bernier's hydrodynamic formulation of the Schr\"odinger bridge problem via maximization of the loss over the interpolant. We stress however that this additional optimization step is not necessary in practice, as our approach leads to bias-free generative models no matter what, albeit not necessarily optimal ones in the OT or Schr\"odinger bridge sense. Additionally, in Section~\ref{sec:rect} we present a unbiased variant of the recitifcation scheme proposed in \cite{liu2022}.

\paragraph{Convergence bounds.}
Inspired by the successes of score-based diffusion, significant recent research effort has been expended to understand the control that can be obtained on suitable distances between the distribution of the generative model and the target data distribution, such as $\mathsf{KL}$, $W_2$, or $\mathsf{TV}$. 
%
Perhaps the first line of work in this direction is~\cite{song2021mle}, which showed that standard score-based diffusion training techniques bound the likelihood of the resulting SDE model. 
%
Importantly, as we show here, the likelihood of the corresponding probability flow is not bounded in general by this technique, as first highlighted in the context of SBDM by~\cite{lu2022higherorder}.
%
Control for SBDM-based techniques was later quantified more rigorously under the assumption of functional inequalities in a discretized setting by~\cite{holden_score1}, which were removed by~\cite{holden_score2} and~\cite{sinho_score1} via Girsanov-based techniques.
%
Most relevant to the PDE-based methods considered here is~\cite{holden_score3}, who apply similar techniques in the SBDM context to obtain sharp guarantees with minimal assumptions.