
In this section, we discuss how to design the stochastic interpolant $x_t$ defined in~\eqref{eq:stochinterp}, focusing in particular on how the choice of  the functions $I(t,x_0,x_1)$ and $\gamma(t)$ impacts the time-dependent density $\rho(t)$ that bridges $\rho_0$ and $\rho_1$. The discussion highlights how the presence of the latent variable $\gamma(t)z$ can simplify the structure of the intermediate density $\rho(t)$, and how it leads to greater design flexibility. Since our ultimate aim is to investigate the properties of practical generative models built upon ODEs or SDEs, we will also study the effect of the parameter $\eps$ that controls the amplitude of the noise in a generative SDE. Finally, the results in this section will allow us to make connections with score-based diffusion models, as well as the rectification procedure introduced in~\cite{liu2022}. Throughout, to build intuition, we choose $\rho_0$ and $\rho_1$ to be Gaussian mixture densities, for which the drift coefficients can be computed analytically (see Appendix~\ref{app:Gauss:mixt}). This enables us to visualize the effect of each choice on the resulting generative models.

\subsection{Spatially linear interpolants}
\label{sec:reg:noise}
\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figs/gmm_rhot.pdf}   
    \caption{\textbf{The effect of $\gamma(t)$ on $\rho(t)$.} A visualization of how the choice of $\gamma(t)$ changes the density $\rho(t)$ of $x_t=\alpha(t) x_0 + \beta(t) x_1+ \gamma(t)z $ when $\rho_0$ and $\rho_1$ are Gaussian mixture densities with two modes and three modes, respectively.
    %
    The first row depicts $\gamma(t)=0$, which reduces to the stochastic interpolant developed in~\cite{albergo2023building}. This case forms a valid transport between $\rho_0$ and $\rho_1$, but produces spurious intermediate modes in $\rho(t)$.
    %
    The second row depicts the choice of $\gamma(t) = \sqrt{2t(1-t)}$. In this case, the spurious modes are partially damped by the addition of the latent variable, leading to a simpler $\rho(t)$.
    %
    The final row shows the Gaussian encoding-decoding, which smoothly encodes $\rho_0$ into a standard normal distribution on the time interval $[0, 1/2)$, which is then decoded into $\rho_1$ on the interval $(1/2, 1]$. In this case, no intermediate modes form in $\rho(t)$: the two modes in $\rho_0$ collide to form $\mathsf{N}(0, 1)$ at $t=\tfrac12$, which then spreads into the three modes of $\rho_1$. 
    % 
    A visualization of individual sample trajectories from deterministic and stochastic generative models based on ODEs and SDEs whose solutions have density $\rho(t)$ can be seen in Figure~\ref{fig:gmm_trajs}.
    }
    \label{fig:gmm_rhot}
\end{figure}

If we make the dependence of $I(t, x_0, x_1)$ on~$(x_0,x_1)$ complex, it becomes difficult to specify \textit{a~priori}, and may be preferable to optimize as discussed in Section~\ref{sec:si:schb}. As a simpler starting point, it is natural to consider a class of functions $I$ that is linear in both $x_0$ and $x_1$, i.e.
\begin{equation}
    \label{eq:lin:interp}
    x_t = \alpha(t) x_0+ \beta(t) x_1 + \gamma(t) z,
\end{equation} 
where $\alpha, \beta, \gamma^2\in C^2([0,1])$, $\gamma\in C^1((0,1))$, and satisfy
\begin{equation}
    \label{eq:lin:interp:a:b}
    \begin{aligned}
         & \alpha(0)=\beta(1)=1; \quad \alpha(1)=\beta(0)=\gamma(0) = \gamma(1) = 0; \\
         & \forall t\in[0,1] \ : \ \alpha(t)\ge 0, \ \dot \alpha(t)\le 0,\ \beta(t)\ge0, \ \dot \beta(t)\ge 0,\ \gamma(t) \ge 0.
    \end{aligned}
\end{equation}
As we now show, despite its simplicity, this setup offers significant design flexibility. 

It is useful to assume that both $\rho_0$ and $\rho_1$ have been scaled to have zero mean and identity covariance (which can be achieved in practice, for example, 
by an affine transformation of the data). In this case, the time-dependent mean and covariance of~\eqref{eq:lin:interp} are given by
\begin{equation}
\label{eq:mean:var}
\EE x_t = 0, \qquad \EE [ x_t x_t^\T] = (\alpha^2(t)+\beta^2(t)+\gamma^2(t)) \Id.
\end{equation}
Preserving the identity covariance at all times therefore leads to the constraint
\begin{equation}
\label{eq:sumto1}
\forall t \in [0,1] \quad : \quad \alpha^2(t)+\beta^2(t)+\gamma^2(t) = 1.
\end{equation}
This choice is also sensible if $\rho_0$ and $\rho_1$ have covariances that are not the identity but are on a similar scale. In this case we no longer need to enforce~\eqref{eq:sumto1} exactly, and could, for example, take three functions whose sum of squares is of order one. For definiteness, in the sequel we discuss possible choices that satisfy~\eqref{eq:sumto1} exactly, with the understanding that the corresponding functions $\alpha$, $\beta$, and $\gamma$ could all be slightly modified without significantly affecting the conclusions.


\paragraph{Linear and trigonometric interpolants.} One way to ensure that~\eqref{eq:sumto1} holds while maintaining the influence of $\rho_0$ and $\rho_1$ everywhere on $[0,1]$ except at the endpoints is to choose 
\begin{equation}
\label{eq:lin:a:b:c}
\alpha(t) = t, \qquad \beta(t) = 1-t, \qquad \gamma(t) = \sqrt{2t(1-t)}.
\end{equation}
This choice leads to the stochastic interpolant specified in~\eqref{eq:lin:interp}. It is also the choice that was advocated in~\cite{liu2022}, without the inclusion of the latent variable ($\gamma =0$). Another possibility that gives more leeway is to pick any $\gamma: [0,1]\to[0,1]$ and set
\begin{equation}
\label{eq:trig}
\alpha(t) = \sqrt{1-\gamma^2(t)} \cos(\tfrac 12 \pi t), \qquad \beta(t) = \sqrt{1-\gamma^2(t)} \sin(\tfrac 12 \pi t).
\end{equation}
With $\gamma=0$, this was the choice preferred in~\cite{albergo2023building}. 
%
As shown in Theorem~\ref{prop:interpolate}, the presence of the latent variable $\gamma(t) z$ for $\gamma \neq 0$ smooths both the density $\rho(t)$ and the velocity $b$ defined in~\eqref{eq:b:ode:def} spatially, which provides a computational advantage at sample generation time because it simplifies the required numerical integration of~\eqref{eq:ode:1},~\eqref{eq:sde:1}, and~\eqref{eq:sde:R}. 
%
Intuitively, this is because the density $\rho(t)$ of $x_t$ can be represented exactly as the density that would be obtained with $\gamma = 0$ convolved with $\mathsf{N}(0, \gamma^2(t)Id)$ at each $t\in(0,1)$. A comparison between the density $\rho(t)$ obtained with trigonometric interpolants with $\gamma(t) = 0$ and $\gamma(t) = \sqrt{2t(1-t)}$ can be seen in the first and second row of Figure~\ref{fig:gmm_rhot}.
%
%
\begin{wrapfigure}[12]{r}{0.51\textwidth}
\centering
\vspace{-0.0cm}
  \includegraphics[width=1.0\linewidth]{figs/alphas-betas-gammas.pdf}
  \caption{%\textbf{Interpolant visualization.} 
\textbf{The functions $\alpha(t)$, $\beta(t)$, and $\gamma(t)$} for the linear \eqref{eq:lin:a:b:c}, trigonometric~\eqref{eq:trig} with $\gamma(t)=\sqrt{2t(1-t)}$, and Gaussian encoding-decoding~\eqref{eq:alpha:beta:2} interpolants.}
  \label{fig:interps}
\end{wrapfigure}
%

\paragraph{Gaussian encoding-decoding.}  Adding the latent variable $\gamma(t) z$ to~\eqref{eq:stochinterp} has another potential advantage, in that we can use the density of $\gamma(t) z$ as an intermediate to interpolate between $\rho_0$ and $\rho_1$. 


This can be advantageous if $\rho_0$ and $\rho_1$ have distinct complex features, which would be duplicated in $\rho(t)$ at intermediate times if not for the smoothing effect of the latent variable; this behavior is seen in Figure~\ref{fig:gmm_rhot}, where it is most prominent in the first row with $\gamma(t) = 0$. From a statistical learning perspective, eliminating the formation of spurious features will simplify the estimation of the velocity field $b$, which becomes smoother as the formation of such features is suppressed. A useful limiting case is to devolve the data from $\rho_0$ completely into noise by the halfway point $t=\tfrac12$ and to reconstruct $\rho_1$ completely from noise starting from $t=\frac12$. One choice that allows us to do so while satisfying~\eqref{eq:sumto1} is
\begin{equation}
    \label{eq:alpha:beta:2}
    \alpha(t) = \cos^2(\pi t) 1_{[0,\frac12)}(t), \qquad \beta(t) = \cos^2(\pi t)1_{(\frac12,1]}(t), \qquad \gamma(t) = \sin^2(\pi t),
\end{equation}
where $1_{A}(t)$ is the indicator function of $A$, i.e. $1_A(t) =1$ if $t\in A$ and $1_A(t)=0$ otherwise.
With this choice, it is easy to see that $x_{t=\frac12} = \gamma(\tfrac{1}{2}) z \sim {\sf N}(0,\gamma^2(\tfrac{1}{2}))$, which seamlessly glues together two interpolants: one between $\rho_0$ and a standard Gaussian, and one between a standard Gaussian and $\rho_1$.

Even though the choice~\eqref{eq:alpha:beta:2} encodes $\rho_0$ into pure noise on the interval $[0,\tfrac12]$, which is then decoded into $\rho_1$ on the interval $[\tfrac12,1]$ (and vice-versa when proceeding backwards in time), the resulting velocity $b$ still defines a single continuity equation that maps $\rho_0$ to $\rho_1$ on $[0, 1]$. This is most clearly seen at the level of the probability flow~\eqref{eq:ode:1}, since its solution $X_t$ is a bijection between the initial and final conditions $X_{t=0}$ and $X_{t=1}$, but a similar pairing can also be observed in the solutions to the forward and backward SDEs~\eqref{eq:sde:1} and~\eqref{eq:sde:R}, whose solutions at time $t=1$ or $t=0$ remain correlated with the initial or final condition used. This allows for a more direct means of image-to-image translation with diffusions when compared to the recent approach described in \cite{su2023dual}.  The choice~\eqref{eq:alpha:beta:2} is depicted in the final row of Figure~\ref{fig:gmm_rhot}, where no spurious modes form at all; individual sample trajectories of the deterministic and stochastic generative models based on ODEs and SDEs whose solutions have this $\rho(t)$ as density can be seen in the panels forming the third column in Figure~\ref{fig:gmm_trajs}.  

Unsurprisingly, it is necessary to have $\gamma(t) > 0$ for the choice~\eqref{eq:alpha:beta:2}: for $\gamma(t) = 0$, the density $\rho(t)$ collapses to a Dirac measure at $t=\frac12$. This consideration highlights that the inclusion of the latent variable $\gamma(t) z$ matters even for the deterministic dynamics~\eqref{eq:ode:1}, and its presence is distinct from the stochasticity inherent to the SDEs~\eqref{eq:sde:1} and \eqref{eq:sde:R}.

\begin{table}[t!]
    \centering
    \captionsetup{justification=raggedright} % Center caption
    \newcolumntype{C}{>{\centering\arraybackslash}p{1.5cm}} % Define a new centered column type with a fixed width
    \begin{tabular}{S{c}|S{C}|S{C}|S{C}|S{C}}
        % \hline
           $\gamma(t):$ & $\sqrt{a t(1-t)}$ & $ t(1-t)$ & $\hat \sigma(t)$ & $\sin^2(\pi t)$ \\
        \hline
        $C^1$ at $t=0,1$ & \ding{55}   & \ding{51}   & \ding{51} &  \ding{51} \\ \hline
        % $\gamma \not\in C^1([0,1]) $ \textbf{?}  & \ding{51}   & \ding{55}   & \ding{55}  & \ding{55}  \\ \hline
        % \hline
    \end{tabular}
    \caption{\textbf{Differentiability of $\gamma(t)$.} A characterization of the possible choices of $\gamma(t)$ with respect to their differentiability. The column specified by $\hat \sigma(t)$ is sum of sigmoid functions, made compact by the notation $\hat\sigma(t) = \sigma(f (t + 1)) - \sigma(f (t -1 )) - \sigma(f) + \sigma(-f) $, where $\sigma(t) = e^t/(1+e^t)$ and $f$ is a scaling factor.  }
    \label{tab:gammas}
\end{table}

\paragraph{The influence of $\gamma$ on $b$ at $t=0$ and $t=1$.} Another potential advantage of including the latent variable $\gamma(t) z$ is its impact on the velocity $b$ at the end points. Consider the decomposition of $b$ in~\eqref{eq:b:decomp} -- for the linear interpolant~\eqref{eq:lin:interp}, this decomposition can be written
\begin{equation}
    \label{eq:b:decomp:lin}
    b(t,x) = \alpha(t) \EE ( x_0 | x_t = x) + \beta(t) \EE ( x_1 | x_t = x) - \dot \gamma(t) \gamma(t) s(t,x),
\end{equation}
where $s$ is the score given in~\eqref{eq:s:def}. Since $x_{t=0}=x_0$ and $x_{t=1}=x_1$, we have
\begin{equation}
    \label{eq:vel:0:1}
    \begin{aligned}
    b(x,0) &= \dot\alpha(0) x + \dot \beta(0) m_1 - \lim_{t\to0} \gamma(t) \dot\gamma(t) s_0(x),\\
    b(x,1) &= \dot\alpha(1) m_0 + \dot \beta(1) x - \lim_{t\to1} \gamma(t) \dot\gamma(t) s_1(x),
    \end{aligned}
\end{equation}
where $m_0 = \EE x_0$,  $m_1 = \EE x_1$, $s_0 = \nabla \log \rho_0$, and $s_1 = \nabla \log \rho_1$. If $\gamma\in C^2([0,1])$, because $\gamma(0)=\gamma(1)=0$, the terms involving the scores $s_0$ and $s_1$ in these expressions vanish. Choosing $\gamma^2 \in C^1([0,1])$ but $\gamma$ not differentiable at $t=0$ or $t=1$ leaves open the possibility that the limits remain nonzero. For example, taking 
\begin{equation}
    \label{eq:gam:Bt}
    \gamma(t) = \sqrt{a t(1-t)}, \quad a < 4
\end{equation}
we obtain
\begin{equation}
    \label{eq:gam:Bt:lim}
    \lim_{t\to0}\gamma(t)\dot{\gamma}(t) = - \lim_{t\to1}\gamma(t)\dot{\gamma}(t) = \frac{a}{2}.
\end{equation}
As a result, the choice~\eqref{eq:gam:Bt} ensures that the velocity $b$ encodes information about the score of the densities $\rho_0$ and $\rho_1$ at the end points, where $\rho(t)$ must converge to one of them. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figs/gmm_trajs.pdf}   
    \caption{\textbf{The effect of $\epsilon$ on sample trajectories.} A visualization of how the choice of $\epsilon$ affects the sample trajectories obtained by solving the ODE~\eqref{eq:ode:1} or the forward SDE~\eqref{eq:sde:1}. The set-up is the same as in Figure~\ref{fig:gmm_rhot}:
    %
    $\rho_0$ and $\rho_1$ are taken to be the same Gaussian mixture densities as in Figure~\ref{fig:gmm_rhot}, and the analytical expressions for $b$ and $s$ are used.
    %
    In the three panels in each column the value of $\gamma$ is the same, and each panel shows trajectories with different $\eps$. 
    Three specific trajectories from the same three initial conditions drawn from $\rho_0$ are also highlighted in white in every panel.
    %
    As $\epsilon$ increases but $\gamma$ stays the same, the density $\rho(t)$ is unchanged, but the individual trajectories become increasingly stochastic.
    %
    While all choices are equivalent with exact $b$ and $s$, Theorem~\ref{thm:kl:bound} shows that nonzero values of $\epsilon$ provide control on the likelihood in terms of the error in $b$ and $s$ when they are approximate.}
    \label{fig:gmm_trajs}
\end{figure}

\begin{remark}[Brownian bridge]
    The choice~\eqref{eq:gam:Bt} is conceptually interesting, as it corresponds to using a latent variable $\gamma(t) z$ that has the same statistical properties at any fixed $t\in[0,1]$ as the scaled Brownian bridge $\sqrt{a} B_t$, i.e. the process realizable in terms of the Wiener process $W_t$ as $B_t = W_t -tW_1$, which by definition satisfies $B_1=B_0=0$. Indeed, both $\sqrt{a} B_t \sim {\sf N}(0,at(1-t)\Id)$ and $\gamma(t) z = \sqrt{at(1-t)} z \sim {\sf N}(0,at(1-t)\Id)$ for all $t\in[0,1]$, so that they lead to the same $\rho$ and $b$. This connection is explored further in Section~\ref{sec:si:schb}.
\end{remark}

\paragraph{Other choices of $\gamma$.} 
While the choice of $\gamma(t)$ given in \eqref{eq:gam:Bt} is appealing because of its nontrivial influence on the velocity $b$ at the endpoints, the user is free to explore a variety of alternatives. We present some examples in Table~\ref{tab:gammas}, specifying the differentiability of $\gamma$ at $t=0$ and $t=1$. The function $\gamma(t)$ specified in~\eqref{eq:gam:Bt} is the only featured case for which the contribution from the score is non-vanishing in the velocity $b$ at the endpoints. In the following section, we illustrate that there are tradeoffs between different choices of $\gamma$, which could be directly related to this fact. When using the ODE as a generative model, the score is only felt through $b$, whereas it is explicit when using the SDE as a generative model.


\subsection{Deterministic vs stochastic generative models}
\label{sec:sde:ode}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.80\linewidth]{figs/big-comparison-paper.pdf}
    \caption{\textbf{The effects of $\gamma(t)$ and $\epsilon$ on sample quality: qualitative comparison.} Kernel density estimates of $\hat\rho(1)$ for models with different choices of $\gamma$ and $\epsilon$.  Sampling with $\epsilon = 0$ corresponds to using the probability flow with the learned drift~$\hat b = \hat v - \gamma\dot \gamma \hat s$, whereas sampling with $\epsilon >0 $ corresponds to using the SDE with  the learned drift~$\hat b$ and score~$\hat s$. We find empirically that SDE sampling is generically better than ODE sampling for this target density, though the gap is smallest for the probability flow specified with $\gamma(t) = \sqrt{t(1-t)}$, in agreement with the remarks in Section~\ref{sec:reg:noise} regarding the influence of $\gamma$ on $b$. The SDE performs well at any noise level, though numerically integrating it for higher $\epsilon$ requires a smaller step size.}
    \label{fig:ode-sde-big}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/f_divergence_checker.pdf}
    \caption{\textbf{The effects of $\gamma(t)$ and $\epsilon$ on sample quality: quantitative comparison.} For each $\gamma$ and each $\epsilon$ specified in Figure~\ref{fig:ode-sde-big}, we compute the mean and variance of the absolute value of the difference of $\log \rho_1$ (exact) and $\log \hat \rho(1)$ (model). The model specified with $\gamma(t) = \sqrt{t(1-t)}$ is the best performing probability flow ($\epsilon = 0$). At large, SDE sampling with the same learned drift~$\hat b$ and score~$\hat s$ performs better, complementing the observations in the previous figure.} 
    \label{fig:fdiv}
\end{figure}


So far, we have been focused on the impact of $\alpha$, $\beta$, and $\gamma$ in~\eqref{eq:lin:interp} on the density $\rho(t)$. However, as shown in Section~\ref{sec:cont:eq}, the evolution of $\rho(t)$ can be captured exactly by either the transport equation~\eqref{eq:transport} or by the forward and backward Fokker-Planck equations~\eqref{eq:fpe} and~\eqref{eq:fpe:tr}. These perspectives lead to generative models that are either based on the deterministic dynamics~\eqref{eq:ode:1} or the forward and backward stochastic dynamics~\eqref{eq:sde:1} and~\eqref{eq:sde:R}, where the intrinsic level of stochasticity can be tuned by varying the value of $\eps$. We showed in Section~\ref{sec:likelihood_bounds} that setting $\eps>0$ can offer better control on the likelihood when using an imperfect velocity~$b$ and an imperfect score~$s$; moreover, the optimal choice of $\eps$ is determined by the relative accuracy of the estimates of~$b$ and~$s$. Having laid out the evolution of $\rho(t)$ for different choices of $\gamma$ in the previous section, we show how different values of~$\epsilon$ can build these densities up from individual trajectories independent of the overall level of stochasticity in Figure~\ref{fig:gmm_trajs}. The intrinsic level of stochasticity increases with $\epsilon$, but by construction the marginal density $\rho(t)$ for fixed $\alpha$, $\beta$ and $\gamma$ is independent of $\epsilon$.


\paragraph{The roles of $\gamma(t)$ and $\eps$ for 2D density estimation.} We explore these tradeoffs of using different level of latent variable via $\gamma$, and using the ODE versus SDE as a generative model for the case where the velocity $b= v- \gamma\dot \gamma s$ is learned over a parametric class defined by feed-forward neural networks. We consider a target density $\rho_1$ whose mass concentrates on a 2-dimensional checkerboard and a base density $\rho_0 = \mathsf{N}(0,\Id)$; here, the target was chosen to highlight the ability of the method to learn a challenging density with sharp boundaries. The same model architecture and training procedure was used to learn both $v$ and $s$ for the variety of choices of $\gamma$ given in Table~\ref{tab:gammas}. The feed-forward network is defined with $4$ layers, each of size $512$, with ReLU \cite{vinod2010} activation functions between each layer. For more experimental specifications, see Appendix~\ref{app:exp}. 



After training, we draw 300,000 samples using either the ODE ($\epsilon=0$) or the SDEs with $\epsilon = 0.5$, $\epsilon=1.0$, and $\epsilon = 2.5$. We compute kernel density estimates for each resulting density, which we compare to the exact density and to the original stochastic interpolant from~\cite{albergo2023building} (obtained by setting $\gamma = 0$). Results are given in Figure~\ref{fig:ode-sde-big} for each $\gamma$ and each $\epsilon$. Sampling with $\epsilon > 0 $ empirically performs better, though the gap is smallest when using the $\gamma$ specified in \eqref{eq:gam:Bt}. Moreover, even when $\epsilon = 0$, using the probability flow with $\gamma$ given by~\eqref{eq:gam:Bt} performs better than the original interpolant from \cite{albergo2023building}. Numerical comparisons of the mean and variance of the absolute value of the difference of $\log \rho_1$ (exact) from $\log \hat\rho(1)$ (model) for the various configurations are given in Figure~\ref{fig:fdiv}, which corroborate the above observations.

\subsection{One-sided stochastic interpolants for Gaussian $\rho_1$}
\label{sec:onesided}
We now study the special case $\rho_1 = \mathsf{N}(0,\Id)$; this setting is practically relevant, as $\mathsf{N}(0, Id)$ is a common choice of base density for generative modeling in the absence of prior information, and it will allow us to instantiate score-based diffusion within our general framework. In this setting, the effect of the latent variable $z$ can be lumped with $x_1$, leading to a simpler type of stochastic interpolant: 

\begin{definition}[One-sided stochastic interpolant]
\label{def:interp:os}
 Given a probability density function $\rho_0: {\RR^d} \rightarrow \RR_{\geq 0}$, a \textit{one-sided stochastic interpolant} between $\rho_0$ and ${\sf N}(0,\Id)$ is a stochastic process $y_t$
\begin{equation}
    \label{eq:stochinterp:os}
    y_t = J(t,x_0) + \beta(t) z,  \qquad t\in [0, 1]
\end{equation}
that fulfills the requirements:
\begin{enumerate}[leftmargin=0.15in]
\item $J: [0,1]\times \RR^d \to \RR^d$ satisfies the boundary conditions $J(0,x_0) = x_0$ and $J(1,x_0) = 0$.
\item $x_0$ and $z$ are  random variables drawn independently from $\rho_0$ and ${\sf N}(0,\Id)$, respectively.
\item $\beta: [0,1] \to \RR $ satisfies $\beta(0)=0$, $\beta(1) = 1$, $\beta(t)>0$ for all $t \in (0, 1]$, and $\beta^2 \in C^1([0,1])$.
\end{enumerate}
\end{definition}

By construction, $y_{t=0} = x_0 \sim \rho_0$ and $y_{t=1} = z \sim {\sf N}(0,\Id)$, so that the distribution of the stochastic process~$y_t$ bridges $\rho_0$ and ${\sf N}(0,\Id)$. It is easy to see that the one-sided stochastic interpolant defined in~\eqref{eq:stochinterp:os} will have the same density as the stochastic interpolant defined in~\eqref{eq:stochinterp} if we set $I(t,x_0,x_1) = J_t(x_0) + \delta(t) x_1$ and take $\delta^2(t)+ \gamma^2(t) = \beta^2(t)$.  Restricting to this case, our earlier theoretical results apply where the velocity field $b$ defined in~\eqref{eq:b:ode:def} becomes
\begin{equation}
    \label{eq:b:ode:def:os}
    b_\ODE(t,x) = \EE ( \partial_t J(t,x_0) + \dot \beta(t) z| y_t = x),
\end{equation}
and the quadratic objective in~\eqref{eq:obj:v} becomes
\begin{equation}
    \label{eq:obj:v:os}
    \mathcal{L}_b[\hat{b}] =\int_0^1   \EE \left( \tfrac12|\hat b(t,y_t)|^2 - \left(\partial_t J(t,x_0) + \dot \beta(t) z\right) \cdot \hat b(t,y_t) \right) dt.
\end{equation}
In the expression above, $y_t$ is given by~\eqref{eq:stochinterp:os} and the expectation $\EE$ is taken independently over $x_0\sim \rho_0$ and $z\sim {\sf N}(0,\Id)$. Similarly, the score is given by
\begin{equation}
    \label{eq:obj:s:os}
    s(t,x) = - \beta^{-1}(t) \EE(z|y_t=x).
\end{equation}
Moreover, we can weaken Assumption~\ref{as:rho:I} to the following requirement:

\begin{assumption}
\label{as:rho:J}
The density $\rho_0 \in C^2(\RR^d)$, satisfies $\rho_0(x) > 0$ for all $x \in \RR^d$, and:
\begin{equation}
    \label{eq:rho0:sc}
     \int_{\RR^d} |\nabla \log \rho_0(x)|^2 \rho_0(x) dx < \infty.
\end{equation}
The function $J \in C^2([0,1],C^2(\RR^d)^d)$ satisfies
\begin{equation}
    \label{eq:bound:dJ}
    \begin{aligned}
        &\exists C_1<\infty  \   : \ 
        &&|\partial_t J(t,x_0)|\le C_1|x_0|
        \quad  &&\text{for all}\quad (t,x_0) \in [0,1]\times \RR^d,
        \end{aligned}
\end{equation}
and
\begin{equation}
    \label{eq:Jt:L2}
    \exists M_1,M_2 < \infty  \ \ : \ \  \EE\big[ |\partial_t J(t,x_0)|^4\big] \le M_1; \quad \EE\big[ |\partial^2_t J(t,x_0)|^2\big] \le M_2, \quad  \text{for all}\quad t\in [0,1],
\end{equation}
where the expectation is taken over $x_0\sim \rho_0$.
\end{assumption}


\begin{remark}
    The construction above can easily be generalized to the case where $\rho_1 = \mathsf{N}(0, C_1)$ with $C_1$ a positive-definite matrix. Without loss of generality, we can then assume that $C_1$ can be represented as $C_1 = \sigma_1 \sigma_1^\T$ where $\sigma_1$ is a lower-triangular matrix and replace \eqref{eq:stochinterp:os}
    \begin{equation}
    \label{eq:stochinterp:os:C}
    x_t = J(t,x_0) + \beta(t) \sigma_1 z,  \qquad t\in [0, 1],
\end{equation}
with $J$ and $\beta$ satisfying the conditions listed in Definition~\ref{def:interp:os} and where $z\sim {\sf N}(0,\Id)$.
\end{remark}

\subsection{Comparison with score-based diffusion models}
\label{sec:SBDM}
Score-based diffusions are based on variants of the Ornstein-Uhlenbeck process
\begin{equation}
    \label{eq:sbdm:sde}
    d Z_t = - Z_t dt + \sqrt{2} dW_t, \qquad Z_{t=0} \sim \rho_0,
\end{equation}
which has the property that the marginal density of its solution at time $t$ converges to a standard normal as $t$ tends towards infinity. By learning the score of the density of $Z_t$, we can write the associated backward SDE for~\eqref{eq:sbdm:sde}, which can then be used as a generative model. Since the solution of \eqref{eq:sbdm:sde} from the initial condition $Z_{t=0} = x_0$ reads
\begin{equation}
    \label{eq:sbdm:sde:sol}
    Z_t = x_0 e^{-t}  + \sqrt{2} \int_0^t e^{-t+s} dW_s,
\end{equation}
the law of $Z_t$ conditional on $Z_{t=0} = x_0$ is given at any time $t\in[0,\infty)$ by
\begin{equation}
    \label{eq:sbdm:sde:sol:2}
    Z_t \sim N(x_0 e^{-t}, (1-e^{-2t})).
\end{equation}
As a result, the time-dependent density of the OU process in~\eqref{eq:sbdm:sde} coincides with the density of the infinite-horizon one-sided stochastic interpolant
\begin{equation}
    \label{eq:sbdm:sde:sol:3}
    y_t = x_0 e^{-t} + \sqrt{1-e^{-2t}}\, z, \qquad x_0 \sim\rho_0, \quad z\sim {\sf N}(0,\Id), \qquad t \in [0, \infty).
\end{equation}

\paragraph{Infinite-horizon.} The above stochastic interpolant does not satisfy a key property that we impose in this paper -- namely, the density of $y_t$ only converges to ${\sf N}(0,\Id)$ as $t\to\infty$. In SBDM, this is handled by capping the evolution of $Z_t$ to a finite time interval $[0, T]$ with $T < \infty$ and using the backward SDE associated with~\eqref{eq:sbdm:sde} restricted to $[0,T]$. However, this introduces an additional source of error that is not present with the finite-time one-sided interpolation procedure introduced in the previous section, because the final conditions used for the backwards SDE in SBDM are drawn from ${\sf N}(0,\Id)$ even though the density of the process~\eqref{eq:sbdm:sde} is not exactly Gaussian at time $T$. %In addition, the final time $T$ becomes an additional meta-parameter, and the learning of the score must be done on the full interval $[0,T]$, rather than $[0,1]$. 

One-sided stochastic interpolants have the advantage that they can be used on $t\in[0,1]$ without bias. The price paid is that we need to learn the velocity field $b$ defined in~\eqref{eq:b:ode:def:os} via minimization of the objective~\eqref{eq:obj:v:os} in addition to the score in order to use the backward SDE~\eqref{eq:sde:R} as a generative model (note that using  the ODE~\eqref{eq:ode:1} only requires learning $b$). Using two-sided stochastic interpolants offers the additional advantage that $\rho_1$ can be any density, rather than just ${\sf N}(0,\Id)$. 


\paragraph{SBDM in finite time.} Interestingly, when the function $J(t,x_0)$ is linear in $x_0$, the finite-horizon one-sided stochastic interpolant satisfies a closed forward SDE:

\begin{restatable}{theorem}{sdeos}
\label{thm:sdeos:ou}
Consider the finite-horizon one-sided stochastic interpolant 
\begin{equation}
    \label{eq:os:lin:x0}
    y_t = \alpha(t) x_0 + \beta(t) z, 
\end{equation}
with $x_0$ and $z$ drawn independently from $\rho_0$ and ${\sf N}(0,\Id)$, respectively, and where the functions $\alpha$ and $\beta^2$ are in $C^1([0,1])$ and satisfy
\begin{equation}
    \label{eq:lin:interp:a:b:2}
    \alpha(0)=\beta(1)=1; \quad \alpha(1)=\beta(0)=0; \quad \forall t\in(0,1) \ : \ \alpha(t)> 0, \ \dot \alpha(t)<0, \beta(t)>0, \ \dot \beta(t) > 0.
\end{equation}
Then at any time $t\in[0,1)$, the law of $y_t$ coincides with the law of the solution to the forward SDE
\begin{equation}
    \label{eq:sde:os:lin}
    dZ^\fwd_t = \alpha^{-1} (t) \dot \alpha(t) Z^\fwd_t dt + \sqrt{2D(t)}  dW_t, \qquad Z^\fwd_{t=0}\sim \rho_0,
\end{equation}
where we defined
\begin{equation}
    \label{eq:sig:def}
    D(t) = \beta(t) \dot \beta(t) - \beta^2(t)\alpha^{-1}(t)\dot\alpha(t).  
\end{equation}
\end{restatable}

The proof is given in Appendix~\ref{app:sdeos:ou}. The conditions in~\eqref{eq:lin:interp:a:b} guarantee that the diffusion coefficient $D(t)$ is positive for $t\in[0,1]$. If we instead consider the infinite time interval $[0,\infty)$ and choose $\alpha(t) = e^{-t}$ and $\beta(t) = \sqrt{1-e^{-2t}}$ as in~\eqref{eq:sbdm:sde:sol}, a direct calculation shows that the SDE~\eqref{eq:sde:os:lin} reduces to the OU process in~\eqref{eq:sbdm:sde}. 

The result of Theorem~\ref{thm:sdeos:ou} indicates that if we learn the score for the linear one-sided stochastic interpolant in~\eqref{eq:os:lin:x0}, e.g. via minimization of the objective in \eqref{eq:obj:s}, we can use the solution to the following backward SDE as a generative model
\begin{equation}
    \label{eq:sde:os:R:lin}
    dZ^\rev_t = \alpha^{-1} (t) \dot \alpha(t) Z^\rev_t dt - 2 D(t) s(t, Z^\rev_t) dt + \sqrt{2D(t)}dW^\rev_t, \qquad Z_{t=1}^\rev\sim {\sf N}(0,\Id).
\end{equation} 
However, the difficulty with this backward SDE (as well as the forward SDE~\eqref{eq:sde:os:lin} if we want to extend its solution until $t=1$) is that, due to the boundary condition $\alpha(1)=0$ and $\alpha\in C^1([0,1])$, we necessarily have 
\begin{equation}
    \label{eq:sing:end}
    \alpha^{-1}(t) \dot \alpha(t) = -(1-t)^{-1} +O(1), \quad D(t)= (1-t)^{-1} +O(1)  \quad \text{as} \ \ t\to1,
\end{equation}
so that the drift and the diffusion coefficients in~\eqref{eq:sde:os:lin} and~\eqref{eq:sde:os:R:lin} are singular at $t=1$.


\begin{remark}[Non-singularity for stochastic interpolants]
Unlike finite-time SBDM, the drifts in the ODE and the forward and backward SDEs listed in Theorem~\eqref{prop:generative} are non-singular. That is, if we write the forward SDE \eqref{eq:sde:os:lin} as a probability flow:
\begin{equation}
    \label{eq:ode:os:lin}
    \frac{d}{dt} X_t = \alpha^{-1} (t) \dot \alpha(t) X_t - D(t) s(t, X_t),
\end{equation}
the singularities individually present in each term on the right hand-side cancel, and  their difference is precisely the velocity $b\in C^0([0,1], C^1(\RR^d)^d)$ defined in~\eqref{eq:b:ode:def} with $I(t,x_0,x_1) = \alpha(t) x_0 +\beta(t) x_1$.  As a result the right hand-side of~\eqref{eq:ode:os:lin}  is well-defined and continuous at $t=0$ and $t=1$:
\begin{equation}
\label{eq:lim}
\lim_{t\to0}\alpha^{-1} (t) \dot \alpha(t) x - D(t) s(t, x) = b(0,x), \qquad 
\lim_{t\to1}\alpha^{-1} (t) \dot \alpha(t) x - D(t) s(t, x) = b(1,x).
\end{equation} 
A better way to take advantage of the extra stability conferred by using stochastic models instead of deterministic ones is to turn the ODE~\eqref{eq:ode:os:lin} into forward and backward SDEs, i.e., to pick some $\eps>0$ and use
\begin{equation}
    \label{eq:sde:f:os:lin}
    d X^\fwd_t = \alpha^{-1} (t) \dot \alpha(t) X^\fwd_t dt + (\eps-D(t)) s(t, X^\fwd_t) dt + \sqrt{2\eps} dW_t,
\end{equation} 
and
\begin{equation}
    \label{eq:sde:b:os:lin}
    d X^\rev_t = \alpha^{-1} (t) \dot \alpha(t) X^\rev_t dt- (\eps+D(t)) s(t, X^\rev_t) dt + \sqrt{2\eps} dW^\rev_t.
\end{equation} 
\end{remark}


\subsection{Rectifying stochastic interpolants}
\label{sec:rect}

Our approach offers a way to apply the rectifying procedure proposed in~\cite{liu2022}, but without introducing a bias -- we now discuss this in the context of
one-sided stochastic interpolants  where the resulting iteration simplifies. Suppose that we started with some $J(t,x_0)$ and have used it to learn $b$ via minimization of~\eqref{eq:obj:v:os}. Denote by $X_t(x_0)$ the solution to the ODE~\eqref{eq:ode:1}  with this $b$ and with the initial condition $X_{t=0}(x_0)=x_0$, i.e.
\begin{equation}
    \label{eq:ode:1:os:x0}
    \frac{d}{dt}  X_t(x_0) = b_\ODE(t, X_t(x_0)), \qquad X_{t=0}(x_0)=x_0.
\end{equation}
If we pick functions $\alpha$ and $\beta$ satisfying the conditions in~\eqref{eq:lin:interp:a:b}, we can define a new one-sided interpolant using
\begin{equation}
    \label{eq:new:os}
    y_t = \alpha(t) X_t(x_0) + \beta(t) z.
\end{equation}
Clearly, $y_{t=0} = x_0\sim \rho_0$ and $y_{t=1} = z\sim {\sf N}(0,\Id)$ since $X_{t=0}(x_0)=x_0$. We can then learn a new velocity $b$ by minimizing~\eqref{eq:obj:v:os}  with the new interpolant. This ``rectification'' procedure is similar to the one proposed in~\cite{liu2022}, but it is unbiased, since~\eqref{eq:new:os} is a valid one-sided interpolant that satisfies all the conditions of Definition~\ref{def:interp:os}. This procedure can then be iterated -- the result is a more expensive learning algorithm that may produce simpler flows, which was the original motivation of~\cite{liu2022}.
An analogous approach can also be formulated with two-sided interpolants. For brevity we omit the discussion, and instead focus on an alternative procedure that allows us to compute the Schr\"odinger between two densities.