@misc{lu2021maximum,
  author = {Lu, Cheng and Zheng, Kaiwen and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Maximum Likelihood Training for Score-Based Diffusion ODEs by High-Order Denoising Score Matching},
  publisher = {arXiv},
  year = {2022}
}

@book{doob1984potential,
	Address = {New York},
	Author = {Doob, J. L.},
	Date-Added = {2012-11-16 15:32:12 -0600},
	Date-Modified = {2012-11-16 15:32:12 -0600},
	Doi = {10.1007/978-1-4612-5208-5},
	Isbn = {0-387-90881-1},
	Mrclass = {31-02 (60-02 60J45)},
	Mrnumber = {731258 (85k:31001)},
	Mrreviewer = {Michael Sharpe},
	Pages = {xxiv+846},
	Publisher = {Springer-Verlag},
	Series = {Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]},
	Title = {Classical potential theory and its probabilistic counterpart},
	Url = {http://dx.doi.org/10.1007/978-1-4612-5208-5},
	Volume = {262},
	Year = {1984},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-1-4612-5208-5}}


@misc{liu2022let,
      title={Let us Build Bridges: Understanding and Extending Diffusion Generative Models}, 
      author={Xingchao Liu and Lemeng Wu and Mao Ye and Qiang Liu},
      year={2022},
      eprint={2208.14699},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{sinho_score1,
  author = {Chen, Sitan and Chewi, Sinho and Li, Jerry and Li, Yuanzhi and Salim, Adil and Zhang, Anru R.},
  title = {Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions},
  journal = {arXiv:2209.11215},
  year = {2022}
}

@article{holden_score3,
    journal={arxiv:2211.01916},
    author = {Chen, Hongrui and Lee, Holden and Lu, Jianfeng},
    title = {Improved Analysis of Score-based Generative Modeling: User-Friendly Bounds under Minimal Smoothness Assumptions},
    year = {2022}
}

@InProceedings{holden_score2,
  title = 	 {Convergence of score-based generative modeling for general data distributions},
  author =       {Lee, Holden and Lu, Jianfeng and Tan, Yixin},
  booktitle = 	 {Proceedings of The 34th International Conference on Algorithmic Learning Theory},
  pages = 	 {946--985},
  year = 	 {2023},
  editor = 	 {Agrawal, Shipra and Orabona, Francesco},
  volume = 	 {201},
  series = 	 {Proceedings of Machine Learning Research}
}

@article{holden_score1,
  author = {Lee, Holden and Lu, Jianfeng and Tan, Yixin},
  journal = {arXiv:2206.06227},
  title = {Convergence for score-based generative modeling with polynomial complexity},
  year = {2022}
}

@article{vincent_connection_2011,
	title = {A {Connection} {Between} {Score} {Matching} and {Denoising} {Autoencoders}},
	volume = {23},
	issn = {0899-7667, 1530-888X},
	number = {7},
	journal = {Neural Computation},
	author = {Vincent, Pascal},
	year = {2011},
	pages = {1661--1674}
}

@article{papamakarios_jmlr,
  author  = {George Papamakarios and Eric Nalisnick and Danilo Jimenez Rezende and Shakir Mohamed and Balaji Lakshminarayanan},
  title   = {Normalizing Flows for Probabilistic Modeling and Inference},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {57},
  pages   = {1--64}
}

@inproceedings{
albergo2023building,
title={Building Normalizing Flows with Stochastic Interpolants},
author={Michael Samuel Albergo and Eric Vanden-Eijnden},
booktitle={International Conference on Learning Representations},
year={2023},
}

@article{leonard2014survey,
title = {A survey of the Schr{\"o}dinger problem and some of its  connections with optimal transport},
journal = {Discrete and Continuous Dynamical Systems},
volume = {34},number = {4},pages = {1533-1574},
year = {2014},
issn = {1078-0947},
author = {Christian L{\'e}onard}
}

@misc{heng2021simulating,
  author = {Heng, Jeremy and De Bortoli, Valentin and Doucet, Arnaud and Thornton, James},
  keywords = {Computation (stat.CO), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Simulating Diffusion Bridges with Score Matching},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{
peluchetti2022nondenoising,
title={Non-Denoising Forward-Time Diffusions},
author={Stefano Peluchetti},
year={2022},
}

@misc{mikulincer2022probability,
  doi = {10.48550/ARXIV.2201.01382},
  author = {Mikulincer, Dan and Shenfeld, Yair},
  keywords = {Probability (math.PR), Analysis of PDEs (math.AP), Functional Analysis (math.FA), FOS: Mathematics, FOS: Mathematics, 39B62 (Primary) 49Q22, 49R05, 35P15 (Secondary)},
  title = {On the Lipschitz properties of transportation along heat flows},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{neklyudov2022action,
  doi = {10.48550/ARXIV.2210.06662},
  author = {Neklyudov, Kirill and Severo, Daniel and Makhzani, Alireza},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Action Matching: A Variational Method for Learning Stochastic Dynamics from Samples},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{santambrogio2015optimal,
  title={Optimal transport for applied mathematicians},
  author={Santambrogio, Filippo},
  journal={Birk{\"a}user, NY},
  volume={55},
  number={58-63},
  pages={94},
  year={2015},
  publisher={Springer}
}

@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2009},
  publisher={Springer}
}

@article{benamou2000computational,
  title={A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem},
  author={Benamou, Jean-David and Brenier, Yann},
  journal={Numerische Mathematik},
  volume={84},
  number={3},
  pages={375--393},
  year={2000},
  publisher={Springer-Verlag}
}

@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}
@inproceedings{Song2019,
 author = {Song, Yang and Ermon, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Modeling by Estimating Gradients of the Data Distribution},
 volume = {32},
 year = {2019}
}

@inproceedings{
dinh2017density,
title={Density estimation using Real {NVP}},
author={Laurent Dinh and Jascha Sohl-Dickstein and Samy Bengio},
booktitle={International Conference on Learning Representations},
year={2017},
}

@inproceedings{ho2020,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 volume = {33},
 year = {2020}
}

@inproceedings{
song2021scorebased,
title={Score-Based Generative Modeling through Stochastic Differential Equations},
author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2021},
}




@InProceedings{huang2018,
  title = 	 {Neural Autoregressive Flows},
  author =       {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2078--2087},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/huang18d/huang18d.pdf},
  abstract = 	 {Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF) (Papamakarios et al., 2017), and to accelerate state-of-the-art WaveNet-based speech synthesis to 20x faster than real-time (Oord et al., 2017), via Inverse Autoregressive Flows (IAF) (Kingma et al., 2016). We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.}
}


@inproceedings{
grathwohl2018scalable,
title={Scalable Reversible Generative Models with Free-form Continuous Dynamics},
author={Will Grathwohl and Ricky T. Q. Chen and Jesse Bettencourt and David Duvenaud},
booktitle={International Conference on Learning Representations},
year={2019},
}


@inproceedings{Feller1949OnTT,
  title={On the Theory of Stochastic Processes, with Particular Reference to Applications},
  author={William Feller},
  year={1949},
  publisher={University of California Press},
  booktitle={Proceedings of the [First] Berkeley Symposium on Mathematical Statistics and Probability}
}


@InProceedings{dickstein2015,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}


@article{Friedman1987,
 ISSN = {01621459},
 abstract = {A new projection pursuit algorithm for exploring multivariate data is presented that has both statistical and computational advantages over previous methods. A number of practical issues concerning its application are addressed. A connection to multivariate density estimation is established, and its properties are investigated through simulation studies and application to real data. The goal of exploratory projection pursuit is to use the data to find low- (one-, two-, or three-) dimensional projections that provide the most revealing views of the full-dimensional data. With these views the human gift for pattern recognition can be applied to help discover effects that may not have been anticipated in advance. Since linear effects are directly captured by the covariance structure of the variable pairs (which are straightforward to estimate) the emphasis here is on the discovery of nonlinear effects such as clustering or other general nonlinear associations among the variables. Although arbitrary nonlinear effects are impossible to parameterize in full generality, they are easily recognized when presented in a low-dimensional visual representation of the data density. Projection pursuit assigns a numerical index to every projection that is a functional of the projected data density. The intent of this index is to capture the degree of nonlinear structuring present in the projected distribution. The pursuit consists of maximizing this index with respect to the parameters defining the projection. Since it is unlikely that there is only one interesting view of a multivariate data set, this procedure is iterated to find further revealing projections. After each maximizing projection has been found, a transformation is applied to the data that removes the structure present in the solution projection while preserving the multivariate structure that is not captured by it. The projection pursuit algorithm is then applied to these transformed data to find additional views that may yield further insight. This projection pursuit algorithm has potential advantages over other dimensionality reduction methods that are commonly used for data exploration. It focuses directly on the "interestingness" of a projection rather than indirectly through the interpoint distances. This allows it to be unaffected by the scale and (linear) correlational structure of the data, helping it to overcome the "curse of dimensionality" that tends to plague methods based on multidimensional scaling, parametric mapping, cluster analysis, and principal components.},
 author = {Jerome H. Friedman},
 journal = {Journal of the American Statistical Association},
 number = {397},
 pages = {249--266},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Exploratory Projection Pursuit},
 volume = {82},
 year = {1987}
}

@inproceedings{Chen2000,
 author = {Chen, Scott and Gopinath, Ramesh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {Gaussianization},
 volume = {13},
 year = {2000}
}

@article{tabak2010,
author = {Esteban G. Tabak and Eric Vanden-Eijnden},
title = {{Density estimation by dual ascent of the log-likelihood}},
volume = {8},
journal = {Communications in Mathematical Sciences},
number = {1},
publisher = {International Press of Boston},
pages = {217 -- 233},
keywords = {Density estimation, machine learning, maximum likelihood},
year = {2010},
doi = {cms/1266935020},
}

@article{tabak2013,
author = {Tabak, E. G. and Turner, Cristina V.},
title = {A Family of Nonparametric Density Estimation Algorithms},
journal = {Communications on Pure and Applied Mathematics},
volume = {66},
number = {2},
pages = {145-164},
doi = {https://doi.org/10.1002/cpa.21423},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.21423},
abstract = {Abstract A new methodology for density estimation is proposed. The methodology, which builds on the one developed by Tabak and Vanden-Eijnden, normalizes the data points through the composition of simple maps. The parameters of each map are determined through the maximization of a local quadratic approximation to the log-likelihood. Various candidates for the elementary maps of each step are proposed; criteria for choosing one includes robustness, computational simplicity, and good behavior in high-dimensional settings. A good choice is that of localized radial expansions, which depend on a single parameter: all the complexity of arbitrary, possibly convoluted probability densities can be built through the composition of such simple maps. © 2012 Wiley Periodicals, Inc.},
year = {2013}
}


@InProceedings{rezende2015,
  title = 	 {Variational Inference with Normalizing Flows},
  author = 	 {Rezende, Danilo and Mohamed, Shakir},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1530--1538},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/rezende15.pdf},
  abstract = 	 {The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.}
}



@article{hutchinson1989,
author = {   M.F.   Hutchinson },
title = {A Stochastic Estimator of the Trace of the Influence Matrix for Laplacian Smoothing Splines},
journal = {Communications in Statistics - Simulation and Computation},
volume = {18},
number = {3},
pages = {1059-1076},
year  = {1989},
publisher = {Taylor & Francis},
}

@inproceedings{song2021mle,
 author = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {1415--1428},
 publisher = {Curran Associates, Inc.},
 title = {Maximum Likelihood Training of Score-Based Diffusion Models},
 volume = {34},
 year = {2021}
}

@misc{bansal2022,
  doi = {10.48550/ARXIV.2208.09392},
  author = {Bansal, Arpit and Borgnia, Eitan and Chu, Hong-Min and Li, Jie S. and Kazemi, Hamid and Huang, Furong and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@InProceedings{lu2022higherorder,
  title = 	 {Maximum Likelihood Training for Score-based Diffusion {ODE}s by High Order Denoising Score Matching},
  author =       {Lu, Cheng and Zheng, Kaiwen and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {14429--14460},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/lu22f/lu22f.pdf},
  abstract = 	 {Score-based generative models have excellent performance in terms of generation quality and likelihood. They model the data distribution by matching a parameterized score network with first-order data score functions. The score network can be used to define an ODE (“score-based diffusion ODE”) for exact likelihood evaluation. However, the relationship between the likelihood of the ODE and the score matching objective is unclear. In this work, we prove that matching the first-order score is not sufficient to maximize the likelihood of the ODE, by showing a gap between the maximum likelihood and score matching objectives. To fill up this gap, we show that the negative likelihood of the ODE can be bounded by controlling the first, second, and third-order score matching errors; and we further present a novel high-order denoising score matching method to enable maximum likelihood training of score-based diffusion ODEs. Our algorithm guarantees that the higher-order matching error is bounded by the training error and the lower-order errors. We empirically observe that by high-order score matching, score-based diffusion ODEs achieve better likelihood on both synthetic data and CIFAR-10, while retaining the high generation quality.}
}

@inproceedings{xiao2022tackling,
title={Tackling the Generative Learning Trilemma with Denoising Diffusion {GAN}s},
author={Zhisheng Xiao and Karsten Kreis and Arash Vahdat},
booktitle={International Conference on Learning Representations},
year={2022},
}

@InProceedings{kingma:adam,
author = {Kingma, Diederick P and Ba, Jimmy},
title = {Adam: A method for stochastic optimization},
booktitle = { International Conference on Learning Representations (ICLR) },
year = {2015}
}

@inproceedings{papamakarios2017,
author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
title = {Masked Autoregressive Flow for Density Estimation},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {2335–2344},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{dormand1980,
title = {A family of embedded Runge-Kutta formulae},
journal = {Journal of Computational and Applied Mathematics},
volume = {6},
number = {1},
pages = {19-26},
year = {1980},
issn = {0377-0427},
doi = {https://doi.org/10.1016/0771-050X(80)90013-3},
author = {J.R. Dormand and P.J. Prince},
abstract = {A family of embedded Runge-Kutta formulae RK5 (4) are derived. From these are presented formulae which have (a) ‘small’ principal truncation terms in the fifth order and (b) extended regions of absolute stability.}
}


@InProceedings{finlay2020,
  title = 	 {How to Train Your Neural {ODE}: the World of {J}acobian and Kinetic Regularization},
  author =       {Finlay, Chris and Jacobsen, Joern-Henrik and Nurbekyan, Levon and Oberman, Adam},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3154--3164},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/finlay20a/finlay20a.pdf},
  abstract = 	 {Training neural ODEs on large datasets has not been tractable due to the necessity of allowing the adaptive numerical ODE solver to refine its step size to very small values. In practice this leads to dynamics equivalent to many hundreds or even thousands of layers. In this paper, we overcome this apparent difficulty by introducing a theoretically-grounded combination of both optimal transport and stability regularizations which encourage neural ODEs to prefer simpler dynamics out of all the dynamics that solve a problem well. Simpler dynamics lead to faster convergence and to fewer discretizations of the solver, considerably decreasing wall-clock time without loss in performance. Our approach allows us to train neural ODE-based generative models to the same performance as the unregularized dynamics, with significant reductions in training time. This brings neural ODEs closer to practical relevance in large-scale applications.}
}

@article{wu2021, title={OT-Flow: Fast and Accurate Continuous Normalizing Flows via Optimal Transport}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17113}, DOI={10.1609/aaai.v35i10.17113}, abstractNote={A normalizing flow is an invertible mapping between an arbitrary probability distribution and a standard normal distribution; it can be used for density estimation and statistical inference. Computing the flow follows the change of variables formula and thus requires invertibility of the mapping and an efficient way to compute the determinant of its Jacobian. To satisfy these requirements, normalizing flows typically consist of carefully chosen components. Continuous normalizing flows (CNFs) are mappings obtained by solving a neural ordinary differential equation (ODE). The neural ODE’s dynamics can be chosen almost arbitrarily while ensuring invertibility. Moreover, the log-determinant of the flow’s Jacobian can be obtained by integrating the trace of the dynamics’ Jacobian along the flow. Our proposed OT-Flow approach tackles two critical computational challenges that limit a more widespread use of CNFs. First, OT-Flow leverages optimal transport (OT) theory to regularize the CNF and enforce straight trajectories that are easier to integrate. Second, OT-Flow features exact trace computation with time complexity equal to trace estimators used in existing CNFs. On five high-dimensional density estimation and generative modeling tasks, OT-Flow performs competitively to state-of-the-art CNFs while on average requiring one-fourth of the number of weights with an 8x speedup in training time and 24x speedup in inference.}, number={10}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Onken, Derek and Wu Fung, Samy and Li, Xingjian and Ruthotto, Lars}, year={2021}, month={May}, pages={9223-9232} }




@misc{ramesh2022,
  doi = {10.48550/ARXIV.2204.06125},
  
  url = {https://arxiv.org/abs/2204.06125},
  
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@InProceedings{hoogeboom22a,
  title = 	 {Equivariant Diffusion for Molecule Generation in 3{D}},
  author =       {Hoogeboom, Emiel and Satorras, V\'{\i}ctor Garcia and Vignac, Cl{\'e}ment and Welling, Max},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {8867--8887},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/hoogeboom22a/hoogeboom22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/hoogeboom22a.html},
  abstract = 	 {This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model (EDM) learns to denoise a diffusion process with an equivariant network that jointly operates on both continuous (atom coordinates) and categorical features (atom types). In addition, we provide a probabilistic analysis which admits likelihood computation of molecules using our model. Experimentally, the proposed method significantly outperforms previous 3D molecular generative methods regarding the quality of generated samples and the efficiency at training time.}
}


@InProceedings{rombach2022,
        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},
        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},
        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
        month     = {June},
        year      = {2022},
        pages     = {10684-10695}
    }
    
@inproceedings{chen2018,
 author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Ordinary Differential Equations},
 url = {https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
 volume = {31},
 year = {2018}
}

@misc{saharia2022,
  doi = {10.48550/ARXIV.2205.11487},
  
  url = {https://arxiv.org/abs/2205.11487},
  
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{
huang2021convex,
title={Convex Potential Flows: Universal Probability Distributions with Optimal Transport and Convex Optimization},
author={Chin-Wei Huang and Ricky T. Q. Chen and Christos Tsirigotis and Aaron Courville},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=te7PVH1sPxJ}
}


@InProceedings{germain15,
  title = 	 {MADE: Masked Autoencoder for Distribution Estimation},
  author = 	 {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {881--889},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/germain15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/germain15.html},
  abstract = 	 {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.}
}


@inproceedings{
cai2022autm,
title={{AUTM} Flow: Atomic Unrestricted Time Machine for Monotonic Normalizing Flows},
author={Difeng Cai and Yuliang Ji and Huan He and Qiang Ye and Yuanzhe Xi},
booktitle={The 38th Conference on Uncertainty in Artificial Intelligence},
year={2022},
url={https://openreview.net/forum?id=SlctILjcxc}
}

@inproceedings{vinod2010,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@inproceedings{djork2016,
  author    = {Djork{-}Arn{\'{e}} Clevert and
               Thomas Unterthiner and
               Sepp Hochreiter},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Fast and Accurate Deep Network Learning by Exponential Linear Units
               (ELUs)},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.07289},
  timestamp = {Sat, 23 Jan 2021 01:12:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/ClevertUH15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
menick2018generating,
title={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},
author={Jacob Menick and Nal Kalchbrenner},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HylzTiC5Km},
}

@inproceedings{razavi2019,
 author = {Razavi, Ali and van den Oord, Aaron and Vinyals, Oriol},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generating Diverse High-Fidelity Images with VQ-VAE-2},
 url = {https://proceedings.neurips.cc/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{ChoiM0E22,
  author    = {Kristy Choi and
               Chenlin Meng and
               Yang Song and
               Stefano Ermon},
  editor    = {Gustau Camps{-}Valls and
               Francisco J. R. Ruiz and
               Isabel Valera},
  title     = {Density Ratio Estimation via Infinitesimal Classification},
  booktitle = {International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2022, 28-30 March 2022, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {151},
  pages     = {2552--2573},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v151/choi22a.html},
  timestamp = {Fri, 20 May 2022 16:11:25 +0200},
  biburl    = {https://dblp.org/rec/conf/aistats/ChoiM0E22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{durkan2019,
 author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Spline Flows},
 url = {https://proceedings.neurips.cc/paper/2019/file/7ac71d433f282034e088473244df8c02-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{boffi2022,
  journal={arXiv:2206.04642},
  author = {Boffi, Nicholas M. and Vanden-Eijnden, Eric},
  title = {Probability flow solution of the {F}okker-{P}lanck equation},
  year = {2022}
}


@article{lee2022,
  author    = {Doyup Lee and
               Chiheon Kim and
               Saehoon Kim and
               Minsu Cho and
               Wook{-}Shin Han},
  title     = {Autoregressive Image Generation using Residual Quantization},
  journal   = {CoRR},
  volume    = {abs/2203.01941},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2203.01941},
  doi       = {10.48550/arXiv.2203.01941},
  eprinttype = {arXiv},
  eprint    = {2203.01941},
  timestamp = {Wed, 16 Mar 2022 16:39:52 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2203-01941.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
doucet2022annealed,
title={Annealed Importance Sampling meets Score Matching},
author={Arnaud Doucet and Will Sussman Grathwohl and Alexander G. de G. Matthews and Heiko Strathmann},
booktitle={ICLR Workshop on Deep Generative Models for Highly Structured Data},
year={2022},
url={https://openreview.net/forum?id=H43MpnN_vZ9}
}

@inproceedings{
brock2018large,
title={Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
author={Andrew Brock and Jeff Donahue and Karen Simonyan},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1xsqj09Fm},
}

@inproceedings{
bortoli2021diffusion,
title={Diffusion Schr\"odinger Bridge with Applications to Score-Based Generative Modeling},
author={Valentin De Bortoli and James Thornton and Jeremy Heng and Arnaud Doucet},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=9BnCwiXB0ty}
}

@inproceedings{
liu2022,
title={Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},
author={Xingchao Liu and Chengyue Gong and Qiang Liu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=XVjTT1nw5z}
}

@misc{liu2022-ot,
  doi = {10.48550/ARXIV.2209.14577},
  
  url = {https://arxiv.org/abs/2209.14577},
  
  author = {Liu, Qiang},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Rectified Flow: A Marginal Preserving Approach to Optimal Transport},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{cifar10,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {2009},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}

@INPROCEEDINGS{imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  doi={10.1109/CVPR.2009.5206848}}
  
  
 @misc{downsampled-imagenet,
  doi = {10.48550/ARXIV.1707.08819},
  
  url = {https://arxiv.org/abs/1707.08819},
  
  author = {Chrabaszcz, Patryk and Loshchilov, Ilya and Hutter, Frank},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{benhamu2022,
  author    = {Heli Ben{-}Hamu and
               Samuel Cohen and
               Joey Bose and
               Brandon Amos and
               Maximilian Nickel and
               Aditya Grover and
               Ricky T. Q. Chen and
               Yaron Lipman},
  editor    = {Kamalika Chaudhuri and
               Stefanie Jegelka and
               Le Song and
               Csaba Szepesv{\'{a}}ri and
               Gang Niu and
               Sivan Sabato},
  title     = {Matching Normalizing Flows and Probability Paths on Manifolds},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {1749--1763},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/ben-hamu22a.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/Ben-HamuCBANGCL22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rozen2021moser,
title={Moser Flow: Divergence-based Generative Modeling on Manifolds},
author={Noam Rozen and Aditya Grover and Maximilian Nickel and Yaron Lipman},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=qGvMv3undNJ}
}



@inproceedings{oord2016,
author = {Van Den Oord, A\"{a}ron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
title = {Pixel Recurrent Neural Networks},
year = {2016},
publisher = {JMLR.org},
abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1747–1756},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}




@InProceedings{parmar2018,
  title = 	 {Image Transformer},
  author =       {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4055--4064},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/parmar18a/parmar18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/parmar18a.html},
  abstract = 	 {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.}
}


@inproceedings{kingma2018,
 author = {Kingma, Durk P and Dhariwal, Prafulla},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
 url = {https://proceedings.neurips.cc/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf},
 volume = {31},
 year = {2018}
}


@inproceedings{
kingma2021on,
title={On Density Estimation with Diffusion Models},
author={Diederik P Kingma and Tim Salimans and Ben Poole and Jonathan Ho},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=2LdBqxc1Yv}
}


@InProceedings{kim2022,
  title = 	 {Soft Truncation: A Universal Training Technique of Score-based Diffusion Model for High Precision Score Estimation},
  author =       {Kim, Dongjun and Shin, Seungjae and Song, Kyungwoo and Kang, Wanmo and Moon, Il-Chul},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {11201--11228},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/kim22i/kim22i.pdf},
  url = 	 {https://proceedings.mlr.press/v162/kim22i.html},
  abstract = 	 {Recent advances in diffusion models bring state-of-the-art performance on image generation tasks. However, empirical results from previous research in diffusion models imply an inverse correlation between density estimation and sample generation performances. This paper investigates with sufficient empirical evidence that such inverse correlation happens because density estimation is significantly contributed by small diffusion time, whereas sample generation mainly depends on large diffusion time. However, training a score network well across the entire diffusion time is demanding because the loss scale is significantly imbalanced at each diffusion time. For successful training, therefore, we introduce Soft Truncation, a universally applicable training technique for diffusion models, that softens the fixed and static truncation hyperparameter into a random variable. In experiments, Soft Truncation achieves state-of-the-art performance on CIFAR-10, CelebA, CelebA-HQ $256\times 256$, and STL-10 datasets.}
}

@InProceedings{Nilsback06,
  author       = "Maria-Elena Nilsback and Andrew Zisserman",
  title        = "A Visual Vocabulary for Flower Classification",
  booktitle    = "IEEE Conference on Computer Vision and Pattern Recognition",
  volume       = "2",
  pages        = "1447--1454",
  year         = "2006",
}


@InProceedings{nichol2021,
  title = 	 {Improved Denoising Diffusion Probabilistic Models},
  author =       {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8162--8171},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nichol21a.html},
  abstract = 	 {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.}
}


@inproceedings{tong_trajectorynet_2020,
  title = 	 {{T}rajectory{N}et: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics},
  author =       {Tong, Alexander and Huang, Jessie and Wolf, Guy and Van Dijk, David and Krishnaswamy, Smita},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9526--9536},
  year = 	 {2020},
  editor = 	 {Hal DaumÃ© III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/tong20a/tong20a.pdf},
  url = 	 {
http://proceedings.mlr.press/v119/tong20a.html
},
  abstract = 	 {It is increasingly common to encounter data in the form of cross-sectional population measurements over time, particularly in biomedical settings. Recent attempts to model individual trajectories from this data use optimal transport to create pairwise matchings between time points. However, these methods cannot model non-linear paths common in many underlying dynamic systems. We establish a link between continuous normalizing flows and dynamic optimal transport to model the expected paths of points over time. Continuous normalizing flows are generally under constrained, as they are allowed to take an arbitrary path from the source to the target distribution. We present \emph{TrajectoryNet}, which controls the continuous paths taken between distributions. We show how this is particularly applicable for studying cellular dynamics in data from single-cell RNA sequencing (scRNA-seq) technologies, and that TrajectoryNet improves upon recently proposed static optimal transport-based models that can be used for interpolating cellular distributions.}
}

@inproceedings{
lipman2022,
title={Flow Matching for Generative Modeling},
author={Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matthew Le},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=PqvMRDCJT9t}
}



@Article{maoutsa2020,
AUTHOR = {Maoutsa, Dimitra and Reich, Sebastian and Opper, Manfred},
TITLE = {Interacting Particle Solutions of Fokker–Planck Equations Through Gradient–Log–Density Estimation},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {802},
URL = {https://www.mdpi.com/1099-4300/22/8/802},
PubMedID = {33286573},
ISSN = {1099-4300},
ABSTRACT = {Fokker&ndash;Planck equations are extensively employed in various scientific fields as they characterise the behaviour of stochastic systems at the level of probability density functions. Although broadly used, they allow for analytical treatment only in limited settings, and often it is inevitable to resort to numerical solutions. Here, we develop a computational approach for simulating the time evolution of Fokker&ndash;Planck solutions in terms of a mean field limit of an interacting particle system. The interactions between particles are determined by the gradient of the logarithm of the particle density, approximated here by a novel statistical estimator. The performance of our method shows promising results, with more accurate and less fluctuating statistics compared to direct stochastic simulations of comparable particle number. Taken together, our framework allows for effortless and reliable particle-based simulations of Fokker&ndash;Planck equations in low and moderate dimensions. The proposed gradient&ndash;log&ndash;density estimator is also of independent interest, for example, in the context of optimal control.},
DOI = {10.3390/e22080802}
}



@InProceedings{Bakry1985,
author="Bakry, D.
and {\'E}mery, M.",
editor="Az{\'e}ma, Jacques
and Yor, Marc",
title="Diffusions hypercontractives",
booktitle="S{\'e}minaire de Probabilit{\'e}s XIX 1983/84",
year="1985",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="177--206",
isbn="978-3-540-39397-9"
}

@article{OTTO2000361,
title = {Generalization of an Inequality by Talagrand and Links with the Logarithmic Sobolev Inequality},
journal = {Journal of Functional Analysis},
volume = {173},
number = {2},
pages = {361-400},
year = {2000},
issn = {0022-1236},
doi = {https://doi.org/10.1006/jfan.1999.3557},
url = {https://www.sciencedirect.com/science/article/pii/S0022123699935577},
author = {F. Otto and C. Villani},
abstract = {We show that transport inequalities, similar to the one derived by M. Talagrand (1996, Geom. Funct. Anal.6, 587–600) for the Gaussian measure, are implied by logarithmic Sobolev inequalities. Conversely, Talagrand's inequality implies a logarithmic Sobolev inequality if the density of the measure is approximately log-concave, in a precise sense. All constants are independent of the dimension and optimal in certain cases. The proofs are based on partial differential equations and an interpolation inequality involving the Wasserstein distance, the entropy functional, and the Fisher information.}
}

@article{Kim2010AGO,
  title={A generalization of Caffarelli’s contraction theorem via (reverse) heat flow},
  author={Young-Heon Kim and Emanuel Milman},
  journal={Mathematische Annalen},
  year={2010},
  volume={354},
  pages={827-862}
}


@article{hyvarinen05a,
  author  = {Aapo Hyv{{\"a}}rinen},
  title   = {Estimation of Non-Normalized Statistical Models by Score Matching},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {24},
  pages   = {695--709},
  url     = {http://jmlr.org/papers/v6/hyvarinen05a.html}
}

@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}




%%%%%%%%%%%%%%%% schrodinger bridge citations %%%%%%%%%%%%%%%%%%%%

@inproceedings{
liu2023learning,
title={Learning Diffusion Bridges on Constrained Domains},
author={Xingchao Liu and Lemeng Wu and Mao Ye and qiang liu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=WH1yCa0TbB}
}

@inproceedings{
su2023dual,
title={Dual Diffusion Implicit Bridges for Image-to-Image Translation},
author={Xuan Su and Jiaming Song and Chenlin Meng and Stefano Ermon},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=5HLoTvVGDe}
}


@misc{chen2014,
  doi = {10.48550/ARXIV.1412.4430},
  
  url = {https://arxiv.org/abs/1412.4430},
  
  author = {Chen, Yongxin and Georgiou, Tryphon and Pavon, Michele},
  
  keywords = {Systems and Control (eess.SY), Mathematical Physics (math-ph), Optimization and Control (math.OC), Probability (math.PR), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Physical sciences, FOS: Physical sciences, FOS: Mathematics, FOS: Mathematics, 93E20},
  
  title = {On the relation between optimal transport and Schrödinger bridges: A stochastic control viewpoint},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{chen2021,
author = {Chen, Yongxin and Georgiou, Tryphon T. and Pavon, Michele},
title = {Stochastic Control Liaisons: Richard Sinkhorn Meets Gaspard Monge on a Schrödinger Bridge},
journal = {SIAM Review},
volume = {63},
number = {2},
pages = {249-313},
year = {2021},
doi = {10.1137/20M1339982},

URL = { 
    
        https://doi.org/10.1137/20M1339982
},
eprint = { 
    
        https://doi.org/10.1137/20M1339982

}
,
    abstract = { In 1931--1932, Erwin Schrödinger studied a hot gas Gedankenexperiment (an instance of large deviations of the empirical distribution). Schrödinger's problem represents an early example of a fundamental inference method, the so-called maximum entropy method, having roots in Boltzmann's work and being developed in subsequent years by Jaynes, Burg, Dempster, and Csiszár. The problem, known as the Schrödinger bridge problem (SBP) with “uniform" prior, was more recently recognized as a regularization of the Monge--Kantorovich optimal mass transport (OMT) problem, leading to effective computational schemes for the latter. Specifically, OMT with quadratic cost may be viewed as a zero-temperature limit of the problem posed by Schrödinger in the early 1930s. The latter amounts to minimization of Helmholtz's free energy over probability distributions that are constrained to possess two given marginals. The problem features a delicate compromise, mediated by a temperature parameter, between minimizing the internal energy and maximizing the entropy. These concepts are central to a rapidly expanding area of modern science dealing with the so-called Sinkhorn algorithm, which appears as a special case of an algorithm first studied in the more challenging continuous space setting by the French analyst Robert Fortet in 1938--1940 specifically for Schrödinger bridges. Due to the constraint on end-point distributions, dynamic programming is not a suitable tool to attack these problems. Instead, Fortet's iterative algorithm and its discrete counterpart, the Sinkhorn iteration, permit computation of the optimal solution by iteratively solving the so-called Schrödinger system. Convergence of the iteration is guaranteed by contraction along the steps in suitable metrics, such as Hilbert's projective metric. In both the continuous as well as the discrete time and space settings, stochastic control provides a reformulation of and a context for the dynamic versions of general Schrödinger bridge problems and of their zero-temperature limit, the OMT problem. These problems, in turn, naturally lead to steering problems for flows of one-time marginals which represent a new paradigm for controlling uncertainty. The zero-temperature problem in the continuous-time and space setting turns out to be the celebrated Benamou--Brenier characterization of the McCann displacement interpolation flow in OMT. The formalism and techniques behind these control problems on flows of probability distributions have attracted significant attention in recent years as they lead to a variety of new applications in spacecraft guidance, control of robot or biological swarms, sensing, active cooling, and network routing as well as in computer and data science. This multifaceted and versatile framework, intertwining SBP and OMT, provides the substrate for the historical and technical overview of the field given in this paper. A key motivation has been to highlight links between the classical early work in both topics and the more recent stochastic control viewpoint, which naturally lends itself to efficient computational schemes and interesting generalizations. }
}


@misc{khrulkov2023,
  doi = {10.48550/ARXIV.2202.07477},
  
  url = {https://arxiv.org/abs/2202.07477},
  
  author = {Khrulkov, Valentin and Ryzhakov, Gleb and Chertkov, Andrei and Oseledets, Ivan},
  
  keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Numerical Analysis (math.NA), Analysis of PDEs (math.AP), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Understanding DDPM Latent Codes Through Optimal Transport},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}






@misc{zhang2018,
  doi = {10.48550/ARXIV.1809.10188},
  
  url = {https://arxiv.org/abs/1809.10188},
  
  author = {Zhang, Linfeng and E, Weinan and Wang, Lei},
  
  keywords = {Machine Learning (cs.LG), Statistical Mechanics (cond-mat.stat-mech), Dynamical Systems (math.DS), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Monge-Ampère Flow for Generative Modeling},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@ARTICLE{Yang2022,
  author={Yang, Liu and Karniadakis, George Em},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Potential Flow Generator With L2 Optimal Transport Regularity for Generative Models}, 
  year={2022},
  volume={33},
  number={2},
  pages={528-538},
  doi={10.1109/TNNLS.2020.3028042}}


@inproceedings{dockhorn2022score,
    title={Score-Based Generative Modeling with Critically-Damped Langevin Diffusion},
    author={Tim Dockhorn and Arash Vahdat and Karsten Kreis},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2022}
}


@inproceedings{Karras2022edm,
  author    = {Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},
  title     = {Elucidating the Design Space of Diffusion-Based Generative Models},
  booktitle = {Proc. NeurIPS},
  year      = {2022}
}

@article{hoogeboom2023simple,
  title={simple diffusion: End-to-end diffusion for high resolution images},
  author={Hoogeboom, Emiel and Heek, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2301.11093},
  year={2023}
}


@inproceedings{
liu2023learning,
title={Learning Diffusion Bridges on Constrained Domains},
author={Xingchao Liu and Lemeng Wu and Mao Ye and qiang liu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=WH1yCa0TbB}
}


@misc{somnath2023aligned,
      title={Aligned Diffusion Schr\"odinger Bridges}, 
      author={Vignesh Ram Somnath and Matteo Pariset and Ya-Ping Hsieh and Maria Rodriguez Martinez and Andreas Krause and Charlotte Bunne},
      year={2023},
      eprint={2302.11419},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
chen2022likelihood,
title={Likelihood Training of Schr\"odinger Bridge using Forward-Backward {SDE}s Theory},
author={Tianrong Chen and Guan-Horng Liu and Evangelos Theodorou},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nioAdKCEdXB}
}