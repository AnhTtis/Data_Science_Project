In recent years, deep learning (DL) methods have significantly improved the computer-assisted diagnosis of chest X-ray (CXR) images. DL methods have been used in multiple ways. For instance, Ma and Lv~\cite{Pneumonia} used a Swin transformer with fully-connected layers to classify CXR images as either normal or indicative of pneumonia. Cicero et al.~\cite{Cicero2017TrainingAV} modeled the diagnosis process as a multi-label classification problem and used GoogleNet to classify CXR images into six categories. In comparison to these classification methods, generated free-text radiology reports can provide more comprehensive information about the impression and findings~\cite{radgraph}. Shin et al.\cite{LearnToRead} pioneered CNN-RNN for automatic text generation. Wang et al.\cite{TieNet} introduced TieNet, generating reports and detecting thorax diseases. Hou et al.\cite{RATCHET} proposed RATCHET, using transformer and attention to generate reports from CXR images. Kaur et al.\cite{kaur2022radiobert} used contextual word representations for useful radiological reports. Cao et al.\cite{cao2023mmtn} proposed Multi-modal Memory Transformer Network for consistent medical reports. Existing approaches for automating medical reporting rely on generating free text, posing challenges for clinical evaluation~\cite{pino2021clinically,few_shot}. To tackle this, ImaGenome~\cite{wu2021chest} and RadGraph Benchmark\cite{radgraph} were introduced to extract structured clinical information from free-text radiology reports, represented as a radiology graph. Each node in a radiology graph corresponds a unique entity, such as an object with bounding boxes annotations or an attribute in ImaGenome or, an anatomical structure or an observation and its presence and uncertainty in RadGraph Benchmark. Although these graph representations of reports have been used to evaluate the clinical correctness of reports~\cite{yu2022evaluating}, generating radiology graphs directly from CXR images has not been explored. In contrast, diverse interactions between object pairs in natural images in the form of scene graph generation have been extensively investigated \cite{lu2016visual}. Li et al.~\cite{Li_2022_CVPR}, and Lu et al.~\cite{lu2021seq2seq} employed two-stage methods to propose dense relationships between predicted connected object pairs. In recent work, Shit et al.~\cite{relationformer} introduced Relationformer, a unified one-stage framework based on DETR \cite{detr} that facilitates the end-to-end generation of graphs from images. It is a state-of-the-art method for detecting and generating graphs from natural images. However, it requires bounding boxes for the detected objects (nodes of the graph) and is not directly applicable to radiology graphs since some nodes (entities) in radiology graphs do not have exact locations, such as ``left`` or ``clear``, and most datasets do not provide bounding boxes annotations.\looseness=-1

Therefore, we propose a detection-free method, Prior-RadGraphFormer, to generate radiology graphs directly from CXR images without requiring bounding boxes for each entity. The method incorporates prior knowledge in the form of probabilistic knowledge graphs (PKG) \cite{cls_by_attention} that model the statistical relationship between anatomies and pathological observations. Experimental results show that Prior-RadGraphFormer achieves competitive results in the radiology-graph-generation task. Moreover, the generated graphs can be used for multiple downstream tasks such as generating free-text reports based on predefined rules, cheXpert labels \cite{chexpert} classification, and populating templates for structured reporting.

In summary, our contributions are the following: 1) proposing a novel detection-free method that generates radiology graphs directly from CXR images; 2) enhancing this method by incorporating prior knowledge, leading to improved performance; 3) extensively evaluating our method using RadGraph metrics and the two downstream tasks of report generation and multi-label classification of pathologies.