\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{viz/figure1_crop.pdf}
    \caption{\textbf{Prior-RadGraphFormer architecture.} A CNN backbone extracts radiology image features, which are fed into the detection transformer (DETR)~\cite{detr} to generate multiple entity tokens and one relation token. Then an initial graph is constructed using entity and relation tokens. Each node is represented by a valid entity token and each edge is represented by a corresponding [entity-relation-entity]-token tuple. The initial graph is incorporated with prior knowledge and refined to output the final radiology graph.}
    \label{model_architecture}
\end{figure}
We define radiology graph generation as the process of transforming CXR images into a graph structure introduced by RadGraph Benchmark~\cite{radgraph} that represents the content of a radiology report describing the image. Each node in the graph corresponds to a unique entity, such as an anatomical structure or an observation, and its presence and uncertainty. The edges between nodes indicate relationships between these entities.
A visual depiction of the proposed method is shown in Fig.~\ref{model_architecture}. Prior-RadGraphFormer consists of two main components: RadGraphFormer and Prior Knowledge Integration. %




\subsection{RadGraphFormer}
Shit et al.~\cite{relationformer} proposed Relationformer, an image-to-graph framework that leverages direct set-based object prediction and incorporates the interaction among the objects to learn an object-relation representation jointly. Given an input image, Relationformer initially outputs a set of discrete object tokens ([obj]-tokens) and a relation token ([rln]-token) where the token embeddings come from applying cross attention between a set of pre-defined and randomly initialized token vectors to input image patches. Relationformer then predicts the corresponding class label and the location of the bounding boxes for each object. In addition, Relationformer predicts a relation label for each pair of detected objects by concatenating each pair of [obj]-tokens with the [rln]-token and applying a relation prediction head on top.  

In our model, we adapt Relationformer as the radiology graph generation
backbone. We modify the entity prediction module by replacing the bounding box prediction head of it with the uncertainty prediction head due to the fact that radiology graphs do not contain bounding boxes but uncertainty information for each entity. Specifically, the entity prediction module comprises two distinct components. The first component is responsible for entity classification. The second component is responsible for predicting the uncertainty associated with each entity. Both are presented by one linear layer. To reflect these changes in terminology, we adjust the naming convention of the model's tokens, from [obj]-token to [ent]-token. Moreover, instead of using ResNet50 as the CNN backbone, we utilize
DenseNet ~\cite{densenet} based on its widespread adoption and effectiveness for processing
CXR images~\cite{RATCHET,few_shot}.






\subsection{Prior Knowledge Integration}
Vanilla RadGraphFormer uses the concatenation of a pair of [ent]-tokens and a shared [rln]-token $(\left\{ ent_{i},r,ent_{j} \right\}_{i \neq j})$, followed by an MLP, for relation prediction. Although the transformer-based model inherently considers the context of the tokens, we argue that these representations may not adequately capture the complexity of the relationships. Consequently, this limited representation may lead to an incomplete or inaccurate understanding of the context, resulting in sub-optimal relation and/or entity prediction.

Following~\cite{cls_by_attention}, in order to address this issue, we propagate higher-level prior knowledge in RadGraphFormer, as shown in the lower part of Fig.~\ref{model_architecture}. To this end, we first construct an initial embedding graph $G$ from all the valid $ent_{i}$s as nodes. Each edge in the graph is represented by $edg_{ij} = MLP_{proj}(\left\{ ent_{i},r,ent_{j} \right\}_{i \neq j})$, where $MLP_{proj}$ helps project the concatenated edge features to the same dimension of node features. This results in a fully-connected bi-directional graph. Then a stack of graph transformers $GTs$~\cite{graph_transformer} are utilized to propagate features of both nodes and edges in $G$. In order to allow the storage and propagation of relational knowledge within our framework, we create a randomly initialized representation for each class as a trainable parameter $s_c$ called schemata~\cite{cls_by_attention} that interacts with the outputs of the graph transformer. We then apply multiple assimilation steps~\cite{cls_by_attention}. The assimilation step is a process where the outputs of $GT$ attend to $s_c$s such that the attention coefficients predict the classification outputs for each node/edge from the $GT$ and the attention values are propagated to the output of $GT$. It is important to note that the attention coefficients are supervised by the ground truth labels for each entity/relation during the training.\looseness=-1







\subsection{Training and Inference.} During training, we apply two supervisions for entity classes. One is for [ent]-tokens generated from DETR and the other is for the attention coefficients of nodes during each assimilation step. The latter one can be regarded as an additional entity class supervision. Entity uncertainty is supervised via [ent]-tokens and relation is supervised via the attention coefficients of edges during each assimilation step. During inference, we evaluate entity metrics solely based on [ent]-tokens. As for relation inference, we use the attention coefficients of edges from the last assimilation step as the classification output.

