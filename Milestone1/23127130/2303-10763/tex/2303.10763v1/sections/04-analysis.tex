\section{Comparing Different Techniques}
\label{sec:analysis}

The second part of our work consists in taking both versions of \dataset and comparing the existing tools on them. This can show us which tools work the best specifically in the setting of a programming contest, and also point the way to possible further improvements. 

\subsubsection{Metrics}

Firstly, we need to select the metrics for the comparison. As the basics, we can use \textit{precision} and \textit{recall}, two of the standard metrics used in software engineering research. At the same time, we need some other metric that would take into account the balance of precision and recall. One single main metric is necessary not only to make some conclusions about the performance of the tools, but also to be used for training the parameters of the tools.

To find the best trade-off between precision and recall, researchers typically use F1 score, \textit{i.e.}, the harmonic mean between their values. However, in our task, these two metrics are not actually completely equal. In real contests, declaring the solution as being plagiarized is a very serious decision, since this inevitably leads to severe consequences. Therefore, any cases of automatic plagiarism detection will be necessarily checked manually, and so finding all potential cases is more important. Luckily, F-score is actually a more general metric:

$$F_\beta = (1 + \beta^2) \cdot \frac{Precision \cdot Recall}{\beta^2 \cdot Precision + Recall}$$

\noindent where $\beta =$ 1 corresponds to the traditional F1 metric, $\beta <$ 1 favors precision, and $\beta >$ 1 favors recall. Since in our case, the recall is more important, we decided to use $\beta =$ 1.5. Further on, we will denote this metric as $F_{1.5}$.

\subsubsection{Approaches}

In our comparison, we used the same six main tools that were tested for the preliminary filtering of the dataset and that were described in Section~\ref{sec:background}. Since these tools are of different types, this comparison can show us which perform the best for programming contests and how much the template code affects them. 

Each tool under investigation has its own parameters that can be tuned. A lot of research in the adjacent area of clone detection indicates that the default parameters of clone detection tools are not actually the best ones~\cite{wang2013searching, golubev2021multi}. Therefore, to ensure the best performance for each tool, its different configurations were considered. The chosen configurations are similar to those used in the recent comparison~\cite{dolos2022}, there was a total of more than 400 configurations. You can find their full list, as well as the best ones, in the replication package~\cite{dataset}.

\subsubsection{Methodology}

In order to choose the best configuration for each tool, the dataset was divided into train (230 pairs) and test (681 pairs) sets. This division of the dataset is motivated by the fact that with the smaller size of the test dataset, it will be more difficult to find significant differences in the performance of the source code plagiarism detection tools.

Firstly, we ran all the tools on the train set. This means running each tool in each of the configurations with all possible similarity thresholds from the range \texttt{[0, 100]}. The best configuration of each tool was deemed the one that reached the maximum value of $F_{1.5}$. This search was carried out independently for two versions of the dataset, obtaining the best configurations separately for them. 

Then, we compared the best configurations of tools on the test set, also separately for the ``raw'' and the ``template-free'' versions of the dataset, thus allowing us to see the differences between them. We used bootstrapping~\cite{diciccio1996bootstrap} to build confidence intervals for the used metrics. Specifically, for each tool, we built 10,000 sub-samples with the same size as the test set, ran the tool on all of them, and then used the distribution of the metric to build a 95\% confidence interval. 

\subsubsection{Results}

Table~\ref{table:results} shows the results for both versions of \dataset. Firstly, we can see that the best results on both datasets are demonstrated by token-based tools. On the raw version of the dataset, JPlag demonstrated the best results in terms of the $F_{1.5}$ metric (0.77), however, MOSS and SIM are close to it (0.72 and 0.72, respectively). On the template-free dataset, JPlag also won (0.80), and all the four token-based tools demonstrate good results. 
As for the text-based tool Sherlock, in both versions of the dataset, it demonstrated good recall (0.76 and 0.81, respectively), but very low precision (0.34 and 0.39, respectively). While recall is in general more important to us than precision, such an extreme difference ceases to be useful. Interestingly, the graph-based tool BPlag also demonstrated results worse than token-based tools ($F_{1.5}$ of 0.55 on the raw dataset). Our investigation showed that it is too sensitive for programming contests --- a lot of pairs that have large similar chunks of code get counted as plagiarized.

\begin{table}[t]
\centering
\begin{tabular}{c  c  c  c } 
 \toprule
 \textbf{Tool}     & \textbf{Precision}          & \textbf{Recall}            & $\mathbf{F_{1.5}}$      \\ \midrule\midrule
 \multicolumn{4}{c}{\textbf{Raw dataset}} \\
 \midrule
 \textbf{Sherlock}  &  0.34 \stats{($\pm 0.05$)} & 0.76 \stats{($\pm 0.06$)} & 0.55 \stats{($\pm 0.05$)}  \\
 \textbf{SIM}       &  0.69 \stats{($\pm 0.06$)} & 0.74 \stats{($\pm 0.06$)} & 0.72 \stats{($\pm 0.05$)}  \\
 \textbf{MOSS}      &  \textbf{0.77} \stats{($\pm 0.06$)} & 0.71 \stats{($\pm 0.06$)} & 0.72 \stats{($\pm 0.05$)}  \\
 \textbf{Dolos}     &  0.68 \stats{($\pm 0.06$)} & 0.65 \stats{($\pm 0.07$)} & 0.66 \stats{($\pm 0.06$)}  \\
 \textbf{JPlag}     &  0.66 \stats{($\pm 0.06$)} & \textbf{0.83} \stats{($\pm 0.05$)} & \textbf{0.77} \stats{($\pm 0.05$)}  \\
 \textbf{BPlag}     &  0.45 \stats{($\pm 0.06$)} & 0.61 \stats{($\pm 0.07$)} & 0.55 \stats{($\pm 0.06$)}   \\ \midrule
 \multicolumn{4}{c}{\textbf{Template-free dataset}} \\
 \midrule
 \textbf{Sherlock} &   0.39 \stats{($\pm 0.05$)} &  0.81 \stats{($\pm 0.05$)} & 0.60 \stats{($\pm 0.05$)} \\ 
 \textbf{SIM}      &   0.73 \stats{($\pm 0.06$)} &  0.75 \stats{($\pm 0.06$)} & 0.74 \stats{($\pm 0.05$)} \\
 \textbf{MOSS}     &   0.66 \stats{($\pm 0.06$)} &  0.81 \stats{($\pm 0.06$)} & 0.75 \stats{($\pm 0.05$)} \\
 \textbf{Dolos}    &   0.72 \stats{($\pm 0.06$)} &  0.83 \stats{($\pm 0.05$)} & 0.79 \stats{($\pm 0.04$)} \\
 \textbf{JPlag}    &   \textbf{0.75} \stats{($\pm 0.06$)} &  0.83 \stats{($\pm 0.05$)} & \textbf{0.80} \stats{($\pm 0.04$)} \\
 \textbf{BPlag}    &   0.52 \stats{($\pm 0.06$)} &  \textbf{0.87} \stats{($\pm 0.05$)} & 0.72 \stats{($\pm 0.04$)} \\ \bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{Results of comparing the best configurations of tools on the test set of \dataset, in two versions. Numbers in the brackets indicate 95\% confidence intervals.}
\vspace{-0.9cm}
\label{table:results}
\end{table}

Finally, the results show that the removal of the template code increases the performance of the studied tools. The difference is especially noticeable for Dolos (from $F_{1.5}$ of 0.66 to 0.79) and BPlag (from $F_{1.5}$ of 0.55 to 0.72). Without templates, both Dolos and BPlag greatly improve their recall, because they get less triggered by large similar pieces of code. These results indicate that the presence of the template code is indeed a serious challenge in correctly detecting plagiarism in programming contests.