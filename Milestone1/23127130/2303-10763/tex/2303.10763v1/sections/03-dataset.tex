\section{Dataset}
\label{sec:dataset}

When comparing source code plagiarism detection tools, researchers use (a) solutions to student assignments~\cite{jplag2003}, (b) synthetic augmentations of the programs~\cite{bplag2021}, or (c) manually changed code~\cite{sim1999}. 
To compare the tools in programming competitions, we decided to collect a dataset of pairs consisting exclusively of solutions to problems from real contests. 
The overall pipeline for collecting \dataset is presented in Figure~\ref{fig:pipeline}, let us now describe each step in greater detail.

\subsubsection{Gathering Data}

CodeForces~\cite{codeforces} is an online platform that hosts competitive programming contests.
The solution to the problem is a single-file program written in any popular language (C++, Python, Java, etc.). We chose CodeForces because it was used in previous research~\cite{majd2019code4bench, lobanov2023predicting}.

To create our dataset, we selected tasks on the platform based on the following two criteria: (a) the solutions must be large enough so that there are not many false positives when comparing them, and (b) there must be at least a hundred Java solutions for this task, otherwise the probability of plagiarism is very low. We collected a dataset of Java submissions because Java is the most popular language in plagiarism research and is the only language supported by all the tools available to us. 

Based on these criteria, we downloaded all successful Java submissions for 21 different problems and left only those that were successfully parsed by all the six studied tools~\cite{sherlock, sim1999, moss2003, dolos2022, jplag2003, bplag2021}, for a total of 4,695 submissions. Finally, for each problem, we built a set of all possible pairs of its solutions. This resulted in a total of 125,481 pairs.

\subsubsection{Excluding Trivial Non-plagiarized Code}

The constructed set of pairs is not feasible to manually label, and even more importantly, on the vast majority of these pairs, all plagiarism detection tools will return small values of similarity, which is true because the majority of contestants actually do not cheat. 
To overcome these difficulties, we decided to use the existing plagiarism detection tools, not to \textit{label} the data (since the idea is to obtain a manually-curated dataset) but to \textit{filter out} ``trivially'' non-plagiarized pairs. In particular, we used six plagiarism detection tools: Sherlock~\cite{sherlock}, SIM~\cite{sim1999}, MOSS~\cite{moss2003}, Dolos~\cite{dolos2022}, JPlag~\cite{jplag2003}, and BPlag~\cite{bplag2021}. The motivation behind using all the tools at once is to minimize the bias of the resulting dataset towards any one algorithm.

We took a sample of 1,000 random pairs and manually labeled them as either plagiarism or not. The labeling in this pilot work was carried out by the first author, who has 5 years of experience in competitive programming. In the future editions of the dataset, we plan to update it with more examples labeled by multiple experts. When the sample was labeled, we ran all the selected tools on these pairs. This way, for each tool, the \textit{minimum similarity} for the plagiarized pair was found. This threshold does not indicate that all the pairs above it are actually plagiarized, it merely indicates that \textit{all pairs below this threshold are not}. This resulted in 60\% for BPlag, 17\% for Dolos, and 0 for other four, meaning that there exists actual plagiarism, for which these tool show a score of 0. From these results, one can see that only BPlag can be used for effective filtering.

To ensure the reliability of the obtained threshold, we took another 1,000 random pairs, and the first author manually labeled them. Among the selected pairs, only 3 plagiarized pairs were detected, for which BPlag demonstrated less than 60\%, which indicates that this threshold allows us to successfully filter out simplistic cases.
Having obtained and evaluated the threshold, we ran BPlag on all the pairs and filtered out trivially non-plagiarized code. This removed approximately 57\% of the pairs, and left us only with ``interesting'' pairs.

\subsubsection{Building the Dataset of Pairs}

Next, we carried out the final manual labeling of the dataset. We took 1,700 pairs from the remaining set, and they were once again labeled by the first author. Overall, the labeling resulted in 251 cases of plagiarism and 660 cases of non-plagiarism. For the remaining 789 pairs, it was problematic to label them unambiguously, so we excluded such pairs.

\subsubsection{Dealing with Templates}

As discussed in Section~\ref{sec:background}, templates are an important and difficult feature of contest plagiarism. Therefore, for a more informative comparison, we decided to create a separate version of the dataset without the template code. Such a comparison would be further away from the real-world scenario, but would allow us to see which tools suffer the most from the templates.

After manually investigating many submissions, we found that all the template code can be generally classified into two groups: (a) fast input-output (I/O) and (b) the implementation of popular algorithms. We noticed that most of the algorithmic template code in the solutions is not used, so such code can be safely removed. However, the code for fast I/O is actually used by the contestant, so it can not be simply deleted, because some tools require the code to be compilable. In particular, this is important for graph-based tools like BPlag~\cite{bplag2021}, which requires the declaration of all the used functions. To deal with this, we generated a separate JAR-file with all such classes present, and in which BPlag could see the declarations of fast I/O methods. After this, we could safely remove the code responsible for fast I/O from each solution. 
The deletion of template code, both algorithmic and responsible for fast I/O, was done manually for each solution in each pair. 

\subsubsection{Dataset Characteristics}

Finally, we obtained a dataset of source code plagiarism in programming contests called \dataset that has a total of 911 pairs that were all manually labeled: 251 plagiarized pairs and 660 non-plagiarized pairs. Additionally, \dataset is presented in two different versions, meaning that there are two different versions of each solution: (a) \textbf{raw} --- each program in its original form, and (b) \textbf{template-free} --- with most template code removed.
\dataset is available online~\cite{dataset}, and comes with an easy-to-use CLI wrapping and detailed instructions. The evaluation of any tool can be done in two simple steps. Firstly, a special Python class should be created that will run the tool on each pair of the desired version of the dataset. Secondly, after the results are obtained, a separate script will compare them with the labeling and output the report, containing different metrics for the evaluated tools. 