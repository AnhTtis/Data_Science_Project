\section{Introduction}

In recent years, competitive programming contests have become increasingly popular~\cite{codeforces_report}. In such contests, participants solve different algorithmic problems, constrained by certain time and memory limits, and are judged by the speed of solving and the operation time of their solutions. 
Unfortunately, as the popularity of programming contests grows, so does the number of cases of \textit{plagiarism}~\cite{contest_warsaw2019}. 
To ensure the fairness of the contest and to enforce the rules of contest platforms, such cases need to be detected.

Unlike other \textit{semantic clones}~\cite{ain2019systematic}, plagiarized code often reuses particular popular transformations to hide the similarities~\cite{comparison2009}. Currently, there exist several tools based on different approaches to detect plagiarism in the source code~\cite{sherlock, sim1999, moss2003, dolos2022, jplag2003, bplag2021}, however, the majority of them are developed for detecting plagiarism in programming homeworks. 
At the same time, contest plagiarism has specific features that distinguish it from other kinds of plagiarism, the most important feature being the reuse of \textit{templates}. Template code in the context of competitive programming is the code that is written by the participant before the contest and copied between tasks in order to save time during the competition. 

Because of this, existing tools may perform worse on competitive plagiarism in comparison to other kinds. However, to the best of our knowledge, there is no single benchmark to compare the existing approaches in the setting of contest plagiarism. 
In this work, we strive to bridge the existing gaps in research, provide the community with the first version of curated dataset of specifically contest plagiarism, and compare available source-code plagiarism detection tools.

In the first part of our work, we curated the dataset. To do this, firstly, we selected 21 problems from the CodeForces platform~\cite{codeforces} and collected all 4,695 accepted Java submissions for them. We chose the Java language because it is the only programming language supported by the majority of plagiarism detection tools, thus allowing us to compare them in equal settings. For each problem, we constructed a set of all pairs from the submissions for this task. However, manually labelling a sample directly from all of the pairs will result in a lot of simple non-plagiarized examples, because the vast majority of submissions are not actually plagiarized. To overcome this problem, we decided to use the existing plagiarism detection tools based on different approaches~\cite{sherlock, sim1999, moss2003, dolos2022, jplag2003, bplag2021} to filter out \textit{trivially non-plagiarized} pairs.

The manual labeling demonstrated that five out of six tools sometimes give very low scores for actual plagiarism. Thus, we used the sixth tool, BPlag~\cite{bplag2021}, which demonstrated no plagiarized pairs below the similarity of 60\%. We filtered out all pairs below this threshold, leaving only ``interesting'' pairs, from which we selected a sample that was once again labeled manually. This resulted in a dataset called \dataset that contains 251 plagiarized pairs and 660 non-plagiarized ones.
Since an important feature of the contest submissions is template code, 
to have the opportunity to compare the tools in different settings, we manually removed templates from all the submissions, and share \dataset in two versions --- with and without them. \dataset is available online~\cite{dataset}.

In the second part of our work, we carry out the first pilot comparison of the existing approaches on both versions of \dataset. In this comparison, we used the same six main available tools as in filtering, since the final dataset only contained manually curated pairs: Sherlock~\cite{sherlock}, SIM~\cite{sim1999}, MOSS~\cite{moss2003}, Dolos~\cite{dolos2022}, JPlag~\cite{jplag2003}, and BPlag~\cite{bplag2021}. On the raw version of the dataset, the best results were obtained using token-based tools: JPlag, MOSS, and SIM. The analysis of the results demonstrates that some of their errors occurred due to the matching of the I/O code, similar in form between solutions. Also, unexpectedly low results were demonstrated by BPlag, the only graph-based algorithm. 
Finally, the approaches demonstrated better results when tested on the version of \dataset without templates, indicating that their removal is an integral part of developing further approaches.

We believe that \dataset and our pilot comparison is a valuable first step in solving the problem of plagiarism in programming contests that can be continued further. 
