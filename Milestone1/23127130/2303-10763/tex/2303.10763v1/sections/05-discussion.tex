\subsubsection{Discussion \& Implications}
\label{sec:discussion}

\textbf{Performance of tools.} The study of the existing tools on \dataset demonstrated that different tools show very different results. The most prominent result is that the graph-based tool BPlag turned out to be too sensitive for contest data, and demonstrated worse performance than simpler token-based tools. However, BPlag was able to detected certain non-trivial plagiarism cases that other tools missed. Future approaches might benefit from combining different kinds of code representation.

\textbf{Template code.} Another noticeable and expected result is that the performance of the tools improved on the version of the dataset without templates. For some tools, this change was very significant. This means that the automatic removal of templates can be considered as one of the most important tasks that can turn state-of-the-art plagiarism detection techniques into state-of-the-practice tools employed in real contests. 

\textbf{Repeating code.} Apart from the template code, solutions to the same task might have general repeated code, like reading or writing. A very important step in improving the quality of plagiarism detectors for contest code is being able to pay less attention to such code and more --- to the actual solution. A promising direction for further research would be automized ways of processing several solutions to the same task and marking the similar parts of the solution.

\textbf{Metric.} A traditional approach of locking a single threshold for the tool may not represent the real-life tool usage, where experts usually run the tool for each problem, get a list of pairs sorted by its similarity, and check pairs from the top of this list. This can be more representative when the tool has its own threshold for each problem. Ranking metrics from the information retrieval field might be a possible area of research. 