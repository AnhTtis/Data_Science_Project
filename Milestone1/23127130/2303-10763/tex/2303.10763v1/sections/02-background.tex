\section{Background}
\label{sec:background}

\subsubsection{Plagiarism in Programming Contests}

Source code plagiarism is a well-researched area of studies~\cite{marins2014survey}, however, the developed solutions are usually focused on finding plagiarism in homework assignments~\cite{novak2016academia, plugiarismreview2019},  
there are only a few works devoted to plagiarism in competitive programming~\cite{contest_bangladesh2021, contest_warsaw2019}. Even then, they are mainly focused on integrating a particular plagiarism detection tool into the online judging system, and not on comparing different existing tools. 

Unlike some other cases, source code in programming contests has its own specifics that directly relate to finding plagiarism in it. In addition to the usual plagiarism hiding techniques~\cite{comparison2009}, the solutions will have a lot of similar code that is natural for contests. Firstly, there is \textit{template} code, \textit{i.e.}, some common implementations of popular algorithms that the contestant copies into every solution. Secondly, there is code that will be almost the same in every solution to the given problem but not actually plagiarized, \textit{e.g.}, reading the input or printing the answer.
Currently, active research is underway on how to find similar code and reduce its influence on the similarity~\cite{common2016, common2020}.

The template code is particularly difficult to take into account, because it can be completely different for different contestants in both size and implementation. Also, functions from the template code can be actively used or ignored completely by the contestant, depending on a particular task, making it very difficult to automatically preprocess all submissions by simply removing all template code from them. It is clear that template code can easily become a weak spot for many algorithms, and this must be taken into account when using plagiarism detection tools on the contest code and when building a benchmark for their comparison.

\subsubsection{Source Code Plagiarism Detection Tools}

Many different tools were developed aimed at detecting source code plagiarism~\cite{plugiarismreview2019}. \textit{Text-based} algorithms treat a program as a simple text, without taking into account its programming language. The advantages of such approaches are language-independence and high performance~\cite{comparison2009}, however, this comes at the expense of lower accuracy. A popular text-based tool is Sherlock~\cite{sherlock}. This tool converts the file into a sequence of string tokens, hashes it, and extracts a subsample of hashes. To determine the similarity of two programs, Sherlock calculates the similarity of the sequences of hashes.

\textit{Token-based} algorithms run a specific lexer on the program and compare token streams. Such approaches are still fast but consider a deeper representation of the program. One of the earliest plagiarism detection tools, SIM~\cite{sim1999}, applies an algorithm for finding the maximum sequence alignment to the resulting token sequence of given programs. The similarity of two programs is then defined as their alignment score. Another popular token-based tool, JPlag~\cite{jplag2003}, defines the similarity as the percent of tokens from the first sequence that can be covered by tokens from the second one. A different token-based tool, MOSS~\cite{moss2003}, is based on comparing the fingerprints of programs. A fingerprint is constructed in three steps: (1) all the \textit{n}-grams for the token stream are built, (2) these \textit{n}-grams are hashed, and (3) to avoid comparing big sets of hashes, MOSS uses a \textit{winnowing} algorithm to select a certain subset of hashes for each program. The idea of winnowing has got popular, and several tools appeared that are based on it. One such tool is Dolos~\cite{dolos2022}. Unlike MOSS, Dolos is open-source, supports more programming languages, and provides powerful visualizations of the results.

Finally, \textit{graph-based} algorithms build a graph (usually, a program dependence graph) of the program, which shows the dependencies of the data within the program. To avoid the naive solving of an NP-hard problem, they use certain heuristics. For example, BPlag~\cite{bplag2021} uses the idea of a Greedy-String-Tiling algorithm to find similar parts in the graphs of two programs.

Overall, it can be seen that there exist a lot of approaches for finding plagiarism in the source code, however, given the specifics of programming contests, it is not clear how well they perform in such a setting. To evaluate the existing tools, to help researchers further improve them for competitive programming, as well as to provide a benchmark for future solutions, in this work, we aim to collect the first dedicated dataset of programming contest plagiarism.

\begin{figure*}[htbp]
\centering
    \includegraphics[width=\textwidth]{figures/pipeline.pdf}
    \centering
    \vspace{-0.5cm}
    \caption{The pipeline of the proposed approach for collecting the dataset.}
    \label{fig:pipeline}
    \vspace{-0.5cm}
\end{figure*}