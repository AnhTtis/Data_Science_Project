%\documentclass[10pt,twocolumn,twoside]{IEEEtran}
%\documentclass[11pt,onecolumn,twoside,draftcls]{IEEEtran}
%\documentclass[5p]{elsarticle}
%\documentclass[preprint,11pt]{elsarticle}
\documentclass[1p,11pt]{elsarticle}
%\documentclass[journal]{IEEEtran}
%\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{bm}
\usepackage{color}
%\usepackage{cite}
\usepackage{subcaption}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}

\biboptions{sort&compress}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
%\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

\allowdisplaybreaks

%\journal{Signal Processing}

\begin{document}

\begin{frontmatter}

\title{Estimation of Scalar Field Distribution in the \\Fourier Domain} 

\author[1]{Alex S. Leong\corref{cor1}%
%\fnref{fn1}
}
\ead{alex.leong@defence.gov.au}
\author[2]{Alexei T. Skvortsov%\fnref{fn2}
}
\ead{alexei.skvortsov@defence.gov.au}

\cortext[cor1]{Corresponding author}

\affiliation{organization={Defence Science and Technology Group},
%addressline={506 Lorimer St},
city={Fishermans Bend},
postcode={Vic. 3207},
country={Australia}} 

%\maketitle

\begin{abstract}
In this paper we consider the problem of estimation of scalar field distribution collected from noisy measurements. The field is modelled as a sum of Fourier components/modes, where the number of modes retained and estimated determines in a natural way the approximation quality. An algorithm for estimating the modes using an online optimization approach is presented, under the assumption that the noisy measurements are quantized. The algorithm can estimate time-varying fields through the introduction of a forgetting factor. Simulation studies demonstrate the effectiveness of the proposed approach. 
\end{abstract}

\end{frontmatter}

\section{Introduction}

Estimation of scalar field distribution from a set of point measurements is an important problem often emerging in ecology, geophysics, and many technological applications. Examples include concentration of pollutant, radiation, temperature in urban areas, carbon dioxide emission, methane sources, and many others, see 
\cite{HutchinsonOh,HutchinsonLiu,NeumannBennetts_advanced_robotics,ThomsonHirst,RisticMorelandeGunatilaka,Selvaratnam_CDC,EslingerMendez,NewazJeong,WeidmannHirst,LiChen,MartinPayton,LaSheng,LaShengChen,MorelandeSkvortsov,RazakSukumarChung_journal,LeongZamani_SP,LeongZamaniShames,TranGarratt} and references therein. This approach is often used for indirect inference of scalar fields (pressure, temperature, radiation)  in inaccessible locations where the direct measurements are prohibited due to some geometrical or physical constraints (blocking obstacles, high temperature, or exposure to hazards). The methods of source localisation \cite{HutchinsonOh,HutchinsonLiu,NeumannBennetts_advanced_robotics,ThomsonHirst,RisticMorelandeGunatilaka,Selvaratnam_CDC,EslingerMendez,NewazJeong,WeidmannHirst,LiChen} and mapping \cite{MartinPayton,LaSheng,LaShengChen,MorelandeSkvortsov,RazakSukumarChung_journal,LeongZamani_SP,LeongZamaniShames,TranGarratt} employing remote (and noisy) measurements have attracted increasing attention in recent years due to tremendous progress in instrumentation for aerial and remote sensing using unmanned aerial vehicles (UAVs) and  unmanned ground vehicles (UGVs). This technological advancement necessitates the development and evaluation of some statistical methods and algorithms that can be applied for the timely estimation of the structure (map) of the scalar field in the environment from an ever-increasing set of noisy measurements acquired in a  sequential or concurrent manner (e.g.,  sensing signals from UAVs and UGVs operating over the hazardous area, the intermittent concentration of methane leaked from the ocean floor, oil surface concentration due to androgenic spill, trigger signals from meteorological stations,  etc). These algorithms may become critical for backtracking and characterization of the main sources of the scalar field in the environment which is important for the  remediation effectiveness and retrospective forensic analysis.   This was the main motivation for the present study.

Conventionally, in work on estimation of scalar fields, the field is modelled as a sum of radial basis functions (RBFs) or Gaussian mixture models, see, e.g., \cite{LaSheng,LaShengChen,MorelandeSkvortsov,RazakSukumarChung_journal,LeongZamani_SP,LeongZamaniShames,TranGarratt}. Field estimation then reduces to a problem of estimating the parameters of these models. In the current work, we assume the field to be an arbitrary 2D function which can be viewed in the Fourier domain using, e.g., the discrete Fourier transform (DFT) or the discrete cosine transform (DCT) \cite{BritanikYipRao}. For intuition of this approach, suppose we  regard the plot of the field as an image. From image processing, it is well-known that the most important parts of an image are concentrated in the lowest (spatial) frequency components/modes. Our approach to field estimation is then to estimate the low frequency Fourier components.\footnote{We will use the terms Fourier component and DCT component interchangeably in this paper.} One of the advantages for using this Fourier component approach compared to the RBF approach is that it offers a perhaps more natural way to control the accuracy of the approximation, e.g., by controlling the number of Fourier modes used/retained. Furthermore, if one wants to refine the field estimate by estimating more modes, existing estimates of the lower order modes can be reused. 

The main contributions of this paper are:
\begin{itemize}
    \item Rather than the use of radial basis function field models, we model the 2D scalar field in the Fourier domain as a sum of Fourier components. 
    \item A numerical comparison of the approximation capabilities of the Fourier components and RBF field models is carried out.
    \item We show that the RBF field model and Fourier component field model have similar forms, which allows one to leverage existing algorithms for field estimation developed for the RBF field model to estimate the Fourier components under various different measurement models.
    \item For the quantized measurements model, we present in detail how Fourier component estimation can be carried out using an online optimization approach similar to \cite{LeongZamaniShames}. We further extend the approach of \cite{LeongZamaniShames} from binary measurements to multi-level quantized measurements, and from static to time-varying fields. 
\end{itemize}

The organization of the paper is as follows: Section \ref{sec:preliminaries} gives preliminaries on the DCT and motivation for its use in field modelling. Section \ref{sec:system_model} presents our field model, as well as various different sensor measurement models. Section~\ref{sec:DCT_RBF_comparison} compares our Fourier component field model with the RBF field model in terms of approximation performance. Section \ref{sec:DCT_estimation} first relates our field model to the RBF field models considered in previous works, and then considers in detail the estimation of Fourier components using  quantized measurements. Numerical studies  are presented in Section \ref{sec:numerical}. 

\section{Preliminaries}
\label{sec:preliminaries}

Consider a region of interest $\mathcal{S} = [X_{\textnormal{min}}, X_{\textnormal{max}}] \times [Y_{\textnormal{min}}, Y_{\textnormal{max}}]$. Discretize $[X_{\textnormal{min}}, X_{\textnormal{max}}]$ into $N_x$ points and $ [Y_{\textnormal{min}}, Y_{\textnormal{max}}]$ into $N_y$ points as
\begin{align*}
\mathcal{X}_d \triangleq \left\{X_{\textnormal{min}} + \Big(\frac{1}{2} + I_x \Big) \Delta_x:   I_x \in \{0, \dots, N_x - 1\} \right\} \\
\mathcal{Y}_d \triangleq \left\{Y_{\textnormal{min}} + \Big(\frac{1}{2} + I_y \Big) \Delta_y:  I_y \in \{0, \dots, N_y - 1\} \right\},
\end{align*}
where 
$$\Delta_x \triangleq \frac{X_{\textnormal{max}} - X_{\textnormal{min}}}{N_x}, \quad \Delta_y \triangleq \frac{Y_{\textnormal{max}} - Y_{\textnormal{min}}}{N_y}. $$


Our aim is the estimation of 2D distribution of scalar field $\phi(x,y)$, $(x,y) \in \mathcal{S}$,  which is assumed either static or slowly varying. We define
$$\phi_d(I_x, I_y) \triangleq \phi\Big(X_{\textnormal{min}} + \left(1/2 + I_x \right) \Delta_x, Y_{\textnormal{min}} + \left(1/2 + I_y \right) \Delta_y \Big)$$
as the field value at the discretized position $\big(X_{\textnormal{min}} + \left(1/2 + I_x \right) \Delta_x, Y_{\textnormal{min}} + \left(1/2 + I_y \right) \Delta_y \big) \in \mathcal{X}_d \times \mathcal{Y}_d$. 
Recall the (Type-II) discrete cosine transform (DCT), see, e.g., \cite{BritanikYipRao,Strang_DCT}: 
\begin{align*}
C(u,v) &= \sum_{I_x=0}^{N_x-1} \sum_{I_y=0}^{N_y-1} \alpha_x(u) \alpha_y(v) \phi_d(I_x,I_y) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right), \\ &\quad\quad u=0,\dots,N_x-1, \quad v=0,\dots,N_y-1,
\end{align*}
where 
$$ \alpha_x(u) \triangleq \left\{\begin{array}{ll} \sqrt{\frac{1}{N_x}}, & u = 0 \\ 
\sqrt{\frac{2}{N_x}}, & u \neq 0
\end{array} \right., \quad 
\alpha_y(v) \triangleq \left\{\begin{array}{ll} \sqrt{\frac{1}{N_y}}, & v = 0 \\
\sqrt{\frac{2}{N_y}}, & v \neq 0.
\end{array} \right. $$

The inverse DCT is given by:
\begin{align}
\phi_d(I_x,I_y) &= \sum_{u=0}^{N_x-1} \sum_{v=0}^{N_y-1} \alpha_x(u) \alpha_y(v) C(u,v) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right), \label{eqn:inverse_DCT} \\ &\quad\quad I_x=0,\dots,N_x-1, \quad I_y=0,\dots,N_y-1. \nonumber
\end{align}
It will be convenient for our purposes to rewrite \eqref{eqn:inverse_DCT} as
\begin{align}
\phi_d(I_x,I_y) &= \sum_{(u,v) \in \mathcal{U}}  \alpha_x(u) \alpha_y(v) C(u,v) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right), \label{eqn:inverse_DCT_single_summation} \\ &\quad\quad I_x=0,\dots,N_x-1, \quad I_y=0,\dots,N_y-1, \nonumber
\end{align}
where 
$$\mathcal{U} \triangleq \{(u,v): u \in \{0, \dots, N_x-1\}, v \in \{0, \dots, N_y-1\} \}.$$

The most important information about the field distribution is concentrated in the low order modes, i.e. the components corresponding to $\cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right)$ with $u$ and $v$ small, while higher order modes define the fine structure of the field distribution. See Figs. \ref{fig:field_seed341_DCT} and \ref{fig:field_seed343_DCT} for examples of how retaining different numbers of modes affects the quality of the approximation to the true field. 


\section{System Model}
\label{sec:system_model}

\subsection{Field Model}
Motivated by the above discussion, we propose to approximate \eqref{eqn:inverse_DCT_single_summation} by
\begin{equation}
\label{field_model} 
\begin{split}
\phi_d(I_x,I_y) & \approx \sum_{(u,v)\in \tilde{\mathcal{U}}} \alpha_x(u) \alpha_y(v) C(u,v) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right), \\ & \quad\quad I_x=0,\dots,N_x-1, \quad I_y=0,\dots,N_y-1 \\ & \triangleq \tilde{\phi}_d(I_x,I_y),
\end{split}
\end{equation}
where $\tilde{\mathcal{U}} \subseteq \mathcal{U}$ is the subset of low order modes that we wish to retain.\footnote{In general one could use in \eqref{field_model} coefficients $\tilde{C}(u,v)$ which are not necessarily equal to $C(u,v)$. One reason for taking the coefficients to be equal to $C(u,v)$ is given in Lemma \ref{lemma:optimal_C_DCT}.}

For example, we could  retain the first $\tilde{N}_x \times \tilde{N}_y$ modes, with $\tilde{N}_x \leq N_x, \tilde{N}_y \leq N_y$, so that 
\begin{equation}
\label{eqn:U_tilde_rect}
 \tilde{\mathcal{U}} = \{(u,v): u \in \{0, \dots, \tilde{N}_x-1\}, v \in \{0, \dots, \tilde{N}_y-1\} \}.
 \end{equation}
The total number  of modes retained $\tilde{N} $ is thus equal to $\tilde{N} = \tilde{N}_x  \tilde{N}_y$.

Another possibility is the following:
\begin{equation}
\label{eqn:U_tilde_largest}
\tilde{\mathcal{U}} = \{ \tilde{N} \textnormal{ pairs } (u,v) \textnormal{ with smallest values of } (u+1)^2 + (v+1)^2 \}
\end{equation}
which tries to retain the $\tilde{N}$ ``largest'' (in magnitude) modes.\footnote{This is of course an approximation, as exactly determining the $\tilde{N}$ largest modes depends on and requires knowledge of the very field that we are trying to estimate.} The motivation for \eqref{eqn:U_tilde_largest} comes from a result that the DCT coefficients $C(u,v)$ decay as $O \big(\frac{1}{(u+1)^2 + (v+1)^2} \big)$ for $u,v \rightarrow \infty$ \cite{YamataniSaito}. Thus the larger components will usually have smaller values of  $(u+1)^2 + (v+1)^2$, leading to the choice \eqref{eqn:U_tilde_largest}. In numerical simulations, we have found \eqref{eqn:U_tilde_largest} to give better approximations than \eqref{eqn:U_tilde_rect} (for the same number of retained modes $\tilde{N}$) in many, though not all, cases. 


\subsection{Measurement Models}
At position $\big(X_{\textnormal{min}} + \left(1/2 + I_x \right) \Delta_x, Y_{\textnormal{min}} + \left(1/2 + I_y \right) \Delta_y \big)$, we have noisy measurements of the field
$$z(I_x,I_y) = h(\phi_d(I_x,I_y), n(I_x, I_y)),$$
where $h(\bm{\cdot},\bm{\cdot})$ is a (in general non-linear) function, and  $n(\bm{\cdot},\bm{\cdot})$ is random noise.

For example, we could have additive noise
\begin{equation}
\label{additive_noise_model}
z(I_x,I_y) = \phi_d(I_x,I_y) + n(I_x, I_y),
\end{equation}
similar to \cite{LaSheng,LaShengChen}. 

One could also further quantize \eqref{additive_noise_model}
\begin{equation}
\label{quantized_measurement_model}
z(I_x,I_y) = q(\phi_d(I_x,I_y) + n(I_x, I_y))
\end{equation}
where $q(\bm{\cdot})$ is a quantizer of $L$ levels, say $\{0, 1, \dots, L-1\}$. The quantizer can be expressed in the form 
\begin{equation}
\label{eqn:quantizer}
q(x) = \left\{\begin{array}{cc} 0, & x < \tau_0 \\ 1, & \tau_0 \leq x < \tau_1 \\ \vdots & \vdots \\ L-2, & \tau_{L-3} \leq x < \tau_{L-2} \\ L-1, & x \geq \tau_{L-2}    \end{array} \right. 
\end{equation}
where the quantizer thresholds $\{\tau_0,\dots,\tau_{L-2}\}$ satisfy $\tau_0 \leq \tau_1 \leq \dots \leq \tau_{L-2}$.


The special case of \eqref{quantized_measurement_model}-\eqref{eqn:quantizer} corresponding to a 1-bit quantizer, or binary measurements, is considered in \cite{LeongZamani_SP,LeongZamaniShames,TranGarratt}. It can be expressed as
\begin{equation}
\label{binary_measurement_model}
z(I_x,I_y) = \mathds{1env} \big(\phi_d(I_x,I_y) + n(I_x, I_y) > \tau \big),
\end{equation}
where $\tau$ is the quantizer threshold, and $\mathds{1}(\bm{\cdot})$ is the indicator function that returns 1 if its argument is true and 0 otherwise. 

Another measurement model which has been considered are Poisson measurements \cite{MorelandeSkvortsov}. Define $\mathbf{x} \triangleq (x,y)$. Then in this model
$$z(\mathbf{x}) \sim \texttt{Poisson}(\lambda(\textbf{x})),$$
where 
$$\lambda(\textbf{x}) = \int k(\mathbf{x}' - \mathbf{x}) \phi(\mathbf{x}') d\mathbf{x}'$$
and 
$$k (\mathbf{x}) = \left\{ \begin{array}{cc} \frac{1}{R^2}, & ||\mathbf{x}|| \leq R \\  \frac{1}{||\mathbf{x}||^2}, & ||\mathbf{x}|| \geq R \end{array} \right.$$
for some constant $R$.

\subsection{Problem Statement}
\label{sec:problem_statement}
The problem we wish to consider in this paper is to estimate the coefficients\footnote{When we refer to \emph{estimation of components/modes} in this paper, we specifically mean estimation of the coefficients $C(u,v)$.}
$$C(u,v), \,\,(u,v) \in \tilde{\mathcal{U}}$$
from noisy measurements $\{z(I_x,I_y)\}$ of the field $\phi_d(I_x,I_y)$. The estimation should be done in an online manner such that the estimates are continually updated as new measurements are collected.


\section{Comparison with RBF Field Model}
\label{sec:DCT_RBF_comparison}
Before we consider the problem of estimating the coefficients $C(u,v)$ (which will be studied in Section \ref{sec:DCT_estimation}), we will in this section compare the use of our Fourier component model
\eqref{field_model}
with the radial basis function model considered in \cite{RazakSukumarChung_journal, LeongZamani_SP,LeongZamaniShames,TranGarratt} (see also \cite{LaSheng,LaShengChen,MorelandeSkvortsov} for similar models), in terms of how well they can approximate a field for a given number of modes (for the Fourier component model) or basis functions (for the RBF model). 

\subsection{Fourier Component Field Model}
Define the mean squared error (MSE):
$$ \textnormal{MSE} \triangleq \frac{1}{N_x N_y} \sum_{I_x=0}^{N_x-1} \sum_{I_y=0}^{N_y-1} \Big( \phi_d (I_x, I_y) - \tilde{\phi}_d (I_x, I_y)  \Big)^2,$$
where 
%$$\phi_d(I_x,I_y) = \sum_{(u,v) \in \mathcal{U}} \alpha_x(u) \alpha_y(v) C(u,v) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right)$$
 $\phi_d(I_x,I_y)$ is the (discretized) true field given by \eqref{eqn:inverse_DCT_single_summation} and
$$\tilde{\phi}_d (I_x, I_y)   \triangleq \sum_{(u,v) \in \tilde{\mathcal{U}} } \alpha_x(u) \alpha_y(v) \tilde{C}(u,v) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right)$$
is the approximation of the true field using a subset of modes $\tilde{U}$ and coefficients $\tilde{C}(u,v)$. The expression for $\tilde{\phi}_d (I_x, I_y) $ is the same as \eqref{field_model} except that the coefficients $\tilde{C}(u,v)$ may be different from $C(u,v)$. However, it turns out that setting $\tilde{C}(u,v)$ to be equal to $C(u,v)$ will minimize the MSE. 

\begin{lemma}
\label{lemma:optimal_C_DCT}
Given a subset of modes $\tilde{U}$, the optimal values of $\tilde{C}(u,v)$ that minimize the MSE satisfy
$$ \tilde{C}^*(u,v) = C(u,v), \, \forall (u,v) \in \tilde{U}.$$
\end{lemma}
\begin{proof}
By definition, 
%\begin{equation}
%\label{eqn:MSE_derivation_DCT}
%\begin{split}
\begin{align}
\textnormal{MSE} &= \frac{1}{N_x N_y} \sum_{I_x=0}^{N_x-1} \sum_{I_y=0}^{N_y-1} \Big( \phi_d (I_x, I_y) - \tilde{\phi}_d (I_x, I_y)  \Big)^2  \nonumber \\
& = \frac{1}{N_x N_y} \sum_{I_x=0}^{N_x-1} \sum_{I_y=0}^{N_y-1}  \Bigg( \sum_{(u,v) \in \mathcal{U}} \alpha_x(u) \alpha_y(v) C(u,v) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right) \nonumber \\
& \quad - \sum_{(u,v) \in \tilde{\mathcal{U}} } \alpha_x(u) \alpha_y(v) \tilde{C}(u,v) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right) \Bigg) ^2 \nonumber \\
&= \frac{1}{N_x N_y} \sum_{I_x=0}^{N_x-1} \sum_{I_y=0}^{N_y-1}  \Bigg( \sum_{(u,v) \in \tilde{\mathcal{U}}} \alpha_x(u) \alpha_y(v) \big(C(u,v) - \tilde{C}(u,v) \big) \nonumber  \\ 
& \quad \quad \times \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right) \nonumber \\
& \quad + \sum_{(u,v) \in \mathcal{U} \setminus \tilde{\mathcal{U}} } \alpha_x(u) \alpha_y(v) C(u,v) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right) \Bigg) ^2 \nonumber \\
& = \frac{1}{N_x N_y} \sum_{I_x=0}^{N_x-1} \sum_{I_y=0}^{N_y-1}  \Bigg[ \Bigg( \sum_{(u,v) \in \tilde{\mathcal{U}}} \alpha_x(u) \alpha_y(v) \big(C(u,v) - \tilde{C}(u,v) \big) \nonumber \\ 
& \quad \quad \times \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right) \Bigg)^2 \nonumber \\
& \quad + 2 \Bigg( \sum_{(u,v) \in \tilde{\mathcal{U}}} \alpha_x(u) \alpha_y(v) \big(C(u,v) - \tilde{C}(u,v) \big) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right) \Bigg) \nonumber \\
& \quad \quad \times \Bigg( \sum_{(u,v) \in \mathcal{U} \setminus \tilde{\mathcal{U}} } \alpha_x(u) \alpha_y(v) C(u,v) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right) \Bigg) \nonumber \\
& \quad + \Bigg(\sum_{(u,v) \in \mathcal{U} \setminus \tilde{\mathcal{U}} } \alpha_x(u) \alpha_y(v) C(u,v) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right) \Bigg)^2 \Bigg] \nonumber \\
& = \frac{1}{N_x N_y} \sum_{I_x=0}^{N_x-1} \sum_{I_y=0}^{N_y-1}  \Bigg[ \Bigg( \sum_{(u,v) \in \tilde{\mathcal{U}}} \alpha_x(u) \alpha_y(v) \big(C(u,v) - \tilde{C}(u,v) \big) \nonumber \\ 
& \quad \quad \times \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right) \Bigg)^2 \nonumber \\
& \quad + \Bigg(\sum_{(u,v) \in \mathcal{U} \setminus \tilde{\mathcal{U}} } \alpha_x(u) \alpha_y(v) C(u,v) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right) \Bigg)^2 \Bigg]  \label{eqn:MSE_derivation_DCT}
\end{align}
%\end{split}
%\end{equation}
The last equality follows since 
\begin{align*}
\sum_{I_x=0}^{N_x-1} \sum_{I_y=0}^{N_y-1} & \alpha_x(u) \alpha_y(v) \big(C(u,v) - \tilde{C}(u,v) \big) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right) \\
& \quad \times \alpha_x(u') \alpha_y(v') C(u',v') \cos \left(\frac{(2I_x+1)\pi u'}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v'}{2 N_y} \right)  
\end{align*}
is equal to zero for all $(u,v) \in \mathcal{U}$ and $ (u',v') \in \mathcal{U} \setminus \tilde{\mathcal{U}}$, by orthogonality of the DCT basis vectors \cite{AhmedNatarajanRao,Strang_DCT}. To conclude the proof, we note that the expression for the MSE given in the last equality of \eqref{eqn:MSE_derivation_DCT} is  clearly minimized when $ \tilde{C}(u,v) = C(u,v), \, \forall (u,v) \in \tilde{U}.$
\end{proof}


\subsection{RBF Field Model}
The following RBF field model is used in \cite{RazakSukumarChung_journal, LeongZamani_SP,LeongZamaniShames,TranGarratt}:
\begin{equation}
\label{field_model_RBF}
\phi(\mathbf{x}) \approx \sum_{j=1}^J \beta_j K_j(\mathbf{x}),
\end{equation}
where $\mathbf{x} \triangleq (x,y)$ and $K_j(\mathbf{x}), j=1,\dots,J$ are radial basis functions. In particular, we consider the choice
 \begin{equation}
 \label{eqn:Gaussian_RBF}
 K_j(\mathbf{x}) = \exp \left(- \frac{\|\mathbf{c}_j-\mathbf{x}\|^2}{\sigma_j^2}\right), \quad j=1,\dots,J,
 \end{equation}
which results in a Gaussian mixture model \cite{MorelandeSkvortsov}.
For a given number of basis functions $J$, we assume that the $\mathbf{c}_j$'s and $\sigma_j$'s are chosen,\footnote{The case where the $\mathbf{c}_j$'s and $\sigma_j$'s are also estimated has been considered, but was found to suffer from identifiability issues and sometimes give very unreliable results \cite{LeongZamani_SP}.} while the $\beta_j$'s are free parameters. Algorithms for estimating the $\beta_j$'s are studied in, e.g., \cite{RazakSukumarChung_journal, LeongZamani_SP,LeongZamaniShames,TranGarratt}. Here we consider instead the problem of finding the optimal  $\beta_j$'s in order to minimize the mean squared error, to see how good the RBF model can be when approximating a field for a given set of basis functions. Define
$$ \textnormal{MSE}_{RBF} \triangleq \frac{1}{|\mathcal{S}_d|} \sum_{\mathbf{x} \in \mathcal{S}_d} \Big( \phi (\mathbf{x}) - \sum_{j=1}^J \beta_j K_j(\mathbf{x}) \Big)^2,$$
where $\phi (\mathbf{x}) $ is the true field value at position~$\mathbf{x}$, $\mathcal{S}_d$ is a discretized set of points in the search region $\mathcal{S}$, and $|\mathcal{S}_d|$ is the cardinality of $\mathcal{S}_d$. 

\begin{lemma}
\label{lemma:optimal_beta_RBF}
Given a set of radial basis functions $\{K_1(.), \dots, K_j(.)\}$ and an ordering $\{\mathbf{x}_1, \dots, \mathbf{x}_{|\mathcal{S}_d|} \}$ of the elements in $\mathcal{S}_d$, the optimal values of $(\beta_1, \dots, \beta_J)$ that minimize $ \textnormal{MSE}_{RBF}$ satisfy
$$ \bm{\beta}^* = \left( \mathcal{K}^T \mathcal{K} \right)^{-1} \mathcal{K}^T \bm{\phi},$$
where $\bm{\beta} = \begin{bmatrix} \beta_1 & \dots & \beta_J \end{bmatrix}^T$, $\bm{\phi} = \begin{bmatrix} \phi(\mathbf{x}_1), \dots, \phi(\mathbf{x}_{|\mathcal{S}_d|}) \end{bmatrix}^T$, and 
$$\mathcal{K} = \begin{bmatrix}
K_1(\mathbf{x}_1) & \dots & K_J(\mathbf{x}_1) \\
\vdots & \ddots & \vdots \\
K_1(\mathbf{x}_{|\mathcal{S}_d|}) & \dots & K_J(\mathbf{x}_{|\mathcal{S}_d|})
\end{bmatrix}.$$
\end{lemma}

\begin{proof}
This is a standard application of the optimal solution to a linear least squares / linear regression problem \cite{CalafioreElGhaoui,Murphy_book1}.    
\end{proof}

\subsection{Numerical Experiments}

\begin{figure}[t!]
\centering 
\includegraphics[scale=0.35]{field_seed341_DCT.pdf} 
\caption{True field and approximations obtained by retaining different numbers of modes}
\label{fig:field_seed341_DCT}
\end{figure} 

\begin{figure}[t!]
\centering 
\includegraphics[scale=0.35]{field_seed341_RBF.pdf} 
\caption{True field and approximations obtained by using different numbers of basis functions}
\label{fig:field_seed341_RBF}
\end{figure} 

\begin{figure}[t!]
\centering 
\includegraphics[scale=0.35]{field_seed343_DCT.pdf} 
\caption{True field and approximations obtained by retaining different numbers of modes}
\label{fig:field_seed343_DCT}
\end{figure} 

\begin{figure}[t!]
\centering 
\includegraphics[scale=0.35]{field_seed343_RBF.pdf} 
\caption{True field and approximations obtained by using different numbers of basis functions}
\label{fig:field_seed343_RBF}
\end{figure} 

In Figs.\ref{fig:field_seed341_DCT}-\ref{fig:field_seed343_RBF}  we show two example fields, and the field approximations that are obtained when various different numbers of modes (for Fourier component model) or radial basis functions (for RBF model) are used. The discretization in the true fields is set as $N_x = N_y = 100$ (so that there are $100^2 = 10000$ modes in total). For the RBF model we set $\mathcal{S}_d = \mathcal{X}_d \times \mathcal{Y}_d$, so that the discretized set of points are the same in the MSE calculations. 
For the Fourier component model, we choose $\tilde{\mathcal{U}}$ as in \eqref{eqn:U_tilde_largest} to retain the $\tilde{N}$ ``largest'' modes. For the RBF model, we use $J = J_x \times J_y$ radial basis functions with $\mathbf{c}_j$'s in \eqref{eqn:Gaussian_RBF} placed uniformly on a grid at locations $\mathcal{X}_{RBF} \times \mathcal{Y}_{RBF}$, where
\begin{align*}
\mathcal{X}_{RBF} \triangleq \left\{X_{\textnormal{min}} + \Big(\frac{1}{2} + i_x \Big) \delta_x:   i_x \in \{0, \dots, J_x - 1\} \right\} \\
\mathcal{Y}_{RBF} \triangleq \left\{Y_{\textnormal{min}} + \Big(\frac{1}{2} + i_y \Big) \delta_y:  i_y \in \{0, \dots, J_y - 1\} \right\}
\end{align*}
and 
$$\delta_x \triangleq \frac{X_{\textnormal{max}} - X_{\textnormal{min}}}{J_x}, \quad \delta_y \triangleq \frac{Y_{\textnormal{max}} - Y_{\textnormal{min}}}{J_y}. $$
The $\sigma_j$'s in \eqref{eqn:Gaussian_RBF} are chosen to be equal to $\sigma_j = \max(\delta_x,\delta_y), \forall j$. The $\beta_j$'s used in \eqref{field_model_RBF} are the optimal values computed according to Lemma \ref{lemma:optimal_beta_RBF}. 

In the figures we show two performance measures, 1) the MSE, and 2) the structural similarity (SSIM) index, which originated in \cite{WangBovikSheikhSimoncelli} and has been widely adopted in the image processing community. The structural similarity index is a measure of the similarity between two images. In our case, we can regard $\Phi = \{\phi_d(I_x, I_y): I_x = 0,\dots, N_x - 1, I_y = 0, \dots, N_y - 1 \}$ and $\tilde{\Phi} = \{\tilde{\phi}_d(I_x, I_y): I_x = 0,\dots, N_x - 1, I_y = 0, \dots, N_y - 1 \}$ as the image representations of the true and approximated fields respectively, and compute the SSIM between these two images. The SSIM gives a scalar value between 0 and 1, with $\textnormal{SSIM} = 1$ if the two images to be compared are identical. 
We refer to \cite{WangBovikSheikhSimoncelli,WangBovik_MSE} for the specific equations used to compute the SSIM. 

We see from Figs.\ref{fig:field_seed341_DCT}-\ref{fig:field_seed343_RBF} that as more modes (for Fourier component model) or basis functions (for RBF model) are used, the approximations to the true field improves.  When using a smaller number of modes / basis functions the RBF model seems to give better approximations than the Fourier component model, while for larger numbers of modes / basis functions the two approaches perform similarly. We also observe that for these two examples, using a relatively small number of modes (when compared to the total number of modes of 10000) or basis functions will still result in a qualitatively reasonable approximation to the true field.

Although the Fourier component model does not seem to offer a significant advantage in terms of approximation quality, there are other reasons where one may consider its use. One advantage of the Fourier model is that it provides a natural way to control the number of model parameters (the coefficients $C(u,v)$) that need to be estimated, in that we simply choose however many modes we wish to retain, whereas with the RBF model one would need to also choose the locations $\mathbf{c}_j$ to place the basis functions and what the values of $\sigma_j$ should be. Additionally, if we want to refine our field model with finer structure by including more model parameters, in the Fourier component model we can reuse any previous estimates  (and further improve them) of the lower order modes, since these remain the same in a model with more modes, whereas in the RBF model one would likely need  to recalculate the estimates of all the parameter values when more basis functions are utilized.

\section{Estimation of Fourier Components}
\label{sec:DCT_estimation}
We now return to the problem of estimating the coefficients $C(u,v), \, (u,v) \in \tilde{\mathcal{U}}$ stated in Section~\ref{sec:problem_statement}. 
Given a set of modes to be retained $\tilde{\mathcal{U}}$, of cardinality $\tilde{N}$, define an ordering on $\tilde{\mathcal{U}}$ indexed by $j \in \{0, \dots, \tilde{N}-1\}$. For instance, the elements of $\tilde{\mathcal{U}}$ could be sorted in lexicographic order.  
Denote the $j$-th element under this ordering  by $(u_j, v_j)$, and define 
$$ C_j \triangleq C(u_j,v_j).$$

%Given $u \in \{0,\dots,\tilde{N}_x-1\}$ and $v \in \{0, \dots, \tilde{N}_y-1\}$, define Alternatively, one can define $j \triangleq v + u \tilde{N}_y$, and conversely $u_j  \triangleq j \textnormal{ mod } \tilde{N}_y $, $v_j \triangleq \lfloor j/\tilde{N}_y \rfloor$. 
%$$j \triangleq u + v \tilde{N}_x$$
%and note that $j \in \{0,\dots, \tilde{N}_x  \tilde{N}_y-1\}$. Conversely, given  $j \in \{0,\dots, \tilde{N}_x  \tilde{N}_y-1\}$, define
%$$ u_j \triangleq  \lfloor j/\tilde{N}_x \rfloor, \quad v_j \triangleq j \textnormal{ mod } \tilde{N}_x$$ 
%Now define
%$$ C_j \triangleq C(u_j,v_j).$$
%Forming $C_j = C(u_j, v_j), j = 0, \dots, \tilde{N}_x \tilde{N}_y - 1$ then corresponds to applying the $\mathtt{vec}$ operation \cite{HornJohnson2} on the matrix with entries $C(u,v)$.

Then we can express
\begin{align*}
\tilde{\phi}_d(I_x,I_y) & = \sum_{(u,v) \in \tilde{\mathcal{U}}} \alpha_x(u) \alpha_y(v) C(u,v) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right)
\end{align*}
in the alternative form 
\begin{equation}
\label{field_model_vector}
 \tilde{\phi}_d(I_x,I_y)  = \sum_{j=0}^{\tilde{N}-1} \alpha_x(u_j) \alpha_y(v_j) C_j \cos \left(\frac{(2I_x+1)\pi u_j}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v_j}{2 N_y} \right),
\end{equation}
which is a linear function of $(C_0, \dots, C_{\tilde{N} -1})$.

%In \cite{LeongZamani_SP,LeongZamaniShames,TranGarratt} (see also \cite{LaSheng,LaShengChen,MorelandeSkvortsov,RazakSukumarChung_journal} for similar models), the field is modelled as \eqref{field_model_RBF}.  
Comparing \eqref{field_model_vector} with the RBF field model \eqref{field_model_RBF}, we see that they are both linear functions of the parameters that are to be estimated. Thus the algorithms developed in e.g. \cite{LaSheng,LaShengChen,RazakSukumarChung_journal, LeongZamani_SP,LeongZamaniShames,TranGarratt,MorelandeSkvortsov}  for estimating fields can in principle also be adapted to work for our field model \eqref{field_model_vector}, under their various assumed measurement models.

\begin{remark}
\label{remark:param_magnitudes}
The DCT coefficients which we are trying to estimate can be of substantially different orders of magnitude, with the higher order components being much smaller in magnitude than the ``DC'' component corresponding to  $u=v=0$, due to the result that the DCT coefficients decay as $O \big(\frac{1}{(u+1)^2 + (v+1)^2} \big)$ \cite{YamataniSaito}.
%For instance, the field in Fig. \ref{fig:field_1_modes} has $C(0,0) = 56.31$, $C(1,1) = 7.749$, $C(2,2) = -2.065$, $C(3,3) = 1.040$, $C(4,4) = 0.544$, $C(5,5) = 0.0156$, $C(6,6) = 0.0284$, $C(7,7) = 0.0075$, \dots. 
In order to estimate parameters with such large differences in magnitude, it is desirable to appropriately scale the parameters that are to be estimated, see \eqref{eqn:param_scaling} below. 
\end{remark}

\subsection{Estimation of Fourier Components Using Quantized Measurements}
\label{sec:DCT_estimation_quantized_measurements}
In this subsection we describe an approach to estimating the parameters $C(u,v), \, u=0, \dots, \tilde{N} - 1$, which assumes the quantized measurement model \eqref{quantized_measurement_model}-\eqref{eqn:quantizer}, with the parameters estimated recursively. 
%A sequential Monte Carlo (SMC) approach is presented in Section \ref{sec:SMC_approach}, while an online optimization approach  is presented in Section \ref{sec:online_optim_approach}. The algorithms are similar to those of \cite{LeongZamani_SP} and \cite{LeongZamaniShames} respectively,  
The algorithm uses an online optimization approach similar to \cite{LeongZamaniShames},
however in this paper we will generalize \cite{LeongZamaniShames} from binary measurements to multi-level quantized measurements, and also extend the approach to handle time-varying fields. 

%\subsection{Sequential Monte Carlo approach}
%\label{sec:SMC_approach}
%In the measurement model \eqref{quantized_measurement_model}-\eqref{eqn:quantizer}, assume the noise has Gaussian distribution
%$n(.,.) \sim \mathcal{N}(0, \sigma_n^2)$. The noise variance $\sigma_n^2$ is assumed unknown as the algorithm can also estimate $\sigma_n^2$. Recalling the observation in Remark \ref{remark:param_magnitudes}, we consider the following scaling of the DCT coefficients:
%\begin{equation}
%\label{eqn:param_scaling}
%\theta_j \triangleq \big((u_j+1)^2 + (v_j+1)^2 \big) C_j.
%\end{equation}
%We then define 
%$$\bm{\theta} \triangleq (\theta_0, \dots, \theta_{\tilde{N}_x \tilde{N}_y-1}, \log \sigma_n)$$
%as the vector of parameters that are estimated. 

%Let $z_k$ denote the measurement,  and $(I_{x,k}, I_{y,k})$ the position index, at time/iteration $k$. For notational compactness we also denote
%\begin{equation}
%\label{eqn:I_x_vector}
%\bm{I}_{\textbf{x},k} \triangleq (I_{x,k}, I_{y,k})
%\end{equation}
%and  
%\begin{equation}
%\label{eqn:K_vector}
%\mathbf{K}(\bm{I}_{\textbf{x},k} ) \triangleq \left[\begin{array}{cccc} K_0 (\bm{I}_{\textbf{x},k} ) & K_1 (\bm{I}_{\textbf{x},k} ) &  \dots & K_{\tilde{N}_x \tilde{N}_y-1}(\bm{I}_{\textbf{x},k} ) \end{array} \right]^T,
%\end{equation}
%where
%\begin{equation}
%\label{eqn:K_vector_components}
%K_j (\bm{I}_{\textbf{x},k} ) \triangleq \frac{\alpha_x(u_j) \alpha_y(v_j) }{(u_j\!+\!1)^2 \!+\! (v_j\!+\!1)^2} \cos \Big(\frac{(2I_{x,k}+1)\pi u_j}{2 N_x} \Big) \cos \Big(\frac{(2I_{y,k}+1)\pi v_j}{2 N_y} \Big).
%\end{equation}
%Denote 
%\begin{equation}
%\label{eqn:z_all}
%z_{1:k} \triangleq \{z_1, \dots, z_k\}    
%\end{equation}
%as the set of measurements collected up to time $k$, with corresponding position indices
%\begin{equation}
%\label{eqn:I_x_all}
%\bm{I}_{\textbf{x},1:k} \triangleq \{(I_{x,1:k}, I_{y,1:k})\}.
%\end{equation}
 
%\begin{algorithm}
%\caption{Estimation of Fourier components using SMC approach}
%\label{alg:DCT_SMC_time_varying}
%\begin{algorithmic}[1]
%\State \textbf{Algorithm Parameters}: $N \in \mathbb{N}$, $a \in (0,1)$, $h = \sqrt{1-a^2}$, prior pdf $p_0(\bm{\theta})$, $b \in [0,1]$, distribution of new parameter values $p_{\bm{\theta}_{k-1}}(.)$
%\State \textbf{Inputs}: Initial location index $\bm{I}_{\textbf{x},1}$
%\State \textbf{Outputs}: Particles $\{\bm{\theta}_k^{(i)}\}$ and weights $\{ \bm{w}_k^{(i)} \}$

%\State Sample particles $\bm{\theta}_0^{(i)}, i=1,\dots,N$ from $p_0(\bm{\theta})$, and assign weights $ \bm{w}_0^{(i)} = \frac{1}{N}, i=1,\dots,N$
%\For{$k=1,2,\dots,$}
%	\State Observe $z_k(\bm{I}_{\textbf{x},k})$ at location index $\bm{I}_{\textbf{x},k}$
%	\For{$i=1,\dots,N$}  \label{line_loop_start:Nemeth}
%		\State Compute $ \textbf{m}_{k-1}^{(i)} = a \bm{\theta}_{k-1}^{(i)}  + (1-a) \bm\bar{\bm{\theta}}_{k-1}$ where $ \bar{\bm{\theta}}_{k-1}  = \sum_{i=1}^N \bm{w}_{k-1}^{(i)} \bm{\theta}_{k-1}^{(i)}$
		 
%		\State Assign $\bm{w}_{1,k}^{(i)} \propto  p(z_k(\bm{I}_{\textbf{x},k})|\textbf{m}_{k-1}^{(i)}; \bm{I}_{\textbf{x},k}) \bm{w}_{k-1}^{(i)}$ \label{line:Nemeth:w_1}
%		\State Sample $\bm{\gamma}_k^{(i)} \sim p_{\bm{\theta}_{k-1}^{(i)}} (.) $
%		\State Assign $\bm{w}_{2,k}^{(i)} \propto  p(z_k(\bm{I}_{\textbf{x},k})| \bm{\gamma}_k^{(i)}; \bm{I}_{\textbf{x},k}) \bm{w}_{k-1}^{(i)}$ \label{line:Nemeth:w_2}
%\EndFor
%	\State Normalize $\{\bm{w}_{1,k}^{(i)} \}$ and $\{\bm{w}_{2,k}^{(i)} \}$ such that $\sum_{i=1}^N \bm{w}_{1,k}^{(i)} = 1$ and $\sum_{i=1}^N \bm{w}_{2,k}^{(i)} = 1$ 
%	\State Sample $N$ times with replacement a set of indices $\{i^-: i=1,\dots,N\} $, from a distribution with probabilities $$\mathbb{P}(i^- = j) = \left\{ \begin{array}{cl} (1-b) \bm{w}_{1,k}^{(j)}, & j \in \{1, \dots,N\} \\  b \bm{w}_{2,k}^{(j-N)}, & j \in \{N+1, \dots,2N\}\end{array} \right.$$
%	\For{$i^- \in \{1,\dots,N\}$}
%		\State Sample  $\bm{\theta}_k^{(i)} \sim \mathcal{N}(\textbf{m}_{k-1}^{(i^-)}, h^{2} \textbf{V}_{k-1})$, where 		$\textbf{V}_{k-1}  = \sum_{i=1}^N \bm{w}_{k-1}^{(i)} (\bm{\theta}_{k-1}^{(i)} - \bar{\bm{\theta}}_{k-1}) (\bm{\theta}_{k-1}^{(i)} - \bar{\bm{\theta}}_{k-1})^T$  
%		\State Assign weights $ \bm{w}_k^{(i)} \propto \frac{p(z_k(\bm{I}_{\textbf{x},k})|\bm{\theta}_k^{(i)}; \bm{I}_{\textbf{x},k})}{p(z_k(\bm{I}_{\textbf{x},k})|\textbf{m}_{k-1}^{(i^-)}; \bm{I}_{\textbf{x},k})}$	\label{line:Nemeth_weights1}
%	\EndFor
%	\For{$i^- \in \{N+1,\dots,2N\}$}
%		\State Set  $\bm{\theta}_k^{(i)} = \bm{\gamma}_k^{(i^- - N)}$
%		\State Assign weights $ \bm{w}_k^{(i)} \propto \frac{p(z_k(\bm{I}_{\textbf{x},k}))|\bm{\theta}_k^{(i)}; \bm{I}_{\textbf{x},k})}{p(z_k(\bm{I}_{\textbf{x},k})|\bm{\gamma}_{k}^{(i^- -N)}; \bm{I}_{\textbf{x},k})}$	\label{line:Nemeth_weights2}
%	\EndFor
%	\State Normalize $\{ \bm{w}_k^{(i)} \}$ such that $\sum_{i=1}^N \bm{w}_{k}^{(i)} = 1$   \label{line_loop_end:Nemeth}
%	\State Determine  $\bm{I}_{\textbf{x},k+1} = \texttt{ActiveSensingSMC}(\bm{I}_{\textbf{x},k}, \{\bm{\theta}_k^{(i)}\})$  using Algorithm \ref{alg:active_sensing_SMC} 
%\EndFor
%\end{algorithmic}
%\end{algorithm} 

%We present in Algorithm \ref{alg:DCT_SMC_time_varying} a sequential Monte Carlo (SMC) \cite{Doucet_book} based algorithm that approximates the posterior pdf $ p(\bm{\theta} | z_{1:k}; \bm{I}_{\textbf{x},1:k})$ by a set of particles $\bm{\theta}_k^{(i)}, i=1,\dots,N$, and associated weights $\bm{w}_k^{(i)}, i=1,\dots,N$. 
%Denoting $\bm{\theta}_k^{(i)} \triangleq (\theta_{0,k}^{(i)}, \dots, \theta_{\tilde{N}_x \tilde{N}_y,k}^{(i)})$,  conditional mean estimates at iteration $k$ can be computed from the particle approximation as:
% \begin{align*}
% \hat{C}_{j,k} &= \sum_{i=1}^N \frac{ \bm{w}_{k}^{(i)} \theta_{j,k}^{(i)} }{(u_j+1)^2 + (v_j+1)^2}, \quad j = 0,\dots,\tilde{N}_x \tilde{N}_y - 1,   \quad \quad \hat{\sigma}_{n,k}^2 = \sum_{i=1}^N \bm{w}_k^{(i)} \exp(2 \theta_{\tilde{N}_x \tilde{N}_y,k}^{(i)}). \end{align*}
%In lines \ref{line:Nemeth_weights1} and \ref{line:Nemeth_weights2} of Algorithm \ref{alg:DCT_SMC_time_varying}, the likelihood functions are computed as:
%\begin{align} 
%p(z_k(\bm{I}_{\textbf{x},k}) = 0|\bm{\theta}_k^{(i)}; \bm{I}_{\textbf{x},k} )  &  =  \Phi \bigg( \frac{\tau_0 - \bm{\theta}_k^{(i)T} \mathbf{K}(\bm{I}_{\textbf{x},k} )}{\exp(\theta_{\tilde{N}_x \tilde{N}_y, k}^{(i)})}\bigg)\nonumber \\
% p(z_k(\bm{I}_{\textbf{x},k}) = l|\bm{\theta}_k^{(i)}; \bm{I}_{\textbf{x},k} ) & =  \Phi \bigg( \frac{\tau_l - \bm{\theta}_k^{(i)T} \mathbf{K}(\bm{I}_{\textbf{x},k} )}{\exp(\theta_{\tilde{N}_x \tilde{N}_y, k}^{(i)})}\bigg) -  \Phi \bigg( \frac{\tau_{l-1} - \bm{\theta}_k^{(i)T} \mathbf{K}(\bm{I}_{\textbf{x},k} )}{\exp(\theta_{\tilde{N}_x \tilde{N}_y, k}^{(i)})}\bigg), \quad l = 1, \dots, L-2 \nonumber 
%\\
% p(z_k(\bm{I}_{\textbf{x},k}) = L-1 | \bm{\theta}_k^{(i)} ; \bm{I}_{\textbf{x},k})  & = 1 -  \Phi \bigg( \frac{\tau_{L-2} - \bm{\theta}_k^{(i)T} \mathbf{K}(\bm{I}_{\textbf{x},k} )}{\exp(\theta_{\tilde{N}_x \tilde{N}_y, k}^{(i)})}\bigg), \label{likelihood_fn_multilevel}
%\end{align}
%and similarly for $p (z_k(\bm{I}_{\textbf{x},k}) | \textbf{m}_k^{(i)} ; \bm{I}_{\textbf{x},k}) $ and $p (z_k(\bm{I}_{\textbf{x},k}) | \bm{\gamma}_k^{(i)} ; \bm{I}_{\textbf{x},k}) $, where $\Phi(x) \triangleq \int_{-\infty}^{x} \frac{1}{\sqrt{2\pi}} \exp\left( - \frac{t^2}{2} \right) dt$ is the cdf of the standard normal distribution $\mathcal{N}(0,1)$. Expression \eqref{likelihood_fn_multilevel} generalizes the likelihood functions for the case of binary measurements considered in \cite{LeongZamani_SP}.
%Finally, we point out that Algorithm \ref{alg:DCT_SMC_time_varying} can be used to estimate slowly varying fields or fields with occasional abrupt changes in parameters \cite{NemethFearnheadMihaylova,LeongZamani_SP}.

%\subsubsection{Measurement location selection using active sensing}

%For choosing the locations in which to make measurements, ``active sensing'' algorithms can be used. Given position $\mathbf{x} \in \mathcal{S}$, denote $\bm{I}_{\textnormal{closest}} (\mathbf{x})$ as the closest location index $(I_x, I_y)$ to $\mathbf{x}$. In Algorithm \ref{alg:active_sensing_SMC} we present the active sensing mechanism from \cite{LeongZamani_SP}, which is an information maximization based approach \cite{RisticSkvortsovGunatilaka}, suitably generalized to the multi-level quantized measurement model \eqref{quantized_measurement_model}-\eqref{eqn:quantizer}. It will return the next location index $\bm{I}_{\textbf{x},k+1}$ to travel to, given the current location index $\bm{I}_{\textbf{x},k}$ and current set of particles $\{\bm{\theta}_k^{(i)}\}$. 

%\begin{algorithm}[t]
%\caption{Active sensing algorithm for SMC approach: $\bm{I}_{\textbf{x},k+1} = \texttt{ActiveSensingSMC}(\bm{I}_{\textbf{x},k}, \{\bm{\theta}_k^{(i)}\})$}
%\label{alg:active_sensing_SMC}
%\begin{algorithmic}[1]
%\State \textbf{Algorithm Parameters}: $\varepsilon \geq 0$, $\alpha \in [0,\infty) \backslash \{1\}$, $\rho_0 \geq 0$, $N_\rho \in \mathbb{N}$, $N_d \in \mathbb{N}$, search region $\mathcal{S}$
%\State \textbf{Inputs}:    $\bm{I}_{\textbf{x},k}$, $\{\bm{\theta}_k^{(i)}\}$
%\State \textbf{Output}: Next measurement location index $\bm{I}_{\textbf{x},k+1}$
%\State Set $\mathbf{x}_k = \big(X_{\textnormal{min}} + \left(1/2 + I_{x,k} \right) \Delta_x, Y_{\textnormal{min}} + \left(1/2 + I_{y,k} \right) \Delta_y \big)$
%	\State With probability $\varepsilon$ set $\bm{I}_{\textbf{x},k+1} $ to a random location index in $\{0,\dots,\tilde{N}_x-1\} \times \{0,\dots,\tilde{N}_y-1\}$, otherwise set $$\bm{I}_{\textbf{x},k+1} = \textrm{arg} \max\limits_{\bm{I}_{\textbf{x}'} \in \mathcal{I}_k} \frac{1}{\alpha-1} \sum_{z_{k+1}=0}^{L-1} \gamma_1(z_{k+1}|\bm{I}_{\textbf{x}'}) \ln  \frac{\gamma_\alpha(z_{k+1}|\bm{I}_{\textbf{x}'})}{(\gamma_1(z_{k+1}|\bm{I}_{\textbf{x}'}))^\alpha}$$ 
%	where
%	\begin{align*}
%	&\mathcal{X}_k = \bigg\{ \mathbf{x}_k + \Big(n \rho_0 \cos\Big( \frac{2\pi\ell}{N_d}\Big), n \rho_0 \sin \Big( \frac{2\pi\ell}{N_d} \Big) \Big): %\\ &\quad\quad\quad\quad\quad 
%  n \in\{ 0,\dots,N_\rho\}, \ell \in \{ 0, 1, \dots, N_d - 1\} \bigg\} \cap \mathcal{S} \\  & \mathcal{I}_k = \{\bm{I}_{\textnormal{closest}} (\mathbf{x}):  \mathbf{x} \in \mathcal{X}_k  \}
% \\	&\gamma_\alpha(z_{k+1} | \bm{I}_{\textbf{x}'}) = \frac{1}{N} \sum_{i=1}^N p(z_{k+1}|\bm{\theta}_k^{(i)}; \bm{I}_{\textbf{x}'})^\alpha
%	\end{align*}
%\end{algorithmic}
%\end{algorithm} 

%\subsection{Online optimization approach}
%\label{sec:online_optim_approach}
For the measurement model \eqref{quantized_measurement_model}-\eqref{eqn:quantizer}, $n(\bm{\cdot},\bm{\cdot})$ is taken as zero mean noise (not necessarily Gaussian). 
Recalling the observation in Remark \ref{remark:param_magnitudes}, we consider the following scaling of the DCT coefficients:
\begin{equation}
\label{eqn:param_scaling}
\beta_j \triangleq \big((u_j+1)^2 + (v_j+1)^2 \big) C_j,
\end{equation}
and define 
$$\bm{\beta} \triangleq (\beta_0, \dots, \beta_{\tilde{N}-1})$$
as the vector of parameters that are to be estimated. 

We first introduce some notation. 
Let $z_k$ denote the measurement,  and $(I_{x,k}, I_{y,k})$ the position index, at time/iteration $k$. For notational compactness we also denote
\begin{equation}
\label{eqn:I_x_vector}
\bm{I}_{\textbf{x},k} \triangleq (I_{x,k}, I_{y,k})
\end{equation}
and  
\begin{equation}
\label{eqn:K_vector}
\mathbf{K}(\bm{I}_{\textbf{x},k} ) \triangleq \left[\begin{array}{cccc} K_0 (\bm{I}_{\textbf{x},k} ) & K_1 (\bm{I}_{\textbf{x},k} ) &  \dots & K_{\tilde{N}-1}(\bm{I}_{\textbf{x},k} ) \end{array} \right]^T,
\end{equation}
where
\begin{equation}
\label{eqn:K_vector_components}
K_j (\bm{I}_{\textbf{x},k} ) \triangleq \frac{\alpha_x(u_j) \alpha_y(v_j) }{(u_j\!+\!1)^2 \!+\! (v_j\!+\!1)^2} \cos \Big(\frac{(2I_{x,k}+1)\pi u_j}{2 N_x} \Big) \cos \Big(\frac{(2I_{y,k}+1)\pi v_j}{2 N_y} \Big).
\end{equation}
Denote 
\begin{equation}
\label{eqn:z_all}
z_{1:k} \triangleq \{z_1, \dots, z_k\}    
\end{equation}
as the set of measurements collected up to time $k$, with corresponding position indices
\begin{equation}
\label{eqn:I_x_all}
\bm{I}_{\textbf{x},1:k} \triangleq \{(I_{x,1:k}, I_{y,1:k})\}.
\end{equation}

The idea is to recursively estimate $\bm{\beta}$ by trying to minimize a cost function 
$$J_k (\bm{\beta}; \bm{I}_{\textbf{x},1:k}, z_{1:k}) = \sum_{t=0}^k g_t (\bm{\beta}; \bm{I}_{\textbf{x},t}, z_t)$$ using online optimization techniques \cite{LesageLandryTaylorShames}.
For binary measurements \eqref{binary_measurement_model}, the following per stage cost function from \cite{LeongZamaniShames} can be used:
\begin{equation}
\label{eqn:per_stage_cost_binary}
g_t (\bm{\beta}; \bm{I}_{\textbf{x}}, z) = \left\{\begin{array}{cc} \log(1+\exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau))), & z = 0 \\ 
\log(1+\exp(-\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau))), & z = 1 
\end{array}
\right.
\end{equation}
where $\eta>0$ is a parameter in the logistic function $\ell(x) \triangleq 1/(1+\exp(\eta x))$, where larger values of $\eta$ will more closely approximate the function $\mathds{1env}(x > 0)$. The cost function \eqref{eqn:per_stage_cost_binary} is similar to cost functions used in binary logistic regression problems \cite[p.516]{CalafioreElGhaoui}. In the current work, we wish to define a cost suitable for multi-level quantized measurements. Note that there are cost functions used in multinomial logistic regression problems \cite{Murphy_book1}, however they are unsuitable for our problem as they usually involve multiple sets of parameters for each possible output $z$, whereas here we just have a single set of parameters $\bm{\beta}$. 

To motivate our cost function, let us look more closely at the binary measurements cost function \eqref{eqn:per_stage_cost_binary}. In the case where the measurement  $z_t$ at time $t$ and position index $\bm{I}_{\textbf{x},t}$ is equal to 0, the cost $g_t (\bm{\beta}; \bm{I}_{\textbf{x},t}, z_t)$ will be small if  $\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x},t}) $ is less than the quantizer threshold $\tau$, and large otherwise. Similarly, when  $z_t=1$,  $g_t (\bm{\beta}; \bm{I}_{\textbf{x},t}, z_t)$ will be small if  $\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x},t}) $ is greater than $\tau$, and large otherwise. For the case of multi-level quantized measurements  with $L$ levels  given by \eqref{eqn:quantizer}, we would like to have a cost function such that 1) when $z_t=0$, $g_t (\bm{\beta}; \bm{I}_{\textbf{x},t}, z_t)$ is small for $\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x},t}) < \tau_0$, and large otherwise, 2)  when $z_t=l, \, l \in \{1,\dots,L-2\}$, $g_t (\bm{\beta}; \bm{I}_{\textbf{x},t}, z_t)$ is small for $\tau_{l-1} \leq \bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x},t}) < \tau_l$, and large otherwise, and 3) when $z_t=L-1$,  $g_t (\bm{\beta}; \bm{I}_{\textbf{x},t}, z_t)$ is small for $\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x},t}) > \tau_{L-2}$, and large otherwise. In this paper we will choose the following per stage cost function, which can be easily checked to satisfy these three requirements:
\begin{equation}
\label{eqn:per_stage_cost_multilevel}
g_t (\bm{\beta}; \bm{I}_{\textbf{x}}, z) \triangleq \left\{\begin{array}{ll} \log(1+\exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_0))), & z = 0 \\ 
\log(1+\exp(-\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_{l-1}))) & \\
 \quad + \log(1+\exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_{l}))), & z = l \in \{1,\dots,L-2\}\\
\log(1+\exp(-\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_{L-2}))), & z = L-1. 
\end{array}
\right.
\end{equation}
We remark that \eqref{eqn:per_stage_cost_multilevel} reduces to \eqref{eqn:per_stage_cost_binary} when the measurements are binary. 

Now that the per stage cost \eqref{eqn:per_stage_cost_multilevel} has been defined, we will present the online estimation algorithm. First, the gradient of $g_t(\bm{\cdot};\bm{\cdot},\bm{\cdot})$ can be derived as 
\begin{equation}
\label{eqn:per_stage_gradient}
\nabla g_t (\bm{\beta}; \bm{I}_{\textbf{x}}, z) = \left\{\begin{array}{ll} 
\frac{\eta}{1+\exp(-\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_0))} \mathbf{K}(\bm{I}_{\textbf{x}}), & z = 0 \\
\Big( \frac{-\eta}{1+\exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_{l-1}))} & \\ \quad \quad + \frac{\eta}{1+\exp(-\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_l))} \Big) \mathbf{K}(\bm{I}_{\textbf{x}}), & z = l \in \{1,\dots,L-2\}\\
 \frac{-\eta}{1+\exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_{L-2}))} \mathbf{K}(\bm{I}_{\textbf{x}}), & z = L-1, 
\end{array}
\right.
\end{equation}
while the Hessian of $g_t(\bm{\cdot};\bm{\cdot},\bm{\cdot})$ can be derived as 
\begin{align}
& \nabla^2 g_t (\bm{\beta}; \bm{I}_{\textbf{x}}, z) \nonumber \\& = \left\{\begin{array}{ll} \frac{\eta^2 \exp(-\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_0))}{(1+\exp(-\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_0)))^2} \mathbf{K}(\bm{I}_{\textbf{x}}) \mathbf{K}(\bm{I}_{\textbf{x}})^T, & z = 0 \\
\Big( \frac{\eta^2 \exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_{l-1}))}{(1+\exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_{l-1})))^2} & \\ \quad \quad + \frac{\eta^2 \exp(-\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_l))}{(1+\exp(-\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_l)))^2} \Big)  \mathbf{K}(\bm{I}_{\textbf{x}}) \mathbf{K}(\bm{I}_{\textbf{x}})^T, & z = l \in \{1,\dots,L-2\}\\
 \frac{\eta^2 \exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_{L-2}))}{(1+\exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_{L-2})))^2} \mathbf{K}(\bm{I}_{\textbf{x}}) \mathbf{K}(\bm{I}_{\textbf{x}})^T, & z = L-1. 
\end{array}
\right. \nonumber \\
& = \left\{\begin{array}{ll} 
\frac{\eta^2 \exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_0))}{(1+\exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_0)))^2} \mathbf{K}(\bm{I}_{\textbf{x}}) \mathbf{K}(\bm{I}_{\textbf{x}})^T, & z = 0 \\
\Big( \frac{\eta^2 \exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_{l-1}))}{(1+\exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_{l-1})))^2} & \\ \quad \quad + \frac{\eta^2 \exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_l))}{(1+\exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_l)))^2} \Big)  \mathbf{K}(\bm{I}_{\textbf{x}}) \mathbf{K}(\bm{I}_{\textbf{x}})^T, & z = l \in \{1,\dots,L-2\}\\
 \frac{\eta^2 \exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_{L-2}))}{(1+\exp(\eta (\bm{\beta}^T \mathbf{K}(\bm{I}_{\textbf{x}}) - \tau_{L-2})))^2} \mathbf{K}(\bm{I}_{\textbf{x}}) \mathbf{K}(\bm{I}_{\textbf{x}})^T, & z = L-1. 
\end{array} \right.  \label{eqn:per_stage_Hessian}
\end{align}
An approximate online Newton method \cite{LeongZamaniShames} for estimating the parameters $\bm{\beta}$ is now given by:
\begin{equation}
\label{eqn:approx_ONM1}
 \hat{\bm{\beta}}_{k+1} = \hat{\bm{\beta}}_k - \left( H_k (\hat{\bm{\beta}}_k; \bm{I}_{\textbf{x},1:k}, z_{1:k})  \right)^{-1} G_k (\hat{\bm{\beta}}_k; \bm{I}_{\textbf{x},k}, z_k), 
\end{equation}
where
\begin{align}
G_k (\hat{\bm{\beta}}_k; \bm{I}_{\textbf{x},k}, z_k) & = \nabla g_k (\hat{\bm{\beta}}_k; \bm{I}_{\textbf{x},k}, z_k)  \nonumber \\
H_k(\hat{\bm{\beta}}_{k}; \bm{I}_{\textbf{x},1:k}, z_{1:k}) & =  H_{k-1}(\hat{\bm{\beta}}_{k-1}; \bm{I}_{\textbf{x},1:k-1}, z_{1:k-1}) + \nabla^2 g_k (\hat{\bm{\beta}}_k; \bm{I}_{\textbf{x},k}, z_k) \nonumber \\
H_0 (\hat{\bm{\beta}}_{0}) & = \varsigma I. \label{eqn:approx_ONM2}
\end{align}
The initialization $H_0 (\hat{\bm{\beta}}_{0}) = \varsigma I$  is a Levenberg-Marquardt type modification \cite{ChongZak} to ensure that the matrices $\{H_k\}$ are always non-singular.\footnote{In \cite{LeongZamaniShames} this is equivalently expressed as a full rank initialization  on $\left(H_0(\hat{\bm{\beta}}_{0})\right)^{-1}$.}

In the case where the field (and hence the parameters $\bm{\beta})$ is time-varying, the algorithm \eqref{eqn:approx_ONM1}-\eqref{eqn:approx_ONM2} may not be able to respond quickly to changes in $\bm{\beta}$, due to all past Hessians (including Hessians from old fields) being used in the computation of $H_k(\hat{\bm{\beta}}_{k}; \bm{I}_{\textbf{x},1:k}, z_{1:k}) $ in \eqref{eqn:approx_ONM2}. To overcome this problem, we  will introduce a \emph{forgetting factor} \cite{ManolakisIngleKogon} into the algorithm, 
where the forgetting factor $\delta$ satisfies $0 < \delta \leq 1$, and typically chosen to be close to one. The final estimation procedure is summarized as Algorithm~\ref{alg:DCT_optim_time_varying}. Compared to \eqref{eqn:approx_ONM2}, we note that the Levenberg-Marquardt modification in Algorithm \ref{alg:DCT_optim_time_varying} is done at every time step by adding $\varsigma I$ to $\tilde{H}_k$, as we found that only doing it once at the beginning can lead to algorithm instability due to exponential decay of initial conditions when using a forgetting factor. We also remark that Algorithm \ref{alg:DCT_optim_time_varying} reduces to \eqref{eqn:approx_ONM1}-\eqref{eqn:approx_ONM2} when the forgetting factor $\delta = 1$. 

\begin{algorithm}
\caption{Estimation of Fourier components using online optimization approach}
\label{alg:DCT_optim_time_varying}
\begin{algorithmic}[1]
\State \textbf{Algorithm Parameters}:  Logistic function parameter $\eta > 0$, Levenberg-Marquardt parameter $\varsigma > 0$, forgetting factor $\delta \in (0,1]$
\State \textbf{Inputs}: Initial position index $\bm{I}_{\textbf{x},0}$
\State \textbf{Outputs}: Parameter estimates $\{ \hat{\bm{\beta}}_k \}$
\State Initialize $\tilde{H}_0(\hat{\bm{\beta}}_{0}) = \mathbf{0}$
\For{$k=0,1,2,\dots,$}
	\State Update estimates 
 \begin{align*}
\hat{\bm{\beta}}_{k+1} &= \hat{\bm{\beta}}_k - \left( H_k (\hat{\bm{\beta}}_k; \bm{I}_{\textbf{x},1:k}, z_{1:k}) \right)^{-1} G_k (\hat{\bm{\beta}}_k; \bm{I}_{\textbf{x},k}, z_k) \nonumber \\
G_k (\hat{\bm{\beta}}_k; \bm{I}_{\textbf{x},k}, z_k) & = \nabla g_k (\hat{\bm{\beta}}_k; \bm{I}_{\textbf{x},k}, z_k) \nonumber \\
\tilde{H}_k(\hat{\bm{\beta}}_{k}; \bm{I}_{\textbf{x},1:k}, z_{1:k}) & =  \delta \tilde{H}_{k-1}(\hat{\bm{\beta}}_{k-1}; \bm{I}_{\textbf{x},1:k-1}, z_{1:k-1}) + \nabla^2 g_k (\hat{\bm{\beta}}_k; \bm{I}_{\textbf{x},k}, z_k) \nonumber \\
H_k(\hat{\bm{\beta}}_{k}; \bm{I}_{\textbf{x},1:k}, z_{1:k}) & = \tilde{H}_k(\hat{\bm{\beta}}_{k}; \bm{I}_{\textbf{x},1:k}, z_{1:k})  + \varsigma I, \label{eqn:approx_ONM_time_varying}
\end{align*}
\,\,\,\,\,\,\, where $\nabla g_k (\bm{\cdot}; \bm{\cdot}, \bm{\cdot})$ and $ \nabla^2 g_k(\bm{\cdot}; \bm{\cdot}, \bm{\cdot})$ are computed using \eqref{eqn:per_stage_gradient}-\eqref{eqn:per_stage_Hessian}	
	
 \State Determine  $\bm{I}_{\textbf{x},k+1} =\texttt{ActiveSensing}(\bm{I}_{\textbf{x},k}, \hat{\bm{\beta}}_{k+1})$  using Algorithm \ref{alg:active_sensing_online_opt}
\EndFor
\end{algorithmic}
\end{algorithm} 

\subsection{Measurement Location Selection Using Active Sensing}
For choosing the positions in which to take measurements, an ``active sensing'' approach \cite{Kreucher_active_sensing,LaShengChen,RisticSkvortsovGunatilaka} can be used, which aims to cleverly choose the next position given information collected so far, in order to more quickly obtain a good estimate of the field. 

In the case of binary measurements, a method for choosing the next measurement location is proposed in \cite{LeongZamaniShames}, that tries to maximize the minimum eigenvalue of an ``expected Hessian'' term $H^+(\bm{I}_{\textbf{x}'})$ over candidate future position indices $\bm{I}_{\textbf{x}'}$. Formally, the problem is:
$$\bm{I}_{\textbf{x},k+1} = \textnormal{arg} \max\limits_{\bm{I}_{\textbf{x}'} \in \mathcal{I}_{k+1}} \lambda_{\min} (H^+(\bm{I}_{\textbf{x}'})),$$
where $\lambda_{\min} (H^+(\bm{I}_{\textbf{x}'}))$ is the minimum eigenvalue of  $H^+(\bm{I}_{\textbf{x}'})$, $\mathcal{I}_{k+1}$ is the set of possible future position indices\footnote{The set $ \mathcal{I}_{k+1}$ may, e.g., capture the set of reachable positions from the current state of the mobile sensor platform.}
and
\begin{equation}
\label{eqn:expected_Hessian_binary}
\begin{split}
 H^+(\bm{I}_{\textbf{x}'}) & \triangleq H_k(\bm{\hat{\beta}}_{k}; \bm{I}_{\textbf{x},1:k}, z_{1:k})  +  \frac{ \eta^2 \exp(\eta (\bm{\hat{\beta}}_{k+1}^T \mathbf{K}(\bm{I}_{\textbf{x}'}) -  \tau)) }{\big(1+\exp(\eta (\bm{\hat{\beta}}_{k+1}^T \mathbf{K}(\bm{I}_{\textbf{x}'}) -  \tau)) \big)^2}  \mathbf{K}(\bm{I}_{\textbf{x}'}) \mathbf{K}(\bm{I}_{\textbf{x}'})^T \mathbb{P}(z' = 0) \\
 & \quad +  \frac{ \eta^2 \exp(\eta (\bm{\hat{\beta}}_{k+1}^T \mathbf{K}(\bm{I}_{\textbf{x}'}) -  \tau)) }{\big(1+\exp(\eta (\bm{\hat{\beta}}_{k+1}^T \mathbf{K}(\bm{I}_{\textbf{x}'}) -  \tau)) \big)^2}  \mathbf{K}(\bm{I}_{\textbf{x}'}) \mathbf{K}(\bm{I}_{\textbf{x}'})^T \mathbb{P}(z' = 1) \\
 & = H_k  (\bm{\hat{\beta}}_{k}; \bm{I}_{\textbf{x},1:k}, z_{1:k}) +  \frac{ \eta^2 \exp(\eta (\bm{\hat{\beta}}_{k+1}^T \mathbf{K}(\bm{I}_{\textbf{x}'}) -  \tau)) }{\big(1+\exp(\eta (\bm{\hat{\beta}}_{k+1}^T \mathbf{K}(\bm{I}_{\textbf{x}'}) -  \tau)) \big)^2}  \mathbf{K}(\bm{I}_{\textbf{x}'}) \mathbf{K}(\bm{I}_{\textbf{x}'})^T, 
\end{split}
\end{equation}
The last line of \eqref{eqn:expected_Hessian_binary} holds since $ \mathbb{P}(z' = 0)  +  \mathbb{P}(z' = 1)  = 1$, irrespective of the distribution of the noise $n(\bm{\cdot},\bm{\cdot})$. 

If we attempt to generalize \eqref{eqn:expected_Hessian_binary} to multi-level measurements, we find that there will be terms $ \mathbb{P}(z' = 0), \mathbb{P}(z' = 1), \dots,  \mathbb{P}(z' = L-1)  $ which cannot all be cancelled, and we will need to specify a noise distribution in order to compute these terms. Since exact knowledge of the noise distribution is usually unavailable in practice, we will instead consider a slightly different objective to optimize, namely a ``predicted Hessian''
\begin{equation}
\label{eqn:predicted_Hessian}
\begin{split}
 \hat{H}(\bm{I}_{\textbf{x}'}) & \triangleq H_k(\bm{\hat{\beta}}_{k}; \bm{I}_{\textbf{x},1:k}, z_{1:k})  +  \nabla^2 g_{k+1}(\bm{\hat{\beta}}_{k+1}; \bm{I}_{\textbf{x}'}, \hat{z}')
\end{split}
\end{equation}
where 
$\hat{z}' \triangleq q\big(\bm{\hat{\beta}}_{k+1}^T \mathbf{K}(\bm{I}_{\textbf{x}'})\big)$ is the predicted future measurement, with the quantizer $q(\bm{\cdot})$ given by \eqref{eqn:quantizer}. Note that \eqref{eqn:predicted_Hessian} reduces to \eqref{eqn:expected_Hessian_binary} in the case of binary measurements. We then maximize the minimum eigenvalue of the predicted Hessian to determine the next measurement location target:
\begin{equation}
\label{prob:maxmin_eig}
\bm{I}_{\textbf{x}}^{\textnormal{target}} = \textnormal{arg} \max\limits_{\bm{I}_{\textbf{x}'} \in \mathcal{I}_{k+1}} \lambda_{\min} (\hat{H}(\bm{I}_{\textbf{x}'})).
\end{equation}

For the set of candidate future position indices  $ \mathcal{I}_{k+1}$, one possible choice could be positions distributed uniformly on a grid within the search region $\mathcal{S}$. 
Once a new location target $\bm{I}_{\textbf{x}}^{\textnormal{target}} $ has been determined, we head in that direction. We will collect measurements and update $\hat{\bm{\beta}}$ along the way, where we collect a new measurement after every $\rho_0$ in distance has been travelled until $ \bm{I}_{\textbf{x}}^{\textnormal{target}}$ is reached, at which time a new location target is determined. 
%$ \bm{I}_{\tilde{k}}^{\textnormal{target}}$ is determined. The index $\tilde{k}$ is the new iteration index, which may be larger than $k+2$ if intermediate measurements have been collected on the way to $ \bm{I}_{k+1}^{\textnormal{target}}$.
The procedure is summarized in Algorithm \ref{alg:active_sensing_online_opt},  where $\bm{I}_{\textnormal{closest}} (\mathbf{x})$ denotes the closest position index $(I_x, I_y)$ to $\mathbf{x} \in \mathcal{S}$.
The condition in line \ref{line:at_target} of Algorithm \ref{alg:active_sensing_online_opt} means the location target has been reached, so that a new location target is determined and a location index $\bm{I}_{\textbf{x},k+1}$ in the direction of the new target is returned. Some random exploration is also included in the algorithm, such that the new location target is random with probability $\varepsilon$, similar to $\varepsilon$-greedy algorithms used in reinforcement learning \cite{SuttonBarto}.  The condition in line \ref{line:within_range_target} means that the agent is within $\rho_0$ of the target, which will be reached at the next time step, while the condition in line \ref{line:outside_range_target} means  the agent will continue heading towards the target and collect measurements along the way. 

%An alternative method is used in \cite{LeongZamaniShames}, where a convex combination of the old and new directions is then determined, and a distance of $\rho_0$ is travelled when a new measurement is collected, $\hat{\bm{\beta}}$ is updated, and problem \eqref{prob:maxmin_eig} solved again. 

\begin{algorithm}[t]
\caption{Active sensing algorithm for online optimization approach: $\bm{I}_{\textbf{x},k+1} = \texttt{ActiveSensing}(\bm{I}_{\textbf{x},k}, \hat{\bm{\beta}}_{k+1})$}
\label{alg:active_sensing_online_opt}
\begin{algorithmic}[1]
\State \textbf{Algorithm Parameters}: Distance $\rho_0 \geq 0$, candidate position indices $\mathcal{I}_{k+1}$, search region $\mathcal{S}$, exploration probability $\varepsilon$
\State \textbf{Inputs}:    $\bm{I}_{\textbf{x},k}$, $\hat{\bm{\beta}}_{k+1}$
\State \textbf{Output}: Next position index $\bm{I}_{\textbf{x},k+1}$
\If{$k=0$}
    \State Initialize $\bm{I}_{\textbf{x}}^{\textnormal{target}}  = \bm{I}_{\textbf{x},0}$
\EndIf
\If{$\bm{I}_{\textbf{x},k} = \bm{I}_{\textbf{x}}^{\textnormal{target}}$} \label{line:at_target}
    \State With probability $\varepsilon$, set new $\bm{I}_{\textbf{x}}^{\textnormal{target}}$ to a random location index in $\{0, \dots, N_x - 1\} \times \{0, \dots, N_y-1\}$, otherwise compute new $\bm{I}_{\textbf{x}}^{\textnormal{target}} = \textnormal{arg} \max\limits_{\bm{I}_{\textbf{x}'} \in \mathcal{I}_{k+1}} \lambda_{\min} (\hat{H}(\bm{I}_{\textbf{x}'})),$ where $\hat{H}(\bm{I}_{\textbf{x}'})$ is given by \eqref{eqn:predicted_Hessian}  \label{line:random_exploration}
    \State Set $\mathbf{x}_{k+1} = \mathbf{x}_k + \rho_0 (\mathbf{x}^\textnormal{target} - \mathbf{x}_k)/||\mathbf{x}^{\textnormal{target}} - \mathbf{x}_k||$ and return $\bm{I}_{\textbf{x},k+1} = \bm{I}_{\textnormal{closest}}(\mathbf{x}_{k+1})$
\ElsIf{$||\mathbf{x}_k - \mathbf{x}^{\textnormal{target}}|| < \rho_0$} \label{line:within_range_target}
    \State Set $\mathbf{x}_{k+1} = \mathbf{x}^{\textnormal{target}}$ and return $\bm{I}_{\textbf{x},k+1} =\bm{I}_{\textbf{x}}^{\textnormal{target}}$
\Else \label{line:outside_range_target}
    \State Set $\mathbf{x}_{k+1} = \mathbf{x}_k + \rho_0 (\mathbf{x}^\textnormal{target} - \mathbf{x}_k)/||\mathbf{x}^{\textnormal{target}} - \mathbf{x}_k||$ and return $\bm{I}_{\textbf{x},k+1} = \bm{I}_{\textnormal{closest}}(\mathbf{x}_{k+1})$
\EndIf

\end{algorithmic}
\end{algorithm} 

\section{Numerical Studies}
\label{sec:numerical}
For performance evaluation of the field estimation algorithms, we will consider two performance measures, the mean squared error (MSE) and structural similarity index (SSIM). These are defined similar to Section \ref{sec:DCT_RBF_comparison}, except that we replace the approximated field with the estimated field 
$$\hat{\phi}_d (I_x, I_y)   \triangleq \sum_{(u,v) \in \tilde{\mathcal{U}} } \alpha_x(u) \alpha_y(v) \hat{C}(u,v) \cos \left(\frac{(2I_x+1)\pi u}{2 N_x} \right) \cos \left(\frac{(2I_y+1)\pi v}{2 N_y} \right).$$


\subsection{Static Fields}
We consider estimation of the (true) field shown in Fig. \ref{fig:true_field_seed355}, with search region $\mathcal{S} = [0,100] \times [0,100]$. The field is discretized using $N_x = 100$ and $N_y = 100$. We use \eqref{eqn:U_tilde_largest} to select the largest modes that we wish to retain and estimate. 
\begin{figure}[t!]
\centering 
\includegraphics[scale=0.6]{true_field_seed355.pdf} 
\caption{Static field}
\label{fig:true_field_seed355}
\end{figure} 

We use Algorithm \ref{alg:DCT_optim_time_varying} with $\eta=5$ and $\varsigma = 1/5000$. As the field is assumed static, the forgetting factor is set to $\delta = 1$. The initial position index is set to $\bm{I}_{\textbf{x},0} = (50,50)$, close to the center of the search region~$\mathcal{S}$. 
A four level quantizer is used with quantizer thresholds $\tau_0=1, \tau_1=2, \tau_2 = 3$. The measurement noise $n(\bm{\cdot},\bm{\cdot})$ is i.i.d. Gaussian with zero mean and variance equal to 0.1. For choosing the measurement locations, we use Algorithm \ref{alg:active_sensing_online_opt} with $\rho_0 = 10$. The candidate position indices $\mathcal{I}_{k+1}$ are chosen to correspond to 36 points placed uniformly on a grid within the search region $\mathcal{S}$. The exploration probability is chosen as $\varepsilon = 0.1$.

\begin{figure}[t!]
\centering 
\includegraphics[scale=0.6]{MSE_time_stepped_seed355.pdf} 
\caption{Static field: MSE vs. $k$}
\label{fig:MSE_time_stepped_seed355}
\end{figure} 

\begin{figure}[t!]
\centering 
\includegraphics[scale=0.6]{SSIM_time_stepped_seed355.pdf} 
\caption{Static field: SSIM vs. $k$}
\label{fig:SSIM_time_stepped_seed355}
\end{figure} 

Fig. \ref{fig:MSE_time_stepped_seed355}  shows the MSE vs $k$ (corresponding to the number of measurements collected), when various numbers of modes are estimated. Fig.  \ref{fig:SSIM_time_stepped_seed355} shows the SSIM vs $k$. Each point in Figs. \ref{fig:MSE_time_stepped_seed355} and \ref{fig:SSIM_time_stepped_seed355} is obtained by averaging over 10 runs. We see from the figures that there is a trade-off between the estimation quality, number of modes/parameters that need to be estimated, and number of measurements collected. If a lot of measurements can be collected, then estimating more modes will allow for a better estimate of the field.\footnote{For example, if multiple mobile agents can be utilized \cite{LeongZamani_SP} or one has a sensor network, then more measurements can be collected in a limited amount of time.} On the other hand, if fewer measurements are available, estimating fewer modes more accurately may give a better field estimate than estimating lots of modes inaccurately.  
In Fig. \ref{fig:estimated_field_seed355} we show a sample plot of the estimated field when 80 modes are estimated, after 2000 measurements have been collected. 
\begin{figure}[t!]
\centering 
\includegraphics[scale=0.6]{estimated_field_seed355.pdf} 
\caption{Static field: Estimated field using 2000 measurements}
\label{fig:estimated_field_seed355}
\end{figure} 

\subsection{Time-varying Fields}
We now consider an example with time-varying fields. Suppose the true field is the same of that of Fig. \ref{fig:field_seed341_DCT} for the first 1000 iterations, but then switches to the true field in Fig. \ref{fig:field_seed343_DCT} for the next 1000 iterations. We will use Algorithms \ref{alg:DCT_optim_time_varying} and \ref{alg:active_sensing_online_opt}  with forgetting factor $\delta = 0.995$, with other parameters the same as in the previous example. 

Figs. \ref{fig:MSE_time_varying_seed341_343} and \ref{fig:SSIM_time_varying_seed341_343} show respectively the MSE and SSIM vs. $k$, when the 60 largest modes are estimated. 
We see that after the field changes at $k=1000$ the accuracy of the field estimate drops, but Algorithm~\ref{alg:DCT_optim_time_varying} is able to recover  and estimate the new field as more measurements are collected. 

For comparison, the MSE and SSIM obtained using forgetting factor $\delta = 1$ are also shown. In this case, as there is no forgetting of old information, the field estimates will take much longer to adjust to the new field. 

\begin{figure}[t!]
\centering 
\includegraphics[scale=0.6]{MSE_time_varying_seed341_343.pdf} 
\caption{Time varying field: MSE vs. $k$}
\label{fig:MSE_time_varying_seed341_343}
\end{figure} 

\begin{figure}[t!]
\centering 
\includegraphics[scale=0.6]{SSIM_time_varying_seed341_343.pdf} 
\caption{Time varying field: SSIM vs. $k$}
\label{fig:SSIM_time_varying_seed341_343}
\end{figure} 

\section{Conclusion}
This paper has studied the estimation of scalar fields, where a field is viewed in the Fourier domain. An algorithm has been presented for estimating the lower order modes of the field under the assumption of noisy quantized measurements. Our approach assumed an agent or agents travelling around a region in order to collect measurements. Future work will consider the use of a sensor network for field estimation, with algorithms constrained by local communication and distributed computation. 


\section*{Acknowledgment}
The authors thank Mr. Shintaro Umeki for suggesting the use of structural similarity as a performance measure while working at DST Group as a summer vacation student. 

\bibliography{IEEEabrv,source_localization}
\bibliographystyle{IEEEtran} 




\end{document}
 