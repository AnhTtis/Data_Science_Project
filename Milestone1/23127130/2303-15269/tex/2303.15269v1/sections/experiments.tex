\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{images/graph.pdf} 
    \caption{Distribution and classification of the characters in the training set of the IAM dataset (in logarithmic scale). We set a threshold on the frequency with which the characters appear equal to \num{1000} to identify the long-tail ones (indicated as a red line).\vspace{-.2cm}}
    \label{fig:iam_class_tail}
\end{figure} 

\section{Experiments}
\label{sec:experiments}

In this section, in addition to analyzing the performance in the standard styled HTG scenarios, we aim to explore the capability of our approach and of SotA ones to generate characters that are long-tail-distributed in the dataset used for training. We believe that this is a relevant aspect to consider when evaluating the HTG performance, which has been so far neglected in the literature on this task. 
Additional analysis is reported in supplementary materials.

\tit{Implementation Details}
All the experiments have been carried out on a single NVIDIA RTX 2080~Ti GPU. For pre-training the convolutional style encoder, we set the batch size to \num{32}. We use the Adam optimizer with an initial learning rate equal to $2 \times 10^-5$ and apply exponential scheduling with a decay factor equal to $10^{-1/90000}$. We stop the training with an early stopping strategy with patience \num{30}. Note that, due to the large amount of samples in the dataset, we are able to feed the convolutional backbone with almost always unseen samples before convergence. For this reason, we count an epoch every \num{1000} iterations. We employ the Adam optimizer also for training the complete HTG model on the real benchmark dataset considered in this work but fix the learning rate to $2\cdot10^{-4}$ and batch size equal to \num{8}. In this case, the training is stopped after \num{7}k epochs.

\tit{Evaluation Protocol}
For validating our proposal and comparing it as fairly as possible against SotA HTG approaches, we consider the widely-used IAM dataset~\cite{marti2002iam}, with images rescaled to have \num{64} pixels in height and proportional width. Moreover, we adopt the same evaluation procedure as in~\cite{kang2020ganwriting, bhunia2021handwriting}. In particular, the dataset contains \num{62857} English words from the Lancaster-Oslo/Bergen (LOB) corpus~\cite{johansson1978manual}, handwritten by multiple users. For this work, we consider the words written by \num{340} of those users for training and those written by the remaining \num{160} for testing. The words in the dataset are composed starting from an alphabet of \num{79} characters distributed in the training set as shown in Figure~\ref{fig:iam_class_tail}. It can be noticed that these characters appear in the dataset in a long-tail distribution: small letters are the most represented (note that `e', `t', `a', and `o' are the most common letters, which reflects the frequency in the English vocabulary), while almost all the capital letters, digits, and punctuation are rare characters in the dataset. In our experiments, we consider long-tail characters those appearing less than \num{1000} times in the training set.
We compare the proposed VATr model against the following learning-based methods for HTG. When available, we use the official implementation and weights released by the authors and evaluate all the models in the same setups. In particular, we consider the non-styled HTG ScrabbleGAN~\cite{fogel2020scrabblegan}, the one-shot styled HTG model HiGAN~\cite{gan2021higan}, the approach proposed in~\cite{davis2020text} (which we refer to as TS-GAN), and the few-shot styled methods GANwriting~\cite{kang2020ganwriting} and HWT~\cite{bhunia2021handwriting}. These two latter approaches use the same number of style examples as we use. Notably, HWT is the most closely related to our proposal for following a Transformer encoder-decoder paradigm for HTG. 
For performance evaluation, we consider the Fr√©chet Inception Distance (FID)~\cite{heusel2017gans} and the Geometry Score (GS)~\cite{khrulkov2018geometry} for measuring the visual quality of the generated images. Moreover, to further evaluate the capability of the considered methods to generate rare characters faithfully, we calculate the Character Error Rate (CER) of an HTR network trained on the IAM dataset~\cite{shi2016end} when recognizing the text in the generated images. Note that, for all the scores, the lower the value, the better.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/qualitatives.pdf}
    \caption{Qualitative comparison between our approach and the few-shot style HTG competitors in generating images with the desired textual content in the desired calligraphic style.\vspace{-0.2cm}}
    \label{fig:overall_qualitatives}
\end{figure*}

\subsection{Ablation Analysis}
First, we validate the benefits of using visual archetypes instead of one-hot vectors by replacing those latter as input to $\mathcal{D}$. The results of this analysis are reported in Table~\ref{tab:components_abl}, both when generating styled words in the whole IAM test set and when generating words containing long-tail characters. It can be observed that our approach is superior to the baseline, especially for the generation of long-tail words.
Moreover, we study the effect of the proposed synthetic pre-training strategy by comparing the performance obtained when training on real images only, and when pre-training to recognize the writers in the IAM dataset. Also these results are reported in Table~\ref{tab:components_abl} and show that the proposed synthetic pre-training brings more gain than training on real data, especially when used in combination with visual archetypes.

\begin{table}[]
    \footnotesize
    \centering
    \setlength{\tabcolsep}{.38em}
    \caption{Ablation analysis on the components of VATr.}
    \label{tab:components_abl}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c c c c}
    \toprule
    \rule{0pt}{-10pt}\textbf{Pre-training} & \textbf{Content input} & \textbf{FID (All)} & \textbf{FID (Long-Tail)}\\
    \midrule
    None         & One-hot vectors   & 18.48 & 24.93 \\ 
    Synthetic    & One-hot vectors   & 19.19 & 23.71 \\ 
    \midrule
    None         & Visual archetypes & 17.91 & 22.15 \\ 
    IAM          & Visual archetypes & 18.93 & 21.88 \\ 
    Synthetic    & Visual archetypes & 17.79 & 21.36 \\ 
    \bottomrule
    \end{tabular}}\vspace{-0.3cm}
\end{table}

\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{.32em}
\caption{Generated image quality evaluation on the IAM test set, regardless of the calligraphic style. Best results in bold.}
\label{tab:overall}
\resizebox{0.72\linewidth}{!}{
\begin{tabular}{l c cc}
\toprule
 && FID & GS\\
\midrule
ScrabbleGAN\cite{fogel2020scrabblegan}&& 20.72 & 2.56$\times$10$^{\text{-2}}$\\
HiGAN\cite{gan2021higan}&& 24.90 & 3.19$\times$10$^{\text{-2}}$\\  
TS-GAN\cite{davis2020text}&& 20.65 & 4.88$\times$10$^{\text{-2}}$\\
HWT\cite{bhunia2021handwriting}&& 19.40 & \textbf{1.01$\times$10$^{\text{-2}}$}\\
\textbf{VATr (Ours)} && \textbf{17.79} & 1.68$\times$10$^{\text{-2}}$\\
\bottomrule
\end{tabular}
}\vspace{-.2cm}
\end{table}

\subsection{Few-Shot Styled HTG}
In this section, we evaluate the capability of the proposed approach to generate realistic handwritten text images, regardless of the calligraphic style. To this end, we calculate the FID and GS on the IAM test set. The results of this study are reported in Table~\ref{tab:overall}. It can be observed that our approach gives the best FID score and is second-best in terms of GS by a small margin, suggesting the realism of its generated images. 
As for the styled HTG evaluation, we follow the procedure proposed in~\cite{kang2020ganwriting}. In particular, we calculate the FID of the generated images in comparison to the real ones for each considered writer separately and then average the scores. We perform our analysis by distinguishing four increasingly challenging scenarios, namely: 1) the IV-S case, in which we generate in-vocabulary words in styles seen during training (\ie~both style and content have been seen during training); 2) the IV-U case, in which the words to generate are in-vocabulary, but the style has never been observed; 3) the OOV-S case, in which the textual content consists of out-of-vocabulary words, but the style has been encountered during training; 4) the OOV-U case, in which both the desired style and words are unseen. The results of this analysis are reported in Table~\ref{tab:iam_scenarios}. It can be observed that our approach outperforms the competitors in all four settings by a large margin. 
Some qualitative results are reported in Figure~\ref{fig:overall_qualitatives}, which refer to the generation of a text with different unseen styles.
It is also worth noting that, thanks to our large-scale synthetic pre-training strategy, VATr is able to focus on the shape attributes of the style to reproduce rather than on the background. This results in clearer generated images that reflect the handwriting of the reference ones rather than nuisances in the background (see Figure~\ref{fig:background_qualitatives}). Note that separating the handwriting from the background can negatively affect the FID score, which works on Inception v3 features, and even more the GS since it is calculated directly on the images. 
Nevertheless, this is an interesting capability for HTG models since it makes them suitable for generating styled text that can be easily superimposed to any desired background without artifacts.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/eagle.pdf}
    \caption{Exemplar generated images from style images with background artifacts.
    \vspace{-.4cm}}
    \label{fig:background_qualitatives}
\end{figure}

\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{.32em}
\caption{Generated image quality evaluation by considering seen and unseen calligraphic style and in-vocabulary and out-of-vocabulary textual content. Best results in bold.}
\label{tab:iam_scenarios}
\resizebox{\linewidth}{!}{
\begin{tabular}{l c c c cc }
\toprule
 && IV-S & IV-U & OOV-S & OOV-U \\
\midrule
TS-GAN\cite{davis2020text} && 118.56 & 128.75 & 127.11 & 136.67\\
GANwriting\cite{kang2020ganwriting} && 120.07 & 124.30 & 125.87 & 130.68\\
HWT\cite{bhunia2021handwriting} && 106.97 & 108.84 & 109.45 & 114.10\\
\textbf{VATr (Ours)} && \textbf{88.20} & \textbf{91.11} & \textbf{98.57} & \textbf{102.22}\\
\bottomrule
\end{tabular}
}\vspace{.3cm}
\end{table}

\begin{figure}[]
    \centering
    \includegraphics[width=\columnwidth]{images/digits.pdf}
    \caption{Comparison of the images of numbers generated by our approach and HWT.
    \vspace{-.3cm}}
    \label{fig:numbers_qualitatives}
\end{figure}

\subsection{Long-Tail Characters Generation}
In this section, we focus on the capability of our visual archetypes-based approach to faithfully render rare characters. Note that the split of the IAM dataset used in training contains a total of \num{66608} word images. Among those, only \num{13064} contain at least one long-tail character. In Table~\ref{tab:longtail}, we present the performance on the generation of test strings that contain those characters, with a further evaluation on words made up of just digits. In particular, we evaluate both style and content preservation by measuring the FID and the CER. 
As can be observed from the values of the FID, SotA approaches relying on one-hot vector encodings of the content struggle to generate realistic images, especially when these contain only rare characters, as in the case of numbers. Our approach, instead, can handle such words more easily by exploiting shape similarity between the visual archetypes of the characters to render. This is confirmed by the qualitative results of the generation of numbers reported in Figure~\ref{fig:numbers_qualitatives}, where we compare our approach against HTW to better highlight the benefit of using the visual archetypes over one-hot encodings. It can be observed that the images generated by HWT do not resemble digits, while those of VATr better preserve the content.

Finally, it is worth mentioning that our approach comes with the machinery to generate, to some extent, also out-of-charset characters, \ie~unseen symbols (\eg~from other alphabets) in different handwriting styles. In particular, when those unseen symbols share visual details with the characters encountered during training (\eg~as in the case of Greek letters `$\delta$' and `$\omega$' and Latin letters `s' and `w'), our model can resort to the geometric patterns learned for the latter. Some qualitative examples of out-of-charset generation are given in Figure~\ref{fig:out_of_charset}. Although the visual quality is inferior compared to that of the seen characters, our VATr strives to generate some out-of-charset symbols.

\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{.32em}
\caption{Generated image quality evaluation by considering words containing at least one among the long-tail characters in the IAM dataset, and just numbers. The CER value calculated on real images is reported for reference. Best results in bold.}
\label{tab:longtail}
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{l c cc c cc}
\toprule
 && \multicolumn{2}{c}{All Long-Tail} && \multicolumn{2}{c}{Digits}\\
 \cmidrule{3-4} \cmidrule{6-7}
 && FID & CER && FID & CER \\
\midrule
Real Images && - & 6.21 && - & 45.80\\
\midrule
HiGAN\cite{gan2021higan}&& 26.08 & \textbf{8.63} && 129.61 & 101.53\\
HWT\cite{bhunia2021handwriting}&& 40.95 & 20.36 && 131.74 & 98.47\\
\textbf{VATr (Ours)} && \textbf{21.36} & 11.85 && \textbf{104.12} & \textbf{94.66}\\
\bottomrule
\end{tabular}
}\vspace{.3cm}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/out_of_charset.pdf}
    \caption{Generated images of some out-of-charset symbols (greek letters) in different styles.}
    \label{fig:out_of_charset}
\end{figure}