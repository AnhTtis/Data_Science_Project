\section{Related Work}
\label{sec:related}

HTG is related to the Font Synthesis task, where the desired style must be represented and used to render characters consistently~\cite{azadi2018multi, cha2020few, park2021few, xie2021dg, lee2022arbitrary}. However, Font Synthesis approaches just need to generate single characters, thus, are more closely related to HTG for ideogrammatic languages~\cite{chang2018generating, gao2019artistic, jiang2019scfont,yuan2022se}. In general, both for ideogrammatic and non-ideogrammatic languages, handwriting can be treated either as a trajectory capturing the shape of the strokes making up the characters or as a static image capturing their overall appearance. Depending on this conception, online or offline approaches to HTG can be applied.

\tit{Online HTG} 
Approaches to online HTG exploit sequential models, such as LSTMs~\cite{graves2013generating}, Conditional Variational RNNs~\cite{aksan2018deepwriting}, or Stochastic Temporal CNNs~\cite{aksan2018stcn}, to predict the pen position point-by-point based on its current position and the input text to be rendered. The first approach following this strategy was proposed in~\cite{graves2013generating} and did not have control over the style. This limitation was then addressed by following works by decoupling and then recombining content and writer's style~\cite{aksan2018deepwriting, aksan2018stcn, kotani2020generating}. Further improvements to online HTG approaches can be obtained by training the sequential model alongside a discriminator~\cite{ji2019generative}, which is a philosophy similar to SotA GAN-based offline HTG approaches. The main drawbacks of approaches following the online HTG strategy are that they struggle to learn long-range dependencies and that they require training data consisting of digital pen recordings, which are difficult to collect or even impossible to obtain for application scenarios such as historical manuscripts. In the sight of these limitations, in this work, we follow the offline HTG paradigm.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{images/model_h.pdf}
    \caption{Overview of our Visual Archetypes-based Transformer for HTG (VATr). Few-shot learning is provided by a few images of the desired calligraphic style, encoded via a convolutional backbone pre-trained on a large synthetic dataset; the output vector is passed through a Transformer encoder for creating a latent space with robust style vectors (the  \emph{Style Encoder} $\mathcal{E}$ on the left). The text to generate is rendered as a sequence of GNU Unifont binary images, representing the visual archetypes of the characters. These are the queries of a Transformer decoder to perform cross-attention with the style vectors. The resulting content-style representation is then fed to a convolutional decoder that outputs the styled handwritten text image. These last two components are our \emph{Content-Guided Encoder} $\mathcal{D}$.\vspace{-.3cm}}
    \label{fig:overview}
\end{figure*}

\tit{Offline HTG} 
Traditional offline HTG solutions~\cite{wang2005combining, lin2007style, thomas2009synthetic, haines2016my} resort to heavy human intervention for glyphs segmentation and then apply handcrafted geometric static-based feature extraction before combining those glyphs with appropriate ligatures and rendering them with texture and background blending. Other than the costly human intervention that these approaches entail, their main limitation is that they can only render the glyphs and ligatures observed for each style. More recent deep learning-based approaches, instead, are able to infer styled glyphs even if not directly observed in the style examples.
Learning-based solutions rely on GANs, either unconditioned (for non-styled HTG) or conditioned on a varying number of handwriting style samples (for styled HTG). In this latter case, style samples can be entire paragraphs or lines~\cite{davis2020text}, a few words~\cite{kang2020ganwriting, bhunia2021handwriting}, or a single word~\cite{gan2021higan, gan2022higan+, luo2022slogan}. Collecting a few handwriting samples from a writer is not much more costly than one and generally results in better performance~\cite{krishnan2021textstylebrush}. The first of these approaches was proposed in~\cite{alonso2019adversarial} and was able to generate fixed-sized images conditioned on the content embedding but with no control over the calligraphic style. Note that, different from natural image generation, generating handwritten text images should entail producing variable-sized images. Thus, the approach presented in~\cite{fogel2020scrabblegan} aims at overcoming this limitation by concatenating character images, still not being able to imitate handwriting style. 

Approaches tackling styled HTG condition the generation not only on the text content but also on a vector representation of the style~\cite{davis2020text, kang2020ganwriting, mattick2021smartpatch, gan2021higan, gan2022higan+}. In such approaches, the style and the content representations are obtained separately and then combined in a later stage for generation. This prevents those approaches from effectively capturing local writing style and patterns. This limitation is addressed by the Transformer-based approach proposed in~\cite{bhunia2021handwriting}, which is able to better capture content-style entanglement by exploiting the cross-attention mechanism between the style vector representation and the content text representation. In this work, we follow the Transformer-based paradigm for its superior capability of rendering local style patterns.

\tit{Content Representation} 
The content tokens used in the approaches mentioned above~\cite{alonso2019adversarial, fogel2020scrabblegan, davis2020text, kang2020ganwriting, mattick2021smartpatch, gan2021higan, gan2022higan+, bhunia2021handwriting} are usually independent one-hot vectors, each representing a character in a finite and generally small charset. This strategy is thus limiting due to the relatively small charset that can be handled with reasonable computational cost and is inefficient for hindering the possibility of leveraging similarities between characters. Our approach, instead, is to exploit character images as text content inputs. Note that the approaches proposed in~\cite{krishnan2021textstylebrush, luo2022slogan} feed the textual input as a whole image containing the desired text written in a typeface font. These images are then rendered in the desired style in a style-transfer fashion, also exploiting the geometry encoded in the typeface-written image for letter spacing and curvature. Different from these approaches, we input text tokens as sequences of character images rendered in the richer GNU Unifont, which is as modular as the approaches employing one-hot encodings and allows exploiting geometric similarities between characters for generation.
