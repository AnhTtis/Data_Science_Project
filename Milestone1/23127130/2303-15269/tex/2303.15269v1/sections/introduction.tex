\section{Introduction}
\label{sec:introduction}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{images/archetypes_nograph_sm.pdf} \\
    \caption{Different from previous approaches that use independent one-hot vectors as input text tokens (\eg~the State-of-the-Art HWT~\cite{bhunia2021handwriting}), we exploit \emph{visual archetypes}, \ie~geometrically-related binary images of characters. By resorting to similarities between the archetypes, we are able to generate both characters that are rarely seen during training (highlighted in red) and frequently observed ones more faithfully.\vspace{-.5cm}}
    \label{fig:first_page}
\end{figure}

Styled handwritten text generation (HTG) is an emerging research area aimed at producing writer-specific handwritten text images mimicking their calligraphic style~\cite{bhunia2021handwriting, fogel2020scrabblegan, kang2020ganwriting}. The practical applications of this research topic range from the synthesis of high-quality training data for personalized Handwritten Text Recognition (HTR) models~\cite{bhunia2021metahtr, bhunia2021text, zhang2019sequence, bhunia2019handwriting, kang2021content, kang2020distilling} to the automatic generation of handwritten notes for physically impaired people. Moreover, the writer-specific style representations that can be obtained as a by-product of models designed for this task can be applied to other tasks such as writer identification, signature verification, and handwriting style manipulation. When focusing on styled handwriting generation, simply adopting style transfer is limiting. In fact, imitating a specific writer's calligraphy does not only concern texture (\eg~the color and texture of background and ink), nor just stroke thickness, slant, skew, and roundness, but also single characters shape and ligatures. Moreover, these visual aspects must be handled properly to avoid artifacts that might result in content change (\eg~even small additional or missing strokes). 

In sight of this, specific approaches have been designed for HTG. The handwriting can be handled in the form of a trajectory (made of the underlying strokes), as done in~\cite{graves2013generating, aksan2018deepwriting, aksan2018stcn, ji2019generative, kotani2020generating}, or of an image that captures its appearance, as done in~\cite{wang2005combining, lin2007style, thomas2009synthetic, haines2016my, alonso2019adversarial, fogel2020scrabblegan, davis2020text, kang2020ganwriting, mattick2021smartpatch, gan2021higan, gan2022higan+, bhunia2021handwriting, krishnan2021textstylebrush, luo2022slogan}. The former approaches adopt online HTG strategies that entail predicting the pen trajectory point-by-point, while the latter ones are offline HTG models that output entire text images directly. We follow the offline HTG paradigm since it has the advantage, over the online one, of not requiring costly pen-recording training data, and thus, being applicable also to scenarios where the information on online handwriting is not available for a specific author (\eg~in the case of historical data) and being easier to train for not suffering of vanishing gradient and being parallelizable.

Specifically, in this work, we focus on the \emph{Few-Shot styled offline HTG} task, in which we have just a few example images of the writer's style to mimic. State-of-the-Art (SotA) approaches tackling this scenario feature an encoder that extracts writer-specific style features and a generative component, which is fed with the style features and the content representations, and produces styled text images conditioned on the desired content. These approaches usually exploit Generative Adversarial Networks (GANs~\cite{goodfellow2014generative, mirza2014conditional}), for example~\cite{alonso2019adversarial, fogel2020scrabblegan, davis2020text, kang2020ganwriting, gan2021higan, gan2022higan+, krishnan2021textstylebrush, luo2022slogan}. A more recent approach~\cite{bhunia2021handwriting} is based on an encoder-decoder generative Transformer model~\cite{vaswani2017attention} that captures character-level style variations better than previous GAN-based strategies thanks to the cross-attention mechanism between style representation and content tokens. In the approaches mentioned above, the encoding of the text content is obtained by starting from one-hot vectors, each representing a different character in a fixed charset. In this way, the characters are all independent by design. Thus, possible geometric and visual similarity among them cannot be modeled nor exploited for generation, which might result in a quality gap between the images generated by these approaches for characters that are highly represented in the training set and rare ones (\ie~long-tail characters). Moreover, for computational tractability, the fixed charset that the approaches relying on a one-hot representation of text tokens can handle is relatively small. 

\tit{Contribution}
Our proposed approach entails representing characters as \emph{continuous variables} and using them as query content vectors of a Transformer decoder for generation. In this way, the generation of characters appearing rarely in the training set (such as numbers, capital letters, and punctuation) is eased by exploiting the low distance in the latent space between rare symbols and more frequent ones (see Figure~\ref{fig:first_page}). In particular, we start from the GNU Unifont font and render each character as a $16{\times}16$ binary image, which can be considered as the \emph{visual archetype} of that character. Then, we learn a dense encoding of the character images and feed such encodings to a Transformer decoder as queries to attend the style vectors extracted by a Transformed encoder. Note that, by resorting to character images rendered in the richer GNU Unifont, which is the most complete in terms of contained Unicode characters, we can handle a huge charset (more than \num{55}k characters) seamlessly, \ie~without the need for additional parameters, as it is the case for the commonly-adopted one-hot encoding. 
Moreover, as for the style encoding part, we exploit a backbone to represent the style example images that has been pre-trained on a large synthetic dataset specifically built to focus on the calligraphic style attributes. This strategy, widely adopted for other tasks, is usually disregarded in HTG. Nonetheless, we demonstrate its effectiveness in leading to strong style representations, especially for unseen styles. We validate our proposal with extensive experimental comparison against recent generative SotA approaches, both quantitatively and qualitatively, and demonstrate the effectiveness of our proposal in generating words with both common and rare characters and in both seen and unseen styles. We call our approach VATr: Visual Archetypes-based Transformer. The code and trained models are available at \url{https://github.com/aimagelab/VATr}.
