\section{Proposed Approach}

The few-shot offline HTG problem that we tackle in this work can be formulated as follows. 
Consider a writer $\mathrm{w}{{\in}}\mathrm{W}$, for which we have $P$ samples of handwritten word images at disposal, $\mathbf{X}_{\mathrm{w}}{=}\{\mathbf{x}_{\mathrm{w},i}\}_{i=0}^P$ (in this work, following~\cite{kang2020ganwriting,bhunia2021handwriting}, we set $P{=}15$). Moreover, consider an arbitrarily long set of $Q$ text words $\mathbf{C}{=}\{\mathbf{c}_{i}\}_{i=0}^Q$, each containing an arbitrary number $n_{i}$ of characters. Our goal is to generate images $\mathbf{Y}_{\mathrm{w}}^{\mathbf{C}}$ of words with the content of the strings in $\mathbf{C}$ and the style of the writer $\mathrm{w}$ (see Figure~\ref{fig:first_page}, bottom).

\tit{Model Overview} 
We devise a Transformer encoder-decoder architecture, in combination with a pre-trained convolutional feature extractor for handling the style samples $\mathbf{X}_{\mathrm{w}}$, and rendered characters images for handling the content strings $\mathbf{C}$. First, a pre-trained convolutional feature extractor handles the style samples $\mathbf{X}_{\mathrm{w}}$ and feeds the resulting vectors to a Transformer encoder that enriches them with the long-range dependencies captured by the self-attention mechanism and outputs a sequence of style vectors $\mathbf{S}_{\mathrm{w}}$. The Transformer decoder performs cross-attention between $\mathbf{S}_{\mathrm{w}}$ and the content strings $\mathbf{C}$ to be rendered, which are represented as a sequence of their visual archetypes. The cross-attention mechanism brings to an entangled content-style representation that better captures local style patterns in addition to global word appearance. Finally, the obtained representation is fed into a convolutional decoder that generates the content and style conditioned word images $\mathbf{Y}_{\mathrm{w}}^{\mathbf{C}}$. We refer to this part of our architecture as the \emph{Content-Guided Decoder} $\mathcal{D}$. 
A schematic overview of our VATr architecture is reported in Figure~\ref{fig:overview}.

\subsection{Style Encoder}
The Style Encoder $\mathcal{E}$, which transforms the few sample images $\mathbf{X}_{\mathrm{w}}$ into the style features $\mathbf{S}_{\mathrm{w}}$, is a pipeline of a convolutional encoder and a Transformer encoder.
This choice is motivated by the data efficiency of convolutional neural networks and their ability to extract representative features and the suitability of the multi-head self-attention mechanism to model long-range dependencies in the style images.
The selected convolutional encoder backbone is a ResNet18~\cite{he2016deep}, which is a popular choice for approaches dealing with text images~\cite{javidi2020deep, zhu2020point, bhunia2021handwriting, manna2022swis}.  A novel additional characteristic is a pre-training process to obtain robust features from the style sample images. For this, we exploit a specifically built large dataset of word images rendered in calligraphic fonts. The details on the pre-training dataset are given in \textsection ~\ref{sssec:font2}.
Once pre-trained, we use the backbone to extract $P$ feature maps $\mathbf{h}_{\mathrm{w},i}{{\in}}\mathbb{R}^{h{\times}w{\times}d}$ from the $P$ style images $\mathbf{x}_{\mathrm{w},i}{{\in}}\mathbf{X}_{\mathrm{w}}$. These feature maps are then flattened along the spatial dimension to obtain a $(h{\cdot}w)$-long sequence of $d$-dimensional vectors. Note that while $h$ and $w$ depend on the input images shape, the embedding size $d$ is fixed and set equal to \num{512} in this work. The elements of this sequence represent adjacent regions of the original images, corresponding to the receptive field of the convolutional backbone.
The flattened feature maps of each style image are further concatenated to obtain the sequence $\mathbf{H}_{\mathrm{w}}{{\in}}\mathbb{R}^{N{\times}d}$, where $N{=}h{\cdot}w{\cdot}P$, which is fed into the first layer of the multi-layer multi-headed self-attention encoder. This encoder comprises $L{=}3$ layers, each with $J{=}8$ attention heads and a multilayer perceptron. The output of the last layer $\mathbf{H}^L{=}\mathbf{S}_{\mathrm{w}} {{\in}} \mathbb{R}^{N{\times}d}$ is the sequence of style features for writer $\mathrm{w}$, which is fed to the Transformer decoder in $\mathcal{D}$.

\subsubsection{Synthetic Pre-training}\label{sssec:font2}
Large-scale pre-training is an effective strategy employed in a number of learning tasks.
For HTG, the pre-training data should be abundant and should capture the shape variability of the glyphs and the texture characteristics of the ink and background.
In the sight of these considerations, to build the dataset used for pre-training the convolutional backbone, we render \numwords random words from the English vocabulary, each in \numfonts freely online available calligraphic fonts and on backgrounds randomly selected from a pool of paper-like images, thus obtaining more than 100M samples. 
To achieve better realism, we apply random transformations such as rotation and elastic deformation via the Thin Plate Spline transformation~\cite{duchon1977splines} to introduce shape variability, gaussian blur to avoid sharp borders and simulate handwriting strokes, and grayscale dilation and color jitter to simulate different ink types\footnote{Available at \url{https://github.com/aimagelab/VATr}}. 
We use the so obtained dataset to train the backbone to recognize the style of the word images by minimizing a Cross-Entropy Loss. Note that by exposing the network to such variability, we force it to extract features that are representative of the calligraphic style rather than the overall image appearance (which is influenced by the textual content, the background, and the ink type or writing tool).

\subsection{Content-Guided Decoder}
The first block of the \emph{Content-Guided Decoder} $\mathcal{D}$ is a multi-layer multi-head decoder with $L{=}3$ layers and $J{=}8$ heads as the encoder. 
The decoder performs self-attention between the content vectors representing the elements in $\mathbf{C}$, followed by cross-attention between the sequence of content vectors (treated as queries) and the style vectors $\mathbf{S}_{\mathrm{w}}$ (used as keys and values). In this way, the model can learn content-style entanglement since each query is forced to attend at the style vectors that are useful to render its final shape other than the general appearance. 

Unlike existing approaches that represent the content queries as embeddings of independent one-hot-encoded characters, in this work, we propose to exploit a representation that captures similarities between characters. In particular, we obtain the content queries as follows. 
Each content string $\mathbf{c}_{i} {{\in}} \mathbf{C}$ is made of a variable number of characters $k_i$, \ie~$\mathbf{c}_{i}{=}\{\mathbf{q}_{j}\}_{j=0}^{k_i}$. First, we render the characters in the GNU Unifont font, which, different from all other typeface fonts, contains all the Unicode characters. The rendering results in $16{\times}16$ binary images, which are then flattened and linearly projected to $d$-dimensional query embeddings, a strategy that is related to the direct use of image patches as input to Vision Transformer-like architectures~\cite{dosovitskiy2020image}. 

In Figure~\ref{fig:embeddings}, we show some exemplar visual archetypes (GNU Unifont characters) and the corresponding handwritten characters in different styles. It can be observed that the geometric similarities among the archetypes are reflected in styled characters. These similarities can be exploited for generating long-tail characters, \ie~characters that are rarely seen during training. In fact, by being fed with independent tokens as content queries, the network is forced to simply memorize content-shape relations. Not being exposed to a sufficient number of such pairs does not allow the model to learn such relations and results in unsatisfactory generation capabilities of long-tail characters. Conversely, with our image-based input, the network can learn to exploit geometric attributes and similarities between highly-represented and long-tail characters for rendering those latter, and thus, can generate them more faithfully. 
It is also worth noting that this character representation makes our model more scalable than one-hot encoding-based solutions. In fact, the query embedding layer we use has $256{\times}d$ parameters and allows us to handle a charset containing up to $2^{(16{\cdot}16)}$ characters. For handling the same amount of characters represented as one-hot vectors, the query embedding layer would have $2^{(16\cdot16)}{\times}d$ parameters.

The output of the last Transformer decoder layer for the content string $\mathbf{c}_{i}$ is a tensor $\mathbf{F}_{c_{i}}{\in}\mathbb{R}^{k_i{\times}d}$. We add normal gaussian noise to $\mathbf{F}_{c_{i}}$, to enhance variability in the generated images, and project it into a $(k_i{\times}\num{8192})$ matrix, which we then reshaped into a $512{\times}4{\times}4k_i$ tensor. This tensor is fed to a convolutional decoder consisting of four residual blocks and a $\mathrm{tanh}$ activation function that outputs the styled word images $\mathbf{Y}_{\mathrm{w}}^{\mathbf{C}}$.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{images/geometry.pdf}
    \caption{Comparison between the Unifont characters (top) and the same characters in different calligraphic styles (bottom). The geometric similarities between the characters are captured by the visual archetypes and thus can be exploited in generation.\vspace{-0.3cm}}
    \label{fig:embeddings}
\end{figure}

\subsection{Model Training}
Formally, our complete VATr model is given by $\mathcal{G}_{\theta}=\mathcal{E}{\circ}\mathcal{D}:(\mathbf{X}_{\mathrm{w}},\textbf{C}){\rightarrow}\mathbf{Y}_{\mathrm{w}}^{\mathbf{C}}$. We train it alongside other modules used to calculate the overall loss for $\mathcal{G}_{\theta}$.

The first of those modules is a convolutional discriminator $\mathcal{D}_{\eta}$, which is trained to distinguish real images from images generated by $\mathcal{G}_{\theta}$, thus forcing the generator to produce realistic images. To optimize $\mathcal{G}_{\theta}$ and $\mathcal{D}_{\eta}$ we follow the adversarial paradigm with the hinge adversarial loss~\cite{lim2017geometric}
\begin{equation*}
\begin{split}
    L_{adv}=& \mathbb{E} \left[\text{max}(1 - \mathcal{D}_{\eta}(\mathbf{X}_{\mathrm{w}}), 0)\right] + \\
    & \mathbb{E} \left[\text{max}(1 + \mathcal{D}_{\eta}(\mathcal{G}_{\theta}(\mathbf{X}_{\mathrm{w}}, \textbf{C})), 0)\right].
\end{split}
\end{equation*}

Additionally, we exploit an HTR model~\cite{shi2016end}, $\mathcal{R}_{\phi}$, which is in charge of recognizing the text in the generated images, thus forcing the generator to reproduce the desired textual content other than rendering the style. The HTR model is trained with the real images $\mathbf{X}_{\mathrm{w}}$ and their ground truth transcription, while its loss value calculated on the generated images $\mathbf{Y}_{\mathrm{w}}^{\mathbf{C}}$ is propagated through the generator $\mathcal{G}_{\theta}$. The loss of the HTR model is obtained as
\begin{equation*}
    L_{HTR}=\mathbb{E}_{\mathbf{x}}\left[ - \sum \text{log}(p(t_{\mathbf{x}} | \mathcal{R}_{\phi}(\mathbf{x})))\right],
\end{equation*}
where $\mathbf{x}$ can be either a real or a generated image, and $t_{\mathbf{x}}$ is its transcription coming from the ground truth label in case $\mathbf{x}{\in}\mathbf{X}_{\mathrm{w}}$ or from $\mathbf{C}$ in case $\mathbf{x}{\in}\mathbf{Y}_{\mathrm{w}}^{\mathbf{C}}$.

Moreover, we employ a convolutional classifier $\mathcal{C}_{\psi}$ in charge of classifying the real and generated images based on their calligraphic style (\ie~the style of writer $\mathrm{w}$), thus forcing the generator $\mathcal{G}_{\theta}$ to render the correct style. As done for the $\mathcal{R}_{\phi}$ module, also this classifier is trained with the real images, and its loss value on the generated images is used to guide the generator. Formally, the loss for this module is
\begin{equation*}
    L_{class}=\mathbb{E}_{\mathbf{x}}\left[ - \sum \text{log}(p(\mathrm{w} | \mathcal{C}_{\psi}(\mathbf{x})))\right].
\end{equation*}
Also in this case, $\mathbf{x}{\in}\mathbf{X}_{\mathrm{w}}$ or $\mathbf{x}{\in}\mathbf{Y}_{\mathrm{w}}^{\mathbf{C}}$.

To further enforce the generation of images in the desired style, we use an additional regularization loss, namely the cycle consistency loss given by:
\begin{equation*}
    L_{cycle}=\mathbb{E} \left[ \left\lVert \mathcal{E}(\mathbf{X}_{\mathrm{w}}) - \mathcal{E}(\mathbf{Y}_{\mathrm{w}}^{\mathbf{C}}) \right\rVert_1 \right].
\end{equation*}
The rationale is to force the generator to produce styled images for which the encoder $\mathcal{E}$ would extract the same style vectors. In other words, we want the style features of the input images to be preserved in the generated ones.

Overall, the complete objective function we use to train our model is given by combining the above loss terms equally weighed, \ie
\begin{equation*}
    L = L_{adv} + L_{HTR} +  L_{class} + L_{cycle}.
\end{equation*}
For an analysis of the role of each loss term on the performance, we refer to the Supplementary material.