\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{pifont}
\usepackage{enumitem}
\usepackage{soul}

\usepackage{dblfloatfix}
\usepackage{siunitx}
\usepackage{balance}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\def \ie {\emph{i.e.},}
\def \eg {\emph{e.g.},}
\def \etal {\emph{et al.}}
\def \wrt {\emph{w.r.t.}}
\def \etc {\emph{etc.}}

\newcommand{\tit}[1]{\smallbreak\noindent\textbf{#1.}}
\newcommand{\medtit}[1]{\medbreak\noindent\textbf{#1.}}
\newcommand{\bigtit}[1]{\bigbreak\noindent\textbf{#1.}}
\newcommand{\tinytit}[1]{\noindent\textbf{#1.}}
\newcommand{\tinytextit}[1]{\noindent\textit{#1:}}
\newcommand{\mbb}[1]{\mathbf{#1}}

% Variables
\newcommand{\netname}{HWViT}
\newcommand{\dataname}{$\text{Font}^2$}
\newcommand{\numfonts}{\num{10400} }
\newcommand{\numwords}{\num{10400} }
\newcommand{\numsamples}{108160000}
\newcommand{\todo}{\textcolor{purple}{\textbf{TODO} }}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Handwritten Text Generation from Visual Archetypes\\Supplementary Material}

\author{Vittorio Pippi, Silvia Cascianelli, Rita Cucchiara\\
University of Modena and Reggio Emilia\\
Via Pietro Vivarelli, 10, Modena (Italy)\\
{\tt\small \{name.surname\}@unimore.it}
}
\maketitle

%%%%%%%%% ABSTRACT
In this document, we present additional experimental analysis of our proposed approach for Few-Shot HTG, VATr. In particular, we show 
an ablation analysis on the role of each of the used loss terms in \textsection\ref{sec:losses}; 
additional details about the synthetic pre-training dataset in \textsection~\ref{sec:dataset}; 
additional qualitative results in \textsection~\ref{sec:qualitatives}; 
a comparison of the charsets generated in different styles in \textsection~\ref{sec:charsets}; 
further analysis of the long-tail characters generation scenario in \textsection~\ref{sec:long-tail}; 
examples of the generation of some out-of-charset characters from other alphabets in \textsection\ref{sec:outofcharset}; 
and some HTR results obtained by training both on real and VATr-generated images in \textsection\ref{sec:outofcharset}.

\section{Role of the Loss Function Terms}\label{sec:losses}
We analyze the effect of each loss term, both quantitatively and qualitatively. The results of this study are reported in Table~\ref{tab:losses_abl} and show that our model needs the included regularization terms to be trained properly. Moreover, when none of the two terms enforcing handwriting style faithfulness (\ie~the writer classification loss $L_{class}$ and the cycle loss $L_{cycle}$) is used, VATr generates images of readable words but does not render the style of the reference images. Conversely, when the text recognition loss $L_{HTR}$ is not employed, it renders "scrabbles" whose overall appearance resembles that of the reference style images.

\section{Synthetic Pre-training Dataset}\label{sec:dataset}
The synthetic dataset used to pre-train the convolutional backbone of the encoder is obtained by combining calligraphic fonts and words to generate word images. The process involves selecting \num{10400} fonts from dedicated websites, ensuring that fonts with small caps and decorative elements such as hearts or stars are discarded. Next, \num{10400} words of varying lengths are randomly chosen from the English vocabulary. All possible combinations between the selected fonts and words are then rendered, resulting in a total of $\num{10400} \times \num{10400} = \num{108160000}$ word images. Some exemplar images are reported in Figure~\ref{fig:font2}.

\section{Additional Styled HTG Qualitative Results}\label{sec:qualitatives}
In Figures~\ref{fig:iam_iv_s}-\ref{fig:iam_oov_u}, we report qualitative examples of images generated with the proposed VATr compared to GANwriting~\cite{kang2020ganwriting} and HWT~\cite{bhunia2021handwriting} in the four styled generation IV-S, IV-U, OOV-S, and OOV-U scenarios, respectively. It can be noticed that the style fidelity in the images produced by VATr does not deteriorate significantly when the style examples are unseen compared with the case in which they have been seen in training. It can be also noticed that VATr consistently disregards the background while reproducing the style, while the HWT competitor reproduces the background artifacts, especially in the unseen style scenarios.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{images/qualitatives_font2.pdf} 
    \caption{Exemplar images from the synthetic dataset used for pre-training the style encoder.}
    \label{fig:font2}
\end{figure} 

\begin{figure*}
    \textbf{IV-S scenario} \\ \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/IV_S/qualitativi_4_scenari008000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/IV_S/qualitativi_4_scenari002000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/IV_S/qualitativi_4_scenari009000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/IV_S/qualitativi_4_scenari004000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/IV_S/qualitativi_4_scenari010000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/IV_S/qualitativi_4_scenari006000.pdf} 
    \caption{Exemplar qualitative results of styled text generation for the IV-S setting on the IAM dataset.}
    \label{fig:iam_iv_s}
\end{figure*}

\begin{figure*}
    \textbf{IV-U scenario} \\ \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/IV_U/qualitativi_4_scenari001000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/IV_U/qualitativi_4_scenari002000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/IV_U/qualitativi_4_scenari003000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/IV_U/qualitativi_4_scenari004000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/IV_U/qualitativi_4_scenari005000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/IV_U/qualitativi_4_scenari006000.pdf} 
    \caption{Exemplar qualitative results of styled text generation for the IV-U setting on the IAM dataset.}
    \label{fig:iam_iv_u}
\end{figure*}

\begin{figure*}
    \textbf{OOV-S scenario} \\ \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/OOV_S/qualitativi_4_scenari008000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/OOV_S/qualitativi_4_scenari002000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/OOV_S/qualitativi_4_scenari003000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/OOV_S/qualitativi_4_scenari004000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/OOV_S/qualitativi_4_scenari005000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/OOV_S/qualitativi_4_scenari006000.pdf} 
    \caption{Exemplar qualitative results of styled text generation for the OOV-S setting on the IAM dataset.}
    \label{fig:iam_oov_s}
\end{figure*}

\begin{figure*}
    \textbf{OOV-U scenario} \\ \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/OOV_U/qualitativi_4_scenari007000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/OOV_U/qualitativi_4_scenari002000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/OOV_U/qualitativi_4_scenari003000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/OOV_U/qualitativi_4_scenari004000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/OOV_U/qualitativi_4_scenari005000.pdf} \\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/OOV_U/qualitativi_4_scenari006000.pdf} 
    \caption{Exemplar qualitative results of styled text generation for the OOV-U setting on the IAM dataset.}
    \label{fig:iam_oov_u}
\end{figure*}


\section{Styled Charsets Generation}\label{sec:charsets}
In Figure~\ref{fig:charsets}, we report qualitative examples of styled generation of the characters in the IAM dataset, sorted by the frequency in which they appear in the training set. Also in this case, we compare VATr against HWT and GANwriting, which have all been trained on the same data. It has to be noted that, by design, GANwriting cannot generate characters that are not letters. On the other hand, HWT includes also punctuation and digits in its charset. Nonetheless, it can be noticed that VATr generates images for a larger number of characters, including long-tail ones, with which both the competitors struggle.

\begin{table}[]
    \footnotesize
    \centering
    \setlength{\tabcolsep}{.38em}
    \renewcommand{\arraystretch}{0.5}
    \caption{Ablation analysis on the loss terms. The qualitative examples refer to the generation of the word \texttt{that} in two different styles (indicated on top).}
    \label{tab:losses_abl}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c c c c c c  c c}
    \toprule
    \textbf{$L_{adv}$} & \textbf{$L_{HTR}$} & \textbf{$L_{class}$} & \textbf{$L_{cycle}$} & \textbf{FID} & & \includegraphics[height=0.3cm]{images/rebuttal/ref1.png} & \includegraphics[height=0.3cm]{images/rebuttal/ref2.png}\\
    \midrule
    \cmark &        &        &        & 224.87 & & \includegraphics[height=0.3cm]{images/rebuttal/D_v1.png} & \includegraphics[height=0.3cm]{images/rebuttal/D_v2.png} \\ 
    \cmark &        &        & \cmark & 261.23 & & \includegraphics[height=0.3cm]{images/rebuttal/D_C_v2.png} & \includegraphics[height=0.3cm]{images/rebuttal/D_C_v1.png} \\ 
    \cmark &        & \cmark &        & 255.11 & & \includegraphics[height=0.3cm]{images/rebuttal/D_W_v1.png} & \includegraphics[height=0.3cm]{images/rebuttal/D_W_v2.png} \\ 
    \cmark &        & \cmark & \cmark & 220.83 & & \includegraphics[height=0.3cm]{images/rebuttal/D_C_W_v2.png} & \includegraphics[height=0.3cm]{images/rebuttal/D_C_W_v1.png} \\ 
    \cmark & \cmark &        &        &  46.61 & & \includegraphics[height=0.3cm]{images/rebuttal/D_O_v1.png} & \includegraphics[height=0.3cm]{images/rebuttal/D_O_v2.png} \\ 
    \cmark & \cmark &        & \cmark &  41.68 & & \includegraphics[height=0.3cm]{images/rebuttal/D_O_C_v1.png} & \includegraphics[height=0.3cm]{images/rebuttal/D_O_C_v2.png} \\ 
    \cmark & \cmark & \cmark &        &  18.70 & & \includegraphics[height=0.3cm]{images/rebuttal/D_O_W_v1.png} & \includegraphics[height=0.3cm]{images/rebuttal/D_O_W_v2.png} \\ 
    \cmark & \cmark & \cmark & \cmark &  \textbf{17.79} & & \includegraphics[height=0.3cm]{images/rebuttal/D_O_C_W_v1.png} & \includegraphics[height=0.3cm]{images/rebuttal/D_O_C_W_v2.png} \\ 
    \bottomrule
    \end{tabular}}
\end{table}

\begin{figure*}
    \includegraphics[width=\textwidth]{images/alph.pdf}
    \caption{Exemplar IAM charsets generated in different styles sorted by frequency.}
    \label{fig:charsets}
\end{figure*}

\section{Additional Long-tail Character Generation Results}~\label{sec:long-tail}
We deepen our analysis on the generation of test words containing rare characters in Figure~\ref{fig:iam_FID_LT}, where we compare VATr against HWT in terms of the FID obtained by changing the Long-tail threshold. This is the value of the frequency of appearance in the training set for which a character can be considered as long-tail. Recall that in the experiments in the main paper we set this threshold to 1000, which was determined by observing the character distribution in the IAM training set, reported both in logarithmic and linear scale in Figure~\ref{fig:combo_graph}. 
It can be noticed that the FID obtained by VATr is generally lower than that obtained by HWT, especially at lower threshold values. This demonstrates the robustness of our approach of exploiting dense representations of the characters, compared to resorting to one-hot vectors.
It can be noticed an increase in the FID of both approaches when the threshold is around \num{3500}. Setting the threshold to this value means excluding almost all the words containing only small letters. Afterward, the percentage of capital letters, digits, and punctuation becomes higher than that of small letters in the generated words. Thus, the capacity to faithfully generate also those symbols is more relevant for those words. 

\begin{figure*}
    \centering
    \includegraphics[width=0.75\textwidth]{images/long_tail_graph.pdf} 
    \caption{FID score on words containing long-tail characters with respect to the threshold set to consider a character as rare in the IAM dataset (the x-axis is in logarithmic scale).}
    \label{fig:iam_FID_LT}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{images/combo_graph.pdf} 
    \caption{Distribution and classification of the characters in the training set of the IAM dataset in linear scale (top) and logarithmic scale (bottom).}
    \label{fig:combo_graph}
\end{figure} 

\section{Additional Out-of-Charset results}\label{sec:outofcharset}
In Figure~\ref{fig:ooc}, we report examples of the generation of out-of-charset characters from non-Latin alphabets (Greek and Coptic) with our proposed approach. Despite this setting being beyond the scope of our work, it can be noticed that VATr is able to reproduce symbols that are not present in its training set, especially in the case these are geometrically similar to in-charset symbols. In fact, when generating Greek letters that are close to Latin ones starting from their respective visual archetypes, VATr can exploit learned geometric regularities. On the other hand, it struggles to generate characters from the Copto alphabet, whose visual archetypes are very different from those of Latin letters.

\section{Styled HTG for HTR}\label{sec:htr}
One of the main applications of styled HTG is providing training data for HTR models to be applied to writer-specific manuscripts. To assess the potential benefits of this strategy when using VATr to improve an HTR model in this setting, we consider  the interesting case of two low-resource single-author HTR datasets (Saint Gall~\cite{fischer2011transcription} and Washington~\cite{fischer2012lexicon}\footnote{\url{https://fki.tic.heia-fr.ch/databases/iam-historical-document-database}}) and generate synthetic training lines with VATr. We then use these as additional samples to train a SotA HTR model~\cite{cojocaru2020watch,cascianelli2022boosting} and compare the transcription results against a baseline not exploiting synthetic data and the baseline exploiting synthetic data generated with the one-hot vectors-based HWT~\cite{bhunia2021handwriting}. These results are reported in Table~\ref{tab:htr}, expressed in terms of Character Error Rate (CER) and Word Error Rate (WER). It can be observed that training on VATr-generated images reduces the errors \wrt both the compared approaches, especially in terms of WER. This suggests that the misrecognized characters are more concentrated in single words.

\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{.32em}
\caption{Performace comparison of an HTR model trained on real data only and on a combination of real and generated styled text images (obtained with our approach and the SotA HWT approach) on two writer-specific datasets.}
\label{tab:htr}
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{l c cc c cc}
\toprule
 && \multicolumn{2}{c}{Saint Gall} && \multicolumn{2}{c}{Washington}\\
 \cmidrule{3-4} \cmidrule{6-7}
 && CER & WER && CER & WER \\
\midrule
Real && \textbf{4.5} & 32.5 && 3.4 & 15.9 \\
\midrule
HWT + Real && 4.6 & 31.2 && 3.7 & 16.5 \\
VATr + Real && \textbf{4.5} & \textbf{30.9} && \textbf{3.0} & \textbf{13.1} \\

\bottomrule
\end{tabular}
}\vspace{0.45cm}
\end{table}

\begin{figure*}
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{images/alph_ooc_greek_copto.pdf} \\
    \caption{Exemplar out-of-charset symbols generated in different styles. The alphabets used are Greek (left) and Coptic (right).}
    \label{fig:ooc}
\end{figure*}

%%%%%%%%% REFERENCES
{\small
\balance
\bibliographystyle{ieee_fullname}
\bibliography{supplementary_main}
}

\end{document}
