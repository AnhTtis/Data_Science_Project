% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbding}
%\usepackage{axessibility}

%\renewcommand{\thefigure}{\Roman{figure}}
%\renewcommand{\thetable}{\Roman{table}}
\renewcommand\thesection{\Alph{section}}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{6662} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}
\newcommand{\lichongyi}[1]{\textbf{\color{cyan}(CL: {#1})}}
\newcommand{\shangchen}[1]{\textbf{\color{blue}(SC: {#1})}}
\newcommand{\todo}[1]{\textbf{\color{red}(TO-DO: {#1})}}
\newcommand{\yuekun}[1]{\textbf{\color{green}(yuekun: {#1})}}
\newcommand{\dataset}{\textit{\text{BracketFlare}}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE

\title{Nighttime Smartphone Reflective Flare Removal \\Using Optical Center Symmetry Prior \\(Supplementary Material)}

\author{
Yuekun Dai$\,\,\,\,$ Yihang Luo $\,\,\,\,$ Shangchen Zhou $\,\,\, $Chongyi Li\thanks{Corresponding author.}  $\,\,\, $ Chen Change Loy \\
S-Lab, Nanyang Technological University \\
\texttt{\small \{YDAI005, c200211, s200094, chongyi.li, ccloy\}@ntu.edu.sg}\\ \vspace{-6mm}
%{\tt\small \url{https://Nukaliad.github.io/projects/Flare7K}}
}
\maketitle
\iffalse
\fi
%%%%%%%%% BODY TEXT


\section{Comparison with Previous Pipelines}

To prove the performance of our dataset, we retrain Wu~\textit{et~al}.~\cite{wu2021train} and Dai~\textit{et~al}.~\cite{dai2022flare7k} on our BracketFlare dataset and conduct additional experiments as shown in Table~\ref{tab:comparison_previous}.
%
Wu~\textit{et~al}.~\cite{wu2021train} use U-Net as a baseline model, but we find it struggled to locate reflective flares without our prior, sometimes reducing PSNR compared to the input.
%
Dai~\textit{et~al}.~\cite{dai2022flare7k} set Uformer as a baseline. Since it does not encode the prior into the network, it has an obvious gap between our baseline method.
%
To show these methods' effects on real data, we also conduct a user study with 20 real-captured flare-corrupted images and 20 participants.
%
The participants are asked to identify which of these models did best at removing the reflective flares.
%
The experiments presented in Table.~\ref{tab:user_study2} illustrates that users strongly prefer our methods over previous methods.

\begin{table}[h]
  \centering
  \vspace{-3mm}
  \caption{Comparison of previous models retrained on our dataset.}
  \vspace{-3mm}
  \resizebox{1.0\linewidth}{!} {
  \begin{tabular}{@{}lcccc@{}}
    \toprule
    Model (Retrained) & PSNR & SSIM & LPIPS & Masked PSNR \\
    \midrule
    Input  &37.30 &0.990 &0.025 &21.68 \\
    Wu et al.~\cite{wu2021train} &31.84 &0.912 &0.032 &24.87 \\
    Dai et al.~\cite{dai2022flare7k} &42.70 &0.987 &0.010 &27.46 \\
    Ours &\textbf{48.41}	&\textbf{0.994} &\textbf{0.004} &\textbf{32.09}\\
    \bottomrule
  \end{tabular}
  }
  \vspace{-6mm}
  \label{tab:comparison_previous}
\end{table}

\begin{table}[t]
  \centering
  \caption{Percentage of users favoring our method and other retrained models.} 
  \vspace{-3mm}
  \resizebox{0.7\linewidth}{!} {
  \begin{tabular}{@{}lcccc@{}}
    \toprule
    Method & Wu et al.~\cite{wu2021train} & Dai et al.~\cite{dai2022flare7k}& Ours \\
    \midrule
    Indoor &0.5\% &3.5\%&\textbf{96\%} \\
    Outdoor &1.5\% &2.5\%&\textbf{96\%} \\
    \bottomrule
  \end{tabular}
  }
  \vspace{-4mm}
  \label{tab:user_study2}
\end{table}


\section{Flare Removal for Downstream Tasks}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.49\textwidth]{Rebuttal/Seg2.pdf}
   \vspace{-6mm}
   \caption{Visual comparison of estimated segmentation for real-world flare-corrupted and flare-removed image pairs. The segmentation maps are calculated by DANNet~\cite{wu2021dannet}, a nighttime semantic segmentation algorithm. }
   \vspace{-7mm}
   \label{fig:seg}
\end{figure}

Fig.~\ref{fig:seg} demonstrates that it can benefit downstream tasks such as image segmentation. 
%
In the figure, it can be observed that the reflective flares are misclassified as a pole, which may pose potential risks for nighttime driving. 
%
Removing such reflective flares can help achieve more robust and reliable results, and ultimately benefit the users.

\begin{figure*}[h]
  \centering
   \includegraphics[width=0.9\linewidth]{Supp_Fig3/Supp_fig3_4.pdf}
   \caption{Our reflective flare removal method can help achieve overexposed region restoration. As shown in  (c), the details of the light source can be reconstructed well by referring to the estimated flare.}
   \vspace{-3mm}
   \label{fig:HDR}
\end{figure*}


\section{Potential in Overexposure Restoration}
%
Due to the limited dynamic range of some cameras, image details especially in light source regions are always saturated and difficult to recover.
%
To achieve the details of the overexposed regions, the mainstream solution is to use HDR (high-dynamic range) imaging with multi-exposure capture~\cite{debevec2008recovering, reinhard2010high}.
%
%Multiple exposures make it difficult for existing HDR algorithms to achieve real-time and tackle the misalignment.
%
Our main idea may provide a potential solution to this issue.

Specifically, since smartphone reflective flare can be considered as a short-exposure image, our method also provides the potential in implementing overexposed regions restoration.
%
As shown in Fig.~\ref{fig:HDR}, we can separate a reflective flare and rotate the estimated flare 180 degrees around the optical center to match the flare with the light source.
%
We suppose the exposure step between the flare and the original input is 12 EV.
%
Then, these two images with low-dynamic ranges will be merged to generate an HDR image with clear details in the light source.
%
The visualization results in Fig.~\ref{fig:HDR} show that our method can recover the details of the saturated regions well.


\section{More Visual Results}
%iPhone
To further demonstrate the effectiveness of our dataset, 
we retrain different neural networks including Uformer~\cite{Uformer}, Restormer~\cite{Restormer}, HINet~\cite{HINet}, and MPRNet~\cite{MPRNet} using our proposed dataset. 
%
The visual results of different networks for restoring real-world images are presented in Fig.~\ref{fig:different_nets}. 
From the visual comparison, we can observe that all the networks retrained on our dataset can remove the flares. Among them, 
MPRNet~\cite{MPRNet}  obtains the best performance on flare-corrupted regions, as highlighted in the red boxes of  Fig.~\ref{fig:different_nets}.
%
The results manifest the effectiveness of our proposed dataset.


To show our dataset and prior's generalization ability in different real-world scenes, we show more results of the flare-corrupted images captured by iPhone 13 Pro that is known for severe reflective flares in Fig.~\ref{fig:more_results1} and Fig.~\ref{fig:more_results2}.
%
The results show that our proposed method can tackle a huge diversity of light sources and achieve good visual performance in different scenes.
%
It is owing to the versatility of our proposed prior, as well as the diversity and balanced distribution of our proposed dataset.

We also capture many flare-corrupted video clips by Huawei P40, iPhone 13 Pro, and ZTE Axon 20 5G.
%
To prevent triggering the anti-shake module of the smartphone, we try to keep the smartphone from large movement.
%
Then, we process these videos with our method frame by frame.
%
The video results can be found in a separate video demo.
%
The video demo shows that our method can achieve robust and consistent flare-removal results even for real-world videos.

\section{Limitation}
%
As shown in Fig. 10 of the main paper, our method can generalize well to different types of smartphones. 
%
However, it is based on the fact that most smartphone cameras satisfy the optical center symmetry prior.
%
For some professional cameras, this prior does not always hold.
%
Besides, due to the dispersion of thick lenses in professional cameras, the main flare spot of the professional cameras are not always the same as the light source's pattern.
%
The solutions for these cases will be left for future work.

\begin{figure*}[t]
  \centering

   \includegraphics[width=0.9\linewidth]{Supp_Fig1/Fig1.pdf}
    \vspace{-8mm}
   \caption{Visual comparison of different networks retrained on our dataset for restoring the real-world flare-corrupted images. These image restoration networks include Uformer~\cite{Uformer}, Restormer~\cite{Restormer}, HINet~\cite{HINet}, and MPRNet~\cite{MPRNet}.}
   \label{fig:different_nets}
\end{figure*}

\begin{figure*}[t]
  \centering

   \includegraphics[width=0.90\linewidth]{Supp_Fig2/Fig2_1_change.pdf}
   \vspace{-20mm}
   \caption{Our results on real-world nighttime flare-corrupted images. In this figure, we use MPRNet~\cite{MPRNet} as  the baseline method to estimate the flare and output image. }
   \label{fig:more_results1}
\end{figure*}

\begin{figure*}[t]
  \centering

   \includegraphics[width=0.90\linewidth]{Supp_Fig2/Fig2_more.pdf}
   \vspace{-20mm}
   \caption{More results on real-world nighttime flare-corrupted images. In this figure, We use MPRNet~\cite{MPRNet} as the baseline method to estimate the flare and output image.}
   \label{fig:more_results2}
\end{figure*}


%\clearpage

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
