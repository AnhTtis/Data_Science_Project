\section{Results and discussion}\label{S:SectionIV}

\subsection{Model Estimation performance}

\subsubsection{Thin Layer Drying models}
The MC estimates for the baseline thin layer drying models are shown in Fig. \ref{fig:Thin_layer_drying_predictions_WIC} and \ref{fig:Thin_layer_drying_predictions_NIC}. None of the thin layer drying models are able to satisfyingly estimate the MC along the entire drying range. The models trained on the entire data range, the WIC models, are able to correctly estimate the mean of the ICD, however the models exhibit bias as can be seen by the coefficient of determination in Table \ref{tbl:results:prediction-performance} and Fig. \ref{fig:Thin_layer_drying_predictions_WIC}. The WIC models are biased towards higher MC estimates for low MC measurements, and lower MC estimates for high experimental MC.

The Midilli et al. models as seen in Fig. \ref{fig-results-midilli_WIC_preds} and \ref{fig-results-midilli_NIC_preds}, and the Henderson NIC model Fig. \ref{fig-results-henderson_NIC_preds} are able to correctly estimate the mean of the MC of the ECD. However it is unable to deal with the large variance of the underlying data, as can be seen by the high MSE and STD measurements in Table \ref{tbl:results:prediction-performance}, thus rendering the estimation methods unsuitable for practical applications. 

The thin layer drying models fitted only on the ECD, are generally unable to estimate the ICD. The performance of MC estimation of the Lewis and Page models both improve significantly by only fitting to the ECD, whereas the more complex Two Term-, Henderson-, Logarithmic- and Midilli et al. models perform significantly better when trained including the ICD. This is as expected as these models are designed in order to improve MC estimation along the entire drying range in contrast to the older Lewis and Page models which were designed for the falling rate period. 





\begin{figure*}[]
    \vspace{-2.5cm}
    \centering
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-newton_WIC-predictions_normalised.pdf} \label{fig-results-newton_WIC_preds}}  \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-modified_page_WIC-predictions_normalised.pdf} \label{fig-results-modified_page_WIC_preds}} \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-two_term_WIC-predictions_normalised.pdf} \label{fig-results-two_term_WIC_preds}}  
    \\[-0.2ex]
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-henderson_WIC-predictions_normalised.pdf} \label{fig-results-henderson_WIC_preds}} \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-logarithmic_WIC-predictions_normalised.pdf} \label{fig-results-logarithmic_WIC_preds}} \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-midilli_WIC-predictions_normalised.pdf} \label{fig-results-midilli_WIC_preds}} 
    \caption{Test fold results of model MC estimates for semi-empirical thin-layer drying models fitted using both ECD and ICD. Blue line indicates perfect estimation. Error bars represent the standard error of the mean, for the five times repeated cross-validation trials.}
    \label{fig:Thin_layer_drying_predictions_WIC}
    \vspace{-2.5cm}
\end{figure*}


\begin{figure*}[]
    \centering
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-newton_NIC-predictions_normalised.pdf} \label{fig-results-newton_NIC_preds}}  \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-modified_page_NIC-predictions_normalised.pdf} \label{fig-results-modified_page_NIC_preds}} \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-two_term_NIC-predictions_normalised.pdf} \label{fig-results-two_term_NIC_preds}}  
    \\[-0.2ex]
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-henderson_NIC-predictions_normalised.pdf} \label{fig-results-henderson_NIC_preds}} \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-logarithmic_NIC-predictions_normalised.pdf} \label{fig-results-logarithmic_NIC_preds}} \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-midilli_NIC-predictions_normalised.pdf} \label{fig-results-midilli_NIC_preds}} 
    \caption{Test fold results of model MC estimates for semi-empirical thin-layer drying models trained fitted using only ECD. Teal line indicates perfect estimation. Blue markers indicates the mean estimate and error bars represent the standard error of the mean, for the five times repeated cross-validation trials. Error bars are best seen on an electronic device with a zoom function. Fig. \ref{fig-results-two_term_NIC_preds}, \ref{fig-results-henderson_NIC_preds}, \ref{fig-results-logarithmic_NIC_preds}, and \ref{fig-results-midilli_NIC_preds} estimates ICD above 100 and results are thus not shown.} 
    \label{fig:Thin_layer_drying_predictions_NIC}
    \vspace{-0.5cm}
\end{figure*}



\begin{figure*}[]
    \vspace{-0.5cm}
    \centering
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-ANN_WIC-predictions_normalised.pdf} \label{fig-results-ANN_WIC_preds}}  \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-rfr_WIC-predictions_normalised.pdf} \label{fig-results-RFR_WIC_preds}} \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-svr_WIC-predictions_normalised.pdf} \label{fig-results-SVR_WIC_preds}}  
    \\[-2ex]
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-PLS_WIC-predictions_normalised.pdf} \label{fig-results-PLS_WIC_preds}} \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-anfis_WIC-predictions_normalised.pdf} \label{fig-results-ANFIS_WIC_preds}} 
    \caption{Test fold results of model MC estimates for machine learning models trained on both ECD and ICD. Teal line indicates perfect estimation. Blue markers indicates the mean estimate and error bars represent the standard error of the mean, for the five times repeated cross-validation trials. Error bars are best seen on an electronic device with a zoom function.}
    \label{fig:data_driven-results_scatter_WIC}
    \vspace{-0.6cm}
\end{figure*}



\begin{figure*}[]
    \centering
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-ANN_NIC-predictions_normalised.pdf} \label{fig-results-ANN_NIC_preds}}  \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-rfr_NIC-predictions_normalised.pdf} \label{fig-results-RFR_NIC_preds}} \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-svr_NIC-predictions_normalised.pdf} \label{fig-results-SVR_NIC_preds}}  
    \\
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-PLS_NIC-predictions_normalised.pdf} \label{fig-results-PLS_NIC_preds}} \,
    \subfloat[]{\includegraphics[width=2in]{figures/results-errorbar-anfis_NIC-predictions_normalised.pdf} \label{fig-results-ANFIS_NIC_preds}} 
    \caption{Test fold results of model MC estimates for machine learning models trained only on ECD. Teal line indicates perfect estimation. Blue markers indicates the mean estimate and error bars represent the standard error of the mean, for the five times repeated cross-validation trials. Error bars are best seen on an electronic device with a zoom function.}
    \label{fig:data_driven-results_scatter_NIC}
    \vspace{-0.6cm}
\end{figure*}




\begin{landscape}
    \begin{table}[!h] % use asterisk for pagewide table 
    	\caption{Estimation performance of models}
    	\label{tbl:results:prediction-performance}
    	\centering
    	\begin{tabular}{p{2.5cm}llll|llll}
    		\hline \hline 
    		& \multicolumn{4}{c}{Evaluated on ECD and ICD}  & \multicolumn{4}{c}{Evaluated on ECD}\\	
    		\cmidrule(r){2-5} \cmidrule(r){6-9}
            Model 				& 	MAE $\pm$ SD			&	MSE $\pm$ SD			& STD $\pm$ SD				& $R^2$ $\pm$ SD			    & MAE $\pm$ SD     		&	MSE $\pm$ SD    		    &	STD $\pm$ SD    		& $R^2$ $\pm$ SD    	\\ \hline
                \rowcolor[HTML]{C0C0C0} Lewis WIC   &   10.0 $\pm$ 0.011		    &	137 $\pm$ 0.22	      & 6.06 $\pm$ 0.0036		    & 0.93 $\pm$ 1.3 $\cdot\text{10}^\text{-4}$  & 9.33 $\pm$ 1.2    &	82.7 $\pm$ 0.43  &	6.11 $\pm$ 0.24 & -0.90	$\pm$ 0.015	    	\\
                \rowcolor[HTML]{C0C0C0} Lewis NIC   &   10.0 $\pm$ 0.0062		    &	137 $\pm$ 0.096	      & 6.05 $\pm$ 0.0065		    & 0.93 $\pm$ $\text{5.0}\cdot\text{10}^\text{-5}$  & 6.94 $\pm$0.012  &	82.3 $\pm$ 0.19  &	5.86 $\pm$ 0.011        	    & -0.87 $\pm$	0.013	    	\\
                \rowcolor[HTML]{EFEFEF} Page WIC		&   6.57 $\pm$ 0.42	    &	81.0 $\pm$ 7.8      & 6.14 $\pm$ 0.18		    & 0.94 $\pm$ 0.0033		    & 7.16 $\pm$ 0.88  &	130 $\pm$ 0.52   &	6.32 $\pm$ 0.50     & -22.9 $\pm$ 10		    	\\
                \rowcolor[HTML]{EFEFEF} Page NIC   	&   9.64 $\pm$ 0.0051&	133 $\pm$ 0.25     & 6.34 $\pm$ 0.019		    & 0.93 $\pm$ 1.3 $\cdot\text{10}^\text{-4}$	 & 6.18 $\pm$ 0.010  &	74.5 $\pm$ 0.49  &	6.04 $\pm$ 0.039  & 0.18 $\pm$ 0.0088		    	\\
                \rowcolor[HTML]{C0C0C0} Two term WIC		    &   5.61 $\pm$ 0.14  &	58.8 $\pm$ 3.3      & 5.23 $\pm$ 0.18		    & 0.96 $\pm$ 0.0025		    & 6.03 $\pm$ 0.66  &	97.0 $\pm$ 6.6  &	5.48 $\pm$ 0.48 & 0.36 $\pm$ 0.016		    	\\
                \rowcolor[HTML]{C0C0C0} Two term NIC   	        &   -   &	 -     & - 	& -	& 6.34 $\pm$ 0.083 & 75.8 $\pm$ 1.4  & 5.98 $\pm$ 0.056   & 0.48 $\pm$ 0.051		    	\\
                \rowcolor[HTML]{EFEFEF} Henderson WIC		&  5.37 $\pm$ 0.0089   &	52.9  $\pm$ 0.15     & 4.91 $\pm$ 0.0095    & 0.96 $\pm$ 1.2 $\cdot\text{10}^\text{-4}$  & 5.66 $\pm$ 0.75   &	85.6 $\pm$ 0.28  &	5.09 $\pm$ 0.41   & -1.28 $\pm$ 0.22		    	\\
                \rowcolor[HTML]{EFEFEF} Henderson NIC   	&  82.5  $\pm$ 0.15  &	$\text{1.3}\cdot\text{10}^\text{4}$ $\pm$ 42     & 77.3 $\pm$ 0.18	    & 0.06 $\pm$ 0.0010		    & 6.12 $\pm$ 0.025   &	72.8 $\pm$ 0.25  &	5.96 $\pm$ 0.022  & 0.22 $\pm$ 0.0090	 	    	\\
                \rowcolor[HTML]{C0C0C0} Logarithmic WIC		&   5.47 $\pm$ 0.11    &	59.0 $\pm$ 1.8      & 5.39 $\pm$ 0.070	    & 0.96 $\pm$ 0.0018		    & 5.82 $\pm$ 0.78    &	97.4 $\pm$ 3.2    &	5.50 $\pm$ 0.67  & 0.23 $\pm$ 0.047		    	\\
                \rowcolor[HTML]{C0C0C0} Logarithmic NIC   	&   81.0 $\pm$ 16    &	$\text{1.6}\cdot\text{10}^\text{4}$ $\pm$ $\text{4.7}\cdot\text{10}^\text{3}$     & 95.7 $\pm$ 14	    & 0.10 $\pm$ 0.070   & 6.77 $\pm$ 0.30  &	86.9 $\pm$ 6.6  &	6.41 $\pm$0.21  & -0.3 $\pm$ 0.26		    	\\ 
                \rowcolor[HTML]{EFEFEF} Midilli et al. WIC		&   4.98 $\pm$ 0.022     &	48.1 $\pm$ 0.48      & 4.83 $\pm$ 0.030 	    & 0.96 $\pm$ 0.00033		    & 5.35 $\pm$ 0.58    &	76.0 $\pm$ 0.85    &	5.09 $\pm$ 0.54  & 0.13 $\pm$ 0.022		    	\\
                \rowcolor[HTML]{EFEFEF} Midilli et al. NIC   	&   439 $\pm$ 20    &	$\text{3.9}\cdot\text{10}^\text{5}$ $\pm$ 1.8$\cdot\text{10}^\text{4}$     & 451 $\pm$ 3.9	    & -0.65 $\pm$ 0.065   & 6.19 $\pm$ 0.032  &	73.3 $\pm$ 0.53  &	5.93 $\pm$0.024  & 0.20	$\pm$ 0.013	    	\\ \hline
                \rowcolor[HTML]{C0C0C0} ANFIS WIC		&   4.16 $\pm$ 0.18   &	46.6 $\pm$ 8.2      &    5.37 $\pm$ 0.65		    & 0.97 $\pm$ 0.0056			    & 4.92 $\pm$ 0.17      &	72.2 $\pm$ 13 	    	        &	6.90 $\pm$ 0.85        & 0.60 $\pm$ 0.067		    	\\
                \rowcolor[HTML]{C0C0C0} ANFIS NIC   	&   234 $\pm$ 94	   &	7.5$\cdot\text{10}^\text{5}$ $\pm$ 5.1 $\cdot\text{10}^\text{5}$ 	      & 766 $\pm$ 320 		 & -0.0044 $\pm$ 0.016     & 108 $\pm$ 65      & 5.2$\cdot\text{10}^\text{5}$ $\pm$ 8.5 $\cdot\text{10}^\text{5}$	       &	518 $\pm$ 493   	    & -5.3 $\cdot\text{10}^\text{-4}$ $\pm$ 0.0048  	\\
                \rowcolor[HTML]{EFEFEF} PLS WIC			&   6.15 $\pm$ 0.031	&	67.6 $\pm$ 1.0		  & 5.46 $\pm$ 0.064     & 0.95 $\pm$ 7.5$\cdot\text{10}^\text{-4}$	& 6.34 $\pm$ 0.89 &	106 $\pm$ 1.9	    	    &	5.48 $\pm$ 0.49   	& 0.61 $\pm$ 0.0044			    \\	
    		  \rowcolor[HTML]{EFEFEF} PLS NIC		  &   20.0 $\pm$ 0.058		  &	  656 $\pm$ 3.7			& 16.0 $\pm$ 0.068				  & -0.50 $\pm$ 0.010			  & 5.11 $\pm$ 0.083       		  &	  50.1 $\pm$ 2.18 	    	      &	  4.90 $\pm$ 0.16        	  & 0.59 $\pm$ 0.017		      \\
                \rowcolor[HTML]{C0C0C0} SVR WIC			&   8.25 $\pm$ 0.021			&	108 $\pm$ 1.1  & 6.32 $\pm$ 0.065	& 0.89 $\pm$ 0.0012 	& 8.23 $\pm$ 0.32  		    &	114 $\pm$ 0.83   &	6.44 $\pm$ 0.36        	& 0.25 $\pm$ 0.0068		    	\\
                \rowcolor[HTML]{C0C0C0} SVR NIC			&   35.5 $\pm$ 0.012 			&	2203 $\pm$ 2.7	  & 30.8 $\pm$ 0.033 				& -46.0 $\pm$ 0.20 		& 5.36 $\pm$ 0.023       		    &	78.5 $\pm$ 0.60 	    &	7.08 $\pm$ 0.053  	& -1.04 $\pm$ 0.014 		    	\\
                \rowcolor[HTML]{EFEFEF} RFR WIC			&   3.47 $\pm$ 0.10	&	29.3 $\pm$ 3.7		  & 4.16 $\pm$ 0.37				& 0.98 $\pm$ 2.6$\cdot\text{10}^\text{-4}$				& 3.56 $\pm$ 0.30   &	43.6 $\pm$ 7.6   &	4.48 $\pm$ 0.78        	& 0.64 $\pm$ 0.96		    	\\
                \rowcolor[HTML]{EFEFEF} RFR NIC			&   27.10 $\pm$ 0.11		&	1318 $\pm$ 9.0 	      & 24.2 $\pm$ 0.096				& -5.27 $\pm$ 0.13 				& 3.92 $\pm$ 0.11   &	45.4 $\pm$ 4.1  	    	    &	5.49 $\pm$ 0.33         	& 0.59 $\pm$ 0.030 		    	\\
                \rowcolor[HTML]{C0C0C0} ANN WIC			&  	\textbf{2.90 $\pm$ 0.050}   &	\textbf{15.1 $\pm$ 0.42}  & \textbf{2.58 $\pm$ 0.039}	& \textbf{0.99 $\pm$ 3.2$\cdot\text{10}^\text{-4}$}         & 2.83 $\pm$ 0.052 		   		    &	16.3 $\pm$ 0.50  	 		    &	\textbf{2.54 $\pm$ 0.18} 	& 0.87 $\pm$ 0.0042               \\
    		  \rowcolor[HTML]{C0C0C0} ANN NIC		  &   12.0 $\pm$ 1.1  		  &	  324 $\pm$ 51			& 13.3 $\pm$ 0.92 				  & 0.60 $\pm$ 0.091  	   		  & \textbf{2.63 $\pm$ 0.091}  	      &   \textbf{14.0 $\pm$ 1.1}       &	  2.67 $\pm$ 0.13	          & \textbf{0.90} $\pm$ 0.011	  \\	
    
    		\hline  \\
                \multicolumn{9}{l}{ - Indicates models that has not converged}
    	\end{tabular}
        \end{table}    
\end{landscape}

\subsubsection{Machine learning models}
Generally, the machine learning models as seen in Fig. \ref{fig:data_driven-results_scatter_WIC} and \ref{fig:data_driven-results_scatter_NIC} outperform the thin layer drying models. 
%
The ANN WIC model as seen in Fig. \ref{fig-results-ANN_WIC_preds} is able to do meaningful MC estimation on both the ICD and ECD with a low variance and low bias. Furthermore, the small standard error indicate that the model consistently performs well, independently on the selection of the cross validation folds.

The ANN NIC model as seen in Fig. \ref{fig-results-ANN_NIC_preds} does not generalize well outside the range of the data it has seen during training. However, when evaluating using only the ECD, i.e., the range of data it has been trained on, it outperforms any other model tested here on all performance metrics except for STD where it is only slightly different from the ANN WIC model. Furthermore both the ANN NIC and ANN WIC models are the only models to achieve a coefficient of determination close to one, indicating a low bias in the estimation results. 
%
The RFR WIC models as seen in Fig. \ref{fig-results-RFR_WIC_preds} is generally able to do the MC estimation task successfully. However it suffers from outliers, resulting in a high MSE, as seen in Table \ref{tbl:results:prediction-performance}, reducing its applicability in a real world scenario. 
%
Both the SVR WIC and PLS WIC models, Fig. \ref{fig-results-SVR_WIC_preds} and \ref{fig-results-PLS_WIC_preds}, generally overestimates the MC. This is preferable to underestimating the MC in a real world scenario, as it will tend to over-dry filter media instead of under-dry which is worse for the product quality. However, it is generally unable to do satisfactory MC estimation for the case of bulky filter media products. 

The performance of the ANFIS WIC and Fig. \ref{fig-results-ANFIS_WIC_preds}, can be separated into three periods. For MC $< 30$ the ANFIS WIC modes perform well, and the performance of the models are independent of the chosen folds during the 10 fold classification. However, for MC $30 < \text{MC} < 70$, it is dependent on what data was chosen for training or not, as can be seen by the standard errors indicated by the errors bars. This estimation variance originates from the tuning process of the ANFIS membership function parameters, where the entire data range has not been seen in its training folds. The ANFIS NIC model suffers badly from this problem of tuning its ANFIS membership function parameters for data outside its training folds. Fig \ref{fig-results-ANFIS_NIC_preds} shows that the ANFIS NIC model is generally unable to perform any useful estimation.
Except for the ANN NIC model, the remaining machine learning NIC models suffer from a bias towards lower estimates for higher experimental MC.

The expected estimation performance is summarized in Table \ref{tbl:results:prediction-performance} across two different ranges, one range containing both the ICD and ECD, and another range containing only the ICD. When evaluated on the both the ICD and ECD the ANN WIC model outperforms all other models on all performance criteria. When considering only the ECD, the ANN NIC model slightly outperforms the ANN WIC model on MAE, MSE and $R^2$ while the ANN WIC performs the best on the $STD$ parameter, however the difference is insignificant.






%\subsection{Prediction performance }
% Figure 5 explanation before figure 6 - otherwise interchange figures
Taking a closer look at the four best competing models, Fig. \ref{fig-results-comparison-all_models-dt-rolling_mean-vs-MAE} shows the performance of the four best models as a function of drying time. This shows that the ANN WIC model significantly outperforms the rest of the models for the dimensionless normalized drying time of 0. The ANN WIC model seems to be able to encapsulate the drying phenomena along the entirety of the drying curve, where as RFR models are unable to correctly estimate the MC both at low and medium drying times. The ANN NIC model is unable to generalize outside of the range of the training data that it has seen. However, inside the training data range, it performs similarly or better than the ANN WIC model. All models are able to correctly estimate the MC for long drying times, however this feature is neither surprising nor interesting in a optimization setup where the goal is to decrease drying times. 


We see that the estimation quality also depends on the size of the estimates. Fig. \ref{fig-results-comparison-prediction-rolling_mean-vs-MAE} shows that smaller estimates result in lower average estimation errors and standard errors. Both the average error and the standard error decreases for all models for estimates below approximately 10\% normalized MC, which is the range that is especially important for industrial applications. The high variance and mean MAE for the RFR- and ANN NIC models are caused by the models inabilities to correctly estimate the MC of the ICD. Only the ANN WIC model can correctly estimate the MC across the entire drying range.


\begin{figure}[] % use asterisk for pagewide figure
	\centering
	\includegraphics[width=3in]{figures/result-comparison-dt-rolling_mean-vs-MAE_normalised.pdf} 
	\caption{Moving average of MC estimate MAE as a function of dimensionless normalized drying time with a window size of $\pm 8$. Colored areas indicate one standard deviation of the absolute residuals inside the rolling window. Area without data indicates no measurements in the area of the underlying data within the rolling window.} 
	\label{fig-results-comparison-all_models-dt-rolling_mean-vs-MAE}
\end{figure}



\subsection{Practical Considerations}

In a manufacturing setting all estimates are not made equal. The most important range of dimensionless MC estimates are below 10\%. Fig. \ref{fig-results-comparison-prediction-rolling_mean-vs-MAE} shows that in this region all models, RFR as well as ANN, perform well enough on average to be used for optimizing and controlling the manufacturing process. However the best performing model is the ANN NIC model, which can also be seen in Table \ref{tbl:results:prediction-performance}.


\begin{figure*}[]
    \centering
    \includegraphics[width=7in]{figures/results-comparison-prediction_normalised.pdf}
    \caption{Model trust as defined by the moving average of MC estimate MAE as a function of model estimates with a window size of $\pm 2.5$ dimensionless normalized MC points. Colored areas indicates the standard error of the mean of the absolute residuals inside the rolling window.}
    \label{fig-results-comparison-prediction-rolling_mean-vs-MAE}
\end{figure*}
