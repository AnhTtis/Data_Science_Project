\section{Development of the artificial neural network}\label{S:SectionII}

We formulate MC estimation as a regression problem. Given the measured feature vector $\mathbf{x}$ as input, we use an ANN to map this multi-dimensional feature vector to the MC value at the output of the network. 
That is, the ANN acts as a parametric function $f(\mathbf{x},\mathbf{w})$, the parameters $\mathbf{w}$ of which are determined so as to minimize the loss function (\ref{eq:loss_func}) of the ANN's response w.r.t. to the real MC values measured on training data as described by
\begin{equation}\label{eq:loss_func}
	\mathcal{L} = \frac{1}{N}  \sum_{k=1}^N  \left(  MC_k - \widehat{MC}_k \right)^2,
\end{equation}
where $N$ is the total number of samples, $MC_k$ is the k'th experimentally measured moisture content, and $\widehat{MC}_k$ is the k'th estimate of the actual moisture content, i.e., it is the response of $f(\cdot,\mathbf{w})$ when the k'th sample in the training set is introduced to it.

The complexity of the function indicated by the structure of the ANN has an effect on the effectiveness of the method. We determine a good structure for the ANN by following the model selection process described next.


\subsection{Model Selection Process}
The estimation performance of an ANN depends on the amount of training data, the quality of training data, the chosen architecture of the neural network, the choice of hyper parameters such as activation functions, optimization algorithm, use of dropout and batch normalisation, choice of learning rate, and mini-batch size. Choosing the best architecture of a neural network is a research field in and of itself called Neural Architecture Search and multiple methods have been developed for this task \cite{ren2021comprehensive,kiranyaz2017pop,tran2020heterogeneous}.

The architecture of the neural network was chosen to be a feedforward multilayer perceptron (MLP) which is used for its simplicity and success in estimating MC of other drying experiments \cite{chasiotisArtificial2020,kalathingal_artificial_2020}. The Rectified Linear Unit (ReLU) \cite{NairH10} was used as activation function of the hidden neurons. The linear activation function was used for the output neuron enabling regression to all values on the real number line.
The architecture of the neural network, the learning rate and the mini-batch size were determined by performing model selection using the values shown in Table \ref{tbl:HyperParameterSearchOptions}. This was done by using the ASHA algorithm \cite{li2018massively} combined with the cross-validation process, and it was orchestrated using the \textit{Tune} framework \cite{liaw2018tune}. 
Table \ref{tbl:HyperParameterChosenOptions} shows the selected hyper parameters. For the rest of the hyper-parameters, such as optimizer, weight-decays, loss function, etc, we use values based on empirically established heuristics in previous works in the literature. For exact values see section \ref{sec:training_strategy}.

The expected error is highly dependent on the randomly chosen validation set, especially as the work in this article is based on a relatively sparse dataset with 322 sets of observations. To combat this issue, we combine cross-validation with the ASHA algorithm. However, instead of using the ASHA algorithm for early stopping of the training procedure, we test each set of chosen hyper parameters in a 10 fold cross validation loop. For each of the \textit{k} iterations, nine folds are used as training and validation (split 80/20) and the last fold is used for testing. The procedure is then repeated to 10 times. The ASHA algorithm is then applied to the mean of the mean squared error (MSE) losses of the test sets, with a grace period of three, a reduction factor of three, using one bracket. For each choice of number of hidden layers 500 trials were performed, the cross validation mean squared error was then plotted for each layer depth with its optimal parameters as can be seen in Fig. \ref{fig-layer-search}, resulting in a an optimal ANN architecture of which a schematic overview can be seen in Fig. \ref{fig-optimal-nn-architecture-schematic}.


\begin{figure}
	\centering
	\includegraphics[width=3in]{figures/dataset-architecture_search_num_hidden_layers.pdf}
	\caption{Average performance cross validation of found optimal hyper parameters for each size of the neural network based on 10 fold cross validation.}
	\label{fig-layer-search}
\end{figure}


\begin{table}[]
	\caption{Selection options for ANN hyperparameters}
	\label{tbl:HyperParameterSearchOptions}
	\begin{tabular}{lll}
		\hline \hline
		Hyperparameter 				& Type  		& Values \\ \hline
		Number of neurons 	        & continuous   	&   [1, 500]   \\
		Number of hidden layers  	& choice     	&   \{1, 2, 3, 4, 5, 6, 7\} \\
		Learning rate 				& continuous    &   log [1e-4, 1e-1] \\
		Batch size 					& choice		& \{2, 4, 8, 16, 32, 64\}  \\
		\hline \hline 
	\end{tabular}
\end{table}


\begin{table}[]
	\caption{Chosen configuration of hyper parameters.}
	\label{tbl:HyperParameterChosenOptions}
	\begin{tabular}{ll}
		\hline \hline
		Hyperparameter & Selected Parameters \\ \hline
		Number of neurons          &   $l_1=231$, $l_2=421$, $l_3=392$   \\
		Number of hidden layers          &    3  \\
		Learning rate          &   0.029924 \\
		Batch size & 16  \\
		\hline \hline 
	\end{tabular}
\end{table}



\begin{figure}
	\centering
	\includegraphics[width=3in]{figures/ChosenNN.pdf}
	\caption{Schematic view of the found optimal neural network}
	\label{fig-optimal-nn-architecture-schematic}
\end{figure}

\subsection{Training Strategy} \label{sec:training_strategy}
Along with hyper parameter optimization, training of the parameters of the ANN is important in improving estimation quality. In order to find a converging point in the training process of the deep neural network, the Adam optimizer  \cite{kingma2014adam} was used with default decay rates of $\beta_1 = 0.9$ and $\beta_2 = 0.999$. The loss function used is the MSE of the MC estimates as defined by (\ref{eq:MSE}).
% MSE stuff was here

Batch normalisation was used before each layer to reduce the internal covariate shift of the activation functions  \cite{pmlr-v37-ioffe15}. Dropout \cite{srivastava2014dropout} was applied in the training phase after each hidden layer with a probability of attenuating each node of $50 \%$. Dropout can be seen as a form of data augmentation \cite{bouthillier2015dropout}, thus improving generalisation of the trained models.
In order to optimize training time, an early stopping scheme with a patience of 200 epochs was used, stopping the training when no improvements have been done on the validation set for 200 epochs. The best performing model was then saved and used for inference.

\subsection{Performance evaluation metrics}
The performance of the model is tested using four different measures, the MSE
\begin{equation}\label{eq:MSE}
	MSE =  \frac{1}{N} \sum_{k=1}^{N}  \left( MC_k-	\widehat{MC_k} \right)^2,
\end{equation}
the mean absolute error (MAE)
\begin{equation}\label{eq:MAE}
	MAE = \frac{1}{N} \sum_{k=1}^{N} \left( |  MC_k-	\widehat{MC_k}    | \right),
\end{equation}
which is less sensitive to outliers, as well as the standard deviation (STD)
\begin{equation}\label{eq:STD}
	STD = \frac{1}{N} \sum_{k=1}^{N} \left( |  MC_k-	\widehat{MC_k}    | \right),
\end{equation}
and the coefficient of determination $R^2$ between the estimates and the experimentally measured values
\begin{equation}\label{eq:R2}
	R^2 = 1 - \frac{\sum_{k=1}^{N} \left(MC_k - \widehat{MC_k}  \right) }{\sum_{k=1}^{N} \left(  MC_k - \widebar{MC} \right)}.
\end{equation}
% no need to define the variables - this has been done in equation (1,2)
where $\widebar{MC}$ is the average experimentally measured moisture content.
% In equation \ref{eq:MSE},\ref{eq:MAE},\ref{eq:STD} and \ref{eq:R2} 

