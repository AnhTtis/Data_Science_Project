\section{Related Work}
\label{sec:related}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/framework.png}
%     \caption{Overall framework of our method. Blue blocks (camera \& SMPL parameters, texture network) represent learnable parameters. 
%     Given a frame index $i$, we use the extrinsics and SMPL parameters for the frame to generate a mesh. 
%     This mesh is passed into two rasterization pipelines - Neural Mesh Renderer~\cite{kato2018renderer} and SoftRas~\cite{liu2019softras} to output differentiable rasterization parameters.
%     The face index and barycentric coordinates output by SoftRas and NMR is fed into the texture network, and this leads to a soft and hard rendering respectively.
%     A dashed line represents the stop-grad op from texture network to SoftRas (since we do not want soft colors to backprop to the network).
%     }
%     \label{fig:framework}
% \end{figure*}

\subsection{Human reconstruction via prediction}
Recovering 3D human shape and pose estimation using a parametric 3D body model %like
\brandonedit{such as} SCAPE~\cite{Pishchulin17scape}, SMPL~\cite{Loper15tog}, SMPL-X~\cite{Pavlakos19smplx}, STAR~\cite{Osman20STAR} or GHUM~\cite{Xu21ghum} is an active area of research in the %CVML communities. 
\brandonedit{CV community}.
Most of these approaches directly predict model parameters using a learned model~\cite{Kanazawa18cvpr,Kocabas20cvpr,Sengupta21iccv,Sengupta20bmvc,Sengupta21bmvc,Sengupta21cvpr,Smith193dv,Varol18bodynet,Yu21iccv,Yu22cvpr}.
Recent approaches (\eg,~\cite{Kolotouros19iccv,Smith193dv,Sengupta20bmvc,Sengupta21cvpr}) 
have improved on prior methods by directly regressing body shape. 
% \brandonedit{(\eg, \cite{Kolotouros19iccv,Smith193dv,Sengupta20bmvc,Sengupta21cvpr})}. 
However, these approaches can only represent the shape and pose of a minimally clothed body and fail to model complex topology due to clothing, hair, \brandonedit{\etc.} 
% 
BodyNet~\cite{Varol18bodynet} and DeepHuman~\cite{Zheng19DeepHuman} attempt to predict volumetric representations of the human model from a single image.
Implicit representations are an interesting alternative for representing high-fidelity 3D geometry without requiring the entire output volume be kept in memory.
In contrast to explicit representations, implicit representations define a surface as a level set of a function.
Recent methods, such as PiFu, PiFuHD and PHORHUM~\cite{Saito19pifu,Saito20pifuhd,Alldieck22phorhum}, learn an implicit surface representation estimated based on pixel aligned features and the depth of 3D locations.
Some recent methods combine the benefits of both explicit and implicit representations to represent clothed people~\cite{Corona22lvd,bhatnagar:eccv2020:ipnet,Zheng20pamir,Cao22jiff,He21ARCHAC,Xiu22icon}.
These methods require 3D ground-truth supervision, limiting their \brandonedit{applicability} to a few datasets and their \brandonedit{ability to generalize beyond in-distribution poses.} %generalization capabilities to OOD poses.
Recently, implicit representations have been used to learn a generative model of 3D people in clothing \cite{Chen22gdna,Alldieck20imghum,Deng19NASA,Saito21SCANAnimate,Chen21SNARF}.
However, these approaches require ground truth posed 3D meshes or RGB-D video sequence to learn a model
\cite{Tao21thuman,Zheng19DeepHuman,Patel21agora,twindom,renderpeople,axyz}. 
All of the methods share a common limitation, \ie, predicting the human shape and texture from a single image is ill-posed, and incorrectly regressed predictions are not iteratively refined using 
auxiliary signals.

%%%% Optimization
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Human reconstruction via optimization}
\textbf{Recovering pose}: Some of the earliest approaches fit model parameters via optimization at test time~\cite{alldieck:cvpr2018:peoplesnap,Bogo16eccv,Kolotouros21iccv}.
Bogo \etal~\cite{Bogo16eccv} optimize SMPL parameters by minimizing the joint reprojection loss and prior terms, whereas~\cite{Kolotouros21iccv,Kolotouros19iccv,Zhang21iccv} 
employ a likelihood term over the pose via a learned network, and perform iterative regression to minimize reprojection or multiview losses.
Pavlakos \etal~\cite{pavlakos:2022:recon3d} reconstruct SMPL parameters of multiple humans from videos of TV shows.
However, these methods only recover a pose with generic shapes and do not capture subject-specific deformations and textures.

\textbf{Recovering shape and texture}: Alldieck \etal~\cite{alldieck:cvpr2018:peoplesnap} 
% propose to optimize the mesh deformation to fit the visual hull from multiple frames.
extended this approach to monocular video by fusing the unposed silhouettes from all frames to generate a consensus mesh. Each human silhouette, extracted from a video frame, defines a constraint on the human shape, which can be used to estimate deviations in shape.
The main problem with this method is that shape is ambiguous, \ie, concave surfaces are not captured. 
One would need to use auxiliary inputs like depths or normals to disambiguate the problem \cite{Xiu22icon,alldieck:3dv2018:humanmono}, but depth prediction systems can introduce their own errors. 

Mildenhall~\etal~\cite{Mildenhall20nerf,Xie22neuralfields} pioneered NeRFs for representing static scenes with a color and density field without requiring any 3D ground truth supervision. 
Recently this approach has been extended to reconstruct clothed humans as well \cite{peng:cvpr2021:neuralbod,Su21anerf,chen:arxiv2021:animner,weng:cvpr2022:humannerf,jiang:eccv2022:neuman,Weng20vid2actor,Liu21neuralactor,wang:eccv2022:arah,Jiang22selfrecon,yao:arxiv2022:monoclothedbody,Te22neuralcapture}. 
% AnimNeRF \cite{chen:arxiv2021:animner} and SelfRecon \cite{Jiang22selfrecon} are closest to our approach, since both of these methods model clothed humans by combining NeRFs with SMPL \cite{Loper15tog}.
These approaches use SMPL as a prior to unpose the human body across multiple frames by transforming the rays from observation space to canonical space which is then rendered using a NeRF.
PINA~\cite{dong:cvpr2022:pina} learns a SDF and a learned deformation field to create an animatable avatar from an RGB-D image sequence.
Chen~\etal~\cite{Chen22mobilenerf} used a polygon rasterization pipeline to speed up NeRF rendering as a post-process; % with similar output quality as volume rendering; 
however, their approach does not reduce NeRF training times.
\sidedit{Some recent methods have improved the efficiency of scene-agnostic NeRFs (\eg~\cite{yu2021plenoctrees,yu_and_fridovichkeil22plenoxels,mueller2022instantngp,Garbin21FastNeRF,Reiser21kilonerf})}.
These methods demonstrate fast training and rendering times, but adapting them to include an animatable human shape prior requires expensive nearest neighbor operations that erode efficiency gains. 
However, most of these approaches are computationally expensive.
% In addition, the resulting geometric reconstruction tends to be inaccurate because it is recovered by discretization of the occupancy field by marching cubes. % operation.
In addition, the resulting representation may have poor generalization to OOD poses.

\begin{table*}[t!]
    \centering
    \small
    \begin{tabularx}{0.97\linewidth}{Xccccccc} \toprule
    % & \multicolumn{3}{|c|}{\textbf{Loss functions }} & \multicolumn{4}{c}{} \\
    \textbf{Method} & \textbf{RGB loss} & \textbf{Mask loss} & \textbf{KPS loss} & \textbf{Representation} & \textbf{Novel pose} & \textbf{Training time} & \textbf{GPU} \\
    \hline
     VideoAvatar~\cite{alldieck:cvpr2018:peoplesnap} & \xmark & \cmark & \cmark & SMPL+D & \cmark  & 16 hours & NA \\
    AnimNeRF~\cite{chen:arxiv2021:animner} & \cmark & \cmark & \xmark & NeRF & \cmark & 26 hours & 48GB \\
    Neuralbody~\cite{peng:cvpr2021:neuralbod} & \cmark & \cmark & \xmark & NeRF & \xmark & 14 hours & 48GB \\
    HumanNeRF~\cite{weng:cvpr2022:humannerf} & \cmark & \cmark & \xmark & NeRF & \cmark & 72 hours & 48GB \\
    NeuralActor~\cite{Liu21neuralactor} & \cmark & \cmark & \xmark & NeRF & \cmark & 48 hours & 256GB \\
    SCARF~\cite{yao:arxiv2022:monoclothedbody} & \cmark & \cmark & \xmark & NeRF+SMPLX-D & \cmark & 40 hours & 32GB \\
    \hline
        Ours & \cmark & \cmark & \cmark & SMPL+D & \cmark & \textbf{$<$1hour} & \textbf{5GB} \\
    \bottomrule
    \end{tabularx}
    \caption{Comparison of different methods for human reconstruction. Our simple yet clever use of NeRF-like losses with an SMPL+D representation bridges the gap between mesh-based and NeRF-based optimization with dramatic speedups and compute savings.}
    \label{tab:compare}
\end{table*}

Our method falls in the category of shape and texture optimization, being most similar to~\cite{alldieck:cvpr2018:peoplesnap,chen:arxiv2021:animner,Jiang22selfrecon,weng:cvpr2022:humannerf}, where we aim to contrast NeRFs with its mesh-based equivalent formulation.
In contrast to NeRFs, an explicit mesh representation combined with a carefully chosen optimization scheme enables our method to recover accurate geometry, while ensuring photo-realistic rendering, at substantially lower computational and time costs.
We show that, contrary to conventional wisdom, our method, when employed with the correct optimization objective, can recover complex geometry (like loose clothing, skirts, long hair, hoodies, \etc).
Because we constrain the solution space using a mesh, our method is computationally inexpensive, and can be run on a single consumer-grade GPU in about an hour.
Our representation allows us to use differentiable rasterization pipelines, drastically reducing its training and inference time.
Note that our goal is not to replace NeRF methods, which have asymptotically better performance due to more
% degrees of freedom. 
representational capacity.
\brandonedit{Rather, \emph{we provide a computationally inexpensive alternative that can be used for real-time rendering and/or on-device applications, or to bootstrap NeRF optimization.}}
Unlike~\cite{peng:cvpr2021:neuralbod,Liu21neuralactor}, which have reported failure cases for \brandonedit{out-of-distribution} poses, our method's accuracy is only limited by the artefacts of the skinning function.
A comparison with relevant methods is provided in Fig.~\ref{tab:compare}.