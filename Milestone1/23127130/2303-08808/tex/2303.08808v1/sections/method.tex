%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% A toy problem
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/toyexample.png}
    \caption{
    A toy example hightlighting that methods based on visual hulls alone cannot recover concavities in the underlying object.
    % A toy example hightlighting that 
    Optimization from visual hull is ill-conditioned, which is disambiguated by multi-view RGB consistency.
    % The rendered silhouettes of a cube and its deformed version is the same from all angles, and therefore ill-conditioned.
    % However, consistency from multiview RGB helps disambiguate the shape - this is the idea NeRFs use to recover the occupancy volume of a scene. 
    This idea is used in NeRFs, and in this work we show that it is possible 
    to use RGB and visual hull to optimize a mesh representation.
    %to perform RGB + visual hull optimization with a mesh representation.
    }
    \label{fig:cube}
\end{figure}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Method}
\label{sec:method}
This paper focuses on jointly recovering accurate geometry and realistic textures from a monocular RGB video of a person using a mesh representation.
%The paper is organized as follows: 
First, we illustrate the ambiguity in mesh reconstruction from visual hull only, and how multi-view RGB consistency can help disambiguate the problem (Section~\ref{sec:toyprob}).
This allows us to avoid using auxiliary inputs, such as depths, landmarks, and normals, which are either expensive to obtain or error-prone if predicted.
% Next, we discuss the shape and texture representations (Section~\ref{sec:shape_model} \& ~\ref{sec:texnet}) used in our multi-view optimization.
Next, we discuss the shape and texture representations (Sec~\ref{sec:shape_model},~\ref{sec:texnet}) used in our optimization.
To enable photogrammetric losses to backpropagate to a parameterized mesh representation, we describe the differentiable rendering pipeline (Section~\ref{sec:diffrender}).
Finally, we describe the forward model (Section~\ref{sec:forward_model}), loss functions (Section~\ref{sec:loss_functions}) and training pipeline (Section~\ref{sec:two_stage_training}) to jointly recover the clothed human geometry and texture of the subject.
% Fig.~\ref{fig:framework} shows the overall framework. 

% % In the literature, mesh-based optimization methods typically produce blurry textures with inaccurate geometry. 
% In the literature, mesh-based optimization methods using silhouettes typically produce inaccurate geometry. 
% % \sidcomment{I think we have to position it in a different way since hybrid of implicit and explicit methods like anim-nerf, icon also build on top of mesh.} 
% % \ganeshcomment{ agreed, the statement can be denied and may be too broad. We have seen methods that can produce accurate geometry AND realistic textures using mesh representation given unlimited budget. For ICON: it seems like the final result is marching cubes applied to an SDF, which means higher level of detail from a continuous representation. Alternate statement: When looking at this problem of joint recovery, accuracy of reconstruction can depend on the mesh level of detail (LOD)[https://arxiv.org/pdf/2101.10994.pdf] and/or the computational budget.}
% Reconstructed textures are unrealistic, especially at faithfully representing high-texture regions like \ganeshedit{human} faces, leading to the common ``uncanny valley'' problem.
% This occurs because visual hulls from the binary masks do not provide enough information to reconstruct the surface of the mesh, i.e. concave parts of the mesh (see Fig.\ref{fig:cube}).
% Since mesh-based optimization is done in multiple stages \cite{alldieck:cvpr2018:peoplesnap,alldieck:3dv2018:humanmono}, where the texture is learned only after fitting the mesh, errors in geometry propagate to texture, which leads to blurry and unreliable texture quality.
% To combat this, mesh-based optimization methods typically use additional inputs like normals, 3D landmarks, depth, to learn better geometry.
% On the other hand, volume rendering based methods \cite{chen:arxiv2021:animner, peng:cvpr2021:neuralbod} learn texture and geometry simultaneously to produce realistic renderings.
% However, we empirically show that the continuous nature of occupancy combined with representation capacity of neural volumes leads to ill-conditioning of the objective, leading to artifacts in novel pose reconstruction.
% \brandoncomment{This first paragraph repeats some introduction and related work points and, regardless, feels like introduction material. Can we cut or move most of it there, and quickly move to the motivating toy problem?}

% \vspace*{-10pt}
\subsection{A motivating toy problem}
\label{sec:toyprob}
We use the following toy problem 
to illustrate the ambiguity of using visual hulls for mesh reconstruction. %, we look at the following toy problem.
% To illustrate why obtaining a mesh from silhouette is an ambiguous problem, we look at the following toy problem.
% To motivate why obtaining a mesh from silhouette is an ambiguous problem, we look at the following toy problem.
Consider a cube and the same cube but with all its faces dented inwards, shown in Fig.~\ref{fig:cube} (top).
Rendering the visual hull of both objects gives us the same set of binary silhouettes for all camera angles, making it ambiguous for any optimization scheme to recover a unique mesh from the set of silhouettes.
% The user is provided a set of ground truth camera parameters and binary silhouettes. 
% This object will lead to the same binary silhouette as (a) from any camera view, making the mesh recovery problem ambiguous, facilitating the need for auxiliary inputs like depth or normals \cite{dong:cvpr2022:pina,Xiu22icon,embodiedhands}. 
This ambiguity necessitates the use of auxiliary inputs, \eg, depth or normals \cite{dong:cvpr2022:pina,Xiu22icon,embodiedhands}. 
Now consider the same scenario, but with the same overlaid texture on both objects.
In this case, the two objects have different renders in Fig.~\ref{fig:cube} (middle \& bottom) 
from the same viewpoints,
%when viewed from certain viewing angles, 
disambiguating the shape of the underlying object when optimized with a multi-view RGB consistency framework.
This is the idea used in NeRFs \cite{Mildenhall20nerf} to recover the occupancy and radiance volume from images alone. 
Therefore, one can use multi-view RGB consistency as a surrogate to depth, normals, \etc.
The key to using RGB images for mesh optimization is to assign a unique RGB value to each point on the mesh, 
% such that with the correct geometry and pose, the mesh can render all viewpoints in the data. 
such that it can guide the mesh vertices to produce consistent renderings in all views. 
However, doing so introduces a `moving target' problem (partially optimized mesh and RGB hinder each other's learning) which is non-trivial to optimize.
Empirically, we find that carefully formulating the optimization  problem (Sec~\ref{sec:two_stage_training}) allows us to capture complex geometry, \brandonedit{including} hoodies, loose shirts, pants, skirts, and \brandonedit{voluminous} hair, better than prior work that uses visual hulls alone for optimization.  
% Empirically, we find that our RGB assignment with correct training losses is able to capture complex geometry like clothes and concave surfaces in faces better than with visual hulls alone (Fig.~\ref{fig:novelview}).

% Empirically, we find that assigning a unique RGB color for each face in the mesh, we are accurately able to capture complex geometry like cloth folds and concave surfaces in faces better than with visual hulls alone (Fig.~\ref{fig:novelview}).
% \ganeshcomment{Motivating example is great. alternate last statement: We find that by assigning a unique RGB value for each point on the mesh surface, we are able to accurately render the mesh...}
% This forms the basis for our mesh optimization procedure. 
% \sidcomment{A high-level summary of the following sections in the method might be useful here before diving deep into the details?}


% \subsection{Subdivided SMPL+D model}
\subsection{Geometry model}
\label{sec:shape_model}
Optimizing a high-fidelity textured avatar from monocular or multi-camera RGB video requires us to learn geometry and corresponding texture on the learned geometry.
Methods using neural rendering learn a canonical or conditional volume from scratch without using any structural priors \cite{Mildenhall20nerf}.
% \sidedit{Recent methods} (\eg, \cite{chen:arxiv2021:animner,Jiang22selfrecon}) use a SMPL prior to canonicalize the per-frame information into a single volume, to use multi-view consistency from frames with different poses.  
Instead of learning a volume from scratch, we use the parametric SMPL human model ~\cite{Loper15tog}.
We learn per-frame pose and camera parameters $\{(\btheta_i, \bR_i, \bt_i)\}_{i \in \{1..n\}}$ and a common shape parameter $\bbeta$.
Since the shape parameter is a low-dimensional embedding capturing human shape, we also learn a per-vertex offset matrix $\bD \in \mathbb{R}^{V\times3}$ where $V$ is the number of vertices in the mesh.
% Unlike ~\cite{alldieck:3dv2018:humanmono} we do not need to use subdivided meshes since the base SMPL model captures most geometric details (Fig.~\ref{fig:geom} and in Appendix). 
Unlike ~\cite{alldieck:3dv2018:humanmono} we show that the base SMPL+D model is enough to capture most geometric details (Fig.~\ref{fig:geom} and in Appendix) and we do not need to use a subdivided SMPL model.
% To improve fidelity of the mesh, we subdivide the SMPL model once using Catmull-Clark subdivision to have $V=27554$ vertices.

% Other works in the literature \cite{alldieck:3dv2018:humanmono} adopt subdivided SMPL meshes, and a variety of different loss functions to encourage better shape learning and argue that the vanilla SMPL+D model is not adequate for modeling clothing and other deformations \cite{alldieck:3dv2018:humanmono, dong:cvpr2022:pina, chen:arxiv2021:animner}.
% \sidedit{In contrast,} we demonstrate that this representation is suitable for capturing complex geometry and texture, is much faster than volume rendering methods, and doesn't produce %cloudy
% \brandonedit{cloud-like} artifacts present in NeRF results (see Section \ref{sec:novelpose}).
% \brandonedit{cloud-like} artifacts like volume rendering (see Section \ref{sec:novelpose}).

\subsection{Texture representation}
\label{sec:texnet}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/texnet.png}
    \caption{(\textbf{Top}) Traditional texturing methods use fixed per-vertex UV coordinates and linearly interpolate the color from the UV coordinates over the face leading to blurry texture. 
    (\textbf{Bottom}) We use learnt per-vertex 3D texture coordinates (in cyan,magenta,yellow) and linearly interpolate the texture coordinates over the face.
    The interpolated texture coordinate is input into the multi-res hash encoding \cite{mueller2022instantngp} which allows us to represent sharp textures within the face (illustrated by how the network treats the input space across multiple resolutions).
    % Face normals input omitted for brevity.
    }
    \label{fig:texnet-illus}
\end{figure}

Mesh-based representations generally use predefined UV coordinates for each vertex \cite{alldieck:cvpr2018:peoplesnap,alldieck:3dv2018:humanmono}, and the RGB value at any point on the face is determined by evaluating the UV coordinate of the point, and using interpolation from the RGB values in the UV map. % to derive its RGB value.
This is converted into a texel-map ~\cite{kato2018renderer,liu2019softras}.
This representation, although simple, is not adaptive to the %amount
\brandonedit{complexity} of texture on the mesh.
Meshes may have regions of low or high texture (\eg, \brandonedit{solid-color} clothing \brandonedit{versus} faces, hair, \brandonedit{or patterned} clothing) which may require a more flexible texture representation.
Bilinear interpolation also contributes to blurry textures
, making it unsuitable for \brandonedit{high-frequency or discontinuous} % high quality quality
textures.
% Moreover, it cannot model normal-dependent diffuse lighting that can occur in a multi-view setup.
Moreover, deferred rendering ~\cite{smplpix,thies2019deferred} may overfit to the UV distribution of the training frames and may not generalize to UV distributions of OOD poses (Fig.~\ref{fig:novelpose}).
% Therefore, the texture representation should be a function of the UV location and normal.
% A texel-based representation \cite{kato2018renderer,liu2019softras} uses a fixed resolution RGB tensor for each face, and these values are populated based on interpolation from local barycentric coordinates in the face.
% This representation also has the same problem with texture quality due to a fixed interpolation scheme and non-adaptive representation.

To alleviate these problems, we take inspiration from \brandonedit{M\"uller \etal~\cite{mueller2022instantngp} who proposed} a multi-resolution hash encoding of the input to capture high-frequency details.
The main idea of this work is to use implicit hash collisions to average the gradients at a particular hash lookup entry, and therefore weigh the representation appropriately.
We use the same idea to learn a high frequency texture representation.
Human avatars have %different amounts of texture 
\brandonedit{varying levels of texture complexity} across their surface---regions such as faces, logos on clothing, \etc have high levels of detail, \brandonedit{whereas} %while
regions \brandonedit{such as} %like 
skin and \brandonedit{solid-color} clothing have low levels of detail, and \brandonedit{require little} %do not need enough 
representation capacity to learn.
The hash-encoding representation would learn this implicitly by finding the best embedding which leads to the %best 
\ganeshedit{least} RGB reconstruction loss, effectively `splitting' its representation capacity between high and low-frequency details.
% This can be augmented by  the normal direction as an auxiliary input.
% To learn diffuse components, the normal direction is used an an auxiliary input.
% 
For a given frame $i$ using shape $\bbeta$, body pose $\btheta_i$ and deformation $\bD$ we produce a mesh $\mathcal{\bM}_i$. 
% Similar to traditional rasterization pipelines, we construct the mesh for a given frame $i$ using shape $\bbeta$, body pose $\btheta_i$ and deformation $\bD$ to produce a mesh $\mathcal{\bM}_i$. 
This mesh is then projected to the image plane using camera extrinsics $(\bR_i, \bt_i)$ and a fixed intrinsics matrix using the projective transformation to mesh $\mathcal{\bm}_i$.
We perform rasterization on the projected mesh to output an image containing face indices and barycentric coordinates of the face index for each pixel.
% We query the barycentric coordinates and face index that is rendered at each pixel.
Let face $f_j$ be rendered at pixel $p$ with barycentric coordinates $\{ w_{j1}, w_{j2}, w_{j3} \}$ such that $w_{jk} \ge 0 $
% \quad \forall k \in \{1,2,3\} $ 
and $\sum_{k=1}^3 w_{jk} = 1$.
If the texture coordinates of $f_j$ are given by $\{ t_{j1}, t_{j2}, t_{j3}\}$, then the rendered point on the face has the texture coordinate $t = \sum_{k=1}^3 w_{jk} t_{jk}$.
% 
% Moreover, the normal in the observation space is also  given by $\vec{r} = \sum_{k=1}^3 w_{jk} \vec{v}_{jk}$ which is rotated back to the observation space.
% The texture coordinate and unit normals $\vec{n} = \vec{r}/|\vec{r}|$ are low-dimensional inputs \brandonedit{that} are passed into the hash encoder and MLP to output the RGB color 
% at pixel $p$.
% 
The texture coordinate is a low-dimensional input that is passed into the hash encoder and MLP to output the RGB color at pixel $p$.
Conventionally, hardcoded 2D texture coordinates are used for vertices of each face, which are interpolated and used to lookup \brandonedit{values in} % from 
a UV map (typically an image).
Due to UV unwrapping of a closed mesh, some mesh vertices have multiple texture coordinates.
%depending on which face they are attached to.
In contrast, we use a learned 3D texture coordinate for each vertex.
This serves two purposes: (1) it sidesteps the need for UV unwrapping by moving the textures coordinates into 3D space, and (2) learnable coordinates allow us to expand or shrink the 3D coordinates of each vertex, which in turn accommodate different levels of detail for each face (See Appendix). %\sidcomment{ablation or citation showing this?}
% In contrast, we use a learned 3D texture coordinate for each vertex, sidestepping the need for UV unwrapping, and expanding or shrinking the size of each face in the texture space, to accomodate varying levels of detail for each face.

% Given a face index $f_j = \{ t_{j1}, t_{j2}, t_{j3}\}$ where $t_{jk}$ represents the $k^{\text{th}}$ texture coordinate and barycentric coordinates $w_{j1}, w_{j2}, w_{j3}$ , the pixel has the texture coordinate 

% To capture realistic texture, we use a texture network instead of learnable per-face texels, which have limited representation capacity.
% However, we use the face index with its barycentric coordinates as input to our texture network, to stay consistent with standard rendering pipelines.
% This is similar to Deferred Neural Rendering \cite{deferredneuralrendering} which uses a rasterized image of UV coordinates of the scene which is processed by a U-Net to render the output. 
% However,  we process each pixel independently using an MLP.
% Since rendering pipelines typically return the rendered face with barycentric coordinates at each pixel, we use this as input to the texture network to output an RGB color.

\subsection{Differentiable rendering}
\label{sec:diffrender}
A \brandonedit{major} %big
advantage of using NeRF-like methods is that the volume rendering process is fully differentiable.
Moreover, the gradients with respect to the integral (to compute the color) takes occlusions into account.
This allows NeRFs to learn a robust occupancy volume and RGB colors to minimize rendering errors.
In contrast, rasterization is an inherently non-differentiable operation with respect to the vertices of the mesh ~\cite{liu2019softras}.
\brandonedit{Many solutions have been proposed} 
to approximate gradients of the vertices from gradients in rendered images
\brandonedit{(\eg, \cite{kato2018renderer, liu2019softras, loper:eccv2014:opendr}).}
We use SoftRas ~\cite{liu2019softras} due to 
its ability to flow gradients to the occluded and far-range vertices, allowing us to perform pose refinement via analysis-by-synthesis.
Complex pose changes such as bringing an occluded limb into view, rotating joints, etc. can now be performed since SoftRas allows us to pass gradients into the occluded parts of the render. 
Empirically, we observe that SoftRas is helpful in updating body pose when a \brandonedit{small amount} of joint rotation is required. 
% which cannot be achieved by the previous state-of-thearts
% its ability to `blur' the faces and see-through occluding vertices, allowing us to pass useful gradients to 
To learn the texture, we need the exact forward rasterization to map texture coordinates to RGB values. 
In SoftRas, we can set the softening parameters $\gamma = \sigma = 0$ and use the RGB loss to guide the texture learning.
% However, this leads to numerical instability in SoftRas.
However, this leads to numerical instability and rendering artefacts in SoftRas.
To mitigate this issue, we use NMR ~\cite{kato2018renderer} to propagate texture gradients.

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/novelview/novelview-m3c-1.png}
%     \includegraphics[width=\linewidth]{figures/novelview/novelview-f3c-1.png}
%     \caption{Novel view synthesis on people-snapshot dataset (\textbf{m3c} and \textbf{f3c}). SMPLpix and neuralbody do not generalize well to novel views. VideoAvatar doesn't capture intricate shape details (green boxes) and face identity. Our method performs competitively with AnimNeRF. More qualitative results in Appendix.}
%     \label{fig:novelview}
% \end{figure}

\subsection{Forward model}
\label{sec:forward_model}
In this section, we describe the forward model for a given frame $i$. 
The SMPL+D and camera parameters $(\bbeta, \btheta_i, \bD, \bR_i, \bt_i)$ are used to generate the mesh $\bM_i$ which is projected into the image plane as $\bm_i$.
The projected mesh and texture network parameters are passed into NMR and SoftRas to give us
an opaque and translucent RGB image respectively. 
%an RGB image.
% For the SoftRas version, we do not need gradients with respect to the texture parameters.
% Therefore, the Softras image is given as:
% $\hat{\bI}_{i,\text{NMR}}$.
% More specifically, we have:
For texture parameters $\phi$, we have:
% \begin{equation}
%     \hat{\bI}_{i,\text{NMR}} = \text{NMR}(\bm_i; \phi) 
% \end{equation}
\begin{equation}
    \left[\hat{\bI}_{i,\text{NMR}}; \hat{\bI}_{i,\text{SR}}\right] = \text{NMR}(\bm_i; \phi), \text{SoftRas}(\bm_i; \text{sg}(\phi),\sigma,\gamma)
\end{equation}
% \begin{equation}
%     \hat{\bI}_{i,\text{SR}} = \text{SoftRas}(\bm_i; \text{sg}(\phi), \sigma, \gamma) 
% \end{equation}
where $\sigma,\gamma$ are the face blur and depth scale parameters of SoftRas, and $\text{sg}$ is the stop-grad operator.
% Note that gradients coming from $\hat{\bI}_{i,\text{SR}}$ only backpropagate to the mesh parameters and not to the texture parameters. 


\subsection{Loss functions}
\label{sec:loss_functions}
\brandonedit{In this section, we describe the losses used to generate our results.}
Let the masked ground-truth image for frame $i$ be $\bI_i$ and ground-truth binary foreground be $\bS_i$.
\vspace*{-5pt}
\paragraph{Image losses.}
The RGB losses are given by:
\begin{equation}
    \mathcal{L}_{i,\text{RGB}} = \| \bI_i - \hat{\bI}_{i,\text{NMR}} \|_1 + \| \bI_i - \hat{\bI}_{i,\text{SR}} \|_1 
\end{equation}
We also obtain a binary silhouette using NMR and projected mesh $\bm_i$, which we denote as $\hat{\bS}_i$.
The silhouette loss is defined as \brandonedit{an IOU loss}:
\begin{equation}
    \mathcal{L}_{i, \text{Sil}} = 1 - \frac{\sum_p \hat{\bS}_i(p)\cdot \bS_i(p)}{\sum_p (\hat{\bS}_i(p) + \bS_i(p) - \hat{\bS}_i(p)\cdot \bS_i(p))}.
\end{equation}

\paragraph{Keypoint loss.}
\brandonedit{We add a keypoint loss to help guide the pose of the SMPL model. 
For simplicity,} we use only keypoints corresponding to limbs (elbows, wrists, knees, ankles) and nose.
From the mesh $\bM_i$, we project keypoints $\hat{\bk}_i$ and \brandonedit{encourage their proximity to target 2D keypoints $\bk_i$ via the loss:}
\begin{equation}
    \mathcal{L}_{i,\text{kps}} = \| \hat{\bk}_i - \bk_i \|_2^2.
\end{equation}
\brandonedit{We run HRNet~\cite{ke:cvpr2019:hrnet} on each frame to produce $\bk_i$ for people-snapshot and use the given keypoints for ZJU Mocap.}
%Finally, we also run HRNet to get sparse 2D keypoints from each frame. 
\begin{table*}[ht!]
    \centering
    \small
    \setlength{\tabcolsep}{5pt}
    \begin{tabularx}{\linewidth}{Xccccc|ccccc|ccccc}
    \toprule 
    \multicolumn{16}{c}{\textbf{Novel view (people-snapshot)}} \\ \hline
    \multirow{2}{*}{\textbf{Subject ID}} & \multicolumn{5}{c|}{\textbf{PSNR}$\uparrow$} & \multicolumn{5}{c|}{\textbf{SSIM}(x100) $\uparrow$} & \multicolumn{5}{c}{\textbf{LPIPS}(x100) $\downarrow$} \\
    % methods
    & \textbf{VA} & \textbf{SP} & \textbf{NB} & \textbf{AN} & \textbf{Ours} & \textbf{VA} & \textbf{SP} & \textbf{NB} & \textbf{AN} & \textbf{Ours} & \textbf{VA} & \textbf{SP} & \textbf{NB} & \textbf{AN} & \textbf{Ours}  \\ \hline
    \textbf{m3c} & 22.91 & 22.94 & 23.98 & \textbf{29.43} & \underline{29.40} & 93.16 & 92.56 & 96.12 & \textbf{97.11} & \underline{96.24} & 4.87 & 6.89 & 7.24 & \textbf{1.85} & \underline{2.65} \\
    \textbf{m4c} & 22.63 & 21.43 & 22.84 & \textbf{27.50} & \underline{26.31} & 93.22 & 92.66 & \underline{94.81} & \textbf{95.87} & 94.27 & 6.00 & 8.04 & 10.93 & \textbf{3.77} & \underline{5.30} \\
    \textbf{f3c} & 22.10 & 21.80 & \underline{23.19} & 22.96 & \textbf{27.25} & 94.35 & 93.95 & \underline{95.83} & 94.56 & \textbf{96.21} & 5.43 & 5.61 & 10.22 & \underline{4.61} & \textbf{3.47} \\
    \textbf{f4c} & 23.49 & 22.64 & 22.18 & \underline{29.03} & \textbf{29.61} & 93.99 & 93.27 & 95.63 & \textbf{96.88} & \underline{96.61} & 4.12 & 5.92 & 8.52 & \textbf{2.10} & \underline{2.42} \\
    \hline
    \multicolumn{16}{c}{\textbf{Novel view (ZJU Mocap)}} \\ \hline
    & \textbf{VA} & \textbf{SP} & \textbf{NB} & \textbf{HN} & \textbf{Ours} & \textbf{VA} & \textbf{SP} & \textbf{NB} & \textbf{HN} & \textbf{Ours} & \textbf{VA} & \textbf{SP} & \textbf{NB} & \textbf{HN} & \textbf{Ours}  \\ \hline
    \textbf{C377}  & 24.48 & 27.28 & 24.81 & \textbf{30.86} & \underline{30.58} & 93.12 & 94.64 & \underline{97.17} & \textbf{97.45} & 96.51 & 9.01 & 5.82 & 5.71 & \textbf{2.58} & \underline{4.48} \\
    \textbf{C386}  & 27.67 & 29.22 & 25.08 & \textbf{33.36} & \underline{33.28} & 93.72 & 95.80 & \underline{97.27} & \textbf{97.29} & 96.22 & 7.98 & 10.65 & 4.86 & \textbf{3.39} & \underline{4.18} \\
    \textbf{C387}  & 23.30 & 24.27 & 23.60 & \textbf{28.58} & \underline{28.07} & 92.58 & 95.08 & \underline{96.08} & \textbf{96.10} & 94.73 & 9.18 & 12.08 & 6.88 & \textbf{3.96} & \underline{6.40} \\
    \textbf{C392}  & 25.70 & 28.66 & 24.35 & \textbf{31.42} & \underline{31.35} & 92.89 & 95.64 & \textbf{96.86} & \underline{96.83} & 96.05 & 9.52 & 7.54 & 6.37 & \textbf{3.76} & \underline{5.37} \\
    \textbf{C393}  & 23.45 & 24.83 & 24.17 & \textbf{28.89} & \underline{28.35} & 92.30 & 93.60 & \textbf{96.35} & \underline{95.83} & 93.88 & 10.99 & 12.77 & 6.66 & \textbf{4.22} & \underline{6.43} \\
    \textbf{C394}  & 24.46 & 27.34 & 23.97 & \underline{30.73} & \textbf{31.21} & 91.67 & 95.58 & \textbf{96.43} & \underline{96.16} & 95.57 & 11.28 & 9.07 & 6.71 & \textbf{3.75} & \underline{4.91} \\ \hline
    \multicolumn{16}{c}{\textbf{Novel pose (ZJU Mocap)}} \\ \hline
    \textbf{C377}  & 24.36 & 27.00 & 23.84 & \textbf{30.50} & \underline{30.48} & 93.25 & 96.51 & \underline{96.78} & \textbf{97.41} & 96.54 & 8.29 & 5.81 & 5.59 & \textbf{2.69} & \underline{4.27} \\
    \textbf{C386}  & 28.34 & 30.38 & 23.26 & \underline{33.55} & \textbf{34.03} & 93.84 & 96.60 & \underline{96.46} & \textbf{97.20} & 96.16 & 7.43 & 9.79 & 5.50 & \textbf{3.41} & \underline{4.00} \\
    \textbf{C387}  & 23.02 & 23.80 & 23.15 & \textbf{29.02} & \underline{28.43} & 92.83 & 95.38 & \underline{95.58} & \textbf{96.44} & 95.23 & 8.46 & 11.47 & 6.77 & \textbf{3.25} & \underline{5.71} \\
    \textbf{C392}  & 25.83 & 29.12 & 22.46 & \underline{31.43} & \textbf{32.22} & 92.98 & \underline{96.44} & 95.97 & \textbf{96.89} & 96.37 & 9.40 & 7.26 & 7.03 & \textbf{3.70} & \underline{5.01} \\
    \textbf{C393}  & 23.50 & 24.79 & 22.41 & \textbf{29.32} & \underline{28.62} & 92.49 & 95.07 & \underline{95.45} & \textbf{96.09} & 94.07 & 10.58 & 12.65 & 7.13 & \textbf{3.86} & \underline{6.47} \\
    \textbf{C394}  & 24.33 & 26.99 & 22.19 & \underline{30.20} & \textbf{30.36} & 91.72 & 95.64 & \underline{95.43} & \textbf{95.95} & 95.10 & 11.09 & 8.44 & 7.29 & \textbf{4.07} & \underline{5.25} \\
    \bottomrule
    \end{tabularx}
    \caption{Results on novel view and pose synthesis on people-snapshot and ZJU datasets. Best results are in \textbf{bold} and 2nd best is \underline{underlined}.}
    \label{tab:novelview}
\end{table*}

\paragraph{Mesh regularization losses.}
%Furthermore, 
% \brandonedit{Due to image noise, ambiguities, \etc, we add some regularization to the mesh representation.}
%We want to regularize the mesh representation, especially %the per-vertex offset $\bD$, which is free-form.
We add some regularization to the mesh representation.
\brandonedit{This is especially important for the free-form per-vertex offsets $\bD$.}
Unlike previous works (\eg, \cite{alldieck:cvpr2018:peoplesnap}), we observe that imposing a low-deformation loss reduces performance (Sec~\ref{sec:geomrecon}). % \sidcomment{Ablation?}
This is because loose clothing need not necessarily correspond to a low deformation from the underlying SMPL model.
We only encourage normal consistency of adjacent faces in the mesh.
Let $\bM_D$ be the mesh generated from the SMPL parameters $(\bbeta, \boldsymbol{0}, \bD, \mathbb{I}, \boldsymbol{0})$
%Moreover, 
\brandonedit{and} let $\bM_0$ be generated from the SMPL parameters $(\bbeta, \boldsymbol{0}, \boldsymbol{0}, \mathbb{I}, \boldsymbol{0})$.
Let $f_j$ be the $j^{\text{th}}$ face of $\bM_D$ and $f'_j$ be the $j^{\text{th}}$ face of $\bM_0$.
With some abuse of notation, for two faces $f_j$ and $f_k$, we denote $|f_j \cap f_k|$ as the number of vertices that are shared between both faces.
The normal consistency loss is then given as:
\begin{equation}
    \mathcal{L}_{NC} = \mathlarger{\sum}_{|f_j \cap f_k| = 2} (1 - \hat{n}_{f_i}\cdot \hat{n}_{f_j}),
\end{equation}
where $\hat{n}_{f}$ is the outward normal of face $f$.
%
We also encourage each face in the mesh to have the same area with and without the deformation.
This respects the relative sizes of faces corresponding to different regions in the mesh.
If $A_{f}$ represents the unsigned area of a face $f$, the face area loss is given as:
\begin{equation}
    \small
    \mathcal{L}_{FA} = \mathlarger{\sum}_j \Bigg( \frac{A_{f_j}}{A_{f'_j}} + \frac{A_{f'_j}}{A_{f_j}}\Bigg).
\end{equation}
We prefer this loss instead of the L2 loss $\| A_{f_j} - A_{f'_j} \|_2^2$ because the gradients of L2 loss
are small when the area $A_{f_j}$ approaches 0.
On the other hand, we want to penalize shrinkage or expansion equally. The loss we propose is of the form $x + \frac{1}{x}$ which achieves its minima at $x = 1$ for positive $x$. 
% \paragraph{Total loss.}
The total loss is: % given as: 
\begin{multline}
    \mathcal{L}_f = \sum_i \left( \lambda_{\text{RGB}}\mathcal{L}_{i,\text{RGB}} + \lambda_{\text{Sil}}\mathcal{L}_{i,\text{Sil}} +  \lambda_{\text{kps}}\mathcal{L}_{i,\text{kps}} \right) \\
    + \lambda_{NC}\mathcal{L}_{NC} + \lambda_{FA}\mathcal{L}_{FA}
\end{multline}

\brandonedit{Unlike \cite{chen:arxiv2021:animner},} 
%An interesting thing to note is that 
we do not add any other regularization terms on $\bbeta$ or temporal pose consistency or deviation terms. % like \cite{chen:arxiv2021:animner}.
% We observe that the photometric loss combined with two-stage training  suffices to faithfully reconstruct a realistic textured avatar. \sidcomment{Two stage training was not defined before this.}

\subsection{Two-stage training}
\label{sec:two_stage_training}
Given a forward model to render RGB and silhouettes from mesh parameters $\bbeta, \btheta_i, \bD, \bR_i, \bt_i$ and texture parameters $\phi$, an intuitive way to learn all the parameters is to jointly optimize them.
Let $\Theta = \{\phi^*, \bbeta^*, \bD^*, \{\btheta_i^*, \bR_i^*, \bt_i^*\}_{i \in \{1\ldots n\}} \}$ be the set of all optimizable parameters. The optimization is of the form:
$
    \Theta^* = \arg\min_\Theta \mathcal{L}_f.
$
% \begin{equation}
% \end{equation}
This optimization is still highly underconstrained. % primarily due to the texture network's learning capacity.
Meshes obtained using this procedure are jagged \brandonedit{with blurry textures} because %are blurry, since   
texture and deformation parameters locally optimize their own parameters (examples in Appendix) leading to the \textit{moving target} problem.
% An ablation is shown in Section \ref{sec:twostage}.
To alleviate this problem, we propose a two-stage training procedure.
In the first stage, we use a \textit{per-face} RGB value as a `base' texture color instead of the full texture network.
This allows \brandonedit{for} %to perform 
a coarse alignment of the mesh vertices for each frame.
Constraining the color of each face to just one optimizes the mesh parameters to place it in the best possible location and scale to minimize RGB and silhouette losses, thus ensuring 
% an additional 
photogrammetric
consistency in the optimization.
% All mesh parameters and the per-face color are jointly optimized.
In the second stage, the deformations, shape, and per-face RGB values are fixed, and the texture network is trained with per-frame pose \brandonedit{refinement.}
This \brandonedit{allows for fine-grained alignment of the poses for each frame.} % and improves performance.}
Implementation details are \brandonedit{provided} %present 
in the Appendix.
% Knowing that one can optimize the pose, shape, and texture to recover an accurate representation of the subject, one might be tempted to jointly optimize all parameters, including shape $\beta$, poses $\{(\theta_i, R_i, t_i)\}_i$ and texture network $T_{\phi}$ simultaneously.
