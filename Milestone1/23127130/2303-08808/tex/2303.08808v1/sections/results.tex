
%%%%%% TODO Fill this table
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/female3c-novelpose.png}
    \includegraphics[width=0.95\linewidth]{figures/male3c-novelpose.png}
    \caption{Novel pose synthesis (on \textbf{m3c} and \textbf{f3c}) from left to right: SMPLpix, VideoAvatar, NeuralBody, AnimNeRF, Ours. NeuralBody does not generalize to novel poses, AnimNeRF introduces cloud and tearing artifacts. Our method preserves detailed texture and doesn't produce artifacts. More results in Appendix.}
    \label{fig:novelpose}
\end{figure}

\section{Results}
\label{sec:results}
To evaluate our method, we look at the following aspects of geometry and texture recovery: (1) novel-view and pose synthesis, (2) training/inference time and compute requirements, (3) geometry reconstruction.
For experiments (1-2), we use People Snapshot ~\cite{alldieck:cvpr2018:peoplesnap} and ZJU Mocap ~\cite{peng:cvpr2021:neuralbod} datasets.
For (3), we use the Self-Recon synthetic dataset ~\cite{Jiang22selfrecon}.
For people-snapshot, we follow the same subjects and experiment setup as ~\cite{chen:arxiv2021:animner}, and for ZJU Mocap we use the same set of subjects as ~\cite{weng:cvpr2022:humannerf}.
For ZJU Mocap, we use frames 0-450 in cameras 1,7,13,19 for training, and the rest of the frames for novel pose reconstruction.
We use frames 0-450 from cameras 5,10,15,20 for novel view reconstruction.
We choose baselines across a spectrum of representation choices: {S}MPL{P}ix ~\cite{smplpix} (\textbf{SP}) which uses deferred rendering, {V}ideo{A}vatar ~\cite{alldieck:cvpr2018:peoplesnap} (\textbf{VA}) which performs SMPL+D optimization, {N}eural{B}ody~\cite{peng:cvpr2021:neuralbod} (\textbf{NB}), {H}uman{N}eRF~\cite{weng:cvpr2022:humannerf} (\textbf{HN}) and {A}nim{N}eRF~\cite{chen:arxiv2021:animner} (\textbf{AN}) which are SOTA NeRF methods.

% To evaluate our method, we look at the following aspects of mesh and texture recovery - (1) novel view synthesis, as defined in the setup of Anim-NeRF \cite{chen:arxiv2021:animner}, (2) novel pose synthesis, (3) training time and compute requirements,  (4) realism of learned texture, and (5) geometry reconstruction.
% For experiments (1-4) we use the People-snapshot \cite{alldieck:cvpr2018:peoplesnap} and ZJU Mocap ~\cite{peng:cvpr2021:neuralbod} datasets, and for experiment (5) we use the Self-Recon synthetic dataset \cite{Jiang22selfrecon}.
% For the people-snapshot dataset, we use the subjects: male-3-casual (\textbf{m3c}), male-4-casual (\textbf{m4c}), female-3-casual (\textbf{f3c}), female-4-casual (\textbf{f4c}), with the same set of training images as Anim-NeRF.
% For ZJU, we use the same subjects as in HumanNeRF~\cite{weng:cvpr2022:humannerf}.
% % For training, we use frames 0-450 from Camera 1,
% % We use the same frames as Anim-NeRF to train our model.
% We compare our method with SMPLpix \cite{smplpix}, VideoAvatar \cite{alldieck:cvpr2018:peoplesnap}, Neural Body \cite{peng:cvpr2021:neuralbod}, HumanNeRF~\cite{weng:cvpr2022:humannerf} and Anim-NeRF \cite{chen:arxiv2021:animner}.
% We choose baselines across a spectrum of representation choices: mesh optimization (VideoAvatar), SMPL combined with advanced deferred rendering pipelines (SMPLpix), and powerful SOTA NeRF methods (NeuralBody, AnimNeRF, HumanNeRF).

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/zju-main.png}
    \caption{\textbf{Novel view on ZJU dataset, Left to right}: VideoAvatar, SMPLPix, NeuralBody, HumanNeRF, Ours. Our method performs dramatically better than VA/SMPLPix and is comparable to NeRF methods at significant training and inference speedups.}
    \label{fig:zju}
\end{figure}

\subsection{Novel view synthesis}
% Similar to AnimNeRF \cite{chen:arxiv2021:animner}, we 
% approximate this 
We evaluate novel view synthesis
by holding back a certain set of test frames of the subject to check the quality of reconstruction for those frames (similar to ~\cite{chen:arxiv2021:animner}).
For ZJU Mocap, we evaluate frames 0-450 from cameras 5,10,15,20.
% We optimize the SMPL poses for those frames, keeping the learned texture and shape parameters constant.
Results are shown in Table \ref{tab:novelview}.
Our method performs very competitively with AnimNeRF and HumanNeRF in terms all three metrics (PSNR, SSIM, LPIPS), and outperforms all other baselines by a substantial margin.
On ZJU Mocap, our method competes with HumanNeRF consistently on the PSNR and LPIPS metrics, showing that our method can faithfully reconstruct accurate and realistic rendering.
NeuralBody and SMPLpix are trained on the training poses only, and fail to generalize to novel views, even if the view deviations are very small from the training frames.
VideoAvatar performs a multi-stage optimization, where the geometry is optimized from silhouettes only, and then a texture optimization step is performed.
Errors from the mesh optimization propagate to the texture, leading to a low quality texture, and subsequently a low quality render.
Our method recovers intricate details like loose clothing, loose pants, hoodies, hair, and skirts (Fig.~\ref{fig:geom}).
% Our method recovers intricate details (\eg, faces, loose clothing in \textit{f3c}, loose pants in \textit{C377,C392,C394}, hoodie in \textit{C387}), and performs competitively with NeRF methods, in a very small time and compute budget (Sec. \ref{sec:trainingtime}).
More qualitative results are in the Appendix.

\begin{table}[t!]
    \centering
    \small
    \begin{tabular}{lrrrr} \toprule
        \multicolumn{5}{c}{\textbf{FID score} $\downarrow$} \\ \midrule
        \textbf{Method} & \textbf{m3c} & \textbf{m4c} & \textbf{f3c} & \textbf{f4c} \\ \midrule
        \textbf{SMPLpix} &  199.04 & 210.27 & 211.09 & 212.16\\
        \textbf{Neural body} & 402.47 & 357.77 & 328.26 & 358.54 \\
        \textbf{VideoAvatar} & 189.82 & \underline{186.03} & \underline{206.07} & 162.79 \\
        \textbf{AnimNeRF} & \underline{183.03} & 200.73 & 237.79 & \textbf{150.80} \\ 
        \textbf{Ours} & \textbf{178.71} & \textbf{184.26} & \textbf{203.42} & \underline{159.31} \\ \midrule
        \multicolumn{5}{c}{\textbf{VGGFace2} $\uparrow$} \\ \midrule
        \textbf{SMPLpix} & .4808 & .6472 & .6222 & .4853\\ 
        \textbf{NeuralBody} & .3713 & .3743 & .4301 & .0000 \\
        \textbf{VideoAvatar} & .8135 & .8799 & .8976 & .8417 \\ 
        \textbf{AnimNeRF} & \textbf{.9079} & \textbf{.8974} & \textbf{.9452} & \textbf{.9259} \\ 
        \textbf{Ours} & \underline{.8766} & \underline{.8926} & \underline{.9380} & \underline{.8948} \\ 
        \bottomrule
    \end{tabular}
    \caption{Quantitative analysis of texture quality of novel poses.} 
    \label{tab:fid}
\end{table}

\subsection{Novel pose synthesis}
\label{sec:novelpose}
We use a set of held-back frames for novel pose synthesis in the ZJU dataset.
Results are in Tab.~\ref{tab:novelview}.
% and Fig.~\ref{fig:novelposezju}.
% Owing to the representation we choose, we also qualitatively compare the quality of novel pose estimations.
% Qualitative results are shown in Fig.~\ref{fig:novelpose}.
SMPLpix and Neural Body do not generalize well because novel poses \sidedit{that are} not seen during training \sidedit{results in} a distribution shift in the inputs of the frameworks.
% VideoAvatar has blurry texture, and has an uncanny valley effect in the faces of its rendered outputs.
VideoAvatar has an uncanny valley effect in the faces of its rendered outputs.
HumanNeRF achieves highly realistic results capturing nuances in body geometry and texture.
% 

\textbf{OOD pose rendering}: However, we notice that the pose distribution is not very different from those in training frames.
Moreover, the people-snapshot dataset doesn't contain frames with other poses than the A-pose.
Therefore, we also compare the realism of textures and faces in an a set of OOD poses.
We curate a set of poses from the AMASS dataset~\cite{AMASS}.
We compare the realism of the models by evaluating the \brandonedit{Fr\'{e}chet Inception Distance (FID score)}~\cite{Seitzer2020FID} of the input frames with novel pose renders, and comparing the face texture of the rendered images with that of the input frames.
% The factors contributing to a non-zero FID score would be errors in the texture representation, and difference in pose distribution of the input frames and the novel pose renders.
Since we use the same novel poses for all methods, the differences in FID must come from texture quality.
To evaluate texture quality of faces, we use face identification as a proxy task.
We use MTCNN \cite{zhang2016mtcnn} to detect faces from images and VGGFace2 \cite{cao2018vggface2} to generate a template feature vector for each method.
We use the face similarity metric between the template of the method and that of the input data, as proposed in \cite{cao2018vggface2}.
Results in Tab.~\ref{tab:fid} and Fig.~\ref{fig:novelpose} shows that our method preserves the subject identity significantly more than VideoAvatar, showing that our method can recover accurate texture 
with a mesh.
AnimNeRF reconstructs the texture well in the parts on the surface, but introduces cloud and tearing artifacts, especially when regions around the unseen areas (armpits and thighs) are stretched too much.
% The novel pose results shown in Anim-NeRF didn't explore the effect of distortion of the occupancy fields when the poses are stretched too much. 
Our method doesn't have such an issue, since our representation is based on a mesh.
Moreover, the texture distortion for novel poses is virtually non-existent.
More qualitative results are shown in Appendix.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/geometry-v2.png}
    \caption{Geometry reconstruction quality of our method (top) and VideoAvatar (bottom) on ZJU Mocap, people-snapshot, and Self-Recon datasets. 
    Our method captures loose fitting pants (1,3) and clothes (2,4,6), hoodie (2), hair (1,2,3), skirts (6).
    }
    \label{fig:geom}
\end{figure}


\subsection{Training/inference time and compute}
\label{sec:trainingtime}

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{lrrr} \toprule
        % \multicolumn{4}{c}{\textbf{people-snapshot}} \\ \midrule
        \textbf{Method} & \textbf{Training} & \textbf{Inference} & \textbf{GPU usage} \\ 
        & \textbf{time (min)} & \textbf{time (sec)} & (\textbf{GB-hrs}) \\ \midrule
        HN~\cite{weng:cvpr2022:humannerf} & 1013.35 & 3.51 & 399.09 \\ % 456.38 actually, I forgot to add the 4GB that is used in the other GPU
        AN~\cite{chen:arxiv2021:animner} & 769.26 & 11.54 & 591.38 \\
        NB~\cite{peng:cvpr2021:neuralbod} & 1020.27 & 0.77 & 62.22 \\
        Ours & \textbf{43.31} & \textbf{0.06} & \textbf{4.11} \\
        \bottomrule
    \end{tabular}
    \caption{Averaged total training and per image inference time (in minutes) and total GPU usage (GB-hr). 
    Our model train upto 24x faster than HumanNeRF on the same data while using upto 4x less compute.
    Results for individual subjects are in the Appendix.
    }
    \label{tab:trainingtime}
    \vspace*{-7pt}
\end{table}

% We analyse the effect of training and inference time and compute required for each of the methods 
%This is shown 
% in Table~\ref{tab:trainingtime}.
We compare the training and inference time and compute required our method against NeRF methods in Tab.~\ref{tab:trainingtime}.
NeRFs have achieved huge successes in representing scenes faithfully with very accurate rendering.
However, they take a prohibitively long time to train a single scene.
Although several %Even though a number of 
improvements have been proposed for static scenes, 
% \brandonedit{(see~\cite{mueller2022instantngp} and its references)}
the main bottleneck for AnimNeRF is the KNN step for each sample along the ray, and unposing the transformation to the canonical space.
This is a computationally expensive step since it has to be done for each point from each ray independently.
Moreover, volume rendering leads to a significantly higher inference time and compute requirements ~\cite{Liu21neuralactor}.
\brandonedit{In contrast, unposing is trivial using our method} 
because the rasterization consists of the face index with barycentric coordinates.
% Our method serves as a highly practical method to learn realistic geometry and texture much faster than its NeRF counterparts.

% Moreover, AnimNeRF uses two RTX 3090s to train with the recommended configuration, while our method only takes upto 5.5GB of GPU memory for the same task.
% This leads to about a 150x factor improvement in GPU hours. % (Table \ref{tab:trainingtime}). 

% \vspace*{-3pt}
% \subsection{Realism of learned texture}
% Beyond qualitative analysis of novel poses, we want to quantify the realism of textures learned in our representation.
% We also quantify the realism of textures in novel poses in this section.
% We do this using two objectives: comparing the realism of the models by comparing the \brandonedit{Fr\'{e}chet Inception Distance (FID score)}~\cite{Seitzer2020FID} of the input frames with novel pose renders, and comparing the face texture of the rendered images with that of the input frames.
% The factors contributing to a non-zero FID score would be errors in the texture representation, and difference in pose distribution of the input frames and the novel pose renders.
% Since we use the same novel poses for all methods, the differences in FID should come from texture quality.
% To evaluate texture quality of faces, we use face identification as a proxy task.
% % Moreover, we compare the face identification score of the input frames with the faces generated by each method.
% We use MTCNN \cite{zhang2016mtcnn} to detect faces from images and VGGFace2 \cite{cao2018vggface2} to generate the template feature vector for each method.
% We use the face similarity metric proposed in \cite{cao2018vggface2}.
% Results in Table~\ref{tab:fid} shows that our method preserves the subject identity significantly more than VideoAvatar, showing that our method can recover accurate texture 
% % while using a mesh.
% with a mesh.

%%% Table for comparison
\begin{table}[t!]
    \centering
    \small
    \begin{tabular}{llrrrrr} \toprule
    \textbf{Metric} & \textbf{Method} & \textbf{F1} & \textbf{F2} & \textbf{F3} & \textbf{M1} & \textbf{M2} \\
    \midrule
        \multirow{2}{*}{Chamfer} & {VideoAvatar} & 1.47 & 1.05 & 1.41 & 1.20 & 1.08 \\
     & {Ours} & \textbf{1.15} & \textbf{0.96} & \textbf{1.05} &  \textbf{1.01} & \textbf{0.93} \\
     \midrule
    \multirow{2}{*}{P2S} & {VideoAvatar} & 0.59 & 0.46 & 0.57 & 0.54 & 0.45 \\
     & {Ours} & \textbf{0.42} & \textbf{0.40} & \textbf{0.37} & \textbf{0.40} & \textbf{0.35} \\
     \bottomrule
    \end{tabular}
    \caption{Reconstruction loss (cm) on Self-Recon synthetic dataset.} %Our method consistently performs better than VideoAvatar on both Chamfer and P2S distance.}
    \vspace*{-5pt}
    \label{tab:geom}
\end{table}

% \begin{table}[ht!]
%     \centering
%     \begin{tabular}{lrrrr} \toprule
%     \multirow{2}{*}{\textbf{Subject}} & \multicolumn{2}{c}{\textbf{P2S} $\downarrow$} & \multicolumn{2}{c}{\textbf{Chamfer} $\downarrow$} \\ 
%     &  \textbf{VideoAvatar} & \textbf{Ours} & \textbf{VideoAvatar} & \textbf{Ours} \\ \midrule
%     {Female 1} & 0.1 & 0 & 0.1 & 0 \\
%     {Female 2} & 0.1 & 0 & 0.1 & 0 \\
%     {Female 3} & 0.1 & 0 & 0.1 & 0 \\
%     {Male 1} & 0.1 & 0 & 0.1 & 0 \\
%     {Male 2} & 0.1 & 0 & 0.1 & 0 \\ \bottomrule
%     \end{tabular}
%     \caption{Quantitative results on geometry reconstruction. \brandoncomment{These are placeholder values.}}
%     \label{tab:geom}
% \end{table}


\subsection{Geometry reconstruction}
\label{sec:geomrecon}
We quantitatively compare the effectiveness of our method to recover the underlying geometry from the set of images 
using the Self-Recon dataset \cite{Jiang22selfrecon}, which consists of renderings of 5 human subjects with their ground truth meshes.
We compute the average Chamfer distance and Point-to-Surface (P2S) measures.
% We compare with VideoAvatar, the other mesh-based optimization method, shown in Tab.\ref{tab:geom}.
Our comparison with VideoAvatar~\cite{alldieck:cvpr2018:peoplesnap}, the other mesh-based optimization method is shown in Tab.~\ref{tab:geom}.
Note that our lower distances show that even a low dimensional mesh can capture complex details like loose clothing, hair, etc. with the right optimization and training scheme.   
Qualitative results on all three datasets are in Fig.~\ref{fig:geom} and Appendix.
% We show qualitative results for subjects \textbf{M2} and \textbf{F3} since they have the most amount of challenging clothing, in Fig.\ref{fig:geom}.
% Qualitative results of geometry reconstruction on people-snapshot and ZJU are provided in Appendix. %Supplementary Material.

% \subsection{Ablation on two stage training}
% \label{sec:twostage}
% Here we compare the results obtained by single stage training.
% Note that using a single stage for training is underconstrained in terms of the texture network and deformation.
% As a result, optimization moves towards a local minima and learned texture and geometry is extremely blurry.
