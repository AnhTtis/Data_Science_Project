\section{Introduction}
\label{sec:intro}
Our goal is to generate detailed, personalized, and animatable 3D human models. 
This has many downstream applications, including teleconferencing, entertainment, surveillance, and realistic synthetic data generation. 
3D body scanners are the gold standard when it comes to 3D reconstructions. 
Results are accurate and realistic, but scanners tend to be expensive and ungainly. 
Moreover, they require additional postprocessing, \brandonedit{such as registration and rigging,} before they can be used.
% An alternative to 3D scanners is 3D reconstruction from 2D RGB images.
% Computer vision technology is capable of accurately capturing the geometry of human bodies, including hair, clothing, and body shape.
% Early approaches used multiple calibrated cameras to recover 3D human geometry and appearance at a specific time instance, or independently for each synchronized frame of a video sequence.
More recently, CV-based systems have demonstrated recovering realistic 3D human geometry and appearance from monocular images or videos.
In the case of monocular \textit{images}, a predictor can be learned from a dataset of real or synthetic humans \cite{Sengupta20bmvc,Xiu22icon}.
However, this is ill-conditioned because a single view is insufficient to estimate the entire 3D human geometry or appearance completely or accurately.
Extending it to multi-view or 360$^\circ$ video input would require 
running inference 
for all frames
and fusing per-frame texture and mesh information.
% and somehow fusing the texture and mesh information.
% 
% In the case of monocular \textit{videos}, the idea is to capture a subject moving in the scene such that they are visible from multiple viewpoints with respect to a single RGB video camera.
3D geometry and texture recovery is therefore formulated as an optimization problem.
This is an alternative to expensive 3D scanning and motion capture pipelines.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth,height=0.8\linewidth]{figures/computeperftradeoff.png}
    \caption{\textbf{Performance vs training time tradeoff}: Our method maximizes the performance to training time tradeoff compared to other methods employing meshes, NeRFs and deferred rendering.}
    \label{fig:perf}
\end{figure}

Human-specific NeRFs have recently become popular under this latter, video-based formulation.
There are two downsides to {typical} NeRFs: (1) the highly unconstrained solution space of NeRFs and the use of volume rendering leads to long training and rendering times \eg, {up to a few days for training~\cite{chen:arxiv2021:animner,weng:cvpr2022:humannerf,Liu21neuralactor} and up to minutes for rendering 
% a $800 \times 800$ image
images~\cite{hedman:iccv2021:bakingnerf}}, and (2) dense viewpoints are required.
% Combined with the ,
% these methods typically require \brandonedit{significant} %a lot of 
% training time and compute.
% due to volume rendering.
% , and the solution space in NeRFs is highly unconstrained.
On the contrary, mesh-based reconstruction methods can be faster and less compute-intensive because they do not optimize over a volume. 
The solution space of the mesh is highly constrained due to flexible yet accurate parameterized human body priors like SMPL~\cite{SMPL:2015}. 
Moreover, mesh-based methods may perform better than NeRFs under sparser viewpoints without expensive pretraining or strong regularization that misses geometric details (~\cite{Niemeyer_2022_CVPR}), due to the human shape prior. 
% Moreover, NeRF-based methods do not fare well under sparser viewpoints \brandonedit{without expensive pretraining or strong regularization that misses geometric details~\cite{Niemeyer_2022_CVPR}}, whereas mesh-based methods may perform better due to the human shape prior.

\brandonedit{However, mesh-based methods that rely on a visual hull or silhouette-based approach (\eg, \cite{alldieck:cvpr2018:peoplesnap}) suffer from ambiguous shape recovery, \ie, any concave surface is not captured by the visual hull} and thus cannot be recovered.
We show that silhouette-based optimization is highly ill-posed (Sec.~\ref{sec:toyprob}). 
To overcome this ambiguity, recent methods augment silhouettes with depths or normals~\cite{Xiu22icon, dong:cvpr2022:pina}.
However, obtaining ground truth depths or normals %information 
is expensive, and prediction can be error-prone.
% 
RGB images provides additional information \brandonedit{that} can be used across multiple frames to better recover 3D information. 
However, directly employing an RGB loss is non-trivial because of \brandonedit{differentiability challenges} in rasterization.
NeRFs use the inherent `softness' of the occupancy field (in the volume integral) to reason about self-occluded portions of the body, allowing pose-refinement~\cite{chen:arxiv2021:animner}.
We use a soft differentiable rendering pipeline~\cite{liu2019softras} to emulate this behavior.
This allows us to use `analysis-by-synthesis` as part of our optimization problem.
% We use a `soft differentiable rendering' pipeline to emulate this behavior inherently present in NeRF's volume rendering, to account for occluded portions of the mesh in the optimization.   
We adopt an approach that uses texture information as part of the optimization similar to NeRFs, but we parameterize the body similar to mesh-based reconstruction methods.
Meshes are simpler and more efficient, which affords significant speedup (up to 24x in training time and 192x in inference time) compared to NeRFs.

However, a naive mesh-based optimization to simultaneously minimize RGB and silhouette losses does not work because of \textit{moving targets}.
% Moving targets refers to the problem of the mesh deformation guided by
The problem of \textit{moving targets} occur when the RGB losses between the image and a partially learnt texture representation hinder the mesh deformation and vice versa.
% Alldieck \etal~\cite{alldieck:cvpr2018:peoplesnap,alldieck:3dv2018:humanmono} propose to mitigate this with multi-stage optimization. 
% However, silhouette-based optimization is highly ill-posed (Sec.~\ref{sec:toyprob}). 
We propose a method to reduce the ill-posedness and mitigate the problem of moving targets in optimization using a two-stage optimization. 
We demonstrate that our 3D reconstruction results are similar in quality and accuracy to NeRFs, and significantly better than existing mesh-based reconstruction methods.
\sidedit{In addition, we show competitive results in novel view and novel pose synthesis} %as compared to NeRFs based methods.} 
\brandonedit{compared to NeRF-based methods.}

%Mesh-based methods that rely on a learning-based (as opposed to optimization-based) strategy either operate on a single view~\cite{Xiu22icon} and cannot accurately recover 360-degree shape and appearance, or require multiple pre-defined viewpoints~\cite{Smith193dv}, which precludes most in-the-wild applications.}
%Computer vision-based approaches, including recent NeRF-based methods~\cite{chen:arxiv2021:animner,jiang:eccv2022:neuman,peng:cvpr2021:neuralbod,weng:cvpr2022:humannerf,yao:arxiv2022:monoclothedbody} avoid these pitfalls by generating results from a monocular RGB video camera, but tend to be prohibitively inefficient – both to generate the model (e.g., 13 hours~\cite{chen:arxiv2021:animner}) and to render novel views and poses (e.g., Y seconds/minutes per frame [REF]) – and results often lack fidelity and detail (e.g., hair, facial features, clothing wrinkles). 
In summary, this paper makes three main contributions:
(1) To our knowledge, we present the first method to incorporate photogrammetric losses in the context of generating human avatars from monocular videos using a mesh representation (Sec.~\ref{sec:method}). This allows us to optimize texture and geometry using \textit{analysis-by-synthesis} in our optimization without additional auxiliary inputs.
(2) An efficient, multi-resolution texture representation using hash encoding capable of capturing fine details is proposed.
Unlike texel-based representations, capacity is not wasted on uniform-texture regions.
% Moreover, this texture can be normal-dependent, allowing us to capture diffuse components in multi-view scenarios.
(3) To mitigate the moving targets problem, where partially learnt texture and geometry hinder each other's loss functions, a novel two-stage optimization is proposed. This ensures stability and optimal convergence.

% \begin{enumerate}
% %% First one to do photogrammetric loss
% \item To our knowledge, we present the first method to incorporate photogrammetric losses in the context of generating human avatars from monocular videos using an explicit shape representation (Section \ref{sec:method}).
% %% Texture representation
% \item An efficient, multi-resolution texture representation using hash encoding capable of capturing fine details; unlike texel-based representations, capacity is not wasted on uniform-texture regions and texture can be normal-dependent (Section \ref{sec:texnet}). 
% % \item A novel combination of NMR~\cite{kato2018renderer} and SoftRas~\cite{liu2019softras} differentiable rendererers to better optimize objectives (Section \ref{sec:diffrender}). 
% \item A novel two-stage training method combined with differentiable rendering using NMR~\cite{kato2018renderer} and SoftRas~\cite{liu2019softras} to optimize RGB and geometry simultaneously, preventing multi-stage optimization in which errors over different stages compound (Section ~\ref{sec:two_stage_training}).
% \end{enumerate}