\section{Overview}
In this supplementary material, we detail the network architectures and training hyperparamters used in our experiments. Section \ref{A} shows how to construct the whole network with basic blocks. Section \ref{B} details the training hyperparamters used in our experiments.

\section{Network Architecture}\label{A}
\subsection{Overall Framework}

We illustrate the architectures of FCN and UNET in Figure \ref{arch}. Only two levels of downsampling/upsampling are shown in the figure for simplicity. Note that for each level only one SSC block is drawn, while in fact there may be one or two blocks according to the size of networks.

FCN-S has 16 filters in the input layer, and one SSC block per level. FCN-H has 24 filters in the input layer, and two SSC blocks per level. Both networks use eight levels of downsampling and upsampling. We increase the number of filters in the networks when downsampling: in particular, we add 16 (S) or 24 (H) filters every time we reduce the scale. 
UNET-S has 16 initial filters and one SSC block per level. UNET-H has 32 initial filters and two SSC blocks per level. Both networks use six levels of downsampling and upsampling. Each downsampling operation adds 16 (S) or 32 (H) filters, while upsampling operation subtracts the same numbers of filters correspondingly.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/supp_block.pdf}
    \caption{Details of the basic blocks. (a) SSC layer, (b) SSC block, (c) Conv layer, (d) DeConv layer.}
    \label{block}
\end{figure*}

When binarizing the network, the first SSC layer and the linear layers are kept real-valued following previous methods~\cite{liu2018bi,liu2020react,martinez2020training}. We adopt our SFSC module in the SSC block for BSC-Manual and BSC-Net, which is detailed in next subsection.

\subsection{Block Detail}
% replace the SSC of the first block with SFSC
We detail the structure of basic blocks contained in the binary FCN and UNET in Figure \ref{block}. For SSC layer, the projection indicates identity mapping when the input and output channel are equal, otherwise it is a 1-bit $1\times 1$ convolution operation. For Conv and DeConv blocks, we keep the $1\times 1$ convolution operation real-valued, which is proved to be essential for binarizing performance~\cite{liu2018bi,martinez2020training}.


\section{Training Hyperparamters}\label{B}
There are 2 training stages when training BSC-Baseline and BSC-Manual while 3 stages when training BSC-Net. For all stages, we set max epoch as 128, weight decay as 0 and adopt Adam optimizer with a stepwise scheduler which steps at 60 and 100 epoch (reduce the learning rate by a factor of 10).
The initial learning rates for the first and second stage when training BSC-Baseline and BSC-Manual are set to 0.001 and 0.0002. While for training BSC-Net, the initial learning rates for the three stages are set to 0.001, 0.001 and 0.0002. The weight of the confidence loss is set to 0.1 for FCN and 0.01 for UNET. Note that for training UNET-H on \emph{2cm} voxel,
we double the max epoch and the stepping epochs as the huge network is hard to converge.

% For NYUDv2, the initial learning rate and $L2$ weight decay for W4A4, W4A1, W1A1 and W1A1-discrete (when training AutoShift) are ($1e-1$, $1e-4$), ($2e-2$, $1e-5$), ($1e-2$, $0$) and ($1e-3$, $0$) respectively. We use SGD with momentum of 0.9, Nesterov updates to optimize the \emph{network parameters}. The learning rate is decayed by a factor of $e^{-0.02}$ after every epoch. For ScanNet, the initial learning rate and $L2$ weight decay for W4A4, W4A1, W1A1 and W1A1-discrete are ($1e-3$, $0$), ($2e-4$, $1e-5$) ,($1e-4$, $0$) and ($1e-3$, $0$) respectively. We use Adam with a stepwise scheduler to optimize the network parameters. For each stage, we set steps at epochs 60 and 100.



%To train the networks, our method uses the 3-stage optimization strategy. 
%For NYUDv2, the initial learning rate and $L2$ weight decay for W4A4, W4A1, W1A1 and W1A1-discrete (when training AutoShift) are ($1e-1$, $1e-4$), ($2e-2$, $1e-5$), ($1e-2$, $0$) and ($1e-3$, $0$) respectively. We use SGD with momentum of 0.9, Nesterov updates to optimize the \emph{network parameters}. The learning rate is decayed by a factor of $e^{-0.02}$ after every epoch. For ScanNet, the initial learning rate and $L2$ weight decay for W4A4, W4A1, W1A1 and W1A1-discrete are ($1e-3$, $0$), ($2e-4$, $1e-5$) ,($1e-4$, $0$) and ($1e-3$, $0$) respectively. We use Adam with a stepwise scheduler to optimize the network parameters. For each stage, we set steps at epochs 60 and 100.
%Different initial learning rate and lr/weight decay are adopted for each stage and dataset. %We use the same method proposed in \cite{wang2019haq} to quantize the weight or activation to 4-bit for initialization.