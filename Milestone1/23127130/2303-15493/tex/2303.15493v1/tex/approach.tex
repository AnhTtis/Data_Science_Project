In this section, we first briefly introduce the preliminary concept of sparse convolution and network binarization. Then we conduct experiments to show the quantization errors of network binarization methods in different convolution patterns, and introduce the shifted sparse convolution (SFSC) operation which is activated for sites in various locations of the receptive field. Finally, we demonstrate the differentiable search to discover the optimal position for active site matching in SFSC, and construct the BSC-Net with alleviated quantization errors and enhanced performance.


%in \ref{a2}, which is the basic operator in our BSC-Net. Finally we show how to search an optimal configuration for SFSC to construct the BSC-Net in \ref{a3}.

\subsection{Preliminaries}\label{a1}
Let $\bf x_u$ be an input feature vector of an active site, located at $3$-dimensional coordinates ${\bf u} \in \mathbb{R}^{D}$. 
As shown in Figure \ref{fig:SSSC}(a), the general sparse convolution~\cite{graham20183d,choy20194d} $F_0$ by a kernel for $\bf x_u$ is formulated as: 
\vspace{-2mm}
\begin{equation}
    %F_0(\boldsymbol W,{\bf x_u})=\sum_{({\bf i}, {\bf offset})\in N^D({\bf u})}{\boldsymbol W_{\bf i}{\bf x_{u+offset}}}
    F_0(\boldsymbol W,{\bf x_u})=\sum_{{\bf i}\in N^D({\bf u})}{\boldsymbol W_{\bf i}{\bf x_{u+i}}}
    \vspace{-2mm}
\end{equation}
where $N^D({\bf u})$ denotes the list of offsets in the 3-dimensional cube centered at origin $\bf u$. 
The convolution kernel can be break down and assigned to each offset parameterized by $\bf W_i$. 

Sparse convolution is a practical substitution for vanilla 3D convolution, and skips the non-active regions that only operates when the center of convolutional kernel covers active voxels. Specifically, active voxels are stored as sparse tensors for the fixed convolution operations, where all active synapes between input and output voxels are found to perform convolution. Therefore, the memory requirement and computational cost are significantly reduced in sparse convolutional networks. 
To further reduce the complexity during inference, network binarization can be leveraged for weight and activation quantization. In a 1-bit sparse convolutional layer, both convolutional kernels and activations are binarized to $-1$ and $+1$. In this way, the time-consuming floating-point matrix multiplication can be replaced by bitwise XNOR and popcount operations:
\vspace{-1mm}
\begin{equation}
    \boldsymbol{A^l_b}=sign({\rm popcount}({\rm XNOR}(\boldsymbol{W^l_b}, \boldsymbol{A^{l-1}_b})))
    \vspace{-1mm}
\end{equation} where $\boldsymbol{A^l_b}$ and $\boldsymbol{W^l_b}$ represent the binarized activations and weights in the $l_{th}$ layer respectively, and $\boldsymbol{W^l_b}$ is defined as the binarzed version of the real-valued latent weights $\boldsymbol{W^l_r}$ via $\boldsymbol{W^l_b}=sign(\boldsymbol{W^l_r})$. 

% Since network binarization degrades the performance sizably, techniques for accuracy improvements have been studied in recent works of model quantization. Through the empirical study shown in Table \ref{tbl:baseline}, we discover the beneficial techniques for performance enhancement and list as follows:

 %\cite{rastegari2016xnor,bulat2019xnor,qin2020bipointnet,liu2018bi,liu2020react,martinez2020training} (ablation studies are shown in Table \ref{tbl:baseline}):

% \noindent\textbf{Block sturcture: }We use the same block structure as ReActNet \cite{liu2020react}, where the operations are ordered as Binarization$\rightarrow$SparseConv$\rightarrow$BatchNorm$\rightarrow$Activation in each basic block.

% \noindent\textbf{Activation function: }PReLU \cite{he2016deep,liu2020react} considers the negative inputs with better convergence, and we substitute all ReLU activation layers with PReLU to strengthen the performance.

% \noindent\textbf{Scaling factor: }We only calculate the layer-wise scaling factor for weights as demonstrated in \cite{martinez2020training}, which is the mean absolute value offull-precision weights.

% \noindent\textbf{Gradient approximation: }A piecewise polynomial function \cite{liu2018bi} is used to approximate the sign function, which acquires more accurate gradient during back propagation.

% \noindent\textbf{Downsampling/upsampling: }Following \cite{liu2018bi}, the skip connection for downsampling layer is composed of an average pooling land a real-valued convolutional layer with kernel size $1$. We also verify that an unpooling layer with a full-precision convolution with kernel size $1$ is beneficial in the skip connection for upsampling layer.

% \noindent\textbf{Initialization: }We first pretrain the network with full-precision weights and activations for initialization. Then the model with binary weights and activations is trained for binarization.

% (6) utilizing a 3-stage training strategy: we first train a real-valued network for initialization. Then the model with with real-valued weights and binary activations and that with binary weights and activations are sequentially trained for binarization.

% \begin{table*}[t]
% \centering
% \caption{The mIoU of binarzed sparse convolutional networks on ScanNet of different baseline techniques, where the UNET architectures are leveraged. The methods from left to right indicate (1) removing all the skip connections; (2) replacing PReLU with Hardtanh; (3) calculating scaling factor for both activations and weights; (4) using STE to approximate the gradient; (5) removing the skip connections for downsampling/upsampling layers.}\label{tbl:baseline}
% \begin{tabular}{l|c|c|c|c|c|c|c}
% \noalign{\smallskip}
% \hline
% \hline
% Method & Simplify BS & Simplify AF & Modify SF & Simplify GA & Simplify DS/US &Full baseline  \\
% \hline
% mIoU (\%) &35.4 &48.5 &44.1 &47.9 & 45.3/44.7 &\textbf{49.7}  \\
% %Corresponding real-valued network & 74.3\\
% \hline
% \hline
% \noalign{\smallskip}
% \noalign{\smallskip}
% \end{tabular}
% \vspace{-0.5cm}
% \end{table*}


\subsection{Shifted Sparse Convolution}\label{a2}

\begin{figure}[t]
    \centering
    \vspace{-2mm}
    \includegraphics[width=1.0\linewidth]{figures/quant_error2.pdf}
    \vspace{-6mm}
    \caption{Sign correspondence of activations for the first binary layer when binarizing convolutional network, sparse convolutional network and shifted sparse convolutional network for point cloud segmentation on ScanNet dataset. All networks share the same kernel weights. We sort x-axis (different patterns of sparse convolution) by their sign correspondence for better visualization.}
    \vspace{-2mm}
    \label{fig:toy}
\end{figure}

Since the fixed operation in sparse convolution is only activated when the central input in the receptive field is active, the constrained exploration of the neighbor active sites makes sparse convolutional networks less robust to binarization.
% In detial: the second layer, calculate sign correspondence on the active sites
To show this, we calculate the \emph{sign correspondence} (the proportion of activations in binary network that own same signs with the corresponding real-valued activations, which can measure the quantization error as proved in \cite{rastegari2016xnor}) for convolutional network and sparse convolutional network with inputs from the ScanNet dataset. We choose the activations of the first binary layer to avoid the accumulation of quantization errors and adopt the same kernel weights for both networks. As shown in Figure \ref{fig:toy}, the sign correspondences for convolutional layer and sparse convolutional layer are 63.1\% and 58.4\% respectively, which confirms that sparse convolution will bring larger quantization errors than standard convolution.

However, it is infeasible to adopt convolutional layers in point cloud analysis networks for reducing quantization errors due to the large computational cost from growing active sites. As an alternative, we try to explore the subset of convolution. For a single active site, a $3\times3\times3$ convolution kernel will operate 27 times while sparse convolution kernel only operates at the center. What if we keep the same number of operations with sparse convolution but operates at other location? To answer it, we extend sparse convolution to enable it to active at different locations.
Here we propose the shifted sparse convolution(SFSC) shown in Figure \ref{fig:SSSC}(b), which is defined as: 
\vspace{-2mm}
\begin{equation}
    F_k(\boldsymbol W,{\bf x_u})=\sum_{{\bf i}\in N^D({\bf u}+s_k)}{\boldsymbol W_{\bf i}{\bf x_{u+i}}}
\end{equation}
\vspace{-4mm}
\begin{equation*}
    s_k\in\mathbb R^3,\ k\in\{1,2,...,n_s\}
    \vspace{-1mm}
\end{equation*}
where ${\bf u}+s_k$ is the center of shifted cube instead of ${\bf u}$. 
$N^D({\bf u}+s_k)$ is then comprised of the offsets in the shifted cube w.r.t. $\bf u$. 
$n_s$ is the number of all unique shifts. 
For example, for a $3\times3\times3$ sparse convolution operation, there are up to $3^3-1=26$ possible shifts. 

For a general sparse convolution operation, it conducts convolution only when the kernel center overlaps with active sites. 
%, where the center of kernel is defined as "calculation point". 
While in our SFSC operation, the kernel center can shift to any other locations of the kernel. 
%For a $3\times3\times3$ sparse convolution operation, there are up to $3^3-1=26$ shift directions. We set the number of available shift directions to $n_s$, 
We use $\boldsymbol F_{n_s}=\{F_0, F_1, F_2, ..., F_{n_s}\}$ to represent the set of all SFSC operations. Note that we consider the general sparse convolution as a special case of SFSC ($F_0$).  %which is included in $\boldsymbol F_{n_s}$.
In a SFSC layer, instead of applying the same sparse convolution operation for all output channels as in a general sparse convolutional layer, we uniformly divide the output channels into several groups (namely channel group), each with a specific SFSC operation. It can be formulated as:
\vspace{-1mm}
\begin{equation}\label{e4}
    y={\rm concat}(f_1(\boldsymbol W_1,x), ..., f_{n_g}(\boldsymbol W_{n_g},x)),\ f_i\in \boldsymbol F_{n_s}
    \vspace{-1mm}
\end{equation}
where $x$ and $y$ are the input and output of this layer. $n_g$ indicates the number of channel groups. $W_i$ refers to the weights for the i-th SFSC operation. The outputs of all SFSC operations are concatenated along the channel dimension, resulting in a tensor with the same shape as the output of a general sparse convolutional layer.

We randomly sample 50 shift configurations for SFSC layers and compute the sign correspondence, which is shown in Figure \ref{fig:toy}. It can be seen that different SFSC layers vary a lot in quantization errors and a proportion of them are more robust to binarization compared to sparse convolutional layer. In another word, if we can find out the (near) optimal configurations for all SFSC layers in a network, the quantization error can be reduced without additional computational cost.

\subsection{Efficient Search for Shift Operation}\label{a3}
Due to the huge design space of shift operation, it is infeasible to decide an optimal configuration for the whole network: the shifted channels and shift directions may be different in each layer, and the total number of possible architectures will be $(8^4)^{13}=9.1\times10^{46}$ for a network with 13 SFSC layers, each layer with 4 channel groups and 8 available shift directions. Although manually designed BSC-Net, which shares the same shift strategy in all SFSC layers, is able to reduce the impact of binarization on the network performance, we resort to automatic architecture search for a better performance. In this section, without further explanation, the default kernel size for original sparse convolution and SFSC is $3\times3\times3$.

\begin{figure}[t]
    \centering
    \vspace{-2mm}
    \includegraphics[width=1.0\linewidth]{figures/fig2_cut.pdf}
    \vspace{-6mm}
    \caption{Demonstration of our efficient search method for shift operation. For each SFSC layer and each channel group, we combine all the shift operations in the search space into a $5\times5\times5$ sparse convolution and assign each direction with a soft selector indicating the importance of the corresponding shift operation, which enables us to directly search the best shift operations via end-to-end gradient descent. $\oplus$ stand for summation.}
    \vspace{-2mm}
    \label{fig:SSSC_NAS}
\end{figure}

%\paragraph{\textbf{\emph{Problem formulation:}}}
In our BSC-Net, the optimal shift direction for each channel group and each layer may differ. Thus the problem is to search the optimal shift direction for each channel group in the SFSC layer. We formulate this by searching the optimal $f_i$ in (\ref{e4}):
\vspace{-2mm}
\begin{equation}\label{eq3}
    f_i=\sum_{j=1}^{n_s}o_{ij}^aF_j,\ i\in\{1,2,...,n_g\}
\end{equation}
\vspace{-4mm}
\begin{equation*}
    {\rm s.t.}\ \sum_j{o_{ij}^a}=1,\ o^a\in\{0,1\}.
    \vspace{-2mm}
\end{equation*}
where $o^a$ is a binary selector of the shift direction.
As searching in a discrete space makes it hard to optimize the choices, we reformulate the discrete search space as a continuous one by switching $f_i$ to a composite function $f_i^*$:
\vspace{-2mm}
\begin{equation}\label{eq4}
    f_i^*=\sum_{j=1}^{n_s}\pi_{ij}^aF_j,\ i\in\{1,2,...,n_g\}
\end{equation}
\vspace{-4mm}
\begin{equation*}
    {\rm s.t.}\ \pi^a\in[0,1],\ \pi^a_{ij}=\frac{1}{1+{\rm exp}(-\alpha_{ij})}
    \vspace{-1mm}
\end{equation*}
where the constraints on weight $\pi^a$ are eliminated by introducing a set of real architecture parameters $\{\alpha_{ij}\}$. This sigmoid relaxation~\cite{chu2020fair} will not introduce competition among different SFSC operations as in softmax relaxation~\cite{cai2020rethinking}, which we find to be a better way to search for BSC-Net.
In this way, the composition of SFSC operations are learned by gradient descent in the space of continuous real parameters $\{\alpha_{ij}\}$, which can be optimized end-to-end efficiently.

%\textbf{Shift combination for efficient and robust search:}
However, according to (\ref{eq4}), the computation and memory increase linearly with the size of search space. All available SFSC operations need to be conducted in weighted summation $f_i^*=\sum_{j=1}^{n_s}\pi_{ij}^aF_j$. Moreover, each SFSC layer owns different parameters, increasing the difficulty of network optimization. To this end, we propose an efficient search method, which absorb all the operations in search space into a larger sparse convolution, as shown in Figure \ref{fig:SSSC_NAS}.

In this way, we convert the SFSC layer into a $5\times5\times5$ composite sparse convolutional layer, which is used to construct a supernet. This enables us to efficiently search the optimal architecture parameters by end-to-end optimization, regardless of the search space. However, it should be clarified that although the size of search space will not affect the computational efficiency of the supernet, a large search space will make the optimization of architecture parameters hard to converge, thus deteriorate the final performance.

%\textbf{Confidence constraint for better discretization:}
Once the supernet is converged, the optimal BSC-Net must be derived by discretizing the soft selector variables $\pi^a$ of (\ref{eq4}) into the binary selectors $o^a$ required by (\ref{eq3}). 
% It is obvious that the closer $\pi^a$ is to a one-hot tensor, the more similar BSC-Net is to the supernet. 
In order to make sure the performance of supernet can precisely reflect the capability of BSC-Net, we constrain $\pi^a$ in each SFSC layer by a confidence loss:
\vspace{-2mm}
\begin{equation}\label{eq_c}
    L_c=-\frac{1}{n_g \cdot n_s}\sum_i^{n_g}{\sum_j^{n_s}{|\pi_{ij}-0.5|}}
    \vspace{-2mm}
\end{equation}
which pushes $\pi^a$ to discrete values.

\textbf{Optimization approach:}
In order to decouple the weights and architecture parameters for robust learning \cite{cai2020rethinking}, we adopt an alternating optimization approach: 1) fix the $\{\alpha_{ij}\}$ and optimize $\{\boldsymbol W_i\}$; 2) fix $\{\boldsymbol W_i\}$ and update $\{\alpha_{ij}\}$. 

When we derive the BSC-Net from a converged supernet, both weights and architecture parameters need to be considered. Here we find the following strategy works best: we first train the supernet with binary weight and activation to search for the optimal architecture parameters, from which we choose the shift directions with the highest architecture parameters. Then we initialize the searched BSC-Net with the weights from the supernet and follow the same training procedure as our baseline (introduced in Section \ref{exp}).
%A) We follow the 3-stage training method of the proposed baseline to train a binary supernet. Then we choose the shift directions with the highest architecture parameters and finetune the weights. B) We adopt a similar strategy as A, but skip the third stage and simultaneously discretize the soft selectors and binarize the activations with weight finetuning. 

