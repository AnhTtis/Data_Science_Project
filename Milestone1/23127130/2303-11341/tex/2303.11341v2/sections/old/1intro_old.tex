\section{Introduction}\label{s.intro}



We do not know how to enforce rules on the development of powerful deep-learning-based AI systems other than by trusting machine learning developers at their word.
Sometimes, this trust cannot be assumed, such as when a company could generate massive profit by secretly breaking the law, or when a rival state could gain an advantage by secretly violating an arms control agreement.

% As large machine learning models increasingly provide real-world capabilities, they also pose increasingly serious risks to public safety and national security.
% This includes both bad actors using systems maliciously, and also irresponsible actors deploying systems for profit without adding proper safeguards.
Consider a scenario which may soon be both possible and profitable: an unscrupulous actor trains a highly-capable code-generation model, provides it code-execution privileges and access to the internet, and rewards it via reinforcement learning for maximizing the number of successful ransomware extortions.
Such an action would be blatantly illegal in most countries, and yet it could also be made almost untraceable.
Worse, if the developer was particularly irresponsible, they might finetune the code-generation model to copy its weights to other sufficiently-equipped servers across the internet,
in the process birthing a virulent computer worm that could paralyze large swaths of the digital economy.
(The 2017 NotPetya worm was estimated to have caused \$10B in damages, and was both far less capable and far more straightforward to disable \cite{greenberg_2018}.)
Such capable code-generating RL systems do not exist at the time of writing, but they would represent a plausible continuation of established trends in language and code modeling \cite{checkpointresearch_2023}.
This is just one of many ways that bad actors' use of advanced ML models may soon threaten public safety and national security \cite{brundage2018malicious}.

Intentional ``AI terrorism'' is only part of the challenge.
The history of software development is littered with undiscovered bugs that cause major problems, and unscrupulous ML developers may choose to deploy highly-capable ML systems without sufficiently validating their systems' consequences to public safety. (NotPetya itself was likely intended only to target Ukraine, and yet spiralled out to cause damage worldwide. \cite{greenberg_2018})
Worse, in contexts of competition between rival companies or states, advanced ML developers face substantial pressure \emph{against} sufficient testing and validation, so as not to ``fall behind''.
Even when rival parties would all prefer to individually do more testing and risk-mitigation, or even to forgo developing particularly dangerous types of models (like those used to generate improved chemical weapons \cite{urbina2022dual}), they may have no way to verify whether their competitors are complying.
This would create pressure for both parties to ``defect'' and deploy increasingly capable and dangerous systems in the hope of outmaneuvering the other economically or militarily.

Such a nightmarish equilibrium is unacceptable.
Governments \emph{must} establish verifiable mechanisms for enforcing rules on highly capable ML models' development.
Yet there is no physically-observable difference between training models for social benefit, and training models for mass destruction --- they require the same hardware, and only differ in the code and data they use.
Given the substantial promise of deep learning technologies to benefit society, it would be a tragedy if governments, in their reasonable attempt to curtail harmful use-cases, ended up repressing the creation of beneficial applications of ML as well.

Such dynamics may have already started: the US Department of Commerce's rationale for its October 2022 export controls  denying the sale of high-performance ML-enabling chips to the People's Republic of China was based in part on fear that those chips might be used to develop weapons against the United States or commit human rights abuses \cite{bis2022controls}.
This regulatory approach is inexact: limiting access to ML training hardware simultaneously limits Chinese companies' development of broadly-beneficial ML systems.
If the US government could reach an agreement with the Chinese government and Chinese firms on a set of allowable beneficial use-cases for the controlled chips, and had a way to verify Chinese companies' compliance with that agreement, it may be possible to reverse this trend.

% As a motivating example, take RL systems designed to autonomously generate and execute code, which are then connected to the internet and can execute code without close human supervision.
% If the reader's reaction is to yell ``why would anyone do that???'', then they are urged to remember that there are clear profit motives to deploy such a system, and there are many, many software developers in the world, at least a few of which are less responsible than themselves.

% The dangers from such systems are substantial, especially considering that current language models will generate unexpected outputs unless heavily tested and tweaked.
% If, by negligence or malice, the AI system model has learned to write custom software vulnerabilities, it may hack into new computer systems and modify those systems' behavior.
% If such RL systems are capable of identifying new servers on which to run, they may be able to copy themselves and thus ``self-replicate''. Like a classical computer worm, this would make their spread extremely hard to trace.
% The worst case would be if such systems could self-modify (e.g. by writing a copy of their own source code that excluded certain safety checks, and then running that copy on a second server), and thus cause harm far beyond their original designer's intention.
% Such capable code-generating RL systems do not exist at the time of writing.
% Still, given the substantial recent progress in code-generation and reinforcement learning, it is prudent to prepare for a world in which such AI models are viable.

% While it may seem unlikely that any sane person would build and deploy such a code-generating RL system without thoroughly testing it to remove such behaviors, the history of software development is littered with undiscovered bugs that cause major problems.
% Worse, in contexts of competition (between rival companies or militaries), AI developers face substantial pressure \emph{against} sufficient testing and validation, so as not to ``fall behind''.
% Even when both parties would prefer to individually do more testing and risk-mitigation, they may have no way to verify whether the other party is complying, leading both parties to defect in a ``prisoner's dilemma''.

Such a system of verification-based checks and balances, distinguishing between ``safe'' and ``dangerous'' ML model training, might seem fanciful.
Yet a similar system already exists.
At the dawn of the nuclear age, the US and USSR faced an analagous problem:
reactor-grade uranium (used for energy) and weapons-grade uranium (used to build nuclear bombs) could be produced using the same types of centrifuges, just run for longer and in a different configuration.
In response, in 1970 the nations of the world adopted the Treaty on the Non-Proliferation of Nuclear Weapons (NPT) and empowered the International Atomic Energy Agency to verify countries' commitments to limiting the spread of nuclear weapons, while still harnessing the benefits of nuclear power. 
This verification framework has helped the world avoid nuclear conflict for over 50 years, limiting nuclear weapons proliferation to just 9 countries while spreading the benefits of safe nuclear power to 33 \cite{wikipedia_2023}.

\subsection{Contributions}

In this paper, we propose a framework for monitoring large-scale ML\footnote{Throughout the text, we use ``ML'' to refer to deep-learning-based machine learning, which has been responsible for many of the breakthroughs of recent years.} training runs based on the framework of the NPT.
The goal of this work is to analyze the system's technical and logistical feasibility, and lay out important unsolved challenges that must be addressed to make it work.
Our proposal focuses on monitoring \emph{training compute}, a crucial input to ML development which may be uniquely amenable to policy oversight \cite{sastry2023computing}.
In particular, we focus on \emph{specialized datacenter ML chips}, which are---as of 2023--- necessary for the training of the largest and most-capable models.
Our proposed solution has three parts:
\begin{enumerate}
    \item Countries should leverage supply-chain monitoring to build a ``chip owner directory'', which keeps track of who owns each specialized ML chip.
    \item Chip owners should provably log information about their chips' usage. We propose a way to do this that is both computationally efficient and maintains the confidentiality of the chip-owner's trade secrets and private data.
    \item By inspecting and analyzing these chip logs, countries' inspectors can provably verify whether any rules-violating ML training runs have been executed in the past few months.
\end{enumerate}
In Section \ref{s.problemform}, we introduce the problem of verifying rules on large-scale ML training. In Section \ref{s.overview}, we provide an overview of the solution, involving chip-level activity logging and occasional physical inspections of those chips.
Sections \ref{s.onchip}, \ref{s.datacenter},  and \ref{s.supplychain} discuss the required interventions at the chip-level, datacenter-level, and supply-chain respectively.
Section \ref{s.discussion} concludes with a discussion of the proposal's political feasibility, and near-term next steps.

To serve as the basis for meaningful international coordination on advanced ML, the framework seeks to reliably detect violations of ML training rules \emph{even in the face of nation-state hackers} attempting to evade discovery.
At the same time, the system has limited scope: it does not force ML developers to disclose their confidential training data or models, and does not impose a burden on individuals' use of their personal computing devices.

\subsection{Related Work}
This paper joins an existing literature examining the role that compute may play in the governance of AI.
Early work by Hwang \cite{hwang2018computational} highlighted the potential of computing power to shape the social impact of ML.
Concurrent work by Sastry et al. \cite{sastry2023computing} identifies attributes of compute that make it a uniquely useful lever for governance, and provides an overview of policy options.
Concurrent and highly-relevant work by Baker \cite{baker2023nuclear} draws lessons from nuclear nonproliferation and arms control to discuss the logistics of international treaties on ML compute.

Rather than focusing on specific policy goals, our work proposes a technical platform for verifying many possible international agreements on ML. 
Prior work has explored the benefits of international coordination on domestic AI regulation \cite{erdelyi2018regulating}.
Many previous works have discussed the general feasibility and desirability of AI arms control \cite{reinhold2022arms}\cite{docherty_2020} \cite{mittelsteadt2021ai}. Analyzing the causes of past arms control successes, \cite{scharre2022artificial} highlight that ``verifying compliance with any arms control regime is critical to its success''.

Our solution will involve proving that a rule-violating ML training run was \emph{not} done, in part by proving which other training runs \emph{were} done.
Our analysis of the latter problem is heavily inspired by the literature on Proof-of-Learning \cite{jia2021proof, fang2022fundamental}.






% \copied{
% 1. Rules on AI have a major problem: we do not know how to enforce rules on AI without trusting AI developers at their word.
%     1. Laws prescribing what you *cannot* do with ML systems are nearly impossible to enforce, because AI training and deployment are computer programs, which can be run by anyone with the necessary computers. Given that there is no trace of which program was run on a machine, anyone violating the law can simply claim they “didn’t”, and there is no physical trace to the contrary.
%     2. This means that the law relies on voluntary self-reporting of compliance. This works for large companies in areas with strong rule of law, and for public-facing systems (like computer vision products), but it is much less effective at covering internal-facing tools, or active criminality (including by dedicated criminal organizations), or the actions of other states.
%     3. Thus, the only laws we realistically think about are those on public-facing AI products, rather than the many non-public yet more dangerous use-cases (e.g. AI cyberweapons). 
%     4. This inability to enforce has had limited impact thus far because the reach of non-public-facing AI systems has been limited. But as these systems become more capable, and their use-cases become more dangerous, it will be vital that society can prevent the most dangerous misuses of AI systems.
% 2. Because governments have no way to verify compliance with laws or agreements, they may be forced to rely on draconian measures.
%     1. For example, the USG, believing that China was using AI for military development and human rights violations, blanket-banned the sale of chips from China entirely.
%     2. This is a massively-costly economic policy that also blocks the many good use-cases of AI from Chinese firms, and costs US chipmakers a lot of sales.
%     3. However, it is understandable that the US views it as necessary, because the US has no other way of verifying compliance with agreements with the Chinese on which use-cases are acceptable.
%     4. An alternative would be requiring complete transparency of how the chips were being used, but this would be unacceptable for data privacy, intellectual property, and industrial secrecy reasons. [IS THERE A REAL-WORLD EXAMPLE?]
%         1. AI development is also borderless - even if an extreme degree of surveillance exists in one country, a criminal can just run the same operations in another country, and transmit the results over the internet.
% 3. We need tools for enforcing rules on AI development that don’t ban AI entirely and let the rest of society continue to reap the technology’s benefits, but that also don’t involve draconian surveillance.
% 4. An immediate answer: that’s impossible.
% }
% \copied{


% Given the general promise of AI systems to improve human life, and their substantial short-term economic utility, halting the continued progress of public AI research may not be tractable or desirable.
% Data, too, is abundant and gathering more is cheap, making it nearly impossible to limit.
% While talented AI experts are currently scarce, it is likely that as the economic significance of AI deployment increases, the availability of knowledgeable engineering talent will similarly spread.

% The one potentially-persistent bottleneck to building advanced AI systems is access to massive quantities of compute.
% As Brundage highlights, compute's ``rivalrousness, excludability, and quantifiability make it a promising intervention point for AI governance''.
% Its production is highly concentrated: as of 2022, all cutting edge MPUs (micro-processing units, like CPUs and GPUs, using process nodes of 20nm or better) were manufacturable in only 29 fabrication plants in the US, Taiwan, South Korea, China, Israel, and Ireland \cite{wiki:List_of_semiconductor_fabrication_plants}.
% Building a new modern fab costs many billions of dollars, and requires access to a massive international supply chain.
% (We will explore whether this will remain the case in more detail in Section \ref{sec:identifying}.)

% In this sense, high-performance compute bears remarkable similarity to enriched uranium (the crucial input into both nuclear energy and nuclear weapons).

% The 1970 Treaty on the Non-Proliferation of Nuclear Weapons, implemented by the International Atomic Energy Agency (IAEA), has succeeded for the last 50 years at preventing the proliferation of nuclear weapons and maintaining verification-based trust between otherwise-opposed states.
% The IAEA framework is based on two principles: monitoring the transportation of nuclear material, and inspecting physical ``chokepoints'', especially uranium enrichment facilities and nuclear power plants.

% In this work, we propose a system that adapts the IAEA framework to the challenge of overseeing datacenter compute, by outlining a scalable procedure for datacenter compute inspection and verification.

% }
% \copied{

% 5. Is this possible? Draw parallels to nuclear fuel enrichment, and highlight how it was very successfully solved
%     1. The oversight of uranium enrichment enshrined in the NPT has been remarkably successful at preventing the proliferation of nuclear weapons for over 50 years, while enabling the spread of nuclear energy to [many countries].
%         1. By creating a light-touch verification framework, countries have successfully kept a balance for decades. While only a few countries have gained nukes since the NPT, ___ countries run enrichment facilities, and 32 countries have nuclear power purchased from these folks. (Even more places have nuclear research reactors.)
%             1. Enrichers: Argentina, Brazil, China, France, Germany, India, Iran, Japan, the Netherlands, North Korea, Pakistan, Russia, the United Kingdom, and the United States
%     2. Uranium enrichment bears remarkable similarity to the training of large-scale ML models.
%     3. Like uranium enrichment, which requires the use of special centrifuges to progressively enrich U-235 into ever-higher purities, large-scale ML training requires the use of large quantities of specialized chips run for weeks or months.
%     4. Like uranium enrichment, the longer AI models are trained for, the more the results are dangerous and requiring of regulation.
%     5. Is it possible to port the international system for monitoring uranium enrichment to the domain of AI?
%     6. “More details in [nuclear parallels section]”
% 6. This paper will propose a method for enforcing regulations on the large the nuclear verification framework to tackle the challenges of AI training monitoring.
%     1. Need to highlight more explicitly why AI is bottlenecked on compute, and why compute is a governable resource.
%     2. The focus is on large *training* runs, since these require large-scale resources, and then assumes that their subsequent use can be regulated once such activity is detected. This doesn’t cover inference (how models are subsequently used), but monitoring training does allow us to curtail the most dangerous types of misuse. For a full description, see [NEXT SECTION].
%     3. We will propose a system by which organizations can prove to governments/peers the length of their longest training runs, along with final model weights and the dataset used for training.
%     4. In practice, this will describe
%     5. Algorithmically, we reduce our problem to solving a known problem in the literature: PoL
%     7. Importantly, this should not infringe on any individuals’ rights to their own personal compute, and should not require AI-developers to divulge trade secrets (e.g. model weights, training data). In particular, it should be light-touch enough to not create a strong incentive for actors to evade it when following the rules.}

% \copied{The core innovation of this framework is that by randomly sampling even a small number of MPUs from each datacenter, we can with high probability detect and characterize any datacenter-scale computing jobs. 
% This is because datacenter compute is modular and homogeneous, and because a sizeable fraction of the chips in a datacenter would need to be used in training advanced AI systems.
% Checking every one of the millions of chips in a datacenter would be hopelessly complex and extremely invasive.
% But small-scale random sampling is comparatively easy. 

% To uniformly sample from all global compute that could be being used to develop advanced AI systems, we need to:
% \begin{itemize}
%     \item Know of every large cluster (either physical or digital) of compute,
%     \item Have an accurate catalog of the IDs of every chip within each cluster, and
%     \item Be able to efficiently inspect a random sample of the catalog's corresponding chips.
%     \item Be able to determine whether a randomly sampled chip was part of an illegal training run or not, by having that chip provably keep logs, s.t. those logs could only attest to legal behavior if no illegal activity occurred.
% \end{itemize}


% }


% \copied{

%     8. The focus of this proposal is only to use existing technologies, and not to rely on future breakthroughs. Furthermore, to have hope of this actually happening, we are focused on interventions where the relevant stakeholders must have a self-interest in complying and enforcing required compliance. Beyond the many uncertainties we highlight, the key required ingredient is organizational will.
% 7. Purpose of this paper
    % 1. We are describing an end-goal governance framework, where cheating is basically impossible. As with all legal enforcement, it in practice makes sense to start with partial solutions, which will catch many less-sophisticated violators. We describe these intermediate measures in section [INSERT].
    % 2. This is a necessarily wide-ranging and ambitious proposal, and will necessarily make assumptions, including about future technological trajectories. Some of these are likely to be wrong, in which case parts of the framework may need to be reworked. We explicitly highlight these to encourage productive discussion. It is our belief that if all these assumptions hold, this framework can tractably be implemented.
%     3. We will start with two assumptions:
%         1. The compute required to train AI models is sufficiently large to be detectable, and this will remain true even after substantial algorithmic innovation.
%             1. Cite the bitter lesson, scaling laws papers, etc.
%             2. On the flipside, cite potential challenges to scaling laws, including data-efficiency and algorithmic innovation (cite Tamay’s paper here)
%         2. AI training will require specific types of chips, both with high interconnect and high flop-count.
%             1. I.e. can’t daisy-chain xboxes
%             2. This is important because this restricts this framework to large-scale industrial supply-chains/operations, and doesn’t affect ordinary consumers.
%             3. These are the chips already targeted by US regulations.
%         3. One thing we *aren’t* assuming here: that these chips must be collocated. For example, multiple datacenters working together, as in GPT-JT. This framework should still work in this case.

% }