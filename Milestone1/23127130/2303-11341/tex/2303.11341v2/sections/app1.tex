% \section{Analogies and disanalogies to nuclear}
% \copied{1. main thing to highlight earlier when linking is that it’s debated by scholars why nuclear verification succeeded, and the similarities vs. differences with AI training will determine whether AI will succeed for a similar reason. We give arguments why it may be easier, and why it may be harder.
% 2. Analogies
%     1. Both involve using a flow (centrifuges/chips) to  aggregate a stock (total training time)
%     2. Small amounts are fine, large amounts are bad
%     3. Positive economic use-cases that everyone should benefit from, negative misuse use-cases that we should limit to the extent possible.
% 3. Disanalogies
%     1. With uranium, after the enrichment occurs, you can still track the physically-produced uranium. (I.e. violations are reversible.) With compute, once a model has been trained, it can be copied at will.
%     2. With nuclear, most countries that have pursued their own nuclear program have eventually been able to discretely build their own centrifuges (e.g. with design-support from AQ Khan). With advanced compute, this seems very unlikely - fabs cost many billions, and are rarely spun up for a single purpose. (Even the US military failed at this.) Thus, compute supply chain is much more concentrated - less possible to “build an AI project in a bunker somewhere” detached from existing supply chains.
%     3. With HEU, inspecting the end-product is sufficient to know how enrichment was done. With compute, it’s hard to determine “how much an NN was trained” without being provided additional information on the process by which we arrived at those weights.
%     4. With HEU, we know ahead of time how much enrichment is sufficient to be dangerous (as there are physical requirements to causing a supercritical fission  reaction). However with AI, algorithmic progress means that the threshold for any particular dangerous use-case decreases over time. That said, “more compute” will always be riskier than “less compute”, and thus a useful heuristic for regulation. Even if relevant thresholds need to change over time, so long as the requirements do not shrink to the point where detection would be impossible, it is still just as important to have a governing framework for compute.
%     5. With HEU, enrichment must happen (mostly) at the same location. With AI training, enrichment can be parallelized over the internet, although there is some critical concentration required per location.
%     6. AI is much more civilian-economically valuable than uranium, and implemented with more parts of the supply chain. Many analysts believe that part of the success of the NPT is that states do not have a strong incentive to cheat as they’re part of security alliances; it is not clear whether this will be the same for AI. It may be that similar alliances are required.
% 4. Inspection mechanisms from nuclear we’d like to copy
%     1. Tracking centrifuge production
%     2. Centrifuge flow monitoring
%     3. Accounting for total usage of centrifuges (i.e. being able to show results)
% }

% \section{Case study in a way this could break: student-teacher}
% \ot{One way to break this scheme: rather than one single long training run, to evade detection, the model can be repeatedly self-distilled into a new model, and then that new model trained for an additional period. This increases a compute-overhead, but so long as the distilling time does not scale linearly with the training time, this can make it possible to hide longer runs as a series of shorter runs. However, the total compute required (and the total time that the in-RAM model is of sufficiently-low-loss to trigger an audit) is still large, meaning that a randomly-sampled chip would still need to attest to being part of a dangerous training run.
% Note also that this behavior would look kind of weird, because each self-distillation run would be using a very large number of chips *in parallel* for a short time. This is because such self-distillation-chains must occur sequentially across time, and at each timestep all the resources go into a particular snapshot.}

\newpage

\section{Discussion on future training requirements}\label{app.howmuch}

\subsection{Will the most capable ML models require large-scale training?}
This paper's proposed framework is premised on the assumption that large-scale training is and continues to be a necessary requirement for the most advanced (and thus most dangerous) ML models.
There is intense disagreement within the field about how important large-scale training is, and how long that will remain the case.

Many of the recent breakthroughs in machine learning model capabilities, across every domain, have come from increasing the model size or quantity of training data, each of which corresponds to a greater usage of compute \cite{kaplan2020scaling, hoffman2022training, zhai2021scaling}.
Indeed, some capabilities, such as chain-of-thought reasoning, appear to only emerge at the largest training scales \cite{wei2022chain}.
At the same time, any one narrow capability can often be achieved with a much smaller compute budget \cite{magister2022teaching, madani2023large}.
Nonetheless, Sutton's ``Bitter Lesson'' \cite{sutton2019bitter} that ``general methods that leverage computation are ultimately the most effective'' is a frequent diagnosis of the likely future of deep learning.
Though algorithmic progress \cite{erdil2022algorithmic} and the continued progress of Moore's Law will continue to reduce the number of chips required for any specific capability, we may compensate by gradually increasing enforcement parameters to work for smaller quantities of specialized compute.
At the same time, the increasing investment in compute by frontier AI firms \cite{lardinois_2022, wiggers_2022} suggests that industry insiders continue to believe that the most capable frontier models --- likeliest to yield new capabilities and surface new risks to public safety --- are expected to require ever more compute.

\subsection{Will large-scale training continue to require specialized datacenter chips?}

Nearly all large-scale training runs are executed on high-end datacenter accelerators \cite{chowdhery2022palm, kaplan2020scaling, zeng2022glm}.
The main difference between these chips and their consumer-oriented counterparts is their much higher inter-chip communication bandwidth (e.g., 900GB/s for the NVIDIA H100 SXM vs. 64GB/s for the NVIDIA GeForce RTX 4090 \cite{nvidiah100, nvidia4090}).
This extra bandwidth is today crucial for parallelizing NN training, especially tensor parallelism and data parallelism, which require frequent transfers of large matrices between many chips \cite{smith2022computation}.
Organizations doing large-scale training also favor these datacenter chips for other reasons: they are generally more energy efficient, and license requirements often prevent organizations from placing consumer-oriented chips in datacenters\cite{moss_2023}.

Still, recent work has suggested it may be \emph{possible} to do large-scale training on consumer chips with low interconnect, though with substantial cost and speed penalties\cite{binhang2022distributed, ryabinin2023distributed}.
If such methods become feasible for bad actors, then we may need to adjust to a different regulatory model for detecting training activity.
Possibilities include focusing on spotting and monitoring datacenters (similar to the IAEA's work to detect undeclared nuclear facilities \cite{harry1996iaea}), or regulating the high-capacity switches that could be necessary to enable fast networking between low-interconnect chips.
So long as they can be detected, it may be possible to retrofit consumer chips (e.g. with a permanently-mated host CPU, see Section \ref{s.onchip}) to enable similar monitoring capabilities.

It is important to note that the current framework \emph{does} apply in the setting where clusters of chips are split across several datacenters (e.g. multiple cloud providers), so long as these high-end chips are used at each datacenter.
