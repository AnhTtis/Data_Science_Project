\section{Introduction}\label{s.intro}
Many of the remarkable advances of the past 5 years in deep learning have been driven by a continuous increase in the quantity of \emph{training compute} used to develop cutting-edge models \cite{kaplan2020scaling, hoffman2022training, sevilla2022compute}.
Such large-scale training has been made possible through the concurrent use of hundreds or thousands of specialized accelerators with high inter-chip communication bandwidth (such as Google TPUs, NVIDIA A100 and H100 GPUs, or AMD MI250 GPUs), employed for a span of weeks or months to compute thousands or millions of gradient updates.
We refer to these specialized accelerators as \emph{ML chips}, which we distinguish from consumer-oriented GPUs with lower interconnect bandwidth (e.g., the NVIDIA RTX 4090, used in gaming computers).

This compute scaling trend has yielded models with ever more useful capabilities.
However, these advanced capabilities also bring with them greater dangers from misuse \cite{brundage2018malicious}.
For instance, it is increasingly plausible that criminals may soon be able to leverage heavily-trained code-generation-and-execution models to autonomously identify and exploit cyber-vulnerabilities, enabling ransomware attacks on an unprecedented scale.
\footnote{The 2017 NotPetya worm was estimated to have caused \$10B in damages, and was both far less capable and far more straightforward to disable \cite{greenberg_2018}.}
Even absent malicious intent, rival companies or countries trapped in an AI ``race dynamic'' may face substantial pressure to cut corners on testing and risk-mitigation, in order to deploy high-capability ML systems in the hopes of outmaneuvering their competitors economically or militarily.
The edge-case behaviors of deep learning models are notoriously difficult to debug \cite{hendrycks2021unsolved}, and without thorough testing and mitigation, such bugs in increasingly capable systems may have increasingly severe consequences.
Even when rival parties would all prefer to individually do more testing and risk-mitigation, or even forgo developing particularly dangerous types of ML models entirely \cite{urbina2022dual}, they may have no way to verify whether their competitors are matching their levels of caution.

In the event that such risks do emerge, governments may wish to enforce limits on the large-scale development of ML models.
While law-abiding companies will comply, criminal actors, negligent companies, and rival governments may not, especially if they believe their rule-violations will go unnoticed.
It would therefore be useful for governments to have methods for reliably \emph{verifying} that large-scale ML training runs comply with agreed rules.

These training runs' current need for large quantities of specialized chips leaves a large physical and logistical footprint, meaning that such activities are generally undertaken by sizable organizations (e.g., corporate or governmental data-center operators) well-equipped to comply with potential regulations.
Yet even if the relevant facilities are known, there is no easily-observable difference between training a model for social benefit, and training a model for criminal misuse --- they require the same hardware, and at most differ in the code and data they use.
Given the substantial promise of deep learning technologies to benefit society, it would be unfortunate if governments, in a reasonable attempt to curtail harmful use-cases but unable to distinguish the development of harmful ML models, ended up repressing the development of beneficial applications of ML as well.
Such dynamics are already appearing: the US Department of Commerce's rationale for its October 2022 export controls denying the sale of high-performance chips to the People's Republic of China, while not specific to ML, was based in part on concern that those chips might be used to develop weapons against the United States or commit human rights abuses \cite{bis2022controls}.
If the US and Chinese governments could reach an agreement on a set of permissible beneficial use-cases for export-controlled chips, and had a way to verify Chinese companies' compliance with that agreement, it may be possible to prevent or reverse future restrictions.

Such a system of verification-based checks and balances, distinguishing between ``safe'' and ``dangerous'' ML model training, might seem infeasible.
Yet a similar system has been created before.
At the dawn of the nuclear age, nations faced an analogous problem:
reactor-grade uranium (used for energy) and weapons-grade uranium (used to build nuclear bombs) could be produced using the same types of centrifuges, just run for longer and in a different configuration.
In response, in 1970 the nations of the world adopted the Treaty on the Non-Proliferation of Nuclear Weapons (NPT) and empowered the International Atomic Energy Agency (IAEA) to verify countries' commitments to limiting the spread of nuclear weapons, while still harnessing the benefits of nuclear power. 
This verification framework has helped the world avoid nuclear conflict for over 50 years, and helped limit nuclear weapons proliferation to just 9 countries while spreading the benefits of safe nuclear power to 33 \cite{wikipedia_2023}.
If future progress in machine learning creates the domestic or international political will for enacting rules on large-scale ML development, it is important that the ML community is ready with technical means for verifying such rules.

\subsection{Contributions}

In this paper, we propose a monitoring framework for enforcing rules on the \emph{training} of ML\footnote{Throughout the text, we use ``ML'' to refer to deep-learning-based machine learning, which has been responsible for much of the progress of recent years.} models using large quantities of specialized ML chips.
Its goal is to enable governments to verify that companies and other governments have complied with agreed guardrails on the development of ML models that would otherwise pose a danger to society or to international stability.
The objective of this work is to lay out a possible system design, analyze its technical and logistical feasibility, and highlight important unsolved challenges that must be addressed to make it work.

The proposed solution has three parts:
\begin{enumerate}
    \item To prove compliance, an ML chip owner employs firmware that logs limited information about that chip's activity, with their employment of that firmware attested via hardware features.
    We propose an activity logging strategy that is both lightweight, and maintains the confidentiality of the chip-owner's trade secrets and private data, based on the NN weights present in the device's high-bandwidth memory.
    \item By inspecting and analyzing the logs of a sufficient subset of the chips, inspectors can provably determine whether the chip-owner executed a rules-violating training run in the past few months, with high probability.
    \item Compute-producing countries leverage supply-chain monitoring to ensure that each chip is accounted for, so that actors can't secretly acquire more ML chips and then underclaim their total to hide from inspectors.
\end{enumerate}
The system is compatible with many different rules on training runs (see Section \ref{s.rules}), including those based on the total chip-hours used to train a model, the type of data and algorithms used, and whether the produced model exceeds a performance threshold on selected benchmarks.
To serve as a foundation for meaningful international coordination, the framework aspires to reliably detect violations of ML training rules \emph{even in the face of nation-state hackers attempting to circumvent it}.
At the same time, the system does not force ML developers to disclose their confidential training data or models.
Also, as its focus is restricted to specialized data-center chips, the system does not affect individuals' use of their personal computing devices.

Section \ref{s.problemform} introduces the problem of verifying rules on large-scale ML training. Section \ref{s.overview} provides an overview of the solution, and describes the occasional inspections needed to validate compliance.
Sections \ref{s.onchip}, \ref{s.datacenter},  and \ref{s.supplychain} discuss the interventions at the chip-level, data-center-level, and supply-chain respectively.
Section \ref{s.discussion} concludes with a discussion of the proposal's benefits for different stakeholders, and lays out near-term next steps.

\subsection{Limitations}
\label{s.limits}
The proposed system's usefulness depends on the continued importance of large-scale training to produce the most advanced (and thus most dangerous) ML models, a topic of uncertainty and ongoing disagreement within the ML community.
The framework's focus is also restricted only to training runs executed on specialized data-center accelerators, which are today effectively necessary to complete the largest-scale training runs without a large efficiency penalty.
In Appendix \ref{app.howmuch}, we discuss whether these two trends are likely to continue.
Additionally, hundreds of thousands of ML chips have already been sold, many of which do not have the hardware security features required by the framework, and may not be retrofittable nor even locatable by governments.
These older chips' importance may gradually decrease with Moore's Law.
But combined with the possibility of less-efficient training using non-specialized chips, these unmonitored compute sources present an implicit lower bound on the minimum training run size that can be verifiably detected by the proposed system.
Still, it may be the case that frontier training runs, which result in models with new emergent capabilities to which society most needs time to adapt, are more likely to require large quantities of monitorable compute.

More generally, the framework does not apply to small-scale ML training, which can often be done with small quantities of consumer GPUs.
We acknowledge that the training of smaller models (or fine-tuning of existing large models) can be used to cause substantial societal harm (e.g., computer vision models for autonomous terrorism drones \cite{pledger2021role}).
Separately, if a model is produced by a large-scale training run in violation of a future law or agreement, that model's weights may from then on be copied undetectably, and it can be deployed using consumer GPUs \cite{sheng2023high} (as ML inference requires far lower inter-chip communication bandwidth).
Preventing the proliferation of dangerous trained models is itself a major challenge, and beyond the scope of this work.
More broadly, society is likely to need laws and regulations to limit the harms from bad actors' misusing such ML models.
However, exhaustively \emph{enforcing} such rules at the hardware-level would require surveilling and policing individual citizens' use of their personal computers, which would be highly unacceptable on ethical grounds.
This work instead focuses attention upstream, regulating whether and how the most dangerous models \emph{are created in the first place}.

Lastly, rather than proposing a comprehensive shovel-ready solution, this work provides a high-level solution design.
Its contribution is in isolating a set of open problems whose solution would be sufficient to enable a system that achieves the policy goal.
If these problems prove unsolvable, the system's design will need to be modified, or its guarantees scaled back.
We hope that by providing a specific proposal to which the community can respond, we will initiate a cycle of feedback, iteration, and counter-proposals that eventually culminates in an efficient and effective method for verifying compliance with large-scale ML training rules.

\subsection{Related Work}
This paper joins an existing literature examining the role that compute may play in the governance of AI.
Early work by Hwang \cite{hwang2018computational} highlighted the potential of computing power to shape the social impact of ML.
Concurrent work by Sastry et al. \cite{sastry2023computing} identifies attributes of compute that make it a uniquely useful lever for governance, and provides an overview of policy options.
Closely-related work by Baker \cite{baker2023nuclear} draws lessons from nuclear arms control for the compute-based verification of international agreements on large-scale ML.

Rather than focusing on specific policies, the work proposes a technical platform for verifying many possible regulations and agreements on ML development.
Already, the EU AI Act has proposed establishing risk-based regulations on AI products \cite{veale2021demystifying}, while US senators have proposed an ``Algorithmic Accountability Act'' to oversee algorithms used in critical decisions \cite{chu_2022}, and the Cyberspace Administration of China (CAC) has established an ``algorithm registry'' for overseeing recommender systems \cite{cac_2022}.
Internationally, many previous works have discussed the general feasibility and desirability of AI arms control \cite{reinhold2022arms, docherty_2020, mittelsteadt2021ai}, with \cite{scharre2022artificial} highlighting the importance of verification measures to the success of potential AI arms control regimes.
Past work has also explored the benefits of international coordination on non-military AI regulation \cite{erdelyi2018regulating}.

The proposed solution involves proving that a rule-violating ML training run was \emph{not} done, in part by proving which other training runs \emph{were} done.
The analysis of the latter problem is heavily inspired by the literature on Proof-of-Learning \cite{jia2021proof, fang2022fundamental} (discussed further in Section \ref{s.datacenter}).
Other works has have used tools from cryptography to train NN models securely across multiple parties \cite{wagh2018securenn}, and to securely prove the correctness of NN inference \cite{lee2020vcnn}.
However, these approaches suffer large efficiency penalties and cannot yet be scaled to cutting-edge model training, rendering them nonviable as a method for verifying rules on large-scale training runs.


% \copied{
% 1. Rules on AI have a major problem: we do not know how to enforce rules on AI without trusting AI developers at their word.
%     1. Laws prescribing what you *cannot* do with ML systems are nearly impossible to enforce, because AI training and deployment are computer programs, which can be run by anyone with the necessary computers. Given that there is no trace of which program was run on a machine, anyone violating the law can simply claim they “didn’t”, and there is no physical trace to the contrary.
%     2. This means that the law relies on voluntary self-reporting of compliance. This works for large companies in areas with strong rule of law, and for public-facing systems (like computer vision products), but it is much less effective at covering internal-facing tools, or active criminality (including by dedicated criminal organizations), or the actions of other states.
%     3. Thus, the only laws we realistically think about are those on public-facing AI products, rather than the many non-public yet more dangerous use-cases (e.g. AI cyberweapons). 
%     4. This inability to enforce has had limited impact thus far because the reach of non-public-facing AI systems has been limited. But as these systems become more capable, and their use-cases become more dangerous, it will be vital that society can prevent the most dangerous misuses of AI systems.
% 2. Because governments have no way to verify compliance with laws or agreements, they may be forced to rely on draconian measures.
%     1. For example, the USG, believing that China was using AI for military development and human rights violations, blanket-banned the sale of chips from China entirely.
%     2. This is a massively-costly economic policy that also blocks the many good use-cases of AI from Chinese firms, and costs US chipmakers a lot of sales.
%     3. However, it is understandable that the US views it as necessary, because the US has no other way of verifying compliance with agreements with the Chinese on which use-cases are acceptable.
%     4. An alternative would be requiring complete transparency of how the chips were being used, but this would be unacceptable for data privacy, intellectual property, and industrial secrecy reasons. [IS THERE A REAL-WORLD EXAMPLE?]
%         1. AI development is also borderless - even if an extreme degree of surveillance exists in one country, a criminal can just run the same operations in another country, and transmit the results over the internet.
% 3. We need tools for enforcing rules on AI development that don’t ban AI entirely and let the rest of society continue to reap the technology’s benefits, but that also don’t involve draconian surveillance.
% 4. An immediate answer: that’s impossible.
% }
% \copied{


% Given the general promise of AI systems to improve human life, and their substantial short-term economic utility, halting the continued progress of public AI research may not be tractable or desirable.
% Data, too, is abundant and gathering more is cheap, making it nearly impossible to limit.
% While talented AI experts are currently scarce, it is likely that as the economic significance of AI deployment increases, the availability of knowledgeable engineering talent will similarly spread.

% The one potentially-persistent bottleneck to building advanced AI systems is access to massive quantities of compute.
% As Brundage highlights, compute's ``rivalrousness, excludability, and quantifiability make it a promising intervention point for AI governance''.
% Its production is highly concentrated: as of 2022, all cutting edge MPUs (micro-processing units, like CPUs and GPUs, using process nodes of 20nm or better) were manufacturable in only 29 fabrication plants in the US, Taiwan, South Korea, China, Israel, and Ireland \cite{wiki:List_of_semiconductor_fabrication_plants}.
% Building a new modern fab costs many billions of dollars, and requires access to a massive international supply chain.
% (We will explore whether this will remain the case in more detail in Section \ref{sec:identifying}.)

% In this sense, high-performance compute bears remarkable similarity to enriched uranium (the crucial input into both nuclear energy and nuclear weapons).

% The 1970 Treaty on the Non-Proliferation of Nuclear Weapons, implemented by the International Atomic Energy Agency (IAEA), has succeeded for the last 50 years at preventing the proliferation of nuclear weapons and maintaining verification-based trust between otherwise-opposed states.
% The IAEA framework is based on two principles: monitoring the transportation of nuclear material, and inspecting physical ``chokepoints'', especially uranium enrichment facilities and nuclear power plants.

% In this work, we propose a system that adapts the IAEA framework to the challenge of overseeing data-center compute, by outlining a scalable procedure for data-center compute inspection and verification.

% }
% \copied{

% 5. Is this possible? Draw parallels to nuclear fuel enrichment, and highlight how it was very successfully solved
%     1. The oversight of uranium enrichment enshrined in the NPT has been remarkably successful at preventing the proliferation of nuclear weapons for over 50 years, while enabling the spread of nuclear energy to [many countries].
%         1. By creating a light-touch verification framework, countries have successfully kept a balance for decades. While only a few countries have gained nukes since the NPT, ___ countries run enrichment facilities, and 32 countries have nuclear power purchased from these folks. (Even more places have nuclear research reactors.)
%             1. Enrichers: Argentina, Brazil, China, France, Germany, India, Iran, Japan, the Netherlands, North Korea, Pakistan, Russia, the United Kingdom, and the United States
%     2. Uranium enrichment bears remarkable similarity to the training of large-scale ML models.
%     3. Like uranium enrichment, which requires the use of special centrifuges to progressively enrich U-235 into ever-higher purities, large-scale ML training requires the use of large quantities of specialized chips run for weeks or months.
%     4. Like uranium enrichment, the longer AI models are trained for, the more the results are dangerous and requiring of regulation.
%     5. Is it possible to port the international system for monitoring uranium enrichment to the domain of AI?
%     6. “More details in [nuclear parallels section]”
% 6. This paper will propose a method for enforcing regulations on the large the nuclear verification framework to tackle the challenges of AI training monitoring.
%     1. Need to highlight more explicitly why AI is bottlenecked on compute, and why compute is a governable resource.
%     2. The focus is on large *training* runs, since these require large-scale resources, and then assumes that their subsequent use can be regulated once such activity is detected. This doesn’t cover inference (how models are subsequently used), but monitoring training does allow us to curtail the most dangerous types of misuse. For a full description, see [NEXT SECTION].
%     3. We will propose a system by which organizations can prove to governments/peers the length of their longest training runs, along with final model weights and the dataset used for training.
%     4. In practice, this will describe
%     5. Algorithmically, we reduce our problem to solving a known problem in the literature: PoL
%     7. Importantly, this should not infringe on any individuals’ rights to their own personal compute, and should not require AI-developers to divulge trade secrets (e.g. model weights, training data). In particular, it should be light-touch enough to not create a strong incentive for actors to evade it when following the rules.}

% \copied{The core innovation of this framework is that by randomly sampling even a small number of MPUs from each data-center, we can with high probability detect and characterize any data-center-scale computing jobs. 
% This is because data-center compute is modular and homogeneous, and because a sizeable fraction of the chips in a data-center would need to be used in training advanced AI systems.
% Checking every one of the millions of chips in a data-center would be hopelessly complex and extremely invasive.
% But small-scale random sampling is comparatively easy. 

% To uniformly sample from all global compute that could be being used to develop advanced AI systems, we need to:
% \begin{itemize}
%     \item Know of every large cluster (either physical or digital) of compute,
%     \item Have an accurate catalog of the IDs of every chip within each cluster, and
%     \item Be able to efficiently inspect a random sample of the catalog's corresponding chips.
%     \item Be able to determine whether a randomly sampled chip was part of an illegal training run or not, by having that chip provably keep logs, s.t. those logs could only attest to legal behavior if no illegal activity occurred.
% \end{itemize}


% }


% \copied{

%     8. The focus of this proposal is only to use existing technologies, and not to rely on future breakthroughs. Furthermore, to have hope of this actually happening, we are focused on interventions where the relevant stakeholders must have a self-interest in complying and enforcing required compliance. Beyond the many uncertainties we highlight, the key required ingredient is organizational will.
% 7. Purpose of this paper
    % 1. We are describing an end-goal governance framework, where cheating is basically impossible. As with all legal enforcement, it in practice makes sense to start with partial solutions, which will catch many less-sophisticated violators. We describe these intermediate measures in section [INSERT].
    % 2. This is a necessarily wide-ranging and ambitious proposal, and will necessarily make assumptions, including about future technological trajectories. Some of these are likely to be wrong, in which case parts of the framework may need to be reworked. We explicitly highlight these to encourage productive discussion. It is our belief that if all these assumptions hold, this framework can tractably be implemented.
%     3. We will start with two assumptions:
%         1. The compute required to train AI models is sufficiently large to be detectable, and this will remain true even after substantial algorithmic innovation.
%             1. Cite the bitter lesson, scaling laws papers, etc.
%             2. On the flipside, cite potential challenges to scaling laws, including data-efficiency and algorithmic innovation (cite Tamay’s paper here)
%         2. AI training will require specific types of chips, both with high interconnect and high flop-count.
%             1. I.e. can’t daisy-chain xboxes
%             2. This is important because this restricts this framework to large-scale industrial supply-chains/operations, and doesn’t affect ordinary consumers.
%             3. These are the chips already targeted by US regulations.
%         3. One thing we *aren’t* assuming here: that these chips must be collocated. For example, multiple data-centers working together, as in GPT-JT. This framework should still work in this case.

% }