\section{At the data-center}\label{s.datacenter}

% 1) Proof-of-Learning: high-level idea for what we do with weight snapshots, and why they tell us about the training run
%     1) Previous work on PoLs
%     2) Compliance model: like finance, give description of what you did, and during audit provide more interactive validation. Not-participating-in-audit is equivalent to having committed the original crime.
% 2) Reporting: what do PoL reports tell us?
% 3) Verification: how do we verify PoL was correct?
%     1) Theoretical: original PoL doesn't give us what we want, we need to modify it slightly.
%         1) PoTT instead of PoL
%         2) Whether PoL also extends to only shards of larger network
%     2) In practice, to verify PoL we need a shared machine (to not leak data). Describe procedures on that machine.
% 4) Operational overview:
%     1) Collect PoLs for every training run, and report them (hashed) to Verifier along with all chip logs, on some interval
%     2) Then Verifier randomly samples some chips, and then looks at those PoL transcripts corresponding to the chips, and picks the transcript steps that need to be verified
%     3) These are all brought to a trusted computer, and PoL verification occurs
%     4) Once PoL is verified, can run arbitrary programs on it to determine if it followed rules
% 5) Talk about the costs
%     1) Talk about the costs of storage
%         1) Both for PoLs, and also mention the costs for the random sampling
%     2) Talk about the cost of reverification, and what it depends on
From Sections \ref{s.sampling} and \ref{s.onchip}, we know how a Verifier can get access to at least one weight shard $\omega$ snapshotted during a large-scale training run.
In this section, we will show how the Verifier can use this weight shard to determine whether the associated training run followed the agreed-upon rules.

On its own, knowing $\omega$ is useless to the Verifier, since the Verifier does not know the values of the other weight-shards comprising the NN, or even how many other weight shards there are.
Yet $\omega$ holds the key to the training run's history. 
Any given weight-shard vector is incredibly unlikely to be yielded by the training process (there are $\gg 2^{10^6}$ possibilities even if we round to 1-bit precision).
But this particular $\omega$ is the nearly-deterministic result of the training run's hyperparameters $\mathbb{M}$ (including its random seed) and sequence of training data $\mathbb{D}$.
We refer to this information, along with information yielded throughout the training run $\mathbb{W}$ (such as intermediate weight snapshots), as the model's \emph{training transcript} $\mathbb{T} = \{ \mathbb{M}, \mathbb{D}, \mathbb{W}\}$.
It is plausible that it would be very difficult for the Prover to find a second ``legitimate'' training transcript $\mathbb{T}' = \{ \mathbb{M}', \mathbb{D}', \mathbb{W}' \} \neq \mathbb{T}$ that would exactly yield a full set of weights that included $\omega$ as a contiguous weight-shard.
It is also plausible that, if the Prover tried to lie and provide an $\mathbb{H}', \mathbb{D}'$, and $\mathbb{M}'$ that would \emph{not} result in $\omega$ but which they \emph{claimed} would result in $\omega$, then the Verifier might find a suite of tests that would detect such a faked transcript reliably.
If the Verifier could reliably detect such fakes, 
then by asking the Prover to provide a training transcript that would yield each logged snapshot $\omega$, the Verifier could force the Prover to reveal the true data $\mathbb{D}$, and hyperparameters $\mathbb{M}$ used in every large-scale training run (along with the full weights of the resulting model, not just $\omega$).
This would be sufficient information to check that the training run complied with any of the training rule types listed in Section \ref{s.rules}.

Unfortunately, having the Verifier simply re-run the full training process to verify a transcript's correctness is unworkable for several reasons.
First, the Prover would likely not be willing to reveal their training data, model weights, and hyperparameters, so the Verifier must do any verification without direct access to the data. (We address this in Section \ref{s.realworld}.)
Second, the compute cost to fully re-run the training transcript would be massive, as large as every original training run.
Third, the training run would likely not be perfectly reproducible: due to hardware-level noise, even two repetitions of the same sequence of training updates would gradually diverge.
Fourth, the Prover \emph{may} be able to construct a second ``spoof'' training transcript, that yields an exact match for $\omega$ but differs from the original training run that yielded $\omega$ in the first place.\footnote{As a trivial example, the Prover could claim that $\omega$ was simply the value of the random initialization, and no training had happened at the time of the snapshot.} 

\future{
\begin{itemize}
    \item \emph{Imperfect retraining}: due to irreducible hardware-level noise, two repetitions of an identical sequence of training steps will gradually diverge, so even the true training transcript would not result in an exact match of $\omega$.
    Thus, we can only verify that training resulted in an $\omega'$ that \emph{approximates} $\omega$.
    \item \emph{Compute cost:} Requiring the Verifier to re-run the entirety of every training run would be hopelessly expensive, creating a minimum 50\% overhead on all large-scale ML training, which is unacceptable. This would also raise the problem of how another party could verify the Verifier's compute usage, which by induction may lead to an infinite reverification cost.
    Thus, the Verifier must be able to verify training \emph{efficiently}, i.e., using $bC$ compute to verify a training run requiring $C$ compute, where $b \ll 1$.
    \item \emph{Privacy/secrecy violations}: In the above protocol, the Prover is forced to provide the Verifier access to all the (often private) training data, the training hyperparameters, and the final model weights, each of which is near-certain to be an unacceptable breach of the Verifier's confidentiality.
    \item \emph{Spoofing possibility}: It may be possible for the Prover to construct a second ``spoof'' training transcript, that yields an exact match for $\omega$.
    As a trivial example, the Prover might claim that $\omega$ was simply the value of the random initialization, and no training had happened at the time of the snapshot.
    Thus, the Verifier needs to be able to either place additional requirements on the types of allowable training runs, or detect each possible spoofing strategy.
\end{itemize}
}

Thanfully, a close variant of this problem has already been studied in the literature, known as ``Proof of Learning'' \cite{jia2021proof}.
The goal of a Proof-of-Learning (PoL) schema is to establish proof of ownership over a model $W_t$ (e.g., to corroborate IP claims) by having the model-trainer save the training transcript $\mathbb{T}$ (including hyperparameters $\mathbb{M}$, data sequence $\mathbb{D}$, and a series of intermediate full-model weight checkpoints\footnote{We use ``weight checkpoints'' as shorthand, but if using an optimizer like Adam \cite{KingmaB14}, the optimizer state should also be included.} $\mathbb{W} = \{W_0, W_{k}, W_{2k} \dots \}$) which only the original model trainer would know.
Jia et al. \cite{jia2021proof} propose a verification procedure that makes it difficult for any third party to construct a spoofed transcript $\mathbb{T}'$, if they only have access to $W_t$ and the unordered dataset.

The solution of \cite{jia2021proof} is as follows: once a Prover reports a training transcript $\mathbb{T}$, the Verifier checks that the initialization appears random, and then chooses a number of pairs of adjacent weight snapshots that are $k$ gradient steps apart $(W_i, W_{i+k}), \dots, (W_j, W_{j+k})$.
Then, rather than re-running all of training, the Verifier only reruns the training of these specific segments, starting at $W_i$ and progressing through the appropriate data batches $D_i \dots D_{i+k}$ to yield a $W_{i+k}'$. 
The Verifier then confirms that the resulting checkpoint is approximately correct: $\| W_{i+k} - W_{i+k}'\|_2 < \epsilon$ for some small $\epsilon$ that depends on the reproduction noise and the number of steps $k$.
The original paper proposed a heuristic for choosing which segments to verify,
% as naively-generated spoofs are likely to end at a different $W_t'$, and large jumps in weight-space would be needed for the spoof to instead appear to end at the target $W_t$).
though subsequent papers have demonstrated spoofing attacks that evade the original verification approach \cite{zhang2022adversarial, fang2022fundamental}. 
% \todo{constructing fake data with particular gradients, abusing epsilon to sneak in microupdates}. \todo{cite} connected the hardness of provably secure PoL on unconstrained ML training to certain open problems in learning theory \todo{cite}./
Thus far, the literature on PoL has been heuristic-based, and discovering PoL protocols with higher degrees of reliability is the subject of ongoing work \cite{fang2022fundamental}.

\subsection{Proof-of-Training-Transcript}

Our requirement, which we will define as ``Proof-of-Training-Transcript'' (PoTT), differs from Proof-of-Learning because our Prover knows a true training transcript that leads to the final weight-snapshot (since they executed the original training run), and the Verifier wants to verify that the Prover has reported the \emph{original} transcript that yielded $\omega$ (or at least one very close to the original).
Further, unlike PoL, we allow that an adversarial Prover might be willing to spend \emph{more than the original training compute} $\rulecomputeflops$ in order to construct a spoof (e.g., an adversary may be willing to pay double the cost in order to execute a rule-violating training run without being detected).
Thus, we must hope to push the cost to construct a spoof to be as high as possible, so as to make evasion impractical.

\begin{definition}
 A PoTT protocol $(R, V)$ is defined by a Prover training and reporting protocol $R$ and a Verifier verification protocol $V$.
A target weight vector $W_t$ is produced as the true result of a training run which complies with a training and reporting protocol $R$, and which can be summarized via a training transcript $\mathbb{T} \in \mathcal{T}$, where $ \mathbb{T} = \{\mathbb{M}, \mathbb{D}, \mathbb{W}\}$, $\mathbb{M}$ is the metadata required to reproduce the training run (including hyperparameters, random seed, and loss function), $\mathbb{D}$ is an ordered sequence of sets of training data batches, and $\mathbb{W}$ is a collection of intermediate weight checkpoints resulting from the training procedure.
The verification protocol $V$ should accept any such true training transcript with high probability, $\Pr[V(\mathbb{T}, W_t) = \text{accept}] > 1 - \delta_1$ for some small $\delta_1$.

A ``spoofed'' training transcript $\mathbb{T}' = \{\mathbb{M}', \mathbb{D}', \mathbb{W}'\}$ is a transcript, which may not correspond to any valid training run, and which is substantially different from the original transcript $\mathbb{T}$ in its data or hyperparameters: $d_1(\mathbb{D}, \mathbb{D}') \geq \delta_3$ for some edit distance $d_1$ quantifying the number of data point insertions/deletions, and/or $d_2(\mathbb{M}, \mathbb{M}') \geq \delta_4$ for some hyperparameter distance $d_2$. 
A reporting/verification protocol pair $(R, V)$ is $J$-efficient and $F$-hard if $V$ runs in at most $J$ time, and there does not exist any spoof-generation algorithm $A \in \mathcal{A}: \mathcal{T} \rightarrow \mathcal{T}$ such that $\Pr[V(A(\mathbb{T}),W_t) = \text{accept}] > 1 - \delta_2$ where $A$ runs in less than $F$ time. \looseness=-1
\end{definition}

Colloquially, we want a Prover training and reporting protocol and Verifier verification protocol such that the Verifier only accepts \emph{original} training transcripts that would result in a final weight checkpoint which contains a shard matching our on-chip weight-shard snapshot $\omega$.
We leave the problem of finding provably secure, efficient methods for PoTT as an important avenue for future work, but we discuss a few promising directions below.

% \future{
% We next provide intuition on why Proof-of-Training-Transcript is likely to be tractable in our setting.
% }
PoTT appears to be strictly harder than PoL, as it requires robustness to a better-resourced adversary has additional information (i.e., they know the true transcript $\mathbb{T}$) and has more compute-time to construct a spoof.
Given that existing PoL schemes are still heuristic-based and not yet provably secure, there may be a long way to go until we have PoTT methods that are both efficient and hard to spoof.
Still, one dynamic favoring the Verifier is that the Prover must \emph{commit} to a training transcript without knowing the Verifier's verification strategies.
Thus, Verifiers can amass secret collections of verification heuristics, much as the IAEA does not disclose all its methods for detecting nuclear activity.
% Moreover, as new verification strategies are discovered over time, these can be applied to the Prover's old transcripts.
% Since the Prover could not have anticipated these strategies, and its spoof was by construction \emph{not} the result of normal gradient-based optimization on natural data, any spoof is likely to eventually be caught by a future verification method.
% \footnote{
% This 
% PoTT methods are likely to evolve as we gain more knowledge about the ``typical'' structure of gradient-based optimization on natural data in neural networks.
% Previous spoofs (which are, by definition, not the result of pure gradient-based optimization on natural data) will not obey this structure, enabling new verification checks.
% }
Even if PoTTs are only ever heuristic-based, the presence of this dynamic may dissuade Provers from taking the risk of being detected by an unexpected test.

Defining conditions on the types of legitimate training runs is another useful source of leverage. 
For example, one Prover cheating strategy could be for the Prover to report one long training run as many shorter training runs, each claimed to be ``initialized'' where the previous training run ended.
A simple prevention would be for the training-and-reporting protocol $R$ to require the Prover to initialize every training run's weights via a known pseudorandom generator and a short seed.
This means that the randomness of the initial weights can later be confirmed by the Verifier.

Another promising strategy may be to require the Prover to \emph{pre-commit} to portions of its training transcript (e.g., the hyperparameters $\mathbb{M}$ and training batches $\mathbb{D}$) at the start of training.
This could be done by having the ML chip firmware log a hash of this precommitment, which would prove that the precommitment preceded the chip's snapshot $\omega$.
At the time of precommitment, the Prover does not know what trajectory the training run will follow or at what time it will be snapshotted, as the training has not yet been performed.
The Prover would be unable to construct a spoofed training transcript that would end at $\omega$ and precommit to it, because $\omega$ isn't known yet.
However, it is not obvious how to extend this approach to online learning settings like online RL, where the training data cannot be known ahead of time.

A final complication of our setting derives from the fact that the Verifier observes only a shard of the weights $\omega$, and not the full weight vector $W_t$.
It could be easier to construct a spoofed training transcript for some $\hat{W}$ which contains a shard matching $\omega$, but which differs from the true original weights $W_t \neq \hat{W}$ on the portion of the weight vector outside the shard.
We briefly describe an intuition for why this is likely to be as hard as general PoTT-spoofing.
Assuming $\omega$ must contain weights from more than a single linear layer, any Prover must at minimum construct a valid PoTT for this smaller NN represented by $\omega$, except without any original training transcript to start from (making it similarly hard to the original Proof of Learning problem).
Alternatively, if the Prover tries to reuse the original training transcript, it will require them to synthesize fake data and labels to exactly reproduce the true inputs/gradients, and these must correspond to a reasonable loss function that is actually decreasing over time.
If the Prover alternatively attempts to fake the surrounding layers' weights, then the weights in these surrounding layers must also have their own valid training trajectories as part of the training transcript \emph{and} must generate the appropriate inputs/gradients to yield $\omega$, simultaneously.

\subsection{Real-world implementation}\label{s.realworld}

As the literature uncovers new methods for verifying training transcripts, the Verifier can run these protocol to check that, for each of the logged weight snapshots on the Prover's chips that the Verifier inspects, the Prover has provided a legitimate corresponding training transcript.\footnote{
    Note that this requires the Prover to save and report training transcripts for all training runs corresponding to chips sampled by the Verifier, not just the largest-scale ones.
    This is because, without a matching training transcript, it's impossible for the Verifier to determine whether a given weight-shard was part of a large-scale training run or a small-scale one.
    Alternate methods for proving that a chip was only involved in a short training run are an important direction for future work.
    }
The Verifier can then examine the data, hyperparameters, and resulting models as reported by the trianing transcript, and thereby detect whether the Prover complied with the training rules.

A key design constraint is that the Prover does not disclose the raw transcripts (which likely contain confidential data that the Prover would prefer the Verifier did not see).
We propose a method for the Verifier to verify the Prover's previously-reported transcript's correctness without ever observing the raw data. 
First, the Prover and Verifier utilize a ``trusted cluster'': a set of chips that both parties trust, which is air-gapped to prevent the Prover's sensitive data from leaking, and which is equipped to execute small amounts of ML inference and training (so as to segment-wise approximately reproduce the Prover's original training process).
\footnote{Maintaining such compatible training clusters may prove quite challenging.
One desirable direction for future work is in verification methods by which the Verifier does not need to directly reexecute model training.
For example, it may be possible for the Verifier to interactively query the Prover for additional information on segments of the training run beyond what was included in the training transcript.
There may be queries that have valid answers \emph{only if} the original training transcript was authentic (e.g., a series of weight sub-checkpoints between two checkpoints, each with progressively lower loss), and the Prover could dynamically recompute answers to these queries using their own data-center.
While some properties of the verification would still need to be confirmed using a neutral cluster to maintain the confidentiality of the query-responses, such clusters may not need to be equipped for large-scale training, and thus be much easier to maintain.
}
Second, instead of reporting the full transcript, the Prover initially only provides the Verifier with hashes of the training transcript $\mathbb{T}$.
Then, at a later verification stage, the Prover supplies the true $\mathbb{M}, \mathbb{D}$, and $\mathbb{W}$ within the trusted cluster (which must match the reported hashes), executes the verification protocol, and reports whether both steps succeeded.
Finally, the Verifier can use this same trusted cluster to run programs checking that the Prover's training transcript complies with all agreed ML training rules (e.g., measuring the performance of the final model weights on agreed benchmarks).
We provide more details on a possible procedure in Appendix \ref{a.trustedcluster}. \looseness=-1

% \todo{topic sentence instead of question}
% Logistically, how could logged weight-shard snapshots be verified in practice via PoTT, assuming that the verification algorithms look similar to \todo{cite Jia et al}'s Proof-of-Learning scheme?
% Our solution is specifically designed to avoid the Prover ever having to directly reveal the hyperparameters $\mathbb{M}$, training data $\mathbb{D}$, and model weights $\mathbb{W}$ to the Verifier, as in many cases these data are either private or proprietary.

% First, when executing the training run, the Prover follows any verifiable restrictions on their training (e.g., starting from a subset of valid initializations, or precommitting to the model weights).
% Throughout training, the Prover saves the hyperparameters (e.g., training code), the sequence of training data batches, and periodic snapshots of the model weights.
% They also make sure to track any randomness used, and generally make sure their training process is repeatable (up to low-level noise) assuming access to similar but non-identical hardware configurations.
% This itself is a nontrivial technical challenge \todo{cite literature on repeatability}, especially in co-training settings like GANs or RL on learned reward models \todo{cite}, and would benefit from further investments in replication tools for common ML frameworks.

% Periodically, the Prover reports their chips' on-chip logged weight-shard snapshot hashes, along with hashed versions of the Proofs-of-Training-Transcript matching each of these logged hashes (including hashes of $\mathbb{M}$, hashes of each model snapshot $\mathbb{W}$, and sequences of hashes of training data points $\mathbb{D}$).
% The Prover may need to further disclose basic information like the distance between weight-snapshot pairs, which may be needed for the Verifier to determine which regions of the transcript to verify.\footnote{This info could be proven to the Verifier securely and privately, for example by using standard ZK-SNARK proof tools to confirm that two given hashes correspond to vectors that have a given $L_2$ distance between them.}

% As in Section \ref{s.chipowner}, the Verifier chooses a subset of these chips whose logs will be confirmed via physical inspection.
% The Verifier inspects the training transcripts and associated metadata, and chooses the subsets of the training transcripts to verify.
% They also select any other components of the training transcript (e.g., a random sample of the training data) that they will need in order to determine that that the training run has complied with the ML training rules.
% At this point, the Verifier has a set of hashes of the inputs to its verification protocol (including model weight snapshots and training data points), but does not have access to any of this information directly.

% The Verifier needs to have faith in the results of the computation, but without seeing the inputs themselves.
% In principle, this might be addressed by cryptographic techniques like \todo{FHE doesn't do exactly this; what does?}, but in practice these techniques cannot efficiently execute computationally intensive programs, like long sequences of gradient updates on billion-parameter models.
% Instead, the Prover and Verifier can agree on a neutral jointly-trusted cluster, which they will use to execute the verification protocol.
% The Verifier needs to be able to trust in the integrity of the cluster's computation.
% The Prover, conversely, needs to trust that the cluster will not reveal its private data, by verifying that it cannot communicate with the outside world and has no persistent storage (a standard feature in many datacenters \todo{cite something highlighting this is possible}).
% Such neutral clusters could be maintained by a trusted third party, or when no such trusted party exists, could be jointly overseen by both the Prover and Verifier. \todo{mention that these clusters have other benefits, see later}

% At agreed-upon intervals, the Verifier supplies the hashes of the inputs (including hyperparameters $\mathbb{M}$) to the neutral cluster, and the Prover supplies the inputs themselves.
% The cluster hashes the Prover's inputs and confirms they match the hashes.
% Then the cluster executes the Verifier's verification protocol.
% For example, when re-executing a training segment to verify that it would reach a particular chip-logged weight shard snapshot $\omega_{i+k}$, the cluster starts at one weight snapshot $W_i$, and then computes optimizer updates $k$ times using the specified inputs (where each update is computed via code generated from the hyperparameters, and even code snippets, defined by $\mathbb{M}$).
% Finally, the cluster checks that the resulting weight vector $W'_{i+k}$ has a slice $\omega'_{i+k}$ that is within an $\delta$ \todo{check this is the right delta} distance of the chip-logged weight shard $\omega_{i+k}$.
% If all the verification protocols passed, the cluster outputs that the Prover has ``passed''.
% If not, the cluster outputs that the Prover ``failed'', prompting an investigation.

% Assuming the training transcript is verified as correct, the Verifier can now compute any functions of the training transcript that would determine its compliance with agreed rules.
% This could include properties of its training data distribution (which can be established from a randomly-selected subset of the training transcript's data), the performance of the final model on specific benchmarks, and properties of the hyperparameters.
% Assuming the results confirm the Prover's compliance, the Verifier could be certain that with probability at least $1 - p - \delta_2$ \todo{make sure p is the right notation from the previous section}, the Prover has not used ML chips to execute a training runs using greater than $C$ compute-hours which violated the agreed upon rules.
% The Prover has also not disclosed any sensitive information to the Verifier, including training data, model weights, or hyperparameters.

% Beyond this, we also need to ensure that the ongoing cost of operating the system is not prohibitive.
When evaluating which training transcript verification protocols to pursue, Verifiers should prioritize those verification strategies that get the most detection-probability, for the lowest cost.
Beyond the upfront costs of building trusted clusters or modifying chip hardware, the system has three ongoing operating costs: 
the efficiency loss from pausing to save weight checkpoints and the weight-shard snapshots (as described in Section \ref{s.onchip}), the storage costs for maintaining training transcripts (and in particular the weight-checkpoints, each of which may require terabytes) until the Verifier inspects them, and the compute costs to execute the training-transcript verification protocols on the trusted clusters.
These costs seem likely to scale linearly with the total compute used by the Prover, and will ultimately depend on the efficiency with which training transcripts can be verified.
Even though governments could in principle pressure Provers into paying the costs of compliance, a 1\% overhead for each dollar spent on training compute would be much easier for Provers to comply with than a 20\% overhead.
Indeed, for International Verifiers, the history of arms control suggests that maximally-stringent verification measures may have limited utility, as they may reduce the likelihood of compliance \cite{reddie2019governing}.
One important avenue for future work is finding cheaper, lower-fidelity alternatives to NN-retraining-based verification, which need only establish limited properties of the weight-shard's corresponding training run, and which could prompt more expensive verification methods if needed.