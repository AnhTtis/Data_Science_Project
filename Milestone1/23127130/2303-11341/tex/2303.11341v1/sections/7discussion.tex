\section{Discussion}\label{s.discussion}
The described additions to the production and operation of ML training chips, if successfully implemented, would enable untrusting parties (like a government and its domestic companies, or the US and Chinese governments) to verify rules and commitments on advanced ML development using these chips.
% Such a system could substantially reduce the likelihood of ``AI arms races'', much as the US-Russia START treaties have limited the proliferation and buildup of nuclear weapons.
There are many useful measures that governments and companies could begin taking today to enable future implementation of such a framework if it proved necessary, and that would simultaneously further businesses' and regulators' other objectives.
\begin{itemize}
    \item Chipmakers can include improved hardware security features in their data-center ML chips, as many of these are already hardware security best practices (and may already be present in some ML chips \cite{nvidiah100}).
    These features are likely to be independently in-demand as the costs of model training increase, and the risk of model theft becomes a major consideration for companies or governments debating whether to train an expensive model that might simply be stolen.
    \item Similarly, many of the security measures required for this system (firmware and code attestation, encryption/decryption modules, verification of produced models without disclosing training code) would also be useful for ``cloud ML training providers'', who wish to prove to security-conscious clients that the clients' data did not leave the chips, and that the clients' models did not have backdoors inserted by a third party \cite{liu2017trojaning}.
    Procurement programs like the US's FedRAMP could encourage such standards for government contracts, and thereby incentivize cloud providers and chipmakers to build out technical infrastructure that could later be repurposed for oversight.
    \item Individual companies and governments can publicly commit to rules on ML development that they would like to abide by, if only they could have confidence that their competitors would follow suit.
    \item Responsible companies can log and publicly disclose (hashed) training transcripts for their large training runs, and assist other companies in verifying these transcripts using simple heuristic.
    This would not prove the companies \emph{hadn't also} trained undisclosed models, but the process would prove technical feasibility and create momentum around an industry standard for (secure) training run disclosure.
    \item Companies and governments can build trusted neutral clusters of the sort described in Section \ref{s.realworld}.
    These would be useful for many other regulatory priorities, such as enabling third-party auditors to analyze companies' models without leaking the model weights.
    \footnote{For similar reasons, the US Census Bureau operates secured Federal Statistical Research Data Centers to securely provide researchers access to sensitive data \cite{uscensusbureau_2022}.}
    \item Governments can improve tracking of ML chip flows via supply-chain monitoring, to identify end-users who own significant quantities of ML chips.
    In the West, such supply-chain oversight is already likely to be a necessary measure for enforcing US-allied export controls.
    \item Responsible companies can work with nonprofits and government bodies to practice the physical inspection of ML chips in datacenters.
    This could help stakeholders create best practices for inspections and gain experience implementing them, while improving estimates of implementation costs.
    \item Researchers can investigate more efficient and robust methods for detecting spoofed training transcripts, which may be useful for in proving that no backdoors were inserted into ML models.
\end{itemize}
For the hardware interventions, the sooner such measures are put in place, the more ML chips they can apply to, and the more useful any verification framework will be.
Starting on these measures early will also allow more cycles to catch any security vulnerabilities in the software and hardware, which often require multiple iterations to get right.

\subsection{Politics of Implementation}
Given the substantial complexity and cost of a monitoring and verification regime for large-scale ML training runs, it will only become a reality if it benefits the key stakeholders required to implement it.
In this last section, we discuss the benefits of this proposal among each of the required stakeholders.
\begin{itemize}
    \item \emph{The global public}: Ordinary citizens should worry about the concentration of power associated with private companies possessing large quantities of ML chips, without any meaningful oversight by the public.
    Training run monitoring is a way to make powerful companies' advanced ML development accountable to the public, and not just the free market.
    Most importantly, ordinary people benefit from the security and stability enabled by laws and agreements that limit the most harmful applications of large-scale ML systems.
    \item \emph{Chipmakers and cloud providers}: Absent mechanisms for verifying whether ML chips are used for rule-violating training runs, governments may increasingly resort to banning the sale of chips (or even cloud-computing access to those chips) to untrusted actors \cite{bis2022controls}.
    By enabling provable monitoring of large-scale ML training runs, chipmakers may reverse this trend and may even be able to resume sales to affected markets.
    \item \emph{AI companies}: Responsible AI companies may themselves prefer not to develop a particular capability into their products, but may feel they have no choice due to competitive pressure exerted by less-scrupulous rivals.
    Verifying training runs would allow responsible AI companies to be recognized for the limits they impose on themselves, and would facilitate industry-wide enforcement of best practices on responsible ML development.
    \item \emph{Governments and militaries}: Governments' and militaries' overarching objective is to ensure the security and prosperity of their country.
    The inability to coordinate with rivals on limits to the development of highly-capable ML systems is a threat to their own national security.
    There would be massive benefit to a system that enabled (even a subset of) countries to verify each others' adherence with ML training agreements, and thus to maintain an equilibrium of responsible ML development. 
\end{itemize}
Even if only a subset of responsible companies and governments comply with the framework, they still benefit from verifiably demonstrating their compliance with self-imposed rules by increasing their rivals' and allies' confidence in their behavior \cite{horowitz2021ai} (and thus reducing their rival's uncertainty and incentive towards recklessness).

Finally, we highlight that the discussed verification framework requires continuous participation and consent by the Prover.
This makes the framework fundamentally non-coercive, and respects national sovereignty much as nuclear nonproliferation and arms control agreements respect national sovereignty.
Indeed, the ongoing success of such a system relies on all parties' self-interest in continuing to live in a world where no one -- neither they, nor their rivals -- violates agreed guardrails on advanced ML development.