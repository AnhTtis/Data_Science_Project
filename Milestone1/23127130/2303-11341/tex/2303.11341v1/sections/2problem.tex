\section{The Problem: Detecting Violations of Large-Scale ML Training Rules}\label{s.problemform}
% In this section we \todo{lay out the goals of this section, as it relates to the vision of the paper presented in the intro}
% \subsection{Prover-Verifier Relationship}
We focus on the setting in which one party (the ``Verifier'') seeks to verify that a given set of ML training rules is being followed, and another party (the ``Prover'') is developing the ML system and wants to prove to the Verifier that it is complying with those rules.
The Verifier can request that the Prover take actions, such as disclosing information on training runs, in order to help the Verifier determine the Prover's compliance.
The Prover is a ``covert adversary'' \cite{aumann2007security} -- they may benefit from \emph{violating} the ML training rule, but will only seek to violate the rule \emph{if they can still appear compliant} to the Verifier.
There are two real-world Prover-Verifier relationships we are particularly interested in:
\begin{itemize}
\item \emph{Domestic Oversight}:
    Governments have a clear interest that the ML systems developed by companies operating within their borders comply with certain rules.
    Regulators can level both civil and criminal penalties on organizations caught violating rules, and often require organizations to maintain records that prove regulatory compliance (e.g., financial transaction record-keeping requirements).
    \future{
    Unfortunately, the benefit of domestically regulating training may be limited, as dangerous ML models can be developed and deployed in one country and yet be transported via the internet and deployed within the borders of another.
    Thus, effectively preventing certain ML use-cases in any one country may require international cooperation (similar to the global nature of policing child sexual abuse material) \future{cite \url{https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12418}}}
% \item \emph{Corporate provider as Verifier, customer as Prover}:
%     Providers of AI development resources (e.g. cloud providers) may want to contractually require that customers not use their products to violate rules.
%     The rules are national laws, and if the provider enables legal violations they themselves may be considered culpable. (Real-world parallel: cloud providers detecting servers used for cybercrime.)
%     Alternatively, the rules may simply be policies that the Provider considers to be in their interest.
%     (Real-world parallel: cloud providers refusing to host neo-Nazi websites.)
%     In either case, the Verifier may threaten to deny the Prover future access unless they comply.
%     \todo{Add setting of inter-lab industry best practices, e.g. as part of a "responsible AI" consortium}
\item \emph{International Oversight}:
The most significant types of ML training rules may be those enforced internationally (on companies and governments in multiple countries), and verified by other governments or international bodies.
    These include enforcing globally-beneficial rules (e.g., combatting disinformation), and verifying arms control agreements (e.g., limiting the development of autonomous code-generating cyberweapons).
    There is precedent for countries abiding by international agreements with strict monitoring regimes when they stand to benefit, such as Russia's historically allowing random U.S. inspections of its missiles as a part of the START treaties, in exchange for certainty that the U.S. was abiding by the same missile limits \cite{schenck2011start}. \looseness=-1
\end{itemize}

Thus, the problem we address is: what minimal set of verifiable actions can the Verifier require the Prover to take that would enable the Verifier to detect, with high probability, whether the Prover violated any training rules?

\subsection{What types of rules can we enforce by monitoring ML training?}\label{s.rules}
It is important that standards and agreements on ML training focus on preventing concrete harm, and otherwise leave society free to realize the broad benefits of highly-capable ML systems.
Indeed, there are many types of ML models that should not only be legal to train, but that should open-sourced so that all of society can benefit from them \cite{solaiman2023gradient}.
The proposed framework focuses only on enforcing rules on the training of those more dangerous models whose creation and distribution would substantially harm society or international security.
Indeed, as mentioned in Section \ref{s.limits}, this framework \emph{could not} prevent smaller-scale training of ML models, and thus limits the risk of overreach by authoritarian Verifiers.
Below are some informative properties that a Verifier could determine by monitoring the training process of an ML model:
% \ot{Use, as a running example, "code-generating+executing RL agents trained with >X compute". This is plausibly an example that we can detect, because its dataset has a certain signature, its training algorithm has a certain signature, and its training length/loss has a certain signature.}
\begin{itemize}
    \item \emph{Total training compute}, which has proven to be an indicator for ML models' capabilities \cite{kaplan2020scaling, sutton2019bitter}.
    \item \emph{Properties of the training data}, such as whether a language model's text dataset contains code for cybersecurity exploits.
    \item \emph{Properties of the hyperparameters}, such as the fraction of steps trained via reinforcement learning.
    \item \emph{The resulting model's performance on benchmarks designed to elicit its capabilities}, including whether the model's capabilities exceed agreed-on thresholds, and including interactive benchmarks (e.g. finetuning the model on a particular task).
    \item Combinations of the above --- for example, ``if a model was trained on RL-for-code-generation for greater than $X$ FLOPs, then it should not be trained beyond $Y$ performance on $Z$ benchmarks.''
\end{itemize}
Ultimately, these rule thresholds should be selected based on the model capabilities that would result.
Current ``scaling law'' extrapolations are not yet able to reliably predict ML models' downstream capabilities \cite{ganguli2022predictability}, so finding principled methods for deciding on rule-thresholds that achieve desired policy outcomes is an important area for future work.

If a Verifier can reliably detect the aforementioned training run properties, that would allow them to mandate several types of rules, such as:
\begin{itemize}
\item 
\emph{Reporting requirements} on large training runs, to make domestic regulators aware of new capabilities or as a confidence-building measure between companies/competitors \cite{horowitz2021ai}.
\item 
\emph{Bans or approval-requirements} for training runs considered overly likely to result in models that would threaten society or international stability. Approval could be conditioned on meeting additional requirements (e.g., willingness to comply with downstream regulations on model use, increased security to prevent model-theft, greater access for auditors).
        % 3. Regulations on inference-sales: if thereâ€™s a way to require that large-scale inference must be done with signed weights (e.g. required for access to cloud-scale inference resources), then can require that the weights be the result of such a training run
\item 
\emph{Requiring that any trained model be modified to include post-hoc safety mitigations} if the unmodified model could be expected to pose a severe accident risk absent those mitigations.
Such safety assessments and mitigations (such as ``Helpful and Harmless'' finetuning \cite{bai2022training}) may involve a prohibitive upfront cost that companies/governments would otherwise avoid.
However, once they have been forced to make the investment and built a less accident-prone model, they may then prefer to use the safer version.
Such rules allow all parties to coordinate spending more resources on safe and responsible innovation, without fearing that their competitors may secretly undercut them by rushing ahead without addressing negative externalities.
\end{itemize}

% Many of the most impressive capability breakthroughs of the past few years have required large quantities of training compute, and the quantity of compute used has proven to be a useful proxy for models' capabilities \cite{kaplan2020scaling, hoffman2022training}.
% Such large-scale training often requires hundreds or thousands of specialized accelerators with high inter-chip communication bandwidth (such as Google TPUs, the NVIDIA V100, A100, and H100 GPUs, or AMD MI250s) used for a span of weeks or months to compute thousands or millions of gradient updates.
% We refer to these specialized accelerators as ``ML chips'', which we distinguish from consumer chips with lower bandwidth (e.g., the NVIDIA RTX 4090).
% These large training runs both leave a large physical footprint for Verifiers to detect, and are generally undertaken by sizable organizations (e.g., companies, governments, academic clusters) more able to comply with regulatory obligations.

% Our framework's major assumption is that \emph{the most capable ML models will continue to require the use of specialized data-center chips, in large quantities and for weeks or months.}
% We discuss arguments for and against this proposition in Appendix \ref{app.howmuch}.

% We acknowledge that even small quantities of consumer GPUs can be used to train smaller models (or finetune existing large models) to pose a substantial risk to public safety and national security (e.g., computer vision models for autonomous combat drones).
% Also, if a rule-violating large-scale model is trained, its weights may from then on be copied undetectably, and it can be deployed using consumer GPUs.
% We are likely to need laws and regulations to limit the harms from bad actors' deploying such tools.
% However, exhaustively \emph{detecting} whether rules were violated across all compute devices would require surveilling and policing individual citizens' use of their personal computers, which would be highly unacceptable on ethical grounds.
% We instead choose to focus our attention upstream, limiting the most dangerous models from being created \emph{at all} by enforcing rules on the large-scale training runs that produce them.

% Unfortunately, smaller ML systems trained with undetectably-small quantities of compute can still pose a substantial public risk, like computer vision models for autonomous military drones.
% Similarly, large-scale models can be pretrained in a rule-abiding fashion and still be fine-tuned towards malicious use-cases using small quantities of compute.
\future{
And if a model has been trained in a rule-violating fashion, its weights can from then on be copied undetectably, and it can be deployed undetectably using only consumer hardware (e.g. gaming GPUs).
We are likely to need laws and regulations to limit the harms from bad actors' deploying cheaply-trainable or fine-tuned ML models.
But exhaustive enforcement of such rules would require surveilling and policing individual citizens' use of their personal computers, which would be highly unacceptable on ethical grounds.
}


\future{Optimistically, research suggests that limited fine-tuning may not endow the model with entirely new capabilities \todo{cite}.
If so, enforcing that large-scale training of the original model did not contain certain capabilities would be sufficient to constrain the malicious capabilities of fine-tuned models.}

\subsection{Other Practical Requirements}
There are several other considerations for such a monitoring system to be practical. Its cost should be limited, both by limiting changes to current hardware, and by minimizing the ongoing compliance costs to the Prover and enforcement costs to the Verifier.
The system should also not pose a high risk of leaking the Prover's proprietary information, including model weights, training data, or hyperparameters. 
Most importantly, the system must be robust to cheating attempts, even by highly-resourced adversaries such as government hacking groups, who may be willing to employ sophisticated hardware, software, and even supply-chain attacks.



%There are several practical requirements this framework will aim to achieve, to make adoption more likely.
\future{
% \begin{itemize}
   % \item 
   
   \future{
    \emph{Limited upfront costs}: We aim to minimize the changes to existing ML training chips, and much of our required functionality may already be present in the current generation of chips (e.g., the NVIDIA H100 GPU \cite{nvidiah100}).
    }
   % \item 
    \emph{Future proof}: We aim for our framework to be compatible with any future innovations in machine learning, so long as they rely on large-scale gradient-based optimization.
   % \item 
   \future{
    \emph{Limited variable costs}: We aim to limit the ongoing compliance costs both to the Prover and Verifier, by minimizing the compute and storage overhead, and the human labor required to physically confirm compliance.
    }
   % \item 
    \emph{Preserving privacy and security}: We never give the Verifier access to the Prover's proprietary information, including model weights, training data, or hyperparameters/code.
    %\item 
    \emph{Robustness to strong adversaries}: Most importantly, we would like this system to be robust to cheating attempts, even by highly-resourced Provers (up to and including advanced government hacking groups, who may be willing to execute sophisticated hardware, software, and supply-chain attacks).
    \future{
    The more robust we can make the system, the more likely we are to get the world's most-powerful governments (e.g. the US and China) to trust it as a basis for mutual assurance that neither are crossing red lines on advanced ML.
    }
%\end{itemize}

}