
\section{Solution Overview}\label{s.overview}

% Assume that in the section before, we highlighted the desire to monitor training runs, that they are large runs, that we wanted it to not be privacy-invading, and that they would be verifiably secure. Also that 
In this section, we outline a high-level technical plan, illustrated in Figure \ref{fig:overview}, for Verifiers to monitor Provers' ML chips for evidence that a large rule-violating training occurred.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/verification_process.pdf}
    \caption{Overview of the proposed monitoring framework.}
    \label{fig:overview}
\end{figure}
The framework revolves around chip inspections: the Verifier will inspect a sufficient random sample of the Prover's chips (Section \ref{s.sampling}), and confirm that none of these chips contributed to a rule-violating training run.
For the Verifier to ascertain compliance from simply inspecting a chip, we will need interventions at three stages: on the chip, at the Prover's data-center, and in the supply chain.
\begin{itemize}
    \item \emph{On the chip} (Section \ref{s.onchip}): When the Verifier gets access to a Prover's chip, they need to be able to confirm whether or not that chip was involved in a rule-violating training run.
    Given that rule violation depends only on the code that was run, our solution will necessitate that ML chips logging infrequent traces of their activity, with logging done via hardware-backed firmware.
    We suggest that ML chips' firmware occasionally log a copy of the current state of the chip's high-bandwidth memory to long-term storage, and in particular, that it logs the shard of the NN's weights stored in memory. 
    These \emph{weight-snapshots} can serve as a fingerprint of the NN training that took place on each chip.
    \item \emph{At the data-center} (Section \ref{s.datacenter}): The Verifier needs a way to interpret the chips' logs, and determine whether or not they are evidence for a rule-violating training run.
    To that end, the Prover, who is training the model, will be required to store a transcript of the training process --- including training data, hyperparameters, and intermediate weight checkpoints --- for each model they train.
    Using protocols similar to ``Proof-of-Learning'' \cite{jia2021proof}, these training transcripts may serve as provenance for the logged weight-snapshots, which are themselves the result of the same training process.
    In practice, for each (hash of a) weight-snapshot logged by a chip, the Prover provides the Verifier (the hashed version of) the matching training transcript.
    Then the Prover and Verifier jointly and securely verify that, with high probability, retraining using the training transcript would have indeed resulted in the logged weight-snapshot (and that no other valid training transcript could have resulted in that snapshot).
    Finally, now that the Verifier knows an approximate training transcript of the training run that had been executed on that chip at that time, they can examine properties of the training transcript to confirm that the Prover has complied with the agreed upon rules.
    \item \emph{At the supply chain} (Section \ref{s.supplychain}): The Verifier needs to know which ML chips the Prover owns, so that the Verifier can randomly inspect a representative sample of those chips, to confirm their ownership and that their logging mechanisms are reporting correctly.
    Without this chip-ownership verification step, a Prover might covertly acquire a large quantity of chips and use them for training without ever notifying the Verifier.
    Keeping track of chip-ownership is viable because the cutting-edge data-center chip supply chain is highly concentrated, meaning that chips originate from a few initial chokepoints and can be monitored thereafter.
\end{itemize}

These steps, put together, enable a chain of guarantees.
\begin{itemize}
\item
    When any organization wants to train a large rule-violating ML model, they must do so using chips that the Verifier is aware of.
\item
    These chips will occasionally log weight-snapshots.
    Each time a chip logs a weight-snapshot, the Prover must report the log to the Verifier, along with (hashes of) training transcripts that establish the provenance of that weight-snapshot.
\item
    If the Prover tries to hide the logs from every chip involved in the training run, the Verifier will eventually find out, because it will sample and physically inspect at least one of those chips with high probability.
\item
    Also, the hashed training transcripts that the Prover provides along with the logged weight-snapshot need to be authentic, reflecting the training run that was run on the chip.
    If they are not authentic, they will be caught by the Verifier's transcript verification procedure.
\item
    If the training transcript \emph{is} authentic, and the run violated the rules, then the Verifier can tell, and will catch the Prover.
\end{itemize}
    
Thus, so long as the Prover complies with the Verifier's steps, the Verifier will detect the Prover's rule-violation with high probability.
Just as in financial audits, a Prover's refusal to comply with the verification steps would itself represent an indication of guilt.
%In the next sections, we will explore the technical requirements for each of the three intervention stages, and highlight remaining open problems.

\subsection{Chip inspections}\label{s.inspection}
% \future{In an ideal world, chips' behavior could simply be guaranteed in hardware, and they could remotely communicate with the Verifier via a unique private key embedded on-chip.
% Unfortunately, in reality, it is generally understood within the hardware security community that there is as-of-yet no viable technology for ``tamper-proofing'' chips, and even secure hardware can be undermined if an attacker is provided physical access. \todo{cite}
% The Prover, being the owner of the chip, will certainly have such access.
% However, tamper-evidence technology is much more mature, and indeed many hardware attacks require physically damaging the chip in ways that would be nearly impossible to hide from a physical inspection by the Verifier.
%  \tdo{Add a note about what's the deal with non-invasive attacks.}
% }
Ideally, chips could remotely report their logs, with on-chip firmware and remote attestation being sufficient to guarantee that those logs were truthfully reported (see Section \ref{s.onchip}).
Such guarantees may be sufficient to deter most Provers from misbehavior.
Unfortunately, these measures may not restrain well-resourced adversarial Provers:
it is generally understood within the hardware security world that, given unlimited physical access to a chip (which the Prover certainly would have), a chip's attestation and signed-firmware-enforcement mechanisms can be undermined \cite{skorobogatov2005semi}.
However, by physically inspecting chips after the fact, it may be possible to reliably detect such tampering.
This is because most hardware attacks require physically damaging the chip in ways that would be nearly-impossible to hide from a Verifier's subsequent inspection, especially if ML chips are supplemented with tamper-evident packaging measures \cite{waksman2010tamper}.
Luckily, rather than needing to inspect every chip, we only need to inspect enough to catch at least one chip from each large training run (Section \ref{s.sampling}).

A chip inspection can be done in one of two ways.
The Verifier can send an inspector to the facility in which a sampled chip is stored, which may be cost-effective if the facility contains multiple chips selected for sampling.
Alternatively, the ML chips can be \emph{transported a neutral inspection site}, as these chips are generally housed in easily-removable servers. (Sensitive components like storage can be removed prior to transport.)
The transporting option may be more attractive to security-conscious Provers who would prefer not to allow Verifier inspectors into their data-centers.

A chip inspection involves several steps.
First, the Verifier confirms the chip's serial number (often burned into chips and accessible both digitally and physically) matches the serial number they requested from the Prover.
This confirms that this is the correct randomly-sampled chip, and also that the Prover still owns the chip and has not secretly resold it to an untrusted party.
Second, the Verifier checks the on-chip logs, from which they extract the weight snapshot hashes which they will check against the Prover's reported training transcripts (Section \ref{s.datacenter}).
Finally, the Verifier checks that the chip's logging mechanisms, such as its secure firmware authentication mechanism (Section \ref{s.onchip}), have not been physically tampered with.


\subsection{How many ML chips does the Verifier need to inspect?}\label{s.sampling}
Sections \ref{s.onchip}, \ref{s.datacenter}, and \ref{s.supplychain} will together guarantee that if the Verifier observes even a single ML chip that logged a weight snapshot hash during a rules-violating training run, then the Verifier can use that weight snapshot to prove that the Prover violated the rules.
For the Verifier to make sure they'd observe at least one such snapshot from a large training run with high probability (or alternatively, catch a chip that'd been tampered with), they would need to inspect at least $s$ of the Prover's $\totalchips$ chips every $T$ days.
We now seek to calculate $s$.

We define a large-scale training run as any training run using more than $\rulecomputeflops$ FLOPs.
We only seek to verify rules on training runs using $> \rulecomputeflops$ FLOPs, meaning that any weight-snapshots from the first $\rulecomputeflops$ FLOPs are not incriminating (as the Prover could have stopped the training run after that snapshot).
Catching a chip that happened to save a weight-snapshot at time $\rulecomputeflops + \epsilon$ is vanishingly unlikely for small $\epsilon$, so we instead provide guarantees on detecting the case where the Prover's training run is executed for $\geq 2 \rulecomputeflops$, substantially exceeding $\rulecomputeflops$.\footnote{We can always keep the detection threshold the same by cutting $\rulecomputeflops$ in half.
Also, the literature on neural scaling laws \cite{kaplan2020scaling, rosenfeld2021scaling, epoch2023scalinglawsliteraturereview} suggests that model loss decreases logarithmically at best given additional compute, so failing to catch a less-than-2x violation may have limited consequences.}
This means that the training run was executed for an additional second period of at least $\rulecomputeflops$ FLOPs, during which any weight snapshot would capture evidence of an ML model that had already been trained for at least $>\rulecomputeflops$ FLOPs.
From hereon, when we describe detecting a snapshot of a ``large-scale training run'', we refer to this second stage of the training run.
If a Verifier wanted to catch a Prover \emph{before} they had completed a $\rulecomputeflops$ rule-violating training run, they can simply shrink their target threshold to a fraction of $\rulecomputeflops$, and sample chips on a rolling basis.

The Verifier, whether due to specific suspicions or routine due diligence, wants to audit a particular Prover's $\totalchips$ chips.
Let $a$ be the FLOPs per day for those chips.
We will conservatively assume that NN training can be perfectly parallelized and utilize all the FLOPs on each chip.
Let $f$ be the expected number of weight-snapshots saved by the chip per day (Section \ref{s.onchip}) snapshots are Poisson distributed.
The Verifier wants to catch a training run within at most $T$ days of its concluding, though the training run itself may have lasted for a longer period
\footnote{For simplicity, we assume $\trainingperiod$ is divisible by $\monitoringperiod$}
$\trainingperiod > \monitoringperiod$.
This implies that the Prover must have used a minimum
of $\frac{\rulecomputeflops}{a\trainingperiod}$ chips.
One might expect that by increasing $\trainingperiod$, the Prover could use fewer chips and therefore evade detection, but this turns out to not to meaningfully improve the Prover's evasion probability (a $<5\%$ increase) because each chip is sampled $\trainingperiod/\monitoringperiod$ times as often. (See Appendix \ref{a.provertricks} for details.)
Thus for simplicity, we assume the Verifier will choose $s$ as though $\trainingperiod$ is fixed.

To detect at least one chip with a snapshot from a rule-violating training run with probability $p$, the Prover needs to sample $s$ chips each time period $T$, where:
\begin{align}
\label{eq:samplingrate}
    s \geq \frac{\log{(1-p)}}{ \log{\left (1 - \frac{\rulecomputeflops}{a\trainingperiod \totalchips} \left ( 1 - e^{-f\monitoringperiod} \right ) \right ) }}
    \cdot \frac{\monitoringperiod}{\trainingperiod} 
\end{align}
We provide a derivation for Equation \ref{eq:samplingrate} in Appendix \ref{a.samplingderivation}.

In Table \ref{tab:samples}, we provide the total required samples-per-year $365 \cdot s/ \monitoringperiod$ to catch several past and projected training runs, for Provers who own different quantities of chips $\totalchips$.
\begin{table}[tbh]
% \begin{tabular}{|p{4.5cm}|r|r|r|r|r|r|}
    \begin{tabular}{
    l
    % p{3.8cm}
    >{\raggedleft\arraybackslash}p{1.6cm}>{\raggedleft\arraybackslash}p{1.6cm}>{\raggedleft\arraybackslash}p{1.8cm}>{\raggedleft\arraybackslash}p{1.6cm}>{\raggedleft\arraybackslash}p{1.6cm}>{\raggedleft\arraybackslash} p{1.6cm}}
    % \begin{tabular}{|l|S[table-format=1.2e2]|S[table-format=1.2e2]|l[table-format=1.2e2]|r|S[table-format=1.2e2]|}
    % \hline
    % \textbf{Model} & \textbf{Training FLOPs $\rulecomputeflops$} & \textbf{H100-days $\computeflops/a$} & \textbf{H100s to train in 3mo} & \textbf{Frac global chips sampled/year} & \textbf{Samples/year (out of 5M)}\\
    % \hline
    \\
\textbf{Model}  & \textbf{Training FLOPs $\rulecomputeflops$}  & \textbf{H100-days $\rulecomputeflops/a$} & \textbf{H100s to train in 1 yr} & \textbf{Samples/yr if $C =10^3$} & \textbf{Samples/yr if $C =10^5$} & \textbf{Samples/yr if $C =10^7$} 
\\
\hline
    \\
GPT-3 \cite{BMR+20} & 3.14e+23 & 3.64e+3 & 10 & 243 & 2.43e+4 & 2.43e+6 \\
Chinchilla \cite{hoffman2022training} & 5.76e+23 & 6.67e+3 & 19 & 132 & 1.33e+4 & 1.33e+6 \\
PaLM \cite{chowdhery2022palm}& 2.56e+24 & 2.96e+4 & 82 & 29 & 2.98e+3 & 2.99e+5 \\
Chinchilla-280B $^\dag$  & 9.9e+24 & 1.15e+5 & 314 & 7 & 771 & 7.72e+4 \\
Chinchilla-1T $^\dag$ & 1.27e+26 & 1.47e+6 & 4.03e+3 & --- & 60 & 6.02e+3 \\
Chinchilla-10T $^\dag$ & 1.3e+28 & 1.5e+8 & 4.12e+5 & --- & --- & 58 \\
\\
    \end{tabular}
 %   \hspace{2cm}
 \caption{
 \small{
 Example numbers of required total \emph{annual} samples $365 \cdot s / \monitoringperiod$ to catch a chip from every large-scale training run within $T=30$ days, 
 given $a = 10^{15} \cdot 24 \cdot 3600$ (the daily 16-bit Tensor Core FLOPs of an NVIDIA H100 SXM GPU \cite{nvidiah100}), 
 $f = 0.1$ weight snapshots per day (see Section \ref{s.onchip}), 
 and the Verifier's desired probability of catching a rule-violating training run $p=0.9$.
 Models marked with $\dag$ are projections for future training requirements \cite{hoffman2022training}.
 }
 }
\label{tab:samples}
\end{table}
As the table shows, the number of inspected chips remains approximately at a fixed fraction of the full stockpile, decreasing with training run size $\rulecomputeflops$.

An additional difficulty is the possibility that multiple Provers collude and each contribute a small fraction of their chips to a shared training run, such that if only one of the Provers is audited, the fraction of their $\totalchips$ chips they used for the training run is much smaller than $\frac{\rulecomputeflops}{a\monitoringperiod\totalchips}$.
This is particularly relevant in the International Oversight setting, where a government Prover might force the participation of multiple firms.
One solution to this is for an international Verifier to do background monitoring of \emph{all} Provers, in effect treating them all as one large Prover with access to the global chip stockpile.
Predictably, this would require a much greater number of samples, and is likely to only be feasible for regulating the largest-scale training runs (which can be caught with fewer samples).
Yet, for training runs at the scale of $10^{25}$ FLOPs or greater, monitoring could be done with a bureaucracy similar in size to the IAEA.\footnote{
We want to estimate the number of inspectors needed to catch a Chinchilla-280B-sized training run, with $10^{25}$ FLOPs, given several more years of hardware progress and global production.
Given $\totalchips = 10^7$ worldwide chips ($>5\times$ global stocks as of 2022), each of which can output $a=3\cdot 10^{15} \cdot 86400$ FLOPs per day ($3\times$ more FLOP/s than the NVIDIA H100), detecting a Chinchilla-280B-sized run within $T=30$ days of its completion anywhere on earth with 90\% probability would require roughly 232,000 worldwide chip samples per year.
A single inspector might be expected to verify at least 1000 chips a year, especially if those chips are brought to a central location (see Section \ref{s.inspection}).
This would require $\approx232$ inspectors, slightly smaller than the 280 active IAEA inspectors as of 2021. 
 \cite{dayinthelife}.}

As individual chips become more powerful ($a$ grows) and algorithmic efficiency increases ($\rulecomputeflops$ shrinks), the number of required samples would need to grow in order to verify rules on a fixed-capability training run.
Still, it may be that the training runs of greatest societal concern are those \emph{near the frontier} of scaling, and those are likely to remain detectable with few samples for the foreseeable future, even if Provers attempt to undermine them with physical tampering.

% We include a table of how many chips would need to be sampled worldwide to catch different scales of training runs, given a near-future quantity of chips and hardware FLOPs.

% \begin{itemize}
%     \item \emph{Sending an inspector to the chip}. The Verifier can send an inspector to the facility in which a sampled chip is stored. 
%     \future{This may be more expensive for the Verifier, as it requires the inspector to travel to a new location, and also may undermine the facility's security guarantees (e.g., if it is otherwise airgapped). On the other hand, it may be cheaper if the same facility contains a large number of chips whose IDs were all randomly sampled.}
%     \item \emph{Sending the chip to an inspector}. ML chips are generally housed in easily-removable servers, and these servers can be removed and transported to a central location to be inspected by the Verifier.
%     This means that physical security (and even location secrecy) of the datacenter does not need to be undermined.
%     \future{
%     If there are any sensitive extraneous components in the server (e.g., persistent memory banks), they can be removed prior to transport.
%     \todo{Mention that this may force us to sample several GPUs at the same time?}
%     }
% \end{itemize}
