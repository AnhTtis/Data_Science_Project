\section{The Prover and Verifier's Cost-Benefit Analysis for Participating}
There are four factors that whether a method for enforcing AI rules will be viable:
\begin{itemize}
    \item How much cost can the Verifier inflict on the Prover, if the Prover refuses to comply with verification activities or is found to be in violation?
    \item How much does it cost to the Prover to prove compliance?
    \item How much does it cost to the Verifier to verify compliance, and how confident are they in their verification results?
    \item What rules can the framework enforce, and how valuable is their enforcement to the Verifier?
\end{itemize}
The Prover will participate if the costs of compliance are smaller than the costs the Verifier would otherwise impose.
The Verifier will participate if their benefit from the rules-to-be-enforced, times the probability that enforcement will work, is greater than the cost of verifying compliance.
Note that participation is not decided once, but continuously. Thus the costs and benefits of compliance must remain attractive to the Prover and Verifier over time.

\subsubsection{Costs to the Prover}
When a Prover is weighing whether to continue participating in an AI rule verification framework, 

\begin{itemize}
    \item Retains future flexibility for the Prover
    * In the AI case, we'd want to not only enable nearly all existing use-cases, but also to enable future technical innovations that change these use-cases
    \item Limited ongoing resource cost
    * In the AI case, this means no major compute cost overhead, increase in development complexity, and so on.
    \item Limited change from status quo (one-time costs)
    * In the AI case, this includes proposals that call for replacing all existing AI compute, or substantially retraining the workforce.
    \item Does not require the Prover to give up reasonable requirements to Privacy/confidentiality/security of their operations.
    * In the AI case, this includes revealing the model weights and training data, possibly even the hyperparameters, and also may involve limiting access to sensitive sites (like airgapped datacenters).
\end{itemize}

\subsubsection{Costs to Verifier}
Any trustless AI enforcement framework thus also requires:

\begin{itemize}
    \item Limited physical and digital enforcement costs for the Verifier.
    \item That the Prover cannot successfully violate the rule without being detected by the Verifier, even after an expected level of adversarial effort.
    \item That applying the framework does not have other significant costs on the Verifier's interests.
\end{itemize}

As an example, in late 2022 the US government apparently decided that it did not have a sufficiently-reliable way to detect whether the Chinese military was using Western firms' high-end datacenter chips (such as NVIDIA A100s) to build weapons, and thus decided to restrict these chips' sale to all of mainland China.

For the US government to consider softening its policies of selling AI-enabling chips to China in the future, it would need an improved way to verify compliance.

\subsubsection{Value of Rules}
Finally, the AI framework must enable the enforcement of \emph{specific AI rules} which are sufficiently valuable to the Verifier, and acceptable to the Prover.
We describe the set of rules we seek to enforce in Section \todo{INSERT}.


In this work, we will lay out our progress towards a technical framework that fulfills all the these criteria, with a particular eye towards enabling government regulators to verify foreign governments'/companies' compliance with agreed AI rules.

We assess the extent to which the current proposal meets each of these criteria in Section \todo{INSERT}.

\todo{Instead of introducing the compute-type assumption here, introduce it more casually as a "we can only regulate this sort of system; we will leave as an assumption that imposing rules on such systems is sufficiently valuable, and discuss it more in an Appendix."}

\subsubsection{Prover-Verifier Examples and Associated Punitive Mechanisms}

\todo{deduplicate section}
Any effective Prover-Verifier relationship rests on the Verifier's ability to punish the Prover in case the Prover refuses to comply.
This punishment must be painful enough to the Prover that they would prefer to appear to comply with the Verifier, than to violate the rule, be caught, and suffer the punishment.
There are three classes of real-world Prover-Verifier relationships of this sort:
\begin{itemize}
\item \emph{Government as Verifier, Company as Prover}:
    Governments have a clear interest that the AI systems developed by companies operating within their borders comply with certain rules.
    They possess the ability to level both civil and criminal penalties on any Prover caught violating these rules, and have dedicated organizations seeking to enforce compliance (regulators and law-enforcement agencies).
    Note that even though the Prover's illegal activity may still occur within another country's borders, the Verifier may still seek to punish the company if its actions harm its country.
    However, effective detection and punishment may require the cooperation of the second country.
    \emph{Real-world parallel}: financial regulators requiring investment firms to track and report investment activity, for the purposes of detecting fraud.
\item \emph{Corporate provider as Verifier, customer as Prover}:
    Providers of AI development resources (e.g. cloud providers) may want to contractually require that customers not use their products to violate rules.
    The rules are national laws, and if the provider enables legal violations they themselves may be considered culpable. (Real-world parallel: cloud providers detecting servers used for cybercrime.)
    Alternatively, the rules may simply be policies that the Provider considers to be in their interest.
    (Real-world parallel: cloud providers refusing to host neo-Nazi websites.)
    In either case, the Verifier may threaten to deny the Prover future access unless they comply.
\item \emph{Government as Verifier, government as Prover}:
    The most significant types of AI rule may be those enforced between countries as part of international agreements.
    These include verifying arms control agreements (e.g. on the development of autonomous AI cyberweapons), as well as enforcing other mutually-beneficial rules (e.g. requirements for limiting the spread of harmful chemicals into the atmosphere).
    Such agreements may also apply to companies operating within each country.
    International agreements seem likely to be particularly important because AI is principally a digital technology: dangerous AI tools can be developed and deployed in one country and yet be used within the border of another, by simply using the internet.
    Thus, effectively preventing certain AI use-cases in any one country may require international cooperation (similar to the global nature of policing child sexual abuse material).
    
    Importantly, unlike in the government-company scenario, there may be limits to the penalties that governments can impose on each other (whether economic or military).
    A country complies with arms control agreements and inspections out of self-interest: it myopically prefers to maintain an equilibrium in which both it and its competitors deny themselves access to certain arms (e.g. Iran and Saudi Arabia both choosing not to acquire nukes), rather than defect and be forced into a more dangerous equilibrium.
    Thus, the Verifier country's threatened ``punishment'' may be that they too would defect and develop dangerous AI arms.
    (Real-world parallel: the international nuclear nonproliferation regime; US-Russia ballistic missile limitation agreements.)
\end{itemize}

