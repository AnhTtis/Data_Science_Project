\section{Logistics for Confidentially Verifying Proofs-of-Training-Transcript}
\label{a.trustedcluster}

In this section, we consider the logistical problem of how to verify, for all the Verifier's observed weight-snapshots from sampled chips, that the Prover has provided authentic training transcript that match each weight-shard. We assume the PoTT verification protocol looks similar to the  Proof-of-Learning scheme of \cite{jia2021proof}.
Our solution is specifically designed to avoid the Prover ever having to directly reveal the hyperparameters $\mathbb{M}$, training data $\mathbb{D}$, and model weights $\mathbb{W}$ to the Verifier, as in many cases these data are either private or proprietary.

First, when executing each training run, the Prover follows any required\footnote{Of course, these restrictions must be retroactively verifiable using the training transcript, as otherwise the Prover might simply not comply.}
restrictions on their training (e.g., starting from a subset of valid initializations, or precommitting to the model weights).
Throughout training, the Prover saves the hyperparameters (e.g., training code), the sequence of training data batches, and periodic snapshots of the model weights.
They also make sure to track any randomness used, and generally make sure their training process is repeatable (up to low-level noise) assuming access to similar but non-identical hardware configurations.
This itself is a nontrivial technical challenge \cite{liu2021reproducibility}, especially in co-training settings like GANs or RL on learned reward models, and would benefit from further investments in replication tools for common ML frameworks.

Periodically, the Prover reports their chips' on-chip logged weight-shard snapshot hashes, along with hashed versions of the Proofs-of-Training-Transcript matching each of these logged hashes (including hashes of $\mathbb{M}$, hashes of each model snapshot $\mathbb{W}$, and sequences of hashes of training data points $\mathbb{D}$).
The Prover may need to further disclose basic information like the distance between weight-snapshot pairs, which may be needed for the Verifier to determine which regions of the transcript to verify.\footnote{This info could be proven to the Verifier securely and privately, for example by using standard ZK-SNARK proof tools to confirm that two given hashes correspond to vectors that have a given $L_2$ distance between them.}

As in Section \ref{s.sampling}, the Verifier chooses a subset of these chips whose logs will be confirmed via physical inspection.
The Verifier inspects the training transcripts and associated metadata, and chooses the subsets of the training transcripts to verify.
They also select any other components of the training transcript (e.g., a random sample of the training data) that they will need in order to determine that that the training run has complied with the ML training rules.
At this point, the Verifier has a set of hashes of the inputs to its verification protocol (including model weight snapshots and training data points), but does not have access to any of this information directly.

The Verifier needs to have faith in the results of the computation, but without seeing the inputs themselves.
In principle, this might be addressed by cryptographic techniques like zero-knowledge proofs, but in practice these techniques cannot efficiently execute computationally intensive programs, like long sequences of gradient updates on billion-parameter models.
Instead, the Prover and Verifier can agree on a neutral jointly-trusted cluster, which they will use to execute the verification protocol.
The Verifier needs to be able to trust in the integrity of the cluster's computation.
The Prover, conversely, needs to trust that the cluster will not reveal its private data, by verifying that it cannot communicate with the outside world and has no persistent storage.
Such neutral clusters could be maintained by a trusted third party, or when no such trusted party exists, could be jointly overseen by both the Prover and Verifier. 
It may be useful to create one trusted cluster for each type of ML training chip, in order to mitigate potential hardware compatibility issues.

At agreed-upon intervals, the Verifier supplies the hashes of the inputs (including hyperparameters $\mathbb{M}$) to the neutral cluster, and the Prover supplies the inputs themselves.
The cluster hashes the Prover's inputs and confirms they match the hashes.
Then the cluster executes the Verifier's verification protocol.
For example, when re-executing a training segment to verify that it would reach a particular chip-logged weight shard snapshot $\omega_{i+k}$, the cluster starts at one weight snapshot $W_i$, and then computes optimizer updates $k$ times using the specified inputs (where each update is computed via code generated from the hyperparameters, and even code snippets, defined by $\mathbb{M}$).
Finally, the cluster checks that the resulting weight vector $W'_{i+k}$ has a slice $\omega'_{i+k}$ that is within an $\epsilon$ distance of the chip-logged weight shard $\omega_{i+k}$.
If all the verification protocols passed, the cluster outputs that the Prover has ``passed''.
If not, the cluster outputs that the Prover ``failed'', prompting an investigation.

Assuming the training transcript is verified as correct, the Verifier can now compute any functions of the training transcript that would determine its compliance with agreed rules.
This could include properties of its training data distribution (which can be established from a randomly-selected subset of the training transcript's data), the performance of the final model on specific benchmarks, and properties of the hyperparameters.
Assuming the results confirm the Prover's compliance, the Verifier could be certain that with probability at least $1 - p - \delta_2$, the Prover has not used ML chips to execute a training runs using greater than $\rulecomputeflops$ FLOPs which violated the agreed upon rules.
The Prover has also not disclosed any sensitive information to the Verifier, including training data, model weights, or hyperparameters.
