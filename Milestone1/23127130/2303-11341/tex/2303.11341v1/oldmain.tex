\documentclass[manuscript,screen,review]{acmart}
\setcopyright{none}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[FAccT '23]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection, June 03--05, 2018, Woodstock, NY} 
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


\usepackage{xcolor}
\usepackage{siunitx}


\definecolor{DarkGreen}{rgb}{0.1,0.5,0.1}
\newcommand{\todo}[1]{\textcolor{DarkGreen}{[TODO: #1]\\}}
\newcommand{\rc}[1]{\textcolor{red}{[Rachel: #1]}}
\newcommand{\ys}[1]{\textcolor{blue}{[Yonadav: #1]}}
% \newcommand{\copied}[1]{\textcolor{orange}{[Copied: #1]}}
\newcommand{\copied}[1]{\textcolor{orange}{[Copied: ]}}

% \newcommand{\ot}[1]{\textcolor{purple}{[Outline: #1]}}
\newcommand{\ot}[1]{\textcolor{purple}{[Outline: ]}}

\newcommand{\assumption}[1]{\textcolor{red}{ASSUMPTION: #1 \\}}
\newcommand{\hparams}{q}
\newcommand{\computehours}{H}
\newcommand{\monitoringperiod}{T}
\newcommand{\totalchips}{C}
% \input{commands.tex}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}


\title{Don't do bad things with AI, bastards}


\author{Yonadav Shavit}
\email{trovato@corporation.com}
\affiliation{%
  \institution{Harvard University}
  \country{USA}
}
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Shavit}

\begin{abstract}
 
\end{abstract}


\maketitle


\section{Introduction}

\todo{Use the language of "enable AI developers to prove limits on their training runs". This is more literally what IAEA enables others to do. It's also something labs are happier to do in general, and involves more of a unilateral each-lab thing, rather than a regulatory top-down thing.}


Rules on AI have a major problem: we do not know how to enforce rules on AI without trusting AI developers at their word.

Sometimes, this trust cannot be assumed: when the company has clear counterinterests, when it's a criminal enterprise, or in the case of verifying the conduct of other countries.

One might question whether there are truly AI rules that *must* be trustlessly enforced.
After all, we hardly enforce any existing AI rules, and while these systems have caused harms, they are policed largely through the existing system of law enforcement has methods to detect those harms. (For example, US companies selling AI products generally comply with subpoenas and FTC requests for information, even when those inquiries expose wrongdoing \todo{Cite some stuff}.)
Yet, there is good reason to believe that the coming wave of AI-enabled technologies contains capabilities that are sufficiently attractive to build, and yet sufficiently harmful to society, that unscrupulous parties would rather lie to the legal system than stop entirely.
We need trustless mechanisms for enforcing compliance on such AI systems' development and use.

As a motivating example, take RL systems designed to autonomously generate and execute code, which are then connected to the internet and can execute code without close human supervision.
If the reader's reaction is to yell ``why would anyone do that???'', then they are urged to remember that there are clear profit motives to deploy such a system, and there are many, many software developers in the world, at least a few of which are less responsible than themselves.

The dangers from such systems are substantial, especially considering that current language models will generate unexpected outputs unless heavily tested and tweaked.
If, by negligence or malice, the AI system model has learned to write custom software vulnerabilities, it may hack into new computer systems and modify those systems' behavior.
If such RL systems are capable of identifying new servers on which to run, they may be able to copy themselves and thus ``self-replicate''. Like a classical computer worm, this would make their spread extremely hard to trace.
The worst case would be if such systems could self-modify (e.g. by writing a copy of their own source code that excluded certain safety checks, and then running that copy on a second server), and thus cause harm far beyond their original designer's intention.
Such capable code-generating RL systems do not exist at the time of writing.
Still, given the substantial recent progress in code-generation and reinforcement learning, it is prudent to prepare for a world in which such AI models are viable.

While it may seem unlikely that any sane person would build and deploy such a code-generating RL system without thoroughly testing it to remove such behaviors, the history of software development is littered with undiscovered bugs that cause major problems.
Worse, in contexts of competition (between rival companies or militaries), AI developers face substantial pressure \emph{against} sufficient testing and validation, so as not to ``fall behind''.
Even when both parties would prefer to individually do more testing and risk-mitigation, they may have no way to verify whether the other party is complying, leading both parties to defect in a ``prisoner's dilemma''.

Governments have a clear interest in regulating code-executing RL agents, and any other AI technology which threatens substantial harm to the public.
Yet at the same time, AI technologies enable substantial benefits for society, and the inputs required to develop beneficial AI systems and harmful AI systems are almost exactly the same.

Indeed, the there is a real risk that the government, in reasonably seeking to curtail harmful use-cases, may substantially limit many beneficial uses of the technology. Tis has already happened with \todo{mention china chip ban}.

\todo{Also mention that any attempts by any one entity to control all AI compute without accountability should be met with skepticism, considering that all accumulations of AI-enabling resources would represent a dangerous and possibly-totalitarian concentration of power. Instead, we should seek a system of mutual verification between parties.}

How might we nonetheless hope to enforce rules on AI systems, without requiring trust?
What could we do?

The nascent nuclear industry found itself in a similar situation in the 1950s: paralleling large-scale AI training, a small amount of nuclear centrifuge use resulted in reactor-grade uranium, while a large amount resulted in highly-enriched ``weapons-grade'' uranium.
In response, the nations of the world implemented the international Nonproliferation Treaty, which has limited the spread by blah blah and enabled blah blah nuclear power.
\todo{Talk about }


\ot{Lay out motivation}

\ot{Make sure that the framing is based on "if there are some powerful AI chips which we decide to regulate, how would we govern their usage?" and then place as an assumption that there exist a set of relevant training chips.}

\copied{
1. Rules on AI have a major problem: we do not know how to enforce rules on AI without trusting AI developers at their word.
    1. Laws prescribing what you *cannot* do with ML systems are nearly impossible to enforce, because AI training and deployment are computer programs, which can be run by anyone with the necessary computers. Given that there is no trace of which program was run on a machine, anyone violating the law can simply claim they “didn’t”, and there is no physical trace to the contrary.
    2. This means that the law relies on voluntary self-reporting of compliance. This works for large companies in areas with strong rule of law, and for public-facing systems (like computer vision products), but it is much less effective at covering internal-facing tools, or active criminality (including by dedicated criminal organizations), or the actions of other states.
    3. Thus, the only laws we realistically think about are those on public-facing AI products, rather than the many non-public yet more dangerous use-cases (e.g. AI cyberweapons). 
    4. This inability to enforce has had limited impact thus far because the reach of non-public-facing AI systems has been limited. But as these systems become more capable, and their use-cases become more dangerous, it will be vital that society can prevent the most dangerous misuses of AI systems.
2. Because governments have no way to verify compliance with laws or agreements, they may be forced to rely on draconian measures.
    1. For example, the USG, believing that China was using AI for military development and human rights violations, blanket-banned the sale of chips from China entirely.
    2. This is a massively-costly economic policy that also blocks the many good use-cases of AI from Chinese firms, and costs US chipmakers a lot of sales.
    3. However, it is understandable that the US views it as necessary, because the US has no other way of verifying compliance with agreements with the Chinese on which use-cases are acceptable.
    4. An alternative would be requiring complete transparency of how the chips were being used, but this would be unacceptable for data privacy, intellectual property, and industrial secrecy reasons. [IS THERE A REAL-WORLD EXAMPLE?]
        1. AI development is also borderless - even if an extreme degree of surveillance exists in one country, a criminal can just run the same operations in another country, and transmit the results over the internet.
3. We need tools for enforcing rules on AI development that don’t ban AI entirely and let the rest of society continue to reap the technology’s benefits, but that also don’t involve draconian surveillance.
4. An immediate answer: that’s impossible.
}
\copied{
How can we limit the ability of rogue actors to build advanced AI systems?
Let us consider the model of Brundage \cite{brundage2022} that AI requires four inputs: algorithms, data, human talent, and computing resources (henceforth referred to as ``compute'').

Given the general promise of AI systems to improve human life, and their substantial short-term economic utility, halting the continued progress of public AI research may not be tractable or desirable.
Data, too, is abundant and gathering more is cheap, making it nearly impossible to limit.
While talented AI experts are currently scarce, it is likely that as the economic significance of AI deployment increases, the availability of knowledgeable engineering talent will similarly spread.

The one potentially-persistent bottleneck to building advanced AI systems is access to massive quantities of compute.
As Brundage highlights, compute's ``rivalrousness, excludability, and quantifiability make it a promising intervention point for AI governance''.
Its production is highly concentrated: as of 2022, all cutting edge MPUs (micro-processing units, like CPUs and GPUs, using process nodes of 20nm or better) were manufacturable in only 29 fabrication plants in the US, Taiwan, South Korea, China, Israel, and Ireland \cite{wiki:List_of_semiconductor_fabrication_plants}.
Building a new modern fab costs many billions of dollars, and requires access to a massive international supply chain.
As to where these chips end up, while many of us do have high-end CPUs in our pockets, the massive energy and bandwidth requirements of training large AI models on huge quantities of compute mean that large-scale AI compute has thus far primarily been located in data-centers.
(We will explore whether this will remain the case in more detail in Section \ref{sec:identifying}.)

In this sense, high-performance compute bears remarkable similarity to enriched uranium (the crucial input into both nuclear energy and nuclear weapons).

The 1970 Treaty on the Non-Proliferation of Nuclear Weapons, implemented by the International Atomic Energy Agency (IAEA), has succeeded for the last 50 years at preventing the proliferation of nuclear weapons and maintaining verification-based trust between otherwise-opposed states.
The IAEA framework is based on two principles: monitoring the transportation of nuclear material, and inspecting physical ``chokepoints'', especially uranium enrichment facilities and nuclear power plants.

In this work, we propose a system that adapts the IAEA framework to the challenge of overseeing datacenter compute, by outlining a scalable procedure for datacenter compute inspection and verification.}
\copied{

5. Is this possible? Draw parallels to nuclear fuel enrichment, and highlight how it was very successfully solved
    1. The oversight of uranium enrichment enshrined in the NPT has been remarkably successful at preventing the proliferation of nuclear weapons for over 50 years, while enabling the spread of nuclear energy to [many countries].
        1. By creating a light-touch verification framework, countries have successfully kept a balance for decades. While only a few countries have gained nukes since the NPT, ___ countries run enrichment facilities, and 32 countries have nuclear power purchased from these folks. (Even more places have nuclear research reactors.)
            1. Enrichers: Argentina, Brazil, China, France, Germany, India, Iran, Japan, the Netherlands, North Korea, Pakistan, Russia, the United Kingdom, and the United States
    2. Uranium enrichment bears remarkable similarity to the training of large-scale ML models.
    3. Like uranium enrichment, which requires the use of special centrifuges to progressively enrich U-235 into ever-higher purities, large-scale ML training requires the use of large quantities of specialized chips run for weeks or months.
    4. Like uranium enrichment, the longer AI models are trained for, the more the results are dangerous and requiring of regulation.
    5. Is it possible to port the international system for monitoring uranium enrichment to the domain of AI?
    6. “More details in [nuclear parallels section]”
6. This paper will propose a method for enforcing regulations on the large the nuclear verification framework to tackle the challenges of AI training monitoring.
    1. Need to highlight more explicitly why AI is bottlenecked on compute, and why compute is a governable resource.
    2. The focus is on large *training* runs, since these require large-scale resources, and then assumes that their subsequent use can be regulated once such activity is detected. This doesn’t cover inference (how models are subsequently used), but monitoring training does allow us to curtail the most dangerous types of misuse. For a full description, see [NEXT SECTION].
    3. We will propose a system by which organizations can prove to governments/peers the length of their longest training runs, along with final model weights and the dataset used for training.
    4. In practice, this will describe
    5. Algorithmically, we reduce our problem to solving a known problem in the literature: PoL
    7. Importantly, this should not infringe on any individuals’ rights to their own personal compute, and should not require AI-developers to divulge trade secrets (e.g. model weights, training data). In particular, it should be light-touch enough to not create a strong incentive for actors to evade it when following the rules.}

\copied{The core innovation of this framework is that by randomly sampling even a small number of MPUs from each datacenter, we can with high probability detect and characterize any datacenter-scale computing jobs. 
This is because datacenter compute is modular and homogeneous, and because a sizeable fraction of the chips in a datacenter would need to be used in training advanced AI systems.
Checking every one of the millions of chips in a datacenter would be hopelessly complex and extremely invasive.
But small-scale random sampling is comparatively easy. 

To uniformly sample from all global compute that could be being used to develop advanced AI systems, we need to:
\begin{itemize}
    \item Know of every large cluster (either physical or digital) of compute,
    \item Have an accurate catalog of the IDs of every chip within each cluster, and
    \item Be able to efficiently inspect a random sample of the catalog's corresponding chips.
    \item Be able to determine whether a randomly sampled chip was part of an illegal training run or not, by having that chip provably keep logs, s.t. those logs could only attest to legal behavior if no illegal activity occurred.
\end{itemize}


}

\copied{One key requirement for this framework is that it not become a global surveillance apparatus. Any monitoring done to chips must not disclose any sensitive information (like training data or proprietary algorithmic innovations). Moreover, the enforcement of the proposal should be minimally invasive, in order to get rival companies or even rival militaries to do mutual inspections.}

\copied{

    8. The focus of this proposal is only to use existing technologies, and not to rely on future breakthroughs. Furthermore, to have hope of this actually happening, we are focused on interventions where the relevant stakeholders must have a self-interest in complying and enforcing required compliance. Beyond the many uncertainties we highlight, the key required ingredient is organizational will.
7. Purpose of this paper
    1. We are describing an end-goal governance framework, where cheating is basically impossible. As with all legal enforcement, it in practice makes sense to start with partial solutions, which will catch many less-sophisticated violators. We describe these intermediate measures in section [INSERT].
    2. This is a necessarily wide-ranging and ambitious proposal, and will necessarily make assumptions, including about future technological trajectories. Some of these are likely to be wrong, in which case parts of the framework may need to be reworked. We explicitly highlight these to encourage productive discussion. It is our belief that if all these assumptions hold, this framework can tractably be implemented.
    3. We will start with two assumptions:
        1. The compute required to train AI models is sufficiently large to be detectable, and this will remain true even after substantial algorithmic innovation.
            1. Cite the bitter lesson, scaling laws papers, etc.
            2. On the flipside, cite potential challenges to scaling laws, including data-efficiency and algorithmic innovation (cite Tamay’s paper here)
        2. AI training will require specific types of chips, both with high interconnect and high flop-count.
            1. I.e. can’t daisy-chain xboxes
            2. This is important because this restricts this framework to large-scale industrial supply-chains/operations, and doesn’t affect ordinary consumers.
            3. These are the chips already targeted by US regulations.
        3. One thing we *aren’t* assuming here: that these chips must be collocated. For example, multiple datacenters working together, as in GPT-JT. This framework should still work in this case.

}

\section{Framing the problem}
We will focus on the setting in which there is one party which wants to verify that an AI rule is being followed (the ``Verifier''), and another party which is developing the AI system and wants to prove to the Verifier that it is complying with the AI rule (the ``Prover'').
However, the Prover may benefit from violating the AI rule, and thus wants to violate the AI rule \emph{if they can still appear compliant} to the Verifier.
Thus, the problem: what minimal set of verifiable actions can the Verifier require the Prover to take, such that the Verifier can detect any rule violation by the Prover with high probability?

There are three classes of real-world Prover-Verifier relationships, which we will discuss here:

\begin{itemize}
\item \emph{Government as Verifier, Company as Prover}:
    Governments have a clear interest that the AI systems developed by companies operating within their borders comply with certain rules.
    They possess the ability to level both civil and criminal penalties on any Prover caught violating these rules, and have dedicated organizations seeking to enforce compliance (regulators and law-enforcement agencies).
    Note that even though the Prover's illegal activity may still occur within another country's borders, the Verifier may still seek to punish the company if its actions harm its country.
    However, effective detection and punishment may require the cooperation of the second country.
    \emph{Real-world parallel}: financial regulators requiring investment firms to track and report investment activity, for the purposes of detecting fraud.
\item \emph{Corporate provider as Verifier, customer as Prover}:
    Providers of AI development resources (e.g. cloud providers) may want to contractually require that customers not use their products to violate rules.
    The rules are national laws, and if the provider enables legal violations they themselves may be considered culpable. (Real-world parallel: cloud providers detecting servers used for cybercrime.)
    Alternatively, the rules may simply be policies that the Provider considers to be in their interest.
    (Real-world parallel: cloud providers refusing to host neo-Nazi websites.)
    In either case, the Verifier may threaten to deny the Prover future access unless they comply.
    \todo{Add setting of inter-lab industry best practices, e.g. as part of a "responsible AI" consortium}

\item \emph{Government as Verifier, government as Prover}:
    The most significant types of AI rule may be those enforced between countries as part of international agreements.
    These include verifying arms control agreements (e.g. on the development of autonomous AI cyberweapons), as well as enforcing other mutually-beneficial rules (e.g. requirements for limiting the spread of harmful chemicals into the atmosphere).
    Such agreements may also apply to companies operating within each country.
    International agreements seem likely to be particularly important because AI is principally a digital technology: dangerous AI tools can be developed and deployed in one country and yet be used within the border of another, by simply using the internet.
    Thus, effectively preventing certain AI use-cases in any one country may require international cooperation (similar to the global nature of policing child sexual abuse material).
    
    Importantly, unlike in the government-company scenario, there may be limits to the penalties that governments can impose on each other (whether economic or military).
    A country complies with arms control agreements and inspections out of self-interest: it myopically prefers to maintain an equilibrium in which both it and its competitors deny themselves access to certain arms (e.g. Iran and Saudi Arabia both choosing not to acquire nukes), rather than defect and be forced into a more dangerous equilibrium.
    Thus, the Verifier country's threatened ``punishment'' may be that they too would defect and develop dangerous AI arms.
    (Real-world parallel: the international nuclear nonproliferation regime; US-Russia ballistic missile limitation agreements.)
\end{itemize}

We focus on the enforcement of rules on the development (\emph{training}) of deep-learning models 
\todo{Go through and change many "AI" references to deep learning}
, rather than on the deployment (\emph{inference}) of trained models.
Running a trained ML model is relatively cheap, and may be done using hardware found in consumer devices (e.g. gaming consoles).
Even limited fine-tuning of a pretrained model can be done on commodity hardware given sufficient time.
Surveilling and policing individual citizens' use of their personal computers would be highly unacceptable on ethical grounds. Enforceable rules on AI must focus elsewhere.

However, the training of large models via gradient descent is currently a major resource-intensive undertaking, requiring hundreds or thousands of specialized accelerators with high inter-chip communication bandwidth \todo{list examples: (e.g. Google TPU, NVIDIA A100, AMD ...)}, often operating for weeks or months \todo{cite Epoch}.
This means that training runs both leave more traces for Verifiers to detect, and that they are generally undertaken by sizable organizations more able to comply with Prover obligations.
\footnote{Even academics will generally place their compute in a datacenter, which can then serve as the Prover organization.}

The major assumption underlying this framework is that the current status quo, that the training of dangerous ML models requires the use of large quantities of specialized hardware for extended periods, continues into the future.
We discuss arguments for and against this proposition in Section \todo{add discussion of why training may remain intensive.}
The more important constraint is the limitation to specialized hardware, like those targeted by the US Department of Commerce's 2022 rule. \todo{Cite more specifics on this.}
While over time algorithmic progress and Moore's law reliably drive down the number of chip-hours required to train a fixed capability \todo{cite epoch}, we can gradually increase enforcement parameters to work for smaller quantities of specialized compute.

Let us formalize our setting.
An AI training run is defined by a set of hyperparameters $\hparams$ and a sequence of datapoints $\datasequence$, such that computing the final model weights requires $\computehours$ compute-hours using chips with a given efficiency.
The Verifier wants to determine whether the Prover has executed a training run requiring at least $\computehours > \rulecomputehours$ compute-time within the last $\totalmonitoringperiods$ monitoring periods, each of length $\monitoringperiod$.
They would also like to learn properties of the data sequence, hyperparameters, and resulting model weights $\modelweights$ of any such run.

For the first $\rulecomputehours$ chip-hours of the run, the training is completely ``legal''.
However, if the Prover is attempting to violate the rule, they must do disallowed training for at least $\computehours - \rulecomputehours$.

\todo{We would like to detect any disallowed training period proportional to how long that period lasted.
The literature on neural scaling laws (either "exponential" or "power-law") suggests that models improve logarithmically with additional compute.
Thus, small-scale additional training may be of limited consequence.
For notational convenience, we will aim to detect the case where the Prover violates the rule by $\computehours > 2\rulecomputehours$, meaning that they do at least $\computehours$ ``disallowed'' training.}

\todo{Change every mention of ``disallowed'' to ``required-to-be-monitored'' or something similar.}

\subsection{What types of rules can we enforce via training?}

What desirable rules could a Verifier enforce solely by monitoring the large-scale \emph{training process} of an AI system, even if it could not detect small-scale retroactive modifications like fine-tuning?

Let us return to \todo{make sure we actually mention this earlier, or alternatively move this discussion to the beginning} the case of code-generating-and-executing RL agents.

\ot{Use, as a running example, "code-generating+executing RL agents trained with >X compute". This is plausibly an example that we can detect, because its dataset has a certain signature, its training algorithm has a certain signature, and its training length/loss has a certain signature.}

RESUME HERE

What sort of rules on model training might limit their harmful consequences?

\begin{itemize}
\item 
        1. Tracking: Any model trained with more than X compute must be reported, along with a model card to a central regulator
        2. Bans or approval-requirements: any model trained with more than X compute must be approved, and the trainer must comply with additional obligations (e.g. inspections, red-teaming, reporting requirements) enforced via human means. 
        3. Regulations on inference-sales: if there’s a way to require that large-scale inference must be done with signed weights (e.g. required for access to cloud-scale inference resources), then can require that the weights be the result of such a training run
        4. Require the developer to prove they have built a version of the model with risk-mitigating postprocessing, at least once. If a developer would need to pay a large extra cost to postprocess a model they train to make it safer, there’s a risk they won't do so to save costs. A Verifier could require that, for any heavily-trained AI model that a Prover constructs, they must also construct a postprocessed version of that model. This doesn’t rule out the possibility that the Prover would nonetheless deploy the not-postprocessed version, but the assumption would be that the postprocessing did make the postprocessed system at-least-slightly more desirable to the Prover.
        5. loss levels, hyperparams
        6. Requirements on training data (e.g. including or excluding more than X samples of a certain type of data)
\end{itemize}

Risk mitigation tax:

\ot{Comment that this format, of requiring the reporting of logs and then having a regulator verify those logs, is the standard way to regulate many industries (e.g. accounting in finance).}


As we will discuss in Section \ref{sec:politicalrequirements}, any viable framework must limit the compliance costs imposed on the Prover and Verifier.

\subsection{Abridged additional political requirenents}
mention privacy, secrecy, including of datacenter, efficiency (small fraction of total compute done), and high degrees of certainty.
also mention that it should be resilient to ~arbitrary attacks (eg Chinese mil) and should last for several years.
also that it doesnt affect consumers, and also that it continues to allow stabdard ML jobs.
and should require well-tested tech (eg cant rely on tamper-proofing).
also as much of enfircement should be automwted as possible

\subsection{Requirements for a politically-viable rule-enforcement framework}
\label{sec:politicalrequirements}

\todo{Move the cost benefit analysis to the Appendix, and mention it's being done somewhere in the main body.}

There are four factors that whether a method for enforcing AI rules will be viable:
\begin{itemize}
    \item How much cost can the Verifier inflict on the Prover, if the Prover refuses to comply with verification activities or is found to be in violation?
    \item How much does it cost to the Prover to prove compliance?
    \item How much does it cost to the Verifier to verify compliance, and how confident are they in their verification results?
    \item What rules can the framework enforce, and how valuable is their enforcement to the Verifier?
\end{itemize}
The Prover will participate if the costs of compliance are smaller than the costs the Verifier would otherwise impose.
The Verifier will participate if their benefit from the rules-to-be-enforced, times the probability that enforcement will work, is greater than the cost of verifying compliance.
Note that participation is not decided once, but continuously. Thus the costs and benefits of compliance must remain attractive to the Prover and Verifier over time.

\subsubsection{Prover-Verifier Examples and Associated Punitive Mechanisms}

\todo{deduplicate section}
Any effective Prover-Verifier relationship rests on the Verifier's ability to punish the Prover in case the Prover refuses to comply.
This punishment must be painful enough to the Prover that they would prefer to appear to comply with the Verifier, than to violate the rule, be caught, and suffer the punishment.
There are three classes of real-world Prover-Verifier relationships of this sort:
\begin{itemize}
\item \emph{Government as Verifier, Company as Prover}:
    Governments have a clear interest that the AI systems developed by companies operating within their borders comply with certain rules.
    They possess the ability to level both civil and criminal penalties on any Prover caught violating these rules, and have dedicated organizations seeking to enforce compliance (regulators and law-enforcement agencies).
    Note that even though the Prover's illegal activity may still occur within another country's borders, the Verifier may still seek to punish the company if its actions harm its country.
    However, effective detection and punishment may require the cooperation of the second country.
    \emph{Real-world parallel}: financial regulators requiring investment firms to track and report investment activity, for the purposes of detecting fraud.
\item \emph{Corporate provider as Verifier, customer as Prover}:
    Providers of AI development resources (e.g. cloud providers) may want to contractually require that customers not use their products to violate rules.
    The rules are national laws, and if the provider enables legal violations they themselves may be considered culpable. (Real-world parallel: cloud providers detecting servers used for cybercrime.)
    Alternatively, the rules may simply be policies that the Provider considers to be in their interest.
    (Real-world parallel: cloud providers refusing to host neo-Nazi websites.)
    In either case, the Verifier may threaten to deny the Prover future access unless they comply.
\item \emph{Government as Verifier, government as Prover}:
    The most significant types of AI rule may be those enforced between countries as part of international agreements.
    These include verifying arms control agreements (e.g. on the development of autonomous AI cyberweapons), as well as enforcing other mutually-beneficial rules (e.g. requirements for limiting the spread of harmful chemicals into the atmosphere).
    Such agreements may also apply to companies operating within each country.
    International agreements seem likely to be particularly important because AI is principally a digital technology: dangerous AI tools can be developed and deployed in one country and yet be used within the border of another, by simply using the internet.
    Thus, effectively preventing certain AI use-cases in any one country may require international cooperation (similar to the global nature of policing child sexual abuse material).
    
    Importantly, unlike in the government-company scenario, there may be limits to the penalties that governments can impose on each other (whether economic or military).
    A country complies with arms control agreements and inspections out of self-interest: it myopically prefers to maintain an equilibrium in which both it and its competitors deny themselves access to certain arms (e.g. Iran and Saudi Arabia both choosing not to acquire nukes), rather than defect and be forced into a more dangerous equilibrium.
    Thus, the Verifier country's threatened ``punishment'' may be that they too would defect and develop dangerous AI arms.
    (Real-world parallel: the international nuclear nonproliferation regime; US-Russia ballistic missile limitation agreements.)
\end{itemize}

\subsubsection{Costs to the Prover}
When a Prover is weighing whether to continue participating in an AI rule verification framework, 

\begin{itemize}
    \item Retains future flexibility for the Prover
    * In the AI case, we'd want to not only enable nearly all existing use-cases, but also to enable future technical innovations that change these use-cases
    \item Limited ongoing resource cost
    * In the AI case, this means no major compute cost overhead, increase in development complexity, and so on.
    \item Limited change from status quo (one-time costs)
    * In the AI case, this includes proposals that call for replacing all existing AI compute, or substantially retraining the workforce.
    \item Does not require the Prover to give up reasonable requirements to Privacy/confidentiality/security of their operations.
    * In the AI case, this includes revealing the model weights and training data, possibly even the hyperparameters, and also may involve limiting access to sensitive sites (like airgapped datacenters).
\end{itemize}

\subsubsection{Costs to Verifier}
Any trustless AI enforcement framework thus also requires:

\begin{itemize}
    \item Limited physical and digital enforcement costs for the Verifier.
    \item That the Prover cannot successfully violate the rule without being detected by the Verifier, even after an expected level of adversarial effort.
    \item That applying the framework does not have other significant costs on the Verifier's interests.
\end{itemize}

As an example, in late 2022 the US government apparently decided that it did not have a sufficiently-reliable way to detect whether the Chinese military was using Western firms' high-end datacenter chips (such as NVIDIA A100s) to build weapons, and thus decided to restrict these chips' sale to all of mainland China.

For the US government to consider softening its policies of selling AI-enabling chips to China in the future, it would need an improved way to verify compliance.

\subsubsection{Value of Rules}
Finally, the AI framework must enable the enforcement of \emph{specific AI rules} which are sufficiently valuable to the Verifier, and acceptable to the Prover.
We describe the set of rules we seek to enforce in Section \todo{INSERT}.


In this work, we will lay out our progress towards a technical framework that fulfills all the these criteria, with a particular eye towards enabling government regulators to verify foreign governments'/companies' compliance with agreed AI rules.

We assess the extent to which the current proposal meets each of these criteria in Section \todo{INSERT}.

\todo{Instead of introducing the compute-type assumption here, introduce it more casually as a "we can only regulate this sort of system; we will leave as an assumption that imposing rules on such systems is sufficiently valuable, and discuss it more in an Appendix."}
Given that we are proposing a future-proof framework for regulating a technology that is actively under-development, we must invariably make some assumptions about the future trajectory of AI development practices.
We will highlight these assumptions wherever we can so that they may be productively critiqued, and the framework modified accordingly.

\ot{SOMETHING LIKE THIS: We claim that, if all these assumptions hold, the proposed framework represents a viable strategy for trustlessly enforcing AI rules, even in the international context.}

\subsection{Types of rules we want to enforce}
Let us outline the specific rules that we aim to enforce.
First, we will principally enforce rules on the development (\emph{training}) of deep-learning systems, rather than on the deployment (\emph{inference}) of trained models.
This is because, while training a large model is often a major resource-intensive task requiring specialized hardware, running a model is relatively cheap, and can be done on many personal computers.
Even small-scale fine-tuning of a pretrained model can be done with ubiquitous commodity hardware given sufficient time.
Policing individual citizens' use of their computers would be highly unacceptable on ethical grounds, and would essentially amount to a surveillance state.

However, industrial-scale AI training, which requires hundreds or thousands of expensive specialized chips running for weeks or months, is a much rarer operation only undertaken by sizeable organizations.
In particular, a unique property of these large-scale training runs is that they currently require \emph{large numbers of specialized chips run for an extended period of time}.
For example, \todo{cite OpenAI, Stanford, MetaAI, Deepmind, Eleuther papers' figures, or even better just the Epoch papers}.

\assumption{AI training will continue to require specialized, non-commodity chips.

* Currently, AI training requires multiple chips in the same training run to send large matrices to each other many times a second, meaning that AI training chips require extremely high bandwidth.
Such bandwidth is generally only found in datacenter chips (e.g. NVIDIA A100s or TPUs).
* While we have seen some trends away from a requirement for centralization (e.g. GPT-JT), neighboring individual nodes still seem to require high interconnect, or there will be a substantial slowdown (and thus effectively a reduction in capability).
* However, if new training strategies change this status quo (e.g. methods that enable extremely sparse parameter updates), then this assumption may cease to hold.
}

\assumption{AI training will continue to require a large number of compute-hours, and thus a large number of chips.

* The number required for a given ``dangerous'' capability may be quite high, especially for capabilities that do not exist yet but which rules may seek to preemptively constrain (including anything from autonomous code-executing RL agents to ``generally-intelligent'' artificial agents \todo{cite something with an AGI definition}).
* This number goes down continuously for algorithmic reasons, and thus any fixed threshold may become obviated over time. 
* However, the primary consequence of the scaling-laws takeaways are that more is more capable, and thus even if a fixed capability becomes doable more cheaply, other more powerful capabilities may still remain restricted.
}

\todo{One possible threshold to insert, as a motivational example: how many chips would it take to approximate the total amount of synapse-firings a human can do in one lifetime? (This will end up being roughly the PaLM-level, I think.)}


What desirable rules could a Verifier enforce solely by monitoring the large-scale \emph{training process} of an AI system, even if it could not detect small-scale retroactive modifications like fine-tuning?

To motivate our exploration, let us return to \todo{make sure we actually mention this earlier, or alternatively move this discussion to the beginning} the case of code-generating-and-executing RL agents.



\ot{Use, as a running example, "code-generating+executing RL agents trained with >X compute". This is plausibly an example that we can detect, because its dataset has a certain signature, its training algorithm has a certain signature, and its training length/loss has a certain signature.}

RESUME HERE



What sort of rules on model training might limit their harmful consequences?

\begin{itemize}
\item 
        1. Tracking: Any model trained with more than X compute must be reported, along with a model card to a central regulator
        2. Bans or approval-requirements: any model trained with more than X compute must be approved, and the trainer must comply with additional obligations (e.g. inspections, red-teaming, reporting requirements) enforced via human means. 
        3. Regulations on inference-sales: if there’s a way to require that large-scale inference must be done with signed weights (e.g. required for access to cloud-scale inference resources), then can require that the weights be the result of such a training run
        4. Require the developer to prove they have built a version of the model with risk-mitigating postprocessing, at least once. If a developer would need to pay a large extra cost to postprocess a model they train to make it safer, there’s a risk they won't do so to save costs. A Verifier could require that, for any heavily-trained AI model that a Prover constructs, they must also construct a postprocessed version of that model. This doesn’t rule out the possibility that the Prover would nonetheless deploy the not-postprocessed version, but the assumption would be that the postprocessing did make the postprocessed system at-least-slightly more desirable to the Prover.
        5. loss levels, hyperparams
\end{itemize}

Risk mitigation tax:

\ot{Comment that this format, of requiring the reporting of logs and then having a regulator verify those logs, is the standard way to regulate many industries (e.g. accounting in finance).}

\ot{Paragraph on the relevant parameters of an enforcement scheme: that it retains flexibility for the Prover (allows for arbitrary future AI training strategies, so long as they use SGD, and allows for standard parallelization strategies like model sharding), that it has limited cost both to the Prover and the Verifier (e.g. provides at most a modest overhead on input of both digital resources (like compute overhead) and physical resources (like the number of physical inspections the verifier must do)), that it can be implemented with minimal changes to existing hardware (either by using existing on-chip hardware or retrofitting, and that it only utilizes proven hardware capabilities/doesn't require trusting the security of new R&D), that it does not require the Prover to reveal sensitive data (like model weights or private training data) that it is comprehensive (catches all reasonable evasion strategies, up-to-and-including those employable by powerful nation-states, incl. hardware attacks), and that it enables enforcement of *useful* laws.

For each of these, have a sentence on what range of parameters we think our framework achieves.}

\ot{Flipside, we also have requirements from the Verifier's direction, in particular that no Prover should be able to undermine this scheme, even (in the case of international agreement enforcement) a nation-state level attacker, who can mount hardware supply chain attacks or invest large quantities of resources.}

\subsection{What rules we aim to enforce}

\ot{Explain that we're constrained in the type of rules we can enforce, because we can only enforce rules on steps of AI development that require a large number of chips.}

\ot{Paragraph on the type of rules we will aim to enforce: detecting, for large models trained with greater than X compute-hours, properties of the process by which they were trained (e.g. num samples, dataset properties, whether it was RL), so long as these rules can be articulated in advance as a function of the dataset/learning algorithm.}

\ot{List of bullet-points of some of the rules that do qualify.}

\copied{

Also: loss levels, properties of training dataset, properties of model hparams (including whether the model used supervised learning), ability to run the model on new inputs

9. Important: to map from “the training dataset plus model” to “plausible estimate of test loss”, we need to basically periodically evaluate the model on points it hasn’t seen yet. We can get these points by just looking at upcoming batches it was trained on! However, there’s an attack here - the Prover can, after the end of real training, keep training a pretend model for a while to throw the Verifier off the trail (whereas the true intended model checkpoints ended earlier). Thus, the “test” future-train-points need to be drawn from the points soon after the current step (as beyond that, the datapoints can’t be trusted). However, this sort of faking-at-end-of-training is also disallowed via precommitments.

}

\ot{List of rules we cannot enforce: on inference, on finetuning, etc. Also mention that rules on these *could* be enforced, if we have control of inference stage (e.g. control of cheap inference hardware and require weights to be signed) or if we restrict access to trained models only to organizations that we trust to abide by laws for other reasons + not share trained models farther. Emphasize that this part of the problem (protecting trained model weights) is out-of-scope, but must be solved by all existing AI governance strategies. Also mention, again, that many models should be open-sourced for public benefit, and that we are specifically describing models that are *dangerous enough that society has chosen to make them illegal.* E.g. code-generating RL models.}

\copied{    2. However, unfortunately, doesn’t cover inference
        1. If someone malicious builds a dangerous system and then spreads it across the internet (through sale or for free), or builds a legitimate system but then finetunes it to be evil, then whatever dangers it represents are solidified.
        2. However, this would enable an “upper bound” to the creation of new such systems (via either increasing compute, or the discovery of new algorithms)
        3. Similarly, doesn’t cover “student-teacher” transfer, but here we assume that the compute required for the student to learn from the teacher and then keep learning by itself is of a similar OOM to the original. (If Sorscher scaling laws hold up, this would cease to be true.)
            1. To analyze more, see this survey here: [https://arxiv.org/pdf/2004.05937.pdf](https://arxiv.org/pdf/2004.05937.pdf) 
            2. In practice, it seems like this sort of student-teacher distillation for peers (called self-distillation?) may indeed be possible, and reduce the number of training samples required. In particular, as the evil Prover, train a teacher, then train a student from the teacher’s data requiring much less compute, and then keep training this student on additional data (making it the new teacher for a subsequent model). Thus, each student appears to do a small amount of training (e.g. the distillation time + an equal amount of normal training), but cumulatively they result in a better and better model. In this way, a large training run can be hidden with a small amount of small training runs.
            3. Two counterpoints: (1) this finetuning is not infinitely-efficient: it still requires a bunch of compute. (2) the total training time across all the different teachers+students remains at least as large, probably larger. Thus, the guarantee shifts from “we can get the full training length of a model that used at least X compute” to “if a model used at least X compute, we can get its training dataset [which, per below bullet, is representative of the original training dataset] and know its performance on that dataset”. This is still useful because we can now in principle regulate models based on their predictive accuracy on their training dataset. (Need to do this on parts of the training dataset that haven’t yet been trained on and are thus “test data”, although in multi-epoch settings this may not be possible.)
}

\ot{Lay out more precise solution parameters: if a Prover trains a model model uses greater than C training compute within a T month period, we'd like the Verifier to notice this fact, and to get access to a snapshot of the model's weights weights from time C-eps (where eps is O(probability the trainer is willing to get caught + time between snapshots)), along with the training data used to produce that snapshot.

Mention that if it remains true that training requires long individual model runs, then "the training data used to produce that snapshot" actually tells us the total length of the run. But this is not necessarily true, due to an avoidance strategy elaborated on in the appendix.

Maybe mention the different parameters of a solution: T length of monitoring period between inspections, k total number of chips, a hardware inspections required, b secure retraining chip-hours, c on-chip overhead, d additional memory for storage.
}

\copied{8. Note that the above limits us in a key way: if a model takes C compute, and we sample the C/2-nd step w.h.p., then we can’t tell that that training run ever yielded a model of quality resulting from C. (Whereas if the whole training lineage existed, we could indeed catch this point.) However, if we double the sampling frequency, then we can catch a point in the second-half-of-training whp, and that’s a constant factor away from the final weights.
}

% \subsection{Goal}
% \todo{Verifier's goal - checking that Prover never creates an evil AI model}
% \todo{Introduce mathematical notation for what the verifier works with, e.g. number of chips, total computation time, total monitoring period time, total model parameters, per-chip chunk parameters, proof-of-learning efficiency parameters? (maybe later)}
% \todo{What are the ingredients we'll ask to know about an AI model in order to determine whether it complied with the law? (What are the necessary inputs into the "is-legal" function?) Answer = training dataset + training strategy + final model weights. (Plus maybe ability to query the model?)}

\section{Solution structure}
Our solution, in brief: large-scale AI training runs require a large number of chips - so large that many such training runs require using a non-negligible fraction of the estimated global AI chip supply. (See Table \todo{insert} for details.) \footnote{Estimates place the global quantity of NVIDIA A100 GPUs, the leading-edge AI training chip as of 2022, at below 2 million; we round up to 10 million for future-proofing.}
Thus, if we could determine whether a chip had participated in an illegal large training run based on physical inspection of some property of the chip, then we could catch such chips with high probability via \emph{random sampling}.

For such ``random sampling'' to effectively enforce AI training rules, we will need to solve three problems:
\begin{itemize}
    \item How do we know where each chip is, and who owns it?
    \item What operations was the chip ``supposed'' to be executing?
    \item Given physical inspection, how can we determine whether the chip was indeed executing those operations?
\end{itemize}

\begin{tabular}{|p{2.6cm}|p{1.5cm}|p{1.2cm}|p{1.8cm}|p{3cm}|p{3cm}|}
% \begin{tabular}{|l|S[table-format=1.2e2]|S[table-format=1.2e2]|l[table-format=1.2e2]|r|S[table-format=1.2e2]|}
\hline
\textbf{Model} & \textbf{Training FLOPs} & \textbf{TPUv4-days} & \textbf{TPUv4s to train in 6mo} & \textbf{Samples every 6mo to catch w/ p=.9} & \textbf{Chips per month (out of 10M)}\\
\hline
GPT-3 & 3.14e23 & 1.21e4 & 67 & 1 in 28 chips & 57,814\\
Chinchilla/Gopher & 5.76e23 & 2.22e4 & 122 & 1 in 52 chips & 31,517 \\
PaLM & 2.56e24 & 9.88e4 & 542 & 1 in 235 chips & 7,092\\
Chinchilla-280B & 9.90e24 & 3.82e5 & 2,093 & 1 in 909 chips & 1,834\\
Chinchilla-1T & 1.27e26 & 4.90e6 & 26,848 & 1 in 11,675 chips & 143 \\
Chinchilla-10T & 1.30e28 & 5.02e8 & 2,748,182 & 1 in 1,395,531 chips & 2\\
\hline
\end{tabular}

\hspace{2cm}

In order for random sampling to work, we will need the following 



\begin{itemize}
    \item The Verifier keeps track of the IDs of chips owned by the Prover, and ensures via supply-chain monitoring that they have hardware-based mechanisms for occasional activity logging (to be elaborated later).
    \item Require the Prover to periodically report any training runs on these chips to the Verifier, along with (what the Prover claims are) every chip's logs. The Prover and Verifier can then run an interactive protocol to confirm that the logs match the claimed training runs.
    This proves what training the Prover \emph{did do}.
    \item The Verifier occasionally samples a small number of the Prover's AI chips to inspect, and physically verifies that the logs match those reported, and that the logging mechanisms were not tampered-with.
    This proves that the Prover \emph{did not do any other training}.
\end{itemize}

This structure mirrors that of financial reporting: financial firms regularly make non-comprehensive reports to regulators, and regulators occasionally audit a firm's records in detail to confirm that they match the reports.
As in financial regulation, refusing to cooperate with the Verifier (e.g. by denying inspectors access to chips or failing to participate in interactive-verification) would itself be considered a rule violation.

% In section AAAA we will discuss step 1, which aim to guarantee that the Verifier can uniformly sample from all the AI-enabling chips accessible to the Prover, and that the Verifier can detect whether a chip correctly saved a trace of its operations.

% In Section AAAAAA we will discuss step 2, which aim to guarantee that if a single chip involved in a regulated training run is sampled, its logs can be used to determine whether the run complied with regulations with high probability. 

This leaves us with two questions: (1) how many chips does the Verifier need to sample? and (2) what information is being saved as a trace on the chip?

\subsection{How many chips does the Verifier need to sample?}

We need to determine the number of samples such that, given a training run of size C, at least one chip in the training run will be sampled (we know how sampling will confirm whether the reports were correct from Section 5, and we know how to use those reports to identify whether the model was compliant from Section 4).

To effectively verify that the reports are truthful, the Verifier needs to confirm that the reported logs match the logs in hardware.
Unfortunately, they cannot rely solely on virtual reports, since the Prover has physical access to the chips, and a sophisticated adversarial prover (like a nation-state hacking group) may physically undermine the chips.
Given appropriate on-chip hardware (discussed in Section \todo{add ref}), we can physically confirm that no such tampering has occurred and that the chips' info was reported accurately, but only via physical checks.

A Prover may own thousands or millions of chips. Inspecting the hardware on every chip every few months would be costly to the point of impossibility.
However, due to our guarantees from Section \todo{add section ref}, we only need to sample a \emph{single} chip involved in the regulated portion of a training run \todo{standardize "regulated portion of a training run", or "guilty chips"} to know that the whole run is suspect.

How many chips need to be sampled to catch a single guilty chip?
To be conservative, assume that ML training computations can be trivially parallelized, and that chips can be fully utilized (with no communication overhead).
Then to complete a regulatable job segment of length $\computehours$ during a single monitoring period of length $\monitoringperiod$, the Prover must use at least $\computehours/\monitoringperiod$ chips.
\footonote{A Prover may instead attempt to spread their training run out over $k$ periods using only $\computehours/(\monitoringperiod k)$ chips.
However, this means that they may get caught during multiple random sampling rounds, effectively subjecting the same chips to $\times k$ samples. 
As we discuss in \todo{ADD DISCUSSION}, spreading across multiple periods will not substantially improve their overall chances of avoiding monitoring.}

Given that the Prover has access to $\totalchips$ chips (including others' chips, e.g. via cloud-computing), for the Verifier to detect any training run using $\computehours/\monitoringperiod$ chips with probability $p$, the Verifier needs to sample $s$ chips, where:

\begin{align}
    p &> 1 - (1 - \frac{\computehours}{\monitoringperiod\totalchips})^s\\
    s &> \frac{\log{(1-p)}}{ \log{\left (1 - \frac{\computehours}{\monitoringperiod\totalchips}\right ) }
    }
\end{align}
Note that $p$ is the probability that any single training run gets caught; if a Prover attempts multiple illegal training runs, they are even more likely

For reference, we include a table of how many chips it would take to train several existing models, and how many chip-samples that would require (assuming current hardware) (in 6 months, and in 5 years).
\todo{Add a graph of "how many chips it would take to train model X in Y months (assuming we used the model's generation of chips), and how many would need to be sampled". Make sure to include Chinchilla-optimal training runs wuth different numbers of parameters.}

This is a surprisingly manageable number of samples.
Moreover, the number of samples requires grows logarithmically with the length of the run. If scaling trends in AI continue, such sampling will only become more efficient.
Indeed, even if an international coalition seeks to verify all training runs globally, the total number of chips that must be sampled remains a small fraction of the total supply. \todo{include calc}

\todo{Add note later: the snapshotting frequency will determine the total number of snapshots. By spreading the job across different chips at different times, these snapshots may be distributed between chips. However, a physical inspection will see all the snapshots on a chip, }

\todo{list full details of chip inspections in section 5}

\subsection{Keeping track of chip activity via weights-in-memory}

What information can be traced on a chip in order to determine whether it's part of a large scale NN training run, and what sort of training is being done?
Whatever this trace is, it must require little change to existing hardware, little additional overhead, and must not violate the privacy/secrecy of the computations the chip performed.

How is this possible? We'll exploit a convenient fact: throughout training, all contemporary AI chips store a close-to-latest copy of their NN weight shards in device memory.

Mention that we're going to presume we're running this via the host CPU (which still only has access to a chunk), which has a secure enclave to have attested firmware, and that we're going to require tamper-evidence to prove that the GPUs were not connected to any other host and that the anti-tampering wasn't broken. The farther away the secure host is from the machine, the more verification needs to be done to ensure no connections were swapped - moving from remote inspections to potential site inspections.

As our trace, we can require that the chip log \emph{hashes of snapshots of the memory regions containing the weights}, saved at randomly-chosen timesteps.
Why a hash? The weights themselves are too large to store multiple copies of on-chip (not to mention that reporting them in the clear would violate privacy), but by saving a hash, we can have the Prover subsequently provide a weight-file matching that hash.

More specifically, we require that the AI chip firmware raise an interrupt with some probability at every timestep. When this interrupt triggers:
\begin{itemize}
\item the chip pauses all processes
\item computes a Merkle hash of the region of memory containing the weight shard (which can generally be computed very quickly on such chips, hence their use for cryptocurrency mining)
\item saves the root Merkle hash of the weight shard in a secure NVRAM log (which only the firmware has write-access to)
\item saves a copy of the current on-chip weight shard, along with the corresponding weight shards on other chips
\end{itemize}
After some allotted time (e.g. once every month), the verifier then reports the chip ID, along with the timestep.

As we will show in the next section, on-chip weights are a remarkably useful trace because they not only identify that an NN training run occurred, but also can serve as a checksum for many of the details of that specific NN run.

A key benefit of weight snapshotting is that because SGD only gradually and continuously changes the weights, a snapshot at randomly-chosen time T implies information about the weights at earlier times, which we will leverage in Section _____. Thus, so long as the snapshotting-time is randomized, we need only to draw infrequent snapshots (e.g. once per hour) to gain comprehensive insight into the chip's behavior needs only to be done infrequently (e.g. once per hour).
This means that monitoring introduces relatively little overhead computational overhead. Furthermore, the infrastructure for saving snapshot-weights is a preexisting part of all AI clusters.

Unfortunately, in order to enable this sort of memory-snapshotting, we run into two obstacles/open questions:

Open Q: how do we reliably determine the region of memory in which the weights are stored? Ideas include requiring software to be signed (like with the iOS app store, except that these apps include PyTorch/JAX) and the signed software stores weights in a reliable region. Alternatively, may be possible to snapshot the loaded software and then retroactively profile it to confirm that the claimed region of weights-memory is the accurate one.

Open Q: how do we determine whether high-end chips are being used for NN training, vs. other HPC use-cases? This is admittedly unclear, and will require much more work to understand how it might be possible to distinguish between these jobs. Hence we leave it as an assumption. Initially, it may make sense to focus on high-interconnect cluster configurations (BUT THIS COUNTERACTS THE OTHER DESCRIBED USE-CASES).

For completeness, we mention some alternative traces that could in principle be used, and their drawbacks:
\begin{itemize}
\item \emph{Training data}. Drawbacks: violates privacy, saving it requires additional operations every step (as opposed to very infrequently)
\item GPU kernels/code. Drawbacks: code is generally the same on a single chip regardless of how long the training run lasts
\item GPU connectivity to other chips. Drawbacks: hard to determine connectivity to other chips, and requires inspecting other chips as well to get the full picture of their collective operation. Also, nearby-connectivity isn't everything. Due to the internet, chips may work on the same job even across datacenters (e.g. GPT-JT). We'd like to still be able to identify large training runs even if they're sharded across the internet. All that said, in some cases (e.g. where the chip does not contain secure logging ability) this sort of connectivity inspection may prove necessary in order to enforce the guarantees of the larger framework.
\end{itemize}

\section{On the chip}

* What info can we look at on a chip, that will tell us what happened with the training run?
* We'll focus on model weights, since they contain traces of data.
* How can we make it so that inspecting the chip informs us of the models that were trained on that chip?
* Require (at a firmware or hardware level) that the weights are stored in a certain region, and then periodically raise an interrupt and hash them.
* Store these hashes in an on-chip log (to prevent needing to phone home)
* Make sure device is tamper-evident (possibly doable via retrofitting), including e.g. that new firmware can't be flashed on.
* On inspection, check the device's logs and 


\section{At the datacenter}

* Collecting logs, 


\todo{Mention that the goal for modified proof-of-learning is that its complexity should be exponential in d, num params.}

\todo{If d-param PoL is hard, shard-PoL is also hard}

\todo{Second Q: given a real PoL, can we construct a modified shard-PoL different from the original, which thus fails our modified-PoL-reqs? Here's an argument for why that'd be hard. Either training data must depend on blocks before/after, or need to propose *alternative* blocks that, w.r.t. some data, generate exactly the inputs/outputs at every batchstep, *and* have PoLs that correctly transition them at every batchstep.}


Assume, based on previously-described mechanisms, that the Verifier now has access to the weight-shard-snapshot-hash logs from a random sample of the Prover's AI chips, including at least one involved in any large \todo{standardize terms} sensitive training run.
As a reminder, the Verifier's goal is to understand properties like the length of each of the Prover's training runs, the type of dataset used, and more.
These weight-shard-hashes allow the Verifier to ask the Prover to provide the corresponding weight-shard snapshot present on the chip at that time, and confirm that it matches the logged hash.
(Such weight-attestations must be done securely without directly revealing sensitive model weights to the Verifier, while still allowing the Verifier to execute tests on the weights. For now, we will continue the explanation as though a matching weight-shard was provided to the Verifier in the clear.)

The weight-shard snapshot is simply a vector of weights; the Verifier does not know how these weights map onto layers of an NN architecture, and similarly has no knowledge of the other weights of the neural net not stored on the chip (assuming the weights were sharded across multiple chips).

% The Verifier cannot just take the Prover at their word on this mapping or other NN hyperparameters, since the Prover could lie and claim a fake mapping of weights to NN layers that would encode a function completely unrelated to the original NN encoded by the weights.
% Worse, since AI chips often only store a shard of the overall weights, the Verifier cannot know the NN layers before and/or after this NN-shard, and a malicious Prover could lie about these as well.

Nonetheless, while a raw vector of NN weights may appear random, these weights are in fact highly structured: they are the quasi-deterministic result of the ``training run'' computation.
Every weight-shard vector is the result of a particular random initialization of the weights, followed by a long series of gradient updates based on a particular sequence of batches of training points.

Intuitively, it seems like it may be very hard to find two random weight initialization and sequences of training batches that would find a similar final weight vector, given the inherent randomness of gradient descent.
Each such weight-shard will often have at least $100M$ parameters, corresponding to $2^{10^8}$ possible vectors even if binarized.

Motivated by this, the Verifier can require the Prover to share \emph{training transcripts} for every training run they execute on the chip: a weight initialization and a sequence of (hashes of) datapoints which, if used in a training run, would yield a weight vector closely matching the weight vector attested to by snapshot-hash.
This setting has been expored as ``proof-of-learning'' \todo{add cite}.

Given a target weight-shard snapshot $\omega_f$ matching a logged snapshot hash $hash(\omega_f)$, let $\hat{W}_f$ be a full-weight vector such that there exists a subset of the weight vector $\hat{W}_f$ (using only slicing, no reordering) denoted by the operator $\mathcal{S}$ such that $\omega_f = \mathcal{S}(\hat{W}_f)$.
\todo{Can also mention that this slicing should be of limited complexity, e.g. can't slice individual values.}
That is, this overall weight vector $\hat{W}_f$ is a plausible match for the weight-shard $\omega_f$ that existed on the chip.

Then, a \emph{training transcript} $\mathbb{T} = \{\mathbb{W}, \mathbb{D}, \mathbb{M}\}$ is a set of model and training hyperparameters $\mathbb{M}$, a sequence of training data batches $\mathbb{D}$, and a weight initialization and sequence of weight vector snapshots $\mathbb{W}$, that are claimed to result sequentially from training on $\mathbb{D}$.
A training transcript $\mathbb{T}$ is said to match a weight vector $\hat{W}_f$ if there exists a weight snapshot $W'$ in $\mathbb{W}$ such that $\| \hat{W}_f - W'\|_2 < \epsilon$ for some small $\epsilon$.

(``Matching'' is not exact, but rather requires a small tolerance $\epsilon$ due to hardware-level floating-point errors making repeated training runs slightly diverge over time.
For a more detailed treatment of the PoL procedure, see \todo{cite PoL}.)

Given a training transcript, the Verifier can run a verification protocol $V(\mathbb{T}, \hat{W}_f)$ to check whether the series of data batches and resulting weight snapshots in $\mathbb{T}$ are self-consistent, and would result in $\hat{W}_f$.
A key proposition of PoL is that the complexity of \emph{verifying} the correctness of a transcript may be doable more cheaply than re-executing the entire training run.
Instead, the Verifier may need only to re-run particular segments of the training run (e.g. chosen randomly), and may exploit the structure of these segments (which each must end where the other begins).
It is important to note that Proof-of-Learning verification schemes are still heuristic-based (rather than provably secure against adversaries), and there is an ongoing literature of attacks and defenses. \todo{cite stuff}

Note that the Prover does not need to leak sensitive info when first attesting to a training transcript \todo{cite PoL paper for this idea}: the Prover can provide the Verifier with hashed data batches, weight-snapshots, and hyperparameters, which only need to be revealed during verification.
(We discuss how verification can be done confidentially in \todo{insert}.)

Our setting differs from the original PoL setting in a few important ways.
PoLs were originally proposed as a tool for proving model ownership (for IP protection purposes) and for proving that cloud-training services executed the training they claimed.
The original PoL problem required a Prover reporting specification $P$, and a Verifier checking protocol $V$, such that given a weight vector $W$, the Prover can only provide a report $p \in P$ that passes the Verifier's checks $V(p)$ with probability $1-\epsilon_2$ \emph{if} the Prover knows the original sequence of training data batches $\mathbb{D}$ and hyperparameters $\mathbb{M}$ \emph{OR} the Prover has done at least $F$ operations to construct a spoofed report.
They also require that the Verifier checking protocol run with $x_V$ complexity.
The original PoL paper's protocol described a Reporter/Verifier protocol which they posited was $F > C$-hard to spoof and $x_V < C$ efficient to verify, where $C$ is the quantity of compute-hours used for the original training run.
However, the verification protocol was not proven secure, and several attacks have been proposed \todo{cite}.

Our problem, which we call ``Proof-of-Training-Transcript'' (PoTT), modifies the original PoL problem as follows.
In our setting, the Prover already has access to a valid training transcript for a $\hat{W}_f$ matching $\omega_f$ (since they trained the original model), but wants to report a \emph{fake} transcript to mislead the Verifier about the nature of the training run (e.g. how many compute-hours it took, what types of datapoints it trained on, whether it used active learning).
The Verifier's job then becomes much harder: they must not only verify that the Prover's training transcript plausibly generated $\hat{W}_f$, but also that it was not excessively modified.
Also, whereas PoL assumed that an adversary would be unwilling to pay $F > C$ compute to claim model-ownership, we must consider the possibility that a well-resourced adversarial Prover may expend $>C$ extra compute to hide their illegal training runs from regulators (which would however reduce their compute-efficiency by $>50\%$).

We require a Prover reporting specification and a Verifier checking protocol with $V$ complexity, such that given a weight vector $W$, \emph{even if} the Prover knows a true transcript $\mathbb{T}_1$ which correctly explains $W$, the Prover must expend at least $F$ compute in order to generate a transcript $\mathbb{T}_2$ which the Verifier will accept with $>1-\epsilon_2$ probability if $\mathbb{D}_2$ differs from $\mathbb{D}_1$ by greater than $c|\mathbb{D}_1|$ additions/deletions. \footnote{One could also expand this to include minimizing modifications to the hyperparameters in $\mathbb{M}$.}
We would like $F$ to be as large as possible, and $V$ to remain much smaller than the original compute required $C$.
Ideally, we may hope to make $F$ require time exponential in the number of parameters in each weight vector $d$ ($O(e^d)$), equivalent to random draws from $d$-dimensional weight-space.

On its surface, it may seem that PoTT verification is strictly harder that PoL verification, and there do not yet exist provably secure solutions even to PoL.
That said, in Section \todo{insert}, we will motivate some reasons for optimism that a solution may be tractable.

We leave formal solutions to the PoTT problem as an important direction for future work.

\subsubsection{Reasons for optimism about PoTT}

First, full-on PoL spoofing is a hard problem, as the dynamics of SGD are poorly understood. It is particularly hard to construct a spoof that is *future proof* for future discoveries that Verufiers will make about new properties of SGD (given that the Prover must commit first and the Verifier can respond with their requests later). Thus, even if this becomes a back and forth, it may structurally favor the defender.

Second, the amount of extra work done by the verifier can be ramped up (e.g. by doing dataset sanitation checks). A particular possibility is that the compute required to efficiently build PoL spoofs is *also* datacenter AI compute (plausible as that’s the operation being spoofed).
\todo{ clean up: Justification: faking a PoL requires running an equivalently-large number of gradient updates, which can only be done efficiently on this type of compute for the same reason that the original training runs can only be efficiently run on this type of compute.}
Then it may be possible to detect telltale signs of such spoofing operations (such as their weight-snapshots sharing a similar training transcript).

Third, there are many under explored mechanisms. We highlight one example: pre commitments to hashes of training-batchplans. This only applies in offline or batch-online settings, where a large # of steps are known ahead of time (precommutments should probably last >1 snapshot). If prexonmitments to a PoL batch-plan can be made in advance, then the Prover could not have known where the training would end up, and thus could not construct a fake transcript to precommit to that would end up in the same place as the real transcript.
Two problems: how do we prove the pre commitment really happened ahead of time, and how do we know the pre commitment wasn’t constructed by an adversary who already knew where this training segment would end up and thus designed an appropriate fake transcript?
For the first, log the precommitment in the same hardware log of each chip immediately *before* training, which would prove sequencing. In principle, it’s possible to do *some* small amount of illegal work without a precommitment stored, but every cycle raises the odds of an interrupt, which would catch a non-precommit Ed point.
For the second, if somewhere else another chip controlled by the Prover has already generated the next weight-snapshot, then whatever chips that training-segment was run on must have been AI training chips themselves. Then, by assumption, they’d be subject to the same monitoring scheme, and they would appear illegal (would they actually?) (because their precommitment must have been honest, as they did not have another chip that already did the work.)

\subsection{Shard-PoTTs vs. full-PoTTs}
A Proof-of-Learning or Proof-of-Training-Transcript attests to the full weight vector $\hat{W}_f$, but each chip may only see a shard of the overall weight vector $\omega_f$.
We can define a shard-PoTT verification protocol $V(\mathbb{T}, \omega_f)$ as returning True only if a training transcript $\mathbb{T}$ results in a weight-vector $\hat{W}$ that contains $\omega_f$ as a shard, i.e. there exists a $\mathcal{S}$ slicing operator such that $\mathcal{S}(\hat{W}) = \omega_f$.
This contrasts with a ``full'' PoTT, described in the previous sections, which attests to a full weight vector.

If $\mathcal{T}$ is the original full-PoTT that generated $W_f$, then it is trivially true that $\mathcal{T}$ is a shard-PoTT for $\omega_f$.

Separately, we can ask, given no full-PoTT, whether it is easier to generate a spoof for a shard-PoTT for $\omega_f$.
We leave this as an open problem for future work.

\todo{Move most of this discussion to an Appendix.}

Intuitively, so long as the shard includes weights from more than one linear layer of the neural network (e.g. via tensor or multi-layer pipeline parallelism), generating a shard-PoTT is equivalent to generating a brand new full-PoTT for a small neural network.
Thus, earlier arguments on the hardness of full-PoTT would extend to shard PoTT.
(However, this would require the Verifier to have access to additional information about the mapping of weights to NN layers, to prevent the Prover from claiming that the weight vector was a single linear layer - which may be more easily spoofed.)

Alternatively, if the Prover pre-commits to the training batch sequence $\mathbb{D}$ as described in the previous section, then spoofing is just as hard in the shard-PoTT case as in the original case.

A separate concern is that access to the weights of a single shard does not imply access to the rest of the weights of the neural network.

We might then fear that an attacker could modify a true full-PoTT $\mathbb{T}$ to a fake PoTT $\mathbb{T}'$, such that $\mathbb{T}'$ correctly attests to $\omega_f$, but changes the weights in the layers $l_{pre}$ preceding the shard with weights $\omega_{pre}$ (to $l'_{pre}$ and $\omega_{pre}'$) or that come after the shard's layer $l_{post}$ with weights $\omega_{post}$ (to $l'_{post}$ and $\omega_{post}'$).
This would mean that the training dataset, and full final weights $\hat{W}_f$, could be entirely falsified.

In order for this spoofed PoTT $\mathbb{T}'$ to yield $\omega_f$ even with the other layers modified, the gradients $\frac{dL(x, y)}{d\omega_f}$ must match the originals at each timestep throughout training. (Otherwise, the original PoTT will not apply.) 
But these gradients are a function of both the input resulting from the preceding layers (and thus $l_{pre}$ and $\omega_{pre}$), and of the backpropagated gradient from the succeeding layers (and thus $l_{post}$ and $\omega_{post}$).
Thus, if a spoof claims to have constructed a new $(l'_{pre}, \omega'_{pre})$ or $(l'_{post}, \omega'_{post})$, their values must change each training step to closely match the originals' effect on $\frac{dL(x, y)}{d\omega_f}$.
But beyond this, all changes in the preceding $\omega'_{pre}$ and succeeding weights $\omega'_{post}$ must \emph{themselves} be explained by a PoTT, as they too are weights in the larger neural network.
Finding such a sequence of weights, which produce the appropriate outputs or gradients at every timestep (not just at occasional weight snapshot points) and that are themselves explained by a PoTT, seems like it may be substantially harder than simply producing a spoofed PoTT that does not rely on modifying the original $\mathbb{T}$.

One additional spoofing strategy could be to start with the true PoTT $\mathbb{T}$, but then modify it to claim that $\omega_f$ does not have any preceding or succeeding layers, and represents a fully self-contained NN.
But then each timestep, the fake inputs $X$, targets $Y$, and loss function $L'$ must be constructed to exactly reconstruct the original $\frac{dL(x, y)}{d\omega_f}$.
However, since in the true training run the inputs and gradients varied with $\omega_{pre}$ and $\omega_{post}$, which changed at each timestep, we know that any function mapping from $X$ to $Y$ must vary throughout the training run.
Thus, there is unlikely to be any function $f$ such that $L(f(x), y) \approx 0$ for IID samples $x, y \in X, Y$.
This means that the final network itself, defined by $\omega_f$, will also not have lower loss on samples $X, Y$, than snapshots from earlier in training (contradicting the universal tendency of NN training to reduce training loss throughout training).
Thus, this evaluating the claimed final NN's training loss on randomly-selected PoTT training batches, and comparing that loss to earlier snapshots from the PoTT, is likely to be an effective test to catch this sort of spoofing.

\subsubsection{Given no access to the true full-PoTT $\mathcal{T}$, the hardness of constructing a shard-PoTT reduces to the hardness of constructing a full-PoTT (though not necessarily for the same dimension of weight-vector).}

Let us consider the case of a weight-shard $\omega$ that is a slice of a single matrix $w_k$ in the $k$th layer of a neural network $W$. (In practice, using tensor-parallelism, a single chip's memory may include many such shards. The same argument will apply.)

Then, spoofing a Proof-of-Learning for $\omega$ requires ... \todo{maybe it only requires constructing a PoL for a linear model? Which is generally going to be less hard. Is it actually straightforward to create a spoof-PoL for a weight-shard? We can make it harder if we require shards on multiple layers.}

* Given access to a true full-PoTT $\mathcal{T}$, it is computationally hard to construct a shard-PoTT that attests to a different full-weight-vector $W' \neq W_f$.


 \subsection{Practical Details}

 How could Proofs-of-Training-Transcripts be verified in a way trusted by both the Prover and Verifier?
 The Prover and Verifier need to agree on a trusted training cluster (perhaps jointly constructed), which can rerun short segments of model training to complete the PoTT-verification protocol.
 The Verifier must be confident that the machine will correctly simulate training, and the Prover must be confident that the machine contains no ability to store the confidential weights or data that the Prover must provide to simulate the training.

 When verifying a PoTT which the Prover previously provided to the Verifier, the Verifier inputs the hash of the training hyperparameters $\mathbb{M}$, and a selected subset of successive weight-snapshot hash pairs and the hashes of the corresponding training batches from $\mathbb{W}$ and $\mathbb{D}$ respectively.
 Then, the Prover inputs the corresponding hyperparameters, full weight-snapshots and training batches.
 The neutral cluster hashes these hyperparameters, weight snapshots, and batches, using the same hash function, and confirms that the hashes match those inputted by the Verifier.
 The cluster then constructs the training loop via the hyperparameters $\mathbb{M}$, which can be specified in a machine-parseable format, e.g. a Python program that defines the training loop via calls to a certain version of PyTorch.
 Then, for each selected pair of successive weight-snapshots $(W_i, W_{i+1}$, it initializes the weights according to $W_i$, trains on each training batch in the provided order, and then checks that the produced final weights nearly match, $\|W'_{i+1} - W_{i+1}\|_2 < \epsilon$.
 (For complete details on these re-simulations, see the original PoL paper \todo{citE}.)
 If all the training segments match, and all the hashes match, then the machine outputs that the process succeeded; otherwise, it raises an error that can prompt further investigation by the Verifier.
 
 If this verification process succeeds for all training transcripts corresponding to all weight snapshots for each Prover chip sampled by the Verifier, then the Verifier can have high confidence that the Prover truthfully reported transcripts of all large training runs done by the Prover.
 Importantly, at no point in this procedure did the Prover ever need to reveal their sensitive model weights, training data, or even hyperparameters to the Verifier.

 Assume that $\mathbb{T}$ requires a weight-snapshot every $k$ steps, and each chip logs a weight-shard-snapshot-hash every $sn$ \todo{standardize notation} steps, where there are a total of $n$ chips involved in the training run.
 Then, the time cost of reverification for a single chip (amortized across all chips) is $O((k/n +  + s)h + bC)$, where $h$ is the time to 

$g$ is the cost of running forward a single training step, on $n$ chips.
$C$ is the total compute cost for the training run, which is equal to $n $

Total compute: $b (k g )$

 
 The time cost of reverification for a single training run is $O(bC +)$ for training reexecution, where $C$ is the total training time across all the Prover's chips and $b$ is the fraction of snapshot-pairs in $\mathbb{W}$ that must be recomputed by the PoTT verification protocol $V$.
 This cost is fixed at a $b$ fraction of total chip utilization, and it is thus important to find verification strategies $V$ whose complexity scales sublinearly with training-run size.
 The total storage cost is $O(|\mathbb{W}| + |\mathbb{D}| + |\mathbb{M}| + s|W|)$, where $s$ is the number of logged on-chip samples. though some of the checkpoints may be abandonable ....
I HATE THIS

 The last step is that the Verifier would like to determine specific properties of the Prover's training runs.
 This can be done by leveraging the same neutral cluster previously used to verify the training transcripts.
 After the training-transcript verification, the Verifier can execute arbitrary programs that take the training transcript $\mathbb{T}$ as input, all without disclosing confidential information.
 This could include model evaluations on the final weights, or analyses of the type of training data used (so long as these metrics are robust to the up-to-$c|\mathbb{D}|$ training data additions/deletions that cannot be verified).
 In a sense, training-run-verification can serve as a platform for verifying compliance with any policy specified on large training runs, so long as it can be verified via a predefined program and without revealing the Prover's private information to the Verifier.
 
 \todo{Mention the mapping from weight shards to full-weight transcripts - it's currently glossed over}

 \todo{Mention that weight snapshots should include optimizer state when using a stateful optimizer}

 \todo{Mention that a limiting factor is the cost of storing weight snapshots - need to find a way to retain the minimal number of snapshots, or to do verification fairly quickly after training occurs}

 \todo{mention actually version-controlling and reconstructing the code is a challenge, and would require infrastructure for doing so. "code-rot" is a real problem when trying to run old code. May need to compile binaries and save them, and then have a snapshot of code dependencies and prove they result in those binaries. Also, this assumes the ability to minimize hardware-level nondeterminism/port across hardware.}

 \todo{PoTTs must justify *all* the snapshots involved in the training run. For true PoTTs, this is good, as it means linear rather than quadratic time. For fake PoTTs, this means that they must either constrain themselves repeatedly to remain near the real training run, or use very short training-transcripts, or (if the fake-PoTT requires a large number of training steps which would've resulted in intermediate snapshot-hashes) artificially insert new snapshot hashes into chip logs (requiring more compute or leaving the possibility of being discovered).}

A major virtue of Proofs-of-Learning, and by extension Proofs-of-Training-Transcript, is that constructing them require little additional overhead on the part of the Prover.
They simply require saving weight-snapshots (already collected in NN training, though a PoL may require higher-frequency snapshotting), and storing the sequence of training data batches.
While holding onto models' training data may not currently be a universal practice, it is a common sense requirement for any AI regulatory framework, and has many internal benefits as well.



\subsection{How this can be used to enforce rules}
If the Verifier had access to authentic Proofs of Learning for all weight-shard-snapshots on a nearly all a Prover's AI chips, what kinds of agreements could they enforce with this information?
\begin{itemize}
    \item Get a model's architecture.
        \ot{Proof-of-Learning also uniquely attests to the model architecture (and weight configuration), since if the Prover lied about the architecture, the model that it would need to generate a PoL for would be different, and given that it didn't actually do that training, this reduces to the problem of generating a brand new PoL, which we're assuming is hard.}
    \item Inspect random samples of training data
    \item Compute the model's training loss, and (for non-terminal models) its validation loss
    \item Get a model's total training time (though this might not account for the time used to generate its datapoints, see appendix)
    \item Be able to run it on chosen test data.
\end{itemize}



\ot{* The point of the PoL is that it is hard to spoof a PoL that is different than the true PoL, unless you have expended at least as much compute as the original training. Assumption: the compute required to spoof PoL is the same type of compute required to do training. (This makes sense, as spoofing a valid PoL requires computing the gradient at least once at every spoofed set of weights, which requires weight matrices in memory to be constantly updated to the new step-to-be-spoofed, and this communication bandwidth is precisely the unique property of DC chips.) Thus, if the Prover tried to construct and report a new PoL, the process of its construction would itself require at least compute C, and would thus be detectable by the Verifier. The Verifier would then ask for a PoL of the spoof-in-development weights being used, and the only such PoL on-hand would then be a perfect match for the spoofed PoL that the Prover wanted to report - raising a red flag. Thus, if the PoL is spoofed, it raises a flag during verification of the spoofing.}
\ot{* Thus, the only way to provide a valid PoL, which should be doable for any training run, is to report honestly.}



\ot{* Of course, a PoL itself contains a lot of sensitive data, and so only the hashes of the ordered datapoints, hyperparameters, and full-model weights should be reported. They will only need to be revealed during verification.}
\ot{* Unfortunately, PoLs as classically formulated here don't quite work. Point to the last subsection.}

\ot{ 3) Given access to a PoL, what kinds of rules can we enforce, and how? Answer: list, as bullets, a few ideas, including total-compute-used, whether the dataset includes X type of unacceptable data identifiable by a function, whether the training algorithm is RL.}


\ot{** How PoL verification would work in practice}
\ot{* Explain that actually verifying a snapshot's PoL means re-running the training. Highlight that this can't really be done cryptographically because this is so expensive. However, due to the need to retain secrecy of data + models, it can't just be done by providing Verifier with data. Instead, it will need to be done by constructing an agreed-upon machine, that both the Prover and Verifier believe is not tampered with. The Verifier has to believe the machine will run the computations required (e.g. via attested computations confirmed by hardware), and the Prover has to believe that the machine does not store or report any components of the secret data beyond what was agreed. In practice, these neutral facilities would need to be maintained by someone and inspected by both parties (posibly onprem for Prover).}

\ot{the Prover brings the data and hparams and model snapshots, the Verifier brings the hashes the Prover had previously attested to, and then the machine runs a program that hashes the Prover's provided info and confirms it matches the Verifier's hashes, and then runs the training process forward and confirms that the resulting weights are within the appropriate tolerance. Then the machine outputs "success", and if any of these steps fail it outputs "failure", prompting a larger investigation.}
\ot{* We'd want to verify all these PoLs, since any single snapshot might be the only snapshot we get of an illegal run. Thus, we want PoL verification to be fairly cheap in compute relative to the original run, since this extra compute is essentially a tax on existing compute quantity. However, given that the point of PoLs is efficient verification, this can hopefully remain a relatively small overhead. Also mention that, if there is more trust between Prover and Verifier, such verification could also only be done during periods of heightened scrutiny due to other indicators. (This is how audits work in finance.)}
\ot{* Conveniently, we don't need to verify a new PoL for every snapshot, just a new PoL for every training run (since multiple snapshots are attested by the same training run). Thus the compute cost added by verifying PoLs is linear in total compute used.}
\ot{* Resulting from this, we now have a guarantee that, assuming the PoLs were valid and the logs represent a 1-eps fraction of utilized compute, we have true PoLs on any training run with greater than C compute w.h.p.}

}


\subsubsection{Chunk PoLs vs. Full PoLs}
\ot{2) Rather than requiring a PoL for a full model, we only require a PoL for the chunk of the weights present on the chip. Due to the way that SGD works, a full-model PoL automatically contains a chunk-PoL. Indeed, given a full-model PoL, the Prover can efficiently prove, without revealing the true model, that this PoL also attests to the chunk, by providing a ZK-SNARK attesting that there exists a full-model weight with the attested hash, where a subset of that weight (defined so that there are a finite number of subsets that don't enable reordering) matches the saved chunk-snapshot-hash. HOWEVER, it is not true that simply given a full-PoL that matches the chunk, that that full-PoL corresponds to real other chunks. For example, you could imagine that an NN has 10 chunks, and the Prover pretends that each one of them is its own full NN, where the data is the preprocessed data run through the earlier NN parts, and the labels are... something? IDDDKKKKKKKK. It does seem like the data used as inputs and outputs would look weird here, and so it may be possible to run some simple data-normality checks. But this requires further work to prove whether a chunk-PoL within a larger PoL is only possible when the larger PoL is itself real.}

\subsection{Ways this setting still favors attackers}
\ot{
Even if current PoL verification techniques are spoofable, the Verifier has a consistent advantage, due to the following asymmetry. The Prover must commit to a PoL once, and then as the Verifier discovers new verification strategies (e.g. due to new discoveries about DL) they can place new tests on the Prover's PoL, including ones using properties of real SGD learning that were unknown at the time of original construction.
}
\ot{Also clarify that this is a harder version of PoL because it requires attesting to multiple snapshots (rather than creating a transcript for a single snapshot), and thus requires the harder version of spoofing, "directed retraining", which has not yet been convincingly broken.}

\ot{High-level point: these PoLs require *directed* retraining bc it’s attesting to multiple snapshots in the same run, meaning that the easiest spoofing approaches don’t work. Efficient directed retraining-based spoofing is an open problem. (Mention the “adversarial examples” paper but highlight that it does not appear to work for nonzero gradients, and mention Deep-Leakage-From-Gradients but highlight that this differs from our case because there may not *be* datapoints other than the originals that move the learning algorithm in the desired direction.)
}

\subsubsection{Additional defense strategies}
Precommitments:
Mention that one way to solve all these problems is by NN-training “precommitments”, wherein the hashes of datapoints are published ahead of time (either from Prover to Verifier, or encoded by Prover within the random seed used to generate the init randomness, such that it cannot be changed later - a similar commitment mechanism to the Bitcoin PoW). However, this is harder to do in the online setting, where the order of future datapoints is not known a priori.

Data integrity checks:
Mention that all PoL attacks that don’t just exploit verification-infrequency involve generating new datapoints, and these datapoints may be identified as atypical (eg if they’re images run an image-detector to confirm; if they may be tuned via noise to cause specific gradients, check the magnitude of the gradient’s gradient wrt the input data, which should be abnormally large).




\subsection{Define PoL}
\todo{Write a concise description of PoL, including intuition and what it would require.}

    

\subsection{Real-world confidential implementation}

\copied{2. On a firmware level, with some probability f at each clock cycle, throw an interrupt, hash the relevant regions of RAM containing the weights (plus optimizer state and any other needed params, e.g. registers) via an agreed-on cryptographic hash function, and store the log in firmware-writeable-only memory.
3. ASICs that are good at matmuls can by-default compute hashes very quickly (GIVE A NUMBER FOR 80GB). Furthermore, this snapshotting can be quite infrequent - just once every thousand gradient steps, for example. Thus the performance overhead is small. That might mean roughly once per chip per hour, or even per day. Do a back-of-the-envelope calculation to show that for 3 years, that only requires a few MB of NVRAM.
4. Then, the Prover/trainer must save the weights in storage. These may subsequently be requested by the Verifier as part of verification, and when hashed will need to match the corresponding logged hash. Such snapshotting already occurs in the ordinary course of training; this may just require an increased frequency.
5. The particular frequency will come down to the requirements of the PoL algorithm. Too-frequently snapshotting would end up incurring a large storage cost for large models, (i.e. terabytes generated every day that need to be stored for months) so the efficiency of the PoL protocol (described later) is important.
6. For chip-level snapshotting, as opposed to central operator snapshotting, each chip only contains a chunk of memory, not the full model weights.
    1. ADD IN EXPLICIT DISCUSSION OF HOW MULTIPLE CHIPS CAN AGREE ON A SNAPSHOTTING SCHEDULE AND COMMUNICATE IT TO EACH OF THEM.

\copied{Verifying the results of a short snippet of gradient descent when given access to the randomness should not be hard. It involves loading the claimed weight snapshots and data points, verifying that they correspond to the hashes, executing the series of gradient updates, and then checking that the distance between the generated final snapshot and the claimed final snapshot is small. If these check out, the verifier’s program should approve, and should reject otherwise.}

\todo{Reiterate the risk from PoL in-the-clear}


\todo{Talk about how to do PoL verification securely, via agreed-upon secure enclaves}
7. A few such neutral machines (which may need to be supercomputers, in order to efficiently verify tiny chunks of large training runs) need to be maintained between all pairs of untrusting parties. For example, a regulator can run one, or can maintain one per company/region, with the companies able to oversee/validate construction. Or one can be built together between a pair of companies/militaries, or an international body.}
\subsection{Doing this at scale}

\copied{8. Actual operation: every reporting period, once the Verifier has selected a subset of chips and their logs, they request an encrypted transcript, including info needed to determine the appropriate transcripts to “rerun”. The Prover provides this, and after the Verifier selects the subcomponents of the transcript to verify, the Prover brings this data to the approved facility, where both parties oversee the results of the computation. If all the results are confirmed valid, then w.h.p. the Prover did not run any training jobs using greater than the approved amount of compute-hours without informing the Verifier.
9. ADDITIONALLY: figure out how to work in verification of the test loss. We can certainly re-verify reported test loss given access to data hashes and a model hash, but for efficiency’s sake, we probably only want to do this in a small number of snapshots. (Probably the same snapshots as training-verification.) Argue that, even if it is only checked on those checkpoints that we run training on (by sampling a few future datapoints), then due to the monotonicity of the test loss, we should expect it to be within a certain margin of accuracy everywhere. (Actual proof statement sketch: if within a k-step period the loss is a-Lipschitz per step, and we have samples within this region with z frequency, then the probability that the loss is misreported by ka, it will be caught with probability (z-1)/z or something.)}

\todo{Need to do this not just for one snapshot on one chip, but many snapshots on many chips. However, this is not quadratic but rather linear, b.c. all snapshots in a run share the same base PoL.}
\todo{Mention how you link a snapshot-hash to a base PoL via Merkle-hash-matching, which can be done with ZK-SNARKs cheaply.}
\todo{Also talk about efficiency considerations, highlight that a key open Q is whether PoL can be made sufficiently cheap to do.}

\copied{The actual verification steps in existing PoL protocols require running some brief period of SGD between two checkpoints. This creates a tradeoff in checkpointing frequency: the more often you checkpoint, the shorter the intervals you need to verify, but also the more weight-snapshots you need to store until verification.}

\section{At the supply chain}

* There are a small number of fabs, we can add inspectors at them, and then track where all the dies end up globally.

* 

\todo{Mention the issue that samping might not be genuinely independent (since each server is stuck to other chips), but that it denotes the number of servers. Also that as flops increase, # of chips needed decreases, and so # of chip-samples increases.}


\subsection{What sort of secure hardware/firmware do you need on a chip to be able to retroactively verify samples?}

\subsection{How does physical sampling work?}

\subsection{What is the chain of trust for the Verifier believing the secure-hardware's operation?}

\subsection{How do we sample known chips to get weight snapshot hashes?}

Conveniently, the only thing that needs to be inspected is an individual server/blade, and not the entire network topology. Thus, we may not need datacenter access - blades/servers can be brought to a central location for inspection, and the datacenter's location itself does not even need to be disclosed.

\copied{The second type of inspection requires only that ICHA inspectors provide the datacenter with a list of MPU IDs, and that datacenter operators go through the datacenter and provide ICHA inspectors with (the servers containing) those same MPUs.
This does not require that inspectors have physical access to the datacenter; the servers can be brought to a nearby location.
Further, if there are sensitive components attached to the MPU (e.g. memory units or custom hardware), these can be removed and any on-chip persistent memory zeroed out (except those needed for hardware guarantees and protected by anti-tamper measures).
Inspectors can then verify that the IDs of each MPU correctly correspond to the list of IDs they provided.
They can also inspect and verify the integrity of any anti-tamper measures, and extract any metadata generated by the hardware guarantees.

Once the inspections are complete, the datacenter may no longer trust the chips accessed by the ICHA inspectors.
In this case the ICHA can reimburse the datacenter for the cost of the MPUs, and at the decision of the datacenter either destroy them or resell them on the second-hand market.

Note that, given a dedicated nation-state effort, it is likely that any quick inspection of anti-tamper measures or the integrity of hardware guarantees can be spoofed.
Thus, a subset of the sampled chips can be shipped to dedicated ICHA labs, where specialists can use more powerful equipment (e.g. electron microscopes) to analyze whether tampering occurred.
\begin{tcolorbox}[colback=white, boxrule=0.2mm,boxsep=0mm]
\begin{assump}
All tampering with tamper-proofed hardware guarantee mechanisms can be identified at a dedicated ICHA lab.
\end{assump}
\end{tcolorbox}
%\begin{assumption}\label{rd.allocate}{All tampering with tamper-proofed hardware guarantee mechanisms can be identified at a dedicated ICHA lab.}
%\end{assumption}
These or other ICHA labs can also be responsible for analyzing the hardware-guarantee metadata, for example to scan any logs of executed instructions for components of neural networks, which may then be useable to identify those networks if they ever appeared in the wild.
Since such metadata can be highly sensitive (as it represents a random draw from a datacenter's entire operations), it may be necessary to provide substantial assurances to datacenter operators, possibly by keeping metadata air-gapped and allowing an operator's representative to travel along with the inspectors and chips, and supervise any metadata inspections. 

How many chips' metadata must be inspected $s_2$ by the ICHA such that with probability $p=0.99$, all jobs utilizing at least a $b=0.01$ fraction of a datacenter's compute will have at least one chip's metadata (and hardware integrity) checked?
\begin{equation}
   s_2 = \frac{\log (1-p)}{\log b} \approx 459
\end{equation}}


\todo{Lay out why even a small number of randomly chosen hashes is sufficient to prevent lying about PoL w.h.p.}
\subsection{How could we know that we are doing (possibly exhaustive) uniform sampling over chips all of which are legitimately reporting their snapshot-hashes?}
\todo{Mention threat model: want to be resistant to nation-state-attackers, who have physical access to the chips.}
\todo{Highlight "chain of trust is possible" as an open question, and point out the parallel to nuclear.}
\todo{Lay out each of the steps in the chain of trust, and why it might be solvable.}

\copied{4. Tracking the supply chain
    1. There are a finite number of fabs capable of producing these chips - track all of them. (This should be relatively easy, but maybe call it out as an assumption.)
5. Monitoring the machines
    1. How do we monitor this in practice, in such a way that can’t be spoofed?
        1. What we want: only run firmware/software with some non-invasive guarantees.
        2. There’s a chain of trust here that is well-established: design+fabrication verifies hardware→hardware verifies signed firmware → firmware implements protocols/enforces specific software
            1. The Verifier needs to be able to verify every step of this chain of trust. The Prover also needs to know that their data is not being spied on.
        3. We need to assume that each of these steps can be hardened to the point of trustworthiness.
        4. Design + fab to hardware may require trusting the designer, which is unacceptable in some cases. However, projects like the Open Silicon HRoT, which are already broadly being rolled out, should be able to give exactly this sort of trust-level (so long as the Verifier can use forensics to prove to themselves that the relevant HRoT design is in fact implemented as described on-chip).
        5. A particular concern is hardware security attacks, as we’re assuming that nation-state Provers potentially have unrestrained physical access to the chip. In general, most cybersecurity guarantees break down in this setting.
        6. This can be solved in one of two ways:
            1. On-chip hardware + tamper-evident packaging. Then we just need to do a few random chip inspections - don’t even need physical access.
            2. For incompatible chips, host-devices with hardware security (e.g. CPU that coordinates GPUs), and facility inspections + sensors/cameras to certify no tampering occurred. This is also how the IAEA does inspections. (CONFIRM)
        7. Describe two types of inspections: per-chip (e.g. mail to a central location), or per-facility (if chips don’t have hardware-level enforcement)
        8. Also, if the firmware is replaced, it may be able to overwrite the NVRAM log. Thus, a necessary capability here is to prevent bad firmware from ever being loaded (without it being subsequently noticeable). This is what authenticated firmware gives us.}

\section{Practical Realities}
\subsection{Incentives}

\todo{Detail why each stakeholder wants this over the status quo.}
\copied{1. When considering a policy, we should ask: why would complying with this framework be in the interest of the relevant stakeholders?
2. The public
    1. The ability to pass AI laws is important
    2. However, a real concern here is concentration of power in the government, surveillance, etc.
    3. However, datacenter-scale compute is already a concentration of power. It is currently only accountable to large actors with the funds to purchase and operate it. This is a way of making this source of power accountable to the public.
3. Chipmakers
    1. Reduces likelihood of export controls reducing the size of the market further
4. AI companies
    1. 
5. Cloud providers/datacenter operators
    1. Enables rule-enforcement for AI laws, and ethical behavior
6. Governments
    1. Enables AI rule enforcement
    2. Enables AI arms control agreements - if they are mutually beneficial for multiple parties}
\subsection{Partial Solutions}
\todo{List 4 different intermediate partial solutions. Highlight that once those exist, broader implementation would be straightforward, much like the original NPT.}
\copied{1. There are many short-term objectives that would get us partial benefits of this system, and have many other compelling short-term justifications. Also, even if our goal is the full system, Rome wasn’t built in a day; the IAEA was itself based on Euratom and Atoms for Peace.
2. Creating a compute-ownership map, with all large concentrations of compute (e.g. owners of >20 chips)
3. Popularizing the voluntary reporting of PoLs of training runs, as an expected reporting requirement, and engaging in verification exercises.
4. Creating pilot “jointly-monitored” datacenters for Chinese academic institutions, housed in a neutral country, and inspected by both
    1. This would closely parallel the “Atoms for Peace” program run in the 1950s, which created the infrastructure needed for the eventual rollout of the NPT.}

\ot{5. Get cloud compute providers to implement RAM-hashing-in-firmware as a way to detect illegal AI usage. This is an easier use-case because we can trust the hardware isn't being tampered-with, and thus only need the first section.}

\appendix
\section{Analogies and disanalogies to nuclear}
\copied{1. main thing to highlight earlier when linking is that it’s debated by scholars why nuclear verification succeeded, and the similarities vs. differences with AI training will determine whether AI will succeed for a similar reason. We give arguments why it may be easier, and why it may be harder.
2. Analogies
    1. Both involve using a flow (centrifuges/chips) to  aggregate a stock (total training time)
    2. Small amounts are fine, large amounts are bad
    3. Positive economic use-cases that everyone should benefit from, negative misuse use-cases that we should limit to the extent possible.
3. Disanalogies
    1. With uranium, after the enrichment occurs, you can still track the physically-produced uranium. (I.e. violations are reversible.) With compute, once a model has been trained, it can be copied at will.
    2. With nuclear, most countries that have pursued their own nuclear program have eventually been able to discretely build their own centrifuges (e.g. with design-support from AQ Khan). With advanced compute, this seems very unlikely - fabs cost many billions, and are rarely spun up for a single purpose. (Even the US military failed at this.) Thus, compute supply chain is much more concentrated - less possible to “build an AI project in a bunker somewhere” detached from existing supply chains.
    3. With HEU, inspecting the end-product is sufficient to know how enrichment was done. With compute, it’s hard to determine “how much an NN was trained” without being provided additional information on the process by which we arrived at those weights.
    4. With HEU, we know ahead of time how much enrichment is sufficient to be dangerous (as there are physical requirements to causing a supercritical fission  reaction). However with AI, algorithmic progress means that the threshold for any particular dangerous use-case decreases over time. That said, “more compute” will always be riskier than “less compute”, and thus a useful heuristic for regulation. Even if relevant thresholds need to change over time, so long as the requirements do not shrink to the point where detection would be impossible, it is still just as important to have a governing framework for compute.
    5. With HEU, enrichment must happen (mostly) at the same location. With AI training, enrichment can be parallelized over the internet, although there is some critical concentration required per location.
    6. AI is much more civilian-economically valuable than uranium, and implemented with more parts of the supply chain. Many analysts believe that part of the success of the NPT is that states do not have a strong incentive to cheat as they’re part of security alliances; it is not clear whether this will be the same for AI. It may be that similar alliances are required.
4. Inspection mechanisms from nuclear we’d like to copy
    1. Tracking centrifuge production
    2. Centrifuge flow monitoring
    3. Accounting for total usage of centrifuges (i.e. being able to show results)
}

\section{Case study in a way this could break: student-teacher}
\ot{One way to break this scheme: rather than one single long training run, to evade detection, the model can be repeatedly self-distilled into a new model, and then that new model trained for an additional period. This increases a compute-overhead, but so long as the distilling time does not scale linearly with the training time, this can make it possible to hide longer runs as a series of shorter runs. However, the total compute required (and the total time that the in-RAM model is of sufficiently-low-loss to trigger an audit) is still large, meaning that a randomly-sampled chip would still need to attest to being part of a dangerous training run.
Note also that this behavior would look kind of weird, because each self-distillation run would be using a very large number of chips *in parallel* for a short time. This is because such self-distillation-chains must occur sequentially across time, and at each timestep all the resources go into a particular snapshot.}

\end{document}

