
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}

\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{cuted}
\usepackage{amsthm}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\usepackage{dsfont}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)
\newcommand{\yxmu}[1]{{\textcolor[RGB]{142, 207, 201}{[(Mu): #1]}}}

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
%   \usepackage[nocompress]{cite}
  \usepackage{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
   \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath
\DeclareMathOperator{\Tr}{Tr}




% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Shihao Zou, Yuxuan Mu, Xinxin Zuo, Sen Wang, Li Cheng
        % <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem S. Zou, Y. Mu, X. Zuo, S. Wang and L. Cheng are with the Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada, T6G 2W3. (E-mail: szou2@ualberta.ca, lcheng5@ualberta.ca) Li Cheng is the corresponding author.\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
}% <-this % stops an unwanted space
\thanks{Manuscript received ...; revised ....}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
Event camera, as an emerging biologically-inspired vision sensor for capturing motion dynamics, presents new potential for 3D human pose tracking, or video-based 3D human pose estimation. However, existing works in pose tracking either require the presence of additional gray-scale images to establish a solid starting pose, or ignore the temporal dependencies all together by collapsing segments of event streams to form static event frames. Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs, a.k.a. \textit{dense deep learning}) has been showcased in many event-based tasks, the use of ANNs tends to neglect the fact that compared to the dense frame-based image sequences, the occurrence of events from an event camera is spatiotemporally much sparser.  
Motivated by the above mentioned issues, we present in this paper a dedicated end-to-end \textit{sparse deep learning} approach for event-based pose tracking: 1) to our knowledge this is the first time that 3D human pose tracking is obtained from events only, thus eliminating the need of accessing to any frame-based images as part of input; 2) our approach is based entirely upon the framework of Spiking Neural Networks (SNNs), which consists of Spike-Element-Wise (SEW) ResNet and a novel Spiking Spatiotemporal Transformer; 3) a large-scale synthetic dataset is constructed that features a broad and diverse set of annotated 3D human motions, as well as longer hours of event stream data, named SynEventHPD. Empirical experiments demonstrate that, with superior performance over the state-of-the-art (SOTA) ANNs counterparts, our approach also achieves a significant computation reduction of 80\% in FLOPS. Furthermore, our proposed method also outperforms SOTA SNNs in the regression task of human pose tracking. Our implementation is available at \href{https://github.com/JimmyZou/HumanPoseTracking\_SNN}{https://github.com/JimmyZou/HumanPoseTracking\_SNN} and dataset will be released upon paper acceptance.
\end{abstract}

%making it a growing interest in finding methods to process events more efficiently.
%In the spiking transformer, our proposed attention score of normalized Hamming similarity between binary spike vectors is proven to be equivalent to inner product similarity between real valued vectors used in the standard transformer. 
%Furthermore, as large-scale datasets are crucial for the success of data-driven approaches in event-based vision tasks, 
%empirically it is shown to lead to both better performance \& greater computation efficiency than that of the ANNs counterpart

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Human Pose Tracking, Event Camera, Spiking Neural Networks
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\newcommand{\beforefigcaption}{\vspace{-8mm}}
\newcommand{\afterfigcaption}{\vspace{-4mm}}
\newcommand{\beforetab}{\vspace{-4mm}}
\newcommand{\aftertab}{\vspace{-4mm}}
\newcommand{\beforesection}{\vspace{-3.5mm}} 
\newcommand{\aftersection}{\vspace{-1mm}}
\newcommand{\beforesubsection}{\vspace{-3.5mm}}
\newcommand{\aftersubsection}{\vspace{-1mm}}
\newcommand{\beforesubsubsection}{\vspace{-3mm}}
\newcommand{\aftersubsubsection}{\vspace{-1mm}}

\beforesection
\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
\aftersection

% start with human pose estimation, then introduce event camera
As an important area of research in computer vision, visual human pose tracking has attracted increasing research attentions in recent years. While most current research efforts have been focused on RGB or depth cameras~\cite{shotton2012efficient,zhao2017simple,wang20193d,xu2017lie,wang2014robust,zhou2016sparseness,park20163d,li2015maximum,kanazawa2018end,pavlakos2018learning,xu2019denserac,kolotouros2019learning,lin2021end,humanMotionKanazawa19,kocabas2019vibe,yuan2022glamr}, event camera~\cite{gallego2019event}, as an emerging vision sensor, presents new opportunities in this area.  As a novel and biologically-inspired vision system, event camera is considerably dissimilar to the conventional frame-based cameras. In particular, by adopting its unique asynchronous and independent address-event representation, event camera is capable of imaging high-speed motions with a very low power consumption. This innovative imaging paradigm has sparked a multitude of research efforts in the field of event-based vision, such as tracking~\cite{gehrig2018asynchronous,mitrokhin2018event,zhang2021object,zhang2022spiking}, recognition~\cite{amir2017low,kim2022ev,fang2021deep}, 3D reconstruction~\cite{rebecq2018emvs,zhang2022discrete}, and a diverse range of applications in robotics, augmented and virtual reality, and autonomous driving~\cite{gallego2019event}. 

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\columnwidth]{figures/tease.pdf}
    \beforefigcaption
    \caption{\textbf{Overview of our approach.} An end-to-end sparse deep learning approach is proposed for 3D single-person pose tracking over time from event streams only. Specifically, our method incorporates SEW ResNet as its backbone, and a novel Spiking Spatiotemporal Transformer to address one-directional temporal dependency issue in SNNs.}
    \afterfigcaption
    \vspace{-3mm}
    \label{fig:tease}
\end{figure}

% 1. pose estimation from events and gray-scale images (three)
% 2. it has been shown in prior works (hand pose) ANN is able to achieve good performance on pose estimation from events only, but events are much sparser, consider more efficient solution
% 3. SNNs is the solution, but existing works on classification tasks, rather than regression tasks
% 4. we are the first attempt to apply SNNs on difficult regression tasks
% 5. data driven approaches reply on event based data, we propose the largest one for human pose tracking with a variety of poses


% existing works on human pose tracking, problems 1. not use events only, 2. not tracking
Recently, data-driven approaches have shown their potential for effective pose estimation from event cameras~\cite{calabrese2019dhp19,xu2020eventcap,zou2021eventhpe,rudnev2021eventhands}. One of the earliest approaches~\cite{calabrese2019dhp19} uses Convolutional Neural Networks (CNNs) to estimate 2D human poses from event frames. EventCap~\cite{xu2020eventcap} expands upon this by capturing fast 3D human motion based on an event stream, as well as a sequence of gray-scale images to establish initial poses over time. The most recent approach, EventHPE~\cite{zou2021eventhpe}, reduces the reliance on gray-scale images by using only the first gray-scale frame to extract the starting pose, then relying solely on the event stream for subsequent pose tracking. The concurrent work of~\cite{rudnev2021eventhands} instead focuses on static hand pose estimation from event camera, by engaging an CNNs method. Unfortunately, existing methods either require additional gray-scale images~\cite{xu2020eventcap,zou2021eventhpe} which are too dense for practical efficiency, or treat the event stream as a static event frame~\cite{calabrese2019dhp19,rudnev2021eventhands}, thus discarding temporal dependencies that could be critical in pose tracking. As a result, the full potential of human pose tracking using only the event stream remains largely unexploited.

% introduce why consider SNNs not ANNs, problems 1. ANN2SNN (quantization error) and training SNN both verify on classification tasks, 2. SNNs one-directional temporal dependency, 3. prior efforts use in-efficient ANNs
% \textcolor{red}{add the analysis of \cite{yao2023attention,zhou2022spikformer}}
Meanwhile, artificial neural networks (ANNs), such as ResNet~\cite{he2016deep} and Transformer~\cite{vaswani2017attention}, have demonstrated their potential in various event-based vision tasks~\cite{zhu18evflownet,amir2017low,rudnev2021eventhands,wang2020eventsr,fang2021deep,fang2021incorporating,kim2022ev,mitrokhin2018event,zhang2021object,sun2022ess,gehrig2019end,gehrig2020video}. However, compared with dense RGB or gray-scale images, event streams are \textit{spatiotemporally much sparser}, resulting in a growing interest in seeking ways to process events more efficiently. One promising strategy is based on the Spiking Neural Networks (SNNs). Unlike traditional ANNs, spiking neurons are employed in SNNs to imitate the event generation process, thus bypassing the unnecessary computations in inactive or non-spiking neurons. Previous efforts have shown the superiority of SNNs in classification tasks, including converting ANNs to SNNs~\cite{rueckauer2017conversion,han2020rmp,li2021free,yan2021near,deng2021optimal}, or training SNNs from scratch~\cite{fang2021deep,fang2021incorporating,li2021differentiable,yao2022glif,yao2023attention,zhou2022spikformer}. There are also attempts~\cite{yao2021temporal,zhang2022spiking} proposing a mixed framework of SNNs and ANNs to balance the efficiency and performance in event-based regression tasks. 
However, the challenge of conducting pose tracking solely using event signals, and exclusively engaging the SNNs architecture to fully exploit the innate sparsity in events data, remains unaddressed.
% It, however, remains unaddressed to conduct pose tracking solely using event signals, and employing only the SNNs architecture, in order to fully exploit the innate sparsity in the events data. 
This may be attributed to the following three challenges. (i) Unlike spike votes used in classification, regression is sensitive to the output values, which may result in additional quantization errors in pose prediction due to the compact spike representation in SNNs. (ii) As opposed to high-level label prediction in the static classification tasks, pose tacking requires fine-grained regression of pose over time. (iii) Spikes are typically unfolded over time, which naturally preserves only one-directional temporal dependency in SNNs. This may lead to insufficient pose information, especially when the character is not moving in the starting phase and thereby few events are observed for pose estimation. 

% 1. investigate the new problem, 2. Propose SNNs, 3. To address one-directional temporal dependency, propose spike spatiotemporal transformer, 4. dataset (next paragraph)
Motivated by the above observations, our work aims to tackle a relatively new problem of tracking 3D human poses solely based on event streams from an event camera, thus completely eliminating the need for additional input dense images. As presented in Fig.~\ref{fig:pipeline}, our approach is an end-to-end sparse deep learning approach that estimates parametric human poses over time solely from events. This model is entirely built upon SNNs, thus having the promise of being more efficient than that of the dense deep learning models (i.e. ANNs). The input event stream goes through a preprocessing step to form a sequence of event voxel grids; Spike-Element-Wise (SEW) ResNet~\cite{fang2021deep} is then employed as the SNNs backbone to extract pose spike features; this is followed by our proposed Spiking Spatiotemporal Transformer that carries bi-directional fusion of the acquired pose spike features, allowing for the distribution of pose information especially to those at the early time intervals. In our spiking transformer, the attention score between binary spike vectors utilizes a normalized Hamming similarity, which, as shown in Proposition~\ref{proposition:hamming-similarity}, amounts to the scaled dot-product similarity between the real valued vectors used by the standard transformer~\cite{vaswani2017attention}. In the final step, 2D average pooling is applied to the spatiotemporally aggregated spike features, which is followed by a direct regression to output the parametric 3D human poses over time. 

%Additionally, we provide a large-scale synthetic dataset, SynEventHPD, for event-based 3D human pose tracking. Although an in-house event-based motion capture dataset, MMHPSD, is introduced in~\cite{zou2021eventhpe}, its limited variety of motions restricts the generalization ability of trained models. \textcolor{red}{To overcome this challenge, we propose to synthesize events data from multiple motion capture datasets (\textit{i.e. Human3.6M~\cite{ionescu2013human3}, AMASS~\cite{mahmood2019amass}, PHSPD~\cite{zou2022human} and MMHPSD-Gray~\cite{zou2021eventhpe}}), which provides a variety of motions not included in MMHPSD, such as juggling, moon-walking, jumping rope, vaulting, and scampering. By combining these sub-datasets together, we finally offer a total of 45.72 hours event streams with the details summarized in Tab.~\ref{tab:dataset-summary}.} This dataset will be instrumental in further facilitating the research on event-based vision and SNNs for human pose tracking.

Our contributions can be summarized as follows:
\begin{itemize}
% \vspace{-1.5mm}
    \item This work addresses a relatively new task of 3D human pose tracking \textbf{solely based on events} from an event camera.
    % \item An end-to-end SNN-based approach is proposed. In particular, it includes a novel \textbf{Spiking Spatiotemporal Transformer} module to address the one-directional temporal dependency issue, enabling the flow of pose related information to facilitate pose estimation of especially those in the early time steps. 
    \item We propose an end-to-end SNN-based approach, which specifically incorporates a novel \textbf{Spiking Spatiotemporal Transformer} module to tackle the one-directional temporal dependency issue. This allows for the propagation of pose-related information to facilitate pose estimation especially for the early time steps.
    %Our proposed attention score, normalized Hamming similarity between binary spike vectors, has been shown to be equivalent to inner product similarity between real valued vectors used in the standard transformer~\cite{vaswani2017attention}.
    %The experiments show that our proposed SNNs achieve competitive or even better performance of pose tracking than previous works~\cite{xu2020eventcap,zou2021eventhpe} and also several ANNs (\textit{i.e. ResNet-GRU~\cite{kocabas2019vibe}, ResNet-TF~\cite{carion2020end} and Video-Swin~\cite{liu2021swin}}), but requires only about 20\% of FLOPs, showcasing the promising prospects of efficient SNNs on various event-based vision tasks.
    %
    Extensive empirical experiments demonstrate the superior performance of our approach over existing SOTA methods, including EventCap~\cite{xu2020eventcap}, EventHPE~\cite{zou2021eventhpe} and a few ANN baselines~\cite{kocabas2019vibe,carion2020end,liu2021swin}. Further, this is achieved by utilizing merely around 20\% of the computation (in FLOPs) required by the SOTA methods. Furthermore, our proposed method also outperforms SOTA SNN baselines~\cite{fang2021deep,yao2023attention,zhou2022spikformer} in this regression task of human pose tracking.
    \item \textbf{A large-scale dataset, SynEventHPD}, is constructed for the task of event-based 3D human pose tracking. It consists of synthesized events data from multiple motion capture datasets (\textit{i.e. Human3.6M~\cite{ionescu2013human3}, AMASS~\cite{mahmood2019amass}, PHSPD~\cite{zou2022human} and MMHPSD-Gray~\cite{zou2021eventhpe}}). Consequently, it covers a variety of motions such as juggling, moon-walking, jumping rope, vaulting and scampering, with a total size of 45.72 hours event streams -- more than 10 times larger than MMHPSD~\cite{zou2021eventhpe}, the largest existing event-based pose tracking dataset. The details are summarized in Tab.~\ref{tab:dataset-summary}, and empirical studies have suggested the usefulness of our new dataset. %After combining these datasets together, we are able to offer a total of 50.11 hours event streams. This dataset will greatly facilitate future research in the field of event-based vision and SNNs.
\end{itemize}

\beforesection
\section{Related Work}
\aftersection

Here we provide a concise overview of the related efforts. More detail could be found in the supplementary file.

\textbf{Human pose estimation} from RGB or depth images has been a popular topic in computer vision in recent years. 
Prior to the deep learning era, research efforts are mainly based on random forest~\cite{shotton2012efficient,xu2017lie} or dictionary learning~\cite{wang2014robust,zhou2016sparseness}. Benefited from the significant performance boosts brought by deep learning, recent efforts in human pose estimation either directly regress 3D pose from images~\cite{park20163d,li2015maximum} or lift 2D pose estimation to 3D~\cite{zhao2017simple,wang20193d}. This trend is further fueled by the development of SMPL~\cite{loper2015smpl}, a parametric human shape model of low-dimensional statistical representation. 
HMR~\cite{kanazawa2018end} is the first such effort in applying convolutional neural networks for human SMPL shape recovery from single images, which produces impressive results. 
% This is followed by a number of works that further include silhouettes~\cite{pavlakos2018learning}, texture map~\cite{xu2019denserac}, 2D pose~\cite{kolotouros2019learning} or vertex-joint interactions~\cite{lin2021end} for the human shape recovery. 
Several recent efforts also emphasize on the exploitation of temporal information in inferring human poses and shapes from videos, including e.g. temporal constraints~\cite{humanMotionKanazawa19,kocabas2019vibe,wei2022capturing}, dynamic cameras~\cite{yuan2022glamr} or event signals~\cite{xu2020eventcap,zou2021eventhpe}. 
% Sensing modalities other than the classical RGB or depth images have also been explored for human pose estimation, including polarization image~\cite{zou2022human}, IMUs~\cite{von2016human} and head-mounted devices~\cite{zhang2022egobody}.

% \textbf{Event camera}~\cite{gallego2019event}, as a new bio-inspired technology of silicon retinas, differs notably from the conventional frame-based imaging sensors, such as RGB or Time-of-Flight cameras, including but not limited to its asynchronous and independent \textit{address-event} representation. The output of an event camera is a sequence of “events” or “spikes”. Let the binary polarity status $p$ indicating the brightness increase or decrease: each readout event may be represented as a tuple $(\mathbf{x}, t, p)$, when the brightness change (\textit{event}) at pixel position $\mathbf{x}$ (\textit{address}) exceeds a preset threshold at time $t$. Instead of densely capturing pixel value at a fixed frame rate for frame-based cameras, event camera records the intensity change for each of the pixels asynchronously and independently, in case an motion occurs. Hence the temporal resolution of event camera is much higher than conventional frame-based cameras. Moreover, as its output consisting of a spatially much sparser stream of motion events, an event camera typically consume considerably less energy in operation. 
%because event camera typically responds to visual stimulus or local motions in the scene, rather than a full stack of pixel intensity.
%The amount of output events per second depends on the speed of the motion in the scene. 

\textbf{Event-based vision} applications have witnessed a substantial increase in recent years.
% including camera pose estimation~\cite{gallego2017event}, feature tracking~\cite{gehrig2018asynchronous}, optical flow~\cite{zhu18evflownet,hagenaars2021self}, multi-view stereo~\cite{rebecq2018emvs,zhang2022discrete}, hand gesture recognition~\cite{amir2017low} and pose estimation~\cite{rudnev2021eventhands}, motion deblurring~\cite{jiang2020learning,sun2022event}, image restoration and super-resolution~\cite{wang2020eventsr}, image classification~\cite{fang2021deep,fang2021incorporating}, object recognition~\cite{kim2022ev} and tracking~\cite{mitrokhin2018event,zhang2021object,zhang2022spiking}, semantic segmentation~\cite{sun2022ess}, events from/to video~\cite{gehrig2019end,gehrig2020video}, depth estimation~\cite{zhang2022spikedepth}, among others. 
As for human pose tracking, DHP19~\cite{calabrese2019dhp19} is perhaps the first effort in engaging event camera for human 2D pose estimation using CNNs. EventCap~\cite{xu2020eventcap} aims to capture 3D motions from both events and gray-scale images provided by an event camera. It starts with a pre-trained CNN-based 3D pose estimation module that takes a sequence of low-frequency gray-scale images as input; the estimated poses are then used as the initial state to infill the intermediate poses for high-frequency motion capture with the constraint of detected event trajectories by~\cite{gehrig2018asynchronous}. These methods, however, require full access to the corresponding gray-scale images as co-input. EventHPE~\cite{zou2021eventhpe} reduces this demand by the milder requirement of only a single gray-scale image of the starting pose. In doing so, a dedicated CNNs module is trained to infer optical flow by self-supervised learning, which is used alongside with the input event stream to reconstruct the 3D full-body shapes. Compared with these existing efforts~\cite{xu2020eventcap,zou2021eventhpe}, our approach requires only the events as input, where a novel SNN-based framework is utilized to produce better shape reconstruction results with much less computation footprint.
%spiking transformer for the bidirectional fusion of temporal information, achieving competitive performance with only around 20\% of the computational cost of traditional ANNs.


\textbf{Event-based datasets} are crucial for data-driven approaches to attain their satisfactory performance. Unfortunately, existing benchmark datasets~\cite{deng2009imagenet,ionescu2013human3} are mostly based on conventional RGB or depth cameras -- thus are infeasible to be directly used in event-based tasks, given the fundamental differences between event and conventional cameras. 
% This have motivated a variety of event-based datasets released in recent years, including DvsGesture~\cite{amir2017low} for hand gesture recognition, CIFAR10-DVS~\cite{li2017cifar10} and ES-ImageNet~\cite{lin2021imagenet} for object classification, DSEC-Semantic~\cite{sun2022ess} for semantic segmentation and EED~\cite{mitrokhin2018event} and FE108~\cite{zhang2021object} for object tracking. 
As for event-based human pose estimation, DHP19 dataset~\cite{calabrese2019dhp19} is the earliest one, which however  contains rather limited amount of events data that also lack pose variety. The most recent released dataset is MMPHSPD~\cite{zou2021eventhpe} built with event camera and 3 other imaging devices. Although the dataset provides more than 4.5 hours event stream and 21 different types of action, it may still lack sufficient degree of pose variety, partly due to its in-house capturing set-up. In this work, we significantly augment the MMPHSPD dataset by synthesizing events data from several human motion capture benchmark datasets~\cite{ionescu2013human3,mahmood2019amass,zou2022human,zou2021eventhpe} -- it gives rise to a large-scale dataset, SynEventHPD, containing a rich variety of poses for the task of event-based human pose tracking. This dataset will be instrumental in further facilitating the research on event-based vision and SNNs for human pose tracking.

% \textbf{Spiking neural networks} have been an emerging learning framework. Spiking neuron, the basic element in SNNs, works by imitating the transmitting mechanism in mammalian’s visual cortex~\cite{gallego2019event}. A spiking neuron maintains a membrane potential, which could be changed only when spikes (\textit{events}) are received from its connected preceding neurons. A spike is produced when the neuronal potential exceeds a preset threshold. Different from the neuron in traditional ANNs, no output would be produced by spiking neurons as long as their potentials are below  threshold, thus no computation is taken place -- the root cause of the remarkable efficiency and sparsity of SNNs when comparing to the dense and computational-heavy ANNs. 

\textbf{Training large-scale SNNs} from scratch presents a significant challenge. To alleviate the non-differentiable issue of neuronal spiking function, one branch of works focus on converting trained ANNs to SNNs~\cite{rueckauer2017conversion,han2020rmp,li2021free,yan2021near,deng2021optimal}. Typically, such ANN2SNN methods map the non-linear activation layer in a trained ANNs to a neuron spiking layer and scale the threshold or the weights accordingly. It is worth noting that only in the realm of classification tasks, excellent results have been demonstrated by the SNNs methods; in fine-grained regression tasks and specifically in human pose estimation, the SNNs performance is still inadequate. Another branch of works, on the other hand, focus on training SNNs from scratch, often by following the back-propagation through time (BPTT) framework and applying surrogate derivatives~\cite{li2021differentiable} to approximate the gradient of neuronal spiking function. This line of works have delivered impressive performance in classification tasks~\cite{fang2021incorporating,fang2021deep,li2021differentiable,yao2022glif,yao2023attention,zhou2022spikformer} as well as regression tasks~\cite{hagenaars2021self}. Our proposed work can be regarded as an attempt along this direction, with its focus on the fine-grained regression task of full-body pose tracking. 
%There have also been efforts~\cite{yao2021temporal,zhang2022spiking} in proposing mixed frameworks blending SNNs and ANNs, in order to maintain a good balance in efficiency and performance for event-based tasks. 


\textbf{Spiking Transformer} has emerged very recently as a new SNNs architecture. To avoid confusion, it is important to note that the spiking transformers presented in~\cite{zhang2022spiking,zhang2022spikedepth} are not SNN-based transformers, but rather ANN-based or mixed models. The two recent works~\cite{yao2023attention,zhou2022spikformer} are most related to our proposed spiking spatiotemporal transformer. In MA-SNN~\cite{yao2023attention}, multi-dimensional attention is proposed in an SNNs framework, yet this attention is instead based on real values of membrane potentials, thus in a sense violating the efficiency design of SNNs. In contrast, the spatiotemporal attention mechanism in our model is entirely based on spike tensors, which is realised by the proposed Hamming similarity score. Our work is also very different from Spikformer~\cite{zhou2022spikformer}, where the scaled dot-product is directly adopted to compute the similarity score. As analysed in Sec.~\ref{sec:spike-transformer}, this function may not be well-defined for binary vectors.


% % moved to supplementary
% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{figures/neural_model.pdf}
%     \caption{\textbf{(a) Illustration of spiking neuron model.} A LIF spiking neuron maintains a membrane potential and modifies it when receiving spiking trains from its connected neurons. The neuron will generate output spikes when its potential exceeds a threshold and then reset the potential. \textbf{(b) Feedforward in SNNs.} This process includes potential leaking and charging, neuron spiking and potential resetting. Feed-forwarding typically rolls over time and propagates from layer $l-1$ to layer $l$. \textbf{(c) Backpropagation Through Time in SNNs.} The gradients are normally computed through time and then back-propagated from layer $l$ to layer $l-1$.}
%     \label{fig:neuron_model}
% \end{figure*}

\begin{figure*}[htp]
    \centering
    \includegraphics[width=\textwidth]{figures/pipeline.pdf}
    \beforefigcaption
    % \vspace{4mm}
    \caption{\textbf{Pipeline of our sparse deep learning approach}. It contains four main sections: 
    \textbf{(i) Preprcessing} in Sec.~\ref{sec:preprocess} converts a stream of events into a sequence of event voxel grids of the same temporal length. 
    \textbf{(ii) SEW-ResNet}~\cite{fang2021deep}, introduced in Sec.~\ref{sec:sew-resnet}, is used as backbone to extract pose spike features given the input event voxel grids. Hidden neurons in spiking (active) status are shown in blue. 
    \textbf{(iii) A novel spiking spatiotemporal transformer} is proposed in Sec.~\ref{sec:spike-transformer} for the bi-directional fusion of pose spike features. The proposed spiking spatiotemporal attention allows bi-directional flow of information over time in SNNs, thus facilitating pose estimation especially for the early time steps.
    \textbf{(iv) Parametric human poses tracking} is presented in Sec.~\ref{sec:regression}, where average pooling is applied to the spatiotemporally aggregated spike features before regressing the human pose and shape parameters.
    }
    \afterfigcaption
    \vspace{-2mm}
    \label{fig:pipeline}
\end{figure*}

\beforesection
\section{Preliminary}
\label{sec:preliminary}
In this section, we will discuss spiking neuron model, feedforward process in SNNs, computational consumption of SNNs, and the attention mechanism in the standard transformer. Details on backpropagation can be found in the supplementary material. 

\textbf{Spiking neuron model} commonly refers to the leaky integrate and fire (LIF) model, a fundamental unit in SNNs. A LIF neuron maintains a membrane potential $u_{[t]}$ with a leaky constant $\tau$, which may be modified only when new spiking trains $X_{[t]}$ are received from its connected neurons in $T$ time steps. The neuron then outputs a spike $s_{[t]}$ and reset its potential by $V_{\text{th}} - u_{\text{rest}}$, if its potential exceeds a predetermined threshold, $V_{\text{th}}$, where soft reset~\cite{fang2021deep} is adopted in our work. The model is formulated as follows:
\begin{align}
    & h_{[t]} = u_{[t-1]} - \frac{1}{\tau}(u_{[t-1]} - u_{\text{rest}}) + X_{[t]}, \label{eq:neuron1}\\
    & s_{[t]} = \Theta (h_{[t]} - V_{\text{th}}), \label{eq:neuron2} \\
    & u_{[t]} = h_{[t]} - (V_{\text{th}} - u_{\text{rest}})s_{[t]}, \label{eq:neuron3}
\end{align}
where $\Theta$ is the Heaviside step function,
\begin{equation}\small
    \Theta(h_{[t]} - V_{\text{th}}) =
    \begin{cases}
    1, & \text{if } h_{[t]} - V_{\text{th}} \geq 0\\
    0, & \text{otherwise.}
    \end{cases}
\end{equation}

\textbf{Feedforward in SNNs} consists of multiple layers of connected spiking neurons. Assume there are $N^{(l)}$ neurons in the $l$-th layer, and use the vector forms of $\mathbf{u}^{(l)}_{[t]} \in \mathbb{R}^{N^{(l)}}$ and $\mathbf{s}^{(l)}_{[t]} \in \{0, 1\}^{N^{(l)}}$ to represent their respective membrane potentials and output spikes at time step $t$. Let $\mathbf{W}^{{l}}\in \mathbb{R}^{N^{(l)}\times N^{(l-1)}}$ denote the connecting weights between layer $l-1$ and $l$, $\lambda=1-\frac{1}{\tau}$ be the leaky constant of LIF neuron model, and set $u_{\text{rest}}$ to 0, feedforward in SNNs becomes
\begin{align}
    \label{eq:neuron5} 
    \mathbf{h}_{[t]}^{(l)} & = 
    \underbrace{\lambda \mathbf{u}^{(l)}_{[t-1]}}_{\text{leak}} + 
    \underbrace{\mathbf{W}^{(l)} \mathbf{s}^{(l-1)}_{[t]}}_{\text{charge}}, \\
    \label{eq:neuron6} 
    \mathbf{s}^{(l)}_{[t]} & = \underbrace{
    \Theta (
        \mathbf{h}_{[t]}^{(l)} - V_{\text{th}}
    )}_{\text{spike}}, \\
    \label{eq:neuron7}
    \mathbf{u}^{(l)}_{[t]} & = 
    \mathbf{h}_{[t]}^{(l)}
    \underbrace{- V_{\text{th}}\mathbf{s}^{(l)}_{[t]}}_{\text{reset}}. 
\end{align}
%where. \yxmu{It's okay to state this after the formula. But it can also elaborate in "*here". The logic is how we modify formula 1.2.3 to get 5.6.7}

\textbf{Computational consumption of SNNs} is often lower than ANNs, partly owing to the binary output of spiking neurons. Considering binary spikes $\mathbf{s}^{(l-1)}_{[t]}$ in layer $l-1$ and $\mathbf{s}^{(l)}_{[t]}$ in layer $l$, the computations of inactive neurons ($s_{[t]}=0$) can be skipped. Assuming the spiking rate is $\rho$ for the layer, from Eq.~(\ref{eq:neuron5}) and~(\ref{eq:neuron6}), a linear layer plus a spiking layer in SNN requires merely $\mathcal{O}(\rho TN^{(l-1)} N^{(l)})$ FLOPs~\footnote{FLOPs refers to the number of floating point operations.} for $T$ time steps, while in ANNs, a linear layer plus a non-linear ReLU layer requires $\mathcal{O}(TN^{(l-1)} N^{(l)})$ FLOPs. Normally, $\rho$ is around 20\% in average, which means SNNs require only around $20\%$ FLOPs of that required by ANNs. 

\textbf{Scaled dot-product attention} is defined as
\begin{equation}\small
    \label{eq:attention}
    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d_k}})\mathbf{V},
\end{equation}
in the standard transformer~\cite{vaswani2017attention}, where $\mathbf{Q}, \mathbf{K} \in \mathbb{R}^{N\times d_k}$ are queries and keys and $\mathbf{V}\in \mathbb{R}^{N\times d_v}$ is values, with $N$ being the length of input sequence, $d_k$ the dimension of a single query $\mathbf{q}$ or key $\mathbf{k}$, and $d_v$ the dimension of a single value $\mathbf{v}$. The scaling factor of $1/\sqrt{d_k}$ is to normalize the dot-product $\mathbf{q}\mathbf{k}^{\top}$ to have mean 0 and variance 1, assuming the components of $\mathbf{q}$ and $\mathbf{k}$ are independent variables with mean 0 and variance 1.

\beforesection
\section{Our approach}
\aftersection

As summarized in Fig.~\ref{fig:pipeline}, our approach consists of four main sections. (i) Preprcessing in Sec.~\ref{sec:preprocess}, is to convert an stream of event into a sequence of event voxel grids~\cite{gallego2019event} with the same time interval. (ii) SEW-ResNet~\cite{fang2021deep} in Sec.~\ref{sec:sew-resnet}, is employed as the backbone to extract pose spike features from the input sequence of event voxel. (iii) Since SEW-ResNet only considers the one-directional temporal relationship, we propose a novel Spiking Spatiotemporal Transformer for the bi-directional fusion of pose spike features in Sec.~\ref{sec:spike-transformer}, allowing for the compensation of missing pose information, especially for the early time steps. (iv) The final stage, illustrated in Sec.~\ref{sec:regression}, is to apply average pooling to the spatiotemporally aggregated spike features and then regress the parametric poses over time. In this work, a stream of events is the sole source of input, thus eliminating the reliance on gray-scale input images as in~\cite{xu2020eventcap} or a prior knowledge of the starting pose as in~\cite{zou2021eventhpe}. Furthermore, our model is completely built upon SNNs instead of traditional ANNs or mixed architecture. 

\beforesubsection
\subsection{Preprocessing}
\aftersubsection
\label{sec:preprocess}
Instead of a sequence of RGB frames captured by an RGB camera, an asynchronous stream of independent event signals is assembled by an event camera as the input signals. This event stream is decomposed into a sequence of $T$ packets of events, with each packet spanning the same time length, $\mathcal{E}=\{\mathcal{E}_{[t]}\}_{t=1}^T$. Here, $t$ indexes a specific event packet in the sequence. Following~\cite{gehrig2019end,zou2021eventhpe}, an event packet, $\mathcal{E}_{[t]}$, is represented as a voxel grid $H\times W\times C$ with each voxel corresponding to a particular spatial and temporal interval. The voxel value will be 1 if the number of events within the voxel is larger than a preset threshold, and 0 otherwise. This representation better preserves the temporal information of events, rather than collapsing them onto a single frame as mentioned in~\cite{gallego2019event}. The processed sequence of event voxel grids is then fed into SNNs as input, denoted as $\mathbf{S}^{\text{in}}\in \{0, 1\}^{T\times H\times W\times C}$. \footnote{
For clarity, we will generally refer to the size of input spike tensor for different blocks or modules as $\footnotesize T\times H\times W\times C$ in subsequent sections.}

\beforesubsection
\subsection{Spike-Element-Wise Residual Networks}
\aftersubsection
\label{sec:sew-resnet}
% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{figures/spiking_cnn.pdf}
%     \caption{\textbf{Architecture of SEW-ResNet34.} Given the input spike tensor $\scriptstyle \mathbf{S}^{\text{in}}\in \{0, 1\}^{T\times H\times W\times C}$, the output spike tensor is $\scriptstyle \mathbf{S}^{\text{out}}\in \{0, 1\}^{T\times \frac{H}{32}\times \frac{W}{32}\times C^{\text{out}}}$ where $\scriptstyle C^{\text{out}}=512$ for SEW-ResNet34. SEW-ResNet is comprised of two types of blocks: the downsample block and the basic block. The downsample block reduces the spatial size of the input spike tensor by 2 and increases the channel size by 4 through convolutional layers, while the basic block maintains the size of the input spike tensor for residual learning in SNNs. The final layer in both types of blocks is element-wise identity mapping via \textit{SEW Function}, where spike-element-wise functions between two input spike tensors are applied, such as ADD, AND or IAND.}
%     \label{fig:spiking-cnn}
% \end{figure}

SEW-ResNet, proposed in~\cite{fang2021deep}, ranks among the most popular SNNs architectures. Originated from ResNet~\cite{he2016deep}, significant differences are made in the redesign of identity mapping for SNNs using the \textit{SEW Function}, which applies element-wise addition to spike tensors rather than the pre-spiking membrane potentials. This design not only establishes the identity mapping of residual learning in SNNs, but also addresses the vanishing or exploding gradient issue. In our pipeline, it is incorporated as the SNN backbone to extract spike pose features. The spatial size of the output is $1/32$ of the input with a channel size of $512$ for SEW-ResNet18, 34 and $2048$ for SEW-ResNet50, 101, 152. The detailed architecture of SEW-ResNet is presented in the supplementary material.

% We show the detailed architecture of SEW-ResNet34~\cite{fang2021deep} in Fig.~\ref{fig:spiking-cnn}. There are also options of SEW-ResNet18, 50, 101 and 152 which have a similar architecture to SEW-ResNet34. The spatial size of the output is $1/32$ of the input with a channel size $C^{\text{out}}$ of $512$ for SEW-ResNet18, 34 and $2048$ for SEW-ResNet50, 101, 152.

% % include in the caption
% SEW-ResNet consists of two types of blocks: the downsample block and the basic block. The downsample block normally reduces the spatial size of input spike tensor by 2 and expands the channel size by 4 through convolutional layers, while the basic block keeps the size of input spike tensor unchanged for residual learning. The final layer in both types of blocks is element-wise identity mapping via \textit{SEW Function}, where spike-element-wise functions between two input spike tensors are applied, such as ADD, AND or IAND. Given the input spike tensor $\mathbf{S}^{\text{in}}\in \{0, 1\}^{T\times H\times W\times C}$, the output spike tensor is assumed to be $\mathbf{S}^{\text{out}}\in \{0, 1\}^{T\times \frac{H}{32}\times \frac{W}{32}\times C^{\text{out}}}$ where $C^{\text{out}}=512$ for SEW-ResNet18 and 34, $C^{\text{out}}=2048$ for SEW-ResNet50, 101 and 152.

\beforesubsection
\subsection{Spiking Spatiotemporal Transformer}
\aftersubsection
\label{sec:spike-transformer}
\begin{figure}[ptb]
    \centering
    \includegraphics[width=\columnwidth]{figures/spiking_transformer.pdf}
    \beforefigcaption
    \caption{\textbf{(a) Architecture of our Spiking Spatiotemporal Transformer} that can be stacked by $N$ layers. \textbf{(b) Architecture of Spiking Spatiotemporal Attention} that enables bi-directional flow of information in SNNs, thus facilitate the circulation of pose related information particularly for pose estimation of the early time steps. Normalized Hamming similarity is proposed for the attention score, which is shown as on par to the inner product similarity between real valued vectors, as used in standard transformer.}
    \afterfigcaption
    \label{fig:spiking-transformer}
\end{figure}

\textbf{Spiking Spatiotemporal Transformer} is shown in Fig.~\ref{fig:spiking-transformer}~(i). Given the input $\mathbf{S}^{\text{in}}\in \{0,1\}^{T \times H \times W\times C}$, the first step is to apply spiking spatiotemporal attention to combine bidirectional space-time features. A more comprehensive explanation of the attention module will be given later. It
is followed by two linear spiking layer with batch normalization, also known as Feed-Forward Network (FFN) in the standard transformer~\cite{vaswani2017attention}. The final step in the module is to apply SEW Function to the output of FFN and input spike tensor for residual learning. Then we get the output $\mathbf{S}^{\text{out}}\in \{0,1\}^{T\times H\times W \times C}$. This entire module can be stacked by $N$ layers similar to the standard transformer~\cite{vaswani2017attention}.

\textbf{Spiking Spatiotemporal Attention} is illustrated in Fig.~\ref{fig:spiking-transformer}~(ii). To address the issue of one-directional temporal dependency flow in spiking layers of SNNs, in this module, self-attention is attended in the full space-time domain of spike tensors. Specifically, provided with the input spike tensor $\mathbf{S}^{\text{in}}\in \{0,1\}^{T\times H \times W \times C}$, to obtain the spike query and key tensors $\mathbf{S}^{q},\mathbf{S}^{k}\in \{0,1\}^{THW \times C_k}$, we use two linear spiking layers with batch normalization to map the channel size from $C$ to $C_k$, and then flatten along the spatiotemporal space $T\times H\times W$. Similarly, the real value tensor $\mathbf{V} \in \mathbb{R}^{THW\times C_v}$ is obtained by mapping the channel size to be $C_v$ via a linear layer without spiking and flatten along the spatiotemporal space. Next, the similarity scores between the spiking queries and keys are computed by $f(\mathbf{S}^{q},\mathbf{S}^{k})$, with its details to be covered later. The softmax function is then applied to obtain the normalized attention weights $\boldsymbol{\alpha}$ for values aggregation, $\boldsymbol{\alpha}\mathbf{V}$. The aggregated value tensor is then unflattened to be $\mathbb{R}^{T\times H\times W\times C_v}$, followed with a batch normalization and spiking layer. Afterwards, we use a spiking linear layer with batch normalization to map the channel size from $C_v$ back to $C$. Finally, the SEW Function is applied to the attention output and the input spike tensor for residual learning, resulting in the output as $\mathbf{S}^{\text{att}}\in \{0,1\}^{T\times H \times W \times C}$. It is important to note that our model also supports multi-head attention, following the same procedure used by the standard transformer.

\textbf{Positional encodings} are added in the first layer of the spiking spatiotemporal attention module, aiming to make the model aware of ordinal information in the input sequence. As the input of the attention module is binary spike tensor while positional encodings are float, direct addition would violate the fast computation in SNNs. So we add the encodings after the linear layer but before the batch normalization and spiking layer. Besides, as the spiking layer generates spikes by rolling over $T$ time steps, we scale the positional encodings by $1/T$ to maintain consistency across models with varying $T$. The definition mostly follows~\cite{vaswani2017attention} as
\begin{equation} \small \begin{aligned}
    & \text{PE}(\text{pos}, 2i) = \frac{1}{T}\sin (\text{pos} / 10000^{2i/ C_k}), \nonumber \\
    & \text{PE}(\text{pos}, 2i+1) = \frac{1}{T}\cos (\text{pos} / 10000^{2i/ C_k}), \nonumber 
\end{aligned} \end{equation} 
where $\text{pos}$ represents the position in the sequence, while $2i$ or $2i+1$ denotes the position of $C_k$ channel.

\textbf{Attention score} of scaled dot-product in the standard transformer~\cite{vaswani2017attention} is not well-defined for the binary spike vectors. Whenever there are zero components in the spike key vector, the dot-product invariably disregards the values of corresponding components in the spike query vector. For instance, assume a spike key vector has its $c$-th component equal to 0 ($\mathbf{s}^{k}[c]=0$). Consider two spike query vectors, $\mathbf{s}^{q}_1$ and $\mathbf{s}^{q}_2$, which differ only in the $c$-th component, such that $\mathbf{s}^{q}_1[c]=0$ and $\mathbf{s}^{q}_2[c]=1$. In this case, the dot-product will always yield the same attention score for the two different queries: $\mathbf{s}^{k\top} \cdot \mathbf{s}^{q}_1=\mathbf{s}^{k\top} \cdot \mathbf{s}^{q}_2$.
% \yxmu{It might lead to some misunderstandings. Why not say spiking value (the basic element). Saying vector with the length of channel nums seems mis-restrict the problem you're dealing with. The metrics will get excessive approximate similarity as long as there's zero element in the key or query. Maybe, use an inequation to describe the bounded similarity error if using the naive metric} 
That is to say, the dot-product used in~\cite{vaswani2017attention} is unable to precisely measure the similarity between two binary spike vectors.

\begin{proposition}[Johnson–Lindenstrauss Lemma on Binary Embedding~\cite{jacques2013robust,yi2015binary}]
\label{proposition:hamming-similarity}
Let $\mathbf{q}_i, \mathbf{k}_j \in\mathbb{R}^{d_k}$ be a real-valued query and key in the standard transformer described in Sec.~\ref{sec:preliminary}. Define $\mathbf{s}^q_i, \mathbf{s}^k_j \in \{0, 1\}^{C_k}$ as the corresponding binary embedding defined as 
\begin{equation}\small
    \mathbf{s}^q_i(\mathbf{q}_i) = \text{sign}(\mathbf{Aq}_i),\quad \mathbf{s}^k_j(\mathbf{k}_j) = \text{sign}(\mathbf{Ak}_j),\nonumber
\end{equation}
where $\mathbf{A}\in \mathbb{R}^{C_k \times d_k}$ is a projection matrix with each entry generated independently from the normal distribution $\mathcal{N}(0, 1)$. Given that $\scriptstyle C_k > \frac{\log M}{\delta^2}$ and $\delta > 0$, we have
\begin{equation}\small
    g(d_{\mathcal{H}}(\mathbf{s}^q_i, \mathbf{s}^k_j) - \delta) \leq d_{\mathcal{C}}(\mathbf{q}_i, \mathbf{k}_j) \leq g(d_{\mathcal{H}}(\mathbf{s}^q_i, \mathbf{s}^k_j) + \delta),
\end{equation}
with probability at least $1-2e^{-\delta^2 C_k}$. Here $g(x)=\cos(\pi (1-x))$ is a continuous and monotone function for $x\in [0, 1]$ and $M$ is the number of all possible keys and queries given by the finite training set.  $d_{\mathcal{H}}\in [0, 1]$ is the normalized Hamming similarity between binary spike vectors defined as
\begin{equation}\small
    \label{eq:hamming-similarity}
    d_{\mathcal{H}}(\mathbf{s}_i^q, \mathbf{s}_j^k) = 1 - \frac{1}{C_k}\sum_{c=1}^{C_k} \mathds{1}(s_{ic}^q \neq s_{jc}^k),
    % = \frac{1}{2}(1-\frac{\mathbf{s}_i^{\top}\mathbf{s}_j}{d_v}),
\end{equation}
and $d_{\mathcal{C}}\in [0, 1]$ is cosine similarity between real-valued vectors defined as
\begin{equation}\small
    \label{eq:cosine-similarity}
    d_{\mathcal{C}}(\mathbf{q}_i, \mathbf{k}_j) =  \frac{\mathbf{q}_i^{\top}\mathbf{k}_j}{\|\mathbf{q}_i\|\|\mathbf{k}_j\|}.
\end{equation}
\end{proposition}

Based on Proposition~\ref{proposition:hamming-similarity}, the cosine similarity $d_{\mathcal{C}}$ between real-valued queries and keys is bounded within $[g(d_{\mathcal{H}} - \delta), g(d_{\mathcal{H}} + \delta)]$, where $d_{\mathcal{H}}$ is the normalized Hamming similarity between corresponding binary spike queries and keys. When the channel size $C_k$ is large enough, $d_{\mathcal{C}}$ ensures equivalency to $g(d_{\mathcal{H}})$ with a high probability. The proof is provided in the supplementary. Consequently, as $g$ is a continuous and monotone for $d_{\mathcal{H}}\in[0, 1]$, we propose to use $d_{\mathcal{H}}$ directly to calculate the attention scores between binary spike queries and keys in our spiking spatiotemporal transformer. 

\textbf{The gradient of normalized Hamming similarity} does not exist since Eq.~\ref{eq:hamming-similarity} is a non-differentiable function. Thus we approximate $d_{\mathcal{H}}$ by
\begin{equation}\small
    \label{eq:hamming-similarity-approx}
    d_{\mathcal{H}}(\mathbf{s}_i^q, \mathbf{s}_j^k) = 1 - \frac{1}{C_k}\sum_{c=1}^{C_k} \left[ \mathbf{s}_{ic}^q\cdot(1-\mathbf{s}_{jc}^k) +  (1-\mathbf{s}_{ic}^q)\cdot\mathbf{s}_{jc}^k \right].
\end{equation}
As a result, the approximate gradients of normalized Hamming similarity function are given by:
\begin{equation}
    \frac{\partial d_{\mathcal{H}}(\mathbf{s}_i^q, \mathbf{s}_j^k)}{\partial \mathbf{s}_i^q} = \frac{2\mathbf{s}_j^k - 1}{C_k}, \quad
    \frac{\partial d_{\mathcal{H}}(\mathbf{s}_i^q, \mathbf{s}_j^k)}{\partial \mathbf{s}_j^k} = \frac{2\mathbf{s}_i^q - 1}{C_k}. \nonumber
\end{equation}

\beforesubsection
\subsection{Parametric Pose and Shape Regression}
\aftersubsection
\label{sec:regression}
\begin{figure}[ptb]
    \centering
    \includegraphics[width=\columnwidth]{figures/regression.pdf}
    \beforefigcaption
    \caption{\textbf{Human Poses and Shapes Regression.} The input spike tensor first undergoes 2D average pooling and is then fed into three linear layers in parallel to regress the global translation $\mathbf{d}$ and SMPL pose and shape parameters $\boldsymbol{\theta}, \boldsymbol{\beta}$ across all $T$ time steps.}
    \afterfigcaption
    \label{fig:regression}
\end{figure}

\textbf{Parametric human pose and shape} used in this work is SMPL model~\cite{loper2015smpl}. Given the shape parameters $\boldsymbol{\beta}$, pose parameters $\boldsymbol{\theta}$ and global translations $\mathbf{d}$, the model outputs a triangular mesh with 6,890 vertices at each time step, that is $\mathcal{M}(\boldsymbol{\beta}, \boldsymbol{\theta}, \mathbf{d})\in\mathbb{R}^{T\times 6890\times 3}$ for $T$ time steps in total. The shape parameters at time step $t$, denoted as $\boldsymbol{\beta}_{[t]}\in\mathbb{R}^{1\times 10}$, are linear coefficients of PCA shape space, learned from a large number of registered body scans. These parameters mainly describe individual body features such height, weight and body proportions. The pose parameters at time step $t$, denoted as $\boldsymbol{\theta}_{[t]}\in\mathbb{R}^{1\times 72}$, represent the articulated poses of the triangular mesh, consisting of a global rotation and relative rotations of the 24 joints in axis-angle form. The global translations of human body at time step $t$ is denoted by $\mathbf{d}_{[t]}\in \mathbb{R}^{1\times 3}$. To produce the final parametric shapes, the template body is deformed using shape- and pose-dependent deformations, articulated through forward kinematics to its target pose, and further transformed through linear blend skinning and global translation. Meanwhile, the 3D and 2D joint positions, denoted as $\mathbf{j}_{\text{3D}}$ and $\mathbf{j}_{\text{2D}}$, are obtained by regressing from the output vertices and projecting the 3D joints onto the 2D images.

We show the process in Fig.~\ref{fig:regression} where we apply the 2D average pooling to the input spike tensor and then directly regress the shape parameters $\boldsymbol{\hat \beta}$, pose parameters $\boldsymbol{\hat \theta}$ and global translations $\mathbf{\hat d}$ via three linear layers in parallel. Based on the predicted parameters, we obtain the corresponding parametric shapes and joint positions $\mathbf{\hat j}^{\text{3D}}, \mathbf{\hat j}^{\text{2D}}$ by SMPL model across $T$ time steps. When projecting 3D joints on 2D images, as the global translations under the camera coordinate are predicted, we can use predefined camera intrinsic parameters to reduce the redundancy of prediction.

\textbf{The training losses for} our model are introduced as follows:
\begin{equation}\small
    \begin{aligned}
        \mathcal{L}=
        \lambda_{\text{pose}}\mathcal{L}_{\text{pose}}+
        \lambda_{\text{shape}}\mathcal{L}_{\text{shape}}+
        \lambda_{\text{trans}}\mathcal{L}_{\text{trans}}+
         \lambda_{\text{3D}}\mathcal{L}_{\text{3D}}+
        \lambda_{\text{2D}}\mathcal{L}_{\text{2D}},
        \nonumber
    \end{aligned}
\end{equation}
where $\lambda_{\text{pose}}$, $\lambda_{\text{shape}}$, $\lambda_{\text{trans}}$, $\lambda_{\text{3D}}$ and $\lambda_{\text{2D}}$ are the corresponding loss weights. We use the losses of pose and shape parameters, global translations 3D and 2D joint positions for training, with the detailed definitions included in the supplementary.

\beforesubsection
\subsection{Our Large-scale Synthetic SynEventHPD Dataset}
\aftersubsection
\begin{figure}[ptb]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/poses_tsne.pdf}
    \beforefigcaption
    \vspace{4mm}
    \caption{\textbf{t-SNE Visualization of Poses} from each sub-dataset in our SynEventHPD dataset. MMHPSD~\cite{gehrig2020video} only covers small areas, while our proposed SynEventHPD, including 4 sub-datasets (EventH36M, EventAMASS, EventPHSPD and SynMMHPSD), span a wide range of pose areas. This highlights the rich variety of poses provided in SynEventHPD, in contrast to the limited range in MMHPSD.}
    \afterfigcaption
    \label{fig:poses-tsne}
\end{figure}

\begin{table}[ptb]
    \centering
    \caption{\textbf{Summary of event-based datasets for 3D human pose tracking}, including existing MMHPSD dataset and 4 sub-datasets in our SynEventHPD dataset, compared in terms of real or synthetic data (R/S), number of subjects (Sub \#), number of event streams (Str \#), total time length of all the event streams in hours (Len), average time length of each stream in minutes (AvgLen), annotated poses (Pose).}
    \beforetab
    \setlength{\tabcolsep}{1.5mm}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{@{}|c|cccccc|@{}}
    \bottomrule \hline
        \makecell[c]{Dataset} & \makecell[c]{R/S} & \makecell[c]{Sub \\ \#} & \makecell[c]{Str \\ \#} & \makecell[c]{Len \\ (hrs)} & \makecell[c]{AvgLen \\ (mins)} & \makecell[c]{Pose} \\
    \hline
        \makecell[c]{MMHPSD~\cite{zou2021eventhpe}} & \makecell[c]{Real} & \makecell[c]{15} & \makecell[c]{178} & \makecell[c]{4.39} & \makecell[c]{1.48}  & \makecell[c]{\checkmark} \\
    \hline
        \makecell[c]{EventH36M} & \makecell[c]{Syn} & \makecell[c]{7} & \makecell[c]{835} & \makecell[c]{12.46} & \makecell[c]{0.90} & \makecell[c]{\checkmark} \\
        \makecell[c]{EventAMASS} & \makecell[c]{Syn} & \makecell[c]{13} & \makecell[c]{8028} & \makecell[c]{23.54} & \makecell[c]{0.18}  & \makecell[c]{\checkmark} \\
        \makecell[c]{EventPHSPD} & \makecell[c]{Syn} & \makecell[c]{12} & \makecell[c]{156} & \makecell[c]{5.33} & \makecell[c]{2.05}  & \makecell[c]{\checkmark} \\
        \makecell[c]{SynMMHPSD} & \makecell[c]{Syn} & \makecell[c]{15} & \makecell[c]{178} & \makecell[c]{4.39} & \makecell[c]{1.48}  & \makecell[c]{\checkmark} \\
    \hline
        \makecell[c]{SynEventHPD (Total)} & \makecell[c]{Syn} & \makecell[c]{47} & \makecell[c]{9197} & \makecell[c]{45.72} & \makecell[c]{0.30} & \makecell[c]{\checkmark} \\
    \hline \toprule  
    \end{tabular}
    }
    \aftertab
    \vspace{-2mm}
    \label{tab:dataset-summary}
\end{table}

Currently, the largest event-based dataset for human pose estimation is MMHPSD, which includes 15 subjects, 21 different actions and a total of 4.39 hours of event streams~\cite{zou2021eventhpe}. However, this dataset's limited variety of motions restricts the generalization ability of trained models. To address this issue, we propose to synthesize event data from multiple motion capture datasets, including \textit{i.e. Human3.6M~\cite{ionescu2013human3}, AMASS~\cite{mahmood2019amass}, PHSPD~\cite{zou2022human} and MMHPSD-Gray~\cite{zou2021eventhpe}}, to construct a large-scale synthetic dataset. Our synthetic dataset, called SynEventHPD, is a meta dataset consisting of 4 sub-datasets: EventH36M, EventAMASS, EventPHSPD and SynMMHPSD. In total, it contains 45.72 hours of event streams, which is more than 10 times the size of MMHPSD. The distribution of poses across all these datasets are visualized in Fig.~\ref{fig:poses-tsne} to highlight the variety. Other details are summarized in Table~\ref{tab:dataset-summary}. This dataset will be public available for research purpose upon paper acceptance.

\textbf{The synthesizing process of our new dataset} is mainly based on the workflow proposed in~\cite{gehrig2020video}. We start by cropping frames according to a global bounding box of person in the video, and then resize to $512\times 512$ to maintain a same image size across different sub-datasets. Next, we apply frame interpolation to increase the frame rate of provided videos, guided by the predicted optical flows. We convert the high frame rate videos to gray-scale images and generate events by checking the brightness change at each pixel over time. This process is straightforward for Human3.6M~\cite{ionescu2013human3}, PHSPD~\cite{zou2022human} and MMHPSD~\cite{zou2021eventhpe}, as they contain RGB or gray-scale videos. However, AMASS~\cite{mahmood2019amass} dataset only contains motion capture data without any RGB videos. In this regard, for each motion capture sequence, an avatar is randomly picked from 13 different avatars, animated and rendered to form its corresponding RGB videos of size $512\times 512$, obtained using one of the 4 predefined lightning conditions.
Annotations provided in our dataset include pose and shape parameters of SMPL model, corresponding 2D/3D joint positions and the global translation under the default camera intrinsic parameters. We demonstrate the effectiveness of our large-scale synthetic dataset by showing four examples of images, event frames, and annotated poses in Fig.~\ref{fig:dataset-demo-frame}. More details and additional examples are included in the supplementary material.

\begin{figure}[ptb]
    \centering
    \includegraphics[width=\columnwidth]{figures/dataset_demo_frames.pdf}
    \beforefigcaption
    \caption{\textbf{Sample examples} of the synthesized event signals from the existing benchmark datasets that are Incorporated as sub-datasets in our SynEventHPD dataset. The displayed event image is for visualization purpose, obtained by collapsing events into one frame. Their corresponding RGB or gray-scale images and the annotated poses are also shown. 
    The synthesized event frame from SynMMHPSD is visually indistinguishable from the real events in MMHPSD.}
    \afterfigcaption
    \label{fig:dataset-demo-frame}
\end{figure}

\beforesection
\section{Experiments}
\aftersection

% \beforesubsection
\subsection{Empirical Results on MMHPSD Dataset}
\aftersubsection

\begin{table*}[phtb]
    \centering
    \caption{\textbf{Quantitative results of human pose tracking on the MMHPSD test set.} Models are trained using the MMHPSD train set for a fair comparison with prior works. The input setting is various, including video (\textit{V}), first gray-scale frame (\textit{G}), events (\textit{E}) and their combination. \textit{VIBE}~\cite{kocabas2019vibe} and \textit{MPS}~\cite{wei2022capturing} are video-based ANN methods. \textit{EventCap}~\cite{xu2020eventcap} and \textit{EventHPE}~\cite{zou2021eventhpe} are most recent works using event camera for human pose tracking. We also include three popular ANN models (\textit{ResNet-GRU}~\cite{kocabas2019vibe}, \textit{ResNet-TF}~\cite{carion2020end} and \textit{Video-Swin}~\cite{liu2021swin}), one mixed model (\textit{SEW-ResNet-TF}) and four SNN models (\textit{ANN2SNN}~\cite{rueckauer2017conversion}, \textit{SEW-ResNet}~\cite{fang2021deep}, \textit{MA-SNN}~\cite{yao2023attention} and \textit{SpikeFormer}~\cite{zhou2022spikformer}) as benchmarks to illustrate the effectiveness of our SNN approach with novel Spiking Spatiotemporal Transformer. 
    Underline denotes the best value except EventHPE(GT).
    $\boldsymbol{^{\dagger}}$\textit{EventHPE(GT)} means the ground-truth starting pose is known in EventHPE, which is considered as the upper bound among all these models due to its perfect information of the starting pose in the first frame.}
    \beforetab
    \setlength{\tabcolsep}{0.5mm}
    \renewcommand{\arraystretch}{1.3}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{@{}|c|c|c|c|cccc|cccc|@{}}
    \bottomrule \hline
        \multirow{2}{*}{Method} & \multirow{2}{*}{\makecell[c]{ANN/\\SNN}} & \multirow{2}{*}{Input} & \multirow{2}{*}{Params} & \multicolumn{4}{c|}{T=8 (1 sec)} & \multicolumn{4}{c|}{T=64 (8 secs)}\\
        \cline{5-12}
        & & & & \makecell[c]{FLOPs} & \makecell[c]{MPJPE $\downarrow$} & \makecell[c]{PEL-MPJPE $\downarrow$} & \makecell[c]{PA-MPJPE $\downarrow$} & \makecell[c]{FLOPs} & \makecell[c]{MPJPE $\downarrow$} & \makecell[c]{PEL-MPJPE $\downarrow$} & \makecell[c]{PA-MPJPE $\downarrow$}\\
    \hline
        % my 256x256 43.125G 41.619M 344.856G | original 224x224 33.379G, 48.287M, 267.030G \makecell[c]{ANN}
        VIBE~\cite{kocabas2019vibe} & \multirow{2}{*}{ANN} & \makecell[c]{V} & \makecell[c]{48.3M} & \makecell[c]{43.4G} & \makecell[c]{-} & \makecell[c]{73.1} & \makecell[c]{50.9} & \makecell[c]{344.9G} & \makecell[c]{-} & \makecell[c]{75.4} & \makecell[c]{53.6}\\
        %  my 256x256 45.6G, 39.6M, 348.3G | original 224x224 35.6G, 39.6M, 284.8G
        MPS~\cite{wei2022capturing} & & \makecell[c]{V} & \makecell[c]{39.6M} & \makecell[c]{45.6G} & \makecell[c]{-} & \makecell[c]{68.0} & \makecell[c]{48.2} & \makecell[c]{348.3G} & \makecell[c]{-} & \makecell[c]{69.2} & \makecell[c]{50.1}\\
    \hline
        % 1.77G * 8 * 10 iters
        EventCap(VIBE)~\cite{xu2020eventcap} & \multirow{5}{*}{ANN} & \makecell[c]{V+E} & \makecell[c]{48.3M} & \makecell[c]{185.0G} & \makecell[c]{-} & \makecell[c]{71.9} & \makecell[c]{50.4} & \makecell[c]{1477.7G} & \makecell[c]{-} & \makecell[c]{74.1} & \makecell[c]{52.9}\\
        %
        EventCap(MPS)~\cite{xu2020eventcap} & & \makecell[c]{V+E} & \makecell[c]{39.6M} & \makecell[c]{187.2G} & \makecell[c]{-} & \makecell[c]{66.6} & \makecell[c]{47.8} & \makecell[c]{1481.1G} & \makecell[c]{-} & \makecell[c]{68.1} & \makecell[c]{49.5}\\
        %
        EventHPE(VIBE)~\cite{zou2021eventhpe} & & \makecell[c]{G+E} & \makecell[c]{49.0M} & \makecell[c]{49.0G} & \makecell[c]{-} & \makecell[c]{69.6} & \makecell[c]{48.9} & \makecell[c]{354.0G} & \makecell[c]{-} & \makecell[c]{71.6} & \makecell[c]{50.2}\\
        %
        EventHPE(MPS)~\cite{zou2021eventhpe} & & \makecell[c]{G+E} & \makecell[c]{39.6M} & \makecell[c]{49.3G} & \makecell[c]{-} & \makecell[c]{65.1} & \makecell[c]{46.5} & \makecell[c]{354.2G} & \makecell[c]{-} & \makecell[c]{66.8} & \makecell[c]{48.1}\\
        %
        $\boldsymbol{^{\dagger}}$EventHPE(GT)~\cite{zou2021eventhpe} & & \makecell[c]{G+E} & \makecell[c]{46.9M} & \makecell[c]{-} & \makecell[c]{71.8} & \makecell[c]{55.0} & \makecell[c]{43.9} & \makecell[c]{-} & \makecell[c]{74.5} & \makecell[c]{58.1} & \makecell[c]{45.3}\\
    \hline
        % CNN+GRU 43.602G 46.865M 348.622G
        ResNet-GRU~\cite{kocabas2019vibe} & \multirow{3}{*}{ANN}  & \makecell[c]{E} & \makecell[c]{46.9M} & \makecell[c]{43.6G} & \makecell[c]{111.2} & \makecell[c]{60.0} & \makecell[c]{45.3} & \makecell[c]{348.6G} & \makecell[c]{115.0} & \makecell[c]{64.2} & \makecell[c]{49.5}\\
        % detr 50.489G 41.315M, 403.819G 41.358M
        ResNet-TF~\cite{carion2020end} & & \makecell[c]{E} & \makecell[c]{41.3M} & \makecell[c]{50.5G} & \makecell[c]{108.5} & \makecell[c]{59.9} & \makecell[c]{\underline{\textbf{44.1}}} & \makecell[c]{403.8G} & \makecell[c]{114.2} & \makecell[c]{66.0} & \makecell[c]{50.1}\\
        % swin transformer 44.727G 48.930M, 359.569G 48.930M
        Video-Swin~\cite{liu2021swin} & & \makecell[c]{E} & \makecell[c]{48.9M} & \makecell[c]{44.7G} & \makecell[c]{124.1} & \makecell[c]{66.5} & \makecell[c]{49.0} & \makecell[c]{359.6G} & \makecell[c]{130.9} & \makecell[c]{72.5} & \makecell[c]{53.1}\\
    \hline
        % 
        SEW-ResNet-TF & \makecell[c]{Mix} & \makecell[c]{E} & \makecell[c]{47.0M} & \makecell[c]{24.5G} & \makecell[c]{110.8} & \makecell[c]{58.9} & \makecell[c]{44.2} & \makecell[c]{199.7G} & \makecell[c]{113.2} & \makecell[c]{65.3} & \makecell[c]{49.3}\\
    \hline
        %
        ANN2SNN~\cite{rueckauer2017conversion} & \multirow{4}{*}{SNN} & \makecell[c]{E} & \makecell[c]{46.9M} & \makecell[c]{12.5G} & \makecell[c]{140.3} & \makecell[c]{74.1} & \makecell[c]{55.8} & \makecell[c]{98.8G} & \makecell[c]{148.2} & \makecell[c]{81.1} & \makecell[c]{60.9}\\
        SEW-ResNet~\cite{fang2021deep} & & \makecell[c]{E} & \makecell[c]{\underline{\textbf{25.8M}}} & \makecell[c]{9.1G} & \makecell[c]{116.8} & \makecell[c]{62.5} & \makecell[c]{48.3} & \makecell[c]{56.7G} & \makecell[c]{122.8} & \makecell[c]{66.3} & \makecell[c]{52.3}\\
        %
        MA-SNN~\cite{yao2023attention} & & \makecell[c]{E} & \makecell[c]{30.2M} & \makecell[c]{\underline{\textbf{7.5G}}} & \makecell[c]{115.2} & \makecell[c]{61.6} & \makecell[c]{47.6} & \makecell[c]{\underline{\textbf{55.3G}}} & \makecell[c]{120.1} & \makecell[c]{64.8} & \makecell[c]{48.9}\\
        %
        SpikeFormer~\cite{zhou2022spikformer} & & \makecell[c]{E} & \makecell[c]{36.8M} & \makecell[c]{13.2G} & \makecell[c]{112.5} & \makecell[c]{60.2} & \makecell[c]{46.8} & \makecell[c]{96.3G} & \makecell[c]{118.1} & \makecell[c]{64.1} & \makecell[c]{48.4}\\
    \hline
        % layers=3: 44.690M, layers=2: 38.4M
        Ours & \makecell[c]{SNN} & \makecell[c]{E} & \makecell[c]{47.7M} & \makecell[c]{9.4G} & \makecell[c]{\underline{\textbf{107.1}}} & \makecell[c]{\underline{\textbf{58.8}}} & \makecell[c]{\underline{\textbf{44.1}}} & \makecell[c]{63.4G} & \makecell[c]{\underline{\textbf{111.8}}} & \makecell[c]{\underline{\textbf{61.7}}} & \makecell[c]{\underline{\textbf{45.6}}}\\
    \hline \toprule 
    \end{tabular}}
    \aftertab
    \label{tab:pose-estimation}
\end{table*}

In this section, we start by outlining the implementation details for training and explaining the reported evaluation metrics. Subsequently, we compare our method with recent video-based and event-based human pose estimation approaches in Sec.~\ref{sec:compare-prior-works}, emphasizing the competence of event signals for human pose tracking. We also compare our SNN model with three popular ANN models in Sec.~\ref{sec:compare-anns}, illustrating the efficiency and effectiveness of our SNN-based approach. Moreover, we contrast our approach with five recently proposed SNN models in Sec.~\ref{sec:compare-snn}, showcasing the superiority of our proposed Spiking Spatiotemporal Transformer for bi-directional temporal information fusion in human pose tracking.

\textbf{Implementation Details.} For a fair comparison with prior works~\cite{xu2020eventcap,zou2021eventhpe}, we follow the train and test set split for the MMHPSD dataset from~\cite{zou2021eventhpe}, where subject 1, 2 and 7 are designated for testing and the remaining 12 subjects for training. We present the results of model trained with $T=8$ time steps and $T=64$ time steps. For event stream preprocessing, we convert each event packet into a voxel grid of size $256\times 256\times 4$. Empirically, we find that $C=4$ is the best choice, as higher values do not show performance improvements in the ablation study. To fairly compare with other baselines in terms of the number of parameters, we use SEW-ResNet50~\cite{fang2021deep} as the backbone and configure the Spiking Spatiotemporal Transformer with 1024 hidden dimension, 1 attention head and 2 layers, resulting in 47.7M parameters. 

During training, to ensure robustness against both fast and slow motions, we augment the training samples in two ways: randomly selecting event stream of (0.5, 1, 2, 3) seconds for $T=8$ and (4, 8, 16, 32) seconds for $T=64$ as the input, spatially rotating the voxel grid with a random degree between -20 and 20. We use parametric LIF neuron with soft reset and retain the backpropagation of reset path in SNNs, where SpikingJelly~\cite{SpikingJelly} is used to implement the model. The loss weights $\lambda_{\text{pose}}$, $\lambda_{\text{shape}}$, $\lambda_{\text{trans}}$, $\lambda_{\text{3D}}$ and $\lambda_{\text{2D}}$ are set to be 10, 1, 50, 1 and 10 respectively. We train the two models for 20 and 25 epochs respectively with batch size being 8. The learning rate starts from 0.01 and is scheduled by CosineAnnealingLR, with maximum epoch number of 21 and 26. The models are trained on a single NVIDIA A100 GPU. For testing, 1 and 8-second event streams are used for $T=8$ and $T=64$ models respectively. 

\textbf{Evaluation metrics.} Similar to previous works~\cite{kocabas2019vibe,zou2021eventhpe}, we report three different metrics, mean per joint position error (MPJPE), pelvis-aligned MPJPE (PEL-MPJPE) and Procrustes-Aligned MPJPE (PA-MPJPE). PA-MPJPE compares predicted and target pose after rotation and translation alignment, while PEL-MPJPE compares after only translation alignment of two pelvis joints.

\beforesubsubsection
\subsubsection{Comparison with Prior Works} 
\aftersubsubsection
\label{sec:compare-prior-works}
% \textbf{SNNs v.s. SOTA} - highlight events only \textbf{Implementation of SOTA Baselines.}
We compare our method with four prior works to highlight the competency of using event signals only for human pose tracking: VIBE~\cite{kocabas2019vibe}, MPS~\cite{wei2022capturing}, EventCap~\cite{xu2020eventcap}, and EventHPE~\cite{zou2021eventhpe}. In Tab.~\ref{tab:pose-estimation}, we use \textit{V}, \textit{G} and \textit{E} to represent the input data of gray-scale video, first gray-scale image and event streams respectively. \textit{VIBE} and \textit{MPS} are applied as the most recent video-based baselines with ResNet50 as the backbone. Note that both methods use weak camera model without global translation, so we will not report their MPJPE. To extract initial poses from the gray-scale video as required by EventCap, we make use of pre-trained VIBE and MPS methods for the extraction, labeled as \textit{EventCap(VIBE)} and \textit{EventCap(MPS)}. Since the authors have not published their code, we re-implement EventCap using PyTorch LBFGS optimizer and PyTorch3D differential render, following~\cite{zou2021eventhpe}. Besides, EventCap is an iterative optimization approach, which typically requires much more FLOPs than end-to-end methods as is indicated in Tab.~\ref{tab:pose-estimation}. For EventHPE, we also use VIBE and MPS for the starting pose extraction, denoted as \textit{EventHPE(VIBE)} and \textit{EventHPE(MPS)}. We also report the results of EventHPE with ground-truth starting pose known, denoted as \textit{EventHPE(GT)}. This method is considered the upper bound as it is assumed to have perfect information of the starting pose in the first frame, without any pose errors induced by VIBE or MPS. 

As shown in Tab.~\ref{tab:pose-estimation}, the most recent MPS outperforms VIBE by approximately 9mm in PEL-MPJPE and 5mm in PA-MPJPE for both $T=8$ and $T=64$. This trend is also evident when comparing EventCap(MPS) with EventCap(VIBE) or EventHPE(MPS) with EventHPE(VIBE), indicating that the performance of the two prior works~\cite{xu2020eventcap,zou2021eventhpe} is significantly impacted by the accuracy of initial poses provided by the pre-trained video-based methods. When the initial poses are inaccurate, they might fall into a local minimum and only improve the initial states by up to 3mm in PEL-MPJPE and 2mm in PA-MPJPE. In constrast, our end-to-end approach directly uses event streams, which are better to capture the motion dynamics than images. Consequently, our SNN-based model achieves the best performance with the smallest gap to the upper bound EventHPE(GT) while using only about 6\% of FLOPs required by the optimization-based EventCap and 20\% of FLOPs needed by the EventHPE. This is further illustrated in Fig.~\ref{fig:vis-pose}, where the inaccurate initial or starting poses given by MPS lead to sub-optimal pose tracking outcomes compared to ours.

\beforesubsubsection
\subsubsection{Comparison with ANN models}
\aftersubsubsection
\label{sec:compare-anns}
To further illustrate the advantages of SNNs over ANNs in event-based human pose tracking, we compare our model with three popular ANN-based models: ResNet with GRU used in~\cite{kocabas2019vibe,zou2021eventhpe} (\textit{ResNet-GRU}), ResNet with standard transformer~\cite{vaswani2017attention} used in DETR~\cite{carion2020end} (\textit{ResNet-TF}) and Video Swin Transformer proposed in~\cite{liu2021swin} (\textit{Video-Swin}). For fair comparisons, we select the architecture with about 45M parameters for all the models. Further details are presented in the supplementary material. The settings for training these ANN models mostly follow those of our approach, except the learning rate, which starts from 0.0001 and is scheduled by StepLR with a 0.1 decay after 15 and 20 epochs for both T=8 and T=64, respectively. This is because ANN models do not converge well using a higher learning rate, such as 0.001. 

% highlight (1) SNNs better temporal enconding and efficiency
For the models of $T=8$ in Tab.~\ref{tab:pose-estimation}, our approach achieves slightly lower pose errors than ResNet-GRU and Video-Swin, while presenting competitive performance with ResNet-TF, which also achieves 44.1mm in PA-MPJPE. In the case of $T=64$, where longer temporal dependencies are necessary for perception, the performance decline of the three ANN-based models is noticeably larger than that of our SNN-based model, with over 4.1mm vs. 1.5mm drop in PA-MPJPE. Furthermore, our model requires only 9.4G and 63.4G FLOPs for $T=8$ and $T=64$, respectively, which is less than 20\% of the FLOPs needed by the three ANN-based models. These results demonstrate the superiority of our SNN-based approach in efficiently encoding long-term temporal dependencies within event streams, primarily due to the fundamentally different working mechanisms of spiking neurons compared to conventional artificial neurons. 

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/vis_pose_tracking.pdf}
    \beforefigcaption
    \caption{\textbf{Qualitative results} of MPS~\cite{wei2022capturing}, EventCap(MPS)~\cite{xu2020eventcap}, EventHPE(MPS)~\cite{zou2021eventhpe} and ours. Two side views are shown in dashed boxes. The two prior works, EventCap and EventHPE, tend to suffer from the inaccurate initial poses or starting pose given by pre-trained video based method, MPS, and thus produce sub-optimal pose tracking outcomes when compared to our approach.}
    \afterfigcaption
    \label{fig:vis-pose}
\end{figure*}

\beforesubsubsection
\subsubsection{Comparison with SNN models} 
\aftersubsubsection
\label{sec:compare-snn}
% highlight spiking spatiotemporal transformer \textbf{SNNs v.s. SNNs + Spiking Transformer} - our technical contribution
In Tab.~\ref{tab:pose-estimation}, we compare our approach  with five recently proposed SNN-based models to highlight the superiority of our proposed Spiking Spatiotemporal Transformer for human pose tracking. \textit{SEW-ResNet-TF} acts as a baseline of mixed architecture that employs SEW-ResNet as the SNN-based backbone followed by an ANN-based standard Transformer for pose tracking. The model architecture settings are similar to our approach. \textit{ANN2SNN} refers to the conversion of the ANNs model of ResNet-GRU to the SNNs model using~\cite{rueckauer2017conversion}. \textit{SEW-ResNet}~\cite{fang2021deep} is the backbone used in our approach without Spiking Spatiotemporal Transformer. \textit{MA-SNN}~\cite{yao2023attention} represents multi-dimensional attention SNNs where SEW-ResNet50 is used for a fair comparison. \textit{SpikeFormer}~\cite{zhou2022spikformer} indicates an SNN-based vision transformer (ViT)~\cite{dosovitskiy2020image}, where dot-product is directly adopted in the self-attention module.

Compared to the mixed model SEW-ResNet-TF, our approach exhibits slightly lower pose errors while requiring less than 50\% of FLOPs. As for the entirely SNN-based models, although ANN2SNN has shown its excellent performance in the image classification task, it falls short in the regression task of pose tracking, producing much higher pose errors than other directly trained SNNs. This is primarily due to the quantization errors introduced during the conversion process. When compared to SEW-ResNet, our approach yields much lower pose errors -- 48.3 vs. 44.1mm in PA-MPJPE for $T=8$ -- and the performance gap widens for $T=64$, at 52.3 vs. 45.6mm. This demonstrates the importance of bi-directional temporal information provided by our proposed Spiking Spatiotemporal Transformer. In terms of MA-SNN, although it requires fewer FLOPs than our approach due to its lower spiking rate of 16.4\% vs. ours of 22.6\%, its performance is still inferior. Additionally, our approach presents moderately lower pose errors and fewer FLOPs than SpikeFormer, which is attributed to our proposed normalized Hamming similarity in the spiking attention module, as opposed to the ill-posed dot-product between spike tensors.

\begin{table}[]
    \centering
    \caption{\textbf{Quantitative results on the real MMHPSD test set.} Models are trained using only the real MMHPSD train set (\textit{Real}), only the synthetic SynEventHPD dataset (\textit{Syn}) or a combination of the synthetic SynEventHPD dataset and the real MMHPSD train set (\textit{Syn\&Real}). Note that the results of \textit{Real} are the same with those in Tab.~\ref{tab:pose-estimation}. The values in the bracket are the improvements of each model trained with \textit{Syn\&Real} compared to trained with \textit{Real} only.}
    \beforetab
    \setlength{\tabcolsep}{0.2mm}
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{@{}|c|c|c|ccc|@{}}
    \bottomrule \hline
        \multirow{2}{*}{Model} & \multirow{2}{*}{\makecell[c]{ANN/\\SNN}} & \multirow{2}{*}{\makecell[c]{Training\\Set}} &  \multicolumn{3}{c|}{T=8 (1 sec)}\\
        \cline{4-6}
        & & & \makecell[c]{MPJPE $\downarrow$} & \makecell[c]{PEL-MPJPE $\downarrow$} & \makecell[c]{PA-MPJPE $\downarrow$}\\
    \hline
        % 
        \multirow{3}{*}{\makecell[c]{ResNet-GRU~\cite{kocabas2019vibe}}} & \multirow{3}{*}{\makecell[c]{ANN}} & \makecell[c]{Syn} & \makecell[c]{113.6} & \makecell[c]{62.2} & \makecell[c]{47.5} \\
        & & \makecell[c]{Real} & \makecell[c]{111.2} & \makecell[c]{60.0} & \makecell[c]{45.3} \\
        & & \makecell[c]{Syn\&Real} & \makecell[c]{105.4 (5.8)} & \makecell[c]{58.9 (1.1)} & \makecell[c]{44.6 (0.7)} \\
    \hline
        %
        \multirow{3}{*}{\makecell[c]{SEW-ResNet-TF}} & \multirow{3}{*}{\makecell[c]{Mix}} & \makecell[c]{Syn} & \makecell[c]{114.1} & \makecell[c]{60.6} & \makecell[c]{45.5} \\
        & & \makecell[c]{Real} & \makecell[c]{110.8} & \makecell[c]{58.9} & \makecell[c]{44.2} \\
        & & \makecell[c]{Syn\&Real} & \makecell[c]{104.2 (6.6)} & \makecell[c]{58.4 (0.5)} & \makecell[c]{43.5 (0.7)} \\
    \hline
        %
        \multirow{3}{*}{\makecell[c]{SEW-ResNet~\cite{fang2021deep}}} & \multirow{9}{*}{\makecell[c]{SNN}} & \makecell[c]{Syn} & \makecell[c]{120.3} & \makecell[c]{63.6} & \makecell[c]{49.1} \\
        & & \makecell[c]{Real} & \makecell[c]{116.8} & \makecell[c]{62.5} & \makecell[c]{48.3} \\
        & & \makecell[c]{Syn\&Real} & \makecell[c]{113.1 (3.7)} & \makecell[c]{61.7 (0.8)} & \makecell[c]{47.8 (0.5)} \\
    \cline{1-1}\cline{3-6}
        % 
        \multirow{3}{*}{\makecell[c]{MA-SNN~\cite{yao2023attention}}} & & \makecell[c]{Syn} & \makecell[c]{119.0} & \makecell[c]{63.1} & \makecell[c]{48.8} \\
        & & \makecell[c]{Real} & \makecell[c]{115.2} & \makecell[c]{61.6} & \makecell[c]{47.6} \\
        & & \makecell[c]{Syn\&Real} & \makecell[c]{112.5 (2.7)} & \makecell[c]{60.7 (0.9)} & \makecell[c]{46.9 (0.7)} \\
    \cline{1-1}\cline{3-6}
        %
        \multirow{3}{*}{\makecell[c]{Ours}} & & \makecell[c]{Syn} & \makecell[c]{110.7} & \makecell[c]{59.4} & \makecell[c]{45.0} \\
        & & \makecell[c]{Real} & \makecell[c]{107.1} & \makecell[c]{58.8} & \makecell[c]{44.1} \\
        & & \makecell[c]{Syn\&Real} & \makecell[c]{103.1 (4.0)} & \makecell[c]{58.4 (0.4)} & \makecell[c]{43.8 (0.3)} \\
    \hline \toprule 
    \end{tabular}}
    \aftertab
    \label{tab:pose-estimation-syn}
\end{table}

% We present the results of models trained on synthetic data and tested on real data in Sec.~\ref{sec:compare-train-synthetic}, illustrating the effectiveness of our large-scale synthetic events data for human pose tracking.
\beforesubsection
\subsection{Empirical Results on SynEventHPD Dataset}
\aftersubsection
\label{sec:compare-train-synthetic}
Although this dataset covers a variety of motions, as illustrated in Fig.~\ref{fig:poses-tsne}, the potential domain gap between synthetic and real events data remains an open question. In this section, we aim to aim to demonstrate the value of our proposed synthetic SynEventHPD dataset. We select five models from Tab.~\ref{tab:pose-estimation} including one ANN model (\textit{ResNet-GRU}~\cite{kocabas2019vibe}), one mixed model (\textit{SEW-ResNet-TF}) and three SNN models (\textit{SEW-ResNet}~\cite{fang2021deep}, \textit{MA-SNN}~\cite{yao2023attention} and \textit{Ours}). All models are evaluated on the real MMHPSD test set, but are trained using either the real MMHPSD train set, the synthetic SynEventHPD dataset or a combination of both synthetic dataset and the real train set. 

The quantitative results are displayed in Tab.~\ref{tab:pose-estimation-syn}. It is evident that, compared to models trained using the real MMHPSD train set, the pose errors are generally higher for models trained on the synthetic SynEventHPD dataset. This is largely due to the domain gap between the synthetic and real events, which results in inferior performance when training only on synthetic data and then evaluating on real data. However, after combining both real and synthetic datasets for training, all the models in Tab.~\ref{tab:pose-estimation-syn} achieve improved performance compared to using either the real MMHPSD train set or the synthetic SynEventHPD dataset alone. This highlights the effectiveness of our proposed SynEventHPD dataset. We also present qualitative results in Fig.~\ref{fig:web_demo}, illustrating the performance of our model trained solely on the synthetic SynEventHPD dataset and applied to unseen scenarios. The left two examples show predictions on synthetic events generated from webcam videos, while the right example displays test results on real data. Despite being trained on the synthetic dataset, our model still demonstrates its generalization ability and applicability.

\begin{figure}[]
    \centering
    \includegraphics[width=\columnwidth]{figures/web_demo_fig.pdf}
    \beforefigcaption
    \caption{\textbf{Generalization ability of our model, trained solely on the synthetic SynEventHPD dataset and applied to unseen scenarios.} The left two examples show predicted poses using events synthesized from webcam videos, while the right example employs real events for inference of poses over time.}
    \afterfigcaption
    \label{fig:web_demo}
\end{figure}

\beforesubsection
\subsection{Emperical Results on DHP19 Dataset}
\aftersubsection
DHP19 dataset~\cite{calabrese2019dhp19} only provides 2-view event streams and annotated 3D joint positions. As a result, we follow the settings in~\cite{calabrese2019dhp19}, using event streams as input and predicting 2D joint heatmaps as output. Subsequently, 3D pose can be reconstructed using the predicted 2-view 2D poses. We include the quantitative results in the supplementary material, demonstrating the effectiveness and efficiency of SNNs and our proposed Spiking Spatiotemporal Transformer for improved bi-directional temporal information fusion.

%  Finally, we present ablation studies in Sec.~\ref{sec:ablation} to demonstrate the significance of individual components in our proposed model, especially the spiking spatiotemporal transformer.
\beforesubsection
\subsection{Ablation Study}\label{sec:ablation}
\aftersubsection
In this section, we perform ablation studies to assess several crucial components in our model. The quantitative results can be found in the corresponding sub-figures of Fig.~\ref{fig:ablation}. 

\textbf{1. Score function in Spiking Spatiotemporal Transformer}: We compare our proposed normalized Hamming similarity between spike vectors to scaled dot-production similarity, normalized Euclidean similarity and normalized Manhattan similarity as detailed below:
\begin{equation}\footnotesize
    \begin{aligned}
        \text{Nomalized Hamming similarity}\quad 1 - \frac{1}{C_k}\sum_{c=1}^{C_k} \mathds{1}(\mathbf{s}_{ic}^q \neq \mathbf{s}_{jc}^k), \nonumber
    \end{aligned}
\end{equation}
\begin{equation}\footnotesize
    \begin{aligned}
        \text{Scaled dot-product similarity}\quad \frac{1}{\sqrt{C_k}}\sum_{c=1}^{C_k} \mathbf{s}_{ic}^q\cdot\mathbf{s}_{jc}^k, \nonumber
    \end{aligned}
\end{equation}
\begin{equation}\footnotesize
    \begin{aligned}
        \text{Normalized Euclidean similarity}\quad 1 - \frac{1}{C_k}\sum_{c=1}^{C_k}(\mathbf{s}_{ic}^q - \mathbf{s}_{jc}^k)^2, \nonumber
    \end{aligned}
\end{equation}
\begin{equation}\footnotesize
    \begin{aligned}
        \text{Normalized Manhattan similarity}\quad 1 - \frac{1}{C_k}\sum_{c=1}^{C_k}|\mathbf{s}_{ic}^q - \mathbf{s}_{jc}^k|. \nonumber
    \end{aligned}
\end{equation}
The quantitative results of PEL-MPJPE depicted in Fig.~\ref{fig:ablation} (i) reveal that our approach outperforms the other three commonly used similarity functions by over 3mm, showcasing the effectiveness of the normalized Hamming similarity as the score function for spike vectors.

\textbf{2. Channel $C$ of input voxel}: We compare channel sizes of 1, 2, 4, 6 and 8 in terms of PEL-MPJPE in Fig.~\ref{fig:ablation} (ii). The results show that $\scriptstyle C=4$ yields lower joint errors compared to sizes of 1 and 2, while nearly the same errors as sizes 6 and 8. Therefore, $\scriptstyle C=4$ is empirically determined to be the appropriate choice for the channel size.

\textbf{3. \# of attention layers}: We compare our model with 0, 1, 2, 4 and 6 layers in Spiking Spatiotemporal Transformer in Fig.~\ref{fig:ablation} (iii). The improvement of PEL-MPJPE is noticeable when using 1 or 2 layers of attention in our spiking transformer. However, this improvement is minimal for 4 and 6 layers, accompanied by a dramatic increase in the number of parameters from 47.7M to 87.4M. 

\begin{figure}[ptb]
    \centering
    \includegraphics[width=\columnwidth]{figures/ablation_study.pdf}
    \beforefigcaption
    \caption{\textbf{Ablation studies.} \textbf{(i) Score Functions} shows the results of four similarity functions used in the spiking spatiotemporal transformer. \textbf{(ii) Channel of Input Voxel} presents the effects of different channel sizes on the performance of pose tracking. \textbf{(iii) \# of Attention Layers} investigates the performance with different number of attention layers in the spiking spatiotemporal transformer.}
    \afterfigcaption
    \label{fig:ablation}
\end{figure}

\beforesubsection
\subsection{Discussion}
\aftersubsection

\begin{figure}[ptb]
    \centering
    \includegraphics[width=\columnwidth]{figures/attention_map.pdf}
    \beforefigcaption
    \caption{\textbf{Visualization of attention score maps.} Our approach determines accurate poses for obscured body parts at an early stage using spiking spatiotemporal attention, in which the query at $t=1$ places significantly greater emphasis on features from later time steps.}
    \afterfigcaption
    \label{fig:attention-map}
\end{figure}

\textbf{Attention scores maps} are shown in Fig.~\ref{fig:attention-map}. For better visualization, we transform the attention score matrix from $THW\times THW$ to $T\times T$, where the attention weights of spatial positions at each time step are summed together. The two examples illustrate that our attention mechanism allows the query at $t=1$ to focus predominantly on features originating from subsequent time steps, thereby providing a more accurate and efficient prediction of body part positions even when they are obscured. The success of these examples can be attributed to its ability to globally adapt to the temporal dependencies present in the input event stream. By emphasizing the relevant features from temporal context, our method can effectively compensate for the lack of information due to occlusion in the initial stages. This results in more accurate and robust pose tracking through time from events only.


\textbf{Failure cases} are displayed in Fig.~\ref{fig:failure-case}, where the pose are not accurately estimated from the events. These cases were primarily attributed to the presence of body part occlusion and the absence of temporal context. The impact of occlusion can be significant, as it hinders the model's ability to detect and analyze essential features required for pose estimation. Moreover, unlike the examples in Fig.~\ref{fig:attention-map}, the lack of temporal context further compounds this issue, as the model cannot effectively leverage information from previous or subsequent frames to compensate for missing or obscured data. Recognizing and addressing these failure cases is crucial for improving the robustness and reliability of our event-based pose tracking method. 

\begin{figure}[ptb]
    \centering
    \includegraphics[width=\columnwidth]{figures/failure_case.pdf}
    \beforefigcaption
    \caption{\textbf{Failure cases.} Owing to the presence of body part occlusion and also the absence of temporal context, our method struggles to accurately estimate poses, as indicated by the red circles.}
    \afterfigcaption
    \label{fig:failure-case}
\end{figure}

\beforesection
\section{Conclusion and Outlook}
\aftersection

We present in this paper a dedicated end-to-end SNN-based approach for event-based pose tracking, where events are the only source of input, thus removing the need of additional RGB or gray-scale images. Our approach is based entirely upon SNNs and our proposed Spiking Spatiotemporal Transformer demonstrates its effectiveness for bi-directional temporal feature compensation. A large-scale synthetic dataset is also constructed, featuring a broad and diverse set of annotated 3D human motions, as well as longer hours of event stream data. Empirical experiments demonstrate the superiority of our approach in both efficacy and efficiency measures. 

For future work, we plan to extend our dataset to incorporate multi-view event streams, as well as the presence and interaction of multiple human characters. They pose new challenges on the development of our SNN-based approach to address multi-view and multi-person scenarios.

%\textbf{Future research}\yxmu{Demonstrate how the synthetic dataset can be used in future work. From Experiments section, the contribution of our synthetic dataset seems not clear. We just show the diversity and the domain similarity. But how this synthetic data can benefit our current hpe task(e.g. can model trained with real and synthetic as data augmentation can achieve better performance?), and even other possible research topic.} could concentrate on several promising avenues to improve the robustness and accuracy of event-based pose tracking. One such direction involves incorporating multi-view event streams, which can significantly enhance the model's ability to handle occlusions, as it can rely on the additional visual information available from different viewpoints to compensate for obscured or missing features. In company with the multi-view setting, the development of SNN-based approach for multi-view spike feature synthesis is another direction. By designing and optimizing SNN-based algorithms for multi-view event-based pose tracking, researchers can further enhance the performance of pose estimation models while maintaining low computational requirements and making them well-suited for real-time applications.

% \appendices




% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank


% % use section* for acknowledgment
% \ifCLASSOPTIONcompsoc
%   % The Computer Society usually uses the plural form
%   \section*{Acknowledgments}
% \else
%   % regular IEEE prefers the singular form
%   \section*{Acknowledgment}
% \fi


% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{}
% Biography text here.
% \end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
% \enlargethispage{-5in}



% that's all folks
\end{document}


