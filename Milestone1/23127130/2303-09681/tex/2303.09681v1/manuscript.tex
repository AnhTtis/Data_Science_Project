
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}

\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{cuted}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\usepackage{dsfont}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
%   \usepackage[nocompress]{cite}
  \usepackage{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
   \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath
\DeclareMathOperator{\Tr}{Tr}




% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Shihao Zou, Yuxuan Mu, Xinxin Zuo, Sen Wang, Li Cheng
        % <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem S. Zou, Y. Mu, X. Zuo, S. Wang and L. Cheng are with the Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada, T6G 2W3. (E-mail: szou2@ualberta.ca, lcheng5@ualberta.ca) Li Cheng is the corresponding author.\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
}% <-this % stops an unwanted space
\thanks{Manuscript received ...; revised ....}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
Event camera, as an emerging biologically-inspired vision sensor for capturing motion dynamics, presents new potential for 3D human pose tracking, or video-based 3D human pose estimation. However, existing works in pose tracking either require the presence of additional gray-scale images to establish a solid starting pose, or ignore the temporal dependencies all together by collapsing segments of event streams to form static image frames. Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs, a.k.a. \textit{dense deep learning}) has been showcased in many event-based tasks, the use of ANNs tends to neglect the fact that compared to the dense frame-based image sequences, the occurrence of events from an event camera is spatiotemporally much sparser.  
Motivated by the above mentioned issues, we present in this paper a dedicated end-to-end \textit{sparse deep learning} approach for event-based pose tracking: 1) to our knowledge this is the first time that 3D human pose tracking is obtained from events only, thus eliminating the need of accessing to any frame-based images as part of input; 2) our approach is based entirely upon the framework of Spiking Neural Networks (SNNs), which consists of Spike-Element-Wise (SEW) ResNet and our proposed spiking spatiotemporal transformer; 3) a large-scale synthetic dataset is constructed that features a broad and diverse set of annotated 3D human motions, as well as longer hours of event stream data, named SynEventHPD. Empirical experiments demonstrate the superiority of our approach in both performance and efficiency measures. For example, with comparable performance to the state-of-the-art ANNs counterparts, our approach achieves a computation reduction of 20\% in FLOPS. Our implementation is made available at \href{https://github.com/JimmyZou/HumanPoseTracking\_SNN}{https://github.com/JimmyZou/HumanPoseTracking\_SNN} and dataset will be released upon paper acceptance.
\end{abstract}

%making it a growing interest in finding methods to process events more efficiently.
%In the spiking transformer, our proposed attention score of normalized Hamming similarity between binary spike vectors is proven to be equivalent to inner product similarity between real valued vectors used in the original transformer. 
%Furthermore, as large-scale datasets are crucial for the success of data-driven approaches in event-based vision tasks, 
%empirically it is shown to lead to both better performance \& greater computation efficiency than that of the ANNs counterpart

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Human Pose Tracking, Event Camera, Spiking Neural Networks
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

% start with human pose estimation, then introduce event camera
As an important area of research in computer vision, visual human pose tracking has attracted increasing research attentions especially in recent years. While most current research efforts have been focused on RGB or Time-of-Flight (ToF) cameras~\cite{shotton2012efficient,zhao2017simple,wang20193d,xu2017lie,wang2014robust,zhou2016sparseness,park20163d,li2015maximum,kanazawa2018end,pavlakos2018learning,xu2019denserac,kolotouros2019learning,lin2021end,humanMotionKanazawa19,kocabas2019vibe,yuan2022glamr}, the emerging event camera~\cite{gallego2019event} presents new opportunities in this area.  As a novel and biologically-inspired vision system, event camera works very differently from the conventional frame-based cameras. In particular, by adopting the unique asynchronous and independent address-event representation, event camera is capable of imaging motions of high speed, and with a very low power consumption. This innovative imaging paradigm has sparked a multitude of research efforts in the field of event-based vision, such as tracking~\cite{gehrig2018asynchronous,mitrokhin2018event,zhang2021object,zhang2022spiking}, recognition~\cite{amir2017low,kim2022ev,fang2021deep}, 3D reconstruction~\cite{rebecq2018emvs,zhang2022discrete}, and a diverse range of applications in robotics, augmented and virtual reality, and autonomous driving~\cite{gallego2019event}. 

% 1. pose estimation from events and gray-scale images (three)
% 2. it has been shown in prior works (hand pose) ANN is able to achieve good performance on pose estimation from events only, but events are much sparser, consider more efficient solution
% 3. SNNs is the solution, but existing works on classification tasks, rather than regression tasks
% 4. we are the first attempt to apply SNNs on difficult regression tasks
% 5. data driven approaches reply on event based data, we propose the largest one for human pose tracking with a variety of poses


% existing works on human pose tracking, problems 1. not use events only, 2. not tracking
Recently, data-driven approaches have shown their potential for effective pose estimation from event cameras.~\cite{calabrese2019dhp19,xu2020eventcap,zou2021eventhpe,rudnev2021eventhands} One of the earliest approach~\cite{calabrese2019dhp19} uses Convolutional Neural Networks (CNNs) to estimate 2D human poses from event frames. EventCap~\cite{xu2020eventcap} expands upon this by capturing fast 3D human motion from the event stream and using a sequence of gray-scale images to establish initial poses over time. The most recent approach, EventHPE~\cite{zou2021eventhpe}, reduces the reliance on gray-scale images by using only the first frame to extract the starting pose and relying solely on the event stream for subsequent pose tracking. Another concurrent work~\cite{rudnev2021eventhands} uses CNNs to estimate static hand pose from event frames. However, these methods either require additional gray-scale images~\cite{xu2020eventcap,zou2021eventhpe} that may not always provide optimal solution for extracting initial or starting poses, or treat the event stream as a static frame~\cite{calabrese2019dhp19,rudnev2021eventhands} that ignore any temporal dependencies of pose tracking over time. As a result, the full potential of human pose tracking using only the event stream remains largely unexploited.

% introduce why consider SNNs not ANNs, problems 1. ANN2SNN (quantization error) and training SNN both verify on classification tasks, 2. SNNs one-way temporal dependency, 3. prior efforts use in-efficient ANNs
% \textcolor{red}{add the analysis of \cite{yao2023attention,zhou2022spikformer}}
Meanwhile, artificial neural networks (ANNs), such as ResNet~\cite{he2016deep} and Transformer~\cite{vaswani2017attention}, have demonstrated their potential in various event-based vision tasks~\cite{zhu18evflownet,amir2017low,rudnev2021eventhands,wang2020eventsr,fang2021deep,fang2021incorporating,kim2022ev,mitrokhin2018event,zhang2021object,sun2022ess,gehrig2019end,gehrig2020video}. However, compared with dense RGB or gray-scale images, event streams are \textit{spatially much sparser}, making it a growing interest in finding methods to process events more efficiently. One promising solution is the use of Spiking Neural Networks (SNNs). Unlike traditional ANNs, SNNs, shown in Fig.~\ref{fig:neuron_model}, employ spiking neurons that imitate the event generation process, enabling the skipping of computations for inactive or non-spiking neurons and thereby improving efficiency. Previous efforts, including converting ANNs to SNNs~\cite{rueckauer2017conversion,han2020rmp,li2021free,yan2021near,deng2021optimal}, or training SNNs from scratch~\cite{fang2021deep,fang2021incorporating,li2021differentiable,yao2022glif,yao2023attention,zhou2022spikformer}, have shown the superiority of SNNs in classification tasks. There are also attempts~\cite{yao2021temporal,zhang2022spiking} proposing a mixed framework of SNNs and ANNs to balance efficiency and performance in event-based tasks. However, the application of only SNNs for pose tracking, a regression task, remains an unresolved problem. There are three main challenges: (1) Unlike spike votes used in classification, regression is sensitive to output values, which may result in additional quantization errors in pose prediction due to compact spike representation in SNNs. (2) As opposed to high-level label prediction in classification, pose tacking requires fine-grained regression of pose at each time step. (3) Spiking neurons typically generate spikes by rolling over time, preserving only one-way temporal dependency in SNNs. This may result in insufficient pose information at early time steps, especially when the person is not moving in the starting phase and thereby producing few events for pose estimation.

% 1. investigate the new problem, 2. Propose SNNs, 3. To address one-way temporal dependency, propose spike spatiotemporal transformer, 4. dataset (next paragraph)
In this work, we tackle a new problem of tracking 3D human poses from only events using SNNs. Specifically, our approach, shown in Fig.~\ref{fig:pipeline}, is an end-to-end model that estimates parametric human poses over time solely from an event stream, without any requirement of dense images. This model is entirely built upon SNNs, providing significantly greater efficiency than ANNs. After preprocessing the input event stream to be a sequence of event voxel grids with the same time interval, Spike-Element-Wise (SEW) ResNet~\cite{fang2021deep} is employed as the SNNs backbone to extract pose spike features. This is followed by our proposed spiking spatiotemporal transformer for the bi-directional fusion of pose spike features, allowing for the compensation of missing pose information, especially for early time steps. In the spiking transformer, the attention score is normalized Hamming similarity between binary spike vectors, which has been proven in Proposition~\ref{proposition:hamming-similarity} to be equivalent to inner product similarity between real valued vectors used in the original transformer~\cite{vaswani2017attention}. The final step is to apply 2D average pooling to the spatiotemporally aggregated spike features and then directly regress the parametric human poses over time. The experiments show that our proposed SNNs achieve competitive or even better performance of pose tracking than several ANNs (\textit{i.e. ResNet-GRU~\cite{kocabas2019vibe}, ResNet-TF~\cite{carion2020end} and Video-Swin~\cite{liu2021swin}}), but requires only about 20\% of FLOPs, which shows promising prospects of efficient SNNs on various event-based vision tasks.


Additionally, we provide a large-scale synthetic dataset for event-based 3D human pose tracking. Although an in-house event-based motion capture dataset, MMHPSD, is introduced in~\cite{zou2021eventhpe}, its limited variety of motions restricts the generalization ability of trained models. To overcome this challenge, we propose to synthesize events data from multiple motion capture datasets (\textit{i.e. Human3.6M~\cite{ionescu2013human3}, AMASS~\cite{mahmood2019amass}, PHSPD~\cite{zou2022human} and MMHPSD-Gray~\cite{zou2021eventhpe}}) and summarize them in Tab.~\ref{tab:dataset-summary}. Combining the existing real dataset of ~\cite{zou2021eventhpe} with our synthetic one, we finally offer a total of 50 hours event stream data with a rich variety of annotated 3D poses. This dataset will be instrumental in further facilitating the research on event-based vision and SNNs for pose tracking.

We summarize our contributions as follows:
\begin{itemize}
    \item We present the first end-to-end approach to the new and challenging task of \textbf{3D human pose tracking from only events, without the need for any dense images. Our proposed solution is fully constructed with SNNs}, providing significantly greater efficiency than ANNs. The extensive experiments demonstrate that our proposed SNN-based approach outperforms or matches the performance of ANN-based models, such as ResNet-GRU~\cite{kocabas2019vibe}, ResNet-TF~\cite{carion2020end} and Video-Swin~\cite{liu2021swin}, while using only about 20\% of the FLOPs required by these ANN-based models.
    \item We propose \textbf{a novel spiking spatiotemporal transformer to address the issue of one-way temporal dependency in SNNs}, enabling the compensation of missing pose information, especially for early time steps. Our proposed attention score, normalized Hamming similarity between binary spike vectors, has been shown to be equivalent to inner product similarity between real valued vectors used in the original transformer~\cite{vaswani2017attention}.
    \item We provide \textbf{a large-scale synthetic dataset for event-based 3D human pose tracking} with a diverse range of annotated motions, named SynEventHPD. This dataset offers a total of 45.72 hours event streams, over 10 times larger than the current largest dataset MMHPSD~\cite{zou2021eventhpe}. After combining these datasets together, we are able to offer a total of 50.11 hours event streams. This dataset will greatly facilitate future research in the field of event-based vision and SNNs.
\end{itemize}


\section{Related Work}
\textbf{Human pose estimation} from RGB or depth images has been a popular topic in computer vision in recent years. Prior to the era of deep learning, earlier works are mainly based on the approaches of random forest~\cite{shotton2012efficient,xu2017lie} or dictionary learning~\cite{wang2014robust,zhou2016sparseness}. Considering the noticeable performance increments brought by deep learning, recent efforts in human pose estimation either directly regress 3D pose from images~\cite{park20163d,li2015maximum} or lift 2D pose estimation to 3D~\cite{zhao2017simple,wang20193d}. This trend is further fueled by the development of SMPL~\cite{loper2015smpl}, a parametric human shape model of low-dimensional statistical representation. HMR~\cite{kanazawa2018end} is the first successful attempt applying convolutional neural networks for human shape recovery from single images. This is followed by a number of works that further include silhouettes~\cite{pavlakos2018learning}, texture map~\cite{xu2019denserac}, 2D pose~\cite{kolotouros2019learning} or vertex-joint interactions~\cite{lin2021end} for the human shape recovery. There are also many efforts that exploit temporal information for the inference of human pose and shape from videos, where temporal constraints~\cite{humanMotionKanazawa19,kocabas2019vibe}, dynamic cameras~\cite{yuan2022glamr} or event signals~\cite{xu2020eventcap,zou2021eventhpe} are considered for better estimation performance. Additionally, other modalities, such as polarization image~\cite{zou2022human}, IMUs~\cite{von2016human} or head-mounted devices~\cite{zhang2022egobody} have also been explored for human pose estimation.

\textbf{Event camera}~\cite{gallego2019event}, as a new bio-inspired technology of silicon retinas, differs from conventional frame-based imaging sensors, such as RGB or Time-of-Flight cameras, in many aspects. The most significant one is the asynchronous and independent \textit{address-event} representation of event camera. The output of an event camera is a sequence of “events” or “spikes”, where each readout event is represented as a tuple $(\mathbf{x}, t, p)$ that means the brightness change (\textit{event}) at pixel position $\mathbf{x}$ (\textit{address}) exceeds a preset threshold at time $t$ with the binary polarity status $p$ indicating the brightness increase or decrease. The amount of output events per second mostly depends on the speed of the motion in the scene. Instead of densely capturing pixel value at a fixed frame rate for frame-based cameras, event camera retains the intensity of each pixel and continuously monitors the intensity change for each pixel asynchronously and independently in case an event occurs at this pixel. Hence the temporal resolution of event camera is much higher than conventional frame-based cameras. Besides, the output event stream is also spatially much sparser, because event camera typically responds to visual stimulus or local motions in the scene, rather than a full stack of pixel intensity.


\textbf{Event-based vision} applications have seen a significant increase in recent years including camera pose estimation~\cite{gallego2017event}, feature tracking~\cite{gehrig2018asynchronous}, optical flow~\cite{zhu18evflownet,hagenaars2021self}, multi-view stereo~\cite{rebecq2018emvs,zhang2022discrete}, hand gesture recognition~\cite{amir2017low} and pose estimation~\cite{rudnev2021eventhands}, motion deblurring~\cite{jiang2020learning,sun2022event}, image restoration and super-resolution~\cite{wang2020eventsr}, image classification~\cite{fang2021deep,fang2021incorporating}, object recognition~\cite{kim2022ev} and tracking~\cite{mitrokhin2018event,zhang2021object,zhang2022spiking}, semantic segmentation~\cite{sun2022ess}, events from/to video~\cite{gehrig2019end,gehrig2020video}, depth estimation~\cite{zhang2022spikedepth}, among others. 
As for human pose estimation or motion capture, DHP19~\cite{calabrese2019dhp19} is perhaps the earliest effort that trains a CNN model to estimate 2D pose and obtain 3D pose by multi-view stereo. EventCap~\cite{xu2020eventcap} aims to capture 3D motions from both events and gray-scale images provided by an event camera. This work starts with a pre-trained CNN-based 3D pose estimation module that takes a sequence of low-frequency gray-scale images as input. The estimated poses are then used as the initial state to infill the intermediate poses for high-frequency motion capture with the constraint of detected event trajectories by~\cite{gehrig2018asynchronous} and the silhouette information gathered from the events. EventHPE~\cite{zou2021eventhpe} further mitigates the requirement of gray-scale images stream to estimate 3D pose over time solely from events, given that the starting pose and shape is known or extracted from the first frame of gray-scale image. It achieves the mitigation by first training a CNN module to infer optical flow via self-supervised learning and then using the flow along with the event stream to predict 3D pose changes through time. Compared with existing efforts~\cite{xu2020eventcap,zou2021eventhpe}, our work uses only events for 3D pose tracking. In place of ANNs, we adopt the efficient biologically-inspired SNNs in this task. Additionally, we propose a novel spiking transformer for the bidirectional fusion of temporal information, achieving competitive performance with only around 20\% of the computational cost of traditional ANNs.


\textbf{Event-based datasets} are crucial for data-driven approaches to demonstrate superior performance in a number of vision tasks. However, most datasets~\cite{deng2009imagenet,ionescu2013human3} are based on standard cameras, which are not feasible to use in event-based tasks due to the fundamental differences between event and standard cameras. This have motivated a variety of event-based datasets released in recent years, including DvsGesture~\cite{amir2017low} for hand gesture recognition, CIFAR10-DVS~\cite{li2017cifar10} and ES-ImageNet~\cite{lin2021imagenet} for object classification, DSEC-Semantic~\cite{sun2022ess} for semantic segmentation and EED~\cite{mitrokhin2018event} and FE108~\cite{zhang2021object} for object tracking. As for event-based human pose estimation, DHP19 dataset~\cite{calabrese2019dhp19} is the earliest one but has limited amount of events data and lacks pose variety. The most recent released dataset is MMPHSPD~\cite{zou2021eventhpe} with event camera and 3 other imaging modalities. Although the dataset provides more than 4.5 hours event stream and 21 different types of action, it still suffers from pose variety because of in-house constraint environment. Our work further augments MMPHSPD dataset by synthesizing events data from several human motion capture datasets (\textit{i.e. Human3.6M~\cite{ionescu2013human3}, AMASS~\cite{mahmood2019amass}, PHSPD~\cite{zou2022human} and MMHPSD-RGB~\cite{zou2021eventhpe}}), and finally provides a large scale dataset with a rich variety of poses for event-based human pose estimation.

\textbf{Spiking neural networks} have emerged recently. Spiking neuron, the basic element in SNNs, imitates the transmitting mechanism in mammalian’s visual cortex.~\cite{gallego2019event} A spiking neuron maintains a membrane potential and modifies it when receiving spikes (\textit{events}) from its connecting neurons. The neuron will generate a spike when its potential exceeds a threshold and reset the potential to initial state. Different from the neuron in traditional ANNs, spiking neuron will not produce any output if it is not spiking, where the computation for this neuron can be skipped. Therefore, SNNs are much more efficient and sparse than ANNs that compute in all neurons at each time step. 

\textbf{Training large-scale SNNs} from scratch presents a significant challenge. To address the non-differentiable issue of neuron spiking function, one branch of works focus on converting trained ANNs to SNNs~\cite{rueckauer2017conversion,han2020rmp,li2021free,yan2021near,deng2021optimal}. In general, ANN2SNN methods map the non-linear activation layer in a trained ANNs to a neuron spiking layer and scale the threshold or the weights accordingly. However, existing methods have only demonstrated their superiority in classification tasks. Their performance in fine-grained regression tasks, such as human pose estimation, is inadequate due to the sensitivity of these tasks to the output values. Another branch of works focus on training SNNs from scratch, following the back-propagation through time (BPTT) framework and applying surrogate derivatives~\cite{li2021differentiable} to approximate the gradient of neuron spiking function. Built on this composition, directly trained SNNs show its excellent performance in the classification tasks~\cite{fang2021incorporating,fang2021deep,li2021differentiable,yao2022glif,yao2023attention,zhou2022spikformer} or regression tasks~\cite{hagenaars2021self}. There have been attempts~\cite{yao2021temporal,zhang2022spiking} proposing a mixed framework of SNNs and ANNs to balance efficiency and performance in event-based tasks. However, our model is entirely built upon the spiking neurons rather than traditional artificial neurons, and showcases its efficiency and efficacy in the fine-grained regression task of human poses tracking. 

\textbf{Spiking Transformer} has emerged recently as a new SNN architecture. To avoid confusion, it is important to clarify that the spiking transformers presented in~\cite{zhang2022spiking,zhang2022spikedepth} are not SNN-based transformers, but rather ANN-based or mixed models. The two recent works~\cite{yao2023attention,zhou2022spikformer} are most related to our proposed spiking spatiotemporal transformer. In MA-SNN~\cite{yao2023attention}, multi-dimensional attention are proposed in SNNs, but this attention is still based on real values of spike charges or membrane potentials, thus violating the efficiency of SNNs. In contrast, we proposes the spatiotemporal attention over binary spike tensors, with theoretical support for the validity of proposed Hamming similarity as the attention score. This is also different from the approach taken by Spikformer~\cite{zhou2022spikformer}, which directly adopts ill-posed scaled dot-product to calculate the similarity of spike vectors.



\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/neural_model.pdf}
    \caption{\textbf{(a) Illustration of spiking neuron model.} A LIF spiking neuron maintains a membrane potential and modifies it when receiving spiking trains from its connected neurons. The neuron will generate output spikes when its potential exceeds a threshold and then reset the potential. \textbf{(b) Feedforward in SNNs.} This process includes potential leaking and charging, neuron spiking and potential resetting. Feed-forwarding typically rolls over time and propagates from layer $l-1$ to layer $l$. \textbf{(c) Backpropagation Through Time in SNNs.} The gradients are normally computed through time and then back-propagated from layer $l$ to layer $l-1$.}
    \label{fig:neuron_model}
\end{figure*}

\section{Preliminary}
\subsection{Spiking Neuron Networks}

\textbf{Spiking neuron model} in Fig.~\ref{fig:neuron_model}~(a) is the commonly used leaky integrate and fire (LIF) model, which is a fundamental unit in SNNs. A LIF neuron maintains a membrane potential $u_{[t]}$ with a leaky constant $\tau$ and modifies it when receiving spiking trains $X_{[t]}$ from its connecting neurons for $T$ time steps. The neuron will output a spike $s_{[t]}$ and reset its potential by $V_{\text{th}} - u_{\text{rest}}$ if its potential exceeds a threshold $V_{\text{th}}$, where soft reset is adopted in our work.
\begin{align}
    & h_{[t]} = u_{[t-1]} - \frac{1}{\tau}(u_{[t-1]} - u_{\text{rest}}) + X_{[t]}, \label{eq:neuron1}\\
    & s_{[t]} = \Theta (h_{[t]} - V_{\text{th}}), \label{eq:neuron2} \\
    & u_{[t]} = h_{[t]} - (V_{\text{th}} - u_{\text{rest}})s_{[t]}, \label{eq:neuron3}
\end{align}
where $\Theta$ is the Heaviside step function as follows,
\begin{equation}\small
    \Theta(h_{[t]} - V_{\text{th}}) =
    \begin{cases}
    1, & \text{if } h_{[t]} - V_{\text{th}} \geq 0\\
    0, & \text{otherwise.}
    \end{cases}
\end{equation}

\textbf{Feedforward in SNNs} in Fig.~\ref{fig:neuron_model}~(b) consists of multiple layers of connected spiking neurons. Assume there are $N^{(l)}$ neurons in the $l$-th layer, we use $\mathbf{u}^{(l)}_{[t]} \in \mathbb{R}^{N^{(l)}}$ and $\mathbf{s}^{(l)}_{[t]} \in \{0, 1\}^{N^{(l)}}$ in vector form to represent their membrane potentials and output spikes at time step $t$. Providing the connecting weights $\mathbf{W}^{{l}}\in \mathbb{R}^{N^{(l)}\times N^{(l-1)}}$ between layer $l-1$ and $l$, feedforward in SNNs can be described as follows,
\begin{align}
    \label{eq:neuron5} 
    \mathbf{h}_{[t]}^{(l)} & = 
    \underbrace{\lambda \mathbf{u}^{(l)}_{[t-1]}}_{\text{leak}} + 
    \underbrace{\mathbf{W}^{(l)} \mathbf{s}^{(l-1)}_{[t]}}_{\text{charge}}, \\
    \label{eq:neuron6} 
    \mathbf{s}^{(l)}_{[t]} & = \underbrace{
    \Theta (
        \mathbf{h}_{[t]}^{(l)} - V_{\text{th}}
    )}_{\text{spike}}, \\
    \label{eq:neuron7}
    \mathbf{u}^{(l)}_{[t]} & = 
    \mathbf{h}_{[t]}^{(l)}
    \underbrace{- V_{\text{th}}\mathbf{s}^{(l)}_{[t]}}_{\text{reset}}, 
\end{align}
where $u_{\text{rest}}$ is usually set to be 0 and $\lambda=1-\frac{1}{\tau}$ is the leaky constant of LIF neuron model.

\textbf{Computational consumption} of SNNs are quite lower than ANNs because of the binary output of spiking neurons. Considering binary spikes $\mathbf{s}^{(l-1)}_{[t]}$ in layer $l-1$ and $\mathbf{s}^{(l)}_{[t]}$ in layer $l$, the computations of inactive neurons ($s_{[t]}=0$) can be skipped. Assuming the spiking rate is $\rho$ for the layer, from Eq.~(\ref{eq:neuron5}) and~(\ref{eq:neuron6}), a linear layer plus a spiking layer in SNN requires only $\mathcal{O}(\rho TN^{(l-1)} N^{(l)})$ FLOPs~\footnote{FLOPs mean the number of floating point operations.} for $T$ time steps, while in traditional ANNs, a linear layer plus a non-linear ReLU layer requires $\mathcal{O}(TN^{(l-1)} N^{(l)})$ FLOPs. Normally, $\rho$ is around 20\% in average, which means SNNs require only around $20\%$ FLOPs of ANNs. 

\textbf{Backpropagation through time in SNNs} is shown in Fig.~\ref{fig:neuron_model}~(c). Given the gradients from the last layer $\frac{\partial \mathcal{L}}{\mathbf{s}^{(l)}_{[t]}}$, we can unfold the rolling update of membrane potential for $T$ time steps and calculate the backpropagate gradients $\frac{\partial \mathcal{L}}{\partial \mathbf{s}^{(l-1)}_{[t]}}$ and parameters gradients $\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}$ as follows\footnote{Numerator layout of matrix differentiation is used.},
\begin{equation} \footnotesize
    \frac{\partial \mathcal{L}}{\partial \mathbf{s}^{(l-1)}_{[t]}} = \sum_{k=t}^{T} 
    \underbrace{\frac{\partial \mathcal{L}}{\partial \mathbf{s}^{(l)}_{[k]}}}_{\substack{\text{gradient from} \\ \text{last layer}}}
    \underbrace{\frac{\partial \mathbf{s}^{(l)}_{[k]}}{\partial \mathbf{h}^{(l)}_{[k]}}}_{\substack{\text{surrogate} \\ \text{gradient}}}
    \Big(
        \mathbf{1} +
        \prod_{\tau=t-1}^{k-1} 
        \big( 
            \lambda - V_{\text{th}}\underbrace{\frac{\partial \mathbf{s}^{(l)}_{[\tau]}}{\partial \mathbf{h}^{(l)}_{[\tau]}}}_{\substack{\text{surrogate} \\ \text{gradient}}}
        \big)
    \Big)
    \mathbf{W}^{(l)},
\end{equation}
\begin{equation} \footnotesize
    \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}} = \sum_{t=0}^T 
    \frac{\partial \mathcal{L}}{\partial \mathbf{s}^{(l)}_{[t]}}
    \frac{\partial \mathbf{s}^{(l)}_{[t]}}{\partial \mathbf{h}^{(l)}_{[t]}}
    \bigg(
        \mathbf{s}_{[t]}^{(l)} + 
        \sum_{k=0}^{t-1} 
        \Big( 
            \prod_{\tau=k}^{t-1} 
            \lambda \big( 
                1 - V_{\text{th}}
                \frac{\partial \mathbf{s}^{(l)}_{[\tau]}}{\partial \mathbf{h}^{(l)}_{[\tau]}}
            \big) 
        \Big) 
        \mathbf{s}^{(l)}_{[k]}
    \bigg).
\end{equation}
Detailed derivatives are provided in the supplementary material. Training SNNs from scratch is difficult mainly due to the non-differentiable property of Heaviside step function and the problem of gradient vanishing. Existing efforts summarized in~\cite{li2021differentiable} solve it by using surrogate derivatives to approximate the gradients of Heaviside step function. Followed~\cite{fang2021deep}, the surrogate gradient function we used in this work is
\begin{equation}
    \frac{\partial s_{[t]}}{\partial h_{[t]}} = 
    % \Theta'(h_{[t]} - V_{\text{th}}) =
    \begin{cases}
    \frac{c}{2(1+(\frac{\pi}{2}c (h_{[t]} - V_{\text{th}}))^2)}, & \text{if } s_{[t]} = 1\\
    0,              & \text{otherwise}
    \end{cases}
\end{equation}
where $c$ is the hyper-parameter to control the smoothness of the surrogate gradients. 
\subsection{Original transformer}
\label{sec:conventional-transformer}
\textbf{Scaled dot-product attention in original transformer} \cite{vaswani2017attention} is
\begin{equation}\small
    \label{eq:attention}
    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d_k}})\mathbf{V},
\end{equation}
where $\mathbf{Q}, \mathbf{K} \in \mathbb{R}^{N\times d_k}$ are queries and keys and $\mathbf{V}\in \mathbb{R}^{N\times d_v}$ is values, with $N$ being the length of input sequence, $d_k$ the dimension of a single query $\mathbf{q}$ or key $\mathbf{k}$, and $d_v$ the dimension of a single value $\mathbf{v}$. The scaling factor of $1/\sqrt{d_k}$ is to normalize the dot-product $\mathbf{q}\mathbf{k}^{\top}$ to have mean 0 and variance 1 if the components of $\mathbf{q}$ and $\mathbf{k}$ are assumed to be independent variables with mean 0 and variance 1.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/pipeline.pdf}
    \caption{\textbf{Pipeline of 3D human pose tracking from only events using SNNs.} There are four main sections: 
    \textbf{(1) Preprcessing} in Sec.~\ref{sec:preprocess} converts a stream of events into a sequence of event voxel grids with the same time interval. 
    \textbf{(2) SEW-ResNet}~\cite{fang2021deep} described in Sec.~\ref{sec:sew-resnet} is used as backbone to extract pose spike features given the input event voxel grids, where hidden neurons in spiking (active) status are shown in blue. 
    \textbf{(3) A novel spiking spatiotemporal transformer} is proposed in Sec.~\ref{sec:spike-transformer} for the bi-directional fusion of pose spike features. The proposed spiking spatiotemporal attention aims to address the one-way flow of information over time in SNNs, allowing for the compensation of missing pose information, particularly for the early time steps.
    \textbf{(4) Parametric human poses and shapes estimation} is presented in Sec.~\ref{sec:regression}, where average pooling is applied to the spatiotemporally aggregated spike features before regression of human pose and shape parameters. Note that our model is completely built upon the SNNs, which has been shown to require much less computations without any performance reduction in human pose tracking.}
    \label{fig:pipeline}
\end{figure*}

\section{Our approach}
Our approach is summarized in Fig.~\ref{fig:pipeline}, which involves four main sections. (i) Preprcessing, described in Sec.~\ref{sec:preprocess}, is to convert an stream of event into a sequence of event voxel grids~\cite{gallego2019event} with the same time interval. (ii) SEW-ResNet~\cite{fang2021deep}, presented in Sec.~\ref{sec:sew-resnet}, is employed as the backbone to extract pose spike features from the input event frames. (iii) Since SNNs only consider the one-way temporal relationship, we propose a novel spiking spatiotemporal transformer for the bi-directional fusion of pose spike features in Sec.~\ref{sec:spike-transformer}, allowing for the compensation of missing pose information, especially for the early time steps. (iv) The final stage, illustrated in Sec.~\ref{sec:regression}, is to apply average pooling to the spatiotemporally aggregated spike features and then directly regress the parametric body shapes over time. In this work, we reply solely on events without the gray-scale images in~\cite{xu2020eventcap} or the prior knowledge of starting pose in~\cite{zou2021eventhpe}. Furthermore, our model is completely built upon SNNs instead of traditional ANNs or mixed architecture, which has been shown to require much less computations without any performance reduction in human pose tracking.


\subsection{Preprocessing}
\label{sec:preprocess}
A stream of events provided by event cameras is a set of asynchronous and independent event signals. We decompose the input event stream into a sequence of $T$ events packets with each packet lasting the same length of time, $\mathcal{E}=\{\mathcal{E}_{[t]}\}_{t=1}^T$. We use $t$ as the index of a particular packet in the sequence for simplicity. Following~\cite{gehrig2019end,zou2021eventhpe}, we convert each events packet $\mathcal{E}_{[t]}$ to a voxel grid $H\times W\times C$ with each voxel representing a particular pixel and time interval. The voxel value will be 1 if the number of events within the voxel is higher than a preset threshold. Otherwise, it will be 0. This representation preserves better temporal information of events than collapsing them on a single frame for each packet and is also not sensitive to the number events used.~\cite{gallego2019event} Finally, we have the processed sequence of event voxel grids as the input to SNNs, denoted by $\mathbf{S}^{\text{in}}\in \{0, 1\}^{T\times H\times W\times C}$. \footnote{
For clarity, we will generally refer to the size of input spike tensor for different blocks or modules as $\footnotesize T\times H\times W\times C$ in subsequent sections.}

\subsection{Spike-Element-Wise Residual Networks}
\label{sec:sew-resnet}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/spiking_cnn.pdf}
    \caption{\textbf{Architecture of SEW-ResNet34.} Given the input spike tensor $\scriptstyle \mathbf{S}^{\text{in}}\in \{0, 1\}^{T\times H\times W\times C}$, the output spike tensor is $\scriptstyle \mathbf{S}^{\text{out}}\in \{0, 1\}^{T\times \frac{H}{32}\times \frac{W}{32}\times C^{\text{out}}}$ where $\scriptstyle C^{\text{out}}=512$ for SEW-ResNet34. SEW-ResNet is comprised of two types of blocks: the downsample block and the basic block. The downsample block reduces the spatial size of the input spike tensor by 2 and increases the channel size by 4 through convolutional layers, while the basic block maintains the size of the input spike tensor for residual learning in SNNs. The final layer in both types of blocks is element-wise identity mapping via \textit{SEW Function}, where spike-element-wise functions between two input spike tensors are applied, such as ADD, AND or IAND.}
    \label{fig:spiking-cnn}
\end{figure}

SEW-ResNet, proposed in~\cite{fang2021deep}, is one of the most popular architectures of SNNs. It is largely based on ResNet~\cite{he2016deep}, but with significant differences in the design of the identity mapping for SNNs, where the element-wise addition is applied to spike tensors rather than pre-spiking potentials. This design achieves accurate identity mapping of residual learning in SNNs and also addresses the vanishing or exploding gradient problem. As a result, we use SEW-ResNet as the backbone in our pipeline to extract spike pose features. We show the detailed architecture of SEW-ResNet34~\cite{fang2021deep} in Fig.~\ref{fig:spiking-cnn}. There are also options of SEW-ResNet18, 50, 101 and 152 which have a similar architecture to SEW-ResNet34. The spatial size of the output is $1/32$ of the input with a channel size $C^{\text{out}}$ of $512$ for SEW-ResNet18, 34 and $2048$ for SEW-ResNet50, 101, 152.

% % include in the caption
SEW-ResNet consists of two types of blocks: the downsample block and the basic block. The downsample block normally reduces the spatial size of input spike tensor by 2 and expands the channel size by 4 through convolutional layers, while the basic block keeps the size of input spike tensor unchanged for residual learning. The final layer in both types of blocks is element-wise identity mapping via \textit{SEW Function}, where spike-element-wise functions between two input spike tensors are applied, such as ADD, AND or IAND. Given the input spike tensor $\mathbf{S}^{\text{in}}\in \{0, 1\}^{T\times H\times W\times C}$, the output spike tensor is assumed to be $\mathbf{S}^{\text{out}}\in \{0, 1\}^{T\times \frac{H}{32}\times \frac{W}{32}\times C^{\text{out}}}$ where $C^{\text{out}}=512$ for SEW-ResNet18 and 34, $C^{\text{out}}=2048$ for SEW-ResNet50, 101 and 152.


\subsection{Spiking Spatiotemporal Transformer}
\label{sec:spike-transformer}
\begin{figure}[ptb]
    \centering
    \includegraphics[width=\columnwidth]{figures/spiking_transformer.pdf}
    \caption{\textbf{(a) Architecture of Spiking Spatiotemporal Transformer} that is entirely based on SNNs and can also be stacked by $N$ layers. \textbf{(b) Architecture of Spiking Spatiotemporal Attention} that aims to address the one-way flow of information over time in SNNs, thus compensating for missing pose information, particularly for the early time steps. Normalized Hamming similarity is proposed for the attention score, which has been shown to be equivalent to inner product similarity between real valued vectors used in original transformer.}
    \label{fig:spiking-transformer}
\end{figure}

\textbf{Spiking Spatiotemporal Transformer} is shown in Fig.~\ref{fig:spiking-transformer}~(a). Given the input $\mathbf{S}^{\text{in}}\in \{0,1\}^{T \times H \times W\times C}$, the first step is to apply spiking spatiotemporal attention to combine bidirectional space-time features. A more comprehensive explanation of the attention module will be given later. It
is followed by two linear spiking layer with batch normalization, also known as Feed-Forward Network (FFN) in the original transformer~\cite{vaswani2017attention}. The final step in the module is to apply SEW Function to the output of FNN and input spike tensor for residual learning. Then we get the output $\mathbf{S}^{\text{out}}\in \{0,1\}^{T\times H\times W \times C}$. This entire module can be stacked by $N$ layers similar to the original transformer~\cite{vaswani2017attention}.

\textbf{Spiking Spatiotemporal Attention} is illustrated in Fig.~\ref{fig:spiking-transformer}~(b) in detail. This module mainly addresses the issue of one-way temporal dependency in spiking layers of SNNs by attending self-attention on the space-time domain of spike tensors. To be specific, given the input spike tensor $\mathbf{S}^{\text{in}}\in \{0,1\}^{T\times H \times W \times C}$, we first map the channel size to $C_k$ through a linear spiking layer with batch normalization and flatten the spatiotemporal domain to generate a sequence of $THW$ spike queries and keys, $\mathbf{S}^{q},\mathbf{S}^{k}\in \{0,1\}^{THW \times C_k}$. 
The values $\mathbf{V}$ are mapped to have the channel size $C_v$ through a linear layer, and flattened to be $\mathbf{V} \in \mathbb{R}^{THW\times C_v}$, which keeps to be real-values without spiking. Next, the similarity scores between the spiking queries and keys are calculated $f(\mathbf{S}^{q},\mathbf{S}^{k})$, with its details covered later. The softmax function is then applied to obtain normalized attention weights $\boldsymbol{\alpha}$ for values aggregation, $\boldsymbol{\alpha}\mathbf{V}$. Afterwards, the aggregated values are unflattened and subjected to batch normalization before producing the output spike tensor. A spiking linear layer with batch normalization is followed to map the size of channel from $C_v$ to $C$. Finally, the SEW Function is applied to the output of attention and the input spike tensor for the residual learning, resulting in $\mathbf{S}^{\text{att}}\in \{0,1\}^{T\times H \times W \times C}$. It is important to note that our model also supports multi-head attention, which follows the similar pattern as the original transformer.

\textbf{Positional encodings} are added when the spiking spatiotemporal attention module is in the first layer to make the model aware of ordinal information in the input sequence. As the input spikes are binary while positional encodings are float numbers, direct addition would violate the fast computation of SNNs. So we add them after the linear layer and before batch normalization and the spiking layer. Besides, as spiking layer generates spikes by rolling over $T$ time steps, we scale the positional encodings by $1/T$ to maintain consistency across models with varying $T$. The definition mostly follows~\cite{vaswani2017attention} as
\begin{equation} \small \begin{aligned}
    & \text{PE}(\text{pos}, 2i) = \frac{1}{T}\sin (\text{pos} / 10000^{2i/ C_k}), \nonumber \\
    & \text{PE}(\text{pos}, 2i+1) = \frac{1}{T}\cos (\text{pos} / 10000^{2i/ C_k}), \nonumber 
\end{aligned} \end{equation} 
where $\text{pos}$ represents the position in the sequence, while $2i$ or $2i+1$ denotes the position of $C_k$ channel.

\textbf{Attention scores} of inner product used in the original transformer~\cite{vaswani2017attention} is not well-defined in our spiking transformer. If the spike key is a zero vector, $\mathbf{s}^{k}_j=\mathbf{0}$, then the attention score will always be zero for any spike query $\mathbf{s}^{q}_i$, that is $\mathbf{0}^{\top} \cdot \mathbf{s}^{q}_j=0$. This means the inner product used in~\cite{vaswani2017attention} is not able to accurately determine the similarity between two spike vectors.

\begin{proposition}[Johnson–Lindenstrauss Lemma on Binary Embedding~\cite{jacques2013robust,yi2015binary}]
\label{proposition:hamming-similarity}
Let $\mathbf{q}_i, \mathbf{k}_j \in\mathbb{R}^{d_k}$ be a single query and key of \textit{real}-valued points in the original transformer described in Sec.~\ref{sec:conventional-transformer}. Define $\mathbf{s}^q_i, \mathbf{s}^k_j \in \{0, 1\}^{C_k}$ as the corresponding \textit{binary} embedding defined as 
\begin{equation}\small
    \mathbf{s}^q_i(\mathbf{q}_i) = \text{sign}(\mathbf{Aq}_i),\quad \mathbf{s}^k_j(\mathbf{k}_j) = \text{sign}(\mathbf{Ak}_j),\nonumber
\end{equation}
where $\mathbf{A}\in \mathbb{R}^{d_k\times C_k}$ is a projection matrix with each entry generated independently from the normal distribution $\mathcal{N}(0, 1)$. Given that $\scriptstyle C_k > \frac{\log M}{\delta^2}$, we have
\begin{equation}\small
    |d_{\mathcal{H}}(\mathbf{s}^q_i, \mathbf{s}^k_j) - d(\mathbf{q}_i, \mathbf{k}_j)| \leq \delta,
\end{equation}
with probability at least $1-2e^{-\delta^2 C_k}$, where $M$ is the number of all possible keys and queries given by the finite training set. Here $d_{\mathcal{H}}$ is the normalized Hamming similarity defined as
\begin{equation}\small
    \label{eq:hamming-similarity}
    d_{\mathcal{H}}(\mathbf{s}_i^q, \mathbf{s}_j^k) = 1 - \frac{1}{C_k}\sum_{c=1}^{C_k} \mathds{1}(\mathbf{s}_{ic}^q \neq \mathbf{s}_{jc}^k),
    % = \frac{1}{2}(1-\frac{\mathbf{s}_i^{\top}\mathbf{s}_j}{d_v}),
\end{equation}
and $d$ is normalized cosine similarity defined as
\begin{equation}\small
    d(\mathbf{q}_i, \mathbf{k}_j) =  \frac{\mathbf{q}_i^{\top}\mathbf{k}_j}{\|\mathbf{q}_i\|\|\mathbf{k}_j\|}.
\end{equation}
\end{proposition}

Based on Proposition~\ref{proposition:hamming-similarity}, in spiking spatiotemporal attention, we propose normalized Hamming similarity, $f=d_{\mathcal{H}}$, as the attention scores between spike queries and keys. When the channel size $C_v$ is large enough, our proposed similarity function ensures equivalency to similarity between real-valued queries and keys in original transformer.

\textbf{The gradient of Hamming similarity} does not exist since Eq.~\ref{eq:hamming-similarity} is a non-differentiable function. Thus we approximate $d_{\mathcal{H}}$ by
\begin{equation}\small
    \label{eq:hamming-similarity-approx}
    d_{\mathcal{H}}(\mathbf{s}_i^q, \mathbf{s}_j^k) = 1 - \frac{1}{C_k}\sum_{c=1}^{C_k} \left[ \mathbf{s}_{ic}^q\cdot(1-\mathbf{s}_{jc}^k) +  (1-\mathbf{s}_{ic}^q)\cdot\mathbf{s}_{jc}^k \right].
\end{equation}
As a result, the approximate gradients of normalized Hamming similarity function are given by:
\begin{equation}
    \frac{\partial d_{\mathcal{H}}(\mathbf{s}_i^q, \mathbf{s}_j^k)}{\partial \mathbf{s}_i^q} = \frac{2\mathbf{s}_j^k - 1}{C_k}, \quad
    \frac{\partial d_{\mathcal{H}}(\mathbf{s}_i^q, \mathbf{s}_j^k)}{\partial \mathbf{s}_j^k} = \frac{2\mathbf{s}_i^q - 1}{C_k}. \nonumber
\end{equation}


\subsection{Parametric Pose and Shape Regression}
\label{sec:regression}
\begin{figure}[ptb]
    \centering
    \includegraphics[width=\columnwidth]{figures/regression.pdf}
    \caption{\textbf{Human Poses and Shapes Regression.} The input spike tensor first undergoes 2D average pooling and is then fed into three linear layers in parallel to regress the global translation $\mathbf{d}$ and SMPL pose and shape parameters $\boldsymbol{\theta}, \boldsymbol{\beta}$ across all $T$ time steps.}
    \label{fig:regression}
\end{figure}

\textbf{Parametric human pose and shape} used in this work is SMPL model~\cite{loper2015smpl}. Given the shape parameters $\boldsymbol{\beta}$, pose parameters $\boldsymbol{\theta}$ and global translations $\mathbf{d}$, the model outputs a triangular mesh with 6,890 vertices at each time step, that is $\mathcal{M}(\boldsymbol{\beta}, \boldsymbol{\theta}, \mathbf{d})\in\mathbb{R}^{T\times 6890\times 3}$ for $T$ time steps in total. The shape parameters at time step $t$, denoted as $\boldsymbol{\beta}_{[t]}\in\mathbb{R}^{1\times 10}$, are linear coefficients of PCA shape space, learned from a large number of registered body scans. These parameters mainly describe individual body features such height, weight and body proportions. The pose parameters at time step $t$, denoted as $\boldsymbol{\theta}_{[t]}\in\mathbb{R}^{1\times 72}$, represent the articulated poses of the triangular mesh, consisting of a global rotation and relative rotations of the 24 joints in axis-angle form. The global translations of human body at time step $t$ is denoted by $\mathbf{d}_{[t]}\in \mathbb{R}^{1\times 3}$. To produce the final parametric shapes, the template body is deformed using shape- and pose-dependent deformations, articulated through forward kinematics to its target pose, and further transformed through linear blend skinning and global translation. Meanwhile, the 3D and 2D joint positions, denoted as $\mathbf{j}_{\text{3D}}$ and $\mathbf{j}_{\text{2D}}$, are obtained by regressing from the output vertices and projecting the 3D joints onto the 2D images.

We show the process in Fig.~\ref{fig:regression} where we apply the 2D average pooling to the input spike tensor and then directly regress the shape parameters $\boldsymbol{\hat \beta}$, pose parameters $\boldsymbol{\hat \theta}$ and global translations $\mathbf{\hat d}$ via three linear layers in parallel. Based on the predicted parameters, we obtain the corresponding parametric shapes and joint positions $\mathbf{\hat j}^{\text{3D}}, \mathbf{\hat j}^{\text{2D}}$ by SMPL model across $T$ time steps. When projecting 3D joints on 2D images, as the global translations under the camera coordinate are predicted, we can use predefined camera intrinsic parameters to reduce the redundancy of prediction.

\textbf{The training losses for} our model can be expressed as follows:
\begin{equation}\small
    \mathcal{L}=
    \lambda_{\text{pose}}\mathcal{L}_{\text{pose}}+
    \lambda_{\text{shape}}\mathcal{L}_{\text{shape}}+
    \lambda_{\text{trans}}\mathcal{L}_{\text{trans}}+
     \lambda_{\text{3D}}\mathcal{L}_{\text{3D}}+
    \lambda_{\text{2D}}\mathcal{L}_{\text{2D}},
    \nonumber
\end{equation}
where $\lambda_{\text{pose}}$, $\lambda_{\text{shape}}$, $\lambda_{\text{trans}}$, $\lambda_{\text{3D}}$ and $\lambda_{\text{2D}}$ are the corresponding loss weights. For the poses loss, we use the 6D representation of rotations, which has been shown to perform better than the 3D axis-angle representation in~\cite{zhou2019continuity,zou2021eventhpe} for human pose estimation. Then we use the geodesic distance in $SO(3)$ to measure the distance between the predicted and target poses:
\begin{equation}\small
    \mathcal{L}_{\text{pose}} = \sum_{t=1}^{T}\sum_{j=1}^{24} \arccos^2\Big(\frac{\Tr\big(R^{\top}(\boldsymbol{\theta}^{j}_{[t]})R(\boldsymbol{\hat \theta}^{j}_{[t]})\big)-1}{2}\Big),
\end{equation}
where $R(\cdot)$ is the function that transforms the 6D rotational representation to the $3\times3$ rotation matrix and $j$ is the joint index. Other losses are basically Euclidean distances between the predicted and target values as follows:
\begin{equation} \small \begin{aligned}
    & \mathcal{L}_{\text{shape}} = \sum_{t=1}^{T} \|\boldsymbol{\beta}_{[t]} - \boldsymbol{\hat \beta}_{[t]}\|^2, \\
    & \mathcal{L}_{\text{trans}} = \sum_{t=1}^{T} \|\mathbf{d}_{[t]} - \mathbf{\hat d}_{[t]}\|^2,\\
    & \mathcal{L}_{\text{3D}} = \sum_{t=1}^{T} \sum_{j=1}^{24} \|\mathbf{j}^j_{\text{3D}[t]} - \mathbf{\hat j}^j_{\text{3D}[t]}\|^2, \\
    & \mathcal{L}_{\text{2D}} = \sum_{t=1}^{T} \sum_{j=1}^{24} \|\mathbf{j}^j_{\text{2D}[t]} - \mathbf{\hat j}^j_{\text{2D}[t]}\|^2.
\end{aligned} \end{equation}

\section{Experiments}
\subsection{Large-Scale Dataset for Event-based Pose Tracking}
\begin{figure}[ptb]
    \centering
    \includegraphics[width=\columnwidth]{figures/dataset_demo_frames.pdf}
    \caption{\textbf{Example Sample from Each Dataset.} The events are displayed as an event frame for better visualization with corresponding RGB or gray-scale image and annotated pose. 
    The synthetic events from SynMMHPSD are visually similar to the real events from MMHPSD. Samples from the other three datasets further highlight the effectiveness of our large-scale synthetic dataset for human pose tracking.}
    \label{fig:dataset-demo-frame}
\end{figure}

\begin{figure}[ptb]
    \centering
    \includegraphics[width=0.65\columnwidth]{figures/poses_tsne.pdf}
    % \includegraphics[width=\columnwidth]{figures/dataset_demo_frames.pdf}
    \caption{\textbf{t-SNE Visualization of Poses in Each dataset.} MMHPSD~\cite{gehrig2020video} only covers small areas, while our synthetic dataset SynEventHPD, including EventH36M, EventAMASS, EventPHSPD and SynMMHPSD, span a wide range of pose areas. This highlights the rich variety of poses provided in SynEventHPD, in contrast to the limited range in MMHPSD.}
    \label{fig:poses-tsne}
\end{figure}

Currently, the largest event-based dataset for human pose estimation is MMHPSD, which includes 15 subjects, 21 different actions and a total of 4.39 hours of event streams~\cite{zou2021eventhpe}. However, due to its in-house constrained environment, this dataset's limited variety of motions restricts the generalization ability of trained models. To address this issue, we propose to synthesize event data from multiple motion capture datasets, including \textit{i.e. Human3.6M~\cite{ionescu2013human3}, AMASS~\cite{mahmood2019amass}, PHSPD~\cite{zou2022human} and MMHPSD-Gray~\cite{zou2021eventhpe}}, to construct a large-scale synthetic dataset. Our synthetic dataset, which we call SynEventHPD, includes EventH36M, EventAMASS, EventPHSPD, and SynMMHPSD, providing a total of 45.72 hours of event streams, more than 10 times that of MMHPSD. We visualize the distribution of poses of all the dataset in Fig.~\ref{fig:poses-tsne} to highlight the variety. Other details are summarized in Table~\ref{tab:dataset-summary}. This dataset will be public available for research purpose upon paper acceptance.

\textbf{Process of events data synthesis} is mainly based on the method proposed in~\cite{gehrig2020video}. Firstly, we crop frames according to a global bounding box of person in the video and resize them to $512\times 512$, aiming to have consistent image size for different datasets. Next, we apply frame interpolation to increase the frame rate of provided videos, guided by the predicted optical flows. We convert the high frame rate videos to gray-scale images and generate events by checking the brightness change at each pixel over time. This process is straight forward for Human3.6M~\cite{ionescu2013human3}, PHSPD~\cite{zou2022human} and MMHPSD~\cite{zou2021eventhpe} as they provide RGB or gray-scale videos. However, AMASS~\cite{mahmood2019amass} dataset only contains motion capture data without any RGB videos. To address this, for each motion capture sequence, we animate an avatar selected randomly from 15 different avatars and render the corresponding RGB videos of size $512\times 512$ using one of the 4 pre-defined lightnings.
Annotations provided in our dataset include pose and shape parameters of SMPL model, corresponding 2D/3D joint positions and the global translation under the default camera intrinsic parameters. We demonstrate the effectiveness of our large-scale synthetic dataset by showing four examples of images, event frames, and annotated poses in Fig.~\ref{fig:dataset-demo-frame}. Additional examples are included in the supplementary material.

\begin{table}[ptb]
    \centering
    \caption{\textbf{Summary of event-based datasets for 3D human pose tracking}, compared in terms of real or synthetic data (R/S), number of number of subjects (Sub \#), number of event streams (Str \#), total time length of all the event streams in hours (Len), average time length of each stream in minutes (AvgLen), annotated poses (Pose).}
    \setlength{\tabcolsep}{1.5mm}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{@{}|c|cccccc|@{}}
    \bottomrule \hline
        \makecell[c]{Dataset} & \makecell[c]{R/S} & \makecell[c]{Sub \\ \#} & \makecell[c]{Str \\ \#} & \makecell[c]{Len \\ (hrs)} & \makecell[c]{AvgLen \\ (mins)} & \makecell[c]{Pose} \\
    \hline
        \makecell[c]{MMHPSD~\cite{zou2021eventhpe}} & \makecell[c]{Real} & \makecell[c]{15} & \makecell[c]{178} & \makecell[c]{4.39} & \makecell[c]{1.48}  & \makecell[c]{\checkmark} \\
    \hline
        \makecell[c]{EventH36M~\cite{ionescu2013human3}} & \makecell[c]{Syn} & \makecell[c]{7} & \makecell[c]{835} & \makecell[c]{12.46} & \makecell[c]{0.90} & \makecell[c]{\checkmark} \\
        \makecell[c]{EventAMASS~\cite{mahmood2019amass}} & \makecell[c]{Syn} & \makecell[c]{13} & \makecell[c]{8028} & \makecell[c]{23.54} & \makecell[c]{0.18}  & \makecell[c]{\checkmark} \\
        \makecell[c]{EventPHSPD~\cite{zou2022human}} & \makecell[c]{Syn} & \makecell[c]{12} & \makecell[c]{156} & \makecell[c]{5.33} & \makecell[c]{2.05}  & \makecell[c]{\checkmark} \\
        \makecell[c]{SynMMHPSD~\cite{zou2021eventhpe}} & \makecell[c]{Syn} & \makecell[c]{15} & \makecell[c]{178} & \makecell[c]{4.39} & \makecell[c]{1.48}  & \makecell[c]{\checkmark} \\
    \hline
        \makecell[c]{SynEventHPD (Total)} & \makecell[c]{Syn} & \makecell[c]{47} & \makecell[c]{9197} & \makecell[c]{45.72} & \makecell[c]{0.30} & \makecell[c]{\checkmark} \\
    \hline \toprule  
    \end{tabular}
    }
    \label{tab:dataset-summary}
\end{table}


\subsection{Empirical Results on MMHPSD Dataset}

\begin{table*}[phtb]
    \centering
    \caption{\textbf{Quantitative results of human pose tracking on MMHPSD test set.} We use \textit{V}, \textit{G} and \textit{E} to represent the input data of video, first gray-scale frame and events respectively. \textit{VIBE}~\cite{kocabas2019vibe} and \textit{MPS}~\cite{wei2022capturing} are video-based ANN methods. \textit{EventCap}~\cite{xu2020eventcap} and \textit{EventHPE}~\cite{zou2021eventhpe} are most recent works using event camera for human pose tracking. We also include three popular ANN models (\textit{ResNet-GRU}~\cite{kocabas2019vibe}, \textit{ResNet-TF}~\cite{carion2020end} and \textit{Video-Swin}~\cite{liu2021swin}), one mixed models (\textit{SEW-ResNet-TF}) and four SNN models (\textit{ANN2SNN}~\cite{rueckauer2017conversion}, \textit{SEW-ResNet}~\cite{fang2021deep}, \textit{Attention-SNN}~\cite{yao2023attention} and \textit{SpikeFormer}~\cite{zhou2022spikformer}) to illustrate the effectiveness of our SNN approach with novel spiking spatiotemporal transformer. 
    Underline denotes the best value except EventHPE(GT).
    $^{\dagger}$\textit{EventHPE(GT)} means the ground-truth starting pose is known in EventHPE, which is considered as the upper bound among all these models due to its perfect information of the starting pose in the first frame.}
    \setlength{\tabcolsep}{0.5mm}
    \renewcommand{\arraystretch}{1.3}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{@{}|c|c|c|c|cccc|cccc|@{}}
    \bottomrule \hline
        \multirow{2}{*}{Method} & \multirow{2}{*}{\makecell[c]{ANN/\\SNN}} & \multirow{2}{*}{Input} & \multirow{2}{*}{Params} & \multicolumn{4}{c|}{T=8 (1 sec)} & \multicolumn{4}{c|}{T=64 (8 secs)}\\
        \cline{5-12}
        & & & & \makecell[c]{FLOPs} & \makecell[c]{MPJPE $\downarrow$} & \makecell[c]{PEL-MPJPE $\downarrow$} & \makecell[c]{PA-MPJPE $\downarrow$} & \makecell[c]{FLOPs} & \makecell[c]{MPJPE $\downarrow$} & \makecell[c]{PEL-MPJPE $\downarrow$} & \makecell[c]{PA-MPJPE $\downarrow$}\\
    \hline
        % my 256x256 43.125G 41.619M 344.856G | original 224x224 33.379G, 48.287M, 267.030G \makecell[c]{ANN}
        VIBE~\cite{kocabas2019vibe} & \multirow{2}{*}{ANN} & \makecell[c]{V} & \makecell[c]{48.3M} & \makecell[c]{43.4G} & \makecell[c]{-} & \makecell[c]{73.1} & \makecell[c]{50.9} & \makecell[c]{344.9G} & \makecell[c]{-} & \makecell[c]{75.4} & \makecell[c]{53.6}\\
        %  my 256x256 45.6G, 39.6M, 348.3G | original 224x224 35.6G, 39.6M, 284.8G
        MPS~\cite{wei2022capturing} & & \makecell[c]{V} & \makecell[c]{39.6M} & \makecell[c]{45.6G} & \makecell[c]{-} & \makecell[c]{68.0} & \makecell[c]{48.2} & \makecell[c]{348.3G} & \makecell[c]{-} & \makecell[c]{69.2} & \makecell[c]{50.1}\\
    \hline
        % 1.77G * 8 * 10 iters
        EventCap(VIBE)~\cite{xu2020eventcap} & \multirow{5}{*}{ANN} & \makecell[c]{V+E} & \makecell[c]{48.3M} & \makecell[c]{185.0G} & \makecell[c]{-} & \makecell[c]{71.9} & \makecell[c]{50.4} & \makecell[c]{1477.7G} & \makecell[c]{-} & \makecell[c]{74.1} & \makecell[c]{52.9}\\
        %
        EventCap(MPS)~\cite{xu2020eventcap} & & \makecell[c]{V+E} & \makecell[c]{39.6M} & \makecell[c]{187.2G} & \makecell[c]{-} & \makecell[c]{66.6} & \makecell[c]{47.8} & \makecell[c]{1481.1G} & \makecell[c]{-} & \makecell[c]{68.1} & \makecell[c]{49.5}\\
        %
        EventHPE(VIBE)~\cite{zou2021eventhpe} & & \makecell[c]{G+E} & \makecell[c]{49.0M} & \makecell[c]{49.0G} & \makecell[c]{-} & \makecell[c]{69.6} & \makecell[c]{48.9} & \makecell[c]{354.0G} & \makecell[c]{-} & \makecell[c]{71.6} & \makecell[c]{50.2}\\
        %
        EventHPE(MPS)~\cite{zou2021eventhpe} & & \makecell[c]{G+E} & \makecell[c]{39.6M} & \makecell[c]{49.3G} & \makecell[c]{-} & \makecell[c]{65.1} & \makecell[c]{46.5} & \makecell[c]{354.2G} & \makecell[c]{-} & \makecell[c]{66.8} & \makecell[c]{48.1}\\
        %
        $^{\dagger}$EventHPE(GT)~\cite{zou2021eventhpe} & & \makecell[c]{G+E} & \makecell[c]{46.9M} & \makecell[c]{-} & \makecell[c]{71.8} & \makecell[c]{55.0} & \makecell[c]{43.9} & \makecell[c]{-} & \makecell[c]{74.5} & \makecell[c]{58.1} & \makecell[c]{45.3}\\
    \hline
        % CNN+GRU 43.602G 46.865M 348.622G
        ResNet-GRU~\cite{kocabas2019vibe} & \multirow{3}{*}{ANN}  & \makecell[c]{E} & \makecell[c]{46.9M} & \makecell[c]{43.6G} & \makecell[c]{111.2} & \makecell[c]{60.0} & \makecell[c]{45.3} & \makecell[c]{348.6G} & \makecell[c]{115.0} & \makecell[c]{64.2} & \makecell[c]{49.5}\\
        % detr 50.489G 41.315M, 403.819G 41.358M
        ResNet-TF~\cite{carion2020end} & & \makecell[c]{E} & \makecell[c]{41.3M} & \makecell[c]{50.5G} & \makecell[c]{108.5} & \makecell[c]{59.9} & \makecell[c]{\underline{44.1}} & \makecell[c]{403.8G} & \makecell[c]{114.2} & \makecell[c]{66.0} & \makecell[c]{50.1}\\
        % swin transformer 44.727G 48.930M, 359.569G 48.930M
        Video-Swin~\cite{liu2021swin} & & \makecell[c]{E} & \makecell[c]{48.9M} & \makecell[c]{44.7G} & \makecell[c]{124.1} & \makecell[c]{66.5} & \makecell[c]{49.0} & \makecell[c]{359.6G} & \makecell[c]{130.9} & \makecell[c]{72.5} & \makecell[c]{53.1}\\
    \hline
        % 
        SEW-ResNet-TF & \makecell[c]{Mix} & \makecell[c]{E} & \makecell[c]{47.0M} & \makecell[c]{24.5G} & \makecell[c]{110.8} & \makecell[c]{58.9} & \makecell[c]{44.2} & \makecell[c]{199.7G} & \makecell[c]{113.2} & \makecell[c]{65.3} & \makecell[c]{49.3}\\
    \hline
        %
        ANN2SNN~\cite{rueckauer2017conversion} & \multirow{4}{*}{SNN} & \makecell[c]{E} & \makecell[c]{46.9M} & \makecell[c]{12.5G} & \makecell[c]{140.3} & \makecell[c]{74.1} & \makecell[c]{55.8} & \makecell[c]{98.8G} & \makecell[c]{148.2} & \makecell[c]{81.1} & \makecell[c]{60.9}\\
        SEW-ResNet~\cite{fang2021deep} & & \makecell[c]{E} & \makecell[c]{\underline{25.8M}} & \makecell[c]{9.1G} & \makecell[c]{116.8} & \makecell[c]{62.5} & \makecell[c]{48.3} & \makecell[c]{56.7G} & \makecell[c]{122.8} & \makecell[c]{68.3} & \makecell[c]{52.3}\\
        %
        MA-SNN~\cite{yao2023attention} & & \makecell[c]{E} & \makecell[c]{30.2M} & \makecell[c]{\underline{7.5G}} & \makecell[c]{115.2} & \makecell[c]{61.6} & \makecell[c]{47.6} & \makecell[c]{\underline{55.3G}} & \makecell[c]{120.1} & \makecell[c]{66.8} & \makecell[c]{48.9}\\
        %
        SpikeFormer~\cite{zhou2022spikformer} & & \makecell[c]{E} & \makecell[c]{36.8M} & \makecell[c]{13.2G} & \makecell[c]{112.5} & \makecell[c]{60.2} & \makecell[c]{46.8} & \makecell[c]{96.3G} & \makecell[c]{118.1} & \makecell[c]{66.1} & \makecell[c]{48.4}\\
    \hline
        % layers=3: 44.690M, layers=2: 38.4M
        Ours & \makecell[c]{SNN} & \makecell[c]{E} & \makecell[c]{47.7M} & \makecell[c]{9.4G} & \makecell[c]{\underline{107.1}} & \makecell[c]{\underline{58.4}} & \makecell[c]{\underline{44.1}} & \makecell[c]{63.4G} & \makecell[c]{\underline{111.8}} & \makecell[c]{\underline{60.7}} & \makecell[c]{\underline{45.6}}\\
    \hline \toprule 
    \end{tabular}}
    \label{tab:pose-estimation}
\end{table*}

In this section, we begin by describing the implementation details for training and explaining the evaluation metrics we report. We then compare our method with recent video-based and event-based human pose tracking approaches in Sec.~\ref{sec:compare-prior-works}, highlighting  the competency of event signals alone for pose tracking. We further compare our SNNs solution with three popular ANN solutions in Sec.~\ref{sec:compare-anns}, showcasing the efficiency and effectiveness of SNNs for event-based human pose tracking. Additionally, we compare our model with five recently proposed SNN-based models in Sec.~\ref{sec:compare-snn}, demonstrating the superiority of our proposed spiking spatiotemporal transformer for bi-directional temporal information fusion. We present the results of models trained on synthetic data and tested on real data in Sec.~\ref{sec:compare-train-synthetic}, illustrating the effectiveness of our large-scale synthetic events data for human pose tracking. Finally, we present ablation studies in Sec.~\ref{sec:ablation} to demonstrate the significance of individual components in our proposed model, especially the spiking spatiotemporal transformer.


\textbf{Implementation Details.} For fair comparison with prior works~\cite{xu2020eventcap,zou2021eventhpe}, we follow~\cite{zou2021eventhpe} to split train and test set of MMHPSD dataset, where subject 1, 2 and 7 are used for test and the other 12 subjects for training. We present the results of model trained with $T=8$ time steps and $T=64$ time steps. For event stream preprocessing, we convert each event packet to the voxel grid of size $256\times 256\times 4$, where $C=4$ is empirically found the best choice as higher values do not show noticeable improvement of performance in the ablation study. For fair comparison with other baselines on the number of parameters, we use SEW-ResNet50~\cite{fang2021deep} as the backbone and set spiking spatiotemporal transformer to be 1024 hidden dimension, 1 attention head and 2 layers, which contains 38.4M parameters. During training, to be robust to both fast and slow motions, we augment the training samples in two ways: randomly selecting event stream of (0.5, 1, 2, 3) seconds for $T=8$ and (4, 8, 16, 32) seconds for $T=64$ as the input, rotating the voxel grid with a random degree between -20 and 20. We use parametric LIF neuron with soft reset and retain the backpropagation of reset path in SNNs, where SpikingJelly~\cite{SpikingJelly} is used to implement the model. The loss weights $\lambda_{\text{pose}}$, $\lambda_{\text{shape}}$, $\lambda_{\text{trans}}$, $\lambda_{\text{3D}}$ and $\lambda_{\text{2D}}$ are set to be 10, 1, 50, 1 and 10. We train the two models for 20 and 25 epochs respectively with batch size being 8. The learning rate starts from 0.01 and is scheduled by CosineAnnealingLR with maximum number of epochs being 21 and 26. Models are trained on a single NVIDIA A100 GPU. For testing, 1 and 8 seconds event streams are used for $T=8$ and $T=64$ models. 

\textbf{Evaluation metrics.} Similar to previous works~\cite{kocabas2019vibe,zou2021eventhpe}, we report three different metrics, mean per joint position error (MPJPE), pelvis-aligned MPJPE (PEL-MPJPE) and Procrustes-Aligned MPJPE (PA-MPJPE). PA-MPJPE compares predicted and target pose after rotation and translation alignment, while PEL-MPJPE compares after only translation alignment of two pelvis joints.


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/vis_pose_tracking.pdf}
    \caption{\textbf{Qualitative results} of MPS~\cite{wei2022capturing}, EventCap(MPS)~\cite{xu2020eventcap}, EventHPE(MPS)~\cite{zou2021eventhpe} and ours. Two side views are shown in dashed boxes. The two prior works, EventCap and EventHPE, tend to suffer from the inaccurate initial poses or starting pose given by pre-trained video based method, MPS, and thus produce sub-optimal pose tracking outcomes when compared to our approach.}
    \label{fig:vis-pose}
\end{figure*}

\subsubsection{Comparison with Prior Works} \label{sec:compare-prior-works}
% \textbf{SNNs v.s. SOTA} - highlight events only \textbf{Implementation of SOTA Baselines.}
We compare our method with four prior works to highlight the competency of event signals alone for human pose tracking: VIBE~\cite{kocabas2019vibe}, MPS~\cite{wei2022capturing}, EventCap~\cite{xu2020eventcap}, and EventHPE~\cite{zou2021eventhpe}. In Tab.~\ref{tab:pose-estimation}, we use \textit{V}, \textit{G} and \textit{E} to represent the input data of gray-scale video, first gray-scale frame and events respectively. \textit{VIBE} and \textit{MPS} are applied as the most recent video-based baselines with ResNet50 as the backbone. Note that both methods use weak camera model without global translation, so we will not report their MPJPE. To extract initial poses from the gray-scale video as required by EventCap, we make use of pre-trained VIBE and MPS methods for the extraction, labeled as \textit{EventCap(VIBE)} and \textit{EventCap(MPS)}. Since the authors have not published their code, we re-implement EventCap using PyTorch LBFGS optimizer and PyTorch3D differential render, following~\cite{zou2021eventhpe}. Besides, EventCap is an iterative optimization approach, which typically requires much more FLOPs than end-to-end methods as is indicated in Tab.~\ref{tab:pose-estimation}. For EventHPE, we also use VIBE and MPS for the starting pose extraction, denoted as \textit{EventHPE(VIBE)} and \textit{EventHPE(MPS)}. We also report the results of EventHPE with ground-truth starting pose known, denoted as \textit{EventHPE(GT)}. This method is considered the upper bound as it is assumed to have perfect information of the starting pose in the first frame, without any errors induced by VIBE or MPS. 

As shown in Tab.~\ref{tab:pose-estimation}, the most recent MPS outperforms VIBE by approximately 9mm of PEL-MPJPE and 5mm of PA-MPJPE for both T=8 and T=64. This trend is also evident when comparing EventCap(MPS) with EventCap(VIBE) or EventHPE(MPS) with EventHPE(VIBE), indicating that the two prior works~\cite{xu2020eventcap,zou2021eventhpe} are significantly affected by the initial poses provided by the pre-trained video-based methods, as they fall into the local minimum and thus improve the initial states by up to 3mm of PEL-MPJPE and 2mm of PA-MPJPE. In constrast, our end-to-end approach circumvents the local minimum by directly using event streams, which are better to capture fast motion dynamics. Consequently, our SNN-based model achieves the best performance with the smallest gap to the upper bound EventHPE(GT) while using only about 6\% of the optimization-based EventCap and 20\% of the other three end-to-end methods. This is further illustrated in Fig.~\ref{fig:vis-pose} where the inaccurate initial poses or starting pose given by MPS~\cite{wei2022capturing}, and thus present sub-optimal pose tracking outcomes when compared to our approach.

\subsubsection{Comparison with ANN models}\label{sec:compare-anns}
To further illustrate the advantages of SNNs over ANNs in event-based human pose tracking, we compare our model with three popular ANN-based architecture: ResNet with GRU used in~\cite{kocabas2019vibe,zou2021eventhpe} (\textit{ResNet-GRU}), ResNet with original Transformer~\cite{vaswani2017attention} used in DETR~\cite{carion2020end} (\textit{ResNet-TF}) and video Swin Transformer proposed in~\cite{liu2021swin} (\textit{Video-Swin}). For fair comparisons, we select the architecture with about 45M parameters for all three models. Further details are presented in the supplementary material. The settings for training these ANN models mostly follow those of our approach, except the learning rate, which starts from 0.0001 and is scheduled by StepLR with a 0.1 decay after 15 and 20 epochs for both T=8 and T=64, respectively, as ANN models do not converge well using a higher learning rate such as 0.001. 

% highlight (1) SNNs better temporal enconding and efficiency
For the models of $T=8$ in Tab.~\ref{tab:pose-estimation}, our approach achieves slightly lower pose errors than ResNet-GRU and Video-Swin, while presenting competitive performance with ResNet-TF, which achieves 44.1mm of PA-MPJPE as well. In the case of $T=64$, where longer temporal dependencies are required to perception, the drop in performance for the three ANN-based models is noticeably larger than our SNN-based model, with up to a 6mm v.s. 1.5mm drop in PA-MPJPE. Furthermore, our model requires only 9.4G and 63.4G FLOPs for $T=8$ and $T=64$, respectively, which is less than 20\% of the FLOPs required by the three ANN-based models. These results demonstrate the superiority of our SNN-based approach in efficiently encoding long-term temporal dependencies within event streams, largely due to the entirely different working mechanism of spiking neurons from conventional artificial neurons. 

\subsubsection{Comparison with SNN models} \label{sec:compare-snn}
% highlight spiking spatiotemporal transformer \textbf{SNNs v.s. SNNs + Spiking Transformer} - our technical contribution
We compare our approach in Tab.~\ref{tab:pose-estimation} with five recently proposed SNN-based models to highlight the superiority of our proposed spiking spatiotemporal transformer for human pose tracking. \textit{SEW-ResNet-TF} acts as a baseline of mixed architecture that uses SEW-ResNet as the SNN-based backbone followed with ANN-based Transformer for pose tracking regression, with the same settings of model architecture. \textit{ANN2SNN} means the conversion of ANN model of ResNet-GRU to SNN model using~\cite{rueckauer2017conversion}. \textit{SEW-ResNet}~\cite{fang2021deep} is the backbone used in our approach without spiking spatiotemporal transformer. \textit{MA-SNN}~\cite{yao2023attention} means multi-dimensional attention SNNs where SEW-ResNet50 is used for fair comparison. \textit{SpikeFormer}~\cite{zhou2022spikformer} denotes SNN-based vision transformer (ViT)~\cite{dosovitskiy2020image} where dot-product is directly adopted in the spiking self-attention.

Compared with the mixed model SEW-ResNet-TF, our approach presents slightly lower pose errors, while requiring less than a half of FLOPs. As for completely SNN-based approaches, though ANN2SNN has shown its excellent performance in the image classification task, it fails in the regression task of pose tracking by producing much higher pose errors than other directly trained SNNs, mainly due to the quantization errors introduced during the conversion process. When compared with SEW-ResNet, our approach gives much lower pose errors, 48.3 v.s. 44.1mm of PA-MPJPE for $T=8$, and the gap of performance becomes larger when looking at $T=64$, 52.3 v.s. 45.6mm, demonstrating the significance of bi-directional temporal information provided by our proposed spiking spatiotemporal transformer. As for MA-SNN, although it requires the less number of FLOPs than ours because of its lower spiking rate of 16.4\% v.s. ours of 22.6\%, its performance is still worse than ours. Additionally, our approach shows moderately lower pose errors and lower FLOPs than SpikeFormer, which attributes to our proposed Hamming similarity in the spiking attention module, rather than ill-posed dot-product between spike tensors. 

\subsubsection{Comparison with Models Trained on Synthetic Data} \label{sec:compare-train-synthetic}
Although our proposed synthetic dataset SynEventHPD provides a large variety of poses shown in Fig.~\ref{fig:poses-tsne}, the possible domain gap between synthetic and real data remains an open question. Therefore, in Tab.~\ref{tab:pose-estimation-syn}, we compare our model, trained on real MMHPSD dataset, with three models that are trained on our synthetic dataset. All the models are tested on the real MMHPSD test set. It can be found that Ours-Syn achieves 58.4 of PEL-MPJPE and 45.0 of PA-MPJPE, marginally higher errors than Ours, largely due to the domain gap between the synthetic and real events. This trend is also observed when comparing the quantitative results of ResNet-GRU and SEW-ResNet-TF in Tab.~\ref{tab:pose-estimation} and~\ref{tab:pose-estimation-syn}. Further, we showcase the qualitative results in Fig.~\ref{fig:web_demo} to illustrate the generalization ability of our model trained on new scenarios, where the left two examples are the predictions on synthetic events from webcam videos and the right example is the test results on real data. Though trained on synthetic SynEventHPD dataset, our model still demonstrates its ability to work in practice.



\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/web_demo_fig.pdf}
    \caption{\textbf{Generalization ability of our model trained on our synthetic SynEventHPD.} The left two examples are the predicted poses from the events synthesized from webcam videos. The right one example is the results of our model trained on synthetic data and tested on real events.}
    \label{fig:web_demo}
\end{figure}


\begin{table}[hptb]
    \centering
    \caption{\textbf{Quantitative results of human pose tracking with models trained with our synthetic dataset and tested on real MMHPSD test set.} \textit{Syn-E} and \textit{Real-E} denotes model trained with synthetic and real events data.}
    \setlength{\tabcolsep}{0.5mm}
    \renewcommand{\arraystretch}{1.3}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{@{}|c|c|c|ccc|@{}}
    \bottomrule \hline
        \multirow{2}{*}{Model} & \multirow{2}{*}{\makecell[c]{ANN/\\SNN}} & \multirow{2}{*}{\makecell[c]{Train\\Data}} &  \multicolumn{3}{c|}{T=8 (1 sec)}\\
        \cline{4-6}
        & & & \makecell[c]{MPJPE $\downarrow$} & \makecell[c]{PEL-MPJPE $\downarrow$} & \makecell[c]{PA-MPJPE $\downarrow$}\\
    \hline
        %
        ResNet-GRU & \makecell[c]{ANN} & \makecell[c]{Syn-E} & \makecell[c]{115.6} & \makecell[c]{61.2} & \makecell[c]{46.5} \\
        %
        SEW-ResNet-TF & \makecell[c]{Mix} & \makecell[c]{Syn-E} & \makecell[c]{114.1} & \makecell[c]{60.6} & \makecell[c]{45.5} \\
        %
        Ours-Syn & \makecell[c]{SNN} & \makecell[c]{Syn-E} & \makecell[c]{110.7} & \makecell[c]{59.3} & \makecell[c]{45.0} \\
    \hline
        %
        Ours & \makecell[c]{SNN} & \makecell[c]{Real-E} & \makecell[c]{107.1} & \makecell[c]{58.4} & \makecell[c]{44.1} \\
    \hline \toprule 
    \end{tabular}}
    \label{tab:pose-estimation-syn}
\end{table}


\subsubsection{Ablation Study}\label{sec:ablation}
\begin{figure}[ptb]
    \centering
    \includegraphics[width=\columnwidth
    ]{figures/ablation_study.pdf}
    \caption{\textbf{Ablation studies.} \textbf{(a) Score Functions} shows the results of four similarity functions used in the spiking spatiotemporal transformer. \textbf{(b) Channel of Input Voxel Grid} presents the effects of different channel sizes on the performance of pose tracking. \textbf{(c) \# of Attention Layers} investigates the performance with different number of attention layers in the spiking spatiotemporal transformer.}
    \label{fig:ablation}
\end{figure}

In this section, we conduct ablation studies to evaluate several components in our model. The comparison of quantitative results is displayed in the corresponding sub-figure in Fig.~\ref{fig:ablation}. 

\textbf{1. Score function in spiking spatiotemporal transformer}: We compare our proposed Hamming similarity between two spike vectors with the scaled dot-production similarity, Euclidean similarity and Manhattan similarity, which are defined as follows:
\begin{equation}\footnotesize
    \begin{aligned}
        \text{Hamming similarity}\quad & 1 - \frac{1}{C_k}\sum_{c=1}^{C_k} \mathds{1}(\mathbf{s}_{ic}^q \neq \mathbf{s}_{jc}^k), \nonumber\\
        \text{Scaled dot-product similarity}\quad & \frac{1}{\sqrt{C_k}}\sum_{c=1}^{C_k} \mathbf{s}_{ic}^q\cdot\mathbf{s}_{jc}^k, \nonumber\\
        \text{Euclidean similarity}\quad & 1 - \frac{1}{C_k}\sum_{c=1}^{C_k}(\mathbf{s}_{ic}^q - \mathbf{s}_{jc}^k)^2, \nonumber\\
        \text{Manhattan similarity}\quad & 1 - \frac{1}{C_k}\sum_{c=1}^{C_k}|\mathbf{s}_{ic}^q - \mathbf{s}_{jc}^k|. \nonumber
    \end{aligned}
\end{equation}
From the quantitative results of PEL-MPJPE shown in Fig.~\ref{fig:ablation} (a), our approach exceeds the other three commonly used similarity functions by a large margin (over 5mm), demonstrating the effectiveness of Hamming similarity as the score function for spike vectors.

\textbf{2. Channel $C$ of input voxel grid}: We compare the channel size of 1, 2, 4, 6 and 8 as the input of our model in terms of PEL-MPJPE in Fig.~\ref{fig:ablation} (b), which shows that the case of $\scriptstyle C=4$ gives lower joint errors than that of 1, 2, while almost the same errors with the case of 6 and 8. Hence, $\scriptstyle C=4$ is empirically the appropriate choice of the channel size of the input voxel grid.

\textbf{3. \# of attention layers}: We compare our model with 0, 1, 2, 4 and 6 layers in spiking spatiotemporal transformer in Fig.~\ref{fig:ablation} (c). The improvement of PEL-MPJPE is noticeable when using 1 or 2 layers of attention layers in our spiking transformer. This improvement is minimal for 4 and 6 layers, but with a dramatic increase of parameters from 47.7M to 87.4M. 



\subsection{Discussion}
\textbf{Attention scores maps} are shown in Fig.~\ref{fig:attention-map}. For better visualization, we transform the attention score matrix from $THW\times THW$ to $T\times T$, where the weights of spatial positions at each time step are summed together. The two examples show illustrate that our attention mechanism allows the query at $t=1$ to focus predominantly on features originating from subsequent time steps, thereby providing a more accurate and efficient prediction of body part positions even when they are obscured. The success of these examples can be attributed to its ability to globally adapt to the temporal dependencies present in the input event stream. By emphasizing the relevant features from temporal context, our method can effectively compensate for the lack of information due to occlusion in the initial stages. This results in more accurate and robust pose tracking through time from events only.

\begin{figure}[tpb]
    \centering
    \includegraphics[width=\columnwidth]{figures/attention_map.pdf}
    \caption{\textbf{Visualization of attention score maps.} Our approach determines accurate poses for obscured body parts at an early stage using spiking spatiotemporal attention, in which the query at $t=1$ places significantly greater emphasis on features from later time steps.}
    \label{fig:attention-map}
\end{figure}

\textbf{Failure cases} are displayed in Fig.~\ref{fig:failure-case}, where the pose are not accurately estimated from the events. These cases were primarily attributed to the presence of body part occlusion and the absence of temporal context. The impact of occlusion can be significant, as it hinders the model's ability to detect and analyze essential features required for pose estimation. Moreover, unlike the examples in Fig.~\ref{fig:attention-map}, the lack of temporal context further compounds this issue, as the model cannot effectively leverage information from previous or subsequent frames to compensate for missing or obscured data. Recognizing and addressing these failure cases is crucial for improving the robustness and reliability of our event-based pose tracking method. 

\textbf{Future research} could concentrate on several promising avenues to improve the robustness and accuracy of event-based pose tracking. One such direction involves incorporating multi-view event streams, which can significantly enhance the model's ability to handle occlusions, as it can rely on the additional visual information available from different viewpoints to compensate for obscured or missing features. In company with the multi-view setting, the development of SNN-based approach for multi-view spike feature synthesis is another direction. By designing and optimizing SNN-based algorithms for multi-view event-based pose tracking, researchers can further enhance the performance of pose estimation models while maintaining low computational requirements and making them well-suited for real-time applications.


\begin{figure}[tpb]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/failure_case.pdf}
    \caption{\textbf{Failure cases.} Owing to the presence of body part occlusion and also the absence of temporal context, our method struggles to accurately estimate poses, as indicated by the red circles.}
    \label{fig:failure-case}
\end{figure}

\section{Conclusion}
We present in this paper a dedicated end-to-end SNN-based approach for event-based pose tracking where events are the only source of data without any requirement of frame-based images. Our approach is based entirely upon SNNs and our proposed spiking spatiotemporal transformer demonstrates its effectiveness for bidirectional temporal feature compensation. Additionally, a large-scale synthetic dataset is constructed that features a broad and diverse set of annotated 3D human motions, as well as longer hours of event stream data. Empirical experiments demonstrate the superiority of our approach in both performance and efficiency measures. With comparable performance to the state-of-the-art ANNs counterparts, our approach achieves a computation reduction of 20\% in FLOPS. 

\appendices




% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{Shihao Zou}
% Biography text here.
% \end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

% \begin{IEEEbiography}
% 	[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{biofigures/shihao.jpg}}]
% 	{Shihao Zou} received the B.Sc. degree from Beijing Institute of Technology, China, in 2017, and the M.Res. degree from University College London, UK, in 2018. He is currently a Ph.D. candidate at University of Alberta. His interests include computer vision and machine learning, especially human pose and shape estimation, motion capture system. 
% \end{IEEEbiography}

% \begin{IEEEbiography}
% 	[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{biofigures/muyuxuan.jpg}}]
% 	{Yuxuan Mu} received the B.Sc. degree from Beijing Institute of Technology, China, in 2021. He is currently a master student at University of Alberta. His research interests include computer vision and graphics, especially on human modeling and physics-based motion modeling.
% \end{IEEEbiography}

% \begin{IEEEbiography}
% 	[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{biofigures/xinxin.jpg}}]
% 	{Xinxin Zuo} received the M.E. degree from Northwestern Polytechnical University and Ph.D. degree from the University of Kentucky. She is currently a Postdoctoral Fellow at University of Alberta. Her interests include computer vision and graphics, especially on 3D reconstruction and human modeling.
% \end{IEEEbiography}

% \begin{IEEEbiography}
% 	[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{biofigures/sen.jpg}}]
% 	{Sen Wang} received the B.E. degree and Ph.D. degree from Northwestern Polytechnical University. From 2015 to 2016, he was a Visiting Ph.D. Student at the University of Kentucky. He is currently a Postdoctoral Fellow at University of Alberta. His research interests include computer vision and robotics. 
% \end{IEEEbiography}

% \begin{IEEEbiography}
% 	[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{biofigures/li.jpg}}]{Li Cheng} received the Ph.D. degree in computer science from the University of Alberta, Canada. He is a professor with the Department of Electrical and Computer Engineering, University of Alberta. Prior to coming back to University of Alberta, He has worked at A*STAR, Singapore, TTI-Chicago, USA, and NICTA, Australia. He is a senior member of the IEEE. His research expertise is mainly on computer vision and machine learning.
% \end{IEEEbiography}

% \vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


