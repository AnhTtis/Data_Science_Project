\section{Introduction}
\label{sec:intro}

Prompting is a technique used to guide the output generation of a pre-trained language model such as GPT-3 \cite{OpenAI-GPT3-2020}. This is achieved by providing input in the form of a question or template, which helps to generate specific responses such as Q\&A, document summarization, and translations. The advent of ChatGPT \cite{ChatGPTvsHuman, chatgpt, wolf2019transfertransfo} has revolutionized the field of NLP by demonstrating the potential of using large pre-trained language models with prompting. Despite this progress, there is still room for improvement in current prompting strategies and techniques, especially for specific target applications. In this study, we investigate the Socratic method \cite{PaltoRepublicURL,SocraticMethidWiki} to identify and evaluate potential prompting strategies, and use the findings to design effective prompt templates.

Traditional NLP tasks involve various sub-tasks, such as named entity recognition, dependency parsing, coreference resolution \cite{dobrovolskii-2021-word}, semantic parsing \cite{pasupat-liang-2015-compositional,dong-lapata-2018-coarse}, and more, to comprehend the meaning of a sentence. By utilizing prompt templates with large language models (LLMs), these sub-tasks can be delegated to the LLM, freeing the template to focus specifically on dialogue design. In this regard, the Socratic method \cite{PaltoRepublic} holds significant relevance, as it is well-known for using questioning (prompting) as a means of promoting critical thinking and delving into complex concepts \cite{Elder2010}.

The Socratic method has a long history of being regarded as the basis of critical thinking. However, some recent studies have cast doubt on its effectiveness in practice. In his paper ``Socratic Irony and Argumentation,'' Airaksinen \cite{Irony2022} criticizes the method for its rigidly defined roles of teacher and student, which can lead to fear of not meeting the teacher's expectations and reluctance to participate. Similarly, Stoddard's ``The Use of Socratic Questioning in Clinical Teaching'' \cite{PimpClinical2016} highlights the risk of the method being misused in a manner that lacks psychological safety for students. Fortunately, when using the Socratic method in a dialogue with an LLM, the absence of emotions and sarcasm, as well as the option to deactivate the model, can alleviate many of the problems associated with human interaction.

This study starts by presenting an overview of the Socratic method's strategies and techniques. To begin, we list ten widely referenced methods \cite{AskRightQ2001} under the Socratic method umbrella and use hypothesis elimination to identify the most relevant ones for our goal of prompt-template development. The selected methods are definition, hypothesis elimination, elenchus, dialectic, maieutics, generalization, and induction. Furthermore, we add to the list counterfactual reasoning, which is a concept in logic that involves considering what might have happened if a particular event had occurred differently. We then perform experiments using GPT-3 to test and evaluate these methods, and offer suggestions for incorporating these strategies and techniques into prompt templates.

\begin{comment}
\begin{figure}[h!]
\vspace{-.1in}
\begin{center}    \centerline{\includegraphics[width=8.8cm,height=5.8cm]{IMG/SocraricQuestiiningThreeTypes.png}}
\vspace{-.1in}
\center{\caption{Socratic Questioning Types. (Figure credit: \cite{Paul2007CriticalTT}.)}}
\label{fig:Q-Types}
\end{center}
\end{figure}
\vspace{-.2in}
\end{comment}

In ``Critical Thinking: The Art of Socratic Questioning,'' Paul and Elder categorize Socratic questioning into three general types: spontaneous, exploratory, and focused \cite{Paul2007CriticalTT}. We will not discuss spontaneous questioning, which is similar to casual conversation. Focused questioning, on the other hand, aims to acquire knowledge and truth, and the {\em definition} method, {\em elenchus} (cross-examination), {\em hypothesis elimination}, {\em dialectic}, and {\em generalization} hold great potential for shaping effective prompting strategies and enhancing the response accuracy of an LLM.
In addition, the use of exploratory thinking through the {\em maieutics} (midwife) method, {\em indcuction}, and {\em counterfactual reasoning} can steer GPT-3 towards producing creative and imaginative writing. Although most of the plot suggestions generated by GPT-3's exploration may not be useful, a few unique recommendations in response to a ``what if'' query can stimulate the writer's imagination and lead to remarkable results. When applied effectively, these methods can turn an LLM into a writer's muse, providing inspiration and guiding the creative process \cite{MuseMasses2010}.

The main contributions of this paper are as follows:
\begin{noindlist}
\item An overview of the strategies of the Socratic method, evaluation of these strategies, and selection of the most relevant ones for the development of effective prompt templates.
\item An examination of how the definition, elenchus, hypothesis elimination, dialectic, and generalization methods can enhance the output's accuracy and conciseness through clarification and verification.
\item An illustration of how maieutics, induction, and counterfactual reasoning can foster productive generalization and creativity.
\end{noindlist}

The remainder of this paper is structured into five sections. Section~\ref{sec:related} provides a review of related work on prompting methods in NLP. In Section~\ref{sec:socratic}, we introduce the ten strategies and methods taught by Socrates and used in Plato's "Dialogues." From these, we select relevant methods along with counterfactual reasoning as our focus for developing prompting templates. Section~\ref{sec:template} details how we engineer these methods into our templates to improve output correctness and stimulate creative writing. Section~\ref{sec:pilot} presents a
pilot study. Finally, in Section~\ref{sec:conc}, 
we present our concluding remarks.

\section{Related Work}
\label{sec:related}

The use of transformer architecture \cite{Vaswani2017AttentionIA} and masked data for pre-training large language models (LLMs) in an unsupervised setting has become {\em the approach} in natural language processing \cite{Devlin2019BERTPO,Lewis2019BARTDS}. The method involves pre-training an LLM on a large text corpus, followed by fine-tuning for specific tasks.

Prompting is a recent innovation in the field, popularized by OpenAI, especially with the release of GPT-3 in 2020. Instead of fine-tuning the model for a specific task, the approach involves providing a specific input, or ``prompt,'' to guide the LLM's output generation, resulting in greater flexibility and efficiency in generating a wide range of responses.

However, designing effective prompt templates remains a challenge, as it requires a deep understanding of the interplay between the LLM and the prompt. According to the survey paper \cite{SocraticModels-Google2022}, there are several factors that impact prompt template engineering, including the type of LLM used, manual vs automatic design, and static vs continuous prompts.

\begin{noindlist}
\item {Left-to-right vs masked LLMs}. For tasks related to generation or tasks solved using a standard left-to-right language model \cite{OpenAI-GPT3-2020}, prefix prompts tend to perform better, as they align with the model's left-to-right nature. For tasks solved using masked language models \cite{Devlin2019BERTPO}, cloze prompts are more suitable, as they closely match the pre-training task form.
\item {Manual vs automatic design}. A prompt template should be tailored to the specific LLM. While manual design may be suitable in the initial flow-design phase, dependencies between the input and expected output, and their variations, should be mined automatically \cite{WhatLMKnowsJiang2020}. Automation can also help in paraphrasing the seed prompt to support various mined dependency patterns, but mistakes can occur \cite{PTRSUN2021}.
\item {Discrete vs continuous prompts}. Discrete prompts involve providing a fixed set of pre-determined input choices to an LLM. Continuous prompts, on the other hand, involve a dialogue or conversation between the model and the user, allowing for a more dynamic and interactive experience. 
\end{noindlist}

More advanced templates can be constructed by combining basic templates with techniques like ensemble methods \cite{Schick2020ExploitingCF}. This involves forming a committee of basic templates that ask the same question using different phrasing \cite{Haviv2021BERTeseLT}.
Most current prompt templates generate short outputs, such as class labels, or outputs with a length that can be predicted based on the task and input, like in the case of translation. However, for tasks that may generate longer or open-ended outputs, additional considerations may be necessary during the template engineering process. 

One approach for generating longer outputs is explanation-based prompting, as proposed by the chain-of-thought method \cite{wei2022chain}. This method generates a sequence of explanations before inferring the answer. However, when dealing with simple math problems, this approach has an error rate of $47\%$. To address the inconsistency issues of explanation-based prompting, \cite{Jung2022MaieuticPL} formulates the problem as a satisfiability problem, which defers inference until a tree of explanations has been expanded abductively (explaining both truth and false branches) and recursively. However, abductive reasoning is often considered weak, incoherent, and even nonexistent \cite{Abductive4Flaws2011}. To improve consistency, a recent work \cite{wang2023selfconsistency} extends the chain-of-thought approach by adding a diverse set of reasoning paths and performing majority voting among them. This method can be viewed as an ensemble method, but it does not alter the nature of abductive reasoning.

In contrast, the Socratic method aims to employ deductive, inductive, and abductive reasoning to ensure consistency and accuracy of inference. The Socratic method deals with all aspects of critical thinking, including definition clarification and cross-examination. This comprehensive approach to template engineering can lead to improved output quality and consistency.

%Recently, Kevin Yang and his team presented a human-in-the-loop method to generate long stories using GPT-3, by prompting it repeatedly and editing the output manually \cite{Yang2022Re3GL}. This approach can generate more coherent stories, as the writer is continuously involved in the revision process. The challenge is to see if LLMs can generate plots that are ingeniously beyond the writer's capabilities through prompting.

The primary objective of this study is to design continuous prompts that enhance response quality and foster guided creativity in generative tasks, such as verifying information, evaluating source credibility, proposing alternatives, recommending plot ideas in creative writing, and generating task-specific surprises. Our approach involves investigating strategies and methods within the Socratic method, and selecting the most relevant ones for further exploration.

As discussed in Section~\ref{sec:intro}, Socratic questioning can be classified into three categories: spontaneous, exploratory, and focused \cite{Paul2007CriticalTT}. When designing a prompt, it is important to consider the category and utilize the most suitable strategies and techniques to achieve the best results. 