
@article{PromptSurvey2023,
author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
title = {Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website including constantly updated survey and paperlist.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {195},
numpages = {35},
keywords = {Pre-trained language models, prompting}
}

@article{Peng2022ModelEI,
  title={Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning},
  author={Xiangyu Peng and Chen Xing and Prafulla Kumar Choubey and Chien-Sheng Wu and Caiming Xiong},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.12587}
}

@inproceedings{dong-lapata-2018-coarse,
    title = "Coarse-to-Fine Decoding for Neural Semantic Parsing",
    author = "Dong, Li  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    pages = "731--742",
    abstract = "Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.",
}

@inproceedings{pasupat-liang-2015-compositional,
    title = "Compositional Semantic Parsing on Semi-Structured Tables",
    author = "Pasupat, Panupong  and
      Liang, Percy",
    booktitle = "Proceedings of the 53rd Annual Meeting of ACL and the 7th International Joint Conference on NLP",
    year = "2015",
    publisher = "Association for Computational Linguistics",
    pages = "1470--1480",
}

@misc{SocraticModels-Google2022,
  author = {Zeng, Andy and et al},
  title = {Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
  publisher = {arXiv},
  year = {2022},
}

@misc{SocraticMethidWiki,
  author = {Wikipedia},
  title = {Socratic method},
  publisher = {Wikipedia, The Free Encyclopedia},
  url = {https://en.wikipedia.org/wiki/Socratic_method},
  year={2023},
}


@Book{Cross-Examination2021,
  author =       "Larry Pozner and Roger J. Dodd",
  title =        "Cross-Examination: Science and Techniques",
  publisher =    "LexisNexis",
  year =         "2021",
  address =      "",
  edition =      "3rd.",
}


@Book{PaulBinkerCT1990,
  author =       "Richard Paul and A. J. A. Binker",
  title =        "Critical Thinking: What Every Person Needs to Survive in a Rapidly Changing World",
  publisher =    "Sonoma State University",
  year =         "1990",
  address =      "Center for Critical Thinking and Moral Critique",
  edition =      "",
}

@misc{501Q2004,
  author = {{LLC Editors, LearningExpress}},
  title = {{501 Critical Reading Questions}},
  publisher = {LearningExpress},
  city = {New York},
  year = {2004},
}

@article{CRITExtended2023,
  title={{CRIT}: An Inquisitive Prompt Template for Critical Reading (extended version)},
  author={Edward Y. Chang},
  journal={Stanford University Technical Report},
  year={2023},
  month = {February},
  url = {infolab.stanford.edu/~echang/CRIT-Ext.pdf},
}

@misc{WHO2021,
author = "{World Health Organization}",
title = {Vaccine efficacy, effectiveness and protection},
year = {2021},
publisher = {WHO Home Page}, 
url = {https://www.who.int/news-room/}
}


@article{Paul2007CriticalTT,
  title={Critical Thinking: The Art of Socratic Questioning, Part III.},
  author={Richard W. Paul and Linda Elder},
  journal={Journal of Developmental Education},
  year={2007},
  volume={31},
  pages={34-35}
}

@Book{WinArgument2006,
  author =       "Madsen Pirie",
  title =        "How to Win Every Argument",
  publisher =    "Continuum",
  year =         "2006",
  address =      "",
  edition =      "",
}

@Book{Elder2010,
  author =       "Linda Elder and Richard Paul",
  title =        "The Thinker's Guide to the Art of Asking Essential Questions",
  publisher =    "Rowman \& Litterfield",
  year =         "2010",
  address =      "",
  edition =      "5th.",
}

@misc{chatgpt,
author = {OpenAI},
title = {{ChatGPT}},
year = {2021},
publisher = {OpenAI},
url = {https:// openai.com/ blog/ chatgpt/}
}

@article{FuncTheoryCounterfactual2008,
author = {Epstude, Kai and Neal J Roese},
title = {The functional theory of counterfactual thinking},
year = {2008},
volume = {12},
number = {2},
pages = {168-192},
journal = {Personality and social psychology review}
}

@article{NormTheory1986,
author = {Kahneman, D. and Miller, D. T.},
title = {Norm theory: Comparing reality to its alternatives},
year = {1986},
volume = {93},
number = {2},
pages = {136-153},
journal = {Psychological Review},
}


@misc{AskRightQ2001,
author = {Browne, M. N. and Keeley, S.},
title = {Asking the Right Questions, A Guide to Critical Thinking},
year = {2001},
publisher = {Prentice Hall}, 
}


@misc{PaltoRepublic,
author = {Plato},
title = {The Republic},
year = {380 BC},
publisher = {Penguin Classics}, 
}

@misc{PaltoRepublicURL,
author = {Chase B. Wrenn},
title = {Internet Encyclopedia of Philosophy},
year = {2023},
publisher = {Naturalistic Epistemology}, 
url = {https://iep.utm.edu/republic/}
}

@article{Irony2022,
author = {Airaksinen, T.},
year = {2012},
pages = {85-100},
title = {Socratic Irony and Argumentation},
volume = {36},
journal = {Argumentation}
}

@article{PimpClinical2016,
author = {Stoddard, H. A. and O'Dell, D. V.},
year = {2016},
pages = {1092-96},
title = {Would Socrates Have Actually Used the "Socratic Method" for Clinical Teaching?},
volume = {31},
number = {9},
journal = {Journal of general internal medicine}
}

@article{NLPScratch2011,
author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
year = {2011},
month = {02},
pages = {2493-2537},
title = {Natural Language Processing (Almost) from Scratch},
volume = {12},
journal = {Journal of Machine Learning Research}
}

@inproceedings{campagna-etal-2022-shot,
    title = "A Few-Shot Semantic Parser for {W}izard-of-{O}z Dialogues with the Precise {T}hing{T}alk Representation",
    author = "Campagna, Giovanni  and
      Semnani, Sina  and
      Kearns, Ryan  and
      Koba Sato, Lucas Jun  and
      Xu, Silei  and
      Lam, Monica",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    pages = "4021--4034"
    }

@Book{NLP-Text-JM3,
  author =       "Daniel Jurafsky and James H. Martin",
  title =        "Speech and Language Processing
An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition",
  publisher =    "(Draft)",
  month = "January",
  year =         "2023",
  address =      "",
  edition =      "3rd.",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  note =         "",
}

@misc{Counterfactual2022,
  doi = {10.48550/ARXIV.2206.13757},
  url = {https://arxiv.org/abs/2206.13757},
  author = {Fryer, Zee and Axelrod, Vera and Packer, Ben and Beutel, Alex and Chen, Jilin and Webster, Kellie},
  title = {Flexible text generation for counterfactual fairness probing},
  publisher = {arXiv},
  year = {2022},
}


@misc{OpenAI-GPT3-2020,
  doi = {10.48550/ARXIV.2005.14165},
  url = {https://arxiv.org/abs/2005.14165},
  author = {Brown, Tom B. and et al},
  title = {Language Models are Few-Shot Learners},
  publisher = {arXiv},
  year = {2020},
}

@misc{wolf2019transfertransfo,
  author = {Wolf, Thomas and Sanh, Victor and Chaumond, Julien and Delangue, Clement},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents},
  publisher = {arXiv},
  year = {2019},
}

@inproceedings{Schick2021FewShotTG,
  title={Few-Shot Text Generation with Natural Language Instructions},
  author={Timo Schick and Hinrich Sch{\"u}tze},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2021}
}


@inproceedings{Schick2020ExploitingCF,
  title={Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference},
  author={Timo Schick and Hinrich Sch{\"u}tze},
  booktitle={Conference of the European Chapter of the Association for Computational Linguistics},
  year={2020}
}

@article{Haviv2021BERTeseLT,
  title={BERTese: Learning to Speak to BERT},
  author={Adi Haviv and Jonathan Berant and Amir Globerson},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.05327}
}

@inproceedings{dobrovolskii-2021-word,
    title = "Word-Level Coreference Resolution",
    author = "Dobrovolskii, Vladimir",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    pages = "7670--7675",
}

@article{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  journal={ArXiv},
  year={2017},
  volume={abs/1706.03762}
}

@misc{PTRSUN2021,
  url = {https://arxiv.org/abs/2105.11259},
  author = {Han, Xu and Zhao, Weilin and Ding, Ning and Liu, Zhiyuan and Sun, Maosong},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {PTR: Prompt Tuning with Rules for Text Classification},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.04805}
}

@inproceedings{Lewis2019BARTDS,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Veselin Stoyanov and Luke Zettlemoyer},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}

@misc{LMorNB2019,
  doi = {10.48550/ARXIV.1909.01066},
  url = {https://arxiv.org/abs/1909.01066},
  author = {Petroni, Fabio and Rocktäschel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Language Models as Knowledge Bases?},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{WhatLMKnowsJiang2020,
    author = {Jiang, Zhengbao and Xu, Frank F. and Araki, Jun and Neubig, Graham},
    title = "{How Can We Know What Language Models Know?}",
    journal = {Trans. of the Association for Computational Linguistics},
    pages = {423-438},
    year = {2020},
    month = {07},
}

@article{DialoguewithAttention2023,
author = {Zhang, Weinan and Cui, Yiming and Zhang, Kaiyan and Wang, Yifa and Zhu, Qingfu and Li, Lingzhi and Liu, Ting},
title = {A Static and Dynamic Attention Framework for Multi-Turn Dialogue Generation},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
abstract = {Recently, research on open domain dialogue systems have attracted extensive interests of academic and industrial researchers. The goal of an open domain dialogue system is to imitate humans in conversations. Previous works on single turn conversation generation have greatly promoted the research of open domain dialogue systems. However, understanding multiple single turn conversations is not equal to the understanding of multi turn dialogue due to the coherent and context dependent properties of human dialogue. Therefore, in open domain multi turn dialogue generation, it is essential to modeling the contextual semantics of the dialogue history rather than only according to the last utterance. Previous research had verified the effectiveness of the hierarchical recurrent encoder-decoder framework on open domain multi turn dialogue generation. However, using an RNN-based model to hierarchically encoding the utterances to obtain the representation of dialogue history still face the problem of a vanishing gradient. To address this issue, in this article, we proposed a static and dynamic attention-based approach to model the dialogue history and then generate open domain multi turn dialogue responses. Experimental results on the Ubuntu and Opensubtitles datasets verify the effectiveness of the proposed static and dynamic attention-based approach on automatic and human evaluation metrics in various experimental settings. Meanwhile, we also empirically verify the performance of combining the static and dynamic attentions on open domain multi turn dialogue generation.},
journal = {ACM Trans. Inf. Syst.},
month = {Jan.},
articleno = {15},
numpages = {30},
keywords = {Open domain dialogue systems, attentive neural network, dialogue generation, multi turn dialogue}
}

@inproceedings{Campagna2020AFS,
  title={A Few-Shot Semantic Parser for Wizard-of-Oz Dialogues with the Precise ThingTalk Representation},
  author={Giovanni Campagna and Sina J. Semnani and Ryan Kearns and Lucas Jun Koba Sato and Silei Xu and Monica S. Lam},
  booktitle={Findings},
  year={2020}
}

@inproceedings{zhou-etal-2021-structure,
    title = "Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based {AMR} Parsing",
    author = "Zhou, Jiawei  and
      Naseem, Tahira  and et al",
    booktitle = "Proc. Conf. on Empirical Methods in NLP",
    year = "2021",
    pages = "6279--90",
}

@article{MuseMasses2010,
author = {Thrash, Todd and Maruskin, Laura and Cassidy, Scott and Fryer, James and Ryan, Richard},
year = {2010},
month = {03},
pages = {469-87},
title = {Mediating Between the Muse and the Masses: Inspiration and the Actualization of Creative Ideas},
volume = {98},
journal = {Journal of personality and social psychology},
}

@misc{ChatGPTvsHuman,
  url = {https://arxiv.org/abs/2301.07597},
  author = {Guo, Biyang and Zhang, Xin and Wang, Ziyuan and Jiang, Minqi and Nie, Jinran and Ding, Yuxuan and Yue, Jianwei and Wu, Yupeng},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {How Close is {ChatGPT} to Human Experts? Comparison Corpus, Evaluation, and Detection},
  publisher = {arXiv},
  year = {2023},
}

@Book{DeweyDemocracy1916,
  author =       "John Dewey",
  title =        "Democracy and Education: An Introduction to the Philosophy of Education",
  publisher =    "Project Gutenberg (1997)",
  year =         "1916",
  address =      "",
  edition =      "",
}

@Book{DeweyHowThink1910,
  author =       "John Dewey",
  title =        "How We Think",
  publisher =    "Project Gutenberg (2011)",
  year =         "1910",
  address =      "",
  edition =      "",
}

@misc{CounterfactualStories2019,
  url = {https://arxiv.org/abs/1909.04076},
  author = {Qin, Lianhui and Bosselut, Antoine and Holtzman, Ari and Bhagavatula, Chandra and Clark, Elizabeth and Choi, Yejin},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Counterfactual Story Reasoning and Generation},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Yang2022Re3GL,
  title={Re3: Generating Longer Stories With Recursive Reprompting and Revision},
  author={Kevin Yang and Nanyun Peng and Yuandong Tian and Dan Klein},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.06774}
}

@book{Pearl2009,
  title={Counterfactuals and Causal Inference: Methods and Principles for Social Research},
  author={Pearl, Judea},
  year={2009},
  publisher={Cambridge University Press}
}

@inproceedings{lai-etal-2017-race,
    title = "{RACE}: Large-scale {R}e{A}ding Comprehension Dataset From Examinations",
    author = "Lai, Guokun  and
      Xie, Qizhe  and
      Liu, Hanxiao  and
      Yang, Yiming  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1082",
    doi = "10.18653/v1/D17-1082",
    pages = "785--794",
}



@inproceedings{
wang2023selfconsistency,
title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc V Le and Ed H. Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=1PL1NIMMrw}
}

@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@inproceedings{Jung2022MaieuticPL,
  title={Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations},
  author={Jaehun Jung and Lianhui Qin and Sean Welleck and Faeze Brahman and Chandra Bhagavatula and Ronan Le Bras and Yejin Choi},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2022}
}

@article{Abductive4Flaws2011,
author = {Plutynski, Anya},
year = {2011},
month = {09},
pages = {227-248},
title = {Four Problems of Abduction: A Brief History},
volume = {1},
journal = {HOPOS: The Journal of the International Society for the History of Philosophy of Science},
doi = {10.1086/660746}
}