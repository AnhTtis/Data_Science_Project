\section{Method}
\label{sec:preliminary}

In this work, we investigate the end-to-end trainable joint optimization of varying intrinsic  and extrinsic camera parameters as well as \acs{nvs}. 
The extrinsic camera parameters are learned by employing Gaussian Fourier feature mapping. For the intrinsic camera parameters, we apply dynamic parameter learning for each given camera. This allows \approach to learn independent intrinsic camera parameters for each camera. As depicted in Fig.~\ref{fig:overall_approach}, the output of extrinsic and intrinsic camera parameter estimation is then used for the \ac{nerf} training and jointly optimized with the \ac{nerf}. 

\subsection{Camera Parameters}
Following the definition of the pinhole camera model~\cite{hartley_multiple_2004}, $\Pi=K\left[R|t\right]$, the extrinsic camera parameters $\left[R|t\right]$ are used to convert from the world coordinate system to the camera coordinate system. The intrinsic camera parameters $K$ convert the points from the camera coordinate system into the image coordinate system. These parameters are the field of view, the focal length $f_x, f_y$, the principal point, skew and the geometric distortion. In summary, the camera parameters $\Pi$ influence which part of the scene around the camera is later part of the captured image. In turn, for \ac{nerf}, this defines the projection of rays and is essential for training  \ac{nerf}.

\subsubsection{Intrinsic Camera Parameters}
The number of possible images that are used for  \ac{nerf} is known a priori. For this, we construct a lightweight parameter array. The length of the array equals the number of varying cameras to optimize each image differentiated by their given camera towards their focal length. Thus, the images with varying height and width are independent from each other and independent $f = (f_x$, $f_y)$ values can be learned. Following the assumption of Wang et al.~\cite{wang_nerf_2021}, we consider the camera's principle points $c_x \approx W/2$ and $c_y \approx H/2$, where $H,~W$ denote the image height and width respectively. Thus, initialization is done individually for each intrinsic. The focal length parametrization, see equation~\ref{eq:focallength}, is a learned scaling factor $s_i$, which is initialized with $1.0$ for each camera $i$. Optimizing the square root of $s$, leads to better results in \ac{nerf}$\text{-}\text{-}$, which is why we adopted this 2nd-order trick. 
\begin{equation}
    \label{eq:focallength}
     \begin{split}
    f_{x_i} = s_{i}^{2}W,~
    f_{y_i} = s_{i}^{2}H
    \end{split}
\end{equation}



\begin{figure*}[t!]
  \centering
   \includegraphics[width=0.9\textwidth]{Zeichnung-2.png}
  \caption{\textbf{Architecture of \approach, our end-to-end trainable approach.} We utilize Gaussian Fourier features (GF) in our pose \acs{mlp} to learn the extrinsic camera parameters and learn individual intrinsic camera parameters per given camera. We further stabilize the convergence of the pose \ac{mlp} with a \acs{ssim} loss function. Our framework is jointly optimized using the  \acs{ssim} loss ($L_{\text{SSIM}}$) and photometric loss ($L_{pix}$).\label{fig:overall_approach}}
\end{figure*}

\subsubsection{Gaussian Fourier Feature Mapping for Extrinsic Camera Parameters}

The extrinsic camera parameters, rotation $R \in SO(3)$ and translation $t \in \mathbb{R}^3$, are represented by a camera-to-world transformation matrix $\left[R|t\right]$ in $SE(3)$. We use  Gaussian Fourier feature mapping in a \ac{mlp} to learn these parameters. By utilizing different Fourier feature mappings, \acp{mlp} can learn high-frequency features from a low-dimensional input $\mathbf{v}$. Tancik et al.~\cite{tancik_fourier_2020} compared positional encoding given in equation~\ref{eq:posenc} and Gaussian Fourier feature mapping given in equation~\ref{eq:fourier}.

\begin{equation}\label{eq:posenc}
\gamma (\mathbf{v}) = \left[\text{cos}(2\pi\sigma^{j/m}\mathbf{v}), \text{sin}(2\pi\sigma^{j/m}\mathbf{v}) \right]^T\text{,} j=0,\text{...},m-1
\end{equation}

For both mappings, the scale $\sigma$ is determined through a hyperparameter search. While the positional encoding is deterministic, every value in matrix $\mathbf{B}$ for the Gaussian mapping is sampled from $\mathit{N}(0, \sigma^2)$.
Due to this sampling of random features, the bias towards axis aligned data, as in positional encoding, is avoided. Tancik et al. applied this only the image input to boost the performance of~\ac{nerf}.  %, but only applied to the image input

\begin{equation}\label{eq:fourier}
\gamma (\mathbf{v}) = \left [ \text{cos}(2\pi\mathbf{Bv}), \text{sin}(2\pi\mathbf{Bv}) \right ]^T, \mathbf{B} \in \mathbb{R}^{m\times d}
\end{equation}

We instead apply this for our extrinsic camera parameter estimation. The Gaussian Fourier feature mapping is used to map the index of each camera to a higher-dimensional space. As frequency parameter $m$ for the matrix $B$, we use 128 which results in an embedding size of 256. Our lightweight \ac{mlp} consists of three layers with a hidden size of $64$ and GELU activation functions. The embedding scale $\sigma$ is set to 10. 

\subsection{Neural Radiance Fields}

Using the camera parameters $\Pi$,  \ac{nerf} enable the generation of novel views. A scene in NeRF~\cite{mildenhall_nerf_2020} is represented as 5D vector function $f$ consisting of the 3D location $\textbf{x} = (x, y, z)$ and 2D viewing direction $\textbf{d} = (\theta, \phi)$ as input. A \ac{nerf} maps the 3D location $\textbf{x}$ and viewing direction $\textbf{d}$ to a radiance color $\textbf{c} = (r, g, b)$ and volume density $\sigma$, namely $f : \mathbb{R}^5\rightarrow\mathbb{R}^4$. The 5D coordinates are sampled along camera rays and the output of the \ac{mlp} is used in classical volumetric rendering techniques. 


This differential volumetric rendering allows a fully optimizable pipeline for obtaining the pixel values based on the input coordinates. To render images from \ac{nerf}, the color from each pixel $\mathbf{p} = (u,v)$ on the image plane $\hat{I}_i$ is obtained by rendering the function $\mathcal{R}$, considering known camera parameters $\Pi$~\cite{mildenhall_nerf_2020,wang_nerf_2021}, see equation~\ref{eq:int}. 

\begin{equation}
\label{eq:int}
    \hat{I}_i (p) = \mathcal{R} (\mathbf{p},\pi_i|\Theta)) = \int_{h_n}^{h_f} T(\textit{h})\sigma(\mathbf{r}(h))\mathbf{c}(r(h),\mathbf{d})\diff h,
\end{equation}

The near and far bounds are denoted as $h_n$ (near) and $h_f$ (far)~\cite{wang_nerf_2021}. $\pi_i$ denotes the camera parameters and $T(h) = \text{exp}(-\int_{h_n}^{h_f}) \sigma(\mathbf{r}(s)ds)$ describes the accumulated transmission factor along the ray. To optimize the radiance field, \ac{nerf} minimize the mean squared error between rendered color and ground truth color also called photometric loss. In summary, the general \ac{nerf} framework can be formulated as $\Theta^{*} = \text{arg min} ~\mathcal{L} (\hat{I}|I, \Pi)$.

Wang et al.~\cite{wang_nerf_2021} adapt this framework to jointly optimize the \ac{nerf} as well as the intrinsic and extrinsic camera parameters, with the focus on forward-facing scenes. To achieve the joint optimization, Wang et al. reformulate the \ac{nerf} framework as denoted in equation~\ref{eq:nerfmm}. However, this framework is restricted to one single camera.

\begin{equation}
\label{eq:nerfmm}
    \Theta^{*}, \Pi^{*} = \text{arg min}~\mathcal{L} (I, PI | I)
\end{equation}

Our approach investigates the end-to-end trainable joint optimization of differing intrinsic and extrinsic camera parameters and \ac{nerf}. \approach learns varying intrinsic and extrinsic camera parameters along with the scene representation, see Fig.~\ref{fig:overall_approach}. As input, a set of RGB images $I$, potentially captured by varying cameras are used. We especially investigate intrinsic and extrinsic parameter optimization. Therefore, we adapt the \ac{nerf} framework, see equation~\ref{eq:approach}. Our framework considers differing intrinsic camera parameters $K^{*}_{\textit{cam}}$ and utilizes Gaussian Fourier features to predict the pose $\left[R|t\right]^{*}$ of each camera.

\begin{equation}
\label{eq:approach}
    \Theta^{*}, K^{*}_{\textit{cam}}, \left[R|t\right]^{*} = \text{arg min}~\mathcal{L} (\hat{I} \hat{K}_{\textit{cam}}, [\hat{R}| \hat{t}]| I)
\end{equation}

Consequently, we jointly optimize the intrinsic $K^{*}_{\textit{cam}}$ and extrinsic $\left[R|t\right]^{*}$ camera parameters along with the \ac{nerf}, while also allowing that images can be taken by different cameras ($\textit{cam}$). Our approach is depicted in Fig.~\ref{fig:overall_approach}, which shows the individual steps. 













