%%%%%%%%% BODY TEXT
\newpage\null\thispagestyle{empty}
\begin{minipage}[t]{\textwidth}
\centering
\textbf{\huge{Supplementary Material}}
\end{minipage}

\newpage\null\thispagestyle{empty}\newpage\null\thispagestyle{empty}

\section{Camera Parameter Experiments}
\approach aims to improve the extrinsic camera parameter estimation using Gaussian Fourier features and to predict differing intrinsic camera parameters if they are given in a dataset. We first show a deeper analysis of extrinsic camera parameter estimation. Secondly, we show additional results for the camera intrinsic parameter estimation. We further analyze novel view synthesis. Lastly, we provide details about the focal length and example images of our \datasetname dataset.


\subsection{Extrinsic Camera Parameters}

For the extrinsic camera parameters we conducted additional experiments. In these we compare positional encoding and Gaussian Fourier features as well as different embedding sizes for our pose \ac{mlp}. Furthermore, we analyze mirror poses, investigate the COLMAP initialization, the influence of the \ac{ssim} loss and conduct another breaking point analysis.

\begin{table}[t!]
    \centering

   \begin{center}
    \resizebox{\columnwidth}{!}{ 
    \begin{tabular}{l|cc|cc} \hline \hline
       \multirow{2}{*}{Scene} & \multicolumn{2}{c|}{Pos. Enc.} & \multicolumn{2}{c}{Gaussian F.F.} \\
        & Rot. Err. & Trans. Err. & Rot. Err. & Trans. Err. \\ \hline 
       Fern     & 36.18 & 0.035 & \textbf{23.28} & \textbf{0.026}  \\
       Flower   & 39.64 & \textbf{0.025} & \textbf{5.36} & 0.027  \\
       Fortress & 38.05 & \textbf{0.033} & \textbf{27.47} & 0.047  \\
       Horns    & 46.51 & \textbf{0.051} & \textbf{4.90} & \textbf{0.051}  \\
       Leaves   & 42.55 & \textbf{0.020} & \textbf{4.03} & 0.022 \\
       Orchids  & \textbf{22.48} & \textbf{0.025} & 23.73 & \textbf{0.025}  \\
       Room     & 64.49 & 0.086 & \textbf{41.03} & \textbf{0.069}  \\
       T-Rex    & \textbf{77.77} & 0.065 & 85.50 & \textbf{0.052}\\ \hline
       Mean     & 45.95 & 0.042 & \textbf{26.91} & \textbf{0.039}  \\ \hline \hline
    \end{tabular}
    }
   \end{center}
    \caption{\textbf{Quantitative comparison between positional encoding and Gaussian Fourier features on \acs{llff}.} The results are computed after 500 epochs. The high error values are the result of ten random seeds for each scene.}
    \label{tab:pos_encVSgaussian}
\end{table}


\begin{table}[t!]
    \centering
   \begin{center}
    \resizebox{\columnwidth}{!}{ 
    \begin{tabular}{l|cc|cc|cc} \hline \hline
       \multirow{2}{*}{Scene} & \multicolumn{2}{c|}{\acs{psnr}} & \multicolumn{2}{c|}{Rot. Err.} & \multicolumn{2}{c}{Trans. Err.} \\
        & 64 & 256  & 64 & 256  & 64 & 256 \\ \hline 
       Fern     & 21.63  & 22.75 & 1.59 & 0.94 & 0.009 & 0.004\\
       Flower   & 24.66 & 25.95 & 3.98 & 1.59  & 0.023 & 0.017 \\
       Fortress & 27.24 & 28.39 & 1.30 & 1.32 & 0.014 & 0.015\\
       Horns    & 22.46 & 25.20 & 1.05 & 1.45 & 0.014  & 0.005\\
       Leaves   & 17.24 & 19.00 & 4.26 & 4.03 & 0.012 & 0.016\\
       Orchids  & 17.39 & 16.87 & 2.86 & 4.12 & 0.005 & 0.024 \\
       Room     & 27.04 & 27.54 & 0.97  & 1.15 & 0.009 & 0.036\\
       T-Rex    & 23.04 & 24.25 & 3.36 & 3.85 & 0.012 & 0.009 \\ \hline
       Mean     & 22.59 & \textbf{23.74}  & 2.42 & \textbf{2.31}  & \textbf{0.012}  & 0.016\\ \hline \hline
    \end{tabular}
    }
   \end{center}
    \caption{\textbf{Quantitative comparison between two different embedding sizes for the pose \acs{mlp}, i.e. 64 and 256 on \acs{llff}.} The embedding size of 256 leads to an higher \acs{psnr} and lower rotation error.}
    \label{tab:64vs256embeddingsize}
\end{table}
\begin{figure}[t!]
\centering
    \subfloat{\includegraphics[width=\columnwidth]{pos64_allinone.pdf}}\hskip1ex
    \subfloat{\includegraphics[width=\columnwidth]{pos256_allinone.pdf}}
    \caption{\textbf{Rotation error and translation error for the first 100 epochs between two different embedding sizes i.e. 64 (top) and 256 (bottom). } The embedding size 256 shows less outliers compared to an embedding size of 64.}
\label{fig:embeddingsizePlot}
\end{figure}

\subsubsection{Positional Encoding}
Our pose \ac{mlp} uses Gaussian Fourier features while another approach is the use of positional encoding~\cite{lin_barf_2021,truong_sparf_2022}. Therefore, we test positional encoding in our pose \ac{mlp}. We train our pose \ac{mlp} on each scene of the \ac{llff} dataset~\cite{mildenhall_local_2019}. For training we use ten random seeds.  In Table~\ref{tab:pos_encVSgaussian} we compare the results of Gaussian Fourier features and positional encoding after 500 epochs. Gaussian Fourier features averagely outperform positional encoding in mean rotation and translation error over the ten runs. The high mean errors occur due to mirror poses in the ten runs. We also include  the experiments with a mirror pose in our calculation to show that mirror poses are less likely to occur when using Gaussian Fourier features.




\subsubsection{Embedding Size}
In addition to testing positional encoding, we investigate the influence of different embedding sizes for the pose \ac{mlp}, see Table~\ref{tab:64vs256embeddingsize}. We apply Gaussian Fourier feature mapping and use the embedding sizes 64 and 256 respectively. As visualized in Fig.~\ref{fig:embeddingsizePlot} the embedding size of 64 shows more outliers for the rotation convergence, for example in the fern and orchids scene, and for the translation convergence in the room scene. Based on the results showing more stability of 256, we select the embedding size of 256 for our pose \ac{mlp}. 



\begin{table*}
    \centering
       \resizebox{\textwidth}{!}{ 
    \begin{tabular}{|c|P{40pt}P{40pt}|P{40pt}P{40pt}|P{40pt}P{40pt}|P{40pt}P{40pt}|P{40pt}P{40pt}|} \hline
    Skip/Scene & \multicolumn{2}{c|}{Fireplug} &  \multicolumn{2}{c|}{Stormtrooper}  &  \multicolumn{2}{c|}{T1}  & \multicolumn{2}{c|}{Bike} & \multicolumn{2}{c|}{Brick House} \\ \hline
     & \scriptsize{\acs{nerf}$\text{-}\text{-}$}  & \scriptsize{OURS}  & \scriptsize{\acs{nerf}$\text{-}\text{-}$}  & \scriptsize{OURS} & \scriptsize{\acs{nerf}$\text{-}\text{-}$}  & \scriptsize{OURS} & \scriptsize{\acs{nerf}$\text{-}\text{-}$}  & \scriptsize{OURS} & \scriptsize{\acs{nerf}$\text{-}\text{-}$}  & \scriptsize{OURS} \\ 
    2 & \green & \green & \green & \green  & \green & \green & \red & \green & \green & \green  \\
    3 & \green & \green & \green & \green  & \green & \green & \red & \red & \green & \green  \\
    4 & \green & \green & \green & \red & \red & \green & \red & \green & \green & \green  \\
    5 & \green & \red & \green & \red  & \red & \green & \red & \red & \green & \green  \\ \hline

    \end{tabular}
    }
    \caption{\textbf{Breaking point analysis of our Gaussian Fourier feature-based Pose \acs{mlp} vs. \acs{nerf}$\text{-}\text{-}$ on \datasetname.} We used every second, third, fourth or fifth image during training. A rotation error below  $20^{\circ}$ is considered as success (green). We slightly outperform \acs{nerf}$\text{-}\text{-}$ as we succeed on 15 scenes while \acs{nerf}$\text{-}\text{-}$ only succeeds on 14 scenes.\label{tab:breaking_ana_iff}}
\end{table*}

\subsubsection{Mirror Poses}

Wang et al.~\cite{wang_nerf_2021} mentioned the occurrence of mirror poses when using COLMAP. In their experiments on the \ac{bleff} dataset, COLMAP shows an error close to $180^{\circ}$ for the roundtable scene. For the parameter-based or \ac{mlp}-based pose estimation similar errors can appear. This can occur due to an ambiguous pose estimation caused by several local minima in the loss function~\cite{schweighofer_robust_2006,oberkampf_iterative_1996}.

Nevertheless, the \ac{nerf} does not fail to learn and generate accurate novel views. Despite high image quality, a clear shift of the objects in the scene is visible. Especially for the roundtable scene, low rotation errors lead to a better result in the rotation comparison. Still, the visual novel view quality is poor despite high \ac{ssim} and \ac{psnr} values. An example of this can be seen in Fig.~\ref{fig:roundtable_error}. The picture in the center of Fig.~\ref{fig:roundtable_error} shows a mirror pose that occurred during training with a train \ac{psnr} of $47.13$ and an average rotation error above $150^{\circ}$. However, the generated image is of high quality compared to the image on the right. There, the scene has a comparatively low average rotation error of below $30^{\circ}$, but only a train \ac{psnr} of $39.69$. Although the rotation error in the right image is $<30^{\circ}$, the \ac{psnr} value for the middle figure is much higher. Nevertheless, the outline of the ground truth scene shows a highly incorrect camera pose in the center image. Thus, both metric types, \ac{nvs} quality metrics and \ac{ate}, are decisive for joint optimization approaches.

\begin{figure}[t!]
\centering
    \subfloat{\includegraphics[width=\columnwidth]{no_ssim_allinone.pdf}}
    
    \subfloat{\includegraphics[width=\columnwidth]{ssim_allinone.pdf}}
    \caption{\textbf{Comparison of ATE statistics on all \acs{llff} scenes.} We show the comparison without \acs{ssim} loss (top) and with \acs{ssim} loss (bottom) for 500 epochs.}
\label{fig:ssimcomp}
\end{figure}
\begin{figure}[t!]
  \centering
    \includegraphics[width=\columnwidth]{roundtable.pdf}
  \caption{\textbf{Comparison of the same scene of \ac{bleff} and same frame at different rotation error levels.} We highlight the original shift from the ground truth scene (left) with a blue outline around the table. In the pink rectangles we show the same cutout from the same coordinates in the image. 
  % The actual ground truth scene (left) and rotation error near 180 degrees and a \ac{psnr} value of $47.13$ (middle) against a novel view with error at 28 degrees with a \ac{psnr} value of $39.69$ (right). 
  }
  \label{fig:roundtable_error}
\end{figure}




\begin{table*}[t!]
    \centering

   \begin{center}
    \resizebox{\textwidth}{!}{ 
    \begin{tabular}{l|cccc|cccc|cccc} \hline \hline
       \multirow{2}{*}{Scene} & \multicolumn{4}{c|}{Rot. Err.} & \multicolumn{4}{c|}{Trans. Err.} & \multicolumn{4}{c}{Focal. Err.} \\
        & \acs{nerf}$\text{-}\text{-}$& SiNeRF & OURS & OURS$^\text{(COLMAP)}$ & \acs{nerf}$\text{-}\text{-}$& SiNeRF & OURS & OURS$^\text{(COLMAP)}$ & \acs{nerf}$\text{-}\text{-}$& SiNeRF & OURS & OURS$^\text{(COLMAP)}$\\ \hline 
       Fern     &1.78 & 1.17 & 1.34 & \textbf{0.41} & 0.029  &  0.006 & 0.006 & \textbf{0.001} & 153.5 & 112.7 & 161.6 & \textbf{1.76}\\
       Flower   & 4.84 & 1.38 & 0.89 & \textbf{0.39} & 0.016 & 0.007 & 0.007 & \textbf{0.002} & \textbf{13.2} & 80.4 & 53.6 & \textbf{18.63}\\
       Fortress & 1.36 & 2.02 & 0.91 & \textbf{0.15} & 0.025 & 0.048 & 0.006 & \textbf{0.002} & 144.1 & 59.7 & 129.3 & \textbf{1.05} \\
       Horns    & 5.55 & 83.34 & 1.89 & \textbf{0.41} & 0.044 & 0.133 & 0.014 & \textbf{0.002} & 156.2 &282.5 & 92.5 & \textbf{25.8} \\
       Leaves   & 3.90 & 14.46 & \textbf{2.65} & 5.04 & 0.016 & 0.10 & 0.005 & \textbf{0.004} & 59.0 & 18.8 &  24.4 & \textbf{10.3} \\
       Orchids  & 4.96 & 3.97 & 2.75 &\textbf{0.31} & 0.051 & 0.014 & 0.009 & \textbf{0.004} & 199.3 & 155.7 & 165.9 & \textbf{11.96} \\
       Room     & 2.77 & 4.92 & 1.33 & \textbf{0.14} & 0.030 & 0.022 & 0.006 & \textbf{0.001} & 331.8 &313.6 & \textbf{102.7} & \textbf{6.06} \\
       T-Rex    & 4.67 & 7.19 & \textbf{3.90} & \textbf{0.14} & 0.036 & 0.027 & 0.008 & \textbf{0.002} & 89.3 & 38.3 & 100.4 & \textbf{7.54} \\ \hline
       Mean     & 3.73 & 14.81 & \textbf{2.10} & \textbf{0.83} & 0.031 & 0.033 & 0.008 & \textbf{0.002} & 143.3 &  155.2 & 103.7 & \textbf{10.4} \\ \hline \hline
    \end{tabular}
    }
   \end{center}
    \caption{\textbf{Quantitative comparison between \approach (OURS),  \approach initialized with COLMAP (OURS$^{COLMAP}$), \acs{nerf}$\text{-}\text{-}$~\cite{wang_nerf_2021} and SiNeRF~\cite{xia_sinerf_2022} on \acs{llff}.} For SiNeRF we retrained the approach with a layer dimension of 128 to ensure comparability with our approach and \ac{nerf}$\text{-}\text{-}$. We report the camera extrinsic parameter errors, in detail translation and rotation error. We tested ten random seeds for SiNeRF; for \ac{nerf}$\text{-}\text{-}$ we report the values from Wang et. al~\cite{wang_nerf_2021}.}
    \label{tab:extrinsic_llff_colmap}
\end{table*}


\begin{table*}[t]
   \begin{center}
       \resizebox{\textwidth}{!}{ 
    \begin{tabular}{l|cccc|cccc|cccc} \hline \hline
       \multirow{2}{*}{Scene} & \multicolumn{4}{c|}{PSNR$\uparrow$} & \multicolumn{4}{c|}{SSIM$\uparrow$} & \multicolumn{4}{c}{LPIPS$\downarrow$} \\  % \cmidrule(r){2.5-4.5}
        & \ac{nerf} & \ac{nerf}$\text{-}\text{-}$ & OURS & OURS$^\text{(COLMAP)}$ & \ac{nerf} & \ac{nerf}$\text{-}\text{-}$ &  OURS & OURS$^\text{(COLMAP)}$ &  \ac{nerf} & \ac{nerf}$\text{-}\text{-}$ & OURS & OURS$^\text{(COLMAP)}$\\ \hline
       Fern     & \textbf{22.22} & 21.67 & 21.82 & 22.19 & \textbf{0.64} & 0.61 & 0.62 & \textbf{0.64} & 0.47 & 0.50 & 0.49 & 0.41 \\
       Flower   & 25.25 & 25.34 & 25.39 & \textbf{25.78} & 0.71 & 0.71 & 0.72 & \textbf{0.73} & 0.36 & 0.37 & 0.37 & \textbf{0.35}\\
       Fortress & 27.60 & 26.20 & 27.28 & \textbf{28.13} & 0.73 & 0.63 & 0.70 & \textbf{0.74} & 0.38 & 0.49 & 0.41 & \textbf{0.37} \\
       Horns   & 24.25 & 22.53 & 24.00 & \textbf{24.45} & 0.68 & 0.61 & 0.67 & \textbf{0.68} & \textbf{0.44} & 0.50 & 0.45 & \textbf{0.44} \\
       Leaves   & 18.81 & 18.88 & \textbf{18.97} & 16.65 & 0.52 & \textbf{0.53} & 0.50 & 0.43 & \textbf{0.47} & \textbf{0.47} & 0.49 & 0.53\\
       Orchids  & 19.09 & 16.73 & 17.41 & \textbf{19.30} & 0.51 & 0.39 & 0.43 & \textbf{0.52} & \textbf{0.46} & 0.55 & 0.52 & \textbf{0.46} \\
       Room     & \textbf{27.77} & 25.84 & 27.12 & 27.72 & \textbf{0.87} & 0.84 & 0.82 & \textbf{0.87} & 0.40 & 0.44 & 0.43 & \textbf{0.41} \\
       T-Rex    & 23.19 & 22.67 & 22.92 & \textbf{23.20} & \textbf{0.74} & 0.72 & \textbf{0.74} & \textbf{0.74} & \textbf{0.41} & 0.44 & 0.43 & \textbf{0.41} \\ \hline
       Mean     & 23.52 & 22.48 & 23.14 & \textbf{23.70} & 0.68 & 0.63 & 0.66 & \textbf{0.74} & 0.42 & 0.47 & 0.44 & \textbf{0.41} \\ \hline \hline
    \end{tabular}
    }
   \end{center}
     \caption{\textbf{Quantitative comparison between \approach (OURS),  \approach with COLMAP initialization (OURS$^{COLMAP}$), \ac{nerf}$\text{-}\text{-}$~\cite{wang_nerf_2021} and COLMAP-based \ac{nerf}~\cite{wang_nerf_2021} on \ac{llff}}. We follow the results of Wang et. al~\cite{wang_nerf_2021} for COLMAP-based \ac{nerf}. Thus, the reported \ac{nvs} quality is overall lower compared to vanilla \ac{nerf} as the layer size is 128 instead of 256. We report \ac{psnr}, \ac{ssim} and \acs{lpips}. When using COLMAP initialization we outperform COLMAP-based \acs{nerf} in \acs{nvs} quality.}
    \label{tab:benchmark_llff_colmap_init}
\end{table*}


\subsubsection{SSIM Loss}
To test whether the additional \ac{ssim} loss supports the pose estimation, we train on each scene of \ac{llff} for 500 epochs with fixed seeds. As can be observed in Fig.~\ref{fig:ssimcomp} the altered training objective leads to a convergence in scenes where a mirror pose would otherwise occur. The convergence speed for the rotation and translation error also increases. Since the \ac{psnr} is lower when \ac{ssim} loss is used in the whole training process, we apply it only in the first 500 epochs. 





\subsubsection{Breaking Point Analysis on \datasetname}

Our main paper includes a breaking point analysis on \ac{llff}, where we outperform  \ac{nerf}$\text{-}\text{-}$. We also run this comparison on \datasetname. A rotation error lower than $20^{\circ}$ is considered as success. For the training, we used every second, third, fourth and fifth image. As shown in Table~\ref{tab:breaking_ana_iff}, we succeed in one more scene than \ac{nerf}$\text{-}\text{-}$. This allows the conclusion, that differing intrinsic camera parameters make the estimation of camera extrinsic parameters more challenging.

\begin{table*}[t!]
   \begin{center}
       \resizebox{\textwidth}{!}{ 
    \begin{tabular}{l|ccc|ccc|ccc|ccc|ccc} \hline \hline
       \multirow{2}{*}{Scene} & \multicolumn{3}{c|}{PSNR$\uparrow$} & \multicolumn{3}{c|}{SSIM$\uparrow$} & \multicolumn{3}{c|}{LPIPS$\downarrow$} & \multicolumn{3}{c|}{Rot. Err.} & \multicolumn{3}{c}{Trans. Err.} \\  % \cmidrule(r){2.5-4.5}
        & \ac{nerf}$\text{-}\text{-}$ & SiNeRF & OURS & \ac{nerf}$\text{-}\text{-}$ &  SiNeRF & OURS &  \ac{nerf}$\text{-}\text{-}$ & SiNeRF & OURS  & \ac{nerf}$\text{-}\text{-}$& SiNeRF & OURS & \ac{nerf}$\text{-}\text{-}$& SiNeRF & OURS  \\ \hline
       Fern & 22.15 & 22.48 &  \textbf{22.75} & 0.65 & 0.67 &  \textbf{0.69} & 0.46 & 0.44 &  \textbf{0.41} & 1.57 & 0.74 & 0.94 & 0.008 & 0.004 & 0.004\\
       Flower & 26.61 & \textbf{27.23} & 25.95 & 0.77 & \textbf{0.80} & 0.74 & 0.30 & 0.30 & 0.33 & 3.21 & 0.51 & 2.3 & 0.012 & 0.008 & 0.022\\
       Fortress & 25.60 & 27.47 &  \textbf{28.39} & 0.60 & 0.72  &  \textbf{0.77} & 0.54 & 0.39 &  \textbf{0.31} & 2.41 & 1.77 & 1.32 & 0.060 & 0.041 & 0.015\\
       Horns & 23.17 & 24.14 &  \textbf{25.20} & 0.64 & 0.68 &  \textbf{0.73} & 0.51 & 0.43 &  \textbf{0.37} & 3.04 & 2.66 & 1.45 & 0.015 & 0.022 & 0.005\\
       Leaves &  \textbf{19.74} & 19.15 & 19.00 &  \textbf{0.61} & 0.57 & 0.55 &  \textbf{0.39} &  \textbf{0.39} & 0.43  & 6.78 & 8.76 & 4.03 & 0.006 & 0.008 & 0.016\\
       Orchids & 15.86 &  \textbf{16.92} & 16.87 & 0.35 &  \textbf{0.41} &  \textbf{0.41} & 0.55 & 0.53 &  \textbf{0.50} & 5.46 & 3.24 & 4.19 & 0.022 & 0.013 & 0.024\\
       Room  & 25.68 & 26.10 &  \textbf{27.54} & 0.84 & 0.84 &  \textbf{0.87} & 0.41 & 0.43 &  \textbf{0.37} & 3.75 & 2.08 & 1.15 & 0.021 & 0.021 & 0.036\\
       T-Rex & 23.38 & \textbf{24.94} & 24.25 & 0.76 & \textbf{0.82} & 0.79 & 0.39 & 0.36 & 0.36 & 6.34 & 0.86 & 3.36 & 0.015 & 0.005 & 0.009\\ \hline
       Mean & 22.77 & 23.55 & \textbf{23.74} & 0.65 & \textbf{0.69} & \textbf{0.69} & 0.44 & 0.41 & \textbf{0.39} & 4.07 & 2.58 & \textbf{2.32} & 0.019 & \textbf{0.015} & 0.016 \\ \hline \hline
    \end{tabular}
    }
   \end{center}
     \caption{\textbf{Quantitative comparison between \approach (OURS), \ac{nerf}$\text{-}\text{-}$~\cite{wang_nerf_2021} and SiNeRF~\cite{xia_sinerf_2022} on  \ac{llff}, with a layer dimension of 256 for all \ac{nerf} frameworks.} We follow Xia et. al~\cite{xia_sinerf_2022} for the results of \ac{nerf}$\text{-}\text{-}$ with a layer dimension of 256. We report \ac{psnr}, \ac{ssim}, \acs{lpips} and the extrinsic camera error, in detail translation and rotation error. \approach outperforms \ac{nerf}$\text{-}\text{-}$~\cite{wang_nerf_2021} and SiNeRF in \acs{psnr} and \acs{lpips}.}
    \label{tab:benchmark_llff_256}
\end{table*}



\subsubsection{COLMAP Initialization for Camera Parameter Estimation}
\approach per se learns the camera parameters from scratch,{so without prior initialization}. 
Besides learning the camera parameters without initializing the pose \ac{mlp}, COLMAP pre-processed poses can be used for its initialization. Research on $360^{\circ}$ pose estimation and joint \ac{nvs} optimization initializes the pose network either directly with COLMAP or refines from noisy poses also calculated from COLMAP poses~\cite{lin_barf_2021}.{For ablation studies,} we initialize the pose \ac{mlp} with COLMAP poses and then train the joint optimization. The results are denoted in Table~\ref{tab:extrinsic_llff_colmap} for \ac{llff} and in Table~\ref{tab:extrinsic_iff_colmap} for \datasetname. We provide results for joint optimization approaches and for the COLMAP baseline on both dataset. Again, the results show that our improved pose prediction reduces the focal length error and improves \ac{nvs}. Also, on \datasetname with diverse intrinsic and extrinsic parameters our approach outperforms COLMAP-based \ac{nerf}, \ac{nerf} in the joint optimization.
% Besides training the network from scratch we also tested how our approach performs when using COLMAP initialization. 



\begin{table}[t!]
    \centering

   \begin{center}
    % \resizebox{\columnwidth}{!}{ 
    \begin{tabular}{l|ccc} \hline \hline
       \multirow{2}{*}{Scene} & \multicolumn{3}{c}{Focal. Err.} \\
        & \ac{nerf}$\text{-}\text{-}$ & SiNeRF & OURS \\ \hline 
       Fern     & 153.5 & \textbf{112.7} & 161.6\\
       Flower   & \textbf{13.2} & 80.4 & 53.6\\
       Fortress & 144.1 & \textbf{59.7} & 129.3 \\
       Horns    & 156.2 & 282.5 & \textbf{92.5} \\
       Leaves   & 59.0 & 198.8 &  \textbf{24.4} \\
       Orchids  & 199.3 & \textbf{155.7} & 165.9 \\
       Room     & 331.8 & 313.6 & \textbf{102.7}\\
       T-Rex    & 89.3 & \textbf{38.3} & 100.4\\ \hline
       Mean     & 143.3 &  155.2 & \textbf{103.7} \\ \hline \hline
    \end{tabular}
   %  }
   \end{center}
    \caption{\textbf{Quantitative comparison between \approach (OURS), \ac{nerf}$\text{-}\text{-}$~\cite{wang_nerf_2021} and SiNeRF~\cite{xia_sinerf_2022} with a \ac{nerf} layer dimension of 128 on \ac{llff}.} We report the focal pixel error. Our approach outperforms \ac{nerf}$\text{-}\text{-}$~\cite{wang_nerf_2021} and SiNeRF~\cite{xia_sinerf_2022}.}
    \label{tab:extrinsic_and_intrinsics_llff}
\end{table}

\begin{table}[t!]
    \centering
    
       \resizebox{\columnwidth}{!}{ 
    \begin{tabular}{l|l} \hline \hline
        Scene & Focal Lengths \\ \hline
        T1 &{$16\times740.90$, $14\times1673.73$, $1\times980.71$}  \\ 
        Fireplug &{$21\times3531.73$, $2\times2663.47$, $8\times683.74$} \\ 
        Bike &{$16\times3022.43$, $15\times756.16$} \\ 
        Stormtrooper &{$29x 1573.18, 1x 1380.96, 1x 585.26$} \\ 
        Brick House &{$10\times3000.51$, $2\times3164.69.86$, $10\times1483.85$, $9\times746.25$} \\ \hline \hline
    \end{tabular}
    }
    
    \caption{\textbf{Focal lengths of the individual scenes from \datasetname.} The camera parameters were estimated using COLMAP.}
    \label{tab:ibleff_license}
\end{table}

\begin{table*}[t!]
    \centering

   \begin{center}
    \resizebox{\textwidth}{!}{     
    \begin{tabular}{l|cccc|cccc|ccc|ccc|ccc} \hline \hline
        \multirow{2}{*}{Scene} & \multicolumn{4}{c|}{\acs{psnr}} & \multicolumn{4}{c|}{\acs{ssim}} & \multicolumn{3}{c|}{Focal Err.} & \multicolumn{3}{c|}{Rot Err.} & \multicolumn{3}{c}{Trans Err.}\\
        &{\ac{nerf}} & \ac{nerf}$\text{-}\text{-}$ & \ac{nerf}$\text{-}\text{-}$ + I & I+GF &{\ac{nerf}} & \ac{nerf}$\text{-}\text{-}$ & \ac{nerf}$\text{-}\text{-}$ + I & I+GF  & \ac{nerf}$\text{-}\text{-}$ & I & I+GF & \ac{nerf}$\text{-}\text{-}$ & I & I+GF & \ac{nerf}$\text{-}\text{-}$ & I & I+GF\\ \hline  
        T1 &{25.91} & 27.26 & 27.61 & \textbf{28.86} &{0.83} & 0.86 & 0.85 & \textbf{0.89} & 207.96 & 76.11 & 50.41 & 2.31 & 1.15 & 0.34 & 0.174 & 0.022 & \textbf{0.010}\\ 
        Brick House &{25.44} & 24.94 & 27.85 & 28.57 &{0.74} & 0.75 & 0.77 & \textbf{0.79} & 170.07 & 51.63 & 28.85 & \textbf{2.45} & 5.29 & 3.09 & 0.148 & 0.046 & \textbf{0.034} \\
        Fireplug &{22.60} & 22.52 & 22.95 & \textbf{25.15} &{0.65} & 0.61 & 0.63 & \textbf{0.71} & 267.89 & 123.24 & \textbf{92.14} & \textbf{5.45} & 5.61 & 5.51 & 0.006 & \textbf{0.001} & 0.007 \\
        Bike &{25.81} & 15.94 & 22.38 & \textbf{26.53} &{0.84} & 0.27 & 0.71 & \textbf{0.85} & 326.48 & 207.72 & \textbf{43.61} & 7.50 & 4.71 & \textbf{3.26} & 0.080 & 0.031 & 0.001 \\
        Stormtropper &{25.00} & 30.00 & 34.61 & \textbf{36.99} &{0.90} & 0.94 & 0.98 & \textbf{0.99} & 121.47 & 74.83 & \textbf{34.69} & \textbf{7.40} & \textbf{7.40} & 11.54 & \textbf{0.896} & \textbf{0.896} & 1.224 \\
        Mean &{24.95} & 24.16 & 27.57 & \textbf{29.50} & 0.79 & 0.69 & 0.81 & \textbf{0.86} & 208.44 & 101.61 & \textbf{48.06} & 5.41 & 4.85 & \textbf{4.53} & 0.263 & \textbf{0.200} & 0.232 \\ \hline \hline
    
    \end{tabular}
   }
   \end{center}
    \caption{\textbf{Quantitative comparison between{COLMAP-based \ac{nerf}}, \ac{nerf}$\text{-}\text{-}$~\cite{wang_nerf_2021}, \ac{nerf}$\text{-}\text{-}$ and our improved intrinsic estimation (I) and \approach (I+GF) on \datasetname with COLMAP initialization.} We report \acs{psnr}, \acs{ssim}, rotation error, translation error and focal length error.}
    \label{tab:extrinsic_iff_colmap}
\end{table*}



\subsection{Intrinsic Camera Parameters}

In addition to the extrinsic camera parameter estimation, we compare our intrinsic camera parameter estimation against \ac{nerf}$\text{-}\text{-}$ and SiNeRF on \ac{llff}. As denoted in Table~\ref{tab:extrinsic_and_intrinsics_llff}, we outperform both approaches in the mean focal length prediction. While we clearly outperform \ac{nerf}$\text{-}\text{-}$ in the individual values, SiNeRF shows comparable results. However, SiNeRF produces a significant error in the room scene. Thus, their mean focal length error suffers. When excluding the room scene from the mean value calculation, we still outperform SiNeRF, with a focal pixel error of 103.9 compared to 132.6.

Moreover, the COLMAP initialization tested on \ac{llff} shows also a lower focal length error.  As denoted in Table~\ref{tab:extrinsic_llff_colmap},   a reduced pose error leads to a lower focal length error.

\subsection{A Higher \acs{nerf} Layer Dimension and its Influence on Camera Parameters}

\approach is off-the-shelf comparable with \ac{nerf}$\text{-}\text{-}$, as we use a layer dimension of 128. SiNeRF was originally trained with a layer dimension of 256. When applying this layer dimension to our \ac{nerf}, the results for the intrinsic and extrinsic camera parameters change due to the joint optimization. For the focal length we report a mean error of 72.12. With a layer dimension of 128 we reported a mean focal length error of 103.7, see Table~\ref{tab:extrinsic_and_intrinsics_llff}. The extrinsic camera parameter results are denoted in Table~\ref{tab:benchmark_llff_256}. \approach outperforms the other joint optimization approaches in rotation error and translation error when using a layer dimension of 256.

\section{Novel View Synthesis} 

\subsection{COLMAP Initialization for NVS Quality}

Initializing the pose \ac{mlp} with COLMAP shows an improved prediction of the intrinsic and extrinsic camera parameters, see Table~\ref{tab:extrinsic_llff_colmap}{and Table~\ref{tab:extrinsic_iff_colmap}}. Our further experiments{on \ac{llff} and \datasetname} demonstrate that a better pose estimation leads to an improved \ac{nvs}. Table~\ref{tab:benchmark_llff_colmap_init} shows the \ac{psnr}, \ac{ssim} and \ac{lpips} results for all scenes on \ac{llff}. The \ac{nvs} quality is higher in the joint optimization than when purely training on COLMAP poses. These findings corrospond with other related works~\cite{lin_barf_2021,truong_sparf_2022}. % They already show that joint optimization of camera poses and \ac{nerf} lead to increased \ac{nvs} quality.  

\subsection{Higher Layer Dimension for NVS Quality}
In our ablation study we compare \approach with the original SiNeRF and their adapted \ac{nerf}$\text{-}\text{-}$ with a layer dimension of 256, see Table~\ref{tab:benchmark_llff_256}. \approach outperforms both other joint optimization  approaches. We achieve better results in \ac{psnr} and \ac{lpips}. For \ac{ssim} we perform equally compared to SiNeRF but outperform \ac{nerf}$\text{-}\text{-}$.

\begin{figure*}[t!]
  \centering
    \subfloat[Brick House]{
        \includegraphics[height=2cm]{brick_house.JPG}}
    \hfill % \hspace{2cm}
    \subfloat[Stormtrooper]{
        \includegraphics[height=2cm]{0000.jpg}}
    \hfill
    \subfloat[Bike]{
        \includegraphics[height=2cm]{0001.jpg}}
    \hfill % \hspace{2cm}
    \subfloat[Fireplug]{
        \includegraphics[height=2cm]{0010.jpg}}
    \hfill
    \subfloat[T1]{
        \includegraphics[height=2cm]{0011.png}}
        
    \caption{\textbf{Example images from \datasetname.}\label{fig:example_images} We have three indoor and two outdoor scenes. The brick house is captured with an iPad Air 2 and an iPhone 13 mini. The stormtrooper, bike and fireplug are captured with an iPhone 13 mini with different resolutions. The T1 is captured with an iPhone 13 mini and the OAK D-Lite camera.}
\end{figure*}



\section{Dataset} 



In our \datasetname dataset we captured \numberofscenes real world scenes. These \numberofscenes scenes are namely T1, brick house, bike, fireplug and stormtrooper. Example images from \datasetname are depicted in Fig.~\ref{fig:example_images}. The scenes are captured with an OAK-D Lite camera with varying resolution, an iPhone mini 13 and an iPad Air 2. We also applied varying resizing factors to later receive more differing intrinsic estimations from COLMAP. The focal lengths of the individual scenes are denoted in Table~\ref{tab:ibleff_license}. All scenes were captured from a video stream. From this video stream we extract one frame per second. This results in the final images of the dataset. To receive the pseudo ground truth we applied COLMAP.







