\section{Evaluation}

We evaluate the camera parameter estimation and \ac{nvs} quality of \approach on two real-world datasets, namely \ac{llff} and our intrinsic forward-facing (\datasetname) dataset. On \ac{llff}, we compare our approach in detail with the results from \ac{nerf}$\text{-}\text{-}$~\cite{wang_nerf_2021} and SiNeRF~\cite{xia_sinerf_2022}. To evaluate the performance of joint optimization approaches when diverse intrinsic camera parameters are present, we compare \ac{nerf}$\text{-}\text{-}$ and our approach on \datasetname. Additionally, we compare our results  with \ac{nerf}$\text{-}\text{-}$~\cite{wang_nerf_2021} on the synthetic \ac{bleff} dataset. 

\subsection{Datasets}

\textbf{LLFF:} Eight forward-facing scenes are included in \ac{llff}~\cite{mildenhall_local_2019}. These scenes have a varying number of images ranging from 20 up to 62. The pseudo ground truth of \ac{llff} derives from COLMAP.

\textbf{BLEFF:} Wang et al.~\cite{wang_nerf_2021} presented \acs{bleff} to evaluate camera parameter estimation accuracy and \acs{nvs} rendering quality. \acs{bleff} contains 14 scenes with 31 images each with a resolution of $1040 \times 1560$. For comparability, with \ac{nerf}$\text{-}\text{-}$ we followed the downscaling to a resolution of $520 \times 780$ and the $t010r010$ \acs{bleff} setup. 

\textbf{\datasetname:}  
We captured \numberofscenes real-world scenes namely T1, brick house, bike, fireplug and stormtrooper with 31 images each. The scenes were captured with an OAK-D Lite, an iPhone mini 13, and an iPad Air 2 with varying resolutions. Additionally, we added resizing, to show the influence of various image sizes. We received the pseudo ground truth by applying COLMAP. Details about the focal length and example images are included in the supplementary material. % Each scene of the dataset contains 31 images. 

\subsection{Evaluation Metrics}

To evaluate our approach, we consider two aspects. First, the rendering quality of the novel views. Here, we report \ac{psnr}, \ac{ssim}~\cite{wang_image_2004} and \ac{lpips}~\cite{zhang_unreasonable_2018}. The second aspect is the camera parameter estimation. To report on the intrinsic camera parameter quality, we measure the focal length error in pixels. For the extrinsic camera parameters, we use the  \ac{ate}~\cite{zhang_tutorial_2018,sturm_benchmark_2012} and similarity transformation $Sim(3)$ to align the ground truth and predicted poses. As metric, we report the rotation and translation error.

\subsection{Loss Function}

The commonly used loss $L_{pix} = \sum_{r \in R_i}^{}||I(r)- \hat{I}(r)||^2$, also called photometric loss, is applied to train the joined optimization.
% We conducted the photometric loss $L_{pix} = \sum_{r \in R_i}^{}||I(r)- \hat{I}(r)||^2$ which is commonly used to train a \ac{nerf}. 
Moreover, we found that the pose prediction stability can be strengthened by applying the \ac{ssim} loss $\mathit{L}_{\text{SSIM}}(P)=\frac{1}{N}\sum_{p \in P}^{} 1 - \text{SSIM}(p)$ during the starting phase of the training. 


\input{overview}

\input{llff}


\subsection{Implementation Details} 

\approach is implemented in PyTorch. It (a) excludes the hierarchical sampling strategy; (b) has a layer dimension of 128 instead of 256; (c) samples 1024 pixels
from each input image and 128 points along each ray~\cite{wang_nerf_2021}. To initialize our \ac{nerf}, we use Kaiming initialisation~\cite{he_delving_2015}. The focal length is initialized by the individual camera height and width considering the given resize factor of the individual dataset and optimized during training. The camera poses are initialized in $-z$ direction. 

We use Adam optimizer for the camera parameters and \ac{nerf}. The initial learning rate is set to $10^{-3}$ for all models. We decay the \ac{nerf} learning rate all 10 epochs by multiplying with 0.9954. The focal and pose learning rate are decayed every 100 epochs by multiplying 0.9. 

To ensure comparability with \approach, we retrain SiNeRF with a layer dimension of 128 as the original SiNeRF has a layer dimension of 256. For SiNeRF we tried 10 random seeds per scene and report the best results on \ac{llff}. In our supplementary material we show the comparison of \approach and SiNeRF with a layer dimension of 256. 



\input{room_fig}



\subsection{Novel View Synthesis Quality}
We compare our approach with COLMAP-based \ac{nerf}, on \ac{llff} and \ac{bleff}. The COLMAP-based \acs{nerf} follows the same 128-layer dimension as \ac{nerf}$\text{-}\text{-}$. This results in lower \ac{nvs} quality compared to the vanilla \ac{nerf}~\cite{mildenhall_nerf_2020}.

As shown in Table~\ref{tab:overview}, for \acs{llff} and \acs{bleff} we achieve competitive \ac{nvs} compared to COLMAP without any need for preprocessing and outperform existing joint optimization approaches. On \ac{llff}, we outperform these approaches in \ac{psnr}, \ac{ssim} and \ac{lpips}. When using COLMAP initialization for the joint optimization we also outperform COLMAP-based \ac{nerf}. Detailed results for the COLMAP initialization can be found in the supplementary material. The detailed results on \ac{llff} for \acs{nvs} quality can be found in Table~\ref{tab:benchmark_llff}. We outperform \ac{nerf}$\text{-}\text{-}$ on all scenes on \ac{llff} and SiNeRF on seven out of eight scenes in \ac{psnr}. Overall we achieve better results for mean \ac{psnr}, mean \acs{ssim} and mean \acs{lpips} compared to all other joint optimization methods. 
We also report visual results in Fig.~\ref{fig:fortress_fig}, comparing our result on \ac{nvs} to \ac{nerf}$\text{-}\text{-}$. As shown in the room scene of \ac{llff}, we better reconstruct fine-grained details in the focus of the image and in the corners. 

Moreover, we outperform \ac{nerf}$\text{-}\text{-}$ on \ac{bleff} in \ac{psnr}, and perform equally in \ac{ssim}. The results for each scene are reported in Table~\ref{tab:benchmark_bleff}. 

On \datasetname we compared \ac{nerf}$\text{-}\text{-}$ and \approach (OURS), as shown in Table~\ref{tab:benchmark_iff}. Here, we first tested only our intrinsic camera prediction module, as \datasetname contains diverse focal lengths. Using our intrinsic camera module alone, we achieved an improved \ac{nvs} quality. Thereby, we already outperformed \ac{nerf}$\text{-}\text{-}$ in \ac{psnr} and \ac{ssim}. Combined with our Gaussian Fourier feature mapping for the extrinsic camera prediction we achieved an even better \ac{nvs} quality. As shown in Table~\ref{tab:benchmark_iff}, we outperform \ac{nerf}$\text{-}\text{-}$ on all scenes in \ac{psnr} and \ac{ssim}.

\input{breaking_point}

\input{bleff}
\subsection{Camera Parameter Estimation}

\subsubsection{Extrinsic Camera Parameters}
For each scene in \ac{llff}, we report the rotation and translation error of \ac{nerf}$\text{-}\text{-}$, SiNeRF$128$ and our approach in Table~\ref{tab:benchmark_llff}. 
We outperform both approaches with \approach. Furthermore, we show the advantage of the \acs{ssim} loss, exemplarily for the scenes in \ac{llff} in our supplementary material. For \ac{bleff} and \datasetname, we report the detailed results of \ac{nerf}$\text{-}\text{-}$ and \approach in Table~\ref{tab:benchmark_bleff} and in Table~\ref{tab:benchmark_iff}. Our approach shows an improved extrinsic camera parameter estimation on all datasets. 


\paragraph{Positional Encoding}
Approaches like BARF~\cite{lin_barf_2021}  recognize the benefits of Fourier features for noisy pose estimation. However, they do not use Gaussian Fourier features but instead the so-called positional encoding. Our experiments show that the use of Gaussian Fourier features in comparison leads to a better convergence. For this purpose, we train our network with Gaussian Fourier features and positional encoding with ten different random seeds on each scene. The Gaussian Fourier features lead to an average rotation error of \textit{26.91} compared to the positional encoding with an average rotation error of \textit{45.95}. While the gap between the translation error remains small, the Gaussian Fourier features still outperform the positional encoding with an error of 0.039 compared to 0.042. The high error is caused by mirror poses in the ten runs. The runs are included in the score calculation to show that mirror poses are less likely with Gaussian Fourier features. A detailed overview of the error in each scene can be found in the supplementary material. 

\paragraph{Breaking Point Analysis}
A breaking point analysis for our pose \acs{mlp} shows an improved stability on the pose predictions by using fewer images. We compare our approach with \ac{nerf}$\text{-}\text{-}$ on \acs{llff}. A rotation error greater than $20^{\circ}$ is considered as failed. For the training, we used every second, third, fourth, fifth and sixth image. As shown in Table~\ref{tab:breaking_ana}, we succeed in 31 scenes, while \ac{nerf}$\text{-}\text{-}$ only succeeds in 24. % Additionally, breaking-point analysis on \datasetname can be found in the supplementary material. There we succee

\input{iff}

\subsubsection{Intrinsic Camera Parameters}

\input{bulli_fig}

In the focal length estimation, we outperform existing approaches on \ac{llff} and \ac{bleff}, see Table~\ref{tab:overview}. This indicates, that the joint optimization and an improved extrinsic camera parameter estimation also supports the intrinsic camera parameter estimation. As described in detail in the supplementary material, we outperform \ac{nerf}$\text{-}\text{-}$ on five out of eight scenes on \ac{llff} in the focal length estimation. On \ac{bleff} we achieve a better focal length estimation on eight out of 14 scenes, see Table~\ref{tab:benchmark_bleff}.  The scenes in both datasets are captured by a single camera. For this reason, we introduce \datasetname. The focal lengths in the scenes vary as we used different cameras and rescaling factors. While \ac{nerf}$\text{-}\text{-}$ averages the scaling factor over all images, we learn the individual intrinsic camera parameter per given camera. To show the influence of varying camera parameters, we trained \ac{nerf}$\text{-}\text{-}$, \ac{nerf}$\text{-}\text{-}$ combined with our intrinsic camera parameter method and our \approach on \datasetname. As presented in Table~\ref{tab:benchmark_iff}, the network better estimates the focal lengths. Using only our intrinsic module we already show an improved focal length estimation.  \approach predicts the focal length even better, possibly due to the improved prediction of the camera pose. A more accurate focal length has an impact on the results, as demonstrated in Fig.~\ref{fig:bulli_fig}. Unlike the image produced by \ac{nerf}$\text{-}\text{-}$, the image produced by \approach shows no distortion and is more accurate compared to the ground truth.






