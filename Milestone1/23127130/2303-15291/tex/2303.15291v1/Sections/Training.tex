% \section{Training}

For training AI models, PyTorch packages were used. The code is maintained in a public Github repository (\url{https://github.com/FAIR-UMN/FAIR-UMN-ECAL}). Conda environments are provided so that the users of the datasets can use any API of their liking. The project details can also be found in \url{https://fair-umn.github.io/FAIR-UMN-Docs}.

%\subsection{Training LSTM Model}
%The LSTM model was trained using the Tensorflow library. 

\subsection{Data Pre-Processing}

% todo list
%\todo{Reference to the actual notebook could be useful here.}

1080 csv files from the datasets mentioned in \cref{datasets} are used for the training and prediction. 360 csv files (\texttt{df\_skimmed\_xtal\_54000\_2016} to \texttt{df\_skimmed\_xtal\_54359\_2016}; size 203 MB) are used for training, and 720 csv files (\texttt{df\_skimmed\_xtal\_54000\_2017} to \texttt{df\_skimmed\_xtal\_}\\\texttt{54359\_2017} and \texttt{df\_skimmed\_xtal\_54000\_2018} to \texttt{df\_skimmed\_xtal\_54359\_2018}; size 406 MB) are used for prediction. 

The difference between subsequent entries in the dataset is the integrated luminosity delivered between the two consecutive measurements. Before training the networks, the measured calibration values and the luminosity differences in the training dataset are normalized to unity using the \textit{StandardScaler} from the sklearn library. Next, to obtain the input $X$ and the true output $Y_{\text{true}}$ used for model training, we performed the following steps:
\begin{itemize}
\item Define the input length $L_E$ (e.g., $L_E=24$), corresponding to the number of LSTM units in the encoder, and the output length $L_D$ (e.g., $L_D=24$), corresponding to the number of LSTM units in the decoder for each individual sample. The Seq2Seq model will be trained to use a sequence of calibration values and luminosity differences of length $L_E$ and learn to predict the next $L_D$ calibration values. 
\item To avoid any overlap between the prediction sequences, a separation stride $L_S$ is set to be the same as $L_{D}$. Therefore, the total number of samples is 
\[N_{sample} = \frac{N-L_{E}-L_{D}}{L_{S}}+1,\]
where N is the total number of entries in the training dataset. For each individual sample, the input is a sequence starting from $T$ to $T+L_{E}-1$ and the output is a sequence starting from $T+L_{E}$ to $T+L_{E}+L_{D}-1$. 
\item In PyTorch, the LSTM module takes a 3D tensor as the input whose dimensions are given by (sequence, batch, features). In this problem, the input to encoder (X$_{\text{encoder}}$), the input to decoder (X$_{\text{decoder}}$) and the output of the decoder (Y$_{\text{decoder}}$) can be represented as below:
\begin{align*}
    &X_{\text{encoder}} \in \R^{L_{E}\times N_{\text{sample}}\times N_{E}},\\
    &X_{\text{decoder}} \in \R^{L_{D}\times N_{\text{sample}}\times N_{D}},\\
    &Y_{\text{decoder}} \in \R^{L_{D}\times N_{\text{sample}}\times 1},
\end{align*}
where N$_{E}$ and N$_{D}$ are the number of features used in the encoder and decoder, respectively. In this study, N$_{E}$ and N$_{D}$ are set to 2, which represents features calibration value and luminosity difference. But more features, such as the difference in the timestamps between two entries, can be added if needed.
\end{itemize}

\subsection{Training Seq2Seq Model}
The Seq2Seq model was built using the PyTorch library. Both encoder and decoder blocks were set with 1024 hidden layers. The number of LSTM cells in the encoder and the decoder is varied to scan for the optimal input and output lengths. The LSTMs were initialized with a sigmoid activation for input, forget, and output gates and a hyper-tangent activation for the cell gates. The Mean Squared Error (MSE) loss function along with the Adam~\cite{kingma2014adam} optimizer is used to train the model. The model is trained for 200 epochs with a batch size of 128 and a learning rate of $10^{-3}$. A higher number of epochs (3000) were used to check if the model performance improves, but it was found that it converges after about 200 epochs. All trainings and predictions were performed on machine with Intel Xeon Silver 4214R@2.40GHz, and Nvidia RTX A6000 graphics card with 48 GB memory.

With a large number of data points, there are several options available for training a model. A single model can be developed for each of the individual crystals. On the other hand, the data points of different crystals that are at equal distances from the center of ECAL, i.e., within one i$\eta$-ring of the ECAL, can be combined together. The assumption is that these crystals receive an equal amount of radiation dose because of the radial symmetry and hence will have similar behavior in laser response over a course of time. Then this model trained with more data points would be able to predict calibrations for all the crystals in the corresponding i$\eta$-ring. In addition to changing the number of crystals, three different strategies were used which are given as follows:
\begin{enumerate}
\item Recursive: In this setting, as shown in Figure \ref{fig:train_strg} (left), we feed the token from $Y_{\text{pred}}$ from the previous time step as the input to the current time step.
\item Teacher Forcing: In this setting, as shown in Figure \ref{fig:train_strg} (right), we feed the token from $Y_{\text{true}}$ (instead of the token from $Y_{\text{pred}}$) from the previous time step as the input to the current time step.
\item Mixed: In this setting, the previous two strategies can be combined in different ratios. For example, a mixed training with a teacher forcing ratio of 0.7 means that only 70\% of the batches in the decoder training use the teacher forcing strategy. 
\end{enumerate}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.45\textwidth]{img/our_seq2seq_recursive.png}
  \includegraphics[width=0.45\textwidth]{img/our_seq2seq_tf.png}
  \caption{\textbf{(left)}: Seq2Seq model with recursive training; \textbf{(right)}: Se2Seq model with teacher forcing.}
  \label{fig:train_strg}
\end{figure}