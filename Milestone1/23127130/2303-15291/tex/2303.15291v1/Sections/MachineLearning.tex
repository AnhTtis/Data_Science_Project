\section{Machine Learning Solutions}

We describe in this and subsequent sections two ML solutions that we have developed to address this problem, their training and the results we have obtained. The source code is published on GitHub at \textcolor{blue}{\href{https://github.com/FAIR-UMN/fair_ecal_monitoring}{https://github.com/FAIR-UMN/fair\_ecal\_monitoring}}.

Neural networks (NNs) are machine learning (ML) methods that mimic the biological structure and functioning of neurons in a brain. A NN that does not involve any cyclic connections is called a Feedforward neural network (FNN). Deep neural networks (DNNs) are structures that consist of many stages of interconnected neurons. DNNs perform well for hard learning tasks, such as object identification and speech recognition. However, they require that the inputs and outputs of the task be encoded into vectors with fixed dimensionality \cite{sutskever2014sequence}. 

%It consists of units called \textit{neurons}, each activating to produce an output based on the inputs received. The inputs received by a neuron are weighted according to the weight assigned for each input connection and in some cases a bias is added. Following the input transformation, an activation function is applied to the input to produce an output. The model is \textit{trained} to tune the weights assigned to the input connections to maximize the accuracy of the final output. The number of neurons and their inter-connectivity can vary depending on the given problem. Due to the non-linearity introduced by the \textit{activation functions} at each node, determining the weights becomes analytically non-trivial. In NN models which are trained using teacher-based \textit{Supervised Learning}, an algorithm based on gradient descent method, known as \textit{backpropagation}\cite{backpropagation} efficiently determines the optimum set of weights.  

Many problems, like machine translation and speech recognition, have a sequential structure, since their input and output lengths are not known \textit{a-priori}~\cite{sutskever2014sequence}. 
%The same architectures have also been implemented to solve time series prediction problems. 
A class of neural networks called recurrent neural networks (RNNs) are a type of FNNs that pass the data sequentially between different nodes. This architecture allows the network to learn and to retain past knowledge when processing data points from a given data series. RNNs have some shortcomings---in particular, the vanishing gradient problem~\cite{vanishing-gradient} while training the network. Other architectures have been developed in the past to address these issues.

% This feature allows them learn the trends in data series over long ranges and make accurate forecasts. 
\input{Sections/LSTM}
\input{Sections/Seq2Seq}