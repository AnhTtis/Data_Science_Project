% \subsection{Evaluation}
To quantify the performance of a LSTM or Seq2Seq model, we use Mean Absolute Percentage Error (MAPE) as the metric, which is defined as
\begin{align}
    \text{MAPE} = \frac{100\%}{n} \sum_{t-1}^n \abs{\frac{A_t-F_t}{A_t}},
\end{align}
where $A_t$ is the actual value and $F_t$ is the forecast value.

To determine an appropriate input length ($L_E$) and output length ($L_D$) for the Seq2Seq model, the models were trained using different sequence lengths with the recursive training strategy. We set $L_E=L_D$ in this example. As shown in Figure \ref{fig:diff_window_size}, the sequence length equals $24$ gives the lowest MAPE. Hence, this value will be used for all the trainings described in this section.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.75\textwidth]{img/diff_seq_len.png}

  \caption{Seq2Seq model trained with different sequence lengths.} 
  \label{fig:diff_window_size}
  \end{figure}
  
\subsection{Different Training Strategies}

For the purpose of this study, the data taken in the year 2016 corresponding to single crystal with ID 54000, from i$\eta$-ring 66, were used to train a Seq2Seq model (Model-S). Another model (Model-R) was trained using the data taken in the year 2016 corresponding to all 360 crystals in i$\eta$-ring 66, with ID ranging from 54000 to 54359. The response of the two models was evaluated on different crystal from the same i$\eta$-ring 66 using the data taken in the year of 2017 and 2018.

For making these predictions, two cases were used:
\begin{enumerate}
\item Case 1: The ground truth is provided as the model input at each prediction window (Figure \ref{fig:2cases} (left)).
\item Case 2: In this case only the first input is provided and the model would recursively ``reuses" the predictions from its previous prediction window as its input (Figure \ref{fig:2cases} (right)).  to make predictions and then evaluate their performance separately. Therefore, in terms of learning, this case is more challenging than Case 1 as we use less prior information.
\end{enumerate}
% The laser response for crystal with ID 54300 is different from the crystal with ID 54000 even though they are at the same distance from the point of collision. 

% Crystal with ID 54300 has much more degradation in its transparency throughout the Run 2 (2016-2018). The model was also tested on that crystal and it's able to make predictions with comparable MAPE (Figure \ref{fig:multiple_xtal_54300})

% \textcolor{red}{This paragraph is not very clear: is it evaluation or prediction? Both Case1 and Case2, when evaluated, they both compare their prediction with the ground truth; but Case1 and Case2 differ in the way of how they get their predictions?} 

% We use both Case 1 (see Figure \ref{fig:2cases} (left)) and Case 2 (see Figure \ref{fig:2cases} (right)) to evaluate the model performance---Case 1, where the outputs are evaluated individually on all the sets of input (ground truth) measurements, and Case 2, where only the first input is provided to the model and it recursively evaluates the next set of predictions by using the previous set of predictions as input.

% \textcolor{red}{We use both Case 1 (see Figure \ref{fig:2cases} (left)) and Case 2 (see Figure \ref{fig:2cases} (right)) to make predictions and then evaluate their performance separately. For the Case 1, the ground truth is provided as the model input at each prediction window; while for the Case 2, only the first input is provided and the model would recursively ``reuses" the predictions from its previous prediction window as its input. Therefore, Case 2 is more challenging than Case 1 as we use less ``prior" information.}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.5\textwidth]{img/case1.png}
  \includegraphics[width=0.48\textwidth]{img/case2.png}
  \caption{\textbf{(left)}: Case 1: without using prediction as the input of the next round prediction; \textbf{(right)}: Case 2: using prediction as the input of the next round prediction}
  \label{fig:2cases}
\end{figure}


% Two types of evaluations are used to check the performance of the model : case 1, where the outputs are evaluated individually on all the sets of input measurements, and case 2, where only the first input is provided to the model and it recursively evaluates the next set of predictions by using the previous set of predictions as input.

The input to each LSTM cell in the decoder contains the luminosity delivered ($\Delta L_i$) between the current and the next timestamp, and the previous calibration value. Typically, during training, the output of the previous LSTM cell in the decoder is used as input to the next LSTM as a current calibration value, along with the $\Delta L_i$. However, in case of teacher forcing, the true calibration value for input is used instead of the output from the previous LSTM cell. The teacher forcing ratio can be used to define the fraction of batches in the decoder training that would get true calibration values as input during training.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.495\textwidth]{img/result_xtal_ID_54300/single_xtal_MAPE_case1_2017.png}
  \includegraphics[width=0.495\textwidth]{img/result_xtal_ID_54300/multiple_xtal_MAPE_case1_2017.png}\\
  \includegraphics[width=0.495\textwidth]{img/result_xtal_ID_54300/single_xtal_MAPE_case2_2017.png}
  \includegraphics[width=0.495\textwidth]{img/result_xtal_ID_54300/multiple_xtal_MAPE_case2_2017.png}
  \caption{A demonstration of the true calibration curve vs the predicted calibration curve. (\textbf{top left}) Case 1 predictions using Model-S (mixed; teacher forcing ratio = 0.5): the response of the crystal ID 54300 in 2017. (\textbf{top right}) Case 1 predictions using Model-R (mixed; teacher forcing ratio = 0.5): the response of the crystal ID 54300 in 2017. (\textbf{bottom left}) Case 2 predictions using Model-S (mixed; teacher forcing ratio = 0.5): the response of the crystal ID 54300 in 2017. (\textbf{top right}) Case 2 predictions using Model-R (mixed; teacher forcing ratio = 0.5): the response of the crystal ID 54300 in 2017.} 
  \label{fig:demo}
\end{figure}

The demonstration of our predicted calibration curve is shown in Figure \ref{fig:demo}: Both Model-S and Model-R in Case 1 can successfully predict the calibration in future time steps with low MAPE. However, both Model-S and Model-R in Case 2 give worse predictions after certain time steps.

% \textcolor{red}{Question: do we only need to provide figures for case1? or do we need to provide similar figure5 for case2? If Fig5 is enough, then please ignore my question.}

% Our Seq2Seq model can successfully predict the calibration in future time steps. Models trained on crystal ID from 54000 to 54359. The MAPE is evaluated on all the 360 crystals from the ring as indicated in \ref{fig:multiple_case1_hist} and \ref{fig:multiple_case2_hist} for case (1) and case (2) respectively.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.995\textwidth]{img/MAPE.png}
  \caption{MAPE histograms from prediction on year 2016, 2017 and 2018. (a)(b)(c)(d) use Case 1 prediction strategy, while (e)(f)(g)(h) use Case 2 prediction strategy. (a) and (e) use model trained on single crystal 54000 of 2016. (b)(c)(d)(f)(g)(h) use model trained on all crystals in ring 66 (crystal ID: 54000-54359) of 2016. (b)(f) use recursive strategy in the encoder; (c)(g) use mixed strategy (teacher forcing ratio $=0.5$) in the encoder; (d)(h) use teacher forcing strategy in the encoder.}
  \label{fig:multiple_mape_hist}
\end{figure}

\begin{table}[!ht]
\begin{center}
\begin{tabular}[t]{ |c|c|c|c|c|c| } 
 \hline
 Year & Prediction  & Model-S(M) & Model-R(R) & Model-R(M) & Model-R(T)  \\ 
 \hline
 2016 & Case 1 & 0.194 & 0.168 & 0.180 & 0.191 \\  
 2017 & Case 1 & 0.223 & 0.228 & 0.234 & 0.263 \\ 
 2018 & Case 1 & 0.291 &  0.323& 0.330 & 0.391 \\ 
 \hline
 2016 & Case 2 & 0.888 &  0.516& 0.577 & 0.530 \\  
 2017 & Case 2 & 0.836 &  0.680& 0.713 & 0.673 \\ 
 2018 & Case 2 & 1.24 &  1.216 & 1.147 & 1.327 \\ 
 \hline
\end{tabular}
\end{center}
\caption{Average MAPE from prediction on all 360 crystals. M: mixed strategy of teacher forcing and recursive (teacher forcing ratio$=0.5$); R: recursive strategy; T: teacher forcing strategy.}
\label{tab:MAPE}
\end{table} 

Furthermore, MAPE has been evaluated for all 360 crystals (crystal ID:34000-34359) using the data for three years (2016, 2017 and 2018) and the distribution is shown in Figure \ref{fig:multiple_mape_hist}. Also, the corresponding average MAPE among all predictions is shown in Table \ref{tab:MAPE}. As shown in both Figure \ref{fig:multiple_mape_hist} and Table \ref{tab:MAPE} Model-R with recursive, mixed, and teacher forcing strategy have different behavior: the recursive version leads to lower MAPE than the teacher forcing version, which indicates a potential overfitting when using the teacher forcing strategy. Also, in the Case 1 prediction, the mixed strategy has MAPE between recursive and teacher-forcing; while in the Case 2, the mixed strategy gets a worse prediction in the 2016 and 2017 data, but gets a better prediction in the 2018 data.

As shown in both Figure \ref{fig:multiple_mape_hist} (a)(c)(e)(g) and Table \ref{tab:MAPE}, Model-S (M) is worse than Model-R (M) in the Case 2 prediction but better than Model-R (M) in the Case 1 prediction (2017 and 2018). Besides that, the Case 1 prediction would be recommended, as it always has better performance than the Case 2 prediction. 

% \textcolor{red}{do we need to be consistent and make all "Case 1 or Case 2" to be "Case 1 or Case 2"?}