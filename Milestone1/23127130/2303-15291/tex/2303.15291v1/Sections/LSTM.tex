\subsection{Long Short Term Memory (LSTM) Models}

LSTM models are a type of RNNs that include feedback components. RNNs are good at tracking arbitrary long-term dependencies in a sequence but have a tendency to be unstable during training. LSTMs solve the vanishing-gradient problem through an additive gradient structure. The LSTM cell is shown in Figure \ref{fig:encoder-decoder}, which is the key part of the Seq2Seq model. For each element in the input sequence, each layer of the LSTM computes the following functions: 
\begin{align}
    \begin{split}
        & \vi_t = \sigma(\mW_{ii}\vx_t + \mathbf{b}_{ii} + \mW_{hi}\vh_{t-1} + \mathbf{b}_{hi} ) \\
        & \vf_t = \sigma(\mW_{if}\vx_t + \mathbf{b}_{if} + \mW_{hf}\vh_{t-1} + \mathbf{b}_{hf} ) \\ 
        & \vg_t = \tanh(\mW_{ig}\vx_t + \mathbf{b}_{ig} + \mW_{hg}\vh_{t-1} + \mathbf{b}_{hg} ) \\
        & \vo_t = \sigma(\mW_{io}\vx_t + \mathbf{b}_{io} + \mW_{ho}\vh_{t-1} + \mathbf{b}_{ho} ) \\
        & \vc_t = \vf_t \odot \vc_{t-1} + \vi_t \odot \vg_t \\
        & \vh_t = \vo_t \odot \tanh{\vc_t} 
    \end{split}
\end{align}

where $\vh_t,\vc_t$ are the hidden and cell state at time $t$, $\vx_t$ is the input at time $t$, $\vi_t,\vf_t,\vg_t,\vo_t$ are the input, forget, cell, and output gates, respectively. $\sigma$ is the sigmoid function and $\odot$ is the Hadamard product~\cite{lstm}.

% In multi layer LSTM, the input $\vx_t^{(l)}$ of the $l$-th layer ($l\geq 2$) is the hidden state $\vh_t^{(l-1)}$ of the previous layer multiplied by dropout $\delta_t^{(l-1)}$.