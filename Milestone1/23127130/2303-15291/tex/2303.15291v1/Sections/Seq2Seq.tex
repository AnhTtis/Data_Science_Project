% \input{Utils/math_commands.tex}
% \input{Utils/ju_math.tex}

\subsection{Seq2Seq Model}

A sequence-to-sequence (Seq2Seq) model is an architecture that combines two or more LSTMs. It consists of two parts---the encoder and the decoder, each of which is built by using separate LSTMs. This type of model has been developed for automatic language translation, where a sentence from one language is translated to another language \cite{sutskever2014sequence}. The encoder is used to process each token in the input sentence, and encode all the input sequence information into a fixed-length vector. The transform vector, known as context vector, is a vector in a latent space and it encapsulates the whole meaning of the input sequence. The decoder reads the context vector and predicts the target sequence token by token.

Figure \ref{fig:encoder-decoder} shows the basic architecture of the encoder-decoder network used for this problem. The encoder block consists of LSTM units connected in series that take in a set of calibration values and the luminosity differences between those calibration values as input. All the information from the input sequence is encapsulated in the internal states $\vh_t$ (hidden state) and $\vc_t$ (cell state). The decoder block is another block of LSTM units connected in series. The final states $(\vh_t,\vc_t)$ of the encoder are used as the initial states $(\vh_0,\vc_0)$ of the decoder, which is the context vector used to predict the target sequence. The decoder network also takes input along with the initial states to predict the target sequence. The input to the decoder varies according to the method used for the training.

The Seq2Seq model can be trained using teacher forcing method, where the decoder is trained using the target output (ground truth output) instead of the output generated by the decoder in the previous step of the sequence. However, during the evaluation step, the decoder generates the output sequentially using the output generated in the previous step. Using teacher forcing during training has been shown to improve the training process. To determine if the training process has converged, the model was trained for 300 epochs and the validation e
%In this problem, the input $X$ is an English sentence (e.g., "nice to meet you"), and the Seq2Seq model is used to find a prediction $Y_{pred}$ that is closest to its corresponding French translation $Y_{true}$ (e.g., "ravi de vous rencontrer"). A token in this case is a single word in the sentence.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1.0\textwidth]{img/Seq2Seq_arch.png}
  \caption{Seq2Seq model for used for predicting future calibration values (\textbf{lower left}). The Encoder block (\textbf{upper left}) and Decoder block (\textbf{lower right}) are a set of sequentially connected LSTM units (\textbf{upper right}).}
  \label{fig:encoder-decoder}
\end{figure}