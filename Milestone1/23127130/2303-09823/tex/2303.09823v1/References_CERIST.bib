@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = "Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS 2017)",
  address = "Long Beach, CA, USA",
  year={2017},
  pages = {1-11},
}
@book{ravichandiran2021getting,
  title="Getting Started with Google {BERT}: Build and train state-of-the-art natural language processing models using {BERT}",
  author={Ravichandiran, Sudharsan},
  year={2021},
  publisher={Packt Publishing Ltd}
}
@article{lin2021survey,
    title = {A survey of transformers},
    journal = {AI Open},
    volume = {3},
    pages = {111-132},
    year = {2022},
    issn = {2666-6510},
    doi = {10.1016/j.aiopen.2022.10.001},
    author = {Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
    keywords = {Transformer, Self-attention, Pre-trained models, Deep learning},
    abstract = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.}
}
@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = 2019,
    address = "Minneapolis, Minnesota",
    publisher = "ACL",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}
@article{mohammed2021survey,
  title="Survey of {BERT} (Bidirectional Encoder Representation Transformer) types",
  author={Mohammed, Athar Hussein and Ali, Ali H},
  journal={Journal of Physics: Conference Series},
  volume={1963},
  number={1},
  pages={012173},
  year={2021},
  organization={IOP Publishing}
}
@software{ali_safaya_2020_4718724,
  author       = {Ali Safaya},
  title        = "Arabic-{ALBERT}",
  month        = aug,
  year         = 2020,
  publisher    = {Zenodo},
  version      = {1.0.0},
  doi          = {10.5281/zenodo.4718724},
}
@inproceedings{antoun2020arabert,
    title = "{A}ra{BERT}: Transformer-based Model for {A}rabic Language Understanding",
    author = "Antoun, Wissam  and
      Baly, Fady  and
      Hajj, Hazem",
    booktitle = "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "ELRA",
    pages = "9--15",
}
@inproceedings{antoun-etal-2021-aragpt2,
    title = "{A}ra{GPT}2: Pre-Trained Transformer for {A}rabic Language Generation",
    author = "Antoun, Wissam  and
      Baly, Fady  and
      Hajj, Hazem",
    booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
    month = apr,
    year = "2021",
    address = "Kyiv, Ukraine (Virtual)",
    publisher = "ACL",
    pages = "196--207",
}
@inproceedings{antoun-etal-2021-araelectra,
    title = "{A}ra{ELECTRA}: Pre-Training Text Discriminators for {A}rabic Language Understanding",
    author = "Antoun, Wissam  and Baly, Fady  and Hajj, Hazem",
    booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
    month = apr,
    year = "2021",
    address = "Kyiv, Ukraine (Virtual)",
    publisher = "ACL",
    pages = "191--195",
}
@inproceedings{conneau2019unsupervised,
  title="Unsupervised Cross-lingual Representation Learning at Scale",
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, {\'E}douard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8440--8451},
  year={2020},
  address = "Online",
  publisher = "ACL",
}
@inproceedings{magnossao-de-paula-etal-2022-upv,
    title = "{UPV} at the {A}rabic Hate Speech 2022 Shared Task: Offensive Language and Hate Speech Detection using Transformers and Ensemble Models",
    author = "Magnoss{\~a}o de Paula, Angel Felipe  and
      Rosso, Paolo  and
      Bensalem, Imene  and
      Zaghouani, Wajdi",
    booktitle = "Proceedinsg of the 5th Workshop on Open-Source Arabic Corpora and Processing Tools with Shared Tasks on Qur'an QA and Fine-Grained Hate Speech Detection",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "ELRA",
    pages = "181--185",
    abstract = "This paper describes our participation in the shared task Fine-Grained Hate Speech Detection on Arabic Twitter at the 5th Workshop on Open-Source Arabic Corpora and Processing Tools (OSACT). The shared task is divided into three detection subtasks: (i) Detect whether a tweet is offensive or not; (ii) Detect whether a tweet contains hate speech or not; and (iii) Detect the fine-grained type of hate speech (race, religion, ideology, disability, social class, and gender). It is an effort toward the goal of mitigating the spread of offensive language and hate speech in Arabic-written content on social media platforms. To solve the three subtasks, we employed six different transformer versions: AraBert, AraElectra, Albert-Arabic, AraGPT2, mBert, and XLM-Roberta. We experimented with models based on encoder and decoder blocks and models exclusively trained on Arabic and also on several languages. Likewise, we applied two ensemble methods: Majority vote and Highest sum. Our approach outperformed the official baseline in all the subtasks, not only considering F1-macro results but also accuracy, recall, and precision. The results suggest that the Highest sum is an excellent approach to encompassing transformer output to create an ensemble since this method offered at least top-two F1-macro values across all the experiments performed on development and test data.",
}
@inproceedings{depaula2021exist,
    title = "Sexism Prediction in Spanish and English Tweets Using Monolingual and Multilingual BERT and Ensemble Models",
    author = "Magnoss{\~a}o de Paula, Angel Felipe and da Silva, Roberto Fray and Schlicht, Ipek Baris",
    booktitle = {Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2021) co-located with the XXXVII International Conference of the Spanish Society for Natural Language Processing (SEPLN 2021), CEUR Workshop proceedings Vol. 2943},
    month = sep,
    year = "2021",
    address = "M{\'a}laga, Spain",
    publisher = {CEUR-WS.org},
    pages = "356-373",
    volme= "2943",
    abstract = "The popularity of social media has created problems such as hate speech and sexism. The identification and classification of sexism in social media are very relevant tasks, as they would allow building a healthier social environment. Nevertheless, these tasks are considerably challenging. This work proposes a system to use multilingual and monolingual BERT and data points translation and ensemble strategies for sexism identification and classification in English and Spanish. It was conducted in the context of the sEXism Identification in Social neTworks shared 2021 (EXIST 2021) task, proposed by the Iberian Languages Evaluation Forum (IberLEF). The proposed system and its main components are described, and an in-depth hyperparameters analysis is conducted. The main results observed were: (i) the system obtained better results than the baseline model (multilingual BERT); (ii) ensemble models obtained better results than monolingual models; and (iii) an ensemble model considering all individual models and the best standardized values obtained the best accuracies and F1-scores for both tasks. This work obtained first place in both tasks at EXIST, with the highest accuracies (0.780 for task 1 and 0.658 for task 2) and F1-scores (F1-binary of 0.780 for task 1 and F1-macro of 0.579 for task 2).",
}
@inproceedings{depaula2022exist,
    title = "Detection and Classification of Sexism on Social Media Using Multiple Languages, Transformers, and Ensemble Models",
    author = "Magnoss{\~a}o de Paula, Angel Felipe  and da Silva, Roberto Fray",
    booktitle = "Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2022) co-located with the XXXVIII International Conference of the Spanish Society for Natural Language Processing (SEPLN 2022), CEUR Workshop proceedings Vol. 3202",
    month = sep,
    year = "2022",
    address = "La Coru{\~n}a, Spain",
    publisher = {CEUR-WS.org},
      pages = {1-11},
    abstract = "Identifying and classifying sexist content in social media posts is a highly complex and relevant problem. Some characteristics such as sarcasm and multiple forms of sexism increase the difficulty of detecting and identifying this type of content. Nevertheless, it is essential to improve prediction quality to improve decision-making such as post removal, and user ban, among others. The main objective of this work is to propose a methodology and explore the use of different transformers architectures for two tasks in English and Spanish: sexism detection and sexism classification. Single-language and multilingual versions of the BERT, RoBERTa, and single-language versions of the Electra, and GPT2 architectures were evaluated on the EXIST 2022 shared task challenge at IberLEF 2022 dataset. It was observed that: (i) the use of the translation of the posts to English and then using single-language English and multilingual models present the best results; (ii) the best architectures were BERT and RoBERTa; (iii) using single language Spanish models provided the worst results; (iv) sexism classification was more difficult than sexism detection; and (v) the use of ensembles were better than the GPT2 and Electra models, but worse than English single-language generally and multilingual models. An in-depth hyperparameters analysis was also conducted.",
}
@inproceedings{depaula2021detoxis,
    title = "{AI-UPV} at {IberLEF}-2021 {DETOXIS} task: Toxicity Detection in Immigration-Related Web News Comments Using Transformers and Statistical Models",
    author = "Magnoss{\~a}o de Paula, Angel Felipe and da Silva, Roberto Fray and Schlicht, Ipek Baris",
    booktitle = "Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2021) co-located with the XXXVII International Conference of the Spanish Society for Natural Language Processing (SEPLN 2021), CEUR Workshop Proceedings Vol. 2943",
    month = sep,
    year = "2021",
    address = "M{\'a}laga, Spain",
    publisher = "CEUR-WS.org",
    pages = "547-566",
    volme= "",
    abstract = "This paper describes our participation in the DEtection of TOXicity in comments In Spanish (DETOXIS) shared task 2021 at the 3rd Workshop on Iberian Languages Evaluation Forum. The shared task is divided into two related classification tasks: (i) Task 1: toxicity detection and; (ii) Task 2: toxicity level detection. They focus on the xenophobic problem exacerbated by the spread of toxic comments posted in different online news articles related to immigration. One of the necessary efforts towards mitigating this problem is to detect toxicity in the comments. Our main objective was to implement an accurate model to detect xenophobia in comments about web news articles within the DETOXIS shared task 2021, based on the competition’s official metrics:the F1-score for Task 1 and the Closeness Evaluation Metric (CEM) for Task 2. To solve the tasks, we worked with two types of machine learning models: (i) statistical models and (ii) Deep Bidirectional Transformers for Language Understanding (BERT) models. We obtained our best results in both tasks using BETO, a BERT model trained on a big Spanish corpus. We obtained the 3rd place in Task 1 official ranking with the F1-score of 0.5996, and we achieved the 6th place in Task 2 official ranking with the CEM of 0.7142. Our results suggest: (i) BERT models obtain better results than statistical models for toxicity detection in text comments; (ii) Monolingual BERT models have an advantage over multilingual BERT models in toxicity detection in text comments in their pre-trained language.",
}

@Article{info13060273,
AUTHOR = {Alkomah, Fatimah and Ma, Xiaogang},
TITLE = {A Literature Review of Textual Hate Speech Detection Methods and Datasets},
JOURNAL = {Information},
VOLUME = {13},
YEAR = {2022},
NUMBER = {6},
ARTICLE-NUMBER = {273},
ISSN = {2078-2489},
DOI = {10.3390/info13060273},
publisher = {MDPI}
}



@inproceedings{Mulki2021,
author = {Mulki, Hala and Ghanem, Bilal},
booktitle = {Working Notes of FIRE 2021, CEUR Workshop proceedings Vol. 3159},
pages = {820--830},
title = {{ArMI at FIRE 2021: Overview of the First Shared Task on Arabic Misogyny Identification}},
publisher ={CEUR-WS.org},
year = {2021}
}

@article{HadjAmeur2021,
author = {{Hadj Ameur}, Mohamed Seghir and Aliane, Hassina},
doi = {10.1016/j.procs.2021.05.086},
issn = {22128271},
journal = {Procedia Computer Science, Part of special issue: AI in Computational Linguistics},
pages = {232--241},
title = {{AraCOVID19-MFH: Arabic COVID-19 Multi-label Fake News \& Hate Speech Detection Dataset}},
publisher = {Elsevier},
volume = {189},
year = {2021}
}


@article{Husain2021,
author = {Husain, Fatemah and Uzuner, Ozlem},
doi = {10.1145/3421504},
issn = {23754702},
journal = {ACM Transactions on Asian and Low-Resource Language Information Processing},
number = {1},
pages = {1--44},
publisher = {ACM},
title = {{A Survey of Offensive Language Detection for the Arabic Language}},
volume = {20},
year = {2021}
}

@inproceedings{Mahdaouy2021,
abstract = {The prevalence of toxic content on social media platforms, such as hate speech, offensive language, and misogyny, presents serious challenges to our interconnected society. These challenging issues have attracted widespread attention in Natural Language Processing (NLP) community. In this paper, we present the submitted systems to the first Arabic Misogyny Identification shared task. We investigate three multi-task learning models as well as their single-task counterparts. In order to encode the input text, our models rely on the pre-trained MARBERT language model. The overall obtained results show that all our submitted models have achieved the best performances (top three ranked submissions) in both misogyny identification and categorization tasks.},
archivePrefix = {arXiv},
arxivId = {2206.08407v1},
author = {Mahdaouy, Abdelkader El and Mekki, Abdellah El and Oumar, Ahmed and Mousannif, Hajar and Berrada, Ismail},
booktitle = {Working Notes of FIRE 2021, CEUR Workshop proceedings Vol. 3159},
pages = {852--860},
publisher = {CEUR-WS.org},
title = {{Deep Multi-Task Models for Misogyny Identification and Categorization on Arabic Social Media}},
year = {2021}
}
@article{Mubarak2022Emoj,
archivePrefix = {arXiv},
arxivId = {2201.06723},
author = {Mubarak, Hamdy and Hassan, Sabit and Chowdhury, Shammur Absar},
eprint = {2201.06723},
journal = {arXiv preprint arXiv:2201.06723},
pages = {1--21},
title = {{Emojis as Anchors to Detect Arabic Offensive Language and Hate Speech}},
year = {2022}
}
@inproceedings{Hassan2020,
author = {Hassan, Sabit and Samih, Younes and Mubarak, Hamdy and Abdelali, Ahmed and Rashed, Ammar and Chowdhury, Shammur Absar},
booktitle = {Proceedings of the OSACT 4 Workshop @LREC 2020},
pages = {61--65},
title = {{ALT Submission for OSACT Shared Task on Offensive Language Detection}},
publisher = {ELRA},
year = {2020}
}
@inproceedings{Mubarak2022OSACT5,

address = {Marseille, France},
author = {Mubarak, Hamdy and Al-Khalifa, Hend and Al-Thubaity, Abdulmohsen},
booktitle = {Proceedings of the OSACT 2022 Workshop @LREC2022},
pages = {162--166},
publisher = {ELRA},
title = {{Overview of OSACT5 Shared Task on Arabic Offensive Language and Hate Speech Detection}},
year = {2022}
}
@inproceedings{BenNessir2022,
address = {Marseille},
author = {{Ben Nessir}, Mohamed Aziz and Rhouma, Malek and Haddad, Hatem and Fourati, Chayma},
booktitle = {Proceedings ofthe OSACT 2022 Workshop @LREC2022},
file = {:F\:/Imene_doc/Mendeley Desktop/Ben Nessir et al. - 2022 - iCompass at Arabic Hate Speech 2022 Detect Hate Speech Using QRNN and Transformers.pdf:pdf},
keywords = {arabic,hate speech detection,marbert,multi-task,qrnn},
mendeley-groups = {CERIST paper},
number = {June},
pages = {176--180},
publisher = {ELRA},
title = {{iCompass at Arabic Hate Speech 2022 : Detect Hate Speech Using QRNN and Transformers}},
year = {2022}
}
@inproceedings{Mubarak2020,
address = {Marseille},
author = {Mubarak, Hamdy and Darwish, Kareem and Magdy, Walid and Al-Khalifa, Hend},
booktitle = {Proceedings of the OSACT 2020 Workshop @LREC 2020},
file = {:F\:/Imene_doc/Mendeley Desktop/Mubarak et al. - 2020 - Overview of OSACT4 Arabic Offensive Language Detection Shared Task.pdf:pdf},
keywords = {Arabic Hate Speech,Arabic Offensive Language,CodaLab,OSACT,Shared Task},
mendeley-groups = {Offensive language detection,Offensive language detection/Ar datasets,CERIST paper},
pages = {48--52},
publisher = {ELRA},
title = {{Overview of OSACT4 Arabic Offensive Language Detection Shared Task}},
year = {2020},
abstract = {This paper provides an overview of the offensive language detection shared task at the 4th workshop on Open-Source Arabic Corpora and Processing Tools (OSACT4). There were two subtasks, namely: Subtask A, involving the detection of offensive language, which contains unacceptable or vulgar content in addition to any kind of explicit or implicit insults or attacks against individuals or groups; and Subtask B, involving the detection of hate speech, which contains insults or threats targeting a group based on their nationality, ethnicity, race, gender, political or sport affiliation, religious belief, or other common characteristics. In total, 40 teams signed up to participate in Subtask A, and 14 of them submitted test runs. For Subtask B, 33 teams signed up to participate and 13 of them submitted runs. We present and analyze all submissions in this paper.}
}

@inproceedings{Zampieri2020,
address = {Barcelona (online)},
author = {Zampieri, Marcos and Nakov, Preslav and Rosenthal, Sara and Atanasova, Pepa and Karadzhov, Georgi and Mubarak, Hamdy and Derczynski, Leon and Pitenis, Zeses and {\c{C}}{\"{o}}ltekin, {\c{C}}ağrı},
booktitle = {Proceedings of the Fourteenth Workshop on Semantic Evaluation},
pages = {1425--1447},
publisher = {International Committee for Computational},
title = {{SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (OffensEval 2020)}},
year = {2020}
}


