%auto-ignore
\onecolumn
\section*{\centering{\LARGE{Appendix}}}

 
 
 Experiments implementations are available at \url{https://github.com/zizhao-hu/BVAE}

\section*{1.Latent Visualizations}
\subsection*{1.1 Pair-wise 4-D Latent Visualizations in 2-D}
Visualization for the latent space(dimension = 4) of VAEs and the binarized versions. Symmetry is shown in the binarized VAEs and posterior collapse does not appear in binarized disentangling $\beta$-VAE. We use MNIST training dataset.
\begin{figure*}[h]
\includegraphics[width=\linewidth]{sup_material/4_latent_visual.png}
    \caption{Pair-wise latent space visualization }
    \label{fig:4_latent_vis}
\end{figure*}
In row 1, the binarized latent space is more symmetric. The correlation between two latent dimensions is also reduced. The data points are spread out in a larger area without requiring additional training resources.

In row 2, the binarized latent space does not suffer from posterior collapse. 
\section*{2. UMAP 16-D Latent Visualizations in 2-D}
Four models(VAE, $\beta$-VAE,  disentangling  $\beta$-VAE,  GMVAE) with four binarized versions. Column one denotes the original models corresponding to binarizing hyperparameter $r=0$. Column $2,3,4$ correspond to $r=0.1$,$r=1$ and $r=2$. We use MNIST training dataset. The best disentanglement of classes happens at $r = 1$ for all unimodal Gaussian prior models. $r = 0.1$ for Gaussian mixture prior.
\begin{figure*}[h]
    \includegraphics[width=\linewidth]{sup_material/umap_clustering.png}
    \caption{UMAP visualization of the latent manifolds in 2-D.}
    \label{fig:my_label1}
\end{figure*}
In rows 1, 2, and 3, the best disentanglement happens at $r=1$. When $r=2$, the latent is over-disentangled.

In row 4, the binarized model detaches the border between classes, and achieves better disentanglement. 

\section*{3. Visualization of Different Hyperparameters}
\subsection*{3.1 Reconstruction}
We test $r=0,0.01,0.1,1$ settings on the CelebA dataset. Reconstruction loss is reduced for binarized models $r=0.01,0.1$. The running time is also slightly reduced(Table \ref{tab:recon_time}).


\begin{table}[H]
\centering
\begin{tabular}{|l|ll|}
\hline
              & Reconstruction loss & Running time(Average of 50 epochs) \\ \hline
r=0(original) & 0.0137              & 115.26                             \\
r=0.01        & \textbf{0.0128}     & 115.36                             \\
r=0.1         & \textbf{0.0128}     & \textbf{113.32}                    \\
r=1           & 0.0160              & 116.8                              \\ \hline
\end{tabular}
\caption{Reconstruction quality and running time}
\label{tab:recon_time}
\end{table}

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{sup_material/r=0recon.png}
    \centering
    \\r=0(original)
    \label{fig:my_label2}
\end{minipage}%
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{sup_material/r=0.01recon.png}
    \centering
    \\{r=0.01}
    \label{fig:my_label3}
\end{minipage}
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{sup_material/r=0.1recon.png}
    \centering
    \\{r=0.1}
    \label{fig:my_label4}
\end{minipage}%
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{sup_material/r=1rcon.png}
    \centering
    \\{r=1}
    \label{fig:my_label5}
\end{minipage}
 \caption{Reconstruction with different r settings}
\end{figure}
\subsection*{3.2 Sampling}
Sampling is conducted on the same models as in the last section. We use the same latent space vector to generate the image in all four models. We can see that when $r$ is small, the samples have more distinct facial features, sharper edges, and  vibrant colors. When $r$ is large, the samples sampled from the original prior are from the ``ambiguous region", since the sampling prior is between the binarized centers. And the generated images are ambiguous and lack distinction as expected. 
\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{sup_material/r=0sample.png}
    \centering
    \\r=0(original)
    \label{fig:my_label6}
\end{minipage}%
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{sup_material/r=0.01sample.png}
    \centering
    \\{r=0.01}
    \label{fig:my_label7}
\end{minipage}
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{sup_material/r=0.1sample.png}
    \centering
    \\{r=0.1}
    \label{fig:my_label8}
\end{minipage}%
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{sup_material/r=1sample.png}
    \centering
    \\{r=1}
    \label{fig:my_label9}
\end{minipage}
 \caption{Reconstruction with different r settings}
\end{figure}
\section*{4. Reduced-bias Sampling for High-variance Posteriors}
\subsection*{4.1 Binarized approximate posterior}
Optimal approximate posterior estimation for VAE with a Gaussian prior $ p_{\phi}(z|x)\sim \mathcal{N}(\mu,\sigma^2)$, assuming some error $\epsilon_\mu$ and $\epsilon_\sigma$
\begin{equation}
    p_{\phi}(z|x)\sim \mathcal{N}(\mu+\epsilon_\mu,(\sigma+\epsilon_\sigma)^2) = \frac{1}{(\sigma+\epsilon_\sigma)\sqrt{2\pi}} 
  \exp\left( -\frac{1}{2}\left(\frac{x-\mu-\epsilon_\mu}{(\sigma+\epsilon_\sigma)}\right)^{\!2}\,\right)
\end{equation}
For binarized VAE: 
\begin{equation}
p_{\phi}(z|x)_{Binarized}\sim \mathcal{N}(|\mu+\epsilon_\sigma|-r,(\sigma+\epsilon_\sigma)^2) = \frac{1}{(\sigma+\epsilon_\sigma)\sqrt{2\pi}} 
  \exp\left( -\frac{1}{2}\frac{(|x-\mu-\epsilon_\mu|-r)^2}{(\sigma+\epsilon_\sigma)^2}\,\right)
  \end{equation}
Difference:
\begin{equation}
 p_{\phi}(z|x) - p_{\phi}(z|x)_{Binarized} = \frac{1}{(\sigma+\epsilon_\sigma)\sqrt{2\pi}} 
  \left(\exp\left( -\frac{1}{2}\frac{(x-\mu-\epsilon_\mu)^2}{(\sigma+\epsilon_\sigma)^2}\,\right)-\exp\left( -\frac{1}{2}\frac{(x-\mu-\epsilon_\mu)^2-2r|x-\mu-\epsilon_\mu|+r^2}{(\sigma+\epsilon_\sigma)^2}\,\right) \right)
\end{equation}
When $x\approx\mu$ we have $(x-\mu-\epsilon_\mu)^2 \approx {\epsilon_\mu}^2<<(\sigma+\epsilon_\sigma)^2$, when $\sigma$ is sufficiently large, then approximate by first-order Taylor expansion:

\begin{equation}
\begin{aligned}
 p_{\phi}(z|x) - p_{\phi}(z|x)_{Binarized}&\approx \frac{1}{(\sigma+\epsilon_\sigma)\sqrt{2\pi}} \left(1 -\frac{1}{2}\frac{(x-\mu-\epsilon_\mu)^2}{(\sigma+\epsilon_\sigma)^2} -1 + \frac{1}{2}\frac{(x-\mu-\epsilon_\mu)^2-2r|x-\mu-\epsilon_\mu|+r^2}{(\sigma+\epsilon_\sigma)^2}\,\right)\\[1em]
&= \frac{-2r|x-\mu-\epsilon_\mu|+r^2}{2(\sigma+\epsilon_\sigma)^3\sqrt{2\pi}}
 \end{aligned}
\end{equation}
Thus, we have
\begin{equation}
  p_{\phi}(z|x) > p_{\phi}(z|x)_{Binarized} \; \text{when} \; |x-\mu-\epsilon_\mu|<\frac{r}{2}\; , \; x\approx\mu \; ,\text{and} \; \sigma \; \text{is sufficiently large}
\end{equation}
This allows the binarized approximate posterior to be less distributed around the mean. When sampled using the same prior distribution, reduced bias is thus achieved. (Also see Figure \ref{fig:re_bias}. The red curve denotes high-variance posterior. Column 1 is a scenario when the estimated posterior has a larger or smaller variance than the prior. Column 2 is the difference between the posteriors and the prior. Columns 3, and 4 are binarized models.)
\begin{figure}[h]
    \centering
    \includegraphics[width = 0.76\linewidth]{sup_material/re_bias.png}
    \caption{Estimation and sampling distributions}
    \label{fig:re_bias}
\end{figure}
