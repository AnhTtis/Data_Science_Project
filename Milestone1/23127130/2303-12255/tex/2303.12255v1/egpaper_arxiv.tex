\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[numbers]{natbib}
\usepackage{multirow}
\usepackage{tablefootnote}
\usepackage{adjustbox}
\usepackage[table,xcdraw]{xcolor}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
 

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{ Encoding Binary Concepts in the Latent Space of Generative Models for Enhancing Data Representation}

\author{Zizhao Hu\\
University of Southern California\\
{\tt\small zizhaoh@usc.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Mohammad Rostami\\
University of Southern California\\
{\tt\small rostamim@usc.edu}
}

\maketitle
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
    Binary concepts\footnote{``Concepts" are also known as ``factors'' in unsupervised learning literature. We use these terms interchangeably.} are empirically used by humans to generalize efficiently. And they are based on Bernoulli distribution which is the building block of information. These concepts span both low-level and high-level features such as  ``large vs small" and ``a neuron is active or inactive''. Binary concepts are ubiquitous features and can be used to transfer knowledge to improve model generalization. We propose a novel binarized regularization to facilitate learning of binary concepts to improve the quality of data generation in autoencoders. We introduce a binarizing hyperparameter $r$ in data generation process to disentangle the latent space symmetrically. We demonstrate that this method can be applied easily to existing variational autoencoder (VAE) variants to encourage symmetric disentanglement, improve reconstruction quality, and prevent posterior collapse without computation overhead. We also demonstrate that this method can boost existing models to learn more transferable representations and generate more representative samples for the input distribution which can alleviate catastrophic forgetting using generative replay under continual learning settings.

 
    
\end{abstract}

\section{Introduction}

There is a famous game named Akinator. It asks you a series of binary answer questions such as ``is he young or old?'' or ``is she an actress'' to find out what subject you are thinking of based on your answers. A recent study \cite{akinator} suggests humans use a similar strategy to ask questions with binary answers to maximize information gain. This implies that humans use binary concepts to efficiently encode information internally to discriminate between concepts. In reinforcement learning, there is evidence \cite{GalilAI} that an agent can generalize low-level physical properties by sequentially binarizing observations. The above evidence serves as an inspiration to benefit from binarized concepts to learn latent space representations for an input distribution such that they disentangle concepts in a binary scheme to potentially facilitate   learning   all factors. We especially apply our approach on VAEs since they learn   stable latent representations.

\begin{figure} 
    \centering
    \includegraphics[width=0.47\textwidth]{ images/bvae_concept.png}
    \caption{Two binary concepts represented by a mixture of two symmetric Gaussians in each dimension of a latent space with dimension $d=2$. It has $2^d$ global density maxima.}
    \label{fig:mesh1}
\end{figure} 

Most existing variants of VAEs focus on regularizing the latent space to learn more disentangled factors to boost efficiency and the quality of generated data. Considering that (i) low-level information-dense factors are difficult to disentangle, and are often treated as noise (posterior collapse) and (ii) high-level information-scarce factors are easy to disentangle, they usually lead to learning low-variance representations. As a result, generated samples are highly biased toward the mean of the learned distribution in the latent space and hence are less diverse compared to the real-world data.
To our knowledge,  existing methods are not exploring the possibility of learning an inherent property shared by all levels of factors in real-world data.  We argue that binary concepts in the latent representation are a suitable candidate for this purpose (see Figure~\ref{fig:mesh1}). % and encoding it into latent space to facilitate the learning of both kinds. And binary concept is such a candidate property. 

We introduce a general regularization technique to improve VAEs. We  call it   ``binarized" regularization  because it enforces learning latent distributions that approximate the posterior using two symmetric modalities in each dimension of the latent space (see Figure \ref{fig:mesh1}). As a result, the latent distribution mass deterministically moves towards one of the closest prior distributions on a high-dimensional grid of a mixture distribution that allows generating diverse samples (note Figure \ref{fig:mesh1} is an example for a 2D  latent embedding space).
We demonstrate that our regularization technique improves disentanglement of the latent and simultaneously improves reconstruction quality through extensive experiments.
Our specific contributions include:

\begin{itemize}
    
\item  We develop a   regularization technique to binarize the VAE objective and demonstrate that binarized VAEs are able to further disentangle the latent, resolve noisy low-level factors in generated images, and can generate significantly more variant high-level features.

\item  We demonstrated that our method can prevent posterior collapsing by restoring the collapsed latent representation to learn low-level factors. 

\item  We further adopt our idea to address catastrophic forgetting in continual learning. Results demonstrate that our method is able to generate more diverse samples for the past tasks and compares favorably against state-of-the-art generative replay methods on the most challenging incremental-class classification tasks.
\end{itemize}


\section{Background and Related Work}


% MR: This is too much for a conference paper. This section should be 1/2 to 3/4 page in total. We will have two paragraphs. One on VAE and another on CL. Condense the VAE methods to at most 3/4 a column so that the rest can be used for CL. 

\paragraph{Variational Autoencoders} have an extremely rich literature. We propose a unified framework to formulate all current VAE-based methods in Figure~\ref{fig:vae_pipe}. We argue that a Gaussian Mixture Model (GMM) can be used to model any complicatedly-distributed latent concepts using a combination of an infinite number of simply-distributed high-level   to low-level concepts. These concepts continuously span from high-variance to low-variance. The gaps in Figure~\ref{fig:vae_pipe} represent how close the distributions are between
 % I have difficulty to understand what prior and posterior are referring to. I guess you are talking about the latent distribution but what is the different between posterior and true posterior?
\textbf{Gap 1:} prior and true posterior\footnote{the optimal mapping from data to a regularized distribution}, \textbf{Gap 2 (L/R):} approximate posterior and true posterior, and \textbf{Gap 3 (L/R):} approximate posterior and prior. A smaller Gap 2 leads to a better reconstruction quality. A smaller Gap 3 leads to better disentanglement. The goal of the VAE optimizations can be generalized as reducing Gap 3 (preferably without reducing Gap 2). Within this formulation, the prominent VAE variants can be categorized by the way they achieve this goal:
  


\textbf{(i) Reducing the size of Gap 1:} 
These methods indirectly reduce both Gaps 2 and 3 on certain types of true posteriors.
\textbf{Gaussian VAE} \cite{vae} uses a unimodal Gaussian distribution as the prior. It can serve as a good approximation for many true posteriors but the generated samples are biased towards its mean and may not be very diverse. 
\textbf{Hyperspherical VAE (NVAE)}  \cite{svae} uses von Mises-Fisher (vMF)   prior that is closer to some specific true posteriors.

\textbf{(ii) Reducing the size of Gap 3 by trading-off the size of Gap 2:}
\textbf{$\beta$-VAE} \cite{bvae} uses a hyperparameter $\beta$ to do so.

\textbf{(iii) Reducing the size of Gap 3 with adaptable priors:}
\textbf{GMVAE} \cite{gmvae} creates a finite set of candidate priors.
\textbf{disentangling $\beta$-VAE}\cite{dbvae} creates an infinite set of candidate priors on an ``equal KL-divergence'' line (see Figure \ref{fig:priors} row 2 column 1 all dashed curves for clarity). 

\textbf{(iv) Reducing the size of Gap 3 by adapting the prior to approximate the posterior:}
\textbf{VAMP} \cite{vamp} has a mixture prior, and uses aggregate posterior\footnote{The observed latent distribution on aggregate seen data.} to move prior towards the approximated posterior during training.
\textbf{$\beta$-TCVAE} \cite{btcvae} adds a regularization term to optimize the prior towards the posterior during training.
\textbf{DIP-VAE} \cite{dipvae} uses the concept of ``moment'' to predict the future posterior, and uses this extra information to move the prior towards the approximate posterior. \textbf{FactorVAE}  \cite{factorvae} adds a regularization term to factorize prior to adapt to low-variance posterior.


\begin{figure}
    \centering
    \includegraphics[width =8cm]{ images/VAE_pipe.png}
    \caption{Latent optimization process. We only show two out of infinite true posteriors here to simplify the representation.}
    \label{fig:vae_pipe}
\end{figure}

\textbf{(v) Reducing the size of Gap 2R and Gap 3R with a low-variance local prior:}
To achieve this goal without increasing Gap 2L and 3L, having many low-variance approximate posteriors is necessary to approximate the high-variance distribution. These posteriors result in an inaccurate approximation of hte high-level factors when a model is small. The extreme case is \textbf{Autoencoders} \cite{ae}.
\textbf{WAE} \cite{wae} only maintains prior-like global approximate posteriors, but reduces the variance of local posteriors.
\textbf{VQVAE} \cite{vqvae} nears the extreme with a no-variance quantized latent prior. 

\textbf{(vi) Reducing the size of Gap 2 and Gap 3 with larger models:}
A larger model can reduce Gap 2 and Gap 3 by providing an approximate posterior with a larger variance range.   \textbf{VDVAE}\cite{vdvae} is an example in this category.


\textbf{(vii) Reducing the size of Gap 2 with a less regularized likelihood prior:}
This approach is not within the VAE literature   since it does not reduce Gap 3, but it is worth mentioning it due to its compatibility with this framework. The reconstruction prior used in VAEs is local low-variance Bournourli distribution. This requirement can be relaxed to more global distributions seen in \textbf{GAN} \cite{gan}, or distributed throughout the full model as in \textbf{diffusion models} \cite{ddpm}. 

Our   method employs   ideas of (i) and (iii). When used on a unimodal Gaussian VAE, it is a GMVAE with $2^d$ components, with deterministic  assignment conditioned on the approximate posterior, and unimodal Gaussian reparameterization to distributed the mass in the latent embedding.

 \paragraph{Continual Learning} aims to learn a set of sequentially arriving tasks using a shared model. When the model is updated to learn the current task,  we observe catastrophic forgetting~\cite{kirkpatrick2017overcoming} in the model due to deviation from the optimal parameters for the past learned tasks. 
CL algorithms employ three primary strategies to mitigate catastrophic forgetting. One approach involves regularizing a fixed shared model, such that different information pathways are used to learn each task's weights~\cite{kirkpatrick2017overcoming,aljundi2018memory,jin2021learn}. The aim is to identify critical model parameters that encode the learned knowledge of each task and consolidate these parameters while updating the model to learn new tasks. The downside is that the learning capacity of the model is compromised as more weights are consolidated. Another approach relies on model expansion~\cite{rusu2016progressive,yoonlifelong}, which involves adding new weights to a base model and customizing the network to learn new tasks via these additional weights. The downside is that the model can grow unboundedly as more tasks are learned. Finally, most algorithms employ pseudo-rehearsal through experience replay~\cite{rolnick2019experience,rostami2021detection,rostami2021lifelong}. This method involves storing a representative subset of training data for each task in a memory buffer and replaying those samples along with the current task's data to maintain encoded knowledge about previously learned. A group of algorithms relaxes the need for a memory buffer by building a generative model that learns to generate pseudo-samples that are very similar to the real samples of the past learned tasks. We demonstrate that our approach helps to improve this category of CL methods.


\section{Problem Description}
\subsection{Variational Autoencoders Objective}

Given a dataset $\mathcal{D} = \{x_i\}_{i=1}^n$,  VAEs learn an encoder $p_{\phi}(z|x)$ and a decoder $p_{\theta}(x|z)$ that maximize the likelihood $p_{\phi,\theta}(x)$ to approximate the empirical evidence $p_{\mathcal{D}}(x)$. The encoder   maps   $p_{\mathcal{D}}(x)$ to a simple latent prior $p(z)$ which has better sampling properties. The objective is to maximize:
\begin{align*}
% \ln p_{\phi,\theta}(x) &= \ln \int_z p_{\phi,\theta}(x,z) \\
% &=\ln \int_z p_{\theta}(x|z) p(z) \frac{p_{\phi}(z|x)}{q_{\phi}(z|x)}\; \text{(Bayes')}\\
% &\geq \mathbb{E}_{p_{\phi}(z|x)}[\ln \frac{p_{\theta}(x|z) p(z)}{q_{\phi}(z|x)}] \; \text{(Jensen's Inequality)}\\
% &= \mathbb{E}_{q_{\phi}(z|x)}[\ln p_{\theta}(x|z)]+ \mathbb{E}_{q_{\phi}(z|x)}[ \ln \frac{p(z)}{q_{\phi}(z|x)}] \\
% &= \mathbb{E}_{q_{\phi}(z|x)}[\ln p_{\theta}(x|z)]+ \int_z q_{\phi}(z|x) \ln \frac{p(z)}{q_{\phi}(z|x)} \\
\ln p_{\phi,\theta}(x)  \geq \mathbb{E}_{p_{\phi}(z|x)}[\ln p_{\theta}(x|z)] - D_{KL} [p_{\phi}(z|x) || p(z)],
\end{align*}
where the right hand side is ELBO, served as the proxy objective we maximize to learn the latent distribution or   as the objective in the following minimization problem:
\begin{equation}
\mathcal{L} = \underbrace{-\mathbb{E}_{p_{\phi}(z|x)}[\ln p_{\theta}(x|z)]}_\text{Reconstruction Loss} + \underbrace{D_{KL} [p_{\phi}(z|x) || p(z)]}_\text{KL Loss},
\label{eq1}
\end{equation}
where the first term is the negative likelihood of data after passing through the model. It is   modeled as the reconstruction loss in VAEs. The second term is the KL-divergence between the approximate posterior and the latent prior.

\subsection{Shortcoming of VAEs}
The two terms in Eq.~\eqref{eq1} conflict with each other. The first term trades disentanglement for reconstruction quality, emphasizing it results in low interpretability and control of latent representation. The second term trades reconstruction quality for disentanglement, emphasizing that it results in   low variances of high-level concepts that lead to a high correlation with latent dimensions and high noise levels of low-level concepts that lead to posterior collapse. We develop a method to to mitigate these challenges.

\section{Proposed Solution}

\subsection{Binarized Modification}

We define a new binarized objective that instead of minimizing the distributional distance between the approximate posterior and the prior minimizes the closest distributional distance between the approximate posterior and two binarized priors which are located $r$ away from the mean of the prior. The new binarized loss   can be written as:

\begin{equation}
\begin{aligned}  
\mathcal{L}_\mathcal{B} = &-\mathbb{E}_{p_{\phi}(z|x)}[\ln p_{\theta}(x|z)] \\&+\mathbf{1}_{\mu_{p_{\phi}(z|x)}>\mu_{p(z)}} D [p_{\phi}(z|x) || p(z-r)]\\
&+ \mathbf{1}_{\mu_{p_{\phi}(z|x)}<\mu_{p(z)}} D [p_{\phi}(z|x) || p(z+r)],
\end{aligned}
\end{equation}
where $r \in [0, +\infty]$ is a model hyperparameter. It decides how disentangled the two candidate distributions are (see Figure~\ref{fig:mesh1}). Note that the loss reduces to the original VAE loss when $r = 0$. A more general form can be written as: 
\begin{equation}
\begin{aligned}  
\mathcal{L}_\mathcal{B} = \mathcal{L}(|\mu_{p(z)}-\mu_{p_{\phi}(z|x)}|-r),
\end{aligned}
\end{equation}
for any loss function $\mathcal{L}(\mu_{p(z)}-\mu_{p_{\phi}(z|x)})$, where $\mu_{p(z)}$ is a location variable of a prior distribution, and $\mu_{p_{\phi}(z|x)}$ is a location variable of an approximate posterior distribution.

\subsection{Binarized Gaussian Prior}
\textbf{Gaussian Prior Closed-form Loss}

To add the binarized modification to a unimodal Gaussian prior $p(z)\sim \mathcal{N}(\mu,\sigma^2)$, we have the closed form: 
\begin{equation}
\begin{aligned}   
\mathcal{L}_\mathcal{B} = &-\mathbb{E}_{p_{\phi}(z|x)}[\ln p_{\theta}(x|z)] - \frac{1}{2}\\
&+\ln(\frac{\sigma_{p_{\phi}(z|x)}}{\sigma_{p(z)}}) + \frac{\sigma_{{p_{\phi}(z|x)}}^2 + (|\mu_{p_{\phi}(z|x)} - \mu_{p(z)}| - r)^2}{2\sigma_{p(z)}^2} 
\end{aligned}
\label{eq2}
\end{equation}

 We can show that this is equivalent to ELBO with a binarized $z$-dependent prior as the following:
 
\begin{equation} \label{eq22}
p_B (z)= 
\begin{cases}
   \mathcal{N}(\mu_{p(z)}+r,\, \sigma_{p(z)}^2),
   & \text{if } \mu_{p_{\phi}(z|x)} - \mu_{p(z)}\geq 0\\\\
   \mathcal{N}(\mu_{p(z)}-r,\, \sigma_{p(z)}^2), &\text{otherwise}
\end{cases}
\end{equation}

This equivalent prior can be visualized as in Figure~\ref{fig:priors}. For more details on derivation please refer to the Appendix.

\begin{figure}
    \includegraphics[width=0.98\linewidth]{images/priors.png}
    \caption{Equivalent priors for binarized models.}
    \label{fig:priors}
\end{figure}

\subsection{Reparametrization}
We still use the original reparametrization trick since we still consider the original prior once the estimated posterior is decided. This assumption maintains the continuous property of the approximate posterior at the prior mean which is broken by the deterministic assignment. The probabilistic reparametrization and the deterministic assignment create an instability close to the prior mean, which can alleviate posterior collapse. The source of the instability is the opposing force provided by the Bernoulli-distribution-like encoding. It will have   maximum information capacity when posterior collapse happens which discourages collapse.

\subsection{Sampling}

The instability at the prior mean will force the latent to deviate from the high probability density center. This allows more samples from the underrepresented outer region of high-variance factors, and fewer samples from the biased center high-density region, achieving reduced-bias sampling. Although this will negatively impact low-variance factors, in practice, the combined effect achieves better total sampling quality. This might be due to the alleviated posterior collapse compensating for this negative impact.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{ images/celeba-recon-4models.png}
    \caption{Reconstruction visualization}
    \label{fig:recon4model}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=7cm, height=4.375cm]{ images/valid_recon_loss_1.png}
    \caption{Validation reconstruction loss. 
  With ``B-" meaning binarized, ``(D)BETA" meaning (disentangling)$\beta$-VAE, ``r",``b",``C" are binarizing, $\beta$-VAE, and disentangling $\beta$-VAE hyperparameters $r$, $\beta(\gamma)$, and $C$, followed by the values.}
    \label{fig:curve4model}
\end{figure}

\textbf{Reduced-bias Sampling:}
For   the two binarized distributions, we assign an equal probability to sample from:
\[P(z_{\text{sample}}) = \frac{1}{2}\mathcal{N}(\mu_{p(z)}+r,\, \sigma_{p(z)}^2)\\ + \frac{1}{2} \mathcal{N}(\mu_{p(z)}-r,\, \sigma_{p(z)}^2)
\]
which is approximately sampling from the   Gaussian:
\[\mathcal{N}(\mu_{p(z)},\, \sigma_{p(z)}^2)\]
when $r$ is small.  In all of our experiments, this sampling method is used due to its simplicity and nice properties.

\textbf{Biased Sampling:}
If we want to sample from the   distribution when the input dataset is imbalanced,
we restore the distribution of the dataset by conditioning the sampling prior on the latent statistics of the full dataset:
\begin{equation*}
\begin{aligned}
p(z> \mu_{p(z)}|X) \mathcal{N}(\mu_{p(z)}+r,\, \sigma_{p(z)}^2) \\
+ p(z<\mu_{p(z)}|X) \mathcal{N}(\mu_{p(z)}-r,\, \sigma_{p(z)}^2)
\end{aligned}
\end{equation*}
This sampling would for a skewed distribution.

\section{Empirical Results}
\textbf{Experimental set-up:}
we used four types of  VAE  models: 
\textbf{toyVAE} is a feed-forward VAE, used to test theories and conduct ablation tests on the MNIST dataset. 
\textbf{ConvVAE} is a medium-size convolutional VAE, used for analytical experiments on MNIST and CelebA(32x32, 64x64) datasets.
\textbf{L-ConvVAE} is a large-size convolutional VAE, used for qualitative visualization experiments on CelebA( 64x64) dataset. 
\textbf{Cl-VAE} is used for continual learning on splitMNIST, permutedMnist, and CIFAR100 datasets. All comparisons   have identical model architecture, training settings, and data preprocessing. For   details and implementation,   refer to supplementary material.

\subsection{Reconstruction and Unsupervised Clustering}

First, we use toyVAE as the base model to compare the performance of binarized VAE with VAE on the MNIST dataset. Reconstruction is evaluated using image-sum pixel-wise binary cross-entropy (BCE). Unsupervised clustering is evaluated by normalized mutual information (NMI). The result is presented in Table \ref{table:MNIST}. We observe that binarized VAE is able to achieve better reconstruction and clustering performances with different latent emebdding sizes. 

\input{mnist_table.tex}

Next, we scale the model to ConvVAE. We test VAE, $\beta$-VAE, disentangling $\beta$-VAE, and their binarized modifications (6 models) on the CelebA dataset (32x32 face-aligned). We again observe that the binarized models showed lower reconstruction loss (Figures \ref{fig:recon4model} and \ref{fig:curve4model}), lower variational loss, and higher ELBO compared to the base models.

\subsection{Analyzing Reconstruction and Sampling}
We then further scale up the base model to L-ConvVAE. We test VAE, $\beta$-VAE, disentangling $\beta$-VAE, DIP-VAE, WAE and their binarized modifications on the CelebA (64x64 face-aligned) dataset, and visualize the reconstruction and sampling results in Figure \ref{fig:recon_samples}. We use the same sampling latent space between the original and binarized models.

\begin{figure}[ht]
\includegraphics[width=0.48\textwidth]{images/recon-samples.png}
    \caption{Reconstruction and sampling visualization. Binarized models lead to more variant and realistic samples.}
    \label{fig:recon_samples}
\end{figure}

For reconstruction, the binarized versions have sharper edges and more detailed facial features in general and are more accurate in reconstructing those underrepresented categories in CelebA dataset such as old, facial hair, wearing a cap, etc. (Figure \ref{fig:binary_concepts}).
\begin{figure}[h]
    \centering
\includegraphics[width=0.4\textwidth]{ images/binary_concepts.png}
    \caption{Reconstruction of samples with underrepresented or ambiguous properties}
\label{fig:binary_concepts}
\end{figure}
For sampling, we observe that low-level properties are similar between the samples, while high-level properties are well-distinctive and more variant in their binarized versions. This observation aligns well with our hypothesis. An interesting observation is that when binarizing WAE, the binarized model is able to generate samples with significantly more realistic facial features. %The reason behind this needs further investigation.   

 
\subsection{Resolving Posterior Collapsing}
We use ConvVAE as the base model to visualize the latent space for MNIST and CelebA(32x32, 64x64 face-aligned) reduced-size(60000 instances) datasets. We tested VAE, $\beta$-VAE, disentangling $\beta$-VAE, and the binarized models. All the base models suffer from posterior collapse due to the adjusted smaller dataset compared to the large capacity of the ConvVAE decoder. We see that the binarized models are able to maintain the collapsed latent dimensions (see Figures \ref{fig:restore_var} and \ref{fig:lowvar}), by introducing instability at the prior mean.
concept  (see Figure \ref{fig:restore_var}).

\begin{figure}[h]
    \centering
\includegraphics[width=0.45\textwidth]{ images/lvar_restore.png}
    \caption{ \small Binarized modification reused  collapsed latent dimensions. Shown are 2 out of 16 latent spaces in disentangling $\beta$-VAE. Row 1 is CelebA(32x32 face-aligned) and Row 2 is  MNIST.}
    \label{fig:restore_var}
\end{figure}
Then we look at what exact concepts are learned in the high-variance and low-variance latent dimensions. We see more variations of low-level binary concepts are learned by the binarized models in the high-variance dimensions  (see Figure \ref{fig:highvar}).
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{ images/iter_latent_highvar.png}
    \caption{Top-2 variance latent interpolation. The binarized models learn high variance, efficient binary concepts}
    \label{fig:highvar}
\end{figure}
While high-level concepts are learned in the low-variance dimensions restored by the binarized models. While these concepts are unlearnable by the original VAE models (see Figure \ref{fig:lowvar}).
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{ images/iter_latent_lowvar.png}
    \caption{Bottom-2 variance latent interpolation using the CelebA dataset. The binarized models learn low-level properties, while the original models collapse}
    \label{fig:lowvar}
\end{figure}
% \input{celeba_sample.tex}





\subsection{Generative Replay in Continual Learning}



Generative replay requires the fit-sample-replay loop~\cite{shin2017continual,rostami2019complementary,rostami2020generative}, which highly depends on the generated quality of the samples. First, we tested the compatibility of the Gaussian mixture prior to binarized regularization, since it is commonly used for conditional replay and proves to be effective. We test a GMVAE with a binarized GMVAE on MNIST (see Figure \ref{fig:test_gm}). We have compared state-of-the-art CL methods that use generative replay to mitigate catastrophic forgetting. We observe that binarized GMVAE can better disentangle the classes with fewer ambiguities. 
\begin{figure}[h]
    \centering
\begin{minipage}{.23\textwidth}
  \centering
  \includegraphics[width=\linewidth]{ images/gmvaemnist.png}
\end{minipage}
\begin{minipage}{.23\textwidth}
  \centering
  \includegraphics[width=\linewidth]{ images/bgmvae-0.1-mnist_mnist_samplle.png}
\end{minipage}
    \caption{Samples from unsupervised training on MNIST. Each component (row) with 10 samples (column) GMVAE (left) vs. Binarized GMVAE (right)}
    \label{fig:test_gm}
\end{figure}

Then we test the binarized generative replay models on the most challenging incremental-class learning scenario. We performed experiments using generative replay(GR) and brain-inspired generative replay (BIR). We conclude that with our modification, the forgetting effect of early tasks can be generally reduced, under single modal (Gaussian) and multimodal (GMM) latent representation, with or without conditional replays (see Table \ref{table:bir}).  

In Figure~\ref{fig:sample_cl}, we observe that the variance in earlier tasks is maintained better, which results in less forgetting of earlier tasks which is an important challenge. The worse performance in more recent tasks and better performance on average suggests that the improvement is not from the ability to reconstruct better samples in general, but from maintaining more diverse knowledge in the model. This observation indicates that we have achieved learning more general and robust factors in the latent space.

\begin{figure}[h]
    \centering
\begin{minipage}{.23\textwidth}
  \centering
  \includegraphics[width=\linewidth]{ images/GMM.png}
  \label{fig:test1}
\end{minipage}
\begin{minipage}{.23\textwidth}
  \centering
  \includegraphics[width=\linewidth]{ images/BGMM_R=0.01.png}
  \label{fig:test2}
\end{minipage}
    \caption{Conditional Samplings from task 1 and task 2 after task 10. Brain-inspired Replay (GMVAE) vs. Brain-inspired Replay(B-GMVAE, r = 0.01). Binarized prior shows higher variances in replayed early tasks.}
    \label{fig:sample_cl}
\end{figure}

\subsection{Ablation Experiments}

We study the effect of major hyperparameters on the quality of reconstrcuted samples.

\subsubsection{Behavior with different $r$ and latent dimension}
We   study how different latent sizes $d_z$ and different $r$ can affect the reconstruction and disentanglement quality using the MNIST dataset. Results are presented in Figures \ref{fig:heat_recon} and \ref{fig:heat_nmi}. We observe see that a small $r([0.1,2])$ can improve both the reconstruction and disentanglement quality. A large $r(>2)$ can impact the reconstruction while staying on par for unsupervised clustering with the original model.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{ images/recon_heatmap.png}
    \caption{Normalized reconstruction error with different latent dimension (verticle axis) and $r$ (horizontal axis).  Smaller values(lighter color)denote better reconstruction performance.}
    \label{fig:heat_recon}
\end{figure}


 \begin{table*}[htb]
\input{bir_table.tex}
\end{table*}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{ images/nmi_heatmap.png}
    \caption{Normalized mutual information with different latent dimension(verticle axis) and $r$(horizontal axis). Larger values(lighter color)denote better clustering performance.}
    \label{fig:heat_nmi}
   
\end{figure}

\textbf{Choosing r}:
The hyperparameter $r$ decides how disentangled the binarized priors are in the latent embedding space. The center (mean) distance between the binarized priors in a $d$-dimensional space has range the $[2r, 2r\sqrt{d}]$. We can see at large $d$, with a low learning rate, a large $r$ will make it difficult for a latent distribution to shift to a distant mean when new representations are learned. This is undesirable at the beginning of training or for learning high-level concepts subject to large changes. Hence, a small $r$ works well in practice. Typically a smaller $r$ is required for a more complex dataset, lower learning rate, and higher latent dimension. For our experiments, ($r \in [0.5,2]$ for MNIST, $r \in [0.1,1]$ for CelebA, and $r \in [0.0001,0.01]$ for continual learning). (CL representations are subject to drastic changes, so $r$ needs to be extremely small)
\subsubsection{Source of improvement}

We empirically investigate why using our regularization techniques leads to improved performance.  

\textbf{Not from Exponential growth of high-density area:} Since our method also employs the idea to exponentially increase the high-density size like hyperspherical-VAE (NVAE) as a way to resolve the issues with Gaussian distribution. We compare a binarized VAE and NVAE in table \ref{table:MNIST}, and observe a significant performance improvement compared to NVAE.
\textbf{No extra resource required:} In table \ref{table:MNIST}, we also compared the time cost to train a binarized model. We observed that binarized VAE does not increase the time, and in reality, there is a $\approx1\%$ performance increase for binarized toyVAEs. This might be due to more minor variance in the binarized $\mu_{q_{\phi}(z|x)} - \mu_{p(z)}$ matrix, resulting in better floating-number matrix multiplication in the program.

\section{Conclusions}
We proposed binarized regularization to improve VAEs. Our approach helps to distribute the distribution mass in the latent embedding space to represent the input distribution in a more distributed way.  Extensive experiments demonstrate that our approach exhibits good properties such as better disentanglement, better reconstruction quality, and better sampling variance. The effectiveness of our model is   exhibited in all cases we tested, with simple modification and no extra cost. The encoding using symmetric bimodal distributions has great potential in applications of VAE, as demonstrated in the case of generative replay for CL. Future work includes applying our approach  to large hierarchical VAE models and other latent space models.% and a more systematic  way to tune the binarizing hyperparameter. 
 
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }
\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{aljundi2018memory}
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and
  Tinne Tuytelaars.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 139--154, 2018.

\bibitem{dbvae}
Christopher~P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters,
  Guillaume Desjardins, and Alexander Lerchner.
\newblock Understanding disentangling in $\beta$-vae, 2018.

\bibitem{btcvae}
Tian~Qi Chen, Xuechen Li, Roger~B. Grosse, and David Duvenaud.
\newblock Isolating sources of disentanglement in variational autoencoders.
\newblock {\em CoRR}, abs/1802.04942, 2018.

\bibitem{vdvae}
Rewon Child.
\newblock Very deep vaes generalize autoregressive models and can outperform
  them on images.
\newblock {\em CoRR}, abs/2011.10650, 2020.

\bibitem{svae}
Tim~R. Davidson, Luca Falorsi, Nicola De~Cao, Thomas Kipf, and Jakub~M.
  Tomczak.
\newblock Hyperspherical variational auto-encoders.
\newblock 2018.

\bibitem{gmvae}
Nat Dilokthanakul, Pedro A.~M. Mediano, Marta Garnelo, Matthew C.~H. Lee, Hugh
  Salimbeni, Kai Arulkumaran, and Murray Shanahan.
\newblock Deep unsupervised clustering with gaussian mixture variational
  autoencoders, 2016.

\bibitem{gan}
Ian~J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
  Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks, 2014.

\bibitem{bvae}
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot,
  Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner.
\newblock beta-{VAE}: Learning basic visual concepts with a constrained
  variational framework.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{ddpm}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models, 2020.

\bibitem{jin2021learn}
Xisen Jin, Bill~Yuchen Lin, Mohammad Rostami, and Xiang Ren.
\newblock Learn continually, generalize rapidly: Lifelong knowledge
  accumulation for few-shot learning.
\newblock In {\em Findings of Empirical Methods in Natural Language Processing
  (Findings of EMNLP)}, 2021.

\bibitem{factorvae}
Hyunjik Kim and Andriy Mnih.
\newblock Disentangling by factorising, 2018.

\bibitem{vae}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes, 2013.

\bibitem{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  114(13):3521--3526, 2017.

\bibitem{wae}
Soheil Kolouri, Phillip~E. Pope, Charles~E. Martin, and Gustavo~K. Rohde.
\newblock Sliced wasserstein auto-encoders.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{dipvae}
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan.
\newblock {VARIATIONAL} {INFERENCE} {OF} {DISENTANGLED} {LATENT} {CONCEPTS}
  {FROM} {UNLABELED} {OBSERVATIONS}.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{vqvae}
Aaron van~den Oord, Oriol Vinyals, and Koray Kavukcuoglu.
\newblock Neural discrete representation learning, 2017.

\bibitem{rolnick2019experience}
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
  Wayne.
\newblock Experience replay for continual learning.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{rostami2021lifelong}
Mohammad Rostami.
\newblock Lifelong domain adaptation via consolidated internal distribution.
\newblock {\em Advances in neural information processing systems},
  34:11172--11183, 2021.

\bibitem{rostami2020generative}
Mohammad Rostami, Soheil Kolouri, Praveen Pilly, and James McClelland.
\newblock Generative continual concept learning.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pages 5545--5552, 2020.

\bibitem{rostami2019complementary}
Mohammad Rostami, Soheil Kolouri, and Praveen~K Pilly.
\newblock Complementary learning for overcoming catastrophic forgetting using
  experience replay.
\newblock In {\em Proceedings of the 28th International Joint Conference on
  Artificial Intelligence}, pages 3339--3345, 2019.

\bibitem{rostami2021detection}
Mohammad Rostami, Leonidas Spinoulas, Mohamed Hussein, Joe Mathai, and Wael
  Abd-Almageed.
\newblock Detection and continual learning of novel face presentation attacks.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 14851--14860, 2021.

\bibitem{ae}
D~E Rumelhart and J~L Mcclelland.
\newblock Parallel distributed processing: explorations in the microstructure
  of cognition. volume 1. foundations.
\newblock 1 1986.

\bibitem{rusu2016progressive}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock {\em arXiv preprint arXiv:1606.04671}, 2016.

\bibitem{akinator}
Gal Sasson and Yoed~N. Kenett.
\newblock A mirror to human question asking: Analyzing the akinator online
  question game.
\newblock {\em Big Data and Cognitive Computing}, 7(1), 2023.

\bibitem{shin2017continual}
Hanul Shin, Jung~Kwon Lee, Jaehong Kim, and Jiwon Kim.
\newblock Continual learning with deep generative replay.
\newblock {\em Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem{GalilAI}
Sumedh~A. Sontakke, Stephen Iota, Zizhao Hu, Arash Mehrjou, Laurent Itti, and
  Bernhard Sch√∂lkopf.
\newblock Galilai: Out-of-task distribution detection using causal active
  experimentation for safe transfer rl.
\newblock In Gustau Camps-Valls, Francisco J.~R. Ruiz, and Isabel Valera,
  editors, {\em International Conference on Artificial Intelligence and
  Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event}, volume 151 of
  {\em Proceedings of Machine Learning Research}, pages 7518--7530. PMLR, 2022.

\bibitem{vamp}
Jakub~M. Tomczak and Max Welling.
\newblock Vae with a vampprior, 2017.

\bibitem{yoonlifelong}
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung~Ju Hwang.
\newblock Lifelong learning with dynamically expandable networks.
\newblock In {\em International Conference on Learning Representations}.

\end{thebibliography}

\onecolumn

\section*{\centering{\LARGE{Appendix}}}

 
 
 Experiments implementations are available at \url{https://github.com/zizhao-hu/BVAE}

\section*{1.Latent Visualizations}
\subsection*{1.1 Pair-wise 4-D Latent Visualizations in 2-D}
Visualization for the latent space(dimension = 4) of VAEs and the binarized versions. Symmetry is shown in the binarized VAEs and posterior collapse does not appear in binarized disentangling $\beta$-VAE. We use MNIST training dataset.
\begin{figure*}[h]
\includegraphics[width=\linewidth]{sup_material/4_latent_visual.png}
    \caption{Pair-wise latent space visualization }
    \label{fig:4_latent_vis}
\end{figure*}
In row 1, the binarized latent space is more symmetric. The correlation between two latent dimensions is also reduced. The data points are spread out in a larger area without requiring additional training resources.

In row 2, the binarized latent space does not suffer from posterior collapse. 
\section*{2. UMAP 16-D Latent Visualizations in 2-D}
Four models(VAE, $\beta$-VAE,  disentangling  $\beta$-VAE,  GMVAE) with four binarized versions. Column one denotes the original models corresponding to binarizing hyperparameter $r=0$. Column $2,3,4$ correspond to $r=0.1$,$r=1$ and $r=2$. We use MNIST training dataset. The best disentanglement of classes happens at $r = 1$ for all unimodal Gaussian prior models. $r = 0.1$ for Gaussian mixture prior.
\begin{figure*}[h]
    \includegraphics[width=\linewidth]{sup_material/umap_clustering.png}
    \caption{UMAP visualization of the latent manifolds in 2-D.}
    \label{fig:my_label1}
\end{figure*}
In rows 1, 2, and 3, the best disentanglement happens at $r=1$. When $r=2$, the latent is over-disentangled.

In row 4, the binarized model detaches the border between classes, and achieves better disentanglement. 

\section*{3. Visualization of Different Hyperparameters}
\subsection*{3.1 Reconstruction}
We test $r=0,0.01,0.1,1$ settings on the CelebA dataset. Reconstruction loss is reduced for binarized models $r=0.01,0.1$. The running time is also slightly reduced(Table \ref{tab:recon_time}).


\begin{table}[ht]
\centering
\begin{tabular}{|l|ll|}
\hline
              & Reconstruction loss & Running time(Average of 50 epochs) \\ \hline
r=0(original) & 0.0137              & 115.26                             \\
r=0.01        & \textbf{0.0128}     & 115.36                             \\
r=0.1         & \textbf{0.0128}     & \textbf{113.32}                    \\
r=1           & 0.0160              & 116.8                              \\ \hline
\end{tabular}
\caption{Reconstruction quality and running time}
\label{tab:recon_time}
\end{table}

\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{sup_material/r=0recon.png}
    \centering
    \\r=0(original)
    \label{fig:my_label2}
\end{minipage}%
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{sup_material/r=0.01recon.png}
    \centering
    \\{r=0.01}
    \label{fig:my_label3}
\end{minipage}
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{sup_material/r=0.1recon.png}
    \centering
    \\{r=0.1}
    \label{fig:my_label4}
\end{minipage}%
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{sup_material/r=1rcon.png}
    \centering
    \\{r=1}
    \label{fig:my_label5}
\end{minipage}
 \caption{Reconstruction with different r settings}
\end{figure}
\subsection*{3.2 Sampling}
Sampling is conducted on the same models as in the last section. We use the same latent space vector to generate the image in all four models. We can see that when $r$ is small, the samples have more distinct facial features, sharper edges, and  vibrant colors. When $r$ is large, the samples sampled from the original prior are from the ``ambiguous region", since the sampling prior is between the binarized centers. And the generated images are ambiguous and lack distinction as expected. 
\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{sup_material/r=0sample.png}
    \centering
    \\r=0(original)
    \label{fig:my_label6}
\end{minipage}%
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{ sup_material/r=0.01sample.png}
    \centering
    \\{r=0.01}
    \label{fig:my_label7}
\end{minipage}
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{ sup_material/r=0.1sample.png}
    \centering
    \\{r=0.1}
    \label{fig:my_label8}
\end{minipage}%
\begin{minipage}{.5\textwidth}
    \includegraphics[width=0.99\linewidth]{ sup_material/r=1sample.png}
    \centering
    \\{r=1}
    \label{fig:my_label9}
\end{minipage}
 \caption{Reconstruction with different r settings}
\end{figure}
\section*{4. Reduced-bias Sampling for High-variance Posteriors}
\subsection*{4.1 Binarized approximate posterior}
Optimal approximate posterior estimation for VAE with a Gaussian prior $ p_{\phi}(z|x)\sim \mathcal{N}(\mu,\sigma^2)$, assuming some error $\epsilon_\mu$ and $\epsilon_\sigma$
\begin{equation}
    p_{\phi}(z|x)\sim \mathcal{N}(\mu+\epsilon_\mu,(\sigma+\epsilon_\sigma)^2) = \frac{1}{(\sigma+\epsilon_\sigma)\sqrt{2\pi}} 
  \exp\left( -\frac{1}{2}\left(\frac{x-\mu-\epsilon_\mu}{(\sigma+\epsilon_\sigma)}\right)^{\!2}\,\right)
\end{equation}
For binarized VAE: 
\begin{equation}
p_{\phi}(z|x)_{Binarized}\sim \mathcal{N}(|\mu+\epsilon_\sigma|-r,(\sigma+\epsilon_\sigma)^2) = \frac{1}{(\sigma+\epsilon_\sigma)\sqrt{2\pi}} 
  \exp\left( -\frac{1}{2}\frac{(|x-\mu-\epsilon_\mu|-r)^2}{(\sigma+\epsilon_\sigma)^2}\,\right)
  \end{equation}
Difference:
\begin{equation}
 p_{\phi}(z|x) - p_{\phi}(z|x)_{Binarized} = \frac{1}{(\sigma+\epsilon_\sigma)\sqrt{2\pi}} 
  \left(\exp\left( -\frac{1}{2}\frac{(x-\mu-\epsilon_\mu)^2}{(\sigma+\epsilon_\sigma)^2}\,\right)-\exp\left( -\frac{1}{2}\frac{(x-\mu-\epsilon_\mu)^2-2r|x-\mu-\epsilon_\mu|+r^2}{(\sigma+\epsilon_\sigma)^2}\,\right) \right)
\end{equation}
When $x\approx\mu$ we have $(x-\mu-\epsilon_\mu)^2 \approx {\epsilon_\mu}^2<<(\sigma+\epsilon_\sigma)^2$, when $\sigma$ is sufficiently large, then approximate by first-order Taylor expansion:

\begin{equation}
\begin{aligned}
 p_{\phi}(z|x) - p_{\phi}(z|x)_{Binarized}&\approx \frac{1}{(\sigma+\epsilon_\sigma)\sqrt{2\pi}} \left(1 -\frac{1}{2}\frac{(x-\mu-\epsilon_\mu)^2}{(\sigma+\epsilon_\sigma)^2} -1 + \frac{1}{2}\frac{(x-\mu-\epsilon_\mu)^2-2r|x-\mu-\epsilon_\mu|+r^2}{(\sigma+\epsilon_\sigma)^2}\,\right)\\[1em]
&= \frac{-2r|x-\mu-\epsilon_\mu|+r^2}{2(\sigma+\epsilon_\sigma)^3\sqrt{2\pi}}
 \end{aligned}
\end{equation}
Thus, we have
\begin{equation}
  p_{\phi}(z|x) > p_{\phi}(z|x)_{Binarized} \; \text{when} \; |x-\mu-\epsilon_\mu|<\frac{r}{2}\; , \; x\approx\mu \; ,\text{and} \; \sigma \; \text{is sufficiently large}
\end{equation}
This allows the binarized approximate posterior to be less distributed around the mean. When sampled using the same prior distribution, reduced bias is thus achieved. (Also see Figure \ref{fig:re_bias}. The red curve denotes high-variance posterior. Column 1 is a scenario when the estimated posterior has a larger or smaller variance than the prior. Column 2 is the difference between the posteriors and the prior. Columns 3, and 4 are binarized models.)
\begin{figure}[h]
    \centering
    \includegraphics[width = 0.76\linewidth]{ sup_material/re_bias.png}
    \caption{Estimation and sampling distributions}
    \label{fig:re_bias}
\end{figure}

\end{document}