% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\input{custom_commands.tex}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{booktabs}


\usepackage{xcolor}
\definecolor{dgreen}{rgb}{0,0.5,0}

\usepackage{tikz}
\pgfdeclarelayer{background}
\pgfdeclarelayer{main}
\pgfsetlayers{background,main}
\usetikzlibrary{calc}
\usepackage{ifthen}

% Setup plots libraries
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{stfloats}
\usepackage{tikz}
\usepackage{pgfplots}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{patterns}
\pgfplotsset{compat=1.8}

\newenvironment{customlegend}[1][]{%
    \begingroup
    % inits/clears the lists (which might be populated from previous
    % axes):
    \csname pgfplots@init@cleared@structures\endcsname
    \pgfplotsset{#1}%
}{%
    % draws the legend:
    \csname pgfplots@createlegend\endcsname
    \endgroup
}%

\def\addlegendimage{\csname pgfplots@addlegendimage\endcsname}




% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\slic: Faster Long-Range Transformers with Conditional Computation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
Joshua Ainslie\thanks{\- Author contributions are outlined in Appendix \ref{sec:appendix-contributions}. Correspondence author: jainslie@google.com.},~ Tao Lei,~ Michiel de Jong,~ Santiago Onta\~{n}\'{o}n \\
{\bf Siddhartha Brahma},~ {\bf Yury Zemlyanskiy},~ {\bf David Uthus},~ {\bf Mandy Guo} \\
{\bf James Lee-Thorp},~ {\bf Yi Tay},~ {\bf Yun-Hsuan Sung},~ {\bf Sumit Sanghai}
  \AND
  {\rm \Large Google Research}\\
  }

\begin{document}
\maketitle
\begin{abstract}
Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -{}- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose \slic, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that \slic achieves stronger performance than \longt with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, \slic can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.

\end{abstract}

\section{Introduction}\label{sec:intro}

Many natural language processing tasks, such as summarization~\cite{cohan2018arxiv} or question answering over long documents~\cite{joshi2017triviaqa}, require machine learning models to encode long-form text. Processing long documents with a Transformer model is computationally expensive, both because attention cost scales quadratically with input length and because feedforward and attention projection layers have to be applied to each input token.

Over the past few years, many ``efficient Transformer'' approaches have been proposed that reduce the cost of the attention mechanism over long inputs~\cite{child2019generating, ainslie2020etc, beltagy2020longformer, zaheer2020big, wang2020linformer, tay2021long, guo2022longt5}. However, especially for larger models, the feedforward and projection layers actually make up the majority of the computational burden and can render processing long inputs intractable. 

This paper presents \slic (Conditional LongT5), a new family of models that, building on top of \longt~\cite{guo2022longt5}, enables fast processing of long inputs by combining architecture improvements for both attention and feedforward layers. \slic is based on the intuition that some tokens are more important than others, and we can achieve better quality for lower cost by devoting more computation to important tokens. Moreover, the fraction of important tokens is likely to diminish with document length, allowing for tractable processing of long documents.
\input{figures/slic_architecture}
\input{figures/perf_vs_time}

In particular, \slic divides each feedforward layer and each attention layer into a {\em light branch} which is applied to all tokens and a {\em heavy branch} which is applied to a set of important tokens, selected specifically for that input and component. The light feedforward branch has lower hidden dimension than standard \longt while the heavy feedforward branch has higher hidden dimension. The light attention branch has fewer heads and applies only local attention, while the heavy attention branch performs full attention over another separately selected set of important tokens. Figure \ref{fig:slic} provides an overview of the \slic conditional mechanism.

Finally, \slic also includes two other modifications to the \longt architecture. \slic adds multi-query cross-attention~\citep{shazeer2019mq}, significantly speeding up inference. \slic also employs the UL2~\cite{tay2022ul2} pre-training objective, which we demonstrate allows for in-context learning over long inputs.

% We show that \slic performs much faster fine-tuning and inference with similar or better model quality on the SCROLLS benchmark~\citep{shaham2022scrolls} as well as arXiv summarization~\citep{cohan2018arxiv} and TriviaQA question answering~\citep{joshi2017triviaqa} datasets. 

We show that \slic performs much faster fine-tuning and inference with similar or better model quality, improving over \longt on arXiv summarization~\citep{cohan2018arxiv} and TriviaQA question answering~\citep{joshi2017triviaqa} datasets and achieving SOTA on the SCROLLS benchmark~\citep{shaham2022scrolls}. Moreover, \slic achieves further gains in quality and speed for tasks with extremely long inputs (64k tokens), with less-than-linear scaling of ``focus'' tokens.


\section{Background}\label{sec:background}

\paragraph{Transformer FLOPs} \slic follows an extensive line of work in attempting to reduce the computational cost of Transformer models, particularly over long inputs. The computational burden of Transformer models has several distinct elements, and different approaches focus on reducing the cost of different components. For that reason, it is helpful to start by providing a breakdown of the computational cost of Transformer components. Table \ref{table:transformer_flops} shows the FLOPs\footnote{Each multiply-add is counted as a single FLOP.} for each component of a Transformer encoder layer~\citep{kaplan2020scaling}.

\input{tables/transformer_flops}

\paragraph{Sparse attention} The first challenge of applying a Transformer to a long input is that the FLOPs of the self-attention mechanism scales quadratically in the input length, becoming intractable for long inputs. A large body of work focuses on reducing self-attention cost, restricting attention between a subset of inputs~\citep{child2019generating, ainslie2020etc, beltagy2020longformer, zaheer2020big, wang2020linformer, guo2022longt5} or to a subset of layers~\citep{zemlyanskiy2021readtwice}. In \longt~\citep{guo2022longt5}, the most closely related model to \slic, tokens attend within a local window as well as to a mean-pooled summary representation for each block of 16 tokens in the input. \longt attention leads to sharply reduced (though still non-negligible) FLOPs (Table \ref{table:transformer_flops}).

\paragraph{Conditional computation} After applying a sparse attention mechanism, the feedforward and attention projection layers account for the majority of the FLOPs. These costs scale with the length of the input, such that processing long inputs is still prohibitively expensive. A common approach to reduce the remaining cost is to employ some form of \textit{conditional computation}, avoiding applying all model parameters to the entire input. CALM~\cite{schuster2022confident} applies a varying number of decoder layers to each decoded token, outputting a token early if the model is confident in its prediction. Mixture-of-Experts models~\cite{shazeer2017moe, fedus2021switch, zoph2022stmoe} route inputs through a small proportion of expert sub-modules, bringing to bear only the parameters most relevant to the input. In the context of retrieval-augmented models, numerous works re-rank retrieved passages by their relevance to the query and process only the highest scoring passages~\citep{readerguidererank, r3rerank, kgfid} and vary the number of processed passages depending on model confidence~\citep{adaptiveretrieval, canext}. Concurrent work CoDA~\citep{lei2023conditional} employs a related conditional computation mechanism, designed for efficient adaptation rather than modeling long documents.

\paragraph{Device utilization} FLOPs do not tell the whole story, as modeling choices can influence the effective speed of operations achieved by accelerators. For long text inputs, autoregressive decoder inference is very slow due to memory bandwidth constraints from repeatedly loading the long sequence of keys and values~\citep{shazeer2019mq, dejong2022fido}. \citet{shazeer2019mq} introduces multi-query attention (MQA), sharing heads for keys and values to reduce memory bandwidth overhead. \citet{pope2022efficiently} studies how to shard large models, especially in the context of MQA, to obtain optimal device utilization and therefore speed.

\paragraph{Training objectives} T5 introduced the span corruption objective~\citep{raffel2020t5}, a modification of masked language modeling~\citep{devlinbert2019}. \longt made use of the PEGASUS~\cite{zhang2020pegasus} sentence reconstruction objective for improved summarization performance. \citet{tay2022ul2} proposes UL2, a mixture of span corruption, prefix, and causal language modeling, and shows that it leads to strong performance on both short-output and generative tasks.

\section{\slic}\label{sec:approach}


\subsection{Conditional computation}

As discussed in the previous section, a large proportion of Transformer FLOPs arise from feedforward and projection layers that scale with the length of the input sequence. Therefore, \longt training and inference on long documents remains expensive. 

\slic further reduces the cost of processing long documents through \textit{conditional computation}, following the intuition that some tokens are more important and therefore benefit more than others from heavy computation. First, some types of tokens may inherently require less computation, such as filler words and punctuation. Second, especially in long documents, large parts of the input may not be relevant to the current question, task, or processing stage.

The \slic conditional computation mechanism consists of three components: routing modules, conditional feedforward layers, and conditional attention layers. All tokens are processed by standard, lightweight attention and feedforward layers. Routing modules additionally select important tokens from an input at each attention or feedforward layer, and a heavy conditional layer applies additional computation to routed tokens. This section describes each component in detail. Figure~\ref{fig:slic} provides an overview of the \slic conditional computation mechanism, and Table \ref{table:total_transformer_flops} compares \slic and \longt FLOPs.

\input{tables/total_flops}

\paragraph{Routing}

In order to separately select important tokens for each component in each layer, we need a \textit{learnable} and \textit{tractable} routing function. We follow the simple three-step mechanism from \citet{lei2023conditional}: (1) multiply inputs with a learned embedding to obtain routing scores, (2) normalize, and (3) select the top-$k$ highest scoring inputs.

Let $X_i$ be the representation of token $i$, and $u$ a $d$-dimensional learnable embedding. Then the routing score of token $i$ is 
\begin{equation*}
 s_i = X_i \cdot u
\end{equation*}
We select the top-$k$ highest scoring inputs. In order to provide a learning signal to the scoring embedding, we make sure the contribution of the routed tokens to the layer update is \textit{scaled} according to the routing score, as will be seen later. To provide a better distributed signal to all tokens, we also globally normalize the routing scores to sum up to the number of desired routed tokens using a generalized softmax, resulting in normalized scores $\tilde{s}_i$.
Each \slic layer has three independent routers, one each for the feedforward layer, attention queries, and attention key-values.

\paragraph{Conditional Feedforward}
Intuitively, some token representations may benefit from more processing than others. The \slic conditional feedforward layer applies an additional high-capacity feedforward layer to selected tokens. In particular, let $X_i$ be the model state of the $i$th token and $\tilde{s}_i$ denote the normalized routing score (set to 0 for non-routed tokens). Then the feedforward update for \slic is given by
\begin{equation*}
    X_i = X_i + \textrm{FFd}_{\textrm{Light}}(X_i) + \tilde{s}_i \cdot \textrm{FFd}_{\textrm{Heavy}}(X_i)
\end{equation*}
The light and heavy feedforward branches differ only in their hidden dimension, with the light branch having smaller hidden dimension than the standard T5 feedforward layer and the heavy branch larger. Let $n$ denote the number of input tokens, $m$ the number of selected tokens, and $r_L$ and $r_H$ the ratios of light and heavy hidden dimension to standard T5 hidden dimension. Then the FLOPs of the \slic layer are given by
\begin{equation*}
    \text{FLOPs}_{\text{FFd}} = \underbrace{8 n r_L d^2}_{\text{Light branch}} + \underbrace{8 m r_H d^2}_{\text{Heavy branch}}
\end{equation*}
We set the light and heavy ratios as $r_L = \frac12$ and $r_H = 4$, half and quadruple the standard T5 hidden dimension respectively. For our main experiments, a fraction $\frac{1}{16}$ of tokens are routed to the heavy branch. As a result the approximate FLOPs from the \slic feedforward layer equals 
\begin{equation*}
    \text{FLOPs}_{\text{FFd}} = \underbrace{4 n d^2}_{\text{Light branch}} + \underbrace{2 n d^2}_{\text{Heavy branch}}
\end{equation*}
consuming 75\% of the FLOPs of a standard T5 feedforward layer.
\input{tables/headline_results}
\paragraph{Conditional Attention}
\input{figures/slic_attention}
\slic conditional attention operates on the intuition that most tokens have simple, local interactions, but some tokens benefit from heavier processing and long-range interactions. The \slic conditional attention layer applies an additional high-capacity attention layer that attends from selected query tokens to selected key-value tokens. Let $\tilde{s}_i^{q}$ denote the normalized routing query score for token $i$, and $\tilde{s}^{kv}$ the key-value scores for all tokens (set to $0$ if not routed). Then the attention update for \slic is given by
\begin{equation*}
    X_i = X_i + \textrm{A}_{\textrm{Light}}(X_i, X) + \tilde{s}_i^{q} \cdot \textrm{A}_{\textrm{Heavy}}(X_i, \tilde{s}^{kv}X)
\end{equation*}
The light and heavy branches differ in the number of heads and tokens attended to: the light branch has fewer heads and attends to a local context window, while the heavy branch has more heads and attends to all routed key-value tokens. Separately selecting query and key-value tokens also allows the model to differentiate between tokens that \textit{require} additional information and those that \textit{possess} such information. Figure \ref{fig:slic_attention} shows the \slic attention pattern. Let $q, v$ be the number of selected query and key-value tokens, $w$ the size of the local attention window and $r_L, r_H$ the proportion of light and heavy heads relative to standard T5. Then the FLOPs of the \slic  attention layer are given by
\begin{align*}
    \text{FLOPs}_{\text{Att}} &= \underbrace{4 n \cdot r_L d^2}_{\text{Local projection}} + \underbrace{2 n  w \cdot r_L d}_{\text{Local attention}} \\ &+ \underbrace{2 q \cdot r_H d^2 + 2 v \cdot r_H d^2}_{\text{Global projection}} + \underbrace{2 q v \cdot r_{H} d}_{\text{Global attention}}
\end{align*}
We set the light and heavy head ratios as $r_L = \frac14$ and $r_H = \frac34$, keeping the total number of heads across the light and heavy branches equal to standard T5 heads. For our main experiments a fraction $\frac{1}{16}$ query tokens and $\frac{1}{8}$ key-value tokens are routed to the heavy branch, so $q = \frac{n}{16}$ and $v = \frac{n}{8}$. Ignoring local attention computation, we approximate attention FLOPS by\footnote{Global projection and attention FLOPs rounded to readable fractions, exact values are $\frac{9}{32}$ and $\frac{3}{256}$. Complexity assumes constant fraction of routed tokens; we show we can do better in practice for extremely long inputs.}

\begin{equation*}
    \text{FLOPs}_{\text{Att}} \approx \underbrace{n d^2}_{\text{Local proj.}} + \underbrace{\frac{1}{4}n d^2}_{\text{Global proj.}}  + \underbrace{\frac{1}{84}n^2 d}_{\text{Global att.}}
\end{equation*}
with less than half projection FLOPs and order-of-magnitude smaller quadratic length scaling compared to \longt. Table \ref{table:total_transformer_flops} shows total FLOPs for the \slic layer. In general, we set $q = m$ and $v = 2m$, and use $m$ to summarize the number of routed tokens going forward.



\subsection{Multi-query Attention}

Conditional computation effectively reduces the computational cost of the encoder. However, for encoder-decoder models with long inputs the majority of inference time is spent in the decoder due to memory bandwidth constraints~\citep{shazeer2019mq, dejong2022fido}. Most of the overhead is caused by repeatedly reading all the input token keys and values from memory for every output token that is autoregressively decoded during cross attention. Multi-query attention~\citep{shazeer2019mq} (MQA) allows all query heads to share a single key and value head, alleviating this bottleneck. Accordingly, we apply MQA in cross-attention layers for much faster inference. Note however that MQA does not improve training speed since target tokens are processed in parallel during training, avoiding this memory bandwidth bottleneck.

\subsection{UL2}

The UL2 pre-training objective~\citep{tay2022ul2} combines different denoising objectives, extending the span corruption pre-training used in T5 to a variety of noise rates / average span lengths and adding a prefix language modeling objective more similar to typical decoder-only model pre-training. UL2 has been shown to lead to improved in-context learning. We train \slic on UL2 instead of PEGASUS~\cite{zhang2020pegasus}, endowing \slic with in-context learning capabilities.

\section{Experiments}\label{sec:experiments}

In order to evaluate \slic, we perform the following experiments: (1) our main results compare \slic and \longt on a collection of long input datasets using input length of 16k tokens; (2) we evaluate \slic on extremely long inputs up to 64k tokens and compare scaling against \longt; (3) demonstrate \slic's few-shot capability, investigating how performance changes as input length and number of shots increase, (4) perform a series of ablations to understand the effect of individual \slic components, and (5) investigate empirical routing patterns. The remainder of the section outlines our experimental setup, and then describes each of the experiments above.

\subsection{Experimental setup}

\paragraph{Configurations}

\slic is based on the T5.1.1 architecture ~\citep{raffel2020t5}, implemented with JAX~\citep{jax}, Flax~\citep{flax}, and Flaxformer\footnote{https://github.com/google/flaxformer}. Following \longt, we experiment with Base, Large, and XL model sizes. \slic models use the same embedding dimension, number of layers, and total attention heads as corresponding \longt models of the same size, with more overall parameters (but less compute) due to the conditional branch. See Appendix \ref{sec:appendix-parameters} for additional details on model configuration.

\paragraph{Pre-training}
We pre-train \slic for 1M steps on the C4 dataset~\citep{raffel2020t5} using a variant of the UL2 objective~\citep{tay2022ul2} with batch size 256, input length 4096, and output length 910. In particular, our mixture contains four objectives in equal proportion: prefix-LM with noise rate 0.5, and span corruption~\citep{raffel2020t5} with noise rate 0.15 and average span lengths 3, 8, and 64. We use the Adafactor optimizer~\citep{adafactor} with the T5.1.1 inverse square root learning rate schedule and no dropout. \slic is trained with the T5X~\citep{t5x} framework. For pre-training, we route $m = 512$ tokens, $\frac{1}{8}$th of the input length.

\paragraph{Fine-tuning}
For fine-tuning we use a constant learning rate of 0.001, batch size 128, and dropout rate 0.1 for all tasks. Main results use input length of 16384 for all datasets other than ContractNLI, which uses 8192. Question answering datasets use output length 128 and summarization datasets use output length 512, except for GovRep which uses output length 1024. We route $m = 1024$ tokens, $\frac{1}{16}$th of the input length. We train until convergence and select the checkpoint with the highest dev performance. We use greedy decoding for inference.

\paragraph{Data}

We evaluate \slic on TriviaQA~\citep{joshi2017triviaqa}, arXiv~\citep{cohan2018arxiv}, and the SCROLLS benchmark~\citep{shaham2022scrolls}. SCROLLS contains question-answering datasets:  NarrativeQA~\citep{kocisky2018narrativeqa}, QASPER~\citep{dasigi2021qasper}, and QuALITY~\citep{pang2021quality}, an NLI dataset: ContractNLI~\citep{koreeda-manning-2021-contractnli}, and summarization datasets: SummScreenFD~\citep{chen-etal-2022-summscreen}, QMSum~\citep{zhong2021qmsum}, and GovReport~\citep{huang2021govreport}. Table \ref{table:data_stats} provides an overview of the size and input length for each dataset.

\input{tables/data_stats}

\paragraph{Timing}

We report time per sample per TPUv4 chip, as measured by xprof \citep{xprof}. For inference we use a single TPUv4 with batch size 16 or the largest that fits in memory. For fine-tuning we profile with 8 TPUv4 chips, sharded separately for each model to maximize throughput. 


\subsection{Main results}

Figure \ref{fig:perf_vs_time} compares the quality-speed trade-off for \longt\footnote{Note that \longt does not use MQA, but for profiling we add MQA to \longt for a conservative baseline.} and \slic, showing that \slic is better at any speed. For 16k input length, \slic matches or exceeds \longt quality for Large and XL with 35-75\% training speedup and 50-100\% inference speedup on top of the order-of-magnitude inference speedup from MQA. Encoder speedups are even greater (Appendix \ref{sec:appendix-results}). \slic-XL also achieves SOTA performance on the SCROLLS benchmark. Table \ref{table:headline_results} contains all main results.

\subsection{Scaling to extremely long inputs}
\input{figures/perf_vs_time_len}
We hypothesize that the advantage of \slic over \longt strengthens with input length, as the fraction of important tokens decreases and \slic can route a greater proportion of important tokens to the heavy branch. Figure \ref{fig:perf_vs_time_len} compares the quality-speed trade-off for \longt and \slic on NarrativeQA, sweeping over input length rather than model size. The number of routed tokens is $\frac{1}{16}$th of the input length, except that we do not increase routed tokens going from 32k to 64k, so at 64k we route only $\frac{1}{32}$nd of the input length. \slic achieves both stronger performance and faster inference speed at all input lengths and is able to effectively make use of extremely long inputs. We note that \slic achieves large quality gains by going from 32k to 64k tokens even while keeping the number of routed tokens constant, providing more evidence for our hypothesis. 

\subsection{In-context learning}

\begin{figure}[h!]
\begin{subfigure}[t!]{0.24\textwidth}
\hspace{-10pt}
\begin{tikzpicture}[baseline={(current bounding box.south)},scale=1.0]
\begin{axis}[
    xbar,
    % bar shift=0pt,
    enlarge y limits=0.2,
    width=1.21\columnwidth,
    height=1.5\columnwidth,
    major y tick style = transparent,    
    xmajorgrids = true,
    symbolic y coords={1k, 2k, 4k, 8k, 16k},    
    ytick = data,
    y dir=reverse,
    % xmin=0,
    axis y line*=none,
    axis x line*=bottom,
]
    \addlegendimage{empty legend}\addlegendentry{NaturalQ}
    \addplot[style={fewnqcolor,fill=fewnqcolor,mark=none}]
        coordinates {(0.0361,1k) (0.1001,2k) (0.1836,4k) (0.2651,8k) (0.3062,16k)};
\end{axis}
\end{tikzpicture}            
\end{subfigure} \hspace{-4pt}
\begin{subfigure}[t!]{0.24\textwidth}
\begin{tikzpicture}[baseline={(current bounding box.south)},scale=1.0]
\begin{axis}[
    xbar,
    % bar shift=0pt,
    enlarge y limits=0.2,
    width=1.21\columnwidth,
    height=1.5\columnwidth,
    major y tick style = transparent,    
    xmajorgrids = true,
    % xlabel = {TriviaQA},
    symbolic y coords={1k, 2k, 4k, 8k, 16k},  
    yticklabels={},    
    ytick = data,
    y dir=reverse,
    % xmin=0,
    axis y line*=none,
    axis x line*=bottom,
    /pgf/number format/fixed,
]
    \addlegendimage{empty legend}\addlegendentry{TriviaQA}
    \addplot[style={fewtqacolor,fill=fewtqacolor,mark=none}]
        coordinates {(0.0137,1k) (0.0303,2k) (0.0591,4k) (0.0840,8k) (0.1055,16k)};

\end{axis}
\end{tikzpicture}            
\end{subfigure}
\caption{\textbf{\slic can use its long-input capability to benefit from more shots for in-context learning.} Few-shot exact match for \slic-Large on Natural Questions and TriviaQA dev sets as a function of input tokens, fitting as many examples as possible. Each example contains question, context, and answer. Inputs length used are 1024, 2048, 4096, 8192, 16384.}
\label{fig:few_shot}
\end{figure}



Models trained on the UL2 objective have shown strong few-shot in-context learning (ICL) capabilities\footnote{We initially evaluated ICL for models pre-trained with PEGASUS but found performance to be nearly 0.} even at smaller sizes~\citep{tay2022ul2}. \slic enables tractable inference with long inputs. Here, we leverage this for scaling the number of examples used for in-context learning. 

We test the above hypothesis by evaluating few-shot learning performance on Natural Questions~\citep{kiwatkowski2019nq} and TriviaQA as a function of input length, using as many examples as fit in the context. We consider the open book setting, such that each example consists of question, context document, and answer. Table \ref{table:few_shot_n_shots} shows the number of examples by input length. We evaluate on the full dev set, randomly sampling examples from the training set for each dev sample until no further examples fit in the input length. We found that \slic can perform in-context learning only up to the input length it was trained on, so for these experiments we continued pre-training a \slic-Large model on input length 16384 for another 100k steps. For the same reason we route $m = 512$ tokens as in pre-training.

Figure \ref{fig:few_shot} displays \slic few-shot performance as a function of input length, showing that \slic is able to apply its long-input capabilities to extract information from increasing numbers of examples.

\begin{table}[h]
\small
\centering
\vspace{0.35cm}
\begin{tabular}{l|ccccc}
    \textbf{Dataset} & \textbf{1024} & \textbf{2048} & \textbf{4096} &  \textbf{8192} & \textbf{16384} \\
    \toprule
    NQ & 0.1 & 0.7 & 1.7 & 3.4 & 5.6 \\
    TriviaQA & 1.6 & 2.3 & 3.8 & 7.0 & 9.8 \\
    \bottomrule
\end{tabular}
\caption{Average number of Natural Questions and TriviaQA few-shot examples that fit in input length.}
\label{table:few_shot_n_shots}
\end{table}

\subsection{Ablations}
\input{tables/ablations}
\vspace{-0.1cm}

This section studies the effect of different choices in the \slic recipe. Table \ref{table:ablations} contains results of a series of experiments that change a single component for \slic Base. 

\paragraph{Routing} First, we note that static routing -{}- evenly distributing routed tokens over the input -{}- leads to massive drop in performance. The importance of routing provides evidence that the model learns to devote capacity to important tokens and the advantage of \slic is not merely a result of additional parameters. Sharing routing decisions for query and KV tokens should be compared with v=q, and leads to a modest reduction in quality and increase in speed.

The optimal number of routed tokens represents a trade-off between improved performance and computational cost of applying heavier layers. Table \ref{table:ablations} shows strong gains going from 512 to 1024 (baseline) routed tokens and diminishing returns for further increases.

\paragraph{Attention} 

\slic relies on routing to identify not only tokens that can benefit from important information elsewhere in the input, but also which tokens contain such important information. We study whether \slic is successful in this task by comparing performance with two different attention settings -{}- v=all, in which routed tokens attend to the entire input, and v=q, which uses equal number of routed keys and values as queries, rather than twice as many. \slic appears to occupy a sweet spot, as using fewer routed key-values modestly decreases performance at similar speed but attending to all inputs barely helps at sharply increased cost.

\paragraph{Other} 

We compare \slic to \longt with multi-query cross-attention, confirming that \longt indeed does not achieve an unexpected quality gain from MQA, and our conservative assumptions in Figures \ref{fig:perf_vs_time}, \ref{fig:perf_vs_time_len} are valid. Next, we evaluate multi-head cross-attention for \slic, finding that it leads to modestly improved \slic performance. However, as MHA exhibits order-of-magnitude slower inference, MQA is clearly favored. Finally, PEGASUS appears to fine-tune slightly better than UL2, though the difference is small and UL2 enables few-shot learning.

\subsection{Routing analysis}

\input{figures/routing_proportion}

It is interesting to ask whether \slic routed tokens line up with what we consider intuitively important tokens in each document. We investigate this question by studying routing patterns of a Large \slic model fine-tuned on TriviaQA. We divide tokens into three categories: (1) question tokens, (2) answer tokens, and (3) other tokens. Figure \ref{fig:routing_prop} shows the average fraction of each type of token that is routed through the heavy path for MLP and attention layers on TriviaQA. We note that question and answer tokens are significantly more likely to be routed than other tokens, for feedforward as well as attention queries and keys/values. Appendix \ref{sec:appendix-routing} presents more detailed routing analysis; e.g., semantically important tokens are much more likely to be selected in later layers.


\section{Conclusion}\label{sec:conclusions}

We propose \slic, a new model for long-range inputs that employs conditional computation for higher quality and faster speed. \slic has light feedforward and attention layers that apply to the entire input, as well as heavy branches that are applied only to a subset of important tokens selected by a learned router. We show that \slic achieves stronger performance at any speed compared to \longt on a variety of long-input datasets, and can effectively and efficiently make use of extremely long inputs up to 64k tokens.

\section*{Limitations}

\slic applies conditional computation only in the encoder. Applying conditional computation in the decoder is more complicated; the routing method in \slic is not causal, so it isn't applicable when generating token by token. Since decoder-only models and applications with long outputs have become more popular recently, this is a strong limitation of the current approach. Although the routing method in \slic could potentially be applied to the \emph{input} context in a decoder-only model, we didn't investigate this setup.

\slic is specialized towards long sequences and has to be trained from scratch. For large-scale training and deployment, it is desirable to either train a single model that can handle both short and long sequences, or develop a long-input architecture that can be adapted from an existing large model. 

\section*{Acknowledgements}

We would like to thank Srinadh Bhojanapalli, Luke Vilnis, Zachary Fisher, Jianmo Ni, Tal Schuster, Vaclav Cvicek, Sudeep Gandhe, Bhargav Kanagal, Kenton Lee, Ming-Wei Chang, Afroz Mohiuddin, Raphael Hoffmann, and others at Google Research for helpful advice and discussion. 


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\clearpage
\input{tables/model_hyperparameters}
\input{tables/full_timing}
\appendix

\section{Contributions}
\label{sec:appendix-contributions}
Joshua led the project, developed the initial conditional attention mechanisms, and conducted most experimental ablations.  Tao developed the heavy/light formulation for heterogeneous conditional computation, comprising the routing and conditional feedforward mechanisms, and iterated with Joshua on initial experiments demonstrating feasibility.  Michiel helped to scope the paper, performed most of the writing, and oversaw speed benchmarking.  Santiago designed and conducted all the few-shot experiments, initiated the routing analysis visualization, and integrated UL2 into the codebase.  Siddhartha developed the separate routing for query and key/value tokens in the conditional attention component and demonstrated the resulting quality improvements.  Yury designed and conducted all experiments for inputs larger than 16k tokens, demonstrating favorable scaling up to 64k.  David integrated all SCROLLS tasks into the codebase and ran early experiments, especially comparing UL2 with PEGASUS.  Mandy developed the leaderboard comparisons with LongT5 and helped run several experiments.  James advised on and ran early comparisons with MoE conditional computation.  Yi advised on the adaptation of UL2 to 4k input length pre-training.  Finally, Yun-Hsuan and Sumit provided guidance and support for the project overall.



\section{Model Hyperparameters}
\label{sec:appendix-parameters}

Table \ref{table:parameters} shows \longt and \slic hyperparameters, including parameter counts. For \longt, we report numbers for the TGlobal configuration, which match T5.1.1. Notice that \slic's parameter counts are larger due to using conditional compute. Similar to other conditional compute architectures such as mixture-of-experts, computational cost does not necessarily increase with parameter count.

We use the same 127-token local radius for \slic as \longt.  This results in a local attention window $w$ of 255 since 127 tokens are attended to the left and 127 to the right.

\section{Routing Normalization Hyperparameters}
\label{sec:appendix-routing-hyperparams}

To normalize the routing scores for differentiable top-$k$ token selection, we use the iterative soft top-$k$ algorithm from \citet{lei2023conditional} and \citet{qian2022multi} with $\epsilon = 1.0$ and 50 iterations.  During training we allow the top $\frac{9}{8} k$ tokens to have nonzero weight instead of just the top $k$ in order to provide a slightly improved training signal.



\section{Additional Experimental Results}
\label{sec:appendix-results}




Table \ref{table:full_timing} compares \longt and \slic inference speed in more detail, splitting off encoder and total time per sample. Since \slic applies conditional computation only in the encoder, encoder speed gains are larger than overall speed gain, and total speed gains are largest for shorter output length. Trade-offs are even more in the favor of \slic when paired with other decoder optimizations.

\input{tables/full_summarization_results.tex}

Table \ref{table:full_summarization_results} shows full (Rouge-1, Rouge-2, Rouge-L) results for summarization datasets.


\input{figures/routing_proportion_layer}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/slic_routing_finetuned_triviaQA.pdf}
    \caption{Visualization of token routing weights for some fragments of an example on TriviaQA.}
    \label{fig:routing-viz}
\end{figure*}

\section{Computational Resources}
\label{sec:appendix-compute}

For pre-training we generally used 128 TPUv4 chips for Base and 256 TPUv4 chips for Large and XL. Pre-training took approximately 2.5 days for Base, 3.7 days for Large, and 12.8 days for XL. For fine-tuning we generally used 64, 128, and 256 TPUv4 chips for Base, Large, and XL, respectively, with training time varying with dataset size.


\section{Routing Analysis}
\label{sec:appendix-routing}


In this section we take a closer look at the routing mechanisms in \slic. There are three routing processes in each layer of \slic : (1) Routing of attention keys and values (``KV-routing''), (2) routing of attention queries (``Q-routing'') and (3) routing of MLP tokens (``MLP-routing''). For simplicity, we will say that a token is {\em selected}, when it is routed to the heavy alternative (of either MLP or attention). We are interested in understanding what tokens are selected and whether these mechanisms select similar or different tokens in each layer.


\paragraph{Which tokens are selected} 

We divide input tokens into three categories: (1) question tokens, (2) answer tokens (found via simple normalized string match of the ground truth answer), and (3) other tokens. Figure \ref{fig:routing_prop_layer} shows the proportion of each token type that is routed by a fine-tuned \slic-Large model on the TriviaQA dev set, by layer and routing component. 

Earlier we showed that question and answer tokens are more likely to be selected, but separating routing decisions by layer reveals interesting patterns. At early layers question and answer tokens are only modestly more likely to be selected, with routing probability sharply increasing at later layers and peaking in the last layer. This makes intuitive sense: in early layers the model has not yet had the opportunity to identify which tokens and parts of the document are important. However, the increase is not monotonic and there is strong variation between layers. This variation may imply that different layers focus on different types of tokens, or that some routing components do not successfully learn to identify important tokens.

To gain a better insight into this, Figure \ref{fig:routing-viz} visualizes routing on two sample fragments from a TriviaQA example (notice that, given the large input length used in \slic, we do not show the complete example in the figure). The two fragments shown correspond to the beginning of the example (where the question is located), and the part of the context surrounding the correct answer. We have added a colored background to the figure, where each of the three CMY channels are mapped to the KV-routing weights in different layers of the model. {\em Cyan} corresponds to layer 1, {\em Magenta} to layer 12, and {\em Yellow} to layer 24. As we can see, question and answer are heavily yellow colored, showing those tokens are selected in the last layer.

\paragraph{Correlation between routing processes.} Table \ref{table:routing_correlation} shows the Pearson correlation coefficient between the routing weights of the different routing mechanisms in each layer in a \slic\ {\em Large} model (MLP-routing correlation with KV-routing, MLP-routing with Q-routing, and KV-routing with Q-routing). We show numbers for both the pre-trained checkpoint, as well as a fine-tuned model on TriviaQA. As we can see, the routing of keys/values and routing of queries is highly correlated at all layers except the first two, while the routing of tokens in the MLP has lower correlation to the other two processes. Interestingly correlation between MLP and attention routing increases in the last layers of the model.


\begin{table}[H]
\centering
\footnotesize
\setlength{\tabcolsep}{1pt}
\begin{tabular}{|l|c|c|c||c|c|c|}
\hline
& \multicolumn{3}{c||}{Pre-trained} & \multicolumn{3}{c|}{Fine-tuned} \\ \hline
& {\bf MLP-KV} & {\bf MLP-Q} & {\bf KV-Q} & {\bf MLP-KV} & {\bf MLP-Q} & {\bf KV-Q} \\ \hline
 1 & \correlationBar{-0.06} &	\correlationBar{-0.06} &	\correlationBar{-0.09} &	\correlationBar{-0.06} &	\correlationBar{-0.09} &	\correlationBar{-0.26} \\
 2 & \correlationBar{0.27} &	\correlationBar{0.52} &	\correlationBar{0.04} &	\correlationBar{0.27} &	\correlationBar{0.39} &	\correlationBar{0.02} \\
 3 & \correlationBar{-0.05} &	\correlationBar{-0.03} &	\correlationBar{0.75} &	\correlationBar{0.05} &	\correlationBar{-0.01} &	\correlationBar{0.69} \\
 4 & \correlationBar{0.05} &	\correlationBar{0.09} &	\correlationBar{0.76} &	\correlationBar{0.18} &	\correlationBar{0.14} &	\correlationBar{0.72} \\
 5 & \correlationBar{0.02} &	\correlationBar{-0.01} &	\correlationBar{0.75} &	\correlationBar{0.22} &	\correlationBar{0.26} &	\correlationBar{0.68} \\
 6 & \correlationBar{0.02} &	\correlationBar{-0.01} &	\correlationBar{0.78} &	\correlationBar{0.31} &	\correlationBar{0.33} &	\correlationBar{0.70} \\
 7 & \correlationBar{0.02} &	\correlationBar{0.00} &	\correlationBar{0.73} &	\correlationBar{0.26} &	\correlationBar{0.27} &	\correlationBar{0.70} \\
 8 & \correlationBar{0.00} &	\correlationBar{-0.02} &	\correlationBar{0.44} &	\correlationBar{0.11} &	\correlationBar{-0.07} &	\correlationBar{0.29} \\
 9 & \correlationBar{0.13} &	\correlationBar{0.11} &	\correlationBar{0.74} &	\correlationBar{0.36} &	\correlationBar{0.40} &	\correlationBar{0.70} \\
10 & \correlationBar{-0.06} &	\correlationBar{-0.08} &	\correlationBar{0.08} &	\correlationBar{-0.15} &	\correlationBar{-0.15} &	\correlationBar{0.12} \\
11 & \correlationBar{-0.05} &	\correlationBar{-0.07} &	\correlationBar{0.31} &	\correlationBar{-0.08} &	\correlationBar{-0.03} &	\correlationBar{0.18} \\
12 & \correlationBar{-0.04} &	\correlationBar{-0.08} &	\correlationBar{0.27} &	\correlationBar{0.03} &	\correlationBar{0.00} &	\correlationBar{0.28} \\
13 & \correlationBar{-0.10} &	\correlationBar{-0.09} &	\correlationBar{0.87} &	\correlationBar{-0.13} &	\correlationBar{-0.03} &	\correlationBar{0.72} \\
14 & \correlationBar{-0.04} &	\correlationBar{-0.05} &	\correlationBar{0.76} &	\correlationBar{-0.06} &	\correlationBar{-0.12} &	\correlationBar{0.67} \\
15 & \correlationBar{0.53} &	\correlationBar{0.64} &	\correlationBar{0.69} &	\correlationBar{0.51} &	\correlationBar{0.55} &	\correlationBar{0.67} \\
16 & \correlationBar{0.08} &	\correlationBar{0.12} &	\correlationBar{0.63} &	\correlationBar{0.06} &	\correlationBar{0.57} &	\correlationBar{0.24} \\
17 & \correlationBar{0.28} &	\correlationBar{0.30} &	\correlationBar{0.65} &	\correlationBar{0.27} &	\correlationBar{0.32} &	\correlationBar{0.69} \\
18 & \correlationBar{0.28} &	\correlationBar{0.02} &	\correlationBar{0.84} &	\correlationBar{0.31} &	\correlationBar{0.20} &	\correlationBar{0.76} \\
19 & \correlationBar{0.45} &	\correlationBar{0.77} &	\correlationBar{0.59} &	\correlationBar{0.19} &	\correlationBar{0.38} &	\correlationBar{0.64} \\
20 & \correlationBar{0.30} &	\correlationBar{0.39} &	\correlationBar{0.64} &	\correlationBar{0.38} &	\correlationBar{0.47} &	\correlationBar{0.62} \\
21 &	\correlationBar{0.05} &	\correlationBar{-0.04} &	\correlationBar{0.49} &	\correlationBar{0.18} &	\correlationBar{0.11} &	\correlationBar{0.47} \\
22 & \correlationBar{0.05} &	\correlationBar{0.00} &	\correlationBar{0.69} &	\correlationBar{0.21} &	\correlationBar{0.16} &	\correlationBar{0.68} \\
23 & \correlationBar{0.39} &	\correlationBar{0.33} &	\correlationBar{0.68} &	\correlationBar{0.60} &	\correlationBar{0.79} &	\correlationBar{0.69} \\
24 & \correlationBar{0.43} &	\correlationBar{0.39} &	\correlationBar{0.59} &	\correlationBar{0.57} &	\correlationBar{0.63} &	\correlationBar{0.65} \\
\hline
\end{tabular}
\caption{Pearson correlation coefficient between the routing weights of the different routing mechanisms in each layer in a \slic\ {\em Large} model. We show numbers for both the pre-trained checkpoint, as well as a fine-tuned model on TriviaQA. Blue bars visualize positive correlation, whereas red bars visualize negative correlation.}
\label{table:routing_correlation}
\end{table}

\end{document}
