@misc{lei2023conditional,
      title={Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference}, 
      author={Tao Lei and Junwen Bai and Siddhartha Brahma and Joshua Ainslie and Kenton Lee and Yanqi Zhou and Nan Du and Vincent Y. Zhao and Yuexin Wu and Bo Li and Yu Zhang and Ming-Wei Chang},
      year={2023},
      eprint={2304.04947},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{raffel2020t5,
  author    = {Colin Raffel and
               Noam Shazeer and
               Adam Roberts and
               Katherine Lee and
               Sharan Narang and
               Michael Matena and
               Yanqi Zhou and
               Wei Li and
               Peter J. Liu},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
               Transformer},
  journal   = {J. Mach. Learn. Res.},
  volume    = {21},
  pages     = {140:1--140:67},
  year      = {2020},
  url       = {http://jmlr.org/papers/v21/20-074.html},
  timestamp = {Fri, 05 Feb 2021 15:43:41 +0100},
  biburl    = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Efficient attention/long inputs

@inproceedings{guo2022longt5,
    title = "{L}ong{T}5: {E}fficient Text-To-Text Transformer for Long Sequences",
    author = "Guo, Mandy  and
      Ainslie, Joshua  and
      Uthus, David  and
      Onta\~{n}\'{o}n, Santiago  and
      Ni, Jianmo  and
      Sung, Yun-Hsuan  and
      Yang, Yinfei",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.55",
    doi = "10.18653/v1/2022.findings-naacl.55",
    pages = "724--736",
    abstract = "Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present LongT5, a new model that explores the effects of scaling both the input length and model size at the same time. Specifically, we integrate attention ideas from long-input transformers (ETC), and adopt pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call Transient Global (TGlobal), which mimics ETC{'}s local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization and question answering tasks, as well as outperform the original T5 models on these tasks. We have open sourced our architecture and training code, as well as our pre-trained model checkpoints.",
}


@article{ainslie2020etc,
  title={{ETC}: Encoding long and structured inputs in transformers},
  author={Ainslie, Joshua and Onta\~{n}\'{o}n, Santiago and Alberti, Chris and Cvicek, Vaclav and Fisher, Zachary and Pham, Philip and Ravula, Anirudh and Sanghai, Sumit and Wang, Qifan and Yang, Li},
  url={https://arxiv.org/abs/2004.08483},
  journal={arXiv preprint arXiv:2004.08483},
  year={2020}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Onta\~{n}\'{o}n, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}


@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@inproceedings{zemlyanskiy2021readtwice,
  title={ReadTwice: Reading Very Large Documents with Memories},
  author={Zemlyanskiy, Yury and Ainslie, Joshua and de Jong, Michiel and Pham, Philip and Eckstein, Ilya and Sha, Fei},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={5189--5195},
  year={2021}
}

@article{ivgi2022sled,
  title={Efficient long-text understanding with short-text models},
  author={Ivgi, Maor and Shaham, Uri and Berant, Jonathan},
  journal={arXiv preprint arXiv:2208.00748},
  year={2022}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

% Utilization

@article{pope2022efficiently,
  title={Efficiently Scaling Transformer Inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={arXiv preprint arXiv:2211.05102},
  year={2022}
}

@article{dejong2022fido,
  title={Fi{DO}: Fusion-in-Decoder optimized for stronger performance and faster inference},
  author={de Jong, Michiel and Zemlyanskiy, Yury and Ainslie, Joshua and FitzGerald, Nicholas and Sanghai, Sumit and Sha, Fei and Cohen, William},
  url={https://arxiv.org/abs/2212.08153},
  journal={arXiv preprint arXiv:2212.08153},
  year={2022}
}

@article{shazeer2019mq,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

@article{kaplan2020scaling,
  author    = {Jared Kaplan and
               Sam McCandlish and
               Tom Henighan and
               Tom B. Brown and
               Benjamin Chess and
               Rewon Child and
               Scott Gray and
               Alec Radford and
               Jeffrey Wu and
               Dario Amodei},
  title     = {Scaling Laws for Neural Language Models},
  journal   = {CoRR},
  volume    = {abs/2001.08361},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.08361},
  eprinttype = {arXiv},
  eprint    = {2001.08361},
  timestamp = {Wed, 03 Jun 2020 10:55:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Sparsity

@inproceedings{shazeer2017moe,
  author    = {Noam Shazeer and
               Azalia Mirhoseini and
               Krzysztof Maziarz and
               Andy Davis and
               Quoc V. Le and
               Geoffrey E. Hinton and
               Jeff Dean},
  title     = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
               Layer},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=B1ckMDqlg},
  timestamp = {Thu, 25 Jul 2019 14:25:44 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ShazeerMMDLHD17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{fedus2021switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@inproceedings{fan2020layerdrop,
  author    = {Angela Fan and
               Edouard Grave and
               Armand Joulin},
  title     = {Reducing Transformer Depth on Demand with Structured Dropout},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=SylO2yStDr},
  timestamp = {Thu, 07 May 2020 17:11:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/FanGJ20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zoph2022stmoe,
  title={ST-MoE: Designing Stable and Transferable Sparse Expert Models},
  author={Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  journal={arXiv preprint arXiv:2202.08906},
  year={2022}
}

% Conditional compute

@article{schuster2022confident,
  title={Confident adaptive language modeling},
  author={Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh Q and Tay, Yi and Metzler, Donald},
  journal={arXiv preprint arXiv:2207.07061},
  year={2022}
}

@inproceedings{kgfid,
  author    = {Donghan Yu and
               Chenguang Zhu and
               Yuwei Fang and
               Wenhao Yu and
               Shuohang Wang and
               Yichong Xu and
               Xiang Ren and
               Yiming Yang and
               Michael Zeng},
  editor    = {Smaranda Muresan and
               Preslav Nakov and
               Aline Villavicencio},
  title     = {KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain
               Question Answering},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational
               Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
               May 22-27, 2022},
  pages     = {4961--4974},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://doi.org/10.18653/v1/2022.acl-long.340},
  doi       = {10.18653/v1/2022.acl-long.340},
  timestamp = {Mon, 01 Aug 2022 16:27:51 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/Yu0F0WXRY022.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{r3rerank,
  author    = {Shuohang Wang and
               Mo Yu and
               Xiaoxiao Guo and
               Zhiguo Wang and
               Tim Klinger and
               Wei Zhang and
               Shiyu Chang and
               Gerry Tesauro and
               Bowen Zhou and
               Jing Jiang},
  editor    = {Sheila A. McIlraith and
               Kilian Q. Weinberger},
  title     = {R\({}^{\mbox{3}}\): Reinforced Ranker-Reader for Open-Domain Question
               Answering},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {5981--5988},
  publisher = {{AAAI} Press},
  year      = {2018},
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712},
  timestamp = {Tue, 08 Mar 2022 21:46:35 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/WangYGWKZCTZJ18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{readerguidererank,
  author    = {Yuning Mao and
               Pengcheng He and
               Xiaodong Liu and
               Yelong Shen and
               Jianfeng Gao and
               Jiawei Han and
               Weizhu Chen},
  editor    = {Chengqing Zong and
               Fei Xia and
               Wenjie Li and
               Roberto Navigli},
  title     = {Reader-Guided Passage Reranking for Open-Domain Question Answering},
  booktitle = {Findings of the Association for Computational Linguistics: {ACL/IJCNLP}
               2021, Online Event, August 1-6, 2021},
  series    = {Findings of {ACL}},
  volume    = {{ACL/IJCNLP} 2021},
  pages     = {344--350},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.findings-acl.29},
  doi       = {10.18653/v1/2021.findings-acl.29},
  timestamp = {Mon, 30 May 2022 13:48:58 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/MaoHLSGHC21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{canext,
  author    = {Neeraj Varshney and
               Man Luo and
               Chitta Baral},
  title     = {Can Open-Domain {QA} Reader Utilize External Knowledge Efficiently
               like Humans?},
  journal   = {CoRR},
  volume    = {abs/2211.12707},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2211.12707},
  doi       = {10.48550/arXiv.2211.12707},
  eprinttype = {arXiv},
  eprint    = {2211.12707},
  timestamp = {Tue, 29 Nov 2022 17:41:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2211-12707.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{adaptiveretrieval,
  author    = {Bernhard Kratzwald and
               Stefan Feuerriegel},
  editor    = {Ellen Riloff and
               David Chiang and
               Julia Hockenmaier and
               Jun'ichi Tsujii},
  title     = {Adaptive Document Retrieval for Deep Question Answering},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural
               Language Processing, Brussels, Belgium, October 31 - November 4, 2018},
  pages     = {576--581},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://doi.org/10.18653/v1/d18-1055},
  doi       = {10.18653/v1/d18-1055},
  timestamp = {Fri, 06 Aug 2021 00:40:24 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/KratzwaldF18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% Long-range data

@article{shaham2022scrolls,
  title={SCROLLS: Standardized CompaRison Over Long Language Sequences},
  author={Uri Shaham and Elad Segal and Maor Ivgi and Avia Efrat and Ori Yoran and Adi Haviv and Ankit Gupta and Wenhan Xiong and Mor Geva and Jonathan Berant and Omer Levy},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.03533}
}

@inproceedings{tay2021long,
title={Long Range Arena : A Benchmark for Efficient Transformers },
author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qVyeW-grC2k}
}

@inproceedings{cohan2018arxiv,
    title = "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents",
    author = "Cohan, Arman  and
      Dernoncourt, Franck  and
      Kim, Doo Soon  and
      Bui, Trung  and
      Kim, Seokhwan  and
      Chang, Walter  and
      Goharian, Nazli",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2097",
    doi = "10.18653/v1/N18-2097",
    pages = "615--621",
    abstract = "Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single, longer-form documents (e.g., research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document, and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly outperforms state-of-the-art models.",
}

@InProceedings{joshi2017triviaqa,
     author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel S. and Zettlemoyer, Luke},
     title = {TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
     booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics},
     month = {July},
     year = {2017},
     address = {Vancouver, Canada},
     publisher = {Association for Computational Linguistics},
}

% Training Objectives

@inproceedings{zhang2020pegasus,
  title={Pegasus: Pre-training with extracted gap-sentences for abstractive summarization},
  author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle={International Conference on Machine Learning},
  pages={11328--11339},
  year={2020},
  organization={PMLR}
}

@article{tay2022ul2,
  title={Unifying Language Learning Paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  journal={arXiv preprint arXiv:2205.05131},
  year={2022}
}

@inproceedings{devlinbert2019,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1423},
  doi       = {10.18653/v1/n19-1423},
  timestamp = {Mon, 26 Sep 2022 12:21:55 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Software

@software{jax,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@software{flax,
  author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
  title = {{F}lax: A neural network library and ecosystem for {JAX}},
  url = {http://github.com/google/flax},
  version = {0.6.0},
  year = {2020},
}

@article{t5x,
  url = {https://arxiv.org/abs/2203.17189},
  author = {Roberts, Adam and Chung, Hyung Won and Levskaya, Anselm and Mishra, Gaurav and Bradbury, James and Andor, Daniel and Narang, Sharan and Lester, Brian and Gaffney, Colin and Mohiuddin, Afroz and Hawthorne, Curtis and Lewkowycz, Aitor and Salcianu, Alex and van Zee, Marc and Austin, Jacob and Goodman, Sebastian and Soares, Livio Baldini and Hu, Haitang and Tsvyashchenko, Sasha and Chowdhery, Aakanksha and Bastings, Jasmijn and Bulian, Jannis and Garcia, Xavier and Ni, Jianmo and Chen, Andrew and Kenealy, Kathleen and Clark, Jonathan H. and Lee, Stephan and Garrette, Dan and Lee-Thorp, James and Raffel, Colin and Shazeer, Noam and Ritter, Marvin and Bosma, Maarten and Passos, Alexandre and Maitin-Shepard, Jeremy and Fiedel, Noah and Omernick, Mark and Saeta, Brennan and Sepassi, Ryan and Spiridonov, Alexander and Newlan, Joshua and Gesmundo, Andrea},
  title = {Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$},
  journal={arXiv preprint arXiv:2203.17189},
  year = {2022},
}

@inproceedings{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{adafactor,
  author    = {Noam Shazeer and
               Mitchell Stern},
  editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {4603--4611},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/shazeer18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/ShazeerS18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{xprof,
  author = {Google},
  year = {2020},
  title = {{P}rofile your model with Cloud TPU tools},
  howpublished = {\url{https://cloud.google.com/tpu/docs/cloud-tpu-tools}},
  note = {Accessed: 2022-11-11}
} 

@misc{tpuv4,
  author = {Google},
  year = {2022},
  title = {{S}ystem Architecture TPU VM},
  howpublished = {\url{https://cloud.google.com/tpu/docs/system-architecture-tpu-vm}},
  note = {Accessed: 2022-11-19}
}

@article{qian2022multi,
  title={Multi-Vector Retrieval as Sparse Alignment},
  author={Qian, Yujie and Lee, Jinhyuk and Duddu, Sai Meher Karthik and Dai, Zhuyun and Brahma, Siddhartha and Naim, Iftekhar and Lei, Tao and Zhao, Vincent Y},
  journal={arXiv preprint arXiv:2211.01267},
  url={https://arxiv.org/abs/2211.01267},
  year={2022}
}


%  Data

@inproceedings{huang2021govreport,
    title = "Efficient Attentions for Long Document Summarization",
    author = "Huang, Luyang  and
      Cao, Shuyang  and
      Parulian, Nikolaus  and
      Ji, Heng  and
      Wang, Lu",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.112",
    doi = "10.18653/v1/2021.naacl-main.112",
    pages = "1419--1436"
}

@article{chen2021summscreen,
      title={SummScreen: A Dataset for Abstractive Screenplay Summarization}, 
      author={Mingda Chen and Zewei Chu and Sam Wiseman and Kevin Gimpel},
      year={2021},
      eprint={2104.07091},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{zhong2021qmsum,
    title = "{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization",
    author = "Zhong, Ming  and
      Yin, Da  and
      Yu, Tao  and
      Zaidi, Ahmad  and
      Mutuma, Mutethia  and
      Jha, Rahul  and
      Awadallah, Ahmed Hassan  and
      Celikyilmaz, Asli  and
      Liu, Yang  and
      Qiu, Xipeng  and
      Radev, Dragomir",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.472",
    doi = "10.18653/v1/2021.naacl-main.472",
    pages = "5905--5921"
}

@inproceedings{dasigi2021qasper,
    title = "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
    author = "Dasigi, Pradeep  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Cohan, Arman  and
      Smith, Noah A.  and
      Gardner, Matt",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.365",
    doi = "10.18653/v1/2021.naacl-main.365",
    pages = "4599--4610"
}

@article{kocisky2018narrativeqa,
    title = "The {N}arrative{QA} Reading Comprehension Challenge",
    author = "Ko{\v{c}}isk{\'y}, Tom{\'a}{\v{s}}  and
      Schwarz, Jonathan  and
      Blunsom, Phil  and
      Dyer, Chris  and
      Hermann, Karl Moritz  and
      Melis, G{\'a}bor  and
      Grefenstette, Edward",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1023",
    doi = "10.1162/tacl_a_00023",
    pages = "317--328"
}

@article{pang2021quality,
  title={{QuALITY}: Question Answering with Long Input Texts, Yes!},
  author={Pang, Richard Yuanzhe and Parrish, Alicia and Joshi, Nitish and Nangia, Nikita and Phang, Jason and Chen, Angelica and Padmakumar, Vishakh and Ma, Johnny and Thompson, Jana and He, He and Bowman, Samuel R.},
  journal={arXiv preprint arXiv:2112.08608},
  year={2021}
}

@inproceedings{koreeda-manning-2021-contractnli,
    title = "{C}ontract{NLI}: A Dataset for Document-level Natural Language Inference for Contracts",
    author = "Koreeda, Yuta  and
      Manning, Christopher",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.164",
    doi = "10.18653/v1/2021.findings-emnlp.164",
    pages = "1907--1919"
}

@article{kiwatkowski2019nq,
  author    = {Tom Kwiatkowski and
               Jennimaria Palomaki and
               Olivia Redfield and
               Michael Collins and
               Ankur P. Parikh and
               Chris Alberti and
               Danielle Epstein and
               Illia Polosukhin and
               Jacob Devlin and
               Kenton Lee and
               Kristina Toutanova and
               Llion Jones and
               Matthew Kelcey and
               Ming{-}Wei Chang and
               Andrew M. Dai and
               Jakob Uszkoreit and
               Quoc Le and
               Slav Petrov},
  title     = {Natural Questions: a Benchmark for Question Answering Research},
  journal   = {Trans. Assoc. Comput. Linguistics},
  volume    = {7},
  pages     = {452--466},
  year      = {2019},
  url       = {https://doi.org/10.1162/tacl\_a\_00276},
  doi       = {10.1162/tacl\_a\_00276},
  timestamp = {Tue, 16 Aug 2022 23:05:11 +0200},
  biburl    = {https://dblp.org/rec/journals/tacl/KwiatkowskiPRCP19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
