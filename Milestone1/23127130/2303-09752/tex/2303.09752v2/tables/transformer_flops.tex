
\begin{table}[h!]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Encoder Layer Component} & \textbf{Flops} \\
\midrule
Vanilla self-attention computation& $2 n^2 d$ \\
Attention QKV and output projections & $4 n d^2$ \\
Feedforward layer& $8 n d^2$ \\
\longt local attention computation & $2 nwd$\\
\longt global attention computation & $\frac{n^2 }{8}d$\\
\bottomrule
\end{tabular}

\caption{Computational cost of encoder layer transformer components measured in FLOPs. $n$ is the input length, $d$ is the model dimensionality, and $w$ is the size of the local attention window.}
\label{table:transformer_flops}
\end{table}