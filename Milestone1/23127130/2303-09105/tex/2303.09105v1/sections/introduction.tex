\section{Introduction}
\label{sec:intro}


Deep learning has achieved remarkable progress over the past decade in various computer vision tasks and has been widely deployed in real-world applications~\cite{lecun2015deep}. However, deep neural networks (DNNs) are vulnerable to adversarial examples \cite{fgsm,szegedy2013intriguing}, which are crafted by imposing human-imperceptible perturbations to natural examples. Adversarial examples can mislead the predictions of the victim model, posing great threats to the security of DNNs and their applications~\cite{adversarialexampleinphysicalworld, Sharif2016Accessorize}. The study of generating adversarial examples, known as adversarial attack, has attracted tremendous attention since it can provide a better understanding of the working mechanism of DNNs \cite{dong2017towards}, evaluate the robustness of different models~\cite{carlini2017towards}, and help to design more robust and reliable algorithms~\cite{pgd}.


\begin{figure}[]
\centering
\includegraphics[width=0.99\columnwidth]{./figures/figure1.pdf}
\caption{Black-box attack success rate (\%) on adversarially trained models by different algorithms. The proposed MI-CWA, which crafts the adversarial examples on an ensemble of models, outperforms existing methods in transfer attack with significant margins, because the optimized noises are at flat landscape and close to the local minima of victim models. The models are detailed in \cref{sec:classification}.}
\vspace{-1ex}
\label{figure:1}
\end{figure}


Adversarial attacks can be generally categorized into white-box attacks and black-box attacks according to the adversary's knowledge of the victim model. 
With limited information of the model, black-box attacks either rely on query feedback \cite{chen2017zoo} or leverage the transferability \cite{Liu2016} to generate adversarial examples. Particularly, transfer-based attacks generate adversarial examples against white-box surrogate models and directly use them to attack black-box models based on the transferability. Thus, they do not demand query access to the black-box models, making them more practical in numerous real-world scenarios \cite{MI,Liu2016}.
%Black-box attacks with limited information of the model are more practical and feasible in real-world scenarios and existing methods for black-box attacks are mostly query-based~\cite{dong2021query,dong2019efficient} or transfer-based~\cite{MI,vmi,DI}. 
%Considering the poor query efficiency and the potential limits of access, many studies focus on transfer-based attacks, which generate adversarial examples against surrogate white-box models and directly use them to attack black-box models. 
With the development of adversarial defenses~\cite{pgd,tramer2017ensemble,wong2020fast} and diverse network architectures~\cite{vit,liu2021swin,liu2022convnext}, the transferability of existing methods can be largely affected.



% Adversarial attacks can be divided into two categories, one is the white-box attack, where the attacker can access the gradient information of the model~\cite{pgd}, and the other is the black-box attack, where the attacker cannot access the gradient information of the model, which is more practical in real-world scenario, threatening the application of AI. Since the adversarial examples of a model can often be transferred to other models, black-box attacks often use this efficient and effective "transfer attack" algorithm~\cite{MI, DI, TI, vmi}. Consequently, Many researchers have made efforts to enhance the transferability of adversarial examples. \yichi{Shall we narrow down the question to boosting the transferability of adversarial examples generated from \textbf{ensembled models}?}





The transferability of adversarial examples generated on surrogate models is naturally analogous to the generalization of neural network training~\cite{MI}. Therefore, many researchers improve the transferability by designing advanced optimization algorithms to avoid undesirable optimum~\cite{MI, NI, vmi, emi} or leveraging data augmentation strategies to prevent overfitting~\cite{TI,NI,  DI}. Meanwhile, generating adversarial examples on multiple surrogate models can further improve the transferability, similar to training on more data to improve model generalization \cite{MI}. Early approaches simply average the outputs of multiple models in loss \cite{Liu2016} or in logits \cite{MI}, but ignore the different properties of them. %However, this method is less effective compared to modern attack methods, despite being simple and computationally cheap. 
To improve the transferability of adversarial attacks, a recent work \cite{svre} introduces the SVRG optimizer to reduce variance of gradients of different models during optimization. Nevertheless, recent studies \cite{yang2021trs} demonstrate that the variance of gradients does not always correlate with the generalization performance, highlighting the need for alternative approaches.
From the model perspective, increasing the number of surrogate models can reduce the generalization error
\cite{tsea}, and some studies \cite{tsea,li2020learning} propose to create surrogate models from existing ones. However, the number of surrogate models in adversarial attacks is usually limited due to the high computational costs, and the surrogate models created by heuristic methods do not satisfy the i.i.d. assumption, which limits their ability to improve the generalization (\ie, transferability).
%Although previous researchers propose many effective ensemble methods and theoretically analyze the ensemble method in adversarial attacks, the effectiveness of the ensemble in adversarial attacks may be more complicated.


% The transferability of adversarial examples generated on surrogate models naturally corresponds to the generalization of neural network training~\cite{MI}. Therefore, researchers turn to methods for better generalization, especially in the optimization with gradients. \cite{MI, NI} learn from the optimization of deep neural networks, introducing the momentum into the optimization of adversarial examples to have better transferability. \cite{vmi} considers gradient variance to stabilize the update direction and escape from poor local optima. Another popular paradigm is to apply input transformation to enhance the transferability. \cite{DI, TI} apply affine transformations (\eg, padding, resizing, translation) on the adversarial examples to boost the transferability. \cite{NI} also studies scale-invariance served as a model augmentation.\yinpeng{the above related works could be summarized. e.g., Many works improve the transferability of adversarial examples by designing advanced optimization algorithms or leveraging data augmentation strategies.}
%Meanwhile, generating adversarial examples on ensemble models can further improve the transferability and most of methods mentioned above aggregate with ensemble attack\yinpeng{why ensemble can help?}. Training of adversarial examples on one or multiple models can be analogized with empirical risk minimization (ERM)~\cite{erm} in model training. \cite{tsea} proves this and suggests that increasing the number of models can reduce the generalization error. However, the limited number of surrogate models could lead to large upper bound of the ERM generalization error and therefore worse transferability. \yinpeng{what are the limitations of existing method? e.g., they simply average models, but not consider the different characteristics of them?}

% Researchers have designed many excellent transfer attack algorithms by comparing the transferability of adversarial samples and the generalization of deep learning models. \cite{MI, NI} analogize to the optimization of deep learning models, introducing the momentum into the optimization of adversarial examples to help them avoid undesirable local minima to have better transferability. Inspired by the data augmentation methods in the training process of deep learning models, \cite{DI, TI} apply affine transformations on the adversarial examples to boost the transferability. \cite{DI, TI, MI, tsea} also introduce model ensemble into adversarial examples training. They ensemble models by averaging the loss, softmax probabilities, or logits. Although these methods achieve excellent results, this analogy could be more precise. \cite{tsea} points out that the training of the adversarial examples conforms to empirical risk minimization (ERM)~\cite{erm}, and increasing the number of models can reduce the generalization error, thereby increasing the transferability of adversarial samples. But in reality, the number of models in adversarial example training is often only a few or dozens, which is far from the thousands or millions of data in model training. In this case, the upper bound of the ERM's generalization error is so large that it becomes completely meaningless. 


In this paper, we rethink the ensemble method in adversarial attacks. By using a quadratic approximation of the expected objective over the victim models, we observe that the second-order term involves the Hessian matrix of the loss function and the squared $
\ell_2$ distance to the local optimum of each model, both of which are strongly correlated with the adversarial transferability (as illustrated in Fig.~\ref{figure:illustration}), especially when there are only few models in the ensemble. Based on these two terms, we define the \textbf{common weakness} of an ensemble of models as the solution that is at the flat landscape and close to the models' local optima. To generate adversarial examples that exploit the common weakness of model ensemble, we propose a \textbf{Common Weakness Attack (CWA)} composed of two sub-methods named Sharpness Aware Minimization (SAM) and Cosine Similarity Encourager (CSE), which are designed to optimize the two properties of common weakness separately. 
%Assuming the independence between the two terms, we empirically and theoretically validate their positive effects on the transferability, \ie, generalization of adversarial examples, and optimize these two terms individually. Based on these two terms, we propose a novel property of common weakness to illustrate whether the generated adversarial examples are at the flat landscape and close to the models' optima. This results in two sub-methods named Sharpness Aware Minimization (SAM)~\cite{SAM} for adversarial attack and Cosine Similarity Encourager (CSE), which can be combined together as Common Weakness Attacker (CWA). 
Since our methods are orthogonal to previous methods, \eg, MI \cite{MI}, TI~\cite{TI}, VMI~\cite{vmi} and Adam~\cite{kingma2014adam}, our methods can be incorporated with them seamlessly to achieve improved performance. 
%For instance, both MI-SAM and MI-CSE can significantly enhance the transferability of MI-FGSM and the performance is further improved when they are fused as MI-CWA. 

% In this paper, we propose Common Weakness Attack (CWA) to further boost the transferability of ensemble attack\yinpeng{what challenges do we address?}, by generating adversarial examples at flatter loss landscapes with closer victim model optima around them. Instead of training adversarial examples directly with empirical average objective over surrogate models, we use a quadratic approximation of the expected objective over the target models and focus on the second-order term which contains the Hessian matrix and the distance from the local optimum for each model. Assuming the independence between these two terms, we could theoretically and empirically prove their positive effects on transferability, \ie, generalization of adversarial examples, and optimize these two terms individually. This results in two sub-methods named Sharpness Aware Minimization (SAM)~\cite{SAM} for adversarial attack and Cosine Similarity Encourager (CSE), which can be combined together as CWA. Since the proposed methods is orthogonal to previous gradient-based or transformation-based methods like MI-FGSM~\cite{MI}, VMI-FGSM~\cite{vmi}, TI-FGSM~\cite{TI} and Adam~\cite{kingma2014adam}, they can be incorporated with them easily to achieve better performance. For instance, both MI-SAM and MI-CSE can significantly enhance the transferability of MI-FGSM and the performance is further improved when they are fused as MI-CWA. 



% In this work, we rethink the model ensemble in adversarial examples training. We discover that the upperbound of generalization error strongly correlated with the distance between the optimum of the ensemble model and each model and the Frobenius norm of the hessian matrix. We both theoretically and empirically show that the poor generalization of ERM is due to the inconsistency of training loss and test loss, \textbf{but the training loss and the testing loss of these two term discovered by us are strongly correlated, therefore they are strongly correlated with generalization error and adversarial example transferability}. Inspired by this, we propose a novel attack scheme, named Common Weakness Attackers(CWA), to boost the transferability of ensemble-attack. More specifically, CWA algorithm consist of two algorithm named SAM, CSE to minimize the Frobenius norm of hessian matrix and the distance between the optimum of ensemble model and each model respectively. Finally, we incorporate our algorithm with MI-FGSM~\cite{MI}, TI-FGSM~\cite{TI}, VMI-FGSM~\cite{vmi} and Adam optimizer~\cite{kingma2014adam} to further boosting the transferability of adversarial examples.


% We conduct comprehensive experiments to verify the attack effectiveness and efficiency of our method. We first apply ASC in the popular dataset COCO~\cite{lin2015microsoft} 
% %which is considered to be a generic dataset for object detection. We 
% and carry out adversarial attacks in three representative tasks, including object vanishing, object mislabeling and bounding box shift. We also test our method in auto-driving with two different datasets of CityScapes~\cite{Cordts2016Cityscapes} and BDD100K~\cite{bdd100k} under the task of object vanishing. Experimental results show that, with the same $\ell_0$ budget (\eg, no more than 5\% of the object area in the COCO case), we can achieve better attack results compared to other manually designed patterns, especially for two-stage detectors and DETR. The convergence curves and transfer-based attacks indicate that ASC achieves a successful attack with fewer iterations of texture optimization and better transferability in black-box adversarial attacks thanks to the appropriate contour priors. To our best knowledge, this is the first attempt to conduct $\ell_0$ attack on DETR. Our results suggest that Transformer-based detectors are vulnerable to perturbations with semantics.

We conduct extensive experiments to confirm the superior transferability of adversarial examples generated by our methods. We first verify in image classification on 24 victim models with various architectures (\eg, CNNs~\cite{resnet}, Transformers~\cite{vit,liu2021swin}) and training settings (\eg, standard training, adversarial training~\cite{salman2020adversarially,wong2020fast}). Impressively, when attacking the state-of-the-art defense models, MI-CWA surpasses MI with the logits ensemble strategy \cite{MI} by 30\% under the black-box setting, as shown in Fig.~\ref{figure:1}. We further extend our method to object detection by generating a universal adversarial patch that can achieve mAP of 9.85 on average over 8 modern detectors, surpassing the recent work~\cite{tsea}. Ablation studies also validate that our methods indeed flatten the loss landscape and increase the cosine similarity of gradients of different models. %and also discuss about the computational efficiency. 
The comprehensive quantitative analysis demonstrates the effectiveness of our proposed methods.
 
%\yichi{Do we need to list the contributions point-by-point?}

% We named our methods after Common Weakness Attackers (CWA). We test our algorithm on several settings, including attacking the state-of-the-art models on the imagenet, the defense models, and object detectors. Our algorithm outperforms the baseline a lot. More surprisingly, when attacking the state-of-the-art defense models, MI-CWA surpasses MI-FGSM 30\% at the black-box setting. Generally, our methods are easy to use and get better with more models. We hope our algorithm can be applied to real scenarios as an effective and straightforward attacker.


%In summary, our contributions are:
%\begin{itemize}
%    \item 
%\end{itemize}