\section{Experiments}



In this section, we conduct comprehensive experimental analysis of our methods, showing their superiority in transferability to previous methods. The tasks vary from image classification to object detection, which demonstrates the universality of our methods.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \usepackage{tabularray}
% \usepackage{tabularray}
% \usepackage{tabularray}
% \usepackage{tabularray}
% \usepackage{tabularray}
\begin{table*}
\centering
\resizebox{\linewidth}{!}{
\begin{tblr}{
colspec={llcccccccccc},
  cell{2}{1} = {r=16}{},
  cell{22}{1} = {r=2}{},
  cell{24}{1} = {r=2}{},
  vline{2-3} = {1-25}{},
  vline{3} = {3-25}{},
  hline{1-2,18,26} = {-}{},
  hline{19-22,24} = {1-2}{},
}
      Method &  Backbone                     & FGSM & BIM  & MI   & DI-MI & TI-MI & VMI  & MI-SVRE & MI-SAM & MI-CSE        & MI-CWA        \\
Normal & AlexNet               & 76.4 & 54.9 & 73.2 & 78.9  & 78.0  & 83.3 & 82.5    & 81.0   & 93.6          & \textbf{94.6} \\
& VGG-16                & 68.9 & 86.1 & 91.9 & 92.9  & 82.5  & 94.8 & 96.4    & 95.6   & \textbf{99.6} & 99.5          \\
& GoogleNet             & 54.4 & 76.6 & 89.1 & 92.0  & 77.8  & 94.2 & 95.7    & 94.4   & 98.8          & \textbf{99.0} \\
       & Inception-V3         & 54.5 & 64.9 & 84.6 & 89.0  & 75.7  & 91.1 & 92.6    & 89.2   & \textbf{97.3} & 97.2          \\
       & ResNet-152              & 54.5 & 96.0 & 96.6 & 93.8  & 87.8  & 97.1 & 99.0    & 97.9   & \textbf{99.9} & 99.8          \\
       & DenseNet-121           & 57.4 & 93.0 & 95.8 & 93.8  & 88.0  & 96.6 & 99.1    & 98.0   & \textbf{99.9} & 99.8          \\
       & SqueezeNet   & 85.0 & 80.4 & 89.4 & 92.9  & 85.8  & 94.2 & 96.1    & 94.1   & 99.1          & \textbf{99.3} \\
       & ShuffleNet-V2 & 81.2 & 65.3 & 79.9 & 85.7  & 78.2  & 89.9 & 90.3    & 87.9   & 97.2          & \textbf{97.3} \\
       & MobileNet-V3  & 58.9 & 55.6 & 71.8 & 78.6  & 74.5  & 87.3 & 80.6    & 80.7   & 94.6          & \textbf{95.7} \\
       & EfficientNet-B0   & 50.8 & 80.2 & 90.1 & 91.5  & 76.8  & 94.6 & 96.7    & 95.2   & 98.8          & \textbf{98.9} \\
       & MNasNet          & 64.1 & 80.8 & 88.8 & 91.5  & 75.5  & 94.1 & 94.2    & 94.3   & \textbf{99.1} & 98.7          \\
       & RegNetX-400MF    & 57.1 & 81.1 & 89.3 & 91.2  & 82.4  & 95.3 & 95.4    & 93.9   & 98.9          & \textbf{99.4} \\
       & ConvNeXt-T  & 39.8 & 68.6 & 81.6 & 85.4  & 56.2  & 92.4 & 88.2    & 90.1   & \textbf{96.2} & 95.4          \\
       & ViT-B/16  & 33.8 & 35.0 & 59.2 & 66.8  & 56.9  & 81.8 & 65.8    & 68.9   & \textbf{89.6} & \textbf{89.6} \\
       & Swin-S & 34.0 & 48.2 & 66.0 & 74.2  & 40.9  & 84.2 & 73.4    & 75.1   & \textbf{88.6} & 87.6          \\
       & MaxViT-T & 31.3 & 49.7 & 66.1 & 73.2  & 32.7  & 83.5 & 71.1    & 75.6   & 85.8          & \textbf{85.9} \\
       
FGSMAT \cite{adversarialMLAtScale}      & Inception-V3    & 53.9 & 43.4 & 55.9 & 61.8  & 66.1  & 72.3 & 66.8    & 64.5   & \textbf{89.6} & \textbf{89.6} \\
EnsAT \cite{tramer2017ensemble}      & IncRes-V2          & 32.5 & 28.5 & 42.5 & 52.9  & 58.5  & 66.4 & 46.8    & 47.9   & 78.2          & \textbf{79.1} \\
FastAT \cite{wong2020fast}      & ResNet-50                & 45.6 & 41.6 & 45.7 & 47.1  & 49.3  & 51.4 & 51.0    & 50.6   & \textbf{75.0} & 74.6          \\
PGDAT \cite{Engstrom2019Robustness}      & ResNet-50             & 36.3 & 30.9 & 37.4 & 38.0  & 43.9  & 47.1 & 43.9    & 43.9   & 73.5          & \textbf{73.6} \\
PGDAT \cite{salman2020adversarially}      & ResNet-18             & 46.8 & 41.0 & 45.7 & 47.7  & 50.7  & 48.9 & 48.5    & 48.0   & 68.4          & \textbf{69.5} \\
       & WRN-50-2              & 27.7 & 20.9 & 27.8 & 31.3  & 37.0  & 36.2 & 33.0    & 33.4   & 64.4          & \textbf{64.8} \\
PGDAT$^\dagger$ \cite{debenedetti2022light}      & XCiT-M12             & 23.0 & 16.4 & 22.8 & 25.4  & 29.4  & 33.4 & 30.2    & 31.8   & 77.5          & \textbf{77.8} \\
       & XCiT-L12             & 19.8 & 15.7 & 19.8 & 21.7  & 26.9  & 30.8 & 26.7    & 26.9   & 71.0          & \textbf{71.7} 
\end{tblr}
}
\vspace{0.2ex}
\caption{\textbf{Black-box attack success rate (\%,$\uparrow$) on NIPS2017 dataset.} While our methods lead the performance on 16 normally trained models with various architectures, they significantly empower the transfer-based attack with great margins on 8 adversarially trained models available on RobustBench \cite{croce2020robustbench}. Note that PGDAT$^\dagger$ \cite{debenedetti2022light} is a variant of PGDAT tuned by bag of tricks.}\vspace{-1ex}
\label{table:classification}
\end{table*}




\subsection{Attack in Image Classification}
\label{sec:classification}
% In this section, we conduct extensive experiments on image classification. We first share our setting about datasets, baseline, hyper-parameters. Then, we analyze the result of attacking the normal models and adversarially trained models.

% We also develop a framework for adversarial attacks on image classification. We will release our code as soon as our paper is accepted.




\textbf{Experimental Settings.} \textbf{(1) Dataset:} Similar to previous works, we use the NIPS2017 dataset\footnote{\url{https://www.kaggle.com/competitions/nips-2017-non-targeted-adversarial-attack}}, which is comprised of 1000 images compatible with ImageNet~\cite{russakovsky2015imagenet}. All the images are resized to $224\times224$. \textbf{(2) Surrogate Models:} We choose four normally trained models --- ResNet-18, ResNet-32, ResNet-50, ResNet-101 \cite{resnet} from Torchvision \cite{marcel2010torchvision}, and two adversarially trained model --- ResNet-50 \cite{salman2020adversarially}, XCiT-S12 \cite{debenedetti2022light} from RobustBench \cite{croce2020robustbench}. They contain both normal and robust models since we aim to attack black-box defenses, with reasonable sizes for training.  %There are many types of training models under this setting, which can reflect the ability of our algorithm to attack common weaknesses. 
We further conduct experiments with other surrogate models in~\cref{sec:moreexp}. \textbf{(3) Black-box Models:} We evaluate the performance of different attacks on 24 black-box models, including 16 normally trained models with different architectures --- AlexNet \cite{alexnet}, VGG-16 \cite{vgg}, GoogleNet \cite{googlenet}, Inception-V3 \cite{inception}, ResNet-152 \cite{resnet}, DenseNet-121 \cite{densenet}, SqueezeNet \cite{iandola2016squeezenet}, ShuffleNet-V2 \cite{ma2018shufflenet}, MobileNet-V3 \cite{mobilenet}, EfficientNet-B0 \cite{tan2019efficientnet}, MNasNet \cite{tan2019mnasnet}, ResNetX-400MF \cite{regnet}, ConvNeXt-T \cite{liu2022convnext}, ViT-B/16 \cite{vit}, Swin-S \cite{liu2021swin}, MaxViT-T \cite{tu2022maxvit}, and 8 adversarially trained models available on RobustBench \cite{croce2020robustbench} --- FGSMAT \cite{adversarialMLAtScale} with Inception-V3 , Ensemble AT (EnsAT) \cite{tramer2017ensemble} with Inception-ResNet-V2, FastAT \cite{wong2020fast} with ResNet-50, PGDAT \cite{Engstrom2019Robustness,salman2020adversarially} with ResNet-50, ResNet-18, Wide-ResNet-50-2, a variant of PGDAT tuned by bag-of-tricks (PGDAT$^\dagger$) \cite{debenedetti2022light} with XCiT-M12 and XCiT-L12. Most defense models are state-of-the-art on RobustBench \cite{croce2020robustbench}.
\textbf{(4) Compared Methods:} 
We compare our methods MI-SAM, MI-SCE and MI-CWA with FGSM \cite{fgsm}, BIM \cite{bim}, MI \cite{MI}, DI-MI \cite{DI}, TI-MI \cite{TI}, VMI \cite{vmi}, and MI-SVRE~\cite{svre}. All these attacks adopt the logits ensemble strategy following \cite{MI} for better performance. \textbf{(5) Hyper-parameters:}
We set the perturbation budget $\epsilon=16/255$, total iteration $T=10$, decay factor $\mu = 1$, step sizes $\beta=50$, $r=16/255/15$, and $\alpha=16/255/5$.



\textbf{Results on Normal Models.}
%For normally trained models, we select one for each type of model in torchvision, greatly increasing the diversity of our experiments. 
The results of black-box attacks are shown in the upper part of \cref{table:classification}.
Both MI-SAM and MI-CSE greatly improve the attack success rate compared with MI. Note that MI-CSW improves the attack success rate significantly if the black-box model is similar to one of the surrogate models. For example, when attacking ViT-B/16, the attack success rate of MI-CSE is nearly 30\% higher than MI. This is because MI-CSE attacks the common weakness of the surrogate models, so as long as there is any surrogate model similar to the black-box model, the attack success rate will be very high. While other methods cannot attack the common weakness of surrogate models, and the information of the only surrogate model which is similar to the black-box model will be overwhelmed by other surrogate models. Note that MI-SAM increases the attack success rate consistently, no matter whether the surrogate models are similar to the black-box model or not. This is because MI-SAM boosts the transferability of adversarial examples via making them converge into flatter optima.

When incorporating the MI-SAM into MI-CSE to form MI-CWA, the attack success rate is further improved, although the success rate of MI-CWA is lower than that of MI-CSE for some models. For the model in which MI-CWA increases the attack success rate compared with MI-CSE, the attack success rate tends to increase more. This is because MI-SAM and MI-CSE aim to optimize two different training objectives, and they are compatible, just as shown in \cref{theorem:objective2upperbound}. By incorporating these two algorithms into MI-CWA, we can not only let our adversarial examples converge into a flat region, but also be close to the optimum of each surrogate model. Therefore, MI-CWA further boosts the transferability of adversarial examples.




\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|cccccccc|c} 
\hline
Method & Surrogate       & YOLOv2    & YOLOv3    & YOLOv3-T & YOLOv4    & YOLOv4-T & YOLOv5    & FasterRCNN & SSD   & Avg.    \\ 
\hline
Single  &YOLOv3       & 54.63 & 12.35 & 53.99   & 58.20 & 53.38   & 69.21 & 50.81        & 58.13 & 51.34  \\
Single &YOLOv5       & 30.45 & 34.17 & 33.26   & 53.55 & 54.54   & 7.98  & 37.87        & 37.00 & 36.10  \\
Loss Ensemble & YOLOv3+YOLOv5 & 25.84 & 8.08  & 38.50   & 47.22 & 43.50   & 19.21 & 34.41        & 35.04 & 31.48  \\
Adam-CWA& YOLOv3+YOLOv5  & \bf{6.59}  & \bf{2.32}  & \bf{8.44}    & \bf{11.07} & \bf{8.33}    & \bf{2.06}  & \bf{14.41}        & \bf{25.56} & \bf{9.85}   \\
\hline
\end{tabular}}
\vspace{0.2ex}
\caption{\textbf{mAP (\%, $\downarrow$) of black-box detectors under attack on INRIA dataset.} The universal adversarial patch trained on YOLOv3 and YOLOv5 by Adam-CWA achieves the lowest mAPs on multiple modern detectors (9.85 on average) with large margins.}
\label{table:detection}
\end{table*}



\textbf{Results on Defense Models.}
For adversarially trained models, the results are shown in the lower part of \cref{table:classification}.
Since our method attacks the common weakness of surrogate models, we can make full use of the information of XCiT-S12 \cite{debenedetti2022light} to attack other adversarially trained XCiT models, without being interfered by four normally trained ResNets in the ensemble. Because \cite{wong2020fast}, \cite{Engstrom2019Robustness}, and \cite{salman2020adversarially} are adversarially trained ResNets, our algorithm significantly improves the attack success rate by 40\% over existing methods via making full use of the information of ResNets and defense models in the ensemble.



\subsection{Attack in Object Detection}
\begin{figure}[t]
\centering
\subfigure[YOLOv3 Only]{
\includegraphics[width=3.5cm]{./figures/v3.png}
}
\quad
\subfigure[YOLOv5 Only]{
\includegraphics[width=3.5cm]{./figures/v5.png}
}
\quad
\subfigure[Loss Ensemble]{
\includegraphics[width=3.5cm]{./figures/ensemble.png}
}
\quad
\subfigure[Adam-CWA with Ensemble]{
\includegraphics[width=3.5cm]{./figures/CW.png}
}
\caption{\textbf{Visualization of adversarial patches from different methods.} The patch simply trained by loss ensemble looks like the fusion of those trained by YOLOv3 and YOLOv5. Adam-CWA captures the common weakness of YOLOv3 and YOLOv5, and therefore generates an completely different patch.}
\vspace{-2ex}
\label{figure:patch}
\end{figure}

\textbf{Experimental Settings.} 
\textbf{(1) Dataset:} We generate universal adversarial patches on the training set of the INRIA dataset~\cite{INRIA} and evaluate them on black-box models on its test set, presenting challenges for adversarial patches to transfer across both different samples and various models.
\textbf{(2) Surrogate Models:} We choose YOLOv3~\cite{redmon2018yolov3} and YOLOv5~\cite{yolov5} as our surrogate models.
\textbf{(3) Testing Models:} We both evaluate adversarial patches on 2 white-box models metioned before and 6 black-box object detectors, including YOLOv2~\cite{redmon2017yolo9000}, YOLOv3-T~\cite{redmon2018yolov3}, YOLOv4, YOLOv4-T~\cite{bochkovskiy2020yolov4}, FasterRCNN~\cite{ren2015fasterrcnn}, and SSD~\cite{liu2016ssd}.
\textbf{(4) Compared Methods:}
Since the Adam method is often used to optimize adversarial patches for object detection~\cite{thys2019fooling,zhang2023make}, we first combine the proposed CWA with Adam to form Adam-CWA and compare its performance with patches trained by YOLOv3 only, YOLOv5 only, and Loss Ensemble of YOLOv3 and YOLOv5~(Enhanced Baseline in \cite{tsea}).
\textbf{(5) Training Procedure:} We follow the ``Enhanced Baseline'' settings in~\cite{tsea} including the image size, detector weight, the way to paste the patch on the image, learning rate, \etc.
 

\textbf{Results on Modern Detectors.}
As shown in \cref{table:detection}, our method exceeds the Loss Ensemble method by 20\% on average over all models and has a larger margin compared to patches generated on one detector. It is noticeable that the universal adversarial patch generated by Adam-CWA has even lower mAPs (2.32 on YOLOv3 and 2.06 on YOLOv5) on the two white-box models compared with results of white-box attacks (12.35 on YOLOv3 and 7.98 on YOLOv5), which means that our method boosts the transferability of adversarial patch not only between different models, but also between different samples.


\textbf{Visualization of Adversarial Patches.} As illustrated in \cref{figure:patch}, we observe that YOLOv5 is significantly more susceptible to adversarial attacks compared to YOLOv3. Therefore the patch generated by the Loss Ensemble method closely resembles the one obtained by YOLOv5, resulting in similar performance for both patches. We hypothesize that the Loss Ensemble method does not attack the common weakness of YOLOv3 and YOLOv5, and instead nearly solely relies on information from YOLOv5. Our proposed method, on the other hand, aims to exploit this common vulnerability and generates a patch that differs significantly from both YOLOv3 and YOLOv5. As a result, our patch achieves a much higher level of effectiveness in attacking the detection model.


% \subsubsection{Strongest Adversarial Patch}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=3.5cm]{iccv2023AuthorKit/figures/strongest_patch.png}
%     \caption{Strongest patch trained by Adam-CWA algorithm.}
%     \label{fig:my_label}
% \label{figure:strongestpatch}
% \end{figure}

% In order to craft the strongest universal adversarial patch on object detection, we ensemble all the models in \cite{tsea} and train by our CWA algorithm. The patch is visualized in \cref{figure:strongestpatch}.
% Our patch outperforms previous patches by a large margin. Compared with the previous state-of-the-art methods, the loss ensemble by enhanced baseline in \cite{tsea}, our approach improves by 4.26\%, achieving 4.69\% mAP on eight testing models in \cref{table:detection}.\yichi{write this result in table 2.}











\subsection{Additional Experiments and Discussions}

In this section, we conduct some additional experiments to prove our claims about the effectiveness of our methods, with respect to flattening the loss landscapes, maximizing cosine similarity of gradients to boost the transferability, computational efficiency and extension to other algorithms.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\centering
\subfigure[MI]{
\includegraphics[width=5.5cm]{figures/MI-FGSM.pdf}
\label{figure:milandscape}
}
\subfigure[MI-CWA]{
\includegraphics[width=5.5cm]{figures/MI-CW.pdf}
\label{figure:cwlandscape}
}
\subfigure[$T$]{
\includegraphics[width=5.5cm]{figures/T.pdf}
\label{figure:ablateT}
}
\caption{\textbf{Additional results. }(a-b): The loss landscape around the convergence point optimized by MI and MI-CWA respectively of different models. (c): Attack success rate under different attack iterations of $T$.}
\end{figure*}









%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-2ex}
\subsubsection{Visualization of Landscape}
\label{sec:visualizelandscape}
\vspace{-1ex}

We first illustrate our claim about the relationship between the loss landscape and the transferability of adversarial examples by visualizing the landscapes by different algorithms. In this part, we generate the adversarial example on the ensemble of six surrogate models used in~\cref{sec:classification} and test on all the defense models in RobustBench~\cite{croce2020robustbench}.

% In this sec\yichi{}we visualize the landscape of adversarial examples trained by different algorithms. The training model of each algorithm is the six models mentioned in \cref{sec:classification}, and the testing model is all the defense models mentioned in \cref{sec:classification}.


For each algorithm, we first craft an adversarial example $x$ via this algorithm, which is expected to be the convergence point during training and near the optimum of each black-box model. Then, we use BIM~\cite{bim} to fine-tune the adversarial example on each black-box model to get the position of the optimum of each model $p_i$ (note that this process is white-box). To visualize the loss landscapes of test models around $x$, we perturb $x$ along the unit direction $(p_i - x)/||p_i-x||_{\infty}$ for each model and plot the loss curves in one plane. The results of MI and our method MI-CWA are shown \cref{figure:milandscape} and \cref{figure:cwlandscape} respectively.

% \yichi{rewritten till here.}
As shown in the figures, the testing losses of MI-CWA on each model are smaller than those of MI and the landscapes for each model around the convergence point by MI-CWA are flatter than MI, which leads to better transferability of adversarial examples. This supports our claim that CWA can converge at a flatter loss landscape and thus lead to better transferability. It is also noticeable that when we shift the plot of MI-CWA to its left, it looks similar to the plot of MI, indicating the optimum of the ensemble (adversarial example) by MI-CWA is closer to each model's optimum, which corresponds to the term $||x-p_i||_2^2$ in~\cref{eq:upperbound}.



\vspace{-2ex}
\subsubsection{Cosine Similarity of Gradients}
\label{sec:cosinesimilarity}
\vspace{-1ex}

We also measure the average cosine similarity of the gradients between training (surrogate) models and testing (black-box) models. The results are shown in \cref{table:cosine}. Our method improves the cosine similarity of gradients both between training models and testing models compared with MI. It shows that our method is more likely to find the common weakness of the training models, and the common weakness of the training models tends to generalize to testing models.

\begin{table}[t]
\centering
\begin{tabular}{l|ll}
\hline
                          & MI & MI-CWA  \\ 
\hline
$\mathcal{F}_{train}$x$\mathcal{F}_{train}$ & 0.185   & 0.193  \\
$\mathcal{F}_{train}$x$\mathcal{F}$            & 0.024   & 0.032  \\
$\mathcal{F}$x$\mathcal{F}$                      & 0.061   & 0.076  \\
\hline
\end{tabular}
\vspace{0.5ex}
\caption{\textbf{Cosine Similarity of gradients.} MI-CWA universally increases the cosine similarity of gradients among models, regardless of whether they are from $\mathcal{F}_{train}$ or $\mathcal{F}$.}
\vspace{-2ex}
\label{table:cosine}
\end{table}




















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-2ex}
\subsubsection{Computational Efficiency}
\vspace{-1ex}

When the total number of  iterations $T$ as in~\cref{alg:mi-cwa} remains the same, the proposed CWA algorithm requires twice the computational cost of MI and TI-MI, so it is unfair to compare with other algorithms directly. Therefore, we measure the average attack success rate among the 24 models in \cref{table:classification} of the adversarial examples crafted by different algorithms under different $T$. The result is shown in \cref{figure:ablateT}. Our algorithm outperforms other algorithms for any number of iterations, even if $T=5$ of the CWA algorithm. This indicates that the CWA algorithm is effective not because of the large number of iterations, but because it captures the common weakness of different models.

\vspace{-2ex}
\subsubsection{VMI-CWA and TI-CWA}
\vspace{-1ex}

To demonstrate the flexibility of our method to adapt with existing attack methods, we also combine our algorithm with VMI~\cite{vmi} and TI-MI~\cite{TI} under the same settings as~\cref{sec:classification}. The results are listed in~\cref{sec:moreexp}. In most cases, the combined algorithms exceed both MI-CWA algorithm and original TI-MI, VMI algorithms. However, VMI and TI weaken the ability of the Cosine Similarity Encourager to approximate the second-order differential, so there exists some cases where the performance is slightly inferior. Due to the high time complexity of VMI, such combinations have little practical significance. Therefore, in this paper, we mainly use MI-CWA in the classification task and Adam-CWA in the detection task since it is easier for an optimizer with second-order momentum to optimize the universal patch for object detection~\cite{tsea}.

