\section{Methodology}
\label{sec:method}

In this section, we present our formulation of common weakness based on a second-order approximation and propose the \textbf{Common Weakness Attack (CWA)} composed of Sharp Aware Minimization (SAM) and Cosine Similarity Encourager (CSE).

\begin{figure}[t]
\centering
\subfigure[Not flat and not close]{
\includegraphics[width=3.6cm]{./figures/notflatnotclose.pdf}
\label{figure:notflatnotclose}
}
\quad
\subfigure[Flat and not close]{
\includegraphics[width=3.6cm]{./figures/flatnotclose.pdf}
\label{figure:flatnotclose}
}
\quad
\subfigure[Not flat and close]{
\includegraphics[width=3.6cm]{./figures/notflatclose.pdf}
\label{figure:notflatclose}
}
\quad
\subfigure[Flat and close]{
\includegraphics[width=3.6cm]{./figures/flatclose.pdf}
\label{figure:flatclose}
}
\caption{\textbf{Illustration of Common Weakness.} The generalization error is strongly correlated with the flatness of loss landscape and the distance between the solution and the closest local optimum of each model. We define common weakness of model ensemble as the solution that is at the flat landscape and close to local optima of training models, as shown in (d).}
\label{figure:illustration}
\end{figure}

\subsection{Motivation of Common Weakness}
\label{sec:motivation}

We let $\mathcal{F}$ denote the set of all image classifiers. Given a natural image $x_{nat}$ and the corresponding label $y$, the goal of transfer-based attacks is to craft an adversarial example $x$ that is misclassified by all models in $\mathcal{F}$. It can be formulated as a constrained optimization problem: 
%minimizing $E_{f_i \in \mathcal{F}}[L(f_i(x), y)]$, but with the constraint $\|x-x_{benign}\|<\epsilon$. Here $\|\cdot\|$ denote any norm distance. \yichi{problem formulation, do not use inline formula.}%
\begin{equation}
    \min_{x} \mathbb{E}_{f\in \mathcal{F}}[L(f(x),y)],\;\text{s.t. }\|x-x_{nat}\|_\infty \leq \epsilon, 
\label{eq:formulation}
\end{equation}
where $L$ is the loss function (\eg, negative cross-entropy loss), and we consider the $\ell_\infty$-norm threat model. 
The objective~\eqref{eq:formulation} is generally approximated by using a few ``training'' classifiers $\mathcal{F}_t\subset \mathcal{F}$ as $\frac{1}{n} \sum_{f \in \mathcal{F}_{t}} L(f(x),y)$ with $n$ being the number of classifiers in $\mathcal{F}_t$. Previous works~\cite{MI,TI,vmi,DI} propose methods of gradient computation or input transformation based on this empirical loss for better transferability. However, a recent work \cite{tsea} points out that the optimization of adversarial example conforms to Empirical Risk Minimization (ERM)~\cite{erm} and the limited number of training models could lead to a large generalization error.

Differently, we consider a quadratic approximation of the objective \eqref{eq:formulation}. By doing so, the flatness of landscape and the closeness between optima of different models, which are proven beneficial for the generalization and transferability of the trained adversarial example, appear explicitly in the second-order term. \cref{figure:illustration} provides a visual illustration of the impacts on generalization from the flatness of landscape and closeness between optima -- a solution at a flatter landscape with closer optima around it results in smaller generalization error (\ie, better transferability), which makes up the proposed concept of \textbf{common weakness} among victim models. 



%During training, some researchers use $\frac{1}{n} \sum_{f \in \mathcal{F}_{t}} L(f(x),y)$ as objective function to approximate the expectation over the universal set, where n is the cardinality of $\mathcal{F}_{t}$. Instead of only focusing on the loss value which is the first-order information\yichi{this is not first-order by definition in taylor-expansion?} during training, we furthermore pay attention to second-order information during optimization, such as the flatness of the landscape, and the distance between optimum of each model. Comparing \cref{figure:notflatnotclose} and \cref{figure:flatnotclose}, the flatter region during training tends to also be flat during testing, leading to smaller testing error. Comparing \cref{figure:notflatnotclose} and \cref{figure:notflatclose}, the optima of each testing models tend to be close if the optimum of each training model is close, which means the optimum of each testing model also close to our convergence point, leading to smaller testing error. In general, paying attention to the first-order information and second-order information during optimization at the same time will get better results than only focusing on the first-order information.


% \begin{theorem}
% \label{thm:bigtheorem}
% Denote $d$ is a complexity measuring of hypothesis space, with the probability $1-\sigma$ satisfying:
% \begin{equation}
%     \begin{aligned}
%         E_{f \in \mathcal{F}}[L(f(x), y)] \leq &   \frac{1}{n}\sum_{f \in {\mathcal{F}_{t}}} L( f(x),y) + \sqrt{ \frac{\log \frac{d}{\sigma}}{2n}}
%     \end{aligned}
% \end{equation}
% \label{theorem:erm}
% \end{theorem}

% Theorem \cref{theorem:erm} indicate that more training models tend to lead a better transferability of adversarial examples when the hypothesis space of adversarial examples remains the same. In most of the scenarios, the number of models in adversarial examples training is much smaller than the number of data in model training, which means that simply ensemble these model still has a large upper bound of generalization error. In order to give an more tight upper bound and make full use of each model's information instead of just ensemble them by loss or logit, we want to find the relationship between each model's local optimum and the ensemble model's optimum. \yichi{Motivation Declaration. We do not make the bound smaller. Rather, we use more information of each model. (what is more information)}


% Formally, let's denote the local optimum of $E_{f \in \mathcal{F}_{t}}[L(f(x), y)]$ we converge into as $x$, the optimum of each model $f_i \in \mathcal{F}_{t}$ closest to $x$ form a set $O_{c_{t}}$. Denote the optimum of $E_{f \in \mathcal{F}}[L(f(x), y)]$ closest to $x$ as $c$\yinpeng{why do we need $c$ to be closest to $x$?}, the optimum of each model $f_i \in \mathcal{F}$ closest to $c$ form a set $O_{c}$, and denote the hessian matrix of $L(f_i(x), y)$ at $p_i$ as $H_i$. We use Taylor expansion to expand our objective at $p_i \in O_{c}$ for each model $f_i$ in the expectation:\yichi{consider the notation. Make clear why we expand every term at their own optimum (make full use of their information, focus on the second-order term)}

Formally, we let $p_i$ denote the closest optimum of model $f_i \in \mathcal{F}$ to $x$ and $H_i$ denote the Hessian matrix of $L(f_i(x), y)$ at $p_i$. We employ the second-order Taylor expansion to approximate the objective~\eqref{eq:formulation} at $p_i$ for each model $f_i$ as
\begin{equation}
    \mathbb{E}_{f_i \in \mathcal{F}}\left[L(f_i(p_i), y)+\frac{1}{2}(x-p_i)^\top H_i(x-p_i)\right].
    %+o(\|(x-p_i)\|^3)
\label{eq:expansion}
\end{equation}

In the following, we  omit the subscript of the expectation in Eq.~\eqref{eq:expansion} for simplicity.
Based on Eq.~\eqref{eq:expansion} , we can see that a smaller value of $\mathbb{E}[L(f_i(p_i), y)]$ and $\mathbb{E}[(x-p_i)^\top H_i(x-p_i)]$ means a smaller test error. The first term represents the loss value of each model at its own optimum $p_i$. Some previous works~\cite{localisglobal, alllocalareglobal} have proven that the local optima have nearly the same value with the global optimum in neural networks. Consequently, there may be no room for further improving this term, and we put our main focus on the second term to achieve better transferability. 
%We aim to minimize the second term to achieve better transferability. 

In the following theorem, we derive an upper bound for the second term, since directly optimizing it which requires third-order derivative is intractable. 
\begin{theorem}
\label{theorem:upperbound}
(Proof in \cref{sec:upperboundproof}) Assume that the covariance of $H_i$ and $\|p_i-x\|_2$ is zero, we can get the upper bound of the second term as
    \begin{equation}
    \mathbb{E}[(x-p_i)^\top H_i(x-p_i)] \leq \mathbb{E}[\|H_i\|_F]\mathbb{E}[\|(x-p_i)\|_2^2].
    \label{eq:upperbound}
\end{equation}
\end{theorem}

Intuitively, $\|H_i\|_F$ represents the sharpness/flatness of loss landscape~\cite{losslandscapeandoptimizationinoverparameterized,normalizedflatminima,tu2016low} and $\|(x-p_i)\|_2^2$ describes the translation of landscape. The two terms can be assumed independent and therefore their covariance can be assumed as zero. 
As shown in \cref{theorem:upperbound}, a smaller value of $\mathbb{E}[\|H_i\|_F]$ and $\mathbb{E}[\|(x-p_i)\|_2^2]$ provides a smaller bound and further leads to a smaller test error. Previous works have pointed out that a smaller norm of the Hessian matrix means a flatter landscape of the training objective, which is strongly correlated with better generalization~\cite{chen2022bootstrap,visualoss, wu2017toward}. As for $\mathbb{E}[\|(x-p_i)\|_2^2]$ 
representing the squared $\ell_2$ distance from $x$ to the closest optimum of each model, we empirically show that it also has a tight connection with generalization and adversarial transferability in~\cref{sec:visualizelandscape} and theoretically prove this under a simple case in~\cref{appedixA}.
\cref{figure:illustration} shows that the flatness of loss landscape and closeness between local optima of different models can help with adversarial transferability.
% The first term is the hessian matrix of the loss function of each model at their own optimum, which represents the flatness of the landscape, and lots of previous works have point out that the landscape of training objective strongly correlated with the generalization error~\cite{visualoss, wu2017toward, chen2022bootstrap}. The second term represent the distance between the optimum of the ensemble model and the optimum of each model. This term also strongly correlated with generalization error and adversarial transferability. We empirically shows it at section \cref{sec:visualizelandscape}, and theoretically prove it under simple case \cref{assumption} at appendix \cref{appedixA}. 

% Both term are important properties of loss landscape, both are second-order information during optimization, and both have a strong correlation with generalization. Therefore, it is very reasonable to consider these two terms are irrelevant, and minimize these two items respectively.\yichi{is this paragraph to explain the assumption that the covariance is zero? Not coherent in the context}

% \textbf{In summary, if we converge to a local minima where the landscape is flat and closed to the local minimums of each model during training, then there is a high probability that the landscape around this local optimum is also flat during testing, and the local minimums of each testing model is also closed to this minima. In this case, the testing loss will be small, therefore we will have an small generalization error.}



% \yinpeng{This is the key! The new objective in (3) is also estimated by few models, why it generalize better than ERM? Try to provide some intuition or explain it with previous works, if our theory does not fit to the main paper.}


Based on the analysis above, the optimization of an adversarial example turns into looking for a point near the optima of victim models (\ie, minimizing the original objective), while pursuing that the landscape at the point is flat and the distance from it to each optimum is close.
The latter two targets are our main findings and we here define the concept of \textbf{common weakness} with these two terms as a local point $x$ who has a small value of $\mathbb{E}[\|H_i\|_F]$ and $\mathbb{E}[\|(x-p_i)\|_2^2]$. Note that there is no clear boundary between common weakness and non-common weakness -- the smaller these two terms are, the more likely $x$ to be the common weakness. The ultimate goal is to find an example that has the properties of common weakness. As~\cref{theorem:upperbound} indicates, we can achieve this by optimizing these two terms separately.



% All in all, the transfer attack performance of adversarial examples is related to three factors: the loss value, the sharpness of the landscape, and the distance from the minimum point of each model to the convergence point. Inspired by this insight, we define common weakness as local optimum $c$ who has a small value of $E_{f_i \in \mathcal{F}}[\|H_i\|_F]$ and $E_{p_i \in O_{x}}[\|(c-p_i)\|_2^2]$. At training time, we use the common weakness of training model to estimate the common weakness of all the model. Note that there is no clear boundary between common weakness and non-common weakness. The smaller $E_{f_i \in \mathcal{F}}[\|H_i\|_F]$ and $E_{p_i \in O_{x}}[\|(c-p_i)\|_2^2]$ is, the more common weakness $c$ is. 





\subsection{Sharpness Aware Minimization}

To cope with the flatness of loss landscape, we minimize $\|H_i\|_F$ for each training model in the ensemble. However, this requires third-order derivative \wrt $x$, which is computationally expensive. There are some researches to ease the sharpness of loss landscape in model training \cite{SAM, swa,kwon2021asam} and we adopt the Sharpness Aware Minimization (SAM) \cite{SAM} algorithm to acquire a flatter landscape, which will naturally lead to smaller Frobenius norm of the Hessian matrix.

In the context of generating adversarial examples, which is restricted by the $\ell_\infty$ norm, we aim to optimize the flatness of landscape in the space of $\ell_\infty$ norm to boost the transferability, which is different from the original SAM in the space of $\ell_2$ norm. Therefore, we derive a modified SAM algorithm suitable for the $\ell_\infty$ norm (in \cref{appendixB}).
As shown in \cref{fig:samillustration}, at the $t$-th iteration of adversarial attacks, the SAM algorithm first performs a gradient ascent step at the current adversarial example $x_t$ with a step size $r$ as
\begin{equation*}
x_{t}^{r}=\mathrm{clip}_{x_{nat},\epsilon}\left(x_{t}+r\cdot \mathrm{sign}\Big(\nabla_x L\Big(\frac{1}{n}\sum_{i=1}^{n} f_i(x_{t}),y\Big)\Big)\right),
\end{equation*}
where $\mathrm{clip}_{x_{nat},\epsilon}(x)$ would project a point $x$ to the $\ell_\infty$ ball around $x_{nat}$ with radius $\epsilon$. The SAM algorithm then performs a gradient descent step at $x_t^r$ with a step size $\alpha$ as
\begin{equation*}
x_{t}^f=\mathrm{clip}_{x_{nat},\epsilon}\left(x_{t}^{r}-\alpha \cdot \mathrm{sign}\Big(\nabla_x L\Big(\frac{1}{n}\sum_{i=1}^{n} f_i(x_{t}^{r}),y\Big)\Big)\right).
\end{equation*}

Note that in the updates of the SAM algorithm, we apply SAM on the model ensemble rather than each training model. In this way, we can not only perform parallel computation during the backward pass to improve efficiency, but also obtain better results using the logits ensemble strategy~\cite{MI}.

Moreover, we can combine the reverse step and forward step of SAM as a single update direction $x_{t}^{f}-x_{t}$, and integrate it into existing attack algorithms. For example, the integration of MI and SAM yields the MI-SAM algorithm with the following update (as shown in \cref{figure:samoptimizingmisam})
\begin{equation}
    m = \mu \cdot m + x_{t}^{f}-x_{t}; \;\; x_{t+1} = \mathrm{clip}_{x_{nat},\epsilon}(x_t + m),
\end{equation}
where $m$ accumulates the gradients with a decay factor $\mu$. By iteratively repeating this procedure, the generated adversarial example will converge into a flatter loss landscape, leading to better transferability of adversarial examples.
%\yichi{the explanation can be improved}.



% Instead of optimizing $E_{f_i \in F_{train}}[\|H_i\|_F]$, which requires third order derivative with respect to $x$, we adjust Sharpness Aware Minimization(SAM)\cite{SAM} algorithm to infinity norm so as to bootstrap our adversarial examples converge into flat optimum. The derivation of SAM algorithm in infinity norm is in Appendix \cref{appendixB}. We also combine SAM with MI-FGSM\cite{MI} to get MI-SAM algorithm. 





\begin{figure}[t]
    \centering
    \subfigure[MI]{
\includegraphics[width=2.3cm]{./figures/samoptimizingmi.pdf}
\label{figure:samoptimizingmi}
}\hspace{-2ex}
\subfigure[SAM]{
\includegraphics[width=2.3cm]{./figures/samoptimizingsam.pdf}
\label{figure:samoptimizingsam}
}\hspace{-2ex}
\subfigure[MI-SAM]{
\includegraphics[width=3.4cm]{./figures/samoptimizingmisam.pdf}
\label{figure:samoptimizingmisam}
}\caption{\textbf{Illustration of MI \cite{MI}, SAM, and MI-SAM algorithms.}} %The black arrow represents the final direction of each update.}
\vspace{-1ex}
    \label{fig:samillustration}
\end{figure}

 
%A straightforward explanation is that when a model is much easier to attack than other models, the calculated gradient will be dominated by it, leading to bad result.

% Here we provide an comprehension for why ensemble by logit averaging tends to perform better than loss averaging. Suppose we are using cross entropy loss and averaging loss of training model as our training objective, and the softmax probability of two training model is denoted as $pre_1$ and $pre_2$, loss of each model is denoted as $L_1$ and $L_2$. We can derive the gradient with respect to the input as:

% \begin{equation*}
%     \begin{aligned}
%         &\frac{\partial}{\partial p} (L_1+L_2) \\
% =& \frac{\partial L_1}{\partial logit} \frac{\partial logit}{\partial p} + \frac{\partial L_2}{\partial logit} \frac{\partial logit}{\partial p} \\
% =&(pre_1-y)\frac{\partial logit}{\partial p}+(pre_2-y)\frac{\partial logit}{\partial p}
%     \end{aligned}
% \end{equation*}

% When the first model is much hard to attack than the second model, which means $\|pre_1-y\| \ll \|pre_2-y\|$, therefore $\|\frac{\partial}{\partial p} L_1\| \ll  \|\frac{\partial}{\partial p} L_2\|$, the adversarial examples will be dominated by the second model, just like adversarial patches shown in figure \cref{figure:patch}.

% The intuition of SAM algorithm is that, the first step aims to find the worst case in the range of infinity norm $\epsilon$. The second step aims to optimize the worst case. By repeating these procedure, the worst case of final optimum is much better than directly optimizing the objective function. Therefore the landscape is flatter than other algorithm. Note that the SAM algorithm does not require multiple training models, it is also effective when there is only one training model.


\subsection{Cosine Similarity Encourager}
We then design an algorithm to let the adversarial example converge to a point which is close to the local optimum of each model. Instead of directly optimizing $\frac{1}{n}\sum_{i=1}^n\|(x-p_i)\|_2^2$ with the training models in the ensemble, since it is hard to compute the gradient w.r.t. $x$, we derive an upper bound for this loss. 

\begin{theorem}
\label{theorem:objective2upperbound}
    (Proof in \cref{appendixC}) The upper bound of $\frac{1}{n}\sum_{i=1}^n\|(x-p_i)\|_2^2$ is proportional to the dot product similarity between the gradients of all models:
    \begin{equation}
        \begin{aligned}
        \frac{1}{n}\sum_{i=1}^n\|(x-p_i)\|_2^2 \leq -\frac{2M}{n}\sum_{i=1}^{n}\sum_{j=1}^{i-1}g_i g_j,
    \end{aligned}
    \end{equation}
    where $M=\max \|H_i^{-1}\|_F^2$.
    %\yinpeng{is the bound tight and under what conditions?}
\end{theorem}

Based on \cref{theorem:objective2upperbound}, the minimization of the distance to local optimum of each model turns into the maximization of dot product between gradients of different models. To solve this problem, \cite{MAML} proposes an efficient algorithm via first-order derivative approximation. We apply this algorithm to generate adversarial examples by successively performing gradient updates using each model $f_i$ sampled from the ensemble $\mathcal{F}_{t}$ with a small step size $\beta$. The update process is
\begin{equation*}
\begin{aligned}
&x_{t}^{i}=\mathrm{clip}_{x_{nat},\epsilon}(x_{t}^{i-1}-\beta \cdot \nabla_x L(f_i(x_{t}^{i-1}),y)),
\end{aligned}
\end{equation*}
where $x_{t}^0=x_{t}$.
Once the update for every model is complete, we calculate the final update using a larger step size $\alpha$ as
\begin{equation*}
\begin{aligned}
&x_{t+1}=\mathrm{clip}_{x_{nat},\epsilon}(x_{t}+\alpha \cdot (x_{t}^{n}-x_{t})).
\end{aligned}
\end{equation*}

Although directly applying this algorithm can achieve good results, it is incompatible with SAM due to the varying scales of gradient norm \cite{MI}. To solve this problem, we normalize the gradient at each update by their $\ell_2$ norm. We discover that the modified version actually maximizes the cosine similarity between gradients (proof in \cref{AppendixD}). Thus, we call it Cosine Similarity Encourager (CSE), which can be further combined with MI as MI-CSE. MI-CSE involves an inner momentum term to accumulate the gradients of each model. We provide the pseudocode in \cref{AppendixD}.

%which is based on the assumption that $\frac{g_ig_i^T}{\|g_i\|_2}$ is negligible so that $I-\frac{g_ig_i^T}{\|g_i\|_2} \approx I$ is satisfied in the derivation. The pseudocode of MI-CSE is in~\cref{alg:mi-cse}. 


%However, maximizing dot product similarity between the gradient of each model tends to let our training objective converge into a sharp optimum, which leads to gradient explosion and incompatibility with MI-SAM, resulting in bad transferability. So we decide to maximize the cosine similarity instead of the dot product similarity. However, the gradient of cosine similarity between the gradient of each model requires the third order derivative of the loss function with respect to $x$, which is unacceptable. Inspired by \cite{MAML}, we derive an algorithm to maximize the cosine similarity between gradient just using first order derivative. The derivation of Cosine Similarity Encourager algorithm can be found in Appendix \cref{AppendixD}, and the pseudocode of Cosine Similarity Encourager is algorithm \cref{algorithm3}.


\subsection{Common Weakness Attack}

Given the respective algorithms to optimize the flatness of landscape and the closeness between local optima of different models, we thus need to combine them as a unified Common Weakness Attack (CWA) to achieve better transferability. In consideration of the feasibility of parallel gradient backpropagation and time complexity, we substitute the second step of SAM with CSE, and the resulting algorithm is called CWA. %Specifically, the first step of SAM algorithm aims to find the worst case under the range of infinity norm $\epsilon$, and the second step aims to optimizing the worst case with CSE, which only costs one more iteration comparing with the original CSE. We call this algorithm Common Weakness Attacker(CWA), and we combine our algorithm with MI-FGSM similar with the previous two algorithms. 
We also combine CWA with MI to obtain MI-CWA, with the psuedocode shown in~\cref{alg:mi-cwa}.

% Furthermore, we combine the Cosine Similarity Encourager with MI-SAM algorithm. We conceived three ways to fuse MI-FGSM and Cosine Similarity Encourager. The first two ways are inserting the MI-SAM algorithm into the inner loop or outer loop of Cosine Similarity Encourager, which requires much more computational time comparing with the Cosine Similarity Encourager due to non-parallelization of backpropagation for MI-SAM, although it achieve better result than both MI-SAM and Cosine Similarity Encourager.

% From our point of view, the first step of SAM algorithm aims to find the worst case under the range of infinity norm $eps$, and the second step aims to optimizing the worst case. Inspired by this insight, we substitute the second step of MI-SAM with Cosine Similarity Encourager, which can achieve best results with the lowest time complexity, only cost one more iteration comparing with the Cosine Similarity Encourager. We call this algorithm Common Weakness Attacker(CWA), and we combine our algorithm with MI-FGSM. The psudocode is algorithm~\cref{algorithm2}:

\begin{algorithm}[t] %tb
\small
\label{algorithm2}
   \caption{MI-CWA algorithm}
   \label{alg:mi-cwa}
\begin{algorithmic}[1]
   \REQUIRE
   natural image $x_{nat}$, label $y$, perturbation budget $\epsilon$, 
   iterations $T$, loss function $L$, model ensemble $\mathcal{F}_{t}=\{f_i\}_{i=1}^n$, decay factor $\mu$, step sizes $r$, $\beta$ and $\alpha$.
   \STATE \textbf{Initialize:} $m=0$, inner momentum $\hat{m}=0$, $x_0=x_{nat}$;
   \FOR{$t=0$ {\bfseries to} $T-1$}
   %\STATE \# first step
   \STATE Calculate $g=\nabla_x L(\frac{1}{n}\sum_{i=1}^{n} f_i(x_{t}),y)$;
   \STATE Update $x_t$ by $x_{t}^{0}=\mathrm{clip}_{x_{nat},\epsilon}(x_{t}+r\cdot \mathrm{sign}(g))$;
   %\STATE \# second step, also Cosine Similarity Encourager
   \FOR{$i=1$ {\bfseries to} $n$}
%   \STATE pick the jth model $f_j$
   \STATE Calculate $g=\nabla_x L(f_i(x_{t}^{i-1}),y)$;
   \STATE Update inner momentum by $\hat{m} = \mu \cdot \hat{m} + \frac{g}{\|g\|_2}$;
   \STATE Update $x_t^i$ by $x_{t}^{i} = \mathrm{clip}_{x_{nat},\epsilon}(x_{t}^{i-1}- \beta \cdot \hat{m})$;
   \ENDFOR
   \STATE Calculate the update $g=x_{t}^{n}-x_{t}$;
   \STATE Update momentum $m = \mu \cdot m + g$;
   \STATE update $x_{t+1}$ by $x_{t+1}=\mathrm{clip}_{x_{nat},\epsilon}(x_{t}+\alpha \cdot \mathrm{sign}(m))$;
   \ENDFOR
\STATE \textbf{Return:} $x_T$.
\end{algorithmic}
\end{algorithm}

Our algorithm can also be incorporated with other adversarial attack algorithms, including TI~\cite{TI}, VMI~\cite{vmi}, \etc. The details of these algorithms and experimental results are provided in~\cref{sec:generalizedCW}.