\section{Related Work}
In this section, we briefly review the existing transfer-based adversarial attack algorithms. Overall, these methods usually make an analogy between the transferability of adversarial examples and the generalization of deep learning models, and we categorize them into 3 classes as follows. %\yichi{Any Equation to illustrate?}

\subsection{Gradient-based Optimization}
Comparing the optimization of adversarial examples and the training of deep learning models, researchers introduce some optimization techniques to boost the transferability of adversarial examples. Momentum Iterative (MI) method~\cite{MI} and Nesterov Iterative (NI) method~\cite{NI} introduce the momentum optimizer and Nesterov accelerated gradient to prevent the adversarial examples from falling into undesired local optima. Enhanced Momentum Iterative (EMI) method \cite{emi} accumulates the average gradient of the data points sampled in the gradient direction of the previous iteration so as to stabilize the update direction and escape from poor local maxima. Variance-tuning Momentum Iterative (VMI) method \cite{vmi} reduces the variance of the gradient during the optimization via tuning the current gradient with the gradient variance in the neighborhood of the previous data point.


\subsection{Input Transformation}
These methods transform the input images before feeding them to the classifier for diversity, similar to data augmentation techniques in deep learning. Diverse Inputs (DI) method \cite{DI} applies random resizing and padding to the input images. Translation-Invariant (TI) attack \cite{TI} derives an efficient algorithm for calculating the gradients \wrt the translated images, which is equivalent to applying translations to the input images.
Scale-Invariant (SI) attack \cite{NI} scales the images with different factors based on the observation that the model has similar performance on these scaled images.





\subsection{Ensemble Attack}
As mentioned in \cite{tsea}, increasing the number of classifiers in adversarial attacks can reduce the generalization error upper bound in Empirical Risk Minimization (ERM), just like increasing the number of training samples in deep learning. Researchers propose to alternatively average over losses, predicted probabilities, or logits of surrogate models to form an ensemble~\cite{MI}. Other works \cite{tsea,li2020learning} also propose novel methods to generate massive variants of the surrogate models and then take the average value of losses or logits. Xiong et al. \cite{svre} introduce the SVRG optimizer~\cite{svrgoptimizer} into ensemble adversarial attack to reduce the variance of the gradients during optimization. 
%Besides gathering multiple models, \cite{tsea, wu2020skip} introduce self-ensemble by generating countless surrogate models via heuristic methods.

However, the number of surrogate models in practice is usually small, which cannot guarantee a satisfying generalization upper bound in the theory of ERM, and only focusing on variance does not necessarily lead to better generalization~\cite{yang2021trs}. Additionally, self-ensembled surrogate models are not i.i.d. and therefore have limited transfer performance. In this paper, we employ a quadratic approximation of the expected objective over the victim models and find two properties emerged from the second-order term closely correlated with the transferability. Motivated by this insight, we define common weakness of model ensemble to reflect these two properties and propose methods to optimize them, which efficiently enhance the adversarial transferability.
% However, the number of models in adversarial examples training is much smaller than the number of data in model training, which means that simply ensemble these model still have an large upper bound of generalization error. From our point of view, we should make full use of the information of each model instead of just fusing them by logit or loss in the case that we only have few models. Therefore, we propose an novel method of ensemble attack by attacking the common weakness of the training model, largely improve the transferability of adversarial examples, especially when attacking the adversarially trained models.