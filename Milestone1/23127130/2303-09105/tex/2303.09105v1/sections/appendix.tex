\newpage
\appendix
\onecolumn



\section{Proof of the Theorem \ref{theorem:upperbound}}
\label{sec:upperboundproof}
Using Cauchy-Swartz theorem, we can prove this theorem:
\begin{proof}
    \begin{equation*}
        \begin{aligned}
             &\mathbb{E}[(x-p_i)^TH_i(x-p_i)] \\&
= \mathbb{E}[||(x-p_i)^TH_i(x-p_i)||_F] \\&
\leq \mathbb{E}[||H_i||_F||(x-p_i)||_F^2] \\&
= \mathbb{E}[||H_i||_F ||(x-p_i)||_2^2] \\&
= \mathbb{E}[||H_i||_F] \mathbb{E}[||(x-p_i)||_2^2]
        \end{aligned}
    \end{equation*}
\end{proof}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Common Weakness of Training Models Tends to be the Common Weakness of All models}
\label{appedixA}
Let $c_t$ be the final optimum of the training objective $\sum_{f_i \in \mathcal{F}_t} \frac{1}{n} L(f_i(x), y)$ that we converge into, and let $c$ be the optimum of the testing objective $\mathbb{E}_{f_i \in \mathcal{F}} L(f_i(x), y)$ nearest to $c$. Let $O_{c_t}$ be the set of closest optimum of model $f_i \in \mathcal{F}_t$ to $c_t$, and let $O_c$ be the set of closest optimum of model $f_i \in F$ to $c$. Let the hessian matrices at $p_i \in O_c$ form a set $H_c$, and the hessian matrices at $p_i \in O_{c_t}$ form a set $H_{c_t}$. To find the optimum $c$ with a smaller $\mathbb{E}_{H_i \in H_c}[||H_i||_F]$ and $\mathbb{E}_{p_i \in O_{c}}[||(c-p_i)||_2^2]$ at test time, we need to optimize $\frac{1}{n}\sum_{H_i \in H_{c_t}}||H_i||_F$ and $\frac{1}{n}\sum_{p_i \in O_{c_{t}}}||(c_t-p_i)||_2^2$ at training time. In the following, we will show that both terms have a strong relationship between the training objective and the test error. Therefore, optimizing the training objective will lead to a smaller test error , consequently, a smaller generalization error.

Several studies~\cite{visualoss, wu2017toward, chen2022bootstrap} have shown that the $||H_i||_F$ term corresponds to the flatness of the landscape, and a flatter landscape during training leads to better generalization ability, which is the transferability when training adversarial examples. The $||(c-p_i)||_2^2$ term, which measures the distance between the optimum of the ensemble model and the optimum of each individual model, also has a strong relationship between training and testing. Under assumption \ref{assumption}, the following theorem shows that smaller values of $\frac{1}{n}\sum_{p_i \in O_{c_t}}||(c_t-p_i)||_2^2$ lead to smaller values of $\mathbb{E}_{p_i \in O_c}[||(c-p_i)||_2^2]$.

\begin{assumption}
The optimum of each model $p_i$ in $O_c$ is gaussianly distributed with mean $c$ and unknown variance $\sigma^2$. And the optimum of the ensemble of training models $c_t$ is the mean of $p_i \in O_{c_t}$. That is:
\begin{equation*}
    \begin{aligned}
        &\forall p \in O_c, \ p \sim N(c, \sigma^2 I) \\
        &c_t = \frac{1}{n}\sum_{p_i \in O_{c_t}} p_i
    \end{aligned}
\end{equation*}
\label{assumption}
\end{assumption}




\begin{theorem}
\label{theorem:relationship}
Denote $F(m, n)$ as F-distribution with parameter $m$ and $n$, $F_{\alpha}(m, n)$ as $P(F(m, n) > \alpha)$, For any two different optimum of ensemble model $c^1$ and $c^2$ and corresponding $\hat{\sigma_1}=\frac{1}{n}\sum_{p_i \in O_{c_{t}^1}}(p_i-c_{t}^1)^2$, $\hat{\sigma_2}=\frac{1}{n}\sum_{p_i \in O_{c_{t}^2}}(p_i-c_{t}^2)^2$, there is $F_{\frac{\hat{\sigma_1}}{\hat{\sigma_2}}}(n-1, n-1)$ probability that:
\begin{equation}
    \mathbb{E}_{p_i \in O_{c^1}}[||(c_1-p_i)||^2] \leq \mathbb{E}_{p_i \in O_{c^2}}[||(c_2-p_i)||^2]
\end{equation}
\end{theorem}

This theorem suggests that if the training error of one optimum is smaller, the test error also tends to be smaller. In other words, a smaller training error, $\frac{1}{n}\sum_{p_i \in O_{c_t}}||(c_t-p_i)||_2^2$, tends to lead to a smaller testing error, $\mathbb{E}_{p_i \in O_c}[||(c-p_i)||_2^2].$ We prove this theorem using Lemma \ref{lemma:A1} and Lemma \ref{lemma:A2} in the following sections.


 The training models $F_t$ can be viewed as sampling from the set of all models $F$. Then, by Maximum Likelihood Estimation or Moment Estimation, we can estimate the $\sigma$ as:
\begin{equation*}
    \hat{\sigma}=\frac{1}{n}\sum_{p_i \in O_{c_t}}(p_i-c_t)^2
\end{equation*}


\begin{lemma}
\label{lemma:A1}
    Denote $F(m, n)$ as F distribution with parameter $m$ and $n$, $F_{\alpha}(m, n)$ as $P(F(m, n) > \alpha)$. There is $F_{\frac{\hat{\sigma_1}}{\hat{\sigma_2}}}(n-1, n-1)$ probability that $\sigma_1 \leq \sigma_2$.
\end{lemma}

\begin{proof}
    Because that $\frac{\hat{\sigma}}{n\sigma}$ follows chi-square distribution, that is:
    \begin{equation*}
    \hat{\sigma}=\sum_{i=1}^{n}(p_i-c_t)^2
    = \sum_{i=1}^{n}(p_i-\frac{1}{n} \sum_{j=1}^{n} p_i)^2
    \sim n\sigma \cdot  \mathcal{X}^2(n-1)
    \end{equation*}

    Therefore $\frac{\hat{\sigma_1}\sigma_2}{\hat{\sigma_2}\sigma_1}$ follows $F$ distribution.
    \begin{equation*}
    \frac{\hat{\sigma_1}}{\hat{\sigma_2}} \sim \frac{\sigma_1}{\sigma_2} \cdot F(n-1, n-1)
    \end{equation*}

    Therefore we can get that:
    \begin{equation*}
    \frac{\sigma_1}{\sigma_2}
     \sim \frac{\hat{\sigma_1}}{\hat{\sigma_2} \cdot F(n-1, n-1)} 
    \end{equation*}

    Therefore, the probability of $\sigma_1 \leq \sigma_2$ is:
    \begin{equation*}
        P(\sigma_1\leq \sigma_2) = P(\frac{\sigma_1}{\sigma_2} \leq 1) = P(\frac{\hat{\sigma_1}}{\hat{\sigma_2} \cdot F(n-1, n-1)}   \leq 1) = P(\frac{\hat{\sigma_1}}{\hat{\sigma_2}}  \leq F(n-1, n-1)) = F_{\frac{\hat{\sigma_1}}{\hat{\sigma_2}}}(n-1, n-1)
    \end{equation*}
\end{proof}


This shows that smaller value of $\frac{1}{n}\sum_{i=1}^{n}(p_i-c_t)^2$ tends to indicate a smaller value of $\sigma$. Now we prove that the smaller $\sigma$ is, the better transferability our adversarial examples tends to have. 

\begin{lemma}
\label{lemma:A2}
    Test error $\mathbb{E}_{p_i \in O_c}[||(c-p_i)||_2^2]$ is a function that increases monotonically with $\sigma$. That is:
    \begin{equation*}
        \frac{\partial}{\partial \sigma}\mathbb{E}_{p_i \in O_c}[||(c-p_i)||_2^2] \geq 0
    \end{equation*}
\end{lemma}

\begin{proof}
Reparameterize $p_i$ as $\sigma \epsilon_i + c$, where $\epsilon_i \sim N(0, I)$. We can get:
\begin{equation*}
    \begin{aligned}
         &\mathbb{E}_{p_i \in O_c}[||(c-p_i)||] \\
         =& \mathbb{E}_{\epsilon_i \sim N(0, I)}[||\sigma \epsilon_i||] \\
         =& \sigma \mathbb{E}_{\epsilon_i \sim N(0, I)}[|| \epsilon_i||]
    \end{aligned}
\end{equation*}

Hence, we have $\frac{\partial}{\partial \sigma}\mathbb{E}_{p_i \in O_c}[||(c-p_i)||_2^2] \geq 0$. It is worth noting that the expectation of any norm of the vector $c-p_i$ is a monotonically increasing function of the parameter $\sigma$, not just the second norm.

\end{proof}

According to lemma \ref{lemma:A1} and lemma \ref{lemma:A2}, so there is an probability $F_{\frac{\hat{\sigma_1}}{\hat{\sigma_2}}}(n-1, n-1)$, that $\sigma_1 \leq \sigma_2$, that is $\mathbb{E}_{p_i \in O_{c^1}}[||(c_1-p_i)||^2] \leq \mathbb{E}_{p_j \in O_{c^2}}[||(c_2-p_j)||^2]$. Therefore, both $||H_i||_F$ and $||p_i-c||_2^2$ exhibit a strong correlation between the training objective and test error, indicating that optimizing the training objective can result in improved performance at test time. This, in turn, enhances the transferability of adversarial examples.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Sharpness Aware Minimization Algorithm under Infinity Norm}
\label{appendixB}
In this section, for simplicity, we use $L(x)$ to denote $\mathbb{E}_{f_i \in \mathcal{F}_t}[L(f_i(x), y)]$. Following the procedure at \cite{SAM}, we derive the Sharpness Aware Minimization Algorithm under infinity norm.
Our training objective is:
\begin{equation}
\label{objective2}
    min [\max_{||\delta||_{\inf}<\epsilon} L(x+\delta) - L(x)] + L(x)
\end{equation}

The first term corresponds to the sharpness of the landscape, while the second term represents the vanilla loss function. Although the Frobenius norm of the Hessian matrix $||H||_F$ could also be a potential choice, it requires the computation of third-order derivatives with respect to $p$, which is computationally expensive when crafting adversarial examples.

To optimize objective \ref{objective2}, first, we need to compute $\delta$. We use the Taylor expansion to get $\delta$:
\begin{equation*}
    \begin{aligned}
        \delta &= \arg\max_{||\delta||_{\inf}<\epsilon} L(x+\delta) \\
        &\approx \arg\max_{||\delta||_{\inf}<\epsilon} L(x) +\delta^T sign(\nabla_x L(x)) \\
        &= \arg\max_{||\delta||_{\inf}<\epsilon} \delta^T sign(\nabla_x L(x)) \\
        &= \epsilon \cdot sign(\nabla_x L(x))
    \end{aligned}
\end{equation*}

Then, we can get the derivative of our objective function:
\begin{equation}
\label{objective3}
    \begin{aligned}
        \nabla_x L(x+\delta) 
        = \nabla_{x+\delta} L(x+\delta) \cdot \nabla_{x} (x+\delta)
        = \nabla_{x+\delta} L(x+\delta) + \nabla_{x+\delta} L(x+\delta) \cdot \nabla_{x} \delta
    \end{aligned}
\end{equation}

To align with the procedure in \cite{SAM}, we discard the second term of objective \ref{objective3} as it has negligible influence on the optimization of the SAM algorithm.

In general, the procedure of the SAM algorithm under the infinity norm involves first finding the worst case perturbation $\delta$, followed by optimizing the worst case $L(x+\delta)$. The detail is at \cref{alg:mi-sam}.


\begin{algorithm}[t] %tb
\small
\label{algorithm1}
   \caption{MI-SAM}
   \label{alg:mi-sam}
\begin{algorithmic}[1]
   \REQUIRE
   natural image $x_{nat}$, label $y$, perturbation budget $\epsilon$, 
   iterations $T$, loss function $L$, model ensemble $\mathcal{F}_{t}=\{f_i\}_{i=1}^n$, decay factor $\mu$, step sizes $r$, $\beta$ and $\alpha$.
   \STATE \textbf{Initialize:} $m=0$, $x_0=x_{nat}$;
   \FOR{$t=0$ {\bfseries to} $T-1$}
   %\STATE \# first step
   \STATE Calculate $g=\nabla_x L(\frac{1}{n}\sum_{i=1}^{n} f_i(x_{t}),y)$;
   \STATE Update $x_t$ by $x_{t}^{r}=\mathrm{clip}_{x_{nat},\epsilon}(x_{t}+r\cdot \mathrm{sign}(g))$;
   \STATE Calculate $g=\nabla_x L(\frac{1}{n}\sum_{i=1}^{n} f_i(x_{t}^r),y)$;
   \STATE Update $x_t^r$ by $x_{t}^{f}=\mathrm{clip}_{x_{nat},\epsilon}(x_{t}^r-\beta \cdot \mathrm{sign}(g))$;
   \STATE Calculate the update $g=x_{t}^{f}-x_{t}$;
   \STATE Update momentum $m = \mu \cdot m + g$;
   \STATE update $x_{t+1}$ by $x_{t+1}=\mathrm{clip}_{x_{nat},\epsilon}(x_{t}+\alpha \cdot (m))$;
   \ENDFOR
\STATE \textbf{Return:} $x_{T}$.
\end{algorithmic}
\end{algorithm}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%v
\section{Proof of Theorem \ref{theorem:objective2upperbound}}
\label{appendixC}
In this section, we aim to prove that dot product similarity between gradient of each model is the upper bound of $\frac{1}{n}\sum_{i=1}^{n}(c-p_i)^2$.

Using Cauchy-Swartz theorem, we can get:

\begin{equation*}
    \begin{aligned}
        \sum_{i=1}^{n}||(c-p_i)||_2^2 
 = \sum_{i=1}^{n}(H_i^{-1}g_i)^T(H_i^{-1}g_i) 
 = \sum_{i=1}^{n}||(H_i^{-1}g_i)||_2^2 
\leq \sum_{i=1}^{n}||H_i^{-1}||_F^2||g_i||_2^2 
    \end{aligned}
\end{equation*}

The treatment of $||H_i||_F$ is discussed in Appendix \ref{appendixB}. In this section, we set $M$ as the maximum value of $||H_i^{-1}||_F^2$, which allows us to obtain the following result:
\begin{equation*}
    \begin{aligned}
        \sum_{i=1}^{n}||(c-p_i)||_2^2 
        \leq M \sum_{i=1}^{n}g_i^Tg_i = M[(\sum_{i=1}^{n}g_i)^2-2\sum_{i=1}^{n}\sum_{j=1}^{i-1}g_ig_j]
    \end{aligned}
\end{equation*}
Since $c$ is the optimal solution for the ensemble model, we have $(\sum_{i=1}^{n}g_i)^2=0$. Consequently, our final training objective is as follows:
\begin{equation}
    \max \sum_{i=1}^{n}\sum_{j=1}^{i-1}g_ig_j
\end{equation}
Which is the dot product similarity between the gradient of each model. Because maximizing the dot product similarity will cause gradient explosion and incompatible with MI-SAM, we choose to maximize cosine similarity between the gradient of each model.


The equality sign holds when $p_i=x$ for all $i$. This condition implies that the local optimum for each model is identical, which results in a dot product similarity of 0 between the gradients.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%v


\section{Derivation of Cosine Similarity Encourager}
\label{AppendixD}
In this section, we aim to derive an algorithm for maximizing the cosine similarity between the gradient of each model. Directly optimizing this objective requires second order derivative with respect to the adversarial examples, which is time-expensive and memory-consuming due to the backpropagation of the hessian matrix. Inspired by \cite{MAML}, we design our Cosine Similarity Encourager Algorithm \ref{algorithm3}.



\begin{algorithm}[h] %tb
\small
\label{algorithm3}
   \caption{MI-CSE algorithm}
   \label{alg:mi-cse}
\begin{algorithmic}[1]
   \REQUIRE
   natural image $x_{nat}$, label $y$, perturbation budget $\epsilon$, 
   iterations $T$, loss function $L$, model ensemble $\mathcal{F}_{t}=\{f_i\}_{i=1}^n$, decay factor $\mu$, step sizes $\beta$ and $\alpha$.
   \STATE \textbf{Initialize:} $m=0$, inner momentum $\hat{m}=0$, $x_0=x_{nat}$;
   \FOR{$t=0$ {\bfseries to} $T-1$}
   \FOR{$i=1$ {\bfseries to} $n$}
%   \STATE pick the jth model $f_j$
   \STATE Calculate $g=\nabla_x L(f_i(x_{t}^{i-1}),y)$;
   \STATE Update inner momentum by $\hat{m} = \mu \cdot \hat{m} + \frac{g}{\|g\|_2}$;
   \STATE Update $x_t^i$ by $x_{t}^{i} = \mathrm{clip}_{x_{nat},\epsilon}(x_{t}^{i-1}- \beta \cdot \hat{m})$;
   \ENDFOR
   \STATE Calculate the update $g=x_{t}^{n}-x_{t}$;
   \STATE Update momentum $m = \mu \cdot m + g$;
   \STATE update $x_{t+1}$ by $x_{t+1}=\mathrm{clip}_{x_{nat},\epsilon}(x_{t}+\alpha \cdot \mathrm{sign}(m))$;
   \ENDFOR
\STATE \textbf{Return:} $x_T$.
\end{algorithmic}
\end{algorithm}

In the following, we will prove that this algorithm can maximizing the cosine similarity of the gradient between each model.

\begin{theorem}
    Denote $g_i$ as $\nabla_{x} L(f_i(x), y)$ and $g_j$ as $\nabla_{x} L(f_j(x), y)$. When the learning rate $\beta \approx 0$, and the $\frac{g_jg_j^T}{||g_j||_2}$ is negligible, updating by algorithm \ref{algorithm3} will increase the cosine similarity of the gradient between each model.
\label{theorem:updatecosinesimilarity}
\end{theorem}


\subsection{Proof of Theorem \ref{theorem:updatecosinesimilarity}}

\begin{proof}
    The derivative of cosine similarity between gradient of each model can be written as:
\begin{equation*}
    \begin{aligned}
        \frac{\partial}{\partial x}\frac{g_ig_j}{||g_i||_2||g_j||_2} = \frac{H_i}{||g_i||_2}(I-\frac{g_ig_i^T}{||g_i||_2})\frac{g_j}{||g_j||_2}+\frac{H_j}{||g_j||_2}(I-\frac{g_jg_j^T}{||g_j||_2})\frac{g_i}{||g_i||_2}
    \end{aligned}
\end{equation*}

Because $\frac{g_jg_j^T}{||g_j||_2}$ is negligible, which is verified at \cref{sec:verificationofcse}, the gradient can be approximated as:
\begin{equation}
\label{objective5}
    \begin{aligned}
        \frac{\partial}{\partial x}\frac{g_ig_j}{||g_i||_2||g_j||_2}
        \approx \frac{H_i}{||g_i||_2}\frac{g_j}{||g_j||_2}+\frac{H_j}{||g_j||_2}\frac{g_i}{||g_i||_2}
    \end{aligned}
\end{equation}

Because $i, j$ only represent the $i$th model and $j$th model in $\mathcal{F}_{t}$, and we can permute the set $\mathcal{F}_{t}$, therefore we can get that:
\begin{equation*}
    \mathbb{E}[\frac{H_i}{||g_i||_2}\frac{g_j}{||g_j||_2}]=\mathbb{E}[\frac{H_j}{||g_j||_2}\frac{g_i}{||g_i||_2}]
\end{equation*}

Therefore the expectation of the derivative is:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[\frac{\partial}{\partial x}\frac{g_ig_j}{||g_i||_2||g_j||_2}]
        \approx 2 \mathbb{E}[\frac{H_i}{||g_i||_2}\frac{g_j}{||g_j||_2}]
    \end{aligned}
\end{equation*}


Now we prove that the update of algorithm \ref{algorithm3} contains the gradient of vanilla loss $L(f_i(x),y)$ and the derivative of the cosine similarity between the gradient of each model, which is shown in equation \ref{objective5}.

Denote $x_i$ as the adversarial example after the ith iteration in the inner loop of the algorithm \ref{algorithm3}, $g_i'$ as the gradient at ith iteration in the inner loop, we can represent $g_i'$ by $g_i$ using Taylor expansion:

\begin{equation*}
    \begin{aligned}
        g_i'=g_i+H_i(x_i-x_1) 
=g_i-\beta H_i \sum_{j=1}^{i-1}\frac{ g_j'}{|| g_j'||_2} 
= g_i-\beta H_i \sum_{j=1}^{i-1}\frac{ g_j+o(\beta)}{|| g_j+o(\beta)||_2} = g_i-\beta H_i \sum_{j=1}^{i-1}\frac{ g_j}{|| g_j||_2} +O(\beta^2)
    \end{aligned}
\end{equation*}
The update over the entire inner loop is:
\begin{equation*}
    \begin{aligned}
        x-x_n=\beta \sum_{i=1}^{n}\frac{g_i'}{||g_i'||_2}
    \end{aligned}
\end{equation*}
Substitute $g_i'$ with $g_i-\beta H_i \sum_{j=1}^{i-1}\frac{ g_j}{|| g_j||_2} +O(\beta^2)$, $||g_i'||_2$ with $||g_i+O(\beta)||_2 \approx ||g_i||_2$, we can get that:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[x-x_n]=\mathbb{E}[\beta \sum_{i=1}^{n}[g_i-\beta H_i \sum_{j=1}^{i-1}\frac{ g_j}{|| g_j||_2} +O(\beta^2)]/||g_i'||_2] \\ 
=
\mathbb{E}[\beta \sum_{i=1}^{n}\frac{g_i}{||g_i||_2}-\beta^2 \sum_{i=1}^{n}\sum_{j=1}^{i-1}H_i\frac{ g_j}{||g_i||_2|| g_j||_2} +\sum_{i=1}^{n}O(\beta^3)] \\
=\beta \mathbb{E}[ \sum_{i=1}^{n} \frac{g_i}{||g_i||_2}]-\frac{\beta^2}{2} \mathbb{E}[\sum_{i,j}^{i<j}\frac{\partial \frac{g_ig_j}{||g_i||_2||g_j||_2}}{\partial x}]+\sum_{i=1}^{n}O(\beta^3)
    \end{aligned}
\end{equation*}

Hence, updating by algorithm \ref{algorithm3} will also increase the cosine similarity between gradient of each model. When using the gradient ascent algorithm, we can either multiply the training objective by $-1$ and solve it as a gradient descent problem, or we can derive an algorithm specifically for gradient ascent.
\end{proof}

\subsection{Verification of the Approximation $I-\frac{g_ig_i^T}{||g_i||_2} \approx I$}
\label{sec:verificationofcse}
To verify whether $I-\frac{g_ig_i^T}{||g_i||_2} \approx I$ holds when generating adversarial examples, We first average the value of  $I-\frac{g_ig_i^T}{||g_i||_2}$ over NIPS17 dataset during training, and then downsample this matrix to (256, 256), finally we visualize this matrix in the form of heatmap. The result is ~\cref{figure:heatmap}.

\begin{figure}[h]
    \centering
    \includegraphics[width=3.5cm]{./figures/heatmapofmatrix.pdf}
    \caption{Heatmap of the matrix.}
    \label{fig:my_label}
\label{figure:heatmap}
\end{figure}

As shown in the figure,  the elements on the diagonal are close to 1, while the elements in other positions are close to 0. As a result, the approximation $I-\frac{g_ig_i^T}{||g_i||_2} \approx I$ is reasonable during training. This supports the tenability of our algorithm derivation for CSE.





\section{Generalized Common Weakness Algorithm}
\label{sec:generalizedCW}
CWA algorithm can incorporating with not only MI-FGSM, but also other arbitrary optimizer, such as Adam, VMI-FGSM. The pseudo-code is Algorithm \ref{algorithm:generalizedCW}. For each optimizer, the $step$ function taken the gradient with respect to $x$, and do gradient descent. For generalized Common Weakness algorithm, we need 3 optimizer in total. Note that these three optimizers can be the same type of optimizer or different types of optimizers.


\begin{algorithm}[h] %tb
\label{algorithm:generalizedCW}
   \caption{Generalized Common Weakness Algorithm(CWA)}
   
\begin{algorithmic}
   \REQUIRE
   image $x_0$; label $y$; 
   total iteration $T$; loss function $L$, model set $F_{train}$; momentum weight $\mu$; inner optimizer $\beta$, reverse optimizer $r$, outer optimizer $\alpha$
   \STATE Calculate the number of models $n$
   \FOR{$t=1$ {\bfseries to} $T$}
   \STATE $o=copy(x)$
   \STATE \# first step
   \STATE calculate the gradient $g=\nabla_x L(\frac{1}{n}\sum_{i=1}^{n} f_i(x),y)$
   \STATE update x by $r.step(-g)$
   \STATE \# second step
   \FOR{$j=1$ {\bfseries to} $n$}
   \STATE pick the jth model $f_j$
   \STATE calculate the gradient $g=\nabla_x L(f_j(x),y)$
   \STATE update x by $\beta.step(\frac{g}{||g||_2})$
   \ENDFOR
   \STATE calculate the update in this iteration $g=o-x$
   \STATE update x by $\alpha.step(g)$
   \ENDFOR
\STATE return x
\end{algorithmic}
\label{alg:example}
\end{algorithm}




\section{More Experiments}
\label{sec:moreexp}


To further demonstrate the effectiveness of the CWA algorithm when there are fewer types of training models, we conducted additional experiments in this section. Specifically, we only use resnet18, resnet32, resnet50, and resnet101~\cite{resnet} as training models. The results are presented in \ref{table:extraexp}.

% \usepackage{tabularray}
\begin{table}
\centering
\label{table:extraexp}
\resizebox{\linewidth}{!}{
\begin{tblr}{
colspec={llccccccccc},
  cell{2}{1} = {r=16}{},
  vline{2-3} = {1-25}{},
  vline{3} = {3-17}{},
  hline{1-2,18,26} = {-}{},
  hline{19-22,24} = {1-2}{},
}
       &                                 & MI   & DI-MI & TI-MI & VMI           & MI-SAM & MI-CSE        & MI-CWA        & TI-CWA        & VMI-CWA        \\
Normal & AlexNet                         & 73.5 & 79.5  & 79.0  & 84.8          & 79.0   & 83.3          & 82.6          & 88.1          & \textbf{89.8}  \\
& VGG16                          & 92.2 & 96.7  & 84.5  & 96.9          & 96.3   & 99.2          & 99.6          & 97.2          & \textbf{99.8}  \\
& GoogleNet                       & 90.2 & 94.9  & 79.4  & 96.5          & 95.8   & 97.9          & 97.6          & 93.7          & \textbf{99.6}  \\
& InceptionV3                   & 83.4 & 91.7  & 76.9  & 93.7          & 88.4   & 92.8          & 91.2          & 90.1          & \textbf{98.3}  \\
& ResNet-152                       & 97.7 & 96.5  & 89.4  & 98.1          & 99.3   & \textbf{99.9} & \textbf{99.9} & 98.7          & \textbf{99.9}  \\ 
       & DenseNet121                     & 97.2 & 97.2  & 90.7  & 98.0          & 98.9   & 99.7          & 99.9          & 99.4          & \textbf{100.0} \\
       & SqueezeNet                  & 91.1 & 95.6  & 87.0  & 95.4          & 95.5   & 97.6          & 98.4          & 97.7          & \textbf{99.4}  \\
         & ShuffleNetV2           & 82.3 & 87.3  & 78.0  & 91.1          & 87.8   & 91.5          & 91.7          & 91.1          & \textbf{96.2}  \\
            & MobileNetV3-S            & 71.1 & 80.6  & 77.5  & 89.5          & 78.0   & 79.3          & 79.3          & 89.0          & \textbf{92.1}  \\
       & EfficientNetB0                & 91.6 & 95.3  & 77.8  & 96.8          & 96.1   & 98.4          & 98.4          & 92.7          & \textbf{99.5}  \\
       
       

       
       & MNasNet                    & 88.4 & 94.4  & 75.1  & 96.1          & 94.6   & 97.5          & 97.3          & 90.5          & \textbf{99.3}  \\

       & RegNetX-400MF                & 90.5 & 94.6  & 85.7  & 96.9          & 94.8   & 97.1          & 97.0          & 97.0          & \textbf{99.4}  \\
              & ConvNeXt-T                  & 82.9 & 89.5  & 56.6  & 94.2          & 89.4   & 90.7          & 88.6          & 59.1          & \textbf{97.6}  \\

       & ViT-B/16                      & 55.1 & 65.9  & 54.2  & \textbf{81.4} & 61.9   & 49.8          & 45.8          & 50.7          & 69.5           \\
       & Swin-S                         & 64.5 & 73.3  & 39.1  & \textbf{84.2} & 69.7   & 60.6          & 57.4          & 31.8          & 77.3           \\
       & MaxViT-T                       & 64.2 & 74.8  & 32.2  & \textbf{84.5} & 70.5   & 57.4          & 54.0          & 21.0          & 77.4           \\
       
    FGSMAT~\cite{tensorflowmodelgarden2020}   & InceptionV3              & 54.7 & 60.5  & 65.2  & 73.2          & 58.7   & 61.0          & 60.7          & \textbf{77.4} & 71.9           \\
    EnsAT~\cite{tramer2017ensemble}   & IncRes-v2ens & 38.6 & 51.2  & 55.8  & 65.6          & 39.8   & 38.7          & 37.7          & \textbf{68.8} & 52.8           \\
    FastAT~\cite{wong2020fast}   & ResNet-50                    & 43.7 & 45.2  & 46.9  & 47.3          & 45.5   & 47.2          & 46.2          & \textbf{51.3} & 48.8           \\
  PGDAT~\cite{Engstrom2019Robustness}     & ResNet-50          & 33.1 & 34.4  & 38.5  & 41.3          & 36.5   & 35.2          & 35.7          & \textbf{45.1} & 39.5           \\
   PGDAT~\cite{salman2020adversarially}    & ResNet-18               & 44.2 & 45.6  & 47.5  & 47.3          & 46.2   & 46.5          & 46.6          & \textbf{52.7} & 48.5           \\
       & WRN-50-2             & 25.1 & 26.7  & 30.6  & 32.4          & 28.1   & 28.2          & 26.6          & \textbf{36.7} & 30.8           \\
   PGDAT$^\dagger$~\cite{debenedetti2022light}    &  XCiT-M12 & 17.9 & 20.6  & 23.2  & 24.9          & 21.9   & 21.2          & 20.6          & \textbf{26.6} & 23.4           \\
       &  XCiT-L12 & 15.4 & 17.0  & 20.7  & 22.1          & 17.9   & 18.6          & 18.1          & \textbf{25.7} & 21.6           
\end{tblr}
}
\caption{The attack success rate(\%) against different models.}
\end{table}




\section{Ablation studies}
In this section, we ablate the additional two hyperparameters, reverse step size $r$ and inner step size $\beta$.


\begin{figure*}[h]
\centering
\subfigure[$\beta$]{
\includegraphics[width=5.5cm]{./figures/ablateinnerstepsize.pdf}
\label{figure:ablatebeta}
}
\subfigure[$r$]{
\includegraphics[width=5.5cm]{./figures/ablater.pdf}
\label{figure:ablater}
}
\end{figure*}

\subsection{Ablation study on inner step size $\beta$}
As shown in figure \ref{figure:ablatebeta}, since the learning rate of the gradient of the original loss is $\beta$, and the learning rate of the regularization term we added is $\beta^2$, as the value of $\beta$ increases, the proportion of the regularization term we added gradually increases, leading to an increase in the attack success rate. However, when $\beta$ becomes too large, the error of the algorithm also increases, causing the attack success rate to plateau or decrease. Therefore, we need to choose an appropriate value of $\beta$ to balance between the effectiveness of the regularization term and the overall performance of the algorithm.


\subsection{Ablation study on reverse step size $r$}
Figure \ref{figure:ablater} shows that the attack success rate initially increases and then decreases as the reciprocal of the reverse step size increases. This behavior is due to the fact that, when the reverse step size is too large, the optimization direction is opposite to the forward step, leading to a decrease in the attack success rate. Thus, decreasing the reverse step size initially increases the attack success rate. As the reverse step size continues to decrease, MI-CWA gradually degrade to MI-CSE, causing the attack success rate to converge to that of MI-CSE.