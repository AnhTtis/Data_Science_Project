

\vspace{-1ex}
\section{Experiments}
\vspace{-1ex}


In this section, we conduct comprehensive experiments to show the superiority of our methods. The tasks range from image classification to object detection and image description with the recent large vision-language models, demonstrating the universality of our methods.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{tables/table1}



\vspace{-1ex}
\subsection{Attack in Image Classification}
\vspace{-1ex}
\label{sec:classification}
% In this section, we conduct extensive experiments on image classification. We first share our setting about datasets, baseline, hyper-parameters. Then, we analyze the result of attacking the normal models and adversarially trained models.

% We also develop a framework for adversarial attacks on image classification. We will release our code as soon as our paper is accepted.




\textbf{Experimental settings.} \textbf{(1) Dataset:} Similar to previous works, we adopt the NIPS2017 dataset\footnote{https://www.kaggle.com/competitions/nips-2017-non-targeted-adversarial-attack}, which is comprised of 1000 images compatible with ImageNet~\citep{russakovsky2015imagenet}. All the images are resized to $224\times224$. 
\textbf{(2) Surrogate models:} We choose four normally trained models -- ResNet-18, ResNet-32, ResNet-50, ResNet-101 \citep{resnet} from TorchVision \citep{marcel2010torchvision}, and two adversarially trained models -- ResNet-50 \citep{salman2020adversarially}, XCiT-S12 \citep{debenedetti2022light} from RobustBench \citep{croce2020robustbench}. They contain both normal and robust models, which is effective to assess the attacker's ability to utilize diverse surrogate models. We further conduct experiments with other surrogate models and other settings~(\eg, using less diverse surrogates, settings in \citet{MI}, settings in \citet{naseer2020self_NRP}, attacks under $\epsilon=4/255$) in~\cref{sec:moreexp}. 
\textbf{(3) Black-box models:} We choose a model from each model type available in TorchVision and RobustBench as the black-box model. We also include 7 other prominent defenses in our evaluation. In total, there are 31 models selected. For further details, please refer to \cref{sec:black_box_models}. 
\textbf{(4) Compared methods:} We integrate our methods into MI \citep{MI} as MI-SAM, MI-CSE and MI-CWA. We also integrate CWA into two state-of-the-art methods VMI \citep{vmi} and SSA~\citep{long2022frequency} as VMI-CWA and SSA-CWA. We compare them with FGSM \citep{goodfellow2014explaining}, BIM \citep{bim}, MI \citep{MI}, DI \citep{DI}, TI \citep{TI}, VMI \citep{vmi}, PI~\citep{gao2020patch_pi}, SSA~\citep{long2022frequency}, RAP~\citep{qin2022boosting_RAP} and SVRE~\citep{svre}. All these attacks adopt the logits ensemble strategy following \cite{MI} for better performance. Except FGSM and BIM, all algorithms have been integrated with MI.
\textbf{(5) Hyper-parameters:} We set the perturbation budget $\epsilon=16/255$, total iteration $T=10$, decay factor $\mu = 1$, step sizes $\beta=50$, $r=16/255/15$, and $\alpha=16/255/5$. For compared methods, we employ their optimal hyper-parameters as reported in their respective papers.



\textbf{Results on normal models.}
%For normally trained models, we select one for each type of model in torchvision, greatly increasing the diversity of our experiments. 
The results of black-box attacks are shown in the upper part of \cref{table:classification}.
Both MI-SAM and MI-CSE greatly improve the attack success rate compared with MI. Note that MI-CSE improves the attack success rate significantly if the black-box model is similar to one of the surrogate models. For example, when attacking ViT-B/16, the attack success rate of MI-CSE is nearly 30\% higher than MI. This is because MI-CSE attacks the common weakness of the surrogate models, so as long as there is any surrogate model similar to the black-box model, the attack success rate will be very high. While other methods cannot attack the common weakness of surrogate models, and the information of the only surrogate model which is similar to the black-box model will be overwhelmed by other surrogate models. Note that MI-SAM increases the attack success rate consistently, no matter whether the surrogate models are similar to the black-box model or not. This is because MI-SAM boosts the transferability of adversarial examples via making them converge into flatter optima.

When incorporating the MI-SAM into MI-CSE to form MI-CWA, the attack success rate is further improved.
%, although the success rate of MI-CWA is lower than that of MI-CSE for some models. 
%For the model in which MI-CWA increases the attack success rate compared with MI-CSE, the attack success rate tends to increase more. This is 
Because MI-SAM and MI-CSE aim to optimize two different training objectives, and they are compatible as shown in \cref{theorem:objective2upperbound}, the resultant MI-CWA can not only make adversarial examples converge into a flat region, but also close to the optimum of each surrogate model. Therefore, MI-CWA further boosts the transferability of adversarial examples.

By integrating our proposed CWA with recent state-of-the-art attacks VMI and SSA, the resultant attacks VMI-CWA and SSA-CWA achieve a significant level of attacking performance. Typically, SSA-CWA achieves more than 99\% attack success rates for most normal models. VMI-CWA and SSA-CWA can also outperform their vanilla versions VMI and SSA by a large margin. The results not only demonstrate the effectiveness of our proposed method when integrating with other attacks, but also prove the vulnerability of existing image classifiers under strong transfer-based attacks.








\textbf{Results on adversarially trained models.}
As shown in \cref{table:classification},
since our method attacks the common weakness of surrogate models, we can make full use of the information of XCiT-S12 \citep{debenedetti2022light} to attack other adversarially trained XCiT models, without being interfered by four normally trained ResNets in the ensemble. Because FastAT \citep{wong2020fast} and PGDAT \citep{Engstrom2019Robustness,salman2020adversarially} are adversarially trained ResNets, our algorithm significantly improves the attack success rate by 40\% over existing methods via making full use of the information of ResNets and defense models in the ensemble.


\textbf{Results on other defense models.}
From \cref{table:classification}, we observe that our algorithm significantly boosts the existing attacks against state-of-the-art defenses. For instance, when integrated with SSA~\citep{long2022frequency}, our method attains a 100\% attack success rate against 4 randomized defenses, and 97.5\% against the recently proposed defense -- DiffPure~\citep{nie2022diffpure}. These findings underscore the remarkable efficacy of our proposed method against defense models.

\begin{table*}[t]
\vspace{-7ex}
\centering
\footnotesize
\setlength{\tabcolsep}{2pt}
\caption{\textbf{mAP (\%,  $\downarrow$) of black-box detectors under attacks on INRIA dataset.} The universal adversarial patch trained on YOLOv3 and YOLOv5 by Adam-CWA achieves the lowest mAPs on multiple modern detectors (9.85 on average) with large margins.}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|cccccccc|c} 
\hline
Method & Surrogate       & YOLOv2    & YOLOv3    & YOLOv3-T & YOLOv4    & YOLOv4-T & YOLOv5    & FasterRCNN & SSD   & Avg.    \\ 
\hline
Single  &YOLOv3       & 54.63 & 12.35 & 53.99   & 58.20 & 53.38   & 69.21 & 50.81        & 58.13 & 51.34  \\
Single &YOLOv5       & 30.45 & 34.17 & 33.26   & 53.55 & 54.54   & 7.98  & 37.87        & 37.00 & 36.10  \\
Loss Ensemble & YOLOv3+YOLOv5 & 25.84 & 8.08  & 38.50   & 47.22 & 43.50   & 19.21 & 34.41        & 35.04 & 31.48  \\
Adam-CWA& YOLOv3+YOLOv5  & \bf{6.59}  & \bf{2.32}  & \bf{8.44}    & \bf{11.07} & \bf{8.33}    & \bf{2.06}  & \bf{14.41}        & \bf{25.56} & \bf{9.85}   \\
\hline
\end{tabular}}
\vspace{-1ex}
\label{table:detection}
\end{table*}


















%-------------------------------------------------
%\vspace{-1ex}
\subsection{Attack in Object Detection}%\vspace{-1ex}

\textbf{Experimental settings.} 
\textbf{(1) Dataset:} We generate universal adversarial patches on the training set of the INRIA dataset~\citep{INRIA} and evaluate them on black-box models on its test set, presenting challenges for adversarial patches to transfer across different samples and various models.
\textbf{(2) Surrogate models:} We choose YOLOv3~\citep{redmon2018yolov3} and YOLOv5~\citep{yolov5} as our surrogate models.
\textbf{(3) Testing models:} We evaluate adversarial patches on 2 white-box models mentioned above and 6 black-box object detectors, 
including YOLOv2 \citep{redmon2017yolo9000}, YOLOv3-T \citep{redmon2018yolov3}, YOLOv4, YOLOv4-T \citep{bochkovskiy2020yolov4}, FasterRCNN \citep{ren2015fasterrcnn}, and SSD \citep{liu2016ssd}.
\textbf{(4) Compared methods:}
Since the Adam method is often used to optimize adversarial patches for object detection \citep{thys2019fooling,zhang2023make}, we first combine the proposed CWA with Adam to form Adam-CWA and compare its performance with patches trained by YOLOv3 only, YOLOv5 only, and Loss Ensemble of YOLOv3 and YOLOv5~(Enhanced Baseline in \citet{tsea}).
\textbf{(5) Training Procedure:} We follow the ``Enhanced Baseline'' settings in \citet{tsea} including the image size, detector weight, the way to paste the patch on the image, learning rate, \etc.
 

\textbf{Results.}
As shown in \cref{table:detection}, our method exceeds the Loss Ensemble method by 20\% on average over all models and has a larger margin compared to patches generated on one detector. It is noticeable that the universal adversarial patch generated by Adam-CWA has even lower mAPs (2.32 on YOLOv3 and 2.06 on YOLOv5) on the two white-box models compared with results of white-box attacks (12.35 on YOLOv3 and 7.98 on YOLOv5), which means that our method boosts the transferability of the adversarial patch not only between different models, but also between different samples.
































%---------------------------------------------------------------------

\vspace{-1ex}
\subsection{Attack in Large Vision-Language Models}\vspace{-1ex}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/big.pdf}
    \vspace{-2.0ex}
    \caption{Examples of transfer-based black-box attacks against Google's Bard.}
     \vspace{-3.7ex}
    \label{fig:bard}
\end{figure}


\textbf{Experimental settings.} 
\textbf{(1) Dataset:} We randomly choose 100 images from the NIPS2017 dataset. \textbf{(2) Surrogate models:} We adopt the vision encoders of ViT-B/16 \citep{vit},  CLIP \citep{radford2021learning_CLIP}, and BLIP-2 \citep{li2023blip2}  as surrogate models, using the attack objective as
\begin{equation}
    \max_{\bm{x}} \mathbb{E}_{f \in \mathcal{F}}[\|f(\bm{x})-f(\bm{x}_{nat})\|_2^2], \; \text{s.t. } \|\bm{x}-\bm{x}_{nat}\|_\infty \leq \epsilon,
\label{eq:loss_big_model}
\end{equation}
where the distance between the features of adversarial example and the original image is maximized. \textbf{(3) Testing models:}  We test the attack success rate of our generated adversarial examples on Google's Bard\footnote{\url{https://bard.google.com/}}. \textbf{(4) Hyper-parameters:} We use exactly the same hyper-parameters as in~\cref{sec:classification}.


\begin{wraptable}{r}{6.2cm}
\vspace{-4ex}
    \footnotesize
    \setlength{\tabcolsep}{1pt}
    \caption{\textbf{Black-box attack success rate (\%, $\uparrow$)} against Bard.}
    \scalebox{0.9}{
    \begin{tabular}{c|cccc}
    \hline
       Attack  & $\epsilon$ & Misdescription & Blindness & Refusal \\
    \hline
       SSA  & 16/255 & 4 & 16 & 2\\
       SSA-CWA  & 16/255 & 12 & 27 & 4\\
       \hline
       SSA  & 32/255 & 20  & 36 & 1 \\
       SSA-CWA  & 32/255 & 28  & 38 & 4\\
    \hline
    \end{tabular}
    }
    \label{tab:bard}
\end{wraptable}


\textbf{Results.} As shown in \cref{tab:bard}, SSA-CWA outperforms SSA by a large margin under both $\epsilon=16/255$ and $\epsilon=32/255$. We observe that adversarial examples can lead to three wrong behaviors of Bard, including misdescription, blindness, and refusal, as shown in \cref{fig:bard}. SSA-CWA leads to an 8\% increase in misdescription compared to SSA. Moreover, SSA-CWA exhibits a substantially higher rejection rate than SSA. These results underscore the potent efficacy of our algorithm against cutting-edge commercial large models, emphasizing the imperative to develop defenses tailored to such models. More results can be found in our follow-up work \citep{dong2023robust}.


% \textbf{Results.} We generate adversarial examples using perturbation budgets $\epsilon=16/255$ and $\epsilon=32/255$. For $\epsilon=16/255$, our SSA-CWA attains a 16\% attack success rate, surpassing SSA by 10\%. Furthermore, 27\% of the samples crafted by SSA-CWA are rejected by Bard, compared to 16\% rejection rate of SSA. For $\epsilon=32/255$, SSA-CWA achieves a 31\% attack success rate, significantly outperforming SSA's 21\% success rate. Additionally, Bard rejects 38\% of the samples crafted by SSA-CWA, while rejecting 36\% of those generated by SSA. Fig.~\ref{fig:bard} shows some examples for attacking Bard. These results underscore the potent efficacy of our algorithm against cutting-edge commercial large models, emphasizing the imperative to develop defenses tailored to such models.







\vspace{-1ex}
\subsection{Analysis and Discussion}
\vspace{-1ex}

In this section, we conduct some additional experiments to prove our claims about the effectiveness of our methods, regarding flattening the loss landscapes, encouraging the closeness between local optima, and computational efficiency compared to other algorithms.





\begin{figure*}[!t]
\vspace{-6ex}
\centering
\subfigure[MI]{
\includegraphics[width=4.3cm]{figures/MI-FGSM.pdf}
\label{figure:milandscape}
}
\subfigure[MI-CWA]{
\includegraphics[width=4.3cm]{figures/MI-CW.pdf}
\label{figure:cwlandscape}
}
\subfigure[$T$]{
\includegraphics[width=4.3cm]{figures/T.pdf}
\label{figure:ablateT}
}\vspace{-2ex}
\caption{\textbf{Additional results. }(a-b): The loss landscape around the convergence point optimized by MI and MI-CWA respectively. (c): Attack success rate under different attack iterations of $T$.}\vspace{-3ex}
\end{figure*}



\textbf{Visualization of landscape.}
We first analyze the relationship between the loss landscape and the transferability of adversarial examples by visualizing the landscapes by different algorithms (detailed in \cref{sec:visualizelandscape}). As shown in \cref{figure:milandscape} and \ref{figure:cwlandscape}, the landscape at the adversarial example crafted by MI-CWA is much flatter than that of MI. It is also noticeable that the optima of the models in MI-CWA are much closer than those in MI. This supports our claim that CWA encourages the flatness of the landscape and closeness between local optima, thus leading to better transferability.



\textbf{Computational efficiency.}
When the total number of  iterations $T$ in~\cref{alg:mi-cwa} remains the same, the proposed CWA algorithm requires twice the computational cost of MI, so it is unfair to compare with other algorithms directly. Therefore, we measure the average attack success rate among the 31 models in \cref{table:classification} of the adversarial examples crafted by different algorithms under different $T$. The result is shown in \cref{figure:ablateT}. Our algorithm outperforms other algorithms for any number of iterations, even if $T=5$ of the CWA algorithm. This indicates that the CWA algorithm is effective not because of the large number of iterations, but because it captures the common weakness of different models.

