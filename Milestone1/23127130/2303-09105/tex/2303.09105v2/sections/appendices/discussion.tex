

\section{Analysis and Discussions}

In this section, we conduct some additional experiments to prove our claims about the effectiveness of our methods, regarding flattening the loss landscapes and maximizing the cosine similarity of gradients to boost the transferability.




\subsection{Verification of the Approximation in CSE}
\label{sec:verificationofcse}
To verify whether $\bm{I}-\frac{\bm{g}_i\bm{g}_i^T}{\|\bm{g}_i\|_2} \approx \bm{I}$ holds, We first average the value of  $\bm{I}-\frac{\bm{g}_i\bm{g}_i^T}{\|\bm{g}_i\|_2}$ over NIPS17 dataset, and then downsample this matrix to (256, 256), finally we visualize this matrix in the form of heatmap. The result is shown in \cref{figure:heatmap}.

\begin{figure}[h]
    \centering
    \includegraphics[width=3.5cm]{./figures/heatmapofmatrix.pdf}
    \caption{Heatmap of the matrix.}
\label{figure:heatmap}
\end{figure}

As shown in the figure, the diagonal elements are close to 1, while the others are close to 0. As a result, the approximation $\bm{I}-\frac{\bm{g}_i\bm{g}_i^T}{\|\bm{g}_i\|_2} \approx \bm{I}$ is reasonable. This supports the Assumption \ref{assumption}.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure*}[t]
% \centering
% \subfigure[MI]{
% \includegraphics[width=4.3cm]{figures/MI-FGSM.pdf}
% \label{figure:milandscape}
% }
% \subfigure[MI-CWA]{
% \includegraphics[width=4.3cm]{figures/MI-CW.pdf}
% \label{figure:cwlandscape}
% }
% \subfigure[$T$]{
% \includegraphics[width=4.3cm]{figures/T.pdf}
% \label{figure:ablateT}
% }
% \caption{\textbf{Additional results. }(a-b): The loss landscape around the convergence point optimized by MI and MI-CWA respectively of different models. (c): Attack success rate under different attack iterations of $T$.}
% \end{figure*}









%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Visualization of Landscape}
\label{sec:visualizelandscape}


We illustrate our claim about the relationship between the loss landscape and the transferability of adversarial examples by visualizing the landscapes by different algorithms. In this part, we generate the adversarial example on the ensemble of six surrogate models used in~\cref{sec:classification} and test on all the defense models in RobustBench~\citep{croce2020robustbench}.

% In this sec\yichi{}we visualize the landscape of adversarial examples trained by different algorithms. The training model of each algorithm is the six models mentioned in \cref{sec:classification}, and the testing model is all the defense models mentioned in \cref{sec:classification}.


For each algorithm, we first craft an adversarial example $\bm{x}$ via this algorithm, which is expected to be the convergence point during training and near the optimum of each black-box model. Then, we use BIM~\citep{bim} to fine-tune the adversarial example on each black-box model to get the position of the optimum of each model $\bm{p}_i$ (note that this process is white-box). To visualize the loss landscapes of test models around $\bm{x}$, we perturb $\bm{x}$ along the unit direction $(\bm{p}_i -\bm{x})/||\bm{p}_i-\bm{x}||_{\infty}$ for each model and plot the loss curves in one plane. The results of MI and our method MI-CWA are shown \cref{figure:milandscape} and \cref{figure:cwlandscape} respectively.

As shown in \cref{figure:milandscape} and \ref{figure:cwlandscape}, the landscape at the adversarial example crafted by MI-CWA is much flatter than that of MI. It is also noticeable that the optima of the models in MI-CWA are much closer than those in MI. This supports our claim that CWA encourage the flatness of the landscape and closeness between local optima, thus leading to better transferability.




\subsection{Cosine Similarity of Gradients}
\label{sec:cosinesimilarity}


We also measure the average cosine similarity of the gradients between training (surrogate) models and testing (black-box) models. The results are shown in \cref{table:cosine}. Our method improves the cosine similarity of gradients both between training models and testing models compared with MI. It shows that our method is more likely to find the common weakness of the training models, and the common weakness of the training models tends to generalize to testing models.

\begin{table}[h]
\centering
\caption{\textbf{Cosine Similarity of gradients.} MI-CWA universally increases the cosine similarity of gradients among models, regardless of whether they are from $\mathcal{F}_{train}$ or $\mathcal{F}$.}
\begin{tabular}{l|ll}
\hline
                          & MI & MI-CWA  \\ 
\hline
$\mathcal{F}_{t}\times \mathcal{F}_{t}$ & 0.185   & 0.193  \\
$\mathcal{F}_{t}\times \mathcal{F}$            & 0.024   & 0.032  \\
$\mathcal{F}\times \mathcal{F}$                      & 0.061   & 0.076  \\
\hline
\end{tabular}
\label{table:cosine}
\end{table}









\subsection{Time complexity analysis}

To intuitively illustrate the time complexity of different methods in \cref{sec:classification}, we list their Number of Function Evaluations~(NFEs) in \cref{table:NFE}, where $n$ is the number of surrogate models. As shown, our methods MI-SAM, MI-CSE and MI-CWA are quite efficient. When combined with previous attacks, they maintain their efficiency and enhance their efficacy, showcasing the potential of integrating our approach with leading-edge attacks and optimizers.

We also test the real time cost of generating adversarial examples using different methods, utilizing the surrogate models mentioned in \cref{sec:classification}. As shown in \cref{table:NFE}, our method incurs only a slight computational overhead compared to the baseline attacks. This demonstrates that our method can serve as a plug-and-play algorithm to effectively and efficiently enhance transfer attack performance.


\begin{table}[h]
\small
\setlength{\tabcolsep}{2pt}
\centering
\caption{The number of function evaluations~(NFEs) and real time cost of methods that we use in \cref{sec:classification}.}
\label{table:NFE}
\scalebox{0.85}{
\begin{tabu}{c|cccccccccc|ccccc} 
\hline
Method & FGSM & BIM & MI  & DI  & TI  & VMI  & SVRE & PI  & SSA  & RAP                       & MI-SAM & MI-CSE & MI-CWA & VMI-CWA & SSA-CWA  \\ 
\hline
NFEs    & $n$    & 10$n$ & 10$n$ & 10$n$ & 10$n$ & 200$n$ & 30$n$  & 10$n$ & 200$n$ & 3400$n$ & 20$n$    & 10$n$    & 20$n$    & 210$n$    & 210$n$     \\
Time (s)    & 0.2    & 0.8 & 0.8 & 0.8 & 0.9 & 2.1 & 3.0 & 0.7  & 18.5  & 168.2 & 1.7    & 1.0    & 1.8    & 21.2    & 26.3     \\
\hline
\end{tabu}
}
\end{table}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
