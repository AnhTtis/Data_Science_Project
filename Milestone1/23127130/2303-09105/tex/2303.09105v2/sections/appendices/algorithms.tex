%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Algorithms}
\subsection{Sharpness Aware Minimization Algorithm under Infinity Norm}
\label{appendixB}


\begin{algorithm}[t] %tb
\small
\caption{MI-SAM}
\begin{algorithmic}[1]
   \REQUIRE
   natural image $\bm{x}_{nat}$, label $y$, perturbation budget $\epsilon$, 
   iterations $T$, loss function $L$, model ensemble $\mathcal{F}_{t}=\{f_i\}_{i=1}^n$, decay factor $\mu$, step sizes $r$, $\beta$ and $\alpha$.
   \STATE \textbf{Initialize:} $\bm{m}=0$, $\bm{x}_0=\bm{x}_{nat}$;
   \FOR{$t=0$ {\bfseries to} $T-1$}
   %\STATE \# first step
   \STATE Calculate $g=\nabla_{\bm{x}} L(\frac{1}{n}\sum_{i=1}^{n} f_i(\bm{x}_{t}),y)$;
   \STATE Update $\bm{x}_t$ by $\bm{x}_{t}^{r}=\mathrm{clip}_{\bm{x}_{nat},\epsilon}(\bm{x}_{t}+r\cdot \mathrm{sign}(\bm{g}))$;
   \STATE Calculate $g=\nabla_{\bm{x}} L(\frac{1}{n}\sum_{i=1}^{n} f_i(\bm{x}_{t}^r),y)$;
   \STATE Update $\bm{x}_t^r$ by $\bm{x}_{t}^{f}=\mathrm{clip}_{\bm{x}_{nat},\epsilon}(\bm{x}_{t}^r-\beta \cdot \mathrm{sign}(\bm{g}))$;
   \STATE Calculate the update $g=\bm{x}_{t}^{f}-\bm{x}_{t}$;
   \STATE Update momentum $\bm{m} = \mu \cdot \bm{m} + \bm{g}$;
   \STATE update $\bm{x}_{t+1}$ by $\bm{x}_{t+1}=\mathrm{clip}_{\bm{x}_{nat},\epsilon}(\bm{x}_{t}+\alpha \cdot (\bm{m}))$;
   \ENDFOR
\STATE \textbf{Return:} $\bm{x}_{T}$.
\end{algorithmic}
\label{alg:mi-sam}
\end{algorithm}


In this section, for simplicity, we represent $\mathbb{E}_{f_i \in \mathcal{F}_t}$ by $L(\bm{x})$.
Our training objective is slightly different from \citet{SAM}. We emphasize only the flatness of the landscape, without simultaneously minimizing $L(\bm{x})$. This approach yields better results:
\begin{equation}
\label{objective2}
    \min [\max_{\|\bm{\delta}\|_{\inf}<\epsilon} L(\bm{x}+\bm{\delta}) - L(\bm{x})].
\end{equation}
Although the Frobenius norm of the Hessian matrix $\|\bm{H}\|_F$ could also be a potential choice, it requires the computation of third-order derivatives with respect to $\bm{p}$, which is computationally expensive.

To optimize objective in  \cref{objective2}, first, we need to compute $\bm{\delta}$. We use the Taylor expansion to get $\bm{\delta}$:
\begin{equation*}
    \begin{aligned}
        \bm{\delta} &= \arg\max_{\|\bm{\delta}\|_{\inf}<\epsilon} L(\bm{x}+\bm{\delta}) \\
        &\approx \arg\max_{\|\bm{\delta}\|_{\inf}<\epsilon} L(\bm{x}) +\bm{\delta}^\top \mathrm{sign}(\nabla_{\bm{x}} L(\bm{x})) \\
        &= \arg\max_{\|\bm{\delta}\|_{\inf}<\epsilon} \bm{\delta}^\top \mathrm{sign}(\nabla_{\bm{x}} L(\bm{x})) \\
        &= \epsilon \cdot \mathrm{sign}(\nabla_{\bm{x}} L(\bm{x})).
    \end{aligned}
\end{equation*}
Then, we can get the derivative of $\nabla_{\bm{x}} L(\bm{x}+\bm{\delta})$:
\begin{equation}
\label{objective3}
    \begin{aligned}
        \nabla_{\bm{x}} L(\bm{x}+\bm{\delta}) 
        = \nabla_{\bm{x}+\bm{\delta}} L(\bm{x}+\bm{\delta}) \cdot \nabla_{\bm{x}} (\bm{x}+\bm{\delta})
        = \nabla_{\bm{x}+\bm{\delta}} L(\bm{x}+\bm{\delta}) + \nabla_{\bm{x}+\bm{\delta}} L(\bm{x}+\bm{\delta}) \cdot \nabla_{\bm{x}} \bm{\delta}.
    \end{aligned}
\end{equation}
Following previous works \citep{SAM,wei2023sharpness}, we discard the second term of  \cref{objective3} as it has negligible influence on the optimization of the SAM algorithm.

In general, the procedure of the SAM algorithm under the infinity norm involves first finding the worst case perturbation $\bm{\delta}$, followed by optimizing $L(\bm{x}+\bm{\delta}) - L(\bm{x})$. The detail is at \cref{alg:mi-sam}.














%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%v


\subsection{Derivation of Cosine Similarity Encourager}
\label{AppendixD}
In this section, our objective is to develop an algorithm that maximizes the cosine similarity between the gradients of each model. Directly pursuing this goal necessitates a second-order derivative with respect to the adversarial examples. However, this is both time-intensive and memory-intensive. Inspired by \citet{MAML}, we design our Cosine Similarity Encourager, as shown in Algorithm~\ref{alg:mi-cse}.



\begin{algorithm}[t] %tb
\small
\caption{MI-CSE algorithm}
\begin{algorithmic}[1]
   \REQUIRE
   natural image $\bm{x}_{nat}$, label $y$, perturbation budget $\epsilon$, 
   iterations $T$, loss function $L$, model ensemble $\mathcal{F}_{t}=\{f_i\}_{i=1}^n$, decay factor $\mu$, step sizes $\beta$ and $\alpha$.
   \STATE \textbf{Initialize:} $\bm{m}=0$, inner momentum $\hat{\bm{m}}=0$, $\bm{x}_0=\bm{x}_{nat}$;
   \FOR{$t=0$ {\bfseries to} $T-1$}
   \FOR{$i=1$ {\bfseries to} $n$}
%   \STATE pick the jth model $f_j$
   \STATE Calculate $\bm{g}=\nabla_{\bm{x}} L(f_i(\bm{x}_{t}^{i-1}),y)$;
   \STATE Update inner momentum by $\hat{\bm{m}} = \mu \cdot \hat{\bm{m}} + \frac{\bm{g}}{\|\bm{g}\|_2}$;
   \STATE Update $\bm{x}_t^i$ by $\bm{x}_{t}^{i} = \mathrm{clip}_{\bm{x}_{nat},\epsilon}(\bm{x}_{t}^{i-1}- \beta \cdot \hat{\bm{m}})$;
   \ENDFOR
   \STATE Calculate the update $\bm{g}=\bm{x}_{t}^{n}-\bm{x}_{t}$;
   \STATE Update momentum $\bm{m} = \mu \cdot \bm{m} + \bm{g}$;
   \STATE update $\bm{x}_{t+1}$ by $\bm{x}_{t+1}=\mathrm{clip}_{\bm{x}_{nat},\epsilon}(\bm{x}_{t}+\alpha \cdot \mathrm{sign}(\bm{m}))$;
   \ENDFOR
\STATE \textbf{Return:} $\bm{x}_T$.
\end{algorithmic}
\label{alg:mi-cse}
\end{algorithm}

In the following sections, we will demonstrate that this algorithm effectively maximizes the cosine similarity of the gradient between each model.


% ----------------- rewrite till here % ----------------------------------


\begin{theorem}
    Denote $\bm{g}_i$ as $\nabla_{\bm{x}} L(f_i(\bm{x}), y)$ and $\bm{g}_j$ as $\nabla_{\bm{x}} L(f_j(\bm{x}), y)$. When $\beta \to 0$, and the $\frac{\bm{g}_j\bm{g}_j^\top}{\|\bm{g}_j\|_2}$ is negligible, updating by Algorithm \ref{alg:mi-cse} will increase the cosine similarity of the gradient between each model.
\label{theorem:updatecosinesimilarity}
\end{theorem}



\begin{proof}
    The derivative of cosine similarity between gradient of each model can be written as:
\begin{equation*}
    \begin{aligned}
        \frac{\partial}{\partial \bm{x}}\frac{\bm{g}_i\bm{g}_j}{\|\bm{g}_i\|_2\|\bm{g}_j\|_2} = \frac{\bm{H}_i}{\|\bm{g}_i\|_2}\left(\bm{I}-\frac{\bm{g}_i\bm{g}_i^\top}{\|\bm{g}_i\|_2}\right)\frac{\bm{g}_j}{\|\bm{g}_j\|_2}+\frac{\bm{H}_j}{\|\bm{g}_j\|_2}\left(\bm{I}-\frac{\bm{g}_j\bm{g}_j^\top}{\|\bm{g}_j\|_2}\right)\frac{\bm{g}_i}{\|\bm{g}_i\|_2}.
    \end{aligned}
\end{equation*}

Because $\frac{\bm{g}_j\bm{g}_j^\top}{\|\bm{g}_j\|_2}$ is negligible, which is verified in \cref{sec:verificationofcse}, the gradient can be approximated as:
\begin{equation}
\label{objective5}
    \begin{aligned}
        \frac{\partial}{\partial \bm{x}}\frac{\bm{g}_i\bm{g}_j}{\|\bm{g}_i\|_2\|\bm{g}_j\|_2}
        \approx \frac{\bm{H}_i}{\|\bm{g}_i\|_2}\frac{\bm{g}_j}{\|\bm{g}_j\|_2}+\frac{\bm{H}_j}{\|\bm{g}_j\|_2}\frac{\bm{g}_i}{\|\bm{g}_i\|_2}.
    \end{aligned}
\end{equation}

Because $i, j$ only represent the $i$-th model and $j$-th model in $\mathcal{F}_{t}$, and we can permute the set $\mathcal{F}_{t}$, therefore we can get that:
\begin{equation*}
    \mathbb{E}\left[\frac{\bm{H}_i}{\|\bm{g}_i\|_2}\frac{\bm{g}_j}{\|\bm{g}_j\|_2}\right]=\mathbb{E}\left[\frac{\bm{H}_j}{\|\bm{g}_j\|_2}\frac{\bm{g}_i}{\|\bm{g}_i\|_2}\right].
\end{equation*}
Therefore the expectation of the derivative is:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}\left[\frac{\partial}{\partial \bm{x}}\frac{\bm{g}_i\bm{g}_j}{\|\bm{g}_i\|_2\|\bm{g}_j\|_2}\right]
        \approx 2 \mathbb{E}\left[\frac{\bm{H}_i}{\|\bm{g}_i\|_2}\frac{\bm{g}_j}{\|\bm{g}_j\|_2}\right].
    \end{aligned}
\end{equation*}


In the following, we will prove that the update of Algorithm \ref{alg:mi-cse} contains the gradient of vanilla loss $L(f_i(\bm{x}),y)$ and the derivative of the cosine similarity between the gradients of each model, which is shown in  \cref{objective5}.

Denote $\bm{x}^{i}$ as the adversarial example after the i-th iteration in the inner loop of the Algorithm \ref{alg:mi-cse}, $\bm{g}_i'$ as the gradient at i-th iteration in the inner loop, we can represent $\bm{g}_i'$ by $\bm{g}_i$ using Taylor expansion:
\begin{equation*}
    \begin{aligned}
        \bm{g}_i'&=\bm{g}_i+\bm{H}_i(\bm{x}^{i}-\bm{x}^1) \\
&=\bm{g}_i-\beta \bm{H}_i \sum_{j=1}^{i-1}\frac{ \bm{g}_j'}{\| \bm{g}_j'\|_2}  \\
&= \bm{g}_i-\beta \bm{H}_i \sum_{j=1}^{i-1}\frac{ \bm{g}_j+o(\beta)}{\| \bm{g}_j+o(\beta)\|_2} \\
&= \bm{g}_i-\beta \bm{H}_i \sum_{j=1}^{i-1}\frac{ \bm{g}_j}{\| \bm{g}_j\|_2} +O(\beta^2).
    \end{aligned}
\end{equation*}
The update over the entire inner loop is:
\begin{equation*}
    \begin{aligned}
        \bm{x}-\bm{x}^n=\beta \sum_{i=1}^{n}\frac{\bm{g}_i'}{\|\bm{g}_i'\|_2}.
    \end{aligned}
\end{equation*}
Substitute $\bm{g}_i'$ with $\bm{g}_i-\beta \bm{H}_i \sum_{j=1}^{i-1}\frac{ \bm{g}_j}{\| \bm{g}_j\|_2} +O(\beta^2)$, $\|\bm{g}_i'\|_2$ with $\|\bm{g}_i+O(\beta)\|_2 \approx \|\bm{g}_i\|_2$, we can get:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[\bm{x}-\bm{x}^n]=\mathbb{E}[\beta \sum_{i=1}^{n}[\bm{g}_i-\beta \bm{H}_i \sum_{j=1}^{i-1}\frac{ \bm{g}_j}{\| \bm{g}_j\|_2} +O(\beta^2)]/\|\bm{g}_i'\|_2] \\ 
=
\mathbb{E}[\beta \sum_{i=1}^{n}\frac{\bm{g}_i}{\|\bm{g}_i\|_2}-\beta^2 \sum_{i=1}^{n}\sum_{j=1}^{i-1}\bm{H}_i\frac{ \bm{g}_j}{\|\bm{g}_i\|_2\| \bm{g}_j\|_2} +\sum_{i=1}^{n}O(\beta^3)] \\
=\beta \mathbb{E}[ \sum_{i=1}^{n} \frac{\bm{g}_i}{\|\bm{g}_i\|_2}]-\frac{\beta^2}{2} \mathbb{E}[\sum_{i,j}^{i<j}\frac{\partial \frac{\bm{g}_i\bm{g}_j}{\|\bm{g}_i\|_2\|\bm{g}_j\|_2}}{\partial \bm{x}}]+\sum_{i=1}^{n}O(\beta^3).
    \end{aligned}
\end{equation*}

Consequently, using Algorithm \ref{alg:mi-cse} will enhance the cosine similarity between the gradients of each model. For the gradient ascent algorithm, we have two options: either multiply the training objective by -1 and address it as a gradient descent issue, or develop an algorithm tailored for gradient ascent.
\end{proof}









\subsection{Generalized Common Weakness Algorithm}
\label{sec:generalizedCW}
The CWA algorithm can be incorporated with not only MI, but also other arbitrary optimizers and attackers, such as Adam, VMI, and SSA. This is outlined in Algorithm \ref{algorithm:generalizedCW}. For each optimizer, the $step$ function takes the gradient with respect to $\bm{x}$ and performs gradient descent. For the generalized Common Weakness algorithm, we require a total of three optimizers. It's important to note that these three optimizers can either be of the same type or different types.


\begin{algorithm}[t] %tb
\caption{Generalized Common Weakness Algorithm (CWA)}
   
\begin{algorithmic}
   \REQUIRE
   image $\bm{x}_0$; label $y$; 
   total iteration $T$; loss function $L$, model set $F_{train}$; momentum weight $\mu$; inner optimizer $\beta$, reverse optimizer $r$, outer optimizer $\alpha$
   \STATE Calculate the number of models $n$
   \FOR{$t=1$ {\bfseries to} $T$}
   \STATE $\bm{o}=copy(\bm{x})$
   \STATE \# first step
   \STATE calculate the gradient $\bm{g}=\nabla_{\bm{x}} L(\frac{1}{n}\sum_{i=1}^{n} f_i(\bm{x}),y)$
   \STATE update $\bm{x}$ by $r.step(-\bm{g})$
   \STATE \# second step
   \FOR{$j=1$ {\bfseries to} $n$}
   \STATE pick the jth model $f_j$
   \STATE calculate the gradient $\bm{g}=\nabla_{\bm{x}} L(f_j(\bm{x}),y)$
   \STATE update $\bm{x}$ by $\beta.step(\frac{\bm{g}}{\|\bm{g}\|_2})$
   \ENDFOR
   \STATE calculate the update in this iteration $\bm{g}=\bm{o}-\bm{x}$
   \STATE update $\bm{x}$ by $\alpha.step(\bm{g})$
   \ENDFOR
\STATE return $\bm{x}$
\end{algorithmic}
\label{algorithm:generalizedCW}
\end{algorithm}

