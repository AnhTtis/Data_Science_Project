
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\section{Proofs and Derivations}





\subsection{Proof of Theorem \ref{theorem:upperbound}}
\label{sec:upperboundproof}
With HÃ¶lder's inequality, we can prove this theorem:
\begin{proof}
    \begin{equation*}
        \begin{aligned}
             &\mathbb{E}[(\bm{x}-\bm{p}_i)^\top \bm{H}_i(\bm{x}-\bm{p}_i)] \\
= & \mathbb{E}[\|(\bm{x}-\bm{p}_i)\|_p \|\bm{H}_i(\bm{x}-\bm{p}_i)\|_q]  \quad(\text{where}\; \frac{1}{p}+\frac{1}{q}=1)\\
\leq& \mathbb{E}[\|(\bm{x}-\bm{p}_i)\|_p \|\bm{H}_i\|_{r,q}\|(\bm{x}-\bm{p}_i)\|_r] \\
= & \mathbb{E}[\|\bm{H}_i\|_{r,q}] \mathbb{E}[\|(\bm{x}-\bm{p}_i)\|_p \|(\bm{x}-\bm{p}_i)\|_r],
        \end{aligned}
    \end{equation*}
where $\|\cdot\|_{r,q}$ is an induced matrix norm. 

\textbf{Special case}: When $p=q=r=2$, we have
\begin{equation*}
    \mathbb{E}[(\bm{x}-\bm{p}_i)^\top \bm{H}_i(\bm{x}-\bm{p}_i)] \leq \mathbb{E}[\|\bm{H}_i\|_2] \mathbb{E}[\|(\bm{x}-\bm{p}_i)\|_2^2],
\end{equation*}
where $\|\bm{H}_i\|_2$ is the spectral norm of $\bm{H}_i$. As we also have $\|\bm{H}_i\|_2 \leq \|\bm{H}_i\|_F$, we obtain
\begin{equation*}
    \mathbb{E}[(\bm{x}-\bm{p}_i)^\top \bm{H}_i(\bm{x}-\bm{p}_i)] \leq \mathbb{E}[\|\bm{H}_i\|_F] \mathbb{E}[\|(\bm{x}-\bm{p}_i)\|_2^2],
\end{equation*}
where $\|\bm{H}_i\|_F$ is the Frobenius norm of $\bm{H}_i$.
\end{proof}

Note that both the spectral norm and Frobenius norm of the Hessian matrix correspond to the flatness of loss landscape, such that we can adopt sharpness aware minimization for optimization.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{The generalization ability of common weakness}
\label{appedixA}


Let $\bm{c}_t$ be the final optimum of the training objective $\sum_{f_i \in \mathcal{F}_t} \frac{1}{n} L(f_i(\bm{x}), y)$ that we converge to, and let $\bm{c}$ be the optimum of the testing objective $\mathbb{E}_{f_i \in \mathcal{F}} L(f_i(\bm{x}), y)$ nearest to $\bm{c}$. Let $O_{\bm{c}_t}$ be the set of closest optimum of model $f_i \in \mathcal{F}_t$ to $\bm{c}_t$, and let $O_c$ be the set of closest optimum of model $f_i \in F$ to $\bm{c}$. Let the hessian matrices at $\bm{p}_i \in O_c$ form a set $H_c$, and the hessian matrices at $\bm{p}_i \in O_{\bm{c}_t}$ form a set $H_{\bm{c}_t}$. To find the optimum $\bm{c}$ with a smaller $\mathbb{E}_{\bm{H}_i \in H_c}[\|\bm{H}_i\|_F]$ and $\mathbb{E}_{\bm{p}_i \in O_{c}}[\|(\bm{c}-\bm{p}_i)\|_2^2]$ at test time, we need to optimize $\frac{1}{n}\sum_{\bm{H}_i \in H_{\bm{c}_t}}\|\bm{H}_i\|_F$ and $\frac{1}{n}\sum_{\bm{p}_i \in O_{c_{t}}}\|(\bm{c}_t-\bm{p}_i)\|_2^2$ at training time. In the following, we will show that both terms have a strong relationship between the training objective and the test error. Therefore, optimizing the training objective will lead to a smaller test error, consequently, a smaller generalization error.

Several studies~\citep{visualoss, wu2017toward, chen2022bootstrap} have shown that the $\|\bm{H}_i\|_F$ term corresponds to the flatness of the landscape. A flatter landscape during training has consistently been associated with improved generalization ability, which pertains to the transferability when training adversarial examples. The $\|(\bm{c}-\bm{p}_i)\|_2^2$ term, which measures the distance between the local optima, also exhibits a strong relationship between the training and testing phases. Intuitively, assuming that \((\bm{c}-\bm{p}_i)\) follows some distribution with the existing variance, the training objective is merely an empirical observation from the testing objective. Thus, the testing objective can be probabilistically bounded by the training objective through a convergence bound like the Chebyshev inequality, i.e.,
\begin{equation*}
    \mathbb{P}\{|\frac{1}{n-1}\|\bm{c}_t-\bm{p}_i\|_2^2-\mathbb{E}_{\bm{p}_i \in O_c}[\|(\bm{c}-\bm{p}_i)\|_2^2]| > \delta\} \leq \frac{\text{Var}\|\bm{c}_t-\bm{p}_i\|_2^2}{(n-1)^2\delta^2}.
\end{equation*}
In other words, as long as \(\text{Var}\|\bm{c}_t-\bm{p}_i\|_2^2\) exists, the smaller the training error, the higher the probability that the test error are smaller.

In the following, we consider a simple case (Assumption \ref{assumption}) where the local optima are Gaussianly distributed, the following theorem shows that smaller values of $\frac{1}{n}\sum_{\bm{p}_i \in O_{\bm{c}_t}}\|(\bm{c}_t-\bm{p}_i)\|_2^2$ tend to lead to smaller values of $\mathbb{E}_{\bm{p}_i \in O_c}[\|(\bm{c}-\bm{p}_i)\|_2^2]$.

\begin{assumption}
The optimum of each model $\bm{p}_i$ in $O_c$ follows a Gaussian distribution with mean $\bm{c}$ and unknown variance $\sigma^2$. Meanwhile, the optimum of the ensemble training model, $\bm{c}_t$, is given by the mean of all $\bm{p}_i$ within $O_{\bm{c}_t}$. That is:
\begin{equation*}
    \begin{aligned}
        &\forall \bm{p} \in O_{\bm{c}}, \ \bm{p} \sim N(\bm{c}, \sigma^2 \bm{I}); \\
        &\bm{c}_t = \frac{1}{n}\sum_{\bm{p}_i \in O_{\bm{c}_t}} \bm{p}_i.
    \end{aligned}
\end{equation*}
\label{assumption}
\end{assumption}




\begin{theorem}
\label{theorem:relationship}
Denote $F(m, n)$ as F-distribution with parameter $m$ and $n$, $F_{\alpha}(m, n)$ as the upper alpha quantile, For any two different optimum of ensemble model $\bm{c}^1$ and $\bm{c}^2$ and corresponding $s_1^2=\frac{1}{n}\sum_{\bm{p}_i \in O_{c_{t}^1}}(\bm{p}_i-c_{t}^1)^2$, $s_2^2=\frac{1}{n}\sum_{\bm{p}_i \in O_{c_{t}^2}}(\bm{p}_i-c_{t}^2)^2$, if \(\frac{s_1^2}{s_2^2} \geq F_\alpha(n-1, n-1)\), then
\begin{equation}
    \mathbb{E}_{\bm{p}_i \in O_{\bm{c}^1}}[\|(\bm{c}_1-\bm{p}_i)\|^2] \geq \mathbb{E}_{\bm{p}_i \in O_{\bm{c}^2}}[\|(\bm{c}_2-\bm{p}_i)\|^2]
\end{equation}
holds with type one error of \(\alpha\).
\end{theorem}

This theorem suggests that when the optima of surrogate models are closer, as indicated by a smaller value of $\frac{1}{n}\sum_{\bm{p}_i \in O_{\bm{c}_t}}\|(\bm{c}_t-\bm{p}_i)\|_2^2$, the optima of the target models also tend to be closer, which is represented by a smaller value of $\mathbb{E}_{\bm{p}_i \in O_c}[\|(\bm{c}-\bm{p}_i)\|_2^2].$ 


\begin{proof}
The training models $F_t$ can be viewed as sampling from the set of all models $F$. Therefore, the sample variance $s^2$ is:
\begin{equation*}
    s^2=\frac{1}{n}\sum_{\bm{p}_i \in O_{\bm{c}_t}}(\bm{p}_i-\bm{c}_t)^2.
\end{equation*}




Because that $\frac{s^2}{n\sigma^2}$ follows chi-square distribution, that is:
\begin{equation*}
    s^2=\sum_{i=1}^{n}(\bm{p}_i-\bm{c}_t)^2
    = \sum_{i=1}^{n}(\bm{p}_i-\frac{1}{n} \sum_{j=1}^{n} \bm{p}_i)^2
    \sim n\sigma^2 \cdot  \mathcal{X}^2(n-1).
    \end{equation*}
    Consequently, $\frac{s_1^2\sigma_2^2}{s_2^2\sigma_1^2}$ follows $F$ distribution.
    \begin{equation*}
    \frac{s_1^2}{s_2^2} \sim \frac{\sigma_1^2}{\sigma_2^2} \cdot F(n-1, n-1).
    \end{equation*}
    We perform F-test here. We set the null hypothesis \(H_0=\{\sigma_1\leq \sigma_2\}\), and the alternative hypothesis \(H_1=\{\sigma_1\geq \sigma_2\}\). Thus, if \(\frac{s_1^2}{s_2^2} \geq F_\alpha(n-1, n-1)\), with type one error of $\alpha$:
    \begin{align*}
        \sigma_1^2 \geq \sigma_2^2 \rightarrow \sigma_1 \geq \sigma_2.
    \end{align*}



This indicates that smaller value of $\frac{1}{n}\sum_{i=1}^{n}(\bm{p}_i-\bm{c}_t)^2$ tends to indicate a smaller value of $\sigma$. Next, we will demonstrate that a smaller $\sigma$ implies enhanced transferability of our adversarial examples.

\begin{lemma}
\label{lemma:A2}
    $\mathbb{E}_{\bm{p}_i \in O_c}[\|(\bm{c}-\bm{p}_i)\|_2^2]$ is a function that monotonically increases with $\sigma$. That is:
    \begin{equation*}
        \frac{\partial}{\partial \sigma}\mathbb{E}_{\bm{p}_i \in O_c}[\|(\bm{c}-\bm{p}_i)\|_2^2] \geq 0.
    \end{equation*}
\end{lemma}


Reparameterize $\bm{p}_i$ as $\sigma \bm{\epsilon}_i + c$, where $\bm{\epsilon}_i \sim N(0, I)$. We can get:
\begin{equation*}
    \begin{aligned}
         &\mathbb{E}_{\bm{p}_i \in O_c}[\|(\bm{c}-\bm{p}_i)\|] \\
         =& \mathbb{E}_{\bm{\epsilon} \sim N(0, I)}[\|\sigma \bm{\epsilon}\|] \\
         =& \sigma \mathbb{E}_{\bm{\epsilon} \sim N(0, I)}[\| \bm{\epsilon}\|].
    \end{aligned}
\end{equation*}
Therefore $\frac{\partial}{\partial \sigma}\mathbb{E}_{\bm{p}_i \in O_c}[\|(\bm{c}-\bm{p}_i)\|_2^2] \geq 0$. Importantly, the expectation of any norm of $\bm{c}-\bm{p}_i$ monotonically increases with the parameter $\sigma$, not limited solely to the second norm. 



\end{proof}

Therefore, both $\|\bm{H}_i\|_F$ and $\|\bm{p}_i-\bm{c}\|_2^2$ exhibit a strong correlation between the training and testing phase, indicating that encouraging the flatness of the landscape and closeness of local optima can result in improved generalization ability.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%v
\subsection{Proof of Theorem \ref{theorem:objective2upperbound}}
\label{appendixC}

In this section, we aim to prove that dot product similarity between gradient of each model is the upper bound of $\frac{1}{n}\sum_{i=1}^{n}(\bm{c}-\bm{p}_i)^2$.

Using Cauchy-Swartz theorem, we can get:
\begin{equation*}
    \begin{aligned}
        \sum_{i=1}^{n}\|(\bm{c}-\bm{p}_i)\|_2^2 
 = \sum_{i=1}^{n}(\bm{H}_i^{-1}\bm{g}_i)^{\top}(\bm{H}_i^{-1}\bm{g}_i) 
 = \sum_{i=1}^{n}\|(\bm{H}_i^{-1}\bm{g}_i)\|_2^2 
\leq \sum_{i=1}^{n}\|\bm{H}_i^{-1}\|_F^2\|\bm{g}_i\|_2^2. 
    \end{aligned}
\end{equation*}
The treatment of $\|\bm{H}_i\|_F$ has already been discussed in Appendix \ref{appendixB}. In this section, we set $M$ as the maximum value of $\|\bm{H}_i^{-1}\|_F^2$, which allows us to obtain the following result:
\begin{equation*}
    \begin{aligned}
        \sum_{i=1}^{n}\|(\bm{c}-\bm{p}_i)\|_2^2 
        \leq M \sum_{i=1}^{n}\bm{g}_i^{\top}\bm{g}_i = M\left[(\sum_{i=1}^{n}\bm{g}_i)^2-2\sum_{i=1}^{n}\sum_{j=1}^{i-1}\bm{g}_i\bm{g}_j\right].
    \end{aligned}
\end{equation*}
Since $\bm{c}$ is the optimal solution for the ensemble model, we have $(\sum_{i=1}^{n}\bm{g}_i)^2=0$. Consequently, our final training objective is:
\begin{equation}
    \max \sum_{i=1}^{n}\sum_{j=1}^{i-1}\bm{g}_i\bm{g}_j,
\end{equation}
Which is the dot product similarity between the gradient of each model. However, maximizing this dot product similarity can lead to gradient explosion, making it incompatible with MI-SAM. Therefore, we opt to maximize the cosine similarity between the gradients of each model.


The equality sign holds when $\bm{p}_i=\bm{x}$ for all $i$. This condition implies that the local optimum for each model is identical, which results in a dot product similarity of 0 between the gradients.
