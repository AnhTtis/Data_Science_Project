\section{Introduction}
\label{sec:intro}


Deep learning has achieved remarkable progress over the past decade and has been widely deployed in real-world applications \citep{lecun2015deep}. However, deep neural networks (DNNs) are vulnerable to adversarial examples \citep{szegedy2013intriguing,goodfellow2014explaining}, which are crafted by imposing human-imperceptible perturbations to natural examples. Adversarial examples can mislead the predictions of the victim model, posing great threats to the security of DNNs and their applications~\citep{bim, Sharif2016Accessorize}. The study of generating adversarial examples, known as adversarial attack, has attracted tremendous attention since it can provide a better understanding of the working mechanism of DNNs \citep{dong2017towards}, evaluate the robustness of different models~\citep{carlini2017towards}, and help to design more robust and reliable algorithms~\citep{pgd}.


% \begin{figure}[t]
% \centering
% \includegraphics[width=0.5\columnwidth]{./figures/figure1.pdf}
% \caption{Black-box attack success rate (\%) on adversarially trained models by different algorithms. The proposed MI-CWA, which crafts the adversarial examples on an ensemble of models, outperforms existing methods in transfer attack with significant margins, because the optimized noises are at flat landscape and close to the local minima of victim models. The models are detailed in \cref{sec:classification}.}
% \vspace{-1ex}
% \label{figure:1}
% \end{figure}


Adversarial attacks can be generally categorized into white-box and black-box attacks according to the adversary's knowledge of the victim model. 
With limited model information, black-box attacks either rely on query feedback \citep{chen2017zoo} or leverage the transferability \citep{Liu2016} to generate adversarial examples. Particularly, transfer-based attacks use adversarial examples generated for white-box surrogate models  to mislead black-box models, which do not demand query access to the black-box models, making them more practical in numerous real-world scenarios \citep{Liu2016,MI}. With the development of adversarial defenses~\citep{pgd,tramer2017ensemble,wong2020fast, wei2023jailbreak} and diverse network architectures~\citep{vit,liu2021swin,liu2022convnext}, the transferability of existing methods can be largely degraded.



By making an analogy between the transferability of adversarial examples and the generalization of deep neural networks \citep{MI}, many researchers improve the transferability by designing advanced optimization algorithms to avoid undesirable optimum \citep{MI, NI, vmi, emi} or leveraging data augmentation strategies to prevent overfitting \citep{DI,TI,NI}. Meanwhile, generating adversarial examples for multiple surrogate models can further improve the transferability, similar to training on more data to improve model generalization \citep{MI}. Early approaches simply average the outputs of multiple models in loss \citep{Liu2016} or in logits \citep{MI}, but ignore their different properties. %However, this method is less effective compared to modern attack methods, despite being simple and computationally cheap. 
\citet{svre} introduce the SVRG optimizer to reduce the variance of gradients of different models during optimization. Nevertheless, recent studies \citep{yang2021trs} demonstrate that the variance of gradients does not always correlate with the generalization performance.
%, highlighting the need for alternative approaches.
From the model perspective, increasing the number of surrogate models can reduce the generalization error
\citep{tsea}, and some studies \citep{li2020learning,tsea} propose to create surrogate models from existing ones. However, the number of surrogate models in adversarial attacks is usually limited due to the high computational costs, making it necessary to develop new algorithms that can leverage fewer surrogate models while achieving improved attack success rates.


% and the surrogate models created by heuristic methods do not satisfy the i.i.d. assumption, which limits their ability to improve the generalization (\ie, transferability).


\begin{figure*}[t]
\vspace{-6ex}
\centering
\subfigure[Not flat and not close]{
\includegraphics[width=3cm]{./figures/notflatnotclose.pdf}
\label{figure:notflatnotclose}
}
\subfigure[Flat and not close]{
\includegraphics[width=3cm]{./figures/flatnotclose.pdf}
\label{figure:flatnotclose}
}
\subfigure[Not flat and close]{
\includegraphics[width=3cm]{./figures/notflatclose.pdf}
\label{figure:notflatclose}
}
\subfigure[Flat and close]{
\includegraphics[width=3cm]{./figures/flatclose.pdf}
\label{figure:flatclose}
}\vspace{-1ex}
\caption{\textbf{Illustration of Common Weakness.} The generalization error is strongly correlated with the flatness of loss landscape and the distance between the solution and the closest local optimum of each model. We define the common weakness of model ensemble as the solution that is at the flat landscape and close to local optima of training models, as shown in (d).}
\vspace{-1ex}
\label{figure:illustration}
\end{figure*}

In this paper, we rethink the ensemble methods in adversarial attacks. By using a quadratic approximation of the expected attack objective over the victim models, we observe that the second-order term involves the Hessian matrix of the loss function and the squared $
\ell_2$ distance to the local optimum of each model, both of which are strongly correlated with the adversarial transferability (as illustrated in Fig.~\ref{figure:illustration}), especially when there are only a few models in the ensemble. Based on these two terms, we define the \textbf{common weakness} of an ensemble of models as the solution that is at the flat landscape and close to the models' local optima. To generate adversarial examples that exploit the common weakness of model ensemble, we propose a \textbf{Common Weakness Attack (CWA)} composed of two sub-methods named Sharpness Aware Minimization (SAM) and Cosine Similarity Encourager (CSE), which are designed to optimize the two properties of common weakness separately. 
%Assuming the independence between the two terms, we empirically and theoretically validate their positive effects on the transferability, \ie, generalization of adversarial examples, and optimize these two terms individually. Based on these two terms, we propose a novel property of common weakness to illustrate whether the generated adversarial examples are at the flat landscape and close to the models' optima. This results in two sub-methods named Sharpness Aware Minimization (SAM)~\citep{SAM} for adversarial attack and Cosine Similarity Encourager (CSE), which can be combined together as Common Weakness Attacker (CWA). 
As our methods are orthogonal to previous methods, \eg, MI \citep{MI},  VMI \citep{vmi}, SSA \citep{long2022frequency}, our methods can be incorporated with them seamlessly for improved performance. 
%For instance, both MI-SAM and MI-CSE can significantly enhance the transferability of MI-FGSM and the performance is further improved when they are fused as MI-CWA. 

% In this paper, we propose Common Weakness Attack (CWA) to further boost the transferability of ensemble attack\yinpeng{what challenges do we address?}, by generating adversarial examples at flatter loss landscapes with closer victim model optima around them. Instead of training adversarial examples directly with empirical average objective over surrogate models, we use a quadratic approximation of the expected objective over the target models and focus on the second-order term which contains the Hessian matrix and the distance from the local optimum for each model. Assuming the independence between these two terms, we could theoretically and empirically prove their positive effects on transferability, \ie, generalization of adversarial examples, and optimize these two terms individually. This results in two sub-methods named Sharpness Aware Minimization (SAM)~\citep{SAM} for adversarial attack and Cosine Similarity Encourager (CSE), which can be combined together as CWA. Since the proposed methods is orthogonal to previous gradient-based or transformation-based methods like MI-FGSM~\citep{MI}, VMI-FGSM~\citep{vmi}, TI-FGSM~\citep{TI} and Adam~\citep{kingma2014adam}, they can be incorporated with them easily to achieve better performance. For instance, both MI-SAM and MI-CSE can significantly enhance the transferability of MI-FGSM and the performance is further improved when they are fused as MI-CWA. 



% In this work, we rethink the model ensemble in adversarial examples training. We discover that the upperbound of generalization error strongly correlated with the distance between the optimum of the ensemble model and each model and the Frobenius norm of the hessian matrix. We both theoretically and empirically show that the poor generalization of ERM is due to the inconsistency of training loss and test loss, \textbf{but the training loss and the testing loss of these two term discovered by us are strongly correlated, therefore they are strongly correlated with generalization error and adversarial example transferability}. Inspired by this, we propose a novel attack scheme, named Common Weakness Attackers(CWA), to boost the transferability of ensemble-attack. More specifically, CWA algorithm consist of two algorithm named SAM, CSE to minimize the Frobenius norm of hessian matrix and the distance between the optimum of ensemble model and each model respectively. Finally, we incorporate our algorithm with MI-FGSM~\citep{MI}, TI-FGSM~\citep{TI}, VMI-FGSM~\citep{vmi} and Adam optimizer~\citep{kingma2014adam} to further boosting the transferability of adversarial examples.


% We conduct comprehensive experiments to verify the attack effectiveness and efficiency of our method. We first apply ASC in the popular dataset COCO~\citep{lin2015microsoft} 
% %which is considered to be a generic dataset for object detection. We 
% and carry out adversarial attacks in three representative tasks, including object vanishing, object mislabeling and bounding box shift. We also test our method in auto-driving with two different datasets of CityScapes~\citep{Cordts2016Cityscapes} and BDD100K~\citep{bdd100k} under the task of object vanishing. Experimental results show that, with the same $\ell_0$ budget (\eg, no more than 5\% of the object area in the COCO case), we can achieve better attack results compared to other manually designed patterns, especially for two-stage detectors and DETR. The convergence curves and transfer-based attacks indicate that ASC achieves a successful attack with fewer iterations of texture optimization and better transferability in black-box adversarial attacks thanks to the appropriate contour priors. To our best knowledge, this is the first attempt to conduct $\ell_0$ attack on DETR. Our results suggest that Transformer-based detectors are vulnerable to perturbations with semantics.

We conduct extensive experiments to confirm the superior transferability of adversarial examples generated by our methods. We first verify in image classification for 31 victim models with various architectures (\eg, CNNs~\citep{resnet}, Transformers~\citep{vit,liu2021swin}) and training settings (\eg, standard training, adversarial training~\citep{salman2020adversarially,wong2020fast}, input purification~\citep{naseer2020self_NRP,nie2022diffpure}). Impressively, when attacking the state-of-the-art defense models, MI-CWA surpasses MI with the logits ensemble strategy \citep{MI} by 30\% under the black-box setting. We extend our method to object detection by generating a universal adversarial patch that achieves an averaged mAP of 9.85 over 8 modern detectors, surpassing the recent work~\citep{tsea}. Ablation studies also validate that our methods indeed flatten the loss landscape and increase the cosine similarity of gradients of different models.
Moreover, we demonstrate successful black-box attacks against a recent large vision-language model (\ie, Google's Bard) based on our method, outperforming the baseline method by a large margin.  
%and also discuss about the computational efficiency. 
%The comprehensive quantitative analysis demonstrates the effectiveness of our proposed methods.
 
%\yichi{Do we need to list the contributions point-by-point?}

% We named our methods after Common Weakness Attackers (CWA). We test our algorithm on several settings, including attacking the state-of-the-art models on the imagenet, the defense models, and object detectors. Our algorithm outperforms the baseline a lot. More surprisingly, when attacking the state-of-the-art defense models, MI-CWA surpasses MI-FGSM 30\% at the black-box setting. Generally, our methods are easy to use and get better with more models. We hope our algorithm can be applied to real scenarios as an effective and straightforward attacker.


%In summary, our contributions are:
%\begin{itemize}
%    \item 
%\end{itemize}