\section{Methodology}
\label{sec:method}

In this section, we present our formulation of common weakness based on a second-order approximation and propose the \textbf{Common Weakness Attack (CWA)} composed of Sharp Aware Minimization (SAM) and Cosine Similarity Encourager (CSE).

\subsection{Preliminaries}

We let $\mathcal{F} := \{f\}$ denote the set of possible image classifiers for a given task, where each classifier $f: \mathbb{R}^D \rightarrow \mathbb{R}^K$ outputs the logits over $K$ classes for an input $\bm{x}\in\mathbb{R}^D$. Given a natural image $\bm{x}_{nat}$ and the corresponding label $y$, transfer-based attacks aim to craft an adversarial example $\bm{x}$ that could be misclassified by the models in $\mathcal{F}$. It can be formulated as a constrained optimization problem: 
\begin{equation}
    \min_{\bm{x}} \mathbb{E}_{f\in \mathcal{F}}[L(f(\bm{x}),y)],\;\text{s.t. }\|\bm{x}-\bm{x}_{nat}\|_\infty \leq \epsilon, 
\label{eq:formulation}
\end{equation}
where $L$ is the loss function, \eg, negative cross-entropy loss as $L(f(\bm{x}), y) = \log (\mathrm{softmax}(f(\bm{x}))_y)$, and we study the $\ell_\infty$ norm. 
\cref{eq:formulation} can be approximated using a few ``training'' classifiers $\mathcal{F}_t := \{f_i\}_{i=1}^n\subset \mathcal{F}$ (known as ensemble) as $\frac{1}{n} \sum_{i=1}^n L(f_i(\bm{x}),y)$  (\ie, loss ensemble \citep{Liu2016}) or $ L(\frac{1}{n} \sum_{i=1}^n f_i(\bm{x}),y)$ (\ie, logits ensemble \citep{MI}).
Previous works~\citep{MI,TI,DI,vmi} propose gradient computation or input transformation methods based on this empirical loss for better transferability. For example, the momentum iterative (MI) method \citep{MI} performs gradient update (as shown in Fig. \ref{figure:samoptimizingmi}) as
\begin{equation*}
    \bm{m} = \mu\cdot\bm{m}+\frac{\nabla_{\bm{x}}L(\frac{1}{n}\sum_{i=1}^n f_i(\bm{x}), y)}{\|\nabla_{\bm{x}}L(\frac{1}{n}\sum_{i=1}^n f_i(\bm{x}), y)\|_1}; \quad \bm{x}_{t+1} = \mathrm{clip}_{\bm{x}_{nat}, \epsilon}(\bm{x}_t+\alpha\cdot\mathrm{sign}(\bm{m})),
\end{equation*}
where $\bm{m}$ accumulates the gradients, $\mu$ is the decay factor, $\alpha$ is the step size, $\mathrm{clip}_{\bm{x}_{nat},\epsilon}(\bm{x})$ would project $\bm{x}$ to the $\ell_\infty$ ball around $\bm{x}_{nat}$ with radius $\epsilon$, and we use the logits ensemble strategy here.

\subsection{Motivation of Common Weakness}
\label{sec:motivation}

Although the existing methods can improve the transferability to some extent, a recent work \citep{tsea} points out that the optimization of adversarial example conforms to Empirical Risk Minimization (ERM)~\citep{erm} and the limited number of training models could lead to a large generalization error. 
In this paper, we consider a quadratic approximation of the objective in \cref{eq:formulation}. 
%By doing so, the flatness of loss landscape and the closeness between optima of different models, which are proven beneficial for the generalization and transferability of the trained adversarial example, appear explicitly in the second-order term. \cref{figure:illustration} provides a visual illustration of the impacts on generalization from the flatness of landscape and closeness between optima -- a solution at a flatter landscape with closer optima around it results in smaller generalization error (\ie, better transferability), which makes up the proposed concept of \textbf{common weakness} among victim models. 



Formally, we let $\bm{p}_i$ denote the closest optimum of model $f_i \in \mathcal{F}$ to $\bm{x}$ and $\bm{H}_i$ denote the Hessian matrix of $L(f_i(\bm{x}), y)$ at $\bm{p}_i$. We employ the second-order Taylor expansion to approximate \cref{eq:formulation} at $\bm{p}_i$ for each model $f_i$ as
\begin{equation}
    \mathbb{E}_{f_i \in \mathcal{F}}\left[L(f_i(\bm{p}_i), y)+\frac{1}{2}(\bm{x}-\bm{p}_i)^\top \bm{H}_i(\bm{x}-\bm{p}_i)\right].
    %+o(\|(\bm{x}-\bm{p}_i)\|^3)
\label{eq:expansion}
\end{equation}
In the following, we  omit the subscript of the expectation in \cref{eq:expansion} for simplicity.
Based on \cref{eq:expansion} , we can see that a smaller value of $\mathbb{E}[L(f_i(\bm{p}_i), y)]$ and $\mathbb{E}[(\bm{x}-\bm{p}_i)^\top \bm{H}_i(\bm{x}-\bm{p}_i)]$ means a smaller loss on testing models, \ie, better transferability. The first term represents the loss value of each model at its own optimum $\bm{p}_i$. Some previous works \citep{alllocalareglobal,localisglobal} have proven that the local optima have nearly the same value with the global optimum in neural networks. Consequently, there may be no room for further improving this term, and we put our main focus on the second term to achieve better transferability. 
%We aim to minimize the second term to achieve better transferability. 

In the following theorem, we derive an upper bound for the second term, since directly optimizing it which requires third-order derivative is intractable. 
\begin{theorem}
\label{theorem:upperbound}
(Proof in \cref{sec:upperboundproof}) Assume that the covariance between $\|\bm{H}_i\|_F$ and $\|\bm{p}_i-\bm{x}\|_2$ is zero, we can get the upper bound of the second term as
    \begin{equation}
    \mathbb{E}[(\bm{x}-\bm{p}_i)^\top \bm{H}_i(\bm{x}-\bm{p}_i)] \leq \mathbb{E}[\|\bm{H}_i\|_F]\mathbb{E}[\|(\bm{x}-\bm{p}_i)\|_2^2].
    \label{eq:upperbound}
\end{equation}
\end{theorem}

Intuitively, $\|\bm{H}_i\|_F$ represents the sharpness/flatness of loss landscape~\citep{tu2016low,normalizedflatminima,losslandscapeandoptimizationinoverparameterized,wei2023sharpness} and $\|(\bm{x}-\bm{p}_i)\|_2^2$ describes the translation of landscape. The two terms can be assumed independent and therefore their covariance can be assumed as zero. 
As shown in \cref{theorem:upperbound}, a smaller value of $\mathbb{E}[\|\bm{H}_i\|_F]$ and $\mathbb{E}[\|(\bm{x}-\bm{p}_i)\|_2^2]$ provides a smaller bound and further leads to a smaller loss on testing models. Previous works have pointed out that a smaller norm of the Hessian matrix indicates a flatter landscape of the objective, which is strongly correlated with better generalization \citep{wu2017toward,visualoss,chen2022bootstrap}. As for $\mathbb{E}[\|(\bm{x}-\bm{p}_i)\|_2^2]$ 
representing the squared $\ell_2$ distance from $\bm{x}$ to the closest optimum $\bm{p}_i$ of each model, we empirically show that it also has a tight connection with generalization and adversarial transferability in \cref{sec:visualizelandscape} and theoretically prove this under a simple case in \cref{appedixA}.
\cref{figure:illustration} provides an illustration that the flatness of loss landscape and closeness between local optima of different models can help with adversarial transferability.
% The first term is the hessian matrix of the loss function of each model at their own optimum, which represents the flatness of the landscape, and lots of previous works have point out that the landscape of training objective strongly correlated with the generalization error~\citep{visualoss, wu2017toward, chen2022bootstrap}. The second term represent the distance between the optimum of the ensemble model and the optimum of each model. This term also strongly correlated with generalization error and adversarial transferability. We empirically shows it at section \cref{sec:visualizelandscape}, and theoretically prove it under simple case \cref{assumption} at appendix \cref{appedixA}. 

% Both term are important properties of loss landscape, both are second-order information during optimization, and both have a strong correlation with generalization. Therefore, it is very reasonable to consider these two terms are irrelevant, and minimize these two items respectively.\yichi{is this paragraph to explain the assumption that the covariance is zero? Not coherent in the context}

% \textbf{In summary, if we converge to a local minima where the landscape is flat and closed to the local minimums of each model during training, then there is a high probability that the landscape around this local optimum is also flat during testing, and the local minimums of each testing model is also closed to this minima. In this case, the testing loss will be small, therefore we will have an small generalization error.}



% \yinpeng{This is the key! The new objective in (3) is also estimated by few models, why it generalize better than ERM? Try to provide some intuition or explain it with previous works, if our theory does not fit to the main paper.}


Based on the analysis above, the optimization of an adversarial example turns into looking for a point near the optima of victim models (\ie, minimizing the original objective), while pursuing that the landscape at the point is flat and the distance from it to each optimum is close.
The latter two targets are our main findings and we here define the concept of \textbf{common weakness} with these two terms as a local point $\bm{x}$ who has a small value of $\mathbb{E}[\|\bm{H}_i\|_F]$ and $\mathbb{E}[\|(\bm{x}-\bm{p}_i)\|_2^2]$. Note that there is no clear boundary between common weakness and non-common weakness -- the smaller these two terms are, the more likely $\bm{x}$ to be the common weakness. The ultimate goal is to find an example that has the properties of common weakness. As~\cref{theorem:upperbound} indicates, we can achieve this by optimizing these two terms separately.



% All in all, the transfer attack performance of adversarial examples is related to three factors: the loss value, the sharpness of the landscape, and the distance from the minimum point of each model to the convergence point. Inspired by this insight, we define common weakness as local optimum $c$ who has a small value of $E_{f_i \in \mathcal{F}}[\|\bm{H}_i\|_F]$ and $E_{\bm{p}_i \in O_{x}}[\|(c-\bm{p}_i)\|_2^2]$. At training time, we use the common weakness of training model to estimate the common weakness of all the model. Note that there is no clear boundary between common weakness and non-common weakness. The smaller $E_{f_i \in \mathcal{F}}[\|\bm{H}_i\|_F]$ and $E_{\bm{p}_i \in O_{x}}[\|(c-\bm{p}_i)\|_2^2]$ is, the more common weakness $c$ is. 





\subsection{Sharpness Aware Minimization}

To cope with the flatness of loss landscape, we minimize $\|\bm{H}_i\|_F$ for each model in the ensemble. However, this requires third-order derivative \wrt $\bm{x}$, which is computationally expensive. There are some research to ease the sharpness of loss landscape in model training \citep{SAM, swa,kwon2021asam}. The Sharpness Aware Minimization (SAM) \citep{SAM} is an effective algorithm to acquire a flatter landscape, which is formulated as a min-max optimization problem. The inner maximization aims to find a direction along which the loss changes more rapidly; while the outer problem minimizes the loss at this direction to improve the flatness of loss landscape. 

In the context of generating adversarial examples restricted by the $\ell_\infty$ norm as in \cref{eq:formulation}, we aim to optimize the flatness of landscape in the space of $\ell_\infty$ norm to boost the transferability, which is different from the original SAM in the space of $\ell_2$ norm. Therefore, we derive a modified SAM algorithm suitable for the $\ell_\infty$ norm (in \cref{appendixB}).
As shown in \cref{fig:samillustration}, at the $t$-th iteration of adversarial attacks, the SAM algorithm first performs an inner gradient ascent step at the current adversarial example $\bm{x}_t$ with a step size $r$ as
\begin{equation}\label{eq:4}\small
\bm{x}_{t}^{r}=\mathrm{clip}_{\bm{x}_{nat},\epsilon}\left(\bm{x}_{t}+r\cdot \mathrm{sign}\Big(\nabla_{\bm{x}} L\Big(\frac{1}{n}\sum_{i=1}^{n} f_i(\bm{x}_{t}),y\Big)\Big)\right),
\end{equation}
%where $\mathrm{clip}_{\bm{x}_{nat},\epsilon}(x)$ would project a point $\bm{x}$ to the $\ell_\infty$ ball around $\bm{x}_{nat}$ with radius $\epsilon$. 
and then performs an outer gradient descent step at $\bm{x}_t^r$ with a step size $\alpha$ as
\begin{equation}\label{eq:5}\small
\bm{x}_{t}^f=\mathrm{clip}_{\bm{x}_{nat},\epsilon}\left(\bm{x}_{t}^{r}-\alpha \cdot \mathrm{sign}\Big(\nabla_{\bm{x}} L\Big(\frac{1}{n}\sum_{i=1}^{n} f_i(\bm{x}_{t}^{r}),y\Big)\Big)\right).
\end{equation}
Note that in \cref{eq:4} and \cref{eq:5}, we apply SAM on the model ensemble rather than each training model. Thus, we can not only perform parallel computation during the backward pass to improve efficiency, but also obtain better results using the logits ensemble strategy~\citep{MI}.

Moreover, we can combine the reverse step and forward step of SAM as a single update direction $\bm{x}_{t}^{f}-\bm{x}_{t}$, and integrate it into existing attack algorithms. For example, the integration of MI and SAM yields the MI-SAM algorithm with the following update (as shown in \cref{figure:samoptimizingmisam})
\begin{equation}\label{eq:6}
    \bm{m} = \mu \cdot \bm{m} + \bm{x}_{t}^{f}-\bm{x}_{t}; \;\; \bm{x}_{t+1} = \mathrm{clip}_{\bm{x}_{nat},\epsilon}(\bm{x}_t + \bm{m}),
\end{equation}
where $\bm{m}$ accumulates the gradients with a decay factor $\mu$. By iteratively repeating this procedure, the adversarial example will converge into a flatter loss landscape, leading to better transferability.
%\yichi{the explanation can be improved}.



% Instead of optimizing $E_{f_i \in F_{train}}[\|\bm{H}_i\|_F]$, which requires third order derivative with respect to $\bm{x}$, we adjust Sharpness Aware Minimization(SAM)\citep{SAM} algorithm to infinity norm so as to bootstrap our adversarial examples converge into flat optimum. The derivation of SAM algorithm in infinity norm is in Appendix \cref{appendixB}. We also combine SAM with MI-FGSM\citep{MI} to get MI-SAM algorithm. 





\begin{figure}[t]
\vspace{-8ex}
    \centering
    \subfigure[MI]{
\includegraphics[width=3.4cm]{./figures/samoptimizingmi.pdf}
\label{figure:samoptimizingmi}
}
\subfigure[SAM]{
\includegraphics[width=2.8cm]{./figures/samoptimizingsam.pdf}
\label{figure:samoptimizingsam}
}
\subfigure[MI-SAM]{
\includegraphics[width=4cm]{./figures/samoptimizingmisam.pdf}
\label{figure:samoptimizingmisam}
}
\vspace{-2ex}
\caption{Illustration of MI, SAM, and MI-SAM. The symbols are introduced in Eq. (\ref{eq:4})-(\ref{eq:6})}. %The black arrow represents the final direction of each update.}
\vspace{-4ex}
    \label{fig:samillustration}
\end{figure}

 
%A straightforward explanation is that when a model is much easier to attack than other models, the calculated gradient will be dominated by it, leading to bad result.

% Here we provide an comprehension for why ensemble by logit averaging tends to perform better than loss averaging. Suppose we are using cross entropy loss and averaging loss of training model as our training objective, and the softmax probability of two training model is denoted as $pre_1$ and $pre_2$, loss of each model is denoted as $L_1$ and $L_2$. We can derive the gradient with respect to the input as:

% \begin{equation*}
%     \begin{aligned}
%         &\frac{\partial}{\partial p} (L_1+L_2) \\
% =& \frac{\partial L_1}{\partial logit} \frac{\partial logit}{\partial p} + \frac{\partial L_2}{\partial logit} \frac{\partial logit}{\partial p} \\
% =&(pre_1-y)\frac{\partial logit}{\partial p}+(pre_2-y)\frac{\partial logit}{\partial p}
%     \end{aligned}
% \end{equation*}

% When the first model is much hard to attack than the second model, which means $\|pre_1-y\| \ll \|pre_2-y\|$, therefore $\|\frac{\partial}{\partial p} L_1\| \ll  \|\frac{\partial}{\partial p} L_2\|$, the adversarial examples will be dominated by the second model, just like adversarial patches shown in figure \cref{figure:patch}.

% The intuition of SAM algorithm is that, the first step aims to find the worst case in the range of infinity norm $\epsilon$. The second step aims to optimize the worst case. By repeating these procedure, the worst case of final optimum is much better than directly optimizing the objective function. Therefore the landscape is flatter than other algorithm. Note that the SAM algorithm does not require multiple training models, it is also effective when there is only one training model.


\subsection{Cosine Similarity Encourager}
We then develop an algorithm to make the adversarial example converge to a point which is close to the local optimum of each model. Instead of directly optimizing $\frac{1}{n}\sum_{i=1}^n\|(\bm{x}-\bm{p}_i)\|_2^2$ with the training models in the ensemble, since it is hard to compute the gradient w.r.t. $\bm{x}$, we derive an upper bound for this loss. 

\begin{theorem}
\label{theorem:objective2upperbound}
    (Proof in \cref{appendixC}) The upper bound of $\frac{1}{n}\sum_{i=1}^n\|(\bm{x}-\bm{p}_i)\|_2^2$ is proportional to the dot product similarity between the gradients of all models:
    \begin{equation}
        \begin{aligned}
        \frac{1}{n}\sum_{i=1}^n\|(\bm{x}-\bm{p}_i)\|_2^2 \leq -\frac{2M}{n}\sum_{i=1}^{n}\sum_{j=1}^{i-1}\bm{g}_i \bm{g}_j,
    \end{aligned}
    \end{equation}
    where $M=\max \|\bm{H}_i^{-1}\|_F^2$ and $\bm{g}_i=\nabla_{\bm{x}} L(f_i(\bm{x}), y)$ represents the gradient of the $i$-th model.
    %\yinpeng{is the bound tight and under what conditions?}
\end{theorem}

Based on \cref{theorem:objective2upperbound}, the minimization of the distance to the local optimum of each model turns into the maximization of the dot product between gradients of different models. To solve this problem, \citet{MAML} propose an efficient algorithm via first-order derivative approximation. We apply this algorithm to generate adversarial examples by successively performing gradient updates using each model $f_i$ sampled from the ensemble $\mathcal{F}_{t}$ with a small step size $\beta$. The update process is
\begin{equation}
\begin{aligned}
&\bm{x}_{t}^{i}=\mathrm{clip}_{\bm{x}_{nat},\epsilon}(\bm{x}_{t}^{i-1}-\beta \cdot \nabla_{\bm{x}} L(f_i(\bm{x}_{t}^{i-1}),y)),
\end{aligned}
\end{equation}
where $\bm{x}_{t}^0=\bm{x}_{t}$.
Once the update for every model is complete, we calculate the final update using a larger step size $\alpha$ as
\begin{equation}
\begin{aligned}
&\bm{x}_{t+1}=\mathrm{clip}_{\bm{x}_{nat},\epsilon}(\bm{x}_{t}+\alpha \cdot (\bm{x}_{t}^{n}-\bm{x}_{t})).
\end{aligned}
\end{equation}
Although directly applying this algorithm can achieve good results, it is incompatible with SAM due to the varying scales of gradient norm \citep{MI}. To solve this problem, we normalize the gradient at each update by their $\ell_2$ norm. We discover that the modified version actually maximizes the cosine similarity between gradients (proof in \cref{AppendixD}). Thus, we call it Cosine Similarity Encourager (CSE), which can be further combined with MI as MI-CSE. MI-CSE involves an inner momentum to accumulate the gradients of each model. We provide the pseudocode in \cref{AppendixD}.

%which is based on the assumption that $\frac{\bm{g}_i\bm{g}_i^T}{\|\bm{g}_i\|_2}$ is negligible so that $I-\frac{\bm{g}_i\bm{g}_i^T}{\|\bm{g}_i\|_2} \approx I$ is satisfied in the derivation. The pseudocode of MI-CSE is in~\cref{alg:mi-cse}. 


%However, maximizing dot product similarity between the gradient of each model tends to let our training objective converge into a sharp optimum, which leads to gradient explosion and incompatibility with MI-SAM, resulting in bad transferability. So we decide to maximize the cosine similarity instead of the dot product similarity. However, the gradient of cosine similarity between the gradient of each model requires the third order derivative of the loss function with respect to $\bm{x}$, which is unacceptable. Inspired by \citep{MAML}, we derive an algorithm to maximize the cosine similarity between gradient just using first order derivative. The derivation of Cosine Similarity Encourager algorithm can be found in Appendix \cref{AppendixD}, and the pseudocode of Cosine Similarity Encourager is algorithm \cref{algorithm3}.


\subsection{Common Weakness Attack}

Given the respective algorithms to optimize the flatness of loss landscape and the closeness between local optima of different models, we thus need to combine them as a unified Common Weakness Attack (CWA) to achieve better transferability. In consideration of the feasibility of parallel gradient backpropagation and time complexity, we substitute the second step of SAM with CSE, and the resulting algorithm is called CWA. %Specifically, the first step of SAM algorithm aims to find the worst case under the range of infinity norm $\epsilon$, and the second step aims to optimizing the worst case with CSE, which only costs one more iteration comparing with the original CSE. We call this algorithm Common Weakness Attacker(CWA), and we combine our algorithm with MI-FGSM similar with the previous two algorithms. 
We also combine CWA with MI to obtain MI-CWA, with the psuedocode shown in~\cref{alg:mi-cwa}. CWA can also be incorporated with other strong adversarial attack algorithms, including VMI \citep{vmi}, SSA \citep{long2022frequency}, \etc. The details of these algorithms are provided in~\cref{sec:generalizedCW}.

% Furthermore, we combine the Cosine Similarity Encourager with MI-SAM algorithm. We conceived three ways to fuse MI-FGSM and Cosine Similarity Encourager. The first two ways are inserting the MI-SAM algorithm into the inner loop or outer loop of Cosine Similarity Encourager, which requires much more computational time comparing with the Cosine Similarity Encourager due to non-parallelization of backpropagation for MI-SAM, although it achieve better result than both MI-SAM and Cosine Similarity Encourager.

% From our point of view, the first step of SAM algorithm aims to find the worst case under the range of infinity norm $eps$, and the second step aims to optimizing the worst case. Inspired by this insight, we substitute the second step of MI-SAM with Cosine Similarity Encourager, which can achieve best results with the lowest time complexity, only cost one more iteration comparing with the Cosine Similarity Encourager. We call this algorithm Common Weakness Attacker(CWA), and we combine our algorithm with MI-FGSM. The psudocode is algorithm~\cref{algorithm2}:

\begin{algorithm}[t] %tb
\small
\label{algorithm2}
   \caption{MI-CWA algorithm}
   \label{alg:mi-cwa}
\begin{algorithmic}[1]
   \REQUIRE
   natural image $\bm{x}_{nat}$, label $y$, perturbation budget $\epsilon$, 
   iterations $T$, loss function $L$, model ensemble $\mathcal{F}_{t}=\{f_i\}_{i=1}^n$, decay factor $\mu$, step sizes $r$, $\beta$ and $\alpha$.
   \STATE \textbf{Initialize:} $\bm{m}=0$, inner momentum $\hat{\bm{m}}=0$, $\bm{x}_0=\bm{x}_{nat}$;
   \FOR{$t=0$ {\bfseries to} $T-1$}
   %\STATE \# first step
   \STATE Calculate $\bm{g}=\nabla_{\bm{x}} L(\frac{1}{n}\sum_{i=1}^{n} f_i(\bm{x}_{t}),y)$;
   \STATE Update $\bm{x}_t$ by $\bm{x}_{t}^{0}=\mathrm{clip}_{\bm{x}_{nat},\epsilon}(\bm{x}_{t}+r\cdot \mathrm{sign}(\bm{g}))$;
   %\STATE \# second step, also Cosine Similarity Encourager
   \FOR{$i=1$ {\bfseries to} $n$}
%   \STATE pick the jth model $f_j$
   \STATE Calculate $\bm{g}=\nabla_{\bm{x}} L(f_i(\bm{x}_{t}^{i-1}),y)$;
   \STATE Update inner momentum by $\hat{\bm{m}} = \mu \cdot \hat{\bm{m}} + \frac{\bm{g}}{\|\bm{g}\|_2}$;
   \STATE Update $\bm{x}_t^i$ by $\bm{x}_{t}^{i} = \mathrm{clip}_{\bm{x}_{nat},\epsilon}(\bm{x}_{t}^{i-1}- \beta \cdot \hat{\bm{m}})$;
   \ENDFOR
   \STATE Calculate the update $\bm{g}=\bm{x}_{t}^{n}-\bm{x}_{t}$;
   \STATE Update momentum $\bm{m} = \mu \cdot \bm{m} + \bm{g}$;
   \STATE update $\bm{x}_{t+1}$ by $\bm{x}_{t+1}=\mathrm{clip}_{\bm{x}_{nat},\epsilon}(\bm{x}_{t}+\alpha \cdot \mathrm{sign}(\bm{m}))$;
   \ENDFOR
\STATE \textbf{Return:} $\bm{x}_T$.
\end{algorithmic}
\end{algorithm}
