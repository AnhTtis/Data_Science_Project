% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{amsfonts}
\usepackage[dvipsnames]{xcolor}
\usepackage{booktabs}
\usepackage{amsmath}
\include{math_cmds}
\usepackage{multibib}
\newcites{SM}{SM References}
\newcommand{\TODO}[1]{{\bf \color{red}{TODO: #1}}}
\usepackage{bbm}
\usepackage[ruled]{algorithm2e}
\newcommand{\IE}{{\it i.e.}}
\newcommand{\EG}{{\it e.g.}}
\usepackage{wrapfig}
% \usepackage{natbib}
\usepackage{graphicx}
\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
     }
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Feedback Effect in User Interaction with Intelligent Assistants: Delayed Engagement, Adaption and Drop-out}
% \title{How the Feedback Effect Shapes User Behavior with Intelligent Assistants: delayed engagement, drop-out and adaption}
% \title{User behavior shifts with Intelligent Assistants: Drop-out, adaption, and feedback effect}
\author{Zidi Xiu\inst{1}\textsuperscript{$\dagger$}, Kai-Chen Cheng\inst{1}, David Q. Sun\inst{1}\textsuperscript{$\dagger$}, Jiannan Lu\inst{1}, Hadas Kotek\inst{1}, Yuhan Zhang\inst{2}\thanks{\small{Contributions made during the internship at Apple in the summer of 2022.}}, Paul McCarthy\inst{1},  Christopher Klein\inst{1}, Stephen Pulman\inst{1}, Jason D. Williams\inst{1}% All authors must be in the same font size and format. Use \Large and \textbf to achieve this result when breaking a line
}
% \author{Zidi Xiu\textsuperscript{*}, Kai-Chen Cheng, David Q. Sun\textsuperscript{*}, Jiannan Lu, Hadas Kotek, Yuhan Zhang \TODO{separately list affiliation: dept of ling, harvard U; add footnote: Contributions made during internship at Apple in the summer of 2022.}, Paul McCarthy,  Christopher Klein, Stephen Pulman, Jason D. Williams% All authors must be in the same font size and format. Use \Large and \textbf to achieve this result when breaking a line
% }

\authorrunning{Z. Xiu, K. Cheng, D. Sun et al.}
\institute{Apple, One Apple Park Way, Cupertino, CA 95014, USA \and
Department of Linguistics, Harvard University, Cambridge, MA 02138\\
\textsuperscript{$\dagger$}\email{\{z\_xiu,dqs\}@apple.com}}
\titlerunning{The Feedback Effect with IA}
% \authorrunning{Anoymous et al.}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
  
% % % First names are abbreviated in the running head.
% % % If there are more than two authors, 'et al.' is used.
% % %
% \institute{Princeton University, Princeton NJ 08544, USA \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{z\_xiu,dqs\}@uni-heidelberg.de}}
% %
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
% With the growing popularity of Intelligent assistants (IAs), evaluating IA quality becomes an increasingly active field of research. The complexity of IA architectures and broad spectrum of supported use cases have caused many to focus on \textit{user satisfaction} when developing evaluation frameworks. This user-centered approach is an abstraction which enables us to evaluate independent of underlying architecture differences. 
% This paper identifies and quantifies a novel component in IA-user interactions, the \textit{``feedback effect''}: how the capabilities and limitations of the IA influence user behavior over time. We provide a quantitative investigation of the feedback effect in the context of a real-world intelligent assistant. First, we demonstrate that unhelpful responses from the IA cause users to delay or reduce subsequent interactions in the short term. Next, we expand the time horizon to examine behavior changes, and show that as users discover the limitations of the IA’s understanding and functional capabilities, they learn to adjust the scope and wording of their requests to increase the likelihood of receiving a helpful response from the IA. Our findings highlight the impact of the \emph{feedback effect} at both the micro and meso level. We further discuss its macro-level consequences: unsatisfactory interactions continuously reduce the likelihood and diversity of future user engagements in a feedback loop.
With the growing popularity of intelligent assistants (IAs), evaluating IA quality becomes an increasingly active field of research. 
% The complexity of IA architectures and broad spectrum of supported use cases have caused many to focus on \textit{user satisfaction} when developing evaluation frameworks. This user-centered approach is an abstraction which enables us to evaluate independent of underlying architecture differences. \ZX{Feel that the mentioning of user satisfaction is somewhat irrelevant to the following arguments}
This paper identifies and quantifies the \textit{feedback effect}, a novel component in IA-user interactions -- how the capabilities and limitations of the IA influence user behavior over time. 
% We provide a quantitative investigation of the feedback effect in the context of a real-world intelligent assistant. 
First, we demonstrate that unhelpful responses from the IA cause users to delay or reduce subsequent interactions in the short term via an observational study. 
Next, we expand the time horizon to examine behavior changes and show that as users discover the limitations of the IA’s understanding and functional capabilities, they learn to adjust the scope and wording of their requests to increase the likelihood of receiving a helpful response from the IA. Our findings highlight the impact of the feedback effect at both the micro and meso levels. We further discuss its macro-level consequences: unsatisfactory interactions continuously reduce the likelihood and diversity of future user engagements in a feedback loop.
\vspace{-1em}
% \keywords{First keyword  \and Second keyword \and Another keyword.}
\keywords{Data Mining \and Intelligent Assistant Evaluation}
\end{abstract}
%
%
\vspace{-3em}
\section{Introduction}
% helpful refenrece:
% https://arxiv.org/pdf/1908.11404.pdf
\vspace{-1em}
Originated from spoken dialog systems (SDS), intelligent assistants (IAs) had rapid growth since the 1990s \cite{glass1999challenges}, with both research prototypes and industry applications. 
% (\EG, Amazon Alexa \cite{alexa}, Apple Siri \cite{siri} and Google Assistant \cite{googlehome}, Microsoft Cortana \cite{msftcortana}). 
As their capabilities grow with recent advancements in machine learning and increased adoption of smart devices, IAs are becoming increasingly popular in daily life \cite{de2020intelligent,kepuska2018next}. Such IAs often offer a voice user interface, allowing users to fulfill everyday tasks, get answers to knowledge queries, or start casual social conversations, by simply speaking to their device \cite{lee2015natural,purington2017alexa};
%https://reader.elsevier.com/reader/sd/pii/S0957417420300191?token=6D74CBFA79CC6ABCCE59A6E12E4A8B90BE729955ED678060165A4B70353978BDF88ADE4191CF9EBF21442D0E713A3FCA&originRegion=us-east-1&originCreation=20220428065033
that is, they take human voice as input, which they process in order to provide an appropriate response \cite{santos2016intelligent}. The evolution of these hands-free human-device interaction systems brings new challenges and opportunities to the data mining community. 

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.8\linewidth]{WSDM-Pavlov/cartoon/SiriIllustration.jpg}
%     %%\vspace{-10pt}
%     \caption{Simplified work flow of an IA system and the logic of helpfulness review process.}
%     \label{fig:illustrationIA}
%     %%\vspace{-1.8em}
% \end{figure*}

IA systems often consist of several interconnected modules: Automated Speech Recognition (ASR), Natural Language Understanding (NLU), Response Generation \& Text-to-Speech (TTS), Task Execution (\EG, sending emails, setting alarms and playing songs), and Question Answering \cite{glass1999challenges,jiang2015automatic,lopatovska2019talk}.
Many of the active developments in the field are formulated as supervised learning problems where the model predicts a target from an input, e.g., a piece of text from a speech audio input (ASR), a predefined language representation from a piece of text (NLU), or a clip of audio from a string of text (TTS). Naturally, the evaluation of these models often involves comparing model predictions to some ground-truth datasets. 
%In industry settings, such datasets are often sampled from logged user interactions with the IA and labeled by human annotators \cite{wu2016inferring}.
% \TODO{add citation}

When building such an evaluation dataset from real-world usage, we inevitably introduce user behavior into the measurement. User interactions with IA are likely to be influenced by the their pre-existing perception of IA's capabilities and limitations, therefore introducing a bias in the distribution of ``chances of success'' in logged user interactions -- users are more likely to ask what they know the IA can handle. This hypothesis makes intuitive sense and has been partly suggested by an earlier study on vocabulary convergence in users learning to speak to an SDS \cite{levow2003learning}. 

In this context, we define \emph{feedback effect} as the behavior pattern changes in users of an interactive intelligent system (\EG, IA) that are attributable to their cumulative experiences with said system. Our contributions can be summarized as follows. First, we establish a causal link between IA performance and immediate subsequent user activities, and quantify its impact on users of a real-world IA. Second, we identify distinct dynamics of behavior change for a cohort of new users over a set period of time, demonstrating how users first explore the IA's capabilities before eventually adapting or quitting.
% \item Having examined the \emph{feedback effect} in detail, we analyze its impact on metrics constructed from two common evaluation methodologies, and provide generalizable recommendations to mitigate its bias.
Third, having examined the \emph{feedback effect} and its impact in detail, we
% analyze its impact on metrics constructed from two common evaluation methodologies, and 
provide generalizable recommendations to mitigate its bias in IA evaluation.


% Skewed user traffic has been a long-existing challenge in industry data science studies \cite{wang2019heavy}.
\vspace{-1em}
\section{Related Work}
\vspace{-1em}
 \textbf{IA evaluation methods and metrics.} Many studies have been devoted to addressing the challenges in IA evaluation. Objective metrics like accuracy cannot present a comprehensive view of the system \cite{gao2019neural}.
 %, especially for non-task oriented-interactions where success is not well defined \cite{venkatesh2018evaluating}. 
 Human annotation is a crucial part of the process, but it incurs a high expense and is hard to scale \cite{kiseleva2016understanding}. Apart from human evaluation, \IE, user self-reported scores or annotated scores, subjective metrics have been introduced. Jiang \cite{jiang2015automatic} designed a user satisfaction score prediction model based on user behavior patterns, ungrammatical sentence structures, and device features. Other implicit feedback from users (\EG, acoustic features) are helpful to approximate success rates \cite{komatani2007analyzing}.

\textbf{User adaptation and lexical convergence.} 
\emph{Adaptation} (or \emph{entrainment}) describes the phenomenon whereby the vocabulary and syntax used by speakers converge as they engage in a conversation over time
\cite{reitter2006computational}. Convergence can be measured by observing repetitive use of tokens in users' requests \cite{duplessis2017automatic} and high frequency words \cite{hirschberg2008high}. Adaptation happens subconsciously and leads to more successful conversations \cite{friedberg2012lexical}.
In SDS, the speakers in a dialogue are the IA and the user. When the IA actively adapts to the user in the conversation, the quality of the generated IA responses increases substantially \cite{walker2007individual,wen2015semantically}. The phenomenon of lexical adaptation of users to the IA system has been investigated as well \cite{levow2003learning,parent2010lexical}. Currently, most IAs are built upon a limited domain with restricted vocabulary size \cite{chattaraman2019should}. Users' vocabulary variability tends to decrease as they engage with the IA over time. 
This naturally limits the linguistic diversity of user queries, although out-of-domain queries can happen from time to time  \cite{glass1999challenges}. 

% Vocabulary convergence has been investigated by \cite{levow2003learning} in 2003, showing that users tend to have less and less vocabulary variability over time, and instead they adapt to the system's vocabulary \cite{parent2010lexical}.
% \cite{pitardi2021alexa} discussed how IA systems gain trust from users in social science experiments.


\vspace{-1.5em}
% \section{Results}
\section{Data Collection}
\vspace{-1em}
% \subsection{Context}
% \vspace{-0.5em}
We analyzed logged interactions (both user queries and the associated IA responses) from a real-world IA system. All data originate from users who have given consent to data collection, storage, and review. The data is associated with a \emph{random, device-generated} identifier. Therefore, when we use the term `user' in the context of the interaction data analysis, we are \emph{actually} referring to this random identifier. While the identifier is a reasonable proxy of a user, we must recognize its limitations -- in our analysis, we are unable to differentiate multiple users who share a single device to interact with the IA, nor to associate requests from a single user that were initiated on multiple devices. %We treat the aforementioned limitations as acceptable compromises given our focus on smartphone devices -- personal phones are not often shared, and individuals usually carry only one phone.%

The population of interest is US English-speaking smartphone users who interacted with the IA in 2021 and 2022. We randomly sampled interaction data from two distinct time periods \emph{before} and \emph{after} a special event in late 2021. This event entailed new software and hardware releases, potentially introducing nontrivial changes to user behavior and demographics, while simultaneously presenting unique opportunities for our particular investigation. %We expand on the details below. %

\vspace{-1.5em}
\subsection{Study 1: Pre-event Control Period}\label{sec:causalData}
\vspace{-0.5em}
To investigate the feedback effect on user engagement, we randomly sampled interaction data from a two-week period in August 2021. The choice of a relatively \emph{short} time period \emph{before} the special event helps us (i) directly control for seasonality and (ii) avoid the impact of the special event, where product releases and feature announcements usually stimulate user engagement and attract new users. (We return to a discussion of new users below.)

% \ZX{changed sampling period to study period here and moved the footnote into main text.}

We further control for software and hardware versions, before taking a random sample of approximately 14,000 users who had at least one interaction with the IA during the study period. We then randomly sampled \emph{one} interaction per user and used human-label review to determine whether the IA response was helpful. We additionally analyzed the frequency of interactions for the user in the 2 weeks prior to and 2 weeks following our causal analysis. 
 In our sample, approximately 80\% of the interactions were labeled as \emph{helpful} to the user.\footnote{\tiny{This value is not necessarily a reflection of the aggregated or expected satisfaction metric, due to the sampling method and potential bias in the subpopulation of choice.}} The results of this study are presented in Section \ref{sec:causal}.

\vspace{-1.3em}
\subsection{Study 2: Post-event New User Period}\label{sec:collapseData}
\vspace{-0.5em}

To investigate language convergence among a new user cohort, we randomly sampled data from a six-month period immediately \emph{after} the special event. With our interest in analyzing long-term behavior changes of a new user cohort, this choice of sampling period has two interesting implications. First, special events often lead to a surge in new users of the IA. Second, feature announcements at the special event may cause some existing users to perceive the updated IA as ``new" and explore it with a mindset akin to that of a new user. Given the challenges inherent to determining new user cohorts (to be further discussed in Section~\ref{sec:semanticConvergence}), these two factors are valuable as they collectively increase the share of new users, thus boosting the observability of the cohort. From this six-month period, we took a random sample of 5,000  users who used the new software version of the IA. For each user, all interactions with the IA in the full study period were used in our analysis. 
The study is described in Section \ref{sec:semanticConvergence}. 
% A discussion which provides unified conclusions across the two studies is presented in Section \ref{sec:discussion}.

\vspace{-1.5em}
\section{Feedback Effect on Engagement}\label{sec:causal}
% \subsection{Problem Setup}\label{Sec:causal-pre}
\vspace{-1.0em}

\begin{wrapfigure}{l}{0.42\linewidth}
\vspace{-1.8em}
% \begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{ cartoon/CausalDiag.pdf}
    \caption{Causal graph illustrating the observational study of IA feedback effect accounting for the existence of confounding factors. 
    % The directed arrow $W\rightarrow Y$ is the desired causal relationship, from IA performance to user's following engagement.
    }
    \label{fig:causalDiag}
    \vspace{-2em}
% \end{figure}
\end{wrapfigure}
% emphasizing on novel applications
% \ZX{borrowed some sentences from the Sec 3.1}
Intuitively, unhelpful responses from an IA may discourage users from future interactions with the IA. Our work aims to empirically shed light on the relation between IA helpfulness\footnote{\tiny{The IA helpfulness of a given user request is defined as the user's satisfaction with the IA's response to the request, as determined by human annotators. 
% We may use the terms `(IA) helpfulness' and `(user) satisfaction' interchangeably hereafter.
}} (\emph{helpful} or \emph{unhelpful} on a single interaction) and users' subsequent engagement patterns with the IA, as illustrated in Figure~\ref{fig:causalDiag}. To establish such a causal relationship from IA performance to user engagement, we adopt an observational analysis framework with IA related features.

% The overall aim of the first study is to empirically shed light on the relationship between IA performance (\EG, the treatment group) and user's engagement behavior (\EG, the outcome) after the interaction.

% The aim of standard analysis is to infer associations among variables, estimate the likelihood of past and future events, as well as update our beliefs in the presence of new evidence or new data points, using samples drawn from that population. The analysis can be generalized well if the experimental conditions remain the same. Causal analysis goes one step further than association analysis - its aim is to infer aspects of the data generating process \cite{imbens2015causal}. In the light of such aspects, one can deduce not only the likelihood of events under static conditions, but also the dynamics of events under changing conditions and heterogeneous environment. Standard parametric adjustment in regression models is often sensitive to model misspecification when groups differ greatly in observed characteristics \cite{hernan2010causal}. In most cases, association does not imply causation, we cannot substantiate causal claims from associations alone.

Given a dataset with $N$ users, we denote unit $i$ having (i) covariates $\Xmat_i \in \BR^p$, (ii) a treatment variable $Z_i \in \{0,1\}$, indicating users experienced an \emph{unhelpful} interaction with the IA or a \emph{helpful} one respectively, and (iii) what would have happened if the unit is assigned to treatment and control, denoted by $Y_i(1)$ and $Y_i(0)$ respectively, according to the potential outcomes framework \cite{holland1986statistics,rubin1974estimating}. Consequently, the causal effect for unit $i$ is defined as $\tau_i = Y_i(1) - Y_i(0)$, namely the difference between the outcomes if treated differently on the same user. However, the fundamental problem of causal inference is that only the potential outcome -- the outcome in the group the subject was assigned to -- can be observed, \IE, $Y_i = Z_iY(Z_i) + (1-Z_i)Y(1-Z_i).$ Individual level causal estimands, the contrast of values between the two potential outcomes, cannot be expressed with functions of observed data alone. Consequently, our primary focus is on population level causal effects like the Average Treatment Effect (ATE),
% defined as $\tau = \EE ( \tau_i )$.
$\tau = \EE ( \tau_i )$.
%\begin{equation}\label{eq:ATE}
%\tau = \EE[Y_i(1)-Y_i(0)]. 
%\end{equation}

% With random trials, the group assignment mechanism is known, and we can deliver trustworthy causal effects, \IE, ATE, based on randomization. 


With a randomized controlled experiment, treated assignment mechanism is known and unconfounded, therefore we can directly and  accurately estimate and infer causal effects (\EG, ATE) from the observed data. However, in real-world scenarios, delicately designed experiments can be difficult or impossible to conduct. Instead, we must rely on observational techniques.
% This is where observational study techniques get on the stage.
% An observational study can be conceptualized as a conditionally randomized experiment if the identifiability conditions holds, i) unconfoundedness \cite{rosenbaum1984association}, $\{Y(0), Y(1) \perp W|X\}$, assuming no unobserved variables that affect both the cause and the potential outcome, ii) positivity, $P(W|X,Y(0),Y(1))\in (0,1)$ \cite{rosenbaum1983central}, iii) consistency, $Y(Z_i)=Y_i$, also called Stable Unit Treatment Value Assumption (SUTVA) \cite{holland1986statistics}. With the partially observed nature, classical literatures mainly focused on sample-based adjustment strategies like matching and weighting. Matching pairs of units that are similar with respect to particular matching criteria \cite{li2019addressing}, forming basic elements of synthetic 'randomized trials'; weighting reassigns adjustable weights to each unit to create a pseudo population with better balance \cite{hainmueller2012entropy, li2018balancing,li2019addressing}. The classical methods met challenges with modern high-dimensional data \cite{hill2011bayesian}.
%\vspace{-0.8em}

\vspace{-1.5em}
\subsection{Covariates and Outcome Variables}\label{sec:confounding}
\vspace{-1em}

Observational studies are susceptible to \textit{selection bias} due to confounding factors, which affect both the treatment $\Zmat$ and the outcome $\Ymat$, as
%in the directed arrows from $X$ to both $W$ and $Y$ 
shown in Figure~\ref{fig:causalDiag}. 
% In the context of IA interaction, it is quite likely that despite any randomization efforts, sampled requests with human labeled reviews may be confounded with user/device/task attributes \cite{sano2016prediction, kiseleva2016understanding}.  
% Despite untestable, 
To address any confoundness to the best of our ability, we have collected rich sets of the following IA related features, and assuming that there are no unobserved observed confounders.
(i) Device features: The type of device used to interact with the IA system, and the operating system version, (ii) Task features: The input sentence transcribed by the ASR system, the number of tokens in the input sentence, the word error rate (WER) of the transcription, and the domain that the IA executed with a confidence score provided by the NLU model (\EG, weather, phone, etc.), (iii) User related features: Prior activity levels measured as the number of active days before the interaction, and temporal features including local day of the week and time of the day when the interaction happened.
% \begin{enumerate}
%     \item \textbf{Device features}: The type of device used to interact with the IA system, and the operating system version.
%     \item \textbf{Task features}: The input sentence transcribed by the ASR system, the number of tokens in the input sentence, the word error rate (WER) of the transcription, and the domain that the IA executed with a confidence score provided by the NLU model (\EG, weather, payment, phone, etc.).
%     \item \textbf{User related features}: Prior activity levels measured as the number of active days before the interaction, and temporal features including local day of the week and time of the day when the interaction happened.
% \end{enumerate}

To quantify user engagement after the annotated IA interaction, Section \ref{sec:time2event} focuses on time to next session (``immediate shock"), and Section \ref{sec:NactiveDays} focuses on active day counts (``aftermath"). 

\vspace{-1.5em}
\subsection{Observational Causal Methods}
\vspace{-0.8em}

\indent\textbf{Matching methods.} Matching is a non-parametric method to alleviate the effects of confounding factors in observational studies. The goal is to obtain well-matched samples from different treatment groups, hoping to replicate randomized trial settings. The popular Coarsened Exact Matching (CEM) model is based on a monotonic imbalance reducing matching method at a pre-defined granularity with no assumption on assignment mechanisms \cite{iacus2012causal}. 

\textbf{Weighting methods.} 
%you could also say that weighting methods have the advantage of 1) using all the data, and 2) are easy to integrate in optimization/learning procedures
Apart from matching covariates, weighting based methods use all of the high-dimensional data via summarizing scores, like the propensity score (PS).
%and are versatile to integrate in optimization procedures. 
PS reflects the probability of being assigned to treatment based on user's background attributes \cite{li2018balancing,rubin1974estimating}, $e(x)=P(Z_i=1|X_i=x)=\EE(\Zmat|\Xmat)$
% \vspace{-5pt}
% \begin{equation}\label{eq:psscore}
%     e(x)=P(Z_i=1|X_i=x)=\EE(\Zmat|\Xmat).
% \end{equation}
% \vspace{-5pt}
% PS reflects the probability of assigned to treatment based on user's background attributes \cite{rubin1974estimating,li2018balancing}.
Since the true PS is unknown, we adopt generalized linear regression models (GLMs) to estimate it, which are widely adopted by the scientific community.
% Rooted from this summary score, causal inference can be drawn through stratification, matching, weighting, etc. easily, comparing to the original high dimensional inputs. 

With PS estimates available, the next question is how to leverage them. Li \cite{li2018balancing} proposed a family of balancing weights which enjoys balanced weighted distributions of covariates among treatment groups. Inverse-Probability Weights (IPW) are a special case of this family, shown in Eq.\eqref{eq:IPW}. 
As the name suggests, the weight is the inverse of the probability that a unit is assigned to the observed group, and the corresponding estimand is the ATE. However, IPW is very sensitive to outliers, \IE, when PS scores approach 0 or 1. 
% To conquer this challenge, \cite{li2018balancing} introduced overlap weights (OW) which emphasizes a target population with the most covariates overlap $e(x)(1-e(x))$.
To mitigate this challenge, Overlap Weights (OW) which emphasize a target population with the most covariate overlap \cite{li2018balancing}, shown in Eq.\eqref{eq:OverlapWeight}. 
% To be more specific, below are IPW and OW:
% The overlap weights are defined in Eq.\eqref{eq:OverlapWeight}. 
% \ZX{Need to remove the tilting function part to make it concise, still working on it}
% \hspace{-1pt}

% %\vspace{-3pt}
\begin{minipage}{0.49\linewidth}
% \hspace{-10em}
\begin{equation}\label{eq:IPW}
            \begin{cases}     w^{\text{IPW}}_1(x) =\frac{1}{e(x)}\\   w^{\text{IPW}}_0(x) =\frac{1}{1-e(x)} \end{cases}
\end{equation}
\end{minipage}
\begin{minipage}{0.49\linewidth}
\begin{equation}\label{eq:OverlapWeight}
        \begin{cases}     w^{\text{OW}}_1(x) =(1-e(x))\\      w^{\text{OW}}_0(x) =e(x), \end{cases}
\end{equation}
\end{minipage}
% \vspace{5pt}
\noindent where $w_1$ corresponds to the weight assigned to the treatment group, and $w_0$ to the control group, respectively. Then the population level causal estimands of interest, the Weighted Average Treatment Effect (WATE), are derived from the balanced weights.
% Let $f(x)$ denote the covariates distribution of the population, and $f_0(x), f_1(x)$ as the control or treatment group distribution respectively. With the balancing weights and tilting function $h(x)$, the weighted distributions for different treatment groups are evened out,
% \begin{equation*}
    % $f_1(x)w_1(x)=f_0(x)w_0(x)=f(x)h(x)$.
% \end{equation*}
% The tilting function $h( )$ defines the target population and the estimands of interest, and also determines the weights accordingly.
% to define target causal estimands on user-specified target populations,
% \begin{equation}\label{eq:balancingWt}
%         \begin{cases}     w^{h}_1(x) \propto \frac{h(x)}{e(x)},& \text{for }W=  1\\      w^{h}_0(x) \propto \frac{h(x)}{1-e(x)},& \text{for }W=0. \end{cases}
% \end{equation}
% When $h(x)=1$, it is the inverse-probability weights (IPW), shown in Eq.\eqref{eq:IPW}.
% Inverse-probability weights (IPW), shown in Eq.\eqref{eq:IPW}, 
The target population varies with different weighting strategy.
The causal estimand shown in Eq. \eqref{eq:WATE} then becomes the average treatment effect for the overlap population.
\begin{equation}\label{eq:WATE}
    \hat{\tau}^w=\frac{\sum_{i=1}^Nw_1(\bf{x_i})Z_iY_i}{\sum_{i=1}^Nw_1(\bf{x_i})Z_i} - \frac{\sum_{i=1}^Nw_0(\bf{x_i})(1-Z_i)Y_i}{\sum_{i=1}^Nw_0(\bf{x_i})(1-Z_i)}
\end{equation}






%Since the true propensity score is unknown, robust estimation of the assignment mechanism 
% %\vspace{1em}

% It enjoys the following properties, i) balancing property, $W\perp X | e(X)$ and ii) unconfoundedness, generalized from the fact that $W$ is unconfounded given $X$. 
%\vspace{-0.8em}
\begin{figure}[t!]
%\vspace{-1em}
    \centering
    \includegraphics[width=.65\linewidth]{ cartoon/time2next_illustration_simplified.pdf}
    %\vspace{-1.8em}
    \caption{Illustration of the users' engagement (blue dots) after the request was annotated at time $t_i^0$ for user $i$. We observed the time-to-next-engagement for user 1 and 2, but user 3 was censored (the next engagement was not observed).}
    \label{fig:time2eventIllustration}
    \vspace{-2.2em}
\end{figure}

\vspace{-2em}

\subsection{Time to Next Engagement }\label{sec:time2event}
% \JL{Too abstract, add a toy example here}
% \ZX{do you mean by an illustration plot?}
\vspace{-0.7em}

%https://arxiv.org/pdf/2103.00605.pdf
In this section, we establish causal links between interaction quality with the IA (as implied by the annotated helpfulness) 
% \YHZ{referring to the helpfulness? if so, the term interaction quality is a little bit vague} 
and the user's time to next engagement. Specifically, our main hypothesis is that if a user has a helpful interaction with the IA, they are more likely to further engage with the IA in the future.

Unlike standard observational studies with well-defined and observable outcomes, time-to-event measures fall into the range of survival analyses, which focus on the length of time until the occurrence of a well-defined outcome \cite{miller2011survival,xiu2020variational}. 
A characteristic feature in the study of time-to-event distributions is the presence of \emph{censored} instances: events that do \emph{not} occur during the follow-up period of a subject. This can happen when the unit drops out during the study (right censoring), presenting challenges to standard statistical analysis tools. 

As illustrated in Figure~\ref{fig:time2eventIllustration}, assume for user $i$: the time-to-next engagement is $T_i$ with censoring time $C_i$, the observed outcome $\tilde{T}_i=T_i\wedge C_i$, and the censoring indicator $\Delta_i=\mathbbm{1}\{T_i\le C_i\}$. Under time-to-event settings, we observe a quadruplet $\{Z_i, \Xmat_i, \tilde{T}_i, \Delta_i\}$ for each sample. 
Each user also has a set of potential outcomes, $\{T_i(1), T_i(0)\}$.
Users may use the IA system at some point in our research and be assigned a \emph{helpfulness} score, but not show up again before the data collection period ends (\EG, User $3$ illustrated in Figure~\ref{fig:time2eventIllustration}). This yields a censored time $C_3$ instead of a definite time-to-next-engagement outcome $T_3$ which is not observed within the study period.


Following Zeng \cite{zeng2021propensity}, the causal estimand of interest is defined based on a function of the potential survival times, $\nu(T_i(z);t)=\mathbbm{1}\{T_i(z)\ge t\}$. It can be interpreted as an at-risk function with the potential outcome $T_i(z)$. The expectation of the risk function corresponding to the potential survival function of user $i$, \IE, the probability of no interaction with the IA until time $t$. Accordingly, the \emph{re-engagement} probability for users in treatment group $z$ within time $t$ is therefore defined as Eq.\eqref{eq:SPCE}.
% is shown in \eqref{eq:SPCE}.
% \begin{equation}\label{eq:SPCE}
%     \EE[\nu(T_i(z);t)]=\PP[T_i(z)\ge t]=\BS_i(t;z).
% \end{equation}
 % \begin{equation}\label{eq:prob-engage}
 %    \PP_{\text{re-engagement}}(t;z)=1-\BS(t;z)
 % \end{equation}

 \begin{minipage}{0.55\linewidth}
\vspace{-10pt}
\begin{equation}\label{eq:SPCE}
    \EE[\nu(T_i(z);t)]=\PP[T_i(z)\ge t]=\BS_i(t;z)
\end{equation}
\end{minipage}
\begin{minipage}{0.40\linewidth}
 \begin{equation}\label{eq:prob-engage}
    \PP(t;z)=1-\BS(t;z)
 \end{equation}
\end{minipage}

To properly apply balancing weights \eqref{eq:OverlapWeight} with survival outcomes, right censoring needs to be accounted for. Pseudo-observation is therefore constructed based on re-sampling (a jack-knife statistic) and is interpreted as the individual contribution to the target estimate from a complete sample without censoring \cite{andersen2017causal}. Given a time $t$, denote the expectation of the risk function at that time point , \IE, $\EE[\nu(T_i(z);t)]$ in Eq.\eqref{eq:SPCE}, as $\theta(t)$,
% $\theta(t;z)= \EE[\nu(T_i(z);t)]=\EE[\BS(t; z_i)]=\BS(t;z)$, 
% $= \EE[\nu(T_i(z);t)]$,
which is a population parameter. Without loss of generality, we discuss the pseudo observation omitting the potential outcome notations.
% The population \emph{re-engagement} probability for group $z$ is $1-\theta(t;z)$.  
The pseudo-observation for each unit $i$ can be specified as,
% \vspace{-2em}
% \begin{equation}\label{eq:pseudo}
    $\hat{\theta}_i(t) = N\hat{\theta}(t) - (N-1)\hat{\theta}_{-i}(t)$,
% \end{equation}
% \vspace{-0.3em}
where $\hat{\theta}(t)$ is the Kaplan-Meier estimator of the population risk at time $t$, which is based on $\Delta_i$ and $T_i$. $\hat{\theta}_{-i}(t)$ is calculated without unit $i$.
In this way, classic propensity score methods become applicable. 
% starting from Eq.\eqref{eq:OverlapWeight} and Eq.\eqref{eq:pseudo}, 
Then the conditional causal effect averaged over a target population at time $t$ is:
\begin{align}
\hat{\tau}^w(t)&=\frac{\sum_{i=1}^Nw_1(\bf{x_i})Z_i \hat{\theta}_i(t)}{\sum_{i=1}^Nw_1(\bf{x_i})Z_i} - \frac{\sum_{i=1}^Nw_0(\bf{x_i})(1-Z_i) \hat{\theta}_i(t)}{\sum_{i=1}^Nw_0(\bf{x_i})(1-Z_i)} \nonumber\\ 
    &=(1-\hat{\BS}^{w_0}(t;0)) - (1-\hat{\BS}^{w_1}(t;1))= \hat{\PP}^{w_0}(t;0) - \hat{\PP}^{w_1}(t;1) \label{eq:pseudo-survival}
\end{align}
% \begin{equation}\label{eq:pseudo-survival}
% \begin{split}
%     \hat{\tau}(t)&=\frac{\sum_{i=1}^Nw_1(\bf{x_i})Z_i \hat{\theta}_i(t;Z_i)}{\sum_{i=1}^Nw_1(\bf{x_i})Z_i} - \frac{\sum_{i=1}^Nw_0(\bf{x_i})(1-Z_i) \hat{\theta}_i(t;Z_i)}{\sum_{i=1}^Nw_0(\bf{x_i})(1-Z_i)}\\
%         &= \hat{\BS}^{w_1}(t;1) - \hat{\BS}^{w_0}(t;0)\\
%         &= (1-\hat{\BS}^{w_0}(t;0)) - (1-\hat{\BS}^{w_1}(t;1))\\
%          &= \hat{\PP}^{w_0}_\text{re-engagement}(t;0) - \hat{\PP}^{w_1}_\text{re-engagement}(t;1)
% \end{split}
% \end{equation}

% \begin{align}
%     \hat{\tau}^w(t)&=\frac{\sum_{i=1}^Nw_1(\bf{x_i})Z_i \hat{\theta}_i(t)}{\sum_{i=1}^Nw_1(\bf{x_i})Z_i} - \frac{\sum_{i=1}^Nw_0(\bf{x_i})(1-Z_i) \hat{\theta}_i(t)}{\sum_{i=1}^Nw_0(\bf{x_i})(1-Z_i)} \nonumber \\
%         % &= \hat{\BS}^{w_1}(t;1) - \hat{\BS}^{w_0}(t;0)  = (1-\hat{\BS}^{w_0}(t;0)) - (1-\hat{\BS}^{w_1}(t;1)) \nonumber \\
%          &= \hat{\PP}^{w_0}(t;0) - \hat{\PP}^{w_1}(t;1) \label{eq:pseudo-survival}
% \end{align}

%  %\vspace{-0.75em}
% \begin{algorithm}[t]
% \SetAlgoLined
% {\bf Input:} Confounding variables $\Xmat$, IA helpfulness indicator $\Zmat$, observed time-to-next-engagement or censored time $\tilde{T}$ with censoring indicator $\Delta$.\\
% \For{$t \in (0, t_{max}]$}{
% {\bf Step 1:} Obtain pseudo-observations $\hat{\theta}_i(t)$  Eq.\eqref{eq:pseudo}\\
% {\bf Step 2:} Estimate propensity score $\hat{e}(x)$ Eq.\eqref{eq:psscore}\\
% {\bf Step 3:} Calculate balancing weights $w_1(x), w_0(x)$ Eq.\eqref{eq:OverlapWeight}\\
% {\bf Step 4:} Derive the causal estimands of re-engagement probability difference (RPCE) $\hat{\tau}^w(t)$ Eq.\eqref{eq:pseudo-survival}\\
% }
%  \caption{Causal discoveries of IA helpfulness on users time-to-next-engagement.}
%  \label{algo:causalSurvival}
% \end{algorithm}
%  %\vspace{-0.75em}

The estimator in Eq.\eqref{eq:pseudo-survival} represents the survival probability causal effect, \IE, the difference of the weighted \emph{re-engagement} probability in the \emph{Unhelpful} group and the \emph{Helpful} group, or the \emph{Re-engagement} Probability Causal Effect (RPCE). 
% The analysis framework is presented in Algorithm~\ref{algo:causalSurvival} and 
The results are shown in Figure~\ref{fig:time2next}. 
The confidence interval is calculated based on the estimated standard error of SPCE \cite{zeng2021propensity}.

The difference in estimated re-engagement probability is negative within the 336 hours (two weeks) following the initial interaction, with a maximum causal difference of $3.2\%$ at around 24 hours ($p$-value is $0.007$). The time window where the difference between the \emph{Helpful} and \emph{Unhelpful} groups is consistently statistically significant is between hours 8-65. The IPW result yields similar conclusions.
Our main takeaway is that the inhibition effect of an unhelpful interaction reaches peak around 24 hours after the interaction and then gradually weakening. 

Specifically, we conclude the following,
% \begin{itemize}
(i) An unhelpful interaction tends to have a stronger effect on whether the user wants to use the assistant again around the same hour on the next few days, perhaps affecting daily tasks like starting navigation to work,
(ii) About one week later, the re-engagement probability difference becomes insignificant, as users’ recollections of the unhelpful interaction fade away.
% \end{itemize}

% \subsubsection{Time-to-next interaction}
% We observed statistical significantly difference between the low-satisfaction group and the high-satisfaction group in both engagement metrics with multiple observational techniques. 
\begin{figure}[t]
\vspace{-1.5em}
    \centering
    \includegraphics[width=.7\linewidth]{causalResults/OWtime2next.png}
    \vspace{-1.5em}
    \caption{The re-engagement probability causal estimands (RPCE) as a function of time after the annotated interaction, with associated 95\% confidence interval (shaded gray). 
    % The dotted horizontal line represents the difference is 0.
    % The average re-engagement probability in \emph{Unhelpful} cohort is lower than \emph{Helpful} cohort in the following 336 hours (2 weeks) period. The significant gap happens around the $8 \sim 65$ hours with p-value $<0.05$.
    }
    \label{fig:time2next}
    \vspace{-2em}
\end{figure}

\vspace{-1.6em}
\subsection{Number of Active Days}\label{sec:NactiveDays}
\vspace{-0.8em}
% \end{figure}
%From Ricardo, it's about consistency rather than certainty, rephrase this part.
% \TODO{it's about consistency rather than certainty, rephrase this part}
Section \ref{sec:time2event} established the immediate effect of IA helpfulness on time-to-next engagement. In this section, we widen the analysis window and focus on the number of active days after the annotated interaction. Let $A^{(k)}$ denote the number of active days within $k$-day window, $k\in\{3,14\}$. The average treatment
effect (ATE) is defined as $\EE[A_i^{(k)}(1)-A_i^{(k)}(0)]$.
% With propensity score weighting framework, according to Eq.\eqref\label{eq:WATE}, the WATE is defined as
% \begin{equation}
%     \hat{\tau}^h=\hat{\mu}^h_1-\hat{\mu}^h_0=\frac{\sum_{i=1}^Nw_1(\bf{x_i})Z_iA_i^{(k)}}{\sum_{i=1}^Nw_1(\bf{x_i})Z_i} - \frac{\sum_{i=1}^Nw_0(\bf{x_i})Z_iA_i^{(k)}}{\sum_{i=1}^Nw_0(\bf{x_i})Z_i}.
% \end{equation}

To estimate the causal effects with consistency, we applied four different statistical analysis tools at the two time windows respectively, belonging to two major branches of causal analysis. The first branch is weighting (IPW, entropy weights, overlap weights).\footnote{\tiny{Propensity weighting methods: https://cran.r-project.org/web/packages/PSweight}} The corresponding WATE function is defined similarly as Eq.\eqref{eq:WATE}.
% $\hat{\tau}^{w,(k)}
% =
% \frac{\sum_{i=1}^Nw_1(\bf{x_i})Z_iA_i^{(k)}}{\sum_{i=1}^Nw_1(\bf{x_i})Z_i} 
% - 
% \frac{\sum_{i=1}^Nw_0(\bf{x_i})(1-Z_i)A_i^{(k)}}{\sum_{i=1}^Nw_0(\bf{x_i})(1-Z_i)}.$
% \begin{equation*}
%     \hat{\tau}=\hat{a}_1-\hat{a}_0=\frac{\sum_{i=1}^Nw_1(\bf{x_i})Z_iA_i^{(k)}}{\sum_{i=1}^Nw_1(\bf{x_i})Z_i} - \frac{\sum_{i=1}^N_0(\bf{x_i})Z_iA_i^{(k)}}{\sum_{i=1}^Nw_0(\bf{x_i})Z_i}.
% \end{equation*}
The second branch is matching. Considering the dimensionality, we used the CEM method \cite{iacus2012causal}.
% \footnote{\tiny{CEM: https://cran.r-project.org/web/packages/cem}}

% \begin{table}[ht]
% \begin{tabular}{@{}lll@{}}
% \toprule
% Method  & Target population & Tilting function $h(x)$                                                                                                              \\ \midrule
% IPW     & Combined          & $1$                                                                                                                   \\
% OW      & Overlapped        & $e(x)(1-e(x))$                                                                                                      \\
% Entropy & Entropy based     & \begin{tabular}[c]{@{}l@{}} $e(x)\log(e(x))$ \\ - $(1-e(x))\log(1-e(x))$\end{tabular} \\ \bottomrule
% \end{tabular}
% \caption{Target population, tilting functions comparison for the balancing weights methods}\label{tab:balance}
% %\vspace{-2.6em}
% \end{table}

\begin{wrapfigure}{l}{.6\linewidth}
% \begin
\vspace{-1em}
        \centering
    \includegraphics[width=\linewidth]{ causalResults/N_Active_Days.pdf}
    \vspace{-1.5em}
\caption{Causal effect of an unhelpful IA interaction on activity levels. Bar length indicates 95\% CI
%; significance codes: $0.001: ^{***}, 0.01: ^{**}$.
% In the four causal analysis tools, all are statistically significant for the active level within 3 days. 
% Significant codes: $0.001: ^{***}, 0.01: ^{**}$.
}
\label{fig:N-Active-Days}
\vspace{-2em}
\end{wrapfigure}
In line with our previous findings, we observe statistically significant causal impacts on the activity level 3 days after the annotated IA interaction, shown in Figure~\ref{fig:N-Active-Days} (left). All four analysis tools yield $p$-values $<0.001$. 
This also supports the finding that the inhibition effect of an unhelpful engagement fades in time.
When we zoom out to a 14-day window, we observe that though the causal effects are not always significant, the directional consistency suggests a lessened effect of the unhelpful engagement compared to the 3-day window.

% \begin{figure}[t]
%\vspace{-1.3em}

% \end{figure}

% \YHZ{I reflect on how Section 4 reveals the nature of \textit{feedback effect}. The key words are short-term and causal, but Section 4 has nothing to do with USER'S PERCEPTION on the capabilities of IA because we didn't directly collect what USERs think, instead, we had human annotators. Therefore, the part in the introduction that talks about USERs' perception needs revised.} \ZX{Good point, we are trying to make the sections coherent. Probably need to emphasize it in the introduction.}

\vspace{-1.5em}
\section{Language Convergence in New User Cohort} \label{sec:semanticConvergence}
% \section{New an Existing User Cohort Study on Semantic Collapse}

\vspace{-0.8em}
% \subsection{Background context}

% \YHZ{Is there theoretical consequence to say "semantic convergence" vs. "vocabulary convergence" as in the literature?} \ZX{not really. Early in SDS literatures they used vocabulary size shrinking to explain. we didn't find closely related literatures recently. Only in NLP models where they try to evaluate the output diversity with some metrics.}
%After observing the inhibition effect of an \emph{unhelpful} interaction, a natural question is that when new users are exposed to the IA system, what will happen afterwards?
%Previous studies have also shown that users will adapt to the vocabulary of an IA system with time , i.e. using the words that are pre-defined in the IA system.
%To validate the statement, we designed a cohort study with our logging data on new and existing users, to track the language diversity changes in a 4 month period.

Having established the inhibition effect of an unhelpful interaction on a user's activity levels immediately following the interaction, we now expand both the scope and the time horizon of our analysis, to explore how prior engagements in turn shape users' linguistic choices over time. 
% Partly inspired by prior studies \cite{levow2003learning} which suggest that novice users adapt to the limited vocabulary of an IA, we designed a cohort study based on real-world IA usage data and observe a comparable phenomenon in this new setting. 
%First, we confirm the strong correlation between the request language complexity and the quality of the IA response \cite{adiwardana2020towards}. Second, we suggest the use perplexity score to measure lexical, syntactic, and semantic diversity for an IA system. Third, we adopt the metric in a cohort study to explore adaptation and dropout mechanisms among new and existing users.

\vspace{-1.5em}
\subsection{New and Existing User Cohort Definition}
\vspace{-0.8em}
% \ZX{Expanded it to a subsection}
Canonically, a \emph{new user} to an IA is an individual who started using the IA for the first time in the observation window. As our data does not allow us to identify new users in this way, we
% \JL{sounds weak, may get challenged} 
rely instead on the following conservative, \emph{necessary but insufficient}, condition for cohort determination: a user is assigned to the `new user cohort' if they (i) had at least one interaction with the IA in the study period, and (ii) had no interaction with the IA in the first 60 days of the study period. By erring on the side of including existing users in the new user group, we can
%add extra noise associated with this imperfect classification method (existing users) to the group of interest (new users), 
 ensure that any patterns that remain are robust. Therefore, we argue that this determination method offers a reasonable (and likely inflated) approximation of the true new user cohort. In our dataset containing 6 months of interaction data, approximately 17\% of all unique users were assigned to the new user cohort. 

%Apple paper: \cite{muralidharan2019leveraging}
% \HK{Erring on the side of including existing users in the new group based on the conservative threshold, any patterns that remain are still robust, since we have introduced extra noises (existing users) into the group of interest (new users).}

% In this section, we study the cohort level user behavior difference between new and the existing user groups. The hypothesis is that the new users are generally less familiar with the capability of the IA system and could be more exploratory so that on average they could have a slightly lower success rate compared to the existing users \cite{levow2003learning}. We expect to see a decline in language complexity or topic diversity over time in the new user group. 

%\vspace{-1em}
% \DQS{need edit, questionable claims and relevance} 
% \ZX{reorganizing the structure, listed in the quip doc}

\begin{table}[t]
    \centering
    \resizebox{.99\columnwidth}{!}{
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Low Perplexity}& \textbf{High Perplexity: syntactically complex sentences} & \textbf{High Perplexity: lexically diverse and rare topics}\\ \midrule
What is the \textbf{weather}, 3.3 & Show me hourly \textbf{weather} forecast, 17.1 & What is the \textbf{UV index}, 11.8  \\
 & Could I have the \textbf{weather} for rest of the week in \textless{}Location\textgreater please, 20.8 & Is there \textbf{tornado} nearby, 13.6\\
What is the \textbf{temperature}, 3.5 & When is the \textbf{rain} supposed to start again, 19.5 & How fast is the \textbf{wind} going, 15.6 \\
Will it \textbf{rain} today, 5.9 & When the \textbf{rain} going to stop, 21.8 & When is the full \textbf{moon}, 15.7 \\
Is it going to \textbf{snow} today, 7.6 & How many inches of \textbf{snow} are we supposed to get, 20.4 & What is the \textbf{barometric pressure} at \textless{}Location\textgreater{}, 27.1 \\
& How tall will the \textbf{snow} get tonight, 27.6 & \\ \bottomrule
\end{tabular}}
 \caption{Examples of low and high perplexity requests about weather.}\label{tab:PP_example}
\vspace{-3em}
\end{table}

% \YHZ{Maybe include which test was used? I can also add the percentage.}

%
% \begin{wraptable}{l}{.45\columnwidth}
% \begin{table}[t]
% %\vspace{-8pt}
% \resizebox{.5\columnwidth}{!}{
% \begin{tabular}{ccc|c}
% \hline
%           & High Perplexity & Low Perplexity & Total \\ \hline
% Helpful   & 1245            & 11592          & 12837 \\
% Unhelpful & 308             & 839            & 1147  \\ \hline
% Total     & 1553            & 12431          & 13984 \\ \hline
% \end{tabular}
% }
%     \caption{Contingency table of IA Quality (helpful vs.\ unhelpful) and PP score, with a statistically significant correlation. 
%     % The association is statistically significant: IA responses to High Perplexity requests are more likely to be unhelpful.
%     }
%     \label{tab:helpfulness-preplexity}
%     \vspace{-2.5em}
% \end{table}
% % \end{wraptable}
%

% The overall complexity 
% % \JL{an aggregated score of some sort?}
% implies the difficulty of the tasks to be completed and therefore closely related to the success of an IA interaction. Also, the input text complexity is associated with usage frequency, \EG, simple tasks like `Setting an alarm at 8' are more frequently used, but complex ones like `Turn off the garage light when the sun is down' are less common. 
% 
% \subsection{Semantic Divergence of New Users}
%\vspace{-0.75em}
% \subsection{The Adaptation and Dropout for the New Users and the Self-Selection Bias}
%\subsection{Self-Selection Bias in New Users}
\vspace{-1.5em}
\subsection{New user's self-Selection: Drop-out or adaption}
\vspace{-0.8em}
% \textbf{Quantification of language complexity and diversity.} 
We use a domain-specific language model-based perplexity (PP) score \cite{lee2021towards}, which provides a comprehensive summary of the request's complexity characteristics \cite{adiwardana2020towards}.
% Perplexity is a common metric for evaluating language model (LM) performance \cite{lee2021towards}. 
PP score is defined as the inverse joint probability that a sentence belongs to the trained language model normalized by the number of tokens in the sentence \cite{jurafsky2000speech}, $PP(W) = \sqrt[N]{\frac{1}{\PP(w_1w_2...w_N)}}$,
% \begin{equation}\label{eq:pp}
%   PP(W) = \sqrt[N]{\frac{1}{P(w_1w_2...w_N)}} = \sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i|w_1...w_{i-1}))}},
% \end{equation}
% \begin{equation}\label{eq:pp}
%   PP(W) = \sqrt[N]{\frac{1}{\PP(w_1w_2...w_N)}}
% \end{equation}
% \YHZ{Additional bracket here. Also could the probability mass be represented as $P(w_i|w_1 ... w_{i-1})P(w_1)$ and $i$ starts from $2$?}
where $W$ is the target sentence, $w_k$ is individual token and $N$ is the token count of the sentence. In our analysis, we adopted a tri-gram language model \cite{bird2009natural}.
% Note that the perplexity score is normalize by the number of tokens (sentence length).
Table \ref{tab:PP_example} presents examples of requests with perplexity scores.
% \footnote{We use paraphrased variants rather than actual user data for illustration purposes.} 
Here we use paraphrased variants rather than actual user data for illustration purposes.
Higher perplexity correlates with more complex sentence structures,
% \YHZ{more thoughts on perplexity: "high perplexity intuitively maps to high language complexity. It directly means that the request contains word tokens and n-gram structures that are less frequent in the domain, which could be because the request contains less-frequent or more extended vocabulary, is more complex in the sentence structure, and/or presents a different topic in semantics."}, 
more diverse language representations and broader topics.
% (Table~\ref{tab:PP_example} 3rd column).

% \textbf{Language complexity decreases in new users.} 
Intuitively, new users tend to explore the limits of the IA system, with broader vocabulary and diverse paraphrases of their requests. 
In this study, we track the average PP scores of new and existing user cohorts over a six month period. 
First, we empirically show that the \emph{existing user} cohort has a lower and more stable perplexity score over time compared to the \emph{new user} cohort (Figure ~\ref{fig:convergence}). This result suggests that
% requests from existing user cohorts tend to use more common phrasing and to be less semantically complex. 
requests from existing users are more likely to conform to the typical wording of requests within a certain domain.
% \YHZ{maybe "the requests from the existing user cohorts are more likely to conform with the typical wording of requests within a certain domain."}. 

Second, we discover that the perplexity score in the new user cohort is 30\% higher than in the existing user group in the first month, 
but it gradually converges to that of the existing user cohort.
This trend suggests that new users are less familiar with the IA's capabilities and are more exploratory when they are first introduced to the system. Over time, they gradually familiarize themselves with it. Eventually, they adopt similar sentence structures and other linguistic characteristics to those used by existing users when expressing similar intents. 
% The high perplexity scores also imply that new users may encounter more negative experiences when they are first introduced to the IA system.  

% \textbf{Adaptation and dropout for the new users.}
% Within the new user cohort, we propose that users can be divided into two subgroups: the \emph{retain} group and the \emph{dropout} group. 
% \begin{wrapfigure}{l}{.6\linewidth}
\begin{figure}[ht!]
\vspace{-1.8em}
    \centering
    \includegraphics[width=.58\linewidth]{collapseResults/convergence1.png}
    \vspace{-1.4em}
\caption{New user cohort has a higher average perplexity in the beginning and converges toward existing user cohort in language perplexity over time. \label{fig:convergence}
}
\vspace{-2.5em}
\end{figure}
% \end{wrapfigure}
Next, within the new user cohort, we dive deeper into two subgroups:
% based on the dropout time: the \emph{retain} group and the \emph{dropout} group. % \DQS{The definition/group assignment deserve bit more technical details and footnote for rationale}  
The \emph{retained} group consists of users who were active for more than three out of the four month follow-up period. The  \emph{dropout} group includes users who were active for no more than 30 days within the study period.
% \footnote{We omit the users with active periods in-between the two, due to the uncertainty of assignment with limited duration of observation.} 
% In the analysis, we omit the users with active periods in-between the two, due to the uncertainty of assignment with limited duration of observation.
Based on these criteria, the retained group has an average perplexity score of 7.5,
% which is 20\% higher than the PP score of baseline existing user cohort (5.8), while
% 
%\HK{how many datapoints have been omitted?}
while the dropout group has an average perplexity score of 10.6 and the difference has $p$-value $<0.001$.
% \footnote{For comparison, the baseline PP score of the existing user cohort is 5.8.}
% which is 80\% higher than the the baseline score. 
That is, users who stop using the IA within the first month tend to have substantially higher perplexity scores than users who are retained. Further, higher PP scores are closely related to higher unhelpful rates in the IA interactions. Following our findings from the previous sections, we expect unhelpful experiences to discourage users from continuing to engage with the IA.
% The high perplexity scores also imply that new users may encounter more negative experiences when they are first introduced to the IA system.  
% All of these users eventually stopped trying and left the IA system within our study period. \HK{This is true by definition!}

%  in the beginning and shows a clear converging trend to the the baseline group (existing users) in the 3rd and 4th month \HK{This was the point of the previous paragraph, it's not relevant here. You excluded all the data that would allow for a  comparison in convergence rates for the "dropped" group!}

% \HK{I don't think Figure 6(b) is fair. I would suggest omitting it and replacing it with a table that shows baseline PP rates for (a) dropout users, (b) retained users, and (c) established users. The point about convergence is made entirely by Figure 6(a) and is misleading in 6(b) since you're artificially omitting some relevant data to make the graphs look more convergent.}

In summary, we conclude that there are two plausible mechanisms that may explain the convergence of the perplexity score over time in the new user cohort:
\begin{enumerate}
    \item \textbf{Dropout}: some new users who are either unfamiliar with the supported functionality or the language of the IA system suffer negative experiences. These high-perplexity language users stop using the system after a few tries.
    \item  \textbf{Adaptation}: despite some potential negative experiences in the beginning, some new users familiarize themselves with the system and adapt to its limitations. They continue to use the system after the first few months.
\end{enumerate}

This represents a self-selection process among the users who choose to interact with the IA system: users adapt to the IA system in a way that lowers their language perplexity and consequently improves their experience, or they stop using it altogether.
% The other group of users simply walks away. 
Crucially, as users adapt their behavior to the system  over time, we expect to observe fewer and fewer requests that may lead to unhelpful interactions with the IA---as a result of the \emph{feedback effect}. Consequently, we observe a bias that introduces a significant challenge to the meaningful offline evaluation of the IA system based on naive samples of the usage traffic. %We take a closer look at the impacts of this bias on evaluation metrics below.

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth]{ collapseResults/convergence2.png}
% \caption{New User Dropout and Adaptation Mechanism. The dropout group has the highest perplexity and the retained group converges toward the existing user cohort.}
% \label{fig:convergence2}
% \end{figure}



\vspace{-1.2em}
\section{The Feedback Effect: Challenges to Meaningful Metrics}\label{sec:challenges}

\vspace{-0.3em}
\subsection{User-based vs. Usage-based evaluations}

Offline evaluation methodologies in the IA space mostly fall into two broad categories: \emph{user-based} and \emph{usage-based} approaches \cite{jiang2015automatic}. User-based approaches typically measure the overall satisfaction of a user, while usage-based approaches focus on success rates of the IA in correctly responding to a collection of requests. In this section, we discuss how the feedback effect introduces challenges to the construction of meaningful metrics for both types of approaches, informed by our findings in Sections \ref{sec:causal} and \ref{sec:semanticConvergence}.

\vspace{-0.8em}
\subsection{Implications of the Inhibition Effect}

As we established in Section \ref{sec:causal}, users who experience an unhelpful interaction with the IA are less likely to re-engage with it in the next few days. We may conclude, without loss of generality, that users who had unsatisfactory experiences in a preceding period are less likely to engage with the IA in the current period. Hence, if we are to survey \emph{active} users in a fixed time period to measure their expected satisfaction levels, unsatisfied users would have a lower probability of being surveyed than their satisfied counterparts. This ``heavy-user'' bias is ubiquitous in data mining \cite{wang2019heavy}. 

% Assume for the preceding period $T^{(k-1)}$, there were $N^{(k-1)}$ active users. Let $s$ be the share of users  who were satisfied in their experiences with the IA, hence $(1-s)$ represents the unsatisfied share. Let $p$ be the re-engagement probability for the satisfied users to re-engage in the current period (so that they may be surveyed), and $\Delta p$ be the difference in re-engagement probability for the unsatisfied users. Additionally, assume there are $N'$ active users in the current period who were not active in the $T_{i-1}$. Additionally, since the inhibition effect fades away with time, we may assume for the $N'$ active users in the current period, the difference in re-engagement probability is trivial so that we have $s  N'$ satisfied and $(1-s)  N'$ unsatisfied users in this group.

As an illustration, suppose that for the preceding period $T^{(k-1)}$ there were $N$ active users within the period, and no new users joined. Further, let $N'$ denote the number of re-engaged active users in the current period $T^{(k)}$ who were not active in $T^{(k-1)}$. Let $s$ be the share of users who were satisfied with their experiences with the IA, hence $(1-s)$ represents the unsatisfied share. Let $p$ be the probability of the satisfied users to re-engage in the current period (so that they may be surveyed), and $\Delta p$ indicates the difference in re-engagement probability for the unsatisfied users.
% satisfied users against the satisfied ones. Note that we assume $s$ is stable from the previous period $T^{(k-1)}$ to the current period $T^{(k)}$. 
% Additionally, since the inhibition effect fades away with time, we may assume for the $N'$ new active users in the current period, the difference in re-engagement probability is trivial so that we have $s  N'$ satisfied and $(1-s)  N'$ unsatisfied users in this group.
Among the re-engaged users in this period, there are $sN'$ satisfied users and $(1-s)N'$ unsatisfied users.

Accordingly, the total number of active users in the current period $T^{(k)}$ is $s p N + sN'$, and the total number of users remaining in the study is $s p N + (1-s)(p-\Delta p)N + N'$, and the estimand of user satisfaction rate in the current period $\hat{s}$ is defined accordingly. However, this is not an unbiased estimator of $s$ due to the feedback effect, shown in \eqref{eq:epsilon-s}. We further assume that the system of interest has reached long-term equilibrium s.t.\ the number of active users in adjacent time periods is nearly identical, and empirically $\Delta p$ is reasonably small s.t.\ $N'+p  N \simeq N$. With these assumptions, we propose an estimator of the measurement error as in \eqref{eq:epsilon-s-hat}.
% a survey of the active users would lead to a measured user satisfaction rate of:
% \ZX{Remove the equation \eqref{eq:s-hat} }
% \begin{equation}\label{eq:s-hat}
% \hat{s} = \frac{s  N  p + N'  s}{s  N  p + (1-s)  N  (p-\Delta p) + N'}
% \end{equation}
% \begin{equation}\label{eq:epsilon-s}
% \epsilon = \hat{s} - s = \frac{s \Delta p   (1-s)}{p-\Delta p + s  \Delta p + \frac{N'}{N}}.
% \end{equation}

\hspace*{-1.5em}
\begin{minipage}{0.59\linewidth}
\raggedleft
\begin{equation}\label{eq:epsilon-s}
\epsilon = \hat{s} - s = \frac{s \Delta p   (1-s)}{p-\Delta p + s  \Delta p + \frac{N'}{N}}    
\end{equation}
\end{minipage}
\begin{minipage}{0.42\linewidth}
\begin{equation}\label{eq:epsilon-s-hat}
\hat{\epsilon} = \frac{s\Delta p (1-s)}{1-\Delta p (1-s)}
\end{equation}
\end{minipage}
%
% \begin{equation}\label{eq:epsilon-s-hat}
% \hat{\epsilon} = \frac{s\Delta p (1-s)}{1-\Delta p (1-s)}
% \end{equation}
%

%\vspace{0.5em}
For an IA with an actual user satisfaction rate $s=60\%$, $\Delta p = 0.3$, a simple survey of active users in the current period would yield a user satisfaction rate of $68\%$. A simulation study is presented in Figure~\ref{fig:simulation}. This error would be further amplified should the feedback effect (quantified by $\Delta p$) be stronger.
% as illustrated in Figure~\ref{fig:meaningfulInhibition}.

\vspace{-0.8em}
\begin{figure}[t]
%\vspace{-0.75em}
    \centering
    \includegraphics[width=.8\linewidth]{meaningfulMetrics/simulation-nogrid.png}
    %\vspace{-3em}
    \caption{Estimated measurement error on user satisfaction rate for different $\Delta p$ based on \eqref{eq:epsilon-s-hat}}
    \label{fig:simulation}
    \vspace{-1.5em}
\end{figure}


% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{WSDM-Pavlov/meaningfulMetrics/estimator.png}
%     \caption{Estimated Measurement Error on User Satisfaction Rate based on Eq.~\eqref{eq:epsilon-s-hat}}
%     \label{fig:meaningfulInhibition}
%     %\vspace{-1em}
% \end{figure}

%\vspace{-0.7em}
\subsection{Implications of the Language Convergence}

Language convergence has an equally profound impact on usage-based evaluation: as shown in  Table ~\ref{tab:helpfulness-preplexity}, the IA is nearly 3 times more likely to return a unhelpful response to high perplexity requests -- which account for a sizable share of the exploratory usage in the new user cohort -- than to low perplexity ones.

Should the true purpose of evaluation be to understand how well the IA addresses what the users \emph{truly want}, rather than a limited set of tasks that the users have \emph{compromised on}, we should always give sufficient consideration to the exploratory usage in our evaluation datasets. For emergent IAs with fast-growing user bases, this means one should carefully analyze the exploratory usage and iterate on new functionalities so that new users are retained at a higher level of perplexity, a reasonable proxy for request diversity and activity levels. For more established IAs, one should make a proactive effort to identify new user cohorts and decide on appropriate sampling strategies that balance the new and old.

\vspace{-0.8em}
\subsection{In Search of Meaningful Metrics: Some Recommendations}

While much of our discussion thus far has been in the context of IAs, it is perhaps not too wild a conjecture that the same phenomena can be observed more broadly in any intelligent systems that entail some form of an interactive user interface (e.g. dictation software, handwriting recognition, and etc.). We therefore provide some recommendations on how to construct more meaningful metrics in the presence the feedback effect:

\begin{enumerate}
    \item \textbf{Error Estimation and Selection Bias Mitigation}: for user-based studies, one may build an estimator to correct for measurement error after validating and quantifying the impact of the feedback effect on engagement; to control for the selection bias, one may elect to sample from a more comprehensive list of users (or the true population if feasible) rather than a list of active users in some fixed period to form the cohort of analysis;
    \item \textbf{Stratified Sampling along Multiple Dimensions}: one may identify key dimensions of interest to form stratified sampling strategies with enhanced coverage, such as system-designed function areas, user frequency, linguistic representations, and perplexities.
    \item \textbf{Exploratory Usage Retention:} exploratory usage often contains a more complete set of requests that users wish to accomplish through the system, and/or a more diverse set of user request patterns (accents in speech recognition, handwriting styles); it is a highly informative to collect data points that are yet to be subjugated to the inevitable influences of the feedback effect.
\end{enumerate}


\vspace{-1.3em}
\section{Discussions}\label{sec:discussion}
\vspace{-1em}
% potentially adding JL's section here
Evaluation of IA systems is an important yet challenging problem. On the one hand, the capabilities and limitations of IAs shape user behaviors (\EG, delayed engagement, dropout, and adaption). On the other hand, these very user behavior shifts in turn influence data collection and consequently the assessment of the IAs' capabilities and limitations. To our knowledge, this two-sided problem has not been formally discussed in the literature, at least in the context of real-world IAs. To fill this gap, this paper empirically studied the ``feedback effect'' nature of IA evaluation. On the one hand, we demonstrated that unhelpful interactions with the IA led to delayed and reduced user engagements, both short-term and mid-term. On the other hand, we examined long-term user behaviors, which suggested that as users gradually learned the limitations of the IA, they either dropped out or adapted (\IE, ``gave in''), and consequently increased the likelihood of helpful interactions with the IA. 


Beside raising awareness within the data mining community, this paper aims to equip researchers and practitioners with tools for trustworthy IA evaluations. First, in cases where randomized controlled experiments are infeasible, we offered best practices on properly employing observational causal inference methods, and constructing offline metrics that take the censoring of user engagements into account. 
% Second, Hadas's point above on data collection and sampling. 
Second, to reduce the \emph{feedback loop} problem in data collection and sampling, it is important to gauge users' experience with the IA and control for confounding factors if possible. When not possible, researchers should consider stratified sampling or boosting the signals from more complex intents, or creating synthetic test data that varies in complexity, especially targeting more complex sentence structures and intent linguistic features which may be under-represented.
%
% Third, Stephen's point above on making IA more informative and transparent about its capabilities and limitations, and new user ``training.''
% Third, to encourage new users to keep using the IA after unhelpful interactions, IA can generate or paraphrase a simplified version of the requests to speed up the users' adaptation to the IA system.
Third, we have demonstrated that a key factor contributing to unsatisfactory IA experiences for new users is that the language they use is too complex in some way. We have also shown that users who fail to adapt by using simpler language often do not continue to use the IA. These insights immediately suggest growth opportunities to capitalize on.  For example, multiple existing IAs offer a set of example conversations in different domains, in order to ``train'' new users to use the IA successfully right from the get go.
%These findings raise a number of other questions, in particular, as to what more might be done to help users engage with IAs more productively. How can we convey to the user an accurate model of the capabilities and limitations of the system? Many current IAs offer a set of example conversations in different domains in order to do this, but there is usually no requirement or check that a new user has actually looked at the samples. Could there be an alternative, perhaps more interactive method for "training" new users to use the IA successfully right from the get go?

Our work implies multiple future directions, from both product and research perspectives. First, other than new user training (that might very well be skipped), what more can we do to convey the IA's capabilities and limitations, and help users engage more productively? Alternatively, how can we intervene early on and retain those ``drop-outs,'' who provide invaluable feedback to help improve our system? Second, although we collected a rich set of covariates to ensure unconfoundedness, we can further assess the robustness of the established causal links, by leveraging classic sensitivity analysis techniques \cite{liu2013introduction}. Third, while this paper focuses on off-line evaluation for IAs, it is possible to apply the proposed methodologies and recommendations in other settings (\EG, on-line experimentation) and software products (\EG, search engines). 

\vspace{-1.3em}
\subsection*{Acknowledgements}\label{sec:ack}
\vspace{-0.8em}
\small{
This work was made possible by Zak Aldeneh, Russ Webb, Barry Theobald, Patrick Miller, Julia Lin, Tony Y. Li, Leneve Gorbaty, Jessica Maria Echterhof and many others at Apple. We also thank Ricardo Henao and Shuxi Zeng at Duke University for their support and feedback.
}
% \SP{Possible contribution to concluding discussion:}

\vspace{-1.5em}
\bibliographystyle{splncs04}
\small{\bibliography{FeedbackEffect-Full}}

\appendix
\beginsupplement
\include{SupplementalMaterialforFeedbackEffect}
\end{document}
