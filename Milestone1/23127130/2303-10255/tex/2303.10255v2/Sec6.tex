
\vspace{-1.2em}
\section{The Feedback Effect: Challenges to Meaningful Metrics}\label{sec:discussion}

\vspace{-0.3em}
\subsection{User-based vs. Usage-based evaluations}

Offline evaluation methodologies in the IA space mostly fall into two broad categories: \emph{user-based} and \emph{usage-based} approaches \citep{jiang2015automatic}. User-based approaches typically measure the overall satisfaction of a user, while usage-based approaches focus on success rates of the IA in correctly responding to a collection of requests. In this section, we discuss how the feedback effect introduces challenges to the construction of meaningful metrics for both types of approaches, informed by our findings in Sections \ref{sec:causal} and \ref{sec:semanticConvergence}.

\vspace{-0.8em}
\subsection{Implications of the Inhibition Effect}

As we established in Section \ref{sec:causal}, users who experience an unhelpful interaction with the IA are less likely to re-engage with it in the next few days. We may conclude, without loss of generality, that users who had unsatisfactory experiences in a preceding period are less likely to engage with the IA in the current period. Hence, if we are to survey \emph{active} users in a fixed time period to measure their expected satisfaction levels, unsatisfied users would have a lower probability of being surveyed than their satisfied counterparts. This ``heavy-user'' bias is ubiquitous in data mining \cite{wang2019heavy}. 

% Assume for the preceding period $T^{(k-1)}$, there were $N^{(k-1)}$ active users. Let $s$ be the share of users  who were satisfied in their experiences with the IA, hence $(1-s)$ represents the unsatisfied share. Let $p$ be the re-engagement probability for the satisfied users to re-engage in the current period (so that they may be surveyed), and $\Delta p$ be the difference in re-engagement probability for the unsatisfied users. Additionally, assume there are $N'$ active users in the current period who were not active in the $T_{i-1}$. Additionally, since the inhibition effect fades away with time, we may assume for the $N'$ active users in the current period, the difference in re-engagement probability is trivial so that we have $s  N'$ satisfied and $(1-s)  N'$ unsatisfied users in this group.

As an illustration, suppose that for the preceding period $T^{(k-1)}$ there were $N$ active users within the period, and no new users joined. Further, let $N'$ denote the number of re-engaged active users in the current period $T^{(k)}$ who were not active in $T^{(k-1)}$. Let $s$ be the share of users who were satisfied with their experiences with the IA, hence $(1-s)$ represents the unsatisfied share. Let $p$ be the probability of the satisfied users to re-engage in the current period (so that they may be surveyed), and $\Delta p$ indicates the difference in re-engagement probability for the unsatisfied users.
% satisfied users against the satisfied ones. Note that we assume $s$ is stable from the previous period $T^{(k-1)}$ to the current period $T^{(k)}$. 
% Additionally, since the inhibition effect fades away with time, we may assume for the $N'$ new active users in the current period, the difference in re-engagement probability is trivial so that we have $s  N'$ satisfied and $(1-s)  N'$ unsatisfied users in this group.
Among the re-engaged users in this period, there are $sN'$ satisfied users and $(1-s)N'$ unsatisfied users.

Accordingly, the total number of active users in the current period $T^{(k)}$ is $s p N + sN'$, and the total number of users remaining in the study is $s p N + (1-s)(p-\Delta p)N + N'$, and the estimand of user satisfaction rate in the current period $\hat{s}$ is defined accordingly. However, this is not an unbiased estimator of $s$ due to the feedback effect, shown in \eqref{eq:epsilon-s}. We further assume that the system of interest has reached long-term equilibrium s.t.\ the number of active users in adjacent time periods is nearly identical, and empirically $\Delta p$ is reasonably small s.t.\ $N'+p  N \simeq N$. With these assumptions, we propose an estimator of the measurement error as in \eqref{eq:epsilon-s-hat}.
% a survey of the active users would lead to a measured user satisfaction rate of:
% \ZX{Remove the equation \eqref{eq:s-hat} }
% \begin{equation}\label{eq:s-hat}
% \hat{s} = \frac{s  N  p + N'  s}{s  N  p + (1-s)  N  (p-\Delta p) + N'}
% \end{equation}
% \begin{equation}\label{eq:epsilon-s}
% \epsilon = \hat{s} - s = \frac{s \Delta p   (1-s)}{p-\Delta p + s  \Delta p + \frac{N'}{N}}.
% \end{equation}

\hspace*{-1.5em}
\begin{minipage}{0.59\linewidth}
\raggedleft
\begin{equation}\label{eq:epsilon-s}
\epsilon = \hat{s} - s = \frac{s \Delta p   (1-s)}{p-\Delta p + s  \Delta p + \frac{N'}{N}}    
\end{equation}
\end{minipage}
\begin{minipage}{0.42\linewidth}
\begin{equation}\label{eq:epsilon-s-hat}
\hat{\epsilon} = \frac{s\Delta p (1-s)}{1-\Delta p (1-s)}
\end{equation}
\end{minipage}
%
% \begin{equation}\label{eq:epsilon-s-hat}
% \hat{\epsilon} = \frac{s\Delta p (1-s)}{1-\Delta p (1-s)}
% \end{equation}
%

%\vspace{0.5em}
For an IA with an actual user satisfaction rate $s=60\%$, $\Delta p = 0.3$, a simple survey of active users in the current period would yield a user satisfaction rate of $68\%$. A simulation study is presented in Figure~\ref{fig:simulation}. This error would be further amplified should the feedback effect (quantified by $\Delta p$) be stronger.
% as illustrated in Figure~\ref{fig:meaningfulInhibition}.

\vspace{-0.8em}
\begin{figure}[t]
%\vspace{-0.75em}
    \centering
    \includegraphics[width=.8\linewidth]{ CHI23/meaningfulMetrics/simulation-nogrid.png}
    %\vspace{-3em}
    \caption{Estimated measurement error on user satisfaction rate for different $\Delta p$ based on \eqref{eq:epsilon-s-hat}}
    \label{fig:simulation}
    \vspace{-1.5em}
\end{figure}


% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{WSDM-Pavlov/meaningfulMetrics/estimator.png}
%     \caption{Estimated Measurement Error on User Satisfaction Rate based on Eq.~\eqref{eq:epsilon-s-hat}}
%     \label{fig:meaningfulInhibition}
%     %\vspace{-1em}
% \end{figure}

%\vspace{-0.7em}
\subsection{Implications of the Language Convergence}

Language convergence has an equally profound impact on usage-based evaluation: as shown in  Table ~\ref{tab:helpfulness-preplexity}, the IA is nearly 3 times more likely to return a unhelpful response to high perplexity requests -- which account for a sizable share of the exploratory usage in the new user cohort -- than to low perplexity ones.

Should the true purpose of evaluation be to understand how well the IA addresses what the users \emph{truly want}, rather than a limited set of tasks that the users have \emph{compromised on}, we should always give sufficient consideration to the exploratory usage in our evaluation datasets. For emergent IAs with fast-growing user bases, this means one should carefully analyze the exploratory usage and iterate on new functionalities so that new users are retained at a higher level of perplexity, a reasonable proxy for request diversity and activity levels. For more established IAs, one should make a proactive effort to identify new user cohorts and decide on appropriate sampling strategies that balance the new and old.

\vspace{-0.8em}
\subsection{In Search of Meaningful Metrics: Some Recommendations}

While much of our discussion thus far has been in the context of IAs, it is perhaps not too wild a conjecture that the same phenomena can be observed more broadly in any intelligent systems that entail some form of an interactive user interface (e.g. dictation software, handwriting recognition, and etc.). We therefore provide some recommendations on how to construct more meaningful metrics in the presence the feedback effect:

\begin{enumerate}
    \item \textbf{Error Estimation and Selection Bias Mitigation}: for user-based studies, one may build an estimator to correct for measurement error after validating and quantifying the impact of the feedback effect on engagement; to control for the selection bias, one may elect to sample from a more comprehensive list of users (or the true population if feasible) rather than a list of active users in some fixed period to form the cohort of analysis;
    \item \textbf{Stratified Sampling along Multiple Dimensions}: one may identify key dimensions of interest to form stratified sampling strategies with enhanced coverage, such as system-designed function areas, user frequency, linguistic representations, and perplexities.
    \item \textbf{Exploratory Usage Retention:} exploratory usage often contains a more complete set of requests that users wish to accomplish through the system, and/or a more diverse set of user request patterns (accents in speech recognition, handwriting styles); it is a highly informative to collect data points that are yet to be subjugated to the inevitable influences of the feedback effect.
\end{enumerate}
