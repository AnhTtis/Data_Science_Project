% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/implementation.pdf}
%     \vspace{-0.2in}
%     \caption{Diffusion model for generation (the above) and augmentation (the bottom).}
%     \label{fig:implementation}
%     \vspace{-0.2in}
% \end{figure*}

\subsection{Graph Property Prediction}
% \paragraph{Graph Property Prediction}

Graph neural networks (GNNs)~\citep{kipf2017semi,xu2018powerful} are commonly used for graph property prediction in chemistry and polymer informatics tasks~\citep{otsuka2011polyinfo,hu2020open}.
However, it is hard to annotate enough labels in these domains. 
Recent work used \textit{self-supervised tasks} such as node attribute prediction and graph structure prediction~\citep{hu2019strategies,you2021graph,kim2022graph} to pre-train architecture-fixed GNNs. \citet{sun2022does} observed that the existing methods might fail to transfer knowledge from unlabeled graph data.
% for unlabeled knowledge transfer may not always succeed.
Flexible GNN architectures for downstream tasks would be desirable.

\textit{Graph data augmentation} (GDA) methods do not restrict GNN architecture choices to improve prediction accuracy~\citep{trivedianalyzing,zhao2022graph}.
They learn to create new examples that preserve the properties of original graphs~\citep{kong2022robust,han2022g,luo2022automated}.
However, they purely manipulate labeled examples and thus \textit{cannot utilize the knowledge in unlabeled graphs}.
Our \method combines the knowledge from the unlabeled dataset and the labeled task dataset. It creates label-preserved graph examples with the knowledge transferred from the unlabeled data. It allows the GNN models to have flexible architectures.

% \textit{Data augmentation on graphs} has no restriction on GNN architectures, it hardly preserves graph labels~\citep{trivedianalyzing,zhao2022graph}. 
% Recent works advanced graph data augmentation by learning to preserve properties~\citep{kong2022robust,liu2022graph,han2022g,luo2022automated}. 
% However, the augmented knowledge is not as diverse and comprehensive as that contained in unlabeled graphs.

% Our proposed \method combines the benefits of both of the above techniques. It creates label-preserved graphs with the knowledge transferred from unlabeled data and trains GNN with flexible architectures.

\vspace{-0.05in}
\subsection{Learning from Unlabeled Data}

\textit{Pre-training on self-supervised tasks} such as masked image modeling and autoregressive text generation is effective for large language and vision models~\citep{brown2020language,he2022masked}. However, the hand-crafted self-supervised tasks could hardly help models learn useful knowledge from unlabeled graphs \emph{due to the gap between these label-agnostic tasks and the downstream prediction tasks} towards drug discovery and material discovery~\citep{kim2021pubchem,sun2021mocl,kim2022graph}.
A universal self-supervised task to learn from the unlabeled graphs remains under-explored~\citep{sun2022does,trivedianalyzing}.

% For unlabeled texts and images, simple and efficient \textit{self-supervised tasks} such as image masking and text autoregressive prediction are demonstrated useful in pre-training the model parameters for large language and vision models~\citep{brown2020language,he2022masked}.
% However, for unlabeled graphs, the domain knowledge in drug discovery and material science is hard to be correctly and comprehensively covered with hand-crafted self-supervised tasks~\citep{kim2021pubchem,sun2021mocl,kim2022graph}. A universal self-supervised task to learn from unlabeled graphs remains under-explored~\citep{sun2022does,trivedianalyzing}.

% Self-supervised pre-training on unlabeled data is demonstrated in large language and vision models~\citep{brown2020language,he2022masked}. 
% However, advances from images and texts are hard to be reproduced on graphs~\citep{sun2022does,trivedianalyzing}. Because the domain knowledge in graph tasks is hard to be comprehensively covered with hand-crafted self-supervised tasks on unlabeled graphs~\citep{kim2021pubchem,sun2021mocl,kim2022graph}. 

% \textit{Semi-supervised learning} commonly assumes that unlabeled and labeled data are from the same source~\citep{sohn2020fixmatch}. In this case, the semi-supervised learner, jointly trained on labeled and unlabeled images, could improve the performance of the self-supervision~\citep{tian2020makes,chen2020big}.
% However, the graph and label spaces between unlabeled and labeled graphs are relatively large, making it not straightforward to apply the image semi-supervised learning to graphs~\citep{sun2019infograph,tian2021divide}. 
% For example, \textit{self-training} on graphs may assign material-related properties (labels) to drug-related unlabeled graphs or vice versa. And it would negatively impact the prediction performance.

\textit{Semi-supervised learning} commonly assumes that unlabeled and labeled data are from the same source~\citep{sohn2020fixmatch}.
% Its learning objective in the latent space is mutual information maximization that encourages similarity between the representations of unlabeled and labeled graphs~\citep{sun2019infograph}.
The learning objective in the latent space is usually mutual information maximization that encourages similarity between the representations of unlabeled and labeled graphs~\citep{sun2019infograph}.
However, \textit{the distributions of the unlabeled and labeled data could be very different} due to the different types of sources~\citep{hu2019strategies}, leading to negative impacts on the property prediction on the labeled graphs.
\textit{Self-training}, as a specific type of semi-supervised learning methods, selects the unlabeled graphs of confidently predictable labels and ignore the huge number of any other unlabeled graphs~\citep{huang2022uncertainty}. Therefore, it cannot fully utilize the knowledge in the unlabeled graphs. 

% \textit{Semi-supervised learning} commonly assumes that unlabeled and labeled data are from the same source~\citep{sohn2020fixmatch}. 
% However, the distribution of labeled graphs often varies from that of unlabeled graphs~\citep{hu2019strategies}. Mutual information maximization in the latent space encourages similarity between the representation from unlabeled and labeled graphs \citep{sun2019infograph}. But it hurts prediction when unlabeled graphs are structurally very different from the labeled graphs. Self-training on graphs selects suitable unlabeled graphs and filters out noisy graphs~\citep{huang2022uncertainty}. In this case, self-training is not able to effectively utilize all the knowledge in unlabeled graphs. 

In contrast, our \method employs a diffusion model to extract knowledge (as the diffusion and reverse processes) from \textit{all the unlabeled graphs}. \method represents the knowledge as task-specific labeled examples to augment the target dataset, instead of uninterpretable pre-trained model parameters. We note that self- or semi-supervised learning does not conflict with \method, and we leave their combinations for future work.

% It transfers the knowledge by creating the \textit{label-preserved} augmented examples.


% pseudo-labels from \textit{self-training} are assumed useful when the unlabeled and labeled data are from the same source~\citep{chen2020big}. It negatively impacts the model performance if we assign pseudo-labels for material-related properties to drug-related unlabeled graphs, and vice versa.
% on images and texts utilizes task-specific data, as well as unlabeled data, to improve model performance~\citep{tian2020makes}. And it also improves the self-supervision by using unlabeled data for a second time~\citep{chen2020big}. 

% We note that self-supervised tasks pre-train model parameters, while the \method creates useful data. They are not conflict and we leave their combinations for future work.


% And the negative transfer may occur when the difference between unlabeled and labeled data is relatively large~\citep{sun2019infograph,tian2021divide}.






% Remove ---
% Data augmentation and self-training, which are popular \textit{data-centric} approaches, avoid the use of manually created tasks that are not appropriate. Graph data augmentation~\citep{zhao2022graph} creates graphs with preserved labels~\citep{luo2022automated} but uses no unlabeled graphs. The augmented knowledge is also not as diverse and comprehensive as that contained in unlabeled graphs. Self-training takes advantage of labeled data to automatically assign tasks for unlabeled data (\ie pseudo-labels). However, self-training is not as efficient since it frequently requires access to unlabeled graphs for each task. Pseudo-labels are only assumed useful when the unlabeled and labeled data are from the same source~\citep{chen2020big}. For example, it negatively impacts the model performance if we assign pseudo-labels for material-related properties to drug-related unlabeled graphs, and vice versa.
% ----



% However, they may cause over-fitting because the augmented knowledge is limited by insufficient labels. 


% We focus on using data augmentation to increase the size of the dataset to improve model performance. Augmentation aims to transform the input data example to another view without changing the ground-truth label~\citep{shorten2019survey,kashefi2020quantifying,balestriero2022effects}.


% Self-training takes advantage of labeled data to automatically assign tasks for unlabeled data (\ie pseudo-labels). However, self-training is not as efficient since it frequently requires access to unlabeled graphs for each task. Pseudo-labels are only assumed useful when the unlabeled and labeled data are from the same source~\citep{chen2020big}. For example, it negatively impacts the model performance if we assign pseudo-labels for material-related properties to drug-related unlabeled graphs, and vice versa.


% Beneficial knowledge from unlabeled data is controlled to prevent negative transfer when creating task-specific examples.

% Different from the above methods transferring knowledge through model parameters, our proposed \method transfers knowledge by data. \method on graphs could

% 1. Self-supervised learning
% 2. Semi-supervised learning
% 3. ours data-centric transfer.
% we use data augmentation as the center for knowledge transfer, leveraging the unlabeled examples to increase the diversity of the 


% \paragraph{Data Augmentation}
% Data augmentation creates new examples with preserved labels but uses no unlabeled data~\citep{shorten2019survey,kashefi2020quantifying,balestriero2022effects}. Examples of heuristic data augmentation techniques include flipping, distorting, and rotating images~\citep{shorten2019survey}, using lexical substitution, inserting words, and shuffling sentences in texts~\citep{kashefi2020quantifying}, and deleting nodes and dropping edges in graphs~\citep{zhao2022graph}. While human knowledge can be used to improve data diversity and reduce over-fitting in heuristic methods, it is difficult to use a single heuristic method to preserve the different labels for different tasks~\citep{balestriero2022effects, cubuk2019autoaugment}. So, automated augmentation~\citep{cubuk2019autoaugment} learned from data to search for the best policy to combine a bunch of predefined heuristic augmentations. Generative-like augmentation~\citep{antoniou2017data,bowles2018gan,han2022g} learned generative models to create in-class examples. Other learning ideas such as \textsc{FATTEN}~\citep{liu2018feature} and \textsc{GREA}~\citep{liu2022graph} learned to split the latent space for augmentation. However, learning and augmenting from insufficient labels at the same time may limit the diversity of new examples and cause over-fitting. Our proposed \method leverages unlabeled data to avoid them.