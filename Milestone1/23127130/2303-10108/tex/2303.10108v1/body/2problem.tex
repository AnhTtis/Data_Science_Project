% \paragraph{Graph Property Prediction Tasks}

Given $K$ property prediction tasks, there are ${N^{[k]}}$ labeled graph examples for the $k$-th task. They are $\{(G_i, y_i) \mid G_i \in \mathcal{G}^{[k]}, y_i \in \mathcal{Y}^{[k]}\}_{i=1}^{N^{[k]}}$, where $\mathcal{G}^{[k]}$  is the graph space and $\mathcal{Y}^{[k]}$ is the label space of the task. 
The prediction model with parameters $\theta$ is defined as $f^{[k]}_\theta: \mathcal{G}^{[k]} \rightarrow \mathcal{Y}^{[k]}$. $f^{[k]}_\theta$ consists of a GNN and a multi-layer perceptron (MLP).
Without the loss of generality, we consider Graph Isomorphism Networks (\gin)~\citep{xu2018powerful} to encode graph structures. Given a graph $G=(\mathcal{V},\mathcal{E}) \in \mathcal{G}^{[k]}$ in the task $k$, \gin updates the representation vector of node $v \in \mathcal{V}$ at $l$-layer:
\begin{equation}\label{eq:gin update msg}
\mathbf{h}_v^{l}=\opfunc{mlp}^{l}\left(\left(1+\epsilon \right) \cdot \mathbf{h}_v^{l-1}+\sum_{u \in \mathcal{N}(v)} \mathbf{h}_u^{l-1}\right),
\end{equation}
where $\epsilon$ is a learnable scalar and $u \in \mathcal{N}(v)$ is one of node $v$'s neighbor nodes.
% denotes that $u$ is from $v$'s neighbors.
After stacking $L$ layers, the $\opfunc{readout}$ function (\eg~summation) gets the graph representation across all the nodes. The predicted label is:
\begin{align}
\label{eqn:eq:gin readout and predict}
\begin{split}
\hat{y}&=\opfunc{mlp} \left( \opfunc{readout}\left(\left\{\mathbf{h}^{L}_v \mid v \in G\right\}\right) \right).
\end{split}
\end{align}
However, $f^{[k]}_\theta$ could hardly be well trained because it is hard to collect graph labels at a large scale ($N^{[k]}$ is small).
% would perform bad predictions because the data collection is difficult and the labeled $N^{[k]}$ is small.

Fortunately, regardless of the tasks, a large number of \textbf{unlabeled graphs} are usually available from the same or similar domains.
Self-supervised learning methods~\citep{hu2019strategies} rely on hand-crafted tasks to extract \textit{knowledge} from the unlabeled examples $\{G_j \in \mathcal{G}^{[U]}, j=1,\dots,N\}$ as \textit{pre-trained model parameters} $\theta$. The uninterpretable parameters are transferred to warm up the prediction models $\{f^{[k]}_\theta\}^{K}_{k=1}$ on the $K$ downstream graph property prediction tasks. 
However, the gap and even conflict between the self-supervised tasks and the property prediction tasks lead to suboptimal performance of the prediction models. In the next section, we present the \method framework that transfers knowledge from the unlabeled graphs with a data-centric approach.

% Self-supervised learning~\citep{hu2019strategies} addresses this problem using unlabeled graphs from another graph space $\mathcal{G}^{[U]}$ to pre-train $\theta$ for all possible $f^{[k]}_\theta (\cdot)$. Given $N (N \gg N^{[k]})$ unlabeled graphs $\{G_j \in \mathcal{G}^{[U]}\}_{j=1}^{N}$, self-supervised learning relies on hand-crafted tasks to pre-train model parameters and extract useful knowledge from them. The pre-trained parameters are transferred to different downstream tasks and are expected to benefit the task predictor $f^{[k]}_\theta(\cdot)$. 

% to initialize the model parameters for any downstream predictor $f^{[k]}_\theta(\cdot)$. The knowledge is transferred to the downstream prediction model by 
% transfers knowledge to any downstream task $k$ by initializing the model parameters in $f^{[k]}_\theta(\cdot)$. 
% $\theta$. For a specific task $k$, the pre-trained parameters are used to initialize the task-specific prediction model $f^{[k]}_\theta(\cdot)$, and the knowledge in the parameters is expected to benefit the task prediction.

% Heuristic ways to implement $\phi$ (\ie randomly delete edges and nodes) are found hard to preserve the label of $G$~\citep{trivedianalyzing}, \ie $y^\prime \neq y$.
% So, many works learn to preserve labels for augmentation~\citep{luo2022automated,liu2022graph}. However, learning methods are limited by data insufficiency as well.




% [FIRST AUGMENTATION, THEN SELF-SUPERVISED LEARNING]

% [CONNECTION TO THE FIRST PARAGRAPH]
% \paragraph{Self-supervised Pre-training} Besides graphs from $K$ tasks, suppose we have $N$ unlabeled graphs ($N \gg N^{[k]}$ for any $k$): $\{G_j \in \mathcal{G}^{[U]}\}_{j=1}^{N}$ from another graph space $\mathcal{G}^{[U]}$. Researchers may use unlabeled graphs with self-supervised tasks~\citep{hu2019strategies} to initialize the parameters of the prediction model $f^{[k]}_\theta(\cdot)$ for the task $k$.

% Since the number of labeled examples $N$ is often small, researchers often pre-train $f_\theta(\cdot)$ with self-supervised tasks~\citep{hu2019strategies,you2021graph,kim2022graph} on a large size of unlabeled data and then fine-tune $\theta$ with labeled data.

% may initialize $f_\theta(\cdot)$ with additional $M$ unlabeled graphs (usually $M \gg N$) 
% \tz{elaborate more here about how the model is initiliazed by unlabeled data, would this be ssl or something else?} 
% and then fine-tune $f_\theta(\cdot)$ with $N$ labeled data. This idea is often referred to as the graph pre-training\tz{cite}.

% \paragraph{Data Augmentation on Graphs} Given the task $k$ and a graph pair $(G \in \mathcal{G}^{[k]}, y \in \mathcal{Y}^{[k]} )$, the augmentation function  $\T^{[k]}_\phi: \mathcal{G}^{[k]} \rightarrow \mathcal{G}^{[k]}$ with parameters $\phi$ outputs a new example $G^\prime$ with label $y^\prime$ such that $y^\prime = y$ to enrich the training examples. Heuristic ways to implement $\phi$ (\ie randomly delete edges and nodes) are found hard to preserve the label of $G$~\citep{trivedianalyzing}, \ie $y^\prime \neq y$. So, many works learn to preserve labels for augmentation~\citep{luo2022automated,liu2022graph}. However, learning methods are limited by data insufficiency as well. [TODO: define insufficiency.]

% \tz{mixup actually creates new labels so they don't preserve labels}

% [TWO METHOD CONNECTON. OUR NEW PROBLEM SHOULD BE HERE] Pre-training struggles to bridge the gap between self-supervised tasks and real-world tasks, while data augmentation fails to use unlabeled graphs. In the next section, we present the \method framework to learn from unlabeled graphs and transfer knowledge by data augmentation.







% Suppose there are $K$ tasks. The $k$-th task has a graph space $\mathcal{G}^{[k]}$ and a label space $\mathcal{Y}^{[k]}$. The dataset with $N^{[k]}$ training graphs in the task is: $\{(G_i, y_i) \mid G_i \in \mathcal{G}^{[k]}, y_i \in \mathcal{Y}^{[k]}\}_{i=1}^{N^{[k]}}$.



% data augmentation suffer from a trade-off between extra knowledge and label-preserving (see \cref{fig: info relationship}).
% to flexibly transfer useful knowledge from unlabeled data and preserve task-relevant labels.

% into different downstream tasks by augmenting examples.
% \tz{strong claim, should reason and cite}. 
% Graph pre-training inspires the proposed \method to address this trade-off in the next section. 
% \tz{why and how graph pretraining can help? elaborating this also helps introducing our method. the current sentence after this comment is very weird. don't say "something will be proposed" in the paper} 
% A novel strategy will be proposed to learn a universal augmentation function that could flexibly transfer useful knowledge from unlabeled data to different downstream tasks when creating augmented examples.

% Although heuristic designs of $\T_\phi(\cdot)$ may introduce prior knowledge, it is difficult to preserve labels after heuristic augmentation. Learning $\T_\phi(\cdot)$ and $f_\theta(\cdot)$ at the same time heavily relies on the labeled examples. So, the augmented examples from the learned $\T_\phi(\cdot)$ may bring limited extra knowledge and suffer from the limitations of the labeled data (\eg data insufficiency and data imbalance)~\citep{balestriero2022effects}. Graph pre-training inspires the proposed \method in the next section to address the existing problems in $\T_\phi(\cdot)$. A novel strategy is proposed to learn the universal $\T_\phi(\cdot)$ that could flexibly transfer useful knowledge from unlabeled data to different downstream tasks when creating augmented examples.

