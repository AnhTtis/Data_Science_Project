\input{figures/tables/data_stats}
\input{figures/tables/molecules.tex}

In this section, we present and analyze experimental results to demonstrate the outstanding performance of \method, the usefulness of new optimization objectives, the effect of hyperparameters and iterative process, and the interpretability of ``visible'' knowledge transfer from unlabeled graphs.

\subsection{Experimental Setup}

\paragraph{Tasks and metrics:}
As presented in~\cref{tab:dataset_stat}, experiments are conducted on 15 graph property prediction tasks in chemistry, material science, and biology, including seven molecule classification (\hiv, \toxcast, \toxt, \bbbp, \bace, \clintox, \sider), three molecule regression tasks (\lipo, \esol, \freesolv) from open graph benchmarks~\citep{hu2020open}, four polymer regression tasks (\glass, \melting, \oxygen, and thermal conductivity prediction \thermal), and also protein function prediction (\ppi)~\citep{hu2019strategies}. We use the area under the ROC curve (AUC) to evaluate classifiers and mean absolute error (MAE) for regressors. 


\paragraph{Baselines and implementation:}
Besides \gin, there are three lines of baseline methods: (1) \textit{self-supervised learing methods} including \edgepred, \attrmask, \contextpred in~\citep{hu2019strategies}, \infomax~\citep{velickovic2019deep}, \joao~\citep{you2021graph}, \graphlog~\citep{xu2021self}, and \dsla~\citep{kim2022graph}, (2) \textit{semi-supervised learning methods} including self-training with selected unlabeled graphs (\streal) and generated graphs (\stgen) and \infograph~\citep{sun2019infograph}, and (3) \textit{graph data augmentation (GDA) methods} including \flag~\citep{kong2022robust}, \grea~\citep{liu2022graph}, and \gmix~\citep{han2022g}.
For self-supervised pre-training, we follow their own settings and directly use their pre-trained models if available. For semi-supervised learning methods and \method, we use 113K QM9~\citep{ramakrishnan2014quantum} and 306K \ppi graphs~\citep{hu2019strategies} as unlabeled data sources for the tasks on molecules/polymers and proteins, respectively.
% \gmix is not implemented for regression.
For \method, we tune three major hyper-parameters: the number of perturbation steps $D \in [1,10]$, the number of negative samples $M \in [1,10]$, and top-$n$~\% labeled graphs of lowest property prediction loss selected for data augmentation.
% where $n \in [10,100]$.

\vspace{-0.05in}
\subsection{Outstanding Property Prediction Performance}

We report the mean and standard deviation of the model performance over 10 runs with randomly initialized parameters, as shown in~\cref{tab:result of molecules}. \method is the best solution on all the 15 tasks compared to strong baselines. Our observations are:
% insights from our systematic studies are as follows:

\textbf{(1) \gin is the most competitive baseline and outperforms self-supervised learning methods.} On 7 of 15 tasks, \gin outperforms all the 7 self-supervised learning methods. Because self-supervised pre-training imposes constraints on the model architecture, it undermines the true power of GNNs and under-performs the GNNs that are properly used. 

% On tasks \toxcast and \clintox, \gin outperforms all baselines that leverage unlabeled graphs. Because these baselines cannot effectively learn from unlabeled graphs when the unlabeled graphs are too different from the labeled graphs in some tasks.
% A reason is that graph self-supervised pre-training imposes constraints on the model architecture. It underestimates the true power of GNNs and under-performs GNNs that are properly used. This issue is often overlooked. 
% We observe that tuning GNN architectures and hyper-parameters may be more important in graph property tasks, which is also confirmed by~\citet{sun2022does}. 
% the phenomena of negative transfer (compared to the \gin model) are observed for all self-supervised baselines. 
% \glass, and \melting, non-pre-trained but tuned \gin outperform all self-supervised baselines.

\textbf{(2) Self-training and GDA methods perform better than \gin but cannot effectively learn from unlabeled data.} Self-training (\streal and \stgen) is often the best baseline in regression tasks.
GDA (\grea and \gmix) methods outperform self-training in most classification tasks except \sider,
because they are often designed to exploit categorical labeled data and remain under-explored for regression.
Although self-training benefits from selecting unlabeled examples in some graph regression tasks, they are \textit{negatively} affected by the unlabeled graphs in the classification tasks such as \toxcast and \clintox.
As indicated in~\cref{fig:compare gnn runs}, it is inappropriate to pseudo-label unlabeled graphs in self-training due to the huge gap between the unlabeled data and target task.

% leverage unlabeled graphs and maybe benefifical 
% because GDA methods are often proposed for classifications and remain under exploration for regression. On the other hand,  GDA (\grea and \gmix) methods outperform self-training in most classification tasks except \sider. 
% tend to be the best baselines for these tasks the use of unlabeled graphs have negative effects on molecule classifications such as \toxcast and \clintox.  the between unlabeled and labeled graphs is too large, making it inappropriate to directly assign task labels to unlabeled graphs in self-training.
% we find that self-training methods that involve learning from unlabeled graphs may have negative effects on molecule classification tasks, while augmentation methods (\grea and \gmix) tend to be the best baselines for these tasks. It confirms our analysis in Section~\ref{sec:method pretrain rationale} that the distance between unlabeled and labeled graphs is too large, making it inappropriate to directly assign task labels to unlabeled graphs in self-training.

\textbf{(3) \method transfers useful knowledge from unlabeled data by data augmentation.} 
\method outperforms the best baseline relatively by +5.1\%, +13.4\%, and +10.2\% when there are only 1,141, 513, and 4,303 training graphs on the three tasks \sider, \freesolv, and \glass, respectively.
% The transfer performance of \method is significant
Compared to the self-supervised baselines, the improvement from \method is more significant, so the knowledge transfer is more effective.
On \freesolv and \oxygen, where we have a very limited number of labeled graphs, \method performs better than the best self-supervised baselines relatively by +45.8\% and +8.0\%, respectively. On regression tasks that involve knowledge transfer across domains (\eg from molecules to polymers), \method reduces MAE relatively by 1.9\%~$\sim$~10.2\% compared to the best baseline. All these results demonstrate the outstanding performance of task-specific data augmentation in \method.

\input{figures/tables/ablation.tex}
\subsection{Ablation Studies and Performance Analysis}

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.98\linewidth]{figures/sensi_d_m.pdf} \label{fig:sensi d and m}
        \vspace{-0.1in}
    \end{subfigure}%
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=0.98\linewidth]{figures/sensi_topn.pdf}\label{fig:sensi topn}
        \vspace{-0.1in}
    \end{subfigure}%
    \caption{Effect of hyper-parameters, including the number of perturbation steps $D \in [1,10]$, the number of negative graphs $M \in [1,10]$, and top-$n$~\% labeled graphs whose labels are predicted the most accurately and that are selected for data augmentation, where $n \in [10,100]$.}
    \label{fig:sensi all}
    \vspace{-0.1in}
\end{figure*}


\textbf{Comprehensive ablation studies:}
In~\cref{tab:ablation finetune}, we investigate how the task-related objectives in~\cref{eq:fine-tune objective} impact the performance of \method.
First, \method outperforms the top baseline even if the two task-related optimization objectives are disabled.
This is because \method generates new training examples based on original labeled graphs: the data augmentation has already improved the diversity of the training dataset a little bit.
% using the diffusion model for graph data augmentation without task-related optimization has already outperformed the top baselines by increasing the diversity of the training dataset.
Second, adding the objective $\I_1$ further improves the performance by encouraging the generation of diverse examples, because it minimizes the similarity between the original graph and augmented graph in the graph space.
Third, we receive the best performance of \method when it combines $\I_1$ and $\I_2$ objectives to generate task-related and diverse augmented graphs.
When we change the unlabeled data source from QM9 to the ZINC dataset from~\citep{jo2022score}, similar observations confirm the necessity of the task-related objectives.

% for minimal sufficient knowledge transfer.
% FIGURE XX visualizes an example of the changes of the $\I_1$ and $\I_2$ losses at different steps in the diffusion-based augmentation.

% \textbf{The fine-tune objective} in diffusion-based augmentation
% Here results + \textbf{The size of unlabeled graphs} ZINC and QM9 for better use of unlabeled data?
% Here results in \cref{tab:result of ablation studies}. Also, \textbf{Visualization} of fine-tune loss.

\vspace{-0.04in}
\textbf{Effect of hyper-parameters:} The impacts of three hyper-parameters of \method are studied:
% for their impacts on augmenting useful data:
the number of perturbation steps $D$, the number of negative samples $M$ in~\cref{eq:upper bound infonce}, and the number of augmented graphs in each iteration (\ie top-$n$~\% selected graph for augmentation).
Results from~\cref{fig:sensi all} show that \method is robust to a wide range of $D$ and $M$ valued from 0 to 10. They suggest that $D$ and $M$ can be set as 5 in most cases. As for the number of the augmented graphs in each iteration, results show that noisy graphs are often created when $n$ is higher than 30\%, because the predictor cannot effectively guide the data augmentation for those labeled graphs whose labels are hard to predict. So, 10\% is suggested as the default of top-$n$\%.

% the regression predictor is more sensitive to $D$ and $M$ than the classification predictor. When we increase the size of the augmented graphs, more noise may be included. And it leads to poor prediction performance. \cref{fig:sensi all} also suggests some options for these hyper-parameters. $D=5$ and $M=5$ would not be a bad option. And the number of augmented graphs in each iteration should be close to 10\% of the training examples.

\begin{figure}[ht!]
    \centering
    \vspace{-0.1in}
    \includegraphics[width=0.8\linewidth]{figures/mutual_enhanace.pdf}
    \vspace{-0.1in}
    \caption{Data augmentation and predictor model training mutually enhance each other over epochs. The predictor is saved every 20 epochs to guide the generation of augmented graphs. The performance of \gin trained on these augmented graphs is used to reflect the quality of the augmented data.}
    \label{fig:mutual enhance}
    % \vspace{-0.2in}
\end{figure}

\begin{figure}[ht]
    \centering
    \vspace{-0.1in}
    \includegraphics[width=0.8\linewidth]{figures/example.pdf}
    \vspace{-0.1in}
    \caption{Case studies of augmented graphs. The green highlighted subgraphs are from \gin with top-k pooling. Examples show that the augmented graphs from~\method identify and preserve the core structures of original graphs. Key concepts in the unlabeled graphs like some chemical rules are transferred to downstream tasks. Domain knowledge such as the relationship between the permeability and the fluorine atom/methyl group is captured to guide the generation of task-specific labeled graphs.
    % Example (a) from~\bace shows that the augmented graphs keep core structures of the original graphs for prediction. For (b) and (c) from \oxygen, the fluorine atom and the methyl group are negatively and positively correlated to the permeability, respectively. Example (b) replaces the fluorine with the methyl group and correctly encourages \gin to predict a higher label value. Example (c) replaces the methyl group with the fluorine and correctly encourages \gin to predict a lower label value.
    }
    \label{fig:visual}
    % \vspace{-0.1in}
\end{figure}
\vspace{-0.04in}
\textbf{Iterative process:} 
% The relationship between data augmentation and graph property predictor in \method is studied using \bace and \oxygen. Results are in~\cref{fig:mutual enhance}.
\cref{fig:mutual enhance} investigates the relationship between the quality of augmented graphs and the accuracy of property prediction models.
We save a predictor checkpoint every 20 epochs to guide the generation of the augmented examples.
We evaluate the quality of augmented graphs by using them to train \gin and report AUC/MAE. The data augmentation gradually decreases the training loss of property prediction. On the other hand, the increased \gin performance indicates that the quality of augmented examples is also improved over epochs. The data augmentation and predictor training mutual enhance each other.

% The relationship between data augmentation and prediction model in \method is presented in~\cref{fig:mutual enhance}. We save a predictor checkpoint every 20 epochs to guide the generation of augmented examples. We evaluate the quality of augmented graphs by training \gin on  augmented graphs. It is shown that data augmentation leads to a gradual decrease in training loss. The increased \gin performance on the augmented data indicates that the quality of augmented examples is also improved over epochs.

\subsection{Interpretability of Visible Knowledge Transfer}

Knowledge transfer by data augmentation gives visible examples, allowing us to study what is learned. We visualize a few augmented graphs in \method using \bace and \oxygen. We adapt top-k pooling~\citep{knyazev2019understanding} to select the subgraphs that \gin used for prediction. The selected subgraphs are highlighted in green in~\cref{fig:visual}. The three examples show that \emph{the augmented graphs can identify and preserve the core structures} that \gin uses to predict property values. These augmented graphs are chemically valid, showing that \emph{concepts such as some chemical rules from the unlabeled graphs are successfully transferred to downstream tasks}. Regarding task-specific knowledge, it is known that the fluorine atom and the methyl group are usually negatively and positively correlated to the permeability, respectively~\citep{park2003gas,corrado2020macromolecular}.
The augmented examples show that \emph{\method captures this domain knowledge with the task-related objectives}. In example (b), \method replaces most of the fluorine atoms with the methyl groups. It encourages \gin to learn the positive relationship between the methyl group and the permeability so that \gin predicts a high label value. In example (c), \method replaces the methyl groups with fluorine atoms. It encourages \gin to learn the negative relationship between the fluorine atom and the permeability so that \gin predicts a low label value.

% concepts such as the construction of molecules according to chemical rules
% (1) most gas molecule is non-polar molecules. F is polar. hinder the permeability.

% (2) F has small structure space. CH3 larger.

% In the example (b), the augmented example (c) replace F (XX) with methyl group and correctly encourages \gin to predict a higher label value. The augmented example (d) replaces the methyl group with F(XX) and correctly encourages \gin to predict a lower label value. 
% \textbf{(1)} EVALUTE RATIONALE SUBGRAPH IN OXYGEN DATASET

% \textbf{(1)} EVALUTE AUGMENTED EXAMPLES USING MODELS (POST HOC OR USE GREA) TO SEE WHETHER

% As presented in FIGURE XXXX. [ANALYSIS]

% Visualization of augmentation function.

% [ANALYSIS] In every iteration, we choose the top $n$ examples with the smallest losses to augment, and we investigate how varying $n$ affects model performance. Results are in figure XXS. [ANALYSIS]

% \textbf{The effect of negative examples} in $\I_\text{bound}$ in a figure with two ranges. 

% More studies on mutual enhancement between augmentation and model training, the sensitivity analysis of the hyper-parameter $K_s$, and raw features on graphs for~\cref{eq:upper bound infonce} are in Appendix.
% \textbf{The effect of graph similarity calculation} in $\I_\text{bound}$

% \begin{enumerate}
%     \item molecule and polymer graphs from \textbf{Chemistry} and \textbf{Material Science} Domain. 7 OGBG classification. 3 OGBG regression and 3 polymer regression
%     \item protein graphs from bioinformatics domain. Protein-protein interaction Networks 306,925 unlabel graphs, 88000 label graphs from \citep{hu2019strategies}. (GDSS is only tested on Enzymes dataset with 587 examples)
%     % \item Social graphs. \citep{you2021graph} used them in semi-supervised settings (COLLAB REDD-B REDD-M5k GITHUB). \citep{kim2022graph} used them for link predictions. Generally they are from TUDataset, we could use the reddit\_threads dataset from TUDataset (203,088 examples) and use 8:1.2:0.2:0.6 splitting for pretraining, fine-finetuning, validation and test. Other downstream tasks may also include (IMDB-B IMDB-M REDD-B REDD-M5k).
% \end{enumerate}
