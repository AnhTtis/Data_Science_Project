\subsection{Graph Property Prediction}

Graph neural networks (GNNs)~\citep{kipf2017semi,xu2018powerful} are commonly used for graph property prediction in chemistry and polymer informatics tasks~\citep{otsuka2011polyinfo,hu2020open,zhou2022jointly}.
However, it is hard to annotate enough labels in these domains. 
Recent work used \textit{self-supervised tasks} such as node attribute prediction and graph structure prediction~\citep{hu2019strategies,you2021graph,kim2022graph} to pre-train architecture-fixed GNNs. \citet{sun2022does} observed that the existing methods might fail to transfer knowledge from unlabeled graph data. Flexible GNN architectures for downstream tasks would be desirable.

\textit{Graph data augmentation} (GDA) methods do not restrict GNN architecture choices to improve prediction accuracy~\citep{trivedianalyzing,zhao2022learning,zhao2022graph,ding2022data}.
They learn to create new examples that preserve the properties of original graphs~\citep{liu2022local,liu2022graph,kong2022robust,han2022g,luo2022automated}.
However, they purely manipulate labeled examples and thus \textit{cannot utilize the knowledge in unlabeled graphs}.
Our \method combines the knowledge from the unlabeled dataset and the labeled task dataset. It creates label-preserved graph examples with the knowledge transferred from the unlabeled data. It allows the GNN models to have flexible architectures.

\subsection{Learning from Unlabeled Data}

\textit{Pre-training on self-supervised tasks} such as masked image modeling and autoregressive text generation is effective for large language and vision models~\citep{brown2020language,he2022masked}. However, the hand-crafted self-supervised tasks could hardly help models learn useful knowledge from unlabeled graphs \emph{due to the gap between these label-agnostic tasks and the downstream prediction tasks} towards drug discovery and material discovery~\citep{sun2021mocl,kim2022graph,inae2023motif}.
A universal self-supervised task to learn from the unlabeled graphs remains under-explored~\citep{sun2022does,trivedianalyzing}.

\textit{Semi-supervised learning} assumes that unlabeled and labeled data are from the same source~\citep{liu2023semi}. The learning objective in the latent space is usually mutual information maximization that encourages similarity between the representations of unlabeled and labeled graphs~\citep{sun2019infograph}. However, \textit{the distributions of the unlabeled and labeled data could be very different} due to the different types of sources~\citep{hu2019strategies}, leading to negative impacts on the property prediction on the labeled graphs.
\textit{Self-training}, as a specific type of semi-supervised learning method, selects the unlabeled graphs of confidently predictable labels and assigns pseudo-labels for them~\citep{lee2013pseudo,iscen2019label}. Many studies have explored improving uncertainty estimation~\citep{gal2016dropout,tagasovska2019single,amini2020deep} to help the model filter out noise for reliable pseudo-labels. Recently, pseudo-labels have been applied in imbalanced learning~\citep{liu2023semi} and representation learning~\citep{ghiasi2021multi}. However, self-training is restricted to confidently predictable labels and may ignore the huge number of any other unlabeled graphs~\citep{huang2022uncertainty}. Therefore, it cannot fully utilize the knowledge in the unlabeled graphs. 

In contrast, our \method employs a diffusion model to extract knowledge (as the diffusion and reverse processes) from \textit{all the unlabeled graphs}. \method represents the knowledge as task-specific labeled examples to augment the target dataset, instead of uninterpretable pre-trained model parameters. We note that self- or semi-supervised learning does not conflict with \method, and we leave their combinations for future work.

\subsection{Diffusion Models on Graphs}
Recent works have improved the diffusion models on graphs~\citep{niu2020permutation,jo2022score,vignac2022digress,kong2023autoregressive,chen2023efficient}. EDP-GNN~\citep{niu2020permutation} employed score matching for permutation-invariant graph data distribution. GDSS~\citep{jo2022score} extended the continuous-time framework [6] to model node-edge joint distribution. DiGress~\citep{vignac2022digress} used the transition matrix to preserve the discrete natures of the graph structure. GraphARM~\citep{kong2023autoregressive} introduced a node-absorbing autoregressive diffusion process. EDGE~\citep{chen2023efficient} focused on efficiently generating larger graphs. Instead of improving the generation performance of the diffusion model, our model builds on score-based diffusion models~\citep{jo2022score,song2020score} for predictive tasks, \ie, graph classification and graph regression.