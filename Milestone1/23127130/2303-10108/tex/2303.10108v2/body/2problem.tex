Given $K$ property prediction tasks, there are ${N^{[k]}}$ labeled graph examples for the $k$-th task. They are $\{(G_i, y_i) \mid G_i \in \mathcal{G}^{[k]}, y_i \in \mathcal{Y}^{[k]}\}_{i=1}^{N^{[k]}}$, where $\mathcal{G}^{[k]}$  is the graph space and $\mathcal{Y}^{[k]}$ is the label space of the task. 
The prediction model with parameters $\theta$ is defined as $f^{[k]}_\theta: \mathcal{G}^{[k]} \rightarrow \mathcal{Y}^{[k]}$. $f^{[k]}_\theta$ consists of a GNN and a multi-layer perceptron (MLP).
Without the loss of generality, we consider Graph Isomorphism Networks (\gin)~\citep{xu2018powerful} to encode graph structures. Given a graph $G=(\mathcal{V},\mathcal{E}) \in \mathcal{G}^{[k]}$ in the task $k$, \gin updates the representation vector of node $v \in \mathcal{V}$ at $l$-layer:
\begin{equation}\label{eq:gin update msg}
\mathbf{h}_v^{l}=\opfunc{mlp}^{l}\left(\left(1+\epsilon \right) \cdot \mathbf{h}_v^{l-1}+\sum_{u \in \mathcal{N}(v)} \mathbf{h}_u^{l-1}\right),
\end{equation}
where $\epsilon$ is a learnable scalar and $u \in \mathcal{N}(v)$ is one of node $v$'s neighbor nodes.
After stacking $L$ layers, the $\opfunc{readout}$ function (\eg~summation) gets the graph representation across all the nodes. The predicted label is:
\begin{align}
\label{eqn:eq:gin readout and predict}
\begin{split}
\hat{y}&=\opfunc{mlp} \left( \opfunc{readout}\left(\left\{\mathbf{h}^{L}_v \mid v \in G\right\}\right) \right).
\end{split}
\end{align}
$f^{[k]}_\theta$ is hard to be well-trained because it is hard to collect graph labels at a large scale
($N^{[k]}$ is small).

Fortunately, regardless of the tasks, a large number of \textbf{unlabeled graphs} are usually available from the same or similar domains.
Self-supervised learning methods~\citep{hu2019strategies} rely on hand-crafted tasks to extract \textit{knowledge} from the unlabeled examples $\{G_j \in \mathcal{G}^{[U]}, j=1,\dots,N\}$ as \textit{pre-trained model parameters} $\theta$. The uninterpretable parameters are transferred to warm up the prediction models $\{f^{[k]}_\theta\}^{K}_{k=1}$ on the $K$ downstream graph property prediction tasks. 
However, the gap and even conflict between the self-supervised tasks and the property prediction tasks lead to suboptimal performance of the prediction models. In the next section, we present the \method framework that transfers knowledge from the unlabeled graphs with a data-centric approach.
