\input{figures/tables/data_stats}
In this section, we present and analyze experimental results to demonstrate the outstanding performance of \method, the usefulness of new optimization objectives, the effect of hyperparameters and iterative process, and the interpretability of ``visible'' knowledge transfer from unlabeled graphs.

\subsection{Experimental Setup}
\paragraph{Tasks and metrics:} 
Experiments are conducted on 15 graph property prediction tasks in chemistry, material science, and biology, including seven molecule classification, three molecule regression tasks from open graph benchmarks~\citep{hu2020open}, four polymer regression tasks, and protein function prediction (\ppi)~\citep{hu2019strategies}. Dataset statistics is presented in~\cref{tab:dataset_stat}. We use the area under the ROC curve (AUC) to evaluate classifiers and mean absolute error (MAE) for regressors. 

% \vspace{-0.1in}
\paragraph{Baselines and implementation:}
Besides \gin, there are three lines of baseline methods: (1) \textit{self-supervised learning methods} including \edgepred, \attrmask, \contextpred in~\citep{hu2019strategies}, \infomax~\citep{velickovic2019deep}, \joao~\citep{you2021graph}, \graphlog~\citep{xu2021self}, \mgssl~\cite{zhang2021motif} and \dsla~\citep{kim2022graph}, (2) \textit{semi-supervised learning methods} including self-training with selected unlabeled graphs (\streal) and generated graphs (\stgen) and \infograph~\citep{sun2019infograph}, and (3) \textit{graph data augmentation (GDA) methods} including \flag~\citep{kong2022robust}, \grea~\citep{liu2022graph}, and \gmix~\citep{han2022g}.
For self-supervised pre-training, we follow their own settings and directly use their pre-trained models if available. For semi-supervised learning methods and \method, we use 113K QM9~\citep{ramakrishnan2014quantum} and 306K \ppi graphs~\citep{hu2019strategies} as unlabeled data sources for the tasks on molecules/polymers and proteins, respectively. For \method, we tune three major hyper-parameters: the number of perturbation steps $D \in [1,10]$, the number of negative samples $M \in [1,10]$, and top-$n$~\% labeled graphs of lowest property prediction loss selected for data augmentation.

\input{figures/tables/molecules.tex}
\subsection{Outstanding Property Prediction Performance}
We report the model performance using mean and standard deviation over 10 runs~\cref{tab:result of molecules}. \method is the best on all 15 tasks compared to the state-of-the-art baselines. Our observations are:

\textbf{(1) \gin is the most competitive baseline and outperforms self-supervised learning methods.} On 7 of 15 tasks, \gin outperforms all the 7 self-supervised learning methods. Because self-supervised pre-training imposes constraints on the model architecture, it undermines the true power of GNNs and under-performs the GNNs that are properly used. 

\textbf{(2) Self-training and GDA methods perform better than \gin but cannot effectively learn from unlabeled data.} Self-training (\streal and \stgen) is often the best baseline in regression tasks.
GDA (\grea and \gmix) methods outperform self-training in most classification tasks except \sider,
because they are often designed to exploit categorical labeled data and remain under-explored for regression.
Although self-training benefits from selecting unlabeled examples in some graph regression tasks, they are \textit{negatively} affected by the unlabeled graphs in the classification tasks such as \toxcast and \clintox.
As indicated in~\cref{fig:compare gnn runs}, it is inappropriate to pseudo-label unlabeled graphs in self-training due to the huge gap between the unlabeled data and target task.

\textbf{(3) \method transfers useful knowledge from unlabeled data by data augmentation.} 
\method outperforms the best baseline relatively by +3.9\%, +13.4\%, and +10.2\% when there are only 1,210, 513, and 4,303 training graphs on \bace, \freesolv, and \glass, respectively. Compared to the self-supervised baselines, the improvement from \method is more significant, so the knowledge transfer is more effective. For example, on \freesolv and \oxygen, \method performs better than the best self-supervised baselines relatively by +45.8\% and +8.0\%, respectively. On regression tasks that involve knowledge transfer across domains (\eg from molecules to polymers), \method reduces MAE relatively by 1.9\%~$\sim$~10.2\% compared to the best baseline. All these results demonstrate the outstanding performance of task-specific data augmentation in \method.

\subsection{Ablation Studies and Performance Analysis}
\textbf{Comprehensive ablation studies:}
In~\cref{tab:ablation finetune}, we investigate how the task-related objectives in~\cref{eq:fine-tune objective} impact the performance of \method.
First, \method outperforms the top baseline even if the two task-related optimization objectives are disabled.
This is because \method generates new training examples based on original labeled graphs: the data augmentation has already improved the diversity of the training dataset a little bit.
Second, adding the objective $\I_1$ further improves the performance by encouraging the generation of diverse examples, because it minimizes the similarity between the original graph and augmented graph in the graph space.
Third, we receive the best performance of \method when it combines $\I_1$ and $\I_2$ objectives to generate task-related and diverse augmented graphs.
When we change the unlabeled data source from QM9 to the ZINC dataset from~\citep{jo2022score}, similar observations confirm the necessity of the task-related objectives.

\input{figures/tables/ablation}
% \input{figures/tables/wrap_ablation}
\begin{figure*}[t]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{figures/sensi_d_m.pdf} \label{fig:sensi d and m}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{figures/sensi_topn.pdf}\label{fig:sensi topn}
    \end{subfigure}%
    \caption{Effect of hyper-parameters, including the number of perturbation steps $D \in [1,10]$, the number of negative graphs $M \in [1,10]$, and top-$n$~\% labeled graphs whose labels are predicted the most accurately and that are selected for data augmentation, where $n \in [10,100]$.}
    \label{fig:sensi all}
\end{figure*}

\textbf{Effect of hyper-parameters:} The impacts of three hyper-parameters of \method are studied: the number of perturbation steps $D$, the number of negative samples $M$ in~\cref{eq:upper bound infonce}, and the number of augmented graphs in each iteration (\ie top-$n$~\% selected graph for augmentation). Results from~\cref{fig:sensi all} show that \method is robust to a wide range of $D$ and $M$ valued from 0 to 10. They suggest that $D$ and $M$ can be set as 5 in most cases. As for the number of the augmented graphs in each iteration, results show that noisy graphs are often created when $n$ is higher than 30\%, because the predictor cannot effectively guide the data augmentation for those labeled graphs whose labels are hard to predict. So, 10\% is suggested as the default of top-$n$\%.

\begin{figure}[t]
\centering
\begin{minipage}[c]{0.61\textwidth}
    \includegraphics[width=0.95\linewidth]{figures/mutual_enhanace.pdf}
\end{minipage}\hfill
\begin{minipage}[c]{0.38\textwidth}
    \caption{Data augmentation and model training mutually enhance each other over epochs. The predictor is saved every 20 epochs to guide the generation of augmented graphs. The performance of \gin trained on these augmented graphs reflects the quality of the augmented data.}
    \label{fig:mutual enhance}
\end{minipage}
\end{figure}

\textbf{Iterative process:} 
\cref{fig:mutual enhance} investigates the relationship between the quality of augmented graphs and the accuracy of property prediction models.
We save a predictor checkpoint every 20 epochs to guide the generation of the augmented examples.
We evaluate the quality of augmented graphs by using them to train \gin and report AUC/MAE. The data augmentation gradually decreases the training loss of property prediction. On the other hand, the increased \gin performance indicates that the quality of augmented examples is also improved over epochs. The data augmentation and predictor training mutual enhance each other.

\subsection{Interpretability of Visible Knowledge Transfer}
\begin{figure}[t]
    \centering
    \vspace{-0.25in}
    \begin{minipage}[c]{0.61\textwidth}
    \includegraphics[width=\linewidth]{figures/example.pdf}
    \end{minipage}\hfill
    \begin{minipage}[c]{0.38\textwidth}
    \caption{Case studies of augmented graphs. The green highlighted subgraphs are from \gin with top-k pooling. Examples show that the augmented graphs from~\method preserve the core structures of original graphs. Key concepts in the unlabeled graphs like chemical validity are transferred to downstream tasks. Domain knowledge such as the relationship between the permeability and the fluorine atom/methyl group is captured to guide task-specific generation. 
    }
    \label{fig:visual}
    \end{minipage}
    \vspace{-0.15in}
\end{figure}

\vspace{-0.09in}
Knowledge transfer by data augmentation gives visible examples, allowing us to study what is learned. We visualize a few augmented graphs in \method using \bace and \oxygen. We adapt top-k pooling~\citep{knyazev2019understanding} to select the subgraphs that \gin used for prediction. The selected subgraphs are highlighted in green in~\cref{fig:visual}. The three examples show that \emph{the augmented graphs can identify and preserve the core structures} that \gin uses to predict property values. These augmented graphs are chemically valid, showing that \emph{concepts such as some chemical rules from the unlabeled graphs are successfully transferred to downstream tasks}. More results are in~\cref{add:sec:chemical_valid}. Regarding task-specific knowledge, it is known that the fluorine atom and the methyl group are usually negatively and positively correlated to the permeability, respectively~\citep{park2003gas,corrado2020macromolecular}.
The augmented examples show that \emph{\method captures this domain knowledge with the task-related objectives}. In example (b), \method replaces most of the fluorine atoms with the methyl groups. It encourages \gin to learn the positive relationship between the methyl group and the permeability so that \gin predicts a high label value. In example (c), \method replaces the methyl groups with fluorine atoms. It encourages \gin to learn the negative relationship between the fluorine atom and the permeability so that \gin predicts a low label value.
