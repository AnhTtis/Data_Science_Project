\documentclass[10pt,twocolumn]{article}
% \documentclass[journal]{IEEEtran}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\renewcommand\thesection{\Alph{section}}
\usepackage{booktabs}
\usepackage{color}
\usepackage{url}
\usepackage{amsmath,amsfonts}


\usepackage{array}
\usepackage{diagbox}

\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
% \usepackage[justification=centering]{caption}%
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}

\bibliographystyle{unsrt}
\usepackage{listings}

\usepackage{graphicx}
% \usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{makecell}
\usepackage{bbding}
\let\Bbbk\relax
\usepackage{amssymb}
\usepackage{microtype}
\usepackage{float}
\usepackage{makecell}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{epstopdf}
\usepackage{booktabs}

\usepackage[normalem]{ulem}


% \usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}
% \title{Weakly-Supervised Temporal Action Localization by\\ Inferring Snippet-Feature Affinity}
In this supplementary material, we provide more details that are not presented in the main paper due to space limitations. In the following, we first show the details of each training loss function in Sec.~\ref{sec:tra}. Then, we show more details of the Affinity Inference Module and implementation details in Sec.~\ref{sec:aff} and Sec.~\ref{sec:imp}, respectively.
% Furthermore, we show more ablation experiments in Sec.~\ref{sec:abl}. 
Finally, Sec.~\ref{sec:vis} illustrates T-SNE visualization of foreground and background embedding features and more qualitative comparisons on THUMOS14 dataset. 


\section{Training loss}
\label{sec:tra}
In the main paper, our total loss function consists of video-level classification loss $\mathcal{L}_\mathrm{cls}$~\cite{Huang2021ForegroundActionCN}, knowledge distillation loss $\mathcal{L}_\mathrm{kd}$ ~\cite{Huang_2022_CVPR} and attention normalization loss $\mathcal{L}_\mathrm{att}$~\cite{10.1007/978-3-030-58539-6_3}, which is formulated as:
\begin{equation}
\mathcal{L}=\mathcal{L}_\mathrm{c l s}+\beta \mathcal{L}_\mathrm{k d}+\lambda\mathcal{L}_\mathrm{a t t},
\end{equation}
where $\beta,\lambda$ denote trade-off factors. 

\textbf{Video-level classification loss} $\mathcal{L}_\mathrm{cls}$: 
The classification head is mainly divided into two parts: the Class-agnostic Attention~(CA) head and the Multiple Instance Learning~(MIL) head.
Specifically, the CA head first exploits the video snippet-features $\mathcal{F}\in \mathbb{R}^{T \times D}$ and a foreground classifier $\mathcal{W}_f \in \mathbb{R}^{D}$ to get the foreground attention scores $\mathcal{S}_f \in \mathbb{R}^{T}$. 
Then, aggregating $\mathcal{S}_f$ and feature $\mathcal{F}$ to obtain a video-specific foreground feature. 
Finally, uses the video-specific foreground feature and an action classifier $\mathcal{W}_a\in \mathbb{R}^{(C+1) \times D}$  to get video-level class confidence scores 
$\boldsymbol{p}^{ca} \in \mathbb{R}^{C+1}$.
Afterwards, the $\boldsymbol{p}^{ca}$ with its ground-truth class-level labels to jointly guide the CA optimization procedure by the cross-entropy loss $\mathcal{L}_\mathrm{CA}$ as follows:
\begin{equation}
\mathcal{L}_\mathrm{CA}=-\sum_{c=1}^{C+1}\hat{\mathbf{y}}_c \log \boldsymbol{p}^{ca}_c,
\end{equation}
where $\hat{\mathbf{y}}\in \mathbb{R}^{(C+1)}$ is the ground-truth vector
and the $(C+1)$-th value is setting as 0.

Multiple Instance Learning head
first exploits the video snippet-features $\mathcal{F}$ and the action classifier $\mathcal{W}_a$ to get the class activation scores $\mathcal{S}_a \in \mathbb{R}^{T \times (C+1)}$. Then, the softmax operation is performed on $S_a$ to obtain the class-wise attention scores $\mathcal{S}_c  \in \mathbb{R}^{T \times (C+1)}$.
Finally, we get the video-level class activation scores $\boldsymbol{p}^{mil} \in \mathbb{R}^{C+1}$ through aggregating $\mathcal{S}_a$ and $\mathcal{S}_c$. Afterwards, we optimize the MIL head by the cross-entropy loss $\mathcal{L}_\mathrm{MIL}$ as follows:
\begin{equation}
\mathcal{L}_{\mathrm{MIL}}=-\sum_{c=1}^{C+1}\hat{\mathbf{y}'}_c \log \boldsymbol{p}^{mil}_c,
\end{equation}
where $\hat{\mathbf{y}'}\in \mathbb{R}^{(C+1)}$ is similar as $\hat{\mathbf{y}}$ except the $(C+1)$-th value is setting as 1.

To sum up, the video-level classification loss is defined as:
\begin{equation}
\mathcal{L}_\mathrm{cls}=\mathcal{L}_\mathrm{CA} + \theta\mathcal{L}_\mathrm{MIL}.
\end{equation}


\textbf{Knowledge distillation loss $\mathcal{L}_\mathrm{kd}$}~\cite{Huang_2022_CVPR}: It is used to implement the process of ${\mathcal{T}^p}$ supervising $\mathcal{T}$ for training.
\begin{equation}
\mathcal{L}_\mathrm{k d}= -\frac{1}{{T}} \sum_{t=1}^T \mathcal{T}^p_t \log (\mathcal{T}_t),
\end{equation}
where ${\mathcal{T}^p}$ represents the output predicted by the information interaction module after classification head, $\mathcal{T}$ denotes the output predicted by the base branch after classification head and $T$ is the number of snippets.

\textbf{Attention normalization loss} $\mathcal{L}_\mathrm{att}$~\cite{10.1007/978-3-030-58539-6_3}: It is used in the output of the base branch and the purpose is to explicitly avoid the ambiguity of attention, which can be formulated as:
\begin{equation}
\mathcal{L}_{\mathrm{a t t}}=\frac{1}{T} \min \sum_{a \in  A} a-\frac{1}{T} \max \sum_{a \in A} a,
\end{equation}
where $T$ denotes the number of snippets. $A$ represents the attention value of each snippet, which is obtained by a fully-connected layer.


 \begin{algorithm}[ht]
 \caption{The process of inferring foreground and background candidates.}
 \label{alg:1}
 \LinesNumbered 
 \KwIn{Video snippet-features $\mathcal{F}$; Snippet number $T$; Feature dimensional $D$; Parameter $K$.}
 \KwOut{Coarse pseudo labels $\mathrm{B}=\{b_i\}_{i=1}^T$ of foreground and background candidates.}
 \For{$t = 1, t \leq T, t+\hspace{-0.1cm}+$}{Calculate affinity value ${\tau}_{(t-1, t)}$ between feature $f_{t-1}$ and feature $f_t$ by $\sum_{d=1}^D|\operatorname{diff}(f_t, f_{t-1}), d)|$\;}   
Obtain affinity set $\tau$ of each temporal neighborhood snippet pair of the video\; 
 ${\operatorname{Sorted}}(\tau) \leftarrow \operatorname{Descending \;sort}(\mathcal{\tau})$\;  
\eIf{$\tau_{(t, t+1)} \in$ First $(\operatorname{sorted}(\tau), K)$}
{Select the snippet $f_t$ as foreground candidates and set $b_t=1$ \;}
{Select the snippet $f_t$ as background candidates and set $b_t=0$ \;}
Obtain coarse pseudo labels $\mathrm{B}$
\end{algorithm}
% -----------------------------------------------------



\section{More Details of Affinity Inference Module}
\label{sec:aff}
We present the process of how to infer foreground and background candidates in the affinity inference module in Alg.~\ref{alg:1}. Specifically, we first calculate the affinity value $\tau_{(t-1, t)}$ of each temporal adjacent snippet-features pair, and obtain the affinity set $\tau$ of the whole video. Then, we perform a descending sorting on the affinity set $\tau$ and then regard the first $K$ sorted scores as the potential foregrounds and other ones as the potential backgrounds. Finally, coarse pseudo labels are generated. 



\section{More Implementation Details }
\label{sec:imp}
During the test phase, we follow~\cite{lee2020BaS-Net} to use a set of thresholds from 0.001 to 0.04 with the step 0.002 and perform non-maximum suppression~(NMS) with threshold 0.45 in THUMOS14 dataset to obtain the action instance proposals. 
The number of snippets $N$ with the high scores for each video category is stored as 9 in memory. For ActivityNet v1.3 dataset, we set the NMS with a threshold of 0.9 and $N$ is set to 5. 

\noindent
\textbf{ More details about test phase.}~
As mentioned in the main paper, we use the base branch and intra-video information interaction branch to obtain the action instance proposals during the test phase. We evaluate the impact of different variants on the test results, the result and MACs are shown in Table~\ref{tab:2}. The different variants include 1) \textit{Base} denotes using only the base branch; 2) \textit{Base + intra} denote using base branch and intra-video information interaction branch; 3) \textit{Base + intra + inter} denotes using base branch, intra- and inter-video information interaction branch.
We find that the same performance is achieved when 2) and 3), but smaller MACs is obtained when using the base branch and intra-video information interaction branch. Therefore we utilize the base branch and intra-video information interaction branch to obtain the action instance proposals.

\noindent
\textbf{ More details about memory bank.}~
We employ additional action information from videos of the same category to help improve the discriminative nature of the snippet-features and the quality of the generated pseudo-labels. Therefore, we explore the effects of the number of snippets $N$ each class stored in memory, and the results are shown in the Figure~\ref{fig:1}. Our model achieves optimal performance when $N$ is 9.

\noindent
\textbf{ More details about the impact of different variants of intra-video information interaction.}~
As mentioned in the main paper, we evaluate the impact of different variants of intra-video information interaction. To visualize the differences between these variants, we show these variants in Figure~\ref{fig:2}, in which 
1) \textit{self} denotes temporal attention between video snippet-features $\mathcal{F}$ and itself;
2) \textit{w/o fore} and \textit{w/o back} denote removing the foreground and background snippet-features, respectively;
3) \textit{fore + back} denotes directly adding the two types of features together; 
4) \textit{dynamic mixing} denotes using dynamic mixing operation to fuse two types of snippet-features; 
5) \textit{temporal} denotes enhancing video snippet-features only using the temporal attention. 



\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.0}
\setlength{\tabcolsep}{3.8mm}{
\begin{tabular}{l|cc}
\specialrule{0.1em}{0.5pt}{0.5pt}
\hline
\multirow{1}{*}{Method}     & AVG   & MACs  \\ \hline
RSKP~\cite{Huang_2022_CVPR}                & 45.1    &  0.48G  \\
ASM-Loc~\cite{he2022asm}             & 45.1    &  0.66G       \\ 
Base                & 46.1    &   0.28G      \\
Base + intra        & 46.8    &  0.33G       \\ 
Base + intra + inter  & 46.8    &   0.44G      \\ \hline
\specialrule{0.1em}{0.5pt}{0.5pt}
\end{tabular}}
\vspace{2.5mm}
\caption{Ablation study about the different components in test phase on THUMOS14 dataset. The AVG column shows the averaged mAP under the IoU thresholds [0.1:0.1:0.7].}
\label{tab:2}
\end{table}

\begin{figure}[t]
\begin{center}
\setlength{\fboxrule}{0pt}
\setlength{\fboxsep}{0cm}
\vspace{-2.0mm}
\fbox{\rule{0pt}{0in} \rule{.0\linewidth}{0pt}
\hspace{-3.0mm}
\includegraphics[width=0.85\linewidth]{ab.pdf}}
    \end{center}
\vspace{-2.8mm}
    \caption{Ablation experiments about the number of $N$ on THUMOS14 dataset.}
\label{fig:1}
\end{figure}


%-------------------------------------
\begin{figure*}[ht]
\begin{center}
\hspace{-2.5mm}
    \includegraphics[width=1.0\linewidth]{var.pdf}
\end{center}
\vspace{-0.5mm}
    \caption{Illustration of the different variants of intra-video information interaction.} 
\label{fig:2}
\end{figure*}




\section{More Visualization analysis}
\label{sec:vis}
\noindent
\textbf{T-SNE visualization of foreground and background embedding features.}~
To intuitively demonstrate the discrimination capability of the learned features, we visualize results about the embedding features on different video examples on THUMOS14 dataset by T-SNE~\cite{articledata}. The results are presented in Figure~\ref{fig:3}.  
We observe that our method can accurately bring the embedding features of foregrounds together, and make them away from the background.
These findings indicate that the discriminative capability of the learned features by our optimized model can effectively distinguish the backgrounds and foregrounds, which leads to more accurate action localization results.

\noindent
\textbf{Qualitative results}. We present more qualitative results on THUMOS14 dataset. Figure~\ref{fig:4} shows the visualization results by comparing our proposed method with the base branch, and also presents the affinity values among each temporal neighborhood snippet pair. 
These results demonstrate the feasibility of inferring the foreground and background candidates by the affinity inference module. Meanwhile, our method can locate more accurate action regions than the base branch, which intuitively indicates the effectiveness of the model in this paper.


%-------------------------------------
\begin{figure*}[ht]
\begin{center}
\hspace{-2.5mm}
    \includegraphics[width=1.0\linewidth]{featuresup.pdf}
\end{center}
\vspace{-0.5mm}
    \caption{T-SNE visualization of foreground and background embedding features.
    We compare the (a) Base branch and (b) our method on different examples of THUMOS14 dataset, where red points mean the foregrounds and blue points indicate the backgrounds.
    } 
\label{fig:3}
\end{figure*}
%-------------------------------------

%--------------------------
\begin{figure*}[t]
\begin{center}
\setlength{\fboxrule}{0pt}
\setlength{\fboxsep}{0cm}
\hspace{-2.5mm}
    \includegraphics[width=1.03\linewidth]{resultsup.pdf}
\end{center}
\vspace{-0.5mm}
    \caption{Qualitative comparisons of our method and Base branch on THUMOS14 dataset. Meanwhile, we also present the affinity values among each temporal neighborhood snippet pair. We show the action scores on different examples from top to bottom. Moreover, red points mean the foregrounds and black points indicate the backgrounds.} 
\label{fig:4}
\end{figure*}
%-------------------------------------

{\small
\bibliographystyle{ieee_fullname}
\bibliography{supplementary}
}

\end{document}
