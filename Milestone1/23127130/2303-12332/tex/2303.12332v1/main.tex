\documentclass[10pt,twocolumn]{article}
% \documentclass[journal]{IEEEtran}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color}
\usepackage{url}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{diagbox}
\usepackage{authblk}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}

\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}


\bibliographystyle{unsrt}
\usepackage{listings}

\usepackage{graphicx}

\usepackage{multirow}
\usepackage{mathtools}
\usepackage{makecell}
\usepackage{bbding}
\let\Bbbk\relax
\usepackage{amssymb}
\usepackage{microtype}
\usepackage{float}
\usepackage{makecell}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{epstopdf}
\usepackage{booktabs}

\usepackage[normalem]{ulem}


% \usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Weakly-Supervised Temporal Action Localization by\\ Inferring Snippet-Feature Affinity}

\author{Wulian Yun}
\author{Mengshi Qi}
\author{Chuanming Wang}
\author{Huadong Ma}
\affil{Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia,\\ Beijing University of Posts and Telecommunications \authorcr \tt \small \{yunwl,qms,wcm,mhd\}@bupt.edu.cn}

\maketitle
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi
\begin{abstract}
Weakly-supervised temporal action localization aims to locate action regions and identify action categories in untrimmed videos, only taking video-level labels as the supervised information. Pseudo label generation is a promising strategy to solve the challenging problem, but most existing methods are limited to employing snippet-wise classification results to guide the generation, and they ignore that the natural temporal structure of the video can also provide rich information to assist such a generation process. In this paper, we propose a novel weakly-supervised temporal action localization method by inferring snippet-feature affinity. First, we design an affinity inference module that exploits the affinity relationship between temporal neighbor snippets to generate initial coarse pseudo labels. Then, we introduce an information interaction module that refines the coarse labels by enhancing the discriminative nature of snippet-features through exploring intra- and inter-video relationships. Finally, the high-fidelity pseudo labels generated from the information interaction module are used to supervise the training of the action localization network. Extensive experiments on two publicly available datasets, \emph{i.e.}, THUMOS14 and ActivityNet v1.3, demonstrate our proposed method achieves significant improvements compared to the state-of-the-art methods.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Temporal action localization (TAL)~\cite{7780488,8237579,8578222,Huang_2022_CVPR,he2022asm, Lin_2018_ECCV, 8953536} aims to find action instances from untrimmed videos, \emph{i.e.}, predicting the start positions, end positions, and categories of certain actions. It is an important yet challenging task in video understanding and has been widely used for surveillance and video summarization. 
For accurate localization, most existing methods~\cite{7780488,8237579,8578222, Lin_2018_ECCV, 8953536} rely on training a model in a fully supervised manner with the help of human-labeled precise temporal annotations. However, fine-detail labeling of videos is labor-intensive and expensive. In contrast, weakly-supervised methods only utilize video-level labels for temporal action localization, achieving competitive results while reducing the cost of manual annotations, which has gained increasing attention from both academia and industry.

\begin{figure}[t]
\begin{center}
\setlength{\fboxrule}{0pt}
\setlength{\fboxsep}{0cm}
% \vspace{-2.5mm}
\fbox{\rule{0pt}{0in} \rule{.0\linewidth}{0pt}
\hspace{-0.5mm}
     \includegraphics[width=0.95\linewidth]{mota2.pdf}}
\end{center}
\vspace{-0.5mm}
    \caption{ Illustration of the relationships among snippets, action scores and the ground truth~(GT). The action snippets are marked as red and the background snippets are marked as black.}
    \vspace{-1.em}
\label{fig:1}
\end{figure}
%--------------------------

Weakly-supervised TAL methods~\cite{10.1609/aaai.v33i01.33019070,8953341,DBLP:conf/eccv/MinC20,Shi_2020_CVPR,qu_2021_acmnet,DBLP:conf/aaai/LeeWLB21,9578220,AUMN,DBLP:conf/iccv/NarayanCHK0021} mainly utilize a ``localization by classification'' framework, where a series of Temporal Class Activation Maps~(TCAMs)~\cite{8578804,paul2018w} are obtained by snippet-wise classification, and then TCAMs are used to generate temporal proposals for action localization. 
However, the classifiers focus mostly on the easily distinguishable snippets while ignoring the less salient ones, so there is a discrepancy between classification and localization which leads to inaccurate action localization results.
Recently, pseudo label based methods~\cite{Huang_2022_CVPR, he2022asm,9423165,10.1007/978-3-030-58539-6_3,DBLP:conf/eccv/LuoGSKWDX20,Li_2022_CVPR} have been proposed to alleviate this problem, most of which use the action proposals obtained from the snippet-wise classification to guide the pseudo label generation.  
However, we observe that the video naturally provides affinity relationship (variation between snippets) in temporal dimension that can help guide pseudo label generation. As shown in Figure~\ref{fig:1}, neighbor snippet-features of action and background have obvious distinctively affinity relationships.
The action (left) is represented by a red dot, exhibiting a substantial variation among neighbor snippet-features, indicating a higher affinity value. In contrast, the background (right) is denoted by a black dot, which is associated with a smaller affinity value.

Inspired by this phenomenon, we propose a novel weakly-supervised TAL method, which takes a novel perspective that leverages the temporal affinity to tag each snippet with a high-fidelity pseudo label.
First, we design an affinity inference module that leverages the variation between snippet-features and calculates the affinity relationships of neighbor snippet pairs, providing initial coarse pseudo labels for these snippets.
However, this process only considers local relationships and ignores the global structure information of the video. Thus, we propose an information interaction module to refine the initial coarse pseudo labels by exploring the intra- and inter-video relationships. The former helps to distinguish foreground and background, while the latter assists to compact the snippet-features of the same class.
Finally, the output features of the information interaction module are fed into the classification head to generate the final refined pseudo labels for high-reliable supervision. The contributions of this paper can be summarized as follows:


\par\textbf{(1)} We propose a simple yet effective pseudo label generation strategy by inferring snippet-feature affinity, which exploits the temporal variation to 
% \rev{provide a relatively reliable foreground and background candidates.}
 guide the generation of high-fidelity pseudo labels for weakly-supervised TAL. 
% \rev{tag each snippet with a coarse pseudo label.}


\par\textbf{(2)} We design a novel information interaction module that enhances the discriminative nature of snippet-features by establishing connections within and across videos.


%To verify the effectiveness and superiority of the proposed SADA, 
\par\textbf{(3)} We conduct extensive experiments on two widely used datasets, and the results show that our model achieves 46.8 and 25.8 average mAP on THUMOS14 and ActivityNet v1.3, respectively, demonstrating its superiority.

  

\section{Related Work}
% \subsubsection{Fully-supervised temporal action localization.}
\noindent
\textbf{Fully-supervised temporal action localization.}
Fully supervised TAL has been an active research area for many years and existing methods are divided into two categories,~\emph{i.e.,} one-stage methods and two-stage methods. One-stage methods~\cite{7780488,8578222,8953536, Zhao2020BottomUpTA} predict action boundaries as well as labels simultaneously. On the contrary, two-stage methods ~\cite{7780488,8237579,Lin_2018_ECCV,8578222, LinLWTLCWLHJ20} first find candidate action proposals and then predict their labels. However, these fully supervised methods are trained with instance-level human annotation, leading to an expensive and time-consuming process. 



% \subsubsection{Weakly-supervised temporal action localization.} 
\noindent
\textbf{Weakly-supervised temporal action localization.}
Weakly-supervised TAL methods~\cite{10.1609/aaai.v33i01.33019070,8953341,DBLP:conf/eccv/MinC20,Shi_2020_CVPR,qu_2021_acmnet,DBLP:conf/aaai/LeeWLB21,9578220,AUMN,DBLP:conf/iccv/NarayanCHK0021,9423165,DBLP:conf/eccv/LuoGSKWDX20,10.1007/978-3-030-58539-6_3,9577332, Huang_2022_CVPR,he2022asm, mengyuan2022ECCV_DELU} mainly learn from video-level labels, which avoid a labor-intensive annotation process compared to the fully supervised methods. UntrimmedNet~\cite{inproceedingsU} and STPN~\cite{8578804} generate class activation sequences by Multiple Instance Learning (MIL) framework and then locate action instances by thresholding processing. RPN ~\cite{DBLP:conf/aaai/HuangHOW20a} and 3C-Net~\cite{9008791} use metric learning algorithms to learn more discriminative features. Lee~\emph{et al.}~\cite{lee2020BaS-Net} design a background suppression network to suppress background snippet activations. DGAM~\cite{Shi_2020_CVPR} studies the problem of action-context separation.
%%Zhang~\emph{et al.}~\cite{zhang2021cola} utilize a contrastive learning strategy to take action and background snippets more distinguishable. Huang~\emph{et al.}~\cite{Huang2021ForegroundActionCN} explore the action-foreground consistency problem and improve the accuracy of localization.
%with hybrid attention.
%%Chen~\emph{et al.}~\cite{mengyuan2022ECCV_DELU} extend the traditional paradigm of EDL to better adapt to the multi-label classification for action-background ambiguity. % However, these methods are difficult to ensure the consistency of foreground and action, and there is an inconsistency between classification and localization.
However, there is a discrepancy between classiﬁcation and localization. Recently, numerous methods~\cite{9423165,DBLP:conf/eccv/LuoGSKWDX20,10.1007/978-3-030-58539-6_3,9577332, Huang_2022_CVPR,he2022asm} attempt to generate pseudo labels to supervise the model training process and thus alleviate the discrepancy.
RefineLoc~\cite{9423165} alleviates the discrepancy between classification and localization by extending the previous detection results to generate pseudo labels.
Luo~\emph{et al.}~\cite{DBLP:conf/eccv/LuoGSKWDX20} exploit the Expectation–Maximization framework~\cite{543975} to generate pseudo labels by alternately updating the key-instance assignment branch and the foreground classification branch.
TSCN~\cite{10.1007/978-3-030-58539-6_3} generates frame-level pseudo labels by later fusing attention sequences in consideration of two-stream consensus.
%%Yang~\emph{et al.}~\cite{9577332} present an 
%%uncertainty-guided collaborative training strategy to reduce the noise in the labels while generating pseudo labels.
Li~\emph{et al.}~\cite{Li_2022_CVPR} exploit contrastive representation learning to enhance the feature discrimination ability.
%%Huang~\emph{et al.}~\cite{Huang_2022_CVPR} propose a representative snippet summarization and propagation framework to generate the pseudo label,
%%which is used to guide the training of the model.
ASM-Loc~\cite{he2022asm} generates action proposals as pseudo labels by using the standard MIL-based methods. In this paper, our method differs in that we start from the video features themselves and exploit the variation between neighbor snippet-features to find affinity relationship, which is used to generate initial coarse pseudo labels. Furthermore, we design an intra-/inter- video information interaction module to generate fine-grained pseudo labels.


%-------------------------------------
\begin{figure*}[t]
\begin{center}
\setlength{\fboxrule}{0pt}
\setlength{\fboxsep}{0cm}
% \vspace{-4mm}
\hspace{-2.5mm}
     % \includegraphics[width=0.95\linewidth, height=0.545\textwidth]{LaTeX/figure/model.pdf}
     \includegraphics[width=0.9\linewidth]{presentation1.pdf}
% \fbox{\rule{0pt}{0in} \rule{.0\linewidth}{0pt}
% }
\end{center}
\vspace{-3.5mm}
\caption{
Overview of our model. Firstly, the base branch (a) extracts features from RGB and optical flow in a video and uses the classification head to predict TCAMs. Then, the affinity inference module~(b) exploits the affinity relationship between snippet-features to assign initial coarse pseudo labels to each snippet. Next, the information interaction module (c) utilizes the multi-level attention to refine the labels by exploring intra- and inter-video relationships.
Finally, (c) generates high-fidelity pseudo labels to supervise the base branch.
}
\label{fig:2}
\end{figure*}
%-----------------------------------------
\section{Methodology}
In this section, we will begin by presenting the problem definition of weakly-supervised TAL and provide an overview of our proposed method. Next, we will describe the different modules of our method in detail, which are designed to generate high-fidelity pseudo labels by utilizing the snippet-feature affinity. Finally, we introduce the training details of optimizing the temporal localization model.

\noindent
\textbf{Problem definition.}
% \del{Given a set of untrimmed training videos $V$ and the corresponding ground-truth labels $y \in \mathbb{R}^{C}$, where $C$ denotes the number of action categories. We aim to predict a set of action instances {($c$, $q$, $t_s$, $t_e$)} for each test video. }
Weakly-supervised TAL aims to predict a group of action instances ($c$, $q$, $t_s$, $t_e$) for each test video with the assistance of a set of untrimmed training videos $\{V_i\}^N_{i=1}$ and their corresponding ground-truth labels $\{{y_i}\}^N_{i=1}$. Specifically, $y_i \in \mathbb{R}^C$ is a binary vector indicating the presence/absence of each of $C$ actions. For one action instance, $c$ denotes the action category, $q$ refers to the prediction confidence score, $t_s$ and $t_e$ mean the start time and end time of the action, respectively.

\noindent
\textbf{Overview.}
The overview of our proposed method is shown in Figure~\ref{fig:2}, which mainly contains three parts: \emph{(a) the base branch}, \emph{(b) the affinity inference module}, and \emph{(c) the information interaction module}.
First, in the base branch, we exploit a fixed pre-trained backbone network~(e.g., I3D) to extract $T$ snippet-features from both the appearance (RGB) and motion (optical flow) of the input video. Then, a learnable classification head is adopted to classify each snippet and obtain the predicted TCAMs.
Second, the affinity inference module generates the affinity values by calculating the difference between adjacent pairs of snippet-features, and initial coarse labels $\{b_i\}^T_i$ are assigned to these snippets based on the obtained affinity.
Subsequently, the information interaction module utilizes multi-level attention to explore the  intra- and inter-video relationships, thereby enhancing the separability of action snippet-features from those of the background and other classes. 
Finally, the output features are fed into the classification head to generate high-fidelity pseudo labels as a supervised signal for the base branch.
 

%-----------------------------------------

\subsection{Base Branch}
Given an untrimmed video $V$, we follow~\cite{8578804,Huang_2022_CVPR} to spilt it into multiple non-overlapping snippets $\{v_i\}^T_{i=1}$, and then we use the I3D~\cite{8099985} network pre-trained on the Kinetics-400~\cite{DBLP:journals/corr/KayCSZHVVGBNSZ17} dataset to extract features from the RGB and optical flow streams for each snippet.
An embedding layer takes the concatenation of these two types of features to fuse them together, and the fused features of all snippets are treated as the snippet-features of the video $\mathcal{F}=\{f_1, f_2, \cdots, f_T\} \in \mathbb{R}^{T \times D}$, where T is the number of snippets and D denotes the dimension number of one snippet-feature.
%\subsubsection{Action Prediction}
Next, we use the classification head to obtain Temporal Class Activation Maps~(TCAMs) $\mathcal{T} \in \mathbb{R}^{T \times (C+1)}$, where $C+1$ denotes the number of action categories plus the background class. 
% \del{Specifically, we follow~\cite{Huang_2022_CVPR} to use the Class-agnostic Attention~(CA) head and Multiple Instance Learning~(MIL) head to generate video-level class confidence scores.}
Specifically, following previous work~\cite{Huang_2022_CVPR}, the classification head consists of a Class-agnostic Attention~(CA) head and a Multiple Instance Learning~(MIL) head.

\subsection{Affinity Inference Module}
The variation of temporal neighbor snippets can indicate whether each snippet belongs to the foreground or background, with higher variation between foreground snippets and lower variation between background snippets, as shown in Figure~\ref{fig:1}. Therefore, we propose an affinity inference module that utilizes the variation to explore the affinity relationships of neighbor snippet pairs and then use it to produce foreground and background candidates in the video.


Given a video and its snippet-level representation $\mathcal{F} \in \mathbb{R}^{T \times D}$, we first calculate the affinity value $\tau_{(t-1,t)}$ of each pair of temporal adjacent snippet-features $\{f_{t-1}$, $f_{t}\}$ in the formulation of:
% }\del{, which can be formulated as}: 
\begin{equation}
\label{eq:2}
\tau_{(t-1,t)}= \sum_{d=1}^{D}|\mathrm{diff}(f_{t}, f_{t-1}, d)|,
\end{equation}
where $\mathrm{diff}$ denotes the operation of dimensional-wise reduction, and $d\in D$ means the element index of the feature. 
Subsequently, we obtain the affinity set $\tau$ of the input video by calculating the difference for all pairs:
\begin{equation}
\tau=\{\tau_{(1,2)},\tau_{(2,3)},\cdots,\tau_{(t-1,t)}\}.
\end{equation}
%%\textcolor{red}{the ranking process need to be described more here.}

To  obtain foreground snippets of the video, we first perform a descending 
% \note{if you add minus symbol in Eq.2, change descending to ascending} 
sort on the affinity set $\tau$, and
then assign the initial coarse pseudo labels $\mathcal{B}=\{b_{i}\}_{i=1}^{T}$ to each snippet based on the sorted $\tau$. The snippets with the first $K$ sorted scores are selected as potential foreground, while the remaining are selected as potential background, and the process of assigning pseudo labels can be formulated as:
\begin{equation}
    b_{t} = \left\{ 
        \begin{aligned} 
        1&, \text{if} \ \tau_{(t,t+1)}\in \text{First}({\operatorname{sorted}}(\tau), K) \\
        0&, otherwise \\
        \end{aligned} 
    \right
    .,
\end{equation}
where $b_t=1$ denotes that its corresponding snippet $f_t$ belongs to the foreground candidates, otherwise to the background candidates. 
% \del{Finally, we assign every snippet with a coarse pseudo label through the proposed affinity inference module.} 
Finally, coarse pseudo labels are generated for the snippets in a simple manner. Since the backbone is fixed, the affinity will not change during training, which ensures stability. However, directly utilizing them to supervise the learning of the base branch will achieve poor performance since there are many misidentified samples, but it establishes a start for generating high-fidelity pseudo labels. Next, we will present how to utilize these candidates to refine these coarse pseudo labels. %generate high-fidelity pseudo labels.


\subsection{Information Interaction Module}

In the affinity inference module, we calculate the affinity values between each pair of adjacent snippets, and the operation can be seen as one type of exploiting local relationships, but the relationship among non-local snippets is still underexplored. Therefore, we propose an information interaction module that enhances the discriminative nature of snippet-features by exploring intra- and inter-video relationships, improving the fine-grained quality of generated pseudo labels. We collect the features of foreground ($b_i$=1) and background ($b_i$=0) candidates to form $\mathcal{F}^a\in\mathbb{R}^{T^a\times D}$ and $\mathcal{F}^b\in\mathbb{R}^{T^b\times D}$, respectively, where $\mathcal{F}^a \cup \mathcal{F}^b = \mathcal{F}$ , $T^a + T^b = T$, $T^a$ denotes the number of foreground snippets, and $T^b$ denotes the number of background snippets.

\textbf{Intra-video information interaction.}
In order to make more separable between foreground and background snippet-features, we explore the relationship among the foreground, background, and video snippet-features within the same video through performing a multi-level attention along the channel and temporal dimensions, respectively.
First, we learning a channel-wise attention in the squeeze-and-excitation pattern
% ~\cite{hu2018senet}
to generate the feature $\hat{\mathcal{F}}^{a} \in \mathbb{R}^{T^a \times D}$:

\begin{equation}
\label{eq:5}
\hat{\mathcal{F}}^{a}= \frac{\exp \left(\theta(\mathcal{F}^a)\right)}{\sum_{d=1}^D \exp\left(\theta(\mathcal{F}^a_{\cdot, d})\right)} \otimes \mathcal{F}^{a} + \mathcal{F}^a,
% \in \mathbb{R}^{T^a \times D},
\end{equation}
where $\otimes$ denotes the element-wise multiplication. $\theta$ is a simple multi-layer preceptron, which is consisted by the sequence of \textit{FC-ReLU-FC}. We set the weight of the first \textit{FC} to ${\mathbf{W}_1} \in \mathbb{R}^{D \times (D/r)}$ and that of the second \textit{FC} to $ {\mathbf{W}_2} \in \mathbb{R}^{(D/r) \times D}$, and $r$ is a scaling factor.
% \rev{, which is set to 16 in experiments}
Residual connection is adopted to maintain the stability of training. 

Then, we conduct a temporal-level attention to capture the global contextual relationships between $\hat{\mathcal{F}}^{a}$ and $\mathcal{F}$ 
as the following equation:
\begin{equation}
\label{eq:6}
\tilde{\mathcal{F}}^a= \operatorname{softmax} (\mathcal{F} \odot (\hat{\mathcal{F}}^a)^T) \odot \hat{\mathcal{F}}^a,
\end{equation}
where $\odot$ denotes the matrix multiplication. 
% \note{there misses a transpose symbol, maybe $(\mathcal{\hat{F}}^a)^T$}
By integrating such multi-level attention learning, we obtain a set of discriminative snippet-features $\mathcal{\tilde{F}}^a \in \mathbb{R}^{T\times D}$. 

However, some information contained in $\mathcal{F}^b$ is neglected, which contains some misidentified foreground snippets or action-related information. 
Thus, utilizing the information in $\mathcal{F}^b$ can help boost the diversity of snippet-features, and we also perform the multi-level attention between $\mathcal{F}^{b}$ and $\mathcal{F}$ to generate background-enhanced features $\tilde{\mathcal{F}}^{b}$ through Eq.(\ref{eq:5}) and Eq.(\ref{eq:6}). Note that the parameters in Eq.\eqref{eq:5} are not shared between $\mathcal{F}^a$ and $\mathcal{F}^b$.

Subsequently, we apply a dynamic mixing operation to balance the contribution between $\tilde{\mathcal{F}}^{a}$ and $\tilde{\mathcal{F}}^{b}$ to obtain the enhanced features $\tilde{\mathcal{F}} \in \mathbb{R}^{T\times D}$ as follows:
\begin{equation}
\tilde{\mathcal{F}}=\operatorname{mix}(\tilde{\mathcal{F}}^{a}, \tilde{\mathcal{F}}^{b}, \sigma) = \sigma  \tilde{\mathcal{F}}^{a}+(1-\sigma)  \tilde{\mathcal{F}}^{b},
\end{equation}
where $\sigma$ denotes a trade-off factor. 

\textbf{Inter-video information interaction.} 
Taking into account action information from videos of the same category can provide additional clues that help improve the discriminative nature of the snippet-features and the quality of the generated pseudo-labels.
Therefore, we design an inter-video interaction module that utilizes the correlation among videos to compact the snippet-features of the same category and make the features of different categories distinguishable.

First, we introduce a memory bank $\mathcal{M} \in \mathbb{R}^{C \times N \times D}$ to store the information from the entire dataset during training, where $C$ denotes the number of classes, $N$ indicates the number of stored snippets of each class, and D is the dimension number.
Initially, we use the classification head to predict the scores of the foreground candidates and select the snippets with the highest $N$ classification scores to initialize the memory $\mathcal{M}$ along with the scores. At $t$-th training iteration, we select $N$ snippet-features $\mathcal{F}^{(t)}_{[c]}$ with the high scores for each class to update the memory of last iteration $\mathcal{M}_{[c]}^{(t-1)}$. The process can be formulated as:

\begin{equation}
\mathcal{M}_{[c]}^{(t)} \leftarrow(1-\eta) \cdot \mathcal{M}_{[c]}^{(t-1)}+\eta \cdot \mathcal{F}_{[c]}^{(t)},
\end{equation}
where $\eta$ denotes the momentum coefficient. In order to boost the robustness, we exploit the momentum update strategy~\cite{9157636} to update memory $\mathcal{M}$, so $\eta$ is adjusted by: 
\begin{equation}
\eta=\eta_0 \cdot \log \left(\exp \left({e}/{E}\right)+1\right),
\end{equation}
where $\eta_0$ denotes the initial momentum coefficient, $e$ is the current epoch, $E$ denotes the total epoch,
% $\mathcal{F}^{M}$ is foreground features of the current sample 
and $c$ is the class index of the current snippet. Meanwhile, we use the temporal-level attention operation 
% \del{ on the mixed features $\tilde{\mathcal{F}}$ and the memory $\hat{\mathcal{M}}_{[c]}^{(t)}$ belong to the corresponding video-level class in the memory $\mathcal{M}$}
between the mixed features $\tilde{\mathcal{F}}$ in the intra-video interaction module and memory $\mathcal{M}_{[c]}^{(t)}$ to bring the class information of the entire dataset into $\tilde{\mathcal{F}}$, which can be formulated as:
% of the same video class in memory, 
%%\textcolor{red}{the description of memory and feature $F^{M}$ is not clear.}
\begin{equation}
%\begin{aligned}
%& Q=F W_{Q}, K=F^{g} W_{K}, V=F^{g} W_{V}; \\
\hat{\mathcal{F}} =\operatorname{softmax}(\tilde{\mathcal{F}} \odot (\mathcal{M}_{[c]}^{(t)})^{T}) \odot \mathcal{M}_{[c]}^{(t)}.
%\end{aligned}
\end{equation}

Finally, we get the output features $\tilde{\mathcal{F}}$ and $\hat{\mathcal{F}}$ from the video information interaction module. Then, we feed them to the classification head to output two TCAMs $\tilde{\mathcal{T}}$ and $\hat{\mathcal{T}}$, which are summed after to obtain ${\mathcal{T}^p}$ as the pseudo labels to supervise the learning of the base branch.     

\subsection{Training loss}
% We fed the output features of the information interaction module to the classification head to generate high-fidelity pseudo labels and

% We then utilize the knowledge distillation loss $\mathcal{L}_\mathrm{kd}$ in~\cite{Huang_2022_CVPR} to achieve the supervision process. So, 
Following previous methods, the whole learning process is jointly driven by video-level classification loss $\mathcal{L}_\mathrm{cls}$, 
% Affinity loss $\mathcal{L}_{\mathrm{aff}}$,
knowledge distillation loss $\mathcal{L}_\mathrm{kd}$ and attention normalization loss $\mathcal{L}_\mathrm{att}$~\cite{10.1007/978-3-030-58539-6_3}. 
The total loss function can be formulated as:
\begin{equation}
\mathcal{L}=\mathcal{L}_\mathrm{c l s}
% +\alpha \mathcal{L}_{\mathrm{aff}}
+\beta \mathcal{L}_\mathrm{k d}+\lambda\mathcal{L}_\mathrm{a t t},
\end{equation}
 % $\alpha,
where $\beta,\lambda$ denote trade-off factors. 
The knowledge distillation $\mathcal{L}_\mathrm{kd}$ in~\cite{Huang_2022_CVPR} is used to implement the process of ${\mathcal{T}^p}$ supervising $\mathcal{T}$ for training.
% The loss $\mathcal{L}_\mathrm{kd}$ can be formulated as：
The video-level classification loss is the combination of two losses calculated from the CA head and MIL head, which can be formulated as:
\begin{equation}
\mathcal{L}_\mathrm{cls}=\mathcal{L}_\mathrm{CA} + \theta\mathcal{L}_\mathrm{MIL},
\end{equation}
\noindent where $\theta$ is a hyper-parameter.
% \del{ and more details about the loss please refer to the supplementary. }
More details about each loss function here please refer to the supplementary.


\begin{table*}[ht]
% \renewcommand{\arraystretch}{0.95}
\setlength{\tabcolsep}{1.5mm}{
\centering
% \vspace{1.5mm}
\begin{tabular}{c|l|c|ccccccc|ccc}
\specialrule{0.1em}{1pt}{1pt}
\hline  
\multirow{2}{*}{Supervision} & \multirow{2}{*}{~Method} & \multirow{2}{*}{Publication} & \multicolumn{7}{c|}{mAP@IoU(\%)}               & AVG       & AVG       & AVG       \\ \cline{4-13} 
                             &                         &                              & 0.1  & 0.2  & 0.3  & 0.4  & 0.5  & 0.6  & 0.7  & (0.1:0.5) & (0.3:0.7) & (0.1:0.7) \\ \hline
\multirow{5}{*}{Full}        & S-CNN \cite{7780488}                  & CVPR 2016                    & 47.7 & 43.5 & 36.3 & 28.7 & 19.0 & 10.3 & 5.3  & 35.0      &   19.9        & 27.3      \\
                             & SSN \cite{8237579}                    & ICCV 2017                    & 66.0 & 59.4 & 51.9 & 41.0 & 29.8 & -    & -    & 49.6      & -         & -         \\
                             & TAL-Net \cite{8578222}                & CVPR 2018                    & 59.8 & 57.1 & 53.2 & 48.5 & 42.8 & 33.8 & 20.8 & 52.3       & 39.8      & 45.1         \\
                             % & BSN~\cite{Lin_2018_ECCV}                     & ECCV 2018                    & -    & -    & 53.5 & 45.0 & 36.9 & 28.4 & 20.0 & -         & 36.8         & -         \\
                             & GTAN \cite{8953536}                   & CVPR 2019                    & 69.1 & 63.7 & 57.8 & 47.2 & 38.8 & -    & -    & 55.3      & -         & -         \\ \hline 
\multirow{2}{*}{Weak*}       & STAR \cite{10.1609/aaai.v33i01.33019070}                   & AAAI 2019                    & 68.8 & 60.0 & 48.7 & 34.7 & 23.0 & -    & -    & 47.0      & -         & -         \\
                             & 3C-Net \cite{9008791}                 & ICCV 2019                    & 59.1 & 53.5 & 44.2 & 34.1 & 26.6 & -    & 8.1  & 43.5      & -         & -         \\ 
                             \hline
\multirow{18}{*}{Weak}       & STPN~\cite{8578804}                    & CVPR 2018                    & 52.0 & 44.7 & 35.5 & 25.8 & 16.9 & 9.9  & 4.3  & 35.0      & 18.5     & 27.0      \\
                             & RPN \cite{DBLP:conf/aaai/HuangHOW20a}                    & AAAI 2020                    & 62.3 & 57.0 & 48.2 & 37.2 & 27.9 & 16.7 & 8.1  & 46.5      & 27.6      & 36.8      \\
                             % & BaS-Net \cite{lee2020BaS-Net}                 & AAAI 2020                    & 58.2 & 52.3 & 44.6 & 36.0 & 27.0 & 18.6 & 10.4 & 43.6      & 27.3     & 35.3      \\
                             & DGAM  \cite{Shi_2020_CVPR}                  & CVPR 2020                    & 60.0 & 56.0 & 46.6 & 37.5 & 26.8 & 17.6 & 9.0  & 45.6      & 27.5      & 37.0      \\
                             & TSCN \cite{10.1007/978-3-030-58539-6_3}                   & ECCV 2020                    & 63.4 & 57.6 & 47.8 & 37.7 & 28.7 & 19.4 & 10.2 & 47.0      & 28.8      & 37.8      \\
                             & EM-MIL  \cite{DBLP:conf/eccv/LuoGSKWDX20}                 & ECCV 2020                    & 59.1 & 52.7 & 45.5 & 36.8 & 30.5 & 22.7 & \textbf{16.4} & 45.0      & 30.4      & 37.7      \\
                             & A2CL-PT  \cite{DBLP:conf/eccv/MinC20}               & ECCV 2020                    & 61.2 & 56.1 & 48.1 & 39.0 & 30.1 & 19.2 & 10.6 & 46.9      & 29.4      & 37.8      \\
                             & ACM-Net \cite{qu_2021_acmnet}                & TIP 2021                    & 68.9    & 62.7    & 55.0 & 44.6 & 34.6 & 21.8 & 10.8  & 53.2         & 33.4      & 42.6 \\
                             & UM  \cite{DBLP:conf/aaai/LeeWLB21}                    & AAAI 2021                    & 67.5 & 61.2 & 52.3 & 43.4 & 33.7 & 22.9 & 12.1 & 51.6      & 32.9      & 41.9      \\
                             & CoLA~\cite{zhang2021cola}                   & CVPR 2021                    & 66.2 & 59.5 & 51.5 & 41.9 & 32.2 & 22.0 & 13.1 & 50.3      & 32.1      & 40.9      \\
                             & AUMN \cite{AUMN}                   & CVPR 2021                    & 66.2 & 61.9 & 54.9 & 44.4 & 33.3 & 20.5 & 9.0  & 52.1      & 32.4      & 41.5      \\
                              & UGCT \cite{9577332}                   & CVPR 2021                    & 69.2 & 62.9 & 55.5 & 46.5 & 35.9 & 23.8 & 11.4  & 54.0      & 34.6      & 43.6      \\
                             & D2-Net  \cite{DBLP:conf/iccv/NarayanCHK0021}                & ICCV 2021                    & 65.7 & 60.2 & 52.3 & 43.4 & 36.0 & -    & -    & 51.5      & -         & -         \\
                             & FAC-Net \cite{Huang2021ForegroundActionCN}                & ICCV 2021                    & 67.6 & 62.1 & 52.6 & 44.3 & 33.4 & 22.5 & 12.7 & 52.0      & 33.1      & 42.2      \\
                             & DCC~\cite{Li_2022_CVPR}                     & CVPR 2022                    & 69.0 & 63.8 & 55.9 & 45.9 & 35.7 & 24.3 & 13.7 & 54.1      & 35.1      & 44.0      \\
                             & RSKP~\cite{Huang_2022_CVPR}                    & CVPR 2022                    & 71.3 & 65.3 & 55.8 & 47.5 & 38.2 & 25.4 & 12.5 & 55.6      & 35.9      & 45.1      \\
                             & ASM-Loc ~\cite{he2022asm}                & CVPR 2022                    & 71.2 & 65.5 & 57.1 & 46.8 & 36.6 & 25.2 & 13.4 & 55.4      & 35.8      & 45.1      \\ 
                              % & DELU~\cite{mengyuan2022ECCV_DELU} &ECCV 2022 & 71.5 & 66.2 & 56.5 & 47.7 & 40.5 & \textbf{27.2} & 15.3 & 56.5      & 37.4      & 46.4 \\
                             \cline{2-13} 
                             & \textbf{Ours}                    & -                            &  \textbf{72.4}    &  \textbf{66.9}    &   \textbf{58.4}     &  \textbf{49.7}   &  \textbf{41.8}    & \textbf{25.5}     &  12.8    &  \textbf{57.8}  &  \textbf{37.6}       & \textbf{46.8}     \\ 
                                                    \hline     
                                                    \specialrule{0.1em}{1pt}{1pt}
\end{tabular}
\caption{Comparison with state-of-the-art methods on THUMOS14 dataset.The AVG columns show average mAP under IoU thresholds of 0.1:0.5, 0.3:0.7 and 0.1:0.7. * indicates the methods use extra information. The best results are highlighted in bold.} 
\label{tab:1}  

}
% \vspace{-0.5em}
\end{table*}


\section{Experiments}
\subsection{Datasets and Evaluation Metrics. }
We conduct our experiments on the two commonly-used benchmark datasets, including THUMOS14~\cite{THU} and AcitivityNet v1.3~\cite{7298698}. Following the general weak-supervised setting, we only use the video-level category labels in the training process.

\textbf{THUMOS14} includes 200 untrimmed validation videos and 212 untrimmed test videos, where videos are collected from 20 action categories. Following the previous work ~\cite{inproceedingsU,he2022asm,Huang2021ForegroundActionCN}, we use the validation videos to train our model and test videos for evaluation.

\textbf{ActivityNet v1.3} contains 10,024 training videos, 4,926 validation videos, and 5,044 testing videos of 200 action categories.
Following~\cite{DBLP:conf/aaai/LeeWLB21,Huang_2022_CVPR}, we use the training videos to train our model and validation videos for evaluation.

\textbf{Evaluation metrics.} 
We evaluate the performance of our method with the standard evaluation metrics: mean average precise (mAP) under different intersection over union (IoU) thresholds. For THUMOS14 dataset, we report the mAP under thresholds IoU=\{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7\}. For ActivityNet v1.3 dataset, we report the mAP under thresholds [0.5:0.05:0.95]. At the same time, we also calculate the average mAP for different IoU ranges on the two datasets.
 
\subsection{Implementation Details}
We implement our model with the PyTorch framework and train the model with Adam optimizer~\cite{2014Adam}. The scaling factor $r$ is set to 4. The hyper-parameter $\theta, \beta,$ and $\lambda$ are set to 0.2, 1.0, and 0.1, respectively. The feature is extracted using the I3D~\cite{8099985}, which is pre-trained on the Kinetics-400~\cite{DBLP:journals/corr/KayCSZHVVGBNSZ17} dataset.
For THUMOS14 dataset, we train 180 epochs with a learning rate of 0.00005, the batch size is set to 10, $\sigma$ is set to 0.88, and $K$ is set to $\lfloor50\%*T\rfloor$, where T is the number of video snippets.
For ActivityNet v1.3 dataset, we train 100 epochs with a learning rate of 0.0001, the batch size is set to 32, $\sigma$ is set to 0.9, and $K$ is set to $\lfloor90\%*T\rfloor$. During the test phase, we use the base branch and intra-video information interaction module to obtain the action instance proposals.


\subsection{Comparison with State-of-the-Art Methods}
%----------------------------------------------------------------------
\noindent
\textbf{THUMOS14}. We first compare our method with state of the arts (SoTAs) on THUMOS14 dataset. These SoTAs contain fully-supervised methods~\cite{7780488, 8237579,8578222,Lin_2018_ECCV,8953536} and weak-supervised methods~\cite{10.1609/aaai.v33i01.33019070,9008791,8578804,DBLP:conf/aaai/HuangHOW20a,lee2020BaS-Net,Shi_2020_CVPR,10.1007/978-3-030-58539-6_3,DBLP:conf/eccv/LuoGSKWDX20,DBLP:conf/eccv/MinC20,qu_2021_acmnet,DBLP:conf/aaai/LeeWLB21,zhang2021cola,AUMN,9577332,DBLP:conf/iccv/NarayanCHK0021,Huang2021ForegroundActionCN,Li_2022_CVPR,Huang_2022_CVPR,he2022asm} and the results are shown in Table~\ref{tab:1}. We can clearly observe that our proposed model outperforms the state-of-the-art weakly-supervised temporal action localization methods.
% under different mAP@IoU.
% \del{different}\rev{a large range of} mAP@IoU \note{0.7 is not the best}, \del{~\emph{i.e.,}}\del{Furthermore,} 
Our proposed method reaches 57.8 at average mAP for IoU thresholds 0.1:0.5, 37.6 at average mAP for IoU thresholds 0.3:0.7, and 46.8 at average mAP for IoU thresholds 0.1:0.7. 
Meanwhile, our result can reach 41.8 at mAP@0.5.
The reasons for the improved performance stem from 1) our method uses the affinity relationships between snippet-features to generate coarse labels and then considers contextual information, which improves the separability between foreground and background snippet-features; 2) we introduce additional action clues to fully leverage the relationships between videos in the training data, improving the discriminative nature of the snippet-features. Thus, generating more high-fidelity pseudo labels can significantly improve the performance of action localization.


\noindent
\textbf{ActivityNet v1.3}. Table~\ref{tab:2} shows the evaluation results in terms of mAP@IoU on ActivityNet v1.3 dataset. From the table, our model achieves competitive performance compared to other SoTAs. In addition, our method achieve 25.8 for average mAP, which is 0.7 higher than ASM-Loc, demonstrating the superiority of our method.
% weak-supervised temporal action localization methods, demonstrating the superiority of our method.

\begin{table}[ht]
\renewcommand{\arraystretch}{1.055}
\setlength{\tabcolsep}{1.6mm}{
\centering
% \vspace{-1.5mm}
\begin{tabular}{l|c|cccc}
\specialrule{0.1em}{1pt}{1pt}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Publication} & \multicolumn{4}{c}{mAP@IoU(\%)} \\ \cline{3-6} 
                        &                              & 0.5    & 0.75   & 0.95  & AVG   \\ \hline
STPN~\cite{8578804}                    & CVPR 2018                    & 29.3   & 16.9   & 2.6   & 16.3  \\
CMCS~\cite{8953341}                  & CVPR 2019                    & 34.0   & 20.9   & 5.7   & 21.2  \\
BaS-Net~\cite{lee2020BaS-Net}                 & AAAI 2020                    & 34.5   & 22.5   & 4.9   & 22.2  \\
TSCN~\cite{10.1007/978-3-030-58539-6_3}                   & ECCV 2020                    & 35.3   & 21.4   & 5.3   & 21.7  \\
A2CL-PT~\cite{DBLP:conf/eccv/MinC20}                 & ECCV 2020                    & 36.8   & 22.0   & 5.2   & 22.5  \\
% ACM-Net                 & TIP 2021                     & 37.6   & 24.7   &\textbf{6.5}   & 24.4  \\
TS-PAC~\cite{9578220}                 & CVPR 2021                    & 37.4   & 23.5   & 5.9   & 23.7  \\
UGCT~\cite{9577332}                   & CVPR 2021                    & 39.1   & 22.4   & 5.8   & 23.8  \\
AUMN~\cite{AUMN}                  & CVPR 2021                    & 38.3   & 23.5   & 5.2   & 23.5  \\
FAC-Net~\cite{Huang2021ForegroundActionCN}                 & ICCV 2021                    & 37.6   & 24.2   & 6.0   & 24.0  \\
DCC~\cite{Li_2022_CVPR}                     & CVPR 2022                    & 38.8   & 24.2   & 5.7   & 24.3  \\
RSKP~\cite{Huang_2022_CVPR}                    & CVPR 2022                    & 40.6   & 24.6   & 5.9   & 25.0  \\
ASM-Loc~\cite{he2022asm}                  & CVPR 2022                    & \textbf{41.0}   & 24.9   & 6.2   & 25.1  \\ \hline
Ours                    &    -                          & 39.4       & \textbf{25.8}       & \textbf{6.4}      &  \textbf{25.8}    \\ \hline
\specialrule{0.1em}{1pt}{1pt}
\end{tabular}
\caption{Comparison with state-of-the-art methods on ActivityNet v1.3 dataset. The AVG column shows the averaged mAP under the IoU thresholds [0.5:0.05:0.95].}
\label{tab:2}}
\vspace{-0.5em}
\end{table}
%-----------------------------------------------------------------------------------

\subsection{Ablation Study}
We conduct ablation studies to demonstrate the impact of different components in our method on THUMOS14 dataset. 

\noindent
\textbf{Impact of Affinity Inference Module.}
To find a proper function in Eq.\eqref{eq:2}, we explore several strategies to calculate the affinity between each pair of neighbor snippets, including cosine distance, $\text{L}_1$ distance, and $\text{L}_2$ distance,
and the results are reported in Table~\ref{tab:3}. The results show that $\text{L}_2$ distance can achieve higher mAP than cosine distance, and $\text{L}_1$ distance yields the best results compared to other methods, so we adopt it as the default \textit{diff} function. 


\begin{table}[t]
\renewcommand{\arraystretch}{1.0}
\setlength{\tabcolsep}{2.4mm}{
    \centering
        \begin{tabular}{l|ccccc}
        \specialrule{0.1em}{1pt}{1pt}
        \hline
        \multirow{2}{*}{Method} & \multicolumn{5}{c}{mAP@IoU($\%$)} \\ \cline{2-6} 
                                & 0.1           & 0.3           & 0.5           & 0.7           & AVG           \\ \hline
        cosine distance       & 68.6          & 53.5          & 36.6          & 12.3          & 43.3          \\
        $L_2$ distance          & 70.7          & 55.5          & 36.6          & 12.3          & 44.2          \\
        $L_1$ distance     & \textbf{72.4} & \textbf{58.4} & \textbf{41.8} & \textbf{12.8} & \textbf{46.8} \\ \hline
        \specialrule{0.1em}{1pt}{1pt}
        \end{tabular}
    }
    \caption{Ablation studies about different strategies of detecting foreground on THUMOS14 dataset. The AVG column shows the averaged mAP under the IoU thresholds [0.1:0.1:0.7].}
        \label{tab:3}
\vspace{-0.5em}
\end{table}



%-------------------------------------
\noindent
\textbf{Impact of Information Interaction Module.}
Primarily, we evaluate the effect of the two information interaction modules. The results are presented in Table~\ref{tab:4}. We set the base branch as \textit{Base} and progressively add the intra-video module and inter-video modules to Base, and the performances are continuously improved by 3.4\% and 6.3\% for average mAP.


\begin{table}[t]
\renewcommand{\arraystretch}{1.0}
\setlength{\tabcolsep}{2.mm}{
    \centering
    \begin{tabular}{l|ccccc}
    \specialrule{0.1em}{1pt}{1pt}
    \hline
    \multirow{2}{*}{Method} & \multicolumn{5}{c}{mAP@IoU($\%$)} \\ \cline{2-6} 
                                    & 0.1           & 0.3           & 0.5           & 0.7           & AVG           \\ \hline
    Base       & 62.7          & 45.5          & 29.3          & 10.4          & 37.1          \\
    Base + intra          & 66.3          & 49.7          & 32.6         & 11.3          & 40.5          \\
    Base + intra + inter       & \textbf{72.4} & \textbf{58.4} & \textbf{41.8} & \textbf{12.8} & \textbf{46.8} \\ 
    \hline
    \specialrule{0.1em}{1pt}{1pt}
    \end{tabular}
    \caption{The effects of intra-video and inter-video components in the information interaction module on THUMOS14 dataset.}
    \label{tab:4}
}
\end{table}


%---------------------------------
\begin{table}[t]
\renewcommand{\arraystretch}{1.0}
\setlength{\tabcolsep}{2.6mm}{
    \centering
    \begin{tabular}{l|ccccc}
    \specialrule{0.1em}{1pt}{1pt}
    \hline
    \multirow{2}{*}{Method} & \multicolumn{5}{c}{mAP@IoU(\%)} \\ \cline{2-6} 
                            & 0.1    & 0.3    & 0.5   & 0.7   & AVG   \\ \hline   
    self                    & 65.8   & 48.1   & 30.6   & 10.9  &  39.2      \\
    w/o fore                    & 49.9   & 34.8   & 20.3  & 5.9  &  27.6  \\
    w/o back                    & 71.7   & 56.9   & 39.9   & 12.4  & 45.9       \\
    fore + back               & 71.0   & 54.1   & 35.0   & 10.3  &  43.1       \\
    dynamic mixing   &  \textbf{72.4}     &   \textbf{58.4}        &  \textbf{41.8}       &  \textbf{12.8}         & \textbf{46.8}       \\ 
    temporal  & 71.4  & 57.4   & 39.5   & 12.8 & 46.0 \\
    \hline
    \specialrule{0.1em}{1pt}{1pt}
    \end{tabular}
    \caption{The effect of different components in intra-video information interaction module on THUMOS14 dataset.}
        \label{tab:5}
    }    
\end{table}
%----------------------------------------

%-------------------------------------
\begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{3.9mm}{
        \begin{tabular}{c|cc}
        \specialrule{0.1em}{1pt}{1pt}
        \hline
        Dataset  & Affinity  & Ours \\ \hline
        THUMOS14       & 50.12      & 95.95   \\ 
        ActivityNet v1.3  & 70.52    & 116.88     \\ \hline
        \specialrule{0.1em}{1pt}{1pt}
        \end{tabular}
        \caption{Statistics results on the average number of action snippets correctly predicted by different modules on the whole dataset, where affinity denotes affinity inference module.}
                \label{tab:6}
    }
\end{table}
%-------------



%-------------------------------------
\begin{table}[t]
\renewcommand{\arraystretch}{1.0}
\setlength{\tabcolsep}{2.mm}{
    \centering
    \begin{tabular}{l|ccccc}
    \specialrule{0.1em}{1pt}{1pt}
    \hline
    \multirow{2}{*}{Method} & \multicolumn{5}{c}{mAP@IoU(\%)} \\ \cline{2-6} 
                                                    & 0.1 & 0.3   & 0.5   & 0.7  & AVG   \\ \hline
    direct update
    & 71.9  & 58.1   & 40.3   & 12.3 & 46.4 \\
    momentum update                            & 71.0   & 55.7   & 37.4  & 11.3  & 44.5\\\hline
    \textbf{Ours}   &  \textbf{72.4}     &   \textbf{58.4}        &  \textbf{41.8}       &  \textbf{12.8}         & \textbf{46.8}       \\ \hline
    \specialrule{0.1em}{1pt}{1pt}
    \end{tabular}
    \caption{The effect of different memory update strategies on THUMOS14 dataset. \textit{AVG} denotes the averaged mAP under the IoU thresholds [0.1:0.1:0.7].} 
    \label{tab:8}
}
    
% \vspace{-1.3em}
\end{table}




We then evaluate the impact of different variants of intra-video information interaction. The results are reported in Table~\ref{tab:5}, in which
%We experiment by changing the way the information interacts within the class, 
1) \textit{self} denotes temporal attention between video snippet-features $\mathcal{F}$ and itself;
% We make the video self-attention between its own features, which define as SA.
2) \textit{w/o fore} and \textit{w/o back} denote removing the foreground and background snippet-features, respectively;
3) \textit{fore + back} denotes directly adding the two types of features together; 
4) \textit{dynamic mixing} denotes using dynamic mixing operation to fuse two types of snippet-features; 
5) \textit{temporal} denotes enhancing video snippet-features only using the temporal attention.
We can observe that 1) the foreground snippet-features have a significant influence on the performance, remove which will lead to a significant drop in the performance; 2) directly adding background snippet-features will impair the final performance; 3) dynamic mixing is much more effective, which can assist the utilizing of the information in background snippet-features;4) attention operations at both channel level and temporal level can better enhances the discriminative nature of snippet-features.

To show the effect of our proposed information interaction module quantitatively, we count the average number of foreground snippets inferred correctly by the affinity inference module and that refined by the information interaction module. The statistical results are shown in Table~\ref{tab:6}. Due to the neglect of global video structure information, assigning snippets according to affinity can only produce a relatively low number of correct inferences. However, the addition of our information interaction module (denoted as \textit{Ours} in the Table) significantly increases the accuracy of inferences, indicating that integrating the inter- and intra-video relationship is an effective way to reduce the limitations of the affinity inference module and boost the accuracy of temporal localization.

\noindent
\textbf{Impact of Memory Update Strategies.} We explore the impact of different memory update strategies in the inter-video information interaction module. 
The evaluation results are shown in Table~\ref{tab:8}. We evaluate two variants of memory update strategy, \emph{i.e.}, only using the high-confidence foreground snippet-features to direct update memory, and only using the momentum update strategy to update memory.
From the table, we can see that our method obtains better performance than only using the momentum update strategy, because directly utilizing the momentum update strategy will include many noisy features and impair the learning of intra-video relation. The results indicate that our method is effective in incorporating more action information, compared with the direct update strategy.


\begin{figure*}[htbp]
\begin{center}
    \setlength{\fboxrule}{0pt}
    \setlength{\fboxsep}{0cm}
    % \vspace{-2.5mm}
    \fbox{\rule{0pt}{0in} \rule{.0\linewidth}{0pt}
    \hspace{-0.3mm}
    % \includegraphics[width=0.98\linewidth, height=0.39\textwidth]
    \includegraphics[width=0.95\linewidth]{result3.pdf}}
\end{center}
% \vspace{-2.5mm}
\caption{Qualitative comparisons of our method, Base branch, and ASM-Loc on THUMOS14 dataset. We show the action scores on two examples of “Diving” on the left and “TennisSwing" on the right.} 
% The black dashed box indicates the area where other methods misjudged.}
\label{fig:3}
\vspace{-1em}
\end{figure*}


\begin{figure}[t]
\begin{center}
\setlength{\fboxrule}{0pt}
\setlength{\fboxsep}{0cm}
% \vspace{-2.5mm}
\fbox{\rule{0pt}{0in} \rule{.0\linewidth}{0pt}
\hspace{-3.0mm}
\includegraphics[width=0.98\linewidth]{feature4.pdf}}
    \end{center}
\vspace{-2.8mm}
    \caption{T-SNE visualization of foreground and background embedding features. We compare the Base Branch and our method on example “CliffDiving" from THUMOS14 dataset, where blue points indicate the backgrounds and red points mean the foregrounds.}
\label{fig:4}
\end{figure}


\begin{figure}[t]
\begin{center}
\setlength{\fboxrule}{0pt}
\setlength{\fboxsep}{0cm}
% \vspace{-2.5mm}
\fbox{\rule{0pt}{0in} \rule{.0\linewidth}{0pt}
\hspace{-2.3mm}
     \includegraphics[width=0.98\linewidth]{featurelei.pdf}}
    \end{center}
\vspace{-2.5mm}
    \caption{T-SNE visualization of feature space for THUMOS14 dataset. Different color points represent different categories.}
\label{fig:5}
\vspace{-1em}
\end{figure}


\begin{figure}[t]
\begin{center}
\setlength{\fboxrule}{0pt}
\setlength{\fboxsep}{0cm}
% \vspace{-2.5mm}
\fbox{\rule{0pt}{0in} \rule{.0\linewidth}{0pt}
\hspace{-3.3mm}
\includegraphics[width=1.0\linewidth]{difference.pdf}}
    \end{center}
\vspace{-2.8mm}
    \caption{Qualitative comparisons of the inferred action snippets. From top to bottom, we show the affinity between snippet-features and  the coarse labels inferred by the affinity inference module; the fine labels inferred by the information interaction module; the ground truth(GT). The darker color denotes the foreground snippets.}
\label{fig:6}
\vspace{-0.95em}
\end{figure}


\subsection{Qualitative results}
To help understand the effect of our proposed method, we present some qualitative results in the subsection. First, we show some cases selected from THUMOS14 dataset in Figure~\ref{fig:3}, and we observe that our method can locate more accurate action regions than the base branch and ASM-Loc~(black dashed boxes).
Meanwhile, we adopt t-SNE technology to project the embedding features of one video in THUMOS14 dataset into a 2-dimensional features space, and the results are shown in Figure~\ref{fig:4}. 
We observe that our method can accurately bring the embedding features of foregrounds together, and make them away from the background.
The visualization results validate the discriminative capability of the learned features and thus support the accurate estimated action localization results. 
Then, we project the snippet-features of different categories into the 2-dimensional features space through t-SNE, and the results are shown in Figure~\ref{fig:5}, which demonstrates that, compared to the base branch, our method can closely aggregate snippet-features with the same class together and separate different classes. 

To further verify the refinement ability of the video information interaction module, we selected the “SoccerPenalty” example from THUMOS14 dataset to visualize the affinity values inferred by the affinity inference module, the coarse labels generated according to the affinity, and the fine labels refined by the information interaction module. The results are shown in Figure~\ref{fig:6}, and we can see that the information interaction module can predict more accurate action snippets compared to the affinity inference module demonstrating its effectiveness.
Specifically, the affinity inference module predicts the background snippets in the black box with a large affinity value, which is actually caused by mutation of the background, leading to the wrong coarse label. The misidentified snippet is corrected by the information interaction module, which demonstrates its feasibility,
as well as proves that our information interaction module can improve the inference result of the affinity inference module to produce more accurate pseudo labels. Snippet in the orange box is predicted as the action, which is actually the junction part of the background and action.

\section{Conclusion}
In this paper, we propose a novel weakly-supervised TAL method by inferring snippet-feature affinity. An affinity inference module is designed to guide pseudo label generation by exploring the variation between snippet-features, and an information interaction module is introduced to generate high-fidelity pseudo labels to supervise model training through multi-level attention. Comprehensive experiments on two public benchmarks demonstrate the effectiveness and superiority of our proposed method.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\end{document}

