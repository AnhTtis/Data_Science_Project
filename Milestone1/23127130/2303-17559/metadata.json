{
    "arxiv_id": "2303.17559",
    "paper_title": "DDP: Diffusion Model for Dense Visual Prediction",
    "authors": [
        "Yuanfeng Ji",
        "Zhe Chen",
        "Enze Xie",
        "Lanqing Hong",
        "Xihui Liu",
        "Zhaoqiang Liu",
        "Tong Lu",
        "Zhenguo Li",
        "Ping Luo"
    ],
    "submission_date": "2023-03-30",
    "revised_dates": [
        "2023-03-31"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "We propose a simple, efficient, yet powerful framework for dense visual predictions based on the conditional diffusion pipeline. Our approach follows a \"noise-to-map\" generative paradigm for prediction by progressively removing noise from a random Gaussian distribution, guided by the image. The method, called DDP, efficiently extends the denoising diffusion process into the modern perception pipeline. Without task-specific design and architecture customization, DDP is easy to generalize to most dense prediction tasks, e.g., semantic segmentation and depth estimation. In addition, DDP shows attractive properties such as dynamic inference and uncertainty awareness, in contrast to previous single-step discriminative methods. We show top results on three representative tasks with six diverse benchmarks, without tricks, DDP achieves state-of-the-art or competitive performance on each task compared to the specialist counterparts. For example, semantic segmentation (83.9 mIoU on Cityscapes), BEV map segmentation (70.6 mIoU on nuScenes), and depth estimation (0.05 REL on KITTI). We hope that our approach will serve as a solid baseline and facilitate future research",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.17559v1"
    ],
    "publication_venue": null
}