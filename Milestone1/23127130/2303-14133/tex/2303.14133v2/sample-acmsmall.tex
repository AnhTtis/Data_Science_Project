\PassOptionsToPackage{table,xcdraw}{xcolor}
\documentclass[acmsmall]{acmart}

\usepackage{makecell}
\usepackage{multirow}
\usepackage[table]{xcolor}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{XXXXXXX.XXXXXXX}


%%
%% These commands are for a JOURNAL article.
\acmJournal{CSUR}
\acmVolume{XX}
\acmNumber{X}
%\acmArticle{XXX}
\acmMonth{4}


\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Survey on Adversarial Attack and Defense for Medical Image Analysis: Methods and Challenges}


\author{Junhao Dong}
\email{dongjh8@mail2.sysu.edu.cn}
\orcid{0000-0002-6232-9157}

\author{Junxi Chen}
\email{chenjx353@mail2.sysu.edu.cn}
\orcid{0009-0000-2512-4438}

\author{Xiaohua Xie}
\email{xiexiaoh6@mail.sysu.edu.cn}
\orcid{0000-0002-0310-4679}
\authornote{Corresponding author.}

\author{Jianhuang Lai}
\email{stsljh@mail.sysu.edu.cn}
\orcid{0000-0003-3883-2024}
\affiliation{%
	\institution{School of Computer Science and Engineering, Sun Yat-sen University, and Guangdong Province Key Laboratory of Information Security Technology}
	%\streetaddress{No. 132, Waihuan East Road}
	\city{Guangzhou}
	%\state{Guangdong}
	\country{China}
	%\postcode{510006}
}

\author{Hao Chen}
\authornotemark[1]
\affiliation{%
	\institution{Department of Computer Science and Engineering, Department of Chemical and Biological Engineering, and Division of Life Science, $\!\!$ Hong Kong University of Science and Technology}
	% \streetaddress{Clear Water Bay}
	%	\city{}
	\country{$\!$Hong Kong,$\!$ China}}
\email{jhc@cse.ust.hk}
\orcid{0000-0002-8400-3780}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Dong et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Deep learning techniques have achieved superior performance in computer-aided medical image analysis, yet they are still vulnerable to imperceptible adversarial attacks, resulting in potential misdiagnosis in clinical practice. Oppositely, recent years have also witnessed remarkable progress in defense against these tailored adversarial examples in deep medical diagnosis systems. In this exposition, we present a comprehensive survey on recent advances in adversarial attacks and defenses for medical image analysis with a systematic taxonomy in terms of the application scenario. We also provide a unified framework for different types of adversarial attack and defense methods in the context of medical image analysis. For a fair comparison, we establish a new benchmark for adversarially robust medical diagnosis models obtained by adversarial training under various scenarios. To the best of our knowledge, this is the first survey paper that provides a thorough evaluation of adversarially robust medical diagnosis models. By analyzing qualitative and quantitative results, we conclude this survey with a detailed discussion of current challenges for adversarial attack and defense in medical image analysis systems to shed light on future research directions. Code is available on \href{https://github.com/tomvii/Adv_MIA}{\color{red}{GitHub}}.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
	<ccs2012>
	<concept>
	<concept_id>10010147.10010257.10010293.10010294</concept_id>
	<concept_desc>Computing methodologies~Neural networks</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10002978.10003029</concept_id>
	<concept_desc>Security and privacy~Human and societal aspects of security and privacy</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10010405.10010444</concept_id>
	<concept_desc>Applied computing~Life and medical sciences</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Neural networks}
\ccsdesc[500]{Security and privacy~Human and societal aspects of security and privacy}
\ccsdesc[500]{Applied computing~Life and medical sciences}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Adversarial machine learning, Medical image analysis, Deep learning, Adversarial example, Evaluation}

\received{15 April 2024}
\received[revised]{12 July 2024}
\received[accepted]{19 October 2024}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}
%\vspace{-3mm}
\label{sec:1}
Driven by the success of Deep Neural Networks (DNNs) in natural image processing tasks \cite{he2016deep, wang2020deep}, they have also been demonstrated to have expert-level performance for various medical imaging tasks, including but not limited to skin lesion diagnosis \cite{ge2017skin}, diabetic retinopathy detection \cite{gondal2017weakly}, and tumor segmentation \cite{pereira2016brain}. Among these medical applications, Artificial Intelligence (AI)-based diabetic retinopathy detection system was the first that was approved for marketing by the US Food and Drug Administration \cite{FDA2018}. In clinical practice, deep learning-driven diagnosis models can save the overall cost of manual work and even improve patient outcomes by early detection \cite{alzubaidi2021role}.

% Adversarial examples XXX, which hinders real-world | Medical even worse 
Although deep learning has emerged as a promising technique for fundamental research in multiple disciplines \cite{dong2021visually, zemskova2022deep}, it still suffers from adversarial examples \cite{SzegedyZSBEGF13}, which can induce a catastrophic disruption to DNNs. Generally, the adversary can be obtained by adding a visually imperceptible perturbation to the legitimate example, which makes it easy to bypass the manual check \cite{carlini2017adversarial}. The existence of such tailored examples becomes one of the major hindrances to practically applying DNNs in safety-critical scenarios, \textit{e.g.} medical image analysis \cite{paschali2018generalizability_A1, finlayson2019adversarial_A3, ma2021understanding_D10}. In particular, the adversarial vulnerability can be even worse for diagnosis systems, which may result in misdiagnosis, insurance fraud, and even a crisis of confidence in AI-based medical technology \cite{shah2018susceptibility_A4, finlayson2019adversarial_A3}. Moreover, the diagnosis system is complex and unlikely to update, which can be difficult to imagine how these adversarial attacks could be operationalized. Recent works have revealed the adversarial vulnerability of diagnosis models under various scenarios. Consequently, adversarial robustness has been considered a new measurement for the security of medical applications.

% Defending against AE for medical: reason + methods | 引出现有方法缺少统一评估，来motivates us to conduct a comprehensive survey, develop a powerful baseline
Considering the huge healthcare economy and pervasive medical fraud, numerous studies have been conducted to defend against these malicious examples in medical imaging. Generally, early research efforts focus on adversarial training for improving network robustness \cite{ren2019brain_D11, vatian2019impact_D13, zhou2021ssmd_D41} or adversarial detection to identify adversarial examples \cite{ma2021understanding_D10, park2020robustification_D19, watson2021attack_D7}. Besides, a small fraction of works incorporate image-level pre-processing \cite{xu2022medrdf_D2, liu2020defending_D23} or feature enhancement \cite{taghanaki2019kernelized_D20, xue2019improving_D22} into the adversarial defense framework. The family of adversarial defense methods has been demonstrated to be effective in establishing adversarial robustness for unified pattern recognition in medical diagnosis \cite{li2021defending_D1, xu2021towards_D9}. However, there exists a considerable gap in the research-oriented setting and also evaluation between various defense methods, inducing difficulties in comparisons. This thus motivates us to construct a systematical survey, developing the benchmark implementation and evaluation for the most effective adversarial defense approach, \textit{a.k.a.} adversarial training.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.6\linewidth]{imgs/Papers_per_year.pdf}
	\vspace{-6mm}
	\caption{Number of publications per year related to adversarial attack and defense for medical image analysis, inclusive of data up to September 2024.}
	\vspace{-6mm}
	\label{fig:1}
\end{figure}

% Compared to existing surveys + line graph (Attack&Defense)
Several survey papers \cite{sipola2020model, apostolidis2021survey, shamshiri2022security, kaviani2022adversarial} have so far summarized the adversarial attack and defense for medical image analysis. However, many of them only focus on a certain medical task, e.g., Coronavirus 2019 (COVID-19) analysis \cite{shamshiri2022security}. Otherwise, these surveys do not provide a detailed taxonomy or a comprehensive evaluation of existing attack and defense methods for computer-aided diagnosis models. Furthermore, recent advances in adversarial attack and defense for medical image analysis systems have not been covered in these surveys. In this paper, we aim to address these gaps and provide a taxonomic overview of recent advances in adversarial attack and defense for medical image analysis with a discussion of their benefits and limitations. Although a considerable number of adversarial attack and defense methods emerge every year in the field of medical image analysis (see Fig. \ref{fig:1}), there still remains a lack of unified and fair measurement for various defense methods. Hence, we construct a benchmark evaluation of adversarial training methods for future development. The main contributions of this work lie in the following aspects:

\begin{itemize}
	\item We provide a comprehensive review of adversarial attack and defense methods in the field of medical image analysis, including a family of attack and defense methods with a systematic taxonomy based on the application scenario.
	\item We establish a unified framework for different types of adversarial attack and defense approaches with respect to diverse medical imaging tasks.
	\item To the best of our knowledge, this is the first survey that establishes a benchmark for adversarially robust diagnosis models under various scenarios. We further present a systematic analysis of adversarial attack and defense in the context of medical images.
	\item We identify current challenges and provide insightful guidance for future research in adversarial attack and defense for medical image analysis.
\end{itemize}

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.93\linewidth]{imgs/Survey_structure_Fig.pdf}
	\vspace{-4mm}
	\caption{Outline of our Survey: Starting with the background of adversarial machine learning and medical image analysis, we comprehensively review recent advancements in medical adversarial attack and defense mechanisms. We also evaluate the robustness of medical diagnosis models against adversarial attacks and conclude with a discussion on current challenges, providing insights into potential research directions.}
	\vspace{-4mm}
	\label{fig:new-2}
\end{figure}

A pivotal motivation for focusing on adversarial learning within medical image analysis is the distinct nature of medical images compared to natural images, which necessitates specific modifications in both attack and defense mechanisms. This divergence is rooted in several unique characteristics of medical images: \textbf{1. Modality-Specific Characteristics}: Medical images, derived from varied modalities such as MRI, CT, X-ray, and ultrasound, possess unique imaging attributes including intensity distributions and channel configurations. These attributes demand tailored adversarial strategies that account for modality-specific nuances rather than general approaches used for natural images. \textbf{2. Limited and Imbalanced Datasets}: The scarcity and imbalance typical of medical datasets—stemming from privacy concerns, the rarity of certain conditions, and the high costs of data acquisition—pose distinct challenges. These challenges exacerbate the risk of overfitting and adversarial manipulation, necessitating robust and specialized defensive techniques. \textbf{3. Clinical Significance and Ethical Considerations}: The direct impact of medical image analysis on patient care prioritizes the need for accuracy and reliability. The severe consequences of adversarial attacks in this context not only heighten the stakes but also introduce profound ethical considerations, driving the need for exceptionally robust defense mechanisms. \textbf{4. Anatomical Structures and Domain-Specific Knowledge}: The consistent presence of specific anatomical structures across individuals introduces additional layers of complexity. Adversarial attacks can exploit these structures to induce subtle yet impactful misdiagnoses, while defenses must enhance their focus on clinically relevant regions to ensure diagnostic accuracy.

% In the following sections
The remainder of this survey is structured as follows: We start with a unified framework to formally define both medical adversarial attack and defense in Section \ref{sec:2}. Next, we give a comprehensive review of adversarial attack techniques targeting medical machine learning systems with a taxonomy based on the attack scenarios (Section \ref{sec:3}). In Section \ref{sec:4}, we summarize and categorize existing adversarial defense methods in the context of medical image analysis. Then, we present benchmark evaluation results of medical attack and defense algorithms in various settings (Section \ref{sec:5}). Lastly, we discuss current challenges and future research directions in Section \ref{sec:6}. A tree diagram is also presented in Figure \ref{fig:new-2} to summarize our survey and aid reader comprehension.

\vspace{-3mm}
\section{Background}
\vspace{-0.6mm}
\label{sec:2}
% Deep learning various security hazards including poison/model inversion/... + Adversarial attack is the most harmful(悄无声息) Attack的危害
Although the deep learning paradigm has made significant breakthroughs in numerous AI fields, it is still vulnerable to various security threats. Among them, adversarial attack has attracted the most attention from the community of deep learning security, as it raises a series of potential safety issues to the application of deep learning. Apart from disrupting the inference stage of DNNs, adversarial attacks can easily bypass the manual check, since the adversarial example is visually similar to its clean counterpart. This insidious security threat can further be magnified for computer-aided diagnosis models, which may result in catastrophic misdiagnosis and even a crisis of social confidence \cite{finlayson2019adversarial_A3}. In this section, we formally define adversarial attack and defense for medical image analysis and highlight the vulnerability of medical images to adversarial attacks.

We first clarify the notations and definitions used in this survey. Considering a specific dataset $(\mathbf{x}, y) \sim \mathcal{D}$ where $\mathcal{D}$ is a data distribution over pairs of given examples $\mathbf{x}$ and their corresponding labels $y$. We denote $f_{\boldsymbol{\theta}} (\cdot)$ as the deep learning-based medical analysis model with network parameter $\boldsymbol{\theta}$. Generally, adversarial examples $\mathbf{\hat{x}}$ are typically created by appending an imperceptible noise $\boldsymbol{\delta}$ to clean examples $\mathbf{x}$, which can be formally defined as below: 
\begin{equation}
	\begin{aligned}
		\mathbf{\hat{x}} := \mathbf{x} + \boldsymbol{\delta} \textrm{~~with~} f_{\boldsymbol{\theta}} (\mathbf{\hat{x}}) \neq y \textrm{~and~} d(\mathbf{x}, \mathbf{\hat{x}}) \leq \epsilon,
	\end{aligned}
	\label{eq:1}
\end{equation}
where $d(\cdot, \cdot)$ is the distance metric, and $\epsilon$ is the maximum allowed perturbation bound for imperceptibility. By definition, adversarial examples $\mathbf{\hat{x}}$ need to be close to their legitimate counterparts $\mathbf{x}$ under a certain distance metric, e.g., $\ell_{p}$ distance. In other words, the adversarial perturbation is $\ell_{p}$-norm bounded as $\left\| \boldsymbol{\delta} \right\|_{p} \leq \epsilon$. In this survey, we mainly focus on attacks under $\ell_{\infty}$-norm threat model, which can further be formulated as the following optimization problem:
\begin{equation}
	\begin{aligned}
		\max\limits_{\left\| \boldsymbol{\delta} \right\|_{\infty} \leq \epsilon}\mathcal{L}\left( f_{\boldsymbol{\theta}} \left( \mathbf{x}+\boldsymbol{\delta}\right) , y \right) ,
		\label{eq:2}
	\end{aligned}
\end{equation}
where $\mathcal{L}$ primarily depends on a certain task (such as the cross-entropy loss for classification). Normally, the above-mentioned optimization problem can be solved by quasi-Newton methods \cite{SzegedyZSBEGF13, carlini2017towards} or gradient descent-based algorithms. Specifically, we can obtain the worst-case example (or the strongest adversarial example) by iterative Projected Gradient Descent (PGD) \cite{MadryMSTV18} on the negative loss function with step size $\alpha$, as follows:
\begin{equation}
	\begin{aligned}
		\mathbf{\hat{x}}^{t+1} = \Pi_{\mathbb{B}(\mathbf{x}, \epsilon)} \left(  \mathbf{\hat{x}}^{t} + \alpha \cdot \operatorname{sign} \left( \nabla_{\mathbf{\hat{x}}^{t}}\mathcal{L}\left( \mathbf{\hat{x}}^{t}, y \right)  \right)  \right),
		\label{eq:3}
	\end{aligned}
\end{equation}
where $\mathbf{\hat{x}}^{t}$ represents the $t^\text{th}$ iteration update, and $\mathbb{B}(\mathbf{x}, \epsilon)$ denotes $\ell_{\infty}$-norm bound with radius $\epsilon$ around clean examples $\mathbf{x}$. Note that adversarial examples can be generated by numerous types of methods, including the Limited-memory BFGS method \cite{SzegedyZSBEGF13} and Fast Gradient Sign Method (FGSM) \cite{GoodfellowSS14}. Instead of the aforementioned white-box attack that can access the full knowledge of the target model, there also exist some other threat models, e.g., unrestricted attack \cite{chen2024content}, black-box attack \cite{bai2023query, ma2021finding}, which poses a more significant security issue to computer-aided diagnosis models in real-world scenario \cite{paschali2018generalizability_A1, bms2022analysis_A36}. Note that the aforementioned adversarial attack framework can be generalized to diverse tasks by merely altering the target loss function $\mathcal{L}$. We will discuss further details of all these threat models for medical image analysis in Section \ref{sec:3}.

Numerous types of defense methods have been proposed to enhance the robustness of diagnosis models against adversarial examples \cite{asgari2018vulnerability_D17, li2020robust_D24, tripathi2020fuzzy_D4}. Among them, adversarial training \cite{GoodfellowSS14, MadryMSTV18} has received the greatest attention, which can improve the robustness via augmenting adversaries as training data. The adversarially trained model is expected to correctly predict both clean and adversarial examples during the inference stage. Specifically, the standard adversarial training \cite{MadryMSTV18} can be extended based on Equation (\ref{eq:2}) as the following min-max optimization problem:
\begin{equation}
\small
	\begin{aligned}
		\min\limits_{\boldsymbol{\theta}} \mathbb{E}_{\left( \mathbf{x}, y\right)\sim \mathcal{D} }\left[ \max\limits_{\left\| \boldsymbol{\delta} \right\|_{\infty} \leq \epsilon}\mathcal{L}\left( f_{\boldsymbol{\theta}} \left( \mathbf{x}+\boldsymbol{\delta}\right) , y \right)  \right].
		\label{eq:4}
	\end{aligned}
\end{equation}
The inner maximization is to search for the worst-case adversarial examples to disrupt the target network. The outer minimization mainly focuses on optimizing empirical adversarial risk over network parameter $\theta$. Generally, adversarial training improves the intrinsic robustness of DNNs while preserving their performance on clean inputs during the inference stage, which can be improved by integrating additional regularization modules \cite{terzi2021adversarial, zhou2022improving, xu2022infoat, yu2022understanding, zhou2022modeling}.

Besides enhancing the robustness against adversaries, several defense methods focus on data pre-processing (both clean and adversarial examples) without affecting the subsequent diagnosis systems \cite{xu2022medrdf_D2, kansal2022defending_D51}. Generally, data pre-processing aims at transforming adversaries into their benign versions for the subsequent inference while making clean inputs remain unchanged, achieving superior generalization ability against unforeseen adversaries \cite{yoon2021adversarial, zhou2021removing, dai2022deep, zhou2021towards, zhou2023eliminating, lee2023robust, sun2023critical}. We can thus formulate a pre-processing-based defense as the following optimization:
\begin{equation}
	\small
	\begin{aligned}
		\min\limits_{\psi} \mathbb{E}_{\left( \mathbf{x}, y\right)\sim \mathcal{D} }\left[ \mathcal{L}\left( f_{\boldsymbol{\theta}} \left( \psi\left( \mathbf{x}+\boldsymbol{\delta}\right) \right) , y \right) + \lambda \cdot \mathcal{L}\left( f_{\boldsymbol{\theta}} \left( \psi\left( \mathbf{x}\right) \right) , y \right) \right].
		\label{eq:5}
	\end{aligned}
\end{equation}
where $\lambda$ is the weighting factor, and $\psi$ denotes the pre-processing module, which can be a parametric or non-parametric operator to mitigate the effect of adversarial perturbations. However, discriminative regions in medical images usually occupy only a few pixels. Thus, the pre-processing methods in the context of medical images still suffer from a higher risk of discriminative feature loss than that of natural images.

Different from medical adversarial defense methods that aim at inferencing adversaries correctly, some works focus on the detection of input adversarial examples \cite{watson2021attack_D7, li2020robust_D24, yang2022defense_D67}. Adversarial detection can thus be regarded as a binary classification task to distinguish legitimate and adversarial examples. The main objective of adversarial detection is formalized as the following optimization: 
\begin{equation}
	\small
	\begin{aligned}
		\min\limits_{\boldsymbol{\omega}} \mathbb{E}_{\left( \mathbf{x}, y\right)\sim \mathcal{D} }\left[ \mathcal{L}_\text{CE}\left( f_{\boldsymbol{\omega}} \left(  \mathbf{x}+\boldsymbol{\delta} \right) , 1 \right) + \beta \cdot \mathcal{L}_\text{CE}\left( f_{\boldsymbol{\omega}} \left(  \mathbf{x}\right) , 0 \right) \right].
		\label{eq:6}
	\end{aligned}
\end{equation}
where $f_{\boldsymbol{\omega}}$ is the binary classifier with parameter $\boldsymbol{\omega}$ for adversarial detection, and $\mathcal{L}_\text{CE}$ represents the cross-entropy loss. $\beta$ is the weighting factor to determine the focus of detection models. The adversarial detector is desired to correctly distinguish legitimate examples (0) and their adversarial counterparts (1). Note that adversarial detection can be either parametric (learnable) or non-parametric. Other than adversarial training and pre-processing-based defense, adversarial detection mainly focuses on detecting adversarial examples in advance but not robust inference. Note that adversarial detection is effective for all medical imaging tasks by establishing an input-level defense. Similarly, adversarial training can also be generalized to different medical imaging tasks by rebuilding its target loss function. We will further discuss the details of these adversarial defense methods and some other types of approaches in Section \ref{sec:4}.

Numerous adversarial attack and defense methods have been demonstrated to yield excellent performance for natural images \cite{aldahdooh2022adversarial}. However, there exist fundamental differences between natural vision tasks and medical imaging tasks in terms of data sizes, features, and task patterns. Therefore, it is difficult to transfer adversarial attack and defense methods for natural images directly to the medical domain. In addition, several works have shown that medical images can even suffer from more severe adversarial attacks than natural images \cite{finlayson2019adversarial_A3, ma2021understanding_D10, yao2021hierarchical_A15, rasaee2021explainable_A44}. Considering the massive healthcare industry and the significant impact of computer-aided diagnosis, it is necessary to pay close attention to the security and reliability of computer-aided diagnosis models. Furthermore, we emphasize that medical diagnosis demands high robustness and explainability, which are crucial for maintaining social trust in medical AI applications. Mistakes induced by adversarial attacks can lead to serious consequences, including patient harm and erosion of confidence in automated systems. Hence, we summarize this survey to highlight the significance of focusing on adversarial attacks and defenses for medical image analysis, a domain where the stakes are exceptionally high.

Substantial efforts have also been made to understand and explain the existence of adversarial examples \cite{GoodfellowSS14, ilyas2019adversarial, ZhangGL000S022}. Departing from the initial discovery of adversarial vulnerabilities in deep learning-based models \cite{SzegedyZSBEGF13}, Goodfellow \textit{et al.} \cite{GoodfellowSS14} proposed that the linear nature of neural networks, despite their inherent non-linearities, contributes significantly to their susceptibility to adversarial perturbations. This perspective suggests that small, carefully crafted perturbations can lead to significant changes in the output due to linear behavior in high-dimensional spaces. Building upon this, Ilyas \textit{et al.} \cite{ilyas2019adversarial} introduced the notion that adversarial examples are not merely bugs but arise from non-robust yet useful features that models exploit from the data. They further showed that adversarial perturbations, while disruptive, are a result of the model's capacity to leverage any available signal to maximize performance on the training data, suggesting that these features are not bugs but intrinsic to the dataset. Further advancing the theoretical understanding, Zhang \textit{et al.} \cite{ZhangGL000S022} made the first step in a causal perspective for understanding and mitigating adversarial examples. They show the significance of the correlation between style variables and ground-truth labels by constructing the causal graph based on adversary generation.

In the context of medical image analysis, efforts to explain adversaries have also been undertaken. Studies such as \cite{bortsova2021adversarial_A18, ma2021understanding_D10, watson2021attack_D7} have empirically explored various factors that contribute to the generation of adversarial examples against deep diagnostic models. These works delve into the specific vulnerabilities of medical imaging systems, considering factors like data distribution, model architecture, and the unique characteristics of medical images that may influence diagnosis models.

\vspace{-3mm}
\section{Medical Adversarial Attacks}
\label{sec:3}
\textbf{Medical adversarial attacks} are designed to deliberately generate adversarial examples that compromise the integrity of medical diagnosis models during the inference stage. These attacks pose significant challenges to the reliability and safety of computer-aided diagnosis systems, which are increasingly reliant on deep learning-based models. The primary incentive for conducting these attacks could be to assess the robustness of these diagnosis systems or, in a more adversarial scenario, to manipulate medical outcomes. We further establish a comprehensive taxonomy of adversarial attacks tailored to medical image analysis, guiding readers through the various attack modalities that mirror real-world threats. We categorize these into four main types based on the attacker's knowledge and access levels. Each category involves distinct methodologies and presents unique challenges in terms of detection and mitigation. By analyzing these categories modularly, we can better understand how adversarial attacks can be tailored to specific medical image processing tasks and scenarios, providing a clearer picture of both the vulnerabilities exposed by these attacks and the potential defense mechanisms in the context of medical image analysis.

% 第一段介绍这个section说了哪些东西 (图说明医疗对抗样本放在实验部分给)

\begin{figure}[t]
	\centering
	\includegraphics[width=0.6\linewidth]{imgs/Attack_Taxonomy.pdf}
	\vspace{-3mm}
	\caption{Taxonomy of medical adversarial attacks in terms of application scenarios. Following \cite{chen2017zoo, dong2022restricted}, we categorize adversarial attack methods into four classes according to the degrees of accessible knowledge, including Backward Propagation (BP) gradients of target DNN during the training and inference stage of the adversary generator. ``Query'' represents the accessibility to outputs of the target DNN.}
	\vspace{-4mm}
	\label{fig:2}
\end{figure}

\vspace{-3mm}
\subsection{Taxonomy of Adversarial Attacks}
% subsection taxonomy 介绍
In the past few years, various methods have been proposed to generate adversarial examples against computer-aided diagnosis models. According to several studies on the motivations for conducting medical adversarial attacks \cite{finlayson2019adversarial_A3, levy2022security_A27}, we consider the financial gain can be the biggest incentive to attack healthcare systems, as health insurance frauds have grown on a large scale over the years \cite{villegas2021fourteen}. The attacker might try to deliberately cause misdiagnosis for AI-based medical reimbursement systems to make an erroneous decision. In addition, the adversarial vulnerability of the computer-aided diagnosis mechanism can even be involved in terrorism and unfair competition in the future. Most importantly, a growing number of attacks against medical image diagnosis will induce a crisis of confidence in the autonomous AI diagnostic system. Existing attack methods concentrate on different vulnerabilities of DNNs and also diverse medical imaging tasks. More importantly, they are designed in terms of application scenarios (degrees of accessible knowledge), as shown in Fig. \ref{fig:2}. Consistent with the taxonomy of adversarial attacks for natural images in a general setting \cite{chen2017zoo, pitropakis2019taxonomy, dong2022restricted}, we categorize all medical attack methods into four classes (order by accessible knowledge to target model in decreasing order): white-box attack, semi-white-box (gray-box) attack, black-box attack, and restricted black-box (no-box) attack.  

In addition to the taxonomy of adversarial attacks for diverse model-accessible scenarios, we here also delineate an alternative taxonomy based on the adversary generation (optimization) strategies for adversarial attack. We now categorize adversary generation strategies into four main classes: 

\noindent\textbf{(1) Gradient-based optimization.}
In a white-box setting, this approach leverages gradient information to derive corresponding adversarial perturbations/transformations. Previous studies in medical adversarial attacks have predominantly employed (iterative) gradient ascent to maximize objective functions \cite{finlayson2019adversarial_A3, liu2019robustifying_A31, morshuis2022adversarial_A26}. Such a gradient-based optimization technique can further be extended to a universal (image-agnostic) setting, which means that a set of examples are possible to be disrupted by a universal adversarial perturbation \cite{cheng2020adversarial_A2, hirano2021universal_A7_D55, minagi2022natural_A19}.

\noindent\textbf{(2) Zeroth-order optimization.}
The gradient information is generally inaccessible in the black-box setting, while the only accessible information is the corresponding outputs of the target model. Thus, a possible way is to approximate the gradient of the target model via a classic type of derivative-free optimization method\textemdash the zeroth-order method using differences of function values. This was initially applied to the black attacks against natural images \cite{chen2017zoo}, and then extended to medical adversarial attacks in the black-box setting \cite{byra2020adversarial_A11, li2022query_A50}.

\noindent\textbf{(3) Evolutionary optimization.}
Evolutionary optimization typically utilizes bio-inspired algorithms to efficiently solve optimization problems where the underlying functions are not explicitly known or are too complex to model directly \cite{nguyen2012evolutionary}. Such an optimization strategy is thus applied to select the optimal perturbation of one pixel in the context of natural images \cite{su2019one} and medical images \cite{tsai2023adversarial_A2023_3} under the white-box scenario. Specifically in the context of black-box medical adversarial attacks, Cui \textit{et al.} \cite{cui2021deattack_A48} further designed a differential evolution attack against medical image segmentation models in the black-box setting. 

\noindent\textbf{(4) Deep generative models.}
In addition to adversary generation through optimization, a series of methods have been proposed to conduct one-stage adversary generation via deep generative models for natural images \cite{xiao2018generating, jandial2019advgan++}. By learning the medical-specific domain knowledge, such generative models can also be used to produce visually undetectable adversarial perturbations corresponding to the original medical images \cite{allyn2020adversarial_A6, wang2021adversarial_A37}. The generated adversaries can obtain a terribly high attack success rate on diverse computer-aided diagnosis tasks.

% 第二段介绍四种攻击方式的区别
Under the white-box scenario, the attacker has full information about the target model, which can also access inference outputs of the target DNN unlimited times. Nonetheless, the semi-white-box (gray-box) setting \cite{xiao2018generating} mainly concentrates on constructing a generative model to produce adversaries against the target DNN. Generally, the gray-box attackers train the adversary generator in the white-box setting, while they do not require the accessibility of the target DNN during the inference (adversary generation) stage. The standard black-box adversarial attack can only access the DNN output (probabilities) using a limited number of queries. In contrast, the restricted black-box (no-box) attack does not need even a single-time query to the inaccessible DNN. 

% 第三段介绍四种攻击方式的影响并且引出后续四个段落
Both white-box and gray-box settings are ideal for the medical adversarial attack, which rely heavily on the accessibility of prior knowledge about the target DNN. Specifically, the attackers focus on the backward gradients to update adversaries. Black-box attacks concentrate on zeroth-order optimization according to numerous times of query outputs of the target DNN. In comparison, no-box attacks highly depend on the transferability of adversaries among different models. Note that Black-box and no-box scenarios are close to computer-aided diagnosis systems in practice, which poses a more significant threat to model deployment. In the following sections, we will survey recent advances in these four types of adversarial attacks for medical image analysis.

\vspace{-3mm}
\subsection{Summary of Medical Attack Works}
\label{sec:3_Sum}
To facilitate future research activities, we present an overview of adversarial attack works for medical image analysis with a detailed taxonomy in Table \ref{tab:1}. Particularly, we include attack methods designed for natural images and medical images (in \textbf{bold}). We list adversarial attack methods tailored for natural images that are extended to these medical works, including L-BFGS \cite{SzegedyZSBEGF13}, CW attack \cite{carlini2017towards}, DeepFool \cite{moosavi2016deepfool}, Universal Adversarial Perturbation (UAP) \cite{moosavi2017universal}, Jacobian Saliency Map Attack (JSMA)\cite{papernot2016limitations}, Zeroth Order Optimization (ZOO) \cite{chen2017zoo}, One-Pixel Attack (OPA) \cite{su2019one}, etc.

\begin{table*}[t!]
	\centering
	\tiny 
	\renewcommand{\arraystretch}{0.7}
	\caption{Summary of adversarial attack works in the context of medical image analysis (time ascending). The newly proposed adversarial attack methods tailored for medical images are in \textbf{bold}.}
	\vspace{-4mm}
	\label{tab:1}
	\rowcolors{2}{gray!0}{gray!8}
	\resizebox{\linewidth}{!}{  
		
		\begin{tabular}{|c|c|c|c|c|c|}
			%			\toprule
			\hline
			Reference & Year & Task &  Attack scenario  &  Method  &  Data Modality\\ 
			\hline
			
			\cite{paschali2018generalizability_A1} & 2018 & Classification, Segmentation &  White-box, No-box  & FGSM, DeepFool, JSMA & MRI, Dermoscopy \\ 
			
			\cite{finlayson2019adversarial_A3} & 2018 & Classification &  White-box, No-box  & PGD, AdvPatch &\begin{tabular}[c]{@{}c@{}}Fundoscopy \\ X-ray, Dermoscopy \end{tabular}  \\ 
			
			\cite{shah2018susceptibility_A4} & 2018 & Classification &  No-box  & FGSM & Fundoscopy \\ 
			
			\cite{liu2019robustifying_A31} & 2019 & Segmentation &  White-box  & FGSM, I-FGSM, TI-FGSM & MRI \\ 
			
			\cite{chen2019intelligent_A13} & 2019 & Segmentation &  White-box  & \textbf{Multi-task VAE} & CT \\ 
			
			\cite{ozbulak2019impact_A12} & 2019 & Segmentation &  White-box  & \textbf{Adaptive Segmentation Mask Attack} & Fundoscopy, Dermoscopy \\
			
			\cite{kovalev2019influence_A5} & 2019 & Classification &  White-box  & PGD & X-ray, Histology \\
			
			\cite{rao2020thorough_A45_D60} & 2020 & Classification &  White-box, No-box  & \begin{tabular}[c]{@{}c@{}}FGSM, PGD, MI-FGSM, \\ DAA, DII-FGSM \end{tabular} & X-ray  \\
			
			\cite{cheng2020adversarial_A43} & 2020 & Classification &  White-box, No-box  & \textbf{Adversarial Exposure Attack} & Fundoscopy  \\
			
			\cite{rahman2020adversarial_A22} & 2020 & Classification, Object detection &  White-box, Gray-box, No-box  & \begin{tabular}[c]{@{}c@{}}FGSM, DeepFool, CW,  \\ BIM, L-BFGS, PGD, JSMA \end{tabular}  & CT, X-ray \\
			
			\cite{byra2020adversarial_A11} & 2020 & Classification &  Black-box  & ZOO & Ultrasound \\ 
			
			\cite{yao2020miss_A34} & 2020 & Landmark detection &  White-box  & \textbf{Adaptive Targeted I-FGSM} & MRI, X-ray  \\
			
			\cite{yoo2020outcomes_A32} & 2020 & Classification &  White-box, No-box  & FGSM & Fundoscopy \\ 
			
			\cite{cheng2020adversarial_A2} & 2020 & Segmentation &  White-box  & UAP & MRI \\ 
			
			\cite{allyn2020adversarial_A6} & 2020 & Classification &  White-box  & Generative model & Dermoscopy \\ 
			
			\cite{gongye2020new_A24} & 2020 & Classification &  White-box  & FGSM, PGD & X-ray \\ 
			
			\cite{hirano2021universal_A7_D55} & 2021 & Classification &  White-box  & UAP & OCT, X-ray, Dermoscopy \\ 
			
			\cite{joel2021adversarial_A33} & 2021 & Classification &  White-box  & FGSM, BIM, PGD & CT, MRI, X-ray  \\
			
			\cite{chen2021adversarial_A30} & 2021 & Segmentation &  White-box  & \textbf{IND and OOD Attacks} & MRI \\ 
			
			\cite{QiGS0Z21_A17} & 2021 & Classification, Segmentation &  White-box  & \textbf{Stabilized Medical Attack} & CT, Endoscopy, Fundoscopy \\ 
			
			\cite{bortsova2021adversarial_A9} & 2021 & Segmentation &  White-box, No-box  & PGD & X-ray \\
			
			\cite{kovalev2021biomedical_A51} & 2021 & Classification &  White-box, No-box  & CW & CT, X-ray, Microscopy  \\  
			
			\cite{pal2021vulnerability_A8} & 2021 & Classification &  White-box  & FGSM & CT, X-ray \\ 
			
			\cite{shao2021target_A16} & 2021 & Segmentation &  White-box  & \textbf{Multi-scale Attack} & Fundoscopy, Dermoscopy \\ 
			
			\cite{tian2021bias_A14} & 2021 & Classification &  White-box, No-box  & \textbf{Adversarial Bias Field Attack} & X-ray \\ 
			
			\cite{foote2021now_A39} & 2021 & Classification &  White-box  & PGD, UAP & Microscopy  \\ 
			
			\cite{yilmaz2021assessment_A10} & 2021 & Classification &  White-box  & FGSM & X-ray \\ 
			
			\cite{kulkarni2021kryptonite_A28} & 2021 & Classification &  White-box  & \textbf{Kryptonite Attack} & MRI, Dermoscopy \\ 
			
			\cite{gougeh2021adversarial_A23} & 2021 & Classification &  White-box  & FGSM, PGD, CW, ST & X-ray \\
			
			\cite{koga2021simple_A29} & 2021 & Classification &  Black-box  & \textbf{Black-box UAP} & X-ray, Dermoscopy, Fundoscopy \\ 
			
			\cite{rasaee2021explainable_A44} & 2021 & Classification &  White-box  & I-FGSM & Ultrasound  \\ 
			
			\cite{yao2021hierarchical_A15} & 2021 & Classification &  White-box  & \textbf{Hierarchical Feature Constraint} & X-ray, Fundoscopy \\ 
			
			\cite{bortsova2021adversarial_A18} & 2021 & Classification &  No-box  & FGSM, PGD & X-ray, Fundoscopy,  Microscopy \\
			
			\cite{diyasa2021grasping_A38} & 2021 & Classification &  White-box  & \begin{tabular}[c]{@{}c@{}}FGSM, BIM, CW, RFGSM \\ PGD, FAB, DeepFool, SparseFool \end{tabular} & Microscopy  \\
			
			\cite{cui2021deattack_A48} & 2021 & Segmentation &  Black-box  & \textbf{Differential Evolution Attack} & CT, Dermoscopy, Ultrasound  \\ 
			
			\cite{zhou2021machine_A46} & 2021 & Classification &  Gray-box  & \textbf{GAN-based Adversary Generator} & X-ray  \\
			
			\cite{wang2021adversarial_A37} & 2022 & Classification &  Black-box  & \textbf{AmdGAN} & \begin{tabular}[c]{@{}c@{}}CT, OCT, X-ray, Fundoscopy,  \\ Dermoscopy, Ultrasound, Microscopy \end{tabular}    \\
			
			\cite{levy2022security_A27} & 2022 & Classification, Segmentation &  White-box  & \begin{tabular}[c]{@{}c@{}} \textbf{Modified FGSM with} \\ \textbf{with tricks to break defences} \end{tabular} & \begin{tabular}[c]{@{}c@{}}CT, MRI, X-ray,  \\ Dermoscopy, Fundoscopy \end{tabular}  \\ 
			
			\cite{minagi2022natural_A19} & 2022 & Classification &  White-box  & UAP & X-ray, Fundoscopy, Dermoscopy \\
			
			\cite{patel2022predictive_A49} & 2022 & Classification &  White-box, No-box  & \textbf{Attention-based I-FGSM} & CT  \\ 
			
			\cite{kwon2022advu_A35} & 2022 & Segmentation &  White-box  & FGSM & Microscopy  \\
			
			\cite{de2022evaluation_A40} & 2022 & Classification &  White-box  & FGSM & X-ray  \\ 
			
			\cite{apostolidis2022digital_A21} & 2022 & Classification &  No-box  & \textbf{Digital Watermarking} & CT, MRI, X-ray \\
			
			\cite{wei2022analysis_A42} & 2022 & Classification &  White-box, No-box  & \begin{tabular}[c]{@{}c@{}}FGSM, BIM, PGD \\ \textbf{No-sign Operation} \end{tabular} & X-ray  \\
			
			\cite{selvakkumar2022addressing_A25} & 2022 & Classification &  White-box  & FGSM & Fundoscopy \\
			
			\cite{ahmed2022failure_A47} & 2022 & Classification &  White-box  & FGSM, PGD, CW & CT, Dermoscopy, Microscopy  \\
			
			\cite{li2022query_A50} & 2022 & Segmentation &  Black-box  & \textbf{Improved Adaptive Square Attack} & X-ray  \\ 
			
			\cite{bms2022analysis_A36} & 2022 & Classification &  Black-box  & FGSM, BIM, PGD, MI-FGSM & CT, Fundoscopy  \\ 
			
			\cite{morshuis2022adversarial_A26} & 2022 & Reconstruction &  White-box  & \begin{tabular}[c]{@{}c@{}}\textbf{Adversarial k-space Noise},  \\ \textbf{Adversarial Rotation} \end{tabular} & MRI \\ 
			
			\cite{bharath2022analysis_A41} & 2022 & Classification &  White-box  & FGSM, L-BFGS & Fundoscopy  \\ 
			
			\cite{wang2022feature_A20} & 2022 & Classification &  Gray-box  & \textbf{Feature Space-Restricted Attention Attack} & X-ray, Fundoscopy, Dermoscopy \\ 
			
			\cite{yao2023adversarial_A2023_1} & 2023 & Classification & White-box & \textbf{Hierarchical Feature Constraint} & X-ray, Fundoscopy \\
			
			\cite{ding2023vith_A2023_2} & 2023 & Classification & White-box, No-box & \textbf{Vision Transformer Hashing} & X-ray, Dermoscopy \\
			
			\cite{tsai2023adversarial_A2023_3} & 2023 & Classification & White-box & OPA, \textbf{Differential Evolution Attack} & OCT, X-ray, Dermoscopy \\
			\cite{chen2023frequency_A2023_4} & 2023 & Classification & White-box, No-box & \textbf{Frequency Constraint-based Attack} & X-ray, Ultrasound \\
			\cite{li2023threat_A2023_5} & 2023 & Classification & White-box & FGSM & CT \\
			\cite{lee2024adversarial_A2024_1} & 2024 & Classification & White-box & \textbf{Dynamic Adaptive Instance Normalization} & CT \\
			\cite{chen2024rae_A2024_2} & 2024 & \makecell{Classification, Segmentation, \\ Object detection} & No-box & \textbf{Visible Watermark Perturbation} & \makecell{MRI, X-ray, Fundoscopy, \\ Dermoscopy, Microscopy} \\
			%			\bottomrule
			\hline
		\end{tabular}
	}
	\vspace{-6mm}
\end{table*}

\vspace{-3mm}
\subsection{White-box Attacks}
% 本段宗旨
Most works of adversarial attack for medical image analysis focus on the white-box scenario. In particular, these works primarily consider the adversarial vulnerability of computer-aided diagnosis models in various medical imaging tasks with full knowledge of medical DNNs. Specifically, the attacker can treat the target diagnosis DNN as a locally deployed model during adversary generation.

% 每段介绍一类（几个）工作
To the best of our knowledge, Paschali \textit{et al.} \cite{paschali2018generalizability_A1} were the first to systematically evaluate the white-box attack against a variety of medical imaging models of several medical tasks, including skin lesion classification and whole brain segmentation. Concretely speaking, they transfer adversarial attack methods for natural images \cite{GoodfellowSS14, moosavi2016deepfool, papernot2016limitations} directly to medical images, which poses an underlying threat to modern diagnosis DNN models. In the meantime, the attack success rate can be further reduced when attackers are in the face of deeper diagnosis models.

Generally, white-box adversarial attacks are conducted by solving a certain optimization problem. Nevertheless, Allyn \textit{et al.} \cite{allyn2020adversarial_A6} leveraged a generative model to produce visually undetectable adversarial perturbations corresponding to original input images. The generated adversarial examples can obtain a terribly high attack success (fooling) rate to dermoscopic image recognition systems, presenting a significant risk of misdiagnosis in real-world scenarios.

In addition to generating the adversarial perturbation corresponding to a certain medical image, several works \cite{hirano2021universal_A7_D55, foote2021now_A39, minagi2022natural_A19} consider obtaining a universal perturbation that is adaptive to a large proportion of medical images from a certain dataset, causing a security hole in the computer-aided clinical diagnosis. Hirano \textit{et al.} \cite{hirano2021universal_A7_D55} conducted a universal adversarial attack for medical image classification under both targeted and non-targeted scenarios. Note that the main goal of the targeted attack is to enable adversaries to be misclassified as a specified target class. Oppositely, non-targeted attacks just focus on making DNN models output wrong results. They discovered that non-targeted adversarial attacks can achieve better transferability than targeted attacks. 

% 专门医疗分类的白盒攻击方法
Numerous researchers spare no effort to design custom adversarial attack methods \cite{tian2021bias_A14, yao2021hierarchical_A15, QiGS0Z21_A17, kulkarni2021kryptonite_A28, yao2023adversarial_A2023_1, lee2024adversarial_A2024_1} to better adapt to medical imaging tasks. By theoretically investigating the vulnerability of medical image representations, Yao \cite{yao2021hierarchical_A15} designed a novel hierarchical feature constraint as auxiliary guidance to hide the adversarial representations in the clean feature domain, which can further be a plug-and-play module for existing attack methods to reduce the risk of being detected. Considering various modalities of medical images, Qi \textit{et al.} \cite{QiGS0Z21_A17} proposed a new medical adversary generation method by optimizing a well-defined objective function that is composed of a deviation loss term and a stabilization loss term. Specifically, the deviation aims at enlarging the prediction gap between adversarial outputs and their corresponding ground truth. The stabilization term can be regarded as a regularization to constrain adversarial perturbations to low variance, which avoids the local optima induced by instance-wise noise during optimization. 

% 分割
Apart from white-box adversarial attacks for medical classification tasks, there also exist various researchers that explore the vulnerabilities of other medical imaging tasks. The majority of these works concentrate on medical segmentation \cite{chen2019intelligent_A13, ozbulak2019impact_A12, cheng2020adversarial_A2, bortsova2021adversarial_A9, shao2021target_A16, chen2021adversarial_A30, kwon2022advu_A35}. Ozbulak \textit{et al.} \cite{ozbulak2019impact_A12} started the first attempt to expose the adversarial vulnerability of medical segmentation tasks by designing the Adaptive Segmentation Mask Attack (ASMA) method. The proposed ASMA incorporates the adaptive segmentation mask and the dynamic perturbation multiplier to generate targeted adversaries. Beyond the superior disruption against various DNNs, the authors also demonstrated the generalizability of the proposed method with different distance metrics for adversarial perturbations. Based on feature-level ASMA attack \cite{ozbulak2019impact_A12}, Shao \textit{et al.} \cite{shao2021target_A16} further incorporated multi-scale gradients to generate adversarial perturbations to biomedical image segmentation models. The above-mentioned medical segmentation attacks mainly depend on in-distribution adversarial examples. In contrast, the proposed method in \cite{chen2021adversarial_A30} considers out-of-distribution adversarial attacks against the lumbar disk shape reconstruction problem. Specifically, PGD \cite{MadryMSTV18} is applied to optimize the out-of-distribution perturbation to bypass defense methods.

% 目标检测 Landmark检测
In addition, Yao \textit{et al.} \cite{yao2020miss_A34} proposed an Adaptive Targeted Iterative FGSM (ATI-FGSM) with a comprehensive evaluation to study the vulnerability of multiple landmark detection systems. ATI-FGSM aims at moving a cohort of landmarks by dynamically assigning a weight for the loss term of a specific landmark in each iteration, facilitating the convergence of adversary generation. In addition, Rahman et al. \cite{rahman2020adversarial_A22} examined the adversarial robustness of nine deep diagnosis applications for COVID-19 detection, which demonstrates their extreme susceptibility to adversarial attacks.

% 重建
Apart from adversarial attacks against medical pattern recognition systems, a recent work \cite{morshuis2022adversarial_A26} investigated the adversarial vulnerability of Magnetic Resonance (MR) image reconstruction from k-space data. In addition to appending adversarial perturbation directly to k-space measurements, the authors also consider a visually slight rotation during the acquisition to obtain the adversarial effect, which can also be optimized by the iterative PGD approach.

\vspace{-4mm}
\subsection{Semi-white-box Attacks}
\vspace{-1mm}
Semi-white-box (Gray-box) attacks have been widely explored for natural images \cite{xiao2018generating, wang2019gan, jandial2019advgan++}. However, there remain rare academic works to explore this attack scenario for medical image analysis \cite{rahman2020adversarial_A22, zhou2021machine_A46, wang2022feature_A20}. Generally, the semi-white-box adversarial attack involves two stages: 1) the attacker trains a generative model to produce adversarial examples against the target DNN model. During the training process, the attacker has full access to the target model, including gradients of backward propagation. 2) During the application stage, the adversary generator can directly obtain adversarial examples against the target model with the input of legitimate images, which do not requires any information about the target model as in the totally black-box scenario.

% 讲2篇半白盒的
Generative Adversarial Networks (GANs) have also been shown to have superior performance in generating highly plausible adversarial images \cite{xiao2018generating, wang2019gan, jandial2019advgan++, dong2022restricted}. To investigate the safety issues for AI-based computer-aided diagnosis of breast cancer on digital mammograms, Zhou \textit{et al.} \cite{zhou2021machine_A46} developed two GAN-based models at two different resolutions, which can synthesize highly plausible adversarial images to further induce a wrong diagnosis of breast cancer. The authors also pointed out another potential problem that GAN-based adversarial attack methods can also be trained using external medical data, as mammography is the widely available imaging modality in clinical practice. Recently, the  Feature-Space-Restricted Attention Attack \cite{wang2022feature_A20} is proposed to efficiently generate adversarial examples for various medical modalities with less visual perturbation. Specifically, the authors added the feature-level restriction to make sure that adversarial examples are close to the classification boundary in the feature space. Furthermore, an attention mechanism constraint was also proposed to regularize the image-level perturbations to focus on the lesion area, which incorporates class-wise attention information into the adversarial perturbation generation.

%\vspace{-3mm}
\subsection{Black-box Attacks}

% define black-box attack 5 篇工作 后续分两段 分类3篇 分割2篇
Existing white-box adversarial attack mainly requires multiple backward gradients of target models. In other words, the attacker treats the target DNN as the locally deployed model to generate corresponding adversarial examples. However, the white-box setting can be unreliable in the real-world scenario, as it needs complete knowledge of the DNN model to attack. In comparison, the general black-box scenario can be a more suitable setting to simulate practical adversarial attacks. Numerous works have been proposed to explore black-box attacks for natural images \cite{chen2017zoo, ilyas2019prior, andriushchenko2020square, yatsura2021meta}. On account of the inaccessibility to backward gradients (or first-order information) of the target DNN, black-box adversarial attacks primarily resort to the zeroth order optimization for the gradient estimation, which requires querying the output probabilities of the target DNN with numerous times. There also exist several studies that focus on the black-box adversarial attack in the context of medical image analysis \cite{byra2020adversarial_A11, wang2021adversarial_A37, bms2022analysis_A36, cui2021deattack_A48, li2022query_A50}.

To explore the practical security threat against medical image analysis tasks, Byra \textit{et al.} \cite{byra2020adversarial_A11} devised a black-box adversarial attack approach that is specific to ultrasound image reconstruction. In comparison to existing attack methods that manipulate pixels of medical images, the proposed adversarial perturbation mainly concentrates on the reconstruction parameters. Moreover, the parameter-level attack is conducted for each radio-frequency data frame separately based on zeroth-order optimization \cite{chen2017zoo}. On account of the difficulty in accessing the target diagnosis model during the black-box scenario, Wang \textit{et al.} \cite{wang2021adversarial_A37} employed the knowledge distillation method to learn a GAN-based model to generate adversarial examples. Specifically, the distillation model aims at simulating the inference outputs of target black-box DNN with multiple queries. Subsequently, the proposed generator can efficiently produce adversarial examples based on any clean inputs against the targeted diagnosis model.

Considering the scarcity of adversarial attacks against medical image segmentation, Cui \textit{et al.} \cite{cui2021deattack_A48} proposed a Differential Evolution Attack (DEAttack) against medical image segmentation models in the black-box setting with only tampering a few pixels of medical images. Moreover, the DEAttack remains a much higher efficiency in creating adversarial examples than directly applying the differential evolution algorithm via incorporating pre-selected sensitive regions and colors from original medical images. The authors also demonstrated that the DEAttack can be conducted in the black-box setting by only compensating the modification for 1\% of pixels of medical images. Furthermore, Li \textit{et al.} \cite{li2022query_A50} improved the square attack by designing a more accurate gradient estimation for better convergence, which efficiently alters the predictions of foreground pixels with a few queries to the medical segmentation model. A learnable variance of the adaptive distribution is also enabled to limit sampling regions to a small area for improved efficiency.


\vspace{-3mm}
\subsection{Restricted Black-box (No-box) Attacks}
The above-mentioned methods either rely on having the whole knowledge of the targeted diagnosis model or require multiple queries to the black-box model. However, the attacker might not be able to access the target diagnosis model directly in most real-world situations. Particularly, the restricted black-box (no-box) setting can effectively represent the hardest (worst) case for the practical adversarial attack, which even do not require querying the target black-box DNN. The no-box attack mainly depends on the transferability \cite{LiuCLS17, ilyas2019adversarial, demontis2019adversarial} of adversarial examples among diverse DNN models. For instance, the no-box attacker can craft adversarial images based on a locally deployed surrogate model, which can directly transfer to target medical diagnosis systems. A well-established study has demonstrated that restricted black-box adversarial attacks pose a more stealthy security threat for natural vision tasks \cite{cheng2019improving, jiang2019black, WeiLCC19, lu2020enhancing, YinWYGKDLL21}. Nevertheless, existing studies related to no-box adversarial attacks for medical image analysis are still lacking \cite{bortsova2021adversarial_A18, apostolidis2022digital_A21, ding2023vith_A2023_2, chen2023frequency_A2023_4} and mainly based on approaches for natural images \cite{shah2018susceptibility_A4, rao2020thorough_A45_D60}, which requires further research efforts.

% A4 第一篇研究迁移性 揭示了susceptibility +  A18 给出了迁移性的相关因素  + A42 no-sign的影响
To uncover the potential security threat of medical diagnosis systems, Shah \textit{et al.} \cite{shah2018susceptibility_A4} first investigated the black-box adversarial vulnerability of diabetic retinopathy detection models. The adversarial examples generated against a specific diagnosis DNN can also be transferred to other models, which can cause grave consequences for practical diagnostic prediction. Furthermore, hybrid lesion-based algorithms \cite{chiem2007novel} that are composed of multiple lesion detectors are demonstrated a greater ability to defend against transferable adversarial examples. To further deploy a reliable diagnosis system in practice, Bortsova \textit{et al.} \cite{bortsova2021adversarial_A18} studied several unexplored factors affecting the no-box adversarial attack against medical image analysis systems. In addition, the transferable attack success rate can be enhanced by bridging the gap in training data and also network architectures between different diagnosis models. Hence, it is essential to consider the above-mentioned factors during the development of security-critical medical image analysis systems in clinical practice.

% 分类中特别的方法 A21 用 watermark
Unfortunately, very few works have investigated custom black-box adversarial attack methods for medical image analysis. Innovatively, Apostolidis \textit{et al.} \cite{apostolidis2022digital_A21} highlighted a novel view of digital watermarking to restricted black-box attacks in the context of medical image analysis. Specifically, the Krawtchouk orthogonal moments \cite{yap2003image} are also incorporated to generate adversarial watermarks against three different medical modalities. Massive experiments demonstrated that CT scans are extremely vulnerable against no-box attacks across various models.

% A9 在分割中探究了迁移性 + A1
Other than no-box adversarial attacks against medical image classification, there also exist scarce studies that explore the no-box adversarial vulnerability of medical image segmentation models. Bortsova \textit{et al.} \cite{bortsova2021adversarial_A9} investigated both the white-box and no-box adversarial attacks against medical image segmentation and their relationships. A surrogate network is used to generate targeted and untargeted adversarial images that can be further transferred to black-box segmentation models. Note that the training datasets of the surrogate model and the black-box model have no intersection to simulate clinical practice. The attacker can effectively misguide the target model to produce specific segmentation results, \textit{e.g.}, a heart symbol. Particularly, the transferability of restricted black-box attack relies on a high-level adversarial noise, which is also in alignment with \cite{paschali2018generalizability_A1}.

\vspace{-3mm}
\section{Medical Adversarial Defenses}
\label{sec:4}
% 第一段介绍这个section说了哪些东西: Adversarial Training, Adversarial Detection, Image-level Pre-processing, Feature Enhancement, Distillation 
\textbf{Medical adversarial defense} focuses on developing strategies to protect diagnosis models from adversaries, which are malicious inputs designed to mislead models into making incorrect diagnoses. Given the critical role of reliability and trust in healthcare applications, robust defense mechanisms are essential to safeguard against threats to the healthcare industry posed by adversaries. Thus, we highlight the significance of establishing robust defenses within computer-aided diagnosis systems to ensure patient safety and maintain trust in automated medical assessments. Furthermore, we introduce a systematic taxonomy of medical adversarial defense methods, categorized based on the operational stage of the defense mechanism. This categorization aids in understanding where each defense method applies within the pipeline\textemdash from input handling to final decision making\textemdash and how each method contributes to overall system robustness for trustworthy medical image analysis.

\vspace{-3mm}
\subsection{Taxonomy of Adversarial Defenses}
% subsection taxonomy 介绍 
On account of catastrophic failures caused by adversarial examples, various methods have been proposed to build trustworthy AI systems for natural images \cite{carlini2017adversarial, MadryMSTV18, XieWZRY18, dong2022improving, dong2023enemy}. In the meantime, establishing robust computer-aided diagnosis models for clinical applications also contributes to the delivery of reliable healthcare services to millions of people. It is thus significant to investigate adversarially robust models for medical image analysis. Considering its application prospect, we can imagine that healthcare or automatic diagnosis systems claim they can provide robust and reliable services to billions of people. Below, we systematically categorize adversarial defense methods for medical image analysis into five classes: adversarial training, adversarial detection, image-level pre-processing, feature enhancement, and knowledge distillation.

% 第二段介绍五种防御方式的区别
Adversarial training and its variants have been demonstrated to be the most effective methods to defend against adversarial examples. The primary goal of adversarial training is to improve the robustness of a certain model against adversarial attacks, which can be easily achieved by augmenting adversarial examples into training examples \cite{MadryMSTV18, zhang2019theoretically, wang2019improving}. In comparison, adversarial detection aims at distinguishing adversarial examples in advance to prevent the deep learning-based system from further catastrophes. Note that we have formally defined both adversarial training and detection in Section \ref{sec:2}. Image-level pre-processing aims to remove the visually imperceptible perturbation from adversarial examples while not affecting the subsequent inference of clean examples. Furthermore, feature enhancement methods focus on feature-level processing to mitigate the adversarial effect, which can be conducted by additional modules or novel frameworks. There also exists another type of defense method that distills a lightweight and adversarially robust model from a large-scale model for deployment in real-time healthcare applications.

% 第三段介绍五种防御方法的影响并且引出后续五个段落
Note that adversarial detection methods are incapable of conducting a correct diagnosis for adversaries, which mainly rely on distinguishing adversaries in advance to reject these malicious inputs. In comparison, the other four types of methods aim to establish a robust medical system, which treats all inputs equally for subsequent diagnosis. In the following, we provide a comprehensive review related to these five types of adversarial defense methods for medical imaging.

%\vspace{-3mm}
\subsection{Summary of Medical Defense Works}
Overall, we provide a summary of adversarial defense works in the context of medical image analysis in Table \ref{tab:2}. In addition to a significant number of adversarial attack methods mentioned in Section \ref{sec:3_Sum}, several attack approaches also serve as robustness evaluation metrics for adversarial defense in the context of medical images. We thus provide a list of used evaluation metrics for reference: Dense Adversary Generation (DAG) \cite{xie2017adversarial}, Generative Adversarial Perturbations (GAP) \cite{poursaeed2018generative}, Distributionally Adversarial Attack (DAA) \cite{zheng2019distributionally}, Diverse Inputs I-FGSM (DII-FGSM) \cite{xie2019improving}, HopSkipJumpAttack \cite{chen2020hopskipjumpattack}, AutoPGD \cite{croce2020reliable}, SPSA \cite{uesato2018adversarial}, Stabilized Medical Attack (SMA) \cite{QiGS0Z21_A17}.

\begin{table*}[t!]
	\centering
	\tiny 
	\renewcommand{\arraystretch}{0.7}
	\caption{Summary of adversarial defense methods for medical image analysis (time ascending). The newly proposed adversarial attack approaches tailored for robustness evaluation of medical images are in \textbf{bold}.}
	\vspace{-4mm}
	\label{tab:2}
	\rowcolors{2}{gray!0}{gray!8}
	\resizebox{\linewidth}{!}{  
		
		\begin{tabular}{|c|c|c|c|c|c|}
			%			\toprule
			\hline
			Reference & Year & Task &  Defense type  &  Evaluation metrics  &  Data Modality\\ 
			\hline
			\cite{asgari2018vulnerability_D17} & 2018 & Classification & Feature Enhancement & \begin{tabular}[c]{@{}c@{}} FGSM, PGD, BIM,\\L-BFGS, DeepFool \end{tabular} & X-ray \\
			
			\cite{huang2018some_D21} & 2018 & Reconstruction & Adversarial Training & FGSM, BIM & CT \\
			
			\cite{ren2019brain_D11} & 2019 & Segmentation & Adversarial Training & FGSM & MRI \\
			
			\cite{xue2019improving_D22} & 2019 & Classification & Feature Enhancement & FGSM, I-FGSM, CW & X-ray, Dermoscopy \\
			
			\cite{taghanaki2019kernelized_D20} & 2019 & \begin{tabular}[c]{@{}c@{}} Classification, Segmentation, \\ Object Detection \end{tabular} & Feature Enhancement & \begin{tabular}[c]{@{}c@{}} FGSM, CW, PGD, BIM,\\GN, SPSA, MI-FGSM\end{tabular} & X-ray, Dermoscopy \\
			
			\cite{vatian2019impact_D13} & 2019 & Classification & Adversarial Training & GN & CT, MRI \\
			
			\cite{he2019non_D16} & 2019 & Segmentation & Feature Enhancement & I-FGSM & X-ray, Dermoscopy \\
			
			\cite{li2019volumetric_D46} & 2019 & Segmentation & Adversarial Training & FGSM, I-FGSM & CT \\
			
			\cite{kotia2020risk_D49} & 2019 & Classification & Adversarial Training & FGSM & MRI \\
			
			\cite{stimpel2019multi_D62} & 2019 & Low-level vision & Feature Enhancement & \textbf{Optimization-based Attack} & X-ray, MRI \\
			
			\cite{park2020robustification_D19} & 2020 & Segmentation & Adversarial Detection & DAG & MRI \\
			
			\cite{li2020anatomical_D40} & 2020 & Regression & Feature Enhancement & FGSM, I-FGSM & MRI \\
			
			\cite{rao2020thorough_A45_D60} & 2020 & Classification & Adversarial Training, Pre-processing & \begin{tabular}[c]{@{}c@{}} FGSM, PGD, DAA, \\ MI-FGSM, DII-FGSM \end{tabular} & X-ray \\
			
			\cite{paul2020mitigating_D8}  & 2020 & Classification & Adversarial Training & OPA, FGSM & CT \\
			
			\cite{wu2020classification_D15} & 2020 & Classification & Adversarial Training & PGD & Fundoscopy \\
			
			\cite{anand2020self_D48} & 2020 & Classification, Segmentation & Adversarial Training & PGD, FGSM & X-ray, MRI \\
			
			\cite{ma2020increasing_D5}  & 2020 & Classification, Segmentation & Adversarial Training & PGD, I-FGSM & CT, MRI \\
			
			\cite{cheng2020addressing_D18} & 2020 & Reconstruction & Adversarial Training & \textbf{False-Negative Adversarial Feature} & MRI \\
			
			\cite{raj2020improving_D61} & 2020 & Reconstruction & Adversarial Training & \textbf{GAN-based Attack} & CT, X-ray\\
			
			\cite{huq2020analysis_D47} & 2020 & Classification & Adversarial Training & PGD, FGSM & Dermoscopy \\
			
			\cite{tripathi2020fuzzy_D4}  & 2020 & Classification & Pre-processing & \begin{tabular}[c]{@{}c@{}} PGD, BIM, CW, \\ FGSM, DeepFool \end{tabular} & CT, X-ray \\
			
			\cite{liu2020no_D12} & 2020 & Classification & Adversarial Training & PGD & CT \\
			
			\cite{chen2020realistic_D14} & 2020 & Segmentation & Adversarial Training & \textbf{Adversarial Bias Attack} & MRI \\
			
			\cite{liu2020defending_D23} & 2020 & Segmentation & Pre-processing & ASMA & Fundoscopy, Dermoscopy \\
			
			\cite{shen2020robustness_D36} & 2020 & Classification & Adversarial Training & \textbf{Random and Optimized Attacks} & CT \\
			
			\cite{li2020robust_D24} & 2020 & Classification & Adversarial Detection & FGSM, PGD, BIM, MI-FGSM & X-ray \\
			
			\cite{ma2021understanding_D10} & 2021 & Classification & Adversarial Detection & FGSM, BIM, PGD, CW & X-ray, Fundoscopy, Dermoscopy \\
			
			\cite{watson2021attack_D7}  & 2021 & Classification & Adversarial Detection & PGD, CW & X-ray \\
			
			\cite{carannante2021trustworthy_D54} & 2021 & Segmentation & Feature Enhancement & PGD, FGSM & CT, MRI \\
			
			\cite{hirano2021universal_A7_D55} & 2021 & Classification & Adversarial Training & UAP & OCT, X-ray, Dermoscopy \\
			
			\cite{li2021defending_D1} & 2021 & Classification & Adversarial Training, Adversarial Detection & FGSM, PGD, CW & OCT \\
			
			\cite{xu2021towards_D9}  & 2021 & Classification & Adversarial Training & PGD, GAP & X-ray, Fundoscopy, Dermoscopy \\
			
			\cite{lal2021adversarial_D3}  & 2021 & Classification & Adversarial Training, Feature Enhancement & \begin{tabular}[c]{@{}c@{}} FGSM, DeepFool, \\ \textbf{Speckle Noise Attack} \end{tabular} & X-ray, Fundoscopy \\
			
			\cite{pervin2021adversarial_D25} & 2021 & Segmentation & Adversarial Training & FGSM & CT \\
			
			\cite{uwimana2021out_D26} & 2021 & Classification                                 & Adversarial Detection & FGSM, BIM, CW, DeepFool & Microscopy \\
			
			\cite{han2021advancing_D35} & 2021 & Classification & Feature Enhancement & PGD & CT, MRI, X-ray \\
			
			\cite{zhou2021ssmd_D41} & 2021 & Object Detection & Adversarial Training & FGSM & CT, Microscopy \\
			
			\cite{liu2021robustifying_D6}  & 2021 & Segmentation & Distillation & FGSM, I-FGSM, TI-FGSM & MRI \\
			
			\cite{daza2021towards_D27} & 2021 & Segmentation & Feature Enhancement, Adversarial Training & PGD, AA & CT, MRI \\
			
			\cite{chen2021enhancing_D33} & 2021 & Classification & Feature Enhancement & PGD, FGSM & X-ray \\
			
			\cite{yao2021medical_D45} & 2021 & Classification & Adversarial Detection, Pre-processing & \begin{tabular}[c]{@{}c@{}} FGSM, BIM, PGD, CW, PGD,\\MI-FGSM, TI-FGSM, DI-FGSM \end{tabular} & X-ray, Fundoscopy \\
			
			\cite{gupta2022vulnerability_D56} & 2022 & Classification & Adversarial Training & FGSM & MRI \\
			
			\cite{kansal2022defending_D51} & 2022 & Classification & Pre-processing & FGSM, PGD & X-ray \\
			
			\cite{jaiswal2022ros_D66} & 2022 & Classification & Distillation & FGSM, PGD & X-ray, Dermoscopy \\
			
			\cite{yang2022defense_D67} & 2022 & Classification & Adversarial Detection & HopSkipJumpAttack & MRI, X-ray, Microscopy \\
			
			\cite{xu2022medrdf_D2}  & 2022 & Classification & Pre-processing & I-FGSM, PGD, CW & X-ray, Dermoscopy \\
			
			\cite{joel2022using_D32} & 2022 & Classification & Adversarial Training & FGSM, PGD, BIM & CT, MRI, X-ray \\
			
			\cite{alatalo2022detecting_D63} & 2022 & Classification & Adversarial Detection & OPA & Microscopy \\
			
			\cite{hu2022adversarial_D29} & 2022 & Classification & Adversarial Training & DDN & MRI \\
			
			\cite{rodriguez2022role_D30} & 2022 & Classification & Adversarial Training & FGSM, PGD & OCT, X-ray, Dermoscopy \\
			
			\cite{ma2022adaptive_D53} & 2022 & \begin{tabular}[c]{@{}c@{}} Segmentation, Object Detection, \\ Landmark Detection \end{tabular} & Adversarial Training & PGD, I-FGSM & MRI, X-ray, Microscopy \\
			
			\cite{xie2022effective_D38} & 2022 & Classification & Adversarial Training & FGSM, PGD, BIM & CT, MRI, X-ray  \\
			
			\cite{wang2022fight_D43} & 2022 & Classification & Pre-processing & \begin{tabular}[c]{@{}c@{}} FGSM, BIM, CW, \\ PGD, AA, DI-FGSM \end{tabular} & Dermoscopy \\
			
			\cite{ghaffari2022adversarial_D28} & 2022 & Classification & Feature Enhancement & FGSM, PGD, FAB, SquareAttack & Microscopy \\
			
			\cite{maliamanis2022resilient_D39} & 2022 & Classification, Segmentation & Adversarial Training & \begin{tabular}[c]{@{}c@{}} FGSM, PGD, SquareAttack, \\ \textbf{Moment-based Adversarial Attack} \end{tabular} & X-ray, Microscopy \\
			
			\cite{almalik2022self_D42} & 2022 & Classification & Adversarial Detection, Feature Enhancement & FGSM, PGD, BIM, AutoPGD & X-ray, Fundoscopy \\
			
			\cite{SunXVG22_D44} & 2022 & Classification & Adversarial Training, Feature Enhancement & FGSM, PGD, CW & Ultrasound \\
			
			\cite{pandey2022adversarially_D52} & 2022 & Segmentation & Feature Enhancement & FGSM, PGD, SMA & CT \\
			
			\cite{bharath2022analysis_D58} & 2022 & Classification & Adversarial Training, Distillation & L-BFGS, FGSM & Fundoscopy \\
			
			\cite{roh2022impact_D59} & 2022 & Classification & Adversarial Training & FGSM & Microscopy \\
			
			\cite{daanouni2022nsl_D64} & 2022 & Classification                                 & Feature Enhancement & FGSM & Fundoscopy \\
			
			\cite{le2022efficient_D65} & 2022 & Segmentation & Pre-processing & DAG, I-FGSM & MRI, X-ray, Fundoscopy \\
			
			\cite{chen2022enhancing_D31} & 2022 & Segmentation & Adversarial Training & PGD & MRI \\
			
			\cite{shi2022robust_D57} & 2022 & Classification & Feature Enhancement & FGSM, PGD & X-ray, Fundoscopy \\
			
			\cite{dai2023improving_D2023_1} & 2023 & Classification & Feature Enhancement & BIM, PGD, Deepfool, AA & X-ray, Fundoscopy, Dermoscopy \\
			\cite{wang2023reversing_D2023_2} & 2023 & Classification & Pre-processing & \makecell{FGSM, BIM, PGD, \\ CW, DI-FGSM, AA} & Dermoscopy \\
			\cite{xiang2023toward_D2023_3} & 2023 & Classification & Adversarial Training, Feature Enhancement & FGSM, PGD, MIM, AA & CT \\
			\cite{wang2023adversarial_D2023_5} & 2023 & Classification & Adversarial Detection & FGSM, PGD, SPSA & Ultrasound \\
			
			
			\cite{li2024dynamic_D2024_4} & 2024 & Classification & Adversarial Training & I-FGSM, PGD & Dermoscopy \\
			\cite{zafar2024robust_D2024_6} & 2024 & Classification & Adversarial Training, Pre-processing & FGSM, PGD, BIM & X-ray \\
			
			
			
			
			
			%			\bottomrule
			\hline
		\end{tabular}
	}
	\vspace{-6mm}
\end{table*}

%\vspace{-3mm}
\subsection{Adversarial Training}

% 分类中用已有常规方法对抗训练(D15, D29, D32) D13 D9 D34 D8 D32
The majority of medical adversarial defense methods concentrate on adversarial training to establish robust diagnosis systems. Among them, a large proportion of works extend existing adversarial training methods for natural images to medical classification tasks \cite{paul2020mitigating_D8, xu2021towards_D9, vatian2019impact_D13, wu2020classification_D15, hu2022adversarial_D29, joel2022using_D32, xiang2023toward_D2023_3, li2024dynamic_D2024_4, zafar2024robust_D2024_6}. Vatian \textit{et al.} \cite{vatian2019impact_D13} investigated adversarial examples for medical imaging and tried several approaches to defend against these malicious examples. Specifically, both FGSM \cite{GoodfellowSS14} and JSMA \cite{papernot2016limitations} are incorporated to generate adversarial examples during the adversarial training stage. To further achieve better robustness, the authors also utilized the Gaussian noise to augment adversarially trained data and replaced the original Rectified Linear Units (ReLU) with Bounded ReLU. In addition to providing a robustness evaluation of several computer-aided diagnosis models, Xu \textit{et al.} adapt PGD-based adversarial training \cite{MadryMSTV18} and Misclassification Aware adveRsarial Training (MART) \cite{wang2019improving} to robustify computer-aided diagnosis models. A new medical dataset named \textit{Robust-Benchmark} is also proposed to evaluate the robustness of common perturbations. The above-mentioned works mainly focus on enhancing the robustness of a single model. An ensemble adversarial training method is also employed to establish a robust malignancy prediction system based on lung nodules \cite{paul2020mitigating_D8}. Specifically, various models with diverse architectures are adversarially trained with different initialized weights in advance. The ensemble prediction results can be further obtained by averaging the output probabilities from adversarially trained models.


% 分类为医疗特制的方法 D12 D3 D44 D39
In addition to transferring natural defense methods to diagnosis models, there also exist several adversarial defense methods that are tailored for medical image analysis \cite{lal2021adversarial_D3, liu2020no_D12, SunXVG22_D44, maliamanis2022resilient_D39}. Liu \textit{et al.} \cite{liu2020no_D12} investigate three types of adversarial augmentation examples that can be added to the training dataset for robustness improvement. Projected Gradient Descent (PGD) \cite{MadryMSTV18} is utilized to iteratively search the worst latent code to synthesize adversarial nodules that the target diagnosis model fails to detect. Furthermore, image-level adversarial perturbations are also generated to augment the training data. Extensive experiments demonstrate the effectiveness of the proposed method in enhancing the detection performance on real CT data and also robustness to unforeseen noise perturbations. Moreover, the Multi-Instance Robust Self-Training method with Drop-Max layer (MIRST-DM) \cite{SunXVG22_D44} is proposed to learn a smooth decision boundary for robust classification of breast cancer. In particular, MIRST-DM employs a sequence of adversarial images produced during the iterative adversary generation stage to accelerate the model convergence and also boost the robustness. The drop-max layer is proposed to discard the maximum value and forward the second-largest value to the next layer, which can depress unstable features to enhance the adversarial robustness. Note that speckle noise is one of the most common noises in retinal fundus images, which may hinder subsequent detection.

% 除了有监督方法以外 D1讨论 无监督 半监督       D48 自监督
Although an array of adversarial training methods have been explored in the context of medical image analysis, the majority of them concentrate on supervised learning. Nonetheless, the manual annotation cost for medical images can sometimes be extremely expensive. To explore a low-cost training paradigm to enhance adversarial robustness for computer-aided diagnosis models, Li \textit{et al.} \cite{li2021defending_D1} proposed a novel medical defense method based on Semi-Supervised Adversarial Training (SSAT). Typically, SSAT produces the pseudo labels for unlabeled images in advance and then minimizes the empirical risk to enhance the network robustness. Furthermore, a systematic investigation of the adversarial robustness in the context of biomedical image analysis \cite{anand2020self_D48} has indicated that self-supervised learning can exhibit better adversarial robustness and also natural performance compared with transfer learning schemes on small medical datasets. 

% 分割 (D11 D46) D5 D25 D14
Except for adversarial training for biomedical image classification, several academic works explore improving the intrinsic network robustness for medical segmentation \cite{ren2019brain_D11, li2019volumetric_D46, chen2022enhancing_D31, chen2020realistic_D14}. Generally, adversarial training can result in a clean accuracy drop \cite{zhang2019theoretically, XieY20, RadeM22}, which is unacceptable for various medical diagnosis applications. To mitigate this issue, Ma \textit{et al.} \cite{ma2020increasing_D5} proposed the Increasing Margin Adversarial (IMA) training method to adjust the upper bound of each adversarial perturbation during the training stage so that the intensity of the perturbation can be adaptively reduced. Similar to friendly adversarial training \cite{zhang2020attacks}, this work is inspired by the hypothesis that excessively strong adversarial examples might mislead the training process to cause a clean performance drop. The generated proper adversarial examples can effectively reduce the occurrence of overfitting decision boundary that impairs the standard performance of biomedical image applications. Chen \textit{et al.} \cite{chen2020realistic_D14} proposed a novel adversarial augmentation method for enhancing intrinsic network robustness via simulating underlying artifacts in clinical MR imaging. Specifically, the authors utilize the PGD method to search the specific control points to produce an adversarial bias field that can disrupt the intensity of original images by multiplication. Augmenting these physical adversarial examples into the training set can further promote robust feature learning for MR image segmentation, which is also effective in both low-data and cross-population settings.

% 除了模式识别任务外，重建 D21 D18 D61
Aside from adversarial training for medical pattern recognition, several researchers spare no effort to improving the robustness for medical image reconstruction \cite{cheng2020addressing_D18, raj2020improving_D61}. Especially for limited angle tomography, Huang \textit{et al.} \cite{huang2018some_D21} investigated the vulnerability of U-Net \cite{ronneberger2015u} against the Poisson noise during the artifact reduction. The reconstruction robustness can be further enhanced by adding projection-domain Poisson perturbation to training data. False-Negative Adversarial Feature (FNAF) \cite{cheng2020addressing_D18} is designed to simulate the worst-case perceptible small features in the clinical diagnosis setting, which disappear after MRI reconstruction. In the meantime, FNAF can also be embedded into the robust training process to improve MRI reconstruction for small and infrequent structures. Furthermore, Raj \textit{et al.} \cite{raj2020improving_D61} proposed a robust learning strategy with theoretical analysis to improve the robustness of various image-level reconstruction tasks, including face reconstruction and CT reconstruction. Unlike previous adversary generation methods that resort to iterative optimization, an auxiliary generative network is utilized to produce adversaries during the training stage. The image reconstruction model thus minimizes the target loss of adversary generation. 

%\vspace{-3mm}
\subsection{Adversarial Detection}
% 分类上的对抗检测 (D24 D63) D7 D10 D42
Different from establishing robustness during the training stage of computer-aided diagnosis models, adversarial detection aims at distinguishing adversarial examples from input examples during the application stage. In order to prevent subsequent misdiagnosis caused by adversarial examples, various adversarial detection methods have been proposed in the context of biomedical image analysis \cite{li2020robust_D24, alatalo2022detecting_D63, almalik2022self_D42}. In particular, medical adversarial detection can also be regarded as anomaly detection, which can be solved by incorporating explainability techniques \cite{watson2021attack_D7}. Based on the observation that adversarial attacks induce a shift in the distribution of SHAP values for medical imaging and electronic health record data, the authors thus proposed both full- and semi-supervised detection trained on SHAP values. Ma \textit{et al.} \cite{ma2021understanding_D10} provided a comprehensive analysis of medical adversarial examples and their relations to natural imaging. Moreover, two subspace distance-based methods in terms of kernel density and local intrinsic dimensionality, respectively. The robustness of Vision Transformers (ViT) for medical image classification is first investigated in \cite{almalik2022self_D42}. Specifically, the Kullback-Leibler (KL) divergence between various Multi-Layer Perceptrons (MLPs) can be applied to distinguish medical adversaries against ViT.

% 无监督对抗检测 + 新指标 D1
Although adversarially trained models can improve the in-distribution robustness of DNNs, they still suffer from a significant robustness drop against Out-Of-Distribution (OOD) adversarial examples. To further mitigate the disruption from OOD adversarial examples, Li \textit{et al.} \cite{li2021defending_D1} proposed an unsupervised adversarial detection method based on the latent features from the penultimate layer of computer-aided diagnosis models. Specifically, a Gaussian mixture model is employed to estimate the probability density for these latent features of clean examples. During the inference stage, the unsupervised detector rejects OOD adversarial examples based on the deviation of the extracted latent features from their corresponding probability density.

% 分割上的对抗检测 D19
Most existing adversarial detection methods are designed for the medical classification scenario. There still remains limited research devoted to detecting adversarial examples for medical segmentation models. Park \textit{et al.} \cite{park2020robustification_D19} resorted to the frequency domain to better distinguish adversaries based on the reconstruction error. In addition, the reformer network is also incorporated to purify medical images to the manifold of legitimate examples for the subsequent segmentation stage.


\vspace{-4mm}
\subsection{Image-level Pre-processing}
\vspace{-1mm}
In general, an adversarial image consists of a clean image and its corresponding adversarial perturbation. Hence, denoising the adversarial example to get rid of the perturbation part can further be an effective strategy to facilitate the subsequent network diagnosis. Thus, there is no need to re-train or modify medical models when applying image-level pre-processing, which can be convenient and safe for biomedical image analysis. In this section, we introduce several image-level pre-processing works concentrating on protecting computer-aided diagnosis systems against adversarial examples. 

% 分类 (D45 D60) D2 D4 D43 D51
The vast majority of pre-processing-based defense methods are designed for medical classification tasks \cite{yao2021medical_D45, rao2020thorough_A45_D60, kansal2022defending_D51, wang2023reversing_D2023_2}. Hence, the image-level pre-processing is required not to destroy distinguishable parts for the subsequent diagnosis. The Medical Retrain-less Diagnostic Framework (MedRDF) \cite{xu2022medrdf_D2} is proposed to convert a pre-trained non-robust diagnosis model into its robust counterpart during inference. Specifically, MedRDF creates various copies of medical images that are perturbed by isotropic noises. These copies are then predicted via majority voting after denoising by a customized denoiser. A robust metric is also proposed to provide the confidence score of MedRDF, which further assists doctors in clinical practice. Beyond pixel-level denoising, Kansal \textit{et al.} \cite{kansal2022defending_D51} extended a high-level representation guided denoiser to protect medical applications against adversaries. The guidance of such high-level information further facilitates the image-level elimination of the adversarial effect to the final diagnosis result rather than visual disruptions. 

% 分割 D65 D23
Current research on adversarial pre-processing for robust biomedical segmentation systems is still lacking and time-consuming. To this end, Liu \textit{et al.} \cite{liu2020defending_D23} proposed a low-cost image compression-based method to eliminate image-level adversarial perturbations against biomedical segmentation. In particular, a fine-grained frequency refinement approach is utilized to redesign the quantization table in JPEG compression-based pre-processing. Moreover, the optimized quantization step constraints are carefully set to prioritize defense efficiency and also compensate for the accuracy reduction caused by quantization errors. Furthermore, Le \textit{et al.} \cite{le2022efficient_D65} introduced a learnable adversarial denoising method by utilizing the U-Net \cite{ronneberger2015u} as the defender model. The defender first pre-processes medical images before feeding them into the subsequent biomedical segmentation models that are required to be effective for both clean examples and their adversarial counterparts. 


%\vspace{-3mm}
\subsection{Feature Enhancement}
% 先给定义，再给分类 (D57 D35 D33 D64) D17 D22 D35
% 缺20 62
Adversarial examples have been demonstrated to be attributed to non-robust features (extracted from certain patterns in the data distribution) \cite{ilyas2019adversarial}, resulting in the mismatching of robustness between human and machine vision. Therefore, it is more than necessary to enhance the feature representation for robust inference. For a formal definition, we regard the modification of architectures or mapping functions as the feature enhancement in this paper. A myriad of feature enhancement methods have been designed to boost the robustness of medical classification models \cite{shi2022robust_D57, han2021advancing_D35, chen2021enhancing_D33, daanouni2022nsl_D64}. Taghanaki \textit{et al.} \cite{taghanaki2019kernelized_D20} modified the medical classification networks by replacing max-pooling layers with average-pooling layers. This modification significantly boosts the robustness against adversarial examples for different network architectures. The plausible reason for the robustness enhancement is that average-pooling can capture more global-level contextual information than the max-pooling, which increases the difficulty of adversarial attacks. Moreover, AutoEncoder (AE) can also be embedded into computer-aided diagnosis models for the feature-level denoising \cite{xue2019improving_D22}, which is independent of the image-level pre-processing procedure. In the meantime, the guidance of feature invariance is incorporated to reduce the model sensitivity against adversaries. Han \textit{et al.} \cite{han2021advancing_D35} introduced a dual-batch normalization to adversarial training, which improves the robustness of diagnostic models without the degradation of clean accuracy.

Apart from the mainstream medical classification task, feature enhancement has also been applied in several medical imaging tasks, including segmentation \cite{carannante2021trustworthy_D54}, object detection \cite{taghanaki2019kernelized_D20}, and low-level vision \cite{stimpel2019multi_D62}. Non-Local Context Encoder (NLCE) \cite{he2019non_D16} is proposed to improve the robustness of biomedical image segmentation models as a plug-and-play module. Similar to the observation from \cite{taghanaki2019kernelized_D20}, the NLCE module aims at capturing the global-level spatial dependencies and also contexts to strengthen features, which can be easily applied to various DNN-based medical image segmentation models. Stimpel \textit{et al.} \cite{stimpel2019multi_D62} utilize the guided filter with a learned guidance map for medical image super resolution and denoising. The guided filter exhibits a solid ability to limit the effectiveness of unforeseen adversarial examples on their generated outputs.


\vspace{-3mm}
\subsection{Knowledge Distillation}
% 第一段先介绍蒸馏，可以先讲原始蒸馏，再讲adversarial knowledge distillation for natural images
% 分类：D66 D6
In the machine learning community, knowledge distillation can be an effective technique to transfer the learned knowledge from a complex (teacher) model to a lightweight (student) model. Accordingly, self-distillation refers specifically to the situation when the teacher and student models share the same network architecture. Moreover, adversarial knowledge distillation has also been widely explored for the natural imaging domain \cite{goldblum2020adversarially, zi2021revisiting, ZhuY0ZL0ZXY22}, which transfers the adversarial robustness from the large teacher model to a lightweight student model. 

There also exist a few works that concentrate on enhancing adversarial robustness by knowledge distillation in the context of medical image analysis. The Robust Stochastic Knowledge Distillation (RoS-KD) framework \cite{jaiswal2022ros_D66} is designed to distill robust knowledge from multiple teacher models to a specific student model. Note that the distillation is conducted on noisy labeled data to efficiently simulate practical adversarial examples. The smooth parameter update mechanism is also proposed using weight averaging on multiple checkpoints. Furthermore, Liu \textit{et al.} \cite{liu2021robustifying_D6} adapted the defensive distillation to brain tumor segmentation for MRI data. The authors also showed that the defensive distillation achieves a better robust performance against FGSM attacks than adversarial training.




\begin{figure}[t]
	\centering
	\includegraphics[width=0.6\linewidth]{imgs/Adv_cases.pdf}
	\vspace{-4mm}
	\caption{Visualization of medical adversarial examples with predictions under diverse perturbation size $\epsilon$. The generated segmentation masks are superimposed on the original images for visualization.}
	\vspace{-4mm}
	\label{fig:3}
\end{figure}

\vspace{-3mm}
\section{Experimental Evaluation}
\label{sec:5}
% 介绍这段主题，先介绍用的数据集和度量手段，然后说我们度量了不同场景的对抗攻击对医疗的影响，最后说防御部分，我们在医疗任务复现了对抗训练的方法
In this section, we provide a systematic evaluation of both adversarial attack and defense for computer-aided diagnosis models. We first introduce our experimental settings, including datasets and measurements. Moreover, we measure the effect of diverse adversarial attack methods on several medical imaging tasks in various scenarios. We also establish a benchmark of adversarial training for biomedical image analysis systems to facilitate future research. 

\begin{table}[!t]
	\centering
	%	\tiny 
	\caption{White-box robustness results under different attack configurations using ResNet-18 for medical classification. We present accuracy (\%) and percentage decrease for binary and multi-class classification tasks.}
	\vspace{-4mm}
	%\small
	\renewcommand{\arraystretch}{0.6}
	% \resizebox{0.75\linewidth}{!}{
		\resizebox{0.6\linewidth}{!}{  
			\begin{tabular}{cccccc}
				\toprule
				\multirow{2}{*}{Adversarial Type}&\multirow{2}{*}{$\epsilon$}&\multicolumn{2}{c}{Messidor (Fundoscopy)}&\multicolumn{2}{c}{ISIC (Dermoscopy)} \\
				\cmidrule(lr){3-4} \cmidrule(lr){5-6}
				&&Binary&Multi-class&Binary&Multi-class\\
				\midrule
				None & 0 & 71.3 & 50.0 & 64.9 & 60.0\\
				\midrule
				\multirow{4}{*}{\makecell{FGSM \cite{GoodfellowSS14}}} & 1/255 & 47.1$_\textit{(-33.9\%)}$ & 22.1$_\textit{(-55.8\%)}$ & 41.5$_\textit{(-36.1\%)}$ & 12.5$_\textit{(-79.2\%)}$ \\
				& 2/255 & 38.8$_\textit{(-45.6\%)}$ & 13.8$_\textit{(-72.4\%)}$ & 37.6$_\textit{(-42.1\%)}$ & 4.8$_\textit{(-92.0\%)}$ \\
				& 4/255 & 27.9$_\textit{(-60.9\%)}$ & 9.2$_\textit{(-81.6\%)}$ & 35.7$_\textit{(-45.0\%)}$ & 4.5$_\textit{(-92.5\%)}$ \\
				& 8/255 & 24.2$_\textit{(-66.1\%)}$ & 20.4$_\textit{(-59.2\%)}$ & 34.5$_\textit{(-46.8\%)}$ & 8.0$_\textit{(-86.7\%)}$ \\
				\midrule
				\multirow{4}{*}{\makecell{PGD \cite{MadryMSTV18}}} & 1/255 & 21.3$_\textit{(-70.1\%)}$ & 12.5$_\textit{(-75.0\%)}$ & 38.4$_\textit{(-40.8\%)}$ & 8.9$_\textit{(-85.1\%)}$ \\
				& 2/255 & 8.8$_\textit{(-87.7\%)}$ & 4.6$_\textit{(-90.8\%)}$ & 22.8$_\textit{(-64.9\%)}$ & 2.6$_\textit{(-95.7\%)}$ \\
				& 4/255 & 1.7$_\textit{(-97.6\%)}$ & 0.4$_\textit{(-99.2\%)}$ & 12.5$_\textit{(-80.7\%)}$ & 0.8$_\textit{(-98.7\%)}$ \\
				& 8/255 & 0.0$_\textit{(-100.0\%)}$ & 0.0$_\textit{(-100.0\%)}$ & 12.8$_\textit{(-80.3\%)}$ & 0.4$_\textit{(-99.3\%)}$ \\
				\midrule
				\multirow{4}{*}{\makecell{CW \cite{carlini2017towards}}} & 1/255 & 15.8$_\textit{(-77.8\%)}$ & 12.5$_\textit{(-75.0\%)}$ & 37.5$_\textit{(-42.2\%)}$ & 9.1$_\textit{(-84.8\%)}$ \\
				& 2/255 & 7.1$_\textit{(-90.0\%)}$ & 6.7$_\textit{(-86.6\%)}$ & 22.5$_\textit{(-65.3\%)}$ & 2.5$_\textit{(-95.8\%)}$ \\
				& 4/255 & 1.3$_\textit{(-98.2\%)}$ & 2.1$_\textit{(-95.8\%)}$ & 9.3$_\textit{(-85.7\%)}$ & 0.9$_\textit{(-98.5\%)}$ \\
				& 8/255 & 0.0$_\textit{(-100.0\%)}$ & 0.0$_\textit{(-100.0\%)}$ & 2.1$_\textit{(-96.8\%)}$ & 0.4$_\textit{(-99.3\%)}$ \\
				\midrule
				\multirow{4}{*}{\makecell{SMA \cite{QiGS0Z21_A17}}} & 1/255 & 14.6$_\textit{(-79.5\%)}$ & 12.3$_\textit{(-75.4\%)}$ & 37.2$_\textit{(-42.7\%)}$ & 8.7$_\textit{(-85.5\%)}$ \\
				& 2/255 & 5.7$_\textit{(-92.0\%)}$ & 5.2$_\textit{(-89.6\%)}$ & 22.6$_\textit{(-65.2\%)}$ & 2.5$_\textit{(-95.8\%)}$ \\
				& 4/255 & 0.0$_\textit{(-100.0\%)}$ & 0.5$_\textit{(-99.0\%)}$ & 9.9$_\textit{(-84.7\%)}$ & 1.0$_\textit{(-98.3\%)}$ \\
				& 8/255 & 0.0$_\textit{(-100.0\%)}$ & 0.0$_\textit{(-100.0\%)}$ & 1.7$_\textit{(-97.4\%)}$ & 0.3$_\textit{(-99.5\%)}$ \\
				\midrule
				\multirow{4}{*}{\makecell{AA \cite{croce2020reliable}}} & 1/255 & 12.8$_\textit{(-82.1\%)}$ & 6.7$_\textit{(-86.6\%)}$ & 36.9$_\textit{(-43.1\%)}$ & 8.5$_\textit{(-85.8\%)}$ \\
				& 2/255& 3.8$_\textit{(-94.6\%)}$ & 2.5$_\textit{(-95.0\%)}$ & 22.1$_\textit{(-66.0\%)}$ & 2.4$_\textit{(-96.0\%)}$ \\
				& 4/255& 0.0$_\textit{(-100.0\%)}$ & 0.0$_\textit{(-100.0\%)}$ & 8.4$_\textit{(-87.1\%)}$ & 0.8$_\textit{(-98.7\%)}$ \\
				& 8/255& 0.0$_\textit{(-100.0\%)}$ & 0.0$_\textit{(-100.0\%)}$ & 1.6$_\textit{(-97.5\%)}$ & 0.1$_\textit{(-99.8\%)}$ \\
				\bottomrule
				
			\end{tabular}
		}
		% }
	\vspace{-4mm}
	\label{tab:3}
\end{table}



%\vspace{-3mm}
\subsection{Experimental Settings}
\subsubsection{Datasets}
% 介绍分类的3个数据集，分割的1个数据集
In this paper, we mainly use four standard benchmark datasets to explore adversarial attack and defense for medical image analysis: 1) \textbf{Messidor}\footnote{Kindly provided by the Messidor program partners (see https://www.adcis.net/en/third-party/messidor/).} dataset \cite{decenciere2014feedback}, containing 1,200 eye fundus color numerical images for detecting diabetic retinopathy of four classes according to retinopathy grade; 2) International Skin Imaging Collaboration (\textbf{ISIC 2017}) dataset \cite{codella2018skin} of 2,750 dermoscopic images of three classes for skin lesion classification and segmentation. 3) \textbf{ChestX-ray 14} dataset \cite{wang2017chestx}, consisting of 112,120 frontal-view X-ray images of 14 thorax diseases. 4) \textbf{COVID-19} database \cite{chowdhury2020can}, comprising 21,165 chest X-ray images with lung masks for segmentation. 

For medical classification, we consider both binary and multi-class classification tasks. Following \cite{xu2021towards_D9}, we randomly select 960 fundus images from Messidor dataset as the training set. All the fundus images are processed with several data augmentation operations, including random rotating and flipping. We also conduct pre-processing on skin lesion images from ISIC 2017 dataset with resizing and center-cropping. Due to the computational budget, we evenly sample 10,000 X-ray images with random flipping and normalization from ChestX-ray 14 dataset and then randomly choose 8,000 examples from them as the training set. Considering the computational cost, we sample 2,750 X-ray images and corresponding lung masks from the COVID-19 database for medical segmentation. Following the dataset division from ISIC 2017 dataset, 2,000 chest X-ray images are utilized for training, and the rest are used for evaluation.




\begin{table}[!t]
	\centering
	\begin{minipage}{0.51\textwidth}
	\centering
	\vspace{-2mm}
	\caption{White-box robustness (\%) against PGD attacks in diverse setups with U-Net for medical segmentation.}
	\vspace{-4mm}
	%	\footnotesize
	\renewcommand{\arraystretch}{0.6}
	% \resizebox{0.75\linewidth}{!}{
		\resizebox{1\linewidth}{!}{  
			\begin{tabular}{cccccc}
				\toprule
				\multirow{2}{*}{Adversarial Loss}&\multirow{2}{*}{$\epsilon$}&\multicolumn{2}{c}{ISIC (Dermoscopy)}&\multicolumn{2}{c}{COVID-19 (X-ray)} \\
				\cmidrule(lr){3-4} \cmidrule(lr){5-6}
				&&mIOU&Dice&mIOU&Dice\\
				\midrule
				None & 0 & 0.818 & 0.883 & 0.977 & 0.988\\
				\midrule
				\multirow{4}{*}{BCE} & 1/255 & 0.580 & 0.680 & 0.747 & 0.840\\
				& 2/255 & 0.405 & 0.517 & 0.559 & 0.690\\
				& 4/255 & 0.248 & 0.354 & 0.355 & 0.492\\
				& 8/255 & 0.167 & 0.255 & 0.230 & 0.350\\
				\midrule
				\multirow{4}{*}{IOU} & 1/255 & 0.542 & 0.672 & 0.695 & 0.829\\
				& 2/255 & 0.328 & 0.487 & 0.454 & 0.648\\
				& 4/255 & 0.145 & 0.305 & 0.239 & 0.418\\
				& 8/255 & 0.073 & 0.193 & 0.136 & 0.283\\
				\midrule
				\multirow{4}{*}{Dice} & 1/255 & 0.561 & 0.663 & 0.704 & 0.807\\
				& 2/255 & 0.340 & 0.450 & 0.473 & 0.610\\
				& 4/255 & 0.158 & 0.243 & 0.265 & 0.391\\
				& 8/255 & 0.082 & 0.140 & 0.152 & 0.246\\
				\bottomrule
				
			\end{tabular}
		}
		% }
	\vspace{-6mm}
	\label{tab:4}
	\end{minipage}
	\hfill
	\begin{minipage}{0.44\textwidth}
	\centering
	\vspace{-2mm}
	\caption{Black-box and no-box adversarial robustness (\%) under different settings with ResNet-18 for medical classification.}
	\vspace{-4mm}
	%\small
	\renewcommand{\arraystretch}{0.3}
	% \resizebox{0.75\linewidth}{!}{
		\resizebox{1\linewidth}{!}{  
			\begin{tabular}{cccccc}
				\toprule
				\multirow{2}{*}{Adversarial Type}&\multirow{2}{*}{$\epsilon$}&\multicolumn{2}{c}{Messidor (Fundoscopy)}&\multicolumn{2}{c}{ISIC (Dermoscopy)} \\
				\cmidrule(lr){3-4} \cmidrule(lr){5-6}
				&&Binary&Multi-class&Binary&Multi-class\\
				\midrule
				None & 0 & 71.3 & 50.0 & 64.9 & 60.0\\
				\midrule
				\multirow{4}{*}{\makecell{Square Attack \cite{andriushchenko2020square} \\ (\textbf{Black-box})}} & 1/255 & 13.3 & 9.6 & 45.1 & 19.3\\
				& 2/255 & 5.8 & 3.8 & 30.3 & 6.0\\
				& 4/255 & 0.0 & 0.4 & 15.6 & 1.6\\
				& 8/255 & 0.0 & 0.0 & 2.9 & 0.5\\
				\midrule
				\multirow{4}{*}{\makecell{PGD \cite{MadryMSTV18} \\ (\textbf{No-box})}} & 1/255 & 47.5 & 41.3 & 59.1 & 58.5\\
				& 2/255 & 36.7 & 37.9 & 49.7 & 55.6\\
				& 4/255 & 27.9 & 27.9 & 35.3 & 50.4\\
				& 8/255 & 10.4 & 18.7 & 21.2 & 38.5\\
				\midrule
				\multirow{4}{*}{\makecell{CW \cite{carlini2017towards} \\ (\textbf{No-box})}} & 1/255 & 42.1 & 37.1 & 59.1 & 58.0\\
				& 2/255 & 35.4 & 37.1 & 48.9 & 52.9\\
				& 4/255 & 35.4 & 40.4 & 34.8 & 41.7\\
				& 8/255 & 19.6 & 12.9 & 20.5 & 19.3\\
				\midrule
				\multirow{4}{*}{\makecell{SMA \cite{QiGS0Z21_A17} \\ (\textbf{No-box})}} & 1/255 & 43.0 & 38.6 & 60.5 & 56.8\\
				& 2/255 & 37.5 & 35.2 & 51.8 & 49.9\\
				& 4/255 & 35.5 & 29.3 & 40.6 & 36.1\\
				& 8/255 & 17.9 & 12.6 & 21.9 & 17.2\\
				\bottomrule
				
			\end{tabular}
		}
		% }
	\vspace{-6mm}
	\label{tab:5}
	\end{minipage}
\end{table}

\vspace{-2mm}
\subsubsection{Evaluation Metrics}
% 攻击的任务
% 攻击模模型 和防御模型 先说明对抗样本设定视觉上较小的度量通常8/255的无穷范数，主要介绍分类的指标，同时也说明分割的指标
In the context of our study, adversarial attacks aim to degrade the performance of neural networks by causing misclassifications or incorrect segmentations. For medical image classification tasks, we use classification accuracy and the Area Under the Receiver Operating Characteristic Curve (AUC) as the primary evaluation metrics, as they quantify the model's ability to correctly classify images and its discriminative power across various thresholds.

For image segmentation\textemdash especially critical in biomedical applications\textemdash it is essential to quantify how accurately the predicted segmentation maps correspond to the ground truth. To achieve this, we employ two widely recognized evaluation metrics: Mean Intersection over Union (mIoU) and Dice Coefficient. mIoU measures the average overlap between the predicted segmentation and the ground truth mask across all classes. The Dice coefficient, also known as the Sørensen-Dice index, is defined as twice the area of overlap divided by the total number of pixels in both the prediction and the ground truth. Both metrics evaluate the similarity between the predicted segmentation and the ground truth, providing insights into the precision of segmentation models. Thus, adversarial attacks against segmentation aims at reducing the mIoU or Dice coefficient between the generated segmentation mask and target mask, resulting in wrong segmentation results. Oppositely, adversarial defense focuses on keeping the network outputs of adversarial and clean examples as similar as possible. In other words, adversarial defense aims to enhancing the robustness of DNNs against adversaries. In this paper, we focus on building a unified benchmark for medical adversarial training to enhance the robustness of computer-aided diagnosis models.

Following the setting from RobustBench \cite{CroceASDFCM021}, we utilize ResNet-18 \cite{he2016deep} and MobileNetV2 \cite{sandler2018mobilenetv2} as target networks to conduct adversarial attack and defense for medical classification. For biomedical segmentation, we adopt U-Net \cite{ronneberger2015u} and SegNet \cite{badrinarayanan2017segnet} to generate segmentation masks. We use Stochastic Gradient Descent (SGD) optimizer with the Nesterov momentum factor \cite{Nesterov1983AMF} of 0.9 and the cyclic learning rate schedule \cite{smith2019super} with the maximum learning rate of 0.01. In this paper, we mainly consider the most common scenario, attacks under $\ell_{\infty}$-norm threat model. We conduct all the experiments on a single NVIDIA Tesla A100. We report the accuracies on clean examples as well as adversaries obtained by five strong adversarial attack methods: FGSM \cite{GoodfellowSS14}, PGD \cite{MadryMSTV18} with 20 steps with the step size of $1/255$, CW \cite{carlini2017towards}, Square Attack \cite{andriushchenko2020square}, and Auto Attack (AA) \cite{croce2020reliable}. Note that the maximum $\ell_{\infty}$-norm perturbation is set as $\epsilon=8/255$. For adversarial training, we adopt PGD to generate adversaries as the training data for 100 epochs.


\vspace{-2mm}
\subsection{Adversarial Attack}
\vspace{-1mm}
% 说明本文另外构建了针对医疗系统的攻击，将常规对抗攻击方法进行迁移，得到非常好的效果
In this section, we transfer several adversarial attack methods for natural images to the medical imaging domain under various attack scenarios for comprehensive evaluation. We also consider medical classification and segmentation in diverse biomedical imaging modalities. In addition to standard adversarial attack methods, we also explore the effect of Stabilized Medical Attack (SMA) \cite{QiGS0Z21_A17} that is tailored for medical images in diverse tasks.


\begin{table}[!t]
	\centering
	\begin{minipage}{0.54\textwidth}
	\centering
	\vspace{-0.5mm}
	\caption{No-box robustness (\%) against PGD attacks in different settings with U-Net for medical segmentation.}
	\vspace{-4mm}
	%\small
	\renewcommand{\arraystretch}{0.6}
	% \resizebox{0.75\linewidth}{!}{
		\resizebox{1\linewidth}{!}{  
			\begin{tabular}{cccccc}
				\toprule
				\multirow{2}{*}{Adversarial Loss}&\multirow{2}{*}{$\epsilon$}&\multicolumn{2}{c}{ISIC (Dermoscopy)}&\multicolumn{2}{c}{COVID-19 (X-ray)} \\
				\cmidrule(lr){3-4} \cmidrule(lr){5-6}
				&&mIOU&Dice&mIOU&Dice\\
				\midrule
				None & 0 & 0.818 & 0.883 & 0.977 & 0.988\\
				\midrule
				\multirow{4}{*}{BCE} & 1/255 & 0.801 & 0.863 & 0.961 & 0.979\\
				& 2/255 & 0.783 & 0.851 & 0.933 & 0.963\\
				& 4/255 & 0.755 & 0.823 & 0.870 & 0.922\\
				& 8/255 & 0.698 & 0.780 & 0.774 & 0.856\\
				\midrule
				\multirow{4}{*}{Dice} & 1/255 & 0.807 & 0.875 & 0.966 & 0.982\\
				& 2/255 & 0.797 & 0.866 & 0.954 & 0.976\\
				& 4/255 & 0.775 & 0.848 & 0.926 & 0.959\\
				& 8/255 & 0.722 & 0.804 & 0.883 & 0.931\\
				
				\bottomrule
				
			\end{tabular}
		}
		% }
	\vspace{-4mm}
	\label{tab:6}
	\end{minipage}
	\hfill
	\begin{minipage}{0.4\textwidth}
	\centering
	\footnotesize
	\caption{Average time cost comparison of adversary generation methods for diverse medical imaging tasks.}
	\vspace{-4mm}
	%\small
	\renewcommand{\arraystretch}{0.6}
	% \resizebox{0.75\linewidth}{!}{
	\resizebox{\linewidth}{!}{  
			\begin{tabular}{ccc}
				\toprule
				\multicolumn{3}{l}{\textbf{Medical Classification:}} \\
				\midrule
				Adversarial Type & Messidor (Fundoscopy) & ISIC (Dermoscopy) \\
				
				\midrule
				FGSM & 0.6 s & 1.9 s\\
				PGD & 22.6 s & 40.3 s\\
				CW & 33.7 s & 61.0 s\\
				SMA & 35.8 s & 69.5 s \\
				AA & 738.6 s & 1464.0 s\\
				Square Attack & 548.1 s & 1769.3 s\\
				\midrule
				\multicolumn{3}{l}{\textbf{Medical Segmentation:}} \\
				\midrule
				Adversarial Type & ISIC (Dermoscopy) & COVID-19 (X-ray) \\
				\midrule
				PGD-BCE & 78.6 s & 78.7 s\\
				PGD-Dice & 84.5 s & 84.8 s\\
				
				\bottomrule
				
			\end{tabular}
			%    }
 	}
	\vspace{-4mm}
	\label{tab:7}
	\end{minipage}
\end{table}

\begin{table}[!t]
	\centering
	\caption{White-box \textbf{B}inary (\textbf{B}) and \textbf{M}ulti-class (\textbf{M}) accuracy (\%) of adversarially trained medical classification models for fundoscopy and dermoscopy in different settings. }
	\vspace{-4mm}
	%\small
	\begin{minipage}{0.49\linewidth}
	\renewcommand{\arraystretch}{0.6}
		\resizebox{\linewidth}{!}{  
			\begin{tabular}{cccccccccccccc}
				\toprule
				\multicolumn{14}{l}{\textbf{Fundoscopy Classification on Messidor \cite{decenciere2014feedback}: }} \\
				\midrule
				\multirow{2}{*}{Adv. Type} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{PGD-AT} & \multicolumn{2}{c}{TRADES} & \multicolumn{2}{c}{MART} & \multicolumn{2}{c}{MPAdvT} & \multicolumn{2}{c}{HAT} \\
				\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14}
				&&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}\\
				\midrule
				None & 0 & 71.3 & 50.0 & 58.8 & 46.7 & 62.1 & 44.6 & 55.0 & 42.5 & 59.5 & 47.3 & 62.5 & 46.7 \\
				\midrule
				\multirow{4}{*}{FGSM} & 1/255 & 47.1 & 22.1 & 57.5 & 45.8 & 58.3 & 41.7 & 54.6 & 42.5 & 58.7 & 46.9 & 56.7 & 42.5 \\
				& 2/255 & 38.8 & 13.8 & 55.8 & 44.2 & 55.8 & 37.5 & 54.6 & 42.1 & 56.3 & 45.8 & 53.8 & 39.2 \\
				& 4/255 & 27.9 & 9.2 & 52.9 & 39.2 & 48.8 & 30.4 & 52.5 & 40.8 & 53.0 & 41.6 & 46.3 & 30.8 \\
				& 8/255 & 24.2 & 20.4 & 45.4 & 35.8 & 29.6 & 19.2 & 49.6 & 39.2 & 46.3 & 37.7 & 28.8 & 13.3 \\
				\midrule
				\multirow{4}{*}{PGD} & 1/255 & 21.3 & 12.5 & 57.5 & 45.8 & 58.3 & 41.7 & 54.6 & 42.5 & 58.7 & 46.9 & 56.7 & 42.5 \\
				& 2/255 & 8.8 & 4.6 & 55.8 & 42.9 & 55.8 & 37.5 & 54.6 & 42.1 & 56.3 & 45.8 & 53.8 & 38.3 \\
				& 4/255 & 1.7 & 0.4 & 52.6 & 36.7 & 46.7 & 29.2 & 52.5 & 40.4 & 52.8 & 39.7 & 46.3 & 27.9 \\
				& 8/255 & 0.0 & 0.0 & 43.3 & 30.0 & 25.8 & 15.4 & 49.2 & 37.1 & 44.5 & 34.3 & 21.3 & 11.3 \\
				\midrule
				\multirow{4}{*}{CW} & 1/255 & 15.8 & 12.5 & 57.5 & 45.4 & 58.3 & 40.8 & 54.6 & 42.5 & 58.7 & 46.3 & 56.7 & 42.5 \\
				& 2/255 & 7.1 & 6.7 & 55.4 & 41.3 & 55.8 & 35.8 & 54.6 & 41.7 & 55.9 & 43.7 & 53.8 & 36.7 \\
				& 4/255 & 1.3 & 2.1 & 52.5 & 35.4 & 46.7 & 27.1 & 52.5 & 40.4 & 52.7 & 37.5 & 46.3 & 24.6 \\
				& 8/255 & 0.0 & 0.0 & 42.5 & 27.9 & 26.3 & 13.3 & 49.2 & 36.3 & 43.6 & 30.8 & 22.5 & 7.9 \\
				\midrule
				\multirow{4}{*}{SMA} & 1/255 & 14.6 & 12.3 & 57.5 & 45.4 & 58.3 & 40.8 & 54.6 & 42.5 & 58.7 & 46.3 & 56.7 & 42.5 \\
				& 2/255 & 5.7 & 5.2 & 55.2 & 41.3 & 55.8 & 35.9 & 54.6 & 41.7 & 55.6 & 43.4 & 53.8 & 36.9 \\
				& 4/255 & 0.0 & 0.5 & 52.5 & 35.1 & 46.6 & 26.7 & 52.5 & 40.3 & 52.7 & 37.0 & 46.0 & 23.8 \\
				& 8/255 & 0.0 & 0.0 & 42.3 & 28.6 & 26.0 & 12.9 & 49.0 & 36.4 & 43.2 & 30.3 & 21.6 & 4.8 \\
				\midrule
				\multirow{4}{*}{AA} & 1/255 & 12.8 & 6.7 & 57.5 & 45.8 & 58.3 & 40.8 & 54.6 & 42.5 & 58.7 & 46.1 & 56.7 & 42.5 \\
				& 2/255 & 3.8 & 2.5 & 55.2 & 41.3 & 55.8 & 36.3 & 54.6 & 41.7 & 55.6 & 43.0 & 53.8 & 37.5 \\
				& 4/255 & 0.0 & 0.0 & 52.3 & 35.0 & 46.3 & 26.3 & 52.5 & 40.4 & 52.5 & 36.2 & 46.3 & 23.3 \\
				& 8/255 & 0.0 & 0.0 & 42.1 & 28.3 & 25.4 & 12.5 & 49.2 & 36.7 & 42.7 & 29.8 & 20.0 & 1.7 \\
				\bottomrule
			\end{tabular}
		}
	\end{minipage}
	\begin{minipage}{0.49\linewidth}
	\renewcommand{\arraystretch}{0.6}
		\resizebox{\linewidth}{!}{  
				\begin{tabular}{cccccccccccccc}
				\toprule
				\multicolumn{14}{l}{\textbf{Dermoscopy Classification on ISIC \cite{codella2018skin}:}} \\
				\midrule
				\multirow{2}{*}{Adv. Type} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{PGD-AT} & \multicolumn{2}{c}{TRADES} & \multicolumn{2}{c}{MART} & \multicolumn{2}{c}{MPAdvT} & \multicolumn{2}{c}{HAT} \\
				\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14}
				
				&&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}\\
				\midrule
				None & 0 & 64.9 & 60.0 & 61.2 & 52.0 & 62.3 & 52.9 & 61.1 & 49.6 & 61.2 & 52.8 & 68.6 & 57.1 \\
				\midrule
				\multirow{4}{*}{FGSM} & 1/255 & 41.5 & 12.5 & 58.0 & 48.0 & 58.1 & 49.5 & 58.9 & 45.8 & 58.3 & 49.4 & 63.3 & 51.1 \\
				& 2/255 & 37.6 & 4.8 & 55.6 & 44.4 & 53.5 & 44.0 & 56.7 & 42.4 & 55.9 & 45.7 & 58.8 & 46.8 \\
				& 4/255 & 35.7 & 4.5 & 49.2 & 38.0 & 44.9 & 35.3 & 51.6 & 33.7 & 49.8 & 38.6 & 48.5 & 35.3 \\
				& 8/255 & 34.5 & 8.0 & 39.6 & 25.5 & 33.1 & 22.3 & 40.9 & 21.7 & 40.3 & 26.2 & 37.5 & 20.7 \\
				\midrule
				\multirow{4}{*}{PGD} & 1/255 & 38.4 & 8.9 & 57.9 & 48.0 & 58.0 & 49.2 & 58.9 & 45.6 & 58.3 & 49.4 & 63.2 & 51.1 \\
				& 2/255 & 22.8 & 2.6 & 55.6 & 44.4 & 52.9 & 43.7 & 56.5 & 41.9 & 55.7& 45.7 & 57.6 & 45.9 \\
				& 4/255 & 12.5 & 0.8 & 48.4 & 36.8 & 42.0 & 32.8 & 49.9 & 31.2 & 49.2 & 37.3 & 44.4 & 33.6 \\
				& 8/255 & 12.8 & 0.4 & 35.1 & 22.4 & 21.5 & 14.4 & 34.5 & 14.9 & 38.0 & 24.3 & 25.7 & 14.4 \\
				\midrule
				\multirow{4}{*}{CW} & 1/255 & 37.5 & 9.1 & 57.9 & 47.9 & 58.0 & 49.1 & 58.9 & 45.1 & 58.3 & 49.2 & 63.2 & 50.5 \\
				& 2/255 & 22.5 & 2.5 & 55.6 & 43.6 & 52.9 & 43.2 & 56.5 & 41.3 & 55.7 & 44.9 & 57.6 & 45.3 \\
				& 4/255 & 9.3 & 0.9 & 48.4 & 36.0 & 41.9 & 31.9 & 49.9 & 30.9 & 49.1 & 36.6 & 44.4 & 32.8 \\
				& 8/255 & 2.1 & 0.4 & 36.3 & 22.7 & 21.9 & 14.3 & 34.7 & 15.2 & 37.4 & 23.0 & 26.5 & 13.9 \\
				\midrule
				\multirow{4}{*}{SMA} & 1/255 & 37.2 & 8.7 & 57.9 & 47.9 & 58.0 & 49.1 & 58.9 & 45.7 & 58.4 & 49.2 & 63.4 & 50.8 \\
				& 2/255 & 22.6 & 2.5 & 55.6 & 43.5 & 52.9 & 43.3 & 56.4 & 41.0 & 55.7 & 44.5 & 57.7 & 45.3 \\
				& 4/255 & 9.9 & 1.0 & 48.4 & 35.8 & 41.8 & 32.0 & 50.1 & 30.4 & 49.1 & 36.3 & 44.6 & 12.9 \\
				& 8/255 & 1.7 & 0.3 & 35.0 & 21.6 & 22.3 & 16.8 & 33.7 & 14.9 & 36.2 & 23.1 & 25.5 & 14.3 \\
				\midrule
				\multirow{4}{*}{AA} & 1/255 & 36.9 & 8.5 & 57.9 & 47.9 & 58.0 & 49.1 & 58.9 & 45.5 & 58.2 & 49.1 & 63.2 & 50.7 \\
				& 2/255 & 22.1 & 2.4 & 55.6 & 43.7 & 52.7 & 43.1 & 56.4 & 41.1 & 55.7 & 43.8 & 57.6 & 45.3 \\
				& 4/255 & 8.4 & 0.8 & 48.4 & 35.5 & 41.5 & 31.6 & 49.7 & 30.3 & 48.9 & 36.2 & 44.1 & 32.8 \\
				& 8/255 & 1.6 & 0.1 & 34.8 & 20.9 & 18.9 & 12.7 & 32.3 & 14.0 & 35.7 & 22.5 & 22.3 & 12.5 \\

				\bottomrule
				
			\end{tabular}
		}
		\end{minipage}
	\vspace{-4mm}
	\label{tab:8}
\end{table}

\begin{table}[!t]
\centering
\caption{White-box Multi-class accuracy (\%) of adversarially trained medical classification models for fundoscopy and dermoscopy. Adversarial perturbations are restricted within the $\ell_\infty$-norm radius $\epsilon=8/255$.}
\vspace{-4mm}
\renewcommand{\arraystretch}{0.6}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccccccccccc}
\toprule
&\multicolumn{8}{c}{\textbf{Fundoscopy Classification on Messidor \cite{decenciere2014feedback}}} &\multicolumn{8}{c}{\textbf{Dermoscopy Classification on ISIC \cite{codella2018skin}}} \\
\cmidrule(lr){2-9} \cmidrule(lr){10-17}
\multirow{2}{*}{Method} & \multicolumn{4}{c}{Wide-ResNet-28-10} & \multicolumn{4}{c}{MobileNetV2} & \multicolumn{4}{c}{Wide-ResNet-28-10} & \multicolumn{4}{c}{MobileNetV2} \\ 
\cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13} \cmidrule(lr){14-17}
& Clean & PGD & CW & AA & Clean & PGD & CW & AA & Clean & PGD & CW & AA & Clean & PGD & CW & AA \\
\midrule
NAT & 63.7 & 0.0 & 0.0 & 0.0 & 48.5 & 0.0 & 0.0 & 0.0 & 69.6 & 0.0 & 0.0 & 0.0 & 57.3 & 0.0 & 0.0 & 0.0 \\

PGD-AT \cite{MadryMSTV18} & 57.9 & 45.7 & 42.3 & 41.5 & 44.9 & 28.3 & 25.4 & 24.2 & 62.4 & 35.5 & 32.7 & 31.8 & 50.6 & 26.7 & 23.9 & 23.2 \\

TRADES \cite{zhang2019theoretically} & 58.5 & 46.0 & 44.5 & 42.8 & 43.6 & 27.5 & 26.3 & 25.0 & 63.1 & 35.9 & 33.2 & 32.3 & 51.3 & 27.1 & 24.5 & 23.7 \\

MART \cite{wang2019improving} & 56.6 & 44.7 & 41.4 & 40.3 & 40.8 & 30.4 & 27.5 & 26.8 & 61.8 & 37.6 & 34.9 & 33.8 & 49.2 & 28.5 & 26.0 & 25.3 \\

MPAdvT \cite{xu2021towards_D9} & 59.2 & 47.1 & 45.9 & 43.7 & 45.6 & 31.3 & 27.9 & 27.0 & 63.7 & 37.3 & 35.1 & 34.0 & 52.8 & 28.3 & 26.2 & 25.5 \\

HAT \cite{RadeM22} & 61.2 & 47.3 & 45.8 & 43.9 & 46.1 & 31.8 & 28.2 & 27.5 & 65.2 & 38.4 & 36.3 & 34.7 & 53.7 & 30.2 & 27.8 & 27.3 \\

\bottomrule
\end{tabular}
}
\vspace{-4mm}
\label{supp-tab:1}
\end{table}

\subsubsection{White-box Attack}
% 白盒的分类/分割攻击，给一张图4行，上两行分类，下面两行分割，每列是扰动大小和正确类预测结果的置信度。
To begin with, we present several cases of the adversarial attack against diverse medical diagnosis tasks under different attack strengths in Fig. \ref{fig:3}. We can easily observe that computer-aided diagnosis models are extremely vulnerable to adversarial examples, even with a small adversarial perturbation size. The medical classification models are misguided by adversaries to make high-confidence misdiagnoses, which can interfere with clinical judgment. Moreover, incorrect segmentation results induced by adversaries can lead to false treatment suggestions.

% 同时也给出两张表分别表示分类(分类需要考虑二分类和多分类)和分割，包含有下降率
To comprehensively measure the adversarial effect on medical classification models, we report the accuracy against several white-box attacks in both binary and multi-class classification settings, as shown in Table \ref{tab:3}. Note that we adopt the cross-entropy loss function for adversary generation. It can be seen that there exists a significant plunge of accuracy as increasing the adversarial perturbation size. In addition, we can easily observe that multi-class classification models suffer from a more severe accuracy drop than binary classification models. However, existing medical adversarial defense methods mainly focus on binary diagnosis, which is relatively easy to construct robust models. In this paper, we claim that constructing robustness for multi-class medical classification is more challenging and can further be generalized to various clinical scenarios.

Beyond adversarial attacks against medical classification, we also evaluate the performance of medical segmentation against PGD-based adversaries, as shown in Table \ref{tab:4}. We adopt Binary Cross-Entropy (BCE) loss, IOU loss and Dice loss as the adversarial loss for adversary generation. Intuitively, optimizing IOU loss during adversary generation significantly degrades mIOU, while using Dice loss as the adversarial loss notably reduces the Dice value.



\begin{table}[!t]
	\centering
	\caption{White-box robustness against PGD attacks in diverse settings using U-Net for medical segmentation.}
	\vspace{-4mm}
	\begin{minipage}{0.49\linewidth}
	\renewcommand{\arraystretch}{0.6}
		\resizebox{1\linewidth}{!}{  
			\begin{tabular}{ccccccc}
				\toprule
				\multirow{2}{*}{Task} & \multirow{2}{*}{Adversarial Loss} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{ADV} \\
				\cmidrule(lr){4-5} \cmidrule(lr){6-7}
				& & & mIOU & Dice & mIOU & Dice \\
				\midrule
				\multirow{14}{*}{\makecell{ISIC \\ (Dermoscopy)}} & None & 0 & 0.818 & 0.883 & 0.803 & 0.867 \\
				\cmidrule(lr){2-7}
				& \multirow{4}{*}{BCE} & 1/255 & 0.580 & 0.680 & 0.788 &0.855 \\
				& & 2/255 & 0.405 & 0.517 & 0.773 & 0.842 \\
				& & 4/255 & 0.248 & 0.354 & 0.733 & 0.810 \\
				& & 8/255 & 0.167 & 0.255 & 0.601 & 0.700 \\
				\cmidrule(lr){2-7}
				& \multirow{4}{*}{IOU} & 1/255 & 0.542 & 0.672 & 0.783 & 0.854 \\
				& & 2/255 & 0.328 & 0.487 & 0.762 & 0.840 \\
				& & 4/255 & 0.145 & 0.305 & 0.716 & 0.807 \\
				& & 8/255 & 0.073 & 0.193 & 0.573 & 0.698 \\
				\cmidrule(lr){2-7}
				& \multirow{4}{*}{Dice} & 1/255 & 0.561 & 0.663 & 0.786 & 0.853 \\
				& & 2/255 & 0.340 & 0.450 & 0.767 & 0.838 \\
				& & 4/255 & 0.158 & 0.243 & 0.723 & 0.803 \\
				& & 8/255 & 0.082 & 0.140 & 0.594 & 0.694 \\
				\bottomrule
			\end{tabular}
		}
	\end{minipage}
	\begin{minipage}{0.47\linewidth}
		\renewcommand{\arraystretch}{0.6}
		\resizebox{1\linewidth}{!}{ 
			\begin{tabular}{ccccccc}
				\toprule
				\multirow{2}{*}{Task} & \multirow{2}{*}{Adversarial Loss} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{ADV} \\
				\cmidrule(lr){4-5} \cmidrule(lr){6-7}
				& & & mIOU & Dice & mIOU & Dice \\
				\midrule
				\multirow{14}{*}{\makecell{COVID-19 \\ (X-ray)}} & None & 0 & 0.977 & 0.988 & 0.964 & 0.981 \\
				\cmidrule(lr){2-7}
				& \multirow{4}{*}{BCE} & 1/255 & 0.747 & 0.840 & 0.949 & 0.973 \\
				& & 2/255 & 0.559 & 0.690 & 0.931 & 0.963 \\
				& & 4/255 & 0.355 & 0.492 & 0.890 & 0.940 \\
				& & 8/255 & 0.230 & 0.350 & 0.812 & 0.887 \\
				\cmidrule(lr){2-7}
				& \multirow{4}{*}{IOU} & 1/255 & 0.695 & 0.829 & 0.943 & 0.987 \\
				& & 2/255 & 0.454 & 0.648 & 0.912 & 0.969 \\
				& & 4/255 & 0.239 & 0.418 & 0.865 & 0.938 \\
				& & 8/255 & 0.136 & 0.283 & 0.753 & 0.874 \\
				\cmidrule(lr){2-7}
				& \multirow{4}{*}{Dice} & 1/255 & 0.704 & 0.807 & 0.964 & 0.981 \\
				& & 2/255 & 0.473 & 0.610 & 0.923 & 0.958 \\
				& & 4/255 & 0.265 & 0.391 & 0.880 & 0.932 \\
				& & 8/255 & 0.152 & 0.246 & 0.774 & 0.860 \\
				
				\bottomrule
				
			\end{tabular}
		}
	\end{minipage}
	\vspace{-4mm}
	\label{tab:9}
\end{table}





\subsubsection{Black-box Attack}
% 黑盒攻击，分类给出一张表同时含有Square和迁移攻击的
% 分割给出一张表说明迁移的效果
Apart from white-box adversarial attacks against medical image analysis, we also evaluate the performance of computer-aided diagnosis models against black-box adversarial attacks, which can be a more practical attack scenario. We report the accuracy of medical classifiers against both Black-box and Restricted black-box (No-box) attacks in Table \ref{tab:5}. Note that the no-box attacks are conducted by transferring adversaries generated against a MobileNetV2 \cite{sandler2018mobilenetv2} model to the target ResNet-18 \cite{he2016deep} classification model. Due to the accessibility to outputs of the target model, the black-box adversarial attack can achieve a better attack success rate than no-box attacks. In the meantime, we can also observe that conducting no-box attacks for multi-class medical classification models is much harder than for binary classifiers. The plausible reason is that multi-class classifiers have more complex classification decision boundary, which varies with each architecture.

We also evaluate the effectiveness of restricted black-box adversarial attacks against medical segmentation models, as shown in Table \ref{tab:6}. The no-box attacks are obtained by transferring PGD-based adversarial examples generated against the SegNet \cite{badrinarayanan2017segnet} model to directly attack the target U-Net \cite{ronneberger2015u} segmentation model. Despite adopting the Dice loss as the adversarial loss achieves a superior performance in the white-box scenario, attacking the BCE loss can obtain more transferable adversarial examples for medical segmentation in the no-box setting.

\begin{table}[!t]
	\centering
	\footnotesize
	\caption{White-box robustness under different attack configurations using ResNet-18 for multi-label medical classification. We report the AUC score (\%) of thorax disease classification models against PGD attack.}
	\vspace{-4mm}
	%\small
	\renewcommand{\arraystretch}{0.6}
	% \resizebox{0.75\linewidth}{!}{
		\resizebox{0.8\linewidth}{!}{  
			\begin{tabular}{cccccccc}
				\toprule
				\multirow{2}{*}{Task} & \multirow{2}{*}{Adversarial Type} & \multirow{2}{*}{$\epsilon$} & & \multicolumn{2}{c}{Training $\epsilon=8/255$} & \multicolumn{2}{c}{Training $\epsilon=4/255$} \\
				\cmidrule(lr){5-6} \cmidrule(lr){7-8}
				&&& NAT & PGD-AT & MPAdvT & PGD-AT & MPAdvT \\
				\midrule
				\multirow{5}{*}{\makecell{ChestX-ray 14 \cite{wang2017chestx} \\ (Multi-label)}} & None & 0 & 71.0 & 71.4 & 70.2 & 73.6 & 72.3 \\
				\cmidrule(lr){2-8}
				& \multirow{4}{*}{PGD} & 1/255 & 23.3 & 67.1 & 66.7 & 65.9 & 64.8 \\
				& & 2/255 & 15.7 & 62.6 & 62.4 & 60.2 & 59.6 \\
				& & 4/255 & 14.0 & 53.1 & 52.8 & 51.7 & 51.0 \\
				& & 8/255 & 11.7 & 35.5 & 34.9 & 30.4 & 29.6 \\
				
				\bottomrule
				
			\end{tabular}
		}
		% }
	\vspace{-4mm}
	\label{tab:10}
\end{table}

\begin{table}[!t]
	\centering
	\renewcommand{\arraystretch}{0.6}
	\caption{Robust medical CLIP with AUC evaluation on clean and PGD-20 adversaries in zero-shot setting.}
	\vspace{-0.4cm}
	\resizebox{0.5\linewidth}{!}{
		\begin{tabular}{ccccccc}
			\toprule
			\multirow{2}{*}{Method} & \multicolumn{2}{c}{ChestXray14} & \multicolumn{2}{c}{CheXpert} & \multicolumn{2}{c}{PadChest} \\
			\cmidrule(l){2-3} \cmidrule(l){4-5} \cmidrule(l){6-7}
			& Clean & PGD & Clean & PGD & Clean & PGD \\
			\midrule
			TeCoA \cite{MaoGYWV23} & 0.674 & 0.526 & 0.857 & 0.685 & 0.602 & 0.483 \\
			PMG-FT \cite{wang2024pre} & 0.692 & 0.538 & 0.850 & 0.688 & 0.619 & 0.495 \\
			FARE \cite{schlarmann2024robust} & 0.702 & 0.541 & 0.866 & 0.694 & 0.627 & 0.505 \\
			
			\bottomrule
		\end{tabular}
 	}
	\label{supp-tab:3}
	\vspace{-0.4cm}
\end{table}

\subsubsection{Time Cost Analysis for Adversarial Attacks}
Furthermore, we present an analysis of the computational cost related to adversary generation for medical imaging. Specifically, we measure the time cost for generating adversaries by various methods for diverse medical imaging tasks, as shown in Table \ref{tab:7}. Note that we adopt ResNet-18 for medical classification and U-Net for medical segmentation, respectively. It can be seen that both Auto attack and Square attack require relatively considerable computing resources to produce strong adversaries. The primary time cost gap of two PGD attacks for medical segmentation comes from the efficiency of different loss computations.

\vspace{-3mm}
\subsection{Adversarial Training for Defense}
% 同攻击一样说明这里，防御使用对抗训练，由于对抗训练的种优势，本文构建了一个对抗训练的公平度量的benchmark用来做防御。
In this section, we construct a unified benchmark with the most effective defense method, adversarial training to establish robustness for medical diagnosis systems for future research. Specifically, we extend several existing adversarial training methods for the natural imaging domain to medical imaging analysis. We also measure the adversarial robustness of adversarially trained diagnosis models under various attack scenarios for systematic evaluation. In addition to standard adversarial attack methods, we investigate the effect of Multi-Perturbations Adversarial Training (MPAdvT) \cite{xu2021towards_D9}, which is specially designed for deep diagnostic models in diverse tasks.

\begin{table}[!t]
	\centering
	\caption{Black-box \textbf{B}inary (\textbf{B}) and \textbf{M}ulti-class (\textbf{M}) accuracy (\%) of ResNet-18-based adversarially trained medical classification models for fundoscopy and dermoscopy.}
	\vspace{-4mm}
	%\small
	\begin{minipage}{0.49\linewidth}
	\renewcommand{\arraystretch}{0.6}
		\resizebox{\linewidth}{!}{  
			\begin{tabular}{cccccccccccccc}
				\toprule
				\multicolumn{14}{l}{\textbf{Fundoscopy Classification on Messidor \cite{decenciere2014feedback}: }} \\
				\midrule
				\multirow{2}{*}{Adv. Type} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{PGD-AT} & \multicolumn{2}{c}{TRADES} & \multicolumn{2}{c}{MART} & \multicolumn{2}{c}{MPAdvT} & \multicolumn{2}{c}{HAT} \\
				\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14}
				&&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}\\
				\midrule
				None & 0 & 71.3 & 50.0 & 58.8 & 46.7 & 62.1 & 44.6 & 55.0 & 42.5 & 59.5 & 47.3 & 62.5 & 46.7 \\
				\midrule
				\multirow{4}{*}{\makecell{Square Attack \cite{andriushchenko2020square} \\ (\textbf{Black-box}) }} & 1/255 & 13.3 & 9.6 & 57.5 & 45.8 & 58.3 & 42.1 & 54.6 & 42.5 & 57.9 & 46.0 & 56.7 & 42.5 \\
				& 2/255 & 5.8 & 3.8 & 55.8 & 42.9 & 55.8 & 37.9 & 54.6 & 42.5 & 56.2 & 43.8 & 56.7 & 38.3 \\
				& 4/255 & 0.0 & 0.4 & 52.9 & 37.9 & 48.8 & 29.6 & 52.5 & 41.7 & 53.5 & 38.6 & 46.7 & 24.6 \\
				& 8/255 & 0.0 & 0.0 & 43.8 & 29.2 & 28.8 & 16.3 & 50.0 & 40.4 & 44.3 & 30.7 & 21.3 & 6.3 \\
				\midrule
				\multirow{4}{*}{\makecell{PGD \cite{MadryMSTV18} \\ (\textbf{No-box})}} & 1/255 & 47.5 & 41.3 & 58.8 & 46.7 & 61.3 & 43.8 & 55.0 & 42.5 & 59.5 & 47.3 & 60.0 & 46.7 \\
				& 2/255 & 36.7 & 37.9 & 59.2 & 46.7 & 58.3 & 42.9 & 55.4 & 42.5 & 59.3 & 47.0 & 59.2 & 45.4 \\
				& 4/255 & 27.9 & 27.9 & 56.7 & 46.3 & 56.7 & 40.4 & 55.4 & 42.9 & 57.4 & 46.7 & 57.9 & 45.8 \\
				& 8/255 & 10.4 & 18.7 & 55.8 & 44.6 & 50.0 & 36.3 & 55.8 & 42.5 & 56.1 & 44.9 & 50.0 & 40.8 \\
				\midrule
				\multirow{4}{*}{\makecell{CW \cite{carlini2017towards} \\ (\textbf{No-box})}} & 1/255 & 42.1 & 37.1 & 58.8 & 46.7 & 61.3 & 43.3 & 55.0 & 42.5 & 59.5 & 47.3 & 60.0 & 46.3 \\
				& 2/255 & 35.4 & 37.1 & 59.2 & 46.7 & 58.3 & 42.5 & 55.4 & 42.5 & 59.3 & 46.9 & 59.2 & 45.4 \\
				& 4/255 & 35.4 & 40.4 & 56.7 & 45.8 & 56.7 & 40.4 & 55.4 & 42.9 & 57.2 & 46.5 & 57.9 & 43.8 \\
				& 8/255 & 19.6 & 12.9 & 56.3 & 44.2 & 50.0 & 36.3 & 55.8 & 42.5 & 56.1 & 44.2 & 52.1 & 40.8 \\
				
				\midrule
				\multirow{4}{*}{\makecell{SMA \cite{QiGS0Z21_A17} \\ (\textbf{No-box})}} & 1/255 & 43.0 & 38.6 & 58.8 & 46.6 & 61.0 & 43.7 & 55.0 & 42.5 & 59.5 & 47.3 & 59.4 & 46.0 \\
				& 2/255 & 37.5 & 35.2 & 58.5 & 46.1 & 58.2 & 41.9 & 54.8 & 42.5 & 59.2 & 46.6 & 57.9 & 44.4 \\
				& 4/255 & 35.5 & 29.3 & 56.0 & 45.8 & 56.8 & 39.8 & 54.3 & 42.0 & 57.1 & 46.3 & 56.2 & 43.0 \\
				& 8/255 & 17.9 & 12.6 & 55.7 & 44.1 & 49.6 & 35.5 & 53.7 & 41.5 & 56.0 & 43.7 & 50.9 & 40.1 \\
				
				\bottomrule
			\end{tabular}
		}
	\end{minipage}
	\begin{minipage}{0.49\linewidth}
		\renewcommand{\arraystretch}{0.6}
		\resizebox{\linewidth}{!}{  
			\begin{tabular}{cccccccccccccc}

				\toprule
				
				\multicolumn{14}{l}{\textbf{Dermoscopy Classification on ISIC \cite{codella2018skin}:}} \\
				\midrule
				\multirow{2}{*}{Adversarial Type} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{PGD-AT} & \multicolumn{2}{c}{TRADES} & \multicolumn{2}{c}{MART} & \multicolumn{2}{c}{MPAdvT} & \multicolumn{2}{c}{HAT} \\
				\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14}
				&&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}\\
				\midrule
				None & 0 & 64.9 & 60.0 & 61.2 & 52.0 & 62.3 & 52.9 & 61.1 & 49.6 & 61.2 & 52.8 & 68.6 & 57.1 \\
				\midrule
				\multirow{4}{*}{\makecell{Square Attack \cite{andriushchenko2020square} \\ (\textbf{Black-box}) }} & 1/255 & 45.1 & 19.3 & 58.4 & 48.0 & 58.8 & 50.3 & 59.1 & 45.9 & 59.0 & 48.7 & 63.9 & 51.6 \\
				& 2/255 & 30.3 & 6.0 & 56.7 & 44.4 & 54.7 & 45.6 & 57.2 & 42.7 & 57.2 & 45.5 & 59.7 & 47.5 \\
				& 4/255 & 15.6 & 1.6 & 50.7 & 37.9 & 45.5 & 36.7 & 52.8 & 34.3 & 52.4 & 39.1 & 46.7 & 36.3 \\
				& 8/255 & 2.9 & 0.5 & 40.0 & 23.5 & 27.6 & 18.8 & 41.9 & 19.7 & 43.5 & 24.0 & 28.9 & 19.2 \\
				\midrule
				\multirow{4}{*}{\makecell{PGD \cite{MadryMSTV18} \\ (\textbf{No-box})}} & 1/255 & 59.1 & 58.5 & 60.8 & 51.9 & 60.1 & 52.5 & 60.0 & 49.6 & 61.2 & 52.7 & 66.1 & 56.0 \\
				& 2/255 & 49.7 & 55.6 & 60.0 & 51.5 & 59.1 & 52.0 & 59.3 & 48.9 & 60.8 & 51.7 & 64.0 & 55.1 \\
				& 4/255 & 35.3 & 50.4 & 59.1 & 50.5 & 54.0 & 51.3 & 57.3 & 47.6 & 59.6 & 51.0 & 58.5 & 54.0 \\
				& 8/255 & 21.2 & 38.5 & 58.1 & 47.5 & 47.3 & 47.6 & 55.2 & 45.6 & 58.4 & 47.8 & 52.1 & 50.7 \\
				\midrule
				\multirow{4}{*}{\makecell{CW \cite{carlini2017towards} \\ (\textbf{No-box})}} & 1/255 & 59.1 & 58.0 & 60.8 & 51.9 & 60.1 & 52.5 & 60.0 & 49.6 & 61.2 & 52.6 & 66.1 & 56.1 \\
				& 2/255 & 48.9 & 52.9 & 60.0 & 52.0 & 59.1 & 52.0 & 59.5 & 49.1 & 60.7 & 51.3 & 64.0 & 54.9 \\
				& 4/255 & 34.8 & 41.7 & 59.3 & 50.9 & 54.7 & 51.2 & 57.2 & 48.3 & 59.5 & 51.2 & 58.1 & 53.7 \\
				& 8/255 & 20.5 & 19.3 & 58.5 & 48.0 & 48.3 & 48.5 & 55.6 & 46.7 & 58.2 & 48.0 & 52.8 & 50.7 \\
				\midrule
				\multirow{4}{*}{\makecell{SMA \cite{QiGS0Z21_A17} \\ (\textbf{No-box})}} & 1/255 & 60.5 & 56.8 & 60.7 & 51.3 & 59.9 & 52.6 & 60.3 & 49.4 & 61.2 & 52.6 & 65.8 & 56.0 \\
				& 2/255 & 51.8 & 49.9 & 59.7 & 50.6 & 58.9 & 52.2 & 59.4 & 48.7 & 60.8 & 51.3 & 63.9 & 54.6 \\
				& 4/255 & 40.6 & 36.1 & 58.9 & 49.5 & 54.4 & 51.1 & 56.9 & 48.0 & 59.7 & 51.0 & 58.0 & 53.4 \\
				& 8/255 & 21.9 & 17.2 & 57.3 & 47.4 & 50.3 & 48.2 & 55.2 & 46.2 & 58.5 & 48.5 & 51.7 & 50.5 \\

				\bottomrule
				
			\end{tabular}
		}
	\end{minipage}
	\vspace{-0.6cm}
	\label{tab:11}
\end{table}



\subsubsection{Defense against White-box Attacks}
% 白盒二分类和多分类的对抗训练的防御效果
% 白盒分割的对抗训练防御效果
% 单标签分类
Cutting-edge adversarial training methods mainly focus on augmenting adversaries as training data to obtain a robust decision boundary for both clean and adversarial examples. In this paper, we primarily extend the adversarial robustness to the biomedical imaging domain by transferring four widely-used adversarial training methods: PGD-AT \cite{MadryMSTV18}, TRADES \cite{zhang2019theoretically}, MART \cite{wang2019improving}, and HAT \cite{RadeM22}. The adversarial robustness results for both binary and multi-class medical classification are reported in Table \ref{tab:8}. Note that the robustness results for NAtural Training (NAT) are also provided for reference. We can observe that adversarially trained models can well preserve the robustness under different attack configurations. In the meantime, the naturally trained medical models are vulnerable to high-strength adversarial attacks, especially for the multi-class classification scenario. Note that both PGD-AT and MART can relatively achieve better robustness than TRADES and HAT for medical classification. The plausible reason for the robustness gap is that these two types of methods focus on different inner adversary generation styles. Although adversarial training can significantly enhance the adversarial robustness, the clean performance still suffers from a slight drop compared with naturally trained models.

Furthermore, we expand our experimental analysis to include a broader range of model architectures, in addition to the initial evaluation on ResNet-18 \cite{he2016deep}. We now present results from experiments involving adversarial training on Wide-ResNet-28-10 (WRN-28) \cite{ZagoruykoK16} and MobileNetV2 (MNV2) \cite{sandler2018mobilenetv2}, applied to medical classification models for fundoscopy and dermoscopy across diverse settings. The evaluation results, detailed in Table \ref{supp-tab:1}, reveal that adversarial robustness correlates strongly with network capacity. Notably, the robustness of Wide-ResNet, with its deeper and wider architecture, surpasses that of the more lightweight MobileNet in both natural performance and robustness. While our current evaluations are primarily based on computational metrics, it is significant to note that the ground-truth labels in the datasets we employed are derived from clinical assessments by medical experts, following their respective database construction criteria \cite{decenciere2014feedback, codella2018skin, wang2017chestx, chowdhury2020can}. This further underscores the clinical relevance of our computational findings of both adversarial attacks and defenses in the context of medical image analysis.

Besides establishing adversarial robustness for single-label classification models, we also conduct adversarial training for medical segmentation tasks. We present the performance for adversarially trained medical segmentation models under different attack configurations in Table \ref{tab:9}. Note that we adopt the most widely-used adversarial training method, PGD-AT, to enhance the intrinsic network robustness of medical segmentation models. It can be seen that the adversarially trained segmentation models can achieve better robustness against different degrees of adversarial attacks than naturally trained models. Intuitively, we observe that optimizing the IOU loss results in a more significant degradation of the mIOU, whereas employing the Dice loss as the objective function leads to a greater decrease in the Dice value.

Generally, adversarial training can induce a mild performance drop for clean examples. However, we observe an intriguing phenomenon for multi-label classification in the context of medical images, as shown in Table \ref{tab:10}. We report AUC of multi-label thorax disease classification models against adversarial examples of different attack strengths. In particular, we observe that reducing the perturbation radius during adversarial training implicitly enhances natural performance, although it may reduce adversarial robustness. This observation suggests that utilizing a lower adversarial perturbation contributes to improved natural accuracy. We attribute this enhancement to the similar background and biomedical structures inherent in medical imaging data, where small perturbations can effectively augment original images, leading to better generalization ability. Similarly, Hu \textit{et al.} \cite{hu2024protecting} introduced targeted adversarial samples with rectal artifact-pattern noise during training to enhance the natural performance of prostate cancer classification using MRI data.


\subsubsection{Extension with Medical CLIP for Zero-Shot Robustness}
We here expand our analysis to include a comprehensive evaluation of the generalization capabilities of adversarial defense methods across diverse medical imaging datasets (distributions) and modalities. Specifically, we investigated the zero-shot adversarial robustness of computer-aided medical imaging diagnosis using the standard vision-language model CLIP, particularly within the context of chest X-ray imaging (radiology) paired with multi-label datasets. In this challenging zero-shot adversarial robustness setup \cite{MaoGYWV23}, attackers have unrestricted access to ground truth data from new datasets at the inference stage, while defenders, with no prior exposure to these datasets, are required to maintain robustness against all unforeseen adversarial images derived from them.

Following the methodology in \cite{tiu2022expert}, we utilized a Vision Transformer (ViT-B/16)-based CLIP model pre-trained specifically on radiology datasets. We conducted adversarial fine-tuning on the MIMIC dataset\textemdash a comprehensive collection of chest radiographs paired with detailed radiology text reports\textemdash to refine the CLIP model using different adversarial training approaches \cite{MaoGYWV23, wang2024pre, schlarmann2024robust}. Subsequently, we evaluated both the natural and robust accuracy of these adversarially fine-tuned CLIP models across three multi-label radiology datasets: ChestXray14 \cite{wang2017chestx}, CheXpert \cite{irvin2019chexpert}, and PadChest \cite{bustos2020padchest} in the zero-shot setting, as presented in Table \ref{supp-tab:3}. Notably, the PadChest dataset poses a significant challenge due to its long-tail distribution of 192 diseases, including rare conditions. Our findings indicate that the zero-shot robustness of medical vision-language models generalizes effectively to diverse biomedical databases. This suggests that models trained on specific modalities can maintain their robustness when applied to different datasets without additional fine-tuning, thus demonstrating the potential for broader applicability of adversarial defense methods across various medical imaging datasets (distributions).



\begin{table}[!t]
	\centering
	\caption{No-box robustness (\%) of naturally trained and adversarially trained medical segmentation models for dermoscopy and X-ray against PGD attacks using U-Net.}
	\vspace{-3mm}
	\begin{minipage}{0.49\linewidth}
	\renewcommand{\arraystretch}{0.6}
		\resizebox{1\linewidth}{!}{  
			\begin{tabular}{ccccccc}
				\toprule
				\multirow{2}{*}{Task} & \multirow{2}{*}{Adversarial Loss} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{ADV} \\
				\cmidrule(lr){4-5} \cmidrule(lr){6-7}
				& & & mIOU & Dice & mIOU & Dice \\
				\midrule
				\multirow{14}{*}{\makecell{ISIC \cite{codella2018skin} \\ (Dermoscopy)}} & None & 0 & 0.818 & 0.883 & 0.803 & 0.867 \\
				\cmidrule(lr){2-7}
				& \multirow{4}{*}{BCE} & 1/255 & 0.801 & 0.863 & 0.800 & 0.865 \\
				& & 2/255 & 0.783 & 0.851 & 0.799 & 0.864 \\
				& & 4/255 & 0.755 & 0.823 & 0.797 & 0.862 \\
				& & 8/255 & 0.698 & 0.780 & 0.794 & 0.860 \\
				\cmidrule(lr){2-7}
				& \multirow{4}{*}{IOU} & 1/255 & 0.805 & 0.881 & 0.802 & 0.863 \\
				& & 2/255 & 0.791 & 0.872 & 0.800 & 0.860 \\
				& & 4/255 & 0.760 & 0.859 & 0.799 & 0.854 \\
				& & 8/255 & 0.704 & 0.823 & 0.797 & 0.847 \\
				\cmidrule(lr){2-7}
				& \multirow{4}{*}{Dice} & 1/255 & 0.807 & 0.875 & 0.801 & 0.865 \\
				& & 2/255 & 0.797 & 0.866 & 0.800 & 0.864 \\
				& & 4/255 & 0.775 & 0.848 & 0.798 & 0.863 \\
				& & 8/255 & 0.722 & 0.804 & 0.796 & 0.861 \\
				\bottomrule
			\end{tabular}
		}
	\end{minipage}
	\begin{minipage}{0.49\linewidth}
		\renewcommand{\arraystretch}{0.6}
		\resizebox{1\linewidth}{!}{  
			\begin{tabular}{ccccccc}
				\toprule
				\multirow{2}{*}{Task} & \multirow{2}{*}{Adversarial Loss} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{ADV} \\
				\cmidrule(lr){4-5} \cmidrule(lr){6-7}
				& & & mIOU & Dice & mIOU & Dice \\
				\midrule
				\multirow{14}{*}{\makecell{COVID-19 \cite{chowdhury2020can} \\ (X-ray)}} & None & 0 & 0.977 & 0.988 & 0.964 & 0.981 \\
				\cmidrule(lr){2-7}
				& \multirow{4}{*}{BCE} & 1/255 & 0.961 & 0.979 & 0.963 & 0.981 \\
				& & 2/255 & 0.933 & 0.963 & 0.963 & 0.981 \\
				& & 4/255 & 0.870 & 0.922 & 0.963 & 0.980 \\
				& & 8/255 & 0.774 & 0.856 & 0.963 & 0.980 \\
				\cmidrule(lr){2-7}
				& \multirow{4}{*}{IOU} & 1/255 & 0.964 & 0.984 & 0.963 & 0.981 \\
				& & 2/255 & 0.949 & 0.979 & 0.963 & 0.980 \\
				& & 4/255 & 0.885 & 0.966 & 0.962 & 0.978 \\
				& & 8/255 & 0.831 & 0.940 & 0.960 & 0.975 \\
				\cmidrule(lr){2-7}
				& \multirow{4}{*}{Dice} & 1/255 & 0.966 & 0.982 & 0.963 & 0.981 \\
				& & 2/255 & 0.954 & 0.976 & 0.963 & 0.981 \\
				& & 4/255 & 0.926 & 0.959 & 0.963 & 0.981 \\
				& & 8/255 & 0.883 & 0.931 & 0.963 & 0.980 \\
				
				\bottomrule
				
			\end{tabular}
		}
	\end{minipage}
	\vspace{-13pt}
	\label{tab:12}
\end{table}

\begin{table}[!t]
	\centering
	\footnotesize
	%\vspace{-1mm}
	\caption{Time cost comparison of medical adv. training methods. We report the training time per epoch.}
	\vspace{-4mm}
	%\small
	\renewcommand{\arraystretch}{0.5}
	 \resizebox{0.4\linewidth}{!}{
		%    \resizebox{\linewidth}{!}{  
			\begin{tabular}{ccc}
				\toprule
				\multicolumn{3}{l}{\textbf{Medical Classification:}} \\
				\midrule
				Method & Messidor (Fundoscopy) & ISIC (Dermoscopy) \\
				
				\midrule
				NAT & 16.2 s & 17.3 s\\
				PGD-AT & 53.5 s & 65.1 s\\
				TRADES & 56.0 s & 67.7 s\\
				MART & 55.0 s & 65.9 s\\
				MPAdvT & 54.7 s & 65.5 s \\
				HAT & 62.0 s & 74.9 s\\
				\midrule
				\multicolumn{3}{l}{\textbf{Medical Segmentation:}} \\
				\midrule
				Adversarial Type & ISIC (Dermoscopy) & COVID-19 (X-ray) \\
				\midrule
				NAT & 13.8 s & 14.4 s\\
				PGD-AT & 148.5 s & 155.0 s\\
				
				\bottomrule
				
			\end{tabular}
			%    }
		 }
	\vspace{-6mm}
	\label{tab:13}
\end{table}

\subsubsection{Defense against Black-box Attacks}
% 黑盒二分类和多分类的对抗训练的防御效果
% 黑盒分割的对抗训练防御效果
Besides the evaluation of white-box attacks against robust diagnosis models, we also emphasize the importance of defense against black-box attacks on account of real-world scenarios. Here, we conduct black-box and no-box adversarial attacks for evaluations to simulate medical defense in clinical practice. We commence with the evaluation of robust diagnosis models in both binary and multi-class classification settings (see Table \ref{tab:11}). The adversarially trained diagnosis models effectively remain robust against adversaries of different attack strengths. We also observe a similar trend of black-box accuracy to white-box one as the attack strength increases. Hence, adversarial training still remains in effectiveness to build robustness for white-box and black-box attacks across different settings for biomedical image analysis.

Likewise, we measure the performance of robust medical segmentation models against no-box adversarial attacks for comprehensive evaluation. The no-box performance of different settings is reported in Table \ref{tab:12}. It shows that adversarial training achieves outstanding performance in enhancing robustness in lung segmentation. We observe that using the BCE loss as the adversarial loss in the no-box scenario achieves a better attack success rate than adopting the Dice loss.

\subsubsection{Time Cost Analysis for Adversarial Training}
For systematic evaluation, we also provide the time cost of several adversarial training methods for medical imaging tasks in Table \ref{tab:13}. For a fair comparison, we conduct all the training experiments on a single NVIDIA Tesla A100 GPU with the same batch size. We report the average time cost over 10 runs of several adversarial training approaches. We can easily observe that adversarial training generally takes several times as long as natural training. The time cost gap mainly comes from the iterative gradient computation during the inner adversary generation, which is further enlarged for medical segmentation.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.58\linewidth]{imgs/Adv_visualization.pdf}
	\vspace{-10pt}
	\caption{Heat-map visualization of medical adversarial examples under diverse attack strengths $\epsilon$ corresponding to NATurally (\textbf{NAT}) and ADVersarially (\textbf{ADV}) trained classification models.}
	\vspace{-10pt}
	\label{fig:4}
	
\end{figure}

\subsubsection{Visualization}
% 可视化热力激活图
In general, adversaries can lead to severe mistakes during the inference stage of target medical imaging models. To better represent the negative impact of adversarial examples, we utilize Gradient-weighted Class Activation Mapping (Grad-CAM) \cite{selvaraju2017grad} to create heat maps that represent the discriminative region for medical classification (see Figure \ref{fig:4}). It can be seen that naturally trained models are extremely susceptible to adversaries, especially for high-strength attacks. The class-discriminative region for the naturally trained model varied greatly with the increase of attack strength ($\epsilon$). In comparison, the adversarially trained model can achieve robust predictions and also keep the class-discriminative region the same as the attack intensity increases.

\vspace{-3mm}
\section{Challenges and Future Directions}
\label{sec:6}
% 第一个challenge 没有benchmark
% challenges和future directions按关注度依次划分
% 讲完一个challeng马上说direction
Despite the exceptional success of adversarial machine learning for medical image analysis, there still remain several challenges that are worthy of exploration. We also summarize some ongoing or future research directions below:

\noindent
\textbf{Evaluation benchmarks.}
Existing medical adversarial attack and defense methods are primarily based on customized evaluation metrics and settings, which might lead to difficulties in efficacy and efficiency assessments. Furthermore, both adversarial attack and defense are required to be realistic for clinical practice. Hence, for the sake of fair comparison and practical application, the community has to acknowledge a unified and systematical evaluation benchmark. A feasible solution is to establish an open-ended standard benchmark for robustness evaluation in the context of biomedical image analysis. Accordingly, the adversarial robustness benchmark in the medical field can follow the classical robustness benchmark for natural imaging, \textit{i.e.}, RobustBench \cite{CroceASDFCM021}. In the meantime, adversarial attacks need to be adaptive for standardized evaluation, which means that attacks are tailored for a given defense method \cite{tramer2020adaptive}. In this paper, we establish a unified benchmark for adversarial training in the context of biomedical image analysis to facilitate future research.

\noindent
\textbf{Trade-off.}
The trade-off between performance on legitimate and adversarial examples has been widely explored in the natural imaging field \cite{zhang2019theoretically, RadeM22, dong2023enemy}. The enhancement of robust accuracy inevitably induces a decrease in performance on clean examples. We also discovered such a phenomenon for adversarially trained medical diagnosis models in this survey. However, biomedical applications are usually required to be sensitive to the precision of diagnosis, especially in clinical practice. The ideal medical models should achieve excellent performance on clean examples and also remain robust against potential attacks. Therefore, finding a balance between clean and robust performance for medical diagnosis models can be an important research topic in the future. Moreover, we also observe that adversarially trained medical models can sometimes obtain the same or better performance on legitimate medical examples than regularly trained models. While an explicit index to balance accuracy and robustness remains an open research question, our results highlight the potential for tailored adversarial training strategies with a small perturbation radius in medical imaging tasks that can optimize both objectives without suffering from the trade-off issue. We believe it will be possible to further improve the adversarial robustness of medical models without compromising the clean accuracy.


\noindent
\textbf{Computational efficiency.}
Current medical adversarial defense methods mostly require an expensive computational cost, particularly for adversarial training. In general, the time cost of adversarial training can be dozens of times that of natural training due to the expensive cost of adversary generation, resulting in impediments to practical medical applications. Several fast adversarial training variants spare no effort to efficiently construct robustness in the natural imaging domain \cite{shafahi2019adversarial, andriushchenko2020understanding}, whereas there is rare research focusing on medical imaging tasks. Hence, it is necessary to put effort into efficient adversarial defense for medical image analysis. In addition, reducing computational costs can facilitate the defense within reach of organizations with modest computing resources to serve the community better.

\noindent
\textbf{Defense tailored for medical image analysis.}
Existing methods primarily transfer adversarial defense approaches for natural images directly to the medical imaging domain. Nevertheless, there remain very few defense methods tailored for medical images to enhance the adversarial robustness of computer-aided diagnosis systems. Unlike natural examples with large distribution variances, biomedical examples usually share a specific biological structure and a unified distribution, which can be further utilized as priors to construct robustness for medical image analysis systems. In the meantime, we also discover that the inner adversary generation strategy during adversarial training has a more significant impact on the robustness establishment for computer-aided diagnosis models than for natural models. Therefore, we argue that adversarial defense for the medical imaging domain has the potential to be improved by incorporating prior knowledge in biomedical images. 

%\vspace{-1mm}
\noindent
\textbf{Potential ethical issues.}
% trade-off + crisis of confidence
As we mentioned earlier, the performance of adversarially trained models on clean examples slightly decreases with the drastic enhancement of adversarial robustness. In other words, establishing adversarial robustness will inevitably induce a performance drop on legitimate examples. This is common in the field of natural images but raises a clinical ethical dilemma for medical imaging analysis: is a defense against potential adversarial threats more important than an accurate diagnosis on clean medical examples? In other words, how do we weigh the clean and robust accuracy of medical image analysis systems? The trade-off phenomenon and also inevitable clean accuracy drop are still unresolved issues for defense on clean images. Fortunately, we discover that adversarial training can enhance the robustness without losing natural performance for several medical imaging tasks and modalities. This also demonstrates that it is possible to simultaneously enhance adversarial robustness and also the natural performance of medical image analysis systems in the near future. Furthermore, threats of adversarial examples might also pose a crisis of confidence related to computer-aided diagnosis systems. Hence, it is essential to establish reliable and interpretable computer-aided diagnosis systems to build trust with potential users under real-world scenarios.

\vspace{-3mm}
\section{Conclusion}
In this work, we present a detailed survey of adversarial attack and defense methods for medical image analysis, which is driven by a systematic taxonomy in terms of the application scenario. This survey also incorporates a unified framework with a comprehensive analysis of different types of attack and defense methods in the context of medical images. In addition, we establish a benchmark for adversarially trained diagnosis models under various scenarios to facilitate future research. Finally, we point out representative challenges and promising future research directions in this domain. We hope this survey can further attract new efforts towards better interpretability and application of adversarial machine learning in the medical field.

%\section*{Acknowledgments}
%This work was supported by the National Natural Science Foundation of China (No. 12326618, 62072482, and 62202403), Hong Kong Innovation and Technology Fund (Project No. ITS/028/21FP and MHP/002/22), and Shenzhen Science and Technology Innovation Committee Fund (Project No. SGDX20210823103201011).



\vspace{-2mm}
\begin{acks}
\vspace{-1mm}
This work was supported by the National Natural Science Foundation of China (No. 12326618, 62072482, and 62202403), Hong Kong Innovation and Technology Fund (Project No. ITS/028/21FP and MHP/002/22), and Shenzhen Science and Technology Innovation Committee Fund (Project No. SGDX20210823103201011), and the Project of Guangdong Provincial Key Laboratory of Information Security Technology (Grant No. 2023B1212060026).
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
{
\bibliographystyle{ACM-Reference-Format}
\bibliography{egbib}
}


\end{document}
\endinput
%%
%% End of file `sample-acmsmall.tex'.
