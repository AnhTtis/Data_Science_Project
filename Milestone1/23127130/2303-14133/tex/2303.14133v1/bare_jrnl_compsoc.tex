\documentclass[10pt,journal,compsoc]{IEEEtran}
%

\usepackage{marvosym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{url}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{makecell}


\hyphenation{op-tical net-works semi-conduc-tor}

% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi




\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Adversarial Attack and Defense for Medical Image Analysis: Methods and Applications}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Junhao~Dong,
		Junxi~Chen,
		Xiaohua~Xie\textsuperscript{\Letter},
		Jianhuang~Lai,~\IEEEmembership{Senior~Member,~IEEE},
        and~Hao~Chen\textsuperscript{\Letter},~\IEEEmembership{Senior~Member,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem This project is supported by the NSFC (62072482).  
\IEEEcompsocthanksitem{Junhao Dong, Junxi Chen, Xiaohua Xie, and Jianhuang Lai are with the School of Computer Science and Engineering, Sun Yat-sen University, China. (E-mail: \{dongjh8, chenjx353\}@mail2.sysu.edu.cn; \{xiexiaoh6, stsljh\}@mail.sysu.edu.cn.) Hao Chen is with the Department of Computer Science and Engineering and the Department of Chemical and Biological Engineering, The Hong Kong University of Science and Technology, Hong Kong, China. E-mail: jhc@cse.ust.hk}
\IEEEcompsocthanksitem \Letter~denotes Corresponding author.}%
\thanks{Manuscript received XXX XX, XXXX; revised XXX XX, XXXX.}}




% The paper headers
\markboth{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX. XXXX}%
{****for peer review only****}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


\IEEEtitleabstractindextext{%
\begin{abstract}
Deep learning techniques have achieved superior performance in computer-aided medical image analysis, yet they are still vulnerable to imperceptible adversarial attacks, resulting in potential misdiagnosis in clinical practice. Oppositely, recent years have also witnessed remarkable progress in defense against these tailored adversarial examples in deep medical diagnosis systems. In this exposition, we present a comprehensive survey on recent advances in adversarial attack and defense for medical image analysis with a novel taxonomy in terms of the application scenario. We also provide a unified theoretical framework for different types of adversarial attack and defense methods for medical image analysis. For a fair comparison, we establish a new benchmark for adversarially robust medical diagnosis models obtained by adversarial training under various scenarios. To the best of our knowledge, this is the first survey paper that provides a thorough evaluation of adversarially robust medical diagnosis models. By analyzing qualitative and quantitative results, we conclude this survey with a detailed discussion of current challenges for adversarial attack and defense in medical image analysis systems to shed light on future research directions.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Adversarial attack, adversarial defense, medical image analysis, taxonomy, evaluation.
\end{IEEEkeywords}}


% make the title area
\maketitle



\IEEEdisplaynontitleabstractindextext

\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\IEEEPARstart{D}{riven} by the success of Deep Neural Networks (DNNs) in natural image processing tasks \cite{he2016deep, wang2020deep, minaee2021image}, they have also been demonstrated to have expert-level performance for various medical image analysis tasks, including but not limited to skin lesion diagnosis \cite{ge2017skin, zhang2019attention}, diabetic retinopathy detection \cite{gondal2017weakly, qummar2019deep}, and tumor segmentation \cite{pereira2016brain, muhammad2020deep}. Among these medical applications, Artificial Intelligence (AI)-based diabetic retinopathy detection system was the first that was approved for marketing by the US Food and Drug Administration (FDA) \cite{FDA2018}. In clinical practice, deep learning-driven medical diagnosis models can further save the overall cost of manual work and even improve patient outcomes by early detection \cite{alzubaidi2021role}.


Although deep learning has emerged as a promising technique for fundamental research in multiple disciplines \cite{koohi2019predicting, piloto2022intuitive, zemskova2022deep}, it still suffers from adversarial examples \cite{SzegedyZSBEGF13}, which can induce a catastrophic disruption to DNNs. In addition, the adversarial example can be obtained by adding a visually imperceptible perturbation to the legitimate example, which makes it easy to bypass the manual check \cite{carlini2017adversarial}. The existence of such tailored examples becomes one of the major hindrances to practically applying DNNs in safety-critical scenarios, like medical image analysis \cite{paschali2018generalizability_A1, finlayson2019adversarial_A3, ma2021understanding_D10}. In particular, the adversarial vulnerability can be even worse for medical diagnosis systems, which may result in misdiagnosis, insurance fraud, and even a crisis of confidence in AI-based medical technology \cite{shah2018susceptibility_A4, finlayson2019adversarial_A3}. In addition, the medical diagnosis system is complex and unlikely to update, which can be difficult to imagine how these adversarial attacks could be operationalized. Recent works have revealed the adversarial vulnerability of computer-aided diagnosis models under various scenarios, including the white box \cite{finlayson2019adversarial_A3, yao2021hierarchical_A15, tian2021bias_A14}, semi-white box \cite{rahman2020adversarial_A22, wang2022feature_A20}, black box \cite{byra2020adversarial_A11, bortsova2021adversarial_A18}, etc. Consequently, the robustness against adversarial examples has been considered a new measurement for the security of medical image analysis.

Considering the huge healthcare economy and pervasive medical fraud, numerous studies have been conducted to defend against these malicious examples in medical image analysis. Generally, early research efforts mainly focus on adversarial training for improving intrinsic network robustness \cite{ren2019brain_D11, vatian2019impact_D13, zhou2021ssmd_D41} or adversarial detection to identify adversarial examples \cite{ma2021understanding_D10, park2020robustification_D19, watson2021attack_D7}. Besides, a small fraction of works incorporate image-level pre-processing \cite{xu2022medrdf_D2, liu2020defending_D23} or feature enhancement \cite{taghanaki2019kernelized_D20, xue2019improving_D22} into the adversarial defense framework. The family of adversarial defense methods has been demonstrated to be effective in establishing robustness against adversarial examples for unified pattern recognition in medical diagnosis \cite{li2021defending_D1, xu2021towards_D9}. However, there exists a considerable gap in the research-oriented setting and also evaluation between various defense methods, inducing difficulties in comparison. This thus motivates us to construct a systematical survey, developing the benchmark implementation and evaluation for the most effective defense approach, \textit{a.k.a.} adversarial training. 

\begin{figure}[t]
	\centering
	\includegraphics[width=0.85\linewidth]{imgs/Papers_per_year.pdf}
	\vspace{-6mm}
	\caption{The number of papers published per year related to adversarial attack and defense for medical image analysis.}
	\vspace{-3mm}
	\label{fig:1}
\end{figure}

% Compared to existing surveys + line graph (Attack&Defense)
Several survey papers \cite{sipola2020model, apostolidis2021survey, shamshiri2022security, kaviani2022adversarial} have so far summarized the adversarial attack and defense for medical image analysis. However, many of them only focus on a certain medical task, e.g., Coronavirus 2019 (COVID-19) analysis \cite{shamshiri2022security}. Otherwise, these surveys do not provide a detailed taxonomy or a comprehensive evaluation of existing attack and defense methods for computer-aided diagnosis models. Furthermore, recent advances in adversarial attack and defense for medical image analysis systems have not been covered in these surveys. In this paper, we aim to address these gaps and provide a taxonomic overview of recent advances in adversarial attack and defense for medical image analysis with a discussion of their benefits and limitations. Although a considerable number of adversarial attack and defense methods emerge every year in the field of medical image analysis (see Fig. \ref{fig:1}), there still remains a lack of unified and fair measurement for various defense methods. Hence, we construct a benchmark evaluation of adversarial training methods for future development. The main contributions of this work lie in the following aspects:

\begin{itemize}
	\item We provide a comprehensive review of adversarial attack and defense methods in the field of medical image analysis, including a family of attack and defense methods with a novel taxonomy based on the application scenario.
	\item We establish a unified theoretical framework for different types of adversarial attack and defense approaches in medical imaging tasks.
	\item To the best of our knowledge, this is the first survey that establishes a benchmark for adversarially robust diagnosis models under various scenarios. We further present a systematic analysis of adversarial attack and defense in the context of medical images.
	\item We identify current challenges and provide insightful guidance for future research in adversarial attack and defense for medical image analysis.
\end{itemize}



The rest of this survey is structured as follows. We start with a unified framework to theoretically define both medical adversarial attack and defense in Section \ref{sec:2}. Next, we give a detailed review of adversarial attack methods on medical machine learning systems with the taxonomy of attack scenarios (Section \ref{sec:3}). In Section \ref{sec:4}, we summarize and categorize existing adversarial defense methods for medical image analysis. Then we present benchmark evaluation results of medical attack and defense algorithms in various settings (Section \ref{sec:5}). Lastly, we discuss current challenges and future research directions in Section \ref{sec:6}.

\vspace{-3mm}
\section{Background}
%\vspace{-0.6mm}
\label{sec:2}


Although the deep learning paradigm has made significant breakthroughs in numerous AI fields, it is still vulnerable to various security threats, including poison attack \cite{tian2022comprehensive}, model inversion attack \cite{fredrikson2015model}, etc. Among them, adversarial attack has attracted the most attention from the community of deep learning security, as it raises a series of potential safety issues to the application of deep learning. Apart from disrupting the inference stage of DNNs, adversarial attacks can easily bypass the manual check, since the adversarial example is visually similar to its clean counterpart. This insidious security threat can further be magnified for computer-aided diagnosis models, which may result in catastrophic misdiagnosis and even a crisis of social confidence \cite{finlayson2019adversarial_A3}. In this section, we formally define adversarial attack and defense for medical image analysis and highlight the extreme vulnerability of medical images to adversarial attacks.

We first clarify the notations and definitions used in this survey. Considering a specific dataset $(\mathbf{x}, y) \sim \mathcal{D}$ where $\mathcal{D}$ is a data distribution over pairs of given examples $\mathbf{x}$ and their corresponding labels $y$. We denote $f_{\boldsymbol{\theta}} (\cdot)$ as the deep learning-based medical analysis model with network parameter $\boldsymbol{\theta}$. Generally, adversarial examples $\mathbf{\hat{x}}$ can be are typically created by appending an imperceptible noise $\boldsymbol{\delta}$ to clean examples $\mathbf{x}$, which can be formally defined as below: 
\begin{equation}
	\begin{aligned}
	\mathbf{\hat{x}} := \mathbf{x} + \boldsymbol{\delta} \textrm{~~with~} f_{\boldsymbol{\theta}} (\mathbf{\hat{x}}) \neq y \textrm{~and~} d(\mathbf{x}, \mathbf{\hat{x}}) \leq \epsilon,
	\end{aligned}
	\label{eq:1}
\end{equation}
where $d(\cdot, \cdot)$ is the distance metric, and $\epsilon$ is the maximum allowed perturbation bound for imperceptibility. By definition, adversarial examples $\mathbf{\hat{x}}$ need to be close to their legitimate counterparts $\mathbf{x}$ under a certain distance metric, e.g., $\ell_{p}$ distance. In other words, the adversarial perturbation is $\ell_{p}$-norm bounded as $\left\| \boldsymbol{\delta} \right\|_{p} \leq \epsilon$. In this survey, we mainly focus on attacks under $\ell_{\infty}$-norm threat model, which can be formulated as the following optimization problem:
\begin{equation}
	\begin{aligned}
		\max\limits_{\left\| \boldsymbol{\delta} \right\|_{\infty} \leq \epsilon}\mathcal{L}\left( f_{\boldsymbol{\theta}} \left( \mathbf{x}+\boldsymbol{\delta}\right) , y \right) ,
		\label{eq:2}
	\end{aligned}
\end{equation}
where $\mathcal{L}$ primarily depends on a certain task (such as the cross-entropy loss for classification). Normally, the above-mentioned optimization problem can be solved by quasi-Newton methods \cite{SzegedyZSBEGF13, carlini2017towards} or gradient descent-based algorithms. Specifically, we can obtain the worst-case example (or the strongest adversarial example) by iterative Projected Gradient Descent (PGD) \cite{MadryMSTV18} on the negative loss function with step size $\alpha$, as follows:
\begin{equation}
	\begin{aligned}
		\mathbf{\hat{x}}^{t+1} = \Pi_{\mathbb{B}(\mathbf{x}, \epsilon)} \left(  \mathbf{\hat{x}}^{t} + \alpha \cdot \operatorname{sign} \left( \nabla_{\mathbf{\hat{x}}^{t}}\mathcal{L}\left( \mathbf{\hat{x}}^{t}, y \right)  \right)  \right),
		\label{eq:3}
	\end{aligned}
\end{equation}
where $\mathbf{\hat{x}}^{t}$ represents the $t^\text{th}$ iteration update, and $\mathbb{B}(\mathbf{x}, \epsilon)$ denotes $\ell_{\infty}$-norm bound with radius $\epsilon$ around clean examples $\mathbf{x}$. Note that adversarial examples can be generated by numerous types of methods, including the Limited-memory BFGS method \cite{SzegedyZSBEGF13} and Fast Gradient Sign Method (FGSM) \cite{GoodfellowSS14}. Instead of the aforementioned white-box attack that can access the full knowledge of the target model, there also exist some other threat models, e.g., black-box attack, which poses a more significant security issue to computer-aided diagnosis models in real-world scenario \cite{paschali2018generalizability_A1, bms2022analysis_A36}. We will discuss the details of these threat models in Section \ref{sec:3}.

Numerous types of defense methods have been proposed to enhance the robustness of computer-aided diagnosis systems against adversarial examples \cite{asgari2018vulnerability_D17, li2020robust_D24, tripathi2020fuzzy_D4}. Among them, adversarial training \cite{GoodfellowSS14, MadryMSTV18} has received the greatest attention, which can improve intrinsic network robustness via augmenting adversarial examples as training data. The adversarially trained model is expected to correctly predict both clean and adversarial examples during the inference stage. Specifically, the standard adversarial training \cite{MadryMSTV18} can be extended based on Equation (\ref{eq:2}) as the following min-max optimization problem:
\begin{equation}
	\begin{aligned}
		\min\limits_{\boldsymbol{\theta}} \mathbb{E}_{\left( \mathbf{x}, y\right)\sim \mathcal{D} }\left[ \max\limits_{\left\| \boldsymbol{\delta} \right\|_{\infty} \leq \epsilon}\mathcal{L}\left( f_{\boldsymbol{\theta}} \left( \mathbf{x}+\boldsymbol{\delta}\right) , y \right)  \right].
		\label{eq:4}
	\end{aligned}
\end{equation}
The inner maximization is to search for the worst-case adversarial examples to disrupt the target network. The outer minimization mainly focuses on optimizing empirical adversarial risk over network parameter $\theta$. Generally, adversarial training improves the intrinsic robustness of DNNs without any auxiliary modules while preserving their inference ability for legitimate examples. 

Besides enhancing the intrinsic network robustness against adversaries, several defense methods focus on data pre-processing (both clean and adversarial examples) without affecting the subsequent computer-aided analysis systems \cite{xu2022medrdf_D2, kansal2022defending_D51}. Generally, the data pre-processing aims at transforming adversarial examples into their benign versions for subsequent inference while making clean inputs remain unchanged. We can thus formulate a pre-processing-based defense as the following optimization problem:
\begin{equation}
	\small
	\begin{aligned}
		\min\limits_{\psi} \mathbb{E}_{\left( \mathbf{x}, y\right)\sim \mathcal{D} }\left[ \mathcal{L}\left( f_{\boldsymbol{\theta}} \left( \psi\left( \mathbf{x}+\boldsymbol{\delta}\right) \right) , y \right) + \lambda \cdot \mathcal{L}\left( f_{\boldsymbol{\theta}} \left( \psi\left( \mathbf{x}\right) \right) , y \right) \right].
		\label{eq:5}
	\end{aligned}
\end{equation}
where $\lambda$ is the weighting factor, and $\psi$ denotes the pre-processing module, which can be a parametric or non-parametric operator to mitigate the effect of adversarial perturbations. However, discriminative regions in medical images usually occupy a few pixels. The pre-processing methods for medical images still suffer from a higher risk of discriminative feature loss than for natural images.

Different from medical adversarial defense methods that aim at inferencing adversaries correctly, some works focus on the detection of input adversarial examples \cite{watson2021attack_D7, li2020robust_D24, yang2022defense_D67}. Adversarial detection can thus be regarded as a binary classification task to distinguish legitimate and adversarial examples. The main goal of adversarial detection is formalized as the following optimization: 
\begin{equation}
	\small
	\begin{aligned}
		\min\limits_{\boldsymbol{\omega}} \mathbb{E}_{\left( \mathbf{x}, y\right)\sim \mathcal{D} }\left[ \mathcal{L}_\text{CE}\left( f_{\boldsymbol{\omega}} \left(  \mathbf{x}+\boldsymbol{\delta} \right) , 1 \right) + \beta \cdot \mathcal{L}_\text{CE}\left( f_{\boldsymbol{\omega}} \left(  \mathbf{x}\right) , 0 \right) \right].
		\label{eq:6}
	\end{aligned}
\end{equation}
where $f_{\boldsymbol{\omega}}$ is the binary classifier with parameter $\boldsymbol{\omega}$ for adversarial detection, and $\mathcal{L}_\text{CE}$ represents the cross-entropy loss. $\beta$ is the weighting factor to determine the focus of detection models. The adversarial detector is desired to correctly distinguish legitimate examples (0) and their adversarial counterparts (1). Note that adversarial detection can be either parametric (learnable) or non-parametric. Other than adversarial training and pre-processing-based defense, adversarial detection mainly focuses on detecting adversarial examples in advance but not robust inference. We will further discuss the details of these adversarial defense methods and some other types of approaches in Section \ref{sec:4}.

Numerous adversarial attack and defense methods have been demonstrated to yield excellent performance for natural images \cite{aldahdooh2022adversarial}. However, there exist fundamental differences between natural vision tasks and medical imaging tasks in terms of data sizes, features, and task patterns. Therefore, it is difficult to transfer adversarial attack and defense methods for natural images directly to the medical domain. In addition, several works have shown that medical images can even suffer from more severe adversarial attacks than natural images \cite{finlayson2019adversarial_A3, ma2021understanding_D10, yao2021hierarchical_A15, rasaee2021explainable_A44}. Considering the massive healthcare industry and the significant impact of computer-aided diagnosis, it is necessary to pay close attention to the security and reliability of computer-aided diagnosis models. Hence, we summarize this survey for a comprehensive overview of recent advances in both adversarial attack and defense for medical image analysis.

\vspace{-3mm}
\section{Medical Adversarial Attacks}
\label{sec:3}
Medical adversarial attack aims at generating adversarial examples to disrupt medical diagnosis models during the inference stage. In this section, we first introduce incentives for attacks on computer-aided diagnosis systems and then provide the taxonomy of adversarial attacks for medical image analysis. We concentrate on modularly analyzing each adversarial attack algorithm against various medical image processing tasks under diverse scenarios.



\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{imgs/Attack_Taxonomy.pdf}
	\vspace{-4mm}
	\caption{Taxonomy of medical adversarial attacks in terms of application scenarios for attackers. Following \cite{chen2017zoo, dong2022restricted}, we categorize adversarial attack methods into four classes according to the degrees of accessible knowledge, including Backward Propagation (BP) gradients of target DNN during the training and inference stage of the adversary generator. ``Query'' represents the accessibility to inference outputs of target DNN.}
	\vspace{-3mm}
	\label{fig:2}
\end{figure}

\vspace{-3mm}
\subsection{Taxonomy of Adversarial Attacks}
% 
In the past few years, various methods have been proposed to generate adversarial examples against computer-aided diagnosis models. According to several studies on the motivations for conducting medical adversarial attacks \cite{finlayson2019adversarial_A3, levy2022security_A27}, we consider the financial gain can be the biggest incentive to attack healthcare systems, as health insurance frauds have grown on a large scale over the years \cite{villegas2021fourteen}. The attacker might try to deliberately cause misdiagnosis for AI-based medical reimbursement systems to make an erroneous decision. In addition, the adversarial vulnerability of the computer-aided diagnosis mechanism can even be involved in terrorism and unfair competition in the future. Most importantly, a growing number of attacks against medical image diagnosis will induce a crisis of confidence in the autonomous AI diagnostic system. Existing attack methods concentrate on different vulnerabilities of DNNs and also diverse medical imaging tasks. More importantly, they are designed in terms of application scenarios (degrees of accessible knowledge), as shown in Fig. \ref{fig:2}. Consistent with the taxonomy of adversarial attacks for natural images in a general setting \cite{chen2017zoo, pitropakis2019taxonomy, dong2022restricted}, we categorize medical adversarial attack methods into four classes (order by accessible knowledge to target DNN in decreasing order): white-box attack, semi-white-box (gray-box) attack, black-box attack, and restricted black-box (no-box) attack.  

% 
Under the white-box scenario, the attacker has full information about the target model, which can also access inference outputs of the target DNN unlimited times. Nonetheless, the semi-white-box (gray-box) setting \cite{xiao2018generating} mainly concentrates on constructing a generative model to produce adversarial examples against the target DNN. Generally, the gray-box attackers train the adversary generator in the white-box setting, while they do not require the accessibility of the target DNN during the inference (adversary generation) stage. The standard black-box adversarial attack can only access the DNN output (probabilities) using a limited number of queries. In contrast, the restricted black-box (no-box) adversarial attack does not need even a single-time query to the inaccessible DNN. 

% 
Both white-box and gray-box settings are ideal for the medical adversarial attack, which rely heavily on the accessibility of prior knowledge about the target DNN. Specifically, the attackers focus on the backward gradients to update adversarial examples. Black-box attacks concentrate on zeroth-order optimization according to numerous times of query outputs of the target DNN. In comparison, no-box attacks strongly depend on the transferability between different models of adversarial examples. Note that Black-box and also no-box scenarios are close to computer-aided diagnosis systems in practice, which poses a more significant threat to model deployment and maintenance. In the following sections, we will survey recent advances in these four types of adversarial attacks for medical image analysis.

\vspace{-3mm}
\subsection{Summary of Medical Attack Works}
\label{sec:3_Sum}
To facilitate future research activities, we present an overview of adversarial attack works for medical image analysis with a detailed taxonomy in Table \ref{tab:1}. Particularly, we include attack methods designed for natural images and also medical images (in \textbf{bold}). Here, we also list a series of adversarial attack methods tailored for natural images that are extended to these medical works, including L-BFGS \cite{SzegedyZSBEGF13}, CW attack \cite{carlini2017towards}, DeepFool \cite{moosavi2016deepfool}, Universal Adversarial Perturbation (UAP) \cite{moosavi2017universal}, Jacobian Saliency Map Attack (JSMA)\cite{papernot2016limitations}, Zeroth Order Optimization (ZOO) \cite{chen2017zoo}, etc.

\begin{table*}[t!]
	\centering
	\tiny 
	\renewcommand{\arraystretch}{0.85}
	\caption{Summary of adversarial attack works in the context of medical image analysis (time ascending). The newly proposed adversarial attack methods tailored for medical images are in \textbf{bold}.}
	\vspace{-10pt}
	\label{tab:1}
	\rowcolors{2}{gray!0}{gray!8}
	\resizebox{\linewidth}{!}{  
		
		\begin{tabular}{|c|c|c|c|c|c|}
%			\toprule
			\hline
			Ref. & Year & Task &  Attack scenario  &  Method  &  Data Modality\\ 
			\hline
			
			\cite{paschali2018generalizability_A1} & 2018 & Classification, Segmentation &  White-box, No-box  & FGSM, DeepFool, JSMA & MRI, Dermoscopy \\ 
			
			\cite{finlayson2019adversarial_A3} & 2018 & Classification &  White-box, No-box  & PGD, AdvPatch &\begin{tabular}[c]{@{}c@{}}Fundoscopy \\ X-ray, Dermoscopy \end{tabular}  \\ 
			
			\cite{shah2018susceptibility_A4} & 2018 & Classification &  No-box  & FGSM & Fundoscopy \\ 
			
			\cite{liu2019robustifying_A31} & 2019 & Segmentation &  White-box  & FGSM, I-FGSM, TI-FGSM & MRI \\ 
			
			\cite{chen2019intelligent_A13} & 2019 & Segmentation &  White-box  & \textbf{Multi-task VAE} & CT \\ 
			
			\cite{ozbulak2019impact_A12} & 2019 & Segmentation &  White-box  & \textbf{Adaptive Segmentation Mask Attack} & Fundoscopy, Dermoscopy \\
			
			\cite{kovalev2019influence_A5} & 2019 & Classification &  White-box  & PGD & X-ray, Histology \\
			
			\cite{rao2020thorough_A45_D60} & 2020 & Classification &  White-box, No-box  & \begin{tabular}[c]{@{}c@{}}FGSM, PGD, MI-FGSM, \\ DAA, DII-FGSM \end{tabular} & X-ray  \\
	 
			\cite{cheng2020adversarial_A43} & 2020 & Classification &  White-box, No-box  & \textbf{Adversarial Exposure Attack} & Fundoscopy  \\
			
			\cite{rahman2020adversarial_A22} & 2020 & Classification, Object detection &  White-box, Gray-box, No-box  & \begin{tabular}[c]{@{}c@{}}FGSM, DeepFool, CW,  \\ BIM, L-BFGS, PGD, JSMA \end{tabular}  & CT, X-ray \\
			
			\cite{byra2020adversarial_A11} & 2020 & Classification &  Black-box  & ZOO & Ultrasound \\ 
			
			\cite{yao2020miss_A34} & 2020 & Landmark detection &  White-box  & \textbf{Adaptive Targeted I-FGSM} & MRI, X-ray  \\
			
			\cite{yoo2020outcomes_A32} & 2020 & Classification &  White-box, No-box  & FGSM & Fundoscopy \\ 
			
			\cite{cheng2020adversarial_A2} & 2020 & Segmentation &  White-box  & UAP & MRI \\ 
			
			\cite{allyn2020adversarial_A6} & 2020 & Classification &  White-box  & Generative model & Dermoscopy \\ 

			\cite{gongye2020new_A24} & 2020 & Classification &  White-box  & FGSM, PGD & X-ray \\ 
			
			\cite{hirano2021universal_A7_D55} & 2021 & Classification &  White-box  & UAP & OCT, X-ray, Dermoscopy \\ 
			
			\cite{joel2021adversarial_A33} & 2021 & Classification &  White-box  & FGSM, BIM, PGD & CT, MRI, X-ray  \\
			
			\cite{chen2021adversarial_A30} & 2021 & Segmentation &  White-box  & \textbf{IND and OOD Attacks} & MRI \\ 
			
			\cite{QiGS0Z21_A17} & 2021 & Classification, Segmentation &  White-box  & \textbf{Stabilized Medical Image Attack} & CT, Endoscopy, Fundoscopy \\ 
			
			\cite{bortsova2021adversarial_A9} & 2021 & Segmentation &  White-box, No-box  & PGD & X-ray \\
			
			\cite{kovalev2021biomedical_A51} & 2021 & Classification &  White-box, No-box  & CW & CT, X-ray, Microscopy  \\  
			
			\cite{pal2021vulnerability_A8} & 2021 & Classification &  White-box  & FGSM & CT, X-ray \\ 
			
			\cite{shao2021target_A16} & 2021 & Segmentation &  White-box  & \textbf{Multi-scale Attack} & Fundoscopy, Dermoscopy \\ 
			
			\cite{tian2021bias_A14} & 2021 & Classification &  White-box, No-box  & \textbf{Adversarial Bias Field Attack} & X-ray \\ 
			
			\cite{foote2021now_A39} & 2021 & Classification &  White-box  & PGD, UAP & Microscopy  \\ 
			
			\cite{yilmaz2021assessment_A10} & 2021 & Classification &  White-box  & FGSM & X-ray \\ 
			
			\cite{kulkarni2021kryptonite_A28} & 2021 & Classification &  White-box  & \textbf{Kryptonite Attack} & MRI, Dermoscopy \\ 
			
			\cite{gougeh2021adversarial_A23} & 2021 & Classification &  White-box  & FGSM, PGD, CW, ST & X-ray \\
			
			\cite{koga2021simple_A29} & 2021 & Classification &  Black-box  & \textbf{Black-box UAP} & X-ray, Dermoscopy, Fundoscopy \\ 
			
			\cite{rasaee2021explainable_A44} & 2021 & Classification &  White-box  & I-FGSM & Ultrasound  \\ 
			
			\cite{yao2021hierarchical_A15} & 2021 & Classification &  White-box  & \textbf{Hierarchical Feature Constraint} & X-ray, Fundoscopy \\ 

			\cite{bortsova2021adversarial_A18} & 2021 & Classification &  No-box  & FGSM, PGD & X-ray, Fundoscopy,  Microscopy \\

			\cite{diyasa2021grasping_A38} & 2021 & Classification &  White-box  & \begin{tabular}[c]{@{}c@{}}FGSM, BIM, CW, RFGSM \\ PGD, FAB, DeepFool, SparseFool \end{tabular} & Microscopy  \\

			\cite{cui2021deattack_A48} & 2021 & Segmentation &  Black-box  & \textbf{Differential Evolution Attack} & CT, Dermoscopy, Ultrasound  \\ 
	 
			\cite{zhou2021machine_A46} & 2021 & Classification &  Gray-box  & \textbf{GAN-based Adversary Generator} & X-ray  \\
			
			\cite{wang2021adversarial_A37} & 2022 & Classification &  Black-box  & \textbf{AmdGAN} & \begin{tabular}[c]{@{}c@{}}CT, OCT, X-ray, Fundoscopy,  \\ Dermoscopy, Ultrasound, Microscopy \end{tabular}    \\
			
			\cite{levy2022security_A27} & 2022 & Classification, Segmentation &  White-box  & \begin{tabular}[c]{@{}c@{}} \textbf{Modified FGSM with} \\ \textbf{with tricks to break defences} \end{tabular} & \begin{tabular}[c]{@{}c@{}}CT, MRI, X-ray,  \\ Dermoscopy, Fundoscopy \end{tabular}  \\ 
			
			\cite{minagi2022natural_A19} & 2022 & Classification &  White-box  & UAP & X-ray, Fundoscopy, Dermoscopy \\
			
			\cite{patel2022predictive_A49} & 2022 & Classification &  White-box, No-box  & \textbf{Attention-based I-FGSM} & CT  \\ 
			
			\cite{kwon2022advu_A35} & 2022 & Segmentation &  White-box  & FGSM & Microscopy  \\
			
			\cite{de2022evaluation_A40} & 2022 & Classification &  White-box  & FGSM & X-ray  \\ 
			
			\cite{apostolidis2022digital_A21} & 2022 & Classification &  No-box  & \textbf{Digital Watermarking} & CT, MRI, X-ray \\
			
			\cite{wei2022analysis_A42} & 2022 & Classification &  White-box, No-box  & \begin{tabular}[c]{@{}c@{}}FGSM, BIM, PGD \\ \textbf{No-sign Operation} \end{tabular} & X-ray  \\
 
			\cite{selvakkumar2022addressing_A25} & 2022 & Classification &  White-box  & FGSM & Fundoscopy \\
			
			\cite{ahmed2022failure_A47} & 2022 & Classification &  White-box  & FGSM, PGD, CW & CT, Dermoscopy, Microscopy  \\
			
			\cite{li2022query_A50} & 2022 & Segmentation &  Black-box  & \textbf{Improved Adaptive Square Attack} & X-ray  \\ 
			
			\cite{bms2022analysis_A36} & 2022 & Classification &  Black-box  & FGSM, BIM, PGD, MI-FGSM & CT, Fundoscopy  \\ 
			
			\cite{morshuis2022adversarial_A26} & 2022 & Reconstruction &  White-box  & \begin{tabular}[c]{@{}c@{}}\textbf{Adversarial k-space Noise},  \\ \textbf{Adversarial Rotation} \end{tabular} & MRI \\ 
			
			\cite{bharath2022analysis_A41} & 2022 & Classification &  White-box  & FGSM, L-BFGS & Fundoscopy  \\ 
			
			\cite{wang2022feature_A20} & 2022 & Classification &  Gray-box  & \textbf{Feature Space-Restricted Attention Attack} & X-ray, Fundoscopy, Dermoscopy \\ 
%			\bottomrule
			\hline
		\end{tabular}
	}
	\vspace{-15pt}
\end{table*}

\vspace{-3mm}
\subsection{White-box Attacks}
% 
Most works of adversarial attack for medical image analysis focus on the white-box scenario. In particular, these works primarily consider the adversarial vulnerability of computer-aided diagnosis models in various medical imaging tasks with full knowledge of medical DNNs. The attacker can treat the target diagnosis DNN as a locally deployed model when conducting adversary generation.

% 
To the best of our knowledge, Paschali \textit{et al.} \cite{paschali2018generalizability_A1} were the first to systematically evaluate the white-box attack against a variety of medical imaging models of several medical tasks, including skin lesion classification and whole brain segmentation. Concretely speaking, they transfer adversarial attack methods for natural images \cite{GoodfellowSS14, moosavi2016deepfool, papernot2016limitations} directly to medical images, which poses an underlying threat to modern diagnosis DNN models. In the meantime, the attack success rate can be further reduced when attackers are in the face of deeper diagnosis models.

Generally, white-box adversarial attacks are conducted by solving a certain optimization problem. Nevertheless, Allyn \textit{et al.} \cite{allyn2020adversarial_A6} leveraged a generative model to produce visually undetectable adversarial perturbations corresponding to original input images. The generated adversarial examples can obtain a terribly high attack success (fooling) rate to dermoscopic image recognition systems, presenting a significant risk of misdiagnosis.

In addition to generating the adversarial perturbation corresponding to a certain medical image, several works \cite{hirano2021universal_A7_D55, foote2021now_A39, minagi2022natural_A19} consider obtaining a universal perturbation that is adaptive to a large proportion of medical images from a certain dataset, causing a security hole in the computer-aided clinical diagnosis. Hirano \textit{et al.} \cite{hirano2021universal_A7_D55} conducted a universal adversarial attack for medical image classification under both targeted and non-targeted scenarios. Note that the main goal of the targeted attack is to enable the adversarial example to be misclassified as a specified target category. Oppositely, the non-targeted attack just focuses on making the DNN model conduct wrong inference results. They discovered that non-targeted adversarial attacks can further achieve better transferability than targeted attacks. 

% 
Numerous researchers spare no effort to design custom adversarial attack methods \cite{tian2021bias_A14, yao2021hierarchical_A15, QiGS0Z21_A17, kulkarni2021kryptonite_A28} to better adapt to medical imaging tasks. By theoretically investigating the vulnerability of medical image representations, Yao \cite{yao2021hierarchical_A15} designed a novel Hierarchical Feature Constraint (HFC) as auxiliary guidance to hide the adversarial representations in the clean feature domain, which can further be a plug-and-play module for existing attack methods to reduce the risk of being detected. Technically, HFC encourages adversarial features to close to the high-density regions of the distribution of normal features via maximizing the log-likelihood of adversarial features. Considering various modalities of medical images, Qi \textit{et al.} \cite{QiGS0Z21_A17} proposed a new medical adversary generation method by optimizing a well-defined objective function that is composed of a deviation loss term and a stabilization loss term. Specifically, the deviation aims at enlarging the prediction gap between adversarial outputs and their corresponding ground truth. The stabilization term can be regarded as a regularization to constrain adversarial perturbations to low variance, which also avoids the local optima during the optimization process induced by instance-wise image noise. 

%
Apart from white-box adversarial attacks for medical classification tasks, there also exist various researchers that explore the vulnerabilities of other medical imaging tasks. The majority of these works concentrate on medical segmentation \cite{chen2019intelligent_A13, ozbulak2019impact_A12, cheng2020adversarial_A2, bortsova2021adversarial_A9, shao2021target_A16, chen2021adversarial_A30, kwon2022advu_A35}. Ozbulak \textit{et al.} \cite{ozbulak2019impact_A12} started the first attempt to expose the adversarial vulnerability of medical segmentation tasks by designing the Adaptive Segmentation Mask Attack (ASMA) method. The proposed ASMA incorporates the adaptive segmentation mask and also the dynamic perturbation multiplier to generate targeted adversarial examples that are indistinguishable from their legitimate counterparts. In addition to the superior disruption against various DNN models, the authors also demonstrated the generalizability of the proposed method with different distance metrics for adversarial perturbations. Based on feature-level ASMA attack \cite{ozbulak2019impact_A12}, Shao \textit{et al.} \cite{shao2021target_A16} further incorporated multi-scale gradients to generate adversarial perturbations to biomedical image segmentation models. The above-mentioned medical segmentation attack methods mainly depend on in-distribution adversarial examples. In contrast, the proposed method in \cite{chen2021adversarial_A30} considers out-of-distribution adversarial attacks against the lumbar disk shape reconstruction problem. Specifically, the PGD method \cite{MadryMSTV18} is applied to optimize the out-of-distribution perturbation to bypass defense methods.

%
In addition, Yao \textit{et al.} \cite{yao2020miss_A34} proposed an Adaptive Targeted Iterative FGSM (ATI-FGSM) with a comprehensive evaluation to study the vulnerability of multiple landmark detection systems. ATI-FGSM aims at moving a cohort of landmarks by dynamically assigning a weight for the loss term of a specific landmark in each iteration, which facilitates the fast convergence of adversary generation. In addition, Rahman et al. \cite{rahman2020adversarial_A22} examined the adversarial robustness of nine existing deep learning-based diagnosis applications for COVID-19 and demonstrated their extreme susceptibility to adversarial attacks.

% 
Apart from attacks against medical pattern recognition systems, a recent work \cite{morshuis2022adversarial_A26} investigated the adversarial vulnerability of Magnetic Resonance (MR) image reconstruction from k-space data. In addition to appending adversarial perturbation directly to k-space measurements, the authors also consider a visually slight rotation during the acquisition to obtain the adversarial effect, which can also be optimized by the PGD method.

\vspace{-3mm}
\subsection{Semi-white-box Attacks}

Semi-white-box (Gray-box) attacks have been widely explored for natural images \cite{xiao2018generating, wang2019gan, jandial2019advgan++}. However, there remain rare academic works to explore this attack scenario for medical image analysis \cite{rahman2020adversarial_A22, zhou2021machine_A46, wang2022feature_A20}. Generally, the semi-white-box adversarial attack involves two stages: 1) the attacker trains a generative model to produce adversarial examples against the target DNN model. During the training process, the attacker has full access to the target model, including gradients of backward propagation. 2) During the application stage, the adversary generator can directly obtain adversarial examples against the target model with the input of legitimate images, which do not requires any information about the target model as in the totally black-box scenario.

% 
Generative Adversarial Networks (GANs) have also been shown to have superior performance in generating highly plausible adversarial images \cite{xiao2018generating, wang2019gan, jandial2019advgan++, dong2022restricted}. To investigate the safety issues for AI-based computer-aided diagnosis of breast cancer on digital mammograms, Zhou \textit{et al.} \cite{zhou2021machine_A46} developed two GAN-based models at two different resolutions, which can synthesize highly plausible adversarial images to further induce a wrong diagnosis of breast cancer. The authors also pointed out another potential problem that GAN-based adversarial attack methods can also be trained using external medical data, as mammography is the widely available imaging modality in clinical practice. Recently, the  Feature-Space-Restricted Attention Attack \cite{wang2022feature_A20} is proposed to efficiently generate adversarial examples for various medical modalities with less visual perturbation. Specifically, the authors added the feature-level restriction to make sure that adversarial examples are close to the classification boundary in the feature space. Furthermore, an attention mechanism constraint was also proposed to regularize the image-level perturbations to focus on the lesion area, which incorporates class-wise attention information into the adversarial perturbation generation.

\vspace{-3mm}
\subsection{Black-box Attacks}

% 
Existing white-box adversarial attack mainly requires multiple backward gradients of target models. In other words, the attacker treats the target DNN as the locally deployed model to generate corresponding adversarial examples. However, the white-box setting can be unreliable in the real-world scenario, as it needs complete knowledge of the DNN model to attack. In comparison, the general black-box scenario can be a more suitable setting to simulate practical adversarial attacks. Numerous works have been proposed to explore black-box attacks for natural images \cite{chen2017zoo, ilyas2019prior, andriushchenko2020square, yatsura2021meta}. On account of the inaccessibility to backward gradients (or first-order information) of the target DNN, black-box adversarial attacks primarily resort to the zeroth order optimization for the gradient estimation, which requires querying the output probabilities of the target DNN with numerous times. There also exist several studies that focus on the black-box adversarial attack in the context of medical image analysis \cite{byra2020adversarial_A11, wang2021adversarial_A37, bms2022analysis_A36, cui2021deattack_A48, li2022query_A50}.

To explore the practical security threat against medical image analysis tasks, Byra \textit{et al.} \cite{byra2020adversarial_A11} devised a black-box adversarial attack approach that is specific to ultrasound image reconstruction. In comparison to existing attack methods that manipulate pixels of medical images, the proposed adversarial perturbation mainly concentrates on the reconstruction parameters. Moreover, the parameter-level attack is conducted for each radio-frequency data frame separately based on zeroth-order optimization \cite{chen2017zoo, liu2018signsgd}. On account of the difficulty in accessing the target diagnosis model during the black-box scenario, Wang \textit{et al.} \cite{wang2021adversarial_A37} employed the knowledge distillation method to learn a GAN-based model to generate adversarial examples. Specifically, the distillation model aims at simulating the inference outputs of target black-box DNN with multiple queries. Subsequently, the proposed generator can efficiently produce adversarial examples against the targeted diagnosis model.

Considering the scarcity of adversarial attacks against medical image segmentation, Cui \textit{et al.} \cite{cui2021deattack_A48} proposed a Differential Evolution Attack (DEAttack) against medical image segmentation models in the black-box setting with only tampering a few pixels of medical images. Moreover, the DEAttack remains a much higher efficiency in creating adversarial examples than directly applying the differential evolution algorithm \cite{storn1997differential} via incorporating pre-selected sensitive regions and colors from original medical images. The authors also demonstrated that the DEAttack can be conducted in the black-box setting by only compensating the modification for 1\% of pixels of medical images. Furthermore, Li \textit{et al.} \cite{li2022query_A50} improved the adaptive square attack \cite{li2020adaptive} by designing a more accurate gradient estimation for better convergence, which can efficiently alter the predicted results of foreground pixels with a few queries to the medical segmentation model. A learnable variance of the adaptive distribution is also enabled to limit sampling regions to a small area for acceleration.


\vspace{-3mm}
\subsection{Restricted Black-box (No-box) Attacks}
The above-mentioned methods either rely on having the whole knowledge of the targeted diagnosis model or require multiple queries to the black-box model. However, the attacker might not be able to access the target diagnosis model directly in most real-world situations. Particularly, the restricted black-box (no-box) setting can effectively represent the hardest (worst) case for the practical adversarial attack, which even do not require querying the target black-box DNN. The no-box attack mainly depends on the transferability \cite{LiuCLS17, ilyas2019adversarial, demontis2019adversarial} of adversarial examples among diverse DNN models. For instance, the no-box attacker can craft adversarial images based on a locally deployed surrogate model, which can directly transfer to target medical diagnosis systems. A well-established study has demonstrated that restricted black-box adversarial attacks pose a more stealthy threat for natural vision tasks \cite{cheng2019improving, jiang2019black, WeiLCC19, lu2020enhancing, YinWYGKDLL21}. Nevertheless, existing studies related to no-box attacks for medical image analysis are still lacking \cite{bortsova2021adversarial_A18, apostolidis2022digital_A21} and mainly based on approaches for natural images \cite{shah2018susceptibility_A4, rao2020thorough_A45_D60}, which requires more research efforts.

% 
To uncover the potential security threat of medical diagnosis systems, Shah \textit{et al.} \cite{shah2018susceptibility_A4} first investigated the black-box adversarial vulnerability of diabetic retinopathy detection models. The adversarial examples generated against a specific diagnosis DNN can also be transferred to other models, which can cause grave consequences for practical diagnostic prediction. Furthermore, hybrid lesion-based algorithms \cite{chiem2007novel} that are composed of multiple lesion detectors are demonstrated a greater ability to defend against transferable adversarial examples. In order to further deploy a reliable diagnosis system in practice, Bortsova \textit{et al.} \cite{bortsova2021adversarial_A18} studied several unexplored factors affecting the no-box adversarial attack against medical image analysis systems. Extensive experiments have demonstrated that pre-training can be a significant factor in enhancing the transferability between the surrogate model and the target black-box model. In addition, the transferable attack success rate can be enhanced by bridging the gap in training data and also network architectures between different diagnosis models. Hence, it is essential to consider the above-mentioned factors during the development of security-critical medical image analysis systems in clinical practice.

% 
Unfortunately, very few works have investigated custom black-box adversarial attack methods for medical image analysis. Innovatively, Apostolidis \textit{et al.} \cite{apostolidis2022digital_A21} highlighted a novel view of digital watermarking to restricted black-box adversarial attacks in the context of medical image analysis. Specifically, the Krawtchouk orthogonal moments \cite{yap2003image} are also incorporated to generate adversarial watermarks against three different medical modalities. Massive experiments demonstrated that CT scans are extremely vulnerable against no-box attacks on various diagnosis models.

% 
Other than no-box attacks against medical classification, there also exist scarce studies that explore the no-box adversarial vulnerability of medical segmentation models. Bortsova \textit{et al.} \cite{bortsova2021adversarial_A9} investigated both the white-box and no-box adversarial attacks against medical segmentation and their relationships. A surrogate network is used to generate targeted and untargeted adversarial images that can be further transferred to black-box segmentation models. Note that the training datasets of the surrogate model and the black-box model have no intersection to simulate clinical practice. The attacker can effectively misguide the target model to produce specific segmentation results, \textit{e.g.}, a heart symbol. Particularly, the transferability of restricted black-box attack relies on a high-level adversarial noise, which is also in alignment with \cite{paschali2018generalizability_A1}.

\vspace{-3mm}
\section{Medical Adversarial Defenses}
\label{sec:4}
% 
Considering the considerable threats posed to the healthcare industry, several defense methods have been proposed to defend against medical adversarial attacks. In this section, we first restate the significance of establishing defenses against adversarial attacks for computer-aided diagnosis systems. Furthermore, we introduce a new taxonomy for medical adversarial defense methods based on the stage at which they operate. In particular, we provide a systematic analysis of each type of adversarial defense method, including its requirements, constraints, and outcomes.

\vspace{-3mm}
\subsection{Taxonomy of Adversarial Defenses}
% 
On account of catastrophic failures caused by adversarial examples, a massive number of methods have been proposed to build trustworthy deep learning-based systems for natural images \cite{carlini2017adversarial, MadryMSTV18, XieWZRY18, silva2020opportunities, BaiL0WW21}. In the meantime, establishing robust computer-aided diagnosis models for clinical applications also contributes to the delivery of reliable healthcare services to millions of people. It is thus significant to investigate adversarially robust models in the context of medical image analysis. Considering the application prospect, we can imagine that healthcare or automatic diagnosis systems claim they can provide robust and also reliable services to billions of people. In this section, we systematically categorize adversarial defense methods for medical image analysis into five classes: adversarial training, adversarial detection, image-level pre-processing, feature enhancement, and knowledge distillation.

%
Adversarial training and its variants have been demonstrated to be the most effective methods to defend against adversarial examples. The primary goal of adversarial training is to improve the robustness of a certain model against adversarial attacks, which can be easily achieved by augmenting adversarial examples into training examples \cite{MadryMSTV18, zhang2019theoretically, wang2019improving}. In comparison, adversarial detection aims at distinguishing adversarial examples in advance to prevent the deep learning-based system from further catastrophes. Note that we have formally defined both adversarial training and detection in Section \ref{sec:2}. Image-level pre-processing aims to remove the visually imperceptible perturbation from adversarial examples while not affecting the subsequent inference of clean examples. Furthermore, feature enhancement methods focus on feature-level processing to mitigate the adversarial effect, which can be conducted by additional modules or novel frameworks. There also exists another type of defense method that distills a lightweight and adversarially robust model from a large-scale model for deployment in real-time healthcare applications.

%
Note that adversarial detection-based methods are incapable of conducting a correct diagnosis for adversarial examples, which mainly rely on distinguishing adversarial examples in advance to reject these malicious examples. In comparison, the other four types of methods aim to establish an adversarially robust medical system, which treats all input examples equally for subsequent diagnosis. In the following sections, we will provide a comprehensive survey related to the above-mentioned five types of adversarial methods for medical image analysis.

\vspace{-3mm}
\subsection{Summary of Medical Defense Works}
Overall, we also provide a summary of adversarial defense works in the context of medical image analysis in Table \ref{tab:2}. In addition to a significant number of adversarial attack methods mentioned in Section \ref{sec:3_Sum}, several attack approaches also serve as robustness evaluation metrics for adversarial defense in the context of medical images. We thus provide a list of used evaluation metrics for reference: Dense Adversary Generation (DAG) \cite{xie2017adversarial}, One-Pixel Attack (OPA) \cite{su2019one}, Generative Adversarial Perturbations (GAP) \cite{poursaeed2018generative}, Distributionally Adversarial Attack (DAA) \cite{zheng2019distributionally}, Diverse Inputs I-FGSM (DII-FGSM) \cite{xie2019improving}, HopSkipJumpAttack \cite{chen2020hopskipjumpattack}, AutoPGD \cite{croce2020reliable}, Stabilized Medical Attack (SMA) \cite{QiGS0Z21_A17}.

\begin{table*}[t!]
	\centering
	\tiny 
	\renewcommand{\arraystretch}{0.85}
	\caption{Summary of adversarial defense methods for medical image analysis (time ascending). The newly proposed adversarial attack approaches tailored for robustness evaluation of medical images are in \textbf{bold}.}
	\vspace{-10pt}
	\label{tab:2}
	\rowcolors{2}{gray!0}{gray!8}
	\resizebox{\linewidth}{!}{  
		
		\begin{tabular}{|c|c|c|c|c|c|}
%			\toprule
			\hline
			Ref. & Year & Task &  Defense type  &  Evaluation metrics  &  Data Modality\\ 
			\hline
			\cite{asgari2018vulnerability_D17} & 2018 & Classification & Feature Enhancement & \begin{tabular}[c]{@{}c@{}} FGSM, PGD, BIM,\\L-BFGS, DeepFool \end{tabular} & X-ray \\
			
			\cite{huang2018some_D21} & 2018 & Reconstruction & Adversarial Training & FGSM, BIM & CT \\
			
			\cite{ren2019brain_D11} & 2019 & Segmentation & Adversarial Training & FGSM & MRI \\
			
			\cite{xue2019improving_D22} & 2019 & Classification & Feature Enhancement & FGSM, I-FGSM, CW & X-ray, Dermoscopy \\
			
			\cite{taghanaki2019kernelized_D20} & 2019 & \begin{tabular}[c]{@{}c@{}} Classification, Segmentation, \\ Object Detection \end{tabular} & Feature Enhancement & \begin{tabular}[c]{@{}c@{}} FGSM, CW, PGD, BIM,\\GN, SPSA, MI-FGSM\end{tabular} & X-ray, Dermoscopy \\
			
			\cite{vatian2019impact_D13} & 2019 & Classification & Adversarial Training & GN & CT, MRI \\
			
			\cite{he2019non_D16} & 2019 & Segmentation & Feature Enhancement & I-FGSM & X-ray, Dermoscopy \\
			
			\cite{li2019volumetric_D46} & 2019 & Segmentation & Adversarial Training & FGSM, I-FGSM & CT \\
			
			\cite{kotia2020risk_D49} & 2019 & Classification & Adversarial Training & FGSM & MRI \\
			
			\cite{stimpel2019multi_D62} & 2019 & Low-level vision & Feature Enhancement & \textbf{Optimization-based Attack} & X-ray, MRI \\
			
			\cite{park2020robustification_D19} & 2020 & Segmentation & Adversarial Detection & DAG & MRI \\
			
			\cite{li2020anatomical_D40} & 2020 & Regression & Feature Enhancement & FGSM, I-FGSM & MRI \\
			
			\cite{rao2020thorough_A45_D60} & 2020 & Classification & Adversarial Training, Pre-processing & \begin{tabular}[c]{@{}c@{}} FGSM, PGD, DAA, \\ MI-FGSM, DII-FGSM \end{tabular} & X-ray \\
			
			\cite{paul2020mitigating_D8}  & 2020 & Classification & Adversarial Training & OPA, FGSM & CT \\
			
			\cite{wu2020classification_D15} & 2020 & Classification & Adversarial Training & PGD & Fundoscopy \\
			
			\cite{anand2020self_D48} & 2020 & Classification, Segmentation & Adversarial Training & PGD, FGSM & X-ray, MRI \\
			
			\cite{ma2020increasing_D5}  & 2020 & Classification, Segmentation & Adversarial Training & PGD, I-FGSM & CT, MRI \\
			
			\cite{cheng2020addressing_D18} & 2020 & Reconstruction & Adversarial Training & \textbf{False-Negative Adversarial Feature} & MRI \\
			
			\cite{raj2020improving_D61} & 2020 & Reconstruction & Adversarial Training & \textbf{GAN-based Attack} & CT, X-ray\\
			
			\cite{huq2020analysis_D47} & 2020 & Classification & Adversarial Training & PGD, FGSM & Dermoscopy \\
			
			\cite{tripathi2020fuzzy_D4}  & 2020 & Classification & Pre-processing & \begin{tabular}[c]{@{}c@{}} PGD, BIM, CW, \\ FGSM, DeepFool \end{tabular} & CT, X-ray \\
			
			\cite{liu2020no_D12} & 2020 & Classification & Adversarial Training & PGD & CT \\
			
			\cite{chen2020realistic_D14} & 2020 & Segmentation & Adversarial Training & \textbf{Adversarial Bias Attack} & MRI \\
			
			\cite{liu2020defending_D23} & 2020 & Segmentation & Pre-processing & ASMA & Fundoscopy, Dermoscopy \\
			
			\cite{shen2020robustness_D36} & 2020 & Classification & Adversarial Training & \textbf{Random and Optimized Attacks} & CT \\
			
			\cite{li2020robust_D24} & 2020 & Classification & Adversarial Detection & FGSM, PGD, BIM, MI-FGSM & X-ray \\
			
			\cite{ma2021understanding_D10} & 2021 & Classification & Adversarial Detection & FGSM, BIM, PGD, CW & X-ray, Fundoscopy, Dermoscopy \\
			
			\cite{watson2021attack_D7}  & 2021 & Classification & Adversarial Detection & PGD, CW & X-ray \\
			
			\cite{carannante2021trustworthy_D54} & 2021 & Segmentation & Feature Enhancement & PGD, FGSM & CT, MRI \\
			
			\cite{hirano2021universal_A7_D55} & 2021 & Classification & Adversarial Training & UAP & OCT, X-ray, Dermoscopy \\
			
			\cite{li2021defending_D1} & 2021 & Classification & Adversarial Training, Adversarial Detection & FGSM, PGD, CW & OCT \\
			
			\cite{xu2021towards_D9}  & 2021 & Classification & Adversarial Training & PGD, GAP & X-ray, Fundoscopy, Dermoscopy \\
			
			\cite{lal2021adversarial_D3}  & 2021 & Classification & Adversarial Training, Feature Enhancement & \begin{tabular}[c]{@{}c@{}} FGSM, DeepFool, \\ \textbf{Speckle Noise Attack} \end{tabular} & X-ray, Fundoscopy \\
			
			\cite{pervin2021adversarial_D25} & 2021 & Segmentation & Adversarial Training & FGSM & CT \\
			
			\cite{uwimana2021out_D26} & 2021 & Classification                                 & Adversarial Detection & FGSM, BIM, CW, DeepFool & Microscopy \\
			
			\cite{han2021advancing_D35} & 2021 & Classification & Feature Enhancement & PGD & CT, MRI, X-ray \\
			
			\cite{zhou2021ssmd_D41} & 2021 & Object Detection & Adversarial Training & FGSM & CT, Microscopy \\
			
			\cite{liu2021robustifying_D6}  & 2021 & Segmentation & Distillation & FGSM, I-FGSM, TI-FGSM & MRI \\
			
			\cite{daza2021towards_D27} & 2021 & Segmentation & Feature Enhancement, Adversarial Training & PGD, AA & CT, MRI \\
			
			\cite{chen2021enhancing_D33} & 2021 & Classification & Feature Enhancement & PGD, FGSM & X-ray \\
			
			\cite{yao2021medical_D45} & 2021 & Classification & Adversarial Detection, Pre-processing & \begin{tabular}[c]{@{}c@{}} FGSM, BIM, PGD, CW, PGD,\\MI-FGSM, TI-FGSM, DI-FGSM \end{tabular} & X-ray, Fundoscopy \\
			
			\cite{gupta2022vulnerability_D56} & 2022 & Classification & Adversarial Training & FGSM & MRI \\
			
			\cite{kansal2022defending_D51} & 2022 & Classification & Pre-processing & FGSM, PGD & X-ray \\
			
			\cite{jaiswal2022ros_D66} & 2022 & Classification & Distillation & FGSM, PGD & X-ray, Dermoscopy \\
			
			\cite{yang2022defense_D67} & 2022 & Classification & Adversarial Detection & HopSkipJumpAttack & MRI, X-ray, Microscopy \\
			
			\cite{xu2022medrdf_D2}  & 2022 & Classification & Pre-processing & I-FGSM, PGD, CW & X-ray, Dermoscopy \\
			
			\cite{joel2022using_D32} & 2022 & Classification & Adversarial Training & FGSM, PGD, BIM & CT, MRI, X-ray \\
			
			\cite{alatalo2022detecting_D63} & 2022 & Classification & Adversarial Detection & OPA & Microscopy \\
			
			\cite{hu2022adversarial_D29} & 2022 & Classification & Adversarial Training & DDN & MRI \\
			
			\cite{rodriguez2022role_D30} & 2022 & Classification & Adversarial Training & FGSM, PGD & OCT, X-ray, Dermoscopy \\
			
			\cite{ma2022adaptive_D53} & 2022 & \begin{tabular}[c]{@{}c@{}} Segmentation, Object Detection, \\ Landmark Detection \end{tabular} & Adversarial Training & PGD, I-FGSM & MRI, X-ray, Microscopy \\
			
			\cite{xie2022effective_D38} & 2022 & Classification & Adversarial Training & FGSM, PGD, BIM & CT, MRI, X-ray  \\
			
			\cite{wang2022fight_D43} & 2022 & Classification & Pre-processing & \begin{tabular}[c]{@{}c@{}} FGSM, BIM, CW, \\ PGD, AA, DI-FGSM \end{tabular} & Dermoscopy \\
			
			\cite{ghaffari2022adversarial_D28} & 2022 & Classification & Feature Enhancement & FGSM, PGD, FAB, SquareAttack & Microscopy \\
			
			\cite{maliamanis2022resilient_D39} & 2022 & Classification, Segmentation & Adversarial Training & \begin{tabular}[c]{@{}c@{}} FGSM, PGD, SquareAttack, \\ \textbf{Moment-based Adversarial Attack} \end{tabular} & X-ray, Microscopy \\
			
			\cite{almalik2022self_D42} & 2022 & Classification & Adversarial Detection, Feature Enhancement & FGSM, PGD, BIM, AutoPGD & X-ray, Fundoscopy \\
			
			\cite{SunXVG22_D44} & 2022 & Classification & Adversarial Training, Feature Enhancement & FGSM, PGD, CW & Ultrasound \\
			
			\cite{pandey2022adversarially_D52} & 2022 & Segmentation & Feature Enhancement & FGSM, PGD, SMA & CT \\
			
			\cite{bharath2022analysis_D58} & 2022 & Classification & Adversarial Training, Distillation & L-BFGS, FGSM & Fundoscopy \\
			
			\cite{roh2022impact_D59} & 2022 & Classification & Adversarial Training & FGSM & Microscopy \\
			
			\cite{daanouni2022nsl_D64} & 2022 & Classification                                 & Feature Enhancement & FGSM & Fundoscopy \\
			
			\cite{le2022efficient_D65} & 2022 & Segmentation & Pre-processing & DAG, I-FGSM & MRI, X-ray, Fundoscopy \\
			
			\cite{chen2022enhancing_D31} & 2022 & Segmentation & Adversarial Training & PGD & MRI \\
			
			\cite{shi2022robust_D57} & 2022 & Classification & Feature Enhancement & FGSM, PGD & X-ray, Fundoscopy \\
			
			
			
			
			
%			\bottomrule
			\hline
		\end{tabular}
	}
	\vspace{-15pt}
\end{table*}

\vspace{-3mm}
\subsection{Adversarial Training}

% 
The majority of medical adversarial defense methods concentrate on adversarial training to establish robust diagnosis systems. Among them, a large proportion of works extend existing adversarial training methods for natural images to medical classification tasks \cite{paul2020mitigating_D8, xu2021towards_D9, vatian2019impact_D13, wu2020classification_D15, hu2022adversarial_D29, joel2022using_D32}. Vatian \textit{et al.} \cite{vatian2019impact_D13} investigated adversarial examples for medical imaging and tried several approaches to defend against these malicious examples. Specifically, both FGSM \cite{GoodfellowSS14} and JSMA \cite{papernot2016limitations} are incorporated to generate adversarial examples during the adversarial training stage. To further achieve better robustness, the authors also utilized the Gaussian noise to augment adversarially trained data and replaced the original Rectified Linear Units (ReLU) with Bounded ReLU. In addition to providing a robustness evaluation of several computer-aided diagnosis models, Xu \textit{et al.} adapt PGD-based adversarial training \cite{MadryMSTV18} and Misclassification Aware adveRsarial Training (MART) \cite{wang2019improving} to robustify computer-aided diagnosis models. A new medical dataset named \textit{Robust-Benchmark} is also proposed to evaluate the robustness of common perturbations. The above-mentioned works mainly focus on enhancing the adversarial robustness of a single model. An ensemble adversarial training method \cite{TramerKPGBM18} is also extended to establish a robust malignancy prediction system based on lung nodules \cite{paul2020mitigating_D8}. Specifically, various models with different architectures are adversarially trained with different initialized weights in advance. The ensemble prediction results can be further obtained by averaging the output probabilities from adversarially trained models.


% 
In addition to transferring natural defense methods to diagnosis models, there also exist several adversarial defense methods that are tailored for medical image analysis \cite{lal2021adversarial_D3, liu2020no_D12, SunXVG22_D44, maliamanis2022resilient_D39}. Liu \textit{et al.} \cite{liu2020no_D12} investigate three types of adversarial augmentation examples that can be added to the training dataset for robustness improvement. Projected Gradient Descent (PGD) \cite{MadryMSTV18} is utilized to iteratively search the worst latent code to synthesize adversarial nodules that the target diagnosis model fails to detect. Furthermore, image-level adversarial perturbations are also generated to augment the training data. Extensive experiments demonstrate the effectiveness of the proposed method in enhancing the detection performance on real CT data and also robustness to unforeseen noise perturbations. Moreover, the Multi-Instance Robust Self-Training method with Drop-Max layer (MIRST-DM) \cite{SunXVG22_D44} is proposed to learn a smooth decision boundary for robust classification of breast cancer. In particular, MIRST-DM employs a sequence of adversarial images produced during the iterative adversary generation stage to accelerate the model convergence and also boost the robustness. The drop-max layer is proposed to discard the maximum value and forward the second-largest value to the next layer, which can depress unstable features to enhance the adversarial robustness. Speckle noise is the most common noise in retinal fundus images, which may hinder subsequent detection.

% 
Although an array of adversarial training methods have been explored in the context of medical image analysis, the majority of them concentrate on supervised learning. Nonetheless, the manual annotation cost for medical images can sometimes be extremely expensive. To explore a low-cost training paradigm to enhance adversarial robustness for computer-aided diagnosis models, Li \textit{et al.} \cite{li2021defending_D1} proposed a novel medical defense method based on Semi-Supervised Adversarial Training (SSAT). Typically, SSAT produces the pseudo labels for unlabeled images in advance and then minimizes the empirical risk to enhance the network robustness. Furthermore, a systematic investigation of the adversarial robustness in the context of biomedical image analysis \cite{anand2020self_D48} has indicated that self-supervised learning can give better robustness and also clean performance than transfer learning for small medical datasets. 

% 
Except for adversarial training for biomedical image classification, several academic works explore improving the intrinsic network robustness for medical segmentation \cite{ren2019brain_D11, li2019volumetric_D46, chen2022enhancing_D31, chen2020realistic_D14}. Generally, adversarial training can result in a clean accuracy drop \cite{zhang2019theoretically, XieY20, RadeM22}, which is unacceptable for various medical diagnosis applications. To mitigate this issue, Ma \textit{et al.} \cite{ma2020increasing_D5} proposed the Increasing Margin Adversarial (IMA) training method to adjust the upper bound of each adversarial perturbation during the training stage so that the intensity of the perturbation can be adaptively reduced. Similar to friendly adversarial training \cite{zhang2020attacks}, this work is inspired by the hypothesis that excessively strong adversarial examples might mislead the training process to cause a clean performance drop. The generated proper adversarial examples can effectively reduce the occurrence of overfitting decision boundary that impairs the standard performance of biomedical image applications. Chen \textit{et al.} \cite{chen2020realistic_D14} proposed a novel adversarial augmentation method for enhancing intrinsic network robustness via simulating underlying artifacts in clinical MR imaging. Specifically, the authors utilize the PGD method to search the specific control points to produce an adversarial bias field that can disrupt the intensity of original images by multiplication. Augmenting these physical adversarial examples into the training set can further promote robust feature learning for MR image segmentation, which is also effective in both low-data and cross-population settings.

% 
Aside from adversarial training for medical pattern recognition systems, several researchers spare no effort to improving the intrinsic robustness for medical reconstruction tasks \cite{cheng2020addressing_D18, raj2020improving_D61}. Especially for limited angle tomography, Huang \textit{et al.} \cite{huang2018some_D21} investigated the vulnerability of U-Net \cite{ronneberger2015u} against the Poisson noise during the artifact reduction. The reconstruction network robustness can be further enhanced by adding projection-domain domain Poisson perturbation to training data. False-Negative Adversarial Feature (FNAF) \cite{cheng2020addressing_D18} is designed to simulate the worst-case perceptible small features in the clinical diagnosis setting, which disappear after MRI reconstruction. In the meantime, FNAF can also be embedded into the robust training process to improve MRI reconstruction for small and infrequent structures. Furthermore, Raj \textit{et al.} \cite{raj2020improving_D61} proposed a novel training strategy with theoretical analysis to improve the robustness of various image-level reconstruction tasks, including face reconstruction and CT reconstruction. Unlike previous adversary generation methods that resort to iterative optimization, an auxiliary generative network is utilized to produce adversarial examples during the robust training stage. The target image reconstruction network thus minimizes the reconstruction loss of adversary generation. 

\vspace{-3mm}
\subsection{Adversarial Detection}
% 
Different from establishing robustness during the training stage of computer-aided diagnosis models, adversarial detection aims at distinguishing adversarial examples from input examples during the application stage. In order to prevent subsequent misdiagnosis caused by adversarial examples, various adversarial detection methods have been proposed in the context of biomedical image analysis \cite{li2020robust_D24, alatalo2022detecting_D63, almalik2022self_D42}. In particular, medical adversarial detection can also be regarded as an anomaly detection problem, which can be solved by incorporating explainability techniques \cite{watson2021attack_D7}. Based on the observation that adversarial attack induces a shift in the distribution of SHAP values for medical imaging and electronic health record data, the authors thus proposed both full- and semi-supervised detection methods trained on SHAP values. Ma \textit{et al.} \cite{ma2021understanding_D10} provided a comprehensive analysis of medical adversarial examples and their relations to adversarial samples for natural imaging. Moreover, two subspace distance-based methods in terms of Kernel Density \cite{feinman2017detecting} and Local Intrinsic Dimensionality \cite{Ma0WEWSSHB18}, respectively. The robustness of Vision Transformers (ViT) \cite{DosovitskiyB0WZ21} for medical image classification is first investigated in \cite{almalik2022self_D42}. Specifically, the Kullback-Leibler (KL) divergence between various Multi-Layer Perceptrons (MLPs) can be applied to distinguish medical adversarial examples against ViT.

%
Although adversarially trained models can improve the in-distribution robustness of DNNs, they still suffer from a significant robustness drop against Out-Of-Distribution (OOD) adversarial examples \cite{augustin2020adversarial}. To further mitigate the disruption from OOD adversarial examples, Li \textit{et al.} \cite{li2021defending_D1} proposed an unsupervised adversarial detection method based on the latent features from the penultimate layer of computer-aided diagnosis models. Specifically, a Gaussian mixture model is employed to estimate the probability density for these latent features of clean examples. During the inference stage, the unsupervised detector rejects OOD adversarial examples based on the deviation of the extracted latent features from their corresponding probability density.

% 
Most existing adversarial detection methods are designed for the medical classification scenario. There still remains limited research devoted to detecting adversarial examples for medical segmentation models. Park \textit{et al.} \cite{park2020robustification_D19} resorted to the frequency domain to better distinguish adversarial examples based on the reconstruction error. In addition, the reformer network \cite{meng2017magnet} is also incorporated to purify the input medical images to the manifold of legitimate examples for the subsequent segmentation stage.


\vspace{-3mm}
\subsection{Image-level Pre-processing}
In general, an adversarial image consists of a clean image and the corresponding adversarial perturbation. In the meantime, DNNs have been demonstrated to achieve high performance for clean images while remaining susceptible to adversarial examples \cite{SzegedyZSBEGF13, ilyas2019adversarial}. Hence, denoising the adversarial example to get rid of the perturbation part can further facilitate the subsequent network diagnosis. Furthermore, there is no need to re-train or modify medical models when applying image-level pre-processing, which can be convenient and safe in the context of biomedical image analysis. In this section, we introduce several image-level pre-processing works concentrating on protecting computer-aided diagnosis systems against adversarial examples. 

% 
The vast majority of pre-processing-based defense methods are designed for medical classification tasks \cite{yao2021medical_D45, rao2020thorough_A45_D60, kansal2022defending_D51}. Hence, the image-level pre-processing is required not to destroy distinguishable parts for the subsequent diagnosis. The Medical Retrain-less Diagnostic Framework (MedRDF) \cite{xu2022medrdf_D2} is proposed to convert a pre-trained non-robust diagnosis model into a robust one during the inference stage. Specifically, MedRDF first creates various copies of input images that are perturbed by isotropic noises, respectively. These copies are predicted via majority voting after denoising by a custom denoiser. In addition, a robust metric is also proposed to provide the confidence score of MedRDF diagnosis, which can assist doctors in clinical practice. Instead of pixel-level denoising, Kansal \textit{et al.} \cite{kansal2022defending_D51} extended the High-level representation Guided Denoiser (HGD) \cite{liao2018defense} to protect medical image applications against adversarial examples in both white-box and black-box settings. The guidance of high-level information can further facilitate the image-level elimination of the adversarial effect to the final diagnosis rather than visual disruption. 

% 
Current research on adversarial pre-processing for robust medical segmentation systems is still lacking and time-consuming. To this end, Liu \textit{et al.} \cite{liu2020defending_D23} proposed a low-cost image compression-based method to eliminate image-level adversarial perturbations against biomedical segmentation models. In particular, a fine-grained frequency refinement approach is utilized to redesign the quantization table in JPEG compression-based pre-processing. Moreover, the optimized quantization step constraints are carefully set to prioritize defense efficiency and also compensate for the accuracy reduction caused by quantization errors. Furthermore, Le \textit{et al.} \cite{le2022efficient_D65} introduced a learnable adversarial denoising method by utilizing the U-Net \cite{ronneberger2015u} as the defender model. The defender model first pre-processes the input medical images before feeding them into the subsequent medical segmentation models that are required to be effective for both clean examples and adversarial examples. 


\vspace{-3mm}
\subsection{Feature Enhancement}
% 
% 
Adversarial examples have been demonstrated to be attributed to non-robust features (extracted from certain patterns in the data distribution) \cite{ilyas2019adversarial}, resulting in the mismatching of robustness between human and machine vision. Therefore, it is more than necessary to enhance the feature representation for robust inference systems. For a formal definition, we regard the modification of architectures or mapping functions as the feature enhancement in this paper. A myriad of feature enhancement methods have been designed to boost the robustness of medical classification models \cite{shi2022robust_D57, han2021advancing_D35, chen2021enhancing_D33, daanouni2022nsl_D64}. Pooling layers are commonly used in neural network modeling to reduce the dimensions of feature maps. Taghanaki \textit{et al.} \cite{taghanaki2019kernelized_D20} modified the medical classification networks by replacing max-pooling layers with average-pooling layers. This modification significantly boosts the robustness against adversarial examples for different network architectures. The plausible reason for the robustness enhancement is that average-pooling can capture more global-level contextual information instead of selecting the maximum value as the max-pooling, which increases the difficulty of adversarial attacks. Moreover, AutoEncoder (AE) can also be embedded into computer-aided diagnosis models for the feature-level denoising \cite{xue2019improving_D22}, which is independent of the image-level pre-processing procedure. In the meantime, the guidance of feature invariance is incorporated to reduce the model sensitivity against adversaries. In accordance with \cite{xie2020adversarial}, Han \textit{et al.} \cite{han2021advancing_D35} also introduced dual-batch normalization to adversarial training, which significantly improves the robustness of diagnostic models without the degradation of clean accuracy.

Apart from the mainstream medical classification task, feature enhancement has also been applied in several medical imaging tasks, including segmentation \cite{carannante2021trustworthy_D54}, object detection \cite{taghanaki2019kernelized_D20}, and low-level vision \cite{stimpel2019multi_D62}. Non-Local Context Encoder (NLCE) \cite{he2019non_D16} is proposed to improve the robustness of biomedical image segmentation models as a plug-and-play module. Similar to the observation from \cite{taghanaki2019kernelized_D20}, the NLCE module aims at capturing the global-level spatial dependencies and also contexts to strengthen features, which can be easily applied to various DNN-based medical image segmentation models. Stimpel \textit{et al.} \cite{stimpel2019multi_D62} utilize the guided filter with a learned guidance map for medical image super resolution and denoising. The guided filter exhibits a solid ability to limit the effectiveness of adversarial examples on generated outputs.


\vspace{-3mm}
\subsection{Knowledge Distillation}
% 
% 
In the machine learning community, knowledge distillation can be an effective technique to transfer the learned knowledge from a complex (teacher) model to a lightweight (student) model. Accordingly, self-distillation refers specifically to the situation when the teacher and student models share the same network architecture. Moreover, adversarial knowledge distillation has also been widely explored for the natural imaging domain \cite{goldblum2020adversarially, zi2021revisiting, ZhuY0ZL0ZXY22}, which transfers the adversarial robustness from the large teacher model to a lightweight student model. 

There also exist a few works that concentrate on enhancing adversarial robustness by knowledge distillation in the context of medical image analysis. The Robust Stochastic Knowledge Distillation (RoS-KD) framework \cite{jaiswal2022ros_D66} is designed to distill robust knowledge from multiple teacher models to a specific student model. Note that the distillation is conducted on noisy labeled data to efficiently simulate practical adversarial examples. The smooth parameter update mechanism is also proposed using weight averaging on multiple checkpoints. Furthermore, Liu \textit{et al.} \cite{liu2021robustifying_D6} adapted the defensive distillation \cite{papernot2016distillation} to brain tumor segmentation for MR data. The authors also showed that the defensive distillation method achieves a better robust performance against FGSM-based attacks than adversarial training.




\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{imgs/Adv_cases.pdf}
	\vspace{-8mm}
	\caption{Visualization of medical adversarial examples with predictions under diverse perturbation size $\epsilon$. The generated segmentation masks are superimposed on the original images for visualization.}
	\vspace{-3mm}
	\label{fig:3}
\end{figure}

\vspace{-3mm}
\section{Experimental Evaluation}
\label{sec:5}
% 
In this section, we provide a systematic evaluation of both adversarial attack and defense for computer-aided diagnosis models. We first introduce our experimental settings, including datasets and measurements. Moreover, we measure the effect of diverse adversarial attack methods on several medical imaging tasks in various scenarios. We also establish a benchmark of adversarial training for biomedical image analysis systems to facilitate future research. 

\begin{table}[!t]
	\centering
%	\tiny 
	\caption{White-box robustness results under different attack configurations using ResNet-18 for medical classification. We present accuracy (\%) and percentage decrease for binary and multi-class classification tasks.}
    \vspace{-2mm}
	%\small
	\renewcommand{\arraystretch}{0.8}
        % \resizebox{0.75\linewidth}{!}{
    \resizebox{\linewidth}{!}{  
	\begin{tabular}{cccccc}
		\toprule
		\multirow{2}{*}{Adversarial Type}&\multirow{2}{*}{$\epsilon$}&\multicolumn{2}{c}{Fundoscopy \cite{decenciere2014feedback}}&\multicolumn{2}{c}{Dermoscopy \cite{codella2018skin}} \\
		\cmidrule(lr){3-4} \cmidrule(lr){5-6}
		&&Binary&Multi-class&Binary&Multi-class\\
		\midrule
		None & 0 & 71.3 & 50.0 & 64.9 & 60.0\\
		\midrule
		\multirow{4}{*}{FGSM \cite{GoodfellowSS14}} & 1/255 & 47.1$_\textit{(-33.9\%)}$ & 22.1$_\textit{(-55.8\%)}$ & 41.5$_\textit{(-36.1\%)}$ & 12.5$_\textit{(-79.2\%)}$ \\
		& 2/255 & 38.8$_\textit{(-45.6\%)}$ & 13.8$_\textit{(-72.4\%)}$ & 37.6$_\textit{(-42.1\%)}$ & 4.8$_\textit{(-92.0\%)}$ \\
		& 4/255 & 27.9$_\textit{(-60.9\%)}$ & 9.2$_\textit{(-81.6\%)}$ & 35.7$_\textit{(-45.0\%)}$ & 4.5$_\textit{(-92.5\%)}$ \\
		& 8/255 & 24.2$_\textit{(-66.1\%)}$ & 20.4$_\textit{(-59.2\%)}$ & 34.5$_\textit{(-46.8\%)}$ & 8.0$_\textit{(-86.7\%)}$ \\
		\midrule
		\multirow{4}{*}{PGD \cite{MadryMSTV18}} & 1/255 & 21.3$_\textit{(-70.1\%)}$ & 12.5$_\textit{(-75.0\%)}$ & 38.4$_\textit{(-40.8\%)}$ & 8.9$_\textit{(-85.1\%)}$ \\
		& 2/255 & 8.8$_\textit{(-87.7\%)}$ & 4.6$_\textit{(-90.8\%)}$ & 22.8$_\textit{(-64.9\%)}$ & 2.6$_\textit{(-95.7\%)}$ \\
		& 4/255 & 1.7$_\textit{(-97.6\%)}$ & 0.4$_\textit{(-99.2\%)}$ & 12.5$_\textit{(-80.7\%)}$ & 0.8$_\textit{(-98.7\%)}$ \\
		& 8/255 & 0.0$_\textit{(-100.0\%)}$ & 0.0$_\textit{(-100.0\%)}$ & 12.8$_\textit{(-80.3\%)}$ & 0.4$_\textit{(-99.3\%)}$ \\
		\midrule
		\multirow{4}{*}{CW \cite{carlini2017towards}} & 1/255 & 15.8$_\textit{(-77.8\%)}$ & 12.5$_\textit{(-75.0\%)}$ & 37.5$_\textit{(-42.2\%)}$ & 9.1$_\textit{(-84.8\%)}$ \\
		& 2/255 & 7.1$_\textit{(-90.0\%)}$ & 6.7$_\textit{(-86.6\%)}$ & 22.5$_\textit{(-65.3\%)}$ & 2.5$_\textit{(-95.8\%)}$ \\
		& 4/255 & 1.3$_\textit{(-98.2\%)}$ & 2.1$_\textit{(-95.8\%)}$ & 9.3$_\textit{(-85.7\%)}$ & 0.9$_\textit{(-98.5\%)}$ \\
		& 8/255 & 0.0$_\textit{(-100.0\%)}$ & 0.0$_\textit{(-100.0\%)}$ & 2.1$_\textit{(-96.8\%)}$ & 0.4$_\textit{(-99.3\%)}$ \\
		\midrule
		\multirow{4}{*}{AA \cite{croce2020reliable}} & 1/255 & 12.8$_\textit{(-82.1\%)}$ & 6.7$_\textit{(-86.6\%)}$ & 36.9$_\textit{(-43.1\%)}$ & 8.5$_\textit{(-85.8\%)}$ \\
		& 2/255& 3.8$_\textit{(-94.6\%)}$ & 2.5$_\textit{(-95.0\%)}$ & 22.1$_\textit{(-66.0\%)}$ & 2.4$_\textit{(-96.0\%)}$ \\
		& 4/255& 0.0$_\textit{(-100.0\%)}$ & 0.0$_\textit{(-100.0\%)}$ & 8.4$_\textit{(-87.1\%)}$ & 0.8$_\textit{(-98.7\%)}$ \\
		& 8/255& 0.0$_\textit{(-100.0\%)}$ & 0.0$_\textit{(-100.0\%)}$ & 1.6$_\textit{(-97.5\%)}$ & 0.1$_\textit{(-99.8\%)}$ \\
		\bottomrule
			
	\end{tabular}
    }
 % }
	\vspace{-4mm}
	\label{tab:3}
\end{table}

\vspace{-3mm}
\subsection{Experimental Settings}
\subsubsection{Datasets}
% 
In this paper, we mainly use four publicly available benchmark datasets to explore adversarial attack and defense for medical image analysis: 1) \textbf{Messidor}\footnote{Kindly provided by the Messidor program partners (see https://www.adcis.net/en/third-party/messidor/).} dataset \cite{decenciere2014feedback}, containing 1,200 eye fundus color numerical images for detecting diabetic retinopathy of four classes according to retinopathy grade; 2) International Skin Imaging Collaboration (\textbf{ISIC 2017}) dataset \cite{codella2018skin} of 2,750 dermoscopic images of three classes for skin lesion classification and segmentation. 3) \textbf{ChestX-ray 14} dataset \cite{wang2017chestx}, consisting of 112,120 frontal-view X-ray images of 14 thorax diseases. 4) \textbf{COVID-19} database \cite{chowdhury2020can}, comprising 21,165 chest X-ray images with lung masks for segmentation. 

For medical classification, we consider both binary and multi-class classification tasks. Following \cite{xu2021towards_D9}, we randomly select 960 fundus images from Messidor dataset as the training set. All the fundus images are processed with several data augmentation operations, including random rotating and flipping. We also conduct pre-processing on skin lesion images from ISIC 2017 dataset with resizing and center-cropping. Due to the computational budget, we evenly sample 10,000 X-ray images with random flipping and normalization from ChestX-ray 14 dataset and then randomly choose 8,000 examples from them as the training set. Considering the computational cost, we sample 2,750 X-ray images and corresponding lung masks from the COVID-19 database for medical segmentation. Following the dataset division from ISIC 2017 dataset, 2,000 chest X-ray images are utilized for training, and the rest are used for evaluation.



\begin{table}[!t]
	\centering
	\caption{White-box robustness results against PGD attack under different attack configurations using U-Net for medical segmentation.}
        \vspace{-3mm}
	\footnotesize
	\renewcommand{\arraystretch}{0.8}
        % \resizebox{0.75\linewidth}{!}{
    \resizebox{\linewidth}{!}{  
	\begin{tabular}{cccccc}
		\toprule
		\multirow{2}{*}{Adversarial Loss}&\multirow{2}{*}{$\epsilon$}&\multicolumn{2}{c}{Dermoscopy \cite{codella2018skin}}&\multicolumn{2}{c}{COVID-19 \cite{chowdhury2020can}} \\
		\cmidrule(lr){3-4} \cmidrule(lr){5-6}
		&&mIOU&Dice&mIOU&Dice\\
		\midrule
		None & 0 & 0.818 & 0.883 & 0.977 & 0.988\\
		\midrule
		\multirow{4}{*}{BCE} & 1/255 & 0.580 & 0.680 & 0.747 & 0.840\\
		& 2/255 & 0.405 & 0.517 & 0.559 & 0.690\\
		& 4/255 & 0.248 & 0.354 & 0.355 & 0.492\\
		& 8/255 & 0.167 & 0.255 & 0.230 & 0.350\\
		\midrule
		\multirow{4}{*}{Dice} & 1/255 & 0.561 & 0.663 & 0.704 & 0.807\\
		& 2/255 & 0.340 & 0.450 & 0.473 & 0.610\\
		& 4/255 & 0.158 & 0.243 & 0.265 & 0.391\\
		& 8/255 & 0.082 & 0.140 & 0.152 & 0.246\\
		\bottomrule
			
	\end{tabular}
    }
 % }
	\vspace{-4mm}
	\label{tab:4}
\end{table}

\begin{table}[!t]
	\centering
	\caption{Black-box and no-box robust accuracy (\%) under different attack configurations using ResNet-18 for medical classification.}
        \vspace{-3mm}
	%\small
	\renewcommand{\arraystretch}{0.8}
        % \resizebox{0.75\linewidth}{!}{
    \resizebox{\linewidth}{!}{  
	\begin{tabular}{cccccc}
		\toprule
		\multirow{2}{*}{Adversarial Type}&\multirow{2}{*}{$\epsilon$}&\multicolumn{2}{c}{Fundoscopy \cite{decenciere2014feedback}}&\multicolumn{2}{c}{Dermoscopy \cite{codella2018skin}} \\
		\cmidrule(lr){3-4} \cmidrule(lr){5-6}
		&&Binary&Multi-class&Binary&Multi-class\\
		\midrule
		None & 0 & 71.3 & 50.0 & 64.9 & 60.0\\
		\midrule
		\multirow{4}{*}{\makecell{Square Attack \cite{andriushchenko2020square} \\ (\textbf{Black-box})}} & 1/255 & 13.3 & 9.6 & 45.1 & 19.3\\
		& 2/255 & 5.8 & 3.8 & 30.3 & 6.0\\
		& 4/255 & 0.0 & 0.4 & 15.6 & 1.6\\
		& 8/255 & 0.0 & 0.0 & 2.9 & 0.5\\
		\midrule
		\multirow{4}{*}{\makecell{PGD \cite{MadryMSTV18} \\ (\textbf{No-box})}} & 1/255 & 47.5 & 41.3 & 59.1 & 58.5\\
		& 2/255 & 36.7 & 37.9 & 49.7 & 55.6\\
		& 4/255 & 27.9 & 27.9 & 35.3 & 50.4\\
		& 8/255 & 10.4 & 18.7 & 21.2 & 38.5\\
		\midrule
		\multirow{4}{*}{\makecell{CW \cite{carlini2017towards} \\ (\textbf{No-box})}} & 1/255 & 42.1 & 37.1 & 59.1 & 58.0\\
		& 2/255 & 35.4 & 37.1 & 48.9 & 52.9\\
		& 4/255 & 35.4 & 40.4 & 34.8 & 41.7\\
		& 8/255 & 19.6 & 12.9 & 20.5 & 19.3\\
		\bottomrule
			
	\end{tabular}
    }
 % }
	\vspace{-6mm}
	\label{tab:5}
\end{table}

\vspace{-2mm}
\subsubsection{Evaluation Metrics}
% 
% 
Generally, adversarial attack aims at reducing the prediction success rate of neural networks. For instance, adversarial attacks against classification can misguide the target model to make a wrong prediction. Likewise, adversarial attacks against image segmentation concentrate on reducing the mean Intersection over Union (mIoU) or Dice coefficient between the generated segmentation mask and target mask, resulting in wrong segmentation results. Oppositely, adversarial defense focuses on keeping the network outputs of adversarial and clean examples as similar as possible. In other words, the adversarial defense contributes to enhancing the robustness of neural networks against adversarial examples. In this paper, we mainly focus on constructing a unified benchmark for medical adversarial training to enhance the robustness of computer-aided diagnosis systems.

Following the setting on RobustBench \cite{CroceASDFCM021}, we utilize ResNet-18 \cite{he2016deep} and MobileNetV2 \cite{sandler2018mobilenetv2} as the target networks to conduct adversarial attack and defense for medical classification tasks. For biomedical segmentation tasks, we adopt U-Net \cite{ronneberger2015u} and SegNet \cite{badrinarayanan2017segnet} to generate segmentation masks. We use Stochastic Gradient Descent (SGD) optimizer with the Nesterov momentum factor \cite{Nesterov1983AMF} of 0.9 and the cyclic learning rate schedule \cite{smith2019super} with the maximum learning rate of 0.01. In this paper, we mainly consider the most common scenario, attacks under $\ell_{\infty}$-norm threat model. We conduct all the experiments on a single NVIDIA Tesla A100. We report the accuracies on clean examples as well as adversarial examples obtained by five strong adversarial attack methods: FGSM \cite{GoodfellowSS14}, PGD \cite{MadryMSTV18} with 20 steps with the step size of $1/255$, CW \cite{carlini2017towards}, Square Attack \cite{andriushchenko2020square}, and Auto Attack (AA) \cite{croce2020reliable}. Note that the maximum $\ell_{\infty}$-norm perturbation is set as $\epsilon=8/255$. For adversarial training, we adopt the PGD method to generate adversarial examples as the training data for 100 epochs.


\vspace{-2mm}
\subsection{Adversarial Attack}
\vspace{-1mm}
% 
In this section, we transfer several adversarial attack methods for natural images to the medical imaging domain under various attack scenarios for comprehensive evaluation. We also consider medical classification and segmentation in diverse biomedical imaging modalities.





\begin{table}[!t]
	\centering
	\caption{No-box robustness results against PGD attack under different attack configurations using U-Net for medical segmentation.}
        \vspace{-3mm}
	%\small
	\renewcommand{\arraystretch}{0.8}
        % \resizebox{0.75\linewidth}{!}{
    \resizebox{\linewidth}{!}{  
	\begin{tabular}{cccccc}
		\toprule
		\multirow{2}{*}{Adversarial Loss}&\multirow{2}{*}{$\epsilon$}&\multicolumn{2}{c}{Dermoscopy \cite{codella2018skin}}&\multicolumn{2}{c}{COVID-19 \cite{chowdhury2020can}} \\
		\cmidrule(lr){3-4} \cmidrule(lr){5-6}
		&&mIOU&Dice&mIOU&Dice\\
		\midrule
		None & 0 & 0.818 & 0.883 & 0.977 & 0.988\\
		\midrule
		\multirow{4}{*}{BCE} & 1/255 & 0.801 & 0.863 & 0.961 & 0.979\\
		& 2/255 & 0.783 & 0.851 & 0.933 & 0.963\\
		& 4/255 & 0.755 & 0.823 & 0.870 & 0.922\\
		& 8/255 & 0.698 & 0.780 & 0.774 & 0.856\\
		\midrule
		\multirow{4}{*}{Dice} & 1/255 & 0.807 & 0.875 & 0.966 & 0.982\\
		& 2/255 & 0.797 & 0.866 & 0.954 & 0.976\\
		& 4/255 & 0.775 & 0.848 & 0.926 & 0.959\\
		& 8/255 & 0.722 & 0.804 & 0.883 & 0.931\\
		
		\bottomrule
			
	\end{tabular}
    }
 % }
	\vspace{-4mm}
	\label{tab:6}
\end{table}

\begin{table}[!t]
	\centering
	\footnotesize
	\caption{Average time cost comparison of adversary generation methods for diverse medical imaging tasks.}
	\vspace{-3mm}
	%\small
	\renewcommand{\arraystretch}{0.8}
        % \resizebox{0.75\linewidth}{!}{
%    \resizebox{\linewidth}{!}{  
	\begin{tabular}{ccc}
		\toprule
		\multicolumn{3}{l}{\textbf{Medical Classification:}} \\
		\midrule
		Adversarial Type & Fundoscopy \cite{decenciere2014feedback} & Dermoscopy \cite{codella2018skin}\\
		
		\midrule
		FGSM \cite{GoodfellowSS14} & 0.6 s & 1.9 s\\
		PGD \cite{MadryMSTV18} & 22.6 s & 40.3 s\\
		CW \cite{carlini2017towards} & 33.7 s & 61.0 s\\
		AA \cite{croce2020reliable} & 738.6 s & 1464.0 s\\
		Square Attack \cite{andriushchenko2020square} & 548.1 s & 1769.3 s\\
		\midrule
		\multicolumn{3}{l}{\textbf{Medical Segmentation:}} \\
		\midrule
		Adversarial Type & Dermoscopy \cite{codella2018skin} & COVID-19 \cite{chowdhury2020can} \\
		\midrule
		PGD-BCE \cite{MadryMSTV18} & 78.6 s & 78.7 s\\
		PGD-Dice \cite{MadryMSTV18} & 84.5 s & 84.8 s\\
		
		\bottomrule
			
	\end{tabular}
%    }
 % }
	\vspace{-4mm}
	\label{tab:7}
\end{table}

\begin{table}[!t]
	\centering
	\caption{White-box \textbf{B}inary (\textbf{B}) and \textbf{M}ulti-class (\textbf{M}) accuracy (\%) of adversarially trained medical classification models in different settings. }
        \vspace{-2mm}
	%\small
	\renewcommand{\arraystretch}{0.8}
        % \resizebox{0.75\linewidth}{!}{
    \resizebox{\linewidth}{!}{  
	\begin{tabular}{cccccccccccc}
		\toprule
		\multicolumn{6}{l}{\textbf{Fundoscopy \cite{decenciere2014feedback} Classification: }} \\
		\midrule
		\multirow{2}{*}{Adversarial Type} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{PGD-AT \cite{MadryMSTV18}} & \multicolumn{2}{c}{TRADES \cite{zhang2019theoretically}} & \multicolumn{2}{c}{MART \cite{wang2019improving}} & \multicolumn{2}{c}{HAT \cite{RadeM22}} \\
		\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
		&&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}\\
		\midrule
		None & 0 & 71.3 & 50.0 & 58.8 & 46.7 & 62.1 & 44.6 & 55.0 & 42.5 & 62.5 & 46.7 \\
		\midrule
		\multirow{4}{*}{FGSM \cite{GoodfellowSS14}} & 1/255 & 47.1 & 22.1 & 57.5 & 45.8 & 58.3 & 41.7 & 54.6 & 42.5 & 56.7 & 42.5 \\
		& 2/255 & 38.8 & 13.8 & 55.8 & 44.2 & 55.8 & 37.5 & 54.6 & 42.1 & 53.8 & 39.2 \\
		& 4/255 & 27.9 & 9.2 & 52.9 & 39.2 & 48.8 & 30.4 & 52.5 & 40.8 & 46.3 & 30.8 \\
		& 8/255 & 24.2 & 20.4 & 45.4 & 35.8 & 29.6 & 19.2 & 49.6 & 39.2 & 28.8 & 13.3 \\
		\midrule
		\multirow{4}{*}{PGD \cite{MadryMSTV18}} & 1/255 & 21.3 & 12.5 & 57.5 & 45.8 & 58.3 & 41.7 & 54.6 & 42.5 & 56.7 & 42.5 \\
		& 2/255 & 8.8 & 4.6 & 55.8 & 42.9 & 55.8 & 37.5 & 54.6 & 42.1 & 53.8 & 38.3 \\
		& 4/255 & 1.7 & 0.4 & 52.6 & 36.7 & 46.7 & 29.2 & 52.5 & 40.4 & 46.3 & 27.9 \\
		& 8/255 & 0.0 & 0.0 & 43.3 & 30.0 & 25.8 & 15.4 & 49.2 & 37.1 & 21.3 & 11.3 \\
		\midrule
		\multirow{4}{*}{CW \cite{carlini2017towards}} & 1/255 & 15.8 & 12.5 & 57.5 & 45.4 & 58.3 & 40.8 & 54.6 & 42.5 & 56.7 & 42.5 \\
		& 2/255 & 7.1 & 6.7 & 55.4 & 41.3 & 55.8 & 35.8 & 54.6 & 41.7 & 53.8 & 36.7 \\
		& 4/255 & 1.3 & 2.1 & 52.5 & 35.4 & 46.7 & 27.1 & 52.5 & 40.4 & 46.3 & 24.6 \\
		& 8/255 & 0.0 & 0.0 & 42.5 & 27.9 & 26.3 & 13.3 & 49.2 & 36.3 & 22.5 & 7.9 \\
		\midrule
		\multirow{4}{*}{AA \cite{croce2020reliable}} & 1/255 & 12.8 & 6.7 & 57.5 & 45.8 & 58.3 & 40.8 & 54.6 & 42.5 & 56.7 & 42.5 \\
		& 2/255 & 3.8 & 2.5 & 55.2 & 41.3 & 55.8 & 36.3 & 54.6 & 41.7 & 53.8 & 37.5 \\
		& 4/255 & 0.0 & 0.0 & 52.3 & 35.0 & 46.3 & 26.3 & 52.5 & 40.4 & 46.3 & 23.3 \\
		& 8/255 & 0.0 & 0.0 & 42.1 & 28.3 & 25.4 & 12.5 & 49.2 & 36.7 & 20.0 & 1.7 \\
		
		
		\midrule

		\multicolumn{6}{l}{\textbf{Dermoscopy \cite{codella2018skin} Classification:}} \\
		\midrule
		\multirow{2}{*}{Adversarial Type} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{PGD-AT \cite{MadryMSTV18}} & \multicolumn{2}{c}{TRADES \cite{zhang2019theoretically}} & \multicolumn{2}{c}{MART \cite{wang2019improving}} & \multicolumn{2}{c}{HAT \cite{RadeM22}} \\
		\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
		
		&&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}\\
		\midrule
		None & 0 & 64.9 & 60.0 & 61.2 & 52.0 & 62.3 & 52.9 & 61.1 & 49.6 & 68.6 & 57.1 \\
		\midrule
		\multirow{4}{*}{FGSM \cite{GoodfellowSS14}} & 1/255 & 41.5 & 12.5 & 58.0 & 48.0 & 58.1 & 49.5 & 58.9 & 45.8 & 63.3 & 51.1 \\
		& 2/255 & 37.6 & 4.8 & 55.6 & 44.4 & 53.5 & 44.0 & 56.7 & 42.4 & 58.8 & 46.8 \\
		& 4/255 & 35.7 & 4.5 & 49.2 & 38.0 & 44.9 & 35.3 & 51.6 & 33.7 & 48.5 & 35.3 \\
		& 8/255 & 34.5 & 8.0 & 39.6 & 25.5 & 33.1 & 22.3 & 40.9 & 21.7 & 37.5 & 20.7 \\
		\midrule
		\multirow{4}{*}{PGD \cite{MadryMSTV18}} & 1/255 & 38.4 & 8.9 & 57.9 & 48.0 & 58.0 & 49.2 & 58.9 & 45.6 & 63.2 & 51.1 \\
		& 2/255 & 22.8 & 2.6 & 55.6 & 44.4 & 52.9 & 43.7 & 56.5 & 41.9 & 57.6 & 45.9 \\
		& 4/255 & 12.5 & 0.8 & 48.4 & 36.8 & 42.0 & 32.8 & 49.9 & 31.2 & 44.4 & 33.6 \\
		& 8/255 & 12.8 & 0.4 & 35.1 & 22.4 & 21.5 & 14.4 & 34.5 & 14.9 & 25.7 & 14.4 \\
		\midrule
		\multirow{4}{*}{CW \cite{carlini2017towards}} & 1/255 & 37.5 & 9.1 & 57.9 & 47.9 & 58.0 & 49.1 & 58.9 & 45.1 & 63.2 & 50.5 \\
		& 2/255 & 22.5 & 2.5 & 55.6 & 43.6 & 52.9 & 43.2 & 56.5 & 41.3 & 57.6 & 45.3 \\
		& 4/255 & 9.3 & 0.9 & 48.4 & 36.0 & 41.9 & 31.9 & 49.9 & 30.9 & 44.4 & 32.8 \\
		& 8/255 & 2.1 & 0.4 & 36.3 & 22.7 & 21.9 & 14.3 & 34.7 & 15.2 & 26.5 & 13.9 \\
		\midrule
		\multirow{4}{*}{AA \cite{croce2020reliable}} & 1/255 & 36.9 & 8.5 & 57.9 & 47.9 & 58.0 & 49.1 & 58.9 & 45.5 & 63.2 & 50.7 \\
		& 2/255 & 22.1 & 2.4 & 55.6 & 43.7 & 52.7 & 43.1 & 56.4 & 41.1 & 57.6 & 45.3 \\
		& 4/255 & 8.4 & 0.8 & 48.4 & 35.5 & 41.5 & 31.6 & 49.7 & 30.3 & 44.1 & 32.8 \\
		& 8/255 & 1.6 & 0.1 & 34.8 & 20.9 & 18.9 & 12.7 & 32.3 & 14.0 & 22.3 & 12.5 \\
		
		
		\bottomrule
			
	\end{tabular}
    }
 % }
	\vspace{-4mm}
	\label{tab:8}
\end{table}

\subsubsection{White-box Attack}
% 
To begin with, we present several cases of the adversarial attack against diverse medical diagnosis tasks under different attack strengths in Fig. \ref{fig:3}. We can easily observe that computer-aided diagnosis models are extremely vulnerable to adversarial examples, even with a small adversarial perturbation size. The medical classification models are misguided by adversaries to make high-confidence misdiagnoses, which can further interfere with clinical judgment. In addition, incorrect segmentation results induced by adversaries can also lead to false treatment suggestions.

% 
To comprehensively measure the adversarial effect on medical classification models, we report the accuracy against several white-box attacks in both binary and multi-class classification settings, as shown in Table \ref{tab:3}. Note that we adopt the cross-entropy loss function for adversary generation. It can be seen that there exists a significant plunge of accuracy as increasing the adversarial perturbation size. In addition, we can easily observe that multi-class classification models suffer from a more severe accuracy drop than binary classification models. However, existing medical adversarial defense methods mainly focus on binary diagnosis, which is relatively easy to construct robust models. In this paper, we claim that constructing adversarial robustness for multi-class medical classification is more challenging and can further be generalized to various clinical scenarios.

In addition to adversarial attacks against medical classification, we also evaluate the performance of medical segmentation models against PGD-based adversarial examples, as shown in Table \ref{tab:4}. We adopt Binary Cross-Entropy (BCE) loss and also Dice loss as the adversarial loss for adversary generation. It can be seen that adversarial examples generated by reducing Dice loss can achieve a better attack performance against medical segmentation models.



\begin{table}[!t]
	\centering
	\caption{White-box robustness results against PGD attack under different attack settings using U-Net for biomedical segmentation.}
        \vspace{-2mm}
	%\small
	\renewcommand{\arraystretch}{0.8}
        % \resizebox{0.75\linewidth}{!}{
    \resizebox{\linewidth}{!}{  
	\begin{tabular}{ccccccc}
		\toprule
		\multirow{2}{*}{Task} & \multirow{2}{*}{Adversarial Loss} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{ADV} \\
		\cmidrule(lr){4-5} \cmidrule(lr){6-7}
		& & & mIOU & Dice & mIOU & Dice \\
		\midrule
		\multirow{9}{*}{Dermoscopy \cite{codella2018skin}} & None & 0 & 0.818 & 0.883 & 0.803 & 0.867 \\
		\cmidrule(lr){2-7}
		& \multirow{4}{*}{BCE} & 1/255 & 0.580 & 0.680 & 0.788 &0.855 \\
		& & 2/255 & 0.405 & 0.517 & 0.773 & 0.842 \\
		& & 4/255 & 0.248 & 0.354 & 0.733 & 0.810 \\
		& & 8/255 & 0.167 & 0.255 & 0.601 & 0.700 \\
		\cmidrule(lr){2-7}
		& \multirow{4}{*}{Dice} & 1/255 & 0.561 & 0.663 & 0.786 & 0.853 \\
		& & 2/255 & 0.340 & 0.450 & 0.767 & 0.838 \\
		& & 4/255 & 0.158 & 0.243 & 0.723 & 0.803 \\
		& & 8/255 & 0.082 & 0.140 & 0.594 & 0.694 \\
		
		\midrule
		\multirow{9}{*}{COVID-19 \cite{chowdhury2020can}} & None & 0 & 0.977 & 0.988 & 0.964 & 0.981 \\
		\cmidrule(lr){2-7}
		& \multirow{4}{*}{BCE} & 1/255 & 0.747 & 0.840 & 0.949 & 0.973 \\
		& & 2/255 & 0.559 & 0.690 & 0.931 & 0.963 \\
		& & 4/255 & 0.355 & 0.492 & 0.890 & 0.940 \\
		& & 8/255 & 0.230 & 0.350 & 0.812 & 0.887 \\
		\cmidrule(lr){2-7}
		& \multirow{4}{*}{Dice} & 1/255 & 0.704 & 0.807 & 0.964 & 0.981 \\
		& & 2/255 & 0.473 & 0.610 & 0.923 & 0.958 \\
		& & 4/255 & 0.265 & 0.391 & 0.880 & 0.932 \\
		& & 8/255 & 0.152 & 0.246 & 0.774 & 0.860 \\
		
		\bottomrule
			
	\end{tabular}
    }
 % }
	
	\label{tab:9}
\end{table}





\subsubsection{Black-box Attack}
% 
% 
Apart from white-box adversarial attacks against medical image analysis, we also evaluate the performance of computer-aided diagnosis models against black-box adversarial attacks, which can be a more practical attack scenario. We report the accuracy of medical classifiers against both Black-box and Restricted black-box (No-box) adversarial attacks in Table \ref{tab:5}. Note that the no-box attacks are conducted by transferring adversarial examples generated against a MobileNetV2 \cite{sandler2018mobilenetv2} model to the target ResNet-18 \cite{he2016deep} classification model. Due to the accessibility to outputs of the target model, the black-box adversarial attack can achieve a better attack success rate than no-box attacks. In the meantime, we can also observe that conducting no-box attacks for multi-class medical classification models is much harder than for binary classifiers. The plausible reason is that multi-class classifiers have more complex classification decision boundary, which varies with each architecture.

We also evaluate the effectiveness of restricted black-box adversarial attacks against medical segmentation models, as shown in Table \ref{tab:6}. The no-box attacks are obtained by transferring PGD-based adversarial examples generated against the SegNet \cite{badrinarayanan2017segnet} model to directly attack the target U-Net \cite{ronneberger2015u} segmentation model. Despite adopting the Dice loss as the adversarial loss achieves a superior performance in the white-box scenario, attacking the BCE loss can obtain more transferable adversarial examples for medical segmentation in the no-box setting.

\begin{table}[!t]
	\centering
	\footnotesize
	\caption{White-box robustness results under different attack configurations using ResNet-18 for multi-label medical classification. We report the AUC score (\%) of thorax disease classification models against PGD attack.}
        \vspace{-2mm}
	%\small
	\renewcommand{\arraystretch}{0.8}
        % \resizebox{0.75\linewidth}{!}{
    \resizebox{\linewidth}{!}{  
	\begin{tabular}{ccccc}
		\toprule
		Task & Adversarial Type & $\epsilon$ & NAT & PGD-AT \cite{MadryMSTV18} \\
		\midrule
		\multirow{5}{*}{\makecell{X-ray \cite{wang2017chestx} \\ (Multi-label)}} & None & 0 & 71.0 & 71.4 \\
		\cmidrule(lr){2-5}
		& \multirow{4}{*}{PGD \cite{MadryMSTV18}} & 1/255 & 23.3 & 67.1 \\
		& & 2/255 & 15.7 & 62.6 \\
		& & 4/255 & 14.0 & 53.1 \\
		& & 8/255 & 11.7 & 35.5 \\
		
		
		
		\bottomrule
			
	\end{tabular}
    }
 % }
	
	\label{tab:10}
\end{table}

\subsubsection{Time Cost Analysis for Adversarial Attacks}
Furthermore, we present an analysis of the computational cost related to adversary generation for medical image analysis. Specifically, we measure the time cost for generating adversarial examples by various attack methods for diverse medical imaging tasks, as shown in Table \ref{tab:7}. Note that we adopt ResNet-18 for medical classification and U-Net for medical segmentation, respectively. It can be seen that both Auto attack and Square attack require relatively considerable computing resources to produce strong adversarial examples. The primary time cost gap of two PGD-based attack methods for medical segmentation comes from the efficiency of different loss calculations.

\vspace{-3mm}
\subsection{Adversarial Training for Defense}
% 
In this section, we construct a unified benchmark with the most effective defense method, adversarial training to establish robustness for medical diagnosis systems for future research. Specifically, we extend several existing adversarial training methods for the natural imaging domain to medical imaging analysis. We also measure the robustness of adversarially trained diagnosis models under various attack scenarios for systematic evaluation. 

\begin{table}[!t]
	\centering
	\caption{Black-box \textbf{B}inary (\textbf{B}) and \textbf{M}ulti-class (\textbf{M}) accuracy of ResNet-18-based adversarially trained medical classification models in diverse settings.}
        \vspace{-2mm}
	%\small
	\renewcommand{\arraystretch}{0.8}
        % \resizebox{0.75\linewidth}{!}{
    \resizebox{\linewidth}{!}{  
	\begin{tabular}{cccccccccccc}
		\toprule
		\multicolumn{6}{l}{\textbf{Fundoscopy \cite{decenciere2014feedback} Classification:}} \\
		\midrule
		\multirow{2}{*}{Adversarial Type} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{PGD-AT \cite{MadryMSTV18}} & \multicolumn{2}{c}{TRADES \cite{zhang2019theoretically}} & \multicolumn{2}{c}{MART \cite{wang2019improving}} & \multicolumn{2}{c}{HAT \cite{RadeM22}} \\
		\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
		&&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}\\
		\midrule
		None & 0 & 71.3 & 50.0 & 58.8 & 46.7 & 62.1 & 44.6 & 55.0 & 42.5 & 62.5 & 46.7 \\
		\midrule
		\multirow{4}{*}{\makecell{Square Attack \cite{andriushchenko2020square} \\ (Black-box)}} & 1/255 & 13.3 & 9.6 & 57.5 & 45.8 & 58.3 & 42.1 & 54.6 & 42.5 & 56.7 & 42.5 \\
		& 2/255 & 5.8 & 3.8 & 55.8 & 42.9 & 55.8 & 37.9 & 54.6 & 42.5 & 56.7 & 38.3 \\
		& 4/255 & 0.0 & 0.4 & 52.9 & 37.9 & 48.8 & 29.6 & 52.5 & 41.7 & 46.7 & 24.6 \\
		& 8/255 & 0.0 & 0.0 & 43.8 & 29.2 & 28.8 & 16.3 & 50.0 & 40.4 & 21.3 & 6.3 \\
		\midrule
		\multirow{4}{*}{\makecell{PGD \cite{MadryMSTV18} \\ (No-box)}} & 1/255 & 47.5 & 41.3 & 58.8 & 46.7 & 61.3 & 43.8 & 55.0 & 42.5 & 60.0 & 46.7 \\
		& 2/255 & 36.7 & 37.9 & 59.2 & 46.7 & 58.3 & 42.9 & 55.4 & 42.5 & 59.2 & 45.4 \\
		& 4/255 & 27.9 & 27.9 & 56.7 & 46.3 & 56.7 & 40.4 & 55.4 & 42.9 & 57.9 & 45.8 \\
		& 8/255 & 10.4 & 18.7 & 55.8 & 44.6 & 50.0 & 36.3 & 55.8 & 42.5 & 50.0 & 40.8 \\
		\midrule
		\multirow{4}{*}{\makecell{CW \cite{carlini2017towards} \\ (No-box)}} & 1/255 & 42.1 & 37.1 & 58.8 & 46.7 & 61.3 & 43.3 & 55.0 & 42.5 & 60.0 & 46.3 \\
		& 2/255 & 35.4 & 37.1 & 59.2 & 46.7 & 58.3 & 42.5 & 55.4 & 42.5 & 59.2 & 40.8 \\
		& 4/255 & 35.4 & 40.4 & 56.7 & 45.8 & 56.7 & 40.4 & 55.4 & 42.9 & 57.9 & 45.4 \\
		& 8/255 & 19.6 & 12.9 & 56.3 & 44.2 & 50.0 & 36.3 & 55.8 & 42.5 & 52.1 & 43.8 \\
		
		\midrule
		
		\multicolumn{6}{l}{\textbf{Dermoscopy \cite{codella2018skin} Classification:}} \\
		\midrule
		\multirow{2}{*}{Adversarial Type} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{PGD-AT \cite{MadryMSTV18}} & \multicolumn{2}{c}{TRADES \cite{zhang2019theoretically}} & \multicolumn{2}{c}{MART \cite{wang2019improving}} & \multicolumn{2}{c}{HAT \cite{RadeM22}} \\
		\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
		&&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}&\textbf{B}&\textbf{M}\\
		\midrule
		None & 0 & 64.9 & 60.0 & 61.2 & 52.0 & 62.3 & 52.9 & 61.1 & 49.6 & 68.6 & 57.1 \\
		\midrule
		\multirow{4}{*}{\makecell{Square Attack \cite{andriushchenko2020square} \\ (Black-box)}} & 1/255 & 45.1 & 19.3 & 58.4 & 48.0 & 58.8 & 50.3 & 59.1 & 45.9 & 63.9 & 51.6 \\
		& 2/255 & 30.3 & 6.0 & 56.7 & 44.4 & 54.7 & 45.6 & 57.2 & 42.7 & 59.7 & 47.5 \\
		& 4/255 & 15.6 & 1.6 & 50.7 & 37.9 & 45.5 & 36.7 & 52.8 & 34.3 & 46.7 & 36.3 \\
		& 8/255 & 2.9 & 0.5 & 40.0 & 23.5 & 27.6 & 18.8 & 41.9 & 19.7 & 28.9 & 19.2 \\
		\midrule
		\multirow{4}{*}{\makecell{PGD \cite{MadryMSTV18} \\ (No-box)}} & 1/255 & 59.1 & 58.5 & 60.8 & 51.9 & 60.1 & 52.5 & 60.0 & 49.6 & 66.1 & 56.0 \\
		& 2/255 & 49.7 & 55.6 & 60.0 & 51.5 & 59.1 & 52.0 & 59.3 & 48.9 & 64.0 & 55.1 \\
		& 4/255 & 35.3 & 50.4 & 59.1 & 50.5 & 54.0 & 51.3 & 57.3 & 47.6 & 58.5 & 54.0 \\
		& 8/255 & 21.2 & 38.5 & 58.1 & 47.5 & 47.3 & 47.6 & 55.2 & 45.6 & 52.1 & 50.7 \\
		\midrule
		\multirow{4}{*}{\makecell{CW \cite{carlini2017towards} \\ (No-box)}} & 1/255 & 59.1 & 58.0 & 60.8 & 51.9 & 60.1 & 52.5 & 60.0 & 49.6 & 66.1 & 56.1 \\
		& 2/255 & 48.9 & 52.9 & 60.0 & 52.0 & 59.1 & 52.0 & 59.5 & 49.1 & 64.0 & 54.9 \\
		& 4/255 & 34.8 & 41.7 & 59.3 & 50.9 & 54.7 & 51.2 & 57.2 & 48.3 & 58.1 & 53.7 \\
		& 8/255 & 20.5 & 19.3 & 58.5 & 48.0 & 48.3 & 48.5 & 55.6 & 46.7 & 52.8 & 50.7 \\
		
		
		
		
		\bottomrule
			
	\end{tabular}
    }
 % }
	
	\label{tab:11}
\end{table}



\subsubsection{Defense against White-box Attacks}
% 
% 
% 
Cutting-edge adversarial training methods mainly focus on augmenting adversarial images as training data to obtain a robust decision boundary for both clean and adversarial examples. In this paper, we primarily extend the adversarial robustness to the biomedical imaging domain by transferring four widely-used adversarial training methods: PGD-AT \cite{MadryMSTV18}, TRADES \cite{zhang2019theoretically}, MART \cite{wang2019improving}, and HAT \cite{RadeM22}. The adversarial robustness results for both binary and multi-class medical classification are reported in Table \ref{tab:8}. Note that the robustness results for NAtural Training (NAT) are also provided for reference. We can easily observe that adversarially trained models can well preserve the robustness under different attack configurations. In the meantime, the naturally trained medical models are vulnerable to high-strength adversarial attacks, especially for the multi-class classification scenario. Note that both PGD-AT and MART can relatively achieve better robustness than TRADES and HAT for medical classification models. The plausible reason for the robustness gap is that these two types of adversarial training methods focus on different styles of inner adversary generation. Although adversarial training methods can significantly enhance the adversarial robustness, the clean performance still suffers from a slight drop compared with naturally trained models.

Besides establishing adversarial robustness for single-label classification models, we also conduct adversarial training for medical segmentation tasks. We present the performance for adversarially trained medical segmentation models under different attack configurations in Table \ref{tab:9}. Note that we adopt the most widely-used adversarial training method, PGD-AT, to enhance the intrinsic network robustness of medical segmentation models. It can be seen that the adversarially trained segmentation models can achieve better robustness against different degrees of adversarial attacks than naturally trained models. We can also find that attacking the Dice loss can still achieve a better attack success rate than attacking the BCE loss.

Generally, adversarial training can induce a mild performance drop for clean examples. However, we observe an intriguing phenomenon for multi-label classification in the context of medical images, as shown in Table \ref{tab:10}. We report the Area Under the Curve (AUC) of multi-label thorax disease classification models against adversarial examples of different attack strengths. In particular, we can observe an intriguing phenomenon that adversarially trained medical models can even achieve better performance for clean examples. In other words, adversarial training can effectively enhance the robustness of multi-label thorax disease classification with no loss of clean performance. We hypothesize that the improvement in clean performance is attributed to the inter-class structural similarity in chest X-ray images.






\begin{table}[!t]
	\centering
	\caption{No-box robustness results against PGD attack of adversarially trained medical segmentation models using U-Net.}
        \vspace{-2mm}
	%\small
	\renewcommand{\arraystretch}{0.8}
        % \resizebox{0.75\linewidth}{!}{
    \resizebox{\linewidth}{!}{  
	\begin{tabular}{ccccccc}
		\toprule
		\multirow{2}{*}{Task} & \multirow{2}{*}{Adversarial Loss} & \multirow{2}{*}{$\epsilon$} & \multicolumn{2}{c}{NAT} & \multicolumn{2}{c}{ADV} \\
		\cmidrule(lr){4-5} \cmidrule(lr){6-7}
		& & & mIOU & Dice & mIOU & Dice \\
		\midrule
		\multirow{9}{*}{Dermoscopy \cite{codella2018skin}} & None & 0 & 0.818 & 0.883 & 0.803 & 0.867 \\
		\cmidrule(lr){2-7}
		& \multirow{4}{*}{BCE} & 1/255 & 0.801 & 0.863 & 0.800 & 0.865 \\
		& & 2/255 & 0.783 & 0.851 & 0.799 & 0.864 \\
		& & 4/255 & 0.755 & 0.823 & 0.797 & 0.862 \\
		& & 8/255 & 0.698 & 0.780 & 0.794 & 0.860 \\
		\cmidrule(lr){2-7}
		& \multirow{4}{*}{Dice} & 1/255 & 0.807 & 0.875 & 0.801 & 0.865 \\
		& & 2/255 & 0.797 & 0.866 & 0.800 & 0.864 \\
		& & 4/255 & 0.775 & 0.848 & 0.798 & 0.863 \\
		& & 8/255 & 0.722 & 0.804 & 0.796 & 0.861 \\
		
		\midrule
		\multirow{9}{*}{COVID-19 \cite{chowdhury2020can}} & None & 0 & 0.977 & 0.988 & 0.964 & 0.981 \\
		\cmidrule(lr){2-7}
		& \multirow{4}{*}{BCE} & 1/255 & 0.961 & 0.979 & 0.963 & 0.981 \\
		& & 2/255 & 0.933 & 0.963 & 0.963 & 0.981 \\
		& & 4/255 & 0.870 & 0.922 & 0.963 & 0.980 \\
		& & 8/255 & 0.774 & 0.856 & 0.963 & 0.980 \\
		\cmidrule(lr){2-7}
		& \multirow{4}{*}{Dice} & 1/255 & 0.966 & 0.982 & 0.963 & 0.981 \\
		& & 2/255 & 0.954 & 0.976 & 0.963 & 0.981 \\
		& & 4/255 & 0.926 & 0.959 & 0.963 & 0.981 \\
		& & 8/255 & 0.883 & 0.931 & 0.963 & 0.980 \\
		
		\bottomrule
			
	\end{tabular}
    }
 % }
	\vspace{-10pt}
	\label{tab:12}
\end{table}

\subsubsection{Defense against Black-box Attacks}
% 
% 
Besides the evaluation of white-box attacks against adversarially trained diagnosis models, we also emphasize the importance of defense against black-box attacks on account of real-world scenarios. In this section, we conduct both black-box and no-box adversarial attacks for evaluation to simulate medical defense in clinical practice. We first report the robustness results of adversarially trained medical models in both binary and multi-class classification settings, as shown in Table \ref{tab:11}. The adversarially trained diagnosis models can effectively remain robust against adversarial examples of different attack strengths. Furthermore, we can also observe a similar trend of black-box accuracy to white-box one as the attack strength increases. Hence, adversarial training can still remain in effect to establish intrinsic robustness for white-box and black-box attacks under different settings in the context of biomedical image analysis.

Likewise, we measure the performance of adversarially trained medical segmentation models against no-box adversarial attacks for comprehensive evaluation. The no-box performance of different settings is reported in Table \ref{tab:12}. Adversarial training achieve outstanding performance in enhancing robustness in the context of X-ray images for lung segmentation. We also observe that using the BCE loss as the adversarial loss in the no-box scenario can achieve a better attack success rate than adopting the Dice loss.

\begin{table}[!t]
	\centering
	\footnotesize
	\caption{Time cost comparison of adversarial training methods for diverse medical imaging tasks. We report the average training time per epoch.}
	\vspace{-2mm}
	%\small
	\renewcommand{\arraystretch}{0.8}
        % \resizebox{0.75\linewidth}{!}{
%    \resizebox{\linewidth}{!}{  
	\begin{tabular}{ccc}
		\toprule
		\multicolumn{3}{l}{\textbf{Medical Classification:}} \\
		\midrule
		Method & Fundoscopy \cite{decenciere2014feedback} & Dermoscopy \cite{codella2018skin}\\
		
		\midrule
		NAT & 16.2 s & 17.3 s\\
		PGD-AT \cite{MadryMSTV18} & 53.5 s & 65.1 s\\
		TRADES \cite{zhang2019theoretically} & 56.0 s & 67.7 s\\
		MART \cite{wang2019improving} & 55.0 s & 65.9 s\\
		HAT \cite{RadeM22} & 62.0 s & 74.9 s\\
		\midrule
		\multicolumn{3}{l}{\textbf{Medical Segmentation:}} \\
		\midrule
		Adversarial Type & Dermoscopy \cite{codella2018skin} & COVID-19 \cite{chowdhury2020can} \\
		\midrule
		NAT & 13.8 s & 14.4 s\\
		PGD-AT \cite{MadryMSTV18} & 148.5 s & 155.0 s\\
		
		\bottomrule
			
	\end{tabular}
%    }
 % }
	\vspace{-5pt}
	\label{tab:13}
\end{table}


\subsubsection{Time Cost Analysis for Adversarial Training}
For systematic evaluation, we also provide the time cost of several adversarial training methods for medical imaging tasks in Table \ref{tab:13}. For a fair comparison, we conduct all the training experiments on a single NVIDIA Tesla A100 GPU with the same batch size. We report the average time cost over 10 runs of several adversarial training approaches. We can easily observe that adversarial training generally takes several times as long as natural training. The time cost gap mainly comes from the iterative gradient computation during the inner adversary generation, which is further enlarged for medical segmentation.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{imgs/Adv_visualization.pdf}
	\vspace{-20pt}
	\caption{Heat-map visualization of medical adversarial examples under diverse attack strengths $\epsilon$ corresponding to NATurally (\textbf{NAT}) and ADVersarially (\textbf{ADV}) trained classification models.}
	\vspace{-10pt}
	\label{fig:4}
	
\end{figure}

\subsubsection{Visualization}
% 
In general, adversarial examples can lead to severe mistakes during the inference stage of target medical imaging models. To better represent the negative impact of adversarial examples, we utilize Gradient-weighted Class Activation Mapping (Grad-CAM) \cite{selvaraju2017grad} to create heat maps that represent the discriminative region for medical classification, as shown in Figure \ref{fig:4}. It can be seen that naturally trained classification models are extremely susceptible to adversaries, especially for high-strength adversarial attacks. The class-discriminative region for the naturally trained model varied greatly with the increase of attack strength ($\epsilon$). In comparison, the adversarially trained model can achieve robust prediction results and also keep the class-discriminative region the same as the attack intensity increases.

\vspace{-3mm}
\section{Challenges and Future Directions}
\label{sec:6}
% 
% 
% 
Despite the exceptional success of adversarial machine learning for medical image analysis, there still remain several challenges that are worthy of exploration. We also summarize some ongoing or future research directions below:

\noindent
\textbf{Evaluation benchmarks.}
Existing medical adversarial attack and defense methods are primarily based on customized evaluation metrics and settings, which might lead to difficulties in efficacy and efficiency assessments. Furthermore, both adversarial attack and defense are required to be realistic for clinical practice. Hence, for the sake of fair comparison and practical application, the community has to acknowledge a unified and systematical evaluation benchmark. A feasible solution is to establish an open-ended standard benchmark for robustness evaluation in the context of biomedical image analysis. Accordingly, the adversarial robustness benchmark in the medical field can follow the classical robustness benchmark for natural imaging, \textit{i.e.}, RobustBench \cite{CroceASDFCM021}. In the meantime, adversarial attacks need to be adaptive for standardized evaluation, which means that attacks are specifically designed for a given defense method \cite{tramer2020adaptive}. In this paper, we also establish a unified benchmark for adversarial training in the context of biomedical image analysis to facilitate future research.

\noindent
\textbf{Trade-off.}
The trade-off between performance on legitimate and adversarial examples has been widely explored in the natural imaging field \cite{zhang2019theoretically, RadeM22}. The enhancement of robust accuracy inevitably induces a decrease in performance on clean examples. We also discovered such a phenomenon for adversarially trained medical diagnosis models in this survey. However, biomedical applications are usually required to be sensitive to the precision of diagnosis, especially in clinical practice. The ideal medical models should achieve excellent performance on clean examples and also remain robust against potential attacks. Therefore, finding a balance between clean and robust performance for medical diagnosis models can be an important research topic in the future. Moreover, we also observe that adversarially trained medical models can sometimes obtain the same or better performance on legitimate medical examples than regularly trained models. We believe it will be possible to improve the adversarial robustness of medical models without compromising the clean accuracy.


\noindent
\textbf{Computational efficiency.}
Current medical adversarial defense methods mostly require an expensive computational cost, particularly for adversarial training. In general, the time cost of adversarial training can be dozens of times that of natural training due to the iterative adversary generation, resulting in impediments to practical medical applications. Several fast adversarial training variants spare no effort to efficiently construct robustness in the natural imaging domain \cite{shafahi2019adversarial, andriushchenko2020understanding}, whereas there is rare research focusing on medical imaging tasks. Hence, it is quite necessary to put effort into efficient adversarial defense. In addition, reducing computational costs can further facilitate adversarial defense within reach of organizations with modest computing resources to serve the community better.

\noindent
\textbf{Defense tailored for medical image analysis.}
Existing methods primarily transfer adversarial defense approaches for natural images directly to the medical imaging domain. Nevertheless, there remain very few defense methods tailored for medical images to enhance the adversarial robustness of computer-aided diagnosis systems. Unlike natural examples with large distribution variances, biomedical examples usually share a specific biological structure and a unified distribution, which can be further utilized as priors to construct robustness for medical image analysis systems. In the meantime, we also discover that the inner adversary generation strategy during adversarial training has a more significant impact on the robustness establishment for computer-aided diagnosis models than for natural models. Therefore, we argue that adversarial defense for the medical imaging domain has the potential to be further improved by incorporating prior knowledge in biomedical images. 

\noindent
\textbf{Potential ethical issues.}
% 
As we mentioned earlier, the performance of adversarially trained models on clean examples slightly decreases with the drastic enhancement of adversarial robustness. In other words, establishing adversarial robustness will inevitably induce a performance drop on legitimate examples. This is common in the field of natural images but raises a clinical ethical dilemma for medical imaging analysis: is a defense against potential adversarial threats more important than an accurate diagnosis on clean medical examples? In other words, how do we weigh the clean and robust accuracy of medical image analysis systems? The trade-off phenomenon and also inevitable clean accuracy drop are still unresolved issues for defense on clean images. Fortunately, we discover that adversarial training can enhance the robustness without losing clean performance for some medical imaging tasks and modalities. This also shows that it is possible to simultaneously enhance adversarial robustness and also the clean performance of medical image analysis systems in the near future. Furthermore, threats of adversarial examples might also pose a crisis of confidence related to computer-aided diagnosis systems. Hence, it is essential to establish reliable and interpretable computer-aided diagnosis systems to build trust with potential users.

\vspace{-3mm}
\section{Conclusion}
In this work, we present a detailed survey of adversarial attack and defense methods for medical image analysis, which is driven by a novel taxonomy in terms of the application scenario. This survey also incorporates a unified theoretical framework with a comprehensive analysis of different types of attack and defense methods in the context of medical images. In addition, we establish a benchmark for adversarially trained diagnosis models under various scenarios to facilitate future research. Finally, we point out representative challenges and promising future research directions in this domain. We hope this survey can further attract new efforts towards better interpretability and application of adversarial machine learning in the medical field.

{\normalsize
\bibliographystyle{IEEEtran}
\bibliography{egbib}}




\end{document}


