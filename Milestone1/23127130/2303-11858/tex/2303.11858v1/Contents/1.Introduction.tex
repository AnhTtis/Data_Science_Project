% Introduction to KG and link prediction task
% KGs (KGs) such as Wikidata \citep{Wikidata} and Freebase \citep{Freebase}
% %, and YAGO \citep{YAGO} 
% represent real-world facts as a set of triplets of the form (head entity, relation, tail entity). 
%Representing KGs in a low-dimensional space has been a fundamental task. Many popular embedding methods \citep{TransE, RotatE, TransH, ComplEx, DistMult} have been introduced to properly preserve the semantic and structural information in KGs. All of these embedding methods are trained with respect to the link prediction task, i.e. prediction of missing links between KG entities.
% Introduction to logical query answering task, what it is (example) and why it is important
%Beyond the link prediction task over incomplete KGs (KGs), 
Answering first-order logical (FOL) queries over knowledge graphs (KGs) has been an important and challenging problem \cite{BetaE}. 
% The problem itself is challenging because KGs are incomplete. 
%The methods which are based on traversing the large-scale KGs  exponential computational cost of traversing the large-scale KGs for reasoning.
Among various approaches,
logical query embedding \citep{GQE,Query2box,ConE,BetaE} has received huge attention due to its great efficiency and effectiveness. 
Logical query embeddings take as input the KGs and a set of first-order logical queries that include existential quantification ($\exists$), conjunction ($\land$), disjunction ($\lor$), and negation ($\neg$). 
%They then compute the vector representations of KGs and queries to measure if a candidate answer is plausible to be the answer to a query. 
%Such queries are usually named first-order logical (FOL) queries or complex logical queries. 
Figure \ref{fig:FOL query example} shows a concrete FOL query example that corresponds to the natural language query “List the capitals of non-European countries that have held either World Cup or Olympics”. 
%% Temporary example
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Pictures/FOL-query-example.pdf}
    \caption{An example of FOL query corresponds to "List the capitals of non-European countries that have held either World Cup or Olympics".}
    \label{fig:FOL query example}
\end{figure}
% Summarize existing methods: traditional methods and drawbacks (KG incompleteness)
% Summarize existing methods: what are query embedding methods? why do they work in the incomplete graphs? what is the research gap?
%Answering first-order logical queries on KGs is an important yet challenging task mainly due to the incompleteness of KGs. Even for the most large-scale KGs, it is infeasible to include every real-world fact in them. Thus, reasoning in the presence of missing information is non-trivial for the existing query-answering models. 
%To tackle complex queries on incomplete KGs, 
The methods model logic operations by neural operators that act in the vector space. In particular, they represent a set of entities as geometric shapes and design neural-network-based logical operators to compute the embedding of the logical queries. 
The similarity between the embedded logical query and a candidate answer is calculated to measure plausibility.
%The geometric query embedding methods achieved great success in inventing predictions given the incompleteness of KGs and endowing logical operations with geometric explanations.
%\Mojtaba{I checked until here ...}

%KG relation patterns are also known as KG connectivity patterns which are modeled by existing embedding methods. 
Relations in KGs may form particular patterns, e.g.,
some relations are symmetric (e.g., spouse) while others are anti-symmetric (e.g., parent$\_$of); some relations are the inverse of other relations (e.g., son$\_$of and father$\_$of). %and some relations may be composed by others (e.g., my mother’s husband is my father). 
Modeling relational patterns can potentially improve the generalization capability and has been extensively studied in link prediction tasks \cite{RotatE,nayyeri2021losspattern}. 
This has been shown particularly in \citet{RotatE} where the RotatE model utilizes the rotation operation in Complex space to model relational patterns and enhance link prediction task. 
However, current logical query embedding models adopt deep neural logical operators and are not able to model relational patterns, which do matter for logical query answering. Figure \ref{relation pattern} shows concrete examples of how relation patterns can impact complex query reasoning in KGs. 
% The challenge of the current logical query embedding model is that they use deep neural logical operators, and deep neural networks have been already shown to have less pattern inference power than a few shallow algebraic operators such as rotation \cite{RotatE} or translation \cite{TransE}.


% Introduction to our method: what are relational patterns? (example) why do they influence the query reasoning performance in KGs? Why do the existing methods fail to capture these patterns? What is our method? How does it capture the relational patterns?

% In spite of the importance of relation patterns for query reasoning, none of the existing query embedding methods has shown the explicit ability to capture these patterns through fully neural logical operators and predictor. 
% Even if these neural predictors have shown good performance on complex query reasoning, it still remains a black-box question that if they can learn these relation patterns implicitly and if these relation patterns indeed help with the reasoning process.

%\begin{figure*}[t!]
%    \centering 
%    \subfloat[\centering]{{\includegraphics[width=0.2\textwidth]{Pictures/left.pdf}}}
    % \qquad
%    \subfloat[\centering]{{\includegraphics[width=0.58\textwidth]{Pictures/center.pdf}}}
%    \subfloat[\centering]
%    {{\includegraphics[width=0.2\textwidth]{Pictures/right.pdf}}}
%    \caption{Examples on how relation patterns influence query answering over incomplete KGs. The left panel illustrates the rotations corresponding to different relation patterns on 2D complex plane. The right panel displays how the captured relation patterns help with reasoning a two-hop query given an incomplete KG: the first step is to find out who is Judy's father-in-law, which cannot be directly extracted from the given facts. However, based on the learned potential relation patterns from other parts of the graph and the existing relations between Judy, Justin, and Ryan, the model is able to derive that Ryan is Judy's father-in-law through relational rotation in complex space. The second step is simply to derive where Ryan graduated from by another relational rotation.}
%    \label{relation pattern}
%\end{figure*}
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.4]{Pictures/center.pdf}
    \caption{(\emph{top}) An example showing how relation patterns influence query answering over incomplete KGs: 
    the intermediate variable Judy's father-in-law in the query cannot be directly extracted from the given facts;
    (\emph{bottom}) An illustration of cone rotation. Based on the existing relations between Judy, Justin, and Ryan, and the learned potential relation patterns from other parts of the graph, the model is able to derive the following information by relational rotation: (i) Justin is Judy's spouse (symmetric rotation) (ii) Justin is the child of Ryan (inversion rotation) (iii) Ryan is Judy's father in law (compositional rotation). With the predicted query embedding on $V$, the model is able to derive where $V$ graduated from by another relational rotation.}
    \label{relation pattern}
\end{figure}


% \begin{figure}
% \begin{subfigure}
% \centering
% \includegraphics[width=0.2\textwidth]{Pictures/left.pdf}
% \caption{$y=x$}
% \label{fig:y equals x}
% \end{subfigure}
% \begin{subfigure}
% \centering
% \includegraphics[width=0.2\textwidth]{Pictures/center.pdf}
% \caption{$y=3\sin x$}
% \label{fig:three sin x}
% \end{subfigure}
% \hfill
% \begin{subfigure}
% \centering
% \includegraphics[width=0.2\textwidth]{Pictures/right.pdf}
% \caption{$y=5/x$}
% \label{fig:five over x}
% \end{subfigure}
% %\includegraphics[scale=0.4]{Pictures/relation patterns.pdf}
% \caption{Examples on how relation patterns influence query answering over incomplete KGs. The left panel illustrates the rotations corresponding to different relation patterns on 2D complex plane. The right panel displays how the captured relation patterns help with reasoning a two-hop query given an incomplete KG: the first step is to find out who is Judy's father-in-law, which cannot be directly extracted from the given facts. However, based on the learned potential relation patterns from other parts of the graph and the existing relations between Judy, Justin, and Ryan, the model is able to derive that Ryan is Judy's father-in-law through relational rotation in complex space. The second step is simply to derive where Ryan graduated from by another relational rotation. }
% \label{relation pattern}
% \end{figure}

To model and infer various KG relational patterns in logical queries answering process, we propose a novel method called RoConE  that combines the advantages
of Cone as a well-specified geometric representation for query embedding \cite{ConE}, and also the
rotation operator \cite{RotatE} as a powerful algebraic operation for pattern inference.
We define each relation as a rotation from the source entity set to the answer/intermediate entity set and perform neural logical operators upon the selected entity sets in the complex vector space. We provide theoretical proof of its ability to model relational patterns, as well as experimental results on how the relational patterns influence logical queries answering over three established benchmark datasets.
% Introduction to our contributions: 

%In this paper, we primarily make the following contribution: