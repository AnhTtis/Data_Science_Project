To accommodate the learning of relation patterns in query answering over KGs, we propose a new model RoConE, which models entity sets as cones and relations as anti-clockwise angular rotations on cones in the complex plane. Each cone $\mathbf{q}$ is parameterized by $\mathbf{q} =(\mathbf{h}_{U}, \mathbf{h}_{L})$, where $|\mathbf{h}_{\{U,L\}}|=\textbf{1}$, and
$\mathbf{h}_{U}$, $\mathbf{h}_{L} \in \mathbb{C}^d$ represent the counter-clockwise upper and lower boundaries of a cone, such that 
\begin{equation}
\begin{split}
    \mathbf{h}_{U} \equiv e^{i\pmb{\theta}_{U}} \equiv e^{i(\pmb{\theta}_{ax} + \pmb{\theta}_{ap}/2)}, \\
    \mathbf{h}_{L} \equiv e^{i\pmb{\theta}_{L}} \equiv e^{i(\pmb{\theta}_{ax} - \pmb{\theta}_{ap}/2)},
\end{split}
\end{equation}
where $\pmb{\theta}_{ax} \in [-\pi,\pi)^d$ represents the angle between the symmetry axis of the cone and $\pmb{\theta}_{ap} \in [0,2\pi]^d$ represents the cone aperture, and $d$ is the embedding dimension.
The query and the set of entities are modeled as cones and each entity instance is modeled as a vector such that $\mathbf{h}_{U} = \mathbf{h}_{L}$.

\subsection{Logical Operators}
As illustrated in Figure \ref{fig:FOL query example}, each logical query can be represented as a directed acyclic graph (DAG) tree, where the tree nodes correspond to constants/anchor node entities or variables, and the edges correspond to atom relations or logical operations in a query. Logical operations are performed along the DAG tree from constants to the target answer variable. Figure \ref{loigcal operators} visualizes these operations on 2D complex space.
The logical operators can be defined as follows

\paragraph{Relational rotating projection}
Given a set of entities $\mathcal{S} \subset \mathcal{E}$ and a relation $r \in \mathcal{R}$, the projection operator selects the neighbouring entities $\mathcal{S}^{'}\subset\mathcal{E}$ by relation such that $\mathcal{S}^{'} = \{e \in \mathcal{S}, e^{'} \in \mathcal{S}^{'}: r(e, e^{'}) = True\}$. 
Existing query embedding methods \citep{ConE,Query2box,GQE,BetaE} apply multi-layer perceptron networks to accomplish this task. They do not accommodate the learning of potential KG relational patterns which might help in reasoning logical queries. Motivated by RotatE \citep{RotatE}, we represent each relation $\mathbf{r}$ as a counterclockwise relational rotation on query embeddings about the origin of the complex plane such that $\mathbf{r} = (\mathbf{r}_{U}, \mathbf{r}_{L})$, where $|\mathbf{r}_{\{U,L\}}|=\textbf{1}$, and $\mathbf{r}_{U}, \mathbf{r}_{L} \in \mathbb{C}^{d}$. Given the query embedding $\mathbf{q} = (\mathbf{h}_{U}, \mathbf{h}_{L})$ and a relation $\mathbf{r}$, the selected query embedding $\mathbf{q}^{'} = (\mathbf{h}_{U}^{'}, \mathbf{h}_{L}^{'})$ is
\begin{equation}
    \begin{split}
    \mathbf{h}_{U}^{'} &= \mathbf{h}_{U} \circ \mathbf{r}_{U} \equiv e^{i(\pmb{\theta}_{ax} + \pmb{\theta}_{ax,r} + (\pmb{\theta}_{ap}+\pmb{\theta}_{ap,r})/2)}, \\
    \mathbf{h}_{L}^{'} &= \mathbf{h}_{L} \circ \mathbf{r}_{L} \equiv e^{i(\pmb{\theta}_{ax} + \pmb{\theta}_{ax,r} - (\pmb{\theta}_{ap}+\pmb{\theta}_{ap,r})/2)},  
    \end{split}
\end{equation}
where $\circ$ is the Hadmard (element-wise) product, and $\pmb{\theta}_{ax,r}$, $\pmb{\theta}_{ap,r}$ correspond to the equivalent relational rotation on $\pmb{\theta}_{ax}$, $\pmb{\theta}_{ap}$. Specifically, for each element of the cone embeddings, we have $h_{U,i}^{'} = h_{U,i}r_{U,i}$ and $h_{L,i}^{'} = h_{L,i}r_{L,i}$.
Each element $r_i$ of the relational rotation $\mathbf{r}_{\{U,L\}}$ corresponds to a counterclockwise rotation on the matching element of upper or lower boundaries by $\theta_{r,i}$ radians about the origin of the complex plane.
By modeling the projection as relational rotation on a cone in the complex space, RoConE is shown to model and infer all three types of relation patterns introduced in Section 3. 
The lemmas and their proofs are in Appendix and the rotations corresponding to different relation patterns are visualized in Figure \ref{relation pattern rotations}.

\paragraph{Intersection}
For the input cone embeddings of entity sets \(\{ \mathbf{q}_{1},...,\mathbf{q}_{n}\}\), the intersection operator selects the intersection $\mathbf{q}^{'} = \cap_{j=1}^{n} \mathbf{q}_{j}$ with the \textbf{SemanticAverage}\footnote{ \textbf{SemanticAverage} and \textbf{CardMin} are explained in Appendix \ref{Intersection Operator}}($\cdot$) and \textbf{CardMin}($\cdot$) \citep{ConE}, which calculate the semantic centers and apertures of cones respectively. Since we have each cone \(\mathbf{q}_{j} = (\mathbf{h}_{j,U}, \mathbf{h}_{j,L}) \equiv (e^{i(\pmb{\theta}_{j,ax}+\pmb{\theta}_{j,ap}/2)}, e^{i(\pmb{\theta}_{j,ax}-\pmb{\theta}_{j,ap}/2)})\), the intersection $\mathbf{q}^{'} = (\mathbf{h}_{U}^{'}, \mathbf{h}_{L}^{'})$ can be defined as follows
\begin{equation}
\begin{split}
     &\mathbf{h}_{U}^{'} = e^{i(\pmb{\theta}_{ax}^{'} +\pmb{\theta}_{ap}^{'}/2)},\\
     &\mathbf{h}_{L}^{'} = e^{i(\pmb{\theta}_{ax}^{'}
    -\pmb{\theta}_{ap}^{'}/2)},
 \end{split}
\end{equation}
where 

\begin{equation}
\begin{split}
    \pmb{\theta}_{ax}^{'} &= \textbf{SemanticAverage}(\{(\pmb{\theta}_{j,ax}, \pmb{\theta}_{j,ap})\}_{j=1}^{n}),\\
    \pmb{\theta}_{ap}^{'} &= \textbf{CardMin}(\{(\pmb{\theta}_{j,ax}, \pmb{\theta}_{j,ap})\}_{j=1}^{n}).
\end{split}
\end{equation}

\paragraph{Disjunction}
Given the input cone embeddings of entity sets \(\{ \mathbf{q}_{1},...,\mathbf{q}_{n}\}\) where $\mathbf{q}_{j} = (\mathbf{h}_{j,U}, \mathbf{h}_{j,L})$, the disjunction operator finds the union set $\mathbf{q}^{'} = \cup_{j=1}^{n} \mathbf{q}_{j} = \{\mathbf{q}_{1}, \dots, \mathbf{q}_{n}\}$, which is equivalent to
\begin{equation*}
    \resizebox{\hsize}{!}{$(\{(\text{h}_{1,U}^{1}, \text{h}_{1,L}^{1}),\dots,(\text{h}_{n,U}^{1}, \text{h}_{n,L}^{1})\},\dots,\{(\text{h}_{1,U}^{d}, \text{h}_{1,L}^{d}),\dots,(\text{h}_{n,U}^{d}, \text{h}_{n,L}^{d})\})$}
\end{equation*}
Following \citet{Query2box}, we also adopt DNF technique to translate FOL queries into the disjunction of conjunctive queries and only perform the disjunction operator in the last step in the computation graph.
\paragraph{Negation}
Given a set of entities $\mathcal{S} \subset \mathcal{E}$, the negation operator finds its complementary negation $\bar{S} = \mathcal{E}\setminus\mathcal{S}$. Given the cone embedding of entity set $\mathcal{S}$, $\mathbf{q}^{\mathcal{S}} = (\mathbf{h}_{U}^{\mathcal{S}}, \mathbf{h}_{L}^{\mathcal{S}})$, its corresponding complementary negation \(\bar{\mathbf{q}} = (\mathbf{h}_{L}^{\mathcal{S}}, \mathbf{h}_{U}^{\mathcal{S}})\).

\subsection{Optimization}
Given a set of training samples, our goal is to minimize the distance between the query cone embedding $\mathbf{q} = (\mathbf{h}_{U}^{q}, \mathbf{h}_{L}^{q})$ and the answer entity vector $\mathbf{h}^{*}$ while maximizing the distance between this query and negative samples. Thus, we define our training objective, the negative sample loss as
\begin{equation}
    \resizebox{\hsize}{!}{
    $L = -log \sigma (\gamma - d(\mathbf{h}^{*},\mathbf{q})) - \frac{1}{k}\sum_{i=1}^{k}log\sigma(d(\mathbf{h}^{*}_{i},\mathbf{q})-\gamma)
    $}
\end{equation}
where $d(\cdot)$ is the combined distance specifically defined in Appendix \ref{Distance function}, $\gamma$ is a margin, $\mathbf{h}^{*}$ is a positive entity and $\mathbf{h}^{*}_{i}$ is the i-th negative entity, k is the number of negative samples, and $\sigma(\cdot)$ represents the sigmoid function.

 