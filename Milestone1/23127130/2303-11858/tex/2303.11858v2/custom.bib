@inproceedings{nayyeri2021losspattern,
  title={Loss-aware pattern inference: A correction on the wrongly claimed limitations of embedding models},
  author={Nayyeri, Mojtaba and Xu, Chengjin and Yaghoobzadeh, Yadollah and Vahdati, Sahar and Alam, Mirza Mohtashim and Yazdi, Hamed Shariat and Lehmann, Jens},
  booktitle={Pacific-Asia Conference on Knowledge Discovery and Data Mining},
  pages={77--89},
  year={2021},
  organization={Springer}
}

@article{RotatE,
  author    = {Zhiqing Sun and
               Zhi{-}Hong Deng and
               Jian{-}Yun Nie and
               Jian Tang},
  title     = {RotatE: Knowledge Graph Embedding by Relational Rotation in Complex
               Space},
  journal   = {CoRR},
  volume    = {abs/1902.10197},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.10197},
  eprinttype = {arXiv},
  eprint    = {1902.10197},
  timestamp = {Fri, 26 Jul 2019 07:57:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-10197.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Query2Box,
  author    = {Hongyu Ren and
               Weihua Hu and
               Jure Leskovec},
  title     = {Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box
               Embeddings},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=BJgr4kSFDS},
  timestamp = {Thu, 07 May 2020 17:11:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/RenHL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{GQE,
  author    = {William L. Hamilton and
               Payal Bajaj and
               Marinka Zitnik and
               Dan Jurafsky and
               Jure Leskovec},
  editor    = {Samy Bengio and
               Hanna M. Wallach and
               Hugo Larochelle and
               Kristen Grauman and
               Nicol{\`{o}} Cesa{-}Bianchi and
               Roman Garnett},
  title     = {Embedding Logical Queries on Knowledge Graphs},
  booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference
               on Neural Information Processing Systems 2018, NeurIPS 2018, December
               3-8, 2018, Montr{\'{e}}al, Canada},
  pages     = {2030--2041},
  year      = {2018},
  url       = {https://proceedings.neurips.cc/paper/2018/hash/ef50c335cca9f340bde656363ebd02fd-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/HamiltonBZJL18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{FuzzQE,
  doi = {10.48550/ARXIV.2205.10128},
  
  url = {https://arxiv.org/abs/2205.10128},
  
  author = {Zhu, Zhaocheng and Galkin, Mikhail and Zhang, Zuobai and Tang, Jian},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural-Symbolic Models for Logical Queries on Knowledge Graphs},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{CQD,
  author    = {Erik Arakelyan and
               Daniel Daza and
               Pasquale Minervini and
               Michael Cochez},
  title     = {Complex Query Answering with Neural Link Predictors},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=Mos9F9kDwkz},
  timestamp = {Thu, 14 Oct 2021 10:00:37 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ArakelyanDMC21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Freebase,
author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
title = {Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376746},
doi = {10.1145/1376616.1376746},
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1247–1250},
numpages = {4},
keywords = {tuple store, semantic network, collaborative systems},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{Yago,
author = {Suchanek, Fabian M. and Kasneci, Gjergji and Weikum, Gerhard},
title = {Yago: A Core of Semantic Knowledge},
year = {2007},
isbn = {9781595936547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1242572.1242667},
doi = {10.1145/1242572.1242667},
abstract = {We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.},
booktitle = {Proceedings of the 16th International Conference on World Wide Web},
pages = {697–706},
numpages = {10},
keywords = {WordNet, wikipedia},
location = {Banff, Alberta, Canada},
series = {WWW '07}
}


@inproceedings{NELL,
author = {Carlson, Andrew and Betteridge, Justin and Kisiel, Bryan and Settles, Burr and Hruschka, Estevam R. and Mitchell, Tom M.},
title = {Toward an Architecture for Never-Ending Language Learning},
year = {2010},
publisher = {AAAI Press},
abstract = {We consider here the problem of building a never-ending language learner; that is, an intelligent computer agent that runs forever and that each day must (1) extract, or read, information from the web to populate a growing structured knowledge base, and (2) learn to perform this task better than on the previous day. In particular, we propose an approach and a set of design principles for such an agent, describe a partial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs with an estimated precision of 74% after running for 67 days, and discuss lessons learned from this preliminary attempt to build a never-ending learning agent.},
booktitle = {Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence},
pages = {1306–1313},
numpages = {8},
location = {Atlanta, Georgia},
series = {AAAI'10}
}

@inproceedings{BetaE,
  author    = {Hongyu Ren and
               Jure Leskovec},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/e43739bba7cdb577e9e3e4e42447f5a5-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:57:17 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/RenL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{TransE/FB15K,
author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Dur\'{a}n, Alberto and Weston, Jason and Yakhnenko, Oksana},
title = {Translating Embeddings for Modeling Multi-Relational Data},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2787–2795},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@inproceedings{FB15k-237,
    title = "Observed versus latent features for knowledge base and text inference",
    author = "Toutanova, Kristina  and
      Chen, Danqi",
    booktitle = "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-4007",
    doi = "10.18653/v1/W15-4007",
    pages = "57--66",
}

@inproceedings{NELL995,
    title = "{D}eep{P}ath: A Reinforcement Learning Method for Knowledge Graph Reasoning",
    author = "Xiong, Wenhan  and
      Hoang, Thien  and
      Wang, William Yang",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1060",
    doi = "10.18653/v1/D17-1060",
    pages = "564--573",
    abstract = "We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector-space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.",
}

@book{davey_priestley_2002, place={Cambridge}, edition={2}, title={Introduction to Lattices and Order}, DOI={10.1017/CBO9780511809088}, publisher={Cambridge University Press}, author={Davey, B. A. and Priestley, H. A.}, year={2002}}


@inproceedings{ConE,
  author    = {Zhanqiu Zhang and
               Jie Wang and
               Jiajun Chen and
               Shuiwang Ji and
               Feng Wu},
  editor    = {Marc'Aurelio Ranzato and
               Alina Beygelzimer and
               Yann N. Dauphin and
               Percy Liang and
               Jennifer Wortman Vaughan},
  title     = {ConE: Cone Embeddings for Multi-Hop Reasoning over Knowledge Graphs},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {19172--19183},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/a0160709701140704575d499c997b6ca-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:48 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/ZhangWCJW21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{TransE,
author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Dur\'{a}n, Alberto and Weston, Jason and Yakhnenko, Oksana},
title = {Translating Embeddings for Modeling Multi-Relational Data},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2787–2795},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@inproceedings{TransH,
author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
title = {Knowledge Graph Embedding by Translating on Hyperplanes},
year = {2014},
publisher = {AAAI Press},
abstract = {We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {1112–1119},
numpages = {8},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}



@inproceedings{ComplEx,
author = {Trouillon, Th\'{e}o and Welbl, Johannes and Riedel, Sebastian and Gaussier, \'{E}ric and Bouchard, Guillaume},
title = {Complex Embeddings for Simple Link Prediction},
year = {2016},
publisher = {JMLR.org},
abstract = {In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {2071–2080},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}


@misc{DistMult,
  doi = {10.48550/ARXIV.1412.6575},
  
  url = {https://arxiv.org/abs/1412.6575},
  
  author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Embedding Entities and Relations for Learning and Inference in Knowledge Bases},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{EmQL,
  author    = {Haitian Sun and
               Andrew O. Arnold and
               Tania Bedrax{-}Weiss and
               Fernando Pereira and
               William W. Cohen},
  title     = {Guessing What's Plausible But Remembering What's True: Accurate Neural
               Reasoning for Question-Answering},
  journal   = {CoRR},
  volume    = {abs/2004.03658},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.03658},
  eprinttype = {arXiv},
  eprint    = {2004.03658},
  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-03658.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Wikidata,
author = {Vrande\v{c}i\'{c}, Denny and Kr\"{o}tzsch, Markus},
title = {Wikidata: A Free Collaborative Knowledgebase},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/2629489},
doi = {10.1145/2629489},
abstract = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.},
journal = {Commun. ACM},
month = {sep},
pages = {78–85},
numpages = {8}
}

@inproceedings{YAGO,
author = {Suchanek, Fabian M. and Kasneci, Gjergji and Weikum, Gerhard},
title = {Yago: A Core of Semantic Knowledge},
year = {2007},
isbn = {9781595936547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1242572.1242667},
doi = {10.1145/1242572.1242667},
abstract = {We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95\%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.},
booktitle = {Proceedings of the 16th International Conference on World Wide Web},
pages = {697–706},
numpages = {10},
keywords = {WordNet, wikipedia},
location = {Banff, Alberta, Canada},
series = {WWW '07}
}

@inproceedings{Freebase,
author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
title = {Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376746},
doi = {10.1145/1376616.1376746},
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1247–1250},
numpages = {4},
keywords = {tuple store, semantic network, collaborative systems},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{deeppath,
    title = "{D}eep{P}ath: A Reinforcement Learning Method for Knowledge Graph Reasoning",
    author = "Xiong, Wenhan  and
      Hoang, Thien  and
      Wang, William Yang",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1060",
    doi = "10.18653/v1/D17-1060",
    pages = "564--573",
    abstract = "We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector-space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.",
}


@InProceedings{GNNQ,
author="Pflueger, Maximilian
and Tena Cucala, David J.
and Kostylev, Egor V.",
editor="Sattler, Ulrike
and Hogan, Aidan
and Keet, Maria
and Presutti, Valentina
and Almeida, Jo{\~a}o Paulo A.
and Takeda, Hideaki
and Monnin, Pierre
and Pirr{\`o}, Giuseppe
and d'Amato, Claudia",
title="GNNQ: A Neuro-Symbolic Approach to Query Answering over Incomplete Knowledge Graphs",
booktitle="The Semantic Web -- ISWC 2022",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="481--497",
abstract="Real-world knowledge graphs (KGs) are usually incomplete---that is, miss some facts representing valid information. So, when applied to such KGs, standard symbolic query engines fail to produce answers that are expected but not logically entailed by the KGs. To overcome this issue, state-of-the-art ML-based approaches first embed KGs and queries into a low-dimensional vector space, and then produce query answers based on the proximity of the candidate entity and the query embeddings in the embedding space. This allows embedding-based approaches to obtain expected answers that are not logically entailed. However, embedding-based approaches are not applicable in the inductive setting, where KG entities (i.e., constants) seen at runtime may differ from those seen during training. In this paper, we propose a novel neuro-symbolic approach to query answering over incomplete KGs applicable in the inductive setting. Our approach first symbolically augments the input KG with facts representing parts of the KG that match query fragments, and then applies a generalisation of the Relational Graph Convolutional Networks (RGCNs) to the augmented KG to produce the predicted query answers. We formally prove that, under reasonable assumptions, our approach can capture an approach based on vanilla RGCNs (and no KG augmentation) using a (often substantially) smaller number of layers. Finally, we empirically validate our theoretical findings by evaluating an implementation of our approach against the RGCN baseline on several dedicated benchmarks.",
isbn="978-3-031-19433-7"
}

@article{BiQE, title={Answering Complex Queries in Knowledge Graphs with Bidirectional Sequence Encoders}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/16630}, DOI={10.1609/aaai.v35i6.16630}, abstractNote={Representation learning for knowledge graphs (KGs) has focused on the problem of answering simple link prediction queries. In this work we address the more ambitious challenge of predicting the answers of conjunctive queries with multiple missing entities. We propose Bidirectional Query Embedding (BiQE), a method that embeds conjunctive queries with models based on bi-directional attention mechanisms. Contrary to prior work, bidirectional self-attention can capture interactions among all the elements of a query graph. We introduce two new challenging datasets for studying conjunctive query inference and conduct experiments on several benchmark datasets that demonstrate BiQE significantly outperforms state of the art baselines.}, number={6}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Kotnis, Bhushan and Lawrence, Carolin and Niepert, Mathias}, year={2021}, month={May}, pages={4968-4977} }

@inproceedings{GNNQE,
  author    = {Zhaocheng Zhu and
               Mikhail Galkin and
               Zuobai Zhang and
               Jian Tang},
  editor    = {Kamalika Chaudhuri and
               Stefanie Jegelka and
               Le Song and
               Csaba Szepesv{\'{a}}ri and
               Gang Niu and
               Sivan Sabato},
  title     = {Neural-Symbolic Models for Logical Queries on Knowledge Graphs},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {27454--27478},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/zhu22c.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/Zhu0Z022.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NEURIPS2021_88d25099,
 author = {Boratko, Michael and Zhang, Dongxu and Monath, Nicholas and Vilnis, Luke and Clarkson, Kenneth L and McCallum, Andrew},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {16423--16436},
 publisher = {Curran Associates, Inc.},
 title = {Capacity and Bias of Learned Geometric Embeddings for Directed Graphs},
 url = {https://proceedings.neurips.cc/paper/2021/file/88d25099b103efd638163ecb40a55589-Paper.pdf},
 volume = {34},
 year = {2021}
}

@misc{deepsets,
  doi = {10.48550/ARXIV.1703.06114},
  
  url = {https://arxiv.org/abs/1703.06114},
  
  author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Sets},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{DBLP:journals/corr/abs-1912-01703,
  author    = {Adam Paszke and
               Sam Gross and
               Francisco Massa and
               Adam Lerer and
               James Bradbury and
               Gregory Chanan and
               Trevor Killeen and
               Zeming Lin and
               Natalia Gimelshein and
               Luca Antiga and
               Alban Desmaison and
               Andreas K{\"{o}}pf and
               Edward Z. Yang and
               Zach DeVito and
               Martin Raison and
               Alykhan Tejani and
               Sasank Chilamkurthy and
               Benoit Steiner and
               Lu Fang and
               Junjie Bai and
               Soumith Chintala},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  journal   = {CoRR},
  volume    = {abs/1912.01703},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.01703},
  eprinttype = {arXiv},
  eprint    = {1912.01703},
  timestamp = {Tue, 02 Nov 2021 15:18:32 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-01703.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}