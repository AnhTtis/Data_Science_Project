\addcontentsline{toc}{section}{Appendices}
\renewcommand{\thesubsection}{\Alph{subsection}}

%\subsection{Connecting Relational Rotation Between Different Parameterizations}
\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.6]{Logical_Operators.png}
    \caption{Visualization of logical operators \citep{ConE}}
    \label{loigcal operators}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.5]{relation_rotation.pdf}
    \caption{Relational rotations corresponding to different relation patterns on 2D complex plane.}
    \label{relation pattern rotations}
\end{figure*}



\subsection{Lemmas and Corresponding Proofs}
\begin{theorem}
RoConE can infer the symmetry and anti-symmetry patterns. 
\end{theorem}
\begin{proof}
Given cone query embeddings $X = (\mathbf{h}_{U}^{x},\mathbf{h}_{L}^{x})$, $Y = (\mathbf{h}_{U}^{y},\mathbf{h}_{L}^{y})$ and relation $r = (\mathbf{r}_{U},\mathbf{r}_{L})$,
if $(X,r,Y)$ and $(Y,r,X)$ hold, we have:
\begin{equation}
\begin{split}
    (\mathbf{h}_{U}^{x}\circ\mathbf{r}_{U},\mathbf{h}_{L}^{x}\circ\mathbf{r}_{L}) &=(\mathbf{h}_{U}^{y},\mathbf{h}_{L}^{y}),\\
    (\mathbf{h}_{U}^{y}\circ\mathbf{r}_{U},\mathbf{h}_{L}^{y}\circ\mathbf{r}_{L}) &=(\mathbf{h}_{U}^{x},\mathbf{h}_{L}^{x}),
\end{split}
\end{equation}
\[\rightarrow (\mathbf{h}_{U}^{y}\circ\mathbf{r}_{U}\circ\mathbf{r}_{U},\mathbf{h}_{L}^{y}\circ\mathbf{r}_{L}\circ\mathbf{r}_{L})=(\mathbf{h}_{U}^{y},\mathbf{h}_{L}^{y})\]
thus, it is inferrable that $X$ is symmetric to $Y$ if and only if $(\mathbf{r}_{U}\circ\mathbf{r}_{U} = \mathbf{1},\mathbf{r}_{L}\circ\mathbf{r}_{L} = \mathbf{1})$. \\
Similarly, if $(X,r,Y)$ and $\neg(Y,r,X)$ hold, we have:
\begin{equation}
\begin{split}
        (\mathbf{h}_{U}^{x}\circ\mathbf{r}_{U},\mathbf{h}_{L}^{x}\circ\mathbf{r}_{L})&=(\mathbf{h}_{U}^{y},\mathbf{h}_{L}^{y}),\\(\mathbf{h}_{U}^{y}\circ\mathbf{r}_{U},\mathbf{h}_{L}^{y}\circ\mathbf{r}_{L})&\neq(\mathbf{h}_{U}^{x},\mathbf{h}_{L}^{x}),
\end{split}
\end{equation}
\[\rightarrow (\mathbf{h}_{U}^{x}\circ\mathbf{r}_{U}\circ\mathbf{r}_{U},\mathbf{h}_{L}^{x}\circ\mathbf{r}_{L}\circ\mathbf{r}_{L})\neq(\mathbf{h}_{U}^{x},\mathbf{h}_{L}^{x})\]
thus, it is inferrable that $X$ is anti-symmetric to $Y$ if and only if $(\mathbf{r}_{U}\circ\mathbf{r}_{U} \neq \mathbf{1},\mathbf{r}_{L}\circ\mathbf{r}_{L} \neq \mathbf{1})$. 
\end{proof}


\begin{theorem}
RoConE can infer the inversion pattern.
\end{theorem}
\begin{proof}
Given cone query embeddings $X = (\mathbf{h}_{U}^{x},\mathbf{h}_{L}^{x})$, $Y = (\mathbf{h}_{U}^{y},\mathbf{h}_{L}^{y})$ and relations $r_{1} = (\mathbf{r}_{U,1},\mathbf{r}_{L,1})$, $r_{2} = (\mathbf{r}_{U,2},\mathbf{r}_{L,2})$
if $(X,r_{1},Y)$ and $(Y,r_{2},X)$ hold, we have:
\begin{equation}
\begin{split}
        (\mathbf{h}_{U}^{x}\circ\mathbf{r}_{U,1},\mathbf{h}_{L}^{x}\circ\mathbf{r}_{L,1})&=(\mathbf{h}_{U}^{y},\mathbf{h}_{L}^{y}),\\(\mathbf{h}_{U}^{y}\circ\mathbf{r}_{U,2},\mathbf{h}_{L}^{y}\circ\mathbf{r}_{L,2})&=(\mathbf{h}_{U}^{x},\mathbf{h}_{L}^{x}),
\end{split}
\end{equation}
\[\rightarrow (\mathbf{h}_{U}^{y}\circ\mathbf{r}_{U,2}\circ\mathbf{r}_{U,1},\mathbf{h}_{L}^{y}\circ\mathbf{r}_{L,2}\circ\mathbf{r}_{L,1})=(\mathbf{h}_{U}^{y},\mathbf{h}_{L}^{y})\]
thus, it is inferrable that X is inversible to Y if and only if $\mathbf{r}_{U,2}\circ\mathbf{r}_{U,1}=1$ and $\mathbf{r}_{L,2}\circ\mathbf{r}_{L,1}=1$.
\end{proof}


\begin{theorem}
RoConE can infer the composition pattern.
\end{theorem}
\begin{proof}
Given cone query embeddings $X = (\mathbf{h}_{U}^{x},\mathbf{h}_{L}^{x})$, $Y = (\mathbf{h}_{U}^{y},\mathbf{h}_{L}^{y})$, $Z = (\mathbf{h}_{U}^{z},\mathbf{h}_{L}^{z})$ and relations $r_{1} = (\mathbf{r}_{U,1},\mathbf{r}_{L,1})$, $r_{2} = (\mathbf{r}_{U,2},\mathbf{r}_{L,2})$
if $(X,r_{1},Y)$ and $(Y,r_{2},Z)$ hold, we have:
\begin{equation}
    \begin{split}
    (\mathbf{h}_{U}^{x}\circ\mathbf{r}_{U,1},\mathbf{h}_{L}^{x}\circ\mathbf{r}_{L,1})&=(\mathbf{h}_{U}^{y},\mathbf{h}_{L}^{y}),\\
    (\mathbf{h}_{U}^{y}\circ\mathbf{r}_{U,2},\mathbf{h}_{L}^{y}\circ\mathbf{r}_{L,2})&=(\mathbf{h}_{U}^{z},\mathbf{h}_{L}^{z}),\\
    (\mathbf{h}_{U}^{x}\circ\mathbf{r}_{U,3},\mathbf{h}_{L}^{x}\circ\mathbf{r}_{L,3})&=(\mathbf{h}_{U}^{z},\mathbf{h}_{L}^{z}),
    \end{split}
\end{equation}

$\rightarrow$
\begin{equation*}
    \resizebox{\hsize}{!}{
    $(\mathbf{h}_{U}^{x}\circ\mathbf{r}_{U,1}\circ\mathbf{r}_{U,2},\mathbf{h}_{L}^{x}\circ\mathbf{r}_{L,1}\circ\mathbf{r}_{L,2}) = (\mathbf{h}_{U}^{x}\circ\mathbf{r}_{U,3},\mathbf{h}_{L}^{x}\circ\mathbf{r}_{L,3})$
    }
\end{equation*}
thus, it is inferrable that relation $r_3$ is compositional of $r_1$ and $r_2$ if and only if $\mathbf{r}_{U,1}\circ\mathbf{r}_{U,2}=\mathbf{r}_{U,3}$.
\end{proof}


\subsection{Details of Intersection Operator}
\label{Intersection Operator}
In this section, we explain the details of two important components of the intersection operator, \textbf{SemanticAverage}($\cdot$) and \textbf{CardMin}($\cdot$) 
\paragraph{SemanticAverage($\cdot$)} is expected to compute the semantic center $\pmb{\theta}_{ax}^{'}$ of the input $\{(\pmb{\theta}_{j,ax}, \pmb{\theta}_{j,ap})\}_{j=1}^{n}$. Specifically, the computation process is provided as:
\begin{equation}
\begin{split}
        [\mathbf{x};\mathbf{y}] &= \sum_{i=1}^{n} [\mathbf{a}_{j}\circ \text{cos}(\pmb{\theta}_{j,ax});\mathbf{a}_{j}\circ \text{sin}(\pmb{\theta}_{j,ax})],\\
    \pmb{\theta}_{ax}^{'} &= \textbf{Arg}(\mathbf{x},\mathbf{y}),
\end{split}
\end{equation}
where cos and sin represent element-wise cosine and sine functions. $\textbf{Arg}$($\cdot$) computes the argument given $\mathbf{x}$ and $\mathbf{y}$. $\mathbf{a}_{j} \in \mathbb{R}^d$ are attention weights such that 
\begin{equation}
    \resizebox{0.95\hsize}{!}{%
        $[\mathbf{a}_{j}]_{k} = \frac{\text{exp}([\textbf{MLP}([\pmb{\theta}_{j,ax} - \pmb{\theta}_{j,ap/2};\pmb{\theta}_{j,ax} + \pmb{\theta}_{j,ap/2}])]_{k})}{\sum_{m=1}^{n}\text{exp}([\textbf{MLP}([\pmb{\theta}_{m,ax} - \pmb{\theta}_{m,ap/2};\pmb{\theta}_{m,ax} + \pmb{\theta}_{m,ap/2}])]_{k})}, $%      
        }
\end{equation}
where \textbf{MLP} : $\mathbb{R}^{2d} \rightarrow \mathbb{R}^{d}$ is a multi-layer perceptron network, [$\cdot$;$\cdot$] is the concatenation of two vectors. 

\paragraph{CardMin($\cdot$)} predicts the aperture $\pmb{\theta}_{ap}^{'}$ of the intersection set such that $[\pmb{\theta}_{ap}^{'}]_{i}$ should be no larger than any $\pmb{\theta}_{j,ap}^{i}$, since the intersection set is the subset of all input entity sets.
\begin{equation}
    \resizebox{0.98\hsize}{!}{%
    $[\theta_{ap}^{'}]_{i} = \text{min}\{\theta_{1,ap}^{i},\dots,\theta_{n,ap}^{i}\}\cdot\sigma([\textbf{DeepSets}(\{(\pmb{\theta}_{j,ax},\pmb{\theta}_{j,ap})\}_{j=1}^{n})]_{i})$%
    }
\end{equation}
where \textbf{DeepSets}$(\{(\pmb{\theta}_{j,ax}, \pmb{\theta}_{j,ap})\}_{j=1}^{n})$ \citep{deepsets} is given by
\begin{equation*}
    \resizebox{0.95\hsize}{!}{%
    $\textbf{MLP}(\frac{1}{n}\sum_{j=1}^{n}\textbf{MLP}([\pmb{\theta}_{j,ax} - \pmb{\theta}_{j,ap/2};\pmb{\theta}_{j,ax} + \pmb{\theta}_{j,ap/2}])).$%
    }
\end{equation*}

\subsection{Distance Function}
\label{Distance function}
Inspired by \citet{ConE,Query2box}, the distance between $\mathbf{q}$ and $\mathbf{h}^{*}$ is defined as a combination of inside and outside distances, $d_{com}(\mathbf{q}, \mathbf{h}^{*}) = d_{o} + \lambda d_{i}$:
\begin{equation}
\begin{split}
    d_{o}(\mathbf{q}, \mathbf{h}^{*}) &= min\{\|\mathbf{h}_{U}^{q} - \mathbf{h}^{*}\|_{1}, \| \mathbf{h}_{L}^{q} - \mathbf{h}^{*}\|_{1}\},\\
    d_{i}(\mathbf{q}, \mathbf{h}^{*}) &= min\{\|\mathbf{h}_{ax}^{q} - \mathbf{h}^{*}\|_{1}, \|\mathbf{h}_{U}^{q} - \mathbf{h}_{ax}^{q}\|_{1}\},
\end{split}
\end{equation}
where $\|\cdot\|_{1}$ is the $L_1$ norm, $\mathbf{h}_{ax}^{q}$ represents the cone center, and $\lambda \in (0,1)$.\\
Note that $d_{com}$ can only be used for measuring the distance between a single query embedding and an answer entity vector. Since we represent the disjunctive queries in Disjunctive Normal Form as a set of query embeddings $\{\mathbf{q}_{1},\dots,\mathbf{q}_{n}\}$, the distance between the answer vector and such set of embeddings is the minimum distance between the vector and each of these sets:
\begin{equation}
    \resizebox{\hsize}{!}{
    $d_{dis}(\mathbf{q},\mathbf{h}^{*}) = min\{d_{com}(\mathbf{q}_{1}, \mathbf{h}^{*}),\dots,d_{com}(\mathbf{q}_{n}, \mathbf{h}^{*})\}
    $}
\end{equation}
Thus, the overall distance function can be defined as:
\begin{equation}
\resizebox{\hsize}{!}{
  $d(\mathbf{q},\mathbf{h}^{*}) =
    \begin{cases}
      d_{com}(\mathbf{q},\mathbf{h}^{*}), & \text{query without disjunction}\\
      d_{dis}(\mathbf{q},\mathbf{h}^{*}), & \text{query with disjunction}
    \end{cases}
    $}
\end{equation}



\subsection{More Details about Experiments}
\label{Experiment}
In this section, we elaborate more details about our experiments. The code is anonymously available at \url{ https://anonymous.4open.science/r/RoConE-1C1E}.
\subsubsection{Dataset and Query structures}
RoConE is compared with various state-of-the-art query embedding models, including GQE \citep{GQE}, Query2Box \citep{Query2box}, BetaE \citep{BetaE}, and ConE \citep{ConE}. For a fair comparison with existing query embedding models, we use the same query structures and datasets NELL995 \citep{deeppath} and FB15k-237, and the open-sourced framework created by \cite{BetaE} for logical query answering tasks. Table \ref{Dataset Statistics} summarizes the descriptive statistics about the benchmark datasets. Figure \ref{query structures} illustrates all query structures used in our experiments. 

\begin{table*}
\centering
\begin{tabular}{lllllll}
\hline
\textbf{Dataset} & \multicolumn{2}{c}{\textbf{Training}} & \multicolumn{2}{c}{\textbf{Validation}}& \multicolumn{2}{c}{\textbf{Testing}}\\
\hline
& \textbf{EPFOL} & \textbf{Negation} & \hspace{3mm}\textbf{1p} & \textbf{others} & \hspace{3mm}\textbf{1p} & \textbf{others}\\ 
FB15k-237 & 149,689 & 14,968 & 20,101 & 5,000 & 22,812 & 5,000 \\
NELL995 & 107,982 & 10,798 & 16,927 & 4,000 & 17,034 & 4,000 \\
\hline
\end{tabular}
\caption{\label{Dataset Statistics}
Statistics of benchmark datasets : FB15k-237 and NELL995. EPFOL represents queries without negation, namely 1p/2p/3p/2i/3i.}
\end{table*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.7]{query_structures.png}
    \caption{Fourteen types of queries used in the experiments. "$p$" represents relational projection, "$i$" represents intersection, "$u$" represents union and "n" represents negation. The upper of queries is used in the training stage, while all queries are evaluated in the validation and test stages.}
    \label{query structures}
\end{figure*}

\subsubsection{Hyperparameters and Computational Resources}
All of our experiments are implemented in Pytorch \citep{DBLP:journals/corr/abs-1912-01703} framework and run on four Nvidia A100 GPU cards. For hyperparameters search, we performed a grid search of learning rates in $\{5\times 10^{-5},10^{-4},5\times10^{-4}\}$, the batch size in $\{256, 512, 1024\}$, the negative sample sizes in $\{128,64\}$, the regularization coefficient $\lambda$ in $\{0.02, 0.05, 0.08, 0.1\}$ and the margin $\gamma$ in $\{20, 30, 40, 50\}$. The best hyperparameters are saved in Table \ref{Hyperparameters}. 

\begin{table}
\centering
\resizebox{\hsize}{!}{
\begin{tabular}{lllllll}
\hline
\textbf{Dataset} & d & b & n & $\gamma$ & $\hspace{4mm} l$ & $\lambda$\\
\hline
FB15k-237 & 1600 & 128 & 512 & 30 & 5 $\times 10^{-5} $ & 0.1\\
NELL995 & 800 & 128 & 512 & 20 & $1 \times 10^{-4}$ & 0.02\\
\hline
\end{tabular}
}
\caption{\label{Hyperparameters}
Hyperparameters found by grid search. d represents the embedding dimension, b is the batch size, n is the negative sampling size, $\gamma$ is the margin in loss, l represents the learning rate, $\lambda$ is the regularization parameter in the distance function.
}
\end{table}


\subsubsection{Evaluation Metrics}
In this paper, we use Mean Reciprocal Rank (MRR) as the evaluation metric. Given a sample of queries Q, Mean reciprocal rank represents the average of the reciprocal ranks of results:
\[MRR = \frac{1}{|Q|}\sum_{i=1}^{|Q|}\frac{1}{rank_{i}}\]

\subsubsection{Variants of relational projection strategies}
\label{Variants of RoConE}
To better incorporate the learning the relation patterns in query embedding model, we proposed alternative two variants of the relational rotation projection strategy:
\begin{itemize}
    \item RoConE (Trunc): after relational rotation, the two boundaries of query embedding are truncated to ensure that aperture will not be greater than $\pi$.  
    
    \item RoConE (S.E): replaces relational rotation on boundaries by a non-linear sigmoid function which maps the boundaries to shrink or expand and only maintains rotations on $\theta_{ax}$.
\end{itemize}

\subsubsection{Error Bars of Main Results}
We have run RoConE 10 times with different random seeds, and obtain mean values and standard deviations of RoConE’s MRR results on EPFO and negation queries. Table \ref{EPFOL(sd)} shows that mean values and standard deviations of RoConE’s MRR results on EPFOL queries. Table \ref{negation(sd)} shows mean values and standard deviations of RoConE’s MRR results on negation queries.
\begin{table}
\centering
\resizebox{\hsize}{!}{
\begin{tabular}{llllllllll}
\hline
\textbf{Dataset} & \textbf{1p} & \textbf{2p} & \textbf{3p} & \textbf{2i} & \textbf{3i} & \textbf{pi} & \textbf{ip} & \textbf{2u} & \textbf{up}\\
\hline
$\multirow{2}{5em}{FB15k-237}$ & 42.2$\pm$ & 10.5$\pm$ & 7.5$\pm$ & 33.5$\pm$ & 48.1$\pm$ & 23.5$\pm$ & 14.5$\pm$ & 12.8$\pm$ & 8.9$\pm$\\
& 0.054 & 0.108 & 0.142 & 0.075 & 0.172 & 0.105 & 0.183 & 0.096 & 0.193\\
\hline
$\multirow{2}{5em}{NELL995}$ & 54.5$\pm$ & 17.7$\pm$ & 14.4$\pm$ & 41.9$\pm$ & 53.0$\pm$ & 26.1$\pm$ & 20.7$\pm$ & 16.5$\pm$ & 12.8$\pm$\\
& 0.097 & 0.152 & 0.192 & 0.130 & 0.038 & 0.084 & 0.146 & 0.112 & 0.137\\
\hline
\end{tabular}
}
\caption{RoConE's MRR mean values and standard variances ($\%$) on answering EPFO ($\exists$, $\land$, $\lor$) queries}
\label{EPFOL(sd)}
\end{table}


\begin{table}
\centering
\resizebox{\hsize}{!}{
\begin{tabular}{llllll}
\hline
 \textbf{Dataset} & \textbf{2in} & \textbf{3in} & \textbf{inp} & \textbf{pin} & \textbf{pni}\\
\hline
$\multirow{2}{5em}{FB15k-237}$ & 4.1$\pm$ & 7.9$\pm$ & 6.9$\pm$ & 3.1$\pm$ & 2.8$\pm$\\
 & 0.089 & 0.092 & 0.161 & 0.083 & 0.077\\
\hline
$\multirow{2}{5em}{NELL995}$ & 5.2$\pm$ & 7.7$\pm$ & 9.4$\pm$ & 3.2$\pm$ & 3.7$\pm$\\
& 0.012 & 0.079 & 0.154 & 0.012 & 0.097\\
\hline
\end{tabular}
}
\caption{RoConE's MRR mean values and standard variances ($\%$) on answering negation queries}
\label{negation(sd)}
\end{table}
