\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{soul}
\usepackage{soulutf8}
\usepackage{xcolor}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
%\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
%\usepackage{subfigure}
\usepackage{calc}
\usepackage{multicol}
\usepackage{accents}
\usepackage{footnote}
\usepackage{multirow}
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage[mathscr]{euscript}
\usepackage{bigints}
\usepackage{upgreek}
\usepackage{soul}
\usepackage[percent]{overpic}
\usepackage{bm}
\usepackage{physics}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{soul}
\usepackage{yhmath}
\usepackage{tikz}
%\usepackage{blockdiagram}
%\usepackage{fourier}
\usepackage{comment}
\usepackage{cellspace}
 \usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan,}
\usepackage{natbib}
\bibliographystyle{ieeetr}
\setcitestyle{numbers,square}

\graphicspath{{./figures/}}

\newcommand{\hhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}

% -- notation
\DeclareMathAlphabet{\mathbfit}{OT1}{cmr}{bx}{it}
% scalaire = $l, E$
% point 2D = $\mathbfit{p}$
% point 3D = $\mathbf{p}$
% vecteur = $\mathbf{m}$
% matrice = $\mathbf{I}$

% -- soul 
\DeclareRobustCommand{\todo}[1]{{\sethlcolor{yellow}\hl{#1}}}

\newcommand{\GL}[1]{\color{orange}(GL : #1)\color{black}\,}
\newcommand{\gl}[1]{\textcolor{orange}{#1}}
\newcommand{\BT}[1]{\color{green}(BT : #1)\color{black}\,}
\newcommand{\bt}[1]{\textcolor{green}{#1}}
\newcommand{\MD}[1]{\color{blue}(MD : #1)\color{black}\,}
\newcommand{\md}[1]{\textcolor{blue}{#1}}

\newcommand{\karimhl}[2][green]{\begingroup\sethlcolor{#1}\hl{#2}\endgroup}

\begin{document}


\title{RoCNet: 3D Robust Registration of Point-Clouds using Deep Learning}
%\title{RoCNet: 3D Pose Estimation and Registration of Point-Clouds}

\author{Karim Slimani$^{1}$, Brahim Tamadazte$^{1}$, and Catherine Achard$^{1}$

\thanks{This work was supported by the French ANR program MARSurg (ANR-21-CE19-0026).}

\thanks{$^{1}$ authors are with Sorbonne Universit\'e, CNRS UMR 7222, INSERM U1150, ISIR, F-75005, Paris, France. \protect\url{karim.slimani@isir.upmc.fr}}
}

% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
%{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\maketitle
\begin{abstract}
This paper introduces a new method for 3D point cloud registration based on deep learning. The architecture is composed of three distinct blocs: (\emph{i}) an encoder composed of a convolutional graph-based descriptor that encodes the immediate neighbourhood of each point and an attention mechanism that encodes the variations of the surface normals. Such descriptors are refined by highlighting attention between the points of the same set and then between the points of the two sets. (\emph{ii}) a matching process that estimates a matrix of correspondences using the Sinkhorn algorithm. (\emph{iii}) Finally, the rigid transformation between the two point clouds is calculated by RANSAC using the $K^c$ best scores from the correspondence matrix. We conduct experiments on the ModelNet40 dataset, and our proposed architecture shows very promising results, outperforming state-of-the-art methods in most of the simulated configurations, including partial overlap and data augmentation with Gaussian noise.
\end{abstract}

\begin{IEEEkeywords}
Point clouds, Registration, Deep Learning, Attention Mechanisms, Pose Estimation
\end{IEEEkeywords}
%
% ------------------------
\section{INTRODUCTION}\label{sec.intro}
% ------------------------
%
Point cloud registration is a  widespread problem and a key task in 3D pose estimation in robotics and computer vision, with applications in autonomous driving~\cite{chen20203d}, simultaneous localization and mapping (SLAM)~\cite{mdgat}, etc. The registration process involves matching points between the input and target point clouds, eliminating outliers, and estimating the rigid transformation parameters that align one point cloud to the other. Traditional algorithms, such as Iterative Closest Point (ICP)~\cite{besl1992method}, alternate between matching and aligning iterations. Recently, neural network-based techniques, such as~\cite{wang2019prnet, Wang_2019_ICCV, aoki2019pointnetlk}, are increasingly used to address these problems. These techniques typically encode each point and its neighbourhood through a learned descriptor, such as~\cite{dgcnn, qi2017pointnet++}, and use a transformer module~\cite{mdgat, Wang_2019_ICCV} to propagate local and global information between the two points sets. Point matching is often performed based on similarities in the descriptor space, and the transformation matrix can be estimated by integrating differentiable Singular Value Decomposition (SVD) into the network. Alternatively, WsDesc~\cite{li2022wsdesc} suggested tackling the matching problem by looking, for each pair point, their nearest neighbour in the target point cloud during the training phase. 
Furthermore, recent work~\cite{mdgat} suggested using an optimization procedure on the scoring matrix using the iterative Sinkhorn algorithm~\cite{sinkhorn1967concerning}. Therefore, the estimation of the rigid transformation can be achieved in two different ways, either end-to-end learning by integrating differentiable SVD~\cite{soft_svd} into the network as proposed in~\cite{Wang_2019_ICCV}, or by applying a simple SVD or a RANSAC based on feature matching.

Certainly, the learning-based approaches have made significant progress and have led to overcome the numerous limitations of iterative methods, such as converging to a local minimum, conditioned by a correct initialization, and may find it difficult to estimate large transformations. Additionally, most of these methods extract point-wise features using the neighbourhood of each point by applying learning-based descriptors only. This makes these methods more sensitive to noise and outliers. The methods that apply RANSAC to overcome this problem may need a large number of iterations (e.g., 50, 000 iterations) to reach a correct estimation of the transformation. 
%
\begin{figure}[t]
\centerline{\includegraphics[width= \columnwidth]{concept.pdf}}
\caption{Overall concept of the proposed RoCNet method.} %It combines \emph{DGCNN} and a \emph{Normal Encoder} to extract local features from input points. These features are refined using an \emph{Attention Mechanism}  composed of self and cross-attention and then used to construct a confidence matrix for point-wise matching. Finally, a \emph{RANSAC}-based method is employed to estimate the rigid transformation.}
\label{fig.concept}
\end{figure}

In this paper, we proposed a new architecture (Fig.~\ref{fig.concept}), called RoCNet, which includes three main blocks: 1) a descriptor that is composed of a convolutional graph-based network that encodes the immediate neighbourhood of each point and an attention mechanism that encodes the variations of the surface normals, 2) a matching module that estimates a matrix of correspondences using the Sinkhorn algorithm, 3) a RANSAC module which computes the rigid transformation using the $K^c$ (e.g., 256) best matches with a limited number of iterations (e.g., 500 iterations). The proposed architecture was assessed using ModelNet40 dataset~\cite{wang2019prnet} in different favourable and unfavourable conditions. It has been demonstrated that our method outperformed the related state-of-the-art algorithms, especially in unfavourable conditions, e.g., with noisy data and partial occlusions.
%
% --------------------
\section{RELATED WORK}\label{sec.related}
% --------------------
%
This section provides a review of the state-of-the-art regarding the main methods for 3D pose estimation and point cloud registration. We begin by introducing some interesting descriptors for 3D points cloud. Then, the main registration approaches are presented, which can be classified into three main categories: 1) iterative methods, 2) matching learning 3) transformation learning. For each category, descriptors can be handcrafted or learned.
%
% ------------
\subsection{3D Point Cloud Descriptors} 
The first iterative methods presented below, use the 3D coordinates points directly as input to their system or handcrafted features like Fast Point Feature Histograms (FPFH)~\cite{rusu2009fast}. Since the rise of deep learning (DL), several methods have been developed to learn 3D point cloud descriptors. For instance, Qi~\emph{et al.}~\cite{qi2016pointnet} proposed a neural architecture named PointNet describing unordered 3D points for tasks such as classification, segmentation, and registration. An extension named PointNet++~\cite{qi2017pointnet++}, which exploits metric space distances, has then been proposed by the same authors. Wang \textit{et al.}~\cite{dgcnn} proposed the Dynamic Graph CNN (DGCNN) descriptor based on a module called EdgeConv that acts on graphs computed in each layer of the network. It captures semantic characteristics over potentially long distances as shown in segmentation and classification tasks. This descriptor is also used for registration tasks as reported in~\cite{wang2019prnet}. It is important to note that both previous descriptors are learnt on segmentation or classification tasks before being used, without new learning, on registration. In~\cite{mdgat}, the authors build a feature with two main components, a descriptor encoder that highlights handcrafted features obtained with FPFH and a positional encoder that highlights spatial properties of the point cloud. A multiplex dynamic graph attention network is added to reinforce the matching power of the descriptor. This descriptor is learnt conjointly with the matching process, in an end-to-end way.
%
% ------------
\subsection{Iterative Methods}\label{Iterative Methods}
The ICP~\cite{besl1992method} is probably the most popular method to address a point cloud registration problem. Given two sets of 3D points, the purpose of the algorithm is to minimise the Euclidean distance between the points. At each iteration, a mapping of the two sets of points and the computation of the 3D rigid transformation using an SVD are performed. This procedure is repeated until convergence. In RANSAC~\cite{fischler1981random}, the two-point clouds are randomly split into subsets on which a transformation is estimated. The final transformation is chosen among them using a criterion such as the weighted error on the set of points~\cite{el2021unsupervisedr} or by selecting the transformation generating the largest number of inliers. Both methods have been associated with neural network based learned descriptors like in~\cite{Wang_2019_ICCV,r_pointhop} for ICP and in~\cite{li2022wsdesc} for RANSAC. A most recent approach called Fast Global Registration (FGR)~\cite{zhou2016fast} operates on candidate matches that cover the surfaces. The surface alignment is defined with an objective optimized thanks to an iterative procedure.
%
% ----------------
\subsection{Methods based on Matching Learning} 
%
Recent registration methods investigated DL architecture to match the points. Later, standard methods like RANSAC or SVD can be used to estimate the rigid transformation. For instance, Predator~\cite{huang2021predator} is trained with three different weighted matching losses to be more robust to low partial overlap between the input point clouds. Alternatively, 3DFeat-Net~\cite{yew2018-3dfeatnet} train their architecture to detect key points and predict discriminative descriptors in a weakly supervised manner using the triplet loss~\cite{triplet_loss}, while D3Feat~\cite{bai2020d3feat} combined two losses, one for the descriptor and the other for the detector. All these methods use a RANSAC module on feature matching to estimate the transformation parameters. In the MDGAT method~\cite{mdgat}, proposed to learn the matching using a new loss inspired by the triplet function. Roufosse~\textit{et al.}~\cite{roufosse2019unsupervised} proposed an unsupervised matching approach by optimizing the global structural properties of functional map~\cite{ ovsjanikov2012functional}, such as their bijectivity or approximate isometry. Such properties allow the creation of a loss function that does not require the knowledge of the ground truth during the learning process. 
%
%------------------
\subsection{Methods based on Transformation Learning} 
In the Deep Closest Point (DCP) architecture~\cite{Wang_2019_ICCV}, the descriptor is first performed using DGCNN and an attention-based module is introduced into a transformer. Then a soft SVD, where soft matching is used, allows computing the rigid transformation between both sets of points clouds. The training loss function is defined from this estimated rigid transformation that is compared to the ground truth one. In GeoTransformer~\cite{geotransformer}, the input point clouds are down-sampled in several super-points (a subset of points) that are described using the Geometric Transformer which encodes intra-point-cloud and inter-point-cloud geometric structures. A matching module extracts super-point correspondences, each one being used to compute a soft SVD (from the subset of points corresponding to the super point) and estimate the transformation. The global transformation is the one that admits the most inlier matches over the transformation obtained for each super point. The loss function is composed of two terms: one measuring the alignment quality of super points and the other one measuring the alignment quality of the whole set of points. Wang \textit{et al.} proposes PRNET~\cite{wang2019prnet} that, similarly to ICP, is designed to be applied iteratively to estimate the transformation between points clouds. The matching is performed using an approximately differentiable version of Gumbel-Softmax and the transformation is obtained using an SVD. Another approach was proposed in~\cite{li2022wsdesc} which uses a differentiable nearest neighbour search algorithm in the descriptor space to match the points, and then proposes to relax the registration problem and seeks to estimate an affine transformation matrix computed by a least squares optimisation. An end-to-end architecture~\cite{aoki2019pointnetlk} includes a descriptor based on PointNet~\cite{qi2016pointnet} and a neural network version of the Lucas $\&$ Kanade algorithm that allows incrementally updating the transformation.
%
% --------------------
\section{METHOD}\label{sec.method}
% --------------------
%
%This section details the main steps of the proposed RoCNet algorithm which include setting up a new descriptor, matching and estimating the rigid transformation. 
%
\subsection{Problem Statement}\label{sub.sec.problem}
Let us start by defining a common problem of 3D point cloud registration. Considering two-point clouds $\boldsymbol{X}$ and $\boldsymbol{Y}$ such that: 
$\boldsymbol{X}=\{ \boldsymbol{x}_1,...,\boldsymbol{x}_i,...,\boldsymbol{x}_M\} \subset \mathbb{R}^{3\times M}$ and $ \boldsymbol{Y}=\{ \boldsymbol{y}_1,...,\boldsymbol{y}_j,...,\boldsymbol{y}_N\} \subset \mathbb{R}^{3\times N}$. It is assumed that the two sets at least partially overlap, so that there are $K$ pairs of matches between $\boldsymbol{X}$ and $\boldsymbol{Y}$, with $K \leq min(M,N)$. The two subsets containing the matching points in the first and second point clouds are defined by: $\Bar{\boldsymbol{X}} \subset \mathbb{R}^{3\times K} $ and $ \Bar{\boldsymbol{Y}} \subset \mathbb{R}^{3\times K}$, respectively. Note that the set $\Bar{\boldsymbol{Y}}$ is obtained by applying a rotation $\bold{R} \in SO(3)$ and a translation $\bold{t} \in \mathbb{R}^{3}$ of the set $\Bar{\boldsymbol{X}}$. Both the rotation matrix $\bold{R}$ and translation vector $\bold{t}$ define the $4 \times 4$ rigid transformation we are looking for, noticed $\bold{T}$. 
%
%
% ------------------
\subsection{Descriptor}\label{subsec.descriptor}
% ------------------
%
One of the most fundamental components in a point cloud registration problem is the relevance and quality of the descriptor used to encode the points. Therefore, we proposed a new descriptor by projecting the initial sets of points $\boldsymbol{X}$ and $\boldsymbol{Y}$ in a new base of higher dimension, de facto, more discriminating than the initial spatial representation, and as invariant as possible to rotations and translations. It combines a geometrical-based descriptor and a normal-based one, followed by an attention mechanism.
%
% --------
\subsubsection{Geometrical-based descriptor}\label{DGCNN} 
Different types of descriptors, that learn local geometrical properties around each point, were reported in the literature such as PointNet~\cite{qi2016pointnet}, PointNet++~\cite{qi2017pointnet++} or DGCNN descriptor~\cite{dgcnn}.
We integrated DGCNN as a part of our descriptor because it better captures local geometric features of point clouds while still maintaining permutation invariance. It consists of mainly \emph{EdgeConv} convolution layers where the points represent nodes connected by arcs to their $k$ nearest neighbours in the encoding space to build graphs that express the local geometric structure surrounding each point and then spread dynamically the information at a higher level (global encoding). Lets us denote $\boldsymbol{f}^X_i$ the extracted feature vector of dimension $d$ for point $\boldsymbol{x}_i$.
%
\subsubsection{Normal based descriptor}\label{sub.sec.normals}
The main idea of this descriptor is to better encode the surface around each point using the variation of the normals of points in the neighbourhood: in a flat surface, there is no variation of the normals, along a ridge, the normals vary only in one direction, whereas on a summit, the normals vary in all directions. Thus, the variation of the angle of the normals in a neighbourhood is informative about the type of surface.

The normals are estimated using Principal Component Analysis (PCA). Indeed, for each point $\boldsymbol{x}_i \in \boldsymbol{X}$, a local neighbourhood subset of points $S_i = \left \{ \boldsymbol{x}_j /  \left\| \boldsymbol{x}_j - \boldsymbol{x}_i\right\| ^2 \leq r \right \}$ is defined while delimiting the size of the set points with $|S_i|<K_{nn}$. $r$ is the radius of a sphere centred in $\boldsymbol{x}_i$ and $K_{nn}$ the maximum number of points included in the set $S_i$. The eigenvalue decomposition of the covariance matrix $Cov(S_i)$ allows defining the normal $\boldsymbol{n}_i$ as the vector associated with the smallest eigenvalue. The covariance matrix $Cov(S_i)$ is expressed as follows:
%
\begin{equation}
Cov(S_i) = \frac{1}{|S_i|} \sum_{\boldsymbol{x}_j \in S_i} (\boldsymbol{x}_j - \boldsymbol{x}_i)(\boldsymbol{x}_j - \boldsymbol{x}_i)^\top
\end{equation}
where $|S_i|$ represents the number of points in $S_i$.

Since the PCA does not inherently determine the direction of the normal vector which can point in either direction, we propose to address the ambiguity of the sign by using a new vector $\boldsymbol{z}_i$. It is colinear to $\boldsymbol{n}_i$ and is defined by ensuring that it points towards the side of the surface with a higher point density. This means that the normal vector should be pointing away from sparse areas and towards denser surface areas. Similar to~\cite{li2022wsdesc}, we solve this ambiguity thanks to the:  
%
\begin{equation}
\boldsymbol{z}_i = 
\begin{cases}
\boldsymbol{n}_i, & \text{if } ~~\underset{{\boldsymbol{x}_j \in S_i}}\sum \boldsymbol{n}_i^T ~( \boldsymbol{x}_i - \boldsymbol{x}_j ) \geq 0 \\\
-\boldsymbol{n}_i, & \text{otherwise}
\end{cases}
\end{equation}
  
Finally, we build the final encoding based on~\cite{geotransformer} and~\cite{vaswani2017attention} using sinusoidal functions of different frequencies. Knowing the angle between the normals of two points $\boldsymbol{x}_i$ and $\boldsymbol{x}_j$ noted $\angle(\boldsymbol{z}_i,\boldsymbol{z}_j)$, the vector $\boldsymbol{g}_{\boldsymbol{x}_i,\boldsymbol{x}_j}$ encoding the normals is given by:  
%
\begin{equation} \label{eq.normal_embed}
\begin{cases}
\boldsymbol{g}_{\boldsymbol{x}_i,\boldsymbol{x}_j}^{2ind} = \sin\left(\frac{\angle(\boldsymbol{z}_i,\boldsymbol{z}_j) }{\tau \times 10000^{2ind/d}}\right) \\
\boldsymbol{g}_{\boldsymbol{x}_i,\boldsymbol{x}_j}^{2ind+1} = \cos\left(\frac{\angle(\boldsymbol{z}_i,\boldsymbol{z}_j)}{\tau \times 10000^{2ind/d}}\right)
\end{cases}   
\end{equation}
where $ind$ is the current value index of $\boldsymbol{g}_{\boldsymbol{x}_i,\boldsymbol{x}_j}$, $\tau$ a normalisation coefficient and $d$ the dimension of the descriptor $\boldsymbol{g}_{\boldsymbol{x}_i,\boldsymbol{x}_j}$ fixed to the same size as the geometrically based descriptor DGCNN. A fully connected layer is then applied to $\boldsymbol{g}_{\boldsymbol{x}_i,\boldsymbol{x}_j}$ to obtain the final embedding 
\begin{equation}
    \boldsymbol{e}_{i,j}^X = \boldsymbol{g}_{\boldsymbol{x}_i,\boldsymbol{x}_j} \bold{W}^s_E
\end{equation}
where $\bold{W}^s_E \in \mathbb{R}^{d \times d}$ is a learned projection matrix.
%
% -----------------
\subsubsection{Attention mechanism} \label{normal_transf}
A key point of recent descriptors of points cloud is the introduction of an attention mechanism that highlights some features dynamically.
SuperGlue~\cite{sarlin20superglue} uses a module based on attention graphs that alternately stacks 'self-attention' and 'cross-attention' layers. The former links all the nodes of a point cloud to each other, while the latter links each point of set $\boldsymbol{X}$  to all points of set $\boldsymbol{Y}$. 
Contrary to SuperGlue~\cite{sarlin20superglue} or MDGAT~\cite{mdgat}, which compute the attention weights on the encoding vectors, some methods propose adding information on local inter-points geometry at the entry of the mechanism.  
For instance, \cite{zhao2021point} associates the 3D coordinates of each point with the descriptor, while GeoTransformer~\cite{geotransformer} proposes to use the distances and angles between each point and its $k$ nearest neighbours. Alternatively, in our approach, we propose the use of four attention heads with geometric self-attention inside each set $\boldsymbol{X}$ and $\boldsymbol{Y}$ integrating the associated normals embeddings $\boldsymbol{e}^X$ and $\boldsymbol{e}^Y$, respectively, followed by cross-attention between the two sets of points and then alternate between them for $L$ times.
%²
\subsubsection{Self-attention} 
This type of layer predicts an attention-based feature $\bar{\boldsymbol{f}}_i$ for each point of a point cloud ($\boldsymbol{X}$ or $\boldsymbol{Y}$), paying attention to all the other points of the same cloud. In the following, the algorithm is detailed for a point $\boldsymbol{x}_i \in \boldsymbol{X}$, the same is used for all the points in $\boldsymbol{X}$  and $\boldsymbol{Y}$. Thus, an attention weight is then obtained for each query/key pair:
%
\begin{equation}\label{wei}
\alpha_{ij}^X =  \underset{j}{softmax} \bigg( \frac{(\boldsymbol{f}_i^X \bold{W}^s_Q )(\boldsymbol{f}_j^X \bold{W}^s_K+\boldsymbol{e}_{i,j}^X \bold{W}^s_R)^T}{\sqrt{{d}}}\bigg)
\end{equation}
where $\bold{W}^s_Q$, $\bold{W}^s_K$ and $\bold{W}^s_R \in \mathbb{R}^{d\times d}$ are the learnt projection matrices for queries, keys and normal-based embeddings, $d$ is the dimension of the features $\boldsymbol{f}_i^X$ and $\boldsymbol{e}^X_{i,j}$. These weights are used to rate which elements we have to pay attention to, and to obtain the final self-attention-based feature $\bar{\boldsymbol{f}}_i^X$: 
%
\begin{equation}\label{wei}
\bar{\boldsymbol{f}}_i^X =   \sum_{j=1} \alpha_{ij} \boldsymbol{v}_j
\end{equation}
with,
\begin{equation}\label{wei}
\boldsymbol{v}_j = \boldsymbol{f}_j^X \bold{W}^s_V
\end{equation}
where $\bold{W}^s_V \in \mathbb{R}^{d\times d}$  is the learnt projection matrix for values. 
%
% --------
\subsubsection{Cross-Attention} 
A \emph{cross-attention} layer is used to propagate the local information between the two previously obtained representations $\bar{\boldsymbol{f}}_i^X$ and $\bar{\boldsymbol{f}}_j^Y$ of $\boldsymbol{x}_i$ and $\boldsymbol{y}_j$ belonging respectively to point clouds $\boldsymbol{X}$ and $\boldsymbol{Y}$. Formally, it works similarly as for the \emph{self-attention} layer, except for the estimation of the attention key, which now uses a point in the second point cloud. The final encoding for any point $\boldsymbol{x}_i$ (or $\boldsymbol{y}_j)$ is given by:

\small
\begin{equation}\label{wei}
{\boldsymbol{h}}_i^X =   \sum_{j=1}^{\lvert  \boldsymbol{Y} \rvert } \Bigg( \underset{j}{softmax} \bigg( \frac{(\bar{\boldsymbol{f}}_i^X \bold{W}^c_Q )(\bar{\boldsymbol{f}}_j^Y \bold{W}^c_K)^T}{\sqrt{{d}}}\bigg) \Bigg ) \Bigg ( \bar{\boldsymbol{f}}_j^Y \bold{W}^c_V \Bigg )
\end{equation}
\normalsize
where $\bold{W}^c_Q$, $\bold{W}^c_K$ and $\bold{W}^c_V \in \mathbb{R}^{d\times d}$ are the learnt projection matrices for queries, keys and values in the cross-attention layers.
%
% ----------------
\subsection{Point Matching} \label{matching}
%
The second step of the proposed algorithm is the matching procedure. We first estimate a score matrix  $\bold{C} \in \mathbb{R}^{M\times N}$ between each point $x_i \in \boldsymbol{X}$ and $y_j \in \boldsymbol{Y}$: 
%
\begin{center}
\begin{equation}\label{eq.score_matrix}
\bold{C}_{i,j} = {\boldsymbol{h}_i^X}^\top \boldsymbol{h}_j^Y
\end{equation}
\end{center}
where $\boldsymbol{h}_i^X$ and $\boldsymbol{h}_j^Y$ are the final encoding of the points $\boldsymbol{x}_i$ and $\boldsymbol{y}_j$ defined previously. To build a matrix of correspondence probabilities $\bold{\Bar{C}}$, we first augment the dimensions of $\bold{C}$ to $M+1$ and $N+1$ respectively, such that the non-matched points will explicitly be assigned to the last dimensions. We then use the differentiable \emph{Sinkhorn Algorithm}~\cite{sinkhorn1967concerning} which is widely used in optimal transport and graph-matching problems.

As all the previous steps are differentiable, the weights of the networks can be learnt by introducing a loss function. To do so, we follow~\cite{mdgat} and adopt the gap loss function~(\ref{gap_loss_equation}) which allows enlarging the assignment scores difference between the true matches and the wrong matches. It is expressed as follows:

\small
\begin{multline}\label{gap_loss_equation}
    L_{Gap} = 
    \sum_{i=1}^M \log ( \sum_{n=1}^{N+1}[\max((-\log \bold{\Bar{C}}_{i,\bar{i}} + \log \bold{\Bar{C}}_{i,n} +\alpha),0)]+1 ) \\ + 
    \sum_{j=1}^N \log ( \sum_{n=1}^{M+1}[\max((-\log \bold{\Bar{C}}_{j,\bar{j}} + \log \bold{\Bar{C}}_{n,j} +\alpha),0)]+1 ) 
\end{multline}
\normalsize
where $\alpha$ is a positive scalar having a value of 0.5, $\bold{\Bar{C}}_{i,\bar{i}}$ and $\bold{\Bar{C}}_{j,\bar{j}}$ are the scores for the ground truth true matches of the points $\boldsymbol{x}_i$ and $\boldsymbol{y}_j$, respectively.
 

\begin{figure*}
\centerline{\includegraphics[width=2\columnwidth]{architecture}}
\caption{Overview of the proposed RoCNet architecture.}
\label{architecture}
\end{figure*}

\begin{figure}[!h]
\centering
\includegraphics[width=.8\columnwidth]{matchs} 
\caption{Example of a performed 3D matching of point clouds in different configurations: (a) clean data, (b) partial overlap, and (c) noisy data and partial overlap. The green lines show the correct matches and the red lines show the wrong ones. }
\label{fig.matching}
\end{figure}
%
%------------
 \subsection{Pose Estimation}\label{RANSAC}


In the evaluation phase, we build a hard assignment binary matrix $\bold{A}$ thanks to the following algorithm: 
%
\begin{equation}
    \bold{A^{1}}_{i,j} = \begin{cases} 1 &\text{if } \bold{\Bar{C}}_{i,j} = \underset{n}\max(\bold{\Bar{C}}_{i,n})\\ 0 &\text{otherwise,} \end{cases}
\end{equation}
%
\begin{equation}
    \bold{A^{2}}_{i,j} = \begin{cases} 1 &\text{if } \bold{\Bar{C}}_{j,i} = \underset{n}\max(\bold{\Bar{C}}_{j,n})\\ 0 &\text{otherwise,} \end{cases}
\end{equation}
%
\begin{equation}
    \bold{A}_{i,j} = \bold{A^{1}}_{i,j} \times \bold{A^{2}}_{i,j}
\end{equation}

The matrix $\bold{A}$ gives us the two final sets of matched points $\Bar{\boldsymbol{X}} \in \mathbb{R}^K$ and $\Bar{\boldsymbol{Y}} \in \mathbb{R}^K$ by re-indexing the original point clouds $\boldsymbol{X} \in \mathbb{R}^M$ and $\boldsymbol{Y} \in \mathbb{R}^N$ with the row and column indices of the non-zero values of the matrix $\bold{A}$ respectively. An example of a performed matching is depicted in Fig.~\ref{fig.matching}. 
Once the sets of matched points are built, different techniques can be used to determine the rigid transformation. A classical SVD to the cross-covariance matrix between the centred subsets $\Bar{\boldsymbol{X}}$ and $\Bar{\boldsymbol{Y}}$ is used in MDGAT~\cite{mdgat}, while DCP~\cite{Wang_2019_ICCV} suggested a differentiable and soft SVD where the weights of each point are determined by applying a \emph{Softmax} to the score matrix $\bold{C}$. An alternative method is to apply a RANSAC technique based on feature matching as reported in~\cite{huang2021predator, li2022wsdesc}. In our method, we propose to use RANSAC based on our predicted correspondences to reduce the computational cost. Moreover, instead of considering all the $K$ matched points, we only use the $K^c$ most relevant ones allowing us to filter the outliers before the first iteration such that the transformation is performed in 500 iterations maximum. % A comparison between the use of an SVD or RANSAC for the transformation estimation is performed and reported in Table~\ref{tab.abla.ransac}. It can be highlighted that the RANSAC-based approach outperforms the SVD one, namely for the estimation of the rotation.  
%
% --------------------------------
\section{EXPERIMENTS}\label{sec.exp}
%
\subsection{Dataset and Parametrisation}
To assess the RoCNet, we opted for the ModelNet40 dataset~\cite{wang2019prnet}. It is a synthetic database representing 40 classes of objects designed using Computer-Aided Design (CAD) software. The database consists of 12,311 3D point clouds divided into 9,843 sets for training and 2,468 for testing. Each of these point clouds is scaled to fit inside a sphere of radius $r = 1$~m. For each initial point cloud called $\boldsymbol{X}$, a new point cloud $\boldsymbol{Y}$ is created by applying a random rigid transformation with a rotation ranging from 0$^\circ$ to 45$^\circ$ around each axis and a translation ranging from 0~cm to 50~cm in each direction, and finally, a random permutation of the points is performed. In the end, a point cloud of 1024 points is generated for each object. In addition, to be able to evaluate the robustness of registration or pose estimation methods, the initial points clouds are reduced by a range of points emulating partial occlusions. 

The following lists the different configurations used to assess our method in terms of accuracy, robustness and generalization: 
%
\begin{itemize}
\item \emph{Partial overlap}: To simulate partial occlusions, 768 points are subsampled from the 1024 components of both the first and the second points clouds $\boldsymbol{X}$ and $\boldsymbol{Y}$, respectively. Specifically, between 512 and 768 points from $\boldsymbol{X}$ have a match in $\boldsymbol{Y}$.
%
\item \emph{Noisy data}: Clipped Gaussian noise with a range of $[-0.05, 0.05]$, a mean of $\mu = 0$, and a variance of 0.01 is added to each point.
%
\item \emph{Partial overlap and noisy data}: The same type of noise is also added to the subsampled point clouds. 
%
\item \emph{Unseen objects}: To test the generalization of the registration and pose estimation methods, the model is trained only on the first 20 classes of objects and tested on the remaining 20 categories.
%
\end{itemize}

RoCNet is trained for 30 epochs on the clean data and for 80 epochs on the noisy data with a learning rate of $10^{-4}$ in both cases. The number of predicted correspondences used as input in RANSAC is $K^c=256$. The parameters $d$ and $L$ are set to $96$ and $6$ respectively. 
%
% -----------------
\subsection{Metrics} \label{metriques}
%
To benchmark our RoCNet architecture against state-of-the-art methods, we opted for two metrics widely used in the literature which consist of the Root Mean Square Error (RMSE) and the Mean Absolute Error (MAE) used to compute the difference in rotation and translation between the estimated transformation and the provided ground truth. The RMSE and MAE are respectively defined as follows:
%
\begin{equation} \label{RMSE}
    \text{RMSE}(\boldsymbol{p}) = \Big(\frac{1}{M}\sum_{i = 1 }^{M}\left\| \bold{p}_i - \bold{p}^{GT}_i\right\|^2\Big)^\frac{1}{2}
\end{equation}
%
\begin{equation} \label{MAE}
    \text{MAE}(\bold{p}) = \frac{1}{M}\sum_{i = 1 }^{M}\left|\bold{p}_i - \bold{p}^{GT}_i\right| 
\end{equation}
with, $M$ is the number of pairs of point clouds in the test set, $\bold{p}_i$ and $\bold{p}^{GT}_i$ are the estimated rotation (Euler angles) or translation and the ground truth ones, respectively. 
%
% ----------------
\subsection{Results}\label{sub.sec.results}
RoCNet is assessed in different configurations (i.e., favourable and unfavourable) and compared to the main DL-based methods of the state-of-the-art as well as with the traditional ICP approach. The recent VRNet~\cite{zhang2022vrnet} has summarised nicely the performance of most of the related methods reported in the literature. We use this performance as a basis for comparison to which we have added those of recently published methods such as WsDesc~\cite{li2022wsdesc} and R-PointHop~\cite{r_pointhop}. Note that in all the tables, $\bold{R}$ is given in degrees, and $\bold{t}$ is in meters, while the best results are highlighted in bold and the second ones are underlined.
%
% ------
\subsubsection{Model trained on all classes with clean data}
The first configuration consists of the evaluation of RoCNet when the model is trained across all the provided 40 classes of objects. All the data are clean and do not involve occlusions. 
%
\small
\begin{table}[!h]
\caption{Performances of RoCNet using all classes without noise and occlusions.}
\begin{center}
\begin{tabular}{l | l  l | l l} 
\hline 

\hline 

\hline
Method & RMSE($\bold{R}$) & MAE($\bold{R}$) & RMSE($\bold{t}$) & MAE($\bold{t}$)  \\ 
\hline
ICP'92~\cite{besl1992method}       & 12.28  & 4.613      & 0.04774 & 0.00228 \\
%FGR'16~\cite{zhou2016fast}        & 20.05  & 7.146      & 0.04412 & 0.01642 \\
PTLK'19~\cite{aoki2019pointnetlk}       & 13.75  & 3.893      & 0.01990 & 0.00445 \\
DCP-V2'19~\cite{Wang_2019_ICCV}     & 1.090  & 0.752     & 0.00172 & 0.00117 \\
PRNET'19~\cite{wang2019prnet}      & 1.722  & 0.665  & 0.00637 & 0.00465 \\
R-PointHop'22~\cite{r_pointhop} & 0.340  & 0.240      & \underline{0.00037} & 0.00029 \\ 
VRNet'22~\cite{zhang2022vrnet}     & \underline{0.091} & \underline{0.012} & \textbf{0.00029} & \textbf{0.00005} \\
\hline
\textbf{Ours} & \textbf{0.082} & \textbf{0.011} & 0.00047 & \underline{0.00008} \\
\hline 

\hline 

\hline
\end{tabular}
\label{tab.all.clean.data}
\end{center}
\end{table}
\normalsize

Table~\ref{tab.all.clean.data} provides the results. It can be highlighted that our method outperforms the other methods in rotation with an improvement of a dozen per cent for the RMSE and MAE \emph{versus} the second best method VRNet~\cite{zhang2022vrnet}. However, VRNet remains the best in translation although the difference is slight compared to RoCNet, specifically for MAE($\bold{t}$) where RoCNet is second. From a purely numerical point of view, RoCNet provides an RMSE($\bold{R}$) of 0.082$^\circ$ (respectively, an MAE($\bold{R}$) of 0.011$^\circ$) (mean of the three Euler angle rotations) and an RMSE($\bold{t}$) of $47\times10^{-5}$~m (respectively, an MAE($\bold{t}$) of $8\times10^{-5}$~m) (mean of the three translations along $x$, $y$, and $z$ axes). 
%
% -----------
\subsubsection{Model trained on all classes with noisy data}
%

Table~\ref{tab.all.noisy.data} depicts the results of RoCNet under the same conditions as the first assessment but with adding Gaussian noise to the initial data. RoCNet achieves the best performance on three of the four metrics with an improvement of $25\%$, $45\%$ and $56\%$ for RMSE($\bold{R}$), MAE($\bold{R}$) and RMSE($\bold{t}$) respectively, and ranked second in MAE($\bold{t}$) behind R-PointHop. From a numerical point of view, RoCNet provides an RMSE($\bold{R}$) of 1.92$^\circ$ (respectively, an MAE($\bold{R}$) of 0.55$^\circ$) and an RMSE($\bold{t}$) of $2.6\times10^{-3}$~m (respectively, an MAE($\bold{t}$) of $1.8\times10^{-3}$~m). 

\small
\begin{table}[!h]
%\setlength\tabcolsep{5pt}
%\setlength\extrarowheight{.5pt}
\caption{Performances of RoCNet when the model is trained in all the classes with noisy data and without occlusions.}
\begin{center}
\begin{tabular}{l | l  l | l l}
\hline 

\hline 

\hline

Method & RMSE($\bold{R}$) & MAE($\bold{R}$) & RMSE($\bold{t}$) & MAE($\bold{t}$)  \\ 
\hline%\hline
ICP'92~\cite{besl1992method}  & 11.971         & 4.497         & 0.04832       & 0.00433 \\
%FGR'16~\cite{zhou2016fast}  & 18.359         & 6.367         & 0.03910       & 0.01448 \\
PTLK'19~\cite{aoki2019pointnetlk}       & 15.692         & 3.992         & 0.02395       & 0.00563 \\
DCP-V2'19~\cite{Wang_2019_ICCV}      & 08.417         & 5.685         & 0.03183       & 0.02337 \\
PRNET'19~\cite{wang2019prnet}      & 03.218         & 1.446         & 0.11178       & 0.00837 \\
R-PointHop'22~\cite{r_pointhop} & 02.780         & 0.980         & 0.01400       & \textbf{0.00080} \\
VRNet'22~\cite{zhang2022vrnet}  & \underline{02.558} & \underline{1.016} & \underline{0.00570} & 0.00289 \\

\hline 

\textbf{Ours} & \textbf{01.920}  & \textbf{0.555} & \textbf{0.00260} & \underline{0.00180} \\
\hline

\hline 

\hline
\end{tabular}
\label{tab.all.noisy.data}
\end{center}
\end{table}
\normalsize
%
% -----------
\subsubsection{Model trained on half classes with clean data}
The third configuration consists of the evaluation of the generalisation capacity of RoCNet and the related methods when the models are trained on only half the data (i.e., 20 classes), and tested on the 20 remaining classes. The obtained performances are reported in Table~\ref{tab.half.clean.data}. RoCNet outperforms the state-of-the-art method in one metric which is the MAE($\bold{R}$) and ranked second for both RMSE($\bold{R}$) and MAE($\bold{t}$). Overall, the performance of our method is similar to that of VRNet and R-PointHop. 
%
\small
\begin{table}[!h]
\caption{Performances of RoCNet when the model is trained in half the classes with clean data and without occlusions.}
\begin{center}
\begin{tabular}{l | l  l | l l}
\hline 

\hline 

\hline
Method & RMSE($\bold{R}$) & MAE($\bold{R}$) & RMSE($\bold{t}$) & MAE($\bold{t}$)  \\ 
\hline
ICP'92~\cite{besl1992method} & 12.707 & 5.075 & 0.04853 & 0.00235 \\
%FGR'16~\cite{zhou2016fast}  & 21.323 & 8.077 & 0.04577 & 0.00180 \\
PTLK'19~\cite{aoki2019pointnetlk} & 15.901 & 4.032 & 0.02611 & 0.00621 \\
DCP-V2'19~\cite{Wang_2019_ICCV} & 3.256 & 2.102 & 0.00631 & 0.00462 \\
PRNET'19~\cite{wang2019prnet} & 3.060 & 1.326 & 0.01009 & 0.00759 \\
R-PointHop'22~\cite{r_pointhop} & 0.340 & 0.240 & \textbf{0.00040} & 0.00030 \\
VRNet'22~\cite{zhang2022vrnet} & \textbf{0.209} & \underline{0.028} & \underline{0.00078} & \textbf{0.00009} \\

\hline
\textbf{Ours} & \underline{0.235}  & \textbf{0.026} & 0.00180 & \underline{0.00020} \\
\hline 

\hline 

\hline
\end{tabular}
\label{tab.half.clean.data}
\end{center}
\end{table}
%
\normalsize
% BIG FIGURE  ----------------------
\begin{figure*}
\centerline{\includegraphics[width=2\columnwidth]{set_objets}}
\caption{Illustration of some examples of performed registrations using RoCNet in case of clean data and without occlusions \textit{versus} the ground truth registrations.}
\label{fig.examples}
\end{figure*}


% -----------
\subsubsection{Model trained on all the classes with clean data and under partial occlusions} 
In this assessment, we evaluate the behaviour of our method and the other methods when only a part is shared by the two point clouds to be aligned. This simulates, for example, partial occlusions. From Table~\ref{tab.partial.clean.data}, it can be highlighted that RoCNet outperforms significantly the others methods in all the metrics. Overall, our method reduces the registration error by roughly half in comparison with the second-ranked methods, i.e., VRNet and R-PointHop.    
%
\small
\begin{table}[!h]
\caption{Performances of RoCNet when the model is trained in all the classes with clean data and partial occlusions.}

\begin{center}
\begin{tabular}{l | l  l | l l}
\hline 

\hline 

\hline
Method & RMSE($\bold{R}$) & MAE($\bold{R}$) & RMSE($\bold{t}$) & MAE($\bold{t}$)  \\ 
\hline
ICP'92~\cite{besl1992method}         & 33.683  & 25.045 & 0.293 & 0.2500 \\
%FGR'16~\cite{zhou2016fast}          & 11.238  & 02.832 & 0.030 & 0.0080 \\
PTLK'19~\cite{aoki2019pointnetlk}         & 16.735  & 07.550 & 0.045 & 0.0250 \\
DCP-V2'19~\cite{Wang_2019_ICCV}        & 06.709  & 04.448 & 0.027 & 0.0200 \\
PRNET'19~\cite{wang2019prnet}        & 03.199  & 01.454 & 0.016 & 0.0100 \\
R-PointHop'22~\cite{r_pointhop} & 01.660   & \underline{00.350} & 0.014 & \underline{0.0008} \\
VRNet'22~\cite{zhang2022vrnet}         & \underline{00.982}  & 00.496 & \underline{0.006} & 0.0039 \\
WsDesc'22~\cite{li2022wsdesc}        & 01.187  & 00.975     & 0.008 & 0.0070 \\


\hline
\textbf{Ours} & \textbf{00.412}  & \textbf{00.133} & \textbf{0.002} & \textbf{0.0002} \\
\hline 

\hline 

\hline
\end{tabular}
\label{tab.partial.clean.data}
\end{center}
\end{table}
\normalsize
%
% -----------
\subsubsection{Model trained on all the classes with noisy data and under partial occlusions} 
%
The last configuration concerns the evaluation of the proposed method under partial occlusions (partial overlap) using noisy data. As can be seen in Table~\ref{tab.partial.noisy.data}, RoCNet outperforms the other methods on all metrics, both in rotation and translation. RoCNet allows significant enhancement of the registration error, ranging from two-thirds to one-quarter compared to the method ranked second, i.e., WsDesc and even more in comparison to VRNet. This can be explained by the robustness of RoCNet to partial occlusions or noise or both at the same time. 
%
\small
\begin{table}[!h]
\caption{Performances of RoCNet when the model is trained in all the classes with noisy data and partial occlusions.}
\begin{center}
\begin{tabular}{l | l  l | l l}
\hline 

\hline 

\hline
Method & RMSE($\bold{R}$) & MAE($\bold{R}$) & RMSE($\bold{t}$) & MAE($\bold{t}$)  \\ 
\hline
ICP'92~\cite{besl1992method} & 33.067 & 25.564 & 0.294 & 0.250 \\
%FGR'16~\cite{zhou2016fast}  & 27.653 & 13.794 & 0.070 & 0.039 \\
PTLK'19~\cite{aoki2019pointnetlk}  & 19.939 & 9.076 & 0.057 & 0.032 \\
DCP-V2'19~\cite{Wang_2019_ICCV}  & 06.883 & 4.534 & 0.028 & 0.021 \\
PRNET'19~\cite{wang2019prnet} & 04.323  & 2.051 & 0.017 & 0.012 \\
VRNet'22~\cite{zhang2022vrnet} & 03.615 & 1.637 & 0.010 & 0.006 \\
WsDesc'22~\cite{li2022wsdesc} & \underline{03.500}  & \underline{0.759} & \underline{0.006} & \underline{0.004} \\
\hline
\textbf{Ours} & \textbf{01.810}  & \textbf{0.620} & \textbf{0.004} & \textbf{0.003} \\
\hline 

\hline 

\hline
\end{tabular}
\label{tab.partial.noisy.data}
\end{center}
\end{table}
\normalsize

Finally, the performance of RoCNet and its ranking in the context of wider state-of-the-art, including the eleven best methods, can be seen in Fig.~\ref{fig.SoA}. It can be highlighted that our method outperforms all the methods when considering simultaneously performances in rotation and translation, this both on clean data (Fig.~\ref{fig.SoA}(a)) and on noisy data (Fig.~\ref{fig.SoA}(b)). 
%
\begin{figure}[!h]
\includegraphics[width=\columnwidth]{SoA}
\caption{Comparison of RoCNet performances against the eleven most relevant state-of-the-art methods. (a) in the case of clean data and (b) in the case of data with Gaussian noise, both with partial overlap.}
\label{fig.SoA}
\end{figure}

Figure~\ref{fig.examples} depicts some examples of aligned points clouds (objects) available in the ModelNet40 dataset. The first row shows the initial positions of the points cloud $\boldsymbol{X}$ and $\boldsymbol{Y}$ to be aligned, the second row shows the performed registrations and the third shows the ground truth ones. 
%

Furthermore, to be able to assess visually the robustness ability of the proposed method, we performed different registrations by progressively decreasing (from 95$\%$ to 50$\%$) the rate of shared points between $\boldsymbol{X}$ and $\boldsymbol{Y}$. Figure~\ref{fig.rate} depicts the obtained results of one object. As can be seen, RoCNet can register point clouds even with only 50$\%$ of the data without much difficulty. On the other hand, the method shows its limits for objects with perfect symmetry when the overlap is low. 
%

\begin{figure}[!h]
\centerline{\includegraphics[width=\columnwidth]{set_partials}}
\caption{Illustration of the robustness of RoCNet against partial occlusions (i.e., partial overall between the point clouds to be aligned).}
\label{fig.rate}
\end{figure}

%
% -----------------
\section{ABLATION STUDY}\label{sec.ablation}
% -----------------
%
We conduct ablation studies on the three main blocks of the proposed architecture, i.e., the descriptor, the transformer, and the RANSAC-based estimation of the transformation.
%
% ---------------
\subsection{Ablation of Descriptor}
%
To study the impact of our descriptor which uses both normals and DGCNN as inputs on the transformer, we compared the performance of our architecture to that of MDGAT~\cite{mdgat} in case of point matching problem. For a proper comparison, both methods are trained in the same configuration and with the same number of epochs. Two types of data are used: 1) clean data and 2) noisy data, both with partial overlap. To achieve the comparison, we use the following metrics: Precision ($\mathbf{P}$), Accuracy ($\mathbf{A}$), Recall ($\mathbf{R}$) and F1-score ($\mathbf{F1}$).  Table~\ref{matching_compare} gives an insight into the ablation study of the descriptor. It can be underlined that our descriptor outperforms MDGAT one except for the Precision ($\mathbf{P}$) in the case of clean data with partial overlap. The difference is substantial, in favour of our descriptor when it concerns noisy data with partial overlap. 
%
\begin{table}
\centering
\caption{Performances assessment in a matching challenge with partial overlap matching (\textbf{P}: Precision, \textbf{A}: Accuracy, \textbf{R}: Recall, and \textbf{F1}: F1-score). }

\begin{tabular}{l|cccc|cccc}
\hline 

\hline 

\hline

  & \multicolumn{4}{c|}{clean data} & \multicolumn{4}{c}{noisy data} \\ \cline{2-9} 
\multirow{-2}{*}{{ Method}} & \textbf{P} & \textbf{A} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{A} & \textbf{R} & \textbf{F1} \\
\hline
\multicolumn{1}{c|}{MDGAT}  & \textbf{98.1} & 93.7 & 93.4 & 95.7 & 85.7 & 68.5 & 75.3 & 80.2 \\
\multicolumn{1}{c|}{Ours} & 98.0 &\textbf{97.2} &\textbf{97.6} &\textbf{97.8} & \textbf{85.8} & \textbf{85.9} & \textbf{85.5} & \textbf{85.7} \\

\hline 

\hline 

\hline

\end{tabular}
\label{matching_compare}
\end{table}

%
\small
\begin{table*}%[!h]
%\setlength\tabcolsep{5pt}
\setlength\extrarowheight{1pt}
\centering
\caption{SVD \emph{versus} RANSAC for transformation estimation in case of full overlap, noisy data and partial overlap.}

\begin{tabular}{l|ccc|ccc|ccc|ccc}
\hline 

\hline 

\hline
{ } & \multicolumn{3}{c|}{{ RMSE($\bold{R}$)}} & \multicolumn{3}{c|}{{ MAE($\bold{R}$)}} & \multicolumn{3}{c|}{{ RMSE($\bold{t}$)}} & \multicolumn{3}{c}{{ MAE($\bold{t}$)}} \\ \cline{2-13} 
\multirow{-2}{*}{{Method}} & {full} & {noisy} & {partial} & {full overlap} & {noisy} & {partial} & {full } & {noisy} & {partial} & {full } & {noisy} & {partial} \\
\hline
{SVD} & 3.94 & 1.94 & 4.21 & 0.124 & \textbf{0.484} & 0.168 & \textbf{0.0005} & \textbf{0.0017} & \textbf{0.001} & \textbf{0.0001} & \textbf{0.0018} & \textbf{0.0001} \\
{RANSAC} & \textbf{0.06} & \textbf{1.92} & \textbf{0.40} & \textbf{0.010} & 0.555 & \textbf{0.133} & \textbf{0.0005} & 0.0026 & 0.002 & \textbf{0.0001} & \textbf{0.0018} & 0.0002 \\
\hline 

\hline 

\hline
\end{tabular}
\label{tab.abla.ransac}
\end{table*}
\normalsize
%
% --------------
\subsection{Ablation of DGCNN and Transformer} RoCNet is compared to other alternatives in which our descriptor and attention mechanism are changed to those proposed in~\cite{mdgat}. In view of evaluating the contribution of the proposed attention mechanism, another architecture of the DGCNN is associated with a classical mechanism without the normals gradient embedding~\cite{sarlin20superglue}.  Table~\ref{tab.abla.dgcnn} reports the obtained results showing that RoCNet architecture is more relevant on three of the four used metrics emphasizing a significant contribution (about $10\%$) of the association of DGCNN and normals compared to a classical attention mechanism.
%
\small
\begin{table} 
\caption{Performances assessment in a matching challenge with clean data and partial overlap.}
\begin{center}
\begin{tabular}{l| l |c c c c}
\hline 

\hline 

\hline
\textbf{Descriptor} & \textbf{{Attention}}& $\mathbf{P}$ & $\mathbf{A}$ & $\mathbf{R}$ & $\mathbf{F1}$ \\
\hline
MDGAT’s descriptor & \textbf{Ours} & 92.1 & 84.6 & 84.8 & 88.3 \\

\textbf{Ours} & MDGAT & 96.6 & \underline{96.3} & \underline{96.3} & 95.4\\
\textbf{Ours} & \textbf{Ours} w/o normals  & 79.8 & 80.2 & 80.5 & 80.1\\
\hline
 MDGAT’s descriptor & MDGAT & \textbf{98.1} & 93.7 & 93.4 &  \underline{95.7}  \\
\hline 
\textbf{Ours} & \textbf{Ours} & \underline{98.0} & \textbf{97.2 }& \textbf{97.6} & \textbf{97.8}  \\
\hline 

\hline 

\hline

\end{tabular}

\label{tab.abla.dgcnn}
\end{center}
\end{table}
\normalsize
%
% ----------
\subsection{Ablation of RANSAC} 
The last ablation study consists of the comparison of the contribution of an SVD \emph{versus} RANSAC to estimate the rigid transformation when the matching is performed. Table~\ref{tab.abla.ransac} reports the performances of each alternative using: 1) clean data with full overlap (full), 2) noisy data with full overlap (noisy) and 3) clean data with partial overlap (partial). It can be seen that RANSAC approach  outperforms slightly the SVD one. 
%
% ----------------
\section{CONCLUSION}
% ---------------
%
This paper presented a new 3D point cloud registration and pose estimation using a deep learning architecture. The proposed architecture is composed of three main blocks: 1) the newly designed descriptor which encodes the neighbourhood of each point and an attention mechanism that encodes the variations of the surface normals, 2) the matching method that estimates a matrix of correspondences using the Sinkhorn algorithm, and 3) the estimation of the rigid transformation using a RANSAC applied to the $K^c$ best matches from the correspondence matrix. The proposed architecture was evaluated using the ModelNet40 dataset in different favourable and unfavourable configurations. It has been demonstrated that our method outperformed the related state-of-the-art algorithms, especially in unfavourable conditions, e.g., with noisy data and partial occlusions. 

In the future, we intend to extend this work to a new approach where the descriptor will be expressed in the frequency range. This will certainly improve the accuracy of our architecture, but also its robustness to noise and partial occlusions. 

% ----- BIBLIOGRAPHY
\begin{thebibliography}{10}

\bibitem{chen20203d}
S.~Chen, B.~Liu, C.~Feng, {\em et~al.}, ``3d point cloud processing and
  learning for autonomous driving: Impacting map creation, localization, and
  perception,'' {\em IEEE Sig. Process. Mag.}, vol.~38, pp.~68--86, 2020.

\bibitem{mdgat}
C.~Shi, X.~Chen, K.~Huang, {\em et~al.}, ``Keypoint matching for point cloud
  registration using multiplex dynamic graph attention networks,'' {\em IEEE
  Rob. and Auto. Let.}, vol.~6, pp.~8221--8228, 2021.

\bibitem{besl1992method}
P.~J. Besl and N.~D. McKay, ``Method for registration of 3-d shapes,'' in {\em
  Sensor fusion IV: control paradigms and data structures}, vol.~1611,
  pp.~586--606, 1992.

\bibitem{wang2019prnet}
Y.~Wang and J.~M. Solomon, ``Prnet: Self-supervised learning for
  partial-to-partial registration,'' {\em Adv. Neural. Inf. Process. Syst.},
  vol.~32, 2019.

\bibitem{Wang_2019_ICCV}
Y.~Wang and J.~M. Solomon, ``Deep closest point: Learning representations for
  point cloud registration,'' in {\em IEEE Int. Conf. on Comput. Vision}, 2019.

\bibitem{aoki2019pointnetlk}
Y.~Aoki, H.~Goforth, R.~A. Srivatsan, {\em et~al.}, ``Pointnetlk: Robust \&
  efficient point cloud registration using pointnet,'' in {\em IEEE/CVF Conf.
  Comput. Vision Pattern Recognit.}, pp.~7163--7172, 2019.

\bibitem{dgcnn}
Y.~Wang, Y.~Sun, {\em et~al.}, ``Dynamic graph cnn for learning on point
  clouds,'' {\em ACM Trans. on Grap.}, 2019.

\bibitem{qi2017pointnet++}
C.~R. Qi, L.~Yi, H.~Su, {\em et~al.}, ``Pointnet++: Deep hierarchical feature
  learning on point sets in a metric space,'' {\em Adv. Neural. Inf. Process.
  Syst.}, vol.~30, 2017.

\bibitem{li2022wsdesc}
L.~Li, H.~Fu, and M.~Ovsjanikov, ``Wsdesc: Weakly supervised 3d local
  descriptor learning for point cloud registration,'' {\em IEEE Trans. Vis.
  Comput. Graph.}, 2022.

\bibitem{sinkhorn1967concerning}
R.~Sinkhorn and P.~Knopp, ``Concerning nonnegative matrices and doubly
  stochastic matrices,'' {\em Pacific J. of Math.}, vol.~21, pp.~343--348,
  1967.

\bibitem{soft_svd}
T.~Papadopoulo and M.~L. Lourakis, {\em Estimating the jacobian of the singular
  value decomposition: Theory and applications}.
\newblock PhD thesis, Inria, 2000.

\bibitem{rusu2009fast}
R.~B. Rusu, N.~Blodow, {\em et~al.}, ``Fast point feature histograms (fpfh) for
  3d registration,'' in {\em IEEE Int. Conf. on Rob. and Auto.},
  pp.~3212--3217, 2009.

\bibitem{qi2016pointnet}
C.~R. Qi, H.~Su, K.~Mo, and L.~J. Guibas, ``Pointnet: Deep learning on point
  sets for 3d classification and segmentation,'' {\em arXiv preprint
  arXiv:1612.00593}, 2016.

\bibitem{fischler1981random}
M.~A. Fischler and R.~C. Bolles, ``Random sample consensus: a paradigm for
  model fitting with applications to image analysis and automated
  cartography,'' {\em Communications of the ACM}, vol.~24, pp.~381--395, 1981.

\bibitem{el2021unsupervisedr}
M.~El~Banani, L.~Gao, and J.~Johnson, ``Unsupervisedr\&r: Unsupervised point
  cloud registration via differentiable rendering,'' in {\em IEEE/CVF Conf.
  Comput. Vision Pattern Recognit.}, pp.~7129--7139, 2021.

\bibitem{r_pointhop}
P.~Kadam, M.~Zhang, S.~Liu, {\em et~al.}, ``R-pointhop: A green, accurate, and
  unsupervised point cloud registration method,'' {\em IEEE Trans. on Ima.
  Process.}, vol.~31, pp.~2710--2725, 2022.

\bibitem{zhou2016fast}
Q.~Zhou, J.~Park, {\em et~al.}, ``Fast global registration,'' in {\em Eur.
  Conf. Comput. Vis.}, vol.~9906, pp.~694--711, 2016.

\bibitem{huang2021predator}
S.~Huang, Z.~Gojcic, M.~Usvyatsov, {\em et~al.}, ``Predator: Registration of 3d
  point clouds with low overlap,'' in {\em IEEE/CVF Conf. Comput. Vision
  Pattern Recognit.}, pp.~4267--4276, 2021.

\bibitem{yew2018-3dfeatnet}
Z.~J. Yew and G.~H. Lee, ``3dfeat-net: Weakly supervised local 3d features for
  point cloud registration,'' in {\em Euro. Confe. on Comput. Vision}, 2018.

\bibitem{triplet_loss}
M.~Khoury, Q.-Y. Zhou, {\em et~al.}, ``Learning compact geometric features,''
  in {\em IEEE Int. Conf. Comput. Vision}, pp.~153--161, 2017.

\bibitem{bai2020d3feat}
X.~Bai, Z.~Luo, L.~Zhou, {\em et~al.}, ``D3feat: Joint learning of dense
  detection and description of 3d local features,'' in {\em IEEE/CVF Conf.
  Comput. Vision Pattern Recognit.}, pp.~6359--6367, 2020.

\bibitem{roufosse2019unsupervised}
J.-M. Roufosse, Sharma, {\em et~al.}, ``Unsupervised deep learning for
  structured shape matching,'' in {\em IEEE/CVF Int. Conf. Comput. Vision},
  pp.~1617--1627, 2019.

\bibitem{ovsjanikov2012functional}
M.~Ovsjanikov, M.~Ben-Chen, J.~Solomon, A.~Butscher, and L.~Guibas,
  ``Functional maps: a flexible representation of maps between shapes,'' {\em
  ACM Trans. on Grap.}, vol.~31, pp.~1--11, 2012.

\bibitem{geotransformer}
Z.~Qin, H.~Yu, C.~Wang, {\em et~al.}, ``Geometric transformer for fast and
  robust point cloud registration,'' in {\em IEEE/CV Conf. Comput. Vision
  Pattern Recognit.}, pp.~11143--11152, 2022.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, {\em et~al.}, ``Attention is all you need,'' {\em Adv.
  Neural. Inf. Process. Syst.}, vol.~30, 2017.

\bibitem{sarlin20superglue}
P.-E. Sarlin, D.~DeTone, {\em et~al.}, ``{SuperGlue}: Learning feature matching
  with graph neural networks,'' in {\em Conf. Comput. Vision Pattern
  Recognit.}, 2020.

\bibitem{zhao2021point}
H.~Zhao, L.~Jiang, {\em et~al.}, ``Point transformer,'' in {\em IEEE/CVF Int.
  Conf. Comput. Vision}, pp.~259--268, 2021.

\bibitem{zhang2022vrnet}
Z.~Zhang, J.~Sun, Y.~Dai, {\em et~al.}, ``Vrnet: Learning the rectified virtual
  corresponding points for 3d point cloud registration,'' {\em IEEE Trans. on
  Cir. and Sys. for Video Tech.}, vol.~32, pp.~4997--5010, 2022.

\end{thebibliography}



\end{document}





