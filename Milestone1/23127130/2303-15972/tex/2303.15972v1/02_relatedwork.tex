% \section{Related Work}
% \label{sec:relatedwork}
%  Our approach to multi-robot shared autonomy shares motivation with existing works involving human and robot multi-agent systems. Our method draws heavily on concepts in Learning from Demonstration, time warping, and shared autonomy. To contextualize our work, we first provide a brief review of relevant work in supervisor allocation and multi-agent scheduling. We conclude with a brief review of the motivating methods in LfD and shared autonomy.

% \subsection{Supervisor Allocation}
% There are a variety of algorithms that determine when to query a supervisor for assistance. In multi-robot teaming, many decision support systems (DDS) have been proposed to help operators provide effective assistance to robots \cite{gao2012teamwork}. For example, Crandall et al. \cite{crandall2010computing} devise a utility-maximizing operator allocation and interface that guides operator attention during oversight of unmanned vehicles. In interactive robot learning, many of the supervision algorithms are \emph{robot-gated} where a robotic agent filters requests for assistance through metrics based on confidence, safety, criticality, and context switching \cite{zhang2016query,menda2019ensembledagger,hoque2021lazydagger,hoque2021thriftydagger}.

% Many existing works in multi-agent systems focus on determining an operator allocation policy for temporary teleoperation of robots that could benefit from assistance (e.g., to improve performance or recover from a fault state). Rosenfield et al. \cite{rosenfeld2017intelligent} model the multi-agent setting as a Markov Decision Process (MDP) and solve for short-term operator allocation using a heuristic. Dahiya et al. \cite{dahiya2022scalable} formulate the allocation policy by decomposing the full MDP into multiple single-agent policies using a restless multi-arm bandit formulation. Swamy et al. \cite{swamy2020scaled} determine the desired allocation policy by studying operator supervision decision-making across a small number of robots and extrapolating to larger fleets. Fleet-DAgger \cite{hoque2022fleet} combines allocation and interactive learning where the fleet improves its policy from the operator's teleoperation and the allocation policy is calculated based on an agent priority function. 

% Other works allocate collocated supervisor assistance across multiple agents. Ji et al. \cite{ji2022traversing} formulate the physical supervision and assistance problem for a fleet of deployed robots as a dynamic graph traversal problem. In oracular reinforcement learning, collocated robots determine when to query available human resources in navigation tasks based on cost in a partially-observable Markov decision process (POMDP) \cite{rosenthal2011modeling}.

% \subsection{Multi-Agent Scheduling}
% Methods in multi-robot scheduling generally seek to optimize the assignment of sub-tasks to agents while respecting a set of scheduling constraints (e.g., pre-conditions) \cite{yan2013survey}. Many methods formulate the scheduling problem as a mixed-integer linear program (MILP) \cite{chaudhry2016research}. For example, Gombolay et al. \cite{gombolay2013fast} present a method for scheduling human and robot teams based on temporal deadlines and spatial restrictions between agents.

% Other methods use multi-agent scheduling to aid in the supervision of multiple robots. Cai et al. \cite{cai2022scheduling} devise an operator teleoperation schedule for multiple agents by identifying and compensating for blocking-type subtasks. Ramchurn et al \cite{ramchurn2015study} propose a supervisory control interface to assign agent task allocations to multiple unmanned aerial vehicles (UAVs) in dynamic environments based on suggestions from a max-sum algorithm. Most similar to our method is the work of Zanlongo et al. \cite{zanlongo2021scheduling} that solves the supervised multi-agent scheduling problem for robots with deterministic, known critical states as a geometric path planning problem.

% \subsection{Motivating Work in LfD and Shared Autonomy}
% In Learning from Demonstration (LfD), an agent learns a policy through examples provided from a teacher \cite{argall2009survey}. LfD methods have grown steadily in the robotics literature in the past fifteen years and seen application in many areas, from manufacturing to healthcare \cite{ravichandar2020recent}. Many methods learn behaviors from multiple demonstrations which often requires demonstration alignment. Demonstration data can be aligned through techniques such as dynamic time warping \cite{senin2008dynamic} and keyframe alignment \cite{akgun2012keyframe}.

% LfD has seen limited use in multi-agent systems where the focus is generally on learning multi-agent policies from group demonstrations. Le et al. \cite{le2017coordinated} propose a method for coordinated multi-agent imitation learning based on learning a latent coordination model alongside individual agent policies. Tung et al. \cite{tung2021learning} created \emph{multi-arm roboturk} which allows multiple users to simultaneously teleoperate arms to collect and learn from multi-arm tasks. Other methods use inverse reinforcement learning to infer the policies for the multi-agent systems from demonstrations \cite{bogert2014multi}.

% In shared autonomy, humans and robots work together to complete tasks \cite{selvaggio2021autonomy}. Shared autonomy methods typically adjust the robot's level of automation automatically and arbitrate assistance to the user based on task knowledge or intent \cite{losey2018review}. Many methods in shared autonomy infer a belief over user goals during teleoperation and provide assistance based on confidence \cite{dragan2013policy,hauser2013recognition}. Jeon et al. \cite{jeon2020shared} couple goal prediction with latent space control to ease the input for the user. Reddy et al. \cite{reddy2018shared} propose a model-free deep reinforcement learning method for shared autonomy assistance that doesn't require explicit specification of the user's goals.

% Other methods in shared autonomy operate at a higher level of autonomy \cite{parasuraman2000model} and allow operators to provide differential input to augment the robot's current policy \cite{nemec2018efficient,masone2014semi,losey2017trajectory,bajcsy2017learning}. For example, the method in Cognetti et al. \cite{cognetti2020perception} allows operators to provide canonical path transformations to affect the trajectories of mobile robots. Similar to goal-oriented shared autonomy methods, differential methods also leverage prior task knowledge and latent space controls for the differential input. For example, Hagenow et al. \cite{hagenow2021corrective}\cite{hagenow2021informing} propose methods where the types of differential corrections an operator can provide are inferred from variability in expert demonstrations. In this work, we extend shared autonomy methods for differential control to the multi-agent setting.
