\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}



\title{Comparing Machine Learning Methods for Estimating Heterogeneous Treatment Effects by Combining Data from Multiple Randomized Controlled Trials}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{
 Carly Lupton Brantner \\
  Department of Biostatistics\\
  Johns Hopkins Bloomberg School of Public Health\\
  \texttt{clupton1@jhu.edu} \\
  %% examples of more authors
   \And
 Trang Quynh Nguyen \\
 Department of Mental Health\\
 Johns Hopkins Bloomberg School of Public Health
  \And
  Tengjie Tang \\
  Department of Statistical Science\\
  Duke University
  \And
  Congwen Zhao \\
  Department of Biostatistics and Bioinformatics\\
  Duke University
  \And
  Hwanhee Hong\\
  Department of Biostatistics and Bioinformatics\\
  Duke University
  \And
  Elizabeth A. Stuart \\
  Department of Biostatistics\\
  Johns Hopkins Bloomberg School of Public Health
 \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	Individualized treatment decisions can improve health outcomes, but using data to make these decisions in a reliable, precise, and generalizable way is challenging with a single dataset. Leveraging multiple randomized controlled trials allows for the combination of datasets with unconfounded treatment assignment to improve the power to estimate heterogeneous treatment effects. This paper discusses several non-parametric approaches for estimating heterogeneous treatment effects using data from multiple trials. We extend single-study methods to a scenario with multiple trials and explore their performance through a simulation study, with data generation scenarios that have differing levels of cross-trial heterogeneity. The simulations demonstrate that methods that directly allow for heterogeneity of the treatment effect across trials perform better than methods that do not, and that the choice of single-study method matters based on the functional form of the treatment effect. Finally, we discuss which methods perform well in each setting and then apply them to four randomized controlled trials to examine effect heterogeneity of treatments for major depressive disorder.
\end{abstract}

\keywords{treatment effect heterogeneity, combining data, personalized medicine, machine learning}

\maketitle

\section{Introduction}

When tailoring treatment regimens to individual patients, one must strive to understand how different treatment options might affect the specific patient based on their characteristics or context. Rather than using a one-size-fits-all approach, clinicians and researchers are turning more towards personalized medicine with the goal of improving clinical outcomes. In this setting, the focus of estimation becomes conditional average treatment effects, i.e., how well the treatment is expected to work conditional on the person's known characteristics.

The benchmark for estimating treatment effects in an unbiased manner is most often a randomized controlled trial (RCT). In an RCT, participants are randomly assigned to treatment or control, therefore ensuring unconfounded treatment assignment and unbiased treatment effect estimates in the given sample. However, these trials often have sample sizes that are large enough to detect main effects but lack power to estimate heterogeneous treatment effects \citep{enderlein_fleiss_1988}. To overcome these specific issues, researchers have started combining information from multiple studies to improve treatment effect estimation. Multiple studies allow for larger sample sizes and at times a more representative sample of the target population. In the setting with multiple RCTs, meta-analysis or hierarchical models are common techniques to combine studies and estimate treatment effects \citep{debray_get_2015, seo_comparing_2021}. These approaches often do not explicitly target conditional average treatment effects though, and often only use aggregate-level data which makes it challenging to estimate treatment effects conditional on individual-level characteristics. Furthermore, meta-analysis is commonly applied within a parametric framework, which is highly interpretable but requires prespecification of effect moderators and distributional assumptions for parameters. Non-parametric approaches are worth exploring in this setting because they allow for high levels of flexibility in outcome and treatment effect functions. Relationships between covariates and treatment effect can be complex and non-linear in reality, and non-parametric machine learning methods can better handle those scenarios.

Many non-parametric approaches exist to estimate heterogeneous treatment effects \citep{kunzel2019metalearners, athey_generalized_2019, green_modeling_2012, kennedy_optimal_2020, nie_quasi-oracle_2021, dandl2022makes}; however, these approaches have generally been developed only for the single-study setting. Several of the common approaches are discussed in the section to follow (\ref{SSL}), and we subsequently extend these methods for use in multiple studies. Recent research has investigated a few non-parametric approaches for the multiple study setting, mostly geared towards combining data from one RCT with a large observational dataset \citep{yang_elastic_2020,yang_improved_2022,kallus_removing_2018,rosenman_combining_2020}. In that work, the focus is often on estimating the bias present in the observational data to determine the level at which the observational study estimates can be combined with the RCT estimates. These methods are therefore not as straightforward to use in the multiple RCT setting. With multiple RCTs, each individual trial has the benefit of unconfounded treatment assignment, but significant cross-trial heterogeneity could still exist due to both observed and unobserved factors. The focus in this case is no longer de-biasing one of the datasets, but instead determining the amount of heterogeneity present and how to account for it. 

Brantner and colleagues \cite{brantner2023review} wrote a comprehensive review of methods geared towards combining datasets to estimate treatment effect heterogeneity. That review included approaches for multiple RCTs; the most common were individual participant-level data one-stage meta-analyses \citep{debray_get_2015}. One alternative approach focuses on combining RCTs to estimate conditional average treatment effects in a non-parametric framework \citep{tan_tree-based_2021}. That work by Tan and colleagues was done in the federated learning setting, in which individual-level data could not be shared across study sites and instead only aggregate results or models could be shared. In the sections to follow, we tailor Tan et al.'s method to when individual-level data can be shared across trials.

To our knowledge, this paper is the first to describe and compare machine learning options for estimating heterogeneous treatment effects using data from multiple RCTs, in the setting in which all data can be shared across trials. Because not many methods exist to do this, we demonstrate several options for extending current methods for single studies to the multiple-study setting. We also build off of Tan et al.'s \cite{tan_tree-based_2021} approach by adapting it to the case when individual-level data can be shared across trials. We conduct extensive simulations with varying data generation parameters to determine which of the single-study and aggregation methods perform best depending on different amounts of cross-trial heterogeneity in the effects. We then apply the approaches to a set of four RCTs of depression treatments and discuss the variability in estimates across the approaches and potential substantive conclusions that can be made.

\section{Notation}

The estimand considered in this paper is the conditional average treatment effect (CATE), defined under Rubin's potential outcomes framework  \citep{rubin_estimating_1974}. Let $A$ denote a binary treatment indicator (often treatment versus control), $\boldsymbol{X}$ represent covariates, and $Y$ represent a continuous outcome. Under Rubin's framework, $Y(0)$ and $Y(1)$ denote the potential outcomes under control and treatment, respectively. Let $S$ be a categorical variable representing the trial in which the individual participated and ranging from $1$ to $K$, where $K$ is the total number of RCTs. Finally, represent the probability of receiving treatment given covariates and trial membership (propensity score) as $\pi_s(\boldsymbol{X}) = P(A=1|\boldsymbol{X},S=s)$.

With a continuous outcome, the CATE is
\begin{equation} \label{unicate}
    \tau(\boldsymbol{X}) = E(Y(1)|\boldsymbol{X}) - E(Y(0)|\boldsymbol{X}).
\end{equation} In this paper, we note that the goal estimand is this ``universal'' CATE (\ref{unicate}) built off of potential outcomes that are not dependent upon study membership. However, many methods in the following sections target a study-specific CATE:

\begin{equation} \label{studycate}
    \tau_s(\boldsymbol{X}) = E(Y(1)|\boldsymbol{X},S=s) - E(Y(0)|\boldsymbol{X},S=s).
\end{equation} 

To identify the estimand when combining data across RCTs, many of the standard causal inference assumptions are required, including the Stable Unit Treatment Value Assumption (SUTVA) within each RCT. Other standard assumptions include: unconfoundedness (Assumption \ref{asx1}), consistency (Assumption \ref{asx2}) and positivity (Assumptions \ref{asx3} and \ref{asx4}) \citep{tan_tree-based_2021}. Assumption \ref{asx2} varies slightly depending on the estimand; under the universal CATE estimand (Equation \ref{unicate}), we assume overall consistency, while under the study-specific estimand (Equation \ref{studycate}), we assume consistency within each study. Assumption \ref{asx4}, which requires that any $\boldsymbol{X}$ is possible to be observed in all studies, can be relaxed depending on the method.

\newcommand{\indep}{\perp \!\!\! \perp}
\newtheorem{assumption}{Assumption}

\begin{assumption} \label{asx1} 
$\{Y(0), Y(1)\} \indep A \mid \boldsymbol{X}, S=s$~~for all studies $s$.
\end{assumption}
\begin{assumption} \label{asx2} 
$Y = AY(1) + (1-A)Y(0)$~~almost surely (in each study).
\end{assumption}
\begin{assumption} \label{asx3} There exists a constant $c>0$ such that $c<\pi_s(\boldsymbol{x})<1-c$ for all studies $s$ and for all $\boldsymbol{x}$ values in each study.
\end{assumption}
\begin{assumption} \label{asx4} (\textit{Can be relaxed})
There exists a constant $d>0$ such that $d<P(S = s|\boldsymbol{X}=\boldsymbol{x})<1-d$ for all $\boldsymbol{x}$ and $s$.
\end{assumption}

%Finally, depending on the aggregation method used, the method either relies on the assumption that there is a universal $\tau$ function shared across all trials (Assumption 5\ref{asx5a}), or this function is allowed to be variable across trials (Assumption 5\ref{asx5b}). 
%\begin{assumption} For $j$ and $k$ representing two different trials where the CATE is expressed as in equation \ref{studycate}, either:

%\begin{enumerate}[a.]
%    \item \label{asx5a} $\tau_j(\boldsymbol{X})= \tau_k(\boldsymbol{X})$ OR
%    \item \label{asx5b} $\tau_j(\boldsymbol{X})\neq \tau_k(\boldsymbol{X})$
%\end{enumerate}
%\end{assumption}


\section{Methods}

This paper includes methods developed for treatment effect estimation in a single study and aggregation approaches that apply these methods to multiple studies. We chose some methods based off of those explored in Tan et al.'s \cite{tan_tree-based_2021} paper that have a similar goal to the present work. This section discusses three single-study methods and five aggregation options that apply the single-study methods to the multi-study setting.

%\begin{table}[h]
    %\centering
    %\begin{tabular}{|l|l|}
    %\hline
        %\textbf{Single-Study Method} & %\textbf{Aggregation Method} \\
        %\hline
        %S-Learner & Complete Pooling \\
        %& Pooling with Trial Indicator \\
       % & Ensemble Tree\\
      %  & Ensemble Forest\\
     %   & Ensemble Lasso\\
    %    \hline
   %     X-Learner & Complete Pooling \\
  %      & Pooling with Trial Indicator \\
 %       & Ensemble Tree\\
 %       & Ensemble Forest\\
%        & Ensemble Lasso\\
%        \hline
%        Causal Forest & Complete Pooling \\
%        & Pooling with Trial Indicator \\
%        & Ensemble Tree\\
%        & Ensemble Forest\\
%        & Ensemble Lasso\\
%        \hline
%         & Meta-Analysis\\
%         \hline
%    \end{tabular}
%    \caption{List of methods explored in this paper, broken down by single-study and aggregation approaches.}
%    \label{methods}
%\end{table}

\subsection{Single-Study Methods} 
\label{SSL}

For a given RCT, many machine learning methods have been developed for CATE estimation. The single-study methods that exist can be grouped into multiple categories, as delineated by Brantner et al (\citeyear{brantner2023review}). For ease of comparison, three approaches are included that are common in practice, user-friendly, and have been shown to be effective in previous literature: the S-learner, X-learner \citep{kunzel2019metalearners}, and causal forest \citep{athey_generalized_2019}. The first two approaches are multi-step procedures that involve first estimating the conditional outcome mean under treatment or control and then combining the two into one CATE function, while the causal forest involves tree-based partitioning of the covariate space by treatment effect. In this paper, we use random forests as the base learners for both the S-learner and the X-learner to best compare with the causal forest, which is inherently forest-based.

\subsubsection{S-Learner}

The first single-study machine learning method used in this paper is called the "S-learner" \citep{kunzel2019metalearners}. This method is classified as a "meta-learner" in that it combines base learners (i.e., regression models) of any form in a specific way \citep{kunzel2019metalearners}. The S-learner uses a base learner (i.e., a random forest) to estimate a conditional outcome mean function given observed covariates and assigned treatment: $$\mu(\boldsymbol{X}, A) = E(Y|\boldsymbol{X}, A).$$ The conditional outcome mean function in this approach is not specific to treatment group, but instead treatment is included together with the covariates as features to be used by the random forest. The CATE can then be directly estimated by plugging in $0$ and $1$ for the treatment indicator to obtain predicted outcomes under treatment and control for each individual and calculate  $$\hat{\tau}(\boldsymbol{X}) = \hat{\mu}(\boldsymbol{X},1)-\hat{\mu}(\boldsymbol{X},0).$$

\subsubsection{X-Learner}

The second approach considered here is another meta-learner called the "X-learner" \citep{kunzel2019metalearners}. The X-learner takes a similar approach as the S-learner by modeling the conditional outcome mean functions before estimating the CATE directly. However, rather than estimating one outcome mean function for $Y(1)$ and $Y(0)$ simultaneously, the X-learner estimates two functions separately and then imputes treatment effects for each treatment group.

Specifically, the X-learner involves three steps. First, the conditional outcome mean functions are estimated using base learners (in this case, random forests) like in the S-learner, but separately by treatment group: $$\mu_1(\boldsymbol{X}) = E(Y(1)|\boldsymbol{X})~~~\text{and}~~~\mu_0(\boldsymbol{X}) = E(Y(0)|\boldsymbol{X}).$$ Next, the unobserved potential outcomes for individuals in the treatment and control groups are predicted using those models: $$\Tilde{D}_{i: A=1} = Y_{i: A=1}-\hat{\mu}_0(\boldsymbol{X}_{i: A=1})~~~\text{and}~~~\Tilde{D}_{i: A=0} = \hat{\mu}_1(\boldsymbol{X}_{i: A=0})-Y_{i: A=0}.$$
Then $\Tilde{D}$ is regressed on $\boldsymbol{X}$ to estimate $\tau(\boldsymbol{X})$. This is done within each treatment group separately, resulting in two estimates, labeled $\hat{\tau}_1(\boldsymbol{X})$ and $\hat{\tau}_0(\boldsymbol{X})$. Finally, these are combined to obtain one CATE estimate: $$\hat{\tau}(\boldsymbol{X}) = g(\boldsymbol{X})\hat{\tau}_1(\boldsymbol{X}) + (1-g(\boldsymbol{X}))\hat{\tau}_0(\boldsymbol{X}),$$ where the weight $g(\boldsymbol{X})$ is often an estimate of the propensity score (the case in this paper) or can be chosen otherwise \citep{kunzel2019metalearners}. 

\subsubsection{Causal Forest} \label{cf}

The third single-study approach is the causal forest \citep{athey_generalized_2019}. The causal forest is similar to a random forest, but the focal estimand is the treatment effect itself, rather than the outcome for a given individual. The causal forest is based off of a causal tree, which involves recursive partitioning of the covariates to best split based on treatment effect heterogeneity. Here, the treatment effect is estimated as the difference in average outcomes between the treatment and control group individuals within leaves. From there, the causal forest is the weighted aggregation of many causal trees.

One potential challenge with causal forests is that bias could occur when there is overlap between the data used to form the trees and data used to estimate the treatment effects within leaves. A solution to that problem, called "honesty", has been proposed \citep{wager_estimation_2018} .This concept ensures that for every individual involved in creating the tree, their outcome is used either for splitting the tree or estimating the treatment effect within a leaf, but not both. Honesty has been used some in the literature, but there is not a widespread conclusion as to whether trees should be fit with or without honesty depending on the scenario. Dandl and colleagues compared honesty versus adaptive (not honest) forests in their simulations including causal forests and found that in their setting that was meant to represent an RCT, the adaptive forests performed better \citep{dandl2022makes}. Additionally, honesty requires large sample sizes. Thus, we do not include honesty in the causal forests in the primary simulations but do investigate it in a second round of method comparisons. 

\subsection{Aggregation Methods}

In many contexts, there are multiple RCTs available that compare the same two treatments. It is then worth considering methods that allow combining across trials. When aggregating to the multi-study level, the question becomes: how much does the treatment effect vary based on study membership? This variability can range along a continuum, where on one end is the possibility that the trials are all very homogeneous in terms of the CATE, meaning that participants in trial $j$ and in trial $k$ who have the same covariate values would have the same treatment effect. At the other extreme, individuals with the same covariates but in different trials could have completely different treatment effects. These differences can be due to heterogeneity in the sites in which the trials were conducted, heterogeneity in trial procedures (including the treatment or control conditions themselves), heterogeneity in trial samples, or other reasons. The aggregation methods to follow take different approaches to incorporating trial membership into the treatment effect estimation, ranging from assuming trial membership does not matter at all, to allowing it to matter just as much as any other characteristic.

\subsubsection{Complete Pooling}

A complete pooling approach is very straightforward: the researcher simply takes all data from each of the $K$ RCTs, creates a single dataset, and then fits one of the three previously described methods (S-learner, X-learner, or causal forest) to the pooled dataset. This approach is quick and easy to do, but requires many assumptions. Namely, this approach assumes a high level of homogeneity across trials and specifically that the CATE function is shared across studies. This method is included because it represents a naive comparison point and because it provides universal CATE estimates (i.e., not study-specific).

\subsubsection{Pooling with Trial Indicator}

An alternative pooling approach is to incorporate trial membership in the models but essentially still perform the pooling as before. Here, all of the individual data from each RCT is combined into one comprehensive dataset, but a categorical variable is included that represents the trial in which the individual participated. Then, the researcher can apply one of the single-study approaches to this full dataset, allowing for all of the covariates, including trial membership, to be involved in the treatment effect function. In this way, if trial membership is important for estimating effects, estimates should be somewhat informed by trial membership; otherwise, the treatment effect estimates should be similar across trials. While the previous complete pooling approach gives estimates that were not trial-specific, this approach yields trial-specific CATE estimates.

\subsubsection{Ensemble Approach}
The next approach is based off of Tan and colleagues' (\citeyear{tan_tree-based_2021}) methods for federated learning, originally developed for scenarios in which individual data cannot be shared across trial sites. Their original approach fits trial-specific models and then applies those models to data from a single coordinating site to derive an ensemble. We propose an adaptation of Tan's approach for settings where individual-level data from all trials are available to the analyst.

This adaptation of Tan et al.'s approach involves three steps. 
\begin{enumerate}
    \item First, the researcher builds localized models for the CATE within each trial, using one of the three single-study methods previously discussed (S-learner, X-learner, or causal forest).
    \item Next, they apply these localized models to each individual across all of the RCTs to get for each individual their \textit{trial-specific CATE estimates}, i.e., the predicted effects had the individual been part of study 1, study 2 and so on. For $K$ studies with a total of $N$ individuals in all studies combined, there will be $K$ trial-specific CATE models. Once each of these models are applied to all $N$ data points, every individual will have $K$ different estimates of their CATE. So there will ultimately be $N*K$ CATE estimates in what Tan et al. define as an "augmented" dataset. The difference between the second step here and what Tan et al. did is that we apply the study-specific models to all data points in all trials, rather than having to restrict to a single coordinating site.
    \item The third and final step is to fit an ensemble model to the augmented dataset that has CATE estimates for every individual crossed with every trial. In this ensemble model, the response variable is the CATE estimate, and the predictors are the individual covariates and a categorical variable indicating the local model that had been used to compute the CATE estimate. We use three different options for this final ensemble model fit to the augmented dataset: a regression tree, a random forest, and a lasso regression. The regression tree and random forest were explored in Tan et al.'s paper (\citeyear{tan_tree-based_2021}), while the lasso regression was added to provide a parametric comparison point.
\end{enumerate} 

The resulting functions from these ensemble approaches are trial-specific estimates of the CATE; however, they have been adapted based on the CATEs from the other trials. Therefore, this method allows for trial heterogeneity but incorporates information across trials to hopefully improve the model from each trial. 

\subsubsection{IPD Meta-Analysis}

As a comparison point in the simulations to follow, we also include an individual patient-level data (IPD) meta-analysis with a random intercept for trial membership. This method serves as a parametric comparison to the primarily non-parametric approaches outlined above. A meta-analysis does not employ a single-study method like the S-learner, X-learner, or causal forest; instead, all of the data is pooled together and trial-level relationships can be included as fixed or random effects. The meta-analysis model can be parametrized in many different ways; in this paper we set it up to mostly mimic the setup of the first scenario in the simulation to follow:
$$Y = (\alpha_0 + a_s) + 
(\alpha_1+b_s)X_1 + \alpha_2X_2 + \alpha_3X_3 + \alpha_4X_4 + (\zeta + z_s) A + (\theta + t_s) X_1 A + \epsilon.$$ In this model, we allow the intercept to include a fixed component ($\alpha_0$) and a random component by study ($a_s \sim N(0, \sigma_a^2))$, and our residual error is $\epsilon \sim N(0, \sigma^2)$. The fixed effects are  $\boldsymbol{\alpha} = \{\alpha_1, \alpha_2, \alpha_3, \alpha_4\}$, the coefficients relating the covariates to the outcome; $\zeta$, the coefficient for treatment; and $\theta$, the coefficient of the interaction between treatment and a moderator $X_1$ \citep{seo_comparing_2021}. The random effects by study are $b_s \sim N(0, \sigma_b^2)$, the random slope for the covariate $X_1$; $z_s \sim N(0, \sigma_z^2)$, the random slope for treatment; and $t_s \sim N(0, \sigma_t^2)$, the random slope for the treatment-$X_1$ interaction term. From here, the estimate of the conditional average treatment effect can be calculated as $\hat{\tau}_s(\boldsymbol{X}) = (\hat{\zeta}+\hat{z}_s) + (\hat{\theta}+\hat{t}_s)X_1$.

The meta-analysis framework assumes that the CATE function is shared across studies, but that the mean potential outcome under control can differ across studies. Notably, this functional form of the CATE assumes linear relationships, and one must prespecify all variables that might be relevant to the main effect of the covariates and to the treatment effect.

\section{Simulation Setup}

To compare both the single-study and aggregation methods, we performed a simulation study, simulating data from multiple randomized controlled trials and changing parameter values to compare which methods achieve the lowest mean squared error (MSE) between the estimated and true individual CATEs. Because there were three single-study methods and five aggregation methods being compared along with meta-analysis, there were $3*5+1=16$ total combinations of methods applied to each simulated dataset.

\subsection{Data Generating Mechanism}

The data generating mechanism is based somewhat off of Tan et al. \cite{tan_tree-based_2021} and Kunzel et al. \cite{kunzel2019metalearners} since both used methods similar to those compared here. The potential outcomes are generated using the following model: \begin{equation} \label{outcome}
    Y_i(a) = m(\boldsymbol{x}_i,s_i) + \frac{2a-1}{2}*\tau(\boldsymbol{x}_i,s_i) + \epsilon_i
\end{equation} where $m(\boldsymbol{x}_i,s_i)$ represents the outcome mean conditional on covariates and trial, and $\tau(\boldsymbol{x}_i,s_i)$ is the CATE. In the main setting for the data generation, we employed two options for $m$ and $\tau$. The first setup (1a) involves a linear $m$ and piecewise linear $\tau$: $$m(\boldsymbol{x},s) = x_1/2 + \sum_{j=2}^4x_j + \beta_s + \delta_s*x_1 \text{   and   } \tau(\boldsymbol{x},s) = x_1*I(x_1>0) + \beta_s + \delta_s*x_1.$$ The second setup (1b) involves a more complicated non-linear function for $\tau$: $$m(\boldsymbol{x},s) = 0 \text{   and   } \tau(\boldsymbol{x},s) = g(x_1)g(x_2) + \beta_s + \delta_s*x_1$$ where $g(x) = \frac{2}{1+\exp(-12(x-1/2))}$.\citep{kunzel2019metalearners} In both of these, the coefficients $\beta_s$ represent trial-specific main effect coefficients, and $\delta_s$ represent trial-specific interaction effect coefficients (interaction between trial and the moderator $x_1$). In both setups, $x_1$ is an effect moderator, and in the second setup, $x_2$ is as well. If the coefficients $\beta_s$ and $\delta_s$ differ across $s$ (i.e., trial membership), then trial is making an impact in the moderation.

From this information, the components simulated are listed as follows:

\begin{enumerate}
    \item For each simulation, the number of trials was $K=10$.
    \item Each trial had a sample size of 500 individuals.
    \item Within each trial, we simulated five continuous covariates per person, $\boldsymbol{X}_i \sim N(0, I_5)$, with each following a standard normal distribution.
    \item Each person was then assigned a treatment status, $0$ or $1$, according to a propensity score of $\pi_i = 0.5$ within each trial.
    \item Each person was also assigned an error term for their outcome function, so $\epsilon_i \sim N(0, .01)$.
    \item We then sampled trial-specific main effect and interaction effect terms. Each of the $K=10$ studies was assigned a main effect term according to $\beta_s \sim N(0, \sigma_{\beta}^2)$ and an interaction effect term according to $\delta_s \sim N(0, \sigma_{\delta}^2)$. The values of the standard deviations were: $(\sigma_{\beta}, \sigma_{\delta}) \in \{(0.5,0), (1,0), (1, 0.5), (1, 1), (3, 1)\}$. These standard deviation pairs are defined as {(Low-Low), (Med-Low), (Med-Med), (Med-High), (High-High)}, respectively.
    \item From this information, $m$, $\tau$, and $Y$ were calculated under either of the two setups described above (1a and 1b).
\end{enumerate}

From the above setups, there were five standard deviation pairs for the trial effects, and two functional forms for $m$ and $\tau$, therefore yielding 10 total scenarios. 

We then included one other scenario (2) to see how the methods would perform when the functional form of the CATE itself differed across trials - a particularly challenging situation for pooling. For this scenario, we used the same form for $Y_i$ as in Equation (\ref{outcome}), and now we set $m$ and $\tau$ to be such that $m$ is linear and $\tau$ depends on study: $$m(\boldsymbol{x},s) = x_1/2 + \sum_{j=2}^4x_j$$ and $$\tau(\boldsymbol{x},s) = I(s \in \{1,2,3,4\})*g(x_1)g(x_2) + I(s \in \{5,6,7,8\})*x_1*I(x_1>0) + I(s \in \{9,10\})*0$$ where $g(x)$ is as previously defined.

For each simulation setup, we generated 1,000 simulated datasets. Necessary packages included causalToolbox for the S-learner and X-learner \citep{kunzel2019metalearners}, grf for the causal forest\citep{athey_generalized_2019}, rpart for the ensemble tree  \citep{therneau2015package}, ranger for the ensemble forest \citep{wright2015ranger}, glmnet for the ensemble lasso \citep{friedman2017package}, and lme4 for the mixed effects meta-analysis \citep{bates2010lme4}. Some functions were based off of those in the ifedtree package \citep{tan_tree-based_2021} but were adapted to the setting in which data could be shared across trials. For each method and each iteration, performance of the different approaches was assessed based on the mean squared error (MSE) between the true individual CATE estimates and the estimated individual CATE estimates, and these MSEs were ultimately averaged across the 1,000 repetitions.

\section{Simulation Results}

The following tables and figures display the performance results across 1,000 iterations of each parameter combination/scenario. Table A\ref{mses} in the Appendix shows the average and standard deviation of the MSEs across all iterations of a given scenario and approach. Figure \ref{fig1} shows these average MSEs for every approach for each scenario, broken down by the standard deviations of the trial main and interaction effects. In the piecewise linear and non-linear CATE scenarios, as the trial coefficients (both main and interaction effects) increase in variability, the MSE increases, meaning the methods estimate individual CATEs more poorly. This is consistent with the idea that when trial membership is involved in the treatment effect function, the CATEs vary across trials and therefore are harder to estimate when data is pooled across studies. Notably, this increase in MSE happens much more quickly for the complete pooling approaches.

\begin{figure}[ht]
    \centering
    \includegraphics[width=18cm]{plots/MLSims_Fig1_14Feb2023.jpeg}
    \caption{Average MSE for each parameter combination across all approaches.*\\
    \footnotesize *The y-axis was cut off for ease of visualization; note that for the High-High SD combination, some methods were therefore missing from the graph because the MSEs were very high (complete pooling and meta-analysis). The SD of study main and study interaction coefficient pairs are as follows: Low-Low: 0.5, 0; Med-Low: 1, 0; Med-Med: 1, 0.5; Med-High: 1, 1; High-High: 3, 1. Facet labels refer to the simulation scenarios: 1a, 1b, and 2.}
    \label{fig1}
\end{figure}

In the piecewise linear scenario (1a), the most consistently effective approaches in terms of MSE are when the causal forest is used as the single-study method and when the aggregation approach is either pooling with trial indicator, ensemble forest, or ensemble lasso. The X-learner also performs relatively well in terms of MSE. Meta-analysis performs well, which is expected because the model was set up to mostly match the true functional form of the CATE in this scenario. For the non-linear scenario (1b), the ensemble lasso and meta-analysis perform notably worse, which makes sense due to the complexity of the functional form of the CATE, as it includes the product of two expit functions, and the lasso and meta-analysis assume a parametric linear relationship between covariates and outcome. The ensemble forest and pooling with trial indicator again estimate the CATEs well, with all single-study methods performing similarly. While the S-learner was not very effective with the piecewise linear CATE (1a), it was more effective with the non-linear CATE (1b).

Figure \ref{fig1} also displays the results for the variable CATE scenario (2). Here, the causal forest is clearly performing the best of the three single-study methods, while the S-learner is not performing as well. The most effective aggregation methods are again pooling with trial indicator and ensemble forests, along with ensemble trees. The least successful aggregation approach is the ensemble lasso, followed by meta-analysis and complete pooling.

Figure \ref{fig2} displays the average MSE across all data generation setups (i.e., piecewise linear, non-linear, and variable CATE) and all iterations. Consistent with our scenario-specific findings, the complete pooling approaches are ineffective at estimating the individual CATEs compared to the pooling with study indicator, ensemble tree, ensemble forest, ensemble lasso, and meta-analysis. Within those more effective aggregation approaches, the three single-study options perform somewhat similarly but with the S-learner doing the worst in almost all cases. The overall best approaches, in terms of average MSE, are the causal forest with pooling with trial indicator, the causal forest with an ensemble forest, and the X-learner with an ensemble forest.

\begin{figure}[ht]
    \centering
    \includegraphics[width=12cm]{plots/MLSims_Fig2_14Feb2023.jpeg}
    \caption{Average MSE across all scenarios and iterations.}
    \label{fig2}
\end{figure}

To more formally examine these results, we regressed the average MSE across iterations on the methods and parameter combinations, just within the piecewise linear and non-linear CATE scenarios and excluding meta-analysis. Specifically, the regression is such that: \begin{align*}
   MSE =  &\beta_0 + \beta_1*singlestudy + \beta_2*aggregation + \beta_3*singlestudy*aggregation \\
   &+ \beta_4*main_{sd} + \beta_5*interaction_{sd} + \beta_6*scenario + \epsilon.
\end{align*} From this regression, there were no significant differences in performance across single-study methods, but all aggregation methods performed significantly better than complete pooling. The ensemble forest had the best average MSE for the S-learner and X-learner, and pooling with trial indicator had the best average MSE for the causal forest. 

%In terms of trial coefficient standard deviations, data with a standard deviation of 3 for the trial main effect coefficients had significantly higher MSE than data with a standard deviation of 0.5 or 1 for the main effects. The piecewise linear and non-linear scenarios had similar MSEs overall. 

Finally, we also performed 500 more iterations of each scenario and parameter combination with the same methods previously described, but with honest causal forests instead of traditional ``adaptive'' causal forests. The resulting average MSEs are presented in the Appendix; we found similar results to the original 1,000 repetitions with adaptive causal forests, but the honest causal forests had slightly higher MSEs on average, indicating worse estimation accuracy than the adaptive causal forests. The honest causal forests had higher average MSE compared to the X-learner with each of the aggregation methods except for pooling with trial indicator (Figure A\ref{ahonest}).

\section{Application to Real Dataset}

After the simulations demonstrated differences across methods in several data generation setups, we applied the various methods to an existing dataset containing multiple randomized controlled trials that compared the same two medications.

\subsection{Treatments for Major Depressive Disorder}

The applied dataset used in the current paper consists of four randomized controlled trials  \citep{mahableshwarkar_randomized_2013,mahableshwarkar_randomized_2015,boulenger_efficacy_2014,baldwin2012randomised}, each of which included three treatments: duloxetine, vortioxetine, and placebo, where duloxetine and vortioxetine are both treatments for major depressive disorder (MDD). At the time of the trials, duloxetine had been more commonly used to treat MDD so was primarily included in the trials as an active reference, while vortioxetine was a newer treatment not yet marketed  \cite{schatzberg2014overview}. Each of the four trials compared at least two different dosages of vortioxetine and therefore had more participants taking vortioxetine as opposed to duloxetine or placebo. For the purposes of the current application, we removed placebo participants and lumped all dosages of vortioxetine together to investigate the potential differences between the efficacy of the active medications (duloxetine and vortioxetine), as well as identify features that might be moderating this difference.

Participants in each of the four trials shared similar eligibility criteria. All four trials required patients to be between the ages of 18 to 75, to have a Major Depressive Episode (MDE) as a primary diagnosis according to the DSM-IV-TR criteria over at least three months, and to have a Montgomery-Asberg Depression Rating Scale (MADRS) \cite{montgomery1979new} score of at least 22 (one trial) or 26 (three trials) at both screening and baseline\citep{mahableshwarkar_randomized_2013,mahableshwarkar_randomized_2015,boulenger_efficacy_2014,baldwin2012randomised}. A primary outcome in the trials is the change in MADRS (Montgomery-Asberg Depression Rating Scale) \cite{montgomery1979new} score from baseline to the last observed follow-up in the study. Participants were meant to stay in the study for 8 weeks, at which point their final MADRS score was collected. For those who did not remain in the trial for 8 weeks, a last observation carried forward imputation approach was used for simplicity. This imputation approach is not the best way to account for missing data and many other options exist \citep{little2012prevention}, but it is used here for simplicity because this example is primarily illustrative. Predictors/effect modifiers used in the models were age, sex (female or male), smoking status (ever smoked or never smoked), weight, baseline MADRS score, baseline HAM-A (Hamilton Anxiety Rating) score \cite{hamilton1959assessment}, comorbidity indicators (if ever had diabetes mellitus, hypothyroidism, anxiety), and medication indicators (if they are concomittantly taking an antidepressant, antipsychotic, thyroid medication). Since the outcome is the difference in MADRS score (MADRS at follow up minus MADRS at baseline), a more negative outcome indicates a better result. 

After removing any individuals with missing treatment assignment or missing outcomes and removing individuals who were assigned to the placebo group, sample sizes were 575, 436, 418, and 418 for each of the trials. Further descriptive information about the samples in the four RCTs is reported in the Appendix (Table A\ref{desc}). Little missing covariate data was present in the sample; however, conditional mean imputation was performed for missing values of weight (n=1) and baseline HAM-A score (n=2).

Following data preparation, we used each of the aforementioned method combinations (i.e., causal forest, S-learner, and X-learner as single-study methods paired with complete pooling, pooling with trial indicator, ensemble tree, ensemble forest, and ensemble lasso) to estimate the CATEs for every individual across the four trials. We then compared the CATE estimates across methods to see their concordance levels. Notably, it is not possible to compare the method performances with the truth, as the true CATEs are unknown in this real dataset.

\subsection{Results}

All methods broadly led to the conclusion of a positive average CATE. This indicates that vortioxetine is estimated to have less of a beneficial effect on the MADRS score on average. In each of the four RCTs, both treatments were associated with a reduction in depressive symptom severity over time (shown through a reduction in MADRS score), but this reduction was smaller for the vortioxetine group than the duloxetine group. Table \ref{cates} contains the mean and standard deviation of the CATEs according to each method. Broadly, the S-learner approaches estimated lower CATEs on average than the other approaches, and there is some consistency between the aggregation approaches within each single-study method (S-learner, X-learner, and causal forest). There were especially high levels of similarity in the average CATE estimates across the causal forest methods, shown in the last column of Table \ref{cates}. The variability of the CATE estimates differs depending on the approach as well; causal forest approaches had higher standard deviations than approaches that used the S-learner and X-learner. Complete pooling also yielded the highest standard deviations for CATE estimates out of all of the aggregation approaches. As a comparison point, we used a multiple linear regression to estimate an average treatment effect of 2.39 (SE = 0.5), which is similar to the averages of the CATEs according to the X-learner and causal forest approaches.



\begin{table}[ht]
    \centering
    \begin{tabular}{|l|c|c|c|}
         \hline
         & \textbf{S-Learner} & \textbf{X-Learner} & \textbf{Causal Forest}\\
         \hline
         \textbf{Complete Pooling} & 1.38 (1.6) & 2.57 (1.4) & 2.37 (2.8) \\
         \textbf{Pooling with Trial Indicator} & 0.91 (1.3) & 2.52 (1.3) & 2.37 (2.7)\\
         \textbf{Ensemble Tree} & 0.89 (1.3) & 2.35 (1.5) & 2.23 (2.5)\\
         \textbf{Ensemble Forest} & 0.89 (1.1) & 2.36 (1.4) & 2.30 (2.2)\\
         \textbf{Ensemble Lasso} & 0.89 (1.2) & 2.32 (1.4) & 2.23 (2.1)\\
         \hline
    \end{tabular}
    \caption{Mean (SD) of CATEs from all individuals in sample according to different single-study and aggregation method combinations.*\\
    \footnotesize *The CATEs are individual-level estimates that indicate the difference in the estimated effect of vortioxetine versus duloxetine on the difference in MADRS score for a given patient. A positive CATE indicates that vortioxetine is estimated to have a smaller reduction of the MADRS score.}
    \label{cates}
\end{table}

We then focused in on the resulting model from the causal forest with pooling with trial indicator, since that approach performed the best on average in the simulations. The CATE estimates according to this approach with their associated 95\% confidence intervals are displayed in Figure \ref{catecis}. These estimates support that the majority of individuals have a positive CATE estimate, but they also display very high levels of uncertainty, with all confidence intervals including zero. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=12cm]{plots/cate_cis_25Jan2023.png}
    \caption{Point estimates and 95\% confidence intervals for CATEs according to causal forest with pooling with trial indicator.}
    \label{catecis}
\end{figure}


%To investigate individual moderators, it is challenging to directly describe the relationships of covariates that moderated the treatment effect in a non-parametric approach like those used here. This is a topic of current extensions. One initial approach is to visualize the relationship between variables and the estimated CATE. Figure \ref{studyhist} displays the distribution of CATE estimates broken down by study membership. These distributions demonstrate that there does not seem to be a major difference in the CATE distributions according to study membership, indicating that study membership is likely not a major moderator of the treatment effect. Another plot displayed in the Appendix (Figure A\ref{aage}) visualizes the relationship between age and CATE estimate.

To learn more about the moderation within the CATE model, we can explore variable importance measures. In the causal forest as applied through the grf package \cite{athey_generalized_2019}, variable importance is calculated as a weighted sum of the number of times the variable was used in a split at each level of the forest. Figure \ref{varimp} displays the variable importance measures according to the grf package \citep{athey_generalized_2019} for all covariates, first in separate causal forests for each study (\ref{studyspec_varimp}), and second according to the causal forest with pooling with trial indicator (\ref{cfpool_varimp}). From Figure \ref{studyspec_varimp}, there are a few variables that are consistently identified as effect moderators across studies (age, weight, baseline MADRS score, and baseline HAM-A score), and there are several that are not found to be major moderators (the comorbidity and medication indicators). However, notably there are some differences according to the separate models, indicating that the treatment effect functions are slightly different within each study. Figure \ref{cfpool_varimp} then displays the resulting importance measures from one aggregation model fit to all studies. Here, we can see that the same four variables (age, weight, baseline MADRS, and baseline HAM-A) are involved in a high proportion of the splits in the causal forest, and study membership is involved in some splits as well. The fact that these study indicators are not more highly involved in the partitioning of the treatment effect is a good sign, though, that there is not a very high level of heterogeneity in CATE estimates across studies. 

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/studyspec_imp_3Feb2023.png} 
        \caption{Variable importance for study-specific causal forest models.} \label{studyspec_varimp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/cfpool_imp_3Feb2023.png} 
        \caption{Variable importance for causal forest with pooling with trial indicator} \label{cfpool_varimp}
    \end{subfigure}
    \caption{Variable importance measures (a) within studies, and (b) according to the causal forest with pooling with trial indicator}
    \label{varimp}
\end{figure}

The variable importance plots do not demonstrate the direction of the moderating effect, however. We briefly investigate these directional effects through an interpretation tree (Figure \ref{inttree}) and through exploratory plots such as Figure A\ref{aage}. This interpretation tree was formed by fitting a regression tree, where the CATE estimates according to the causal forest with pooling with trial indicator were the outcomes, and the features (predictors) were every covariate in the original CATE model. The tree confirms what was shown in Figure \ref{varimp} -- that age, weight, baseline MADRS, and baseline HAM-A score are the strongest effect moderators. Study membership does not show up in this interpretation tree, supporting that there is low heterogeneity across trials. This is a helpful visual to see the direction of the relationships aggregated across trials, but it is exploratory and should not be interpreted in great detail. Another similar approach for investigating the CATE function in terms of individual moderators is to fit the best linear projection of the CATE estimates using a function in the grf package \cite{athey_generalized_2019}; the resulting coefficients from this regression using doubly-robust estimates of the CATE are reported in Table A\ref{atab2}.


\begin{figure}[ht]
    \centering
    \includegraphics[width=13cm]{plots/inttree_9Jan2023.png}
    \caption{Interpretation tree for causal forest with pooling with trial indicator.*\\
    \footnotesize *Circled numbers represent the average CATE estimate for individuals in that leaf.}
     \label{inttree}
\end{figure}


Broadly, these interpretations of the CATE function derived by the causal forest with pooling with trial indicator do not display high levels of heterogeneity or highlight clear groups of individuals who might differ in their treatment effect. Although the interpretation tree shows a partition of the covariate space into groups that have different average CATEs, the other plots and best linear projection do not show high levels of heterogeneity, except for slightly higher average CATE estimates for older individuals. This data application shows how to effectively apply the methods compared in simulations to a real dataset and assess potential moderation. The methods all agree broadly on the direction of the average treatment effect but have some differences in the individual CATE estimates and which variables generate effect heterogeneity.

%This lack of notable heterogeneity was confirmed through a calibration test provided in the grf package. \cite{athey_generalized_2019} This test provides a value for differential forest predictions, which when significantly different from zero indicates that there is underlying heterogeneity found in the causal forest. In this example, the differential forest prediction coefficient was 0.26 with a p-value of 0.08, indicating potential levels of heterogeneity but not a significant amount. 

\section{Discussion}

In this paper, we compared methods to estimate the conditional average treatment effect in a single trial and methods to extend the single-trial approaches to multiple trials. In the absence of notable cross-trial heterogeneity of treatment effects, the methods examined all performed well, but when trial membership was involved in the treatment effect function, some methods performed worse than others. Specifically, and not surprisingly, methods that ignore trial membership (complete pooling) do not effectively estimate the CATE when there is cross-trial heterogeneity. On the other hand, some methods performed well no matter the level of heterogeneity: pooling with trial indicator and ensemble forests had consistently low mean squared error despite increasing the variability of the trial membership coefficients in the treatment effect. This was especially true when the single-study method used was the causal forest (Figure \ref{fig1}). 

When considering the three single-study approaches, the most consistently favorable method in the simulations was the causal forest, followed by the X-learner. The S-learner performed well in certain scenarios, such as scenario 1b, where the treatment effect function involved a bounded, non-linear expit function. The performances of the S-learner and X-learner in our simulations and applied example were consistent with results found previously \citep{kunzel2019metalearners}, in that the S-learner seemed to be somewhat biased towards 0 in the applied example (Table \ref{cates}) and performed worse in the simulations when the treatment effect function was complicated (variable CATE scenario and the piecewise linear and non-linear CATE with high variability). The X-learner performed well in the simulations with complex CATEs and with structural forms of the CATE, again consistent with previous work \citep{kunzel2019metalearners}. The causal forest performed well across all scenarios. These simulation results and the results from the applied data example of MDD medications demonstrate that it is important to carefully select the single-study method for a given question, as each of the three options can provide different estimates. A good starting point would be to consider expert knowledge of how heterogeneous across studies and complicated the outcomes or treatment effect might be. These results also indicate the need for more diagnostics in future work to help researchers determine which approach to choose.

The simulations also incorporated some comparisons between the non-parametric and parametric approaches. Specifically, the usage of a lasso regression as an ensemble showed how a parametric ensemble could perform compared to the ensemble tree and forest. The lasso performed very well when the treatment effect function was piecewise linear (scenario 1a) but quickly suffered in performance when the function was more non-linear (scenarios 1b and 2). Furthermore, the inclusion of a mixed effects meta-analysis demonstrated a common parametric technique used in the multiple-study setting. This model was set up to perform well when the CATE function was piecewise linear (scenario 1a), but it yielded high MSE in the non-linear and complex scenarios that it was not correctly parametrized for (scenario 1b and 2). These comparisons demonstrate that non-parametric machine learning approaches are very beneficial when the treatment effect function is complicated and non-linear, as the non-parametric methods do not require correct specification of any parameters. Although interpretability becomes more of a challenge, the non-parametric methods allow for flexible relationships and hopefully high levels of accuracy in CATE estimation. 

In this work, we did not explore an exhaustive list of potential single-study and aggregation methods. We attempted to select single-study methods that were common, user-friendly, and shown to be effective or potentially effective in previous literature. However, as this is an ever-growing field, future work could include other single-study methods \citep{wendling_comparing_2018,powers_methods_2018} to see how they compare to the ones used in this study. For example, it would be interesting to investigate the performance of the X-learner with a different base learner, such as Bayesian additive regression trees (BART) \cite{chipman2010bart,kunzel2019metalearners}. The simulations included three scenarios and some sub-scenarios that should be possible in real data, but there of course are many other possible data generation setups that could be implemented and could show slightly different results.

It is important to note that with the exception of the complete pooling and meta-analysis approaches, the resulting CATE estimates are trial-specific. Unless trial was not picked up in the aggregation methods, the majority of the methods discussed will produce trial-specific estimates of the CATE. This allows for improved accuracy of estimates but might be less helpful in real world applications. We are interested in continuing to identify ways in which researchers could aggregate across trials to develop estimates that are accurate but not trial-specific -- this could be crucial for use of the resulting methods and models in practice, on data not coming from the specific trials used in the model formulation. However, the trial-specific estimates can still be useful; for example, if trials were done in separate hospitals, CATEs of future patients could be predicted using the hospital that they are being treated in, and the model that estimates their treatment effect should be more accurate after taking into consideration the data from the other hospitals. 

In the MDD trials, duloxetine was included as a reference medication because it was already marketed at the time of the trials, and patients were excluded from the study if they had previously not responded to duloxetine. On the other hand, vortioxetine was not yet marketed and was the more experimental medication; therefore, some bias could arise due to participants being excluded if they had previously not responded to duloxetine. Acknowledging this, we were able to estimate treatment effects according to each method combination, and we used variable importance and interpretation trees to investigate which variables might be important moderators of the treatment effect. Variable importance is a limited measure and can often be biased towards continuous variables with more possible split points  \cite{strobl2007bias}, so we encourage caution when interpreting those results. This example dataset shows how to combine multiple RCTs to get an improved assessment of treatment effect heterogeneity and better determine which treatment would be best suited to a given individual, based on their features and their estimated CATE.  Notably, the four trials used in this dataset were run by the same organizations and had very similar protocols; this helps ensure that we can confidently combine datasets but also might limit the potential heterogeneity across trials that might exist in other applications. We also did not see high levels of heterogeneity in the treatment effects based on other covariates in these trials. A general idea is that studies need to be four times larger to identify effect moderators compared to an average treatment effect \cite{enderlein_fleiss_1988}, and this study included precisely four trials. Therefore, our findings would become more robust and we could more confidently assess heterogeneity or lack thereof with the inclusion of more studies.

There are many openings for future work, some of which have been previously mentioned. Broadly, it is important to further refine these methods and identify which are most helpful in specific data scenarios. It will also be helpful to determine when it is appropriate to develop universal CATE estimates, versus when the CATE estimates should be trial-specific. This paper demonstrated several approaches that take data from multiple studies and estimate heterogeneous treatment effects, using flexible models that allow for complex relationships - which is often the case in the real world.

\section{Acknowledgments}

The study was funded by the Patient-Centered Outcomes Research Institute (PCORI) through PCORI Award ME-2020C3-21145 (PI: Stuart) and the National Institute of Mental Health (NIMH) through Award R01MH126856 (PI: Stuart). Disclaimer: Opinions and information in this content are those of the study authors and do not necessarily represent the views of PCORI or NIMH. Accordingly, PCORI and NIMH cannot make any guarantees with respect to the accuracy or reliability of the information and data.

Furthermore, this paper is based on research using data from data contributors, Takeda and Lundbeck, that has been made available through Vivli, Inc. Vivli has not contributed to or approved, and is not in any way responsible for, the contents of this publication.

\bibliographystyle{unsrtnat}
\bibliography{MLSims_Arxiv}

\newpage

\appendix

\begin{table}[ht]
\label{atab1}
\resizebox{\textwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    & \multicolumn{5}{c|}{\textbf{Piecewise Linear (1a)}} & \multicolumn{5}{c|}{\textbf{Non-Linear (1b)}} & \textbf{Variable (2)} \\
    \hline
    & \textbf{Low-Low} & \textbf{Med-Low} & \textbf{Med-Med} & \textbf{Med-High} & \textbf{High-High} & \textbf{Low-Low} & \textbf{Med-Low} & \textbf{Med-Med} & \textbf{Med-High} & \textbf{High-High} & \\
    \hline
    S - Pool & 0.43 (0.11) & 1.07 (0.42) & 1.3 (0.43) & 1.94 (0.58) & 8.75 (3.56) & 0.2 (0.09) & 0.77 (0.36) & 0.98 (0.4) & 1.57 (0.53) & 7.9 (3.33) & 0.42 (0.02)\\
    \hline
    X - Pool & 0.25 (0.11) & 0.92 (0.45) & 1.16 (0.45) & 1.82 (0.6) & 9.06 (3.76) & 0.22 (0.1) & 0.86 (0.4) & 1.1 (0.45) & 1.75 (0.58) & 8.83 (3.72) & 0.29 (0.01)\\
    \hline
    CF - Pool & 0.26 (0.12) & 0.99 (0.48) & 1.25 (0.48) & 1.98 (0.66) & 9.8 (4.04) & 0.23 (0.1) & 0.89 (0.42) & 1.15 (0.46) & 1.84 (0.61) & 9.17 (3.82) & 0.31 (0.01)\\
    \hline
    S - Indicator & 0.41 (0.09) & 0.6 (0.16) & 0.77 (0.18) & 1.27 (0.39) & 0.91 (0.6) & 0.02 (0.02) & 0.05 (0.07) & 0.08 (0.08) & 0.15 (0.14) & 0.49 (0.7) & 0.43 (0.02)\\
    \hline
    X - Indicator & 0.09 (0.05) & 0.22 (0.16) & 0.3 (0.18) & 0.53 (0.29) & 1.83 (1.58) & 0.06 (0.04) & 0.18 (0.16) & 0.25 (0.17) & 0.44 (0.27) & 1.78 (1.45) & 0.18 (0.01)\\
    \hline
    CF - Indicator & \textbf{0.03 (0)} & \textbf{0.03 (0.01)} & \textbf{0.05 (0.01)} & \textbf{0.09 (0.03)} & \textbf{0.11 (0.03)} & \textbf{0.02 (0)} & \textbf{0.02 (0)} & \textbf{0.04 (0.01)} & \textbf{0.07 (0.02)} & \textbf{0.07 (0.02)} & \textbf{0.03 (0.01)}\\
    \hline
    S - Tree & 0.5 (0.05) & 0.52 (0.06) & 0.57 (0.1) & 0.74 (0.16) & 0.95 (0.28) & 0.21 (0.05) & 0.19 (0.04) & 0.34 (0.08) & 0.6 (0.18) & 1.25 (0.41) & 0.57 (0.03)\\
    \hline
    X - Tree & 0.11 (0.02) & 0.15 (0.03) & 0.21 (0.05) & 0.34 (0.11) & 0.78 (0.28) & 0.16 (0.02) & 0.18 (0.02) & 0.31 (0.07) & 0.53 (0.14) & 1.29 (0.4) & 0.16 (0.02)\\
    \hline
    CF - Tree & 0.06 (0.01) & 0.1 (0.03) & 0.17 (0.05) & 0.3 (0.1) & 0.74 (0.27) & 0.13 (0.02) & 0.15 (0.02) & 0.28 (0.06) & 0.49 (0.13) & 1.18 (0.4) & 0.06 (0.01)\\
    \hline
    S - Forest & 0.48 (0.04) & 0.46 (0.05) & 0.48 (0.08) & 0.58 (0.1) & 0.41 (0.09) & 0.13 (0.05) & 0.09 (0.04) & 0.12 (0.05) & 0.23 (0.12) & 0.14 (0.07) & 0.55 (0.03)\\
    \hline
    X - Forest & 0.08 (0.01) & 0.08 (0.01) & 0.1 (0.01) & 0.15 (0.04) & 0.15 (0.04) & 0.07 (0.01) & 0.07 (0.01) & 0.09 (0.01) & 0.16 (0.04) & 0.16 (0.04) & 0.17 (0.02)\\
    \hline
    CF - Forest & 0.04 (0) & 0.04 (0) & 0.06 (0.01) & 0.1 (0.03) & \textbf{0.11 (0.03)} & 0.04 (0) & 0.04 (0) & 0.06 (0.01) & 0.11 (0.04) & 0.11 (0.03) & 0.07 (0.01)\\
    \hline
    S - Lasso & 0.5 (0.05) & 0.48 (0.06) & 0.5 (0.09) & 0.56 (0.1) & 0.34 (0.08) & 0.78 (0.03) & 0.77 (0.03) & 0.78 (0.03) & 0.84 (0.08) & 0.8 (0.05) & 0.64 (0.03)\\
    \hline
    X - Lasso & 0.13 (0.01) & 0.13 (0.01) & 0.13 (0.01) & 0.14 (0.01) & 0.15 (0.02) & 0.76 (0.02) & 0.76 (0.02) & 0.77 (0.02) & 0.79 (0.03) & 0.79 (0.02) & 0.38 (0.02)\\
    \hline
    CF - Lasso & 0.1 (0) & 0.1 (0) & 0.11 (0.01) & 0.12 (0.01) & 0.12 (0.01) & 0.75 (0.02) & 0.75 (0.02) & 0.76 (0.02) & 0.77 (0.02) & 0.77 (0.02) & 0.35 (0.01)\\
    \hline
    Meta-Analysis & 0.09 (0.03) & 0.1 (0.09) & 0.1 (0.1) & 0.1 (0.11) & 0.22 (1.11) & 0.93 (0.03) & 0.94 (0.11) & 0.93 (0.09) & 0.94 (0.13) & 1.01 (0.91) & 0.41 (0.02)\\
    \hline
    \end{tabular}}
    \caption{Average MSE for each method and data generation setup.*\\
    \footnotesize *The lowest average MSE for each scenario (column) is bolded.}
    \label{mses}
\end{table}


\begin{figure}[htp]
    \centering
    \includegraphics[width=13cm]{plots/MLSims_Honest_15Feb2023.jpeg}
    \caption{Average MSE across all scenarios and iterations using honest causal forests.}
    \label{ahonest}
\end{figure}

\begin{table}[ht]
    \centering
    \includegraphics[width=13cm]{plots/table1_update.png}
    \caption{Descriptive statistics of participants of four randomized controlled trials, broken down by treatment group.}
    \label{desc}
\end{table}

%\begin{figure}[htp]
%    \centering
%    \includegraphics[width=18cm]{plots/histograms_9Jan2023.png}
%    \caption{Distribution of CATE estimates for the MDD RCTs according to each method.*\\
%    \footnotesize *Note that uncertainty of the CATE estimates is not reflected in this plot.}
%    \label{ahist}
%\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=13cm]{plots/age_cates_3Feb2023.png}
    \caption{CATE estimates by age of individual according to causal forest with pooling with trial indicator.*\\
    \footnotesize *Note that uncertainty of the CATE estimates is not reflected in this plot.}
    \label{aage}
\end{figure}

%\begin{figure}[htp]
%   \centering
%    \includegraphics[width=13cm]{madrshama_cate_9Jan2023.png}
%    \caption{Baseline MADRS score, baseline HAM-A score, and CATE estimate according to causal forest with pooling with trial indicator.*\\
%    \footnotesize *Note that uncertainty of the CATE estimates is not reflected in this plot.}
%    \label{amadrs}
%\end{figure}

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|c|c|c|}
    \hline
          & \textbf{Estimate} & \textbf{Standard Error} & \textbf{P-Value} \\
          \hline
         (Intercept) & -6.32 & 5.06 & 0.21 \\
         \hline
         Age & 0.09 & 0.04 & 0.03* \\
         \hline
         Female & 0.45 & 1.07 & 0.67 \\
         \hline
         Smoker & -1.47 & 1.10 & 0.18 \\
         \hline
         Weight & -0.01 & 0.03 & 0.72 \\
         \hline
         Baseline MADRS & 0.09 & 0.13 & 0.49 \\
         \hline
         Baseline HAM-A & 0.08 & 0.09 & 0.38 \\
         \hline
         Has Diabetes Mellitus & -3.97 & 3.36 & 0.24 \\
         \hline
         Has Hypothyroidism & -1.81 & 3.56 & 0.61 \\
         \hline
         Has Anxiety & 3.58 & 3.63 & 0.32 \\
         \hline
         Takes Antidepressant & 1.55 & 1.32 & 0.24 \\
         \hline
         Takes Antipsychotic & -0.21 & 1.93 & 0.91 \\
         \hline
         Takes Thyroid Medication & 2.23 & 4.22 & 0.60 \\
         \hline
         Study NCT00635219 & -1.09 & 1.63 & 0.50 \\
         \hline
         Study NCT01140906 & 2.71 & 1.62 & 0.09 \\
         \hline
         Study NCT00672620 & 2.93 & 1.58 & 0.06 \\
         \hline
    \end{tabular}
    \caption{Results of best linear projection of the CATE according to the causal forest with pooling with trial indicator.\\
    \footnotesize *Indicates a p-value less than 0.05.}
    \label{atab2}
\end{table}

\end{document}
