{
    "arxiv_id": "2303.07863",
    "paper_title": "You Can Ground Earlier than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos",
    "authors": [
        "Xiang Fang",
        "Daizong Liu",
        "Pan Zhou",
        "Guoshun Nan"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
    ],
    "abstract": "Given an untrimmed video, temporal sentence grounding (TSG) aims to locate a\ntarget moment semantically according to a sentence query. Although previous\nrespectable works have made decent success, they only focus on high-level\nvisual features extracted from the consecutive decoded frames and fail to\nhandle the compressed videos for query modelling, suffering from insufficient\nrepresentation capability and significant computational complexity during\ntraining and testing. In this paper, we pose a new setting, compressed-domain\nTSG, which directly utilizes compressed videos rather than fully-decompressed\nframes as the visual input. To handle the raw video bit-stream input, we\npropose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF)\nframework, which extracts and aggregates three kinds of low-level visual\nfeatures (I-frame, motion vector and residual features) for effective and\nefficient grounding. Particularly, instead of encoding the whole decoded frames\nlike previous works, we capture the appearance representation by only learning\nthe I-frame feature to reduce delay or latency. Besides, we explore the motion\ninformation not only by learning the motion vector feature, but also by\nexploring the relations of neighboring frames via the residual feature. In this\nway, a three-branch spatial-temporal attention layer with an adaptive\nmotion-appearance fusion module is further designed to extract and aggregate\nboth appearance and motion information for the final grounding. Experiments on\nthree challenging datasets shows that our TCSF achieves better performance than\nother state-of-the-art methods with lower complexity.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.07863v1"
    ],
    "publication_venue": "Accepted by CVPR-23"
}