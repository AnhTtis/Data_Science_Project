\documentclass[letter, 12pt]{article}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{natbib}
\setlength{\bibsep}{0pt plus 0.1ex}
\usepackage{amssymb, blkarray, amsmath,xcolor,graphicx,xspace,colortbl,rotating} % 
\usepackage[raggedrightboxes]{ragged2e} 
\usepackage{textcomp}
\usepackage{appendix}  %% 100
\usepackage{bm}  %% 100
\usepackage{boxedminipage}  %% 100
\usepackage{color}  %% 100
\usepackage{endnotes}  %% 100
\usepackage{ragged2e}  %% 100
\usepackage[onehalfspacing]{setspace}  %% 100
\usepackage{tabulary}  %% 100  
\usepackage{varioref}  %% 100
\usepackage{wrapfig}  %% 100  

\usepackage[margin=1in]{geometry}
\DeclareGraphicsExtensions {.pdf,.svg,.eps,.ps,.png,.jpg,.jpeg}
\newtheorem {theorem}{Theorem}
\newtheorem {acknowledgement}{Acknowledgement}
%\newtheorem {algorithm}[theorem]{Algorithm}
\newtheorem {assumption}{Assumption}
\newtheorem {axiom}[theorem]{Axiom}
\newtheorem {case}[theorem]{Case}
\newtheorem {claim}{Claim}
\newtheorem {conclusion}[theorem]{Conclusion}
\newtheorem {condition}[theorem]{Condition}
\newtheorem {conjecture}[theorem]{Conjecture}
\newtheorem {corollary}{Corollary}
\newtheorem {criterion}[theorem]{Criterion}
\newtheorem {definition}{Definition}
\newtheorem {example}{Example}
\newtheorem {exercise}[theorem]{Exercise}
\newtheorem {lemma}{Lemma}
\newtheorem {notation}[theorem]{Notation}
\newtheorem {problem}[theorem]{Problem}
\newtheorem {proposition}{Proposition}
\newtheorem {remark}{Remark}
\newtheorem {solution}[theorem]{Solution}
\newtheorem {summary}[theorem]{Summary}
\newcommand{\newgw}[1]{{\color{blue} #1}}
\newcommand{\delgw}[1]{{\color{green} [DELETED: #1]}}
\newcommand{\newbl}[1]{{\color{red} #1}}
\newenvironment {proof}[1][Proof]{\noindent \textbf {#1.} }{\ \rule {0.5em}{0.5em}}


\begin{document}
%\title{On nonlinear Markov Chains with an Aggregator}
%\author{Bar Light}
%\maketitle

\title{Invariant Distributions in Nonlinear Markov Chains with an Aggregator: Theory and Applications}

\author{Bar Light\protect\thanks{Business School and Institute of Operations Research and Analytics, National University of Singapore, Singapore. e-mail: \textsf{barlight@nus.edu.sg} }} 
\maketitle

\thispagestyle{empty}

 \noindent \noindent \textsc{Abstract}:
\begin{quote}
	
We study the properties of a subclass of stochastic processes called discrete-time nonlinear Markov chains with an aggregator. In these chains, the next period’s distribution depends on both the current state and a real-valued function of the current distribution. For these chains, we provide conditions for the uniqueness of an invariant distribution that do not rely on typical contraction arguments. Instead, our approach leverages flexible monotonicity properties imposed on the nonlinear Markov kernel. We also provide existence results and introduce a straightforward computational method for finding the invariant distribution. We demonstrate the necessity of these monotonicity conditions in proving the uniqueness of an invariant distribution through simple examples. We apply our findings to analyze invariant distributions in strategic queueing systems, establish conditions under which a class of nonlinear equations in $\mathbb{R}^{n}$ has a unique solution, and examine the properties of stationary wealth distributions in large dynamic economies.

		 
\end{quote}


%\noindent {\small Keywords: }Dynamic programming;.

%\smallskip \noindent \emph{JELcis plassification}: 

\newpage 


\section{Introduction} 


Nonlinear Markov chains are stochastic processes in which the distribution of the process in the next period  depends on both the current state of the chain and the current distribution. These chains naturally model systems of interacting particle systems and have been extensively studied across various fields, including the McKean–Vlasov process \citep{mckean1966class}, mean-field games \citep{huang2006large,lasry2007mean}), population games \citep{sandholm2010population}, and evolutionary biology \citep{kolokoltsov2010nonlinear}. 
Nonlinear Markov chains with an aggregator are a subclass of nonlinear Markov chains,  where the next period's distribution of the process depends on both the current state of the chain and a real-valued function of the current distribution that is called an aggregator.\footnote{The terminology comes from the game theory literature where the distribution of the process represents the distribution of players' states \citep{acemoglu2012,light2022mean}. We keep this terminology for the current paper despite the fact that we study general nonlinear Markov chains that are not necessarily related to game theory.} These chains naturally appear in large dynamic economies e.g., the wealth distribution's evolution in heterogeneous-agent macroeconomics models \citep{aiyagari1994} or the industry dynamics' evolution  \citep{weintraub2008markov}, and in the evolution of opinion dynamics and other stochastic processes described in  \cite{kolokoltsov2010nonlinear}.\footnote{
Numerous dynamic economic models incorporate an aggregator function, as detailed in works such as \cite{acemoglu2012},  \cite{acemoglu2018equilibrium}, and \cite{light2022mean}, which explore a variety of dynamic models featuring an aggregator. Nonlinear Markov chains equipped with an aggregator, studied in this paper, effectively capture the dynamics of these systems, where the invariant distribution often represents the equilibrium of the economic model (see Section \ref{sec:wealth} for a specific example of this type). In these dynamic economic models the aggregator is typically monotone as required in our setting (e.g., it is monotone in all the applications in \cite{acemoglu2012} and \cite{light2022mean}).}



In this paper, we study discrete-time nonlinear Markov chain with an aggregator and provide conditions that ensure the uniqueness of an invariant distribution for these chains without relying on contraction arguments.   Our approach to prove uniqueness is based on monotonicity properties imposed on the nonlinear Markov kernel. These monotonicity conditions are flexible and can be tailored to the specific nonlinear Markov chain being studied (see Example \ref{example:flexible} in Section \ref{sec:flexibility}). We provide simple examples that demonstrate that uniqueness may fail when the monotonicity conditions do not hold (see Examples \ref{example:decreasing} and \ref{example:preserving} in Section \ref{sec:necessity}). We also prove the existence of an invariant distribution under certain continuity and boundedness conditions (see Section \ref{Section: Existence}) and introduce a simple algorithm that uses a bisection method to compute an invariant distribution of the nonlinear chain (see Section \ref{sec:compute}), which enhances the practical applicability of our results. 


In Section \ref{sec:applications}, we explore three distinct applications where our results can be naturally applied. The first application addresses a strategic G/G/1 queueing system, where customer arrivals are influenced by expected waiting times. Under natural conditions on the arrival process that imply that when the expected waiting time is higher less agents join the queue,  we demonstrate that there is a unique invariant distribution for the nonlinear dynamics describing the queueing system. We also compute the unique equilibrium expected waiting time for a specific M/G/1 queueing system case. The second application studies nonlinear equations in $\mathbb{R}^{n}$, which, despite lacking contraction properties, still possess a unique solution under certain monotonicity conditions that we provide. The third application examines the general evolution of wealth distributions within dynamic economic models. We introduce economic assumptions on agents' decisions that ensure the uniqueness of the invariant equilibrium wealth distribution. Together, these applications demonstrate the versatility of our findings in establishing the uniqueness of an invariant distribution across a diverse range of nonlinear Markov chains. 



\cite{butkovsky2014ergodic} provides conditions for the ergodicity of nonlinear Markov chains. \cite{saburov2016ergodicity} establishes ergodicity conditions for finite state nonlinear Markov chains and \cite{shchegolev2022new} provides improved convergence rates (see \cite{budhiraja2015local} and \cite{ying2018approximation} for further related results). However, these conditions are significantly stronger than those required for the ergodicity of standard linear Markov chains and are not applicable to many settings of interest including the applications we study in this paper. Additionally, in Example \ref{Example:convergence}, we demonstrate that even for one of the most basic nonlinear Markov chains with two states, which satisfies our uniqueness conditions, the chain is not ergodic and 
 does not converge to the unique invariant distribution. This example illustrates that the concepts of uniqueness and ergodicity are distinct, with the separation, intuitively, being more pronounced in nonlinear Markov chains.  In Example \ref{Example: Average Convergence} we further show that a law of large numbers does not hold for the nonlinear Markov chains even when our uniqueness conditions hold.  Despite these negative results, we   provide some important applications where the uniqueness of the invariant measure is of interest. For example, the invariant measure can correspond to the solution of nonlinear equations in $\mathbb{R}^{n}$ or the equilibrium wealth distribution of large dynamic economies (see Section \ref{sec:applications}).\footnote{Another  related area of literature is mean field games, where conditions for uniqueness have been studied in \cite{lasry2007mean}, \cite{light2022mean}, and \cite{anahtarci2023learning} in different settings (see also \cite{wikecek2020discrete} and references therein for insights into the connection between discrete-time mean field games and nonlinear Markov chains).} 
  In a continuous-time setting with a finite state space, \cite{neumann2023nonlinear} provides conditions that imply the uniqueness of an invariant measure, based on specific assumptions about differentiability and non-singularity related to the generator of the Markov chain. Furthermore,  \cite{neumann2023nonlinear} illustrates peculiar behaviors exhibited by nonlinear Markov chains in continuous-time through several examples. Unlike prior works that depend heavily on differentiability or contraction conditions, our results focus on nonlinear Markov chains with an aggregator structure and leverage monotonicity conditions instead. This approach enables us to apply our uniqueness result in settings that previous methods cannot address, such as the applications in Section \ref{sec:applications} we described above. 




\section{Model and Definitions}

This section introduces the model and preliminaries.



\subsection{Nonlinear Markov Chains with an Aggregator} \label{sec:setting}

Let $S$ be a polish space and $\mathcal{B}(S)$ be the Borel $\sigma$-algebra on $S$. We denote by $\mathcal{P}(S)$ the space of all probability measures on the measurable space $(S,\mathcal{B}(S))$. We study 
 the properties of the nonlinear Markov chain $(X_{t})_{t \in \mathbb{N}}$ on $S$  given by
\begin{equation} \label{Eq:w}
X_{t+1} = w(X_{t},H(\mu _{t}), \epsilon _{t+1})
\end{equation}
where $w: S \times \mathcal{H} \times E \rightarrow S$ is a measurable function, $\mu _{t}$ is the law of $X_{t}$, $H: \mathcal{P}(S) \rightarrow \mathbb{R}$ is a measurable function that is called an aggregator, $\mathcal{H} = \{ H(\mu): \mu \in \mathcal{P}(S) \} $ is the image of $H$, and $(\epsilon_{t})_{t \in \mathbb{N}}$ are independent and identically distributed (I.I.D) random variables that take values in a polish space $E$ with a law $\phi$. 


Let $Q$ be the nonlinear Markov kernel that describes the transitions of the nonlinear Markov chain  $(X_{t})_{t \in \mathbb{N}}$, i.e., 
\begin{equation}
    Q(x,h,B) = \phi (\epsilon \in E : w(x,h,\epsilon) \in B)
\end{equation}
for all $B \in \mathcal{B}(S)$, $x \in S$, $h \in \mathcal{H}$. That is, $Q(x,h,B)$ is the probability that the next period's state would lie in the set $B$ when the current state is $x$ and the current aggregator is $h$. We define the operator \( T: \mathcal{P}(S) \rightarrow \mathcal{P}(S) \) by
$$
T\mu(B) = \int_{S} Q(x, H(\mu), B) \, \mu(dx)
$$
for every measurable set \( B \in \mathcal{B}(S) \). A probability measure \( \mu \in \mathcal{P}(S) \) is an invariant distribution of \( Q \) if it satisfies \( T\mu = \mu \), meaning that \( \mu \) is a fixed point of the operator \( T \).




We are interested in finding conditions that imply that $T$ has a unique fixed point. The operator $T$ is nonlinear and generally not a contraction so standard methods cannot be applied. Instead, we prove uniqueness by leveraging monotonicity conditions over the nonlinear Markov kernel $Q$ that we describe in Section \ref{sec:definition}. 


\subsection{Preliminaries} \label{sec:definition} 

We assume throughout the paper that $S$ is endowed with a closed partial order $ \geq $.\footnote{The partial order $ \geq $ on $S$ is closed if $x_{n} \geq y_{n}$ for all $n$, $y_{n},x_{n} \in S$, $y_{n} \rightarrow y$ and $x_{n} \rightarrow x$, $y,x \in S$, imply $x \geq y$. For example, the standard product order on $S \subseteq \mathbb{R}^{n}$ is closed. } 
 We say that a function $f :S \rightarrow \mathbb{R}$ is increasing if $f (y) \geq f (x)$ whenever $y \geq x$. When $S \subseteq \mathbb{R}^{n}$ we will assume that $S$ is endowed with the standard product order unless otherwise stated (that is, $x \geq y$ for $x,y \in \mathbb{R}^{n}$ if $x_{i} \geq y_{i}$ for each $i=1,\ldots,n$). 

The space of probability measures $\mathcal{P} (S)$ is endowed with the weak topology. A sequence of measures $\mu_{n} \in \mathcal{P} (S)$ converges weakly to $\mu \in \mathcal{P} (S)$ if for all bounded and continuous functions $f :S \rightarrow \mathbb{R}$ we have
\begin{equation*}\underset{n \rightarrow \infty }{\lim }\int _{S}f (s) \mu_{n} (d s) =\int _{S}f (s) \mu (d s).
\end{equation*}

Let  $D \subseteq \mathbb{R}^{S}$ be a convex set where $\mathbb{R}^{S}$ is the set of all functions from $S$ to $\mathbb{R}$. When $\mu _{1}$ and $\mu _{2}$ are probability measures on $(S ,\mathcal{B}(S))$, we write $\mu _{2} \succeq _{D}\mu _{1}$ if \begin{equation*}\int _{S}f(s)\mu _{2}(ds) \geq \int _{S}f(s)\mu _{1}(ds)
\end{equation*}for all Borel measurable functions $f \in D$ such that the integrals exist. With slight abuse of notation, for two random variables $X,Y$, we write $X \succeq_{D} Y$ if $\mu _{X} \succeq _{D} \mu_{Y} $ where $ \mu_{X}$ is the law of $X$ and $\mu_{Y}$ is the law of $Y$.  



The binary relation $\succeq _{D}$ is called a stochastic order. When $D$ is the set of all increasing functions on $S$, we write $\mu _{2} \succeq _{SD}\mu _{1}$ and say that $\mu _{2}$ first order stochastically dominates $\mu _{1}$. 


To prove that $T$ has a unique fixed point it is  convenient  to assume that the linear Markov kernel  $Q(x,h,\cdot)$ has a unique invariant distribution when the aggregator $h \in \mathcal{H}$ is fixed. That is, the operator $M_{h} :\mathcal{P} (S) \rightarrow \mathcal{P} (S)$ has a unique fixed point where $M_{h}$ is the operator given by 
\begin{equation*}M_{h} \theta  (B) =\int _{S}Q (x ,h ,B) \theta  (d x)
\end{equation*}
that is parameterized by a fixed aggregator $h \in \mathcal{H}$. 


When \( T \) does not depend on the aggregator (that is, when \( T\mu = M_h\mu \) for some fixed \( h \)), the operator \( T \) simplifies to \( M_h \). In this case, the behavior of \( T \) is entirely determined by \( M_h \), meaning that if \( M_h \) has multiple invariant distributions, so does \( T \).

\begin{definition} \label{def:propU} (Property (U)). 
We say that $Q$ satisfies Property (U) if for any $h \in \mathcal{H}$, the operator $M_{h}$ has a unique fixed point $\mu _{h}$. 
\end{definition}


 
A stronger version of Property (U), which we refer to as Property (C), states that the Markov kernel $M_{h}^{n} \theta $ converges weakly to $\mu _{h}$ for any probability measure $\theta  \in \mathcal{P} (S)$ where $M_{h}^{n}$ means applying the operator $M_{h}$, $n$ times. 



\begin{definition} \label{def:Xerg} (Property (C)). 
We say that $Q$ satisfies Property (C) 
if $Q$ satisfies Property (U) and  $M_{h}^{n} \theta $ converges weakly to $\mu _{h}$ for any probability measure $\theta  \in \mathcal{P} (S)$ and any $h \in \mathcal{H}$ where $\mu _{h}$ is the unique fixed point of $M_{h}$. 
\end{definition}
Under certain conditions, Property (C) can be established using standard results regarding the stability of Markov chains in general state spaces (e.g., Theorem 13.3.1 or Theorem 16.2.3 in  \cite{meyn2012markov}). When the state space $S$ is finite, Property (C) can be established by assuming that $M_{h}$ is irreducible and aperiodic and Property (U) can be established by assuming that $M_{h}$ is irreducible.  

 
The key assumption that implies that the operator $T$ has at most one fixed point relates to the following monotonicity and preservation properties. 
\begin{definition}
Let $D \subseteq \mathbb{R}^{S}$. 

We say that $Q$ is $D$-decreasing  if for each $x \in S$, we have $Q (x ,h_{1} , \cdot ) \succeq  _{D}Q (x , h_{2}, \cdot )$ whenever $h_{2} \geq h_{1}$, $h_{1},h_{2} \in \mathcal{H}$.

We say that  $Q$ is increasing in $x$  with respect to $\succeq _{D}$ if for each  $h \in \mathcal{H}$, we have $Q (x _{2},h , \cdot ) \succeq  _{D}Q (x_{1} ,h, \cdot )$ whenever $x_{2} \geq x_{1}$.

We say that $Q$ is $D$-preserving if for all  $h \in \mathcal{H}$ the function 
\begin{equation*}
    v(x):=\int f (y) Q(x ,h ,dy) 
\end{equation*}
is in $D$ whenever $f \in D$. 


\end{definition}

Note that when $D$ is the set of all increasing functions then $\succeq _{D}$  reduces to the standard stochastic dominance order and $Q$ is increasing in $x$  with respect to $\succeq _{D}$ if and only if $Q$ is $D$-preserving (see, for example, Corollary 3.9.1 in \cite{topkis2011supermodularity}). In the case that $Q$ is increasing in $x$, Property (C) can be established using results from the theory of monotone Markov chains. These results typically require a splitting condition (see \cite{bhattacharya1988asymptotics},  \cite{kamihigashi2014stochastic}, and \cite{light2024note}) and hold in a wide range of applications.  




We say that $H$ is increasing with respect to $\succeq _{D}$ if $H(\mu_{2}) \geq H(\mu_{1})$ whenever $ \mu_{2} \succeq _{D} \mu_{1}$. 



 
A stochastic order $\succeq_{D}$ is said to be closed with respect to weak convergence if $\mu^{1}_{n} \succeq_{D} \mu^{2}_{n} $ for all $n$, $\mu^{1}_{n}$ converges weakly to $\mu^{1}$, and $\mu^{2}_{n}$ converges weakly to $\mu^{2}$ imply $\mu^{1} \succeq_{D} \mu^{2}$. Many stochastic orders of interest are closed with respect to weak convergence, e.g., the standard stochastic dominance order $\succeq_{SD}$. For a textbook treatment  of the closure properties of stochastic orders see, for example, Theorems 4.B.10  and 3.A.5  in \cite{shaked2007stochastic} .   

We say that $H$ is continuous if  $ \lim _{n \rightarrow \infty} H(\mu_{n}) = H(\mu)$ whenever $\mu_{n}$ converges weakly to $\mu$. We say that $Q$ is continuous if $Q(x_{n},h_{n}, \cdot)$ converges weakly to  $Q(x,h, \cdot )$ whenever $(x_{n},h_{n}) \rightarrow (x,h)$. Also, for a parametrized random variable \( Y(z) \) depending on a parameter $z \in \mathbb{R}^{n}$, we say that \( Y(z) \) is continuous in \( z \) if \( z_n \to z \) implies that the law of \( Y(z_n) \) converges weakly to the law of \( Y(z) \).


Recall that a partially ordered set $(Z , \geq )$ is said to be a lattice if for all $x ,y \in Z$, $\sup \{x ,y\}$ and $\inf \{y ,x\}$ exist in $Z$. $(Z , \geq )$ is a complete lattice if for all non-empty subsets $Z^{ \prime } \subseteq Z$ the elements $\sup Z^{ \prime }$ and $\inf Z^{ \prime }$ exist in $Z$.



\section{Main Results}



In this section we present our main results.  In Section \ref{sec:unique} we present the monotonicity conditions that imply that  the nonlinear Markov chain has at most one invariant distribution. In Section \ref{Section: Existence} we provide two distinct existence results. In Section \ref{sec:flexibility} we provide examples that demonstrate the flexibility of the monotonicity conditions. In Section \ref{sec:necessity} we show that these monotonicity conditions are necessary to prove uniqueness in our setting and in Section \ref{sec:convergence} we show that the nonlinear Markov chain does not necessarily converge to the unique invariant distribution even for a very simple two-state case. In Section \ref{sec:compute} we provide a simple method to compute the invariant distribution. In Section \ref{sec:localresults} we provide local uniqueness results. 


\subsection{Uniqueness Theorem} \label{sec:unique}
In this section we provide the monotonicity conditions that ensure \( Q \) has at most one invariant distribution. 
The proofs of all the paper's results are deferred to the Appendix. 



\begin{theorem} \label{Theorem: unique}
Let $D \subseteq \mathbb{R}^{S}$ be a non-empty set such that $H$ is increasing with respect to $\succeq  _{D}$. Assume that $Q$ is $D$-preserving and $D$-decreasing.

Assume that either of the following conditions hold: 

(i) $Q$ satisfies Property (C) and $\succeq_{D}$ is closed with respect to weak convergence. 

(ii) $Q$ satisfies Property (U) and $(\mathcal{P}(S),\succeq_{D})$ is a complete lattice. 


Then $Q$ has at most one invariant distribution. 
\end{theorem}

The conditions in Theorem \ref{Theorem: unique}, which establish that \( Q \) has at most one invariant distribution, do not rely on compactness or continuity assumptions, and hence, the existence of an invariant distribution is not guaranteed.
In Section \ref{Section: Existence}, we present conditions that ensure the existence of an invariant distribution.


We now provide a few comments on Theorem \ref{Theorem: unique}.


\textbf{Applications:} In many applications, verifying whether the nonlinear Markov kernel \( Q \) is both \( D \)-preserving and \( D \)-decreasing is straightforward. In Section \ref{sec:applications}, we present several applications of Theorem \ref{Theorem: unique}, including queueing systems and the dynamic evolution of wealth distributions. In these cases, the monotonicity properties of \( Q \) naturally arise from the underlying behavioral or economic assumptions governing the dynamics of the stochastic systems.



\textbf{Local Results:} The proof of Theorem \ref{Theorem: unique} indicates that it suffices to assume Property (U) only for \( h \in \mathcal{H} \), where \( h = H(\mu) \) and \( \mu \) is an invariant distribution of \( Q \). This relaxation means that Property (U) does not need to hold for all \( h \in \mathcal{H} \), which can simplify the verification of the condition in specific applications.


The monotonicity conditions required for proving Theorem \ref{Theorem: unique} are global, meaning they must hold across all probability measures on $S$. However, in certain applications, only a subset of these probability measures includes relevant candidates for invariant distributions or is of particular interest. In Proposition \ref{Prop:local}, introduced in Section \ref{sec:localresults}, we provide a local version of Theorem \ref{Theorem: unique} that allows for establishing uniqueness within a restricted set of probability measures. 



\textbf{The finite case and complete lattices:}  Condition (ii) of Theorem \ref{Theorem: unique} is particularly useful for the case that $S$ is a finite set or a compact set in $\mathbb{R}$.
For example, suppose that $S=\{s_{1},\ldots,s_{n}\}$ is an ordered set of numbers with $s_{1} \leq s_{2} \leq ... \leq s_{n}$ and $\mathcal{P}(S)$ is endowed with the standard stochastic dominance order $\succeq  _{SD}$. It is immediate to see that $(\mathcal{P}(S),\succeq_{SD})$ is a complete lattice with
$$ \sup \{ \mu , \lambda \}  (\{s_{t}, \ldots, s_{n} \})=  \max \{ \mu   (\{s_{t}, \ldots, s_{n} \}) ,\lambda   (\{s_{t}, \ldots, s_{n} \}) \} $$
and 
$$ \inf \{ \mu , \lambda \} (\{s_{t}, \ldots, s_{n} \})=  \min \{ \mu (\{s_{t}, \ldots, s_{n} \}) ,\lambda  (\{s_{t}, \ldots, s_{n} \}) \} $$
for all $t =1,\ldots, n$  
(recall that $\mu  \succeq  _{SD} \lambda$ if and only if for every upper set $B$ we have $\mu (B) \geq \lambda (B)$ where $B \in \mathcal{B} (S)$ is an upper set if $x_{1} \in B$ and $x_{2} \geq x_{1}$ imply $x_{2} \in B$). In a similar fashion, $(\mathcal{P}(S),\succeq_{SD})$ is a complete lattice  when $S$ is a compact set in $\mathbb{R}$ when $\mathbb{R}$ is endowed with the standard partial order. For this result and other examples of stochastic orders that generate lattices of probability measures see  \cite{muller2006stochastic}. 





\subsection{Existence of Invariant Distribution} \label{Section: Existence}
In this section, we study the existence of an invariant distribution. We present two distinct results. 
The first existence result, Proposition \ref{Prop:existence}, holds  for the case where $S$ is compact and $Q$ and $H$ are continuous and follows from standard fixed-point arguments. Extending this existence result to non-compact state spaces remains an interesting avenue for future research.



\begin{proposition} \label{Prop:existence}
Suppose that $H$ and $Q$ are continuous and that $S$ is compact. Then $Q$ has an invariant distribution. 
\end{proposition}

The second existence result relies on continuity of $H$ and $Q$, a boundedness condition for the aggregator and a tightness condition instead of compactness of the state space. This result is especially useful in applications where the state space is not finite or compact, such as the queuing systems studied in Section \ref{sec:applications} or the autoregressive processes discussed in Example \ref{example:flexible}.

Recall that a sequence of probability measures $\{\mu_{k}\}$ on $S$ is called tight if for all $\epsilon > 0$ there is a compact subset $K_{\epsilon}$ of $S$ such that $\mu_{k}(S \setminus K_{\epsilon}) \leq \epsilon$ for all $k$. Tightness is a standard assumption in order to ensure the existence of a invariant distribution in the usual linear Markov chain theory (see \cite{meyn2012markov} for an extensive study of invariant distributions).

\begin{proposition} \label{Prop:existence2}
Suppose that $H$ and $Q$ are continuous and that Property (U) holds. In addition, assume that there exist $h',h'' \in \mathbb{R}$, $h''>h'$, such that $h'' \geq H(\mu_{h''})$ and $h' \leq H(\mu_{h'})$ where $\mu_{h}$ is the unique fixed point of $M_{h}$ (see Definition \ref{def:propU}) and $\mu_{h} \in \mathcal{P}(S)$ for all $h \in [h',h'']$. 
Assume that for any sequence $\{h_{n}\} $, $h_{n} \in [h',h'']$ that converges to some $h$, the sequence $\{\mu_{h_{n}} \}$ is a tight sequence of probability measures. 

Then $Q$ has an invariant distribution.
\end{proposition}


The existence result in Proposition \ref{Prop:existence2}
 not only establishes the existence of an invariant distribution but also provides the basis for an algorithm to finding this distribution. Specifically, we provide a bisection method to find the invariant distribution (see Section \ref{sec:compute}) which complements the theoretical existence results. 




%Specifically, the function \( f(h) = h - H(\mu_h) \) defined in the proof has the properties necessary to apply the bisection method. The bisection method works by iteratively narrowing down an interval \([h', h'']\) where \( f(h') < 0 \) and \( f(h'') > 0 \), to locate a root of \( f \), say \( h^* \), for which \( f(h^*) = 0 \).

%Since a root of \( f \) corresponds to an invariant distribution \( \mu_{h^*} \) of \( Q \), the bisection method provides an efficient way to compute the invariant distribution. By updating the bounds of the interval \([h', h'']\) at each iteration, the algorithm converges to \( h^* \), where the corresponding probability measure \( \mu_{h^*} \) is the invariant distribution of \( Q \). 

\subsection{Flexibility of the Monotonicity Conditions} \label{sec:flexibility}

In applications, it may seem natural to select $D$ as the set of all increasing functions. 
However, the versatility in choosing the set $D$ in Theorem \ref{Theorem: unique} is fruitful for proving uniqueness for various nonlinear Markov chains. 
 Carefully selecting an appropriate set $D$ can be essential for effectively applying Theorem \ref{Theorem: unique}. The following examples  demonstrate the importance of this choice. 

\begin{example} \label{example:flexible} (Flexibility of the set $D$).
(i) Consider the following nonlinear Markov chain 
\begin{equation} \label{Eq:ex_1(i)} X_{t+1} = a X_{t} - H(\mu_{t}) + \epsilon_{t+1} \end{equation}
on $\mathbb{R}$ where $0<a<1$, $\epsilon _{t}$ are I.I.D random variables with finite expectations and variances, and  the aggregator is given by $H(\mu) = \int m(x)  \mu(dx)$ for some increasing, continuous and bounded function $m:\mathbb{R} \rightarrow \mathbb{R}$. Then, we can use Theorem \ref{Theorem: unique}  to show that the nonlinear Markov chain $(X_{t})_{t \in \mathbb{N}}$ has at most one invariant distribution and Proposition \ref{Prop:existence2} to show that an invariant distribution exists.  The proofs of the claims are provided in the appendix. 
\claim \label{claim1} {The Markov chain given in Equation (\ref{Eq:ex_1(i)}) has a unique invariant distribution.}

\

Now consider the nonlinear Markov chain \begin{equation}\label{eq:ex_1_2dim} (X_{1,t+1},X_{2,t+1}) =  (a X_{1,t} - H(\mu_{t}) + \epsilon_{1,t+1}, k(X_{2,t}) + \epsilon_{2,t+1})\end{equation} 
on $\mathbb{R}^{2}$ where $0<a<1$, $\epsilon _{1,t} , \epsilon_{2,t}$ are I.I.D random variables with finite expectations and variances, $k$ is a function that is continuous and bounded but not increasing, and the aggregator is given by $H(\mu) = \int m(x_{1})  \mu(d(x_{1},x_{2}))$ for some increasing continuous and bounded function $m:\mathbb{R} \rightarrow \mathbb{R}$. In this case,   $Q$ is not necessarily $D$-preserving when $D$ is the set of all increasing functions because $k$ is not increasing. However, if we let $D$ to be the set of all the functions that are increasing in the first argument, it can be verified that $Q$ is both $D$-preserving and $D$-decreasing (see the claim below).  
\claim \label{claim2} {Consider the Markov chain given in Equation (\ref{eq:ex_1_2dim}). Then it has a unique invariant distribution if Property (C) holds.\footnote{Establishing Property (C) for such Markov chains has been extensively studied in the literature \citep{meyn2012markov} so we omit the details for brevity. }} 
\end{example}

\begin{example}
(Flexibility of the set $D$). Consider the $n$-dimensional nonlinear Markov chain on $\mathbb{R}^{n}$ 
with 
\begin{equation} \label{Eq_Ex1_ii} X_{i,t+1} =  a_{i} X_{i,t} - \beta_{i} H(\mu_{t}) + \epsilon_{i,t+1} \end{equation}
for $i=1,\ldots,n$ where $0<a_{i}<1$, $\epsilon _{i,t} $ are I.I.D random variables with finite expectations and variances,  and the aggregator is given by 
$H(\mu)  = \int \sum _{i=1}^{n} 
 \gamma_{i} x_{i}\mu(d(x_{1},x_{2},\ldots,x_{n}))$ for some vector $\gamma = (\gamma_{1},\ldots,\gamma_{n})$ in $\mathbb{R}^{n}$. 

 Let $O$ be the set of vectors in $\mathbb{R}^{n}$ such that $x_{i}$ is non-negative for an odd $i$ and non-positive for an even $i$, that is, $O = \{ x \in \mathbb{R}^{n}: x_{i} \geq 0, i \text{ is odd }, x_{i} \leq 0,  i \text{ is even}\} $. Assume that $\beta = (\beta _{1} ,\ldots,\beta_{n})$ and $\gamma = (\gamma_{1} , \ldots ,\gamma_{n})$ are in $O$. It is easy to see that we cannot use $D$ as the set of all increasing functions in order to apply Theorem \ref{Theorem: unique}. However, 
 consider the set of functions $D$ such that $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ is in $D$ if $f(x) = \sum _{i=1}^{n} y_{i}x_{i} + c$ for some $y \in O$ and $c \in \mathbb{R}$. Under this set of functions $D$, we show that we can use Theorem \ref{Theorem: unique} to prove that the nonlinear Markov chain has at most one invariant distribution. 
 \claim \label{claim3} {The nonlinear Markov chain given in Equation (\ref{Eq_Ex1_ii}) has a unique invariant distribution.} 
\end{example}

 

We note that the contraction conditions  given in \cite{butkovsky2014ergodic} are generally not satisfied for the nonlinear Markov chains in the previous examples (see also the simple two-state nonlinear Markov chain we provide in Example \ref{Example:convergence}) so we should not generally expect the nonlinear Markov chain to converge to the unique invariant distribution. Despite this, we provide a general method to find the unique invariant distribution in Section \ref{sec:compute}.


\subsection{Necessity of the Monotonicity Conditions} \label{sec:necessity} 
In this section, we show that without the \( D \)-preserving and \( D \)-decreasing properties, there are simple examples in which uniqueness of the invariant distribution fails.



\begin{example} \label{example:decreasing}  ($Q$ is not $D$-decreasing). Suppose that $S=\{0,1\}$ endowed with the standard order ($1 \geq 1, 0 \geq 0,  1 > 0$) and $H(\mu) = \mu (\{1\}) $. Assume that $D$ is the set of all increasing functions so $\succeq_{D}$ is the standard stochastic dominance $\succeq_{SD}$. Note that  $H$ is increasing with respect $\succeq_{SD}$.   

 Consider the nonlinear Markov chain 
\[
Q'  = 
        \begin{blockarray}{c@{\hspace{2pt}}rr@{\hspace{3pt}}}
         & 0   & 1   \\
        \begin{block}{r@{\hspace{2pt}}|@{\hspace{2pt}}
    |@{\hspace{2pt}}rr@{\hspace{2pt}}||}
        0 & 1- \min(0.5,\mu (\{1\} )) & \min(0.5,\mu (\{1\})) \\
        1 & 0.5 & 0.5  \\
        \end{block}
    \end{blockarray}
\]
It is immediate that $ \pi (\{ 1 \}) = 1/2 = \pi(\{0\})$ and $ \pi ' (\{ 1 \}) = 0,  \pi '  (\{ 0 \}) = 1$ are invariant distributions of $Q'$. 
It is easy to verify that $Q'$ satisfies property (ii) of Theorem 1, and that $Q'$ is $D$-preserving but not $D$-decreasing. Hence all the conditions of Theorem 1 are satisfied except for the condition that $Q'$ is $D$-decreasing and $Q'$ has two invariant distributions. 

\end{example}

\begin{example} \label{example:preserving}
 ($Q$ is not $D$-preserving). Suppose that $S=\{0,1,2\}$ is endowed with the standard order and $H(\mu) = \mu (\{1\}) + \mu (\{2\}) $. Assume that $D$ is the set of all increasing functions so $\succeq_{D}$ is the standard stochastic dominance $\succeq_{SD}$. Note that  $H$ is increasing with respect $\succeq_{SD}$.   


Consider the nonlinear Markov chain  
\[
Q'' = 
        \begin{blockarray}{c@{\hspace{1pt}}rrr@{\hspace{3pt}}}
         & 0   & 1   & 2 \\
        \begin{block}{r@{\hspace{1pt}}|@{\hspace{1pt}}
    |@{\hspace{1pt}}rrr@{\hspace{1pt}}|@{\hspace{1pt}}|}
        0 & 1/3 & 1/3 & 1/3 \\
        1 & 0 & H(\mu) & 1 - H(\mu) \\
        2 & H(\mu)  & 0   & 1-H(\mu)   \\
        \end{block}
    \end{blockarray}
\]

The distributions $ \pi (\{ 0 \}) =  \pi (\{ 1 \}) =  \pi (\{ 2 \}) = 1/3$ and $ \pi' (\{ 0 \}) = 0,  \pi' (\{ 1 \}) = 1,  \pi' (\{ 2 \})=0 $  are invariant distributions of $Q''$. 
It is easy to see that the Markov chain $Q''$ satisfies property (ii) of Theorem 1 and is $D$-decreasing. In addition, $Q''$ is not increasing in $x$, and hence, is not $D$-preserving as $Q''(1,h , \{1,2\} ) > Q''(2,h , \{1,2\} )$ for any $h >0$. Hence all the conditions of Theorem 1 are satisfied except to the condition that $Q''$ is $D$-preserving and $Q''$ has two invariant distributions. 

\end{example}


%\begin{example}
%Consider a Markov chain on  $\{0,1,2,\ldots,\}$ such that $X_{t+1}= X_{t} + 1$ with probability $\min ( \max (q_{1},\mathbb{E}(X_{t})) , q_{2} ) $ and $X_{t+1}= \max \{ X_{t} - 1 , 0 \}$ with probability $1 - \min ( \max (q_{1},\mathbb{E}(X_{t})) , q_{2} ) $  where $ q_{1} < q_{2} < 1/2$. 
%\end{example}



\subsection{Non-Convergence to the Invariant Distribution} \label{sec:convergence} Theorem \ref{Theorem: unique} and Proposition \ref{Prop:existence} provide sufficient conditions for the uniqueness of an invariant distribution for the nonlinear Markov kernel $Q$. However, these results do not provide conditions under which the sequence of measures $\mu_{t}$  converges weakly to the unique invariant distribution of $Q$.  Unfortunately, the following example shows that even in a very simple case, the monotonicity conditions that imply uniqueness do not imply convergence.  This is in sharp contrast with the contraction approach to study the invariant distributions of nonlinear Markov chain that guarantees convergence (e.g., \cite{butkovsky2014ergodic}).



\begin{example} \label{Example:convergence} ($\mu_{t}$ does not converge to the unique invariant distribution).
Suppose that $S=\{0,1\}$ is endowed with the standard order and $H(\mu) = \mu (\{1\}) $. Assume that $D$ is the set of all increasing functions so $\succeq_{D}$ is the standard stochastic dominance $\succeq_{SD}$. Note that $H$ is increasing with respect $\succeq_{SD}$.    Consider the nonlinear Markov chain 
\[
Q = 
        \begin{blockarray}{c@{\hspace{2pt}}rr@{\hspace{3pt}}}
         & 0   & 1   \\
        \begin{block}{r@{\hspace{2pt}}|@{\hspace{2pt}}
    |@{\hspace{2pt}}rr@{\hspace{2pt}}||}
        0 & \mu (\{1\} ) & \mu (\{0\}) \\
        1 & \mu (\{1\}) & \mu (\{0\})  \\
        \end{block}
    \end{blockarray}
\]
It is easy to see that $ \pi (\{ 1 \}) = 1/2 = \pi(\{0\})$ is the unique invariant distributions of $Q$ and $Q$ 
 satisfies all the conditions of Theorem 1. Note that for any initial distribution $\mu_{1} (\{1\} )  = \gamma $ and $\mu_{1} (\{ 0 \}) = 1-\gamma$ with $\gamma \neq 1/2$, $\mu_{t}$ does not converge to $\pi$ as $\mu_{t} (\{1\} )  = \gamma $ and $\mu_{t} (\{ 0 \}) = 1-\gamma$ for an odd $t$ and $\mu_{t} (\{1\} )  = 1- \gamma $ and $\mu_{t} (\{ 0 \}) = \gamma$ for an even $t$. 
\end{example}



 Example \ref{Example:convergence} illustrates that the sequence of measures $\{ \mu _{t} \}$ does not converge to the unique invariant distribution in a simple example showing  that we can't expect the sequence of measures $\{ \mu _{t} \}$ to converge in typical applications. In that example, $ \sum _{t=1}^{T} \mu_{t} /T$ converges to the unique invariant distribution. However, Example \ref{Example: Average Convergence} shows that this is not always the case even when the conditions for uniqueness provided in Theorem \ref{Theorem: unique} hold. 

 \begin{example} \label{Example: Average Convergence}
     ($\sum _{t=1}^{T} \mu_{t} /T$ does not converge to the unique invariant distribution).
Suppose that $S=\{0,1\}$ is endowed with the standard order and $H(\mu) = \mu (\{1\}) $. Assume that $D$ is the set of all increasing functions so $\succeq_{D}$ is the standard stochastic dominance $\succeq_{SD}$. Note that $H$ is increasing with respect $\succeq_{SD}$.    Consider the nonlinear Markov chain 
\[
Q = 
        \begin{blockarray}{c@{\hspace{2pt}}rr@{\hspace{3pt}}}
         & 0   & 1   \\
        \begin{block}{r@{\hspace{2pt}}|@{\hspace{2pt}}
    |@{\hspace{2pt}}rr@{\hspace{2pt}}||}
        0 & \mu (\{1\} ) & \mu (\{0\}) \\
        1 & 1-f( \mu (\{0\})) & f( \mu (\{0\})  \\
        \end{block}
    \end{blockarray}
\]
with 
\begin{align*}
    f(x ) & = x1_{ \{ x \leq 0.3 
\} } + (1.2x  -0.06) 1_{ \{ 0.3 < x \leq  0.5  \} } 
+  (0.8x  + 0.14) 1_{ \{ 0.5 < x \leq  0.7 \}  } +  x1_{ \{x > 0.7 
\} }
\end{align*} 
for $x \in [0,1]$. 
Note that $f (x ) \geq x$ and $f$ is increasing, and hence, the conditions of Theorem \ref{Theorem: unique} hold and there exists at most one invariant distribution. In addition, $f$ is continuous so from Proposition \ref{Prop:existence} the nonlinear Markov kernel $Q$ has a unique invariant distribution. 



As in Example \ref{Example:convergence}, if the initial distribution is $\mu _{1} (\{0\} )  = 0.7$, then $\mu _{2}(\{0\})  = 0.3$, and $\mu_{3} (\{0\})  = 0.7$ and so on. 
But $\pi ( \{ 0 \} ) = \pi (\{1\}) = 1/2 $ is not an invariant distribution so $\sum _{t=1}^{T} \mu_{t} /T$ does not converge to the invariant distribution.  

 \end{example}

%Designing algorithms that ensure convergence to the unique invariant distribution of 
%the nonlinear Markov kernel $Q$ remains an interesting open question.



\subsection{Computation of the  Invariant Distribution } \label{sec:compute}


As discussed in the introduction, it is essential to develop a method capable of computing the invariant distribution of the nonlinear Markov chain. In this section, under the conditions of Proposition \ref{Prop:existence2},  we show that a straightforward bisection method achieves this computational goal.  In this method, we use bisection method for the function $f(h) = h - H(\mu_{h})$ on the interval $[h',h'']$  to find the root of $f$. 
   We now describe a simple algorithm to compute the invariant distribution of $Q$. 
   
 \begin{algorithm}[H]
\caption{Bisection Method for finding an Invariant Distribution}
\label{alg:bisection_simplified}
\begin{algorithmic}[1]
\State \textbf{Input:} $f(h) = h - H(\mu_h)$,  $h_1 = h'$, $h_2 = h''$
\State \textbf{Output:} $h^*$ such that $f(h^*) = 0$, and invariant distribution $\mu_{h^*}$

\State Initialize $a = h_1$, $b = h_2$, set $n = 0$
\While{$f(a) \neq 0$ and $f(b) \neq 0$}
    \State Set $h_n = \frac{a + b}{2}$  
    \State Solve for $\mu_{h_n}$: $\mu_{h_n}(B) = \int Q(x, h_n, B) \mu_{h_n}(dx)$
    \State Compute $f(h_n) = h_n - H(\mu_{h_n})$
    
    \If{$f(h_n) = 0$}
        \State \textbf{Return:} $h^* = h_n$, $\mu_{h^*} = \mu_{h_n}$
    \ElsIf{$f(h_n) < 0$}
        \State Set $a = h_n$
    \Else
        \State Set $b = h_n$
    \EndIf
    
    \State Increment $n = n + 1$
\EndWhile

\State \textbf{Return:} $h^* = \frac{a + b}{2}$ and $\mu_{h^*}$
\end{algorithmic}
\end{algorithm}



  \begin{proposition} \label{prop:compute}
   Suppose the assumptions of Proposition \ref{Prop:existence2} hold. Let $\{h_n\}$ be the sequence generated by Algorithm 1 with $h'$ and $h''$ as defined in Proposition \ref{Prop:existence2}. Then, $\{h_n\}$ converges to $h^*$, and $\mu_{h^*}$ is an invariant distribution of $Q$.
   \end{proposition}





We note that under the conditions of Theorem \ref{Theorem: unique}, Algorithm 1 finds the unique invariant distribution of $Q$. 
In this case, it is typically immediate to compute the points $h_{1}$ and $h_{2}$ by using the monotonicity conditions. For example, consider the finite case $S = \{s_{1} ,\ldots , s_{n} \}$ with the standard order $s_{i} \geq s_{j}$ whenever $i \geq j$ and $\mathcal{P}(S)$ endowed with the standard stochastic dominance order $\succeq_{SD}$. Then $h_{1}$ and $h_{2}$ can be easily computed by applying the function $H$ to the Dirac measure centered on $s_{n}$  and the Dirac measure centered on $s_{1}$. For example, if $H (\mu)$ is the expected value operator, i.e., $H(\mu) = \sum _{s \in S} s \mu(\{s \}) $, then $h_{1}=s_{1}$ and $h_{2}=s_{n}$. Hence, the initial interval chosen by the algorithm is $[s_{1},s_{n}]$. 

As an illustration, consider Example \ref{Example:convergence} where we provided a simple Markov chain that does not converge to the unique invariant distribution. We first identify the interval $[0,1]$ and $h_{1}=0$, $h_{2}=1$ as explained above.  It is immediate that $H(\mu_{h}) = 1-h$, and hence, $f(h) = h - (1-h)) = 2h-1$. Thus, the algorithm generates $h_{3} = 1/2$ which is the root of $f$ so the algorithm converges in the first iteration and the unique invariant distribution is $\mu_{h_{3}}$.  


For the finite state space described above with $n$ variables, the method described in Algorithm 1 is generally efficient and straightforward to implement. In each iteration, the algorithm solves a linear equation with $n$ variables and $n+1$ constraints (enforcing that $\mu_{h}$ is a probability measure) to find the invariant distribution $\mu_{h}$. Then, the function $f$ is evaluated to proceed with the bisection method.

This approach is consistent with many well-known algorithms for solving hard optimization problems, where each iteration involves solving a simpler subproblem. For example, in cutting-plane methods to solve integer programming problems, each iteration requires solving a linear program to refine the feasible region. In the case of Algorithm \ref{alg:bisection_simplified}  described above, each iteration requires solving a linear equation in order to find the solution  of the nonlinear equation that describes the invariant distribution of the nonlinear Markov kernel $Q$. 

%We provide a detailed example in Section ?. 

 \subsection{Local Results} \label{sec:localresults}


In this section, we present a localized version of Theorem \ref{Theorem: unique}. Rather than applying the monotonicity conditions and Properties (U) and (C) to all probability measures as in Theorem 1, we introduce localized versions of these conditions that apply only in certain regions of the probability space. These local versions pertain only to a particular subset of probability measures that have specific interest. These conditions ensure that, within this subset, $Q$ has at most one invariant distribution. This subset may encompass probability measures that naturally emerge as candidates for invariant distributions or probability measures that are relevant for an application of interest. For a non-empty subset $\mathcal{W}$ of $\mathcal{P}(S)$ let $\mathcal{H}_{\mathcal{W}} = \{ H(\mu) : \mu \in \mathcal{W} \}$ . 





\begin{definition} Let $\mathcal{W}$ be a non-empty subset of $\mathcal{P} (S)$  

(i) We say that $Q$ satisfies Property (U) on $\mathcal{W}$  if for any $h \in \mathcal{H}_{\mathcal{W}}$, the operator $M_{h}$ has a unique fixed point $\mu _{h}$. 

(ii) We say that $Q$ satisfies Property (C) on $\mathcal{W}$
if $Q$ satisfies Property (U) on $\mathcal{W}$ and  $M_{h}^{n} \theta $ converges weakly to $\mu _{h}$ for any probability measure $\theta  \in \mathcal{W}$ and any $h \in \mathcal{H}_{\mathcal{W}}$. 
\end{definition}


 







Similarly, we provide local versions for the monotonicity and preservation properties introduced in Section \ref{sec:definition}. 


\begin{definition}
Let $D \subseteq \mathbb{R}^{S}$. 

We say that $Q$ is $D$-decreasing on $\mathcal{W}$ if for each $x \in S$, we have $Q (x ,h_{1} , \cdot ) \succeq  _{D}Q (x , h_{2}, \cdot )$ whenever $h_{2} \geq h_{1}$, $h_{1},h_{2} \in \mathcal{H}_{\mathcal{W}}$.

We say that $Q$ is $D$-preserving on $\mathcal{W}$ if for all  $h \in \mathcal{H}_{\mathcal{W}}$ the function 
\begin{equation*}
    v(x):=\int f (y) Q(x ,h ,dy) 
\end{equation*}
is in $D$ whenever $f \in D$. 


\end{definition}



The following Proposition generalizes Theorem \ref{Theorem: unique}. 

\begin{proposition} \label{Prop:local}

    
    Let $\mathcal{W}$ be a non-empty subset of  $\mathcal{P}(S)$. 
    Let $D \subseteq \mathbb{R}^{S}$ be a non-empty set such that $H$ is increasing with respect to $\succeq  _{D}$ on $\mathcal{W}$. 


    
    
    Assume that $Q$ is $D$-preserving on $\mathcal{W}$ and $D$-decreasing on $\mathcal{W}$.


Suppose that $M_{h} \theta \in \mathcal{W}$ whenever $\theta \in \mathcal{W}$ and $h \in \mathcal{H}_{\mathcal{W}}$. 

Assume that either of the following conditions hold: 

(i) $Q$ satisfies Property (C) on $\mathcal{W}$ and $\succeq_{D}$ is closed with respect to weak convergence. 

(ii) $Q$ satisfies Property (U)  on $\mathcal{W}$ and $(\mathcal{W},\succeq_{D})$ is a complete lattice. 


Then $Q$ has at most one invariant distribution on $\mathcal{W}$. 
\end{proposition}

The proof of Proposition \ref{Prop:local} is similar to the proof of Theorem \ref{Theorem: unique} and is given in the Appendix.

\section{Applications} \label{sec:applications}

In this section we present our applications. In Section 3.1 we study the invariant distribution of a G/G/1 queueing system where arrivals depend on the expected waiting times. In Section 3.2 we study non-linear equations that do not necessarily satisfy contraction properties and have a unique solution. In Section 3.3 we study the invariant distribution of wealth distributions in dynamic economies where the rate of returns depend on the aggregate savings in the economy. 

\subsection{Strategic Behavior in Queuing Systems} 
 A considerable body of literature exists on strategic behavior in queueing systems. Within this domain, the inter-arrival times often depend on the queue length or expected waiting time, as agents, being strategic, can opt not to join the queue if they foresee an extended waiting period  \citep{hassin2003queue}.  Typically, queueing systems are examined in the steady state, making it essential to study the existence of a unique steady state generated by the system to obtain robust comparative statics results that do not depend on the specific choice of equilibrium. We will now demonstrate how Theorem \ref{Theorem: unique} can be used to establish that there is a unique invariant distribution for the waiting time distribution within a general $G/G/1$ strategic queueing system, wherein the inter-arrival times are contingent on the expected waiting time.\footnote{Other nonlinear Markov chains were analyzed in the strategic queueing literature. For example, \cite{xu2013supermarket} show that a supermarket game where customers strategically choose which queue to join has a unique equilibrium under certain monotonicity conditions. See  \cite{mukhopadhyay2016randomized} and \cite{yang2018mean} for further related models. }

Consider a $G/G/1$ queue where the time between the $t$th and $t+1$th arrivals is given by the random variable $T_{t}$ and the service time of the $t$ customer is given by the random variable $S_{t}$. Because agents are strategic they are less likely to join the queue when the waiting time is longer. We assume that the time between arrivals depends on the expected waiting time, represented as 
$ T_t( \min \{ \mathbb{E}(X_t), M \} ) $, where $ X_t $ is the waiting time of the $ t $th customer and $ M > 0 $ is a positive large upper bound. 
To capture that when the expected waiting time increases, fewer agents join the queue, we assume that $ T_t(h) \succeq_{SD} T_t(h') $ whenever $ h \geq h' $, for $ h, h' \in \mathbb{R}_{+} $. 
In other words, the time between arrivals becomes stochastically longer as the expected waiting time rises, up to the bound $M$ that can be chosen to be large.\footnote{The assumption of the bound $M$ is crucial for ensuring the existence of an invariant distribution, but it is not necessary for establishing that there is at most one invariant distribution.}  We assume that $
(S_{t})_{t \in \mathbb{N}}$ are identically distributed and independent random variables with positive finite expectations and finite variances, and $T_{t}(h)$ has a bounded first two moments, is continuous and $\{ T_{t}(h) \}$ are independent random variables across time for each $h \geq 0$. We also assume $\mathbb{E}T_{t}(0) > \mathbb{E}S_{t}$ so the G/G/1 queuing system is stable and a invariant distribution exists.

The expected waiting times experienced by customers in the queue evolve by the following nonlinear Markov chain on $\mathbb{R}_{+}$: 
\begin{equation} \label{Eq:queue} X_{t+1} = \max (0, X_{t} + S_{t} - T_{t} ( \min \{ \mathbb{E}(X_t), M \} ) .
\end{equation}




It can be easily verified that $Q$ is $D$-preserving and $D$-decreasing when $D$ is the set of all increasing functions. Under the 
 assumption stated above that the queue does not explode, i.e., $\mathbb{E}S_{t} < \mathbb{E}T_{t}(0)$, a standard argument from the Markov chain literature (e.g., Theorem 
19.3.5 in \cite{meyn2012markov}) shows that Property (C) holds. Hence, we can use Theorem \ref{Theorem: unique} to conclude that there exists at most one waiting time equilibrium steady state distribution. Existence of an invariant distribution follows from Proposition \ref{Prop:existence2}. The proofs of all the Corollaries are deferred to Section \ref{Sec:CorrProofs} in the Appendix. 

\begin{corollary} \label{Corr:Queue}
    The nonlinear Markov chain describing the queueing system in Equation (\ref{Eq:queue}) has a unique invariant distribution. 
\end{corollary}


As a particular example, we study  an M/G/1 queuing system where the arrival rate depends on the expected waiting time and provide a closed-form expression for the stationary expected waiting time. 


\begin{example} ($M/G/1$ queue). 
Consider an $M/G/1$ queue so the time between arrivals has an exponential distribution. Let  $Law(S_{t}) = Law (S)$ and $Law(T_{t}(h)) $ has an exponential distribution with the parameter $\lambda(h)$. Suppose that the mean interarrival time equals the expected waiting time so $\lambda(h) = 1/h$ (we ignore the large bound 
$M$ in this example, as it does not affect the result when 
$M$ is sufficiently large). 

\begin{claim} \label{claim_queue} There is a unique invariant distribution for the nonlinear Markov chain given in Equation (\ref{Eq:queue}) and the expected value of the stationary waiting time $X_{\infty}$ is given by the closed-form expression
$$ \mathbb{E}(X_{\infty}) = \frac{\mathbb{E}(S^{2})} {\sqrt{\mathbb{E}(S)^{2}+2\mathbb{E}(S^{2})} - \mathbb{E}(S)} .$$

In particular, if the queue is an $M/M/1$ queue so $S$ is an exponential random variable then 
$$ \mathbb{E}(X_{\infty}) = \frac{ 2\mathbb{E}(S)} {\sqrt{5}-1}. $$
    
\end{claim}


%we must have $1/\lambda = \lambda\mathbb{E}(S^{2})/(2(1-\lambda\mathbb{E}(S))$ which yields 
%$$ \lambda = \frac{\sqrt{(\mathbb{E}(S))^{2}+2\mathbb{E}(S^{2})} - \mathbb{E}(S)}{\mathbb{E}(S^{2})}$$
%and is valid when $\lambda \mathbb{E}(S) < 1$. 

\end{example}


\subsection{Nonlinear Equations}


The study of nonlinear systems of equations in $\mathbb{R}^{n}$ has long been a significant area of interest in mathematics and its applications. Finding conditions that ensure a unique solution to such systems is crucial as it offers insights into the properties and stability of solutions, which in turn, have far-reaching implications across various fields, including operations, engineering, economics, and optimization \citep{ortega2000iterative}. 
It is generally uncommon to identify a comprehensive set of conditions that guarantee a unique global solution for a system of nonlinear equations in $\mathbb{R}^{n}$ that do not satisfy contraction properties. We apply Theorem \ref{Theorem: unique} to determine conditions that ensure a unique solution for a specific class of nonlinear equations, which we define subsequently. These conditions are based  on monotonicity concerning the majorization order. 




Let $\Delta_{n} = \{ \boldsymbol{x} \in \mathbb{R}^{n}: \sum_{i=1}^{n}  x_{i} = 1, x_{i} \geq 0 \text{ }\forall i \} $ be the $n$-dimensional simplex.  
Consider a stochastic matrix $\boldsymbol{P} (G(\boldsymbol{x})) \in \mathbb{R}^{n\times n}$ that is parameterized by $G(\boldsymbol{x})$ where  $G:\Delta _{n} \rightarrow A$  and $A \subseteq \mathbb{R}$ is the image of $G$, i.e., $P_{ij} (a) \geq 0$, and $\sum _{j=1}^{n} P_{ij} (a) = 1$ for all $a \in A$. We assume that $G$ is a continuous function. 



For $\boldsymbol{x},\boldsymbol{y} \in \mathbb{R}^{n}$ write $\boldsymbol{x} \geq _{m} \boldsymbol{y}$ if $ \sum _{j=k}^{n} x_{j} \geq \sum _{j=k}^{n} y_{j}$ for all $1 \leq k \leq n$ and $\sum_{j=1}^{n} x_{j} = \sum _{j=1}^{n} y_{j}$ (the order $\geq _{m}$ is sometimes called majorization between vectors in $\mathbb{R}^{n}$). We denote by $\boldsymbol{P}_{i}(a)$ the $i$th row of the matrix $\boldsymbol{P}$. 


The following Corollary follows  from applying Theorem \ref{Theorem: unique} and Proposition \ref{Prop:existence}. 

\begin{corollary} \label{Coro:non-linear}
Let $G:\Delta _{n} \rightarrow A$ be a continuous function that  is increasing with respect to $ \geq _{m}$. The nonlinear system of equations $\boldsymbol{x} = \boldsymbol{x} \boldsymbol{P} (G(\boldsymbol{x}))$ on $\Delta_{n}$ where $\boldsymbol{P} (G(\boldsymbol{x}))$ is a stochastic matrix that is parameterized by $G(\boldsymbol{x})$ has a unique solution if the following three conditions hold: 


(1) For all $a \in A$, $i \geq i'$, we have  $\boldsymbol{P}_{i}(a) \geq _{m} \boldsymbol{P}_{i'}(a)$.

(2) For all $1 \leq  i \leq n$, $a' \geq a$, $a,a' \in A$, we have  $\boldsymbol{P}_{i}(a) \geq _{m} \boldsymbol{P}_{i}(a')$.

%and all $1 \leq k \leq n$ we have $\sum _{j=k}^{n} P_{ij}(G(\boldsymbol{x})) \geq \sum _{j=k}^{n} P_{ij}(G(\boldsymbol{x}'))$.

(3) For all $a \in A$, the linear system of equations $\boldsymbol{x} = \boldsymbol{x} \boldsymbol{P} (a)$ for $\boldsymbol{x} \in \Delta_{n}$ has a unique solution. 


\end{corollary}




%Corollary \ref{Coro:non-linear} leverages Theorem \ref{Theorem: unique} to show that a class  of nonlinear equations have a unique solution. This Corollary can be used as a tool to generate nonlinear systems of equations that are known to have a unique solution. Theorem \ref{Theorem: unique} can also be used to show the uniqueness of solutions for nonlinear equations in many other different ways  than Corollary \ref{Coro:non-linear}, e.g., see Example \ref{example:flexible}.










\subsection{Wealth Distributions} \label{sec:wealth}


In heterogeneous-agents macroeconomic models (see \cite{stachurski2009economic} for a recent textbook treatment of economic dynamic models), agents determine their consumption, savings, and allocation of savings across financial assets based on their current wealth level in each period. 


An extensive literature exists on these models, specifically focusing on the analysis of stationary equilibria and the associated stationary wealth distributions. Despite the vast body of research, the conditions ensuring the uniqueness of equilibrium are restricted to a handful of special cases.\footnote{For instance, see \cite{light2020uniqueness, light2023general, achdou2022income}. } In this section, we employ Theorem \ref{Theorem: unique} to prove the uniqueness of a stationary equilibrium under a typical progression of wealth dynamics in these models, given that agents' savings increase with the rate of returns and their current wealth levels. We proceed to outline the model.


In each period $t$, there are $n$ non-negative random variables $R_{1,t},\ldots,R_{n,t}$ with bounded supports $[0, \overline{r}]$ that represent returns from different financial assets $i=1,\ldots,n$.  
The random return $R_{i,t}$ of asset $i$ is parameterized by a continuous aggregator $H(\mu)$ and we write $R_{i,t}(H(\mu))$ to capture this dependence. The aggregator is a function of the wealth distribution in the economy $\mu$ and  is increasing with respect to stochastic dominance. In many applications the aggregator is given by the total savings or wealth in the economy (e.g., \cite{aiyagari1994}). We assume that $R_{i,t}(h)$ is independent and identically distributed across time for each $i=1,\ldots,n$ and each $h$. For notational simplicity we sometimes write $R_{i}(h)$ instead of $R_{i,t}(h)$ to describe the random return of asset $i$ given the aggregator, and we assume that $R_{i}(h)$ is continuous for $i=1,\ldots,n$. 



Each agent has a Markovian policy $\boldsymbol{g} = (g_{1},\ldots,g_{n})$, which is a vector of functions that determines how wealth is allocated across assets. Specifically,  $g_{i}(R_{1}(\mu),\ldots,R_{n}(\mu),x)$ represents the non-negative amount that an agent with wealth $x$ allocates to asset $i$ when the current  returns are given by $(R_{1}(\mu),\ldots,R_{n}(\mu)))$. More formally, 
let \(\mathcal{T}\) denote the space of random variables with support on \([0, \overline{r}]\) then each function \( g_i : \mathcal{T}^n \times \mathbb{R}_+ \to \mathbb{R}_+ \) determines the allocation to asset \( i \) based on the returns and the agent’s wealth.\footnote{We assume for simplicity that the agents policy function depends on their current  wealth and returns only. All the results in this section can be easily extended to the case when each agent uses a different policy that depends on the agent's specific features such as preferences or behavioral biases.} In applications, the agent's policy is typically derived from a consumption-saving dynamic programming problem.  In our analysis, we assume a general policy function that can be deduced from rational agents, behavioral biases  \citep{acemoglu2018equilibrium}, myopic agents, or learning algorithms. We assume that $g_{i}$ is continuous for $i=1,\ldots,n$. 



 In each period $t$, each agent $j$ receives a non-negative random income $Y_{t}^{j}$ that is independent and identically distributed across time and across agents and has a bounded support $[0,\overline{y}]$. Note that the returns $R_{i,t}(h)$ depend on the wealth distribution in the economy and are common to all agents while the random income $Y_{t}^{j}$ represents agent-specific noise.



Each agent's wealth evolution is described by the following nonlinear Markov chain:
\begin{equation} \label{eq:wealth}
X^{j}_{t+1} = \sum _{i=1}^{n} g_{i}(R_{1}(H(\mu _{t} )),\ldots , R_{n} (H(\mu_{t}) ) ,X^{j}_{t} )R_{i}(H(\mu _{t}) ) + Y ^{j}_{t+1} 
\end{equation}
 where $X_{t}^{j}$ is the current wealth agent $j$ has,  and $\mu_{t}$ is the law of $X_{t}^{j}$ which describes the  wealth distribution across agents in period $t$. Thus, if an agent has a current wealth of  $x_{t}$, the agent allocates $g_{i}$ to asset $i$, then the next period's wealth is given by the sum of the returns on these investments plus the income received in the next period. A stationary equilibrium in this economy is defined by an invariant distribution of the nonlinear Markov chain given in Equation (\ref{eq:wealth}) with the interpretation that this distribution represents the long run equilibrium wealth distribution across agents \citep{aiyagari1994,acemoglu2012}. 

 Under standard assumptions, the policy function is increasing in the current wealth, i.e., savings increase when the agent's wealth is higher, and the returns are decreasing in the savings with respect to first order stochastic dominance, i.e., the returns are (stochastically) lower when the total savings are higher (see \cite{acemoglu2012}, and \cite{acemoglu2018equilibrium}). Under these assumptions, we can apply Theorem \ref{Theorem: unique} to conclude that there is at most one stationary wealth distribution equilibrium if the total amount of savings $\sum g_{i}$ is increasing in the rate of returns. In the economics literature, this property means that the substitution effect dominates the income effect. Hence, the key condition that implies that there is at most one stationary wealth distribution  equilibrium is that  savings increase with the rate of returns (a special case of this result with one financial asset and rational agents is studied in \cite{light2020uniqueness}). We now present this result formally. 

 \begin{corollary} \label{Coro:wealth}
     Suppose that $H(\mu)$ is increasing with respect to $\succeq_{SD}$ and assume that $g_{i} \leq M$ for some $M$ for each $i$.\footnote{The assumption that $g_{i}$ is used only to prove existence. We note that the existence of the stationary wealth distribution equilibrium is widely studied in the literature (e.g., \cite{acikgoz2015existence},  \cite{acemoglu2012}, \cite{zhu2020existence}, and \cite{light2022mean}) where the boundness of $g_{i}$ can be established by considering the consumption-savings dynamic programming problem the agents' solve or by assuming an exogenous savings bound.} Assume that:

     (1) Property (C) holds.\footnote{There is a vast literature on conditions that ensure that Property (C) holds in different models of wealth dynamics by employing results from the standard Markov chain literature. For recent results see \cite{ma2020income}.}
     
     (2) The function $\sum g_{i}$ is increasing in $x$ and decreasing in the aggregator in the sense that 
     $$ \sum _{i=1}^{n} g_{i} (R_{1} (h_{2} ), \ldots , R_{n}  ( h_{2} ) , x_{2} ) \geq  \sum _{i=1}^{n} g_{i} (R_{1} (h_{1} ), \ldots , R_{n}  ( h_{1} ) , x_{1} )  $$
     whenever $x_{2} \geq x_{1}$ and $h_{1} \geq h_{2}$. 

     (3) For $i=1,\ldots,n$, $R_{i}(h_{2}) \succeq_{SD} R_{i}(h_{1})$ whenever $h_{1} \geq h_{2}$. 

     Then the nonlinear Markov chain on  described in Equation (\ref{eq:wealth}) has a unique invariant distribution. 
 \end{corollary}


\section{Conclusions} 

This paper studies discrete-time nonlinear Markov chains with an aggregator and establishes  conditions that imply the uniqueness and existence of an invariant distribution for these chains. Unlike traditional approaches that rely on contraction properties of the chains, our conditions leverage monotonicity properties and the aggregator structure to establish uniqueness.
 We provide a computational method to compute the invariant distribution and apply our results to different settings including strategic queueing systems, non-linear equations, and the evolution of wealth distributions in dynamic economies. We believe that our results can be applied to other models where the flexible monotonicity conditions we provide naturally met.




There are  remaining important open questions 
concerning nonlinear Markov chains. 
For instance, proving the existence of an invariant distribution for nonlinear Markov chains without an aggregator  in general state spaces remains largely unresolved. Additionally, our examples demonstrate that even in a simple two-state chain, convergence to an invariant distribution is not guaranteed even if it is unique. Therefore, developing algorithms that ensure convergence to an invariant distribution for nonlinear Markov chains without an aggregator that are beyond the scope of the bisection method we introduced in Section \ref{sec:compute} remains essential for practical computation of the invariant distributions.


\newpage


\section{Appendix} 


\subsection{Proofs of Theorem \ref{Theorem: unique} and Propositions \ref{Prop:existence}, \ref{Prop:existence2}, \ref{prop:compute},   \ref{Prop:local}}
We will use the following Proposition to prove Theorem \ref{Theorem: unique} (see Corollary 2.5.2 in \cite{topkis2011supermodularity}). 

\begin{proposition}
\label{Topkis Fixed point}Suppose that $Z$ is a non-empty complete lattice, $E$ is a partially ordered set, and $f$ is an increasing function from $Z \times E$ into $Z$. Then, for each $e \in E$, the greatest and least fixed points 
of $f$ exist and are increasing in $e$ on $E$. 
\end{proposition}



\begin{proof}[Proof of Theorem \ref{Theorem: unique}]
Let $\theta _{1} ,\theta _{2} \in \mathcal{P} (S)$ and assume that $\theta _{1}  \succeq  _{D}\theta _{2}$. 
Let $\mu_{1} ,\mu_{2}$ be two invariant distributions of $Q$. Assume without loss of generality that $h_{2} := H( \mu_{2}) \geq H( \mu_{1}) :=h_{1}$ and let $f:S \rightarrow \mathbb{R}$ be a function such that $f \in D$. We have 
\begin{align*}\int _{S}  f(x) M_{h_{2}} \theta _{2} (dx)  &  =\int _{S} \int_{S} f (y) Q(x ,h_{2} ,dy)   \theta _{2} (d x) \\
 &  \leq  \int _{S} \int_{S} f (y) Q(x ,h_{1},dy)   \theta _{2} (d x) \\
 &  \leq \int _{S} \int_{S} f (y) Q(x ,h_{1} ,dy)   \theta _{1} (d x) \\
 & = \int _{S}  f(x) M_{h_{1}} \theta _{1} (dx). 
 \end{align*}

 Thus, $M_{h_{1}} \theta _{1}  \succeq  _{D}M_{h_{2}} \theta _{2}$. The first inequality follows from the fact that $Q$ is $D$-decreasing. The second inequality follows from the facts that $\theta _{1}  \succeq  _{D}\theta _{2}$ and $Q$ is $D$-preserving. 
We conclude that $M_{h_{1}}^{n} \theta _{1}  \succeq  _{D}M_{h_{2}}^{n} \theta _{2}$ for all $n \in \mathbb{N}$. 

Assume that condition (i) of the theorem holds. 
The fact that $Q$ satisfies Property (C) implies that $M_{h_{i}}^{n} \theta _{i}$ converges weakly to the unique fixed point of $M_{h_{i}}$ which is given by $\mu _{h_{i}}$ for $i=1,2$. Because $\mu_{1}$ and $\mu_{2}$ are invariant distributions of $Q$ we have  $\mu _{h_{i}} =\mu_{i}$ for $i=1,2$. Because $ \succeq  _{D}$ is closed with respect to weak convergence,  we have $\mu_{1} \succeq  _{D} \mu_{2}$. Using the fact that $H$ is increasing with respect to $\succeq  _{D} $ implies $h_{1} \geq h_{2}$. 

We conclude that if $\mu_{1}$ and $\mu_{2}$ are invariant distributions of $Q$ then $H(\mu_{1}) = H(\mu_{2})$. Thus, $Q (x ,H(\mu_{1}) ,B) =Q (x,  H(\mu_{2}) ,B)$ for all $x \in S$ and $B \in \mathcal{B} (S)$. Because $Q$ satisfies assumption (U) the operators $M_{H(\mu_{1})}$ and $M_{H(\mu_{2})}$ have unique fixed points. Thus, $\mu _{H(\mu_{1})} = \mu _{H(\mu_{2})}$, i.e., $\mu_{1} = \mu_{2}$. 
We conclude that if an invariant distribution of $Q$ exists, it is unique. 

Now assume that condition (ii) of the theorem holds. Define the order $\geq '$ on $\mathbb{R}$ by $x \geq  'y$ whenever $y \geq  x$.   Under this assumption, the arguments above imply that 
the operator $M$ is increasing from $\mathcal{P}(S) \times \mathcal{H}$ to $\mathcal{P}(S)$ on the complete lattice $( \mathcal{P}(S),\succeq  _{D})$ when $\mathcal{H}$ is endowed with $\geq '$. 
Then by applying Proposition \ref{Topkis Fixed point} to the increasing operator $M$  we have $\mu _{h_{1}} \succeq _{D} \mu _{h_{2}}$, i.e., $\mu_{1} \succeq  _{D} \mu_{2}$. Now we can use the same arguments as the arguments for the case that condition (i) holds to show that if an invariant distribution of $Q$ exists, it is unique. 
\end{proof}


In order to establish the existence of an invariant distribution we will use chauder-Tychonoff's following  fixed-point theorem (see Corollary 17.56 in \cite{aliprantis2006infinite}).

\begin{proposition}
(Schauder-Tychonoff) Let $K$ be a non-empty, compact, convex subset of a locally convex Hausdorff space, and let $f :K \rightarrow K$ be a continuous function. Then the set of fixed points of $f$ is compact and non-empty. 
\end{proposition}

\begin{proof}[Proof of Proposition \ref{Prop:existence}]
  Because $S$ is a compact polish space  $\mathcal{P} (S)$ is a compact polish space under the weak topology  (see Theorem 15.11 in \cite{aliprantis2006infinite}).
Clearly $\mathcal{P} (S)$ is convex. $\mathcal{P} (S)$ endowed with the weak topology is a locally convex Hausdorff space. Thus, if $T $ is continuous, we can apply Schauder-Tychonoff's fixed point theorem to conclude that $T $ has a fixed point. 

To show that $T$ is continuous, take a sequence of measures $\{ \mu_{n} \}$ and assume that it converges weakly to $\mu$.  

Let $f :S \rightarrow \mathbb{R}$ be a continuous and bounded function. Because $Q$ and $H$ are continuous we have $\lim _{n \rightarrow \infty} \int_{S} f(y) Q(x_{n},H(\mu_{n}),dy) = \int_{S} f(y) Q(x,H(\mu),dy) $ whenever $x_{n} \rightarrow x$. Define $m_{n}(x) : = \int_{S} f(y) Q(x,H(\mu_{n}),dy) $. Then $m_{n}(x)$ is a uniformly bounded sequence of functions such that $m_{n}(x_{n}) \rightarrow m(x)$ whenever $x_{n} \rightarrow x$. Thus, by Lebesgue's Convergence Theorem for varying measures (see Theorem 3.5 in \cite{serfozo1982convergence} and Section 5 in \cite{feinberg2020fatou}) we have $\lim _{n \rightarrow \infty} \int m_{n}(x) \mu_{n}(dx) = \int m(x) \mu (dx)$. Hence,
\begin{align*}\underset{n \rightarrow \infty }{\lim }\int _{S}f (x) T  \mu_{n} (d x) &  =\underset{n \rightarrow \infty }{\lim }\int _{S} \int_{S} f(y) Q(x,H(\mu_{n}),dy) \mu_{n} (d x) \\
 &  =\int _{S} \int_{S} f(y) Q(x,H(\mu),dy) \mu (d x) \\
 &  =\int _{S}f (x) T \mu(dx) .\end{align*}
 Thus, $T\mu_{n}$ converges weakly to $T \mu$. We conclude that $T$ is continuous. Thus, by the Schauder-Tychonoff's fixed point theorem, $T $ has a fixed point. 
\end{proof}


 \begin{proof} [Proof of Proposition \ref{Prop:existence2}]
 Consider the function $f(h)= h- H(\mu_{h})$ from $[h',h'']$ to $\mathbb{R}$ which is well defined because $\mu_{h} \in \mathcal{P}(S)$ for all $h \in [h',h'']$. 

We first claim that a root $f$, say $h^{*}$, corresponds to an invariant distribution $\mu_{h^{*}}$ of $Q$. To see this, let $h^{*}$ be a root of $f$, that is, $H(\mu_{h^{*}} ) = h^{*}$. 



From Property (U), $\mu_{h^{*}}$ is the unique probability measure that satisfies
 $$\mu_{h^{*}}(B) = \int Q(x, h^{*}, B)  \mu_{h^{*}}(dx), $$
so $H(\mu_{h^{*}} ) = h^{*}$ implies that 
 $$\mu_{h^{*}}(B) = \int Q(x, H(\mu_{h^{*}} ), B)  \mu_{h^{*}}(dx), $$
i.e., $\mu_{h^{*}}$ is an invariant distribution of $Q$. 


If $h'' = H(\mu_{h''})$ or $h' = H(\mu_{h'})$ then $f$ has a root, and hence, $Q$ has invariant distribution. If 
 $h'' > H(\mu_{h''})$ and $h' < H(\mu_{h'})$, we have $f(h'') > 0 > f(h')$ so if $f$ is continuous we can apply the intermediate value theorem to prove that $f$ has a root, that is, $Q$ has an invariant distribution. 

We will now show that $f$ is continuous to conclude the proof. 

 Consider a sequence $\{h_{n}\}$, $h_{n} \in [h',h'']$ such that  $h_{n}$ converges to $h$ and let $\{\mu_{h_{k}}\}$ be a subsequence of $\{ \mu_{h_{n}} \}$ that converges to $\lambda$.  From Lebesgue's Convergence Theorem for varying measures (see Theorem 3.5 in \cite{serfozo1982convergence}) and using the same logic as in the proof of Proposition \ref{Prop:existence}, for every continuous and bounded function $m:S \rightarrow \mathbb{R}$, we have
 \begin{align*}\underset{k \rightarrow \infty }{\lim }\int _{S}m (x) \mu_{h_{k}} (d x) &  =\underset{k \rightarrow \infty }{\lim }\int _{S} \int_{S} m(y) Q(x,h_{k},dy) \mu_{h_{k}} (d x) \\
 &  =\int _{S} \int_{S} m(y) Q(x,h,dy) \lambda (d x) \\
 &  =\int _{S}m (x) M_{h} \lambda(dx) .\end{align*}
Because $\{ \mu_{h_{k}} \}$ converges to $\lambda$ we also have
$$ \lim _{k \rightarrow \infty} \int_{S} m(x) \mu_{h_{k}}  (dx) = \int_{S} m(x) \lambda  (dx).$$
Thus, $\lambda =M_{h}\lambda$. From assumption (U), $\mu_{h} $ is the unique fixed point of $M_{h}$, and thus,  $\lambda =\mu_{h} $. 

We conclude that any subsequence of $\{ \mu_{h_{n}} \}$ that converges weakly at all converges weakly to $\mu_{h}$. Furthermore, from assumption, the sequence $\{ \mu_{h_{n}} \}$ is a tight sequence of probability measures. Thus, $\{ \mu_{h_{n}} \}$ converges weakly to $\mu _{h}$ (see the Corollary after Theorem 25.10 in \cite{billingsley2008probability}). 

Because $H$ is continuous, we conclude that $f(h) = h - H(\mu_{h})$ is continuous on $[h',h'']$ which completes the proof. 
\end{proof}






\begin{proof} [Proof of Proposition \ref{prop:compute}]
From Proposition \ref{Prop:existence2} the function $f$ is continuous and has opposite signs at $h_{1}$ and $h_{2}$. Hence, the sequence $h_{n}$ defined in the statement of the proposition converges linearly to the root of $f$ (see for example, Theorem 2.1 in \cite{burden19852}).
   
   From Proposition \ref{Prop:existence2} if $h^{*}$ is a root of $f$, then $\mu_{h^{*}}$ is an invariant distribution of $Q$ which completes the proof. 
\end{proof}



\begin{proof}[Proof of Proposition \ref{Prop:local}]
The proof is similar to the proof of Theorem \ref{Theorem: unique}. We provide it here for completeness. Let $\theta _{1} ,\theta _{2} \in \mathcal{W}$ such that $\theta _{1}  \succeq  _{D}\theta _{2}$. 
Let $\mu_{1} ,\mu_{2} \in \mathcal{W}$ be two invariant distributions of $Q$. 

Assume without loss of generality that $h_{2} := H( \mu_{2}) \geq H( \mu_{1}) :=h_{1}$ so $h_{1},h_{2} \in \mathcal{H}_{\mathcal{W}}$ and let $f:S \rightarrow \mathbb{R}$ be a function such that $f \in D$. We have 
\begin{align*}\int _{S}  f(x) M_{h_{2}} \theta _{2} (dx)  &  =\int _{S} \int_{S} f (y) Q(x ,h_{2} ,dy)   \theta _{2} (d x) \\
 &  \leq  \int _{S} \int_{S} f (y) Q(x ,h_{1},dy)   \theta _{2} (d x) \\
 &  \leq \int _{S} \int_{S} f (y) Q(x ,h_{1} ,dy)   \theta _{1} (d x) \\
 & = \int _{S}  f(x) M_{h_{1}} \theta _{1} (dx). 
 \end{align*}

 Thus, $M_{h_{1}} \theta _{1}  \succeq  _{D}M_{h_{2}} \theta _{2}$. The first inequality follows from the fact that $Q$ is $D$-decreasing on $\mathcal{W}$. The second inequality follows from the facts that $\theta _{1}  \succeq  _{D}\theta _{2}$ and $Q$ is $D$-preserving on $\mathcal{W}$. Now because $\theta_{1},\theta_{2} \in \mathcal{W}$ and $h_{1},h_{2} \in \mathcal{H}_{\mathcal{W}}$, we have $M_{h_{1}} \theta _{1}, M_{h_{2}} \theta _{2} \in \mathcal{W}$. Applying the same argument as above again, we conclude that $M_{h_{1}}^{n} \theta _{1}  \succeq  _{D}M_{h_{2}}^{n} \theta _{2}$ for all $n \in \mathbb{N}$. 

Now the proof continues exactly as in the proof of Theorem \ref{Theorem: unique}.
\end{proof}



\subsection{Proof of Corollaries \ref{Corr:Queue}, \ref{Coro:non-linear}, \ref{Coro:wealth}} \label{Sec:CorrProofs}



\begin{proof} [Proof of Corollary \ref{Corr:Queue}]
  Let $H(\mu) = \min \{ \int _{\mathbb{R} _{+}} x \mu(dx) , M \}$, $Law(S_{t}) = Law (S)$ and $Law(T_{t} (h) ) = Law (T(h) )$. Let $D$ be the set of increasing functions, so $\succeq_{D}$ is equivalent to the first order stochastic dominance order $\succeq_{SD}$ and $H$ is increasing with respect to $\succeq_{D}$. From  Theorem 19.3.5 in \cite{meyn2012markov}, Property (C) is satisfied because $\mathbb{E}(T(h)) \geq \mathbb{E} (T(0)) > \mathbb{E} (S)$ for all $h  \geq 0$.
  
Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be increasing.  Then
$$ \int f(y)Q(x, h, dy) = \mathbb{E} f \left ( \max \{x + S - T(h) , 0 \} \right ) $$ 
where the expectations is taken with respect to the random variables $S$ and $T(h)$ is increasing in $x$ and decreasing in $h$ (recall that $T$ is stochastically increasing in $h$ by assumption) so $Q$ is $D$-preserving and $D$-decreasing.
   Hence, from Theorem \ref{Theorem: unique} we conclude that the nonlinear Markov chain given in Equation (\ref{Eq:queue}) has at most one invariant distribution.

  For existence, first note that the function $f(h) = H(\mu_{h})$ is bounded so we can find $h',h'' \in \mathbb{R}_{+}$ such that $h'' \geq H(\mu_{h''})$ and $h' \leq H(\mu_{h'})$ (e.g., $h'=0$ and $h''=M$). We already established that property (C) holds, and hence, property (U) holds too. It is immediate to verify that $H$ and $Q$ are continuous. In addition, for any sequence of non-negative numbers $h_{n}$ that converges to some $h$, the assumptions that $ \mathbb{E} (T(0)) > \mathbb{E} (S)$ and that $T(h)$ and $S$ have bounded variances, guarantee that the sequence of invariant distributions of the G/G/1 queue $\mu_{h_{n}}$ has bounded first two moments, and hence, it is tight. Thus, we can apply Proposition \ref{Prop:existence2} to conclude that an invariant distribution exists which completes the proof.  
\end{proof}


\begin{proof} [Proof of Corollary \ref{Coro:non-linear}]
Existence follows 
immediately
 from Proposition \ref{Prop:existence}.  For uniqueness,  we need to show that the conditions of Theorem \ref{Theorem: unique} holds. We let $S=\{1,\ldots,n\}$ with the standard order, $H(\mu)=G \left (\mu(\{1\}),\ldots,\mu ( \{n \}) \right )$, and $D$ to be the set of increasing functions, so $\succeq_{D}$ is equivalent to $\succeq_{SD}$ and $(\mathcal{P}(S),\succeq_{D})$ is a complete lattice. Note that $H$ is increasing with respect to $\succeq_{SD}$ because $\mu \succeq_{SD} \mu' $ holds if and only if $\left (\mu(\{1\}),\ldots,\mu ( \{n \}) \right ) \geq_{m}\left (\mu'(\{1\}),\ldots,\mu ' ( \{n \}) \right ) $ and from the assumption that $G$ is increasing with respect to $\geq_{m}$. 
    
     Condition (1) implies that $Q$ is $D$-preserving, Condition (2) implies that $Q$ is $D$-decreasing, and Condition (3) implies that Property (U) holds.  Thus, we can apply Theorem \ref{Theorem: unique} to prove that $Q$ has at most one invariant distribution.  

    We can identify $Q$ with the stochastic matrix $P$ by $P_{ij} (\cdot) = Q(i,\cdot,\{ j \})$, and hence, using the definition of the invariant distribution, the Corollary follows from Theorem \ref{Theorem: unique}. 
\end{proof}


\begin{proof} [Proof of Corollary \ref{Coro:wealth}]
    For existence, continuity of $H$ and $Q$ follows immediately from the assumptions. Now note that the state space is bounded because the random variables $R_{i}$, $Y$, and the policy functions $g_{i}$ are bounded. In particular, we let the state space be the compact set $S=[0,nM\overline{r} + \overline{y}]$. Hence, we can use Proposition \ref{Prop:existence}  to conclude that $Q$ has an invariant distribution.
      
     For uniqueness, we need to show that the conditions of Theorem \ref{Theorem: unique} hold. We let $D$ to be the set of increasing functions, so $\succeq_{D}$ is equivalent to $\succeq_{SD}$. 
    
   It is immediate that Condition (2) implies that $Q$ is $D$-preserving and Conditions (2) and (3) imply that $Q$ is $D$-decreasing. Thus, we can apply Theorem \ref{Theorem: unique} to prove that $Q$ has at most one invariant distribution.
\end{proof}

\subsection{Proof of Claims 1,2,3,4}


\begin{proof}
    [Proof of Claim \ref{claim1}]
We let $D$ to be the set of all  increasing functions. Clearly $H$ is increasing with respect to $\succeq_{D}$ because $m$ is increasing. Property (C) holds for AR(1) process with $a \in (0,1)$, (see, for example, \cite{light2024note}).   Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be increasing.  Then 
$$ \int f(y)Q(x, h, dy) = \int f(ax -  h + \epsilon ) \phi (d(\epsilon)) $$ 
is increasing in $x$ and decreasing in $h$ so $Q$ is $D$-preserving and $D$-decreasing. Hence, we can apply Theorem \ref{Theorem: unique} to conclude that $Q$ has at most one invariant distribution.

For existence, it is immediate that $Q$ and $H$ are continuous as $m$ is bounded and continuous. Because $m$ is bounded, we can find  $h',h'' \in \mathbb{R}$, $h''>h'$, such that $h'' \geq H(\mu_{h''})$ and $h' \leq H(\mu_{h'})$. In addition, if $h_{n}$ converges to $h$, then it follows that the sequence $\mu_{h_{n}}(dx)$ of invariant distributions of the AR(1) process given the parameter $h_{n}$ has bounded first two moments (recall that the noise term $\epsilon$ has finite expectation and variance), and hence,  $\{\mu_{h_{n}} \}$ is a tight sequences of probability measures. 
\end{proof}

\begin{proof}
    [Proof of Claim \ref{claim2}]
We let $D$ to be the set of all the functions that are increasing in the first argument. Clearly $H$ is increasing with respect to $\succeq_{D}$. We need to show that  $Q$ is $D$-preserving and $D$-decreasing in order to use Theorem \ref{Theorem: unique}. 
Let $f \in \mathbb{R}^{\mathbb{R}^{2} }$ be increasing in the first argument.  Then 
$$ \int f(y_{1},y_{2})Q((x_{1},x_{2}),h, dy) = \int f(ax_{1} -  h + \epsilon_{1},k(x_{2}) + \epsilon_{2}) \phi (d(\epsilon_{1},\epsilon_{2})) $$ 
is increasing in the first argument and decreasing in $h$ so $Q$ is $D$-preserving and $D$-decreasing. Hence, we can apply Theorem \ref{Theorem: unique} to conclude that $Q$ has at most one invariant distribution. Existence of an invariant distribution follows by the same argument as in Claim \ref{claim1}.
\end{proof}

\begin{proof}
    [Proof of Claim \ref{claim3}]
Consider the set of functions $D$ such that $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ is in $D$ if $f(x) = \sum _{i=1}^{n} y_{i}x_{i} + c$ for some $y \in O$ and $c \in \mathbb{R}$. Property (C) holds (see Example 1 in \cite{light2024note}).  It is immediate that $H$ is increasing with respect to $\succeq _{D}$. 
   
   We now show that $Q$ is $D$-preserving and $D$-decreasing. Let $f \in D$ so $f(x) = \sum _{i=1}^{n} y_{i}x_{i} + b$ for some $y \in O$.
   
   We have 
 \begin{align*}
     v(x):=\int f(x')Q(x,h,dx') & = \int f(a_{1}x_{1} - \beta_{1}h+\epsilon_{1},\ldots,a_{n}x_{n}-\beta_{n}h + \epsilon_{n}) \phi (d \epsilon ) \\
     & = \int \sum _{i=1}^{n} y_{i}(a_{i}x_{i} - \beta_{i}h + \epsilon_{i})\phi (d \epsilon ) + b \\
     & = \sum _{i=1}^{n} y_{i}'x_{i} + b'
      \end{align*}
with $y_{i}' = a_{i}y_{i}$ and $b' = \int \sum _{i=1}^{n} y_{i} ( -\beta_{i}h + \epsilon_{i})\phi (d \epsilon ) + b$. Note that $y' $ is in $O$ as $y \in O$ and $a_{i} \geq 0$ for all $i$. Hence, $v$ is in $D$ which means that $Q$ is $D$-preserving. 


To show that $Q$ is $D$-decreasing  let $h_{2} \geq h_{1}$ and note that 
\begin{align*}
\int f(x')Q(x,h_{2},dx') & =  \int \sum _{i=1}^{n} y_{i}(a_{i}x_{i} - \beta_{i}h_{2} + \epsilon_{i})\phi (d \epsilon ) + b \\
     & \leq  \int \sum _{i=1}^{n} y_{i}(a_{i}x_{i} - \beta_{i}h_{1} + \epsilon_{i})\phi (d \epsilon ) + b  \\
     & = \int f(x')Q(x,h_{1},dx')
\end{align*}
where the inequality follows from the fact that $y$ and $\beta$ are in $O$ so $y_{i}\beta_{i} \geq 0$ for all $i$. Thus, $Q$ is $D$-decreasing.

To prove existence, note that we can find $H(\mu_{h})$ directly. A simple calculation shows that $H(\mu_{h}) = \sum _{i=1}^{n} \gamma_{i}(-h+e_{i})/(1-a_{i})$ where $e_{i}$ is the expected value of $\epsilon_{i}$. Thus, we can find  $h',h'' \in \mathbb{R}$, $h''>h'$, such that $h'' > H(\mu_{h''})$ and $h' < H(\mu_{h'})$. In addition, it is easy to see that the tightness condition of Proposition \ref{Prop:existence2} holds as the sequence $\{\mu_{h_{k}} \}$ has bounded first two moments whenever $h_{k}$ converges to some $h$.
\end{proof}

\begin{proof}
    [Proof of Claim \ref{claim_queue}]
Let 
\begin{equation} \label{Eq:x} h = \frac{\mathbb{E}(S^{2})} {\sqrt{\mathbb{E}(S)^{2}+2\mathbb{E}(S^{2})} - \mathbb{E}(S)} \end{equation}

and consider the linear Markov chain $W_{t+1} = \max (0, W_{t} + S_{t} - T_{t}(h) )$. Then it has a unique invariant distribution if $\mathbb{E}T_{t}(h) = h > \mathbb{E}(S) $ (see Theorem 19.3.5 in \cite{meyn2012markov}) which holds because
$$ \mathbb{E}(S)\sqrt{\mathbb{E}(S)^{2}+2\mathbb{E}(S^{2})} =\sqrt{\mathbb{E}(S)^{4}+2\mathbb{E}(S^{2})\mathbb{E}(S)^{2}}  < \sqrt{\left ( \mathbb{E}(S)^{2} + \mathbb{E}(S^{2}) \right ) ^{2}} =  \mathbb{E}(S)^{2} + \mathbb{E}(S^{2})  $$
which implies that $h > \mathbb{E}(S)$. 
Let $W_{\infty}$ be the random variable with the law $\mu^{*}$ where $\mu^{*}$ is unique invariant distribution of  the linear Markov chain $(W_{t})_{t \in \mathbb{N}}$. 



From the Pollaczek-Khinchin formula (see Equation (8.1) in Chapter 8 in \cite{cooperintroduction}) the stationary expected waiting time  is given by $\mathbb{E}(W_{\infty }) = \lambda (h) \mathbb{E}(S^{2})/(2(1-\lambda(h)\mathbb{E}(S)))$. Using the fact that $\lambda (h) = 1/h$, and algebraic manipulations, we see that $h = \mathbb{E}(W_{\infty })$. Hence, $\mu^{*}$ is an invariant distribution of the nonlinear Markov chain given in Equation (\ref{Eq:queue}). Uniqueness follows from Corollary \ref{Corr:Queue}.  

For $M/M/1$ queue $S$ is an exponential random variable with a parameter $\mu$, so $\mathbb{E}(S) = 1/\mu$ and $\mathbb{E}(S^{2}) = 2/\mu^{2}$ and we get $$\mathbb{E} (W_{ \infty})  = \frac{ 2}  {(\sqrt{5}-1)\mu} = \frac{ 2\mathbb{E}(S)} {\sqrt{5}-1}$$
which completes the proof. 
\end{proof}



\bibliographystyle{ecta}
\bibliography{unique}







\end{document}