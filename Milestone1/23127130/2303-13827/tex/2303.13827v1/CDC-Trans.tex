\documentclass{article}
\usepackage{graphicx}
% \usepackage[outdir=./]{epstopdf}
\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphicshttps://www.overleaf.com/project/63bfb12780d15a4c4a261c4e
\graphicspath{{images/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
%\fancyhead[LO]{Running Title for Header}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Efficient Mixed-Type Wafer Defect Pattern Recognition Using Compact Deformable Convolutional Transformers
%%%% Cite as
%%%% Update your official citation here when published 

}

\author{
  Nitish Shukla, \\
  \textit{Independent Researcher} \\
  \texttt{nitishshukla86@gmail.com} \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}
Manufacturing wafers is an intricate task involving thousands of steps. Defect Pattern Recognition (DPR) of wafer maps is crucial to find the root cause of the issue and further improving the yield in the wafer foundry. Mixed-type DPR is much more complicated compared to single-type DPR due to varied spatial features, the uncertainty of defects, and the number of defects present. To accurately predict the number of defects as well as the types of defects, we propose a novel compact deformable convolutional transformer (DC Transformer). Specifically, DC Transformer focuses on the global features present in the wafer map by virtue of learnable deformable kernels and multi-head attention to the global features. The proposed method succinctly models the internal relationship between the wafer maps and the defects. DC Transformer is evaluated on a real dataset containing 38 defect patterns. Experimental results show that DC Transformer performs exceptionally well in recognizing both single and mixed-type defects. The proposed method outperforms the current state of the models by a considerable margin.
\end{abstract}


% keywords can be removed
\keywords{Transformers \and Wafer Defect Classification s \and Deformable Convolutions }


\section{Introduction}
Wafers are silicon substrates on which integrated circuits (ICs) are constructed. The two most important stages of the IC manufacturing process are wafer fabrication and testing\cite{ref-1}\cite{ref-2}. The design of the specific type of IC includes several intricate procedures for wafer production. 
Highly advanced machines measure the fabrication layers at the nanoscale. If not corrected at an early stage, flaws introduced into a wafer during this fabrication process could seriously harm the production line. During testing, each die is tested against numerous parameters and only the qualified dies proceed for further development. The results of the testing are stored on a wafer bin map\cite{ref-3} which generally contains the parameter values along with the spatial information of the dies. Defect pattern recognition (DPR) labels wafers according to the distribution of unqualified dies. This distribution is typically the shape of the defective dies which gives valuable information on determining the root cause of the defects\cite{ref-4}\cite{ref-5}.

Historically, skilled engineers and subject-matter specialists manually examined such defects using a variety of statistical techniques\cite{ref-6}. Various techniques have been used for the DPR of mixed-type defects. A method like \cite{ref1} processed the wafer maps using filtering and clustering. The method applied a nonparametric Bayesian model to cluster various defect patterns after employing a filtering procedure named the connected-path filtering approach to strengthen the local systematic patterns and eliminate the random patterns. However, detecting such errors is gradually becoming more effective with the development of Deep Neural Networks and CNNs. Different supervised and unsupervised models are currently being used for detecting the type of defects
on the wafer map\cite{ref-8}\cite{ref-9}. A recent study on semi-supervised pattern
recognition on wafer maps has found a total of 14 defect
types. Some of the common ones are “Centre”, “Donut”,
“Scratch”, “Edge-ring” etc.  

Based on CNN, Kyeong and Kim\cite{ref-7} proposed a DPR model that used convolutions and pooling to sample features and independently determined whether the relevant defects were present or not. Other works include a deformable convolutional network\cite{ref3} (DC-Net), built to flexibly convolve over the wafer image by selectively sampling the regions in the image effectively filtering out the unrelated features. Real-life wafer datasets are unbalanced as some defects are very common and some occur seldom. To tackle this, an adaptive balancing generative adversarial network was introduced by Wang et al.\cite{ref2}. The generator in the model used a deconvolutional neural network to balance out missing data. The discriminator model classified the defect patterns.

Most of the current studies on mixed-type DPR reduced the defect set to about 10 defects\cite{ref-12}, which is an oversimplification of the task, given the number of commonly occurring defects is more than 30. Although methods like CNN perform better than others, they still suffer the following issues.
\begin{enumerate}
    \item CNNs require fixed-size input. Consequently, the wafers that may come from different fabrication designs need to be downsampled or padded. This results in some defects getting ignored or emphasized more. For example, defects like ``Edge-Ring'', and ``Edge-Local'' occur along the rims of the wafer, and downsampling  may make CNN ignore them.

    \item Defects occur variably on the wafer with different locations and angles. Fixed-sized windows of kernels in CNN limit the perception of local features. However, most of the defects are scattered throughout the wafer map.
\end{enumerate}

Motivated by the issues mentioned, we propose a novel deformable convolutional compact transformer (DC Transformer). The proposed model effectively uses deformable convolutions to synthesize global features in the wafer map to produce globally informed feature maps. The deformable convolution block learns to deform the perception window of CNN's covering intricate shapes like ``Donut''. The features obtained are fed to a transformer that utilizes a parallel multi-head attention mechanism to further encode the global feature information of the images. The multiple application of global-level feature extraction helps the framework to better generalize the defects in the wafers. Our experiments clearly support the claim with an overall accuracy of $97.27\%$ on 38 defect patterns and $98.70\%$ accuracy on single defect patterns outperforming the current state-of-the-art model with a decent margin.

To the best of our knowledge, we are the first to apply a transformer with a deformable tokenizer head for mixed-type DPR. The contribution of the paper is as follows:
\begin{enumerate}
    \item  We explore the mixed-type wafer defect recognition task using Transformer architecture with a modified tokenizer head for the first time.
    \item We integrate the deformable convolutional tokenizer head into the compact Transformer capable of extracting much succinct feature set.
    \item We propose a novel framework (DC Transformer) for mixed-type wafer defect detection. The framework utilizes the  local perception capability of deformable convolutions and the Transformer's global perception to efficiently detect defects.
    \item The proposed method achieves state-of-the-art performance on a data-set with 38 defects. The method performs exceptionally well on wafers with multiple defects.
\end{enumerate}

The rest of the paper is organized as follows: In section \ref{background}, we discuss the background in deformable convolutions and the transformers. In section \ref{method}, we describe the dataset used, preprocessing, and the model architecture. Finally, the experimental results are presented in section \ref{results} and conclude our study in section \ref{conclusion}. 
% In summary, our contributions are as follows
% \begin{itemize}
%     \item 
% \end{itemize}

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{images/diagram.pdf}
    \caption{Illustration of regular convolution (Left) with fixed-sized kernel and dilation 1. Deformed convolution (Right) with $3\times3$ offset matrix. }
    \label{fig:conv-deform}
\end{figure}

\section{Background}
\label{background}

\subsection{Deformable Convolutional Neural Network}
The Deformable Convolutional(DC) Neural Network\cite{ref6} differs from the conventional CNN architecture on the basis that it selectively samples from the input based on the position and displacement information by adding 2D offsets compared to regular grid sampling locations in the standard convolution. The offsets are learned using another convolution layer(s) from the preceding feature maps. Deformable CNNs have been extensively applied in fields ranging from object detection\cite{ref7}, hyperspectral image classification\cite{ref4} to medical image analysis\cite{ref5}. Several works also applied the method to detect small, dense objects and align different frames at the feature level to handle large motions in video restoration tasks\cite{ref-10}.



Due to the efficacy of deformable convolution networks in object detection tasks involving intricate spatial information, it is but natural to apply the same on multi-angle wafer defective pattern recognition. With the deformable sampling locations, by packing surrounding similar structural information into fixed grids, deformable features can be generated. DC units can extract features with various graphic shapes in the DPR of mixed-typed wafer maps, which may be used to extract features for each type of fundamental wafer map defect. A number of DC units with a decomposition mechanism can be utilized to represent the mixed-type defect feature, which can then be fused by standard convolutional processes. In order to cope with the varied graphic characteristics in mixed-type wafer defect pattern recognition, a deformable convolutional network method is advanced.

A 2D convolution consists of sampling a 2D regular grid $\mathcal{R}$ over an input feature map $\textbf{x}$. The values in the grid are summed and weighted by \textbf{w}. The grid $\mathcal{R}$ defines the receptive field of the convolution. For instance, the grid
\begin{center}
    $\mathcal{R}$=\{(-1,-1),(-1,0),....,(0,1),(1,1)\}
\end{center}
describes a $3\times3$ kernel with dilation 1.
The output feature  map $\textbf{y}$ is then computed as
\begin{equation}
\label{conv}
      \textbf{y}(\textbf{p}_0)=\sum\limits_{\textbf{p}_n \in \mathcal{R}} \textbf{w}(\textbf{p}_n)\cdot \textbf{x}(\textbf{p}_0 + \textbf{p}_n)
\end{equation}
where $\textbf{p}_0$ is a location on $\textbf{y}$.

In deformable convolution, the regular grid $\mathcal{R}$ is augmented with offsets $\{\Delta\textbf{p}_n\ | n=1,2,..,|\mathcal{R}|\}$. The output feature map in deformable convolution is then defined as

\begin{equation}
\label{dconv}
      \textbf{y}(\textbf{p}_0)=\sum\limits_{\textbf{p}_n \in \mathcal{R}} \textbf{w}(\textbf{p}_n)\cdot \textbf{x}(\textbf{p}_0 + \textbf{p}_n + \Delta\textbf{p}_n )
\end{equation}

Figure \ref{fig:conv-deform} shows the difference between convolution and its deformable variant. The convolutional kernel focuses on a fixed set of windows whereas the deformable convolution with learned offsets emphasizes more on the global feature sets.


\subsection{Vision Transformers}
Transformers\cite{ref8} are an effective model based on a self-attention mechanism developed to overcome the problems of long-range sequence forgetting in LSTM and gated-RNNs. The transformers were originally designed for tasks in NLP\cite{ref-13} but have also been successful in vision-related tasks. Vision Transformers (ViT) \cite{ref9} mainly comprise of the following components:

\textbf{Image Tokenization}. The input to a standard transformer is a sequence of vectors known as \textit{tokens}. These tokens are generally embeddings of words in sentences. For NLP tasks, the ordering of tokens is natural but when the input is an image, it is not so obvious. To tokenize an image $\textbf{x}\in \mathbb{R}^{C\times H\times W}$, the tokenization layer subdivides the image into patches $x_p\in \mathbb{R}^{P^2\times C}$. Each token is obtained by flattening non-overlapping $P\times P$ patches in the image and then transforming to latent vectors of dimension $d$.

\textbf{Positional Embedding}: The ordering of the input to the transformer is crucial. Sequential data in NLP has natural ordering but to add order in the patches of an image, additional positional embedding is added to each token $x_p$. This positional embedding essentially supplements the spatial information into the sequence.
Generally, the positional embeddings in the tokens are fixed vectors generated from two sine waves with high frequencies but they can also be learned. The positional embeddings provide sufficient information for the model to learn that there exists a positional relationship between the set of tokens.

\textbf{Transformer Encoder}: A transformer comprises of several stacked embedding layers. Each encoding module consists of two sub-layers: Multi-Headed Self-Attention (MHA) and a Multi-Layer Perceptron (MLP) head. Each sub-layer is preceded by a layer normalization (LN), and followed by a residual connection to the next sub-layer.

\textbf{Classification}: Along with the positional embedding, vision transformers typically add an extra learnable token to the sequence of embedded patches. This \texttt{[class]} token adds the information about the class of input image and its state after the transformer encoder can be used for classification. The final classification is performed by a classification head attached to the output of the transformer encoder implemented by an MLP.



\section{Method}
\label{method}
\subsection{Dataset and Preprocessing}
\begin{figure}
    \centering 
     \fbox{\includegraphics[width=12cm,height=5.5cm]{images/One-defect.pdf}}\\
     \fbox{\includegraphics[width=12cm,height=8cm]{images/Two-defect.pdf}}\\
     \fbox{\includegraphics[width=12cm,height=5.5cm]{images/Three-defect.pdf}}\\
      \fbox{\includegraphics[width=12cm,height=2.7cm]{images/Four-defect.pdf}}
    \caption{The 38 types of original wafer map defects in the MixedWM38 dataset.   }
    \label{fig:38def}
\end{figure}


\label{sec:dataset}
We conduct our experiments on  MixedWM38 WaferMap\cite{ref3} dataset.
It contains 38K wafer maps comprising both original and synthetically generated wafer maps images. Due to variability in designs, the shape of the wafer map may differ. Therefore, we resize all wafer images to 52$\times$52. The wafer maps are classified into 38 categories based on the type and number of defects present in a wafer map. Out of 38 classes, the \textit{normal} class does not contain any defect, 8 have a single unique defect and the remaining have various uniquely identified combinations of single defects.
A sample of each class is presented in Figure \ref{fig:38def}. The single defects are Center(C), Donut(D), Edge-Loc(EL), Edge-Ring(ER), Loc(L), Near-Full(NF), Scratch(S) and Random(R)\cite{ref-11}. We describe each of the defect patterns in Table \ref{tab:def_desc}.

\begin{table}[]
    \centering
    \begin{tabular}{cp{5cm}}
        Defect & Description \\
        \hline
        Center(C)&  Defect pattern concentrated at the center of the wafer\\
Donut(D) & Defect pattern in the shape of an annular disc/donut\\
Edge-Loc (EL)&   Defect pattern is concentrated around the edge of the wafer\\
Edge-Ring (ER)&  Global defect pattern occurring around the entire circumference of the wafer\\
Loc(L) & Defect pattern in a concentrated region other than the center.\\
Near-Full (NF)& Defect pattern present throughout the entirety of the wafer,\\
Scratch(S) & Defect pattern in the form of a narrow line along the wafer.\\
Random(R) & Defect pattern at random locations.    \\
\hline
    \end{tabular} 
    \caption{Unique defect patterns in MixedWM38 dataset.}
    \label{tab:def_desc}
\end{table}
Some of the defects are observed more frequently than others. For example, Near-Full defect is seldom observed. Apart from single defects, wafers possess different combinations of single defects. The mixed defect patterns have been conveniently named as
2 mixed-type, 3 mixed-type, and 4 mixed-type based on the number of unique single-type defects present in a wafer. The defect classes are labeled as C1 through C38. C1 is the ``normal" class free of any defects. C1 is an important category as detecting normal wafers is critical to the fabrication process. C2-C9 form the single defect patterns. 2 mixed-type defects are allotted C10 to C22, while 3 mixed-type defect patterns are identified as C23 through C34. Finally, the wafers with 4 single-type defects on them are
labeled as C35-C38. 

The pattern name for each mixed defect pattern defines the unique defects present in it. For example, class C30 is named D+ER+L, which means it consists of a Donut, Edge-Ring, and Loc defect patterns. 

\textbf{Label encoding.} Previous works[cite DCNET]showed that using a one-hot label as a label for each class produces the best results. We follow the same strategy to label the classes. Each label vector is an 8-length binary vector consisting of 1's at position $i$ if $i$'th defect is present in the wafer. 
For example, class C30 is encoded as \texttt{[0 1 0 1 1  0 0 0]} having 1's at positions 2, 4, and 5,
which corresponds to Donut, Edge-Ring, and Loc defect patterns being together simultaneously.
This scheme contrasts with the normal encoding where each class is a binary vector with a length equal to the number of classes.

\subsection{Architecture}
The network consists of two modules. Firstly, a deformable convolution module that takes the wafer map as input and outputs feature maps. The feature map outputted is passed to the transformer as tokens. The deformable convolution head consists of 2 layers where each layer is a 7$\times$7 deformable convolution followed by ReLU activation and 3$\times$3 max-pool operation. The output of the tokenizer head is a 16$\times$516 tensor where 516 is the embedding dimension agreed upon beforehand. These tokens are then fed to the transformer module to get the classification score. The transformer module consisted of 12 encoder-decoder layers each having 12 attention heads. The attention dropout and stochastic depth rate are set to 0.1 each. The transformer head outputs 768-dimensional embedding vectors which are transformed to an 8-dimensional classification score using an MLP.

Let $f$ be the proposed DC transformer parametrized by $\theta$. The training aims to minimize 
\begin{equation}
\mathcal{L}=\min_\theta\sum\limits_{(\textbf{x},\textbf{y})} \mathcal{L}_{bce} (\sigma(f(\textbf{x})),\textbf{y})
    \label{eq:loss}
\end{equation}

%    \mathcal{L_{KD}}= \alpha\sum_{i=1}^m(f(x^{(i)})-g(x^{(i)}))^2 + (1-\alpha) \sum_{i=1}^m  \mathcal{L}_{bce}(g(x^{(i)}),\textit{rnd}(f(x^{(i)})) )
where $\sigma$ is the \textit{sigmoid} function defined as $\sigma(x)=\frac{1}{1+e^{-x}}$ and $\mathcal{L}_{bce}$ is the \textit{binary cross entropy} error defined as $\mathcal{L}_{bce}(x,y)= -ylog(x)-(1-y)log(1-x)$. It is worth noting that a wafer is considered correctly classified if and only if all the defects in the wafer are correctly identified individually.

\begin{table}[]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{|p{1.2cm}|p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.3cm}p{.5cm}|p{.9cm}|}
\toprule
\hline
Class &     C1 &     C2 &      C3 &      C4 &      C5 &      C6 &      C7 &      C8 &      C9 &     C10 &     C11 &     C12 &     C13 &     C14 &     C15 &     C16 &    C17 &     C18 &     C19 &     C20 &     C21 &     C22 &     C23 &     C24 &     C25 &     C26 &     C27 &     C28 &     C29 &     C30 &     C31 &    C32 &     C33 &     C34 &     C35 &     C36 &     C37 &     C38 &  Recall \\
\midrule
\hline
C1        &    206 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  100.0\% \\
C2        &      0 &    178 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       1 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  99.44\% \\
C3        &      0 &      0 &     217 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      1 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  99.54\% \\
C4        &      2 &      0 &       0 &     191 &       5 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  96.46\% \\
C5        &      0 &      0 &       0 &       1 &     214 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  99.53\% \\
C6        &      0 &      0 &       0 &       0 &       0 &     192 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       1 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  99.48\% \\
C7        &      1 &      0 &       0 &       0 &       0 &       0 &      29 &       0 &       2 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  90.62\% \\
C8        &      0 &      0 &       0 &       0 &       0 &       0 &       0 &     160 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       1 &       0 &       0 &       1 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  98.77\% \\
C9        &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &     144 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       1 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  99.31\% \\
C10       &      0 &      1 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &     199 &       3 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       1 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  97.55\% \\
C11       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       1 &     187 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       1 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  98.94\% \\
C12       &      0 &      3 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &     196 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       7 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  95.15\% \\
C13       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &     207 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       1 &       0 &       0 &       1 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  99.04\% \\
C14       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &     183 &       3 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       1 &       1 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  97.34\% \\
C15       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       1 &     214 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       1 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  99.07\% \\
C16       &      0 &      0 &       4 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &     201 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      4 &       0 &       0 &       0 &       0 &       0 &       0 &  96.17\% \\
C17       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &    195 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      1 &       0 &       0 &       0 &       0 &       0 &       0 &  99.49\% \\
C18       &      0 &      0 &       0 &       4 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &     218 &       0 &       0 &       0 &       0 &       1 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       3 &       0 &       0 &       0 &       0 &       0 &  96.46\% \\
C19       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       2 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &     212 &       0 &       2 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  98.15\% \\
C20       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &     181 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  100.0\% \\
C21       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &     205 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       1 &       0 &       0 &       0 &       0 &  99.51\% \\
C22       &      0 &      0 &       0 &       0 &       0 &       3 &       0 &       2 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &     195 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &   97.5\% \\
C23       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       3 &       0 &       1 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &     192 &       0 &       1 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       6 &       0 &       0 &       0 &  94.58\% \\
C24       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       2 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &     390 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       2 &       0 &       0 &       0 &  98.98\% \\
C25       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       2 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       1 &       0 &     180 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       2 &       0 &       0 &   97.3\% \\
C26       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       1 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       1 &       0 &     177 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &  98.88\% \\
C27       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       3 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &     202 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       1 &       0 &       0 &       0 &  98.06\% \\
C28       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       2 &       0 &       2 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &     186 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       4 &       0 &  95.88\% \\
C29       &      0 &      0 &       1 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       1 &       0 &       0 &      1 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &     200 &       0 &       9 &      0 &       0 &       0 &       0 &       0 &       2 &       0 &  93.46\% \\
C30       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       3 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &     219 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       6 &  96.05\% \\
C31       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       1 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &     199 &      0 &       0 &       0 &       0 &       0 &       0 &       2 &  98.51\% \\
C32       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       2 &      3 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &    186 &       0 &       0 &       0 &       0 &       0 &       0 &  97.38\% \\
C33       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       6 &       0 &       0 &       0 &       3 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &     184 &       2 &       0 &       0 &       0 &       0 &  94.36\% \\
C34       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       3 &       1 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &     191 &       0 &       0 &       0 &       0 &  97.95\% \\
C35       &      1 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       1 &       3 &       0 &       0 &       1 &       0 &       0 &       0 &       0 &      0 &       1 &       0 &     186 &       5 &       0 &       0 &  93.94\% \\
C36       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      10 &       5 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       2 &       0 &     191 &       0 &       0 &  91.83\% \\
C37       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       3 &       4 &       0 &       0 &      5 &       0 &       0 &       0 &       0 &     197 &       7 &   91.2\% \\
C38       &      0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &      0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       0 &       6 &       2 &      0 &       0 &       0 &       0 &       0 &       0 &     193 &  96.02\% \\
\hline
Precision &  \rotatebox[origin=c]{90}{98.1\%} &  \rotatebox[origin=c]{90}{ 97.8\% } &  \rotatebox[origin=c]{90}{ 97.75\% } &  \rotatebox[origin=c]{90}{ 97.45\% } &  \rotatebox[origin=c]{90}{ 97.72\% } &  \rotatebox[origin=c]{90}{ 98.46\% } &  \rotatebox[origin=c]{90}{ 100.0\% } &  \rotatebox[origin=c]{90}{ 97.56\% } &  \rotatebox[origin=c]{90}{ 98.63\% } &  \rotatebox[origin=c]{90}{ 98.03\% } &  \rotatebox[origin=c]{90}{ 96.89\% } &  \rotatebox[origin=c]{90}{ 99.49\% } &  \rotatebox[origin=c]{90}{ 97.18\% } &  \rotatebox[origin=c]{90}{ 97.86\% } &  \rotatebox[origin=c]{90}{ 96.83\% } &  \rotatebox[origin=c]{90}{ 98.05\% } &  \rotatebox[origin=c]{90}{ 97.5\% } &  \rotatebox[origin=c]{90}{ 97.32\% } &  \rotatebox[origin=c]{90}{ 99.53\% } &  \rotatebox[origin=c]{90}{ 98.37\% } &  \rotatebox[origin=c]{90}{ 98.56\% } &  \rotatebox[origin=c]{90}{ 97.01\% } &  \rotatebox[origin=c]{90}{ 97.96\% } &  \rotatebox[origin=c]{90}{ 98.73\% } &  \rotatebox[origin=c]{90}{ 94.24\% } &  \rotatebox[origin=c]{90}{ 96.72\% } &  \rotatebox[origin=c]{90}{ 95.73\% } &  \rotatebox[origin=c]{90}{ 97.89\% } &  \rotatebox[origin=c]{90}{ 97.56\% } &  \rotatebox[origin=c]{90}{ 97.33\% } &  \rotatebox[origin=c]{90}{ 94.31\% } &  \rotatebox[origin=c]{90}{ 94.9\% } &  \rotatebox[origin=c]{90}{ 97.87\% } &  \rotatebox[origin=c]{90}{ 97.45\% } &  \rotatebox[origin=c]{90}{ 95.38\% } &  \rotatebox[origin=c]{90}{ 96.46\% } &  \rotatebox[origin=c]{90}{ 97.04\% } &  \rotatebox[origin=c]{90}{ 92.79\% } &     - \\
\hline
\bottomrule
\end{tabular}
}
    % \captionsetup{justification=centering} 
    \\
    \caption{Confusion matrix of predictions made by DC Transformer.}
    \label{tab: confusion matrix}
\end{table}

\section{Experiments and Results}
\label{results}
In this section, we evaluate the performance of our DC Transformer. We conduct our experiments on the MixedWM38 dataset containing 38K wafer images as described in \ref{sec:dataset}. We randomly sample 80\% of the dataset as a training set and the remaining 20\% as the test set. The experiments are conducted in Python compiler, Pytorch 1.12, and CUDA 11.3, the computer with option: Linux system, Intel(R) Xeon(R) CPU @ 2.20GHz and Tesla T4 GPU. 

\begin{table}[h]
    \centering
    
    \begin{tabular}{c|c}
        Description & Value\\
        \hline
         Initial Learning rate & 0.01 \\
         Optimizer & SGD\\
         Scheduler & Cosine Annealing \\
         Batch Size & 64\\
         Decay &  $5e^{-4}$\\
         Momentum &   0.9\\
    \end{tabular}

    \caption{Experimental Parameter Settings used while Training}
    \label{experimental setup}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{cc}
        \includegraphics[scale=0.3]{images/accuracies-1-9.pdf} & \includegraphics[scale=0.3]{images/accuracies-10-22.pdf} \\
        \includegraphics[scale=0.3]{images/accuracies-23-34.pdf} & \includegraphics[scale=0.3]{images/accuracies-34-38.pdf}
    \end{tabular}
    \caption{We report the accuracy of DC-Transformer on DPR based on number of defects on the wafer. (Top, Left to Right) Accuracy of DC Transformer and comparable models on single-type defects and mixed type 2 defects. (Bottom, Left to Right) Accuracy of DC Transformer and comparable models on mixed type 3 defects and mixed type 4 defects. }
    \label{tab:acc-comp}
\end{table}

% \begin{figure}
%     \centering
%     \includegraphics[scale=0.3]{images/accuracies_1-9.pdf}
%     \caption{Accuracy of DC Transformer and comparable models on single type defects.}
%     \label{fig:acc1}
% \end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[scale=0.3]{images/accuracies_10-22.pdf}
%     \caption{Accuracy of DC Transformer and comparable models on mixed type 2 defects.}
%     \label{fig:acc10}
% \end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[scale=0.3]{images/accuracies_23-34.pdf}
%     \caption{Accuracy of DC Transformer and comparable models on mixed type 3 defects.}
%     \label{fig:acc23}
% \end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[scale=0.3]{images/accuracies_34-38.pdf}
%     \caption{Accuracy of DC Transformer and comparable models on mixed type 4 defects.}
%     \label{fig:acc34}
% \end{figure}

\begin{table}[]
    \centering
    \begin{tabular}{cc}
           \includegraphics[scale=0.5]{images/train-test-acc.pdf}  &     \includegraphics[scale=0.5]{images/train-test-loss.pdf} \\
         & 
    \end{tabular}
    \caption{(Left) We report the train and test accuracy of the DC Transformer. The proposed method quickly learns the classification mapping within a few epochs. (Right) The train and test loss drops quickly and stabilizes.  }
    \label{tab:acc-loss}
\end{table}
% \begin{figure}
%     \centering
%     \includegraphics[scale=0.5]{images/train_test_acc.pdf}
%     \caption{We report the train and test accuracy of the DC Transformer. The proposed method quickly learns the classification mapping within a few epochs.}
%     \label{train_test_acc}
% \end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[scale=0.5]{images/train_test_loss.pdf}
%     \caption{We report the train and test loss of the DC Transformer. The loss drops quickly}
%     \label{train_test_loss}
% \end{figure}

\textbf{Results on MixedWM38 dataset.} We train the network with the setting mentioned in Table \ref{experimental setup} for 100 epochs. During training, the accuracy and loss curves are presented in Figure \ref{tab:acc-loss}. We observe that the model very quickly learns the mapping within the first few epochs and the training error becomes less than 0.1\% within 20 epochs. This shows that the DC Transformer converges quickly and reaches maximum accuracy. Moreover, as reaching the best accuracy, the model loss remains steady demonstrating the stability of the DC Transformer. Additionally, the small difference between the blue and orange curves (the train loss curve and validation loss curve, respectively) shows that our model avoided over-fitting during training. This demonstrates that DC Transformer is able to quickly converge to a reasonably low level of validation loss, hence assuring its performance in mixed-type DPR. 

Table \ref{tab:accuracy} shows the results of DC Transformer compared with GRU\cite{ref10}, LSTM\cite{ref11}, and DC-Net\cite{ref3} which are considered as state-of-the-art models for mixed type wafer defect classification. The DC Transformer performs competitively on the validation set outperforming all three evaluated models. The overall validation accuracy of DC Transformer is $97.29\%$ which is $3.08\%$ higher than GRU and $3.09\%$, $4.09\%$ higher than LSTM and DC Net respectively. This demonstrates the significant gain in performance of DC Transformers compared to other contemporary methods used in this study.

\begin{table}[ht]
    \centering
    \resizebox{.5\textwidth}{!}{
\begin{tabular}{ccccc}
\toprule
  Class &    GRU &   LSTM & DC Net & DC Transformer \\
  \hline
\midrule
 C1-C9 & 96.52\% & 96.85\% & 97.04\% &         98.07\% \\
C10-C22 & 92.57\% &  93.9\% & 94.64\% &          97.8\% \\
C23-C34 & 93.54\% & 92.06\% & 89.74\% &         96.73\% \\
C35-C38 &  92.5\% & 92.42\% & 89.61\% &         94.05\% \\
\bottomrule
\hline
\end{tabular}
}
    \caption{Accuracy of compared models based on the number of defect patterns present in the wafer.}
    \label{tab:per_class}
\end{table}

We also report the accuracy of the proposed DC Transformer on the basis of a number of defects present on the wafer in Table \ref{tab:per_class}. The average accuracy of the DC Transformer outperforms all three evaluated models. As shown in Figure \ref{tab:acc-comp}, the proposed network performs remarkably better in wafer maps with $\geq3$ defects when compared to other methods. The average accuracy of DC Transformer from C23 to C34 is $96.73\%$ which is $3.94\%$ higher than GRU, $4.67\%$ higher than LSTM, and $6.99\%$ higher than DC Net. Similarly, from C35 to C38, DC Transformer attains an accuracy of $94.05\%$ which is $1.55\%$, $1.63\%$, and $4.44\%$ higher than the GRu, LSTM, and DC Net respectively. This demonstrates that the proposed DC Transformer is very effective for complex mixed-type patterns compared to other methods used in this study. The defect class C1 denotes the normal wafer free of any defects. Correctly classifying C1 is crucial as any misclassification may lead to reliability issues later in the production stage. The proposed method achieves an accuracy of $100\%$ for C1, making it an effective tool for industrial practice.

To accurately measure false classification and missing classification, we evaluate the proposed DC Transformer on the statistical parameters of \textit{Precision}, \textit{Recall} and \textit{F1} which are defined below:

True Positive (TP): predicting positive, the actual is positive.

False Positive (FP): predicting positive, the actual is negative.

False Negative (FN): predicting negative, the actual is positive.

True Negative (TN): predicting negative, the actual is negative.

\begin{equation}
    Recall= \frac{TP}{TP+FN} \\
\end{equation}
\begin{equation}
    Precision= \frac{TP}{TP+FP} \\
\end{equation}
\begin{equation}
    F1= \frac{2\cdot Precision\times Recall}{Precision + Recall}
\end{equation}

\begin{table}[t!]
    \centering
   \begin{tabular}{cccc}
\toprule
  Class & Precision & recall &     F1 \\
  \hline
\midrule
     C1 &     98.1\% & 100.0\% & 99.04\% \\
     C2 &     97.8\% & 99.44\% & 98.61\% \\
     C3 &    97.75\% & 99.54\% & 98.64\% \\
     C4 &    97.45\% & 96.46\% & 96.95\% \\
     C5 &    97.72\% & 99.53\% & 98.62\% \\
     C6 &    98.46\% & 99.48\% & 98.97\% \\
     C7 &    100.0\% & 90.62\% & 95.08\% \\
     C8 &    97.56\% & 98.77\% & 98.16\% \\
     C9 &    98.63\% & 99.31\% & 98.97\% \\
    C10 &    98.03\% & 97.55\% & 97.79\% \\
    C11 &    96.89\% & 98.94\% & 97.91\% \\
    C12 &    99.49\% & 95.15\% & 97.27\% \\
    C13 &    97.18\% & 99.04\% &  98.1\% \\
    C14 &    97.86\% & 97.34\% &  97.6\% \\
    C15 &    96.83\% & 99.07\% & 97.94\% \\
    C16 &    98.05\% & 96.17\% &  97.1\% \\
    C17 &     97.5\% & 99.49\% & 98.48\% \\
    C18 &    97.32\% & 96.46\% & 96.89\% \\
    C19 &    99.53\% & 98.15\% & 98.83\% \\
    C20 &    98.37\% & 100.0\% & 99.18\% \\
    C21 &    98.56\% & 99.51\% & 99.03\% \\
    C22 &    97.01\% &  97.5\% & 97.26\% \\
    C23 &    97.96\% & 94.58\% & 96.24\% \\
    C24 &    98.73\% & 98.98\% & 98.86\% \\
    C25 &    94.24\% &  97.3\% & 95.74\% \\
    C26 &    96.72\% & 98.88\% & 97.79\% \\
    C27 &    95.73\% & 98.06\% & 96.88\% \\
    C28 &    97.89\% & 95.88\% & 96.88\% \\
    C29 &    97.56\% & 93.46\% & 95.47\% \\
    C30 &    97.33\% & 96.05\% & 96.69\% \\
    C31 &    94.31\% & 98.51\% & 96.37\% \\
    C32 &     94.9\% & 97.38\% & 96.12\% \\
    C33 &    97.87\% & 94.36\% & 96.08\% \\
    C34 &    97.45\% & 97.95\% &  97.7\% \\
    C35 &    95.38\% & 93.94\% & 94.66\% \\
    C36 &    96.46\% & 91.83\% & 94.09\% \\
    C37 &    97.04\% &  91.2\% & 94.03\% \\
    C38 &    92.79\% & 96.02\% & 94.38\% \\
    \hline
Average &    97.33\% & 97.16\% & 97.22\% \\
\hline
\bottomrule
\end{tabular}


    \caption{The Performance of DC Transformer in Precision and Recall}
    \label{tab:pred-rec}
\end{table}

The proposed DC transformer procures high precision and recall, both nearing $100\%$. This shows that the DC Transformer makes reliable predictions and misses correct classification meagerly.

As shown in Figure \ref{tab:accuracy} DC Transformer significantly outperformed the baseline approaches on defects of the 3-mixed-type and 4-mixed-type. The combination of deformable convolution's  global perception scale along with Transformer's global perception scale aids the entire model in combining information  in various aspects and obtaining both detailed and complete features. In conclusion, the majority of the correct classification were made by our model, which also maintained the highest levels of precision, recall, and accuracy among the three approaches compared in this study. Additionally, the proposed model demonstrated its aptitude for identifying more complex defects, whereas previous approaches only achieved relatively low scores for the same defects. As a result, our experiment has conclusively shown that DC Transformer can outperform other approaches in mixed-type DPR and achieve state-of-the-art performance.



\begin{table}[h]
    \centering
\begin{tabular}{ccccc}
\toprule
  Class &    GRU &   LSTM & DC Net & DC Transformer \\
  \hline
\midrule
      C1 & 100.0\% & 100.0\% &  99.7\% &         100.0\% \\
     C2 & 97.21\% & 98.32\% &  97.8\% &         99.44\% \\
     C3 & 96.79\% & 98.17\% &  96.5\% &         99.54\% \\
     C4 & 93.43\% & 93.94\% &  94.4\% &         96.46\% \\
     C5 & 96.28\% & 95.35\% &  99.8\% &         99.53\% \\
     C6 & 96.37\% & 95.34\% &  93.8\% &         99.48\% \\
     C7 & 100.0\% & 99.31\% &  95.8\% &         90.62\% \\
     C8 &  96.3\% & 98.77\% &  93.4\% &         98.77\% \\
     C9 & 93.75\% & 93.75\% & 100.0\% &         99.31\% \\
    C10 &  95.1\% & 95.59\% &  99.2\% &         97.55\% \\
    C11 & 97.35\% & 95.24\% &  97.9\% &         98.94\% \\
    C12 & 92.72\% & 94.66\% &  98.5\% &         95.15\% \\
    C13 & 95.22\% & 99.52\% &  96.7\% &         99.04\% \\
    C14 & 93.62\% & 94.68\% &  99.3\% &         97.34\% \\
    C15 & 97.22\% & 92.13\% &  96.1\% &         99.07\% \\
    C16 & 87.08\% & 90.91\% &  98.3\% &         96.17\% \\
    C17 & 93.37\% & 96.43\% &  92.8\% &         99.49\% \\
    C18 & 90.27\% & 92.48\% &  93.9\% &         96.46\% \\
    C19 & 92.59\% & 93.52\% &  92.3\% &         98.15\% \\
    C20 & 91.16\% & 93.37\% &  94.6\% &         100.0\% \\
    C21 & 96.12\% & 97.57\% &  90.7\% &         99.51\% \\
    C22 &  93.5\% &  93.5\% &  90.3\% &          97.5\% \\
    C23 & 83.25\% &  86.7\% &  88.9\% &         94.58\% \\
    C24 & 98.48\% & 96.45\% &  89.4\% &         98.98\% \\
    C25 & 92.97\% & 88.11\% &  91.4\% &          97.3\% \\
    C26 & 97.77\% & 95.53\% &  92.5\% &         98.88\% \\
    C27 & 95.63\% &  93.2\% &  90.5\% &         98.06\% \\
    C28 & 91.24\% & 95.88\% &  88.3\% &         95.88\% \\
    C29 & 91.59\% & 89.25\% &  90.5\% &         93.46\% \\
    C30 & 96.05\% & 97.37\% &  92.3\% &         96.05\% \\
    C31 & 92.08\% & 92.57\% &  91.5\% &         98.51\% \\
    C32 & 92.67\% & 87.43\% &  88.3\% &         97.38\% \\
    C33 & 89.74\% & 88.21\% &  86.2\% &         94.36\% \\
    C34 & 93.33\% & 93.85\% &  89.0\% &         97.95\% \\
    C35 & 90.91\% & 86.87\% &  87.0\% &         93.94\% \\
    C36 & 90.87\% & 89.42\% &  90.6\% &         91.83\% \\
    C37 & 90.74\% & 90.28\% &  86.4\% &          91.2\% \\
    C38 & 94.53\% & 96.02\% &  88.2\% &         96.02\% \\
    \hline
Average & 94.21\% & 94.12\% & 93.20\% &         97.29\% \\
\bottomrule
\hline
\end{tabular}

    \caption{Experimental results of DC Transformer compared with state-of-the-art models.}
    \label{tab:accuracy}
\end{table}



\section{Conclusion}
\label{conclusion}
In this work, we have introduced a novel transformer-based model equipped with a deformable convolution head, namely DC Transformer for mixed-type defect pattern recognition. We combine deformable CNNs along with a transformer to effectively generalize the global defect information in the wafer maps. The deformable convolutional head helps the whole network to better focus on important areas and suppress unimportant features.  The experiments clearly demonstrate the effectiveness of the proposed DC Transformer compared to the state-of-the-art model's performances. We measure the performance of DC Transformer on various benchmarks including defect counts per wafer. The DC Transformer consistently and stably recognizes the defects especially on wafers with multiple defects.

%Bibliography
% \bibliographystyle{unsrt}  
% \bibliography{references}  
\begin{thebibliography}{1}

\bibitem{ref-1}
 L. Mönch, R. Uzsoy, and J. W. Fowler, "A survey of semiconductor supply chain models part I: Semiconductor supply chains strategic network design and supply chain simulation", Int. J. Prod. Res., vol. 56, no. 13, pp. 4524-4545, 2017.
 \bibitem{ref-2}
 W.-T. K. Chen and C. H.-J. Huang, "Practical ‘building-in reliability’ approaches for semiconductor manufacturing", IEEE Trans. Rel., vol. 51, no. 4, pp. 469-481, Dec. 2002.
 \bibitem{ref-3}
  S. F. Yang and W.-T. K. Chien, "Electromigration lifetime optimization by uniform designs and a new lifetime index", IEEE Trans. Rel., vol. 64, no. 4, pp. 1158-1163, Dec. 2015.

\bibitem{ref-4}
F. Adly et al., "Simplified subspaced regression network for identification of defect patterns in semiconductor wafer maps", IEEE Trans. Ind. Informat., vol. 11, no. 6, pp. 1267-1276, Dec. 2015.
\bibitem{ref-5}
C.-F. Chien, C.-Y. Hsu and P.-N. Chen, "Semiconductor fault detection and classification for yield enhancement and manufacturing intelligence", Flexible Services Manuf. J., vol. 25, no. 3, pp. 367-388, 2013.
\bibitem{ref-6}
S. Kang, S. Cho, D. An and J. Rim, "Using wafer map features to better predict die-level failures in final test", IEEE Trans. Semicond. Manuf., vol. 28, no. 3, pp. 431-437, Aug. 2015.
\bibitem{ref-7}
K. Kyeong and H. Kim, "Classification of mixed-type defect patterns in wafer bin maps using convolutional neural networks", IEEE Trans. Semicond. Manuf., vol. 31, no. 3, pp. 395-402, Aug. 2018.
\bibitem{ref-8}
Y. Xia, H. Yu and F. Wang, "Accurate and robust eye center localization via fully convolutional networks", IEEE/CAA J. Automatica Sinica, vol. 6, no. 5, pp. 1127-1138, Sep. 2019.
\bibitem{ref-9}
L. Wen, X. Li, L. Gao and Y. Zhang, "A new convolutional neural network-based data-driven fault diagnosis method", IEEE Trans. Ind. Electron., vol. 65, no. 7, pp. 5990-5998, Jul. 2017.
\bibitem{ref-10}
 X. Wang, K. C. K. Chan, K. Yu, C. Dong and C. C. Loy, "EDVR: Video restoration with enhanced deformable convolutional networks", Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Work, pp. 1954-1963, Jun. 2019.
 \bibitem{ref-11}
 M.-J. Wu, J.-S. R. Jang and J.-L. Chen, "Wafer map failure pattern recognition and similarity ranking for large-scale data sets", IEEE Trans. Semicond. Manuf., vol. 28, no. 1, pp. 1-12, Feb. 2015.
 \bibitem{ref-12}
 Y. Kong and D. Ni, "Recognition and location of mixed-type patterns in wafer bin maps", Proc. IEEE Int. Conf. Smart Manuf. Ind. Logist. Eng. (SMILE), pp. 4-8, 2019.

 \bibitem{ref-13}
 P. M. Nadkarni, L. Ohno-Machado and W. W. Chapman, "Natural language processing: An introduction", J. Amer. Med. Inform. Assoc., vol. 18, no. 5, pp. 544-551, 2011.
\bibitem{ref1}
Kim, Y. Lee, and H. Kim, “Detection and clustering of mixed type defect patterns in wafer bin maps,” IISE Trans., vol. 50, no. 2,
pp. 99–111, 2018.
\bibitem{ref2}
J. Wang, Z. Yang, J. Zhang, Q. Zhang, and W.-K. Chien, “AdaBalGAN:
An improved generative adversarial network with imbalanced learning
for wafer defective pattern recognition,” IEEE Trans. Semicond. Manuf.,
vol. 32, no. 3, pp. 310–319, Aug. 2019.


\bibitem{ref3}
 J. Wang, C. Xu, Z. Yang, J. Zhang, and X. Li, “Deformable convolutional
networks for efficient mixed-type wafer defect pattern recognition,”
IEEE Trans. Semicond. Manuf., vol. 33, no. 4, pp. 587–596, Nov. 2020.

\bibitem{ref4}
J. Zhu, L. Fang and P. Ghamisi, "Deformable convolutional neural networks for hyperspectral image classification", IEEE Geosci. Remote Sens. Lett., vol. 15, no. 8, pp. 1254-1258, Aug. 2018.

\bibitem{ref5}F. Liu, D. Liu, J. Tian, X. Xie, X. Yang and K. Wang, "Cascaded one-shot deformable convolutional neural networks: Developing a deep learning model for respiratory motion estimation in ultrasound sequences", Med. Image Anal., vol. 65, Oct. 2020.

\bibitem{ref6}
 J. Dai et al., "Deformable convolutional networks", Proc. Int. Conf. Comput. Vis., pp. 764-773, 2017.

\bibitem{ref7}
Cao, D., Chen, Z. and Gao, L. An improved object detection algorithm based on multi-scaled and deformable convolutional neural networks. Hum. Cent. Comput. Inf. Sci. 10, 14 (2020). https://doi.org/10.1186/s13673-020-00219-9

\bibitem{ref8}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, u., and Polosukhin, I. (2017). Attention is All you Need. In Advances in Neural Information Processing Systems. Curran Associates, Inc..

\bibitem{ref9}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR.
\bibitem{ref10}
K. Cho et al., "Learning phrase representations using RNN encoder-decoder for statistical machine translation" in arXiv:1406.1078, 2014.
\bibitem{ref11}
 X. Shi, Z. Chen, H. Wang, D. Yeung, W. Wong and W. Woo, "Convolutional LSTM network: A machine learning approach for precipitation nowcasting", Advances in Neural Information Processing Systems, pp. 802-810, 2015.



\end{thebibliography}


\end{document}
