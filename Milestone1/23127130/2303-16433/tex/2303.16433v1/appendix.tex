\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}
% printing the subsection counter in capital alphabetic form
\renewcommand{\thesubsection}{\Alph{subsection}}

\newtheorem{appdxlemma}{Lemma}
\numberwithin{appdxlemma}{subsection} % important bit
\numberwithin{equation}{subsection} % numbering of each equation to be of the form ([Appendix section index].[index of equation])

\subsection{Properties of the Feedback Utilization Strategy}\label{appd:delay-strategy}
\begin{proof}(Proof of Lemma~\ref{le:fdbk-prop})
For claim $(\romannum{1})$, our focus will be proving that $K^i_{\varnothing}(k) \leq \bar{d}(k) + 1$ when $\bar{d}(k) + 1 < k$, since it is evident that $K^i_{\varnothing}(k) \leq k$. 
For a fixed $k \in \nset{}{+}$, we denote the constant $D_k \coloneqq \bar{d}(k)$, and the delay times up to the iteration $k$ satisfy $d^i_t \leq D_k$ for $t=1, \ldots, k$. 
If each realized $\hat{J}^i_t$ arrives $D_k$ iterations later, then throughout the iterate, $(\hat{J}^i_t)_{t=1, \cdots, k - D_k}$ will be received in sequence. 
At the $t$-th iteration with $D_k + 2 \leq t \leq k$, player $i$ will have access to the estimate $G^i_{t-D_k}$ and employ it in the action update. 
The total count of iterations without action update equals $D_k + 1$.
Now we return to the case that the delay time is characterized by the random variable $d^i_t$. 
Since $d^i_t \leq D_k$, the estimates $(G^i_{t})_{t=2, \ldots, k-D_k}$ will be available no later than the constant case above and hence will be used in the action update, which further implies that $K^i_{\varnothing}(k) \leq \bar{d}(k) + 1$. 

For claim $(\romannum{2})$, we will prove it by induction. 
Before proceeding to the analysis, we make some notational conventions regarding the iteration indices and the sequence $(s^i(k))_{k \in \nset{}{+}}$. 
Recall that when the estimate cache $\mathcal{P}^i_G$ is empty at iteration $k$, $s^i(k)$ is manually set to $1$. 
We let $(\ell_m)_{m \in \nset{}{+}} \coloneqq (k: s^i(k) \neq 1)_{k \in \nset{}{+}}$ to denote the iteration indicies with action update. 
In addition, we will use $\mathcal{P}^i_G(k)$ to represent the temporary state of the cache $\mathcal{P}^i_G$ at line 14 of the $k$-th iteration and $t \in \mathcal{P}^i_G(k)$ to indicate $G^i_t \in \mathcal{P}^i_G(k)$. 

\textbf{Initial condition: }For $\ell_1$ and $G^i_{s^i(\ell_1)}$, we have either $s^i(\ell_1) + \bar{d}(s^i(\ell_1)) \geq \ell_1$ or $s^i(\ell_1)-1 + \bar{d}(s^i(\ell_1)-1) \geq \ell_1$; 
otherwise, $G^i_{s^i(\ell_1)}$ can be evaluated and consumed at an earlier iteration, since it is the first estimate and there is no queuing issue in $\mathcal{P}^i_G$. 

\textbf{Induction step: }For an arbitrary $k \in \nset{}{+}$, we assume that $s^i(\ell_k) + \bar{d}(s^i(\ell_k)) \geq \ell_k$ and need to show that the statement hold for $k+1$, i.e., $s^i(\ell_{k+1}) + \bar{d}(s^i(\ell_{k+1})) \geq \ell_{k+1}$. \\
\textbf{Case I ($s^i(\ell_{k+1}) \in \mathcal{P}^i_{G}(\ell_k)$): }
In this case, the first observation is that $s^i(\ell_{k+1}) \geq s^i(\ell_{k}) + 1$ since $\mathcal{P}^i_{G}$ pops out the estimate with the earliest timestamp and $s^i(\ell_k)$ is used first. 
Moreover, given that $G^i_{\ell_{k+1}}$ is already available at the $\ell_k$-th iteration, we have $\ell_{k+1} = \ell_k + 1$. 
Altogether, $s^i(\ell_{k+1}) + \bar{d}(s^i(\ell_{k+1})) \geq s^i(\ell_{k}) + 1 + \bar{d}(s^i(\ell_{k})) \geq \ell_k + 1 = \ell_{k+1}$. \\
\textbf{Case II ($s^i(\ell_{k+1}) \notin \mathcal{P}^i_{G}(\ell_k)$): } In the case where $s^i(\ell_{k+1}) \notin \mathcal{P}^i_{G}(\ell_k)$, it can be possible that $\abs{\mathcal{P}^i_G(\ell_k)} = 1$ or $\abs{\mathcal{P}^i_G(\ell_k)} \geq 2$ but $G^i_{s^i(\ell_{k+1})}$ becomes available at $\ell_k + 1$ and has the earliest timestamp among all available estimates in $\mathcal{P}^i_{G}(\ell_{k+1})$. 
In either case, we must have either $s^i(\ell_{k+1}) + \bar{d}(s^i(\ell_{k+1})) \geq \ell_{k+1}$ or $s^i(\ell_{k+1})-1 + \bar{d}(s^i(\ell_{k+1})-1) \geq \ell_{k+1}$ in the similar vein of the initial condition; otherwise, $G^i_{s^i(\ell_{k+1})}$ can be evaluated and consumed at an earlier stage or $s^i(\ell_{k+1}) \in \mathcal{P}^i_{G}(\ell_k)$. 
\end{proof}

\subsection{Proof of Lemma~\ref{le:bounded-rpg}}
\begin{proof}
We start by deriving a recurrent inequality for the sequence $(\hat{G}_k)_{k \in \nset{}{+}} = (G_{s(k)})_{k \in \nset{}{+}}$. 
For the segment corresponding to player $i$, we notice that
\begin{align*}
& \norm{\hat{G}^i_k}_* = \norm{G^i_{s^i(k)}}_* \leq \Bnorm{\frac{n^i}{\delta_{s^i(k)}}(\hat{J}^i_{s^i(k)} - \hat{J}^i_{s^i(k)-1})u^i_{s^i(k)}}_* \\
& \overset{(a)}{\leq} \frac{n^i}{\delta_{s^i(k)}} \abs{\langle \nabla_{x}J^i(\Tilde{X}), \hat{X}_{s^i(k)} - \hat{X}_{s^i(k)-1}\rangle}\norm{u^i_{s^i(k)}}_* \\
& \overset{(b)}{\leq} \frac{n^i}{\delta_{s^i(k)}} \bar{\nabla}_i \cdot \norm{\hat{X}_{s^i(k)} - \hat{X}_{s^i(k)-1}} \cdot \bar{u}^i_*, 
\end{align*}
where $(a)$ follows from the mean value theorem, and $\Tilde{X}$ denotes some convex combination of $\hat{X}_{s^i(k)}$ and $\hat{X}_{s^i(k)-1}$; 
in $(b)$, we take the maximum $\bar{\nabla}_i \coloneqq \max_{x \in \mathcal{X}} \norm{\nabla_x J^i(x)}_*$ and denote the constant $\bar{u}^i_* \coloneqq \norm{u}_*$ where $\norm{u}_2 = 1$. 
Based on this, we next derive a bound for $\norm{\hat{X}_{s^i(k)} - \hat{X}_{s^i(k)-1}}$ as follows
\begin{align*}
& \norm{\hat{X}_{s^i(k)} - \hat{X}_{s^i(k)-1}} \overset{(a)}{=} \norm{X_{s^i(k)} - X_{s^i(k)-1} + \delta_{s^i(k)}\varphi_{s^i(k)} \\
& \qquad - \delta_{s^i(k)-1}\varphi_{s^i(k)-1}} \overset{(b)}{\leq}  \norm{X_{s^i(k)} - X_{s^i(k)-1}} + \delta_{s^i(k)}\Bar{\varphi}, 
\end{align*}
where we let $\varphi_{s^i(k)} \coloneqq R^{-1}(p - X_{s^i(k)} + Ru_{s^i(k)})$ in $(a)$; 
for $(b)$, we can find a constant $\bar{\varphi}$ such that $\bar{\varphi} \geq \norm{\varphi_{s^i(k)} - (\delta_{s^i(k)-1}/\delta_{s^i(k)})\varphi_{s^i(k)-1}}$ given that $\varphi_{s^i(k)}$ resides inside a bounded set and the ratio $\delta_{s^i(k)-1}/\delta_{s^i(k)}$ is uniformly upper bounded by some constant. 
By applying the MD iterate and the $1/\Tilde{\mu}$-Lipschitz continuity of the mirror map from Lemma~\ref{le:md-lips}, we have 
\begin{align*}
& \norm{X_{s^i(k)} - X_{s^i(k)-1}}  = \norm{\nabla\psi^*(\nabla \psi(X_{s^i(k)-1}) - \gamma_{s^i(k)-1}\hat{G}_{s^i(k)-1})\\
& \qquad 
 - \nabla \psi^*(\nabla \psi(X_{s^i(k)-1}))}  \leq \frac{\gamma_{s^i(k)-1}}{\Tilde{\mu}}\norm{\hat{G}_{s^i(k)-1}}_*. 
\end{align*}
Thus, the pseudo-gradient of the game can be characterized by the following relation:
\begin{align*}
\norm{\hat{G}_k}_* & \leq \sum_{i \in \mathcal{N}} \norm{\hat{G}^i_k}_* \leq \sum_{i \in \mathcal{N}}\Big(\frac{\gamma_{s^i(k)-1}}{\delta_{s^i(k)}}\frac{n^i\bar{\nabla}_i\bar{u}^i_*}{\Tilde{\mu}}\norm{\hat{G}_{s^i(k)-1}}_*\Big)+\beta_1 \bar{\varphi}  \\
& \leq \frac{\beta_1}{\Tilde{\mu}}\sum_{i \in \mathcal{N}}\Big(\frac{\gamma_{s^i(k)-1}}{\delta_{s^i(k)}}\norm{\hat{G}_{s^i(k)-1}}_*\Big) + \beta_1 \bar{\varphi}, 
\end{align*}
where $\beta_1 \coloneqq \sum_{i \in \mathcal{N}} n^i\bar{\nabla}_i \bar{u}^i_*$. 
For an arbitrary random sample $\omega \in \Omega$, we can obtain the following deterministic inequality:
\begin{align*}
\norm{\hat{G}_k}_*(\omega) \leq \frac{\beta_1}{\Tilde{\mu}}\sum_{i \in \mathcal{N}}\Big(\frac{\gamma_{s^i(k)-1}}{\delta_{s^i(k)}}\norm{\hat{G}_{s^i(k)-1}}_*(\omega)\Big) + \beta_1 \bar{\varphi}. 
\end{align*}
Lemma~\ref{le:fdbk-prop} suggests that $s^i(k) \geq k - \bar{d}(s^i(k)) \geq k - \bar{d}(k)$, and we define the map $\pi_{\omega}: \nset{}{+} \to \nset{}{+}$ parameterized by $\omega \in \Omega$ as: 
\begin{align*}
\pi_{\omega}(k) = \argmax_{1\vee(k - \bar{d}(k))\leq t\leq k-1, t \in \nset{}{+}} \frac{\gamma_{t}}{\delta_{t+1}}\norm{\hat{G}_{t}}_*(\omega). 
\end{align*}
By definition, $\pi_{\omega}(k) < k$, $\forall \omega$ and $k$. 
With the introduction of $\pi_{\omega}$, we can tackle the heterogeneity in $s^i(k)$ and obtain: 
\begin{align*}
\norm{\hat{G}_k}_*(\omega) & \leq \frac{\beta_1}{\Tilde{\mu}}\sum_{i \in \mathcal{N}}\big(\frac{\gamma_{\pi_{\omega}(k)}}{\delta_{\pi_{\omega}(k)+1}}\norm{\hat{G}_{\pi_{\omega}(k)}}_*(\omega)\big) + \beta_1 \bar{\varphi} \\
& = \beta_2(\pi_{\omega}(k))\norm{\hat{G}_{\pi_{\omega}(k)}}_*(\omega) + \beta_1 \bar{\varphi}, 
\end{align*}
where we let $\beta_2(t) \coloneqq\frac{\beta_1N}{\Tilde{\mu}}\cdot \frac{\gamma_{t}}{\delta_{t+1}}$. 
Observe that as $k \to \infty$, it follows that $k - \bar{d}_k \to \infty$, which further implies that $\beta_2(\pi_{\omega}(k)) \to 0$. 
Let $\bar{\beta}_2(k) \coloneqq \beta_2(k) \vee 1$, and we can recursively construct a constant $g_\star$ that could serve as a worst-case upper bound regardless of $\omega$ as 
\begin{align*}
g_\star = \beta_1\bar{\varphi} (1 + \sum_{\ubar{T}=1}^{K\star-1}\prod_{t=\ubar{T}}^{K_\star-1}\bar{\beta}_2(t)). 
\end{align*}
Thus, we can find a constant index $K_\star$ independent of $\omega$, such that $\beta_2(k) < \varepsilon$ for some $\varepsilon < 1$ and all $k \geq K_\star$. 
For another thing, for an arbitrary $k \in \nset{}{+}$, $\norm{\hat{G}_k}_*(\omega)$ can be recurrently upper bounded regarding the sequence
$(\norm{\hat{G}_k}_*, \norm{\hat{G}_{\pi_{\omega}(k)}}_*, \norm{\hat{G}_{(\pi_{\omega})^2(k)}}_*, \ldots, \norm{\hat{G}_{1}}_*)$, where $\omega$ is omitted for brevity. 
If $\pi_{\omega}(k) < K_{\star}$, there will be less than $k-1$ recurrent inequalities to link $\norm{\hat{G}_k}_*$ back to $\norm{\hat{G}_1}_* = 0$. 
As such, the constant $g_\star$ serves as a uniform upper bound for all the $\norm{\hat{G}_k}_*$ with $\pi_{\omega}(k) < K_{\star}$. 
In the case where $\pi_{\omega}(k) \geq K_{\star}$, we focus on the latter portion of the estimate sequence, i.e., $(\norm{\hat{G}_k}_*, \norm{\hat{G}_{\pi_{\omega}(k)}}_*, \ldots, \norm{\hat{G}_{K_\Delta}}_*, \norm{\hat{G}_{\pi_{\omega}(K_\Delta)}}_*)$ with $\pi_{\omega}(K_\Delta) < K_\star \leq K_\Delta$ and $\norm{\hat{G}_{\pi_{\omega}(K_\Delta)}}_*(\omega) \leq g_\star$. 
For this subsequence, we have $\norm{\hat{G}_t}_*(\omega) \leq \varepsilon \norm{\hat{G}_{\pi_{\omega}(t)}}_*(\omega) + \beta_1\bar{\varphi}$, which gives us a stable linear discrete-time system with $\varepsilon < 1$. 
Thus, there exists a constant $\bar{g}_\star$ such that $\sup_{k \in \nset{}{+}, \omega \in \Omega} \norm{\hat{G}_k}_*(\omega) \leq \bar{g}_\star$. 
\end{proof}

\subsection{Almost-Sure Convergence of the Proposed Algorithm}

To facilitate our later discussion, denote the event $E^i_k \coloneqq \{\mathcal{P}^i_{G} \neq \varnothing \text{\;at iteration\;}k\}$ and notice that $E^i_k \in \Tilde{\mathcal{F}}_k$. 
In addition, let $E_k \coloneqq \cap_{i \in \mathcal{N}}E^i_k$. 

\begin{appdxlemma}\label{le:insum-gamma}
Suppose that step size $\gamma_k = \gamma_0/(k + K_{\gamma})^{\alpha_\gamma}$ with $\alpha_\gamma \leq 1$, and Assumption~\ref{asp:delay} holds. Then, $\sum_{k \in \nset{}{}} \gamma_k \mathds{1}_{E_k}(\omega) = \infty$ for all $\omega \in \Omega$. 
\end{appdxlemma}
\begin{proof}
For arbitrary $\omega \in \Omega$, we have 
\begin{align*}
& \sum_{k\in \nset{}{+}} \gamma_k \mathds{1}_{E_k}(\omega) = \lim_{K \to \infty} \sum_{k=1}^{K} \gamma_k \mathds{1}_{E_k}(\omega) \overset{(a)}{\geq} \lim_{K \to \infty}\sum^K_{K \wedge \ceil{\bar{d}(K) + 2}N}\gamma_k \\
& = \lim_{K \to \infty}\sum^K_{\ceil{\bar{d}(K) + 2}N}\gamma_k \quad \cdots \cdots \quad (\star), 
\end{align*}
where $(a)$ is a result of Lemma~\ref{le:fdbk-prop}, i.e., from the perspective of each player, for the first $K$ iterations, there are at most $\bar{d}(K)+1$ iterations without action update. 
On account of the monotonically decreasing property of $\gamma_k$, the worst-case scenario is that these $\bar{d}(K)+1$ iterations sit at the very beginning of the process and are different across this group of players, contributing to a factor of $N$. 
When $\alpha_\gamma < 1$, $(\star) \geq \lim_{K \to \infty} \int_{\ceil{\bar{d}(K) + 2}N}^{K}\gamma_0(s + K_{\gamma})^{-\alpha_\gamma}ds = \lim_{K \to \infty}\frac{\gamma_0}{1 - \alpha_{\gamma}}[(s + K_{\gamma})^{1 - \alpha_{\gamma}}]^K_{\ceil{\bar{d}(K) + 2}N} = \infty$ since $1 - \alpha_{\gamma} > 0$ and $\Bar{d}(K) \propto K^{\alpha_d}$ with $\alpha_d < 1$. 
Likewise, when $\alpha_\gamma = 1$, $(\star) \geq \lim_{K \to \infty} \int_{\ceil{\bar{d}(K) + 2}N}^{K}\gamma_0(s + K_{\gamma})^{-1}ds = \lim_{K \to \infty}\gamma_0[\log(s + K_{\gamma})]^K_{\ceil{\bar{d}(K) + 2}N} = \infty$ given $\alpha_d < 1$.   
\end{proof}

In light of this lemma, we can now proceed to prove our main result, which establishes the a.s. convergence of the proposed algorithm. 

\begin{proof}(Proof of Theorem~\ref{thm:as-convg})
By applying the standing inequality of mirror descent (\cite[Lemma~A.2]{huang2023zeroth}), for an arbitrary CP $x_* \in \mathcal{X}$, we have:
\begin{align}\label{eq:std-ineq}
D(x_*, X_{k+1}) \leq D(x_*, X_k) - \gamma_k \langle \hat{G}_k, X_k - x_*\rangle + \frac{\gamma_k^2}{2\Tilde{\mu}}\norm{\hat{G}_k}_*^2. 
\end{align}
By the fact that $\sum_{k \in \nset{}{+}}\gamma_k^2 < \infty$ from the assumption and $\sup_{k \in \nset{}{+}, \omega \in \Omega} \norm{\hat{G}_k}_* \leq \bar{g}_{\star}$ from Lemma~\ref{le:bounded-rpg}, we can claim that $\sum_{k\in \nset{}{+}} \gamma_k^2/2\Tilde{\mu}\norm{\hat{G}_k}_*^2 < \infty$, i.e., this part will play a comparatively negligible role in the convergence analysis. 
If $E^i_k$ happens, $\langle\hat{G}^i_k, X^i_k - x^i_*\rangle$ can be decomposed as
\begin{align*}
& \langle\hat{G}^i_k, X^i_k - x^i_*\rangle = \langle \nabla_{x^i}J^i(X_{k}), X^i_k - x^i_*\rangle + \sum_{t=s^i(k) + 1}^{k}\langle \nabla_{x^i}J^i(X_{t-1}) \\
& \quad - \nabla_{x^i}J^i(X_{t}) , X^i_k - x^i_*\rangle + \langle B^i_{s^i(k)} + V^i_{s^i(k)}, X^i_k - x_*\rangle
\end{align*}
The stacked inner product in \eqref{eq:std-ineq} can then be examined individually and be decomposed as follows:
\begin{align*}
& -\langle \hat{G}_k, X_k - x_*\rangle = -\langle F(X_k), X_k - x_*\rangle\mathds{1}_{E_k} - \sum_{i \in \mathcal{N}} \mathds{1}_{(E_k)^c\cap E^i_k}\cdot \\
& \quad  \langle \nabla_{x^i} J^i(X_k), X^i_k - x^i_*\rangle - \sum_{i \in \mathcal{N}} \Big(\sum_{t=s^i(k) + 1}^{k}\langle \nabla_{x^i}J^i(X_{t-1}) - \nabla_{x^i}J^i(X_{t}), \\
& \qquad X^i_k - x^i_*\rangle + \langle B^i_{s^i(k)} + V^i_{s^i(k)}, X^i_k - x_*\rangle\Big)\mathds{1}_{E^i_k}\\
& \overset{(a)}{\leq} -\langle F(X_k), X_k - x_*\rangle\mathds{1}_{E_k} - \sum_{i \in \mathcal{N}} \mathds{1}_{(E_k)^c\cap E^i_k}\langle \nabla_{x^i} J^i(X_k), X^i_k - x^i_*\rangle\\
& + \sum_{i \in \mathcal{N}}\Big(\sum_{t=s^i(k) + 1}^{k}L^iD_{\mathcal{X}^i}\norm{X_{t-1} - X_{t}} + \langle B^i_{s^i(k)} + V^i_{s^i(k)}, X^i_k - x^i_*\rangle\Big)\mathds{1}_{E^i_k} \\
& \overset{(b)}{\leq} -\langle F(X_k), X_k - x_*\rangle\mathds{1}_{E_k} - \sum_{i \in \mathcal{N}} \mathds{1}_{(E_k)^c\cap E^i_k} \langle \nabla_{x^i} J^i(X_k), X^i_k - x^i_*\rangle\\
& + \sum_{i \in \mathcal{N}}\Big(\sum_{t=s^i(k) + 1}^{k}L^iD_{\mathcal{X}^i}\norm{X_{t-1} - X_{t}} + \alpha_BD_{\mathcal{X}^i}\delta_{s^i(k)} \\
& \qquad + \langle V^i_{s^i(k)}, X^i_k - x^i_*\rangle\mathds{1}_{E^i_k} \Big),
\end{align*}
where the relation (a) follows from the $L^i$-Lipschitz continuity of $\nabla_x J^i$; the relation (b) can be derived by letting $D_{\mathcal{X}^i} \coloneqq \max_{x,y\in \mathcal{X}^i}\norm{x - y}$ and applying Lemma~\ref{le:bias}.
For each $\norm{X_{t-1} - X_t}$, it entails Lemma~\ref{le:md-lips} that
$\norm{X_{t-1} - X_{t}} = \norm{\nabla \psi^*(\nabla \psi(X_{t-1})) - \nabla \psi^*(\nabla \psi(X_{t-1}) - \gamma_{t-1}\hat{G}_{t-1})} \leq \gamma_{t-1}\norm{\hat{G}_{t-1}}_*/\Tilde{\mu} \leq \gamma_{t-1}\bar{g}_\star/\Tilde{\mu}$. 
Lemma~\ref{le:fdbk-prop} indicates that $s^i(k) \geq k - \bar{d}(k)$ for all $i$. 
Furthermore, since $X^i_k, \mathds{1}_{E^i_k} \in \Tilde{\mathcal{F}}_k$ while $V^i_{s^i(k)}$ is independent of $\Tilde{\mathcal{F}}_k$, $\expt{}{\langle V^i_{s^i(k)}, X^i_k - x^i_*\rangle\mathds{1}_{E^i_k} \mid \Tilde{\mathcal{F}}_k} = \langle \expt{}{V^i_{s^i(k)}\mid \Tilde{\mathcal{F}}_k}, X^i_k - x^i_*\rangle\mathds{1}_{E^i_k} = 0$.
If we further take the conditional expectation $\expt{}{\cdot \mid \Tilde{\mathcal{F}}_k}$ of both sides of the above inequality, it yields that
\begin{align*}
& \expt{}{-\langle \hat{G}_k, X_k - x_*\rangle \mid \Tilde{\mathcal{F}}_k} \leq -\langle F(X_k), X_k - x_*\rangle\mathds{1}_{E_k} \\
& + \sum_{i \in \mathcal{N}} \mathds{1}_{(E_k)^c\cap E^i_k} \bar{\nabla}_iD_{\mathcal{X}^i} + \beta_3\sum_{t=k-\bar{d}(k)+1}^{k}\gamma_{t-1} + \beta_4\delta_{k-\bar{d}(k)}, 
\end{align*}
where we let $\beta_3 \coloneqq \sum_{i \in \mathcal{N}}\frac{L^iD_{\mathcal{X}^i}\bar{g}_{\star}}{\Tilde{\mu}}$ and $\beta_4 \coloneqq \sum_{i \in \mathcal{N}} \alpha_BD_{\mathcal{X}^i}$. 
We then take $\expt{}{\cdot \mid \Tilde{\mathcal{F}}_k}$ of both sides of \eqref{eq:std-ineq} and apply the bound derived above to procure:
\begin{align*}
\begin{split}
& \expt{}{D(x_*, X_{k+1})\mid \Tilde{\mathcal{F}}_k} \leq D(x_*, X_{k})-\gamma_k\langle F(X_k), X_k - x_*\rangle\mathds{1}_{E_k} + \gamma_k \\
& \sum_{i \in \mathcal{N}}\mathds{1}_{(E_k)^c\cap E^i_k} \bar{\nabla}_iD_{\mathcal{X}^i} + \gamma_k\beta_3\sum_{t=k-\bar{d}(k)+1}^{k}\gamma_{t-1} + \beta_4\gamma_k\delta_{k-\bar{d}(k)} + \frac{\bar{g}_\star^2}{2\Tilde{\mu}}\gamma_k^2. 
\end{split}
\stepcounter{equation}\tag{\theequation}\label{eq:md-recr-ineq}
\end{align*}
Note that under Assumption~\ref{asp:delay-plus}$(\romannum{1})$, $(E^i_k)^c$ happens for only finitely many $k$, i.e., $\mathds{1}_{(E^i_k)^c} = 1$ for at most $\ceil{\bar{d} + 1}$ iterations, while under Assumption~\ref{asp:delay-plus}$(\romannum{2})$, $\mathds{1}_{(E_k)^c\cap E^i_k} \equiv 0$.
In either case, $\sum_{k \in \nset{}{+}}\gamma_k\sum_{i \in \mathcal{N}} \mathds{1}_{(E_k)^c\cap E^i_k} \bar{\nabla}_iD_{\mathcal{X}^i} < \infty$. 
For the next error term associated with delays, we have $\gamma_k\sum_{t=k-\bar{d}(k)+1}^{k}\gamma_{t-1} \leq \gamma_k\cdot \bar{d}(k) \gamma_{k-\bar{d}(k)} \propto O(k^{\alpha_d - 2\alpha_\gamma})$ and by choosing the parameters such that $2\alpha_\gamma - \alpha_d > 1$, we ensure that this term is summable. 
For the last two terms, trivially, $\gamma_k\delta_{k - \bar{d}(k)} \propto O(k^{-\alpha_{\gamma} - \alpha_{\delta}})$ and $\gamma_k^2 \propto O(k^{-2\alpha_{\gamma}})$, the summability of which follows from the assumptions $\alpha_{\gamma} + \alpha_{\delta} > 1$ and $\alpha_{\gamma} > 0.5$ imposed. 
By the Robbins-Siegmund theorem \cite[Thm.~1]{robbins1971convergence}, we arrive at the claims: 
$(\romannum{1})$ $D(x_*, X_{k})$ converges a.s. to a random variable that is finite a.s.;  
$(\romannum{2})$ $\sum_{k \in \nset{}{+}}\gamma_k\langle F(X_k), X_k - x_*\rangle\mathds{1}_{E_k} < \infty$ a.s. 
For each $\omega \in \Tilde{\Omega}$ with $\Tilde{\Omega}$ defined as a sample subset with probability one, by utilizing Lemma~\ref{le:insum-gamma}, i.e., $\sum_{k \in \nset{}{+}}\gamma_k\mathds{1}_{E_k}(\omega) = \infty$, we deduce that $\liminf_{k \to \infty} \langle F(X_k), X_k - x_*\rangle(\omega) = 0$. 
Thus, along a subsequence $(k_m)_{m \in \nset{}{+}}$, we have $\lim_{k \to \infty} \langle F(X_{k_m}), X_{k_m} - x_*\rangle(\omega) = 0$. 
By applying the boundedness of $\mathcal{X}$ and $X_k \in \mathcal{X}$, we can find a further subsequence $(\ell_m)_{m \in \nset{}{+}} \subseteq (k_m)_{m \in \nset{}{+}}$ such that $X_{\ell_m}(\omega) \to X_\star(\omega)$. 
Since $F$ is a continuous operator, $\lim_{m \to \infty} \langle F(X_{\ell_m}(\omega)), X_{\ell_m}(\omega) - x_*\rangle = \langle F(X_\star(\omega)), X_\star(\omega)(\omega) - x_*\rangle = 0$. 
Since $x_*$ is a CP, $\langle F(x_*), X_\star(\omega)(\omega) - x_*\rangle \geq 0$, which, together with the pseudo-monotone plus property of $F$, implies that $F(x_*) = F(X_\star(\omega))$. 
It then readily follows that for any $x \in \mathcal{X}$, $\langle F(X_\star(\omega)), x - X_\star(\omega)\rangle = \langle F(X_\star(\omega)), x - x_* + x_* - X_\star(\omega)\rangle \geq 0$, which implies that $X_\star(\omega)$ is also a CP. 
Then we can replace $x_*$ in \eqref{eq:md-recr-ineq} with $X_\star(\omega)$ and it follows that $D(X_\star(\omega), X_k)$ converges a.s.  
In addition, along the subsequence $(\ell_m)_{m \in \nset{}{+}}$, $D(X_\star(\omega), X_{\ell_m}(\omega)) \to 0$ by Assumption~\ref{asp:recip}. 
Therefore, $D(X_\star(\omega), X_k(\omega)) \to 0$ and we come to the conclusion that $X_k$ converges to a CP a.s. 
Thus, the convergence result also holds for the actual sequence of play $(\hat{X}_k)_{k \in \nset{}{+}}$ since $\delta_k \overset{k\to\infty}{\to} 0$.  
\end{proof}
