\section{FasCo}\label{sec:LC}

%In this section, we will introduce the light tree-based cardinality-estimation-free model for cost estimation, called LightCost. First, we discuss how we select and utilize the features in Section~\ref{sec:feature}. Next, we present the model design of LightCost in Section~\ref{sec:modeldesign}. Finally, we will describe how we leverage fewer training samples to train LightCost in Section~\ref{sec:training}.
%
%In this section, we introduce {\LC}, a simple yet effective cost estimator which uses much fewer training execution plans than the state of the arts. We will discuss how we select and utilize the features in Section~\ref{sec:feature}, present the main model design in Section~\ref{sec:modeldesign}, and describe how we leverage fewer execution plans for training in Section~\ref{sec:training}.

% \subsection{Main Idea}
%  \label{sec:mainidea}

% As shown in Figure~\ref{fig:modeldesign}, the keys LightCost can keep both high efficiency and accuracy are efficient model design, explicit feature input and reasonable training. In addition, cardinality correction can improved accuracy significantly.
%
{\LC} is a model designed to accurately and efficiently estimate the runtime (cost) of an execution plan generated by a database system. The model consists of four main stages, which are outlined in Figure~\ref{fig:modeldesign}: First, {\LC} takes an execution plan corresponding to a SQL statement as input. The execution plan is a tree structure with nodes representing specific operations (e.g., {\it Sequential Scan}) required to execute the SQL statement. For each node, {\LC} then generates an embedding that conveys abundant information. The embedding of a parent node is generated by feeding the embeddings from its two children into a simple neural network. This allows {\LC} to be lightweight and efficient, using 98\% fewer training plans, incurring $40$ times lower training cost, and achieving higher accuracy than previous methods. While {\LC} incorporates a tree-structured network similar to QPPNet~\cite{marcus2019plan}, we highlight the main differences and novelties of {\LC} as follows. (1) \textbf{Efficient model design.} To accelerate training and inference, we propose a lightweight network architecture. First, we simplify the execution plan tree structure by merging each unary node and its only child node. As such, a unary node is no longer treated as an independent node and hence reduces the training and inference costs which are proportional to the number of nodes. Then, we share the information between the nodes effectively so that the model can be compressed to three sets of MLPs with only a few layers (as shown in Figure~\ref{fig:modeldesign}(d)). This allows us to avoid learning a separate model for each operator as in~\cite{marcus2019plan}. %This architecture can maintain high accuracy because we use more explicit features and more reasonable training methods, which will also be discussed in this section. 
We find that such a simple architecture becomes surprisingly powerful when integrated with the features we carefully selected. Due to the simplicity of the model, this architecture significantly speeds up the training and inference. In addition, we incorporate a weighted Q-error into the loss function, making our model more accurate (see Section~\ref{sec:modeldesign}).
%
% Third, to further improve convergence speed and accuracy, we design a weighted Q-error for loss function. This loss function can make the model prioritize learn more important and accurate information.
%
%
%
%
% \noindent \textbf{Explicit features as input.} To improve the efficiency and accuracy of the model, we input more explicit features from both the current node and leaves (as shown in Figure~\ref{fig:modeldesign} (c)). What needs to be highlighted is that we add the cardinalities and costs of the leaves into the feature. Inputting more explicit information can speed up the convergence and improve the accuracy.
%
%
% \vspace{1mm}
% \noindent \textbf{Cherry-picked Explicit features.} {\color{blue} We integrate more features as explicit input as shown in Figure~\ref{fig:modeldesign} (c). We highlight that we also add the cardinalities and costs of the child nodes into the feature, and we also carefully design the initialization of cardinalities. We find that these features are important to enhance the accuracy of the model. More details of feature extraction will be discussed in Section~\ref{sec:feature}.}
(2) \textbf{Cherry-picked explicit features.} { We integrate more features directly related to the execution plan as explicit inputs as shown in Figure~\ref{fig:modeldesign} (c). %We highlight that we also add the cardinalities and costs of the child nodes into the feature, and we also carefully design the initialization of cardinalities. 
Previous works rarely rely on these explicit features (e.g., \textit{Subquery} and \textit{Cardinality}) for model training and some of them rely on adding hidden features to secure accuracy. We find that by incorporating these explicit features, we can significantly reduce the size of hidden features. We note that these explicit features are important to enhance the accuracy of the model. More details of feature extraction will be discussed in Section~\ref{sec:feature}.}
%
%
% \noindent \textbf{Reasonable training} We adapt the pipeline for generating training data 
% and adopt tree-structure back propagation to optimize LightCost.  We will elaborate the process in Section~\ref{sec:training}.
%
% \noindent \textbf{Cardinality correction.} Besides above techniques, we also investigate cardinality given by histograms and proposed cardinality correction to further improve the accuracy.  The correction is based on sampling without sacrificing much efficiency.  We will elaborate the process in Section~\ref{sec:correction}.
(3) \textbf{Cardinality calibration.} We also investigate the cardinality modeling given by histograms and propose {\it cardinality calibration} to further improve the accuracy.  This technique is based on sampling without sacrificing much efficiency.


\vspace{-2mm}
\subsection{Model Design}   
\label{sec:modeldesign}
%{\color{red} what exactly is the model here? A model should usually have an objective function and parameters to train. All these are unclear.}
%In this section, we will describe the flow of data through {\LC} from the generated plans to the cost estimates, including preprocessing, model architecture, and loss function.


% \noindent \textbf{Preprocessing.} To make full use of the information provided by the database system, the base structure of {\LC} follows the tree of execution plans. However, we modify the process about materialization nodes to improve efficiency. Among the nodes of all types of sub-plans, we observe that the materialization nodes contain less information than the other two types of nodes (scan nodes and merge nodes). Moreover, each materialized node has one and only one leaf (merge node has two, scan node has no leaves), and almost all its information comes from its leaf. For example, In the 3-rd node of the example tree in Figure~\ref{fig:modeldesign}, the Aggregate node computes the count of a set of input values from the Hash Join node. So we merge the materialization nodes to their leaves as shown in Figure~\ref{fig:preprocess}. The method is to take the total cost of the two nodes as the cost of the merged node, and other information comes from the leaf before the merge. The materialization nodes no longer act as independent node, reducing model inference latency and training cost.

% \vspace{1mm}
\noindent \textbf{Preprocessing.} The base structure of {\LC} is geared to the tree of the input execution plan. To improve the efficiency, we simplify the tree to have fewer nodes, making the subsequent operators simpler and more efficient. Particularly, among the nodes of all types of sub-plans, the unary node with a single child node contains less information than the other types of nodes. This is because most of its information comes from its immediate child node. For example, In Node 3 of the example tree in Figure~\ref{fig:modeldesign} (a), the {\it Aggregate} node computes the count of a set of input 
tuples from the {\it Hash Join} node. Hence, we merge each unary node to its child node as shown in Figure~\ref{fig:modeldesign} (a). By merging we mean to take the total cost of the two nodes as the cost of the merged node, and the merged node also contains other information that comes from the original child node. After merging, the unary node no longer acts as an independent node, and the whole structure is simplified, thereby reducing model inference latency and training cost. %{\color{blue} This is because the forward propagation (will be discussed in the next part) of {\LC} is calculated once for each tree node. A materialization node and its child node need to be calculated twice before merging while the merged node only needs to be calculated once. }

% Besides merging nodes, we also encode the text-type features by ordinal encoding in preprocessing. For example, we encode the ``Outer'' to 0 and ``Inner'' to 1 in the feature ``parent realtionship''. We will explain the details shortly in Section~\ref{sec:feature}.

% The input of each node is the concatenated vector, containing embeddings of the selected features, as well as cardinality and cost from its leaf nodes. The information of the leaf node is transferred to its root node implicitly and explicitly through hidden states and direct input, respectively. 
% We simulate the tree-structure model as a stack, which can be formalized as Algorithm~\ref{alg:stack}.
% \begin{algorithm}[t]
% \caption{Simulating LightCost with stack}
% \label{alg:stack}
% \KwIn{A post-order list of nodes in an execution plan $\{n_i\}$;  The parameters of LightCost model $\theta$} 
% \KwOut{The set of cost $C$}
% \Begin{
% Initialize the stack of cost $C$\;
% Initialize an stack of plan nodes $P$\;
% \For{$n_i \in \{n_i\}$}{
% \If{$length(P) >=2$ $and$ $level(P[-1])=level(P[-2])$}{
% $leaf_{l} = pop(P)$ \;
% $leaf_{r} = pop(P)$ \;
% Concat the selected features in $leaf_{l},leaf_{r}$ and $n_i$ as $input$\;
% $state,cost = inference(input,\theta)$\;
% Add $state$ and $cost$ into $n_i$ as features\;
% $P=push(n_i,P)$\;
% $C=push(cost,C)$\;
% }
% \Else{
% Initialize the feature of null node $f$\;
% Concat the selected features in $n_i$ and $f$ as $input$\;
% $state,cost = inference(input,\theta)$\;
% Add $state$ and $cost$ into $n_i$ as features\;
% $P=push(n_i,P)$\;
% $C=push(cost,C)$\;
% }
% }
% }
% \end{algorithm}
% The step of concatenating the features from leaves and $i$th current node to get the input can be formalized as:
% \begin{displaymath}
% \mathcal{F}(leaf_i^l) = Concat(E_{op}(Op_i^l),Cost_i^l,Card_i^l,E_{jk}(JK_i^l),State_i^l)
% \end{displaymath}
% \begin{displaymath}
% \mathcal{F}(leaf_i^r) = Concat(E_{op}(Op_i^r),Cost_i^r,Card_i^r,E_{jk}(JK_i^r),State_i^r)
% \end{displaymath}
% \begin{displaymath}
% \mathcal{F}(cur_i) = Concat(E_{op}(Op_{i}),Card_{i},Filter_{i},Parent_{i})
% \end{displaymath}
% \begin{displaymath}
% \mathcal{I}_i = Concat(\mathcal{F}(leaf_i^l),\mathcal{F}(cur_i),\mathcal{F}(leaf_i^r)),
% \end{displaymath}
% where $Op$ denotes the physical operation, $Card$ denotes the cardinality, $State$ denotes the hidden state, $JK$ denotes the join keys, and $Parent$ denotes parent relationship which are all described in Section~\ref{sec:feature}. 
\begin{figure*}[ht]
\vspace{-1.5mm}
  \centering
  \includegraphics[width=0.86\linewidth]{figure/model.pdf}
  \vspace{-3mm}
  \caption{Model design of {\LC}.}
\label{fig:modeldesign}
\vspace{-5mm}
\end{figure*}

% After feature integration, the input is fed into the backbone MLP layer and activation layer sequentially. The activation layer here we apply is $Tanh$, because it is zero-centered and all neurons can be fully utilized than $ReLU$ without the dead ReLU problem. Then the state and cost will be calculated respectively by state MLP and cost MLP. The activation layer we use when calculating $Cost$ is $Sigmoid$, because it needs to be bounded and strictly greater than 0.
% Figure~\ref{fig:modeldesign} shows an example of how we process an execution plan. It can be seen from the right side that we only adopt MLP and activation layer in the model, which simplifies the calculation extremely and keep the model size under 20 kilobytes. 
% \begin{figure}[h!]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figure/preprocess.pdf}
%   \caption{Preprocessing.}
% \label{fig:preprocess}
% \end{figure}

% \vspace{1mm}
\noindent \textbf{Model architecture.} {\LC} adopts a tree-based model architecture because (1) it resembles the structure of the execution plan and hence gives a nice one-one correspondence between the network node and plan node; and (2) it was proved to be effective~\cite{marcus2019plan,sun13end,hilprecht2022zero}. Further, to make the model lightweight, we use MLP, which is the simplest neural network. Our experiments demonstrate that this simplicity does not lead to worse effectiveness. 
%\textcolor{black}{We follow the tree-based model because it resembles the structure of execution-plan trees well. From the view of each node, we try to use only the simplest neural network, MLP, we will elaborate as follows. (R1.D1)}


The training follows a post-order traversal of the execution tree, with each plan node corresponding to a model node that takes input from its two child nodes. For example, the three-node plan in Figure~\ref{fig:modeldesign}(a) (right) is transformed into a model with three corresponding components in Figure~\ref{fig:modeldesign}(d) (left).
The $j$-th and $k$-th nodes are the left and right child nodes of the current $i$-th node, and in this concrete example, $j$,$k$, and $i$ are 1,2 and 3 respectively. We collect three sets of information from them. The {\bf first one} is the feature $x_i$, which denotes the full information of the current node, and some information (e.g., cardinality) of its two child nodes. $x_i$ can be obtained in the execution plan tree and we defer the details to Section~\ref{sec:feature}. The {\bf second one} is the hidden states $s_j$ and $s_k$, which denote the implicit information derived from its left and right child nodes respectively. These hidden states help correct the cost and cardinality estimates and provide more information of context beyond just using the explicit features $x_i$. % They can detect the abnormal estimations due to purely using feature $x_i$ as input, and calibrate the estimation automatically during learning. For example, sometimes the cardinality we input to the current node is extremely abnormal. The hidden state will record information such as joins or operations of the previous nodes, and judge whether the cardinality is too large or too small, thereby helping us to correct the results.
The {\bf third one} is the estimated costs $c_j$ and $c_k$ that are respectively from its left and right child node. As the ultimate goal is to get an accurate total estimated cost for the whole execution plan, the accumulation of the cost of each node is also crucial in establishing the final estimate.

% The {estimated cost} is the cost for executing the sub-plan rooted at the node. As the ultimate goal is to get an accurate estimated node cost for the root of the whole execution plan, and the estimated cost is derived from the children node, the estimated  cost for each node is also crucial in establishing the final estimation. 
% For normalization, we set an upper-bound on the maximum allowable execution time, denoted as $c_{max}$. $c_{max}$ can be set based on the maximum time in the generated training execution plans. Then, the normalization can be formalized as $norm(c) = \frac{\log c}{\log c_{max}}$.
% \begin{displaymath}
%   norm(c) = \frac{\log c}{\log c_{max}}
% \end{displaymath}
% The advantage of this logarithm normalization over uniform normalization is that it can get a more even distribution, rectifying the excessively long execution time incurred by extreme cases.

The inference based on the model is performed in a propagation following the execution plan tree in a post-order manner. %All the above information will be descried in detail in Section~\ref{sec:feature}. Figure~\ref{fig:back} shows the process of forward propagation related to the $i$-th node. If omitting the embedding layers and the biases, we only need to compute three multiplications of threthe e sets of weight matrices and their input. To simplify the formula, we use $W$,$U$,$V$ to denote the weights of backbone, state and cost MLP in Figure~\ref{fig:modeldesign}(d) respectively. Note that they do not necessarily represent only one linear layer, but can be a combination of multiple linear layers and activation layers. Then the forward propagation of the non-leaf $i$-th node can be formalized as:
In Figure~\ref{fig:modeldesign}(d), the feature $x_3$ (for Node 3), together with ($s_1$, $c_1$) from Node 1 and ($s_2$, $c_2$) from Node 2, is input into an MLP (named Backbone MLP) to produce an output $o_3$, which is then fed into two MLPs (named State MLP and Cost MLP) to generate the state $s_3$ and cost $c_3$. These values are propagated upward along the tree similarly and finally, we get the estimated cost at the root node. 
% \vspace{-1mm}

The internal structure of these MLPs is designed to be concise with a stack of linear layers and $Tanh$ activation function. Meanwhile, the network size can be compacted to a few layers with only thousands of parameters, leading to extremely low latency. 

% More specifically, let us respectively denote $l_{bone}$,$l_{state}$,$l_{cost}$ the number of layers in the Backbone, State, and Cost MLPs, and $Tanh$, $Sigmoid$ are common activation layers~\cite{nwankpa2018activation}. The variables for Node $i$ can be calculated as follows:
% \begin{align}
% o_i&=MLP_{bone}([s_k,c_k,x_i,c_j,s_j];l_{bone},Tanh,Tanh)\nonumber\\
% s_i&=MLP_{state}(o_i;l_{state},Tanh,Tanh)\nonumber\\
% c_i&= MLP_{cost}(o_i;l_{cost},Tanh,Sigmoid)\nonumber
% \end{align}
% \begin{algorithm}[t]
% \LinesNumbered  
% \SetAlgoVlined
% % 
% \caption{The function of $MLP(X_0; l,\sigma_m,\sigma_f)$ \label{alg:mlp}}
% \KwIn{Input $X_0$; Number of layers $l$; Intermediate activation layer $\sigma_m$; The final activation layer $\sigma_f$; A weight matrix $W$; A bias matrix $b$;}
% \KwOut{Output $X_l$}
% \For{i = $1$ to $l-1$ }{
%         $X_i = \sigma_m(W_i \times X_{i-1} + b_i)$ 
%     }
%           $X_l = \sigma_f(W_l \times X_{l-1} + b_l)$ 
% return $X_l$\;
% \end{algorithm}
% %For the leaf nodes, the input $s$, $c$ and some part of $x$ will be initialized, which will be discussed in Section~\ref{sec:feature}.
% The function of $MLP()$ is defined as Algorithm~\ref{alg:mlp}, where the choice of the number of layers will be discussed in the experimental section. The input values for leaf nodes will be discussed in Section~\ref{sec:feature}.

% \noindent \textbf{Estimated Node Cost.} is defined as the estimated cost for executing the sub-plan rooted at the node. Our ultimate goal is to get an accurate estimated node cost for the root of the whole execution plan. As in our model, the estimated node cost is derived from the children node. Hence, this feature is also crucial as it is passed between the parent node and children nodes. %We input the costs of the child nodes into each current node. Because the cost estimate of the current node is based on the cost of its child nodes.

% The database system will give two types of time at each node: the startup time and the total time, both of which start timing when the entire plan starts executing. The startup time may overlap with the execution time of the previous nodes, and there is some randomness, which will confuse the model and increase the training cost. Instead, the total times increase strictly in post-order, and the model can directly learn the final target. So we only select the total time as one of the features. 

%For normalization, we assume that execution time is bounded in practical use, whose bound is $(0,Max]$ ($Max$ is the maximum execution time in all generated training samples), then the normalization can be formalized as:
% We set an upper-bound on the maximum allowable execution time, denoted as $c_{max}$. $c_{max}$ can be set based on the maximum time in the generated training execution plans. Then, the normalization can be formalized as:
% \begin{displaymath}
%   norm(c) = \frac{\log c}{\log c_{max}}
% \end{displaymath}
% The advantage of this logarithm normalization over uniform normalization is that it can get a more evenly distribution, rectifying the excessively long execution time incurred by some extreme cases.% At each non-leaf node we input the execution time of its leaf nodes, while at leaf nodes we initialize the inputs as 0.  
% and fluctuates greatly under the influence of the database system itself. 
% This is because (1) in general, a leaf node usually starts from time 0 because of its independence; (2) occasionally if it starts during the execution of other nodes, then this time should also be counted as part of the execution time of this node, because it is waiting for other resources, which can also be learned by the model.


\noindent \textbf{Loss Function.} Based on the aforementioned model and the corresponding inference, our goal is to minimize the estimate error at each node. The estimated cost $c_i$ for Node $i$ can be compared with the ground-truth cost $l_i$, giving us a loss as: $\text{Q-error}(c_{i},l_{i}) = max(\frac{c_{i}}{l_{i}},\frac{l_{i}}{c_{i}})$.
% \begin{align}
% q_i=\text{Q-error}(c_{i},l_{i}) = max(\frac{c_{i}}{l_{i}},\frac{l_{i}}{c_{i}})\label{eq:qerror}
% \end{align}
% where $c_{i}$ and $l_{i}$ respectively denote the estimated cost and ground truth. 
% The reason why Q-error can be used directly as a loss function is that when the distributions of $c_{i}$ and $l_{i}$ are roughly the same, not only are the magnitude similar in the two cases, but the magnitudes of the gradient are also similar. Let the input of the last MLP be $x$, then the estimated cost is:
% \begin{displaymath}
% c_{i}= \sigma(V \times o_i + b),
% \end{displaymath}
% in which $\sigma$ denotes the $Sigmoid$ function, $W$ and $b$ denote the weight and bias of the last MLP. Then the gradient of $q_i$ can be formalized as below, 
% The gradient is well defined,
% when $c_{i}>l_{i}$:
% \begin{displaymath}
% \frac{ \partial q_i}{\partial V} = \frac{1}{l_{i}}\sigma'(V \times o_i + b)o_i;
% \end{displaymath}
% when $c_{i}<l_{i}$:
% \begin{displaymath}
% \frac{ \partial q_i}{\partial V} = -\frac{l_{i}}{c_{i}^2}\sigma'(V \times o_i + b)o_i
% \end{displaymath}
% When the distributions of $l_{i}$ and $c_{i}$ are the same, $\frac{1}{l_{i}}$ and $\frac{l_{i}}{c_{i}^2}$ are equal-order, leading to a stable gradient.
As each node may have different impacts on the final estimation, we add weights to aggregate the final loss as:
\vspace{-2mm}
\begin{equation}
\label{equ:3}
L=\frac{1}{n}\sum_i \lambda_i \text{Q-error}(c_{i},l_{i}),
\vspace{-1mm}
\end{equation}
% \vspace{-1mm}
where $n$ is the number of nodes in an execution plan and $\lambda_i$ is the weight of $i$-th node. %As we will discuss in the experiments, it is sufficient to set only two different values of $\lambda's$ which respectively correspond to the leaf nodes and non-leaf nodes.
%
%
% \begin{figure}[h]
%   \centering
%     \begin{subfigure}{\linewidth}
%         \includegraphics[height=3.6cm]{figure/plan-actual.pdf}
%         \caption{Original plan cardinalities vs. actual cardinalities.}
%         \label{fig:plan-actual}
%     \end{subfigure}
%     \begin{subfigure}{\linewidth}
%         \includegraphics[height=3.6cm]{figure/correct-actual.pdf}
%         \caption{Corrected plan cardinalities vs. actual cardinalities.}
%         \label{fig:correct-actual}
%     \end{subfigure}
%   \caption{Plan cardinalities vs. actual cardinalities.}
% \label{fig:planrows}
% \end{figure}
%
 To determine how to set $\lambda$, we investigate the actual costs for different types of nodes. From a statistical perspective, we observe that the execution time of nodes using an index (index nodes, e.g., {\it Index Scan} and {\it Bitmap Scan}) are usually short and subject to fluctuation, which increases the difficulties in model learning. 
 On the contrary, the nodes without using an index (non-index nodes, e.g., {\it Seq Scan} and { \it Nested Loop}) would take longer, bringing more reliable cost information. Moreover, the index nodes' runtime is only a small fraction of the total execution time. Since our final target is to estimate the total cost, the estimation accuracy largely depends on the estimates at the non-index nodes. Hence, we assign larger $\lambda$ values for nodes without using indexes. The setting of $\lambda$'s will be further discussed in the experimental section.

\vspace{-3mm}
\subsection{Feature Extraction}   
\label{sec:feature}
% Different from existing methods, we adopt more explicit inputs at each node instead of letting the model learn excessive implicit information (as shown in Figure~\ref{fig:modeldesign} (c)) to reduce the difficulty of model learning. The features we selected all have a direct impact on execution time, so as to avoid useless features generating meaningless costs. In this section, we will describe in detail the reason why we select and how to process these features.

% Existing methods use excessive implicit variables 
% This section introduces the details of feature $x_i$ for Node $i$. Existing tree-based methods~\cite{sun13end,marcus2019plan} usually regard excessive features as implicit variables to train the model. This unavoidably brings significant training or inference overheads. Different from existing methods, we adopt more explicit input features to reduce the model complexity and ease the learning process. Our intuition is that there are existing explicit plan-related features involving which significantly boost the model accuracy and training efficiency. We next describe the features we selected in detail.

This section introduces the details of feature $x_i$ for Node $i$. Unlike existing tree-based methods~\cite{sun13end,marcus2019plan} that employ excessive implicit features, resulting in significant training or inference overhead, we adopt more explicit input features to reduce complexity and improve the learning process. Our intuition is that involving more explicit plan-related features can significantly boost accuracy and efficiency. We will describe the selected features in detail.

% \vspace{1mm}
% \noindent \textit{\underline{Physical Operator}} is the most influential factor in the execution process which is also known as node type in an execution plan. The reason we include it as the first feature is that the execution time of different types of operators varies widely which has been discussed in Section~\ref{sec:problem}.
\noindent \textit{\underline{Physical Operator}} is an influential factor in the execution process since the runtime of different types of operators varies widely.
We use an embedding layer to process this feature. The advantage is that the embedding layer avoids the dependency on the order of features when encoding. Instead, the neural network will automatically learn the relationship and distinction between them. 
% Embedding layer is a special form of multi-layer perceptron (MLP). Supposing the $i$-th physical operation $x_i \in \{x_0,x_1,\ldots,x_{n-1}\}$, in which $n$ is the number of features that need to be embedded, we first convert it to one-hot encoding: $x_i^T = [0_{(0)},0_{(1)},\ldots,1_{(i)},\ldots,0_{(n-1)},]$. Then a MLP layer is adopted:
% \begin{displaymath}
% E(x_i)= Tanh(W_e \times x_i   + b_e),
% \end{displaymath}
% \noindent
% where $W_e$, $b_e$ are the weight and bias of the embedding layer. 

% After this process, taking PostgreSQL~\cite{postgresql1996postgresql} on the JOB and JOB-light~\cite{leis2015good} as an example, there are a total of 32 physical operations, and we use an embedding layer whose dimension equals 5 (according to the experimental estimation of $\log_2(n)$) for embedding.




% \vspace{1mm}
% \noindent \textit{\underline{Parent Relationship}} is a feature describing the relationship between the sibling sub-plans under the same parent node. The relationship can be either {\it outer} or {\it inner}. {\it Outer} means that the node queries the data from a raw table. {\it Inner} means that the node queries the data from the result of its sibling node, which means the node is {\it based on} its sibling node. We simply encode the {\it Outer} and {\it Inner} to ``0'' and ``1'' without embedding layers, and pad ``0'' to the nodes without this feature.
\noindent \textit{\underline{Subquery}} in the context refers specifically to the relationship between sibling sub-plan nodes. If the current node is the second child of its parent and it is a {\it Subquery} of its sibling, it means that the node queries the data from the result of its sibling node. Otherwise, the node will query the data from a raw table. We simply encode the two conditions to ``1'' and ``0'' without embedding layers.


% \vspace{1mm}
\noindent \textit{\underline{Cardinality}} associated in a node is the number of rows that are qualified as results after the execution of the operations of the sub-plan rooted at the node. We find that it is a vital factor in estimating the cost of an execution plan. %In {\LC}, we input the child nodes' cardinalities and the cardinality of the node itself as an explicit supplement to hidden information. For example, the cardinalities of the input of two child nodes are large, but the cardinality of the current node is small. If the current node cannot obtain this information, the output cost of the current node will be small.


% Compared with neural networks in other applications, the initialization of the cost estimation model should be paid more attention to due to its higher interpretability. For hidden states, we initialize them to 0, as other recurrent networks like RNN and LSTM often do.
% At the same time, the costs we input into leaf nodes are also assigned to 0, which is also in line with our model design. For the join key and physical operations of the leaves, we respectively set a class of "initialization" as additional inputs of the embedding layers for training.

%The initialization of cardinality of the leaf nodes is worth diving into, which is also one of the explicit information we offer to the model. 
% We find the initialization of cardinality of the leaf nodes is vital and worth investigating, which can be divided into three cases: (1) When the current node is not {\it based on} its sibling nodes, which means the current node directly queries from a table, we set its first cardinality input that corresponds to the left child as the count of rows of the table it is querying, and the second that corresponds to the right child as 1; (2) When the query of the current node is {\it based on} its sibling node and only has the filtering predicate, which means the query target of the current node comes from the result of its sibling node, we set its first cardinality as the row count of the result tuples of its sibling node, and the second as 1;  (3) When the query of the current node is {\it based on} its sibling node but has the join predicate, which means the current node will merge the tuples in the sibling node with a table by the join keys, we set its left-child cardinality as the cardinality of its sibling node, and the right one as the row count of the table.
The initialization of leaf node cardinalities is vital and can be divided into three cases: (1) When the current node is not a subquery of its sibling nodes, we set its first cardinality input that corresponds to the left child as the count of rows of the table it is querying, and the second that corresponds to the right child to 1; (2) When the current node is subquery of its sibling node and only has the filtering predicate, we set its first cardinality as the row count of the result tuples of its sibling node, and the second to 1;  (3) When the current node is a subquery of its sibling but has the join predicate, meaning it merges the results in its sibling node with a raw table, we set its left-child cardinality to the cardinality of its sibling node, and the right one to the row count of the table.
% The initialized cardinality of a leaf node usually represents a table stored on disk, while the cardinality of a merge node denotes the row count of tuples in the cache. Nonetheless, with other information initialized differently, the model can learn how to deal with these cardinalities.

%We also normalize the cardinality. Since it is difficult to estimate its maximum value, we simply divide it by a constant to avoid overwhelming other features in the small model. For the inaccuracy of statistical methods in the database system, we proposed a correction method, which will be introduced in Section~\ref{sec:correction}. 

% \noindent \textbf{Filter} is the number of condition in predicates, which has been discussed in Section~\ref{sec:problem}. Although we do not need to represent the entire predicate, it is useful for {\LC} to know how many filters are in the predicates, because the execution times of
% queries with different numbers of filters are different. For example, queries with ``BETWEEN'' (which can be regarded as ``>='' and ``<='') take longer time than queries with only one ``>=''. For example, in the example plan tree in Figure~\ref{fig:modeldesign}, the first, second, and third nodes have 2, 0, and 1 filters respectively.


% \vspace{1mm}
\noindent \textit{\underline{Filter}} is the number of conditions in predicates. To make it clear, in the example plan tree in Figure~\ref{fig:modeldesign}, the first, second, and third nodes have 2, 0, and 1 filters respectively. We find that it is useful for {\LC} to integrate this feature, because the query execution time is positively related to the number of filters used. %For example, queries with ``BETWEEN'' (which can be regarded as ``>='' and ``<='') take longer time than queries with only one ``>=''. 

% For example, there are two filters in ``company\_name.country\_code != '[pl]' AND company\_type.kind = 'production companies' '' while one in ``company\_type.kind = 'production companies' '', which can be directly got in the execution plan node. So the filter feature is the count of the conditions in the predicate.

\noindent \textit{\underline{Join Keys}} denote the attributes that are used when the tuples from two tables are joined. This feature conveys the information related to the execution cost because the join cost can be related to which keys to be joined between the tables. %Due to the difference in the row counts and storage of tables, there will still be a gap in the cost of different joins. We did not adopt embeddings of tables because individual join key provides more granular information. 
We also use an embedding layer to process this feature. For example, in the third node in Figure~\ref{fig:modeldesign} (a), the two join keys {\it t.id} and {\it ci.movie\_id} are converted to a vector. Note that the leaf nodes can also have join keys. Because some scan nodes are subqueries of their sibling nodes and have join predicates as mentioned in the feature \textit{Cardinality}. Similarly, some merge nodes may not have join keys because the join operation is done in one of their child nodes. In this case, we embed the join key as an extra category. 
% For example, there are 30 join keys in the IMDB dataset (including the initialization key for leaf nodes). 
% Also according to the estimation of $\log_2(n)$), we use an embedding layer of size 5.



% is the value we see most often which means ``take in the rows from this operation as input, process them and pass them on''; (2) ``inner'' is only ever seen on the second leaf of join operations, and is always seen there. This is the “inner” part of the loop. i.e., for each outer row, we look up its match according to the first leaf. 


% Since information cannot be passed between leaf nodes, we input this feature at the current node and pass it through the hidden state. It should be noted that there is a special case where some materialization nodes such as ``Hash'' will be marked "inner". However, it actually refers to getting data from its leaf node, not from the previous leaf node. This is one of the reasons why we want to merge nodes: we can use the parent relationship of its leaf node as the attribute of the merged node.



% \vspace{-5pt}
% \subsection{Training}   
% \label{sec:training}

% To train the model, {\LC} collects training samples which are execution plans with ground-truth costs at each node. Compared to previous studies, our model can be trained with much fewer training execution plans. 
% % In this paper, training samples refers to execution plans for training, which contain plan (devised by DBMS) information and actual information. After we collect training samples, we can process and fed them into the model as described in Section~\ref{sec:modeldesign}.
% To generate a training execution plan for real-world databases, the first step is to collect all the potential join keys to form the join graph, where the vertices are tables and each edge connects two joinable tables. Then we randomly select $N$ connected tables from the join graph, in which the distribution of $N$ is determined by the test set. A predicate is composed of columns, operators, and values. Thus, we select each column in the columns that are queried in the test set with uniform probability. Next, we randomly pick all operators that are valid to be applied in numerical or string column.
% % including ``=,<,>,>=,<=,!=,BETWEEN'' for columns with numerical values and ``=,!=,LIKE,NOT LIKE,IN,IS NULL,IS NOT NULL'' for columns with string values. 
% Then the values are also randomly retrieved from the corresponding columns with uniform probability. After obtaining the training queries, we generate execution plans by using the plan analysis tool in the database system. Then we serialize the nodes in the execution plan in post-order for training. 

% In real-world datasets, since randomly generated training queries may lead to skew distribution of results, some generated execution plans will not retrieve any matching tuple from the first node at all. This will result in zero cardinality and the costs in all subsequent nodes approximately being 0. This type of execution plans is repetitive and not useful in training the model, hence we generate extra plans and queries to exclude these zero-cardinality execution plans. 
% % For example, if we want to train 2,000 samples on JOB workloads, we will generate about 20\% more data. Then from the total 2400 data, we extract samples according to whether the cardinality of last node is 0 or not. Finally, we can get 1,800 samples whose last cardinalities are not 0 while the other 200 ones are allowed to have 0 cardinality in the nodes.  
% This practice can not only reduce the training cost, but also improve accuracy by alleviating the gap between the distribution of training and test execution plans due to the random generation.

% Note that if the test set is generated from templates (e.g. TPC-H~), then our training plans are also generated from the templates to ensure that all types of complex queries are included.

% \begin{algorithm}[t]
% \LinesNumbered  
% \SetAlgoVlined
% % 
% \caption{Generate a training plan for real-world database \label{alg:training}}
% \KwIn{Join graph $J$; numerical operator sets $Op^{num}$; string operator sets $O^{str}$; Samples of every tables $S$;}
% \KwOut{A training plan $\mathcal{P}$}
% Randomly select $N$ connected tables $T$ from $J$\;
% Initialize the predicate set $P: {(col_x,op_x,val_x)}$\; 
% Add all the join predicates into $P$\;
% \ForEach{$t_i \in T$}{
%         \ForEach{\text{column} $col_j \in t_i$}{
%             \If {$col_j$ is selected randomly}{
%                 Randomly select $val_j$ from $s_{i,j}$ in $S$\;
%                 \If {$col_j$ is a numerical columns}{
%                     Randomly select $op_j$ from $Op^{num}$\;
%                 }
%                 \Else{
%                     Randomly select $op_j$ from $Op^{str}$\;
%                 }
%                 Add $(col_j,op_j,val_j)$ into $P$
%             }
%             \Else{
%                 continue;
%             }
%         }
% }
% Gather $P$ to form a query $Q$\;
% Input $Q$ to the database system and get $\mathcal{P}$\;
% return $\mathcal{P}$\;
% \end{algorithm}

  
% \begin{figure}[h!]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figure/back.pdf}
%   \caption{Forward propagation and back propagation.{\color{red} where is this figure mentioned?}}
% \label{fig:back}
% \end{figure}
% \noindent \textbf{Initialization.} Compared with neural networks in other applications, the initialization of the cost estimation model should be paid more attention to due to its higher interpretability. For hidden states, we initialize them to 0, as other recurrent networks like RNN and LSTM often do.
% At the same time, the costs we input into leaf nodes are also assigned to 0, which is also in line with our model design. For the join key and physical operations of the leaves, we respectively set a class of "initialization" as additional inputs of the embedding layers for training.

% Cardinality is more special, and is divided into three cases: (1) When the query of the current node does not depend on other nodes, we set its first cardinality 
% as the count of rows of the table it is querying, and the second as 1; (2) When the query of the current node depends on its previous node (usually the first leaf under the same root) and only has the filtering function (the query target of the second node only comes from the result of the first node), we set its first cardinality as the row count of the result tuples of the first node, and the second as 1;  (3) When the query of the current node depends on its previous node but has the join function (will query and match the tuples of the first node from other sources by the join key), we set its first cardinality as the row count of the result tuples of the first node, and the second as the row count of the other source; The initialized cardinality of a leaf node usually represents a table stored on disk, while the cardinality of a merge node denotes the row count of tuples in the cache. Nonetheless, with other information initialized differently, the model can learn how to deal with these cardinalities.

% \vspace{1mm}
% \noindent \textbf{Back propagation.} Although we have added many explicit features during the inference and tree-based structure seems complicated, the computational graph of the model is deterministic and differentiable, which has a well defined gradient. In machine learning, back propagation is a widely used algorithm for training deep neural networks. Similar to forward propagation (see Section~\ref{fig:modeldesign}), we only need to  optimize three sets of weights $V$, $W$ and $U$ besides embedding layers and biases. The back propagation is shown in Figure~\ref{fig:back}. The gradient of  $V$ is related to the current loss $q_i$ and cost $c_i$:
% \begin{displaymath}
% \frac{\partial q_i}{\partial V}=\frac{\partial q_i}{\partial c_i}\frac{\partial c_i}{\partial V}
% \end{displaymath}
% And the gradient of $W$ can be formalized as:
% \begin{displaymath}
% \frac{\partial q_i}{\partial W}=\frac{\partial q_i}{\partial c_i}\frac{\partial c_i}{\partial o_i}\frac{\partial o_i}{\partial W}
% \end{displaymath}
% Since $o_i$ is derived from concatenating the current input $x_i$ and its leaf nodes' states $s_k$ and $s_j$, and costs $c_k$ and $c_j$, its partial derivative of $W$ can be further inferred as:
% \begin{displaymath}
% \begin{aligned}
% \frac{\partial o_i}{\partial W}=&\frac{\partial (W [c_k,s_k,x_i,s_j,c_j])}{\partial W}\\
% =&[c_{k}+W\frac{\partial c_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial W},s_{k}+W\frac{\partial s_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial W},x_i,\\
% &s_{j}+W\frac{\partial s_{j}}{\partial o_{j}}\frac{\partial o_{j}}{\partial W},c_{j}+W\frac{\partial c_{j}}{\partial o_{j}}\frac{\partial o_{j}}{\partial W}]
% \end{aligned}
% \end{displaymath}
% So $\frac{\partial o_{i}}{\partial W}$ is a recursive term. When $i=0$ or $i \bmod 2 = 1$,that is, when the node is a leaf node, it reaches the end point because the state $s_{-1}$ and cost $c_{-1}$ of the input are initialized.
% Similarly, the partial derivative of $U$ can be derived as:
% \begin{displaymath}
% \frac{\partial q_i}{\partial U}=\frac{\partial q_i}{\partial c_i}\frac{\partial c_i}{\partial o_i}\frac{\partial o_i}{\partial U},
% \end{displaymath}\
% where $\frac{\partial o_{i}}{\partial U}$ is also a recursive term:
% \begin{displaymath}
% \begin{aligned}
% \frac{\partial o_i}{\partial U}=&\frac{\partial (W [c_{k},s_{k},x_i,s_{j},c_{j}])}{\partial U}\\
% =&[W\frac{\partial c_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial W},W\frac{\partial s_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial W},0,
%  W\frac{\partial s_{j}}{\partial o_{j}}\frac{\partial o_{j}}{\partial U},W\frac{\partial c_{j}}{\partial o_{j}}\frac{\partial o_{j}}{\partial W}]
% \end{aligned}
% \end{displaymath}
% In addition, the gradient of the embedding layer is also easy to calculate, just continue to chain the derivative in the back direction. As the depth of the tree increases, the gradient brought by the last node becomes smaller. And since the last node will give the final estimation directly, we will also adjust its $\lambda$ in Equation~\ref{equ:3}.

% \vspace{1mm}
% \noindent \textbf{Batch Training.} Batch training is a technique that combines forward propagation and backward propagation of several training plans into one to accelerate the training or inference process~\cite{ruder2016overview}. Forward propagation refers to calculating the intermediate variables of the model from input to output, and backward propagation is the process of computing the gradient of the network from output to input, both of which are classical and effective methods~\cite{rumelhart1995backpropagation}. However, because we calculate the loss of all nodes in each iteration, it is already equivalent to batch training in the way of backward propagation. We found in our experiments that if we continued to use the batch technique, it was difficult for our model to converge in expected iterations. This is because in batch training the gradient is averaged over the entire batch, so the process of descent is slower. In the inference, due to great diversity in the number of nodes in real-world plan trees, lots of padding needs to be adopted, so the acceleration is also not obvious. However, for possible further research, we still provide a batch version of {\LC}. First, we sample $N$ plans with similar depth in the training data to avoid excessive padding which will reduce efficiency. Let $D$ denote the max depth of the training execution plans, then we rearrange the training execution plans as a $D*N$ tensor, in which each column is a sequence of nodes in the post order of the plan tree and short plans will be padded with 0. This way every time we infer the {\LC} model, we're going to input a $N$-dimensional tensor instead of 1 which theoretically reduces training time by $N$ times.