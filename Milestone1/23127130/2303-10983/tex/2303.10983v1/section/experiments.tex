
\section{EXPERIMENTS} 
\label{sec:experiments}
% \begin{table*}[th!]
%   \caption{Accuracy comparison.}
%   \vspace{-2mm}
%   \label{tab:acc}

% \begin{tabular}{lcccccccccccccccc}\toprule
% Workloads  & \multicolumn{4}{c}{JOB-M} & \multicolumn{4}{c}{JOB-light} & \multicolumn{4}{c}{JOB-light-ranges} & \multicolumn{4}{c}{TPC-H}   \\ \midrule
% Methods    & Mean & 50th & 95th & 99th & Mean  & 50th  & 95th & 99th & Mean    & 50th   & 95th   & 99th   & Mean & 50th & 95th & 99th \\ \midrule
% PostgreSQL &46.4&5.3&292.7&422.4&  9.5&2.8&31.1&125.7&         12.3&2.3&29.6&114.5&    3.0&2.7&6.7&7.9      \\
% MSCN       &   \multicolumn{1}{c}{--}   &    \multicolumn{1}{c}{--}  &   \multicolumn{1}{c}{--}   & \multicolumn{1}{c}{--} & 26.9&5.4&47.6&494.7&      31.8&11.4&467.6&494.7&   \multicolumn{1}{c}{--}     &  \multicolumn{1}{c}{--}    &   \multicolumn{1}{c}{--}     &  \multicolumn{1}{c}{--}        \\
% TLSTMCost  &6.9&4.0&17.6&64.7&   5.8&\textbf{1.9}&22.9&95.0&         6.2&2.5&30.9&88.0&   \multicolumn{1}{c}{--}   &    \multicolumn{1}{c}{--}    &    \multicolumn{1}{c}{--}  & \multicolumn{1}{c}{--}     \\
% QPPNet     &8.1&4.4&20.9&79.6& 6.1&3.9&24.1&136.8&          8.1&4.7&37.1&99.0&  \textbf{1.2}&1.1 &\textbf{1.7}&1.8      \\\midrule
% LightCost  &\textbf{4.3}&\textbf{2.4}&\textbf{13.6}&\textbf{41.1}&  \textbf{4.9}&2.0&\textbf{8.3}&\textbf{58.0}& \textbf{5.7}&\textbf{2.2}&\textbf{22.9}&\textbf{60.9}&   \textbf{1.2}& \textbf{1.0}&\textbf{1.7}&\textbf{1.7}     \\ \bottomrule
% \end{tabular}
% \end{table*}

\begin{table*}[th!]%
  \caption{Accuracy comparison.}
  \vspace{-4mm}
  \label{tab:acc}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccccccccccccc}\toprule
Workloads  & \multicolumn{4}{c}{JOB-M} & \multicolumn{4}{c}{JOB-light} & \multicolumn{4}{c}{JOB-light-ranges} & \multicolumn{4}{c}{TPC-H}& \multicolumn{4}{c}{Stack Overflow}   \\ \midrule
Methods    & Mean & 50th & 95th & 99th & Mean  & 50th  & 95th & 99th & Mean    & 50th   & 95th   & 99th   & Mean & 50th & 95th & 99th & Mean & 50th & 95th & 99th \\ \midrule
PostgreSQL &19.8&3.1&48.4&440.3&  9.5&2.8&31.1&125.7&         12.3&2.3&29.6&114.5&    3.0&2.7&6.7&7.9 &7.2&3.0&23.1&71.8     \\
MSCN       &   \multicolumn{1}{c}{--}   &    \multicolumn{1}{c}{--}  &   \multicolumn{1}{c}{--}   & \multicolumn{1}{c}{--} & 26.9&5.4&47.6&494.7&      31.8&11.4&467.6&494.7&   \multicolumn{1}{c}{--}     &  \multicolumn{1}{c}{--}    &   \multicolumn{1}{c}{--}     &  \multicolumn{1}{c}{--} & \multicolumn{1}{c}{--}& \multicolumn{1}{c}{--} & \multicolumn{1}{c}{--}  & \multicolumn{1}{c}{--}      \\
TLSTMCost  &6.9&4.0&17.6&64.7&   5.8&\textbf{1.9}&22.9&95.0&         6.2&2.5&30.9&88.0&   \multicolumn{1}{c}{--}   &    \multicolumn{1}{c}{--}    &    \multicolumn{1}{c}{--}& \multicolumn{1}{c}{--} & \multicolumn{1}{c}{--}  & \multicolumn{1}{c}{--} & \multicolumn{1}{c}{--}& \multicolumn{1}{c}{--}     \\
QPPNet     &8.1&4.4&20.9&79.6& 6.1&3.9&24.1&136.8&          8.1&4.7&37.1&99.3&  \textbf{1.2}&1.1 &\textbf{1.7}&1.8  &6.9&3.1&24.5&56.7    \\\midrule
\LC  &\textbf{4.3}&\textbf{2.4}&\textbf{13.6}&\textbf{41.1}&  \textbf{4.9}&2.0&\textbf{8.3}&\textbf{58.0}& \textbf{5.7}&\textbf{2.2}&\textbf{22.9}&\textbf{60.9}&   \textbf{1.2}& \textbf{1.0}&\textbf{1.7}&\textbf{1.7}&\textbf{4.2}&\textbf{2.1}&\textbf{11.6}&\textbf{39.7}     \\ \bottomrule
\end{tabular}}
%\vspace{-2mm}
\end{table*}



%In this section, we describe the experimental study we conducted for our proposed model from three aspects. (1) The accuracy and efficiency of our tree-structured model on different datasets and workloads compared state-of-the-art cost estimators. (2) The effectiveness of our model on dynamic environment. (3) The ablation study of our model.
In this section, we evaluate {\LC} regarding (1) its accuracy and efficiency compared with the state-of-the-art cost estimators (2) its effectiveness in a dynamic environment (3) its ablation study.
%\vspace{-2mm}
\subsection{Experiment Setting}
% \noindent \textbf{Datasets and Workloads.} We use the real dataset IMDB and a series of real workloads derived from JOB~\cite{leis2015good}. It is much harder to estimate the cardinality and cost on the IMDB dataset than TPC-H, because of the correlations and skew distributions of the real-world data. The IMDB dataset includes 22 tables, which are joined on primary keys and foreign keys. We build indexes on primary keys. We use three types of query workloads.
\noindent \textbf{Workloads.} We conduct the evaluation on five workloads, namely JOB-M~\cite{leis2015good,yang2020neurocard}, JOB-light~\cite{yang2020neurocard},  JOB-light-ranges~\cite{yang2020neurocard}, TPC-H~\cite{poess2000new} and Stack~\cite{marcus2020bao}. The first three real-world workloads are derived from the JOB~\cite{leis2015good} workload that is coupled with the IMDB. The IMDB database contains 22 tables, and each table is joined by primary-foreign keys. We evaluate the family of JOB workloads on the test sets formatted in the previous researches~\cite{sun13end,wu2021unified,yang2020neurocard}. Following the training data generation in~\cite{sun13end}, we allow all possible values for operators and values without its restrictions. For TPC-H, we initialize a 100GB database and generate training and test plans by given templates. We also evaluate another real-world dataset, named Stack, which contains questions and answers from StackExchange websites, e.g., StackOverflow. Following~\cite{marcus2020bao}, we include 100GB of data and 25 query templates. For the last two workloads, we randomly split the 4000 template-generated samples into training and test sets in a 1:1 ratio.

%Following~\cite{yang2020neurocard,sun13end}, we create indexes on the primary keys of each table. We describe each of the settings in more detail as follows.
% \begin{enumerate}[noitemsep,leftmargin=*]

% \begin{itemize}[wide, labelwidth=!, labelindent=0pt ]
% \item \textbf{JOB-M~\cite{leis2015good,yang2020neurocard}} includes 16 tables in IMDB with multiple join keys. For example, the table {\it movie\_companies} can join table {\it title} on attribute {\it movie\_id}, and join table {\it company\_type} on attribute {\it company\_type\_id}. 
% Following~\cite{yang2020neurocard,wu2021unified}, we adopt the queries in JOB workload by limiting every table to appear at most once in each query. Each query joins 2–11 tables. The full outer join contains $10^{13}$ tuples. The predicates contain both string and numeric filters. 

% \item \textbf{JOB-light~\cite{yang2020neurocard}} contains 70 queries and 6 tables, which are {\it title} (primary), {\it movie\_info}, {\it movie\_info\_idx}, {\it cast\_info}, {\it movie\_companies}, , {\it movie\_keyword}. The join graph is a typical star graph, which means each non-primary table only joins the primary table {\it title} on attribute {\it title.id}. The full outer join includes about $2 \times 10^{12}$ tuples. The predicates only contain numeric filters, except for the range filter on table {\it title.production\_year}, the others are all equivalent filters. Each query involves 2 to 5 joinable tables.


% \item  \textbf{JOB-light-ranges~\cite{yang2020neurocard}} is also employed by us, whose 1000 queries are derived from the same setting of JOB-light by enriching the the variety of filters. The full outer join also includes about $2 \times 10^{12}$ tuples, showing its sophistication. Particularly, we follow the process in~\cite{yang2020neurocard,wu2021unified}. First, we uniformly sample 1000 queries over 18 pairs of join keys of JOB-light. For each pair of join keys, we randomly select a tuple without null column from the join result. Then the values of the filter are drawn from the sampled tuple, which is associated with 3–6 random comparison operators. The operators are selected according to whether the column can support range (i.e. {$<=$, $>=$, $BETWEEN$}) or equality filters.The distribution of the query results in JOB-light-ranges is more skew than those in JOB-light and JOB-M because the test set is also generated randomly. The sampler of tested samples excludes the empty plans by checking the final cardinalities. 

% % \item  \textbf{TPC-H.} Despite the data is generated by a fixed procedure, we still conduct experiments on TPC-H~\cite{poess2000new} due to its complex structure of queries. The data generator follows predefined distribution and relation to generate data for 8 tables. Each table has one column as the primary key and there are also 8 foreign keys connecting them to others. The query generator produces queries according to 22 templates, which contains more complex predication than JOB workload, such as nested query including ``having'' operator. The predicates contain both string and numeric filters. We slightly modify some of these templates for compatibility with PostgreSQL. 
% \item  \textbf{TPC-H~\cite{poess2000new}} is one of the most widely used workload benchmarks in evaluating the performance of database systems. It contains complex structures of queries. To conduct the evaluation based on TPC-H, the data generator follows a predefined distribution suggested by TPC-H to generate the data containing 8 tables. Each table has one column as the primary key and there are in total 8 foreign keys exist in these tables for join operations. The query generator produces queries according to 22 templates containing more complex predicates than the JOB workload. These include the nested query such as the ``having'' operator. The predicates contain both string and numeric filters. We slightly modify some of these templates for compatibility with PostgreSQL. 
% \end{itemize}
% \noindent \textbf{Metrics.} Based on Q-error~\cite{moerkotte2009preventing}, the mean is the average error of all the tested queries. The max is the maximum errors in the tested workload. The median is the median of errors of all the tested queries. The $K$th is the average of top-{1-$K$\%} largest errors in the tested workload.
% \vspace{1mm}
\noindent \textbf{Metrics.} We use the {\it mean error} ({\it mean} for short), the {\it maximum error} ({\it max} for short), and the {\it $K$-th (quantile) error} ({\it $K$-th} for short) to measure the accuracy of the estimators. The error of each query is defined based on the most common Q-error~\cite{moerkotte2009preventing}. The mean and max are the averages and maximum errors while the $K$-th is the top-\{1-$K$\%\} largest errors in the tested workload.

%\vspace{2mm}
\noindent \textbf{Baseline Methods.} %We compare LightCost to 3 cardinality or cost estimation methods, including state-of-the-art and the newest methods.
We compare {\LC} to the following state-of-the-art methods: \textbf{{\MSCN}}~\cite{kipf2018learned}, \textbf{{\TLSTMCost}}~\cite{sun13end}, \textbf{{\QPPNet}}~\cite{marcus2019plan}, and \textbf{PostgreSQL}~\cite{postgresql1996postgresql}. We note that {\MSCN} does not support string predicates, and hence we only evaluate it on numeric workloads. Also, for \textbf{PostgreSQL} we need to transform the output by the EXPLAIN command to a time cost. We employ a simple linear transformation for this purpose and the transforming factor is selected by minimizing the mean Q-error of training sets.
% \begin{enumerate}[noitemsep,leftmargin=*,itemindent=0em ]
% \begin{itemize}[wide, labelwidth=!, labelindent=0pt ]

% \item  \textbf{{\MSCN}}~\cite{kipf2018learned} employs a query-driven approach based on convolutional networks which was originally designed to estimate the result cardinality. To enable cost estimation, we follow the method in~\cite{sun13end} to add a cost head for the cost estimation. We include {\MSCN} as a baseline to show that a pure cardinality estimator can be hard to be transformed to a high-quality cost estimator. 
% We note that {\MSCN} does not support string predicates, and hence we only evaluate it on numeric workloads.


% \item \textbf{{\TLSTMCost}}~\cite{sun13end} proposes a tree-structured LSTM network for both cardinality and cost estimations. The main idea is to encode both queries and physical into a tree-structured model. {\TLSTMCost} also designs a pattern-based method that employs selected patterns to represent string values. %To compensate for the inaccuracy of cardinality estimation, they also use a sample bitmap to represent the data distribution. So the size of the feature vector becomes very large and requires hundreds of thousands of tuples for training. Although these methods can improve the generalization ability for predicate matching, the high training expense is unacceptable. 


% \item \textbf{{\QPPNet}}~\cite{marcus2019plan} introduces plan-structured deep neural networks, which are designed to estimate the cost of execution plans by constructing the neural units in an isomorphic tree dynamically. 
%{\QPPNet} also uses statistical cardinalities instead of embeddings of predication. However, due to little explicit input information, the model needs more samples to learn, and the accuracy is not high on real datasets. Although there is no cardinality prediction in the model, the training time is almost as long as~\cite{sun13end}.

% \item \textbf{PostgreSQL}~\cite{postgresql1996postgresql}The final method we compare is the plan cost given by the EXPLAIN command of the official PostgreSQL. The costs are determined by the cardinalities and the system's cost parameters. The method of estimating the cardinality has been described in detail in Section~\ref{sec:analysis}. The parameters includes the cost of a disk page fetch, processing each row during a query and so on. For example, in a normal Seq Scan node, the cost usually computed as (disk pages read * page cost) + (rows scanned * row cost). We directly adopt the default settings, and it is difficult to adjust these parameters in practice. Note that the cost estimates in PostgreSQL are expressed in arbitrary units, so it cannot be directly converted into actual time. After experiments we choose 0.003 as the factor, which means multiplying the cost 
% estimation by 0.003 to convert it into milliseconds. Because we find that it achieves optimal performances on the workloads of both JOB and TPC-H in our environment.

%\item \textbf{PostgreSQL}~\cite{postgresql1996postgresql} refers to the plan cost given by the EXPLAIN command of the official PostgreSQL. The costs are determined by the cardinalities and the system's cost parameters. %The method of estimating cardinality has been described in detail in Section~\ref{sec:correction}. 
%We note that we need to transform the output by the EXPLAIN command to a time cost. We employ a simple linear transformation for this purpose and the transforming factor is selected by minimizing the mean q-error using validation datasets.
% \end{itemize}
\noindent \textbf{Environment.} We conduct all the experiments on Intel(R) Xeon(R) Gold 6326 CPU with 256GB Memory. The queries are executed with PostgreSQL 12.9 on Ubuntu 20.04.

\noindent \textbf{Hyper Parameters.} We utilize the Adam optimizer~\cite{kingma2015adam} and initialize the learning rate to 0.001. The model is trained for 10 epochs. The sampling rate of cardinality calibration keeps the size of each lookup list under 5MB, which will not impose a burden on memory. To demonstrate the generalization ability of our method, we use these settings on all the datasets and workloads.

\vspace{-4mm}
\subsection{Accuracy}
% \begin{table}[t]
%   \caption{Accuracy performance on JOB-M.}
%   \label{tab:jobm-acc}
%   \vspace{-2mm}
%   \begin{tabularx}{\linewidth}{lllllll}
%     \toprule
%     Methods & Mean & Median & 90th & 95th & 99th & Max\\
%     \midrule
%     PostgreSQL & 46.4 & 5.27 & 93.9 & 292.7 & 422.4 & 556.5 \\
%     {\TLSTMCost} & 6.9 & 4.0 & 14.2 & 17.6 & 64.7 & 67.1 \\
%     {\QPPNet} & 8.1 & 4.4 & 19.2 & 20.9 & 79.6 & 91.3 \\
%         \midrule
%     {\LC} & \textbf{4.0} & \textbf{2.2} & \textbf{7.8} & \textbf{11.6} & \textbf{41.1} & \textbf{48.5} \\
%   \bottomrule
% \end{tabularx}
% \end{table}
% \begin{table}[t]
%   \caption{Accuracy performance on JOB-light.}
%   \vspace{-2mm}
%   \label{tab:jobl-acc}
%   \begin{tabularx}{\linewidth}{lllllll}
%     \toprule
%     Methods & Mean & Median & 90th & 95th & 99th & Max\\
%     \midrule
%     PostgreSQL & 9.5 & 2.8 & 11.8 & 31.1 & 125.7 & 205.3 \\
%     {\MSCN} & 26.9 & 5.4 & 11.7 & 47.6 & 494.7 & 867.1 \\
%     {\TLSTMCost} & 5.8 & 1.9 & 13.4 & 22.9 & 95.0 & 123.0 \\
%     {\QPPNet} & 6.1 & 3.9 & 19.3 & 24.1 & 136.8 & 225.3 \\
%             \midrule
%     {\LC} & \textbf{5.1} & \textbf{1.8} & \textbf{7.1} & \textbf{11.2} & \textbf{64.8} & \textbf{97.8} \\
%   \bottomrule
% \end{tabularx}
% \end{table}

% Tables~\ref{tab:jobm-acc}\textasciitilde~\ref{tab:tpc-acc} show the experimental results of all models on both real-world plans and generated plans. The results show that LightCost matches or significantly exceeds the best estimator across the board, not only in terms of mean and median, but also in terms of max, which demonstrates the generation of LightCost in handling different datasets and workloads. From these experimental results, we conclude several major findings as follows.

Table~\ref{tab:acc} presents the experimental results on all tested workloads. {\LC} achieves comparable or better accuracy against state-of-the-art estimators. The results show several major insights:
% Tables~\ref{tab:acc} presents the experimental results on all tested workloads. The results show that {\LC} achieves a comparable or better accuracy compared with all the state-of-the-art estimators. We conclude several major insights as follows.
% \begin{enumerate}[noitemsep,leftmargin=*]
% \item \textbf{LightCost outperforms other estimator in most cases.} In the real-word workloads JOB-M, because LightCost can skip the step of learning string predications, the average error is significantly reduced than that of the best existing method {\TLSTMCost} by 42\%. On the workloads of JOB-light and JOB-light-ranges, LightCost can outperforms other methods with cardinality correction, although the data distribution becomes more skew. The mean errors of LightCost on JOB-light workloads are 12\% and 16\% lower than those of {\TLSTMCost} and {\QPPNet}, respectively, and 8\% and 30\% on JOB-light-ranges workloads. 
% \item \textbf{The highest errors are due to skew and strongly correlated data distribution.} We analyzed the worst cases of LightCost and found that they are caused by extremely unbalanced data distribution. For example, on JOB-light-ranges workloads, the last sub-plan of a query estimates the row count to be 1 while it is actually closer to 2 million. Cardinality correction can improve these cases to some extent. And the model can collect statistics from
% the training samples automatically, such as the impact of join key and operation on results to alleviate the errors of extreme samples.

\noindent  \textbf{{\LC} consistently outperforms baselines in most cases.} For the JOB-M workload, {\LC} has achieved significantly lower errors than {\TLSTMCost} because {\LC} avoids the hardness of learning predicates embeddings. For the workloads of JOB-light and JOB-light-ranges with numeric predicates, {\LC} can still outperform {\TLSTMCost} and {\QPPNet} due to cardinality calibration. Particularly, the mean errors of {\LC} on JOB-light workload are 12\% and 16\% lower than those of {\TLSTMCost} and {\QPPNet}, respectively, and 8\% and 30\% lower for JOB-light-ranges. 
% \textcolor{blue}{Although queries in Stack have more complex structures, its skewness of cardinalities is less than JOB workloads. Hence, PostgreSQL becomes more accurate, while our method still consistently outperforms histograms and {\QPPNet}.(R3.D4)} 
Although queries in Stack, another real-world dataset, are distinct from JOB, our method still consistently outperforms PostgreSQL and {\QPPNet}.

% \item \textbf{The highest errors are due to skew and strongly correlated data distribution.} We analyzed the worst cases of {\LC} and found that they are caused by extremely unbalanced data distribution. For example, on JOB-light-ranges workloads, the last sub-plan of a query estimates the row count to be 1 while it is actually closer to 2 million. Cardinality correction can improve these cases to some extent. And the model can collect statistics from
% the training samples automatically, such as the impact of join key and operation on results to alleviate the errors of extreme samples.

% \noindent \textbf{The highest errors are due to skew and strongly correlated data distribution.} We find that the worst cases of {\LC} happen when the data are extremely unbalanced. For example, on JOB-light-ranges workloads, the last sub-plan of a query estimates the row count to be 1 while it is actually closer to 2 million. Cardinality correction can improve these cases to some extent. And the model can collect statistics from the training samples automatically, such as the impact of join key and operation on results to alleviate the errors of extreme samples.

% \item \textbf{The estimator in PostgreSQL may not be as useless as previously thought.} Although in most scenarios, the performance of
% PostgreSQL is still far behind the learning based methods, it is not as bad as previously thought. In~\cite{sun13end}, the results shows that the average error of PostgreSQL's estimator is 15-30 times higher than {\TLSTMCost}. The gap between the max errors is even bigger. This may be caused by not adjusting the parameters properly. After we adjust the factor of arbitrary units, the accuracy of PostgreSQL is significantly improved. It even outperforms {\MSCN} on most of cases of the numeric workloads. Through this phenomenon we want to show that it is possible to construct an acceptable estimator using only statistical cardinalities.
% \item \textbf{Simply transferring a cardinality estimator to a cost estimator is difficult.} As shown in Table~\ref{tab:jobl-acc} and Table~\ref{tab:tpc-acc}, the performance of {\MSCN} is not only the worst among learning-based estimators, but even worse than PostgreSQL in most cases. This is because {\MSCN} produces the cost in one step so it cannot include information about sub-plans. If a tree-structure estimator is to be implemented, {\MSCN} has to calculate the cardinalities of all the sub-plans, which will undoubtedly bring a great computational burden. In addition, {\MSCN} is difficult to deal with string values in predications, which is also a common drawback of many query-driven methods.


% \begin{table}[t]
%   \caption{Accuracy performance on JOB-light-ranges.}
%   \vspace{-2mm}
%   \label{tab:joblr-acc}
%   \begin{tabularx}{\linewidth}{lllllll}
%     \toprule
%     Methods & Mean & Median & 90th & 95th & 99th & Max\\
%     \midrule
%     PostgreSQL & 12.3 & 2.3 & 11.8 & 29.6 & 114.5 & 1755.3 \\
%     {\MSCN} & 31.8 & 11.4 & 34.7 & 467.6 & 494.7 & 1262.1 \\
%     {\TLSTMCost} & 6.2 & 2.5 & 18.2 & 30.9 & 88.0 & \textbf{131.0} \\
%     {\QPPNet} & 8.1 & 4.7 & 21.0 & 37.1 & 99.0 & 201.2 \\
%             \midrule
%     {\LC} & \textbf{5.7} & \textbf{2.2} & \textbf{13.0} & \textbf{22.9} & \textbf{60.9} & 137.4 \\
%   \bottomrule
% \end{tabularx}
% \end{table}
% \begin{table}[t]
%   \caption{Accuracy performance on TPC-H.}
%   \vspace{-2mm}
%   \label{tab:tpc-acc}
%   \begin{tabularx}{\linewidth}{lllllll}
%     \toprule
%     Methods & Mean & Median & 90th & 95th & 99th & Max\\
%     \midrule
%     PostgreSQL & 2.96 & 2.67 & 4.10 & 6.68  & 6.87 & 7.93 \\
%     {\QPPNet} & 1.20 & 1.12 & 1.63 & 1.68 & 1.80 & 4.21 \\
%             \midrule
%     {\LC} & \textbf{1.18} & \textbf{1.05} & \textbf{1.58} & \textbf{1.67} & \textbf{1.73} & 1.75 \\
%   \bottomrule
% \end{tabularx}
% \vspace{-2mm}
% \end{table}
\noindent  \textbf{Transforming a cardinality estimator to a cost estimator is hard.} Interestingly, we find that a fine-tuned PostgreSQL can sometimes perform better than {\MSCN}, which is originally designed for cardinality estimation. For example, for JOB-light, PostgreSQL entails a mean error of 9.5, whereas {\MSCN} entails a higher mean error of 26.9. This result shows that even though {\MSCN} is well-performing in cardinality estimation, directly transforming it for the cost estimation can significantly degrade the performance. Particularly, {\MSCN} produces the cost in one iteration and the information about sub-plans is ignored. If a tree-structure estimator is to be implemented, {\MSCN} has to calculate the cardinalities of all the sub-plans, which will undoubtedly bring a great computational burden. In addition, {\MSCN} is hard to deal with string values in predicates, which is a common drawback of many query-driven methods. %On the other hand, this result shows the potential of fine-tuning PostpreSQL to achieve better performance as we also observe that an ill-setting of PostgreSQL can be sub-performing.


% \item \textbf{Simply transferring a cardinality estimator to a cost estimator is difficult.} As shown in Table~\ref{tab:jobl-acc} and Table~\ref{tab:tpc-acc}, the performance of {\MSCN} is not only the worst among learning-based estimators, but even worse than PostgreSQL in most cases. This is because {\MSCN} produces the cost in one step so it cannot include information about sub-plans. If a tree-structure estimator is to be implemented, {\MSCN} has to calculate the cardinalities of all the sub-plans, which will undoubtedly bring a great computational burden. In addition, {\MSCN} is difficult to deal with string values in predications, which is also a common drawback of many query-driven methods.

% \item \textbf{Cost estimators with embeddings of representation are not necessarily more efficient than LightCost.} As an outstanding end-to-End learning-based cost estimator, {\TLSTMCost} does not outperform LightCost. This is because, despite great efforts, the performances of the cardinality estimates are unsatisfactory. In the experimental results of~\cite{sun13end}, the average error of cardinality estimates is 4.3-6.7 times cost error. Such a result is hardly to say that cardinality estimation brings a gain to cost estimation. Not to mention, original {\TLSTMCost} has difficulty handling complex filters in TPC-H, such as combinations of ``substring'' and ``IN''.

\noindent  \textbf{{\LC} has a comparable or better accuracy than {\TLSTMCost}.} {\TLSTMCost} achieves quite satisfactory accuracy in general, due to significant efforts have been paid in passing important information between nodes. In general, {\LC} has a comparable or better accuracy than {\TLSTMCost}. We note that there are two advantages of {\LC} over {\TLSTMCost}. First, original {\TLSTMCost} cannot handle some complex predicates in TPC-H such as predicates with the results from sub-queries, rendering it unable to produce results for TPC-H. In contrast, {\LC} can be applied to any type of predicates. Second, {\TLSTMCost} has to use much more training cost and processing time than {\LC} in order to achieve a comparable accuracy, as we will explain shortly in Section~\ref{sec:exp_efficiency}. %As an outstanding end-to-End learning-based cost estimator, {\TLSTMCost} does not outperform LightCost. This is because, despite great efforts, the performances of the cardinality estimates are unsatisfactory. %In the experimental results of~\cite{sun13end}, the average error of cardinality estimates is 4.3-6.7 times cost error. Such a result is hardly to say that cardinality estimation brings a gain to cost estimation. Not to mention, original {\TLSTMCost} has difficulty handling complex filters in TPC-H, such as combinations of ``substring'' and ``IN''.

% \item \textbf{Methods that simply reproduce the tree structure into the model can be improved.} Similar to {\LC}, {\QPPNet} also adopts a tree structure and statistical cardinality. The comparison between {\LC} and However, due to the lack of elaborated design, the model has to learn more implicit content on its own. In addition, without cardinality correction, its accuracy is far lower than LightCost. This is because the estimator needs to fit uncorrected extreme errors at the expense of accuracy on most other 
% samples.
% \vspace{-2mm}
\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{figure/efficient.pdf}
  \vspace{-4mm}
  \caption{Efficiency comparison on JOB-M (left) and JOB-light (right) including accuracy vs. training plans, model inference latency and training cost. %Note that all coordinates representing quantities are log scale due to huge gaps in performances of the methods.
  }
\label{fig:eff}
\vspace{-6mm}
\end{figure}
% 


% \begin{figure}[th]
%     \centering
%     \begin{subfigure}{0.48\linewidth}
%         \includegraphics[width=0.97\linewidth]{figure/samples-m.pdf}
%         \caption{Accuracy vs. Training Plans}
%         \label{fig:m-samp}
%     \end{subfigure}
%     \begin{subfigure}{0.48\linewidth}
%         \includegraphics[width=0.99\linewidth]{figure/training-m.pdf}
%         \caption{Training Cost}
%         \label{fig:m-training}
%     \end{subfigure}
%     % \begin{subfigure}{0.242\textwidth}
%     %     \includegraphics[width=\linewidth]{figure/size-m.pdf}
%     %     \caption{Space Cost}
%     %     \label{fig:m-space}
%     % \end{subfigure}
%     \begin{subfigure}{0.48\linewidth}
%         \includegraphics[width=0.8\linewidth]{figure/latency-m.pdf}
%         \caption{model inference latency}
%         \label{fig:m-lat}
%     \end{subfigure}
%     \begin{subfigure}{0.48\linewidth}
%         \includegraphics[width=0.7\linewidth]{figure/samples-light.pdf}
%         \caption{Accuracy vs. Training Plans}
%         \label{fig:light-samp}
%     \end{subfigure}
%         \begin{subfigure}{0.48\linewidth}
%         \includegraphics[width=0.98\linewidth]{figure/training-light.pdf}
%         \caption{Training Cost}
%         \label{fig:light-training}
%     \end{subfigure}
%     %     \begin{subfigure}{0.242\linewidth}
%     %     \includegraphics[width=0.96\linewidth]{figure/size-light.pdf}
%     %     \caption{Space Cost}
%     %     \label{fig:light-space}
%     % \end{subfigure}
%     \begin{subfigure}{0.48\linewidth}
%         \includegraphics[width=0.99\linewidth]{figure/latency-light.pdf}
%         \caption{model inference latency}
%         \label{fig:light-lat}
%     \end{subfigure}
%     \caption{Efficiency performance on JOB-M (top) and JOB-light (bottom). Note that all coordinates representing quantities are log scale due to huge gaps in the performances of the methods.}
%     %\vspace{-0.1in}
% \label{fig:eff}
% \vspace{-10pt}
% \end{figure}

\noindent  \textbf{{\LC} has a better accuracy than {\QPPNet} thanks to cardinality calibration.} Similar to {\LC}, {\QPPNet} also adopts a tree structure and statistical cardinality. However, due to the lack of an elaborated design, the model has to learn more implicit content (e.g. cardinalities of child nodes) on its own. In contrast, {\LC} cherry-picks more features as explicit input and adopts a more reasonable model design. In addition, because of the employment of cardinality calibration, {\LC} is better at handling cases with extreme errors, making its overall accuracy much better than {\QPPNet}.
\begin{figure}[th]
\vspace{-1mm}
  \centering
  \includegraphics[width=1.0\linewidth]{figure/box.pdf}
  \vspace{-6mm}
  \caption{Actual errors on the JOB-M (left) and JOB-light (right) workloads. The box boundaries represent the 25th/50th/75th percentiles. Outliers are the points outside the box that are beyond 1.5 times the interquartile range.}
\label{fig:box}
\vspace{-4mm}
\end{figure}

\noindent  \textcolor{black}{\textbf{The actual error of {\LC} under different conditions is within a reasonable range.} In addition to Q-error, we also evaluate the actual errors (i.e. $cost_{est.}-cost_{act.}$ ) on JOB-M and JOB-light. As shown in Figure~\ref{fig:box}, although {\LC} sometimes may underestimate the cost of queries with abnormal statistical cardinalities, the cardinality calibration has significantly reduced the outliers compared to {\QPPNet}. Similarly, {\LC}'s outliers are less than the other two estimators on JOB-light workload with more skewed data.}
%\vspace{-2mm}
\subsection{Efficiency}\label{sec:exp_efficiency}
%\vspace{-1mm}
% \begin{table}
%   \caption{Efficiency performance on JOB-M}
%   \label{tab:jobm-eff}
%   \begin{tabular}{p{40pt}p{40pt}p{30pt}p{30pt}p{30pt}}
%     \toprule
%     Methods & Training Samples\quad    $\times$ Time&Model Size  & Training Time &  model inference latency\\
%     \midrule
%     {\TLSTMCost} &150,000\quad$\times$ 4.8s& 5.9MB & >30h & 70.3ms \\
%     {\QPPNet} &120,000\quad$\times$ 4.8s &250KB $\times$ 16 &>25h & 51.1ms \\
%             \midrule
%     LightCost &\textbf{2400}\quad\quad $\times$ 4.8s&16KB & \textbf{8.3m} & \textbf{6.9ms} \\
%   \bottomrule
% \end{tabular}
% \end{table}


%The efficiency of LightCost is what we most want to highlight. 
% We compare the efficiency of the estimators using JOB-M as an example, as the workloads are closest to the real-world expense.  We use the same generator in all of these methods, which has been described in detail in Section~\ref{sec:training}. The times we count in Table~\ref{tab:jobm-eff} are from single-process executions, which are almost impossible in practice for {\TLSTMCost} and {\QPPNet}. Because their execution time exceeds 8 and 6 days respectively. Whereas LightCost only takes about three hours even with a single process as it cut off about 99\% of the training samples. Note that the unit time to generate training samples is longer than that of generating prepared test samples. Because random processes can skew data, and it is possible to produce tuples with extremely large cardinality. Although a timeout can be set, that cases cannot be completely avoided.

We compare the efficiency of methods by evaluating their accuracy on the JOB-M and JOB-light workloads. As shown in Figure~\ref{fig:eff}, {\LC} performs well with only 2000 training samples, while {\QPPNet} and {\TLSTMCost} require $80,000$ and $100,000$ training samples to achieve desired accuracy.
% We use the same generator in all of these methods, as described in Section~\ref{sec:training}. All the experiments are evaluated in a single-threaded manner. 
% Figure~\ref{fig:eff} shows different aspects of efficiency comparisons on JOB-M and JOB-light. %The times we count in Table~\ref{tab:jobm-eff} are from single-process executions, which are almost impossible in practice for {\TLSTMCost} and {\QPPNet}. 
% As shown in Figure~\ref{fig:eff}, when there are 2000 training samples, {\LC} can achieve a satisfactory accuracy on JOB-M. The effect of further increasing samples becomes diminished. In contrast, {\QPPNet} and {\TLSTMCost} have to use around 80k and 100k training samples to get relatively better accuracy.

For sample generation and training, both {\QPPNet} and {\TLSTMCost} require multiple days to finish the whole process, whereas {\LC} takes only about 3 hours for samples generation and 8 minutes for training. {\LC} has a significantly improved efficiency as it uses only about 2\% samples of that in {\TLSTMCost}.

% Note that the unit time to generate training samples is longer than that of generating prepared test samples{\color{red} I do not understand this sentence}. Because random processes can skew data{\color{red} I do not understand this sentence}, and it is possible to produce tuples with extremely large cardinality. Although a timeout can be set, that cases cannot be completely avoided.{\color{red} I do not understand the purpose of this sentence}

% In addition to the number of training plans, the training time is also related to the model size. As shown in Figure~\ref{fig:m-space}, in terms of model size, {\TLSTMCost} and {\QPPNet} are 375 and 250 times larger than {\LC}. Looking into {\TLSTMCost}, we find that the MLP processing sample bitmaps exceeds 1000 in terms of input size, not to mention bigger layers such as representation embeddings of predicates. Similarly, {\QPPNet} uses many layers with 128 neurons. Meanwhile, {\QPPNet} does not use embedding technique, resulting in more costs to train separate models for different operators. Instead, {\LC} only uses several MLPs with small sizes. For example, the largest backbone MLP only has a size of 32 $\times$ 16. 

% From the other two methods, because the training time is too long and unstable, we can only count the approximate minimum time. It can be seen that the advantages of LightCost are very obvious, and it only takes about 0.4\% to 0.5\% of their time.

Inference latency is also mainly determined by the model size.  As shown in Figure~\ref{fig:eff}, {\LC} is about 7 and 10 times faster than {\QPPNet} and {\TLSTMCost} on average. Note that the latency of {\LC} includes cardinality calibration.

On JOB-light workload, {\LC} consistently outperforms {\QPPNet} and {\TLSTMCost} on model training, sample generation as well as model inference latency. {\MSCN}'s inference latency is close to {\LC} with GPU (GeForce GTX 1080 Ti) acceleration as shown in Figure~\ref{fig:eff}. %, as the result of ignoring some physical information. 
From other perspectives, {\LC} significantly outperforms {\MSCN}.
% We do not count results involving batch techniques, because this can also be done with asynchronous multi-threading in database systems. 
%Based on the decomposition from {\LC}, 
The cost of the cardinality calibration is minor. It takes around 2-4 ms on average on the IMDB dataset with pre-loaded lookup lists, accounting for less than 50\% of the model inference latency.

In conclusion, due to the elaborate model design and efficient cardinality calibration, {\LC} can achieve better performance with fewer training samples and shorter latency.
\vspace{-1mm}
\subsection{Performance for Dynamic Scenarios}
We also conduct experiments to show that {\LC} is more suitable to be applied in a dynamic database environment, i.e., the data is gradually updating. This mimics the database application environment in the real world. Our setting is that the database does not need to collect training queries and retrain the model after relatively minor updates. %Given that {\TLSTMCost} uses predicate embeddings, inserting data without retraining is not realistic. So we randomly delete 20\% tuples in each table except the type tables, such as company\_type and role\_type. In this way, the 
We randomly delete 20\% tuples in each table except the type tables, such as {\it role\_type}. In this way, the 
correlation of the data is changed without loss of authenticity. Our update strategy for {\LC} is simple: we only update the lookup lists of cardinality calibration, which is described in detail in Section~\ref{sec:correction}. The cost of updating the lists is minuscule in comparison to the cost of collecting training data and retraining the model. For {\TLSTMCost} and 
{\QPPNet}, since we cannot find any such fast update strategy, we directly adopt the stale model on the updated database.

As shown in Table~\ref{tab:jobm-dacc}, the performances of methods relying on statistical cardinality, i.e. PostgreSQL, {\QPPNet} and {\LC}, are almost unchanged. Whereas the accuracy of {\TLSTMCost} drops significantly since the predicate embeddings still represent the original distribution. As the row counts, which are the dominant factor, have been re-analyzed, the updated histograms and cardinality calibration lead to satisfactory performance. 
% \vspace{-0.5mm} 
\begin{table}\small %
  \caption{Dynamic accuracy performance on JOB-M.}
  \vspace{-3mm}
    \label{tab:jobm-dacc}
    \resizebox{0.95\linewidth}{!}{

  \begin{tabular}{lllllll}
    \toprule
    Methods & Mean & 50th & 90th & 95th & 99th & Max\\
    \midrule
    PostgreSQL & 39.9 & 4.27 & 89.9 & 212.3 & 322.4 & 456.5 \\
    {\TLSTMCost} & 9.9 & 7.0 & 29.2 & 40.6 & 124.7 & 247.1 \\
    {\QPPNet} & 8.2 & 4.6 & 20.1 & 23.2 & 88.0 & 101.0 \\
        \midrule
    {\LC} & \textbf{4.5} & \textbf{2.6} & \textbf{13.8} & \textbf{16.9} & \textbf{43.5} & \textbf{54.7} \\
  \bottomrule
\end{tabular}}
\vspace{-1mm}
\end{table}

Hence in practice, {\LC} can be adjusted to fit well in dynamic scenarios: first, we train the model with the initial database. Then, we
maintain lookup lists in the dynamic environment, and estimate costs by the stale model with updated cardinality calibration. When the degree of update reaches a user-defined threshold, we collect training samples and retrain the model. Then the new model can estimate the cost when its retraining is finished. Considering that other methods take a long time to collect training data and retrain the model, {\LC} can maintain a stable accuracy during this period.
% \vspace{-2mm}

% \begin{figure}[h]
%   \centering
% \includegraphics[width=0.9\linewidth]{figure/bs.pdf}
% \caption{Training with different batch size.}
%  \label{fig:bs}
% \end{figure}

% \vspace{1mm}
% \noindent \textbf{Batch Size.}
% As described in Section~\ref{sec:training}, we provide methods to train {\LC} using batch technique. Figure~\ref{fig:bs} shows the trend of mean error during training under different batch sizes. When the batch size is larger than 1, the error will decrease more slowly. This is because: (1) Each backward propagation of {\LC} already includes many nodes in a plan, which is equivalent to adopting the batch technique; (2) Based on this, increasing the batch size will cause the gradient of each iteration to be averaged by more samples, and a bigger batch size will reduce the number of iterations, leading to slower convergence. Therefore, in order to converge the model faster, the default batch size in the setting of {\LC} is 1.
\begin{table}\small %
  \caption{Ablation study of cardinality calibration (cc). }
  \vspace{-3mm}
  \label{tab:cc}
\resizebox{0.95\linewidth}{!}{

  \begin{tabular}{lllllll}
    \toprule
    Methods & Mean & 50th & 90th & 95th & 99th & Max\\
    \midrule
    % {\LC} w/ ac & \textbf{2.4} & \textbf{1.5} & \textbf{5.4} & \textbf{8.7} & \textbf{14.4} & \textbf{15.2} \\
    {\LC} w/o cc & 7.3 & 4.3 & 10.7 & 18.3 & 77.7 & 89.2 \\
    {\LC} w/ cc & \textbf{4.3} & \textbf{2.4} & \textbf{8.2} & \textbf{13.6} & \textbf{41.1} & \textbf{48.5} \\
    {\LC} w/ bitmap & 6.7 & 4.2 & 9.2 & 18.0 & 62.1 & 79.5 \\
    {\QPPNet} w/ cc & 5.4 & 2.7 & 8.4 & 13.3 & 51.1 & 68.5 \\
    
  \bottomrule
\end{tabular}
}
\vspace{-1mm}
\end{table}

\begin{table}\small %
  \caption{Accuracy vs. $\lambda$. Notation $\mathcal{N}$ refers to the set of non-index nodes and $\mathcal{L}$ denotes the last node.}
    
  \vspace{-3mm}
  \label{tab:lam}
  \resizebox{0.95\linewidth}{!}{

  \begin{tabular}{llllllll}
    \toprule
    $\lambda$ of $\mathcal{N}$ & $\lambda$ of $\mathcal{L}$ & Mean & 50th & 90th & 95th & 99th & Max\\
    \midrule
    1 & 1  & 5.0 & 2.9 & 8.4 & 14.7 & 49.7 & 57.2 \\
    1 & 2  & 4.8 & 2.6 & 7.2 & 14.3 & 47.2 & 56.1 \\
    2 & 2  & 4.5 & 2.3 & 8.2 & \textbf{12.5} & \textbf{42.3} & 56.6 \\
    2 & 10  & 5.2 & 3.1 & 9.2 & 13.1 & 54.1 & 67.1 \\
    \midrule
    2 & 4 & \textbf{4.3} & \textbf{2.4} & \textbf{8.2} & 13.3 & 43.5 & \textbf{54.7} \\
  \bottomrule
\end{tabular}}
\vspace{-1mm}
\end{table}


% \vspace{-2mm}
\subsection{Dissecting {\LC} }
To gain more insights, we evaluate the importance of design components in {\LC} on JOB-M.
% \vspace{1mm}

\noindent \textbf{Cardinality calibration.} It can be seen from Table~\ref{tab:cc} that the effectiveness of cardinality calibration is significant. The mean Q-error drops by 42\% compared to the method without calibration. The results show that if the cardinality calibration is removed, the performance of {\LC} is less accurate. Therefore, cardinality calibration significantly improves the accuracy of the method using statistical cardinality. When this technique is substituted by a sampling-based bitmap (a binary embedding following {\TLSTMCost}\cite{sun13end}), the improvements are compromised, since bitmaps provide less precise and explicit information than cardinality calibration. We also try embedding cardinality calibration into {\QPPNet} to prove its independent efficiency. The results show that though the accuracy cannot exceed {\LC}, the mean error has been reduced by 33.3\% compared with the original {\QPPNet} (5.4 vs. 8.1). However, the training cost is still much higher than {\LC} due to redundant parameters and less shared information between different plan nodes.


% By the way, we also evaluate the performance with actual cardinality, which shows the power of the remaining part when the cardinality estimates are accurate. 


% \vspace{1mm}
\noindent \textbf{Weights of Loss Function.}
% $\lambda$ is the weight of each node when calculating the loss, which was described in Section~\ref{sec:modeldesign}. Since the non-index nodes and the last node are more important, we mainly increase the weights of the two, while fixing a weight $1$ for the nodes using the index.
In the loss calculation, the weight of each node, $\lambda$, is assigned based on the nodes' importance. So non-index nodes and the last node are given higher weights, while index nodes are assigned a weight of 1. As shown in Table~\ref{tab:lam}, when we appropriately increase the weights of the two types of nodes, the accuracy gradually improves.
% We consider that the last node has several particularities: (1) The last node outputs the final result, which directly affects the evaluation; (2) The path of its backward propagation is the longest, and the gradient is easy to become smaller; (3) It is also a non-index node itself. Hence, the weight of the last node can be additionally increased. 
The overall performance is optimal when the weights of the non-index nodes and the last node are 2 and 4. The average error is 20\% lower compared with the case when $\lambda$ is not adjusted. However, when the weights increase excessively, it will affect the learning in other nodes. This is why the accuracy drops drastically when the weight of the last node is 10. 

\begin{table}\small %
  \caption{Performance vs. network architectures.}
  \vspace{-1mm}
  \label{tab:arc}
  \resizebox{0.95\linewidth}{!}{
  \begin{tabular}{llllll}
    \toprule
      $bone$ &  $state$ &  $cost$ & Mean & 95th & Latency\\
    \midrule
    $1\times MLP$ & $1\times MLP$ & $1\times MLP$ & 4.5 & 14.7 & \textbf{7.2ms} \\
    $1\times MLP$ & $1\times MLP$ & $3\times MLP$ & \textbf{4.3} & 14.2 & 7.5ms \\
    $2\times MLP$ & $1\times MLP$ &$2\times MLP$ & 4.4 & \textbf{13.3} & 7.9ms \\
    $1\times MLP$ & $2\times MLP$ & $2\times MLP$ & 4.4 & 13.9 & 7.7ms \\
    $1\times CNN$ & $1\times CNN$ & $2\times CNN$ & 4.6 & 14.1 & 8.1ms \\
    \midrule
    $1\times MLP$ & $1\times MLP$ & $2\times MLP$ & \textbf{4.3} & \textbf{13.3} & 7.4ms \\
  \bottomrule
\end{tabular}}
% \vspace{-3mm}
\end{table}

\begin{figure}[]
\vspace{-1mm}
  \centering
  \includegraphics[width=0.9\linewidth]{figure/ratio.pdf}
  \vspace{-1mm}
  \caption{The ratio of JOB-M execution time before and after applying {\LC} to PostgreSQL. Lower ratio indicates better performance.}
\vspace{-1mm}
\label{fig:ratio}
\end{figure}

\noindent \textbf{Network Architecture.}
We conduct experiments on several combinations of the three sets of MLPs (see Section~\ref{sec:modeldesign}) and Table~\ref{tab:arc} shows the results. The default setting is 2 layers in Cost MLP and 1 layer in the other two MLPs. When the layers changes, the accuracy remains almost the same. This phenomenon demonstrates the accuracy of {\LC} is not very sensitive to the layers of MLPs. We also implemented a Convolutional Neural Network(CNN)-based tree according to~\cite{marcus2019neo,marcus2020bao,mou2016convolutional}. While the accuracy is not obviously improved, the latency and training time on CPUs are longer due to the slow sliding window operation.



\vspace{-1mm}
\subsection{Case Study: Embedded into PostgreSQL}
We apply {\LC} to PostgreSQL's plan optimizer. %as a cost model to evaluate its effectiveness. Given the massive plans enumeration in PostgreSQL, 
In a plan, we use {\LC} to estimate the cost of the lowest merge nodes and propagate the calibrated cardinalities and costs upwards. Each bar in Figure~\ref{fig:ratio} corresponds to a query, showing the execution cost ratio between the two plans selected by {\LC} and PostgreSQL. The ratios are sorted in an increasing order for better visualization. Compared with the default PostgreSQL, {\LC} performs better than PostgreSQL in around $80\%$ cases, with an average improvement of 7\% in execution time. We remark that {\LC} is independent of how the execution plans are formed. In PostgreSQL, the quality of the candidate plan worked out by its search method is relatively limited, which diminishes the benefit brought by {\LC}. In future works, we will experiment with stronger search methods employed by other systems such as Neo~\cite{marcus2019neo} and Bao~\cite{marcus2020bao}. We can foresee the potential benefits because of {\LC}'s low training overhead and demonstrated adaptability in dynamic scenarios.
% \vspace{-1mm}


