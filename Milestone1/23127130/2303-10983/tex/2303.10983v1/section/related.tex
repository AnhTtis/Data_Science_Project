
%Most cost estimators are based on cardinality estimation. Hence, we first introduce various cardinality estimators, then introduce methods of cost estimation.
% \vspace{-1mm}
\noindent \textbf{Cardinality estimation.}
Histogram-based model is a representative data-driven method, which is also the most common method used in relational database management systems. 1-D histogram~\cite{selinger1979access} assumes all attributes are mutually independent and the selectivity of each column is built as a cumulative histogram. In addition, multidimensional histograms~\cite{poosala1996improved,poosala1997selectivity,gunopulos2005selectivity} are more precise than 1-D histograms by capturing inter-column correlations. However, these density models usually lose information between columns and tables by taking an independence or uniformity assumption. %Moreover, the requirements for learning the desirable structure also entail expensive training costs.
%
% And graphical models were also proposed to predict cardinality from join tables. 
%
Recently, a fixed tree-structure density estimator called sum-product networks (SPN)~\cite{poon2011sum} has been applied to estimating cardinality. 
% Intuitively, {\it sum nodes} that split the population (i.e., the rows of data set) into clusters and product nodes split independent variables of a population (i.e., the columns of a data set). Leaf nodes represent a single attribute and approximate the distribution of that attribute using histograms or other linear functions~\cite{molina2018mixed}. 
An SPN is a tree structure where each leaf node stands for an estimated selectivity of a subset on a row or a column split by the sum nodes or product nodes.  
Both~\cite{hilprecht2019deepdb} and~\cite{zhu2020flat} generalize SPN variants to cardinality estimation and achieve a superior performance on both accuracy and efficiency. %However, SPNs have limited expressiveness, because some distributions cannot be efficiently captured by SPNs of any depth~\cite{martens2014expressive}.  
% From the perspective of cost estimation, Zeroshot~\cite{hilprecht2022zero} employs {\DeepDB}~\cite{hilprecht2019deepdb} to estimate the cost. However, they need many datasets to pre-train the model and the model inference latency is even more than {\TLSTMCost} due to the large cardinality model which has been discussed in Section~\ref{sec:intro}.  
%
Another approach to constructing a density cardinality model relies on deep autoregressive models from the machine learning community~\cite{germain2015made,nash2019autoregressive,radford2019language,vaswani2017attention}. The main idea is to learn the correlation between columns sequentially by the neural networks.~\cite{yang2020neurocard,yang2019deep} develop strategies to overcome the practical challenges in cardinality estimation, such as skipping the wildcards and factorizing complex columns. Unfortunately, these methods are still subject to representing predicates, and large models can also lead to excessive model inference latency.  

%\noindent \textbf{Query-driven cardinality estimation.} 
% To fully leverage past or collected queries, query-driven cardinality estimators~\cite{markl2003leo,wu2018towards} build a mapping between the featured queries to predicted cardinalities. Various regression models can be applied. 
Query-driven cardinality estimators~\cite{markl2003leo,wu2018towards} leverage past or collected queries to predict cardinalities by building a mapping between them with regression models. Traditional machine learning tools such as KDE~\cite{heimel2015self,kiefer2017estimating} and mixture models~\cite{park2020quicksel} have been explored as hybrid methods in predicting row counts. Among these methods, {\MSCN}~\cite{kipf2018learned} is the representation of neural networks, which adopts a multi-set convolutional network to learn the correlations between joins.~\cite{dutt2019selectivity} applies a lightweight XGBoost on cardinality estimation, reducing model inference latency effectively. 

% However, the limitations of this family are also obvious, that is, a large amount of training samples needs to be collected and training the model is time consuming.

%However, the limitations of embedding a cardinality estimator to the cost model are obvious. The most serious of them is that it will bring lots of extra model inference latency, which has been discussed in Section~\ref{sec:intro} 

% For example,~\cite{hilprecht2022zero} has integrated {\DeepDB}~\cite{hilprecht2019deepdb} into their cost model. Although the accuracy has been improved, the loss outweighs the gain. First, the expense of training a accurate cardinality estimator is high (For {\DeepDB}, memory is the biggest challenge~\cite{yang2020neurocard}.); Most importantly, cardinality estimator impose unexpected long model inference latency (the max latency of {\DeepDB} on JOB-M can be more than 100ms).  

\noindent \textbf{Cost estimation.} 
%Due to the limitations of learning cardinality estimators on cost estimation, many studies apply machine learning to estimate cost directly. Among them,
There is a plethora of studies on cost estimation~\cite{sun13end,marcus2019plan,venkataraman2016ernest,wu2013predicting,zhang2005statistical,zhang2006xseed,wu2013towards,duggan2011performance,duggan2014contender,zhou2020query,hilprecht2022zero}. Some studies~\cite{zhang2005statistical,zhang2006xseed,wu2013towards,duggan2011performance,duggan2014contender,zhou2020query} belong to classic non-machine-learning-based methods. Among the machine-learning based approaches, Zeroshot~\cite{hilprecht2022zero}, {\TLSTMCost}~\cite{sun13end} and {\QPPNet}~\cite{marcus2019plan} are the state of the arts. Zeroshot~\cite{hilprecht2022zero} employs a cardinality estimator {\DeepDB}~\cite{hilprecht2019deepdb} to estimate the cost, and it needs many datasets to pre-train the model. In contrast, {\LC} does not require a pre-training phase. Our scope also does not include other pre-training works~\cite{paul2021database,lu2021pre}.

{\TLSTMCost}~\cite{sun13end} and {\QPPNet}~\cite{marcus2019plan} are the closest works to {\LC}, because they also employ plan-structured networks. Compared with {\TLSTMCost}~\cite{sun13end}, {\LC} uses a more lightweight model architecture. Also, in terms of performance, {\LC} demands much fewer training samples than {\TLSTMCost}, which reduces the training cost significantly. In addition, {\LC} does not need to learn the representation of predicates, which means it can be applied to any type of SQL query. In contrast, {\TLSTMCost} cannot be applied on complex SQL, such as some queries in TPC-H. \textcolor{black}{Recently there are some techniques that aim to improve the representations of predicates~\cite{zhao2022queryformer}. However, these embedding models are still plagued by complex predicates, such as the ``Like'' predicate, and they cannot effectively reduce the training cost on string predicates.}


Compared with {\QPPNet}~\cite{marcus2019plan} that first proposes the plan-structured network, the first and second differences as compared to {\TLSTMCost} also apply. Our {\LC} inherits the basic tree-based framework but our techniques including featurization and loss function along the training and inference process are surprisingly effective. Also, {\QPPNet} learns a model for each operator, which can lead to a long training time since the independent model cannot share information. In contrast, {\LC} learns all the operations in one model.  In addition, {\LC} adopts the cardinality calibration which significantly improves the accuracy while {\QPPNet} uses the original plan cardinalities.  
\textcolor{black}{
There are other problem settings around the cost model, such as the end-to-end optimizer. Generally, their cost models serve as components to estimate the costs of plans produced by certainly learned optimizers~\cite{marcus2020bao,marcus2019neo}. Our setting requires estimating the cost of an arbitrary plan. Therefore, these approaches are not suitable for our settings.}
\vspace{-2mm}
%Some earlier works are based on plan-level models~\cite{venkataraman2016ernest,wu2013predicting} and they treat the execution plan as a whole, ignoring the internal structural information. Later operator-level methods~\cite{akdere2012learning,li2012robust} divide the execution plan into sub-plans for processing. However, they make a much stronger assumption that an operator only interacts with its direct parent. There are also many traditional methods without adopting machine learning. For example~\cite{zhang2005statistical,zhang2006xseed} focus on predicting statistics about queries in XML databases.~\cite{wu2013towards} reuses the optimizer cost models from database systems with the aid of sampled data. In general, the above methods all require human experts to determine appropriate features and the corrections after analysis. Furthermore, there is little interaction between nodes in traditional methods, so the errors in intermediate steps are easily accumulated. Some techniques~\cite{duggan2011performance,duggan2014contender,zhou2020query} also extend to concurrent query performance prediction for analytical queries. These techniques assume prior knowledge of query templates and also they are different from our work in the problem definition. There are also some cost estimators employing learning cardinality model. For example, Zeroshot~\cite{hilprecht2022zero} employs {\DeepDB}~\cite{hilprecht2019deepdb} to estimate the cost. However, they need many datasets to pre-train the model and the model inference latency is even more than {\TLSTMCost} due to the large cardinality model which has been discussed in Section~\ref{sec:intro}.


%, so we are not going to discuss them further.
% \noindent \textbf{Tree-based neural network:} The most common underlying structure of neural network that has been explored so far is a linear chain. However, the optimizer in database system exhibit syntactic properties that would naturally combine sub-plans to plan.~\cite{mou2016convolutional,socher2011parsing,tai2015improved} have investigated the potential of interacting node information through different types of tree-structured neural networks. Researchers in database community~\cite{chi2011sla,marcus2019plan,sun13end} also have adopted tree-based neural network for various purposes.