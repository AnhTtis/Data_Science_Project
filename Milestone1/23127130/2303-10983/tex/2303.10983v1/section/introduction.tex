%\vspace{-2mm}
\section{Introduction}
\label{sec:intro}

% Query optimization is a critical component in a database system, for the purpose of selecting the optimal execution plan~\cite{selinger1989access} based on cost estimation. Recently, the database community attempts to utilize learning-based models to improve the estimation, which can be divided into cardinality and card estimator. 

% Cost estimation, the task of predicting the latency of a query plan, is a critical component in a database system, for the purpose of selecting the optimal execution plan~\cite{selinger1989access}. While predicting the costs is notoriously difficult, as the latency of a plan is highly dependent on a number of factors, such as the operators chosen, the underlying data distribution, and the resource availability. Classical cost estimators in practical database systems are performed using linear models that requires fine tuning of parameters by experts. Meanwhile the models also make several simplifying assumptions about correlation 
% between data and system settings, leading to suboptimal planning decisions that degrade the overall query performance~\cite{leis2018query,lohman2014query}. As a result, the database community attempts to utilize machine learning models to improve cost estimation recently.

%Selecting a suitable SQL query execution plan among many possible plans is at the core of a database system. 
In database systems, estimating the running time of a query execution plan, abbreviated as {\it Cost Estimation}, is an indispensable function~\cite{selinger1989access}. Cost estimation is notoriously difficult, as the actual execution cost of a plan is highly dependent on a wide spectrum of factors including the involved operators, the underlying data distribution, and the resource availability. 

% Cardinality estimator predicted the rows count of query results, while can be trained from sampled data~\cite{wu2021unified,yang2019deep,yang2020neurocard,hilprecht2019deepdb}, or generated queries~\cite{kipf2018learned,dutt2019selectivity,hasan2020deep}. These methods have a fatal limitation: they cannot predict cost without other auxiliaries, so they are hard to be generalized to support normal SQL queries. For example, cardinality estimator may be able to give the order of the joins by the predicted cardinality, however, this is only accurate 
% based on the assumption that physical operations are consistent among all the scans and joins. In a real database system, besides normal sequence scan and nested loop, other merge and scan methods are also widely used, such as Hash Join, Index Scan. These methods vary significantly in terms of execution time, which brings inaccuracy to the estimation. In addition, some operations other than merge and scan, such as hash and sort, cannot be predicted only based on cardinality.

% Cost models rely heavily on cardinality, which represents the row count of tuples in the query result. A prevailing family of learning-based cost models are based on learned cardinality, and they are generally closely associated with learning-based cardinality estimators, which can be further divided into two categories: query-driven methods~\cite{kipf2018learned,dutt2019selectivity,hasan2020deep} and data-driven ones~\cite{wu2021unified,yang2019deep,yang2020neurocard,hilprecht2019deepdb}. Note that it is not practical to apply the cardinality estimators directly to cost estimation. Because compared with cost model, cardinality estimators have fatal limitations. For example, they cannot receive the information of operators, such as how the tuples are traversed, by index or in sequence. Cost models based on query-driven cardinality estimator need to collect training execution plans to train the model. For example, {\TLSTMCost}~\cite{sun13end} requires 100,000 training plans. Assuming the average time to generate a sample is 4.8 seconds, a single-thread generator will take more than 5 days.
% Moreover, these methods need to train embeddings (vectors serve as unique features) to represent the predicates, which cost time and other resources significantly. For example, {\TLSTMCost}~\cite{sun13end} needs to train more than 2 days on JOB workloads~\cite{leis2015good}. In contrast, cost models~\cite{hilprecht2022zero} based on data-driven cardinality estimator do not have to learn predicate embeddings, they learn the distribution directly from the data. However, they still face the efficiency challenges. First, despite not collecting queries, their space and time costs of training are still expensive. Second, combining models will introduce a lot of extra model inference latency. 
% In addition, all of the above methods have to represent predicates, so complex predicates or filters, such as predicates with results from sub-queries in TPC-H~\cite{poess2000new} and ranges filters in JOB-light-ranges~\cite{yang2020neurocard}, may not be supported.
% \begin{table}[t]
%   \caption{%Model size and mean model inference latency of cost (cardinality) estimator on the workloads of JOB-light~\cite{leis2015good}
%   Existing ML-based cost estimation models either incurs large model or high latency (JOB-light workload).}
%   \label{tab:jobm-dacc}
%   \vspace{-5pt}
%   \begin{tabularx}{\linewidth}{lll}
%     \toprule
%     {\bf Methods} &  {\bf Model size} & {\bf  Latency} \\
%     \midrule
%     {\DeepDB}~\cite{hilprecht2019deepdb} (cardinality estimation) & 4.0MB & 51.3ms \\
%     {\MSCN}~\cite{kipf2018learned} (performed on CPU) & 2.8MB & 7.1ms \\
%     {\TLSTMCost}~\cite{sun13end} & 6.0MB & 43.3ms \\
%     {\QPPNet}~\cite{marcus2019plan} & 4.5MB & 29.3ms \\
%         \midrule
%     {\LC} (ours) & 16KB & 4.0ms \\
%   \bottomrule
% \end{tabularx}
% \vspace{-5pt}
% \end{table}
% \vspace{-1mm}
\begin{figure}[t!]
\centering
\vspace{-1mm}
  \includegraphics[width=0.7\linewidth]{figure/intro.pdf}  
  \vspace{-4mm}
  \caption{Existing ML-based cost estimation models either
incurs large models or high latency (JOB-light workload).}
\label{fig:intro}
  \vspace{-6mm}

\end{figure}

% {\small
% \begin{table}
%   \caption{%Model size and mean model inference latency of cost (cardinality) estimator on the workloads of JOB-light~\cite{leis2015good}
%   Existing ML-based cost estimation models either incurs large model or high latency (JOB-light workload).}
%   \label{tab:jobm-dacc}
%   \begin{tabularx}{\linewidth}{lllll}
%     \toprule
%     {\bf } &  {\DeepDB} & {\MSCN} & {\TLSTMCost} & {\QPPNet} & {\LC} \\
%     {\bf } &  \cite{hilprecht2019deepdb} & \cite{kipf2018learned} & \cite{sun13end} & \cite{marcus2019plan} & Ours\\
%     \midrule

%   %  PostgerSQL~\cite{postgresql1996postgresql} & - & 2.1ms \\
%      Model & 4.0MB & 2.8MB & 16KB & 4.5MB & 16KB \\
%     Latency  & 51.3ms & 7.1ms & 43.3ms & 29.3ms & 4.0ms\\
%   \bottomrule
% \end{tabularx}
% \end{table}

Cost estimation has been extensively studied~\cite{leis2018query,lohman2014query, kipf2018learned,dutt2019selectivity,hasan2020deep,wu2021unified,yang2019deep,yang2020neurocard,hilprecht2019deepdb}. Classic cost estimators~\cite{leis2018query,lohman2014query} \textcolor{black}{embedded in the existing database systems often require a human expert to determine miscellaneous cost constants. %(R1.D1).
} %, entailing a laborious and error-prone process. 
Recent researches~\cite{kipf2018learned,dutt2019selectivity,hasan2020deep,wu2021unified,yang2019deep,yang2020neurocard,hilprecht2019deepdb} employ advanced machine learning techniques to enhance the model expressiveness, aiming to relieve the burden of human experts and improve the estimation accuracy. Most models heavily rely on the estimation of the {\it cardinality}, which refers to the number of returned tuples after performing a certain query. When cardinality estimation is finished, the estimated execution cost is derived via an explicitly or implicitly learned relationship between the actual cost and the cardinality. These machine learning methods can achieve relatively high estimation accuracy~\cite{wang2021we,sun13end,marcus2019plan}.

% \vspace{1mm}
%\noindent{\bf Limitations of existing cost estimators.} 
Many existing machine learning-based cost estimators entail high model training or model inference costs, which are often rooted at the costly cardinality estimation process. Cardinality estimation approaches can be either query-driven~\cite{kipf2018learned,dutt2019selectivity,hasan2020deep} or data-driven~\cite{wu2021unified,yang2019deep,yang2020neurocard,hilprecht2019deepdb}. The former collects sufficient queries for model training, whereas the latter directly learns the cardinality from the data distribution. {Built upon these cardinality estimation models, the cost estimation models can be query-driven or data-driven, correspondingly.} While being effective, existing cost estimation models entail high training costs for large datasets. As an exemplar of a highly accurate query-driven cost estimator, {\TLSTMCost}~\cite{sun13end}, may necessitate multiple days of preprocessing or training for the benchmark JOB workload~\cite{leis2015good}, as pointed out by~\cite{hilprecht2022zero}. % The root cause is that {\TLSTMCost} requires around $10^5$ pre-generated execution plans for learning predicate representations. %Particularly, it can require $10^5$ pre-generated execution plans for training to learn the predicate representations. Generating an execution plan typically costs a few seconds, making it more than 5 days to generate these training plans. 
Data-driven approaches~\cite{hilprecht2022zero,hilprecht2019deepdb}, in contrast, require smaller training sets. However, they employ a relatively heavy model with a large parameter space for high estimation accuracy, leading to high training and model inference costs. For example, 
{\DeepDB}~\cite{hilprecht2019deepdb}, a cardinality estimator, has a model inference latency significantly higher than the vanilla estimator in PostgreSQL. Correspondingly, the cost estimator Zeroshot~\cite{hilprecht2022zero} built on top of {\DeepDB} has a longer model inference latency. 
The high training or inference cost can easily limit the scalability of these approaches, hindering their deployment to data-intensive applications.

%is In contrast, cost models~\cite{hilprecht2022zero} based on data-driven cardinality estimator do not have to learn predicate embeddings, they learn the distribution directly from the data. However, they still face the efficiency challenges. First, despite requiring fewer training plans, their space and time costs of training are still expensive due to a large number of parameters in cardinality model. Second, combining models will introduce a lot of extra model inference latency. 
%In addition, all of the above methods have to represent predicates, so complex predicates or filters, such as predicates with results from sub-queries in TPC-H~\cite{poess2000new} and ranges filters in JOB-light-ranges~\cite{yang2020neurocard}, may not be supported.

\vspace{1mm}
\noindent
{\bf Our solution: \LC.} %Our intuition is that existing cost estimation models might be too aggressive in enhancing the model expressiveness, resulting in higher model complexity that requires heavy training and inference.
Our intuition is that the high cost of existing models either comes from training set size (e.g., a large number of execution plans in query-driven methods) or large model space (e.g., excessive parameters in data-driven methods). 
We argue that {\it a simple model can work better} in the context of cost estimation, motivated by which we design {\LC}, a \textbf{Fas}t \textbf{Co}st estimation model. Compared with state-of-the-art approaches, {\LC} exhibits a reduced model size, lower training overhead, and faster model inference latency, while still achieving superior or comparable accuracy with only about 2\% pre-generated training samples. As a preview, Figure~\ref{fig:intro} shows a comparison between {\LC} and the state-of-the-art approaches regarding the model size and model inference latency. {\LC}'s model size is at least two orders of magnitude smaller than {\DeepDB}, {\QPPNet}, and {\MSCN}, and it has an inference latency that is also about one order of magnitude shorter than {\TLSTMCost}. Interestingly, the design of {\LC} is surprisingly simple. Its model is a stack of multi-layer perceptrons (MLPs) following the execution plan tree. The effectiveness of the model is secured two-fold. First, {\LC} employs features that are selected carefully based on the internal structure of execution plans. Second, {\LC} incorporates a simple and effective data-driven cardinality calibration technique. 

\vspace{1mm}
\noindent{\bf Contributions.}
{Our contributions} are summarized as follows:



%(i.e.,  but elaborate estimator xxx by efficient model design and cherry-picked explicit features for both efficiency and accuracy purpose. xxx while incorporates a data-driven cardinality correction technique. {\LC}  First, we carefully investigate the information provided by the execution plans, and integrate them through a lightweight but elaborate estimator by efficient model design and cherry-picked explicit features for both efficiency and accuracy purpose. Second, we adopt a sampling method to correct the statistical cardinality to further improve accuracy. Compared with previous learning-based cost estimator, our model improves the efficiency of training and inference significantly, and still outperforms these methods on accuracy. 

% Another paradigm of cost estimation is based on statistical cardinalities by histograms in database systems. This family of methods is less common due to the inaccuracy of histograms. {\QPPNet}~\cite{marcus2019plan} introduces plan-structured model based on statistical cardinalities. However, there are still some shortcomings in this attempt. First, because of the unreasonable model design, the training cost is still expensive. Second, directly using the cardinality given by the histogram results in a drop in accuracy on complex real-work workload~\cite{zhou2020query}.

% In conclusion, there are currently two main challenges in constructing a good learning-based cost estimator. First, cost models based on learned cardinality estimators face substantial training costs and model inference latency. Second, existing estimators using statistical cardinality are unsatisfactory in terms of accuracy and efficiency.

% Cost estimators output expected execution cost directly, which can avoid extra steps and accumulative errors. However, existing cost estimation methods~\cite{zhou2020query,sun13end} still need to train a predicate embedding to predict cardinality, which cost time and other resources significantly. For example, {\TLSTMCost}~\cite{sun13end} needs to train almost 2 days on JOB workloads~\cite{leis2015good}. On another task setting, even the training predicates totally come from the test set, Graph~\cite{zhou2020query} still takes more than 40 minutes. In fact, the methods of predicates embedding used by the two are almost the same. According to the experiments, the training time of Graph~\cite{zhou2020query} under normal settings will also exceed 4 hours. A more serious problem is the huge time cost to generate execution plans: {\TLSTMCost}~\cite{sun13end} requires 150,000 training samples. Assuming the average time to generate a sample is 4.8 seconds, a single-process work will take more than 8 days.

% From the works stated above, for a long time, the database community has focused on how to accurately predict cardinality. Despite such a huge time cost, the accuracy is still unsatisfactory (e.g., the state-of-the-art estimator NeuroCard~\cite{yang2020neurocard} still has a Q-error~\cite{moerkotte2009preventing} more than 20,000 on JOB~\cite{leis2015good} workloads.). Then a question arises, do we need such accurate row count prediction? 
% After our survey, we found that commercial database systems tend to estimate cardinality rapidly with simple statistics and assumptions. For example, PostgreSQL~\cite{postgresql1996postgresql} retrieves the sampled histogram for corresponding columns from statistical tables. Then the optimizer locates the bucket that the bound values are in and gets the counts in the range. All alone, these traditional methods have been underutilized by learned estimation due to low accuracy. In~\cite{marcus2019plan,wu2013predicting} the optimizerâ€™s cardinality estimates have been considered, but because of the unreasonable model design, the training time and accuracy are still unsatisfactory. 

% To address these challenges, we propose a lightweight learning-based cost estimation model with cardinality correction named {\LC}. First, we carefully investigate the information provided by the execution plans, and integrate them through a lightweight but elaborate estimator by efficient model design and cherry-picked explicit features for both efficiency and accuracy purpose. Second, we adopt a sampling method to correct the statistical cardinality to further improve accuracy. Compared with previous learning-based cost estimator, our model improves the efficiency of training and inference significantly, and still outperforms these methods on accuracy. 


% \vspace{1mm}
% \noindent
% {\bf Contributions.} Our contributions can be summarized as follows:

%\begin{enumerate}[noitemsep,leftmargin=*]
$\bullet$ One important contribution of this study is to build a way toward using lightweight cost estimation models for real-world database systems. Data are continuously updated in real-world database systems, and cost estimators have to be frequently retrained for high accuracy. Many existing methods focus on enhancing accuracy by integrating heavier machine learning models that require a high training cost. Inevitably, the existing high-cost models may be more difficult to be applied in such a dynamic environment. Our new model sheds light on that a lightweight machine learning model can be a better fit for this scenario. %still be suitable for cost estimations in database systems.


$\bullet$  We present {\LC}, a tree-structured cost estimator with cardinality calibration. Compared with previous tree-based models such as {\QPPNet}~\cite{marcus2019plan}, {\LC} reduces training costs and model inference latency significantly. {\LC} is simple for being only composed of small-sized MLPs, but as well effective for integrating many informative plan-related features to unleash the potential power of a simple model (see Section~\ref{sec:feature}). Furthermore, to compensate for the inaccuracy of cardinality provided by the histogram-based model, {\LC} employs a simple but very effective cardinality calibration method based on a sampling technique (see Section~\ref{sec:correction}).

$\bullet$  We conduct extensive experiments in real workloads on static and dynamic databases. Compared with existing works, both the accuracy and efficiency of {\LC} are significantly improved on various benchmarks. In addition, {\LC} also exhibits a significant advantage over the state-of-the-art methods when applied to dynamic database systems. 

\vspace{-1mm}
% \noindent
% {\bf Organization.} The rest of the paper is organized as follows. We describe the preliminaries and problem settings about cost estimation in Section~\ref{sec:problem}.  %Then we present the main idea and highlights of {\LC} in Section~\ref{sec:mainidea}. We discuss the novelties from the perspective of model design and feature extraction to show how {\LC} can improve the efficiency significantly in Section~\ref{sec:modeldesign} and Section~\ref{sec:feature}. Then Section~\ref{sec:training} describe the training settings of {\LC}. 
% Then we describe the {\LC} in Section~\ref{sec:LC}. %We propose the cardinality correction technique to enhance {\LC} in Section~\ref{sec:correction}. 
% We present experimental evaluations in Section~\ref{sec:experiments}. Finally, we discuss the related works in Section~\ref{sec:related}, and conclude the paper in Section~\ref{sec:conclusion}.
% %\end{enumerate}
    