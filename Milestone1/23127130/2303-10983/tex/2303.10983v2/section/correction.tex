\vspace{-4mm}
\subsection{Cardinality Calibration}
\label{sec:correction}
The cardinality of executing a sub-plan (i.e., a sub-tree in the plan tree) is an important feature for estimating the total plan execution cost. As the actual cardinality corresponding to each node in the plan tree is not known beforehand, an {\it estimated cardinality} is often used. Among the learning-based methods employing such an approach, there are two issues. First, the classic histogram-based cardinality estimator embedded in the common DBMS can be vulnerable, rendering the final estimate of the execution cost unreliable. Second, these methods often require a large number of training samples to fit the model. %The work of~\cite{marcus2019plan,wu2013predicting} tries to leverage the original cardinality predictor embedded in a database system (e.g., PostgreSQL) and learn to map the prediction to the actual execution costs. %These methods require a large number of training samples to fit the two distributions that differ widely and the accuracy of cost estimates will be hurt. To alleviate this problem, in this part, we will present our correction method of cardinality. Since our method is a modified version of the histogram-based method, we first briefly introduce the histogram-based cardinality models in DBMS.
%Unfortunately, these methods require a large number of training samples to fit the model. 
To alleviate these issues, we present a simple but effective cardinality calibration technique based on the classic histogram-based model. 

We first briefly introduce the histogram-based cardinality models adopted in common databases.
% The main idea is to transform cardinality estimation to selectivity estimation, where the {\it selectivity} of a table is defined as the percentage of the rows that are qualified for the predicates applied in the table. Particularly, given an SQL query, we let %$a_i$ denote the $i$-th column queried in the SQL query and 
% $\theta_i$ denote the $i$-th predicate (i.e., filtering predicate or join predicate) for the $i$-th table. The {\it selectivity} regarding the $i$-th table is the normalized form of the cardinality, formalized as:
% \vspace{-2mm}
% \begin{equation}
% \label{equ:sel-card}
% %\begin{displaymath}
% sel(\theta_i)=card(\theta_i)/|T|,
% %\end{displaymath}
% \end{equation}
% where $|T|$ denotes the total row count of the table. Since $sel(\theta_i)$ can be estimated by a histogram-based method based on several statistical techniques~\cite{postgresql1996postgresql}, the cardinality for the node corresponding to the $i$-th table can be estimated by $sel(\theta_i)\times |T|$. 
\textcolor{black}{The main idea is to first estimate the row count of each predicate by a histogram-based method based on several statistical techniques~\cite{postgresql1996postgresql}. Then the cardinality of a merge node can be estimated by the {\it product} of the cardinalities under its 
sub-predicates, and this propagates upward along the tree to estimate the cardinality of the root node of the whole plan. We note that the {\it product} is used based on the assumption that the predicates applied in different tables are independent. }
% 
%
% \begin{equation}
% \label{equ:indepent-card}
% card(\theta)=\Pi_{1\leq i\leq n} card(\theta_i).
% \end{equation}
% The cardinality of each merge node can be estimated by the product of two cardinalities under two predicates $\theta$, and this propagates upward along the tree to estimate the cardinality of the root node of the whole plan. 
% Note that the cardinality of a merge node is not necessarily equal to the product of the cardinality of its two children when the children are not {\it based on} each other. This is because in this case, one of the child nodes does not go through the join predicate. % We note that Equation~\ref{equ:indepent-card} is used based on the assumption that the predicates applied in different tables are independent. 


% :
% \begin{equation}
% \label{equ:1}
% card(\theta)=\Pi_{1\leq i\leq n} sel(a_i,\theta_i).
% \end{equation}
% Then, the overall cardinality can be estimated by $sel(a,\theta)\times |T|$.



% {\color{blue} 
% Particularly, given an SQL query, and we let $\theta_i$ denote the set of predicates that query the $i$-th table queried in the SQL (e.g. {\it title.production\_year $\geq$ 1980} and {\it title.production\_year $\leq$ 2010} are regarded as one set of predicates). The final cardinality of a query is calculated as the product of the cardinalities of each set of predicates:
% \begin{equation}
% \label{equ:indepent-card}
% card(\theta)=\Pi_{1\leq i\leq n} card(\theta_i).
% \end{equation}
% }

% \vspace{-1mm}
\begin{figure}[h]
\vspace{-4mm}
\centering
  \includegraphics[width=0.85\linewidth]{figure/calib.pdf}
  \vspace{-3mm}
  \caption{The process of cardinality calibration.}
\label{fig:correct}
\vspace{-4mm}
\end{figure}
% The major advantage of the histogram-based methods is efficient with no training and extreme low model inference latency. Existing learning-based cardinality estimators not only require extra resources to train, but also have high model inference latency. For tree-structure cost estimators, the more serious problem is that due to the independence of some leaf nodes, the cardinality model must be inferred at almost every leaf nodes, which will impose significant long model inference latency. Whereas estimated cardinalities in database systems can be used directly, but are often ignored because of the inaccuracy. 

The histogram-based method for cardinality estimation is known to be efficient as it requires no training. The downside is its low estimation accuracy for cardinalities. If we directly use this estimator for estimating the execution cost, we may suffer from low accuracy as well. %In contrast, existing learning-based cardinality estimators not only require extra resources to train, but also have a high model inference latency. As a return, they often have higher estimation accuracies. 
{\it We argue that it is possible to apply a simple calibration of the classic cardinality estimator so that the cardinality estimation accuracy can be enhanced and becomes suitable to contribute positively to the cost estimator.} %Taking tree-structure cost estimators as an example, the more serious problem is that due to the independence of some leaf nodes, the cardinality model must be inferred at almost every leaf nodes, which will impose significant long model inference latency. Whereas estimated cardinalities in database systems can be used directly, but are often ignored because of the inaccuracy. 

% To prove that the cardinalities given by histogram is not useless, we have sampled some execution plans in the JOB workload. As shown on the left side of Figure~\ref{fig:planrows}, we find that the gap between the two is indeed huge. But when we calculate the cosine similarity, the result shows different views. The cosine similarity can be formalized as:
\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{figure/plan-actual.pdf}
  \vspace{-4mm}
  \caption{Q-error of vanilla estimated cardinalities vs. calibrated cardinalities on JOB-M workload.}
\label{fig:plan-actual}
\vspace{-6mm}
\end{figure}
Our main insight is that there is a strong correlation between the estimated cardinality of histogram-based methods and the actual cardinality. However, the accuracy of a histogram-based method is sometimes disrupted by disastrous estimates. As shown in Figure~\ref{fig:plan-actual}, we observe a significant gap between the estimated and actual cardinalities, but we do not that their cosine similarity is more than 0.90, which indicates a strong positive correlation between the two sets of cardinalities. 

%While the strong correlation shows that the mapping between the cardinality and execution cost is {\it learnable}, it may still suffer from disastrous estimates. 
The strong correlation indicates that the estimated cardinality can be calibrated. Serious errors that occur at merge nodes whose two children are both scan nodes often propagate to the remaining nodes. For example, Node {\it n} in Figure~\ref{fig:correct} is a merge node of this type, which are referred to as {\it lowest-level merge nodes}. The errors at {\it lowest-level merge nodes} propagate because the final estimate is the product of the cardinality estimates of all corresponding nodes. This also enlightens us that calibrating these rooted errors is more cost-effective. Therefore, we propose to {\it 
calibrate} the cardinality in these {\it lowest-level merge nodes} to avoid propagating the errors to the final estimate.
% Let us take an extreme example to illustrate this. 
%
% So the cardinality of a current node is obtained by multiplying the cardinalities of its two child nodes. 
%
% To explain, we first briefly introduce how the histogram helps in estimating cardinalities. Let $a_i$ denote the $i$-th column queried in the SQL and $\theta_i$ denote the $i$-th predicate (i.e., filtering conditions). To simplify the formulation, we introduce the {\it selectivity}, which can be regarded as the normalized form of the cardinality, and it can be formalized as:
% \begin{displaymath}
% sel(a_i,\theta_i)=card(a_i,\theta_i)/|T|,
% \end{displaymath}
% where $|T|$ denotes the total row count of the table. Based on the histogram method, the selectivity of a query is calculated as the product of the selectivity of each predicate:
% \begin{equation}
% \label{equ:1}
% sel(a,\theta)=\Pi_{1\leq i\leq n} sel(a_i,\theta_i).
% \end{equation}
% In this equation, each table and column is assumed to be independent for simplicity. 
% {\color{blue} For example, if the cardinality of the first scan node is 0, according to the Equation~\ref{equ:1}, the estimated cardinalities of its ancestor nodes are all 0. It is an extreme bad case for histogram method, and it will hurt the cost estimates badly.}
% To avoid a node with 0 cardinality leading to a ``zero'' plan, for example, PostgreSQL add an extra constant when $const$calculating each single selectivity. For histogram method, if the value $val$ of the predication fall into the $j$th bucket $bkt$, then we can calculate the selectivity as: 
% \begin{displaymath}
% sel(a_i,\theta_i)=\frac{1}{num(bkt)} \left[  const + \frac{val - min(bkt_j)}{max(bkt_j) - min(bkt_j)} \right]
% \end{displaymath}
% This method is acceptable for computing continuous numerical columns. For queries on columns which have more common values, the list of most common values (MCVs) is used to determine the selectivity. The database gather the statistics about the $n$ values with the highest probability of occurrence and stores their probability in the $mcv$ list. If the value in the predicate exists in $mcv$, it can be taken directly from it; If not, the selectivity is calculated as:
% \begin{displaymath}
% sel(a_i,\theta_i)=\frac{const-\sum_j^nmcv_j}{num(dst_i)-n},
% \end{displaymath}
% where $dst_i$ denotes the all distinct values in $i$th column.
% The real problem comes when the two sets of tuples are joined by index. Because all the values in index column are unique, so the database system can only use an equation that relies only on the number of distinct values for both relations together with their null fractions:
% \begin{displaymath}
% \begin{split}
% sel(a_i,\theta_i)*sel(a_{i+1},\theta_{i+1})=(const-null)(const-null) \\
% \times min(\frac{1}{num(dst_i)},\frac{1}{num(dst_{i+1})}),
% \end{split}
% \end{displaymath}
% where $null$ means null fractions usually assigned 0. For nodes using index, this method will get the same results regardless of the input tuples.
% \subsection{Correction of Cardinality}
% \label{sec:sample}

%It is not realistic to improve the estimates of each node due to huge computational complexity, so we need a cost-effective method. It can be seen from the above case that if there is a serious error in the cardinality estimates in the first merge node, the error will be accumulated in subsequent calculations. Moreover, we should avoid excessive high latency caused by the correction process. So correcting the cardinalities of first merge node in each execution plan by querying the samples of its actual result is cost-effective. We propose a sampling method to achieve it. 

% \vspace{1mm}
\noindent
{\bf Calibration Procedure.} Our method is based on sampling. First, in preprocessing we sample rows from the inner join results of each pair of tables with common attributes. The samples are stored in a {\it lookup list} and we set a low sample rate to store it in the main memory. Let us denote the sample rate as $\frac{1}{p}$ for some $p>1$. The sampling can be done in parallel to generate training plans. In {\LC}'s training and inference, we can query the corresponding lookup list to get a second cardinality estimate of the lowest-level merge node in the plan tree besides of vanilla estimate from DBMS. Particularly, we filter the samples in the lookup list with the predicate of the lowest-level merge node and get the qualified row counts, $c$. The second estimate for the merge node is then calculated as $c\cdot p$. If $\tilde{c}$ is the 
vanilla cardinality estimate, then the bias factor is $\frac{c\cdot p+{p}}{\tilde{c}+p}$.
%
% \begin{equation}
% \label{equ:bias}
% bias=\frac{c\cdot p+{p}}{\tilde{c}+p}
% \end{equation}
%
In the numerator and denominator we both add a value $p$, which is the inverse of the sample rate. This helps counter the error that can be introduced when the sample rate is small, as it reduces the weight given to $c$.
% Next we calculate the bias between the corrected cardinality and the original plan cardinality given by the database system. The bias is the ratio of the corrected cardinality and original cardinality. Then we propagate the bias to all nodes based on the first merge node by multiplying the original cardinality by the bias. 

% Let $p(a_0)=sel(a_0,\theta_i)$ and $p(a_0)=sel(a_0,\theta_i)$, then 


% According to Equation~\ref{equ:1}, the cardinality of the first merge node estimated by the database system is 
% % \begin{displaymath}
% % sel_d(a_0,a_1,\theta_0,\theta_1)=sel(a_0,\theta_0)sel(a_1,\theta_1)
% % \end{displaymath}
% \begin{displaymath}
% card_d(\theta_0,\theta_1)=card(\theta_0)card(\theta_1)
% \end{displaymath}
% So the actual meaning of the bias is:
% % \begin{equation}
% % \label{equ:2}
% % bias=\frac{sel_a(a_0,a_1)}{sel_d(a_0,a_1)} = \frac{sel((a_0,\theta_0)|(a_1,\theta_1))}{sel(a_1,\theta_1)}
% % \end{equation}
% \begin{equation}
% \label{equ:2}
% bias=\frac{card_a(\theta_0,\theta_1)}{card_d(\theta_0,\theta_1)} = \frac{card(\theta_0|\theta_1)}{card(\theta_1)}
% \end{equation}

% \begin{algorithm}[t]
% \LinesNumbered  
% \SetAlgoVlined
% % 
% \caption{Cardinality Correction from a merge node \label{alg:card}}
% \KwIn{Sub-plan nodes $N$; Vanilla estimated cardinalities $V$; Lookup lists $L$;}
% \KwOut{Corrected estimated cardinalities $C$}
% Find a {\it low-level merge node} $n_x$ from $N$\;
% Query the cardinality $c_x$ of $n_x$ from $L$\; 
% Calculate the $bias$\;% based on Equation~\ref{equ:bias}\;
% \ForEach{$n_i \in N$}{
% Draw the corresponding vanilla est. card. $v_i$ from $V$\;
%     \If{$n_i$ is the ancestor of $n_x$}{
%         $c_i = bias \times v_i$;
%     }
%     \ElseIf{$n_i$ is the sibling of $n_x$ or the sibling of ancestors of $n_x$ or the child of $n_x$}{
%          \If{$n_i$ is related with $n_x$}{
%             $c_i = bias \times v_i$;
%          }
%          \Else{
%             $c_i = v_i$;
%          }
%     }
%     \ElseIf{$i \neq x$}{
%         $c_i = v_i$;
%     }
% }
% $C = \{c_i\}$\;
% return $C$\;
% \end{algorithm}

% \begin{algorithm}[t]
% \LinesNumbered  
% \SetAlgoVlined
% % 
% \caption{Cardinality Correction from a merge node \label{alg:card}}
% \KwIn{Sub-plan nodes $N$; Vanilla estimated cardinalities $V$; Lookup lists $L$;}
% \KwOut{Corrected estimated cardinalities $C$}
% Find a {\it low-level merge node} $n_x$ from $N$\;
% Query the cardinality $c_x$ of $n_x$ from $L$\; 
% Calculate the $bias$\;% based on Equation~\ref{equ:bias}\;
% \ForEach{$n_i \in N$}{
% Draw the corresponding vanilla est. card. $v_i$ from $V$\;
%     \If{$n_i$ is related of $n_x$}{
%         $c_i = bias \times v_i$;
%     }
%      \Else{
%         $c_i = v_i$;
%      }
    

% }
% $C = \{c_i\}$\;
% return $C$\;
% \end{algorithm}


Figure~\ref{fig:correct} illustrates the calibration process. In Node {\it n} there is a large estimate error (200 vs. 20k), causing an unacceptable error (500 vs. 50k) finally. To address the issue, during inference or training, our calibration method identifies Node {\it n} as a {\it lowest-level merge node} and inspects its lookup list. Then, we query the sampled lookup list and obtain a value of 239. Suppose that the sample rate is $\frac{1}{100}$, i.e., $p=100$. Then, the $factor$ is computed to be $\frac{239\times 100+{100}}{200+100}=80$. The calibration will be propagated to other nodes related to Node {\it n}.

% For example, in the left of Figure~\ref{fig:correct}, we have a plan tree with original and actual cardinalities, whose cardinality of the root node differs from the actual value by more than 100 times (6k vs. 700k). According to the steps of cardinality correction, in the right of Figure~\ref{fig:correct}, first we collect the samples and save the lookup lists. In inference or training, when we meet the first Nested Loop node (the third node in the  Figure~\ref{fig:correct}), we query from the corresponding look lists and get a corrected cardinality ``30k'', which is pretty closer to the real cardinality ``30k''. Then we can get bias by dividing the two value and get 96. In practice, the corrected cardinality may fluctuate around the actual ones due to random sampling. So to avoid the huge errors caused by very little actual cardinalities, we have added a constant to the numerator and denominator respectively. For instance, in IMDB dataset, we sample each join result by a ratio of one percent considering the inference efficiency. So if the real cardinality is 1, we might get the corrected rows to be 20. Then if we calculate the bias directly and get 20, we will impose an irrational factor on subsequent cardinalities. As the fluctuation will increase as the sampling rate decreases, we refer to the inverse of sample rate and assign this constant as 100 in this case. 

%After the calculation of the bias, we propagate it to the nodes related with the 3rd node. 
%We need to further find out whether other nodes are related to the 3rd node to complete the correction. 
To identify the related nodes of the Node {\it n}, the following conditions are used: First, all ancestor nodes of Node {\it n} are related to it, because their results come from it; Second, the sibling nodes of Node {\it n} and its ancestor nodes can be identified based on the {\it Subquery} (see Section~\ref{sec:feature}). If the node is the {\it Subquery} of Node {\it n} or its ancestors, it is relevant. Finally, the two child nodes of the Node {\it n} can also be distinguished by {\it Subquery} as the second condition. The estimated cardinalities of the related nodes are then calibrated by multiplying the factor, while the estimated cardinalities of unrelated nodes are left unchanged. This calibration improves the overall Q-error, as shown in Figure~\ref{fig:plan-actual}.%{\color {blue} Note that if the two child nodes of Node 3 are not {\it based on} each other, the estimated cardinality of the Node 3 is not necessarily equal to the product of the cardinalities of its two child nodes. The reason is that the estimated cardinality of one of the nodes has not gone through the join predicate, whereas the join predicate is applied on Node 3.}

\noindent
\textcolor{black}{\textbf{Remarks.} Compared with fully sampling-based cardinality estimators, each lookup list in cardinality calibration only includes the merge results of two tables, which avoids huge memory overhead. Compared with other sampling strategies embedded in ML-based models, such as sampling bitmaps~\cite{wu2021unified,sun13end}, our cardinality calibration provides more precise and explicit information, which is more efficient for a lightweight model without extra embeddings of predicates.}
%\vspace{-2mm}
%
% In this example, Node 2, Node 5, and Node 7 are related to Node 3. Hence, they are corrected by multiplying the bias computed by Equation~\ref{equ:bias}. Other nodes are not related to Node 3, so they keep their vanilla estimated cardinalities. We summarize the procedure in Algorithm~\ref{alg:card}. %{\color {blue} We note that we only start the cardinality correction on the nodes whose child nodes are both leaf nodes, such as Node 3. If there are multiple such nodes in one execution plan (rare in database systems), our correction starts from each of such nodes and propagation the biases respectively.}
% The remaining two merge nodes (the 5th and 7th node in Figure~\ref{fig:correct}) are based on the third node. The first Seq Scan (the first node in Figure~\ref{fig:correct}) is not based on the third node, so it is not corrected. In addition, the second Index Scan and the second Seq Scan nodes (the 4th and 6th node in Figure~\ref{fig:correct}) are independent of the third node, which means they do not need to be corrected. The first Index Scan node is based on the first Seq Scan node, whose cardinality is expected to be $p(a_1|a_0)$ while it is assigned as $p(a_1)$ in the database system. So it can be corrected by multiplying the bias according to Equation~\ref{equ:2}.

%Although this method cannot completely solve the problem of inaccurate predictions, it can effectively alleviate extreme errors. As shown on the right in Figure~\ref{fig:correct}, several plans with original cardinalities of almost 0 were corrected in this way. In the other hand, other several plans with abnormally high rows estimations were also improved. Quantitatively, the cosine similarity between plan and actual rows increased from 0.90 to 0.95. Using this method makes it easier for the model to learn the mapping from actual cardinality to plan cardinality.

% \noindent
% {\bf Inference Complexity.} We assume that the average row count of the join result of every two joinable tables is $m$, and the sample rate is $\frac{1}{p}$, then the complexity of querying one lookup list is $\mathcal{O}(\frac{m}{p})$. Let $n$ denote the number of nodes in the execution tree, then the total complexity of the correction procedure is about $\mathcal{O}(\frac{m}{p}+n)$. The propagation time of the $bias$ ($\mathcal{O}(n)$) is negligible compared to the querying time.  Compared with calculating the sampling cardinalities of each merge node (approximately $\mathcal{O}(\frac{m}{p} \times \frac{n}{2})$), our proposed cardinality correction reduces the inference time by about $\mathcal{O}(\frac{n}{2})$ times.

% \noindent
% {\bf Remarks.} We note that the purpose of the correction is not to fully eliminate the inaccuracies. Instead, we aim to remove the disastrous estimates that would finally lead to a significant bias of the actual cost estimates. As shown in Figure~\ref{fig:plan-actual}, the corrected estimated cardinalities are much closer to the actual cardinalities than the vanilla ones. We find that such a correction procedure can already produce satisfactory performance when it is incorporated into our model introduced in Section~\ref{sec:modeldesign}.