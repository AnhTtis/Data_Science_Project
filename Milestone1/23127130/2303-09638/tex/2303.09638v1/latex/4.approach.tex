\section{Multi-Site rPPG Approach}
\label{sec:approach}
Our pipeline to perform remote pulse estimation on our MSPM dataset consists of preprocessing, signal extraction and pulse rate estimation. We applied three pulse estimation algorithms: a chrominace-based method using color difference signals (CHROM)~\cite{DeHaan2013}, plane-orthogonal-to-skin tone (POS)~\cite{Wang2017}, and a 3D-CNN deep learning method RPNet~\cite{Speth_CVIU_2021}. These three methods perform well for both stationary and moving subjects, which are characteristic of our MSPM dataset. The RPNet model was trained on face videos from the large and challenging deception detection and physiological monitoring dataset (DDPM)~\cite{Speth_IJCB_2021,Vance2022}. The model was not fine-tuned on any data from MSPM, so we tested how well the model could transfer to new subjects, lighting, standoff, and movement. Since the model was only trained on cropped faces, we also investigated its ability to transfer to spatially dissimilar regions such as the arms and legs. One of the benefits of CHROM and POS is that they do not have any spatial priors and only expect RGB traces from skin pixels.

\subsection{Preprocessing}
\subsubsection{ROI selection}\label{sec:ROI}
Given skin pixels including face, arms, and legs collected by MSPM, we selected these five different body parts as our ROIs. We applied Self-Correction Human Parsing (SCHP)~\cite{li2020self} for body segmentation to the eighty-seven subjects of MSPM. Frame (b) of Fig.~\ref{fig:frames} shows masks of the twenty parts from SCHP for a single frame. Masked skin pixels of each ROI were averaged and supplied to CHROM and POS for signal extractions. For each masked ROI, we also calculated the minimum and maximum values of $(x, y)$ locations of each mask to form bounding boxes for RPNet. Similarly, we created bounding boxes for palms using four key points of the palm generated by Mediapipe Hand detection~\cite{Zhang2020MPHands}. Frame (c) of Fig.~\ref{fig:frames} shows the bounding box for the palm.

\subsubsection{Human Key Points Detection}
To better investigate rPPG performance, we applied Mediapipe Pose detection~\cite{Bazarevsky2020} to detect key points of the human skeleton for local error estimation of the whole body (described later in Sec. \ref{sec:local}). There are thirty-three key points in total which define locations of many of the body's joints, including detailed locations for hands. 
Frame (d) of Fig.~\ref{fig:frames} shows an example of the skeleton detection results.

\subsection{Signal Extraction}
\subsubsection{Color Transformation Methods}

For each body part, we calculated spatial averages of skin pixels to reduce camera quantization error and generate 1D signals for each channel of RGB. CHROM~\cite{DeHaan2013} and POS~\cite{Wang2017} combined the signals of the three channels linearly to remove noise from movement or lighting to generate more robust pulse signals.

\input{tables/errors}

\subsubsection{Learning-Based Method}
We used the generated bounding box coordinates to crop ROIs from each frame for each body part and downsized these ROIs to 64x64 pixels using bicubic interpolation. The RPNet model we used was trained on the DDPM dataset~\cite{Speth_IJCB_2021,Vance2022} where the frame rate is 90 frames per second (fps), which is the same as our MSPM dataset. The trained RPNet model was fed video clips of 135 frames (1.5 seconds) as described in the original paper~\cite{Speth_CVIU_2021}.

\subsection{Filtering and Pulse Rate Calculation}
Remote pulse estimation over non-face regions is challenging, because of their lower signal-to-noise ratio than the face. To improve signal quality for all approaches, we applied a 4th order Butterworth bandpass filter with cutoff frequencies of 40 bpm and 180 bpm. Bandpass filtering was not used in the original POS and RPNet implementations, but we found the POS estimates in particular to contain high frequency noise before filtering. To compute the dominant pulsatile component in the rPPG waveforms, we applied the short-time Fourier transform (STFT) over a sliding 10-second window (900 frames in our videos), and selected the spectral peak as the pulse rate.

