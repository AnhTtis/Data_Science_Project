% CVPR Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage[accsupp]{axessibility}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Analyzing Effects of Mixed Sample Data Augmentation \\ on Model Interpretability}

\author{Soyoun Won, Sung-Ho Bae, Seong Tae Kim\\
Department of Computer Science and Engineering, Kyung Hee University\\
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Data augmentation strategies are actively used when training deep neural networks (DNNs). Recent studies suggest that they are effective at various tasks. However, the effect of data augmentation on DNNs' interpretability is not yet widely investigated. In this paper, we explore the relationship between interpretability and data augmentation strategy in which models are trained with different data augmentation methods and are evaluated in terms of interpretability. To quantify the interpretability, we devise three evaluation methods based on alignment with humans, faithfulness to the model, and the number of human-recognizable concepts in the model. Comprehensive experiments show that models trained with mixed sample data augmentation show lower interpretability, especially for CutMix and SaliencyMix augmentations. This new finding suggests that it is important to carefully adopt mixed sample data augmentation due to the impact on model interpretability, especially in mission-critical applications.

\end{abstract}

\section{Introduction}
\label{submission}


Data augmentation is proven to enhance the generalization ability of deep neural networks (DNNs). Modern data augmentation strategies often cut small regions \cite{devries2017cutout} or overlap more than one class of input to create augmented data \cite{uddin2021saliencyMix, Yun2019CutMix, zhang2018mixup}. Models trained with these methods achieve higher performance than the vanilla counterpart on various tasks such as image classification \cite{Yun2019CutMix, cao2022survey}, object recognition \cite{Dabouei2021objectRecognition, Harris2020objectRecognition}, semi-supervised learning \cite{berthelot2019semi}, and self-supervised learning \cite{ren2022self, kalantidis2020self}.  Furthermore, recent studies reveal that mixed sample data augmentation also improves the robustness of a model \cite{Yun2019CutMix, kim2020puzzleMix, uddin2021saliencyMix}. 
While mixed sample data augmentation strategies offer a lot of benefits, there is still an unsolved question. 
Do recent mixed sample data augmentation strategies make an impact on the model interpretability? 
We will address the question in this paper. 

DNNs achieved high performances in many fields. However, due to their black-box nature, it is difficult to understand their decision-making process. A wide range of efforts has been made to make DNN models reliable and interpretable for humans~\cite{selvaraju2017gradCAM, Schulz2020IBA, Wang2020scoreCAM, Zhang2021inputIBA, chen2019protoPnet, nauta2021protoTree, bau2017dissection, bau2020dissection, khakzar2021neural, khakzar2021explaining, Ahn2023LINe}. The popular approach includes feature attribution that assigns an importance score to input features, e.g., creating a heatmap that each pixel value represents an attribution score. Other lines of research include concept-based studies. It aims to give insight into the internal unit's role in recognizing human-perceptible concepts.

While augmentation strategies’ effect on model performance and robustness is widely studied and verified \cite{pinto2022robust, lee2020robust}, their effect on interpretability is still under the shadow. In this paper, we shed light on the subject, revealing that mixed sample augmentation is not an all-round player. 
Models trained with various augmentation methods learn different features, depending on the augmentation strategies even though they are trained upon the same architecture.
To investigate the interpretability of each model, we need a metric that allows comparison between different models. However, most of the previous interpretation studies focus on the evaluation of different interpretation methods on the same model. It is challenging to evaluate the interpretability of a model itself and compare the interpretability of different models. 

In this study, we define interpretability in terms of (1) alignment with humans, (2) faithfulness to the model, and (3) the number of human-recognizable concepts found in the inner units of the model. In particular, we propose a new model interpretability measure to assess different models quantitatively, supplementing the shortcomings of the existing interpretability metrics, i.e., deletion/insertion, which is restricted to the comparison of different feature attribution methods on the same models. We find that modern mixed sample augmentation strategies degrade model interpretability on all three probed criteria, and therefore might not be suitable for assisting humans in the field that utilizes DNNs in the decision-making process, e.g., medial AI for doctors.
    The main contribution of the paper is as follows:

\begin{enumerate}
\item{We present extensive experiments on models trained with mixed sample data augmentation strategies to evaluate their effect on model interpretability. }

\item{For evaluating model interpretability, we define three interpretability criteria. Building upon \cite{Samek2017deletion}, we propose new inter-model interpretability evaluation metrics: Inter-model deletion and Inter-model insertion.}

\item{We find that mixed sample data augmentation strategies have a negative influence on the model interpretability even though they boost model performance.}
\end{enumerate} 



\section{Preliminary}
\subsection{Advanced Data Augmentation Strategies} 

\begin{figure}[t]
\begin{center}
\scalebox{0.5}{
\centerline{\includegraphics[width=\textwidth]{figures/overview.pdf}}
}
\caption{An overview of the augmented image of Cutout, Mixup, CutMix, and SaliencyMix.}
\label{overview}
\end{center}
\end{figure}

In this study, we explore how mixed sample augmentation influences interpretability. To this end, we analyze popular mix-based augmentation strategies: Mixup \cite{zhang2018mixup} and CutMix \cite{Yun2019CutMix}. We additionally inspect Cutout \cite{devries2017cutout} and SaliencyMix \cite{uddin2021saliencyMix} to understand the effect of CutMix-based augmentation further, as CutMix can be seen as a combination of Cutout and Mixup, and SaliencyMix introduces ``saliency guidance" to CutMix. 
An overview of each method is shown in \cref{overview}. 



\textbf{Cutout} drops fixed-size square-shaped regions from a randomly chosen position at input space. Dropped regions are filled with zero. Cutout is interpreted as regional dropout, which is a subfield of the regularization techniques. 
Regional dropout randomly removes continuous regions from an image or feature space. The main difference between another well-known regional dropout method such as \cite{zhong2020randomErasing} is that Cutout erases a fixed-size box. 


\textbf{Mixup} combines two randomly chosen images into one input image by linear interpolation \cite{zhang2018mixup}. The mix ratio is drawn from the beta distribution. Labels are determined by the proportion of each class in a new augmented image. Mixup is head of various mix-based augmentation strategies \cite{mai2021metaMixup, verma2019manifoldMixup, choi2022tokenMixup, yin2021batchMixup}. 

\textbf{CutMix} is inspired by Cutout and Mixup \cite{Yun2019CutMix}. It drops small square regions of an image like Cutout, and fills the regions with another randomly chosen image, like Mixup. The mix ratio is sampled from the beta distribution, and the label is modified as a proportion of the augmented image as in Mixup. 


\textbf{SaliencyMix} builds upon CutMix, but the fundamental difference is that filled regions are selected carefully so that “salient” regions are to be mixed \cite{uddin2021saliencyMix}. Salient regions are selected using the algorithm proposed in ~\cite{Montabone2010}. The mixed patch is positioned so that the salient region is attached to the corresponding position of the target image. 


The classification performance (top-1 error) of architecture ResNet-50 on ImageNet is as follows: SaliencyMix (21.26\%), CutMix (21.40\%), Mixup (22.58\%), Cutout (22.93\%), and baseline (23.68\%). 

\subsection{Feature Attribution}
Feature attribution is a post-hoc explanation method that assigns importance scores to input features that contribute to the model’s output. It is an important tool for understanding the inner decision-making process of black-box DNNs. Especially for DNNs that assist humans in the field, neural networks should be able to explain themselves. A survey on explainable AI suggests that participants are more likely to accept the model with lower performance if they present their output with an explanation (e.g., an attribution map), particularly at high-risk tasks \cite{Kim2022hive}. Extensive research has been made to visualize DNNs' decision-making reasons. There are several approaches, including activation-based methods and perturbation-based methods. 

\textbf {Activation-based Methods} exploit activation maps to assign importance scores to each pixel in the input space \cite{zhou2016learning,selvaraju2017gradCAM,chattopadhay2018grad}. The weighted average of activation maps is typically calculated in the last convolutional layer utilizing their weights or gradients. We use GradCAM \cite{selvaraju2017gradCAM} as a representative of the activation-based method. It is widely used to provide a visual explanation of DNNs \cite{jin2020gcuse, zhang2020gcuse, ozturk2020gcuse}. GradCAM makes use of the gradient of the model score against the target class to weight the activation. 

\textbf{Perturbation-based Methods} calculate weights by observing the change in the model score against input perturbation \cite{Fong2017mp,fong2019understanding,kim2021robust,Schulz2020IBA}. Perturbation is often done by adding occlusions/noise or optimizing the mask that affects the model performance the most/the least. We utilize Information Bottlenecks for Attribution (IBA) which adopts information theory, adding noise to latent feature maps to assign importance scores \cite{Schulz2020IBA}.



\section{Related Work}
\subsection{Robustness and Interpretability}
Other lines of work studied the relationship between adversarial robustness and interpretability of the model. ~\cite{etmann2019robust} found that adversarially robust models exhibit more interpretable behavior (i.e., their attribution maps are more alike with the input). ~\cite{wang2022robust} further explains this observation by considering the decision boundary. They showed that robust models have more smooth decision boundaries, and therefore are more interpretable. 

However, ~\cite{etmann2019robust} and ~\cite{wang2022robust} defined interpretability as the similarity between an input image and the attribution map. In other words, if the visual pattern of the saliency map resembles the input image, it is considered interpretable. However, this definition of interpretability is very limited and  an entirely different concept from our work. Our study defines interpretability in three criteria that are more general - alignment with humans, faithfulness to the model, and the number of human recognizable concepts found in the inner units of the model. The first two criteria utilize the evaluation of the feature attribution method, which we address in the next subsection. 

\subsection{Evaluation of Feature Attribution}
A wide range of research has been made to make DNNs understandable by humans \cite{arrieta2020explainable, bau2017dissection}. Feature attribution method is actively studied to achieve this goal \cite{selvaraju2017gradCAM, Wang2020scoreCAM, Schulz2020IBA, Zhang2021inputIBA}. However, the evaluation of the feature attribution method is not a simple task because there is no existing ground truth \cite{Rao2022}. Naturally, another line of research has been made to quantify the interpretability of attribution maps \cite{Samek2017deletion, Zhang2021inputIBA}. The output of a feature attribution method, a heatmap, is usually evaluated with other heatmaps created by different methods on the same model \cite{Wang2020scoreCAM, Schulz2020IBA}. State-of-the-art attribution methods aim to improve the interpretability of a model by using more elaborate ways to extract important features \cite{Zhang2021inputIBA, Wang2020scoreCAM}. This is related to our work as we use attribution maps to quantify interpretability. However, our study focuses on the comparison of different models in terms of interpretability. 



\subsection{Bonus Effect of Data Augmentation}
\textbf{Calibration} ~\cite{Thulasidasan2019mixupCalibration} studied the overlooked effect of Mixup: their effect on calibration. They found that DNNs trained with Mixup outputs softmax scores that are significantly closer to the actual likelihood than the vanilla DNNs. ~\cite{zhang2022mixupCalibration} provided theoretical proof for this discovery. Also, ~\cite{chun2020calibration} observed better calibration at Cutout and CutMix-trained models. 

\textbf{Localization} ~\cite{Yun2019CutMix} reported weakly supervised object localization (WSOL) score enhancement to models trained with Cutout and CutMix. While our works include localization measures, we focus on capturing the alignment with humans, rather than the model’s localization performance for better measuring the interpretability of models. 



\section{Evaluation of Model Interpretability}
In this study, we define interpretability using three different criteria - Human-Model alignment, Faithfulness, and Human-recognizable concepts. Human-model alignment and faithfulness aim to evaluate the interpretability of the model when the explanation behind the prediction of the model is given by the attribution maps. Human-recognizable concepts evaluate how the model is interpretable when we look at the internal units of the model. The more interpretable model provides more disentangled concepts for each unit \cite{bau2017dissection}. 

\textbf{Human-Model Alignment} is designed to evaluate the extent to which attribution maps align with the object-bounding boxes annotated by humans. Weakly supervised object localization (WSOL) has been used to report localization performance in the literature regarding augmentation strategies \cite{Yun2019CutMix}, measuring the overlap between the ground truth object bounding box and estimated bounding box which is obtained through attribution map (usually class activation mapping \cite{zhou2016learning}) after thresholding.
However, WSOL task only considers the shape of the attribution map, but not the value of each pixel point. 
For example, consider two attribution maps that have the same silhouette (\cref{wsol-weakness}). One attributes high values on the object and low values on the near-object background, and the other attributes low values on the object and high values on the near-object background. These two heatmaps clearly tell different things, but the same bounding box will be estimated in WSOL. Therefore, WSOL task is not appropriate when evaluating interpretability - the alignment between the human-annotation and feature attribution map of the model. Therefore, we use two localization metrics - Energy-based Pointing Game \cite{Wang2020scoreCAM} and Effective Hit Ratios \cite{Zhang2021inputIBA} - to measure the alignment in this study. 

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/wsol.pdf}}
\caption{Two different heatmaps after clipping where their shape and size are the same. The green box denotes the estimated bounding box from the attribution maps' silhouette. Left: higher attribution at the center, lower attribution at the edge. Right: higher attribution at the edge, lower attribution at the center. Nonetheless, the same bounding box is used for evaluation in WSOL. }
\label{wsol-weakness}
\end{center}
\vskip -0.2in
\end{figure}



\textbf{Model Faithfulness} refers to the ability to reflect whether the feature attribution correctly represents the importance of the features on the model output. The goal of feature attribution methods is to explain the underlying reason for the model decision. Therefore, it should rely on the model, rather than be plausible to humans (e.g., it should avoid merely highlighting the edge of an object regardless of the real evidence behind the model's decision \cite{Adebayo2018sanityCheck}). Faithfulness is often measured using deletion/insertion scores, which observe the model’s score change after removing small regions from the original input \cite{Fong2017mp,Samek2017deletion,Schulz2020IBA,Zhang2021inputIBA}. The underlying idea is that the more attributed region will affect the model's decision significantly, and the less attributed region will have the opposite effect. We evaluate models trained from different augmentation strategies on deletion and insertion scores. To this end, we propose a new deletion/insertion scheme that enables comparison between different models.   



\textbf{Human-understandable Concepts} are defined using Network Dissection \cite{bau2017dissection, bau2020dissection}. One can quantify the interpretability of different models by understanding the inner units’ roles~\cite{khakzar2021towards}. Individual units can detect concepts, although these concepts are not annotated in the training data. For example, units emerge that detect a single concept such as “desk”, “computer”, or “keyboard” when the input is simply provided as “office”. The main idea of Network Dissection is to quantify the disentangled representations learned by individual units of the network. “Concepts” consists of high-leveled ones (e.g., objects) and low-leveled ones (e.g., colors). Network Dissection scans the entire dataset, unit (neuron), and predefined concepts to find detector units. A unit is defined to be a detector of a specific concept if the overlap of the activation of the unit and segmentation annotation of the concept exceeds the threshold. One can understand the role of the inner units by looking at the concepts detected by the units. 


\section{Experiments}

\subsection{Experimental Setup}

We introduce the experimental setup used in this study. Experiments are conducted on a large-scale object recognition dataset: ImageNet \cite{russakovsky2015imagenet}. We randomly sample 1,000 images from the validation set. In \cref{alignment} and \cref{faithfulness}, all samples are chosen in a way that the output score is more than 0.6 for all five models, and the total size of objects’ ground truth bounding box or boxes take less than 50\% and more than 10\% of the original input size for discrimination. We utilize GradCAM and IBA to obtain the attribution map. $\beta$ = 10 is used for IBA. 


We use models released by ~\cite{Yun2019CutMix} and ~\cite{uddin2021saliencyMix}. All models share the base structure of ResNet-50 \cite{he2016deep}. 
Models are trained with a batch size of 256, and for 300 epochs with weight decay - the initial learning rate is set to 0.1 and later decayed by the factor of 0.1 at three epoch steps (75, 150, and 225). 
The baseline is trained with traditional data augmentation strategies such as flipping, cropping, and resizing.

\subsection{Human-Model Alignment}
\label{alignment}

\begin{table*}[t]
\caption{Alignment score of five models, trained with different augmentation strategies, respectively. The average and the standard error are reported in the table. The baseline model exhibits the best performance for all measures and attribution methods.}
\vskip 0.15in
\label{tab:alignment}
\centering
\scalebox{0.98}{
\begin{tabular}{llllll}
\toprule
        & \multicolumn{2}{c}{GradCAM} & \multicolumn{2}{c}{IBA}  \\

\cmidrule(lr){2-3} \cmidrule(lr){4-5}

\rule {0pt}{2ex} & EnergyPG & EHR & EnergyPG & EHR  \\

\midrule
Baseline & \bf0.566$\pm$ 0.005   & \bf0.486 $\pm$ 0.005 &  \bf0.645 $\pm$ 0.006  & \bf0.453 $\pm$ 0.004  \\
Cutout   & 0.544$\pm$ 0.005      & 0.471 $\pm$ 0.005    &  0.575    $\pm$ 0.006  & 0.417 $\pm$ 0.004  \\
Mixup    & 0.541$\pm$ 0.005      & 0.479 $\pm$ 0.005    &  0.619    $\pm$ 0.006  & 0.412   $\pm$ 0.005   \\
CutMix   & 0.528$\pm$ 0.005      & 0.466 $\pm$ 0.005    &  0.570    $\pm$ 0.006  & 0.417   $\pm$ 0.004  \\ 
SaliencyMix  & 0.524$\pm$ 0.005   & 0.467 $\pm$ 0.005    &  0.561    $\pm$ 0.006 & 0.413  $\pm$ 0.004   \\ 

\bottomrule
\end{tabular}
}
\end{table*}



\begin{figure*}[ht]
    \centering
    \includegraphics[width=17.1cm]{figures/qualitative_assess.pdf}
     \caption{Qualitative comparison of Baseline, Cutout, Mixup, CutMix, and SaliencyMix for EnergyPG and WSOL on ImageNet using GradCAM and IBA. Ground truth annotations are depicted as red boxes. \emph{bounding box} and \emph{whole} represent the energy inside of the bounding box and the whole image respectively. \emph{Gain ratio} denotes the amount of increased energy inside the bounding box divided by the amount of increased energy of the total attribution map. The threshold of WSOL is set to 0.15 as in ~\cite{Yun2019CutMix}.}
    \label{fig:qualitative}
\end{figure*}

WSOL task requires a binarized attribution map based on the pre-defined threshold to create a  mask. Then, the smallest box that includes the whole mask region is estimated. Finally, the Intersection over union (IoU) between the estimated box and ground truth object bounding box is measured. 
However, this approach is unsuitable for evaluating feature attribution maps due to the following reasons. Firstly, it requires a synthetically estimated box that adds bias to the original heatmap. Secondly, WSOL task does not consider each pixel value (i.e., the importance of the feature) within the heatmap.  Thirdly, it is sensitive to the threshold parameter. Therefore, we consider the Energy-based Pointing Game (energyPG) and Effective Hit Ratio (EHR) to alleviate the aforementioned problems. These two metrics are synthetic box-free and threshold-free metrics that make use of every pixel value in the calculation. 

\textbf{Energy-based Pointing Game (EnergyPG)}~\cite{Wang2020scoreCAM} proposed EnergyPG that measures the proportion of energy (i.e., attribution) contained in the ground-truth object bounding box over the whole energy of the attribution map. $L^c$ denotes the attribution map for class $c$. 

\begin{equation}
Score_{EPG} = \frac{\sum{L^c_{(i,j)\in bbox}}}{\sum{L^c_{(i,j)\in bbox}} + \sum{L^c_{(i,j)\notin bbox}}}
\end{equation}


\textbf{Effective Heat Ratio (EHR)} ~\cite{Zhang2021inputIBA} proposed EHR which calculates the energy in the ground-truth bounding box divided by the total number of pixels above multiple quantiles of thresholds, which are values between 0.0 to 1.0. Then, a plot representing EHR  score at each threshold is obtained. Finally, the area under the curve (AUC) is computed. Let $S^{c,\lambda}_{(i,j)} = (L^c_{(i,j)} > \lambda)$ where $\lambda$ denotes a threshold. 

\begin{equation}
Score_{EHR} = \sum_{\lambda}\Biggr[{\frac{\sum{L^c_{(i,j)\in bbox}}}{\sum{S^{c,\lambda}_{(i,j)\in bbox}} + \sum{S^{c,\lambda}_{(i,j)\notin bbox}}}}\Biggr]
\end{equation}





Experimental results on human-model alignment are given in \cref{tab:alignment}. To our surprise, some data augmentation strategies that showed better performance on WSOL (Cutout and CutMix \cite{Yun2019CutMix}), showed the opposite results in our human-model alignment experiments. Attribution maps calculated from baseline ResNet-50 scored the highest alignment with human-drawn bounding boxes. This does not indicate that the mixed sample data augmentation techniques degrade localization ability. 
Rather, it shows that the features used importantly for the prediction of models are less consistent with that of humans when the model is trained with the augmentation strategy.
CutMix and SaliencyMix consistently score last or second last except for the IBA-EHR experiment, where the score difference between the four augmentation methods is marginal. 


Qualitative analysis at \cref{fig:qualitative} shows wider regions of the feature attribution map from models trained with augmented data. However, this does not directly lead to better human-model alignment performance. While they assign more importance to broader areas, attributions inside the bounding box do not increase as much as the total attributions from the entire heatmap. In other words, models trained with the augmentation method attribute more energy to the object, but, at the same time, they assign higher importance scores to non-relevant regions. This leads to relatively small energy in the bounding box and large energy outside the bounding box. \cref{fig:qualitative} also depicts limitations of WSOL. Intersection over union calculation of WSOL favors attribution maps with broad silhouettes, ignoring each pixel value. More qualitative analysis is provided in the appendix. 




\subsection{Model Faithfulness}
\label{faithfulness}


\begin{figure*}[t]
    \centering
    \includegraphics[width=17cm]{figures/RaO.pdf}
     \caption{Faithfulness evaluated on the Baseline, Cutout, Mixup, CutMix, and SaliencyMix using GradCAM. Normalized model score represents the predicted probability of the top-1 class normalized to the scale of 0.0 to 1.0. 
     Deletion and insertion curves from IBA are provided in the appendix.
     \textbf{Top}: Subtraction of Random Order deletion (RaO deletion) from the Least Relevant First deletion (LeRF deletion) curve. (a) LeRF deletion curve. Faithfulness and robustness are entangled. (b) RaO deletion curve. Robustness is represented. (c) Inter-model deletion curve. It is obtained by subtracting (b) from (a) to reduce the effect of robustness and leave faithfulness. The area under the curve in (c) is measured for the faithfulness score. Higher is better. The exact measure is calculated in \cref{tab:LeRF-RaO}.
     \textbf{Bottom}: Subtraction of RaO insertion curve from MoRF insertion curve. (d) MoRF insertion curve. (e) RaO insertion curve. (f) Inter-model insertion curve. It is obtained by subtracting (e) from (d). The area under the curve (f) is measured for the faithfulness score. Higher is better. The exact measure is provided in \cref{tab:MoRF-RaO}.}
    \label{fig:LeRF-RaO}
\end{figure*}


\begin{table}[t]
\caption{\footnotesize The area under inter-model deletion curve of baseline model and models trained with augmentation strategies with different attribution methods. Higher is better. The standard error is averaged over each deletion step. }
\centering
\tabcolsep=0.35cm

\begin{tabular}{@{}lccc@{}}
\toprule
            & \multicolumn{2}{c}{Inter-Model Deletion} \\
\cmidrule(lr){2-3}
\rule{0pt}{2ex} & GradCAM & IBA \\ 
\midrule
Baseline  & 67.416 $\pm$ 0.0079           & 69.750  $\pm$ 0.0082\\
Cutout    & \textbf{71.411} $\pm$ 0.0072  & \textbf{73.679} $\pm$ 0.0072\\
Mixup     & 65.115 $\pm$ 0.0079            & 68.673 $\pm$ 0.0083\\
CutMix    & 63.908 $\pm$ 0.0072            & 67.890 $\pm$ 0.0074\\
SaliencyMix    & 64.711 $\pm$ 0.0070      &  68.348 $\pm$ 0.0071\\ \bottomrule
\end{tabular}


\label{tab:LeRF-RaO}
\end{table}

\textbf{Deletion} replaces $n \times n$ size grid regions in an input image with a constant value (e.g., an average pixel value) iteratively until the entire grid is replaced. Replacing order is determined by their importance rank. Given an attribution map, every region is divided into grids of the same size $n \times n$. Then, each grid is ranked by the sum of attributions in them. As grids in the input image are gradually substituted with a constant value, and the new input is fed to the network, the output score shall drop step by step. By observing the score drop curve, one can quantify the faithfulness of the attribution map against the model. 


We degrade an input image starting from the least relevant (i.e., the smallest sum of attribution) grid. 
Originally, removing the most relevant grid first (MoRF) is proposed in the literature \cite{Samek2017deletion}. However, ~\cite{Ancona2017LeRF} found that the grid that makes the model score drop the most rapidly is not necessarily an important grid. They proposed to reverse the removing order - remove the least important grid first (LeRF). LeRF expects the model score to drop gradually at first, and steeper through iteration. 

We plot our LeRF experiment in \cref{fig:LeRF-RaO}a. Grid size is set to $7 \times 7$. In our LeRF experiment, a slow decrease at early iteration steps, and a sharp decrease at later steps in the model's score tell two different things in two different subjects. The first category is the interpretability of the attribution method. It suggests that the attribution method is faithful to the model. Since removing the region where the attribution method pointed as redundant caused little change in the model score, it can be interpreted that the model did not take into account the region when making the decision. The second category is the robustness of the model. It suggests the model is robust. This is because removing small parts of the original input did not affect the model much, meaning that the model is robust to small occlusions. Reversely, the output score's steep drop at early steps and gradual drop at later steps can be interpreted as less faithful, or more sensitive. 

As so, it is not possible to separate the faithfulness and robustness in the deletion experiment. This is not a problem when the task is to compare the faithfulness of different attribution methods on the \emph{same} model. However, what we are doing is quantifying the faithfulness of \emph{different} models (different parameter set due to various augmentation schemes on the same architecture) on the same attribution method. Therefore, to analyze faithfulness only, we conduct an additional procedure - selecting the replaced grid randomly in the deletion experiment (RaO - Random Order) - depicted in \cref{fig:LeRF-RaO}b. We repeat five different random orders per image to alleviate the impact of randomness. The rest of the settings are the same as LeRF. RaO tells only about robustness because the grids do not contain any information about the relevance scores created by the attribution map. Then, we obtain a new plot that the effect of robustness is removed by subtracting RaO from LeRF (\cref{fig:LeRF-RaO}c). We call this \emph{inter-model deletion}. Lastly, the area under the curve (AUC) is calculated for the final faithfulness score. The process is depicted in \cref{fig:LeRF-RaO}. We only plot the process curve on GradCAM because IBA follows the same order and tendency. A detailed explanation of the attribution map created by IBA is provided in the appendix. 

The experimental result of inter-model deletion is given in \cref{tab:LeRF-RaO}. Mixup, CutMix, and SaliencyMix scored lower than baseline, and Cutout scored higher than baseline. High scores at Cutout are natural because augmented images from cutout are somewhat similar to the alternated images used during the deletion experiment. A model trained with Cutout is more exposed to images where certain square regions are deleted (cut-outed). It reminiscences the deletion process done in the experiment. The big score drops are observed in CutMix and SaliencyMix.

\begin{table}[t]
\caption{\footnotesize The area under inter-model insertion of baseline model and models trained with augmentation strategies with different attribution methods. Higher is better. The standard error is averaged over each insertion step.}
\centering
\tabcolsep=0.35cm
{
\begin{tabular}{@{}lccc@{}}

\toprule
            & \multicolumn{2}{c}{Inter-Model Insertion} \\
\cmidrule(lr){2-3}
\rule{0pt}{2ex} & GradCAM & IBA \\ 
\midrule
Baseline  & 67.044 $\pm$ 0.0079     & 69.321 $\pm$ 0.0082  \\
Cutout    & \textbf{71.274} $\pm$ 0.0073      & \textbf{73.595} $\pm$ 0.0073 \\
Mixup     & 65.000 $\pm$ 0.0079      & 68.679 $\pm$ 0.0083 \\
CutMix    & 63.625 $\pm$ 0.0073     & 67.276 $\pm$ 0.0075 \\
SaliencyMix    & 64.443 $\pm$ 0.0070       & 68.079 $\pm$ 0.0071 \\ \bottomrule
\end{tabular}
}

\label{tab:MoRF-RaO}
\end{table}


Reverting the grid deletion process, we conduct \textbf{insertion} experiments. It also divides the input image into $7 \times 7$ grids and ranks them by the sum of attribution. The difference from the deletion experiment is that the image is restored rather than degraded at each iteration. The insertion experiment requires an image starting from a constant value. Then, a grid from the original image is added, and the model score is measured iteratively until the whole image is reconstructed. Therefore, reversing the order, we obtain \emph{inter-model insertion} score by subtracting RaO insertion curve from MoRF insertion curve to measure faithfulness. The experimental result of inter-model insertion is given in \cref{tab:MoRF-RaO}. A similar tendency to \cref{tab:LeRF-RaO} is observed. CutMix and SaliencyMix dropped the faithfulness score significantly in both deletion and insertion experiments. 



\subsection{Concept-Based Study}

Network dissection \cite{bau2017dissection, bau2020dissection} aims to find disentangled concepts detected by individual units (neurons). It evaluates every unit's activation map for every image in the entire dataset. Firstly, an activation map $A_{k}$ of unit $k$ is upscaled by $S_{k}$ to compare it with the input resolution mask for concept $c$, $L_{c}$. Then, a binary mask $M_{k}$ is created by applying threshold $T_k$ so that only activation above the threshold is set to 1 and others to 0. $T_k$ is determined by the distribution of a unit’s activation $a_k$ that satisfies $P(a_{k} > T_k) = 0.01$. Finally, the intersection over union (IoU) $IoU_{k,c}$ of $M_{k}$ and $L_{c}$ is calculated. $IoU_{k,c}$ refers to the accuracy of unit $k$ in detecting concept $c$. Unit $k$ is defined to be a detector unit of concept $c$ if $IoU_{k,c}$ exceeds a threshold. 

\begin{equation}
IoU_{k,c} = \frac{\sum | M_k(\textbf{x})\cap L_c(\textbf{x})|}{\sum | M_k(\textbf{x})\cup L_c(\textbf{x})|}
\end{equation}

In our experiment, we set IoU threshold to 0.04 (i.e., a unit is a detector unit for concept c if $IoU_{k,c} > 0.04$). We divide concepts into four categories - object, part, material, and color. We use the segmentation model UPerNet (Unified Perceptual Parsing Network) \cite{xiao2018unified} trained on the ADE20K dataset \cite{zhou2017scene}. The segmentation model has trained to segment predefined concepts. We evaluate five models trained with ImageNet and probe the final convolutional layer to count the number of unique concepts detected by the detector unit. 


\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/dissection.pdf}}
\caption{The number of unique concepts of the baseline and models trained with Cutout, Mixup, CutMix, and SaliencyMix at the last convolutional layer. }
\label{dissection}
\end{center}

\end{figure}


The experiment result is shown in \cref{dissection}. 
The number of human-recognizable concepts is decreased in augmented models. Cutout and Mixup increase only the number of color detectors of all categories. For CutMix, more object detectors are found, but fewer detectors are observed with all other categories. The least number of detectors are found in SaliencyMix in every category. 
The interesting thing is that the number of object detectors also decreased on models trained with the Cutout and SaliencyMix. 
It is known that the model classification score is positively correlated with the number of unique object detectors (i.e., the higher the classification score, the more unique object detectors are found). Therefore, the reasonable assumption is that the greatest number of object detectors will be found at SaliencyMix (because SaliencyMix provides the best classification accuracy on ImageNet-trained ResNet-50). However, despite the performance gain, models trained with SaliencyMix have the fewest number of object detectors, presenting the smallest degree of disentanglement.

The decrease of concept detectors in models trained with augmentation strategies despite the performance gain supports strong ground for our experiment result on the attribution map: mixed sample data augmentation strategies are not a panacea - they boost model performance on various tasks but degrade interpretability. 

\section{Discussion and Conclusion}
In this paper, we analyzed the unexplored side of mixed data augmentation methodologies: their effect on interpretability. We defined the term “interpretability” at the aspect of post-hoc explanation, i.e., feature attribution and model disentanglement. More specifically, the quality of feature attribution is evaluated on human-model alignment and model faithfulness through localization performance, and inter-model deletion/insertion experiment respectively. Model disentanglement is compared through the number of disentangled concepts. Throughout the experiments, we observed the degradation of interpretability in models trained with mixed sample data augmentation strategies, especially CutMix and SaliencyMix. In the first section of our experiment, models trained with advanced data augmentation techniques showed sparse attribution maps. They assign more importance scores to overall regions of the input, but not so much to the critical parts. This tendency brings misalignment with the ground-truth bounding box. In the next section of the experiment, we found that less faithful attribution maps are created from models trained with CutMix-based augmentation techniques. We hypothesize that augmented images change the data distribution \cite{cao2022survey}, resulting in a degradation of interpretability. In the last section, we observed a decrease in the number of detected disentangled concepts when models are trained with advanced augmentation strategies. Data combining strategy in mix-based methods potentially hinders the emergence of neurons that detect disentangled concepts. 

However, our experimental results do \emph{not} suggest that researchers should stop using mixed sample data augmentation strategies. Rather, our work encourages the community to look at aspects that were overlooked before - their effect on interpretability. This is especially important for some applications where interpretation is required, such as medical AI. The medical domain is where providing an explanation of the model’s decision-making process is important. At the same time, it is one of the fields where large-scale data is hard to obtain \cite{chlap2021medicalReview}. DNNs are data-deprived \cite{mahajan2018exploring}, and data shortage can be alleviated by data augmentation. Therefore, the community needs powerful data augmentation strategies that achieve high performance without affecting a negative influence on interpretability. 




\section{Limitation}
In this paper, we conducted experiments only on vision data. The analysis of textual, audio, and multimodal data is an important subject for future research. 

This study focuses on intrinsic (automatic) interpretability measures, which enable fair and objective evaluation. However, extrinsic evaluation (e.g., a survey through Amazon Mechanical Turk) which asks humans for their opinion on the provided explanation will also be an important line of interpretability assessment \cite{Kim2022hive,Nguyen2021effectiveness} because the ultimate goal of explainable AI is to help human users, e.g., model debugging or bias detecting for researchers, and trust building for common users 


\section*{Acknowledgements}
This work was supported in part by the Institute of Information and Communications Technology Planning and Evaluation (IITP) grant funded by the Korea Government (MSIT)(No. 2022-0-00078: Explainable Logical Reasoning for Medical Knowledge Generation, No. 2021-0-02068: Artificial Intelligence Innovation Hub, No. RS-2022-00155911: Artificial Intelligence Convergence Innovation Human Resources Development (Kyung Hee University)) and by the National Research Foundation of Korea (NRF) grant funded by the Korea government(MSIT) (No. 2021R1G1A1094990).

\nocite{langley00}

\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{Adebayo2018sanityCheck}
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt,
  and Been Kim.
\newblock Sanity checks for saliency maps.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem{Ancona2017LeRF}
Marco Ancona, Enea Ceolini, Cengiz Ã–ztireli, and Markus Gross.
\newblock A unified view of gradient-based attribution methods for deep neural
  networks.
\newblock In {\em NIPS Deep Learning Workshop on Interpreting, Explaining and
  Visualizing}, 2017.

\bibitem{arrieta2020explainable}
Alejandro~Barredo Arrieta, Natalia D{\'\i}az-Rodr{\'\i}guez, Javier Del~Ser,
  Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garc{\'\i}a, Sergio
  Gil-L{\'o}pez, Daniel Molina, Richard Benjamins, et~al.
\newblock Explainable artificial intelligence (xai): Concepts, taxonomies,
  opportunities and challenges toward responsible ai.
\newblock {\em Information fusion}, 58:82--115, 2020.

\bibitem{bau2017dissection}
David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba.
\newblock Network dissection: Quantifying interpretability of deep visual
  representations.
\newblock In {\em Computer Vision and Pattern Recognition}, 2017.

\bibitem{bau2020dissection}
David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and
  Antonio Torralba.
\newblock Understanding the role of individual units in a deep neural network.
\newblock {\em Proceedings of the National Academy of Sciences}, 2020.

\bibitem{khakzar2021explaining}
Ashkan Khakzar, Yang Zhang, Wejdene Mansour, Yuezhi Cai, Yawei Li, Yucheng Zhang, Seong Tae Kim, and Nassir Navab.
\newblock Explaining COVID-19 and Thoracic Pathology Model Predictions by Identifying Informative Input Features.
\newblock {\em International Conference on Medical Image Computing and Computer Assisted Intervention}, pages 391--401, 2021.

\bibitem{berthelot2019semi}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
  Oliver, and Colin~A Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{cao2022survey}
Chengtai Cao, Fan Zhou, Yurou Dai, and Jianping Wang.
\newblock A survey of mix-based data augmentation: Taxonomy, methods,
  applications, and explainability.
\newblock {\em arXiv e-prints}, pages arXiv--2212, 2022.

\bibitem{chattopadhay2018grad}
Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth~N
  Balasubramanian.
\newblock Grad-cam++: Generalized gradient-based visual explanations for deep
  convolutional networks.
\newblock In {\em IEEE Winter Conference on Applications of Computer
  Vision}, pages 839--847, 2018.

\bibitem{chen2019protoPnet}
Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and
  Jonathan~K Su.
\newblock This looks like that: deep learning for interpretable image
  recognition.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{chlap2021medicalReview}
Phillip Chlap, Hang Min, Nym Vandenberg, Jason Dowling, Lois Holloway, and
  Annette Haworth.
\newblock A review of medical image data augmentation techniques for deep
  learning applications.
\newblock {\em Journal of Medical Imaging and Radiation Oncology},
  65(5):545--563, 2021.

\bibitem{choi2022tokenMixup}
Hyeong~Kyu Choi, Joonmyung Choi, and Hyunwoo~J Kim.
\newblock Tokenmixup: Efficient attention-guided token-level data augmentation
  for transformers.
\newblock {\em Conference on Neural Information Processing Systems (NeurIPS)},
  2022.

\bibitem{chun2020calibration}
Sanghyuk Chun, Seong~Joon Oh, Sangdoo Yun, Dongyoon Han, Junsuk Choe, and
  Youngjoon Yoo.
\newblock An empirical evaluation on robustness and uncertainty of
  regularization methods.
\newblock {\em ICML Workshop on Uncertainty and Robustness in Deep Learning},
  2019.

\bibitem{Dabouei2021objectRecognition}
Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, and Nasser~M. Nasrabadi.
\newblock Supermix: Supervising the mixing data augmentation.
\newblock In {\em IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13794--13803, 2021.

\bibitem{devries2017cutout}
Terrance DeVries and Graham~W Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock {\em arXiv preprint arXiv:1708.04552}, 2017.

\bibitem{etmann2019robust}
Christian Etmann, Sebastian Lunz, Peter Maass, and Carola Schoenlieb.
\newblock On the connection between adversarial robustness and saliency map
  interpretability.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of {\em Proceedings of Machine Learning Research}, pages
  1823--1832. PMLR, 09--15 Jun 2019.

\bibitem{fong2019understanding}
Ruth Fong, Mandela Patrick, and Andrea Vedaldi.
\newblock Understanding deep networks via extremal perturbations and smooth
  masks.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 2950--2958, 2019.

\bibitem{Fong2017mp}
Ruth~C. Fong and Andrea Vedaldi.
\newblock Interpretable explanations of black boxes by meaningful perturbation.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, 2017.

\bibitem{Harris2020objectRecognition}
Ethan Harris, Antonia Marcu, Matthew Painter, Mahesan Niranjan, Adam
  Pr{\"u}gel-Bennett, and Jonathon Hare.
\newblock Fmix: Enhancing mixed sample data augmentation.
\newblock {\em arXiv preprint arXiv:2002.12047}, 2020.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 770--778, 2016.

\bibitem{jin2020gcuse}
Cheng Jin, Weixiang Chen, Yukun Cao, Zhanwei Xu, Zimeng Tan, Xin Zhang, Lei
  Deng, Chuansheng Zheng, Jie Zhou, Heshui Shi, et~al.
\newblock Development and evaluation of an artificial intelligence system for
  covid-19 diagnosis.
\newblock {\em Nature communications}, 11(1):1--14, 2020.

\bibitem{kalantidis2020self}
Yannis Kalantidis, Mert~Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and
  Diane Larlus.
\newblock Hard negative mixing for contrastive learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:21798--21809, 2020.

\bibitem{kim2021robust}
Junho Kim, Seongyeop Kim, Seong~Tae Kim, and Yong~Man Ro.
\newblock Robust perturbation for visual explanation: Cross-checking mask
  optimization to avoid class distortion.
\newblock {\em IEEE Transactions on Image Processing}, 31:301--313, 2021.

\bibitem{kim2020puzzleMix}
Jang-Hyun Kim, Wonho Choo, and Hyun~Oh Song.
\newblock Puzzle mix: Exploiting saliency and local statistics for optimal
  mixup.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, volume 119, pages 5275--5285, 2020.

\bibitem{Kim2022hive}
Sunnie~SY Kim, Nicole Meister, Vikram~V Ramaswamy, Ruth Fong, and Olga
  Russakovsky.
\newblock Hive: evaluating the human interpretability of visual explanations.
\newblock In {\em European Conference on Computer Vision}, pages 280--298.
  Springer, 2022.

\bibitem{langley00}
P. Langley.
\newblock Crafting papers on machine learning.
\newblock In Pat Langley, editor, {\em Proceedings of the 17th International
  Conference on Machine Learning (ICML 2000)}, pages 1207--1216, 2000.

\bibitem{lee2020robust}
Saehyung Lee, Hyungyu Lee, and Sungroh Yoon.
\newblock Adversarial vertex mixup: Toward better adversarially robust
  generalization.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 272--281, 2020.

\bibitem{mahajan2018exploring}
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
  Yixuan Li, Ashwin Bharambe, and Laurens Van Der~Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 181--196, 2018.

\bibitem{Ahn2023LINe}
Yong Hyun Ahn, Gyeong-Moon Park, and Seong Tae Kim
\newblock LINe: Out-of-Distribution Detection by Leveraging Important Neurons
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2023.

\bibitem{mai2021metaMixup}
Zhijun Mai, Guosheng Hu, Dexiong Chen, Fumin Shen, and Heng~Tao Shen.
\newblock Metamixup: Learning adaptive interpolation policy of mixup with
  metalearning.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  2021.

\bibitem{Montabone2010}
Sebastian Montabone and Alvaro Soto.
\newblock Human detection using a mobile platform and novel features derived
  from a visual saliency mechanism.
\newblock {\em Image and Vision Computing}, 28(3):391--402, 2010.

\bibitem{nauta2021protoTree}
Meike Nauta, Ron van Bree, and Christin Seifert.
\newblock Neural prototype trees for interpretable fine-grained image
  recognition.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 14933--14943, 2021.

\bibitem{Nguyen2021effectiveness}
Giang Nguyen, Daeyoung Kim, and Anh Nguyen.
\newblock The effectiveness of feature attribution methods and its correlation
  with automatic evaluation scores.
\newblock {\em Advances in Neural Information Processing Systems},
  34:26422--26436, 2021.

\bibitem{ozturk2020gcuse}
Tulin Ozturk, Muhammed Talo, Eylul~Azra Yildirim, Ulas~Baran Baloglu, Ozal
  Yildirim, and U~Rajendra Acharya.
\newblock Automated detection of covid-19 cases using deep neural networks with
  x-ray images.
\newblock {\em Computers in biology and medicine}, 121:103792, 2020.

\bibitem{pinto2022robust}
Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip~HS Torr, and Puneet~K Dokania.
\newblock Regmixup: Mixup as a regularizer can surprisingly improve accuracy
  and out distribution robustness.
\newblock {\em Conference on Neural Information Processing Systems (NeurIPS)},
  2022.

\bibitem{Rao2022}
Sukrut Rao, Moritz Böhle, and Bernt Schiele.
\newblock Towards better understanding attribution methods.
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pages 10213--10222, 2022.

\bibitem{ren2022self}
Sucheng Ren, Huiyu Wang, Zhengqi Gao, Shengfeng He, Alan Yuille, Yuyin Zhou,
  and Cihang Xie.
\newblock A simple data mixing prior for improving self-supervised learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 14595--14604, 2022.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of computer vision}, 115(3):211--252,
  2015.

\bibitem{Samek2017deletion}
Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, and
  Klaus-Robert Müller.
\newblock Evaluating the visualization of what a deep neural network has
  learned.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems}, 2017.

\bibitem{Schulz2020IBA}
Karl Schulz, Leon Sixt, Federico Tombari, and Tim Landgraf.
\newblock Restricting the flow: Information bottlenecks for attribution.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{selvaraju2017gradCAM}
Ramprasaath~R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam,
  Devi Parikh, and Dhruv Batra.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 618--626, 2017.

\bibitem{Thulasidasan2019mixupCalibration}
Sunil Thulasidasan, Gopinath Chennupati, Jeff~A Bilmes, Tanmoy Bhattacharya,
  and Sarah Michalak.
\newblock On mixup training: Improved calibration and predictive uncertainty
  for deep neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{uddin2021saliencyMix}
A~F M~Shahab Uddin, Mst.~Sirazam Monira, Wheemyung Shin, TaeChoong Chung, and
  Sung-Ho Bae.
\newblock Saliencymix: A saliency guided data augmentation strategy for better
  regularization.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{verma2019manifoldMixup}
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas,
  David Lopez-Paz, and Yoshua Bengio.
\newblock Manifold mixup: Better representations by interpolating hidden
  states.
\newblock In {\em International Conference on Machine Learning}, pages
  6438--6447. PMLR, 2019.

\bibitem{khakzar2021towards}
Ashkan Khakzar, Sabrina Musatian, Jonas Buchberger, Icxel Quiroz Valeriano, Nikolaus Pinger, Soroosh Baselizadeh, Seong Tae Kim, Nassir Navab
\newblock Towards semantic interpretation of thoracic disease and covid-19 diagnosis models
\newblock In {\em International Conference on Medical Image Computing and Computer Assisted Intervention}, pages 499--508. 2021.

\bibitem{Wang2020scoreCAM}
Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr
  Mardziel, and Xia Hu.
\newblock Score-cam: Score-weighted visual explanations for convolutional
  neural networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, 2020.

\bibitem{wang2022robust}
Zifan Wang, Matt Fredrikson, and Anupam Datta.
\newblock Robust models are more interpretable because attributions look
  normal.
\newblock In {\em Proceedings of the 39th International Conference on Machine
  Learning}, volume 162, pages 22625--22651, 2022.

\bibitem{xiao2018unified}
Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.
\newblock Unified perceptual parsing for scene understanding.
\newblock {\em European conference on computer vision}, pages 418--434, 2018.

\bibitem{yin2021batchMixup}
Wenpeng Yin, Huan Wang, Jin Qu, and Caiming Xiong.
\newblock Batchmixup: Improving training by interpolating hidden states of the
  entire mini-batch.
\newblock In {\em Findings of the Association for Computational Linguistics:
  ACL-IJCNLP 2021}, pages 4908--4912, 2021.

\bibitem{Yun2019CutMix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, 2019.

\bibitem{zhang2018mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N. Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{zhang2020gcuse}
Jianpeng Zhang, Yutong Xie, Yi Li, Chunhua Shen, and Yong Xia.
\newblock Covid-19 screening on chest x-ray images using deep learning based
  anomaly detection.
\newblock {\em arXiv preprint arXiv:2003.12338}, 27, 2020.

\bibitem{khakzar2021neural}
Ashkan Khakzar, Soroosh Baselizadeh, Saurabh Khanduja, Christian Rupprecht, Seong Tae Kim, and Nassir Navab.
\newblock Neural response interpretation through the lens of critical pathways
\newblock {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 13528--13538, 2021.

  

\bibitem{zhang2022mixupCalibration}
Linjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou.
\newblock When and how mixup improves calibration.
\newblock In {\em International Conference on Machine Learning}, pages
  26135--26160. PMLR, 2022.

\bibitem{Zhang2021inputIBA}
Yang Zhang, Ashkan Khakzar, Yawei Li, Azade Farshad, Seong~Tae Kim, and Nassir
  Navab.
\newblock Fine-grained neural network explanation by identifying input features
  with predictive information.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~34, pages 20040--20051, 2021.

\bibitem{zhong2020randomErasing}
Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang.
\newblock Random erasing data augmentation.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pages 13001--13008, 2020.

\bibitem{zhou2016learning}
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba.
\newblock Learning deep features for discriminative localization.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2921--2929, 2016.

\bibitem{zhou2017scene}
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio
  Torralba.
\newblock Scene parsing through ade20k dataset.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 633--641, 2017.

\end{thebibliography}


\newpage
\appendix
\onecolumn



\section{Qualitative Assessment}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=16.5cm]{appendix_figures/qualitative.pdf}
     \caption{Qualitative comparison of five models for energyPG and WSOL using GradCAM and IBA. }
    \label{fig:qualitative}
\end{figure*}

\newpage
\section{Inter-model deletion and insertion using IBA
}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=17cm]{appendix_figures/RaO-iba.pdf}
     \caption{Faithfulness evaluated on the Baseline, Cutout, Mixup, CutMix, and SaliencyMix using IBA. 
     % Deletion/insertion curves from IBA are provided in the appendix.
     \textbf{Top}: Subtraction of Random Order deletion (RaO deletion) from the Least Relevant First deletion (LeRF deletion) curve. (a) LeRF deletion curve. Faithfulness and robustness are entangled. (b) RaO deletion curve. Robustness is represented. (c) Inter-model deletion curve. It is obtained by subtracting (b) from (a) to reduce the effect of robustness and leave faithfulness. The area under the curve in (c) is measured for the faithfulness score. Higher is better. The exact measure is calculated in \cref{tab:LeRF-RaO}.
     \textbf{Bottom}: Subtraction of RaO insertion curve from MoRF insertion curve. (d) MoRF insertion curve. (e) RaO insertion curve. (f) Inter-model insertion curve. It is obtained by subtracting (e) from (d). The area under the curve (f) is measured for the faithfulness score. Higher is better. The exact measure is provided in \cref{tab:MoRF-RaO} in the pain paper. }
    \label{fig:LeRF-RaO-IBA}
\end{figure*}

\cref{fig:LeRF-RaO-IBA} depicts the process to obtain the inter-model deletion/insertion score using IBA. While the tendency and order between subject models follow GradCAM result in the main paper, IBA exhibits higher faithfulness scores, verifying the experiment result in ~\cite{Schulz2020IBA}. Random deletion and insertion curves in \cref{fig:LeRF-RaO-IBA}b and \cref{fig:LeRF-RaO-IBA}e are the same regardless of the attribution method used. 



\end{document}
