% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{multirow}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}

\usepackage{algorithm}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
 

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Model Robustness Meets Data Privacy: Adversarial Robustness Distillation without Original Data}

\author{Yuzheng Wang$^\ast$, $\quad$ Zhaoyu Chen\thanks{Equal contribution}$\;\,$, $\quad$ Dingkang Yang, $\quad$ Pinxue Guo,\\ Kaixun Jiang, $\quad$ Wenqiang Zhang$^\dagger$, $\quad$ Lizhe Qi\thanks{Corresponding author}\\ \\
Academy for Engineering \& Technology, Fudan University, Shanghai, China
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Large-scale deep learning models have achieved great performance based on large-scale datasets.
Moreover, the existing Adversarial Training (AT) can further improve the robustness of these large models.
However, these large models are difficult to deploy to mobile devices, and the effect of AT on small models is very limited.
In addition, the data privacy issue (e.g., face data and diagnosis report) may lead to the original data being unavailable, which relies on data-free knowledge distillation technology for training.
To tackle these issues, we propose a challenging novel task called Data-Free Adversarial Robustness Distillation (DFARD), which tries to train small, easily deployable, robust models without relying on the original data.
We find the combination of existing techniques resulted in degraded model performance due to fixed training objectives and scarce information content.
First, an interactive strategy is designed for more efficient knowledge transfer to find more suitable training objectives at each epoch.
Then, we explore an adaptive balance method to suppress information loss and obtain more data information than previous methods.
Experiments show that our method improves baseline performance on the novel task.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Deep learning has achieved great success in many fields, such as computer vision \cite{karras2019style,dosovitskiy2020image,yang2023target,yang2022disentangled,yang2022learning} and signal processing \cite{devlin2018bert,radford2021learning,9987686,liu2022appearance,liu2022learning,liu2022efficient}.
However, recent large-scale models \cite{ramesh2022hierarchical} and data privacy issues \cite{burton2015data} prevent this technology from being applied to mobile devices, driverless cars and tiny robots.
As a result, Lopes \etal \cite{lopes2017data} propose Data-Free Knowledge Distillation (DFKD) to tackle the issue.
They use synthetic data to replace the privacy data to avoid leakage.
Simultaneously, knowledge is transferred from large redundant models to small easy-to-deploy models with the help of these synthetic data.
Therefore, DFKD is a technique to deal with the problem of model lightweight under data privacy.


\begin{figure}[t]
	\centering
	\includegraphics[scale=0.263]{page1.pdf}
	\vspace{-0.5cm}
	\caption{Diagrams of (a) Knowledge Distillation (KD), (b) Data-Free Knowledge Distillation (DFKD), (c) Adversarial Robustness Distillation (ARD), and (d) Data-Free Adversarial Robustness Distillation (DFARD). \textbf{S} and \textbf{T} represent the student and the teacher network respectively. $\mathcal{F}_{s}$ and $\mathcal{F}_{t}$ represent the search spaces of the student and teacher. $x_{ori}$ and $x_{gen}$ is the original and synthetic data. $x_{adv}$ is the adversarial augmentation of the input.}
	\label{fig1}
	\vspace{-0.5cm}
\end{figure}


In addition, model robustness has been a topic of much concern.
Studies have shown that well-trained deep learning models are vulnerable to adversarial examples containing only minor changes \cite{szegedy2013intriguing,goodfellow2014explaining,chen2022shape,chen2022towards}.
For better adversarial robustness, various methods have been proposed \cite{jia2019comdefend,kurakin2018adversarial,wu2021wider,wang2023adversarial}.
Among them, Adversarial Training (AT) has been considered the most effective approach \cite{croce2020reliable}.
It provides adversarial augmentation of the natural examples to the model for training. 
Therefore, it can significantly improve the robustness of large-capacity models, but the performance is struggling for small models.
\cite{goldblum2020adversarially} propose an Adversarial Robustness Distillation (ARD) method to improve the robustness of small models and ensure mobile deployment.
Therefore, ARD is a technique to improve the robustness of small models but relies on original data.



However, these technologies are still difficult to deploy in high-security scenes.
For example, when deploying a large-scale robust face recognition system on a personal mobile device, since the face data is private and cannot be made public, data-free technology is required.
Simultaneously, it should not be easily broken by adversarial attacks as a security system, and it is necessary to improve the robustness of small models.
Therefore, one question is whether we can efficiently train small, easily deployable, robust models without relying on the original data.
Based on this, we propose a novel task called Data-Free Adversarial Robustness Distillation (DFARD).
The robust small model is trained via the highly robust teacher model without private data.
The diagrams of the various tasks are shown in Figure \ref{fig1}.


We try this new DFARD task on the image classification on CIFAR datasets.
However, we find that DFARD does not simply combine data-free techniques and ARD.
Whatever the nature or the robustness of the scene, the simple combination results in declines in student performance.
We analyze the issue in two parts.
First, the past methods use a fixed temperature parameter in the generation and distillation stage, which may not be suitable for all training periods, e.g., it is difficult for the student to directly imitate the teacher at the beginning of training.
Second, almost all data-free generation techniques blindly pursue improving the confidence of the teacher model, which leads to a decrease in the information content of the synthetic data.
As a result, the DFARD task is challenging for existing methods.



To tackle these issues, we optimize the proposed DFARD from two aspects.
To find learning objectives that are suitable at various stages of training, we first propose an Interactive Temperature Adjustment (ITA) strategy to find a suitable distillation temperature for each training epoch.
Specifically, the student feeds the current learning status to interactively adjust the temperature parameters.
To better balance the similarity of the datasets and the knowledge content of the synthetic data, we then propose an Adaptive Generator Balance (AGB) method.
When the teacher's confidence is too high, blindly pursuing similarity is suppressed by reducing its weight.
Specifically, the primary contributions and experiments are summarized below:
\begin{itemize}
    \vspace{-0.1cm}
    \item For the first time, we explore a novel task named DFARD to apply higher security level application scenes, which combines the data privacy of data-free technology and the model robustness of ARD.
    \vspace{-0.1cm}
    \item We design an ITA strategy to adjust the temperature parameters according to the student's learning status instead of the fixed temperature parameter may not satisfy all training epochs.     
    \vspace{-0.1cm}
    \item We propose an AGB method to preserve the information content of synthetic data while improving the similarity between data domains.
    \vspace{-0.1cm}
    \item Experiments show that our optimized DFARD method oversteps all combinations of existing technologies and improves the state-of-the-art (SOTA) performances.
    \vspace{-0.1cm}
\end{itemize}


\section{Related Work}

\subsection{Data-Free Knowledge Distillation}


The setting of data-free knowledge distillation is that only the trained teacher model is available, and the original training data is not available due to privacy and other reasons.
Therefore, the key point is how to generate and supplement alternative training data to help knowledge transfer. Lopes \etal
\cite{lopes2017data} first propose the concept of DFKD to fit the original data by obtaining prior knowledge of the original data distribution.
However, the generative method is only suitable for small datasets, e.g., MNIST.
Chen \etal \cite{chen2019data} first introduce the generator into DFKD to get stronger generation capabilities.
Furthermore, they propose many generative losses widely used.
After that, to obtain the generated data that the student does not learn well, Micaelli \etal \cite{micaelli2019zero} introduce the method of adversarial generation.
They prompt the generator to generate data with larger differences between the predictions of the student and teacher so that the shortcomings are made up in the learning process.
Choi \etal \cite{choi2020data} combine DFKD with other model lightweight methods to further improve the lightweight effect.
To further improve the distillation speed, Fang \etal \cite{fang2022up} propose feature sharing to simplify the generation process of each step.
To improve generation quality, Bhardwaj \etal \cite{bhardwaj2019dream} introduce model inversion to synthesize the data.
They use the intermediate layer statistics of the teacher model to restore the original data.
Based on this, Yin \etal \cite{yin2020dreaming} introduce adversarial inversion to help students learn more difficult samples, and Fang \etal \cite{fang2021contrastive} introduce contrastive learning to enhance the learning effect further.


\subsection{Adversarial Robustness Distillation}

Early approaches to improve model adversarial robustness focuses on learning directly from adversarial examples \cite{madry2017towards}.
However, the expansion of the adversarial training set leads to increased training costs.
More importantly, the robustness improvement of small models is not obvious due to the limitation of model capacity.
Adversarial robustness distillation is proposed to address these issues.
The setting is that both robustly trained teacher models and original training data are available.
Goldblum \etal \cite{goldblum2020adversarially} first propose the concept of Adversarial robustness distillation.
They show that it is feasible to improve the robustness of small models without additional training costs.
Zi \etal \cite{zi2021revisiting} find that the soft labels given by the teacher are very effective and can significantly improve the robustness performance of the students.
Zhu \etal \cite{zhu2022reliable} find that the teacher's confidence in the student's adversarial examples continue to decline, which may not be able to give the correct guidance.
They propose a multi-stage strategy to allow the student to learn independently in later training.



\section{Why is it challenging to distill adversarial robustness without original data?}
\label{sec3}

DFARD is aimed at a highly confidential scenario focusing on data privacy and model robustness.
On the one hand, it is based on data-free technology \cite{chen2019data}, so it can not access confidential data.
On the other hand, it is based on the adversarial robustness distillation technology \cite{goldblum2020adversarially} to improve the robustness of the small model.
However, we ascertain that DFARD does not simply combine existing technologies, i.e., existing technologies are difficult to cope with the new challenges.
We find that the essentials lie in distillation temperature on the difficulty of the soft target and the knowledge capacity of synthetic data.
In addition, we prove that adversarially robust distillation is more susceptible to these factors than conventional knowledge distillation, and data-free scenes are more susceptible than data-based. Related further analysis and proofs can be found in the \textbf{Appendix A}. 
We discuss these two issues in more detail.


\subsection{Why is the appropriate temperature better?}

Distillation temperature enables the teacher network to provide suitable soft labels to transfer knowledge from the cumbersome model to a small model \cite{hinton2015distilling, romero2014fitnets}.
However, existing methods rely heavily on testing the effect of different temperature hyperparameters multiple times and keeping the same parameters constant throughout the learning process.
This brings a lot of extra computational costs. More importantly, the fixed temperature parameter does not guarantee training efficiency in different training epochs.
Some researchers have shown that simply selecting distillation temperature to construct a soft label does not necessarily improve student performance \cite{muller2019does, liasymmetric, zi2021revisiting}.
In the previous DFKD task, we can simply pick a fixed parameter to obtain suboptimal results for the normal scene.
However, in the robust scene, this issue may increase the gap between the robust scene and the normal scene, thereby severely inhibiting the adversarially robust performance of the students.
Therefore, it is challenging to apply the data-free technology from DFKD to DFARD.
To clear the phenomenon, we try to explain it from both theory and empiric.


For the theory, we first analyze the factors that may affect the model learning.
According to the VC theory \cite{vapnik1999overview}, we ponder that capacity and flexibility is the decisive factor.
We give its definition as \cref{def:inj}.

\begin{definition}
\label{def:inj}
(VC theory). For a well-trained classifier model $f_s$, given the real search space $f_{gt} \in \mathcal{F}_{gt} $, its classification error can be expressed as:
$$
R(f_s)-R(f_{gt})\le  O (\frac{\left | \mathcal{F}_{s}  \right |_C }{n^{\alpha_{sg} } } )+\epsilon_{sg},
$$
where $O(\cdot)$ is the estimation error, which is related to the statistical procedure for learning given the number of data points $n$.
$\epsilon_{sg}$ is the approximation error determined by the model's capacity.
$ \frac{1}{2} \le \alpha \le 1$ is the learning rate, which decreases with increasing prediction difficulty.
\end{definition}
For knowledge distillation, the above process can be expressed as:
\begin{equation}\label{eq1}
R(f_s)-R(f_{t})\le  O (\frac{\left | \mathcal{F}_{s}  \right |_C }{n^{\alpha_{st} } } )+\epsilon_{st},
\end{equation}
where $f_s, f_t$ denote the student and teacher models.
Depending on the temperature of distillation, the student receives different levels of knowledge.
We define the difficulty degree of knowledge transfer.
\begin{theorem}
\label{thm1}
For given teacher network, high temperature of knowledge representation $t_1$, and low temperature of knowledge representation $t_2$, satisfying $\left |  \mathcal{F}_{2}-\mathcal{F}_{gt}   \right | \le \left |  \mathcal{F}_{1}-\mathcal{F}_{gt}   \right | $. Knowledge transfer from $t_2$ is more difficult than from $t_1$. The proof can be found in \textbf{Appendix B}.
\end{theorem}
\begin{assumption}
\label{ass1}
\emph{
The low temperature provides a larger target logit or less varied wrong logits. Therefore, the low-temperature knowledge representation $t_2$ can provide more effective correct guidance than a high-temperature representation $t_1$.}
\end{assumption}
For \cref{ass1}, intuitively, the low-temperature knowledge representation tends to be more confident about the correct class. 
It can give the student more knowledge about correct guidance. 
For the conclusion, Li \etal \cite{liasymmetric} have provided a detailed analysis and proof. 
Therefore, the proof process is not repeated in this paper.



We re-examine the entire knowledge transfer process by combining \cref{thm1} and \cref{ass1}.
On the one hand, the low-temperature knowledge representation has greater confidence for the main class, which usually provides more correct guidance.
But, knowledge transfer is more difficult.
On the other hand, the high-temperature knowledge representation is easier to transfer knowledge.
But, low confidence may lead to insufficient learning of the main class.
As a result, the effectiveness of knowledge transfer is sensitive to distillation temperature.
Therefore, the choice of temperature is a trade-off and may change as the student progresses.
The issue needs to be optimized and solved in the DFARD task, which may be why the previous methods are difficult in dealing with new challenges.




\begin{figure}[h]
	\centering
	\includegraphics[scale=0.41]{fig1.pdf}
    \vspace{-0.2cm}
	\caption{Composition of various labels in knowledge distillation. Teacher models with different distillation temperatures provide learning objectives with different difficulties.}
	\label{fig2}
\end{figure}


\begin{figure}[h]
	\centering
	\includegraphics[scale=0.38]{fig2.pdf}
	\vspace{-0.2cm}
	\caption{Student performance under various distillation temperatures with the same teacher. (a) and (b) are different application scenarios of DFKD and DFARD, respectively (clean accuracy in DFKD and robust accuracy under AA attack in DFARD).}
	\label{fig3}
	%\vspace{-0.4cm}
\end{figure}


For empirical analysis, we focus on the ‘softness’ of the soft labels caused by different distillation temperatures.
According to \cite{shen2021label}, proper smoothing factor for KD helps to better results. 
Different degrees of softness indicates the difficulty of different learning objectives.
In this part, we use different temperatures \cite{muller2019does} to construct various soft labels as shown in Figure \ref{fig2}.
At the same time, we carry out experiments in both DFKD and DFARD tasks to obtain comprehensive results.
The experimental results are shown in Figure \ref{fig3}. 
(a) and (b) show the student’s learning effectiveness of learning objectives with different difficulties, which are adjusted by different distillation temperatures.
We distill all student models for 50 epochs. The results in the figure are clean accuracy in DFKD and robust accuracy under AA attack \cite{croce2020reliable} in DFARD.
An obvious conclusion is that, in both DFKD and DFARD tasks, a too-difficult learning target (low distillation temperature) or a too-simple target (high distillation temperature) leads to a decline in the student’s performance.
Besides, we also show that maintaining a fixed temperature throughout the training process may affect the student’s performance. The results can be found in Table \ref{tab4}.



\subsection{Why is simply authentic generation not the best?}

To further reduce the dependence of adversarially robust distillation technology on the original data, we try to apply the existing data-free technology.
The result is that existing methods are not suitable for the new scene (in Table \ref{tab2}).
The reason for our analysis is that almost all previous data-free generation technologies attach great importance to the similarity between the synthetic set $\hat{\textbf{\textrm{x}}}$ and the original data \cite{chen2019data,yin2020dreaming,choi2020data,fang2021contrastive,fang2022up}.
They set a fixed generation loss weight to train a generator for the similarity during the training process.
This setting can help the student at the beginning of training, but it converges prematurely and is limited by some factors.
We consider the knowledge capacity in the generated data a very important decisive factor.
The fixed generation loss weight eventually causes the information content in the generated data to be ignored, which is not conducive to better learning.


To obtain more real generated data, the teacher model's predictions $f_{t}(\hat{\textbf{\textrm{x}}})$ should be close to the one-hot labels $\textbf{\textrm{y}}$, e.g., minimize them with the following cross-entropy loss.
\begin{equation}\label{eq2}
\mathcal{L}_{cls}=CE(f_{t}(\hat{\textbf{\textrm{x}}}), \textbf{\textrm{y}}),
\end{equation}
where $\textbf{\textrm{y}}$ can be randomly generated labels or pseudo-labels of the teacher.
$\hat{\textbf{\textrm{x}}}$ is synthesized by the generator through random noise $\textbf{\textrm{z}}$ and the label $\textbf{\textrm{y}}$: $\hat{\textbf{\textrm{x}}}=g(\textbf{\textrm{z}},\textbf{\textrm{y}})$.
Similar predictions help promote similar data distributions.
However, we think such a setting reduces the knowledge content of the generated data as \cref{ass2}.
\begin{assumption}
\label{ass2}
\emph{
When the teacher's prediction confidence continues to increase and approaches the one-hot label, the information entropy in the data continues to decrease. The proof can be found in \textbf{Appendix C}.
}
\end{assumption}
According to \cref{ass2}, the similarity between the generated data and the original data distribution and its information capacity is in a trade-off relationship.
Like all previous methods, blindly pursuing similarity is inappropriate and likely to interfere with the student's performance.
Therefore, new generation strategies are needed to balance these two parts for new challenges.




\section{Data-Free Adversarial Robustness Distillation}
\label{sec5}
According to \cref{sec3}, we summarize the issues in the current DFKD technology and why is the DFARD technology difficult to achieve.
To tackle the problems and successfully apply existing technologies to the new field of DFARD, we propose an Interactive Temperature Adjustment (ITA) strategy, which dynamically adjusts the distillation temperature according to the training status of the students in the current training epoch.
Then we design an Adaptive Generator Balance (AGB) method to balance similarity and information capacity.
The pipeline is shown in Figure \ref{fig4}, which contains the generation and distillation stages.
First, the generator is trained via the ITA and AGB method.
Then, the student is trained with the ITA strategy.
The detailed training process is shown in Algorithm \ref{alg1}.


\begin{figure*}[t]
	\centering
	\includegraphics[scale=0.43]{fig3.pdf}
	\vspace{-0.2cm}
	\caption{The pipeline of our DFARD method, which consists of two stages: (1) In the generation stage, we design an Interactive Temperature Adjustment strategy to adjust the temperature $\tilde{\tau}$ according to the student's learning. Simultaneously, we propose an Adaptive Generator Balance method and compute the adaptive weights $\lambda$ to balance the similarity between data domains and the information content of data. (2) In the distillation stage, we keep the interactive temperature to help the student learn better.}
	\label{fig4}
	\vspace{-0.2cm}
\end{figure*}


\subsection{Interactive Temperature Adjustment}

Since an efficient knowledge transfer process requires multiple attempts to find a suitable teacher and a suitable distillation temperature. 
In this part, we expect to use only the specific teacher model without specifically setting the distillation temperature.
Our strategy adjusts the teacher label softness through the interactive distillation temperature $\tilde{\tau}$ so that the confidence gap between the teacher and student models is kept in a suitable range.



In generate stage, the generated data should transfer the information of decision boundary from the teacher model to the student model as effectively as possible \cite{heo2019knowledge}.
Unlike previous methods, our generator does not directly generate data close to the teacher's decision boundary but starts from easy data for the student to learn.
We maximize the predictions of the student and the teacher to find effective generated data via adversarial loss as:
\begin{equation}\label{eq3}
\mathcal{L}_{adv}=-KL(f_t(\hat{\textbf{\textrm{x}}}; \theta_t), f_s(\hat{\textbf{\textrm{x}}}; \theta_s), \tilde{\tau}),
\end{equation}
where $KL(\cdot, \cdot)$ denotes the Kullback-Leibler (KL) divergence loss. 
Then we calculate the confidence of the teacher $Con_t$ and the class it predicts $c$ as $Con_t,c = \mathop{\arg\max}f_t(\hat{\textbf{\textrm{x}}})$. 
The student's confidence for category $c$ can be directly obtained as $Con_s$. The adaptive distillation temperature $\tilde{\tau}$ can be calculated as:
\begin{equation}\label{eq4}
\tilde{\tau}= max\left \{ \frac{1}{bs} \sum_{i=1}^{bs}  \left | Con_t-Con_s \right | \cdot C, 1 \right \}, 
\end{equation}
where $C$ is total number of classes and $bs$ denotes the batch size.
The calculated absolute value represents the difference between the student's and the teacher's prediction in the current training epoch, thus reflecting the current learning situation.
In the early stages of student initiation, larger distillation temperatures are set to obtain softer easy-to-learn teacher predictions due to larger prediction variances.
As the student's prediction gets closer to the teacher's, the distillation temperature decreases to generate data closer to the teacher's decision boundary.
In this process, $\tilde{\tau}$ does not require any artificial test adjustment, and its value is greater than or equal to 1.


Similarly, we consider the effectiveness of knowledge transfer in the distillation stage.
As the progress of the students continues to increase the learning difficulty, we define the interactive knowledge distillation loss as:
\begin{equation}\label{eq5}
\mathcal{L}_{K\!D}=\sum_{\hat{\textbf{\textrm{x}}}'\in{\hat{\textbf{\textrm{X}}}'}} KL(f_t({\hat{\textbf{\textrm{x}}}'}; \theta_t), f_s(\hat{\textbf{\textrm{x}}}'; \theta_s), \tilde{\tau}), 
\end{equation}
where $\hat{\textbf{\textrm{x}}}'$ is generated data $\hat{\textbf{\textrm{x}}}$ after adversarial robustness data augmentations \cite{goldblum2020adversarially}.
The teacher gives easy-to-learn reference answers at an early stage and gradually turns into the teacher's original predictions as the student improves.


\subsection{Adaptive Generator Balance}
The generation loss weights in the previous data-free generation methods need multiple attempts, which leads to additional computational costs. 
Concurrently, the constant weights are not conducive to weighing the similarity between synthetic and original data and the information content of the synthetic data.
Based on this, we design an AGB method, which can adaptively adjust the generation loss weights according to the current confidence of the teacher.

For the generator $g$, we combine Equation (\ref{eq2}) and (\ref{eq3}) as:
\begin{equation}\label{eq6}
\mathcal{L}_{gen}=\lambda\cdot\mathcal{L}_{cls}+(1-\lambda)\cdot\mathcal{L}_{adv}, 
\end{equation}
where $\lambda$ is the generation loss trade-off parameter.
To better weigh the similarity and the data information, $\lambda$ depends on the prediction confidence of the teacher model in the current generation state. 
When the teacher's confidence is too high, the information content of the data may be ignored.
At this time, $\lambda$ adaptively reduces to avoid blindly pursuing similarity.
Specifically, $\lambda$ is calculated as:
\begin{equation}\label{eq7}
\lambda = \frac{1}{C\cdot Con_t}. 
\end{equation}
The confidence $Con_t$ is greater than or equal to the expected value $\frac{1}{C}$ for the dataset with the number of classes $C$.
Therefore, it always satisfies $0<\lambda<1$.
With the help of AGB, we can increase the amount of information in the synthetic data while satisfying the similarity, which helps the student's performance.
Simultaneously, we no longer need to try many different weight combinations to test results.
Therefore, our method is more simple and more convenient.

\begin{algorithm}[h]
  %\vspace{-0.3cm}
  \caption{Training process of our Data-Free Adversarial Robustness Distillation}
  \label{alg1}
  \begin{algorithmic}[1]
    \Require
      A trained teacher network $f_{t}$, generator $g$ with parameter $\theta_g$, the student $f_{s}$ with parameter $\theta_s$, distillation epochs $T$, number of training rounds $T\!s$ for student training in each epoch.

    \State Initialize parameter $\theta_g$ and $\theta_s$
    \For {$i$ in $[1,\dots,T]$}
        \State $//$ \textit{\textbf{Generation stage}}
        \State Randomly sample noises and labels $(\textbf{\textrm{z}},\textbf{\textrm{y}})$
        \State Synthesize training data $\hat{\textbf{\textrm{x}}}=g(\textbf{\textrm{z}},\textbf{\textrm{y}})$
        \State Update generator $g$ through Equation (\ref{eq6})

        \State $//$ \textit{\textbf{Distillation stage}}
        \For {$j$ in $[1,\dots,T\!s]$}
        \State Synthesize training data $\hat{\textbf{\textrm{x}}}=g(\textbf{\textrm{z}},\textbf{\textrm{y}})$
        \State Adversarial robustness augmentation
        $\hat{\textbf{\textrm{x}}}\to\hat{\textbf{\textrm{x}}}'$ 
        \State Distill the student $f_{s}$ through Equation (\ref{eq5})
        \EndFor
    \EndFor
    \Ensure The student $f_{s}$ with adversarial robustness.
    \end{algorithmic}
\end{algorithm}

\vspace{-0.2cm}
\section{Experiments}

\subsection{Experimental Setup}

\begin{table*}[t]
\centering
\caption{Teacher models and their robust performance (\%).}
\vspace{-0.3cm}
\setlength{\tabcolsep}{3mm}
\begin{tabular}{@{}cc|cccccc@{}}
\toprule
Dataset   & Teacher          & Clean & FGSM  & PGD${\rm _{S}}$   & PGD${\rm _{T}}$   & CW    & AA    \\ \midrule
CIFAR-10  & WideResNet-34-10 & 84.92 & 60.87 & 55.33 & 56.61 & 53.98 & 53.08 \\
CIFAR-100 & WideResNet-70-16 & 60.86 & 35.68 & 33.56 & 33.99 & 42.15 & 30.03 \\ \bottomrule
\end{tabular}
\label{tab1}
\vspace{0.1cm}
\end{table*}



\begin{table*}[t]
\centering
\caption{Adversarial robustness accuracy (\%) on CIFAR-10 and CIFAR-100 datasets. The maximum adversarial perturbation $\epsilon$ is 8/255. RN-18 and MN-V2 are abbreviations of ResNet-18 and MobileNetV2, respectively.
\textbf{Bold} numbers denote the best results.
Average indicates the average value of the robustness test, which does not include the clean accuracy.
\textbf{Ours} means use our generation and distillation method as Algorithm \ref{alg1}.
\textbf{Ours*} means only use our generation method as Equation (\ref{eq6}).
}
\vspace{-0.2cm}
\setlength{\tabcolsep}{2mm}
\scalebox{0.83}{
\begin{tabular}{@{}c|c|cccccccccccccc@{}}
\toprule
\multirow{3}{*}{Model} &
  \multirow{3}{*}{Method} &
  \multicolumn{7}{c}{CIFAR-10} &
  \multicolumn{7}{c}{CIFAR-100} \\ \cmidrule(l){3-16} 
 &
   &
  \multicolumn{7}{c|}{Attacks Evaluation} &
  \multicolumn{7}{c}{Attacks Evaluation} \\
 &
   &
  Clean &
  FGSM &
  PGD${\rm _{S}}$ &
  PGD${\rm _{T}}$ &
  CW &
  AA &
  \multicolumn{1}{c|}{Average} &
  Clean &
  FGSM &
  PGD${\rm _{S}}$ &
  PGD${\rm _{T}}$ &
  CW &
  AA &
  Average \\ \midrule
\multirow{9}{*}{RN-18} &
  Dream &
  \textbf{68.26} &
  34.76 &
  29.72 &
  31.36 &
  27.96 &
  26.70 &
  \multicolumn{1}{c|}{30.10} &
  22.00 &
  10.18 &
  9.52 &
  9.85 &
  7.11 &
  6.68 &
  8.67 \\
 &
  DeepInv &
  64.53 &
  35.18 &
  31.26 &
  32.49 &
  28.77 &
  27.93 &
  \multicolumn{1}{c|}{31.13} &
  40.91 &
  19.46 &
  17.86 &
  18.68 &
  15.27 &
  14.54 &
  17.16 \\
 &
  DAFL &
  54.98 &
  27.04 &
  24.75 &
  25.87 &
  22.90 &
  22.25 &
  \multicolumn{1}{c|}{24.56} &
  41.67 &
  21.42 &
  20.13 &
  20.81 &
  17.96 &
  17.16 &
  19.50 \\
 &
  ZSKT &
  58.08 &
  31.98 &
  29.94 &
  30.92 &
  27.21 &
  26.68 &
  \multicolumn{1}{c|}{29.35} &
  38.91 &
  20.16 &
  18.78 &
  19.41 &
  16.38 &
  15.52 &
  18.05 \\
 &
  DFQ &
  54.44 &
  26.90 &
  24.63 &
  25.78 &
  22.37 &
  21.57 &
  \multicolumn{1}{c|}{24.25} &
  45.24 &
  22.49 &
  20.78 &
  21.61 &
  18.24 &
  17.38 &
  20.10 \\
 &
  CMI &
  53.28 &
  25.78 &
  23.14 &
  23.97 &
  21.03 &
  20.38 &
  \multicolumn{1}{c|}{22.86} &
  45.04 &
  22.78 &
  21.02 &
  21.90 &
  17.90 &
  16.97 &
  20.11 \\
 &
  Fast &
  61.13 &
  31.40 &
  28.01 &
  29.17 &
  26.26 &
  25.42 &
  \multicolumn{1}{c|}{28.05} &
  36.75 &
  18.66 &
  17.72 &
  18.33 &
  15.57 &
  14.77 &
  17.01 \\
 &
  \textbf{Ours*} &
  65.10 &
  36.36 &
  33.47 &
  34.89 &
  30.79 &
  30.06 &
  \multicolumn{1}{c|}{33.11} &
  45.33 &
  24.08 &
  22.71 &
  23.38 &
  19.84 &
  19.00 &
  21.80 \\
 &
  \textbf{Ours} &
  66.44 &
  \textbf{38.53} &
  \textbf{35.94} &
  \textbf{37.15} &
  \textbf{32.79} &
  \textbf{32.14} &
  \multicolumn{1}{c|}{\textbf{35.31}} &
  \textbf{46.33} &
  \textbf{24.56} &
  \textbf{22.94} &
  \textbf{23.59} &
  \textbf{20.12} &
  \textbf{19.19} &
  \textbf{22.08} \\ \midrule
\multirow{9}{*}{MN-V2} &
  Dream &
  \textbf{64.95} &
  32.03 &
  26.09 &
  27.63 &
  23.83 &
  22.28 &
  \multicolumn{1}{c|}{26.37} &
  18.73 &
  9.78 &
  8.96 &
  9.37 &
  6.93 &
  6.33 &
  8.27 \\
 &
  DeepInv &
  59.53 &
  31.76 &
  28.42 &
  29.74 &
  25.86 &
  24.99 &
  \multicolumn{1}{c|}{28.15} &
  37.75 &
  16.94 &
  15.54 &
  16.19 &
  12.65 &
  11.80 &
  14.62 \\
 &
  DAFL &
  47.53 &
  24.51 &
  21.18 &
  22.09 &
  19.50 &
  18.86 &
  \multicolumn{1}{c|}{21.23} &
  40.46 &
  20.63 &
  19.03 &
  19.78 &
  16.54 &
  15.82 &
  18.36 \\
 &
  ZSKT &
  57.02 &
  30.29 &
  27.07 &
  28.25 &
  24.89 &
  24.40 &
  \multicolumn{1}{c|}{26.98} &
  25.16 &
  12.34 &
  11.36 &
  11.78 &
  9.69 &
  9.16 &
  10.87 \\
 &
  DFQ &
  44.25 &
  21.13 &
  19.14 &
  20.07 &
  16.87 &
  16.20 &
  \multicolumn{1}{c|}{18.68} &
  40.26 &
  19.45 &
  17.74 &
  18.44 &
  15.14 &
  14.35 &
  17.02 \\
 &
  CMI &
  44.53 &
  21.34 &
  19.67 &
  19.97 &
  16.25 &
  15.97 &
  \multicolumn{1}{c|}{18.64} &
  40.23 &
  19.76 &
  17.96 &
  18.56 &
  14.86 &
  14.02 &
  17.03 \\
 &
  Fast &
  54.06 &
  28.23 &
  25.69 &
  26.83 &
  23.18 &
  22.42 &
  \multicolumn{1}{c|}{25.27} &
  38.69 &
  18.58 &
  16.77 &
  17.58 &
  14.62 &
  13.75 &
  16.26 \\
 &
  \textbf{Ours*} &
  59.79 &
  32.25 &
  29.25 &
  30.24 &
  26.18 &
  25.56 &
  \multicolumn{1}{c|}{28.70} &
  40.94 &
  21.47 &
  20.18 &
  20.89 &
  17.60 &
  16.82 &
  19.39 \\
 &
  \textbf{Ours} &
  61.16 &
  \textbf{34.46} &
  \textbf{31.66} &
  \textbf{32.80} &
  \textbf{28.40} &
  \textbf{27.90} &
  \multicolumn{1}{c|}{\textbf{31.04}} &
  \textbf{41.78} &
  \textbf{22.04} &
  \textbf{20.84} &
  \textbf{21.68} &
  \textbf{17.93} &
  \textbf{17.04} &
  \textbf{19.91} \\ \bottomrule
\end{tabular}
}
\label{tab2}
\vspace{-0.2cm}
\end{table*}



\textbf{Dataset and Model.} 
We evaluate the proposed DFARD method on 32$\times$32 CIFAR-10, and
CIFAR-100 datasets \cite{krizhevsky2009learning}, which are the most commonly used datasets for testing adversarial robustness.
For a fair comparison, we use the same pre-trained teacher models with \cite{zi2021revisiting} as shown in Table \ref{tab1}.
Furthermore, we evaluate all methods using the student with ResNet-18 \cite{he2016deep} and MobileNetV2 \cite{sandler2018mobilenetv2} followed ARD methods \cite{zhu2022reliable,zi2021revisiting}.


\textbf{Baselines.} 
We compare our proposed method with different data-free generation methods, including: Dream \cite{bhardwaj2019dream}, DeepInv \cite{yin2020dreaming}, DAFL \cite{chen2019data}, ZSKT \cite{micaelli2019zero}, DFQ \cite{choi2020data}, CMI \cite{fang2021contrastive}, and Fast \cite{fang2022up}.
The detailed experimental setup is shown in \textbf{Appendix E}.
Considering that other generative methods may not be suitable for our proposed distillation loss in equation (\ref{eq5}).
For a fair comparison, the distillation process uses the same generic loss as ARD \cite{goldblum2020adversarially} ($\tilde{\tau}$ is always equal to 1 in Equation (\ref{eq5}).
Afterward, we compare whether Equation (\ref{eq5}) is helpful while maintaining our proposed generative method during the generation process.


\textbf{Implementation Details.} 
Our proposed method and all others are implemented in PyTorch \cite{paszke2019pytorch}.
All models are trained on RTX 3090 GPUs.
Our students are trained via SGD optimizer with cosine annealing learning rate with an initial value of 0.05, momentum of 0.9, and weight decay of 1e-4.
Our generators are trained via Adam optimizer with a learning rate of 1e-3, $\beta_1$ of 0.5, $\beta_2$ of 0.999.
The generators architecture can be found in \textbf{Appendix D}.
The distillation batch size and the synthesis batch size are both 256.
The distillation epochs $T$ is 200, and the number of training rounds $T\!s$ for student training in each epoch is 5.
Both the student model and the generator are randomly initialized.
A 10-step PGD (PGD-10) with a random start size of 0.001 and step size of 2/255 is used for adversarial augmentation.
The perturbation bounds are set to $L_\infty $ norm $\epsilon=8/255$.
Note that, our method requires no additional hyperparameters, simplifying the training process.


\textbf{Attack Evaluation.}
We evaluate the adversarial robustness with five adversarial attacks: FGSM \cite{goodfellow2014explaining}, PGD${\rm _{S}}$ \cite{madry2017towards}, PGD${\rm _{T}}$ \cite{zhang2019theoretically}, CW$_\infty$ \cite{carlini2017towards} and AutoAttack(AA) \cite{croce2020reliable}.
These methods are the most commonly used for adversarial robustness evaluation.
The maximum perturbation is set as $\epsilon=8/255$.
The perturbation steps for PGD${\rm _{S}}$, PGD${\rm _{T}}$ and CW$_\infty$ are 20 epochs.
In addition, we test the accuracy of the models in normal conditions without adversarial attacks with normal data augmentation (Clean).


\begin{figure*}[t]
	\centering
	\includegraphics[scale=0.48]{fig4.pdf}
	\vspace{-0.2cm}
	\caption{Visualization of original data and synthetic data in three data-free methods on CIFAR-10 dataset.}
	\label{fig5}
	\vspace{-0.3cm}
\end{figure*}


\subsection{Performance Comparison}

To compare the effects of different data-free generation methods, we set the same distillation process as ARD \cite{goldblum2020adversarially}.
Therefore, the difference lies in the generative loss function used by the generative module.
We select and report the best checkpoint of all methods among all epochs.
The best checkpoints are based on the adversarial robustness performance against PGD${\rm _{T}}$ attack.


The robustness performances of our and other baseline methods are shown in Table \ref{tab2}. 
Our generation method (Ours*) achieves SOTA adversarial robustness performance in all baselines.
The results demonstrate that our interactive and adaptive approach can be more effective for the new and more challenging task of DFARD.
In different backbone and dataset combinations, our method improves the average adversarial robustness by 1.98\%, 0.55\%, 1.69\%, and 1.03\%, respectively, compared to other best results.


Surprisingly, most other methods produce large fluctuations in different experimental settings, especially since the performance varies greatly on different datasets.
We consider that one reason is that some methods lack measures to satisfy data diversity.
Dream \cite{bhardwaj2019dream} first invert enough data, then use this training data for normal training.
However, these data may be too similar, resulting in poor performance on multi-class datasets (e.g., CIFAR-100).
Similarly, ZSKT \cite{micaelli2019zero} only uses adversarial loss in the generation process to widen the prediction gap between teacher and student, ignoring the diversity of data.
DeepInv \cite{yin2020dreaming} keeps the same teacher predictions throughout the generation process to generate synthetic data adversarial inversion.
The prediction goal can be difficult for the randomly initialized student, especially in datasets with more complex classes.
Fast \cite{fang2022up} uses a feature-sharing method, but fewer features can be shared in complex datasets leading to performance degradation.
In contrast, some early methods (DAFL \cite{chen2019data}, DFQ \cite{choi2020data} and CMI \cite{fang2021contrastive}) are more stable and effective, but they all rely on more than three generation losses, so the choice of weights requires much experimental exploration.


Based on these observations and analysis, our method achieves progress in performance. 
More importantly, it is simple and convenient.
Our method does not require multiple hyperparameter tests and additional calculation costs.
Simultaneously, our method can cope with different experimental settings.


\subsection{Further Analysis}
\textbf{Interactive Temperature in Distillation.}
In the last part, we have compared the effects of different data-free generation methods while keeping the distillation process the same.
In this part, we use a distillation method different from the previous ARD methods, i.e., the temperature $\tilde{\tau}$ obtained interactively is applied to the distillation process as in Algorithm \ref{alg1}.
Other settings are the same as the last part.
The robustness performances are shown in the last line of Table \ref{tab2} (Ours).
An interactive temperature in both generation and distillation stages is more efficient than just using it in generation stage both in nature and robust scenes.
Such results also suggest that it may be more difficult to consistently use the teacher's original prediction objectives during the knowledge transfer process.
We think the model capacity gap between the teacher and the student makes knowledge transfer challenging (e.g., WideResNet \cite{zagoruyko2016wide} and ResNet \cite{he2016deep}).
This gap comes from the setting of the ARD task itself, i.e., the robustness knowledge is transferred from the large model to the small model.
In this case, our interactive temperature improves baseline performance during distillation.
Such easy-to-difficult method may provide a novel idea for the challenging DFARD task.





\textbf{Impact of Interactive Temperature in Generation.}
To further verify the method's effectiveness, we first verify the impact of multiple fixed temperatures and our interactive temperature in the generation process.
We test on the CIFAR-10 dataset with a student of ResNet-18.
All experiments with the Adaptive Generator Balance are performed for the same 50 epochs.
Table \ref{tab4} shows the results for the best checkpoint only.
An interactive temperature outperforms the fixed temperature in most indicators.
The student's feedback can dynamically adjust the difficulty of generating data for learning.
Therefore, it is helpful for the student's learning in the primary stage (50 epochs).
These results suggest that an interactive temperature is more potential than a fixed one.




\textbf{Impact of Adaptive Generator Balance.}
Furthermore, we evaluate the effectiveness of the proposed Adaptive Generator Balance method.
The settings are the same with Table \ref{tab4}.
All experiments are performed with the Interactive Temperature Adjustment strategy by default.
The results are shown in Table \ref{tab5}.
When $\lambda$ is used as a fixed hyperparameter, its size greatly impacts the results, which usually require multiple attempts to get the optimal value.
These attempts undoubtedly incur additional computational costs.
In contrast, our adaptive approach omits the above process while significantly improving the baseline performance.
These results show that blindly pursuing the similarity between synthetic and training data while reducing the data information entropy harms the student's performance.


\begin{table}[h]
\centering
\caption{The effect of interactive or fixed generator temperature on adversarial robustness. Ours represents our interactive strategy.}
\vspace{-0.2cm}
\setlength{\tabcolsep}{1.65mm}
\scalebox{0.89}{
\begin{tabular}{@{}c|ccccccc@{}}
\toprule
\multirow{2}{*}{T} & \multicolumn{7}{c}{Attacks Evaluation}                  \\
                   & Clean & FGSM  & PGD${\rm _{S}}$   & PGD${\rm _{T}}$   & CW    & AA    & Average \\ \midrule
1                  & 42.33 & 21.89 & 20.48 & 21.40 & 18.32 & 17.76 & 19.97   \\
3                  & 42.64 & 22.15 & 20.84 & 21.95 & 18.95 & 18.39 & 20.46   \\
5                  & 42.43 & 21.74 & 20.44 & 21.14 & 18.63 & 18.03 & 20.00   \\
7                  & 41.11 & 20.61 & 19.12 & 19.92 & 17.70 & 17.11 & 18.89   \\
9                  & 41.42 & 19.81 & 18.20 & 18.97 & 16.35 & 15.69 & 17.80   \\
\textbf{Ours}               & \textbf{43.52} & \textbf{22.65} & \textbf{21.45} & \textbf{22.45} &   \textbf{18.95} & \textbf{18.42} & \textbf{20.78}   \\ \bottomrule
\end{tabular}
}
\label{tab4}
%\vspace{0.2cm}
\end{table}



\begin{table}[h]
\centering
\caption{The effect of adaptive or fixed generator loss weights on adversarial robustness. Ours represents our adaptive method.}
\vspace{-0.2cm}
\setlength{\tabcolsep}{1.65mm}
\scalebox{0.89}{
\begin{tabular}{@{}c|ccccccc@{}}
\toprule
\multirow{2}{*}{$\lambda$} & \multicolumn{7}{c}{Attacks Evaluation}                  \\
                   & Clean & FGSM  & PGD${\rm _{S}}$   & PGD${\rm _{T}}$   & CW    & AA    & Average \\ \midrule
0.1                & 56.41 & 28.06 & 25.53 & 26.63 & 23.1  & 22.47 & 25.16   \\
0.3                & 59.16 & 31.05 & 28.59 & 29.9  & 24.87 & 24.21 & 27.72   \\
0.5                & 31.66 & 13.83 & 13.04 & 13.73 & 9.07  & 8.59  & 11.65   \\
0.7                & 19.35 & 11.45 & 10.68 & 11.28 & 9.15  & 8.96  & 10.30   \\
0.9                & 10.87 & 9.58  & 9.42  & 9.44  & 8.77  & 8.78  & 9.20    \\
\textbf{Ours}                & \textbf{65.10} & \textbf{36.36} & \textbf{33.47} & \textbf{34.89} & \textbf{30.79} & \textbf{30.06} & \textbf{33.11}   \\ \bottomrule
\end{tabular}
}
\label{tab5}
\vspace{-0.3cm}
\end{table}


\textbf{Visualization of synthetic data.}
To show some data-free methods more intuitively, we visualize part of synthetic data and the original data on CIFAR-10 in Figure \ref{fig5}.
From the visualization result, we can see that our method is significantly better than the other methods in terms of sample diversity.
Therefore, our interactive and adaptive method can better meet the challenge of DFARD.


\section{Conclusion}
In this paper, we propose a novel task named Data-Free Adversarial Robustness Distillation (DFARD) to deal with realistic scenes with higher security levels.
We demonstrate that the task is more challenging than existing tasks, which makes previous methods less effective.
To tackle new challenges, we first design an Interactive Temperature Adjustment strategy to dynamically adjust the temperature parameter in the generation and distillation stages according to the current student learning status.
Then, we propose an Adaptive Generator Balance method to avoid the information content of the data being ignored.
Our method is simple and effective, achieving the best results against various adversarial attacks.
We believe that the proposed technique helps apply deep learning techniques to real scenes while maintaining a high degree of security.



%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{DFARD}
}

\end{document}
