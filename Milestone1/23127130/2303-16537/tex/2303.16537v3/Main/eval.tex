\section{Experiments}

%\subsection{Dataset}
\subsection{Experiment Settings}
In our experiments, \zc{we evaluate the performance of the candidate approaches using two datasets: CommonsenseQA and OpenBookQA. CommonsenseQA consists of 12,247 five-way multiple-choice questions designed to test commonsense knowledge, while OpenBookQA comprises 5,957 four-way multiple-choice questions that assess a model's ability to reason with science knowledge. The key difference between these datasets lies in the nature of the knowledge being evaluated, making them complementary benchmarks for evaluating the reasoning capabilities of LMs across diverse domains.}
% we use the CommonsenseQA and OpenBookQA datasets to evaluate the performance of the candidate approaches. CommonsenseQA consists of 12,247 questions created by crowd-workers, which are designed to test commonsense knowledge through a 5-way multiple-choice QA task. OpenBookQA consists of 5,957 four-way multiple-choice questions designed to evaluate models' reasoning with elementary science knowledge.

%\subsection{Baselines}
Our evaluation can be divided into two parts. In the first part, we focus on \textbf{model performance}.
We compare~\methodname{} with two sets of baseline models on the CommonsenseQA and OpenBookQA. The first set comprises KG-augmented versions of RoBERTa-large. 
It includes the current SOTA LM+KG method on CommonsenseQA, MHGRN \citep{feng2020scalable}, KagNet~\citep{lin2019kagnet}, GconAttn~\citep{wang2019improving}, RGCN~\citep{schlichtkrull2018modeling}, RN~\citep{santoro2017simple},
QA-GNN \citep{yasunaga2021qagnn}, GreaseLM \citep{zhang2022greaselm}.
The second set consists of LLM Llama-2-7B~\citep{touvron2023llama-2}, which demonstrates the capabilities of LMs without interpretation. The LMs we used are from Huggingface\footnote{https://huggingface.co/}.

% consists of fine-tuned LM RoBERTa-large \citep{liu2019roberta} \jianda{and LLM Llama-2-7B~\citep{touvron2023llama-2}}, which demonstrate the capabilities of LMs without interpretation. 
% The second set of baseline models includes KG-augmented versions of RoBERTa-large, using ConceptNet as the source of common sense knowledge and following the approach in \citep{lin2019kagnet}. The third set of baseline models is the current SOTA commonsense reasoning method on CommonsenseQA, MHGRN \citep{feng2020scalable}, QA-GNN \citep{yasunaga2021qagnn}, GreaseLM \citep{zhang2022greaselm}. The LMs we used are from Huggingface\footnote{https://huggingface.co/}.

In the second part, we evaluate~\methodname{} on \textbf{explanation ability}. To establish a baseline for comparison, two prior works, namely PathReasoner \citep{ZHAN2022107612} and Explanations for CommonsenseQA (ECQA) \citep{aggarwal-etal-2021-explanations}, were employed as benchmarks. These works are recognized for providing natural and comprehensible explanations \zc{for model's reasoning behavior}.

%\subsection{Experiment Settings}
% \jianda{We train two variants of \methodname{},
% % our method, \methodname{}(RoBERTa-large) and \methodname{}(Llama-2-7B),
% which comprises with RoBERTa-large and Llama-2-7B, respectively, as the LM.} 

We train two variants of \methodname{}, each utilizing a different LM: RoBERTa-large and Llama-2-7B, respectively. We train the models on a single NVIDIA A100 GPU. Further details regarding the model hyperparameters, training settings, and knowledge graph preprocessing steps can be found in Appendix \ref{app:experimental_settings}.



\subsection{Experimental Results}



We present our experimental results in Table~\ref{tab:acc} and Table~\ref{tab:bookacc}, where the accuracy of~\methodname{} is evaluated on the CommonsenseQA and OpenBookQA datasets. Our empirical findings indicate that~\methodname{} leads to consistent improvements in performance compared to existing baseline methods on both datasets. Specifically, the test performance on CommonsenseQA is improved by 4.71\% over the prior best LM+KG method, GreaseLM, 5.35\% over the included KG augmented LMs, and 7.12\% over fine-tuned LMs. 
The test performance achieves comparable results to the prior best LM+KG method, GreaseLM, on OpenBookQA. 
However, our proposed~\methodname{} utilizing LLM Llama-2 significantly outperforms baseline LM+KG method by 8.6\%. 
It is worth noting that LLM \mbox{Llama-2} is trained with a huge amount of data, so that 
finetuning LLM Llama-2 without KG is able to achieve comparable results to ~\methodname{}.

\noindent
\begin{minipage}[htbp]{0.48\textwidth}
  \centering
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{lrr}
    \toprule
    \textbf{Method}                             & \textbf{IHdev-Acc.} & \textbf{IHtest-Acc.} \\
    \midrule
    \textbf{Baselines} \citep{feng2020scalable} &                     &                      \\
    MHGRN (2020)                                & 73.69\%             & 71.08\%              \\
    KagNet (2019)                               & 73.47\%             & 69.01\%              \\
    GconAttn (2019)                             & 72.61\%             & 68.59\%              \\
    RGCN (2018)                                 & 72.69\%             & 68.41\%              \\
    RN (2017)                                   & 74.57\%             & 69.08\%              \\
    \midrule
    \textbf{Baselines} (our implementation)     &                     &                      \\
    GreaseLM (2022)                             & 76.17\%             & 72.60\%              \\
    QA-GNN (2021)                               & 74.94\%             & 72.36\%              \\
     Llama-2-7B (w/o KG) (2023)              & 81.49\%    & 78.24\%     \\
    \midrule
   ~\methodname{} (RoBERTa-large)            & \textbf{77.97\%}    & \textbf{77.31\%}     \\ 
   ~\methodname{} (Llama-2-7B)               & \textbf{82.88\%}    & \textbf{77.36\%}     \\
    \bottomrule
  \end{tabular}
  }
  \captionof{table}{Comparative performance of \methodname{} on CommonsenseQA In-House Split: Our model surpasses all baselines, achieving accuracies of 77.97\% and 77.31\% with RoBERTa-large, and 82.88\% and 77.36\% with Llama-2-7B on IHdev and IHtest, respectively. While the LMExplainer (Llama-2-7B) closely matches the performance of Llama-2-7B, it offers the benefit of explainability.}
  \label{tab:acc}

\end{minipage}\hfill
\begin{minipage}[htbp]{0.48\textwidth}

  \centering
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{lrr}
    \toprule
    \textbf{Method}                             & \textbf{Dev-Acc.} & \textbf{Test-Acc.} \\
    \midrule
    \textbf{Baselines} \citep{feng2020scalable} &                   &                    \\
    MHGRN (2020)                                & 68.10\%           & 66.85\%            \\
    GconAttn (2019)                             & 64.30\%           & 61.90\%            \\
    RGCN (2018)                                 & 64.65\%           & 62.45\%            \\
    RN (2017)                                   & 67.00\%           & 65.20\%            \\
    \midrule
    \textbf{Baselines} (our implementation)     &                   &                    \\
    GreaseLM (2022)                             & 71.80\%           & 70.80\%   \\
    QA-GNN (2021)                               & 63.00\%           & 59.80\%            \\
    Llama-2-7B (w/o KG) (2023)            & 80.60\%    & 78.40\%     \\
    \midrule
   ~\methodname{} (RoBERTa-large)                          &\textbf{ 69.20\%}           & \textbf{68.00\%}           \\
   ~\methodname{} (Llama-2-7B)                          & \textbf{80.80\%}           & \textbf{79.40\%}            \\   
    \bottomrule
  \end{tabular}
  }
  \captionof{table}{Performance Comparison on OpenBookQA: 
  ~\methodname{} demonstrates competitive results against various baselines, closely matching the best. Notably, while GreaseLM is optimized for accuracy in QA tasks, \methodname{} focuses on explaining the reasoning process behind its answers. \methodname{} + Llama-2-7B achieves the best performance, combining high accuracy with the added value of explainability. }
  \label{tab:bookacc}
  \vspace{8pt}
\end{minipage}

Beyond achieving high accuracy, \methodname{} also provides transparency in reasoning, enhancing human understanding of the LM's decision-making behavior.
% It is worth noting that GreaseLM is specifically designed to improve accuracy for QA tasks, while our~\methodname{} focuses on providing explanations for the reasoning process. Despite this difference in focus, our~\methodname{} not only offers insight into the underlying reasoning but also demonstrates an improvement in performance. This finding highlights the potential benefits of incorporating explainability into the model design, as it may lead to enhanced performance in addition to fostering a better understanding of the decision-making process.
This finding highlights the potential benefits of incorporating interpretability components that narrow the reasoning space into the model architecture. By focusing on the most relevant elements, these components not only facilitate a better understanding of the decision-making behavior but also contribute to enhance model performance.
To more thoroughly understand the influence of various components of \methodname{} on its overall performance, we have conducted a ablation study in Appendix \ref{sec:ablation}.


\vspace{-1mm}
\subsection{Explanation Results}

% \input{Main/pos_neg_box}
\input{Main/expl_example}

\begin{wraptable}{r}{0.6\textwidth} 
  \vspace{-14mm}
  \centering
  \resizebox{0.58\textwidth}{!}{%
    \begin{tabular}{@{}c|l@{}}
      \toprule
      \textbf{\makecell{PathReasoner \\ \citep{ZHAN2022107612}}} & \begin{tabular}[c]{@{}l@{}} \textbf{Output (Model Prediction):} ``A. Reading'' \\\textbf{Explanation:} \\
      quietly [related to] quiet [at location] a library [used for] reading \\\end{tabular} \\ \midrule
      \textbf{\makecell{ECQA \\\citep{aggarwal-etal-2021-explanations}}} & \begin{tabular}[c]{@{}l@{}} \textbf{Output (Model Prediction):} ``A. Reading'' \\\textbf{Explanation:} \\ While meditating and sleeping, eyes don't move, eyes are closed. \end{tabular} \\ \bottomrule
    \end{tabular}%
  }
  \caption{Examples from PathReasoner and ECQA illustrate the LM's reasoning, offering insights yet lacking in clarity and faithfulness. See Appendix (Table~\ref{app:example}) for a detailed comparison.}
  \label{tab:explanation}
  % \vspace{-2mm}
\end{wraptable}


Our explanation results are in Figure~\ref{fig:correct_expl} and Table~\ref{tab:explanation}. 
The LM $f_{LM}$ used in our explanation is RoBERTa-large, paired with GPT-4-turbo \citep{OpenAI2023GPT4TR} as the explanation generator and LM Debugger. Note that  $f_{LM}$ serves as a representative example and can be replaced with other LMs as required. To further demonstrate the effectiveness of our approach, we compare it with two SOTA methods, PathReasoner \citep{zhan2022pathreasoner} and ECQA \citep{aggarwal-etal-2021-explanations}. PathReasoner uses structured information to explain the reasoning path, while ECQA first is created by human-annotated explanations and then leverages a generation model to organize the final explanation. 
% In Table~\ref{tab: explanation}, we present the inputs and results of our approach, which include ranked \emph{reason-elements} and explanations of the reasoning process. These examples highlight the ability of~\methodname{} in generating comprehensive and interpretable explanations for the LMs. Complete results are shown in Appendix \ref{sec:appendix-example}.

% In comparison to PathReasoner explanations, which only provide structured reasoning paths that are non-informative and require manual selection of a specific path, our proposed approach not only offers a complete reasoning path but also provides a justification for the predicted answer. 
As illustrated in Table~\ref{tab:explanation} and Table~\ref{app:example} (complete results), PathReasoner presents four reasoning paths, including redundant paths, making it difficult to identify the faithful reasoning path. % consists of human-annotated explanations that provide highly accurate descriptions of the reasoning process. However, its explanations are 
While ECQA simply combines the positive and negative examples, which fails to explain the actual decision-making process of the LM. 
% In contrast, our explanations are not a mere combination of sentences but are inferred and logically derived. \methodname{} provides a more comprehensive and accurate depiction of the LM's actual reasoning behavior and improves the overall interpretability. 
% and usefulness of the generated explanations. 
% In addition, the \emph{why-not-choose} explanation explains why the model does not choose other answers, which gives people a better understanding of the model’s reasoning behavior and increases the transparency of the LM.
Compared to other explanation methods, \methodname{} provides a more comprehensive and structured explanation, covering both the chosen answer and the reasons for not selecting other options. The LM Debugger's evaluation and advice for improvement are unique features of \methodname{}, offering insights into the explanation's quality and potential areas for enhancing the LM's performance.
We provide a comprehensive case study in Appendix \ref{app:case}.


% In contrast, our method provides a clear and concise natural language explanation for the chosen answer (\emph{why-choose} explanation), which greatly enhances the understandability and smoothness of the explanation.

% The ECQA consists of human-annotated explanations that provide highly accurate descriptions of the reasoning process. However, as shown in Table~\ref{tab:explanation} and Table~\ref{app:example}, its explanations are simply a combination of positive and negative examples provided by humans, which fails to illustrate the actual decision-making process of the model.
% While this approach can generate high-quality explanations from a human perspective, it fails to illustrate the actual decision-making process of the model. 
% In contrast, our explanations are not a mere combination of sentences but are inferred and logically derived.~\methodname{} provides a more comprehensive and accurate depiction of the reasoning process and improves the overall interpretability and usefulness of the generated explanations. In addition, the \emph{why-not-choose} explanation explains why the model does not choose other answers, which gives people a better understanding of the model’s reasoning process and increases the transparency of the model.
% For an in-depth understanding, a detailed case study is available in Appendix \ref{sec:case}.

% \vspace{-1mm}
\subsection{Evaluation of Explanations}
% \vspace{-1mm}
We evaluate the \methodname{} generated explanations from two perspectives: simulated reasoning and user perception. 
% The simulated reasoning perspective evaluates the explanations' alignment with the reasoning process of an ``prefect'' LM, using the LM Debugger. 
% This evaluation helps users understand the reasoning behind the explanations, assess the LM's behavior, and identify the potential limitations. 
% This evaluation helps identify areas where the actual LM's capabilities can be improved to more closely mirror the expected behavior of the simulated 'ideal' LM. 
% The user perception perspective focuses on the effectiveness of \methodname{} in helping users understand the LM's reasoning process. 
% We evaluate the \methodname{} generated explanations from two aspects: quantitative and qualitative. The quantitative aspect assesses the quality of explanations using the LM Debugger, which serves as an evaluator of explanation quality and helps users interpret and trust the explanations provided by \methodname{}. The qualitative aspect focuses on the effectiveness of \methodname{} in helping users understand the LM's reasoning process. 
We randomly select 20 QA pairs from CommonsenseQA dataset, using RoBERTa-large as the LM ($f_{LM}$, acc=77.97\%) and GPT-3.5-turbo as the explanation generator and GPT-4-turbo as LM Debugger.


\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \vspace{-12mm}
    \includegraphics[width=1\linewidth]{figure/explanation_quality.pdf}
    \caption{Simulated reasoning scores split by LM's prediction correctness.}
    \label{fig:explanation_quality}
    \vspace{-5mm}
\end{wrapfigure}
\zc{\subsubsection{Simulated Reasoning Perspective}}
The simulated reasoning perspective evaluates the explanations' alignment with the reasoning process of a ``prefect'' LM, using the LM Debugger. 
This evaluation helps users understand the reasoning behind the explanations, assess the LM's behavior, and identify the potential limitations. 

We evaluate the explanations across four key dimensions from a debugging perspective, following the evaluation metrics of \citep{bills2023language}: faithfulness, completeness, minimality, and accuracy. Each dimension is scored on a scale from 1 to 5, utilizing strict scoring criteria, where higher scores indicate better performance. The details of the scoring criteria and effectiveness evaluation of LM Debugger are shown in Appendix \ref{app:lm_debugger}.






% Figure~\ref{fig:} presents the effectiveness of the LM Debugger.
% We set a threshold score of 3. Explanations with scores equal to or above this threshold are considered satisfactory. 

% The results demonstrate the high effectiveness of the LM Debugger in accurately identifying the quality of explanations based on the model's prediction correctness. For both correct and incorrect predictions, the LM Debugger achieves 100\% accuracy in assessing faithfulness and accuracy. It also performs well in identifying completeness, with 88.24% accuracy for correct predictions and 100% for incorrect predictions.





% \begin{table}[htb]
% \centering
% \begin{tabular}{lcc}
% \hline
% \textbf{Dimension} & \textbf{Correct Prediction} & \textbf{Incorrect Prediction} \\
% \hline
% Faithfulness & 100\% & 100\% \\
% Accuracy & 100\% & 100\% \\
% \hline
% \end{tabular}
% \caption{Percentage of cases where the LM Debugger correctly identifies the reliability of the model's predictions based on explanation quality.}
% \label{tab:explanation_quality}
% \end{table}

% ---
% \begin{table}[htb]
% \centering
% \begin{tabular}{lcccc}
% \hline
% \textbf{Dimension} & \textbf{Correct Prediction} & \textbf{Incorrect Prediction} \\
% \hline
% Faithfulness & 3.59 & 2.33 \\
% Completeness & 2.94 & 2.00 \\
% Minimality & 3.94 & 2.67 \\
% Accuracy & 3.29 & 1.67 \\
% \hline
% \end{tabular}
% \caption{Explanation quality scores split by prediction correctness.}
% \label{tab:explanation_quality_correctness}
% \end{table}

The simulated reasoning scores are presented in Figure \ref{fig:explanation_quality}, showing a striking difference in scores between correct and incorrect predictions. This allows users to easily distinguish between reliable and unreliable LM results. We provide a comparative case study in Appendix \ref{app:case}, showcasing how explanations differ when the LM's prediction is correct or incorrect, highlighting \methodname{}'s effectiveness in generating informative explanations.




% \subsubsection{Evaluation of Explanation}
\zc{\subsubsection{User Perception Perspective}}
\begin{table*}[ht]
\resizebox{\textwidth}{!}{%
\footnotesize
\begin{tabular}{@{}l|c|ccccccc@{}}
\toprule
{} & {} & \multicolumn{7}{c}{\textbf{Evaluation Metrics}} \\
\cline{3-9}
\multirow{-2}{*}{\textbf{Evaluator}} & \multirow{-2}{*}{\textbf{Strong AI Understanding?}} & \textbf{Overall Quality} & \textbf{Understandability} & \textbf{Trustworthiness} & \textbf{Satisfaction} & \textbf{Sufficiency of Detail} & \textbf{Completeness} & \textbf{Accuracy} \\ \midrule
Human Experts & \textcolor{green}{\ding{52}} & 0.91 & 0.97 & 0.95 & 0.89 & 0.98 & 0.97 & 0.93 \\ 
Crowdsourcing & \textcolor{red}{\ding{56}} & 0.85 & 0.89 & 0.86 & 0.80 & 0.83 & 0.81 & 0.85 \\ 
GPT-3.5 & \textcolor{green}{\ding{52}} & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 \\
GPT-4 & \textcolor{green}{\ding{52}} & 0.90 & 0.93 & 0.87 & 0.87 & 0.88 & 0.87 & 0.88 \\ \bottomrule
\end{tabular}


}
\caption{Evaluation by automated evaluator GPT-3.5-turbo, GPT-4-turbo, human experts, and crowdsourcing on 7 evaluation metrics. \zc{Our explanations are accessible to all regardless of AI knowledge.}}
\vspace{-10pt}
\label{tab:simulator_score}
\end{table*}

The user perception perspective focuses on the effectiveness of \methodname{} in helping users understand the LM's reasoning process. Our user perception evaluation includes three approaches: human expert review, crowdsourcing, and automated methods. Our expert panel consists of individuals with graduate-level education, taught in English, and a minimum of three years of research experience in NLP. We also hire 50 general users through the crowdsourcing platform Prolific~\footnote{https://www.prolific.com}, ensuring a gender-balanced participant pool of native English speakers, all possessing at least a high school education. For automated evaluation, we utilize GPT-3.5-turbo and GPT-4-turbo to further validate the explanations. The evaluation follows the methodology in \citep{hoffman2018metrics} and involves seven evaluative dimensions: overall quality, clarity, credibility, satisfaction, detail adequacy, completeness, and accuracy. Participants rate these aspects using a three-point Likert scale, and scores are normalized to a range $[0,1]$, with higher scores indicating better quality. 

The scores are shown in Table \ref{tab:simulator_score}. Human experts highly commend the Understandability, Trustworthiness, and Completeness (above 0.95) of our explanations. They acknowledge our adeptness in producing comprehensive and reliable explanations. The crowdsourcing results are slightly lower across all metrics. This outcome potentially mirrors the diverse and less specialized viewpoints of the general public. Overall, the general users are able to understand how LMs reason through our explanations. Automated evaluators GPT-3.5-turbo and GPT-4-turbo provide evaluations of our explanations' quality that closely match human expert assessments, demonstrating consistency across key metrics.
GPT-3.5-turbo agrees with our strong performance in Overall Quality, Understandability, and Accuracy, with each scoring 0.98. Similarly, GPT-4-turbo gives a comparable evaluation, with its highest score in Understandability (0.93). 

% The notably lower scores in ``Irrelevance'' indicate incorrect inferences result in irrelevant information in our explanations. This issue, easily identified by evaluators, highlights a potential area for future human-centered explanations.



These results highlight the high quality of our explanations. Notably, even participants without a strong AI background provided high scores, indicating the accessibility and effectiveness of \methodname{}'s explanations for a wide audience. The consistency across key metrics emphasizes the effectiveness and reliability of the explanations generated by \methodname{}. The details of the automated evaluation process and questionnaires are outlined in Appendix \ref{appendix:auto-eval}.

% These results highlight the high quality of our explanations. 
% The consistency across key metrics emphasises the effectiveness and reliability of the explanations generated by \methodname{}. 
% The details of the automated evaluation process and questionnaires are outlined in Appendix \ref{appendix:auto-eval}.


% \subsubsection{Impact of Explanation Generators}
% In this section, we investigate the robustness of \methodname{} against variations in explanation generators. We focus on three generators: Llama-2-70B, GPT-3.5-turbo, and GPT-4. We present the results in Figure \ref{fig:example}.

% \begin{figure}[t]
%   \begin{center}
%     \includegraphics[width=0.49\textwidth]{figure/example.pdf}
%   \end{center}
%   \caption{The \textit{why-choose} explanations generated by Llama-2-70B and GPT-4. The example of GPT-3.5-turbo is shown in Table \ref{tab:explanation}. They exhibit a high degree of semantic consistency. The similarity scores are detailed in Appendix \ref{appendix:generators}. All experimental settings are the same.}
%   \label{fig:example}
%   \vspace{-10pt}
% \end{figure}

% Due to the limited space, we include detailed results in the Appendix \ref{appendix:generators}. The explanations generated by the three models are largely consistent in semantic meaning, demonstrating that under our constrained prompt instruction, these models primarily functioned as ``translators''. They convert the reasoning process into human-understandable language. However, it is important to note that the capability of the generator influenced the readability of the explanations. For instance, Llama-2 tends to produce more repetitive language (in red), while GPT-3.5-turbo and GPT-4 show consistency and conciseness. Based on these observations, we recommend using GPT-3.5-turbo or GPT-4 as the explanation generator for optimal clarity.

