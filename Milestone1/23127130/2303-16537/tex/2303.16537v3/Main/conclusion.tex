\section{Conclusions}
In this paper, we propose~\methodname{}, a novel model that incorporates an interpretation module to enhance the performance of LMs while also providing grounded and trustworthy explanations of the LM's reasoning behavior.
Our explanation results are presented in a logical and comprehensive manner, making it easier for humans to understand the model's reasoning in natural language. 
\zc{The debugging process helps users to refine LMs for improved performance, marking a step towards democratizing AI.}
Our experimental results demonstrate superior performance compared to prior SOTA works across standard datasets in the commonsense domain. Our analysis shows that~\methodname{} not only improves the model's performance but also provides humans with a better understanding of the model.


% \paragraph{Limitation.}
% Although datasets like CommonsenseQA and OpenBookQA cover various topics, they don't fully represent the diverse domains where LMs are applied. Future work will evaluate \methodname{}'s effectiveness in specialized fields such as legal and medical.





\section{Ethics Statement}
The primary ethical concern in our work relates to the use of LLMs for explanation generation.
Specifically, if the explanation generator is of low quality or deemed unsafe, it presents a significant risk. This could adversely affect the integrity and reliability of the content and style of the explanations. We acknowledge the importance of ensuring the quality and safety of the explanation generator to maintain ethical standards in our outputs and to prevent the dissemination of potentially harmful or misleading information.