
\section{Experimental Settings}\label{app:experimental_settings}

We set our GNN module to have 200 dimensions and 5 layers, where a dropout rate of 0.2 is applied to each layer. We train the model on a single NVIDIA A100 GPU with a batch size of 64. The learning rates for the language model and the GNN module are set to $1e-5$ and $1e-3$, respectively. 
We opt for the RAdam optimizer for RoBERTa-large, while employing AdamW for Llama-2-7B.
These settings are adopted in the first part of the evaluation to investigate the performance of the GNN module.


We employ ConceptNet \citep{speer2017conceptnet} as our external knowledge source for CommonsenseQA and OpenBookQA. ConceptNet contains a vast amount of information with 799,273 nodes and 2,487,810 edges, which provides a valuable resource for improving the accuracy of QA systems. We extract the $G_k$ with a hop size of 2, and subsequently prune the obtained graph to retain only the top 200 nodes.
