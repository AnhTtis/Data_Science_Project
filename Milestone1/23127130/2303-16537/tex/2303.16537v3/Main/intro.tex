\section{Introduction}

Language models (LMs) have recently attracted significant attention due to their impressive state-of-the-art (SOTA) performance on various natural language processing (NLP) tasks \citep{Zhao2024,liu2023pre,zhao2023survey,weifinetuned,zhou2022learning}. These tasks include language translation \citep{barrault2023seamless}, text generation \citep{dekoninck2024controlled}, and text classification \citep{liu-etal-2023-enhancing}, among others. One of the main advantages of LMs is their ability to capture the nuances and the complexity of human languages.

% \begin{figure}[t]
%   \begin{center}
%     \includegraphics[width=0.48\textwidth]{figure/intro.pdf}
%   \end{center}
%   \caption{\methodname{} demystifies the decision-making process of LMs for better human understanding. It includes a graph surrogate for structural reasoning, attention-based interpretation for decision rationales, and an explanation generator that provides explanations of \textit{``why-choose''} and \textit{``why-not-choose''} to bridge the gap between LMs and human understandability.}
%   \label{fig:intro}
%   \vspace{-10pt}
% \end{figure}

However, a major limitation of LMs is a lack of interpretability \citep{meng2022interpretability}. It is often difficult to provide explanations about their ``black box'' decision-making processes. LMs use techniques such as attention mechanisms, which allow them to focus on specific parts of the input data when making decisions \citep{ devlin2019bert,liu2019roberta,mrini-etal-2020-rethinking}. These mechanisms can be difficult for people to understand, as they produce abstract and non-transparent internal learning representations \citep{jain2019attention}. %But these mechanisms can elude human intuition, and produce internal learning representations that are abstract and non-transparent to humans \citep{jain2019attention}.
For example, a model embedding might capture relationships and meanings as a result of passages through millions of neurons. However, such meanings might not be immediately apparent to humans. This lack of interpretability poses a challenge to mission critical domains (e.g., healthcare \citep{jung2023essential} and online education \citep{farrow2023possibilities}) as it hampers users' trust on the responses made by the models.
%Improving model interpretability has the added benefit of addressing issues such as fairness, privacy, and safety. Methods that explain the behaviors of LMs can help overcome the black-box nature of neural networks.



Due to the opaque nature of LMs, a promising approach for explaining how they work is by generating explanations on a more transparent surrogate (e.g., a knowledge graph (KG)).
\citep{geng2022path} leverages a KG as a submodel to ground the explainability of LM-based recommendations.
Such methods provide insights into how to interpret the complex model by translating it into more comprehensible counterparts. Attention-based explanations have also gained significant attention. For example, \citep{vig-2019-multiscale} proposes a visualization method for attention in the LM, enhancing our understanding of how these models allocate focus across input tokens. However, \citep{zini2022explainability} pointed out that attention is not equal to explanation. Individual token representations are not enough. A surrogate that maps tokens to specific knowledge elements that align with the reasoning process of the LM is imperative.

% In this paper, we explore the potential of using explanations to serve two purposes (Figure \ref{fig:intro}): \textbf{1) helping humans in understanding the model}, and \textbf{2) enhancing the model's understanding of the task at hand through interpretation during the explanation process}. 

To address the limitations of current approaches, we propose the~\methodname{} approach, a novel method for \zc{explaining the mysteries behind LM's decisions. Unlike other methods that focus on single token or attention, \methodname{} treats reasoning as an integrated process. }
% explaining the responses made by LMs.
It is designed to efficiently locate the most relevant knowledge within a large-scale KG via the graph attention neural network (GAT) \citep{velickovic2018gat} to extract key decision signals reflecting the rationale behind the responses made by LMs. \zc{Through the graph framework, we narrow the reasoning space and reduce the potential noise, thereby improving the inference accuracy and providing a transparent structure for explaining model's reasoning behavior. Our method achieves grounded explanations by anchoring the reasoning process in verifiable knowledge, which reduces hallucinations. Furthermore, we incorporate a debugging process to quantitatively understand the generated explanations and provide suggestions for future enhancements.}

We experimentally evaluate~\methodname{} on the question-answering (QA) task using the CommonsenseQA~\citep{talmor2019commonsenseqa} and OpenBookQA~\citep{mihaylov2018can} datasets. The results demonstrate that~\methodname{} outperforms most LM+KG QA methods and large LMs (LLMs) on CommonsenseQA and OpenBookQA.
\zc{Furthermore, we show that~\methodname{} surpasses prior explanation methods by providing insights of the reasoning behind a LM's decision in a human-understandable form and LM development suggestions. We open up new possibilities for AI systems to be not only more transparent and reliable but also equitable.}
To the best of our knowledge,~\methodname{} is the Ô¨Årst work capable of leveraging graph-based knowledge in generating natural language explanations on the rationale behind LM behaviors.