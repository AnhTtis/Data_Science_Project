\section{The Proposed \methodname{} Approach}
The~\methodname{} architecture is shown in Figure~\ref{fig:archi}. It consists of three main steps: \textbf{(1) key element extraction and building} (Section~\ref{sec:extraction}), \textbf{(2) element-graph interpretation} (Section~\ref{sec:interpretation}), and \textbf{(3) explanation generation and debugging} (Section~\ref{sec:explanation}). 
% In the first step, we extract the relevant elements from the input data and the knowledge retrieved from the KG, and build an element-graph representation. In the second step, we leverage GAT to interpret the element-graph and identify the \emph{reason-elements} behind LM predictions. In the third step, we design an instruction-based method to generate human-understandable explanations of the decision-making process based on the identified \emph{reason-elements}.
\methodname{} is flexible and applicable to a range of LMs (e.g., RoBERTa \citep{liu2019roberta}, GPT-2 \citep{radford2019language}, and Llama-2 \citep{touvron2023llama-2}).


\begin{figure*}[h]
  \begin{center}
    %\framebox[4.0in]{$\;$}
    %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    \includegraphics[width=1\textwidth]{figure/archi.pdf}
  \end{center}
  \caption{\zc{The~\methodname{} architecture. \methodname{} addresses the lack of transparency in black-box LMs by providing interpretable explanations for their reasoning behavior. Given an input context, \methodname{} retrieves relevant knowledge from a KG, integrates it with LM embeddings to construct a surrogate subgraph, and interprets it using graph attention to identify key reason-elements contributing to the LM's decision. Based on these elements, LMExplainer generates explanations clarifying why the LM chose the predicted answer and ruled out alternatives. The LM Debugger further enhances user trust by evaluating the faithfulness, completeness, minimality, and accuracy of the generated explanations.} }
  % The~\methodname{} architecture. Given an input content $z$, we first generate language embeddings using a pre-trained LM. Simultaneously, it retrieves relevant knowledge from a KG to construct a subgraph. The language embeddings and subgraph are then combined to obtain GNN embeddings. This combined representation is then passed through a GAT to obtain the attention. The attention serves two purposes. Firstly, it weighs the importance of the GNN embeddings and is used with the language embeddings for the final prediction. Secondly, they are used to generate explanations by highlighting the most important parts of the reasoning process. }
  \label{fig:archi}
  \vspace{-10pt}
\end{figure*}

\subsection{Task Definition}
We define the task of generating reasoning-level explanations for inferences made by LMs. 
% As an example, we use a QA task. 
\zc{Given a LM $f_{\text{LM}}$, input $x$, and a predicted output $y'$, the goal is to generate a natural language explanation $E$ that elucidates the reasoning process behind the prediction. The task can be expressed as:
    \begin{equation}
        E \gets \text{GenerateExplanation}(f_{\text{LM}}, x, y')
    \end{equation}
As an example, we use a QA task, where input $x$ consists of question $q$ and a set of answer choices $\mathcal{A} = {a_1, a_2, \ldots, a_n}$, the LM $f_{\text{LM}}$ predicts an answer $y' \in A$. The explanation $E$ consists of $E_{0}$ and $E_{1}$. $E_{0}$ is for why $f_{LM}$ chooses $y'$ and $E_{1}$ is for why $f_{LM}$ does not choose other choices $\mathcal{A} \setminus \{ y'\}$.}

% Given an LM $f_{LM}$ with input question $q$, answer choice set $\mathcal{A}$ and predicted answer $y' \in \mathcal{A}$, the goal is to generate an explanation $E_{0}$ for why $f_{LM}$ chooses $y'$ and an explanation $E_{1}$ for why $f_{LM}$ does not choose other options $\mathcal{A} \setminus \{ y'\}$. This task can be expressed as:
% \begin{equation}
%   (E_{0},E_{1}) \gets \textit{GenerateExplanation}(f_{LM}, q,\mathcal{A}, y').
% \end{equation}


\subsection{Key Elements Extraction and Building} \label{sec:extraction}
\zc{The reasoning patterns of LMs can be traced by key elements that reveal their decision-making logic \citep{huang2023towards}.
To capture these complex reasoning signals, we propose a more structured and transparent surrogate method that effectively maps the LM's reasoning process into an interprerable representation. 
We first tokenize the input $x$ and let $z$ denote the resulting tokens.}
% To capture these essential elements, we first tokenize a set of sentences $\{q\} \cup \mathcal{A}$ into tokens $\{x_1, x_2,\ldots, x_n\}$. Let $z$ denote this set of resulting tokens. 
The tokens $z$ are then used to construct a multi-relational graph, following the approach from~\citeauthor{yasunaga2021qagnn}.
\zc{We retrieve the $L$-hop neighborhood graph $G_k$ of $z$ from a large-scale knowledge graph $\mathcal{G}$, such as ConceptNet \citep{speer2017conceptnet}. To narrow down the reasoning space and align it with the LM's own reasoning patterns, we construct a relevant subgraph of $\mathcal{G}_k$, referred to as the \textit{element-graph} $\mathcal{G}_e$, by leveraging the LM's knowledge to guide the pruning process. This pruning step is crucial for two reasons. First, it effectively focuses on the most informative and relevant elements, ensuring that the selected elements are consistent with the LM's decision-making process. Second, it captures and reflects the LM's capabilities, preferences, and potential biases, making them transparent and interpretable.

For each node $v$ in $\mathcal{G}_k$, we define a relevance score $s_v$ that measures the importance of the node with respect to the input text $x$. The relevance score is computed using the LM's probability function $f_{\text{prob}}$ as follows:
\begin{equation}
    s_v = f_{\text{prob}}(z_{\text{emb}}, v_{\text{emb}})
\end{equation}
where $z_{\text{emb}}$ and $v_{\text{emb}}$ are the embeddings of the input tokens $z$ and the node $v$, respectively, obtained from the LM. The probability function $f_{\text{prob}}$ takes the concatenated embeddings as input and outputs a scalar value indicating the relevance of the node to the input text. We select the top-$K$ nodes based on their relevance scores to construct the \textit{element-graph} $\mathcal{G}_e$. The procedure for constructing the \textit{element-graph} is outlined in the Appendix (Algorithm \ref{algo:subgraph}).

}


% Firstly, the $L$-hop neighbor $G_k$ of $z$ is extracted from ConceptNet \citep{speer2017conceptnet} to integrate external knowledge, following the approach from ~\cite{feng2020scalable}. However, $G_k$ can still contain a large number of edges, which lead to a huge reasoning space. Our main goal is therefore to construct a relevant sub-graph of $G_k$, referred to as the \textit{element-graph} $G_e$.
% This allows us to identify essential elements that play a key role, and analyze the relations among them.
% We integrate the embedding from LMs to guide the pruning for $G_k$. Specifically,
% for every node $v$ in $G_k$, we define an associated score for pruning purposes, which is expressed as:
% \begin{equation}
%   v_{score} = f_{prob}(z_{emb},  v_{emb}),
% \end{equation}
% where $f_{prob}$ is the probability computation function of the pre-trained LM, $z_{emb}$ and $v_{emb}$ are the embeddings derived from textual representations of $z$ and $v$ respectively, are concatenated to $f_{prob}$. The score captures the correlation between the node $v$ and input content $z$, and is used to remove irrelevant nodes. We select the top $K$ nodes based on their scores. The resulting pruned graph is denoted by $G_e$, which is referred to as the \textit{element-graph}. We outline the procedure for constructing the \textit{element-graph} in the Appendix (Algorithm \ref{algo:subgraph}).


 \subsection{Element-Graph Interpretation}  \label{sec:interpretation}
Given an element-graph $G_e$, we follow~\cite{yasunaga2021qagnn} to extract the representation for graph reasoning. The method leverages the GAT~\citep{velickovic2018gat} to preserve the structure and context of the input through the connections between the nodes.
\citeauthor{velickovic2018gat} use the graph attention operation to take a set of node features as input and output a corresponding set of new node features. Formally, the input to the $k \text {th}$ attention layer is denoted as $\boldsymbol{h}_k=\{h_{k1},h_{k2}, \ldots,h_{kN}\}$, where $h_{kj} \in \mathbb{R}^F$ is the intermediate feature for node $v_j$, $F$ is the input feature size and $N$ is the number of nodes in the graph. The attention layer outputs a new set of corresponding node features, $\boldsymbol{h}_{k+1}=\{h_{k+1,1}, h_{k+1,2},\ldots, h_{k+1,N}\}$ with $h_{kj} \in \mathbb{R}^{F^{'}}$.

A parameterized transformation $m: \mathbb{R}^F \xrightarrow{} \mathbb{R}^M $ is first applied to $\boldsymbol{h}_k$ to generate the transformation $m(\boldsymbol{h}_k)$. A parameterized self-attention mechanism $a: \mathbb{R}^F \times \mathbb{R}^F \xrightarrow{} \mathbb{R}$ is then used to obtain attention scores on $\boldsymbol{h}_k$. To retain structural information within the graph, attention scope for node $v_i$ is limited to nodes in its 1-hop neighborhood which is denoted as $\mathcal{N}_i$. Furthermore, the attention scores are normalized over the neighborhood $\mathcal{N}_i$ to generate attention coefficients:
\begin{equation}
  \alpha_{ij} = \frac{\exp(a(h_{ki},h_{kj}))}{\sum \limits_{v_l \in \mathcal{N}_i } \exp(a(h_{ki},h_{kl}))}.
  \vspace{-2mm}
\end{equation}

The output feature $h_{k+1,i}$ is an attentive linear combination of neighboring features with an optional activation:
\begin{equation}
  h_{k+1,i} = \sigma( \sum \limits_{v_j \in \mathcal{N}_i }  \alpha_{ij} m( h_{kj}))
    \vspace{-2mm}
\end{equation}

We build the graph reasoning network based on the above graph attention operation. Specifically, we employ a parameterized MLP $f_m$ for feature transformation. This MLP $f_m$ explicitly associates the node $v_i$ with its neighboring nodes $v_j \in \mathcal{N}_i$ by processing the feature $h_{ki}$, the recorded node type $u_i$ of node $v_i$ and the recorded relation types $r_{ij}$ to $v_j$, all of which are sourced from the element-graph. 
The attention scores $\alpha_{ij}$ are computed using another parameterized MLP that takes features $h_{ki},h_{kj}$, node and relation types $u_i,r_{ij}$ and node scores of $v_i$ and $v_j$ as input. The detailed information can be found in the Appendix~\ref{sec:graph}.

The output activation is implemented as a third 2-layer parameterized MLP $f_\sigma$ and the output features are thus obtained by:
\begin{equation}\label{eq:h_ve}
  h_{k+1,i} = f_\sigma ( \sum \limits_{v_j \in \mathcal{N}_i }  \alpha_{ij} m(h_{kj},u_i,r_{ij}))+ h_{kj},
    \vspace{-2mm}
\end{equation}
where the output feature size is the same as the input feature size.
The initial input features $\boldsymbol{h}_0$ is obtained by a linear transformation of node embeddings $v_{emb}$.



\subsubsection{Learning and Inference} \label{sec:inference}
In our task, each question $q$ is associated with a set of answer choices $\mathcal{A}$, with only one being the correct answer. We leverage the information from the LM embedding and the node embedding from the element-graph. Specifically, we define the probability of choosing an answer as $P(a|q)\propto \exp(\textit{MLP}(\mathbb{H}^{LM}, \boldsymbol{h}_K , \boldsymbol{\alpha}_K))$, where $\boldsymbol{h}_K$ is the output features and $\boldsymbol{\alpha}_K$ is the last-layer attention coefficients of a K-layer graph reasoning network given $G_e$ as input, and $\mathbb{H}^{LM}$ is the representation embedding from LM. The corresponding nodes (i.e., the \emph{reason-elements}) in $G_e$ are used to generate textual explanations about the decision-making process of the LM.
We optimize the model by minimizing the cross-entropy loss.


\subsection{Attention-aware Explanation Generation and Debugging} \label{sec:explanation}
% In prior work, ~\cite{chen2021kace} proposed a counterfactual explanation generator that pairs input text with counterfactual examples to fine-tune LMs to generate explanations in the form of ``why A and not B''. However, this approach does not provide a complete explanation of the LM decision-making process.

% In contrast, 
The \methodname{} explanation generator consists of three steps: (1) explanation component extraction, (2) instruction-based explanation generation and \zc{(3) enhancing explanation comprehension with LLM-based debugging. }

\subsubsection{Explanation Component Extraction}
We first extract the key components that are essential to the LM decision-making process. These key components consist of the final answer, \emph{reason-elements} and the attention $\alpha$. The final answer and \emph{reason-elements} are used to trace the important explanation nodes. The attention is used to sort the nodes and select the top $w$ nodes most relevant to the decision. 
% Each node represents an element, so we have $w$ most important components for the explanation. 
We use $\mathcal{Q}$ to represent the set of extracted key components. 
% The output, $E$, is a natural language explanation. 
We outline the procedure to interpret the \emph{element-graph} and extract the \emph{reason-elements} in the Appendix (Algorithm \ref{algo:element}).


\subsubsection{Instruction-based Explanation Generation}
We integrate the key component set $\mathcal{Q}$ into our instruction-based explanation generator. 
\zc{We leverage a set of predefined structures to guide the explanation generation. }
% To guide the generation of explanations, we leverage a set of predefined structures, including the input $q$ and $\mathcal{A}$, model predicted output $y'$, the trigger sentences, and the extracted key components $\mathcal{Q}$.
The \methodname{} explanation generation involves two stages: (1) \emph{why-choose} for explaining why the model chose the specific answer, and (2) \emph{why-not-choose} for explaining why the model did not choose the other explanations. 
\zc{In the \emph{why-choose} stage, we use instructions in the form of ``\textbf{Basis:} [TASK\_TYPE], \textbf{Input:} [$q$, $\mathcal{A}$], Output: [$y'$, $\mathcal{Q}$], \textbf{Explanation (Stage 1):} [$y'$]''. The \emph{why-choose} explanation is denoted as $E_0$. In the \emph{why-not-choose} stage, we use instructions in the form of ``\textbf{Explanation (Stage 2):} [$E_0$, $\mathcal{A}\setminus\{y'\}$]''. }
\zc{\textit{Basis}, \textit{Input}, \textit{Output}, \textit{Explanation (Stage 1)}, and \textit{Explanation (Stage 2)} are the 
predefined structures.}
% instructions for an explanation generator to generate the literal explanations of the reasoning process of a given LM.}
% Q, A, R, P and T are instructions for an explanation generator to generate the literal explanations of the reasoning process of a given LM. 
The generator outputs a natural language explanation in the form of a sentence or a paragraph. The details of our instruction are shown in Appendix (Figure~\ref{fig:instructions}).


\zc{\subsubsection{Enhancing Explanation Comprehension with LLM-based Debugging}}

The \textit{why-choose} and \textit{why-not-choose} explanations provide insights into the LM's reasoning behavior. However, for individuals without a strong background in AI, these explanations may still prove challenging to develop LMs. Inspired by the Transformer Debugger \citep{mossing2024tdb,bills2023language}, we include a debugging process named LM debugger to make explanations more accessible and understandable, regardless of the user's AI expertise. 
% an LM debugger tool associated with \methodname{} that leverages LLMs to make explanations more accessible and understandable, regardless of the user's AI expertise. 
The LM Debugger uses a LLM to simulate an ``prefect" LM (e.g., one with 100\% accuracy) and leverages explanations generated from \methodname{} to reconstruct the reasoning process of the target LM, which is the model under evaluation. It calculates the simulated reasoning score based on the differences between these simulations. We use a set of structured instructions to conduct the debugging process. The instructions are as follows: ``\textbf{System Prompt:} [MODEL\_NAME], \textbf{Explanation Content:} [TASK\_TYPE, $q$, $\mathcal{A}$, $y'$, $\mathcal{Q}$, $E$], \textbf{Evaluation Criteria:} \textit{[optional]}, \textbf{Advice Instruction:} \textit{[optional]}, \textbf{Debugging Instruction:} [MODEL\_NAME]". 
\textit{System Prompt}, \textit{Explanation Content}, \textit{Evaluation Criteria}, \textit{Advice Instruction}, and \textit{Debugging Instruction} are predefined instructions. 
The debugging process focuses on four aspects: faithfulness, completeness, minimality, and accuracy. By analyzing the explanations through these lenses, the LM Debugger offers users suggestions on potential areas for improvement within the LM and highlights any noteworthy observations. Our approach democratizes the understanding of LM reasoning behavior, empowering users from diverse backgrounds to engage with and benefit from the insights provided by the explanations. We provide a detailed description of the LM Debugger in the Appendix~\ref{app:lm_debugger}.



% \begin{table}[]
% \centering
% \resizebox{0.5\columnwidth}{!}{%
% \begin{tabular}{@{}c|l@{}}
% \toprule
% \begin{tabular}[c]{@{}c@{}}System\\  Prompt\end{tabular} & \begin{tabular}[c]{@{}l@{}}You're a professional researcher in NLP. \\ Write it step by step.\end{tabular}                                                 \\ \midrule
% Q             & Question content is...                                                                                         \\ \midrule
% A             & The predicted choice is...                                                                                     \\ \midrule
% R             & \begin{tabular}[c]{@{}l@{}}According to the model top reason-elements + \\ $\mathcal{K}$ + explain the model reasoning process with ``since...''\end{tabular}\\ \midrule
% P             & According to...                                                                                                \\ \midrule
% T             & \begin{tabular}[c]{@{}l@{}}Explain why the model doesn't choose other options \\ with ``The other potential choices...''\end{tabular}                          \\ \bottomrule
% \end{tabular}
% }
% \caption{The instructions for explanation generators. }
% \label{tab:instruction}
% \vspace{-10pt}
% \end{table}

